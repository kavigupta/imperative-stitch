{"run_example.py": "# import tutorials.keras.text_NER as ft\n# import tutorials.keras.brat_tag as ft\nimport tutorials.RecommenderSystems.rs_rating_demo as ft\n# from middleware.utils import TimeStat, Chart\n# import matplotlib.pyplot as plt\n# import matplotlib.gridspec as gridspec\n# from matplotlib.font_manager import FontProperties\n# plt.rcParams['font.sans-serif'] = ['SimHei']\n# plt.rcParams['axes.unicode_minus'] = False\n\n\ndef main():\n    ft.main()\n    # x=y=[1,2,3]\n    # plt.plot(x, y, color='g', linestyle='-')  # \u7ed8\u5236\n    # plt.grid(True, ls = '--')\n    # plt.show()\n\nif __name__ == \"__main__\":\n    main()\n", "src/script.py": "# coding: utf-8\nimport os\nimport sys\n\n\ndef format_file(filename, str1, str2):\n    \"\"\"\n    \u6587\u4ef6\u5185\u5bb9\u7684\u66ff\u6362\u529f\u80fd\n    :return:\n    \"\"\"\n    with open(filename, 'r') as f:\n        var_object = f.read()\n        if \"gitalk\" not in var_object:\n            var_object = var_object.replace(str1, str2)\n        # print(var_object)\n\n    f = open(filename, \"w\")\n    f.write(var_object)\n\n\nif __name__ == \"__main__\":\n    if len(sys.argv) == 3:\n        version, u_type = sys.argv[1], sys.argv[2]\n    else:\n        print(\"Usage: \u53c2\u6570\u4e2a\u6570\u4e3a%s - \u9519\u8bef\uff0c\u5e94\u8be5\u6539\u4e3a3\" % len(sys.argv))\n        sys.exit(-1)\n\n    tag = True\n    if u_type == \"index\":\n        tag = False\n        # if version == \"home\":\n        #     filename = \"_book/index.html\"\n        # else:\n        #     filename = \"_book/docs/%s/index.html\" % version\n        # str1 = \"\"\"\n        # </head>\n        # <body>\n        # \"\"\"\n\n        # str2 = \"\"\"\n        # <script type=\"text/javascript\">\n        #     function hidden_left(){\n        #         document.getElementsByClassName(\"btn pull-left js-toolbar-action\")[0].click()\n        #     }\n        #     // window.onload = hidden_left();\n        # </script>\n        # </head>\n        # <body onload=\"hidden_left()\">\n        # \"\"\"\n    elif u_type == \"book\":\n        if version == \"home\":\n            filename = \"book.json\"\n            tag = False\n        else:\n            filename = \"docs/%s/book.json\" % version\n            str1 = \"https://github.com/apachecn/AiLearning/blob/master\"\n            str2 = \"https://github.com/apachecn/AiLearning/blob/master/docs/%s\" % version\n\n    elif u_type == \"powered\":\n        if version == \"home\":\n            filename = \"node_modules/gitbook-plugin-tbfed-pagefooter/index.js\"\n        else:\n            filename = \"docs/%s/node_modules/gitbook-plugin-tbfed-pagefooter/index.js\" % version\n        str1 = \"powered by Gitbook\"\n        str2 = \"\u7531 ApacheCN \u56e2\u961f\u63d0\u4f9b\u652f\u6301\"\n\n    elif u_type == \"gitalk\":\n        if version == \"home\":\n            filename = \"node_modules/gitbook-plugin-tbfed-pagefooter/index.js\"\n        else:\n            filename = \"docs/%s/node_modules/gitbook-plugin-tbfed-pagefooter/index.js\" % version\n        str1 = \"\"\"      var str = ' \\\\n\\\\n<footer class=\"page-footer\">' + _copy +\n        '<span class=\"footer-modification\">' +\n        _label +\n        '\\\\n{{file.mtime | date(\"' + _format +\n        '\")}}\\\\n</span></footer>'\"\"\"\n\n        str2 = \"\"\"\n      var str = '\\\\n\\\\n'+\n      '\\\\n<hr/>'+\n      '\\\\n<div align=\"center\">'+\n      '\\\\n    <p><a href=\"http://www.apachecn.org\" target=\"_blank\"><font face=\"KaiTi\" size=\"6\" color=\"red\">\u6211\u4eec\u4e00\u76f4\u5728\u52aa\u529b</font></a></p>'+\n      '\\\\n    <p><a href=\"https://github.com/apachecn/AiLearning/\" target=\"_blank\">apachecn/AiLearning</a></p>'+\n      '\\\\n    <p><iframe align=\"middle\" src=\"https://ghbtns.com/github-btn.html?user=apachecn&repo=AiLearning&type=watch&count=true&v=2\" frameborder=\"0\" scrolling=\"0\" width=\"100px\" height=\"25px\"></iframe>'+\n      '\\\\n    <iframe align=\"middle\" src=\"https://ghbtns.com/github-btn.html?user=apachecn&repo=AiLearning&type=star&count=true\" frameborder=\"0\" scrolling=\"0\" width=\"100px\" height=\"25px\"></iframe>'+\n      '\\\\n    <iframe align=\"middle\" src=\"https://ghbtns.com/github-btn.html?user=apachecn&repo=AiLearning&type=fork&count=true\" frameborder=\"0\" scrolling=\"0\" width=\"100px\" height=\"25px\"></iframe>'+\n      '\\\\n    <a target=\"_blank\" href=\"//shang.qq.com/wpa/qunwpa?idkey=bcee938030cc9e1552deb3bd9617bbbf62d3ec1647e4b60d9cd6b6e8f78ddc03\"><img border=\"0\" src=\"http://data.apachecn.org/img/logo/ApacheCN-group.png\" alt=\"ML\u00a0|\u00a0ApacheCN\" title=\"ML\u00a0|\u00a0ApacheCN\"></a></p>'+\n      '\\\\n</div>'+\n      '\\\\n <div style=\"text-align:center;margin:0 0 10.5px;\">'+\n      '\\\\n     <script async src=\"//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js\"></script>'+\n      '\\\\n     <ins class=\"adsbygoogle\"'+\n      '\\\\n         style=\"display:inline-block;width:728px;height:90px\"'+\n      '\\\\n         data-ad-client=\"ca-pub-3565452474788507\"'+\n      '\\\\n         data-ad-slot=\"2543897000\">'+\n      '\\\\n     </ins>'+\n      '\\\\n     <script>(adsbygoogle = window.adsbygoogle || []).push({});</script>'+\n      '\\\\n'+\n      '\\\\n    <script>'+\n      '\\\\n      var _hmt = _hmt || [];'+\n      '\\\\n      (function() {'+\n      '\\\\n        var hm = document.createElement(\"script\");'+\n      '\\\\n        hm.src = \"https://hm.baidu.com/hm.js?33149aa6702022b1203201da06e23d81\";'+\n      '\\\\n        var s = document.getElementsByTagName(\"script\")[0]; '+\n      '\\\\n        s.parentNode.insertBefore(hm, s);'+\n      '\\\\n      })();'+\n      '\\\\n    </script>'+\n      '\\\\n'+\n      '\\\\n    <script async src=\"https://www.googletagmanager.com/gtag/js?id=UA-127082511-1\"></script>'+\n      '\\\\n    <script>'+\n      '\\\\n      window.dataLayer = window.dataLayer || [];'+\n      '\\\\n      function gtag(){dataLayer.push(arguments);}'+\n      '\\\\n      gtag(\\\\'js\\\\', new Date());'+\n      '\\\\n'+\n      '\\\\n      gtag(\\\\'config\\\\', \\\\'UA-127082511-1\\\\');'+\n      '\\\\n    </script>'+\n     '\\\\n</div>'+\n      '\\\\n'+\n      '\\\\n<meta name=\"google-site-verification\" content=\"pyo9N70ZWyh8JB43bIu633mhxesJ1IcwWCZlM3jUfFo\" />'+\n      '\\\\n<iframe src=\"https://www.bilibili.com/read/cv2710377\" style=\"display:none\"></iframe>'+ \n      '\\\\n<img src=\"http://t.cn/AiCoDHwb\" hidden=\"hidden\" />'\n\n      str += '\\\\n\\\\n'+\n      '\\\\n<div>'+\n      '\\\\n    <link rel=\"stylesheet\" href=\"https://unpkg.com/gitalk/dist/gitalk.css\">'+\n      '\\\\n    <script src=\"https://unpkg.com/gitalk/dist/gitalk.min.js\"></script>'+\n      '\\\\n    <script src=\"https://cdn.bootcss.com/blueimp-md5/2.10.0/js/md5.min.js\"></script>'+\n      '\\\\n    <div id=\"gitalk-container\"></div>'+\n      '\\\\n    <script type=\"text/javascript\">'+\n      '\\\\n        const gitalk = new Gitalk({'+\n      '\\\\n        clientID: \\\\'2e62dee5b9896e2eede6\\\\','+\n      '\\\\n        clientSecret: \\\\'ca6819a54656af0d87960af15315320f8a628a53\\\\','+\n      '\\\\n        repo: \\\\'AiLearning\\\\','+\n      '\\\\n        owner: \\\\'apachecn\\\\','+\n      '\\\\n        admin: [\\\\'jiangzhonglian\\\\', \\\\'wizardforcel\\\\'],'+\n      '\\\\n        id: md5(location.pathname),'+\n      '\\\\n        distractionFreeMode: false'+\n      '\\\\n        })'+\n      '\\\\n        gitalk.render(\\\\'gitalk-container\\\\')'+\n      '\\\\n    </script>'+\n      '\\\\n</div>'\n\n      str += '\\\\n\\\\n<footer class=\"page-footer\">' + _copy + '<span class=\"footer-modification\">' + _label + '\\\\n{{file.mtime | date(\"' + _format + '\")}}\\\\n</span></footer>'\n        \"\"\"\n\n    # \u72b6\u6001\u4e3a True \u5c31\u8fdb\u884c\u66ff\u6362\n    if tag: format_file(filename, str1, str2)\n", "src/py2.x/ml/15.BigData_MapReduce/py27dbg.py": "'''\nCreated on Feb 27, 2011\nMapReduce version of Pegasos SVM\nUsing mrjob to automate job flow\nAuthor: Peter\n'''\nfrom mrjob.job import MRJob\n\nimport pickle\nfrom numpy import *\n\nclass MRsvm(MRJob):\n                                                 \n    def map(self, mapperId, inVals): #needs exactly 2 arguments\n        if False: yield\n        yield (1, 22)\n\n    def reduce(self, _, packedVals):\n        yield \"fuck ass\" \n        \n    def steps(self):\n        return ([self.mr(mapper=self.map, reducer=self.reduce)])\n\nif __name__ == '__main__':\n    MRsvm.run()\n", "src/py2.x/ml/15.BigData_MapReduce/mrMeanReducer.py": "#!/usr/bin/python\n# coding:utf8\n\n'''\nCreated on 2017-04-06\nUpdate  on 2017-06-20\nMachine Learning in Action Chapter 18\nMap Reduce Job for Hadoop Streaming\nAuthor: Peter/ApacheCN-xy/\u7247\u523b\nGitHub: https://github.com/apachecn/AiLearning\n'''\nfrom __future__ import print_function\nimport sys\n\n'''\n    mapper \u63a5\u53d7\u539f\u59cb\u7684\u8f93\u5165\u5e76\u4ea7\u751f\u4e2d\u95f4\u503c\u4f20\u9012\u7ed9 reducer\u3002\n    \u5f88\u591a\u7684mapper\u662f\u5e76\u884c\u6267\u884c\u7684\uff0c\u6240\u4ee5\u9700\u8981\u5c06\u8fd9\u4e9bmapper\u7684\u8f93\u51fa\u5408\u5e76\u6210\u4e00\u4e2a\u503c\u3002\n    \u5373: \u5c06\u4e2d\u95f4\u7684 key/value \u5bf9\u8fdb\u884c\u7ec4\u5408\u3002\n'''\n\n\ndef read_input(file):\n    for line in file:\n        yield line.rstrip()\t\t\t\t\t\t# \u8fd4\u56de\u503c\u4e2d\u5305\u542b\u8f93\u5165\u6587\u4ef6\u7684\u6bcf\u4e00\u884c\u7684\u6570\u636e\u7684\u4e00\u4e2a\u5927\u7684List\n\n\ninput = read_input(sys.stdin)\t\t\t\t\t# \u521b\u5efa\u4e00\u4e2a\u8f93\u5165\u7684\u6570\u636e\u884c\u7684\u5217\u8868list\n\n# \u5c06\u8f93\u5165\u884c\u5206\u5272\u6210\u5355\u72ec\u7684\u9879\u76ee\u5e76\u5b58\u50a8\u5728\u5217\u8868\u7684\u5217\u8868\u4e2d\nmapperOut = [line.split('\\t') for line in input]\n# \u8f93\u5165 \u6570\u636e\u7684\u4e2a\u6570\uff0cn\u4e2a\u6570\u636e\u7684\u5747\u503c\uff0cn\u4e2a\u6570\u636e\u5e73\u65b9\u4e4b\u540e\u7684\u5747\u503c\nprint (mapperOut)\n\n# \u7d2f\u8ba1\u6837\u672c\u603b\u548c\uff0c\u603b\u548c \u548c \u5e73\u5206\u548c\u7684\u603b\u548c\ncumN, cumVal, cumSumSq = 0.0, 0.0, 0.0\nfor instance in mapperOut:\n    nj = float(instance[0])\n    cumN += nj\n    cumVal += nj*float(instance[1])\n    cumSumSq += nj*float(instance[2])\n\n# \u8ba1\u7b97\u5747\u503c( varSum\u662f\u8ba1\u7b97\u65b9\u5dee\u7684\u5c55\u5f00\u5f62\u5f0f )\nmean_ = cumVal/cumN\nvarSum = (cumSumSq - 2*mean_*cumVal + cumN*mean_*mean_)/cumN\n# \u8f93\u51fa \u6570\u636e\u603b\u91cf\uff0c\u5747\u503c\uff0c\u5e73\u65b9\u7684\u5747\u503c\uff08\u65b9\u5dee\uff09\nprint (\"\u6570\u636e\u603b\u91cf: %d\\t\u5747\u503c: %f\\t\u65b9\u5dee: %f\" % (cumN, mean_, varSum))\nprint(\"reduce report: still alive\", file=sys.stderr)\n", "src/py2.x/ml/15.BigData_MapReduce/mrSVM.py": "#!/usr/bin/python\n# coding:utf8\n'''\nCreated on 2017-04-07\nUpdate  on 2017-06-20\nMapReduce version of Pegasos SVM\nUsing mrjob to automate job flow\nAuthor: Peter/ApacheCN-xy/\u7247\u523b\nGitHub: https://github.com/apachecn/AiLearning\n'''\nfrom mrjob.job import MRJob\n\nimport pickle\nfrom numpy import *\n\n\nclass MRsvm(MRJob):\n    DEFAULT_INPUT_PROTOCOL = 'json_value'\n\n    def __init__(self, *args, **kwargs):\n        super(MRsvm, self).__init__(*args, **kwargs)\n        self.data = pickle.load(open('/opt/git/MachineLearnidata/15.BigData_MapReduce/svmDat27'))\n        self.w = 0\n        self.eta = 0.69\n        self.dataList = []\n        self.k = self.options.batchsize\n        self.numMappers = 1\n        self.t = 1  # iteration number\n\n    def configure_options(self):\n        super(MRsvm, self).configure_options()\n        self.add_passthrough_option(\n            '--iterations', dest='iterations', default=2, type='int',\n            help='T: number of iterations to run')\n        self.add_passthrough_option(\n            '--batchsize', dest='batchsize', default=100, type='int',\n            help='k: number of data points in a batch')\n\n    def map(self, mapperId, inVals):  # \u9700\u8981 2 \u4e2a\u53c2\u6570\n        # input: nodeId, ('w', w-vector) OR nodeId, ('x', int)\n        if False:\n            yield\n        if inVals[0] == 'w':                  # \u79ef\u7d2f w\u5411\u91cf\n            self.w = inVals[1]\n        elif inVals[0] == 'x':\n            self.dataList.append(inVals[1])   # \u7d2f\u79ef\u6570\u636e\u70b9\u8ba1\u7b97\n        elif inVals[0] == 't':                # \u8fed\u4ee3\u6b21\u6570\n            self.t = inVals[1]\n        else:\n            self.eta = inVals                 # \u8fd9\u7528\u4e8e debug\uff0c eta\u672a\u5728map\u4e2d\u4f7f\u7528\n\n    def map_fin(self):\n        labels = self.data[:, -1]\n        X = self.data[:, :-1]                # \u5c06\u6570\u636e\u91cd\u65b0\u5f62\u6210 X \u548c Y\n        if self.w == 0:\n            self.w = [0.001] * shape(X)[1]   # \u5728\u7b2c\u4e00\u6b21\u8fed\u4ee3\u65f6\uff0c\u521d\u59cb\u5316 w\n        for index in self.dataList:\n            p = mat(self.w)*X[index, :].T    # calc p=w*dataSet[key].T\n            if labels[index]*p < 1.0:\n                yield (1, ['u', index])      # \u786e\u4fdd\u4e00\u5207\u6570\u636e\u5305\u542b\u76f8\u540c\u7684key\n        yield (1, ['w', self.w])             # \u5b83\u4eec\u5c06\u5728\u540c\u4e00\u4e2a reducer\n        yield (1, ['t', self.t])\n\n    def reduce(self, _, packedVals):\n        for valArr in packedVals:            # \u4ece\u6d41\u8f93\u5165\u83b7\u53d6\u503c\n            if valArr[0] == 'u':\n                self.dataList.append(valArr[1])\n            elif valArr[0] == 'w':\n                self.w = valArr[1]\n            elif valArr[0] == 't':\n                self.t = valArr[1]\n\n        labels = self.data[:, -1]\n        X = self.data[:, 0:-1]\n        wMat = mat(self.w)\n        wDelta = mat(zeros(len(self.w)))\n\n        for index in self.dataList:\n            wDelta += float(labels[index]) * X[index, :]  # wDelta += label*dataSet\n        eta = 1.0/(2.0*self.t)       # calc new: eta\n        # calc new: w = (1.0 - 1/t)*w + (eta/k)*wDelta\n        wMat = (1.0 - 1.0/self.t)*wMat + (eta/self.k)*wDelta\n        for mapperNum in range(1, self.numMappers+1):\n            yield (mapperNum, ['w', wMat.tolist()[0]])    # \u53d1\u51fa w\n            if self.t < self.options.iterations:\n                yield (mapperNum, ['t', self.t+1])        # \u589e\u91cf T\n                for j in range(self.k/self.numMappers):   # emit random ints for mappers iid\n                    yield (mapperNum, ['x', random.randint(shape(self.data)[0])])\n\n    def steps(self):\n        return ([self.mr(mapper=self.map, reducer=self.reduce, mapper_final=self.map_fin)] * self.options.iterations)\n\n\nif __name__ == '__main__':\n    MRsvm.run()\n", "src/py2.x/ml/15.BigData_MapReduce/wc.py": "#!/usr/bin/python\n# coding:utf8\nfrom mrjob.job import MRJob\n\n\nclass MRWordCountUtility(MRJob):\n\n    def __init__(self, *args, **kwargs):\n        super(MRWordCountUtility, self).__init__(*args, **kwargs)\n        self.chars = 0\n        self.words = 0\n        self.lines = 0\n\n    def mapper(self, _, line):\n        if False:\n            yield  # I'm a generator!\n\n        self.chars += len(line) + 1  # +1 for newline\n        self.words += sum(1 for word in line.split() if word.strip())\n        self.lines += 1\n\n    def mapper_final(self):\n        yield('chars', self.chars)\n        yield('words', self.words)\n        yield('lines', self.lines)\n\n    def reducer(self, key, values):\n        yield(key, sum(values))\n\n\nif __name__ == '__main__':\n    MRWordCountUtility.run()\n", "src/py2.x/ml/15.BigData_MapReduce/mrSVMkickStart.py": "'''\nCreated on Feb 27, 2011\n\nAuthor: Peter\n'''\nfrom mrjob.protocol import JSONProtocol\nfrom numpy import *\n\nfw=open('kickStart2.txt', 'w')\nfor i in [1]:\n    for j in range(100):\n        fw.write('[\"x\", %d]\\n' % random.randint(200))\nfw.close()", "src/py2.x/ml/15.BigData_MapReduce/proximalSVM.py": "#!/usr/bin/python\n# coding:utf8\n'''\nCreated on 2011-02-25\nUpdate  on 2017-06-20\nAuthor: Peter/ApacheCN-xy/\u7247\u523b\nGitHub: https://github.com/apachecn/AiLearning\n'''\nfrom __future__ import print_function\nimport base64\nimport pickle\n\nimport numpy\n\n\ndef map(key, value):\n   # input key= class for one training example, e.g. \"-1.0\"\n   classes = [float(item) for item in key.split(\",\")]   # e.g. [-1.0]\n   D = numpy.diag(classes)\n\n   # input value = feature vector for one training example, e.g. \"3.0, 7.0, 2.0\"\n   featurematrix = [float(item) for item in value.split(\",\")]\n   A = numpy.matrix(featurematrix)\n\n   # create matrix E and vector e\n   e = numpy.matrix(numpy.ones(len(A)).reshape(len(A), 1))\n   E = numpy.matrix(numpy.append(A, -e, axis=1)) \n\n   # create a tuple with the values to be used by reducer\n   # and encode it with base64 to avoid potential trouble with '\\t' and '\\n' used\n   # as default separators in Hadoop Streaming\n   producedvalue = base64.b64encode(pickle.dumps((E.T*E, E.T*D*e)))\n\n   # note: a single constant key \"producedkey\" sends to only one reducer\n   # somewhat \"atypical\" due to low degree of parallism on reducer side\n   print(\"producedkey\\t%s\" % (producedvalue))\n\ndef reduce(key, values, mu=0.1):\n  sumETE = None\n  sumETDe = None\n\n  # key isn't used, so ignoring it with _ (underscore).\n  for _, value in values:\n    # unpickle values\n    ETE, ETDe = pickle.loads(base64.b64decode(value))\n    if sumETE == None:\n      # create the I/mu with correct dimensions\n      sumETE = numpy.matrix(numpy.eye(ETE.shape[1])/mu)\n    sumETE += ETE\n\n    if sumETDe == None:\n      # create sumETDe with correct dimensions\n      sumETDe = ETDe\n    else:\n      sumETDe += ETDe\n\n    # note: omega = result[:-1] and gamma = result[-1]\n    # but printing entire vector as output\n    result = sumETE.I*sumETDe\n    print(\"%s\\t%s\" % (key, str(result.tolist())))\n", "src/py2.x/ml/15.BigData_MapReduce/pegasos.py": "#!/usr/bin/python\n# coding:utf8\n'''\nCreated on 2017-04-07\nSequential Pegasos \nthe input T is k*T in Batch Pegasos\nAuthor: Peter/ApacheCN-xy\n'''\nfrom __future__ import print_function\nfrom numpy import *\n\n\ndef loadDataSet(fileName):\n    dataMat = []\n    labelMat = []\n    fr = open(fileName)\n    for line in fr.readlines():\n        lineArr = line.strip().split('\\t')\n        # dataMat.append([float(lineArr[0]), float(lineArr[1]), float(lineArr[2])])\n        dataMat.append([float(lineArr[0]), float(lineArr[1])])\n        labelMat.append(float(lineArr[2]))\n    return dataMat, labelMat\n\n\ndef seqPegasos(dataSet, labels, lam, T):\n    m, n = shape(dataSet)\n    w = zeros(n)\n    for t in range(1, T+1):\n        i = random.randint(m)\n        eta = 1.0/(lam*t)\n        p = predict(w, dataSet[i, :])\n        if labels[i]*p < 1:\n            w = (1.0 - 1/t)*w + eta*labels[i]*dataSet[i, :]\n        else:\n            w = (1.0 - 1/t)*w\n        print(w)\n    return w\n\n\ndef predict(w, x):\n    return w*x.T  # \u5c31\u662f\u9884\u6d4b y \u7684\u503c\n\n\ndef batchPegasos(dataSet, labels, lam, T, k):\n    \"\"\"batchPegasos()\n\n    Args:\n        dataMat    \u7279\u5f81\u96c6\u5408\n        labels     \u5206\u7c7b\u7ed3\u679c\u96c6\u5408\n        lam        \u56fa\u5b9a\u503c\n        T          \u8fed\u4ee3\u6b21\u6570\n        k          \u5f85\u5904\u7406\u5217\u8868\u5927\u5c0f\n    Returns:\n        w          \u56de\u5f52\u7cfb\u6570\n    \"\"\"\n    m, n = shape(dataSet)\n    w = zeros(n)  # \u56de\u5f52\u7cfb\u6570\n    dataIndex = range(m)\n    for t in range(1, T+1):\n        wDelta = mat(zeros(n))  # \u91cd\u7f6e wDelta\n\n        # \u5b83\u662f\u5b66\u4e60\u7387\uff0c\u4ee3\u8868\u4e86\u6743\u91cd\u8c03\u6574\u5e45\u5ea6\u7684\u5927\u5c0f\u3002\uff08\u4e5f\u53ef\u4ee5\u7406\u89e3\u4e3a\u968f\u673a\u68af\u5ea6\u7684\u6b65\u957f\uff0c\u4f7f\u5b83\u4e0d\u65ad\u51cf\u5c0f\uff0c\u4fbf\u4e8e\u62df\u5408\uff09\n        # \u8f93\u5165T\u548cK\u5206\u522b\u8bbe\u5b9a\u4e86\u8fed\u4ee3\u6b21\u6570\u548c\u5f85\u5904\u7406\u5217\u8868\u7684\u5927\u5c0f\u3002\u5728T\u6b21\u8fed\u4ee3\u8fc7\u7a0b\u4e2d\uff0c\u6bcf\u6b21\u9700\u8981\u91cd\u65b0\u8ba1\u7b97eta\n        eta = 1.0/(lam*t)\n        random.shuffle(dataIndex)\n        for j in range(k):      # \u5168\u90e8\u7684\u8bad\u7ec3\u96c6  \u5185\u5faa\u73af\u4e2d\u6267\u884c\u6279\u5904\u7406\uff0c\u5c06\u5206\u7c7b\u9519\u8bef\u7684\u503c\u5168\u90e8\u505a\u7d2f\u52a0\u540e\u66f4\u65b0\u6743\u91cd\u5411\u91cf\n            i = dataIndex[j]\n            p = predict(w, dataSet[i, :])              # mapper \u4ee3\u7801\n\n            # \u5982\u679c\u9884\u6d4b\u6b63\u786e\uff0c\u5e76\u4e14\u9884\u6d4b\u7ed3\u679c\u7684\u7edd\u5bf9\u503c>=1\uff0c\u56e0\u4e3a\u6700\u5927\u95f4\u9694\u4e3a1, \u8ba4\u4e3a\u6ca1\u95ee\u9898\u3002\n            # \u5426\u5219\u7b97\u662f\u9884\u6d4b\u9519\u8bef, \u901a\u8fc7\u9884\u6d4b\u9519\u8bef\u7684\u7ed3\u679c\uff0c\u6765\u7d2f\u8ba1\u66f4\u65b0w.\n            if labels[i]*p < 1:                        # mapper \u4ee3\u7801\n                wDelta += labels[i]*dataSet[i, :].A    # \u7d2f\u79ef\u53d8\u5316\n        # w\u901a\u8fc7\u4e0d\u65ad\u7684\u968f\u673a\u68af\u5ea6\u7684\u65b9\u5f0f\u6765\u4f18\u5316\n        w = (1.0 - 1/t)*w + (eta/k)*wDelta             # \u5728\u6bcf\u4e2a T\u4e0a\u5e94\u7528\u66f4\u6539\n        # print '-----', w\n    # print '++++++', w\n    return w\n\n\ndatArr, labelList = loadDataSet('data/15.BigData_MapReduce/testSet.txt')\ndatMat = mat(datArr)\n# finalWs = seqPegasos(datMat, labelList, 2, 5000)\nfinalWs = batchPegasos(datMat, labelList, 2, 50, 100)\nprint(finalWs)\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nfig = plt.figure()\nax = fig.add_subplot(111)\nx1 = []\ny1 = []\nxm1 = []\nym1 = []\nfor i in range(len(labelList)):\n    if labelList[i] == 1.0:\n        x1.append(datMat[i, 0])\n        y1.append(datMat[i, 1])\n    else:\n        xm1.append(datMat[i, 0])\n        ym1.append(datMat[i, 1])\nax.scatter(x1, y1, marker='s', s=90)\nax.scatter(xm1, ym1, marker='o', s=50, c='red')\nx = arange(-6.0, 8.0, 0.1)\ny = (-finalWs[0, 0]*x - 0)/finalWs[0, 1]\n# y2 = (0.43799*x)/0.12316\ny2 = (0.498442*x)/0.092387  # 2 iterations\nax.plot(x, y)\nax.plot(x, y2, 'g-.')\nax.axis([-6, 8, -4, 5])\nax.legend(('50 Iterations', '2 Iterations'))\nplt.show()\n", "src/py2.x/ml/15.BigData_MapReduce/mrMean.py": "#!/usr/bin/python\n# coding:utf8\n'''\nCreated on 2017-04-07\nUpdate  on 2017-06-20\nAuthor: Peter/ApacheCN-xy/\u7247\u523b\nGitHub: https://github.com/apachecn/AiLearning\n'''\nfrom mrjob.job import MRJob\n\n\nclass MRmean(MRJob):\n    def __init__(self, *args, **kwargs):  # \u5bf9\u6570\u636e\u521d\u59cb\u5316\n        super(MRmean, self).__init__(*args, **kwargs)\n        self.inCount = 0\n        self.inSum = 0\n        self.inSqSum = 0\n\n    # \u63a5\u53d7\u8f93\u5165\u6570\u636e\u6d41\n    def map(self, key, val):  # \u9700\u8981 2 \u4e2a\u53c2\u6570\uff0c\u6c42\u6570\u636e\u7684\u548c\u4e0e\u5e73\u65b9\u548c\n        if False:\n            yield\n        inVal = float(val)\n        self.inCount += 1\n        self.inSum += inVal\n        self.inSqSum += inVal*inVal\n\n    # \u6240\u6709\u8f93\u5165\u5230\u8fbe\u540e\u5f00\u59cb\u5904\u7406\n    def map_final(self):  # \u8ba1\u7b97\u6570\u636e\u7684\u5e73\u5747\u503c\uff0c\u5e73\u65b9\u7684\u5747\u503c\uff0c\u5e76\u8fd4\u56de\n        mn = self.inSum/self.inCount\n        mnSq = self.inSqSum/self.inCount\n        yield (1, [self.inCount, mn, mnSq])\n\n    def reduce(self, key, packedValues):\n        cumN, cumVal, cumSumSq = 0.0, 0.0, 0.0\n        for valArr in packedValues:  # \u4ece\u8f93\u5165\u6d41\u4e2d\u83b7\u53d6\u503c\n            nj = float(valArr[0])\n            cumN += nj\n            cumVal += nj*float(valArr[1])\n            cumSumSq += nj*float(valArr[2])\n        mean = cumVal/cumN\n        var = (cumSumSq - 2*mean*cumVal + cumN*mean*mean)/cumN\n        yield (mean, var)  # \u53d1\u51fa\u5e73\u5747\u503c\u548c\u65b9\u5dee\n\n    def steps(self):\n        \"\"\"\n        step\u65b9\u6cd5\u5b9a\u4e49\u6267\u884c\u7684\u6b65\u9aa4\u3002\n        \u6267\u884c\u987a\u5e8f\u4e0d\u5fc5\u5b8c\u5168\u9075\u5faamap-reduce\u6a21\u5f0f\u3002\n        \u4f8b\u5982: \n            1. map-reduce-reduce-reduce\n            2. map-reduce-map-reduce-map-reduce\n        \u5728step\u65b9\u6cd5\u91cc\uff0c\u9700\u8981\u4e3amrjob\u6307\u5b9amapper\u548creducer\u7684\u540d\u79f0\u3002\u5982\u679c\u6ca1\u6709\uff0c\u5b83\u5c06\u9ed8\u8ba4\u8c03\u7528mapper\u548creducer\u65b9\u6cd5\u3002\n\n        \u5728mapper \u548c mapper_final\u4e2d\u8fd8\u53ef\u4ee5\u5171\u4eab\u72b6\u6001\uff0cmapper \u6216 mapper_final \u4e0d\u80fd reducer\u4e4b\u95f4\u5171\u4eab\u72b6\u6001\u3002\n        \"\"\"\n        return ([self.mr(mapper=self.map, mapper_final=self.map_final, reducer=self.reduce,)])\n\n\nif __name__ == '__main__':\n    MRmean.run()\n", "src/py2.x/ml/15.BigData_MapReduce/mrMeanMapper.py": "#!/usr/bin/python\n# coding:utf8\n'''\nCreated on 2017-04-06\nUpdate  on 2017-06-20\nMachine Learning in Action Chapter 18\nMap Reduce Job for Hadoop Streaming\nAuthor: Peter/ApacheCN-xy/\u7247\u523b\nGitHub: https://github.com/apachecn/AiLearning\n'''\nfrom __future__ import print_function\nimport sys\nfrom numpy import mat, mean, power\n\n'''\n    \u8fd9\u4e2amapper\u6587\u4ef6\u6309\u884c\u8bfb\u53d6\u6240\u6709\u7684\u8f93\u5165\u5e76\u521b\u5efa\u4e00\u7ec4\u5bf9\u5e94\u7684\u6d6e\u70b9\u6570\uff0c\u7136\u540e\u5f97\u5230\u6570\u7ec4\u7684\u957f\u5ea6\u5e76\u521b\u5efaNumPy\u77e9\u9635\u3002\n    \u518d\u5bf9\u6240\u6709\u7684\u503c\u8fdb\u884c\u5e73\u65b9\uff0c\u6700\u540e\u5c06\u5747\u503c\u548c\u5e73\u65b9\u540e\u7684\u5747\u503c\u53d1\u9001\u51fa\u53bb\u3002\u8fd9\u4e9b\u503c\u5c06\u7528\u6765\u8ba1\u7b97\u5168\u5c40\u7684\u5747\u503c\u548c\u65b9\u5dee\u3002\n\n    Args: \n        file \u8f93\u5165\u6570\u636e\n    Return: \n'''\n\n\ndef read_input(file):\n    for line in file:\n        yield line.rstrip()             # \u8fd4\u56de\u4e00\u4e2a yield \u8fed\u4ee3\u5668\uff0c\u6bcf\u6b21\u83b7\u53d6\u4e0b\u4e00\u4e2a\u503c\uff0c\u8282\u7ea6\u5185\u5b58\u3002\n\n\ninput = read_input(sys.stdin)            # \u521b\u5efa\u4e00\u4e2a\u8f93\u5165\u7684\u6570\u636e\u884c\u7684\u5217\u8868list\ninput = [float(line) for line in input]  # \u5c06\u5f97\u5230\u7684\u6570\u636e\u8f6c\u5316\u4e3a float \u7c7b\u578b\nnumInputs = len(input)                   # \u83b7\u53d6\u6570\u636e\u7684\u4e2a\u6570\uff0c\u5373\u8f93\u5165\u6587\u4ef6\u7684\u6570\u636e\u7684\u884c\u6570\ninput = mat(input)                       # \u5c06 List \u8f6c\u6362\u4e3a\u77e9\u9635\nsqInput = power(input, 2)                # \u5c06\u77e9\u9635\u7684\u6570\u636e\u5206\u522b\u6c42 \u5e73\u65b9\uff0c\u5373 2\u6b21\u65b9\n\n# \u8f93\u51fa \u6570\u636e\u7684\u4e2a\u6570\uff0cn\u4e2a\u6570\u636e\u7684\u5747\u503c\uff0cn\u4e2a\u6570\u636e\u5e73\u65b9\u4e4b\u540e\u7684\u5747\u503c\n# \u7b2c\u4e00\u884c\u662f\u6807\u51c6\u8f93\u51fa\uff0c\u4e5f\u5c31\u662freducer\u7684\u8f93\u51fa\n# \u7b2c\u4e8c\u884c\u8bc6\u6807\u51c6\u9519\u8bef\u8f93\u51fa\uff0c\u5373\u5bf9\u4e3b\u8282\u70b9\u4f5c\u51fa\u7684\u54cd\u5e94\u62a5\u544a\uff0c\u8868\u660e\u672c\u8282\u70b9\u5de5\u4f5c\u6b63\u5e38\u3002\n# \u3010\u8fd9\u4e0d\u5c31\u662f\u9762\u8bd5\u7684\u88c5\u903c\u91cd\u70b9\u5417\uff1f\u5982\u4f55\u8bbe\u8ba1\u76d1\u542c\u67b6\u6784\u7ec6\u8282\u3011\u6ce8\u610f: \u4e00\u4e2a\u597d\u7684\u4e60\u60ef\u662f\u60f3\u6807\u51c6\u9519\u8bef\u8f93\u51fa\u53d1\u9001\u62a5\u544a\u3002\u5982\u679c\u67d0\u4efb\u52a110\u5206\u949f\u5185\u6ca1\u6709\u62a5\u544a\u8f93\u51fa\uff0c\u5219\u5c06\u88abHadoop\u4e2d\u6b62\u3002\nprint(\"%d\\t%f\\t%f\" % (numInputs, mean(input), mean(sqInput)))  # \u8ba1\u7b97\u5747\u503c\nprint(\"map report: still alive\", file=sys.stderr)\n", "src/py2.x/ml/13.PCA/pca.py": "#!/usr/bin/python\n# coding: utf-8\n\n'''\nCreated on Jun 1, 2011\nUpdate  on 2017-05-18\nAuthor: Peter Harrington/\u7247\u523b\nGitHub: https://github.com/apachecn/AiLearning\n'''\nfrom __future__ import print_function\nfrom numpy import *\nimport matplotlib.pyplot as plt\nprint(__doc__)\n\n\ndef loadDataSet(fileName, delim='\\t'):\n    fr = open(fileName)\n    stringArr = [line.strip().split(delim) for line in fr.readlines()]\n    datArr = [map(float, line) for line in stringArr]\n    return mat(datArr)\n\n\ndef pca(dataMat, topNfeat=9999999):\n    \"\"\"pca\n\n    Args:\n        dataMat   \u539f\u6570\u636e\u96c6\u77e9\u9635\n        topNfeat  \u5e94\u7528\u7684N\u4e2a\u7279\u5f81\n    Returns:\n        lowDDataMat  \u964d\u7ef4\u540e\u6570\u636e\u96c6\n        reconMat     \u65b0\u7684\u6570\u636e\u96c6\u7a7a\u95f4\n    \"\"\"\n\n    # \u8ba1\u7b97\u6bcf\u4e00\u5217\u7684\u5747\u503c\n    meanVals = mean(dataMat, axis=0)\n    # print 'meanVals', meanVals\n\n    # \u6bcf\u4e2a\u5411\u91cf\u540c\u65f6\u90fd\u51cf\u53bb \u5747\u503c\n    meanRemoved = dataMat - meanVals\n    # print 'meanRemoved=', meanRemoved\n\n    # cov\u534f\u65b9\u5dee=[(x1-x\u5747\u503c)*(y1-y\u5747\u503c)+(x2-x\u5747\u503c)*(y2-y\u5747\u503c)+...+(xn-x\u5747\u503c)*(yn-y\u5747\u503c)+]/(n-1)\n    '''\n    \u65b9\u5dee: \uff08\u4e00\u7ef4\uff09\u5ea6\u91cf\u4e24\u4e2a\u968f\u673a\u53d8\u91cf\u5173\u7cfb\u7684\u7edf\u8ba1\u91cf\n    \u534f\u65b9\u5dee:  \uff08\u4e8c\u7ef4\uff09\u5ea6\u91cf\u5404\u4e2a\u7ef4\u5ea6\u504f\u79bb\u5176\u5747\u503c\u7684\u7a0b\u5ea6\n    \u534f\u65b9\u5dee\u77e9\u9635: \uff08\u591a\u7ef4\uff09\u5ea6\u91cf\u5404\u4e2a\u7ef4\u5ea6\u504f\u79bb\u5176\u5747\u503c\u7684\u7a0b\u5ea6\n\n    \u5f53\u00a0cov(X, Y)>0\u65f6\uff0c\u8868\u660eX\u4e0eY\u6b63\u76f8\u5173\uff1b(X\u8d8a\u5927\uff0cY\u4e5f\u8d8a\u5927\uff1bX\u8d8a\u5c0fY\uff0c\u4e5f\u8d8a\u5c0f\u3002\u8fd9\u79cd\u60c5\u51b5\uff0c\u6211\u4eec\u79f0\u4e3a\u201c\u6b63\u76f8\u5173\u201d\u3002)\n    \u5f53\u00a0cov(X, Y)<0\u65f6\uff0c\u8868\u660eX\u4e0eY\u8d1f\u76f8\u5173\uff1b\n    \u5f53\u00a0cov(X, Y)=0\u65f6\uff0c\u8868\u660eX\u4e0eY\u4e0d\u76f8\u5173\u3002\n    '''\n    covMat = cov(meanRemoved, rowvar=0)\n\n    # eigVals\u4e3a\u7279\u5f81\u503c\uff0c eigVects\u4e3a\u7279\u5f81\u5411\u91cf\n    eigVals, eigVects = linalg.eig(mat(covMat))\n    # print 'eigVals=', eigVals\n    # print 'eigVects=', eigVects\n    # \u5bf9\u7279\u5f81\u503c\uff0c\u8fdb\u884c\u4ece\u5c0f\u5230\u5927\u7684\u6392\u5e8f\uff0c\u8fd4\u56de\u4ece\u5c0f\u5230\u5927\u7684index\u5e8f\u53f7\n    # \u7279\u5f81\u503c\u7684\u9006\u5e8f\u5c31\u53ef\u4ee5\u5f97\u5230topNfeat\u4e2a\u6700\u5927\u7684\u7279\u5f81\u5411\u91cf\n    '''\n    >>> x = np.array([3, 1, 2])\n    >>> np.argsort(x)\n    array([1, 2, 0])  # index,1 = 1; index,2 = 2; index,0 = 3\n    >>> y = np.argsort(x)\n    >>> y[::-1]\n    array([0, 2, 1])\n    >>> y[:-3:-1]\n    array([0, 2])  # \u53d6\u51fa -1, -2\n    >>> y[:-6:-1]\n    array([0, 2, 1])\n    '''\n    eigValInd = argsort(eigVals)\n    # print 'eigValInd1=', eigValInd\n\n    # -1\u8868\u793a\u5012\u5e8f\uff0c\u8fd4\u56detopN\u7684\u7279\u5f81\u503c[-1 \u5230 -(topNfeat+1) \u4f46\u662f\u4e0d\u5305\u62ec-(topNfeat+1)\u672c\u8eab\u7684\u5012\u53d9]\n    eigValInd = eigValInd[:-(topNfeat+1):-1]\n    # print 'eigValInd2=', eigValInd\n    # \u91cd\u7ec4 eigVects \u6700\u5927\u5230\u6700\u5c0f\n    redEigVects = eigVects[:, eigValInd]\n    # print 'redEigVects=', redEigVects.T\n    # \u5c06\u6570\u636e\u8f6c\u6362\u5230\u65b0\u7a7a\u95f4\n    # print \"---\", shape(meanRemoved), shape(redEigVects)\n    lowDDataMat = meanRemoved * redEigVects\n    reconMat = (lowDDataMat * redEigVects.T) + meanVals\n    # print 'lowDDataMat=', lowDDataMat\n    # print 'reconMat=', reconMat\n    return lowDDataMat, reconMat\n\n\ndef replaceNanWithMean():\n    datMat = loadDataSet('data/13.PCA/secom.data', ' ')\n    numFeat = shape(datMat)[1]\n    for i in range(numFeat):\n        # \u5bf9value\u4e0d\u4e3aNaN\u7684\u6c42\u5747\u503c\n        # .A \u8fd4\u56de\u77e9\u9635\u57fa\u4e8e\u7684\u6570\u7ec4\n        meanVal = mean(datMat[nonzero(~isnan(datMat[:, i].A))[0], i])\n        # \u5c06value\u4e3aNaN\u7684\u503c\u8d4b\u503c\u4e3a\u5747\u503c\n        datMat[nonzero(isnan(datMat[:, i].A))[0],i] = meanVal\n    return datMat\n\n\ndef show_picture(dataMat, reconMat):\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    ax.scatter(dataMat[:, 0].flatten().A[0], dataMat[:, 1].flatten().A[0], marker='^', s=90)\n    ax.scatter(reconMat[:, 0].flatten().A[0], reconMat[:, 1].flatten().A[0], marker='o', s=50, c='red')\n    plt.show()\n\n\ndef analyse_data(dataMat):\n    meanVals = mean(dataMat, axis=0)\n    meanRemoved = dataMat-meanVals\n    covMat = cov(meanRemoved, rowvar=0)\n    eigvals, eigVects = linalg.eig(mat(covMat))\n    eigValInd = argsort(eigvals)\n\n    topNfeat = 20\n    eigValInd = eigValInd[:-(topNfeat+1):-1]\n    cov_all_score = float(sum(eigvals))\n    sum_cov_score = 0\n    for i in range(0, len(eigValInd)):\n        line_cov_score = float(eigvals[eigValInd[i]])\n        sum_cov_score += line_cov_score\n        '''\n        \u6211\u4eec\u53d1\u73b0\u5176\u4e2d\u6709\u8d85\u8fc720%\u7684\u7279\u5f81\u503c\u90fd\u662f0\u3002\n        \u8fd9\u5c31\u610f\u5473\u7740\u8fd9\u4e9b\u7279\u5f81\u90fd\u662f\u5176\u4ed6\u7279\u5f81\u7684\u526f\u672c\uff0c\u4e5f\u5c31\u662f\u8bf4\uff0c\u5b83\u4eec\u53ef\u4ee5\u901a\u8fc7\u5176\u4ed6\u7279\u5f81\u6765\u8868\u793a\uff0c\u800c\u672c\u8eab\u5e76\u6ca1\u6709\u63d0\u4f9b\u989d\u5916\u7684\u4fe1\u606f\u3002\n\n        \u6700\u524d\u976215\u4e2a\u503c\u7684\u6570\u91cf\u7ea7\u5927\u4e8e10^5\uff0c\u5b9e\u9645\u4e0a\u90a3\u4ee5\u540e\u7684\u503c\u90fd\u53d8\u5f97\u975e\u5e38\u5c0f\u3002\n        \u8fd9\u5c31\u76f8\u5f53\u4e8e\u544a\u8bc9\u6211\u4eec\u53ea\u6709\u90e8\u5206\u91cd\u8981\u7279\u5f81\uff0c\u91cd\u8981\u7279\u5f81\u7684\u6570\u76ee\u4e5f\u5f88\u5feb\u5c31\u4f1a\u4e0b\u964d\u3002\n\n        \u6700\u540e\uff0c\u6211\u4eec\u53ef\u80fd\u4f1a\u6ce8\u610f\u5230\u6709\u4e00\u4e9b\u5c0f\u7684\u8d1f\u503c\uff0c\u4ed6\u4eec\u4e3b\u8981\u6e90\u81ea\u6570\u503c\u8bef\u5dee\u5e94\u8be5\u56db\u820d\u4e94\u5165\u62100.\n        '''\n        print('\u4e3b\u6210\u5206: %s, \u65b9\u5dee\u5360\u6bd4: %s%%, \u7d2f\u79ef\u65b9\u5dee\u5360\u6bd4: %s%%' % (format(i+1, '2.0f'), format(line_cov_score/cov_all_score*100, '4.2f'), format(sum_cov_score/cov_all_score*100, '4.1f')))\n\n\nif __name__ == \"__main__\":\n    # # \u52a0\u8f7d\u6570\u636e\uff0c\u5e76\u8f6c\u5316\u6570\u636e\u7c7b\u578b\u4e3afloat\n    # dataMat = loadDataSet('data/13.PCA/testSet.txt')\n    # # \u53ea\u9700\u89811\u4e2a\u7279\u5f81\u5411\u91cf\n    # lowDmat, reconMat = pca(dataMat, 1)\n    # # \u53ea\u9700\u89812\u4e2a\u7279\u5f81\u5411\u91cf\uff0c\u548c\u539f\u59cb\u6570\u636e\u4e00\u81f4\uff0c\u6ca1\u4efb\u4f55\u53d8\u5316\n    # # lowDmat, reconMat = pca(dataMat, 2)\n    # # print shape(lowDmat)\n    # show_picture(dataMat, reconMat)\n\n    # \u5229\u7528PCA\u5bf9\u534a\u5bfc\u4f53\u5236\u9020\u6570\u636e\u964d\u7ef4\n    dataMat = replaceNanWithMean()\n    print(shape(dataMat))\n    # \u5206\u6790\u6570\u636e\n    analyse_data(dataMat)\n    # lowDmat, reconMat = pca(dataMat, 20)\n    # print shape(lowDmat)\n    # show_picture(dataMat, reconMat)\n", "src/py2.x/ml/4.NaiveBayes/bayes.py": "#!/usr/bin/env python\n# -*- coding:utf-8 -*-\n'''\nCreated on Oct 19, 2010\nUpdate  on 2017-05-18\nAuthor: Peter Harrington/\u7f8a\u4e09/\u5c0f\u7476\nGitHub: https://github.com/apachecn/AiLearning\n'''\nfrom __future__ import print_function\nfrom numpy import *\n\"\"\"\np(xy)=p(x|y)p(y)=p(y|x)p(x)\np(x|y)=p(y|x)p(x)/p(y)\n\"\"\"\n\n\n# \u9879\u76ee\u6848\u4f8b1: \u5c4f\u853d\u793e\u533a\u7559\u8a00\u677f\u7684\u4fae\u8fb1\u6027\u8a00\u8bba\n\ndef loadDataSet():\n    \"\"\"\n    \u521b\u5efa\u6570\u636e\u96c6\n    :return: \u5355\u8bcd\u5217\u8868postingList, \u6240\u5c5e\u7c7b\u522bclassVec\n    \"\"\"\n    postingList = [['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'], #[0,0,1,1,1......]\n                   ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n                   ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n                   ['stop', 'posting', 'stupid', 'worthless', 'garbage'],\n                   ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],\n                   ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]\n    classVec = [0, 1, 0, 1, 0, 1]  # 1 is abusive, 0 not\n    return postingList, classVec\n\n\ndef createVocabList(dataSet):\n    \"\"\"\n    \u83b7\u53d6\u6240\u6709\u5355\u8bcd\u7684\u96c6\u5408\n    :param dataSet: \u6570\u636e\u96c6\n    :return: \u6240\u6709\u5355\u8bcd\u7684\u96c6\u5408(\u5373\u4e0d\u542b\u91cd\u590d\u5143\u7d20\u7684\u5355\u8bcd\u5217\u8868)\n    \"\"\"\n    vocabSet = set([])  # create empty set\n    for document in dataSet:\n        # \u64cd\u4f5c\u7b26 | \u7528\u4e8e\u6c42\u4e24\u4e2a\u96c6\u5408\u7684\u5e76\u96c6\n        vocabSet = vocabSet | set(document)  # union of the two sets\n    return list(vocabSet)\n\n\ndef setOfWords2Vec(vocabList, inputSet):\n    \"\"\"\n    \u904d\u5386\u67e5\u770b\u8be5\u5355\u8bcd\u662f\u5426\u51fa\u73b0\uff0c\u51fa\u73b0\u8be5\u5355\u8bcd\u5219\u5c06\u8be5\u5355\u8bcd\u7f6e1\n    :param vocabList: \u6240\u6709\u5355\u8bcd\u96c6\u5408\u5217\u8868\n    :param inputSet: \u8f93\u5165\u6570\u636e\u96c6\n    :return: \u5339\u914d\u5217\u8868[0,1,0,1...]\uff0c\u5176\u4e2d 1\u4e0e0 \u8868\u793a\u8bcd\u6c47\u8868\u4e2d\u7684\u5355\u8bcd\u662f\u5426\u51fa\u73b0\u5728\u8f93\u5165\u7684\u6570\u636e\u96c6\u4e2d\n    \"\"\"\n    # \u521b\u5efa\u4e00\u4e2a\u548c\u8bcd\u6c47\u8868\u7b49\u957f\u7684\u5411\u91cf\uff0c\u5e76\u5c06\u5176\u5143\u7d20\u90fd\u8bbe\u7f6e\u4e3a0\n    returnVec = [0] * len(vocabList)# [0,0......]\n    # \u904d\u5386\u6587\u6863\u4e2d\u7684\u6240\u6709\u5355\u8bcd\uff0c\u5982\u679c\u51fa\u73b0\u4e86\u8bcd\u6c47\u8868\u4e2d\u7684\u5355\u8bcd\uff0c\u5219\u5c06\u8f93\u51fa\u7684\u6587\u6863\u5411\u91cf\u4e2d\u7684\u5bf9\u5e94\u503c\u8bbe\u4e3a1\n    for word in inputSet:\n        if word in vocabList:\n            returnVec[vocabList.index(word)] = 1\n        else:\n            print(\"the word: %s is not in my Vocabulary!\" % word)\n    return returnVec\n\n\ndef _trainNB0(trainMatrix, trainCategory):\n    \"\"\"\n    \u8bad\u7ec3\u6570\u636e\u539f\u7248\n    :param trainMatrix: \u6587\u4ef6\u5355\u8bcd\u77e9\u9635 [[1,0,1,1,1....],[],[]...]\n    :param trainCategory: \u6587\u4ef6\u5bf9\u5e94\u7684\u7c7b\u522b[0,1,1,0....]\uff0c\u5217\u8868\u957f\u5ea6\u7b49\u4e8e\u5355\u8bcd\u77e9\u9635\u6570\uff0c\u5176\u4e2d\u76841\u4ee3\u8868\u5bf9\u5e94\u7684\u6587\u4ef6\u662f\u4fae\u8fb1\u6027\u6587\u4ef6\uff0c0\u4ee3\u8868\u4e0d\u662f\u4fae\u8fb1\u6027\u77e9\u9635\n    :return:\n    \"\"\"\n    # \u6587\u4ef6\u6570\n    numTrainDocs = len(trainMatrix)\n    # \u5355\u8bcd\u6570\n    numWords = len(trainMatrix[0])\n    # \u4fae\u8fb1\u6027\u6587\u4ef6\u7684\u51fa\u73b0\u6982\u7387\uff0c\u5373trainCategory\u4e2d\u6240\u6709\u76841\u7684\u4e2a\u6570\uff0c\n    # \u4ee3\u8868\u7684\u5c31\u662f\u591a\u5c11\u4e2a\u4fae\u8fb1\u6027\u6587\u4ef6\uff0c\u4e0e\u6587\u4ef6\u7684\u603b\u6570\u76f8\u9664\u5c31\u5f97\u5230\u4e86\u4fae\u8fb1\u6027\u6587\u4ef6\u7684\u51fa\u73b0\u6982\u7387\n    pAbusive = sum(trainCategory) / float(numTrainDocs)\n    # \u6784\u9020\u5355\u8bcd\u51fa\u73b0\u6b21\u6570\u5217\u8868\n    p0Num = zeros(numWords) # [0,0,0,.....]\n    p1Num = zeros(numWords) # [0,0,0,.....]\n\n    # \u6574\u4e2a\u6570\u636e\u96c6\u5355\u8bcd\u51fa\u73b0\u603b\u6570\n    p0Denom = 0.0\n    p1Denom = 0.0\n    for i in range(numTrainDocs):\n        # \u904d\u5386\u6240\u6709\u7684\u6587\u4ef6\uff0c\u5982\u679c\u662f\u4fae\u8fb1\u6027\u6587\u4ef6\uff0c\u5c31\u8ba1\u7b97\u6b64\u4fae\u8fb1\u6027\u6587\u4ef6\u4e2d\u51fa\u73b0\u7684\u4fae\u8fb1\u6027\u5355\u8bcd\u7684\u4e2a\u6570\n        if trainCategory[i] == 1:\n            p1Num += trainMatrix[i] #[0,1,1,....]->[0,1,1,...]\n            p1Denom += sum(trainMatrix[i])\n        else:\n            # \u5982\u679c\u4e0d\u662f\u4fae\u8fb1\u6027\u6587\u4ef6\uff0c\u5219\u8ba1\u7b97\u975e\u4fae\u8fb1\u6027\u6587\u4ef6\u4e2d\u51fa\u73b0\u7684\u4fae\u8fb1\u6027\u5355\u8bcd\u7684\u4e2a\u6570\n            p0Num += trainMatrix[i]\n            p0Denom += sum(trainMatrix[i])\n    # \u7c7b\u522b1\uff0c\u5373\u4fae\u8fb1\u6027\u6587\u6863\u7684[P(F1|C1),P(F2|C1),P(F3|C1),P(F4|C1),P(F5|C1)....]\u5217\u8868\n    # \u5373 \u57281\u7c7b\u522b\u4e0b\uff0c\u6bcf\u4e2a\u5355\u8bcd\u51fa\u73b0\u6b21\u6570\u7684\u5360\u6bd4\n    p1Vect = p1Num / p1Denom# [1,2,3,5]/90->[1/90,...]\n    # \u7c7b\u522b0\uff0c\u5373\u6b63\u5e38\u6587\u6863\u7684[P(F1|C0),P(F2|C0),P(F3|C0),P(F4|C0),P(F5|C0)....]\u5217\u8868\n    # \u5373 \u57280\u7c7b\u522b\u4e0b\uff0c\u6bcf\u4e2a\u5355\u8bcd\u51fa\u73b0\u6b21\u6570\u7684\u5360\u6bd4\n    p0Vect = p0Num / p0Denom\n    return p0Vect, p1Vect, pAbusive\n\n\ndef trainNB0(trainMatrix, trainCategory):\n    \"\"\"\n    \u8bad\u7ec3\u6570\u636e\u4f18\u5316\u7248\u672c\n    :param trainMatrix: \u6587\u4ef6\u5355\u8bcd\u77e9\u9635\n    :param trainCategory: \u6587\u4ef6\u5bf9\u5e94\u7684\u7c7b\u522b\n    :return:\n    \"\"\"\n    # \u603b\u6587\u4ef6\u6570\n    numTrainDocs = len(trainMatrix)\n    # \u603b\u5355\u8bcd\u6570\n    numWords = len(trainMatrix[0])\n    # \u4fae\u8fb1\u6027\u6587\u4ef6\u7684\u51fa\u73b0\u6982\u7387\n    pAbusive = sum(trainCategory) / float(numTrainDocs)\n    # \u6784\u9020\u5355\u8bcd\u51fa\u73b0\u6b21\u6570\u5217\u8868\n    # p0Num \u6b63\u5e38\u7684\u7edf\u8ba1\n    # p1Num \u4fae\u8fb1\u7684\u7edf\u8ba1 \n    # \u907f\u514d\u5355\u8bcd\u5217\u8868\u4e2d\u7684\u4efb\u4f55\u4e00\u4e2a\u5355\u8bcd\u4e3a0\uff0c\u800c\u5bfc\u81f4\u6700\u540e\u7684\u4e58\u79ef\u4e3a0\uff0c\u6240\u4ee5\u5c06\u6bcf\u4e2a\u5355\u8bcd\u7684\u51fa\u73b0\u6b21\u6570\u521d\u59cb\u5316\u4e3a 1\n    p0Num = ones(numWords)#[0,0......]->[1,1,1,1,1.....]\n    p1Num = ones(numWords)\n\n    # \u6574\u4e2a\u6570\u636e\u96c6\u5355\u8bcd\u51fa\u73b0\u603b\u6570\uff0c2.0\u6839\u636e\u6837\u672c/\u5b9e\u9645\u8c03\u67e5\u7ed3\u679c\u8c03\u6574\u5206\u6bcd\u7684\u503c\uff082\u4e3b\u8981\u662f\u907f\u514d\u5206\u6bcd\u4e3a0\uff0c\u5f53\u7136\u503c\u53ef\u4ee5\u8c03\u6574\uff09\n    # p0Denom \u6b63\u5e38\u7684\u7edf\u8ba1\n    # p1Denom \u4fae\u8fb1\u7684\u7edf\u8ba1\n    p0Denom = 2.0\n    p1Denom = 2.0\n    for i in range(numTrainDocs):\n        if trainCategory[i] == 1:\n            # \u7d2f\u52a0\u8fb1\u9a82\u8bcd\u7684\u9891\u6b21\n            p1Num += trainMatrix[i]\n            # \u5bf9\u6bcf\u7bc7\u6587\u7ae0\u7684\u8fb1\u9a82\u7684\u9891\u6b21 \u8fdb\u884c\u7edf\u8ba1\u6c47\u603b\n            p1Denom += sum(trainMatrix[i])\n        else:\n            p0Num += trainMatrix[i]\n            p0Denom += sum(trainMatrix[i])\n    # \u7c7b\u522b1\uff0c\u5373\u4fae\u8fb1\u6027\u6587\u6863\u7684[log(P(F1|C1)),log(P(F2|C1)),log(P(F3|C1)),log(P(F4|C1)),log(P(F5|C1))....]\u5217\u8868\n    p1Vect = log(p1Num / p1Denom)\n    # \u7c7b\u522b0\uff0c\u5373\u6b63\u5e38\u6587\u6863\u7684[log(P(F1|C0)),log(P(F2|C0)),log(P(F3|C0)),log(P(F4|C0)),log(P(F5|C0))....]\u5217\u8868\n    p0Vect = log(p0Num / p0Denom)\n    return p0Vect, p1Vect, pAbusive\n\n\ndef classifyNB(vec2Classify, p0Vec, p1Vec, pClass1):\n    \"\"\"\n    \u4f7f\u7528\u7b97\u6cd5: \n        # \u5c06\u4e58\u6cd5\u8f6c\u6362\u4e3a\u52a0\u6cd5\n        \u4e58\u6cd5: P(C|F1F2...Fn) = P(F1F2...Fn|C)P(C)/P(F1F2...Fn)\n        \u52a0\u6cd5: P(F1|C)*P(F2|C)....P(Fn|C)P(C) -> log(P(F1|C))+log(P(F2|C))+....+log(P(Fn|C))+log(P(C))\n    :param vec2Classify: \u5f85\u6d4b\u6570\u636e[0,1,1,1,1...]\uff0c\u5373\u8981\u5206\u7c7b\u7684\u5411\u91cf\n    :param p0Vec: \u7c7b\u522b0\uff0c\u5373\u6b63\u5e38\u6587\u6863\u7684[log(P(F1|C0)),log(P(F2|C0)),log(P(F3|C0)),log(P(F4|C0)),log(P(F5|C0))....]\u5217\u8868\n    :param p1Vec: \u7c7b\u522b1\uff0c\u5373\u4fae\u8fb1\u6027\u6587\u6863\u7684[log(P(F1|C1)),log(P(F2|C1)),log(P(F3|C1)),log(P(F4|C1)),log(P(F5|C1))....]\u5217\u8868\n    :param pClass1: \u7c7b\u522b1\uff0c\u4fae\u8fb1\u6027\u6587\u4ef6\u7684\u51fa\u73b0\u6982\u7387\n    :return: \u7c7b\u522b1 or 0\n    \"\"\"\n    # \u8ba1\u7b97\u516c\u5f0f  log(P(F1|C))+log(P(F2|C))+....+log(P(Fn|C))+log(P(C))\n    # \u4f7f\u7528 NumPy \u6570\u7ec4\u6765\u8ba1\u7b97\u4e24\u4e2a\u5411\u91cf\u76f8\u4e58\u7684\u7ed3\u679c\uff0c\u8fd9\u91cc\u7684\u76f8\u4e58\u662f\u6307\u5bf9\u5e94\u5143\u7d20\u76f8\u4e58\uff0c\u5373\u5148\u5c06\u4e24\u4e2a\u5411\u91cf\u4e2d\u7684\u7b2c\u4e00\u4e2a\u5143\u7d20\u76f8\u4e58\uff0c\u7136\u540e\u5c06\u7b2c2\u4e2a\u5143\u7d20\u76f8\u4e58\uff0c\u4ee5\u6b64\u7c7b\u63a8\u3002\n    # \u6211\u7684\u7406\u89e3\u662f: \u8fd9\u91cc\u7684 vec2Classify * p1Vec \u7684\u610f\u601d\u5c31\u662f\u5c06\u6bcf\u4e2a\u8bcd\u4e0e\u5176\u5bf9\u5e94\u7684\u6982\u7387\u76f8\u5173\u8054\u8d77\u6765\n    # \u53ef\u4ee5\u7406\u89e3\u4e3a 1.\u5355\u8bcd\u5728\u8bcd\u6c47\u8868\u4e2d\u7684\u6761\u4ef6\u4e0b\uff0c\u6587\u4ef6\u662fgood \u7c7b\u522b\u7684\u6982\u7387 \u4e5f\u53ef\u4ee5\u7406\u89e3\u4e3a 2.\u5728\u6574\u4e2a\u7a7a\u95f4\u4e0b\uff0c\u6587\u4ef6\u65e2\u5728\u8bcd\u6c47\u8868\u4e2d\u53c8\u662fgood\u7c7b\u522b\u7684\u6982\u7387\n    p1 = sum(vec2Classify * p1Vec) + log(pClass1)\n    p0 = sum(vec2Classify * p0Vec) + log(1.0 - pClass1)\n    if p1 > p0:\n        return 1\n    else:\n        return 0\n\n\ndef bagOfWords2VecMN(vocabList, inputSet):\n    returnVec = [0] * len(vocabList)\n    for word in inputSet:\n        if word in vocabList:\n            returnVec[vocabList.index(word)] += 1\n    return returnVec\n\n\ndef testingNB():\n    \"\"\"\n    \u6d4b\u8bd5\u6734\u7d20\u8d1d\u53f6\u65af\u7b97\u6cd5\n    \"\"\"\n    # 1. \u52a0\u8f7d\u6570\u636e\u96c6\n    listOPosts, listClasses = loadDataSet()\n    # 2. \u521b\u5efa\u5355\u8bcd\u96c6\u5408\n    myVocabList = createVocabList(listOPosts)\n    # 3. \u8ba1\u7b97\u5355\u8bcd\u662f\u5426\u51fa\u73b0\u5e76\u521b\u5efa\u6570\u636e\u77e9\u9635\n    trainMat = []\n    for postinDoc in listOPosts:\n        # \u8fd4\u56dem*len(myVocabList)\u7684\u77e9\u9635\uff0c \u8bb0\u5f55\u7684\u90fd\u662f0\uff0c1\u4fe1\u606f\n        trainMat.append(setOfWords2Vec(myVocabList, postinDoc))\n    # 4. \u8bad\u7ec3\u6570\u636e\n    p0V, p1V, pAb = trainNB0(array(trainMat), array(listClasses))\n    # 5. \u6d4b\u8bd5\u6570\u636e\n    testEntry = ['love', 'my', 'dalmation']\n    thisDoc = array(setOfWords2Vec(myVocabList, testEntry))\n    print(testEntry, 'classified as: ', classifyNB(thisDoc, p0V, p1V, pAb))\n    testEntry = ['stupid', 'garbage']\n    thisDoc = array(setOfWords2Vec(myVocabList, testEntry))\n    print(testEntry, 'classified as: ', classifyNB(thisDoc, p0V, p1V, pAb))\n\n\n# ------------------------------------------------------------------------------------------\n# \u9879\u76ee\u6848\u4f8b2: \u4f7f\u7528\u6734\u7d20\u8d1d\u53f6\u65af\u8fc7\u6ee4\u5783\u573e\u90ae\u4ef6\n\n# \u5207\u5206\u6587\u672c\ndef textParse(bigString):\n    '''\n    Desc:\n        \u63a5\u6536\u4e00\u4e2a\u5927\u5b57\u7b26\u4e32\u5e76\u5c06\u5176\u89e3\u6790\u4e3a\u5b57\u7b26\u4e32\u5217\u8868\n    Args:\n        bigString -- \u5927\u5b57\u7b26\u4e32\n    Returns:\n        \u53bb\u6389\u5c11\u4e8e 2 \u4e2a\u5b57\u7b26\u7684\u5b57\u7b26\u4e32\uff0c\u5e76\u5c06\u6240\u6709\u5b57\u7b26\u4e32\u8f6c\u6362\u4e3a\u5c0f\u5199\uff0c\u8fd4\u56de\u5b57\u7b26\u4e32\u5217\u8868\n    '''\n    import re\n    # \u4f7f\u7528\u6b63\u5219\u8868\u8fbe\u5f0f\u6765\u5207\u5206\u53e5\u5b50\uff0c\u5176\u4e2d\u5206\u9694\u7b26\u662f\u9664\u5355\u8bcd\u3001\u6570\u5b57\u5916\u7684\u4efb\u610f\u5b57\u7b26\u4e32\n    listOfTokens = re.split(r'\\W*', bigString)\n    return [tok.lower() for tok in listOfTokens if len(tok) > 2]\n\n\ndef spamTest():\n    '''\n    Desc:\n        \u5bf9\u8d1d\u53f6\u65af\u5783\u573e\u90ae\u4ef6\u5206\u7c7b\u5668\u8fdb\u884c\u81ea\u52a8\u5316\u5904\u7406\u3002\n    Args:\n        none\n    Returns:\n        \u5bf9\u6d4b\u8bd5\u96c6\u4e2d\u7684\u6bcf\u5c01\u90ae\u4ef6\u8fdb\u884c\u5206\u7c7b\uff0c\u82e5\u90ae\u4ef6\u5206\u7c7b\u9519\u8bef\uff0c\u5219\u9519\u8bef\u6570\u52a0 1\uff0c\u6700\u540e\u8fd4\u56de\u603b\u7684\u9519\u8bef\u767e\u5206\u6bd4\u3002\n    '''\n    docList = []\n    classList = []\n    fullText = []\n    for i in range(1, 26):\n        # \u5207\u5206\uff0c\u89e3\u6790\u6570\u636e\uff0c\u5e76\u5f52\u7c7b\u4e3a 1 \u7c7b\u522b\n        wordList = textParse(open('data/4.NaiveBayes/email/spam/%d.txt' % i).read())\n        docList.append(wordList)\n        classList.append(1)\n        # \u5207\u5206\uff0c\u89e3\u6790\u6570\u636e\uff0c\u5e76\u5f52\u7c7b\u4e3a 0 \u7c7b\u522b\n        wordList = textParse(open('data/4.NaiveBayes/email/ham/%d.txt' % i).read())\n        docList.append(wordList)\n        fullText.extend(wordList)\n        classList.append(0)\n    # \u521b\u5efa\u8bcd\u6c47\u8868    \n    vocabList = createVocabList(docList)\n    trainingSet = range(50)\n    testSet = []\n    # \u968f\u673a\u53d6 10 \u4e2a\u90ae\u4ef6\u7528\u6765\u6d4b\u8bd5\n    for i in range(10):\n        # random.uniform(x, y) \u968f\u673a\u751f\u6210\u4e00\u4e2a\u8303\u56f4\u4e3a x - y \u7684\u5b9e\u6570\n        randIndex = int(random.uniform(0, len(trainingSet)))\n        testSet.append(trainingSet[randIndex])\n        del(trainingSet[randIndex])\n    trainMat = []\n    trainClasses = []\n    for docIndex in trainingSet:\n        trainMat.append(setOfWords2Vec(vocabList, docList[docIndex]))\n        trainClasses.append(classList[docIndex])\n    p0V, p1V, pSpam = trainNB0(array(trainMat), array(trainClasses))\n    errorCount = 0\n    for docIndex in testSet:\n        wordVector = setOfWords2Vec(vocabList, docList[docIndex])\n        if classifyNB(array(wordVector), p0V, p1V, pSpam) != classList[docIndex]:\n            errorCount += 1\n    print('the errorCount is: ', errorCount)\n    print('the testSet length is :', len(testSet))\n    print('the error rate is :', float(errorCount)/len(testSet))\n\n\ndef testParseTest():\n    print(textParse(open('data/4.NaiveBayes/email/ham/1.txt').read()))\n\n\n# -----------------------------------------------------------------------------------\n# \u9879\u76ee\u6848\u4f8b3: \u4f7f\u7528\u6734\u7d20\u8d1d\u53f6\u65af\u4ece\u4e2a\u4eba\u5e7f\u544a\u4e2d\u83b7\u53d6\u533a\u57df\u503e\u5411\n\n# \u5c06\u6587\u672c\u6587\u4ef6\u89e3\u6790\u6210 \u8bcd\u6761\u5411\u91cf\ndef setOfWords2VecMN(vocabList,inputSet):\n    returnVec=[0]*len(vocabList)  # \u521b\u5efa\u4e00\u4e2a\u5176\u4e2d\u6240\u542b\u5143\u7d20\u90fd\u4e3a0\u7684\u5411\u91cf\n    for word in inputSet:\n        if word in vocabList:\n                returnVec[vocabList.index(word)]+=1\n    return returnVec\n\n\n#\u6587\u4ef6\u89e3\u6790\ndef textParse(bigString):\n    import re\n    listOfTokens=re.split(r'\\W*', bigString)\n    return [tok.lower() for tok in listOfTokens if len(tok)>2]\n\n\n#RSS\u6e90\u5206\u7c7b\u5668\u53ca\u9ad8\u9891\u8bcd\u53bb\u9664\u51fd\u6570\ndef calcMostFreq(vocabList,fullText):\n    import operator\n    freqDict={}\n    for token in vocabList:  #\u904d\u5386\u8bcd\u6c47\u8868\u4e2d\u7684\u6bcf\u4e2a\u8bcd\n        freqDict[token]=fullText.count(token)  #\u7edf\u8ba1\u6bcf\u4e2a\u8bcd\u5728\u6587\u672c\u4e2d\u51fa\u73b0\u7684\u6b21\u6570\n    sortedFreq=sorted(freqDict.iteritems(),key=operator.itemgetter(1),reverse=True)  #\u6839\u636e\u6bcf\u4e2a\u8bcd\u51fa\u73b0\u7684\u6b21\u6570\u4ece\u9ad8\u5230\u5e95\u5bf9\u5b57\u5178\u8fdb\u884c\u6392\u5e8f\n    return sortedFreq[:30]   #\u8fd4\u56de\u51fa\u73b0\u6b21\u6570\u6700\u9ad8\u768430\u4e2a\u5355\u8bcd\ndef localWords(feed1,feed0):\n    import feedparser\n    docList=[];classList=[];fullText=[]\n    minLen=min(len(feed1['entries']),len(feed0['entries']))\n    for i in range(minLen):\n        wordList=textParse(feed1['entries'][i]['summary'])   #\u6bcf\u6b21\u8bbf\u95ee\u4e00\u6761RSS\u6e90\n        docList.append(wordList)\n        fullText.extend(wordList)\n        classList.append(1)\n        wordList=textParse(feed0['entries'][i]['summary'])\n        docList.append(wordList)\n        fullText.extend(wordList)\n        classList.append(0)\n    vocabList=createVocabList(docList)\n    top30Words=calcMostFreq(vocabList,fullText)\n    for pairW in top30Words:\n        if pairW[0] in vocabList:vocabList.remove(pairW[0])    #\u53bb\u6389\u51fa\u73b0\u6b21\u6570\u6700\u9ad8\u7684\u90a3\u4e9b\u8bcd\n    trainingSet=range(2*minLen);testSet=[]\n    for i in range(20):\n        randIndex=int(random.uniform(0,len(trainingSet)))\n        testSet.append(trainingSet[randIndex])\n        del(trainingSet[randIndex])\n    trainMat=[];trainClasses=[]\n    for docIndex in trainingSet:\n        trainMat.append(bagOfWords2VecMN(vocabList,docList[docIndex]))\n        trainClasses.append(classList[docIndex])\n    p0V,p1V,pSpam=trainNB0(array(trainMat),array(trainClasses))\n    errorCount=0\n    for docIndex in testSet:\n        wordVector=bagOfWords2VecMN(vocabList,docList[docIndex])\n        if classifyNB(array(wordVector),p0V,p1V,pSpam)!=classList[docIndex]:\n            errorCount+=1\n    print('the error rate is:',float(errorCount)/len(testSet))\n    return vocabList,p0V,p1V\n\n\n# \u6700\u5177\u8868\u5f81\u6027\u7684\u8bcd\u6c47\u663e\u793a\u51fd\u6570\ndef getTopWords(ny,sf):\n    import operator\n    vocabList,p0V,p1V=localWords(ny,sf)\n    topNY=[];topSF=[]\n    for i in range(len(p0V)):\n        if p0V[i]>-6.0:topSF.append((vocabList[i],p0V[i]))\n        if p1V[i]>-6.0:topNY.append((vocabList[i],p1V[i]))\n    sortedSF=sorted(topSF,key=lambda pair:pair[1],reverse=True)\n    print(\"SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**SF**\")\n    for item in sortedSF:\n        print(item[0])\n    sortedNY=sorted(topNY,key=lambda pair:pair[1],reverse=True)\n    print(\"NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**NY**\")\n    for item in sortedNY:\n        print(item[0])\n\n\nif __name__ == \"__main__\":\n    # testingNB()\n    spamTest()\n    # laTest()\n", "src/py2.x/ml/4.NaiveBayes/sklearn-nb-demo.py": "#!/usr/bin/python\n# coding:utf8\n\n\"\"\"\nCreated on 2017-06-28\nUpdated on 2017-06-28\nNaiveBayes: \u6734\u7d20\u8d1d\u53f6\u65af\nAuthor: \u5c0f\u7476\nGitHub: https://github.com/apachecn/AiLearning\n\"\"\"\nfrom __future__ import print_function\n\n\n# GaussianNB_\u9ad8\u65af\u6734\u7d20\u8d1d\u53f6\u65af\nimport numpy as np\nX = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\nY = np.array([1, 1, 1, 2, 2, 2])\nfrom sklearn.naive_bayes import GaussianNB\nclf = GaussianNB()\nclf.fit(X, Y)\nprint(clf.predict([[-0.8, -1]]))\nclf_pf = GaussianNB()\nclf_pf.partial_fit(X, Y, np.unique(Y))\nprint(clf_pf.predict([[-0.8, -1]]))\n\n# MultinomialNB_\u591a\u9879\u6734\u7d20\u8d1d\u53f6\u65af\n'''\nimport numpy as np\nX = np.random.randint(5, size=(6, 100))\ny = np.array([1, 2, 3, 4, 5, 6])\nfrom sklearn.naive_bayes import MultinomialNB\nclf = MultinomialNB()\nclf.fit(X, y)\nprint clf.predict(X[2:3])\n'''\n\n# BernoulliNB_\u4f2f\u52aa\u5229\u6734\u7d20\u8d1d\u53f6\u65af\n'''\nimport numpy as np\nX = np.random.randint(2, size=(6, 100))\nY = np.array([1, 2, 3, 4, 4, 5])\nfrom sklearn.naive_bayes import BernoulliNB\nclf = BernoulliNB()\nclf.fit(X, Y)\nprint clf.predict(X[2:3])\n'''\n", "src/py2.x/ml/2.KNN/sklearn-knn-demo.py": "#!/usr/bin/python\n# coding:utf8\n\n\"\"\"\nCreated on 2017-06-28\nUpdated on 2017-06-28\nKNN: k\u8fd1\u90bb\u7b97\u6cd5\nAuthor: \u5c0f\u7476\nGitHub: https://github.com/apachecn/AiLearning\n\"\"\"\nfrom __future__ import print_function\nprint(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom numpy import *\nfrom matplotlib.colors import ListedColormap\nfrom sklearn import neighbors, datasets\n\nn_neighbors = 3\n\n# \u5bfc\u5165\u4e00\u4e9b\u8981\u73a9\u7684\u6570\u636e\n# iris = datasets.load_iris()\n# X = iris.data[:, :2]  # \u6211\u4eec\u53ea\u91c7\u7528\u524d\u4e24\u4e2afeature. \u6211\u4eec\u53ef\u4ee5\u4f7f\u7528\u4e8c\u7ef4\u6570\u636e\u96c6\u907f\u514d\u8fd9\u4e2a\u4e11\u964b\u7684\u5207\u7247\n# y = iris.target\n\n# print 'X=', type(X), X\n# print 'y=', type(y), y\n\nX = array([[-1.0, -1.1], [-1.0, -1.0], [0, 0], [1.0, 1.1], [2.0, 2.0], [2.0, 2.1]])\ny = array([0, 0, 0, 1, 1, 1])\n\n# print 'X=', type(X), X\n# print 'y=', type(y), y\n\nh = .02  # \u7f51\u683c\u4e2d\u7684\u6b65\u957f\n\n# \u521b\u5efa\u5f69\u8272\u7684\u5730\u56fe\n# cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\n# cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n\ncmap_light = ListedColormap(['#FFAAAA', '#AAFFAA'])\ncmap_bold = ListedColormap(['#FF0000', '#00FF00'])\n\nfor weights in ['uniform', 'distance']:\n    # \u6211\u4eec\u521b\u5efa\u4e86\u4e00\u4e2aknn\u5206\u7c7b\u5668\u7684\u5b9e\u4f8b\uff0c\u5e76\u9002\u5408\u6570\u636e\u3002\n    clf = neighbors.KNeighborsClassifier(n_neighbors, weights=weights)\n    clf.fit(X, y)\n\n    # \u7ed8\u5236\u51b3\u7b56\u8fb9\u754c\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u5c06\u4e3a\u6bcf\u4e2a\u5206\u914d\u4e00\u4e2a\u989c\u8272\n    # \u6765\u7ed8\u5236\u7f51\u683c\u4e2d\u7684\u70b9 [x_min, x_max]x[y_min, y_max].\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                         np.arange(y_min, y_max, h))\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n\n    # \u5c06\u7ed3\u679c\u653e\u5165\u4e00\u4e2a\u5f69\u8272\u56fe\u4e2d\n    Z = Z.reshape(xx.shape)\n    plt.figure()\n    plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n\n    # \u7ed8\u5236\u8bad\u7ec3\u70b9\n    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold)\n    plt.xlim(xx.min(), xx.max())\n    plt.ylim(yy.min(), yy.max())\n    plt.title(\"3-Class classification (k = %i, weights = '%s')\"\n              % (n_neighbors, weights))\n\nplt.show()", "src/py2.x/ml/2.KNN/kNN.py": "#!/usr/bin/env python\n# coding: utf-8\n'''\nCreated on Sep 16, 2010\nUpdate  on 2017-05-18\nAuthor: Peter Harrington/\u7f8a\u4e09/\u5c0f\u7476\nGitHub: https://github.com/apachecn/AiLearning\n'''\nfrom __future__ import print_function\nfrom numpy import *\n# \u5bfc\u5165\u79d1\u5b66\u8ba1\u7b97\u5305numpy\u548c\u8fd0\u7b97\u7b26\u6a21\u5757operator\nimport operator\nfrom os import listdir\nfrom collections import Counter\n\n\ndef createDataSet():\n    \"\"\"\n    \u521b\u5efa\u6570\u636e\u96c6\u548c\u6807\u7b7e\n\n     \u8c03\u7528\u65b9\u5f0f\n     import kNN\n     group, labels = kNN.createDataSet()\n    \"\"\"\n    group = array([[1.0, 1.1], [1.0, 1.0], [0, 0], [0, 0.1]])\n    labels = ['A', 'A', 'B', 'B']\n    return group, labels\n\n\ndef classify0(inX, dataSet, labels, k):\n    \"\"\"\n    inx[1,2,3]\n    DS=[[1,2,3],[1,2,0]]\n    inX: \u7528\u4e8e\u5206\u7c7b\u7684\u8f93\u5165\u5411\u91cf\n    dataSet: \u8f93\u5165\u7684\u8bad\u7ec3\u6837\u672c\u96c6\n    labels: \u6807\u7b7e\u5411\u91cf\n    k: \u9009\u62e9\u6700\u8fd1\u90bb\u5c45\u7684\u6570\u76ee\n    \u6ce8\u610f: labels\u5143\u7d20\u6570\u76ee\u548cdataSet\u884c\u6570\u76f8\u540c\uff1b\u7a0b\u5e8f\u4f7f\u7528\u6b27\u5f0f\u8ddd\u79bb\u516c\u5f0f.\n\n    \u9884\u6d4b\u6570\u636e\u6240\u5728\u5206\u7c7b\u53ef\u5728\u8f93\u5165\u4e0b\u5217\u547d\u4ee4\n    kNN.classify0([0,0], group, labels, 3)\n    \"\"\"\n\n    # -----------\u5b9e\u73b0 classify0() \u65b9\u6cd5\u7684\u7b2c\u4e00\u79cd\u65b9\u5f0f----------------------------------------------------------------------------------------------------------------------------\n    # 1. \u8ddd\u79bb\u8ba1\u7b97\n    dataSetSize = dataSet.shape[0]\n    # tile\u751f\u6210\u548c\u8bad\u7ec3\u6837\u672c\u5bf9\u5e94\u7684\u77e9\u9635\uff0c\u5e76\u4e0e\u8bad\u7ec3\u6837\u672c\u6c42\u5dee\n    \"\"\"\n    tile: \u5217-3\u8868\u793a\u590d\u5236\u7684\u884c\u6570\uff0c \u884c-1\uff0f2\u8868\u793a\u5bf9inx\u7684\u91cd\u590d\u7684\u6b21\u6570\n\n    In [8]: tile(inx, (3, 1))\n    Out[8]:\n    array([[1, 2, 3],\n        [1, 2, 3],\n        [1, 2, 3]])\n\n    In [9]: tile(inx, (3, 2))\n    Out[9]:\n    array([[1, 2, 3, 1, 2, 3],\n        [1, 2, 3, 1, 2, 3],\n        [1, 2, 3, 1, 2, 3]])\n    \"\"\"\n    diffMat = tile(inX, (dataSetSize, 1)) - dataSet\n    \"\"\"\n    \u6b27\u6c0f\u8ddd\u79bb:  \u70b9\u5230\u70b9\u4e4b\u95f4\u7684\u8ddd\u79bb\n       \u7b2c\u4e00\u884c:  \u540c\u4e00\u4e2a\u70b9 \u5230 dataSet\u7684\u7b2c\u4e00\u4e2a\u70b9\u7684\u8ddd\u79bb\u3002\n       \u7b2c\u4e8c\u884c:  \u540c\u4e00\u4e2a\u70b9 \u5230 dataSet\u7684\u7b2c\u4e8c\u4e2a\u70b9\u7684\u8ddd\u79bb\u3002\n       ...\n       \u7b2cN\u884c:  \u540c\u4e00\u4e2a\u70b9 \u5230 dataSet\u7684\u7b2cN\u4e2a\u70b9\u7684\u8ddd\u79bb\u3002\n\n    [[1,2,3],[1,2,3]]-[[1,2,3],[1,2,0]]\n    (A1-A2)^2+(B1-B2)^2+(c1-c2)^2\n    \"\"\"\n    # \u53d6\u5e73\u65b9\n    sqDiffMat = diffMat ** 2\n    # \u5c06\u77e9\u9635\u7684\u6bcf\u4e00\u884c\u76f8\u52a0\n    sqDistances = sqDiffMat.sum(axis=1)\n    # \u5f00\u65b9\n    distances = sqDistances ** 0.5\n    # \u6839\u636e\u8ddd\u79bb\u6392\u5e8f\u4ece\u5c0f\u5230\u5927\u7684\u6392\u5e8f\uff0c\u8fd4\u56de\u5bf9\u5e94\u7684\u7d22\u5f15\u4f4d\u7f6e\n    # argsort() \u662f\u5c06x\u4e2d\u7684\u5143\u7d20\u4ece\u5c0f\u5230\u5927\u6392\u5217\uff0c\u63d0\u53d6\u5176\u5bf9\u5e94\u7684index\uff08\u7d22\u5f15\uff09\uff0c\u7136\u540e\u8f93\u51fa\u5230y\u3002\n    # \u4f8b\u5982: y=array([3,0,2,1,4,5]) \u5219\uff0cx[3]=-1\u6700\u5c0f\uff0c\u6240\u4ee5y[0]=3;x[5]=9\u6700\u5927\uff0c\u6240\u4ee5y[5]=5\u3002\n    # print 'distances=', distances\n    sortedDistIndicies = distances.argsort()\n    # print 'distances.argsort()=', sortedDistIndicies\n\n    # 2. \u9009\u62e9\u8ddd\u79bb\u6700\u5c0f\u7684k\u4e2a\u70b9\n    classCount = {}\n    for i in range(k):\n        # \u627e\u5230\u8be5\u6837\u672c\u7684\u7c7b\u578b\n        voteIlabel = labels[sortedDistIndicies[i]]\n        # \u5728\u5b57\u5178\u4e2d\u5c06\u8be5\u7c7b\u578b\u52a0\u4e00\n        # \u5b57\u5178\u7684get\u65b9\u6cd5\n        # \u5982: list.get(k,d) \u5176\u4e2d get\u76f8\u5f53\u4e8e\u4e00\u6761if...else...\u8bed\u53e5,\u53c2\u6570k\u5728\u5b57\u5178\u4e2d\uff0c\u5b57\u5178\u5c06\u8fd4\u56delist[k];\u5982\u679c\u53c2\u6570k\u4e0d\u5728\u5b57\u5178\u4e2d\u5219\u8fd4\u56de\u53c2\u6570d,\u5982\u679cK\u5728\u5b57\u5178\u4e2d\u5219\u8fd4\u56dek\u5bf9\u5e94\u7684value\u503c\n        # l = {5:2,3:4}\n        # print l.get(3,0)\u8fd4\u56de\u7684\u503c\u662f4\uff1b\n        # Print l.get\uff081,0\uff09\u8fd4\u56de\u503c\u662f0\uff1b\n        classCount[voteIlabel] = classCount.get(voteIlabel, 0) + 1\n    # 3. \u6392\u5e8f\u5e76\u8fd4\u56de\u51fa\u73b0\u6700\u591a\u7684\u90a3\u4e2a\u7c7b\u578b\n    # \u5b57\u5178\u7684 items() \u65b9\u6cd5\uff0c\u4ee5\u5217\u8868\u8fd4\u56de\u53ef\u904d\u5386\u7684(\u952e\uff0c\u503c)\u5143\u7ec4\u6570\u7ec4\u3002\n    # \u4f8b\u5982: dict = {'Name': 'Zara', 'Age': 7}   print \"Value : %s\" %  dict.items()   Value : [('Age', 7), ('Name', 'Zara')]\n    # sorted \u4e2d\u7684\u7b2c2\u4e2a\u53c2\u6570 key=operator.itemgetter(1) \u8fd9\u4e2a\u53c2\u6570\u7684\u610f\u601d\u662f\u5148\u6bd4\u8f83\u7b2c\u51e0\u4e2a\u5143\u7d20\n    # \u4f8b\u5982: a=[('b',2),('a',1),('c',0)]  b=sorted(a,key=operator.itemgetter(1)) >>>b=[('c',0),('a',1),('b',2)] \u53ef\u4ee5\u770b\u5230\u6392\u5e8f\u662f\u6309\u7167\u540e\u8fb9\u76840,1,2\u8fdb\u884c\u6392\u5e8f\u7684\uff0c\u800c\u4e0d\u662fa,b,c\n    # b=sorted(a,key=operator.itemgetter(0)) >>>b=[('a',1),('b',2),('c',0)] \u8fd9\u6b21\u6bd4\u8f83\u7684\u662f\u524d\u8fb9\u7684a,b,c\u800c\u4e0d\u662f0,1,2\n    # b=sorted(a,key=opertator.itemgetter(1,0)) >>>b=[('c',0),('a',1),('b',2)] \u8fd9\u4e2a\u662f\u5148\u6bd4\u8f83\u7b2c2\u4e2a\u5143\u7d20\uff0c\u7136\u540e\u5bf9\u7b2c\u4e00\u4e2a\u5143\u7d20\u8fdb\u884c\u6392\u5e8f\uff0c\u5f62\u6210\u591a\u7ea7\u6392\u5e8f\u3002\n    # sortedClassCount = sorted(classCount.items(), key=operator.itemgetter(1), reverse=True)\n    # return sortedClassCount[0][0]\n    # 3.\u5229\u7528max\u51fd\u6570\u76f4\u63a5\u8fd4\u56de\u5b57\u5178\u4e2dvalue\u6700\u5927\u7684key\n    maxClassCount = max(classCount, key=classCount.get)\n    return maxClassCount\n    \n    # ------------------------------------------------------------------------------------------------------------------------------------------\n    # \u5b9e\u73b0 classify0() \u65b9\u6cd5\u7684\u7b2c\u4e8c\u79cd\u65b9\u5f0f\n\n    # \"\"\"\n    # 1. \u8ba1\u7b97\u8ddd\u79bb\n    \n    # \u6b27\u6c0f\u8ddd\u79bb:  \u70b9\u5230\u70b9\u4e4b\u95f4\u7684\u8ddd\u79bb\n    #    \u7b2c\u4e00\u884c:  \u540c\u4e00\u4e2a\u70b9 \u5230 dataSet\u7684\u7b2c\u4e00\u4e2a\u70b9\u7684\u8ddd\u79bb\u3002\n    #    \u7b2c\u4e8c\u884c:  \u540c\u4e00\u4e2a\u70b9 \u5230 dataSet\u7684\u7b2c\u4e8c\u4e2a\u70b9\u7684\u8ddd\u79bb\u3002\n    #    ...\n    #    \u7b2cN\u884c:  \u540c\u4e00\u4e2a\u70b9 \u5230 dataSet\u7684\u7b2cN\u4e2a\u70b9\u7684\u8ddd\u79bb\u3002\n\n    # [[1,2,3],[1,2,3]]-[[1,2,3],[1,2,0]]\n    # (A1-A2)^2+(B1-B2)^2+(c1-c2)^2\n    \n    # inx - dataset \u4f7f\u7528\u4e86numpy broadcasting\uff0c\u89c1 https://docs.scipy.org/doc/numpy-1.13.0/user/basics.broadcasting.html\n    # np.sum() \u51fd\u6570\u7684\u4f7f\u7528\u89c1 https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.sum.html\n    # \"\"\"\n\t#   dist = np.sum((inx - dataset)**2, axis=1)**0.5\n    \n    # \"\"\"\n    # 2. k\u4e2a\u6700\u8fd1\u7684\u6807\u7b7e\n    \n    # \u5bf9\u8ddd\u79bb\u6392\u5e8f\u4f7f\u7528numpy\u4e2d\u7684argsort\u51fd\u6570\uff0c \u89c1 https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.sort.html#numpy.sort\n    # \u51fd\u6570\u8fd4\u56de\u7684\u662f\u7d22\u5f15\uff0c\u56e0\u6b64\u53d6\u524dk\u4e2a\u7d22\u5f15\u4f7f\u7528[0 : k]\n    # \u5c06\u8fd9k\u4e2a\u6807\u7b7e\u5b58\u5728\u5217\u8868k_labels\u4e2d\n    # \"\"\"\n    # k_labels = [labels[index] for index in dist.argsort()[0 : k]]\n\t# \"\"\"\n    # 3. \u51fa\u73b0\u6b21\u6570\u6700\u591a\u7684\u6807\u7b7e\u5373\u4e3a\u6700\u7ec8\u7c7b\u522b\n    \n    # \u4f7f\u7528collections.Counter\u53ef\u4ee5\u7edf\u8ba1\u5404\u4e2a\u6807\u7b7e\u7684\u51fa\u73b0\u6b21\u6570\uff0cmost_common\u8fd4\u56de\u51fa\u73b0\u6b21\u6570\u6700\u591a\u7684\u6807\u7b7etuple\uff0c\u4f8b\u5982[('lable1', 2)]\uff0c\u56e0\u6b64[0][0]\u53ef\u4ee5\u53d6\u51fa\u6807\u7b7e\u503c\n\t# \"\"\"\n    # label = Counter(k_labels).most_common(1)[0][0]\n    # return label\n\n    # ------------------------------------------------------------------------------------------------------------------------------------------\n\n\ndef test1():\n    \"\"\"\n    \u7b2c\u4e00\u4e2a\u4f8b\u5b50\u6f14\u793a\n    \"\"\"\n    group, labels = createDataSet()\n    print(str(group))\n    print(str(labels))\n    print(classify0([0.1, 0.1], group, labels, 3))\n\n\n# ----------------------------------------------------------------------------------------\ndef file2matrix(filename):\n    \"\"\"\n    \u5bfc\u5165\u8bad\u7ec3\u6570\u636e\n    :param filename: \u6570\u636e\u6587\u4ef6\u8def\u5f84\n    :return: \u6570\u636e\u77e9\u9635returnMat\u548c\u5bf9\u5e94\u7684\u7c7b\u522bclassLabelVector\n    \"\"\"\n    fr = open(filename)\n    # \u83b7\u5f97\u6587\u4ef6\u4e2d\u7684\u6570\u636e\u884c\u7684\u884c\u6570\n    numberOfLines = len(fr.readlines())\n    # \u751f\u6210\u5bf9\u5e94\u7684\u7a7a\u77e9\u9635\n    # \u4f8b\u5982: zeros(2\uff0c3)\u5c31\u662f\u751f\u6210\u4e00\u4e2a 2*3\u7684\u77e9\u9635\uff0c\u5404\u4e2a\u4f4d\u7f6e\u4e0a\u5168\u662f 0 \n    returnMat = zeros((numberOfLines, 3))  # prepare matrix to return\n    classLabelVector = []  # prepare labels return\n    fr = open(filename)\n    index = 0\n    for line in fr.readlines():\n        # str.strip([chars]) --\u8fd4\u56de\u79fb\u9664\u5b57\u7b26\u4e32\u5934\u5c3e\u6307\u5b9a\u7684\u5b57\u7b26\u751f\u6210\u7684\u65b0\u5b57\u7b26\u4e32\n        line = line.strip()\n        # \u4ee5 '\\t' \u5207\u5272\u5b57\u7b26\u4e32\n        listFromLine = line.split('\\t')\n        # \u6bcf\u5217\u7684\u5c5e\u6027\u6570\u636e\n        returnMat[index, :] = listFromLine[0:3]\n        # \u6bcf\u5217\u7684\u7c7b\u522b\u6570\u636e\uff0c\u5c31\u662f label \u6807\u7b7e\u6570\u636e\n        classLabelVector.append(int(listFromLine[-1]))\n        index += 1\n    # \u8fd4\u56de\u6570\u636e\u77e9\u9635returnMat\u548c\u5bf9\u5e94\u7684\u7c7b\u522bclassLabelVector\n    return returnMat, classLabelVector\n\n\ndef autoNorm(dataSet):\n    \"\"\"\n    \u5f52\u4e00\u5316\u7279\u5f81\u503c\uff0c\u6d88\u9664\u5c5e\u6027\u4e4b\u95f4\u91cf\u7ea7\u4e0d\u540c\u5bfc\u81f4\u7684\u5f71\u54cd\n    :param dataSet: \u6570\u636e\u96c6\n    :return: \u5f52\u4e00\u5316\u540e\u7684\u6570\u636e\u96c6normDataSet,ranges\u548cminVals\u5373\u6700\u5c0f\u503c\u4e0e\u8303\u56f4\uff0c\u5e76\u6ca1\u6709\u7528\u5230\n\n    \u5f52\u4e00\u5316\u516c\u5f0f: \n        Y = (X-Xmin)/(Xmax-Xmin)\n        \u5176\u4e2d\u7684 min \u548c max \u5206\u522b\u662f\u6570\u636e\u96c6\u4e2d\u7684\u6700\u5c0f\u7279\u5f81\u503c\u548c\u6700\u5927\u7279\u5f81\u503c\u3002\u8be5\u51fd\u6570\u53ef\u4ee5\u81ea\u52a8\u5c06\u6570\u5b57\u7279\u5f81\u503c\u8f6c\u5316\u4e3a0\u52301\u7684\u533a\u95f4\u3002\n    \"\"\"\n    # \u8ba1\u7b97\u6bcf\u79cd\u5c5e\u6027\u7684\u6700\u5927\u503c\u3001\u6700\u5c0f\u503c\u3001\u8303\u56f4\n    minVals = dataSet.min(0)\n    maxVals = dataSet.max(0)\n    # \u6781\u5dee\n    ranges = maxVals - minVals\n    # -------\u7b2c\u4e00\u79cd\u5b9e\u73b0\u65b9\u5f0f---start-------------------------\n    normDataSet = zeros(shape(dataSet))\n    m = dataSet.shape[0]\n    # \u751f\u6210\u4e0e\u6700\u5c0f\u503c\u4e4b\u5dee\u7ec4\u6210\u7684\u77e9\u9635\n    normDataSet = dataSet - tile(minVals, (m, 1))\n    # \u5c06\u6700\u5c0f\u503c\u4e4b\u5dee\u9664\u4ee5\u8303\u56f4\u7ec4\u6210\u77e9\u9635\n    normDataSet = normDataSet / tile(ranges, (m, 1))  # element wise divide\n    # -------\u7b2c\u4e00\u79cd\u5b9e\u73b0\u65b9\u5f0f---end---------------------------------------------\n    \n    # # -------\u7b2c\u4e8c\u79cd\u5b9e\u73b0\u65b9\u5f0f---start---------------------------------------\n    # norm_dataset = (dataset - minvalue) / ranges\n    # # -------\u7b2c\u4e8c\u79cd\u5b9e\u73b0\u65b9\u5f0f---end---------------------------------------------\n    return normDataSet, ranges, minVals\n\n\ndef datingClassTest():\n    \"\"\"\n    \u5bf9\u7ea6\u4f1a\u7f51\u7ad9\u7684\u6d4b\u8bd5\u65b9\u6cd5\n    :return: \u9519\u8bef\u6570\n    \"\"\"\n    # \u8bbe\u7f6e\u6d4b\u8bd5\u6570\u636e\u7684\u7684\u4e00\u4e2a\u6bd4\u4f8b\uff08\u8bad\u7ec3\u6570\u636e\u96c6\u6bd4\u4f8b=1-hoRatio\uff09\n    hoRatio = 0.1  # \u6d4b\u8bd5\u8303\u56f4,\u4e00\u90e8\u5206\u6d4b\u8bd5\u4e00\u90e8\u5206\u4f5c\u4e3a\u6837\u672c\n    # \u4ece\u6587\u4ef6\u4e2d\u52a0\u8f7d\u6570\u636e\n    datingDataMat, datingLabels = file2matrix('data/2.KNN/datingTestSet2.txt')  # load data setfrom file\n    # \u5f52\u4e00\u5316\u6570\u636e\n    normMat, ranges, minVals = autoNorm(datingDataMat)\n    # m \u8868\u793a\u6570\u636e\u7684\u884c\u6570\uff0c\u5373\u77e9\u9635\u7684\u7b2c\u4e00\u7ef4\n    m = normMat.shape[0]\n    # \u8bbe\u7f6e\u6d4b\u8bd5\u7684\u6837\u672c\u6570\u91cf\uff0c numTestVecs:m\u8868\u793a\u8bad\u7ec3\u6837\u672c\u7684\u6570\u91cf\n    numTestVecs = int(m * hoRatio)\n    print('numTestVecs=', numTestVecs)\n    errorCount = 0.0\n    for i in range(numTestVecs):\n        # \u5bf9\u6570\u636e\u6d4b\u8bd5\n        classifierResult = classify0(normMat[i, :], normMat[numTestVecs:m, :], datingLabels[numTestVecs:m], 3)\n        print(\"the classifier came back with: %d, the real answer is: %d\" % (classifierResult, datingLabels[i]))\n        if (classifierResult != datingLabels[i]): errorCount += 1.0\n    print(\"the total error rate is: %f\" % (errorCount / float(numTestVecs)))\n    print(errorCount)\n\n\ndef img2vector(filename):\n    \"\"\"\n    \u5c06\u56fe\u50cf\u6570\u636e\u8f6c\u6362\u4e3a\u5411\u91cf\n    :param filename: \u56fe\u7247\u6587\u4ef6 \u56e0\u4e3a\u6211\u4eec\u7684\u8f93\u5165\u6570\u636e\u7684\u56fe\u7247\u683c\u5f0f\u662f 32 * 32\u7684\n    :return: \u4e00\u7ef4\u77e9\u9635\n    \u8be5\u51fd\u6570\u5c06\u56fe\u50cf\u8f6c\u6362\u4e3a\u5411\u91cf: \u8be5\u51fd\u6570\u521b\u5efa 1 * 1024 \u7684NumPy\u6570\u7ec4\uff0c\u7136\u540e\u6253\u5f00\u7ed9\u5b9a\u7684\u6587\u4ef6\uff0c\n    \u5faa\u73af\u8bfb\u51fa\u6587\u4ef6\u7684\u524d32\u884c\uff0c\u5e76\u5c06\u6bcf\u884c\u7684\u593432\u4e2a\u5b57\u7b26\u503c\u5b58\u50a8\u5728NumPy\u6570\u7ec4\u4e2d\uff0c\u6700\u540e\u8fd4\u56de\u6570\u7ec4\u3002\n    \"\"\"\n    returnVect = zeros((1, 1024))\n    fr = open(filename)\n    for i in range(32):\n        lineStr = fr.readline()\n        for j in range(32):\n            returnVect[0, 32 * i + j] = int(lineStr[j])\n    return returnVect\n\n\ndef handwritingClassTest():\n    # 1. \u5bfc\u5165\u6570\u636e\n    hwLabels = []\n    trainingFileList = listdir('data/2.KNN/trainingDigits')  # load the training set\n    m = len(trainingFileList)\n    trainingMat = zeros((m, 1024))\n    # hwLabels\u5b58\u50a80\uff5e9\u5bf9\u5e94\u7684index\u4f4d\u7f6e\uff0c trainingMat\u5b58\u653e\u7684\u6bcf\u4e2a\u4f4d\u7f6e\u5bf9\u5e94\u7684\u56fe\u7247\u5411\u91cf\n    for i in range(m):\n        fileNameStr = trainingFileList[i]\n        fileStr = fileNameStr.split('.')[0]  # take off .txt\n        classNumStr = int(fileStr.split('_')[0])\n        hwLabels.append(classNumStr)\n        # \u5c06 32*32\u7684\u77e9\u9635->1*1024\u7684\u77e9\u9635\n        trainingMat[i, :] = img2vector('data/2.KNN/trainingDigits/%s' % fileNameStr)\n\n    # 2. \u5bfc\u5165\u6d4b\u8bd5\u6570\u636e\n    testFileList = listdir('data/2.KNN/testDigits')  # iterate through the test set\n    errorCount = 0.0\n    mTest = len(testFileList)\n    for i in range(mTest):\n        fileNameStr = testFileList[i]\n        fileStr = fileNameStr.split('.')[0]  # take off .txt\n        classNumStr = int(fileStr.split('_')[0])\n        vectorUnderTest = img2vector('data/2.KNN/testDigits/%s' % fileNameStr)\n        classifierResult = classify0(vectorUnderTest, trainingMat, hwLabels, 3)\n        print(\"the classifier came back with: %d, the real answer is: %d\" % (classifierResult, classNumStr))\n        if (classifierResult != classNumStr): errorCount += 1.0\n    print(\"\\nthe total number of errors is: %d\" % errorCount)\n    print(\"\\nthe total error rate is: %f\" % (errorCount / float(mTest)))\n\n\nif __name__ == '__main__':\n    # test1()\n    # datingClassTest()\n    handwritingClassTest()\n", "src/py2.x/ml/11.Apriori/apriori.py": "#!/usr/bin/python\n# coding: utf8\n\n'''\nCreated on Mar 24, 2011\nUpdate  on 2017-05-18\nCh 11 code\nAuthor: Peter/\u7247\u523b\nGitHub: https://github.com/apachecn/AiLearning\n'''\nfrom __future__ import print_function\nprint(__doc__)\nfrom numpy import *\n\n# \u52a0\u8f7d\u6570\u636e\u96c6\ndef loadDataSet():\n    return [[1, 3, 4], [2, 3, 5], [1, 2, 3, 5], [2, 5]]\n\n# \u521b\u5efa\u96c6\u5408 C1\u3002\u5373\u5bf9 dataSet \u8fdb\u884c\u53bb\u91cd\uff0c\u6392\u5e8f\uff0c\u653e\u5165 list \u4e2d\uff0c\u7136\u540e\u8f6c\u6362\u6240\u6709\u7684\u5143\u7d20\u4e3a frozenset\ndef createC1(dataSet):\n    \"\"\"createC1\uff08\u521b\u5efa\u96c6\u5408 C1\uff09\n\n    Args:\n        dataSet \u539f\u59cb\u6570\u636e\u96c6\n    Returns:\n        frozenset \u8fd4\u56de\u4e00\u4e2a frozenset \u683c\u5f0f\u7684 list\n    \"\"\"\n\n    C1 = []\n    for transaction in dataSet:\n        for item in transaction:\n            if not [item] in C1:\n                # \u904d\u5386\u6240\u6709\u7684\u5143\u7d20\uff0c\u5982\u679c\u4e0d\u5728 C1 \u51fa\u73b0\u8fc7\uff0c\u90a3\u4e48\u5c31 append\n                C1.append([item])\n    # \u5bf9\u6570\u7ec4\u8fdb\u884c `\u4ece\u5c0f\u5230\u5927` \u7684\u6392\u5e8f\n    # print 'sort \u524d=', C1\n    C1.sort()\n    # frozenset \u8868\u793a\u51bb\u7ed3\u7684 set \u96c6\u5408\uff0c\u5143\u7d20\u65e0\u6539\u53d8\uff1b\u53ef\u4ee5\u628a\u5b83\u5f53\u5b57\u5178\u7684 key \u6765\u4f7f\u7528\n    # print 'sort \u540e=', C1\n    # print 'frozenset=', map(frozenset, C1)\n    return map(frozenset, C1)\n\n# \u8ba1\u7b97\u5019\u9009\u6570\u636e\u96c6 CK \u5728\u6570\u636e\u96c6 D \u4e2d\u7684\u652f\u6301\u5ea6\uff0c\u5e76\u8fd4\u56de\u652f\u6301\u5ea6\u5927\u4e8e\u6700\u5c0f\u652f\u6301\u5ea6\uff08minSupport\uff09\u7684\u6570\u636e\ndef scanD(D, Ck, minSupport):\n    \"\"\"scanD\uff08\u8ba1\u7b97\u5019\u9009\u6570\u636e\u96c6 CK \u5728\u6570\u636e\u96c6 D \u4e2d\u7684\u652f\u6301\u5ea6\uff0c\u5e76\u8fd4\u56de\u652f\u6301\u5ea6\u5927\u4e8e\u6700\u5c0f\u652f\u6301\u5ea6 minSupport \u7684\u6570\u636e\uff09\n\n    Args:\n        D \u6570\u636e\u96c6\n        Ck \u5019\u9009\u9879\u96c6\u5217\u8868\n        minSupport \u6700\u5c0f\u652f\u6301\u5ea6\n    Returns:\n        retList \u652f\u6301\u5ea6\u5927\u4e8e minSupport \u7684\u96c6\u5408\n        supportData \u5019\u9009\u9879\u96c6\u652f\u6301\u5ea6\u6570\u636e\n    \"\"\"\n\n    # ssCnt \u4e34\u65f6\u5b58\u653e\u9009\u6570\u636e\u96c6 Ck \u7684\u9891\u7387. \u4f8b\u5982: a->10, b->5, c->8\n    ssCnt = {}\n    for tid in D:\n        for can in Ck:\n            # s.issubset(t)  \u6d4b\u8bd5\u662f\u5426 s \u4e2d\u7684\u6bcf\u4e00\u4e2a\u5143\u7d20\u90fd\u5728 t \u4e2d\n            if can.issubset(tid):\n                if not ssCnt.has_key(can):\n                    ssCnt[can] = 1\n                else:\n                    ssCnt[can] += 1\n    numItems = float(len(D)) # \u6570\u636e\u96c6 D \u7684\u6570\u91cf\n    retList = []\n    supportData = {}\n    for key in ssCnt:\n        # \u652f\u6301\u5ea6 = \u5019\u9009\u9879\uff08key\uff09\u51fa\u73b0\u7684\u6b21\u6570 / \u6240\u6709\u6570\u636e\u96c6\u7684\u6570\u91cf\n        support = ssCnt[key]/numItems\n        if support >= minSupport:\n            # \u5728 retList \u7684\u9996\u4f4d\u63d2\u5165\u5143\u7d20\uff0c\u53ea\u5b58\u50a8\u652f\u6301\u5ea6\u6ee1\u8db3\u9891\u7e41\u9879\u96c6\u7684\u503c\n            retList.insert(0, key)\n        # \u5b58\u50a8\u6240\u6709\u7684\u5019\u9009\u9879\uff08key\uff09\u548c\u5bf9\u5e94\u7684\u652f\u6301\u5ea6\uff08support\uff09\n        supportData[key] = support\n    return retList, supportData\n\n# \u8f93\u5165\u9891\u7e41\u9879\u96c6\u5217\u8868 Lk \u4e0e\u8fd4\u56de\u7684\u5143\u7d20\u4e2a\u6570 k\uff0c\u7136\u540e\u8f93\u51fa\u6240\u6709\u53ef\u80fd\u7684\u5019\u9009\u9879\u96c6 Ck\ndef aprioriGen(Lk, k):\n    \"\"\"aprioriGen\uff08\u8f93\u5165\u9891\u7e41\u9879\u96c6\u5217\u8868 Lk \u4e0e\u8fd4\u56de\u7684\u5143\u7d20\u4e2a\u6570 k\uff0c\u7136\u540e\u8f93\u51fa\u5019\u9009\u9879\u96c6 Ck\u3002\n       \u4f8b\u5982: \u4ee5 {0},{1},{2} \u4e3a\u8f93\u5165\u4e14 k = 2 \u5219\u8f93\u51fa {0,1}, {0,2}, {1,2}. \u4ee5 {0,1},{0,2},{1,2} \u4e3a\u8f93\u5165\u4e14 k = 3 \u5219\u8f93\u51fa {0,1,2}\n       \u4ec5\u9700\u8981\u8ba1\u7b97\u4e00\u6b21\uff0c\u4e0d\u9700\u8981\u5c06\u6240\u6709\u7684\u7ed3\u679c\u8ba1\u7b97\u51fa\u6765\uff0c\u7136\u540e\u8fdb\u884c\u53bb\u91cd\u64cd\u4f5c\n       \u8fd9\u662f\u4e00\u4e2a\u66f4\u9ad8\u6548\u7684\u7b97\u6cd5\uff09\n\n    Args:\n        Lk \u9891\u7e41\u9879\u96c6\u5217\u8868\n        k \u8fd4\u56de\u7684\u9879\u96c6\u5143\u7d20\u4e2a\u6570\uff08\u82e5\u5143\u7d20\u7684\u524d k-2 \u76f8\u540c\uff0c\u5c31\u8fdb\u884c\u5408\u5e76\uff09\n    Returns:\n        retList \u5143\u7d20\u4e24\u4e24\u5408\u5e76\u7684\u6570\u636e\u96c6\n    \"\"\"\n    \n    retList = []\n    lenLk = len(Lk)\n    for i in range(lenLk):\n        for j in range(i+1, lenLk):\n            L1 = list(Lk[i])[: k-2]\n            L2 = list(Lk[j])[: k-2]\n            # print '-----i=', i, k-2, Lk, Lk[i], list(Lk[i])[: k-2]\n            # print '-----j=', j, k-2, Lk, Lk[j], list(Lk[j])[: k-2]\n            L1.sort()\n            L2.sort()\n            # \u7b2c\u4e00\u6b21 L1,L2 \u4e3a\u7a7a\uff0c\u5143\u7d20\u76f4\u63a5\u8fdb\u884c\u5408\u5e76\uff0c\u8fd4\u56de\u5143\u7d20\u4e24\u4e24\u5408\u5e76\u7684\u6570\u636e\u96c6\n            # if first k-2 elements are equal\n            if L1 == L2:\n                # set union\n                # print 'union=', Lk[i] | Lk[j], Lk[i], Lk[j]\n                retList.append(Lk[i] | Lk[j])\n    return retList\n\n# \u627e\u51fa\u6570\u636e\u96c6 dataSet \u4e2d\u652f\u6301\u5ea6 >= \u6700\u5c0f\u652f\u6301\u5ea6\u7684\u5019\u9009\u9879\u96c6\u4ee5\u53ca\u5b83\u4eec\u7684\u652f\u6301\u5ea6\u3002\u5373\u6211\u4eec\u7684\u9891\u7e41\u9879\u96c6\u3002\ndef apriori(dataSet, minSupport=0.5):\n    \"\"\"apriori\uff08\u9996\u5148\u6784\u5efa\u96c6\u5408 C1\uff0c\u7136\u540e\u626b\u63cf\u6570\u636e\u96c6\u6765\u5224\u65ad\u8fd9\u4e9b\u53ea\u6709\u4e00\u4e2a\u5143\u7d20\u7684\u9879\u96c6\u662f\u5426\u6ee1\u8db3\u6700\u5c0f\u652f\u6301\u5ea6\u7684\u8981\u6c42\u3002\u90a3\u4e48\u6ee1\u8db3\u6700\u5c0f\u652f\u6301\u5ea6\u8981\u6c42\u7684\u9879\u96c6\u6784\u6210\u96c6\u5408 L1\u3002\u7136\u540e L1 \u4e2d\u7684\u5143\u7d20\u76f8\u4e92\u7ec4\u5408\u6210 C2\uff0cC2 \u518d\u8fdb\u4e00\u6b65\u8fc7\u6ee4\u53d8\u6210 L2\uff0c\u7136\u540e\u4ee5\u6b64\u7c7b\u63a8\uff0c\u77e5\u9053 CN \u7684\u957f\u5ea6\u4e3a 0 \u65f6\u7ed3\u675f\uff0c\u5373\u53ef\u627e\u51fa\u6240\u6709\u9891\u7e41\u9879\u96c6\u7684\u652f\u6301\u5ea6\u3002\uff09\n\n    Args:\n        dataSet \u539f\u59cb\u6570\u636e\u96c6\n        minSupport \u652f\u6301\u5ea6\u7684\u9608\u503c\n    Returns:\n        L \u9891\u7e41\u9879\u96c6\u7684\u5168\u96c6\n        supportData \u6240\u6709\u5143\u7d20\u548c\u652f\u6301\u5ea6\u7684\u5168\u96c6\n    \"\"\"\n    # C1 \u5373\u5bf9 dataSet \u8fdb\u884c\u53bb\u91cd\uff0c\u6392\u5e8f\uff0c\u653e\u5165 list \u4e2d\uff0c\u7136\u540e\u8f6c\u6362\u6240\u6709\u7684\u5143\u7d20\u4e3a frozenset\n    C1 = createC1(dataSet)\n    # print 'C1: ', C1\n    # \u5bf9\u6bcf\u4e00\u884c\u8fdb\u884c set \u8f6c\u6362\uff0c\u7136\u540e\u5b58\u653e\u5230\u96c6\u5408\u4e2d\n    D = map(set, dataSet)\n    # print 'D=', D\n    # \u8ba1\u7b97\u5019\u9009\u6570\u636e\u96c6 C1 \u5728\u6570\u636e\u96c6 D \u4e2d\u7684\u652f\u6301\u5ea6\uff0c\u5e76\u8fd4\u56de\u652f\u6301\u5ea6\u5927\u4e8e minSupport \u7684\u6570\u636e\n    L1, supportData = scanD(D, C1, minSupport)\n    # print \"L1=\", L1, \"\\n\", \"outcome: \", supportData\n\n    # L \u52a0\u4e86\u4e00\u5c42 list, L \u4e00\u5171 2 \u5c42 list\n    L = [L1]\n    k = 2\n    # \u5224\u65ad L \u7684\u7b2c k-2 \u9879\u7684\u6570\u636e\u957f\u5ea6\u662f\u5426 > 0\u3002\u7b2c\u4e00\u6b21\u6267\u884c\u65f6 L \u4e3a [[frozenset([1]), frozenset([3]), frozenset([2]), frozenset([5])]]\u3002L[k-2]=L[0]=[frozenset([1]), frozenset([3]), frozenset([2]), frozenset([5])]\uff0c\u6700\u540e\u9762 k += 1\n    while (len(L[k-2]) > 0):\n        # print 'k=', k, L, L[k-2]\n        Ck = aprioriGen(L[k-2], k) # \u4f8b\u5982: \u4ee5 {0},{1},{2} \u4e3a\u8f93\u5165\u4e14 k = 2 \u5219\u8f93\u51fa {0,1}, {0,2}, {1,2}. \u4ee5 {0,1},{0,2},{1,2} \u4e3a\u8f93\u5165\u4e14 k = 3 \u5219\u8f93\u51fa {0,1,2}\n        # print 'Ck', Ck\n\n        Lk, supK = scanD(D, Ck, minSupport) # \u8ba1\u7b97\u5019\u9009\u6570\u636e\u96c6 CK \u5728\u6570\u636e\u96c6 D \u4e2d\u7684\u652f\u6301\u5ea6\uff0c\u5e76\u8fd4\u56de\u652f\u6301\u5ea6\u5927\u4e8e minSupport \u7684\u6570\u636e\n        # \u4fdd\u5b58\u6240\u6709\u5019\u9009\u9879\u96c6\u7684\u652f\u6301\u5ea6\uff0c\u5982\u679c\u5b57\u5178\u6ca1\u6709\uff0c\u5c31\u8ffd\u52a0\u5143\u7d20\uff0c\u5982\u679c\u6709\uff0c\u5c31\u66f4\u65b0\u5143\u7d20\n        supportData.update(supK)\n        if len(Lk) == 0:\n            break\n        # Lk \u8868\u793a\u6ee1\u8db3\u9891\u7e41\u5b50\u9879\u7684\u96c6\u5408\uff0cL \u5143\u7d20\u5728\u589e\u52a0\uff0c\u4f8b\u5982: \n        # l=[[set(1), set(2), set(3)]]\n        # l=[[set(1), set(2), set(3)], [set(1, 2), set(2, 3)]]\n        L.append(Lk)\n        k += 1\n        # print 'k=', k, len(L[k-2])\n    return L, supportData\n\n# \u8ba1\u7b97\u53ef\u4fe1\u5ea6\uff08confidence\uff09\ndef calcConf(freqSet, H, supportData, brl, minConf=0.7):\n    \"\"\"calcConf\uff08\u5bf9\u4e24\u4e2a\u5143\u7d20\u7684\u9891\u7e41\u9879\uff0c\u8ba1\u7b97\u53ef\u4fe1\u5ea6\uff0c\u4f8b\u5982:  {1,2}/{1} \u6216\u8005 {1,2}/{2} \u770b\u662f\u5426\u6ee1\u8db3\u6761\u4ef6\uff09\n\n    Args:\n        freqSet \u9891\u7e41\u9879\u96c6\u4e2d\u7684\u5143\u7d20\uff0c\u4f8b\u5982: frozenset([1, 3])    \n        H \u9891\u7e41\u9879\u96c6\u4e2d\u7684\u5143\u7d20\u7684\u96c6\u5408\uff0c\u4f8b\u5982: [frozenset([1]), frozenset([3])]\n        supportData \u6240\u6709\u5143\u7d20\u7684\u652f\u6301\u5ea6\u7684\u5b57\u5178\n        brl \u5173\u8054\u89c4\u5219\u5217\u8868\u7684\u7a7a\u6570\u7ec4\n        minConf \u6700\u5c0f\u53ef\u4fe1\u5ea6\n    Returns:\n        prunedH \u8bb0\u5f55 \u53ef\u4fe1\u5ea6\u5927\u4e8e\u9608\u503c\u7684\u96c6\u5408\n    \"\"\"\n    # \u8bb0\u5f55\u53ef\u4fe1\u5ea6\u5927\u4e8e\u6700\u5c0f\u53ef\u4fe1\u5ea6\uff08minConf\uff09\u7684\u96c6\u5408\n    prunedH = []\n    for conseq in H: # \u5047\u8bbe freqSet = frozenset([1, 3]), H = [frozenset([1]), frozenset([3])]\uff0c\u90a3\u4e48\u73b0\u5728\u9700\u8981\u6c42\u51fa frozenset([1]) -> frozenset([3]) \u7684\u53ef\u4fe1\u5ea6\u548c frozenset([3]) -> frozenset([1]) \u7684\u53ef\u4fe1\u5ea6\n\n        # print 'confData=', freqSet, H, conseq, freqSet-conseq\n        conf = supportData[freqSet]/supportData[freqSet-conseq] # \u652f\u6301\u5ea6\u5b9a\u4e49: a -> b = support(a | b) / support(a). \u5047\u8bbe  freqSet = frozenset([1, 3]), conseq = [frozenset([1])]\uff0c\u90a3\u4e48 frozenset([1]) \u81f3 frozenset([3]) \u7684\u53ef\u4fe1\u5ea6\u4e3a = support(a | b) / support(a) = supportData[freqSet]/supportData[freqSet-conseq] = supportData[frozenset([1, 3])] / supportData[frozenset([1])]\n        if conf >= minConf:\n            # \u53ea\u8981\u4e70\u4e86 freqSet-conseq \u96c6\u5408\uff0c\u4e00\u5b9a\u4f1a\u4e70 conseq \u96c6\u5408\uff08freqSet-conseq \u96c6\u5408\u548c conseq\u96c6\u5408 \u662f\u5168\u96c6\uff09\n            print(freqSet-conseq, '-->', conseq, 'conf:', conf)\n            brl.append((freqSet-conseq, conseq, conf))\n            prunedH.append(conseq)\n    return prunedH\n\n# \u9012\u5f52\u8ba1\u7b97\u9891\u7e41\u9879\u96c6\u7684\u89c4\u5219\ndef rulesFromConseq(freqSet, H, supportData, brl, minConf=0.7):\n    \"\"\"rulesFromConseq\n\n    Args:\n        freqSet \u9891\u7e41\u9879\u96c6\u4e2d\u7684\u5143\u7d20\uff0c\u4f8b\u5982: frozenset([2, 3, 5])    \n        H \u9891\u7e41\u9879\u96c6\u4e2d\u7684\u5143\u7d20\u7684\u96c6\u5408\uff0c\u4f8b\u5982: [frozenset([2]), frozenset([3]), frozenset([5])]\n        supportData \u6240\u6709\u5143\u7d20\u7684\u652f\u6301\u5ea6\u7684\u5b57\u5178\n        brl \u5173\u8054\u89c4\u5219\u5217\u8868\u7684\u6570\u7ec4\n        minConf \u6700\u5c0f\u53ef\u4fe1\u5ea6\n    \"\"\"\n    # H[0] \u662f freqSet \u7684\u5143\u7d20\u7ec4\u5408\u7684\u7b2c\u4e00\u4e2a\u5143\u7d20\uff0c\u5e76\u4e14 H \u4e2d\u6240\u6709\u5143\u7d20\u7684\u957f\u5ea6\u90fd\u4e00\u6837\uff0c\u957f\u5ea6\u7531 aprioriGen(H, m+1) \u8fd9\u91cc\u7684 m + 1 \u6765\u63a7\u5236\n    # \u8be5\u51fd\u6570\u9012\u5f52\u65f6\uff0cH[0] \u7684\u957f\u5ea6\u4ece 1 \u5f00\u59cb\u589e\u957f 1 2 3 ...\n    # \u5047\u8bbe freqSet = frozenset([2, 3, 5]), H = [frozenset([2]), frozenset([3]), frozenset([5])]\n    # \u90a3\u4e48 m = len(H[0]) \u7684\u9012\u5f52\u7684\u503c\u4f9d\u6b21\u4e3a 1 2\n    # \u5728 m = 2 \u65f6, \u8df3\u51fa\u8be5\u9012\u5f52\u3002\u5047\u8bbe\u518d\u9012\u5f52\u4e00\u6b21\uff0c\u90a3\u4e48 H[0] = frozenset([2, 3, 5])\uff0cfreqSet = frozenset([2, 3, 5]) \uff0c\u6ca1\u5fc5\u8981\u518d\u8ba1\u7b97 freqSet \u4e0e H[0] \u7684\u5173\u8054\u89c4\u5219\u4e86\u3002\n    m = len(H[0])\n    if (len(freqSet) > (m + 1)):\n        # print 'freqSet******************', len(freqSet), m + 1, freqSet, H, H[0]\n        # \u751f\u6210 m+1 \u4e2a\u957f\u5ea6\u7684\u6240\u6709\u53ef\u80fd\u7684 H \u4e2d\u7684\u7ec4\u5408\uff0c\u5047\u8bbe H = [frozenset([2]), frozenset([3]), frozenset([5])]\n        # \u7b2c\u4e00\u6b21\u9012\u5f52\u8c03\u7528\u65f6\u751f\u6210 [frozenset([2, 3]), frozenset([2, 5]), frozenset([3, 5])]\n        # \u7b2c\u4e8c\u6b21 \u3002\u3002\u3002\u6ca1\u6709\u7b2c\u4e8c\u6b21\uff0c\u9012\u5f52\u6761\u4ef6\u5224\u65ad\u65f6\u5df2\u7ecf\u9000\u51fa\u4e86\n        Hmp1 = aprioriGen(H, m+1)\n        # \u8fd4\u56de\u53ef\u4fe1\u5ea6\u5927\u4e8e\u6700\u5c0f\u53ef\u4fe1\u5ea6\u7684\u96c6\u5408\n        Hmp1 = calcConf(freqSet, Hmp1, supportData, brl, minConf)\n        print('Hmp1=', Hmp1)\n        print('len(Hmp1)=', len(Hmp1), 'len(freqSet)=', len(freqSet))\n        # \u8ba1\u7b97\u53ef\u4fe1\u5ea6\u540e\uff0c\u8fd8\u6709\u6570\u636e\u5927\u4e8e\u6700\u5c0f\u53ef\u4fe1\u5ea6\u7684\u8bdd\uff0c\u90a3\u4e48\u7ee7\u7eed\u9012\u5f52\u8c03\u7528\uff0c\u5426\u5219\u8df3\u51fa\u9012\u5f52\n        if (len(Hmp1) > 1):\n            # print '----------------------', Hmp1\n            # print len(freqSet),  len(Hmp1[0]) + 1\n            rulesFromConseq(freqSet, Hmp1, supportData, brl, minConf)\n\n# \u751f\u6210\u5173\u8054\u89c4\u5219\ndef generateRules(L, supportData, minConf=0.7):\n    \"\"\"generateRules\n\n    Args:\n        L \u9891\u7e41\u9879\u96c6\u5217\u8868\n        supportData \u9891\u7e41\u9879\u96c6\u652f\u6301\u5ea6\u7684\u5b57\u5178\n        minConf \u6700\u5c0f\u7f6e\u4fe1\u5ea6\n    Returns:\n        bigRuleList \u53ef\u4fe1\u5ea6\u89c4\u5219\u5217\u8868\uff08\u5173\u4e8e (A->B+\u7f6e\u4fe1\u5ea6) 3\u4e2a\u5b57\u6bb5\u7684\u7ec4\u5408\uff09\n    \"\"\"\n    bigRuleList = []\n    # \u5047\u8bbe L = [[frozenset([1]), frozenset([3]), frozenset([2]), frozenset([5])], [frozenset([1, 3]), frozenset([2, 5]), frozenset([2, 3]), frozenset([3, 5])], [frozenset([2, 3, 5])]]\n    for i in range(1, len(L)):\n        # \u83b7\u53d6\u9891\u7e41\u9879\u96c6\u4e2d\u6bcf\u4e2a\u7ec4\u5408\u7684\u6240\u6709\u5143\u7d20\n        for freqSet in L[i]:\n            # \u5047\u8bbe: freqSet= frozenset([1, 3]), H1=[frozenset([1]), frozenset([3])]\n            # \u7ec4\u5408\u603b\u7684\u5143\u7d20\u5e76\u904d\u5386\u5b50\u5143\u7d20\uff0c\u5e76\u8f6c\u5316\u4e3a frozenset \u96c6\u5408\uff0c\u518d\u5b58\u653e\u5230 list \u5217\u8868\u4e2d\n            H1 = [frozenset([item]) for item in freqSet]\n            # 2 \u4e2a\u7684\u7ec4\u5408\uff0c\u8d70 else, 2 \u4e2a\u4ee5\u4e0a\u7684\u7ec4\u5408\uff0c\u8d70 if\n            if (i > 1):\n                rulesFromConseq(freqSet, H1, supportData, bigRuleList, minConf)\n            else:\n                calcConf(freqSet, H1, supportData, bigRuleList, minConf)\n    return bigRuleList\n\n\ndef getActionIds():\n    from time import sleep\n    from votesmart import votesmart\n    # votesmart.apikey = 'get your api key first'\n    votesmart.apikey = 'a7fa40adec6f4a77178799fae4441030'\n    actionIdList = []\n    billTitleList = []\n    fr = open('data/11.Apriori/recent20bills.txt')\n    for line in fr.readlines():\n        billNum = int(line.split('\\t')[0])\n        try:\n            billDetail = votesmart.votes.getBill(billNum) # api call\n            for action in billDetail.actions:\n                if action.level == 'House' and (action.stage == 'Passage' or action.stage == 'Amendment Vote'):\n                    actionId = int(action.actionId)\n                    print('bill: %d has actionId: %d' % (billNum, actionId))\n                    actionIdList.append(actionId)\n                    billTitleList.append(line.strip().split('\\t')[1])\n        except:\n            print(\"problem getting bill %d\" % billNum)\n        sleep(1)                                      # delay to be polite\n    return actionIdList, billTitleList\n\n\ndef getTransList(actionIdList, billTitleList): #this will return a list of lists containing ints\n    itemMeaning = ['Republican', 'Democratic']#list of what each item stands for\n    for billTitle in billTitleList:#fill up itemMeaning list\n        itemMeaning.append('%s -- Nay' % billTitle)\n        itemMeaning.append('%s -- Yea' % billTitle)\n    transDict = {}#list of items in each transaction (politician)\n    voteCount = 2\n    for actionId in actionIdList:\n        sleep(3)\n        print('getting votes for actionId: %d' % actionId)\n        try:\n            voteList = votesmart.votes.getBillActionVotes(actionId)\n            for vote in voteList:\n                if not transDict.has_key(vote.candidateName):\n                    transDict[vote.candidateName] = []\n                    if vote.officeParties == 'Democratic':\n                        transDict[vote.candidateName].append(1)\n                    elif vote.officeParties == 'Republican':\n                        transDict[vote.candidateName].append(0)\n                if vote.action == 'Nay':\n                    transDict[vote.candidateName].append(voteCount)\n                elif vote.action == 'Yea':\n                    transDict[vote.candidateName].append(voteCount + 1)\n        except:\n            print(\"problem getting actionId: %d\" % actionId)\n        voteCount += 2\n    return transDict, itemMeaning\n\n\n# \u6682\u65f6\u6ca1\u7528\u4e0a\n# def pntRules(ruleList, itemMeaning):\n#     for ruleTup in ruleList:\n#         for item in ruleTup[0]:\n#             print itemMeaning[item]\n#         print \"           -------->\"\n#         for item in ruleTup[1]:\n#             print itemMeaning[item]\n#         print \"confidence: %f\" % ruleTup[2]\n#         print       #print a blank line\n\ndef testApriori():\n    # \u52a0\u8f7d\u6d4b\u8bd5\u6570\u636e\u96c6\n    dataSet = loadDataSet()\n    print('dataSet: ', dataSet)\n\n    # Apriori \u7b97\u6cd5\u751f\u6210\u9891\u7e41\u9879\u96c6\u4ee5\u53ca\u5b83\u4eec\u7684\u652f\u6301\u5ea6\n    L1, supportData1 = apriori(dataSet, minSupport=0.7)\n    print('L(0.7): ', L1)\n    print('supportData(0.7): ', supportData1)\n\n    print('->->->->->->->->->->->->->->->->->->->->->->->->->->->->')\n\n    # Apriori \u7b97\u6cd5\u751f\u6210\u9891\u7e41\u9879\u96c6\u4ee5\u53ca\u5b83\u4eec\u7684\u652f\u6301\u5ea6\n    L2, supportData2 = apriori(dataSet, minSupport=0.5)\n    print('L(0.5): ', L2)\n    print('supportData(0.5): ', supportData2)\n\ndef testGenerateRules():\n    # \u52a0\u8f7d\u6d4b\u8bd5\u6570\u636e\u96c6\n    dataSet = loadDataSet()\n    print('dataSet: ', dataSet)\n\n    # Apriori \u7b97\u6cd5\u751f\u6210\u9891\u7e41\u9879\u96c6\u4ee5\u53ca\u5b83\u4eec\u7684\u652f\u6301\u5ea6\n    L1, supportData1 = apriori(dataSet, minSupport=0.5)\n    print('L(0.7): ', L1)\n    print('supportData(0.7): ', supportData1)\n\n    # \u751f\u6210\u5173\u8054\u89c4\u5219\n    rules = generateRules(L1, supportData1, minConf=0.5)\n    print('rules: ', rules)\n\ndef main():\n    # \u6d4b\u8bd5 Apriori \u7b97\u6cd5\n    testApriori()\n\n    # \u751f\u6210\u5173\u8054\u89c4\u5219\n    # testGenerateRules()\n\n    # # \u9879\u76ee\u6848\u4f8b\n    # # \u6784\u5efa\u7f8e\u56fd\u56fd\u4f1a\u6295\u7968\u8bb0\u5f55\u7684\u4e8b\u52a1\u6570\u636e\u96c6\n    # actionIdList, billTitleList = getActionIds()\n    # # \u6d4b\u8bd5\u524d2\u4e2a\n    # # transDict, itemMeaning = getTransList(actionIdList[: 2], billTitleList[: 2])\n    # # transDict \u8868\u793a action_id\u7684\u96c6\u5408\uff0ctransDict[key]\u8fd9\u4e2a\u5c31\u662faction_id\u5bf9\u5e94\u7684\u9009\u9879\uff0c\u4f8b\u5982 [1, 2, 3]\n    # transDict, itemMeaning = getTransList(actionIdList, billTitleList)\n    # # \u5f97\u5230\u5168\u96c6\u7684\u6570\u636e\n    # dataSet = [transDict[key] for key in transDict.keys()]\n    # L, supportData = apriori(dataSet, minSupport=0.3)\n    # rules = generateRules(L, supportData, minConf=0.95)\n    # print rules\n\n    # # \u9879\u76ee\u6848\u4f8b\n    # # \u53d1\u73b0\u6bd2\u8611\u83c7\u7684\u76f8\u4f3c\u7279\u6027\n    # # \u5f97\u5230\u5168\u96c6\u7684\u6570\u636e\n    # dataSet = [line.split() for line in open(\"data/11.Apriori/mushroom.dat\").readlines()]\n    # L, supportData = apriori(dataSet, minSupport=0.3)\n    # # 2\u8868\u793a\u6bd2\u8611\u83c7\uff0c1\u8868\u793a\u53ef\u98df\u7528\u7684\u8611\u83c7\n    # # \u627e\u51fa\u5173\u4e8e2\u7684\u9891\u7e41\u5b50\u9879\u51fa\u6765\uff0c\u5c31\u77e5\u9053\u5982\u679c\u662f\u6bd2\u8611\u83c7\uff0c\u90a3\u4e48\u51fa\u73b0\u9891\u7e41\u7684\u4e5f\u53ef\u80fd\u662f\u6bd2\u8611\u83c7\n    # for item in L[1]:\n    #     if item.intersection('2'):\n    #         print item\n\n    # for item in L[2]:\n    #     if item.intersection('2'):\n    #         print item\n\nif __name__ == \"__main__\":\n    main()\n", "src/py2.x/ml/8.Regression/sklearn-regression-demo.py": "#!/usr/bin/python\n# coding:utf8\n\n'''\nCreated on Jan 8, 2011\nUpdate  on 2017-05-18\nAuthor: Peter Harrington/\u5c0f\u7476\nGitHub: https://github.com/apachecn/AiLearning\n'''\nfrom __future__ import print_function\n\n\n# Isotonic Regression \u7b49\u5f0f\u56de\u5f52\nprint(__doc__)\n\n# Author: Nelle Varoquaux <nelle.varoquaux@gmail.com>\n#         Alexandre Gramfort <alexandre.gramfort@inria.fr>\n# License: BSD\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.collections import LineCollection\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.isotonic import IsotonicRegression\nfrom sklearn.utils import check_random_state\n\nn = 100\nx = np.arange(n)\nrs = check_random_state(0)\ny = rs.randint(-50, 50, size=(n,)) + 50. * np.log(1 + np.arange(n))\n\nir = IsotonicRegression()\n\ny_ = ir.fit_transform(x, y)\n\nlr = LinearRegression()\nlr.fit(x[:, np.newaxis], y)  # \u7ebf\u6027\u56de\u5f52\u7684 x \u9700\u8981\u4e3a 2d\n\nsegments = [[[i, y[i]], [i, y_[i]]] for i in range(n)]\nlc = LineCollection(segments, zorder=0)\nlc.set_array(np.ones(len(y)))\nlc.set_linewidths(0.5 * np.ones(n))\n\nfig = plt.figure()\nplt.plot(x, y, 'r.', markersize=12)\nplt.plot(x, y_, 'g.-', markersize=12)\nplt.plot(x, lr.predict(x[:, np.newaxis]), 'b-')\nplt.gca().add_collection(lc)\nplt.legend(('Data', 'Isotonic Fit', 'Linear Fit'), loc='lower right')\nplt.title('Isotonic regression')\nplt.show()\n\n# Kernel ridge regression ( \u5185\u6838\u5cad\u56de\u5f52 )\n\n# 2.1 Comparison of kernel ridge regression and SVR ( \u5185\u6838\u5cad\u56de\u5f52\u4e0e SVR \u7684\u6bd4\u8f83 )\n\n# Authors: Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>\n# License: BSD 3 clause\n\n'''\nfrom __future__ import division\nimport time\n\nimport numpy as np\n\nfrom sklearn.svm import SVR\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.kernel_ridge import KernelRidge\nimport matplotlib.pyplot as plt\n\nrng = np.random.RandomState(0)\n\n# \u751f\u6210\u6837\u672c\u6570\u636e\nX = 5 * rng.rand(10000, 1)\ny = np.sin(X).ravel()\n\n# \u7ed9\u76ee\u6807\u589e\u52a0\u566a\u97f3\ny[::5] += 3 * (0.5 - rng.rand(X.shape[0] // 5))\n\nX_plot = np.linspace(0, 5, 100000)[:, None]\n\n# Fit regression model ( \u62df\u5408 \u56de\u5f52 \u6a21\u578b )\ntrain_size = 100\nsvr = GridSearchCV(SVR(kernel='rbf', gamma=0.1), cv=5,\n                   param_grid={\"C\": [1e0, 1e1, 1e2, 1e3],\n                               \"gamma\": np.logspace(-2, 2, 5)})\n\nkr = GridSearchCV(KernelRidge(kernel='rbf', gamma=0.1), cv=5,\n                  param_grid={\"alpha\": [1e0, 0.1, 1e-2, 1e-3],\n                              \"gamma\": np.logspace(-2, 2, 5)})\n\nt0 = time.time()\nsvr.fit(X[:train_size], y[:train_size])\nsvr_fit = time.time() - t0\nprint(\"SVR complexity and bandwidth selected and model fitted in %.3f s\"\n      % svr_fit)\n\nt0 = time.time()\nkr.fit(X[:train_size], y[:train_size])\nkr_fit = time.time() - t0\nprint(\"KRR complexity and bandwidth selected and model fitted in %.3f s\"\n      % kr_fit)\n\nsv_ratio = svr.best_estimator_.support_.shape[0] / train_size\nprint(\"Support vector ratio: %.3f\" % sv_ratio)\n\nt0 = time.time()\ny_svr = svr.predict(X_plot)\nsvr_predict = time.time() - t0\nprint(\"SVR prediction for %d inputs in %.3f s\"\n      % (X_plot.shape[0], svr_predict))\n\nt0 = time.time()\ny_kr = kr.predict(X_plot)\nkr_predict = time.time() - t0\nprint(\"KRR prediction for %d inputs in %.3f s\"\n      % (X_plot.shape[0], kr_predict))\n\n# \u67e5\u770b\u7ed3\u679c\nsv_ind = svr.best_estimator_.support_\nplt.scatter(X[sv_ind], y[sv_ind], c='r', s=50, label='SVR support vectors',\n            zorder=2)\nplt.scatter(X[:100], y[:100], c='k', label='data', zorder=1)\nplt.hold('on')\nplt.plot(X_plot, y_svr, c='r',\n         label='SVR (fit: %.3fs, predict: %.3fs)' % (svr_fit, svr_predict))\nplt.plot(X_plot, y_kr, c='g',\n         label='KRR (fit: %.3fs, predict: %.3fs)' % (kr_fit, kr_predict))\nplt.xlabel('data')\nplt.ylabel('target')\nplt.title('SVR versus Kernel Ridge')\nplt.legend()\n\n# \u53ef\u89c6\u5316\u8bad\u7ec3\u548c\u9884\u6d4b\u65f6\u95f4\nplt.figure()\n\n# \u751f\u6210\u6837\u672c\u6570\u636e\nX = 5 * rng.rand(10000, 1)\ny = np.sin(X).ravel()\ny[::5] += 3 * (0.5 - rng.rand(X.shape[0] // 5))\nsizes = np.logspace(1, 4, 7, dtype=np.int)\nfor name, estimator in {\"KRR\": KernelRidge(kernel='rbf', alpha=0.1,\n                                           gamma=10),\n                        \"SVR\": SVR(kernel='rbf', C=1e1, gamma=10)}.items():\n    train_time = []\n    test_time = []\n    for train_test_size in sizes:\n        t0 = time.time()\n        estimator.fit(X[:train_test_size], y[:train_test_size])\n        train_time.append(time.time() - t0)\n\n        t0 = time.time()\n        estimator.predict(X_plot[:1000])\n        test_time.append(time.time() - t0)\n\n    plt.plot(sizes, train_time, 'o-', color=\"r\" if name == \"SVR\" else \"g\",\n             label=\"%s (train)\" % name)\n    plt.plot(sizes, test_time, 'o--', color=\"r\" if name == \"SVR\" else \"g\",\n             label=\"%s (test)\" % name)\n\nplt.xscale(\"log\")\nplt.yscale(\"log\")\nplt.xlabel(\"Train size\")\nplt.ylabel(\"Time (seconds)\")\nplt.title('Execution Time')\nplt.legend(loc=\"best\")\n\n# \u53ef\u89c6\u5316\u5b66\u4e60\u66f2\u7ebf\nplt.figure()\n\nsvr = SVR(kernel='rbf', C=1e1, gamma=0.1)\nkr = KernelRidge(kernel='rbf', alpha=0.1, gamma=0.1)\ntrain_sizes, train_scores_svr, test_scores_svr = \\\n    learning_curve(svr, X[:100], y[:100], train_sizes=np.linspace(0.1, 1, 10),\n                   scoring=\"neg_mean_squared_error\", cv=10)\ntrain_sizes_abs, train_scores_kr, test_scores_kr = \\\n    learning_curve(kr, X[:100], y[:100], train_sizes=np.linspace(0.1, 1, 10),\n                   scoring=\"neg_mean_squared_error\", cv=10)\n\nplt.plot(train_sizes, -test_scores_svr.mean(1), 'o-', color=\"r\",\n         label=\"SVR\")\nplt.plot(train_sizes, -test_scores_kr.mean(1), 'o-', color=\"g\",\n         label=\"KRR\")\nplt.xlabel(\"Train size\")\nplt.ylabel(\"Mean Squared Error\")\nplt.title('Learning curves')\nplt.legend(loc=\"best\")\n\nplt.show()\n'''\n", "src/py2.x/ml/8.Regression/regression.py": "#!/usr/bin/python\n# coding:utf8\n'''\nCreated on Jan 8, 2011\nUpdate  on 2017-05-18\nAuthor: Peter Harrington/\u5c0f\u7476\nGitHub: https://github.com/apachecn/AiLearning\n'''\nfrom __future__ import print_function\n\nfrom numpy import *\nimport matplotlib.pylab as plt\n\n\ndef loadDataSet(fileName):\n    \"\"\" \u52a0\u8f7d\u6570\u636e\n        \u89e3\u6790\u4ee5tab\u952e\u5206\u9694\u7684\u6587\u4ef6\u4e2d\u7684\u6d6e\u70b9\u6570\n    Returns: \n        dataMat :   feature \u5bf9\u5e94\u7684\u6570\u636e\u96c6\n        labelMat :  feature \u5bf9\u5e94\u7684\u5206\u7c7b\u6807\u7b7e\uff0c\u5373\u7c7b\u522b\u6807\u7b7e\n    \"\"\"\n    # \u83b7\u53d6\u6837\u672c\u7279\u5f81\u7684\u603b\u6570\uff0c\u4e0d\u7b97\u6700\u540e\u7684\u76ee\u6807\u53d8\u91cf \n    numFeat = len(open(fileName).readline().split('\\t')) - 1\n    dataMat = []\n    labelMat = []\n    fr = open(fileName)\n    for line in fr.readlines():\n        # \u8bfb\u53d6\u6bcf\u4e00\u884c\n        lineArr = []\n        # \u5220\u9664\u4e00\u884c\u4e2d\u4ee5tab\u5206\u9694\u7684\u6570\u636e\u524d\u540e\u7684\u7a7a\u767d\u7b26\u53f7\n        curLine = line.strip().split('\\t')\n        # i \u4ece0\u52302\uff0c\u4e0d\u5305\u62ec2 \n        for i in range(numFeat):\n            # \u5c06\u6570\u636e\u6dfb\u52a0\u5230lineArr List\u4e2d\uff0c\u6bcf\u4e00\u884c\u6570\u636e\u6d4b\u8bd5\u6570\u636e\u7ec4\u6210\u4e00\u4e2a\u884c\u5411\u91cf           \n            lineArr.append(float(curLine[i]))\n            # \u5c06\u6d4b\u8bd5\u6570\u636e\u7684\u8f93\u5165\u6570\u636e\u90e8\u5206\u5b58\u50a8\u5230dataMat \u7684List\u4e2d\n        dataMat.append(lineArr)\n        # \u5c06\u6bcf\u4e00\u884c\u7684\u6700\u540e\u4e00\u4e2a\u6570\u636e\uff0c\u5373\u7c7b\u522b\uff0c\u6216\u8005\u53eb\u76ee\u6807\u53d8\u91cf\u5b58\u50a8\u5230labelMat List\u4e2d\n        labelMat.append(float(curLine[-1]))\n    return dataMat, labelMat\n\n\ndef standRegres(xArr, yArr):\n    '''\n    Description: \n        \u7ebf\u6027\u56de\u5f52\n    Args:\n        xArr : \u8f93\u5165\u7684\u6837\u672c\u6570\u636e\uff0c\u5305\u542b\u6bcf\u4e2a\u6837\u672c\u6570\u636e\u7684 feature\n        yArr : \u5bf9\u5e94\u4e8e\u8f93\u5165\u6570\u636e\u7684\u7c7b\u522b\u6807\u7b7e\uff0c\u4e5f\u5c31\u662f\u6bcf\u4e2a\u6837\u672c\u5bf9\u5e94\u7684\u76ee\u6807\u53d8\u91cf\n    Returns:\n        ws: \u56de\u5f52\u7cfb\u6570\n    '''\n\n    # mat()\u51fd\u6570\u5c06xArr\uff0cyArr\u8f6c\u6362\u4e3a\u77e9\u9635 mat().T \u4ee3\u8868\u7684\u662f\u5bf9\u77e9\u9635\u8fdb\u884c\u8f6c\u7f6e\u64cd\u4f5c\n    xMat = mat(xArr)\n    yMat = mat(yArr).T\n    # \u77e9\u9635\u4e58\u6cd5\u7684\u6761\u4ef6\u662f\u5de6\u77e9\u9635\u7684\u5217\u6570\u7b49\u4e8e\u53f3\u77e9\u9635\u7684\u884c\u6570\n    xTx = xMat.T * xMat\n    # \u56e0\u4e3a\u8981\u7528\u5230xTx\u7684\u9006\u77e9\u9635\uff0c\u6240\u4ee5\u4e8b\u5148\u9700\u8981\u786e\u5b9a\u8ba1\u7b97\u5f97\u5230\u7684xTx\u662f\u5426\u53ef\u9006\uff0c\u6761\u4ef6\u662f\u77e9\u9635\u7684\u884c\u5217\u5f0f\u4e0d\u4e3a0\n    # linalg.det() \u51fd\u6570\u662f\u7528\u6765\u6c42\u5f97\u77e9\u9635\u7684\u884c\u5217\u5f0f\u7684\uff0c\u5982\u679c\u77e9\u9635\u7684\u884c\u5217\u5f0f\u4e3a0\uff0c\u5219\u8fd9\u4e2a\u77e9\u9635\u662f\u4e0d\u53ef\u9006\u7684\uff0c\u5c31\u65e0\u6cd5\u8fdb\u884c\u63a5\u4e0b\u6765\u7684\u8fd0\u7b97                   \n    if linalg.det(xTx) == 0.0:\n        print(\"This matrix is singular, cannot do inverse\")\n        return\n    # \u6700\u5c0f\u4e8c\u4e58\u6cd5\n    # http://cwiki.apachecn.org/pages/viewpage.action?pageId=5505133\n    # \u4e66\u4e2d\u7684\u516c\u5f0f\uff0c\u6c42\u5f97w\u7684\u6700\u4f18\u89e3\n    ws = xTx.I * (xMat.T * yMat)\n    return ws\n\n\n# \u5c40\u90e8\u52a0\u6743\u7ebf\u6027\u56de\u5f52\ndef lwlr(testPoint, xArr, yArr, k=1.0):\n    '''\n        Description: \n            \u5c40\u90e8\u52a0\u6743\u7ebf\u6027\u56de\u5f52\uff0c\u5728\u5f85\u9884\u6d4b\u70b9\u9644\u8fd1\u7684\u6bcf\u4e2a\u70b9\u8d4b\u4e88\u4e00\u5b9a\u7684\u6743\u91cd\uff0c\u5728\u5b50\u96c6\u4e0a\u57fa\u4e8e\u6700\u5c0f\u5747\u65b9\u5dee\u6765\u8fdb\u884c\u666e\u901a\u7684\u56de\u5f52\u3002\n        Args: \n            testPoint: \u6837\u672c\u70b9\n            xArr: \u6837\u672c\u7684\u7279\u5f81\u6570\u636e\uff0c\u5373 feature\n            yArr: \u6bcf\u4e2a\u6837\u672c\u5bf9\u5e94\u7684\u7c7b\u522b\u6807\u7b7e\uff0c\u5373\u76ee\u6807\u53d8\u91cf\n            k:\u5173\u4e8e\u8d4b\u4e88\u6743\u91cd\u77e9\u9635\u7684\u6838\u7684\u4e00\u4e2a\u53c2\u6570\uff0c\u4e0e\u6743\u91cd\u7684\u8870\u51cf\u901f\u7387\u6709\u5173\n        Returns:\n            testPoint * ws: \u6570\u636e\u70b9\u4e0e\u5177\u6709\u6743\u91cd\u7684\u7cfb\u6570\u76f8\u4e58\u5f97\u5230\u7684\u9884\u6d4b\u70b9\n        Notes:\n            \u8fd9\u5176\u4e2d\u4f1a\u7528\u5230\u8ba1\u7b97\u6743\u91cd\u7684\u516c\u5f0f\uff0cw = e^((x^((i))-x) / -2k^2)\n            \u7406\u89e3: x\u4e3a\u67d0\u4e2a\u9884\u6d4b\u70b9\uff0cx^((i))\u4e3a\u6837\u672c\u70b9\uff0c\u6837\u672c\u70b9\u8ddd\u79bb\u9884\u6d4b\u70b9\u8d8a\u8fd1\uff0c\u8d21\u732e\u7684\u8bef\u5dee\u8d8a\u5927\uff08\u6743\u503c\u8d8a\u5927\uff09\uff0c\u8d8a\u8fdc\u5219\u8d21\u732e\u7684\u8bef\u5dee\u8d8a\u5c0f\uff08\u6743\u503c\u8d8a\u5c0f\uff09\u3002\n            \u5173\u4e8e\u9884\u6d4b\u70b9\u7684\u9009\u53d6\uff0c\u5728\u6211\u7684\u4ee3\u7801\u4e2d\u53d6\u7684\u662f\u6837\u672c\u70b9\u3002\u5176\u4e2dk\u662f\u5e26\u5bbd\u53c2\u6570\uff0c\u63a7\u5236w\uff08\u949f\u5f62\u51fd\u6570\uff09\u7684\u5bbd\u7a84\u7a0b\u5ea6\uff0c\u7c7b\u4f3c\u4e8e\u9ad8\u65af\u51fd\u6570\u7684\u6807\u51c6\u5dee\u3002\n            \u7b97\u6cd5\u601d\u8def: \u5047\u8bbe\u9884\u6d4b\u70b9\u53d6\u6837\u672c\u70b9\u4e2d\u7684\u7b2ci\u4e2a\u6837\u672c\u70b9\uff08\u5171m\u4e2a\u6837\u672c\u70b9\uff09\uff0c\u904d\u53861\u5230m\u4e2a\u6837\u672c\u70b9\uff08\u542b\u7b2ci\u4e2a\uff09\uff0c\u7b97\u51fa\u6bcf\u4e00\u4e2a\u6837\u672c\u70b9\u4e0e\u9884\u6d4b\u70b9\u7684\u8ddd\u79bb\uff0c\n            \u4e5f\u5c31\u53ef\u4ee5\u8ba1\u7b97\u51fa\u6bcf\u4e2a\u6837\u672c\u8d21\u732e\u8bef\u5dee\u7684\u6743\u503c\uff0c\u53ef\u4ee5\u770b\u51faw\u662f\u4e00\u4e2a\u6709m\u4e2a\u5143\u7d20\u7684\u5411\u91cf\uff08\u5199\u6210\u5bf9\u89d2\u9635\u5f62\u5f0f\uff09\u3002\n    '''\n    # mat() \u51fd\u6570\u662f\u5c06array\u8f6c\u6362\u4e3a\u77e9\u9635\u7684\u51fd\u6570\uff0c mat().T \u662f\u8f6c\u6362\u4e3a\u77e9\u9635\u4e4b\u540e\uff0c\u518d\u8fdb\u884c\u8f6c\u7f6e\u64cd\u4f5c\n    xMat = mat(xArr)\n    yMat = mat(yArr).T\n    # \u83b7\u5f97xMat\u77e9\u9635\u7684\u884c\u6570\n    m = shape(xMat)[0]\n    # eye()\u8fd4\u56de\u4e00\u4e2a\u5bf9\u89d2\u7ebf\u5143\u7d20\u4e3a1\uff0c\u5176\u4ed6\u5143\u7d20\u4e3a0\u7684\u4e8c\u7ef4\u6570\u7ec4\uff0c\u521b\u5efa\u6743\u91cd\u77e9\u9635weights\uff0c\u8be5\u77e9\u9635\u4e3a\u6bcf\u4e2a\u6837\u672c\u70b9\u521d\u59cb\u5316\u4e86\u4e00\u4e2a\u6743\u91cd                   \n    weights = mat(eye((m)))\n    for j in range(m):\n        # testPoint \u7684\u5f62\u5f0f\u662f \u4e00\u4e2a\u884c\u5411\u91cf\u7684\u5f62\u5f0f\n        # \u8ba1\u7b97 testPoint \u4e0e\u8f93\u5165\u6837\u672c\u70b9\u4e4b\u95f4\u7684\u8ddd\u79bb\uff0c\u7136\u540e\u4e0b\u9762\u8ba1\u7b97\u51fa\u6bcf\u4e2a\u6837\u672c\u8d21\u732e\u8bef\u5dee\u7684\u6743\u503c\n        diffMat = testPoint - xMat[j, :]\n        # k\u63a7\u5236\u8870\u51cf\u7684\u901f\u5ea6\n        weights[j, j] = exp(diffMat * diffMat.T / (-2.0 * k**2))\n    # \u6839\u636e\u77e9\u9635\u4e58\u6cd5\u8ba1\u7b97 xTx \uff0c\u5176\u4e2d\u7684 weights \u77e9\u9635\u662f\u6837\u672c\u70b9\u5bf9\u5e94\u7684\u6743\u91cd\u77e9\u9635\n    xTx = xMat.T * (weights * xMat)\n    if linalg.det(xTx) == 0.0:\n        print(\"This matrix is singular, cannot do inverse\")\n        return\n    # \u8ba1\u7b97\u51fa\u56de\u5f52\u7cfb\u6570\u7684\u4e00\u4e2a\u4f30\u8ba1\n    ws = xTx.I * (xMat.T * (weights * yMat))\n    return testPoint * ws\n\n\ndef lwlrTest(testArr, xArr, yArr, k=1.0):\n    '''\n        Description: \n            \u6d4b\u8bd5\u5c40\u90e8\u52a0\u6743\u7ebf\u6027\u56de\u5f52\uff0c\u5bf9\u6570\u636e\u96c6\u4e2d\u6bcf\u4e2a\u70b9\u8c03\u7528 lwlr() \u51fd\u6570\n        Args: \n            testArr: \u6d4b\u8bd5\u6240\u7528\u7684\u6240\u6709\u6837\u672c\u70b9\n            xArr: \u6837\u672c\u7684\u7279\u5f81\u6570\u636e\uff0c\u5373 feature\n            yArr: \u6bcf\u4e2a\u6837\u672c\u5bf9\u5e94\u7684\u7c7b\u522b\u6807\u7b7e\uff0c\u5373\u76ee\u6807\u53d8\u91cf\n            k: \u63a7\u5236\u6838\u51fd\u6570\u7684\u8870\u51cf\u901f\u7387\n        Returns: \n            yHat: \u9884\u6d4b\u70b9\u7684\u4f30\u8ba1\u503c\n    '''\n    # \u5f97\u5230\u6837\u672c\u70b9\u7684\u603b\u6570\n    m = shape(testArr)[0]\n    # \u6784\u5efa\u4e00\u4e2a\u5168\u90e8\u90fd\u662f 0 \u7684 1 * m \u7684\u77e9\u9635\n    yHat = zeros(m)\n    # \u5faa\u73af\u6240\u6709\u7684\u6570\u636e\u70b9\uff0c\u5e76\u5c06lwlr\u8fd0\u7528\u4e8e\u6240\u6709\u7684\u6570\u636e\u70b9 \n    for i in range(m):\n        yHat[i] = lwlr(testArr[i], xArr, yArr, k)\n    # \u8fd4\u56de\u4f30\u8ba1\u503c\n    return yHat\n\n\ndef lwlrTestPlot(xArr, yArr, k=1.0):\n    '''\n        Description:\n            \u9996\u5148\u5c06 X \u6392\u5e8f\uff0c\u5176\u4f59\u7684\u90fd\u4e0elwlrTest\u76f8\u540c\uff0c\u8fd9\u6837\u66f4\u5bb9\u6613\u7ed8\u56fe\n        Args: \n            xArr: \u6837\u672c\u7684\u7279\u5f81\u6570\u636e\uff0c\u5373 feature\n            yArr: \u6bcf\u4e2a\u6837\u672c\u5bf9\u5e94\u7684\u7c7b\u522b\u6807\u7b7e\uff0c\u5373\u76ee\u6807\u53d8\u91cf\uff0c\u5b9e\u9645\u503c\n            k: \u63a7\u5236\u6838\u51fd\u6570\u7684\u8870\u51cf\u901f\u7387\u7684\u6709\u5173\u53c2\u6570\uff0c\u8fd9\u91cc\u8bbe\u5b9a\u7684\u662f\u5e38\u91cf\u503c 1\n        Return: \n            yHat: \u6837\u672c\u70b9\u7684\u4f30\u8ba1\u503c\n            xCopy: xArr\u7684\u590d\u5236\n    '''\n    # \u751f\u6210\u4e00\u4e2a\u4e0e\u76ee\u6807\u53d8\u91cf\u6570\u76ee\u76f8\u540c\u7684 0 \u5411\u91cf\n    yHat = zeros(shape(yArr))\n    # \u5c06 xArr \u8f6c\u6362\u4e3a \u77e9\u9635\u5f62\u5f0f\n    xCopy = mat(xArr)\n    # \u6392\u5e8f\n    xCopy.sort(0)\n    # \u5f00\u59cb\u5faa\u73af\uff0c\u4e3a\u6bcf\u4e2a\u6837\u672c\u70b9\u8fdb\u884c\u5c40\u90e8\u52a0\u6743\u7ebf\u6027\u56de\u5f52\uff0c\u5f97\u5230\u6700\u7ec8\u7684\u76ee\u6807\u53d8\u91cf\u4f30\u8ba1\u503c\n    for i in range(shape(xArr)[0]):\n        yHat[i] = lwlr(xCopy[i], xArr, yArr, k)\n    return yHat, xCopy\n\n\ndef rssError(yArr, yHatArr):\n    '''\n        Desc:\n            \u8ba1\u7b97\u5206\u6790\u9884\u6d4b\u8bef\u5dee\u7684\u5927\u5c0f\n        Args:\n            yArr: \u771f\u5b9e\u7684\u76ee\u6807\u53d8\u91cf\n            yHatArr: \u9884\u6d4b\u5f97\u5230\u7684\u4f30\u8ba1\u503c\n        Returns:\n            \u8ba1\u7b97\u771f\u5b9e\u503c\u548c\u4f30\u8ba1\u503c\u5f97\u5230\u7684\u503c\u7684\u5e73\u65b9\u548c\u4f5c\u4e3a\u6700\u540e\u7684\u8fd4\u56de\u503c\n    '''\n    return ((yArr - yHatArr)**2).sum()\n\n\ndef ridgeRegres(xMat, yMat, lam=0.2):\n    '''\n        Desc: \n            \u8fd9\u4e2a\u51fd\u6570\u5b9e\u73b0\u4e86\u7ed9\u5b9a lambda \u4e0b\u7684\u5cad\u56de\u5f52\u6c42\u89e3\u3002\n            \u5982\u679c\u6570\u636e\u7684\u7279\u5f81\u6bd4\u6837\u672c\u70b9\u8fd8\u591a\uff0c\u5c31\u4e0d\u80fd\u518d\u4f7f\u7528\u4e0a\u9762\u4ecb\u7ecd\u7684\u7684\u7ebf\u6027\u56de\u5f52\u548c\u5c40\u90e8\u7ebf\u6027\u56de\u5f52\u4e86\uff0c\u56e0\u4e3a\u8ba1\u7b97 (xTx)^(-1)\u4f1a\u51fa\u73b0\u9519\u8bef\u3002\n            \u5982\u679c\u7279\u5f81\u6bd4\u6837\u672c\u70b9\u8fd8\u591a\uff08n > m\uff09\uff0c\u4e5f\u5c31\u662f\u8bf4\uff0c\u8f93\u5165\u6570\u636e\u7684\u77e9\u9635x\u4e0d\u662f\u6ee1\u79e9\u77e9\u9635\u3002\u975e\u6ee1\u79e9\u77e9\u9635\u5728\u6c42\u9006\u65f6\u4f1a\u51fa\u73b0\u95ee\u9898\u3002\n            \u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u4e0b\u8fb9\u8bb2\u4e00\u4e0b: \u5cad\u56de\u5f52\uff0c\u8fd9\u662f\u6211\u4eec\u8981\u8bb2\u7684\u7b2c\u4e00\u79cd\u7f29\u51cf\u65b9\u6cd5\u3002\n        Args: \n            xMat: \u6837\u672c\u7684\u7279\u5f81\u6570\u636e\uff0c\u5373 feature\n            yMat: \u6bcf\u4e2a\u6837\u672c\u5bf9\u5e94\u7684\u7c7b\u522b\u6807\u7b7e\uff0c\u5373\u76ee\u6807\u53d8\u91cf\uff0c\u5b9e\u9645\u503c\n            lam: \u5f15\u5165\u7684\u4e00\u4e2a\u03bb\u503c\uff0c\u4f7f\u5f97\u77e9\u9635\u975e\u5947\u5f02\n        Returns: \n            \u7ecf\u8fc7\u5cad\u56de\u5f52\u516c\u5f0f\u8ba1\u7b97\u5f97\u5230\u7684\u56de\u5f52\u7cfb\u6570\n    '''\n\n    xTx = xMat.T * xMat\n    # \u5cad\u56de\u5f52\u5c31\u662f\u5728\u77e9\u9635 xTx \u4e0a\u52a0\u4e00\u4e2a \u03bbI \u4ece\u800c\u4f7f\u5f97\u77e9\u9635\u975e\u5947\u5f02\uff0c\u8fdb\u800c\u80fd\u5bf9 xTx + \u03bbI \u6c42\u9006\n    denom = xTx + eye(shape(xMat)[1]) * lam\n    # \u68c0\u67e5\u884c\u5217\u5f0f\u662f\u5426\u4e3a\u96f6\uff0c\u5373\u77e9\u9635\u662f\u5426\u53ef\u9006\uff0c\u884c\u5217\u5f0f\u4e3a0\u7684\u8bdd\u5c31\u4e0d\u53ef\u9006\uff0c\u4e0d\u4e3a0\u7684\u8bdd\u5c31\u662f\u53ef\u9006\u3002\n    if linalg.det(denom) == 0.0:\n        print(\"This matrix is singular, cannot do inverse\")\n        return\n    ws = denom.I * (xMat.T * yMat)\n    return ws\n\n\ndef ridgeTest(xArr, yArr):\n    '''\n        Desc: \n            \u51fd\u6570 ridgeTest() \u7528\u4e8e\u5728\u4e00\u7ec4 \u03bb \u4e0a\u6d4b\u8bd5\u7ed3\u679c\n        Args: \n            xArr: \u6837\u672c\u6570\u636e\u7684\u7279\u5f81\uff0c\u5373 feature\n            yArr: \u6837\u672c\u6570\u636e\u7684\u7c7b\u522b\u6807\u7b7e\uff0c\u5373\u771f\u5b9e\u6570\u636e\n        Returns: \n            wMat: \u5c06\u6240\u6709\u7684\u56de\u5f52\u7cfb\u6570\u8f93\u51fa\u5230\u4e00\u4e2a\u77e9\u9635\u5e76\u8fd4\u56de\n    '''\n\n    xMat = mat(xArr)\n    yMat = mat(yArr).T\n    # \u8ba1\u7b97Y\u7684\u5747\u503c\n    yMean = mean(yMat, 0)\n    # Y\u7684\u6240\u6709\u7684\u7279\u5f81\u51cf\u53bb\u5747\u503c\n    yMat = yMat - yMean\n    # \u6807\u51c6\u5316 x\uff0c\u8ba1\u7b97 xMat \u5e73\u5747\u503c\n    xMeans = mean(xMat, 0)\n    # \u7136\u540e\u8ba1\u7b97 X\u7684\u65b9\u5dee\n    xVar = var(xMat, 0)\n    # \u6240\u6709\u7279\u5f81\u90fd\u51cf\u53bb\u5404\u81ea\u7684\u5747\u503c\u5e76\u9664\u4ee5\u65b9\u5dee\n    xMat = (xMat - xMeans) / xVar\n    # \u53ef\u4ee5\u5728 30 \u4e2a\u4e0d\u540c\u7684 lambda \u4e0b\u8c03\u7528 ridgeRegres() \u51fd\u6570\u3002\n    numTestPts = 30\n    # \u521b\u5efa30 * m \u7684\u5168\u90e8\u6570\u636e\u4e3a0 \u7684\u77e9\u9635\n    wMat = zeros((numTestPts, shape(xMat)[1]))\n    for i in range(numTestPts):\n        # exp() \u8fd4\u56de e^x \n        ws = ridgeRegres(xMat, yMat, exp(i - 10))\n        wMat[i, :] = ws.T\n    return wMat\n\n\ndef regularize(xMat):  # \u6309\u5217\u8fdb\u884c\u89c4\u8303\u5316\n    inMat = xMat.copy()\n    inMeans = mean(inMat, 0)  # \u8ba1\u7b97\u5e73\u5747\u503c\u7136\u540e\u51cf\u53bb\u5b83\n    inVar = var(inMat, 0)  # \u8ba1\u7b97\u9664\u4ee5Xi\u7684\u65b9\u5dee\n    inMat = (inMat - inMeans) / inVar\n    return inMat\n\n\ndef stageWise(xArr, yArr, eps=0.01, numIt=100):\n    xMat = mat(xArr)\n    yMat = mat(yArr).T\n    yMean = mean(yMat, 0)\n    yMat = yMat - yMean  # \u4e5f\u53ef\u4ee5\u89c4\u5219\u5316ys\u4f46\u4f1a\u5f97\u5230\u66f4\u5c0f\u7684coef\n    xMat = regularize(xMat)\n    m, n = shape(xMat)\n    # returnMat = zeros((numIt,n)) # \u6d4b\u8bd5\u4ee3\u7801\u5220\u9664\n    ws = zeros((n, 1))\n    wsTest = ws.copy()\n    wsMax = ws.copy()\n    for i in range(numIt):\n        print(ws.T)\n        lowestError = inf\n        for j in range(n):\n            for sign in [-1, 1]:\n                wsTest = ws.copy()\n                wsTest[j] += eps * sign\n                yTest = xMat * wsTest\n                rssE = rssError(yMat.A, yTest.A)\n                if rssE < lowestError:\n                    lowestError = rssE\n                    wsMax = wsTest\n        ws = wsMax.copy()\n        # returnMat[i,:]=ws.T\n    # return returnMat\n\n    # def scrapePage(inFile,outFile,yr,numPce,origPrc):\n    #    from BeautifulSoup import BeautifulSoup\n    #    fr = open(inFile); fw=open(outFile,'a') #a is append mode writing\n    #    soup = BeautifulSoup(fr.read())\n    #    i=1\n    #    currentRow = soup.findAll('table', r=\"%d\" % i)\n    #    while(len(currentRow)!=0):\n    #        title = currentRow[0].findAll('a')[1].text\n    #        lwrTitle = title.lower()\n    #        if (lwrTitle.find('new') > -1) or (lwrTitle.find('nisb') > -1):\n    #            newFlag = 1.0\n    #        else:\n    #            newFlag = 0.0\n    #        soldUnicde = currentRow[0].findAll('td')[3].findAll('span')\n    #        if len(soldUnicde)==0:\n    #            print \"item #%d did not sell\" % i\n    #        else:\n    #            soldPrice = currentRow[0].findAll('td')[4]\n    #            priceStr = soldPrice.text\n    #            priceStr = priceStr.replace('$','') #strips out $\n    #            priceStr = priceStr.replace(',','') #strips out ,\n    #            if len(soldPrice)>1:\n    #                priceStr = priceStr.replace('Free shipping', '') #strips out Free Shipping\n    #            print \"%s\\t%d\\t%s\" % (priceStr,newFlag,title)\n    #            fw.write(\"%d\\t%d\\t%d\\t%f\\t%s\\n\" % (yr,numPce,newFlag,origPrc,priceStr))\n    #        i += 1\n    #        currentRow = soup.findAll('table', r=\"%d\" % i)\n    #    fw.close()\n\n    # --------------------------------------------------------------\n    # \u9884\u6d4b\u4e50\u9ad8\u73a9\u5177\u5957\u88c5\u7684\u4ef7\u683c ------ \u6700\u521d\u7684\u7248\u672c\uff0c\u56e0\u4e3a\u73b0\u5728 google \u7684 api \u53d8\u5316\uff0c\u65e0\u6cd5\u83b7\u53d6\u6570\u636e\n    # \u6545\u6539\u4e3a\u4e86\u4e0b\u8fb9\u7684\u6837\u5b50\uff0c\u4f46\u662f\u9700\u8981\u5b89\u88c5\u4e00\u4e2a beautifulSoup \u8fd9\u4e2a\u7b2c\u4e09\u65b9\u722c\u866b\u5e93\uff0c\u5b89\u88c5\u5f88\u7b80\u5355\uff0c\u89c1\u4e0b\u8fb9\n\n\n'''\nfrom time import sleep\nimport json\nimport urllib2\ndef searchForSet(retX, retY, setNum, yr, numPce, origPrc):\n    sleep(10)\n    myAPIstr = 'AIzaSyD2cR2KFyx12hXu6PFU-wrWot3NXvko8vY'\n    searchURL = 'https://www.googleapis.com/shopping/search/v1/public/products?key=%s&country=US&q=lego+%d&alt=json' % (myAPIstr, setNum)\n    pg = urllib2.urlopen(searchURL)\n    retDict = json.loads(pg.read())\n    for i in range(len(retDict['items'])):\n        try:\n            currItem = retDict['items'][i]\n            if currItem['product']['condition'] == 'new':\n                newFlag = 1\n            else: newFlag = 0\n            listOfInv = currItem['product']['inventories']\n            for item in listOfInv:\n                sellingPrice = item['price']\n                if  sellingPrice > origPrc * 0.5:\n                    print (\"%d\\t%d\\t%d\\t%f\\t%f\" % (yr,numPce,newFlag,origPrc, sellingPrice))\n                    retX.append([yr, numPce, newFlag, origPrc])\n                    retY.append(sellingPrice)\n        except: print ('problem with item %d' % i)\n\ndef setDataCollect(retX, retY):\n    searchForSet(retX, retY, 8288, 2006, 800, 49.99)\n    searchForSet(retX, retY, 10030, 2002, 3096, 269.99)\n    searchForSet(retX, retY, 10179, 2007, 5195, 499.99)\n    searchForSet(retX, retY, 10181, 2007, 3428, 199.99)\n    searchForSet(retX, retY, 10189, 2008, 5922, 299.99)\n    searchForSet(retX, retY, 10196, 2009, 3263, 249.99)\n\ndef crossValidation(xArr,yArr,numVal=10):\n    m = len(yArr)                           \n    indexList = range(m)\n    errorMat = zeros((numVal,30))#create error mat 30columns numVal rows\u521b\u5efaerror mat 30columns numVal \u884c\n    for i in range(numVal):\n        trainX=[]; trainY=[]\n        testX = []; testY = []\n        random.shuffle(indexList)\n        for j in range(m):#create training set based on first 90% of values in indexList\n                          #\u57fa\u4e8eindexList\u4e2d\u7684\u524d90%\u7684\u503c\u521b\u5efa\u8bad\u7ec3\u96c6\n            if j < m*0.9: \n                trainX.append(xArr[indexList[j]])\ngt56                trainY.append(yArr[indexList[j]])\n            else:\n                testX.append(xArr[indexList[j]])\n                testY.append(yArr[indexList[j]])\n        wMat = ridgeTest(trainX,trainY)    #get 30 weight vectors from ridge\n        for k in range(30):#loop over all of the ridge estimates\n            matTestX = mat(testX); matTrainX=mat(trainX)\n            meanTrain = mean(matTrainX,0)\n            varTrain = var(matTrainX,0)\n            matTestX = (matTestX-meanTrain)/varTrain #regularize test with training params\n            yEst = matTestX * mat(wMat[k,:]).T + mean(trainY)#test ridge results and store\n            errorMat[i,k]=rssError(yEst.T.A,array(testY))\n            #print errorMat[i,k]\n    meanErrors = mean(errorMat,0)#calc avg performance of the different ridge weight vectors\n    minMean = float(min(meanErrors))\n    bestWeights = wMat[nonzero(meanErrors==minMean)]\n    #can unregularize to get model\n    #when we regularized we wrote Xreg = (x-meanX)/var(x)\n    #we can now write in terms of x not Xreg:  x*w/var(x) - meanX/var(x) +meanY\n    xMat = mat(xArr); yMat=mat(yArr).T\n    meanX = mean(xMat,0); varX = var(xMat,0)\n    unReg = bestWeights/varX\n    print (\"the best model from Ridge Regression is:\\n\",unReg)\n    print (\"with constant term: \",-1*sum(multiply(meanX,unReg)) + mean(yMat))\n'''\n\n# ----------------------------------------------------------------------------\n# \u9884\u6d4b\u4e50\u9ad8\u73a9\u5177\u5957\u88c5\u7684\u4ef7\u683c \u53ef\u8fd0\u884c\u7248\u672c\uff0c\u6211\u4eec\u628a\u4e50\u9ad8\u6570\u636e\u5b58\u50a8\u5230\u4e86\u6211\u4eec\u7684 input \u6587\u4ef6\u5939\u4e0b\uff0c\u4f7f\u7528 beautifulSoup \u722c\u53bb\u4e00\u4e0b\u5185\u5bb9\n# \u524d\u63d0: \u5b89\u88c5 BeautifulSoup \u7b2c\u4e09\u65b9\u722c\u866b\u5e93\uff0c\u6b65\u9aa4\u5982\u4e0b\n# \u5728\u8fd9\u4e2a\u9875\u9762 https://www.crummy.com/software/BeautifulSoup/bs4/download/4.4/ \u4e0b\u8f7d\uff0cbeautifulsoup4-4.4.1.tar.gz \n# \u5c06\u4e0b\u8f7d\u6587\u4ef6\u89e3\u538b\uff0c\u4f7f\u7528 windows \u7248\u672c\u7684 cmd \u547d\u4ee4\u884c\uff0c\u8fdb\u5165\u89e3\u538b\u7684\u5305\uff0c\u8f93\u5165\u4ee5\u4e0b\u4e24\u884c\u547d\u4ee4\u5373\u53ef\u5b8c\u6210\u5b89\u88c5\n# python setup.py build\n# python setup.py install \n'''\nfrom numpy import *\nfrom bs4 import BeautifulSoup\n\n# \u4ece\u9875\u9762\u8bfb\u53d6\u6570\u636e\uff0c\u751f\u6210retX\u548cretY\u5217\u8868\ndef scrapePage(retX, retY, inFile, yr, numPce, origPrc):\n\n    # \u6253\u5f00\u5e76\u8bfb\u53d6HTML\u6587\u4ef6\n    fr = open(inFile)\n    soup = BeautifulSoup(fr.read())\n    i=1\n\n    # \u6839\u636eHTML\u9875\u9762\u7ed3\u6784\u8fdb\u884c\u89e3\u6790\n    currentRow = soup.findAll('table', r=\"%d\" % i)\n    while(len(currentRow)!=0):\n        currentRow = soup.findAll('table', r=\"%d\" % i)\n        title = currentRow[0].findAll('a')[1].text\n        lwrTitle = title.lower()\n\n        # \u67e5\u627e\u662f\u5426\u6709\u5168\u65b0\u6807\u7b7e\n        if (lwrTitle.find('new') > -1) or (lwrTitle.find('nisb') > -1):\n            newFlag = 1.0\n        else:\n            newFlag = 0.0\n\n        # \u67e5\u627e\u662f\u5426\u5df2\u7ecf\u6807\u5fd7\u51fa\u552e\uff0c\u6211\u4eec\u53ea\u6536\u96c6\u5df2\u51fa\u552e\u7684\u6570\u636e\n        soldUnicde = currentRow[0].findAll('td')[3].findAll('span')\n        if len(soldUnicde)==0:\n            print \"item #%d did not sell\" % i\n        else:\n            # \u89e3\u6790\u9875\u9762\u83b7\u53d6\u5f53\u524d\u4ef7\u683c\n            soldPrice = currentRow[0].findAll('td')[4]\n            priceStr = soldPrice.text\n            priceStr = priceStr.replace('$','') #strips out $\n            priceStr = priceStr.replace(',','') #strips out ,\n            if len(soldPrice)>1:\n                priceStr = priceStr.replace('Free shipping', '')\n            sellingPrice = float(priceStr)\n\n            # \u53bb\u6389\u4e0d\u5b8c\u6574\u7684\u5957\u88c5\u4ef7\u683c\n            if  sellingPrice > origPrc * 0.5:\n                    print \"%d\\t%d\\t%d\\t%f\\t%f\" % (yr,numPce,newFlag,origPrc, sellingPrice)\n                    retX.append([yr, numPce, newFlag, origPrc])\n                    retY.append(sellingPrice)\n        i += 1\n        currentRow = soup.findAll('table', r=\"%d\" % i)\n\n# \u4f9d\u6b21\u8bfb\u53d6\u516d\u79cd\u4e50\u9ad8\u5957\u88c5\u7684\u6570\u636e\uff0c\u5e76\u751f\u6210\u6570\u636e\u77e9\u9635        \ndef setDataCollect(retX, retY):\n    scrapePage(retX, retY, 'data/8.Regression/setHtml/lego8288.html', 2006, 800, 49.99)\n    scrapePage(retX, retY, 'data/8.Regression/setHtml/lego10030.html', 2002, 3096, 269.99)\n    scrapePage(retX, retY, 'data/8.Regression/setHtml/lego10179.html', 2007, 5195, 499.99)\n    scrapePage(retX, retY, 'data/8.Regression/setHtml/lego10181.html', 2007, 3428, 199.99)\n    scrapePage(retX, retY, 'data/8.Regression/setHtml/lego10189.html', 2008, 5922, 299.99)\n    scrapePage(retX, retY, 'data/8.Regression/setHtml/lego10196.html', 2009, 3263, 249.99)\n\n\n# \u4ea4\u53c9\u9a8c\u8bc1\u6d4b\u8bd5\u5cad\u56de\u5f52\ndef crossValidation(xArr,yArr,numVal=10):\n    # \u83b7\u5f97\u6570\u636e\u70b9\u4e2a\u6570\uff0cxArr\u548cyArr\u5177\u6709\u76f8\u540c\u957f\u5ea6\n    m = len(yArr)\n    indexList = range(m)\n    errorMat = zeros((numVal,30))\n\n    # \u4e3b\u5faa\u73af \u4ea4\u53c9\u9a8c\u8bc1\u5faa\u73af\n    for i in range(numVal):\n        # \u968f\u673a\u62c6\u5206\u6570\u636e\uff0c\u5c06\u6570\u636e\u5206\u4e3a\u8bad\u7ec3\u96c6\uff0890%\uff09\u548c\u6d4b\u8bd5\u96c6\uff0810%\uff09\n        trainX=[]; trainY=[]\n        testX = []; testY = []\n\n        # \u5bf9\u6570\u636e\u8fdb\u884c\u6df7\u6d17\u64cd\u4f5c\n        random.shuffle(indexList)\n\n        # \u5207\u5206\u8bad\u7ec3\u96c6\u548c\u6d4b\u8bd5\u96c6\n        for j in range(m):\n            if j < m*0.9: \n                trainX.append(xArr[indexList[j]])\n                trainY.append(yArr[indexList[j]])\n            else:\n                testX.append(xArr[indexList[j]])\n                testY.append(yArr[indexList[j]])\n\n        # \u83b7\u5f97\u56de\u5f52\u7cfb\u6570\u77e9\u9635\n        wMat = ridgeTest(trainX,trainY)\n\n        # \u5faa\u73af\u904d\u5386\u77e9\u9635\u4e2d\u768430\u7ec4\u56de\u5f52\u7cfb\u6570\n        for k in range(30):\n            # \u8bfb\u53d6\u8bad\u7ec3\u96c6\u548c\u6570\u636e\u96c6\n            matTestX = mat(testX); matTrainX=mat(trainX)\n            # \u5bf9\u6570\u636e\u8fdb\u884c\u6807\u51c6\u5316\n            meanTrain = mean(matTrainX,0)\n            varTrain = var(matTrainX,0)\n            matTestX = (matTestX-meanTrain)/varTrain\n\n            # \u6d4b\u8bd5\u56de\u5f52\u6548\u679c\u5e76\u5b58\u50a8\n            yEst = matTestX * mat(wMat[k,:]).T + mean(trainY)\n\n            # \u8ba1\u7b97\u8bef\u5dee\n            errorMat[i,k] = ((yEst.T.A-array(testY))**2).sum()\n\n    # \u8ba1\u7b97\u8bef\u5dee\u4f30\u8ba1\u503c\u7684\u5747\u503c\n    meanErrors = mean(errorMat,0)\n    minMean = float(min(meanErrors))\n    bestWeights = wMat[nonzero(meanErrors==minMean)]\n\n    # \u4e0d\u8981\u4f7f\u7528\u6807\u51c6\u5316\u7684\u6570\u636e\uff0c\u9700\u8981\u5bf9\u6570\u636e\u8fdb\u884c\u8fd8\u539f\u6765\u5f97\u5230\u8f93\u51fa\u7ed3\u679c\n    xMat = mat(xArr); yMat=mat(yArr).T\n    meanX = mean(xMat,0); varX = var(xMat,0)\n    unReg = bestWeights/varX\n\n    # \u8f93\u51fa\u6784\u5efa\u7684\u6a21\u578b\n    print \"the best model from Ridge Regression is:\\n\",unReg\n    print \"with constant term: \",-1*sum(multiply(meanX,unReg)) + mean(yMat)\n'''\n\n\n# test for standRegression\ndef regression1():\n    xArr, yArr = loadDataSet(\"data/8.Regression/data.txt\")\n    xMat = mat(xArr)\n    yMat = mat(yArr)\n    ws = standRegres(xArr, yArr)\n    fig = plt.figure()\n    ax = fig.add_subplot(\n        111)  # add_subplot(349)\u51fd\u6570\u7684\u53c2\u6570\u7684\u610f\u601d\u662f\uff0c\u5c06\u753b\u5e03\u5206\u62103\u884c4\u5217\u56fe\u50cf\u753b\u5728\u4ece\u5de6\u5230\u53f3\u4ece\u4e0a\u5230\u4e0b\u7b2c9\u5757\n    ax.scatter(\n        [xMat[:, 1].flatten()],\n        [yMat.T[:, 0].flatten().A[0]])  # scatter \u7684x\u662fxMat\u4e2d\u7684\u7b2c\u4e8c\u5217\uff0cy\u662fyMat\u7684\u7b2c\u4e00\u5217\n    xCopy = xMat.copy()\n    xCopy.sort(0)\n    yHat = xCopy * ws\n    ax.plot(xCopy[:, 1], yHat)\n    plt.show()\n\n\n# test for LWLR\ndef regression2():\n    xArr, yArr = loadDataSet(\"data/8.Regression/data.txt\")\n    yHat = lwlrTest(xArr, xArr, yArr, 0.003)\n    xMat = mat(xArr)\n    srtInd = xMat[:, 1].argsort(\n        0)  #argsort()\u51fd\u6570\u662f\u5c06x\u4e2d\u7684\u5143\u7d20\u4ece\u5c0f\u5230\u5927\u6392\u5217\uff0c\u63d0\u53d6\u5176\u5bf9\u5e94\u7684index(\u7d22\u5f15)\uff0c\u7136\u540e\u8f93\u51fa\n    xSort = xMat[srtInd][:, 0, :]\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    ax.plot(xSort[:, 1], yHat[srtInd])\n    ax.scatter(\n        [xMat[:, 1].flatten().A[0]], [mat(yArr).T.flatten().A[0]],\n        s=2,\n        c='red')\n    plt.show()\n\n\n# test for abloneDataSet\ndef abaloneTest():\n    '''\n    Desc:\n        \u9884\u6d4b\u9c8d\u9c7c\u7684\u5e74\u9f84\n    Args:\n        None\n    Returns:\n        None\n    '''\n    # \u52a0\u8f7d\u6570\u636e\n    abX, abY = loadDataSet(\"data/8.Regression/abalone.txt\")\n    # \u4f7f\u7528\u4e0d\u540c\u7684\u6838\u8fdb\u884c\u9884\u6d4b\n    oldyHat01 = lwlrTest(abX[0:99], abX[0:99], abY[0:99], 0.1)\n    oldyHat1 = lwlrTest(abX[0:99], abX[0:99], abY[0:99], 1)\n    oldyHat10 = lwlrTest(abX[0:99], abX[0:99], abY[0:99], 10)\n    # \u6253\u5370\u51fa\u4e0d\u540c\u7684\u6838\u9884\u6d4b\u503c\u4e0e\u8bad\u7ec3\u6570\u636e\u96c6\u4e0a\u7684\u771f\u5b9e\u503c\u4e4b\u95f4\u7684\u8bef\u5dee\u5927\u5c0f\n    print((\"old yHat01 error Size is :\", rssError(abY[0:99], oldyHat01.T)))\n    print((\"old yHat1 error Size is :\", rssError(abY[0:99], oldyHat1.T)))\n    print((\"old yHat10 error Size is :\", rssError(abY[0:99], oldyHat10.T)))\n\n    # \u6253\u5370\u51fa \u4e0d\u540c\u7684\u6838\u9884\u6d4b\u503c \u4e0e \u65b0\u6570\u636e\u96c6\uff08\u6d4b\u8bd5\u6570\u636e\u96c6\uff09\u4e0a\u7684\u771f\u5b9e\u503c\u4e4b\u95f4\u7684\u8bef\u5dee\u5927\u5c0f\n    newyHat01 = lwlrTest(abX[100:199], abX[0:99], abY[0:99], 0.1)\n    print((\"new yHat01 error Size is :\", rssError(abY[0:99], newyHat01.T)))\n    newyHat1 = lwlrTest(abX[100:199], abX[0:99], abY[0:99], 1)\n    print((\"new yHat1 error Size is :\", rssError(abY[0:99], newyHat1.T)))\n    newyHat10 = lwlrTest(abX[100:199], abX[0:99], abY[0:99], 10)\n    print((\"new yHat10 error Size is :\", rssError(abY[0:99], newyHat10.T)))\n\n    # \u4f7f\u7528\u7b80\u5355\u7684 \u7ebf\u6027\u56de\u5f52 \u8fdb\u884c\u9884\u6d4b\uff0c\u4e0e\u4e0a\u9762\u7684\u8ba1\u7b97\u8fdb\u884c\u6bd4\u8f83\n    standWs = standRegres(abX[0:99], abY[0:99])\n    standyHat = mat(abX[100:199]) * standWs\n    print((\"standRegress error Size is:\", rssError(abY[100:199], standyHat.T.A)))\n\n\n# test for ridgeRegression\ndef regression3():\n    abX, abY = loadDataSet(\"data/8.Regression/abalone.txt\")\n    ridgeWeights = ridgeTest(abX, abY)\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    ax.plot(ridgeWeights)\n    plt.show()\n\n\n# test for stageWise\ndef regression4():\n    xArr, yArr = loadDataSet(\"data/8.Regression/abalone.txt\")\n    stageWise(xArr, yArr, 0.01, 200)\n    xMat = mat(xArr)\n    yMat = mat(yArr).T\n    xMat = regularize(xMat)\n    yM = mean(yMat, 0)\n    yMat = yMat - yM\n    weights = standRegres(xMat, yMat.T)\n    print(weights.T)\n\n\n# predict for lego's price\ndef regression5():\n    lgX = []\n    lgY = []\n\n    setDataCollect(lgX, lgY)\n    crossValidation(lgX, lgY, 10)\n\n\nif __name__ == \"__main__\":\n    regression1()\n    # regression2()\n    # abaloneTest()\n    # regression3()\n    # regression4()\n    # regression5()\n", "src/py2.x/ml/7.AdaBoost/adaboost.py": "#!/usr/bin/python\n# coding:utf8\n\n'''\nCreated on Nov 28, 2010\nUpdate  on 2017-05-18\nAdaboost is short for Adaptive Boosting\nAuthor: Peter/\u7247\u523b\nGitHub: https://github.com/apachecn/AiLearning\n'''\nfrom __future__ import print_function\nfrom numpy import *\n\n\ndef loadSimpData():\n    \"\"\" \u6d4b\u8bd5\u6570\u636e\n    Returns:\n        dataArr   feature\u5bf9\u5e94\u7684\u6570\u636e\u96c6\n        labelArr  feature\u5bf9\u5e94\u7684\u5206\u7c7b\u6807\u7b7e\n    \"\"\"\n    dataArr = array([[1., 2.1], [2., 1.1], [1.3, 1.], [1., 1.], [2., 1.]])\n    labelArr = [1.0, 1.0, -1.0, -1.0, 1.0]\n    return dataArr, labelArr\n\n\n# general function to parse tab -delimited floats\ndef loadDataSet(fileName):\n    # get number of fields\n    numFeat = len(open(fileName).readline().split('\\t'))\n    dataArr = []\n    labelArr = []\n    fr = open(fileName)\n    for line in fr.readlines():\n        lineArr = []\n        curLine = line.strip().split('\\t')\n        for i in range(numFeat-1):\n            lineArr.append(float(curLine[i]))\n        dataArr.append(lineArr)\n        labelArr.append(float(curLine[-1]))\n    return dataArr, labelArr\n\n\ndef stumpClassify(dataMat, dimen, threshVal, threshIneq):\n    \"\"\"stumpClassify(\u5c06\u6570\u636e\u96c6\uff0c\u6309\u7167feature\u5217\u7684value\u8fdb\u884c \u4e8c\u5206\u6cd5\u5207\u5206\u6bd4\u8f83\u6765\u8d4b\u503c\u5206\u7c7b)\n\n    Args:\n        dataMat    Matrix\u6570\u636e\u96c6\n        dimen      \u7279\u5f81\u5217\n        threshVal  \u7279\u5f81\u5217\u8981\u6bd4\u8f83\u7684\u503c\n    Returns:\n        retArray \u7ed3\u679c\u96c6\n    \"\"\"\n    # \u9ed8\u8ba4\u90fd\u662f1\n    retArray = ones((shape(dataMat)[0], 1))\n    # dataMat[:, dimen] \u8868\u793a\u6570\u636e\u96c6\u4e2d\u7b2cdimen\u5217\u7684\u6240\u6709\u503c\n    # threshIneq == 'lt'\u8868\u793a\u4fee\u6539\u5de6\u8fb9\u7684\u503c\uff0cgt\u8868\u793a\u4fee\u6539\u53f3\u8fb9\u7684\u503c\n    # print '-----', threshIneq, dataMat[:, dimen], threshVal\n    if threshIneq == 'lt':\n        retArray[dataMat[:, dimen] <= threshVal] = -1.0\n    else:\n        retArray[dataMat[:, dimen] > threshVal] = -1.0\n    return retArray\n\n\ndef buildStump(dataArr, labelArr, D):\n    \"\"\"buildStump(\u5f97\u5230\u51b3\u7b56\u6811\u7684\u6a21\u578b)\n\n    Args:\n        dataArr   \u7279\u5f81\u6807\u7b7e\u96c6\u5408\n        labelArr  \u5206\u7c7b\u6807\u7b7e\u96c6\u5408\n        D         \u6700\u521d\u7684\u6837\u672c\u7684\u6240\u6709\u7279\u5f81\u6743\u91cd\u96c6\u5408\n    Returns:\n        bestStump    \u6700\u4f18\u7684\u5206\u7c7b\u5668\u6a21\u578b\n        minError     \u9519\u8bef\u7387\n        bestClasEst  \u8bad\u7ec3\u540e\u7684\u7ed3\u679c\u96c6\n    \"\"\"\n    # \u8f6c\u6362\u6570\u636e\n    dataMat = mat(dataArr)\n    labelMat = mat(labelArr).T\n    # m\u884c n\u5217\n    m, n = shape(dataMat)\n\n    # \u521d\u59cb\u5316\u6570\u636e\n    numSteps = 10.0\n    bestStump = {}\n    bestClasEst = mat(zeros((m, 1)))\n    # \u521d\u59cb\u5316\u7684\u6700\u5c0f\u8bef\u5dee\u4e3a\u65e0\u7a77\u5927\n    minError = inf\n\n    # \u5faa\u73af\u6240\u6709\u7684feature\u5217\uff0c\u5c06\u5217\u5207\u5206\u6210 \u82e5\u5e72\u4efd\uff0c\u6bcf\u4e00\u6bb5\u4ee5\u6700\u5de6\u8fb9\u7684\u70b9\u4f5c\u4e3a\u5206\u7c7b\u8282\u70b9\n    for i in range(n):\n        rangeMin = dataMat[:, i].min()\n        rangeMax = dataMat[:, i].max()\n        # print 'rangeMin=%s, rangeMax=%s' % (rangeMin, rangeMax)\n        # \u8ba1\u7b97\u6bcf\u4e00\u4efd\u7684\u5143\u7d20\u4e2a\u6570\n        stepSize = (rangeMax-rangeMin)/numSteps\n        # \u4f8b\u5982:  4=(10-1)/2   \u90a3\u4e48  1-4(-1\u6b21)   1(0\u6b21)  1+1*4(1\u6b21)   1+2*4(2\u6b21)\n        # \u6240\u4ee5:  \u5faa\u73af -1/0/1/2\n        for j in range(-1, int(numSteps)+1):\n            # go over less than and greater than\n            for inequal in ['lt', 'gt']:\n                # \u5982\u679c\u662f-1\uff0c\u90a3\u4e48\u5f97\u5230rangeMin-stepSize; \u5982\u679c\u662fnumSteps\uff0c\u90a3\u4e48\u5f97\u5230rangeMax\n                threshVal = (rangeMin + float(j) * stepSize)\n                # \u5bf9\u5355\u5c42\u51b3\u7b56\u6811\u8fdb\u884c\u7b80\u5355\u5206\u7c7b\uff0c\u5f97\u5230\u9884\u6d4b\u7684\u5206\u7c7b\u503c\n                predictedVals = stumpClassify(dataMat, i, threshVal, inequal)\n                # print predictedVals\n                errArr = mat(ones((m, 1)))\n                # \u6b63\u786e\u4e3a0\uff0c\u9519\u8bef\u4e3a1\n                errArr[predictedVals == labelMat] = 0\n                # \u8ba1\u7b97 \u5e73\u5747\u6bcf\u4e2a\u7279\u5f81\u7684\u6982\u73870.2*\u9519\u8bef\u6982\u7387\u7684\u603b\u548c\u4e3a\u591a\u5c11\uff0c\u5c31\u77e5\u9053\u9519\u8bef\u7387\u591a\u9ad8\n                # \u4f8b\u5982:  \u4e00\u4e2a\u90fd\u6ca1\u9519\uff0c\u90a3\u4e48\u9519\u8bef\u7387= 0.2*0=0 \uff0c 5\u4e2a\u90fd\u9519\uff0c\u90a3\u4e48\u9519\u8bef\u7387= 0.2*5=1\uff0c \u53ea\u95193\u4e2a\uff0c\u90a3\u4e48\u9519\u8bef\u7387= 0.2*3=0.6\n                weightedError = D.T*errArr\n                '''\n                dim            \u8868\u793a feature\u5217\n                threshVal      \u8868\u793a\u6811\u7684\u5206\u754c\u503c\n                inequal        \u8868\u793a\u8ba1\u7b97\u6811\u5de6\u53f3\u98a0\u5012\u7684\u9519\u8bef\u7387\u7684\u60c5\u51b5\n                weightedError  \u8868\u793a\u6574\u4f53\u7ed3\u679c\u7684\u9519\u8bef\u7387\n                bestClasEst    \u9884\u6d4b\u7684\u6700\u4f18\u7ed3\u679c\n                '''\n                # print \"split: dim %d, thresh %.2f, thresh ineqal: %s, the weighted error is %.3f\" % (i, threshVal, inequal, weightedError)\n                if weightedError < minError:\n                    minError = weightedError\n                    bestClasEst = predictedVals.copy()\n                    bestStump['dim'] = i\n                    bestStump['thresh'] = threshVal\n                    bestStump['ineq'] = inequal\n\n    # bestStump \u8868\u793a\u5206\u7c7b\u5668\u7684\u7ed3\u679c\uff0c\u5728\u7b2c\u51e0\u4e2a\u5217\u4e0a\uff0c\u7528\u5927\u4e8e\uff0f\u5c0f\u4e8e\u6bd4\u8f83\uff0c\u9608\u503c\u662f\u591a\u5c11\n    return bestStump, minError, bestClasEst\n\n\ndef adaBoostTrainDS(dataArr, labelArr, numIt=40):\n    \"\"\"adaBoostTrainDS(adaBoost\u8bad\u7ec3\u8fc7\u7a0b\u653e\u5927)\n\n    Args:\n        dataArr   \u7279\u5f81\u6807\u7b7e\u96c6\u5408\n        labelArr  \u5206\u7c7b\u6807\u7b7e\u96c6\u5408\n        numIt     \u5b9e\u4f8b\u6570\n    Returns:\n        weakClassArr  \u5f31\u5206\u7c7b\u5668\u7684\u96c6\u5408\n        aggClassEst   \u9884\u6d4b\u7684\u5206\u7c7b\u7ed3\u679c\u503c\n    \"\"\"\n    weakClassArr = []\n    m = shape(dataArr)[0]\n    # \u521d\u59cb\u5316 D\uff0c\u8bbe\u7f6e\u6bcf\u884c\u6570\u636e\u7684\u6837\u672c\u7684\u6240\u6709\u7279\u5f81\u6743\u91cd\u96c6\u5408\uff0c\u5e73\u5747\u5206\u4e3am\u4efd\n    D = mat(ones((m, 1))/m)\n    aggClassEst = mat(zeros((m, 1)))\n    for i in range(numIt):\n        # \u5f97\u5230\u51b3\u7b56\u6811\u7684\u6a21\u578b\n        bestStump, error, classEst = buildStump(dataArr, labelArr, D)\n\n        # alpha \u76ee\u7684\u4e3b\u8981\u662f\u8ba1\u7b97\u6bcf\u4e00\u4e2a\u5206\u7c7b\u5668\u5b9e\u4f8b\u7684\u6743\u91cd(\u52a0\u548c\u5c31\u662f\u5206\u7c7b\u7ed3\u679c)\n        # \u8ba1\u7b97\u6bcf\u4e2a\u5206\u7c7b\u5668\u7684 alpha \u6743\u91cd\u503c\n        alpha = float(0.5*log((1.0-error)/max(error, 1e-16)))\n        bestStump['alpha'] = alpha\n        # store Stump Params in Array\n        weakClassArr.append(bestStump)\n\n        # print \"alpha=%s, classEst=%s, bestStump=%s, error=%s \" % (alpha, classEst.T, bestStump, error)\n        # \u5206\u7c7b\u6b63\u786e: \u4e58\u79ef\u4e3a1\uff0c\u4e0d\u4f1a\u5f71\u54cd\u7ed3\u679c\uff0c-1\u4e3b\u8981\u662f\u4e0b\u9762\u6c42e\u7684-alpha\u6b21\u65b9\n        # \u5206\u7c7b\u9519\u8bef: \u4e58\u79ef\u4e3a -1\uff0c\u7ed3\u679c\u4f1a\u53d7\u5f71\u54cd\uff0c\u6240\u4ee5\u4e5f\u4e58\u4ee5 -1\n        expon = multiply(-1*alpha*mat(labelArr).T, classEst)\n        # print '\\n'\n        # print 'labelArr=', labelArr\n        # print 'classEst=', classEst.T\n        # print '\\n'\n        # print '\u4e58\u79ef: ', multiply(mat(labelArr).T, classEst).T\n        # \u5224\u65ad\u6b63\u786e\u7684\uff0c\u5c31\u4e58\u4ee5-1\uff0c\u5426\u5219\u5c31\u4e58\u4ee51\uff0c \u4e3a\u4ec0\u4e48\uff1f \u4e66\u4e0a\u7684\u516c\u5f0f\u3002\n        # print '(-1\u53d6\u53cd)\u9884\u6d4b\u503cexpon=', expon.T\n        # \u8ba1\u7b97e\u7684expon\u6b21\u65b9\uff0c\u7136\u540e\u8ba1\u7b97\u5f97\u5230\u4e00\u4e2a\u7efc\u5408\u7684\u6982\u7387\u7684\u503c\n        # \u7ed3\u679c\u53d1\u73b0:  \u5224\u65ad\u9519\u8bef\u7684\u6837\u672c\uff0cD\u5bf9\u4e8e\u7684\u6837\u672c\u6743\u91cd\u503c\u4f1a\u53d8\u5927\u3002\n        D = multiply(D, exp(expon))\n        D = D/D.sum()\n        # print \"D: \", D.T\n        # print '\\n'\n\n        # \u9884\u6d4b\u7684\u5206\u7c7b\u7ed3\u679c\u503c\uff0c\u5728\u4e0a\u4e00\u8f6e\u7ed3\u679c\u7684\u57fa\u7840\u4e0a\uff0c\u8fdb\u884c\u52a0\u548c\u64cd\u4f5c\n        # print '\u5f53\u524d\u7684\u5206\u7c7b\u7ed3\u679c: ', alpha*classEst.T\n        aggClassEst += alpha*classEst\n        # print \"\u53e0\u52a0\u540e\u7684\u5206\u7c7b\u7ed3\u679caggClassEst: \", aggClassEst.T\n        # sign \u5224\u65ad\u6b63\u4e3a1\uff0c 0\u4e3a0\uff0c \u8d1f\u4e3a-1\uff0c\u901a\u8fc7\u6700\u7ec8\u52a0\u548c\u7684\u6743\u91cd\u503c\uff0c\u5224\u65ad\u7b26\u53f7\u3002\n        # \u7ed3\u679c\u4e3a: \u9519\u8bef\u7684\u6837\u672c\u6807\u7b7e\u96c6\u5408\uff0c\u56e0\u4e3a\u662f !=,\u90a3\u4e48\u7ed3\u679c\u5c31\u662f0 \u6b63, 1 \u8d1f\n        aggErrors = multiply(sign(aggClassEst) != mat(labelArr).T, ones((m, 1)))\n        errorRate = aggErrors.sum()/m\n        # print \"total error=%s \" % (errorRate)\n        if errorRate == 0.0:\n            break\n    return weakClassArr, aggClassEst\n\n\ndef adaClassify(datToClass, classifierArr):\n    # do stuff similar to last aggClassEst in adaBoostTrainDS\n    dataMat = mat(datToClass)\n    m = shape(dataMat)[0]\n    aggClassEst = mat(zeros((m, 1)))\n\n    # \u5faa\u73af \u591a\u4e2a\u5206\u7c7b\u5668\n    for i in range(len(classifierArr)):\n        # \u524d\u63d0:  \u6211\u4eec\u5df2\u7ecf\u77e5\u9053\u4e86\u6700\u4f73\u7684\u5206\u7c7b\u5668\u7684\u5b9e\u4f8b\n        # \u901a\u8fc7\u5206\u7c7b\u5668\u6765\u6838\u7b97\u6bcf\u4e00\u6b21\u7684\u5206\u7c7b\u7ed3\u679c\uff0c\u7136\u540e\u901a\u8fc7alpha*\u6bcf\u4e00\u6b21\u7684\u7ed3\u679c \u5f97\u5230\u6700\u540e\u7684\u6743\u91cd\u52a0\u548c\u7684\u503c\u3002\n        classEst = stumpClassify(dataMat, classifierArr[i]['dim'], classifierArr[i]['thresh'], classifierArr[i]['ineq'])\n        aggClassEst += classifierArr[i]['alpha']*classEst\n        # print aggClassEst\n    return sign(aggClassEst)\n\n\ndef plotROC(predStrengths, classLabels):\n    \"\"\"plotROC(\u6253\u5370ROC\u66f2\u7ebf\uff0c\u5e76\u8ba1\u7b97AUC\u7684\u9762\u79ef\u5927\u5c0f)\n\n    Args:\n        predStrengths  \u6700\u7ec8\u9884\u6d4b\u7ed3\u679c\u7684\u6743\u91cd\u503c\n        classLabels    \u539f\u59cb\u6570\u636e\u7684\u5206\u7c7b\u7ed3\u679c\u96c6\n    \"\"\"\n    print('predStrengths=', predStrengths)\n    print('classLabels=', classLabels)\n\n    import matplotlib.pyplot as plt\n    # variable to calculate AUC\n    ySum = 0.0\n    # \u5bf9\u6b63\u6837\u672c\u7684\u8fdb\u884c\u6c42\u548c\n    numPosClas = sum(array(classLabels)==1.0)\n    # \u6b63\u6837\u672c\u7684\u6982\u7387\n    yStep = 1/float(numPosClas)\n    # \u8d1f\u6837\u672c\u7684\u6982\u7387\n    xStep = 1/float(len(classLabels)-numPosClas)\n    # argsort\u51fd\u6570\u8fd4\u56de\u7684\u662f\u6570\u7ec4\u503c\u4ece\u5c0f\u5230\u5927\u7684\u7d22\u5f15\u503c\n    # get sorted index, it's reverse\n    sortedIndicies = predStrengths.argsort()\n    # \u6d4b\u8bd5\u7ed3\u679c\u662f\u5426\u662f\u4ece\u5c0f\u5230\u5927\u6392\u5217\n    print('sortedIndicies=', sortedIndicies, predStrengths[0, 176], predStrengths.min(), predStrengths[0, 293], predStrengths.max())\n\n    # \u5f00\u59cb\u521b\u5efa\u6a21\u7248\u5bf9\u8c61\n    fig = plt.figure()\n    fig.clf()\n    ax = plt.subplot(111)\n    # cursor\u5149\u6807\u503c\n    cur = (1.0, 1.0)\n    # loop through all the values, drawing a line segment at each point\n    for index in sortedIndicies.tolist()[0]:\n        if classLabels[index] == 1.0:\n            delX = 0\n            delY = yStep\n        else:\n            delX = xStep\n            delY = 0\n            ySum += cur[1]\n        # draw line from cur to (cur[0]-delX, cur[1]-delY)\n        # \u753b\u70b9\u8fde\u7ebf (x1, x2, y1, y2)\n        print(cur[0], cur[0]-delX, cur[1], cur[1]-delY)\n        ax.plot([cur[0], cur[0]-delX], [cur[1], cur[1]-delY], c='b')\n        cur = (cur[0]-delX, cur[1]-delY)\n    # \u753b\u5bf9\u89d2\u7684\u865a\u7ebf\u7ebf\n    ax.plot([0, 1], [0, 1], 'b--')\n    plt.xlabel('False positive rate')\n    plt.ylabel('True positive rate')\n    plt.title('ROC curve for AdaBoost horse colic detection system')\n    # \u8bbe\u7f6e\u753b\u56fe\u7684\u8303\u56f4\u533a\u95f4 (x1, x2, y1, y2)\n    ax.axis([0, 1, 0, 1])\n    plt.show()\n    '''\n    \u53c2\u8003\u8bf4\u660e: http://blog.csdn.net/wenyusuran/article/details/39056013\n    \u4e3a\u4e86\u8ba1\u7b97 AUC \uff0c\u6211\u4eec\u9700\u8981\u5bf9\u591a\u4e2a\u5c0f\u77e9\u5f62\u7684\u9762\u79ef\u8fdb\u884c\u7d2f\u52a0\u3002\n    \u8fd9\u4e9b\u5c0f\u77e9\u5f62\u7684\u5bbd\u5ea6\u662fxStep\uff0c\u56e0\u6b64\u53ef\u4ee5\u5148\u5bf9\u6240\u6709\u77e9\u5f62\u7684\u9ad8\u5ea6\u8fdb\u884c\u7d2f\u52a0\uff0c\u6700\u540e\u518d\u4e58\u4ee5xStep\u5f97\u5230\u5176\u603b\u9762\u79ef\u3002\n    \u6240\u6709\u9ad8\u5ea6\u7684\u548c(ySum)\u968f\u7740x\u8f74\u7684\u6bcf\u6b21\u79fb\u52a8\u800c\u6e10\u6b21\u589e\u52a0\u3002\n    '''\n    print(\"the Area Under the Curve is: \", ySum*xStep)\n\n\nif __name__ == \"__main__\":\n    # # \u6211\u4eec\u8981\u5c065\u4e2a\u70b9\u8fdb\u884c\u5206\u7c7b\n    # dataArr, labelArr = loadSimpData()\n    # print 'dataArr', dataArr, 'labelArr', labelArr\n\n    # # D\u8868\u793a\u6700\u521d\u503c\uff0c\u5bf91\u8fdb\u884c\u5747\u5206\u4e3a5\u4efd\uff0c\u5e73\u5747\u6bcf\u4e00\u4e2a\u521d\u59cb\u7684\u6982\u7387\u90fd\u4e3a0.2\n    # # D\u7684\u76ee\u7684\u662f\u4e3a\u4e86\u8ba1\u7b97\u9519\u8bef\u6982\u7387:  weightedError = D.T*errArr\n    # D = mat(ones((5, 1))/5)\n    # print 'D=', D.T\n\n    # # bestStump, minError, bestClasEst = buildStump(dataArr, labelArr, D)\n    # # print 'bestStump=', bestStump\n    # # print 'minError=', minError\n    # # print 'bestClasEst=', bestClasEst.T\n\n    # # \u5206\u7c7b\u5668: weakClassArr\n    # # \u5386\u53f2\u7d2f\u8ba1\u7684\u5206\u7c7b\u7ed3\u679c\u96c6\n    # weakClassArr, aggClassEst = adaBoostTrainDS(dataArr, labelArr, 9)\n    # print '\\nweakClassArr=', weakClassArr, '\\naggClassEst=', aggClassEst.T\n\n    # \"\"\"\n    # \u53d1\u73b0:\n    # \u5206\u7c7b\u7684\u6743\u91cd\u503c: \u6700\u5927\u7684\u503c\uff0c\u4e3aalpha\u7684\u52a0\u548c\uff0c\u6700\u5c0f\u503c\u4e3a-\u6700\u5927\u503c\n    # \u7279\u5f81\u7684\u6743\u91cd\u503c: \u5982\u679c\u4e00\u4e2a\u503c\u8bef\u5224\u7684\u51e0\u7387\u8d8a\u5c0f\uff0c\u90a3\u4e48D\u7684\u7279\u5f81\u6743\u91cd\u8d8a\u5c11\n    # \"\"\"\n\n    # # \u6d4b\u8bd5\u6570\u636e\u7684\u5206\u7c7b\u7ed3\u679c, \u89c2\u6d4b: aggClassEst\u5206\u7c7b\u7684\u6700\u7ec8\u6743\u91cd\n    # print adaClassify([0, 0], weakClassArr).T\n    # print adaClassify([[5, 5], [0, 0]], weakClassArr).T\n\n    # \u9a6c\u759d\u75c5\u6570\u636e\u96c6\n    # \u8bad\u7ec3\u96c6\u5408\n    dataArr, labelArr = loadDataSet(\"data/7.AdaBoost/horseColicTraining2.txt\")\n    weakClassArr, aggClassEst = adaBoostTrainDS(dataArr, labelArr, 40)\n    print(weakClassArr, '\\n-----\\n', aggClassEst.T)\n    # \u8ba1\u7b97ROC\u4e0b\u9762\u7684AUC\u7684\u9762\u79ef\u5927\u5c0f\n    plotROC(aggClassEst.T, labelArr)\n    # \u6d4b\u8bd5\u96c6\u5408\n    dataArrTest, labelArrTest = loadDataSet(\"data/7.AdaBoost/horseColicTest2.txt\")\n    m = shape(dataArrTest)[0]\n    predicting10 = adaClassify(dataArrTest, weakClassArr)\n    errArr = mat(ones((m, 1)))\n    # \u6d4b\u8bd5: \u8ba1\u7b97\u603b\u6837\u672c\u6570\uff0c\u9519\u8bef\u6837\u672c\u6570\uff0c\u9519\u8bef\u7387\n    print(m, errArr[predicting10 != mat(labelArrTest).T].sum(), errArr[predicting10 != mat(labelArrTest).T].sum()/m)\n", "src/py2.x/ml/7.AdaBoost/sklearn-adaboost-demo.py": "#!/usr/bin/python\n# coding:utf8\n\"\"\"\nCreated on 2017-07-10\nUpdated on 2017-07-10\nAuthor: \u7247\u523b\uff0fNoel Dawe\nGitHub: https://github.com/apachecn/AiLearning\nsklearn-AdaBoost\u8bd1\u6587\u94fe\u63a5: http://cwiki.apachecn.org/pages/viewpage.action?pageId=10813457\n\"\"\"\nfrom __future__ import print_function\n\nimport matplotlib.pyplot as plt\n# importing necessary libraries\nimport numpy as np\nfrom sklearn import metrics\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.tree import DecisionTreeRegressor\n\nprint(__doc__)\n\n\n# Create the dataset\nrng = np.random.RandomState(1)\nX = np.linspace(0, 6, 100)[:, np.newaxis]\ny = np.sin(X).ravel() + np.sin(6 * X).ravel() + rng.normal(0, 0.1, X.shape[0])\n# dataArr, labelArr = loadDataSet(\"data/7.AdaBoost/horseColicTraining2.txt\")\n\n\n# Fit regression model\nregr_1 = DecisionTreeRegressor(max_depth=4)\nregr_2 = AdaBoostRegressor(DecisionTreeRegressor(max_depth=4), n_estimators=300, random_state=rng)\n\nregr_1.fit(X, y)\nregr_2.fit(X, y)\n\n# Predict\ny_1 = regr_1.predict(X)\ny_2 = regr_2.predict(X)\n\n# Plot the results\nplt.figure()\nplt.scatter(X, y, c=\"k\", label=\"training samples\")\nplt.plot(X, y_1, c=\"g\", label=\"n_estimators=1\", linewidth=2)\nplt.plot(X, y_2, c=\"r\", label=\"n_estimators=300\", linewidth=2)\nplt.xlabel(\"data\")\nplt.ylabel(\"target\")\nplt.title(\"Boosted Decision Tree Regression\")\nplt.legend()\nplt.show()\n\nprint('y---', type(y[0]), len(y), y[:4])\nprint('y_1---', type(y_1[0]), len(y_1), y_1[:4])\nprint('y_2---', type(y_2[0]), len(y_2), y_2[:4])\n\n# \u9002\u54082\u5206\u7c7b\ny_true = np.array([0, 0, 1, 1])\ny_scores = np.array([0.1, 0.4, 0.35, 0.8])\nprint('y_scores---', type(y_scores[0]), len(y_scores), y_scores)\nprint(metrics.roc_auc_score(y_true, y_scores))\n\n# print \"-\" * 100\n# print metrics.roc_auc_score(y[:1], y_2[:1])\n", "src/py2.x/ml/10.kmeans/kMeans.py": "#!/usr/bin/python\n# coding:utf8\n'''\nCreated on Feb 16, 2011\nUpdate on 2017-05-18\nk Means Clustering for Ch10 of Machine Learning in Action\nAuthor: Peter Harrington/\u90a3\u4f0a\u62b9\u5fae\u7b11\nGitHub: https://github.com/apachecn/AiLearning\n'''\nfrom __future__ import print_function\nfrom numpy import *\n\n\n# \u4ece\u6587\u672c\u4e2d\u6784\u5efa\u77e9\u9635\uff0c\u52a0\u8f7d\u6587\u672c\u6587\u4ef6\uff0c\u7136\u540e\u5904\u7406\ndef loadDataSet(fileName):  # \u901a\u7528\u51fd\u6570\uff0c\u7528\u6765\u89e3\u6790\u4ee5 tab \u952e\u5206\u9694\u7684 floats\uff08\u6d6e\u70b9\u6570\uff09\n    dataSet = []\n    fr = open(fileName)\n    for line in fr.readlines():\n        curLine = line.strip().split('\\t')\n        fltLine = map(float, curLine)  # \u6620\u5c04\u6240\u6709\u7684\u5143\u7d20\u4e3a float\uff08\u6d6e\u70b9\u6570\uff09\u7c7b\u578b\n        dataSet.append(fltLine)\n    return dataSet\n\n\n# \u8ba1\u7b97\u4e24\u4e2a\u5411\u91cf\u7684\u6b27\u5f0f\u8ddd\u79bb\uff08\u53ef\u6839\u636e\u573a\u666f\u9009\u62e9\uff09\ndef distEclud(vecA, vecB):\n    return sqrt(sum(power(vecA - vecB, 2)))  # la.norm(vecA-vecB)\n\n\n# \u4e3a\u7ed9\u5b9a\u6570\u636e\u96c6\u6784\u5efa\u4e00\u4e2a\u5305\u542b k \u4e2a\u968f\u673a\u8d28\u5fc3\u7684\u96c6\u5408\u3002\u968f\u673a\u8d28\u5fc3\u5fc5\u987b\u8981\u5728\u6574\u4e2a\u6570\u636e\u96c6\u7684\u8fb9\u754c\u4e4b\u5185\uff0c\u8fd9\u53ef\u4ee5\u901a\u8fc7\u627e\u5230\u6570\u636e\u96c6\u6bcf\u4e00\u7ef4\u7684\u6700\u5c0f\u548c\u6700\u5927\u503c\u6765\u5b8c\u6210\u3002\u7136\u540e\u751f\u6210 0~1.0 \u4e4b\u95f4\u7684\u968f\u673a\u6570\u5e76\u901a\u8fc7\u53d6\u503c\u8303\u56f4\u548c\u6700\u5c0f\u503c\uff0c\u4ee5\u4fbf\u786e\u4fdd\u968f\u673a\u70b9\u5728\u6570\u636e\u7684\u8fb9\u754c\u4e4b\u5185\u3002\ndef randCent(dataMat, k):\n    n = shape(dataMat)[1]  # \u5217\u7684\u6570\u91cf\n    centroids = mat(zeros((k, n)))  # \u521b\u5efak\u4e2a\u8d28\u5fc3\u77e9\u9635\n    for j in range(n):  # \u521b\u5efa\u968f\u673a\u7c07\u8d28\u5fc3\uff0c\u5e76\u4e14\u5728\u6bcf\u4e00\u7ef4\u7684\u8fb9\u754c\u5185\n        minJ = min(dataMat[:, j])  # \u6700\u5c0f\u503c\n        rangeJ = float(max(dataMat[:, j]) - minJ)  # \u8303\u56f4 = \u6700\u5927\u503c - \u6700\u5c0f\u503c\n        centroids[:, j] = mat(minJ + rangeJ * random.rand(k, 1))  # \u968f\u673a\u751f\u6210\n    return centroids\n\n\n# k-means \u805a\u7c7b\u7b97\u6cd5\n# \u8be5\u7b97\u6cd5\u4f1a\u521b\u5efak\u4e2a\u8d28\u5fc3\uff0c\u7136\u540e\u5c06\u6bcf\u4e2a\u70b9\u5206\u914d\u5230\u6700\u8fd1\u7684\u8d28\u5fc3\uff0c\u518d\u91cd\u65b0\u8ba1\u7b97\u8d28\u5fc3\u3002\n# \u8fd9\u4e2a\u8fc7\u7a0b\u91cd\u590d\u6570\u6b21\uff0c\u77e5\u9053\u6570\u636e\u70b9\u7684\u7c07\u5206\u914d\u7ed3\u679c\u4e0d\u518d\u6539\u53d8\u4f4d\u7f6e\u3002\n# \u8fd0\u884c\u7ed3\u679c\uff08\u591a\u6b21\u8fd0\u884c\u7ed3\u679c\u53ef\u80fd\u4f1a\u4e0d\u4e00\u6837\uff0c\u53ef\u4ee5\u8bd5\u8bd5\uff0c\u539f\u56e0\u4e3a\u968f\u673a\u8d28\u5fc3\u7684\u5f71\u54cd\uff0c\u4f46\u603b\u7684\u7ed3\u679c\u662f\u5bf9\u7684\uff0c \u56e0\u4e3a\u6570\u636e\u8db3\u591f\u76f8\u4f3c\uff0c\u4e5f\u53ef\u80fd\u4f1a\u9677\u5165\u5c40\u90e8\u6700\u5c0f\u503c\uff09\ndef kMeans(dataMat, k, distMeas=distEclud, createCent=randCent):\n    m = shape(dataMat)[0]  # \u884c\u6570\n    clusterAssment = mat(zeros(\n        (m, 2)))  # \u521b\u5efa\u4e00\u4e2a\u4e0e dataMat \u884c\u6570\u4e00\u6837\uff0c\u4f46\u662f\u6709\u4e24\u5217\u7684\u77e9\u9635\uff0c\u7528\u6765\u4fdd\u5b58\u7c07\u5206\u914d\u7ed3\u679c\n    centroids = createCent(dataMat, k)  # \u521b\u5efa\u8d28\u5fc3\uff0c\u968f\u673ak\u4e2a\u8d28\u5fc3\n    clusterChanged = True\n    while clusterChanged:\n        clusterChanged = False\n        for i in range(m):  # \u5faa\u73af\u6bcf\u4e00\u4e2a\u6570\u636e\u70b9\u5e76\u5206\u914d\u5230\u6700\u8fd1\u7684\u8d28\u5fc3\u4e2d\u53bb\n            minDist = inf\n            minIndex = -1\n            for j in range(k):\n                distJI = distMeas(centroids[j, :],\n                                  dataMat[i, :])  # \u8ba1\u7b97\u6570\u636e\u70b9\u5230\u8d28\u5fc3\u7684\u8ddd\u79bb\n                if distJI < minDist:  # \u5982\u679c\u8ddd\u79bb\u6bd4 minDist\uff08\u6700\u5c0f\u8ddd\u79bb\uff09\u8fd8\u5c0f\uff0c\u66f4\u65b0 minDist\uff08\u6700\u5c0f\u8ddd\u79bb\uff09\u548c\u6700\u5c0f\u8d28\u5fc3\u7684 index\uff08\u7d22\u5f15\uff09\n                    minDist = distJI\n                    minIndex = j\n            if clusterAssment[i, 0] != minIndex:  # \u7c07\u5206\u914d\u7ed3\u679c\u6539\u53d8\n                clusterChanged = True  # \u7c07\u6539\u53d8\n                clusterAssment[\n                    i, :] = minIndex, minDist**2  # \u66f4\u65b0\u7c07\u5206\u914d\u7ed3\u679c\u4e3a\u6700\u5c0f\u8d28\u5fc3\u7684 index\uff08\u7d22\u5f15\uff09\uff0cminDist\uff08\u6700\u5c0f\u8ddd\u79bb\uff09\u7684\u5e73\u65b9\n        print(centroids)\n        for cent in range(k):  # \u66f4\u65b0\u8d28\u5fc3\n            ptsInClust = dataMat[nonzero(\n                clusterAssment[:, 0].A == cent)[0]]  # \u83b7\u53d6\u8be5\u7c07\u4e2d\u7684\u6240\u6709\u70b9\n            centroids[cent, :] = mean(\n                ptsInClust, axis=0)  # \u5c06\u8d28\u5fc3\u4fee\u6539\u4e3a\u7c07\u4e2d\u6240\u6709\u70b9\u7684\u5e73\u5747\u503c\uff0cmean \u5c31\u662f\u6c42\u5e73\u5747\u503c\u7684\n    return centroids, clusterAssment\n\n\n# \u4e8c\u5206 KMeans \u805a\u7c7b\u7b97\u6cd5, \u57fa\u4e8e kMeans \u57fa\u7840\u4e4b\u4e0a\u7684\u4f18\u5316\uff0c\u4ee5\u907f\u514d\u9677\u5165\u5c40\u90e8\u6700\u5c0f\u503c\ndef biKMeans(dataMat, k, distMeas=distEclud):\n    m = shape(dataMat)[0]\n    clusterAssment = mat(zeros((m, 2)))  # \u4fdd\u5b58\u6bcf\u4e2a\u6570\u636e\u70b9\u7684\u7c07\u5206\u914d\u7ed3\u679c\u548c\u5e73\u65b9\u8bef\u5dee\n    centroid0 = mean(dataMat, axis=0).tolist()[0]  # \u8d28\u5fc3\u521d\u59cb\u5316\u4e3a\u6240\u6709\u6570\u636e\u70b9\u7684\u5747\u503c\n    centList = [centroid0]  # \u521d\u59cb\u5316\u53ea\u6709 1 \u4e2a\u8d28\u5fc3\u7684 list\n    for j in range(m):  # \u8ba1\u7b97\u6240\u6709\u6570\u636e\u70b9\u5230\u521d\u59cb\u8d28\u5fc3\u7684\u8ddd\u79bb\u5e73\u65b9\u8bef\u5dee\n        clusterAssment[j, 1] = distMeas(mat(centroid0), dataMat[j, :])**2\n    while (len(centList) < k):  # \u5f53\u8d28\u5fc3\u6570\u91cf\u5c0f\u4e8e k \u65f6\n        lowestSSE = inf\n        for i in range(len(centList)):  # \u5bf9\u6bcf\u4e00\u4e2a\u8d28\u5fc3\n            ptsInCurrCluster = dataMat[nonzero(\n                clusterAssment[:, 0].A == i)[0], :]  # \u83b7\u53d6\u5f53\u524d\u7c07 i \u4e0b\u7684\u6240\u6709\u6570\u636e\u70b9\n            centroidMat, splitClustAss = kMeans(\n                ptsInCurrCluster, 2, distMeas)  # \u5c06\u5f53\u524d\u7c07 i \u8fdb\u884c\u4e8c\u5206 kMeans \u5904\u7406\n            sseSplit = sum(splitClustAss[:, 1])  # \u5c06\u4e8c\u5206 kMeans \u7ed3\u679c\u4e2d\u7684\u5e73\u65b9\u548c\u7684\u8ddd\u79bb\u8fdb\u884c\u6c42\u548c\n            sseNotSplit = sum(\n                clusterAssment[nonzero(clusterAssment[:, 0].A != i)[0],\n                               1])  # \u5c06\u672a\u53c2\u4e0e\u4e8c\u5206 kMeans \u5206\u914d\u7ed3\u679c\u4e2d\u7684\u5e73\u65b9\u548c\u7684\u8ddd\u79bb\u8fdb\u884c\u6c42\u548c\n            print(\"sseSplit, and notSplit: \", sseSplit, sseNotSplit)\n            if (sseSplit + sseNotSplit) < lowestSSE:\n                bestCentToSplit = i\n                bestNewCents = centroidMat\n                bestClustAss = splitClustAss.copy()\n                lowestSSE = sseSplit + sseNotSplit\n        # \u627e\u51fa\u6700\u597d\u7684\u7c07\u5206\u914d\u7ed3\u679c    \n        bestClustAss[nonzero(bestClustAss[:, 0].A == 1)[0], 0] = len(\n            centList)  # \u8c03\u7528\u4e8c\u5206 kMeans \u7684\u7ed3\u679c\uff0c\u9ed8\u8ba4\u7c07\u662f 0,1. \u5f53\u7136\u4e5f\u53ef\u4ee5\u6539\u6210\u5176\u5b83\u7684\u6570\u5b57\n        bestClustAss[nonzero(bestClustAss[:, 0].A == 0)[0],\n                     0] = bestCentToSplit  # \u66f4\u65b0\u4e3a\u6700\u4f73\u8d28\u5fc3\n        print('the bestCentToSplit is: ', bestCentToSplit)\n        print('the len of bestClustAss is: ', len(bestClustAss))\n        # \u66f4\u65b0\u8d28\u5fc3\u5217\u8868\n        centList[bestCentToSplit] = bestNewCents[0, :].tolist()[\n            0]  # \u66f4\u65b0\u539f\u8d28\u5fc3 list \u4e2d\u7684\u7b2c i \u4e2a\u8d28\u5fc3\u4e3a\u4f7f\u7528\u4e8c\u5206 kMeans \u540e bestNewCents \u7684\u7b2c\u4e00\u4e2a\u8d28\u5fc3\n        centList.append(\n            bestNewCents[1, :].tolist()[0])  # \u6dfb\u52a0 bestNewCents \u7684\u7b2c\u4e8c\u4e2a\u8d28\u5fc3\n        clusterAssment[nonzero(clusterAssment[:, 0].A == bestCentToSplit)[\n            0], :] = bestClustAss  # \u91cd\u65b0\u5206\u914d\u6700\u597d\u7c07\u4e0b\u7684\u6570\u636e\uff08\u8d28\u5fc3\uff09\u4ee5\u53caSSE\n    return mat(centList), clusterAssment\n\n\ndef testBasicFunc():\n    # \u52a0\u8f7d\u6d4b\u8bd5\u6570\u636e\u96c6\n    dataMat = mat(loadDataSet('data/10.KMeans/testSet.txt'))\n\n    # \u6d4b\u8bd5 randCent() \u51fd\u6570\u662f\u5426\u6b63\u5e38\u8fd0\u884c\u3002\n    # \u9996\u5148\uff0c\u5148\u770b\u4e00\u4e0b\u77e9\u9635\u4e2d\u7684\u6700\u5927\u503c\u4e0e\u6700\u5c0f\u503c\n    print('min(dataMat[:, 0])=', min(dataMat[:, 0]))\n    print('min(dataMat[:, 1])=', min(dataMat[:, 1]))\n    print('max(dataMat[:, 1])=', max(dataMat[:, 1]))\n    print('max(dataMat[:, 0])=', max(dataMat[:, 0]))\n\n    # \u7136\u540e\u770b\u770b randCent() \u51fd\u6570\u80fd\u5426\u751f\u6210 min \u5230 max \u4e4b\u95f4\u7684\u503c\n    print('randCent(dataMat, 2)=', randCent(dataMat, 2))\n\n    # \u6700\u540e\u6d4b\u8bd5\u4e00\u4e0b\u8ddd\u79bb\u8ba1\u7b97\u65b9\u6cd5\n    print(' distEclud(dataMat[0], dataMat[1])=', distEclud(dataMat[0], dataMat[1]))\n\n\ndef testKMeans():\n    # \u52a0\u8f7d\u6d4b\u8bd5\u6570\u636e\u96c6\n    dataMat = mat(loadDataSet('data/10.KMeans/testSet.txt'))\n\n    # \u8be5\u7b97\u6cd5\u4f1a\u521b\u5efak\u4e2a\u8d28\u5fc3\uff0c\u7136\u540e\u5c06\u6bcf\u4e2a\u70b9\u5206\u914d\u5230\u6700\u8fd1\u7684\u8d28\u5fc3\uff0c\u518d\u91cd\u65b0\u8ba1\u7b97\u8d28\u5fc3\u3002\n    # \u8fd9\u4e2a\u8fc7\u7a0b\u91cd\u590d\u6570\u6b21\uff0c\u77e5\u9053\u6570\u636e\u70b9\u7684\u7c07\u5206\u914d\u7ed3\u679c\u4e0d\u518d\u6539\u53d8\u4f4d\u7f6e\u3002\n    # \u8fd0\u884c\u7ed3\u679c\uff08\u591a\u6b21\u8fd0\u884c\u7ed3\u679c\u53ef\u80fd\u4f1a\u4e0d\u4e00\u6837\uff0c\u53ef\u4ee5\u8bd5\u8bd5\uff0c\u539f\u56e0\u4e3a\u968f\u673a\u8d28\u5fc3\u7684\u5f71\u54cd\uff0c\u4f46\u603b\u7684\u7ed3\u679c\u662f\u5bf9\u7684\uff0c \u56e0\u4e3a\u6570\u636e\u8db3\u591f\u76f8\u4f3c\uff09\n    myCentroids, clustAssing = kMeans(dataMat, 4)\n\n    print('centroids=', myCentroids)\n\n\ndef testBiKMeans():\n    # \u52a0\u8f7d\u6d4b\u8bd5\u6570\u636e\u96c6\n    dataMat = mat(loadDataSet('data/10.KMeans/testSet2.txt'))\n\n    centList, myNewAssments = biKMeans(dataMat, 3)\n\n    print('centList=', centList)\n\n\nif __name__ == \"__main__\":\n\n    # \u6d4b\u8bd5\u57fa\u7840\u7684\u51fd\u6570\n    # testBasicFunc()\n\n    # \u6d4b\u8bd5 kMeans \u51fd\u6570\n    # testKMeans()\n\n    # \u6d4b\u8bd5\u4e8c\u5206 biKMeans \u51fd\u6570\n    testBiKMeans()\n", "src/py2.x/ml/10.kmeans/kMeansSklearn.py": "# -*- coding:UTF-8 -*-\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\n\n# \u52a0\u8f7d\u6570\u636e\u96c6\ndataMat = []\nfr = open(\"data/10.KMeans/testSet.txt\") # \u6ce8\u610f\uff0c\u8fd9\u4e2a\u662f\u76f8\u5bf9\u8def\u5f84\uff0c\u8bf7\u4fdd\u8bc1\u662f\u5728 MachineLearning \u8fd9\u4e2a\u76ee\u5f55\u4e0b\u6267\u884c\u3002\nfor line in fr.readlines():\n    curLine = line.strip().split('\\t')\n    fltLine = map(float,curLine)    # \u6620\u5c04\u6240\u6709\u7684\u5143\u7d20\u4e3a float\uff08\u6d6e\u70b9\u6570\uff09\u7c7b\u578b\n    dataMat.append(fltLine)\n\n# \u8bad\u7ec3\u6a21\u578b\nkm = KMeans(n_clusters=4) # \u521d\u59cb\u5316\nkm.fit(dataMat) # \u62df\u5408\nkm_pred = km.predict(dataMat) # \u9884\u6d4b\ncenters = km.cluster_centers_ # \u8d28\u5fc3\n\n# \u53ef\u89c6\u5316\u7ed3\u679c\nplt.scatter(np.array(dataMat)[:, 1], np.array(dataMat)[:, 0], c=km_pred)\nplt.scatter(centers[:, 1], centers[:, 0], c=\"r\")\nplt.show()\n", "src/py2.x/ml/7.RandomForest/randomForest.py": "#!/usr/bin/python\n# coding:utf8\n\n'''\nCreated 2017-04-25\nUpdate  on 2017-05-18\nRandom Forest Algorithm on Sonar Dataset\nAuthor: Flying_sfeng/\u7247\u523b\nGitHub: https://github.com/apachecn/AiLearning\n---\n\u6e90\u4ee3\u7801\u7f51\u5740: http://www.tuicool.com/articles/iiUfeim\nFlying_sfeng\u535a\u5ba2\u5730\u5740: http://blog.csdn.net/flying_sfeng/article/details/64133822 (\u611f\u8c22\u4f5c\u8005\u8d21\u732e)\n'''\nfrom __future__ import print_function\nfrom random import seed, randrange, random\n\n\n# \u5bfc\u5165csv\u6587\u4ef6\ndef loadDataSet(filename):\n    dataset = []\n    with open(filename, 'r') as fr:\n        for line in fr.readlines():\n            if not line:\n                continue\n            lineArr = []\n            for featrue in line.split(','):\n                # strip()\u8fd4\u56de\u79fb\u9664\u5b57\u7b26\u4e32\u5934\u5c3e\u6307\u5b9a\u7684\u5b57\u7b26\u751f\u6210\u7684\u65b0\u5b57\u7b26\u4e32\n                str_f = featrue.strip()\n\n                # isdigit \u5982\u679c\u662f\u6d6e\u70b9\u578b\u6570\u503c\uff0c\u5c31\u662f false\uff0c\u6240\u4ee5\u6362\u6210 isalpha() \u51fd\u6570\n                # if str_f.isdigit():   # \u5224\u65ad\u662f\u5426\u662f\u6570\u5b57\n                if str_f.isalpha():     # \u5982\u679c\u662f\u5b57\u6bcd\uff0c\u8bf4\u660e\u662f\u6807\u7b7e\n                    # \u6dfb\u52a0\u5206\u7c7b\u6807\u7b7e\n                    lineArr.append(str_f)\n                else:\n                    # \u5c06\u6570\u636e\u96c6\u7684\u7b2ccolumn\u5217\u8f6c\u6362\u6210float\u5f62\u5f0f\n                    lineArr.append(float(str_f))\n            dataset.append(lineArr)\n    return dataset\n\n\ndef cross_validation_split(dataset, n_folds):\n    \"\"\"cross_validation_split(\u5c06\u6570\u636e\u96c6\u8fdb\u884c\u62bd\u91cd\u62bd\u6837 n_folds \u4efd\uff0c\u6570\u636e\u53ef\u4ee5\u91cd\u590d\u91cd\u590d\u62bd\u53d6\uff0c\u6bcf\u4e00\u6b21list\u7684\u5143\u7d20\u662f\u65e0\u91cd\u590d\u7684)\n\n    Args:\n        dataset     \u539f\u59cb\u6570\u636e\u96c6\n        n_folds     \u6570\u636e\u96c6dataset\u5206\u6210n_flods\u4efd\n    Returns:\n        dataset_split    list\u96c6\u5408\uff0c\u5b58\u653e\u7684\u662f: \u5c06\u6570\u636e\u96c6\u8fdb\u884c\u62bd\u91cd\u62bd\u6837 n_folds \u4efd\uff0c\u6570\u636e\u53ef\u4ee5\u91cd\u590d\u91cd\u590d\u62bd\u53d6\uff0c\u6bcf\u4e00\u6b21list\u7684\u5143\u7d20\u662f\u65e0\u91cd\u590d\u7684\n    \"\"\"\n    dataset_split = list()\n    dataset_copy = list(dataset)       # \u590d\u5236\u4e00\u4efd dataset,\u9632\u6b62 dataset \u7684\u5185\u5bb9\u6539\u53d8\n    fold_size = len(dataset) / n_folds\n    for i in range(n_folds):\n        fold = list()                  # \u6bcf\u6b21\u5faa\u73af fold \u6e05\u96f6\uff0c\u9632\u6b62\u91cd\u590d\u5bfc\u5165 dataset_split\n        while len(fold) < fold_size:   # \u8fd9\u91cc\u4e0d\u80fd\u7528 if\uff0cif \u53ea\u662f\u5728\u7b2c\u4e00\u6b21\u5224\u65ad\u65f6\u8d77\u4f5c\u7528\uff0cwhile \u6267\u884c\u5faa\u73af\uff0c\u76f4\u5230\u6761\u4ef6\u4e0d\u6210\u7acb\n            # \u6709\u653e\u56de\u7684\u968f\u673a\u91c7\u6837\uff0c\u6709\u4e00\u4e9b\u6837\u672c\u88ab\u91cd\u590d\u91c7\u6837\uff0c\u4ece\u800c\u5728\u8bad\u7ec3\u96c6\u4e2d\u591a\u6b21\u51fa\u73b0\uff0c\u6709\u7684\u5219\u4ece\u672a\u5728\u8bad\u7ec3\u96c6\u4e2d\u51fa\u73b0\uff0c\u6b64\u5219\u81ea\u52a9\u91c7\u6837\u6cd5\u3002\u4ece\u800c\u4fdd\u8bc1\u6bcf\u68f5\u51b3\u7b56\u6811\u8bad\u7ec3\u96c6\u7684\u5dee\u5f02\u6027            \n            index = randrange(len(dataset_copy))\n            # \u5c06\u5bf9\u5e94\u7d22\u5f15 index \u7684\u5185\u5bb9\u4ece dataset_copy \u4e2d\u5bfc\u51fa\uff0c\u5e76\u5c06\u8be5\u5185\u5bb9\u4ece dataset_copy \u4e2d\u5220\u9664\u3002\n            # pop() \u51fd\u6570\u7528\u4e8e\u79fb\u9664\u5217\u8868\u4e2d\u7684\u4e00\u4e2a\u5143\u7d20\uff08\u9ed8\u8ba4\u6700\u540e\u4e00\u4e2a\u5143\u7d20\uff09\uff0c\u5e76\u4e14\u8fd4\u56de\u8be5\u5143\u7d20\u7684\u503c\u3002\n            # fold.append(dataset_copy.pop(index))  # \u65e0\u653e\u56de\u7684\u65b9\u5f0f\n            fold.append(dataset_copy[index])  # \u6709\u653e\u56de\u7684\u65b9\u5f0f\n        dataset_split.append(fold)\n    # \u7531dataset\u5206\u5272\u51fa\u7684n_folds\u4e2a\u6570\u636e\u6784\u6210\u7684\u5217\u8868\uff0c\u4e3a\u4e86\u7528\u4e8e\u4ea4\u53c9\u9a8c\u8bc1\n    return dataset_split\n\n\n# Split a dataset based on an attribute and an attribute value # \u6839\u636e\u7279\u5f81\u548c\u7279\u5f81\u503c\u5206\u5272\u6570\u636e\u96c6\ndef test_split(index, value, dataset):\n    left, right = list(), list()\n    for row in dataset:\n        if row[index] < value:\n            left.append(row)\n        else:\n            right.append(row)\n    return left, right\n\n\n'''\nGini\u6307\u6570\u7684\u8ba1\u7b97\u95ee\u9898\uff0c\u5047\u5982\u5c06\u539f\u59cb\u6570\u636e\u96c6D\u5207\u5272\u4e24\u90e8\u5206\uff0c\u5206\u522b\u4e3aD1\u548cD2\uff0c\u5219\nGini(D|\u5207\u5272) = (|D1|/|D| ) * Gini(D1) + (|D2|/|D|) * Gini(D2)\n\u5b66\u4e60\u5730\u5740: \n    http://bbs.pinggu.org/thread-5986969-1-1.html\n    http://www.cnblogs.com/pinard/p/6053344.html\n\u800c\u539f\u6587\u4e2d \u8ba1\u7b97\u65b9\u5f0f\u4e3a: \nGini(D|\u5207\u5272) = Gini(D1) + Gini(D2)\n\n# Calculate the Gini index for a split dataset\ndef gini_index(groups, class_values):    # \u4e2a\u4eba\u7406\u89e3: \u8ba1\u7b97\u4ee3\u4ef7\uff0c\u5206\u7c7b\u8d8a\u51c6\u786e\uff0c\u5219 gini \u8d8a\u5c0f\n    gini = 0.0\n    for class_value in class_values:     # class_values = [0, 1] \n        for group in groups:             # groups = (left, right)\n            size = len(group)\n            if size == 0:\n                continue\n            proportion = [row[-1] for row in group].count(class_value) / float(size)\n            gini += (proportion * (1.0 - proportion))    # \u4e2a\u4eba\u7406\u89e3: \u8ba1\u7b97\u4ee3\u4ef7\uff0c\u5206\u7c7b\u8d8a\u51c6\u786e\uff0c\u5219 gini \u8d8a\u5c0f\n    return gini\n'''\n\n\ndef gini_index(groups, class_values):    # \u4e2a\u4eba\u7406\u89e3: \u8ba1\u7b97\u4ee3\u4ef7\uff0c\u5206\u7c7b\u8d8a\u51c6\u786e\uff0c\u5219 gini \u8d8a\u5c0f\n    gini = 0.0\n    D = len(groups[0]) + len(groups[1])\n    for class_value in class_values:     # class_values = [0, 1]\n        for group in groups:             # groups = (left, right)\n            size = len(group)\n            if size == 0:\n                continue\n            proportion = [row[-1] for row in group].count(class_value) / float(size)\n            gini += float(size)/D * (proportion * (1.0 - proportion))    # \u4e2a\u4eba\u7406\u89e3: \u8ba1\u7b97\u4ee3\u4ef7\uff0c\u5206\u7c7b\u8d8a\u51c6\u786e\uff0c\u5219 gini \u8d8a\u5c0f\n    return gini\n\n\n# \u627e\u51fa\u5206\u5272\u6570\u636e\u96c6\u7684\u6700\u4f18\u7279\u5f81\uff0c\u5f97\u5230\u6700\u4f18\u7684\u7279\u5f81 index\uff0c\u7279\u5f81\u503c row[index]\uff0c\u4ee5\u53ca\u5206\u5272\u5b8c\u7684\u6570\u636e groups\uff08left, right\uff09\ndef get_split(dataset, n_features):\n    class_values = list(set(row[-1] for row in dataset))  # class_values =[0, 1]\n    b_index, b_value, b_score, b_groups = 999, 999, 999, None\n    features = list()\n    while len(features) < n_features:\n        index = randrange(len(dataset[0])-1)  # \u5f80 features \u6dfb\u52a0 n_features \u4e2a\u7279\u5f81\uff08 n_feature \u7b49\u4e8e\u7279\u5f81\u6570\u7684\u6839\u53f7\uff09\uff0c\u7279\u5f81\u7d22\u5f15\u4ece dataset \u4e2d\u968f\u673a\u53d6\n        if index not in features:\n            features.append(index)\n    for index in features:                    # \u5728 n_features \u4e2a\u7279\u5f81\u4e2d\u9009\u51fa\u6700\u4f18\u7684\u7279\u5f81\u7d22\u5f15\uff0c\u5e76\u6ca1\u6709\u904d\u5386\u6240\u6709\u7279\u5f81\uff0c\u4ece\u800c\u4fdd\u8bc1\u4e86\u6bcf\u8bfe\u51b3\u7b56\u6811\u7684\u5dee\u5f02\u6027\n        for row in dataset:\n            groups = test_split(index, row[index], dataset)  # groups=(left, right), row[index] \u904d\u5386\u6bcf\u4e00\u884c index \u7d22\u5f15\u4e0b\u7684\u7279\u5f81\u503c\u4f5c\u4e3a\u5206\u7c7b\u503c value, \u627e\u51fa\u6700\u4f18\u7684\u5206\u7c7b\u7279\u5f81\u548c\u7279\u5f81\u503c\n            gini = gini_index(groups, class_values)\n            # \u5de6\u53f3\u4e24\u8fb9\u7684\u6570\u91cf\u8d8a\u4e00\u6837\uff0c\u8bf4\u660e\u6570\u636e\u533a\u5206\u5ea6\u4e0d\u9ad8\uff0cgini\u7cfb\u6570\u8d8a\u5927\n            if gini < b_score:\n                b_index, b_value, b_score, b_groups = index, row[index], gini, groups  # \u6700\u540e\u5f97\u5230\u6700\u4f18\u7684\u5206\u7c7b\u7279\u5f81 b_index,\u5206\u7c7b\u7279\u5f81\u503c b_value,\u5206\u7c7b\u7ed3\u679c b_groups\u3002b_value \u4e3a\u5206\u9519\u7684\u4ee3\u4ef7\u6210\u672c\n    # print b_score\n    return {'index': b_index, 'value': b_value, 'groups': b_groups}\n\n\n# Create a terminal node value # \u8f93\u51fagroup\u4e2d\u51fa\u73b0\u6b21\u6570\u8f83\u591a\u7684\u6807\u7b7e\ndef to_terminal(group):\n    outcomes = [row[-1] for row in group]           # max() \u51fd\u6570\u4e2d\uff0c\u5f53 key \u53c2\u6570\u4e0d\u4e3a\u7a7a\u65f6\uff0c\u5c31\u4ee5 key \u7684\u51fd\u6570\u5bf9\u8c61\u4e3a\u5224\u65ad\u7684\u6807\u51c6\n    return max(set(outcomes), key=outcomes.count)   # \u8f93\u51fa group \u4e2d\u51fa\u73b0\u6b21\u6570\u8f83\u591a\u7684\u6807\u7b7e  \n\n\n# Create child splits for a node or make terminal  # \u521b\u5efa\u5b50\u5206\u5272\u5668\uff0c\u9012\u5f52\u5206\u7c7b\uff0c\u76f4\u5230\u5206\u7c7b\u7ed3\u675f\ndef split(node, max_depth, min_size, n_features, depth):  # max_depth = 10, min_size = 1, n_features=int(sqrt((len(dataset[0])-1)\n    left, right = node['groups']\n    del(node['groups'])\n# check for a no split\n    if not left or not right:\n        node['left'] = node['right'] = to_terminal(left + right)\n        return\n# check for max depth\n    if depth >= max_depth:   # max_depth=10 \u8868\u793a\u9012\u5f52\u5341\u6b21\uff0c\u82e5\u5206\u7c7b\u8fd8\u672a\u7ed3\u675f\uff0c\u5219\u9009\u53d6\u6570\u636e\u4e2d\u5206\u7c7b\u6807\u7b7e\u8f83\u591a\u7684\u4f5c\u4e3a\u7ed3\u679c\uff0c\u4f7f\u5206\u7c7b\u63d0\u524d\u7ed3\u675f\uff0c\u9632\u6b62\u8fc7\u62df\u5408\n        node['left'], node['right'] = to_terminal(left), to_terminal(right)\n        return\n# process left child\n    if len(left) <= min_size:\n        node['left'] = to_terminal(left)\n    else:\n        node['left'] = get_split(left, n_features)  # node['left']\u662f\u4e00\u4e2a\u5b57\u5178\uff0c\u5f62\u5f0f\u4e3a{'index':b_index, 'value':b_value, 'groups':b_groups}\uff0c\u6240\u4ee5node\u662f\u4e00\u4e2a\u591a\u5c42\u5b57\u5178\n        split(node['left'], max_depth, min_size, n_features, depth+1)  # \u9012\u5f52\uff0cdepth+1\u8ba1\u7b97\u9012\u5f52\u5c42\u6570\n# process right child\n    if len(right) <= min_size:\n        node['right'] = to_terminal(right)\n    else:\n        node['right'] = get_split(right, n_features)\n        split(node['right'], max_depth, min_size, n_features, depth+1)\n\n\n# Build a decision tree\ndef build_tree(train, max_depth, min_size, n_features):\n    \"\"\"build_tree(\u521b\u5efa\u4e00\u4e2a\u51b3\u7b56\u6811)\n\n    Args:\n        train           \u8bad\u7ec3\u6570\u636e\u96c6\n        max_depth       \u51b3\u7b56\u6811\u6df1\u5ea6\u4e0d\u80fd\u592a\u6df1\uff0c\u4e0d\u7136\u5bb9\u6613\u5bfc\u81f4\u8fc7\u62df\u5408\n        min_size        \u53f6\u5b50\u8282\u70b9\u7684\u5927\u5c0f\n        n_features      \u9009\u53d6\u7684\u7279\u5f81\u7684\u4e2a\u6570\n    Returns:\n        root            \u8fd4\u56de\u51b3\u7b56\u6811\n    \"\"\"\n\n    # \u8fd4\u56de\u6700\u4f18\u5217\u548c\u76f8\u5173\u7684\u4fe1\u606f\n    root = get_split(train, n_features)\n\n    # \u5bf9\u5de6\u53f32\u8fb9\u7684\u6570\u636e \u8fdb\u884c\u9012\u5f52\u7684\u8c03\u7528\uff0c\u7531\u4e8e\u6700\u4f18\u7279\u5f81\u4f7f\u7528\u8fc7\uff0c\u6240\u4ee5\u5728\u540e\u9762\u8fdb\u884c\u4f7f\u7528\u7684\u65f6\u5019\uff0c\u5c31\u6ca1\u6709\u610f\u4e49\u4e86\n    # \u4f8b\u5982:  \u6027\u522b-\u7537\u5973\uff0c\u5bf9\u7537\u4f7f\u7528\u8fd9\u4e00\u7279\u5f81\u5c31\u6ca1\u4efb\u4f55\u610f\u4e49\u4e86\n    split(root, max_depth, min_size, n_features, 1)\n    return root\n\n\n# Make a prediction with a decision tree\ndef predict(node, row):   # \u9884\u6d4b\u6a21\u578b\u5206\u7c7b\u7ed3\u679c\n    if row[node['index']] < node['value']:\n        if isinstance(node['left'], dict):       # isinstance \u662f Python \u4e2d\u7684\u4e00\u4e2a\u5185\u5efa\u51fd\u6570\u3002\u662f\u7528\u6765\u5224\u65ad\u4e00\u4e2a\u5bf9\u8c61\u662f\u5426\u662f\u4e00\u4e2a\u5df2\u77e5\u7684\u7c7b\u578b\u3002\n            return predict(node['left'], row)\n        else:\n            return node['left']\n    else:\n        if isinstance(node['right'], dict):\n            return predict(node['right'], row)\n        else:\n            return node['right']\n\n\n# Make a prediction with a list of bagged trees\ndef bagging_predict(trees, row):\n    \"\"\"bagging_predict(bagging\u9884\u6d4b)\n\n    Args:\n        trees           \u51b3\u7b56\u6811\u7684\u96c6\u5408\n        row             \u6d4b\u8bd5\u6570\u636e\u96c6\u7684\u6bcf\u4e00\u884c\u6570\u636e\n    Returns:\n        \u8fd4\u56de\u968f\u673a\u68ee\u6797\u4e2d\uff0c\u51b3\u7b56\u6811\u7ed3\u679c\u51fa\u73b0\u6b21\u6570\u505a\u5927\u7684\n    \"\"\"\n\n    # \u4f7f\u7528\u591a\u4e2a\u51b3\u7b56\u6811trees\u5bf9\u6d4b\u8bd5\u96c6test\u7684\u7b2crow\u884c\u8fdb\u884c\u9884\u6d4b\uff0c\u518d\u4f7f\u7528\u7b80\u5355\u6295\u7968\u6cd5\u5224\u65ad\u51fa\u8be5\u884c\u6240\u5c5e\u5206\u7c7b\n    predictions = [predict(tree, row) for tree in trees]\n    return max(set(predictions), key=predictions.count)\n\n\n# Create a random subsample from the dataset with replacement\ndef subsample(dataset, ratio):   # \u521b\u5efa\u6570\u636e\u96c6\u7684\u968f\u673a\u5b50\u6837\u672c\n    \"\"\"random_forest(\u8bc4\u4f30\u7b97\u6cd5\u6027\u80fd\uff0c\u8fd4\u56de\u6a21\u578b\u5f97\u5206)\n\n    Args:\n        dataset         \u8bad\u7ec3\u6570\u636e\u96c6\n        ratio           \u8bad\u7ec3\u6570\u636e\u96c6\u7684\u6837\u672c\u6bd4\u4f8b\n    Returns:\n        sample          \u968f\u673a\u62bd\u6837\u7684\u8bad\u7ec3\u6837\u672c\n    \"\"\"\n\n    sample = list()\n    # \u8bad\u7ec3\u6837\u672c\u7684\u6309\u6bd4\u4f8b\u62bd\u6837\u3002\n    # round() \u65b9\u6cd5\u8fd4\u56de\u6d6e\u70b9\u6570x\u7684\u56db\u820d\u4e94\u5165\u503c\u3002\n    n_sample = round(len(dataset) * ratio)\n    while len(sample) < n_sample:\n        # \u6709\u653e\u56de\u7684\u968f\u673a\u91c7\u6837\uff0c\u6709\u4e00\u4e9b\u6837\u672c\u88ab\u91cd\u590d\u91c7\u6837\uff0c\u4ece\u800c\u5728\u8bad\u7ec3\u96c6\u4e2d\u591a\u6b21\u51fa\u73b0\uff0c\u6709\u7684\u5219\u4ece\u672a\u5728\u8bad\u7ec3\u96c6\u4e2d\u51fa\u73b0\uff0c\u6b64\u5219\u81ea\u52a9\u91c7\u6837\u6cd5\u3002\u4ece\u800c\u4fdd\u8bc1\u6bcf\u68f5\u51b3\u7b56\u6811\u8bad\u7ec3\u96c6\u7684\u5dee\u5f02\u6027\n        index = randrange(len(dataset))\n        sample.append(dataset[index])\n    return sample\n\n\n# Random Forest Algorithm\ndef random_forest(train, test, max_depth, min_size, sample_size, n_trees, n_features):\n    \"\"\"random_forest(\u8bc4\u4f30\u7b97\u6cd5\u6027\u80fd\uff0c\u8fd4\u56de\u6a21\u578b\u5f97\u5206)\n\n    Args:\n        train           \u8bad\u7ec3\u6570\u636e\u96c6\n        test            \u6d4b\u8bd5\u6570\u636e\u96c6\n        max_depth       \u51b3\u7b56\u6811\u6df1\u5ea6\u4e0d\u80fd\u592a\u6df1\uff0c\u4e0d\u7136\u5bb9\u6613\u5bfc\u81f4\u8fc7\u62df\u5408\n        min_size        \u53f6\u5b50\u8282\u70b9\u7684\u5927\u5c0f\n        sample_size     \u8bad\u7ec3\u6570\u636e\u96c6\u7684\u6837\u672c\u6bd4\u4f8b\n        n_trees         \u51b3\u7b56\u6811\u7684\u4e2a\u6570\n        n_features      \u9009\u53d6\u7684\u7279\u5f81\u7684\u4e2a\u6570\n    Returns:\n        predictions     \u6bcf\u4e00\u884c\u7684\u9884\u6d4b\u7ed3\u679c\uff0cbagging \u9884\u6d4b\u6700\u540e\u7684\u5206\u7c7b\u7ed3\u679c\n    \"\"\"\n\n    trees = list()\n    # n_trees \u8868\u793a\u51b3\u7b56\u6811\u7684\u6570\u91cf\n    for i in range(n_trees):\n        # \u968f\u673a\u62bd\u6837\u7684\u8bad\u7ec3\u6837\u672c\uff0c \u968f\u673a\u91c7\u6837\u4fdd\u8bc1\u4e86\u6bcf\u68f5\u51b3\u7b56\u6811\u8bad\u7ec3\u96c6\u7684\u5dee\u5f02\u6027\n        sample = subsample(train, sample_size)\n        # \u521b\u5efa\u4e00\u4e2a\u51b3\u7b56\u6811\n        tree = build_tree(sample, max_depth, min_size, n_features)\n        trees.append(tree)\n\n    # \u6bcf\u4e00\u884c\u7684\u9884\u6d4b\u7ed3\u679c\uff0cbagging \u9884\u6d4b\u6700\u540e\u7684\u5206\u7c7b\u7ed3\u679c\n    predictions = [bagging_predict(trees, row) for row in test]\n    return predictions\n\n\n# Calculate accuracy percentage\ndef accuracy_metric(actual, predicted):  # \u5bfc\u5165\u5b9e\u9645\u503c\u548c\u9884\u6d4b\u503c\uff0c\u8ba1\u7b97\u7cbe\u786e\u5ea6\n    correct = 0\n    for i in range(len(actual)):\n        if actual[i] == predicted[i]:\n            correct += 1\n    return correct / float(len(actual)) * 100.0\n\n\n# \u8bc4\u4f30\u7b97\u6cd5\u6027\u80fd\uff0c\u8fd4\u56de\u6a21\u578b\u5f97\u5206\ndef evaluate_algorithm(dataset, algorithm, n_folds, *args):\n    \"\"\"evaluate_algorithm(\u8bc4\u4f30\u7b97\u6cd5\u6027\u80fd\uff0c\u8fd4\u56de\u6a21\u578b\u5f97\u5206)\n\n    Args:\n        dataset     \u539f\u59cb\u6570\u636e\u96c6\n        algorithm   \u4f7f\u7528\u7684\u7b97\u6cd5\n        n_folds     \u6570\u636e\u7684\u4efd\u6570\n        *args       \u5176\u4ed6\u7684\u53c2\u6570\n    Returns:\n        scores      \u6a21\u578b\u5f97\u5206\n    \"\"\"\n\n    # \u5c06\u6570\u636e\u96c6\u8fdb\u884c\u62bd\u91cd\u62bd\u6837 n_folds \u4efd\uff0c\u6570\u636e\u53ef\u4ee5\u91cd\u590d\u91cd\u590d\u62bd\u53d6\uff0c\u6bcf\u4e00\u6b21 list \u7684\u5143\u7d20\u662f\u65e0\u91cd\u590d\u7684\n    folds = cross_validation_split(dataset, n_folds)\n    scores = list()\n    # \u6bcf\u6b21\u5faa\u73af\u4ece folds \u4ece\u53d6\u51fa\u4e00\u4e2a fold \u4f5c\u4e3a\u6d4b\u8bd5\u96c6\uff0c\u5176\u4f59\u4f5c\u4e3a\u8bad\u7ec3\u96c6\uff0c\u904d\u5386\u6574\u4e2a folds \uff0c\u5b9e\u73b0\u4ea4\u53c9\u9a8c\u8bc1\n    for fold in folds:\n        train_set = list(folds)\n        train_set.remove(fold)\n        # \u5c06\u591a\u4e2a fold \u5217\u8868\u7ec4\u5408\u6210\u4e00\u4e2a train_set \u5217\u8868, \u7c7b\u4f3c union all\n        \"\"\"\n        In [20]: l1=[[1, 2, 'a'], [11, 22, 'b']]\n        In [21]: l2=[[3, 4, 'c'], [33, 44, 'd']]\n        In [22]: l=[]\n        In [23]: l.append(l1)\n        In [24]: l.append(l2)\n        In [25]: l\n        Out[25]: [[[1, 2, 'a'], [11, 22, 'b']], [[3, 4, 'c'], [33, 44, 'd']]]\n        In [26]: sum(l, [])\n        Out[26]: [[1, 2, 'a'], [11, 22, 'b'], [3, 4, 'c'], [33, 44, 'd']]\n        \"\"\"\n        train_set = sum(train_set, [])\n        test_set = list()\n        # fold \u8868\u793a\u4ece\u539f\u59cb\u6570\u636e\u96c6 dataset \u63d0\u53d6\u51fa\u6765\u7684\u6d4b\u8bd5\u96c6\n        for row in fold:\n            row_copy = list(row)\n            row_copy[-1] = None\n            test_set.append(row_copy)\n        predicted = algorithm(train_set, test_set, *args)\n        actual = [row[-1] for row in fold]\n\n        # \u8ba1\u7b97\u968f\u673a\u68ee\u6797\u7684\u9884\u6d4b\u7ed3\u679c\u7684\u6b63\u786e\u7387\n        accuracy = accuracy_metric(actual, predicted)\n        scores.append(accuracy)\n    return scores\n\n\nif __name__ == '__main__':\n\n    # \u52a0\u8f7d\u6570\u636e\n    dataset = loadDataSet('data/7.RandomForest/sonar-all-data.txt')\n    # print dataset\n\n    n_folds = 5        # \u5206\u62105\u4efd\u6570\u636e\uff0c\u8fdb\u884c\u4ea4\u53c9\u9a8c\u8bc1\n    max_depth = 20     # \u8c03\u53c2\uff08\u81ea\u5df1\u4fee\u6539\uff09 #\u51b3\u7b56\u6811\u6df1\u5ea6\u4e0d\u80fd\u592a\u6df1\uff0c\u4e0d\u7136\u5bb9\u6613\u5bfc\u81f4\u8fc7\u62df\u5408\n    min_size = 1       # \u51b3\u7b56\u6811\u7684\u53f6\u5b50\u8282\u70b9\u6700\u5c11\u7684\u5143\u7d20\u6570\u91cf\n    sample_size = 1.0  # \u505a\u51b3\u7b56\u6811\u65f6\u5019\u7684\u6837\u672c\u7684\u6bd4\u4f8b\n    # n_features = int((len(dataset[0])-1))\n    n_features = 15     # \u8c03\u53c2\uff08\u81ea\u5df1\u4fee\u6539\uff09 #\u51c6\u786e\u6027\u4e0e\u591a\u6837\u6027\u4e4b\u95f4\u7684\u6743\u8861\n    for n_trees in [1, 10, 20, 30, 40, 50]:  # \u7406\u8bba\u4e0a\u6811\u662f\u8d8a\u591a\u8d8a\u597d\n        scores = evaluate_algorithm(dataset, random_forest, n_folds, max_depth, min_size, sample_size, n_trees, n_features)\n        # \u6bcf\u4e00\u6b21\u6267\u884c\u672c\u6587\u4ef6\u65f6\u90fd\u80fd\u4ea7\u751f\u540c\u4e00\u4e2a\u968f\u673a\u6570\n        seed(1)\n        print('random=', random())\n        print('Trees: %d' % n_trees)\n        print('Scores: %s' % scores)\n        print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))\n", "src/py2.x/ml/6.SVM/svm-complete.py": "#!/usr/bin/python\n# coding:utf8\n\n\"\"\"\nCreated on Nov 4, 2010\nUpdate on 2017-05-18\nChapter 5 source file for Machine Learing in Action\nAuthor: Peter/geekidentity/\u7247\u523b\nGitHub: https://github.com/apachecn/AiLearning\n\"\"\"\nfrom __future__ import print_function\nfrom numpy import *\nimport matplotlib.pyplot as plt\n\n\nclass optStruct:\n    \"\"\"\n    \u5efa\u7acb\u7684\u6570\u636e\u7ed3\u6784\u6765\u4fdd\u5b58\u6240\u6709\u7684\u91cd\u8981\u503c\n    \"\"\"\n    def __init__(self, dataMatIn, classLabels, C, toler, kTup):\n        \"\"\"\n        Args:\n            dataMatIn    \u6570\u636e\u96c6\n            classLabels  \u7c7b\u522b\u6807\u7b7e\n            C   \u677e\u5f1b\u53d8\u91cf(\u5e38\u91cf\u503c)\uff0c\u5141\u8bb8\u6709\u4e9b\u6570\u636e\u70b9\u53ef\u4ee5\u5904\u4e8e\u5206\u9694\u9762\u7684\u9519\u8bef\u4e00\u4fa7\u3002\n                \u63a7\u5236\u6700\u5927\u5316\u95f4\u9694\u548c\u4fdd\u8bc1\u5927\u90e8\u5206\u7684\u51fd\u6570\u95f4\u9694\u5c0f\u4e8e1.0\u8fd9\u4e24\u4e2a\u76ee\u6807\u7684\u6743\u91cd\u3002\n                \u53ef\u4ee5\u901a\u8fc7\u8c03\u8282\u8be5\u53c2\u6570\u8fbe\u5230\u4e0d\u540c\u7684\u7ed3\u679c\u3002\n            toler   \u5bb9\u9519\u7387\n            kTup    \u5305\u542b\u6838\u51fd\u6570\u4fe1\u606f\u7684\u5143\u7ec4\n        \"\"\"\n\n        self.X = dataMatIn\n        self.labelMat = classLabels\n        self.C = C\n        self.tol = toler\n\n        # \u6570\u636e\u7684\u884c\u6570\n        self.m = shape(dataMatIn)[0]\n        self.alphas = mat(zeros((self.m, 1)))\n        self.b = 0\n\n        # \u8bef\u5dee\u7f13\u5b58\uff0c\u7b2c\u4e00\u5217\u7ed9\u51fa\u7684\u662feCache\u662f\u5426\u6709\u6548\u7684\u6807\u5fd7\u4f4d\uff0c\u7b2c\u4e8c\u5217\u7ed9\u51fa\u7684\u662f\u5b9e\u9645\u7684E\u503c\u3002\n        self.eCache = mat(zeros((self.m, 2)))\n\n        # m\u884cm\u5217\u7684\u77e9\u9635\n        self.K = mat(zeros((self.m, self.m)))\n        for i in range(self.m):\n            self.K[:, i] = kernelTrans(self.X, self.X[i, :], kTup)\n\n\ndef kernelTrans(X, A, kTup):  # calc the kernel or transform data to a higher dimensional space\n    \"\"\"\n    \u6838\u8f6c\u6362\u51fd\u6570\n    Args:\n        X     dataMatIn\u6570\u636e\u96c6\n        A     dataMatIn\u6570\u636e\u96c6\u7684\u7b2ci\u884c\u7684\u6570\u636e\n        kTup  \u6838\u51fd\u6570\u7684\u4fe1\u606f\n\n    Returns:\n\n    \"\"\"\n    m, n = shape(X)\n    K = mat(zeros((m, 1)))\n    if kTup[0] == 'lin':\n        # linear kernel:   m*n * n*1 = m*1\n        K = X * A.T\n    elif kTup[0] == 'rbf':\n        for j in range(m):\n            deltaRow = X[j, :] - A\n            K[j] = deltaRow * deltaRow.T\n        # \u5f84\u5411\u57fa\u51fd\u6570\u7684\u9ad8\u65af\u7248\u672c\n        K = exp(K / (-1 * kTup[1] ** 2))  # divide in NumPy is element-wise not matrix like Matlab\n    else:\n        raise NameError('Houston We Have a Problem -- That Kernel is not recognized')\n    return K\n\n\ndef loadDataSet(fileName):\n    \"\"\"loadDataSet\uff08\u5bf9\u6587\u4ef6\u8fdb\u884c\u9010\u884c\u89e3\u6790\uff0c\u4ece\u800c\u5f97\u5230\u7b2c\u884c\u7684\u7c7b\u6807\u7b7e\u548c\u6574\u4e2a\u6570\u636e\u77e9\u9635\uff09\n\n    Args:\n        fileName \u6587\u4ef6\u540d\n    Returns:\n        dataMat  \u6570\u636e\u77e9\u9635\n        labelMat \u7c7b\u6807\u7b7e\n    \"\"\"\n    dataMat = []\n    labelMat = []\n    fr = open(fileName)\n    for line in fr.readlines():\n        lineArr = line.strip().split('\\t')\n        dataMat.append([float(lineArr[0]), float(lineArr[1])])\n        labelMat.append(float(lineArr[2]))\n    return dataMat, labelMat\n\n\ndef calcEk(oS, k):\n    \"\"\"calcEk\uff08\u6c42 Ek\u8bef\u5dee: \u9884\u6d4b\u503c-\u771f\u5b9e\u503c\u7684\u5dee\uff09\n\n    \u8be5\u8fc7\u7a0b\u5728\u5b8c\u6574\u7248\u7684SMO\u7b97\u6cd5\u4e2d\u966a\u51fa\u73b0\u6b21\u6570\u8f83\u591a\uff0c\u56e0\u6b64\u5c06\u5176\u5355\u72ec\u4f5c\u4e3a\u4e00\u4e2a\u65b9\u6cd5\n    Args:\n        oS  optStruct\u5bf9\u8c61\n        k   \u5177\u4f53\u7684\u67d0\u4e00\u884c\n\n    Returns:\n        Ek  \u9884\u6d4b\u7ed3\u679c\u4e0e\u771f\u5b9e\u7ed3\u679c\u6bd4\u5bf9\uff0c\u8ba1\u7b97\u8bef\u5deeEk\n    \"\"\"\n    fXk = float(multiply(oS.alphas, oS.labelMat).T * oS.K[:, k] + oS.b)\n    Ek = fXk - float(oS.labelMat[k])\n    return Ek\n\n\ndef selectJrand(i, m):\n    \"\"\"\n    \u968f\u673a\u9009\u62e9\u4e00\u4e2a\u6574\u6570\n    Args:\n        i  \u7b2c\u4e00\u4e2aalpha\u7684\u4e0b\u6807\n        m  \u6240\u6709alpha\u7684\u6570\u76ee\n    Returns:\n        j  \u8fd4\u56de\u4e00\u4e2a\u4e0d\u4e3ai\u7684\u968f\u673a\u6570\uff0c\u57280~m\u4e4b\u95f4\u7684\u6574\u6570\u503c\n    \"\"\"\n    j = i\n    while j == i:\n        j = int(random.uniform(0, m))\n    return j\n\n\ndef selectJ(i, oS, Ei):  # this is the second choice -heurstic, and calcs Ej\n    \"\"\"selectJ\uff08\u8fd4\u56de\u6700\u4f18\u7684j\u548cEj\uff09\n\n    \u5185\u5faa\u73af\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\u3002\n    \u9009\u62e9\u7b2c\u4e8c\u4e2a(\u5185\u5faa\u73af)alpha\u7684alpha\u503c\n    \u8fd9\u91cc\u7684\u76ee\u6807\u662f\u9009\u62e9\u5408\u9002\u7684\u7b2c\u4e8c\u4e2aalpha\u503c\u4ee5\u4fdd\u8bc1\u6bcf\u6b21\u4f18\u5316\u4e2d\u91c7\u7528\u6700\u5927\u6b65\u957f\u3002\n    \u8be5\u51fd\u6570\u7684\u8bef\u5dee\u4e0e\u7b2c\u4e00\u4e2aalpha\u503cEi\u548c\u4e0b\u6807i\u6709\u5173\u3002\n    Args:\n        i   \u5177\u4f53\u7684\u7b2ci\u4e00\u884c\n        oS  optStruct\u5bf9\u8c61\n        Ei  \u9884\u6d4b\u7ed3\u679c\u4e0e\u771f\u5b9e\u7ed3\u679c\u6bd4\u5bf9\uff0c\u8ba1\u7b97\u8bef\u5deeEi\n\n    Returns:\n        j  \u968f\u673a\u9009\u51fa\u7684\u7b2cj\u4e00\u884c\n        Ej \u9884\u6d4b\u7ed3\u679c\u4e0e\u771f\u5b9e\u7ed3\u679c\u6bd4\u5bf9\uff0c\u8ba1\u7b97\u8bef\u5deeEj\n    \"\"\"\n    maxK = -1\n    maxDeltaE = 0\n    Ej = 0\n    # \u9996\u5148\u5c06\u8f93\u5165\u503cEi\u5728\u7f13\u5b58\u4e2d\u8bbe\u7f6e\u6210\u4e3a\u6709\u6548\u7684\u3002\u8fd9\u91cc\u7684\u6709\u6548\u610f\u5473\u7740\u5b83\u5df2\u7ecf\u8ba1\u7b97\u597d\u4e86\u3002\n    oS.eCache[i] = [1, Ei]\n\n    # print 'oS.eCache[%s]=%s' % (i, oS.eCache[i])\n    # print 'oS.eCache[:, 0].A=%s' % oS.eCache[:, 0].A.T\n    # \"\"\"\n    # # \u8fd4\u56de\u975e0\u7684: \u884c\u5217\u503c\n    # nonzero(oS.eCache[:, 0].A)= (\n    #     \u884c:  array([ 0,  2,  4,  5,  8, 10, 17, 18, 20, 21, 23, 25, 26, 29, 30, 39, 46,52, 54, 55, 62, 69, 70, 76, 79, 82, 94, 97]), \n    #     \u5217:  array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,0, 0, 0, 0, 0])\n    # )\n    # \"\"\"\n    # print 'nonzero(oS.eCache[:, 0].A)=', nonzero(oS.eCache[:, 0].A)\n    # # \u53d6\u884c\u7684list\n    # print 'nonzero(oS.eCache[:, 0].A)[0]=', nonzero(oS.eCache[:, 0].A)[0]\n    # \u975e\u96f6E\u503c\u7684\u884c\u7684list\u5217\u8868\uff0c\u6240\u5bf9\u5e94\u7684alpha\u503c\n    validEcacheList = nonzero(oS.eCache[:, 0].A)[0]\n    if (len(validEcacheList)) > 1:\n        for k in validEcacheList:  # \u5728\u6240\u6709\u7684\u503c\u4e0a\u8fdb\u884c\u5faa\u73af\uff0c\u5e76\u9009\u62e9\u5176\u4e2d\u4f7f\u5f97\u6539\u53d8\u6700\u5927\u7684\u90a3\u4e2a\u503c\n            if k == i:\n                continue  # don't calc for i, waste of time\n\n            # \u6c42 Ek\u8bef\u5dee: \u9884\u6d4b\u503c-\u771f\u5b9e\u503c\u7684\u5dee\n            Ek = calcEk(oS, k)\n            deltaE = abs(Ei - Ek)\n            if (deltaE > maxDeltaE):\n                # \u9009\u62e9\u5177\u6709\u6700\u5927\u6b65\u957f\u7684j\n                maxK = k\n                maxDeltaE = deltaE\n                Ej = Ek\n        return maxK, Ej\n    else:  # \u5982\u679c\u662f\u7b2c\u4e00\u6b21\u5faa\u73af\uff0c\u5219\u968f\u673a\u9009\u62e9\u4e00\u4e2aalpha\u503c\n        j = selectJrand(i, oS.m)\n\n        # \u6c42 Ek\u8bef\u5dee: \u9884\u6d4b\u503c-\u771f\u5b9e\u503c\u7684\u5dee\n        Ej = calcEk(oS, j)\n    return j, Ej\n\n\ndef updateEk(oS, k):\n    \"\"\"updateEk\uff08\u8ba1\u7b97\u8bef\u5dee\u503c\u5e76\u5b58\u5165\u7f13\u5b58\u4e2d\u3002\uff09\n\n    \u5728\u5bf9alpha\u503c\u8fdb\u884c\u4f18\u5316\u4e4b\u540e\u4f1a\u7528\u5230\u8fd9\u4e2a\u503c\u3002\n    Args:\n        oS  optStruct\u5bf9\u8c61\n        k   \u67d0\u4e00\u5217\u7684\u884c\u53f7\n    \"\"\"\n\n    # \u6c42 \u8bef\u5dee: \u9884\u6d4b\u503c-\u771f\u5b9e\u503c\u7684\u5dee\n    Ek = calcEk(oS, k)\n    oS.eCache[k] = [1, Ek]\n\n\ndef clipAlpha(aj, H, L):\n    \"\"\"clipAlpha(\u8c03\u6574aj\u7684\u503c\uff0c\u4f7faj\u5904\u4e8e L<=aj<=H)\n    Args:\n        aj  \u76ee\u6807\u503c\n        H   \u6700\u5927\u503c\n        L   \u6700\u5c0f\u503c\n    Returns:\n        aj  \u76ee\u6807\u503c\n    \"\"\"\n    if aj > H:\n        aj = H\n    if L > aj:\n        aj = L\n    return aj\n\n\ndef innerL(i, oS):\n    \"\"\"innerL\n    \u5185\u5faa\u73af\u4ee3\u7801\n    Args:\n        i   \u5177\u4f53\u7684\u67d0\u4e00\u884c\n        oS  optStruct\u5bf9\u8c61\n\n    Returns:\n        0   \u627e\u4e0d\u5230\u6700\u4f18\u7684\u503c\n        1   \u627e\u5230\u4e86\u6700\u4f18\u7684\u503c\uff0c\u5e76\u4e14oS.Cache\u5230\u7f13\u5b58\u4e2d\n    \"\"\"\n\n    # \u6c42 Ek\u8bef\u5dee: \u9884\u6d4b\u503c-\u771f\u5b9e\u503c\u7684\u5dee\n    Ei = calcEk(oS, i)\n\n    # \u7ea6\u675f\u6761\u4ef6 (KKT\u6761\u4ef6\u662f\u89e3\u51b3\u6700\u4f18\u5316\u95ee\u9898\u7684\u65f6\u7528\u5230\u7684\u4e00\u79cd\u65b9\u6cd5\u3002\u6211\u4eec\u8fd9\u91cc\u63d0\u5230\u7684\u6700\u4f18\u5316\u95ee\u9898\u901a\u5e38\u662f\u6307\u5bf9\u4e8e\u7ed9\u5b9a\u7684\u67d0\u4e00\u51fd\u6570\uff0c\u6c42\u5176\u5728\u6307\u5b9a\u4f5c\u7528\u57df\u4e0a\u7684\u5168\u5c40\u6700\u5c0f\u503c)\n    # 0<=alphas[i]<=C\uff0c\u4f46\u7531\u4e8e0\u548cC\u662f\u8fb9\u754c\u503c\uff0c\u6211\u4eec\u65e0\u6cd5\u8fdb\u884c\u4f18\u5316\uff0c\u56e0\u4e3a\u9700\u8981\u589e\u52a0\u4e00\u4e2aalphas\u548c\u964d\u4f4e\u4e00\u4e2aalphas\u3002\n    # \u8868\u793a\u53d1\u751f\u9519\u8bef\u7684\u6982\u7387: labelMat[i]*Ei \u5982\u679c\u8d85\u51fa\u4e86 toler\uff0c \u624d\u9700\u8981\u4f18\u5316\u3002\u81f3\u4e8e\u6b63\u8d1f\u53f7\uff0c\u6211\u4eec\u8003\u8651\u7edd\u5bf9\u503c\u5c31\u5bf9\u4e86\u3002\n    '''\n    # \u68c0\u9a8c\u8bad\u7ec3\u6837\u672c(xi, yi)\u662f\u5426\u6ee1\u8db3KKT\u6761\u4ef6\n    yi*f(i) >= 1 and alpha = 0 (outside the boundary)\n    yi*f(i) == 1 and 0<alpha< C (on the boundary)\n    yi*f(i) <= 1 and alpha = C (between the boundary)\n    '''\n    if ((oS.labelMat[i] * Ei < -oS.tol) and (oS.alphas[i] < oS.C)) or ((oS.labelMat[i] * Ei > oS.tol) and (oS.alphas[i] > 0)):\n        # \u9009\u62e9\u6700\u5927\u7684\u8bef\u5dee\u5bf9\u5e94\u7684j\u8fdb\u884c\u4f18\u5316\u3002\u6548\u679c\u66f4\u660e\u663e\n        j, Ej = selectJ(i, oS, Ei)\n        alphaIold = oS.alphas[i].copy()\n        alphaJold = oS.alphas[j].copy()\n\n        # L\u548cH\u7528\u4e8e\u5c06alphas[j]\u8c03\u6574\u52300-C\u4e4b\u95f4\u3002\u5982\u679cL==H\uff0c\u5c31\u4e0d\u505a\u4efb\u4f55\u6539\u53d8\uff0c\u76f4\u63a5return 0\n        if (oS.labelMat[i] != oS.labelMat[j]):\n            L = max(0, oS.alphas[j] - oS.alphas[i])\n            H = min(oS.C, oS.C + oS.alphas[j] - oS.alphas[i])\n        else:\n            L = max(0, oS.alphas[j] + oS.alphas[i] - oS.C)\n            H = min(oS.C, oS.alphas[j] + oS.alphas[i])\n        if L == H:\n            # print(\"L==H\")\n            return 0\n\n        # eta\u662falphas[j]\u7684\u6700\u4f18\u4fee\u6539\u91cf\uff0c\u5982\u679ceta==0\uff0c\u9700\u8981\u9000\u51fafor\u5faa\u73af\u7684\u5f53\u524d\u8fed\u4ee3\u8fc7\u7a0b\n        # \u53c2\u8003\u300a\u7edf\u8ba1\u5b66\u4e60\u65b9\u6cd5\u300b\u674e\u822a-P125~P128<\u5e8f\u5217\u6700\u5c0f\u6700\u4f18\u5316\u7b97\u6cd5>\n        eta = 2.0 * oS.K[i, j] - oS.K[i, i] - oS.K[j, j]  # changed for kernel\n        if eta >= 0:\n            print(\"eta>=0\")\n            return 0\n\n        # \u8ba1\u7b97\u51fa\u4e00\u4e2a\u65b0\u7684alphas[j]\u503c\n        oS.alphas[j] -= oS.labelMat[j] * (Ei - Ej) / eta\n        # \u5e76\u4f7f\u7528\u8f85\u52a9\u51fd\u6570\uff0c\u4ee5\u53caL\u548cH\u5bf9\u5176\u8fdb\u884c\u8c03\u6574\n        oS.alphas[j] = clipAlpha(oS.alphas[j], H, L)\n        # \u66f4\u65b0\u8bef\u5dee\u7f13\u5b58\n        updateEk(oS, j)\n\n        # \u68c0\u67e5alpha[j]\u662f\u5426\u53ea\u662f\u8f7b\u5fae\u7684\u6539\u53d8\uff0c\u5982\u679c\u662f\u7684\u8bdd\uff0c\u5c31\u9000\u51fafor\u5faa\u73af\u3002\n        if (abs(oS.alphas[j] - alphaJold) < 0.00001):\n            # print(\"j not moving enough\")\n            return 0\n\n        # \u7136\u540ealphas[i]\u548calphas[j]\u540c\u6837\u8fdb\u884c\u6539\u53d8\uff0c\u867d\u7136\u6539\u53d8\u7684\u5927\u5c0f\u4e00\u6837\uff0c\u4f46\u662f\u6539\u53d8\u7684\u65b9\u5411\u6b63\u597d\u76f8\u53cd\n        oS.alphas[i] += oS.labelMat[j] * oS.labelMat[i] * (alphaJold - oS.alphas[j])\n        # \u66f4\u65b0\u8bef\u5dee\u7f13\u5b58\n        updateEk(oS, i)\n\n        # \u5728\u5bf9alpha[i], alpha[j] \u8fdb\u884c\u4f18\u5316\u4e4b\u540e\uff0c\u7ed9\u8fd9\u4e24\u4e2aalpha\u503c\u8bbe\u7f6e\u4e00\u4e2a\u5e38\u6570b\u3002\n        # w= \u03a3[1~n] ai*yi*xi => b = yi- \u03a3[1~n] ai*yi(xi*xj)\n        # \u6240\u4ee5:   b1 - b = (y1-y) - \u03a3[1~n] yi*(a1-a)*(xi*x1)\n        # \u4e3a\u4ec0\u4e48\u51cf2\u904d\uff1f \u56e0\u4e3a\u662f \u51cf\u53bb\u03a3[1~n]\uff0c\u6b63\u597d2\u4e2a\u53d8\u91cfi\u548cj\uff0c\u6240\u4ee5\u51cf2\u904d\n        b1 = oS.b - Ei - oS.labelMat[i] * (oS.alphas[i] - alphaIold) * oS.K[i, i] - oS.labelMat[j] * (oS.alphas[j] - alphaJold) * oS.K[i, j]\n        b2 = oS.b - Ej - oS.labelMat[i] * (oS.alphas[i] - alphaIold) * oS.K[i, j] - oS.labelMat[j] * (oS.alphas[j] - alphaJold) * oS.K[j, j]\n        if (0 < oS.alphas[i]) and (oS.C > oS.alphas[i]):\n            oS.b = b1\n        elif (0 < oS.alphas[j]) and (oS.C > oS.alphas[j]):\n            oS.b = b2\n        else:\n            oS.b = (b1 + b2) / 2.0\n        return 1\n    else:\n        return 0\n\n\ndef smoP(dataMatIn, classLabels, C, toler, maxIter, kTup=('lin', 0)):\n    \"\"\"\n    \u5b8c\u6574SMO\u7b97\u6cd5\u5916\u5faa\u73af\uff0c\u4e0esmoSimple\u6709\u4e9b\u7c7b\u4f3c\uff0c\u4f46\u8fd9\u91cc\u7684\u5faa\u73af\u9000\u51fa\u6761\u4ef6\u66f4\u591a\u4e00\u4e9b\n    Args:\n        dataMatIn    \u6570\u636e\u96c6\n        classLabels  \u7c7b\u522b\u6807\u7b7e\n        C   \u677e\u5f1b\u53d8\u91cf(\u5e38\u91cf\u503c)\uff0c\u5141\u8bb8\u6709\u4e9b\u6570\u636e\u70b9\u53ef\u4ee5\u5904\u4e8e\u5206\u9694\u9762\u7684\u9519\u8bef\u4e00\u4fa7\u3002\n            \u63a7\u5236\u6700\u5927\u5316\u95f4\u9694\u548c\u4fdd\u8bc1\u5927\u90e8\u5206\u7684\u51fd\u6570\u95f4\u9694\u5c0f\u4e8e1.0\u8fd9\u4e24\u4e2a\u76ee\u6807\u7684\u6743\u91cd\u3002\n            \u53ef\u4ee5\u901a\u8fc7\u8c03\u8282\u8be5\u53c2\u6570\u8fbe\u5230\u4e0d\u540c\u7684\u7ed3\u679c\u3002\n        toler   \u5bb9\u9519\u7387\n        maxIter \u9000\u51fa\u524d\u6700\u5927\u7684\u5faa\u73af\u6b21\u6570\n        kTup    \u5305\u542b\u6838\u51fd\u6570\u4fe1\u606f\u7684\u5143\u7ec4\n    Returns:\n        b       \u6a21\u578b\u7684\u5e38\u91cf\u503c\n        alphas  \u62c9\u683c\u6717\u65e5\u4e58\u5b50\n    \"\"\"\n\n    # \u521b\u5efa\u4e00\u4e2a optStruct \u5bf9\u8c61\n    oS = optStruct(mat(dataMatIn), mat(classLabels).transpose(), C, toler, kTup)\n    iter = 0\n    entireSet = True\n    alphaPairsChanged = 0\n\n    # \u5faa\u73af\u904d\u5386: \u5faa\u73afmaxIter\u6b21 \u5e76\u4e14 \uff08alphaPairsChanged\u5b58\u5728\u53ef\u4ee5\u6539\u53d8 or \u6240\u6709\u884c\u904d\u5386\u4e00\u904d\uff09\n    while (iter < maxIter) and ((alphaPairsChanged > 0) or (entireSet)):\n        alphaPairsChanged = 0\n\n        #  \u5f53entireSet=true or \u975e\u8fb9\u754calpha\u5bf9\u6ca1\u6709\u4e86\uff1b\u5c31\u5f00\u59cb\u5bfb\u627e alpha\u5bf9\uff0c\u7136\u540e\u51b3\u5b9a\u662f\u5426\u8981\u8fdb\u884celse\u3002\n        if entireSet:\n            # \u5728\u6570\u636e\u96c6\u4e0a\u904d\u5386\u6240\u6709\u53ef\u80fd\u7684alpha\n            for i in range(oS.m):\n                # \u662f\u5426\u5b58\u5728alpha\u5bf9\uff0c\u5b58\u5728\u5c31+1\n                alphaPairsChanged += innerL(i, oS)\n                # print(\"fullSet, iter: %d i:%d, pairs changed %d\" % (iter, i, alphaPairsChanged))\n            iter += 1\n\n        # \u5bf9\u5df2\u5b58\u5728 alpha\u5bf9\uff0c\u9009\u51fa\u975e\u8fb9\u754c\u7684alpha\u503c\uff0c\u8fdb\u884c\u4f18\u5316\u3002\n        else:\n            # \u904d\u5386\u6240\u6709\u7684\u975e\u8fb9\u754calpha\u503c\uff0c\u4e5f\u5c31\u662f\u4e0d\u5728\u8fb9\u754c0\u6216C\u4e0a\u7684\u503c\u3002\n            nonBoundIs = nonzero((oS.alphas.A > 0) * (oS.alphas.A < C))[0]\n            for i in nonBoundIs:\n                alphaPairsChanged += innerL(i, oS)\n                # print(\"non-bound, iter: %d i:%d, pairs changed %d\" % (iter, i, alphaPairsChanged))\n            iter += 1\n\n        # \u5982\u679c\u627e\u5230alpha\u5bf9\uff0c\u5c31\u4f18\u5316\u975e\u8fb9\u754calpha\u503c\uff0c\u5426\u5219\uff0c\u5c31\u91cd\u65b0\u8fdb\u884c\u5bfb\u627e\uff0c\u5982\u679c\u5bfb\u627e\u4e00\u904d \u904d\u5386\u6240\u6709\u7684\u884c\u8fd8\u662f\u6ca1\u627e\u5230\uff0c\u5c31\u9000\u51fa\u5faa\u73af\u3002\n        if entireSet:\n            entireSet = False  # toggle entire set loop\n        elif (alphaPairsChanged == 0):\n            entireSet = True\n        print(\"iteration number: %d\" % iter)\n    return oS.b, oS.alphas\n\n\ndef calcWs(alphas, dataArr, classLabels):\n    \"\"\"\n    \u57fa\u4e8ealpha\u8ba1\u7b97w\u503c\n    Args:\n        alphas        \u62c9\u683c\u6717\u65e5\u4e58\u5b50\n        dataArr       feature\u6570\u636e\u96c6\n        classLabels   \u76ee\u6807\u53d8\u91cf\u6570\u636e\u96c6\n\n    Returns:\n        wc  \u56de\u5f52\u7cfb\u6570\n    \"\"\"\n    X = mat(dataArr)\n    labelMat = mat(classLabels).transpose()\n    m, n = shape(X)\n    w = zeros((n, 1))\n    for i in range(m):\n        w += multiply(alphas[i] * labelMat[i], X[i, :].T)\n    return w\n\n\ndef testRbf(k1=1.3):\n    dataArr, labelArr = loadDataSet('data/6.SVM/testSetRBF.txt')\n    b, alphas = smoP(dataArr, labelArr, 200, 0.0001, 10000, ('rbf', k1))  # C=200 important\n    datMat = mat(dataArr)\n    labelMat = mat(labelArr).transpose()\n    svInd = nonzero(alphas.A > 0)[0]\n    sVs = datMat[svInd]  # get matrix of only support vectors\n    labelSV = labelMat[svInd]\n    print(\"there are %d Support Vectors\" % shape(sVs)[0])\n    m, n = shape(datMat)\n    errorCount = 0\n    for i in range(m):\n        kernelEval = kernelTrans(sVs, datMat[i, :], ('rbf', k1))\n\n        # \u548c\u8fd9\u4e2asvm-simple\u7c7b\u4f3c:  fXi = float(multiply(alphas, labelMat).T*(dataMatrix*dataMatrix[i, :].T)) + b\n        predict = kernelEval.T * multiply(labelSV, alphas[svInd]) + b\n        if sign(predict) != sign(labelArr[i]):\n            errorCount += 1\n    print(\"the training error rate is: %f\" % (float(errorCount) / m))\n\n    dataArr, labelArr = loadDataSet('data/6.SVM/testSetRBF2.txt')\n    errorCount = 0\n    datMat = mat(dataArr)\n    labelMat = mat(labelArr).transpose()\n    m, n = shape(datMat)\n    for i in range(m):\n        kernelEval = kernelTrans(sVs, datMat[i, :], ('rbf', k1))\n        predict = kernelEval.T * multiply(labelSV, alphas[svInd]) + b\n        if sign(predict) != sign(labelArr[i]):\n            errorCount += 1\n    print(\"the test error rate is: %f\" % (float(errorCount) / m))\n\n\ndef img2vector(filename):\n    returnVect = zeros((1, 1024))\n    fr = open(filename)\n    for i in range(32):\n        lineStr = fr.readline()\n        for j in range(32):\n            returnVect[0, 32 * i + j] = int(lineStr[j])\n    return returnVect\n\n\ndef loadImages(dirName):\n    from os import listdir\n    hwLabels = []\n    print(dirName)\n    trainingFileList = listdir(dirName)  # load the training set\n    m = len(trainingFileList)\n    trainingMat = zeros((m, 1024))\n    for i in range(m):\n        fileNameStr = trainingFileList[i]\n        fileStr = fileNameStr.split('.')[0]  # take off .txt\n        classNumStr = int(fileStr.split('_')[0])\n        if classNumStr == 9:\n            hwLabels.append(-1)\n        else:\n            hwLabels.append(1)\n        trainingMat[i, :] = img2vector('%s/%s' % (dirName, fileNameStr))\n    return trainingMat, hwLabels\n\n\ndef testDigits(kTup=('rbf', 10)):\n\n    # 1. \u5bfc\u5165\u8bad\u7ec3\u6570\u636e\n    dataArr, labelArr = loadImages('data/6.SVM/trainingDigits')\n    b, alphas = smoP(dataArr, labelArr, 200, 0.0001, 10000, kTup)\n    datMat = mat(dataArr)\n    labelMat = mat(labelArr).transpose()\n    svInd = nonzero(alphas.A > 0)[0]\n    sVs = datMat[svInd]\n    labelSV = labelMat[svInd]\n    # print(\"there are %d Support Vectors\" % shape(sVs)[0])\n    m, n = shape(datMat)\n    errorCount = 0\n    for i in range(m):\n        kernelEval = kernelTrans(sVs, datMat[i, :], kTup)\n        # 1*m * m*1 = 1*1 \u5355\u4e2a\u9884\u6d4b\u7ed3\u679c\n        predict = kernelEval.T * multiply(labelSV, alphas[svInd]) + b\n        if sign(predict) != sign(labelArr[i]): errorCount += 1\n    print(\"the training error rate is: %f\" % (float(errorCount) / m))\n\n    # 2. \u5bfc\u5165\u6d4b\u8bd5\u6570\u636e\n    dataArr, labelArr = loadImages('data/6.SVM/testDigits')\n    errorCount = 0\n    datMat = mat(dataArr)\n    labelMat = mat(labelArr).transpose()\n    m, n = shape(datMat)\n    for i in range(m):\n        kernelEval = kernelTrans(sVs, datMat[i, :], kTup)\n        predict = kernelEval.T * multiply(labelSV, alphas[svInd]) + b\n        if sign(predict) != sign(labelArr[i]): errorCount += 1\n    print(\"the test error rate is: %f\" % (float(errorCount) / m))\n\n\ndef plotfig_SVM(xArr, yArr, ws, b, alphas):\n    \"\"\"\n    \u53c2\u8003\u5730\u5740: \n       http://blog.csdn.net/maoersong/article/details/24315633\n       http://www.cnblogs.com/JustForCS/p/5283489.html\n       http://blog.csdn.net/kkxgx/article/details/6951959\n    \"\"\"\n\n    xMat = mat(xArr)\n    yMat = mat(yArr)\n\n    # b\u539f\u6765\u662f\u77e9\u9635\uff0c\u5148\u8f6c\u4e3a\u6570\u7ec4\u7c7b\u578b\u540e\u5176\u6570\u7ec4\u5927\u5c0f\u4e3a\uff081,1\uff09\uff0c\u6240\u4ee5\u540e\u9762\u52a0[0]\uff0c\u53d8\u4e3a(1,)\n    b = array(b)[0]\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n\n    # \u6ce8\u610fflatten\u7684\u7528\u6cd5\n    ax.scatter(xMat[:, 0].flatten().A[0], xMat[:, 1].flatten().A[0])\n\n    # x\u6700\u5927\u503c\uff0c\u6700\u5c0f\u503c\u6839\u636e\u539f\u6570\u636e\u96c6dataArr[:, 0]\u7684\u5927\u5c0f\u800c\u5b9a\n    x = arange(-1.0, 10.0, 0.1)\n\n    # \u6839\u636ex.w + b = 0 \u5f97\u5230\uff0c\u5176\u5f0f\u5b50\u5c55\u5f00\u4e3aw0.x1 + w1.x2 + b = 0, x2\u5c31\u662fy\u503c\n    y = (-b-ws[0, 0]*x)/ws[1, 0]\n    ax.plot(x, y)\n\n    for i in range(shape(yMat[0, :])[1]):\n        if yMat[0, i] > 0:\n            ax.plot(xMat[i, 0], xMat[i, 1], 'cx')\n        else:\n            ax.plot(xMat[i, 0], xMat[i, 1], 'kp')\n\n    # \u627e\u5230\u652f\u6301\u5411\u91cf\uff0c\u5e76\u5728\u56fe\u4e2d\u6807\u7ea2\n    for i in range(100):\n        if alphas[i] > 0.0:\n            ax.plot(xMat[i, 0], xMat[i, 1], 'ro')\n    plt.show()\n\n\nif __name__ == \"__main__\":\n\n    # \u65e0\u6838\u51fd\u6570\u7684\u6d4b\u8bd5\n    # \u83b7\u53d6\u7279\u5f81\u548c\u76ee\u6807\u53d8\u91cf\n    dataArr, labelArr = loadDataSet('data/6.SVM/testSet.txt')\n    # print labelArr\n\n    # b\u662f\u5e38\u91cf\u503c\uff0c alphas\u662f\u62c9\u683c\u6717\u65e5\u4e58\u5b50\n    b, alphas = smoP(dataArr, labelArr, 0.6, 0.001, 40)\n    print('/n/n/n')\n    print('b=', b)\n    print('alphas[alphas>0]=', alphas[alphas > 0])\n    print('shape(alphas[alphas > 0])=', shape(alphas[alphas > 0]))\n    for i in range(100):\n        if alphas[i] > 0:\n            print(dataArr[i], labelArr[i])\n    # \u753b\u56fe\n    ws = calcWs(alphas, dataArr, labelArr)\n    plotfig_SVM(dataArr, labelArr, ws, b, alphas)\n\n    # \u6709\u6838\u51fd\u6570\u7684\u6d4b\u8bd5\n    testRbf(0.8)\n\n    # # \u9879\u76ee\u5b9e\u6218\n    # # \u793a\u4f8b: \u624b\u5199\u8bc6\u522b\u95ee\u9898\u56de\u987e\n    # testDigits(('rbf', 0.1))\n    # testDigits(('rbf', 5))\n    # testDigits(('rbf', 10))\n    # testDigits(('rbf', 50))\n    # testDigits(('rbf', 100))\n    # testDigits(('lin'))\n", "src/py2.x/ml/6.SVM/svm-simple.py": "#!/usr/bin/python\n# coding:utf8\n\n\"\"\"\nCreated on Nov 4, 2010\nUpdate on 2017-05-18\nChapter 5 source file for Machine Learing in Action\nAuthor: Peter/geekidentity/\u7247\u523b\nGitHub: https://github.com/apachecn/AiLearning\n\"\"\"\nfrom __future__ import print_function\nfrom numpy import *\nimport matplotlib.pyplot as plt\n\n\ndef loadDataSet(fileName):\n    \"\"\"\n    \u5bf9\u6587\u4ef6\u8fdb\u884c\u9010\u884c\u89e3\u6790\uff0c\u4ece\u800c\u5f97\u5230\u7b2c\u884c\u7684\u7c7b\u6807\u7b7e\u548c\u6574\u4e2a\u7279\u5f81\u77e9\u9635\n    Args:\n        fileName \u6587\u4ef6\u540d\n    Returns:\n        dataMat  \u7279\u5f81\u77e9\u9635\n        labelMat \u7c7b\u6807\u7b7e\n    \"\"\"\n    dataMat = []\n    labelMat = []\n    fr = open(fileName)\n    for line in fr.readlines():\n        lineArr = line.strip().split('\\t')\n        dataMat.append([float(lineArr[0]), float(lineArr[1])])\n        labelMat.append(float(lineArr[2]))\n    return dataMat, labelMat\n\n\ndef selectJrand(i, m):\n    \"\"\"\n    \u968f\u673a\u9009\u62e9\u4e00\u4e2a\u6574\u6570\n    Args:\n        i  \u7b2c\u4e00\u4e2aalpha\u7684\u4e0b\u6807\n        m  \u6240\u6709alpha\u7684\u6570\u76ee\n    Returns:\n        j  \u8fd4\u56de\u4e00\u4e2a\u4e0d\u4e3ai\u7684\u968f\u673a\u6570\uff0c\u57280~m\u4e4b\u95f4\u7684\u6574\u6570\u503c\n    \"\"\"\n    j = i\n    while j == i:\n        j = int(random.uniform(0, m))\n    return j\n\n\ndef clipAlpha(aj, H, L):\n    \"\"\"clipAlpha(\u8c03\u6574aj\u7684\u503c\uff0c\u4f7faj\u5904\u4e8e L<=aj<=H)\n    Args:\n        aj  \u76ee\u6807\u503c\n        H   \u6700\u5927\u503c\n        L   \u6700\u5c0f\u503c\n    Returns:\n        aj  \u76ee\u6807\u503c\n    \"\"\"\n    if aj > H:\n        aj = H\n    if L > aj:\n        aj = L\n    return aj\n\n\ndef smoSimple(dataMatIn, classLabels, C, toler, maxIter):\n    \"\"\"smoSimple\n\n    Args:\n        dataMatIn    \u6570\u636e\u96c6\n        classLabels  \u7c7b\u522b\u6807\u7b7e\n        C   \u677e\u5f1b\u53d8\u91cf(\u5e38\u91cf\u503c)\uff0c\u5141\u8bb8\u6709\u4e9b\u6570\u636e\u70b9\u53ef\u4ee5\u5904\u4e8e\u5206\u9694\u9762\u7684\u9519\u8bef\u4e00\u4fa7\u3002\n            \u63a7\u5236\u6700\u5927\u5316\u95f4\u9694\u548c\u4fdd\u8bc1\u5927\u90e8\u5206\u7684\u51fd\u6570\u95f4\u9694\u5c0f\u4e8e1.0\u8fd9\u4e24\u4e2a\u76ee\u6807\u7684\u6743\u91cd\u3002\n            \u53ef\u4ee5\u901a\u8fc7\u8c03\u8282\u8be5\u53c2\u6570\u8fbe\u5230\u4e0d\u540c\u7684\u7ed3\u679c\u3002\n        toler   \u5bb9\u9519\u7387\uff08\u662f\u6307\u5728\u67d0\u4e2a\u4f53\u7cfb\u4e2d\u80fd\u51cf\u5c0f\u4e00\u4e9b\u56e0\u7d20\u6216\u9009\u62e9\u5bf9\u67d0\u4e2a\u7cfb\u7edf\u4ea7\u751f\u4e0d\u7a33\u5b9a\u7684\u6982\u7387\u3002\uff09\n        maxIter \u9000\u51fa\u524d\u6700\u5927\u7684\u5faa\u73af\u6b21\u6570\n    Returns:\n        b       \u6a21\u578b\u7684\u5e38\u91cf\u503c\n        alphas  \u62c9\u683c\u6717\u65e5\u4e58\u5b50\n    \"\"\"\n    dataMatrix = mat(dataMatIn)\n    # \u77e9\u9635\u8f6c\u7f6e \u548c .T \u4e00\u6837\u7684\u529f\u80fd\n    labelMat = mat(classLabels).transpose()\n    m, n = shape(dataMatrix)\n\n    # \u521d\u59cb\u5316 b\u548calphas(alpha\u6709\u70b9\u7c7b\u4f3c\u6743\u91cd\u503c\u3002)\n    b = 0\n    alphas = mat(zeros((m, 1)))\n\n    # \u6ca1\u6709\u4efb\u4f55alpha\u6539\u53d8\u7684\u60c5\u51b5\u4e0b\u904d\u5386\u6570\u636e\u7684\u6b21\u6570\n    iter = 0\n    while (iter < maxIter):\n        # w = calcWs(alphas, dataMatIn, classLabels)\n        # print(\"w:\", w)\n\n        # \u8bb0\u5f55alpha\u662f\u5426\u5df2\u7ecf\u8fdb\u884c\u4f18\u5316\uff0c\u6bcf\u6b21\u5faa\u73af\u65f6\u8bbe\u4e3a0\uff0c\u7136\u540e\u518d\u5bf9\u6574\u4e2a\u96c6\u5408\u987a\u5e8f\u904d\u5386\n        alphaPairsChanged = 0\n        for i in range(m):\n            # print 'alphas=', alphas\n            # print 'labelMat=', labelMat\n            # print 'multiply(alphas, labelMat)=', multiply(alphas, labelMat)\n            # \u6211\u4eec\u9884\u6d4b\u7684\u7c7b\u522b y = w^Tx[i]+b; \u5176\u4e2d\u56e0\u4e3a w = \u03a3(1~n) a[n]*lable[n]*x[n]\n            fXi = float(multiply(alphas, labelMat).T*(dataMatrix*dataMatrix[i, :].T)) + b\n            # \u9884\u6d4b\u7ed3\u679c\u4e0e\u771f\u5b9e\u7ed3\u679c\u6bd4\u5bf9\uff0c\u8ba1\u7b97\u8bef\u5deeEi\n            Ei = fXi - float(labelMat[i])\n\n            # \u7ea6\u675f\u6761\u4ef6 (KKT\u6761\u4ef6\u662f\u89e3\u51b3\u6700\u4f18\u5316\u95ee\u9898\u7684\u65f6\u7528\u5230\u7684\u4e00\u79cd\u65b9\u6cd5\u3002\u6211\u4eec\u8fd9\u91cc\u63d0\u5230\u7684\u6700\u4f18\u5316\u95ee\u9898\u901a\u5e38\u662f\u6307\u5bf9\u4e8e\u7ed9\u5b9a\u7684\u67d0\u4e00\u51fd\u6570\uff0c\u6c42\u5176\u5728\u6307\u5b9a\u4f5c\u7528\u57df\u4e0a\u7684\u5168\u5c40\u6700\u5c0f\u503c)\n            # 0<=alphas[i]<=C\uff0c\u4f46\u7531\u4e8e0\u548cC\u662f\u8fb9\u754c\u503c\uff0c\u6211\u4eec\u65e0\u6cd5\u8fdb\u884c\u4f18\u5316\uff0c\u56e0\u4e3a\u9700\u8981\u589e\u52a0\u4e00\u4e2aalphas\u548c\u964d\u4f4e\u4e00\u4e2aalphas\u3002\n            # \u8868\u793a\u53d1\u751f\u9519\u8bef\u7684\u6982\u7387: labelMat[i]*Ei \u5982\u679c\u8d85\u51fa\u4e86 toler\uff0c \u624d\u9700\u8981\u4f18\u5316\u3002\u81f3\u4e8e\u6b63\u8d1f\u53f7\uff0c\u6211\u4eec\u8003\u8651\u7edd\u5bf9\u503c\u5c31\u5bf9\u4e86\u3002\n            '''\n            # \u68c0\u9a8c\u8bad\u7ec3\u6837\u672c(xi, yi)\u662f\u5426\u6ee1\u8db3KKT\u6761\u4ef6\n            yi*f(i) >= 1 and alpha = 0 (outside the boundary)\n            yi*f(i) == 1 and 0<alpha< C (on the boundary)\n            yi*f(i) <= 1 and alpha = C (between the boundary)\n            '''\n            if ((labelMat[i]*Ei < -toler) and (alphas[i] < C)) or ((labelMat[i]*Ei > toler) and (alphas[i] > 0)):\n\n                # \u5982\u679c\u6ee1\u8db3\u4f18\u5316\u7684\u6761\u4ef6\uff0c\u6211\u4eec\u5c31\u968f\u673a\u9009\u53d6\u975ei\u7684\u4e00\u4e2a\u70b9\uff0c\u8fdb\u884c\u4f18\u5316\u6bd4\u8f83\n                j = selectJrand(i, m)\n                # \u9884\u6d4bj\u7684\u7ed3\u679c\n                fXj = float(multiply(alphas, labelMat).T*(dataMatrix*dataMatrix[j, :].T)) + b\n                Ej = fXj - float(labelMat[j])\n                alphaIold = alphas[i].copy()\n                alphaJold = alphas[j].copy()\n\n                # L\u548cH\u7528\u4e8e\u5c06alphas[j]\u8c03\u6574\u52300-C\u4e4b\u95f4\u3002\u5982\u679cL==H\uff0c\u5c31\u4e0d\u505a\u4efb\u4f55\u6539\u53d8\uff0c\u76f4\u63a5\u6267\u884ccontinue\u8bed\u53e5\n                # labelMat[i] != labelMat[j] \u8868\u793a\u5f02\u4fa7\uff0c\u5c31\u76f8\u51cf\uff0c\u5426\u5219\u662f\u540c\u4fa7\uff0c\u5c31\u76f8\u52a0\u3002\n                if (labelMat[i] != labelMat[j]):\n                    L = max(0, alphas[j] - alphas[i])\n                    H = min(C, C + alphas[j] - alphas[i])\n                else:\n                    L = max(0, alphas[j] + alphas[i] - C)\n                    H = min(C, alphas[j] + alphas[i])\n                # \u5982\u679c\u76f8\u540c\uff0c\u5c31\u6ca1\u53d1\u4f18\u5316\u4e86\n                if L == H:\n                    print(\"L==H\")\n                    continue\n\n                # eta\u662falphas[j]\u7684\u6700\u4f18\u4fee\u6539\u91cf\uff0c\u5982\u679ceta==0\uff0c\u9700\u8981\u9000\u51fafor\u5faa\u73af\u7684\u5f53\u524d\u8fed\u4ee3\u8fc7\u7a0b\n                # \u53c2\u8003\u300a\u7edf\u8ba1\u5b66\u4e60\u65b9\u6cd5\u300b\u674e\u822a-P125~P128<\u5e8f\u5217\u6700\u5c0f\u6700\u4f18\u5316\u7b97\u6cd5>\n                eta = 2.0 * dataMatrix[i, :]*dataMatrix[j, :].T - dataMatrix[i, :]*dataMatrix[i, :].T - dataMatrix[j, :]*dataMatrix[j, :].T\n                if eta >= 0:\n                    print(\"eta>=0\")\n                    continue\n\n                # \u8ba1\u7b97\u51fa\u4e00\u4e2a\u65b0\u7684alphas[j]\u503c\n                alphas[j] -= labelMat[j]*(Ei - Ej)/eta\n                # \u5e76\u4f7f\u7528\u8f85\u52a9\u51fd\u6570\uff0c\u4ee5\u53caL\u548cH\u5bf9\u5176\u8fdb\u884c\u8c03\u6574\n                alphas[j] = clipAlpha(alphas[j], H, L)\n                # \u68c0\u67e5alpha[j]\u662f\u5426\u53ea\u662f\u8f7b\u5fae\u7684\u6539\u53d8\uff0c\u5982\u679c\u662f\u7684\u8bdd\uff0c\u5c31\u9000\u51fafor\u5faa\u73af\u3002\n                if (abs(alphas[j] - alphaJold) < 0.00001):\n                    print(\"j not moving enough\")\n                    continue\n                # \u7136\u540ealphas[i]\u548calphas[j]\u540c\u6837\u8fdb\u884c\u6539\u53d8\uff0c\u867d\u7136\u6539\u53d8\u7684\u5927\u5c0f\u4e00\u6837\uff0c\u4f46\u662f\u6539\u53d8\u7684\u65b9\u5411\u6b63\u597d\u76f8\u53cd\n                alphas[i] += labelMat[j]*labelMat[i]*(alphaJold - alphas[j])\n                # \u5728\u5bf9alpha[i], alpha[j] \u8fdb\u884c\u4f18\u5316\u4e4b\u540e\uff0c\u7ed9\u8fd9\u4e24\u4e2aalpha\u503c\u8bbe\u7f6e\u4e00\u4e2a\u5e38\u6570b\u3002\n                # w= \u03a3[1~n] ai*yi*xi => b = yj- \u03a3[1~n] ai*yi(xi*xj)\n                # \u6240\u4ee5:   b1 - b = (y1-y) - \u03a3[1~n] yi*(a1-a)*(xi*x1)\n                # \u4e3a\u4ec0\u4e48\u51cf2\u904d\uff1f \u56e0\u4e3a\u662f \u51cf\u53bb\u03a3[1~n]\uff0c\u6b63\u597d2\u4e2a\u53d8\u91cfi\u548cj\uff0c\u6240\u4ee5\u51cf2\u904d\n                b1 = b - Ei- labelMat[i]*(alphas[i]-alphaIold)*dataMatrix[i, :]*dataMatrix[i, :].T - labelMat[j]*(alphas[j]-alphaJold)*dataMatrix[i, :]*dataMatrix[j, :].T\n                b2 = b - Ej- labelMat[i]*(alphas[i]-alphaIold)*dataMatrix[i, :]*dataMatrix[j, :].T - labelMat[j]*(alphas[j]-alphaJold)*dataMatrix[j, :]*dataMatrix[j, :].T\n                if (0 < alphas[i]) and (C > alphas[i]):\n                    b = b1\n                elif (0 < alphas[j]) and (C > alphas[j]):\n                    b = b2\n                else:\n                    b = (b1 + b2)/2.0\n                alphaPairsChanged += 1\n                print(\"iter: %d i:%d, pairs changed %d\" % (iter, i, alphaPairsChanged))\n        # \u5728for\u5faa\u73af\u5916\uff0c\u68c0\u67e5alpha\u503c\u662f\u5426\u505a\u4e86\u66f4\u65b0\uff0c\u5982\u679c\u5728\u66f4\u65b0\u5219\u5c06iter\u8bbe\u4e3a0\u540e\u7ee7\u7eed\u8fd0\u884c\u7a0b\u5e8f\n        # \u77e5\u9053\u66f4\u65b0\u5b8c\u6bd5\u540e\uff0citer\u6b21\u5faa\u73af\u65e0\u53d8\u5316\uff0c\u624d\u63a8\u51fa\u5faa\u73af\u3002\n        if (alphaPairsChanged == 0):\n            iter += 1\n        else:\n            iter = 0\n        print(\"iteration number: %d\" % iter)\n    return b, alphas\n\n\ndef calcWs(alphas, dataArr, classLabels):\n    \"\"\"\n    \u57fa\u4e8ealpha\u8ba1\u7b97w\u503c\n    Args:\n        alphas        \u62c9\u683c\u6717\u65e5\u4e58\u5b50\n        dataArr       feature\u6570\u636e\u96c6\n        classLabels   \u76ee\u6807\u53d8\u91cf\u6570\u636e\u96c6\n\n    Returns:\n        wc  \u56de\u5f52\u7cfb\u6570\n    \"\"\"\n    X = mat(dataArr)\n    labelMat = mat(classLabels).transpose()\n    m, n = shape(X)\n    w = zeros((n, 1))\n    for i in range(m):\n        w += multiply(alphas[i] * labelMat[i], X[i, :].T)\n    return w\n\n\ndef plotfig_SVM(xMat, yMat, ws, b, alphas):\n    \"\"\"\n    \u53c2\u8003\u5730\u5740: \n       http://blog.csdn.net/maoersong/article/details/24315633\n       http://www.cnblogs.com/JustForCS/p/5283489.html\n       http://blog.csdn.net/kkxgx/article/details/6951959\n    \"\"\"\n\n    xMat = mat(xMat)\n    yMat = mat(yMat)\n\n    # b\u539f\u6765\u662f\u77e9\u9635\uff0c\u5148\u8f6c\u4e3a\u6570\u7ec4\u7c7b\u578b\u540e\u5176\u6570\u7ec4\u5927\u5c0f\u4e3a\uff081,1\uff09\uff0c\u6240\u4ee5\u540e\u9762\u52a0[0]\uff0c\u53d8\u4e3a(1,)\n    b = array(b)[0]\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n\n    # \u6ce8\u610fflatten\u7684\u7528\u6cd5\n    ax.scatter(xMat[:, 0].flatten().A[0], xMat[:, 1].flatten().A[0])\n\n    # x\u6700\u5927\u503c\uff0c\u6700\u5c0f\u503c\u6839\u636e\u539f\u6570\u636e\u96c6dataArr[:, 0]\u7684\u5927\u5c0f\u800c\u5b9a\n    x = arange(-1.0, 10.0, 0.1)\n\n    # \u6839\u636ex.w + b = 0 \u5f97\u5230\uff0c\u5176\u5f0f\u5b50\u5c55\u5f00\u4e3aw0.x1 + w1.x2 + b = 0, x2\u5c31\u662fy\u503c\n    y = (-b-ws[0, 0]*x)/ws[1, 0]\n    ax.plot(x, y)\n\n    for i in range(shape(yMat[0, :])[1]):\n        if yMat[0, i] > 0:\n            ax.plot(xMat[i, 0], xMat[i, 1], 'cx')\n        else:\n            ax.plot(xMat[i, 0], xMat[i, 1], 'kp')\n\n    # \u627e\u5230\u652f\u6301\u5411\u91cf\uff0c\u5e76\u5728\u56fe\u4e2d\u6807\u7ea2\n    for i in range(100):\n        if alphas[i] > 0.0:\n            ax.plot(xMat[i, 0], xMat[i, 1], 'ro')\n    plt.show()\n\n\nif __name__ == \"__main__\":\n    # \u83b7\u53d6\u7279\u5f81\u548c\u76ee\u6807\u53d8\u91cf\n    dataArr, labelArr = loadDataSet('data/6.SVM/testSet.txt')\n    # print labelArr\n\n    # b\u662f\u5e38\u91cf\u503c\uff0c alphas\u662f\u62c9\u683c\u6717\u65e5\u4e58\u5b50\n    b, alphas = smoSimple(dataArr, labelArr, 0.6, 0.001, 40)\n    print('/n/n/n')\n    print('b=', b)\n    print('alphas[alphas>0]=', alphas[alphas > 0])\n    print('shape(alphas[alphas > 0])=', shape(alphas[alphas > 0]))\n    for i in range(100):\n        if alphas[i] > 0:\n            print(dataArr[i], labelArr[i])\n    # \u753b\u56fe\n    ws = calcWs(alphas, dataArr, labelArr)\n    plotfig_SVM(dataArr, labelArr, ws, b, alphas)\n", "src/py2.x/ml/6.SVM/sklearn-svm-demo.py": "#!/usr/bin/python\n# coding:utf8\n\n\"\"\"\nCreated on 2017-06-28\nUpdated on 2017-06-28\nSVM: \u6700\u5927\u8fb9\u8ddd\u5206\u79bb\u8d85\u5e73\u9762\nAuthor: \u7247\u523b\nGitHub: https://github.com/apachecn/AiLearning\nsklearn-SVM\u8bd1\u6587\u94fe\u63a5: http://cwiki.apachecn.org/pages/viewpage.action?pageId=10031359\n\"\"\"\nfrom __future__ import print_function\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn import svm\n\nprint(__doc__)\n\n\n# \u521b\u5efa40\u4e2a\u5206\u79bb\u70b9\nnp.random.seed(0)\n# X = np.r_[np.random.randn(20, 2) - [2, 2], np.random.randn(20, 2) + [2, 2]]\n# Y = [0] * 20 + [1] * 20\n\n\ndef loadDataSet(fileName):\n    \"\"\"\n    \u5bf9\u6587\u4ef6\u8fdb\u884c\u9010\u884c\u89e3\u6790\uff0c\u4ece\u800c\u5f97\u5230\u7b2c\u884c\u7684\u7c7b\u6807\u7b7e\u548c\u6574\u4e2a\u6570\u636e\u77e9\u9635\n    Args:\n        fileName \u6587\u4ef6\u540d\n    Returns:\n        dataMat  \u6570\u636e\u77e9\u9635\n        labelMat \u7c7b\u6807\u7b7e\n    \"\"\"\n    dataMat = []\n    labelMat = []\n    fr = open(fileName)\n    for line in fr.readlines():\n        lineArr = line.strip().split('\\t')\n        dataMat.append([float(lineArr[0]), float(lineArr[1])])\n        labelMat.append(float(lineArr[2]))\n    return dataMat, labelMat\n\n\nX, Y = loadDataSet('data/6.SVM/testSet.txt')\nX = np.mat(X)\n\nprint((\"X=\", X))\nprint((\"Y=\", Y))\n\n# \u62df\u5408\u4e00\u4e2aSVM\u6a21\u578b\nclf = svm.SVC(kernel='linear')\nclf.fit(X, Y)\n\n# \u83b7\u53d6\u5206\u5272\u8d85\u5e73\u9762\nw = clf.coef_[0]\n# \u659c\u7387\na = -w[0]/w[1]\n# \u4ece-5\u52305\uff0c\u987a\u5e8f\u95f4\u9694\u91c7\u683750\u4e2a\u6837\u672c\uff0c\u9ed8\u8ba4\u662fnum=50\n# xx = np.linspace(-5, 5)  # , num=50)\nxx = np.linspace(-2, 10)  # , num=50)\n# \u4e8c\u7ef4\u7684\u76f4\u7ebf\u65b9\u7a0b\nyy = a * xx - (clf.intercept_[0]) / w[1]\nprint((\"yy=\", yy))\n\n# plot the parallels to the separating hyperplane that pass through the support vectors\n# \u901a\u8fc7\u652f\u6301\u5411\u91cf\u7ed8\u5236\u5206\u5272\u8d85\u5e73\u9762\nprint((\"support_vectors_=\", clf.support_vectors_))\nb = clf.support_vectors_[0]\nyy_down = a * xx + (b[1] - a * b[0])\nb = clf.support_vectors_[-1]\nyy_up = a * xx + (b[1] - a * b[0])\n\n# plot the line, the points, and the nearest vectors to the plane\nplt.plot(xx, yy, 'k-')\nplt.plot(xx, yy_down, 'k--')\nplt.plot(xx, yy_up, 'k--')\n\nplt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=80, facecolors='none')\nplt.scatter([X[:, 0]], [X[:, 1]], c=Y, cmap=plt.cm.Paired)\n\nplt.axis('tight')\nplt.show()\n", "src/py2.x/ml/6.SVM/svm-complete_Non-Kernel.py": "#!/usr/bin/python\n# coding:utf8\n\n\"\"\"\nCreated on Nov 4, 2010\nUpdate on 2017-05-18\nChapter 5 source file for Machine Learing in Action\nAuthor: Peter/geekidentity/\u7247\u523b\nGitHub: https://github.com/apachecn/AiLearning\n\"\"\"\nfrom __future__ import print_function\nfrom numpy import *\nimport matplotlib.pyplot as plt\n\n\nclass optStruct:\n    def __init__(self, dataMatIn, classLabels, C, toler):  # Initialize the structure with the parameters\n        self.X = dataMatIn\n        self.labelMat = classLabels\n        self.C = C\n        self.tol = toler\n        self.m = shape(dataMatIn)[0]\n        self.alphas = mat(zeros((self.m, 1)))\n        self.b = 0\n        self.eCache = mat(zeros((self.m, 2)))  # first column is valid flag\n\n\ndef loadDataSet(fileName):\n    \"\"\"loadDataSet\uff08\u5bf9\u6587\u4ef6\u8fdb\u884c\u9010\u884c\u89e3\u6790\uff0c\u4ece\u800c\u5f97\u5230\u7b2c\u884c\u7684\u7c7b\u6807\u7b7e\u548c\u6574\u4e2a\u6570\u636e\u77e9\u9635\uff09\n\n    Args:\n        fileName \u6587\u4ef6\u540d\n    Returns:\n        dataMat  \u6570\u636e\u77e9\u9635\n        labelMat \u7c7b\u6807\u7b7e\n    \"\"\"\n    dataMat = []\n    labelMat = []\n    fr = open(fileName)\n    for line in fr.readlines():\n        lineArr = line.strip().split('\\t')\n        dataMat.append([float(lineArr[0]), float(lineArr[1])])\n        labelMat.append(float(lineArr[2]))\n    return dataMat, labelMat\n\n\ndef selectJrand(i, m):\n    \"\"\"\n    \u968f\u673a\u9009\u62e9\u4e00\u4e2a\u6574\u6570\n    Args:\n        i  \u7b2c\u4e00\u4e2aalpha\u7684\u4e0b\u6807\n        m  \u6240\u6709alpha\u7684\u6570\u76ee\n    Returns:\n        j  \u8fd4\u56de\u4e00\u4e2a\u4e0d\u4e3ai\u7684\u968f\u673a\u6570\uff0c\u57280~m\u4e4b\u95f4\u7684\u6574\u6570\u503c\n    \"\"\"\n    j = i\n    while j == i:\n        j = int(random.uniform(0, m))\n    return j\n\n\ndef clipAlpha(aj, H, L):\n    \"\"\"clipAlpha(\u8c03\u6574aj\u7684\u503c\uff0c\u4f7faj\u5904\u4e8e L<=aj<=H)\n    Args:\n        aj  \u76ee\u6807\u503c\n        H   \u6700\u5927\u503c\n        L   \u6700\u5c0f\u503c\n    Returns:\n        aj  \u76ee\u6807\u503c\n    \"\"\"\n    if aj > H:\n        aj = H\n    if L > aj:\n        aj = L\n    return aj\n\n\ndef calcEk(oS, k):\n    \"\"\"calcEk\uff08\u6c42 Ek\u8bef\u5dee: \u9884\u6d4b\u503c-\u771f\u5b9e\u503c\u7684\u5dee\uff09\n\n    \u8be5\u8fc7\u7a0b\u5728\u5b8c\u6574\u7248\u7684SMO\u7b97\u6cd5\u4e2d\u966a\u51fa\u73b0\u6b21\u6570\u8f83\u591a\uff0c\u56e0\u6b64\u5c06\u5176\u5355\u72ec\u4f5c\u4e3a\u4e00\u4e2a\u65b9\u6cd5\n    Args:\n        oS  optStruct\u5bf9\u8c61\n        k   \u5177\u4f53\u7684\u67d0\u4e00\u884c\n\n    Returns:\n        Ek  \u9884\u6d4b\u7ed3\u679c\u4e0e\u771f\u5b9e\u7ed3\u679c\u6bd4\u5bf9\uff0c\u8ba1\u7b97\u8bef\u5deeEk\n    \"\"\"\n    fXk = float(multiply(oS.alphas, oS.labelMat).T * (oS.X * oS.X[k, :].T)) + oS.b\n    Ek = fXk - float(oS.labelMat[k])\n    return Ek\n\n\ndef selectJ(i, oS, Ei):  # this is the second choice -heurstic, and calcs Ej\n    \"\"\"selectJ\uff08\u8fd4\u56de\u6700\u4f18\u7684j\u548cEj\uff09\n\n    \u5185\u5faa\u73af\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\u3002\n    \u9009\u62e9\u7b2c\u4e8c\u4e2a(\u5185\u5faa\u73af)alpha\u7684alpha\u503c\n    \u8fd9\u91cc\u7684\u76ee\u6807\u662f\u9009\u62e9\u5408\u9002\u7684\u7b2c\u4e8c\u4e2aalpha\u503c\u4ee5\u4fdd\u8bc1\u6bcf\u6b21\u4f18\u5316\u4e2d\u91c7\u7528\u6700\u5927\u6b65\u957f\u3002\n    \u8be5\u51fd\u6570\u7684\u8bef\u5dee\u4e0e\u7b2c\u4e00\u4e2aalpha\u503cEi\u548c\u4e0b\u6807i\u6709\u5173\u3002\n    Args:\n        i   \u5177\u4f53\u7684\u7b2ci\u4e00\u884c\n        oS  optStruct\u5bf9\u8c61\n        Ei  \u9884\u6d4b\u7ed3\u679c\u4e0e\u771f\u5b9e\u7ed3\u679c\u6bd4\u5bf9\uff0c\u8ba1\u7b97\u8bef\u5deeEi\n\n    Returns:\n        j  \u968f\u673a\u9009\u51fa\u7684\u7b2cj\u4e00\u884c\n        Ej \u9884\u6d4b\u7ed3\u679c\u4e0e\u771f\u5b9e\u7ed3\u679c\u6bd4\u5bf9\uff0c\u8ba1\u7b97\u8bef\u5deeEj\n    \"\"\"\n    maxK = -1\n    maxDeltaE = 0\n    Ej = 0\n    # \u9996\u5148\u5c06\u8f93\u5165\u503cEi\u5728\u7f13\u5b58\u4e2d\u8bbe\u7f6e\u6210\u4e3a\u6709\u6548\u7684\u3002\u8fd9\u91cc\u7684\u6709\u6548\u610f\u5473\u7740\u5b83\u5df2\u7ecf\u8ba1\u7b97\u597d\u4e86\u3002\n    oS.eCache[i] = [1, Ei]\n\n    # print 'oS.eCache[%s]=%s' % (i, oS.eCache[i])\n    # print 'oS.eCache[:, 0].A=%s' % oS.eCache[:, 0].A.T\n    # \"\"\"\n    # # \u8fd4\u56de\u975e0\u7684: \u884c\u5217\u503c\n    # nonzero(oS.eCache[:, 0].A)= (\n    #     \u884c:  array([ 0,  2,  4,  5,  8, 10, 17, 18, 20, 21, 23, 25, 26, 29, 30, 39, 46,52, 54, 55, 62, 69, 70, 76, 79, 82, 94, 97]), \n    #     \u5217:  array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,0, 0, 0, 0, 0])\n    # )\n    # \"\"\"\n    # print 'nonzero(oS.eCache[:, 0].A)=', nonzero(oS.eCache[:, 0].A)\n    # # \u53d6\u884c\u7684list\n    # print 'nonzero(oS.eCache[:, 0].A)[0]=', nonzero(oS.eCache[:, 0].A)[0]\n    # \u975e\u96f6E\u503c\u7684\u884c\u7684list\u5217\u8868\uff0c\u6240\u5bf9\u5e94\u7684alpha\u503c\n    validEcacheList = nonzero(oS.eCache[:, 0].A)[0]\n    if (len(validEcacheList)) > 1:\n        for k in validEcacheList:  # \u5728\u6240\u6709\u7684\u503c\u4e0a\u8fdb\u884c\u5faa\u73af\uff0c\u5e76\u9009\u62e9\u5176\u4e2d\u4f7f\u5f97\u6539\u53d8\u6700\u5927\u7684\u90a3\u4e2a\u503c\n            if k == i:\n                continue  # don't calc for i, waste of time\n\n            # \u6c42 Ek\u8bef\u5dee: \u9884\u6d4b\u503c-\u771f\u5b9e\u503c\u7684\u5dee\n            Ek = calcEk(oS, k)\n            deltaE = abs(Ei - Ek)\n            if (deltaE > maxDeltaE):\n                maxK = k\n                maxDeltaE = deltaE\n                Ej = Ek\n        return maxK, Ej\n    else:  # \u5982\u679c\u662f\u7b2c\u4e00\u6b21\u5faa\u73af\uff0c\u5219\u968f\u673a\u9009\u62e9\u4e00\u4e2aalpha\u503c\n        j = selectJrand(i, oS.m)\n\n        # \u6c42 Ek\u8bef\u5dee: \u9884\u6d4b\u503c-\u771f\u5b9e\u503c\u7684\u5dee\n        Ej = calcEk(oS, j)\n    return j, Ej\n\n\ndef updateEk(oS, k):  # after any alpha has changed update the new value in the cache\n    \"\"\"updateEk\uff08\u8ba1\u7b97\u8bef\u5dee\u503c\u5e76\u5b58\u5165\u7f13\u5b58\u4e2d\u3002\uff09\n\n    \u5728\u5bf9alpha\u503c\u8fdb\u884c\u4f18\u5316\u4e4b\u540e\u4f1a\u7528\u5230\u8fd9\u4e2a\u503c\u3002\n    Args:\n        oS  optStruct\u5bf9\u8c61\n        k   \u67d0\u4e00\u5217\u7684\u884c\u53f7\n    \"\"\"\n\n    # \u6c42 \u8bef\u5dee: \u9884\u6d4b\u503c-\u771f\u5b9e\u503c\u7684\u5dee\n    Ek = calcEk(oS, k)\n    oS.eCache[k] = [1, Ek]\n\n\ndef innerL(i, oS):\n    \"\"\"innerL\n    \u5185\u5faa\u73af\u4ee3\u7801\n    Args:\n        i   \u5177\u4f53\u7684\u67d0\u4e00\u884c\n        oS  optStruct\u5bf9\u8c61\n\n    Returns:\n        0   \u627e\u4e0d\u5230\u6700\u4f18\u7684\u503c\n        1   \u627e\u5230\u4e86\u6700\u4f18\u7684\u503c\uff0c\u5e76\u4e14oS.Cache\u5230\u7f13\u5b58\u4e2d\n    \"\"\"\n\n    # \u6c42 Ek\u8bef\u5dee: \u9884\u6d4b\u503c-\u771f\u5b9e\u503c\u7684\u5dee\n    Ei = calcEk(oS, i)\n\n    # \u7ea6\u675f\u6761\u4ef6 (KKT\u6761\u4ef6\u662f\u89e3\u51b3\u6700\u4f18\u5316\u95ee\u9898\u7684\u65f6\u7528\u5230\u7684\u4e00\u79cd\u65b9\u6cd5\u3002\u6211\u4eec\u8fd9\u91cc\u63d0\u5230\u7684\u6700\u4f18\u5316\u95ee\u9898\u901a\u5e38\u662f\u6307\u5bf9\u4e8e\u7ed9\u5b9a\u7684\u67d0\u4e00\u51fd\u6570\uff0c\u6c42\u5176\u5728\u6307\u5b9a\u4f5c\u7528\u57df\u4e0a\u7684\u5168\u5c40\u6700\u5c0f\u503c)\n    # 0<=alphas[i]<=C\uff0c\u4f46\u7531\u4e8e0\u548cC\u662f\u8fb9\u754c\u503c\uff0c\u6211\u4eec\u65e0\u6cd5\u8fdb\u884c\u4f18\u5316\uff0c\u56e0\u4e3a\u9700\u8981\u589e\u52a0\u4e00\u4e2aalphas\u548c\u964d\u4f4e\u4e00\u4e2aalphas\u3002\n    # \u8868\u793a\u53d1\u751f\u9519\u8bef\u7684\u6982\u7387: labelMat[i]*Ei \u5982\u679c\u8d85\u51fa\u4e86 toler\uff0c \u624d\u9700\u8981\u4f18\u5316\u3002\u81f3\u4e8e\u6b63\u8d1f\u53f7\uff0c\u6211\u4eec\u8003\u8651\u7edd\u5bf9\u503c\u5c31\u5bf9\u4e86\u3002\n    '''\n    # \u68c0\u9a8c\u8bad\u7ec3\u6837\u672c(xi, yi)\u662f\u5426\u6ee1\u8db3KKT\u6761\u4ef6\n    yi*f(i) >= 1 and alpha = 0 (outside the boundary)\n    yi*f(i) == 1 and 0<alpha< C (on the boundary)\n    yi*f(i) <= 1 and alpha = C (between the boundary)\n    '''\n    if ((oS.labelMat[i] * Ei < -oS.tol) and (oS.alphas[i] < oS.C)) or ((oS.labelMat[i] * Ei > oS.tol) and (oS.alphas[i] > 0)):\n        # \u9009\u62e9\u6700\u5927\u7684\u8bef\u5dee\u5bf9\u5e94\u7684j\u8fdb\u884c\u4f18\u5316\u3002\u6548\u679c\u66f4\u660e\u663e\n        j, Ej = selectJ(i, oS, Ei)\n        alphaIold = oS.alphas[i].copy()\n        alphaJold = oS.alphas[j].copy()\n\n        # L\u548cH\u7528\u4e8e\u5c06alphas[j]\u8c03\u6574\u52300-C\u4e4b\u95f4\u3002\u5982\u679cL==H\uff0c\u5c31\u4e0d\u505a\u4efb\u4f55\u6539\u53d8\uff0c\u76f4\u63a5return 0\n        if (oS.labelMat[i] != oS.labelMat[j]):\n            L = max(0, oS.alphas[j] - oS.alphas[i])\n            H = min(oS.C, oS.C + oS.alphas[j] - oS.alphas[i])\n        else:\n            L = max(0, oS.alphas[j] + oS.alphas[i] - oS.C)\n            H = min(oS.C, oS.alphas[j] + oS.alphas[i])\n        if L == H:\n            print(\"L==H\")\n            return 0\n\n        # eta\u662falphas[j]\u7684\u6700\u4f18\u4fee\u6539\u91cf\uff0c\u5982\u679ceta==0\uff0c\u9700\u8981\u9000\u51fafor\u5faa\u73af\u7684\u5f53\u524d\u8fed\u4ee3\u8fc7\u7a0b\n        # \u53c2\u8003\u300a\u7edf\u8ba1\u5b66\u4e60\u65b9\u6cd5\u300b\u674e\u822a-P125~P128<\u5e8f\u5217\u6700\u5c0f\u6700\u4f18\u5316\u7b97\u6cd5>\n        eta = 2.0 * oS.X[i, :] * oS.X[j, :].T - oS.X[i, :] * oS.X[i, :].T - oS.X[j, :] * oS.X[j, :].T\n        if eta >= 0:\n            print(\"eta>=0\")\n            return 0\n\n        # \u8ba1\u7b97\u51fa\u4e00\u4e2a\u65b0\u7684alphas[j]\u503c\n        oS.alphas[j] -= oS.labelMat[j] * (Ei - Ej) / eta\n        # \u5e76\u4f7f\u7528\u8f85\u52a9\u51fd\u6570\uff0c\u4ee5\u53caL\u548cH\u5bf9\u5176\u8fdb\u884c\u8c03\u6574\n        oS.alphas[j] = clipAlpha(oS.alphas[j], H, L)\n        # \u66f4\u65b0\u8bef\u5dee\u7f13\u5b58\n        updateEk(oS, j)\n\n        # \u68c0\u67e5alpha[j]\u662f\u5426\u53ea\u662f\u8f7b\u5fae\u7684\u6539\u53d8\uff0c\u5982\u679c\u662f\u7684\u8bdd\uff0c\u5c31\u9000\u51fafor\u5faa\u73af\u3002\n        if (abs(oS.alphas[j] - alphaJold) < 0.00001):\n            print(\"j not moving enough\")\n            return 0\n\n        # \u7136\u540ealphas[i]\u548calphas[j]\u540c\u6837\u8fdb\u884c\u6539\u53d8\uff0c\u867d\u7136\u6539\u53d8\u7684\u5927\u5c0f\u4e00\u6837\uff0c\u4f46\u662f\u6539\u53d8\u7684\u65b9\u5411\u6b63\u597d\u76f8\u53cd\n        oS.alphas[i] += oS.labelMat[j] * oS.labelMat[i] * (alphaJold - oS.alphas[j])\n        # \u66f4\u65b0\u8bef\u5dee\u7f13\u5b58\n        updateEk(oS, i)\n\n        # \u5728\u5bf9alpha[i], alpha[j] \u8fdb\u884c\u4f18\u5316\u4e4b\u540e\uff0c\u7ed9\u8fd9\u4e24\u4e2aalpha\u503c\u8bbe\u7f6e\u4e00\u4e2a\u5e38\u6570b\u3002\n        # w= \u03a3[1~n] ai*yi*xi => b = yj \u03a3[1~n] ai*yi(xi*xj)\n        # \u6240\u4ee5:   b1 - b = (y1-y) - \u03a3[1~n] yi*(a1-a)*(xi*x1)\n        # \u4e3a\u4ec0\u4e48\u51cf2\u904d\uff1f \u56e0\u4e3a\u662f \u51cf\u53bb\u03a3[1~n]\uff0c\u6b63\u597d2\u4e2a\u53d8\u91cfi\u548cj\uff0c\u6240\u4ee5\u51cf2\u904d\n        b1 = oS.b - Ei - oS.labelMat[i] * (oS.alphas[i] - alphaIold) * oS.X[i, :] * oS.X[i, :].T - oS.labelMat[j] * (oS.alphas[j] - alphaJold) * oS.X[i, :] * oS.X[j, :].T\n        b2 = oS.b - Ej - oS.labelMat[i] * (oS.alphas[i] - alphaIold) * oS.X[i, :] * oS.X[j, :].T - oS.labelMat[j] * (oS.alphas[j] - alphaJold) * oS.X[j, :] * oS.X[j, :].T\n        if (0 < oS.alphas[i]) and (oS.C > oS.alphas[i]):\n            oS.b = b1\n        elif (0 < oS.alphas[j]) and (oS.C > oS.alphas[j]):\n            oS.b = b2\n        else:\n            oS.b = (b1 + b2) / 2.0\n        return 1\n    else:\n        return 0\n\n\ndef smoP(dataMatIn, classLabels, C, toler, maxIter):\n    \"\"\"\n    \u5b8c\u6574SMO\u7b97\u6cd5\u5916\u5faa\u73af\uff0c\u4e0esmoSimple\u6709\u4e9b\u7c7b\u4f3c\uff0c\u4f46\u8fd9\u91cc\u7684\u5faa\u73af\u9000\u51fa\u6761\u4ef6\u66f4\u591a\u4e00\u4e9b\n    Args:\n        dataMatIn    \u6570\u636e\u96c6\n        classLabels  \u7c7b\u522b\u6807\u7b7e\n        C   \u677e\u5f1b\u53d8\u91cf(\u5e38\u91cf\u503c)\uff0c\u5141\u8bb8\u6709\u4e9b\u6570\u636e\u70b9\u53ef\u4ee5\u5904\u4e8e\u5206\u9694\u9762\u7684\u9519\u8bef\u4e00\u4fa7\u3002\n            \u63a7\u5236\u6700\u5927\u5316\u95f4\u9694\u548c\u4fdd\u8bc1\u5927\u90e8\u5206\u7684\u51fd\u6570\u95f4\u9694\u5c0f\u4e8e1.0\u8fd9\u4e24\u4e2a\u76ee\u6807\u7684\u6743\u91cd\u3002\n            \u53ef\u4ee5\u901a\u8fc7\u8c03\u8282\u8be5\u53c2\u6570\u8fbe\u5230\u4e0d\u540c\u7684\u7ed3\u679c\u3002\n        toler   \u5bb9\u9519\u7387\n        maxIter \u9000\u51fa\u524d\u6700\u5927\u7684\u5faa\u73af\u6b21\u6570\n    Returns:\n        b       \u6a21\u578b\u7684\u5e38\u91cf\u503c\n        alphas  \u62c9\u683c\u6717\u65e5\u4e58\u5b50\n    \"\"\"\n\n    # \u521b\u5efa\u4e00\u4e2a optStruct \u5bf9\u8c61\n    oS = optStruct(mat(dataMatIn), mat(classLabels).transpose(), C, toler)\n    iter = 0\n    entireSet = True\n    alphaPairsChanged = 0\n\n    # \u5faa\u73af\u904d\u5386: \u5faa\u73afmaxIter\u6b21 \u5e76\u4e14 \uff08alphaPairsChanged\u5b58\u5728\u53ef\u4ee5\u6539\u53d8 or \u6240\u6709\u884c\u904d\u5386\u4e00\u904d\uff09\n    # \u5faa\u73af\u8fed\u4ee3\u7ed3\u675f \u6216\u8005 \u5faa\u73af\u904d\u5386\u6240\u6709alpha\u540e\uff0calphaPairs\u8fd8\u662f\u6ca1\u53d8\u5316\n    while (iter < maxIter) and ((alphaPairsChanged > 0) or (entireSet)):\n        alphaPairsChanged = 0\n\n        #  \u5f53entireSet=true or \u975e\u8fb9\u754calpha\u5bf9\u6ca1\u6709\u4e86\uff1b\u5c31\u5f00\u59cb\u5bfb\u627e alpha\u5bf9\uff0c\u7136\u540e\u51b3\u5b9a\u662f\u5426\u8981\u8fdb\u884celse\u3002\n        if entireSet:\n            # \u5728\u6570\u636e\u96c6\u4e0a\u904d\u5386\u6240\u6709\u53ef\u80fd\u7684alpha\n            for i in range(oS.m):\n                # \u662f\u5426\u5b58\u5728alpha\u5bf9\uff0c\u5b58\u5728\u5c31+1\n                alphaPairsChanged += innerL(i, oS)\n                print(\"fullSet, iter: %d i:%d, pairs changed %d\" % (iter, i, alphaPairsChanged))\n            iter += 1\n        # \u5bf9\u5df2\u5b58\u5728 alpha\u5bf9\uff0c\u9009\u51fa\u975e\u8fb9\u754c\u7684alpha\u503c\uff0c\u8fdb\u884c\u4f18\u5316\u3002\n        else:\n            # \u904d\u5386\u6240\u6709\u7684\u975e\u8fb9\u754calpha\u503c\uff0c\u4e5f\u5c31\u662f\u4e0d\u5728\u8fb9\u754c0\u6216C\u4e0a\u7684\u503c\u3002\n            nonBoundIs = nonzero((oS.alphas.A > 0) * (oS.alphas.A < C))[0]\n            for i in nonBoundIs:\n                alphaPairsChanged += innerL(i, oS)\n                print(\"non-bound, iter: %d i:%d, pairs changed %d\" % (iter, i, alphaPairsChanged))\n            iter += 1\n\n        # \u5982\u679c\u627e\u5230alpha\u5bf9\uff0c\u5c31\u4f18\u5316\u975e\u8fb9\u754calpha\u503c\uff0c\u5426\u5219\uff0c\u5c31\u91cd\u65b0\u8fdb\u884c\u5bfb\u627e\uff0c\u5982\u679c\u5bfb\u627e\u4e00\u904d \u904d\u5386\u6240\u6709\u7684\u884c\u8fd8\u662f\u6ca1\u627e\u5230\uff0c\u5c31\u9000\u51fa\u5faa\u73af\u3002\n        if entireSet:\n            entireSet = False  # toggle entire set loop\n        elif (alphaPairsChanged == 0):\n            entireSet = True\n        print(\"iteration number: %d\" % iter)\n    return oS.b, oS.alphas\n\n\ndef calcWs(alphas, dataArr, classLabels):\n    \"\"\"\n    \u57fa\u4e8ealpha\u8ba1\u7b97w\u503c\n    Args:\n        alphas        \u62c9\u683c\u6717\u65e5\u4e58\u5b50\n        dataArr       feature\u6570\u636e\u96c6\n        classLabels   \u76ee\u6807\u53d8\u91cf\u6570\u636e\u96c6\n\n    Returns:\n        wc  \u56de\u5f52\u7cfb\u6570\n    \"\"\"\n    X = mat(dataArr)\n    labelMat = mat(classLabels).transpose()\n    m, n = shape(X)\n    w = zeros((n, 1))\n    for i in range(m):\n        w += multiply(alphas[i] * labelMat[i], X[i, :].T)\n    return w\n\n\ndef plotfig_SVM(xArr, yArr, ws, b, alphas):\n    \"\"\"\n    \u53c2\u8003\u5730\u5740: \n       http://blog.csdn.net/maoersong/article/details/24315633\n       http://www.cnblogs.com/JustForCS/p/5283489.html\n       http://blog.csdn.net/kkxgx/article/details/6951959\n    \"\"\"\n\n    xMat = mat(xArr)\n    yMat = mat(yArr)\n\n    # b\u539f\u6765\u662f\u77e9\u9635\uff0c\u5148\u8f6c\u4e3a\u6570\u7ec4\u7c7b\u578b\u540e\u5176\u6570\u7ec4\u5927\u5c0f\u4e3a\uff081,1\uff09\uff0c\u6240\u4ee5\u540e\u9762\u52a0[0]\uff0c\u53d8\u4e3a(1,)\n    b = array(b)[0]\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n\n    # \u6ce8\u610fflatten\u7684\u7528\u6cd5\n    ax.scatter(xMat[:, 0].flatten().A[0], xMat[:, 1].flatten().A[0])\n\n    # x\u6700\u5927\u503c\uff0c\u6700\u5c0f\u503c\u6839\u636e\u539f\u6570\u636e\u96c6dataArr[:, 0]\u7684\u5927\u5c0f\u800c\u5b9a\n    x = arange(-1.0, 10.0, 0.1)\n\n    # \u6839\u636ex.w + b = 0 \u5f97\u5230\uff0c\u5176\u5f0f\u5b50\u5c55\u5f00\u4e3aw0.x1 + w1.x2 + b = 0, x2\u5c31\u662fy\u503c\n    y = (-b-ws[0, 0]*x)/ws[1, 0]\n    ax.plot(x, y)\n\n    for i in range(shape(yMat[0, :])[1]):\n        if yMat[0, i] > 0:\n            ax.plot(xMat[i, 0], xMat[i, 1], 'cx')\n        else:\n            ax.plot(xMat[i, 0], xMat[i, 1], 'kp')\n\n    # \u627e\u5230\u652f\u6301\u5411\u91cf\uff0c\u5e76\u5728\u56fe\u4e2d\u6807\u7ea2\n    for i in range(100):\n        if alphas[i] > 0.0:\n            ax.plot(xMat[i, 0], xMat[i, 1], 'ro')\n    plt.show()\n\n\nif __name__ == \"__main__\":\n    # \u83b7\u53d6\u7279\u5f81\u548c\u76ee\u6807\u53d8\u91cf\n    dataArr, labelArr = loadDataSet('data/6.SVM/testSet.txt')\n    # print labelArr\n\n    # b\u662f\u5e38\u91cf\u503c\uff0c alphas\u662f\u62c9\u683c\u6717\u65e5\u4e58\u5b50\n    b, alphas = smoP(dataArr, labelArr, 0.6, 0.001, 40)\n    print('/n/n/n')\n    print('b=', b)\n    print('alphas[alphas>0]=', alphas[alphas > 0])\n    print('shape(alphas[alphas > 0])=', shape(alphas[alphas > 0]))\n    for i in range(100):\n        if alphas[i] > 0:\n            print(dataArr[i], labelArr[i])\n    # \u753b\u56fe\n    ws = calcWs(alphas, dataArr, labelArr)\n    plotfig_SVM(dataArr, labelArr, ws, b, alphas)\n", "src/py2.x/ml/3.DecisionTree/DecisionTree.py": "#!/usr/bin/python\n# coding:utf-8\n'''\nCreated on Oct 12, 2010\nUpdate on 2017-05-18\nDecision Tree Source Code for Machine Learning in Action Ch. 3\nAuthor: Peter Harrington/\u7247\u523b\nGitHub: https://github.com/apachecn/AiLearning\n'''\nfrom __future__ import print_function\nprint(__doc__)\nimport operator\nfrom math import log\nimport decisionTreePlot as dtPlot\nfrom collections import Counter\n\n\ndef createDataSet():\n    \"\"\"DateSet \u57fa\u7840\u6570\u636e\u96c6\n\n    Args:\n        \u65e0\u9700\u4f20\u5165\u53c2\u6570\n    Returns:\n        \u8fd4\u56de\u6570\u636e\u96c6\u548c\u5bf9\u5e94\u7684label\u6807\u7b7e\n    \"\"\"\n    dataSet = [[1, 1, 'yes'],\n               [1, 1, 'yes'],\n               [1, 0, 'no'],\n               [0, 1, 'no'],\n               [0, 1, 'no']]\n    # dataSet = [['yes'],\n    #         ['yes'],\n    #         ['no'],\n    #         ['no'],\n    #         ['no']]\n    # labels  \u9732\u51fa\u6c34\u9762   \u811a\u8e7c\n    labels = ['no surfacing', 'flippers']\n    # change to discrete values\n    return dataSet, labels\n\n\ndef calcShannonEnt(dataSet):\n    \"\"\"calcShannonEnt(calculate Shannon entropy \u8ba1\u7b97\u7ed9\u5b9a\u6570\u636e\u96c6\u7684\u9999\u519c\u71b5)\n\n    Args:\n        dataSet \u6570\u636e\u96c6\n    Returns:\n        \u8fd4\u56de \u6bcf\u4e00\u7ec4feature\u4e0b\u7684\u67d0\u4e2a\u5206\u7c7b\u4e0b\uff0c\u9999\u519c\u71b5\u7684\u4fe1\u606f\u671f\u671b\n    \"\"\"\n    # -----------\u8ba1\u7b97\u9999\u519c\u71b5\u7684\u7b2c\u4e00\u79cd\u5b9e\u73b0\u65b9\u5f0fstart--------------------------------------------------------------------------------\n    # \u6c42list\u7684\u957f\u5ea6\uff0c\u8868\u793a\u8ba1\u7b97\u53c2\u4e0e\u8bad\u7ec3\u7684\u6570\u636e\u91cf\n    numEntries = len(dataSet)\n    # \u4e0b\u9762\u8f93\u51fa\u6211\u4eec\u6d4b\u8bd5\u7684\u6570\u636e\u96c6\u7684\u4e00\u4e9b\u4fe1\u606f\n    # \u4f8b\u5982: <type 'list'> numEntries:  5 \u662f\u4e0b\u9762\u7684\u4ee3\u7801\u7684\u8f93\u51fa\n    # print type(dataSet), 'numEntries: ', numEntries\n\n    # \u8ba1\u7b97\u5206\u7c7b\u6807\u7b7elabel\u51fa\u73b0\u7684\u6b21\u6570\n    labelCounts = {}\n    # the the number of unique elements and their occurance\n    for featVec in dataSet:\n        # \u5c06\u5f53\u524d\u5b9e\u4f8b\u7684\u6807\u7b7e\u5b58\u50a8\uff0c\u5373\u6bcf\u4e00\u884c\u6570\u636e\u7684\u6700\u540e\u4e00\u4e2a\u6570\u636e\u4ee3\u8868\u7684\u662f\u6807\u7b7e\n        currentLabel = featVec[-1]\n        # \u4e3a\u6240\u6709\u53ef\u80fd\u7684\u5206\u7c7b\u521b\u5efa\u5b57\u5178\uff0c\u5982\u679c\u5f53\u524d\u7684\u952e\u503c\u4e0d\u5b58\u5728\uff0c\u5219\u6269\u5c55\u5b57\u5178\u5e76\u5c06\u5f53\u524d\u952e\u503c\u52a0\u5165\u5b57\u5178\u3002\u6bcf\u4e2a\u952e\u503c\u90fd\u8bb0\u5f55\u4e86\u5f53\u524d\u7c7b\u522b\u51fa\u73b0\u7684\u6b21\u6570\u3002\n        if currentLabel not in labelCounts.keys():\n            labelCounts[currentLabel] = 0\n        labelCounts[currentLabel] += 1\n        # print '-----', featVec, labelCounts\n\n    # \u5bf9\u4e8elabel\u6807\u7b7e\u7684\u5360\u6bd4\uff0c\u6c42\u51falabel\u6807\u7b7e\u7684\u9999\u519c\u71b5\n    shannonEnt = 0.0\n    for key in labelCounts:\n        # \u4f7f\u7528\u6240\u6709\u7c7b\u6807\u7b7e\u7684\u53d1\u751f\u9891\u7387\u8ba1\u7b97\u7c7b\u522b\u51fa\u73b0\u7684\u6982\u7387\u3002\n        prob = float(labelCounts[key])/numEntries\n        # log base 2 \n        # \u8ba1\u7b97\u9999\u519c\u71b5\uff0c\u4ee5 2 \u4e3a\u5e95\u6c42\u5bf9\u6570\n        shannonEnt -= prob * log(prob, 2)\n        # print '---', prob, prob * log(prob, 2), shannonEnt\n    # -----------\u8ba1\u7b97\u9999\u519c\u71b5\u7684\u7b2c\u4e00\u79cd\u5b9e\u73b0\u65b9\u5f0fend--------------------------------------------------------------------------------\n\n    # # -----------\u8ba1\u7b97\u9999\u519c\u71b5\u7684\u7b2c\u4e8c\u79cd\u5b9e\u73b0\u65b9\u5f0fstart--------------------------------------------------------------------------------\n    # # \u7edf\u8ba1\u6807\u7b7e\u51fa\u73b0\u7684\u6b21\u6570\n    # label_count = Counter(data[-1] for data in dataSet)\n    # # \u8ba1\u7b97\u6982\u7387\n    # probs = [p[1] / len(dataSet) for p in label_count.items()]\n    # # \u8ba1\u7b97\u9999\u519c\u71b5\n    # shannonEnt = sum([-p * log(p, 2) for p in probs])\n    # # -----------\u8ba1\u7b97\u9999\u519c\u71b5\u7684\u7b2c\u4e8c\u79cd\u5b9e\u73b0\u65b9\u5f0fend--------------------------------------------------------------------------------\n    return shannonEnt\n\n\ndef splitDataSet(dataSet, index, value):\n    \"\"\"splitDataSet(\u901a\u8fc7\u904d\u5386dataSet\u6570\u636e\u96c6\uff0c\u6c42\u51faindex\u5bf9\u5e94\u7684colnum\u5217\u7684\u503c\u4e3avalue\u7684\u884c)\n        \u5c31\u662f\u4f9d\u636eindex\u5217\u8fdb\u884c\u5206\u7c7b\uff0c\u5982\u679cindex\u5217\u7684\u6570\u636e\u7b49\u4e8e value\u7684\u65f6\u5019\uff0c\u5c31\u8981\u5c06 index \u5212\u5206\u5230\u6211\u4eec\u521b\u5efa\u7684\u65b0\u7684\u6570\u636e\u96c6\u4e2d\n    Args:\n        dataSet \u6570\u636e\u96c6                 \u5f85\u5212\u5206\u7684\u6570\u636e\u96c6\n        index \u8868\u793a\u6bcf\u4e00\u884c\u7684index\u5217        \u5212\u5206\u6570\u636e\u96c6\u7684\u7279\u5f81\n        value \u8868\u793aindex\u5217\u5bf9\u5e94\u7684value\u503c   \u9700\u8981\u8fd4\u56de\u7684\u7279\u5f81\u7684\u503c\u3002\n    Returns:\n        index\u5217\u4e3avalue\u7684\u6570\u636e\u96c6\u3010\u8be5\u6570\u636e\u96c6\u9700\u8981\u6392\u9664index\u5217\u3011\n    \"\"\"\n    # -----------\u5207\u5206\u6570\u636e\u96c6\u7684\u7b2c\u4e00\u79cd\u65b9\u5f0f start------------------------------------\n    retDataSet = []\n    for featVec in dataSet: \n        # index\u5217\u4e3avalue\u7684\u6570\u636e\u96c6\u3010\u8be5\u6570\u636e\u96c6\u9700\u8981\u6392\u9664index\u5217\u3011\n        # \u5224\u65adindex\u5217\u7684\u503c\u662f\u5426\u4e3avalue\n        if featVec[index] == value:\n            # chop out index used for splitting\n            # [:index]\u8868\u793a\u524dindex\u884c\uff0c\u5373\u82e5 index \u4e3a2\uff0c\u5c31\u662f\u53d6 featVec \u7684\u524d index \u884c\n            reducedFeatVec = featVec[:index]\n            '''\n            \u8bf7\u767e\u5ea6\u67e5\u8be2\u4e00\u4e0b:  extend\u548cappend\u7684\u533a\u522b\n            list.append(object) \u5411\u5217\u8868\u4e2d\u6dfb\u52a0\u4e00\u4e2a\u5bf9\u8c61object\n            list.extend(sequence) \u628a\u4e00\u4e2a\u5e8f\u5217seq\u7684\u5185\u5bb9\u6dfb\u52a0\u5230\u5217\u8868\u4e2d\n            1\u3001\u4f7f\u7528append\u7684\u65f6\u5019\uff0c\u662f\u5c06new_media\u770b\u4f5c\u4e00\u4e2a\u5bf9\u8c61\uff0c\u6574\u4f53\u6253\u5305\u6dfb\u52a0\u5230music_media\u5bf9\u8c61\u4e2d\u3002\n            2\u3001\u4f7f\u7528extend\u7684\u65f6\u5019\uff0c\u662f\u5c06new_media\u770b\u4f5c\u4e00\u4e2a\u5e8f\u5217\uff0c\u5c06\u8fd9\u4e2a\u5e8f\u5217\u548cmusic_media\u5e8f\u5217\u5408\u5e76\uff0c\u5e76\u653e\u5728\u5176\u540e\u9762\u3002\n            result = []\n            result.extend([1,2,3])\n            print result\n            result.append([4,5,6])\n            print result\n            result.extend([7,8,9])\n            print result\n            \u7ed3\u679c: \n            [1, 2, 3]\n            [1, 2, 3, [4, 5, 6]]\n            [1, 2, 3, [4, 5, 6], 7, 8, 9]\n            '''\n            reducedFeatVec.extend(featVec[index+1:])\n            # [index+1:]\u8868\u793a\u4ece\u8df3\u8fc7 index \u7684 index+1\u884c\uff0c\u53d6\u63a5\u4e0b\u6765\u7684\u6570\u636e\n            # \u6536\u96c6\u7ed3\u679c\u503c index\u5217\u4e3avalue\u7684\u884c\u3010\u8be5\u884c\u9700\u8981\u6392\u9664index\u5217\u3011\n            retDataSet.append(reducedFeatVec)\n    # -----------\u5207\u5206\u6570\u636e\u96c6\u7684\u7b2c\u4e00\u79cd\u65b9\u5f0f end------------------------------------\n\n    # # -----------\u5207\u5206\u6570\u636e\u96c6\u7684\u7b2c\u4e8c\u79cd\u65b9\u5f0f start------------------------------------\n    # retDataSet = [data for data in dataSet for i, v in enumerate(data) if i == axis and v == value]\n    # # -----------\u5207\u5206\u6570\u636e\u96c6\u7684\u7b2c\u4e8c\u79cd\u65b9\u5f0f end------------------------------------\n    return retDataSet\n\n\ndef chooseBestFeatureToSplit(dataSet):\n    \"\"\"chooseBestFeatureToSplit(\u9009\u62e9\u6700\u597d\u7684\u7279\u5f81)\n\n    Args:\n        dataSet \u6570\u636e\u96c6\n    Returns:\n        bestFeature \u6700\u4f18\u7684\u7279\u5f81\u5217\n    \"\"\"\n\n    # -----------\u9009\u62e9\u6700\u4f18\u7279\u5f81\u7684\u7b2c\u4e00\u79cd\u65b9\u5f0f start------------------------------------\n    # \u6c42\u7b2c\u4e00\u884c\u6709\u591a\u5c11\u5217\u7684 Feature, \u6700\u540e\u4e00\u5217\u662flabel\u5217\u561b\n    numFeatures = len(dataSet[0]) - 1\n    # label\u7684\u4fe1\u606f\u71b5\n    baseEntropy = calcShannonEnt(dataSet)\n    # \u6700\u4f18\u7684\u4fe1\u606f\u589e\u76ca\u503c, \u548c\u6700\u4f18\u7684Featurn\u7f16\u53f7\n    bestInfoGain, bestFeature = 0.0, -1\n    # iterate over all the features\n    for i in range(numFeatures):\n        # create a list of all the examples of this feature\n        # \u83b7\u53d6\u6bcf\u4e00\u4e2a\u5b9e\u4f8b\u7684\u7b2ci+1\u4e2afeature\uff0c\u7ec4\u6210list\u96c6\u5408\n        featList = [example[i] for example in dataSet]\n        # get a set of unique values\n        # \u83b7\u53d6\u5254\u91cd\u540e\u7684\u96c6\u5408\uff0c\u4f7f\u7528set\u5bf9list\u6570\u636e\u8fdb\u884c\u53bb\u91cd\n        uniqueVals = set(featList)\n        # \u521b\u5efa\u4e00\u4e2a\u4e34\u65f6\u7684\u4fe1\u606f\u71b5\n        newEntropy = 0.0\n        # \u904d\u5386\u67d0\u4e00\u5217\u7684value\u96c6\u5408\uff0c\u8ba1\u7b97\u8be5\u5217\u7684\u4fe1\u606f\u71b5 \n        # \u904d\u5386\u5f53\u524d\u7279\u5f81\u4e2d\u7684\u6240\u6709\u552f\u4e00\u5c5e\u6027\u503c\uff0c\u5bf9\u6bcf\u4e2a\u552f\u4e00\u5c5e\u6027\u503c\u5212\u5206\u4e00\u6b21\u6570\u636e\u96c6\uff0c\u8ba1\u7b97\u6570\u636e\u96c6\u7684\u65b0\u71b5\u503c\uff0c\u5e76\u5bf9\u6240\u6709\u552f\u4e00\u7279\u5f81\u503c\u5f97\u5230\u7684\u71b5\u6c42\u548c\u3002\n        for value in uniqueVals:\n            subDataSet = splitDataSet(dataSet, i, value)\n            prob = len(subDataSet)/float(len(dataSet))\n            newEntropy += prob * calcShannonEnt(subDataSet)\n        # gain[\u4fe1\u606f\u589e\u76ca]: \u5212\u5206\u6570\u636e\u96c6\u524d\u540e\u7684\u4fe1\u606f\u53d8\u5316\uff0c \u83b7\u53d6\u4fe1\u606f\u71b5\u6700\u5927\u7684\u503c\n        # \u4fe1\u606f\u589e\u76ca\u662f\u71b5\u7684\u51cf\u5c11\u6216\u8005\u662f\u6570\u636e\u65e0\u5e8f\u5ea6\u7684\u51cf\u5c11\u3002\u6700\u540e\uff0c\u6bd4\u8f83\u6240\u6709\u7279\u5f81\u4e2d\u7684\u4fe1\u606f\u589e\u76ca\uff0c\u8fd4\u56de\u6700\u597d\u7279\u5f81\u5212\u5206\u7684\u7d22\u5f15\u503c\u3002\n        infoGain = baseEntropy - newEntropy\n        print('infoGain=', infoGain, 'bestFeature=', i, baseEntropy, newEntropy)\n        if (infoGain > bestInfoGain):\n            bestInfoGain = infoGain\n            bestFeature = i\n    return bestFeature\n    # -----------\u9009\u62e9\u6700\u4f18\u7279\u5f81\u7684\u7b2c\u4e00\u79cd\u65b9\u5f0f end------------------------------------\n\n    # # -----------\u9009\u62e9\u6700\u4f18\u7279\u5f81\u7684\u7b2c\u4e8c\u79cd\u65b9\u5f0f start------------------------------------\n    # # \u8ba1\u7b97\u521d\u59cb\u9999\u519c\u71b5\n    # base_entropy = calcShannonEnt(dataSet)\n    # best_info_gain = 0\n    # best_feature = -1\n    # # \u904d\u5386\u6bcf\u4e00\u4e2a\u7279\u5f81\n    # for i in range(len(dataSet[0]) - 1):\n    #     # \u5bf9\u5f53\u524d\u7279\u5f81\u8fdb\u884c\u7edf\u8ba1\n    #     feature_count = Counter([data[i] for data in dataSet])\n    #     # \u8ba1\u7b97\u5206\u5272\u540e\u7684\u9999\u519c\u71b5\n    #     new_entropy = sum(feature[1] / float(len(dataSet)) * calcShannonEnt(splitDataSet(dataSet, i, feature[0])) \\\n    #                    for feature in feature_count.items())\n    #     # \u66f4\u65b0\u503c\n    #     info_gain = base_entropy - new_entropy\n    #     print('No. {0} feature info gain is {1:.3f}'.format(i, info_gain))\n    #     if info_gain > best_info_gain:\n    #         best_info_gain = info_gain\n    #         best_feature = i\n    # return best_feature\n    # # -----------\u9009\u62e9\u6700\u4f18\u7279\u5f81\u7684\u7b2c\u4e8c\u79cd\u65b9\u5f0f end------------------------------------\n\n\ndef majorityCnt(classList):\n    \"\"\"majorityCnt(\u9009\u62e9\u51fa\u73b0\u6b21\u6570\u6700\u591a\u7684\u4e00\u4e2a\u7ed3\u679c)\n\n    Args:\n        classList label\u5217\u7684\u96c6\u5408\n    Returns:\n        bestFeature \u6700\u4f18\u7684\u7279\u5f81\u5217\n    \"\"\"\n    # -----------majorityCnt\u7684\u7b2c\u4e00\u79cd\u65b9\u5f0f start------------------------------------\n    classCount = {}\n    for vote in classList:\n        if vote not in classCount.keys():\n            classCount[vote] = 0\n        classCount[vote] += 1\n    # \u5012\u53d9\u6392\u5217classCount\u5f97\u5230\u4e00\u4e2a\u5b57\u5178\u96c6\u5408\uff0c\u7136\u540e\u53d6\u51fa\u7b2c\u4e00\u4e2a\u5c31\u662f\u7ed3\u679c\uff08yes/no\uff09\uff0c\u5373\u51fa\u73b0\u6b21\u6570\u6700\u591a\u7684\u7ed3\u679c\n    sortedClassCount = sorted(classCount.iteritems(), key=operator.itemgetter(1), reverse=True)\n    # print 'sortedClassCount:', sortedClassCount\n    return sortedClassCount[0][0]\n    # -----------majorityCnt\u7684\u7b2c\u4e00\u79cd\u65b9\u5f0f end------------------------------------\n\n    # # -----------majorityCnt\u7684\u7b2c\u4e8c\u79cd\u65b9\u5f0f start------------------------------------\n    # major_label = Counter(classList).most_common(1)[0]\n    # return major_label\n    # # -----------majorityCnt\u7684\u7b2c\u4e8c\u79cd\u65b9\u5f0f end------------------------------------\n\n\ndef createTree(dataSet, labels):\n    classList = [example[-1] for example in dataSet]\n    # \u5982\u679c\u6570\u636e\u96c6\u7684\u6700\u540e\u4e00\u5217\u7684\u7b2c\u4e00\u4e2a\u503c\u51fa\u73b0\u7684\u6b21\u6570=\u6574\u4e2a\u96c6\u5408\u7684\u6570\u91cf\uff0c\u4e5f\u5c31\u8bf4\u53ea\u6709\u4e00\u4e2a\u7c7b\u522b\uff0c\u5c31\u53ea\u76f4\u63a5\u8fd4\u56de\u7ed3\u679c\u5c31\u884c\n    # \u7b2c\u4e00\u4e2a\u505c\u6b62\u6761\u4ef6: \u6240\u6709\u7684\u7c7b\u6807\u7b7e\u5b8c\u5168\u76f8\u540c\uff0c\u5219\u76f4\u63a5\u8fd4\u56de\u8be5\u7c7b\u6807\u7b7e\u3002\n    # count() \u51fd\u6570\u662f\u7edf\u8ba1\u62ec\u53f7\u4e2d\u7684\u503c\u5728list\u4e2d\u51fa\u73b0\u7684\u6b21\u6570\n    if classList.count(classList[0]) == len(classList):\n        return classList[0]\n    # \u5982\u679c\u6570\u636e\u96c6\u53ea\u67091\u5217\uff0c\u90a3\u4e48\u6700\u521d\u51fa\u73b0label\u6b21\u6570\u6700\u591a\u7684\u4e00\u7c7b\uff0c\u4f5c\u4e3a\u7ed3\u679c\n    # \u7b2c\u4e8c\u4e2a\u505c\u6b62\u6761\u4ef6: \u4f7f\u7528\u5b8c\u4e86\u6240\u6709\u7279\u5f81\uff0c\u4ecd\u7136\u4e0d\u80fd\u5c06\u6570\u636e\u96c6\u5212\u5206\u6210\u4ec5\u5305\u542b\u552f\u4e00\u7c7b\u522b\u7684\u5206\u7ec4\u3002\n    if len(dataSet[0]) == 1:\n        return majorityCnt(classList)\n\n    # \u9009\u62e9\u6700\u4f18\u7684\u5217\uff0c\u5f97\u5230\u6700\u4f18\u5217\u5bf9\u5e94\u7684label\u542b\u4e49\n    bestFeat = chooseBestFeatureToSplit(dataSet)\n    # \u83b7\u53d6label\u7684\u540d\u79f0\n    bestFeatLabel = labels[bestFeat]\n    # \u521d\u59cb\u5316myTree\n    myTree = {bestFeatLabel: {}}\n    # \u6ce8: labels\u5217\u8868\u662f\u53ef\u53d8\u5bf9\u8c61\uff0c\u5728PYTHON\u51fd\u6570\u4e2d\u4f5c\u4e3a\u53c2\u6570\u65f6\u4f20\u5740\u5f15\u7528\uff0c\u80fd\u591f\u88ab\u5168\u5c40\u4fee\u6539\n    # \u6240\u4ee5\u8fd9\u884c\u4ee3\u7801\u5bfc\u81f4\u51fd\u6570\u5916\u7684\u540c\u540d\u53d8\u91cf\u88ab\u5220\u9664\u4e86\u5143\u7d20\uff0c\u9020\u6210\u4f8b\u53e5\u65e0\u6cd5\u6267\u884c\uff0c\u63d0\u793a'no surfacing' is not in list\n    del(labels[bestFeat])\n    # \u53d6\u51fa\u6700\u4f18\u5217\uff0c\u7136\u540e\u5b83\u7684branch\u505a\u5206\u7c7b\n    featValues = [example[bestFeat] for example in dataSet]\n    uniqueVals = set(featValues)\n    for value in uniqueVals:\n        # \u6c42\u51fa\u5269\u4f59\u7684\u6807\u7b7elabel\n        subLabels = labels[:]\n        # \u904d\u5386\u5f53\u524d\u9009\u62e9\u7279\u5f81\u5305\u542b\u7684\u6240\u6709\u5c5e\u6027\u503c\uff0c\u5728\u6bcf\u4e2a\u6570\u636e\u96c6\u5212\u5206\u4e0a\u9012\u5f52\u8c03\u7528\u51fd\u6570createTree()\n        myTree[bestFeatLabel][value] = createTree(splitDataSet(dataSet, bestFeat, value), subLabels)\n        # print 'myTree', value, myTree\n    return myTree\n\n\ndef classify(inputTree, featLabels, testVec):\n    \"\"\"classify(\u7ed9\u8f93\u5165\u7684\u8282\u70b9\uff0c\u8fdb\u884c\u5206\u7c7b)\n\n    Args:\n        inputTree  \u51b3\u7b56\u6811\u6a21\u578b\n        featLabels Feature\u6807\u7b7e\u5bf9\u5e94\u7684\u540d\u79f0\n        testVec    \u6d4b\u8bd5\u8f93\u5165\u7684\u6570\u636e\n    Returns:\n        classLabel \u5206\u7c7b\u7684\u7ed3\u679c\u503c\uff0c\u9700\u8981\u6620\u5c04label\u624d\u80fd\u77e5\u9053\u540d\u79f0\n    \"\"\"\n    # \u83b7\u53d6tree\u7684\u6839\u8282\u70b9\u5bf9\u4e8e\u7684key\u503c\n    firstStr = inputTree.keys()[0]\n    # \u901a\u8fc7key\u5f97\u5230\u6839\u8282\u70b9\u5bf9\u5e94\u7684value\n    secondDict = inputTree[firstStr]\n    # \u5224\u65ad\u6839\u8282\u70b9\u540d\u79f0\u83b7\u53d6\u6839\u8282\u70b9\u5728label\u4e2d\u7684\u5148\u540e\u987a\u5e8f\uff0c\u8fd9\u6837\u5c31\u77e5\u9053\u8f93\u5165\u7684testVec\u600e\u4e48\u5f00\u59cb\u5bf9\u7167\u6811\u6765\u505a\u5206\u7c7b\n    featIndex = featLabels.index(firstStr)\n    # \u6d4b\u8bd5\u6570\u636e\uff0c\u627e\u5230\u6839\u8282\u70b9\u5bf9\u5e94\u7684label\u4f4d\u7f6e\uff0c\u4e5f\u5c31\u77e5\u9053\u4ece\u8f93\u5165\u7684\u6570\u636e\u7684\u7b2c\u51e0\u4f4d\u6765\u5f00\u59cb\u5206\u7c7b\n    key = testVec[featIndex]\n    valueOfFeat = secondDict[key]\n    print('+++', firstStr, 'xxx', secondDict, '---', key, '>>>', valueOfFeat)\n    # \u5224\u65ad\u5206\u679d\u662f\u5426\u7ed3\u675f: \u5224\u65advalueOfFeat\u662f\u5426\u662fdict\u7c7b\u578b\n    if isinstance(valueOfFeat, dict):\n        classLabel = classify(valueOfFeat, featLabels, testVec)\n    else:\n        classLabel = valueOfFeat\n    return classLabel\n\n\ndef storeTree(inputTree, filename):\n    import pickle\n    # -------------- \u7b2c\u4e00\u79cd\u65b9\u6cd5 start --------------\n    fw = open(filename, 'wb')\n    pickle.dump(inputTree, fw)\n    fw.close()\n    # -------------- \u7b2c\u4e00\u79cd\u65b9\u6cd5 end --------------\n\n    # -------------- \u7b2c\u4e8c\u79cd\u65b9\u6cd5 start --------------\n    with open(filename, 'wb') as fw:\n        pickle.dump(inputTree, fw)\n    # -------------- \u7b2c\u4e8c\u79cd\u65b9\u6cd5 start --------------\n\n\ndef grabTree(filename):\n    import pickle\n    fr = open(filename,'rb')\n    return pickle.load(fr)\n\n\ndef fishTest():\n    # 1.\u521b\u5efa\u6570\u636e\u548c\u7ed3\u679c\u6807\u7b7e\n    myDat, labels = createDataSet()\n    # print myDat, labels\n\n    # \u8ba1\u7b97label\u5206\u7c7b\u6807\u7b7e\u7684\u9999\u519c\u71b5\n    # calcShannonEnt(myDat)\n\n    # # \u6c42\u7b2c0\u5217 \u4e3a 1/0\u7684\u5217\u7684\u6570\u636e\u96c6\u3010\u6392\u9664\u7b2c0\u5217\u3011\n    # print '1---', splitDataSet(myDat, 0, 1)\n    # print '0---', splitDataSet(myDat, 0, 0)\n\n    # # \u8ba1\u7b97\u6700\u597d\u7684\u4fe1\u606f\u589e\u76ca\u7684\u5217\n    # print chooseBestFeatureToSplit(myDat)\n\n    import copy\n    myTree = createTree(myDat, copy.deepcopy(labels))\n    print(myTree)\n    # [1, 1]\u8868\u793a\u8981\u53d6\u7684\u5206\u652f\u4e0a\u7684\u8282\u70b9\u4f4d\u7f6e\uff0c\u5bf9\u5e94\u7684\u7ed3\u679c\u503c\n    print(classify(myTree, labels, [1, 1]))\n    \n    # \u83b7\u5f97\u6811\u7684\u9ad8\u5ea6\n    print(get_tree_height(myTree))\n\n    # \u753b\u56fe\u53ef\u89c6\u5316\u5c55\u73b0\n    dtPlot.createPlot(myTree)\n\n\ndef ContactLensesTest():\n    \"\"\"\n    Desc:\n        \u9884\u6d4b\u9690\u5f62\u773c\u955c\u7684\u6d4b\u8bd5\u4ee3\u7801\n    Args:\n        none\n    Returns:\n        none\n    \"\"\"\n\n    # \u52a0\u8f7d\u9690\u5f62\u773c\u955c\u76f8\u5173\u7684 \u6587\u672c\u6587\u4ef6 \u6570\u636e\n    fr = open('data/3.DecisionTree/lenses.txt')\n    # \u89e3\u6790\u6570\u636e\uff0c\u83b7\u5f97 features \u6570\u636e\n    lenses = [inst.strip().split('\\t') for inst in fr.readlines()]\n    # \u5f97\u5230\u6570\u636e\u7684\u5bf9\u5e94\u7684 Labels\n    lensesLabels = ['age', 'prescript', 'astigmatic', 'tearRate']\n    # \u4f7f\u7528\u4e0a\u9762\u7684\u521b\u5efa\u51b3\u7b56\u6811\u7684\u4ee3\u7801\uff0c\u6784\u9020\u9884\u6d4b\u9690\u5f62\u773c\u955c\u7684\u51b3\u7b56\u6811\n    lensesTree = createTree(lenses, lensesLabels)\n    print(lensesTree)\n    # \u753b\u56fe\u53ef\u89c6\u5316\u5c55\u73b0\n    dtPlot.createPlot(lensesTree)\n    \n    \ndef get_tree_height(tree):\n    \"\"\"\n     Desc:\n        \u9012\u5f52\u83b7\u5f97\u51b3\u7b56\u6811\u7684\u9ad8\u5ea6\n    Args:\n        tree\n    Returns:\n        \u6811\u9ad8\n    \"\"\"\n\n    if not isinstance(tree, dict):\n        return 1\n\n    child_trees = tree.values()[0].values()\n\n    # \u904d\u5386\u5b50\u6811, \u83b7\u5f97\u5b50\u6811\u7684\u6700\u5927\u9ad8\u5ea6\n    max_height = 0\n    for child_tree in child_trees:\n        child_tree_height = get_tree_height(child_tree)\n\n        if child_tree_height > max_height:\n            max_height = child_tree_height\n\n    return max_height + 1\n\n\nif __name__ == \"__main__\":\n    fishTest()\n    # ContactLensesTest()\n", "src/py2.x/ml/3.DecisionTree/DTSklearn.py": "#!/usr/bin/python\n# coding: utf8\n# \u539f\u59cb\u94fe\u63a5:  http://blog.csdn.net/lsldd/article/details/41223147\n# GitHub: https://github.com/apachecn/AiLearning\nfrom __future__ import print_function\nimport numpy as np\nfrom sklearn import tree\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import classification_report\nfrom sklearn.cross_validation import train_test_split\n\n\ndef createDataSet():\n    ''' \u6570\u636e\u8bfb\u5165 '''\n    data = []\n    labels = []\n    with open(\"data/3.DecisionTree/data.txt\") as ifile:\n        for line in ifile:\n            # \u7279\u5f81:  \u8eab\u9ad8 \u4f53\u91cd   label:  \u80d6\u7626\n            tokens = line.strip().split(' ')\n            data.append([float(tk) for tk in tokens[:-1]])\n            labels.append(tokens[-1])\n    # \u7279\u5f81\u6570\u636e\n    x = np.array(data)\n    # label\u5206\u7c7b\u7684\u6807\u7b7e\u6570\u636e\n    labels = np.array(labels)\n    # \u9884\u4f30\u7ed3\u679c\u7684\u6807\u7b7e\u6570\u636e\n    y = np.zeros(labels.shape)\n\n    ''' \u6807\u7b7e\u8f6c\u6362\u4e3a0/1 '''\n    y[labels == 'fat'] = 1\n    print(data, '-------', x, '-------', labels, '-------', y)\n    return x, y\n\n\ndef predict_train(x_train, y_train):\n    '''\n    \u4f7f\u7528\u4fe1\u606f\u71b5\u4f5c\u4e3a\u5212\u5206\u6807\u51c6\uff0c\u5bf9\u51b3\u7b56\u6811\u8fdb\u884c\u8bad\u7ec3\n    \u53c2\u8003\u94fe\u63a5:  http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier\n    '''\n    clf = tree.DecisionTreeClassifier(criterion='entropy')\n    # print(clf)\n    clf.fit(x_train, y_train)\n    ''' \u7cfb\u6570\u53cd\u6620\u6bcf\u4e2a\u7279\u5f81\u7684\u5f71\u54cd\u529b\u3002\u8d8a\u5927\u8868\u793a\u8be5\u7279\u5f81\u5728\u5206\u7c7b\u4e2d\u8d77\u5230\u7684\u4f5c\u7528\u8d8a\u5927 '''\n    print('feature_importances_: %s' % clf.feature_importances_)\n\n    '''\u6d4b\u8bd5\u7ed3\u679c\u7684\u6253\u5370'''\n    y_pre = clf.predict(x_train)\n    # print(x_train)\n    print(y_pre)\n    print(y_train)\n    print(np.mean(y_pre == y_train))\n    return y_pre, clf\n\n\ndef show_precision_recall(x, y, clf,  y_train, y_pre):\n    '''\n    \u51c6\u786e\u7387\u4e0e\u53ec\u56de\u7387\n    \u53c2\u8003\u94fe\u63a5:  http://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_curve.html#sklearn.metrics.precision_recall_curve\n    '''\n    precision, recall, thresholds = precision_recall_curve(y_train, y_pre)\n    # \u8ba1\u7b97\u5168\u91cf\u7684\u9884\u4f30\u7ed3\u679c\n    answer = clf.predict_proba(x)[:, 1]\n\n    '''\n    \u5c55\u73b0 \u51c6\u786e\u7387\u4e0e\u53ec\u56de\u7387\n        precision \u51c6\u786e\u7387\n        recall \u53ec\u56de\u7387\n        f1-score  \u51c6\u786e\u7387\u548c\u53ec\u56de\u7387\u7684\u4e00\u4e2a\u7efc\u5408\u5f97\u5206\n        support \u53c2\u4e0e\u6bd4\u8f83\u7684\u6570\u91cf\n    \u53c2\u8003\u94fe\u63a5: http://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html#sklearn.metrics.classification_report\n    '''\n    # target_names \u4ee5 y\u7684label\u5206\u7c7b\u4e3a\u51c6\n    target_names = ['thin', 'fat']\n    print(classification_report(y, answer, target_names=target_names))\n    print(answer)\n    print(y)\n\n\ndef show_pdf(clf):\n    '''\n    \u53ef\u89c6\u5316\u8f93\u51fa\n    \u628a\u51b3\u7b56\u6811\u7ed3\u6784\u5199\u5165\u6587\u4ef6: http://sklearn.lzjqsdd.com/modules/tree.html\n\n    Mac\u62a5\u9519: pydotplus.graphviz.InvocationException: GraphViz's executables not found\n    \u89e3\u51b3\u65b9\u6848: sudo brew install graphviz\n    \u53c2\u8003\u5199\u5165:  http://www.jianshu.com/p/59b510bafb4d\n    '''\n    # with open(\"testResult/tree.dot\", 'w') as f:\n    #     from sklearn.externals.six import StringIO\n    #     tree.export_graphviz(clf, out_file=f)\n\n    import pydotplus\n    from sklearn.externals.six import StringIO\n    dot_data = StringIO()\n    tree.export_graphviz(clf, out_file=dot_data)\n    graph = pydotplus.graph_from_dot_data(dot_data.getvalue())\n    graph.write_pdf(\"output/3.DecisionTree/tree.pdf\")\n\n    # from IPython.display import Image\n    # Image(graph.create_png())\n\n\nif __name__ == '__main__':\n    x, y = createDataSet()\n\n    ''' \u62c6\u5206\u8bad\u7ec3\u6570\u636e\u4e0e\u6d4b\u8bd5\u6570\u636e\uff0c 80%\u505a\u8bad\u7ec3 20%\u505a\u6d4b\u8bd5 '''\n    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n    print('\u62c6\u5206\u6570\u636e: ', x_train, x_test, y_train, y_test)\n\n    # \u5f97\u5230\u8bad\u7ec3\u7684\u9884\u6d4b\u7ed3\u679c\u96c6\n    y_pre, clf = predict_train(x_train, y_train)\n\n    # \u5c55\u73b0 \u51c6\u786e\u7387\u4e0e\u53ec\u56de\u7387\n    show_precision_recall(x, y, clf, y_train, y_pre)\n\n    # \u53ef\u89c6\u5316\u8f93\u51fa\n    show_pdf(clf)\n", "src/py2.x/ml/3.DecisionTree/decisionTreePlot.py": "#!/usr/bin/python\n# coding:utf8\n\n'''\nCreated on Oct 14, 2010\nUpdate on 2018-01-04\nDecision Tree Source Code for Machine Learning in Action Ch. 3\nAuthor: Peter Harrington/jiangzhonglian/zh0ng\n'''\nimport matplotlib.pyplot as plt\n\n# \u5b9a\u4e49\u6587\u672c\u6846 \u548c \u7bad\u5934\u683c\u5f0f \u3010 sawtooth \u6ce2\u6d6a\u65b9\u6846, round4 \u77e9\u5f62\u65b9\u6846 , fc\u8868\u793a\u5b57\u4f53\u989c\u8272\u7684\u6df1\u6d45 0.1~0.9 \u4f9d\u6b21\u53d8\u6d45\uff0c\u6ca1\u9519\u662f\u53d8\u6d45\u3011\ndecisionNode = dict(boxstyle=\"sawtooth\", fc=\"0.8\")\nleafNode = dict(boxstyle=\"round4\", fc=\"0.8\")\narrow_args = dict(arrowstyle=\"<-\")\n\n\ndef getNumLeafs(myTree):\n    numLeafs = 0\n    firstStr = myTree.keys()[0]\n    secondDict = myTree[firstStr]\n    # \u6839\u8282\u70b9\u5f00\u59cb\u904d\u5386\n    for key in secondDict.keys():\n        # \u5224\u65ad\u5b50\u8282\u70b9\u662f\u5426\u4e3adict, \u4e0d\u662f+1\n        if type(secondDict[key]) is dict:\n            numLeafs += getNumLeafs(secondDict[key])\n        else:\n            numLeafs += 1\n    return numLeafs\n\n\ndef getTreeDepth(myTree):\n    maxDepth = 0\n    firstStr = myTree.keys()[0]\n    secondDict = myTree[firstStr]\n    # \u6839\u8282\u70b9\u5f00\u59cb\u904d\u5386\n    for key in secondDict.keys():\n        # \u5224\u65ad\u5b50\u8282\u70b9\u662f\u4e0d\u662fdict, \u6c42\u5206\u679d\u7684\u6df1\u5ea6\n        if type(secondDict[key]) is dict:\n            thisDepth = 1 + getTreeDepth(secondDict[key])\n        else:\n            thisDepth = 1\n        # \u8bb0\u5f55\u6700\u5927\u7684\u5206\u652f\u6df1\u5ea6\n        if thisDepth > maxDepth:\n            maxDepth = thisDepth\n    return maxDepth\n\n\ndef plotNode(nodeTxt, centerPt, parentPt, nodeType):\n    createPlot.ax1.annotate(nodeTxt, xy=parentPt,  xycoords='axes fraction', xytext=centerPt,\n                            textcoords='axes fraction', va=\"center\", ha=\"center\", bbox=nodeType, arrowprops=arrow_args)\n\n\ndef plotMidText(cntrPt, parentPt, txtString):\n    xMid = (parentPt[0] - cntrPt[0]) / 2.0 + cntrPt[0]\n    yMid = (parentPt[1] - cntrPt[1]) / 2.0 + cntrPt[1]\n    createPlot.ax1.text(xMid, yMid, txtString, va=\"center\", ha=\"center\", rotation=30)\n\n\ndef plotTree(myTree, parentPt, nodeTxt):\n    # \u83b7\u53d6\u53f6\u5b50\u8282\u70b9\u7684\u6570\u91cf\n    numLeafs = getNumLeafs(myTree)\n    # \u83b7\u53d6\u6811\u7684\u6df1\u5ea6\n    # depth = getTreeDepth(myTree)\n\n    # \u627e\u51fa\u7b2c1\u4e2a\u4e2d\u5fc3\u70b9\u7684\u4f4d\u7f6e\uff0c\u7136\u540e\u4e0e parentPt\u5b9a\u70b9\u8fdb\u884c\u5212\u7ebf\n    # x\u5750\u6807\u4e3a (numLeafs-1.)/plotTree.totalW/2+1./plotTree.totalW\uff0c\u5316\u7b80\u5982\u4e0b\n    cntrPt = (plotTree.xOff + (1.0 + float(numLeafs)) / 2.0 / plotTree.totalW, plotTree.yOff)\n    # print cntrPt\n    # \u5e76\u6253\u5370\u8f93\u5165\u5bf9\u5e94\u7684\u6587\u5b57\n    plotMidText(cntrPt, parentPt, nodeTxt)\n\n    firstStr = myTree.keys()[0]\n    # \u53ef\u89c6\u5316Node\u5206\u652f\u70b9\uff1b\u7b2c\u4e00\u6b21\u8c03\u7528plotTree\u65f6\uff0ccntrPt\u4e0eparentPt\u76f8\u540c\n    plotNode(firstStr, cntrPt, parentPt, decisionNode)\n    # \u6839\u8282\u70b9\u7684\u503c\n    secondDict = myTree[firstStr]\n    # y\u503c = \u6700\u9ad8\u70b9-\u5c42\u6570\u7684\u9ad8\u5ea6[\u7b2c\u4e8c\u4e2a\u8282\u70b9\u4f4d\u7f6e]\uff1b1.0\u76f8\u5f53\u4e8e\u6811\u7684\u9ad8\u5ea6\n    plotTree.yOff = plotTree.yOff - 1.0 / plotTree.totalD\n    for key in secondDict.keys():\n        # \u5224\u65ad\u8be5\u8282\u70b9\u662f\u5426\u662fNode\u8282\u70b9\n        if type(secondDict[key]) is dict:\n            # \u5982\u679c\u662f\u5c31\u9012\u5f52\u8c03\u7528[recursion]\n            plotTree(secondDict[key], cntrPt, str(key))\n        else:\n            # \u5982\u679c\u4e0d\u662f\uff0c\u5c31\u5728\u539f\u6765\u8282\u70b9\u4e00\u534a\u7684\u5730\u65b9\u627e\u5230\u8282\u70b9\u7684\u5750\u6807\n            plotTree.xOff = plotTree.xOff + 1.0 / plotTree.totalW\n            # \u53ef\u89c6\u5316\u8be5\u8282\u70b9\u4f4d\u7f6e\n            plotNode(secondDict[key], (plotTree.xOff, plotTree.yOff), cntrPt, leafNode)\n            # \u5e76\u6253\u5370\u8f93\u5165\u5bf9\u5e94\u7684\u6587\u5b57\n            plotMidText((plotTree.xOff, plotTree.yOff), cntrPt, str(key))\n    plotTree.yOff = plotTree.yOff + 1.0 / plotTree.totalD\n\n\ndef createPlot(inTree):\n    # \u521b\u5efa\u4e00\u4e2afigure\u7684\u6a21\u7248\n    fig = plt.figure(1, facecolor='green')\n    fig.clf()\n\n    axprops = dict(xticks=[], yticks=[])\n    # \u8868\u793a\u521b\u5efa\u4e00\u4e2a1\u884c\uff0c1\u5217\u7684\u56fe\uff0ccreatePlot.ax1 \u4e3a\u7b2c 1 \u4e2a\u5b50\u56fe\uff0c\n    createPlot.ax1 = plt.subplot(111, frameon=False, **axprops)\n\n    plotTree.totalW = float(getNumLeafs(inTree))\n    plotTree.totalD = float(getTreeDepth(inTree))\n    # \u534a\u4e2a\u8282\u70b9\u7684\u957f\u5ea6\uff1bxOff\u8868\u793a\u5f53\u524dplotTree\u672a\u904d\u5386\u5230\u7684\u6700\u5de6\u7684\u53f6\u8282\u70b9\u7684\u5de6\u8fb9\u4e00\u4e2a\u53f6\u8282\u70b9\u7684x\u5750\u6807\n    # \u6240\u6709\u53f6\u8282\u70b9\u4e2d\uff0c\u6700\u5de6\u7684\u53f6\u8282\u70b9\u7684x\u5750\u6807\u662f0.5/plotTree.totalW\uff08\u56e0\u4e3atotalW\u4e2a\u53f6\u8282\u70b9\u5728x\u8f74\u65b9\u5411\u662f\u5e73\u5747\u5206\u5e03\u5728[0, 1]\u533a\u95f4\u4e0a\u7684\uff09\n    # \u56e0\u6b64\uff0cxOff\u7684\u521d\u59cb\u503c\u5e94\u8be5\u662f 0.5/plotTree.totalW-\u76f8\u90bb\u4e24\u4e2a\u53f6\u8282\u70b9\u7684x\u8f74\u65b9\u5411\u8ddd\u79bb\n    plotTree.xOff = -0.5 / plotTree.totalW\n    # \u6839\u8282\u70b9\u7684y\u5750\u6807\u4e3a1.0\uff0c\u6811\u7684\u6700\u4f4e\u70b9y\u5750\u6807\u4e3a0\n    plotTree.yOff = 1.0\n    # \u7b2c\u4e8c\u4e2a\u53c2\u6570\u662f\u6839\u8282\u70b9\u7684\u5750\u6807\n    plotTree(inTree, (0.5, 1.0), '')\n    plt.show()\n\n\n# # \u6d4b\u8bd5\u753b\u56fe\n# def createPlot():\n#     fig = plt.figure(1, facecolor='white')\n#     fig.clf()\n#     # ticks for demo purposes\n#     createPlot.ax1 = plt.subplot(111, frameon=False)\n#     plotNode('a decision node', (0.5, 0.1), (0.1, 0.5), decisionNode)\n#     plotNode('a leaf node', (0.8, 0.1), (0.3, 0.8), leafNode)\n#     plt.show()\n\n\n# \u6d4b\u8bd5\u6570\u636e\u96c6\ndef retrieveTree(i):\n    listOfTrees = [\n        {'no surfacing': {0: 'no', 1: {'flippers': {0: 'no', 1: 'yes'}}}},\n        {'no surfacing': {0: 'no', 1: {'flippers': {0: {'head': {0: 'no', 1: 'yes'}}, 1: 'no'}}}}\n    ]\n    return listOfTrees[i]\n\n# \u7528\u6d4b\u8bd5\u6570\u636e\u7ed8\u5236\u6811\n# myTree = retrieveTree(1)\n# createPlot(myTree)\n", "src/py2.x/ml/3.DecisionTree/sklearn_dts_classify_demo.py": "#!/usr/bin/python\n# coding:utf-8\n\n\"\"\"\nCreated on 2017-06-29\nUpdated on 2017-06-29\nDecisionTree: \u51b3\u7b56\u6811\nAuthor: \u5c0f\u7476\nGitHub: https://github.com/apachecn/AiLearning\n\"\"\"\nfrom __future__ import print_function\nprint(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.datasets import load_iris\nfrom sklearn.tree import DecisionTreeClassifier\n\n# \u53c2\u6570\nn_classes = 3\nplot_colors = \"bry\"\nplot_step = 0.02\n\n# \u52a0\u8f7d\u6570\u636e\niris = load_iris()\n\nfor pairidx, pair in enumerate([[0, 1], [0, 2], [0, 3], [1, 2], [1, 3], [2, 3]]):\n    # \u6211\u4eec\u53ea\u7528\u4e24\u4e2a\u76f8\u5e94\u7684features\n    X = iris.data[:, pair]\n    y = iris.target\n\n    # \u8bad\u7ec3\n    clf = DecisionTreeClassifier().fit(X, y)\n\n    # \u7ed8\u5236\u51b3\u7b56\u8fb9\u754c\n    plt.subplot(2, 3, pairidx + 1)\n\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),\n                         np.arange(y_min, y_max, plot_step))\n\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    cs = plt.contourf(xx, yy, Z, cmap=plt.cm.Paired)\n\n    plt.xlabel(iris.feature_names[pair[0]])\n    plt.ylabel(iris.feature_names[pair[1]])\n    plt.axis(\"tight\")\n\n    # \u7ed8\u5236\u8bad\u7ec3\u70b9\n    for i, color in zip(range(n_classes), plot_colors):\n        idx = np.where(y == i)\n        plt.scatter(X[idx, 0], X[idx, 1], c=color, label=iris.target_names[i],\n                    cmap=plt.cm.Paired)\n\n    plt.axis(\"tight\")\n\nplt.suptitle(\"Decision surface of a decision tree using paired features\")\nplt.legend()\nplt.show()\n", "src/py2.x/ml/3.DecisionTree/skelearn_dts_regressor_demo.py": "#!/usr/bin/python\n# coding:utf8\n\n\"\"\"\nCreated on 2017-06-29\nUpdated on 2017-06-29\nDecisionTree: \u51b3\u7b56\u6811\nAuthor: \u5c0f\u7476\nGitHub: https://github.com/apachecn/AiLearning\n\"\"\"\nfrom __future__ import print_function\n\nprint(__doc__)\n\n# \u5f15\u5165\u5fc5\u8981\u7684\u6a21\u578b\u548c\u5e93\nimport numpy as np\nfrom sklearn.tree import DecisionTreeRegressor\nimport matplotlib.pyplot as plt\n\n# \u521b\u5efa\u4e00\u4e2a\u968f\u673a\u7684\u6570\u636e\u96c6\n# \u53c2\u8003 https://docs.scipy.org/doc/numpy-1.6.0/reference/generated/numpy.random.mtrand.RandomState.html\nrng = np.random.RandomState(1)\n# print 'lalalalala===', rng\n# rand() \u662f\u7ed9\u5b9a\u5f62\u72b6\u7684\u968f\u673a\u503c\uff0crng.rand(80, 1)\u5373\u77e9\u9635\u7684\u5f62\u72b6\u662f 80\u884c\uff0c1\u5217\n# sort() \nX = np.sort(5 * rng.rand(80, 1), axis=0)\n# print 'X=', X\ny = np.sin(X).ravel()\n# print 'y=', y\ny[::5] += 3 * (0.5 - rng.rand(16))\n# print 'yyy=', y\n\n# \u62df\u5408\u56de\u5f52\u6a21\u578b\n# regr_1 = DecisionTreeRegressor(max_depth=2)\n# \u4fdd\u6301 max_depth=5 \u4e0d\u53d8\uff0c\u589e\u52a0 min_samples_leaf=6 \u7684\u53c2\u6570\uff0c\u6548\u679c\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\nregr_2 = DecisionTreeRegressor(max_depth=5)\nregr_2 = DecisionTreeRegressor(min_samples_leaf=6)\n# regr_3 = DecisionTreeRegressor(max_depth=4)\n# regr_1.fit(X, y)\nregr_2.fit(X, y)\n# regr_3.fit(X, y)\n\n# \u9884\u6d4b\nX_test = np.arange(0.0, 5.0, 0.01)[:, np.newaxis]\n# y_1 = regr_1.predict(X_test)\ny_2 = regr_2.predict(X_test)\n# y_3 = regr_3.predict(X_test)\n\n# \u7ed8\u5236\u7ed3\u679c\nplt.figure()\nplt.scatter(X, y, c=\"darkorange\", label=\"data\")\n# plt.plot(X_test, y_1, color=\"cornflowerblue\", label=\"max_depth=2\", linewidth=2)\nplt.plot(X_test, y_2, color=\"yellowgreen\", label=\"max_depth=5\", linewidth=2)\n# plt.plot(X_test, y_3, color=\"red\", label=\"max_depth=3\", linewidth=2)\nplt.xlabel(\"data\")\nplt.ylabel(\"target\")\nplt.title(\"Decision Tree Regression\")\nplt.legend()\nplt.show()", "src/py2.x/ml/12.FrequentPattemTree/fpGrowth.py": "#!/usr/bin/python\n# coding:utf8\n\n'''\nCreated on Jun 14, 2011\nUpdate  on 2017-05-18\nFP-Growth FP means frequent pattern\nthe FP-Growth algorithm needs:\n1. FP-tree (class treeNode)\n2. header table (use dict)\nThis finds frequent itemsets similar to apriori but does not find association rules.\nAuthor: Peter/\u7247\u523b\nGitHub: https://github.com/apachecn/AiLearning\n'''\nfrom __future__ import print_function\nprint(__doc__)\n\n\nclass treeNode:\n    def __init__(self, nameValue, numOccur, parentNode):\n        self.name = nameValue\n        self.count = numOccur\n        self.nodeLink = None\n        # needs to be updated\n        self.parent = parentNode\n        self.children = {}\n\n    def inc(self, numOccur):\n        \"\"\"inc(\u5bf9count\u53d8\u91cf\u589e\u52a0\u7ed9\u5b9a\u503c)\n        \"\"\"\n        self.count += numOccur\n\n    def disp(self, ind=1):\n        \"\"\"disp(\u7528\u4e8e\u5c06\u6811\u4ee5\u6587\u672c\u5f62\u5f0f\u663e\u793a)\n\n        \"\"\"\n        print('  '*ind, self.name, ' ', self.count)\n        for child in self.children.values():\n            child.disp(ind+1)\n\n\ndef loadSimpDat():\n    simpDat = [['r', 'z', 'h', 'j', 'p'],\n               ['z', 'y', 'x', 'w', 'v', 'u', 't', 's'],\n               ['z'],\n               ['r', 'x', 'n', 'o', 's'],\n            #    ['r', 'x', 'n', 'o', 's'],\n               ['y', 'r', 'x', 'z', 'q', 't', 'p'],\n               ['y', 'z', 'x', 'e', 'q', 's', 't', 'm']]\n    return simpDat\n\n\ndef createInitSet(dataSet):\n    retDict = {}\n    for trans in dataSet:\n        if not retDict.has_key(frozenset(trans)):\n            retDict[frozenset(trans)] = 1\n        else:\n            retDict[frozenset(trans)] += 1\n    return retDict\n\n\n# this version does not use recursion\ndef updateHeader(nodeToTest, targetNode):\n    \"\"\"updateHeader(\u66f4\u65b0\u5934\u6307\u9488\uff0c\u5efa\u7acb\u76f8\u540c\u5143\u7d20\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u4f8b\u5982:  \u5de6\u8fb9\u7684r\u6307\u5411\u53f3\u8fb9\u7684r\u503c\uff0c\u5c31\u662f\u540e\u51fa\u73b0\u7684\u76f8\u540c\u5143\u7d20 \u6307\u5411 \u5df2\u7ecf\u51fa\u73b0\u7684\u5143\u7d20)\n\n    \u4ece\u5934\u6307\u9488\u7684nodeLink\u5f00\u59cb\uff0c\u4e00\u76f4\u6cbf\u7740nodeLink\u76f4\u5230\u5230\u8fbe\u94fe\u8868\u672b\u5c3e\u3002\u8fd9\u5c31\u662f\u94fe\u8868\u3002\n    \u6027\u80fd: \u5982\u679c\u94fe\u8868\u5f88\u957f\u53ef\u80fd\u4f1a\u9047\u5230\u8fed\u4ee3\u8c03\u7528\u7684\u6b21\u6570\u9650\u5236\u3002\n\n    Args:\n        nodeToTest  \u6ee1\u8db3minSup {\u6240\u6709\u7684\u5143\u7d20+(value, treeNode)}\n        targetNode  Tree\u5bf9\u8c61\u7684\u5b50\u8282\u70b9\n    \"\"\"\n    # \u5efa\u7acb\u76f8\u540c\u5143\u7d20\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u4f8b\u5982:  \u5de6\u8fb9\u7684r\u6307\u5411\u53f3\u8fb9\u7684r\u503c\n    while (nodeToTest.nodeLink is not None):\n        nodeToTest = nodeToTest.nodeLink\n    nodeToTest.nodeLink = targetNode\n\n\ndef updateTree(items, inTree, headerTable, count):\n    \"\"\"updateTree(\u66f4\u65b0FP-tree\uff0c\u7b2c\u4e8c\u6b21\u904d\u5386)\n\n    # \u9488\u5bf9\u6bcf\u4e00\u884c\u7684\u6570\u636e\n    # \u6700\u5927\u7684key,  \u6dfb\u52a0\n    Args:\n        items       \u6ee1\u8db3minSup \u6392\u5e8f\u540e\u7684\u5143\u7d20key\u7684\u6570\u7ec4\uff08\u5927\u5230\u5c0f\u7684\u6392\u5e8f\uff09\n        inTree      \u7a7a\u7684Tree\u5bf9\u8c61\n        headerTable \u6ee1\u8db3minSup {\u6240\u6709\u7684\u5143\u7d20+(value, treeNode)}\n        count       \u539f\u6570\u636e\u96c6\u4e2d\u6bcf\u4e00\u7ec4Kay\u51fa\u73b0\u7684\u6b21\u6570\n    \"\"\"\n    # \u53d6\u51fa \u5143\u7d20 \u51fa\u73b0\u6b21\u6570\u6700\u9ad8\u7684\n    # \u5982\u679c\u8be5\u5143\u7d20\u5728 inTree.children \u8fd9\u4e2a\u5b57\u5178\u4e2d\uff0c\u5c31\u8fdb\u884c\u7d2f\u52a0\n    # \u5982\u679c\u8be5\u5143\u7d20\u4e0d\u5b58\u5728 \u5c31 inTree.children \u5b57\u5178\u4e2d\u65b0\u589ekey\uff0cvalue\u4e3a\u521d\u59cb\u5316\u7684 treeNode \u5bf9\u8c61\n    if items[0] in inTree.children:\n        # \u66f4\u65b0 \u6700\u5927\u5143\u7d20\uff0c\u5bf9\u5e94\u7684 treeNode \u5bf9\u8c61\u7684count\u8fdb\u884c\u53e0\u52a0\n        inTree.children[items[0]].inc(count)\n    else:\n        # \u5982\u679c\u4e0d\u5b58\u5728\u5b50\u8282\u70b9\uff0c\u6211\u4eec\u4e3a\u8be5inTree\u6dfb\u52a0\u5b50\u8282\u70b9\n        inTree.children[items[0]] = treeNode(items[0], count, inTree)\n        # \u5982\u679c\u6ee1\u8db3minSup\u7684dist\u5b57\u5178\u7684value\u503c\u7b2c\u4e8c\u4f4d\u4e3anull\uff0c \u6211\u4eec\u5c31\u8bbe\u7f6e\u8be5\u5143\u7d20\u4e3a \u672c\u8282\u70b9\u5bf9\u5e94\u7684tree\u8282\u70b9\n        # \u5982\u679c\u5143\u7d20\u7b2c\u4e8c\u4f4d\u4e0d\u4e3anull\uff0c\u6211\u4eec\u5c31\u66f4\u65b0header\u8282\u70b9\n        if headerTable[items[0]][1] is None:\n            # headerTable\u53ea\u8bb0\u5f55\u7b2c\u4e00\u6b21\u8282\u70b9\u51fa\u73b0\u7684\u4f4d\u7f6e\n            headerTable[items[0]][1] = inTree.children[items[0]]\n        else:\n            # \u672c\u8d28\u4e0a\u662f\u4fee\u6539headerTable\u7684key\u5bf9\u5e94\u7684Tree\uff0c\u7684nodeLink\u503c\n            updateHeader(headerTable[items[0]][1], inTree.children[items[0]])\n    if len(items) > 1:\n        # \u9012\u5f52\u7684\u8c03\u7528\uff0c\u5728items[0]\u7684\u57fa\u7840\u4e0a\uff0c\u6dfb\u52a0item0[1]\u505a\u5b50\u8282\u70b9\uff0c count\u53ea\u8981\u5faa\u73af\u7684\u8fdb\u884c\u7d2f\u8ba1\u52a0\u548c\u800c\u5df2\uff0c\u7edf\u8ba1\u51fa\u8282\u70b9\u7684\u6700\u540e\u7684\u7edf\u8ba1\u503c\u3002\n        updateTree(items[1:], inTree.children[items[0]], headerTable, count)\n\n\ndef createTree(dataSet, minSup=1):\n    \"\"\"createTree(\u751f\u6210FP-tree)\n\n    Args:\n        dataSet  dist{\u884c: \u51fa\u73b0\u6b21\u6570}\u7684\u6837\u672c\u6570\u636e\n        minSup   \u6700\u5c0f\u7684\u652f\u6301\u5ea6\n    Returns:\n        retTree  FP-tree\n        headerTable \u6ee1\u8db3minSup {\u6240\u6709\u7684\u5143\u7d20+(value, treeNode)}\n    \"\"\"\n    # \u652f\u6301\u5ea6>=minSup\u7684dist{\u6240\u6709\u5143\u7d20: \u51fa\u73b0\u7684\u6b21\u6570}\n    headerTable = {}\n    # \u5faa\u73af dist{\u884c: \u51fa\u73b0\u6b21\u6570}\u7684\u6837\u672c\u6570\u636e\n    for trans in dataSet:\n        # \u5bf9\u6240\u6709\u7684\u884c\u8fdb\u884c\u5faa\u73af\uff0c\u5f97\u5230\u884c\u91cc\u9762\u7684\u6240\u6709\u5143\u7d20\n        # \u7edf\u8ba1\u6bcf\u4e00\u884c\u4e2d\uff0c\u6bcf\u4e2a\u5143\u7d20\u51fa\u73b0\u7684\u603b\u6b21\u6570\n        for item in trans:\n            # \u4f8b\u5982:  {'ababa': 3}  count(a)=3+3+3=9   count(b)=3+3=6\n            headerTable[item] = headerTable.get(item, 0) + dataSet[trans]\n    # \u5220\u9664 headerTable\u4e2d\uff0c\u5143\u7d20\u6b21\u6570<\u6700\u5c0f\u652f\u6301\u5ea6\u7684\u5143\u7d20\n    for k in headerTable.keys():\n        if headerTable[k] < minSup:\n            del(headerTable[k])\n\n    # \u6ee1\u8db3minSup: set(\u5404\u5143\u7d20\u96c6\u5408)\n    freqItemSet = set(headerTable.keys())\n    # \u5982\u679c\u4e0d\u5b58\u5728\uff0c\u76f4\u63a5\u8fd4\u56deNone\n    if len(freqItemSet) == 0:\n        return None, None\n    for k in headerTable:\n        # \u683c\u5f0f\u5316:  dist{\u5143\u7d20key: [\u5143\u7d20\u6b21\u6570, None]}\n        headerTable[k] = [headerTable[k], None]\n\n    # create tree\n    retTree = treeNode('Null Set', 1, None)\n    # \u5faa\u73af dist{\u884c: \u51fa\u73b0\u6b21\u6570}\u7684\u6837\u672c\u6570\u636e\n    for tranSet, count in dataSet.items():\n        # print 'tranSet, count=', tranSet, count\n        # localD = dist{\u5143\u7d20key: \u5143\u7d20\u603b\u51fa\u73b0\u6b21\u6570}\n        localD = {}\n        for item in tranSet:\n            # \u5224\u65ad\u662f\u5426\u5728\u6ee1\u8db3minSup\u7684\u96c6\u5408\u4e2d\n            if item in freqItemSet:\n                # print 'headerTable[item][0]=', headerTable[item][0], headerTable[item]\n                localD[item] = headerTable[item][0]\n        # print 'localD=', localD\n        if len(localD) > 0:\n            # p=key,value; \u6240\u4ee5\u662f\u901a\u8fc7value\u503c\u7684\u5927\u5c0f\uff0c\u8fdb\u884c\u4ece\u5927\u5230\u5c0f\u8fdb\u884c\u6392\u5e8f\n            # orderedItems \u8868\u793a\u53d6\u51fa\u5143\u7ec4\u7684key\u503c\uff0c\u4e5f\u5c31\u662f\u5b57\u6bcd\u672c\u8eab\uff0c\u4f46\u662f\u5b57\u6bcd\u672c\u8eab\u662f\u5927\u5230\u5c0f\u7684\u987a\u5e8f\n            orderedItems = [v[0] for v in sorted(localD.items(), key=lambda p: p[1], reverse=True)]\n            # print 'orderedItems=', orderedItems, 'headerTable', headerTable, '\\n\\n\\n'\n            # \u586b\u5145\u6811\uff0c\u901a\u8fc7\u6709\u5e8f\u7684orderedItems\u7684\u7b2c\u4e00\u4f4d\uff0c\u8fdb\u884c\u987a\u5e8f\u586b\u5145 \u7b2c\u4e00\u5c42\u7684\u5b50\u8282\u70b9\u3002\n            updateTree(orderedItems, retTree, headerTable, count)\n\n    return retTree, headerTable\n\n\ndef ascendTree(leafNode, prefixPath):\n    \"\"\"ascendTree(\u5982\u679c\u5b58\u5728\u7236\u8282\u70b9\uff0c\u5c31\u8bb0\u5f55\u5f53\u524d\u8282\u70b9\u7684name\u503c)\n\n    Args:\n        leafNode   \u67e5\u8be2\u7684\u8282\u70b9\u5bf9\u4e8e\u7684nodeTree\n        prefixPath \u8981\u67e5\u8be2\u7684\u8282\u70b9\u503c\n    \"\"\"\n    if leafNode.parent is not None:\n        prefixPath.append(leafNode.name)\n        ascendTree(leafNode.parent, prefixPath)\n\n\ndef findPrefixPath(basePat, treeNode):\n    \"\"\"findPrefixPath \u57fa\u7840\u6570\u636e\u96c6\n\n    Args:\n        basePat  \u8981\u67e5\u8be2\u7684\u8282\u70b9\u503c\n        treeNode \u67e5\u8be2\u7684\u8282\u70b9\u6240\u5728\u7684\u5f53\u524dnodeTree\n    Returns:\n        condPats \u5bf9\u975ebasePat\u7684\u5012\u53d9\u503c\u4f5c\u4e3akey,\u8d4b\u503c\u4e3acount\u6570\n    \"\"\"\n    condPats = {}\n    # \u5bf9 treeNode\u7684link\u8fdb\u884c\u5faa\u73af\n    while treeNode is not None:\n        prefixPath = []\n        # \u5bfb\u627e\u6539\u8282\u70b9\u7684\u7236\u8282\u70b9\uff0c\u76f8\u5f53\u4e8e\u627e\u5230\u4e86\u8be5\u8282\u70b9\u7684\u9891\u7e41\u9879\u96c6\n        ascendTree(treeNode, prefixPath)\n        # \u907f\u514d \u5355\u72ec`Z`\u4e00\u4e2a\u5143\u7d20\uff0c\u6dfb\u52a0\u4e86\u7a7a\u8282\u70b9\n        if len(prefixPath) > 1:\n            # \u5bf9\u975ebasePat\u7684\u5012\u53d9\u503c\u4f5c\u4e3akey,\u8d4b\u503c\u4e3acount\u6570\n            # prefixPath[1:] \u53d8frozenset\u540e\uff0c\u5b57\u6bcd\u5c31\u53d8\u65e0\u5e8f\u4e86\n            # condPats[frozenset(prefixPath)] = treeNode.count\n            condPats[frozenset(prefixPath[1:])] = treeNode.count\n        # \u9012\u5f52\uff0c\u5bfb\u627e\u6539\u8282\u70b9\u7684\u4e0b\u4e00\u4e2a \u76f8\u540c\u503c\u7684\u94fe\u63a5\u8282\u70b9\n        treeNode = treeNode.nodeLink\n        # print treeNode\n    return condPats\n\n\ndef mineTree(inTree, headerTable, minSup, preFix, freqItemList):\n    \"\"\"mineTree(\u521b\u5efa\u6761\u4ef6FP\u6811)\n\n    Args:\n        inTree       myFPtree\n        headerTable  \u6ee1\u8db3minSup {\u6240\u6709\u7684\u5143\u7d20+(value, treeNode)}\n        minSup       \u6700\u5c0f\u652f\u6301\u9879\u96c6\n        preFix       preFix\u4e3anewFreqSet\u4e0a\u4e00\u6b21\u7684\u5b58\u50a8\u8bb0\u5f55\uff0c\u4e00\u65e6\u6ca1\u6709myHead\uff0c\u5c31\u4e0d\u4f1a\u66f4\u65b0\n        freqItemList \u7528\u6765\u5b58\u50a8\u9891\u7e41\u5b50\u9879\u7684\u5217\u8868\n    \"\"\"\n    # \u901a\u8fc7value\u8fdb\u884c\u4ece\u5c0f\u5230\u5927\u7684\u6392\u5e8f\uff0c \u5f97\u5230\u9891\u7e41\u9879\u96c6\u7684key\n    # \u6700\u5c0f\u652f\u6301\u9879\u96c6\u7684key\u7684list\u96c6\u5408\n    bigL = [v[0] for v in sorted(headerTable.items(), key=lambda p: p[1])]\n    print('-----', sorted(headerTable.items(), key=lambda p: p[1]))\n    print('bigL=', bigL)\n    # \u5faa\u73af\u904d\u5386 \u6700\u9891\u7e41\u9879\u96c6\u7684key\uff0c\u4ece\u5c0f\u5230\u5927\u7684\u9012\u5f52\u5bfb\u627e\u5bf9\u5e94\u7684\u9891\u7e41\u9879\u96c6\n    for basePat in bigL:\n        # preFix\u4e3anewFreqSet\u4e0a\u4e00\u6b21\u7684\u5b58\u50a8\u8bb0\u5f55\uff0c\u4e00\u65e6\u6ca1\u6709myHead\uff0c\u5c31\u4e0d\u4f1a\u66f4\u65b0\n        newFreqSet = preFix.copy()\n        newFreqSet.add(basePat)\n        print('newFreqSet=', newFreqSet, preFix)\n\n        freqItemList.append(newFreqSet)\n        print('freqItemList=', freqItemList)\n        condPattBases = findPrefixPath(basePat, headerTable[basePat][1])\n        print('condPattBases=', basePat, condPattBases)\n\n        # \u6784\u5efaFP-tree\n        myCondTree, myHead = createTree(condPattBases, minSup)\n        print('myHead=', myHead)\n        # \u6316\u6398\u6761\u4ef6 FP-tree, \u5982\u679cmyHead\u4e0d\u4e3a\u7a7a\uff0c\u8868\u793a\u6ee1\u8db3minSup {\u6240\u6709\u7684\u5143\u7d20+(value, treeNode)}\n        if myHead is not None:\n            myCondTree.disp(1)\n            print('\\n\\n\\n')\n            # \u9012\u5f52 myHead \u627e\u51fa\u9891\u7e41\u9879\u96c6\n            mineTree(myCondTree, myHead, minSup, newFreqSet, freqItemList)\n        print('\\n\\n\\n')\n\n\n# import twitter\n# from time import sleep\n# import re\n\n\n# def getLotsOfTweets(searchStr):\n#     \"\"\"\n#     \u83b7\u53d6 100\u4e2a\u641c\u7d22\u7ed3\u679c\u9875\u9762\n#     \"\"\"\n#     CONSUMER_KEY = ''\n#     CONSUMER_SECRET = ''\n#     ACCESS_TOKEN_KEY = ''\n#     ACCESS_TOKEN_SECRET = ''\n#     api = twitter.Api(consumer_key=CONSUMER_KEY, consumer_secret=CONSUMER_SECRET, access_token_key=ACCESS_TOKEN_KEY, access_token_secret=ACCESS_TOKEN_SECRET)\n\n#     # you can get 1500 results 15 pages * 100 per page\n#     resultsPages = []\n#     for i in range(1, 15):\n#         print \"fetching page %d\" % i\n#         searchResults = api.GetSearch(searchStr, per_page=100, page=i)\n#         resultsPages.append(searchResults)\n#         sleep(6)\n#     return resultsPages\n\n\n# def textParse(bigString):\n#     \"\"\"\n#     \u89e3\u6790\u9875\u9762\u5185\u5bb9\n#     \"\"\"\n#     urlsRemoved = re.sub('(http:[/][/]|www.)([a-z]|[A-Z]|[0-9]|[/.]|[~])*', '', bigString)    \n#     listOfTokens = re.split(r'\\W*', urlsRemoved)\n#     return [tok.lower() for tok in listOfTokens if len(tok) > 2]\n\n\n# def mineTweets(tweetArr, minSup=5):\n#     \"\"\"\n#     \u83b7\u53d6\u9891\u7e41\u9879\u96c6\n#     \"\"\"\n#     parsedList = []\n#     for i in range(14):\n#         for j in range(100):\n#             parsedList.append(textParse(tweetArr[i][j].text))\n#     initSet = createInitSet(parsedList)\n#     myFPtree, myHeaderTab = createTree(initSet, minSup)\n#     myFreqList = []\n#     mineTree(myFPtree, myHeaderTab, minSup, set([]), myFreqList)\n#     return myFreqList\n\n\nif __name__ == \"__main__\":\n    # rootNode = treeNode('pyramid', 9, None)\n    # rootNode.children['eye'] = treeNode('eye', 13, None)\n    # rootNode.children['phoenix'] = treeNode('phoenix', 3, None)\n    # # \u5c06\u6811\u4ee5\u6587\u672c\u5f62\u5f0f\u663e\u793a\n    # # print rootNode.disp()\n\n    # load\u6837\u672c\u6570\u636e\n    simpDat = loadSimpDat()\n    # print simpDat, '\\n'\n    # frozen set \u683c\u5f0f\u5316 \u5e76 \u91cd\u65b0\u88c5\u8f7d \u6837\u672c\u6570\u636e\uff0c\u5bf9\u6240\u6709\u7684\u884c\u8fdb\u884c\u7edf\u8ba1\u6c42\u548c\uff0c\u683c\u5f0f: {\u884c: \u51fa\u73b0\u6b21\u6570}\n    initSet = createInitSet(simpDat)\n    print(initSet)\n\n    # \u521b\u5efaFP\u6811\n    # \u8f93\u5165: dist{\u884c: \u51fa\u73b0\u6b21\u6570}\u7684\u6837\u672c\u6570\u636e  \u548c  \u6700\u5c0f\u7684\u652f\u6301\u5ea6\n    # \u8f93\u51fa: \u6700\u7ec8\u7684PF-tree\uff0c\u901a\u8fc7\u5faa\u73af\u83b7\u53d6\u7b2c\u4e00\u5c42\u7684\u8282\u70b9\uff0c\u7136\u540e\u6bcf\u4e00\u5c42\u7684\u8282\u70b9\u8fdb\u884c\u9012\u5f52\u7684\u83b7\u53d6\u6bcf\u4e00\u884c\u7684\u5b57\u8282\u70b9\uff0c\u4e5f\u5c31\u662f\u5206\u652f\u3002\u7136\u540e\u6240\u8c13\u7684\u6307\u9488\uff0c\u5c31\u662f\u540e\u6765\u7684\u6307\u5411\u5df2\u5b58\u5728\u7684\n    myFPtree, myHeaderTab = createTree(initSet, 3)\n    myFPtree.disp()\n\n    # \u62bd\u53d6\u6761\u4ef6\u6a21\u5f0f\u57fa\n    # \u67e5\u8be2\u6811\u8282\u70b9\u7684\uff0c\u9891\u7e41\u5b50\u9879\n    print('x --->', findPrefixPath('x', myHeaderTab['x'][1]))\n    print('z --->', findPrefixPath('z', myHeaderTab['z'][1]))\n    print('r --->', findPrefixPath('r', myHeaderTab['r'][1]))\n\n    # \u521b\u5efa\u6761\u4ef6\u6a21\u5f0f\u57fa\n    freqItemList = []\n    mineTree(myFPtree, myHeaderTab, 3, set([]), freqItemList)\n    print(freqItemList)\n\n    # # \u9879\u76ee\u5b9e\u6218\n    # # 1.twitter\u9879\u76ee\u6848\u4f8b\n    # # \u65e0\u6cd5\u8fd0\u884c\uff0c\u56e0\u4e3a\u6ca1\u53d1\u94fe\u63a5twitter\n    # lotsOtweets = getLotsOfTweets('RIMM')\n    # listOfTerms = mineTweets(lotsOtweets, 20)\n    # print len(listOfTerms)\n    # for t in listOfTerms:\n    #     print t\n\n    # # 2.\u65b0\u95fb\u7f51\u7ad9\u70b9\u51fb\u6d41\u4e2d\u6316\u6398\uff0c\u4f8b\u5982: \u6587\u7ae01\u9605\u8bfb\u8fc7\u7684\u4eba\uff0c\u8fd8\u9605\u8bfb\u8fc7\u4ec0\u4e48\uff1f\n    # parsedDat = [line.split() for line in open('data/12.FPGrowth/kosarak.dat').readlines()]\n    # initSet = createInitSet(parsedDat)\n    # myFPtree, myHeaderTab = createTree(initSet, 100000)\n\n    # myFreList = []\n    # mineTree(myFPtree, myHeaderTab, 100000, set([]), myFreList)\n    # print myFreList\n", "src/py2.x/ml/9.RegTrees/sklearn-regressTree-demo.py": "#!/usr/bin/python\n# coding:utf8\n\n\"\"\"\nCreated on 2017-07-13\nUpdated on 2017-07-13\nRegressionTree: \u6811\u56de\u5f52\nAuthor: \u5c0f\u7476\nGitHub: https://github.com/apachecn/AiLearning\n\"\"\"\nfrom __future__ import print_function\n\nprint(__doc__)\n\n# \u5f15\u5165\u5fc5\u8981\u7684\u6a21\u578b\u548c\u5e93\nimport numpy as np\nfrom sklearn.tree import DecisionTreeRegressor\nimport matplotlib.pyplot as plt\n\n# \u521b\u5efa\u4e00\u4e2a\u968f\u673a\u7684\u6570\u636e\u96c6\n# \u53c2\u8003 https://docs.scipy.org/doc/numpy-1.6.0/reference/generated/numpy.random.mtrand.RandomState.html\nrng = np.random.RandomState(1)\n# print 'lalalalala===', rng\n# rand() \u662f\u7ed9\u5b9a\u5f62\u72b6\u7684\u968f\u673a\u503c\uff0crng.rand(80, 1)\u5373\u77e9\u9635\u7684\u5f62\u72b6\u662f 80\u884c\uff0c1\u5217\n# sort() \nX = np.sort(5 * rng.rand(80, 1), axis=0)\n# print 'X=', X\ny = np.sin(X).ravel()\n# print 'y=', y\ny[::5] += 3 * (0.5 - rng.rand(16))\n# print 'yyy=', y\n\n# \u62df\u5408\u56de\u5f52\u6a21\u578b\n# regr_1 = DecisionTreeRegressor(max_depth=2)\n# \u4fdd\u6301 max_depth=5 \u4e0d\u53d8\uff0c\u589e\u52a0 min_samples_leaf=6 \u7684\u53c2\u6570\uff0c\u6548\u679c\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\nregr_2 = DecisionTreeRegressor(max_depth=5)\nregr_2 = DecisionTreeRegressor(min_samples_leaf=6)\n# regr_3 = DecisionTreeRegressor(max_depth=4)\n# regr_1.fit(X, y)\nregr_2.fit(X, y)\n# regr_3.fit(X, y)\n\n# \u9884\u6d4b\nX_test = np.arange(0.0, 5.0, 0.01)[:, np.newaxis]\n# y_1 = regr_1.predict(X_test)\ny_2 = regr_2.predict(X_test)\n# y_3 = regr_3.predict(X_test)\n\n# \u7ed8\u5236\u7ed3\u679c\nplt.figure()\nplt.scatter(X, y, c=\"darkorange\", label=\"data\")\n# plt.plot(X_test, y_1, color=\"cornflowerblue\", label=\"max_depth=2\", linewidth=2)\nplt.plot(X_test, y_2, color=\"yellowgreen\", label=\"max_depth=5\", linewidth=2)\n# plt.plot(X_test, y_3, color=\"red\", label=\"max_depth=3\", linewidth=2)\nplt.xlabel(\"data\")\nplt.ylabel(\"target\")\nplt.title(\"Decision Tree Regression\")\nplt.legend()\nplt.show()", "src/py2.x/ml/9.RegTrees/RTSklearn.py": "#!/usr/bin/python\n# coding:utf8\n\n# '''\n# Created on 2017-03-10\n# Update on 2017-03-10\n# author: jiangzhonglian\n# content: \u56de\u5f52\u6811\n# '''\n\n# print(__doc__)\n\n\n# # Import the necessary modules and libraries\n# import numpy as np\n# from sklearn.tree import DecisionTreeRegressor\n# import matplotlib.pyplot as plt\n\n\n# # Create a random dataset\n# rng = np.random.RandomState(1)\n# X = np.sort(5 * rng.rand(80, 1), axis=0)\n# y = np.sin(X).ravel()\n# print X, '\\n\\n\\n-----------\\n\\n\\n', y\n# y[::5] += 3 * (0.5 - rng.rand(16))\n\n\n# # Fit regression model\n# regr_1 = DecisionTreeRegressor(max_depth=2, min_samples_leaf=5)\n# regr_2 = DecisionTreeRegressor(max_depth=5, min_samples_leaf=5)\n# regr_1.fit(X, y)\n# regr_2.fit(X, y)\n\n\n# # Predict\n# X_test = np.arange(0.0, 5.0, 0.01)[:, np.newaxis]\n# y_1 = regr_1.predict(X_test)\n# y_2 = regr_2.predict(X_test)\n\n\n# # Plot the results\n# plt.figure()\n# plt.scatter(X, y, c=\"darkorange\", label=\"data\")\n# plt.plot(X_test, y_1, color=\"cornflowerblue\", label=\"max_depth=2\", linewidth=2)\n# plt.plot(X_test, y_2, color=\"yellowgreen\", label=\"max_depth=5\", linewidth=2)\n# plt.xlabel(\"data\")\n# plt.ylabel(\"target\")\n# plt.title(\"Decision Tree Regression\")\n# plt.legend()\n# plt.show()\n\n\n\n\n\n\n\n\n'''\nCreated on 2017-03-10\nUpdate on 2017-03-10\nauthor: jiangzhonglian\ncontent: \u6a21\u578b\u6811\n'''\nfrom __future__ import print_function\n\nprint(__doc__)\n\n# Author: Noel Dawe <noel.dawe@gmail.com>\n#\n# License: BSD 3 clause\n\n# importing necessary libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import AdaBoostRegressor\n\n# Create the dataset\nrng = np.random.RandomState(1)\nX = np.linspace(0, 6, 100)[:, np.newaxis]\ny = np.sin(X).ravel() + np.sin(6 * X).ravel() + rng.normal(0, 0.1, X.shape[0])\n\n# Fit regression model\nregr_1 = DecisionTreeRegressor(max_depth=4)\n\nregr_2 = AdaBoostRegressor(DecisionTreeRegressor(max_depth=4),\n                          n_estimators=300, random_state=rng)\n\nregr_1.fit(X, y)\nregr_2.fit(X, y)\n\n# Predict\ny_1 = regr_1.predict(X)\ny_2 = regr_2.predict(X)\n\n# Plot the results\nplt.figure()\nplt.scatter(X, y, c=\"k\", label=\"training samples\")\nplt.plot(X, y_1, c=\"g\", label=\"n_estimators=1\", linewidth=2)\nplt.plot(X, y_2, c=\"r\", label=\"n_estimators=300\", linewidth=2)\nplt.xlabel(\"data\")\nplt.ylabel(\"target\")\nplt.title(\"Boosted Decision Tree Regression\")\nplt.legend()\nplt.show()\n", "src/py2.x/ml/9.RegTrees/treeExplore.py": "#!/usr/bin/python\n# coding:utf8\n\n'''\nCreated on 2017-03-08\nUpdate  on 2017-05-18\nTree-Based Regression Methods Source Code for Machine Learning in Action Ch. 9\nAuthor: Peter/\u7247\u523b\nGitHub: https://github.com/apachecn/AiLearning\n'''\nfrom __future__ import print_function\nimport regTrees\nfrom Tkinter import *\nfrom numpy import *\n\nimport matplotlib\nfrom matplotlib.figure import Figure\nfrom matplotlib.backends.backend_tkagg import FigureCanvasTkAgg\nmatplotlib.use('TkAgg')\n\n\ndef test_widget_text(root):\n    mylabel = Label(root, text=\"helloworld\")\n    # \u76f8\u5f53\u4e8e\u544a\u8bc9 \u5e03\u5c40\u7ba1\u7406\u5668(Geometry Manager),\u5982\u679c\u4e0d\u8bbe\u5b9a\u4f4d\u7f6e\uff0c\u9ed8\u8ba4\u5728 0\u884c0\u5217\u7684\u4f4d\u7f6e\n    mylabel.grid()\n\n\n# \u6700\u5927\u4e3a\u8bef\u5dee\uff0c \u6700\u5927\u5b50\u53f6\u8282\u70b9\u7684\u6570\u91cf\ndef reDraw(tolS, tolN):\n    # clear the figure\n    reDraw.f.clf()\n    reDraw.a = reDraw.f.add_subplot(111)\n\n    # \u68c0\u67e5\u590d\u9009\u6846\u662f\u5426\u9009\u4e2d\n    if chkBtnVar.get():\n        if tolN < 2:\n            tolN = 2\n        myTree = regTrees.createTree(reDraw.rawDat, regTrees.modelLeaf, regTrees.modelErr, (tolS, tolN))\n        yHat = regTrees.createForeCast(myTree, reDraw.testDat, regTrees.modelTreeEval)\n    else:\n        myTree = regTrees.createTree(reDraw.rawDat, ops=(tolS, tolN))\n        yHat = regTrees.createForeCast(myTree, reDraw.testDat)\n\n    # use scatter for data set\n    reDraw.a.scatter(reDraw.rawDat[:, 0].A, reDraw.rawDat[:, 1].A, s=5)\n    # use plot for yHat\n    reDraw.a.plot(reDraw.testDat, yHat, linewidth=2.0, c='red')\n    reDraw.canvas.show()\n\n\ndef getInputs():\n    try:\n        tolN = int(tolNentry.get())\n    except:\n        tolN = 10\n        print(\"enter Integer for tolN\")\n        tolNentry.delete(0, END)\n        tolNentry.insert(0, '10')\n    try:\n        tolS = float(tolSentry.get())\n    except:\n        tolS = 1.0\n        print(\"enter Float for tolS\")\n        tolSentry.delete(0, END)\n        tolSentry.insert(0, '1.0')\n    return tolN, tolS\n\n\n# \u753b\u65b0\u7684tree\ndef drawNewTree():\n    # #get values from Entry boxes\n    tolN, tolS = getInputs()\n    reDraw(tolS, tolN)\n\n\ndef main(root):\n    # \u6807\u9898\n    Label(root, text=\"Plot Place Holder\").grid(row=0, columnspan=3)\n    # \u8f93\u5165\u680f1, \u53f6\u5b50\u7684\u6570\u91cf\n    Label(root, text=\"tolN\").grid(row=1, column=0)\n    global tolNentry\n    tolNentry = Entry(root)\n    tolNentry.grid(row=1, column=1)\n    tolNentry.insert(0, '10')\n    # \u8f93\u5165\u680f2, \u8bef\u5dee\u91cf\n    Label(root, text=\"tolS\").grid(row=2, column=0)\n    global tolSentry\n    tolSentry = Entry(root)\n    tolSentry.grid(row=2, column=1)\n    # \u8bbe\u7f6e\u8f93\u51fa\u503c\n    tolSentry.insert(0,'1.0')\n\n    # \u8bbe\u7f6e\u63d0\u4ea4\u7684\u6309\u94ae\n    Button(root, text=\"\u786e\u5b9a\", command=drawNewTree).grid(row=1, column=2, rowspan=3)\n\n    # \u8bbe\u7f6e\u590d\u9009\u6309\u94ae\n    global chkBtnVar\n    chkBtnVar = IntVar()\n    chkBtn = Checkbutton(root, text=\"Model Tree\", variable = chkBtnVar)\n    chkBtn.grid(row=3, column=0, columnspan=2)\n\n    # \u9000\u51fa\u6309\u94ae\n    Button(root, text=\"\u9000\u51fa\", fg=\"black\", command=quit).grid(row=1, column=2)\n\n    # \u521b\u5efa\u4e00\u4e2a\u753b\u677f canvas\n    reDraw.f = Figure(figsize=(5, 4), dpi=100)\n    reDraw.canvas = FigureCanvasTkAgg(reDraw.f, master=root)\n    reDraw.canvas.show()\n    reDraw.canvas.get_tk_widget().grid(row=0, columnspan=3)\n\n    reDraw.rawDat = mat(regTrees.loadDataSet('data/9.RegTrees/sine.txt'))\n    reDraw.testDat = arange(min(reDraw.rawDat[:, 0]), max(reDraw.rawDat[:, 0]), 0.01)\n    reDraw(1.0, 10)\n\n\nif __name__ == \"__main__\":\n\n    # \u521b\u5efa\u4e00\u4e2a\u4e8b\u4ef6\n    root = Tk()\n    # test_widget_text(root)\n    main(root)\n\n    # \u542f\u52a8\u4e8b\u4ef6\u5faa\u73af\n    root.mainloop()\n", "src/py2.x/ml/9.RegTrees/regTrees.py": "#!/usr/bin/python\n# coding:utf8\n'''\nCreated on Feb 4, 2011\nUpdate on 2017-12-20\nTree-Based Regression Methods Source Code for Machine Learning in Action Ch. 9\nAuthor: Peter Harrington/\u7247\u523b/\u5c0f\u7476/zh0ng\nGitHub: https://github.com/apachecn/AiLearning\n'''\nfrom __future__ import print_function\nprint(__doc__)\nfrom numpy import *\n\n\n# \u9ed8\u8ba4\u89e3\u6790\u7684\u6570\u636e\u662f\u7528tab\u5206\u9694\uff0c\u5e76\u4e14\u662f\u6570\u503c\u7c7b\u578b\n# general function to parse tab -delimited floats\ndef loadDataSet(fileName):\n    \"\"\"loadDataSet(\u89e3\u6790\u6bcf\u4e00\u884c\uff0c\u5e76\u8f6c\u5316\u4e3afloat\u7c7b\u578b)\n        Desc: \u8be5\u51fd\u6570\u8bfb\u53d6\u4e00\u4e2a\u4ee5 tab \u952e\u4e3a\u5206\u9694\u7b26\u7684\u6587\u4ef6\uff0c\u7136\u540e\u5c06\u6bcf\u884c\u7684\u5185\u5bb9\u4fdd\u5b58\u6210\u4e00\u7ec4\u6d6e\u70b9\u6570\n    Args:\n        fileName \u6587\u4ef6\u540d\n    Returns:\n        dataMat \u6bcf\u4e00\u884c\u7684\u6570\u636e\u96c6array\u7c7b\u578b\n    Raises:\n    \"\"\"\n    # \u5047\u5b9a\u6700\u540e\u4e00\u5217\u662f\u7ed3\u679c\u503c\n    # assume last column is target value\n    dataMat = []\n    fr = open(fileName)\n    for line in fr.readlines():\n        curLine = line.strip().split('\\t')\n        # \u5c06\u6240\u6709\u7684\u5143\u7d20\u8f6c\u5316\u4e3afloat\u7c7b\u578b\n        # map all elements to float()\n        # map() \u51fd\u6570\u5177\u4f53\u7684\u542b\u4e49\uff0c\u53ef\u89c1 https://my.oschina.net/zyzzy/blog/115096\n        fltLine = map(float, curLine)\n        dataMat.append(fltLine)\n    return dataMat\n\n\ndef binSplitDataSet(dataSet, feature, value):\n    \"\"\"binSplitDataSet(\u5c06\u6570\u636e\u96c6\uff0c\u6309\u7167feature\u5217\u7684value\u8fdb\u884c \u4e8c\u5143\u5207\u5206)\n        Description: \u5728\u7ed9\u5b9a\u7279\u5f81\u548c\u7279\u5f81\u503c\u7684\u60c5\u51b5\u4e0b\uff0c\u8be5\u51fd\u6570\u901a\u8fc7\u6570\u7ec4\u8fc7\u6ee4\u65b9\u5f0f\u5c06\u4e0a\u8ff0\u6570\u636e\u96c6\u5408\u5207\u5206\u5f97\u5230\u4e24\u4e2a\u5b50\u96c6\u5e76\u8fd4\u56de\u3002\n    Args:\n        dataMat \u6570\u636e\u96c6\n        feature \u5f85\u5207\u5206\u7684\u7279\u5f81\u5217\n        value \u7279\u5f81\u5217\u8981\u6bd4\u8f83\u7684\u503c\n    Returns:\n        mat0 \u5c0f\u4e8e\u7b49\u4e8e value \u7684\u6570\u636e\u96c6\u5728\u5de6\u8fb9\n        mat1 \u5927\u4e8e value \u7684\u6570\u636e\u96c6\u5728\u53f3\u8fb9\n    Raises:\n    \"\"\"\n    # # \u6d4b\u8bd5\u6848\u4f8b\n    # print 'dataSet[:, feature]=', dataSet[:, feature]\n    # print 'nonzero(dataSet[:, feature] > value)[0]=', nonzero(dataSet[:, feature] > value)[0]\n    # print 'nonzero(dataSet[:, feature] <= value)[0]=', nonzero(dataSet[:, feature] <= value)[0]\n\n    # dataSet[:, feature] \u53d6\u53bb\u6bcf\u4e00\u884c\u4e2d\uff0c\u7b2c1\u5217\u7684\u503c(\u4ece0\u5f00\u59cb\u7b97)\n    # nonzero(dataSet[:, feature] > value)  \u8fd4\u56de\u7ed3\u679c\u4e3atrue\u884c\u7684index\u4e0b\u6807\n    mat0 = dataSet[nonzero(dataSet[:, feature] <= value)[0], :]\n    mat1 = dataSet[nonzero(dataSet[:, feature] > value)[0], :]\n    return mat0, mat1\n\n\n# \u8fd4\u56de\u6bcf\u4e00\u4e2a\u53f6\u5b50\u7ed3\u70b9\u7684\u5747\u503c\n# returns the value used for each leaf\n# \u6211\u7684\u7406\u89e3\u662f: regLeaf \u662f\u4ea7\u751f\u53f6\u8282\u70b9\u7684\u51fd\u6570\uff0c\u5c31\u662f\u6c42\u5747\u503c\uff0c\u5373\u7528\u805a\u7c7b\u4e2d\u5fc3\u70b9\u6765\u4ee3\u8868\u8fd9\u7c7b\u6570\u636e\ndef regLeaf(dataSet):\n    return mean(dataSet[:, -1])\n\n\n# \u8ba1\u7b97\u603b\u65b9\u5dee=\u65b9\u5dee*\u6837\u672c\u6570\n# \u6211\u7684\u7406\u89e3\u662f: \u6c42\u8fd9\u7ec4\u6570\u636e\u7684\u65b9\u5dee\uff0c\u5373\u901a\u8fc7\u51b3\u7b56\u6811\u5212\u5206\uff0c\u53ef\u4ee5\u8ba9\u9760\u8fd1\u7684\u6570\u636e\u5206\u5230\u540c\u4e00\u7c7b\u4e2d\u53bb\ndef regErr(dataSet):\n    # shape(dataSet)[0] \u8868\u793a\u884c\u6570\n    return var(dataSet[:, -1]) * shape(dataSet)[0]\n\n\n# 1.\u7528\u6700\u4f73\u65b9\u5f0f\u5207\u5206\u6570\u636e\u96c6\n# 2.\u751f\u6210\u76f8\u5e94\u7684\u53f6\u8282\u70b9\ndef chooseBestSplit(dataSet, leafType=regLeaf, errType=regErr, ops=(1, 4)):\n    \"\"\"chooseBestSplit(\u7528\u6700\u4f73\u65b9\u5f0f\u5207\u5206\u6570\u636e\u96c6 \u548c \u751f\u6210\u76f8\u5e94\u7684\u53f6\u8282\u70b9)\n\n    Args:\n        dataSet   \u52a0\u8f7d\u7684\u539f\u59cb\u6570\u636e\u96c6\n        leafType  \u5efa\u7acb\u53f6\u5b50\u70b9\u7684\u51fd\u6570\n        errType   \u8bef\u5dee\u8ba1\u7b97\u51fd\u6570(\u6c42\u603b\u65b9\u5dee)\n        ops       [\u5bb9\u8bb8\u8bef\u5dee\u4e0b\u964d\u503c\uff0c\u5207\u5206\u7684\u6700\u5c11\u6837\u672c\u6570]\u3002\n    Returns:\n        bestIndex feature\u7684index\u5750\u6807\n        bestValue \u5207\u5206\u7684\u6700\u4f18\u503c\n    Raises:\n    \"\"\"\n\n    # ops=(1,4)\uff0c\u975e\u5e38\u91cd\u8981\uff0c\u56e0\u4e3a\u5b83\u51b3\u5b9a\u4e86\u51b3\u7b56\u6811\u5212\u5206\u505c\u6b62\u7684threshold\u503c\uff0c\u88ab\u79f0\u4e3a\u9884\u526a\u679d\uff08prepruning\uff09\uff0c\u5176\u5b9e\u4e5f\u5c31\u662f\u7528\u4e8e\u63a7\u5236\u51fd\u6570\u7684\u505c\u6b62\u65f6\u673a\u3002\n    # \u4e4b\u6240\u4ee5\u8fd9\u6837\u8bf4\uff0c\u662f\u56e0\u4e3a\u5b83\u9632\u6b62\u51b3\u7b56\u6811\u7684\u8fc7\u62df\u5408\uff0c\u6240\u4ee5\u5f53\u8bef\u5dee\u7684\u4e0b\u964d\u503c\u5c0f\u4e8etolS\uff0c\u6216\u5212\u5206\u540e\u7684\u96c6\u5408size\u5c0f\u4e8etolN\u65f6\uff0c\u9009\u62e9\u505c\u6b62\u7ee7\u7eed\u5212\u5206\u3002\n    # \u6700\u5c0f\u8bef\u5dee\u4e0b\u964d\u503c\uff0c\u5212\u5206\u540e\u7684\u8bef\u5dee\u51cf\u5c0f\u5c0f\u4e8e\u8fd9\u4e2a\u5dee\u503c\uff0c\u5c31\u4e0d\u7528\u7ee7\u7eed\u5212\u5206\n    tolS = ops[0]\n    # \u5212\u5206\u6700\u5c0f size \u5c0f\u4e8e\uff0c\u5c31\u4e0d\u7ee7\u7eed\u5212\u5206\u4e86\n    tolN = ops[1]\n    # \u5982\u679c\u7ed3\u679c\u96c6(\u6700\u540e\u4e00\u5217\u4e3a1\u4e2a\u53d8\u91cf)\uff0c\u5c31\u8fd4\u56de\u9000\u51fa\n    # .T \u5bf9\u6570\u636e\u96c6\u8fdb\u884c\u8f6c\u7f6e\n    # .tolist()[0] \u8f6c\u5316\u4e3a\u6570\u7ec4\u5e76\u53d6\u7b2c0\u5217\n    if len(set(dataSet[:, -1].T.tolist()[0])) == 1: # \u5982\u679c\u96c6\u5408size\u4e3a1\uff0c\u4e5f\u5c31\u662f\u8bf4\u5168\u90e8\u7684\u6570\u636e\u90fd\u662f\u540c\u4e00\u4e2a\u7c7b\u522b\uff0c\u4e0d\u7528\u7ee7\u7eed\u5212\u5206\u3002\n        #  exit cond 1\n        return None, leafType(dataSet)\n    # \u8ba1\u7b97\u884c\u5217\u503c\n    m, n = shape(dataSet)\n    # \u65e0\u5206\u7c7b\u8bef\u5dee\u7684\u603b\u65b9\u5dee\u548c\n    # the choice of the best feature is driven by Reduction in RSS error from mean\n    S = errType(dataSet)\n    # inf \u6b63\u65e0\u7a77\u5927\n    bestS, bestIndex, bestValue = inf, 0, 0\n    # \u5faa\u73af\u5904\u7406\u6bcf\u4e00\u5217\u5bf9\u5e94\u7684feature\u503c\n    for featIndex in range(n-1): # \u5bf9\u4e8e\u6bcf\u4e2a\u7279\u5f81\n        # [0]\u8868\u793a\u8fd9\u4e00\u5217\u7684[\u6240\u6709\u884c]\uff0c\u4e0d\u8981[0]\u5c31\u662f\u4e00\u4e2aarray[[\u6240\u6709\u884c]]\uff0c\u4e0b\u9762\u7684\u4e00\u884c\u8868\u793a\u7684\u662f\u5c06\u67d0\u4e00\u5217\u5168\u90e8\u7684\u6570\u636e\u8f6c\u6362\u4e3a\u884c\uff0c\u7136\u540e\u8bbe\u7f6e\u4e3alist\u5f62\u5f0f\n        for splitVal in set(dataSet[:, featIndex].T.tolist()[0]):\n            # \u5bf9\u8be5\u5217\u8fdb\u884c\u5206\u7ec4\uff0c\u7136\u540e\u7ec4\u5185\u7684\u6210\u5458\u7684val\u503c\u8fdb\u884c \u4e8c\u5143\u5207\u5206\n            mat0, mat1 = binSplitDataSet(dataSet, featIndex, splitVal)\n            # \u5224\u65ad\u4e8c\u5143\u5207\u5206\u7684\u65b9\u5f0f\u7684\u5143\u7d20\u6570\u91cf\u662f\u5426\u7b26\u5408\u9884\u671f\n            if (shape(mat0)[0] < tolN) or (shape(mat1)[0] < tolN):\n                continue\n            newS = errType(mat0) + errType(mat1)\n            # \u5982\u679c\u4e8c\u5143\u5207\u5206\uff0c\u7b97\u51fa\u6765\u7684\u8bef\u5dee\u5728\u53ef\u63a5\u53d7\u8303\u56f4\u5185\uff0c\u90a3\u4e48\u5c31\u8bb0\u5f55\u5207\u5206\u70b9\uff0c\u5e76\u8bb0\u5f55\u6700\u5c0f\u8bef\u5dee\n            # \u5982\u679c\u5212\u5206\u540e\u8bef\u5dee\u5c0f\u4e8e bestS\uff0c\u5219\u8bf4\u660e\u627e\u5230\u4e86\u65b0\u7684bestS\n            if newS < bestS:\n                bestIndex = featIndex\n                bestValue = splitVal\n                bestS = newS\n    # \u5224\u65ad\u4e8c\u5143\u5207\u5206\u7684\u65b9\u5f0f\u7684\u5143\u7d20\u8bef\u5dee\u662f\u5426\u7b26\u5408\u9884\u671f\n    # if the decrease (S-bestS) is less than a threshold don't do the split\n    if (S - bestS) < tolS:\n        return None, leafType(dataSet)\n    mat0, mat1 = binSplitDataSet(dataSet, bestIndex, bestValue)\n    # \u5bf9\u6574\u4f53\u7684\u6210\u5458\u8fdb\u884c\u5224\u65ad\uff0c\u662f\u5426\u7b26\u5408\u9884\u671f\n    # \u5982\u679c\u96c6\u5408\u7684 size \u5c0f\u4e8e tolN \n    if (shape(mat0)[0] < tolN) or (shape(mat1)[0] < tolN): # \u5f53\u6700\u4f73\u5212\u5206\u540e\uff0c\u96c6\u5408\u8fc7\u5c0f\uff0c\u4e5f\u4e0d\u5212\u5206\uff0c\u4ea7\u751f\u53f6\u8282\u70b9\n        return None, leafType(dataSet)\n    return bestIndex, bestValue\n\n\n# assume dataSet is NumPy Mat so we can array filtering\n# \u5047\u8bbe dataSet \u662f NumPy Mat \u7c7b\u578b\u7684\uff0c\u90a3\u4e48\u6211\u4eec\u53ef\u4ee5\u8fdb\u884c array \u8fc7\u6ee4\ndef createTree(dataSet, leafType=regLeaf, errType=regErr, ops=(1, 4)):\n    \"\"\"createTree(\u83b7\u53d6\u56de\u5f52\u6811)\n        Description: \u9012\u5f52\u51fd\u6570: \u5982\u679c\u6784\u5efa\u7684\u662f\u56de\u5f52\u6811\uff0c\u8be5\u6a21\u578b\u662f\u4e00\u4e2a\u5e38\u6570\uff0c\u5982\u679c\u662f\u6a21\u578b\u6811\uff0c\u5176\u6a21\u578b\u5e08\u4e00\u4e2a\u7ebf\u6027\u65b9\u7a0b\u3002\n    Args:\n        dataSet      \u52a0\u8f7d\u7684\u539f\u59cb\u6570\u636e\u96c6\n        leafType     \u5efa\u7acb\u53f6\u5b50\u70b9\u7684\u51fd\u6570\n        errType      \u8bef\u5dee\u8ba1\u7b97\u51fd\u6570\n        ops=(1, 4)   [\u5bb9\u8bb8\u8bef\u5dee\u4e0b\u964d\u503c\uff0c\u5207\u5206\u7684\u6700\u5c11\u6837\u672c\u6570]\n    Returns:\n        retTree    \u51b3\u7b56\u6811\u6700\u540e\u7684\u7ed3\u679c\n    \"\"\"\n    # \u9009\u62e9\u6700\u597d\u7684\u5207\u5206\u65b9\u5f0f:  feature\u7d22\u5f15\u503c\uff0c\u6700\u4f18\u5207\u5206\u503c\n    # choose the best split\n    feat, val = chooseBestSplit(dataSet, leafType, errType, ops)\n    # if the splitting hit a stop condition return val\n    # \u5982\u679c splitting \u8fbe\u5230\u4e00\u4e2a\u505c\u6b62\u6761\u4ef6\uff0c\u90a3\u4e48\u8fd4\u56de val\n    '''\n    *** \u6700\u540e\u7684\u8fd4\u56de\u7ed3\u679c\u662f\u6700\u540e\u5269\u4e0b\u7684 val\uff0c\u4e5f\u5c31\u662flen\u5c0f\u4e8etopN\u7684\u96c6\u5408\n    '''\n    if feat is None:\n        return val\n    retTree = {}\n    retTree['spInd'] = feat\n    retTree['spVal'] = val\n    # \u5927\u4e8e\u5728\u53f3\u8fb9\uff0c\u5c0f\u4e8e\u5728\u5de6\u8fb9\uff0c\u5206\u4e3a2\u4e2a\u6570\u636e\u96c6\n    lSet, rSet = binSplitDataSet(dataSet, feat, val)\n    # \u9012\u5f52\u7684\u8fdb\u884c\u8c03\u7528\uff0c\u5728\u5de6\u53f3\u5b50\u6811\u4e2d\u7ee7\u7eed\u9012\u5f52\u751f\u6210\u6811\n    retTree['left'] = createTree(lSet, leafType, errType, ops)\n    retTree['right'] = createTree(rSet, leafType, errType, ops)\n    return retTree\n\n\n# \u5224\u65ad\u8282\u70b9\u662f\u5426\u662f\u4e00\u4e2a\u5b57\u5178\ndef isTree(obj):\n    \"\"\"\n    Desc:\n        \u6d4b\u8bd5\u8f93\u5165\u53d8\u91cf\u662f\u5426\u662f\u4e00\u68f5\u6811,\u5373\u662f\u5426\u662f\u4e00\u4e2a\u5b57\u5178\n    Args:\n        obj -- \u8f93\u5165\u53d8\u91cf\n    Returns:\n        \u8fd4\u56de\u5e03\u5c14\u7c7b\u578b\u7684\u7ed3\u679c\u3002\u5982\u679c obj \u662f\u4e00\u4e2a\u5b57\u5178\uff0c\u8fd4\u56detrue\uff0c\u5426\u5219\u8fd4\u56de false\n    \"\"\"\n    return (type(obj).__name__ == 'dict')\n\n\n# \u8ba1\u7b97\u5de6\u53f3\u679d\u4e2b\u7684\u5747\u503c\ndef getMean(tree):\n    \"\"\"\n    Desc:\n        \u4ece\u4e0a\u5f80\u4e0b\u904d\u5386\u6811\u76f4\u5230\u53f6\u8282\u70b9\u4e3a\u6b62\uff0c\u5982\u679c\u627e\u5230\u4e24\u4e2a\u53f6\u8282\u70b9\u5219\u8ba1\u7b97\u5b83\u4eec\u7684\u5e73\u5747\u503c\u3002\n        \u5bf9 tree \u8fdb\u884c\u584c\u9677\u5904\u7406\uff0c\u5373\u8fd4\u56de\u6811\u5e73\u5747\u503c\u3002\n    Args:\n        tree -- \u8f93\u5165\u7684\u6811\n    Returns:\n        \u8fd4\u56de tree \u8282\u70b9\u7684\u5e73\u5747\u503c\n    \"\"\"\n    if isTree(tree['right']):\n        tree['right'] = getMean(tree['right'])\n    if isTree(tree['left']):\n        tree['left'] = getMean(tree['left'])\n    return (tree['left']+tree['right'])/2.0\n\n\n# \u68c0\u67e5\u662f\u5426\u9002\u5408\u5408\u5e76\u5206\u679d\ndef prune(tree, testData):\n    \"\"\"\n    Desc:\n        \u4ece\u4e0a\u800c\u4e0b\u627e\u5230\u53f6\u8282\u70b9\uff0c\u7528\u6d4b\u8bd5\u6570\u636e\u96c6\u6765\u5224\u65ad\u5c06\u8fd9\u4e9b\u53f6\u8282\u70b9\u5408\u5e76\u662f\u5426\u80fd\u964d\u4f4e\u6d4b\u8bd5\u8bef\u5dee\n    Args:\n        tree -- \u5f85\u526a\u679d\u7684\u6811\n        testData -- \u526a\u679d\u6240\u9700\u8981\u7684\u6d4b\u8bd5\u6570\u636e testData \n    Returns:\n        tree -- \u526a\u679d\u5b8c\u6210\u7684\u6811\n    \"\"\"\n    # \u5224\u65ad\u662f\u5426\u6d4b\u8bd5\u6570\u636e\u96c6\u6ca1\u6709\u6570\u636e\uff0c\u5982\u679c\u6ca1\u6709\uff0c\u5c31\u76f4\u63a5\u8fd4\u56detree\u672c\u8eab\u7684\u5747\u503c\n    if shape(testData)[0] == 0:\n        return getMean(tree)\n\n    # \u5224\u65ad\u5206\u679d\u662f\u5426\u662fdict\u5b57\u5178\uff0c\u5982\u679c\u662f\u5c31\u5c06\u6d4b\u8bd5\u6570\u636e\u96c6\u8fdb\u884c\u5207\u5206\n    if (isTree(tree['right']) or isTree(tree['left'])):\n        lSet, rSet = binSplitDataSet(testData, tree['spInd'], tree['spVal'])\n    # \u5982\u679c\u662f\u5de6\u8fb9\u5206\u679d\u662f\u5b57\u5178\uff0c\u5c31\u4f20\u5165\u5de6\u8fb9\u7684\u6570\u636e\u96c6\u548c\u5de6\u8fb9\u7684\u5206\u679d\uff0c\u8fdb\u884c\u9012\u5f52\n    if isTree(tree['left']):\n        tree['left'] = prune(tree['left'], lSet)\n    # \u5982\u679c\u662f\u53f3\u8fb9\u5206\u679d\u662f\u5b57\u5178\uff0c\u5c31\u4f20\u5165\u5de6\u8fb9\u7684\u6570\u636e\u96c6\u548c\u5de6\u8fb9\u7684\u5206\u679d\uff0c\u8fdb\u884c\u9012\u5f52\n    if isTree(tree['right']):\n        tree['right'] = prune(tree['right'], rSet)\n\n    # \u4e0a\u9762\u7684\u4e00\u7cfb\u5217\u64cd\u4f5c\u672c\u8d28\u4e0a\u5c31\u662f\u5c06\u6d4b\u8bd5\u6570\u636e\u96c6\u6309\u7167\u8bad\u7ec3\u5b8c\u6210\u7684\u6811\u62c6\u5206\u597d\uff0c\u5bf9\u5e94\u7684\u503c\u653e\u5230\u5bf9\u5e94\u7684\u8282\u70b9\n\n    # \u5982\u679c\u5de6\u53f3\u4e24\u8fb9\u540c\u65f6\u90fd\u4e0d\u662fdict\u5b57\u5178\uff0c\u4e5f\u5c31\u662f\u5de6\u53f3\u4e24\u8fb9\u90fd\u662f\u53f6\u8282\u70b9\uff0c\u800c\u4e0d\u662f\u5b50\u6811\u4e86\uff0c\u90a3\u4e48\u5206\u5272\u6d4b\u8bd5\u6570\u636e\u96c6\u3002\n    # 1. \u5982\u679c\u6b63\u786e \n    #   * \u90a3\u4e48\u8ba1\u7b97\u4e00\u4e0b\u603b\u65b9\u5dee \u548c \u8be5\u7ed3\u679c\u96c6\u7684\u672c\u8eab\u4e0d\u5206\u679d\u7684\u603b\u65b9\u5dee\u6bd4\u8f83\n    #   * \u5982\u679c \u5408\u5e76\u7684\u603b\u65b9\u5dee < \u4e0d\u5408\u5e76\u7684\u603b\u65b9\u5dee\uff0c\u90a3\u4e48\u5c31\u8fdb\u884c\u5408\u5e76\n    # \u6ce8\u610f\u8fd4\u56de\u7684\u7ed3\u679c:  \u5982\u679c\u53ef\u4ee5\u5408\u5e76\uff0c\u539f\u6765\u7684dict\u5c31\u53d8\u4e3a\u4e86 \u6570\u503c\n    if not isTree(tree['left']) and not isTree(tree['right']):\n        lSet, rSet = binSplitDataSet(testData, tree['spInd'], tree['spVal'])\n        # power(x, y)\u8868\u793ax\u7684y\u6b21\u65b9\uff1b\u8fd9\u65f6tree['left']\u548ctree['right']\u90fd\u662f\u5177\u4f53\u6570\u503c\n        errorNoMerge = sum(power(lSet[:, -1] - tree['left'], 2)) + sum(power(rSet[:, -1] - tree['right'], 2))\n        treeMean = (tree['left'] + tree['right'])/2.0\n        errorMerge = sum(power(testData[:, -1] - treeMean, 2))\n        # \u5982\u679c \u5408\u5e76\u7684\u603b\u65b9\u5dee < \u4e0d\u5408\u5e76\u7684\u603b\u65b9\u5dee\uff0c\u90a3\u4e48\u5c31\u8fdb\u884c\u5408\u5e76\n        if errorMerge < errorNoMerge:\n            print(\"merging\")\n            return treeMean\n        # \u4e24\u4e2areturn\u53ef\u4ee5\u7b80\u5316\u6210\u4e00\u4e2a\n        else:\n            return tree\n    else:\n        return tree\n\n\n# \u5f97\u5230\u6a21\u578b\u7684ws\u7cfb\u6570: f(x) = x0 + x1*featrue1+ x2*featrue2 ...\n# create linear model and return coeficients\ndef modelLeaf(dataSet):\n    \"\"\"\n    Desc:\n        \u5f53\u6570\u636e\u4e0d\u518d\u9700\u8981\u5207\u5206\u7684\u65f6\u5019\uff0c\u751f\u6210\u53f6\u8282\u70b9\u7684\u6a21\u578b\u3002\n    Args:\n        dataSet -- \u8f93\u5165\u6570\u636e\u96c6\n    Returns:\n        \u8c03\u7528 linearSolve \u51fd\u6570\uff0c\u8fd4\u56de\u5f97\u5230\u7684 \u56de\u5f52\u7cfb\u6570ws\n    \"\"\"\n    ws, X, Y = linearSolve(dataSet)\n    return ws\n\n\n# \u8ba1\u7b97\u7ebf\u6027\u6a21\u578b\u7684\u8bef\u5dee\u503c\ndef modelErr(dataSet):\n    \"\"\"\n    Desc:\n        \u5728\u7ed9\u5b9a\u6570\u636e\u96c6\u4e0a\u8ba1\u7b97\u8bef\u5dee\u3002\n    Args:\n        dataSet -- \u8f93\u5165\u6570\u636e\u96c6\n    Returns:\n        \u8c03\u7528 linearSolve \u51fd\u6570\uff0c\u8fd4\u56de yHat \u548c Y \u4e4b\u95f4\u7684\u5e73\u65b9\u8bef\u5dee\u3002\n    \"\"\"\n    ws, X, Y = linearSolve(dataSet)\n    yHat = X * ws\n    # print corrcoef(yHat, Y, rowvar=0)\n    return sum(power(Y - yHat, 2))\n\n\n # helper function used in two places\ndef linearSolve(dataSet):\n    \"\"\"\n    Desc:\n        \u5c06\u6570\u636e\u96c6\u683c\u5f0f\u5316\u6210\u76ee\u6807\u53d8\u91cfY\u548c\u81ea\u53d8\u91cfX\uff0c\u6267\u884c\u7b80\u5355\u7684\u7ebf\u6027\u56de\u5f52\uff0c\u5f97\u5230ws\n    Args:\n        dataSet -- \u8f93\u5165\u6570\u636e\n    Returns:\n        ws -- \u6267\u884c\u7ebf\u6027\u56de\u5f52\u7684\u56de\u5f52\u7cfb\u6570 \n        X -- \u683c\u5f0f\u5316\u81ea\u53d8\u91cfX\n        Y -- \u683c\u5f0f\u5316\u76ee\u6807\u53d8\u91cfY\n    \"\"\"\n    m, n = shape(dataSet)\n    # \u4ea7\u751f\u4e00\u4e2a\u5173\u4e8e1\u7684\u77e9\u9635\n    X = mat(ones((m, n)))\n    Y = mat(ones((m, 1)))\n    # X\u76840\u5217\u4e3a1\uff0c\u5e38\u6570\u9879\uff0c\u7528\u4e8e\u8ba1\u7b97\u5e73\u8861\u8bef\u5dee\n    X[:, 1: n] = dataSet[:, 0: n-1]\n    Y = dataSet[:, -1]\n\n    # \u8f6c\u7f6e\u77e9\u9635*\u77e9\u9635\n    xTx = X.T * X\n    # \u5982\u679c\u77e9\u9635\u7684\u9006\u4e0d\u5b58\u5728\uff0c\u4f1a\u9020\u6210\u7a0b\u5e8f\u5f02\u5e38\n    if linalg.det(xTx) == 0.0:\n        raise NameError('This matrix is singular, cannot do inverse,\\ntry increasing the second value of ops')\n    # \u6700\u5c0f\u4e8c\u4e58\u6cd5\u6c42\u6700\u4f18\u89e3:  w0*1+w1*x1=y\n    ws = xTx.I * (X.T * Y)\n    return ws, X, Y\n\n\n# \u56de\u5f52\u6811\u6d4b\u8bd5\u6848\u4f8b\n# \u4e3a\u4e86\u548c modelTreeEval() \u4fdd\u6301\u4e00\u81f4\uff0c\u4fdd\u7559\u4e24\u4e2a\u8f93\u5165\u53c2\u6570\ndef regTreeEval(model, inDat):\n    \"\"\"\n    Desc:\n        \u5bf9 \u56de\u5f52\u6811 \u8fdb\u884c\u9884\u6d4b\n    Args:\n        model -- \u6307\u5b9a\u6a21\u578b\uff0c\u53ef\u9009\u503c\u4e3a \u56de\u5f52\u6811\u6a21\u578b \u6216\u8005 \u6a21\u578b\u6811\u6a21\u578b\uff0c\u8fd9\u91cc\u4e3a\u56de\u5f52\u6811\n        inDat -- \u8f93\u5165\u7684\u6d4b\u8bd5\u6570\u636e\n    Returns:\n        float(model) -- \u5c06\u8f93\u5165\u7684\u6a21\u578b\u6570\u636e\u8f6c\u6362\u4e3a \u6d6e\u70b9\u6570 \u8fd4\u56de\n    \"\"\"\n    return float(model)\n\n\n# \u6a21\u578b\u6811\u6d4b\u8bd5\u6848\u4f8b\n# \u5bf9\u8f93\u5165\u6570\u636e\u8fdb\u884c\u683c\u5f0f\u5316\u5904\u7406\uff0c\u5728\u539f\u6570\u636e\u77e9\u9635\u4e0a\u589e\u52a0\u7b2c0\u5217\uff0c\u5143\u7d20\u7684\u503c\u90fd\u662f1\uff0c\n# \u4e5f\u5c31\u662f\u589e\u52a0\u504f\u79fb\u503c\uff0c\u548c\u6211\u4eec\u4e4b\u524d\u7684\u7b80\u5355\u7ebf\u6027\u56de\u5f52\u662f\u4e00\u4e2a\u5957\u8def\uff0c\u589e\u52a0\u4e00\u4e2a\u504f\u79fb\u91cf\ndef modelTreeEval(model, inDat):\n    \"\"\"\n    Desc:\n        \u5bf9 \u6a21\u578b\u6811 \u8fdb\u884c\u9884\u6d4b\n    Args:\n        model -- \u8f93\u5165\u6a21\u578b\uff0c\u53ef\u9009\u503c\u4e3a \u56de\u5f52\u6811\u6a21\u578b \u6216\u8005 \u6a21\u578b\u6811\u6a21\u578b\uff0c\u8fd9\u91cc\u4e3a\u6a21\u578b\u6811\u6a21\u578b\uff0c\u5b9e\u5219\u4e3a \u56de\u5f52\u7cfb\u6570\n        inDat -- \u8f93\u5165\u7684\u6d4b\u8bd5\u6570\u636e\n    Returns:\n        float(X * model) -- \u5c06\u6d4b\u8bd5\u6570\u636e\u4e58\u4ee5 \u56de\u5f52\u7cfb\u6570 \u5f97\u5230\u4e00\u4e2a\u9884\u6d4b\u503c \uff0c\u8f6c\u5316\u4e3a \u6d6e\u70b9\u6570 \u8fd4\u56de\n    \"\"\"\n    n = shape(inDat)[1]\n    X = mat(ones((1, n+1)))\n    X[:, 1: n+1] = inDat\n    # print X, model\n    return float(X * model)\n\n\n# \u8ba1\u7b97\u9884\u6d4b\u7684\u7ed3\u679c\n# \u5728\u7ed9\u5b9a\u6811\u7ed3\u6784\u7684\u60c5\u51b5\u4e0b\uff0c\u5bf9\u4e8e\u5355\u4e2a\u6570\u636e\u70b9\uff0c\u8be5\u51fd\u6570\u4f1a\u7ed9\u51fa\u4e00\u4e2a\u9884\u6d4b\u503c\u3002\n# modelEval\u662f\u5bf9\u53f6\u8282\u70b9\u8fdb\u884c\u9884\u6d4b\u7684\u51fd\u6570\u5f15\u7528\uff0c\u6307\u5b9a\u6811\u7684\u7c7b\u578b\uff0c\u4ee5\u4fbf\u5728\u53f6\u8282\u70b9\u4e0a\u8c03\u7528\u5408\u9002\u7684\u6a21\u578b\u3002\n# \u6b64\u51fd\u6570\u81ea\u9876\u5411\u4e0b\u904d\u5386\u6574\u68f5\u6811\uff0c\u76f4\u5230\u547d\u4e2d\u53f6\u8282\u70b9\u4e3a\u6b62\uff0c\u4e00\u65e6\u5230\u8fbe\u53f6\u8282\u70b9\uff0c\u5b83\u5c31\u4f1a\u5728\u8f93\u5165\u6570\u636e\u4e0a\n# \u8c03\u7528modelEval()\u51fd\u6570\uff0c\u8be5\u51fd\u6570\u7684\u9ed8\u8ba4\u503c\u4e3aregTreeEval()\ndef treeForeCast(tree, inData, modelEval=regTreeEval):\n    \"\"\"\n    Desc:\n        \u5bf9\u7279\u5b9a\u6a21\u578b\u7684\u6811\u8fdb\u884c\u9884\u6d4b\uff0c\u53ef\u4ee5\u662f \u56de\u5f52\u6811 \u4e5f\u53ef\u4ee5\u662f \u6a21\u578b\u6811\n    Args:\n        tree -- \u5df2\u7ecf\u8bad\u7ec3\u597d\u7684\u6811\u7684\u6a21\u578b\n        inData -- \u8f93\u5165\u7684\u6d4b\u8bd5\u6570\u636e\uff0c\u53ea\u6709\u4e00\u884c\n        modelEval -- \u9884\u6d4b\u7684\u6811\u7684\u6a21\u578b\u7c7b\u578b\uff0c\u53ef\u9009\u503c\u4e3a regTreeEval\uff08\u56de\u5f52\u6811\uff09 \u6216 modelTreeEval\uff08\u6a21\u578b\u6811\uff09\uff0c\u9ed8\u8ba4\u4e3a\u56de\u5f52\u6811\n    Returns:\n        \u8fd4\u56de\u9884\u6d4b\u503c\n    \"\"\"\n    if not isTree(tree):\n        return modelEval(tree, inData)\n    # \u4e66\u4e2d\u5199\u7684\u662finData[tree['spInd']]\uff0c\u53ea\u9002\u5408inData\u53ea\u6709\u4e00\u5217\u7684\u60c5\u51b5\uff0c\u5426\u5219\u4f1a\u4ea7\u751f\u5f02\u5e38\n    if inData[0, tree['spInd']] <= tree['spVal']:\n        # \u53ef\u4ee5\u628aif-else\u53bb\u6389\uff0c\u53ea\u7559if\u91cc\u9762\u7684\u5206\u652f\n        if isTree(tree['left']):\n            return treeForeCast(tree['left'], inData, modelEval)\n        else:\n            return modelEval(tree['left'], inData)\n    else:\n        # \u540c\u4e0a\uff0c\u53ef\u4ee5\u628aif-else\u53bb\u6389\uff0c\u53ea\u7559if\u91cc\u9762\u7684\u5206\u652f\n        if isTree(tree['right']):\n            return treeForeCast(tree['right'], inData, modelEval)\n        else:\n            return modelEval(tree['right'], inData)\n\n\n# \u9884\u6d4b\u7ed3\u679c\ndef createForeCast(tree, testData, modelEval=regTreeEval):\n    \"\"\"\n    Desc:\n        \u8c03\u7528 treeForeCast \uff0c\u5bf9\u7279\u5b9a\u6a21\u578b\u7684\u6811\u8fdb\u884c\u9884\u6d4b\uff0c\u53ef\u4ee5\u662f \u56de\u5f52\u6811 \u4e5f\u53ef\u4ee5\u662f \u6a21\u578b\u6811\n    Args:\n        tree -- \u5df2\u7ecf\u8bad\u7ec3\u597d\u7684\u6811\u7684\u6a21\u578b\n        testData -- \u8f93\u5165\u7684\u6d4b\u8bd5\u6570\u636e\n        modelEval -- \u9884\u6d4b\u7684\u6811\u7684\u6a21\u578b\u7c7b\u578b\uff0c\u53ef\u9009\u503c\u4e3a regTreeEval\uff08\u56de\u5f52\u6811\uff09 \u6216 modelTreeEval\uff08\u6a21\u578b\u6811\uff09\uff0c\u9ed8\u8ba4\u4e3a\u56de\u5f52\u6811\n    Returns:\n        \u8fd4\u56de\u9884\u6d4b\u503c\u77e9\u9635\n    \"\"\"\n    m = len(testData)\n    yHat = mat(zeros((m, 1)))\n    # print yHat\n    for i in range(m):\n        yHat[i, 0] = treeForeCast(tree, mat(testData[i]), modelEval)\n        # print \"yHat==>\", yHat[i, 0]\n    return yHat\n\n\nif __name__ == \"__main__\":\n    # \u6d4b\u8bd5\u6570\u636e\u96c6\n    testMat = mat(eye(4))\n    print(testMat)\n    print(type(testMat))\n    mat0, mat1 = binSplitDataSet(testMat, 1, 0.5)\n    print(mat0, '\\n-----------\\n', mat1)\n\n    # # \u56de\u5f52\u6811\n    # myDat = loadDataSet('data/9.RegTrees/data1.txt')\n    # # myDat = loadDataSet('data/9.RegTrees/data2.txt')\n    # # print 'myDat=', myDat\n    # myMat = mat(myDat)\n    # # print 'myMat=',  myMat\n    # myTree = createTree(myMat)\n    # print myTree\n\n    # # 1. \u9884\u526a\u679d\u5c31\u662f: \u63d0\u8d77\u8bbe\u7f6e\u6700\u5927\u8bef\u5dee\u6570\u548c\u6700\u5c11\u5143\u7d20\u6570\n    # myDat = loadDataSet('data/9.RegTrees/data3.txt')\n    # myMat = mat(myDat)\n    # myTree = createTree(myMat, ops=(0, 1))\n    # print myTree\n\n    # # 2. \u540e\u526a\u679d\u5c31\u662f: \u901a\u8fc7\u6d4b\u8bd5\u6570\u636e\uff0c\u5bf9\u9884\u6d4b\u6a21\u578b\u8fdb\u884c\u5408\u5e76\u5224\u65ad\n    # myDatTest = loadDataSet('data/9.RegTrees/data3test.txt')\n    # myMat2Test = mat(myDatTest)\n    # myFinalTree = prune(myTree, myMat2Test)\n    # print '\\n\\n\\n-------------------'\n    # print myFinalTree\n\n    # # --------\n    # # \u6a21\u578b\u6811\u6c42\u89e3\n    # myDat = loadDataSet('data/9.RegTrees/data4.txt')\n    # myMat = mat(myDat)\n    # myTree = createTree(myMat, modelLeaf, modelErr)\n    # print myTree\n\n    # # # \u56de\u5f52\u6811 VS \u6a21\u578b\u6811 VS \u7ebf\u6027\u56de\u5f52\n    # trainMat = mat(loadDataSet('data/9.RegTrees/bikeSpeedVsIq_train.txt'))\n    # testMat = mat(loadDataSet('data/9.RegTrees/bikeSpeedVsIq_test.txt'))\n    # # # \u56de\u5f52\u6811\n    # myTree1 = createTree(trainMat, ops=(1, 20))\n    # print myTree1\n    # yHat1 = createForeCast(myTree1, testMat[:, 0])\n    # print \"--------------\\n\"\n    # # print yHat1\n    # # print \"ssss==>\", testMat[:, 1]\n    # # corrcoef \u8fd4\u56de\u76ae\u5c14\u68ee\u4e58\u79ef\u77e9\u76f8\u5173\u7cfb\u6570\n    # print \"regTree:\", corrcoef(yHat1, testMat[:, 1],rowvar=0)[0, 1]\n\n    # # \u6a21\u578b\u6811\n    # myTree2 = createTree(trainMat, modelLeaf, modelErr, ops=(1, 20))\n    # yHat2 = createForeCast(myTree2, testMat[:, 0], modelTreeEval)\n    # print myTree2\n    # print \"modelTree:\", corrcoef(yHat2, testMat[:, 1],rowvar=0)[0, 1]\n\n    # # \u7ebf\u6027\u56de\u5f52\n    # ws, X, Y = linearSolve(trainMat)\n    # print ws\n    # m = len(testMat[:, 0])\n    # yHat3 = mat(zeros((m, 1)))\n    # for i in range(shape(testMat)[0]):\n    #     yHat3[i] = testMat[i, 0]*ws[1, 0] + ws[0, 0]\n    # print \"lr:\", corrcoef(yHat3, testMat[:, 1],rowvar=0)[0, 1]\n", "src/py2.x/ml/1.MLFoundation/NumPy.py": "#!/usr/bin/python\n# coding:utf-8\n\n'''\nCreated on 2017-05-18\nUpdate  on 2017-05-18\nAuthor: Peter Harrington/1988/\u7247\u523b\nGitHub: https://github.com/apachecn/AiLearning\n'''\nfrom __future__ import print_function\n\nfrom numpy import random, mat, eye\n\n'''\n# NumPy \u77e9\u9635\u548c\u6570\u7ec4\u7684\u533a\u522b\nNumPy\u5b58\u57282\u4e2d\u4e0d\u540c\u7684\u6570\u636e\u7c7b\u578b:\n    1. \u77e9\u9635 matrix\n    2. \u6570\u7ec4 array\n\u76f8\u4f3c\u70b9: \n    \u90fd\u53ef\u4ee5\u5904\u7406\u884c\u5217\u8868\u793a\u7684\u6570\u5b57\u5143\u7d20\n\u4e0d\u540c\u70b9: \n    1. 2\u4e2a\u6570\u636e\u7c7b\u578b\u4e0a\u6267\u884c\u76f8\u540c\u7684\u6570\u636e\u8fd0\u7b97\u53ef\u80fd\u5f97\u5230\u4e0d\u540c\u7684\u7ed3\u679c\u3002\n    2. NumPy\u51fd\u6570\u5e93\u4e2d\u7684 matrix \u4e0e MATLAB\u4e2d matrices \u7b49\u4ef7\u3002\n'''\n\n# \u751f\u6210\u4e00\u4e2a 4*4 \u7684\u968f\u673a\u6570\u7ec4\nrandArray = random.rand(4, 4)\n\n# \u8f6c\u5316\u5173\u7cfb\uff0c \u6570\u7ec4\u8f6c\u5316\u4e3a\u77e9\u9635\nrandMat = mat(randArray)\n'''\n.I \u8868\u793a\u5bf9\u77e9\u9635\u6c42\u9006(\u53ef\u4ee5\u5229\u7528\u77e9\u9635\u7684\u521d\u7b49\u53d8\u6362)\n   \u610f\u4e49: \u9006\u77e9\u9635\u662f\u4e00\u4e2a\u5224\u65ad\u76f8\u4f3c\u6027\u7684\u5de5\u5177\u3002\u9006\u77e9\u9635A\u4e0e\u5217\u5411\u91cfp\u76f8\u4e58\u540e\uff0c\u5c06\u5f97\u5230\u5217\u5411\u91cfq\uff0cq\u7684\u7b2ci\u4e2a\u5206\u91cf\u8868\u793ap\u4e0eA\u7684\u7b2ci\u4e2a\u5217\u5411\u91cf\u7684\u76f8\u4f3c\u5ea6\u3002\n   \u53c2\u8003\u6848\u4f8b\u94fe\u63a5: \n   https://www.zhihu.com/question/33258489\n   http://blog.csdn.net/vernice/article/details/48506027\n.T \u8868\u793a\u5bf9\u77e9\u9635\u8f6c\u7f6e(\u884c\u5217\u98a0\u5012)\n    * \u7b49\u540c\u4e8e: .transpose()\n.A \u8fd4\u56de\u77e9\u9635\u57fa\u4e8e\u7684\u6570\u7ec4\n    \u53c2\u8003\u6848\u4f8b\u94fe\u63a5: \n    http://blog.csdn.net/qq403977698/article/details/47254539\n'''\ninvRandMat = randMat.I\nTraRandMat = randMat.T\nArrRandMat = randMat.A\n# \u8f93\u51fa\u7ed3\u679c\nprint('randArray=(%s) \\n' % type(randArray), randArray)\nprint('randMat=(%s) \\n' % type(randMat), randMat)\nprint('invRandMat=(%s) \\n' % type(invRandMat), invRandMat)\nprint('TraRandMat=(%s) \\n' % type(TraRandMat), TraRandMat)\nprint('ArrRandMat=(%s) \\n' % type(ArrRandMat), ArrRandMat)\n# \u77e9\u9635\u548c\u9006\u77e9\u9635 \u8fdb\u884c\u6c42\u79ef (\u5355\u4f4d\u77e9\u9635\uff0c\u5bf9\u89d2\u7ebf\u90fd\u4e3a1\u561b\uff0c\u7406\u8bba\u4e0a4*4\u7684\u77e9\u9635\u5176\u4ed6\u7684\u90fd\u4e3a0)\nmyEye = randMat*invRandMat\n# \u8bef\u5dee\nprint(myEye - eye(4))\n\n'''\n\u5982\u679c\u4e0a\u9762\u7684\u4ee3\u7801\u8fd0\u884c\u6ca1\u6709\u95ee\u9898\uff0c\u8bf4\u660enumpy\u5b89\u88c5\u6ca1\u6709\u95ee\u9898\n'''\n", "src/py2.x/ml/14.SVD/svdRecommend.py": "#!/usr/bin/python\n# coding: utf-8\n\n'''\nCreated on Mar 8, 2011\nUpdate  on 2017-05-18\nAuthor: Peter Harrington/\u5c71\u4e0a\u6709\u8bfe\u6811/\u7247\u523b\nGitHub: https://github.com/apachecn/AiLearning\n'''\nfrom __future__ import print_function\nfrom numpy import linalg as la\nfrom numpy import *\n\n\ndef loadExData3():\n    # \u5229\u7528SVD\u63d0\u9ad8\u63a8\u8350\u6548\u679c\uff0c\u83dc\u80b4\u77e9\u9635\n    return[[2, 0, 0, 4, 4, 0, 0, 0, 0, 0, 0],\n           [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5],\n           [0, 0, 0, 0, 0, 0, 0, 1, 0, 4, 0],\n           [3, 3, 4, 0, 3, 0, 0, 2, 2, 0, 0],\n           [5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0],\n           [0, 0, 0, 0, 0, 0, 5, 0, 0, 5, 0],\n           [4, 0, 4, 0, 0, 0, 0, 0, 0, 0, 5],\n           [0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 4],\n           [0, 0, 0, 0, 0, 0, 5, 0, 0, 5, 0],\n           [0, 0, 0, 3, 0, 0, 0, 0, 4, 5, 0],\n           [1, 1, 2, 1, 1, 2, 1, 0, 4, 5, 0]]\n\n\ndef loadExData2():\n    # \u4e66\u4e0a\u4ee3\u7801\u7ed9\u7684\u793a\u4f8b\u77e9\u9635\n    return[[0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 5],\n           [0, 0, 0, 3, 0, 4, 0, 0, 0, 0, 3],\n           [0, 0, 0, 0, 4, 0, 0, 1, 0, 4, 0],\n           [3, 3, 4, 0, 0, 0, 0, 2, 2, 0, 0],\n           [5, 4, 5, 0, 0, 0, 0, 5, 5, 0, 0],\n           [0, 0, 0, 0, 5, 0, 1, 0, 0, 5, 0],\n           [4, 3, 4, 0, 0, 0, 0, 5, 5, 0, 1],\n           [0, 0, 0, 4, 0, 4, 0, 0, 0, 0, 4],\n           [0, 0, 0, 2, 0, 2, 5, 0, 0, 1, 2],\n           [0, 0, 0, 0, 5, 0, 0, 0, 0, 4, 0],\n           [1, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0]]\n\n\ndef loadExData():\n    \"\"\"\n    # \u63a8\u8350\u5f15\u64ce\u793a\u4f8b\u77e9\u9635\n    return[[4, 4, 0, 2, 2],\n           [4, 0, 0, 3, 3],\n           [4, 0, 0, 1, 1],\n           [1, 1, 1, 2, 0],\n           [2, 2, 2, 0, 0],\n           [1, 1, 1, 0, 0],\n           [5, 5, 5, 0, 0]]\n    \"\"\"\n    # # \u539f\u77e9\u9635\n    # return[[1, 1, 1, 0, 0],\n    #        [2, 2, 2, 0, 0],\n    #        [1, 1, 1, 0, 0],\n    #        [5, 5, 5, 0, 0],\n    #        [1, 1, 0, 2, 2],\n    #        [0, 0, 0, 3, 3],\n    #        [0, 0, 0, 1, 1]]\n\n    # \u539f\u77e9\u9635\n    return[[0, -1.6, 0.6],\n           [0, 1.2, 0.8],\n           [0, 0, 0],\n           [0, 0, 0]]\n\n\n# \u76f8\u4f3c\u5ea6\u8ba1\u7b97\uff0c\u5047\u5b9ainA\u548cinB \u90fd\u662f\u5217\u5411\u91cf\n# \u57fa\u4e8e\u6b27\u6c0f\u8ddd\u79bb\ndef ecludSim(inA, inB):\n    return 1.0/(1.0 + la.norm(inA - inB))\n\n\n# pearsSim()\u51fd\u6570\u4f1a\u68c0\u67e5\u662f\u5426\u5b58\u57283\u4e2a\u6216\u66f4\u591a\u7684\u70b9\u3002\n# corrcoef\u76f4\u63a5\u8ba1\u7b97\u76ae\u5c14\u900a\u76f8\u5173\u7cfb\u6570\uff0c\u8303\u56f4[-1, 1]\uff0c\u5f52\u4e00\u5316\u540e[0, 1]\ndef pearsSim(inA, inB):\n    # \u5982\u679c\u4e0d\u5b58\u5728\uff0c\u8be5\u51fd\u6570\u8fd4\u56de1.0\uff0c\u6b64\u65f6\u4e24\u4e2a\u5411\u91cf\u5b8c\u5168\u76f8\u5173\u3002\n    if len(inA) < 3:\n        return 1.0\n    return 0.5 + 0.5 * corrcoef(inA, inB, rowvar=0)[0][1]\n\n\n# \u8ba1\u7b97\u4f59\u5f26\u76f8\u4f3c\u5ea6\uff0c\u5982\u679c\u5939\u89d2\u4e3a90\u5ea6\uff0c\u76f8\u4f3c\u5ea6\u4e3a0\uff1b\u5982\u679c\u4e24\u4e2a\u5411\u91cf\u7684\u65b9\u5411\u76f8\u540c\uff0c\u76f8\u4f3c\u5ea6\u4e3a1.0\ndef cosSim(inA, inB):\n    num = float(inA.T*inB)\n    denom = la.norm(inA)*la.norm(inB)\n    return 0.5 + 0.5*(num/denom)\n\n\n# \u57fa\u4e8e\u7269\u54c1\u76f8\u4f3c\u5ea6\u7684\u63a8\u8350\u5f15\u64ce\ndef standEst(dataMat, user, simMeas, item):\n    \"\"\"standEst(\u8ba1\u7b97\u67d0\u7528\u6237\u672a\u8bc4\u5206\u7269\u54c1\u4e2d\uff0c\u4ee5\u5bf9\u8be5\u7269\u54c1\u548c\u5176\u4ed6\u7269\u54c1\u8bc4\u5206\u7684\u7528\u6237\u7684\u7269\u54c1\u76f8\u4f3c\u5ea6\uff0c\u7136\u540e\u8fdb\u884c\u7efc\u5408\u8bc4\u5206)\n\n    Args:\n        dataMat         \u8bad\u7ec3\u6570\u636e\u96c6\n        user            \u7528\u6237\u7f16\u53f7\n        simMeas         \u76f8\u4f3c\u5ea6\u8ba1\u7b97\u65b9\u6cd5\n        item            \u672a\u8bc4\u5206\u7684\u7269\u54c1\u7f16\u53f7\n    Returns:\n        ratSimTotal/simTotal     \u8bc4\u5206\uff080\uff5e5\u4e4b\u95f4\u7684\u503c\uff09\n    \"\"\"\n    # \u5f97\u5230\u6570\u636e\u96c6\u4e2d\u7684\u7269\u54c1\u6570\u76ee\n    n = shape(dataMat)[1]\n    # \u521d\u59cb\u5316\u4e24\u4e2a\u8bc4\u5206\u503c\n    simTotal = 0.0\n    ratSimTotal = 0.0\n    # \u904d\u5386\u884c\u4e2d\u7684\u6bcf\u4e2a\u7269\u54c1\uff08\u5bf9\u7528\u6237\u8bc4\u8fc7\u5206\u7684\u7269\u54c1\u8fdb\u884c\u904d\u5386\uff0c\u5e76\u5c06\u5b83\u4e0e\u5176\u4ed6\u7269\u54c1\u8fdb\u884c\u6bd4\u8f83\uff09\n    for j in range(n):\n        userRating = dataMat[user, j]\n        # \u5982\u679c\u67d0\u4e2a\u7269\u54c1\u7684\u8bc4\u5206\u503c\u4e3a0\uff0c\u5219\u8df3\u8fc7\u8fd9\u4e2a\u7269\u54c1\n        if userRating == 0:\n            continue\n        # \u5bfb\u627e\u4e24\u4e2a\u7528\u6237\u90fd\u8bc4\u7ea7\u7684\u7269\u54c1\n        # \u53d8\u91cf overLap \u7ed9\u51fa\u7684\u662f\u4e24\u4e2a\u7269\u54c1\u5f53\u4e2d\u5df2\u7ecf\u88ab\u8bc4\u5206\u7684\u90a3\u4e2a\u5143\u7d20\u7684\u7d22\u5f15ID\n        # logical_and \u8ba1\u7b97x1\u548cx2\u5143\u7d20\u7684\u771f\u503c\u3002\n        overLap = nonzero(logical_and(dataMat[:, item].A > 0, dataMat[:, j].A > 0))[0]\n        # \u5982\u679c\u76f8\u4f3c\u5ea6\u4e3a0\uff0c\u5219\u4e24\u7740\u6ca1\u6709\u4efb\u4f55\u91cd\u5408\u5143\u7d20\uff0c\u7ec8\u6b62\u672c\u6b21\u5faa\u73af\n        if len(overLap) == 0:\n            similarity = 0\n        # \u5982\u679c\u5b58\u5728\u91cd\u5408\u7684\u7269\u54c1\uff0c\u5219\u57fa\u4e8e\u8fd9\u4e9b\u91cd\u5408\u7269\u91cd\u65b0\u8ba1\u7b97\u76f8\u4f3c\u5ea6\u3002\n        else:\n            similarity = simMeas(dataMat[overLap, item], dataMat[overLap, j])\n        # print 'the %d and %d similarity is : %f'(iten,j,similarity)\n        # \u76f8\u4f3c\u5ea6\u4f1a\u4e0d\u65ad\u7d2f\u52a0\uff0c\u6bcf\u6b21\u8ba1\u7b97\u65f6\u8fd8\u8003\u8651\u76f8\u4f3c\u5ea6\u548c\u5f53\u524d\u7528\u6237\u8bc4\u5206\u7684\u4e58\u79ef\n        # similarity  \u7528\u6237\u76f8\u4f3c\u5ea6\uff0c   userRating \u7528\u6237\u8bc4\u5206\n        simTotal += similarity\n        ratSimTotal += similarity * userRating\n    if simTotal == 0:\n        return 0\n    # \u901a\u8fc7\u9664\u4ee5\u6240\u6709\u7684\u8bc4\u5206\u603b\u548c\uff0c\u5bf9\u4e0a\u8ff0\u76f8\u4f3c\u5ea6\u8bc4\u5206\u7684\u4e58\u79ef\u8fdb\u884c\u5f52\u4e00\u5316\uff0c\u4f7f\u5f97\u6700\u540e\u8bc4\u5206\u57280~5\u4e4b\u95f4\uff0c\u8fd9\u4e9b\u8bc4\u5206\u7528\u6765\u5bf9\u9884\u6d4b\u503c\u8fdb\u884c\u6392\u5e8f\n    else:\n        return ratSimTotal/simTotal\n\n\n# \u57fa\u4e8eSVD\u7684\u8bc4\u5206\u4f30\u8ba1\n# \u5728recommend() \u4e2d\uff0c\u8fd9\u4e2a\u51fd\u6570\u7528\u4e8e\u66ff\u6362\u5bf9standEst()\u7684\u8c03\u7528\uff0c\u8be5\u51fd\u6570\u5bf9\u7ed9\u5b9a\u7528\u6237\u7ed9\u5b9a\u7269\u54c1\u6784\u5efa\u4e86\u4e00\u4e2a\u8bc4\u5206\u4f30\u8ba1\u503c\ndef svdEst(dataMat, user, simMeas, item):\n    \"\"\"svdEst( )\n\n    Args:\n        dataMat         \u8bad\u7ec3\u6570\u636e\u96c6\n        user            \u7528\u6237\u7f16\u53f7\n        simMeas         \u76f8\u4f3c\u5ea6\u8ba1\u7b97\u65b9\u6cd5\n        item            \u672a\u8bc4\u5206\u7684\u7269\u54c1\u7f16\u53f7\n    Returns:\n        ratSimTotal/simTotal     \u8bc4\u5206\uff080\uff5e5\u4e4b\u95f4\u7684\u503c\uff09\n    \"\"\"\n    # \u7269\u54c1\u6570\u76ee\n    n = shape(dataMat)[1]\n    # \u5bf9\u6570\u636e\u96c6\u8fdb\u884cSVD\u5206\u89e3\n    simTotal = 0.0\n    ratSimTotal = 0.0\n    # \u5947\u5f02\u503c\u5206\u89e3\n    # \u5728SVD\u5206\u89e3\u4e4b\u540e\uff0c\u6211\u4eec\u53ea\u5229\u7528\u5305\u542b\u4e8690%\u80fd\u91cf\u503c\u7684\u5947\u5f02\u503c\uff0c\u8fd9\u4e9b\u5947\u5f02\u503c\u4f1a\u4ee5NumPy\u6570\u7ec4\u7684\u5f62\u5f0f\u5f97\u4ee5\u4fdd\u5b58\n    U, Sigma, VT = la.svd(dataMat)\n\n    # # \u5206\u6790 Sigma \u7684\u957f\u5ea6\u53d6\u503c\n    # analyse_data(Sigma, 20)\n\n    # \u5982\u679c\u8981\u8fdb\u884c\u77e9\u9635\u8fd0\u7b97\uff0c\u5c31\u5fc5\u987b\u8981\u7528\u8fd9\u4e9b\u5947\u5f02\u503c\u6784\u5efa\u51fa\u4e00\u4e2a\u5bf9\u89d2\u77e9\u9635\n    Sig4 = mat(eye(4) * Sigma[: 4])\n\n    # \u5229\u7528U\u77e9\u9635\u5c06\u7269\u54c1\u8f6c\u6362\u5230\u4f4e\u7ef4\u7a7a\u95f4\u4e2d\uff0c\u6784\u5efa\u8f6c\u6362\u540e\u7684\u7269\u54c1(\u7269\u54c1+4\u4e2a\u4e3b\u8981\u7684\u7279\u5f81)\n    xformedItems = dataMat.T * U[:, :4] * Sig4.I\n    print('dataMat', shape(dataMat))\n    print('U[:, :4]', shape(U[:, :4]))\n    print('Sig4.I', shape(Sig4.I))\n    print('VT[:4, :]', shape(VT[:4, :]))\n    print('xformedItems', shape(xformedItems))\n\n    # \u5bf9\u4e8e\u7ed9\u5b9a\u7684\u7528\u6237\uff0cfor\u5faa\u73af\u5728\u7528\u6237\u5bf9\u5e94\u884c\u7684\u5143\u7d20\u4e0a\u8fdb\u884c\u904d\u5386\n    # \u8fd9\u548cstandEst()\u51fd\u6570\u4e2d\u7684for\u5faa\u73af\u7684\u76ee\u7684\u4e00\u6837\uff0c\u53ea\u4e0d\u8fc7\u8fd9\u91cc\u7684\u76f8\u4f3c\u5ea6\u8ba1\u7b97\u65f6\u5728\u4f4e\u7ef4\u7a7a\u95f4\u4e0b\u8fdb\u884c\u7684\u3002\n    for j in range(n):\n        userRating = dataMat[user, j]\n        if userRating == 0 or j == item:\n            continue\n        # \u76f8\u4f3c\u5ea6\u7684\u8ba1\u7b97\u65b9\u6cd5\u4e5f\u4f1a\u4f5c\u4e3a\u4e00\u4e2a\u53c2\u6570\u4f20\u9012\u7ed9\u8be5\u51fd\u6570\n        similarity = simMeas(xformedItems[item, :].T, xformedItems[j, :].T)\n        # for \u5faa\u73af\u4e2d\u52a0\u5165\u4e86\u4e00\u6761print\u8bed\u53e5\uff0c\u4ee5\u4fbf\u4e86\u89e3\u76f8\u4f3c\u5ea6\u8ba1\u7b97\u7684\u8fdb\u5c55\u60c5\u51b5\u3002\u5982\u679c\u89c9\u5f97\u7d2f\u8d58\uff0c\u53ef\u4ee5\u53bb\u6389\n        print('the %d and %d similarity is: %f' % (item, j, similarity))\n        # \u5bf9\u76f8\u4f3c\u5ea6\u4e0d\u65ad\u7d2f\u52a0\u6c42\u548c\n        simTotal += similarity\n        # \u5bf9\u76f8\u4f3c\u5ea6\u53ca\u5bf9\u5e94\u8bc4\u5206\u503c\u7684\u4e58\u79ef\u6c42\u548c\n        ratSimTotal += similarity * userRating\n    if simTotal == 0:\n        return 0\n    else:\n        # \u8ba1\u7b97\u4f30\u8ba1\u8bc4\u5206\n        return ratSimTotal/simTotal\n\n\n# recommend()\u51fd\u6570\uff0c\u5c31\u662f\u63a8\u8350\u5f15\u64ce\uff0c\u5b83\u9ed8\u8ba4\u8c03\u7528standEst()\u51fd\u6570\uff0c\u4ea7\u751f\u4e86\u6700\u9ad8\u7684N\u4e2a\u63a8\u8350\u7ed3\u679c\u3002\n# \u5982\u679c\u4e0d\u6307\u5b9aN\u7684\u5927\u5c0f\uff0c\u5219\u9ed8\u8ba4\u503c\u4e3a3\u3002\u8be5\u51fd\u6570\u53e6\u5916\u7684\u53c2\u6570\u8fd8\u5305\u62ec\u76f8\u4f3c\u5ea6\u8ba1\u7b97\u65b9\u6cd5\u548c\u4f30\u8ba1\u65b9\u6cd5\ndef recommend(dataMat, user, N=3, simMeas=cosSim, estMethod=standEst):\n    \"\"\"svdEst( )\n\n    Args:\n        dataMat         \u8bad\u7ec3\u6570\u636e\u96c6\n        user            \u7528\u6237\u7f16\u53f7\n        simMeas         \u76f8\u4f3c\u5ea6\u8ba1\u7b97\u65b9\u6cd5\n        estMethod       \u4f7f\u7528\u7684\u63a8\u8350\u7b97\u6cd5\n    Returns:\n        \u8fd4\u56de\u6700\u7ec8 N \u4e2a\u63a8\u8350\u7ed3\u679c\n    \"\"\"\n    # \u5bfb\u627e\u672a\u8bc4\u7ea7\u7684\u7269\u54c1\n    # \u5bf9\u7ed9\u5b9a\u7684\u7528\u6237\u5efa\u7acb\u4e00\u4e2a\u672a\u8bc4\u5206\u7684\u7269\u54c1\u5217\u8868\n    unratedItems = nonzero(dataMat[user, :].A == 0)[1]\n    # \u5982\u679c\u4e0d\u5b58\u5728\u672a\u8bc4\u5206\u7269\u54c1\uff0c\u90a3\u4e48\u5c31\u9000\u51fa\u51fd\u6570\n    if len(unratedItems) == 0:\n        return 'you rated everything'\n    # \u7269\u54c1\u7684\u7f16\u53f7\u548c\u8bc4\u5206\u503c\n    itemScores = []\n    # \u5728\u672a\u8bc4\u5206\u7269\u54c1\u4e0a\u8fdb\u884c\u5faa\u73af\n    for item in unratedItems:\n        # \u83b7\u53d6 item \u8be5\u7269\u54c1\u7684\u8bc4\u5206\n        estimatedScore = estMethod(dataMat, user, simMeas, item)\n        itemScores.append((item, estimatedScore))\n    # \u6309\u7167\u8bc4\u5206\u5f97\u5206 \u8fdb\u884c\u9006\u6392\u5e8f\uff0c\u83b7\u53d6\u524dN\u4e2a\u672a\u8bc4\u7ea7\u7269\u54c1\u8fdb\u884c\u63a8\u8350\n    return sorted(itemScores, key=lambda jj: jj[1], reverse=True)[: N]\n\n\ndef analyse_data(Sigma, loopNum=20):\n    \"\"\"analyse_data(\u5206\u6790 Sigma \u7684\u957f\u5ea6\u53d6\u503c)\n\n    Args:\n        Sigma         Sigma\u7684\u503c\n        loopNum       \u5faa\u73af\u6b21\u6570\n    \"\"\"\n    # \u603b\u65b9\u5dee\u7684\u96c6\u5408\uff08\u603b\u80fd\u91cf\u503c\uff09\n    Sig2 = Sigma**2\n    SigmaSum = sum(Sig2)\n    for i in range(loopNum):\n        SigmaI = sum(Sig2[:i+1])\n        '''\n        \u6839\u636e\u81ea\u5df1\u7684\u4e1a\u52a1\u60c5\u51b5\uff0c\u5c31\u884c\u5904\u7406\uff0c\u8bbe\u7f6e\u5bf9\u5e94\u7684 Singma \u6b21\u6570\n\n        \u901a\u5e38\u4fdd\u7559\u77e9\u9635 80% \uff5e 90% \u7684\u80fd\u91cf\uff0c\u5c31\u53ef\u4ee5\u5f97\u5230\u91cd\u8981\u7684\u7279\u5f81\u5e76\u53d6\u51fa\u566a\u58f0\u3002\n        '''\n        print('\u4e3b\u6210\u5206: %s, \u65b9\u5dee\u5360\u6bd4: %s%%' % (format(i+1, '2.0f'), format(SigmaI/SigmaSum*100, '4.2f')))\n\n\n# \u56fe\u50cf\u538b\u7f29\u51fd\u6570\n# \u52a0\u8f7d\u5e76\u8f6c\u6362\u6570\u636e\ndef imgLoadData(filename):\n    myl = []\n    # \u6253\u5f00\u6587\u672c\u6587\u4ef6\uff0c\u5e76\u4ece\u6587\u4ef6\u4ee5\u6570\u7ec4\u65b9\u5f0f\u8bfb\u5165\u5b57\u7b26\n    for line in open(filename).readlines():\n        newRow = []\n        for i in range(32):\n            newRow.append(int(line[i]))\n        myl.append(newRow)\n    # \u77e9\u9635\u8c03\u5165\u540e\uff0c\u5c31\u53ef\u4ee5\u5728\u5c4f\u5e55\u4e0a\u8f93\u51fa\u8be5\u77e9\u9635\n    myMat = mat(myl)\n    return myMat\n\n\n# \u6253\u5370\u77e9\u9635\ndef printMat(inMat, thresh=0.8):\n    # \u7531\u4e8e\u77e9\u9635\u4fdd\u62a4\u4e86\u6d6e\u70b9\u6570\uff0c\u56e0\u6b64\u5b9a\u4e49\u6d45\u8272\u548c\u6df1\u8272\uff0c\u904d\u5386\u6240\u6709\u77e9\u9635\u5143\u7d20\uff0c\u5f53\u5143\u7d20\u5927\u4e8e\u9600\u503c\u65f6\u6253\u53701\uff0c\u5426\u5219\u6253\u53700\n    for i in range(32):\n        for k in range(32):\n            if float(inMat[i, k]) > thresh:\n                print(1, end=' ')\n            else:\n                print(0, end=' ')\n        print('')\n\n\n# \u5b9e\u73b0\u56fe\u50cf\u538b\u7f29\uff0c\u5141\u8bb8\u57fa\u4e8e\u4efb\u610f\u7ed9\u5b9a\u7684\u5947\u5f02\u503c\u6570\u76ee\u6765\u91cd\u6784\u56fe\u50cf\ndef imgCompress(numSV=3, thresh=0.8):\n    \"\"\"imgCompress( )\n\n    Args:\n        numSV       Sigma\u957f\u5ea6   \n        thresh      \u5224\u65ad\u7684\u9608\u503c\n    \"\"\"\n    # \u6784\u5efa\u4e00\u4e2a\u5217\u8868\n    myMat = imgLoadData('data/14.SVD/0_5.txt')\n\n    print(\"****original matrix****\")\n    # \u5bf9\u539f\u59cb\u56fe\u50cf\u8fdb\u884cSVD\u5206\u89e3\u5e76\u91cd\u6784\u56fe\u50cfe\n    printMat(myMat, thresh)\n\n    # \u901a\u8fc7Sigma \u91cd\u65b0\u6784\u6210SigRecom\u6765\u5b9e\u73b0\n    # Sigma\u662f\u4e00\u4e2a\u5bf9\u89d2\u77e9\u9635\uff0c\u56e0\u6b64\u9700\u8981\u5efa\u7acb\u4e00\u4e2a\u51680\u77e9\u9635\uff0c\u7136\u540e\u5c06\u524d\u9762\u7684\u90a3\u4e9b\u5947\u5f02\u503c\u586b\u5145\u5230\u5bf9\u89d2\u7ebf\u4e0a\u3002\n    U, Sigma, VT = la.svd(myMat)\n    # SigRecon = mat(zeros((numSV, numSV)))\n    # for k in range(numSV):\n    #     SigRecon[k, k] = Sigma[k]\n\n    # \u5206\u6790\u63d2\u5165\u7684 Sigma \u957f\u5ea6\n    analyse_data(Sigma, 20)\n\n    SigRecon = mat(eye(numSV) * Sigma[: numSV])\n    reconMat = U[:, :numSV] * SigRecon * VT[:numSV, :]\n    print(\"****reconstructed matrix using %d singular values *****\" % numSV)\n    printMat(reconMat, thresh)\n\n\nif __name__ == \"__main__\":\n\n    # # \u5bf9\u77e9\u9635\u8fdb\u884cSVD\u5206\u89e3(\u7528python\u5b9e\u73b0SVD)\n    # Data = loadExData()\n    # print 'Data:', Data\n    # U, Sigma, VT = linalg.svd(Data)\n    # # \u6253\u5370Sigma\u7684\u7ed3\u679c\uff0c\u56e0\u4e3a\u524d3\u4e2a\u6570\u503c\u6bd4\u5176\u4ed6\u7684\u503c\u5927\u4e86\u5f88\u591a\uff0c\u4e3a9.72140007e+00\uff0c5.29397912e+00\uff0c6.84226362e-01\n    # # \u540e\u4e24\u4e2a\u503c\u6bd4\u8f83\u5c0f\uff0c\u6bcf\u53f0\u673a\u5668\u8f93\u51fa\u7ed3\u679c\u53ef\u80fd\u6709\u4e0d\u540c\u53ef\u4ee5\u5c06\u8fd9\u4e24\u4e2a\u503c\u53bb\u6389\n    # print 'U:', U\n    # print 'Sigma', Sigma\n    # print 'VT:', VT\n    # print 'VT:', VT.T\n\n    # # \u91cd\u6784\u4e00\u4e2a3x3\u7684\u77e9\u9635Sig3\n    # Sig3 = mat([[Sigma[0], 0, 0], [0, Sigma[1], 0], [0, 0, Sigma[2]]])\n    # print U[:, :3] * Sig3 * VT[:3, :]\n\n    \"\"\"\n    # \u8ba1\u7b97\u6b27\u6c0f\u8ddd\u79bb\n    myMat = mat(loadExData())\n    # print myMat\n    print ecludSim(myMat[:, 0], myMat[:, 4])\n    print ecludSim(myMat[:, 0], myMat[:, 0])\n\n    # \u8ba1\u7b97\u4f59\u5f26\u76f8\u4f3c\u5ea6\n    print cosSim(myMat[:, 0], myMat[:, 4])\n    print cosSim(myMat[:, 0], myMat[:, 0])\n\n    # \u8ba1\u7b97\u76ae\u5c14\u900a\u76f8\u5173\u7cfb\u6570\n    print pearsSim(myMat[:, 0], myMat[:, 4])\n    print pearsSim(myMat[:, 0], myMat[:, 0])\n\n    \"\"\"\n\n    # \u8ba1\u7b97\u76f8\u4f3c\u5ea6\u7684\u65b9\u6cd5\n    myMat = mat(loadExData3())\n    # print myMat\n    # \u8ba1\u7b97\u76f8\u4f3c\u5ea6\u7684\u7b2c\u4e00\u79cd\u65b9\u5f0f\n    print(recommend(myMat, 1, estMethod=svdEst))\n    # \u8ba1\u7b97\u76f8\u4f3c\u5ea6\u7684\u7b2c\u4e8c\u79cd\u65b9\u5f0f\n    print(recommend(myMat, 1, estMethod=svdEst, simMeas=pearsSim))\n\n    # \u9ed8\u8ba4\u63a8\u8350\uff08\u83dc\u9986\u83dc\u80b4\u63a8\u8350\u793a\u4f8b\uff09\n    print(recommend(myMat, 2))\n\n    \"\"\"\n    # \u5229\u7528SVD\u63d0\u9ad8\u63a8\u8350\u6548\u679c\n    U, Sigma, VT = la.svd(mat(loadExData2()))\n    print Sigma                 # \u8ba1\u7b97\u77e9\u9635\u7684SVD\u6765\u4e86\u89e3\u5176\u9700\u8981\u591a\u5c11\u7ef4\u7684\u7279\u5f81\n    Sig2 = Sigma**2             # \u8ba1\u7b97\u9700\u8981\u591a\u5c11\u4e2a\u5947\u5f02\u503c\u80fd\u8fbe\u5230\u603b\u80fd\u91cf\u768490%\n    print sum(Sig2)             # \u8ba1\u7b97\u603b\u80fd\u91cf\n    print sum(Sig2) * 0.9       # \u8ba1\u7b97\u603b\u80fd\u91cf\u768490%\n    print sum(Sig2[: 2])        # \u8ba1\u7b97\u524d\u4e24\u4e2a\u5143\u7d20\u6240\u5305\u542b\u7684\u80fd\u91cf\n    print sum(Sig2[: 3])        # \u4e24\u4e2a\u5143\u7d20\u7684\u80fd\u91cf\u503c\u5c0f\u4e8e\u603b\u80fd\u91cf\u768490%\uff0c\u4e8e\u662f\u8ba1\u7b97\u524d\u4e09\u4e2a\u5143\u7d20\u6240\u5305\u542b\u7684\u80fd\u91cf\n    # \u8be5\u503c\u9ad8\u4e8e\u603b\u80fd\u91cf\u768490%\uff0c\u8fd9\u5c31\u53ef\u4ee5\u4e86\n\n    \"\"\"\n\n    # \u538b\u7f29\u56fe\u7247\n    # imgCompress(2)\n", "src/py2.x/ml/5.Logistic/logistic.py": "#!/usr/bin/python\n# coding: utf8\n'''\nCreated on Oct 27, 2010\nUpdate  on 2017-05-18\nLogistic Regression Working Module\nAuthor: Peter Harrington/\u7f8a\u4e09/\u5c0f\u7476\nGitHub: https://github.com/apachecn/AiLearning\n'''\nfrom __future__ import print_function\nfrom numpy import *\nimport matplotlib.pyplot as plt\n\n# ---------------------------------------------------------------------------\n# \u4f7f\u7528 Logistic \u56de\u5f52\u5728\u7b80\u5355\u6570\u636e\u96c6\u4e0a\u7684\u5206\u7c7b\n\n\n# \u89e3\u6790\u6570\u636e\ndef loadDataSet(file_name):\n    '''\n    Desc: \n        \u52a0\u8f7d\u5e76\u89e3\u6790\u6570\u636e\n    Args:\n        file_name -- \u6587\u4ef6\u540d\u79f0\uff0c\u8981\u89e3\u6790\u7684\u6587\u4ef6\u6240\u5728\u78c1\u76d8\u4f4d\u7f6e\n    Returns:\n        dataMat -- \u539f\u59cb\u6570\u636e\u7684\u7279\u5f81\n        labelMat -- \u539f\u59cb\u6570\u636e\u7684\u6807\u7b7e\uff0c\u4e5f\u5c31\u662f\u6bcf\u6761\u6837\u672c\u5bf9\u5e94\u7684\u7c7b\u522b\n    '''\n    # dataMat\u4e3a\u539f\u59cb\u6570\u636e\uff0c labelMat\u4e3a\u539f\u59cb\u6570\u636e\u7684\u6807\u7b7e\n    dataMat = []\n    labelMat = []\n    fr = open(file_name)\n    for line in fr.readlines():\n        lineArr = line.strip().split()\n        if len(lineArr) == 1:\n            continue    # \u8fd9\u91cc\u5982\u679c\u5c31\u4e00\u4e2a\u7a7a\u7684\u5143\u7d20\uff0c\u5219\u8df3\u8fc7\u672c\u6b21\u5faa\u73af\n        # \u4e3a\u4e86\u65b9\u4fbf\u8ba1\u7b97\uff0c\u6211\u4eec\u5c06 X0 \u7684\u503c\u8bbe\u4e3a 1.0 \uff0c\u4e5f\u5c31\u662f\u5728\u6bcf\u4e00\u884c\u7684\u5f00\u5934\u6dfb\u52a0\u4e00\u4e2a 1.0 \u4f5c\u4e3a X0\n        dataMat.append([1.0, float(lineArr[0]), float(lineArr[1])])\n        labelMat.append(int(lineArr[2]))\n    return dataMat, labelMat\n\n\n# sigmoid\u8df3\u8dc3\u51fd\u6570\ndef sigmoid(inX):\n    # return 1.0 / (1 + exp(-inX))\n\n    # Tanh\u662fSigmoid\u7684\u53d8\u5f62\uff0c\u4e0e sigmoid \u4e0d\u540c\u7684\u662f\uff0ctanh \u662f0\u5747\u503c\u7684\u3002\u56e0\u6b64\uff0c\u5b9e\u9645\u5e94\u7528\u4e2d\uff0ctanh \u4f1a\u6bd4 sigmoid \u66f4\u597d\u3002\n    return 2 * 1.0/(1+exp(-2*inX)) - 1\n\n\n# \u6b63\u5e38\u7684\u5904\u7406\u65b9\u6848\n# \u4e24\u4e2a\u53c2\u6570: \u7b2c\u4e00\u4e2a\u53c2\u6570==> dataMatIn \u662f\u4e00\u4e2a2\u7ef4NumPy\u6570\u7ec4\uff0c\u6bcf\u5217\u5206\u522b\u4ee3\u8868\u6bcf\u4e2a\u4e0d\u540c\u7684\u7279\u5f81\uff0c\u6bcf\u884c\u5219\u4ee3\u8868\u6bcf\u4e2a\u8bad\u7ec3\u6837\u672c\u3002\n# \u7b2c\u4e8c\u4e2a\u53c2\u6570==> classLabels \u662f\u7c7b\u522b\u6807\u7b7e\uff0c\u5b83\u662f\u4e00\u4e2a 1*100 \u7684\u884c\u5411\u91cf\u3002\u4e3a\u4e86\u4fbf\u4e8e\u77e9\u9635\u8ba1\u7b97\uff0c\u9700\u8981\u5c06\u8be5\u884c\u5411\u91cf\u8f6c\u6362\u4e3a\u5217\u5411\u91cf\uff0c\u505a\u6cd5\u662f\u5c06\u539f\u5411\u91cf\u8f6c\u7f6e\uff0c\u518d\u5c06\u5b83\u8d4b\u503c\u7ed9labelMat\u3002\ndef gradAscent(dataMatIn, classLabels):\n    '''\n    Desc:\n        \u6b63\u5e38\u7684\u68af\u5ea6\u4e0a\u5347\u6cd5\n    Args:\n        dataMatIn -- \u8f93\u5165\u7684 \u6570\u636e\u7684\u7279\u5f81 List\n        classLabels -- \u8f93\u5165\u7684\u6570\u636e\u7684\u7c7b\u522b\u6807\u7b7e\n    Returns:\n        array(weights) -- \u5f97\u5230\u7684\u6700\u4f73\u56de\u5f52\u7cfb\u6570\n    '''\n\n    # \u8f6c\u5316\u4e3a\u77e9\u9635[[1,1,2],[1,1,2]....]\n    dataMatrix = mat(dataMatIn)  # \u8f6c\u6362\u4e3a NumPy \u77e9\u9635\n    # \u8f6c\u5316\u4e3a\u77e9\u9635[[0,1,0,1,0,1.....]]\uff0c\u5e76\u8f6c\u5236[[0],[1],[0].....]\n    # transpose() \u884c\u5217\u8f6c\u7f6e\u51fd\u6570\n    # \u5c06\u884c\u5411\u91cf\u8f6c\u5316\u4e3a\u5217\u5411\u91cf   =>  \u77e9\u9635\u7684\u8f6c\u7f6e\n    labelMat = mat(classLabels).transpose()  # \u9996\u5148\u5c06\u6570\u7ec4\u8f6c\u6362\u4e3a NumPy \u77e9\u9635\uff0c\u7136\u540e\u518d\u5c06\u884c\u5411\u91cf\u8f6c\u7f6e\u4e3a\u5217\u5411\u91cf\n    # m->\u6570\u636e\u91cf\uff0c\u6837\u672c\u6570 n->\u7279\u5f81\u6570\n    m, n = shape(dataMatrix)\n    # print m, n, '__'*10, shape(dataMatrix.transpose()), '__'*100\n    # alpha\u4ee3\u8868\u5411\u76ee\u6807\u79fb\u52a8\u7684\u6b65\u957f\n    alpha = 0.001\n    # \u8fed\u4ee3\u6b21\u6570\n    maxCycles = 500\n    # \u751f\u6210\u4e00\u4e2a\u957f\u5ea6\u548c\u7279\u5f81\u6570\u76f8\u540c\u7684\u77e9\u9635\uff0c\u6b64\u5904n\u4e3a3 -> [[1],[1],[1]]\n    # weights \u4ee3\u8868\u56de\u5f52\u7cfb\u6570\uff0c \u6b64\u5904\u7684 ones((n,1)) \u521b\u5efa\u4e00\u4e2a\u957f\u5ea6\u548c\u7279\u5f81\u6570\u76f8\u540c\u7684\u77e9\u9635\uff0c\u5176\u4e2d\u7684\u6570\u5168\u90e8\u90fd\u662f 1\n    weights = ones((n, 1))\n    for k in range(maxCycles):  # heavy on matrix operations\n        # m*3 \u7684\u77e9\u9635 * 3*1 \u7684\u5355\u4f4d\u77e9\u9635 \uff1d m*1\u7684\u77e9\u9635\n        # \u90a3\u4e48\u4e58\u4e0a\u5355\u4f4d\u77e9\u9635\u7684\u610f\u4e49\uff0c\u5c31\u4ee3\u8868: \u901a\u8fc7\u516c\u5f0f\u5f97\u5230\u7684\u7406\u8bba\u503c\n        # \u53c2\u8003\u5730\u5740:  \u77e9\u9635\u4e58\u6cd5\u7684\u672c\u8d28\u662f\u4ec0\u4e48\uff1f https://www.zhihu.com/question/21351965/answer/31050145\n        # print 'dataMatrix====', dataMatrix \n        # print 'weights====', weights\n        # n*3   *  3*1  = n*1\n        h = sigmoid(dataMatrix * weights)  # \u77e9\u9635\u4e58\u6cd5\n        # print 'hhhhhhh====', h\n        # labelMat\u662f\u5b9e\u9645\u503c\n        error = (labelMat - h)  # \u5411\u91cf\u76f8\u51cf\n        # 0.001* (3*m)*(m*1) \u8868\u793a\u5728\u6bcf\u4e00\u4e2a\u5217\u4e0a\u7684\u4e00\u4e2a\u8bef\u5dee\u60c5\u51b5\uff0c\u6700\u540e\u5f97\u51fa x1,x2,xn\u7684\u7cfb\u6570\u7684\u504f\u79fb\u91cf\n        weights = weights + alpha * dataMatrix.transpose() * error  # \u77e9\u9635\u4e58\u6cd5\uff0c\u6700\u540e\u5f97\u5230\u56de\u5f52\u7cfb\u6570\n    return array(weights)\n\n\n# \u968f\u673a\u68af\u5ea6\u4e0b\u964d\n# \u68af\u5ea6\u4e0b\u964d\u4f18\u5316\u7b97\u6cd5\u5728\u6bcf\u6b21\u66f4\u65b0\u6570\u636e\u96c6\u65f6\u90fd\u9700\u8981\u904d\u5386\u6574\u4e2a\u6570\u636e\u96c6\uff0c\u8ba1\u7b97\u590d\u6742\u90fd\u8f83\u9ad8\n# \u968f\u673a\u68af\u5ea6\u4e0b\u964d\u4e00\u6b21\u53ea\u7528\u4e00\u4e2a\u6837\u672c\u70b9\u6765\u66f4\u65b0\u56de\u5f52\u7cfb\u6570\ndef stocGradAscent0(dataMatrix, classLabels):\n    '''\n    Desc:\n        \u968f\u673a\u68af\u5ea6\u4e0b\u964d\uff0c\u53ea\u4f7f\u7528\u4e00\u4e2a\u6837\u672c\u70b9\u6765\u66f4\u65b0\u56de\u5f52\u7cfb\u6570\n    Args:\n        dataMatrix -- \u8f93\u5165\u6570\u636e\u7684\u6570\u636e\u7279\u5f81\uff08\u9664\u53bb\u6700\u540e\u4e00\u5217\uff09\n        classLabels -- \u8f93\u5165\u6570\u636e\u7684\u7c7b\u522b\u6807\u7b7e\uff08\u6700\u540e\u4e00\u5217\u6570\u636e\uff09\n    Returns:\n        weights -- \u5f97\u5230\u7684\u6700\u4f73\u56de\u5f52\u7cfb\u6570\n    '''\n    m, n = shape(dataMatrix)\n    alpha = 0.01\n    # n*1\u7684\u77e9\u9635\n    # \u51fd\u6570ones\u521b\u5efa\u4e00\u4e2a\u51681\u7684\u6570\u7ec4\n    weights = ones(n)  # \u521d\u59cb\u5316\u957f\u5ea6\u4e3an\u7684\u6570\u7ec4\uff0c\u5143\u7d20\u5168\u90e8\u4e3a 1\n    for i in range(m):\n        # sum(dataMatrix[i]*weights)\u4e3a\u4e86\u6c42 f(x)\u7684\u503c\uff0c f(x)=a1*x1+b2*x2+..+nn*xn,\u6b64\u5904\u6c42\u51fa\u7684 h \u662f\u4e00\u4e2a\u5177\u4f53\u7684\u6570\u503c\uff0c\u800c\u4e0d\u662f\u4e00\u4e2a\u77e9\u9635\n        h = sigmoid(sum(dataMatrix[i] * weights))\n        # print 'dataMatrix[i]===', dataMatrix[i]\n        # \u8ba1\u7b97\u771f\u5b9e\u7c7b\u522b\u4e0e\u9884\u6d4b\u7c7b\u522b\u4e4b\u95f4\u7684\u5dee\u503c\uff0c\u7136\u540e\u6309\u7167\u8be5\u5dee\u503c\u8c03\u6574\u56de\u5f52\u7cfb\u6570\n        error = classLabels[i] - h\n        # 0.01*(1*1)*(1*n)\n        # print weights, \"*\" * 10, dataMatrix[i], \"*\" * 10, error\n        weights = weights + alpha * error * dataMatrix[i]\n    return weights\n\n\n# \u968f\u673a\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5\uff08\u968f\u673a\u5316\uff09\ndef stocGradAscent1(dataMatrix, classLabels, numIter=150):\n    '''\n    Desc:\n        \u6539\u8fdb\u7248\u7684\u968f\u673a\u68af\u5ea6\u4e0b\u964d\uff0c\u4f7f\u7528\u968f\u673a\u7684\u4e00\u4e2a\u6837\u672c\u6765\u66f4\u65b0\u56de\u5f52\u7cfb\u6570\n    Args:\n        dataMatrix -- \u8f93\u5165\u6570\u636e\u7684\u6570\u636e\u7279\u5f81\uff08\u9664\u53bb\u6700\u540e\u4e00\u5217\u6570\u636e\uff09\n        classLabels -- \u8f93\u5165\u6570\u636e\u7684\u7c7b\u522b\u6807\u7b7e\uff08\u6700\u540e\u4e00\u5217\u6570\u636e\uff09\n        numIter=150 --  \u8fed\u4ee3\u6b21\u6570\n    Returns:\n        weights -- \u5f97\u5230\u7684\u6700\u4f73\u56de\u5f52\u7cfb\u6570\n    '''\n    m, n = shape(dataMatrix)\n    weights = ones(n)  # \u521b\u5efa\u4e0e\u5217\u6570\u76f8\u540c\u7684\u77e9\u9635\u7684\u7cfb\u6570\u77e9\u9635\uff0c\u6240\u6709\u7684\u5143\u7d20\u90fd\u662f1\n    # \u968f\u673a\u68af\u5ea6, \u5faa\u73af150,\u89c2\u5bdf\u662f\u5426\u6536\u655b\n    for j in range(numIter):\n        # [0, 1, 2 .. m-1]\n        dataIndex = range(m)\n        for i in range(m):\n            # i\u548cj\u7684\u4e0d\u65ad\u589e\u5927\uff0c\u5bfc\u81f4alpha\u7684\u503c\u4e0d\u65ad\u51cf\u5c11\uff0c\u4f46\u662f\u4e0d\u4e3a0\n            alpha = 4 / (\n                1.0 + j + i\n            ) + 0.0001  # alpha \u4f1a\u968f\u7740\u8fed\u4ee3\u4e0d\u65ad\u51cf\u5c0f\uff0c\u4f46\u6c38\u8fdc\u4e0d\u4f1a\u51cf\u5c0f\u52300\uff0c\u56e0\u4e3a\u540e\u8fb9\u8fd8\u6709\u4e00\u4e2a\u5e38\u6570\u98790.0001\n            # \u968f\u673a\u4ea7\u751f\u4e00\u4e2a 0\uff5elen()\u4e4b\u95f4\u7684\u4e00\u4e2a\u503c\n            # random.uniform(x, y) \u65b9\u6cd5\u5c06\u968f\u673a\u751f\u6210\u4e0b\u4e00\u4e2a\u5b9e\u6570\uff0c\u5b83\u5728[x,y]\u8303\u56f4\u5185,x\u662f\u8fd9\u4e2a\u8303\u56f4\u5185\u7684\u6700\u5c0f\u503c\uff0cy\u662f\u8fd9\u4e2a\u8303\u56f4\u5185\u7684\u6700\u5927\u503c\u3002\n            randIndex = int(random.uniform(0, len(dataIndex)))\n            # sum(dataMatrix[i]*weights)\u4e3a\u4e86\u6c42 f(x)\u7684\u503c\uff0c f(x)=a1*x1+b2*x2+..+nn*xn\n            h = sigmoid(sum(dataMatrix[dataIndex[randIndex]] * weights))\n            error = classLabels[dataIndex[randIndex]] - h\n            # print weights, '__h=%s' % h, '__'*20, alpha, '__'*20, error, '__'*20, dataMatrix[randIndex]\n            weights = weights + alpha * error * dataMatrix[dataIndex[randIndex]]\n            del (dataIndex[randIndex])\n    return weights\n\n\n# \u53ef\u89c6\u5316\u5c55\u793a\ndef plotBestFit(dataArr, labelMat, weights):\n    '''\n        Desc:\n            \u5c06\u6211\u4eec\u5f97\u5230\u7684\u6570\u636e\u53ef\u89c6\u5316\u5c55\u793a\u51fa\u6765\n        Args:\n            dataArr:\u6837\u672c\u6570\u636e\u7684\u7279\u5f81\n            labelMat:\u6837\u672c\u6570\u636e\u7684\u7c7b\u522b\u6807\u7b7e\uff0c\u5373\u76ee\u6807\u53d8\u91cf\n            weights:\u56de\u5f52\u7cfb\u6570\n        Returns:\n            None\n    '''\n\n    n = shape(dataArr)[0]\n    xcord1 = []\n    ycord1 = []\n    xcord2 = []\n    ycord2 = []\n    for i in range(n):\n        if int(labelMat[i]) == 1:\n            xcord1.append(dataArr[i, 1])\n            ycord1.append(dataArr[i, 2])\n        else:\n            xcord2.append(dataArr[i, 1])\n            ycord2.append(dataArr[i, 2])\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    ax.scatter(xcord1, ycord1, s=30, c='red', marker='s')\n    ax.scatter(xcord2, ycord2, s=30, c='green')\n    x = arange(-3.0, 3.0, 0.1)\n    \"\"\"\n    y\u7684\u7531\u6765\uff0c\u5367\u69fd\uff0c\u662f\u4e0d\u662f\u6ca1\u770b\u61c2\uff1f\n    \u9996\u5148\u7406\u8bba\u4e0a\u662f\u8fd9\u4e2a\u6837\u5b50\u7684\u3002\n    dataMat.append([1.0, float(lineArr[0]), float(lineArr[1])])\n    w0*x0+w1*x1+w2*x2=f(x)\n    x0\u6700\u5f00\u59cb\u5c31\u8bbe\u7f6e\u4e3a1\u53fb\uff0c x2\u5c31\u662f\u6211\u4eec\u753b\u56fe\u7684y\u503c\uff0c\u800cf(x)\u88ab\u6211\u4eec\u78e8\u5408\u8bef\u5dee\u7ed9\u7b97\u5230w0,w1,w2\u8eab\u4e0a\u53bb\u4e86\n    \u6240\u4ee5:  w0+w1*x+w2*y=0 => y = (-w0-w1*x)/w2   \n    \"\"\"\n    y = (-weights[0] - weights[1] * x) / weights[2]\n    ax.plot(x, y)\n    plt.xlabel('X')\n    plt.ylabel('Y')\n    plt.show()\n\n\ndef simpleTest():\n    # 1.\u6536\u96c6\u5e76\u51c6\u5907\u6570\u636e\n    dataMat, labelMat = loadDataSet(\"data/5.Logistic/TestSet.txt\")\n\n    # print dataMat, '---\\n', labelMat\n    # 2.\u8bad\u7ec3\u6a21\u578b\uff0c  f(x)=a1*x1+b2*x2+..+nn*xn\u4e2d (a1,b2, .., nn).T\u7684\u77e9\u9635\u503c\n    # \u56e0\u4e3a\u6570\u7ec4\u6ca1\u6709\u662f\u590d\u5236n\u4efd\uff0c array\u7684\u4e58\u6cd5\u5c31\u662f\u4e58\u6cd5\n    dataArr = array(dataMat)\n    # print dataArr\n    # weights = gradAscent(dataArr, labelMat)\n    # weights = stocGradAscent0(dataArr, labelMat)\n    weights = stocGradAscent1(dataArr, labelMat)\n    # print '*'*30, weights\n\n    # \u6570\u636e\u53ef\u89c6\u5316\n    plotBestFit(dataArr, labelMat, weights)\n\n\n# --------------------------------------------------------------------------------\n# \u4ece\u759d\u6c14\u75c5\u75c7\u9884\u6d4b\u75c5\u9a6c\u7684\u6b7b\u4ea1\u7387\n# \u5206\u7c7b\u51fd\u6570\uff0c\u6839\u636e\u56de\u5f52\u7cfb\u6570\u548c\u7279\u5f81\u5411\u91cf\u6765\u8ba1\u7b97 Sigmoid\u7684\u503c\ndef classifyVector(inX, weights):\n    '''\n    Desc: \n        \u6700\u7ec8\u7684\u5206\u7c7b\u51fd\u6570\uff0c\u6839\u636e\u56de\u5f52\u7cfb\u6570\u548c\u7279\u5f81\u5411\u91cf\u6765\u8ba1\u7b97 Sigmoid \u7684\u503c\uff0c\u5927\u4e8e0.5\u51fd\u6570\u8fd4\u56de1\uff0c\u5426\u5219\u8fd4\u56de0\n    Args:\n        inX -- \u7279\u5f81\u5411\u91cf\uff0cfeatures\n        weights -- \u6839\u636e\u68af\u5ea6\u4e0b\u964d/\u968f\u673a\u68af\u5ea6\u4e0b\u964d \u8ba1\u7b97\u5f97\u5230\u7684\u56de\u5f52\u7cfb\u6570\n    Returns:\n        \u5982\u679c prob \u8ba1\u7b97\u5927\u4e8e 0.5 \u51fd\u6570\u8fd4\u56de 1\n        \u5426\u5219\u8fd4\u56de 0\n    '''\n    prob = sigmoid(sum(inX * weights))\n    if prob > 0.5: return 1.0\n    else: return 0.0\n\n\n# \u6253\u5f00\u6d4b\u8bd5\u96c6\u548c\u8bad\u7ec3\u96c6,\u5e76\u5bf9\u6570\u636e\u8fdb\u884c\u683c\u5f0f\u5316\u5904\u7406\ndef colicTest():\n    '''\n    Desc:\n        \u6253\u5f00\u6d4b\u8bd5\u96c6\u548c\u8bad\u7ec3\u96c6\uff0c\u5e76\u5bf9\u6570\u636e\u8fdb\u884c\u683c\u5f0f\u5316\u5904\u7406\n    Args:\n        None\n    Returns:\n        errorRate -- \u5206\u7c7b\u9519\u8bef\u7387\n    '''\n    frTrain = open('data/5.Logistic/horseColicTraining.txt')\n    frTest = open('data/5.Logistic/horseColicTest.txt')\n    trainingSet = []\n    trainingLabels = []\n    # \u89e3\u6790\u8bad\u7ec3\u6570\u636e\u96c6\u4e2d\u7684\u6570\u636e\u7279\u5f81\u548cLabels\n    # trainingSet \u4e2d\u5b58\u50a8\u8bad\u7ec3\u6570\u636e\u96c6\u7684\u7279\u5f81\uff0ctrainingLabels \u5b58\u50a8\u8bad\u7ec3\u6570\u636e\u96c6\u7684\u6837\u672c\u5bf9\u5e94\u7684\u5206\u7c7b\u6807\u7b7e\n    for line in frTrain.readlines():\n        currLine = line.strip().split('\\t')\n        lineArr = []\n        for i in range(21):\n            lineArr.append(float(currLine[i]))\n        trainingSet.append(lineArr)\n        trainingLabels.append(float(currLine[21]))\n    # \u4f7f\u7528 \u6539\u8fdb\u540e\u7684 \u968f\u673a\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5 \u6c42\u5f97\u5728\u6b64\u6570\u636e\u96c6\u4e0a\u7684\u6700\u4f73\u56de\u5f52\u7cfb\u6570 trainWeights\n    trainWeights = stocGradAscent1(array(trainingSet), trainingLabels, 500)\n    # trainWeights = stocGradAscent0(array(trainingSet), trainingLabels)\n    errorCount = 0\n    numTestVec = 0.0\n    # \u8bfb\u53d6 \u6d4b\u8bd5\u6570\u636e\u96c6 \u8fdb\u884c\u6d4b\u8bd5\uff0c\u8ba1\u7b97\u5206\u7c7b\u9519\u8bef\u7684\u6837\u672c\u6761\u6570\u548c\u6700\u7ec8\u7684\u9519\u8bef\u7387\n    for line in frTest.readlines():\n        numTestVec += 1.0\n        currLine = line.strip().split('\\t')\n        lineArr = []\n        for i in range(21):\n            lineArr.append(float(currLine[i]))\n        if int(classifyVector(array(lineArr), trainWeights)) != int(\n                currLine[21]):\n            errorCount += 1\n    errorRate = (float(errorCount) / numTestVec)\n    print(\"the error rate of this test is: %f\" % errorRate)\n    return errorRate\n\n\n# \u8c03\u7528 colicTest() 10\u6b21\u5e76\u6c42\u7ed3\u679c\u7684\u5e73\u5747\u503c\ndef multiTest():\n    numTests = 10\n    errorSum = 0.0\n    for k in range(numTests):\n        errorSum += colicTest()\n    print(\"after %d iterations the average error rate is: %f\" % (numTests, errorSum / float(numTests)))\n\n\nif __name__ == \"__main__\":\n    simpleTest()\n    # multiTest()\n", "src/py2.x/ml/5.Logistic/sklearn_logisticRegression_demo.py": "#!/usr/bin/python\n# coding: utf8\n\n'''\nCreated on Oct 27, 2010\nUpdate  on 2017-05-18\nLogistic Regression Working Module\nAuthor: \u5c0f\u7476\nGitHub: https://github.com/apachecn/AiLearning\nscikit-learn\u7684\u4f8b\u5b50\u5730\u5740: http://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n'''\n\n# \u903b\u8f91\u56de\u5f52\u4e2d\u7684 L1 \u60e9\u7f5a\u548c\u7a00\u7f3a\u6027 L1 Penalty and Sparsity in Logistic Regression\n'''\nprint(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import datasets\nfrom sklearn.preprocessing import StandardScaler\n\ndigits = datasets.load_digits()\n\nX, y = digits.data, digits.target\nX = StandardScaler().fit_transform(X)\n\n# \u5c06\u5927\u5c0f\u6570\u5b57\u5206\u7c7b\u4e3a\u5c0f\ny = (y > 4).astype(np.int)\n\n\n# \u8bbe\u7f6e\u6b63\u5219\u5316\u53c2\u6570\nfor i, C in enumerate((100, 1, 0.01)):\n    # \u51cf\u5c11\u8bad\u7ec3\u65f6\u95f4\u77ed\u7684\u5bb9\u5fcd\u5ea6\n    clf_l1_LR = LogisticRegression(C=C, penalty='l1', tol=0.01)\n    clf_l2_LR = LogisticRegression(C=C, penalty='l2', tol=0.01)\n    clf_l1_LR.fit(X, y)\n    clf_l2_LR.fit(X, y)\n\n    coef_l1_LR = clf_l1_LR.coef_.ravel()\n    coef_l2_LR = clf_l2_LR.coef_.ravel()\n\n    # coef_l1_LR contains zeros due to the\n    # L1 sparsity inducing norm\n    # \u7531\u4e8e L1 \u7a00\u758f\u8bf1\u5bfc\u89c4\u8303\uff0ccoef_l1_LR \u5305\u542b\u96f6\n\n    sparsity_l1_LR = np.mean(coef_l1_LR == 0) * 100\n    sparsity_l2_LR = np.mean(coef_l2_LR == 0) * 100\n\n    print(\"C=%.2f\" % C)\n    print(\"Sparsity with L1 penalty: %.2f%%\" % sparsity_l1_LR)\n    print(\"score with L1 penalty: %.4f\" % clf_l1_LR.score(X, y))\n    print(\"Sparsity with L2 penalty: %.2f%%\" % sparsity_l2_LR)\n    print(\"score with L2 penalty: %.4f\" % clf_l2_LR.score(X, y))\n\n    l1_plot = plt.subplot(3, 2, 2 * i + 1)\n    l2_plot = plt.subplot(3, 2, 2 * (i + 1))\n    if i == 0:\n        l1_plot.set_title(\"L1 penalty\")\n        l2_plot.set_title(\"L2 penalty\")\n\n    l1_plot.imshow(np.abs(coef_l1_LR.reshape(8, 8)), interpolation='nearest',\n                   cmap='binary', vmax=1, vmin=0)\n    l2_plot.imshow(np.abs(coef_l2_LR.reshape(8, 8)), interpolation='nearest',\n                   cmap='binary', vmax=1, vmin=0)\n    plt.text(-8, 3, \"C = %.2f\" % C)\n\n    l1_plot.set_xticks(())\n    l1_plot.set_yticks(())\n    l2_plot.set_xticks(())\n    l2_plot.set_yticks(())\n\nplt.show()\n'''\n\n# \u5177\u6709 L1-\u903b\u8f91\u56de\u5f52\u7684\u8def\u5f84\n'''\nprint(__doc__)\n\nfrom datetime import datetime\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn import linear_model\nfrom sklearn import datasets\nfrom sklearn.svm import l1_min_c\n\niris = datasets.load_iris()\nX = iris.data\ny = iris.target\n\nX = X[y != 2]\ny = y[y != 2]\n\nX -= np.mean(X, 0)\n\ncs = l1_min_c(X, y, loss='log') * np.logspace(0, 3)\n\n\nprint(\"Computing regularization path ...\")\nstart = datetime.now()\nclf = linear_model.LogisticRegression(C=1.0, penalty='l1', tol=1e-6)\ncoefs_ = []\nfor c in cs:\n    clf.set_params(C=c)\n    clf.fit(X, y)\n    coefs_.append(clf.coef_.ravel().copy())\nprint(\"This took \", datetime.now() - start)\n\ncoefs_ = np.array(coefs_)\nplt.plot(np.log10(cs), coefs_)\nymin, ymax = plt.ylim()\nplt.xlabel('log(C)')\nplt.ylabel('Coefficients')\nplt.title('Logistic Regression Path')\nplt.axis('tight')\nplt.show()\n'''\n\n# \u7ed8\u5236\u591a\u9879\u5f0f\u548c\u4e00\u5bf9\u4e8c\u7684\u903b\u8f91\u56de\u5f52 Plot multinomial and One-vs-Rest Logistic Regression\n'''\nprint(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_blobs\nfrom sklearn.linear_model import LogisticRegression\n\n# \u5236\u4f5c 3 \u7c7b\u6570\u636e\u96c6\u8fdb\u884c\u5206\u7c7b\ncenters = [[-5, 0], [0, 1.5], [5, -1]]\nX, y = make_blobs(n_samples=1000, centers=centers, random_state=40)\ntransformation = [[0.4, 0.2], [-0.4, 1.2]]\nX = np.dot(X, transformation)\n\nfor multi_class in ('multinomial', 'ovr'):\n    clf = LogisticRegression(solver='sag', max_iter=100, random_state=42,\n                             multi_class=multi_class).fit(X, y)\n\n    # \u6253\u5370\u8bad\u7ec3\u5206\u6570\n    print(\"training score : %.3f (%s)\" % (clf.score(X, y), multi_class))\n\n    # \u521b\u5efa\u4e00\u4e2a\u7f51\u683c\u6765\u7ed8\u5236\n    h = .02  # \u7f51\u683c\u4e2d\u7684\u6b65\u957f\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                         np.arange(y_min, y_max, h))\n\n    # \u7ed8\u5236\u51b3\u7b56\u8fb9\u754c\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u5c06\u4e3a\u7f51\u683c [x_min, x_max]x[y_min, y_max]\u4e2d\u7684\u6bcf\u4e2a\u70b9\u5206\u914d\u4e00\u4e2a\u989c\u8272\u3002\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n    # \u5c06\u7ed3\u679c\u653e\u5165\u5f69\u8272\u56fe\n    Z = Z.reshape(xx.shape)\n    plt.figure()\n    plt.contourf(xx, yy, Z, cmap=plt.cm.Paired)\n    plt.title(\"Decision surface of LogisticRegression (%s)\" % multi_class)\n    plt.axis('tight')\n\n    # \u5c06\u8bad\u7ec3\u70b9\u4e5f\u7ed8\u5236\u8fdb\u5165\n    colors = \"bry\"\n    for i, color in zip(clf.classes_, colors):\n        idx = np.where(y == i)\n        plt.scatter(X[idx, 0], X[idx, 1], c=color, cmap=plt.cm.Paired)\n\n    # \u7ed8\u5236\u4e09\u4e2a\u4e00\u5bf9\u6570\u5206\u7c7b\u5668\n    xmin, xmax = plt.xlim()\n    ymin, ymax = plt.ylim()\n    coef = clf.coef_\n    intercept = clf.intercept_\n\n    def plot_hyperplane(c, color):\n        def line(x0):\n            return (-(x0 * coef[c, 0]) - intercept[c]) / coef[c, 1]\n        plt.plot([xmin, xmax], [line(xmin), line(xmax)],\n                 ls=\"--\", color=color)\n\n    for i, color in zip(clf.classes_, colors):\n        plot_hyperplane(i, color)\n\nplt.show()\n'''\nfrom __future__ import print_function\n\n# Logistic Regression 3-class Classifier \u903b\u8f91\u56de\u5f52 3-\u7c7b \u5206\u7c7b\u5668 \n\nprint(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import linear_model, datasets\n\n# \u5f15\u5165\u4e00\u4e9b\u6570\u636e\u6765\u73a9\niris = datasets.load_iris()\n# \u6211\u4eec\u53ea\u91c7\u7528\u6837\u672c\u6570\u636e\u7684\u524d\u4e24\u4e2afeature\nX = iris.data[:, :2]  \nY = iris.target\n\nh = .02  # \u7f51\u683c\u4e2d\u7684\u6b65\u957f\n\nlogreg = linear_model.LogisticRegression(C=1e5)\n\n# \u6211\u4eec\u521b\u5efa\u4e86\u4e00\u4e2a Neighbours Classifier \u7684\u5b9e\u4f8b\uff0c\u5e76\u62df\u5408\u6570\u636e\u3002\nlogreg.fit(X, Y)\n\n# \u7ed8\u5236\u51b3\u7b56\u8fb9\u754c\u3002\u4e3a\u6b64\u6211\u4eec\u5c06\u4e3a\u7f51\u683c [x_min, x_max]x[y_min, y_max] \u4e2d\u7684\u6bcf\u4e2a\u70b9\u5206\u914d\u4e00\u4e2a\u989c\u8272\u3002\nx_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\ny_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\nZ = logreg.predict(np.c_[xx.ravel(), yy.ravel()])\n\n# \u5c06\u7ed3\u679c\u653e\u5165\u5f69\u8272\u56fe\u4e2d\nZ = Z.reshape(xx.shape)\nplt.figure(1, figsize=(4, 3))\nplt.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired)\n\n# \u5c06\u8bad\u7ec3\u70b9\u4e5f\u540c\u6837\u653e\u5165\u5f69\u8272\u56fe\u4e2d\nplt.scatter(X[:, 0], X[:, 1], c=Y, edgecolors='k', cmap=plt.cm.Paired)\nplt.xlabel('Sepal length')\nplt.ylabel('Sepal width')\n\nplt.xlim(xx.min(), xx.max())\nplt.ylim(yy.min(), yy.max())\nplt.xticks(())\nplt.yticks(())\n\nplt.show()\n\n# Logistic function \u903b\u8f91\u56de\u5f52\u51fd\u6570\n# \u8fd9\u4e2a\u7c7b\u4f3c\u4e8e\u54b1\u4eec\u4e4b\u524d\u8bb2\u89e3 logistic \u56de\u5f52\u7684 Sigmoid \u51fd\u6570\uff0c\u6a21\u62df\u7684\u9636\u8dc3\u51fd\u6570\n\n'''\nprint(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn import linear_model\n\n# \u8fd9\u662f\u6211\u4eec\u7684\u6d4b\u8bd5\u96c6\uff0c\u5b83\u53ea\u662f\u4e00\u6761\u76f4\u7ebf\uff0c\u5e26\u6709\u4e00\u4e9b\u9ad8\u65af\u566a\u58f0\u3002\nxmin, xmax = -5, 5\nn_samples = 100\nnp.random.seed(0)\nX = np.random.normal(size=n_samples)\ny = (X > 0).astype(np.float)\nX[X > 0] *= 4\nX += .3 * np.random.normal(size=n_samples)\n\nX = X[:, np.newaxis]\n# \u8fd0\u884c\u5206\u7c7b\u5668\nclf = linear_model.LogisticRegression(C=1e5)\nclf.fit(X, y)\n\n# \u5e76\u4e14\u753b\u51fa\u6211\u4eec\u7684\u7ed3\u679c\nplt.figure(1, figsize=(4, 3))\nplt.clf()\nplt.scatter(X.ravel(), y, color='black', zorder=20)\nX_test = np.linspace(-5, 10, 300)\n\n\ndef model(x):\n    return 1 / (1 + np.exp(-x))\nloss = model(X_test * clf.coef_ + clf.intercept_).ravel()\nplt.plot(X_test, loss, color='red', linewidth=3)\n\nols = linear_model.LinearRegression()\nols.fit(X, y)\nplt.plot(X_test, ols.coef_ * X_test + ols.intercept_, linewidth=1)\nplt.axhline(.5, color='.5')\n\nplt.ylabel('y')\nplt.xlabel('X')\nplt.xticks(range(-5, 10))\nplt.yticks([0, 0.5, 1])\nplt.ylim(-.25, 1.25)\nplt.xlim(-4, 10)\nplt.legend(('Logistic Regression Model', 'Linear Regression Model'),\n           loc=\"lower right\", fontsize='small')\nplt.show()\n'''\n\n\n\n", "src/py2.x/dl/linear_unit.py": "#!/usr/bin/env python\n# -*- coding: UTF-8 -*-\n\n# \u5f15\u5165 Perceptron \u7c7b\nfrom __future__ import print_function\nfrom perceptron import Perceptron\n\n# \u5b9a\u4e49\u6fc0\u6d3b\u51fd\u6570 f\nf = lambda x: x\n\nclass LinearUnit(Perceptron):\n    '''\n    Desc:\n        \u7ebf\u6027\u5355\u5143\u7c7b\n    Args:\n        Perceptron \u2014\u2014 \u611f\u77e5\u5668\n    Returns:\n        None\n    '''\n    def __init__(self, input_num):\n        '''\n        Desc:\n            \u521d\u59cb\u5316\u7ebf\u6027\u5355\u5143\uff0c\u8bbe\u7f6e\u8f93\u5165\u53c2\u6570\u7684\u4e2a\u6570\n        Args:\n            input_num \u2014\u2014 \u8f93\u5165\u53c2\u6570\u7684\u4e2a\u6570\n        Returns:\n            None\n        '''\n        # \u521d\u59cb\u5316\u6211\u4eec\u7684\u611f\u77e5\u5668\u7c7b\uff0c\u8bbe\u7f6e\u8f93\u5165\u53c2\u6570\u7684\u4e2a\u6570 input_num \u548c \u6fc0\u6d3b\u51fd\u6570 f\n        Perceptron.__init__(self, input_num, f)\n\n# \u6784\u9020\u7b80\u5355\u7684\u6570\u636e\u96c6\ndef get_training_dataset():\n    '''\n    Desc:\n        \u6784\u5efa\u4e00\u4e2a\u7b80\u5355\u7684\u8bad\u7ec3\u6570\u636e\u96c6\n    Args:\n        None\n    Returns:\n        input_vecs \u2014\u2014 \u8bad\u7ec3\u6570\u636e\u96c6\u7684\u7279\u5f81\u90e8\u5206\n        labels \u2014\u2014 \u8bad\u7ec3\u6570\u636e\u96c6\u7684\u6570\u636e\u5bf9\u5e94\u7684\u6807\u7b7e\uff0c\u662f\u4e00\u4e00\u5bf9\u5e94\u7684\n    '''\n    # \u6784\u5efa\u6570\u636e\u96c6\uff0c\u8f93\u5165\u5411\u91cf\u5217\u8868\uff0c\u6bcf\u4e00\u9879\u662f\u5de5\u4f5c\u5e74\u9650\n    input_vecs = [[5], [3], [8], [1.4], [10.1]]\n    # \u671f\u671b\u7684\u8f93\u51fa\u5217\u8868\uff0c\u4e5f\u5c31\u662f\u8f93\u5165\u5411\u91cf\u7684\u5bf9\u5e94\u7684\u6807\u7b7e\uff0c\u4e0e\u5de5\u4f5c\u5e74\u9650\u5bf9\u5e94\u7684\u6536\u5165\u5e74\u85aa\n    labels = [5500, 2300, 7600, 1800, 11400]\n    return input_vecs, labels\n\n\n# \u4f7f\u7528\u6211\u4eec\u7684\u8bad\u7ec3\u6570\u636e\u96c6\u5bf9\u7ebf\u6027\u5355\u5143\u8fdb\u884c\u8bad\u7ec3\ndef train_linear_unit():\n    '''\n    Desc:\n        \u4f7f\u7528\u8bad\u7ec3\u6570\u636e\u96c6\u5bf9\u6211\u4eec\u7684\u7ebf\u6027\u5355\u5143\u8fdb\u884c\u8bad\u7ec3\n    Args:\n        None\n    Returns:\n        lu \u2014\u2014 \u8fd4\u56de\u8bad\u7ec3\u597d\u7684\u7ebf\u6027\u5355\u5143\n    '''\n    # \u521b\u5efa\u611f\u77e5\u5668\u5bf9\u8c61\uff0c\u8f93\u5165\u53c2\u6570\u7684\u4e2a\u6570\u4e5f\u5c31\u662f\u7279\u5f81\u6570\u4e3a 1\uff08\u5de5\u4f5c\u5e74\u9650\uff09\n    lu = LinearUnit(1)\n    # \u83b7\u53d6\u6784\u5efa\u7684\u6570\u636e\u96c6\n    input_vecs, labels = get_training_dataset()\n    # \u8bad\u7ec3\u611f\u77e5\u5668\uff0c\u8fed\u4ee3 10 \u8f6e\uff0c\u5b66\u4e60\u7387\u4e3a 0.01\n    lu.train(input_vecs, labels, 10, 0.01)\n    # \u8fd4\u56de\u8bad\u7ec3\u597d\u7684\u7ebf\u6027\u5355\u5143\n    return lu\n\n\n# \u5c06\u56fe\u50cf\u753b\u51fa\u6765\ndef plot(linear_unit):\n    '''\n    Desc:\n        \u5c06\u6211\u4eec\u8bad\u7ec3\u597d\u7684\u7ebf\u6027\u5355\u5143\u5bf9\u6570\u636e\u7684\u5206\u7c7b\u60c5\u51b5\u4f5c\u56fe\u753b\u51fa\u6765\n    Args:\n        linear_unit \u2014\u2014 \u8bad\u7ec3\u597d\u7684\u7ebf\u6027\u5355\u5143\n    Returns:\n        None\n    '''\n    # \u5f15\u5165\u7ed8\u56fe\u7684\u5e93\n    import matplotlib.pyplot as plt\n    # \u83b7\u53d6\u8bad\u7ec3\u6570\u636e: \u7279\u5f81 input_vecs \u4e0e \u5bf9\u5e94\u7684\u6807\u7b7e labels\n    input_vecs, labels = get_training_dataset()\n    # figure() \u521b\u5efa\u4e00\u4e2a Figure \u5bf9\u8c61\uff0c\u4e0e\u7528\u6237\u4ea4\u4e92\u7684\u6574\u4e2a\u7a97\u53e3\uff0c\u8fd9\u4e2a figure \u4e2d\u5bb9\u7eb3\u7740 subplots\n    fig = plt.figure()\n    # \u5728 figure \u5bf9\u8c61\u4e2d\u521b\u5efa 1\u884c1\u5217\u4e2d\u7684\u7b2c\u4e00\u4e2a\u56fe\n    ax = fig.add_subplot(111)\n    # scatter(x, y) \u7ed8\u5236\u6563\u70b9\u56fe\uff0c\u5176\u4e2d\u7684 x,y \u662f\u76f8\u540c\u957f\u5ea6\u7684\u6570\u7ec4\u5e8f\u5217\n    ax.scatter(map(lambda x: x[0], input_vecs), labels)\n    # \u8bbe\u7f6e\u6743\u91cd\n    weights = linear_unit.weights\n    # \u8bbe\u7f6e\u504f\u7f6e\u9879\n    bias = linear_unit.bias\n    # range(start, stop, step) \u4ece start \u5f00\u59cb\uff0c\u5230 stop \u7ed3\u675f\uff0c\u6b65\u957f\u4e3a step\n    x = range(0, 12, 1)\n    # \u8ba1\u7b97\u611f\u77e5\u5668\u5bf9\u8f93\u5165\u8ba1\u7b97\u5f97\u5230\u7684\u503c\n    y = map(lambda x: weights[0] * x + bias, x)\n    # \u5c06\u56fe\u753b\u51fa\u6765\n    ax.plot(x, y)\n    # \u5c06\u6700\u7ec8\u7684\u56fe\u5c55\u793a\u51fa\u6765\n    plt.show()\n\n\nif __name__ == '__main__':\n    '''\n    Desc:\n        main \u51fd\u6570\uff0c\u8bad\u7ec3\u6211\u4eec\u7684\u7ebf\u6027\u5355\u5143\uff0c\u5e76\u8fdb\u884c\u9884\u6d4b\n    Args:\n        None\n    Returns:\n        None\n    '''\n    # \u9996\u5148\u8bad\u7ec3\u6211\u4eec\u7684\u7ebf\u6027\u5355\u5143\n    linear_unit = train_linear_unit()\n    # \u6253\u5370\u8bad\u7ec3\u83b7\u5f97\u7684\u6743\u91cd \u548c \u504f\u7f6e\n    print(linear_unit)\n    # \u6d4b\u8bd5\n    print('Work 3.4 years, monthly salary = %.2f' % linear_unit.predict([3.4]))\n    print('Work 15 years, monthly salary = %.2f' % linear_unit.predict([15]))\n    print('Work 1.5 years, monthly salary = %.2f' % linear_unit.predict([1.5]))\n    print('Work 6.3 years, monthly salary = %.2f' % linear_unit.predict([6.3]))\n    plot(linear_unit)\n", "src/py2.x/dl/activators.py": "#!/usr/bin/env python\n# -*- coding: UTF-8 -*-\n\n\nimport numpy as np\n\n\nclass ReluActivator(object):\n    def forward(self, weighted_input):\n        #return weighted_input\n        return max(0, weighted_input)\n\n    def backward(self, output):\n        return 1 if output > 0 else 0\n\n\nclass IdentityActivator(object):\n    def forward(self, weighted_input):\n        return weighted_input\n\n    def backward(self, output):\n        return 1\n\n\nclass SigmoidActivator(object):\n    def forward(self, weighted_input):\n        return 1.0 / (1.0 + np.exp(-weighted_input))\n\n    def backward(self, output):\n        return output * (1 - output)\n\n\nclass TanhActivator(object):\n    def forward(self, weighted_input):\n        return 2.0 / (1.0 + np.exp(-2 * weighted_input)) - 1.0\n\n    def backward(self, output):\n        return 1 - output * output", "src/py2.x/dl/lstm.py": "#!/usr/bin/env python\n# -*- coding: UTF-8 -*-\n\n\nfrom __future__ import print_function\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom cnn import element_wise_op\nfrom activators import SigmoidActivator, TanhActivator, IdentityActivator\n\n\nclass LstmLayer(object):\n    def __init__(self, input_width, state_width, \n                 learning_rate):\n        self.input_width = input_width\n        self.state_width = state_width\n        self.learning_rate = learning_rate\n        # \u95e8\u7684\u6fc0\u6d3b\u51fd\u6570\n        self.gate_activator = SigmoidActivator()\n        # \u8f93\u51fa\u7684\u6fc0\u6d3b\u51fd\u6570\n        self.output_activator = TanhActivator()\n        # \u5f53\u524d\u65f6\u523b\u521d\u59cb\u5316\u4e3at0\n        self.times = 0       \n        # \u5404\u4e2a\u65f6\u523b\u7684\u5355\u5143\u72b6\u6001\u5411\u91cfc\n        self.c_list = self.init_state_vec()\n        # \u5404\u4e2a\u65f6\u523b\u7684\u8f93\u51fa\u5411\u91cfh\n        self.h_list = self.init_state_vec()\n        # \u5404\u4e2a\u65f6\u523b\u7684\u9057\u5fd8\u95e8f\n        self.f_list = self.init_state_vec()\n        # \u5404\u4e2a\u65f6\u523b\u7684\u8f93\u5165\u95e8i\n        self.i_list = self.init_state_vec()\n        # \u5404\u4e2a\u65f6\u523b\u7684\u8f93\u51fa\u95e8o\n        self.o_list = self.init_state_vec()\n        # \u5404\u4e2a\u65f6\u523b\u7684\u5373\u65f6\u72b6\u6001c~\n        self.ct_list = self.init_state_vec()\n        # \u9057\u5fd8\u95e8\u6743\u91cd\u77e9\u9635Wfh, Wfx, \u504f\u7f6e\u9879bf\n        self.Wfh, self.Wfx, self.bf = (\n            self.init_weight_mat())\n        # \u8f93\u5165\u95e8\u6743\u91cd\u77e9\u9635Wfh, Wfx, \u504f\u7f6e\u9879bf\n        self.Wih, self.Wix, self.bi = (\n            self.init_weight_mat())\n        # \u8f93\u51fa\u95e8\u6743\u91cd\u77e9\u9635Wfh, Wfx, \u504f\u7f6e\u9879bf\n        self.Woh, self.Wox, self.bo = (\n            self.init_weight_mat())\n        # \u5355\u5143\u72b6\u6001\u6743\u91cd\u77e9\u9635Wfh, Wfx, \u504f\u7f6e\u9879bf\n        self.Wch, self.Wcx, self.bc = (\n            self.init_weight_mat())\n\n    def init_state_vec(self):\n        '''\n        \u521d\u59cb\u5316\u4fdd\u5b58\u72b6\u6001\u7684\u5411\u91cf\n        '''\n        state_vec_list = []\n        state_vec_list.append(np.zeros(\n            (self.state_width, 1)))\n        return state_vec_list\n\n    def init_weight_mat(self):\n        '''\n        \u521d\u59cb\u5316\u6743\u91cd\u77e9\u9635\n        '''\n        Wh = np.random.uniform(-1e-4, 1e-4,\n            (self.state_width, self.state_width))\n        Wx = np.random.uniform(-1e-4, 1e-4,\n            (self.state_width, self.input_width))\n        b = np.zeros((self.state_width, 1))\n        return Wh, Wx, b\n\n    def forward(self, x):\n        '''\n        \u6839\u636e\u5f0f1-\u5f0f6\u8fdb\u884c\u524d\u5411\u8ba1\u7b97\n        '''\n        self.times += 1\n        # \u9057\u5fd8\u95e8\n        fg = self.calc_gate(x, self.Wfx, self.Wfh, \n            self.bf, self.gate_activator)\n        self.f_list.append(fg)\n        # \u8f93\u5165\u95e8\n        ig = self.calc_gate(x, self.Wix, self.Wih,\n            self.bi, self.gate_activator)\n        self.i_list.append(ig)\n        # \u8f93\u51fa\u95e8\n        og = self.calc_gate(x, self.Wox, self.Woh,\n            self.bo, self.gate_activator)\n        self.o_list.append(og)\n        # \u5373\u65f6\u72b6\u6001\n        ct = self.calc_gate(x, self.Wcx, self.Wch,\n            self.bc, self.output_activator)\n        self.ct_list.append(ct)\n        # \u5355\u5143\u72b6\u6001\n        c = fg * self.c_list[self.times - 1] + ig * ct\n        self.c_list.append(c)\n        # \u8f93\u51fa\n        h = og * self.output_activator.forward(c)\n        self.h_list.append(h)\n\n    def calc_gate(self, x, Wx, Wh, b, activator):\n        '''\n        \u8ba1\u7b97\u95e8\n        '''\n        h = self.h_list[self.times - 1] # \u4e0a\u6b21\u7684LSTM\u8f93\u51fa\n        net = np.dot(Wh, h) + np.dot(Wx, x) + b\n        gate = activator.forward(net)\n        return gate\n\n\n    def backward(self, x, delta_h, activator):\n        '''\n        \u5b9e\u73b0LSTM\u8bad\u7ec3\u7b97\u6cd5\n        '''\n        self.calc_delta(delta_h, activator)\n        self.calc_gradient(x)\n\n    def update(self):\n        '''\n        \u6309\u7167\u68af\u5ea6\u4e0b\u964d\uff0c\u66f4\u65b0\u6743\u91cd\n        '''\n        self.Wfh -= self.learning_rate * self.Whf_grad\n        self.Wfx -= self.learning_rate * self.Whx_grad\n        self.bf -= self.learning_rate * self.bf_grad\n        self.Wih -= self.learning_rate * self.Whi_grad\n        self.Wix -= self.learning_rate * self.Whi_grad\n        self.bi -= self.learning_rate * self.bi_grad\n        self.Woh -= self.learning_rate * self.Wof_grad\n        self.Wox -= self.learning_rate * self.Wox_grad\n        self.bo -= self.learning_rate * self.bo_grad\n        self.Wch -= self.learning_rate * self.Wcf_grad\n        self.Wcx -= self.learning_rate * self.Wcx_grad\n        self.bc -= self.learning_rate * self.bc_grad\n\n    def calc_delta(self, delta_h, activator):\n        # \u521d\u59cb\u5316\u5404\u4e2a\u65f6\u523b\u7684\u8bef\u5dee\u9879\n        self.delta_h_list = self.init_delta()  # \u8f93\u51fa\u8bef\u5dee\u9879\n        self.delta_o_list = self.init_delta()  # \u8f93\u51fa\u95e8\u8bef\u5dee\u9879\n        self.delta_i_list = self.init_delta()  # \u8f93\u5165\u95e8\u8bef\u5dee\u9879\n        self.delta_f_list = self.init_delta()  # \u9057\u5fd8\u95e8\u8bef\u5dee\u9879\n        self.delta_ct_list = self.init_delta() # \u5373\u65f6\u8f93\u51fa\u8bef\u5dee\u9879\n\n        # \u4fdd\u5b58\u4ece\u4e0a\u4e00\u5c42\u4f20\u9012\u4e0b\u6765\u7684\u5f53\u524d\u65f6\u523b\u7684\u8bef\u5dee\u9879\n        self.delta_h_list[-1] = delta_h\n        \n        # \u8fed\u4ee3\u8ba1\u7b97\u6bcf\u4e2a\u65f6\u523b\u7684\u8bef\u5dee\u9879\n        for k in range(self.times, 0, -1):\n            self.calc_delta_k(k)\n\n    def init_delta(self):\n        '''\n        \u521d\u59cb\u5316\u8bef\u5dee\u9879\n        '''\n        delta_list = []\n        for i in range(self.times + 1):\n            delta_list.append(np.zeros(\n                (self.state_width, 1)))\n        return delta_list\n\n    def calc_delta_k(self, k):\n        '''\n        \u6839\u636ek\u65f6\u523b\u7684delta_h\uff0c\u8ba1\u7b97k\u65f6\u523b\u7684delta_f\u3001\n        delta_i\u3001delta_o\u3001delta_ct\uff0c\u4ee5\u53cak-1\u65f6\u523b\u7684delta_h\n        '''\n        # \u83b7\u5f97k\u65f6\u523b\u524d\u5411\u8ba1\u7b97\u7684\u503c\n        ig = self.i_list[k]\n        og = self.o_list[k]\n        fg = self.f_list[k]\n        ct = self.ct_list[k]\n        c = self.c_list[k]\n        c_prev = self.c_list[k-1]\n        tanh_c = self.output_activator.forward(c)\n        delta_k = self.delta_h_list[k]\n\n        # \u6839\u636e\u5f0f9\u8ba1\u7b97delta_o\n        delta_o = (delta_k * tanh_c * \n            self.gate_activator.backward(og))\n        delta_f = (delta_k * og * \n            (1 - tanh_c * tanh_c) * c_prev *\n            self.gate_activator.backward(fg))\n        delta_i = (delta_k * og * \n            (1 - tanh_c * tanh_c) * ct *\n            self.gate_activator.backward(ig))\n        delta_ct = (delta_k * og * \n            (1 - tanh_c * tanh_c) * ig *\n            self.output_activator.backward(ct))\n        delta_h_prev = (\n                np.dot(delta_o.transpose(), self.Woh) +\n                np.dot(delta_i.transpose(), self.Wih) +\n                np.dot(delta_f.transpose(), self.Wfh) +\n                np.dot(delta_ct.transpose(), self.Wch)\n            ).transpose()\n\n        # \u4fdd\u5b58\u5168\u90e8delta\u503c\n        self.delta_h_list[k-1] = delta_h_prev\n        self.delta_f_list[k] = delta_f\n        self.delta_i_list[k] = delta_i\n        self.delta_o_list[k] = delta_o\n        self.delta_ct_list[k] = delta_ct\n\n    def calc_gradient(self, x):\n        # \u521d\u59cb\u5316\u9057\u5fd8\u95e8\u6743\u91cd\u68af\u5ea6\u77e9\u9635\u548c\u504f\u7f6e\u9879\n        self.Wfh_grad, self.Wfx_grad, self.bf_grad = (\n            self.init_weight_gradient_mat())\n        # \u521d\u59cb\u5316\u8f93\u5165\u95e8\u6743\u91cd\u68af\u5ea6\u77e9\u9635\u548c\u504f\u7f6e\u9879\n        self.Wih_grad, self.Wix_grad, self.bi_grad = (\n            self.init_weight_gradient_mat())\n        # \u521d\u59cb\u5316\u8f93\u51fa\u95e8\u6743\u91cd\u68af\u5ea6\u77e9\u9635\u548c\u504f\u7f6e\u9879\n        self.Woh_grad, self.Wox_grad, self.bo_grad = (\n            self.init_weight_gradient_mat())\n        # \u521d\u59cb\u5316\u5355\u5143\u72b6\u6001\u6743\u91cd\u68af\u5ea6\u77e9\u9635\u548c\u504f\u7f6e\u9879\n        self.Wch_grad, self.Wcx_grad, self.bc_grad = (\n            self.init_weight_gradient_mat())\n\n       # \u8ba1\u7b97\u5bf9\u4e0a\u4e00\u6b21\u8f93\u51fah\u7684\u6743\u91cd\u68af\u5ea6\n        for t in range(self.times, 0, -1):\n            # \u8ba1\u7b97\u5404\u4e2a\u65f6\u523b\u7684\u68af\u5ea6\n            (Wfh_grad, bf_grad,\n            Wih_grad, bi_grad,\n            Woh_grad, bo_grad,\n            Wch_grad, bc_grad) = (\n                self.calc_gradient_t(t))\n            # \u5b9e\u9645\u68af\u5ea6\u662f\u5404\u65f6\u523b\u68af\u5ea6\u4e4b\u548c\n            self.Wfh_grad += Wfh_grad\n            self.bf_grad += bf_grad\n            self.Wih_grad += Wih_grad\n            self.bi_grad += bi_grad\n            self.Woh_grad += Woh_grad\n            self.bo_grad += bo_grad\n            self.Wch_grad += Wch_grad\n            self.bc_grad += bc_grad\n\n        # \u8ba1\u7b97\u5bf9\u672c\u6b21\u8f93\u5165x\u7684\u6743\u91cd\u68af\u5ea6\n        xt = x.transpose()\n        self.Wfx_grad = np.dot(self.delta_f_list[-1], xt)\n        self.Wix_grad = np.dot(self.delta_i_list[-1], xt)\n        self.Wox_grad = np.dot(self.delta_o_list[-1], xt)\n        self.Wcx_grad = np.dot(self.delta_ct_list[-1], xt)\n\n    def init_weight_gradient_mat(self):\n        '''\n        \u521d\u59cb\u5316\u6743\u91cd\u77e9\u9635\n        '''\n        Wh_grad = np.zeros((self.state_width,\n            self.state_width))\n        Wx_grad = np.zeros((self.state_width,\n            self.input_width))\n        b_grad = np.zeros((self.state_width, 1))\n        return Wh_grad, Wx_grad, b_grad\n\n    def calc_gradient_t(self, t):\n        '''\n        \u8ba1\u7b97\u6bcf\u4e2a\u65f6\u523bt\u6743\u91cd\u7684\u68af\u5ea6\n        '''\n        h_prev = self.h_list[t-1].transpose()\n        Wfh_grad = np.dot(self.delta_f_list[t], h_prev)\n        bf_grad = self.delta_f_list[t]\n        Wih_grad = np.dot(self.delta_i_list[t], h_prev)\n        bi_grad = self.delta_f_list[t]\n        Woh_grad = np.dot(self.delta_o_list[t], h_prev)\n        bo_grad = self.delta_f_list[t]\n        Wch_grad = np.dot(self.delta_ct_list[t], h_prev)\n        bc_grad = self.delta_ct_list[t]\n        return Wfh_grad, bf_grad, Wih_grad, bi_grad, \\\n               Woh_grad, bo_grad, Wch_grad, bc_grad\n\n    def reset_state(self):\n        # \u5f53\u524d\u65f6\u523b\u521d\u59cb\u5316\u4e3at0\n        self.times = 0       \n        # \u5404\u4e2a\u65f6\u523b\u7684\u5355\u5143\u72b6\u6001\u5411\u91cfc\n        self.c_list = self.init_state_vec()\n        # \u5404\u4e2a\u65f6\u523b\u7684\u8f93\u51fa\u5411\u91cfh\n        self.h_list = self.init_state_vec()\n        # \u5404\u4e2a\u65f6\u523b\u7684\u9057\u5fd8\u95e8f\n        self.f_list = self.init_state_vec()\n        # \u5404\u4e2a\u65f6\u523b\u7684\u8f93\u5165\u95e8i\n        self.i_list = self.init_state_vec()\n        # \u5404\u4e2a\u65f6\u523b\u7684\u8f93\u51fa\u95e8o\n        self.o_list = self.init_state_vec()\n        # \u5404\u4e2a\u65f6\u523b\u7684\u5373\u65f6\u72b6\u6001c~\n        self.ct_list = self.init_state_vec()\n\n\ndef data_set():\n    x = [np.array([[1], [2], [3]]),\n         np.array([[2], [3], [4]])]\n    d = np.array([[1], [2]])\n    return x, d\n\n\ndef gradient_check():\n    '''\n    \u68af\u5ea6\u68c0\u67e5\n    '''\n    # \u8bbe\u8ba1\u4e00\u4e2a\u8bef\u5dee\u51fd\u6570\uff0c\u53d6\u6240\u6709\u8282\u70b9\u8f93\u51fa\u9879\u4e4b\u548c\n    error_function = lambda o: o.sum()\n    \n    lstm = LstmLayer(3, 2, 1e-3)\n\n    # \u8ba1\u7b97forward\u503c\n    x, d = data_set()\n    lstm.forward(x[0])\n    lstm.forward(x[1])\n    \n    # \u6c42\u53d6sensitivity map\n    sensitivity_array = np.ones(lstm.h_list[-1].shape,\n                                dtype=np.float64)\n    # \u8ba1\u7b97\u68af\u5ea6\n    lstm.backward(x[1], sensitivity_array, IdentityActivator())\n    \n    # \u68c0\u67e5\u68af\u5ea6\n    epsilon = 10e-4\n    for i in range(lstm.Wfh.shape[0]):\n        for j in range(lstm.Wfh.shape[1]):\n            lstm.Wfh[i,j] += epsilon\n            lstm.reset_state()\n            lstm.forward(x[0])\n            lstm.forward(x[1])\n            err1 = error_function(lstm.h_list[-1])\n            lstm.Wfh[i,j] -= 2*epsilon\n            lstm.reset_state()\n            lstm.forward(x[0])\n            lstm.forward(x[1])\n            err2 = error_function(lstm.h_list[-1])\n            expect_grad = (err1 - err2) / (2 * epsilon)\n            lstm.Wfh[i,j] += epsilon\n            print('weights(%d,%d): expected - actural %.4e - %.4e' % (\n                i, j, expect_grad, lstm.Wfh_grad[i,j]))\n    return lstm\n\n\ndef test():\n    l = LstmLayer(3, 2, 1e-3)\n    x, d = data_set()\n    l.forward(x[0])\n    l.forward(x[1])\n    l.backward(x[1], d, IdentityActivator())\n    return l", "src/py2.x/dl/rnn.py": "#!/usr/bin/env python\n# -*- coding: UTF-8 -*-\n\nfrom __future__ import print_function\nimport numpy as np\nfrom cnn import element_wise_op\nfrom activators import ReluActivator, IdentityActivator\n\ntry:\n    reduce         # Python 2\nexcept NameError:  # Python 3\n    from functools import reduce\n\n\nclass RecurrentLayer(object):\n    '''\n    Desc:\n        \u7528 RecurrentLayer \u7c7b\u6765\u5b9e\u73b0\u4e00\u4e2a\u5faa\u73af\u5c42\u3002\u4e0b\u9762\u7684\u4ee3\u7801\u662f\u521d\u59cb\u5316\u4e00\u4e2a\u5faa\u73af\u5c42\uff0c\u53ef\u4ee5\u5728\u6784\u9020\u51fd\u6570\u4e2d\u8bbe\u7f6e\u5377\u79ef\u5c42\u7684\u8d85\u53c2\u6570\u3002\u6211\u4eec\u6ce8\u610f\u5230\uff0c\u5faa\u73af\u5c42\u6709\u4e24\u4e2a\u6743\u91cd\u6570\u7ec4\uff0cU\u548cW\n    '''\n    def __init__(self, input_width, state_width,\n                 activator, learning_rate):\n        self.input_width = input_width\n        self.state_width = state_width\n        self.activator = activator\n        self.learning_rate = learning_rate\n        self.times = 0       # \u5f53\u524d\u65f6\u523b\u521d\u59cb\u5316\u4e3at0\n        self.state_list = [] # \u4fdd\u5b58\u5404\u4e2a\u65f6\u523b\u7684state\n        self.state_list.append(np.zeros(\n            (state_width, 1)))           # \u521d\u59cb\u5316s0\n        self.U = np.random.uniform(-1e-4, 1e-4,\n            (state_width, input_width))  # \u521d\u59cb\u5316U\n        self.W = np.random.uniform(-1e-4, 1e-4,\n            (state_width, state_width))  # \u521d\u59cb\u5316W\n\n    def forward(self, input_array):\n        '''\n        Desc:\n            \u5b9e\u73b0\u5faa\u73af\u5c42\u7684\u524d\u5411\u8ba1\u7b97\n        '''\n        self.times += 1\n        state = (np.dot(self.U, input_array) +\n                 np.dot(self.W, self.state_list[-1]))\n        element_wise_op(state, self.activator.forward)\n        self.state_list.append(state)\n\n    def backward(self, sensitivity_array, \n                 activator):\n        '''\n        \u5b9e\u73b0BPTT\u7b97\u6cd5\n        '''\n        self.calc_delta(sensitivity_array, activator)\n        self.calc_gradient()\n\n    def update(self):\n        '''\n        \u6309\u7167\u68af\u5ea6\u4e0b\u964d\uff0c\u66f4\u65b0\u6743\u91cd\n        '''\n        self.W -= self.learning_rate * self.gradient\n\n    def calc_delta(self, sensitivity_array, activator):\n        self.delta_list = []  # \u7528\u6765\u4fdd\u5b58\u5404\u4e2a\u65f6\u523b\u7684\u8bef\u5dee\u9879\n        for i in range(self.times):\n            self.delta_list.append(np.zeros(\n                (self.state_width, 1)))\n        self.delta_list.append(sensitivity_array)\n        # \u8fed\u4ee3\u8ba1\u7b97\u6bcf\u4e2a\u65f6\u523b\u7684\u8bef\u5dee\u9879\n        for k in range(self.times - 1, 0, -1):\n            self.calc_delta_k(k, activator)\n\n    def calc_delta_k(self, k, activator):\n        '''\n        \u6839\u636ek+1\u65f6\u523b\u7684delta\u8ba1\u7b97k\u65f6\u523b\u7684delta\n        '''\n        state = self.state_list[k+1].copy()\n        element_wise_op(self.state_list[k+1],\n                    activator.backward)\n        self.delta_list[k] = np.dot(\n            np.dot(self.delta_list[k+1].T, self.W),\n            np.diag(state[:,0])).T\n\n    def calc_gradient(self):\n        self.gradient_list = [] # \u4fdd\u5b58\u5404\u4e2a\u65f6\u523b\u7684\u6743\u91cd\u68af\u5ea6\n        for t in range(self.times + 1):\n            self.gradient_list.append(np.zeros(\n                (self.state_width, self.state_width)))\n        for t in range(self.times, 0, -1):\n            self.calc_gradient_t(t)\n        # \u5b9e\u9645\u7684\u68af\u5ea6\u662f\u5404\u4e2a\u65f6\u523b\u68af\u5ea6\u4e4b\u548c\n        self.gradient = reduce(\n            lambda a, b: a + b, self.gradient_list,\n            self.gradient_list[0]) # [0]\u88ab\u521d\u59cb\u5316\u4e3a0\u4e14\u6ca1\u6709\u88ab\u4fee\u6539\u8fc7\n\n    def calc_gradient_t(self, t):\n        '''\n        \u8ba1\u7b97\u6bcf\u4e2a\u65f6\u523bt\u6743\u91cd\u7684\u68af\u5ea6\n        '''\n        gradient = np.dot(self.delta_list[t],\n            self.state_list[t-1].T)\n        self.gradient_list[t] = gradient\n\n    def reset_state(self):\n        self.times = 0       # \u5f53\u524d\u65f6\u523b\u521d\u59cb\u5316\u4e3at0\n        self.state_list = [] # \u4fdd\u5b58\u5404\u4e2a\u65f6\u523b\u7684state\n        self.state_list.append(np.zeros(\n            (self.state_width, 1)))      # \u521d\u59cb\u5316s0\n\n\ndef data_set():\n    x = [np.array([[1], [2], [3]]),\n         np.array([[2], [3], [4]])]\n    d = np.array([[1], [2]])\n    return x, d\n\n\ndef gradient_check():\n    '''\n    \u68af\u5ea6\u68c0\u67e5\n    '''\n    # \u8bbe\u8ba1\u4e00\u4e2a\u8bef\u5dee\u51fd\u6570\uff0c\u53d6\u6240\u6709\u8282\u70b9\u8f93\u51fa\u9879\u4e4b\u548c\n    error_function = lambda o: o.sum()\n    \n    rl = RecurrentLayer(3, 2, IdentityActivator(), 1e-3)\n\n    # \u8ba1\u7b97forward\u503c\n    x, d = data_set()\n    rl.forward(x[0])\n    rl.forward(x[1])\n    \n    # \u6c42\u53d6sensitivity map\n    sensitivity_array = np.ones(rl.state_list[-1].shape,\n                                dtype=np.float64)\n    # \u8ba1\u7b97\u68af\u5ea6\n    rl.backward(sensitivity_array, IdentityActivator())\n    \n    # \u68c0\u67e5\u68af\u5ea6\n    epsilon = 10e-4\n    for i in range(rl.W.shape[0]):\n        for j in range(rl.W.shape[1]):\n            rl.W[i,j] += epsilon\n            rl.reset_state()\n            rl.forward(x[0])\n            rl.forward(x[1])\n            err1 = error_function(rl.state_list[-1])\n            rl.W[i,j] -= 2*epsilon\n            rl.reset_state()\n            rl.forward(x[0])\n            rl.forward(x[1])\n            err2 = error_function(rl.state_list[-1])\n            expect_grad = (err1 - err2) / (2 * epsilon)\n            rl.W[i,j] += epsilon\n            print('weights(%d,%d): expected - actural %f - %f' % (\n                i, j, expect_grad, rl.gradient[i,j]))\n\n\ndef test():\n    l = RecurrentLayer(3, 2, ReluActivator(), 1e-3)\n    x, d = data_set()\n    l.forward(x[0])\n    l.forward(x[1])\n    l.backward(d, ReluActivator())\n    return l\n", "src/py2.x/dl/bp.py": "#!/usr/bin/env python\n# -*- coding: UTF-8 -*-\n\nfrom __future__ import print_function\nimport random\nfrom numpy import *\n\n# sigmoid \u51fd\u6570\ndef sigmoid(inX):\n    '''\n    Desc:\n        sigmoid \u51fd\u6570\u5b9e\u73b0\n    Args:\n        inX --- \u8f93\u5165\u5411\u91cf\n    Returns:\n        \u5bf9\u8f93\u5165\u5411\u91cf\u4f5c\u7528 sigmoid \u51fd\u6570\u4e4b\u540e\u5f97\u5230\u7684\u8f93\u51fa\n    '''\n    return 1.0 / (1 + exp(-inX))\n\n\n# \u5b9a\u4e49\u795e\u7ecf\u7f51\u7edc\u7684\u8282\u70b9\u7c7b\nclass Node(object):\n    '''\n    Desc:\n        \u795e\u7ecf\u7f51\u7edc\u7684\u8282\u70b9\u7c7b\n    '''\n    def __init__(self, layer_index, node_index):\n        '''\n        Desc:\n            \u521d\u59cb\u5316\u4e00\u4e2a\u8282\u70b9\n        Args:\n            layer_index --- \u5c42\u7684\u7d22\u5f15\uff0c\u4e5f\u5c31\u662f\u8868\u793a\u7b2c\u51e0\u5c42\n            node_index --- \u8282\u70b9\u7684\u7d22\u5f15\uff0c\u4e5f\u5c31\u662f\u8868\u793a\u8282\u70b9\u7684\u7d22\u5f15\n        Returns:\n            None\n        '''\n        # \u8bbe\u7f6e\u8282\u70b9\u6240\u5728\u7684\u5c42\u7684\u4f4d\u7f6e\n        self.layer_index = layer_index\n        # \u8bbe\u7f6e\u5c42\u4e2d\u7684\u8282\u70b9\u7684\u7d22\u5f15\n        self.node_index = node_index\n        # \u8bbe\u7f6e\u6b64\u8282\u70b9\u7684\u4e0b\u6e38\u8282\u70b9\uff0c\u4e5f\u5c31\u662f\u8fd9\u4e2a\u8282\u70b9\u4e0e\u4e0b\u4e00\u5c42\u7684\u54ea\u4e2a\u8282\u70b9\u76f8\u8fde\n        self.downstream = []\n        # \u8bbe\u7f6e\u6b64\u8282\u70b9\u7684\u4e0a\u6e38\u8282\u70b9\uff0c\u4e5f\u5c31\u662f\u54ea\u51e0\u4e2a\u8282\u70b9\u7684\u4e0b\u6e38\u8282\u70b9\u4e0e\u6b64\u8282\u70b9\u76f8\u8fde\n        self.upstream = []\n        # \u6b64\u8282\u70b9\u7684\u8f93\u51fa\n        self.output = 0\n        # \u6b64\u8282\u70b9\u771f\u5b9e\u503c\u4e0e\u8ba1\u7b97\u503c\u4e4b\u95f4\u7684\u5dee\u503c\n        self.delta = 0\n\n    def set_output(self, output):\n        '''\n        Desc:\n            \u8bbe\u7f6e\u8282\u70b9\u7684 output\n        Args:\n            output --- \u8282\u70b9\u7684 output\n        Returns:\n            None\n        '''\n        self.output = output\n\n    def append_downstream_connection(self, conn):\n        '''\n        Desc:\n           \u6dfb\u52a0\u6b64\u8282\u70b9\u7684\u4e0b\u6e38\u8282\u70b9\u7684\u8fde\u63a5\n        Args:\n            conn --- \u5f53\u524d\u8282\u70b9\u7684\u4e0b\u6e38\u8282\u70b9\u7684\u8fde\u63a5\u7684 list\n        Returns:\n            None\n        '''\n        # \u4f7f\u7528 list \u7684 append \u65b9\u6cd5\u6765\u5c06 conn \u4e2d\u7684\u8282\u70b9\u6dfb\u52a0\u5230 downstream \u4e2d\n        self.downstream.append(conn)\n\n    def append_upstream_connection(self, conn):\n        '''\n        Desc:\n            \u6dfb\u52a0\u6b64\u8282\u70b9\u7684\u4e0a\u6e38\u8282\u70b9\u7684\u8fde\u63a5\n        Args:\n            conn ---- \u5f53\u524d\u8282\u70b9\u7684\u4e0a\u6e38\u8282\u70b9\u7684\u8fde\u63a5\u7684 list\n        Returns:\n            None\n        '''\n        # \u4f7f\u7528 list \u7684 append \u65b9\u6cd5\u6765\u5c06 conn \u4e2d\u7684\u8282\u70b9\u6dfb\u52a0\u5230 upstream \u4e2d\n        self.upstream.append(conn)\n\n    def calc_output(self):\n        '''\n        Desc:\n            \u8ba1\u7b97\u8282\u70b9\u7684\u8f93\u51fa\uff0c\u4f9d\u636e output = sigmoid(wTx)\n        Args:\n            None\n        Returns:\n            None\n        '''\n        # \u4f7f\u7528 reduce() \u51fd\u6570\u5bf9\u5176\u4e2d\u7684\u56e0\u7d20\u6c42\u548c\n        output = reduce(lambda ret, conn: ret + conn.upstream_node.output * conn.weight, self.upstream, 0)\n        # \u5bf9\u4e0a\u6e38\u8282\u70b9\u7684 output \u4e58 weights \u4e4b\u540e\u6c42\u548c\u5f97\u5230\u7684\u7ed3\u679c\u5e94\u7528 sigmoid \u51fd\u6570\uff0c\u5f97\u5230\u5f53\u524d\u8282\u70b9\u7684 output\n        self.output = sigmoid(output)\n\n    def calc_hidden_layer_delta(self):\n        '''\n        Desc:\n            \u8ba1\u7b97\u9690\u85cf\u5c42\u7684\u8282\u70b9\u7684 delta\n        Args:\n            output --- \u8282\u70b9\u7684 output\n        Returns:\n            None\n        '''\n        # \u6839\u636e https://www.zybuluo.com/hanbingtao/note/476663 \u7684 \u5f0f4 \u8ba1\u7b97\u9690\u85cf\u5c42\u7684delta\n        downstream_delta = reduce(lambda ret, conn: ret + conn.downstream_node.delta * conn.weight, self.downstream, 0.0)\n        # \u8ba1\u7b97\u6b64\u8282\u70b9\u7684 delta\n        self.delta = self.output * (1 - self.output) * downstream_delta\n\n    def calc_output_layer_delta(self, label):\n        '''\n        Desc:\n            \u8ba1\u7b97\u8f93\u51fa\u5c42\u7684 delta\n        Args:\n            label --- \u8f93\u5165\u5411\u91cf\u5bf9\u5e94\u7684\u771f\u5b9e\u6807\u7b7e\uff0c\u4e0d\u662f\u8ba1\u7b97\u5f97\u5230\u7684\u7ed3\u679c\n        Returns:\n            None\n        '''\n        # \u5c31\u662f\u90a3\u8f93\u51fa\u5c42\u7684 delta\n        self.delta = self.output * (1 - self.output) * (label - self.output)\n\n    def __str__(self):\n        '''\n        Desc:\n            \u5c06\u8282\u70b9\u7684\u4fe1\u606f\u6253\u5370\u51fa\u6765\n        Args:\n            None\n        Returns:\n            None\n        '''\n        # \u6253\u5370\u683c\u5f0f: \u7b2c\u51e0\u5c42 - \u7b2c\u51e0\u4e2a\u8282\u70b9\uff0coutput \u662f\u591a\u5c11\uff0cdelta \u662f\u591a\u5c11\n        node_str = '%u-%u: output: %f delta: %f' % (self.layer_index, self.node_index, self.output, self.delta)\n        # \u4e0b\u6e38\u8282\u70b9\n        downstream_str = reduce(lambda ret, conn: ret + '\\n\\t' + str(conn), self.downstream, '')\n        # \u4e0a\u6e38\u8282\u70b9\n        upstream_str = reduce(lambda ret, conn: ret + '\\n\\t' + str(conn), self.upstream, '')\n        # \u5c06\u672c\u8282\u70b9 + \u4e0b\u6e38\u8282\u70b9 + \u4e0a\u6e38\u8282\u70b9 \u7684\u4fe1\u606f\u6253\u5370\u51fa\u6765\n        return node_str + '\\n\\tdownstream:' + downstream_str + '\\n\\tupstream:' + upstream_str\n\n\n# ConstNode \u5bf9\u8c61\uff0c\u4e3a\u4e86\u5b9e\u73b0\u4e00\u4e2a\u8f93\u51fa\u6052\u4e3a 1 \u7684\u8282\u70b9\uff08\u8ba1\u7b97\u504f\u7f6e\u9879 wb \u65f6\u9700\u8981\uff09\nclass ConstNode(object):\n    '''\n    Desc:\n        \u5e38\u6570\u9879\u5bf9\u8c61\uff0c\u5373\u76f8\u5f53\u4e8e\u8ba1\u7b97\u7684\u65f6\u5019\u7684\u504f\u7f6e\u9879\n    '''\n    def __init__(self, layer_index, node_index):\n        '''\n        Desc:\n            \u521d\u59cb\u5316\u8282\u70b9\u5bf9\u8c61\n        Args:\n            layer_index --- \u8282\u70b9\u6240\u5c5e\u7684\u5c42\u7684\u7f16\u53f7\n            node_index --- \u8282\u70b9\u7684\u7f16\u53f7\n        Returns:\n            None\n        '''    \n        self.layer_index = layer_index\n        self.node_index = node_index\n        self.downstream = []\n        self.output = 1\n\n\n    def append_downstream_connection(self, conn):\n        '''\n        Desc:\n            \u6dfb\u52a0\u4e00\u4e2a\u5230\u4e0b\u6e38\u8282\u70b9\u7684\u8fde\u63a5\n        Args:\n            conn --- \u5230\u4e0b\u6e38\u8282\u70b9\u7684\u8fde\u63a5                                           \n        Returns:\n            None\n        '''\n        # \u4f7f\u7528 list \u7684 append \u65b9\u6cd5\u5c06\u5305\u542b\u4e0b\u6e38\u8282\u70b9\u7684 conn \u6dfb\u52a0\u5230 downstream \u4e2d        \n        self.downstream.append(conn)\n\n\n    def calc_hidden_layer_delta(self):\n        '''\n        Desc:\n            \u8ba1\u7b97\u9690\u85cf\u5c42\u7684 delta\n        Args:\n            None\n        Returns:\n            None\n        '''\n        # \u4f7f\u7528\u6211\u4eec\u7684 \u516c\u5f0f 4 \u6765\u8ba1\u7b97\u4e0b\u6e38\u8282\u70b9\u7684 delta\uff0c\u6c42\u548c\n        downstream_delta = reduce(lambda ret, conn: ret + conn.downstream_node.delta * conn.weight, self.downstream, 0.0)\n        # \u8ba1\u7b97\u9690\u85cf\u5c42\u7684\u672c\u8282\u70b9\u7684 delta\n        self.delta = self.output * (1 - self.output) * downstream_delta\n\n\n    def __str__(self):\n        '''\n        Desc:\n           \u5c06\u8282\u70b9\u4fe1\u606f\u6253\u5370\u51fa\u6765\n        Args:\n            None\n        Returns:\n            None\n        '''\n        # \u5c06\u8282\u70b9\u7684\u4fe1\u606f\u6253\u5370\u51fa\u6765\n        # \u683c\u5f0f \u7b2c\u51e0\u5c42-\u7b2c\u51e0\u4e2a\u8282\u70b9\u7684 output \n        node_str = '%u-%u: output: 1' % (self.layer_index, self.node_index)\n        # \u6b64\u8282\u70b9\u7684\u4e0b\u6e38\u8282\u70b9\u7684\u4fe1\u606f\n        downstream_str = reduce(lambda ret, conn: ret + '\\n\\t' + str(conn), self.downstream, '')\n        # \u5c06\u6b64\u8282\u70b9\u4e0e\u4e0b\u6e38\u8282\u70b9\u7684\u4fe1\u606f\u7ec4\u5408\uff0c\u4e00\u8d77\u6253\u5370\u51fa\u6765\n        return node_str + '\\n\\tdownstream:' + downstream_str\n\n\n# \u795e\u7ecf\u7f51\u7edc\u7684\u5c42\u5bf9\u8c61\uff0c\u8d1f\u8d23\u521d\u59cb\u5316\u4e00\u5c42\u3002\u6b64\u5916\uff0c\u4f5c\u4e3a Node \u7684\u96c6\u5408\u5bf9\u8c61\uff0c\u63d0\u4f9b\u5bf9 Node \u96c6\u5408\u7684\u64cd\u4f5c\nclass Layer(object):\n    '''\n    Desc:\n        \u795e\u7ecf\u7f51\u7edc\u7684 Layer \u7c7b\n    '''\n\n    def __init__(self, layer_index, node_count):\n        '''\n        Desc:\n            \u795e\u7ecf\u7f51\u7edc\u7684\u5c42\u5bf9\u8c61\u7684\u521d\u59cb\u5316\n        Args:\n            layer_index --- \u5c42\u7684\u7d22\u5f15\n            node_count --- \u8282\u70b9\u7684\u4e2a\u6570\n        Returns:\n            None\n        '''\n        # \u8bbe\u7f6e \u5c42\u7684\u7d22\u5f15\n        self.layer_index = layer_index\n        # \u8bbe\u7f6e\u5c42\u4e2d\u7684\u8282\u70b9\u7684 list\n        self.nodes = []\n        # \u5c06 Node \u8282\u70b9\u6dfb\u52a0\u5230 nodes \u4e2d\n        for i in range(node_count):\n            self.nodes.append(Node(layer_index, i))\n        # \u5c06 ConstNode \u8282\u70b9\u4e5f\u6dfb\u52a0\u5230 nodes \u4e2d\n        self.nodes.append(ConstNode(layer_index, node_count))\n\n    def set_output(self, data):\n        '''\n        Desc:\n            \u8bbe\u7f6e\u5c42\u7684\u8f93\u51fa\uff0c\u5f53\u5c42\u662f\u8f93\u5165\u5c42\u65f6\u4f1a\u7528\u5230\n        Args:\n            data --- \u8f93\u51fa\u7684\u503c\u7684 list\n        Returns:\n            None\n        '''\n        # \u8bbe\u7f6e\u8f93\u5165\u5c42\u4e2d\u5404\u4e2a\u8282\u70b9\u7684 output\n        for i in range(len(data)):\n            self.nodes[i].set_output(data[i])\n\n    def calc_output(self):\n        '''\n        Desc:\n            \u8ba1\u7b97\u5c42\u7684\u8f93\u51fa\u5411\u91cf\n        Args:\n            None\n        Returns:\n            None\n        '''\n        # \u904d\u5386\u672c\u5c42\u7684\u6240\u6709\u8282\u70b9\uff08\u9664\u53bb\u6700\u540e\u4e00\u4e2a\u8282\u70b9\uff0c\u56e0\u4e3a\u5b83\u662f\u6052\u4e3a\u5e38\u6570\u7684\u504f\u7f6e\u9879b\uff09\n        # \u8c03\u7528\u8282\u70b9\u7684 calc_output \u65b9\u6cd5\u6765\u8ba1\u7b97\u8f93\u51fa\u5411\u91cf\n        for node in self.nodes[:-1]:\n            node.calc_output()\n\n    def dump(self):\n        '''\n        Desc:\n            \u5c06\u5c42\u4fe1\u606f\u6253\u5370\u51fa\u6765\n        Args:\n            None\n        Returns:\n            None\n        '''\n        # \u904d\u5386\u5c42\u7684\u6240\u6709\u7684\u8282\u70b9 nodes\uff0c\u5c06\u8282\u70b9\u4fe1\u606f\u6253\u5370\u51fa\u6765\n        for node in self.nodes:\n            print(node)\n\n\n# Connection \u5bf9\u8c61\u7c7b\uff0c\u4e3b\u8981\u8d1f\u8d23\u8bb0\u5f55\u8fde\u63a5\u7684\u6743\u91cd\uff0c\u4ee5\u53ca\u8fd9\u4e2a\u8fde\u63a5\u6240\u5173\u8054\u7684\u4e0a\u4e0b\u6e38\u7684\u8282\u70b9\nclass Connection(object):\n    '''\n    Desc:\n        Connection \u5bf9\u8c61\uff0c\u8bb0\u5f55\u8fde\u63a5\u6743\u91cd\u548c\u8fde\u63a5\u6240\u5173\u8054\u7684\u4e0a\u4e0b\u6e38\u8282\u70b9\uff0c\u6ce8\u610f\uff0c\u8fd9\u91cc\u7684 connection \u6ca1\u6709 s \uff0c\u4e0d\u662f\u590d\u6570\n    '''\n    def __init__(self, upstream_node, downstream_node):\n        '''\n        Desc:\n            \u521d\u59cb\u5316 Connection \u5bf9\u8c61\n        Args:\n            upstream_node --- \u4e0a\u6e38\u8282\u70b9\n            downstream_node --- \u4e0b\u6e38\u8282\u70b9\n        Returns:\n            None\n        '''\n        # \u8bbe\u7f6e\u4e0a\u6e38\u8282\u70b9\n        self.upstream_node = upstream_node\n        # \u8bbe\u7f6e\u4e0b\u6e38\u8282\u70b9\n        self.downstream_node = downstream_node\n        # \u8bbe\u7f6e\u6743\u91cd\uff0c\u8fd9\u91cc\u8bbe\u7f6e\u7684\u6743\u91cd\u662f -0.1 \u5230 0.1 \u4e4b\u95f4\u7684\u4efb\u4f55\u6570\n        self.weight = random.uniform(-0.1, 0.1)\n        # \u8bbe\u7f6e\u68af\u5ea6 \u4e3a 0.0\n        self.gradient = 0.0\n\n    def calc_gradient(self):\n        '''\n        Desc:\n            \u8ba1\u7b97\u68af\u5ea6\n        Args:\n            None\n        Returns:\n            None\n        '''\n        # \u4e0b\u6e38\u8282\u70b9\u7684 delta * \u4e0a\u6e38\u8282\u70b9\u7684 output \u8ba1\u7b97\u5f97\u5230\u68af\u5ea6\n        self.gradient = self.downstream_node.delta * self.upstream_node.output\n\n    def update_weight(self, rate):\n        '''\n        Desc:\n            \u6839\u636e\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5\u66f4\u65b0\u6743\u91cd\n        Args:\n            rate --- \u5b66\u4e60\u7387 / \u6216\u8005\u6210\u4e3a\u6b65\u957f\n        Returns:\n            None\n        '''\n        # \u8c03\u7528\u8ba1\u7b97\u68af\u5ea6\u7684\u51fd\u6570\u6765\u5c06\u68af\u5ea6\u8ba1\u7b97\u51fa\u6765\n        self.calc_gradient()\n        # \u4f7f\u7528\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5\u6765\u66f4\u65b0\u6743\u91cd\n        self.weight += rate * self.gradient\n\n    def get_gradient(self):\n        '''\n        Desc:\n            \u83b7\u53d6\u5f53\u524d\u7684\u68af\u5ea6\n        Args:\n            None\n        Returns:\n            \u5f53\u524d\u7684\u68af\u5ea6 gradient \n        '''\n        return self.gradient\n\n    def __str__(self):\n        '''\n        Desc:\n            \u5c06\u8fde\u63a5\u4fe1\u606f\u6253\u5370\u51fa\u6765\n        Args:\n            None\n        Returns:\n            \u8fde\u63a5\u4fe1\u606f\u8fdb\u884c\u8fd4\u56de\n        '''\n        # \u683c\u5f0f\u4e3a: \u4e0a\u6e38\u8282\u70b9\u7684\u5c42\u7684\u7d22\u5f15+\u4e0a\u6e38\u8282\u70b9\u7684\u8282\u70b9\u7d22\u5f15 ---> \u4e0b\u6e38\u8282\u70b9\u7684\u5c42\u7684\u7d22\u5f15+\u4e0b\u6e38\u8282\u70b9\u7684\u8282\u70b9\u7d22\u5f15\uff0c\u6700\u540e\u4e00\u4e2a\u6570\u662f\u6743\u91cd\n        return '(%u-%u) -> (%u-%u) = %f' % (\n            self.upstream_node.layer_index, \n            self.upstream_node.node_index,\n            self.downstream_node.layer_index, \n            self.downstream_node.node_index, \n            self.weight)\n\n\n\n# Connections \u5bf9\u8c61\uff0c\u63d0\u4f9b Connection \u96c6\u5408\u64cd\u4f5c\u3002\nclass Connections(object):\n    '''\n    Desc:\n        Connections \u5bf9\u8c61\uff0c\u63d0\u4f9b Connection \u96c6\u5408\u7684\u64cd\u4f5c\uff0c\u770b\u6e05\u695a\u540e\u9762\u6709\u6ca1\u6709 s \uff0c\u4e0d\u8981\u770b\u9519\n    '''\n    def __init__(self):\n        '''\n        Desc:\n            \u521d\u59cb\u5316 Connections \u5bf9\u8c61\n        Args:\n            None\n        Returns:\n            None\n        '''\n        # \u521d\u59cb\u5316\u4e00\u4e2a\u5217\u8868 list \n        self.connections = []\n\n    def add_connection(self, connection):\n        '''\n        Desc:\n            \u5c06 connection \u4e2d\u7684\u8282\u70b9\u4fe1\u606f append \u5230 connections \u4e2d\n        Args:\n            None\n        Returns:\n            None\n        '''\n        self.connections.append(connection)\n\n    def dump(self):\n        '''\n        Desc:\n            \u5c06 Connections \u7684\u8282\u70b9\u4fe1\u606f\u6253\u5370\u51fa\u6765\n        Args:\n            None\n        Returns:\n            None\n        '''\n        for conn in self.connections:\n            print(conn)\n\n\n# Network \u5bf9\u8c61\uff0c\u63d0\u4f9b\u76f8\u5e94 API\nclass Network(object):\n    '''\n    Desc:\n        Network \u7c7b\n    '''\n    def __init__(self, layers):\n        '''\n        Desc:\n            \u521d\u59cb\u5316\u4e00\u4e2a\u5168\u8fde\u63a5\u795e\u7ecf\u7f51\u7edc\n        Args:\n            layers --- \u4e8c\u7ef4\u6570\u7ec4\uff0c\u63cf\u8ff0\u795e\u7ecf\u7f51\u7edc\u7684\u6bcf\u5c42\u8282\u70b9\u6570\n        Returns:\n            None\n        '''\n        # \u521d\u59cb\u5316 connections\uff0c\u4f7f\u7528\u7684\u662f Connections \u5bf9\u8c61\n        self.connections = Connections()\n        # \u521d\u59cb\u5316 layers\n        self.layers = []\n        # \u6211\u4eec\u7684\u795e\u7ecf\u7f51\u7edc\u7684\u5c42\u6570\n        layer_count = len(layers)\n        # \u8282\u70b9\u6570\n        node_count = 0\n        # \u904d\u5386\u6240\u6709\u7684\u5c42\uff0c\u5c06\u6bcf\u5c42\u4fe1\u606f\u6dfb\u52a0\u5230 layers \u4e2d\u53bb\n        for i in range(layer_count):\n            self.layers.append(Layer(i, layers[i]))\n        # \u904d\u5386\u9664\u53bb\u8f93\u51fa\u5c42\u4e4b\u5916\u7684\u6240\u6709\u5c42\uff0c\u5c06\u8fde\u63a5\u4fe1\u606f\u6dfb\u52a0\u5230 connections \u5bf9\u8c61\u4e2d\n        for layer in range(layer_count - 1):\n            connections = [Connection(upstream_node, downstream_node) for upstream_node in self.layers[layer].nodes for downstream_node in self.layers[layer + 1].nodes[:-1]]\n            # \u904d\u5386 connections\uff0c\u5c06 conn \u6dfb\u52a0\u5230 connections \u4e2d\n            for conn in connections:\n                self.connections.add_connection(conn)\n                # \u4e3a\u4e0b\u6e38\u8282\u70b9\u6dfb\u52a0\u4e0a\u6e38\u8282\u70b9\u4e3a conn\n                conn.downstream_node.append_upstream_connection(conn)\n                # \u4e3a\u4e0a\u6e38\u8282\u70b9\u6dfb\u52a0\u4e0b\u6e38\u8282\u70b9\u4e3a conn\n                conn.upstream_node.append_downstream_connection(conn)\n\n\n    def train(self, labels, data_set, rate, epoch):\n        '''\n        Desc:\n            \u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\n        Args:\n            labels --- \u6570\u7ec4\uff0c\u8bad\u7ec3\u6837\u672c\u6807\u7b7e\uff0c\u6bcf\u4e2a\u5143\u7d20\u662f\u4e00\u4e2a\u6837\u672c\u7684\u6807\u7b7e\n            data_set --- \u4e8c\u7ef4\u6570\u7ec4\uff0c\u8bad\u7ec3\u6837\u672c\u7684\u7279\u5f81\u6570\u636e\u3002\u6bcf\u884c\u6570\u636e\u662f\u4e00\u4e2a\u6837\u672c\u7684\u7279\u5f81\n            rate --- \u5b66\u4e60\u7387\n            epoch --- \u8fed\u4ee3\u6b21\u6570\n        Returns:\n            None\n        '''\n        # \u5faa\u73af\u8fed\u4ee3 epoch \u6b21\n        for i in range(epoch):\n            # \u904d\u5386\u6bcf\u4e2a\u8bad\u7ec3\u6837\u672c\n            for d in range(len(data_set)):\n                # \u4f7f\u7528\u6b64\u6837\u672c\u8fdb\u884c\u8bad\u7ec3\uff08\u4e00\u6761\u6837\u672c\u8fdb\u884c\u8bad\u7ec3\uff09\n                self.train_one_sample(labels[d], data_set[d], rate)\n                # print 'sample %d training finished' % d\n\n    def train_one_sample(self, label, sample, rate):\n        '''\n        Desc:\n            \u5185\u90e8\u51fd\u6570\uff0c\u4f7f\u7528\u4e00\u4e2a\u6837\u672c\u5bf9\u7f51\u7edc\u8fdb\u884c\u8bad\u7ec3\n        Args:\n            label --- \u6837\u672c\u7684\u6807\u7b7e\n            sample --- \u6837\u672c\u7684\u7279\u5f81\n            rate --- \u5b66\u4e60\u7387\n        Returns:\n            None\n        '''\n        # \u8c03\u7528 Network \u7684 predict \u65b9\u6cd5\uff0c\u5bf9\u8fd9\u4e2a\u6837\u672c\u8fdb\u884c\u9884\u6d4b\n        self.predict(sample)\n        # \u8ba1\u7b97\u6839\u636e\u6b64\u6837\u672c\u5f97\u5230\u7684\u7ed3\u679c\u7684 delta\n        self.calc_delta(label)\n        # \u66f4\u65b0\u6743\u91cd\n        self.update_weight(rate)\n\n    def calc_delta(self, label):\n        '''\n        Desc:\n            \u8ba1\u7b97\u6bcf\u4e2a\u8282\u70b9\u7684 delta\n        Args:\n            label --- \u6837\u672c\u7684\u771f\u5b9e\u503c\uff0c\u4e5f\u5c31\u662f\u6837\u672c\u7684\u6807\u7b7e\n        Returns:\n            None\n        '''\n        # \u83b7\u53d6\u8f93\u51fa\u5c42\u7684\u6240\u6709\u8282\u70b9\n        output_nodes = self.layers[-1].nodes\n        # \u904d\u5386\u6240\u6709\u7684 label\n        for i in range(len(label)):\n            # \u8ba1\u7b97\u8f93\u51fa\u5c42\u8282\u70b9\u7684 delta\n            output_nodes[i].calc_output_layer_delta(label[i])\n        # \u8fd9\u4e2a\u7528\u6cd5\u5c31\u662f\u5207\u7247\u7684\u7528\u6cd5\uff0c [-2::-1] \u5c31\u662f\u5c06 layers \u8fd9\u4e2a\u6570\u7ec4\u5012\u8fc7\u6765\uff0c\u4ece\u6ca1\u5012\u8fc7\u6765\u7684\u65f6\u5019\u7684\u5012\u6570\u7b2c\u4e8c\u4e2a\u5143\u7d20\u5f00\u59cb\uff0c\u5230\u7ffb\u8f6c\u8fc7\u6765\u7684\u5012\u6570\u7b2c\u4e00\u4e2a\u6570\uff0c\u6bd4\u5982\u8fd9\u6837: aaa = [1,2,3,4,5,6,7,8,9],bbb = aaa[-2::-1] ==> bbb = [8, 7, 6, 5, 4, 3, 2, 1]\n        # \u5b9e\u9645\u4e0a\u5c31\u662f\u9664\u6389\u8f93\u51fa\u5c42\u4e4b\u5916\u7684\u6240\u6709\u5c42\u6309\u7167\u76f8\u53cd\u7684\u987a\u5e8f\u8fdb\u884c\u904d\u5386\n        for layer in self.layers[-2::-1]:\n            # \u904d\u5386\u6bcf\u5c42\u7684\u6240\u6709\u8282\u70b9\n            for node in layer.nodes:\n                # \u8ba1\u7b97\u9690\u85cf\u5c42\u7684 delta\n                node.calc_hidden_layer_delta()\n\n    def update_weight(self, rate):\n        '''\n        Desc:\n            \u66f4\u65b0\u6bcf\u4e2a\u8fde\u63a5\u7684\u6743\u91cd\n        Args:\n            rate --- \u5b66\u4e60\u7387\n        Returns:\n            None\n        '''\n        # \u6309\u7167\u6b63\u5e38\u987a\u5e8f\u904d\u5386\u9664\u4e86\u8f93\u51fa\u5c42\u7684\u5c42\n        for layer in self.layers[:-1]:\n            # \u904d\u5386\u6bcf\u5c42\u7684\u6240\u6709\u8282\u70b9\n            for node in layer.nodes:\n                # \u904d\u5386\u8282\u70b9\u7684\u4e0b\u6e38\u8282\u70b9\n                for conn in node.downstream:\n                    # \u6839\u636e\u4e0b\u6e38\u8282\u70b9\u6765\u66f4\u65b0\u8fde\u63a5\u7684\u6743\u91cd\n                    conn.update_weight(rate)\n\n    def calc_gradient(self):\n        '''\n        Desc:\n            \u8ba1\u7b97\u6bcf\u4e2a\u8fde\u63a5\u7684\u68af\u5ea6\n        Args:\n            None\n        Returns:\n            None\n        '''\n        # \u6309\u7167\u6b63\u5e38\u987a\u5e8f\u904d\u5386\u9664\u4e86\u8f93\u51fa\u5c42\u4e4b\u5916\u7684\u5c42\n        for layer in self.layers[:-1]:\n            # \u904d\u5386\u5c42\u4e2d\u7684\u6240\u6709\u8282\u70b9\n            for node in layer.nodes:\n                # \u904d\u5386\u8282\u70b9\u7684\u4e0b\u6e38\u8282\u70b9\n                for conn in node.downstream:\n                    # \u8ba1\u7b97\u68af\u5ea6\n                    conn.calc_gradient()\n\n    def get_gradient(self, label, sample):\n        '''\n        Desc:\n            \u83b7\u5f97\u7f51\u7edc\u5728\u4e00\u4e2a\u6837\u672c\u4e0b\uff0c\u6bcf\u4e2a\u8fde\u63a5\u4e0a\u7684\u68af\u5ea6\n        Args:\n            label --- \u6837\u672c\u6807\u7b7e\n            sample --- \u6837\u672c\u7279\u5f81\n        Returns:\n            None\n        '''\n        # \u8c03\u7528 predict() \u65b9\u6cd5\uff0c\u5229\u7528\u6837\u672c\u7684\u7279\u5f81\u6570\u636e\u5bf9\u6837\u672c\u8fdb\u884c\u9884\u6d4b\n        self.predict(sample)\n        # \u8ba1\u7b97 delta\n        self.calc_delta(label)\n        # \u8ba1\u7b97\u68af\u5ea6\n        self.calc_gradient()\n\n    def predict(self, sample):\n        '''\n        Desc:\n            \u6839\u636e\u8f93\u5165\u7684\u6837\u672c\u9884\u6d4b\u8f93\u51fa\u503c\n        Args:\n            sample --- \u6570\u7ec4\uff0c\u6837\u672c\u7684\u7279\u5f81\uff0c\u4e5f\u5c31\u662f\u7f51\u7edc\u7684\u8f93\u5165\u5411\u91cf\n        Returns:\n            \u4f7f\u7528\u6211\u4eec\u7684\u611f\u77e5\u5668\u89c4\u5219\u8ba1\u7b97\u7f51\u7edc\u7684\u8f93\u51fa\n        '''\n        # \u9996\u5148\u4e3a\u8f93\u5165\u5c42\u8bbe\u7f6e\u8f93\u51fa\u503coutput\u4e3a\u6837\u672c\u7684\u8f93\u5165\u5411\u91cf\uff0c\u5373\u4e0d\u53d1\u751f\u4efb\u4f55\u53d8\u5316\n        self.layers[0].set_output(sample)\n        # \u904d\u5386\u9664\u53bb\u8f93\u5165\u5c42\u5f00\u59cb\u5230\u6700\u540e\u4e00\u5c42\n        for i in range(1, len(self.layers)):\n            # \u8ba1\u7b97 output\n            self.layers[i].calc_output()\n        # \u5c06\u8ba1\u7b97\u5f97\u5230\u7684\u8f93\u51fa\uff0c\u4e5f\u5c31\u662f\u6211\u4eec\u7684\u9884\u6d4b\u503c\u8fd4\u56de\n        return map(lambda node: node.output, self.layers[-1].nodes[:-1])\n\n    def dump(self):\n        '''\n        Desc:\n            \u6253\u5370\u51fa\u6211\u4eec\u7684\u7f51\u7edc\u4fe1\u606f\n        Args:\n            None\n        Returns:\n            None\n        '''\n        # \u904d\u5386\u6240\u6709\u7684 layers\n        for layer in self.layers:\n            # \u5c06\u6240\u6709\u7684\u5c42\u7684\u4fe1\u606f\u6253\u5370\u51fa\u6765\n            layer.dump()\n\n\n# # ------------------------- \u81f3\u6b64\uff0c\u57fa\u672c\u4e0a\u6211\u4eec\u628a \u6211\u4eec\u7684\u795e\u7ecf\u7f51\u7edc\u5b9e\u73b0\u5b8c\u6210\uff0c\u4e0b\u9762\u8fd8\u4f1a\u4ecb\u7ecd\u4e00\u4e0b\u5bf9\u5e94\u7684\u68af\u5ea6\u68c0\u67e5\u76f8\u5173\u7684\u7b97\u6cd5\uff0c\u73b0\u5728\u6211\u4eec\u9996\u5148\u56de\u987e\u4e00\u4e0b\u6211\u4eec\u4e0a\u9762\u5199\u9053\u7684\u7c7b\u53ca\u4ed6\u4eec\u7684\u4f5c\u7528 ------------------------\n'''\n1\u3001\u8282\u70b9\u7c7b\u7684\u5b9e\u73b0 Node : \u8d1f\u8d23\u8bb0\u5f55\u548c\u7ef4\u62a4\u8282\u70b9\u81ea\u8eab\u4fe1\u606f\u4ee5\u53ca\u8fd9\u4e2a\u8282\u70b9\u76f8\u5173\u7684\u4e0a\u4e0b\u6e38\u8fde\u63a5\uff0c\u5b9e\u73b0\u8f93\u51fa\u503c\u548c\u8bef\u5dee\u9879\u7684\u8ba1\u7b97\u3002\u5982\u4e0b: \nlayer_index --- \u8282\u70b9\u6240\u5c5e\u7684\u5c42\u7684\u7f16\u53f7\nnode_index --- \u8282\u70b9\u7684\u7f16\u53f7\ndownstream --- \u4e0b\u6e38\u8282\u70b9\nupstream  ---- \u4e0a\u6e38\u8282\u70b9\noutput    ---- \u8282\u70b9\u7684\u8f93\u51fa\u503c\ndelta   ------ \u8282\u70b9\u7684\u8bef\u5dee\u9879\n\n2\u3001ConstNode \u7c7b\uff0c\u504f\u7f6e\u9879\u7c7b\u7684\u5b9e\u73b0: \u5b9e\u73b0\u4e00\u4e2a\u8f93\u51fa\u6052\u4e3a 1 \u7684\u8282\u70b9\uff08\u8ba1\u7b97\u504f\u7f6e\u9879\u7684\u65f6\u5019\u4f1a\u7528\u5230\uff09\uff0c\u5982\u4e0b: \nlayer_index --- \u8282\u70b9\u6240\u5c5e\u5c42\u7684\u7f16\u53f7\nnode_index ---- \u8282\u70b9\u7684\u7f16\u53f7\ndownstream ---- \u4e0b\u6e38\u8282\u70b9\n\u6ca1\u6709\u8bb0\u5f55\u4e0a\u6e38\u8282\u70b9\uff0c\u56e0\u4e3a\u4e00\u4e2a\u504f\u7f6e\u9879\u7684\u8f93\u51fa\u4e0e\u4e0a\u6e38\u8282\u70b9\u7684\u8f93\u51fa\u65e0\u5173\noutput    ----- \u504f\u7f6e\u9879\u7684\u8f93\u51fa\n\n3\u3001layer \u7c7b\uff0c\u8d1f\u8d23\u521d\u59cb\u5316\u4e00\u5c42\u3002\u4f5c\u4e3a\u7684\u662f Node \u8282\u70b9\u7684\u96c6\u5408\u5bf9\u8c61\uff0c\u63d0\u4f9b\u5bf9 Node \u96c6\u5408\u7684\u64cd\u4f5c\u3002\u4e5f\u5c31\u662f\u8bf4\uff0clayer \u5305\u542b\u7684\u662f Node \u7684\u96c6\u5408\u3002\nlayer_index ---- \u5c42\u7684\u7f16\u53f7\nnode_count ----- \u5c42\u6240\u5305\u542b\u7684\u8282\u70b9\u7684\u4e2a\u6570\ndef set_ouput() -- \u8bbe\u7f6e\u5c42\u7684\u8f93\u51fa\uff0c\u5f53\u5c42\u662f\u8f93\u5165\u5c42\u65f6\u4f1a\u7528\u5230\ndef calc_output -- \u8ba1\u7b97\u5c42\u7684\u8f93\u51fa\u5411\u91cf\uff0c\u8c03\u7528\u7684 Node \u7c7b\u7684 \u8ba1\u7b97\u8f93\u51fa \u65b9\u6cd5\n\n4\u3001Connection \u7c7b: \u8d1f\u8d23\u8bb0\u5f55\u8fde\u63a5\u7684\u6743\u91cd\uff0c\u4ee5\u53ca\u8fd9\u4e2a\u8fde\u63a5\u6240\u5173\u8054\u7684\u4e0a\u4e0b\u6e38\u8282\u70b9\uff0c\u5982\u4e0b: \nupstream_node --- \u8fde\u63a5\u7684\u4e0a\u6e38\u8282\u70b9\ndownstream_node -- \u8fde\u63a5\u7684\u4e0b\u6e38\u8282\u70b9\nweight   -------- random.uniform(-0.1, 0.1) \u521d\u59cb\u5316\u4e3a\u4e00\u4e2a\u5f88\u5c0f\u7684\u968f\u673a\u6570\ngradient -------- 0.0 \u68af\u5ea6\uff0c\u521d\u59cb\u5316\u4e3a 0.0 \ndef calc_gradient() --- \u8ba1\u7b97\u68af\u5ea6\uff0c\u4f7f\u7528\u7684\u662f\u4e0b\u6e38\u8282\u70b9\u7684 delta \u4e0e\u4e0a\u6e38\u8282\u70b9\u7684 output \u76f8\u4e58\u8ba1\u7b97\u5f97\u5230\ndef get_gradient() ---- \u83b7\u53d6\u5f53\u524d\u7684\u68af\u5ea6\ndef update_weight() --- \u6839\u636e\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5\u66f4\u65b0\u6743\u91cd\n\n5\u3001Connections \u7c7b: \u63d0\u4f9b\u5bf9 Connection \u96c6\u5408\u64cd\u4f5c\uff0c\u5982\u4e0b: \ndef add_connection() --- \u6dfb\u52a0\u4e00\u4e2a connection\n\n6\u3001Network \u7c7b: \u63d0\u4f9b\u76f8\u5e94\u7684 API\uff0c\u5982\u4e0b: \nconnections --- Connections \u5bf9\u8c61\nlayers -------- \u795e\u7ecf\u7f51\u7edc\u7684\u5c42\nlayer_count --- \u795e\u7ecf\u7f51\u7edc\u7684\u5c42\u6570\nnode_count  --- \u8282\u70b9\u4e2a\u6570\ndef train() --- \u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\ndef train_one_sample() --- \u7528\u4e00\u4e2a\u6837\u672c\u8bad\u7ec3\u7f51\u7edc\ndef calc_delta() --- \u8ba1\u7b97\u8bef\u5dee\u9879\ndef update_weight() --- \u66f4\u65b0\u6bcf\u4e2a\u8fde\u63a5\u6743\u91cd\ndef calc_gradient() --- \u8ba1\u7b97\u6bcf\u4e2a\u8fde\u63a5\u7684\u68af\u5ea6\ndef get_gradient() --- \u83b7\u5f97\u7f51\u7edc\u5728\u4e00\u4e2a\u6837\u672c\u4e0b\uff0c\u6bcf\u4e2a\u8fde\u63a5\u4e0a\u7684\u68af\u5ea6\ndef predict() --- \u6839\u636e\u8f93\u5165\u7684\u6837\u672c\u9884\u6d4b\u8f93\u51fa\u503c \n'''\n\n# #--------------------------------------\u56de\u987e\u5b8c\u6210\u4e86\uff0c\u6709\u4e9b\u95ee\u9898\u53ef\u80fd\u8fd8\u662f\u6ca1\u6709\u5f04\u61c2\uff0c\u6ca1\u4e8b\uff0c\u6211\u4eec\u63a5\u7740\u770b\u4e0b\u9762---------------------------------------------\n\nclass Normalizer(object):\n    '''\n    Desc:\n        \u5f52\u4e00\u5316\u5de5\u5177\u7c7b\n    Args:\n        object --- \u5bf9\u8c61\n    Returns:\n        None\n    '''\n    def __init__(self):\n        '''\n        Desc:\n            \u521d\u59cb\u5316\n        Args:\n            None\n        Returns:\n            None\n        '''\n        # \u521d\u59cb\u5316 16 \u8fdb\u5236\u7684\u6570\uff0c\u7528\u6765\u5224\u65ad\u4f4d\u7684\uff0c\u5206\u522b\u662f\n        # 0x1 ---- 00000001\n        # 0x2 ---- 00000010\n        # 0x4 ---- 00000100\n        # 0x8 ---- 00001000\n        # 0x10 --- 00010000\n        # 0x20 --- 00100000\n        # 0x40 --- 01000000\n        # 0x80 --- 10000000\n        self.mask = [0x1, 0x2, 0x4, 0x8, 0x10, 0x20, 0x40, 0x80]\n\n    def norm(self, number):\n        '''\n        Desc:\n            \u5bf9 number \u8fdb\u884c\u89c4\u8303\u5316\n        Args:\n            number --- \u8981\u89c4\u8303\u5316\u7684\u6570\u636e\n        Returns:\n            \u89c4\u8303\u5316\u4e4b\u540e\u7684\u6570\u636e\n        '''\n        # \u6b64\u65b9\u6cd5\u5c31\u76f8\u5f53\u4e8e\u5224\u65ad\u4e00\u4e2a 8 \u4f4d\u7684\u5411\u91cf\uff0c\u54ea\u4e00\u4f4d\u4e0a\u6709\u6570\u5b57\uff0c\u5982\u679c\u6709\u5c31\u5c06\u8fd9\u4e2a\u6570\u8bbe\u7f6e\u4e3a  0.9 \uff0c\u5426\u5219\uff0c\u8bbe\u7f6e\u4e3a 0.1\uff0c\u901a\u4fd7\u6bd4\u8f83\u6765\u8bf4\uff0c\u5c31\u662f\u6211\u4eec\u8fd9\u91cc\u7528 0.9 \u8868\u793a 1\uff0c\u7528 0.1 \u8868\u793a 0\n        return map(lambda m: 0.9 if number & m else 0.1, self.mask)\n\n    def denorm(self, vec):\n        '''\n        Desc:\n            \u5bf9\u6211\u4eec\u5f97\u5230\u7684\u5411\u91cf\u8fdb\u884c\u53cd\u89c4\u8303\u5316\n        Args:\n            vec --- \u5f97\u5230\u7684\u5411\u91cf\n        Returns:\n            \u6700\u7ec8\u7684\u9884\u6d4b\u7ed3\u679c\n        '''\n        # \u8fdb\u884c\u4e8c\u5206\u7c7b\uff0c\u5927\u4e8e 0.5 \u5c31\u8bbe\u7f6e\u4e3a 1\uff0c\u5c0f\u4e8e 0.5 \u5c31\u8bbe\u7f6e\u4e3a 0\n        binary = map(lambda i: 1 if i > 0.5 else 0, vec)\n        # \u904d\u5386 mask\n        for i in range(len(self.mask)):\n            binary[i] = binary[i] * self.mask[i]\n        # \u5c06\u7ed3\u679c\u76f8\u52a0\u5f97\u5230\u6700\u7ec8\u7684\u9884\u6d4b\u7ed3\u679c\n        return reduce(lambda x,y: x + y, binary)\n\n\ndef mean_square_error(vec1, vec2):\n    '''\n    Desc:\n        \u8ba1\u7b97\u5e73\u5747\u5e73\u65b9\u8bef\u5dee\n    Args:\n        vec1 --- \u7b2c\u4e00\u4e2a\u6570\n        vec2 --- \u7b2c\u4e8c\u4e2a\u6570\n    Returns:\n        \u8fd4\u56de 1/2 * (x-y)^2 \u8ba1\u7b97\u5f97\u5230\u7684\u503c\n    '''\n    return 0.5 * reduce(lambda a, b: a + b, map(lambda v: (v[0] - v[1]) * (v[0] - v[1]), zip(vec1, vec2)))\n\n\n\ndef gradient_check(network, sample_feature, sample_label):\n    '''\n    Desc:\n        \u68af\u5ea6\u68c0\u67e5\n    Args:\n        network --- \u795e\u7ecf\u7f51\u7edc\u5bf9\u8c61\n        sample_feature --- \u6837\u672c\u7684\u7279\u5f81\n        sample_label --- \u6837\u672c\u7684\u6807\u7b7e   \n    Returns:\n        None\n    '''\n    # \u8ba1\u7b97\u7f51\u7edc\u8bef\u5dee\n    network_error = lambda vec1, vec2: 0.5 * reduce(lambda a, b: a + b, map(lambda v: (v[0] - v[1]) * (v[0] - v[1]), zip(vec1, vec2)))\n\n    # \u83b7\u53d6\u7f51\u7edc\u5728\u5f53\u524d\u6837\u672c\u4e0b\u6bcf\u4e2a\u8fde\u63a5\u7684\u68af\u5ea6\n    network.get_gradient(sample_feature, sample_label)\n\n    # \u5bf9\u6bcf\u4e2a\u6743\u91cd\u505a\u68af\u5ea6\u68c0\u67e5    \n    for conn in network.connections.connections: \n        # \u83b7\u53d6\u6307\u5b9a\u8fde\u63a5\u7684\u68af\u5ea6\n        actual_gradient = conn.get_gradient()\n    \n        # \u589e\u52a0\u4e00\u4e2a\u5f88\u5c0f\u7684\u503c\uff0c\u8ba1\u7b97\u7f51\u7edc\u7684\u8bef\u5dee\n        epsilon = 0.0001\n        conn.weight += epsilon\n        error1 = network_error(network.predict(sample_feature), sample_label)\n    \n        # \u51cf\u53bb\u4e00\u4e2a\u5f88\u5c0f\u7684\u503c\uff0c\u8ba1\u7b97\u7f51\u7edc\u7684\u8bef\u5dee\n        conn.weight -= 2 * epsilon # \u521a\u624d\u52a0\u8fc7\u4e86\u4e00\u6b21\uff0c\u56e0\u6b64\u8fd9\u91cc\u9700\u8981\u51cf\u53bb2\u500d\n        error2 = network_error(network.predict(sample_feature), sample_label)\n    \n        # \u6839\u636e\u5f0f6\u8ba1\u7b97\u671f\u671b\u7684\u68af\u5ea6\u503c\n        expected_gradient = (error2 - error1) / (2 * epsilon)\n    \n        # \u6253\u5370\n        print('expected gradient: \\t%f\\nactual gradient: \\t%f' % (expected_gradient, actual_gradient))\n\n\ndef train_data_set():\n    '''\n    Desc:\n        \u83b7\u53d6\u8bad\u7ec3\u6570\u636e\u96c6\n    Args:\n        None\n    Returns:\n        labels --- \u8bad\u7ec3\u6570\u636e\u96c6\u6bcf\u6761\u6570\u636e\u5bf9\u5e94\u7684\u6807\u7b7e\n    '''\n    # \u8c03\u7528 Normalizer() \u7c7b\n    normalizer = Normalizer()\n    # \u521d\u59cb\u5316\u4e00\u4e2a list\uff0c\u7528\u6765\u5b58\u50a8\u540e\u9762\u7684\u6570\u636e\n    data_set = []\n    labels = []\n    # 0 \u5230 256 \uff0c\u5176\u4e2d\u4ee5 8 \u4e3a\u6b65\u957f\n    for i in range(0, 256, 8):\n        # \u8c03\u7528 normalizer \u5bf9\u8c61\u7684 norm \u65b9\u6cd5\n        n = normalizer.norm(int(random.uniform(0, 256)))\n        # \u5728 data_set \u4e2d append n\n        data_set.append(n)\n        # \u5728 labels \u4e2d append n\n        labels.append(n)\n    # \u5c06\u5b83\u4eec\u8fd4\u56de\n    return labels, data_set\n\n\ndef train(network):\n    '''\n    Desc:\n        \u4f7f\u7528\u6211\u4eec\u7684\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u8bad\u7ec3\n    Args:\n        network --- \u795e\u7ecf\u7f51\u7edc\u5bf9\u8c61\n    Returns:\n        None\n    '''\n    # \u83b7\u53d6\u8bad\u7ec3\u6570\u636e\u96c6\n    labels, data_set = train_data_set()\n    # \u8c03\u7528 network \u4e2d\u7684 train\u65b9\u6cd5\u6765\u8bad\u7ec3\u6211\u4eec\u7684\u795e\u7ecf\u7f51\u7edc\n    network.train(labels, data_set, 0.3, 50)\n\n\ndef test(network, data):\n    '''\n    Desc:\n        \u5bf9\u6211\u4eec\u7684\u5168\u8fde\u63a5\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u6d4b\u8bd5\n    Args:\n        network --- \u795e\u7ecf\u7f51\u7edc\u5bf9\u8c61\n        data ------ \u6d4b\u8bd5\u6570\u636e\u96c6\n    Returns:\n        None\n    '''\n    # \u8c03\u7528 Normalizer() \u7c7b\n    normalizer = Normalizer()\n    # \u8c03\u7528 norm \u65b9\u6cd5\uff0c\u5bf9\u6570\u636e\u8fdb\u884c\u89c4\u8303\u5316\n    norm_data = normalizer.norm(data)\n    # \u5bf9\u6d4b\u8bd5\u6570\u636e\u8fdb\u884c\u9884\u6d4b\n    predict_data = network.predict(norm_data)\n    # \u5c06\u7ed3\u679c\u6253\u5370\u51fa\u6765\n    print('\\ttestdata(%u)\\tpredict(%u)' % (data, normalizer.denorm(predict_data)))\n\n\ndef correct_ratio(network):\n    '''\n    Desc:\n        \u8ba1\u7b97\u6211\u4eec\u7684\u795e\u7ecf\u7f51\u7edc\u7684\u6b63\u786e\u7387\n    Args:\n        network --- \u795e\u7ecf\u7f51\u7edc\u5bf9\u8c61\n    Returns:\n        None\n    '''\n    normalizer = Normalizer()\n    correct = 0.0\n    for i in range(256):\n        if normalizer.denorm(network.predict(normalizer.norm(i))) == i:\n            correct += 1.0\n    print('correct_ratio: %.2f%%' % (correct / 256 * 100))\n\n\ndef gradient_check_test():\n    '''\n    Desc:\n        \u68af\u5ea6\u68c0\u67e5\u6d4b\u8bd5\n    Args:\n        None\n    Returns:\n        None\n    '''\n    # \u521b\u5efa\u4e00\u4e2a\u6709 3 \u5c42\u7684\u7f51\u7edc\uff0c\u6bcf\u5c42\u6709 2 \u4e2a\u8282\u70b9\n    net = Network([2, 2, 2])\n    # \u6837\u672c\u7684\u7279\u5f81\n    sample_feature = [0.9, 0.1]\n    # \u6837\u672c\u5bf9\u5e94\u7684\u6807\u7b7e\n    sample_label = [0.9, 0.1]\n    # \u4f7f\u7528\u68af\u5ea6\u68c0\u67e5\u6765\u67e5\u770b\u662f\u5426\u6b63\u786e\n    gradient_check(net, sample_feature, sample_label)\n\n\nif __name__ == '__main__':\n    '''\n    Desc:\n        \u4e3b\u51fd\u6570\n    Args:\n        None\n    Returns:\n        None\n    '''\n    # \u521d\u59cb\u5316\u4e00\u4e2a\u795e\u7ecf\u7f51\u7edc\uff0c\u8f93\u5165\u5c42 8 \u4e2a\u8282\u70b9\uff0c\u9690\u85cf\u5c42 3 \u4e2a\u8282\u70b9\uff0c\u8f93\u51fa\u5c42 8 \u4e2a\u8282\u70b9\n    net = Network([8, 3, 8])\n    # \u8bad\u7ec3\u6211\u4eec\u7684\u795e\u7ecf\u7f51\u7edc\n    train(net)\n    # \u5c06\u6211\u4eec\u7684\u795e\u7ecf\u7f51\u7edc\u7684\u4fe1\u606f\u6253\u5370\u51fa\u6765\n    net.dump()\n    # \u6253\u5370\u51fa\u795e\u7ecf\u7f51\u7edc\u7684\u6b63\u786e\u7387\n    correct_ratio(net)\n", "src/py2.x/dl/fc.py": "#!/usr/bin/env python\n# -*- coding: UTF-8 -*-\n\n\nfrom __future__ import print_function\nimport random\nimport numpy as np\nfrom activators import SigmoidActivator, IdentityActivator\n\ntry:\n    reduce         # Python 2\nexcept NameError:  # Python 3\n    from functools import reduce\n\n\n# \u5168\u8fde\u63a5\u5c42\u5b9e\u73b0\u7c7b\nclass FullConnectedLayer(object):\n    def __init__(self, input_size, output_size, \n                 activator):\n        '''\n        \u6784\u9020\u51fd\u6570\n        input_size: \u672c\u5c42\u8f93\u5165\u5411\u91cf\u7684\u7ef4\u5ea6\n        output_size: \u672c\u5c42\u8f93\u51fa\u5411\u91cf\u7684\u7ef4\u5ea6\n        activator: \u6fc0\u6d3b\u51fd\u6570\n        '''\n        self.input_size = input_size\n        self.output_size = output_size\n        self.activator = activator\n        # \u6743\u91cd\u6570\u7ec4W\n        self.W = np.random.uniform(-0.1, 0.1,\n            (output_size, input_size))\n        # \u504f\u7f6e\u9879b\n        self.b = np.zeros((output_size, 1))\n        # \u8f93\u51fa\u5411\u91cf\n        self.output = np.zeros((output_size, 1))\n\n    def forward(self, input_array):\n        '''\n        \u524d\u5411\u8ba1\u7b97\n        input_array: \u8f93\u5165\u5411\u91cf\uff0c\u7ef4\u5ea6\u5fc5\u987b\u7b49\u4e8einput_size\n        '''\n        # \u5f0f2\n        self.input = input_array\n        self.output = self.activator.forward(\n            np.dot(self.W, input_array) + self.b)\n\n    def backward(self, delta_array):\n        '''\n        \u53cd\u5411\u8ba1\u7b97W\u548cb\u7684\u68af\u5ea6\n        delta_array: \u4ece\u4e0a\u4e00\u5c42\u4f20\u9012\u8fc7\u6765\u7684\u8bef\u5dee\u9879\n        '''\n        # \u5f0f8\n        self.delta = self.activator.backward(self.input) * np.dot(\n            self.W.T, delta_array)\n        self.W_grad = np.dot(delta_array, self.input.T)\n        self.b_grad = delta_array\n\n    def update(self, learning_rate):\n        '''\n        \u4f7f\u7528\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5\u66f4\u65b0\u6743\u91cd\n        '''\n        self.W += learning_rate * self.W_grad\n        self.b += learning_rate * self.b_grad\n\n    def dump(self):\n        print('W: %s\\nb:%s' % (self.W, self.b))\n\n\n# \u795e\u7ecf\u7f51\u7edc\u7c7b\nclass Network(object):\n    def __init__(self, layers):\n        '''\n        \u6784\u9020\u51fd\u6570\n        '''\n        self.layers = []\n        for i in range(len(layers) - 1):\n            self.layers.append(\n                FullConnectedLayer(\n                    layers[i], layers[i+1],\n                    SigmoidActivator()\n                )\n            )\n\n    def predict(self, sample):\n        '''\n        \u4f7f\u7528\u795e\u7ecf\u7f51\u7edc\u5b9e\u73b0\u9884\u6d4b\n        sample: \u8f93\u5165\u6837\u672c\n        '''\n        output = sample\n        for layer in self.layers:\n            layer.forward(output)\n            output = layer.output\n        return output\n\n    def train(self, labels, data_set, rate, epoch):\n        '''\n        \u8bad\u7ec3\u51fd\u6570\n        labels: \u6837\u672c\u6807\u7b7e\n        data_set: \u8f93\u5165\u6837\u672c\n        rate: \u5b66\u4e60\u901f\u7387\n        epoch: \u8bad\u7ec3\u8f6e\u6570\n        '''\n        for i in range(epoch):\n            for d in range(len(data_set)):\n                self.train_one_sample(labels[d], \n                    data_set[d], rate)\n\n    def train_one_sample(self, label, sample, rate):\n        self.predict(sample)\n        self.calc_gradient(label)\n        self.update_weight(rate)\n\n    def calc_gradient(self, label):\n        delta = self.layers[-1].activator.backward(\n            self.layers[-1].output\n        ) * (label - self.layers[-1].output)\n        for layer in self.layers[::-1]:\n            layer.backward(delta)\n            delta = layer.delta\n        return delta\n\n    def update_weight(self, rate):\n        for layer in self.layers:\n            layer.update(rate)\n\n    def dump(self):\n        for layer in self.layers:\n            layer.dump()\n\n    def loss(self, output, label):\n        return 0.5 * ((label - output) * (label - output)).sum()\n\n    def gradient_check(self, sample_feature, sample_label):\n        '''\n        \u68af\u5ea6\u68c0\u67e5\n        network: \u795e\u7ecf\u7f51\u7edc\u5bf9\u8c61\n        sample_feature: \u6837\u672c\u7684\u7279\u5f81\n        sample_label: \u6837\u672c\u7684\u6807\u7b7e\n        '''\n\n        # \u83b7\u53d6\u7f51\u7edc\u5728\u5f53\u524d\u6837\u672c\u4e0b\u6bcf\u4e2a\u8fde\u63a5\u7684\u68af\u5ea6\n        self.predict(sample_feature)\n        self.calc_gradient(sample_label)\n\n        # \u68c0\u67e5\u68af\u5ea6\n        epsilon = 10e-4\n        for fc in self.layers:\n            for i in range(fc.W.shape[0]):\n                for j in range(fc.W.shape[1]):\n                    fc.W[i,j] += epsilon\n                    output = self.predict(sample_feature)\n                    err1 = self.loss(sample_label, output)\n                    fc.W[i,j] -= 2*epsilon\n                    output = self.predict(sample_feature)\n                    err2 = self.loss(sample_label, output)\n                    expect_grad = (err1 - err2) / (2 * epsilon)\n                    fc.W[i,j] += epsilon\n                    print('weights(%d,%d): expected - actural %.4e - %.4e' % (\n                        i, j, expect_grad, fc.W_grad[i,j]))\n\n\nfrom bp import train_data_set\n\n\ndef transpose(args):\n    return map(\n        lambda arg: map(\n            lambda line: np.array(line).reshape(len(line), 1)\n            , arg)\n        , args\n    )\n\n\nclass Normalizer(object):\n    def __init__(self):\n        self.mask = [\n            0x1, 0x2, 0x4, 0x8, 0x10, 0x20, 0x40, 0x80\n        ]\n\n    def norm(self, number):\n        data = map(lambda m: 0.9 if number & m else 0.1, self.mask)\n        return np.array(data).reshape(8, 1)\n\n    def denorm(self, vec):\n        binary = map(lambda i: 1 if i > 0.5 else 0, vec[:,0])\n        for i in range(len(self.mask)):\n            binary[i] = binary[i] * self.mask[i]\n        return reduce(lambda x,y: x + y, binary)\n\ndef train_data_set():\n    normalizer = Normalizer()\n    data_set = []\n    labels = []\n    for i in range(0, 256):\n        n = normalizer.norm(i)\n        data_set.append(n)\n        labels.append(n)\n    return labels, data_set\n\ndef correct_ratio(network):\n    normalizer = Normalizer()\n    correct = 0.0;\n    for i in range(256):\n        if normalizer.denorm(network.predict(normalizer.norm(i))) == i:\n            correct += 1.0\n    print('correct_ratio: %.2f%%' % (correct / 256 * 100))\n\n\ndef test():\n    labels, data_set = transpose(train_data_set())\n    net = Network([8, 3, 8])\n    rate = 0.5\n    mini_batch = 20\n    epoch = 10\n    for i in range(epoch):\n        net.train(labels, data_set, rate, mini_batch)\n        print('after epoch %d loss: %f' % (\n            (i + 1),\n            net.loss(labels[-1], net.predict(data_set[-1]))\n        ))\n        rate /= 2\n    correct_ratio(net)\n\n\ndef gradient_check():\n    '''\n    \u68af\u5ea6\u68c0\u67e5\n    '''\n    labels, data_set = transpose(train_data_set())\n    net = Network([8, 3, 8])\n    net.gradient_check(data_set[0], labels[0])\n    return net\n", "src/py2.x/dl/mnist.py": "#!/usr/bin/env python\n# -*- coding: UTF-8 -*-\n\nfrom __future__ import print_function\nimport struct\nfrom fc import *\nfrom datetime import datetime\n\n\n# \u6570\u636e\u52a0\u8f7d\u5668\u57fa\u7c7b\nclass Loader(object):\n    def __init__(self, path, count):\n        '''\n        \u521d\u59cb\u5316\u52a0\u8f7d\u5668\n        path: \u6570\u636e\u6587\u4ef6\u8def\u5f84\n        count: \u6587\u4ef6\u4e2d\u7684\u6837\u672c\u4e2a\u6570\n        '''\n        self.path = path\n        self.count = count\n\n    def get_file_content(self):\n        '''\n        \u8bfb\u53d6\u6587\u4ef6\u5185\u5bb9\n        '''\n        f = open(self.path, 'rb')\n        content = f.read()\n        f.close()\n        return content\n\n    def to_int(self, byte):\n        '''\n        \u5c06unsigned byte\u5b57\u7b26\u8f6c\u6362\u4e3a\u6574\u6570\n        '''\n        return struct.unpack('B', byte)[0]\n\n\n# \u56fe\u50cf\u6570\u636e\u52a0\u8f7d\u5668\nclass ImageLoader(Loader):\n    def get_picture(self, content, index):\n        '''\n        \u5185\u90e8\u51fd\u6570\uff0c\u4ece\u6587\u4ef6\u4e2d\u83b7\u53d6\u56fe\u50cf\n        '''\n        start = index * 28 * 28 + 16\n        picture = []\n        for i in range(28):\n            picture.append([])\n            for j in range(28):\n                picture[i].append(\n                    self.to_int(content[start + i * 28 + j]))\n        return picture\n\n    def get_one_sample(self, picture):\n        '''\n        \u5185\u90e8\u51fd\u6570\uff0c\u5c06\u56fe\u50cf\u8f6c\u5316\u4e3a\u6837\u672c\u7684\u8f93\u5165\u5411\u91cf\n        '''\n        sample = []\n        for i in range(28):\n            for j in range(28):\n                sample.append(picture[i][j])\n        return sample\n\n    def load(self):\n        '''\n        \u52a0\u8f7d\u6570\u636e\u6587\u4ef6\uff0c\u83b7\u5f97\u5168\u90e8\u6837\u672c\u7684\u8f93\u5165\u5411\u91cf\n        '''\n        content = self.get_file_content()\n        data_set = []\n        for index in range(self.count):\n            data_set.append(\n                self.get_one_sample(\n                    self.get_picture(content, index)))\n        return data_set\n\n\n# \u6807\u7b7e\u6570\u636e\u52a0\u8f7d\u5668\nclass LabelLoader(Loader):\n    def load(self):\n        '''\n        \u52a0\u8f7d\u6570\u636e\u6587\u4ef6\uff0c\u83b7\u5f97\u5168\u90e8\u6837\u672c\u7684\u6807\u7b7e\u5411\u91cf\n        '''\n        content = self.get_file_content()\n        labels = []\n        for index in range(self.count):\n            labels.append(self.norm(content[index + 8]))\n        return labels\n\n    def norm(self, label):\n        '''\n        \u5185\u90e8\u51fd\u6570\uff0c\u5c06\u4e00\u4e2a\u503c\u8f6c\u6362\u4e3a10\u7ef4\u6807\u7b7e\u5411\u91cf\n        '''\n        label_vec = []\n        label_value = self.to_int(label)\n        for i in range(10):\n            if i == label_value:\n                label_vec.append(0.9)\n            else:\n                label_vec.append(0.1)\n        return label_vec\n\n\ndef get_training_data_set():\n    '''\n    \u83b7\u5f97\u8bad\u7ec3\u6570\u636e\u96c6\n    '''\n    image_loader = ImageLoader('train-images-idx3-ubyte', 60000)\n    label_loader = LabelLoader('train-labels-idx1-ubyte', 60000)\n    return image_loader.load(), label_loader.load()\n\n\ndef get_test_data_set():\n    '''\n    \u83b7\u5f97\u6d4b\u8bd5\u6570\u636e\u96c6\n    '''\n    image_loader = ImageLoader('t10k-images-idx3-ubyte', 10000)\n    label_loader = LabelLoader('t10k-labels-idx1-ubyte', 10000)\n    return image_loader.load(), label_loader.load()\n\n\ndef show(sample):\n    str = ''\n    for i in range(28):\n        for j in range(28):\n            if sample[i*28+j] != 0:\n                str += '*'\n            else:\n                str += ' '\n        str += '\\n'\n    print(str)\n\n\ndef get_result(vec):\n    max_value_index = 0\n    max_value = 0\n    for i in range(len(vec)):\n        if vec[i] > max_value:\n            max_value = vec[i]\n            max_value_index = i\n    return max_value_index\n\n\ndef evaluate(network, test_data_set, test_labels):\n    error = 0\n    total = len(test_data_set)\n\n    for i in range(total):\n        label = get_result(test_labels[i])\n        predict = get_result(network.predict(test_data_set[i]))\n        if label != predict:\n            error += 1\n    return float(error) / float(total)\n\n\ndef now():\n    return datetime.now().strftime('%c')\n\n\ndef train_and_evaluate():\n    last_error_ratio = 1.0\n    epoch = 0\n    train_data_set, train_labels = transpose(get_training_data_set())\n    test_data_set, test_labels = transpose(get_test_data_set())\n    network = Network([784, 100, 10])\n    while True:\n        epoch += 1\n        network.train(train_labels, train_data_set, 0.01, 1)\n        print('%s epoch %d finished, loss %f' % (now(), epoch, \n            network.loss(train_labels[-1], network.predict(train_data_set[-1]))))\n        if epoch % 2 == 0:\n            error_ratio = evaluate(network, test_data_set, test_labels)\n            print('%s after epoch %d, error ratio is %f' % (now(), epoch, error_ratio))\n            if error_ratio > last_error_ratio:\n                break\n            else:\n                last_error_ratio = error_ratio\n\nif __name__ == '__main__':\n    train_and_evaluate()", "src/py2.x/dl/perceptron.py": "#!/usr/bin/env python\n# -*- coding: UTF-8 -*-\n\n# \u795e\u7ecf\u5143 / \u611f\u77e5\u5668\n\nfrom __future__ import print_function\nclass Perceptron():\n    '''\n    Desc:\n        \u611f\u77e5\u5668\u7c7b\n    Args:\n        None\n    Returns:\n        None\n    '''\n\n    def __init__(self, input_num, activator):\n        '''\n        Desc:\n            \u521d\u59cb\u5316\u611f\u77e5\u5668\n        Args:\n            input_num \u2014\u2014 \u8f93\u5165\u53c2\u6570\u7684\u4e2a\u6570\n            activator \u2014\u2014 \u6fc0\u6d3b\u51fd\u6570\n        Returns:\n            None\n        '''\n        # \u8bbe\u7f6e\u7684\u6fc0\u6d3b\u51fd\u6570\n        self.activator = activator\n        # \u6743\u91cd\u5411\u91cf\u521d\u59cb\u5316\u4e3a 0\n        self.weights = [0.0 for _ in range(input_num)]\n        # \u504f\u7f6e\u9879\u521d\u59cb\u5316\u4e3a 0\n        self.bias = 0.0\n\n    \n    def __str__(self):\n        '''\n        Desc:\n            \u5c06\u611f\u77e5\u5668\u4fe1\u606f\u6253\u5370\u51fa\u6765\n        Args:\n            None\n        Returns:\n            None\n        '''\n        return('weights\\t:%s\\nbias\\t:%f\\n' % (self.weights, self.bias))\n\n\n    def predict(self, input_vec):\n        '''\n        Desc:\n            \u8f93\u5165\u5411\u91cf\uff0c\u8f93\u51fa\u611f\u77e5\u5668\u7684\u8ba1\u7b97\u7ed3\u679c\n        Args:\n            input_vec \u2014\u2014 \u8f93\u5165\u5411\u91cf\n        Returns:\n            \u611f\u77e5\u5668\u7684\u8ba1\u7b97\u7ed3\u679c\n        '''\n        # \u5c06\u8f93\u5165\u5411\u91cf\u7684\u8ba1\u7b97\u7ed3\u679c\u8fd4\u56de\n        # \u8c03\u7528 \u6fc0\u6d3b\u51fd\u6570 activator \uff0c\u5c06\u8f93\u5165\u5411\u91cf\u8f93\u5165\uff0c\u8ba1\u7b97\u611f\u77e5\u5668\u7684\u7ed3\u679c\n        # reduce() \u51fd\u6570\u662f python 2 \u7684\u5185\u7f6e\u51fd\u6570\uff0c\u4ece python 3 \u5f00\u59cb\u79fb\u5230\u4e86 functools \u6a21\u5757\n        # reduce() \u4ece\u5de6\u5230\u53f3\u5bf9\u4e00\u4e2a\u5e8f\u5217\u7684\u9879\u7d2f\u8ba1\u5730\u5e94\u7528\u6709\u4e24\u4e2a\u53c2\u6570\u7684\u51fd\u6570\uff0c\u4ee5\u6b64\u5408\u5e76\u5e8f\u5217\u5230\u4e00\u4e2a\u5355\u4e00\u503c\uff0c\u4f8b\u5982 reduce(lambda x,y: x+y, [1,2,3,4,5]) \u8ba1\u7b97\u7684\u5c31\u662f ((((1+2)+3)+4)+5)\n        # map() \u63a5\u6536\u4e00\u4e2a\u51fd\u6570 f \u548c\u4e00\u4e2a list \uff0c\u5e76\u901a\u8fc7\u628a\u51fd\u6570 f \u4f9d\u6b21\u4f5c\u7528\u5728 list \u7684\u6bcf\u4e2a\u5143\u7d20\u4e0a\uff0c\u5f97\u5230\u4e00\u4e2a\u65b0\u7684 list \u8fd4\u56de\u3002\u6bd4\u5982\u6211\u4eec\u7684 f \u51fd\u6570\u662f\u8ba1\u7b97\u5e73\u65b9\uff0c map(f, [1,2,3,4,5]) ===> \u8fd4\u56de [1,4,9,16,25]\n        # zip() \u63a5\u6536\u4efb\u610f\u591a\u4e2a\uff08\u5305\u62ec 0 \u4e2a\u548c 1\u4e2a\uff09\u5e8f\u5217\u4f5c\u4e3a\u53c2\u6570\uff0c\u8fd4\u56de\u4e00\u4e2a tuple \u5217\u8868\u3002\u4f8b: x = [1,2,3] y = [4,5,6] z = [7,8,9] xyz = zip(x, y, z) ===> [(1,4,7), (2,5,8), (3,6,9)]\n        return self.activator(reduce(lambda a, b: a + b,map(lambda (x, w): x * w, zip(input_vec, self.weights)), 0.0) + self.bias)\n\n\n    def train(self, input_vecs, labels, iteration, rate):\n        '''\n        Desc:\n            \u8f93\u5165\u8bad\u7ec3\u6570\u636e: \u4e00\u7ec4\u5411\u91cf\u3001\u4e0e\u6bcf\u4e2a\u5411\u91cf\u5bf9\u5e94\u7684 label; \u4ee5\u53ca\u8bad\u7ec3\u8f6e\u6570\u3001\u5b66\u4e60\u7387\n        Args:\n            input_vec \u2014\u2014 \u8f93\u5165\u5411\u91cf\n            labels \u2014\u2014 \u6570\u636e\u5bf9\u5e94\u7684\u6807\u7b7e\n            iteration \u2014\u2014 \u8bad\u7ec3\u7684\u8fed\u4ee3\u8f6e\u6570\n            rate \u2014\u2014 \u5b66\u4e60\u7387\n        Returns:\n            None\n        '''\n        for i in range(iteration):\n            self._one_iteration(input_vecs, labels, rate)\n\n    \n    def _one_iteration(self, input_vecs, labels, rate):\n        '''\n        Desc:\n            \u8bad\u7ec3\u8fc7\u7a0b\u7684\u4e00\u6b21\u8fed\u4ee3\u8fc7\u7a0b\n        Args:\n            input_vecs \u2014\u2014 \u8f93\u5165\u5411\u91cf\n            labels \u2014\u2014 \u6570\u636e\u5bf9\u5e94\u7684\u6807\u7b7e\n            rate \u2014\u2014 \u5b66\u4e60\u7387\n        Returns:\n            None\n        '''\n        # zip() \u63a5\u6536\u4efb\u610f\u591a\u4e2a\uff08\u5305\u62ec 0 \u4e2a\u548c 1\u4e2a\uff09\u5e8f\u5217\u4f5c\u4e3a\u53c2\u6570\uff0c\u8fd4\u56de\u4e00\u4e2a tuple \u5217\u8868\u3002\u4f8b: x = [1,2,3] y = [4,5,6] z = [7,8,9] xyz = zip(x, y, z) ===> [(1,4,7), (2,5,8), (3,6,9)]\n        samples = zip(input_vecs, labels)\n        # \u5bf9\u6bcf\u4e2a\u6837\u672c\uff0c\u6309\u7167\u611f\u77e5\u5668\u89c4\u5219\u66f4\u65b0\u6743\u91cd\n        for (input_vec, label) in samples:\n            # \u8ba1\u7b97\u611f\u77e5\u5668\u5728\u5f53\u524d\u6743\u91cd\u4e0b\u7684\u8f93\u51fa\n            output = self.predict(input_vec)\n            # \u66f4\u65b0\u6743\u91cd\n            output = self._update_weights(input_vec, output, label, rate)\n\n    def _update_weights(self, input_vec, output, label, rate):\n        '''\n        Desc:\n            \u6309\u7167\u611f\u77e5\u5668\u89c4\u5219\u66f4\u65b0\u6743\u91cd\n        Args:\n            input_vec \u2014\u2014 \u8f93\u5165\u5411\u91cf\n            output \u2014\u2014 \u7ecf\u8fc7\u611f\u77e5\u5668\u89c4\u5219\u8ba1\u7b97\u5f97\u5230\u7684\u8f93\u51fa\n            label \u2014\u2014 \u8f93\u5165\u5411\u91cf\u5bf9\u5e94\u7684\u6807\u7b7e\n            rate \u2014\u2014 \u5b66\u4e60\u7387\n        Returns:\n            None\n        '''\n        # \u5229\u7528\u611f\u77e5\u5668\u89c4\u5219\u66f4\u65b0\u6743\u91cd\n        delta = label - output\n        # map() \u63a5\u6536\u4e00\u4e2a\u51fd\u6570 f \u548c\u4e00\u4e2a list \uff0c\u5e76\u901a\u8fc7\u628a\u51fd\u6570 f \u4f9d\u6b21\u4f5c\u7528\u5728 list \u7684\u6bcf\u4e2a\u5143\u7d20\u4e0a\uff0c\u5f97\u5230\u4e00\u4e2a\u65b0\u7684 list \u8fd4\u56de\u3002\u6bd4\u5982\u6211\u4eec\u7684 f \u51fd\u6570\u662f\u8ba1\u7b97\u5e73\u65b9\uff0c map(f, [1,2,3,4,5]) ===> \u8fd4\u56de [1,4,9,16,25]\n        # zip() \u63a5\u6536\u4efb\u610f\u591a\u4e2a\uff08\u5305\u62ec 0 \u4e2a\u548c 1\u4e2a\uff09\u5e8f\u5217\u4f5c\u4e3a\u53c2\u6570\uff0c\u8fd4\u56de\u4e00\u4e2a tuple \u5217\u8868\u3002\u4f8b: x = [1,2,3] y = [4,5,6] z = [7,8,9] xyz = zip(x, y, z) ===> [(1,4,7), (2,5,8), (3,6,9)]\n        self.weights = map(lambda (x, w): w + rate * delta * x, zip(input_vec, self.weights))\n        # \u66f4\u65b0 bias\n        self.bias += rate * delta\n\n    \n\ndef f(x):\n    '''\n    Desc:\n        \u5b9a\u4e49\u6fc0\u6d3b\u51fd\u6570 f\n    Args:\n        x \u2014\u2014 \u8f93\u5165\u5411\u91cf\n    Returns:\n        \uff08\u5b9e\u73b0\u9636\u8dc3\u51fd\u6570\uff09\u5927\u4e8e 0 \u8fd4\u56de 1\uff0c\u5426\u5219\u8fd4\u56de 0\n    '''\n    return 1 if x > 0 else 0\n\n\ndef get_training_dataset():\n    '''\n    Desc:\n        \u57fa\u4e8e and \u771f\u503c\u8868\u6765\u6784\u5efa/\u83b7\u53d6\u8bad\u7ec3\u6570\u636e\u96c6\n    Args:\n        None\n    Returns:\n        input_vecs \u2014\u2014 \u8f93\u5165\u5411\u91cf\n        labels \u2014\u2014 \u8f93\u5165\u5411\u91cf\u5bf9\u5e94\u7684\u6807\u7b7e\n    '''\n    # \u6784\u5efa\u8bad\u7ec3\u6570\u636e\uff0c\u8f93\u5165\u5411\u91cf\u7684\u5217\u8868\n    input_vecs = [[1,1],[0,0],[1,0],[0,1]]\n    # \u671f\u671b\u7684\u8f93\u51fa\u5217\u8868\uff0c\u4e5f\u5c31\u662f\u4e0a\u9762\u7684\u8f93\u5165\u5411\u91cf\u7684\u5217\u8868\u4e2d\u6570\u636e\u5bf9\u5e94\u7684\u6807\u7b7e\uff0c\u662f\u4e00\u4e00\u5bf9\u5e94\u7684\n    labels = [1, 0, 0, 0]\n    return input_vecs, labels\n\n\ndef train_and_perceptron():\n    '''\n    Desc:\n        \u4f7f\u7528 and \u771f\u503c\u8868\u6765\u8bad\u7ec3\u6211\u4eec\u7684\u611f\u77e5\u5668\n    Args:\n        None\n    Returns:\n        p \u2014\u2014 \u8fd4\u56de\u8bad\u7ec3\u597d\u7684\u611f\u77e5\u5668\n    '''\n    # \u521b\u5efa\u611f\u77e5\u5668\uff0c\u8f93\u5165\u53c2\u6570\u7684\u4e2a\u6570\u662f 2 \u4e2a\uff08\u56e0\u4e3a and \u662f\u4e2a\u4e8c\u5143\u51fd\u6570\uff09\uff0c\u6fc0\u6d3b\u51fd\u6570\u4e3a f\n    p = Perceptron(2, f)\n    # \u8fdb\u884c\u8bad\u7ec3\uff0c\u8fed\u4ee3 10 \u8f6e\uff0c\u5b66\u4e60\u901f\u7387\u662f\u6211\u4eec\u8bbe\u5b9a\u7684 rate \uff0c\u4e3a 0.1\n    input_vecs, labels = get_training_dataset()\n    p.train(input_vecs, labels, 10, 0.1)\n    # \u8fd4\u56de\u8bad\u7ec3\u597d\u7684\u611f\u77e5\u5668\n    return p\n\n\nif __name__ == '__main__':\n    '''\n    Desc:\n        \u4e3b\u51fd\u6570\uff0c\u8c03\u7528\u4e0a\u9762\u8fd4\u56de\u7684\u8bad\u7ec3\u597d\u7684\u611f\u77e5\u5668\u8fdb\u884c\u9884\u6d4b\n    Args:\n        None\n    Returns:\n        None\n    '''\n    # \u8bad\u7ec3 and \u611f\u77e5\u5668\n    and_perceptron = train_and_perceptron()\n    # \u6253\u5370\u8bad\u7ec3\u83b7\u5f97\u7684\u6743\u91cd\n    print(and_perceptron)\n    # \u6d4b\u8bd5\n    print('1 and 1 = %d' % and_perceptron.predict([1, 1]))\n    print('0 and 0 = %d' % and_perceptron.predict([0, 0]))\n    print('1 and 0 = %d' % and_perceptron.predict([1, 0]))\n    print('0 and 1 = %d' % and_perceptron.predict([0, 1]))", "src/py2.x/dl/recursive.py": "#!/usr/bin/env python\n# -*- coding: UTF-8 -*-\n\n\nfrom __future__ import print_function\nimport numpy as np\nfrom activators import IdentityActivator\n\n\nclass TreeNode(object):\n    def __init__(self, data, children=[], children_data=[]):\n        self.parent = None\n        self.children = children\n        self.children_data = children_data\n        self.data = data\n        for child in children:\n            child.parent = self\n\n# \u9012\u5f52\u795e\u7ecf\u7f51\u7edc\u5b9e\u73b0\nclass RecursiveLayer(object):\n    def __init__(self, node_width, child_count, \n                 activator, learning_rate):\n        '''\n        \u9012\u5f52\u795e\u7ecf\u7f51\u7edc\u6784\u9020\u51fd\u6570\n        node_width: \u8868\u793a\u6bcf\u4e2a\u8282\u70b9\u7684\u5411\u91cf\u7684\u7ef4\u5ea6\n        child_count: \u6bcf\u4e2a\u7236\u8282\u70b9\u6709\u51e0\u4e2a\u5b50\u8282\u70b9\n        activator: \u6fc0\u6d3b\u51fd\u6570\u5bf9\u8c61\n        learning_rate: \u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5\u5b66\u4e60\u7387\n        '''\n        self.node_width = node_width\n        self.child_count = child_count\n        self.activator = activator\n        self.learning_rate = learning_rate\n        # \u6743\u91cd\u6570\u7ec4W\n        self.W = np.random.uniform(-1e-4, 1e-4,\n            (node_width, node_width * child_count))\n        # \u504f\u7f6e\u9879b\n        self.b = np.zeros((node_width, 1))\n        # \u9012\u5f52\u795e\u7ecf\u7f51\u7edc\u751f\u6210\u7684\u6811\u7684\u6839\u8282\u70b9\n        self.root = None\n\n    def forward(self, *children):\n        '''\n        \u524d\u5411\u8ba1\u7b97\n        '''\n        children_data = self.concatenate(children)\n        parent_data = self.activator.forward(\n            np.dot(self.W, children_data) + self.b\n        )\n        self.root = TreeNode(parent_data, children\n                            , children_data)\n\n    def backward(self, parent_delta):\n        '''\n        BPTS\u53cd\u5411\u4f20\u64ad\u7b97\u6cd5\n        '''\n        self.calc_delta(parent_delta, self.root)\n        self.W_grad, self.b_grad = self.calc_gradient(self.root)\n\n    def update(self):\n        '''\n        \u4f7f\u7528SGD\u7b97\u6cd5\u66f4\u65b0\u6743\u91cd\n        '''\n        self.W -= self.learning_rate * self.W_grad\n        self.b -= self.learning_rate * self.b_grad\n\n    def reset_state(self):\n        self.root = None\n\n    def concatenate(self, tree_nodes):\n        '''\n        \u5c06\u5404\u4e2a\u6811\u8282\u70b9\u4e2d\u7684\u6570\u636e\u62fc\u63a5\u6210\u4e00\u4e2a\u957f\u5411\u91cf\n        '''\n        concat = np.zeros((0,1))\n        for node in tree_nodes:\n            concat = np.concatenate((concat, node.data))\n        return concat\n\n    def calc_delta(self, parent_delta, parent):\n        '''\n        \u8ba1\u7b97\u6bcf\u4e2a\u8282\u70b9\u7684delta\n        '''\n        parent.delta = parent_delta\n        if parent.children:\n            # \u6839\u636e\u5f0f2\u8ba1\u7b97\u6bcf\u4e2a\u5b50\u8282\u70b9\u7684delta\n            children_delta = np.dot(self.W.T, parent_delta) * (\n                self.activator.backward(parent.children_data)\n            )\n            # slices = [(\u5b50\u8282\u70b9\u7f16\u53f7\uff0c\u5b50\u8282\u70b9delta\u8d77\u59cb\u4f4d\u7f6e\uff0c\u5b50\u8282\u70b9delta\u7ed3\u675f\u4f4d\u7f6e)]\n            slices = [(i, i * self.node_width, \n                        (i + 1) * self.node_width)\n                        for i in range(self.child_count)]\n            # \u9488\u5bf9\u6bcf\u4e2a\u5b50\u8282\u70b9\uff0c\u9012\u5f52\u8c03\u7528calc_delta\u51fd\u6570\n            for s in slices:\n                self.calc_delta(children_delta[s[1]:s[2]], \n                                parent.children[s[0]])\n\n    def calc_gradient(self, parent):\n        '''\n        \u8ba1\u7b97\u6bcf\u4e2a\u8282\u70b9\u6743\u91cd\u7684\u68af\u5ea6\uff0c\u5e76\u5c06\u5b83\u4eec\u6c42\u548c\uff0c\u5f97\u5230\u6700\u7ec8\u7684\u68af\u5ea6\n        '''\n        W_grad = np.zeros((self.node_width, \n                            self.node_width * self.child_count))\n        b_grad = np.zeros((self.node_width, 1))\n        if not parent.children:\n            return W_grad, b_grad\n        parent.W_grad = np.dot(parent.delta, parent.children_data.T)\n        parent.b_grad = parent.delta\n        W_grad += parent.W_grad\n        b_grad += parent.b_grad\n        for child in parent.children:\n            W, b = self.calc_gradient(child)\n            W_grad += W\n            b_grad += b\n        return W_grad, b_grad\n\n    def dump(self, **kwArgs):\n        print('root.data: %s' % self.root.data)\n        print('root.children_data: %s' % self.root.children_data)\n        if kwArgs.has_key('dump_grad'):\n            print('W_grad: %s' % self.W_grad)\n            print('b_grad: %s' % self.b_grad)\n\n\ndef data_set():\n    children = [\n        TreeNode(np.array([[1],[2]])),\n        TreeNode(np.array([[3],[4]])),\n        TreeNode(np.array([[5],[6]]))\n    ]\n    d = np.array([[0.5],[0.8]])\n    return children, d\n\n\ndef gradient_check():\n    '''\n    \u68af\u5ea6\u68c0\u67e5\n    '''\n    # \u8bbe\u8ba1\u4e00\u4e2a\u8bef\u5dee\u51fd\u6570\uff0c\u53d6\u6240\u6709\u8282\u70b9\u8f93\u51fa\u9879\u4e4b\u548c\n    error_function = lambda o: o.sum()\n    \n    rnn = RecursiveLayer(2, 2, IdentityActivator(), 1e-3)\n\n    # \u8ba1\u7b97forward\u503c\n    x, d = data_set()\n    rnn.forward(x[0], x[1])\n    rnn.forward(rnn.root, x[2])\n    \n    # \u6c42\u53d6sensitivity map\n    sensitivity_array = np.ones((rnn.node_width, 1),\n                                dtype=np.float64)\n    # \u8ba1\u7b97\u68af\u5ea6\n    rnn.backward(sensitivity_array)\n    \n    # \u68c0\u67e5\u68af\u5ea6\n    epsilon = 10e-4\n    for i in range(rnn.W.shape[0]):\n        for j in range(rnn.W.shape[1]):\n            rnn.W[i,j] += epsilon\n            rnn.reset_state()\n            rnn.forward(x[0], x[1])\n            rnn.forward(rnn.root, x[2])\n            err1 = error_function(rnn.root.data)\n            rnn.W[i,j] -= 2*epsilon\n            rnn.reset_state()\n            rnn.forward(x[0], x[1])\n            rnn.forward(rnn.root, x[2])\n            err2 = error_function(rnn.root.data)\n            expect_grad = (err1 - err2) / (2 * epsilon)\n            rnn.W[i,j] += epsilon\n            print('weights(%d,%d): expected - actural %.4e - %.4e' % (\n                i, j, expect_grad, rnn.W_grad[i,j]))\n    return rnn\n\n\ndef test():\n    children, d = data_set()\n    rnn = RecursiveLayer(2, 2, IdentityActivator(), 1e-3)\n    rnn.forward(children[0], children[1])\n    rnn.dump()\n    rnn.forward(rnn.root, children[2])\n    rnn.dump()\n    rnn.backward(d)\n    rnn.dump(dump_grad='true')\n    return rnn", "src/py2.x/dl/cnn.py": "#!/usr/bin/env python\n# -*- coding: UTF-8 -*-\n\n\nfrom __future__ import print_function\nimport numpy as np\nfrom activators import ReluActivator, IdentityActivator\n\n\n# \u83b7\u53d6\u5377\u79ef\u533a\u57df\ndef get_patch(input_array, i, j, filter_width,\n              filter_height, stride):\n    '''\n    \u4ece\u8f93\u5165\u6570\u7ec4\u4e2d\u83b7\u53d6\u672c\u6b21\u5377\u79ef\u7684\u533a\u57df\uff0c\n    \u81ea\u52a8\u9002\u914d\u8f93\u5165\u4e3a2D\u548c3D\u7684\u60c5\u51b5\n    '''\n    start_i = i * stride\n    start_j = j * stride\n    if input_array.ndim == 2:\n        return input_array[ \n            start_i : start_i + filter_height, \n            start_j : start_j + filter_width]\n    elif input_array.ndim == 3:\n        return input_array[:, \n            start_i : start_i + filter_height, \n            start_j : start_j + filter_width]\n       \n\n# \u83b7\u53d6\u4e00\u4e2a2D\u533a\u57df\u7684\u6700\u5927\u503c\u6240\u5728\u7684\u7d22\u5f15\ndef get_max_index(array):\n    max_i = 0\n    max_j = 0\n    max_value = array[0,0]\n    for i in range(array.shape[0]):\n        for j in range(array.shape[1]):\n            if array[i,j] > max_value:\n                max_value = array[i,j]\n                max_i, max_j = i, j\n    return max_i, max_j\n\n\n# \u8ba1\u7b97\u5377\u79ef\ndef conv(input_array, \n         kernel_array,\n         output_array, \n         stride, bias):\n    '''\n    \u8ba1\u7b97\u5377\u79ef\uff0c\u81ea\u52a8\u9002\u914d\u8f93\u5165\u4e3a2D\u548c3D\u7684\u60c5\u51b5\n    conv\u51fd\u6570\u5b9e\u73b0\u4e862\u7ef4\u548c3\u7ef4\u6570\u7ec4\u7684\u5377\u79ef\n    '''\n    channel_number = input_array.ndim\n    output_width = output_array.shape[1]\n    output_height = output_array.shape[0]\n    kernel_width = kernel_array.shape[-1]\n    kernel_height = kernel_array.shape[-2]\n    for i in range(output_height):\n        for j in range(output_width):\n            output_array[i][j] = (    \n                get_patch(input_array, i, j, kernel_width, \n                    kernel_height, stride) * kernel_array\n                ).sum() + bias\n\n\n# \u4e3a\u6570\u7ec4\u589e\u52a0Zero padding\ndef padding(input_array, zp):\n    '''\n    \u4e3a\u6570\u7ec4\u589e\u52a0Zero padding\uff0c\u81ea\u52a8\u9002\u914d\u8f93\u5165\u4e3a2D\u548c3D\u7684\u60c5\u51b5\n    '''\n    if zp == 0:\n        return input_array\n    else:\n        if input_array.ndim == 3:\n            input_width = input_array.shape[2]\n            input_height = input_array.shape[1]\n            input_depth = input_array.shape[0]\n            padded_array = np.zeros((\n                input_depth, \n                input_height + 2 * zp,\n                input_width + 2 * zp))\n            padded_array[:,\n                zp : zp + input_height,\n                zp : zp + input_width] = input_array\n            return padded_array\n        elif input_array.ndim == 2:\n            input_width = input_array.shape[1]\n            input_height = input_array.shape[0]\n            padded_array = np.zeros((\n                input_height + 2 * zp,\n                input_width + 2 * zp))\n            padded_array[zp : zp + input_height,\n                zp : zp + input_width] = input_array\n            return padded_array\n\n\n# \u5bf9numpy\u6570\u7ec4\u8fdb\u884celement wise\u64cd\u4f5c\ndef element_wise_op(array, op):\n    '''\n    Desc:\n        element_wise_op\u51fd\u6570\u5b9e\u73b0\u4e86\u5bf9numpy\u6570\u7ec4\u8fdb\u884c\u6309\u5143\u7d20\u64cd\u4f5c\uff0c\u5e76\u5c06\u8fd4\u56de\u503c\u5199\u56de\u5230\u6570\u7ec4\u4e2d\n    '''\n    for i in np.nditer(array,\n                       op_flags=['readwrite']):\n        i[...] = op(i)\n\n\nclass Filter(object):\n    '''\n    Desc:\n        Filter\u7c7b\u4fdd\u5b58\u4e86\u5377\u79ef\u5c42\u7684\u53c2\u6570\u4ee5\u53ca\u68af\u5ea6\uff0c\u5e76\u4e14\u5b9e\u73b0\u4e86\u7528\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5\u6765\u66f4\u65b0\u53c2\u6570\u3002\n        \u6211\u4eec\u5bf9\u53c2\u6570\u7684\u521d\u59cb\u5316\u91c7\u7528\u4e86\u5e38\u7528\u7684\u7b56\u7565\uff0c\u5373: \u6743\u91cd\u968f\u673a\u521d\u59cb\u5316\u4e3a\u4e00\u4e2a\u5f88\u5c0f\u7684\u503c\uff0c\u800c\u504f\u7f6e\u9879\u521d\u59cb\u5316\u4e3a0\u3002\n    '''\n    def __init__(self, width, height, depth):\n        self.weights = np.random.uniform(-1e-4, 1e-4,\n            (depth, height, width))\n        self.bias = 0\n        self.weights_grad = np.zeros(\n            self.weights.shape)\n        self.bias_grad = 0\n\n    def __repr__(self):\n        return 'filter weights:\\n%s\\nbias:\\n%s' % (\n            repr(self.weights), repr(self.bias))\n\n    def get_weights(self):\n        return self.weights\n\n    def get_bias(self):\n        return self.bias\n\n    def update(self, learning_rate):\n        self.weights -= learning_rate * self.weights_grad\n        self.bias -= learning_rate * self.bias_grad\n\n\nclass ConvLayer(object):\n    '''\n    Desc:\n        \u7528ConvLayer\u7c7b\u6765\u5b9e\u73b0\u4e00\u4e2a\u5377\u79ef\u5c42\u3002\u4e0b\u9762\u7684\u4ee3\u7801\u662f\u521d\u59cb\u5316\u4e00\u4e2a\u5377\u79ef\u5c42\uff0c\u53ef\u4ee5\u5728\u6784\u9020\u51fd\u6570\u4e2d\u8bbe\u7f6e\u5377\u79ef\u5c42\u7684\u8d85\u53c2\u6570\u3002\n    '''\n    def __init__(self, input_width, input_height, \n                 channel_number, filter_width, \n                 filter_height, filter_number, \n                 zero_padding, stride, activator,\n                 learning_rate):\n        self.input_width = input_width\n        self.input_height = input_height\n        self.channel_number = channel_number\n        self.filter_width = filter_width\n        self.filter_height = filter_height\n        self.filter_number = filter_number\n        self.zero_padding = zero_padding\n        self.stride = stride\n        self.output_width = \\\n            ConvLayer.calculate_output_size(\n            self.input_width, filter_width, zero_padding,\n            stride)\n        self.output_height = \\\n            ConvLayer.calculate_output_size(\n            self.input_height, filter_height, zero_padding,\n            stride)\n        self.output_array = np.zeros((self.filter_number, \n            self.output_height, self.output_width))\n        self.filters = []\n        for i in range(filter_number):\n            self.filters.append(Filter(filter_width, \n                filter_height, self.channel_number))\n        self.activator = activator\n        self.learning_rate = learning_rate\n\n    def forward(self, input_array):\n        '''\n        Desc:\n            \u8ba1\u7b97\u5377\u79ef\u5c42\u7684\u8f93\u51fa,\u8f93\u51fa\u7ed3\u679c\u4fdd\u5b58\u5728 self.output_array\n            ConvLayer \u7c7b\u7684 forward \u65b9\u6cd5\u5b9e\u73b0\u4e86\u5377\u79ef\u5c42\u7684\u524d\u5411\u8ba1\u7b97\uff08\u5373\u8ba1\u7b97\u6839\u636e\u8f93\u5165\u6765\u8ba1\u7b97\u5377\u79ef\u5c42\u7684\u8f93\u51fa\uff09\n        '''\n        self.input_array = input_array\n        self.padded_input_array = padding(input_array,\n            self.zero_padding)\n        for f in range(self.filter_number):\n            filter = self.filters[f]\n            conv(self.padded_input_array, \n                filter.get_weights(), self.output_array[f],\n                self.stride, filter.get_bias())\n        element_wise_op(self.output_array, \n                        self.activator.forward)\n        \n    def backward(self, input_array, sensitivity_array, \n                 activator):\n        '''\n        \u8ba1\u7b97\u4f20\u9012\u7ed9\u524d\u4e00\u5c42\u7684\u8bef\u5dee\u9879\uff0c\u4ee5\u53ca\u8ba1\u7b97\u6bcf\u4e2a\u6743\u91cd\u7684\u68af\u5ea6\n        \u524d\u4e00\u5c42\u7684\u8bef\u5dee\u9879\u4fdd\u5b58\u5728self.delta_array\n        \u68af\u5ea6\u4fdd\u5b58\u5728Filter\u5bf9\u8c61\u7684weights_grad\n        '''\n        self.forward(input_array)\n        self.bp_sensitivity_map(sensitivity_array,\n                                activator)\n        self.bp_gradient(sensitivity_array)\n\n    def update(self):\n        '''\n        \u6309\u7167\u68af\u5ea6\u4e0b\u964d\uff0c\u66f4\u65b0\u6743\u91cd\n        '''\n        for filter in self.filters:\n            filter.update(self.learning_rate)\n\n    def bp_sensitivity_map(self, sensitivity_array,\n                           activator):\n        '''\n        \u8ba1\u7b97\u4f20\u9012\u5230\u4e0a\u4e00\u5c42\u7684sensitivity map\n        sensitivity_array: \u672c\u5c42\u7684sensitivity map\n        activator: \u4e0a\u4e00\u5c42\u7684\u6fc0\u6d3b\u51fd\u6570\n        '''\n        # \u5904\u7406\u5377\u79ef\u6b65\u957f\uff0c\u5bf9\u539f\u59cbsensitivity map\u8fdb\u884c\u6269\u5c55\n        expanded_array = self.expand_sensitivity_map(\n            sensitivity_array)\n        # full\u5377\u79ef\uff0c\u5bf9sensitivitiy map\u8fdb\u884czero padding\n        # \u867d\u7136\u539f\u59cb\u8f93\u5165\u7684zero padding\u5355\u5143\u4e5f\u4f1a\u83b7\u5f97\u6b8b\u5dee\n        # \u4f46\u8fd9\u4e2a\u6b8b\u5dee\u4e0d\u9700\u8981\u7ee7\u7eed\u5411\u4e0a\u4f20\u9012\uff0c\u56e0\u6b64\u5c31\u4e0d\u8ba1\u7b97\u4e86\n        expanded_width = expanded_array.shape[2]\n        zp = (self.input_width +  \n              self.filter_width - 1 - expanded_width) / 2\n        padded_array = padding(expanded_array, zp)\n        # \u521d\u59cb\u5316delta_array\uff0c\u7528\u4e8e\u4fdd\u5b58\u4f20\u9012\u5230\u4e0a\u4e00\u5c42\u7684\n        # sensitivity map\n        self.delta_array = self.create_delta_array()\n        # \u5bf9\u4e8e\u5177\u6709\u591a\u4e2afilter\u7684\u5377\u79ef\u5c42\u6765\u8bf4\uff0c\u6700\u7ec8\u4f20\u9012\u5230\u4e0a\u4e00\u5c42\u7684\n        # sensitivity map\u76f8\u5f53\u4e8e\u6240\u6709\u7684filter\u7684\n        # sensitivity map\u4e4b\u548c\n        for f in range(self.filter_number):\n            filter = self.filters[f]\n            # \u5c06filter\u6743\u91cd\u7ffb\u8f6c180\u5ea6\n            flipped_weights = np.array(map(\n                lambda i: np.rot90(i, 2), \n                filter.get_weights()))\n            # \u8ba1\u7b97\u4e0e\u4e00\u4e2afilter\u5bf9\u5e94\u7684delta_array\n            delta_array = self.create_delta_array()\n            for d in range(delta_array.shape[0]):\n                conv(padded_array[f], flipped_weights[d],\n                    delta_array[d], 1, 0)\n            self.delta_array += delta_array\n        # \u5c06\u8ba1\u7b97\u7ed3\u679c\u4e0e\u6fc0\u6d3b\u51fd\u6570\u7684\u504f\u5bfc\u6570\u505aelement-wise\u4e58\u6cd5\u64cd\u4f5c\n        derivative_array = np.array(self.input_array)\n        element_wise_op(derivative_array, \n                        activator.backward)\n        self.delta_array *= derivative_array\n\n    def bp_gradient(self, sensitivity_array):\n        # \u5904\u7406\u5377\u79ef\u6b65\u957f\uff0c\u5bf9\u539f\u59cbsensitivity map\u8fdb\u884c\u6269\u5c55\n        expanded_array = self.expand_sensitivity_map(\n            sensitivity_array)\n        for f in range(self.filter_number):\n            # \u8ba1\u7b97\u6bcf\u4e2a\u6743\u91cd\u7684\u68af\u5ea6\n            filter = self.filters[f]\n            for d in range(filter.weights.shape[0]):\n                conv(self.padded_input_array[d], \n                     expanded_array[f],\n                     filter.weights_grad[d], 1, 0)\n            # \u8ba1\u7b97\u504f\u7f6e\u9879\u7684\u68af\u5ea6\n            filter.bias_grad = expanded_array[f].sum()\n\n    def expand_sensitivity_map(self, sensitivity_array):\n        depth = sensitivity_array.shape[0]\n        # \u786e\u5b9a\u6269\u5c55\u540esensitivity map\u7684\u5927\u5c0f\n        # \u8ba1\u7b97stride\u4e3a1\u65f6sensitivity map\u7684\u5927\u5c0f\n        expanded_width = (self.input_width - \n            self.filter_width + 2 * self.zero_padding + 1)\n        expanded_height = (self.input_height - \n            self.filter_height + 2 * self.zero_padding + 1)\n        # \u6784\u5efa\u65b0\u7684sensitivity_map\n        expand_array = np.zeros((depth, expanded_height, \n                                 expanded_width))\n        # \u4ece\u539f\u59cbsensitivity map\u62f7\u8d1d\u8bef\u5dee\u503c\n        for i in range(self.output_height):\n            for j in range(self.output_width):\n                i_pos = i * self.stride\n                j_pos = j * self.stride\n                expand_array[:,i_pos,j_pos] = \\\n                    sensitivity_array[:,i,j]\n        return expand_array\n\n    def create_delta_array(self):\n        return np.zeros((self.channel_number,\n            self.input_height, self.input_width))\n\n    @staticmethod\n    def calculate_output_size(input_size, filter_size, zero_padding, stride):\n        '''\n        Desc:\n            \u7528\u6765\u786e\u5b9a\u5377\u79ef\u5c42\u8f93\u51fa\u7684\u5927\u5c0f\n        '''\n        return (input_size - filter_size + \n            2 * zero_padding) / stride + 1\n\n\nclass MaxPoolingLayer(object):\n    def __init__(self, input_width, input_height, \n                 channel_number, filter_width, \n                 filter_height, stride):\n        self.input_width = input_width\n        self.input_height = input_height\n        self.channel_number = channel_number\n        self.filter_width = filter_width\n        self.filter_height = filter_height\n        self.stride = stride\n        self.output_width = (input_width - \n            filter_width) / self.stride + 1\n        self.output_height = (input_height -\n            filter_height) / self.stride + 1\n        self.output_array = np.zeros((self.channel_number,\n            self.output_height, self.output_width))\n\n    def forward(self, input_array):\n        for d in range(self.channel_number):\n            for i in range(self.output_height):\n                for j in range(self.output_width):\n                    self.output_array[d,i,j] = (    \n                        get_patch(input_array[d], i, j,\n                            self.filter_width, \n                            self.filter_height, \n                            self.stride).max())\n\n    def backward(self, input_array, sensitivity_array):\n        self.delta_array = np.zeros(input_array.shape)\n        for d in range(self.channel_number):\n            for i in range(self.output_height):\n                for j in range(self.output_width):\n                    patch_array = get_patch(\n                        input_array[d], i, j,\n                        self.filter_width, \n                        self.filter_height, \n                        self.stride)\n                    k, l = get_max_index(patch_array)\n                    self.delta_array[d, \n                        i * self.stride + k, \n                        j * self.stride + l] = \\\n                        sensitivity_array[d,i,j]\n\n\ndef init_test():\n    a = np.array(\n        [[[0,1,1,0,2],\n          [2,2,2,2,1],\n          [1,0,0,2,0],\n          [0,1,1,0,0],\n          [1,2,0,0,2]],\n         [[1,0,2,2,0],\n          [0,0,0,2,0],\n          [1,2,1,2,1],\n          [1,0,0,0,0],\n          [1,2,1,1,1]],\n         [[2,1,2,0,0],\n          [1,0,0,1,0],\n          [0,2,1,0,1],\n          [0,1,2,2,2],\n          [2,1,0,0,1]]])\n    b = np.array(\n        [[[0,1,1],\n          [2,2,2],\n          [1,0,0]],\n         [[1,0,2],\n          [0,0,0],\n          [1,2,1]]])\n    cl = ConvLayer(5,5,3,3,3,2,1,2,IdentityActivator(),0.001)\n    cl.filters[0].weights = np.array(\n        [[[-1,1,0],\n          [0,1,0],\n          [0,1,1]],\n         [[-1,-1,0],\n          [0,0,0],\n          [0,-1,0]],\n         [[0,0,-1],\n          [0,1,0],\n          [1,-1,-1]]], dtype=np.float64)\n    cl.filters[0].bias=1\n    cl.filters[1].weights = np.array(\n        [[[1,1,-1],\n          [-1,-1,1],\n          [0,-1,1]],\n         [[0,1,0],\n         [-1,0,-1],\n          [-1,1,0]],\n         [[-1,0,0],\n          [-1,0,1],\n          [-1,0,0]]], dtype=np.float64)\n    return a, b, cl\n\n\ndef test():\n    a, b, cl = init_test()\n    cl.forward(a)\n    print(cl.output_array)\n\ndef test_bp():\n    a, b, cl = init_test()\n    cl.backward(a, b, IdentityActivator())\n    cl.update()\n    print(cl.filters[0])\n    print(cl.filters[1])\n\n\ndef gradient_check():\n    '''\n    \u68af\u5ea6\u68c0\u67e5\n    '''\n    # \u8bbe\u8ba1\u4e00\u4e2a\u8bef\u5dee\u51fd\u6570\uff0c\u53d6\u6240\u6709\u8282\u70b9\u8f93\u51fa\u9879\u4e4b\u548c\n    error_function = lambda o: o.sum()\n    \n    # \u8ba1\u7b97forward\u503c\n    a, b, cl = init_test()\n    cl.forward(a)\n    \n    # \u6c42\u53d6sensitivity map\n    sensitivity_array = np.ones(cl.output_array.shape,\n                                dtype=np.float64)\n    # \u8ba1\u7b97\u68af\u5ea6\n    cl.backward(a, sensitivity_array,\n                  IdentityActivator())\n    # \u68c0\u67e5\u68af\u5ea6\n    epsilon = 10e-4\n    for d in range(cl.filters[0].weights_grad.shape[0]):\n        for i in range(cl.filters[0].weights_grad.shape[1]):\n            for j in range(cl.filters[0].weights_grad.shape[2]):\n                cl.filters[0].weights[d,i,j] += epsilon\n                cl.forward(a)\n                err1 = error_function(cl.output_array)\n                cl.filters[0].weights[d,i,j] -= 2*epsilon\n                cl.forward(a)\n                err2 = error_function(cl.output_array)\n                expect_grad = (err1 - err2) / (2 * epsilon)\n                cl.filters[0].weights[d,i,j] += epsilon\n                print('weights(%d,%d,%d): expected - actural %f - %f' % (\n                    d, i, j, expect_grad, cl.filters[0].weights_grad[d,i,j]))\n\n\ndef init_pool_test():\n    a = np.array(\n        [[[1,1,2,4],\n          [5,6,7,8],\n          [3,2,1,0],\n          [1,2,3,4]],\n         [[0,1,2,3],\n          [4,5,6,7],\n          [8,9,0,1],\n          [3,4,5,6]]], dtype=np.float64)\n\n    b = np.array(\n        [[[1,2],\n          [2,4]],\n         [[3,5],\n          [8,2]]], dtype=np.float64)\n\n    mpl = MaxPoolingLayer(4,4,2,2,2,2)\n\n    return a, b, mpl\n\n\ndef test_pool():\n    a, b, mpl = init_pool_test()\n    mpl.forward(a)\n    print('input array:\\n%s\\noutput array:\\n%s' % (a, mpl.output_array))\n\n\ndef test_pool_bp():\n    a, b, mpl = init_pool_test()\n    mpl.backward(a, b)\n    print('input array:\\n%s\\nsensitivity array:\\n%s\\ndelta array:\\n%s' % (a, b, mpl.delta_array))", "src/py3.x/ml/15.BigData_MapReduce/py27dbg.py": "'''\nCreated on Feb 27, 2011\nMapReduce version of Pegasos SVM\nUsing mrjob to automate job flow\nAuthor: Peter\n'''\nfrom mrjob.job import MRJob\n\nimport pickle\nfrom numpy import *\n\nclass MRsvm(MRJob):\n                                                 \n    def map(self, mapperId, inVals): #needs exactly 2 arguments\n        if False: yield\n        yield (1, 22)\n\n    def reduce(self, _, packedVals):\n        yield \"fuck ass\" \n        \n    def steps(self):\n        return ([self.mr(mapper=self.map, reducer=self.reduce)])\n\nif __name__ == '__main__':\n    MRsvm.run()\n", "src/py3.x/ml/15.BigData_MapReduce/mrMeanReducer.py": "#!/usr/bin/python\n# coding:utf-8\n\n'''\nCreated on 2017-04-06\nUpdate  on 2017-11-17\nAuthor: Peter/ApacheCN-xy/\u7247\u523b\nGitHub: https://github.com/apachecn/AiLearning\n'''\n\nfrom __future__ import print_function\n\nimport sys\n\n'''\n    mapper \u63a5\u53d7\u539f\u59cb\u7684\u8f93\u5165\u5e76\u4ea7\u751f\u4e2d\u95f4\u503c\u4f20\u9012\u7ed9 reducer\u3002\n    \u5f88\u591a\u7684mapper\u662f\u5e76\u884c\u6267\u884c\u7684\uff0c\u6240\u4ee5\u9700\u8981\u5c06\u8fd9\u4e9bmapper\u7684\u8f93\u51fa\u5408\u5e76\u6210\u4e00\u4e2a\u503c\u3002\n    \u5373: \u5c06\u4e2d\u95f4\u7684 key/value \u5bf9\u8fdb\u884c\u7ec4\u5408\u3002\n'''\n\n\ndef read_input(file):\n    for line in file:\n        yield line.rstrip()\t\t\t\t\t\t# \u8fd4\u56de\u503c\u4e2d\u5305\u542b\u8f93\u5165\u6587\u4ef6\u7684\u6bcf\u4e00\u884c\u7684\u6570\u636e\u7684\u4e00\u4e2a\u5927\u7684List\n\n\ninput = read_input(sys.stdin)\t\t\t\t\t# \u521b\u5efa\u4e00\u4e2a\u8f93\u5165\u7684\u6570\u636e\u884c\u7684\u5217\u8868list\n\n# \u5c06\u8f93\u5165\u884c\u5206\u5272\u6210\u5355\u72ec\u7684\u9879\u76ee\u5e76\u5b58\u50a8\u5728\u5217\u8868\u7684\u5217\u8868\u4e2d\nmapperOut = [line.split('\\t') for line in input]\n# \u8f93\u5165 \u6570\u636e\u7684\u4e2a\u6570\uff0cn\u4e2a\u6570\u636e\u7684\u5747\u503c\uff0cn\u4e2a\u6570\u636e\u5e73\u65b9\u4e4b\u540e\u7684\u5747\u503c\nprint (mapperOut)\n\n# \u7d2f\u8ba1\u6837\u672c\u603b\u548c\uff0c\u603b\u548c \u548c \u5e73\u5206\u548c\u7684\u603b\u548c\ncumN, cumVal, cumSumSq = 0.0, 0.0, 0.0\nfor instance in mapperOut:\n    nj = float(instance[0])\n    cumN += nj\n    cumVal += nj*float(instance[1])\n    cumSumSq += nj*float(instance[2])\n\n# \u8ba1\u7b97\u5747\u503c( varSum\u662f\u8ba1\u7b97\u65b9\u5dee\u7684\u5c55\u5f00\u5f62\u5f0f )\nmean_ = cumVal/cumN\nvarSum = (cumSumSq - 2*mean_*cumVal + cumN*mean_*mean_)/cumN\n# \u8f93\u51fa \u6570\u636e\u603b\u91cf\uff0c\u5747\u503c\uff0c\u5e73\u65b9\u7684\u5747\u503c\uff08\u65b9\u5dee\uff09\nprint(\"\u6570\u636e\u603b\u91cf: %d\\t\u5747\u503c: %f\\t\u65b9\u5dee: %f\" % (cumN, mean_, varSum))\nprint(\"reduce report: still alive\", file=sys.stderr)\n", "src/py3.x/ml/15.BigData_MapReduce/mrSVM.py": "#!/usr/bin/python\n# coding:utf-8\n\n'''\nCreated on 2017-04-07\nUpdate  on 2017-11-17\nAuthor: Peter/ApacheCN-xy/\u7247\u523b\nGitHub: https://github.com/apachecn/AiLearning\n'''\n\nimport pickle\nfrom numpy import *\nfrom mrjob.job import MRJob\nfrom mrjob.step import MRStep\n\n\nclass MRsvm(MRJob):\n    DEFAULT_INPUT_PROTOCOL = 'json_value'\n\n    def __init__(self, *args, **kwargs):\n        super(MRsvm, self).__init__(*args, **kwargs)\n        self.data = pickle.load(open('/opt/git/MachineLearnidata/15.BigData_MapReduce/svmDat27', 'r'))\n        self.w = 0\n        self.eta = 0.69\n        self.dataList = []\n        self.k = self.options.batchsize\n        self.numMappers = 1\n        self.t = 1  # iteration number\n\n    def configure_args(self):\n        super(MRsvm, self).configure_args()\n        self.add_passthru_arg(\n            '--iterations', dest='iterations', default=2, type=int,\n            help='T: number of iterations to run')\n        self.add_passthru_arg(\n            '--batchsize', dest='batchsize', default=100, type=int,\n            help='k: number of data points in a batch')\n\n    def map(self, mapperId, inVals):  # \u9700\u8981 2 \u4e2a\u53c2\u6570\n        # input: nodeId, ('w', w-vector) OR nodeId, ('x', int)\n        if False:\n            yield\n        if inVals[0] == 'w':                  # \u79ef\u7d2f w\u5411\u91cf\n            self.w = inVals[1]\n        elif inVals[0] == 'x':\n            self.dataList.append(inVals[1])   # \u7d2f\u79ef\u6570\u636e\u70b9\u8ba1\u7b97\n        elif inVals[0] == 't':                # \u8fed\u4ee3\u6b21\u6570\n            self.t = inVals[1]\n        else:\n            self.eta = inVals                 # \u8fd9\u7528\u4e8e debug\uff0c eta\u672a\u5728map\u4e2d\u4f7f\u7528\n\n    def map_fin(self):\n        labels = self.data[:, -1]\n        X = self.data[:, :-1]                # \u5c06\u6570\u636e\u91cd\u65b0\u5f62\u6210 X \u548c Y\n        if self.w == 0:\n            self.w = [0.001] * shape(X)[1]   # \u5728\u7b2c\u4e00\u6b21\u8fed\u4ee3\u65f6\uff0c\u521d\u59cb\u5316 w\n        for index in self.dataList:\n            p = mat(self.w)*X[index, :].T    # calc p=w*dataSet[key].T\n            if labels[index]*p < 1.0:\n                yield (1, ['u', index])      # \u786e\u4fdd\u4e00\u5207\u6570\u636e\u5305\u542b\u76f8\u540c\u7684key\n        yield (1, ['w', self.w])             # \u5b83\u4eec\u5c06\u5728\u540c\u4e00\u4e2a reducer\n        yield (1, ['t', self.t])\n\n    def reduce(self, _, packedVals):\n        for valArr in packedVals:            # \u4ece\u6d41\u8f93\u5165\u83b7\u53d6\u503c\n            if valArr[0] == 'u':\n                self.dataList.append(valArr[1])\n            elif valArr[0] == 'w':\n                self.w = valArr[1]\n            elif valArr[0] == 't':\n                self.t = valArr[1]\n\n        labels = self.data[:, -1]\n        X = self.data[:, 0:-1]\n        wMat = mat(self.w)\n        wDelta = mat(zeros(len(self.w)))\n\n        for index in self.dataList:\n            wDelta += float(labels[index]) * X[index, :]  # wDelta += label*dataSet\n        eta = 1.0/(2.0*self.t)       # calc new: eta\n        # calc new: w = (1.0 - 1/t)*w + (eta/k)*wDelta\n        wMat = (1.0 - 1.0/self.t)*wMat + (eta/self.k)*wDelta\n        for mapperNum in range(1, self.numMappers+1):\n            yield (mapperNum, ['w', wMat.tolist()[0]])    # \u53d1\u51fa w\n            if self.t < self.options.iterations:\n                yield (mapperNum, ['t', self.t+1])        # \u589e\u91cf T\n                for j in range(self.k/self.numMappers):   # emit random ints for mappers iid\n                    yield (mapperNum, ['x', random.randint(shape(self.data)[0])])\n\n    def steps(self):\n        return [MRStep(mapper=self.map, reducer=self.reduce, mapper_final=self.map_fin)] * self.options.iterations\n\n\nif __name__ == '__main__':\n    MRsvm.run()\n", "src/py3.x/ml/15.BigData_MapReduce/wc.py": "#!/usr/bin/python\n# coding:utf8\nfrom mrjob.job import MRJob\n\n\nclass MRWordCountUtility(MRJob):\n\n    def __init__(self, *args, **kwargs):\n        super(MRWordCountUtility, self).__init__(*args, **kwargs)\n        self.chars = 0\n        self.words = 0\n        self.lines = 0\n\n    def mapper(self, _, line):\n        if False:\n            yield  # I'm a generator!\n\n        self.chars += len(line) + 1  # +1 for newline\n        self.words += sum(1 for word in line.split() if word.strip())\n        self.lines += 1\n\n    def mapper_final(self):\n        yield('chars', self.chars)\n        yield('words', self.words)\n        yield('lines', self.lines)\n\n    def reducer(self, key, values):\n        yield(key, sum(values))\n\n\nif __name__ == '__main__':\n    MRWordCountUtility.run()\n", "src/py3.x/ml/15.BigData_MapReduce/mrSVMkickStart.py": "'''\nCreated on Feb 27, 2011\n\nAuthor: Peter\n'''\nfrom mrjob.protocol import JSONProtocol\nfrom numpy import *\n\nfw=open('kickStart2.txt', 'w')\nfor i in [1]:\n    for j in range(100):\n        fw.write('[\"x\", %d]\\n' % random.randint(200))\nfw.close()", "src/py3.x/ml/15.BigData_MapReduce/proximalSVM.py": "#!/usr/bin/python\n# coding:utf8\n'''\nCreated on 2011-02-25\nUpdate  on 2017-06-20\nAuthor: Peter/ApacheCN-xy/\u7247\u523b\nGitHub: https://github.com/apachecn/AiLearning\n'''\nimport base64\nimport pickle\n\nimport numpy\n\n\ndef map(key, value):\n   # input key= class for one training example, e.g. \"-1.0\"\n   classes = [float(item) for item in key.split(\",\")]   # e.g. [-1.0]\n   D = numpy.diag(classes)\n\n   # input value = feature vector for one training example, e.g. \"3.0, 7.0, 2.0\"\n   featurematrix = [float(item) for item in value.split(\",\")]\n   A = numpy.matrix(featurematrix)\n\n   # create matrix E and vector e\n   e = numpy.matrix(numpy.ones(len(A)).reshape(len(A), 1))\n   E = numpy.matrix(numpy.append(A, -e, axis=1))\n\n   # create a tuple with the values to be used by reducer\n   # and encode it with base64 to avoid potential trouble with '\\t' and '\\n' used\n   # as default separators in Hadoop Streaming\n   producedvalue = base64.b64encode(pickle.dumps((E.T*E, E.T*D*e)))\n\n   # note: a single constant key \"producedkey\" sends to only one reducer\n   # somewhat \"atypical\" due to low degree of parallism on reducer side\n   print(\"producedkey\\t%s\" % (producedvalue))\n\ndef reduce(key, values, mu=0.1):\n  sumETE = None\n  sumETDe = None\n\n  # key isn't used, so ignoring it with _ (underscore).\n  for _, value in values:\n    # unpickle values\n    ETE, ETDe = pickle.loads(base64.b64decode(value))\n    if sumETE == None:\n      # create the I/mu with correct dimensions\n      sumETE = numpy.matrix(numpy.eye(ETE.shape[1])/mu)\n    sumETE += ETE\n\n    if sumETDe == None:\n      # create sumETDe with correct dimensions\n      sumETDe = ETDe\n    else:\n      sumETDe += ETDe\n\n    # note: omega = result[:-1] and gamma = result[-1]\n    # but printing entire vector as output\n    result = sumETE.I*sumETDe\n    print(\"%s\\t%s\" % (key, str(result.tolist())))\n", "src/py3.x/ml/15.BigData_MapReduce/pegasos.py": "#!/usr/bin/python\n# coding:utf-8\n\n'''\nCreated on 2017-04-07\nUpdate  on 2017-11-17\nAuthor: Peter/ApacheCN-xy/\u7247\u523b\nGitHub: https://github.com/apachecn/AiLearning\n'''\n\nfrom numpy import *\n\n\ndef loadDataSet(fileName):\n    dataMat = []\n    labelMat = []\n    fr = open(fileName)\n    for line in fr.readlines():\n        lineArr = line.strip().split('\\t')\n        # dataMat.append([float(lineArr[0]), float(lineArr[1]), float(lineArr[2])])\n        dataMat.append([float(lineArr[0]), float(lineArr[1])])\n        labelMat.append(float(lineArr[2]))\n    return dataMat, labelMat\n\n\ndef seqPegasos(dataSet, labels, lam, T):\n    m, n = shape(dataSet)\n    w = zeros(n)\n    for t in range(1, T+1):\n        i = random.randint(m)\n        eta = 1.0/(lam*t)\n        p = predict(w, dataSet[i, :])\n        if labels[i]*p < 1:\n            w = (1.0 - 1/t)*w + eta*labels[i]*dataSet[i, :]\n        else:\n            w = (1.0 - 1/t)*w\n        print(w)\n    return w\n\n\ndef predict(w, x):\n    return w*x.T  # \u5c31\u662f\u9884\u6d4b y \u7684\u503c\n\n\ndef batchPegasos(dataSet, labels, lam, T, k):\n    \"\"\"batchPegasos()\n\n    Args:\n        dataMat    \u7279\u5f81\u96c6\u5408\n        labels     \u5206\u7c7b\u7ed3\u679c\u96c6\u5408\n        lam        \u56fa\u5b9a\u503c\n        T          \u8fed\u4ee3\u6b21\u6570\n        k          \u5f85\u5904\u7406\u5217\u8868\u5927\u5c0f\n    Returns:\n        w          \u56de\u5f52\u7cfb\u6570\n    \"\"\"\n    m, n = shape(dataSet)\n    w = zeros(n)  # \u56de\u5f52\u7cfb\u6570\n    dataIndex = list(range(m))\n    for t in range(1, T+1):\n        wDelta = mat(zeros(n))  # \u91cd\u7f6e wDelta\n\n        # \u5b83\u662f\u5b66\u4e60\u7387\uff0c\u4ee3\u8868\u4e86\u6743\u91cd\u8c03\u6574\u5e45\u5ea6\u7684\u5927\u5c0f\u3002\uff08\u4e5f\u53ef\u4ee5\u7406\u89e3\u4e3a\u968f\u673a\u68af\u5ea6\u7684\u6b65\u957f\uff0c\u4f7f\u5b83\u4e0d\u65ad\u51cf\u5c0f\uff0c\u4fbf\u4e8e\u62df\u5408\uff09\n        # \u8f93\u5165T\u548cK\u5206\u522b\u8bbe\u5b9a\u4e86\u8fed\u4ee3\u6b21\u6570\u548c\u5f85\u5904\u7406\u5217\u8868\u7684\u5927\u5c0f\u3002\u5728T\u6b21\u8fed\u4ee3\u8fc7\u7a0b\u4e2d\uff0c\u6bcf\u6b21\u9700\u8981\u91cd\u65b0\u8ba1\u7b97eta\n        eta = 1.0/(lam*t)\n        random.shuffle(dataIndex)\n        for j in range(k):      # \u5168\u90e8\u7684\u8bad\u7ec3\u96c6  \u5185\u5faa\u73af\u4e2d\u6267\u884c\u6279\u5904\u7406\uff0c\u5c06\u5206\u7c7b\u9519\u8bef\u7684\u503c\u5168\u90e8\u505a\u7d2f\u52a0\u540e\u66f4\u65b0\u6743\u91cd\u5411\u91cf\n            i = dataIndex[j]\n            p = predict(w, dataSet[i, :])              # mapper \u4ee3\u7801\n\n            # \u5982\u679c\u9884\u6d4b\u6b63\u786e\uff0c\u5e76\u4e14\u9884\u6d4b\u7ed3\u679c\u7684\u7edd\u5bf9\u503c>=1\uff0c\u56e0\u4e3a\u6700\u5927\u95f4\u9694\u4e3a1, \u8ba4\u4e3a\u6ca1\u95ee\u9898\u3002\n            # \u5426\u5219\u7b97\u662f\u9884\u6d4b\u9519\u8bef, \u901a\u8fc7\u9884\u6d4b\u9519\u8bef\u7684\u7ed3\u679c\uff0c\u6765\u7d2f\u8ba1\u66f4\u65b0w.\n            if labels[i]*p < 1:                        # mapper \u4ee3\u7801\n                wDelta += labels[i]*dataSet[i, :].A    # \u7d2f\u79ef\u53d8\u5316\n        # w\u901a\u8fc7\u4e0d\u65ad\u7684\u968f\u673a\u68af\u5ea6\u7684\u65b9\u5f0f\u6765\u4f18\u5316\n        w = (1.0 - 1/t)*w + (eta/k)*wDelta             # \u5728\u6bcf\u4e2a T\u4e0a\u5e94\u7528\u66f4\u6539\n        # print '-----', w\n    # print '++++++', w\n    return w\n\n\ndatArr, labelList = loadDataSet('data/15.BigData_MapReduce/testSet.txt')\ndatMat = mat(datArr)\n# finalWs = seqPegasos(datMat, labelList, 2, 5000)\nfinalWs = batchPegasos(datMat, labelList, 2, 50, 100)\nprint(finalWs)\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nfig = plt.figure()\nax = fig.add_subplot(111)\nx1 = []\ny1 = []\nxm1 = []\nym1 = []\nfor i in range(len(labelList)):\n    if labelList[i] == 1.0:\n        x1.append(datMat[i, 0])\n        y1.append(datMat[i, 1])\n    else:\n        xm1.append(datMat[i, 0])\n        ym1.append(datMat[i, 1])\nax.scatter(x1, y1, marker='s', s=90)\nax.scatter(xm1, ym1, marker='o', s=50, c='red')\nx = arange(-6.0, 8.0, 0.1)\ny = (-finalWs[0, 0]*x - 0)/finalWs[0, 1]\n# y2 = (0.43799*x)/0.12316\ny2 = (0.498442*x)/0.092387  # 2 iterations\nax.plot(x, y)\nax.plot(x, y2, 'g-.')\nax.axis([-6, 8, -4, 5])\nax.legend(('50 Iterations', '2 Iterations'))\nplt.show()\n", "src/py3.x/ml/15.BigData_MapReduce/mrMean.py": "#!/usr/bin/python\n# coding:utf-8\n\n'''\nCreated on 2017-04-07\nUpdate  on 2017-11-17\nAuthor: Peter/ApacheCN-xy/\u7247\u523b\nGitHub: https://github.com/apachecn/AiLearning\n'''\n\nfrom mrjob.job import MRJob\nfrom mrjob.step import MRStep\n\n\nclass MRmean(MRJob):\n    def __init__(self, *args, **kwargs):  # \u5bf9\u6570\u636e\u521d\u59cb\u5316\n        super(MRmean, self).__init__(*args, **kwargs)\n        self.inCount = 0\n        self.inSum = 0\n        self.inSqSum = 0\n\n    # \u63a5\u53d7\u8f93\u5165\u6570\u636e\u6d41\n    def map(self, key, val):  # \u9700\u8981 2 \u4e2a\u53c2\u6570\uff0c\u6c42\u6570\u636e\u7684\u548c\u4e0e\u5e73\u65b9\u548c\n        if False:\n            yield\n        inVal = float(val)\n        self.inCount += 1\n        self.inSum += inVal\n        self.inSqSum += inVal*inVal\n\n    # \u6240\u6709\u8f93\u5165\u5230\u8fbe\u540e\u5f00\u59cb\u5904\u7406\n    def map_final(self):  # \u8ba1\u7b97\u6570\u636e\u7684\u5e73\u5747\u503c\uff0c\u5e73\u65b9\u7684\u5747\u503c\uff0c\u5e76\u8fd4\u56de\n        if self.inCount == 0:\n            return\n        mn = self.inSum/self.inCount\n        mnSq = self.inSqSum/self.inCount\n        yield (1, [self.inCount, mn, mnSq])\n\n    def reduce(self, key, packedValues):\n        cumN, cumVal, cumSumSq = 0.0, 0.0, 0.0\n        for valArr in packedValues:  # \u4ece\u8f93\u5165\u6d41\u4e2d\u83b7\u53d6\u503c\n            nj = float(valArr[0])\n            cumN += nj\n            cumVal += nj*float(valArr[1])\n            cumSumSq += nj*float(valArr[2])\n        mean = cumVal/cumN\n        var = (cumSumSq - 2*mean*cumVal + cumN*mean*mean)/cumN\n        yield (mean, var)  # \u53d1\u51fa\u5e73\u5747\u503c\u548c\u65b9\u5dee\n\n    def steps(self):\n        \"\"\"\n        step\u65b9\u6cd5\u5b9a\u4e49\u6267\u884c\u7684\u6b65\u9aa4\u3002\n        \u6267\u884c\u987a\u5e8f\u4e0d\u5fc5\u5b8c\u5168\u9075\u5faamap-reduce\u6a21\u5f0f\u3002\n        \u4f8b\u5982: \n            1. map-reduce-reduce-reduce\n            2. map-reduce-map-reduce-map-reduce\n        \u5728step\u65b9\u6cd5\u91cc\uff0c\u9700\u8981\u4e3amrjob\u6307\u5b9amapper\u548creducer\u7684\u540d\u79f0\u3002\u5982\u679c\u6ca1\u6709\uff0c\u5b83\u5c06\u9ed8\u8ba4\u8c03\u7528mapper\u548creducer\u65b9\u6cd5\u3002\n\n        \u5728mapper \u548c mapper_final\u4e2d\u8fd8\u53ef\u4ee5\u5171\u4eab\u72b6\u6001\uff0cmapper \u6216 mapper_final \u4e0d\u80fd reducer\u4e4b\u95f4\u5171\u4eab\u72b6\u6001\u3002\n        \"\"\"\n        return [MRStep(mapper=self.map, mapper_final=self.map_final, reducer=self.reduce)]\n\n\nif __name__ == '__main__':\n    MRmean.run()\n", "src/py3.x/ml/15.BigData_MapReduce/mrMeanMapper.py": "#!/usr/bin/python\n# coding:utf-8\n\n'''\nCreated on 2017-04-06\nUpdate  on 2017-11-17\nAuthor: Peter/ApacheCN-xy/\u7247\u523b\nGitHub: https://github.com/apachecn/AiLearning\n'''\n\nfrom __future__ import print_function\n\nimport sys\nfrom numpy import mat, mean, power\n\n'''\n    \u8fd9\u4e2amapper\u6587\u4ef6\u6309\u884c\u8bfb\u53d6\u6240\u6709\u7684\u8f93\u5165\u5e76\u521b\u5efa\u4e00\u7ec4\u5bf9\u5e94\u7684\u6d6e\u70b9\u6570\uff0c\u7136\u540e\u5f97\u5230\u6570\u7ec4\u7684\u957f\u5ea6\u5e76\u521b\u5efaNumPy\u77e9\u9635\u3002\n    \u518d\u5bf9\u6240\u6709\u7684\u503c\u8fdb\u884c\u5e73\u65b9\uff0c\u6700\u540e\u5c06\u5747\u503c\u548c\u5e73\u65b9\u540e\u7684\u5747\u503c\u53d1\u9001\u51fa\u53bb\u3002\u8fd9\u4e9b\u503c\u5c06\u7528\u6765\u8ba1\u7b97\u5168\u5c40\u7684\u5747\u503c\u548c\u65b9\u5dee\u3002\n\n    Args: \n        file \u8f93\u5165\u6570\u636e\n    Return: \n'''\n\n\ndef read_input(file):\n    for line in file:\n        yield line.rstrip()             # \u8fd4\u56de\u4e00\u4e2a yield \u8fed\u4ee3\u5668\uff0c\u6bcf\u6b21\u83b7\u53d6\u4e0b\u4e00\u4e2a\u503c\uff0c\u8282\u7ea6\u5185\u5b58\u3002\n\n\ninput = read_input(sys.stdin)            # \u521b\u5efa\u4e00\u4e2a\u8f93\u5165\u7684\u6570\u636e\u884c\u7684\u5217\u8868list\ninput = [float(line) for line in input]  # \u5c06\u5f97\u5230\u7684\u6570\u636e\u8f6c\u5316\u4e3a float \u7c7b\u578b\nnumInputs = len(input)                   # \u83b7\u53d6\u6570\u636e\u7684\u4e2a\u6570\uff0c\u5373\u8f93\u5165\u6587\u4ef6\u7684\u6570\u636e\u7684\u884c\u6570\ninput = mat(input)                       # \u5c06 List \u8f6c\u6362\u4e3a\u77e9\u9635\nsqInput = power(input, 2)                # \u5c06\u77e9\u9635\u7684\u6570\u636e\u5206\u522b\u6c42 \u5e73\u65b9\uff0c\u5373 2\u6b21\u65b9\n\n# \u8f93\u51fa \u6570\u636e\u7684\u4e2a\u6570\uff0cn\u4e2a\u6570\u636e\u7684\u5747\u503c\uff0cn\u4e2a\u6570\u636e\u5e73\u65b9\u4e4b\u540e\u7684\u5747\u503c\n# \u7b2c\u4e00\u884c\u662f\u6807\u51c6\u8f93\u51fa\uff0c\u4e5f\u5c31\u662freducer\u7684\u8f93\u51fa\n# \u7b2c\u4e8c\u884c\u8bc6\u6807\u51c6\u9519\u8bef\u8f93\u51fa\uff0c\u5373\u5bf9\u4e3b\u8282\u70b9\u4f5c\u51fa\u7684\u54cd\u5e94\u62a5\u544a\uff0c\u8868\u660e\u672c\u8282\u70b9\u5de5\u4f5c\u6b63\u5e38\u3002\n# \u3010\u8fd9\u4e0d\u5c31\u662f\u9762\u8bd5\u7684\u88c5\u903c\u91cd\u70b9\u5417\uff1f\u5982\u4f55\u8bbe\u8ba1\u76d1\u542c\u67b6\u6784\u7ec6\u8282\u3011\u6ce8\u610f: \u4e00\u4e2a\u597d\u7684\u4e60\u60ef\u662f\u60f3\u6807\u51c6\u9519\u8bef\u8f93\u51fa\u53d1\u9001\u62a5\u544a\u3002\u5982\u679c\u67d0\u4efb\u52a110\u5206\u949f\u5185\u6ca1\u6709\u62a5\u544a\u8f93\u51fa\uff0c\u5219\u5c06\u88abHadoop\u4e2d\u6b62\u3002\nprint(\"%d\\t%f\\t%f\" % (numInputs, mean(input), mean(sqInput)))  # \u8ba1\u7b97\u5747\u503c\nprint(\"map report: still alive\", file=sys.stderr)\n", "src/py3.x/ml/13.PCA/pca.py": "#!/usr/bin/python\n# coding:utf8\n\n'''\nCreated on Jun 1, 2011\nUpdate  on 2017-12-20\nAuthor: Peter Harrington/\u7247\u523b\nGitHub: https://github.com/apachecn/AiLearning\n'''\nfrom numpy import *\nimport matplotlib.pyplot as plt\nprint(__doc__)\n\n\ndef loadDataSet(fileName, delim='\\t'):\n    fr = open(fileName)\n    stringArr = [line.strip().split(delim) for line in fr.readlines()]\n    datArr = [list(map(float, line)) for line in stringArr]\n    #\u6ce8\u610f\u8fd9\u91cc\u548cpython2\u7684\u533a\u522b\uff0c\u9700\u8981\u5728map\u51fd\u6570\u5916\u52a0\u4e00\u4e2alist\uff08\uff09\uff0c\u5426\u5219\u663e\u793a\u7ed3\u679c\u4e3a map at 0x3fed1d0\n    return mat(datArr)\n\n\ndef pca(dataMat, topNfeat=9999999):\n    \"\"\"pca\n\n    Args:\n        dataMat   \u539f\u6570\u636e\u96c6\u77e9\u9635\n        topNfeat  \u5e94\u7528\u7684N\u4e2a\u7279\u5f81\n    Returns:\n        lowDDataMat  \u964d\u7ef4\u540e\u6570\u636e\u96c6\n        reconMat     \u65b0\u7684\u6570\u636e\u96c6\u7a7a\u95f4\n    \"\"\"\n\n    # \u8ba1\u7b97\u6bcf\u4e00\u5217\u7684\u5747\u503c\n    meanVals = mean(dataMat, axis=0)\n    # print('meanVals', meanVals)\n\n    # \u6bcf\u4e2a\u5411\u91cf\u540c\u65f6\u90fd\u51cf\u53bb \u5747\u503c\n    meanRemoved = dataMat - meanVals\n    # print('meanRemoved=', meanRemoved)\n\n    # cov\u534f\u65b9\u5dee=[(x1-x\u5747\u503c)*(y1-y\u5747\u503c)+(x2-x\u5747\u503c)*(y2-y\u5747\u503c)+...+(xn-x\u5747\u503c)*(yn-y\u5747\u503c)+]/(n-1)\n    '''\n    \u65b9\u5dee: \uff08\u4e00\u7ef4\uff09\u5ea6\u91cf\u4e24\u4e2a\u968f\u673a\u53d8\u91cf\u5173\u7cfb\u7684\u7edf\u8ba1\u91cf\n    \u534f\u65b9\u5dee:  \uff08\u4e8c\u7ef4\uff09\u5ea6\u91cf\u5404\u4e2a\u7ef4\u5ea6\u504f\u79bb\u5176\u5747\u503c\u7684\u7a0b\u5ea6\n    \u534f\u65b9\u5dee\u77e9\u9635: \uff08\u591a\u7ef4\uff09\u5ea6\u91cf\u5404\u4e2a\u7ef4\u5ea6\u504f\u79bb\u5176\u5747\u503c\u7684\u7a0b\u5ea6\n\n    \u5f53\u00a0cov(X, Y)>0\u65f6\uff0c\u8868\u660eX\u4e0eY\u6b63\u76f8\u5173\uff1b(X\u8d8a\u5927\uff0cY\u4e5f\u8d8a\u5927\uff1bX\u8d8a\u5c0fY\uff0c\u4e5f\u8d8a\u5c0f\u3002\u8fd9\u79cd\u60c5\u51b5\uff0c\u6211\u4eec\u79f0\u4e3a\u201c\u6b63\u76f8\u5173\u201d\u3002)\n    \u5f53\u00a0cov(X, Y)<0\u65f6\uff0c\u8868\u660eX\u4e0eY\u8d1f\u76f8\u5173\uff1b\n    \u5f53\u00a0cov(X, Y)=0\u65f6\uff0c\u8868\u660eX\u4e0eY\u4e0d\u76f8\u5173\u3002\n    '''\n    covMat = cov(meanRemoved, rowvar=0)\n\n    # eigVals\u4e3a\u7279\u5f81\u503c\uff0c eigVects\u4e3a\u7279\u5f81\u5411\u91cf\n    eigVals, eigVects = linalg.eig(mat(covMat))\n    # print('eigVals=', eigVals)\n    # print('eigVects=', eigVects)\n    # \u5bf9\u7279\u5f81\u503c\uff0c\u8fdb\u884c\u4ece\u5c0f\u5230\u5927\u7684\u6392\u5e8f\uff0c\u8fd4\u56de\u4ece\u5c0f\u5230\u5927\u7684index\u5e8f\u53f7\n    # \u7279\u5f81\u503c\u7684\u9006\u5e8f\u5c31\u53ef\u4ee5\u5f97\u5230topNfeat\u4e2a\u6700\u5927\u7684\u7279\u5f81\u5411\u91cf\n    '''\n    >>> x = np.array([3, 1, 2])\n    >>> np.argsort(x)\n    array([1, 2, 0])  # index,1 = 1; index,2 = 2; index,0 = 3\n    >>> y = np.argsort(x)\n    >>> y[::-1]\n    array([0, 2, 1])\n    >>> y[:-3:-1]\n    array([0, 2])  # \u53d6\u51fa -1, -2\n    >>> y[:-6:-1]\n    array([0, 2, 1])\n    '''\n    eigValInd = argsort(eigVals)\n    # print('eigValInd1=', eigValInd)\n\n    # -1\u8868\u793a\u5012\u5e8f\uff0c\u8fd4\u56detopN\u7684\u7279\u5f81\u503c[-1 \u5230 -(topNfeat+1) \u4f46\u662f\u4e0d\u5305\u62ec-(topNfeat+1)\u672c\u8eab\u7684\u5012\u53d9]\n    eigValInd = eigValInd[:-(topNfeat+1):-1]\n    # print('eigValInd2=', eigValInd)\n    # \u91cd\u7ec4 eigVects \u6700\u5927\u5230\u6700\u5c0f\n    redEigVects = eigVects[:, eigValInd]\n    # print('redEigVects=', redEigVects.T)\n    # \u5c06\u6570\u636e\u8f6c\u6362\u5230\u65b0\u7a7a\u95f4\n    # print( \"---\", shape(meanRemoved), shape(redEigVects))\n    lowDDataMat = meanRemoved * redEigVects\n    reconMat = (lowDDataMat * redEigVects.T) + meanVals\n    # print('lowDDataMat=', lowDDataMat)\n    # print('reconMat=', reconMat)\n    return lowDDataMat, reconMat\n\n\ndef replaceNanWithMean():\n    datMat = loadDataSet('data/13.PCA/secom.data', ' ')\n    numFeat = shape(datMat)[1]\n    for i in range(numFeat):\n        # \u5bf9value\u4e0d\u4e3aNaN\u7684\u6c42\u5747\u503c\n        # .A \u8fd4\u56de\u77e9\u9635\u57fa\u4e8e\u7684\u6570\u7ec4\n        meanVal = mean(datMat[nonzero(~isnan(datMat[:, i].A))[0], i])\n        # \u5c06value\u4e3aNaN\u7684\u503c\u8d4b\u503c\u4e3a\u5747\u503c\n        datMat[nonzero(isnan(datMat[:, i].A))[0],i] = meanVal\n    return datMat\n\n\ndef show_picture(dataMat, reconMat):\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    ax.scatter(dataMat[:, 0].flatten().A[0], dataMat[:, 1].flatten().A[0], marker='^', s=90)\n    ax.scatter(reconMat[:, 0].flatten().A[0], reconMat[:, 1].flatten().A[0], marker='o', s=50, c='red')\n    plt.show()\n\n\ndef analyse_data(dataMat):\n    meanVals = mean(dataMat, axis=0)\n    meanRemoved = dataMat-meanVals\n    covMat = cov(meanRemoved, rowvar=0)\n    eigvals, eigVects = linalg.eig(mat(covMat))\n    eigValInd = argsort(eigvals)\n\n    topNfeat = 20\n    eigValInd = eigValInd[:-(topNfeat+1):-1]\n    cov_all_score = float(sum(eigvals))\n    sum_cov_score = 0\n    for i in range(0, len(eigValInd)):\n        line_cov_score = float(eigvals[eigValInd[i]])\n        sum_cov_score += line_cov_score\n        '''\n        \u6211\u4eec\u53d1\u73b0\u5176\u4e2d\u6709\u8d85\u8fc720%\u7684\u7279\u5f81\u503c\u90fd\u662f0\u3002\n        \u8fd9\u5c31\u610f\u5473\u7740\u8fd9\u4e9b\u7279\u5f81\u90fd\u662f\u5176\u4ed6\u7279\u5f81\u7684\u526f\u672c\uff0c\u4e5f\u5c31\u662f\u8bf4\uff0c\u5b83\u4eec\u53ef\u4ee5\u901a\u8fc7\u5176\u4ed6\u7279\u5f81\u6765\u8868\u793a\uff0c\u800c\u672c\u8eab\u5e76\u6ca1\u6709\u63d0\u4f9b\u989d\u5916\u7684\u4fe1\u606f\u3002\n\n        \u6700\u524d\u976215\u4e2a\u503c\u7684\u6570\u91cf\u7ea7\u5927\u4e8e10^5\uff0c\u5b9e\u9645\u4e0a\u90a3\u4ee5\u540e\u7684\u503c\u90fd\u53d8\u5f97\u975e\u5e38\u5c0f\u3002\n        \u8fd9\u5c31\u76f8\u5f53\u4e8e\u544a\u8bc9\u6211\u4eec\u53ea\u6709\u90e8\u5206\u91cd\u8981\u7279\u5f81\uff0c\u91cd\u8981\u7279\u5f81\u7684\u6570\u76ee\u4e5f\u5f88\u5feb\u5c31\u4f1a\u4e0b\u964d\u3002\n\n        \u6700\u540e\uff0c\u6211\u4eec\u53ef\u80fd\u4f1a\u6ce8\u610f\u5230\u6709\u4e00\u4e9b\u5c0f\u7684\u8d1f\u503c\uff0c\u4ed6\u4eec\u4e3b\u8981\u6e90\u81ea\u6570\u503c\u8bef\u5dee\u5e94\u8be5\u56db\u820d\u4e94\u5165\u62100.\n        '''\n        print('\u4e3b\u6210\u5206: %s, \u65b9\u5dee\u5360\u6bd4: %s%%, \u7d2f\u79ef\u65b9\u5dee\u5360\u6bd4: %s%%' % (format(i+1, '2.0f'), format(line_cov_score/cov_all_score*100, '4.2f'), format(sum_cov_score/cov_all_score*100, '4.1f')))\n\n\nif __name__ == \"__main__\":\n    # # \u52a0\u8f7d\u6570\u636e\uff0c\u5e76\u8f6c\u5316\u6570\u636e\u7c7b\u578b\u4e3afloat\n    # dataMat = loadDataSet('data/13.PCA/testSet.txt')\n    # # \u53ea\u9700\u89811\u4e2a\u7279\u5f81\u5411\u91cf\n    # lowDmat, reconMat = pca(dataMat, 1)\n    # # \u53ea\u9700\u89812\u4e2a\u7279\u5f81\u5411\u91cf\uff0c\u548c\u539f\u59cb\u6570\u636e\u4e00\u81f4\uff0c\u6ca1\u4efb\u4f55\u53d8\u5316\n    # # lowDmat, reconMat = pca(dataMat, 2)\n    # # print(shape(lowDmat))\n    # show_picture(dataMat, reconMat)\n\n    # \u5229\u7528PCA\u5bf9\u534a\u5bfc\u4f53\u5236\u9020\u6570\u636e\u964d\u7ef4\n    dataMat = replaceNanWithMean()\n    print(shape(dataMat))\n    # \u5206\u6790\u6570\u636e\n    analyse_data(dataMat)\n    # lowDmat, reconMat = pca(dataMat, 20)\n    # print(shape(lowDmat))\n    # show_picture(dataMat, reconMat)\n", "src/py3.x/ml/4.NaiveBayes/bayes.py": "#!/usr/bin/env python\n# -*- coding:utf-8 -*-\n\"\"\"\nCreated on Oct 19, 2010\nUpdate  on 2017-05-18\nAuthor: Peter Harrington/\u7f8a\u4e09/\u5c0f\u7476/BBruceyuan\nGitHub: https://github.com/apachecn/AiLearning\n\"\"\"\n\n# \u6211\u4e2a\u4eba\u975e\u5e38\u4e0d\u559c\u6b22 from numpy import *\n# \u56e0\u4e3a\u8fd9\u6837\u4f1a\u548c\u4e00\u4e9b\u7cfb\u7edf\u51fd\u6570\u51b2\u7a81\uff0c\u6bd4\u5982log, sum\u4e4b\u7c7b\u7684\nimport numpy as np\n\n\"\"\"\n\u8d1d\u53f6\u65af\u516c\u5f0f\np(xy)=p(x|y)p(y)=p(y|x)p(x)\np(x|y)=p(y|x)p(x)/p(y)\n\"\"\"\n\n# ------\u9879\u76ee\u6848\u4f8b1: \u5c4f\u853d\u793e\u533a\u7559\u8a00\u677f\u7684\u4fae\u8fb1\u6027\u8a00\u8bba------\n\n\ndef load_data_set():\n    \"\"\"\n    \u521b\u5efa\u6570\u636e\u96c6,\u90fd\u662f\u5047\u7684 fake data set \n    :return: \u5355\u8bcd\u5217\u8868posting_list, \u6240\u5c5e\u7c7b\u522bclass_vec\n    \"\"\"\n    posting_list = [\n        ['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],\n        ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],\n        ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],\n        ['stop', 'posting', 'stupid', 'worthless', 'gar e'],\n        ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],\n        ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]\n    class_vec = [0, 1, 0, 1, 0, 1]  # 1 is \u4fae\u8fb1\u6027\u7684\u6587\u5b57, 0 is not\n    return posting_list, class_vec\n\n\ndef create_vocab_list(data_set):\n    \"\"\"\n    \u83b7\u53d6\u6240\u6709\u5355\u8bcd\u7684\u96c6\u5408\n    :param data_set: \u6570\u636e\u96c6\n    :return: \u6240\u6709\u5355\u8bcd\u7684\u96c6\u5408(\u5373\u4e0d\u542b\u91cd\u590d\u5143\u7d20\u7684\u5355\u8bcd\u5217\u8868)\n    \"\"\"\n    vocab_set = set()  # create empty set\n    for item in data_set:\n        # | \u6c42\u4e24\u4e2a\u96c6\u5408\u7684\u5e76\u96c6\n        vocab_set = vocab_set | set(item)\n    return list(vocab_set)\n\n\ndef set_of_words2vec(vocab_list, input_set):\n    \"\"\"\n    \u904d\u5386\u67e5\u770b\u8be5\u5355\u8bcd\u662f\u5426\u51fa\u73b0\uff0c\u51fa\u73b0\u8be5\u5355\u8bcd\u5219\u5c06\u8be5\u5355\u8bcd\u7f6e1\n    :param vocab_list: \u6240\u6709\u5355\u8bcd\u96c6\u5408\u5217\u8868\n    :param input_set: \u8f93\u5165\u6570\u636e\u96c6\n    :return: \u5339\u914d\u5217\u8868[0,1,0,1...]\uff0c\u5176\u4e2d 1\u4e0e0 \u8868\u793a\u8bcd\u6c47\u8868\u4e2d\u7684\u5355\u8bcd\u662f\u5426\u51fa\u73b0\u5728\u8f93\u5165\u7684\u6570\u636e\u96c6\u4e2d\n    \"\"\"\n    # \u521b\u5efa\u4e00\u4e2a\u548c\u8bcd\u6c47\u8868\u7b49\u957f\u7684\u5411\u91cf\uff0c\u5e76\u5c06\u5176\u5143\u7d20\u90fd\u8bbe\u7f6e\u4e3a0\n    result = [0] * len(vocab_list)\n    # \u904d\u5386\u6587\u6863\u4e2d\u7684\u6240\u6709\u5355\u8bcd\uff0c\u5982\u679c\u51fa\u73b0\u4e86\u8bcd\u6c47\u8868\u4e2d\u7684\u5355\u8bcd\uff0c\u5219\u5c06\u8f93\u51fa\u7684\u6587\u6863\u5411\u91cf\u4e2d\u7684\u5bf9\u5e94\u503c\u8bbe\u4e3a1\n    for word in input_set:\n        if word in vocab_list:\n            result[vocab_list.index(word)] = 1\n        else:\n            # \u8fd9\u4e2a\u540e\u9762\u5e94\u8be5\u6ce8\u91ca\u6389\uff0c\u56e0\u4e3a\u5bf9\u4f60\u6ca1\u4ec0\u4e48\u7528\uff0c\u8fd9\u53ea\u662f\u4e3a\u4e86\u8f85\u52a9\u8c03\u8bd5\u7684\n            # print('the word: {} is not in my vocabulary'.format(word))\n            pass\n    return result\n\n\ndef _train_naive_bayes(train_mat, train_category):\n    \"\"\"\n    \u6734\u7d20\u8d1d\u53f6\u65af\u5206\u7c7b\u539f\u7248\n    :param train_mat:  type is ndarray\n                    \u603b\u7684\u8f93\u5165\u6587\u672c\uff0c\u5927\u81f4\u662f [[0,1,0,1], [], []]\n    :param train_category: \u6587\u4ef6\u5bf9\u5e94\u7684\u7c7b\u522b\u5206\u7c7b\uff0c [0, 1, 0],\n                            \u5217\u8868\u7684\u957f\u5ea6\u5e94\u8be5\u7b49\u4e8e\u4e0a\u9762\u90a3\u4e2a\u8f93\u5165\u6587\u672c\u7684\u957f\u5ea6\n    :return: \n    \"\"\"\n    train_doc_num = len(train_mat)\n    words_num = len(train_mat[0])\n    # \u56e0\u4e3a\u4fae\u8fb1\u6027\u7684\u88ab\u6807\u8bb0\u4e3a\u4e861\uff0c \u6240\u4ee5\u53ea\u8981\u628a\u4ed6\u4eec\u76f8\u52a0\u5c31\u53ef\u4ee5\u5f97\u5230\u4fae\u8fb1\u6027\u7684\u6709\u591a\u5c11\n    # \u4fae\u8fb1\u6027\u6587\u4ef6\u7684\u51fa\u73b0\u6982\u7387\uff0c\u5373train_category\u4e2d\u6240\u6709\u76841\u7684\u4e2a\u6570\uff0c\n    # \u4ee3\u8868\u7684\u5c31\u662f\u591a\u5c11\u4e2a\u4fae\u8fb1\u6027\u6587\u4ef6\uff0c\u4e0e\u6587\u4ef6\u7684\u603b\u6570\u76f8\u9664\u5c31\u5f97\u5230\u4e86\u4fae\u8fb1\u6027\u6587\u4ef6\u7684\u51fa\u73b0\u6982\u7387\n    pos_abusive = np.sum(train_category) / train_doc_num\n    # \u5355\u8bcd\u51fa\u73b0\u7684\u6b21\u6570\n    # \u539f\u7248\n    p0num = np.zeros(words_num)\n    p1num = np.zeros(words_num)\n\n    # \u6574\u4e2a\u6570\u636e\u96c6\u5355\u8bcd\u51fa\u73b0\u7684\u6b21\u6570\uff08\u539f\u6765\u662f0\uff0c\u540e\u9762\u6539\u62102\u4e86\uff09\n    p0num_all = 0\n    p1num_all = 0\n\n    for i in range(train_doc_num):\n        # \u904d\u5386\u6240\u6709\u7684\u6587\u4ef6\uff0c\u5982\u679c\u662f\u4fae\u8fb1\u6027\u6587\u4ef6\uff0c\u5c31\u8ba1\u7b97\u6b64\u4fae\u8fb1\u6027\u6587\u4ef6\u4e2d\u51fa\u73b0\u7684\u4fae\u8fb1\u6027\u5355\u8bcd\u7684\u4e2a\u6570\n        if train_category[i] == 1:\n            p1num += train_mat[i]\n            p1num_all += np.sum(train_mat[i])\n        else:\n            p0num += train_mat[i]\n            p0num_all += np.sum(train_mat[i])\n    # \u540e\u9762\u9700\u8981\u6539\u6210\u6539\u6210\u53d6 log \u51fd\u6570\n    p1vec = p1num / p1num_all\n    p0vec = p0num / p0num_all\n    return p0vec, p1vec, pos_abusive\n\n\ndef train_naive_bayes(train_mat, train_category):\n    \"\"\"\n    \u6734\u7d20\u8d1d\u53f6\u65af\u5206\u7c7b\u4fee\u6b63\u7248\uff0c\u3000\u6ce8\u610f\u548c\u539f\u6765\u7684\u5bf9\u6bd4\uff0c\u4e3a\u4ec0\u4e48\u8fd9\u4e48\u505a\u53ef\u4ee5\u67e5\u770b\u4e66\n    :param train_mat:  type is ndarray\n                    \u603b\u7684\u8f93\u5165\u6587\u672c\uff0c\u5927\u81f4\u662f [[0,1,0,1], [], []]\n    :param train_category: \u6587\u4ef6\u5bf9\u5e94\u7684\u7c7b\u522b\u5206\u7c7b\uff0c [0, 1, 0],\n                            \u5217\u8868\u7684\u957f\u5ea6\u5e94\u8be5\u7b49\u4e8e\u4e0a\u9762\u90a3\u4e2a\u8f93\u5165\u6587\u672c\u7684\u957f\u5ea6\n    :return: \n    \"\"\"\n    train_doc_num = len(train_mat)\n    words_num = len(train_mat[0])\n    # \u56e0\u4e3a\u4fae\u8fb1\u6027\u7684\u88ab\u6807\u8bb0\u4e3a\u4e861\uff0c \u6240\u4ee5\u53ea\u8981\u628a\u4ed6\u4eec\u76f8\u52a0\u5c31\u53ef\u4ee5\u5f97\u5230\u4fae\u8fb1\u6027\u7684\u6709\u591a\u5c11\n    # \u4fae\u8fb1\u6027\u6587\u4ef6\u7684\u51fa\u73b0\u6982\u7387\uff0c\u5373train_category\u4e2d\u6240\u6709\u76841\u7684\u4e2a\u6570\uff0c\n    # \u4ee3\u8868\u7684\u5c31\u662f\u591a\u5c11\u4e2a\u4fae\u8fb1\u6027\u6587\u4ef6\uff0c\u4e0e\u6587\u4ef6\u7684\u603b\u6570\u76f8\u9664\u5c31\u5f97\u5230\u4e86\u4fae\u8fb1\u6027\u6587\u4ef6\u7684\u51fa\u73b0\u6982\u7387\n    pos_abusive = np.sum(train_category) / train_doc_num\n    # \u5355\u8bcd\u51fa\u73b0\u7684\u6b21\u6570\n    # \u539f\u7248\uff0c\u53d8\u6210ones\u662f\u4fee\u6539\u7248\uff0c\u8fd9\u662f\u4e3a\u4e86\u9632\u6b62\u6570\u5b57\u8fc7\u5c0f\u6ea2\u51fa\n    # p0num = np.zeros(words_num)\n    # p1num = np.zeros(words_num)\n    p0num = np.ones(words_num)\n    p1num = np.ones(words_num)\n    # \u6574\u4e2a\u6570\u636e\u96c6\u5355\u8bcd\u51fa\u73b0\u7684\u6b21\u6570\uff08\u539f\u6765\u662f0\uff0c\u540e\u9762\u6539\u62102\u4e86\uff09\n    p0num_all = 2.0\n    p1num_all = 2.0\n\n    for i in range(train_doc_num):\n        # \u904d\u5386\u6240\u6709\u7684\u6587\u4ef6\uff0c\u5982\u679c\u662f\u4fae\u8fb1\u6027\u6587\u4ef6\uff0c\u5c31\u8ba1\u7b97\u6b64\u4fae\u8fb1\u6027\u6587\u4ef6\u4e2d\u51fa\u73b0\u7684\u4fae\u8fb1\u6027\u5355\u8bcd\u7684\u4e2a\u6570\n        if train_category[i] == 1:\n            p1num += train_mat[i]\n            p1num_all += np.sum(train_mat[i])\n        else:\n            p0num += train_mat[i]\n            p0num_all += np.sum(train_mat[i])\n    # \u540e\u9762\u6539\u6210\u53d6 log \u51fd\u6570\n    p1vec = np.log(p1num / p1num_all)\n    p0vec = np.log(p0num / p0num_all)\n    return p0vec, p1vec, pos_abusive\n\n\ndef classify_naive_bayes(vec2classify, p0vec, p1vec, p_class1):\n    \"\"\"\n    \u4f7f\u7528\u7b97\u6cd5: \n        # \u5c06\u4e58\u6cd5\u8f6c\u6362\u4e3a\u52a0\u6cd5\n        \u4e58\u6cd5: P(C|F1F2...Fn) = P(F1F2...Fn|C)P(C)/P(F1F2...Fn)\n        \u52a0\u6cd5: P(F1|C)*P(F2|C)....P(Fn|C)P(C) -> log(P(F1|C))+log(P(F2|C))+....+log(P(Fn|C))+log(P(C))\n    :param vec2classify: \u5f85\u6d4b\u6570\u636e[0,1,1,1,1...]\uff0c\u5373\u8981\u5206\u7c7b\u7684\u5411\u91cf\n    :param p0vec: \u7c7b\u522b0\uff0c\u5373\u6b63\u5e38\u6587\u6863\u7684[log(P(F1|C0)),log(P(F2|C0)),log(P(F3|C0)),log(P(F4|C0)),log(P(F5|C0))....]\u5217\u8868\n    :param p1vec: \u7c7b\u522b1\uff0c\u5373\u4fae\u8fb1\u6027\u6587\u6863\u7684[log(P(F1|C1)),log(P(F2|C1)),log(P(F3|C1)),log(P(F4|C1)),log(P(F5|C1))....]\u5217\u8868\n    :param p_class1: \u7c7b\u522b1\uff0c\u4fae\u8fb1\u6027\u6587\u4ef6\u7684\u51fa\u73b0\u6982\u7387\n    :return: \u7c7b\u522b1 or 0\n    \"\"\"\n    # \u8ba1\u7b97\u516c\u5f0f  log(P(F1|C))+log(P(F2|C))+....+log(P(Fn|C))+log(P(C))\n    # \u4f7f\u7528 NumPy \u6570\u7ec4\u6765\u8ba1\u7b97\u4e24\u4e2a\u5411\u91cf\u76f8\u4e58\u7684\u7ed3\u679c\uff0c\u8fd9\u91cc\u7684\u76f8\u4e58\u662f\u6307\u5bf9\u5e94\u5143\u7d20\u76f8\u4e58\uff0c\u5373\u5148\u5c06\u4e24\u4e2a\u5411\u91cf\u4e2d\u7684\u7b2c\u4e00\u4e2a\u5143\u7d20\u76f8\u4e58\uff0c\u7136\u540e\u5c06\u7b2c2\u4e2a\u5143\u7d20\u76f8\u4e58\uff0c\u4ee5\u6b64\u7c7b\u63a8\u3002\n    # \u6211\u7684\u7406\u89e3\u662f: \u8fd9\u91cc\u7684 vec2Classify * p1Vec \u7684\u610f\u601d\u5c31\u662f\u5c06\u6bcf\u4e2a\u8bcd\u4e0e\u5176\u5bf9\u5e94\u7684\u6982\u7387\u76f8\u5173\u8054\u8d77\u6765\n    # \u53ef\u4ee5\u7406\u89e3\u4e3a 1.\u5355\u8bcd\u5728\u8bcd\u6c47\u8868\u4e2d\u7684\u6761\u4ef6\u4e0b\uff0c\u6587\u4ef6\u662fgood \u7c7b\u522b\u7684\u6982\u7387 \u4e5f\u53ef\u4ee5\u7406\u89e3\u4e3a 2.\u5728\u6574\u4e2a\u7a7a\u95f4\u4e0b\uff0c\u6587\u4ef6\u65e2\u5728\u8bcd\u6c47\u8868\u4e2d\u53c8\u662fgood\u7c7b\u522b\u7684\u6982\u7387\n    p1 = np.sum(vec2classify * p1vec) + np.log(p_class1)\n    p0 = np.sum(vec2classify * p0vec) + np.log(1 - p_class1)\n    if p1 > p0:\n        return 1\n    else:\n        return 0\n\n\ndef bag_words2vec(vocab_list, input_set):\n    # \u6ce8\u610f\u548c\u539f\u6765\u7684\u505a\u5bf9\u6bd4\n    result = [0] * len(vocab_list)\n    for word in input_set:\n        if word in vocab_list:\n            result[vocab_list.index(word)] += 1\n        else:\n            print('the word: {} is not in my vocabulary'.format(word))\n    return result\n\n\ndef testing_naive_bayes():\n    \"\"\"\n    \u6d4b\u8bd5\u6734\u7d20\u8d1d\u53f6\u65af\u7b97\u6cd5\n    :return: no return \n    \"\"\"\n    # 1. \u52a0\u8f7d\u6570\u636e\u96c6\n    list_post, list_classes = load_data_set()\n    # 2. \u521b\u5efa\u5355\u8bcd\u96c6\u5408\n    vocab_list = create_vocab_list(list_post)\n\n    # 3. \u8ba1\u7b97\u5355\u8bcd\u662f\u5426\u51fa\u73b0\u5e76\u521b\u5efa\u6570\u636e\u77e9\u9635\n    train_mat = []\n    for post_in in list_post:\n        train_mat.append(\n            # \u8fd4\u56dem*len(vocab_list)\u7684\u77e9\u9635\uff0c \u8bb0\u5f55\u7684\u90fd\u662f0\uff0c1\u4fe1\u606f\n            # \u5176\u5b9e\u5c31\u662f\u90a3\u4e2a\u4e1c\u897f\u7684\u53e5\u5b50\u5411\u91cf\uff08\u5c31\u662fdata_set\u91cc\u9762\u6bcf\u4e00\u884c,\u4e5f\u4e0d\u7b97\u53e5\u5b50\u5427)\n            set_of_words2vec(vocab_list, post_in)\n        )\n    # 4. \u8bad\u7ec3\u6570\u636e\n    p0v, p1v, p_abusive = train_naive_bayes(np.array(train_mat), np.array(list_classes))\n    # 5. \u6d4b\u8bd5\u6570\u636e\n    test_one = ['love', 'my', 'dalmation']\n    test_one_doc = np.array(set_of_words2vec(vocab_list, test_one))\n    print('the result is: {}'.format(classify_naive_bayes(test_one_doc, p0v, p1v, p_abusive)))\n    test_two = ['stupid', 'garbage']\n    test_two_doc = np.array(set_of_words2vec(vocab_list, test_two))\n    print('the result is: {}'.format(classify_naive_bayes(test_two_doc, p0v, p1v, p_abusive)))\n\n\n# --------\u9879\u76ee\u6848\u4f8b2: \u4f7f\u7528\u6734\u7d20\u8d1d\u53f6\u65af\u8fc7\u6ee4\u5783\u573e\u90ae\u4ef6--------------\n\n\ndef text_parse(big_str):\n    \"\"\"\n    \u8fd9\u91cc\u5c31\u662f\u505a\u8bcd\u5212\u5206\n    :param big_str: \u67d0\u4e2a\u88ab\u62fc\u63a5\u540e\u7684\u5b57\u7b26\u4e32\n    :return: \u5168\u90e8\u662f\u5c0f\u5199\u7684word\u5217\u8868\uff0c\u53bb\u6389\u5c11\u4e8e 2 \u4e2a\u5b57\u7b26\u7684\u5b57\u7b26\u4e32\n    \"\"\"\n    import re\n    # \u5176\u5b9e\u8fd9\u91cc\u6bd4\u8f83\u63a8\u8350\u7528\u3000\\W+ \u4ee3\u66ff \\W*\uff0c\n    # \u56e0\u4e3a \\W*\u4f1amatch empty patten\uff0c\u5728py3.5+\u4e4b\u540e\u5c31\u4f1a\u51fa\u73b0\u4ec0\u4e48\u95ee\u9898\uff0c\u63a8\u8350\u81ea\u5df1\u4fee\u6539\u5c1d\u8bd5\u4e00\u4e0b\uff0c\u53ef\u80fd\u5c31\u4f1are.split\u7406\u89e3\u66f4\u6df1\u4e86\n    token_list = re.split(r'\\W+', big_str)\n    if len(token_list) == 0:\n        print(token_list)\n    return [tok.lower() for tok in token_list if len(tok) > 2]\n\n\ndef spam_test():\n    \"\"\"\n    \u5bf9\u8d1d\u53f6\u65af\u5783\u573e\u90ae\u4ef6\u5206\u7c7b\u5668\u8fdb\u884c\u81ea\u52a8\u5316\u5904\u7406\u3002\n    :return: nothing\n    \"\"\"\n    doc_list = []\n    class_list = []\n    full_text = []\n    for i in range(1, 26):\n        # \u6dfb\u52a0\u5783\u573e\u90ae\u4ef6\u4fe1\u606f\n        # \u8fd9\u91cc\u9700\u8981\u505a\u4e00\u4e2a\u8bf4\u660e\uff0c\u4e3a\u4ec0\u4e48\u6211\u4f1a\u4f7f\u7528try except \u6765\u505a\n        # \u56e0\u4e3a\u6211\u4eec\u5176\u4e2d\u6709\u51e0\u4e2a\u6587\u4ef6\u7684\u7f16\u7801\u683c\u5f0f\u662f windows 1252\u3000\uff08spam: 17.txt, ham: 6.txt...)\n        # \u8fd9\u91cc\u5176\u5b9e\u8fd8\u53ef\u4ee5 :\n        # import os\n        # \u7136\u540e\u68c0\u67e5 os.system(' file {}.txt'.format(i))\uff0c\u770b\u4e00\u4e0b\u8fd4\u56de\u7684\u662f\u4ec0\u4e48\n        # \u5982\u679c\u6b63\u5e38\u80fd\u8bfb\u8fd4\u56de\u7684\u90fd\u662f: \u3000ASCII text\n        # \u5bf9\u4e8eexcept\u9700\u8981\u5904\u7406\u7684\u90fd\u662f\u8fd4\u56de:  Non-ISO extended-ASCII text, with very long lines\n        try:\n            words = text_parse(open('data/4.NaiveBayes/email/spam/{}.txt'.format(i)).read())\n        except:\n            words = text_parse(open('data/4.NaiveBayes/email/spam/{}.txt'.format(i), encoding='Windows 1252').read())\n        doc_list.append(words)\n        full_text.extend(words)\n        class_list.append(1)\n        try:\n            # \u6dfb\u52a0\u975e\u5783\u573e\u90ae\u4ef6\n            words = text_parse(open('data/4.NaiveBayes/email/ham/{}.txt'.format(i)).read())\n        except:\n            words = text_parse(open('data/4.NaiveBayes/email/ham/{}.txt'.format(i), encoding='Windows 1252').read())\n        doc_list.append(words)\n        full_text.extend(words)\n        class_list.append(0)\n    # \u521b\u5efa\u8bcd\u6c47\u8868\n    vocab_list = create_vocab_list(doc_list)\n    \n    import random\n    # \u751f\u6210\u968f\u673a\u53d610\u4e2a\u6570, \u4e3a\u4e86\u907f\u514d\u8b66\u544a\u5c06\u6bcf\u4e2a\u6570\u90fd\u8f6c\u6362\u4e3a\u6574\u578b\n    test_set = [int(num) for num in random.sample(range(50), 10)]\n    # \u5e76\u5728\u539f\u6765\u7684training_set\u4e2d\u53bb\u6389\u8fd910\u4e2a\u6570\n    training_set = list(set(range(50)) - set(test_set))\n    \n    training_mat = []\n    training_class = []\n    for doc_index in training_set:\n        training_mat.append(set_of_words2vec(vocab_list, doc_list[doc_index]))\n        training_class.append(class_list[doc_index])\n    p0v, p1v, p_spam = train_naive_bayes(\n        np.array(training_mat),\n        np.array(training_class)\n    )\n\n    # \u5f00\u59cb\u6d4b\u8bd5\n    error_count = 0\n    for doc_index in test_set:\n        word_vec = set_of_words2vec(vocab_list, doc_list[doc_index])\n        if classify_naive_bayes(\n            np.array(word_vec),\n            p0v,\n            p1v,\n            p_spam\n        ) != class_list[doc_index]:\n            error_count += 1\n    print('the error rate is {}'.format(\n        error_count / len(test_set)\n    ))\n\n# ----- \u9879\u76ee\u6848\u4f8b3: \u4f7f\u7528\u6734\u7d20\u8d1d\u53f6\u65af\u4ece\u4e2a\u4eba\u5e7f\u544a\u4e2d\u83b7\u53d6\u533a\u57df\u503e\u5411 ------\n# \u5176\u4e2d\u6709\u51e0\u4e2a\u51fd\u6570\u4e0a\u9762\u90fd\u5199\u8fc7\u4e86\uff0c\u6ca1\u5fc5\u8981\u518d\u5199\u4e00\u904d\u4e86\uff0c\u6240\u4ee5\u5220\u4e86\n\n\ndef calc_most_freq(vocab_list, full_text):\n    # RSS\u6e90\u5206\u7c7b\u5668\u53ca\u9ad8\u9891\u8bcd\u53bb\u9664\u51fd\u6570\n    from operator import itemgetter\n    freq_dict = {}\n    for token in vocab_list:\n        freq_dict[token] = full_text.count(token)\n    sorted_freq = sorted(freq_dict.items(), key=itemgetter(1), reverse=True)\n    return sorted_freq[0:30]\n\n\ndef local_words(feed1, feed0):\n    # import feedparser # \u5176\u5b9e\u5462\uff0c\u8fd9\u4e00\u884c\u6ca1\u7528\u5230\uff0c\u6700\u597d\u5220\u4e86\n    # \u4e0b\u9762\u64cd\u4f5c\u548c\u4e0a\u9762\u90a3\u4e2a spam_test\u51fd\u6570\u57fa\u672c\u4e00\u6837\uff0c\u7406\u89e3\u4e86\u4e00\u4e2a\uff0c\u4e24\u4e2a\u90fdok\n    doc_list = []\n    class_list = []\n    full_text = []\n    # \u627e\u51fa\u4e24\u4e2a\u4e2d\u6700\u5c0f\u7684\u4e00\u4e2a\n    min_len = min(len(feed0), len(feed1))\n    for i in range(min_len):\n        # \u7c7b\u522b\u3000\uff11\n        word_list = text_parse(feed1['entries'][i]['summary'])\n        doc_list.append(word_list)\n        full_text.extend(word_list)\n        class_list.append(1)\n        # \u7c7b\u522b\u3000\uff10\n        word_list = text_parse(feed0['entries'][i]['summary'])\n        doc_list.append(word_list)\n        full_text.extend(word_list)\n        class_list.append(0)\n    vocab_list = create_vocab_list(doc_list)\n    # \u53bb\u6389\u9ad8\u9891\u8bcd\n    top30words = calc_most_freq(vocab_list, full_text)\n    for pair in top30words:\n        if pair[0] in vocab_list:\n            vocab_list.remove(pair[0])\n    # \u83b7\u53d6\u8bad\u7ec3\u6570\u636e\u548c\u6d4b\u8bd5\u6570\u636e\n    \n    import random\n    # \u751f\u6210\u968f\u673a\u53d610\u4e2a\u6570, \u4e3a\u4e86\u907f\u514d\u8b66\u544a\u5c06\u6bcf\u4e2a\u6570\u90fd\u8f6c\u6362\u4e3a\u6574\u578b\n    test_set = [int(num) for num in random.sample(range(2 * min_len), 20)]\n    # \u5e76\u5728\u539f\u6765\u7684training_set\u4e2d\u53bb\u6389\u8fd910\u4e2a\u6570\n    training_set = list(set(range(2 * min_len)) - set(test_set))\n    \n    # \u628a\u8fd9\u4e9b\u8bad\u7ec3\u96c6\u548c\u6d4b\u8bd5\u96c6\u53d8\u6210\u5411\u91cf\u7684\u5f62\u5f0f\n    training_mat = []\n    training_class = []\n    for doc_index in training_set:\n        training_mat.append(bag_words2vec(vocab_list, doc_list[doc_index]))\n        training_class.append(class_list[doc_index])\n    p0v, p1v, p_spam = train_naive_bayes(\n        np.array(training_mat),\n        np.array(training_class)\n    )\n    error_count = 0\n    for doc_index in test_set:\n        word_vec = bag_words2vec(vocab_list, doc_list[doc_index])\n        if classify_naive_bayes(\n            np.array(word_vec),\n            p0v,\n            p1v,\n            p_spam\n        ) != class_list[doc_index]:\n            error_count += 1\n    print(\"the error rate is {}\".format(error_count / len(test_set)))\n    return vocab_list, p0v, p1v\n\n\ndef test_rss():\n    import feedparser\n    ny = feedparser.parse('http://newyork.craigslist.org/stp/index.rss')\n    sf = feedparser.parse('http://sfbay.craigslist.org/stp/index.rss')\n    vocab_list, p_sf, p_nf = local_words(ny, sf)\n    # \u8fd4\u56de\u503c\u90fd\u6ca1\u7528\u4e0a\uff0c\u53ef\u4ee5\u7528_, _, _\u4ee3\u66ff\n\n\ndef get_top_words():\n    import feedparser\n    ny = feedparser.parse('http://newyork.craigslist.org/stp/index.rss')\n    sf = feedparser.parse('http://sfbay.craigslist.org/stp/index.rss')\n    vocab_list, p_sf, p_ny = local_words(ny, sf)\n    top_ny = []\n    top_sf = []\n    for i in range(len(p_sf)):\n        if p_sf[i] > -6.0:\n            top_sf.append((vocab_list[i], p_sf[i]))\n        if p_ny[i] > -6.0:\n            top_ny.append((vocab_list[i], p_ny[i]))\n    sorted_sf = sorted(top_sf, key=lambda pair: pair[1], reverse=True)\n    sorted_ny = sorted(top_ny, key=lambda pair: pair[1], reverse=True)\n    print('\\n----------- this is SF ---------------\\n')\n    for item in sorted_sf:\n        print(item[0])\n    print('\\n----------- this is NY ---------------\\n')\n    for item in sorted_ny:\n        print(item[0])\n\n\nif __name__ == \"__main__\":\n    # testing_naive_bayes()\n    # spam_test()\n    # test_rss()\n    get_top_words()\n", "src/py3.x/ml/4.NaiveBayes/sklearn-nb-demo.py": "#!/usr/bin/python\n# -*- coding:utf-8 -*-\n\n\"\"\"\nCreated on 2017-06-28\nUpdated on 2017-06-28\nNaiveBayes: \u6734\u7d20\u8d1d\u53f6\u65af\nAuthor: \u5c0f\u7476\nGitHub: https://github.com/apachecn/AiLearning\n\"\"\"\n\n\n# GaussianNB_\u9ad8\u65af\u6734\u7d20\u8d1d\u53f6\u65af\nimport numpy as np\nX = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\nY = np.array([1, 1, 1, 2, 2, 2])\nfrom sklearn.naive_bayes import GaussianNB\nclf = GaussianNB()\nclf.fit(X, Y)\nprint(clf.predict([[-0.8, -1]]))\nclf_pf = GaussianNB()\nclf_pf.partial_fit(X, Y, np.unique(Y))\nprint(clf_pf.predict([[-0.8, -1]]))\n\n# MultinomialNB_\u591a\u9879\u6734\u7d20\u8d1d\u53f6\u65af\n'''\nimport numpy as np\nX = np.random.randint(5, size=(6, 100))\ny = np.array([1, 2, 3, 4, 5, 6])\nfrom sklearn.naive_bayes import MultinomialNB\nclf = MultinomialNB()\nclf.fit(X, y)\nprint clf.predict(X[2:3])\n'''\n\n# BernoulliNB_\u4f2f\u52aa\u5229\u6734\u7d20\u8d1d\u53f6\u65af\n'''\nimport numpy as np\nX = np.random.randint(2, size=(6, 100))\nY = np.array([1, 2, 3, 4, 4, 5])\nfrom sklearn.naive_bayes import BernoulliNB\nclf = BernoulliNB()\nclf.fit(X, Y)\nprint clf.predict(X[2:3])\n'''\n", "src/py3.x/ml/2.KNN/sklearn-knn-demo.py": "#!/usr/bin/python\n# -*- coding: UTF-8 -*-\n\n\"\"\"\nCreated on 2017-06-28\nUpdated on 2017-06-28\nKNN: k\u8fd1\u90bb\u7b97\u6cd5\nAuthor: \u5c0f\u7476\nGitHub: https://github.com/apachecn/AiLearning\n\"\"\"\nprint(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom numpy import *\nfrom matplotlib.colors import ListedColormap\nfrom sklearn import neighbors, datasets\n\nn_neighbors = 3\n\n# \u5bfc\u5165\u4e00\u4e9b\u8981\u73a9\u7684\u6570\u636e\niris = datasets.load_iris()\nX = iris.data[:, :2]  # \u6211\u4eec\u53ea\u91c7\u7528\u524d\u4e24\u4e2afeature. \u6211\u4eec\u53ef\u4ee5\u4f7f\u7528\u4e8c\u7ef4\u6570\u636e\u96c6\u907f\u514d\u8fd9\u4e2a\u4e11\u964b\u7684\u5207\u7247\ny = iris.target\n\n# print 'X=', type(X), X\n# print 'y=', type(y), y\n\n# X = array([[-1.0, -1.1], [-1.0, -1.0], [0, 0], [1.0, 1.1], [2.0, 2.0], [2.0, 2.1]])\n# y = array([0, 0, 0, 1, 1, 1])\n\n# print 'X=', type(X), X\n# print 'y=', type(y), y\n\nh = .02  # \u7f51\u683c\u4e2d\u7684\u6b65\u957f\n\n# \u521b\u5efa\u5f69\u8272\u7684\u56fe\ncmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\ncmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n\n# cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA'])\n# cmap_bold = ListedColormap(['#FF0000', '#00FF00'])\n\nfor weights in ['uniform', 'distance']:\n    # \u6211\u4eec\u521b\u5efa\u4e86\u4e00\u4e2aknn\u5206\u7c7b\u5668\u7684\u5b9e\u4f8b\uff0c\u5e76\u62df\u5408\u6570\u636e\u3002\n    clf = neighbors.KNeighborsClassifier(n_neighbors, weights=weights)\n    clf.fit(X, y)\n\n    # \u7ed8\u5236\u51b3\u7b56\u8fb9\u754c\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u5c06\u4e3a\u6bcf\u4e2a\u5206\u914d\u4e00\u4e2a\u989c\u8272\n    # \u6765\u7ed8\u5236\u7f51\u683c\u4e2d\u7684\u70b9 [x_min, x_max]x[y_min, y_max].\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                         np.arange(y_min, y_max, h))\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n\n    # \u5c06\u7ed3\u679c\u653e\u5165\u4e00\u4e2a\u5f69\u8272\u56fe\u4e2d\n    Z = Z.reshape(xx.shape)\n    plt.figure()\n    plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n\n    # \u7ed8\u5236\u8bad\u7ec3\u70b9\n    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold)\n    plt.xlim(xx.min(), xx.max())\n    plt.ylim(yy.min(), yy.max())\n    plt.title(\"3-Class classification (k = %i, weights = '%s')\"\n              % (n_neighbors, weights))\n\nplt.show()", "src/py3.x/ml/2.KNN/kNN.py": "#!/usr/bin/env python\n# -*- coding: UTF-8 -*-\n'''\nCreated on Sep 16, 2010\nUpdate  on 2017-05-18\nAuthor: Peter Harrington/\u7f8a\u4e09/\u5c0f\u7476\nGitHub: https://github.com/apachecn/AiLearning\n'''\n\n# \u5bfc\u5165\u79d1\u5b66\u8ba1\u7b97\u5305numpy\u548c\u8fd0\u7b97\u7b26\u6a21\u5757operator\nfrom numpy import *\nimport operator\nimport os\nfrom collections import Counter\n\n\ndef createDataSet():\n    \"\"\"\n    Desc:\n        \u521b\u5efa\u6570\u636e\u96c6\u548c\u6807\u7b7e\n    Args:\n        None\n    Returns:\n        group -- \u8bad\u7ec3\u6570\u636e\u96c6\u7684 features\n        labels -- \u8bad\u7ec3\u6570\u636e\u96c6\u7684 labels\n    \u8c03\u7528\u65b9\u5f0f\n    import kNN\n    group, labels = kNN.createDataSet()\n    \"\"\"\n    group = array([[1.0, 1.1], [1.0, 1.0], [0, 0], [0, 0.1]])\n    labels = ['A', 'A', 'B', 'B']\n    return group, labels\n\n\ndef classify0(inX, dataSet, labels, k):\n    \"\"\"\n    Desc:\n        kNN \u7684\u5206\u7c7b\u51fd\u6570\n    Args:\n        inX -- \u7528\u4e8e\u5206\u7c7b\u7684\u8f93\u5165\u5411\u91cf/\u6d4b\u8bd5\u6570\u636e\n        dataSet -- \u8bad\u7ec3\u6570\u636e\u96c6\u7684 features\n        labels -- \u8bad\u7ec3\u6570\u636e\u96c6\u7684 labels\n        k -- \u9009\u62e9\u6700\u8fd1\u90bb\u7684\u6570\u76ee\n    Returns:\n        sortedClassCount[0][0] -- \u8f93\u5165\u5411\u91cf\u7684\u9884\u6d4b\u5206\u7c7b labels\n\n    \u6ce8\u610f: labels\u5143\u7d20\u6570\u76ee\u548cdataSet\u884c\u6570\u76f8\u540c\uff1b\u7a0b\u5e8f\u4f7f\u7528\u6b27\u5f0f\u8ddd\u79bb\u516c\u5f0f.\n\n    \u9884\u6d4b\u6570\u636e\u6240\u5728\u5206\u7c7b\u53ef\u5728\u8f93\u5165\u4e0b\u5217\u547d\u4ee4\n    kNN.classify0([0,0], group, labels, 3)\n    \"\"\"\n\n    # -----------\u5b9e\u73b0 classify0() \u65b9\u6cd5\u7684\u7b2c\u4e00\u79cd\u65b9\u5f0f----------------------------------------------------------------------------------------------------------------------------\n    # 1. \u8ddd\u79bb\u8ba1\u7b97\n    dataSetSize = dataSet.shape[0]\n    # tile\u751f\u6210\u548c\u8bad\u7ec3\u6837\u672c\u5bf9\u5e94\u7684\u77e9\u9635\uff0c\u5e76\u4e0e\u8bad\u7ec3\u6837\u672c\u6c42\u5dee\n    \"\"\"\n    tile: \u5217-3\u8868\u793a\u590d\u5236\u7684\u884c\u6570\uff0c \u884c-1\uff0f2\u8868\u793a\u5bf9inx\u7684\u91cd\u590d\u7684\u6b21\u6570\n\n    In [8]: tile(inx, (3, 1))\n    Out[8]:\n    array([[1, 2, 3],\n        [1, 2, 3],\n        [1, 2, 3]])\n\n    In [9]: tile(inx, (3, 2))\n    Out[9]:\n    array([[1, 2, 3, 1, 2, 3],\n        [1, 2, 3, 1, 2, 3],\n        [1, 2, 3, 1, 2, 3]])\n    \"\"\"\n    diffMat = tile(inX, (dataSetSize, 1)) - dataSet\n    \"\"\"\n    \u6b27\u6c0f\u8ddd\u79bb:  \u70b9\u5230\u70b9\u4e4b\u95f4\u7684\u8ddd\u79bb\n       \u7b2c\u4e00\u884c:  \u540c\u4e00\u4e2a\u70b9 \u5230 dataSet \u7684\u7b2c\u4e00\u4e2a\u70b9\u7684\u8ddd\u79bb\u3002\n       \u7b2c\u4e8c\u884c:  \u540c\u4e00\u4e2a\u70b9 \u5230 dataSet \u7684\u7b2c\u4e8c\u4e2a\u70b9\u7684\u8ddd\u79bb\u3002\n       ...\n       \u7b2cN\u884c:  \u540c\u4e00\u4e2a\u70b9 \u5230 dataSet \u7684\u7b2cN\u4e2a\u70b9\u7684\u8ddd\u79bb\u3002\n\n    [[1,2,3],[1,2,3]]-[[1,2,3],[1,2,0]]\n    (A1-A2)^2+(B1-B2)^2+(c1-c2)^2\n    \"\"\"\n    # \u53d6\u5e73\u65b9\n    sqDiffMat = diffMat ** 2\n    # \u5c06\u77e9\u9635\u7684\u6bcf\u4e00\u884c\u76f8\u52a0\n    sqDistances = sqDiffMat.sum(axis=1)\n    # \u5f00\u65b9\n    distances = sqDistances ** 0.5\n    # \u6839\u636e\u8ddd\u79bb\u6392\u5e8f\u4ece\u5c0f\u5230\u5927\u7684\u6392\u5e8f\uff0c\u8fd4\u56de\u5bf9\u5e94\u7684\u7d22\u5f15\u4f4d\u7f6e\n    # argsort() \u662f\u5c06x\u4e2d\u7684\u5143\u7d20\u4ece\u5c0f\u5230\u5927\u6392\u5217\uff0c\u63d0\u53d6\u5176\u5bf9\u5e94\u7684index\uff08\u7d22\u5f15\uff09\uff0c\u7136\u540e\u8f93\u51fa\u5230y\u3002\n    # \u4f8b\u5982: y=array([3,0,2,1,4,5]) \u5219\uff0cx[3]=1\u6700\u5c0f\uff0c\u6240\u4ee5y[0]=3;x[5]=5\u6700\u5927\uff0c\u6240\u4ee5y[5]=5\u3002\n    # print 'distances=', distances\n    sortedDistIndicies = distances.argsort()\n    # print 'distances.argsort()=', sortedDistIndicies\n\n    # 2. \u9009\u62e9\u8ddd\u79bb\u6700\u5c0f\u7684k\u4e2a\u70b9\n    classCount = {}\n    for i in range(k):\n        # \u627e\u5230\u8be5\u6837\u672c\u7684\u7c7b\u578b\n        voteIlabel = labels[sortedDistIndicies[i]]\n        # \u5728\u5b57\u5178\u4e2d\u5c06\u8be5\u7c7b\u578b\u52a0\u4e00\n        # \u5b57\u5178\u7684get\u65b9\u6cd5\n        # \u5982: list.get(k,d) \u5176\u4e2d get\u76f8\u5f53\u4e8e\u4e00\u6761if...else...\u8bed\u53e5,\u53c2\u6570k\u5728\u5b57\u5178\u4e2d\uff0c\u5b57\u5178\u5c06\u8fd4\u56delist[k];\u5982\u679c\u53c2\u6570k\u4e0d\u5728\u5b57\u5178\u4e2d\u5219\u8fd4\u56de\u53c2\u6570d,\u5982\u679cK\u5728\u5b57\u5178\u4e2d\u5219\u8fd4\u56dek\u5bf9\u5e94\u7684value\u503c\n        # l = {5:2,3:4}\n        # print l.get(3,0)\u8fd4\u56de\u7684\u503c\u662f4\uff1b\n        # Print l.get\uff081,0\uff09\u8fd4\u56de\u503c\u662f0\uff1b\n        classCount[voteIlabel] = classCount.get(voteIlabel, 0) + 1\n    # 3. \u6392\u5e8f\u5e76\u8fd4\u56de\u51fa\u73b0\u6700\u591a\u7684\u90a3\u4e2a\u7c7b\u578b\n    # \u5b57\u5178\u7684 items() \u65b9\u6cd5\uff0c\u4ee5\u5217\u8868\u8fd4\u56de\u53ef\u904d\u5386\u7684(\u952e\uff0c\u503c)\u5143\u7ec4\u6570\u7ec4\u3002\n    # \u4f8b\u5982: dict = {'Name': 'Zara', 'Age': 7}   print \"Value : %s\" %  dict.items()   Value : [('Age', 7), ('Name', 'Zara')]\n    # sorted \u4e2d\u7684\u7b2c2\u4e2a\u53c2\u6570 key=operator.itemgetter(1) \u8fd9\u4e2a\u53c2\u6570\u7684\u610f\u601d\u662f\u5148\u6bd4\u8f83\u7b2c\u51e0\u4e2a\u5143\u7d20\n    # \u4f8b\u5982: a=[('b',2),('a',1),('c',0)]  b=sorted(a,key=operator.itemgetter(1)) >>>b=[('c',0),('a',1),('b',2)] \u53ef\u4ee5\u770b\u5230\u6392\u5e8f\u662f\u6309\u7167\u540e\u8fb9\u76840,1,2\u8fdb\u884c\u6392\u5e8f\u7684\uff0c\u800c\u4e0d\u662fa,b,c\n    # b=sorted(a,key=operator.itemgetter(0)) >>>b=[('a',1),('b',2),('c',0)] \u8fd9\u6b21\u6bd4\u8f83\u7684\u662f\u524d\u8fb9\u7684a,b,c\u800c\u4e0d\u662f0,1,2\n    # b=sorted(a,key=opertator.itemgetter(1,0)) >>>b=[('c',0),('a',1),('b',2)] \u8fd9\u4e2a\u662f\u5148\u6bd4\u8f83\u7b2c2\u4e2a\u5143\u7d20\uff0c\u7136\u540e\u5bf9\u7b2c\u4e00\u4e2a\u5143\u7d20\u8fdb\u884c\u6392\u5e8f\uff0c\u5f62\u6210\u591a\u7ea7\u6392\u5e8f\u3002\n    sortedClassCount = sorted(classCount.items(), key=operator.itemgetter(1), reverse=True)\n    return sortedClassCount[0][0]\n    \n    # ------------------------------------------------------------------------------------------------------------------------------------------\n    # \u5b9e\u73b0 classify0() \u65b9\u6cd5\u7684\u7b2c\u4e8c\u79cd\u65b9\u5f0f\n\n    # \"\"\"\n    # 1. \u8ba1\u7b97\u8ddd\u79bb\n    \n    # \u6b27\u6c0f\u8ddd\u79bb:  \u70b9\u5230\u70b9\u4e4b\u95f4\u7684\u8ddd\u79bb\n    #    \u7b2c\u4e00\u884c:  \u540c\u4e00\u4e2a\u70b9 \u5230 dataSet\u7684\u7b2c\u4e00\u4e2a\u70b9\u7684\u8ddd\u79bb\u3002\n    #    \u7b2c\u4e8c\u884c:  \u540c\u4e00\u4e2a\u70b9 \u5230 dataSet\u7684\u7b2c\u4e8c\u4e2a\u70b9\u7684\u8ddd\u79bb\u3002\n    #    ...\n    #    \u7b2cN\u884c:  \u540c\u4e00\u4e2a\u70b9 \u5230 dataSet\u7684\u7b2cN\u4e2a\u70b9\u7684\u8ddd\u79bb\u3002\n\n    # [[1,2,3],[1,2,3]]-[[1,2,3],[1,2,0]]\n    # (A1-A2)^2+(B1-B2)^2+(c1-c2)^2\n    \n    # inx - dataset \u4f7f\u7528\u4e86numpy broadcasting\uff0c\u89c1 https://docs.scipy.org/doc/numpy-1.13.0/user/basics.broadcasting.html\n    # np.sum() \u51fd\u6570\u7684\u4f7f\u7528\u89c1 https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.sum.html\n    # \"\"\"\n\t#   dist = np.sum((inx - dataset)**2, axis=1)**0.5\n    \n    # \"\"\"\n    # 2. k\u4e2a\u6700\u8fd1\u7684\u6807\u7b7e\n    \n    # \u5bf9\u8ddd\u79bb\u6392\u5e8f\u4f7f\u7528numpy\u4e2d\u7684argsort\u51fd\u6570\uff0c \u89c1 https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.sort.html#numpy.sort\n    # \u51fd\u6570\u8fd4\u56de\u7684\u662f\u7d22\u5f15\uff0c\u56e0\u6b64\u53d6\u524dk\u4e2a\u7d22\u5f15\u4f7f\u7528[0 : k]\n    # \u5c06\u8fd9k\u4e2a\u6807\u7b7e\u5b58\u5728\u5217\u8868k_labels\u4e2d\n    # \"\"\"\n    # k_labels = [labels[index] for index in dist.argsort()[0 : k]]\n\t# \"\"\"\n    # 3. \u51fa\u73b0\u6b21\u6570\u6700\u591a\u7684\u6807\u7b7e\u5373\u4e3a\u6700\u7ec8\u7c7b\u522b\n    \n    # \u4f7f\u7528collections.Counter\u53ef\u4ee5\u7edf\u8ba1\u5404\u4e2a\u6807\u7b7e\u7684\u51fa\u73b0\u6b21\u6570\uff0cmost_common\u8fd4\u56de\u51fa\u73b0\u6b21\u6570\u6700\u591a\u7684\u6807\u7b7etuple\uff0c\u4f8b\u5982[('lable1', 2)]\uff0c\u56e0\u6b64[0][0]\u53ef\u4ee5\u53d6\u51fa\u6807\u7b7e\u503c\n\t# \"\"\"\n    # label = Counter(k_labels).most_common(1)[0][0]\n    # return label\n\n    # ------------------------------------------------------------------------------------------------------------------------------------------\n\n\ndef test1():\n    \"\"\"\n    \u7b2c\u4e00\u4e2a\u4f8b\u5b50\u6f14\u793a\n    \"\"\"\n    group, labels = createDataSet()\n    print(str(group))\n    print(str(labels))\n    print(classify0([0.1, 0.1], group, labels, 3))\n\n\n# ----------------------------------------------------------------------------------------\ndef file2matrix(filename):\n    \"\"\"\n    \u5bfc\u5165\u8bad\u7ec3\u6570\u636e\n    :param filename: \u6570\u636e\u6587\u4ef6\u8def\u5f84\n    :return: \u6570\u636e\u77e9\u9635returnMat\u548c\u5bf9\u5e94\u7684\u7c7b\u522bclassLabelVector\n    \"\"\"\n    fr = open(filename, 'r')\n    # \u83b7\u5f97\u6587\u4ef6\u4e2d\u7684\u6570\u636e\u884c\u7684\u884c\u6570\n    numberOfLines = len(fr.readlines())\n    # \u751f\u6210\u5bf9\u5e94\u7684\u7a7a\u77e9\u9635\n    # \u4f8b\u5982: zeros(2\uff0c3)\u5c31\u662f\u751f\u6210\u4e00\u4e2a 2*3 \u7684\u77e9\u9635\uff0c\u5404\u4e2a\u4f4d\u7f6e\u4e0a\u5168\u662f 0 \n    returnMat = zeros((numberOfLines, 3))  # prepare matrix to return\n    classLabelVector = []  # prepare labels return\n    fr = open(filename, 'r')\n    index = 0\n    for line in fr.readlines():\n        # str.strip([chars]) --\u8fd4\u56de\u79fb\u9664\u5b57\u7b26\u4e32\u5934\u5c3e\u6307\u5b9a\u7684\u5b57\u7b26\u751f\u6210\u7684\u65b0\u5b57\u7b26\u4e32\n        line = line.strip()\n        # \u4ee5 '\\t' \u5207\u5272\u5b57\u7b26\u4e32\n        listFromLine = line.split('\\t')\n        # \u6bcf\u5217\u7684\u5c5e\u6027\u6570\u636e\uff0c\u5373 features\n        returnMat[index] = listFromLine[0 : 3]\n        # \u6bcf\u5217\u7684\u7c7b\u522b\u6570\u636e\uff0c\u5c31\u662f label \u6807\u7b7e\u6570\u636e\n        classLabelVector.append(int(listFromLine[-1]))\n        index += 1\n    # \u8fd4\u56de\u6570\u636e\u77e9\u9635returnMat\u548c\u5bf9\u5e94\u7684\u7c7b\u522bclassLabelVector\n    return returnMat, classLabelVector\n\n\ndef autoNorm(dataSet):\n    \"\"\"\n    Desc: \n        \u5f52\u4e00\u5316\u7279\u5f81\u503c\uff0c\u6d88\u9664\u5c5e\u6027\u4e4b\u95f4\u91cf\u7ea7\u4e0d\u540c\u5bfc\u81f4\u7684\u5f71\u54cd\n    Args: \n        dataSet -- \u9700\u8981\u8fdb\u884c\u5f52\u4e00\u5316\u5904\u7406\u7684\u6570\u636e\u96c6\n    Returns: \n        normDataSet -- \u5f52\u4e00\u5316\u5904\u7406\u540e\u5f97\u5230\u7684\u6570\u636e\u96c6\n        ranges -- \u5f52\u4e00\u5316\u5904\u7406\u7684\u8303\u56f4\n        minVals -- \u6700\u5c0f\u503c\n\n    \u5f52\u4e00\u5316\u516c\u5f0f: \n        Y = (X-Xmin)/(Xmax-Xmin)\n        \u5176\u4e2d\u7684 min \u548c max \u5206\u522b\u662f\u6570\u636e\u96c6\u4e2d\u7684\u6700\u5c0f\u7279\u5f81\u503c\u548c\u6700\u5927\u7279\u5f81\u503c\u3002\u8be5\u51fd\u6570\u53ef\u4ee5\u81ea\u52a8\u5c06\u6570\u5b57\u7279\u5f81\u503c\u8f6c\u5316\u4e3a0\u52301\u7684\u533a\u95f4\u3002\n    \"\"\"\n    # \u8ba1\u7b97\u6bcf\u79cd\u5c5e\u6027\u7684\u6700\u5927\u503c\u3001\u6700\u5c0f\u503c\u3001\u8303\u56f4\n    minVals = dataSet.min(0)\n    maxVals = dataSet.max(0)\n    # \u6781\u5dee\n    ranges = maxVals - minVals\n    # -------\u7b2c\u4e00\u79cd\u5b9e\u73b0\u65b9\u5f0f---start-------------------------\n    normDataSet = zeros(shape(dataSet))\n    m = dataSet.shape[0]\n    # \u751f\u6210\u4e0e\u6700\u5c0f\u503c\u4e4b\u5dee\u7ec4\u6210\u7684\u77e9\u9635\n    normDataSet = dataSet - tile(minVals, (m, 1))\n    # \u5c06\u6700\u5c0f\u503c\u4e4b\u5dee\u9664\u4ee5\u8303\u56f4\u7ec4\u6210\u77e9\u9635\n    normDataSet = normDataSet / tile(ranges, (m, 1))  # element wise divide\n    # -------\u7b2c\u4e00\u79cd\u5b9e\u73b0\u65b9\u5f0f---end---------------------------------------------\n    \n    # # -------\u7b2c\u4e8c\u79cd\u5b9e\u73b0\u65b9\u5f0f---start---------------------------------------\n    # norm_dataset = (dataset - minvalue) / ranges\n    # # -------\u7b2c\u4e8c\u79cd\u5b9e\u73b0\u65b9\u5f0f---end---------------------------------------------\n    return normDataSet, ranges, minVals\n\n\ndef datingClassTest():\n    \"\"\"\n    Desc: \n        \u5bf9\u7ea6\u4f1a\u7f51\u7ad9\u7684\u6d4b\u8bd5\u65b9\u6cd5\uff0c\u5e76\u5c06\u5206\u7c7b\u9519\u8bef\u7684\u6570\u91cf\u548c\u5206\u7c7b\u9519\u8bef\u7387\u6253\u5370\u51fa\u6765\n    Args: \n        None\n    Returns: \n        None\n    \"\"\"\n    # \u8bbe\u7f6e\u6d4b\u8bd5\u6570\u636e\u7684\u7684\u4e00\u4e2a\u6bd4\u4f8b\uff08\u8bad\u7ec3\u6570\u636e\u96c6\u6bd4\u4f8b=1-hoRatio\uff09\n    hoRatio = 0.1  # \u6d4b\u8bd5\u8303\u56f4,\u4e00\u90e8\u5206\u6d4b\u8bd5\u4e00\u90e8\u5206\u4f5c\u4e3a\u6837\u672c\n    # \u4ece\u6587\u4ef6\u4e2d\u52a0\u8f7d\u6570\u636e\n    datingDataMat, datingLabels = file2matrix(\"data/2.KNN/datingTestSet2.txt\")  # load data setfrom file\n    # \u5f52\u4e00\u5316\u6570\u636e\n    normMat, ranges, minVals = autoNorm(datingDataMat)\n    # m \u8868\u793a\u6570\u636e\u7684\u884c\u6570\uff0c\u5373\u77e9\u9635\u7684\u7b2c\u4e00\u7ef4\n    m = normMat.shape[0]\n    # \u8bbe\u7f6e\u6d4b\u8bd5\u7684\u6837\u672c\u6570\u91cf\uff0c numTestVecs:m\u8868\u793a\u8bad\u7ec3\u6837\u672c\u7684\u6570\u91cf\n    numTestVecs = int(m * hoRatio)\n    print('numTestVecs=', numTestVecs)\n    errorCount = 0\n    for i in range(numTestVecs):\n        # \u5bf9\u6570\u636e\u6d4b\u8bd5\n        classifierResult = classify0(normMat[i], normMat[numTestVecs : m], datingLabels[numTestVecs : m], 3)\n        print(\"the classifier came back with: %d, the real answer is: %d\" % (classifierResult, datingLabels[i]))\n        errorCount += classifierResult != datingLabels[i]\n    print(\"the total error rate is: %f\" % (errorCount / numTestVecs))\n    print(errorCount)\n\n\ndef img2vector(filename):\n    \"\"\"\n    Desc: \n        \u5c06\u56fe\u50cf\u6570\u636e\u8f6c\u6362\u4e3a\u5411\u91cf\n    Args: \n        filename -- \u56fe\u7247\u6587\u4ef6 \u56e0\u4e3a\u6211\u4eec\u7684\u8f93\u5165\u6570\u636e\u7684\u56fe\u7247\u683c\u5f0f\u662f 32 * 32\u7684\n    Returns:\n        returnVect -- \u56fe\u7247\u6587\u4ef6\u5904\u7406\u5b8c\u6210\u540e\u7684\u4e00\u7ef4\u77e9\u9635\n\n    \u8be5\u51fd\u6570\u5c06\u56fe\u50cf\u8f6c\u6362\u4e3a\u5411\u91cf: \u8be5\u51fd\u6570\u521b\u5efa 1 * 1024 \u7684NumPy\u6570\u7ec4\uff0c\u7136\u540e\u6253\u5f00\u7ed9\u5b9a\u7684\u6587\u4ef6\uff0c\n    \u5faa\u73af\u8bfb\u51fa\u6587\u4ef6\u7684\u524d32\u884c\uff0c\u5e76\u5c06\u6bcf\u884c\u7684\u593432\u4e2a\u5b57\u7b26\u503c\u5b58\u50a8\u5728NumPy\u6570\u7ec4\u4e2d\uff0c\u6700\u540e\u8fd4\u56de\u6570\u7ec4\u3002\n    \"\"\"\n    returnVect = zeros((1, 1024))\n    fr = open(filename, 'r')\n    for i in range(32):\n        lineStr = fr.readline()\n        for j in range(32):\n            returnVect[0, 32 * i + j] = int(lineStr[j])\n    return returnVect\n\n\ndef handwritingClassTest():\n    \"\"\"\n    Desc:\n        \u624b\u5199\u6570\u5b57\u8bc6\u522b\u5206\u7c7b\u5668\uff0c\u5e76\u5c06\u5206\u7c7b\u9519\u8bef\u6570\u548c\u5206\u7c7b\u9519\u8bef\u7387\u6253\u5370\u51fa\u6765\n    Args:\n        None\n    Returns:\n        None\n    \"\"\"\n    # 1. \u5bfc\u5165\u6570\u636e\n    hwLabels = []\n    trainingFileList = os.listdir(\"data/2.KNN/trainingDigits\") # load the training set\n    m = len(trainingFileList)\n    trainingMat = zeros((m, 1024))\n    # hwLabels\u5b58\u50a80\uff5e9\u5bf9\u5e94\u7684index\u4f4d\u7f6e\uff0c trainingMat\u5b58\u653e\u7684\u6bcf\u4e2a\u4f4d\u7f6e\u5bf9\u5e94\u7684\u56fe\u7247\u5411\u91cf\n    for i in range(m):\n        fileNameStr = trainingFileList[i]\n        fileStr = fileNameStr.split('.')[0]  # take off .txt\n        classNumStr = int(fileStr.split('_')[0])\n        hwLabels.append(classNumStr)\n        # \u5c06 32*32\u7684\u77e9\u9635->1*1024\u7684\u77e9\u9635\n        trainingMat[i] = img2vector('data/2.KNN/trainingDigits/%s' % fileNameStr)\n\n    # 2. \u5bfc\u5165\u6d4b\u8bd5\u6570\u636e\n    testFileList = os.listdir('data/2.KNN/testDigits')  # iterate through the test set\n    errorCount = 0\n    mTest = len(testFileList)\n    for i in range(mTest):\n        fileNameStr = testFileList[i]\n        fileStr = fileNameStr.split('.')[0]  # take off .txt\n        classNumStr = int(fileStr.split('_')[0])\n        vectorUnderTest = img2vector('data/2.KNN/testDigits/%s' % fileNameStr)\n        classifierResult = classify0(vectorUnderTest, trainingMat, hwLabels, 3)\n        print(\"the classifier came back with: %d, the real answer is: %d\" % (classifierResult, classNumStr))\n        errorCount += classifierResult != classNumStr\n    print(\"\\nthe total number of errors is: %d\" % errorCount)\n    print(\"\\nthe total error rate is: %f\" % (errorCount / mTest))\n\n\nif __name__ == '__main__':\n    # test1()\n    # datingClassTest()\n    handwritingClassTest()\n", "src/py3.x/ml/11.Apriori/apriori.py": "#!/usr/bin/python\n# coding: utf8\n\n'''\nCreated on Mar 24, 2011\nUpdate  on 2017-05-18\nCh 11 code\nAuthor: Peter/\u7247\u523b\nGitHub: https://github.com/apachecn/AiLearning'''\nprint(__doc__)\nfrom numpy import *\n\n# \u52a0\u8f7d\u6570\u636e\u96c6\ndef loadDataSet():\n    return [[1, 3, 4], [2, 3, 5], [1, 2, 3, 5], [2, 5]]\n\n# \u521b\u5efa\u96c6\u5408 C1\u3002\u5373\u5bf9 dataSet \u8fdb\u884c\u53bb\u91cd\uff0c\u6392\u5e8f\uff0c\u653e\u5165 list \u4e2d\uff0c\u7136\u540e\u8f6c\u6362\u6240\u6709\u7684\u5143\u7d20\u4e3a frozenset\ndef createC1(dataSet):\n    \"\"\"createC1\uff08\u521b\u5efa\u96c6\u5408 C1\uff09\n\n    Args:\n        dataSet \u539f\u59cb\u6570\u636e\u96c6\n    Returns:\n        frozenset \u8fd4\u56de\u4e00\u4e2a frozenset \u683c\u5f0f\u7684 list\n    \"\"\"\n\n    C1 = []\n    for transaction in dataSet:\n        for item in transaction:\n            if not [item] in C1:\n                # \u904d\u5386\u6240\u6709\u7684\u5143\u7d20\uff0c\u5982\u679c\u4e0d\u5728 C1 \u51fa\u73b0\u8fc7\uff0c\u90a3\u4e48\u5c31 append\n                C1.append([item])\n    # \u5bf9\u6570\u7ec4\u8fdb\u884c `\u4ece\u5c0f\u5230\u5927` \u7684\u6392\u5e8f\n    # print 'sort \u524d=', C1\n    C1.sort()\n    # frozenset \u8868\u793a\u51bb\u7ed3\u7684 set \u96c6\u5408\uff0c\u5143\u7d20\u65e0\u6539\u53d8\uff1b\u53ef\u4ee5\u628a\u5b83\u5f53\u5b57\u5178\u7684 key \u6765\u4f7f\u7528\n    # print 'sort \u540e=', C1\n    # print 'frozenset=', map(frozenset, C1)\n    return map(frozenset, C1)\n\n# \u8ba1\u7b97\u5019\u9009\u6570\u636e\u96c6 CK \u5728\u6570\u636e\u96c6 D \u4e2d\u7684\u652f\u6301\u5ea6\uff0c\u5e76\u8fd4\u56de\u652f\u6301\u5ea6\u5927\u4e8e\u6700\u5c0f\u652f\u6301\u5ea6\uff08minSupport\uff09\u7684\u6570\u636e\ndef scanD(D, Ck, minSupport):\n    \"\"\"scanD\uff08\u8ba1\u7b97\u5019\u9009\u6570\u636e\u96c6 CK \u5728\u6570\u636e\u96c6 D \u4e2d\u7684\u652f\u6301\u5ea6\uff0c\u5e76\u8fd4\u56de\u652f\u6301\u5ea6\u5927\u4e8e\u6700\u5c0f\u652f\u6301\u5ea6 minSupport \u7684\u6570\u636e\uff09\n\n    Args:\n        D \u6570\u636e\u96c6\n        Ck \u5019\u9009\u9879\u96c6\u5217\u8868\n        minSupport \u6700\u5c0f\u652f\u6301\u5ea6\n    Returns:\n        retList \u652f\u6301\u5ea6\u5927\u4e8e minSupport \u7684\u96c6\u5408\n        supportData \u5019\u9009\u9879\u96c6\u652f\u6301\u5ea6\u6570\u636e\n    \"\"\"\n\n    # ssCnt \u4e34\u65f6\u5b58\u653e\u9009\u6570\u636e\u96c6 Ck \u7684\u9891\u7387. \u4f8b\u5982: a->10, b->5, c->8    \n    ssCnt = {}\n    for tid in D:\n        for can in Ck:\n            # s.issubset(t)  \u6d4b\u8bd5\u662f\u5426 s \u4e2d\u7684\u6bcf\u4e00\u4e2a\u5143\u7d20\u90fd\u5728 t \u4e2d\n            if can.issubset(tid):\n                if not ssCnt.has_key(can):\n                    ssCnt[can] = 1\n                else:\n                    ssCnt[can] += 1\n    numItems = float(len(D)) # \u6570\u636e\u96c6 D \u7684\u6570\u91cf\n    retList = []\n    supportData = {}\n    for key in ssCnt:\n        # \u652f\u6301\u5ea6 = \u5019\u9009\u9879\uff08key\uff09\u51fa\u73b0\u7684\u6b21\u6570 / \u6240\u6709\u6570\u636e\u96c6\u7684\u6570\u91cf\n        support = ssCnt[key]/numItems\n        if support >= minSupport:\n            # \u5728 retList \u7684\u9996\u4f4d\u63d2\u5165\u5143\u7d20\uff0c\u53ea\u5b58\u50a8\u652f\u6301\u5ea6\u6ee1\u8db3\u9891\u7e41\u9879\u96c6\u7684\u503c\n            retList.insert(0, key)\n        # \u5b58\u50a8\u6240\u6709\u7684\u5019\u9009\u9879\uff08key\uff09\u548c\u5bf9\u5e94\u7684\u652f\u6301\u5ea6\uff08support\uff09\n        supportData[key] = support\n    return retList, supportData\n\n# \u8f93\u5165\u9891\u7e41\u9879\u96c6\u5217\u8868 Lk \u4e0e\u8fd4\u56de\u7684\u5143\u7d20\u4e2a\u6570 k\uff0c\u7136\u540e\u8f93\u51fa\u6240\u6709\u53ef\u80fd\u7684\u5019\u9009\u9879\u96c6 Ck\ndef aprioriGen(Lk, k):\n    \"\"\"aprioriGen\uff08\u8f93\u5165\u9891\u7e41\u9879\u96c6\u5217\u8868 Lk \u4e0e\u8fd4\u56de\u7684\u5143\u7d20\u4e2a\u6570 k\uff0c\u7136\u540e\u8f93\u51fa\u5019\u9009\u9879\u96c6 Ck\u3002\n       \u4f8b\u5982: \u4ee5 {0},{1},{2} \u4e3a\u8f93\u5165\u4e14 k = 2 \u5219\u8f93\u51fa {0,1}, {0,2}, {1,2}. \u4ee5 {0,1},{0,2},{1,2} \u4e3a\u8f93\u5165\u4e14 k = 3 \u5219\u8f93\u51fa {0,1,2}\n       \u4ec5\u9700\u8981\u8ba1\u7b97\u4e00\u6b21\uff0c\u4e0d\u9700\u8981\u5c06\u6240\u6709\u7684\u7ed3\u679c\u8ba1\u7b97\u51fa\u6765\uff0c\u7136\u540e\u8fdb\u884c\u53bb\u91cd\u64cd\u4f5c\n       \u8fd9\u662f\u4e00\u4e2a\u66f4\u9ad8\u6548\u7684\u7b97\u6cd5\uff09\n\n    Args:\n        Lk \u9891\u7e41\u9879\u96c6\u5217\u8868\n        k \u8fd4\u56de\u7684\u9879\u96c6\u5143\u7d20\u4e2a\u6570\uff08\u82e5\u5143\u7d20\u7684\u524d k-2 \u76f8\u540c\uff0c\u5c31\u8fdb\u884c\u5408\u5e76\uff09\n    Returns:\n        retList \u5143\u7d20\u4e24\u4e24\u5408\u5e76\u7684\u6570\u636e\u96c6\n    \"\"\"\n    \n    retList = []\n    lenLk = len(Lk)\n    for i in range(lenLk):\n        for j in range(i+1, lenLk):\n            L1 = list(Lk[i])[: k-2]\n            L2 = list(Lk[j])[: k-2]\n            # print '-----i=', i, k-2, Lk, Lk[i], list(Lk[i])[: k-2]\n            # print '-----j=', j, k-2, Lk, Lk[j], list(Lk[j])[: k-2]\n            L1.sort()\n            L2.sort()\n            # \u7b2c\u4e00\u6b21 L1,L2 \u4e3a\u7a7a\uff0c\u5143\u7d20\u76f4\u63a5\u8fdb\u884c\u5408\u5e76\uff0c\u8fd4\u56de\u5143\u7d20\u4e24\u4e24\u5408\u5e76\u7684\u6570\u636e\u96c6\n            # if first k-2 elements are equal\n            if L1 == L2:\n                # set union\n                # print 'union=', Lk[i] | Lk[j], Lk[i], Lk[j]\n                retList.append(Lk[i] | Lk[j])\n    return retList\n\n# \u627e\u51fa\u6570\u636e\u96c6 dataSet \u4e2d\u652f\u6301\u5ea6 >= \u6700\u5c0f\u652f\u6301\u5ea6\u7684\u5019\u9009\u9879\u96c6\u4ee5\u53ca\u5b83\u4eec\u7684\u652f\u6301\u5ea6\u3002\u5373\u6211\u4eec\u7684\u9891\u7e41\u9879\u96c6\u3002\ndef apriori(dataSet, minSupport=0.5):\n    \"\"\"apriori\uff08\u9996\u5148\u6784\u5efa\u96c6\u5408 C1\uff0c\u7136\u540e\u626b\u63cf\u6570\u636e\u96c6\u6765\u5224\u65ad\u8fd9\u4e9b\u53ea\u6709\u4e00\u4e2a\u5143\u7d20\u7684\u9879\u96c6\u662f\u5426\u6ee1\u8db3\u6700\u5c0f\u652f\u6301\u5ea6\u7684\u8981\u6c42\u3002\u90a3\u4e48\u6ee1\u8db3\u6700\u5c0f\u652f\u6301\u5ea6\u8981\u6c42\u7684\u9879\u96c6\u6784\u6210\u96c6\u5408 L1\u3002\u7136\u540e L1 \u4e2d\u7684\u5143\u7d20\u76f8\u4e92\u7ec4\u5408\u6210 C2\uff0cC2 \u518d\u8fdb\u4e00\u6b65\u8fc7\u6ee4\u53d8\u6210 L2\uff0c\u7136\u540e\u4ee5\u6b64\u7c7b\u63a8\uff0c\u77e5\u9053 CN \u7684\u957f\u5ea6\u4e3a 0 \u65f6\u7ed3\u675f\uff0c\u5373\u53ef\u627e\u51fa\u6240\u6709\u9891\u7e41\u9879\u96c6\u7684\u652f\u6301\u5ea6\u3002\uff09\n\n    Args:\n        dataSet \u539f\u59cb\u6570\u636e\u96c6\n        minSupport \u652f\u6301\u5ea6\u7684\u9608\u503c\n    Returns:\n        L \u9891\u7e41\u9879\u96c6\u7684\u5168\u96c6\n        supportData \u6240\u6709\u5143\u7d20\u548c\u652f\u6301\u5ea6\u7684\u5168\u96c6\n    \"\"\"\n    # C1 \u5373\u5bf9 dataSet \u8fdb\u884c\u53bb\u91cd\uff0c\u6392\u5e8f\uff0c\u653e\u5165 list \u4e2d\uff0c\u7136\u540e\u8f6c\u6362\u6240\u6709\u7684\u5143\u7d20\u4e3a frozenset\n    C1 = createC1(dataSet)\n    # print 'C1: ', C1\n    # \u5bf9\u6bcf\u4e00\u884c\u8fdb\u884c set \u8f6c\u6362\uff0c\u7136\u540e\u5b58\u653e\u5230\u96c6\u5408\u4e2d\n    D = map(set, dataSet)\n    # print 'D=', D\n    # \u8ba1\u7b97\u5019\u9009\u6570\u636e\u96c6 C1 \u5728\u6570\u636e\u96c6 D \u4e2d\u7684\u652f\u6301\u5ea6\uff0c\u5e76\u8fd4\u56de\u652f\u6301\u5ea6\u5927\u4e8e minSupport \u7684\u6570\u636e\n    L1, supportData = scanD(D, C1, minSupport)\n    # print \"L1=\", L1, \"\\n\", \"outcome: \", supportData\n\n    # L \u52a0\u4e86\u4e00\u5c42 list, L \u4e00\u5171 2 \u5c42 list\n    L = [L1]\n    k = 2\n    # \u5224\u65ad L \u7684\u7b2c k-2 \u9879\u7684\u6570\u636e\u957f\u5ea6\u662f\u5426 > 0\u3002\u7b2c\u4e00\u6b21\u6267\u884c\u65f6 L \u4e3a [[frozenset([1]), frozenset([3]), frozenset([2]), frozenset([5])]]\u3002L[k-2]=L[0]=[frozenset([1]), frozenset([3]), frozenset([2]), frozenset([5])]\uff0c\u6700\u540e\u9762 k += 1\n    while (len(L[k-2]) > 0):\n        # print 'k=', k, L, L[k-2]\n        Ck = aprioriGen(L[k-2], k) # \u4f8b\u5982: \u4ee5 {0},{1},{2} \u4e3a\u8f93\u5165\u4e14 k = 2 \u5219\u8f93\u51fa {0,1}, {0,2}, {1,2}. \u4ee5 {0,1},{0,2},{1,2} \u4e3a\u8f93\u5165\u4e14 k = 3 \u5219\u8f93\u51fa {0,1,2}\n        # print 'Ck', Ck\n\n        Lk, supK = scanD(D, Ck, minSupport) # \u8ba1\u7b97\u5019\u9009\u6570\u636e\u96c6 CK \u5728\u6570\u636e\u96c6 D \u4e2d\u7684\u652f\u6301\u5ea6\uff0c\u5e76\u8fd4\u56de\u652f\u6301\u5ea6\u5927\u4e8e minSupport \u7684\u6570\u636e\n        # \u4fdd\u5b58\u6240\u6709\u5019\u9009\u9879\u96c6\u7684\u652f\u6301\u5ea6\uff0c\u5982\u679c\u5b57\u5178\u6ca1\u6709\uff0c\u5c31\u8ffd\u52a0\u5143\u7d20\uff0c\u5982\u679c\u6709\uff0c\u5c31\u66f4\u65b0\u5143\u7d20\n        supportData.update(supK)\n        if len(Lk) == 0:\n            break\n        # Lk \u8868\u793a\u6ee1\u8db3\u9891\u7e41\u5b50\u9879\u7684\u96c6\u5408\uff0cL \u5143\u7d20\u5728\u589e\u52a0\uff0c\u4f8b\u5982: \n        # l=[[set(1), set(2), set(3)]]\n        # l=[[set(1), set(2), set(3)], [set(1, 2), set(2, 3)]]\n        L.append(Lk)\n        k += 1\n        # print 'k=', k, len(L[k-2])\n    return L, supportData\n\n# \u8ba1\u7b97\u53ef\u4fe1\u5ea6\uff08confidence\uff09\ndef calcConf(freqSet, H, supportData, brl, minConf=0.7):\n    \"\"\"calcConf\uff08\u5bf9\u4e24\u4e2a\u5143\u7d20\u7684\u9891\u7e41\u9879\uff0c\u8ba1\u7b97\u53ef\u4fe1\u5ea6\uff0c\u4f8b\u5982:  {1,2}/{1} \u6216\u8005 {1,2}/{2} \u770b\u662f\u5426\u6ee1\u8db3\u6761\u4ef6\uff09\n\n    Args:\n        freqSet \u9891\u7e41\u9879\u96c6\u4e2d\u7684\u5143\u7d20\uff0c\u4f8b\u5982: frozenset([1, 3])    \n        H \u9891\u7e41\u9879\u96c6\u4e2d\u7684\u5143\u7d20\u7684\u96c6\u5408\uff0c\u4f8b\u5982: [frozenset([1]), frozenset([3])]\n        supportData \u6240\u6709\u5143\u7d20\u7684\u652f\u6301\u5ea6\u7684\u5b57\u5178\n        brl \u5173\u8054\u89c4\u5219\u5217\u8868\u7684\u7a7a\u6570\u7ec4\n        minConf \u6700\u5c0f\u53ef\u4fe1\u5ea6\n    Returns:\n        prunedH \u8bb0\u5f55 \u53ef\u4fe1\u5ea6\u5927\u4e8e\u9608\u503c\u7684\u96c6\u5408\n    \"\"\"\n    # \u8bb0\u5f55\u53ef\u4fe1\u5ea6\u5927\u4e8e\u6700\u5c0f\u53ef\u4fe1\u5ea6\uff08minConf\uff09\u7684\u96c6\u5408\n    prunedH = []\n    for conseq in H: # \u5047\u8bbe freqSet = frozenset([1, 3]), H = [frozenset([1]), frozenset([3])]\uff0c\u90a3\u4e48\u73b0\u5728\u9700\u8981\u6c42\u51fa frozenset([1]) -> frozenset([3]) \u7684\u53ef\u4fe1\u5ea6\u548c frozenset([3]) -> frozenset([1]) \u7684\u53ef\u4fe1\u5ea6\n\n        # print 'confData=', freqSet, H, conseq, freqSet-conseq\n        conf = supportData[freqSet]/supportData[freqSet-conseq] # \u652f\u6301\u5ea6\u5b9a\u4e49: a -> b = support(a | b) / support(a). \u5047\u8bbe  freqSet = frozenset([1, 3]), conseq = [frozenset([1])]\uff0c\u90a3\u4e48 frozenset([1]) \u81f3 frozenset([3]) \u7684\u53ef\u4fe1\u5ea6\u4e3a = support(a | b) / support(a) = supportData[freqSet]/supportData[freqSet-conseq] = supportData[frozenset([1, 3])] / supportData[frozenset([1])]\n        if conf >= minConf:\n            # \u53ea\u8981\u4e70\u4e86 freqSet-conseq \u96c6\u5408\uff0c\u4e00\u5b9a\u4f1a\u4e70 conseq \u96c6\u5408\uff08freqSet-conseq \u96c6\u5408\u548c conseq\u96c6\u5408 \u662f\u5168\u96c6\uff09\n            print (freqSet-conseq, '-->', conseq, 'conf:', conf)\n            brl.append((freqSet-conseq, conseq, conf))\n            prunedH.append(conseq)\n    return prunedH\n\n# \u9012\u5f52\u8ba1\u7b97\u9891\u7e41\u9879\u96c6\u7684\u89c4\u5219\ndef rulesFromConseq(freqSet, H, supportData, brl, minConf=0.7):\n    \"\"\"rulesFromConseq\n\n    Args:\n        freqSet \u9891\u7e41\u9879\u96c6\u4e2d\u7684\u5143\u7d20\uff0c\u4f8b\u5982: frozenset([2, 3, 5])    \n        H \u9891\u7e41\u9879\u96c6\u4e2d\u7684\u5143\u7d20\u7684\u96c6\u5408\uff0c\u4f8b\u5982: [frozenset([2]), frozenset([3]), frozenset([5])]\n        supportData \u6240\u6709\u5143\u7d20\u7684\u652f\u6301\u5ea6\u7684\u5b57\u5178\n        brl \u5173\u8054\u89c4\u5219\u5217\u8868\u7684\u6570\u7ec4\n        minConf \u6700\u5c0f\u53ef\u4fe1\u5ea6\n    \"\"\"\n    # H[0] \u662f freqSet \u7684\u5143\u7d20\u7ec4\u5408\u7684\u7b2c\u4e00\u4e2a\u5143\u7d20\uff0c\u5e76\u4e14 H \u4e2d\u6240\u6709\u5143\u7d20\u7684\u957f\u5ea6\u90fd\u4e00\u6837\uff0c\u957f\u5ea6\u7531 aprioriGen(H, m+1) \u8fd9\u91cc\u7684 m + 1 \u6765\u63a7\u5236\n    # \u8be5\u51fd\u6570\u9012\u5f52\u65f6\uff0cH[0] \u7684\u957f\u5ea6\u4ece 1 \u5f00\u59cb\u589e\u957f 1 2 3 ...\n    # \u5047\u8bbe freqSet = frozenset([2, 3, 5]), H = [frozenset([2]), frozenset([3]), frozenset([5])]\n    # \u90a3\u4e48 m = len(H[0]) \u7684\u9012\u5f52\u7684\u503c\u4f9d\u6b21\u4e3a 1 2\n    # \u5728 m = 2 \u65f6, \u8df3\u51fa\u8be5\u9012\u5f52\u3002\u5047\u8bbe\u518d\u9012\u5f52\u4e00\u6b21\uff0c\u90a3\u4e48 H[0] = frozenset([2, 3, 5])\uff0cfreqSet = frozenset([2, 3, 5]) \uff0c\u6ca1\u5fc5\u8981\u518d\u8ba1\u7b97 freqSet \u4e0e H[0] \u7684\u5173\u8054\u89c4\u5219\u4e86\u3002\n    m = len(H[0])\n    if (len(freqSet) > (m + 1)):\n        # print 'freqSet******************', len(freqSet), m + 1, freqSet, H, H[0]\n        # \u751f\u6210 m+1 \u4e2a\u957f\u5ea6\u7684\u6240\u6709\u53ef\u80fd\u7684 H \u4e2d\u7684\u7ec4\u5408\uff0c\u5047\u8bbe H = [frozenset([2]), frozenset([3]), frozenset([5])]\n        # \u7b2c\u4e00\u6b21\u9012\u5f52\u8c03\u7528\u65f6\u751f\u6210 [frozenset([2, 3]), frozenset([2, 5]), frozenset([3, 5])]\n        # \u7b2c\u4e8c\u6b21 \u3002\u3002\u3002\u6ca1\u6709\u7b2c\u4e8c\u6b21\uff0c\u9012\u5f52\u6761\u4ef6\u5224\u65ad\u65f6\u5df2\u7ecf\u9000\u51fa\u4e86\n        Hmp1 = aprioriGen(H, m+1)\n        # \u8fd4\u56de\u53ef\u4fe1\u5ea6\u5927\u4e8e\u6700\u5c0f\u53ef\u4fe1\u5ea6\u7684\u96c6\u5408\n        Hmp1 = calcConf(freqSet, Hmp1, supportData, brl, minConf)\n        print ('Hmp1=', Hmp1)\n        print ('len(Hmp1)=', len(Hmp1), 'len(freqSet)=', len(freqSet))\n        # \u8ba1\u7b97\u53ef\u4fe1\u5ea6\u540e\uff0c\u8fd8\u6709\u6570\u636e\u5927\u4e8e\u6700\u5c0f\u53ef\u4fe1\u5ea6\u7684\u8bdd\uff0c\u90a3\u4e48\u7ee7\u7eed\u9012\u5f52\u8c03\u7528\uff0c\u5426\u5219\u8df3\u51fa\u9012\u5f52\n        if (len(Hmp1) > 1):\n            # print '----------------------', Hmp1\n            # print len(freqSet),  len(Hmp1[0]) + 1\n            rulesFromConseq(freqSet, Hmp1, supportData, brl, minConf)\n\n# \u751f\u6210\u5173\u8054\u89c4\u5219\ndef generateRules(L, supportData, minConf=0.7):\n    \"\"\"generateRules\n\n    Args:\n        L \u9891\u7e41\u9879\u96c6\u5217\u8868\n        supportData \u9891\u7e41\u9879\u96c6\u652f\u6301\u5ea6\u7684\u5b57\u5178\n        minConf \u6700\u5c0f\u7f6e\u4fe1\u5ea6\n    Returns:\n        bigRuleList \u53ef\u4fe1\u5ea6\u89c4\u5219\u5217\u8868\uff08\u5173\u4e8e (A->B+\u7f6e\u4fe1\u5ea6) 3\u4e2a\u5b57\u6bb5\u7684\u7ec4\u5408\uff09\n    \"\"\"\n    bigRuleList = []\n    # \u5047\u8bbe L = [[frozenset([1]), frozenset([3]), frozenset([2]), frozenset([5])], [frozenset([1, 3]), frozenset([2, 5]), frozenset([2, 3]), frozenset([3, 5])], [frozenset([2, 3, 5])]]\n    for i in range(1, len(L)):\n        # \u83b7\u53d6\u9891\u7e41\u9879\u96c6\u4e2d\u6bcf\u4e2a\u7ec4\u5408\u7684\u6240\u6709\u5143\u7d20\n        for freqSet in L[i]:\n            # \u5047\u8bbe: freqSet= frozenset([1, 3]), H1=[frozenset([1]), frozenset([3])]\n            # \u7ec4\u5408\u603b\u7684\u5143\u7d20\u5e76\u904d\u5386\u5b50\u5143\u7d20\uff0c\u5e76\u8f6c\u5316\u4e3a frozenset \u96c6\u5408\uff0c\u518d\u5b58\u653e\u5230 list \u5217\u8868\u4e2d\n            H1 = [frozenset([item]) for item in freqSet]\n            # 2 \u4e2a\u7684\u7ec4\u5408\uff0c\u8d70 else, 2 \u4e2a\u4ee5\u4e0a\u7684\u7ec4\u5408\uff0c\u8d70 if\n            if (i > 1):\n                rulesFromConseq(freqSet, H1, supportData, bigRuleList, minConf)\n            else:\n                calcConf(freqSet, H1, supportData, bigRuleList, minConf)\n    return bigRuleList\n\n\ndef getActionIds():\n    from time import sleep\n    from votesmart import votesmart\n    # votesmart.apikey = 'get your api key first'\n    votesmart.apikey = 'a7fa40adec6f4a77178799fae4441030'\n    actionIdList = []\n    billTitleList = []\n    fr = open('data/11.Apriori/recent20bills.txt')\n    for line in fr.readlines():\n        billNum = int(line.split('\\t')[0])\n        try:\n            billDetail = votesmart.votes.getBill(billNum) # api call\n            for action in billDetail.actions:\n                if action.level == 'House' and (action.stage == 'Passage' or action.stage == 'Amendment Vote'):\n                    actionId = int(action.actionId)\n                    print ('bill: %d has actionId: %d' % (billNum, actionId))\n                    actionIdList.append(actionId)\n                    billTitleList.append(line.strip().split('\\t')[1])\n        except:\n            print (\"problem getting bill %d\" % billNum)\n        sleep(1)                                      # delay to be polite\n    return actionIdList, billTitleList\n\n\ndef getTransList(actionIdList, billTitleList): #this will return a list of lists containing ints\n    itemMeaning = ['Republican', 'Democratic']#list of what each item stands for\n    for billTitle in billTitleList:#fill up itemMeaning list\n        itemMeaning.append('%s -- Nay' % billTitle)\n        itemMeaning.append('%s -- Yea' % billTitle)\n    transDict = {}#list of items in each transaction (politician)\n    voteCount = 2\n    for actionId in actionIdList:\n        sleep(3)\n        print ('getting votes for actionId: %d' % actionId)\n        try:\n            voteList = votesmart.votes.getBillActionVotes(actionId)\n            for vote in voteList:\n                if not transDict.has_key(vote.candidateName):\n                    transDict[vote.candidateName] = []\n                    if vote.officeParties == 'Democratic':\n                        transDict[vote.candidateName].append(1)\n                    elif vote.officeParties == 'Republican':\n                        transDict[vote.candidateName].append(0)\n                if vote.action == 'Nay':\n                    transDict[vote.candidateName].append(voteCount)\n                elif vote.action == 'Yea':\n                    transDict[vote.candidateName].append(voteCount + 1)\n        except:\n            print (\"problem getting actionId: %d\" % actionId)\n        voteCount += 2\n    return transDict, itemMeaning\n\n\n# \u6682\u65f6\u6ca1\u7528\u4e0a\n# def pntRules(ruleList, itemMeaning):\n#     for ruleTup in ruleList:\n#         for item in ruleTup[0]:\n#             print itemMeaning[item]\n#         print \"           -------->\"\n#         for item in ruleTup[1]:\n#             print itemMeaning[item]\n#         print \"confidence: %f\" % ruleTup[2]\n#         print       #print a blank line\n\ndef testApriori():\n    # \u52a0\u8f7d\u6d4b\u8bd5\u6570\u636e\u96c6\n    dataSet = loadDataSet()\n    print ('dataSet: ', dataSet)\n\n    # Apriori \u7b97\u6cd5\u751f\u6210\u9891\u7e41\u9879\u96c6\u4ee5\u53ca\u5b83\u4eec\u7684\u652f\u6301\u5ea6\n    L1, supportData1 = apriori(dataSet, minSupport=0.7)\n    print ('L(0.7): ', L1)\n    print ('supportData(0.7): ', supportData1)\n\n    print ('->->->->->->->->->->->->->->->->->->->->->->->->->->->->')\n\n    # Apriori \u7b97\u6cd5\u751f\u6210\u9891\u7e41\u9879\u96c6\u4ee5\u53ca\u5b83\u4eec\u7684\u652f\u6301\u5ea6\n    L2, supportData2 = apriori(dataSet, minSupport=0.5)\n    print ('L(0.5): ', L2)\n    print ('supportData(0.5): ', supportData2)\n\ndef testGenerateRules():\n    # \u52a0\u8f7d\u6d4b\u8bd5\u6570\u636e\u96c6\n    dataSet = loadDataSet()\n    print ('dataSet: ', dataSet)\n\n    # Apriori \u7b97\u6cd5\u751f\u6210\u9891\u7e41\u9879\u96c6\u4ee5\u53ca\u5b83\u4eec\u7684\u652f\u6301\u5ea6\n    L1, supportData1 = apriori(dataSet, minSupport=0.5)\n    print ('L(0.7): ', L1)\n    print ('supportData(0.7): ', supportData1)\n\n    # \u751f\u6210\u5173\u8054\u89c4\u5219\n    rules = generateRules(L1, supportData1, minConf=0.5)\n    print ('rules: ', rules)\n\ndef main():\n    # \u6d4b\u8bd5 Apriori \u7b97\u6cd5\n    # testApriori()\n\n    # \u751f\u6210\u5173\u8054\u89c4\u5219\n    # testGenerateRules()\n\n    ##\u9879\u76ee\u6848\u4f8b\n    # # \u6784\u5efa\u7f8e\u56fd\u56fd\u4f1a\u6295\u7968\u8bb0\u5f55\u7684\u4e8b\u52a1\u6570\u636e\u96c6\n    # actionIdList, billTitleList = getActionIds()\n    # # \u6d4b\u8bd5\u524d2\u4e2a\n    # transDict, itemMeaning = getTransList(actionIdList[: 2], billTitleList[: 2])\n    #transDict \u8868\u793a action_id\u7684\u96c6\u5408\uff0ctransDict[key]\u8fd9\u4e2a\u5c31\u662faction_id\u5bf9\u5e94\u7684\u9009\u9879\uff0c\u4f8b\u5982 [1, 2, 3]\n    # transDict, itemMeaning = getTransList(actionIdList, billTitleList)\n    # # \u5f97\u5230\u5168\u96c6\u7684\u6570\u636e\n    # dataSet = [transDict[key] for key in transDict.keys()]\n    # L, supportData = apriori(dataSet, minSupport=0.3)\n    # rules = generateRules(L, supportData, minConf=0.95)\n    # print (rules)\n\n    # # \u9879\u76ee\u6848\u4f8b\n    # # \u53d1\u73b0\u6bd2\u8611\u83c7\u7684\u76f8\u4f3c\u7279\u6027\n    # # \u5f97\u5230\u5168\u96c6\u7684\u6570\u636e\n     dataSet = [line.split() for line in open(\"data/11.Apriori/mushroom.dat\").readlines()]\n     L, supportData = apriori(dataSet, minSupport=0.3)\n    # # 2\u8868\u793a\u6bd2\u8611\u83c7\uff0c1\u8868\u793a\u53ef\u98df\u7528\u7684\u8611\u83c7\n    # # \u627e\u51fa\u5173\u4e8e2\u7684\u9891\u7e41\u5b50\u9879\u51fa\u6765\uff0c\u5c31\u77e5\u9053\u5982\u679c\u662f\u6bd2\u8611\u83c7\uff0c\u90a3\u4e48\u51fa\u73b0\u9891\u7e41\u7684\u4e5f\u53ef\u80fd\u662f\u6bd2\u8611\u83c7\n     for item in L[1]:\n         if item.intersection('2'):\n             print (item)\n    \n     for item in L[2]:\n         if item.intersection('2'):\n             print (item)\n\nif __name__ == \"__main__\":\n    main()\n", "src/py3.x/ml/8.Regression/sklearn-regression-demo.py": "#!/usr/bin/python\n# coding:utf8\n\n'''\nCreated on Jan 8, 2011\nUpdate  on 2017-05-18\nAuthor: Peter Harrington/\u5c0f\u7476\nGitHub: https://github.com/apachecn/AiLearning\n'''\n\n\n# Isotonic Regression \u7b49\u5f0f\u56de\u5f52\nprint(__doc__)\n\n# Author: Nelle Varoquaux <nelle.varoquaux@gmail.com>\n#         Alexandre Gramfort <alexandre.gramfort@inria.fr>\n# License: BSD\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.collections import LineCollection\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.isotonic import IsotonicRegression\nfrom sklearn.utils import check_random_state\n\nn = 100\nx = np.arange(n)\nrs = check_random_state(0)\ny = rs.randint(-50, 50, size=(n,)) + 50. * np.log(1 + np.arange(n))\n\nir = IsotonicRegression()\n\ny_ = ir.fit_transform(x, y)\n\nlr = LinearRegression()\nlr.fit(x[:, np.newaxis], y)  # \u7ebf\u6027\u56de\u5f52\u7684 x \u9700\u8981\u4e3a 2d\n\nsegments = [[[i, y[i]], [i, y_[i]]] for i in range(n)]\nlc = LineCollection(segments, zorder=0)\nlc.set_array(np.ones(len(y)))\nlc.set_linewidths(0.5 * np.ones(n))\n\nfig = plt.figure()\nplt.plot(x, y, 'r.', markersize=12)\nplt.plot(x, y_, 'g.-', markersize=12)\nplt.plot(x, lr.predict(x[:, np.newaxis]), 'b-')\nplt.gca().add_collection(lc)\nplt.legend(('Data', 'Isotonic Fit', 'Linear Fit'), loc='lower right')\nplt.title('Isotonic regression')\nplt.show()\n\n# Kernel ridge regression ( \u5185\u6838\u5cad\u56de\u5f52 )\n\n# 2.1 Comparison of kernel ridge regression and SVR ( \u5185\u6838\u5cad\u56de\u5f52\u4e0e SVR \u7684\u6bd4\u8f83 )\n\n# Authors: Jan Hendrik Metzen <jhm@informatik.uni-bremen.de>\n# License: BSD 3 clause\n\n'''\nfrom __future__ import division\nimport time\n\nimport numpy as np\n\nfrom sklearn.svm import SVR\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.kernel_ridge import KernelRidge\nimport matplotlib.pyplot as plt\n\nrng = np.random.RandomState(0)\n\n# \u751f\u6210\u6837\u672c\u6570\u636e\nX = 5 * rng.rand(10000, 1)\ny = np.sin(X).ravel()\n\n# \u7ed9\u76ee\u6807\u589e\u52a0\u566a\u97f3\ny[::5] += 3 * (0.5 - rng.rand(X.shape[0] // 5))\n\nX_plot = np.linspace(0, 5, 100000)[:, None]\n\n# Fit regression model ( \u62df\u5408 \u56de\u5f52 \u6a21\u578b )\ntrain_size = 100\nsvr = GridSearchCV(SVR(kernel='rbf', gamma=0.1), cv=5,\n                   param_grid={\"C\": [1e0, 1e1, 1e2, 1e3],\n                               \"gamma\": np.logspace(-2, 2, 5)})\n\nkr = GridSearchCV(KernelRidge(kernel='rbf', gamma=0.1), cv=5,\n                  param_grid={\"alpha\": [1e0, 0.1, 1e-2, 1e-3],\n                              \"gamma\": np.logspace(-2, 2, 5)})\n\nt0 = time.time()\nsvr.fit(X[:train_size], y[:train_size])\nsvr_fit = time.time() - t0\nprint(\"SVR complexity and bandwidth selected and model fitted in %.3f s\"\n      % svr_fit)\n\nt0 = time.time()\nkr.fit(X[:train_size], y[:train_size])\nkr_fit = time.time() - t0\nprint(\"KRR complexity and bandwidth selected and model fitted in %.3f s\"\n      % kr_fit)\n\nsv_ratio = svr.best_estimator_.support_.shape[0] / train_size\nprint(\"Support vector ratio: %.3f\" % sv_ratio)\n\nt0 = time.time()\ny_svr = svr.predict(X_plot)\nsvr_predict = time.time() - t0\nprint(\"SVR prediction for %d inputs in %.3f s\"\n      % (X_plot.shape[0], svr_predict))\n\nt0 = time.time()\ny_kr = kr.predict(X_plot)\nkr_predict = time.time() - t0\nprint(\"KRR prediction for %d inputs in %.3f s\"\n      % (X_plot.shape[0], kr_predict))\n\n# \u67e5\u770b\u7ed3\u679c\nsv_ind = svr.best_estimator_.support_\nplt.scatter(X[sv_ind], y[sv_ind], c='r', s=50, label='SVR support vectors',\n            zorder=2)\nplt.scatter(X[:100], y[:100], c='k', label='data', zorder=1)\nplt.hold('on')\nplt.plot(X_plot, y_svr, c='r',\n         label='SVR (fit: %.3fs, predict: %.3fs)' % (svr_fit, svr_predict))\nplt.plot(X_plot, y_kr, c='g',\n         label='KRR (fit: %.3fs, predict: %.3fs)' % (kr_fit, kr_predict))\nplt.xlabel('data')\nplt.ylabel('target')\nplt.title('SVR versus Kernel Ridge')\nplt.legend()\n\n# \u53ef\u89c6\u5316\u8bad\u7ec3\u548c\u9884\u6d4b\u65f6\u95f4\nplt.figure()\n\n# \u751f\u6210\u6837\u672c\u6570\u636e\nX = 5 * rng.rand(10000, 1)\ny = np.sin(X).ravel()\ny[::5] += 3 * (0.5 - rng.rand(X.shape[0] // 5))\nsizes = np.logspace(1, 4, 7, dtype=np.int)\nfor name, estimator in {\"KRR\": KernelRidge(kernel='rbf', alpha=0.1,\n                                           gamma=10),\n                        \"SVR\": SVR(kernel='rbf', C=1e1, gamma=10)}.items():\n    train_time = []\n    test_time = []\n    for train_test_size in sizes:\n        t0 = time.time()\n        estimator.fit(X[:train_test_size], y[:train_test_size])\n        train_time.append(time.time() - t0)\n\n        t0 = time.time()\n        estimator.predict(X_plot[:1000])\n        test_time.append(time.time() - t0)\n\n    plt.plot(sizes, train_time, 'o-', color=\"r\" if name == \"SVR\" else \"g\",\n             label=\"%s (train)\" % name)\n    plt.plot(sizes, test_time, 'o--', color=\"r\" if name == \"SVR\" else \"g\",\n             label=\"%s (test)\" % name)\n\nplt.xscale(\"log\")\nplt.yscale(\"log\")\nplt.xlabel(\"Train size\")\nplt.ylabel(\"Time (seconds)\")\nplt.title('Execution Time')\nplt.legend(loc=\"best\")\n\n# \u53ef\u89c6\u5316\u5b66\u4e60\u66f2\u7ebf\nplt.figure()\n\nsvr = SVR(kernel='rbf', C=1e1, gamma=0.1)\nkr = KernelRidge(kernel='rbf', alpha=0.1, gamma=0.1)\ntrain_sizes, train_scores_svr, test_scores_svr = \\\n    learning_curve(svr, X[:100], y[:100], train_sizes=np.linspace(0.1, 1, 10),\n                   scoring=\"neg_mean_squared_error\", cv=10)\ntrain_sizes_abs, train_scores_kr, test_scores_kr = \\\n    learning_curve(kr, X[:100], y[:100], train_sizes=np.linspace(0.1, 1, 10),\n                   scoring=\"neg_mean_squared_error\", cv=10)\n\nplt.plot(train_sizes, -test_scores_svr.mean(1), 'o-', color=\"r\",\n         label=\"SVR\")\nplt.plot(train_sizes, -test_scores_kr.mean(1), 'o-', color=\"g\",\n         label=\"KRR\")\nplt.xlabel(\"Train size\")\nplt.ylabel(\"Mean Squared Error\")\nplt.title('Learning curves')\nplt.legend(loc=\"best\")\n\nplt.show()\n'''\n", "src/py3.x/ml/8.Regression/regression.py": "#!/usr/bin/python\n# coding:utf8\n'''\nCreated on Jan 8, 2011\nUpdate  on 2017-05-18\nAuthor: Peter Harrington/\u5c0f\u7476\nGitHub: https://github.com/apachecn/AiLearning\n'''\n\nfrom numpy import *\nimport matplotlib.pylab as plt\nfrom time import sleep\nimport bs4\nfrom bs4 import BeautifulSoup\nimport json\nimport urllib.request   # \u5728Python3\u4e2d\u5c06urllib2\u548curllib3\u5408\u5e76\u4e3a\u4e00\u4e2a\u6807\u51c6\u5e93urllib,\u5176\u4e2d\u7684urllib2.urlopen\u66f4\u6539\u4e3aurllib.request.urlopen\n\n\ndef loadDataSet(fileName):\n    \"\"\" \u52a0\u8f7d\u6570\u636e\n        \u89e3\u6790\u4ee5tab\u952e\u5206\u9694\u7684\u6587\u4ef6\u4e2d\u7684\u6d6e\u70b9\u6570\n    Returns: \n        dataMat :   feature \u5bf9\u5e94\u7684\u6570\u636e\u96c6\n        labelMat :  feature \u5bf9\u5e94\u7684\u5206\u7c7b\u6807\u7b7e\uff0c\u5373\u7c7b\u522b\u6807\u7b7e\n    \"\"\"\n    # \u83b7\u53d6\u6837\u672c\u7279\u5f81\u7684\u603b\u6570\uff0c\u4e0d\u7b97\u6700\u540e\u7684\u76ee\u6807\u53d8\u91cf\n    numFeat = len(open(fileName).readline().split('\\t')) - 1\n    dataMat = []\n    labelMat = []\n    fr = open(fileName)\n    for line in fr.readlines():\n        # \u8bfb\u53d6\u6bcf\u4e00\u884c\n        lineArr = []\n        # \u5220\u9664\u4e00\u884c\u4e2d\u4ee5tab\u5206\u9694\u7684\u6570\u636e\u524d\u540e\u7684\u7a7a\u767d\u7b26\u53f7\n        curLine = line.strip().split('\\t')\n        # i \u4ece0\u52302\uff0c\u4e0d\u5305\u62ec2\n        for i in range(numFeat):\n            # \u5c06\u6570\u636e\u6dfb\u52a0\u5230lineArr List\u4e2d\uff0c\u6bcf\u4e00\u884c\u6570\u636e\u6d4b\u8bd5\u6570\u636e\u7ec4\u6210\u4e00\u4e2a\u884c\u5411\u91cf\n            lineArr.append(float(curLine[i]))\n            # \u5c06\u6d4b\u8bd5\u6570\u636e\u7684\u8f93\u5165\u6570\u636e\u90e8\u5206\u5b58\u50a8\u5230dataMat \u7684List\u4e2d\n        dataMat.append(lineArr)\n        # \u5c06\u6bcf\u4e00\u884c\u7684\u6700\u540e\u4e00\u4e2a\u6570\u636e\uff0c\u5373\u7c7b\u522b\uff0c\u6216\u8005\u53eb\u76ee\u6807\u53d8\u91cf\u5b58\u50a8\u5230labelMat List\u4e2d\n        labelMat.append(float(curLine[-1]))\n    return dataMat, labelMat\n\n\ndef standRegres(xArr, yArr):\n    '''\n    Description: \n        \u7ebf\u6027\u56de\u5f52\n    Args:\n        xArr : \u8f93\u5165\u7684\u6837\u672c\u6570\u636e\uff0c\u5305\u542b\u6bcf\u4e2a\u6837\u672c\u6570\u636e\u7684 feature\n        yArr : \u5bf9\u5e94\u4e8e\u8f93\u5165\u6570\u636e\u7684\u7c7b\u522b\u6807\u7b7e\uff0c\u4e5f\u5c31\u662f\u6bcf\u4e2a\u6837\u672c\u5bf9\u5e94\u7684\u76ee\u6807\u53d8\u91cf\n    Returns:\n        ws: \u56de\u5f52\u7cfb\u6570\n    '''\n\n    # mat()\u51fd\u6570\u5c06xArr\uff0cyArr\u8f6c\u6362\u4e3a\u77e9\u9635 mat().T \u4ee3\u8868\u7684\u662f\u5bf9\u77e9\u9635\u8fdb\u884c\u8f6c\u7f6e\u64cd\u4f5c\n    xMat = mat(xArr)\n    yMat = mat(yArr).T\n    # \u77e9\u9635\u4e58\u6cd5\u7684\u6761\u4ef6\u662f\u5de6\u77e9\u9635\u7684\u5217\u6570\u7b49\u4e8e\u53f3\u77e9\u9635\u7684\u884c\u6570\n    xTx = xMat.T * xMat\n    # \u56e0\u4e3a\u8981\u7528\u5230xTx\u7684\u9006\u77e9\u9635\uff0c\u6240\u4ee5\u4e8b\u5148\u9700\u8981\u786e\u5b9a\u8ba1\u7b97\u5f97\u5230\u7684xTx\u662f\u5426\u53ef\u9006\uff0c\u6761\u4ef6\u662f\u77e9\u9635\u7684\u884c\u5217\u5f0f\u4e0d\u4e3a0\n    # linalg.det() \u51fd\u6570\u662f\u7528\u6765\u6c42\u5f97\u77e9\u9635\u7684\u884c\u5217\u5f0f\u7684\uff0c\u5982\u679c\u77e9\u9635\u7684\u884c\u5217\u5f0f\u4e3a0\uff0c\u5219\u8fd9\u4e2a\u77e9\u9635\u662f\u4e0d\u53ef\u9006\u7684\uff0c\u5c31\u65e0\u6cd5\u8fdb\u884c\u63a5\u4e0b\u6765\u7684\u8fd0\u7b97\n    if linalg.det(xTx) == 0.0:\n        print(\"This matrix is singular, cannot do inverse\")\n        return\n    # \u6700\u5c0f\u4e8c\u4e58\u6cd5\n    # http://cwiki.apachecn.org/pages/viewpage.action?pageId=5505133\n    # \u4e66\u4e2d\u7684\u516c\u5f0f\uff0c\u6c42\u5f97w\u7684\u6700\u4f18\u89e3\n    ws = xTx.I * (xMat.T * yMat)\n    return ws\n\n\ndef lwlr(testPoint, xArr, yArr, k=1.0):\n    '''\n        Description: \n            \u5c40\u90e8\u52a0\u6743\u7ebf\u6027\u56de\u5f52\uff0c\u5728\u5f85\u9884\u6d4b\u70b9\u9644\u8fd1\u7684\u6bcf\u4e2a\u70b9\u8d4b\u4e88\u4e00\u5b9a\u7684\u6743\u91cd\uff0c\u5728\u5b50\u96c6\u4e0a\u57fa\u4e8e\u6700\u5c0f\u5747\u65b9\u5dee\u6765\u8fdb\u884c\u666e\u901a\u7684\u56de\u5f52\u3002\n        Args: \n            testPoint: \u6837\u672c\u70b9\n            xArr: \u6837\u672c\u7684\u7279\u5f81\u6570\u636e\uff0c\u5373 feature\n            yArr: \u6bcf\u4e2a\u6837\u672c\u5bf9\u5e94\u7684\u7c7b\u522b\u6807\u7b7e\uff0c\u5373\u76ee\u6807\u53d8\u91cf\n            k:\u5173\u4e8e\u8d4b\u4e88\u6743\u91cd\u77e9\u9635\u7684\u6838\u7684\u4e00\u4e2a\u53c2\u6570\uff0c\u4e0e\u6743\u91cd\u7684\u8870\u51cf\u901f\u7387\u6709\u5173\n        Returns:\n            testPoint * ws: \u6570\u636e\u70b9\u4e0e\u5177\u6709\u6743\u91cd\u7684\u7cfb\u6570\u76f8\u4e58\u5f97\u5230\u7684\u9884\u6d4b\u70b9\n        Notes:\n            \u8fd9\u5176\u4e2d\u4f1a\u7528\u5230\u8ba1\u7b97\u6743\u91cd\u7684\u516c\u5f0f\uff0cw = e^((x^((i))-x) / -2k^2)\n            \u7406\u89e3: x\u4e3a\u67d0\u4e2a\u9884\u6d4b\u70b9\uff0cx^((i))\u4e3a\u6837\u672c\u70b9\uff0c\u6837\u672c\u70b9\u8ddd\u79bb\u9884\u6d4b\u70b9\u8d8a\u8fd1\uff0c\u8d21\u732e\u7684\u8bef\u5dee\u8d8a\u5927\uff08\u6743\u503c\u8d8a\u5927\uff09\uff0c\u8d8a\u8fdc\u5219\u8d21\u732e\u7684\u8bef\u5dee\u8d8a\u5c0f\uff08\u6743\u503c\u8d8a\u5c0f\uff09\u3002\n            \u5173\u4e8e\u9884\u6d4b\u70b9\u7684\u9009\u53d6\uff0c\u5728\u6211\u7684\u4ee3\u7801\u4e2d\u53d6\u7684\u662f\u6837\u672c\u70b9\u3002\u5176\u4e2dk\u662f\u5e26\u5bbd\u53c2\u6570\uff0c\u63a7\u5236w\uff08\u949f\u5f62\u51fd\u6570\uff09\u7684\u5bbd\u7a84\u7a0b\u5ea6\uff0c\u7c7b\u4f3c\u4e8e\u9ad8\u65af\u51fd\u6570\u7684\u6807\u51c6\u5dee\u3002\n            \u7b97\u6cd5\u601d\u8def: \u5047\u8bbe\u9884\u6d4b\u70b9\u53d6\u6837\u672c\u70b9\u4e2d\u7684\u7b2ci\u4e2a\u6837\u672c\u70b9\uff08\u5171m\u4e2a\u6837\u672c\u70b9\uff09\uff0c\u904d\u53861\u5230m\u4e2a\u6837\u672c\u70b9\uff08\u542b\u7b2ci\u4e2a\uff09\uff0c\u7b97\u51fa\u6bcf\u4e00\u4e2a\u6837\u672c\u70b9\u4e0e\u9884\u6d4b\u70b9\u7684\u8ddd\u79bb\uff0c\n            \u4e5f\u5c31\u53ef\u4ee5\u8ba1\u7b97\u51fa\u6bcf\u4e2a\u6837\u672c\u8d21\u732e\u8bef\u5dee\u7684\u6743\u503c\uff0c\u53ef\u4ee5\u770b\u51faw\u662f\u4e00\u4e2a\u6709m\u4e2a\u5143\u7d20\u7684\u5411\u91cf\uff08\u5199\u6210\u5bf9\u89d2\u9635\u5f62\u5f0f\uff09\u3002\n    '''\n    # mat() \u51fd\u6570\u662f\u5c06array\u8f6c\u6362\u4e3a\u77e9\u9635\u7684\u51fd\u6570\uff0c mat().T \u662f\u8f6c\u6362\u4e3a\u77e9\u9635\u4e4b\u540e\uff0c\u518d\u8fdb\u884c\u8f6c\u7f6e\u64cd\u4f5c\n    xMat = mat(xArr)\n    yMat = mat(yArr).T\n    # \u83b7\u5f97xMat\u77e9\u9635\u7684\u884c\u6570\n    m = shape(xMat)[0]\n    # eye()\u8fd4\u56de\u4e00\u4e2a\u5bf9\u89d2\u7ebf\u5143\u7d20\u4e3a1\uff0c\u5176\u4ed6\u5143\u7d20\u4e3a0\u7684\u4e8c\u7ef4\u6570\u7ec4\uff0c\u521b\u5efa\u6743\u91cd\u77e9\u9635weights\uff0c\u8be5\u77e9\u9635\u4e3a\u6bcf\u4e2a\u6837\u672c\u70b9\u521d\u59cb\u5316\u4e86\u4e00\u4e2a\u6743\u91cd\n    weights = mat(eye((m)))\n    for j in range(m):\n        # testPoint \u7684\u5f62\u5f0f\u662f \u4e00\u4e2a\u884c\u5411\u91cf\u7684\u5f62\u5f0f\n        # \u8ba1\u7b97 testPoint \u4e0e\u8f93\u5165\u6837\u672c\u70b9\u4e4b\u95f4\u7684\u8ddd\u79bb\uff0c\u7136\u540e\u4e0b\u9762\u8ba1\u7b97\u51fa\u6bcf\u4e2a\u6837\u672c\u8d21\u732e\u8bef\u5dee\u7684\u6743\u503c\n        diffMat = testPoint - xMat[j, :]\n        # k\u63a7\u5236\u8870\u51cf\u7684\u901f\u5ea6\n        weights[j, j] = exp(diffMat * diffMat.T / (-2.0 * k ** 2))\n    # \u6839\u636e\u77e9\u9635\u4e58\u6cd5\u8ba1\u7b97 xTx \uff0c\u5176\u4e2d\u7684 weights \u77e9\u9635\u662f\u6837\u672c\u70b9\u5bf9\u5e94\u7684\u6743\u91cd\u77e9\u9635\n    xTx = xMat.T * (weights * xMat)\n    if linalg.det(xTx) == 0.0:\n        print(\"This matrix is singular, cannot do inverse\")\n        return\n    # \u8ba1\u7b97\u51fa\u56de\u5f52\u7cfb\u6570\u7684\u4e00\u4e2a\u4f30\u8ba1\n    ws = xTx.I * (xMat.T * (weights * yMat))\n    return testPoint * ws\n\n\ndef lwlrTest(testArr, xArr, yArr, k=1.0):\n    '''\n        Description: \n            \u6d4b\u8bd5\u5c40\u90e8\u52a0\u6743\u7ebf\u6027\u56de\u5f52\uff0c\u5bf9\u6570\u636e\u96c6\u4e2d\u6bcf\u4e2a\u70b9\u8c03\u7528 lwlr() \u51fd\u6570\n        Args: \n            testArr: \u6d4b\u8bd5\u6240\u7528\u7684\u6240\u6709\u6837\u672c\u70b9\n            xArr: \u6837\u672c\u7684\u7279\u5f81\u6570\u636e\uff0c\u5373 feature\n            yArr: \u6bcf\u4e2a\u6837\u672c\u5bf9\u5e94\u7684\u7c7b\u522b\u6807\u7b7e\uff0c\u5373\u76ee\u6807\u53d8\u91cf\n            k: \u63a7\u5236\u6838\u51fd\u6570\u7684\u8870\u51cf\u901f\u7387\n        Returns: \n            yHat: \u9884\u6d4b\u70b9\u7684\u4f30\u8ba1\u503c\n    '''\n    # \u5f97\u5230\u6837\u672c\u70b9\u7684\u603b\u6570\n    m = shape(testArr)[0]\n    # \u6784\u5efa\u4e00\u4e2a\u5168\u90e8\u90fd\u662f 0 \u7684 1 * m \u7684\u77e9\u9635\n    yHat = zeros(m)\n    # \u5faa\u73af\u6240\u6709\u7684\u6570\u636e\u70b9\uff0c\u5e76\u5c06lwlr\u8fd0\u7528\u4e8e\u6240\u6709\u7684\u6570\u636e\u70b9\n    for i in range(m):\n        yHat[i] = lwlr(testArr[i], xArr, yArr, k)\n    # \u8fd4\u56de\u4f30\u8ba1\u503c\n    return yHat\n\n\ndef lwlrTestPlot(xArr, yArr, k=1.0):\n    '''\n        Description:\n            \u9996\u5148\u5c06 X \u6392\u5e8f\uff0c\u5176\u4f59\u7684\u90fd\u4e0elwlrTest\u76f8\u540c\uff0c\u8fd9\u6837\u66f4\u5bb9\u6613\u7ed8\u56fe\n        Args: \n            xArr: \u6837\u672c\u7684\u7279\u5f81\u6570\u636e\uff0c\u5373 feature\n            yArr: \u6bcf\u4e2a\u6837\u672c\u5bf9\u5e94\u7684\u7c7b\u522b\u6807\u7b7e\uff0c\u5373\u76ee\u6807\u53d8\u91cf\uff0c\u5b9e\u9645\u503c\n            k: \u63a7\u5236\u6838\u51fd\u6570\u7684\u8870\u51cf\u901f\u7387\u7684\u6709\u5173\u53c2\u6570\uff0c\u8fd9\u91cc\u8bbe\u5b9a\u7684\u662f\u5e38\u91cf\u503c 1\n        Return: \n            yHat: \u6837\u672c\u70b9\u7684\u4f30\u8ba1\u503c\n            xCopy: xArr\u7684\u590d\u5236\n    '''\n    # \u751f\u6210\u4e00\u4e2a\u4e0e\u76ee\u6807\u53d8\u91cf\u6570\u76ee\u76f8\u540c\u7684 0 \u5411\u91cf\n    yHat = zeros(shape(yArr))\n    # \u5c06 xArr \u8f6c\u6362\u4e3a \u77e9\u9635\u5f62\u5f0f\n    xCopy = mat(xArr)\n    # \u6392\u5e8f\n    xCopy.sort(0)\n    # \u5f00\u59cb\u5faa\u73af\uff0c\u4e3a\u6bcf\u4e2a\u6837\u672c\u70b9\u8fdb\u884c\u5c40\u90e8\u52a0\u6743\u7ebf\u6027\u56de\u5f52\uff0c\u5f97\u5230\u6700\u7ec8\u7684\u76ee\u6807\u53d8\u91cf\u4f30\u8ba1\u503c\n    for i in range(shape(xArr)[0]):\n        yHat[i] = lwlr(xCopy[i], xArr, yArr, k)\n    return yHat, xCopy\n\n\ndef rssError(yArr, yHatArr):\n    '''\n        Desc:\n            \u8ba1\u7b97\u5206\u6790\u9884\u6d4b\u8bef\u5dee\u7684\u5927\u5c0f\n        Args:\n            yArr: \u771f\u5b9e\u7684\u76ee\u6807\u53d8\u91cf\n            yHatArr: \u9884\u6d4b\u5f97\u5230\u7684\u4f30\u8ba1\u503c\n        Returns:\n            \u8ba1\u7b97\u771f\u5b9e\u503c\u548c\u4f30\u8ba1\u503c\u5f97\u5230\u7684\u503c\u7684\u5e73\u65b9\u548c\u4f5c\u4e3a\u6700\u540e\u7684\u8fd4\u56de\u503c\n    '''\n    return ((yArr - yHatArr) ** 2).sum()\n\n\ndef ridgeRegres(xMat, yMat, lam=0.2):\n    '''\n        Desc: \n            \u8fd9\u4e2a\u51fd\u6570\u5b9e\u73b0\u4e86\u7ed9\u5b9a lambda \u4e0b\u7684\u5cad\u56de\u5f52\u6c42\u89e3\u3002\n            \u5982\u679c\u6570\u636e\u7684\u7279\u5f81\u6bd4\u6837\u672c\u70b9\u8fd8\u591a\uff0c\u5c31\u4e0d\u80fd\u518d\u4f7f\u7528\u4e0a\u9762\u4ecb\u7ecd\u7684\u7684\u7ebf\u6027\u56de\u5f52\u548c\u5c40\u90e8\u73b0\u884c\u56de\u5f52\u4e86\uff0c\u56e0\u4e3a\u8ba1\u7b97 (xTx)^(-1)\u4f1a\u51fa\u73b0\u9519\u8bef\u3002\n            \u5982\u679c\u7279\u5f81\u6bd4\u6837\u672c\u70b9\u8fd8\u591a\uff08n > m\uff09\uff0c\u4e5f\u5c31\u662f\u8bf4\uff0c\u8f93\u5165\u6570\u636e\u7684\u77e9\u9635x\u4e0d\u662f\u6ee1\u79e9\u77e9\u9635\u3002\u975e\u6ee1\u79e9\u77e9\u9635\u5728\u6c42\u9006\u65f6\u4f1a\u51fa\u73b0\u95ee\u9898\u3002\n            \u4e3a\u4e86\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\uff0c\u6211\u4eec\u4e0b\u8fb9\u8bb2\u4e00\u4e0b: \u5cad\u56de\u5f52\uff0c\u8fd9\u662f\u6211\u4eec\u8981\u8bb2\u7684\u7b2c\u4e00\u79cd\u7f29\u51cf\u65b9\u6cd5\u3002\n        Args: \n            xMat: \u6837\u672c\u7684\u7279\u5f81\u6570\u636e\uff0c\u5373 feature\n            yMat: \u6bcf\u4e2a\u6837\u672c\u5bf9\u5e94\u7684\u7c7b\u522b\u6807\u7b7e\uff0c\u5373\u76ee\u6807\u53d8\u91cf\uff0c\u5b9e\u9645\u503c\n            lam: \u5f15\u5165\u7684\u4e00\u4e2a\u03bb\u503c\uff0c\u4f7f\u5f97\u77e9\u9635\u975e\u5947\u5f02\n        Returns: \n            \u7ecf\u8fc7\u5cad\u56de\u5f52\u516c\u5f0f\u8ba1\u7b97\u5f97\u5230\u7684\u56de\u5f52\u7cfb\u6570\n    '''\n\n    xTx = xMat.T * xMat\n    # \u5cad\u56de\u5f52\u5c31\u662f\u5728\u77e9\u9635 xTx \u4e0a\u52a0\u4e00\u4e2a \u03bbI \u4ece\u800c\u4f7f\u5f97\u77e9\u9635\u975e\u5947\u5f02\uff0c\u8fdb\u800c\u80fd\u5bf9 xTx + \u03bbI \u6c42\u9006\n    denom = xTx + eye(shape(xMat)[1]) * lam\n    # \u68c0\u67e5\u884c\u5217\u5f0f\u662f\u5426\u4e3a\u96f6\uff0c\u5373\u77e9\u9635\u662f\u5426\u53ef\u9006\uff0c\u884c\u5217\u5f0f\u4e3a0\u7684\u8bdd\u5c31\u4e0d\u53ef\u9006\uff0c\u4e0d\u4e3a0\u7684\u8bdd\u5c31\u662f\u53ef\u9006\u3002\n    if linalg.det(denom) == 0.0:\n        print(\"This matrix is singular, cannot do inverse\")\n        return\n    ws = denom.I * (xMat.T * yMat)\n    return ws\n\n\ndef ridgeTest(xArr, yArr):\n    '''\n        Desc: \n            \u51fd\u6570 ridgeTest() \u7528\u4e8e\u5728\u4e00\u7ec4 \u03bb \u4e0a\u6d4b\u8bd5\u7ed3\u679c\n        Args: \n            xArr: \u6837\u672c\u6570\u636e\u7684\u7279\u5f81\uff0c\u5373 feature\n            yArr: \u6837\u672c\u6570\u636e\u7684\u7c7b\u522b\u6807\u7b7e\uff0c\u5373\u771f\u5b9e\u6570\u636e\n        Returns: \n            wMat: \u5c06\u6240\u6709\u7684\u56de\u5f52\u7cfb\u6570\u8f93\u51fa\u5230\u4e00\u4e2a\u77e9\u9635\u5e76\u8fd4\u56de\n    '''\n\n    xMat = mat(xArr)\n    yMat = mat(yArr).T\n    # \u8ba1\u7b97Y\u7684\u5747\u503c\n    yMean = mean(yMat, 0)\n    # Y\u7684\u6240\u6709\u7684\u7279\u5f81\u51cf\u53bb\u5747\u503c\n    yMat = yMat - yMean\n    # \u6807\u51c6\u5316 x\uff0c\u8ba1\u7b97 xMat \u5e73\u5747\u503c\n    xMeans = mean(xMat, 0)\n    # \u7136\u540e\u8ba1\u7b97 X\u7684\u65b9\u5dee\n    xVar = var(xMat, 0)\n    # \u6240\u6709\u7279\u5f81\u90fd\u51cf\u53bb\u5404\u81ea\u7684\u5747\u503c\u5e76\u9664\u4ee5\u65b9\u5dee\n    xMat = (xMat - xMeans) / xVar\n    # \u53ef\u4ee5\u5728 30 \u4e2a\u4e0d\u540c\u7684 lambda \u4e0b\u8c03\u7528 ridgeRegres() \u51fd\u6570\u3002\n    numTestPts = 30\n    # \u521b\u5efa30 * m \u7684\u5168\u90e8\u6570\u636e\u4e3a0 \u7684\u77e9\u9635\n    wMat = zeros((numTestPts, shape(xMat)[1]))\n    for i in range(numTestPts):\n        # exp() \u8fd4\u56de e^x\n        ws = ridgeRegres(xMat, yMat, exp(i - 10))\n        wMat[i, :] = ws.T\n    return wMat\n\n\ndef regularize(xMat):  # \u6309\u5217\u8fdb\u884c\u89c4\u8303\u5316\n    inMat = xMat.copy()\n    inMeans = mean(inMat, 0)  # \u8ba1\u7b97\u5e73\u5747\u503c\u7136\u540e\u51cf\u53bb\u5b83\n    inVar = var(inMat, 0)  # \u8ba1\u7b97\u9664\u4ee5Xi\u7684\u65b9\u5dee\n    inMat = (inMat - inMeans) / inVar\n    return inMat\n\n\ndef stageWise(xArr, yArr, eps=0.01, numIt=100):\n    xMat = mat(xArr)\n    yMat = mat(yArr).T\n    yMean = mean(yMat, 0)\n    yMat = yMat - yMean  # \u4e5f\u53ef\u4ee5\u89c4\u5219\u5316ys\u4f46\u4f1a\u5f97\u5230\u66f4\u5c0f\u7684coef\n    xMat = regularize(xMat)\n    m, n = shape(xMat)\n    returnMat = zeros((numIt, n))  # \u6d4b\u8bd5\u4ee3\u7801\u5220\u9664\n    ws = zeros((n, 1))\n    wsTest = ws.copy()\n    wsMax = ws.copy()\n    for i in range(numIt):\n        print(ws.T)\n        lowestError = inf\n        for j in range(n):\n            for sign in [-1, 1]:\n                wsTest = ws.copy()\n                wsTest[j] += eps * sign\n                yTest = xMat * wsTest\n                rssE = rssError(yMat.A, yTest.A)\n                if rssE < lowestError:\n                    lowestError = rssE\n                    wsMax = wsTest\n        ws = wsMax.copy()\n        returnMat[i, :] = ws.T\n    return returnMat\n\n\n# def scrapePage(inFile, outFile, yr, numPce, origPrc):\n#     fr = open(inFile)\n#     fw = open(outFile, 'a')  # a is append mode writing\n#     soup = BeautifulSoup(fr.read())\n#     i = 1\n#     currentRow = soup.findAll('table', r=\"%d\" % i)\n#     while (len(currentRow) != 0):\n#         title = currentRow[0].findAll('a')[1].text\n#         lwrTitle = title.lower()\n#         if (lwrTitle.find('new') > -1) or (lwrTitle.find('nisb') > -1):\n#             newFlag = 1.0\n#         else:\n#             newFlag = 0.0\n#         soldUnicde = currentRow[0].findAll('td')[3].findAll('span')\n#         if len(soldUnicde) == 0:\n#             print(\"item #%d did not sell\" % i)\n#         else:\n#             soldPrice = currentRow[0].findAll('td')[4]\n#             priceStr = soldPrice.text\n#             priceStr = priceStr.replace('$', '')  # strips out $\n#             priceStr = priceStr.replace(',', '')  # strips out ,\n#             if len(soldPrice) > 1:\n#                 priceStr = priceStr.replace('Free shipping', '')  # strips out Free Shipping\n#             print(\"%s\\t%d\\t%s\" % (priceStr, newFlag, title))\n#             fw.write(\"%d\\t%d\\t%d\\t%f\\t%s\\n\" % (yr, numPce, newFlag, origPrc, priceStr))\n#         i += 1\n#         currentRow = soup.findAll('table', r=\"%d\" % i)\n#     fw.close()\n\n\n# --------------------------------------------------------------\n# \u9884\u6d4b\u4e50\u9ad8\u73a9\u5177\u5957\u88c5\u7684\u4ef7\u683c ------ \u6700\u521d\u7684\u7248\u672c\uff0c\u56e0\u4e3a\u73b0\u5728 google \u7684 api \u53d8\u5316\uff0c\u65e0\u6cd5\u83b7\u53d6\u6570\u636e\n# \u6545\u6539\u4e3a\u4e86\u4e0b\u8fb9\u7684\u6837\u5b50\uff0c\u4f46\u662f\u9700\u8981\u5b89\u88c5\u4e00\u4e2a beautifulSoup \u8fd9\u4e2a\u7b2c\u4e09\u65b9\u7f51\u9875\u6587\u672c\u89e3\u6790\u5668\uff0c\u5b89\u88c5\u5f88\u7b80\u5355\uff0c\u89c1\u4e0b\u8fb9\n# from time import sleep\n# import json\n# \u8fd9\u91cc\u7279\u522b\u6307\u51fa \u6b63\u786e\u7684\u4f7f\u7528\u65b9\u6cd5\u4e3a\u4e0b\u9762\u7684\u8bed\u53e5\u4f7f\u7528,from urllib import request \u5c06\u4f1a\u62a5\u9519,\u5177\u4f53\u7ec6\u8282\u67e5\u770b\u5b98\u65b9\u6587\u6863\n# import urllib.request   # \u5728Python3\u4e2d\u5c06urllib2\u548curllib\u7b49\u4e94\u4e2a\u6a21\u5757\u5408\u5e76\u4e3a\u4e00\u4e2a\u6807\u51c6\u5e93urllib,\u5176\u4e2d\u7684urllib2.urlopen\u66f4\u6539\u4e3aurllib.request.urlopen\n\ndef searchForSet(retX, retY, setNum, yr, numPce, origPrc):\n    sleep(10)\n    myAPIstr = 'AIzaSyD2cR2KFyx12hXu6PFU-wrWot3NXvko8vY'\n    searchURL = 'https://www.googleapis.com/shopping/search/v1/public/products?key=%s&country=US&q=lego+%d&alt=json' % (myAPIstr, setNum)\n    pg = urllib.request.urlopen(searchURL)\n    retDict = json.loads(pg.read())    # \u8f6c\u6362\u4e3ajson\u683c\u5f0f\n    for i in range(len(retDict['items'])):\n        try:\n            currItem = retDict['items'][i]\n            if currItem['product']['condition'] == 'new':\n                newFlag = 1\n            else: newFlag = 0\n            listOfInv = currItem['product']['inventories']\n            for item in listOfInv:\n                sellingPrice = item['price']\n                if  sellingPrice > origPrc * 0.5:\n                    print (\"%d\\t%d\\t%d\\t%f\\t%f\" % (yr,numPce,newFlag,origPrc, sellingPrice))\n                    retX.append([yr, numPce, newFlag, origPrc])\n                    retY.append(sellingPrice)\n        except: print ('problem with item %d' % i)\n\ndef setDataCollect(retX, retY):\n    searchForSet(retX, retY, 8288, 2006, 800, 49.99)\n    searchForSet(retX, retY, 10030, 2002, 3096, 269.99)\n    searchForSet(retX, retY, 10179, 2007, 5195, 499.99)\n    searchForSet(retX, retY, 10181, 2007, 3428, 199.99)\n    searchForSet(retX, retY, 10189, 2008, 5922, 299.99)\n    searchForSet(retX, retY, 10196, 2009, 3263, 249.99)\n\ndef crossValidation(xArr,yArr,numVal=10):\n    m = len(yArr)                           \n    indexList = range(m)\n    errorMat = zeros((numVal,30))#create error mat 30columns numVal rows\u521b\u5efaerror mat 30columns numVal \u884c\n    for i in range(numVal):\n        trainX=[]; trainY=[]\n        testX = []; testY = []\n        random.shuffle(indexList)\n        for j in range(m):#create training set based on first 90% of values in indexList\n                          #\u57fa\u4e8eindexList\u4e2d\u7684\u524d90%\u7684\u503c\u521b\u5efa\u8bad\u7ec3\u96c6\n            if j < m*0.9: \n                trainX.append(xArr[indexList[j]])\n                trainY.append(yArr[indexList[j]])\n            else:\n                testX.append(xArr[indexList[j]])\n                testY.append(yArr[indexList[j]])\n        wMat = ridgeTest(trainX,trainY)    #get 30 weight vectors from ridge\n        for k in range(30):#loop over all of the ridge estimates\n            matTestX = mat(testX); matTrainX=mat(trainX)\n            meanTrain = mean(matTrainX,0)\n            varTrain = var(matTrainX,0)\n            matTestX = (matTestX-meanTrain)/varTrain #regularize test with training params\n            yEst = matTestX * mat(wMat[k,:]).T + mean(trainY)#test ridge results and store\n            errorMat[i,k]=rssError(yEst.T.A,array(testY))\n            #print (errorMat[i,k])\n    meanErrors = mean(errorMat,0)#calc avg performance of the different ridge weight vectors\n    minMean = float(min(meanErrors))\n    bestWeights = wMat[nonzero(meanErrors==minMean)]\n    #can unregularize to get model\n    #when we regularized we wrote Xreg = (x-meanX)/var(x)\n    #we can now write in terms of x not Xreg:  x*w/var(x) - meanX/var(x) +meanY\n    xMat = mat(xArr); yMat=mat(yArr).T\n    meanX = mean(xMat,0); varX = var(xMat,0)\n    unReg = bestWeights/varX\n    print (\"the best model from Ridge Regression is:\\n\",unReg)\n    print (\"with constant term: \",-1*sum(multiply(meanX,unReg)) + mean(yMat))\n\n# ----------------------------------------------------------------------------\n# \u9884\u6d4b\u4e50\u9ad8\u73a9\u5177\u5957\u88c5\u7684\u4ef7\u683c \u53ef\u8fd0\u884c\u7248\u672c\uff0c\u6211\u4eec\u628a\u4e50\u9ad8\u6570\u636e\u5b58\u50a8\u5230\u4e86\u6211\u4eec\u7684 input \u6587\u4ef6\u5939\u4e0b\uff0c\u4f7f\u7528 urllib\u722c\u53d6,bs4\u89e3\u6790\u5185\u5bb9\n# \u524d\u63d0: \u5b89\u88c5 BeautifulSoup\uff0c\u6b65\u9aa4\u5982\u4e0b\n# \u5728\u8fd9\u4e2a\u9875\u9762 https://www.crummy.com/software/BeautifulSoup/bs4/download/4.4/ \u4e0b\u8f7d\uff0cbeautifulsoup4-4.4.1.tar.gz\n# \u5c06\u4e0b\u8f7d\u6587\u4ef6\u89e3\u538b\uff0c\u4f7f\u7528 windows \u7248\u672c\u7684 cmd \u547d\u4ee4\u884c\uff0c\u8fdb\u5165\u89e3\u538b\u7684\u5305\uff0c\u8f93\u5165\u4ee5\u4e0b\u4e24\u884c\u547d\u4ee4\u5373\u53ef\u5b8c\u6210\u5b89\u88c5\n# python setup.py build\n# python setup.py install\n# \u5982\u679c\u4e3alinux\u6216\u8005mac\u7cfb\u7edf\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528pip\u8fdb\u884c\u5b89\u88c5 pip3 install bs4\n# ----------------------------------------------------------------------------\n\n\n# \u4ece\u9875\u9762\u8bfb\u53d6\u6570\u636e\uff0c\u751f\u6210retX\u548cretY\u5217\u8868\ndef scrapePage(retX, retY, inFile, yr, numPce, origPrc):\n    # \u6253\u5f00\u5e76\u8bfb\u53d6HTML\u6587\u4ef6\n    fr = open(inFile)    # \u8fd9\u91cc\u63a8\u8350\u4f7f\u7528with open() \u751f\u6210\u5668,\u8fd9\u6837\u8282\u7701\u5185\u5b58\u4e5f\u53ef\u4ee5\u907f\u514d\u6700\u540e\u5fd8\u8bb0\u5173\u95ed\u6587\u4ef6\u7684\u95ee\u9898\n    soup = BeautifulSoup(fr.read())\n    i=1\n    # \u6839\u636eHTML\u9875\u9762\u7ed3\u6784\u8fdb\u884c\u89e3\u6790\n    currentRow = soup.findAll('table', r=\"%d\" % i)\n    while(len(currentRow)!=0):\n        currentRow = soup.findAll('table', r=\"%d\" % i)\n        title = currentRow[0].findAll('a')[1].text\n        lwrTitle = title.lower()\n        # \u67e5\u627e\u662f\u5426\u6709\u5168\u65b0\u6807\u7b7e\n        if (lwrTitle.find('new') > -1) or (lwrTitle.find('nisb') > -1):\n            newFlag = 1.0\n        else:\n            newFlag = 0.0\n        # \u67e5\u627e\u662f\u5426\u5df2\u7ecf\u6807\u5fd7\u51fa\u552e\uff0c\u6211\u4eec\u53ea\u6536\u96c6\u5df2\u51fa\u552e\u7684\u6570\u636e\n        soldUnicde = currentRow[0].findAll('td')[3].findAll('span')\n        if len(soldUnicde)==0:\n            print (\"item #%d did not sell\" % i)\n        else:\n            # \u89e3\u6790\u9875\u9762\u83b7\u53d6\u5f53\u524d\u4ef7\u683c\n            soldPrice = currentRow[0].findAll('td')[4]\n            priceStr = soldPrice.text\n            priceStr = priceStr.replace('$','') #strips out $\n            priceStr = priceStr.replace(',','') #strips out ,\n            if len(soldPrice)>1:\n                priceStr = priceStr.replace('Free shipping', '')\n            sellingPrice = float(priceStr)\n            # \u53bb\u6389\u4e0d\u5b8c\u6574\u7684\u5957\u88c5\u4ef7\u683c\n            if  sellingPrice > origPrc * 0.5:\n                    print (\"%d\\t%d\\t%d\\t%f\\t%f\" % (yr,numPce,newFlag,origPrc, sellingPrice))\n                    retX.append([yr, numPce, newFlag, origPrc])\n                    retY.append(sellingPrice)\n        i += 1\n        currentRow = soup.findAll('table', r=\"%d\" % i)\n\n'''\n# \u4f9d\u6b21\u8bfb\u53d6\u516d\u79cd\u4e50\u9ad8\u5957\u88c5\u7684\u6570\u636e\uff0c\u5e76\u751f\u6210\u6570\u636e\u77e9\u9635        \ndef setDataCollect(retX, retY):\n    scrapePage(retX, retY, 'data/8.Regression/setHtml/lego8288.html', 2006, 800, 49.99)\n    scrapePage(retX, retY, 'data/8.Regression/setHtml/lego10030.html', 2002, 3096, 269.99)\n    scrapePage(retX, retY, 'data/8.Regression/setHtml/lego10179.html', 2007, 5195, 499.99)\n    scrapePage(retX, retY, 'data/8.Regression/setHtml/lego10181.html', 2007, 3428, 199.99)\n    scrapePage(retX, retY, 'data/8.Regression/setHtml/lego10189.html', 2008, 5922, 299.99)\n    scrapePage(retX, retY, 'data/8.Regression/setHtml/lego10196.html', 2009, 3263, 249.99)\n# \u4ea4\u53c9\u9a8c\u8bc1\u6d4b\u8bd5\u5cad\u56de\u5f52\ndef crossValidation(xArr,yArr,numVal=10):\n    # \u83b7\u5f97\u6570\u636e\u70b9\u4e2a\u6570\uff0cxArr\u548cyArr\u5177\u6709\u76f8\u540c\u957f\u5ea6\n    m = len(yArr)\n    indexList = range(m)\n    errorMat = zeros((numVal,30))\n    # \u4e3b\u5faa\u73af \u4ea4\u53c9\u9a8c\u8bc1\u5faa\u73af\n    for i in range(numVal):\n        # \u968f\u673a\u62c6\u5206\u6570\u636e\uff0c\u5c06\u6570\u636e\u5206\u4e3a\u8bad\u7ec3\u96c6\uff0890%\uff09\u548c\u6d4b\u8bd5\u96c6\uff0810%\uff09\n        trainX=[]; trainY=[]\n        testX = []; testY = []\n        # \u5bf9\u6570\u636e\u8fdb\u884c\u6df7\u6d17\u64cd\u4f5c\n        random.shuffle(indexList)\n        # \u5207\u5206\u8bad\u7ec3\u96c6\u548c\u6d4b\u8bd5\u96c6\n        for j in range(m):\n            if j < m*0.9: \n                trainX.append(xArr[indexList[j]])\n                trainY.append(yArr[indexList[j]])\n            else:\n                testX.append(xArr[indexList[j]])\n                testY.append(yArr[indexList[j]])\n        # \u83b7\u5f97\u56de\u5f52\u7cfb\u6570\u77e9\u9635\n        wMat = ridgeTest(trainX,trainY)\n        # \u5faa\u73af\u904d\u5386\u77e9\u9635\u4e2d\u768430\u7ec4\u56de\u5f52\u7cfb\u6570\n        for k in range(30):\n            # \u8bfb\u53d6\u8bad\u7ec3\u96c6\u548c\u6570\u636e\u96c6\n            matTestX = mat(testX); matTrainX=mat(trainX)\n            # \u5bf9\u6570\u636e\u8fdb\u884c\u6807\u51c6\u5316\n            meanTrain = mean(matTrainX,0)\n            varTrain = var(matTrainX,0)\n            matTestX = (matTestX-meanTrain)/varTrain\n            # \u6d4b\u8bd5\u56de\u5f52\u6548\u679c\u5e76\u5b58\u50a8\n            yEst = matTestX * mat(wMat[k,:]).T + mean(trainY)\n            # \u8ba1\u7b97\u8bef\u5dee\n            errorMat[i,k] = ((yEst.T.A-array(testY))**2).sum()\n    # \u8ba1\u7b97\u8bef\u5dee\u4f30\u8ba1\u503c\u7684\u5747\u503c\n    meanErrors = mean(errorMat,0)\n    minMean = float(min(meanErrors))\n    bestWeights = wMat[nonzero(meanErrors==minMean)]\n    # \u4e0d\u8981\u4f7f\u7528\u6807\u51c6\u5316\u7684\u6570\u636e\uff0c\u9700\u8981\u5bf9\u6570\u636e\u8fdb\u884c\u8fd8\u539f\u6765\u5f97\u5230\u8f93\u51fa\u7ed3\u679c\n    xMat = mat(xArr); yMat=mat(yArr).T\n    meanX = mean(xMat,0); varX = var(xMat,0)\n    unReg = bestWeights/varX\n    # \u8f93\u51fa\u6784\u5efa\u7684\u6a21\u578b\n    print (\"the best model from Ridge Regression is:\\n\",unReg)\n    print (\"with constant term: \",-1*sum(multiply(meanX,unReg)) + mean(yMat))\n\n'''\n\n# test for standRegression\ndef regression1():\n    xArr, yArr = loadDataSet(\"data/8.Regression/data.txt\")\n    xMat = mat(xArr)\n    yMat = mat(yArr)\n    ws = standRegres(xArr, yArr)\n    fig = plt.figure()\n    ax = fig.add_subplot(111)  # add_subplot(349)\u51fd\u6570\u7684\u53c2\u6570\u7684\u610f\u601d\u662f\uff0c\u5c06\u753b\u5e03\u5206\u62103\u884c4\u5217\u56fe\u50cf\u753b\u5728\u4ece\u5de6\u5230\u53f3\u4ece\u4e0a\u5230\u4e0b\u7b2c9\u5757\n    ax.scatter([xMat[:, 1].flatten()], [yMat.T[:, 0].flatten().A[0]])  # scatter \u7684x\u662fxMat\u4e2d\u7684\u7b2c\u4e8c\u5217\uff0cy\u662fyMat\u7684\u7b2c\u4e00\u5217\n    xCopy = xMat.copy()\n    xCopy.sort(0)\n    yHat = xCopy * ws\n    ax.plot(xCopy[:, 1], yHat)\n    plt.show()\n\n\ndef regression2():\n    xArr, yArr = loadDataSet(\"data/8.Regression/data.txt\")\n    yHat = lwlrTest(xArr, xArr, yArr, 0.003)\n    xMat = mat(xArr)\n    srtInd = xMat[:, 1].argsort(0)  # argsort()\u51fd\u6570\u662f\u5c06x\u4e2d\u7684\u5143\u7d20\u4ece\u5c0f\u5230\u5927\u6392\u5217\uff0c\u63d0\u53d6\u5176\u5bf9\u5e94\u7684index(\u7d22\u5f15)\uff0c\u7136\u540e\u8f93\u51fa\n    xSort = xMat[srtInd][:, 0, :]\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    ax.plot(xSort[:, 1], yHat[srtInd])\n    ax.scatter([xMat[:, 1].flatten().A[0]], [mat(yArr).T.flatten().A[0]], s=2, c='red')\n    plt.show()\n\n\n# test for abloneDataSet\ndef abaloneTest():\n    '''\n    Desc:\n        \u9884\u6d4b\u9c8d\u9c7c\u7684\u5e74\u9f84\n    Args:\n        None\n    Returns:\n        None\n    '''\n    # \u52a0\u8f7d\u6570\u636e\n    abX, abY = loadDataSet(\"data/8.Regression/abalone.txt\")\n    # \u4f7f\u7528\u4e0d\u540c\u7684\u6838\u8fdb\u884c\u9884\u6d4b\n    oldyHat01 = lwlrTest(abX[0:99], abX[0:99], abY[0:99], 0.1)\n    oldyHat1 = lwlrTest(abX[0:99], abX[0:99], abY[0:99], 1)\n    oldyHat10 = lwlrTest(abX[0:99], abX[0:99], abY[0:99], 10)\n    # \u6253\u5370\u51fa\u4e0d\u540c\u7684\u6838\u9884\u6d4b\u503c\u4e0e\u8bad\u7ec3\u6570\u636e\u96c6\u4e0a\u7684\u771f\u5b9e\u503c\u4e4b\u95f4\u7684\u8bef\u5dee\u5927\u5c0f\n    print(\"old yHat01 error Size is :\", rssError(abY[0:99], oldyHat01.T))\n    print(\"old yHat1 error Size is :\", rssError(abY[0:99], oldyHat1.T))\n    print(\"old yHat10 error Size is :\", rssError(abY[0:99], oldyHat10.T))\n\n    # \u6253\u5370\u51fa \u4e0d\u540c\u7684\u6838\u9884\u6d4b\u503c \u4e0e \u65b0\u6570\u636e\u96c6\uff08\u6d4b\u8bd5\u6570\u636e\u96c6\uff09\u4e0a\u7684\u771f\u5b9e\u503c\u4e4b\u95f4\u7684\u8bef\u5dee\u5927\u5c0f\n    newyHat01 = lwlrTest(abX[100:199], abX[0:99], abY[0:99], 0.1)\n    print(\"new yHat01 error Size is :\", rssError(abY[0:99], newyHat01.T))\n    newyHat1 = lwlrTest(abX[100:199], abX[0:99], abY[0:99], 1)\n    print(\"new yHat1 error Size is :\", rssError(abY[0:99], newyHat1.T))\n    newyHat10 = lwlrTest(abX[100:199], abX[0:99], abY[0:99], 10)\n    print(\"new yHat10 error Size is :\", rssError(abY[0:99], newyHat10.T))\n\n    # \u4f7f\u7528\u7b80\u5355\u7684 \u7ebf\u6027\u56de\u5f52 \u8fdb\u884c\u9884\u6d4b\uff0c\u4e0e\u4e0a\u9762\u7684\u8ba1\u7b97\u8fdb\u884c\u6bd4\u8f83\n    standWs = standRegres(abX[0:99], abY[0:99])\n    standyHat = mat(abX[100:199]) * standWs\n    print(\"standRegress error Size is:\", rssError(abY[100:199], standyHat.T.A))\n\n\n# test for ridgeRegression\ndef regression3():\n    abX, abY = loadDataSet(\"data/8.Regression/abalone.txt\")\n    ridgeWeights = ridgeTest(abX, abY)\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    ax.plot(ridgeWeights)\n    plt.show()\n\n\n# test for stageWise\ndef regression4():\n    xArr, yArr = loadDataSet(\"data/8.Regression/abalone.txt\")\n    stageWise(xArr, yArr, 0.01, 200)\n    xMat = mat(xArr)\n    yMat = mat(yArr).T\n    xMat = regularize(xMat)\n    yM = mean(yMat, 0)\n    yMat = yMat - yM\n    weights = standRegres(xMat, yMat.T)\n    print(weights.T)\n\n\n# predict for lego's price\ndef regression5():\n    lgX = []\n    lgY = []\n    setDataCollect(lgX, lgY)\n    crossValidation(lgX, lgY, 10)\n\n\nif __name__ == '__main__':\n    # regression1()\n    # regression2()\n    # abaloneTest()\n    # regression3()\n    # regression4()\n    # regression5()\n    pass\n\n", "src/py3.x/ml/16.RecommenderSystems/RS-itemcf.py": "#!/usr/bin/python\n# coding:utf8\n'''\nCreated on 2015-06-22\nUpdate  on 2017-05-16\nAuthor: Lockvictor/\u7247\u523b\n\u300a\u63a8\u8350\u7cfb\u7edf\u5b9e\u8df5\u300b\u534f\u540c\u8fc7\u6ee4\u7b97\u6cd5\u6e90\u4ee3\u7801\n\u53c2\u8003\u5730\u5740: https://github.com/Lockvictor/MovieLens-RecSys\n\u66f4\u65b0\u5730\u5740: https://github.com/apachecn/AiLearning\n'''\nfrom __future__ import print_function\nimport sys\nimport math\nimport random\nfrom operator import itemgetter\n\n# \u4f5c\u7528: \u4f7f\u5f97\u968f\u673a\u6570\u636e\u53ef\u9884\u6d4b\nrandom.seed(0)\n\n\nclass ItemBasedCF():\n    ''' TopN recommendation - ItemBasedCF '''\n\n    def __init__(self):\n        self.trainset = {}\n        self.testset = {}\n\n        # n_sim_user: top 20\u4e2a\u7528\u6237\uff0c n_rec_movie: top 10\u4e2a\u63a8\u8350\u7ed3\u679c\n        self.n_sim_movie = 20\n        self.n_rec_movie = 10\n\n        # user_sim_mat: \u7535\u5f71\u4e4b\u95f4\u7684\u76f8\u4f3c\u5ea6\uff0c movie_popular: \u7535\u5f71\u7684\u51fa\u73b0\u6b21\u6570\uff0c movie_count: \u603b\u7535\u5f71\u6570\u91cf\n        self.movie_sim_mat = {}\n        self.movie_popular = {}\n        self.movie_count = 0\n\n        print('Similar movie number = %d' % self.n_sim_movie, file=sys.stderr)\n        print('Recommended movie number = %d' % self.n_rec_movie, file=sys.stderr)\n\n    @staticmethod\n    def loadfile(filename):\n        \"\"\"loadfile(\u52a0\u8f7d\u6587\u4ef6\uff0c\u8fd4\u56de\u4e00\u4e2a\u751f\u6210\u5668)\n\n        Args:\n            filename   \u6587\u4ef6\u540d\n        Returns:\n            line       \u884c\u6570\u636e\uff0c\u53bb\u7a7a\u683c\n        \"\"\"\n        fp = open(filename, 'r')\n        for i, line in enumerate(fp):\n            yield line.strip('\\r\\n')\n            if i > 0 and i % 100000 == 0:\n                print('loading %s(%s)' % (filename, i), file=sys.stderr)\n        fp.close()\n        print('load %s success' % filename, file=sys.stderr)\n\n    def generate_dataset(self, filename, pivot=0.7):\n        \"\"\"loadfile(\u52a0\u8f7d\u6587\u4ef6\uff0c\u5c06\u6570\u636e\u96c6\u6309\u71677:3 \u8fdb\u884c\u968f\u673a\u62c6\u5206)\n\n        Args:\n            filename   \u6587\u4ef6\u540d\n            pivot      \u62c6\u5206\u6bd4\u4f8b\n        \"\"\"\n        trainset_len = 0\n        testset_len = 0\n\n        for line in self.loadfile(filename):\n            # \u7528\u6237ID\uff0c\u7535\u5f71\u540d\u79f0\uff0c\u8bc4\u5206\uff0c\u65f6\u95f4\u6233\n            # user, movie, rating, _ = line.split('::')\n            user, movie, rating, _ = line.split('\\t')\n            # \u901a\u8fc7pivot\u548c\u968f\u673a\u51fd\u6570\u6bd4\u8f83\uff0c\u7136\u540e\u521d\u59cb\u5316\u7528\u6237\u548c\u5bf9\u5e94\u7684\u503c\n            if (random.random() < pivot):\n\n                # dict.setdefault(key, default=None)\n                # key -- \u67e5\u627e\u7684\u952e\u503c\n                # default -- \u952e\u4e0d\u5b58\u5728\u65f6\uff0c\u8bbe\u7f6e\u7684\u9ed8\u8ba4\u952e\u503c\n                self.trainset.setdefault(user, {})\n                self.trainset[user][movie] = int(rating)\n                trainset_len += 1\n            else:\n                self.testset.setdefault(user, {})\n                self.testset[user][movie] = int(rating)\n                testset_len += 1\n\n        print('\u5206\u79bb\u8bad\u7ec3\u96c6\u548c\u6d4b\u8bd5\u96c6\u6210\u529f', file=sys.stderr)\n        print('train set = %s' % trainset_len, file=sys.stderr)\n        print('test set = %s' % testset_len, file=sys.stderr)\n\n    def calc_movie_sim(self):\n        \"\"\"calc_movie_sim(\u8ba1\u7b97\u7528\u6237\u4e4b\u95f4\u7684\u76f8\u4f3c\u5ea6)\"\"\"\n\n        print('counting movies number and popularity...', file=sys.stderr)\n\n        # \u7edf\u8ba1\u5728\u6240\u6709\u7684\u7528\u6237\u4e2d\uff0c\u4e0d\u540c\u7535\u5f71\u7684\u603b\u51fa\u73b0\u6b21\u6570\uff0c user, movies\n        for _, movies in self.trainset.items():\n            for movie in movies:\n                # count item popularity\n                if movie not in self.movie_popular:\n                    self.movie_popular[movie] = 0\n                self.movie_popular[movie] += 1\n\n        print('count movies number and popularity success', file=sys.stderr)\n\n        # save the total number of movies\n        self.movie_count = len(self.movie_popular)\n        print('total movie number = %d' % self.movie_count, file=sys.stderr)\n\n        # \u7edf\u8ba1\u5728\u76f8\u540c\u7528\u6237\u65f6\uff0c\u4e0d\u540c\u7535\u5f71\u540c\u65f6\u51fa\u73b0\u7684\u6b21\u6570\n        itemsim_mat = self.movie_sim_mat\n        print('building co-rated users matrix...', file=sys.stderr)\n        # user, movies\n        for _, movies in self.trainset.items():\n            for m1 in movies:\n                for m2 in movies:\n                    if m1 == m2:\n                        continue\n                    itemsim_mat.setdefault(m1, {})\n                    itemsim_mat[m1].setdefault(m2, 0)\n                    itemsim_mat[m1][m2] += 1\n        print('build co-rated users matrix success', file=sys.stderr)\n\n        # calculate similarity matrix\n        print('calculating movie similarity matrix...', file=sys.stderr)\n        simfactor_count = 0\n        PRINT_STEP = 2000000\n        for m1, related_movies in itemsim_mat.items():\n            for m2, count in related_movies.iteritems():\n                # \u4f59\u5f26\u76f8\u4f3c\u5ea6\n                itemsim_mat[m1][m2] = count / math.sqrt(\n                    self.movie_popular[m1] * self.movie_popular[m2])\n                simfactor_count += 1\n                # \u6253\u5370\u8fdb\u5ea6\u6761\n                if simfactor_count % PRINT_STEP == 0:\n                    print('calculating movie similarity factor(%d)' % simfactor_count, file=sys.stderr)\n\n        print('calculate movie similarity matrix(similarity factor) success', file=sys.stderr)\n        print('Total similarity factor number = %d' % simfactor_count, file=sys.stderr)\n\n    # @profile\n    def recommend(self, user):\n        \"\"\"recommend(\u627e\u51fatop K\u7684\u7535\u5f71\uff0c\u5bf9\u7535\u5f71\u8fdb\u884c\u76f8\u4f3c\u5ea6sum\u7684\u6392\u5e8f\uff0c\u53d6\u51fatop N\u7684\u7535\u5f71\u6570)\n\n        Args:\n            user       \u7528\u6237\n        Returns:\n            rec_movie  \u7535\u5f71\u63a8\u8350\u5217\u8868\uff0c\u6309\u7167\u76f8\u4f3c\u5ea6\u4ece\u5927\u5230\u5c0f\u7684\u6392\u5e8f\n        \"\"\"\n        ''' Find K similar movies and recommend N movies. '''\n        K = self.n_sim_movie\n        N = self.n_rec_movie\n        rank = {}\n        watched_movies = self.trainset[user]\n\n        # \u8ba1\u7b97top K \u7535\u5f71\u7684\u76f8\u4f3c\u5ea6\n        # rating=\u7535\u5f71\u8bc4\u5206, w=\u4e0d\u540c\u7535\u5f71\u51fa\u73b0\u7684\u6b21\u6570\n        # \u8017\u65f6\u5206\u6790: 98.2%\u7684\u65f6\u95f4\u5728 line-154\u884c\n        for movie, rating in watched_movies.iteritems():\n            for related_movie, w in sorted(\n                    self.movie_sim_mat[movie].items(),\n                    key=itemgetter(1),\n                    reverse=True)[0:K]:\n                if related_movie in watched_movies:\n                    continue\n                rank.setdefault(related_movie, 0)\n                rank[related_movie] += w * rating\n        # return the N best movies\n        return sorted(rank.items(), key=itemgetter(1), reverse=True)[0:N]\n\n    def evaluate(self):\n        ''' return precision, recall, coverage and popularity '''\n        print('Evaluation start...', file=sys.stderr)\n\n        # \u8fd4\u56detop N\u7684\u63a8\u8350\u7ed3\u679c\n        N = self.n_rec_movie\n        # varables for precision and recall\n        # hit\u8868\u793a\u547d\u4e2d(\u6d4b\u8bd5\u96c6\u548c\u63a8\u8350\u96c6\u76f8\u540c+1)\uff0crec_count \u6bcf\u4e2a\u7528\u6237\u7684\u63a8\u8350\u6570\uff0c test_count \u6bcf\u4e2a\u7528\u6237\u5bf9\u5e94\u7684\u6d4b\u8bd5\u6570\u636e\u96c6\u7684\u7535\u5f71\u6570\n        hit = 0\n        rec_count = 0\n        test_count = 0\n        # varables for coverage\n        all_rec_movies = set()\n        # varables for popularity\n        popular_sum = 0\n\n        # enumerate\u5c06\u5176\u7ec4\u6210\u4e00\u4e2a\u7d22\u5f15\u5e8f\u5217\uff0c\u5229\u7528\u5b83\u53ef\u4ee5\u540c\u65f6\u83b7\u5f97\u7d22\u5f15\u548c\u503c\n        # \u53c2\u8003\u5730\u5740: http://blog.csdn.net/churximi/article/details/51648388\n        for i, user in enumerate(self.trainset):\n            if i > 0 and i % 500 == 0:\n                print('recommended for %d users' % i, file=sys.stderr)\n            test_movies = self.testset.get(user, {})\n            rec_movies = self.recommend(user)\n\n            # \u5bf9\u6bd4\u6d4b\u8bd5\u96c6\u548c\u63a8\u8350\u96c6\u7684\u5dee\u5f02 movie, w\n            for movie, _ in rec_movies:\n                if movie in test_movies:\n                    hit += 1\n                all_rec_movies.add(movie)\n                # \u8ba1\u7b97\u7528\u6237\u5bf9\u5e94\u7684\u7535\u5f71\u51fa\u73b0\u6b21\u6570log\u503c\u7684sum\u52a0\u548c\n                popular_sum += math.log(1 + self.movie_popular[movie])\n            rec_count += N\n            test_count += len(test_movies)\n\n        precision = hit / (1.0 * rec_count)\n        recall = hit / (1.0 * test_count)\n        coverage = len(all_rec_movies) / (1.0 * self.movie_count)\n        popularity = popular_sum / (1.0 * rec_count)\n\n        print('precision=%.4f \\t recall=%.4f \\t coverage=%.4f \\t popularity=%.4f' % (\n            precision, recall, coverage, popularity), file=sys.stderr)\n\n\nif __name__ == '__main__':\n    # ratingfile = 'data/16.RecommenderSystems/ml-1m/ratings.dat'\n    ratingfile = 'data/16.RecommenderSystems/ml-100k/u.data'\n\n    # \u521b\u5efaItemCF\u5bf9\u8c61\n    itemcf = ItemBasedCF()\n    # \u5c06\u6570\u636e\u6309\u7167 7:3\u7684\u6bd4\u4f8b\uff0c\u62c6\u5206\u6210: \u8bad\u7ec3\u96c6\u548c\u6d4b\u8bd5\u96c6\uff0c\u5b58\u50a8\u5728usercf\u7684trainset\u548ctestset\u4e2d\n    itemcf.generate_dataset(ratingfile, pivot=0.7)\n    # \u8ba1\u7b97\u7528\u6237\u4e4b\u95f4\u7684\u76f8\u4f3c\u5ea6\n    itemcf.calc_movie_sim()\n    # \u8bc4\u4f30\u63a8\u8350\u6548\u679c\n    # itemcf.evaluate()\n    # \u67e5\u770b\u63a8\u8350\u7ed3\u679c\u7528\u6237\n    user = \"2\"\n    print(\"\u63a8\u8350\u7ed3\u679c\", itemcf.recommend(user))\n    print(\"---\", itemcf.testset.get(user, {}))\n", "src/py3.x/ml/16.RecommenderSystems/sklearn-RS-demo.py": "#!/usr/bin/python\n# coding:utf8\n\nimport numpy as np\nfrom sklearn.decomposition import NMF\nimport matplotlib.pyplot as plt\n\nRATE_MATRIX = np.array([[5, 5, 3, 0, 5, 5], \n                        [5, 0, 4, 0, 4, 4],\n                        [0, 3, 0, 5, 4, 5], \n                        [5, 4, 3, 3, 5, 5]])\n\nnmf = NMF(n_components=2)  # \u8bbe\u67092\u4e2a\u9690\u4e3b\u9898\nuser_distribution = nmf.fit_transform(RATE_MATRIX)\nitem_distribution = nmf.components_\n\nprint('\u7528\u6237\u7684\u4e3b\u9898\u5206\u5e03: ')\nprint(user_distribution)\nprint('\u7269\u54c1\u7684\u4e3b\u9898\u5206\u5e03: ')\nprint(item_distribution)\n", "src/py3.x/ml/16.RecommenderSystems/RS-usercf.py": "#!/usr/bin/python\n# coding:utf8\n'''\nCreated on 2015-06-22\nUpdate  on 2017-05-16\nAuthor: Lockvictor/\u7247\u523b\n\u300a\u63a8\u8350\u7cfb\u7edf\u5b9e\u8df5\u300b\u534f\u540c\u8fc7\u6ee4\u7b97\u6cd5\u6e90\u4ee3\u7801\n\u53c2\u8003\u5730\u5740: https://github.com/Lockvictor/MovieLens-RecSys\n\u66f4\u65b0\u5730\u5740: https://github.com/apachecn/AiLearning\n'''\nfrom __future__ import print_function\nimport sys\nimport math\nimport random\nfrom operator import itemgetter\nfrom collections import defaultdict\n# \u8fd9\u662f\u6211\u7684\u81ea\u5b9a\u4e49\u5e93\uff0c\u53ef\u4ee5\u6ce8\u91ca\u6389\uff0c\u4ee3\u7801\u4e5f\u53ef\u4ee5\u6ce8\u91ca TimeStat\nfrom middleware.utils import TimeStat\nprint(__doc__)\n# \u4f5c\u7528: \u4f7f\u5f97\u968f\u673a\u6570\u636e\u53ef\u9884\u6d4b\nrandom.seed(0)\n\n\nclass UserBasedCF():\n    ''' TopN recommendation - UserBasedCF '''\n\n    def __init__(self):\n        self.trainset = {}\n        self.testset = {}\n\n        # n_sim_user: top 20\u4e2a\u7528\u6237\uff0c n_rec_movie: top 10\u4e2a\u63a8\u8350\u7ed3\u679c\n        self.n_sim_user = 20\n        self.n_rec_movie = 10\n\n        # user_sim_mat: \u7528\u6237\u4e4b\u95f4\u7684\u76f8\u4f3c\u5ea6\uff0c movie_popular: \u7535\u5f71\u7684\u51fa\u73b0\u6b21\u6570\uff0c movie_count: \u603b\u7535\u5f71\u6570\u91cf\n        self.user_sim_mat = {}\n        self.movie_popular = {}\n        self.movie_count = 0\n\n        print('similar user number = %d' % self.n_sim_user, file=sys.stderr)\n        print('recommended movie number = %d' % self.n_rec_movie, file=sys.stderr)\n\n    @staticmethod\n    def loadfile(filename):\n        \"\"\"loadfile(\u52a0\u8f7d\u6587\u4ef6\uff0c\u8fd4\u56de\u4e00\u4e2a\u751f\u6210\u5668)\n\n        Args:\n            filename   \u6587\u4ef6\u540d\n        Returns:\n            line       \u884c\u6570\u636e\uff0c\u53bb\u7a7a\u683c\n        \"\"\"\n        fp = open(filename, 'r')\n        for i, line in enumerate(fp):\n            yield line.strip('\\r\\n')\n            if i > 0 and i % 100000 == 0:\n                print('loading %s(%s)' % (filename, i), file=sys.stderr)\n        fp.close()\n        print('load %s success' % filename, file=sys.stderr)\n\n    def generate_dataset(self, filename, pivot=0.7):\n        \"\"\"loadfile(\u52a0\u8f7d\u6587\u4ef6\uff0c\u5c06\u6570\u636e\u96c6\u6309\u71677:3 \u8fdb\u884c\u968f\u673a\u62c6\u5206)\n\n        Args:\n            filename   \u6587\u4ef6\u540d\n            pivot      \u62c6\u5206\u6bd4\u4f8b\n        \"\"\"\n        trainset_len = 0\n        testset_len = 0\n\n        for line in self.loadfile(filename):\n            # \u7528\u6237ID\uff0c\u7535\u5f71\u540d\u79f0\uff0c\u8bc4\u5206\uff0c\u65f6\u95f4\u6233timestamp\n            # user, movie, rating, timestamp = line.split('::')\n            user, movie, rating, _ = line.split('\\t')\n            # \u901a\u8fc7pivot\u548c\u968f\u673a\u51fd\u6570\u6bd4\u8f83\uff0c\u7136\u540e\u521d\u59cb\u5316\u7528\u6237\u548c\u5bf9\u5e94\u7684\u503c\n            if (random.random() < pivot):\n\n                # dict.setdefault(key, default=None)\n                # key -- \u67e5\u627e\u7684\u952e\u503c\n                # default -- \u952e\u4e0d\u5b58\u5728\u65f6\uff0c\u8bbe\u7f6e\u7684\u9ed8\u8ba4\u952e\u503c\n                self.trainset.setdefault(user, {})\n                self.trainset[user][movie] = int(rating)\n                trainset_len += 1\n            else:\n                self.testset.setdefault(user, {})\n                self.testset[user][movie] = int(rating)\n                testset_len += 1\n\n        print('\u5206\u79bb\u8bad\u7ec3\u96c6\u548c\u6d4b\u8bd5\u96c6\u6210\u529f', file=sys.stderr)\n        print('train set = %s' % trainset_len, file=sys.stderr)\n        print('test  set = %s' % testset_len, file=sys.stderr)\n\n    def calc_user_sim(self):\n        \"\"\"calc_user_sim(\u8ba1\u7b97\u7528\u6237\u4e4b\u95f4\u7684\u76f8\u4f3c\u5ea6)\"\"\"\n\n        # build inverse table for item-users\n        # key=movieID, value=list of userIDs who have seen this movie\n        print('building movie-users inverse table...', file=sys.stderr)\n        movie2users = dict()\n\n        # \u540c\u4e00\u4e2a\u7535\u5f71\u4e2d\uff0c\u6536\u96c6\u7528\u6237\u7684\u96c6\u5408\n        # \u7edf\u8ba1\u5728\u6240\u6709\u7684\u7528\u6237\u4e2d\uff0c\u4e0d\u540c\u7535\u5f71\u7684\u603b\u51fa\u73b0\u6b21\u6570\n        for user, movies in self.trainset.items():\n            for movie in movies:\n                # inverse table for item-users\n                if movie not in movie2users:\n                    movie2users[movie] = set()\n                movie2users[movie].add(user)\n                # count item popularity at the same time\n                if movie not in self.movie_popular:\n                    self.movie_popular[movie] = 0\n                self.movie_popular[movie] += 1\n\n        print('build movie-users inverse table success', file=sys.stderr)\n\n        # save the total movie number, which will be used in evaluation\n        self.movie_count = len(movie2users)\n        print('total movie number = %d' % self.movie_count, file=sys.stderr)\n\n        usersim_mat = self.user_sim_mat\n        # \u7edf\u8ba1\u5728\u76f8\u540c\u7535\u5f71\u65f6\uff0c\u4e0d\u540c\u7528\u6237\u540c\u65f6\u51fa\u73b0\u7684\u6b21\u6570\n        print('building user co-rated movies matrix...', file=sys.stderr)\n\n        # for movie, users in movie2users.items():\n        #     for u in users:\n        #         for v in users:\n        #             if u == v:\n        #                 continue\n        #             usersim_mat.setdefault(u, {})\n        #             usersim_mat[u].setdefault(v, 0)\n        #             usersim_mat[u][v] += 1\n        for movie, users in movie2users.items():\n            for u in users:\n                usersim_mat.setdefault(u, defaultdict(int))\n                for v in users:\n                    if u == v:\n                        continue\n                    usersim_mat[u][v] += 1\n        print('build user co-rated movies matrix success', file=sys.stderr)\n\n        # calculate similarity matrix\n        print('calculating user similarity matrix...', file=sys.stderr)\n        simfactor_count = 0\n        PRINT_STEP = 2000000\n        for u, related_users in usersim_mat.items():\n            for v, count in related_users.items():\n                # \u4f59\u5f26\u76f8\u4f3c\u5ea6(\u6709\u95ee\u9898)\n                usersim_mat[u][v] = count / math.sqrt(\n                    len(self.trainset[u]) * len(self.trainset[v]))\n                # \u6770\u5361\u5fb7\uff08Jaccard\uff09\u76f8\u4f3c\u6027\n                # usersim_mat[u][v] = count / (len(set(self.trainset[u] + self.trainset[v]))\n\n                simfactor_count += 1\n                # \u6253\u5370\u8fdb\u5ea6\u6761\n                if simfactor_count % PRINT_STEP == 0:\n                    print('calculating user similarity factor(%d)' % simfactor_count, file=sys.stderr)\n\n        print('calculate user similarity matrix(similarity factor) success', file=sys.stderr)\n        print('Total similarity factor number = %d' % simfactor_count, file=sys.stderr)\n\n    # @profile\n    def recommend(self, user):\n        \"\"\"recommend(\u627e\u51fatop K\u7684\u7528\u6237\uff0c\u6240\u770b\u8fc7\u7684\u7535\u5f71\uff0c\u5bf9\u7535\u5f71\u8fdb\u884c\u76f8\u4f3c\u5ea6sum\u7684\u6392\u5e8f\uff0c\u53d6\u51fatop N\u7684\u7535\u5f71\u6570)\n\n        Args:\n            user       \u7528\u6237\n        Returns:\n            rec_movie  \u7535\u5f71\u63a8\u8350\u5217\u8868\uff0c\u6309\u7167\u76f8\u4f3c\u5ea6\u4ece\u5927\u5230\u5c0f\u7684\u6392\u5e8f\n        \"\"\"\n        ''' Find K similar users and recommend N movies. '''\n        K = self.n_sim_user\n        N = self.n_rec_movie\n        rank = dict()\n        watched_movies = self.trainset[user]\n\n        # \u8ba1\u7b97top K \u7528\u6237\u7684\u76f8\u4f3c\u5ea6\n        # v=similar user, wuv=\u4e0d\u540c\u7528\u6237\u540c\u65f6\u51fa\u73b0\u7684\u6b21\u6570\uff0c\u6839\u636ewuv\u5012\u5e8f\u4ece\u5927\u5230\u5c0f\u9009\u51faK\u4e2a\u7528\u6237\u8fdb\u884c\u6392\u5217\n        # \u8017\u65f6\u5206\u6790: 50.4%\u7684\u65f6\u95f4\u5728 line-160\u884c\n        for v, wuv in sorted(\n                self.user_sim_mat[user].items(), key=itemgetter(1),\n                reverse=True)[0:K]:\n            for movie, rating in self.trainset[v].items():\n                if movie in watched_movies:\n                    continue\n                # predict the user's \"interest\" for each movie\n                rank.setdefault(movie, 0)\n                rank[movie] += wuv * rating\n        # return the N best movies\n        \"\"\"\n        wuv\n        precision=0.3766         recall=0.0759   coverage=0.3183         popularity=6.9194\n\n        wuv * rating\n        precision=0.3865         recall=0.0779   coverage=0.2681         popularity=7.0116\n        \"\"\"\n        return sorted(rank.items(), key=itemgetter(1), reverse=True)[0:N]\n\n    def evaluate(self):\n        ''' return precision, recall, coverage and popularity '''\n        print('Evaluation start...', file=sys.stderr)\n\n        # \u8fd4\u56detop N\u7684\u63a8\u8350\u7ed3\u679c\n        N = self.n_rec_movie\n        # varables for precision and recall\n        # hit\u8868\u793a\u547d\u4e2d(\u6d4b\u8bd5\u96c6\u548c\u63a8\u8350\u96c6\u76f8\u540c+1)\uff0crec_count \u6bcf\u4e2a\u7528\u6237\u7684\u63a8\u8350\u6570\uff0c test_count \u6bcf\u4e2a\u7528\u6237\u5bf9\u5e94\u7684\u6d4b\u8bd5\u6570\u636e\u96c6\u7684\u7535\u5f71\u6570\n        hit = 0\n        rec_count = 0\n        test_count = 0\n        # varables for coverage\n        all_rec_movies = set()\n        # varables for popularity\n        popular_sum = 0\n\n        # enumerate\u5c06\u5176\u7ec4\u6210\u4e00\u4e2a\u7d22\u5f15\u5e8f\u5217\uff0c\u5229\u7528\u5b83\u53ef\u4ee5\u540c\u65f6\u83b7\u5f97\u7d22\u5f15\u548c\u503c\n        # \u53c2\u8003\u5730\u5740: http://blog.csdn.net/churximi/article/details/51648388\n        for i, user in enumerate(self.trainset):\n            if i > 0 and i % 500 == 0:\n                print('recommended for %d users' % i, file=sys.stderr)\n            test_movies = self.testset.get(user, {})\n            rec_movies = self.recommend(user)\n\n            # \u5bf9\u6bd4\u6d4b\u8bd5\u96c6\u548c\u63a8\u8350\u96c6\u7684\u5dee\u5f02 movie, w\n            for movie, _ in rec_movies:\n                if movie in test_movies:\n                    hit += 1\n                all_rec_movies.add(movie)\n                # \u8ba1\u7b97\u7528\u6237\u5bf9\u5e94\u7684\u7535\u5f71\u51fa\u73b0\u6b21\u6570log\u503c\u7684sum\u52a0\u548c\n                popular_sum += math.log(1 + self.movie_popular[movie])\n            rec_count += N\n            test_count += len(test_movies)\n\n        precision = hit / (1.0 * rec_count)\n        recall = hit / (1.0 * test_count)\n        coverage = len(all_rec_movies) / (1.0 * self.movie_count)\n        popularity = popular_sum / (1.0 * rec_count)\n\n        print('precision=%.4f \\t recall=%.4f \\t coverage=%.4f \\t popularity=%.4f' % (\n            precision, recall, coverage, popularity), file=sys.stderr)\n\n\n@TimeStat\ndef main():\n    path_root = \"/Users/jiangzl/work/data/\u673a\u5668\u5b66\u4e60\"\n    # ratingfile = 'data/16.RecommenderSystems/ml-1m/ratings.dat'\n    ratingfile = '%s/16.RecommenderSystems/ml-100k/u.data' % path_root\n\n    # \u521b\u5efaUserCF\u5bf9\u8c61\n    usercf = UserBasedCF()\n    # \u5c06\u6570\u636e\u6309\u7167 7:3\u7684\u6bd4\u4f8b\uff0c\u62c6\u5206\u6210: \u8bad\u7ec3\u96c6\u548c\u6d4b\u8bd5\u96c6\uff0c\u5b58\u50a8\u5728usercf\u7684trainset\u548ctestset\u4e2d\n    usercf.generate_dataset(ratingfile, pivot=0.7)\n    print(usercf.testset)\n    # # \u8ba1\u7b97\u7528\u6237\u4e4b\u95f4\u7684\u76f8\u4f3c\u5ea6\n    # usercf.calc_user_sim()\n    # # \u8bc4\u4f30\u63a8\u8350\u6548\u679c\n    # usercf.evaluate()\n\n\nif __name__ == '__main__':\n    main()\n", "src/py3.x/ml/16.RecommenderSystems/sklearn-RS-demo-user.py": "#!/usr/bin/python\n# coding:utf8\n\nimport numpy as np\nfrom sklearn.decomposition import NMF\nimport matplotlib.pyplot as plt\n\nRATE_MATRIX = np.array([[5, 5, 3, 0, 5, 5], [5, 0, 4, 0, 4, 4],\n                        [0, 3, 0, 5, 4, 5], [5, 4, 3, 3, 5, 5]])\n\nnmf = NMF(n_components=2)\nuser_distribution = nmf.fit_transform(RATE_MATRIX)\nitem_distribution = nmf.components_\n\nusers = ['Ben', 'Tom', 'John', 'Fred']\nzip_data = zip(users, user_distribution)\n\nplt.title(u'the distribution of users (NMF)')\nplt.xlim((-1, 3))\nplt.ylim((-1, 4))\nfor item in zip_data:\n    user_name = item[0]\n    data = item[1]\n    plt.plot(data[0], data[1], \"b*\")\n    plt.text(\n        data[0],\n        data[1],\n        user_name,\n        bbox=dict(facecolor='red', alpha=0.2),\n    )\n\nplt.show()\n", "src/py3.x/ml/16.RecommenderSystems/sklearn-RS-demo-item.py": "#!/usr/bin/python\n# coding:utf8\n\nimport numpy as np\nfrom sklearn.decomposition import NMF\nimport matplotlib.pyplot as plt\n\nRATE_MATRIX = np.array([[5, 5, 3, 0, 5, 5], [5, 0, 4, 0, 4, 4],\n                        [0, 3, 0, 5, 4, 5], [5, 4, 3, 3, 5, 5]])\n\nnmf = NMF(n_components=2)\nuser_distribution = nmf.fit_transform(RATE_MATRIX)\nitem_distribution = nmf.components_\n\nitem_distribution = item_distribution.T\nplt.plot(item_distribution[:, 0], item_distribution[:, 1], \"b*\")\nplt.xlim((-1, 3))\nplt.ylim((-1, 3))\n\nplt.title(u'the distribution of items (NMF)')\ncount = 1\nfor item in item_distribution:\n    plt.text(\n        item[0],\n        item[1],\n        'item ' + str(count),\n        bbox=dict(facecolor='red', alpha=0.2),\n    )\n    count += 1\n\nplt.show()\n", "src/py3.x/ml/16.RecommenderSystems/python/Recommender.py": "import numpy as np\n\n\n# \u81ea\u5b9a\u4e49\u6770\u5361\u5fb7\u76f8\u4f3c\u7cfb\u6570\u51fd\u6570\uff0c\u4ec5\u5bf90-1\u77e9\u9635\u6709\u6548\ndef Jaccard(a, b):\n    return 1.0*(a*b).sum()/(a+b-a*b).sum()\n\n\nclass Recommender():\n\n    # \u76f8\u4f3c\u5ea6\u77e9\u9635\n    sim = None\n\n    # \u8ba1\u7b97\u76f8\u4f3c\u5ea6\u77e9\u9635\u7684\u51fd\u6570\n    def similarity(self, x, distance):\n        y = np.ones((len(x), len(x)))\n        for i in range(len(x)):\n            for j in range(len(x)):\n                y[i, j] = distance(x[i], x[j])\n        return y\n\n    # \u8bad\u7ec3\u51fd\u6570\n    def fit(self, x, distance=Jaccard):\n        self.sim = self.similarity(x, distance)\n\n    # \u63a8\u8350\u51fd\u6570\n    def recommend(self, a):\n        return np.dot(self.sim, a)*(1-a)\n", "src/py3.x/ml/7.AdaBoost/adaboost.py": "#!/usr/bin/python\n# coding:utf8\n\"\"\"\nCreated on Nov 28, 2010\nUpdate  on 2017-05-18\nAdaboost is short for Adaptive Boosting\nAuthor: Peter/\u7247\u523b/BBruceyuan\nGitHub: https://github.com/apachecn/AiLearning\n\"\"\"\nimport numpy as np\n\n\ndef load_sim_data():\n    \"\"\"\n    \u6d4b\u8bd5\u6570\u636e\uff0c\n    :return: data_arr   feature\u5bf9\u5e94\u7684\u6570\u636e\u96c6\n            label_arr  feature\u5bf9\u5e94\u7684\u5206\u7c7b\u6807\u7b7e\n    \"\"\"\n    data_mat = np.matrix([[1.0, 2.1],\n                          [2.0, 1.1],\n                          [1.3, 1.0],\n                          [1.0, 1.0],\n                          [2.0, 1.0]])\n    class_labels = [1.0, 1.0, -1.0, -1.0, 1.0]\n    return data_mat, class_labels\n\n\ndef load_data_set(file_name):\n    \"\"\"\n    \u52a0\u8f7d\u9a6c\u7684\u759d\u6c14\u75c5\u7684\u6570\u636e\n    :param file_name: \u6587\u4ef6\u540d\n    :return: \u5fc5\u987b\u8981\u662fnp.array\u6216\u8005np.matrix\u4e0d\u7136\u540e\u9762\u6ca1\u6709\uff0cshape\n    \"\"\"\n    num_feat = len(open(file_name).readline().split('\\t'))\n    data_arr = []\n    label_arr = []\n    fr = open(file_name)\n    for line in fr.readlines():\n        line_arr = []\n        cur_line = line.strip().split('\\t')\n        for i in range(num_feat - 1):\n            line_arr.append(float(cur_line[i]))\n        data_arr.append(line_arr)\n        label_arr.append(float(cur_line[-1]))\n    return np.matrix(data_arr), label_arr\n\n\ndef stump_classify(data_mat, dimen, thresh_val, thresh_ineq):\n    \"\"\"\n    (\u5c06\u6570\u636e\u96c6\uff0c\u6309\u7167feature\u5217\u7684value\u8fdb\u884c \u4e8c\u5206\u6cd5\u5207\u5206\u6bd4\u8f83\u6765\u8d4b\u503c\u5206\u7c7b)\n    :param data_mat: Matrix\u6570\u636e\u96c6\n    :param dimen: \u7279\u5f81\u7684\u54ea\u4e00\u4e2a\u5217\n    :param thresh_val: \u7279\u5f81\u5217\u8981\u6bd4\u8f83\u7684\u503c\n    :param thresh_ineq: \n    :return: np.array\n    \"\"\"\n    ret_array = np.ones((np.shape(data_mat)[0], 1))\n    # data_mat[:, dimen] \u8868\u793a\u6570\u636e\u96c6\u4e2d\u7b2cdimen\u5217\u7684\u6240\u6709\u503c\n    # thresh_ineq == 'lt'\u8868\u793a\u4fee\u6539\u5de6\u8fb9\u7684\u503c\uff0cgt\u8868\u793a\u4fee\u6539\u53f3\u8fb9\u7684\u503c\n    # \uff08\u8fd9\u91cc\u5176\u5b9e\u6211\u5efa\u8bae\u7406\u89e3\u4e3a\u8f6c\u6362\u5de6\u53f3\u8fb9\uff0c\u5c31\u662f\u4e00\u68f5\u6811\u7684\u5de6\u53f3\u5b69\u5b50\uff0c\u53ef\u80fd\u6709\u70b9\u95ee\u9898\u3002\u3002\u3002\u5f85\u8003\u8bc1\uff09\n    if thresh_ineq == 'lt':\n        ret_array[data_mat[:, dimen] <= thresh_val] = -1.0\n    else:\n        ret_array[data_mat[:, dimen] > thresh_val] = -1.0\n    return ret_array\n\n\ndef build_stump(data_arr, class_labels, D):\n    \"\"\"\n    \u5f97\u5230\u51b3\u7b56\u6811\u7684\u6a21\u578b (\u8fd9\u4e2a\u6bd4\u8f83\u91cd\u8981\uff0c\u9700\u8981\u770b\u61c2\uff09\n    :param data_arr: \u7279\u5f81\u6807\u7b7e\u96c6\u5408\n    :param class_labels: \u5206\u7c7b\u6807\u7b7e\u96c6\u5408\n    :param D: \u6700\u521d\u7684\u7279\u5f81\u6743\u91cd\u503c\n    :return: bestStump    \u6700\u4f18\u7684\u5206\u7c7b\u5668\u6a21\u578b\n            min_error     \u9519\u8bef\u7387\n            best_class_est  \u8bad\u7ec3\u540e\u7684\u7ed3\u679c\u96c6\n    \"\"\"\n    data_mat = np.mat(data_arr)\n    label_mat = np.mat(class_labels).T\n\n    m, n = np.shape(data_mat)\n    num_steps = 10.0\n    best_stump = {}\n    best_class_est = np.mat(np.zeros((m, 1)))\n    # \u65e0\u7a77\u5927\n    min_err = np.inf\n    for i in range(n):\n        range_min = data_mat[:, i].min()\n        range_max = data_mat[:, i].max()\n        step_size = (range_max - range_min) / num_steps\n        for j in range(-1, int(num_steps) + 1):\n            for inequal in ['lt', 'gt']:\n                thresh_val = (range_min + float(j) * step_size)\n                predicted_vals = stump_classify(data_mat, i, thresh_val, inequal)\n                err_arr = np.mat(np.ones((m, 1)))\n                err_arr[predicted_vals == label_mat] = 0\n                # \u8fd9\u91cc\u662f\u77e9\u9635\u4e58\u6cd5\n                weighted_err = D.T * err_arr\n                '''\n                dim            \u8868\u793a feature\u5217\n                thresh_val      \u8868\u793a\u6811\u7684\u5206\u754c\u503c\n                inequal        \u8868\u793a\u8ba1\u7b97\u6811\u5de6\u53f3\u98a0\u5012\u7684\u9519\u8bef\u7387\u7684\u60c5\u51b5\n                weighted_error  \u8868\u793a\u6574\u4f53\u7ed3\u679c\u7684\u9519\u8bef\u7387\n                best_class_est    \u9884\u6d4b\u7684\u6700\u4f18\u7ed3\u679c \uff08\u4e0eclass_labels\u5bf9\u5e94\uff09\n                '''\n                # print('split: dim {}, thresh {}, thresh inequal: {}, the weighted err is {}'.format(\n                #     i, thresh_val, inequal, weighted_err\n                # ))\n                if weighted_err < min_err:\n                    min_err = weighted_err\n                    best_class_est = predicted_vals.copy()\n                    best_stump['dim'] = i\n                    best_stump['thresh'] = thresh_val\n                    best_stump['ineq'] = inequal\n    # best_stump \u8868\u793a\u5206\u7c7b\u5668\u7684\u7ed3\u679c\uff0c\u5728\u7b2c\u51e0\u4e2a\u5217\u4e0a\uff0c\u7528\u5927\u4e8e\uff0f\u5c0f\u4e8e\u6bd4\u8f83\uff0c\u9608\u503c\u662f\u591a\u5c11 (\u5355\u4e2a\u5f31\u5206\u7c7b\u5668)\n    return best_stump, min_err, best_class_est\n\n\ndef ada_boost_train_ds(data_arr, class_labels, num_it=40):\n    \"\"\"\n    adaBoost\u8bad\u7ec3\u8fc7\u7a0b\u653e\u5927\n    :param data_arr: \u7279\u5f81\u6807\u7b7e\u96c6\u5408\n    :param class_labels: \u5206\u7c7b\u6807\u7b7e\u96c6\u5408\n    :param num_it: \u8fed\u4ee3\u6b21\u6570\n    :return: weak_class_arr  \u5f31\u5206\u7c7b\u5668\u7684\u96c6\u5408\n            agg_class_est   \u9884\u6d4b\u7684\u5206\u7c7b\u7ed3\u679c\u503c\n    \"\"\"\n    weak_class_arr = []\n    m = np.shape(data_arr)[0]\n    # \u521d\u59cb\u5316 D\uff0c\u8bbe\u7f6e\u6bcf\u4e2a\u7279\u5f81\u7684\u6743\u91cd\u503c\uff0c\u5e73\u5747\u5206\u4e3am\u4efd\n    D = np.mat(np.ones((m, 1)) / m)\n    agg_class_est = np.mat(np.zeros((m, 1)))\n    for i in range(num_it):\n        # \u5f97\u5230\u51b3\u7b56\u6811\u7684\u6a21\u578b\n        best_stump, error, class_est = build_stump(data_arr, class_labels, D)\n        # print('D: {}'.format(D.T))\n        # alpha \u76ee\u7684\u4e3b\u8981\u662f\u8ba1\u7b97\u6bcf\u4e00\u4e2a\u5206\u7c7b\u5668\u5b9e\u4f8b\u7684\u6743\u91cd(\u52a0\u548c\u5c31\u662f\u5206\u7c7b\u7ed3\u679c)\n        # \u8ba1\u7b97\u6bcf\u4e2a\u5206\u7c7b\u5668\u7684 alpha \u6743\u91cd\u503c\n        alpha = float(0.5 * np.log((1.0 - error) / max(error, 1e-16)))\n        best_stump['alpha'] = alpha\n        # store Stump Params in Array\n        weak_class_arr.append(best_stump)\n        # print('class_est: {}'.format(class_est.T))\n        # \u5206\u7c7b\u6b63\u786e: \u4e58\u79ef\u4e3a1\uff0c\u4e0d\u4f1a\u5f71\u54cd\u7ed3\u679c\uff0c-1\u4e3b\u8981\u662f\u4e0b\u9762\u6c42e\u7684-alpha\u6b21\u65b9\n        # \u5206\u7c7b\u9519\u8bef: \u4e58\u79ef\u4e3a -1\uff0c\u7ed3\u679c\u4f1a\u53d7\u5f71\u54cd\uff0c\u6240\u4ee5\u4e5f\u4e58\u4ee5 -1\n        expon = np.multiply(-1 * alpha * np.mat(class_labels).T, class_est)\n        # \u5224\u65ad\u6b63\u786e\u7684\uff0c\u5c31\u4e58\u4ee5-1\uff0c\u5426\u5219\u5c31\u4e58\u4ee51\uff0c \u4e3a\u4ec0\u4e48\uff1f \u4e66\u4e0a\u7684\u516c\u5f0f\u3002\n        # print('(-1\u53d6\u53cd)\u9884\u6d4b\u503c expon=', expon.T)\n        # \u8ba1\u7b97e\u7684expon\u6b21\u65b9\uff0c\u7136\u540e\u8ba1\u7b97\u5f97\u5230\u4e00\u4e2a\u7efc\u5408\u7684\u6982\u7387\u7684\u503c\n        # \u7ed3\u679c\u53d1\u73b0:  \u5224\u65ad\u9519\u8bef\u7684\u6837\u672c\uff0cD\u5bf9\u4e8e\u7684\u6837\u672c\u6743\u91cd\u503c\u4f1a\u53d8\u5927\u3002\n        # multiply\u662f\u5bf9\u5e94\u9879\u76f8\u4e58\n        D = np.multiply(D, np.exp(expon))\n        D = D / D.sum()\n        # \u9884\u6d4b\u7684\u5206\u7c7b\u7ed3\u679c\u503c\uff0c\u5728\u4e0a\u4e00\u8f6e\u7ed3\u679c\u7684\u57fa\u7840\u4e0a\uff0c\u8fdb\u884c\u52a0\u548c\u64cd\u4f5c\n        # print('\u53e0\u52a0\u524d\u7684\u5206\u7c7b\u7ed3\u679cclass_est: {}'.format(class_est.T))\n        agg_class_est += alpha * class_est\n        # print('\u53e0\u52a0\u540e\u7684\u5206\u7c7b\u7ed3\u679cagg_class_est: {}'.format(agg_class_est.T))\n        # sign \u5224\u65ad\u6b63\u4e3a1\uff0c 0\u4e3a0\uff0c \u8d1f\u4e3a-1\uff0c\u901a\u8fc7\u6700\u7ec8\u52a0\u548c\u7684\u6743\u91cd\u503c\uff0c\u5224\u65ad\u7b26\u53f7\u3002\n        # \u7ed3\u679c\u4e3a: \u9519\u8bef\u7684\u6837\u672c\u6807\u7b7e\u96c6\u5408\uff0c\u56e0\u4e3a\u662f !=,\u90a3\u4e48\u7ed3\u679c\u5c31\u662f0 \u6b63, 1 \u8d1f\n        agg_errors = np.multiply(np.sign(agg_class_est) != np.mat(class_labels).T,\n                                 np.ones((m, 1)))\n        error_rate = agg_errors.sum() / m\n        # print('total error: {}\\n'.format(error_rate))\n        if error_rate == 0.0:\n            break\n    return weak_class_arr, agg_class_est\n\n\ndef ada_classify(data_to_class, classifier_arr):\n    \"\"\"\n    \u901a\u8fc7\u521a\u521a\u4e0a\u9762\u90a3\u4e2a\u51fd\u6570\u5f97\u5230\u7684\u5f31\u5206\u7c7b\u5668\u7684\u96c6\u5408\u8fdb\u884c\u9884\u6d4b\n    :param data_to_class: \u6570\u636e\u96c6\n    :param classifier_arr: \u5206\u7c7b\u5668\u5217\u8868\n    :return: \u6b63\u8d1f\u4e00\uff0c\u4e5f\u5c31\u662f\u8868\u793a\u5206\u7c7b\u7684\u7ed3\u679c\n    \"\"\"\n    data_mat = np.mat(data_to_class)\n    m = np.shape(data_mat)[0]\n    agg_class_est = np.mat(np.zeros((m, 1)))\n    for i in range(len(classifier_arr)):\n        class_est = stump_classify(\n            data_mat, classifier_arr[i]['dim'],\n            classifier_arr[i]['thresh'],\n            classifier_arr[i]['ineq']\n        )\n        agg_class_est += classifier_arr[i]['alpha'] * class_est\n        print(agg_class_est)\n    return np.sign(agg_class_est)\n\n\ndef plot_roc(pred_strengths, class_labels):\n    \"\"\"\n    (\u6253\u5370ROC\u66f2\u7ebf\uff0c\u5e76\u8ba1\u7b97AUC\u7684\u9762\u79ef\u5927\u5c0f)\n    :param pred_strengths: \u6700\u7ec8\u9884\u6d4b\u7ed3\u679c\u7684\u6743\u91cd\u503c\n    :param class_labels: \u539f\u59cb\u6570\u636e\u7684\u5206\u7c7b\u7ed3\u679c\u96c6\n    :return: \n    \"\"\"\n    import matplotlib.pyplot as plt\n    # variable to calculate AUC\n    y_sum = 0.0\n    # \u5bf9\u6b63\u6837\u672c\u7684\u8fdb\u884c\u6c42\u548c\n    num_pos_class = np.sum(np.array(class_labels) == 1.0)\n    # \u6b63\u6837\u672c\u7684\u6982\u7387\n    y_step = 1 / float(num_pos_class)\n    # \u8d1f\u6837\u672c\u7684\u6982\u7387\n    x_step = 1 / float(len(class_labels) - num_pos_class)\n    # np.argsort\u51fd\u6570\u8fd4\u56de\u7684\u662f\u6570\u7ec4\u503c\u4ece\u5c0f\u5230\u5927\u7684\u7d22\u5f15\u503c\n    # get sorted index, it's reverse\n    sorted_indicies = pred_strengths.argsort()\n    # \u6d4b\u8bd5\u7ed3\u679c\u662f\u5426\u662f\u4ece\u5c0f\u5230\u5927\u6392\u5217\n    # \u53ef\u4ee5\u9009\u62e9\u6253\u5370\u770b\u4e00\u4e0b\n    # \u5f00\u59cb\u521b\u5efa\u6a21\u7248\u5bf9\u8c61\n    fig = plt.figure()\n    fig.clf()\n    ax = plt.subplot(111)\n    # cursor\u5149\u6807\u503c\n    cur = (1.0, 1.0)\n    # loop through all the values, drawing a line segment at each point\n    for index in sorted_indicies.tolist()[0]:\n        if class_labels[index] == 1.0:\n            del_x = 0\n            del_y = y_step\n        else:\n            del_x = x_step\n            del_y = 0\n            y_sum += cur[1]\n        # draw line from cur to (cur[0]-delX, cur[1]-delY)\n        # \u753b\u70b9\u8fde\u7ebf (x1, x2, y1, y2)\n        # print cur[0], cur[0]-delX, cur[1], cur[1]-delY\n        ax.plot([cur[0], cur[0] - del_x], [cur[1], cur[1] - del_y], c='b')\n        cur = (cur[0] - del_x, cur[1] - del_y)\n    # \u753b\u5bf9\u89d2\u7684\u865a\u7ebf\u7ebf\n    ax.plot([0, 1], [0, 1], 'b--')\n    plt.xlabel('False positive rate')\n    plt.ylabel('True positive rate')\n    plt.title('ROC curve for AdaBoost horse colic detection system')\n    # \u8bbe\u7f6e\u753b\u56fe\u7684\u8303\u56f4\u533a\u95f4 (x1, x2, y1, y2)\n    ax.axis([0, 1, 0, 1])\n    plt.show()\n    '''\n    \u53c2\u8003\u8bf4\u660e: http://blog.csdn.net/wenyusuran/article/details/39056013\n    \u4e3a\u4e86\u8ba1\u7b97 AUC \uff0c\u6211\u4eec\u9700\u8981\u5bf9\u591a\u4e2a\u5c0f\u77e9\u5f62\u7684\u9762\u79ef\u8fdb\u884c\u7d2f\u52a0\u3002\n    \u8fd9\u4e9b\u5c0f\u77e9\u5f62\u7684\u5bbd\u5ea6\u662fx_step\uff0c\u56e0\u6b64\u53ef\u4ee5\u5148\u5bf9\u6240\u6709\u77e9\u5f62\u7684\u9ad8\u5ea6\u8fdb\u884c\u7d2f\u52a0\uff0c\u6700\u540e\u518d\u4e58\u4ee5x_step\u5f97\u5230\u5176\u603b\u9762\u79ef\u3002\n    \u6240\u6709\u9ad8\u5ea6\u7684\u548c(y_sum)\u968f\u7740x\u8f74\u7684\u6bcf\u6b21\u79fb\u52a8\u800c\u6e10\u6b21\u589e\u52a0\u3002\n    '''\n    print(\"the Area Under the Curve is: \", y_sum * x_step)\n\n\ndef test():\n    # D = np.mat(np.ones((5, 1)) / 5)\n    # data_mat, class_labels = load_sim_data()\n    # print(data_mat.shape)\n    # result = build_stump(data_mat, class_labels, D)\n    # print(result)\n    # classifier_array, agg_class_est = ada_boost_train_ds(data_mat, class_labels, 9)\n    # print(classifier_array, agg_class_est)\n    data_mat, class_labels = load_data_set('data/7.AdaBoost/horseColicTraining2.txt')\n    print(data_mat.shape, len(class_labels))\n    weak_class_arr, agg_class_est = ada_boost_train_ds(data_mat, class_labels, 40)\n    print(weak_class_arr, '\\n-----\\n', agg_class_est.T)\n    plot_roc(agg_class_est, class_labels)\n    data_arr_test, label_arr_test = load_data_set(\"data/7.AdaBoost/horseColicTest2.txt\")\n    m = np.shape(data_arr_test)[0]\n    predicting10 = ada_classify(data_arr_test, weak_class_arr)\n    err_arr = np.mat(np.ones((m, 1)))\n    # \u6d4b\u8bd5: \u8ba1\u7b97\u603b\u6837\u672c\u6570\uff0c\u9519\u8bef\u6837\u672c\u6570\uff0c\u9519\u8bef\u7387\n    print(m,\n          err_arr[predicting10 != np.mat(label_arr_test).T].sum(),\n          err_arr[predicting10 != np.mat(label_arr_test).T].sum() / m\n          )\n\n\nif __name__ == '__main__':\n    test()\n", "src/py3.x/ml/7.AdaBoost/sklearn-adaboost-demo.py": "#!/usr/bin/python\n# coding:utf8\n\"\"\"\nCreated on 2017-07-10\nUpdated on 2017-07-10\nAuthor: \u7247\u523b\uff0fNoel Dawe\nGitHub: https://github.com/apachecn/AiLearning\nsklearn-AdaBoost\u8bd1\u6587\u94fe\u63a5: http://cwiki.apachecn.org/pages/viewpage.action?pageId=10813457\n\"\"\"\n\nimport matplotlib.pyplot as plt\n# importing necessary libraries\nimport numpy as np\nfrom sklearn import metrics\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.tree import DecisionTreeRegressor\n\nprint(__doc__)\n\n\n# Create the dataset\nrng = np.random.RandomState(1)\nX = np.linspace(0, 6, 100)[:, np.newaxis]\ny = np.sin(X).ravel() + np.sin(6 * X).ravel() + rng.normal(0, 0.1, X.shape[0])\n# dataArr, labelArr = loadDataSet(\"data/7.AdaBoost/horseColicTraining2.txt\")\n\n\n# Fit regression model\nregr_1 = DecisionTreeRegressor(max_depth=4)\nregr_2 = AdaBoostRegressor(DecisionTreeRegressor(max_depth=4), n_estimators=300, random_state=rng)\n\nregr_1.fit(X, y)\nregr_2.fit(X, y)\n\n# Predict\ny_1 = regr_1.predict(X)\ny_2 = regr_2.predict(X)\n\n# Plot the results\nplt.figure()\nplt.scatter(X, y, c=\"k\", label=\"training samples\")\nplt.plot(X, y_1, c=\"g\", label=\"n_estimators=1\", linewidth=2)\nplt.plot(X, y_2, c=\"r\", label=\"n_estimators=300\", linewidth=2)\nplt.xlabel(\"data\")\nplt.ylabel(\"target\")\nplt.title(\"Boosted Decision Tree Regression\")\nplt.legend()\nplt.show()\n\nprint('y---', type(y[0]), len(y), y[:4])\nprint('y_1---', type(y_1[0]), len(y_1), y_1[:4])\nprint('y_2---', type(y_2[0]), len(y_2), y_2[:4])\n\n# \u9002\u54082\u5206\u7c7b\ny_true = np.array([0, 0, 1, 1])\ny_scores = np.array([0.1, 0.4, 0.35, 0.8])\nprint('y_scores---', type(y_scores[0]), len(y_scores), y_scores)\nprint(metrics.roc_auc_score(y_true, y_scores))\n\n# print(\"-\" * 100)\n# print(metrics.roc_auc_score(y[:1], y_2[:1]))\n", "src/py3.x/ml/10.kmeans/kMeans.py": "#!/usr/bin/env python\n__coding__ = \"utf-8\"\n__author__ = \"Ng WaiMing\"\n\nfrom numpy import *\nfrom time import sleep\nimport matplotlib\nfrom matplotlib import pyplot as plt\n\n\ndef loadDataSet(fileName):\n    '''\n    \u52a0\u8f7d\u6570\u636e\u96c6\n    :param fileName:\n    :return:\n    '''\n    # \u521d\u59cb\u5316\u4e00\u4e2a\u7a7a\u5217\u8868\n    dataSet = []\n    # \u8bfb\u53d6\u6587\u4ef6\n    fr = open(fileName)\n    # \u5faa\u73af\u904d\u5386\u6587\u4ef6\u6240\u6709\u884c\n    for line in fr.readlines():\n        # \u5207\u5272\u6bcf\u4e00\u884c\u7684\u6570\u636e\n        curLine = line.strip().split('\\t')\n        # \u5c06\u6570\u636e\u8f6c\u6362\u4e3a\u6d6e\u70b9\u7c7b\u578b,\u4fbf\u4e8e\u540e\u9762\u7684\u8ba1\u7b97\n        # fltLine = [float(x) for x in curLine]\n        # \u5c06\u6570\u636e\u8ffd\u52a0\u5230dataMat\n        fltLine = list(map(float,curLine))    # \u6620\u5c04\u6240\u6709\u7684\u5143\u7d20\u4e3a float\uff08\u6d6e\u70b9\u6570\uff09\u7c7b\u578b\n        dataSet.append(fltLine)\n    # \u8fd4\u56dedataMat\n    return dataSet\n\n\ndef distEclud(vecA, vecB):\n    '''\n    \u6b27\u6c0f\u8ddd\u79bb\u8ba1\u7b97\u51fd\u6570\n    :param vecA:\n    :param vecB:\n    :return:\n    '''\n    return sqrt(sum(power(vecA - vecB, 2)))\n\n\ndef randCent(dataMat, k):\n    '''\n    \u4e3a\u7ed9\u5b9a\u6570\u636e\u96c6\u6784\u5efa\u4e00\u4e2a\u5305\u542bK\u4e2a\u968f\u673a\u8d28\u5fc3\u7684\u96c6\u5408,\n    \u968f\u673a\u8d28\u5fc3\u5fc5\u987b\u8981\u5728\u6574\u4e2a\u6570\u636e\u96c6\u7684\u8fb9\u754c\u4e4b\u5185,\u8fd9\u53ef\u4ee5\u901a\u8fc7\u627e\u5230\u6570\u636e\u96c6\u6bcf\u4e00\u7ef4\u7684\u6700\u5c0f\u548c\u6700\u5927\u503c\u6765\u5b8c\u6210\n    \u7136\u540e\u751f\u62100\u52301.0\u4e4b\u95f4\u7684\u968f\u673a\u6570\u5e76\u901a\u8fc7\u53d6\u503c\u8303\u56f4\u548c\u6700\u5c0f\u503c,\u4ee5\u4fbf\u786e\u4fdd\u968f\u673a\u70b9\u5728\u6570\u636e\u7684\u8fb9\u754c\u4e4b\u5185\n    :param dataMat:\n    :param k:\n    :return:\n    '''\n    # \u83b7\u53d6\u6837\u672c\u6570\u4e0e\u7279\u5f81\u503c\n    m, n = shape(dataMat)\n    # \u521d\u59cb\u5316\u8d28\u5fc3,\u521b\u5efa(k,n)\u4e2a\u4ee5\u96f6\u586b\u5145\u7684\u77e9\u9635\n    centroids = mat(zeros((k, n)))\n    # \u5faa\u73af\u904d\u5386\u7279\u5f81\u503c\n    for j in range(n):\n        # \u8ba1\u7b97\u6bcf\u4e00\u5217\u7684\u6700\u5c0f\u503c\n        minJ = min(dataMat[:, j])\n        # \u8ba1\u7b97\u6bcf\u4e00\u5217\u7684\u8303\u56f4\u503c\n        rangeJ = float(max(dataMat[:, j]) - minJ)\n        # \u8ba1\u7b97\u6bcf\u4e00\u5217\u7684\u8d28\u5fc3,\u5e76\u5c06\u503c\u8d4b\u7ed9centroids\n        centroids[:, j] = mat(minJ + rangeJ * random.rand(k, 1))\n    # \u8fd4\u56de\u8d28\u5fc3\n    return centroids\n\n\ndef kMeans(dataMat, k, distMeas=distEclud, createCent=randCent):\n    '''\n    \u521b\u5efaK\u4e2a\u8d28\u5fc3,\u7136\u540e\u5c06\u6bcf\u4e2a\u5e97\u5206\u914d\u5230\u6700\u8fd1\u7684\u8d28\u5fc3,\u518d\u91cd\u65b0\u8ba1\u7b97\u8d28\u5fc3\u3002\n    \u8fd9\u4e2a\u8fc7\u7a0b\u91cd\u590d\u6570\u6b21,\u76f4\u5230\u6570\u636e\u70b9\u7684\u7c07\u5206\u914d\u7ed3\u679c\u4e0d\u518d\u6539\u53d8\u4e3a\u6b62\n    :param dataMat: \u6570\u636e\u96c6\n    :param k: \u7c07\u7684\u6570\u76ee\n    :param distMeans: \u8ba1\u7b97\u8ddd\u79bb\n    :param createCent: \u521b\u5efa\u521d\u59cb\u8d28\u5fc3\n    :return:\n    '''\n    # \u83b7\u53d6\u6837\u672c\u6570\u548c\u7279\u5f81\u6570\n    m, n = shape(dataMat)\n    # \u521d\u59cb\u5316\u4e00\u4e2a\u77e9\u9635\u6765\u5b58\u50a8\u6bcf\u4e2a\u70b9\u7684\u7c07\u5206\u914d\u7ed3\u679c\n    # clusterAssment\u5305\u542b\u4e24\u4e2a\u5217:\u4e00\u5217\u8bb0\u5f55\u7c07\u7d22\u5f15\u503c,\u7b2c\u4e8c\u5217\u5b58\u50a8\u8bef\u5dee(\u8bef\u5dee\u662f\u6307\u5f53\u524d\u70b9\u5230\u7c07\u8d28\u5fc3\u7684\u8ddd\u79bb,\u540e\u9762\u4f1a\u4f7f\u7528\u8be5\u8bef\u5dee\u6765\u8bc4\u4ef7\u805a\u7c7b\u7684\u6548\u679c)\n    clusterAssment = mat(zeros((m, 2)))\n    # \u521b\u5efa\u8d28\u5fc3,\u968f\u673aK\u4e2a\u8d28\u5fc3\n    centroids = createCent(dataMat, k)\n    # \u521d\u59cb\u5316\u6807\u5fd7\u53d8\u91cf,\u7528\u4e8e\u5224\u65ad\u8fed\u4ee3\u662f\u5426\u7ee7\u7eed,\u5982\u679cTrue,\u5219\u7ee7\u7eed\u8fed\u4ee3\n    clusterChanged = True\n    while clusterChanged:\n        clusterChanged = False\n        # \u904d\u5386\u6240\u6709\u6570\u636e\u627e\u5230\u8ddd\u79bb\u6bcf\u4e2a\u70b9\u6700\u8fd1\u7684\u8d28\u5fc3,\n        # \u53ef\u4ee5\u901a\u8fc7\u5bf9\u6bcf\u4e2a\u70b9\u904d\u5386\u6240\u6709\u8d28\u5fc3\u5e76\u8ba1\u7b97\u70b9\u5230\u6bcf\u4e2a\u8d28\u5fc3\u7684\u8ddd\u79bb\u6765\u5b8c\u6210\n        for i in range(m):\n            minDist = inf\n            minIndex = -1\n            for j in range(k):\n                # \u8ba1\u7b97\u6570\u636e\u70b9\u5230\u8d28\u5fc3\u7684\u8ddd\u79bb\n                # \u8ba1\u7b97\u8ddd\u79bb\u662f\u4f7f\u7528distMeas\u53c2\u6570\u7ed9\u51fa\u7684\u8ddd\u79bb\u516c\u5f0f,\u9ed8\u8ba4\u8ddd\u79bb\u51fd\u6570\u662fdistEclud\n                distJI = distMeas(centroids[j, :], dataMat[i, :])\n                # \u5982\u679c\u8ddd\u79bb\u6bd4minDist(\u6700\u5c0f\u8ddd\u79bb)\u8fd8\u5c0f,\u66f4\u65b0minDist(\u6700\u5c0f\u8ddd\u79bb)\u548c\u6700\u5c0f\u8d28\u5fc3\u7684index(\u7d22\u5f15)\n                if distJI < minDist:\n                    minDist = distJI\n                    minIndex = j\n            # \u5982\u679c\u4efb\u4e00\u70b9\u7684\u7c07\u5206\u914d\u7ed3\u679c\u53d1\u751f\u6539\u53d8,\u5219\u66f4\u65b0clusterChanged\u6807\u5fd7\n            if clusterAssment[i, 0] != minIndex: clusterChanged = True\n            # \u66f4\u65b0\u7c07\u5206\u914d\u7ed3\u679c\u4e3a\u6700\u5c0f\u8d28\u5fc3\u7684index(\u7d22\u5f15),minDist(\u6700\u5c0f\u8ddd\u79bb)\u7684\u5e73\u65b9\n            clusterAssment[i, :] = minIndex, minDist ** 2\n        # print(centroids)\n        # \u904d\u5386\u6240\u6709\u8d28\u5fc3\u5e76\u66f4\u65b0\u5b83\u4eec\u7684\u53d6\u503c\n        for cent in range(k):\n            # \u901a\u8fc7\u6570\u636e\u8fc7\u6ee4\u6765\u83b7\u5f97\u7ed9\u5b9a\u7c07\u7684\u6240\u6709\u70b9\n            ptsInClust = dataMat[nonzero(clusterAssment[:, 0].A == cent)[0]]\n            # \u8ba1\u7b97\u6240\u6709\u70b9\u7684\u5747\u503c,axis=0\u8868\u793a\u6cbf\u77e9\u9635\u7684\u5217\u65b9\u5411\u8fdb\u884c\u5747\u503c\u8ba1\u7b97\n            centroids[cent, :] = mean(ptsInClust, axis=0)\n    # \u8fd4\u56de\u6240\u6709\u7684\u7c7b\u8d28\u5fc3\u4e0e\u70b9\u5206\u914d\u7ed3\u679c\n    return centroids, clusterAssment\n\n\ndef biKmeans(dataMat, k, distMeas=distEclud):\n    '''\n    \u5728\u7ed9\u5b9a\u6570\u636e\u96c6,\u6240\u671f\u671b\u7684\u7c07\u6570\u76ee\u548c\u8ddd\u79bb\u8ba1\u7b97\u65b9\u6cd5\u7684\u6761\u4ef6\u4e0b,\u51fd\u6570\u8fd4\u56de\u805a\u7c7b\u7ed3\u679c\n    :param dataMat:\n    :param k:\n    :param distMeas:\n    :return:\n    '''\n    m, n = shape(dataMat)\n    # \u521b\u5efa\u4e00\u4e2a\u77e9\u9635\u6765\u5b58\u50a8\u6570\u636e\u96c6\u4e2d\u6bcf\u4e2a\u70b9\u7684\u7c07\u5206\u914d\u7ed3\u679c\u53ca\u5e73\u65b9\u8bef\u5dee\n    clusterAssment = mat(zeros((m, 2)))\n    # \u8ba1\u7b97\u6574\u4e2a\u6570\u636e\u96c6\u7684\u8d28\u5fc3,\u5e76\u4f7f\u7528\u4e00\u4e2a\u5217\u8868\u6765\u4fdd\u7559\u6240\u6709\u7684\u8d28\u5fc3\n    centroid0 = mean(dataMat, axis=0).tolist()[0]\n    centList = [centroid0]\n    # \u904d\u5386\u6570\u636e\u96c6\u4e2d\u6240\u6709\u70b9\u6765\u8ba1\u7b97\u6bcf\u4e2a\u70b9\u5230\u8d28\u5fc3\u7684\u8bef\u5dee\u503c\n    for j in range(m):\n        clusterAssment[j, 1] = distMeas(mat(centroid0), dataMat[j, :]) ** 2\n    # \u5bf9\u7c07\u4e0d\u505c\u7684\u8fdb\u884c\u5212\u5206,\u76f4\u5230\u5f97\u5230\u60f3\u8981\u7684\u7c07\u6570\u76ee\u4e3a\u6b62\n    while (len(centList) < k):\n        # \u521d\u59cb\u5316\u6700\u5c0fSSE\u4e3a\u65e0\u7a77\u5927,\u7528\u4e8e\u6bd4\u8f83\u5212\u5206\u524d\u540e\u7684SSE\n        lowestSSE = inf\n        # \u901a\u8fc7\u8003\u5bdf\u7c07\u5217\u8868\u4e2d\u7684\u503c\u6765\u83b7\u5f97\u5f53\u524d\u7c07\u7684\u6570\u76ee,\u904d\u5386\u6240\u6709\u7684\u7c07\u6765\u51b3\u5b9a\u6700\u4f73\u7684\u7c07\u8fdb\u884c\u5212\u5206\n        for i in range(len(centList)):\n            # \u5bf9\u6bcf\u4e00\u4e2a\u7c07,\u5c06\u8be5\u7c07\u4e2d\u7684\u6240\u6709\u70b9\u582a\u79f0\u4e00\u4e2a\u5c0f\u7684\u6570\u636e\u96c6\n            ptsInCurrCluster = dataMat[nonzero(clusterAssment[:, 0].A == i)[0], :]\n            # \u5c06ptsInCurrCluster\u8f93\u5165\u5230\u51fd\u6570kMeans\u4e2d\u8fdb\u884c\u5904\u7406,k=2,\n            # kMeans\u4f1a\u751f\u6210\u4e24\u4e2a\u8d28\u5fc3(\u7c07),\u540c\u65f6\u7ed9\u51fa\u6bcf\u4e2a\u7c07\u7684\u8bef\u5dee\u503c\n            centroidMat, splitClustAss = kMeans(ptsInCurrCluster, 2, distMeas)\n            # \u5c06\u8bef\u5dee\u503c\u4e0e\u5269\u4f59\u6570\u636e\u96c6\u7684\u8bef\u5dee\u4e4b\u548c\u4f5c\u4e3a\u672c\u6b21\u5212\u5206\u7684\u8bef\u5dee\n            sseSplit = sum(splitClustAss[:, 1])\n            sseNotSplit = sum(clusterAssment[nonzero(clusterAssment[:, 0].A != i)[0], 1])\n            print('sseSplit, and notSplit: ', sseSplit, sseNotSplit)\n            # \u5982\u679c\u672c\u6b21\u5212\u5206\u7684SSE\u503c\u6700\u5c0f,\u5219\u672c\u6b21\u5212\u5206\u88ab\u4fdd\u5b58\n            if (sseSplit + sseNotSplit) < lowestSSE:\n                bestCentToSplit = i\n                bestNewCents = centroidMat\n                bestClustAss = splitClustAss.copy()\n                lowestSSE = sseSplit + sseNotSplit\n        # \u627e\u51fa\u6700\u597d\u7684\u7c07\u5206\u914d\u7ed3\u679c\n        # \u8c03\u7528kmeans\u51fd\u6570\u5e76\u4e14\u6307\u5b9a\u7c07\u6570\u4e3a2\u65f6,\u4f1a\u5f97\u5230\u4e24\u4e2a\u7f16\u53f7\u5206\u522b\u4e3a0\u548c1\u7684\u7ed3\u679c\u7c07\n        bestClustAss[nonzero(bestClustAss[:, 0].A == 1)[0], 0] = len(centList)\n        # \u66f4\u65b0\u4e3a\u6700\u4f73\u8d28\u5fc3\n        bestClustAss[nonzero(bestClustAss[:, 0].A == 0)[0], 0] = bestCentToSplit\n        print('the bestCentToSplit is: ', bestCentToSplit)\n        print('the len of bestClustAss is: ', len(bestClustAss))\n        # \u66f4\u65b0\u8d28\u5fc3\u5217\u8868\n        # \u66f4\u65b0\u539f\u8d28\u5fc3list\u4e2d\u7684\u7b2ci\u4e2a\u8d28\u5fc3\u4e3a\u4f7f\u7528\u4e8c\u5206kMeans\u540ebestNewCents\u7684\u7b2c\u4e00\u4e2a\u8d28\u5fc3\n        centList[bestCentToSplit] = bestNewCents[0, :].tolist()[0]\n        # \u6dfb\u52a0bestNewCents\u7684\u7b2c\u4e8c\u4e2a\u8d28\u5fc3\n        centList.append(bestNewCents[1, :].tolist()[0])\n        # \u91cd\u65b0\u5206\u914d\u6700\u597d\u7c07\u4e0b\u7684\u6570\u636e(\u8d28\u5fc3)\u4ee5\u53caSSE\n        clusterAssment[nonzero(clusterAssment[:, 0].A == bestCentToSplit)[0], :] = bestClustAss\n    return mat(centList), clusterAssment\n\n\ndef distSLC(vecA, vecB):\n    '''\n    \u8fd4\u56de\u5730\u7403\u8868\u9762\u4e24\u70b9\u95f4\u7684\u8ddd\u79bb,\u5355\u4f4d\u662f\u82f1\u91cc\n    \u7ed9\u5b9a\u4e24\u4e2a\u70b9\u7684\u7ecf\u7eac\u5ea6,\u53ef\u4ee5\u4f7f\u7528\u7403\u9762\u4f59\u5f26\u5b9a\u7406\u6765\u8ba1\u7b97\u4eae\u70b9\u7684\u8ddd\u79bb\n    :param vecA:\n    :param vecB:\n    :return:\n    '''\n    # \u7ecf\u5ea6\u548c\u7ef4\u5ea6\u7528\u89d2\u5ea6\u4f5c\u4e3a\u5355\u4f4d,\u4f46\u662fsin()\u548ccos()\u4ee5\u5f27\u5ea6\u4e3a\u8f93\u5165.\n    # \u53ef\u4ee5\u5c06\u6c5f\u90fd\u9664\u4ee5180\u5ea6\u7136\u540e\u518d\u8bda\u610f\u5706\u5468\u7387pi\u8f6c\u6362\u4e3a\u5f27\u5ea6\n    a = sin(vecA[0, 1] * pi / 180) * sin(vecB[0, 1] * pi / 180)\n    b = cos(vecA[0, 1] * pi / 180) * cos(vecB[0, 1] * pi / 180) * \\\n        cos(pi * (vecB[0, 0] - vecA[0, 0]) / 180)\n    return arccos(a + b) * 6371.0\n\n\ndef clusterClubs(fileName, imgName, numClust=5):\n    '''\n    \u5c06\u6587\u672c\u6587\u4ef6\u7684\u89e3\u6790,\u805a\u7c7b\u4ee5\u53ca\u753b\u56fe\u90fd\u5c01\u88c5\u5728\u4e00\u8d77\n    :param fileName: \u6587\u672c\u6570\u636e\u8def\u5f84\n    :param imgName: \u56fe\u7247\u8def\u5f84\n    :param numClust: \u5e0c\u671b\u5f97\u5230\u7684\u7c07\u6570\u76ee\n    :return:\n    '''\n    # \u521b\u5efa\u4e00\u4e2a\u7a7a\u5217\u8868\n    datList = []\n    # \u6253\u5f00\u6587\u672c\u6587\u4ef6\u83b7\u53d6\u7b2c4\u5217\u548c\u7b2c5\u5217,\u8fd9\u4e24\u5217\u5206\u522b\u5bf9\u5e94\u7ef4\u5ea6\u548c\u7ecf\u5ea6,\u7136\u540e\u5c06\u8fd9\u4e9b\u503c\u5c01\u88c5\u5230datList\n    for line in open(fileName).readlines():\n        lineArr = line.split('\\t')\n        datList.append([float(lineArr[4]), float(lineArr[3])])\n    datMat = mat(datList)\n    # \u8c03\u7528biKmeans\u5e76\u4f7f\u7528distSLC\u51fd\u6570\u4f5c\u4e3a\u805a\u7c7b\u4e2d\u4f7f\u7528\u7684\u8ddd\u79bb\u8ba1\u7b97\u65b9\u5f0f\n    myCentroids, clustAssing = biKmeans(datMat, numClust, distMeas=distSLC)\n    # \u521b\u5efa\u4e00\u5e45\u56fe\u548c\u4e00\u4e2a\u4e3e\u884c,\u4f7f\u7528\u8be5\u77e9\u5f62\u6765\u51b3\u5b9a\u7ed8\u5236\u56fe\u7684\u54ea\u4e00\u90e8\u5206\n    fig = plt.figure()\n    rect = [0.1, 0.1, 0.8, 0.8]\n    # \u6784\u5efa\u4e00\u4e2a\u6807\u8bb0\u5f62\u72b6\u7684\u5217\u8868\u7528\u4e8e\u7ed8\u5236\u6563\u70b9\u56fe\n    scatterMarkers = ['s', 'o', '^', '8', 'p', 'd', 'v', 'h', '>', '<']\n    axprops = dict(xticks=[], yticks=[])\n    ax0 = fig.add_axes(rect, label='ax0', **axprops)\n    # \u4f7f\u7528imread\u51fd\u6570\u57fa\u4e8e\u4e00\u5e45\u56fe\u50cf\u6765\u521b\u5efa\u77e9\u9635\n    imgP = plt.imread(imgName)\n    # \u4f7f\u7528imshow\u7ed8\u5236\u8be5\u77e9\u9635\n    ax0.imshow(imgP)\n    # \u518d\u540c\u4e00\u5e45\u56fe\u4e0a\u7ed8\u5236\u4e00\u5f20\u65b0\u56fe,\u5141\u8bb8\u4f7f\u7528\u4e24\u5957\u5750\u6807\u7cfb\u7edf\u5e76\u4e0d\u505a\u4efb\u4f55\u7f29\u653e\u6216\u504f\u79fb\n    ax1 = fig.add_axes(rect, label='ax1', frameon=False)\n    # \u904d\u5386\u6bcf\u4e00\u4e2a\u7c07\u5e76\u5c06\u5b83\u4eec\u4e00\u4e00\u753b\u51fa\u6765,\u6807\u8bb0\u7c7b\u578b\u4ece\u524d\u9762\u521b\u5efa\u7684scatterMarkers\u5217\u8868\u4e2d\u5f97\u5230\n    for i in range(numClust):\n        ptsInCurrCluster = datMat[nonzero(clustAssing[:, 0].A == i)[0], :]\n        # \u4f7f\u7528\u7d22\u5f15i % len(scatterMarkers)\u6765\u9009\u62e9\u6807\u8bb0\u5f62\u72b6,\u8fd9\u610f\u5473\u8fd9\u5f53\u6709\u66f4\u591a\u7c07\u65f6,\u53ef\u4ee5\u5faa\u73af\u4f7f\u7528\u8fd9\u6807\u8bb0\n        markerStyle = scatterMarkers[i % len(scatterMarkers)]\n        # \u4f7f\u7528\u5341\u5b57\u6807\u8bb0\u6765\u8868\u793a\u7c07\u4e2d\u5fc3\u5e76\u5728\u56fe\u4e2d\u663e\u793a\n        ax1.scatter(ptsInCurrCluster[:, 0].flatten().A[0], ptsInCurrCluster[:, 1].flatten().A[0], marker=markerStyle,\n                    s=90)\n    ax1.scatter(myCentroids[:, 0].flatten().A[0], myCentroids[:, 1].flatten().A[0], marker='+', s=300)\n    plt.show()\n", "src/py3.x/ml/10.kmeans/kMeansSklearn.py": "# -*- coding:UTF-8 -*-\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\n\n# \u52a0\u8f7d\u6570\u636e\u96c6\ndataMat = []\nfr = open(\"data/10.KMeans/testSet.txt\") # \u6ce8\u610f\uff0c\u8fd9\u4e2a\u662f\u76f8\u5bf9\u8def\u5f84\uff0c\u8bf7\u4fdd\u8bc1\u662f\u5728 MachineLearning \u8fd9\u4e2a\u76ee\u5f55\u4e0b\u6267\u884c\u3002\nfor line in fr.readlines():\n    curLine = line.strip().split('\\t')\n    fltLine = list(map(float,curLine))    # \u6620\u5c04\u6240\u6709\u7684\u5143\u7d20\u4e3a float\uff08\u6d6e\u70b9\u6570\uff09\u7c7b\u578b\n    dataMat.append(fltLine)\n\n# \u8bad\u7ec3\u6a21\u578b\nkm = KMeans(n_clusters=4) # \u521d\u59cb\u5316\nkm.fit(dataMat) # \u62df\u5408\nkm_pred = km.predict(dataMat) # \u9884\u6d4b\ncenters = km.cluster_centers_ # \u8d28\u5fc3\n\n# \u53ef\u89c6\u5316\u7ed3\u679c\nplt.scatter(np.array(dataMat)[:, 1], np.array(dataMat)[:, 0], c=km_pred)\nplt.scatter(centers[:, 1], centers[:, 0], c=\"r\")\nplt.show()\n", "src/py3.x/ml/10.kmeans/__init__.py": "#!/usr/bin/env python\n__coding__ = \"utf-8\"\n__author__ = \"Ng WaiMing\"\n\nfrom training.action.unsupervised.kMeans import kMeans\nfrom numpy import *\n\nif __name__ == '__main__':\n    # dataMat = mat(kMeans.loadDataSet('../../../../data/k-means/testSet.txt'))\n    # print('min(dataMat[:, 0])', min(dataMat[:, 0]), '\\n')\n    # print('min(dataMat[:, 1])', min(dataMat[:, 1]), '\\n')\n    # print('max(dataMat[:, 0])', max(dataMat[:, 0]), '\\n')\n    # print('max(dataMat[:, 1])', max(dataMat[:, 1]), '\\n')\n    # print(kMeans.randCent(dataMat, 2),'\\n')\n    # print(kMeans.distEclud(dataMat[0],dataMat[1]))\n    # centroids, clusterAssment = kMeans.kMeans(dataMat, 4)\n    # print('centroids:\\n', centroids, '\\n')\n    # print('clusterAssment:\\n',clusterAssment, '\\n')\n    # dataMat3 = mat(kMeans.loadDataSet('../../../../data/k-means/testSet2.txt'))\n    # centList, myNewAssments = kMeans.biKmeans(dataMat3, 3)\n    # print('centList: \\n', centList, '\\n')\n    fileName = '../../../../data/k-means/places.txt'\n    imgName = '../../../../data/k-means/Portland.png'\n    kMeans.clusterClubs(fileName=fileName, imgName=imgName, numClust=5)\n", "src/py3.x/ml/7.RandomForest/randomForest.py": "#!/usr/bin/python\n# coding:utf8\n\n'''\nCreated 2017-04-25\nUpdate  on 2017-05-18\nRandom Forest Algorithm on Sonar Dataset\nAuthor: Flying_sfeng/\u7247\u523b\nGitHub: https://github.com/apachecn/AiLearning\n---\n\u6e90\u4ee3\u7801\u7f51\u5740: http://www.tuicool.com/articles/iiUfeim\nFlying_sfeng\u535a\u5ba2\u5730\u5740: http://blog.csdn.net/flying_sfeng/article/details/64133822\n\u5728\u6b64\u8868\u793a\u611f\u8c22\u4f60\u7684\u4ee3\u7801\u548c\u6ce8\u89e3\uff0c \u6211\u91cd\u65b0\u4e5f\u5b8c\u5584\u4e86\u4e2a\u4eba\u6ce8\u89e3\n'''\nfrom random import seed, randrange, random\n\n\n# \u5bfc\u5165csv\u6587\u4ef6\ndef loadDataSet(filename):\n    dataset = []\n    with open(filename, 'r') as fr:\n        for line in fr.readlines():\n            if not line:\n                continue\n            lineArr = []\n            for featrue in line.split(','):\n                # strip()\u8fd4\u56de\u79fb\u9664\u5b57\u7b26\u4e32\u5934\u5c3e\u6307\u5b9a\u7684\u5b57\u7b26\u751f\u6210\u7684\u65b0\u5b57\u7b26\u4e32\n                str_f = featrue.strip()\n                if str_f.isdigit():   # \u5224\u65ad\u662f\u5426\u662f\u6570\u5b57\n                    # \u5c06\u6570\u636e\u96c6\u7684\u7b2ccolumn\u5217\u8f6c\u6362\u6210float\u5f62\u5f0f\n                    lineArr.append(float(str_f))\n                else:\n                    # \u6dfb\u52a0\u5206\u7c7b\u6807\u7b7e\n                    lineArr.append(str_f)\n            dataset.append(lineArr)\n    return dataset\n\n\ndef cross_validation_split(dataset, n_folds):\n    \"\"\"cross_validation_split(\u5c06\u6570\u636e\u96c6\u8fdb\u884c\u62bd\u91cd\u62bd\u6837 n_folds \u4efd\uff0c\u6570\u636e\u53ef\u4ee5\u91cd\u590d\u91cd\u590d\u62bd\u53d6\uff0c\u6bcf\u4e00\u6b21list\u7684\u5143\u7d20\u662f\u65e0\u91cd\u590d\u7684)\n\n    Args:\n        dataset     \u539f\u59cb\u6570\u636e\u96c6\n        n_folds     \u6570\u636e\u96c6dataset\u5206\u6210n_flods\u4efd\n    Returns:\n        dataset_split    list\u96c6\u5408\uff0c\u5b58\u653e\u7684\u662f: \u5c06\u6570\u636e\u96c6\u8fdb\u884c\u62bd\u91cd\u62bd\u6837 n_folds \u4efd\uff0c\u6570\u636e\u53ef\u4ee5\u91cd\u590d\u91cd\u590d\u62bd\u53d6\uff0c\u6bcf\u4e00\u6b21list\u7684\u5143\u7d20\u662f\u65e0\u91cd\u590d\u7684\n    \"\"\"\n    dataset_split = list()\n    dataset_copy = list(dataset)       # \u590d\u5236\u4e00\u4efd dataset,\u9632\u6b62 dataset \u7684\u5185\u5bb9\u6539\u53d8\n    fold_size = len(dataset) / n_folds\n    for i in range(n_folds):\n        fold = list()                  # \u6bcf\u6b21\u5faa\u73af fold \u6e05\u96f6\uff0c\u9632\u6b62\u91cd\u590d\u5bfc\u5165 dataset_split\n        while len(fold) < fold_size:   # \u8fd9\u91cc\u4e0d\u80fd\u7528 if\uff0cif \u53ea\u662f\u5728\u7b2c\u4e00\u6b21\u5224\u65ad\u65f6\u8d77\u4f5c\u7528\uff0cwhile \u6267\u884c\u5faa\u73af\uff0c\u76f4\u5230\u6761\u4ef6\u4e0d\u6210\u7acb\n            # \u6709\u653e\u56de\u7684\u968f\u673a\u91c7\u6837\uff0c\u6709\u4e00\u4e9b\u6837\u672c\u88ab\u91cd\u590d\u91c7\u6837\uff0c\u4ece\u800c\u5728\u8bad\u7ec3\u96c6\u4e2d\u591a\u6b21\u51fa\u73b0\uff0c\u6709\u7684\u5219\u4ece\u672a\u5728\u8bad\u7ec3\u96c6\u4e2d\u51fa\u73b0\uff0c\u6b64\u5219\u81ea\u52a9\u91c7\u6837\u6cd5\u3002\u4ece\u800c\u4fdd\u8bc1\u6bcf\u68f5\u51b3\u7b56\u6811\u8bad\u7ec3\u96c6\u7684\u5dee\u5f02\u6027            \n            index = randrange(len(dataset_copy))\n            # \u5c06\u5bf9\u5e94\u7d22\u5f15 index \u7684\u5185\u5bb9\u4ece dataset_copy \u4e2d\u5bfc\u51fa\uff0c\u5e76\u5c06\u8be5\u5185\u5bb9\u4ece dataset_copy \u4e2d\u5220\u9664\u3002\n            # pop() \u51fd\u6570\u7528\u4e8e\u79fb\u9664\u5217\u8868\u4e2d\u7684\u4e00\u4e2a\u5143\u7d20\uff08\u9ed8\u8ba4\u6700\u540e\u4e00\u4e2a\u5143\u7d20\uff09\uff0c\u5e76\u4e14\u8fd4\u56de\u8be5\u5143\u7d20\u7684\u503c\u3002\n            # fold.append(dataset_copy.pop(index))  # \u65e0\u653e\u56de\u7684\u65b9\u5f0f\n            fold.append(dataset_copy[index])  # \u6709\u653e\u56de\u7684\u65b9\u5f0f\n        dataset_split.append(fold)\n    # \u7531dataset\u5206\u5272\u51fa\u7684n_folds\u4e2a\u6570\u636e\u6784\u6210\u7684\u5217\u8868\uff0c\u4e3a\u4e86\u7528\u4e8e\u4ea4\u53c9\u9a8c\u8bc1\n    return dataset_split\n\n\n# Split a dataset based on an attribute and an attribute value # \u6839\u636e\u7279\u5f81\u548c\u7279\u5f81\u503c\u5206\u5272\u6570\u636e\u96c6\ndef test_split(index, value, dataset):\n    left, right = list(), list()\n    for row in dataset:\n        if row[index] < value:\n            left.append(row)\n        else:\n            right.append(row)\n    return left, right\n\n\n# Calculate the Gini index for a split dataset\ndef gini_index(groups, class_values):    # \u4e2a\u4eba\u7406\u89e3: \u8ba1\u7b97\u4ee3\u4ef7\uff0c\u5206\u7c7b\u8d8a\u51c6\u786e\uff0c\u5219 gini \u8d8a\u5c0f\n    gini = 0.0\n    for class_value in class_values:     # class_values = [0, 1] \n        for group in groups:             # groups = (left, right)\n            size = len(group)\n            if size == 0:\n                continue\n            proportion = [row[-1] for row in group].count(class_value) / float(size)\n            gini += (proportion * (1.0 - proportion))    # \u4e2a\u4eba\u7406\u89e3: \u8ba1\u7b97\u4ee3\u4ef7\uff0c\u5206\u7c7b\u8d8a\u51c6\u786e\uff0c\u5219 gini \u8d8a\u5c0f\n    return gini\n\n\n# \u627e\u51fa\u5206\u5272\u6570\u636e\u96c6\u7684\u6700\u4f18\u7279\u5f81\uff0c\u5f97\u5230\u6700\u4f18\u7684\u7279\u5f81 index\uff0c\u7279\u5f81\u503c row[index]\uff0c\u4ee5\u53ca\u5206\u5272\u5b8c\u7684\u6570\u636e groups\uff08left, right\uff09\ndef get_split(dataset, n_features):\n    class_values = list(set(row[-1] for row in dataset))  # class_values =[0, 1]\n    b_index, b_value, b_score, b_groups = 999, 999, 999, None\n    features = list()\n    while len(features) < n_features:\n        index = randrange(len(dataset[0])-1)  # \u5f80 features \u6dfb\u52a0 n_features \u4e2a\u7279\u5f81\uff08 n_feature \u7b49\u4e8e\u7279\u5f81\u6570\u7684\u6839\u53f7\uff09\uff0c\u7279\u5f81\u7d22\u5f15\u4ece dataset \u4e2d\u968f\u673a\u53d6\n        if index not in features:\n            features.append(index)\n    for index in features:                    # \u5728 n_features \u4e2a\u7279\u5f81\u4e2d\u9009\u51fa\u6700\u4f18\u7684\u7279\u5f81\u7d22\u5f15\uff0c\u5e76\u6ca1\u6709\u904d\u5386\u6240\u6709\u7279\u5f81\uff0c\u4ece\u800c\u4fdd\u8bc1\u4e86\u6bcf\u8bfe\u51b3\u7b56\u6811\u7684\u5dee\u5f02\u6027\n        for row in dataset:\n            groups = test_split(index, row[index], dataset)  # groups=(left, right), row[index] \u904d\u5386\u6bcf\u4e00\u884c index \u7d22\u5f15\u4e0b\u7684\u7279\u5f81\u503c\u4f5c\u4e3a\u5206\u7c7b\u503c value, \u627e\u51fa\u6700\u4f18\u7684\u5206\u7c7b\u7279\u5f81\u548c\u7279\u5f81\u503c\n            gini = gini_index(groups, class_values)\n            # \u5de6\u53f3\u4e24\u8fb9\u7684\u6570\u91cf\u8d8a\u4e00\u6837\uff0c\u8bf4\u660e\u6570\u636e\u533a\u5206\u5ea6\u4e0d\u9ad8\uff0cgini\u7cfb\u6570\u8d8a\u5927\n            if gini < b_score:\n                b_index, b_value, b_score, b_groups = index, row[index], gini, groups  # \u6700\u540e\u5f97\u5230\u6700\u4f18\u7684\u5206\u7c7b\u7279\u5f81 b_index,\u5206\u7c7b\u7279\u5f81\u503c b_value,\u5206\u7c7b\u7ed3\u679c b_groups\u3002b_value \u4e3a\u5206\u9519\u7684\u4ee3\u4ef7\u6210\u672c\n    # print(b_score)\n    return {'index': b_index, 'value': b_value, 'groups': b_groups}\n\n\n# Create a terminal node value # \u8f93\u51fagroup\u4e2d\u51fa\u73b0\u6b21\u6570\u8f83\u591a\u7684\u6807\u7b7e\ndef to_terminal(group):\n    outcomes = [row[-1] for row in group]           # max() \u51fd\u6570\u4e2d\uff0c\u5f53 key \u53c2\u6570\u4e0d\u4e3a\u7a7a\u65f6\uff0c\u5c31\u4ee5 key \u7684\u51fd\u6570\u5bf9\u8c61\u4e3a\u5224\u65ad\u7684\u6807\u51c6\n    return max(set(outcomes), key=outcomes.count)   # \u8f93\u51fa group \u4e2d\u51fa\u73b0\u6b21\u6570\u8f83\u591a\u7684\u6807\u7b7e  \n\n\n# Create child splits for a node or make terminal  # \u521b\u5efa\u5b50\u5206\u5272\u5668\uff0c\u9012\u5f52\u5206\u7c7b\uff0c\u76f4\u5230\u5206\u7c7b\u7ed3\u675f\ndef split(node, max_depth, min_size, n_features, depth):  # max_depth = 10, min_size = 1, n_features = int(sqrt((dataset[0])-1))\n    left, right = node['groups']\n    del(node['groups'])\n# check for a no split\n    if not left or not right:\n        node['left'] = node['right'] = to_terminal(left + right)\n        return\n# check for max depth\n    if depth >= max_depth:   # max_depth=10 \u8868\u793a\u9012\u5f52\u5341\u6b21\uff0c\u82e5\u5206\u7c7b\u8fd8\u672a\u7ed3\u675f\uff0c\u5219\u9009\u53d6\u6570\u636e\u4e2d\u5206\u7c7b\u6807\u7b7e\u8f83\u591a\u7684\u4f5c\u4e3a\u7ed3\u679c\uff0c\u4f7f\u5206\u7c7b\u63d0\u524d\u7ed3\u675f\uff0c\u9632\u6b62\u8fc7\u62df\u5408\n        node['left'], node['right'] = to_terminal(left), to_terminal(right)\n        return\n# process left child\n    if len(left) <= min_size:\n        node['left'] = to_terminal(left)\n    else:\n        node['left'] = get_split(left, n_features)  # node['left']\u662f\u4e00\u4e2a\u5b57\u5178\uff0c\u5f62\u5f0f\u4e3a{'index':b_index, 'value':b_value, 'groups':b_groups}\uff0c\u6240\u4ee5node\u662f\u4e00\u4e2a\u591a\u5c42\u5b57\u5178\n        split(node['left'], max_depth, min_size, n_features, depth+1)  # \u9012\u5f52\uff0cdepth+1\u8ba1\u7b97\u9012\u5f52\u5c42\u6570\n# process right child\n    if len(right) <= min_size:\n        node['right'] = to_terminal(right)\n    else:\n        node['right'] = get_split(right, n_features)\n        split(node['right'], max_depth, min_size, n_features, depth+1)\n\n\n# Build a decision tree\ndef build_tree(train, max_depth, min_size, n_features):\n    \"\"\"build_tree(\u521b\u5efa\u4e00\u4e2a\u51b3\u7b56\u6811)\n\n    Args:\n        train           \u8bad\u7ec3\u6570\u636e\u96c6\n        max_depth       \u51b3\u7b56\u6811\u6df1\u5ea6\u4e0d\u80fd\u592a\u6df1\uff0c\u4e0d\u7136\u5bb9\u6613\u5bfc\u81f4\u8fc7\u62df\u5408\n        min_size        \u53f6\u5b50\u8282\u70b9\u7684\u5927\u5c0f\n        n_features      \u9009\u53d6\u7684\u7279\u5f81\u7684\u4e2a\u6570\n    Returns:\n        root            \u8fd4\u56de\u51b3\u7b56\u6811\n    \"\"\"\n\n    # \u8fd4\u56de\u6700\u4f18\u5217\u548c\u76f8\u5173\u7684\u4fe1\u606f\n    root = get_split(train, n_features)\n\n    # \u5bf9\u5de6\u53f32\u8fb9\u7684\u6570\u636e \u8fdb\u884c\u9012\u5f52\u7684\u8c03\u7528\uff0c\u7531\u4e8e\u6700\u4f18\u7279\u5f81\u4f7f\u7528\u8fc7\uff0c\u6240\u4ee5\u5728\u540e\u9762\u8fdb\u884c\u4f7f\u7528\u7684\u65f6\u5019\uff0c\u5c31\u6ca1\u6709\u610f\u4e49\u4e86\n    # \u4f8b\u5982:  \u6027\u522b-\u7537\u5973\uff0c\u5bf9\u7537\u4f7f\u7528\u8fd9\u4e00\u7279\u5f81\u5c31\u6ca1\u4efb\u4f55\u610f\u4e49\u4e86\n    split(root, max_depth, min_size, n_features, 1)\n    return root\n\n\n# Make a prediction with a decision tree\ndef predict(node, row):   # \u9884\u6d4b\u6a21\u578b\u5206\u7c7b\u7ed3\u679c\n    if row[node['index']] < node['value']:\n        if isinstance(node['left'], dict):       # isinstance \u662f Python \u4e2d\u7684\u4e00\u4e2a\u5185\u5efa\u51fd\u6570\u3002\u662f\u7528\u6765\u5224\u65ad\u4e00\u4e2a\u5bf9\u8c61\u662f\u5426\u662f\u4e00\u4e2a\u5df2\u77e5\u7684\u7c7b\u578b\u3002\n            return predict(node['left'], row)\n        else:\n            return node['left']\n    else:\n        if isinstance(node['right'], dict):\n            return predict(node['right'], row)\n        else:\n            return node['right']\n\n\n# Make a prediction with a list of bagged trees\ndef bagging_predict(trees, row):\n    \"\"\"bagging_predict(bagging\u9884\u6d4b)\n\n    Args:\n        trees           \u51b3\u7b56\u6811\u7684\u96c6\u5408\n        row             \u6d4b\u8bd5\u6570\u636e\u96c6\u7684\u6bcf\u4e00\u884c\u6570\u636e\n    Returns:\n        \u8fd4\u56de\u968f\u673a\u68ee\u6797\u4e2d\uff0c\u51b3\u7b56\u6811\u7ed3\u679c\u51fa\u73b0\u6b21\u6570\u505a\u5927\u7684\n    \"\"\"\n\n    # \u4f7f\u7528\u591a\u4e2a\u51b3\u7b56\u6811trees\u5bf9\u6d4b\u8bd5\u96c6test\u7684\u7b2crow\u884c\u8fdb\u884c\u9884\u6d4b\uff0c\u518d\u4f7f\u7528\u7b80\u5355\u6295\u7968\u6cd5\u5224\u65ad\u51fa\u8be5\u884c\u6240\u5c5e\u5206\u7c7b\n    predictions = [predict(tree, row) for tree in trees]\n    return max(set(predictions), key=predictions.count)\n\n\n# Create a random subsample from the dataset with replacement\ndef subsample(dataset, ratio):   # \u521b\u5efa\u6570\u636e\u96c6\u7684\u968f\u673a\u5b50\u6837\u672c\n    \"\"\"random_forest(\u8bc4\u4f30\u7b97\u6cd5\u6027\u80fd\uff0c\u8fd4\u56de\u6a21\u578b\u5f97\u5206)\n\n    Args:\n        dataset         \u8bad\u7ec3\u6570\u636e\u96c6\n        ratio           \u8bad\u7ec3\u6570\u636e\u96c6\u7684\u6837\u672c\u6bd4\u4f8b\n    Returns:\n        sample          \u968f\u673a\u62bd\u6837\u7684\u8bad\u7ec3\u6837\u672c\n    \"\"\"\n\n    sample = list()\n    # \u8bad\u7ec3\u6837\u672c\u7684\u6309\u6bd4\u4f8b\u62bd\u6837\u3002\n    # round() \u65b9\u6cd5\u8fd4\u56de\u6d6e\u70b9\u6570x\u7684\u56db\u820d\u4e94\u5165\u503c\u3002\n    n_sample = round(len(dataset) * ratio)\n    while len(sample) < n_sample:\n        # \u6709\u653e\u56de\u7684\u968f\u673a\u91c7\u6837\uff0c\u6709\u4e00\u4e9b\u6837\u672c\u88ab\u91cd\u590d\u91c7\u6837\uff0c\u4ece\u800c\u5728\u8bad\u7ec3\u96c6\u4e2d\u591a\u6b21\u51fa\u73b0\uff0c\u6709\u7684\u5219\u4ece\u672a\u5728\u8bad\u7ec3\u96c6\u4e2d\u51fa\u73b0\uff0c\u6b64\u5219\u81ea\u52a9\u91c7\u6837\u6cd5\u3002\u4ece\u800c\u4fdd\u8bc1\u6bcf\u68f5\u51b3\u7b56\u6811\u8bad\u7ec3\u96c6\u7684\u5dee\u5f02\u6027\n        index = randrange(len(dataset))\n        sample.append(dataset[index])\n    return sample\n\n\n# Random Forest Algorithm\ndef random_forest(train, test, max_depth, min_size, sample_size, n_trees, n_features):\n    \"\"\"random_forest(\u8bc4\u4f30\u7b97\u6cd5\u6027\u80fd\uff0c\u8fd4\u56de\u6a21\u578b\u5f97\u5206)\n\n    Args:\n        train           \u8bad\u7ec3\u6570\u636e\u96c6\n        test            \u6d4b\u8bd5\u6570\u636e\u96c6\n        max_depth       \u51b3\u7b56\u6811\u6df1\u5ea6\u4e0d\u80fd\u592a\u6df1\uff0c\u4e0d\u7136\u5bb9\u6613\u5bfc\u81f4\u8fc7\u62df\u5408\n        min_size        \u53f6\u5b50\u8282\u70b9\u7684\u5927\u5c0f\n        sample_size     \u8bad\u7ec3\u6570\u636e\u96c6\u7684\u6837\u672c\u6bd4\u4f8b\n        n_trees         \u51b3\u7b56\u6811\u7684\u4e2a\u6570\n        n_features      \u9009\u53d6\u7684\u7279\u5f81\u7684\u4e2a\u6570\n    Returns:\n        predictions     \u6bcf\u4e00\u884c\u7684\u9884\u6d4b\u7ed3\u679c\uff0cbagging \u9884\u6d4b\u6700\u540e\u7684\u5206\u7c7b\u7ed3\u679c\n    \"\"\"\n\n    trees = list()\n    # n_trees \u8868\u793a\u51b3\u7b56\u6811\u7684\u6570\u91cf\n    for i in range(n_trees):\n        # \u968f\u673a\u62bd\u6837\u7684\u8bad\u7ec3\u6837\u672c\uff0c \u968f\u673a\u91c7\u6837\u4fdd\u8bc1\u4e86\u6bcf\u68f5\u51b3\u7b56\u6811\u8bad\u7ec3\u96c6\u7684\u5dee\u5f02\u6027\n        sample = subsample(train, sample_size)\n        # \u521b\u5efa\u4e00\u4e2a\u51b3\u7b56\u6811\n        tree = build_tree(sample, max_depth, min_size, n_features)\n        trees.append(tree)\n\n    # \u6bcf\u4e00\u884c\u7684\u9884\u6d4b\u7ed3\u679c\uff0cbagging \u9884\u6d4b\u6700\u540e\u7684\u5206\u7c7b\u7ed3\u679c\n    predictions = [bagging_predict(trees, row) for row in test]\n    return predictions\n\n\n# Calculate accuracy percentage\ndef accuracy_metric(actual, predicted):  # \u5bfc\u5165\u5b9e\u9645\u503c\u548c\u9884\u6d4b\u503c\uff0c\u8ba1\u7b97\u7cbe\u786e\u5ea6\n    correct = 0\n    for i in range(len(actual)):\n        if actual[i] == predicted[i]:\n            correct += 1\n    return correct / float(len(actual)) * 100.0\n\n\n# \u8bc4\u4f30\u7b97\u6cd5\u6027\u80fd\uff0c\u8fd4\u56de\u6a21\u578b\u5f97\u5206\ndef evaluate_algorithm(dataset, algorithm, n_folds, *args):\n    \"\"\"evaluate_algorithm(\u8bc4\u4f30\u7b97\u6cd5\u6027\u80fd\uff0c\u8fd4\u56de\u6a21\u578b\u5f97\u5206)\n\n    Args:\n        dataset     \u539f\u59cb\u6570\u636e\u96c6\n        algorithm   \u4f7f\u7528\u7684\u7b97\u6cd5\n        n_folds     \u6570\u636e\u7684\u4efd\u6570\n        *args       \u5176\u4ed6\u7684\u53c2\u6570\n    Returns:\n        scores      \u6a21\u578b\u5f97\u5206\n    \"\"\"\n\n    # \u5c06\u6570\u636e\u96c6\u8fdb\u884c\u62bd\u91cd\u62bd\u6837 n_folds \u4efd\uff0c\u6570\u636e\u53ef\u4ee5\u91cd\u590d\u91cd\u590d\u62bd\u53d6\uff0c\u6bcf\u4e00\u6b21 list \u7684\u5143\u7d20\u662f\u65e0\u91cd\u590d\u7684\n    folds = cross_validation_split(dataset, n_folds)\n    scores = list()\n    # \u6bcf\u6b21\u5faa\u73af\u4ece folds \u4ece\u53d6\u51fa\u4e00\u4e2a fold \u4f5c\u4e3a\u6d4b\u8bd5\u96c6\uff0c\u5176\u4f59\u4f5c\u4e3a\u8bad\u7ec3\u96c6\uff0c\u904d\u5386\u6574\u4e2a folds \uff0c\u5b9e\u73b0\u4ea4\u53c9\u9a8c\u8bc1\n    for fold in folds:\n        train_set = list(folds)\n        train_set.remove(fold)\n        # \u5c06\u591a\u4e2a fold \u5217\u8868\u7ec4\u5408\u6210\u4e00\u4e2a train_set \u5217\u8868, \u7c7b\u4f3c union all\n        \"\"\"\n        In [20]: l1=[[1, 2, 'a'], [11, 22, 'b']]\n        In [21]: l2=[[3, 4, 'c'], [33, 44, 'd']]\n        In [22]: l=[]\n        In [23]: l.append(l1)\n        In [24]: l.append(l2)\n        In [25]: l\n        Out[25]: [[[1, 2, 'a'], [11, 22, 'b']], [[3, 4, 'c'], [33, 44, 'd']]]\n        In [26]: sum(l, [])\n        Out[26]: [[1, 2, 'a'], [11, 22, 'b'], [3, 4, 'c'], [33, 44, 'd']]\n        \"\"\"\n        train_set = sum(train_set, [])\n        test_set = list()\n        # fold \u8868\u793a\u4ece\u539f\u59cb\u6570\u636e\u96c6 dataset \u63d0\u53d6\u51fa\u6765\u7684\u6d4b\u8bd5\u96c6\n        for row in fold:\n            row_copy = list(row)\n            row_copy[-1] = None\n            test_set.append(row_copy)\n        predicted = algorithm(train_set, test_set, *args)\n        actual = [row[-1] for row in fold]\n\n        # \u8ba1\u7b97\u968f\u673a\u68ee\u6797\u7684\u9884\u6d4b\u7ed3\u679c\u7684\u6b63\u786e\u7387\n        accuracy = accuracy_metric(actual, predicted)\n        scores.append(accuracy)\n    return scores\n\n\nif __name__ == '__main__':\n\n    # \u52a0\u8f7d\u6570\u636e\n    dataset = loadDataSet('data/7.RandomForest/sonar-all-data.txt')\n    # print(dataset)\n\n    n_folds = 5        # \u5206\u62105\u4efd\u6570\u636e\uff0c\u8fdb\u884c\u4ea4\u53c9\u9a8c\u8bc1\n    max_depth = 20     # \u8c03\u53c2\uff08\u81ea\u5df1\u4fee\u6539\uff09 #\u51b3\u7b56\u6811\u6df1\u5ea6\u4e0d\u80fd\u592a\u6df1\uff0c\u4e0d\u7136\u5bb9\u6613\u5bfc\u81f4\u8fc7\u62df\u5408\n    min_size = 1       # \u51b3\u7b56\u6811\u7684\u53f6\u5b50\u8282\u70b9\u6700\u5c11\u7684\u5143\u7d20\u6570\u91cf\n    sample_size = 1.0  # \u505a\u51b3\u7b56\u6811\u65f6\u5019\u7684\u6837\u672c\u7684\u6bd4\u4f8b\n    # n_features = int((len(dataset[0])-1))\n    n_features = 15     # \u8c03\u53c2\uff08\u81ea\u5df1\u4fee\u6539\uff09 #\u51c6\u786e\u6027\u4e0e\u591a\u6837\u6027\u4e4b\u95f4\u7684\u6743\u8861\n    for n_trees in [1, 10, 20]:  # \u7406\u8bba\u4e0a\u6811\u662f\u8d8a\u591a\u8d8a\u597d\n        scores = evaluate_algorithm(dataset, random_forest, n_folds, max_depth, min_size, sample_size, n_trees, n_features)\n        # \u6bcf\u4e00\u6b21\u6267\u884c\u672c\u6587\u4ef6\u65f6\u90fd\u80fd\u4ea7\u751f\u540c\u4e00\u4e2a\u968f\u673a\u6570\n        seed(1)\n        print('random=', random())\n        print('Trees: %d' % n_trees)\n        print('Scores: %s' % scores)\n        print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))\n", "src/py3.x/ml/6.SVM/svm-complete.py": "#!/usr/bin/python\n# -*- coding:utf-8 -*-\n\n\"\"\"\nCreated on Nov 4, 2010\nUpdate on 2017-05-18\nChapter 5 source file for Machine Learing in Action\nAuthor: Peter/geekidentity/\u7247\u523b\nGitHub: https://github.com/apachecn/AiLearning\n\"\"\"\nfrom numpy import *\nimport matplotlib.pyplot as plt\n\n\nclass optStruct:\n    \"\"\"\n    \u5efa\u7acb\u7684\u6570\u636e\u7ed3\u6784\u6765\u4fdd\u5b58\u6240\u6709\u7684\u91cd\u8981\u503c\n    \"\"\"\n    def __init__(self, dataMatIn, classLabels, C, toler, kTup):\n        \"\"\"\n        Args:\n            dataMatIn    \u6570\u636e\u96c6\n            classLabels  \u7c7b\u522b\u6807\u7b7e\n            C   \u677e\u5f1b\u53d8\u91cf(\u5e38\u91cf\u503c)\uff0c\u5141\u8bb8\u6709\u4e9b\u6570\u636e\u70b9\u53ef\u4ee5\u5904\u4e8e\u5206\u9694\u9762\u7684\u9519\u8bef\u4e00\u4fa7\u3002\n                \u63a7\u5236\u6700\u5927\u5316\u95f4\u9694\u548c\u4fdd\u8bc1\u5927\u90e8\u5206\u7684\u51fd\u6570\u95f4\u9694\u5c0f\u4e8e1.0\u8fd9\u4e24\u4e2a\u76ee\u6807\u7684\u6743\u91cd\u3002\n                \u53ef\u4ee5\u901a\u8fc7\u8c03\u8282\u8be5\u53c2\u6570\u8fbe\u5230\u4e0d\u540c\u7684\u7ed3\u679c\u3002\n            toler   \u5bb9\u9519\u7387\n            kTup    \u5305\u542b\u6838\u51fd\u6570\u4fe1\u606f\u7684\u5143\u7ec4\n        \"\"\"\n\n        self.X = dataMatIn\n        self.labelMat = classLabels\n        self.C = C\n        self.tol = toler\n\n        # \u6570\u636e\u7684\u884c\u6570\n        self.m = shape(dataMatIn)[0]\n        self.alphas = mat(zeros((self.m, 1)))\n        self.b = 0\n\n        # \u8bef\u5dee\u7f13\u5b58\uff0c\u7b2c\u4e00\u5217\u7ed9\u51fa\u7684\u662feCache\u662f\u5426\u6709\u6548\u7684\u6807\u5fd7\u4f4d\uff0c\u7b2c\u4e8c\u5217\u7ed9\u51fa\u7684\u662f\u5b9e\u9645\u7684E\u503c\u3002\n        self.eCache = mat(zeros((self.m, 2)))\n\n        # m\u884cm\u5217\u7684\u77e9\u9635\n        self.K = mat(zeros((self.m, self.m)))\n        for i in range(self.m):\n            self.K[:, i] = kernelTrans(self.X, self.X[i], kTup)\n\n\ndef kernelTrans(X, A, kTup):  # calc the kernel or transform data to a higher dimensional space\n    \"\"\"\n    \u6838\u8f6c\u6362\u51fd\u6570\n    Args:\n        X     dataMatIn\u6570\u636e\u96c6\n        A     dataMatIn\u6570\u636e\u96c6\u7684\u7b2ci\u884c\u7684\u6570\u636e\n        kTup  \u6838\u51fd\u6570\u7684\u4fe1\u606f\n\n    Returns:\n\n    \"\"\"\n    m, n = shape(X)\n    K = mat(zeros((m, 1)))\n    if kTup[0] == 'lin':\n        # linear kernel:   m*n * n*1 = m*1\n        K = X * A.T\n    elif kTup[0] == 'rbf':\n        for j in range(m):\n            deltaRow = X[j, :] - A\n            K[j] = deltaRow * deltaRow.T\n        # \u5f84\u5411\u57fa\u51fd\u6570\u7684\u9ad8\u65af\u7248\u672c\n        K = exp(K / (-1 * kTup[1] ** 2))  # divide in NumPy is element-wise not matrix like Matlab\n    else:\n        raise NameError('Houston We Have a Problem -- That Kernel is not recognized')\n    return K\n\n\ndef loadDataSet(fileName):\n    \"\"\"loadDataSet\uff08\u5bf9\u6587\u4ef6\u8fdb\u884c\u9010\u884c\u89e3\u6790\uff0c\u4ece\u800c\u5f97\u5230\u7b2c\u884c\u7684\u7c7b\u6807\u7b7e\u548c\u6574\u4e2a\u6570\u636e\u77e9\u9635\uff09\n\n    Args:\n        fileName \u6587\u4ef6\u540d\n    Returns:\n        dataMat  \u6570\u636e\u77e9\u9635\n        labelMat \u7c7b\u6807\u7b7e\n    \"\"\"\n    dataMat = []\n    labelMat = []\n    fr = open(fileName)\n    for line in fr.readlines():\n        lineArr = line.strip().split('\\t')\n        dataMat.append([float(lineArr[0]), float(lineArr[1])])\n        labelMat.append(float(lineArr[2]))\n    return dataMat, labelMat\n\n\ndef calcEk(oS, k):\n    \"\"\"calcEk\uff08\u6c42 Ek\u8bef\u5dee: \u9884\u6d4b\u503c-\u771f\u5b9e\u503c\u7684\u5dee\uff09\n\n    \u8be5\u8fc7\u7a0b\u5728\u5b8c\u6574\u7248\u7684SMO\u7b97\u6cd5\u4e2d\u966a\u51fa\u73b0\u6b21\u6570\u8f83\u591a\uff0c\u56e0\u6b64\u5c06\u5176\u5355\u72ec\u4f5c\u4e3a\u4e00\u4e2a\u65b9\u6cd5\n    Args:\n        oS  optStruct\u5bf9\u8c61\n        k   \u5177\u4f53\u7684\u67d0\u4e00\u884c\n\n    Returns:\n        Ek  \u9884\u6d4b\u7ed3\u679c\u4e0e\u771f\u5b9e\u7ed3\u679c\u6bd4\u5bf9\uff0c\u8ba1\u7b97\u8bef\u5deeEk\n    \"\"\"\n    fXk = multiply(oS.alphas, oS.labelMat).T * oS.K[:, k] + oS.b\n    Ek = fXk - float(oS.labelMat[k])\n    return Ek\n\n\ndef selectJrand(i, m):\n    \"\"\"\n    \u968f\u673a\u9009\u62e9\u4e00\u4e2a\u6574\u6570\n    Args:\n        i  \u7b2c\u4e00\u4e2aalpha\u7684\u4e0b\u6807\n        m  \u6240\u6709alpha\u7684\u6570\u76ee\n    Returns:\n        j  \u8fd4\u56de\u4e00\u4e2a\u4e0d\u4e3ai\u7684\u968f\u673a\u6570\uff0c\u57280~m\u4e4b\u95f4\u7684\u6574\u6570\u503c\n    \"\"\"\n    j = i\n    while j == i:\n        j = random.randint(0, m - 1)\n    return j\n\n\ndef selectJ(i, oS, Ei):  # this is the second choice -heurstic, and calcs Ej\n    \"\"\"selectJ\uff08\u8fd4\u56de\u6700\u4f18\u7684j\u548cEj\uff09\n\n    \u5185\u5faa\u73af\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\u3002\n    \u9009\u62e9\u7b2c\u4e8c\u4e2a(\u5185\u5faa\u73af)alpha\u7684alpha\u503c\n    \u8fd9\u91cc\u7684\u76ee\u6807\u662f\u9009\u62e9\u5408\u9002\u7684\u7b2c\u4e8c\u4e2aalpha\u503c\u4ee5\u4fdd\u8bc1\u6bcf\u6b21\u4f18\u5316\u4e2d\u91c7\u7528\u6700\u5927\u6b65\u957f\u3002\n    \u8be5\u51fd\u6570\u7684\u8bef\u5dee\u4e0e\u7b2c\u4e00\u4e2aalpha\u503cEi\u548c\u4e0b\u6807i\u6709\u5173\u3002\n    Args:\n        i   \u5177\u4f53\u7684\u7b2ci\u4e00\u884c\n        oS  optStruct\u5bf9\u8c61\n        Ei  \u9884\u6d4b\u7ed3\u679c\u4e0e\u771f\u5b9e\u7ed3\u679c\u6bd4\u5bf9\uff0c\u8ba1\u7b97\u8bef\u5deeEi\n\n    Returns:\n        j  \u968f\u673a\u9009\u51fa\u7684\u7b2cj\u4e00\u884c\n        Ej \u9884\u6d4b\u7ed3\u679c\u4e0e\u771f\u5b9e\u7ed3\u679c\u6bd4\u5bf9\uff0c\u8ba1\u7b97\u8bef\u5deeEj\n    \"\"\"\n    maxK = -1\n    maxDeltaE = 0\n    Ej = 0\n    # \u9996\u5148\u5c06\u8f93\u5165\u503cEi\u5728\u7f13\u5b58\u4e2d\u8bbe\u7f6e\u6210\u4e3a\u6709\u6548\u7684\u3002\u8fd9\u91cc\u7684\u6709\u6548\u610f\u5473\u7740\u5b83\u5df2\u7ecf\u8ba1\u7b97\u597d\u4e86\u3002\n    oS.eCache[i] = [1, Ei]\n\n    # print('oS.eCache[%s]=%s' % (i, oS.eCache[i]))\n    # print('oS.eCache[:, 0].A=%s' % oS.eCache[:, 0].A.T)\n    # \"\"\"\n    # # \u8fd4\u56de\u975e0\u7684: \u884c\u5217\u503c\n    # nonzero(oS.eCache[:, 0].A)= (\n    #     \u884c:  array([ 0,  2,  4,  5,  8, 10, 17, 18, 20, 21, 23, 25, 26, 29, 30, 39, 46,52, 54, 55, 62, 69, 70, 76, 79, 82, 94, 97]),\n    #     \u5217:  array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,0, 0, 0, 0, 0])\n    # )\n    # \"\"\"\n    # print('nonzero(oS.eCache[:, 0].A)=', nonzero(oS.eCache[:, 0].A))\n    # # \u53d6\u884c\u7684list\n    # print('nonzero(oS.eCache[:, 0].A)[0]=', nonzero(oS.eCache[:, 0].A)[0])\n    # \u975e\u96f6E\u503c\u7684\u884c\u7684list\u5217\u8868\uff0c\u6240\u5bf9\u5e94\u7684alpha\u503c\n    validEcacheList = nonzero(oS.eCache[:, 0].A)[0]\n    if (len(validEcacheList)) > 1:\n        for k in validEcacheList:  # \u5728\u6240\u6709\u7684\u503c\u4e0a\u8fdb\u884c\u5faa\u73af\uff0c\u5e76\u9009\u62e9\u5176\u4e2d\u4f7f\u5f97\u6539\u53d8\u6700\u5927\u7684\u90a3\u4e2a\u503c\n            if k == i:\n                continue  # don't calc for i, waste of time\n\n            # \u6c42 Ek\u8bef\u5dee: \u9884\u6d4b\u503c-\u771f\u5b9e\u503c\u7684\u5dee\n            Ek = calcEk(oS, k)\n            deltaE = abs(Ei - Ek)\n            if deltaE > maxDeltaE:\n                # \u9009\u62e9\u5177\u6709\u6700\u5927\u6b65\u957f\u7684j\n                maxK = k\n                maxDeltaE = deltaE\n                Ej = Ek\n        return maxK, Ej\n    else:  # \u5982\u679c\u662f\u7b2c\u4e00\u6b21\u5faa\u73af\uff0c\u5219\u968f\u673a\u9009\u62e9\u4e00\u4e2aalpha\u503c\n        j = selectJrand(i, oS.m)\n\n        # \u6c42 Ek\u8bef\u5dee: \u9884\u6d4b\u503c-\u771f\u5b9e\u503c\u7684\u5dee\n        Ej = calcEk(oS, j)\n    return j, Ej\n\n\ndef updateEk(oS, k):\n    \"\"\"updateEk\uff08\u8ba1\u7b97\u8bef\u5dee\u503c\u5e76\u5b58\u5165\u7f13\u5b58\u4e2d\u3002\uff09\n\n    \u5728\u5bf9alpha\u503c\u8fdb\u884c\u4f18\u5316\u4e4b\u540e\u4f1a\u7528\u5230\u8fd9\u4e2a\u503c\u3002\n    Args:\n        oS  optStruct\u5bf9\u8c61\n        k   \u67d0\u4e00\u5217\u7684\u884c\u53f7\n    \"\"\"\n\n    # \u6c42 \u8bef\u5dee: \u9884\u6d4b\u503c-\u771f\u5b9e\u503c\u7684\u5dee\n    Ek = calcEk(oS, k)\n    oS.eCache[k] = [1, Ek]\n\n\ndef clipAlpha(aj, H, L):\n    \"\"\"clipAlpha(\u8c03\u6574aj\u7684\u503c\uff0c\u4f7faj\u5904\u4e8e L<=aj<=H)\n    Args:\n        aj  \u76ee\u6807\u503c\n        H   \u6700\u5927\u503c\n        L   \u6700\u5c0f\u503c\n    Returns:\n        aj  \u76ee\u6807\u503c\n    \"\"\"\n    aj = min(aj, H)\n    aj = max(L, aj)\n    return aj\n\n\ndef innerL(i, oS):\n    \"\"\"innerL\n    \u5185\u5faa\u73af\u4ee3\u7801\n    Args:\n        i   \u5177\u4f53\u7684\u67d0\u4e00\u884c\n        oS  optStruct\u5bf9\u8c61\n\n    Returns:\n        0   \u627e\u4e0d\u5230\u6700\u4f18\u7684\u503c\n        1   \u627e\u5230\u4e86\u6700\u4f18\u7684\u503c\uff0c\u5e76\u4e14oS.Cache\u5230\u7f13\u5b58\u4e2d\n    \"\"\"\n\n    # \u6c42 Ek\u8bef\u5dee: \u9884\u6d4b\u503c-\u771f\u5b9e\u503c\u7684\u5dee\n    Ei = calcEk(oS, i)\n\n    # \u7ea6\u675f\u6761\u4ef6 (KKT\u6761\u4ef6\u662f\u89e3\u51b3\u6700\u4f18\u5316\u95ee\u9898\u7684\u65f6\u7528\u5230\u7684\u4e00\u79cd\u65b9\u6cd5\u3002\u6211\u4eec\u8fd9\u91cc\u63d0\u5230\u7684\u6700\u4f18\u5316\u95ee\u9898\u901a\u5e38\u662f\u6307\u5bf9\u4e8e\u7ed9\u5b9a\u7684\u67d0\u4e00\u51fd\u6570\uff0c\u6c42\u5176\u5728\u6307\u5b9a\u4f5c\u7528\u57df\u4e0a\u7684\u5168\u5c40\u6700\u5c0f\u503c)\n    # 0<=alphas[i]<=C\uff0c\u4f46\u7531\u4e8e0\u548cC\u662f\u8fb9\u754c\u503c\uff0c\u6211\u4eec\u65e0\u6cd5\u8fdb\u884c\u4f18\u5316\uff0c\u56e0\u4e3a\u9700\u8981\u589e\u52a0\u4e00\u4e2aalphas\u548c\u964d\u4f4e\u4e00\u4e2aalphas\u3002\n    # \u8868\u793a\u53d1\u751f\u9519\u8bef\u7684\u6982\u7387: labelMat[i]*Ei \u5982\u679c\u8d85\u51fa\u4e86 toler\uff0c \u624d\u9700\u8981\u4f18\u5316\u3002\u81f3\u4e8e\u6b63\u8d1f\u53f7\uff0c\u6211\u4eec\u8003\u8651\u7edd\u5bf9\u503c\u5c31\u5bf9\u4e86\u3002\n    '''\n    # \u68c0\u9a8c\u8bad\u7ec3\u6837\u672c(xi, yi)\u662f\u5426\u6ee1\u8db3KKT\u6761\u4ef6\n    yi*f(i) >= 1 and alpha = 0 (outside the boundary)\n    yi*f(i) == 1 and 0<alpha< C (on the boundary)\n    yi*f(i) <= 1 and alpha = C (between the boundary)\n    '''\n    if ((oS.labelMat[i] * Ei < -oS.tol) and (oS.alphas[i] < oS.C)) or ((oS.labelMat[i] * Ei > oS.tol) and (oS.alphas[i] > 0)):\n        # \u9009\u62e9\u6700\u5927\u7684\u8bef\u5dee\u5bf9\u5e94\u7684j\u8fdb\u884c\u4f18\u5316\u3002\u6548\u679c\u66f4\u660e\u663e\n        j, Ej = selectJ(i, oS, Ei)\n        alphaIold = oS.alphas[i].copy()\n        alphaJold = oS.alphas[j].copy()\n\n        # L\u548cH\u7528\u4e8e\u5c06alphas[j]\u8c03\u6574\u52300-C\u4e4b\u95f4\u3002\u5982\u679cL==H\uff0c\u5c31\u4e0d\u505a\u4efb\u4f55\u6539\u53d8\uff0c\u76f4\u63a5return 0\n        if (oS.labelMat[i] != oS.labelMat[j]):\n            L = max(0, oS.alphas[j] - oS.alphas[i])\n            H = min(oS.C, oS.C + oS.alphas[j] - oS.alphas[i])\n        else:\n            L = max(0, oS.alphas[j] + oS.alphas[i] - oS.C)\n            H = min(oS.C, oS.alphas[j] + oS.alphas[i])\n        if L == H:\n            # print(\"L==H\")\n            return 0\n\n        # eta\u662falphas[j]\u7684\u6700\u4f18\u4fee\u6539\u91cf\uff0c\u5982\u679ceta==0\uff0c\u9700\u8981\u9000\u51fafor\u5faa\u73af\u7684\u5f53\u524d\u8fed\u4ee3\u8fc7\u7a0b\n        # \u53c2\u8003\u300a\u7edf\u8ba1\u5b66\u4e60\u65b9\u6cd5\u300b\u674e\u822a-P125~P128<\u5e8f\u5217\u6700\u5c0f\u6700\u4f18\u5316\u7b97\u6cd5>\n        eta = 2.0 * oS.K[i, j] - oS.K[i, i] - oS.K[j, j]  # changed for kernel\n        if eta >= 0:\n            print(\"eta>=0\")\n            return 0\n\n        # \u8ba1\u7b97\u51fa\u4e00\u4e2a\u65b0\u7684alphas[j]\u503c\n        oS.alphas[j] -= oS.labelMat[j] * (Ei - Ej) / eta\n        # \u5e76\u4f7f\u7528\u8f85\u52a9\u51fd\u6570\uff0c\u4ee5\u53caL\u548cH\u5bf9\u5176\u8fdb\u884c\u8c03\u6574\n        oS.alphas[j] = clipAlpha(oS.alphas[j], H, L)\n        # \u66f4\u65b0\u8bef\u5dee\u7f13\u5b58\n        updateEk(oS, j)\n\n        # \u68c0\u67e5alpha[j]\u662f\u5426\u53ea\u662f\u8f7b\u5fae\u7684\u6539\u53d8\uff0c\u5982\u679c\u662f\u7684\u8bdd\uff0c\u5c31\u9000\u51fafor\u5faa\u73af\u3002\n        if abs(oS.alphas[j] - alphaJold) < 0.00001:\n            # print(\"j not moving enough\")\n            return 0\n\n        # \u7136\u540ealphas[i]\u548calphas[j]\u540c\u6837\u8fdb\u884c\u6539\u53d8\uff0c\u867d\u7136\u6539\u53d8\u7684\u5927\u5c0f\u4e00\u6837\uff0c\u4f46\u662f\u6539\u53d8\u7684\u65b9\u5411\u6b63\u597d\u76f8\u53cd\n        oS.alphas[i] += oS.labelMat[j] * oS.labelMat[i] * (alphaJold - oS.alphas[j])\n        # \u66f4\u65b0\u8bef\u5dee\u7f13\u5b58\n        updateEk(oS, i)\n\n        # \u5728\u5bf9alpha[i], alpha[j] \u8fdb\u884c\u4f18\u5316\u4e4b\u540e\uff0c\u7ed9\u8fd9\u4e24\u4e2aalpha\u503c\u8bbe\u7f6e\u4e00\u4e2a\u5e38\u6570b\u3002\n        # w= \u03a3[1~n] ai*yi*xi => b = yi- \u03a3[1~n] ai*yi(xi*xj)\n        # \u6240\u4ee5:   b1 - b = (y1-y) - \u03a3[1~n] yi*(a1-a)*(xi*x1)\n        # \u4e3a\u4ec0\u4e48\u51cf2\u904d\uff1f \u56e0\u4e3a\u662f \u51cf\u53bb\u03a3[1~n]\uff0c\u6b63\u597d2\u4e2a\u53d8\u91cfi\u548cj\uff0c\u6240\u4ee5\u51cf2\u904d\n        b1 = oS.b - Ei - oS.labelMat[i] * (oS.alphas[i] - alphaIold) * oS.K[i, i] - oS.labelMat[j] * (oS.alphas[j] - alphaJold) * oS.K[i, j]\n        b2 = oS.b - Ej - oS.labelMat[i] * (oS.alphas[i] - alphaIold) * oS.K[i, j] - oS.labelMat[j] * (oS.alphas[j] - alphaJold) * oS.K[j, j]\n        if (0 < oS.alphas[i]) and (oS.C > oS.alphas[i]):\n            oS.b = b1\n        elif (0 < oS.alphas[j]) and (oS.C > oS.alphas[j]):\n            oS.b = b2\n        else:\n            oS.b = (b1 + b2) / 2\n        return 1\n    else:\n        return 0\n\n\ndef smoP(dataMatIn, classLabels, C, toler, maxIter, kTup=('lin', 0)):\n    \"\"\"\n    \u5b8c\u6574SMO\u7b97\u6cd5\u5916\u5faa\u73af\uff0c\u4e0esmoSimple\u6709\u4e9b\u7c7b\u4f3c\uff0c\u4f46\u8fd9\u91cc\u7684\u5faa\u73af\u9000\u51fa\u6761\u4ef6\u66f4\u591a\u4e00\u4e9b\n    Args:\n        dataMatIn    \u6570\u636e\u96c6\n        classLabels  \u7c7b\u522b\u6807\u7b7e\n        C   \u677e\u5f1b\u53d8\u91cf(\u5e38\u91cf\u503c)\uff0c\u5141\u8bb8\u6709\u4e9b\u6570\u636e\u70b9\u53ef\u4ee5\u5904\u4e8e\u5206\u9694\u9762\u7684\u9519\u8bef\u4e00\u4fa7\u3002\n            \u63a7\u5236\u6700\u5927\u5316\u95f4\u9694\u548c\u4fdd\u8bc1\u5927\u90e8\u5206\u7684\u51fd\u6570\u95f4\u9694\u5c0f\u4e8e1.0\u8fd9\u4e24\u4e2a\u76ee\u6807\u7684\u6743\u91cd\u3002\n            \u53ef\u4ee5\u901a\u8fc7\u8c03\u8282\u8be5\u53c2\u6570\u8fbe\u5230\u4e0d\u540c\u7684\u7ed3\u679c\u3002\n        toler   \u5bb9\u9519\u7387\n        maxIter \u9000\u51fa\u524d\u6700\u5927\u7684\u5faa\u73af\u6b21\u6570\n        kTup    \u5305\u542b\u6838\u51fd\u6570\u4fe1\u606f\u7684\u5143\u7ec4\n    Returns:\n        b       \u6a21\u578b\u7684\u5e38\u91cf\u503c\n        alphas  \u62c9\u683c\u6717\u65e5\u4e58\u5b50\n    \"\"\"\n\n    # \u521b\u5efa\u4e00\u4e2a optStruct \u5bf9\u8c61\n    oS = optStruct(mat(dataMatIn), mat(classLabels).transpose(), C, toler, kTup)\n    iter = 0\n    entireSet = True\n    alphaPairsChanged = 0\n\n    # \u5faa\u73af\u904d\u5386: \u5faa\u73afmaxIter\u6b21 \u5e76\u4e14 \uff08alphaPairsChanged\u5b58\u5728\u53ef\u4ee5\u6539\u53d8 or \u6240\u6709\u884c\u904d\u5386\u4e00\u904d\uff09\n    while (iter < maxIter) and ((alphaPairsChanged > 0) or (entireSet)):\n        alphaPairsChanged = 0\n        # ----------- \u7b2c\u4e00\u79cd\u5199\u6cd5 start -------------------------\n        #  \u5f53entireSet=true or \u975e\u8fb9\u754calpha\u5bf9\u6ca1\u6709\u4e86\uff1b\u5c31\u5f00\u59cb\u5bfb\u627e alpha\u5bf9\uff0c\u7136\u540e\u51b3\u5b9a\u662f\u5426\u8981\u8fdb\u884celse\u3002\n        if entireSet:\n            # \u5728\u6570\u636e\u96c6\u4e0a\u904d\u5386\u6240\u6709\u53ef\u80fd\u7684alpha\n            for i in range(oS.m):\n                # \u662f\u5426\u5b58\u5728alpha\u5bf9\uff0c\u5b58\u5728\u5c31+1\n                alphaPairsChanged += innerL(i, oS)\n                # print(\"fullSet, iter: %d i:%d, pairs changed %d\" % (iter, i, alphaPairsChanged))\n            iter += 1\n\n        # \u5bf9\u5df2\u5b58\u5728 alpha\u5bf9\uff0c\u9009\u51fa\u975e\u8fb9\u754c\u7684alpha\u503c\uff0c\u8fdb\u884c\u4f18\u5316\u3002\n        else:\n            # \u904d\u5386\u6240\u6709\u7684\u975e\u8fb9\u754calpha\u503c\uff0c\u4e5f\u5c31\u662f\u4e0d\u5728\u8fb9\u754c0\u6216C\u4e0a\u7684\u503c\u3002\n            nonBoundIs = nonzero((oS.alphas.A > 0) * (oS.alphas.A < C))[0]\n            for i in nonBoundIs:\n                alphaPairsChanged += innerL(i, oS)\n                # print(\"non-bound, iter: %d i:%d, pairs changed %d\" % (iter, i, alphaPairsChanged))\n            iter += 1\n        # ----------- \u7b2c\u4e00\u79cd\u5199\u6cd5 end -------------------------\n\n        # ----------- \u7b2c\u4e8c\u79cd\u65b9\u6cd5 start -------------------------\n        # if entireSet:\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\u904d\u5386\u6574\u4e2a\u6570\u636e\u96c6\n    \t# \talphaPairsChanged += sum(innerL(i, oS) for i in range(oS.m))\n\t\t# else: \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\u904d\u5386\u975e\u8fb9\u754c\u503c\n\t\t# \tnonBoundIs = nonzero((oS.alphas.A > 0) * (oS.alphas.A < C))[0]\t\t\t\t\t\t#\u904d\u5386\u4e0d\u5728\u8fb9\u754c0\u548cC\u7684alpha\n\t\t# \talphaPairsChanged += sum(innerL(i, oS) for i in nonBoundIs)\n\t\t# iter += 1\n        # ----------- \u7b2c\u4e8c\u79cd\u65b9\u6cd5 end -------------------------\n        # \u5982\u679c\u627e\u5230alpha\u5bf9\uff0c\u5c31\u4f18\u5316\u975e\u8fb9\u754calpha\u503c\uff0c\u5426\u5219\uff0c\u5c31\u91cd\u65b0\u8fdb\u884c\u5bfb\u627e\uff0c\u5982\u679c\u5bfb\u627e\u4e00\u904d \u904d\u5386\u6240\u6709\u7684\u884c\u8fd8\u662f\u6ca1\u627e\u5230\uff0c\u5c31\u9000\u51fa\u5faa\u73af\u3002\n        if entireSet:\n            entireSet = False  # toggle entire set loop\n        elif alphaPairsChanged == 0:\n            entireSet = True\n        print(\"iteration number: %d\" % iter)\n    return oS.b, oS.alphas\n\n\ndef calcWs(alphas, dataArr, classLabels):\n    \"\"\"\n    \u57fa\u4e8ealpha\u8ba1\u7b97w\u503c\n    Args:\n        alphas        \u62c9\u683c\u6717\u65e5\u4e58\u5b50\n        dataArr       feature\u6570\u636e\u96c6\n        classLabels   \u76ee\u6807\u53d8\u91cf\u6570\u636e\u96c6\n\n    Returns:\n        wc  \u56de\u5f52\u7cfb\u6570\n    \"\"\"\n    X = mat(dataArr)\n    labelMat = mat(classLabels).T\n    m, n = shape(X)\n    w = zeros((n, 1))\n    for i in range(m):\n        w += multiply(alphas[i] * labelMat[i], X[i].T)\n    return w\n\n\ndef testRbf(k1=1.3):\n    dataArr, labelArr = loadDataSet('data/6.SVM/testSetRBF.txt')\n    b, alphas = smoP(dataArr, labelArr, 200, 0.0001, 10000, ('rbf', k1))  # C=200 important\n    datMat = mat(dataArr)\n    labelMat = mat(labelArr).transpose()\n    svInd = nonzero(alphas.A > 0)[0]\n    sVs = datMat[svInd]  # get matrix of only support vectors\n    labelSV = labelMat[svInd]\n    print(\"there are %d Support Vectors\" % shape(sVs)[0])\n    m, n = shape(datMat)\n    errorCount = 0\n    for i in range(m):\n        kernelEval = kernelTrans(sVs, datMat[i, :], ('rbf', k1))\n\n        # \u548c\u8fd9\u4e2asvm-simple\u7c7b\u4f3c:  fXi = float(multiply(alphas, labelMat).T*(dataMatrix*dataMatrix[i, :].T)) + b\n        predict = kernelEval.T * multiply(labelSV, alphas[svInd]) + b\n        if sign(predict) != sign(labelArr[i]):\n            errorCount += 1\n    print(\"the training error rate is: %f\" % (float(errorCount) / m))\n\n    dataArr, labelArr = loadDataSet('data/6.SVM/testSetRBF2.txt')\n    errorCount = 0\n    datMat = mat(dataArr)\n    labelMat = mat(labelArr).transpose()\n    m, n = shape(datMat)\n    for i in range(m):\n        kernelEval = kernelTrans(sVs, datMat[i, :], ('rbf', k1))\n        predict = kernelEval.T * multiply(labelSV, alphas[svInd]) + b\n        if sign(predict) != sign(labelArr[i]):\n            errorCount += 1\n    print(\"the test error rate is: %f\" % (float(errorCount) / m))\n\n\ndef img2vector(filename):\n    returnVect = zeros((1, 1024))\n    fr = open(filename)\n    for i in range(32):\n        lineStr = fr.readline()\n        for j in range(32):\n            returnVect[0, 32 * i + j] = int(lineStr[j])\n    return returnVect\n\n\ndef loadImages(dirName):\n    from os import listdir\n    hwLabels = []\n    print(dirName)\n    trainingFileList = listdir(dirName)  # load the training set\n    m = len(trainingFileList)\n    trainingMat = zeros((m, 1024))\n    for i in range(m):\n        fileNameStr = trainingFileList[i]\n        fileStr = fileNameStr.split('.')[0]  # take off .txt\n        classNumStr = int(fileStr.split('_')[0])\n        if classNumStr == 9:\n            hwLabels.append(-1)\n        else:\n            hwLabels.append(1)\n        trainingMat[i, :] = img2vector('%s/%s' % (dirName, fileNameStr))\n    return trainingMat, hwLabels\n\n\ndef testDigits(kTup=('rbf', 10)):\n\n    # 1. \u5bfc\u5165\u8bad\u7ec3\u6570\u636e\n    dataArr, labelArr = loadImages('data/6.SVM/trainingDigits')\n    b, alphas = smoP(dataArr, labelArr, 200, 0.0001, 10000, kTup)\n    datMat = mat(dataArr)\n    labelMat = mat(labelArr).transpose()\n    svInd = nonzero(alphas.A > 0)[0]\n    sVs = datMat[svInd]\n    labelSV = labelMat[svInd]\n    # print(\"there are %d Support Vectors\" % shape(sVs)[0])\n    m, n = shape(datMat)\n    errorCount = 0\n    for i in range(m):\n        kernelEval = kernelTrans(sVs, datMat[i, :], kTup)\n        # 1*m * m*1 = 1*1 \u5355\u4e2a\u9884\u6d4b\u7ed3\u679c\n        predict = kernelEval.T * multiply(labelSV, alphas[svInd]) + b\n        if sign(predict) != sign(labelArr[i]): errorCount += 1\n    print(\"the training error rate is: %f\" % (float(errorCount) / m))\n    # 2. \u5bfc\u5165\u6d4b\u8bd5\u6570\u636e\n    dataArr, labelArr = loadImages('data/6.SVM/testDigits')\n    errorCount = 0\n    datMat = mat(dataArr)\n    labelMat = mat(labelArr).transpose()\n    m, n = shape(datMat)\n    for i in range(m):\n        kernelEval = kernelTrans(sVs, datMat[i, :], kTup)\n        predict = kernelEval.T * multiply(labelSV, alphas[svInd]) + b\n        if sign(predict) != sign(labelArr[i]): errorCount += 1\n    print(\"the test error rate is: %f\" % (float(errorCount) / m))\n\n\ndef plotfig_SVM(xArr, yArr, ws, b, alphas):\n    \"\"\"\n    \u53c2\u8003\u5730\u5740: \n       http://blog.csdn.net/maoersong/article/details/24315633\n       http://www.cnblogs.com/JustForCS/p/5283489.html\n       http://blog.csdn.net/kkxgx/article/details/6951959\n    \"\"\"\n\n    xMat = mat(xArr)\n    yMat = mat(yArr)\n\n    # b\u539f\u6765\u662f\u77e9\u9635\uff0c\u5148\u8f6c\u4e3a\u6570\u7ec4\u7c7b\u578b\u540e\u5176\u6570\u7ec4\u5927\u5c0f\u4e3a\uff081,1\uff09\uff0c\u6240\u4ee5\u540e\u9762\u52a0[0]\uff0c\u53d8\u4e3a(1,)\n    b = array(b)[0]\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n\n    # \u6ce8\u610fflatten\u7684\u7528\u6cd5\n    ax.scatter(xMat[:, 0].flatten().A[0], xMat[:, 1].flatten().A[0])\n\n    # x\u6700\u5927\u503c\uff0c\u6700\u5c0f\u503c\u6839\u636e\u539f\u6570\u636e\u96c6dataArr[:, 0]\u7684\u5927\u5c0f\u800c\u5b9a\n    x = arange(-1.0, 10.0, 0.1)\n\n    # \u6839\u636ex.w + b = 0 \u5f97\u5230\uff0c\u5176\u5f0f\u5b50\u5c55\u5f00\u4e3aw0.x1 + w1.x2 + b = 0, x2\u5c31\u662fy\u503c\n    y = (-b-ws[0, 0]*x)/ws[1, 0]\n    ax.plot(x, y)\n\n    for i in range(shape(yMat[0, :])[1]):\n        if yMat[0, i] > 0:\n            ax.plot(xMat[i, 0], xMat[i, 1], 'cx')\n        else:\n            ax.plot(xMat[i, 0], xMat[i, 1], 'kp')\n\n    # \u627e\u5230\u652f\u6301\u5411\u91cf\uff0c\u5e76\u5728\u56fe\u4e2d\u6807\u7ea2\n    for i in range(100):\n        if alphas[i] > 0.0:\n            ax.plot(xMat[i, 0], xMat[i, 1], 'ro')\n    plt.show()\n\n\nif __name__ == \"__main__\":\n\n    # # \u65e0\u6838\u51fd\u6570\u7684\u6d4b\u8bd5\n    # # \u83b7\u53d6\u7279\u5f81\u548c\u76ee\u6807\u53d8\u91cf\n    # dataArr, labelArr = loadDataSet('data/6.SVM/testSet.txt')\n    # # print(labelArr)\n\n    # # b\u662f\u5e38\u91cf\u503c\uff0c alphas\u662f\u62c9\u683c\u6717\u65e5\u4e58\u5b50\n    # b, alphas = smoP(dataArr, labelArr, 0.6, 0.001, 40)\n    # print('/n/n/n')\n    # print('b=', b)\n    # print('alphas[alphas>0]=', alphas[alphas > 0])\n    # print('shape(alphas[alphas > 0])=', shape(alphas[alphas > 0]))\n    # for i in range(100):\n    #     if alphas[i] > 0:\n    #         print(dataArr[i], labelArr[i])\n    # # \u753b\u56fe\n    # ws = calcWs(alphas, dataArr, labelArr)\n    # plotfig_SVM(dataArr, labelArr, ws, b, alphas)\n\n    # \u6709\u6838\u51fd\u6570\u7684\u6d4b\u8bd5\n    # testRbf(0.8)\n\n    # \u9879\u76ee\u5b9e\u6218\n    # \u793a\u4f8b: \u624b\u5199\u8bc6\u522b\u95ee\u9898\u56de\u987e\n    # testDigits(('rbf', 0.1))\n    # testDigits(('rbf', 5))\n    testDigits(('rbf', 10))\n    # testDigits(('rbf', 50))\n    # testDigits(('rbf', 100))\n    # testDigits(('lin', 10))\n", "src/py3.x/ml/6.SVM/svm-simple.py": "#!/usr/bin/python\n# -*- coding:utf-8 -*-\n\n\"\"\"\nCreated on Nov 4, 2010\nUpdate on 2017-05-18\nChapter 5 source file for Machine Learing in Action\nAuthor: Peter/geekidentity/\u7247\u523b\nGitHub: https://github.com/apachecn/AiLearning\n\"\"\"\nfrom numpy import *\nimport matplotlib.pyplot as plt\n\n\ndef loadDataSet(fileName):\n    \"\"\"\n    \u5bf9\u6587\u4ef6\u8fdb\u884c\u9010\u884c\u89e3\u6790\uff0c\u4ece\u800c\u5f97\u5230\u7b2c\u884c\u7684\u7c7b\u6807\u7b7e\u548c\u6574\u4e2a\u7279\u5f81\u77e9\u9635\n    Args:\n        fileName \u6587\u4ef6\u540d\n    Returns:\n        dataMat  \u7279\u5f81\u77e9\u9635\n        labelMat \u7c7b\u6807\u7b7e\n    \"\"\"\n    dataMat = []\n    labelMat = []\n    fr = open(fileName)\n    for line in fr.readlines():\n        lineArr = line.strip().split('\\t')\n        dataMat.append([float(lineArr[0]), float(lineArr[1])])\n        labelMat.append(float(lineArr[2]))\n    return dataMat, labelMat\n\n\ndef selectJrand(i, m):\n    \"\"\"\n    \u968f\u673a\u9009\u62e9\u4e00\u4e2a\u6574\u6570\n    Args:\n        i  \u7b2c\u4e00\u4e2aalpha\u7684\u4e0b\u6807\n        m  \u6240\u6709alpha\u7684\u6570\u76ee\n    Returns:\n        j  \u8fd4\u56de\u4e00\u4e2a\u4e0d\u4e3ai\u7684\u968f\u673a\u6570\uff0c\u57280~m\u4e4b\u95f4\u7684\u6574\u6570\u503c\n    \"\"\"\n    j = i\n    while j == i:\n        j = int(random.uniform(0, m))\n    return j\n\n\ndef clipAlpha(aj, H, L):\n    \"\"\"clipAlpha(\u8c03\u6574aj\u7684\u503c\uff0c\u4f7faj\u5904\u4e8e L<=aj<=H)\n    Args:\n        aj  \u76ee\u6807\u503c\n        H   \u6700\u5927\u503c\n        L   \u6700\u5c0f\u503c\n    Returns:\n        aj  \u76ee\u6807\u503c\n    \"\"\"\n    if aj > H:\n        aj = H\n    if L > aj:\n        aj = L\n    return aj\n\n\ndef smoSimple(dataMatIn, classLabels, C, toler, maxIter):\n    \"\"\"smoSimple\n\n    Args:\n        dataMatIn    \u6570\u636e\u96c6\n        classLabels  \u7c7b\u522b\u6807\u7b7e\n        C   \u677e\u5f1b\u53d8\u91cf(\u5e38\u91cf\u503c)\uff0c\u5141\u8bb8\u6709\u4e9b\u6570\u636e\u70b9\u53ef\u4ee5\u5904\u4e8e\u5206\u9694\u9762\u7684\u9519\u8bef\u4e00\u4fa7\u3002\n            \u63a7\u5236\u6700\u5927\u5316\u95f4\u9694\u548c\u4fdd\u8bc1\u5927\u90e8\u5206\u7684\u51fd\u6570\u95f4\u9694\u5c0f\u4e8e1.0\u8fd9\u4e24\u4e2a\u76ee\u6807\u7684\u6743\u91cd\u3002\n            \u53ef\u4ee5\u901a\u8fc7\u8c03\u8282\u8be5\u53c2\u6570\u8fbe\u5230\u4e0d\u540c\u7684\u7ed3\u679c\u3002\n        toler   \u5bb9\u9519\u7387\uff08\u662f\u6307\u5728\u67d0\u4e2a\u4f53\u7cfb\u4e2d\u80fd\u51cf\u5c0f\u4e00\u4e9b\u56e0\u7d20\u6216\u9009\u62e9\u5bf9\u67d0\u4e2a\u7cfb\u7edf\u4ea7\u751f\u4e0d\u7a33\u5b9a\u7684\u6982\u7387\u3002\uff09\n        maxIter \u9000\u51fa\u524d\u6700\u5927\u7684\u5faa\u73af\u6b21\u6570\n    Returns:\n        b       \u6a21\u578b\u7684\u5e38\u91cf\u503c\n        alphas  \u62c9\u683c\u6717\u65e5\u4e58\u5b50\n    \"\"\"\n    dataMatrix = mat(dataMatIn)\n    # \u77e9\u9635\u8f6c\u7f6e \u548c .T \u4e00\u6837\u7684\u529f\u80fd\n    labelMat = mat(classLabels).transpose()\n    m, n = shape(dataMatrix)\n\n    # \u521d\u59cb\u5316 b\u548calphas(alpha\u6709\u70b9\u7c7b\u4f3c\u6743\u91cd\u503c\u3002)\n    b = 0\n    alphas = mat(zeros((m, 1)))\n\n    # \u6ca1\u6709\u4efb\u4f55alpha\u6539\u53d8\u7684\u60c5\u51b5\u4e0b\u904d\u5386\u6570\u636e\u7684\u6b21\u6570\n    iter = 0\n    while (iter < maxIter):\n        # w = calcWs(alphas, dataMatIn, classLabels)\n        # print(\"w:\", w)\n\n        # \u8bb0\u5f55alpha\u662f\u5426\u5df2\u7ecf\u8fdb\u884c\u4f18\u5316\uff0c\u6bcf\u6b21\u5faa\u73af\u65f6\u8bbe\u4e3a0\uff0c\u7136\u540e\u518d\u5bf9\u6574\u4e2a\u96c6\u5408\u987a\u5e8f\u904d\u5386\n        alphaPairsChanged = 0\n        for i in range(m):\n            # print('alphas=', alphas)\n            # print('labelMat=', labelMat)\n            # print('multiply(alphas, labelMat)=', multiply(alphas, labelMat))\n            # \u6211\u4eec\u9884\u6d4b\u7684\u7c7b\u522b y = w^Tx[i]+b; \u5176\u4e2d\u56e0\u4e3a w = \u03a3(1~n) a[n]*lable[n]*x[n]\n            fXi = float(multiply(alphas, labelMat).T*(dataMatrix*dataMatrix[i, :].T)) + b\n            # \u9884\u6d4b\u7ed3\u679c\u4e0e\u771f\u5b9e\u7ed3\u679c\u6bd4\u5bf9\uff0c\u8ba1\u7b97\u8bef\u5deeEi\n            Ei = fXi - float(labelMat[i])\n\n            # \u7ea6\u675f\u6761\u4ef6 (KKT\u6761\u4ef6\u662f\u89e3\u51b3\u6700\u4f18\u5316\u95ee\u9898\u7684\u65f6\u7528\u5230\u7684\u4e00\u79cd\u65b9\u6cd5\u3002\u6211\u4eec\u8fd9\u91cc\u63d0\u5230\u7684\u6700\u4f18\u5316\u95ee\u9898\u901a\u5e38\u662f\u6307\u5bf9\u4e8e\u7ed9\u5b9a\u7684\u67d0\u4e00\u51fd\u6570\uff0c\u6c42\u5176\u5728\u6307\u5b9a\u4f5c\u7528\u57df\u4e0a\u7684\u5168\u5c40\u6700\u5c0f\u503c)\n            # 0<=alphas[i]<=C\uff0c\u4f46\u7531\u4e8e0\u548cC\u662f\u8fb9\u754c\u503c\uff0c\u6211\u4eec\u65e0\u6cd5\u8fdb\u884c\u4f18\u5316\uff0c\u56e0\u4e3a\u9700\u8981\u589e\u52a0\u4e00\u4e2aalphas\u548c\u964d\u4f4e\u4e00\u4e2aalphas\u3002\n            # \u8868\u793a\u53d1\u751f\u9519\u8bef\u7684\u6982\u7387: labelMat[i]*Ei \u5982\u679c\u8d85\u51fa\u4e86 toler\uff0c \u624d\u9700\u8981\u4f18\u5316\u3002\u81f3\u4e8e\u6b63\u8d1f\u53f7\uff0c\u6211\u4eec\u8003\u8651\u7edd\u5bf9\u503c\u5c31\u5bf9\u4e86\u3002\n            '''\n            # \u68c0\u9a8c\u8bad\u7ec3\u6837\u672c(xi, yi)\u662f\u5426\u6ee1\u8db3KKT\u6761\u4ef6\n            yi*f(i) >= 1 and alpha = 0 (outside the boundary)\n            yi*f(i) == 1 and 0<alpha< C (on the boundary)\n            yi*f(i) <= 1 and alpha = C (between the boundary)\n            '''\n            if ((labelMat[i]*Ei < -toler) and (alphas[i] < C)) or ((labelMat[i]*Ei > toler) and (alphas[i] > 0)):\n\n                # \u5982\u679c\u6ee1\u8db3\u4f18\u5316\u7684\u6761\u4ef6\uff0c\u6211\u4eec\u5c31\u968f\u673a\u9009\u53d6\u975ei\u7684\u4e00\u4e2a\u70b9\uff0c\u8fdb\u884c\u4f18\u5316\u6bd4\u8f83\n                j = selectJrand(i, m)\n                # \u9884\u6d4bj\u7684\u7ed3\u679c\n                fXj = float(multiply(alphas, labelMat).T*(dataMatrix*dataMatrix[j, :].T)) + b\n                Ej = fXj - float(labelMat[j])\n                alphaIold = alphas[i].copy()\n                alphaJold = alphas[j].copy()\n\n                # L\u548cH\u7528\u4e8e\u5c06alphas[j]\u8c03\u6574\u52300-C\u4e4b\u95f4\u3002\u5982\u679cL==H\uff0c\u5c31\u4e0d\u505a\u4efb\u4f55\u6539\u53d8\uff0c\u76f4\u63a5\u6267\u884ccontinue\u8bed\u53e5\n                # labelMat[i] != labelMat[j] \u8868\u793a\u5f02\u4fa7\uff0c\u5c31\u76f8\u51cf\uff0c\u5426\u5219\u662f\u540c\u4fa7\uff0c\u5c31\u76f8\u52a0\u3002\n                if (labelMat[i] != labelMat[j]):\n                    L = max(0, alphas[j] - alphas[i])\n                    H = min(C, C + alphas[j] - alphas[i])\n                else:\n                    L = max(0, alphas[j] + alphas[i] - C)\n                    H = min(C, alphas[j] + alphas[i])\n                # \u5982\u679c\u76f8\u540c\uff0c\u5c31\u6ca1\u53d1\u4f18\u5316\u4e86\n                if L == H:\n                    print(\"L==H\")\n                    continue\n\n                # eta\u662falphas[j]\u7684\u6700\u4f18\u4fee\u6539\u91cf\uff0c\u5982\u679ceta==0\uff0c\u9700\u8981\u9000\u51fafor\u5faa\u73af\u7684\u5f53\u524d\u8fed\u4ee3\u8fc7\u7a0b\n                # \u53c2\u8003\u300a\u7edf\u8ba1\u5b66\u4e60\u65b9\u6cd5\u300b\u674e\u822a-P125~P128<\u5e8f\u5217\u6700\u5c0f\u6700\u4f18\u5316\u7b97\u6cd5>\n                eta = 2.0 * dataMatrix[i, :]*dataMatrix[j, :].T - dataMatrix[i, :]*dataMatrix[i, :].T - dataMatrix[j, :]*dataMatrix[j, :].T\n                if eta >= 0:\n                    print(\"eta>=0\")\n                    continue\n\n                # \u8ba1\u7b97\u51fa\u4e00\u4e2a\u65b0\u7684alphas[j]\u503c\n                alphas[j] -= labelMat[j]*(Ei - Ej)/eta\n                # \u5e76\u4f7f\u7528\u8f85\u52a9\u51fd\u6570\uff0c\u4ee5\u53caL\u548cH\u5bf9\u5176\u8fdb\u884c\u8c03\u6574\n                alphas[j] = clipAlpha(alphas[j], H, L)\n                # \u68c0\u67e5alpha[j]\u662f\u5426\u53ea\u662f\u8f7b\u5fae\u7684\u6539\u53d8\uff0c\u5982\u679c\u662f\u7684\u8bdd\uff0c\u5c31\u9000\u51fafor\u5faa\u73af\u3002\n                if (abs(alphas[j] - alphaJold) < 0.00001):\n                    print(\"j not moving enough\")\n                    continue\n                # \u7136\u540ealphas[i]\u548calphas[j]\u540c\u6837\u8fdb\u884c\u6539\u53d8\uff0c\u867d\u7136\u6539\u53d8\u7684\u5927\u5c0f\u4e00\u6837\uff0c\u4f46\u662f\u6539\u53d8\u7684\u65b9\u5411\u6b63\u597d\u76f8\u53cd\n                alphas[i] += labelMat[j]*labelMat[i]*(alphaJold - alphas[j])\n                # \u5728\u5bf9alpha[i], alpha[j] \u8fdb\u884c\u4f18\u5316\u4e4b\u540e\uff0c\u7ed9\u8fd9\u4e24\u4e2aalpha\u503c\u8bbe\u7f6e\u4e00\u4e2a\u5e38\u6570b\u3002\n                # w= \u03a3[1~n] ai*yi*xi => b = yj- \u03a3[1~n] ai*yi(xi*xj)\n                # \u6240\u4ee5:   b1 - b = (y1-y) - \u03a3[1~n] yi*(a1-a)*(xi*x1)\n                # \u4e3a\u4ec0\u4e48\u51cf2\u904d\uff1f \u56e0\u4e3a\u662f \u51cf\u53bb\u03a3[1~n]\uff0c\u6b63\u597d2\u4e2a\u53d8\u91cfi\u548cj\uff0c\u6240\u4ee5\u51cf2\u904d\n                b1 = b - Ei- labelMat[i]*(alphas[i]-alphaIold)*dataMatrix[i, :]*dataMatrix[i, :].T - labelMat[j]*(alphas[j]-alphaJold)*dataMatrix[i, :]*dataMatrix[j, :].T\n                b2 = b - Ej- labelMat[i]*(alphas[i]-alphaIold)*dataMatrix[i, :]*dataMatrix[j, :].T - labelMat[j]*(alphas[j]-alphaJold)*dataMatrix[j, :]*dataMatrix[j, :].T\n                if (0 < alphas[i]) and (C > alphas[i]):\n                    b = b1\n                elif (0 < alphas[j]) and (C > alphas[j]):\n                    b = b2\n                else:\n                    b = (b1 + b2)/2.0\n                alphaPairsChanged += 1\n                print(\"iter: %d i:%d, pairs changed %d\" % (iter, i, alphaPairsChanged))\n        # \u5728for\u5faa\u73af\u5916\uff0c\u68c0\u67e5alpha\u503c\u662f\u5426\u505a\u4e86\u66f4\u65b0\uff0c\u5982\u679c\u5728\u66f4\u65b0\u5219\u5c06iter\u8bbe\u4e3a0\u540e\u7ee7\u7eed\u8fd0\u884c\u7a0b\u5e8f\n        # \u77e5\u9053\u66f4\u65b0\u5b8c\u6bd5\u540e\uff0citer\u6b21\u5faa\u73af\u65e0\u53d8\u5316\uff0c\u624d\u63a8\u51fa\u5faa\u73af\u3002\n        if (alphaPairsChanged == 0):\n            iter += 1\n        else:\n            iter = 0\n        print(\"iteration number: %d\" % iter)\n    return b, alphas\n\n\ndef calcWs(alphas, dataArr, classLabels):\n    \"\"\"\n    \u57fa\u4e8ealpha\u8ba1\u7b97w\u503c\n    Args:\n        alphas        \u62c9\u683c\u6717\u65e5\u4e58\u5b50\n        dataArr       feature\u6570\u636e\u96c6\n        classLabels   \u76ee\u6807\u53d8\u91cf\u6570\u636e\u96c6\n\n    Returns:\n        wc  \u56de\u5f52\u7cfb\u6570\n    \"\"\"\n    X = mat(dataArr)\n    labelMat = mat(classLabels).transpose()\n    m, n = shape(X)\n    w = zeros((n, 1))\n    for i in range(m):\n        w += multiply(alphas[i] * labelMat[i], X[i, :].T)\n    return w\n\n\ndef plotfig_SVM(xMat, yMat, ws, b, alphas):\n    \"\"\"\n    \u53c2\u8003\u5730\u5740: \n       http://blog.csdn.net/maoersong/article/details/24315633\n       http://www.cnblogs.com/JustForCS/p/5283489.html\n       http://blog.csdn.net/kkxgx/article/details/6951959\n    \"\"\"\n\n    xMat = mat(xMat)\n    yMat = mat(yMat)\n\n    # b\u539f\u6765\u662f\u77e9\u9635\uff0c\u5148\u8f6c\u4e3a\u6570\u7ec4\u7c7b\u578b\u540e\u5176\u6570\u7ec4\u5927\u5c0f\u4e3a\uff081,1\uff09\uff0c\u6240\u4ee5\u540e\u9762\u52a0[0]\uff0c\u53d8\u4e3a(1,)\n    b = array(b)[0]\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n\n    # \u6ce8\u610fflatten\u7684\u7528\u6cd5\n    ax.scatter(xMat[:, 0].flatten().A[0], xMat[:, 1].flatten().A[0])\n\n    # x\u6700\u5927\u503c\uff0c\u6700\u5c0f\u503c\u6839\u636e\u539f\u6570\u636e\u96c6dataArr[:, 0]\u7684\u5927\u5c0f\u800c\u5b9a\n    x = arange(-1.0, 10.0, 0.1)\n\n    # \u6839\u636ex.w + b = 0 \u5f97\u5230\uff0c\u5176\u5f0f\u5b50\u5c55\u5f00\u4e3aw0.x1 + w1.x2 + b = 0, x2\u5c31\u662fy\u503c\n    y = (-b-ws[0, 0]*x)/ws[1, 0]\n    ax.plot(x, y)\n\n    for i in range(shape(yMat[0, :])[1]):\n        if yMat[0, i] > 0:\n            ax.plot(xMat[i, 0], xMat[i, 1], 'cx')\n        else:\n            ax.plot(xMat[i, 0], xMat[i, 1], 'kp')\n\n    # \u627e\u5230\u652f\u6301\u5411\u91cf\uff0c\u5e76\u5728\u56fe\u4e2d\u6807\u7ea2\n    for i in range(100):\n        if alphas[i] > 0.0:\n            ax.plot(xMat[i, 0], xMat[i, 1], 'ro')\n    plt.show()\n\n\nif __name__ == \"__main__\":\n    # \u83b7\u53d6\u7279\u5f81\u548c\u76ee\u6807\u53d8\u91cf\n    dataArr, labelArr = loadDataSet('data/6.SVM/testSet.txt')\n    # print(labelArr)\n\n    # b\u662f\u5e38\u91cf\u503c\uff0c alphas\u662f\u62c9\u683c\u6717\u65e5\u4e58\u5b50\n    b, alphas = smoSimple(dataArr, labelArr, 0.6, 0.001, 40)\n    print('/n/n/n')\n    print('b=', b)\n    print('alphas[alphas>0]=', alphas[alphas > 0])\n    print('shape(alphas[alphas > 0])=', shape(alphas[alphas > 0]))\n    for i in range(100):\n        if alphas[i] > 0:\n            print(dataArr[i], labelArr[i])\n    # \u753b\u56fe\n    ws = calcWs(alphas, dataArr, labelArr)\n    plotfig_SVM(dataArr, labelArr, ws, b, alphas)\n", "src/py3.x/ml/6.SVM/sklearn-svm-demo.py": "#!/usr/bin/python\n# -*- coding:utf-8 -*-\n\n\"\"\"\nCreated on 2017-06-28\nUpdated on 2017-06-28\nSVM: \u6700\u5927\u8fb9\u8ddd\u5206\u79bb\u8d85\u5e73\u9762\nAuthor: \u7247\u523b\nGitHub: https://github.com/apachecn/AiLearning\nsklearn-SVM\u8bd1\u6587\u94fe\u63a5: http://cwiki.apachecn.org/pages/viewpage.action?pageId=10031359\n\"\"\"\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn import svm\n\nprint(__doc__)\n\n\n# \u521b\u5efa40\u4e2a\u5206\u79bb\u70b9\nnp.random.seed(0)\n# X = np.r_[np.random.randn(20, 2) - [2, 2], np.random.randn(20, 2) + [2, 2]]\n# Y = [0] * 20 + [1] * 20\n\n\ndef loadDataSet(fileName):\n    \"\"\"\n    \u5bf9\u6587\u4ef6\u8fdb\u884c\u9010\u884c\u89e3\u6790\uff0c\u4ece\u800c\u5f97\u5230\u7b2c\u884c\u7684\u7c7b\u6807\u7b7e\u548c\u6574\u4e2a\u6570\u636e\u77e9\u9635\n    Args:\n        fileName \u6587\u4ef6\u540d\n    Returns:\n        dataMat  \u6570\u636e\u77e9\u9635\n        labelMat \u7c7b\u6807\u7b7e\n    \"\"\"\n    dataMat = []\n    labelMat = []\n    fr = open(fileName)\n    for line in fr.readlines():\n        lineArr = line.strip().split('\\t')\n        dataMat.append([float(lineArr[0]), float(lineArr[1])])\n        labelMat.append(float(lineArr[2]))\n    return dataMat, labelMat\n\n\nX, Y = loadDataSet('data/6.SVM/testSet.txt')\nX = np.mat(X)\n\nprint(\"X=\", X)\nprint(\"Y=\", Y)\n\n# \u62df\u5408\u4e00\u4e2aSVM\u6a21\u578b\nclf = svm.SVC(kernel='linear')\nclf.fit(X, Y)\n\n# \u83b7\u53d6\u5206\u5272\u8d85\u5e73\u9762\nw = clf.coef_[0]\n# \u659c\u7387\na = -w[0] / w[1]\n# \u4ece-5\u52305\uff0c\u987a\u5e8f\u95f4\u9694\u91c7\u683750\u4e2a\u6837\u672c\uff0c\u9ed8\u8ba4\u662fnum=50\n# xx = np.linspace(-5, 5)  # , num=50)\nxx = np.linspace(-2, 10)  # , num=50)\n# \u4e8c\u7ef4\u7684\u76f4\u7ebf\u65b9\u7a0b\nyy = a * xx - (clf.intercept_[0]) / w[1]\nprint(\"yy=\", yy)\n\n# plot the parallels to the separating hyperplane that pass through the support vectors\n# \u901a\u8fc7\u652f\u6301\u5411\u91cf\u7ed8\u5236\u5206\u5272\u8d85\u5e73\u9762\nprint(\"support_vectors_=\", clf.support_vectors_)\nb = clf.support_vectors_[0]\nyy_down = a * xx + (b[1] - a * b[0])\nb = clf.support_vectors_[-1]\nyy_up = a * xx + (b[1] - a * b[0])\n\n# plot the line, the points, and the nearest vectors to the plane\nplt.plot(xx, yy, 'k-')\nplt.plot(xx, yy_down, 'k--')\nplt.plot(xx, yy_up, 'k--')\n\nplt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=80, facecolors='none')\nplt.scatter(X[:, 0].flat, X[:, 1].flat, c=Y, cmap=plt.cm.Paired)\n\nplt.axis('tight')\nplt.show()\n", "src/py3.x/ml/6.SVM/svm-complete_Non-Kernel.py": "#!/usr/bin/python\n# -*- coding:utf-8 -*-\n\n\"\"\"\nCreated on Nov 4, 2010\nUpdate on 2017-05-18\nChapter 5 source file for Machine Learing in Action\nAuthor: Peter/geekidentity/\u7247\u523b\nGitHub: https://github.com/apachecn/AiLearning\n\"\"\"\nfrom numpy import *\nimport matplotlib.pyplot as plt\n\n\nclass optStruct:\n    def __init__(self, dataMatIn, classLabels, C, toler):  # Initialize the structure with the parameters\n        self.X = dataMatIn\n        self.labelMat = classLabels\n        self.C = C\n        self.tol = toler\n        self.m = shape(dataMatIn)[0]\n        self.alphas = mat(zeros((self.m, 1)))\n        self.b = 0\n        self.eCache = mat(zeros((self.m, 2)))  # first column is valid flag\n\n\ndef loadDataSet(fileName):\n    \"\"\"loadDataSet\uff08\u5bf9\u6587\u4ef6\u8fdb\u884c\u9010\u884c\u89e3\u6790\uff0c\u4ece\u800c\u5f97\u5230\u7b2c\u884c\u7684\u7c7b\u6807\u7b7e\u548c\u6574\u4e2a\u6570\u636e\u77e9\u9635\uff09\n\n    Args:\n        fileName \u6587\u4ef6\u540d\n    Returns:\n        dataMat  \u6570\u636e\u77e9\u9635\n        labelMat \u7c7b\u6807\u7b7e\n    \"\"\"\n    dataMat = []\n    labelMat = []\n    fr = open(fileName)\n    for line in fr.readlines():\n        lineArr = line.strip().split('\\t')\n        dataMat.append([float(lineArr[0]), float(lineArr[1])])\n        labelMat.append(float(lineArr[2]))\n    return dataMat, labelMat\n\n\ndef selectJrand(i, m):\n    \"\"\"\n    \u968f\u673a\u9009\u62e9\u4e00\u4e2a\u6574\u6570\n    Args:\n        i  \u7b2c\u4e00\u4e2aalpha\u7684\u4e0b\u6807\n        m  \u6240\u6709alpha\u7684\u6570\u76ee\n    Returns:\n        j  \u8fd4\u56de\u4e00\u4e2a\u4e0d\u4e3ai\u7684\u968f\u673a\u6570\uff0c\u57280~m\u4e4b\u95f4\u7684\u6574\u6570\u503c\n    \"\"\"\n    j = i\n    while j == i:\n        j = random.randint(0, m - 1)\n    return j\n\n\ndef clipAlpha(aj, H, L):\n    \"\"\"clipAlpha(\u8c03\u6574aj\u7684\u503c\uff0c\u4f7faj\u5904\u4e8e L<=aj<=H)\n    Args:\n        aj  \u76ee\u6807\u503c\n        H   \u6700\u5927\u503c\n        L   \u6700\u5c0f\u503c\n    Returns:\n        aj  \u76ee\u6807\u503c\n    \"\"\"\n    aj = min(aj, H)\n    aj = max(L, aj)\n    return aj\n\n\ndef calcEk(oS, k):\n    \"\"\"calcEk\uff08\u6c42 Ek\u8bef\u5dee: \u9884\u6d4b\u503c-\u771f\u5b9e\u503c\u7684\u5dee\uff09\n\n    \u8be5\u8fc7\u7a0b\u5728\u5b8c\u6574\u7248\u7684SMO\u7b97\u6cd5\u4e2d\u966a\u51fa\u73b0\u6b21\u6570\u8f83\u591a\uff0c\u56e0\u6b64\u5c06\u5176\u5355\u72ec\u4f5c\u4e3a\u4e00\u4e2a\u65b9\u6cd5\n    Args:\n        oS  optStruct\u5bf9\u8c61\n        k   \u5177\u4f53\u7684\u67d0\u4e00\u884c\n\n    Returns:\n        Ek  \u9884\u6d4b\u7ed3\u679c\u4e0e\u771f\u5b9e\u7ed3\u679c\u6bd4\u5bf9\uff0c\u8ba1\u7b97\u8bef\u5deeEk\n    \"\"\"\n    fXk = multiply(oS.alphas, oS.labelMat).T * (oS.X * oS.X[k].T) + oS.b\n    Ek = fXk - float(oS.labelMat[k])\n    return Ek\n\n\ndef selectJ(i, oS, Ei):  # this is the second choice -heurstic, and calcs Ej\n    \"\"\"selectJ\uff08\u8fd4\u56de\u6700\u4f18\u7684j\u548cEj\uff09\n\n    \u5185\u5faa\u73af\u7684\u542f\u53d1\u5f0f\u65b9\u6cd5\u3002\n    \u9009\u62e9\u7b2c\u4e8c\u4e2a(\u5185\u5faa\u73af)alpha\u7684alpha\u503c\n    \u8fd9\u91cc\u7684\u76ee\u6807\u662f\u9009\u62e9\u5408\u9002\u7684\u7b2c\u4e8c\u4e2aalpha\u503c\u4ee5\u4fdd\u8bc1\u6bcf\u6b21\u4f18\u5316\u4e2d\u91c7\u7528\u6700\u5927\u6b65\u957f\u3002\n    \u8be5\u51fd\u6570\u7684\u8bef\u5dee\u4e0e\u7b2c\u4e00\u4e2aalpha\u503cEi\u548c\u4e0b\u6807i\u6709\u5173\u3002\n    Args:\n        i   \u5177\u4f53\u7684\u7b2ci\u4e00\u884c\n        oS  optStruct\u5bf9\u8c61\n        Ei  \u9884\u6d4b\u7ed3\u679c\u4e0e\u771f\u5b9e\u7ed3\u679c\u6bd4\u5bf9\uff0c\u8ba1\u7b97\u8bef\u5deeEi\n\n    Returns:\n        j  \u968f\u673a\u9009\u51fa\u7684\u7b2cj\u4e00\u884c\n        Ej \u9884\u6d4b\u7ed3\u679c\u4e0e\u771f\u5b9e\u7ed3\u679c\u6bd4\u5bf9\uff0c\u8ba1\u7b97\u8bef\u5deeEj\n    \"\"\"\n    maxK = -1\n    maxDeltaE = 0\n    Ej = 0\n    # \u9996\u5148\u5c06\u8f93\u5165\u503cEi\u5728\u7f13\u5b58\u4e2d\u8bbe\u7f6e\u6210\u4e3a\u6709\u6548\u7684\u3002\u8fd9\u91cc\u7684\u6709\u6548\u610f\u5473\u7740\u5b83\u5df2\u7ecf\u8ba1\u7b97\u597d\u4e86\u3002\n    oS.eCache[i] = [1, Ei]\n\n    # print('oS.eCache[%s]=%s' % (i, oS.eCache[i]))\n    # print('oS.eCache[:, 0].A=%s' % oS.eCache[:, 0].A.T)\n    # \"\"\"\n    # # \u8fd4\u56de\u975e0\u7684: \u884c\u5217\u503c\n    # nonzero(oS.eCache[:, 0].A)= (\n    #     \u884c:  array([ 0,  2,  4,  5,  8, 10, 17, 18, 20, 21, 23, 25, 26, 29, 30, 39, 46,52, 54, 55, 62, 69, 70, 76, 79, 82, 94, 97]),\n    #     \u5217:  array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,0, 0, 0, 0, 0])\n    # )\n    # \"\"\"\n    # print('nonzero(oS.eCache[:, 0].A)=', nonzero(oS.eCache[:, 0].A))\n    # # \u53d6\u884c\u7684list\n    # print('nonzero(oS.eCache[:, 0].A)[0]=', nonzero(oS.eCache[:, 0].A)[0])\n    # \u975e\u96f6E\u503c\u7684\u884c\u7684list\u5217\u8868\uff0c\u6240\u5bf9\u5e94\u7684alpha\u503c\n    validEcacheList = nonzero(oS.eCache[:, 0].A)[0]\n    if (len(validEcacheList)) > 1:\n        for k in validEcacheList:  # \u5728\u6240\u6709\u7684\u503c\u4e0a\u8fdb\u884c\u5faa\u73af\uff0c\u5e76\u9009\u62e9\u5176\u4e2d\u4f7f\u5f97\u6539\u53d8\u6700\u5927\u7684\u90a3\u4e2a\u503c\n            if k == i:\n                continue  # don't calc for i, waste of time\n\n            # \u6c42 Ek\u8bef\u5dee: \u9884\u6d4b\u503c-\u771f\u5b9e\u503c\u7684\u5dee\n            Ek = calcEk(oS, k)\n            deltaE = abs(Ei - Ek)\n            if deltaE > maxDeltaE:\n                maxK = k\n                maxDeltaE = deltaE\n                Ej = Ek\n        return maxK, Ej\n    else:  # \u5982\u679c\u662f\u7b2c\u4e00\u6b21\u5faa\u73af\uff0c\u5219\u968f\u673a\u9009\u62e9\u4e00\u4e2aalpha\u503c\n        j = selectJrand(i, oS.m)\n\n        # \u6c42 Ek\u8bef\u5dee: \u9884\u6d4b\u503c-\u771f\u5b9e\u503c\u7684\u5dee\n        Ej = calcEk(oS, j)\n    return j, Ej\n\n\ndef updateEk(oS, k):  # after any alpha has changed update the new value in the cache\n    \"\"\"updateEk\uff08\u8ba1\u7b97\u8bef\u5dee\u503c\u5e76\u5b58\u5165\u7f13\u5b58\u4e2d\u3002\uff09\n\n    \u5728\u5bf9alpha\u503c\u8fdb\u884c\u4f18\u5316\u4e4b\u540e\u4f1a\u7528\u5230\u8fd9\u4e2a\u503c\u3002\n    Args:\n        oS  optStruct\u5bf9\u8c61\n        k   \u67d0\u4e00\u5217\u7684\u884c\u53f7\n    \"\"\"\n\n    # \u6c42 \u8bef\u5dee: \u9884\u6d4b\u503c-\u771f\u5b9e\u503c\u7684\u5dee\n    Ek = calcEk(oS, k)\n    oS.eCache[k] = [1, Ek]\n\n\ndef innerL(i, oS):\n    \"\"\"innerL\n    \u5185\u5faa\u73af\u4ee3\u7801\n    Args:\n        i   \u5177\u4f53\u7684\u67d0\u4e00\u884c\n        oS  optStruct\u5bf9\u8c61\n\n    Returns:\n        0   \u627e\u4e0d\u5230\u6700\u4f18\u7684\u503c\n        1   \u627e\u5230\u4e86\u6700\u4f18\u7684\u503c\uff0c\u5e76\u4e14oS.Cache\u5230\u7f13\u5b58\u4e2d\n    \"\"\"\n\n    # \u6c42 Ek\u8bef\u5dee: \u9884\u6d4b\u503c-\u771f\u5b9e\u503c\u7684\u5dee\n    Ei = calcEk(oS, i)\n\n    # \u7ea6\u675f\u6761\u4ef6 (KKT\u6761\u4ef6\u662f\u89e3\u51b3\u6700\u4f18\u5316\u95ee\u9898\u7684\u65f6\u7528\u5230\u7684\u4e00\u79cd\u65b9\u6cd5\u3002\u6211\u4eec\u8fd9\u91cc\u63d0\u5230\u7684\u6700\u4f18\u5316\u95ee\u9898\u901a\u5e38\u662f\u6307\u5bf9\u4e8e\u7ed9\u5b9a\u7684\u67d0\u4e00\u51fd\u6570\uff0c\u6c42\u5176\u5728\u6307\u5b9a\u4f5c\u7528\u57df\u4e0a\u7684\u5168\u5c40\u6700\u5c0f\u503c)\n    # 0<=alphas[i]<=C\uff0c\u4f46\u7531\u4e8e0\u548cC\u662f\u8fb9\u754c\u503c\uff0c\u6211\u4eec\u65e0\u6cd5\u8fdb\u884c\u4f18\u5316\uff0c\u56e0\u4e3a\u9700\u8981\u589e\u52a0\u4e00\u4e2aalphas\u548c\u964d\u4f4e\u4e00\u4e2aalphas\u3002\n    # \u8868\u793a\u53d1\u751f\u9519\u8bef\u7684\u6982\u7387: labelMat[i]*Ei \u5982\u679c\u8d85\u51fa\u4e86 toler\uff0c \u624d\u9700\u8981\u4f18\u5316\u3002\u81f3\u4e8e\u6b63\u8d1f\u53f7\uff0c\u6211\u4eec\u8003\u8651\u7edd\u5bf9\u503c\u5c31\u5bf9\u4e86\u3002\n    '''\n    # \u68c0\u9a8c\u8bad\u7ec3\u6837\u672c(xi, yi)\u662f\u5426\u6ee1\u8db3KKT\u6761\u4ef6\n    yi*f(i) >= 1 and alpha = 0 (outside the boundary)\n    yi*f(i) == 1 and 0<alpha< C (on the boundary)\n    yi*f(i) <= 1 and alpha = C (between the boundary)\n    '''\n    if ((oS.labelMat[i] * Ei < -oS.tol) and (oS.alphas[i] < oS.C)) or ((oS.labelMat[i] * Ei > oS.tol) and (oS.alphas[i] > 0)):\n        # \u9009\u62e9\u6700\u5927\u7684\u8bef\u5dee\u5bf9\u5e94\u7684j\u8fdb\u884c\u4f18\u5316\u3002\u6548\u679c\u66f4\u660e\u663e\n        j, Ej = selectJ(i, oS, Ei)\n        alphaIold = oS.alphas[i].copy()\n        alphaJold = oS.alphas[j].copy()\n\n        # L\u548cH\u7528\u4e8e\u5c06alphas[j]\u8c03\u6574\u52300-C\u4e4b\u95f4\u3002\u5982\u679cL==H\uff0c\u5c31\u4e0d\u505a\u4efb\u4f55\u6539\u53d8\uff0c\u76f4\u63a5return 0\n        if oS.labelMat[i] != oS.labelMat[j]:\n            L = max(0, oS.alphas[j] - oS.alphas[i])\n            H = min(oS.C, oS.C + oS.alphas[j] - oS.alphas[i])\n        else:\n            L = max(0, oS.alphas[j] + oS.alphas[i] - oS.C)\n            H = min(oS.C, oS.alphas[j] + oS.alphas[i])\n        if L == H:\n            print(\"L==H\")\n            return 0\n\n        # eta\u662falphas[j]\u7684\u6700\u4f18\u4fee\u6539\u91cf\uff0c\u5982\u679ceta==0\uff0c\u9700\u8981\u9000\u51fafor\u5faa\u73af\u7684\u5f53\u524d\u8fed\u4ee3\u8fc7\u7a0b\n        # \u53c2\u8003\u300a\u7edf\u8ba1\u5b66\u4e60\u65b9\u6cd5\u300b\u674e\u822a-P125~P128<\u5e8f\u5217\u6700\u5c0f\u6700\u4f18\u5316\u7b97\u6cd5>\n        eta = oS.X[i] - oS.X[j]\n        eta = - eta * eta.T\n        if eta >= 0:\n            print(\"eta>=0\")\n            return 0\n\n        # \u8ba1\u7b97\u51fa\u4e00\u4e2a\u65b0\u7684alphas[j]\u503c\n        oS.alphas[j] -= oS.labelMat[j] * (Ei - Ej) / eta\n        # \u5e76\u4f7f\u7528\u8f85\u52a9\u51fd\u6570\uff0c\u4ee5\u53caL\u548cH\u5bf9\u5176\u8fdb\u884c\u8c03\u6574\n        oS.alphas[j] = clipAlpha(oS.alphas[j], H, L)\n        # \u66f4\u65b0\u8bef\u5dee\u7f13\u5b58\n        updateEk(oS, j)\n\n        # \u68c0\u67e5alpha[j]\u662f\u5426\u53ea\u662f\u8f7b\u5fae\u7684\u6539\u53d8\uff0c\u5982\u679c\u662f\u7684\u8bdd\uff0c\u5c31\u9000\u51fafor\u5faa\u73af\u3002\n        if (abs(oS.alphas[j] - alphaJold) < 0.00001):\n            print(\"j not moving enough\")\n            return 0\n\n        # \u7136\u540ealphas[i]\u548calphas[j]\u540c\u6837\u8fdb\u884c\u6539\u53d8\uff0c\u867d\u7136\u6539\u53d8\u7684\u5927\u5c0f\u4e00\u6837\uff0c\u4f46\u662f\u6539\u53d8\u7684\u65b9\u5411\u6b63\u597d\u76f8\u53cd\n        oS.alphas[i] += oS.labelMat[j] * oS.labelMat[i] * (alphaJold - oS.alphas[j])\n        # \u66f4\u65b0\u8bef\u5dee\u7f13\u5b58\n        updateEk(oS, i)\n\n        # \u5728\u5bf9alpha[i], alpha[j] \u8fdb\u884c\u4f18\u5316\u4e4b\u540e\uff0c\u7ed9\u8fd9\u4e24\u4e2aalpha\u503c\u8bbe\u7f6e\u4e00\u4e2a\u5e38\u6570b\u3002\n        # w= \u03a3[1~n] ai*yi*xi => b = yj \u03a3[1~n] ai*yi(xi*xj)\n        # \u6240\u4ee5:   b1 - b = (y1-y) - \u03a3[1~n] yi*(a1-a)*(xi*x1)\n        # \u4e3a\u4ec0\u4e48\u51cf2\u904d\uff1f \u56e0\u4e3a\u662f \u51cf\u53bb\u03a3[1~n]\uff0c\u6b63\u597d2\u4e2a\u53d8\u91cfi\u548cj\uff0c\u6240\u4ee5\u51cf2\u904d\n        b1 = oS.b - Ei - oS.labelMat[i] * (oS.alphas[i] - alphaIold) * oS.X[i] * oS.X[i].T - oS.labelMat[j] * (oS.alphas[j] - alphaJold) * oS.X[i] * oS.X[j].T\n        b2 = oS.b - Ej - oS.labelMat[i] * (oS.alphas[i] - alphaIold) * oS.X[i] * oS.X[j].T - oS.labelMat[j] * (oS.alphas[j] - alphaJold) * oS.X[j] * oS.X[j].T\n        if (0 < oS.alphas[i]) and (oS.C > oS.alphas[i]):\n            oS.b = b1\n        elif (0 < oS.alphas[j]) and (oS.C > oS.alphas[j]):\n            oS.b = b2\n        else:\n            oS.b = (b1 + b2) / 2\n        return 1\n    else:\n        return 0\n\n\ndef smoP(dataMatIn, classLabels, C, toler, maxIter):\n    \"\"\"\n    \u5b8c\u6574SMO\u7b97\u6cd5\u5916\u5faa\u73af\uff0c\u4e0esmoSimple\u6709\u4e9b\u7c7b\u4f3c\uff0c\u4f46\u8fd9\u91cc\u7684\u5faa\u73af\u9000\u51fa\u6761\u4ef6\u66f4\u591a\u4e00\u4e9b\n    Args:\n        dataMatIn    \u6570\u636e\u96c6\n        classLabels  \u7c7b\u522b\u6807\u7b7e\n        C   \u677e\u5f1b\u53d8\u91cf(\u5e38\u91cf\u503c)\uff0c\u5141\u8bb8\u6709\u4e9b\u6570\u636e\u70b9\u53ef\u4ee5\u5904\u4e8e\u5206\u9694\u9762\u7684\u9519\u8bef\u4e00\u4fa7\u3002\n            \u63a7\u5236\u6700\u5927\u5316\u95f4\u9694\u548c\u4fdd\u8bc1\u5927\u90e8\u5206\u7684\u51fd\u6570\u95f4\u9694\u5c0f\u4e8e1.0\u8fd9\u4e24\u4e2a\u76ee\u6807\u7684\u6743\u91cd\u3002\n            \u53ef\u4ee5\u901a\u8fc7\u8c03\u8282\u8be5\u53c2\u6570\u8fbe\u5230\u4e0d\u540c\u7684\u7ed3\u679c\u3002\n        toler   \u5bb9\u9519\u7387\n        maxIter \u9000\u51fa\u524d\u6700\u5927\u7684\u5faa\u73af\u6b21\u6570\n    Returns:\n        b       \u6a21\u578b\u7684\u5e38\u91cf\u503c\n        alphas  \u62c9\u683c\u6717\u65e5\u4e58\u5b50\n    \"\"\"\n\n    # \u521b\u5efa\u4e00\u4e2a optStruct \u5bf9\u8c61\n    oS = optStruct(mat(dataMatIn), mat(classLabels).transpose(), C, toler)\n    iter = 0\n    entireSet = True\n    alphaPairsChanged = 0\n\n    # \u5faa\u73af\u904d\u5386: \u5faa\u73afmaxIter\u6b21 \u5e76\u4e14 \uff08alphaPairsChanged\u5b58\u5728\u53ef\u4ee5\u6539\u53d8 or \u6240\u6709\u884c\u904d\u5386\u4e00\u904d\uff09\n    # \u5faa\u73af\u8fed\u4ee3\u7ed3\u675f \u6216\u8005 \u5faa\u73af\u904d\u5386\u6240\u6709alpha\u540e\uff0calphaPairs\u8fd8\u662f\u6ca1\u53d8\u5316\n    while (iter < maxIter) and ((alphaPairsChanged > 0) or (entireSet)):\n        alphaPairsChanged = 0\n        # ----------- \u7b2c\u4e00\u79cd\u5199\u6cd5 start -------------------------\n        #  \u5f53entireSet=true or \u975e\u8fb9\u754calpha\u5bf9\u6ca1\u6709\u4e86\uff1b\u5c31\u5f00\u59cb\u5bfb\u627e alpha\u5bf9\uff0c\u7136\u540e\u51b3\u5b9a\u662f\u5426\u8981\u8fdb\u884celse\u3002\n        if entireSet:\n            # \u5728\u6570\u636e\u96c6\u4e0a\u904d\u5386\u6240\u6709\u53ef\u80fd\u7684alpha\n            for i in range(oS.m):\n                # \u662f\u5426\u5b58\u5728alpha\u5bf9\uff0c\u5b58\u5728\u5c31+1\n                alphaPairsChanged += innerL(i, oS)\n                print(\"fullSet, iter: %d i:%d, pairs changed %d\" % (iter, i, alphaPairsChanged))\n            iter += 1\n        # \u5bf9\u5df2\u5b58\u5728 alpha\u5bf9\uff0c\u9009\u51fa\u975e\u8fb9\u754c\u7684alpha\u503c\uff0c\u8fdb\u884c\u4f18\u5316\u3002\n        else:\n            # \u904d\u5386\u6240\u6709\u7684\u975e\u8fb9\u754calpha\u503c\uff0c\u4e5f\u5c31\u662f\u4e0d\u5728\u8fb9\u754c0\u6216C\u4e0a\u7684\u503c\u3002\n            nonBoundIs = nonzero((oS.alphas.A > 0) * (oS.alphas.A < C))[0]\n            for i in nonBoundIs:\n                alphaPairsChanged += innerL(i, oS)\n                print(\"non-bound, iter: %d i:%d, pairs changed %d\" % (iter, i, alphaPairsChanged))\n            iter += 1\n        # ----------- \u7b2c\u4e00\u79cd\u5199\u6cd5 end -------------------------\n\n        # ----------- \u7b2c\u4e8c\u79cd\u65b9\u6cd5 start -------------------------\n        # if entireSet:\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\u904d\u5386\u6574\u4e2a\u6570\u636e\u96c6\n    \t# \talphaPairsChanged += sum(innerL(i, oS) for i in range(oS.m))\n\t\t# else: \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t#\u904d\u5386\u975e\u8fb9\u754c\u503c\n\t\t# \tnonBoundIs = nonzero((oS.alphas.A > 0) * (oS.alphas.A < C))[0]\t\t\t\t\t\t#\u904d\u5386\u4e0d\u5728\u8fb9\u754c0\u548cC\u7684alpha\n\t\t# \talphaPairsChanged += sum(innerL(i, oS) for i in nonBoundIs)\n\t\t# iter += 1\n        # ----------- \u7b2c\u4e8c\u79cd\u65b9\u6cd5 end -------------------------\n        # \u5982\u679c\u627e\u5230alpha\u5bf9\uff0c\u5c31\u4f18\u5316\u975e\u8fb9\u754calpha\u503c\uff0c\u5426\u5219\uff0c\u5c31\u91cd\u65b0\u8fdb\u884c\u5bfb\u627e\uff0c\u5982\u679c\u5bfb\u627e\u4e00\u904d \u904d\u5386\u6240\u6709\u7684\u884c\u8fd8\u662f\u6ca1\u627e\u5230\uff0c\u5c31\u9000\u51fa\u5faa\u73af\u3002\n        if entireSet:\n            entireSet = False  # toggle entire set loop\n        elif alphaPairsChanged == 0:\n            entireSet = True\n        print(\"iteration number: %d\" % iter)\n    return oS.b, oS.alphas\n\n\ndef calcWs(alphas, dataArr, classLabels):\n    \"\"\"\n    \u57fa\u4e8ealpha\u8ba1\u7b97w\u503c\n    Args:\n        alphas        \u62c9\u683c\u6717\u65e5\u4e58\u5b50\n        dataArr       feature\u6570\u636e\u96c6\n        classLabels   \u76ee\u6807\u53d8\u91cf\u6570\u636e\u96c6\n\n    Returns:\n        wc  \u56de\u5f52\u7cfb\u6570\n    \"\"\"\n    X = mat(dataArr)\n    labelMat = mat(classLabels).T\n    m, n = shape(X)\n    w = zeros((n, 1))\n    for i in range(m):\n        w += multiply(alphas[i] * labelMat[i], X[i].T)\n    return w\n\n\ndef plotfig_SVM(xArr, yArr, ws, b, alphas):\n    \"\"\"\n    \u53c2\u8003\u5730\u5740: \n       http://blog.csdn.net/maoersong/article/details/24315633\n       http://www.cnblogs.com/JustForCS/p/5283489.html\n       http://blog.csdn.net/kkxgx/article/details/6951959\n    \"\"\"\n\n    xMat = mat(xArr)\n    yMat = mat(yArr)\n\n    # b\u539f\u6765\u662f\u77e9\u9635\uff0c\u5148\u8f6c\u4e3a\u6570\u7ec4\u7c7b\u578b\u540e\u5176\u6570\u7ec4\u5927\u5c0f\u4e3a\uff081,1\uff09\uff0c\u6240\u4ee5\u540e\u9762\u52a0[0]\uff0c\u53d8\u4e3a(1,)\n    b = array(b)[0]\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n\n    # \u6ce8\u610fflatten\u7684\u7528\u6cd5\n    ax.scatter(xMat[:, 0].flatten().A[0], xMat[:, 1].flatten().A[0])\n\n    # x\u6700\u5927\u503c\uff0c\u6700\u5c0f\u503c\u6839\u636e\u539f\u6570\u636e\u96c6dataArr[:, 0]\u7684\u5927\u5c0f\u800c\u5b9a\n    x = arange(-1.0, 10.0, 0.1)\n\n    # \u6839\u636ex.w + b = 0 \u5f97\u5230\uff0c\u5176\u5f0f\u5b50\u5c55\u5f00\u4e3aw0.x1 + w1.x2 + b = 0, x2\u5c31\u662fy\u503c\n    y = (- b - ws[0, 0] * x) / ws[1, 0]\n    ax.plot(x, y)\n\n    for i in range(shape(yMat[0])[1]):\n        if yMat[0, i] > 0:\n            ax.plot(xMat[i, 0], xMat[i, 1], 'cx')\n        else:\n            ax.plot(xMat[i, 0], xMat[i, 1], 'kp')\n\n    # \u627e\u5230\u652f\u6301\u5411\u91cf\uff0c\u5e76\u5728\u56fe\u4e2d\u6807\u7ea2\n    for i in range(100):\n        if alphas[i] > 0.0:\n            ax.plot(xMat[i, 0], xMat[i, 1], 'ro')\n    plt.show()\n\n\nif __name__ == \"__main__\":\n    # \u83b7\u53d6\u7279\u5f81\u548c\u76ee\u6807\u53d8\u91cf\n    dataArr, labelArr = loadDataSet('data/6.SVM/testSet.txt')\n    # print(labelArr)\n\n    # b\u662f\u5e38\u91cf\u503c\uff0c alphas\u662f\u62c9\u683c\u6717\u65e5\u4e58\u5b50\n    b, alphas = smoP(dataArr, labelArr, 0.6, 0.001, 40)\n    print('/n/n/n')\n    print('b=', b)\n    print('alphas[alphas>0]=', alphas[alphas > 0])\n    print('shape(alphas[alphas > 0])=', shape(alphas[alphas > 0]))\n    for i in range(100):\n        if alphas[i] > 0:\n            print(dataArr[i], labelArr[i])\n    # \u753b\u56fe\n    ws = calcWs(alphas, dataArr, labelArr)\n    plotfig_SVM(dataArr, labelArr, ws, b, alphas)\n", "src/py3.x/ml/3.DecisionTree/DecisionTree.py": "#!/usr/bin/python\n# -*- coding: UTF-8 -*-\n\n'''\nCreated on Oct 12, 2010\nUpdate on 2017-05-18\nDecision Tree Source Code for Machine Learning in Action Ch. 3\nAuthor: Peter Harrington/\u7247\u523b\nGitHub: https://github.com/apachecn/AiLearning\n'''\nprint(__doc__)\nimport operator\nfrom math import log\nimport decisionTreePlot as dtPlot\nfrom collections import Counter\n\n\ndef createDataSet():\n    \"\"\"\n    Desc:\n        \u521b\u5efa\u6570\u636e\u96c6\n    Args:\n        \u65e0\u9700\u4f20\u5165\u53c2\u6570\n    Returns:\n        \u8fd4\u56de\u6570\u636e\u96c6\u548c\u5bf9\u5e94\u7684label\u6807\u7b7e\n    \"\"\"\n    # dataSet \u524d\u4e24\u5217\u662f\u7279\u5f81\uff0c\u6700\u540e\u4e00\u5217\u5bf9\u5e94\u7684\u662f\u6bcf\u6761\u6570\u636e\u5bf9\u5e94\u7684\u5206\u7c7b\u6807\u7b7e\n    dataSet = [[1, 1, 'yes'],\n               [1, 1, 'yes'],\n               [1, 0, 'no'],\n               [0, 1, 'no'],\n               [0, 1, 'no']]\n    # dataSet = [['yes'],\n    #         ['yes'],\n    #         ['no'],\n    #         ['no'],\n    #         ['no']]\n    # labels  \u9732\u51fa\u6c34\u9762   \u811a\u8e7c\uff0c\u6ce8\u610f: \u8fd9\u91cc\u7684labels\u662f\u5199\u7684 dataSet \u4e2d\u7279\u5f81\u7684\u542b\u4e49\uff0c\u5e76\u4e0d\u662f\u5bf9\u5e94\u7684\u5206\u7c7b\u6807\u7b7e\u6216\u8005\u8bf4\u76ee\u6807\u53d8\u91cf\n    labels = ['no surfacing', 'flippers']\n    # \u8fd4\u56de\n    return dataSet, labels\n\n\ndef calcShannonEnt(dataSet):\n    \"\"\"\n    Desc: \n        calculate Shannon entropy -- \u8ba1\u7b97\u7ed9\u5b9a\u6570\u636e\u96c6\u7684\u9999\u519c\u71b5\n    Args:\n        dataSet -- \u6570\u636e\u96c6\n    Returns:\n        shannonEnt -- \u8fd4\u56de \u6bcf\u4e00\u7ec4 feature \u4e0b\u7684\u67d0\u4e2a\u5206\u7c7b\u4e0b\uff0c\u9999\u519c\u71b5\u7684\u4fe1\u606f\u671f\u671b\n    \"\"\"\n    # -----------\u8ba1\u7b97\u9999\u519c\u71b5\u7684\u7b2c\u4e00\u79cd\u5b9e\u73b0\u65b9\u5f0fstart--------------------------------------------------------------------------------\n    # \u6c42list\u7684\u957f\u5ea6\uff0c\u8868\u793a\u8ba1\u7b97\u53c2\u4e0e\u8bad\u7ec3\u7684\u6570\u636e\u91cf\n    numEntries = len(dataSet)\n    # \u4e0b\u9762\u8f93\u51fa\u6211\u4eec\u6d4b\u8bd5\u7684\u6570\u636e\u96c6\u7684\u4e00\u4e9b\u4fe1\u606f\n    # \u4f8b\u5982: <type 'list'> numEntries:  5 \u662f\u4e0b\u9762\u7684\u4ee3\u7801\u7684\u8f93\u51fa\n    # print(type(dataSet), 'numEntries: ', numEntries)\n\n    # \u8ba1\u7b97\u5206\u7c7b\u6807\u7b7elabel\u51fa\u73b0\u7684\u6b21\u6570\n    labelCounts = {}\n    # the the number of unique elements and their occurance\n    for featVec in dataSet:\n        # \u5c06\u5f53\u524d\u5b9e\u4f8b\u7684\u6807\u7b7e\u5b58\u50a8\uff0c\u5373\u6bcf\u4e00\u884c\u6570\u636e\u7684\u6700\u540e\u4e00\u4e2a\u6570\u636e\u4ee3\u8868\u7684\u662f\u6807\u7b7e\n        currentLabel = featVec[-1]\n        # \u4e3a\u6240\u6709\u53ef\u80fd\u7684\u5206\u7c7b\u521b\u5efa\u5b57\u5178\uff0c\u5982\u679c\u5f53\u524d\u7684\u952e\u503c\u4e0d\u5b58\u5728\uff0c\u5219\u6269\u5c55\u5b57\u5178\u5e76\u5c06\u5f53\u524d\u952e\u503c\u52a0\u5165\u5b57\u5178\u3002\u6bcf\u4e2a\u952e\u503c\u90fd\u8bb0\u5f55\u4e86\u5f53\u524d\u7c7b\u522b\u51fa\u73b0\u7684\u6b21\u6570\u3002\n        if currentLabel not in labelCounts.keys():\n            labelCounts[currentLabel] = 0\n        labelCounts[currentLabel] += 1\n        # print('-----', featVec, labelCounts)\n\n    # \u5bf9\u4e8elabel\u6807\u7b7e\u7684\u5360\u6bd4\uff0c\u6c42\u51falabel\u6807\u7b7e\u7684\u9999\u519c\u71b5\n    shannonEnt = 0.0\n    for key in labelCounts:\n        # \u4f7f\u7528\u6240\u6709\u7c7b\u6807\u7b7e\u7684\u53d1\u751f\u9891\u7387\u8ba1\u7b97\u7c7b\u522b\u51fa\u73b0\u7684\u6982\u7387\u3002\n        prob = float(labelCounts[key])/numEntries\n        # log base 2\n        # \u8ba1\u7b97\u9999\u519c\u71b5\uff0c\u4ee5 2 \u4e3a\u5e95\u6c42\u5bf9\u6570\n        shannonEnt -= prob * log(prob, 2)\n        # print('---', prob, prob * log(prob, 2), shannonEnt)\n    # -----------\u8ba1\u7b97\u9999\u519c\u71b5\u7684\u7b2c\u4e00\u79cd\u5b9e\u73b0\u65b9\u5f0fend--------------------------------------------------------------------------------\n\n    # # -----------\u8ba1\u7b97\u9999\u519c\u71b5\u7684\u7b2c\u4e8c\u79cd\u5b9e\u73b0\u65b9\u5f0fstart--------------------------------------------------------------------------------\n    # # \u7edf\u8ba1\u6807\u7b7e\u51fa\u73b0\u7684\u6b21\u6570\n    # label_count = Counter(data[-1] for data in dataSet)\n    # # \u8ba1\u7b97\u6982\u7387\n    # probs = [p[1] / len(dataSet) for p in label_count.items()]\n    # # \u8ba1\u7b97\u9999\u519c\u71b5\n    # shannonEnt = sum([-p * log(p, 2) for p in probs])\n    # # -----------\u8ba1\u7b97\u9999\u519c\u71b5\u7684\u7b2c\u4e8c\u79cd\u5b9e\u73b0\u65b9\u5f0fend--------------------------------------------------------------------------------\n    return shannonEnt\n\n\ndef splitDataSet(dataSet, index, value):\n    \"\"\"\n    Desc: \n        \u5212\u5206\u6570\u636e\u96c6\n        splitDataSet(\u901a\u8fc7\u904d\u5386dataSet\u6570\u636e\u96c6\uff0c\u6c42\u51faindex\u5bf9\u5e94\u7684colnum\u5217\u7684\u503c\u4e3avalue\u7684\u884c)\n        \u5c31\u662f\u4f9d\u636eindex\u5217\u8fdb\u884c\u5206\u7c7b\uff0c\u5982\u679cindex\u5217\u7684\u6570\u636e\u7b49\u4e8e value\u7684\u65f6\u5019\uff0c\u5c31\u8981\u5c06 index \u5212\u5206\u5230\u6211\u4eec\u521b\u5efa\u7684\u65b0\u7684\u6570\u636e\u96c6\u4e2d\n    Args:\n        dataSet  -- \u6570\u636e\u96c6                 \u5f85\u5212\u5206\u7684\u6570\u636e\u96c6\n        index -- \u8868\u793a\u6bcf\u4e00\u884c\u7684index\u5217        \u5212\u5206\u6570\u636e\u96c6\u7684\u7279\u5f81\n        value -- \u8868\u793aindex\u5217\u5bf9\u5e94\u7684value\u503c   \u9700\u8981\u8fd4\u56de\u7684\u7279\u5f81\u7684\u503c\u3002\n    Returns:\n        index \u5217\u4e3a value \u7684\u6570\u636e\u96c6\u3010\u8be5\u6570\u636e\u96c6\u9700\u8981\u6392\u9664index\u5217\u3011\n    \"\"\"\n    # -----------\u5207\u5206\u6570\u636e\u96c6\u7684\u7b2c\u4e00\u79cd\u65b9\u5f0f start------------------------------------\n    retDataSet = []\n    for featVec in dataSet: \n        # index\u5217\u4e3avalue\u7684\u6570\u636e\u96c6\u3010\u8be5\u6570\u636e\u96c6\u9700\u8981\u6392\u9664index\u5217\u3011\n        # \u5224\u65adindex\u5217\u7684\u503c\u662f\u5426\u4e3avalue\n        if featVec[index] == value:\n            # chop out index used for splitting\n            # [:index]\u8868\u793a\u524dindex\u884c\uff0c\u5373\u82e5 index \u4e3a2\uff0c\u5c31\u662f\u53d6 featVec \u7684\u524d index \u884c\n            reducedFeatVec = featVec[:index]\n            '''\n            \u8bf7\u767e\u5ea6\u67e5\u8be2\u4e00\u4e0b:  extend\u548cappend\u7684\u533a\u522b\n            list.append(object) \u5411\u5217\u8868\u4e2d\u6dfb\u52a0\u4e00\u4e2a\u5bf9\u8c61object\n            list.extend(sequence) \u628a\u4e00\u4e2a\u5e8f\u5217seq\u7684\u5185\u5bb9\u6dfb\u52a0\u5230\u5217\u8868\u4e2d\n            1\u3001\u4f7f\u7528append\u7684\u65f6\u5019\uff0c\u662f\u5c06new_media\u770b\u4f5c\u4e00\u4e2a\u5bf9\u8c61\uff0c\u6574\u4f53\u6253\u5305\u6dfb\u52a0\u5230music_media\u5bf9\u8c61\u4e2d\u3002\n            2\u3001\u4f7f\u7528extend\u7684\u65f6\u5019\uff0c\u662f\u5c06new_media\u770b\u4f5c\u4e00\u4e2a\u5e8f\u5217\uff0c\u5c06\u8fd9\u4e2a\u5e8f\u5217\u548cmusic_media\u5e8f\u5217\u5408\u5e76\uff0c\u5e76\u653e\u5728\u5176\u540e\u9762\u3002\n            result = []\n            result.extend([1,2,3])\n            print(result)\n            result.append([4,5,6])\n            print(result)\n            result.extend([7,8,9])\n            print(result)\n            \u7ed3\u679c: \n            [1, 2, 3]\n            [1, 2, 3, [4, 5, 6]]\n            [1, 2, 3, [4, 5, 6], 7, 8, 9]\n            '''\n            reducedFeatVec.extend(featVec[index+1:])\n            # [index+1:]\u8868\u793a\u4ece\u8df3\u8fc7 index \u7684 index+1\u884c\uff0c\u53d6\u63a5\u4e0b\u6765\u7684\u6570\u636e\n            # \u6536\u96c6\u7ed3\u679c\u503c index\u5217\u4e3avalue\u7684\u884c\u3010\u8be5\u884c\u9700\u8981\u6392\u9664index\u5217\u3011\n            retDataSet.append(reducedFeatVec)\n    # -----------\u5207\u5206\u6570\u636e\u96c6\u7684\u7b2c\u4e00\u79cd\u65b9\u5f0f end------------------------------------\n\n    # # -----------\u5207\u5206\u6570\u636e\u96c6\u7684\u7b2c\u4e8c\u79cd\u65b9\u5f0f start------------------------------------\n    # retDataSet = [data[:index] + data[index + 1:] for data in dataSet for i, v in enumerate(data) if i == index and v == value]\n    # # -----------\u5207\u5206\u6570\u636e\u96c6\u7684\u7b2c\u4e8c\u79cd\u65b9\u5f0f end------------------------------------\n    return retDataSet\n\n\ndef chooseBestFeatureToSplit(dataSet):\n    \"\"\"\n    Desc:\n        \u9009\u62e9\u5207\u5206\u6570\u636e\u96c6\u7684\u6700\u4f73\u7279\u5f81\n    Args:\n        dataSet -- \u9700\u8981\u5207\u5206\u7684\u6570\u636e\u96c6\n    Returns:\n        bestFeature -- \u5207\u5206\u6570\u636e\u96c6\u7684\u6700\u4f18\u7684\u7279\u5f81\u5217\n    \"\"\"\n\n    # -----------\u9009\u62e9\u6700\u4f18\u7279\u5f81\u7684\u7b2c\u4e00\u79cd\u65b9\u5f0f start------------------------------------\n    # \u6c42\u7b2c\u4e00\u884c\u6709\u591a\u5c11\u5217\u7684 Feature, \u6700\u540e\u4e00\u5217\u662flabel\u5217\u561b\n    numFeatures = len(dataSet[0]) - 1\n    # label\u7684\u4fe1\u606f\u71b5\n    baseEntropy = calcShannonEnt(dataSet)\n    # \u6700\u4f18\u7684\u4fe1\u606f\u589e\u76ca\u503c, \u548c\u6700\u4f18\u7684Featurn\u7f16\u53f7\n    bestInfoGain, bestFeature = 0.0, -1\n    # iterate over all the features\n    for i in range(numFeatures):\n        # create a list of all the examples of this feature\n        # \u83b7\u53d6\u6bcf\u4e00\u4e2a\u5b9e\u4f8b\u7684\u7b2ci+1\u4e2afeature\uff0c\u7ec4\u6210list\u96c6\u5408\n        featList = [example[i] for example in dataSet]\n        # get a set of unique values\n        # \u83b7\u53d6\u5254\u91cd\u540e\u7684\u96c6\u5408\uff0c\u4f7f\u7528set\u5bf9list\u6570\u636e\u8fdb\u884c\u53bb\u91cd\n        uniqueVals = set(featList)\n        # \u521b\u5efa\u4e00\u4e2a\u4e34\u65f6\u7684\u4fe1\u606f\u71b5\n        newEntropy = 0.0\n        # \u904d\u5386\u67d0\u4e00\u5217\u7684value\u96c6\u5408\uff0c\u8ba1\u7b97\u8be5\u5217\u7684\u4fe1\u606f\u71b5 \n        # \u904d\u5386\u5f53\u524d\u7279\u5f81\u4e2d\u7684\u6240\u6709\u552f\u4e00\u5c5e\u6027\u503c\uff0c\u5bf9\u6bcf\u4e2a\u552f\u4e00\u5c5e\u6027\u503c\u5212\u5206\u4e00\u6b21\u6570\u636e\u96c6\uff0c\u8ba1\u7b97\u6570\u636e\u96c6\u7684\u65b0\u71b5\u503c\uff0c\u5e76\u5bf9\u6240\u6709\u552f\u4e00\u7279\u5f81\u503c\u5f97\u5230\u7684\u71b5\u6c42\u548c\u3002\n        for value in uniqueVals:\n            subDataSet = splitDataSet(dataSet, i, value)\n            prob = len(subDataSet)/float(len(dataSet))\n            newEntropy += prob * calcShannonEnt(subDataSet)\n        # gain[\u4fe1\u606f\u589e\u76ca]: \u5212\u5206\u6570\u636e\u96c6\u524d\u540e\u7684\u4fe1\u606f\u53d8\u5316\uff0c \u83b7\u53d6\u4fe1\u606f\u71b5\u6700\u5927\u7684\u503c\n        # \u4fe1\u606f\u589e\u76ca\u662f\u71b5\u7684\u51cf\u5c11\u6216\u8005\u662f\u6570\u636e\u65e0\u5e8f\u5ea6\u7684\u51cf\u5c11\u3002\u6700\u540e\uff0c\u6bd4\u8f83\u6240\u6709\u7279\u5f81\u4e2d\u7684\u4fe1\u606f\u589e\u76ca\uff0c\u8fd4\u56de\u6700\u597d\u7279\u5f81\u5212\u5206\u7684\u7d22\u5f15\u503c\u3002\n        infoGain = baseEntropy - newEntropy\n        print('infoGain=', infoGain, 'bestFeature=', i, baseEntropy, newEntropy)\n        if (infoGain > bestInfoGain):\n            bestInfoGain = infoGain\n            bestFeature = i\n    return bestFeature\n    # -----------\u9009\u62e9\u6700\u4f18\u7279\u5f81\u7684\u7b2c\u4e00\u79cd\u65b9\u5f0f end------------------------------------\n\n    # # -----------\u9009\u62e9\u6700\u4f18\u7279\u5f81\u7684\u7b2c\u4e8c\u79cd\u65b9\u5f0f start------------------------------------\n    # # \u8ba1\u7b97\u521d\u59cb\u9999\u519c\u71b5\n    # base_entropy = calcShannonEnt(dataSet)\n    # best_info_gain = 0\n    # best_feature = -1\n    # # \u904d\u5386\u6bcf\u4e00\u4e2a\u7279\u5f81\n    # for i in range(len(dataSet[0]) - 1):\n    #     # \u5bf9\u5f53\u524d\u7279\u5f81\u8fdb\u884c\u7edf\u8ba1\n    #     feature_count = Counter([data[i] for data in dataSet])\n    #     # \u8ba1\u7b97\u5206\u5272\u540e\u7684\u9999\u519c\u71b5\n    #     new_entropy = sum(feature[1] / float(len(dataSet)) * calcShannonEnt(splitDataSet(dataSet, i, feature[0])) \\\n    #                    for feature in feature_count.items())\n    #     # \u66f4\u65b0\u503c\n    #     info_gain = base_entropy - new_entropy\n    #     print('No. {0} feature info gain is {1:.3f}'.format(i, info_gain))\n    #     if info_gain > best_info_gain:\n    #         best_info_gain = info_gain\n    #         best_feature = i\n    # return best_feature\n    # # -----------\u9009\u62e9\u6700\u4f18\u7279\u5f81\u7684\u7b2c\u4e8c\u79cd\u65b9\u5f0f end------------------------------------\n\n\ndef majorityCnt(classList):\n    \"\"\"\n    Desc:\n        \u9009\u62e9\u51fa\u73b0\u6b21\u6570\u6700\u591a\u7684\u4e00\u4e2a\u7ed3\u679c\n    Args:\n        classList label\u5217\u7684\u96c6\u5408\n    Returns:\n        bestFeature \u6700\u4f18\u7684\u7279\u5f81\u5217\n    \"\"\"\n    # -----------majorityCnt\u7684\u7b2c\u4e00\u79cd\u65b9\u5f0f start------------------------------------\n    classCount = {}\n    for vote in classList:\n        if vote not in classCount.keys():\n            classCount[vote] = 0\n        classCount[vote] += 1\n    # \u5012\u53d9\u6392\u5217classCount\u5f97\u5230\u4e00\u4e2a\u5b57\u5178\u96c6\u5408\uff0c\u7136\u540e\u53d6\u51fa\u7b2c\u4e00\u4e2a\u5c31\u662f\u7ed3\u679c\uff08yes/no\uff09\uff0c\u5373\u51fa\u73b0\u6b21\u6570\u6700\u591a\u7684\u7ed3\u679c\n    sortedClassCount = sorted(classCount.items(), key=operator.itemgetter(1), reverse=True)\n    # print('sortedClassCount:', sortedClassCount)\n    return sortedClassCount[0][0]\n    # -----------majorityCnt\u7684\u7b2c\u4e00\u79cd\u65b9\u5f0f end------------------------------------\n\n    # # -----------majorityCnt\u7684\u7b2c\u4e8c\u79cd\u65b9\u5f0f start------------------------------------\n    # major_label = Counter(classList).most_common(1)[0]\n    # return major_label\n    # # -----------majorityCnt\u7684\u7b2c\u4e8c\u79cd\u65b9\u5f0f end------------------------------------\n\n\ndef createTree(dataSet, labels):\n    \"\"\"\n    Desc:\n        \u521b\u5efa\u51b3\u7b56\u6811\n    Args:\n        dataSet -- \u8981\u521b\u5efa\u51b3\u7b56\u6811\u7684\u8bad\u7ec3\u6570\u636e\u96c6\n        labels -- \u8bad\u7ec3\u6570\u636e\u96c6\u4e2d\u7279\u5f81\u5bf9\u5e94\u7684\u542b\u4e49\u7684labels\uff0c\u4e0d\u662f\u76ee\u6807\u53d8\u91cf\n    Returns:\n        myTree -- \u521b\u5efa\u5b8c\u6210\u7684\u51b3\u7b56\u6811\n    \"\"\"\n    classList = [example[-1] for example in dataSet]\n    # \u5982\u679c\u6570\u636e\u96c6\u7684\u6700\u540e\u4e00\u5217\u7684\u7b2c\u4e00\u4e2a\u503c\u51fa\u73b0\u7684\u6b21\u6570=\u6574\u4e2a\u96c6\u5408\u7684\u6570\u91cf\uff0c\u4e5f\u5c31\u8bf4\u53ea\u6709\u4e00\u4e2a\u7c7b\u522b\uff0c\u5c31\u53ea\u76f4\u63a5\u8fd4\u56de\u7ed3\u679c\u5c31\u884c\n    # \u7b2c\u4e00\u4e2a\u505c\u6b62\u6761\u4ef6: \u6240\u6709\u7684\u7c7b\u6807\u7b7e\u5b8c\u5168\u76f8\u540c\uff0c\u5219\u76f4\u63a5\u8fd4\u56de\u8be5\u7c7b\u6807\u7b7e\u3002\n    # count() \u51fd\u6570\u662f\u7edf\u8ba1\u62ec\u53f7\u4e2d\u7684\u503c\u5728list\u4e2d\u51fa\u73b0\u7684\u6b21\u6570\n    if classList.count(classList[0]) == len(classList):\n        return classList[0]\n    # \u5982\u679c\u6570\u636e\u96c6\u53ea\u67091\u5217\uff0c\u90a3\u4e48\u6700\u521d\u51fa\u73b0label\u6b21\u6570\u6700\u591a\u7684\u4e00\u7c7b\uff0c\u4f5c\u4e3a\u7ed3\u679c\n    # \u7b2c\u4e8c\u4e2a\u505c\u6b62\u6761\u4ef6: \u4f7f\u7528\u5b8c\u4e86\u6240\u6709\u7279\u5f81\uff0c\u4ecd\u7136\u4e0d\u80fd\u5c06\u6570\u636e\u96c6\u5212\u5206\u6210\u4ec5\u5305\u542b\u552f\u4e00\u7c7b\u522b\u7684\u5206\u7ec4\u3002\n    if len(dataSet[0]) == 1:\n        return majorityCnt(classList)\n\n    # \u9009\u62e9\u6700\u4f18\u7684\u5217\uff0c\u5f97\u5230\u6700\u4f18\u5217\u5bf9\u5e94\u7684label\u542b\u4e49\n    bestFeat = chooseBestFeatureToSplit(dataSet)\n    # \u83b7\u53d6label\u7684\u540d\u79f0\n    bestFeatLabel = labels[bestFeat]\n    # \u521d\u59cb\u5316myTree\n    myTree = {bestFeatLabel: {}}\n    # \u6ce8: labels\u5217\u8868\u662f\u53ef\u53d8\u5bf9\u8c61\uff0c\u5728PYTHON\u51fd\u6570\u4e2d\u4f5c\u4e3a\u53c2\u6570\u65f6\u4f20\u5740\u5f15\u7528\uff0c\u80fd\u591f\u88ab\u5168\u5c40\u4fee\u6539\n    # \u6240\u4ee5\u8fd9\u884c\u4ee3\u7801\u5bfc\u81f4\u51fd\u6570\u5916\u7684\u540c\u540d\u53d8\u91cf\u88ab\u5220\u9664\u4e86\u5143\u7d20\uff0c\u9020\u6210\u4f8b\u53e5\u65e0\u6cd5\u6267\u884c\uff0c\u63d0\u793a'no surfacing' is not in list\n    del(labels[bestFeat])\n    # \u53d6\u51fa\u6700\u4f18\u5217\uff0c\u7136\u540e\u5b83\u7684branch\u505a\u5206\u7c7b\n    featValues = [example[bestFeat] for example in dataSet]\n    uniqueVals = set(featValues)\n    for value in uniqueVals:\n        # \u6c42\u51fa\u5269\u4f59\u7684\u6807\u7b7elabel\n        subLabels = labels[:]\n        # \u904d\u5386\u5f53\u524d\u9009\u62e9\u7279\u5f81\u5305\u542b\u7684\u6240\u6709\u5c5e\u6027\u503c\uff0c\u5728\u6bcf\u4e2a\u6570\u636e\u96c6\u5212\u5206\u4e0a\u9012\u5f52\u8c03\u7528\u51fd\u6570createTree()\n        myTree[bestFeatLabel][value] = createTree(splitDataSet(dataSet, bestFeat, value), subLabels)\n        # print('myTree', value, myTree)\n    return myTree\n\n\ndef classify(inputTree, featLabels, testVec):\n    \"\"\"\n    Desc:\n        \u5bf9\u65b0\u6570\u636e\u8fdb\u884c\u5206\u7c7b\n    Args:\n        inputTree  -- \u5df2\u7ecf\u8bad\u7ec3\u597d\u7684\u51b3\u7b56\u6811\u6a21\u578b\n        featLabels -- Feature\u6807\u7b7e\u5bf9\u5e94\u7684\u540d\u79f0\uff0c\u4e0d\u662f\u76ee\u6807\u53d8\u91cf\n        testVec    -- \u6d4b\u8bd5\u8f93\u5165\u7684\u6570\u636e\n    Returns:\n        classLabel -- \u5206\u7c7b\u7684\u7ed3\u679c\u503c\uff0c\u9700\u8981\u6620\u5c04label\u624d\u80fd\u77e5\u9053\u540d\u79f0\n    \"\"\"\n    # \u83b7\u53d6tree\u7684\u6839\u8282\u70b9\u5bf9\u4e8e\u7684key\u503c\n    firstStr = list(inputTree.keys())[0]\n    # \u901a\u8fc7key\u5f97\u5230\u6839\u8282\u70b9\u5bf9\u5e94\u7684value\n    secondDict = inputTree[firstStr]\n    # \u5224\u65ad\u6839\u8282\u70b9\u540d\u79f0\u83b7\u53d6\u6839\u8282\u70b9\u5728label\u4e2d\u7684\u5148\u540e\u987a\u5e8f\uff0c\u8fd9\u6837\u5c31\u77e5\u9053\u8f93\u5165\u7684testVec\u600e\u4e48\u5f00\u59cb\u5bf9\u7167\u6811\u6765\u505a\u5206\u7c7b\n    featIndex = featLabels.index(firstStr)\n    # \u6d4b\u8bd5\u6570\u636e\uff0c\u627e\u5230\u6839\u8282\u70b9\u5bf9\u5e94\u7684label\u4f4d\u7f6e\uff0c\u4e5f\u5c31\u77e5\u9053\u4ece\u8f93\u5165\u7684\u6570\u636e\u7684\u7b2c\u51e0\u4f4d\u6765\u5f00\u59cb\u5206\u7c7b\n    key = testVec[featIndex]\n    valueOfFeat = secondDict[key]\n    print('+++', firstStr, 'xxx', secondDict, '---', key, '>>>', valueOfFeat)\n    # \u5224\u65ad\u5206\u679d\u662f\u5426\u7ed3\u675f: \u5224\u65advalueOfFeat\u662f\u5426\u662fdict\u7c7b\u578b\n    if isinstance(valueOfFeat, dict):\n        classLabel = classify(valueOfFeat, featLabels, testVec)\n    else:\n        classLabel = valueOfFeat\n    return classLabel\n\n\ndef storeTree(inputTree, filename):\n    \"\"\"\n    Desc:\n        \u5c06\u4e4b\u524d\u8bad\u7ec3\u597d\u7684\u51b3\u7b56\u6811\u6a21\u578b\u5b58\u50a8\u8d77\u6765\uff0c\u4f7f\u7528 pickle \u6a21\u5757\n    Args:\n        inputTree -- \u4ee5\u524d\u8bad\u7ec3\u597d\u7684\u51b3\u7b56\u6811\u6a21\u578b\n        filename -- \u8981\u5b58\u50a8\u7684\u540d\u79f0\n    Returns:\n        None\n    \"\"\"\n    import pickle\n    # -------------- \u7b2c\u4e00\u79cd\u65b9\u6cd5 start --------------\n    fw = open(filename, 'wb')\n    pickle.dump(inputTree, fw)\n    fw.close()\n    # -------------- \u7b2c\u4e00\u79cd\u65b9\u6cd5 end --------------\n\n    # -------------- \u7b2c\u4e8c\u79cd\u65b9\u6cd5 start --------------\n    with open(filename, 'wb') as fw:\n        pickle.dump(inputTree, fw)\n    # -------------- \u7b2c\u4e8c\u79cd\u65b9\u6cd5 start --------------\n\n\ndef grabTree(filename):\n    \"\"\"\n    Desc:\n        \u5c06\u4e4b\u524d\u5b58\u50a8\u7684\u51b3\u7b56\u6811\u6a21\u578b\u4f7f\u7528 pickle \u6a21\u5757 \u8fd8\u539f\u51fa\u6765\n    Args:\n        filename -- \u4e4b\u524d\u5b58\u50a8\u51b3\u7b56\u6811\u6a21\u578b\u7684\u6587\u4ef6\u540d\n    Returns:\n        pickle.load(fr) -- \u5c06\u4e4b\u524d\u5b58\u50a8\u7684\u51b3\u7b56\u6811\u6a21\u578b\u8fd8\u539f\u51fa\u6765\n    \"\"\"\n    import pickle\n    fr = open(filename, 'rb')\n    return pickle.load(fr)\n\n\ndef fishTest():\n    \"\"\"\n    Desc:\n        \u5bf9\u52a8\u7269\u662f\u5426\u662f\u9c7c\u7c7b\u5206\u7c7b\u7684\u6d4b\u8bd5\u51fd\u6570\uff0c\u5e76\u5c06\u7ed3\u679c\u4f7f\u7528 matplotlib \u753b\u51fa\u6765\n    Args:\n        None\n    Returns:\n        None\n    \"\"\"\n    # 1.\u521b\u5efa\u6570\u636e\u548c\u7ed3\u679c\u6807\u7b7e\n    myDat, labels = createDataSet()\n    # print(myDat, labels)\n\n    # \u8ba1\u7b97label\u5206\u7c7b\u6807\u7b7e\u7684\u9999\u519c\u71b5\n    # calcShannonEnt(myDat)\n\n    # # \u6c42\u7b2c0\u5217 \u4e3a 1/0\u7684\u5217\u7684\u6570\u636e\u96c6\u3010\u6392\u9664\u7b2c0\u5217\u3011\n    # print('1---', splitDataSet(myDat, 0, 1))\n    # print('0---', splitDataSet(myDat, 0, 0))\n\n    # # \u8ba1\u7b97\u6700\u597d\u7684\u4fe1\u606f\u589e\u76ca\u7684\u5217\n    # print(chooseBestFeatureToSplit(myDat))\n\n    import copy\n    myTree = createTree(myDat, copy.deepcopy(labels))\n    print(myTree)\n    # [1, 1]\u8868\u793a\u8981\u53d6\u7684\u5206\u652f\u4e0a\u7684\u8282\u70b9\u4f4d\u7f6e\uff0c\u5bf9\u5e94\u7684\u7ed3\u679c\u503c\n    print(classify(myTree, labels, [1, 1]))\n\n    # \u753b\u56fe\u53ef\u89c6\u5316\u5c55\u73b0\n    dtPlot.createPlot(myTree)\n\n\ndef ContactLensesTest():\n    \"\"\"\n    Desc:\n        \u9884\u6d4b\u9690\u5f62\u773c\u955c\u7684\u6d4b\u8bd5\u4ee3\u7801\uff0c\u5e76\u5c06\u7ed3\u679c\u753b\u51fa\u6765\n    Args:\n        none\n    Returns:\n        none\n    \"\"\"\n\n    # \u52a0\u8f7d\u9690\u5f62\u773c\u955c\u76f8\u5173\u7684 \u6587\u672c\u6587\u4ef6 \u6570\u636e\n    fr = open('data/3.DecisionTree/lenses.txt')\n    # \u89e3\u6790\u6570\u636e\uff0c\u83b7\u5f97 features \u6570\u636e\n    lenses = [inst.strip().split('\\t') for inst in fr.readlines()]\n    # \u5f97\u5230\u6570\u636e\u7684\u5bf9\u5e94\u7684 Labels\n    lensesLabels = ['age', 'prescript', 'astigmatic', 'tearRate']\n    # \u4f7f\u7528\u4e0a\u9762\u7684\u521b\u5efa\u51b3\u7b56\u6811\u7684\u4ee3\u7801\uff0c\u6784\u9020\u9884\u6d4b\u9690\u5f62\u773c\u955c\u7684\u51b3\u7b56\u6811\n    lensesTree = createTree(lenses, lensesLabels)\n    print(lensesTree)\n    # \u753b\u56fe\u53ef\u89c6\u5316\u5c55\u73b0\n    dtPlot.createPlot(lensesTree)\n\n\nif __name__ == \"__main__\":\n    # fishTest()\n    ContactLensesTest()\n", "src/py3.x/ml/3.DecisionTree/DTSklearn.py": "#!/usr/bin/python\n# -*- coding: UTF-8 -*-\n# \u539f\u59cb\u94fe\u63a5:  http://blog.csdn.net/lsldd/article/details/41223147\n# GitHub: https://github.com/apachecn/AiLearning\nimport numpy as np\nfrom sklearn import tree\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import train_test_split\n\n\ndef createDataSet():\n    ''' \u6570\u636e\u8bfb\u5165 '''\n    data = []\n    labels = []\n    with open(\"data/3.DecisionTree/data.txt\") as ifile:\n        for line in ifile:\n            # \u7279\u5f81:  \u8eab\u9ad8 \u4f53\u91cd   label:  \u80d6\u7626\n            tokens = line.strip().split(' ')\n            data.append([float(tk) for tk in tokens[:-1]])\n            labels.append(tokens[-1])\n    # \u7279\u5f81\u6570\u636e\n    x = np.array(data)\n    # label\u5206\u7c7b\u7684\u6807\u7b7e\u6570\u636e\n    labels = np.array(labels)\n    # \u9884\u4f30\u7ed3\u679c\u7684\u6807\u7b7e\u6570\u636e\n    y = np.zeros(labels.shape)\n\n    ''' \u6807\u7b7e\u8f6c\u6362\u4e3a0/1 '''\n    y[labels == 'fat'] = 1\n    print(data, '-------', x, '-------', labels, '-------', y)\n    return x, y\n\n\ndef predict_train(x_train, y_train):\n    '''\n    \u4f7f\u7528\u4fe1\u606f\u71b5\u4f5c\u4e3a\u5212\u5206\u6807\u51c6\uff0c\u5bf9\u51b3\u7b56\u6811\u8fdb\u884c\u8bad\u7ec3\n    \u53c2\u8003\u94fe\u63a5:  http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier\n    '''\n    clf = tree.DecisionTreeClassifier(criterion='entropy')\n    # print(clf)\n    clf.fit(x_train, y_train)\n    ''' \u7cfb\u6570\u53cd\u6620\u6bcf\u4e2a\u7279\u5f81\u7684\u5f71\u54cd\u529b\u3002\u8d8a\u5927\u8868\u793a\u8be5\u7279\u5f81\u5728\u5206\u7c7b\u4e2d\u8d77\u5230\u7684\u4f5c\u7528\u8d8a\u5927 '''\n    print('feature_importances_: %s' % clf.feature_importances_)\n\n    '''\u6d4b\u8bd5\u7ed3\u679c\u7684\u6253\u5370'''\n    y_pre = clf.predict(x_train)\n    # print(x_train)\n    print(y_pre)\n    print(y_train)\n    print(np.mean(y_pre == y_train))\n    return y_pre, clf\n\n\ndef show_precision_recall(x, y, clf,  y_train, y_pre):\n    '''\n    \u51c6\u786e\u7387\u4e0e\u53ec\u56de\u7387\n    \u53c2\u8003\u94fe\u63a5:  http://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_curve.html#sklearn.metrics.precision_recall_curve\n    '''\n    precision, recall, thresholds = precision_recall_curve(y_train, y_pre)\n    # \u8ba1\u7b97\u5168\u91cf\u7684\u9884\u4f30\u7ed3\u679c\n    answer = clf.predict_proba(x)[:, 1]\n\n    '''\n    \u5c55\u73b0 \u51c6\u786e\u7387\u4e0e\u53ec\u56de\u7387\n        precision \u51c6\u786e\u7387\n        recall \u53ec\u56de\u7387\n        f1-score  \u51c6\u786e\u7387\u548c\u53ec\u56de\u7387\u7684\u4e00\u4e2a\u7efc\u5408\u5f97\u5206\n        support \u53c2\u4e0e\u6bd4\u8f83\u7684\u6570\u91cf\n    \u53c2\u8003\u94fe\u63a5: http://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html#sklearn.metrics.classification_report\n    '''\n    # target_names \u4ee5 y\u7684label\u5206\u7c7b\u4e3a\u51c6\n    target_names = ['thin', 'fat']\n    print(classification_report(y, answer, target_names=target_names))\n    print(answer)\n    print(y)\n\n\ndef show_pdf(clf):\n    '''\n    \u53ef\u89c6\u5316\u8f93\u51fa\n    \u628a\u51b3\u7b56\u6811\u7ed3\u6784\u5199\u5165\u6587\u4ef6: http://sklearn.lzjqsdd.com/modules/tree.html\n\n    Mac\u62a5\u9519: pydotplus.graphviz.InvocationException: GraphViz's executables not found\n    \u89e3\u51b3\u65b9\u6848: sudo brew install graphviz\n    \u53c2\u8003\u5199\u5165:  http://www.jianshu.com/p/59b510bafb4d\n    '''\n    # with open(\"testResult/tree.dot\", 'w') as f:\n    #     from sklearn.externals.six import StringIO\n    #     tree.export_graphviz(clf, out_file=f)\n\n    import pydotplus\n    from sklearn.externals.six import StringIO\n    dot_data = StringIO()\n    tree.export_graphviz(clf, out_file=dot_data)\n    graph = pydotplus.graph_from_dot_data(dot_data.getvalue())\n    graph.write_pdf(\"../../../output/3.DecisionTree/tree.pdf\")\n\n    # from IPython.display import Image\n    # Image(graph.create_png())\n\n\nif __name__ == '__main__':\n    x, y = createDataSet()\n\n    ''' \u62c6\u5206\u8bad\u7ec3\u6570\u636e\u4e0e\u6d4b\u8bd5\u6570\u636e\uff0c 80%\u505a\u8bad\u7ec3 20%\u505a\u6d4b\u8bd5 '''\n    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\n    print('\u62c6\u5206\u6570\u636e: ', x_train, x_test, y_train, y_test)\n\n    # \u5f97\u5230\u8bad\u7ec3\u7684\u9884\u6d4b\u7ed3\u679c\u96c6\n    y_pre, clf = predict_train(x_train, y_train)\n\n    # \u5c55\u73b0 \u51c6\u786e\u7387\u4e0e\u53ec\u56de\u7387\n    show_precision_recall(x, y, clf, y_train, y_pre)\n\n    # \u53ef\u89c6\u5316\u8f93\u51fa\n    show_pdf(clf)\n", "src/py3.x/ml/3.DecisionTree/decisionTreePlot.py": "#!/usr/bin/python\n# -*- coding: UTF-8 -*-\n\n'''\nCreated on Oct 14, 2010\nUpdate on 2017-02-27\nDecision Tree Source Code for Machine Learning in Action Ch. 3\nAuthor: Peter Harrington/jiangzhonglian\n'''\nimport matplotlib.pyplot as plt\n\n# \u5b9a\u4e49\u6587\u672c\u6846 \u548c \u7bad\u5934\u683c\u5f0f \u3010 sawtooth \u6ce2\u6d6a\u65b9\u6846, round4 \u77e9\u5f62\u65b9\u6846 , fc\u8868\u793a\u5b57\u4f53\u989c\u8272\u7684\u6df1\u6d45 0.1~0.9 \u4f9d\u6b21\u53d8\u6d45\uff0c\u6ca1\u9519\u662f\u53d8\u6d45\u3011\ndecisionNode = dict(boxstyle=\"sawtooth\", fc=\"0.8\")\nleafNode = dict(boxstyle=\"round4\", fc=\"0.8\")\narrow_args = dict(arrowstyle=\"<-\")\n\n\ndef getNumLeafs(myTree):\n    numLeafs = 0\n    firstStr = list(myTree.keys())[0]\n    secondDict = myTree[firstStr]\n    # \u6839\u8282\u70b9\u5f00\u59cb\u904d\u5386\n    for key in secondDict.keys():\n        # \u5224\u65ad\u5b50\u8282\u70b9\u662f\u5426\u4e3adict, \u4e0d\u662f+1\n        if type(secondDict[key]) is dict:\n            numLeafs += getNumLeafs(secondDict[key])\n        else:\n            numLeafs += 1\n    return numLeafs\n\n\ndef getTreeDepth(myTree):\n    maxDepth = 0\n    firstStr = list(myTree.keys())[0]\n    secondDict = myTree[firstStr]\n    # \u6839\u8282\u70b9\u5f00\u59cb\u904d\u5386\n    for key in secondDict.keys():\n        # \u5224\u65ad\u5b50\u8282\u70b9\u662f\u4e0d\u662fdict, \u6c42\u5206\u679d\u7684\u6df1\u5ea6\n        # ----------\u5199\u6cd51 start ---------------\n        if type(secondDict[key]) is dict:\n            thisDepth = 1 + getTreeDepth(secondDict[key])\n        else:\n            thisDepth = 1\n        # ----------\u5199\u6cd51 end ---------------\n\n        # ----------\u5199\u6cd52 start --------------\n        # thisDepth = 1 + getTreeDepth(secondDict[key]) if type(secondDict[key]) is dict else 1\n        # ----------\u5199\u6cd52 end --------------\n        # \u8bb0\u5f55\u6700\u5927\u7684\u5206\u652f\u6df1\u5ea6\n        maxDepth = max(maxDepth, thisDepth)\n    return maxDepth\n\n\ndef plotNode(nodeTxt, centerPt, parentPt, nodeType):\n    createPlot.ax1.annotate(nodeTxt, xy=parentPt,  xycoords='axes fraction', xytext=centerPt, textcoords='axes fraction', va=\"center\", ha=\"center\", bbox=nodeType, arrowprops=arrow_args)\n\n\ndef plotMidText(cntrPt, parentPt, txtString):\n    xMid = (parentPt[0] - cntrPt[0]) / 2 + cntrPt[0]\n    yMid = (parentPt[1] - cntrPt[1]) / 2 + cntrPt[1]\n    createPlot.ax1.text(xMid, yMid, txtString, va=\"center\", ha=\"center\", rotation=30)\n\n\ndef plotTree(myTree, parentPt, nodeTxt):\n    # \u83b7\u53d6\u53f6\u5b50\u8282\u70b9\u7684\u6570\u91cf\n    numLeafs = getNumLeafs(myTree)\n    # \u83b7\u53d6\u6811\u7684\u6df1\u5ea6\n    # depth = getTreeDepth(myTree)\n\n    # \u627e\u51fa\u7b2c1\u4e2a\u4e2d\u5fc3\u70b9\u7684\u4f4d\u7f6e\uff0c\u7136\u540e\u4e0e parentPt\u5b9a\u70b9\u8fdb\u884c\u5212\u7ebf\n    cntrPt = (plotTree.xOff + (1 + numLeafs) / 2 / plotTree.totalW, plotTree.yOff)\n    # print(cntrPt)\n    # \u5e76\u6253\u5370\u8f93\u5165\u5bf9\u5e94\u7684\u6587\u5b57\n    plotMidText(cntrPt, parentPt, nodeTxt)\n\n    firstStr = list(myTree.keys())[0]\n    # \u53ef\u89c6\u5316Node\u5206\u652f\u70b9\n    plotNode(firstStr, cntrPt, parentPt, decisionNode)\n    # \u6839\u8282\u70b9\u7684\u503c\n    secondDict = myTree[firstStr]\n    # y\u503c = \u6700\u9ad8\u70b9-\u5c42\u6570\u7684\u9ad8\u5ea6[\u7b2c\u4e8c\u4e2a\u8282\u70b9\u4f4d\u7f6e]\n    plotTree.yOff = plotTree.yOff - 1 / plotTree.totalD\n    for key in secondDict.keys():\n        # \u5224\u65ad\u8be5\u8282\u70b9\u662f\u5426\u662fNode\u8282\u70b9\n        if type(secondDict[key]) is dict:\n            # \u5982\u679c\u662f\u5c31\u9012\u5f52\u8c03\u7528[recursion]\n            plotTree(secondDict[key], cntrPt, str(key))\n        else:\n            # \u5982\u679c\u4e0d\u662f\uff0c\u5c31\u5728\u539f\u6765\u8282\u70b9\u4e00\u534a\u7684\u5730\u65b9\u627e\u5230\u8282\u70b9\u7684\u5750\u6807\n            plotTree.xOff = plotTree.xOff + 1 / plotTree.totalW\n            # \u53ef\u89c6\u5316\u8be5\u8282\u70b9\u4f4d\u7f6e\n            plotNode(secondDict[key], (plotTree.xOff, plotTree.yOff), cntrPt, leafNode)\n            # \u5e76\u6253\u5370\u8f93\u5165\u5bf9\u5e94\u7684\u6587\u5b57\n            plotMidText((plotTree.xOff, plotTree.yOff), cntrPt, str(key))\n    plotTree.yOff = plotTree.yOff + 1 / plotTree.totalD\n\n\ndef createPlot(inTree):\n    # \u521b\u5efa\u4e00\u4e2afigure\u7684\u6a21\u7248\n    fig = plt.figure(1, facecolor='green')\n    fig.clf()\n\n    axprops = dict(xticks=[], yticks=[])\n    # \u8868\u793a\u521b\u5efa\u4e00\u4e2a1\u884c\uff0c1\u5217\u7684\u56fe\uff0ccreatePlot.ax1 \u4e3a\u7b2c 1 \u4e2a\u5b50\u56fe\uff0c\n    createPlot.ax1 = plt.subplot(111, frameon=False, **axprops)\n\n    plotTree.totalW = float(getNumLeafs(inTree))\n    plotTree.totalD = float(getTreeDepth(inTree))\n    # \u534a\u4e2a\u8282\u70b9\u7684\u957f\u5ea6\n    plotTree.xOff = -0.5 / plotTree.totalW\n    plotTree.yOff = 1.0\n    plotTree(inTree, (0.5, 1.0), '')\n    plt.show()\n\n\n# # \u6d4b\u8bd5\u753b\u56fe\n# def createPlot():\n#     fig = plt.figure(1, facecolor='white')\n#     fig.clf()\n#     # ticks for demo purposes\n#     createPlot.ax1 = plt.subplot(111, frameon=False)\n#     plotNode('a decision node', (0.5, 0.1), (0.1, 0.5), decisionNode)\n#     plotNode('a leaf node', (0.8, 0.1), (0.3, 0.8), leafNode)\n#     plt.show()\n\n\n# \u6d4b\u8bd5\u6570\u636e\u96c6\ndef retrieveTree(i):\n    listOfTrees = [\n        {'no surfacing': {0: 'no', 1: {'flippers': {0: 'no', 1: 'yes'}}}},\n        {'no surfacing': {0: 'no', 1: {'flippers': {0: {'head': {0: 'no', 1: 'yes'}}, 1: 'no'}}}}\n    ]\n    return listOfTrees[i]\n\n\n# myTree = retrieveTree(1)\n# createPlot(myTree)\n", "src/py3.x/ml/3.DecisionTree/sklearn_dts_classify_demo.py": "#!/usr/bin/python\n# -*- coding: UTF-8 -*-\n\n\"\"\"\nCreated on 2017-06-29\nUpdated on 2017-06-29\nDecisionTree: \u51b3\u7b56\u6811\nAuthor: \u5c0f\u7476\nGitHub: https://github.com/apachecn/AiLearning\n\"\"\"\nprint(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.datasets import load_iris\nfrom sklearn.tree import DecisionTreeClassifier\n\n# \u53c2\u6570\nn_classes = 3\nplot_colors = \"bry\"\nplot_step = 0.02\n\n# \u52a0\u8f7d\u6570\u636e\niris = load_iris()\n\nfor pairidx, pair in enumerate([[0, 1], [0, 2], [0, 3], [1, 2], [1, 3], [2, 3]]):\n    # \u6211\u4eec\u53ea\u7528\u4e24\u4e2a\u76f8\u5e94\u7684features\n    X = iris.data[:, pair]\n    y = iris.target\n\n    # \u8bad\u7ec3\n    clf = DecisionTreeClassifier().fit(X, y)\n\n    # \u7ed8\u5236\u51b3\u7b56\u8fb9\u754c\n    plt.subplot(2, 3, pairidx + 1)\n\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),\n                         np.arange(y_min, y_max, plot_step))\n\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n    Z = Z.reshape(xx.shape)\n    cs = plt.contourf(xx, yy, Z, cmap=plt.cm.Paired)\n\n    plt.xlabel(iris.feature_names[pair[0]])\n    plt.ylabel(iris.feature_names[pair[1]])\n    plt.axis(\"tight\")\n\n    # \u7ed8\u5236\u8bad\u7ec3\u70b9\n    for i, color in zip(range(n_classes), plot_colors):\n        idx = np.where(y == i)\n        plt.scatter(X[idx, 0], X[idx, 1], c=color, label=iris.target_names[i],\n                    cmap=plt.cm.Paired)\n\n    plt.axis(\"tight\")\n\nplt.suptitle(\"Decision surface of a decision tree using paired features\")\nplt.legend()\nplt.show()\n", "src/py3.x/ml/3.DecisionTree/skelearn_dts_regressor_demo.py": "#!/usr/bin/python\n# -*- coding: UTF-8 -*-\n\n\"\"\"\nCreated on 2017-06-29\nUpdated on 2017-06-29\nDecisionTree: \u51b3\u7b56\u6811\nAuthor: \u5c0f\u7476\nGitHub: https://github.com/apachecn/AiLearning\n\"\"\"\n\nprint(__doc__)\n\n# \u5f15\u5165\u5fc5\u8981\u7684\u6a21\u578b\u548c\u5e93\nimport numpy as np\nfrom sklearn.tree import DecisionTreeRegressor\nimport matplotlib.pyplot as plt\n\n# \u521b\u5efa\u4e00\u4e2a\u968f\u673a\u7684\u6570\u636e\u96c6\n# \u53c2\u8003 https://docs.scipy.org/doc/numpy-1.6.0/reference/generated/numpy.random.mtrand.RandomState.html\nrng = np.random.RandomState(1)\n# print('lalalalala===', rng)\n# rand() \u662f\u7ed9\u5b9a\u5f62\u72b6\u7684\u968f\u673a\u503c\uff0crng.rand(80, 1)\u5373\u77e9\u9635\u7684\u5f62\u72b6\u662f 80\u884c\uff0c1\u5217\n# sort() \nX = np.sort(5 * rng.rand(80, 1), axis=0)\n# print('X=', X)\ny = np.sin(X).ravel()\n# print('y=', y)\ny[::5] += 3 * (0.5 - rng.rand(16))\n# print('yyy=', y)\n\n# \u62df\u5408\u56de\u5f52\u6a21\u578b\n# regr_1 = DecisionTreeRegressor(max_depth=2)\n# \u4fdd\u6301 max_depth=5 \u4e0d\u53d8\uff0c\u589e\u52a0 min_samples_leaf=6 \u7684\u53c2\u6570\uff0c\u6548\u679c\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\nregr_2 = DecisionTreeRegressor(max_depth=5)\nregr_2 = DecisionTreeRegressor(min_samples_leaf=6)\n# regr_3 = DecisionTreeRegressor(max_depth=4)\n# regr_1.fit(X, y)\nregr_2.fit(X, y)\n# regr_3.fit(X, y)\n\n# \u9884\u6d4b\nX_test = np.arange(0.0, 5.0, 0.01)[:, np.newaxis]\n# y_1 = regr_1.predict(X_test)\ny_2 = regr_2.predict(X_test)\n# y_3 = regr_3.predict(X_test)\n\n# \u7ed8\u5236\u7ed3\u679c\nplt.figure()\nplt.scatter(X, y, c=\"darkorange\", label=\"data\")\n# plt.plot(X_test, y_1, color=\"cornflowerblue\", label=\"max_depth=2\", linewidth=2)\nplt.plot(X_test, y_2, color=\"yellowgreen\", label=\"max_depth=5\", linewidth=2)\n# plt.plot(X_test, y_3, color=\"red\", label=\"max_depth=3\", linewidth=2)\nplt.xlabel(\"data\")\nplt.ylabel(\"target\")\nplt.title(\"Decision Tree Regression\")\nplt.legend()\nplt.show()\n", "src/py3.x/ml/12.FrequentPattemTree/fpGrowth.py": "#!/usr/bin/python\n# coding:utf8\n\n'''\nCreated on Jun 14, 2011\nUpdate  on 2017-05-18\nFP-Growth FP means frequent pattern\nthe FP-Growth algorithm needs:\n1. FP-tree (class treeNode)\n2. header table (use dict)\nThis finds frequent itemsets similar to apriori but does not find association rules.\nAuthor: Peter/\u7247\u523b\nGitHub: https://github.com/apachecn/AiLearning\n'''\nprint(__doc__)\n\n\nclass treeNode:\n    def __init__(self, nameValue, numOccur, parentNode):\n        self.name = nameValue\n        self.count = numOccur\n        self.nodeLink = None\n        # needs to be updated\n        self.parent = parentNode\n        self.children = {}\n\n    def inc(self, numOccur):\n        \"\"\"inc(\u5bf9count\u53d8\u91cf\u589e\u52a0\u7ed9\u5b9a\u503c)\n        \"\"\"\n        self.count += numOccur\n\n    def disp(self, ind=1):\n        \"\"\"disp(\u7528\u4e8e\u5c06\u6811\u4ee5\u6587\u672c\u5f62\u5f0f\u663e\u793a)\n\n        \"\"\"\n        print('  '*ind, self.name, ' ', self.count)\n        for child in self.children.values():\n            child.disp(ind+1)\n\n\ndef loadSimpDat():\n    simpDat = [['r', 'z', 'h', 'j', 'p'],\n               ['z', 'y', 'x', 'w', 'v', 'u', 't', 's'],\n               ['z'],\n               ['r', 'x', 'n', 'o', 's'],\n            #    ['r', 'x', 'n', 'o', 's'],\n               ['y', 'r', 'x', 'z', 'q', 't', 'p'],\n               ['y', 'z', 'x', 'e', 'q', 's', 't', 'm']]\n    return simpDat\n\n\ndef createInitSet(dataSet):\n    retDict = {}\n    for trans in dataSet:\n        if frozenset(trans) not in retDict.keys():\n            retDict[frozenset(trans)] = 1\n        else:\n            retDict[frozenset(trans)] += 1\n    return retDict\n\n\n# this version does not use recursion\ndef updateHeader(nodeToTest, targetNode):\n    \"\"\"updateHeader(\u66f4\u65b0\u5934\u6307\u9488\uff0c\u5efa\u7acb\u76f8\u540c\u5143\u7d20\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u4f8b\u5982:  \u5de6\u8fb9\u7684r\u6307\u5411\u53f3\u8fb9\u7684r\u503c\uff0c\u5c31\u662f\u540e\u51fa\u73b0\u7684\u76f8\u540c\u5143\u7d20 \u6307\u5411 \u5df2\u7ecf\u51fa\u73b0\u7684\u5143\u7d20)\n\n    \u4ece\u5934\u6307\u9488\u7684nodeLink\u5f00\u59cb\uff0c\u4e00\u76f4\u6cbf\u7740nodeLink\u76f4\u5230\u5230\u8fbe\u94fe\u8868\u672b\u5c3e\u3002\u8fd9\u5c31\u662f\u94fe\u8868\u3002\n    \u6027\u80fd: \u5982\u679c\u94fe\u8868\u5f88\u957f\u53ef\u80fd\u4f1a\u9047\u5230\u8fed\u4ee3\u8c03\u7528\u7684\u6b21\u6570\u9650\u5236\u3002\n\n    Args:\n        nodeToTest  \u6ee1\u8db3minSup {\u6240\u6709\u7684\u5143\u7d20+(value, treeNode)}\n        targetNode  Tree\u5bf9\u8c61\u7684\u5b50\u8282\u70b9\n    \"\"\"\n    # \u5efa\u7acb\u76f8\u540c\u5143\u7d20\u4e4b\u95f4\u7684\u5173\u7cfb\uff0c\u4f8b\u5982:  \u5de6\u8fb9\u7684r\u6307\u5411\u53f3\u8fb9\u7684r\u503c\n    while (nodeToTest.nodeLink is not None):\n        nodeToTest = nodeToTest.nodeLink\n    nodeToTest.nodeLink = targetNode\n\n\ndef updateTree(items, inTree, headerTable, count):\n    \"\"\"updateTree(\u66f4\u65b0FP-tree\uff0c\u7b2c\u4e8c\u6b21\u904d\u5386)\n\n    # \u9488\u5bf9\u6bcf\u4e00\u884c\u7684\u6570\u636e\n    # \u6700\u5927\u7684key,  \u6dfb\u52a0\n    Args:\n        items       \u6ee1\u8db3minSup \u6392\u5e8f\u540e\u7684\u5143\u7d20key\u7684\u6570\u7ec4\uff08\u5927\u5230\u5c0f\u7684\u6392\u5e8f\uff09\n        inTree      \u7a7a\u7684Tree\u5bf9\u8c61\n        headerTable \u6ee1\u8db3minSup {\u6240\u6709\u7684\u5143\u7d20+(value, treeNode)}\n        count       \u539f\u6570\u636e\u96c6\u4e2d\u6bcf\u4e00\u7ec4Kay\u51fa\u73b0\u7684\u6b21\u6570\n    \"\"\"\n    # \u53d6\u51fa \u5143\u7d20 \u51fa\u73b0\u6b21\u6570\u6700\u9ad8\u7684\n    # \u5982\u679c\u8be5\u5143\u7d20\u5728 inTree.children \u8fd9\u4e2a\u5b57\u5178\u4e2d\uff0c\u5c31\u8fdb\u884c\u7d2f\u52a0\n    # \u5982\u679c\u8be5\u5143\u7d20\u4e0d\u5b58\u5728 \u5c31 inTree.children \u5b57\u5178\u4e2d\u65b0\u589ekey\uff0cvalue\u4e3a\u521d\u59cb\u5316\u7684 treeNode \u5bf9\u8c61\n    if items[0] in inTree.children:\n        # \u66f4\u65b0 \u6700\u5927\u5143\u7d20\uff0c\u5bf9\u5e94\u7684 treeNode \u5bf9\u8c61\u7684count\u8fdb\u884c\u53e0\u52a0\n        inTree.children[items[0]].inc(count)\n    else:\n        # \u5982\u679c\u4e0d\u5b58\u5728\u5b50\u8282\u70b9\uff0c\u6211\u4eec\u4e3a\u8be5inTree\u6dfb\u52a0\u5b50\u8282\u70b9\n        inTree.children[items[0]] = treeNode(items[0], count, inTree)\n        # \u5982\u679c\u6ee1\u8db3minSup\u7684dist\u5b57\u5178\u7684value\u503c\u7b2c\u4e8c\u4f4d\u4e3anull\uff0c \u6211\u4eec\u5c31\u8bbe\u7f6e\u8be5\u5143\u7d20\u4e3a \u672c\u8282\u70b9\u5bf9\u5e94\u7684tree\u8282\u70b9\n        # \u5982\u679c\u5143\u7d20\u7b2c\u4e8c\u4f4d\u4e0d\u4e3anull\uff0c\u6211\u4eec\u5c31\u66f4\u65b0header\u8282\u70b9\n        if headerTable[items[0]][1] is None:\n            # headerTable\u53ea\u8bb0\u5f55\u7b2c\u4e00\u6b21\u8282\u70b9\u51fa\u73b0\u7684\u4f4d\u7f6e\n            headerTable[items[0]][1] = inTree.children[items[0]]\n        else:\n            # \u672c\u8d28\u4e0a\u662f\u4fee\u6539headerTable\u7684key\u5bf9\u5e94\u7684Tree\uff0c\u7684nodeLink\u503c\n            updateHeader(headerTable[items[0]][1], inTree.children[items[0]])\n    if len(items) > 1:\n        # \u9012\u5f52\u7684\u8c03\u7528\uff0c\u5728items[0]\u7684\u57fa\u7840\u4e0a\uff0c\u6dfb\u52a0item0[1]\u505a\u5b50\u8282\u70b9\uff0c count\u53ea\u8981\u5faa\u73af\u7684\u8fdb\u884c\u7d2f\u8ba1\u52a0\u548c\u800c\u5df2\uff0c\u7edf\u8ba1\u51fa\u8282\u70b9\u7684\u6700\u540e\u7684\u7edf\u8ba1\u503c\u3002\n        updateTree(items[1:], inTree.children[items[0]], headerTable, count)\n\n\ndef createTree(dataSet, minSup=1):\n    \"\"\"createTree(\u751f\u6210FP-tree)\n\n    Args:\n        dataSet  dist{\u884c: \u51fa\u73b0\u6b21\u6570}\u7684\u6837\u672c\u6570\u636e\n        minSup   \u6700\u5c0f\u7684\u652f\u6301\u5ea6\n    Returns:\n        retTree  FP-tree\n        headerTable \u6ee1\u8db3minSup {\u6240\u6709\u7684\u5143\u7d20+(value, treeNode)}\n    \"\"\"\n    # \u652f\u6301\u5ea6>=minSup\u7684dist{\u6240\u6709\u5143\u7d20: \u51fa\u73b0\u7684\u6b21\u6570}\n    headerTable = {}\n    # \u5faa\u73af dist{\u884c: \u51fa\u73b0\u6b21\u6570}\u7684\u6837\u672c\u6570\u636e\n    for trans in dataSet:\n        # \u5bf9\u6240\u6709\u7684\u884c\u8fdb\u884c\u5faa\u73af\uff0c\u5f97\u5230\u884c\u91cc\u9762\u7684\u6240\u6709\u5143\u7d20\n        # \u7edf\u8ba1\u6bcf\u4e00\u884c\u4e2d\uff0c\u6bcf\u4e2a\u5143\u7d20\u51fa\u73b0\u7684\u603b\u6b21\u6570\n        for item in trans:\n            # \u4f8b\u5982:  {'ababa': 3}  count(a)=3+3+3=9   count(b)=3+3=6\n            headerTable[item] = headerTable.get(item, 0) + dataSet[trans]\n    # \u5220\u9664 headerTable\u4e2d\uff0c\u5143\u7d20\u6b21\u6570<\u6700\u5c0f\u652f\u6301\u5ea6\u7684\u5143\u7d20\n    for k in list(headerTable.keys()):  # python3\u4e2d.keys()\u8fd4\u56de\u7684\u662f\u8fed\u4ee3\u5668\u4e0d\u662flist,\u4e0d\u80fd\u5728\u904d\u5386\u65f6\u5bf9\u5176\u6539\u53d8\u3002\n        if headerTable[k] < minSup:\n            del(headerTable[k])\n\n    # \u6ee1\u8db3minSup: set(\u5404\u5143\u7d20\u96c6\u5408)\n    freqItemSet = set(headerTable.keys())\n    # \u5982\u679c\u4e0d\u5b58\u5728\uff0c\u76f4\u63a5\u8fd4\u56deNone\n    if len(freqItemSet) == 0:\n        return None, None\n    for k in headerTable:\n        # \u683c\u5f0f\u5316:  dist{\u5143\u7d20key: [\u5143\u7d20\u6b21\u6570, None]}\n        headerTable[k] = [headerTable[k], None]\n\n    # create tree\n    retTree = treeNode('Null Set', 1, None)\n    # \u5faa\u73af dist{\u884c: \u51fa\u73b0\u6b21\u6570}\u7684\u6837\u672c\u6570\u636e\n    for tranSet, count in dataSet.items():\n        # print('tranSet, count=', tranSet, count)\n        # localD = dist{\u5143\u7d20key: \u5143\u7d20\u603b\u51fa\u73b0\u6b21\u6570}\n        localD = {}\n        for item in tranSet:\n            # \u5224\u65ad\u662f\u5426\u5728\u6ee1\u8db3minSup\u7684\u96c6\u5408\u4e2d\n            if item in freqItemSet:\n                # print('headerTable[item][0]=', headerTable[item][0], headerTable[item])\n                localD[item] = headerTable[item][0]\n        # print('localD=', localD)\n        # \u5bf9\u6bcf\u4e00\u884c\u7684key \u8fdb\u884c\u6392\u5e8f\uff0c\u7136\u540e\u5f00\u59cb\u5f80\u6811\u6dfb\u52a0\u679d\u4e2b\uff0c\u76f4\u5230\u4e30\u6ee1\n        # \u7b2c\u4e8c\u6b21\uff0c\u5982\u679c\u5728\u540c\u4e00\u4e2a\u6392\u540d\u4e0b\u51fa\u73b0\uff0c\u90a3\u4e48\u5c31\u5bf9\u8be5\u679d\u4e2b\u7684\u503c\u8fdb\u884c\u8ffd\u52a0\uff0c\u7ee7\u7eed\u9012\u5f52\u8c03\u7528\uff01\n        if len(localD) > 0:\n            # p=key,value; \u6240\u4ee5\u662f\u901a\u8fc7value\u503c\u7684\u5927\u5c0f\uff0c\u8fdb\u884c\u4ece\u5927\u5230\u5c0f\u8fdb\u884c\u6392\u5e8f\n            # orderedItems \u8868\u793a\u53d6\u51fa\u5143\u7ec4\u7684key\u503c\uff0c\u4e5f\u5c31\u662f\u5b57\u6bcd\u672c\u8eab\uff0c\u4f46\u662f\u5b57\u6bcd\u672c\u8eab\u662f\u5927\u5230\u5c0f\u7684\u987a\u5e8f\n            orderedItems = [v[0] for v in sorted(localD.items(), key=lambda p: p[1], reverse=True)]\n            # print 'orderedItems=', orderedItems, 'headerTable', headerTable, '\\n\\n\\n'\n            # \u586b\u5145\u6811\uff0c\u901a\u8fc7\u6709\u5e8f\u7684orderedItems\u7684\u7b2c\u4e00\u4f4d\uff0c\u8fdb\u884c\u987a\u5e8f\u586b\u5145 \u7b2c\u4e00\u5c42\u7684\u5b50\u8282\u70b9\u3002\n            updateTree(orderedItems, retTree, headerTable, count)\n\n    return retTree, headerTable\n\n\ndef ascendTree(leafNode, prefixPath):\n    \"\"\"ascendTree(\u5982\u679c\u5b58\u5728\u7236\u8282\u70b9\uff0c\u5c31\u8bb0\u5f55\u5f53\u524d\u8282\u70b9\u7684name\u503c)\n\n    Args:\n        leafNode   \u67e5\u8be2\u7684\u8282\u70b9\u5bf9\u4e8e\u7684nodeTree\n        prefixPath \u8981\u67e5\u8be2\u7684\u8282\u70b9\u503c\n    \"\"\"\n    if leafNode.parent is not None:\n        prefixPath.append(leafNode.name)\n        ascendTree(leafNode.parent, prefixPath)\n\n\ndef findPrefixPath(basePat, treeNode):\n    \"\"\"findPrefixPath \u57fa\u7840\u6570\u636e\u96c6\n\n    Args:\n        basePat  \u8981\u67e5\u8be2\u7684\u8282\u70b9\u503c\n        treeNode \u67e5\u8be2\u7684\u8282\u70b9\u6240\u5728\u7684\u5f53\u524dnodeTree\n    Returns:\n        condPats \u5bf9\u975ebasePat\u7684\u5012\u53d9\u503c\u4f5c\u4e3akey,\u8d4b\u503c\u4e3acount\u6570\n    \"\"\"\n    condPats = {}\n    # \u5bf9 treeNode\u7684link\u8fdb\u884c\u5faa\u73af\n    while treeNode is not None:\n        prefixPath = []\n        # \u5bfb\u627e\u6539\u8282\u70b9\u7684\u7236\u8282\u70b9\uff0c\u76f8\u5f53\u4e8e\u627e\u5230\u4e86\u8be5\u8282\u70b9\u7684\u9891\u7e41\u9879\u96c6\n        ascendTree(treeNode, prefixPath)\n        # \u6392\u9664\u81ea\u8eab\u8fd9\u4e2a\u5143\u7d20\uff0c\u5224\u65ad\u662f\u5426\u5b58\u5728\u7236\u5143\u7d20\uff08\u6240\u4ee5\u8981>1, \u8bf4\u660e\u5b58\u5728\u7236\u5143\u7d20\uff09\n        if len(prefixPath) > 1:\n            # \u5bf9\u975ebasePat\u7684\u5012\u53d9\u503c\u4f5c\u4e3akey,\u8d4b\u503c\u4e3acount\u6570\n            # prefixPath[1:] \u53d8frozenset\u540e\uff0c\u5b57\u6bcd\u5c31\u53d8\u65e0\u5e8f\u4e86\n            # condPats[frozenset(prefixPath)] = treeNode.count\n            condPats[frozenset(prefixPath[1:])] = treeNode.count\n        # \u9012\u5f52\uff0c\u5bfb\u627e\u6539\u8282\u70b9\u7684\u4e0b\u4e00\u4e2a \u76f8\u540c\u503c\u7684\u94fe\u63a5\u8282\u70b9\n        treeNode = treeNode.nodeLink\n        # print(treeNode)\n    return condPats\n\n\ndef mineTree(inTree, headerTable, minSup, preFix, freqItemList):\n    \"\"\"mineTree(\u521b\u5efa\u6761\u4ef6FP\u6811)\n\n    Args:\n        inTree       myFPtree\n        headerTable  \u6ee1\u8db3minSup {\u6240\u6709\u7684\u5143\u7d20+(value, treeNode)}\n        minSup       \u6700\u5c0f\u652f\u6301\u9879\u96c6\n        preFix       preFix\u4e3anewFreqSet\u4e0a\u4e00\u6b21\u7684\u5b58\u50a8\u8bb0\u5f55\uff0c\u4e00\u65e6\u6ca1\u6709myHead\uff0c\u5c31\u4e0d\u4f1a\u66f4\u65b0\n        freqItemList \u7528\u6765\u5b58\u50a8\u9891\u7e41\u5b50\u9879\u7684\u5217\u8868\n    \"\"\"\n    # \u901a\u8fc7value\u8fdb\u884c\u4ece\u5c0f\u5230\u5927\u7684\u6392\u5e8f\uff0c \u5f97\u5230\u9891\u7e41\u9879\u96c6\u7684key\n    # \u6700\u5c0f\u652f\u6301\u9879\u96c6\u7684key\u7684list\u96c6\u5408\n    bigL = [v[0] for v in sorted(headerTable.items(), key=lambda p: p[1][0])]\n    print('-----', sorted(headerTable.items(), key=lambda p: p[1][0]))\n    print('bigL=', bigL)\n    # \u5faa\u73af\u904d\u5386 \u6700\u9891\u7e41\u9879\u96c6\u7684key\uff0c\u4ece\u5c0f\u5230\u5927\u7684\u9012\u5f52\u5bfb\u627e\u5bf9\u5e94\u7684\u9891\u7e41\u9879\u96c6\n    for basePat in bigL:\n        # preFix\u4e3anewFreqSet\u4e0a\u4e00\u6b21\u7684\u5b58\u50a8\u8bb0\u5f55\uff0c\u4e00\u65e6\u6ca1\u6709myHead\uff0c\u5c31\u4e0d\u4f1a\u66f4\u65b0\n        newFreqSet = preFix.copy()\n        newFreqSet.add(basePat)\n        print('newFreqSet=', newFreqSet, preFix)\n\n        freqItemList.append(newFreqSet)\n        print('freqItemList=', freqItemList)\n        condPattBases = findPrefixPath(basePat, headerTable[basePat][1])\n        print('condPattBases=', basePat, condPattBases)\n\n        # \u6784\u5efaFP-tree\n        myCondTree, myHead = createTree(condPattBases, minSup)\n        print('myHead=', myHead)\n        # \u6316\u6398\u6761\u4ef6 FP-tree, \u5982\u679cmyHead\u4e0d\u4e3a\u7a7a\uff0c\u8868\u793a\u6ee1\u8db3minSup {\u6240\u6709\u7684\u5143\u7d20+(value, treeNode)}\n        if myHead is not None:\n            myCondTree.disp(1)\n            print('\\n\\n\\n')\n            # \u9012\u5f52 myHead \u627e\u51fa\u9891\u7e41\u9879\u96c6\n            mineTree(myCondTree, myHead, minSup, newFreqSet, freqItemList)\n        print('\\n\\n\\n')\n\n\n# import twitter\n# from time import sleep\n# import re\n\n\n# def getLotsOfTweets(searchStr):\n#     \"\"\"\n#     \u83b7\u53d6 100\u4e2a\u641c\u7d22\u7ed3\u679c\u9875\u9762\n#     \"\"\"\n#     CONSUMER_KEY = ''\n#     CONSUMER_SECRET = ''\n#     ACCESS_TOKEN_KEY = ''\n#     ACCESS_TOKEN_SECRET = ''\n#     api = twitter.Api(consumer_key=CONSUMER_KEY, consumer_secret=CONSUMER_SECRET, access_token_key=ACCESS_TOKEN_KEY, access_token_secret=ACCESS_TOKEN_SECRET)\n\n#     # you can get 1500 results 15 pages * 100 per page\n#     resultsPages = []\n#     for i in range(1, 15):\n#         print(\"fetching page %d\" % i)\n#         searchResults = api.GetSearch(searchStr, per_page=100, page=i)\n#         resultsPages.append(searchResults)\n#         sleep(6)\n#     return resultsPages\n\n\n# def textParse(bigString):\n#     \"\"\"\n#     \u89e3\u6790\u9875\u9762\u5185\u5bb9\n#     \"\"\"\n#     urlsRemoved = re.sub('(http:[/][/]|www.)([a-z]|[A-Z]|[0-9]|[/.]|[~])*', '', bigString)    \n#     listOfTokens = re.split(r'\\W*', urlsRemoved)\n#     return [tok.lower() for tok in listOfTokens if len(tok) > 2]\n\n\n# def mineTweets(tweetArr, minSup=5):\n#     \"\"\"\n#     \u83b7\u53d6\u9891\u7e41\u9879\u96c6\n#     \"\"\"\n#     parsedList = []\n#     for i in range(14):\n#         for j in range(100):\n#             parsedList.append(textParse(tweetArr[i][j].text))\n#     initSet = createInitSet(parsedList)\n#     myFPtree, myHeaderTab = createTree(initSet, minSup)\n#     myFreqList = []\n#     mineTree(myFPtree, myHeaderTab, minSup, set([]), myFreqList)\n#     return myFreqList\n\n\nif __name__ == \"__main__\":\n    # rootNode = treeNode('pyramid', 9, None)\n    # rootNode.children['eye'] = treeNode('eye', 13, None)\n    # rootNode.children['phoenix'] = treeNode('phoenix', 3, None)\n    # # \u5c06\u6811\u4ee5\u6587\u672c\u5f62\u5f0f\u663e\u793a\n    # # print(rootNode.disp())\n\n    # load\u6837\u672c\u6570\u636e\n    simpDat = loadSimpDat()\n    # print(simpDat, '\\n')\n    # frozen set \u683c\u5f0f\u5316 \u5e76 \u91cd\u65b0\u88c5\u8f7d \u6837\u672c\u6570\u636e\uff0c\u5bf9\u6240\u6709\u7684\u884c\u8fdb\u884c\u7edf\u8ba1\u6c42\u548c\uff0c\u683c\u5f0f: {\u884c: \u51fa\u73b0\u6b21\u6570}\n    initSet = createInitSet(simpDat)\n    print(initSet)\n\n    # \u521b\u5efaFP\u6811\n    # \u8f93\u5165: dist{\u884c: \u51fa\u73b0\u6b21\u6570}\u7684\u6837\u672c\u6570\u636e  \u548c  \u6700\u5c0f\u7684\u652f\u6301\u5ea6\n    # \u8f93\u51fa: \u6700\u7ec8\u7684PF-tree\uff0c\u901a\u8fc7\u5faa\u73af\u83b7\u53d6\u7b2c\u4e00\u5c42\u7684\u8282\u70b9\uff0c\u7136\u540e\u6bcf\u4e00\u5c42\u7684\u8282\u70b9\u8fdb\u884c\u9012\u5f52\u7684\u83b7\u53d6\u6bcf\u4e00\u884c\u7684\u5b57\u8282\u70b9\uff0c\u4e5f\u5c31\u662f\u5206\u652f\u3002\u7136\u540e\u6240\u8c13\u7684\u6307\u9488\uff0c\u5c31\u662f\u540e\u6765\u7684\u6307\u5411\u5df2\u5b58\u5728\u7684\n    myFPtree, myHeaderTab = createTree(initSet, 3)\n    myFPtree.disp()\n\n    # \u62bd\u53d6\u6761\u4ef6\u6a21\u5f0f\u57fa\n    # \u67e5\u8be2\u6811\u8282\u70b9\u7684\uff0c\u9891\u7e41\u5b50\u9879\n    print('x --->', findPrefixPath('x', myHeaderTab['x'][1]))\n    print('z --->', findPrefixPath('z', myHeaderTab['z'][1]))\n    print('r --->', findPrefixPath('r', myHeaderTab['r'][1]))\n\n    # \u521b\u5efa\u6761\u4ef6\u6a21\u5f0f\u57fa\n    freqItemList = []\n    mineTree(myFPtree, myHeaderTab, 3, set([]), freqItemList)\n    print(\"freqItemList: \\n\", freqItemList)\n\n    # # \u9879\u76ee\u5b9e\u6218\n    # # 1.twitter\u9879\u76ee\u6848\u4f8b\n    # # \u65e0\u6cd5\u8fd0\u884c\uff0c\u56e0\u4e3a\u6ca1\u53d1\u94fe\u63a5twitter\n    # lotsOtweets = getLotsOfTweets('RIMM')\n    # listOfTerms = mineTweets(lotsOtweets, 20)\n    # print(len(listOfTerms))\n    # for t in listOfTerms:\n    #     print(t)\n\n    # # 2.\u65b0\u95fb\u7f51\u7ad9\u70b9\u51fb\u6d41\u4e2d\u6316\u6398\uff0c\u4f8b\u5982: \u6587\u7ae01\u9605\u8bfb\u8fc7\u7684\u4eba\uff0c\u8fd8\u9605\u8bfb\u8fc7\u4ec0\u4e48\uff1f\n    # parsedDat = [line.split() for line in open('data/12.FPGrowth/kosarak.dat').readlines()]\n    # initSet = createInitSet(parsedDat)\n    # myFPtree, myHeaderTab = createTree(initSet, 100000)\n\n    # myFreList = []\n    # mineTree(myFPtree, myHeaderTab, 100000, set([]), myFreList)\n    # print myFreList\n", "src/py3.x/ml/9.RegTrees/sklearn-regressTree-demo.py": "#!/usr/bin/python\n# coding:utf8\n\n\"\"\"\nCreated on 2017-07-13\nUpdated on 2017-07-13\nRegressionTree: \u6811\u56de\u5f52\nAuthor: \u5c0f\u7476\nGitHub: https://github.com/apachecn/AiLearning\n\"\"\"\n\nprint(__doc__)\n\n# \u5f15\u5165\u5fc5\u8981\u7684\u6a21\u578b\u548c\u5e93\nimport numpy as np\nfrom sklearn.tree import DecisionTreeRegressor\nimport matplotlib.pyplot as plt\n\n# \u521b\u5efa\u4e00\u4e2a\u968f\u673a\u7684\u6570\u636e\u96c6\n# \u53c2\u8003 https://docs.scipy.org/doc/numpy-1.6.0/reference/generated/numpy.random.mtrand.RandomState.html\nrng = np.random.RandomState(1)\n# print 'lalalalala===', rng\n# rand() \u662f\u7ed9\u5b9a\u5f62\u72b6\u7684\u968f\u673a\u503c\uff0crng.rand(80, 1)\u5373\u77e9\u9635\u7684\u5f62\u72b6\u662f 80\u884c\uff0c1\u5217\n# sort() \nX = np.sort(5 * rng.rand(80, 1), axis=0)\n# print 'X=', X\ny = np.sin(X).ravel()\n# print 'y=', y\ny[::5] += 3 * (0.5 - rng.rand(16))\n# print 'yyy=', y\n\n# \u62df\u5408\u56de\u5f52\u6a21\u578b\n# regr_1 = DecisionTreeRegressor(max_depth=2)\n# \u4fdd\u6301 max_depth=5 \u4e0d\u53d8\uff0c\u589e\u52a0 min_samples_leaf=6 \u7684\u53c2\u6570\uff0c\u6548\u679c\u8fdb\u4e00\u6b65\u63d0\u5347\u4e86\nregr_2 = DecisionTreeRegressor(max_depth=5)\nregr_2 = DecisionTreeRegressor(min_samples_leaf=6)\n# regr_3 = DecisionTreeRegressor(max_depth=4)\n# regr_1.fit(X, y)\nregr_2.fit(X, y)\n# regr_3.fit(X, y)\n\n# \u9884\u6d4b\nX_test = np.arange(0.0, 5.0, 0.01)[:, np.newaxis]\n# y_1 = regr_1.predict(X_test)\ny_2 = regr_2.predict(X_test)\n# y_3 = regr_3.predict(X_test)\n\n# \u7ed8\u5236\u7ed3\u679c\nplt.figure()\nplt.scatter(X, y, c=\"darkorange\", label=\"data\")\n# plt.plot(X_test, y_1, color=\"cornflowerblue\", label=\"max_depth=2\", linewidth=2)\nplt.plot(X_test, y_2, color=\"yellowgreen\", label=\"max_depth=5\", linewidth=2)\n# plt.plot(X_test, y_3, color=\"red\", label=\"max_depth=3\", linewidth=2)\nplt.xlabel(\"data\")\nplt.ylabel(\"target\")\nplt.title(\"Decision Tree Regression\")\nplt.legend()\nplt.show()", "src/py3.x/ml/9.RegTrees/RTSklearn.py": "#!/usr/bin/python\n# coding:utf8\n\n# '''\n# Created on 2017-03-10\n# Update on 2017-03-10\n# author: jiangzhonglian\n# content: \u56de\u5f52\u6811\n# '''\n\n# print(__doc__)\n\n\n# # Import the necessary modules and libraries\n# import numpy as np\n# from sklearn.tree import DecisionTreeRegressor\n# import matplotlib.pyplot as plt\n\n\n# # Create a random dataset\n# rng = np.random.RandomState(1)\n# X = np.sort(5 * rng.rand(80, 1), axis=0)\n# y = np.sin(X).ravel()\n# print X, '\\n\\n\\n-----------\\n\\n\\n', y\n# y[::5] += 3 * (0.5 - rng.rand(16))\n\n\n# # Fit regression model\n# regr_1 = DecisionTreeRegressor(max_depth=2, min_samples_leaf=5)\n# regr_2 = DecisionTreeRegressor(max_depth=5, min_samples_leaf=5)\n# regr_1.fit(X, y)\n# regr_2.fit(X, y)\n\n\n# # Predict\n# X_test = np.arange(0.0, 5.0, 0.01)[:, np.newaxis]\n# y_1 = regr_1.predict(X_test)\n# y_2 = regr_2.predict(X_test)\n\n\n# # Plot the results\n# plt.figure()\n# plt.scatter(X, y, c=\"darkorange\", label=\"data\")\n# plt.plot(X_test, y_1, color=\"cornflowerblue\", label=\"max_depth=2\", linewidth=2)\n# plt.plot(X_test, y_2, color=\"yellowgreen\", label=\"max_depth=5\", linewidth=2)\n# plt.xlabel(\"data\")\n# plt.ylabel(\"target\")\n# plt.title(\"Decision Tree Regression\")\n# plt.legend()\n# plt.show()\n\n\n\n\n\n\n\n\n'''\nCreated on 2017-03-10\nUpdate on 2017-03-10\nauthor: jiangzhonglian\ncontent: \u6a21\u578b\u6811\n'''\n\nprint(__doc__)\n\n# Author: Noel Dawe <noel.dawe@gmail.com>\n#\n# License: BSD 3 clause\n\n# importing necessary libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import AdaBoostRegressor\n\n# Create the dataset\nrng = np.random.RandomState(1)\nX = np.linspace(0, 6, 100)[:, np.newaxis]\ny = np.sin(X).ravel() + np.sin(6 * X).ravel() + rng.normal(0, 0.1, X.shape[0])\n\n# Fit regression model\nregr_1 = DecisionTreeRegressor(max_depth=4)\n\nregr_2 = AdaBoostRegressor(DecisionTreeRegressor(max_depth=4),\n                          n_estimators=300, random_state=rng)\n\nregr_1.fit(X, y)\nregr_2.fit(X, y)\n\n# Predict\ny_1 = regr_1.predict(X)\ny_2 = regr_2.predict(X)\n\n# Plot the results\nplt.figure()\nplt.scatter(X, y, c=\"k\", label=\"training samples\")\nplt.plot(X, y_1, c=\"g\", label=\"n_estimators=1\", linewidth=2)\nplt.plot(X, y_2, c=\"r\", label=\"n_estimators=300\", linewidth=2)\nplt.xlabel(\"data\")\nplt.ylabel(\"target\")\nplt.title(\"Boosted Decision Tree Regression\")\nplt.legend()\nplt.show()\n", "src/py3.x/ml/9.RegTrees/treeExplore.py": "#!/usr/bin/python\n# coding:utf8\n\n'''\nCreated on 2017-03-08\nUpdate  on 2017-05-18\nTree-Based Regression Methods Source Code for Machine Learning in Action Ch. 9\nAauthor: Peter/\u7247\u523b\nGitHub:  https://github.com/apachecn/AiLearning\n'''\nimport regTrees\nfrom Tkinter import *\nfrom numpy import *\n\nimport matplotlib\nfrom matplotlib.figure import Figure\nfrom matplotlib.backends.backend_tkagg import FigureCanvasTkAgg\nmatplotlib.use('TkAgg')\n\n\ndef test_widget_text(root):\n    mylabel = Label(root, text=\"helloworld\")\n    # \u76f8\u5f53\u4e8e\u544a\u8bc9 \u5e03\u5c40\u7ba1\u7406\u5668(Geometry Manager),\u5982\u679c\u4e0d\u8bbe\u5b9a\u4f4d\u7f6e\uff0c\u9ed8\u8ba4\u5728 0\u884c0\u5217\u7684\u4f4d\u7f6e\n    mylabel.grid()\n\n\n# \u6700\u5927\u4e3a\u8bef\u5dee\uff0c \u6700\u5927\u5b50\u53f6\u8282\u70b9\u7684\u6570\u91cf\ndef reDraw(tolS, tolN):\n    # clear the figure\n    reDraw.f.clf()\n    reDraw.a = reDraw.f.add_subplot(111)\n\n    # \u68c0\u67e5\u590d\u9009\u6846\u662f\u5426\u9009\u4e2d\n    if chkBtnVar.get():\n        if tolN < 2:\n            tolN = 2\n        myTree = regTrees.createTree(reDraw.rawDat, regTrees.modelLeaf, regTrees.modelErr, (tolS, tolN))\n        yHat = regTrees.createForeCast(myTree, reDraw.testDat, regTrees.modelTreeEval)\n    else:\n        myTree = regTrees.createTree(reDraw.rawDat, ops=(tolS, tolN))\n        yHat = regTrees.createForeCast(myTree, reDraw.testDat)\n\n    # use scatter for data set\n    reDraw.a.scatter(reDraw.rawDat[:, 0], reDraw.rawDat[:, 1], s=5)\n    # use plot for yHat\n    reDraw.a.plot(reDraw.testDat, yHat, linewidth=2.0, c='red')\n    reDraw.canvas.show()\n\n\ndef getInputs():\n    try:\n        tolN = int(tolNentry.get())\n    except:\n        tolN = 10\n        print(\"enter Integer for tolN\")\n        tolNentry.delete(0, END)\n        tolNentry.insert(0, '10')\n    try:\n        tolS = float(tolSentry.get())\n    except:\n        tolS = 1.0\n        print(\"enter Float for tolS\")\n        tolSentry.delete(0, END)\n        tolSentry.insert(0, '1.0')\n    return tolN, tolS\n\n\n# \u753b\u65b0\u7684tree\ndef drawNewTree():\n    # #get values from Entry boxes\n    tolN, tolS = getInputs()\n    reDraw(tolS, tolN)\n\n\ndef main(root):\n    # \u6807\u9898\n    Label(root, text=\"Plot Place Holder\").grid(row=0, columnspan=3)\n    # \u8f93\u5165\u680f1, \u53f6\u5b50\u7684\u6570\u91cf\n    Label(root, text=\"tolN\").grid(row=1, column=0)\n    global tolNentry\n    tolNentry = Entry(root)\n    tolNentry.grid(row=1, column=1)\n    tolNentry.insert(0, '10')\n    # \u8f93\u5165\u680f2, \u8bef\u5dee\u91cf\n    Label(root, text=\"tolS\").grid(row=2, column=0)\n    global tolSentry\n    tolSentry = Entry(root)\n    tolSentry.grid(row=2, column=1)\n    # \u8bbe\u7f6e\u8f93\u51fa\u503c\n    tolSentry.insert(0,'1.0')\n\n    # \u8bbe\u7f6e\u63d0\u4ea4\u7684\u6309\u94ae\n    Button(root, text=\"\u786e\u5b9a\", command=drawNewTree).grid(row=1, column=2, rowspan=3)\n\n    # \u8bbe\u7f6e\u590d\u9009\u6309\u94ae\n    global chkBtnVar\n    chkBtnVar = IntVar()\n    chkBtn = Checkbutton(root, text=\"Model Tree\", variable = chkBtnVar)\n    chkBtn.grid(row=3, column=0, columnspan=2)\n\n    # \u9000\u51fa\u6309\u94ae\n    Button(root, text=\"\u9000\u51fa\", fg=\"black\", command=quit).grid(row=1, column=2)\n\n    # \u521b\u5efa\u4e00\u4e2a\u753b\u677f canvas\n    reDraw.f = Figure(figsize=(5, 4), dpi=100)\n    reDraw.canvas = FigureCanvasTkAgg(reDraw.f, master=root)\n    reDraw.canvas.show()\n    reDraw.canvas.get_tk_widget().grid(row=0, columnspan=3)\n\n    reDraw.rawDat = mat(regTrees.loadDataSet('data/9.RegTrees/sine.txt'))\n    reDraw.testDat = arange(min(reDraw.rawDat[:, 0]), max(reDraw.rawDat[:, 0]), 0.01)\n    reDraw(1.0, 10)\n\n\nif __name__ == \"__main__\":\n\n    # \u521b\u5efa\u4e00\u4e2a\u4e8b\u4ef6\n    root = Tk()\n    # test_widget_text(root)\n    main(root)\n\n    # \u542f\u52a8\u4e8b\u4ef6\u5faa\u73af\n    root.mainloop()\n", "src/py3.x/ml/9.RegTrees/regTrees.py": "#!/usr/bin/python\n# coding:utf8\n'''\nCreated on Feb 4, 2011\nUpdate on 2017-05-18\nTree-Based Regression Methods Source Code for Machine Learning in Action Ch. 9\nAuthor: Peter Harrington/\u7247\u523b/\u5c0f\u7476\nGitHub: https://github.com/apachecn/AiLearning\n'''\nprint(__doc__)\nfrom numpy import *\n\n\n# \u9ed8\u8ba4\u89e3\u6790\u7684\u6570\u636e\u662f\u7528tab\u5206\u9694\uff0c\u5e76\u4e14\u662f\u6570\u503c\u7c7b\u578b\n# general function to parse tab -delimited floats\ndef loadDataSet(fileName):\n    \"\"\"loadDataSet(\u89e3\u6790\u6bcf\u4e00\u884c\uff0c\u5e76\u8f6c\u5316\u4e3afloat\u7c7b\u578b)\n        Desc: \u8be5\u51fd\u6570\u8bfb\u53d6\u4e00\u4e2a\u4ee5 tab \u952e\u4e3a\u5206\u9694\u7b26\u7684\u6587\u4ef6\uff0c\u7136\u540e\u5c06\u6bcf\u884c\u7684\u5185\u5bb9\u4fdd\u5b58\u6210\u4e00\u7ec4\u6d6e\u70b9\u6570\n    Args:\n        fileName \u6587\u4ef6\u540d\n    Returns:\n        dataMat \u6bcf\u4e00\u884c\u7684\u6570\u636e\u96c6array\u7c7b\u578b\n    Raises:\n    \"\"\"\n    # \u5047\u5b9a\u6700\u540e\u4e00\u5217\u662f\u7ed3\u679c\u503c\n    # assume last column is target value\n    dataMat = []\n    fr = open(fileName)\n    for line in fr.readlines():\n        curLine = line.strip().split('\\t')\n        #\u5c06\u6bcf\u884c\u8f6c\u6362\u6210\u6d6e\u70b9\u6570\n        fltLine = [float(x) for x in curLine]\n        dataMat.append(fltLine)\n    return dataMat\n\n\ndef binSplitDataSet(dataSet, feature, value):\n    \"\"\"binSplitDataSet(\u5c06\u6570\u636e\u96c6\uff0c\u6309\u7167feature\u5217\u7684value\u8fdb\u884c \u4e8c\u5143\u5207\u5206)\n        Description: \u5728\u7ed9\u5b9a\u7279\u5f81\u548c\u7279\u5f81\u503c\u7684\u60c5\u51b5\u4e0b\uff0c\u8be5\u51fd\u6570\u901a\u8fc7\u6570\u7ec4\u8fc7\u6ee4\u65b9\u5f0f\u5c06\u4e0a\u8ff0\u6570\u636e\u96c6\u5408\u5207\u5206\u5f97\u5230\u4e24\u4e2a\u5b50\u96c6\u5e76\u8fd4\u56de\u3002\n    Args:\n        dataMat \u6570\u636e\u96c6\n        feature \u5f85\u5207\u5206\u7684\u7279\u5f81\u5217\n        value \u7279\u5f81\u5217\u8981\u6bd4\u8f83\u7684\u503c\n    Returns:\n        mat0 \u5c0f\u4e8e\u7b49\u4e8e value \u7684\u6570\u636e\u96c6\u5728\u5de6\u8fb9\n        mat1 \u5927\u4e8e value \u7684\u6570\u636e\u96c6\u5728\u53f3\u8fb9\n    Raises:\n    \"\"\"\n    # # \u6d4b\u8bd5\u6848\u4f8b\n    # print 'dataSet[:, feature]=', dataSet[:, feature]\n    # print 'nonzero(dataSet[:, feature] > value)[0]=', nonzero(dataSet[:, feature] > value)[0]\n    # print 'nonzero(dataSet[:, feature] <= value)[0]=', nonzero(dataSet[:, feature] <= value)[0]\n\n    # dataSet[:, feature] \u53d6\u53bb\u6bcf\u4e00\u884c\u4e2d\uff0c\u7b2c1\u5217\u7684\u503c(\u4ece0\u5f00\u59cb\u7b97)\n    # nonzero(dataSet[:, feature] > value)  \u8fd4\u56de\u7ed3\u679c\u4e3atrue\u884c\u7684index\u4e0b\u6807\n    mat0 = dataSet[nonzero(dataSet[:, feature] <= value)[0], :]\n    mat1 = dataSet[nonzero(dataSet[:, feature] > value)[0], :]\n    return mat0, mat1\n\n\n# \u8fd4\u56de\u6bcf\u4e00\u4e2a\u53f6\u5b50\u7ed3\u70b9\u7684\u5747\u503c\n# returns the value used for each leaf\n# \u6211\u7684\u7406\u89e3\u662f: regLeaf \u662f\u4ea7\u751f\u53f6\u8282\u70b9\u7684\u51fd\u6570\uff0c\u5c31\u662f\u6c42\u5747\u503c\uff0c\u5373\u7528\u805a\u7c7b\u4e2d\u5fc3\u70b9\u6765\u4ee3\u8868\u8fd9\u7c7b\u6570\u636e\ndef regLeaf(dataSet):\n    return mean(dataSet[:, -1])\n\n\n# \u8ba1\u7b97\u603b\u65b9\u5dee=\u65b9\u5dee*\u6837\u672c\u6570\n# \u6211\u7684\u7406\u89e3\u662f: \u6c42\u8fd9\u7ec4\u6570\u636e\u7684\u65b9\u5dee\uff0c\u5373\u901a\u8fc7\u51b3\u7b56\u6811\u5212\u5206\uff0c\u53ef\u4ee5\u8ba9\u9760\u8fd1\u7684\u6570\u636e\u5206\u5230\u540c\u4e00\u7c7b\u4e2d\u53bb\ndef regErr(dataSet):\n    # shape(dataSet)[0] \u8868\u793a\u884c\u6570\n    return var(dataSet[:, -1]) * shape(dataSet)[0]\n\n\n# 1.\u7528\u6700\u4f73\u65b9\u5f0f\u5207\u5206\u6570\u636e\u96c6\n# 2.\u751f\u6210\u76f8\u5e94\u7684\u53f6\u8282\u70b9\ndef chooseBestSplit(dataSet, leafType=regLeaf, errType=regErr, ops=(1, 4)):\n    \"\"\"chooseBestSplit(\u7528\u6700\u4f73\u65b9\u5f0f\u5207\u5206\u6570\u636e\u96c6 \u548c \u751f\u6210\u76f8\u5e94\u7684\u53f6\u8282\u70b9)\n\n    Args:\n        dataSet   \u52a0\u8f7d\u7684\u539f\u59cb\u6570\u636e\u96c6\n        leafType  \u5efa\u7acb\u53f6\u5b50\u70b9\u7684\u51fd\u6570\n        errType   \u8bef\u5dee\u8ba1\u7b97\u51fd\u6570(\u6c42\u603b\u65b9\u5dee)\n        ops       [\u5bb9\u8bb8\u8bef\u5dee\u4e0b\u964d\u503c\uff0c\u5207\u5206\u7684\u6700\u5c11\u6837\u672c\u6570]\u3002\n    Returns:\n        bestIndex feature\u7684index\u5750\u6807\n        bestValue \u5207\u5206\u7684\u6700\u4f18\u503c\n    Raises:\n    \"\"\"\n\n    # ops=(1,4)\uff0c\u975e\u5e38\u91cd\u8981\uff0c\u56e0\u4e3a\u5b83\u51b3\u5b9a\u4e86\u51b3\u7b56\u6811\u5212\u5206\u505c\u6b62\u7684threshold\u503c\uff0c\u88ab\u79f0\u4e3a\u9884\u526a\u679d\uff08prepruning\uff09\uff0c\u5176\u5b9e\u4e5f\u5c31\u662f\u7528\u4e8e\u63a7\u5236\u51fd\u6570\u7684\u505c\u6b62\u65f6\u673a\u3002\n    # \u4e4b\u6240\u4ee5\u8fd9\u6837\u8bf4\uff0c\u662f\u56e0\u4e3a\u5b83\u9632\u6b62\u51b3\u7b56\u6811\u7684\u8fc7\u62df\u5408\uff0c\u6240\u4ee5\u5f53\u8bef\u5dee\u7684\u4e0b\u964d\u503c\u5c0f\u4e8etolS\uff0c\u6216\u5212\u5206\u540e\u7684\u96c6\u5408size\u5c0f\u4e8etolN\u65f6\uff0c\u9009\u62e9\u505c\u6b62\u7ee7\u7eed\u5212\u5206\u3002\n    # \u6700\u5c0f\u8bef\u5dee\u4e0b\u964d\u503c\uff0c\u5212\u5206\u540e\u7684\u8bef\u5dee\u51cf\u5c0f\u5c0f\u4e8e\u8fd9\u4e2a\u5dee\u503c\uff0c\u5c31\u4e0d\u7528\u7ee7\u7eed\u5212\u5206\n    tolS = ops[0]\n    # \u5212\u5206\u6700\u5c0f size \u5c0f\u4e8e\uff0c\u5c31\u4e0d\u7ee7\u7eed\u5212\u5206\u4e86\n    tolN = ops[1]\n    #\u5982\u679c\u6570\u636e\u96c6\u7684\u6700\u540e\u4e00\u5217\u6240\u6709\u503c\u76f8\u7b49\u5c31\u9000\u51fa\n    #dataSet[:, -1].T.tolist()[0] \u53d6\u6570\u636e\u96c6\u7684\u6700\u540e\u4e00\u5217\uff0c\u8f6c\u7f6e\u4e3a\u884c\u5411\u91cf\uff0c\u7136\u540e\u8f6c\u6362\u4e3alist,\u53d6\u8be5list\u4e2d\u7684\u7b2c\u4e00\u4e2a\u5143\u7d20\u3002\n    if len(set(dataSet[:, -1].T.tolist()[0])) == 1: # \u5982\u679c\u96c6\u5408size\u4e3a1\uff0c\u4e5f\u5c31\u662f\u8bf4\u5168\u90e8\u7684\u6570\u636e\u90fd\u662f\u540c\u4e00\u4e2a\u7c7b\u522b\uff0c\u4e0d\u7528\u7ee7\u7eed\u5212\u5206\u3002\n        #  exit cond 1\n        return None, leafType(dataSet)\n    # \u8ba1\u7b97\u884c\u5217\u503c\n    m, n = shape(dataSet)\n    # \u65e0\u5206\u7c7b\u8bef\u5dee\u7684\u603b\u65b9\u5dee\u548c\n    # the choice of the best feature is driven by Reduction in RSS error from mean\n    S = errType(dataSet)\n    # inf \u6b63\u65e0\u7a77\u5927\n    bestS, bestIndex, bestValue = inf, 0, 0\n    # \u5faa\u73af\u5904\u7406\u6bcf\u4e00\u5217\u5bf9\u5e94\u7684feature\u503c\n    for featIndex in range(n-1): # \u5bf9\u4e8e\u6bcf\u4e2a\u7279\u5f81\n        # \u4e0b\u9762\u7684\u4e00\u884c\u8868\u793a\u7684\u662f\u5c06\u67d0\u4e00\u5217\u5168\u90e8\u7684\u6570\u636e\u8f6c\u6362\u4e3a\u884c\uff0c\u7136\u540e\u8bbe\u7f6e\u4e3alist\u5f62\u5f0f\n        for splitVal in set(dataSet[:, featIndex].T.tolist()[0]):\n            # \u5bf9\u8be5\u5217\u8fdb\u884c\u5206\u7ec4\uff0c\u7136\u540e\u7ec4\u5185\u7684\u6210\u5458\u7684val\u503c\u8fdb\u884c \u4e8c\u5143\u5207\u5206\n            mat0, mat1 = binSplitDataSet(dataSet, featIndex, splitVal)\n            # \u5224\u65ad\u4e8c\u5143\u5207\u5206\u7684\u65b9\u5f0f\u7684\u5143\u7d20\u6570\u91cf\u662f\u5426\u7b26\u5408\u9884\u671f\n            if (shape(mat0)[0] < tolN) or (shape(mat1)[0] < tolN):\n                continue\n            newS = errType(mat0) + errType(mat1)\n            # \u5982\u679c\u4e8c\u5143\u5207\u5206\uff0c\u7b97\u51fa\u6765\u7684\u8bef\u5dee\u5728\u53ef\u63a5\u53d7\u8303\u56f4\u5185\uff0c\u90a3\u4e48\u5c31\u8bb0\u5f55\u5207\u5206\u70b9\uff0c\u5e76\u8bb0\u5f55\u6700\u5c0f\u8bef\u5dee\n            # \u5982\u679c\u5212\u5206\u540e\u8bef\u5dee\u5c0f\u4e8e bestS\uff0c\u5219\u8bf4\u660e\u627e\u5230\u4e86\u65b0\u7684bestS\n            if newS < bestS:\n                bestIndex = featIndex\n                bestValue = splitVal\n                bestS = newS\n    # \u5224\u65ad\u4e8c\u5143\u5207\u5206\u7684\u65b9\u5f0f\u7684\u5143\u7d20\u8bef\u5dee\u662f\u5426\u7b26\u5408\u9884\u671f\n    # if the decrease (S-bestS) is less than a threshold don't do the split\n    if (S - bestS) < tolS:\n        return None, leafType(dataSet)\n    mat0, mat1 = binSplitDataSet(dataSet, bestIndex, bestValue)\n    # \u5bf9\u6574\u4f53\u7684\u6210\u5458\u8fdb\u884c\u5224\u65ad\uff0c\u662f\u5426\u7b26\u5408\u9884\u671f\n    # \u5982\u679c\u96c6\u5408\u7684 size \u5c0f\u4e8e tolN \n    if (shape(mat0)[0] < tolN) or (shape(mat1)[0] < tolN): # \u5f53\u6700\u4f73\u5212\u5206\u540e\uff0c\u96c6\u5408\u8fc7\u5c0f\uff0c\u4e5f\u4e0d\u5212\u5206\uff0c\u4ea7\u751f\u53f6\u8282\u70b9\n        return None, leafType(dataSet)\n    return bestIndex, bestValue\n\n\n# assume dataSet is NumPy Mat so we can array filtering\n# \u5047\u8bbe dataSet \u662f NumPy Mat \u7c7b\u578b\u7684\uff0c\u90a3\u4e48\u6211\u4eec\u53ef\u4ee5\u8fdb\u884c array \u8fc7\u6ee4\ndef createTree(dataSet, leafType=regLeaf, errType=regErr, ops=(1, 4)):\n    \"\"\"createTree(\u83b7\u53d6\u56de\u5f52\u6811)\n        Description: \u9012\u5f52\u51fd\u6570: \u5982\u679c\u6784\u5efa\u7684\u662f\u56de\u5f52\u6811\uff0c\u8be5\u6a21\u578b\u662f\u4e00\u4e2a\u5e38\u6570\uff0c\u5982\u679c\u662f\u6a21\u578b\u6811\uff0c\u5176\u6a21\u578b\u5e08\u4e00\u4e2a\u7ebf\u6027\u65b9\u7a0b\u3002\n    Args:\n        dataSet      \u52a0\u8f7d\u7684\u539f\u59cb\u6570\u636e\u96c6\n        leafType     \u5efa\u7acb\u53f6\u5b50\u70b9\u7684\u51fd\u6570\n        errType      \u8bef\u5dee\u8ba1\u7b97\u51fd\u6570\n        ops=(1, 4)   [\u5bb9\u8bb8\u8bef\u5dee\u4e0b\u964d\u503c\uff0c\u5207\u5206\u7684\u6700\u5c11\u6837\u672c\u6570]\n    Returns:\n        retTree    \u51b3\u7b56\u6811\u6700\u540e\u7684\u7ed3\u679c\n    \"\"\"\n    # \u9009\u62e9\u6700\u597d\u7684\u5207\u5206\u65b9\u5f0f:  feature\u7d22\u5f15\u503c\uff0c\u6700\u4f18\u5207\u5206\u503c\n    # choose the best split\n    feat, val = chooseBestSplit(dataSet, leafType, errType, ops)\n    # if the splitting hit a stop condition return val\n    # \u5982\u679c splitting \u8fbe\u5230\u4e00\u4e2a\u505c\u6b62\u6761\u4ef6\uff0c\u90a3\u4e48\u8fd4\u56de val\n    if feat is None:\n        return val\n    retTree = {}\n    retTree['spInd'] = feat\n    retTree['spVal'] = val\n    # \u5927\u4e8e\u5728\u53f3\u8fb9\uff0c\u5c0f\u4e8e\u5728\u5de6\u8fb9\uff0c\u5206\u4e3a2\u4e2a\u6570\u636e\u96c6\n    lSet, rSet = binSplitDataSet(dataSet, feat, val)\n    # \u9012\u5f52\u7684\u8fdb\u884c\u8c03\u7528\uff0c\u5728\u5de6\u53f3\u5b50\u6811\u4e2d\u7ee7\u7eed\u9012\u5f52\u751f\u6210\u6811\n    retTree['left'] = createTree(lSet, leafType, errType, ops)\n    retTree['right'] = createTree(rSet, leafType, errType, ops)\n    return retTree\n\n\n# \u5224\u65ad\u8282\u70b9\u662f\u5426\u662f\u4e00\u4e2a\u5b57\u5178\ndef isTree(obj):\n    \"\"\"\n    Desc:\n        \u6d4b\u8bd5\u8f93\u5165\u53d8\u91cf\u662f\u5426\u662f\u4e00\u68f5\u6811,\u5373\u662f\u5426\u662f\u4e00\u4e2a\u5b57\u5178\n    Args:\n        obj -- \u8f93\u5165\u53d8\u91cf\n    Returns:\n        \u8fd4\u56de\u5e03\u5c14\u7c7b\u578b\u7684\u7ed3\u679c\u3002\u5982\u679c obj \u662f\u4e00\u4e2a\u5b57\u5178\uff0c\u8fd4\u56detrue\uff0c\u5426\u5219\u8fd4\u56de false\n    \"\"\"\n    return (type(obj).__name__ == 'dict')\n\n\n# \u8ba1\u7b97\u5de6\u53f3\u679d\u4e2b\u7684\u5747\u503c\ndef getMean(tree):\n    \"\"\"\n    Desc:\n        \u4ece\u4e0a\u5f80\u4e0b\u904d\u5386\u6811\u76f4\u5230\u53f6\u8282\u70b9\u4e3a\u6b62\uff0c\u5982\u679c\u627e\u5230\u4e24\u4e2a\u53f6\u8282\u70b9\u5219\u8ba1\u7b97\u5b83\u4eec\u7684\u5e73\u5747\u503c\u3002\n        \u5bf9 tree \u8fdb\u884c\u584c\u9677\u5904\u7406\uff0c\u5373\u8fd4\u56de\u6811\u5e73\u5747\u503c\u3002\n    Args:\n        tree -- \u8f93\u5165\u7684\u6811\n    Returns:\n        \u8fd4\u56de tree \u8282\u70b9\u7684\u5e73\u5747\u503c\n    \"\"\"\n    if isTree(tree['right']):\n        tree['right'] = getMean(tree['right'])\n    if isTree(tree['left']):\n        tree['left'] = getMean(tree['left'])\n    return (tree['left']+tree['right'])/2.0\n\n\n# \u68c0\u67e5\u662f\u5426\u9002\u5408\u5408\u5e76\u5206\u679d\ndef prune(tree, testData):\n    \"\"\"\n    Desc:\n        \u4ece\u4e0a\u800c\u4e0b\u627e\u5230\u53f6\u8282\u70b9\uff0c\u7528\u6d4b\u8bd5\u6570\u636e\u96c6\u6765\u5224\u65ad\u5c06\u8fd9\u4e9b\u53f6\u8282\u70b9\u5408\u5e76\u662f\u5426\u80fd\u964d\u4f4e\u6d4b\u8bd5\u8bef\u5dee\n    Args:\n        tree -- \u5f85\u526a\u679d\u7684\u6811\n        testData -- \u526a\u679d\u6240\u9700\u8981\u7684\u6d4b\u8bd5\u6570\u636e testData \n    Returns:\n        tree -- \u526a\u679d\u5b8c\u6210\u7684\u6811\n    \"\"\"\n    # \u5224\u65ad\u662f\u5426\u6d4b\u8bd5\u6570\u636e\u96c6\u6ca1\u6709\u6570\u636e\uff0c\u5982\u679c\u6ca1\u6709\uff0c\u5c31\u76f4\u63a5\u8fd4\u56detree\u672c\u8eab\u7684\u5747\u503c\n    if shape(testData)[0] == 0:\n        return getMean(tree)\n\n    # \u5224\u65ad\u5206\u679d\u662f\u5426\u662fdict\u5b57\u5178\uff0c\u5982\u679c\u662f\u5c31\u5c06\u6d4b\u8bd5\u6570\u636e\u96c6\u8fdb\u884c\u5207\u5206\n    if (isTree(tree['right']) or isTree(tree['left'])):\n        lSet, rSet = binSplitDataSet(testData, tree['spInd'], tree['spVal'])\n    # \u5982\u679c\u662f\u5de6\u8fb9\u5206\u679d\u662f\u5b57\u5178\uff0c\u5c31\u4f20\u5165\u5de6\u8fb9\u7684\u6570\u636e\u96c6\u548c\u5de6\u8fb9\u7684\u5206\u679d\uff0c\u8fdb\u884c\u9012\u5f52\n    if isTree(tree['left']):\n        tree['left'] = prune(tree['left'], lSet)\n    # \u5982\u679c\u662f\u53f3\u8fb9\u5206\u679d\u662f\u5b57\u5178\uff0c\u5c31\u4f20\u5165\u5de6\u8fb9\u7684\u6570\u636e\u96c6\u548c\u5de6\u8fb9\u7684\u5206\u679d\uff0c\u8fdb\u884c\u9012\u5f52\n    if isTree(tree['right']):\n        tree['right'] = prune(tree['right'], rSet)\n\n    # \u4e0a\u9762\u7684\u4e00\u7cfb\u5217\u64cd\u4f5c\u672c\u8d28\u4e0a\u5c31\u662f\u5c06\u6d4b\u8bd5\u6570\u636e\u96c6\u6309\u7167\u8bad\u7ec3\u5b8c\u6210\u7684\u6811\u62c6\u5206\u597d\uff0c\u5bf9\u5e94\u7684\u503c\u653e\u5230\u5bf9\u5e94\u7684\u8282\u70b9\n\n    # \u5982\u679c\u5de6\u53f3\u4e24\u8fb9\u540c\u65f6\u90fd\u4e0d\u662fdict\u5b57\u5178\uff0c\u4e5f\u5c31\u662f\u5de6\u53f3\u4e24\u8fb9\u90fd\u662f\u53f6\u8282\u70b9\uff0c\u800c\u4e0d\u662f\u5b50\u6811\u4e86\uff0c\u90a3\u4e48\u5206\u5272\u6d4b\u8bd5\u6570\u636e\u96c6\u3002\n    # 1. \u5982\u679c\u6b63\u786e \n    #   * \u90a3\u4e48\u8ba1\u7b97\u4e00\u4e0b\u603b\u65b9\u5dee \u548c \u8be5\u7ed3\u679c\u96c6\u7684\u672c\u8eab\u4e0d\u5206\u679d\u7684\u603b\u65b9\u5dee\u6bd4\u8f83\n    #   * \u5982\u679c \u5408\u5e76\u7684\u603b\u65b9\u5dee < \u4e0d\u5408\u5e76\u7684\u603b\u65b9\u5dee\uff0c\u90a3\u4e48\u5c31\u8fdb\u884c\u5408\u5e76\n    # \u6ce8\u610f\u8fd4\u56de\u7684\u7ed3\u679c:  \u5982\u679c\u53ef\u4ee5\u5408\u5e76\uff0c\u539f\u6765\u7684dict\u5c31\u53d8\u4e3a\u4e86 \u6570\u503c\n    if not isTree(tree['left']) and not isTree(tree['right']):\n        lSet, rSet = binSplitDataSet(testData, tree['spInd'], tree['spVal'])\n        # power(x, y)\u8868\u793ax\u7684y\u6b21\u65b9\n        errorNoMerge = sum(power(lSet[:, -1] - tree['left'], 2)) + sum(power(rSet[:, -1] - tree['right'], 2))\n        treeMean = (tree['left'] + tree['right'])/2.0\n        errorMerge = sum(power(testData[:, -1] - treeMean, 2))\n        # \u5982\u679c \u5408\u5e76\u7684\u603b\u65b9\u5dee < \u4e0d\u5408\u5e76\u7684\u603b\u65b9\u5dee\uff0c\u90a3\u4e48\u5c31\u8fdb\u884c\u5408\u5e76\n        if errorMerge < errorNoMerge:\n            print(\"merging\")\n            return treeMean\n        else:\n            return tree\n    else:\n        return tree\n\n\n# \u5f97\u5230\u6a21\u578b\u7684ws\u7cfb\u6570: f(x) = x0 + x1*featrue1+ x3*featrue2 ...\n# create linear model and return coeficients\ndef modelLeaf(dataSet):\n    \"\"\"\n    Desc:\n        \u5f53\u6570\u636e\u4e0d\u518d\u9700\u8981\u5207\u5206\u7684\u65f6\u5019\uff0c\u751f\u6210\u53f6\u8282\u70b9\u7684\u6a21\u578b\u3002\n    Args:\n        dataSet -- \u8f93\u5165\u6570\u636e\u96c6\n    Returns:\n        \u8c03\u7528 linearSolve \u51fd\u6570\uff0c\u8fd4\u56de\u5f97\u5230\u7684 \u56de\u5f52\u7cfb\u6570ws\n    \"\"\"\n    ws, X, Y = linearSolve(dataSet)\n    return ws\n\n\n# \u8ba1\u7b97\u7ebf\u6027\u6a21\u578b\u7684\u8bef\u5dee\u503c\ndef modelErr(dataSet):\n    \"\"\"\n    Desc:\n        \u5728\u7ed9\u5b9a\u6570\u636e\u96c6\u4e0a\u8ba1\u7b97\u8bef\u5dee\u3002\n    Args:\n        dataSet -- \u8f93\u5165\u6570\u636e\u96c6\n    Returns:\n        \u8c03\u7528 linearSolve \u51fd\u6570\uff0c\u8fd4\u56de yHat \u548c Y \u4e4b\u95f4\u7684\u5e73\u65b9\u8bef\u5dee\u3002\n    \"\"\"\n    ws, X, Y = linearSolve(dataSet)\n    yHat = X * ws\n    # print corrcoef(yHat, Y, rowvar=0)\n    return sum(power(Y - yHat, 2))\n\n\n # helper function used in two places\ndef linearSolve(dataSet):\n    \"\"\"\n    Desc:\n        \u5c06\u6570\u636e\u96c6\u683c\u5f0f\u5316\u6210\u76ee\u6807\u53d8\u91cfY\u548c\u81ea\u53d8\u91cfX\uff0c\u6267\u884c\u7b80\u5355\u7684\u7ebf\u6027\u56de\u5f52\uff0c\u5f97\u5230ws\n    Args:\n        dataSet -- \u8f93\u5165\u6570\u636e\n    Returns:\n        ws -- \u6267\u884c\u7ebf\u6027\u56de\u5f52\u7684\u56de\u5f52\u7cfb\u6570 \n        X -- \u683c\u5f0f\u5316\u81ea\u53d8\u91cfX\n        Y -- \u683c\u5f0f\u5316\u76ee\u6807\u53d8\u91cfY\n    \"\"\"\n    m, n = shape(dataSet)\n    # \u4ea7\u751f\u4e00\u4e2a\u5173\u4e8e1\u7684\u77e9\u9635\n    X = mat(ones((m, n)))\n    Y = mat(ones((m, 1)))\n    # X\u76840\u5217\u4e3a1\uff0c\u5e38\u6570\u9879\uff0c\u7528\u4e8e\u8ba1\u7b97\u5e73\u8861\u8bef\u5dee\n    X[:, 1: n] = dataSet[:, 0: n-1]\n    Y = dataSet[:, -1]\n\n    # \u8f6c\u7f6e\u77e9\u9635*\u77e9\u9635\n    xTx = X.T * X\n    # \u5982\u679c\u77e9\u9635\u7684\u9006\u4e0d\u5b58\u5728\uff0c\u4f1a\u9020\u6210\u7a0b\u5e8f\u5f02\u5e38\n    if linalg.det(xTx) == 0.0:\n        raise NameError('This matrix is singular, cannot do inverse,\\ntry increasing the second value of ops')\n    # \u6700\u5c0f\u4e8c\u4e58\u6cd5\u6c42\u6700\u4f18\u89e3:  w0*1+w1*x1=y\n    ws = xTx.I * (X.T * Y)\n    return ws, X, Y\n\n\n# \u56de\u5f52\u6811\u6d4b\u8bd5\u6848\u4f8b\n# \u4e3a\u4e86\u548c modelTreeEval() \u4fdd\u6301\u4e00\u81f4\uff0c\u4fdd\u7559\u4e24\u4e2a\u8f93\u5165\u53c2\u6570\ndef regTreeEval(model, inDat):\n    \"\"\"\n    Desc:\n        \u5bf9 \u56de\u5f52\u6811 \u8fdb\u884c\u9884\u6d4b\n    Args:\n        model -- \u6307\u5b9a\u6a21\u578b\uff0c\u53ef\u9009\u503c\u4e3a \u56de\u5f52\u6811\u6a21\u578b \u6216\u8005 \u6a21\u578b\u6811\u6a21\u578b\uff0c\u8fd9\u91cc\u4e3a\u56de\u5f52\u6811\n        inDat -- \u8f93\u5165\u7684\u6d4b\u8bd5\u6570\u636e\n    Returns:\n        float(model) -- \u5c06\u8f93\u5165\u7684\u6a21\u578b\u6570\u636e\u8f6c\u6362\u4e3a \u6d6e\u70b9\u6570 \u8fd4\u56de\n    \"\"\"\n    return float(model)\n\n\n# \u6a21\u578b\u6811\u6d4b\u8bd5\u6848\u4f8b\n# \u5bf9\u8f93\u5165\u6570\u636e\u8fdb\u884c\u683c\u5f0f\u5316\u5904\u7406\uff0c\u5728\u539f\u6570\u636e\u77e9\u9635\u4e0a\u589e\u52a0\u7b2c0\u5217\uff0c\u5143\u7d20\u7684\u503c\u90fd\u662f1\uff0c\n# \u4e5f\u5c31\u662f\u589e\u52a0\u504f\u79fb\u503c\uff0c\u548c\u6211\u4eec\u4e4b\u524d\u7684\u7b80\u5355\u7ebf\u6027\u56de\u5f52\u662f\u4e00\u4e2a\u5957\u8def\uff0c\u589e\u52a0\u4e00\u4e2a\u504f\u79fb\u91cf\ndef modelTreeEval(model, inDat):\n    \"\"\"\n    Desc:\n        \u5bf9 \u6a21\u578b\u6811 \u8fdb\u884c\u9884\u6d4b\n    Args:\n        model -- \u8f93\u5165\u6a21\u578b\uff0c\u53ef\u9009\u503c\u4e3a \u56de\u5f52\u6811\u6a21\u578b \u6216\u8005 \u6a21\u578b\u6811\u6a21\u578b\uff0c\u8fd9\u91cc\u4e3a\u6a21\u578b\u6811\u6a21\u578b\n        inDat -- \u8f93\u5165\u7684\u6d4b\u8bd5\u6570\u636e\n    Returns:\n        float(X * model) -- \u5c06\u6d4b\u8bd5\u6570\u636e\u4e58\u4ee5 \u56de\u5f52\u7cfb\u6570 \u5f97\u5230\u4e00\u4e2a\u9884\u6d4b\u503c \uff0c\u8f6c\u5316\u4e3a \u6d6e\u70b9\u6570 \u8fd4\u56de\n    \"\"\"\n    n = shape(inDat)[1]\n    X = mat(ones((1, n+1)))\n    X[:, 1: n+1] = inDat\n    # print X, model\n    return float(X * model)\n\n\n# \u8ba1\u7b97\u9884\u6d4b\u7684\u7ed3\u679c\n# \u5728\u7ed9\u5b9a\u6811\u7ed3\u6784\u7684\u60c5\u51b5\u4e0b\uff0c\u5bf9\u4e8e\u5355\u4e2a\u6570\u636e\u70b9\uff0c\u8be5\u51fd\u6570\u4f1a\u7ed9\u51fa\u4e00\u4e2a\u9884\u6d4b\u503c\u3002\n# modelEval\u662f\u5bf9\u53f6\u8282\u70b9\u8fdb\u884c\u9884\u6d4b\u7684\u51fd\u6570\u5f15\u7528\uff0c\u6307\u5b9a\u6811\u7684\u7c7b\u578b\uff0c\u4ee5\u4fbf\u5728\u53f6\u8282\u70b9\u4e0a\u8c03\u7528\u5408\u9002\u7684\u6a21\u578b\u3002\n# \u6b64\u51fd\u6570\u81ea\u9876\u5411\u4e0b\u904d\u5386\u6574\u68f5\u6811\uff0c\u76f4\u5230\u547d\u4e2d\u53f6\u8282\u70b9\u4e3a\u6b62\uff0c\u4e00\u65e6\u5230\u8fbe\u53f6\u8282\u70b9\uff0c\u5b83\u5c31\u4f1a\u5728\u8f93\u5165\u6570\u636e\u4e0a\n# \u8c03\u7528modelEval()\u51fd\u6570\uff0c\u8be5\u51fd\u6570\u7684\u9ed8\u8ba4\u503c\u4e3aregTreeEval()\ndef treeForeCast(tree, inData, modelEval=regTreeEval):\n    \"\"\"\n    Desc:\n        \u5bf9\u7279\u5b9a\u6a21\u578b\u7684\u6811\u8fdb\u884c\u9884\u6d4b\uff0c\u53ef\u4ee5\u662f \u56de\u5f52\u6811 \u4e5f\u53ef\u4ee5\u662f \u6a21\u578b\u6811\n    Args:\n        tree -- \u5df2\u7ecf\u8bad\u7ec3\u597d\u7684\u6811\u7684\u6a21\u578b\n        inData -- \u8f93\u5165\u7684\u6d4b\u8bd5\u6570\u636e\n        modelEval -- \u9884\u6d4b\u7684\u6811\u7684\u6a21\u578b\u7c7b\u578b\uff0c\u53ef\u9009\u503c\u4e3a regTreeEval\uff08\u56de\u5f52\u6811\uff09 \u6216 modelTreeEval\uff08\u6a21\u578b\u6811\uff09\uff0c\u9ed8\u8ba4\u4e3a\u56de\u5f52\u6811\n    Returns:\n        \u8fd4\u56de\u9884\u6d4b\u503c\n    \"\"\"\n    if not isTree(tree):\n        return modelEval(tree, inData)\n    if inData[tree['spInd']] <= tree['spVal']:\n        if isTree(tree['left']):\n            return treeForeCast(tree['left'], inData, modelEval)\n        else:\n            return modelEval(tree['left'], inData)\n    else:\n        if isTree(tree['right']):\n            return treeForeCast(tree['right'], inData, modelEval)\n        else:\n            return modelEval(tree['right'], inData)\n\n\n# \u9884\u6d4b\u7ed3\u679c\ndef createForeCast(tree, testData, modelEval=regTreeEval):\n    \"\"\"\n    Desc:\n        \u8c03\u7528 treeForeCast \uff0c\u5bf9\u7279\u5b9a\u6a21\u578b\u7684\u6811\u8fdb\u884c\u9884\u6d4b\uff0c\u53ef\u4ee5\u662f \u56de\u5f52\u6811 \u4e5f\u53ef\u4ee5\u662f \u6a21\u578b\u6811\n    Args:\n        tree -- \u5df2\u7ecf\u8bad\u7ec3\u597d\u7684\u6811\u7684\u6a21\u578b\n        inData -- \u8f93\u5165\u7684\u6d4b\u8bd5\u6570\u636e\n        modelEval -- \u9884\u6d4b\u7684\u6811\u7684\u6a21\u578b\u7c7b\u578b\uff0c\u53ef\u9009\u503c\u4e3a regTreeEval\uff08\u56de\u5f52\u6811\uff09 \u6216 modelTreeEval\uff08\u6a21\u578b\u6811\uff09\uff0c\u9ed8\u8ba4\u4e3a\u56de\u5f52\u6811\n    Returns:\n        \u8fd4\u56de\u9884\u6d4b\u503c\u77e9\u9635\n    \"\"\"\n    m = len(testData)\n    yHat = mat(zeros((m, 1)))\n    # print yHat\n    for i in range(m):\n        yHat[i, 0] = treeForeCast(tree, mat(testData[i]), modelEval)\n        # print \"yHat==>\", yHat[i, 0]\n    return yHat\n\n\nif __name__ == \"__main__\":\n    # # \u6d4b\u8bd5\u6570\u636e\u96c6\n    # testMat = mat(eye(4))\n    # print testMat\n    # print type(testMat)\n    # mat0, mat1 = binSplitDataSet(testMat, 1, 0.5)\n    # print mat0, '\\n-----------\\n', mat1\n\n    # # \u56de\u5f52\u6811\n    # myDat = loadDataSet('data/9.RegTrees/data1.txt')\n    # # myDat = loadDataSet('data/9.RegTrees/data2.txt')\n    # # print 'myDat=', myDat\n    # myMat = mat(myDat)\n    # # print 'myMat=',  myMat\n    # myTree = createTree(myMat)\n    # print myTree\n\n    # # 1. \u9884\u526a\u679d\u5c31\u662f: \u63d0\u8d77\u8bbe\u7f6e\u6700\u5927\u8bef\u5dee\u6570\u548c\u6700\u5c11\u5143\u7d20\u6570\n    # myDat = loadDataSet('data/9.RegTrees/data3.txt')\n    # myMat = mat(myDat)\n    # myTree = createTree(myMat, ops=(0, 1))\n    # print myTree\n\n    # # 2. \u540e\u526a\u679d\u5c31\u662f: \u901a\u8fc7\u6d4b\u8bd5\u6570\u636e\uff0c\u5bf9\u9884\u6d4b\u6a21\u578b\u8fdb\u884c\u5408\u5e76\u5224\u65ad\n    # myDatTest = loadDataSet('data/9.RegTrees/data3test.txt')\n    # myMat2Test = mat(myDatTest)\n    # myFinalTree = prune(myTree, myMat2Test)\n    # print '\\n\\n\\n-------------------'\n    # print myFinalTree\n\n    # # --------\n    # # \u6a21\u578b\u6811\u6c42\u89e3\n    # myDat = loadDataSet('data/9.RegTrees/data4.txt')\n    # myMat = mat(myDat)\n    # myTree = createTree(myMat, modelLeaf, modelErr)\n    # print myTree\n\n    # # \u56de\u5f52\u6811 VS \u6a21\u578b\u6811 VS \u7ebf\u6027\u56de\u5f52\n    trainMat = mat(loadDataSet('data/9.RegTrees/bikeSpeedVsIq_train.txt'))\n    testMat = mat(loadDataSet('data/9.RegTrees/bikeSpeedVsIq_test.txt'))\n    # # \u56de\u5f52\u6811\n    myTree1 = createTree(trainMat, ops=(1, 20))\n    print(myTree1)\n    yHat1 = createForeCast(myTree1, testMat[:, 0])\n    print(\"--------------\\n\")\n    # print yHat1\n    # print \"ssss==>\", testMat[:, 1]\n    print(\"\u56de\u5f52\u6811:\", corrcoef(yHat1, testMat[:, 1],rowvar=0)[0, 1])\n\n    # \u6a21\u578b\u6811\n    myTree2 = createTree(trainMat, modelLeaf, modelErr, ops=(1, 20))\n    yHat2 = createForeCast(myTree2, testMat[:, 0], modelTreeEval)\n    print(myTree2)\n    print(\"\u6a21\u578b\u6811:\", corrcoef(yHat2, testMat[:, 1],rowvar=0)[0, 1])\n\n    # \u7ebf\u6027\u56de\u5f52\n    ws, X, Y = linearSolve(trainMat)\n    print(ws)\n    m = len(testMat[:, 0])\n    yHat3 = mat(zeros((m, 1)))\n    for i in range(shape(testMat)[0]):\n        yHat3[i] = testMat[i, 0]*ws[1, 0] + ws[0, 0]\n    print(\"\u7ebf\u6027\u56de\u5f52:\", corrcoef(yHat3, testMat[:, 1],rowvar=0)[0, 1])\n", "src/py3.x/ml/1.MLFoundation/NumPy.py": "#!/usr/bin/python\n# coding:utf-8\n\n'''\nCreated on 2017-05-18\nUpdate  on 2017-11-17\nAuthor: Peter Harrington/1988/\u7247\u523b\nGitHub: https://github.com/apachecn/AiLearning\n'''\n\nfrom numpy import random, mat, eye\n\n'''\n# NumPy \u77e9\u9635\u548c\u6570\u7ec4\u7684\u533a\u522b\nNumPy\u5b58\u57282\u4e2d\u4e0d\u540c\u7684\u6570\u636e\u7c7b\u578b:\n    1. \u77e9\u9635 matrix\n    2. \u6570\u7ec4 array\n\u76f8\u4f3c\u70b9: \n    \u90fd\u53ef\u4ee5\u5904\u7406\u884c\u5217\u8868\u793a\u7684\u6570\u5b57\u5143\u7d20\n\u4e0d\u540c\u70b9: \n    1. 2\u4e2a\u6570\u636e\u7c7b\u578b\u4e0a\u6267\u884c\u76f8\u540c\u7684\u6570\u636e\u8fd0\u7b97\u53ef\u80fd\u5f97\u5230\u4e0d\u540c\u7684\u7ed3\u679c\u3002\n    2. NumPy\u51fd\u6570\u5e93\u4e2d\u7684 matrix \u4e0e MATLAB\u4e2d matrices \u7b49\u4ef7\u3002\n'''\n\n# \u751f\u6210\u4e00\u4e2a 4*4 \u7684\u968f\u673a\u6570\u7ec4\nrandArray = random.rand(4, 4)\n\n# \u8f6c\u5316\u5173\u7cfb\uff0c \u6570\u7ec4\u8f6c\u5316\u4e3a\u77e9\u9635\nrandMat = mat(randArray)\n'''\n.I \u8868\u793a\u5bf9\u77e9\u9635\u6c42\u9006(\u53ef\u4ee5\u5229\u7528\u77e9\u9635\u7684\u521d\u7b49\u53d8\u6362)\n   \u610f\u4e49: \u9006\u77e9\u9635\u662f\u4e00\u4e2a\u5224\u65ad\u76f8\u4f3c\u6027\u7684\u5de5\u5177\u3002\u9006\u77e9\u9635A\u4e0e\u5217\u5411\u91cfp\u76f8\u4e58\u540e\uff0c\u5c06\u5f97\u5230\u5217\u5411\u91cfq\uff0cq\u7684\u7b2ci\u4e2a\u5206\u91cf\u8868\u793ap\u4e0eA\u7684\u7b2ci\u4e2a\u5217\u5411\u91cf\u7684\u76f8\u4f3c\u5ea6\u3002\n   \u53c2\u8003\u6848\u4f8b\u94fe\u63a5: \n   https://www.zhihu.com/question/33258489\n   http://blog.csdn.net/vernice/article/details/48506027\n.T \u8868\u793a\u5bf9\u77e9\u9635\u8f6c\u7f6e(\u884c\u5217\u98a0\u5012)\n    * \u7b49\u540c\u4e8e: .transpose()\n.A \u8fd4\u56de\u77e9\u9635\u57fa\u4e8e\u7684\u6570\u7ec4\n    \u53c2\u8003\u6848\u4f8b\u94fe\u63a5: \n    http://blog.csdn.net/qq403977698/article/details/47254539\n'''\ninvRandMat = randMat.I\nTraRandMat = randMat.T\nArrRandMat = randMat.A\n# \u8f93\u51fa\u7ed3\u679c\nprint('randArray=(%s) \\n' % type(randArray), randArray)\nprint('randMat=(%s) \\n' % type(randMat), randMat)\nprint('invRandMat=(%s) \\n' % type(invRandMat), invRandMat)\nprint('TraRandMat=(%s) \\n' % type(TraRandMat), TraRandMat)\nprint('ArrRandMat=(%s) \\n' % type(ArrRandMat), ArrRandMat)\n# \u77e9\u9635\u548c\u9006\u77e9\u9635 \u8fdb\u884c\u6c42\u79ef (\u5355\u4f4d\u77e9\u9635\uff0c\u5bf9\u89d2\u7ebf\u90fd\u4e3a1\u561b\uff0c\u7406\u8bba\u4e0a4*4\u7684\u77e9\u9635\u5176\u4ed6\u7684\u90fd\u4e3a0)\nmyEye = randMat*invRandMat\n# \u8bef\u5dee\nprint(myEye - eye(4))\n\n'''\n\u5982\u679c\u4e0a\u9762\u7684\u4ee3\u7801\u8fd0\u884c\u6ca1\u6709\u95ee\u9898\uff0c\u8bf4\u660enumpy\u5b89\u88c5\u6ca1\u6709\u95ee\u9898\n'''\n", "src/py3.x/ml/14.SVD/svdRecommend.py": "#!/usr/bin/python\n# coding: utf-8\n\n'''\nCreated on Mar 8, 2011\nUpdate  on 2017-12-12\nAuthor: Peter Harrington/\u5c71\u4e0a\u6709\u8bfe\u6811/\u7247\u523b/marsjhao\nGitHub: https://github.com/apachecn/AiLearning\n'''\nfrom numpy import linalg as la\nfrom numpy import *\n\n\ndef loadExData3():\n    # \u5229\u7528SVD\u63d0\u9ad8\u63a8\u8350\u6548\u679c\uff0c\u83dc\u80b4\u77e9\u9635\n    # \u53ef\u4ee5\u4fee\u6539\u539f\u6570\u636e\u96c6\u5408\uff0c\u7528\u5bf9\u5bf9\u6bd4\n    # return[[2, 0, 0, 4, 4, 0, 0, 0, 0, 0, 0],\n    #        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5],\n    #        [0, 0, 0, 0, 0, 0, 0, 1, 0, 4, 0],\n    #        [3, 3, 4, 0, 3, 0, 0, 2, 2, 0, 0],\n    #        [5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0],\n    #        [0, 0, 0, 0, 0, 0, 5, 0, 0, 5, 0],\n    #        [4, 0, 4, 0, 0, 0, 0, 0, 0, 0, 5],\n    #        [0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 4],\n    #        [0, 0, 0, 0, 0, 0, 5, 0, 0, 5, 0],\n    #        [0, 0, 0, 3, 0, 0, 0, 0, 4, 5, 0],\n    #        [1, 1, 2, 1, 1, 2, 1, 0, 4, 5, 0]]\n    # \u4fee\u6539\u540e\u7684\u6570\u636e\uff08\u589e\u52a0\u4e86\u7b2c1\u9053\u83dc\u548c\u6700\u540e1\u5230\u83dc\uff0c\u540c\u65f6\u67093\u4e2a\u4eba\u5403\uff0c\u4ece\u800c\u8ba1\u7b97\u57fa\u4e8e\u7269\u54c1\u7684\u534f\u540c\u8fc7\u6ee4\u6548\u679c\uff0c\u539f\u6765\u624d\u4e00\u4e2a\u4eba\uff09\n    return[[2, 0, 0, 4, 4, 0, 0, 0, 0, 0, 0],\n           [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5],\n           [0, 0, 0, 0, 0, 0, 0, 1, 0, 4, 0],\n           [3, 3, 4, 0, 3, 0, 0, 2, 2, 0, 8],\n           [5, 5, 5, 0, 0, 0, 0, 0, 0, 0, 0],\n           [0, 0, 0, 0, 0, 0, 5, 0, 0, 5, 0],\n           [4, 0, 4, 0, 0, 0, 0, 0, 0, 0, 5],\n           [0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 4],\n           [0, 0, 0, 0, 0, 0, 5, 0, 0, 5, 0],\n           [0, 0, 0, 3, 0, 0, 0, 0, 4, 5, 0],\n           [1, 1, 2, 1, 1, 2, 1, 0, 4, 5, 6]]\n\n\ndef loadExData2():\n    # \u4e66\u4e0a\u4ee3\u7801\u7ed9\u7684\u793a\u4f8b\u77e9\u9635\n    return[[0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 5],\n           [0, 0, 0, 3, 0, 4, 0, 0, 0, 0, 3],\n           [0, 0, 0, 0, 4, 0, 0, 1, 0, 4, 0],\n           [3, 3, 4, 0, 0, 0, 0, 2, 2, 0, 0],\n           [5, 4, 5, 0, 0, 0, 0, 5, 5, 0, 0],\n           [0, 0, 0, 0, 5, 0, 1, 0, 0, 5, 0],\n           [4, 3, 4, 0, 0, 0, 0, 5, 5, 0, 1],\n           [0, 0, 0, 4, 0, 4, 0, 0, 0, 0, 4],\n           [0, 0, 0, 2, 0, 2, 5, 0, 0, 1, 2],\n           [0, 0, 0, 0, 5, 0, 0, 0, 0, 4, 0],\n           [1, 0, 0, 0, 0, 0, 0, 1, 2, 0, 0]]\n\n\ndef loadExData():\n    \"\"\"\n    # \u63a8\u8350\u5f15\u64ce\u793a\u4f8b\u77e9\u9635\n    return[[4, 4, 0, 2, 2],\n           [4, 0, 0, 3, 3],\n           [4, 0, 0, 1, 1],\n           [1, 1, 1, 2, 0],\n           [2, 2, 2, 0, 0],\n           [1, 1, 1, 0, 0],\n           [5, 5, 5, 0, 0]]\n    \"\"\"\n    # # \u539f\u77e9\u9635\n    # return[[1, 1, 1, 0, 0],\n    #        [2, 2, 2, 0, 0],\n    #        [1, 1, 1, 0, 0],\n    #        [5, 5, 5, 0, 0],\n    #        [1, 1, 0, 2, 2],\n    #        [0, 0, 0, 3, 3],\n    #        [0, 0, 0, 1, 1]]\n\n    # \u539f\u77e9\u9635\n    return[[0, -1.6, 0.6],\n           [0, 1.2, 0.8],\n           [0, 0, 0],\n           [0, 0, 0]]\n\n\n# \u76f8\u4f3c\u5ea6\u8ba1\u7b97\uff0c\u5047\u5b9ainA\u548cinB \u90fd\u662f\u5217\u5411\u91cf\n# \u57fa\u4e8e\u6b27\u6c0f\u8ddd\u79bb\ndef ecludSim(inA, inB):\n    return 1.0/(1.0 + la.norm(inA - inB))\n\n\n# pearsSim()\u51fd\u6570\u4f1a\u68c0\u67e5\u662f\u5426\u5b58\u57283\u4e2a\u6216\u66f4\u591a\u7684\u70b9\u3002\n# corrcoef\u76f4\u63a5\u8ba1\u7b97\u76ae\u5c14\u900a\u76f8\u5173\u7cfb\u6570\uff0c\u8303\u56f4[-1, 1]\uff0c\u5f52\u4e00\u5316\u540e[0, 1]\ndef pearsSim(inA, inB):\n    # \u5982\u679c\u4e0d\u5b58\u5728\uff0c\u8be5\u51fd\u6570\u8fd4\u56de1.0\uff0c\u6b64\u65f6\u4e24\u4e2a\u5411\u91cf\u5b8c\u5168\u76f8\u5173\u3002\n    if len(inA) < 3:\n        return 1.0\n    return 0.5 + 0.5 * corrcoef(inA, inB, rowvar=0)[0][1]\n\n\n# \u8ba1\u7b97\u4f59\u5f26\u76f8\u4f3c\u5ea6\uff0c\u5982\u679c\u5939\u89d2\u4e3a90\u5ea6\uff0c\u76f8\u4f3c\u5ea6\u4e3a0\uff1b\u5982\u679c\u4e24\u4e2a\u5411\u91cf\u7684\u65b9\u5411\u76f8\u540c\uff0c\u76f8\u4f3c\u5ea6\u4e3a1.0\ndef cosSim(inA, inB):\n    num = float(inA.T*inB)\n    denom = la.norm(inA)*la.norm(inB)\n    return 0.5 + 0.5*(num/denom)\n\n\n# \u57fa\u4e8e\u7269\u54c1\u76f8\u4f3c\u5ea6\u7684\u63a8\u8350\u5f15\u64ce\ndef standEst(dataMat, user, simMeas, item):\n    \"\"\"standEst(\u8ba1\u7b97\u67d0\u7528\u6237\u672a\u8bc4\u5206\u7269\u54c1\u4e2d\uff0c\u4ee5\u5bf9\u8be5\u7269\u54c1\u548c\u5176\u4ed6\u7269\u54c1\u8bc4\u5206\u7684\u7528\u6237\u7684\u7269\u54c1\u76f8\u4f3c\u5ea6\uff0c\u7136\u540e\u8fdb\u884c\u7efc\u5408\u8bc4\u5206)\n    Args:\n        dataMat         \u8bad\u7ec3\u6570\u636e\u96c6\n        user            \u7528\u6237\u7f16\u53f7\n        simMeas         \u76f8\u4f3c\u5ea6\u8ba1\u7b97\u65b9\u6cd5\n        item            \u672a\u8bc4\u5206\u7684\u7269\u54c1\u7f16\u53f7\n    Returns:\n        ratSimTotal/simTotal     \u8bc4\u5206\uff080\uff5e5\u4e4b\u95f4\u7684\u503c\uff09\n    \"\"\"\n    # \u5f97\u5230\u6570\u636e\u96c6\u4e2d\u7684\u7269\u54c1\u6570\u76ee\n    n = shape(dataMat)[1]\n    # \u521d\u59cb\u5316\u4e24\u4e2a\u8bc4\u5206\u503c\n    simTotal = 0.0\n    ratSimTotal = 0.0\n    # \u904d\u5386\u884c\u4e2d\u7684\u6bcf\u4e2a\u7269\u54c1\uff08\u5bf9\u7528\u6237\u8bc4\u8fc7\u5206\u7684\u7269\u54c1\u8fdb\u884c\u904d\u5386\uff0c\u5e76\u5c06\u5b83\u4e0e\u5176\u4ed6\u7269\u54c1\u8fdb\u884c\u6bd4\u8f83\uff09\n    for j in range(n):\n        userRating = dataMat[user, j]\n        # \u5982\u679c\u67d0\u4e2a\u7269\u54c1\u7684\u8bc4\u5206\u503c\u4e3a0\uff0c\u5219\u8df3\u8fc7\u8fd9\u4e2a\u7269\u54c1\n        if userRating == 0:\n            continue\n        # \u5bfb\u627e\u4e24\u4e2a\u7528\u6237\u90fd\u8bc4\u7ea7\u7684\u7269\u54c1\n        # \u53d8\u91cf overLap \u7ed9\u51fa\u7684\u662f\u4e24\u4e2a\u7269\u54c1\u5f53\u4e2d\u5df2\u7ecf\u88ab\u8bc4\u5206\u7684\u90a3\u4e2a\u5143\u7d20\u7684\u7d22\u5f15ID\n        # logical_and \u8ba1\u7b97x1\u548cx2\u5143\u7d20\u7684\u4e3aTrue\u5c31\u4e3aTrue(\u4e5f\u5c31\u662f\u5217\u7684\u503c\u540c\u65f6>0), \u5426\u5219\u5c31\u4e3aFalse\n        # item(0): [[ True] [False] [False] [ True] [ True] [False] [ True] [False] [False] [False] [ True]] \n        # j(10):   [[False] [ True] [False] [False] [False] [False] [ True] [ True] [False] [False] [False]]\n        # +1--     [[False] [False] [False] [False] [False] [False] [ True] [False] [False] [False] [False]]\n        # +2--                                                      [6]\n        # print(\"+++ item(%s): %s --- j(%s): %s\" % (item, dataMat[:, item].A > 0, j, dataMat[:, j].A > 0))\n        # print(\"+1-- %s\" % logical_and(dataMat[:, item].A > 0, dataMat[:, j].A > 0) )\n        # print(\"+2-- %s\" % overLap)\n\n        overLap = nonzero(logical_and(dataMat[:, item].A > 0, dataMat[:, j].A > 0))[0]\n        # \u5982\u679c\u76f8\u4f3c\u5ea6\u4e3a0\uff0c\u5219\u4e24\u7740\u6ca1\u6709\u4efb\u4f55\u91cd\u5408\u5143\u7d20\uff0c\u7ec8\u6b62\u672c\u6b21\u5faa\u73af\n        if len(overLap) == 0:\n            similarity = 0\n        # \u5982\u679c\u5b58\u5728\u91cd\u5408\u7684\u7269\u54c1\uff0c\u5219\u57fa\u4e8e\u8fd9\u4e9b\u91cd\u5408\u7269\u91cd\u65b0\u8ba1\u7b97\u76f8\u4f3c\u5ea6\u3002\n        else:\n            # print(\"-%s-  %s:%s -- %s:%s\" % (overLap, item, dataMat[overLap, item], j, dataMat[overLap, j]) )\n            # \u5982\u679c overLap \u957f\u5ea6\u662f\u4e3a3\uff0c\u8bf4\u660e3\u4e2a\u4eba\u540c\u65f6\u5403\u4e86 \u83dcA\u5e76\u4e14\u4e5f\u540c\u65f6\u5403\u4e86\u83dcB\n            # \u90a3\u4e48\u5c31\u8981\u627e\u5bf9 \u8fd93\u4e2a\u4eba\u5bf9\u5e94 \u83dc\u8bc4\u5206\u7684\u77e9\u9635\n            # -[ 3  6 10](\u4eba)-  0(\u83dc):[[3] [4] [1]] -- 10(\u83dc):[[8] [5] [6]]\n            # \u7136\u540e\u5c31\u53ef\u4ee5\u8ba1\u7b97\u51fa\u6765\u4e24\u4e2a\u83dc\u4e4b\u95f4\u7684\u76f8\u4f3c\u5ea6\n            similarity = simMeas(dataMat[overLap, item], dataMat[overLap, j])\n        print('the %d and %d similarity is : %f' % (item, j, similarity))\n        # \u76f8\u4f3c\u5ea6\u4f1a\u4e0d\u65ad\u7d2f\u52a0\uff0c\u6bcf\u6b21\u8ba1\u7b97\u65f6\u8fd8\u8003\u8651\u76f8\u4f3c\u5ea6\u548c\u5f53\u524d\u7528\u6237\u8bc4\u5206\u7684\u4e58\u79ef\n        # similarity  \u7528\u6237\u76f8\u4f3c\u5ea6\uff0c   userRating \u7528\u6237\u8bc4\u5206\n        simTotal += similarity\n        ratSimTotal += similarity * userRating\n    if simTotal == 0:\n        return 0\n    # \u901a\u8fc7\u9664\u4ee5\u6240\u6709\u7684\u8bc4\u5206\u603b\u548c\uff0c\u5bf9\u4e0a\u8ff0\u76f8\u4f3c\u5ea6\u8bc4\u5206\u7684\u4e58\u79ef\u8fdb\u884c\u5f52\u4e00\u5316\uff0c\u4f7f\u5f97\u6700\u540e\u8bc4\u5206\u57280~5\u4e4b\u95f4\uff0c\u8fd9\u4e9b\u8bc4\u5206\u7528\u6765\u5bf9\u9884\u6d4b\u503c\u8fdb\u884c\u6392\u5e8f\n    else:\n        return ratSimTotal/simTotal\n\n\n# \u57fa\u4e8eSVD\u7684\u8bc4\u5206\u4f30\u8ba1\n# \u5728recommend() \u4e2d\uff0c\u8fd9\u4e2a\u51fd\u6570\u7528\u4e8e\u66ff\u6362\u5bf9standEst()\u7684\u8c03\u7528\uff0c\u8be5\u51fd\u6570\u5bf9\u7ed9\u5b9a\u7528\u6237\u7ed9\u5b9a\u7269\u54c1\u6784\u5efa\u4e86\u4e00\u4e2a\u8bc4\u5206\u4f30\u8ba1\u503c\ndef svdEst(dataMat, user, simMeas, item):\n    \"\"\"svdEst( )\n    Args:\n        dataMat         \u8bad\u7ec3\u6570\u636e\u96c6\n        user            \u7528\u6237\u7f16\u53f7\n        simMeas         \u76f8\u4f3c\u5ea6\u8ba1\u7b97\u65b9\u6cd5\n        item            \u672a\u8bc4\u5206\u7684\u7269\u54c1\u7f16\u53f7\n    Returns:\n        ratSimTotal/simTotal     \u8bc4\u5206\uff080\uff5e5\u4e4b\u95f4\u7684\u503c\uff09\n    \"\"\"\n    # \u7269\u54c1\u6570\u76ee\n    n = shape(dataMat)[1]\n    # \u5bf9\u6570\u636e\u96c6\u8fdb\u884cSVD\u5206\u89e3\n    simTotal = 0.0\n    ratSimTotal = 0.0\n    # \u5947\u5f02\u503c\u5206\u89e3\n    # \u5728SVD\u5206\u89e3\u4e4b\u540e\uff0c\u6211\u4eec\u53ea\u5229\u7528\u5305\u542b\u4e8690%\u80fd\u91cf\u503c\u7684\u5947\u5f02\u503c\uff0c\u8fd9\u4e9b\u5947\u5f02\u503c\u4f1a\u4ee5NumPy\u6570\u7ec4\u7684\u5f62\u5f0f\u5f97\u4ee5\u4fdd\u5b58\n    U, Sigma, VT = la.svd(dataMat)\n\n    # # \u5206\u6790 Sigma \u7684\u957f\u5ea6\u53d6\u503c\n    # analyse_data(Sigma, 20)\n\n    # \u5982\u679c\u8981\u8fdb\u884c\u77e9\u9635\u8fd0\u7b97\uff0c\u5c31\u5fc5\u987b\u8981\u7528\u8fd9\u4e9b\u5947\u5f02\u503c\u6784\u5efa\u51fa\u4e00\u4e2a\u5bf9\u89d2\u77e9\u9635\n    Sig4 = mat(eye(4) * Sigma[: 4])\n\n    # \u5229\u7528U\u77e9\u9635\u5c06\u7269\u54c1\u8f6c\u6362\u5230\u4f4e\u7ef4\u7a7a\u95f4\u4e2d\uff0c\u6784\u5efa\u8f6c\u6362\u540e\u7684\u7269\u54c1(\u7269\u54c1+4\u4e2a\u4e3b\u8981\u7684\u201c\u9690\u5f62\u201d\u7279\u5f81)\n    # \u516c\u5f0f1(\u76ee\u7684\u662f: \u964d\u7ef4-\u6539\u53d8\u5f62\u72b6\uff0c\u4e5f\u6539\u53d8\u5927\u5c0f)  xformedItems = dataMat.T * U[:, :4] * Sig4.I\n    # \u516c\u5f0f2(\u76ee\u7684\u662f: \u538b\u7f29-\u4e0d\u6539\u53d8\u5f62\u72b6\uff0c\u6539\u53d8\u5927\u5c0f)      reconMat = U[:, :4] * Sig4.I * VT[:4, :]\n        # \u5176\u4e2d: imgCompress() \u662f\u8be6\u7ec6\u7684\u6848\u4f8b\n    # \u6700\u8fd1\u770b\u5230\u4e00\u7bc7\u6587\u7ae0\u63cf\u8ff0\uff0c\u611f\u89c9\u633a\u6709\u9053\u7406\u7684\uff0c\u6211\u5c31\u987a\u4fbf\u8865\u5145\u4e00\u4e0b\u6ce8\u91ca: https://blog.csdn.net/qq_36523839/article/details/82347332\n    xformedItems = dataMat.T * U[:, :4] * Sig4.I\n    # print('dataMat', shape(dataMat))\n    # print('U[:, :4]', shape(U[:, :4]))\n    # print('Sig4.I', shape(Sig4.I))\n    # print('VT[:4, :]', shape(VT[:4, :]))\n    # print('xformedItems', shape(xformedItems))\n\n    # \u5bf9\u4e8e\u7ed9\u5b9a\u7684\u7528\u6237\uff0cfor\u5faa\u73af\u5728\u7528\u6237\u5bf9\u5e94\u884c\u7684\u5143\u7d20\u4e0a\u8fdb\u884c\u904d\u5386\n    # \u8fd9\u548cstandEst()\u51fd\u6570\u4e2d\u7684for\u5faa\u73af\u7684\u76ee\u7684\u4e00\u6837\uff0c\u53ea\u4e0d\u8fc7\u8fd9\u91cc\u7684\u76f8\u4f3c\u5ea6\u8ba1\u7b97\u65f6\u5728\u4f4e\u7ef4\u7a7a\u95f4\u4e0b\u8fdb\u884c\u7684\u3002\n    for j in range(n):\n        userRating = dataMat[user, j]\n        if userRating == 0 or j == item:\n            continue\n        # \u76f8\u4f3c\u5ea6\u7684\u8ba1\u7b97\u65b9\u6cd5\u4e5f\u4f1a\u4f5c\u4e3a\u4e00\u4e2a\u53c2\u6570\u4f20\u9012\u7ed9\u8be5\u51fd\u6570\n        similarity = simMeas(xformedItems[item, :].T, xformedItems[j, :].T)\n        # for \u5faa\u73af\u4e2d\u52a0\u5165\u4e86\u4e00\u6761print\u8bed\u53e5\uff0c\u4ee5\u4fbf\u4e86\u89e3\u76f8\u4f3c\u5ea6\u8ba1\u7b97\u7684\u8fdb\u5c55\u60c5\u51b5\u3002\u5982\u679c\u89c9\u5f97\u7d2f\u8d58\uff0c\u53ef\u4ee5\u53bb\u6389\n        print('the %d and %d similarity is: %f' % (item, j, similarity))\n        # \u5bf9\u76f8\u4f3c\u5ea6\u4e0d\u65ad\u7d2f\u52a0\u6c42\u548c\n        simTotal += similarity\n        # \u5bf9\u76f8\u4f3c\u5ea6\u53ca\u5bf9\u5e94\u8bc4\u5206\u503c\u7684\u4e58\u79ef\u6c42\u548c\n        ratSimTotal += similarity * userRating\n    if simTotal == 0:\n        return 0\n    else:\n        # \u8ba1\u7b97\u4f30\u8ba1\u8bc4\u5206\n        return ratSimTotal/simTotal\n\n\n# recommend()\u51fd\u6570\uff0c\u5c31\u662f\u63a8\u8350\u5f15\u64ce\uff0c\u5b83\u9ed8\u8ba4\u8c03\u7528standEst()\u51fd\u6570\uff0c\u4ea7\u751f\u4e86\u6700\u9ad8\u7684N\u4e2a\u63a8\u8350\u7ed3\u679c\u3002\n# \u5982\u679c\u4e0d\u6307\u5b9aN\u7684\u5927\u5c0f\uff0c\u5219\u9ed8\u8ba4\u503c\u4e3a3\u3002\u8be5\u51fd\u6570\u53e6\u5916\u7684\u53c2\u6570\u8fd8\u5305\u62ec\u76f8\u4f3c\u5ea6\u8ba1\u7b97\u65b9\u6cd5\u548c\u4f30\u8ba1\u65b9\u6cd5\ndef recommend(dataMat, user, N=3, simMeas=cosSim, estMethod=standEst):\n    \"\"\"svdEst( )\n    Args:\n        dataMat         \u8bad\u7ec3\u6570\u636e\u96c6\n        user            \u7528\u6237\u7f16\u53f7\n        simMeas         \u76f8\u4f3c\u5ea6\u8ba1\u7b97\u65b9\u6cd5\n        estMethod       \u4f7f\u7528\u7684\u63a8\u8350\u7b97\u6cd5\n    Returns:\n        \u8fd4\u56de\u6700\u7ec8 N \u4e2a\u63a8\u8350\u7ed3\u679c\n    \"\"\"\n    # \u5bfb\u627e\u672a\u8bc4\u7ea7\u7684\u7269\u54c1\n    #    nonzero\uff08a\uff09\u51fd\u6570\u4e00\u822c\u8fd4\u56de\u4e24\u884carray\uff08\uff09\u3002\u5982\u679cmat\uff08\uff09\u4e00\u4e0b\uff0c\u5c31\u662f\u4e2a2*N \u7684\u77e9\u9635\n    #    \u5176\u4e2d (array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), \n    #         array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]))\n    #    \u4f8b\u5982:  \u4e0a\u4e0b\u53d6\u51fa\u6765\u7684\u6570\u636e(0, 0) \u8868\u793a\u5728\u77e9\u9635\u7684\u6a2a\u7eb5\u5750\u6807(\u884c\u3001\u5217)\u7684\u4f4d\u7f6e\n    #    \u7531\u4e8e\u662f2\u4e3a\u77e9\u9635\uff0c\u6240\u4ee5 [1] \u5c31\u662f\u77e9\u9635\u5217\uff0c\u4e5f\u5c31\u662f\u5546\u54c1ID\u7684\u4e3a\u4e3b\n    # \u5bf9\u7ed9\u5b9a\u7684\u7528\u6237\u5efa\u7acb\u4e00\u4e2a\u672a\u8bc4\u5206\u7684\u7269\u54c1\u5217\u8868\n    unratedItems = nonzero(dataMat[user, :].A == 0)[1]\n    # \u5982\u679c\u4e0d\u5b58\u5728\u672a\u8bc4\u5206\u7269\u54c1\uff0c\u90a3\u4e48\u5c31\u9000\u51fa\u51fd\u6570\n    if len(unratedItems) == 0:\n        return 'you rated everything'\n    # \u7269\u54c1\u7684\u7f16\u53f7\u548c\u8bc4\u5206\u503c\n    itemScores = []\n    # \u5728\u672a\u8bc4\u5206\u7269\u54c1\u4e0a\u8fdb\u884c\u5faa\u73af\n    for item in unratedItems:\n        # \u83b7\u53d6 item \u8be5\u7269\u54c1\u7684\u8bc4\u5206\n        estimatedScore = estMethod(dataMat, user, simMeas, item)\n        itemScores.append((item, estimatedScore))\n    # \u6309\u7167\u8bc4\u5206\u5f97\u5206 \u8fdb\u884c\u9006\u6392\u5e8f\uff0c\u83b7\u53d6\u524dN\u4e2a\u672a\u8bc4\u7ea7\u7269\u54c1\u8fdb\u884c\u63a8\u8350\n    return sorted(itemScores, key=lambda jj: jj[1], reverse=True)[: N]\n\n\ndef analyse_data(Sigma, loopNum=20):\n    \"\"\"analyse_data(\u5206\u6790 Sigma \u7684\u957f\u5ea6\u53d6\u503c)\n    Args:\n        Sigma         Sigma\u7684\u503c\n        loopNum       \u5faa\u73af\u6b21\u6570\n    \"\"\"\n    # \u603b\u65b9\u5dee\u7684\u96c6\u5408\uff08\u603b\u80fd\u91cf\u503c\uff09\n    Sig2 = Sigma**2\n    SigmaSum = sum(Sig2)\n    for i in range(loopNum):\n        SigmaI = sum(Sig2[:i+1])\n        '''\n        \u6839\u636e\u81ea\u5df1\u7684\u4e1a\u52a1\u60c5\u51b5\uff0c\u5c31\u884c\u5904\u7406\uff0c\u8bbe\u7f6e\u5bf9\u5e94\u7684 Singma \u6b21\u6570\n        \u901a\u5e38\u4fdd\u7559\u77e9\u9635 80% \uff5e 90% \u7684\u80fd\u91cf\uff0c\u5c31\u53ef\u4ee5\u5f97\u5230\u91cd\u8981\u7684\u7279\u5f81\u5e76\u53d6\u51fa\u566a\u58f0\u3002\n        '''\n        print('\u4e3b\u6210\u5206: %s, \u65b9\u5dee\u5360\u6bd4: %s%%' % (format(i+1, '2.0f'), format(SigmaI/SigmaSum*100, '4.2f')))\n\n\n# \u56fe\u50cf\u538b\u7f29\u51fd\u6570\n# \u52a0\u8f7d\u5e76\u8f6c\u6362\u6570\u636e\ndef imgLoadData(filename):\n    myl = []\n    # \u6253\u5f00\u6587\u672c\u6587\u4ef6\uff0c\u5e76\u4ece\u6587\u4ef6\u4ee5\u6570\u7ec4\u65b9\u5f0f\u8bfb\u5165\u5b57\u7b26\n    for line in open(filename).readlines():\n        newRow = []\n        for i in range(32):\n            newRow.append(int(line[i]))\n        myl.append(newRow)\n    # \u77e9\u9635\u8c03\u5165\u540e\uff0c\u5c31\u53ef\u4ee5\u5728\u5c4f\u5e55\u4e0a\u8f93\u51fa\u8be5\u77e9\u9635\n    myMat = mat(myl)\n    return myMat\n\n\n# \u6253\u5370\u77e9\u9635\ndef printMat(inMat, thresh=0.8):\n    # \u7531\u4e8e\u77e9\u9635\u4fdd\u62a4\u4e86\u6d6e\u70b9\u6570\uff0c\u56e0\u6b64\u5b9a\u4e49\u6d45\u8272\u548c\u6df1\u8272\uff0c\u904d\u5386\u6240\u6709\u77e9\u9635\u5143\u7d20\uff0c\u5f53\u5143\u7d20\u5927\u4e8e\u9600\u503c\u65f6\u6253\u53701\uff0c\u5426\u5219\u6253\u53700\n    for i in range(32):\n        for k in range(32):\n            if float(inMat[i, k]) > thresh:\n                print(1,)\n            else:\n                print(0,)\n        print('')\n\n\n# \u5b9e\u73b0\u56fe\u50cf\u538b\u7f29\uff0c\u5141\u8bb8\u57fa\u4e8e\u4efb\u610f\u7ed9\u5b9a\u7684\u5947\u5f02\u503c\u6570\u76ee\u6765\u91cd\u6784\u56fe\u50cf\ndef imgCompress(numSV=3, thresh=0.8):\n    \"\"\"imgCompress( )\n    Args:\n        numSV       Sigma\u957f\u5ea6   \n        thresh      \u5224\u65ad\u7684\u9608\u503c\n    \"\"\"\n    # \u6784\u5efa\u4e00\u4e2a\u5217\u8868\n    myMat = imgLoadData('data/14.SVD/0_5.txt')\n\n    print(\"****original matrix****\")\n    # \u5bf9\u539f\u59cb\u56fe\u50cf\u8fdb\u884cSVD\u5206\u89e3\u5e76\u91cd\u6784\u56fe\u50cfe\n    printMat(myMat, thresh)\n\n    # \u901a\u8fc7Sigma \u91cd\u65b0\u6784\u6210SigRecom\u6765\u5b9e\u73b0\n    # Sigma\u662f\u4e00\u4e2a\u5bf9\u89d2\u77e9\u9635\uff0c\u56e0\u6b64\u9700\u8981\u5efa\u7acb\u4e00\u4e2a\u51680\u77e9\u9635\uff0c\u7136\u540e\u5c06\u524d\u9762\u7684\u90a3\u4e9b\u5947\u5f02\u503c\u586b\u5145\u5230\u5bf9\u89d2\u7ebf\u4e0a\u3002\n    U, Sigma, VT = la.svd(myMat)\n    # SigRecon = mat(zeros((numSV, numSV)))\n    # for k in range(numSV):\n    #     SigRecon[k, k] = Sigma[k]\n\n    # \u5206\u6790\u63d2\u5165\u7684 Sigma \u957f\u5ea6\n    analyse_data(Sigma, 20)\n\n    SigRecon = mat(eye(numSV) * Sigma[: numSV])\n    reconMat = U[:, :numSV] * SigRecon * VT[:numSV, :]\n    print(\"****reconstructed matrix using %d singular values *****\" % numSV)\n    printMat(reconMat, thresh)\n\n\nif __name__ == \"__main__\":\n\n    # # \u5bf9\u77e9\u9635\u8fdb\u884cSVD\u5206\u89e3(\u7528python\u5b9e\u73b0SVD)\n    # Data = loadExData()\n    # print('Data:', Data)\n    # U, Sigma, VT = linalg.svd(Data)\n    # # \u6253\u5370Sigma\u7684\u7ed3\u679c\uff0c\u56e0\u4e3a\u524d3\u4e2a\u6570\u503c\u6bd4\u5176\u4ed6\u7684\u503c\u5927\u4e86\u5f88\u591a\uff0c\u4e3a9.72140007e+00\uff0c5.29397912e+00\uff0c6.84226362e-01\n    # # \u540e\u4e24\u4e2a\u503c\u6bd4\u8f83\u5c0f\uff0c\u6bcf\u53f0\u673a\u5668\u8f93\u51fa\u7ed3\u679c\u53ef\u80fd\u6709\u4e0d\u540c\u53ef\u4ee5\u5c06\u8fd9\u4e24\u4e2a\u503c\u53bb\u6389\n    # print('U:', U)\n    # print('Sigma', Sigma)\n    # print('VT:', VT)\n    # print('VT:', VT.T)\n\n    # # \u91cd\u6784\u4e00\u4e2a3x3\u7684\u77e9\u9635Sig3\n    # Sig3 = mat([[Sigma[0], 0, 0], [0, Sigma[1], 0], [0, 0, Sigma[2]]])\n    # print(U[:, :3] * Sig3 * VT[:3, :])\n\n    \"\"\"\n    # \u8ba1\u7b97\u6b27\u6c0f\u8ddd\u79bb\n    myMat = mat(loadExData())\n    # print(myMat)\n    print(ecludSim(myMat[:, 0], myMat[:, 4]))\n    print(ecludSim(myMat[:, 0], myMat[:, 0]))\n    # \u8ba1\u7b97\u4f59\u5f26\u76f8\u4f3c\u5ea6\n    print(cosSim(myMat[:, 0], myMat[:, 4]))\n    print(cosSim(myMat[:, 0], myMat[:, 0]))\n    # \u8ba1\u7b97\u76ae\u5c14\u900a\u76f8\u5173\u7cfb\u6570\n    print(pearsSim(myMat[:, 0], myMat[:, 4]))\n    print(pearsSim(myMat[:, 0], myMat[:, 0]))\n    \"\"\"\n\n    # \u8ba1\u7b97\u76f8\u4f3c\u5ea6\u7684\u65b9\u6cd5\n    myMat = mat(loadExData3())\n    # print(myMat)\n\n    # \u65b9\u5f0f1: \u57fa\u4e8e\u7269\u54c1\u7684\u534f\u540c\u8fc7\u6ee4\uff0c\u901a\u8fc7\u4f59\u5f26\u76f8\u4f3c\u5ea6\u8ba1\u7b97\u76f8\u4f3c\u5ea6\n    # print(recommend(myMat, 1))\n    # \u65b9\u5f0f2: \u57fa\u4e8eSVD\uff0c\u901a\u8fc7\u4f59\u5f26\u76f8\u4f3c\u5ea6\u8ba1\u7b97\u76f8\u4f3c\u5ea6\n    print(recommend(myMat, 1, estMethod=svdEst))\n    # \u65b9\u5f0f3: \n    # print(recommend(myMat, 1, estMethod=svdEst, simMeas=pearsSim))\n\n    # # \u9ed8\u8ba4\u63a8\u8350\uff08\u83dc\u9986\u83dc\u80b4\u63a8\u8350\u793a\u4f8b\uff09\n    # print(recommend(myMat, 2))\n\n    # \"\"\"\n    # # \u5229\u7528SVD\u63d0\u9ad8\u63a8\u8350\u6548\u679c\n    # U, Sigma, VT = la.svd(mat(loadExData2()))\n    # print(Sigma)                 # \u8ba1\u7b97\u77e9\u9635\u7684SVD\u6765\u4e86\u89e3\u5176\u9700\u8981\u591a\u5c11\u7ef4\u7684\u7279\u5f81\n    # Sig2 = Sigma**2              # \u8ba1\u7b97\u9700\u8981\u591a\u5c11\u4e2a\u5947\u5f02\u503c\u80fd\u8fbe\u5230\u603b\u80fd\u91cf\u768490%\n    # print(sum(Sig2))             # \u8ba1\u7b97\u603b\u80fd\u91cf\n    # print(sum(Sig2) * 0.9)       # \u8ba1\u7b97\u603b\u80fd\u91cf\u768490%\n    # print(sum(Sig2[: 2]))        # \u8ba1\u7b97\u524d\u4e24\u4e2a\u5143\u7d20\u6240\u5305\u542b\u7684\u80fd\u91cf\n    # print(sum(Sig2[: 3]))        # \u4e24\u4e2a\u5143\u7d20\u7684\u80fd\u91cf\u503c\u5c0f\u4e8e\u603b\u80fd\u91cf\u768490%\uff0c\u4e8e\u662f\u8ba1\u7b97\u524d\u4e09\u4e2a\u5143\u7d20\u6240\u5305\u542b\u7684\u80fd\u91cf\n    # # \u8be5\u503c\u9ad8\u4e8e\u603b\u80fd\u91cf\u768490%\uff0c\u8fd9\u5c31\u53ef\u4ee5\u4e86\n    # \"\"\"\n\n    # # \u538b\u7f29\u56fe\u7247\n    # # imgCompress(2)", "src/py3.x/ml/5.Logistic/logistic.py": "#!/usr/bin/python\n# -*- coding:utf-8 -*-\n\n\"\"\"\nCreated on Oct 27, 2010\nUpdate  on 2017-05-18\nLogistic Regression Working Module\nAuthor: Peter Harrington/\u7f8a\u4e09/\u5c0f\u7476/BBruceyuan\nGitHub: https://github.com/apachecn/AiLearning\n\"\"\"\n\nimport numpy as np\n\n\n# ------\u4f7f\u7528 Logistic \u56de\u5f52\u5728\u7b80\u5355\u6570\u636e\u96c6\u4e0a\u7684\u5206\u7c7b-----------\n\ndef load_data_set():\n    \"\"\"\n    \u52a0\u8f7d\u6570\u636e\u96c6\n    :return:\u8fd4\u56de\u4e24\u4e2a\u6570\u7ec4\uff0c\u666e\u901a\u6570\u7ec4 \n        data_arr -- \u539f\u59cb\u6570\u636e\u7684\u7279\u5f81\n        label_arr -- \u539f\u59cb\u6570\u636e\u7684\u6807\u7b7e\uff0c\u4e5f\u5c31\u662f\u6bcf\u6761\u6837\u672c\u5bf9\u5e94\u7684\u7c7b\u522b\n    \"\"\"\n    data_arr = []\n    label_arr = []\n    f = open('data/5.Logistic/TestSet.txt', 'r')\n    for line in f.readlines():\n        line_arr = line.strip().split()\n        # \u4e3a\u4e86\u65b9\u4fbf\u8ba1\u7b97\uff0c\u6211\u4eec\u5c06 X0 \u7684\u503c\u8bbe\u4e3a 1.0 \uff0c\u4e5f\u5c31\u662f\u5728\u6bcf\u4e00\u884c\u7684\u5f00\u5934\u6dfb\u52a0\u4e00\u4e2a 1.0 \u4f5c\u4e3a X0\n        data_arr.append([1.0, np.float(line_arr[0]), np.float(line_arr[1])])\n        label_arr.append(int(line_arr[2]))\n    return data_arr, label_arr\n\n\ndef sigmoid(x):\n    # \u8fd9\u91cc\u5176\u5b9e\u975e\u5e38\u6709\u5fc5\u8981\u89e3\u91ca\u4e00\u4e0b\uff0c\u4f1a\u51fa\u73b0\u7684\u9519\u8bef RuntimeWarning: overflow encountered in exp\n    # \u8fd9\u4e2a\u9519\u8bef\u5728\u5b66\u4e60\u9636\u6bb5\u867d\u7136\u53ef\u4ee5\u5ffd\u7565\uff0c\u4f46\u662f\u6211\u4eec\u81f3\u5c11\u5e94\u8be5\u77e5\u9053\u4e3a\u4ec0\u4e48\n    # \u8fd9\u91cc\u662f\u56e0\u4e3a\u6211\u4eec\u8f93\u5165\u7684\u6709\u7684 x \u5b9e\u5728\u662f\u592a\u5c0f\u4e86\uff0c\u6bd4\u5982 -6000\u4e4b\u7c7b\u7684\uff0c\u90a3\u4e48\u8ba1\u7b97\u4e00\u4e2a\u6570\u5b57 np.exp(6000)\u8fd9\u4e2a\u7ed3\u679c\u592a\u5927\u4e86\uff0c\u6ca1\u6cd5\u8868\u793a\uff0c\u6240\u4ee5\u5c31\u6ea2\u51fa\u4e86\n    # \u5982\u679c\u662f\u8ba1\u7b97 np.exp\uff08-6000\uff09\uff0c\u8fd9\u6837\u867d\u7136\u4e5f\u4f1a\u6ea2\u51fa\uff0c\u4f46\u662f\u8fd9\u662f\u4e0b\u6ea2\uff0c\u5c31\u662f\u8868\u793a\u6210\u96f6\n    # \u53bb\u7f51\u4e0a\u641c\u4e86\u5f88\u591a\u65b9\u6cd5\uff0c\u6bd4\u5982 \u4f7f\u7528bigfloat\u8fd9\u4e2a\u5e93\uff08\u6211\u7adf\u7136\u6ca1\u6709\u5b89\u88c5\u6210\u529f\uff0c\u5c31\u4e0d\u5c1d\u8bd5\u4e86\uff0c\u53cd\u6b63\u5e94\u8be5\u662f\u6709\u7528\u7684\n    return 1.0 / (1 + np.exp(-x))\n\n\ndef grad_ascent(data_arr, class_labels):\n    \"\"\"\n    \u68af\u5ea6\u4e0a\u5347\u6cd5\uff0c\u5176\u5b9e\u5c31\u662f\u56e0\u4e3a\u4f7f\u7528\u4e86\u6781\u5927\u4f3c\u7136\u4f30\u8ba1\uff0c\u8fd9\u4e2a\u5927\u5bb6\u6709\u5fc5\u8981\u53bb\u770b\u63a8\u5bfc\uff0c\u53ea\u770b\u4ee3\u7801\u611f\u89c9\u4e0d\u592a\u591f\n    :param data_arr: \u4f20\u5165\u7684\u5c31\u662f\u4e00\u4e2a\u666e\u901a\u7684\u6570\u7ec4\uff0c\u5f53\u7136\u4f60\u4f20\u5165\u4e00\u4e2a\u4e8c\u7ef4\u7684ndarray\u4e5f\u884c\n    :param class_labels: class_labels \u662f\u7c7b\u522b\u6807\u7b7e\uff0c\u5b83\u662f\u4e00\u4e2a 1*100 \u7684\u884c\u5411\u91cf\u3002\n                    \u4e3a\u4e86\u4fbf\u4e8e\u77e9\u9635\u8ba1\u7b97\uff0c\u9700\u8981\u5c06\u8be5\u884c\u5411\u91cf\u8f6c\u6362\u4e3a\u5217\u5411\u91cf\uff0c\u505a\u6cd5\u662f\u5c06\u539f\u5411\u91cf\u8f6c\u7f6e\uff0c\u518d\u5c06\u5b83\u8d4b\u503c\u7ed9label_mat\n    :return: \n    \"\"\"\n    # \u6ce8\u610f\u4e00\u4e0b\uff0c\u6211\u628a\u539f\u6765 data_mat_in \u6539\u6210data_arr,\u56e0\u4e3a\u4f20\u8fdb\u6765\u7684\u662f\u4e00\u4e2a\u6570\u7ec4\uff0c\u7528\u8fd9\u4e2a\u6bd4\u8f83\u4e0d\u5bb9\u6613\u641e\u6df7\n    # turn the data_arr to numpy matrix\n    data_mat = np.mat(data_arr)\n    # \u53d8\u6210\u77e9\u9635\u4e4b\u540e\u8fdb\u884c\u8f6c\u7f6e\n    label_mat = np.mat(class_labels).transpose()\n    # m->\u6570\u636e\u91cf\uff0c\u6837\u672c\u6570 n->\u7279\u5f81\u6570\n    m, n = np.shape(data_mat)\n    # \u5b66\u4e60\u7387\uff0clearning rate\n    alpha = 0.001\n    # \u6700\u5927\u8fed\u4ee3\u6b21\u6570\uff0c\u5047\u88c5\u8fed\u4ee3\u8fd9\u4e48\u591a\u6b21\u5c31\u80fd\u6536\u655b2333\n    max_cycles = 500\n    # \u751f\u6210\u4e00\u4e2a\u957f\u5ea6\u548c\u7279\u5f81\u6570\u76f8\u540c\u7684\u77e9\u9635\uff0c\u6b64\u5904n\u4e3a3 -> [[1],[1],[1]]\n    # weights \u4ee3\u8868\u56de\u5f52\u7cfb\u6570\uff0c \u6b64\u5904\u7684 ones((n,1)) \u521b\u5efa\u4e00\u4e2a\u957f\u5ea6\u548c\u7279\u5f81\u6570\u76f8\u540c\u7684\u77e9\u9635\uff0c\u5176\u4e2d\u7684\u6570\u5168\u90e8\u90fd\u662f 1\n    weights = np.ones((n, 1))\n    for k in range(max_cycles):\n        # \u8fd9\u91cc\u662f\u70b9\u4e58  m x 3 dot 3 x 1\n        h = sigmoid(data_mat * weights)\n        error = label_mat - h\n        # \u8fd9\u91cc\u6bd4\u8f83\u5efa\u8bae\u770b\u4e00\u4e0b\u63a8\u5bfc\uff0c\u4e3a\u4ec0\u4e48\u8fd9\u4e48\u505a\u53ef\u4ee5\uff0c\u8fd9\u91cc\u5df2\u7ecf\u662f\u6c42\u5bfc\u4e4b\u540e\u7684\n        weights = weights + alpha * data_mat.transpose() * error\n    return weights\n\n\ndef plot_best_fit(weights):\n    \"\"\"\n    \u53ef\u89c6\u5316\n    :param weights: \n    :return: \n    \"\"\"\n    import matplotlib.pyplot as plt\n    data_mat, label_mat = load_data_set()\n    data_arr = np.array(data_mat)\n    n = np.shape(data_mat)[0]\n    x_cord1 = []\n    y_cord1 = []\n    x_cord2 = []\n    y_cord2 = []\n    for i in range(n):\n        if int(label_mat[i]) == 1:\n            x_cord1.append(data_arr[i, 1])\n            y_cord1.append(data_arr[i, 2])\n        else:\n            x_cord2.append(data_arr[i, 1])\n            y_cord2.append(data_arr[i, 2])\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    ax.scatter(x_cord1, y_cord1, s=30, color='k', marker='^')\n    ax.scatter(x_cord2, y_cord2, s=30, color='red', marker='s')\n    x = np.arange(-3.0, 3.0, 0.1)\n    y = (-weights[0] - weights[1] * x) / weights[2]\n    \"\"\"\n    y\u7684\u7531\u6765\uff0c\u5367\u69fd\uff0c\u662f\u4e0d\u662f\u6ca1\u770b\u61c2\uff1f\n    \u9996\u5148\u7406\u8bba\u4e0a\u662f\u8fd9\u4e2a\u6837\u5b50\u7684\u3002\n    dataMat.append([1.0, float(lineArr[0]), float(lineArr[1])])\n    w0*x0+w1*x1+w2*x2=f(x)\n    x0\u6700\u5f00\u59cb\u5c31\u8bbe\u7f6e\u4e3a1\u53fb\uff0c x2\u5c31\u662f\u6211\u4eec\u753b\u56fe\u7684y\u503c\uff0c\u800cf(x)\u88ab\u6211\u4eec\u78e8\u5408\u8bef\u5dee\u7ed9\u7b97\u5230w0,w1,w2\u8eab\u4e0a\u53bb\u4e86\n    \u6240\u4ee5:  w0+w1*x+w2*y=0 => y = (-w0-w1*x)/w2   \n    \"\"\"\n    ax.plot(x, y)\n    plt.xlabel('x1')\n    plt.ylabel('y1')\n    plt.show()\n\n\ndef stoc_grad_ascent0(data_mat, class_labels):\n    \"\"\"\n    \u968f\u673a\u68af\u5ea6\u4e0a\u5347\uff0c\u53ea\u4f7f\u7528\u4e00\u4e2a\u6837\u672c\u70b9\u6765\u66f4\u65b0\u56de\u5f52\u7cfb\u6570\n    :param data_mat: \u8f93\u5165\u6570\u636e\u7684\u6570\u636e\u7279\u5f81\uff08\u9664\u53bb\u6700\u540e\u4e00\u5217\uff09,ndarray\n    :param class_labels: \u8f93\u5165\u6570\u636e\u7684\u7c7b\u522b\u6807\u7b7e\uff08\u6700\u540e\u4e00\u5217\u6570\u636e\uff09\n    :return: \u5f97\u5230\u7684\u6700\u4f73\u56de\u5f52\u7cfb\u6570\n    \"\"\"\n    m, n = np.shape(data_mat)\n    alpha = 0.01\n    weights = np.ones(n)\n    for i in range(m):\n        # sum(data_mat[i]*weights)\u4e3a\u4e86\u6c42 f(x)\u7684\u503c\uff0c f(x)=a1*x1+b2*x2+..+nn*xn,\n        # \u6b64\u5904\u6c42\u51fa\u7684 h \u662f\u4e00\u4e2a\u5177\u4f53\u7684\u6570\u503c\uff0c\u800c\u4e0d\u662f\u4e00\u4e2a\u77e9\u9635\n        h = sigmoid(sum(data_mat[i] * weights))\n        error = class_labels[i] - h\n        # \u8fd8\u662f\u548c\u4e0a\u9762\u4e00\u6837\uff0c\u8fd9\u4e2a\u5148\u53bb\u770b\u63a8\u5bfc\uff0c\u518d\u5199\u7a0b\u5e8f\n        weights = weights + alpha * error * data_mat[i]\n    return weights\n\n\ndef stoc_grad_ascent1(data_mat, class_labels, num_iter=150):\n    \"\"\"\n    \u6539\u8fdb\u7248\u7684\u968f\u673a\u68af\u5ea6\u4e0a\u5347\uff0c\u4f7f\u7528\u968f\u673a\u7684\u4e00\u4e2a\u6837\u672c\u6765\u66f4\u65b0\u56de\u5f52\u7cfb\u6570\n    :param data_mat: \u8f93\u5165\u6570\u636e\u7684\u6570\u636e\u7279\u5f81\uff08\u9664\u53bb\u6700\u540e\u4e00\u5217\uff09,ndarray\n    :param class_labels: \u8f93\u5165\u6570\u636e\u7684\u7c7b\u522b\u6807\u7b7e\uff08\u6700\u540e\u4e00\u5217\u6570\u636e\n    :param num_iter: \u8fed\u4ee3\u6b21\u6570\n    :return: \u5f97\u5230\u7684\u6700\u4f73\u56de\u5f52\u7cfb\u6570\n    \"\"\"\n    m, n = np.shape(data_mat)\n    weights = np.ones(n)\n    for j in range(num_iter):\n        # \u8fd9\u91cc\u5fc5\u987b\u8981\u7528list\uff0c\u4e0d\u7136\u540e\u9762\u7684del\u6ca1\u6cd5\u4f7f\u7528\n        data_index = list(range(m))\n        for i in range(m):\n            # i\u548cj\u7684\u4e0d\u65ad\u589e\u5927\uff0c\u5bfc\u81f4alpha\u7684\u503c\u4e0d\u65ad\u51cf\u5c11\uff0c\u4f46\u662f\u4e0d\u4e3a0\n            alpha = 4 / (1.0 + j + i) + 0.01\n            # \u968f\u673a\u4ea7\u751f\u4e00\u4e2a 0\uff5elen()\u4e4b\u95f4\u7684\u4e00\u4e2a\u503c\n            # random.uniform(x, y) \u65b9\u6cd5\u5c06\u968f\u673a\u751f\u6210\u4e0b\u4e00\u4e2a\u5b9e\u6570\uff0c\u5b83\u5728[x,y]\u8303\u56f4\u5185,x\u662f\u8fd9\u4e2a\u8303\u56f4\u5185\u7684\u6700\u5c0f\u503c\uff0cy\u662f\u8fd9\u4e2a\u8303\u56f4\u5185\u7684\u6700\u5927\u503c\u3002\n            rand_index = int(np.random.uniform(0, len(data_index)))\n            h = sigmoid(np.sum(data_mat[data_index[rand_index]] * weights))\n            error = class_labels[data_index[rand_index]] - h\n            weights = weights + alpha * error * data_mat[data_index[rand_index]]\n            del(data_index[rand_index])\n    return weights\n\n\ndef test():\n    \"\"\"\n    \u8fd9\u4e2a\u51fd\u6570\u53ea\u8981\u5c31\u662f\u5bf9\u4e0a\u9762\u7684\u51e0\u4e2a\u7b97\u6cd5\u7684\u6d4b\u8bd5\uff0c\u8fd9\u6837\u5c31\u4e0d\u7528\u6bcf\u6b21\u90fd\u5728power shell \u91cc\u9762\u64cd\u4f5c\uff0c\u4e0d\u7136\u9ebb\u70e6\u6b7b\u4e86\n    :return: \n    \"\"\"\n    data_arr, class_labels = load_data_set()\n    # \u6ce8\u610f\uff0c\u8fd9\u91cc\u7684grad_ascent\u8fd4\u56de\u7684\u662f\u4e00\u4e2a matrix, \u6240\u4ee5\u8981\u4f7f\u7528getA\u65b9\u6cd5\u53d8\u6210ndarray\u7c7b\u578b\n    # weights = grad_ascent(data_arr, class_labels).getA()\n    # weights = stoc_grad_ascent0(np.array(data_arr), class_labels)\n    weights = stoc_grad_ascent1(np.array(data_arr), class_labels)\n    plot_best_fit(weights)\n\n\n# -------\u4ece\u759d\u6c14\u75c5\u75c7\u9884\u6d4b\u75c5\u9a6c\u7684\u6b7b\u4ea1\u7387------\n\n\ndef classify_vector(in_x, weights):\n    \"\"\"\n    \u6700\u7ec8\u7684\u5206\u7c7b\u51fd\u6570\uff0c\u6839\u636e\u56de\u5f52\u7cfb\u6570\u548c\u7279\u5f81\u5411\u91cf\u6765\u8ba1\u7b97 Sigmoid \u7684\u503c\uff0c\u5927\u4e8e0.5\u51fd\u6570\u8fd4\u56de1\uff0c\u5426\u5219\u8fd4\u56de0\n    :param in_x: \u7279\u5f81\u5411\u91cf\uff0cfeatures\n    :param weights: \u6839\u636e\u68af\u5ea6\u4e0b\u964d/\u968f\u673a\u68af\u5ea6\u4e0b\u964d \u8ba1\u7b97\u5f97\u5230\u7684\u56de\u5f52\u7cfb\u6570\n    :return: \n    \"\"\"\n    # print(np.sum(in_x * weights))\n    prob = sigmoid(np.sum(in_x * weights))\n    if prob > 0.5:\n        return 1.0\n    return 0.0\n\n\ndef colic_test():\n    \"\"\"\n    \u6253\u5f00\u6d4b\u8bd5\u96c6\u548c\u8bad\u7ec3\u96c6\uff0c\u5e76\u5bf9\u6570\u636e\u8fdb\u884c\u683c\u5f0f\u5316\u5904\u7406,\u5176\u5b9e\u6700\u4e3b\u8981\u7684\u7684\u90e8\u5206\uff0c\u6bd4\u5982\u7f3a\u5931\u503c\u7684\u8865\u5145\uff08\u771f\u7684\u9700\u8981\u5b66\u4f1a\u7684\uff09\uff0c\u4eba\u5bb6\u5df2\u7ecf\u505a\u4e86\n    :return: \n    \"\"\"\n    f_train = open('data/5.Logistic/HorseColicTraining.txt', 'r')\n    f_test = open('data/5.Logistic/HorseColicTest.txt', 'r')\n    training_set = []\n    training_labels = []\n    # \u89e3\u6790\u8bad\u7ec3\u6570\u636e\u96c6\u4e2d\u7684\u6570\u636e\u7279\u5f81\u548cLabels\n    # trainingSet \u4e2d\u5b58\u50a8\u8bad\u7ec3\u6570\u636e\u96c6\u7684\u7279\u5f81\uff0ctrainingLabels \u5b58\u50a8\u8bad\u7ec3\u6570\u636e\u96c6\u7684\u6837\u672c\u5bf9\u5e94\u7684\u5206\u7c7b\u6807\u7b7e\n    for line in f_train.readlines():\n        curr_line = line.strip().split('\\t')\n        if len(curr_line) == 1:\n            continue    # \u8fd9\u91cc\u5982\u679c\u5c31\u4e00\u4e2a\u7a7a\u7684\u5143\u7d20\uff0c\u5219\u8df3\u8fc7\u672c\u6b21\u5faa\u73af\n        line_arr = [float(curr_line[i]) for i in range(21)]\n        training_set.append(line_arr)\n        training_labels.append(float(curr_line[21]))\n    # \u4f7f\u7528 \u6539\u8fdb\u540e\u7684 \u968f\u673a\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5 \u6c42\u5f97\u5728\u6b64\u6570\u636e\u96c6\u4e0a\u7684\u6700\u4f73\u56de\u5f52\u7cfb\u6570 trainWeights\n    train_weights = stoc_grad_ascent1(np.array(training_set), training_labels, 500)\n    error_count = 0\n    num_test_vec = 0.0\n    # \u8bfb\u53d6 \u6d4b\u8bd5\u6570\u636e\u96c6 \u8fdb\u884c\u6d4b\u8bd5\uff0c\u8ba1\u7b97\u5206\u7c7b\u9519\u8bef\u7684\u6837\u672c\u6761\u6570\u548c\u6700\u7ec8\u7684\u9519\u8bef\u7387\n    for line in f_test.readlines():\n        num_test_vec += 1\n        curr_line = line.strip().split('\\t')\n        if len(curr_line) == 1: \n            continue    # \u8fd9\u91cc\u5982\u679c\u5c31\u4e00\u4e2a\u7a7a\u7684\u5143\u7d20\uff0c\u5219\u8df3\u8fc7\u672c\u6b21\u5faa\u73af\n        line_arr = [float(curr_line[i]) for i in range(21)]\n        if int(classify_vector(np.array(line_arr), train_weights)) != int(curr_line[21]):\n            error_count += 1\n    error_rate = error_count / num_test_vec\n    print('the error rate is {}'.format(error_rate))\n    return error_rate\n\n\ndef multi_test():\n    \"\"\"\n    \u8c03\u7528 colicTest() 10\u6b21\u5e76\u6c42\u7ed3\u679c\u7684\u5e73\u5747\u503c\n    :return: nothing \n    \"\"\"\n    num_tests = 10\n    error_sum = 0\n    for k in range(num_tests):\n        error_sum += colic_test()\n    print('after {} iteration the average error rate is {}'.format(num_tests, error_sum / num_tests))\n\n\nif __name__ == '__main__':\n    # \u8bf7\u4f9d\u6b21\u8fd0\u884c\u4e0b\u9762\u4e09\u4e2a\u51fd\u6570\u505a\u4ee3\u7801\u6d4b\u8bd5\n    test()\n    # colic_test()\n    # multi_test()\n\n", "src/py3.x/ml/5.Logistic/sklearn_logisticRegression_demo.py": "#!/usr/bin/python\n# -*- coding:utf-8 -*-\n\n'''\nCreated on Oct 27, 2010\nUpdate  on 2017-05-18\nLogistic Regression Working Module\nAuthor: \u5c0f\u7476\nGitHub: https://github.com/apachecn/AiLearning\nscikit-learn\u7684\u4f8b\u5b50\u5730\u5740: http://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n'''\n\n# \u903b\u8f91\u56de\u5f52\u4e2d\u7684 L1 \u60e9\u7f5a\u548c\u7a00\u7f3a\u6027 L1 Penalty and Sparsity in Logistic Regression\n'''\nprint(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import datasets\nfrom sklearn.preprocessing import StandardScaler\n\ndigits = datasets.load_digits()\n\nX, y = digits.data, digits.target\nX = StandardScaler().fit_transform(X)\n\n# \u5c06\u5927\u5c0f\u6570\u5b57\u5206\u7c7b\u4e3a\u5c0f\ny = (y > 4).astype(np.int)\n\n\n# \u8bbe\u7f6e\u6b63\u5219\u5316\u53c2\u6570\nfor i, C in enumerate((100, 1, 0.01)):\n    # \u51cf\u5c11\u8bad\u7ec3\u65f6\u95f4\u77ed\u7684\u5bb9\u5fcd\u5ea6\n    clf_l1_LR = LogisticRegression(C=C, penalty='l1', tol=0.01)\n    clf_l2_LR = LogisticRegression(C=C, penalty='l2', tol=0.01)\n    clf_l1_LR.fit(X, y)\n    clf_l2_LR.fit(X, y)\n\n    coef_l1_LR = clf_l1_LR.coef_.ravel()\n    coef_l2_LR = clf_l2_LR.coef_.ravel()\n\n    # coef_l1_LR contains zeros due to the\n    # L1 sparsity inducing norm\n    # \u7531\u4e8e L1 \u7a00\u758f\u8bf1\u5bfc\u89c4\u8303\uff0ccoef_l1_LR \u5305\u542b\u96f6\n\n    sparsity_l1_LR = np.mean(coef_l1_LR == 0) * 100\n    sparsity_l2_LR = np.mean(coef_l2_LR == 0) * 100\n\n    print(\"C=%.2f\" % C)\n    print(\"Sparsity with L1 penalty: %.2f%%\" % sparsity_l1_LR)\n    print(\"score with L1 penalty: %.4f\" % clf_l1_LR.score(X, y))\n    print(\"Sparsity with L2 penalty: %.2f%%\" % sparsity_l2_LR)\n    print(\"score with L2 penalty: %.4f\" % clf_l2_LR.score(X, y))\n\n    l1_plot = plt.subplot(3, 2, 2 * i + 1)\n    l2_plot = plt.subplot(3, 2, 2 * (i + 1))\n    if i == 0:\n        l1_plot.set_title(\"L1 penalty\")\n        l2_plot.set_title(\"L2 penalty\")\n\n    l1_plot.imshow(np.abs(coef_l1_LR.reshape(8, 8)), interpolation='nearest',\n                   cmap='binary', vmax=1, vmin=0)\n    l2_plot.imshow(np.abs(coef_l2_LR.reshape(8, 8)), interpolation='nearest',\n                   cmap='binary', vmax=1, vmin=0)\n    plt.text(-8, 3, \"C = %.2f\" % C)\n\n    l1_plot.set_xticks(())\n    l1_plot.set_yticks(())\n    l2_plot.set_xticks(())\n    l2_plot.set_yticks(())\n\nplt.show()\n'''\n\n# \u5177\u6709 L1-\u903b\u8f91\u56de\u5f52\u7684\u8def\u5f84\n'''\nprint(__doc__)\n\nfrom datetime import datetime\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn import linear_model\nfrom sklearn import datasets\nfrom sklearn.svm import l1_min_c\n\niris = datasets.load_iris()\nX = iris.data\ny = iris.target\n\nX = X[y != 2]\ny = y[y != 2]\n\nX -= np.mean(X, 0)\n\ncs = l1_min_c(X, y, loss='log') * np.logspace(0, 3)\n\n\nprint(\"Computing regularization path ...\")\nstart = datetime.now()\nclf = linear_model.LogisticRegression(C=1.0, penalty='l1', tol=1e-6)\ncoefs_ = []\nfor c in cs:\n    clf.set_params(C=c)\n    clf.fit(X, y)\n    coefs_.append(clf.coef_.ravel().copy())\nprint(\"This took \", datetime.now() - start)\n\ncoefs_ = np.array(coefs_)\nplt.plot(np.log10(cs), coefs_)\nymin, ymax = plt.ylim()\nplt.xlabel('log(C)')\nplt.ylabel('Coefficients')\nplt.title('Logistic Regression Path')\nplt.axis('tight')\nplt.show()\n'''\n\n# \u7ed8\u5236\u591a\u9879\u5f0f\u548c\u4e00\u5bf9\u4e8c\u7684\u903b\u8f91\u56de\u5f52 Plot multinomial and One-vs-Rest Logistic Regression\n'''\nprint(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_blobs\nfrom sklearn.linear_model import LogisticRegression\n\n# \u5236\u4f5c 3 \u7c7b\u6570\u636e\u96c6\u8fdb\u884c\u5206\u7c7b\ncenters = [[-5, 0], [0, 1.5], [5, -1]]\nX, y = make_blobs(n_samples=1000, centers=centers, random_state=40)\ntransformation = [[0.4, 0.2], [-0.4, 1.2]]\nX = np.dot(X, transformation)\n\nfor multi_class in ('multinomial', 'ovr'):\n    clf = LogisticRegression(solver='sag', max_iter=100, random_state=42,\n                             multi_class=multi_class).fit(X, y)\n\n    # \u6253\u5370\u8bad\u7ec3\u5206\u6570\n    print(\"training score : %.3f (%s)\" % (clf.score(X, y), multi_class))\n\n    # \u521b\u5efa\u4e00\u4e2a\u7f51\u683c\u6765\u7ed8\u5236\n    h = .02  # \u7f51\u683c\u4e2d\u7684\u6b65\u957f\n    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                         np.arange(y_min, y_max, h))\n\n    # \u7ed8\u5236\u51b3\u7b56\u8fb9\u754c\u3002\u4e3a\u6b64\uff0c\u6211\u4eec\u5c06\u4e3a\u7f51\u683c [x_min, x_max]x[y_min, y_max]\u4e2d\u7684\u6bcf\u4e2a\u70b9\u5206\u914d\u4e00\u4e2a\u989c\u8272\u3002\n    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n    # \u5c06\u7ed3\u679c\u653e\u5165\u5f69\u8272\u56fe\n    Z = Z.reshape(xx.shape)\n    plt.figure()\n    plt.contourf(xx, yy, Z, cmap=plt.cm.Paired)\n    plt.title(\"Decision surface of LogisticRegression (%s)\" % multi_class)\n    plt.axis('tight')\n\n    # \u5c06\u8bad\u7ec3\u70b9\u4e5f\u7ed8\u5236\u8fdb\u5165\n    colors = \"bry\"\n    for i, color in zip(clf.classes_, colors):\n        idx = np.where(y == i)\n        plt.scatter(X[idx, 0], X[idx, 1], c=color, cmap=plt.cm.Paired)\n\n    # \u7ed8\u5236\u4e09\u4e2a\u4e00\u5bf9\u6570\u5206\u7c7b\u5668\n    xmin, xmax = plt.xlim()\n    ymin, ymax = plt.ylim()\n    coef = clf.coef_\n    intercept = clf.intercept_\n\n    def plot_hyperplane(c, color):\n        def line(x0):\n            return (-(x0 * coef[c, 0]) - intercept[c]) / coef[c, 1]\n        plt.plot([xmin, xmax], [line(xmin), line(xmax)],\n                 ls=\"--\", color=color)\n\n    for i, color in zip(clf.classes_, colors):\n        plot_hyperplane(i, color)\n\nplt.show()\n'''\n\n# Logistic Regression 3-class Classifier \u903b\u8f91\u56de\u5f52 3-\u7c7b \u5206\u7c7b\u5668 \n\nprint(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import linear_model, datasets\n\n# \u5f15\u5165\u4e00\u4e9b\u6570\u636e\u6765\u73a9\niris = datasets.load_iris()\n# \u6211\u4eec\u53ea\u91c7\u7528\u6837\u672c\u6570\u636e\u7684\u524d\u4e24\u4e2afeature\nX = iris.data[:, :2]  \nY = iris.target\n\nh = .02  # \u7f51\u683c\u4e2d\u7684\u6b65\u957f\n\nlogreg = linear_model.LogisticRegression(C=1e5)\n\n# \u6211\u4eec\u521b\u5efa\u4e86\u4e00\u4e2a Neighbours Classifier \u7684\u5b9e\u4f8b\uff0c\u5e76\u62df\u5408\u6570\u636e\u3002\nlogreg.fit(X, Y)\n\n# \u7ed8\u5236\u51b3\u7b56\u8fb9\u754c\u3002\u4e3a\u6b64\u6211\u4eec\u5c06\u4e3a\u7f51\u683c [x_min, x_max]x[y_min, y_max] \u4e2d\u7684\u6bcf\u4e2a\u70b9\u5206\u914d\u4e00\u4e2a\u989c\u8272\u3002\nx_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\ny_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\nZ = logreg.predict(np.c_[xx.ravel(), yy.ravel()])\n\n# \u5c06\u7ed3\u679c\u653e\u5165\u5f69\u8272\u56fe\u4e2d\nZ = Z.reshape(xx.shape)\nplt.figure(1, figsize=(4, 3))\nplt.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired)\n\n# \u5c06\u8bad\u7ec3\u70b9\u4e5f\u540c\u6837\u653e\u5165\u5f69\u8272\u56fe\u4e2d\nplt.scatter(X[:, 0], X[:, 1], c=Y, edgecolors='k', cmap=plt.cm.Paired)\nplt.xlabel('Sepal length')\nplt.ylabel('Sepal width')\n\nplt.xlim(xx.min(), xx.max())\nplt.ylim(yy.min(), yy.max())\nplt.xticks(())\nplt.yticks(())\n\nplt.show()\n\n# Logistic function \u903b\u8f91\u56de\u5f52\u51fd\u6570\n# \u8fd9\u4e2a\u7c7b\u4f3c\u4e8e\u54b1\u4eec\u4e4b\u524d\u8bb2\u89e3 logistic \u56de\u5f52\u7684 Sigmoid \u51fd\u6570\uff0c\u6a21\u62df\u7684\u9636\u8dc3\u51fd\u6570\n\n'''\nprint(__doc__)\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom sklearn import linear_model\n\n# \u8fd9\u662f\u6211\u4eec\u7684\u6d4b\u8bd5\u96c6\uff0c\u5b83\u53ea\u662f\u4e00\u6761\u76f4\u7ebf\uff0c\u5e26\u6709\u4e00\u4e9b\u9ad8\u65af\u566a\u58f0\u3002\nxmin, xmax = -5, 5\nn_samples = 100\nnp.random.seed(0)\nX = np.random.normal(size=n_samples)\ny = (X > 0).astype(np.float)\nX[X > 0] *= 4\nX += .3 * np.random.normal(size=n_samples)\n\nX = X[:, np.newaxis]\n# \u8fd0\u884c\u5206\u7c7b\u5668\nclf = linear_model.LogisticRegression(C=1e5)\nclf.fit(X, y)\n\n# \u5e76\u4e14\u753b\u51fa\u6211\u4eec\u7684\u7ed3\u679c\nplt.figure(1, figsize=(4, 3))\nplt.clf()\nplt.scatter(X.ravel(), y, color='black', zorder=20)\nX_test = np.linspace(-5, 10, 300)\n\n\ndef model(x):\n    return 1 / (1 + np.exp(-x))\nloss = model(X_test * clf.coef_ + clf.intercept_).ravel()\nplt.plot(X_test, loss, color='red', linewidth=3)\n\nols = linear_model.LinearRegression()\nols.fit(X, y)\nplt.plot(X_test, ols.coef_ * X_test + ols.intercept_, linewidth=1)\nplt.axhline(.5, color='.5')\n\nplt.ylabel('y')\nplt.xlabel('X')\nplt.xticks(range(-5, 10))\nplt.yticks([0, 0.5, 1])\nplt.ylim(-.25, 1.25)\nplt.xlim(-4, 10)\nplt.legend(('Logistic Regression Model', 'Linear Regression Model'),\n           loc=\"lower right\", fontsize='small')\nplt.show()\n'''\n\n\n\n", "src/py3.x/dl/linear_unit.py": "#!/usr/bin/env python\n# -*- coding: UTF-8 -*-\n\n# \u5f15\u5165 Perceptron \u7c7b\nfrom perceptron import Perceptron\n\n# \u5b9a\u4e49\u6fc0\u6d3b\u51fd\u6570 f\nf = lambda x: x\n\nclass LinearUnit(Perceptron):\n    '''\n    Desc:\n        \u7ebf\u6027\u5355\u5143\u7c7b\n    Args:\n        Perceptron \u2014\u2014 \u611f\u77e5\u5668\n    Returns:\n        None\n    '''\n    def __init__(self, input_num):\n        '''\n        Desc:\n            \u521d\u59cb\u5316\u7ebf\u6027\u5355\u5143\uff0c\u8bbe\u7f6e\u8f93\u5165\u53c2\u6570\u7684\u4e2a\u6570\n        Args:\n            input_num \u2014\u2014 \u8f93\u5165\u53c2\u6570\u7684\u4e2a\u6570\n        Returns:\n            None\n        '''\n        # \u521d\u59cb\u5316\u6211\u4eec\u7684\u611f\u77e5\u5668\u7c7b\uff0c\u8bbe\u7f6e\u8f93\u5165\u53c2\u6570\u7684\u4e2a\u6570 input_num \u548c \u6fc0\u6d3b\u51fd\u6570 f\n        Perceptron.__init__(self, input_num, f)\n\n# \u6784\u9020\u7b80\u5355\u7684\u6570\u636e\u96c6\ndef get_training_dataset():\n    '''\n    Desc:\n        \u6784\u5efa\u4e00\u4e2a\u7b80\u5355\u7684\u8bad\u7ec3\u6570\u636e\u96c6\n    Args:\n        None\n    Returns:\n        input_vecs \u2014\u2014 \u8bad\u7ec3\u6570\u636e\u96c6\u7684\u7279\u5f81\u90e8\u5206\n        labels \u2014\u2014 \u8bad\u7ec3\u6570\u636e\u96c6\u7684\u6570\u636e\u5bf9\u5e94\u7684\u6807\u7b7e\uff0c\u662f\u4e00\u4e00\u5bf9\u5e94\u7684\n    '''\n    # \u6784\u5efa\u6570\u636e\u96c6\uff0c\u8f93\u5165\u5411\u91cf\u5217\u8868\uff0c\u6bcf\u4e00\u9879\u662f\u5de5\u4f5c\u5e74\u9650\n    input_vecs = [[5], [3], [8], [1.4], [10.1]]\n    # \u671f\u671b\u7684\u8f93\u51fa\u5217\u8868\uff0c\u4e5f\u5c31\u662f\u8f93\u5165\u5411\u91cf\u7684\u5bf9\u5e94\u7684\u6807\u7b7e\uff0c\u4e0e\u5de5\u4f5c\u5e74\u9650\u5bf9\u5e94\u7684\u6536\u5165\u5e74\u85aa\n    labels = [5500, 2300, 7600, 1800, 11400]\n    return input_vecs, labels\n\n\n# \u4f7f\u7528\u6211\u4eec\u7684\u8bad\u7ec3\u6570\u636e\u96c6\u5bf9\u7ebf\u6027\u5355\u5143\u8fdb\u884c\u8bad\u7ec3\ndef train_linear_unit():\n    '''\n    Desc:\n        \u4f7f\u7528\u8bad\u7ec3\u6570\u636e\u96c6\u5bf9\u6211\u4eec\u7684\u7ebf\u6027\u5355\u5143\u8fdb\u884c\u8bad\u7ec3\n    Args:\n        None\n    Returns:\n        lu \u2014\u2014 \u8fd4\u56de\u8bad\u7ec3\u597d\u7684\u7ebf\u6027\u5355\u5143\n    '''\n    # \u521b\u5efa\u611f\u77e5\u5668\u5bf9\u8c61\uff0c\u8f93\u5165\u53c2\u6570\u7684\u4e2a\u6570\u4e5f\u5c31\u662f\u7279\u5f81\u6570\u4e3a 1\uff08\u5de5\u4f5c\u5e74\u9650\uff09\n    lu = LinearUnit(1)\n    # \u83b7\u53d6\u6784\u5efa\u7684\u6570\u636e\u96c6\n    input_vecs, labels = get_training_dataset()\n    # \u8bad\u7ec3\u611f\u77e5\u5668\uff0c\u8fed\u4ee3 10 \u8f6e\uff0c\u5b66\u4e60\u7387\u4e3a 0.01\n    lu.train(input_vecs, labels, 10, 0.01)\n    # \u8fd4\u56de\u8bad\u7ec3\u597d\u7684\u7ebf\u6027\u5355\u5143\n    return lu\n\n\n# \u5c06\u56fe\u50cf\u753b\u51fa\u6765\ndef plot(linear_unit):\n    '''\n    Desc:\n        \u5c06\u6211\u4eec\u8bad\u7ec3\u597d\u7684\u7ebf\u6027\u5355\u5143\u5bf9\u6570\u636e\u7684\u5206\u7c7b\u60c5\u51b5\u4f5c\u56fe\u753b\u51fa\u6765\n    Args:\n        linear_unit \u2014\u2014 \u8bad\u7ec3\u597d\u7684\u7ebf\u6027\u5355\u5143\n    Returns:\n        None\n    '''\n    # \u5f15\u5165\u7ed8\u56fe\u7684\u5e93\n    import matplotlib.pyplot as plt\n    # \u83b7\u53d6\u8bad\u7ec3\u6570\u636e: \u7279\u5f81 input_vecs \u4e0e \u5bf9\u5e94\u7684\u6807\u7b7e labels\n    input_vecs, labels = get_training_dataset()\n    # figure() \u521b\u5efa\u4e00\u4e2a Figure \u5bf9\u8c61\uff0c\u4e0e\u7528\u6237\u4ea4\u4e92\u7684\u6574\u4e2a\u7a97\u53e3\uff0c\u8fd9\u4e2a figure \u4e2d\u5bb9\u7eb3\u7740 subplots\n    fig = plt.figure()\n    # \u5728 figure \u5bf9\u8c61\u4e2d\u521b\u5efa 1\u884c1\u5217\u4e2d\u7684\u7b2c\u4e00\u4e2a\u56fe\n    ax = fig.add_subplot(111)\n    # scatter(x, y) \u7ed8\u5236\u6563\u70b9\u56fe\uff0c\u5176\u4e2d\u7684 x,y \u662f\u76f8\u540c\u957f\u5ea6\u7684\u6570\u7ec4\u5e8f\u5217\n    \n    ax.scatter(list(map(lambda x: x[0], input_vecs)), labels)\n\n    # \u8bbe\u7f6e\u6743\u91cd\n    weights = linear_unit.weights\n    # \u8bbe\u7f6e\u504f\u7f6e\u9879\n    bias = linear_unit.bias\n    \n    y1 = 0*linear_unit.weights[0]+linear_unit.bias\n    y2 = 12*linear_unit.weights[0]+ linear_unit.bias\n    # \u5c06\u56fe\u753b\u51fa\u6765\n    plt.plot([0,12],[y1,y2])\n\n    # \u5c06\u6700\u7ec8\u7684\u56fe\u5c55\u793a\u51fa\u6765\n    plt.show()\n\n\nif __name__ == '__main__':\n    '''\n    Desc:\n        main \u51fd\u6570\uff0c\u8bad\u7ec3\u6211\u4eec\u7684\u7ebf\u6027\u5355\u5143\uff0c\u5e76\u8fdb\u884c\u9884\u6d4b\n    Args:\n        None\n    Returns:\n        None\n    '''\n    # \u9996\u5148\u8bad\u7ec3\u6211\u4eec\u7684\u7ebf\u6027\u5355\u5143\n    linear_unit = train_linear_unit()\n    # \u6253\u5370\u8bad\u7ec3\u83b7\u5f97\u7684\u6743\u91cd \u548c \u504f\u7f6e\n    print(linear_unit)\n    # \u6d4b\u8bd5\n    print('Work 3.4 years, monthly salary = %.2f' % linear_unit.predict([3.4]))\n    print('Work 15 years, monthly salary = %.2f' % linear_unit.predict([15]))\n    print('Work 1.5 years, monthly salary = %.2f' % linear_unit.predict([1.5]))\n    print('Work 6.3 years, monthly salary = %.2f' % linear_unit.predict([6.3]))\n    plot(linear_unit)\n\nfrom Perceptron import Perceptron\nfrom matplotlib import  pyplot as plt\n#\u5b9a\u4e49\u6fc0\u6d3b\u51fd\u6570f\nf = lambda x: x\nclass LinearUnit(Perceptron):\n    def __init__(self, input_num):\n        '''\u521d\u59cb\u5316\u7ebf\u6027\u5355\u5143\uff0c\u8bbe\u7f6e\u8f93\u5165\u53c2\u6570\u7684\u4e2a\u6570'''\n        Perceptron.__init__(self, input_num, f)\n\n\ndef get_train_dataset():\n    input_vecs = [[5],[3],[8],[1.4],[10.1]]\n    labels = [5500,2300,7600,1800,11400]\n    return input_vecs,labels\n\ndef train_linear_unit():\n    lu = LinearUnit(1)\n    input_vecs,labels = get_train_dataset()\n    lu.train(input_vecs,labels,10,0.01)\n    return  lu\n\n'''\n#\u753b\u56fe\u6a21\u5757\ndef plot(linear_unit):\n    import matplotlib.pyplot as plt\n    input_vecs, labels = get_training_dataset()\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    ax.scatter(map(lambda x: x[0], input_vecs), labels)\n    weights = linear_unit.weights\n    bias = linear_unit.bias\n    x = range(0,12,1)\n    y = map(lambda x:weights[0] * x + bias, x)\n    ax.plot(x, y)\n    plt.show()\n'''\n\nif __name__=='__main__':\n    linear_unit = train_linear_unit()\n    input_vecs,labels = get_train_dataset()\n    print(linear_unit)\n    print('Work 3.4 years, monthly salary = %.2f' % linear_unit.predict([3.4]))\n    print('Work 15 years, monthly salary = %.2f' % linear_unit.predict([15]))\n    print('Work 1.5 years, monthly salary = %.2f' % linear_unit.predict([1.5]))\n    print('Work 6.3 years, monthly salary = %.2f' % linear_unit.predict([6.3]))\n    print(linear_unit.weights)\n    plt.scatter(input_vecs,labels)\n    y1 = 0*linear_unit.weights[0]+linear_unit.bias\n    y2 = 12*linear_unit.weights[0]+ linear_unit.bias\n    plt.plot([0,12],[y1,y2])\n    plt.show()\n", "src/py3.x/dl/activators.py": "#!/usr/bin/env python\n# -*- coding: UTF-8 -*-\n\n\nimport numpy as np\n\n\nclass ReluActivator(object):\n    def forward(self, weighted_input):\n        #return weighted_input\n        return max(0, weighted_input)\n\n    def backward(self, output):\n        return 1 if output > 0 else 0\n\n\nclass IdentityActivator(object):\n    def forward(self, weighted_input):\n        return weighted_input\n\n    def backward(self, output):\n        return 1\n\n\nclass SigmoidActivator(object):\n    def forward(self, weighted_input):\n        return np.longfloat(1.0 / (1.0 + np.exp(-weighted_input)))\n\n    def backward(self, output):\n        return output * (1 - output)\n\n\nclass TanhActivator(object):\n    def forward(self, weighted_input):\n        return 2.0 / (1.0 + np.exp(-2 * weighted_input)) - 1.0\n\n    def backward(self, output):\n        return 1 - output * output", "src/py3.x/dl/lstm.py": "#!/usr/bin/env python\n# -*- coding: UTF-8 -*-\n\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom cnn import element_wise_op\nfrom activators import SigmoidActivator, TanhActivator, IdentityActivator\n\n\nclass LstmLayer(object):\n    def __init__(self, input_width, state_width, \n                 learning_rate):\n        self.input_width = input_width\n        self.state_width = state_width\n        self.learning_rate = learning_rate\n        # \u95e8\u7684\u6fc0\u6d3b\u51fd\u6570\n        self.gate_activator = SigmoidActivator()\n        # \u8f93\u51fa\u7684\u6fc0\u6d3b\u51fd\u6570\n        self.output_activator = TanhActivator()\n        # \u5f53\u524d\u65f6\u523b\u521d\u59cb\u5316\u4e3at0\n        self.times = 0       \n        # \u5404\u4e2a\u65f6\u523b\u7684\u5355\u5143\u72b6\u6001\u5411\u91cfc\n        self.c_list = self.init_state_vec()\n        # \u5404\u4e2a\u65f6\u523b\u7684\u8f93\u51fa\u5411\u91cfh\n        self.h_list = self.init_state_vec()\n        # \u5404\u4e2a\u65f6\u523b\u7684\u9057\u5fd8\u95e8f\n        self.f_list = self.init_state_vec()\n        # \u5404\u4e2a\u65f6\u523b\u7684\u8f93\u5165\u95e8i\n        self.i_list = self.init_state_vec()\n        # \u5404\u4e2a\u65f6\u523b\u7684\u8f93\u51fa\u95e8o\n        self.o_list = self.init_state_vec()\n        # \u5404\u4e2a\u65f6\u523b\u7684\u5373\u65f6\u72b6\u6001c~\n        self.ct_list = self.init_state_vec()\n        # \u9057\u5fd8\u95e8\u6743\u91cd\u77e9\u9635Wfh, Wfx, \u504f\u7f6e\u9879bf\n        self.Wfh, self.Wfx, self.bf = (\n            self.init_weight_mat())\n        # \u8f93\u5165\u95e8\u6743\u91cd\u77e9\u9635Wfh, Wfx, \u504f\u7f6e\u9879bf\n        self.Wih, self.Wix, self.bi = (\n            self.init_weight_mat())\n        # \u8f93\u51fa\u95e8\u6743\u91cd\u77e9\u9635Wfh, Wfx, \u504f\u7f6e\u9879bf\n        self.Woh, self.Wox, self.bo = (\n            self.init_weight_mat())\n        # \u5355\u5143\u72b6\u6001\u6743\u91cd\u77e9\u9635Wfh, Wfx, \u504f\u7f6e\u9879bf\n        self.Wch, self.Wcx, self.bc = (\n            self.init_weight_mat())\n\n    def init_state_vec(self):\n        '''\n        \u521d\u59cb\u5316\u4fdd\u5b58\u72b6\u6001\u7684\u5411\u91cf\n        '''\n        state_vec_list = []\n        state_vec_list.append(np.zeros(\n            (self.state_width, 1)))\n        return state_vec_list\n\n    def init_weight_mat(self):\n        '''\n        \u521d\u59cb\u5316\u6743\u91cd\u77e9\u9635\n        '''\n        Wh = np.random.uniform(-1e-4, 1e-4,\n            (self.state_width, self.state_width))\n        Wx = np.random.uniform(-1e-4, 1e-4,\n            (self.state_width, self.input_width))\n        b = np.zeros((self.state_width, 1))\n        return Wh, Wx, b\n\n    def forward(self, x):\n        '''\n        \u6839\u636e\u5f0f1-\u5f0f6\u8fdb\u884c\u524d\u5411\u8ba1\u7b97\n        '''\n        self.times += 1\n        # \u9057\u5fd8\u95e8\n        fg = self.calc_gate(x, self.Wfx, self.Wfh, \n            self.bf, self.gate_activator)\n        self.f_list.append(fg)\n        # \u8f93\u5165\u95e8\n        ig = self.calc_gate(x, self.Wix, self.Wih,\n            self.bi, self.gate_activator)\n        self.i_list.append(ig)\n        # \u8f93\u51fa\u95e8\n        og = self.calc_gate(x, self.Wox, self.Woh,\n            self.bo, self.gate_activator)\n        self.o_list.append(og)\n        # \u5373\u65f6\u72b6\u6001\n        ct = self.calc_gate(x, self.Wcx, self.Wch,\n            self.bc, self.output_activator)\n        self.ct_list.append(ct)\n        # \u5355\u5143\u72b6\u6001\n        c = fg * self.c_list[self.times - 1] + ig * ct\n        self.c_list.append(c)\n        # \u8f93\u51fa\n        h = og * self.output_activator.forward(c)\n        self.h_list.append(h)\n\n    def calc_gate(self, x, Wx, Wh, b, activator):\n        '''\n        \u8ba1\u7b97\u95e8\n        '''\n        h = self.h_list[self.times - 1] # \u4e0a\u6b21\u7684LSTM\u8f93\u51fa\n        net = np.dot(Wh, h) + np.dot(Wx, x) + b\n        gate = activator.forward(net)\n        return gate\n\n\n    def backward(self, x, delta_h, activator):\n        '''\n        \u5b9e\u73b0LSTM\u8bad\u7ec3\u7b97\u6cd5\n        '''\n        self.calc_delta(delta_h, activator)\n        self.calc_gradient(x)\n\n    def update(self):\n        '''\n        \u6309\u7167\u68af\u5ea6\u4e0b\u964d\uff0c\u66f4\u65b0\u6743\u91cd\n        '''\n        self.Wfh -= self.learning_rate * self.Whf_grad\n        self.Wfx -= self.learning_rate * self.Whx_grad\n        self.bf -= self.learning_rate * self.bf_grad\n        self.Wih -= self.learning_rate * self.Whi_grad\n        self.Wix -= self.learning_rate * self.Whi_grad\n        self.bi -= self.learning_rate * self.bi_grad\n        self.Woh -= self.learning_rate * self.Wof_grad\n        self.Wox -= self.learning_rate * self.Wox_grad\n        self.bo -= self.learning_rate * self.bo_grad\n        self.Wch -= self.learning_rate * self.Wcf_grad\n        self.Wcx -= self.learning_rate * self.Wcx_grad\n        self.bc -= self.learning_rate * self.bc_grad\n\n    def calc_delta(self, delta_h, activator):\n        # \u521d\u59cb\u5316\u5404\u4e2a\u65f6\u523b\u7684\u8bef\u5dee\u9879\n        self.delta_h_list = self.init_delta()  # \u8f93\u51fa\u8bef\u5dee\u9879\n        self.delta_o_list = self.init_delta()  # \u8f93\u51fa\u95e8\u8bef\u5dee\u9879\n        self.delta_i_list = self.init_delta()  # \u8f93\u5165\u95e8\u8bef\u5dee\u9879\n        self.delta_f_list = self.init_delta()  # \u9057\u5fd8\u95e8\u8bef\u5dee\u9879\n        self.delta_ct_list = self.init_delta() # \u5373\u65f6\u8f93\u51fa\u8bef\u5dee\u9879\n\n        # \u4fdd\u5b58\u4ece\u4e0a\u4e00\u5c42\u4f20\u9012\u4e0b\u6765\u7684\u5f53\u524d\u65f6\u523b\u7684\u8bef\u5dee\u9879\n        self.delta_h_list[-1] = delta_h\n        \n        # \u8fed\u4ee3\u8ba1\u7b97\u6bcf\u4e2a\u65f6\u523b\u7684\u8bef\u5dee\u9879\n        for k in range(self.times, 0, -1):\n            self.calc_delta_k(k)\n\n    def init_delta(self):\n        '''\n        \u521d\u59cb\u5316\u8bef\u5dee\u9879\n        '''\n        delta_list = []\n        for i in range(self.times + 1):\n            delta_list.append(np.zeros(\n                (self.state_width, 1)))\n        return delta_list\n\n    def calc_delta_k(self, k):\n        '''\n        \u6839\u636ek\u65f6\u523b\u7684delta_h\uff0c\u8ba1\u7b97k\u65f6\u523b\u7684delta_f\u3001\n        delta_i\u3001delta_o\u3001delta_ct\uff0c\u4ee5\u53cak-1\u65f6\u523b\u7684delta_h\n        '''\n        # \u83b7\u5f97k\u65f6\u523b\u524d\u5411\u8ba1\u7b97\u7684\u503c\n        ig = self.i_list[k]\n        og = self.o_list[k]\n        fg = self.f_list[k]\n        ct = self.ct_list[k]\n        c = self.c_list[k]\n        c_prev = self.c_list[k-1]\n        tanh_c = self.output_activator.forward(c)\n        delta_k = self.delta_h_list[k]\n\n        # \u6839\u636e\u5f0f9\u8ba1\u7b97delta_o\n        delta_o = (delta_k * tanh_c * \n            self.gate_activator.backward(og))\n        delta_f = (delta_k * og * \n            (1 - tanh_c * tanh_c) * c_prev *\n            self.gate_activator.backward(fg))\n        delta_i = (delta_k * og * \n            (1 - tanh_c * tanh_c) * ct *\n            self.gate_activator.backward(ig))\n        delta_ct = (delta_k * og * \n            (1 - tanh_c * tanh_c) * ig *\n            self.output_activator.backward(ct))\n        delta_h_prev = (\n                np.dot(delta_o.transpose(), self.Woh) +\n                np.dot(delta_i.transpose(), self.Wih) +\n                np.dot(delta_f.transpose(), self.Wfh) +\n                np.dot(delta_ct.transpose(), self.Wch)\n            ).transpose()\n\n        # \u4fdd\u5b58\u5168\u90e8delta\u503c\n        self.delta_h_list[k-1] = delta_h_prev\n        self.delta_f_list[k] = delta_f\n        self.delta_i_list[k] = delta_i\n        self.delta_o_list[k] = delta_o\n        self.delta_ct_list[k] = delta_ct\n\n    def calc_gradient(self, x):\n        # \u521d\u59cb\u5316\u9057\u5fd8\u95e8\u6743\u91cd\u68af\u5ea6\u77e9\u9635\u548c\u504f\u7f6e\u9879\n        self.Wfh_grad, self.Wfx_grad, self.bf_grad = (\n            self.init_weight_gradient_mat())\n        # \u521d\u59cb\u5316\u8f93\u5165\u95e8\u6743\u91cd\u68af\u5ea6\u77e9\u9635\u548c\u504f\u7f6e\u9879\n        self.Wih_grad, self.Wix_grad, self.bi_grad = (\n            self.init_weight_gradient_mat())\n        # \u521d\u59cb\u5316\u8f93\u51fa\u95e8\u6743\u91cd\u68af\u5ea6\u77e9\u9635\u548c\u504f\u7f6e\u9879\n        self.Woh_grad, self.Wox_grad, self.bo_grad = (\n            self.init_weight_gradient_mat())\n        # \u521d\u59cb\u5316\u5355\u5143\u72b6\u6001\u6743\u91cd\u68af\u5ea6\u77e9\u9635\u548c\u504f\u7f6e\u9879\n        self.Wch_grad, self.Wcx_grad, self.bc_grad = (\n            self.init_weight_gradient_mat())\n\n       # \u8ba1\u7b97\u5bf9\u4e0a\u4e00\u6b21\u8f93\u51fah\u7684\u6743\u91cd\u68af\u5ea6\n        for t in range(self.times, 0, -1):\n            # \u8ba1\u7b97\u5404\u4e2a\u65f6\u523b\u7684\u68af\u5ea6\n            (Wfh_grad, bf_grad,\n            Wih_grad, bi_grad,\n            Woh_grad, bo_grad,\n            Wch_grad, bc_grad) = (\n                self.calc_gradient_t(t))\n            # \u5b9e\u9645\u68af\u5ea6\u662f\u5404\u65f6\u523b\u68af\u5ea6\u4e4b\u548c\n            self.Wfh_grad += Wfh_grad\n            self.bf_grad += bf_grad\n            self.Wih_grad += Wih_grad\n            self.bi_grad += bi_grad\n            self.Woh_grad += Woh_grad\n            self.bo_grad += bo_grad\n            self.Wch_grad += Wch_grad\n            self.bc_grad += bc_grad\n\n        # \u8ba1\u7b97\u5bf9\u672c\u6b21\u8f93\u5165x\u7684\u6743\u91cd\u68af\u5ea6\n        xt = x.transpose()\n        self.Wfx_grad = np.dot(self.delta_f_list[-1], xt)\n        self.Wix_grad = np.dot(self.delta_i_list[-1], xt)\n        self.Wox_grad = np.dot(self.delta_o_list[-1], xt)\n        self.Wcx_grad = np.dot(self.delta_ct_list[-1], xt)\n\n    def init_weight_gradient_mat(self):\n        '''\n        \u521d\u59cb\u5316\u6743\u91cd\u77e9\u9635\n        '''\n        Wh_grad = np.zeros((self.state_width,\n            self.state_width))\n        Wx_grad = np.zeros((self.state_width,\n            self.input_width))\n        b_grad = np.zeros((self.state_width, 1))\n        return Wh_grad, Wx_grad, b_grad\n\n    def calc_gradient_t(self, t):\n        '''\n        \u8ba1\u7b97\u6bcf\u4e2a\u65f6\u523bt\u6743\u91cd\u7684\u68af\u5ea6\n        '''\n        h_prev = self.h_list[t-1].transpose()\n        Wfh_grad = np.dot(self.delta_f_list[t], h_prev)\n        bf_grad = self.delta_f_list[t]\n        Wih_grad = np.dot(self.delta_i_list[t], h_prev)\n        bi_grad = self.delta_f_list[t]\n        Woh_grad = np.dot(self.delta_o_list[t], h_prev)\n        bo_grad = self.delta_f_list[t]\n        Wch_grad = np.dot(self.delta_ct_list[t], h_prev)\n        bc_grad = self.delta_ct_list[t]\n        return Wfh_grad, bf_grad, Wih_grad, bi_grad, \\\n               Woh_grad, bo_grad, Wch_grad, bc_grad\n\n    def reset_state(self):\n        # \u5f53\u524d\u65f6\u523b\u521d\u59cb\u5316\u4e3at0\n        self.times = 0       \n        # \u5404\u4e2a\u65f6\u523b\u7684\u5355\u5143\u72b6\u6001\u5411\u91cfc\n        self.c_list = self.init_state_vec()\n        # \u5404\u4e2a\u65f6\u523b\u7684\u8f93\u51fa\u5411\u91cfh\n        self.h_list = self.init_state_vec()\n        # \u5404\u4e2a\u65f6\u523b\u7684\u9057\u5fd8\u95e8f\n        self.f_list = self.init_state_vec()\n        # \u5404\u4e2a\u65f6\u523b\u7684\u8f93\u5165\u95e8i\n        self.i_list = self.init_state_vec()\n        # \u5404\u4e2a\u65f6\u523b\u7684\u8f93\u51fa\u95e8o\n        self.o_list = self.init_state_vec()\n        # \u5404\u4e2a\u65f6\u523b\u7684\u5373\u65f6\u72b6\u6001c~\n        self.ct_list = self.init_state_vec()\n\n\ndef data_set():\n    x = [np.array([[1], [2], [3]]),\n         np.array([[2], [3], [4]])]\n    d = np.array([[1], [2]])\n    return x, d\n\n\ndef gradient_check():\n    '''\n    \u68af\u5ea6\u68c0\u67e5\n    '''\n    # \u8bbe\u8ba1\u4e00\u4e2a\u8bef\u5dee\u51fd\u6570\uff0c\u53d6\u6240\u6709\u8282\u70b9\u8f93\u51fa\u9879\u4e4b\u548c\n    error_function = lambda o: o.sum()\n    \n    lstm = LstmLayer(3, 2, 1e-3)\n\n    # \u8ba1\u7b97forward\u503c\n    x, d = data_set()\n    lstm.forward(x[0])\n    lstm.forward(x[1])\n    \n    # \u6c42\u53d6sensitivity map\n    sensitivity_array = np.ones(lstm.h_list[-1].shape,\n                                dtype=np.float64)\n    # \u8ba1\u7b97\u68af\u5ea6\n    lstm.backward(x[1], sensitivity_array, IdentityActivator())\n    \n    # \u68c0\u67e5\u68af\u5ea6\n    epsilon = 10e-4\n    for i in range(lstm.Wfh.shape[0]):\n        for j in range(lstm.Wfh.shape[1]):\n            lstm.Wfh[i,j] += epsilon\n            lstm.reset_state()\n            lstm.forward(x[0])\n            lstm.forward(x[1])\n            err1 = error_function(lstm.h_list[-1])\n            lstm.Wfh[i,j] -= 2*epsilon\n            lstm.reset_state()\n            lstm.forward(x[0])\n            lstm.forward(x[1])\n            err2 = error_function(lstm.h_list[-1])\n            expect_grad = (err1 - err2) / (2 * epsilon)\n            lstm.Wfh[i,j] += epsilon\n            print('weights(%d,%d): expected - actural %.4e - %.4e' % (\n                i, j, expect_grad, lstm.Wfh_grad[i,j]))\n    return lstm\n\n\ndef test():\n    l = LstmLayer(3, 2, 1e-3)\n    x, d = data_set()\n    l.forward(x[0])\n    l.forward(x[1])\n    l.backward(x[1], d, IdentityActivator())\n    return l\n\ndef test_gradient_check():\n    gradient_check()", "src/py3.x/dl/rnn.py": "#!/usr/bin/env python\n# -*- coding: UTF-8 -*-\n\n\nimport numpy as np\nfrom cnn import element_wise_op\nfrom functools import reduce\nfrom activators import ReluActivator, IdentityActivator\n\n\nclass RecurrentLayer(object):\n    def __init__(self, input_width, state_width,\n                 activator, learning_rate):\n        self.input_width = input_width\n        self.state_width = state_width\n        self.activator = activator\n        self.learning_rate = learning_rate\n        self.times = 0  # \u5f53\u524d\u65f6\u523b\u521d\u59cb\u5316\u4e3at0\n        self.state_list = []  # \u4fdd\u5b58\u5404\u4e2a\u65f6\u523b\u7684state\n        self.state_list.append(np.zeros(\n            (state_width, 1)))  # \u521d\u59cb\u5316s0\n        self.U = np.random.uniform(-1e-4, 1e-4,\n                                   (state_width, input_width))  # \u521d\u59cb\u5316U\n        self.W = np.random.uniform(-1e-4, 1e-4,\n                                   (state_width, state_width))  # \u521d\u59cb\u5316W\n\n    def forward(self, input_array):\n        '''\n        \u6839\u636e\u300e\u5f0f2\u300f\u8fdb\u884c\u524d\u5411\u8ba1\u7b97\n        '''\n        self.times += 1\n        state = (np.dot(self.U, input_array) +\n                 np.dot(self.W, self.state_list[-1]))\n        element_wise_op(state, self.activator.forward)\n        self.state_list.append(state)\n\n    def backward(self, sensitivity_array,\n                 activator):\n        '''\n        \u5b9e\u73b0BPTT\u7b97\u6cd5\n        '''\n        self.calc_delta(sensitivity_array, activator)\n        self.calc_gradient()\n\n    def update(self):\n        '''\n        \u6309\u7167\u68af\u5ea6\u4e0b\u964d\uff0c\u66f4\u65b0\u6743\u91cd\n        '''\n        self.W -= self.learning_rate * self.gradient\n\n    def calc_delta(self, sensitivity_array, activator):\n        self.delta_list = []  # \u7528\u6765\u4fdd\u5b58\u5404\u4e2a\u65f6\u523b\u7684\u8bef\u5dee\u9879\n        for i in range(self.times):\n            self.delta_list.append(np.zeros(\n                (self.state_width, 1)))\n        self.delta_list.append(sensitivity_array)\n        # \u8fed\u4ee3\u8ba1\u7b97\u6bcf\u4e2a\u65f6\u523b\u7684\u8bef\u5dee\u9879\n        for k in range(self.times - 1, 0, -1):\n            self.calc_delta_k(k, activator)\n\n    def calc_delta_k(self, k, activator):\n        '''\n        \u6839\u636ek+1\u65f6\u523b\u7684delta\u8ba1\u7b97k\u65f6\u523b\u7684delta\n        '''\n        state = self.state_list[k + 1].copy()\n        element_wise_op(self.state_list[k + 1],\n                        activator.backward)\n        self.delta_list[k] = np.dot(\n            np.dot(self.delta_list[k + 1].T, self.W),\n            np.diag(state[:, 0])).T\n\n    def calc_gradient(self):\n        self.gradient_list = []  # \u4fdd\u5b58\u5404\u4e2a\u65f6\u523b\u7684\u6743\u91cd\u68af\u5ea6\n        for t in range(self.times + 1):\n            self.gradient_list.append(np.zeros(\n                (self.state_width, self.state_width)))\n        for t in range(self.times, 0, -1):\n            self.calc_gradient_t(t)\n        # \u5b9e\u9645\u7684\u68af\u5ea6\u662f\u5404\u4e2a\u65f6\u523b\u68af\u5ea6\u4e4b\u548c\n        self.gradient = reduce(\n            lambda a, b: a + b, self.gradient_list,\n            self.gradient_list[0])  # [0]\u88ab\u521d\u59cb\u5316\u4e3a0\u4e14\u6ca1\u6709\u88ab\u4fee\u6539\u8fc7\n\n    def calc_gradient_t(self, t):\n        '''\n        \u8ba1\u7b97\u6bcf\u4e2a\u65f6\u523bt\u6743\u91cd\u7684\u68af\u5ea6\n        '''\n        gradient = np.dot(self.delta_list[t],\n                          self.state_list[t - 1].T)\n        self.gradient_list[t] = gradient\n\n    def reset_state(self):\n        self.times = 0  # \u5f53\u524d\u65f6\u523b\u521d\u59cb\u5316\u4e3at0\n        self.state_list = []  # \u4fdd\u5b58\u5404\u4e2a\u65f6\u523b\u7684state\n        self.state_list.append(np.zeros(\n            (self.state_width, 1)))  # \u521d\u59cb\u5316s0\n\n\ndef data_set():\n    x = [np.array([[1], [2], [3]]),\n         np.array([[2], [3], [4]])]\n    d = np.array([[1], [2]])\n    return x, d\n\n\ndef gradient_check():\n    '''\n    \u68af\u5ea6\u68c0\u67e5\n    '''\n    # \u8bbe\u8ba1\u4e00\u4e2a\u8bef\u5dee\u51fd\u6570\uff0c\u53d6\u6240\u6709\u8282\u70b9\u8f93\u51fa\u9879\u4e4b\u548c\n    error_function = lambda o: o.sum()\n\n    rl = RecurrentLayer(3, 2, IdentityActivator(), 1e-3)\n\n    # \u8ba1\u7b97forward\u503c\n    x, d = data_set()\n    rl.forward(x[0])\n    rl.forward(x[1])\n\n    # \u6c42\u53d6sensitivity map\n    sensitivity_array = np.ones(rl.state_list[-1].shape,\n                                dtype=np.float64)\n    # \u8ba1\u7b97\u68af\u5ea6\n    rl.backward(sensitivity_array, IdentityActivator())\n\n    # \u68c0\u67e5\u68af\u5ea6\n    epsilon = 10e-4\n    for i in range(rl.W.shape[0]):\n        for j in range(rl.W.shape[1]):\n            rl.W[i, j] += epsilon\n            rl.reset_state()\n            rl.forward(x[0])\n            rl.forward(x[1])\n            err1 = error_function(rl.state_list[-1])\n            rl.W[i, j] -= 2 * epsilon\n            rl.reset_state()\n            rl.forward(x[0])\n            rl.forward(x[1])\n            err2 = error_function(rl.state_list[-1])\n            expect_grad = (err1 - err2) / (2 * epsilon)\n            rl.W[i, j] += epsilon\n            print('weights(%d,%d): expected - actural %f - %f' % (\n                i, j, expect_grad, rl.gradient[i, j]))\n\n\ndef test():\n    l = RecurrentLayer(3, 2, ReluActivator(), 1e-3)\n    x, d = data_set()\n    l.forward(x[0])\n    l.forward(x[1])\n    l.backward(d, ReluActivator())\n    return l\n\n\n\n", "src/py3.x/dl/bp.py": "#!/usr/bin/env python\n# -*- coding: UTF-8 -*-\n\nimport random\nfrom functools import reduce\nfrom numpy import *\n\n# sigmoid \u51fd\u6570\ndef sigmoid(inX):\n    '''\n    Desc:\n        sigmoid \u51fd\u6570\u5b9e\u73b0\n    Args:\n        inX --- \u8f93\u5165\u5411\u91cf\n    Returns:\n        \u5bf9\u8f93\u5165\u5411\u91cf\u4f5c\u7528 sigmoid \u51fd\u6570\u4e4b\u540e\u5f97\u5230\u7684\u8f93\u51fa\n    '''\n    return 1.0 / (1 + exp(-inX))\n\n\n# \u5b9a\u4e49\u795e\u7ecf\u7f51\u7edc\u7684\u8282\u70b9\u7c7b\nclass Node(object):\n    '''\n    Desc:\n        \u795e\u7ecf\u7f51\u7edc\u7684\u8282\u70b9\u7c7b\n    '''\n    def __init__(self, layer_index, node_index):\n        '''\n        Desc:\n            \u521d\u59cb\u5316\u4e00\u4e2a\u8282\u70b9\n        Args:\n            layer_index --- \u5c42\u7684\u7d22\u5f15\uff0c\u4e5f\u5c31\u662f\u8868\u793a\u7b2c\u51e0\u5c42\n            node_index --- \u8282\u70b9\u7684\u7d22\u5f15\uff0c\u4e5f\u5c31\u662f\u8868\u793a\u8282\u70b9\u7684\u7d22\u5f15\n        Returns:\n            None\n        '''\n        # \u8bbe\u7f6e\u8282\u70b9\u6240\u5728\u7684\u5c42\u7684\u4f4d\u7f6e\n        self.layer_index = layer_index\n        # \u8bbe\u7f6e\u5c42\u4e2d\u7684\u8282\u70b9\u7684\u7d22\u5f15\n        self.node_index = node_index\n        # \u8bbe\u7f6e\u6b64\u8282\u70b9\u7684\u4e0b\u6e38\u8282\u70b9\uff0c\u4e5f\u5c31\u662f\u8fd9\u4e2a\u8282\u70b9\u4e0e\u4e0b\u4e00\u5c42\u7684\u54ea\u4e2a\u8282\u70b9\u76f8\u8fde\n        self.downstream = []\n        # \u8bbe\u7f6e\u6b64\u8282\u70b9\u7684\u4e0a\u6e38\u8282\u70b9\uff0c\u4e5f\u5c31\u662f\u54ea\u51e0\u4e2a\u8282\u70b9\u7684\u4e0b\u6e38\u8282\u70b9\u4e0e\u6b64\u8282\u70b9\u76f8\u8fde\n        self.upstream = []\n        # \u6b64\u8282\u70b9\u7684\u8f93\u51fa\n        self.output = 0\n        # \u6b64\u8282\u70b9\u771f\u5b9e\u503c\u4e0e\u8ba1\u7b97\u503c\u4e4b\u95f4\u7684\u5dee\u503c\n        self.delta = 0\n\n    def set_output(self, output):\n        '''\n        Desc:\n            \u8bbe\u7f6e\u8282\u70b9\u7684 output\n        Args:\n            output --- \u8282\u70b9\u7684 output\n        Returns:\n            None\n        '''\n        self.output = output\n\n    def append_downstream_connection(self, conn):\n        '''\n        Desc:\n           \u6dfb\u52a0\u6b64\u8282\u70b9\u7684\u4e0b\u6e38\u8282\u70b9\u7684\u8fde\u63a5\n        Args:\n            conn --- \u5f53\u524d\u8282\u70b9\u7684\u4e0b\u6e38\u8282\u70b9\u7684\u8fde\u63a5\u7684 list\n        Returns:\n            None\n        '''\n        # \u4f7f\u7528 list \u7684 append \u65b9\u6cd5\u6765\u5c06 conn \u4e2d\u7684\u8282\u70b9\u6dfb\u52a0\u5230 downstream \u4e2d\n        self.downstream.append(conn)\n\n    def append_upstream_connection(self, conn):\n        '''\n        Desc:\n            \u6dfb\u52a0\u6b64\u8282\u70b9\u7684\u4e0a\u6e38\u8282\u70b9\u7684\u8fde\u63a5\n        Args:\n            conn ---- \u5f53\u524d\u8282\u70b9\u7684\u4e0a\u6e38\u8282\u70b9\u7684\u8fde\u63a5\u7684 list\n        Returns:\n            None\n        '''\n        # \u4f7f\u7528 list \u7684 append \u65b9\u6cd5\u6765\u5c06 conn \u4e2d\u7684\u8282\u70b9\u6dfb\u52a0\u5230 upstream \u4e2d\n        self.upstream.append(conn)\n\n    def calc_output(self):\n        '''\n        Desc:\n            \u8ba1\u7b97\u8282\u70b9\u7684\u8f93\u51fa\uff0c\u4f9d\u636e output = sigmoid(wTx)\n        Args:\n            None\n        Returns:\n            None\n        '''\n        # \u4f7f\u7528 reduce() \u51fd\u6570\u5bf9\u5176\u4e2d\u7684\u56e0\u7d20\u6c42\u548c\n        output = reduce(lambda ret, conn: ret + conn.upstream_node.output * conn.weight, self.upstream, 0)\n        # \u5bf9\u4e0a\u6e38\u8282\u70b9\u7684 output \u4e58 weights \u4e4b\u540e\u6c42\u548c\u5f97\u5230\u7684\u7ed3\u679c\u5e94\u7528 sigmoid \u51fd\u6570\uff0c\u5f97\u5230\u5f53\u524d\u8282\u70b9\u7684 output\n        self.output = sigmoid(output)\n\n    def calc_hidden_layer_delta(self):\n        '''\n        Desc:\n            \u8ba1\u7b97\u9690\u85cf\u5c42\u7684\u8282\u70b9\u7684 delta\n        Args:\n            output --- \u8282\u70b9\u7684 output\n        Returns:\n            None\n        '''\n        # \u6839\u636e https://www.zybuluo.com/hanbingtao/note/476663 \u7684 \u5f0f4 \u8ba1\u7b97\u9690\u85cf\u5c42\u7684delta\n        downstream_delta = reduce(lambda ret, conn: ret + conn.downstream_node.delta * conn.weight, self.downstream, 0.0)\n        # \u8ba1\u7b97\u6b64\u8282\u70b9\u7684 delta\n        self.delta = self.output * (1 - self.output) * downstream_delta\n\n    def calc_output_layer_delta(self, label):\n        '''\n        Desc:\n            \u8ba1\u7b97\u8f93\u51fa\u5c42\u7684 delta\n        Args:\n            label --- \u8f93\u5165\u5411\u91cf\u5bf9\u5e94\u7684\u771f\u5b9e\u6807\u7b7e\uff0c\u4e0d\u662f\u8ba1\u7b97\u5f97\u5230\u7684\u7ed3\u679c\n        Returns:\n            None\n        '''\n        # \u5c31\u662f\u90a3\u8f93\u51fa\u5c42\u7684 delta\n        self.delta = self.output * (1 - self.output) * (label - self.output)\n\n    def __str__(self):\n        '''\n        Desc:\n            \u5c06\u8282\u70b9\u7684\u4fe1\u606f\u6253\u5370\u51fa\u6765\n        Args:\n            None\n        Returns:\n            None\n        '''\n        # \u6253\u5370\u683c\u5f0f: \u7b2c\u51e0\u5c42 - \u7b2c\u51e0\u4e2a\u8282\u70b9\uff0coutput \u662f\u591a\u5c11\uff0cdelta \u662f\u591a\u5c11\n        node_str = '%u-%u: output: %f delta: %f' % (self.layer_index, self.node_index, self.output, self.delta)\n        # \u4e0b\u6e38\u8282\u70b9\n        downstream_str = reduce(lambda ret, conn: ret + '\\n\\t' + str(conn), self.downstream, '')\n        # \u4e0a\u6e38\u8282\u70b9\n        upstream_str = reduce(lambda ret, conn: ret + '\\n\\t' + str(conn), self.upstream, '')\n        # \u5c06\u672c\u8282\u70b9 + \u4e0b\u6e38\u8282\u70b9 + \u4e0a\u6e38\u8282\u70b9 \u7684\u4fe1\u606f\u6253\u5370\u51fa\u6765\n        return node_str + '\\n\\tdownstream:' + downstream_str + '\\n\\tupstream:' + upstream_str\n\n\n# ConstNode \u5bf9\u8c61\uff0c\u4e3a\u4e86\u5b9e\u73b0\u4e00\u4e2a\u8f93\u51fa\u6052\u4e3a 1 \u7684\u8282\u70b9\uff08\u8ba1\u7b97\u504f\u7f6e\u9879 wb \u65f6\u9700\u8981\uff09\nclass ConstNode(object):\n    '''\n    Desc:\n        \u5e38\u6570\u9879\u5bf9\u8c61\uff0c\u5373\u76f8\u5f53\u4e8e\u8ba1\u7b97\u7684\u65f6\u5019\u7684\u504f\u7f6e\u9879\n    '''\n    def __init__(self, layer_index, node_index):\n        '''\n        Desc:\n            \u521d\u59cb\u5316\u8282\u70b9\u5bf9\u8c61\n        Args:\n            layer_index --- \u8282\u70b9\u6240\u5c5e\u7684\u5c42\u7684\u7f16\u53f7\n            node_index --- \u8282\u70b9\u7684\u7f16\u53f7\n        Returns:\n            None\n        '''    \n        self.layer_index = layer_index\n        self.node_index = node_index\n        self.downstream = []\n        self.output = 1\n\n\n    def append_downstream_connection(self, conn):\n        '''\n        Desc:\n            \u6dfb\u52a0\u4e00\u4e2a\u5230\u4e0b\u6e38\u8282\u70b9\u7684\u8fde\u63a5\n        Args:\n            conn --- \u5230\u4e0b\u6e38\u8282\u70b9\u7684\u8fde\u63a5                                           \n        Returns:\n            None\n        '''\n        # \u4f7f\u7528 list \u7684 append \u65b9\u6cd5\u5c06\u5305\u542b\u4e0b\u6e38\u8282\u70b9\u7684 conn \u6dfb\u52a0\u5230 downstream \u4e2d        \n        self.downstream.append(conn)\n\n\n    def calc_hidden_layer_delta(self):\n        '''\n        Desc:\n            \u8ba1\u7b97\u9690\u85cf\u5c42\u7684 delta\n        Args:\n            None\n        Returns:\n            None\n        '''\n        # \u4f7f\u7528\u6211\u4eec\u7684 \u516c\u5f0f 4 \u6765\u8ba1\u7b97\u4e0b\u6e38\u8282\u70b9\u7684 delta\uff0c\u6c42\u548c\n        downstream_delta = reduce(lambda ret, conn: ret + conn.downstream_node.delta * conn.weight, self.downstream, 0.0)\n        # \u8ba1\u7b97\u9690\u85cf\u5c42\u7684\u672c\u8282\u70b9\u7684 delta\n        self.delta = self.output * (1 - self.output) * downstream_delta\n\n\n    def __str__(self):\n        '''\n        Desc:\n           \u5c06\u8282\u70b9\u4fe1\u606f\u6253\u5370\u51fa\u6765\n        Args:\n            None\n        Returns:\n            None\n        '''\n        # \u5c06\u8282\u70b9\u7684\u4fe1\u606f\u6253\u5370\u51fa\u6765\n        # \u683c\u5f0f \u7b2c\u51e0\u5c42-\u7b2c\u51e0\u4e2a\u8282\u70b9\u7684 output \n        node_str = '%u-%u: output: 1' % (self.layer_index, self.node_index)\n        # \u6b64\u8282\u70b9\u7684\u4e0b\u6e38\u8282\u70b9\u7684\u4fe1\u606f\n        downstream_str = reduce(lambda ret, conn: ret + '\\n\\t' + str(conn), self.downstream, '')\n        # \u5c06\u6b64\u8282\u70b9\u4e0e\u4e0b\u6e38\u8282\u70b9\u7684\u4fe1\u606f\u7ec4\u5408\uff0c\u4e00\u8d77\u6253\u5370\u51fa\u6765\n        return node_str + '\\n\\tdownstream:' + downstream_str\n\n\n# \u795e\u7ecf\u7f51\u7edc\u7684\u5c42\u5bf9\u8c61\uff0c\u8d1f\u8d23\u521d\u59cb\u5316\u4e00\u5c42\u3002\u6b64\u5916\uff0c\u4f5c\u4e3a Node \u7684\u96c6\u5408\u5bf9\u8c61\uff0c\u63d0\u4f9b\u5bf9 Node \u96c6\u5408\u7684\u64cd\u4f5c\nclass Layer(object):\n    '''\n    Desc:\n        \u795e\u7ecf\u7f51\u7edc\u7684 Layer \u7c7b\n    '''\n\n    def __init__(self, layer_index, node_count):\n        '''\n        Desc:\n            \u795e\u7ecf\u7f51\u7edc\u7684\u5c42\u5bf9\u8c61\u7684\u521d\u59cb\u5316\n        Args:\n            layer_index --- \u5c42\u7684\u7d22\u5f15\n            node_count --- \u8282\u70b9\u7684\u4e2a\u6570\n        Returns:\n            None\n        '''\n        # \u8bbe\u7f6e \u5c42\u7684\u7d22\u5f15\n        self.layer_index = layer_index\n        # \u8bbe\u7f6e\u5c42\u4e2d\u7684\u8282\u70b9\u7684 list\n        self.nodes = []\n        # \u5c06 Node \u8282\u70b9\u6dfb\u52a0\u5230 nodes \u4e2d\n        for i in range(node_count):\n            self.nodes.append(Node(layer_index, i))\n        # \u5c06 ConstNode \u8282\u70b9\u4e5f\u6dfb\u52a0\u5230 nodes \u4e2d\n        self.nodes.append(ConstNode(layer_index, node_count))\n\n    def set_output(self, data):\n        '''\n        Desc:\n            \u8bbe\u7f6e\u5c42\u7684\u8f93\u51fa\uff0c\u5f53\u5c42\u662f\u8f93\u5165\u5c42\u65f6\u4f1a\u7528\u5230\n        Args:\n            data --- \u8f93\u51fa\u7684\u503c\u7684 list\n        Returns:\n            None\n        '''\n        # \u8bbe\u7f6e\u8f93\u5165\u5c42\u4e2d\u5404\u4e2a\u8282\u70b9\u7684 output\n        for i in range(len(data)):\n            self.nodes[i].set_output(data[i])\n\n    def calc_output(self):\n        '''\n        Desc:\n            \u8ba1\u7b97\u5c42\u7684\u8f93\u51fa\u5411\u91cf\n        Args:\n            None\n        Returns:\n            None\n        '''\n        # \u904d\u5386\u672c\u5c42\u7684\u6240\u6709\u8282\u70b9\uff08\u9664\u53bb\u6700\u540e\u4e00\u4e2a\u8282\u70b9\uff0c\u56e0\u4e3a\u5b83\u662f\u6052\u4e3a\u5e38\u6570\u7684\u504f\u7f6e\u9879b\uff09\n        # \u8c03\u7528\u8282\u70b9\u7684 calc_output \u65b9\u6cd5\u6765\u8ba1\u7b97\u8f93\u51fa\u5411\u91cf\n        for node in self.nodes[:-1]:\n            node.calc_output()\n\n    def dump(self):\n        '''\n        Desc:\n            \u5c06\u5c42\u4fe1\u606f\u6253\u5370\u51fa\u6765\n        Args:\n            None\n        Returns:\n            None\n        '''\n        # \u904d\u5386\u5c42\u7684\u6240\u6709\u7684\u8282\u70b9 nodes\uff0c\u5c06\u8282\u70b9\u4fe1\u606f\u6253\u5370\u51fa\u6765\n        for node in self.nodes:\n            print(node)\n\n\n# Connection \u5bf9\u8c61\u7c7b\uff0c\u4e3b\u8981\u8d1f\u8d23\u8bb0\u5f55\u8fde\u63a5\u7684\u6743\u91cd\uff0c\u4ee5\u53ca\u8fd9\u4e2a\u8fde\u63a5\u6240\u5173\u8054\u7684\u4e0a\u4e0b\u6e38\u7684\u8282\u70b9\nclass Connection(object):\n    '''\n    Desc:\n        Connection \u5bf9\u8c61\uff0c\u8bb0\u5f55\u8fde\u63a5\u6743\u91cd\u548c\u8fde\u63a5\u6240\u5173\u8054\u7684\u4e0a\u4e0b\u6e38\u8282\u70b9\uff0c\u6ce8\u610f\uff0c\u8fd9\u91cc\u7684 connection \u6ca1\u6709 s \uff0c\u4e0d\u662f\u590d\u6570\n    '''\n    def __init__(self, upstream_node, downstream_node):\n        '''\n        Desc:\n            \u521d\u59cb\u5316 Connection \u5bf9\u8c61\n        Args:\n            upstream_node --- \u4e0a\u6e38\u8282\u70b9\n            downstream_node --- \u4e0b\u6e38\u8282\u70b9\n        Returns:\n            None\n        '''\n        # \u8bbe\u7f6e\u4e0a\u6e38\u8282\u70b9\n        self.upstream_node = upstream_node\n        # \u8bbe\u7f6e\u4e0b\u6e38\u8282\u70b9\n        self.downstream_node = downstream_node\n        # \u8bbe\u7f6e\u6743\u91cd\uff0c\u8fd9\u91cc\u8bbe\u7f6e\u7684\u6743\u91cd\u662f -0.1 \u5230 0.1 \u4e4b\u95f4\u7684\u4efb\u4f55\u6570\n        self.weight = random.uniform(-0.1, 0.1)\n        # \u8bbe\u7f6e\u68af\u5ea6 \u4e3a 0.0\n        self.gradient = 0.0\n\n    def calc_gradient(self):\n        '''\n        Desc:\n            \u8ba1\u7b97\u68af\u5ea6\n        Args:\n            None\n        Returns:\n            None\n        '''\n        # \u4e0b\u6e38\u8282\u70b9\u7684 delta * \u4e0a\u6e38\u8282\u70b9\u7684 output \u8ba1\u7b97\u5f97\u5230\u68af\u5ea6\n        self.gradient = self.downstream_node.delta * self.upstream_node.output\n\n    def update_weight(self, rate):\n        '''\n        Desc:\n            \u6839\u636e\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5\u66f4\u65b0\u6743\u91cd\n        Args:\n            rate --- \u5b66\u4e60\u7387 / \u6216\u8005\u6210\u4e3a\u6b65\u957f\n        Returns:\n            None\n        '''\n        # \u8c03\u7528\u8ba1\u7b97\u68af\u5ea6\u7684\u51fd\u6570\u6765\u5c06\u68af\u5ea6\u8ba1\u7b97\u51fa\u6765\n        self.calc_gradient()\n        # \u4f7f\u7528\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5\u6765\u66f4\u65b0\u6743\u91cd\n        self.weight += rate * self.gradient\n\n    def get_gradient(self):\n        '''\n        Desc:\n            \u83b7\u53d6\u5f53\u524d\u7684\u68af\u5ea6\n        Args:\n            None\n        Returns:\n            \u5f53\u524d\u7684\u68af\u5ea6 gradient \n        '''\n        return self.gradient\n\n    def __str__(self):\n        '''\n        Desc:\n            \u5c06\u8fde\u63a5\u4fe1\u606f\u6253\u5370\u51fa\u6765\n        Args:\n            None\n        Returns:\n            \u8fde\u63a5\u4fe1\u606f\u8fdb\u884c\u8fd4\u56de\n        '''\n        # \u683c\u5f0f\u4e3a: \u4e0a\u6e38\u8282\u70b9\u7684\u5c42\u7684\u7d22\u5f15+\u4e0a\u6e38\u8282\u70b9\u7684\u8282\u70b9\u7d22\u5f15 ---> \u4e0b\u6e38\u8282\u70b9\u7684\u5c42\u7684\u7d22\u5f15+\u4e0b\u6e38\u8282\u70b9\u7684\u8282\u70b9\u7d22\u5f15\uff0c\u6700\u540e\u4e00\u4e2a\u6570\u662f\u6743\u91cd\n        return '(%u-%u) -> (%u-%u) = %f' % (\n            self.upstream_node.layer_index, \n            self.upstream_node.node_index,\n            self.downstream_node.layer_index, \n            self.downstream_node.node_index, \n            self.weight)\n\n\n\n# Connections \u5bf9\u8c61\uff0c\u63d0\u4f9b Connection \u96c6\u5408\u64cd\u4f5c\u3002\nclass Connections(object):\n    '''\n    Desc:\n        Connections \u5bf9\u8c61\uff0c\u63d0\u4f9b Connection \u96c6\u5408\u7684\u64cd\u4f5c\uff0c\u770b\u6e05\u695a\u540e\u9762\u6709\u6ca1\u6709 s \uff0c\u4e0d\u8981\u770b\u9519\n    '''\n    def __init__(self):\n        '''\n        Desc:\n            \u521d\u59cb\u5316 Connections \u5bf9\u8c61\n        Args:\n            None\n        Returns:\n            None\n        '''\n        # \u521d\u59cb\u5316\u4e00\u4e2a\u5217\u8868 list \n        self.connections = []\n\n    def add_connection(self, connection):\n        '''\n        Desc:\n            \u5c06 connection \u4e2d\u7684\u8282\u70b9\u4fe1\u606f append \u5230 connections \u4e2d\n        Args:\n            None\n        Returns:\n            None\n        '''\n        self.connections.append(connection)\n\n    def dump(self):\n        '''\n        Desc:\n            \u5c06 Connections \u7684\u8282\u70b9\u4fe1\u606f\u6253\u5370\u51fa\u6765\n        Args:\n            None\n        Returns:\n            None\n        '''\n        for conn in self.connections:\n            print(conn)\n\n\n# Network \u5bf9\u8c61\uff0c\u63d0\u4f9b\u76f8\u5e94 API\nclass Network(object):\n    '''\n    Desc:\n        Network \u7c7b\n    '''\n    def __init__(self, layers):\n        '''\n        Desc:\n            \u521d\u59cb\u5316\u4e00\u4e2a\u5168\u8fde\u63a5\u795e\u7ecf\u7f51\u7edc\n        Args:\n            layers --- \u4e8c\u7ef4\u6570\u7ec4\uff0c\u63cf\u8ff0\u795e\u7ecf\u7f51\u7edc\u7684\u6bcf\u5c42\u8282\u70b9\u6570\n        Returns:\n            None\n        '''\n        # \u521d\u59cb\u5316 connections\uff0c\u4f7f\u7528\u7684\u662f Connections \u5bf9\u8c61\n        self.connections = Connections()\n        # \u521d\u59cb\u5316 layers\n        self.layers = []\n        # \u6211\u4eec\u7684\u795e\u7ecf\u7f51\u7edc\u7684\u5c42\u6570\n        layer_count = len(layers)\n        # \u8282\u70b9\u6570\n        node_count = 0\n        # \u904d\u5386\u6240\u6709\u7684\u5c42\uff0c\u5c06\u6bcf\u5c42\u4fe1\u606f\u6dfb\u52a0\u5230 layers \u4e2d\u53bb\n        for i in range(layer_count):\n            self.layers.append(Layer(i, layers[i]))\n        # \u904d\u5386\u9664\u53bb\u8f93\u51fa\u5c42\u4e4b\u5916\u7684\u6240\u6709\u5c42\uff0c\u5c06\u8fde\u63a5\u4fe1\u606f\u6dfb\u52a0\u5230 connections \u5bf9\u8c61\u4e2d\n        for layer in range(layer_count - 1):\n            connections = [Connection(upstream_node, downstream_node) for upstream_node in self.layers[layer].nodes for downstream_node in self.layers[layer + 1].nodes[:-1]]\n            # \u904d\u5386 connections\uff0c\u5c06 conn \u6dfb\u52a0\u5230 connections \u4e2d\n            for conn in connections:\n                self.connections.add_connection(conn)\n                # \u4e3a\u4e0b\u6e38\u8282\u70b9\u6dfb\u52a0\u4e0a\u6e38\u8282\u70b9\u4e3a conn\n                conn.downstream_node.append_upstream_connection(conn)\n                # \u4e3a\u4e0a\u6e38\u8282\u70b9\u6dfb\u52a0\u4e0b\u6e38\u8282\u70b9\u4e3a conn\n                conn.upstream_node.append_downstream_connection(conn)\n\n\n    def train(self, labels, data_set, rate, epoch):\n        '''\n        Desc:\n            \u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\n        Args:\n            labels --- \u6570\u7ec4\uff0c\u8bad\u7ec3\u6837\u672c\u6807\u7b7e\uff0c\u6bcf\u4e2a\u5143\u7d20\u662f\u4e00\u4e2a\u6837\u672c\u7684\u6807\u7b7e\n            data_set --- \u4e8c\u7ef4\u6570\u7ec4\uff0c\u8bad\u7ec3\u6837\u672c\u7684\u7279\u5f81\u6570\u636e\u3002\u6bcf\u884c\u6570\u636e\u662f\u4e00\u4e2a\u6837\u672c\u7684\u7279\u5f81\n            rate --- \u5b66\u4e60\u7387\n            epoch --- \u8fed\u4ee3\u6b21\u6570\n        Returns:\n            None\n        '''\n        # \u5faa\u73af\u8fed\u4ee3 epoch \u6b21\n        for i in range(epoch):\n            # \u904d\u5386\u6bcf\u4e2a\u8bad\u7ec3\u6837\u672c\n            for d in range(len(data_set)):\n                # \u4f7f\u7528\u6b64\u6837\u672c\u8fdb\u884c\u8bad\u7ec3\uff08\u4e00\u6761\u6837\u672c\u8fdb\u884c\u8bad\u7ec3\uff09\n                self.train_one_sample(labels[d], data_set[d], rate)\n                # print 'sample %d training finished' % d\n\n    def train_one_sample(self, label, sample, rate):\n        '''\n        Desc:\n            \u5185\u90e8\u51fd\u6570\uff0c\u4f7f\u7528\u4e00\u4e2a\u6837\u672c\u5bf9\u7f51\u7edc\u8fdb\u884c\u8bad\u7ec3\n        Args:\n            label --- \u6837\u672c\u7684\u6807\u7b7e\n            sample --- \u6837\u672c\u7684\u7279\u5f81\n            rate --- \u5b66\u4e60\u7387\n        Returns:\n            None\n        '''\n        # \u8c03\u7528 Network \u7684 predict \u65b9\u6cd5\uff0c\u5bf9\u8fd9\u4e2a\u6837\u672c\u8fdb\u884c\u9884\u6d4b\n        self.predict(sample)\n        # \u8ba1\u7b97\u6839\u636e\u6b64\u6837\u672c\u5f97\u5230\u7684\u7ed3\u679c\u7684 delta\n        self.calc_delta(label)\n        # \u66f4\u65b0\u6743\u91cd\n        self.update_weight(rate)\n\n    def calc_delta(self, label):\n        '''\n        Desc:\n            \u8ba1\u7b97\u6bcf\u4e2a\u8282\u70b9\u7684 delta\n        Args:\n            label --- \u6837\u672c\u7684\u771f\u5b9e\u503c\uff0c\u4e5f\u5c31\u662f\u6837\u672c\u7684\u6807\u7b7e\n        Returns:\n            None\n        '''\n        # \u83b7\u53d6\u8f93\u51fa\u5c42\u7684\u6240\u6709\u8282\u70b9\n        output_nodes = self.layers[-1].nodes\n        # \u904d\u5386\u6240\u6709\u7684 label\n        for i in range(len(label)):\n            # \u8ba1\u7b97\u8f93\u51fa\u5c42\u8282\u70b9\u7684 delta\n            output_nodes[i].calc_output_layer_delta(label[i])\n        # \u8fd9\u4e2a\u7528\u6cd5\u5c31\u662f\u5207\u7247\u7684\u7528\u6cd5\uff0c [-2::-1] \u5c31\u662f\u5c06 layers \u8fd9\u4e2a\u6570\u7ec4\u5012\u8fc7\u6765\uff0c\u4ece\u6ca1\u5012\u8fc7\u6765\u7684\u65f6\u5019\u7684\u5012\u6570\u7b2c\u4e8c\u4e2a\u5143\u7d20\u5f00\u59cb\uff0c\u5230\u7ffb\u8f6c\u8fc7\u6765\u7684\u5012\u6570\u7b2c\u4e00\u4e2a\u6570\uff0c\u6bd4\u5982\u8fd9\u6837: aaa = [1,2,3,4,5,6,7,8,9],bbb = aaa[-2::-1] ==> bbb = [8, 7, 6, 5, 4, 3, 2, 1]\n        # \u5b9e\u9645\u4e0a\u5c31\u662f\u9664\u6389\u8f93\u51fa\u5c42\u4e4b\u5916\u7684\u6240\u6709\u5c42\u6309\u7167\u76f8\u53cd\u7684\u987a\u5e8f\u8fdb\u884c\u904d\u5386\n        for layer in self.layers[-2::-1]:\n            # \u904d\u5386\u6bcf\u5c42\u7684\u6240\u6709\u8282\u70b9\n            for node in layer.nodes:\n                # \u8ba1\u7b97\u9690\u85cf\u5c42\u7684 delta\n                node.calc_hidden_layer_delta()\n\n    def update_weight(self, rate):\n        '''\n        Desc:\n            \u66f4\u65b0\u6bcf\u4e2a\u8fde\u63a5\u7684\u6743\u91cd\n        Args:\n            rate --- \u5b66\u4e60\u7387\n        Returns:\n            None\n        '''\n        # \u6309\u7167\u6b63\u5e38\u987a\u5e8f\u904d\u5386\u9664\u4e86\u8f93\u51fa\u5c42\u7684\u5c42\n        for layer in self.layers[:-1]:\n            # \u904d\u5386\u6bcf\u5c42\u7684\u6240\u6709\u8282\u70b9\n            for node in layer.nodes:\n                # \u904d\u5386\u8282\u70b9\u7684\u4e0b\u6e38\u8282\u70b9\n                for conn in node.downstream:\n                    # \u6839\u636e\u4e0b\u6e38\u8282\u70b9\u6765\u66f4\u65b0\u8fde\u63a5\u7684\u6743\u91cd\n                    conn.update_weight(rate)\n\n    def calc_gradient(self):\n        '''\n        Desc:\n            \u8ba1\u7b97\u6bcf\u4e2a\u8fde\u63a5\u7684\u68af\u5ea6\n        Args:\n            None\n        Returns:\n            None\n        '''\n        # \u6309\u7167\u6b63\u5e38\u987a\u5e8f\u904d\u5386\u9664\u4e86\u8f93\u51fa\u5c42\u4e4b\u5916\u7684\u5c42\n        for layer in self.layers[:-1]:\n            # \u904d\u5386\u5c42\u4e2d\u7684\u6240\u6709\u8282\u70b9\n            for node in layer.nodes:\n                # \u904d\u5386\u8282\u70b9\u7684\u4e0b\u6e38\u8282\u70b9\n                for conn in node.downstream:\n                    # \u8ba1\u7b97\u68af\u5ea6\n                    conn.calc_gradient()\n\n    def get_gradient(self, label, sample):\n        '''\n        Desc:\n            \u83b7\u5f97\u7f51\u7edc\u5728\u4e00\u4e2a\u6837\u672c\u4e0b\uff0c\u6bcf\u4e2a\u8fde\u63a5\u4e0a\u7684\u68af\u5ea6\n        Args:\n            label --- \u6837\u672c\u6807\u7b7e\n            sample --- \u6837\u672c\u7279\u5f81\n        Returns:\n            None\n        '''\n        # \u8c03\u7528 predict() \u65b9\u6cd5\uff0c\u5229\u7528\u6837\u672c\u7684\u7279\u5f81\u6570\u636e\u5bf9\u6837\u672c\u8fdb\u884c\u9884\u6d4b\n        self.predict(sample)\n        # \u8ba1\u7b97 delta\n        self.calc_delta(label)\n        # \u8ba1\u7b97\u68af\u5ea6\n        self.calc_gradient()\n\n    def predict(self, sample):\n        '''\n        Desc:\n            \u6839\u636e\u8f93\u5165\u7684\u6837\u672c\u9884\u6d4b\u8f93\u51fa\u503c\n        Args:\n            sample --- \u6570\u7ec4\uff0c\u6837\u672c\u7684\u7279\u5f81\uff0c\u4e5f\u5c31\u662f\u7f51\u7edc\u7684\u8f93\u5165\u5411\u91cf\n        Returns:\n            \u4f7f\u7528\u6211\u4eec\u7684\u611f\u77e5\u5668\u89c4\u5219\u8ba1\u7b97\u7f51\u7edc\u7684\u8f93\u51fa\n        '''\n        # \u9996\u5148\u4e3a\u8f93\u5165\u5c42\u8bbe\u7f6e\u8f93\u51fa\u503coutput\u4e3a\u6837\u672c\u7684\u8f93\u5165\u5411\u91cf\uff0c\u5373\u4e0d\u53d1\u751f\u4efb\u4f55\u53d8\u5316\n        self.layers[0].set_output(sample)\n        # \u904d\u5386\u9664\u53bb\u8f93\u5165\u5c42\u5f00\u59cb\u5230\u6700\u540e\u4e00\u5c42\n        for i in range(1, len(self.layers)):\n            # \u8ba1\u7b97 output\n            self.layers[i].calc_output()\n        # \u5c06\u8ba1\u7b97\u5f97\u5230\u7684\u8f93\u51fa\uff0c\u4e5f\u5c31\u662f\u6211\u4eec\u7684\u9884\u6d4b\u503c\u8fd4\u56de\n        return list(map(lambda node: node.output, self.layers[-1].nodes[:-1]))\n\n    def dump(self):\n        '''\n        Desc:\n            \u6253\u5370\u51fa\u6211\u4eec\u7684\u7f51\u7edc\u4fe1\u606f\n        Args:\n            None\n        Returns:\n            None\n        '''\n        # \u904d\u5386\u6240\u6709\u7684 layers\n        for layer in self.layers:\n            # \u5c06\u6240\u6709\u7684\u5c42\u7684\u4fe1\u606f\u6253\u5370\u51fa\u6765\n            layer.dump()\n\n\n# # ------------------------- \u81f3\u6b64\uff0c\u57fa\u672c\u4e0a\u6211\u4eec\u628a \u6211\u4eec\u7684\u795e\u7ecf\u7f51\u7edc\u5b9e\u73b0\u5b8c\u6210\uff0c\u4e0b\u9762\u8fd8\u4f1a\u4ecb\u7ecd\u4e00\u4e0b\u5bf9\u5e94\u7684\u68af\u5ea6\u68c0\u67e5\u76f8\u5173\u7684\u7b97\u6cd5\uff0c\u73b0\u5728\u6211\u4eec\u9996\u5148\u56de\u987e\u4e00\u4e0b\u6211\u4eec\u4e0a\u9762\u5199\u9053\u7684\u7c7b\u53ca\u4ed6\u4eec\u7684\u4f5c\u7528 ------------------------\n'''\n1\u3001\u8282\u70b9\u7c7b\u7684\u5b9e\u73b0 Node : \u8d1f\u8d23\u8bb0\u5f55\u548c\u7ef4\u62a4\u8282\u70b9\u81ea\u8eab\u4fe1\u606f\u4ee5\u53ca\u8fd9\u4e2a\u8282\u70b9\u76f8\u5173\u7684\u4e0a\u4e0b\u6e38\u8fde\u63a5\uff0c\u5b9e\u73b0\u8f93\u51fa\u503c\u548c\u8bef\u5dee\u9879\u7684\u8ba1\u7b97\u3002\u5982\u4e0b: \nlayer_index --- \u8282\u70b9\u6240\u5c5e\u7684\u5c42\u7684\u7f16\u53f7\nnode_index --- \u8282\u70b9\u7684\u7f16\u53f7\ndownstream --- \u4e0b\u6e38\u8282\u70b9\nupstream  ---- \u4e0a\u6e38\u8282\u70b9\noutput    ---- \u8282\u70b9\u7684\u8f93\u51fa\u503c\ndelta   ------ \u8282\u70b9\u7684\u8bef\u5dee\u9879\n\n2\u3001ConstNode \u7c7b\uff0c\u504f\u7f6e\u9879\u7c7b\u7684\u5b9e\u73b0: \u5b9e\u73b0\u4e00\u4e2a\u8f93\u51fa\u6052\u4e3a 1 \u7684\u8282\u70b9\uff08\u8ba1\u7b97\u504f\u7f6e\u9879\u7684\u65f6\u5019\u4f1a\u7528\u5230\uff09\uff0c\u5982\u4e0b: \nlayer_index --- \u8282\u70b9\u6240\u5c5e\u5c42\u7684\u7f16\u53f7\nnode_index ---- \u8282\u70b9\u7684\u7f16\u53f7\ndownstream ---- \u4e0b\u6e38\u8282\u70b9\n\u6ca1\u6709\u8bb0\u5f55\u4e0a\u6e38\u8282\u70b9\uff0c\u56e0\u4e3a\u4e00\u4e2a\u504f\u7f6e\u9879\u7684\u8f93\u51fa\u4e0e\u4e0a\u6e38\u8282\u70b9\u7684\u8f93\u51fa\u65e0\u5173\noutput    ----- \u504f\u7f6e\u9879\u7684\u8f93\u51fa\n\n3\u3001layer \u7c7b\uff0c\u8d1f\u8d23\u521d\u59cb\u5316\u4e00\u5c42\u3002\u4f5c\u4e3a\u7684\u662f Node \u8282\u70b9\u7684\u96c6\u5408\u5bf9\u8c61\uff0c\u63d0\u4f9b\u5bf9 Node \u96c6\u5408\u7684\u64cd\u4f5c\u3002\u4e5f\u5c31\u662f\u8bf4\uff0clayer \u5305\u542b\u7684\u662f Node \u7684\u96c6\u5408\u3002\nlayer_index ---- \u5c42\u7684\u7f16\u53f7\nnode_count ----- \u5c42\u6240\u5305\u542b\u7684\u8282\u70b9\u7684\u4e2a\u6570\ndef set_ouput() -- \u8bbe\u7f6e\u5c42\u7684\u8f93\u51fa\uff0c\u5f53\u5c42\u662f\u8f93\u5165\u5c42\u65f6\u4f1a\u7528\u5230\ndef calc_output -- \u8ba1\u7b97\u5c42\u7684\u8f93\u51fa\u5411\u91cf\uff0c\u8c03\u7528\u7684 Node \u7c7b\u7684 \u8ba1\u7b97\u8f93\u51fa \u65b9\u6cd5\n\n4\u3001Connection \u7c7b: \u8d1f\u8d23\u8bb0\u5f55\u8fde\u63a5\u7684\u6743\u91cd\uff0c\u4ee5\u53ca\u8fd9\u4e2a\u8fde\u63a5\u6240\u5173\u8054\u7684\u4e0a\u4e0b\u6e38\u8282\u70b9\uff0c\u5982\u4e0b: \nupstream_node --- \u8fde\u63a5\u7684\u4e0a\u6e38\u8282\u70b9\ndownstream_node -- \u8fde\u63a5\u7684\u4e0b\u6e38\u8282\u70b9\nweight   -------- random.uniform(-0.1, 0.1) \u521d\u59cb\u5316\u4e3a\u4e00\u4e2a\u5f88\u5c0f\u7684\u968f\u673a\u6570\ngradient -------- 0.0 \u68af\u5ea6\uff0c\u521d\u59cb\u5316\u4e3a 0.0 \ndef calc_gradient() --- \u8ba1\u7b97\u68af\u5ea6\uff0c\u4f7f\u7528\u7684\u662f\u4e0b\u6e38\u8282\u70b9\u7684 delta \u4e0e\u4e0a\u6e38\u8282\u70b9\u7684 output \u76f8\u4e58\u8ba1\u7b97\u5f97\u5230\ndef get_gradient() ---- \u83b7\u53d6\u5f53\u524d\u7684\u68af\u5ea6\ndef update_weight() --- \u6839\u636e\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5\u66f4\u65b0\u6743\u91cd\n\n5\u3001Connections \u7c7b: \u63d0\u4f9b\u5bf9 Connection \u96c6\u5408\u64cd\u4f5c\uff0c\u5982\u4e0b: \ndef add_connection() --- \u6dfb\u52a0\u4e00\u4e2a connection\n\n6\u3001Network \u7c7b: \u63d0\u4f9b\u76f8\u5e94\u7684 API\uff0c\u5982\u4e0b: \nconnections --- Connections \u5bf9\u8c61\nlayers -------- \u795e\u7ecf\u7f51\u7edc\u7684\u5c42\nlayer_count --- \u795e\u7ecf\u7f51\u7edc\u7684\u5c42\u6570\nnode_count  --- \u8282\u70b9\u4e2a\u6570\ndef train() --- \u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\ndef train_one_sample() --- \u7528\u4e00\u4e2a\u6837\u672c\u8bad\u7ec3\u7f51\u7edc\ndef calc_delta() --- \u8ba1\u7b97\u8bef\u5dee\u9879\ndef update_weight() --- \u66f4\u65b0\u6bcf\u4e2a\u8fde\u63a5\u6743\u91cd\ndef calc_gradient() --- \u8ba1\u7b97\u6bcf\u4e2a\u8fde\u63a5\u7684\u68af\u5ea6\ndef get_gradient() --- \u83b7\u5f97\u7f51\u7edc\u5728\u4e00\u4e2a\u6837\u672c\u4e0b\uff0c\u6bcf\u4e2a\u8fde\u63a5\u4e0a\u7684\u68af\u5ea6\ndef predict() --- \u6839\u636e\u8f93\u5165\u7684\u6837\u672c\u9884\u6d4b\u8f93\u51fa\u503c \n'''\n\n# #--------------------------------------\u56de\u987e\u5b8c\u6210\u4e86\uff0c\u6709\u4e9b\u95ee\u9898\u53ef\u80fd\u8fd8\u662f\u6ca1\u6709\u5f04\u61c2\uff0c\u6ca1\u4e8b\uff0c\u6211\u4eec\u63a5\u7740\u770b\u4e0b\u9762---------------------------------------------\n\nclass Normalizer(object):\n    '''\n    Desc:\n        \u5f52\u4e00\u5316\u5de5\u5177\u7c7b\n    Args:\n        object --- \u5bf9\u8c61\n    Returns:\n        None\n    '''\n    def __init__(self):\n        '''\n        Desc:\n            \u521d\u59cb\u5316\n        Args:\n            None\n        Returns:\n            None\n        '''\n        # \u521d\u59cb\u5316 16 \u8fdb\u5236\u7684\u6570\uff0c\u7528\u6765\u5224\u65ad\u4f4d\u7684\uff0c\u5206\u522b\u662f\n        # 0x1 ---- 00000001\n        # 0x2 ---- 00000010\n        # 0x4 ---- 00000100\n        # 0x8 ---- 00001000\n        # 0x10 --- 00010000\n        # 0x20 --- 00100000\n        # 0x40 --- 01000000\n        # 0x80 --- 10000000\n        self.mask = [0x1, 0x2, 0x4, 0x8, 0x10, 0x20, 0x40, 0x80]\n\n    def norm(self, number):\n        '''\n        Desc:\n            \u5bf9 number \u8fdb\u884c\u89c4\u8303\u5316\n        Args:\n            number --- \u8981\u89c4\u8303\u5316\u7684\u6570\u636e\n        Returns:\n            \u89c4\u8303\u5316\u4e4b\u540e\u7684\u6570\u636e\n        '''\n        # \u6b64\u65b9\u6cd5\u5c31\u76f8\u5f53\u4e8e\u5224\u65ad\u4e00\u4e2a 8 \u4f4d\u7684\u5411\u91cf\uff0c\u54ea\u4e00\u4f4d\u4e0a\u6709\u6570\u5b57\uff0c\u5982\u679c\u6709\u5c31\u5c06\u8fd9\u4e2a\u6570\u8bbe\u7f6e\u4e3a  0.9 \uff0c\u5426\u5219\uff0c\u8bbe\u7f6e\u4e3a 0.1\uff0c\u901a\u4fd7\u6bd4\u8f83\u6765\u8bf4\uff0c\u5c31\u662f\u6211\u4eec\u8fd9\u91cc\u7528 0.9 \u8868\u793a 1\uff0c\u7528 0.1 \u8868\u793a 0\n        return list(map(lambda m: 0.9 if number & m else 0.1, self.mask))\n\n    def denorm(self, vec):\n        '''\n        Desc:\n            \u5bf9\u6211\u4eec\u5f97\u5230\u7684\u5411\u91cf\u8fdb\u884c\u53cd\u89c4\u8303\u5316\n        Args:\n            vec --- \u5f97\u5230\u7684\u5411\u91cf\n        Returns:\n            \u6700\u7ec8\u7684\u9884\u6d4b\u7ed3\u679c\n        '''\n        # \u8fdb\u884c\u4e8c\u5206\u7c7b\uff0c\u5927\u4e8e 0.5 \u5c31\u8bbe\u7f6e\u4e3a 1\uff0c\u5c0f\u4e8e 0.5 \u5c31\u8bbe\u7f6e\u4e3a 0\n        binary = list(map(lambda i: 1 if i > 0.5 else 0, vec))\n        # \u904d\u5386 mask\n        for i in range(len(self.mask)):\n            binary[i] = binary[i] * self.mask[i]\n        # \u5c06\u7ed3\u679c\u76f8\u52a0\u5f97\u5230\u6700\u7ec8\u7684\u9884\u6d4b\u7ed3\u679c\n        return reduce(lambda x,y: x + y, binary)\n\n\ndef mean_square_error(vec1, vec2):\n    '''\n    Desc:\n        \u8ba1\u7b97\u5e73\u5747\u5e73\u65b9\u8bef\u5dee\n    Args:\n        vec1 --- \u7b2c\u4e00\u4e2a\u6570\n        vec2 --- \u7b2c\u4e8c\u4e2a\u6570\n    Returns:\n        \u8fd4\u56de 1/2 * (x-y)^2 \u8ba1\u7b97\u5f97\u5230\u7684\u503c\n    '''\n    return 0.5 * reduce(lambda a, b: a + b, map(lambda v: (v[0] - v[1]) * (v[0] - v[1]), zip(vec1, vec2)))\n\n\n\ndef gradient_check(network, sample_feature, sample_label):\n    '''\n    Desc:\n        \u68af\u5ea6\u68c0\u67e5\n    Args:\n        network --- \u795e\u7ecf\u7f51\u7edc\u5bf9\u8c61\n        sample_feature --- \u6837\u672c\u7684\u7279\u5f81\n        sample_label --- \u6837\u672c\u7684\u6807\u7b7e   \n    Returns:\n        None\n    '''\n    # \u8ba1\u7b97\u7f51\u7edc\u8bef\u5dee\n    network_error = lambda vec1, vec2: 0.5 * reduce(lambda a, b: a + b, map(lambda v: (v[0] - v[1]) * (v[0] - v[1]), zip(vec1, vec2)))\n\n    # \u83b7\u53d6\u7f51\u7edc\u5728\u5f53\u524d\u6837\u672c\u4e0b\u6bcf\u4e2a\u8fde\u63a5\u7684\u68af\u5ea6\n    network.get_gradient(sample_feature, sample_label)\n\n    # \u5bf9\u6bcf\u4e2a\u6743\u91cd\u505a\u68af\u5ea6\u68c0\u67e5    \n    for conn in network.connections.connections: \n        # \u83b7\u53d6\u6307\u5b9a\u8fde\u63a5\u7684\u68af\u5ea6\n        actual_gradient = conn.get_gradient()\n    \n        # \u589e\u52a0\u4e00\u4e2a\u5f88\u5c0f\u7684\u503c\uff0c\u8ba1\u7b97\u7f51\u7edc\u7684\u8bef\u5dee\n        epsilon = 0.0001\n        conn.weight += epsilon\n        error1 = network_error(network.predict(sample_feature), sample_label)\n    \n        # \u51cf\u53bb\u4e00\u4e2a\u5f88\u5c0f\u7684\u503c\uff0c\u8ba1\u7b97\u7f51\u7edc\u7684\u8bef\u5dee\n        conn.weight -= 2 * epsilon # \u521a\u624d\u52a0\u8fc7\u4e86\u4e00\u6b21\uff0c\u56e0\u6b64\u8fd9\u91cc\u9700\u8981\u51cf\u53bb2\u500d\n        error2 = network_error(network.predict(sample_feature), sample_label)\n    \n        # \u6839\u636e\u5f0f6\u8ba1\u7b97\u671f\u671b\u7684\u68af\u5ea6\u503c\n        expected_gradient = (error2 - error1) / (2 * epsilon)\n    \n        # \u6253\u5370\n        print('expected gradient: \\t%f\\nactual gradient: \\t%f' % (expected_gradient, actual_gradient))\n\n\ndef train_data_set():\n    '''\n    Desc:\n        \u83b7\u53d6\u8bad\u7ec3\u6570\u636e\u96c6\n    Args:\n        None\n    Returns:\n        labels --- \u8bad\u7ec3\u6570\u636e\u96c6\u6bcf\u6761\u6570\u636e\u5bf9\u5e94\u7684\u6807\u7b7e\n    '''\n    # \u8c03\u7528 Normalizer() \u7c7b\n    normalizer = Normalizer()\n    # \u521d\u59cb\u5316\u4e00\u4e2a list\uff0c\u7528\u6765\u5b58\u50a8\u540e\u9762\u7684\u6570\u636e\n    data_set = []\n    labels = []\n    # 0 \u5230 256 \uff0c\u5176\u4e2d\u4ee5 8 \u4e3a\u6b65\u957f\n    for i in range(0, 256, 8):\n        # \u8c03\u7528 normalizer \u5bf9\u8c61\u7684 norm \u65b9\u6cd5\n        n = normalizer.norm(int(random.uniform(0, 256)))\n        # \u5728 data_set \u4e2d append n\n        data_set.append(n)\n        # \u5728 labels \u4e2d append n\n        labels.append(n)\n    # \u5c06\u5b83\u4eec\u8fd4\u56de\n    return labels, data_set\n\n\ndef train(network):\n    '''\n    Desc:\n        \u4f7f\u7528\u6211\u4eec\u7684\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u8bad\u7ec3\n    Args:\n        network --- \u795e\u7ecf\u7f51\u7edc\u5bf9\u8c61\n    Returns:\n        None\n    '''\n    # \u83b7\u53d6\u8bad\u7ec3\u6570\u636e\u96c6\n    labels, data_set = train_data_set()\n    labels = list(labels)\n    data_set = list(labels)\n    # \u8c03\u7528 network \u4e2d\u7684 train\u65b9\u6cd5\u6765\u8bad\u7ec3\u6211\u4eec\u7684\u795e\u7ecf\u7f51\u7edc\n    network.train(labels, data_set, 0.3, 50)\n\n\ndef test(net,data):\n    #\u6b64\u51fd\u6570\u4e0d\u660e\u89c9\u5389\uff0c\u4f46\u662f\u4f20\u53c2\u5c31\u6709\u95ee\u9898\uff0c\u5982\u679c\u8dd1\u4e0d\u901a\u5c31\u628a\u8fd9\u6bb5\u4ee3\u7801\u6ce8\u91ca\u6389\u5427\u3002\u3002\u3002\n\n    '''\n    Desc:\n        \u5bf9\u6211\u4eec\u7684\u5168\u8fde\u63a5\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u6d4b\u8bd5\n    Args:\n        network --- \u795e\u7ecf\u7f51\u7edc\u5bf9\u8c61\n        data ------ \u6d4b\u8bd5\u6570\u636e\u96c6\n    Returns:\n        None\n    '''\n    # \u8c03\u7528 Normalizer() \u7c7b\n\n    normalizer = Normalizer()\n    # \u8c03\u7528 norm \u65b9\u6cd5\uff0c\u5bf9\u6570\u636e\u8fdb\u884c\u89c4\u8303\u5316\n    norm_data = normalizer.norm(data)\n    norm_data = list(norm_data)\n    # \u5bf9\u6d4b\u8bd5\u6570\u636e\u8fdb\u884c\u9884\u6d4b\n    predict_data = net.predict(norm_data)\n    # \u5c06\u7ed3\u679c\u6253\u5370\u51fa\u6765\n    print('\\ttestdata(%u)\\tpredict(%u)' % (data, normalizer.denorm(predict_data)))\n\n\ndef correct_ratio(network):\n    '''\n    Desc:\n        \u8ba1\u7b97\u6211\u4eec\u7684\u795e\u7ecf\u7f51\u7edc\u7684\u6b63\u786e\u7387\n    Args:\n        network --- \u795e\u7ecf\u7f51\u7edc\u5bf9\u8c61\n    Returns:\n        None\n    '''\n    normalizer = Normalizer()\n    correct = 0.0\n    for i in range(256):\n        if normalizer.denorm(network.predict(normalizer.norm(i))) == i:\n            correct += 1.0\n    print('correct_ratio: %.2f%%' % (correct / 256 * 100))\n\n\ndef gradient_check_test():\n    '''\n    Desc:\n        \u68af\u5ea6\u68c0\u67e5\u6d4b\u8bd5\n    Args:\n        None\n    Returns:\n        None\n    '''\n    # \u521b\u5efa\u4e00\u4e2a\u6709 3 \u5c42\u7684\u7f51\u7edc\uff0c\u6bcf\u5c42\u6709 2 \u4e2a\u8282\u70b9\n    net = Network([2, 2, 2])\n    # \u6837\u672c\u7684\u7279\u5f81\n    sample_feature = [0.9, 0.1]\n    # \u6837\u672c\u5bf9\u5e94\u7684\u6807\u7b7e\n    sample_label = [0.9, 0.1]\n    # \u4f7f\u7528\u68af\u5ea6\u68c0\u67e5\u6765\u67e5\u770b\u662f\u5426\u6b63\u786e\n    gradient_check(net, sample_feature, sample_label)\n\n\nif __name__ == '__main__':\n    '''\n    Desc:\n        \u4e3b\u51fd\u6570\n    Args:\n        None\n    Returns:\n        None\n    '''\n    # \u521d\u59cb\u5316\u4e00\u4e2a\u795e\u7ecf\u7f51\u7edc\uff0c\u8f93\u5165\u5c42 8 \u4e2a\u8282\u70b9\uff0c\u9690\u85cf\u5c42 3 \u4e2a\u8282\u70b9\uff0c\u8f93\u51fa\u5c42 8 \u4e2a\u8282\u70b9\n    net = Network([8, 3, 8])\n    # \u8bad\u7ec3\u6211\u4eec\u7684\u795e\u7ecf\u7f51\u7edc\n    train(net)\n    # \u5c06\u6211\u4eec\u7684\u795e\u7ecf\u7f51\u7edc\u7684\u4fe1\u606f\u6253\u5370\u51fa\u6765\n    net.dump()\n    # \u6253\u5370\u51fa\u795e\u7ecf\u7f51\u7edc\u7684\u6b63\u786e\u7387\n    correct_ratio(net)\n", "src/py3.x/dl/fc.py": "#!/usr/bin/env python\n# -*- coding: UTF-8 -*-\n\n\nimport random\nimport numpy as np\nfrom functools import reduce\nfrom activators import SigmoidActivator, IdentityActivator\n\n\n# \u5168\u8fde\u63a5\u5c42\u5b9e\u73b0\u7c7b\nclass FullConnectedLayer(object):\n    def __init__(self, input_size, output_size, \n                 activator):\n        '''\n        \u6784\u9020\u51fd\u6570\n        input_size: \u672c\u5c42\u8f93\u5165\u5411\u91cf\u7684\u7ef4\u5ea6\n        output_size: \u672c\u5c42\u8f93\u51fa\u5411\u91cf\u7684\u7ef4\u5ea6\n        activator: \u6fc0\u6d3b\u51fd\u6570\n        '''\n        self.input_size = input_size\n        self.output_size = output_size\n        self.activator = activator\n        # \u6743\u91cd\u6570\u7ec4W\n        self.W = np.random.uniform(-0.1, 0.1,\n            (output_size, input_size))\n        # \u504f\u7f6e\u9879b\n        self.b = np.zeros((output_size, 1))\n        # \u8f93\u51fa\u5411\u91cf\n        self.output = np.zeros((output_size, 1))\n\n    def forward(self, input_array):\n        '''\n        \u524d\u5411\u8ba1\u7b97\n        input_array: \u8f93\u5165\u5411\u91cf\uff0c\u7ef4\u5ea6\u5fc5\u987b\u7b49\u4e8einput_size\n        '''\n        # \u5f0f2\n        self.input = input_array\n        self.output = self.activator.forward(\n            np.dot(self.W, input_array) + self.b)\n\n    def backward(self, delta_array):\n        '''\n        \u53cd\u5411\u8ba1\u7b97W\u548cb\u7684\u68af\u5ea6\n        delta_array: \u4ece\u4e0a\u4e00\u5c42\u4f20\u9012\u8fc7\u6765\u7684\u8bef\u5dee\u9879\n        '''\n        # \u5f0f8\n        self.delta = self.activator.backward(self.input) * np.dot(\n            self.W.T, delta_array)\n        self.W_grad = np.dot(delta_array, self.input.T)\n        self.b_grad = delta_array\n\n    def update(self, learning_rate):\n        '''\n        \u4f7f\u7528\u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5\u66f4\u65b0\u6743\u91cd\n        '''\n        self.W += learning_rate * self.W_grad\n        self.b += learning_rate * self.b_grad\n\n    def dump(self):\n        print('W: %s\\nb:%s' % (self.W, self.b))\n\n\n# \u795e\u7ecf\u7f51\u7edc\u7c7b\nclass Network(object):\n    def __init__(self, layers):\n        '''\n        \u6784\u9020\u51fd\u6570\n        '''\n        self.layers = []\n        for i in range(len(layers) - 1):\n            self.layers.append(\n                FullConnectedLayer(\n                    layers[i], layers[i+1],\n                    SigmoidActivator()\n                )\n            )\n\n    def predict(self, sample):\n        '''\n        \u4f7f\u7528\u795e\u7ecf\u7f51\u7edc\u5b9e\u73b0\u9884\u6d4b\n        sample: \u8f93\u5165\u6837\u672c\n        '''\n        output = sample\n        for layer in self.layers:\n            layer.forward(output)\n            output = layer.output\n        return output\n\n    def train(self, labels, data_set, rate, epoch):\n        '''\n        \u8bad\u7ec3\u51fd\u6570\n        labels: \u6837\u672c\u6807\u7b7e\n        data_set: \u8f93\u5165\u6837\u672c\n        rate: \u5b66\u4e60\u901f\u7387\n        epoch: \u8bad\u7ec3\u8f6e\u6570\n        '''\n        for i in range(epoch):\n            for d in range(len(list(data_set))):\n                self.train_one_sample(labels[d], \n                    data_set[d], rate)\n\n    def train_one_sample(self, label, sample, rate):\n        self.predict(sample)\n        self.calc_gradient(label)\n        self.update_weight(rate)\n\n    def calc_gradient(self, label):\n        delta = self.layers[-1].activator.backward(\n            self.layers[-1].output\n        ) * (label - self.layers[-1].output)\n        for layer in self.layers[::-1]:\n            layer.backward(delta)\n            delta = layer.delta\n        return delta\n\n    def update_weight(self, rate):\n        for layer in self.layers:\n            layer.update(rate)\n\n    def dump(self):\n        for layer in self.layers:\n            layer.dump()\n\n    def loss(self, output, label):\n        return 0.5 * ((label - output) * (label - output)).sum()\n\n    def gradient_check(self, sample_feature, sample_label):\n        '''\n        \u68af\u5ea6\u68c0\u67e5\n        network: \u795e\u7ecf\u7f51\u7edc\u5bf9\u8c61\n        sample_feature: \u6837\u672c\u7684\u7279\u5f81\n        sample_label: \u6837\u672c\u7684\u6807\u7b7e\n        '''\n\n        # \u83b7\u53d6\u7f51\u7edc\u5728\u5f53\u524d\u6837\u672c\u4e0b\u6bcf\u4e2a\u8fde\u63a5\u7684\u68af\u5ea6\n        self.predict(sample_feature)\n        self.calc_gradient(sample_label)\n\n        # \u68c0\u67e5\u68af\u5ea6\n        epsilon = 10e-4\n        for fc in self.layers:\n            for i in range(fc.W.shape[0]):\n                for j in range(fc.W.shape[1]):\n                    fc.W[i,j] += epsilon\n                    output = self.predict(sample_feature)\n                    err1 = self.loss(sample_label, output)\n                    fc.W[i,j] -= 2*epsilon\n                    output = self.predict(sample_feature)\n                    err2 = self.loss(sample_label, output)\n                    expect_grad = (err1 - err2) / (2 * epsilon)\n                    fc.W[i,j] += epsilon\n                    print('weights(%d,%d): expected - actural %.4e - %.4e' % (\n                        i, j, expect_grad, fc.W_grad[i,j]))\n\n\nfrom bp import train_data_set\n\n\ndef transpose(args):\n    return map(\n        lambda arg: map(\n            lambda line: np.array(line).reshape(len(line), 1)\n            , arg)\n        , args\n    )\n\n\nclass Normalizer(object):\n    def __init__(self):\n        self.mask = [\n            0x1, 0x2, 0x4, 0x8, 0x10, 0x20, 0x40, 0x80\n        ]\n\n    def norm(self, number):\n        data = list(map(lambda m: 0.9 if number & m else 0.1, self.mask))\n        return np.array(data).reshape(8, 1)\n\n    def denorm(self, vec):\n        binary = list(map(lambda i: 1 if i > 0.5 else 0, vec[:,0]))\n        for i in range(len(self.mask)):\n            binary[i] = binary[i] * self.mask[i]\n        return reduce(lambda x,y: x + y, binary)\n\ndef train_data_set():\n    normalizer = Normalizer()\n    data_set = []\n    labels = []\n    for i in range(0, 256):\n        n = normalizer.norm(i)\n        data_set.append(n)\n        labels.append(n)\n    return labels, data_set\n\ndef correct_ratio(network):\n    normalizer = Normalizer()\n    correct = 0.0;\n    for i in range(256):\n        if normalizer.denorm(network.predict(normalizer.norm(i))) == i:\n            correct += 1.0\n    print('correct_ratio: %.2f%%' % (correct / 256 * 100))\n\n\ndef test():\n    labels, data_set = list(transpose(train_data_set()))\n    labels=list(labels)\n    data_set=list(data_set)\n    net = Network([8, 3, 8])\n    rate = 0.5\n    mini_batch = 20\n    epoch = 10\n    for i in range(epoch):\n        net.train(labels, list(data_set), rate, mini_batch)\n        print('after epoch %d loss: %f' % (\n            (i + 1),\n            net.loss(labels[-1], net.predict(data_set[-1]))\n        ))\n        rate /= 2\n    correct_ratio(net)\n\n\ndef gradient_check():\n    '''\n    \u68af\u5ea6\u68c0\u67e5\n    '''\n    labels, data_set = transpose(train_data_set())\n    net = Network([8, 3, 8])\n    net.gradient_check(data_set[0], labels[0])\n    return net", "src/py3.x/dl/mnist.py": "#!/usr/bin/env python\n# -*- coding: UTF-8 -*-\n\nimport struct\nfrom fc import *\nfrom datetime import datetime\nimport warnings\n#\u5ffd\u7565\u8b66\u544a\u4e00\u628a\u68ad\uff0c\u5ffd\u7565\u4e86sigmoid\u51fd\u6570\u4f4d\u6570\u6ea2\u51fa\u7684\u8b66\u544a\nwarnings.filterwarnings('ignore')\n\n\n# \u6570\u636e\u52a0\u8f7d\u5668\u57fa\u7c7b\nclass Loader(object):\n    def __init__(self, path, count):\n        '''\n        \u521d\u59cb\u5316\u52a0\u8f7d\u5668\n        path: \u6570\u636e\u6587\u4ef6\u8def\u5f84\n        count: \u6587\u4ef6\u4e2d\u7684\u6837\u672c\u4e2a\u6570\n        '''\n        self.path = path\n        self.count = count\n\n    def get_file_content(self):\n        '''\n        \u8bfb\u53d6\u6587\u4ef6\u5185\u5bb9\n        '''\n        f = open(self.path, 'rb')\n        content = f.read()\n        f.close()\n        return list(content)\n\n    def to_int(self, byte):\n        '''\n        \u5c06unsigned byte\u5b57\u7b26\u8f6c\u6362\u4e3a\u6574\u6570\n        '''\n        #return struct.unpack('B', byte)[0]\n        return byte\n\n# \u56fe\u50cf\u6570\u636e\u52a0\u8f7d\u5668\nclass ImageLoader(Loader):\n    def get_picture(self, content, index):\n        '''\n        \u5185\u90e8\u51fd\u6570\uff0c\u4ece\u6587\u4ef6\u4e2d\u83b7\u53d6\u56fe\u50cf\n        '''\n        start = index * 28 * 28 + 16\n        picture = []\n        for i in range(28):\n            picture.append([])\n            for j in range(28):\n                picture[i].append(\n                    self.to_int(content[start + i * 28 + j]))\n        return picture\n\n    def get_one_sample(self, picture):\n        '''\n        \u5185\u90e8\u51fd\u6570\uff0c\u5c06\u56fe\u50cf\u8f6c\u5316\u4e3a\u6837\u672c\u7684\u8f93\u5165\u5411\u91cf\n        '''\n        sample = []\n        for i in range(28):\n            for j in range(28):\n                sample.append(picture[i][j])\n        return sample\n\n    def load(self):\n        '''\n        \u52a0\u8f7d\u6570\u636e\u6587\u4ef6\uff0c\u83b7\u5f97\u5168\u90e8\u6837\u672c\u7684\u8f93\u5165\u5411\u91cf\n        '''\n        content = self.get_file_content()\n        data_set = []\n        for index in range(self.count):\n            data_set.append(\n                self.get_one_sample(\n                    self.get_picture(content, index)))\n        return data_set\n\n\n# \u6807\u7b7e\u6570\u636e\u52a0\u8f7d\u5668\nclass LabelLoader(Loader):\n    def load(self):\n        '''\n        \u52a0\u8f7d\u6570\u636e\u6587\u4ef6\uff0c\u83b7\u5f97\u5168\u90e8\u6837\u672c\u7684\u6807\u7b7e\u5411\u91cf\n        '''\n        content = self.get_file_content()\n        labels = []\n        for index in range(self.count):\n            labels.append(self.norm(content[index + 8]))\n        return labels\n\n    def norm(self, label):\n        '''\n        \u5185\u90e8\u51fd\u6570\uff0c\u5c06\u4e00\u4e2a\u503c\u8f6c\u6362\u4e3a10\u7ef4\u6807\u7b7e\u5411\u91cf\n        '''\n        label_vec = []\n        label_value = self.to_int(label)\n        for i in range(10):\n            if i == label_value:\n                label_vec.append(0.9)\n            else:\n                label_vec.append(0.1)\n        return label_vec\n\n\ndef get_training_data_set():\n    '''\n    \u83b7\u5f97\u8bad\u7ec3\u6570\u636e\u96c6\n    \u539f\u6587\u4e3a60000\u7684\u6570\u636e\u96c6\uff0c\u4f46\u8bad\u7ec3\u901f\u5ea6\u8fc7\u4e8e\u7f13\u6162\uff0c\u8fd9\u91cc\n    '''\n    image_loader = ImageLoader('./data/train-images-idx3-ubyte', 60000)\n    label_loader = LabelLoader('./data/train-labels-idx1-ubyte', 60000)\n    return image_loader.load(), label_loader.load()\n\n\ndef get_test_data_set():\n    '''\n    \u83b7\u5f97\u6d4b\u8bd5\u6570\u636e\u96c6\n    '''\n    image_loader = ImageLoader('t10k-images-idx3-ubyte', 10000)\n    label_loader = LabelLoader('t10k-labels-idx1-ubyte', 10000)\n    return image_loader.load(), label_loader.load()\n\n\ndef show(sample):\n    str = ''\n    for i in range(28):\n        for j in range(28):\n            if sample[i*28+j] != 0:\n                str += '*'\n            else:\n                str += ' '\n        str += '\\n'\n    print(str)\n\n\ndef get_result(vec):\n    max_value_index = 0\n    max_value = 0\n    vec = list(vec)\n    for i in range(len(vec)):\n        if vec[i] > max_value:\n            max_value = vec[i]\n            max_value_index = i\n    return max_value_index\n\n\ndef evaluate(network, test_data_set, test_labels):\n    error = 0\n    total = len(test_data_set)\n\n    for i in range(total):\n        label = get_result(test_labels[i])\n        predict = get_result(network.predict(test_data_set[i]))\n        if label != predict:\n            error += 1\n    return float(error) / float(total)\n\n\ndef now():\n    return datetime.now().strftime('%c')\n\n\ndef train_and_evaluate():\n    last_error_ratio = 1.0\n    epoch = 0\n    train_data_set, train_labels = transpose(get_training_data_set())\n    test_data_set, test_labels = transpose(get_test_data_set())\n    train_data_set =list(train_data_set)\n    train_labels = list(train_labels)\n    test_data_set = list(test_data_set)\n    test_labels = list(test_labels)\n    network = Network([784, 100, 10])\n    while True:\n        epoch += 1\n        network.train(train_labels, train_data_set, 0.01, 1)\n        print('%s epoch %d finished, loss %f' % (now(), epoch,\n            network.loss(train_labels[-1], network.predict(train_data_set[-1]))))\n        if epoch % 2 == 0:\n            error_ratio = evaluate(network, test_data_set, test_labels)\n            print('%s after epoch %d, error ratio is %f' % (now(), epoch, error_ratio))\n            if error_ratio > last_error_ratio:\n                break\n            else:\n                last_error_ratio = error_ratio\n\nif __name__ == '__main__':\n    train_and_evaluate()\n", "src/py3.x/dl/perceptron.py": "#!/usr/bin/env python\n# -*- coding: UTF-8 -*-\n\nfrom functools import reduce\n\ndef add(x,y):\n    return  x+y\n\n\nclass Perceptron(object):\n    '''\n       Desc:\n           \u611f\u77e5\u5668\u7c7b\n       Args:\n           None\n       Returns:\n           None\n       '''\n    def __init__(self,input_num,activator):\n        '''\n              Desc:\n                  \u521d\u59cb\u5316\u611f\u77e5\u5668\n              Args:\n                  input_num \u2014\u2014 \u8f93\u5165\u53c2\u6570\u7684\u4e2a\u6570\n                  activator \u2014\u2014 \u6fc0\u6d3b\u51fd\u6570\n              Returns:\n                  None\n        '''\n        # \u8bbe\u7f6e\u7684\u6fc0\u6d3b\u51fd\u6570\n        self.activator = activator\n        # \u6743\u91cd\u5411\u91cf\u521d\u59cb\u5316\u4e3a 0\n        self.weights = [0.0 for _ in range(input_num)]\n        # \u504f\u7f6e\u9879\u521d\u59cb\u5316\u4e3a 0\n        self.bias = 0.0\n\n    def __str__(self):\n        '''\n        Desc:\n            \u5c06\u611f\u77e5\u5668\u4fe1\u606f\u6253\u5370\u51fa\u6765\n        Args:\n            None\n        Returns:\n            None\n        '''\n        return  'weights\\t:%s\\nbias\\t:%f\\n' % (self.weights, self.bias)\n\n    def predict(self,input_vec):\n        '''\n        Desc:\n            \u8f93\u5165\u5411\u91cf\uff0c\u8f93\u51fa\u611f\u77e5\u5668\u7684\u8ba1\u7b97\u7ed3\u679c\n        Args:\n            input_vec \u2014\u2014 \u8f93\u5165\u5411\u91cf\n        Returns:\n            \u611f\u77e5\u5668\u7684\u8ba1\u7b97\u7ed3\u679c\n        '''\n        # \u5c06\u8f93\u5165\u5411\u91cf\u7684\u8ba1\u7b97\u7ed3\u679c\u8fd4\u56de\n        # \u8c03\u7528 \u6fc0\u6d3b\u51fd\u6570 activator \uff0c\u5c06\u8f93\u5165\u5411\u91cf\u8f93\u5165\uff0c\u8ba1\u7b97\u611f\u77e5\u5668\u7684\u7ed3\u679c\n        # reduce() \u51fd\u6570\u662f python 2 \u7684\u5185\u7f6e\u51fd\u6570\uff0c\u4ece python 3 \u5f00\u59cb\u79fb\u5230\u4e86 functools \u6a21\u5757\n        # reduce() \u4ece\u5de6\u5230\u53f3\u5bf9\u4e00\u4e2a\u5e8f\u5217\u7684\u9879\u7d2f\u8ba1\u5730\u5e94\u7528\u6709\u4e24\u4e2a\u53c2\u6570\u7684\u51fd\u6570\uff0c\u4ee5\u6b64\u5408\u5e76\u5e8f\u5217\u5230\u4e00\u4e2a\u5355\u4e00\u503c\uff0c\u4f8b\u5982 reduce(lambda x,y: x+y, [1,2,3,4,5]) \u8ba1\u7b97\u7684\u5c31\u662f ((((1+2)+3)+4)+5)\n        # map() \u63a5\u6536\u4e00\u4e2a\u51fd\u6570 f \u548c\u4e00\u4e2a list \uff0c\u5e76\u901a\u8fc7\u628a\u51fd\u6570 f \u4f9d\u6b21\u4f5c\u7528\u5728 list \u7684\u6bcf\u4e2a\u5143\u7d20\u4e0a\uff0c\u5f97\u5230\u4e00\u4e2a\u65b0\u7684 list \u8fd4\u56de\u3002\u6bd4\u5982\u6211\u4eec\u7684 f \u51fd\u6570\u662f\u8ba1\u7b97\u5e73\u65b9\uff0c map(f, [1,2,3,4,5]) ===> \u8fd4\u56de [1,4,9,16,25]\n        # zip() \u63a5\u6536\u4efb\u610f\u591a\u4e2a\uff08\u5305\u62ec 0 \u4e2a\u548c 1\u4e2a\uff09\u5e8f\u5217\u4f5c\u4e3a\u53c2\u6570\uff0c\u8fd4\u56de\u4e00\u4e2a tuple \u5217\u8868\u3002\u4f8b: x = [1,2,3] y = [4,5,6] z = [7,8,9] xyz = zip(x, y, z) ===> [(1,4,7), (2,5,8), (3,6,9)]\n\n        pack = zip(input_vec,self.weights)\n        multi = []\n        for (x,w) in pack:\n            multi.append(x*w)\n        activtion = reduce(add, multi)\n        # \u6b64\u5904python3 lambda\u65e0\u6cd5\u4f20\u5165\u4e00\u4e2atuple\u7684\u4e24\u4e2a\u53d8\u91cf\uff0c\u56e0\u6b64\u5c06tuple\u5f53\u4f5c\u4e00\u4e2a\u6574\u4f53\uff0ctp[0]\u4e3ainput_vec,tp[1]\u4e3aself.weights\n        return self.activator(activtion + self.bias)\n        #\u8fd8\u6709\u4e00\u79cd\u66f4\u52a0\u7b80\u6d01\u660e\u4e86\u7684\u5199\u6cd5\uff0c\u5f88\u6e05\u695a\u660e\u767d\n        # return self.activator(sum([x*w for (x,w) in zip(input_vec,self.weights)])+self.bias) \n\n    def train(self,input_vecs,labels,iteration,rate):\n        '''\n        Desc:\n            \u8f93\u5165\u8bad\u7ec3\u6570\u636e: \u4e00\u7ec4\u5411\u91cf\u3001\u4e0e\u6bcf\u4e2a\u5411\u91cf\u5bf9\u5e94\u7684 label; \u4ee5\u53ca\u8bad\u7ec3\u8f6e\u6570\u3001\u5b66\u4e60\u7387\n        Args:\n            input_vec \u2014\u2014 \u8f93\u5165\u5411\u91cf\n            labels \u2014\u2014 \u6570\u636e\u5bf9\u5e94\u7684\u6807\u7b7e\n            iteration \u2014\u2014 \u8bad\u7ec3\u7684\u8fed\u4ee3\u8f6e\u6570\n            rate \u2014\u2014 \u5b66\u4e60\u7387\n        Returns:\n            None\n        '''\n        for i in range(iteration):\n            self._one_iteration(input_vecs,labels,rate)\n\n    def _one_iteration(self,input_vecs,labels,rate):\n        '''\n        Desc:\n            \u8bad\u7ec3\u8fc7\u7a0b\u7684\u4e00\u6b21\u8fed\u4ee3\u8fc7\u7a0b\n        Args:\n            input_vecs \u2014\u2014 \u8f93\u5165\u5411\u91cf\n            labels \u2014\u2014 \u6570\u636e\u5bf9\u5e94\u7684\u6807\u7b7e\n            rate \u2014\u2014 \u5b66\u4e60\u7387\n        Returns:\n            None\n        '''\n        # zip() \u63a5\u6536\u4efb\u610f\u591a\u4e2a\uff08\u5305\u62ec 0 \u4e2a\u548c 1\u4e2a\uff09\u5e8f\u5217\u4f5c\u4e3a\u53c2\u6570\uff0c\u8fd4\u56de\u4e00\u4e2a tuple \u5217\u8868\u3002\u4f8b: x = [1,2,3] y = [4,5,6] z = [7,8,9] xyz = zip(x, y, z) ===> [(1,4,7), (2,5,8), (3,6,9)]\n        samples = zip(input_vecs, labels)\n        # \u5bf9\u6bcf\u4e2a\u6837\u672c\uff0c\u6309\u7167\u611f\u77e5\u5668\u89c4\u5219\u66f4\u65b0\u6743\u91cd\n        for (input_vec, label) in samples:\n            # \u8ba1\u7b97\u611f\u77e5\u5668\u5728\u5f53\u524d\u6743\u91cd\u4e0b\u7684\u8f93\u51fa\n            output = self.predict(input_vec)\n            # \u66f4\u65b0\u6743\u91cd\n            output = self._update_weights(input_vec, output, label, rate)\n\n    def _update_weights(self,input_vecs,output,labels,rate):\n        '''\n        Desc:\n            \u6309\u7167\u611f\u77e5\u5668\u89c4\u5219\u66f4\u65b0\u6743\u91cd\n        Args:\n            input_vec \u2014\u2014 \u8f93\u5165\u5411\u91cf\n            output \u2014\u2014 \u7ecf\u8fc7\u611f\u77e5\u5668\u89c4\u5219\u8ba1\u7b97\u5f97\u5230\u7684\u8f93\u51fa\n            label \u2014\u2014 \u8f93\u5165\u5411\u91cf\u5bf9\u5e94\u7684\u6807\u7b7e\n            rate \u2014\u2014 \u5b66\u4e60\u7387\n        Returns:\n            None\n        '''\n        # \u5229\u7528\u611f\u77e5\u5668\u89c4\u5219\u66f4\u65b0\u6743\u91cd\n        \n        delta = labels -output\n        # map() \u63a5\u6536\u4e00\u4e2a\u51fd\u6570 f \u548c\u4e00\u4e2a list \uff0c\u5e76\u901a\u8fc7\u628a\u51fd\u6570 f \u4f9d\u6b21\u4f5c\u7528\u5728 list \u7684\u6bcf\u4e2a\u5143\u7d20\u4e0a\uff0c\u5f97\u5230\u4e00\u4e2a\u65b0\u7684 list \u8fd4\u56de\u3002\u6bd4\u5982\u6211\u4eec\u7684 f \u51fd\u6570\u662f\u8ba1\u7b97\u5e73\u65b9\uff0c map(f, [1,2,3,4,5]) ===> \u8fd4\u56de [1,4,9,16,25]\n        # zip() \u63a5\u6536\u4efb\u610f\u591a\u4e2a\uff08\u5305\u62ec 0 \u4e2a\u548c 1\u4e2a\uff09\u5e8f\u5217\u4f5c\u4e3a\u53c2\u6570\uff0c\u8fd4\u56de\u4e00\u4e2a tuple \u5217\u8868\u3002\u4f8b: x = [1,2,3] y = [4,5,6] z = [7,8,9] xyz = zip(x, y, z) ===> [(1,4,7), (2,5,8), (3,6,9)]\n        # \u6b64\u5904python3\u5fc5\u987b\u5bf9map\u51fd\u6570\u8fdb\u884clist\u64cd\u4f5c\uff0c\u4e0d\u7136 self.weights\u4e3amap\u7c7b\u578b\uff0c\u6700\u540e\u65e0\u6cd5\u6253\u5370\u51fa\u5177\u4f53\u6570\u503c\n        pack  = zip(input_vecs,self.weights)\n        tmp = []\n        for (x,w) in pack:\n            tmp.append(w+x*delta*rate)\n        self.weights = tmp\n        # \u66f4\u65b0 bias\n        self.bias = self.bias + delta*rate\n\ndef f(x):\n    '''\n    Desc:\n        \u5b9a\u4e49\u6fc0\u6d3b\u51fd\u6570 f\n    Args:\n        x \u2014\u2014 \u8f93\u5165\u5411\u91cf\n    Returns:\n        \uff08\u5b9e\u73b0\u9636\u8dc3\u51fd\u6570\uff09\u5927\u4e8e 0 \u8fd4\u56de 1\uff0c\u5426\u5219\u8fd4\u56de 0\n    '''\n    if x>0:\n        return 1\n    else:\n        return 0\n\ndef get_training_dataset():\n    '''\n    Desc:\n        \u57fa\u4e8e and \u771f\u503c\u8868\u6765\u6784\u5efa/\u83b7\u53d6\u8bad\u7ec3\u6570\u636e\u96c6\n    Args:\n        None\n    Returns:\n        input_vecs \u2014\u2014 \u8f93\u5165\u5411\u91cf\n        labels \u2014\u2014 \u8f93\u5165\u5411\u91cf\u5bf9\u5e94\u7684\u6807\u7b7e\n    '''\n    # \u6784\u5efa\u8bad\u7ec3\u6570\u636e\uff0c\u8f93\u5165\u5411\u91cf\u7684\u5217\u8868\n    input_vecs = [[1,1],[0,0],[1,0],[0,1]]\n    # \u671f\u671b\u7684\u8f93\u51fa\u5217\u8868\uff0c\u4e5f\u5c31\u662f\u4e0a\u9762\u7684\u8f93\u5165\u5411\u91cf\u7684\u5217\u8868\u4e2d\u6570\u636e\u5bf9\u5e94\u7684\u6807\u7b7e\uff0c\u662f\u4e00\u4e00\u5bf9\u5e94\u7684\n    \n    labels = [1,0,0,0]\n    return input_vecs,labels\n\ndef train_and_perception():\n    '''\n    Desc:\n        \u4f7f\u7528 and \u771f\u503c\u8868\u6765\u8bad\u7ec3\u6211\u4eec\u7684\u611f\u77e5\u5668\n    Args:\n        None\n    Returns:\n        p \u2014\u2014 \u8fd4\u56de\u8bad\u7ec3\u597d\u7684\u611f\u77e5\u5668\n    '''\n    # \u521b\u5efa\u611f\u77e5\u5668\uff0c\u8f93\u5165\u53c2\u6570\u7684\u4e2a\u6570\u662f 2 \u4e2a\uff08\u56e0\u4e3a and \u662f\u4e2a\u4e8c\u5143\u51fd\u6570\uff09\uff0c\u6fc0\u6d3b\u51fd\u6570\u4e3a f\n    p = Perceptron(2, f)\n    # \u8fdb\u884c\u8bad\u7ec3\uff0c\u8fed\u4ee3 10 \u8f6e\uff0c\u5b66\u4e60\u901f\u7387\u662f\u6211\u4eec\u8bbe\u5b9a\u7684 rate \uff0c\u4e3a 0.1\n    input_vecs, labels = get_training_dataset()\n    p.train(input_vecs, labels, 10, 0.1)\n    # \u8fd4\u56de\u8bad\u7ec3\u597d\u7684\u611f\u77e5\u5668\n    return p\n\nif __name__ == '__main__':\n    '''\n    Desc:\n        \u4e3b\u51fd\u6570\uff0c\u8c03\u7528\u4e0a\u9762\u8fd4\u56de\u7684\u8bad\u7ec3\u597d\u7684\u611f\u77e5\u5668\u8fdb\u884c\u9884\u6d4b\n    Args:\n        None\n    Returns:\n        None\n    '''\n    # \u8bad\u7ec3 and \u611f\u77e5\u5668\n    and_perceptron = train_and_perceptron()\n    # \u6253\u5370\u8bad\u7ec3\u83b7\u5f97\u7684\u6743\u91cd\n    print(and_perceptron)\n    # \u6d4b\u8bd5\n    print('1 and 1 = %d' % and_perceptron.predict([1, 1]))\n    print('0 and 0 = %d' % and_perceptron.predict([0, 0]))\n    print('1 and 0 = %d' % and_perceptron.predict([1, 0]))\n    print('0 and 1 = %d' % and_perceptron.predict([0, 1]))\n", "src/py3.x/dl/recursive.py": "#!/usr/bin/env python\n# -*- coding: UTF-8 -*-\n\n\nimport numpy as np\nfrom activators import IdentityActivator\n\n\nclass TreeNode(object):\n    def __init__(self, data, children=[], children_data=[]):\n        self.parent = None\n        self.children = children\n        self.children_data = children_data\n        self.data = data\n        for child in children:\n            child.parent = self\n\n# \u9012\u5f52\u795e\u7ecf\u7f51\u7edc\u5b9e\u73b0\nclass RecursiveLayer(object):\n    def __init__(self, node_width, child_count, \n                 activator, learning_rate):\n        '''\n        \u9012\u5f52\u795e\u7ecf\u7f51\u7edc\u6784\u9020\u51fd\u6570\n        node_width: \u8868\u793a\u6bcf\u4e2a\u8282\u70b9\u7684\u5411\u91cf\u7684\u7ef4\u5ea6\n        child_count: \u6bcf\u4e2a\u7236\u8282\u70b9\u6709\u51e0\u4e2a\u5b50\u8282\u70b9\n        activator: \u6fc0\u6d3b\u51fd\u6570\u5bf9\u8c61\n        learning_rate: \u68af\u5ea6\u4e0b\u964d\u7b97\u6cd5\u5b66\u4e60\u7387\n        '''\n        self.node_width = node_width\n        self.child_count = child_count\n        self.activator = activator\n        self.learning_rate = learning_rate\n        # \u6743\u91cd\u6570\u7ec4W\n        self.W = np.random.uniform(-1e-4, 1e-4,\n            (node_width, node_width * child_count))\n        # \u504f\u7f6e\u9879b\n        self.b = np.zeros((node_width, 1))\n        # \u9012\u5f52\u795e\u7ecf\u7f51\u7edc\u751f\u6210\u7684\u6811\u7684\u6839\u8282\u70b9\n        self.root = None\n\n    def forward(self, *children):\n        '''\n        \u524d\u5411\u8ba1\u7b97\n        '''\n        children_data = self.concatenate(children)\n        parent_data = self.activator.forward(\n            np.dot(self.W, children_data) + self.b\n        )\n        self.root = TreeNode(parent_data, children\n                            , children_data)\n\n    def backward(self, parent_delta):\n        '''\n        BPTS\u53cd\u5411\u4f20\u64ad\u7b97\u6cd5\n        '''\n        self.calc_delta(parent_delta, self.root)\n        self.W_grad, self.b_grad = self.calc_gradient(self.root)\n\n    def update(self):\n        '''\n        \u4f7f\u7528SGD\u7b97\u6cd5\u66f4\u65b0\u6743\u91cd\n        '''\n        self.W -= self.learning_rate * self.W_grad\n        self.b -= self.learning_rate * self.b_grad\n\n    def reset_state(self):\n        self.root = None\n\n    def concatenate(self, tree_nodes):\n        '''\n        \u5c06\u5404\u4e2a\u6811\u8282\u70b9\u4e2d\u7684\u6570\u636e\u62fc\u63a5\u6210\u4e00\u4e2a\u957f\u5411\u91cf\n        '''\n        concat = np.zeros((0,1))\n        for node in tree_nodes:\n            concat = np.concatenate((concat, node.data))\n        return concat\n\n    def calc_delta(self, parent_delta, parent):\n        '''\n        \u8ba1\u7b97\u6bcf\u4e2a\u8282\u70b9\u7684delta\n        '''\n        parent.delta = parent_delta\n        if parent.children:\n            # \u6839\u636e\u5f0f2\u8ba1\u7b97\u6bcf\u4e2a\u5b50\u8282\u70b9\u7684delta\n            children_delta = np.dot(self.W.T, parent_delta) * (\n                self.activator.backward(parent.children_data)\n            )\n            # slices = [(\u5b50\u8282\u70b9\u7f16\u53f7\uff0c\u5b50\u8282\u70b9delta\u8d77\u59cb\u4f4d\u7f6e\uff0c\u5b50\u8282\u70b9delta\u7ed3\u675f\u4f4d\u7f6e)]\n            slices = [(i, i * self.node_width, \n                        (i + 1) * self.node_width)\n                        for i in range(self.child_count)]\n            # \u9488\u5bf9\u6bcf\u4e2a\u5b50\u8282\u70b9\uff0c\u9012\u5f52\u8c03\u7528calc_delta\u51fd\u6570\n            for s in slices:\n                self.calc_delta(children_delta[s[1]:s[2]], \n                                parent.children[s[0]])\n\n    def calc_gradient(self, parent):\n        '''\n        \u8ba1\u7b97\u6bcf\u4e2a\u8282\u70b9\u6743\u91cd\u7684\u68af\u5ea6\uff0c\u5e76\u5c06\u5b83\u4eec\u6c42\u548c\uff0c\u5f97\u5230\u6700\u7ec8\u7684\u68af\u5ea6\n        '''\n        W_grad = np.zeros((self.node_width, \n                            self.node_width * self.child_count))\n        b_grad = np.zeros((self.node_width, 1))\n        if not parent.children:\n            return W_grad, b_grad\n        parent.W_grad = np.dot(parent.delta, parent.children_data.T)\n        parent.b_grad = parent.delta\n        W_grad += parent.W_grad\n        b_grad += parent.b_grad\n        for child in parent.children:\n            W, b = self.calc_gradient(child)\n            W_grad += W\n            b_grad += b\n        return W_grad, b_grad\n\n    def dump(self, **kwArgs):\n        print('root.data: %s' % self.root.data)\n        print('root.children_data: %s' % self.root.children_data)\n        if 'dump_grad'in kwArgs:\n            print('W_grad: %s' % self.W_grad)\n            print('b_grad: %s' % self.b_grad)\n\n\ndef data_set():\n    children = [\n        TreeNode(np.array([[1],[2]])),\n        TreeNode(np.array([[3],[4]])),\n        TreeNode(np.array([[5],[6]]))\n    ]\n    d = np.array([[0.5],[0.8]])\n    return children, d\n\n\ndef gradient_check():\n    '''\n    \u68af\u5ea6\u68c0\u67e5\n    '''\n    # \u8bbe\u8ba1\u4e00\u4e2a\u8bef\u5dee\u51fd\u6570\uff0c\u53d6\u6240\u6709\u8282\u70b9\u8f93\u51fa\u9879\u4e4b\u548c\n    error_function = lambda o: o.sum()\n    \n    rnn = RecursiveLayer(2, 2, IdentityActivator(), 1e-3)\n\n    # \u8ba1\u7b97forward\u503c\n    x, d = data_set()\n    rnn.forward(x[0], x[1])\n    rnn.forward(rnn.root, x[2])\n    \n    # \u6c42\u53d6sensitivity map\n    sensitivity_array = np.ones((rnn.node_width, 1),\n                                dtype=np.float64)\n    # \u8ba1\u7b97\u68af\u5ea6\n    rnn.backward(sensitivity_array)\n    \n    # \u68c0\u67e5\u68af\u5ea6\n    epsilon = 10e-4\n    for i in range(rnn.W.shape[0]):\n        for j in range(rnn.W.shape[1]):\n            rnn.W[i,j] += epsilon\n            rnn.reset_state()\n            rnn.forward(x[0], x[1])\n            rnn.forward(rnn.root, x[2])\n            err1 = error_function(rnn.root.data)\n            rnn.W[i,j] -= 2*epsilon\n            rnn.reset_state()\n            rnn.forward(x[0], x[1])\n            rnn.forward(rnn.root, x[2])\n            err2 = error_function(rnn.root.data)\n            expect_grad = (err1 - err2) / (2 * epsilon)\n            rnn.W[i,j] += epsilon\n            print('weights(%d,%d): expected - actural %.4e - %.4e' % (\n                i, j, expect_grad, rnn.W_grad[i,j]))\n    return rnn\n\n\ndef test():\n    children, d = data_set()\n    rnn = RecursiveLayer(2, 2, IdentityActivator(), 1e-3)\n    rnn.forward(children[0], children[1])\n    rnn.dump()\n    rnn.forward(rnn.root, children[2])\n    rnn.dump()\n    rnn.backward(d)\n    rnn.dump(dump_grad='true')\n    return rnn\n\ndef test_gradient_check():\n    gradient_check()", "src/py3.x/dl/cnn.py": "#!/usr/bin/env python\n# -*- coding: UTF-8 -*-\n\n\nimport numpy as np\nfrom activators import ReluActivator, IdentityActivator\n\n\n# \u83b7\u53d6\u5377\u79ef\u533a\u57df\ndef get_patch(input_array, i, j, filter_width,\n              filter_height, stride):\n    '''\n    \u4ece\u8f93\u5165\u6570\u7ec4\u4e2d\u83b7\u53d6\u672c\u6b21\u5377\u79ef\u7684\u533a\u57df\uff0c\n    \u81ea\u52a8\u9002\u914d\u8f93\u5165\u4e3a2D\u548c3D\u7684\u60c5\u51b5\n    '''\n    start_i = i * stride\n    start_j = j * stride\n    if input_array.ndim == 2:\n        return input_array[\n               start_i: start_i + filter_height,\n               start_j: start_j + filter_width]\n    elif input_array.ndim == 3:\n        return input_array[:,\n               start_i: start_i + filter_height,\n               start_j: start_j + filter_width]\n\n\n# \u83b7\u53d6\u4e00\u4e2a2D\u533a\u57df\u7684\u6700\u5927\u503c\u6240\u5728\u7684\u7d22\u5f15\ndef get_max_index(array):\n    max_i = 0\n    max_j = 0\n    max_value = array[0, 0]\n    for i in range(array.shape[0]):\n        for j in range(array.shape[1]):\n            if array[i, j] > max_value:\n                max_value = array[i, j]\n                max_i, max_j = i, j\n    return max_i, max_j\n\n\n# \u8ba1\u7b97\u5377\u79ef\ndef conv(input_array,\n         kernel_array,\n         output_array,\n         stride, bias):\n    '''\n    \u8ba1\u7b97\u5377\u79ef\uff0c\u81ea\u52a8\u9002\u914d\u8f93\u5165\u4e3a2D\u548c3D\u7684\u60c5\u51b5\n    '''\n    channel_number = input_array.ndim\n    output_width = output_array.shape[1]\n    output_height = output_array.shape[0]\n    kernel_width = kernel_array.shape[-1]\n    kernel_height = kernel_array.shape[-2]\n    for i in range(output_height):\n        for j in range(output_width):\n            output_array[i][j] = (\n                                         get_patch(input_array, i, j, kernel_width,\n                                                   kernel_height, stride) * kernel_array\n                                 ).sum() + bias\n\n\n# \u4e3a\u6570\u7ec4\u589e\u52a0Zero padding\ndef padding(input_array, zp):\n    '''\n    \u4e3a\u6570\u7ec4\u589e\u52a0Zero padding\uff0c\u81ea\u52a8\u9002\u914d\u8f93\u5165\u4e3a2D\u548c3D\u7684\u60c5\u51b5\n    '''\n    if zp == 0:\n        return input_array\n    else:\n        if input_array.ndim == 3:\n            input_width = input_array.shape[2]\n            input_height = input_array.shape[1]\n            input_depth = input_array.shape[0]\n            padded_array = np.zeros((\n                input_depth,\n                input_height + 2 * zp,\n                input_width + 2 * zp))\n            padded_array[:,\n            zp: zp + input_height,\n            zp: zp + input_width] = input_array\n            return padded_array\n        elif input_array.ndim == 2:\n            input_width = input_array.shape[1]\n            input_height = input_array.shape[0]\n            padded_array = np.zeros((\n                input_height + 2 * zp,\n                input_width + 2 * zp))\n            padded_array[zp: zp + input_height,\n            zp: zp + input_width] = input_array\n            return padded_array\n\n\n# \u5bf9numpy\u6570\u7ec4\u8fdb\u884celement wise\u64cd\u4f5c\ndef element_wise_op(array, op):\n    for i in np.nditer(array,\n                       op_flags=['readwrite']):\n        i[...] = op(i)\n\n\nclass Filter(object):\n    def __init__(self, width, height, depth):\n        self.weights = np.random.uniform(-1e-4, 1e-4,\n                                         (depth, height, width))\n        self.bias = 0\n        self.weights_grad = np.zeros(\n            self.weights.shape)\n        self.bias_grad = 0\n\n    def __repr__(self):\n        return 'filter weights:\\n%s\\nbias:\\n%s' % (\n            repr(self.weights), repr(self.bias))\n\n    def get_weights(self):\n        return self.weights\n\n    def get_bias(self):\n        return self.bias\n\n    def update(self, learning_rate):\n        self.weights -= learning_rate * self.weights_grad\n        self.bias -= learning_rate * self.bias_grad\n\n\nclass ConvLayer(object):\n    def __init__(self, input_width, input_height,\n                 channel_number, filter_width,\n                 filter_height, filter_number,\n                 zero_padding, stride, activator,\n                 learning_rate):\n        self.input_width = input_width\n        self.input_height = input_height\n        self.channel_number = channel_number\n        self.filter_width = filter_width\n        self.filter_height = filter_height\n        self.filter_number = filter_number\n        self.zero_padding = zero_padding\n        self.stride = stride\n        self.output_width = \\\n            ConvLayer.calculate_output_size(\n                self.input_width, filter_width, zero_padding,\n                stride)\n        self.output_height = \\\n            ConvLayer.calculate_output_size(\n                self.input_height, filter_height, zero_padding,\n                stride)\n        self.output_array = np.zeros((self.filter_number,\n                                      self.output_height, self.output_width))\n        self.filters = []\n        for i in range(filter_number):\n            self.filters.append(Filter(filter_width,\n                                       filter_height, self.channel_number))\n        self.activator = activator\n        self.learning_rate = learning_rate\n\n    def forward(self, input_array):\n        '''\n        \u8ba1\u7b97\u5377\u79ef\u5c42\u7684\u8f93\u51fa\n        \u8f93\u51fa\u7ed3\u679c\u4fdd\u5b58\u5728self.output_array\n        '''\n        self.input_array = input_array\n        self.padded_input_array = padding(input_array,\n                                          self.zero_padding)\n        for f in range(self.filter_number):\n            filter = self.filters[f]\n            conv(self.padded_input_array,\n                 filter.get_weights(), self.output_array[f],\n                 self.stride, filter.get_bias())\n        element_wise_op(self.output_array,\n                        self.activator.forward)\n\n    def backward(self, input_array, sensitivity_array,\n                 activator):\n        '''\n        \u8ba1\u7b97\u4f20\u9012\u7ed9\u524d\u4e00\u5c42\u7684\u8bef\u5dee\u9879\uff0c\u4ee5\u53ca\u8ba1\u7b97\u6bcf\u4e2a\u6743\u91cd\u7684\u68af\u5ea6\n        \u524d\u4e00\u5c42\u7684\u8bef\u5dee\u9879\u4fdd\u5b58\u5728self.delta_array\n        \u68af\u5ea6\u4fdd\u5b58\u5728Filter\u5bf9\u8c61\u7684weights_grad\n        '''\n        self.forward(input_array)\n        self.bp_sensitivity_map(sensitivity_array,\n                                activator)\n        self.bp_gradient(sensitivity_array)\n\n    def update(self):\n        '''\n        \u6309\u7167\u68af\u5ea6\u4e0b\u964d\uff0c\u66f4\u65b0\u6743\u91cd\n        '''\n        for filter in self.filters:\n            filter.update(self.learning_rate)\n\n    def bp_sensitivity_map(self, sensitivity_array,\n                           activator):\n        '''\n        \u8ba1\u7b97\u4f20\u9012\u5230\u4e0a\u4e00\u5c42\u7684sensitivity map\n        sensitivity_array: \u672c\u5c42\u7684sensitivity map\n        activator: \u4e0a\u4e00\u5c42\u7684\u6fc0\u6d3b\u51fd\u6570\n        '''\n        # \u5904\u7406\u5377\u79ef\u6b65\u957f\uff0c\u5bf9\u539f\u59cbsensitivity map\u8fdb\u884c\u6269\u5c55\n        expanded_array = self.expand_sensitivity_map(\n            sensitivity_array)\n        # full\u5377\u79ef\uff0c\u5bf9sensitivitiy map\u8fdb\u884czero padding\n        # \u867d\u7136\u539f\u59cb\u8f93\u5165\u7684zero padding\u5355\u5143\u4e5f\u4f1a\u83b7\u5f97\u6b8b\u5dee\n        # \u4f46\u8fd9\u4e2a\u6b8b\u5dee\u4e0d\u9700\u8981\u7ee7\u7eed\u5411\u4e0a\u4f20\u9012\uff0c\u56e0\u6b64\u5c31\u4e0d\u8ba1\u7b97\u4e86\n        expanded_width = expanded_array.shape[2]\n        zp = (self.input_width +\n              self.filter_width - 1 - expanded_width) // 2\n        padded_array = padding(expanded_array, zp)\n        # \u521d\u59cb\u5316delta_array\uff0c\u7528\u4e8e\u4fdd\u5b58\u4f20\u9012\u5230\u4e0a\u4e00\u5c42\u7684\n        # sensitivity map\n        self.delta_array = self.create_delta_array()\n        # \u5bf9\u4e8e\u5177\u6709\u591a\u4e2afilter\u7684\u5377\u79ef\u5c42\u6765\u8bf4\uff0c\u6700\u7ec8\u4f20\u9012\u5230\u4e0a\u4e00\u5c42\u7684\n        # sensitivity map\u76f8\u5f53\u4e8e\u6240\u6709\u7684filter\u7684\n        # sensitivity map\u4e4b\u548c\n        for f in range(self.filter_number):\n            filter = self.filters[f]\n            # \u5c06filter\u6743\u91cd\u7ffb\u8f6c180\u5ea6\n            flipped_weights = np.array(list(map(lambda i: np.rot90(i, 2), filter.get_weights())))\n            # \u8ba1\u7b97\u4e0e\u4e00\u4e2afilter\u5bf9\u5e94\u7684delta_array\n            delta_array = self.create_delta_array()\n            for d in range(delta_array.shape[0]):\n                conv(padded_array[f], flipped_weights[d],\n                     delta_array[d], 1, 0)\n            self.delta_array += delta_array\n        # \u5c06\u8ba1\u7b97\u7ed3\u679c\u4e0e\u6fc0\u6d3b\u51fd\u6570\u7684\u504f\u5bfc\u6570\u505aelement-wise\u4e58\u6cd5\u64cd\u4f5c\n        derivative_array = np.array(self.input_array)\n        element_wise_op(derivative_array,\n                        activator.backward)\n        self.delta_array *= derivative_array\n\n    def bp_gradient(self, sensitivity_array):\n        # \u5904\u7406\u5377\u79ef\u6b65\u957f\uff0c\u5bf9\u539f\u59cbsensitivity map\u8fdb\u884c\u6269\u5c55\n        expanded_array = self.expand_sensitivity_map(\n            sensitivity_array)\n        for f in range(self.filter_number):\n            # \u8ba1\u7b97\u6bcf\u4e2a\u6743\u91cd\u7684\u68af\u5ea6\n            filter = self.filters[f]\n            for d in range(filter.weights.shape[0]):\n                conv(self.padded_input_array[d],\n                     expanded_array[f],\n                     filter.weights_grad[d], 1, 0)\n            # \u8ba1\u7b97\u504f\u7f6e\u9879\u7684\u68af\u5ea6\n            filter.bias_grad = expanded_array[f].sum()\n\n    def expand_sensitivity_map(self, sensitivity_array):\n        depth = sensitivity_array.shape[0]\n        # \u786e\u5b9a\u6269\u5c55\u540esensitivity map\u7684\u5927\u5c0f\n        # \u8ba1\u7b97stride\u4e3a1\u65f6sensitivity map\u7684\u5927\u5c0f\n        expanded_width = (self.input_width -\n                          self.filter_width + 2 * self.zero_padding + 1)\n        expanded_height = (self.input_height -\n                           self.filter_height + 2 * self.zero_padding + 1)\n        # \u6784\u5efa\u65b0\u7684sensitivity_map\n        expand_array = np.zeros((depth, expanded_height,\n                                 expanded_width))\n        # \u4ece\u539f\u59cbsensitivity map\u62f7\u8d1d\u8bef\u5dee\u503c\n        for i in range(self.output_height):\n            for j in range(self.output_width):\n                i_pos = i * self.stride\n                j_pos = j * self.stride\n                expand_array[:, i_pos, j_pos] = \\\n                    sensitivity_array[:, i, j]\n        return expand_array\n\n    def create_delta_array(self):\n        return np.zeros((self.channel_number,\n                         self.input_height, self.input_width))\n\n    @staticmethod\n    def calculate_output_size(input_size,\n                              filter_size, zero_padding, stride):\n        return (input_size - filter_size +\n                2 * zero_padding) // stride + 1\n\n\nclass MaxPoolingLayer(object):\n    def __init__(self, input_width, input_height,\n                 channel_number, filter_width,\n                 filter_height, stride):\n        self.input_width = input_width\n        self.input_height = input_height\n        self.channel_number = channel_number\n        self.filter_width = filter_width\n        self.filter_height = filter_height\n        self.stride = stride\n        self.output_width = (input_width -\n                             filter_width) // self.stride + 1\n        self.output_height = (input_height -\n                              filter_height) // self.stride + 1\n        self.output_array = np.zeros((self.channel_number,\n                                      self.output_height, self.output_width))\n\n    def forward(self, input_array):\n        for d in range(self.channel_number):\n            for i in range(self.output_height):\n                for j in range(self.output_width):\n                    self.output_array[d, i, j] = (\n                        get_patch(input_array[d], i, j,\n                                  self.filter_width,\n                                  self.filter_height,\n                                  self.stride).max())\n\n    def backward(self, input_array, sensitivity_array):\n        self.delta_array = np.zeros(input_array.shape)\n        for d in range(self.channel_number):\n            for i in range(self.output_height):\n                for j in range(self.output_width):\n                    patch_array = get_patch(\n                        input_array[d], i, j,\n                        self.filter_width,\n                        self.filter_height,\n                        self.stride)\n                    k, l = get_max_index(patch_array)\n                    self.delta_array[d,\n                                     i * self.stride + k,\n                                     j * self.stride + l] = \\\n                        sensitivity_array[d, i, j]\n\n\ndef init_test():\n    a = np.array(\n        [[[0, 1, 1, 0, 2],\n          [2, 2, 2, 2, 1],\n          [1, 0, 0, 2, 0],\n          [0, 1, 1, 0, 0],\n          [1, 2, 0, 0, 2]],\n         [[1, 0, 2, 2, 0],\n          [0, 0, 0, 2, 0],\n          [1, 2, 1, 2, 1],\n          [1, 0, 0, 0, 0],\n          [1, 2, 1, 1, 1]],\n         [[2, 1, 2, 0, 0],\n          [1, 0, 0, 1, 0],\n          [0, 2, 1, 0, 1],\n          [0, 1, 2, 2, 2],\n          [2, 1, 0, 0, 1]]])\n    b = np.array(\n        [[[0, 1, 1],\n          [2, 2, 2],\n          [1, 0, 0]],\n         [[1, 0, 2],\n          [0, 0, 0],\n          [1, 2, 1]]])\n    cl = ConvLayer(5, 5, 3, 3, 3, 2, 1, 2, IdentityActivator(), 0.001)\n    cl.filters[0].weights = np.array(\n        [[[-1, 1, 0],\n          [0, 1, 0],\n          [0, 1, 1]],\n         [[-1, -1, 0],\n          [0, 0, 0],\n          [0, -1, 0]],\n         [[0, 0, -1],\n          [0, 1, 0],\n          [1, -1, -1]]], dtype=np.float64)\n    cl.filters[0].bias = 1\n    cl.filters[1].weights = np.array(\n        [[[1, 1, -1],\n          [-1, -1, 1],\n          [0, -1, 1]],\n         [[0, 1, 0],\n          [-1, 0, -1],\n          [-1, 1, 0]],\n         [[-1, 0, 0],\n          [-1, 0, 1],\n          [-1, 0, 0]]], dtype=np.float64)\n    return a, b, cl\n\n\ndef test():\n    a, b, cl = init_test()\n    cl.forward(a)\n    print(\n    cl.output_array)\n\n\ndef test_bp():\n    a, b, cl = init_test()\n    cl.backward(a, b, IdentityActivator())\n    cl.update()\n    print(\n    cl.filters[0])\n    print(\n    cl.filters[1])\n\ndef gradient_check():\n    '''\n    \u68af\u5ea6\u68c0\u67e5\n    '''\n    # \u8bbe\u8ba1\u4e00\u4e2a\u8bef\u5dee\u51fd\u6570\uff0c\u53d6\u6240\u6709\u8282\u70b9\u8f93\u51fa\u9879\u4e4b\u548c\n    error_function = lambda o: o.sum()\n\n    # \u8ba1\u7b97forward\u503c\n    a, b, cl = init_test()\n    cl.forward(a)\n\n    # \u6c42\u53d6sensitivity map\n    sensitivity_array = np.ones(cl.output_array.shape,\n                                dtype=np.float64)\n    # \u8ba1\u7b97\u68af\u5ea6\n    cl.backward(a, sensitivity_array,\n                IdentityActivator())\n    # \u68c0\u67e5\u68af\u5ea6\n    epsilon = 10e-4\n    for d in range(cl.filters[0].weights_grad.shape[0]):\n        for i in range(cl.filters[0].weights_grad.shape[1]):\n            for j in range(cl.filters[0].weights_grad.shape[2]):\n                cl.filters[0].weights[d, i, j] += epsilon\n                cl.forward(a)\n                err1 = error_function(cl.output_array)\n                cl.filters[0].weights[d, i, j] -= 2 * epsilon\n                cl.forward(a)\n                err2 = error_function(cl.output_array)\n                expect_grad = (err1 - err2) / (2 * epsilon)\n                cl.filters[0].weights[d, i, j] += epsilon\n                print(\n                'weights(%d,%d,%d): expected - actural %f - %f' % (\n                    d, i, j, expect_grad, cl.filters[0].weights_grad[d, i, j]))\n\n\ndef init_pool_test():\n    a = np.array(\n        [[[1, 1, 2, 4],\n          [5, 6, 7, 8],\n          [3, 2, 1, 0],\n          [1, 2, 3, 4]],\n         [[0, 1, 2, 3],\n          [4, 5, 6, 7],\n          [8, 9, 0, 1],\n          [3, 4, 5, 6]]], dtype=np.float64)\n\n    b = np.array(\n        [[[1, 2],\n          [2, 4]],\n         [[3, 5],\n          [8, 2]]], dtype=np.float64)\n\n    mpl = MaxPoolingLayer(4, 4, 2, 2, 2, 2)\n\n    return a, b, mpl\n\n\ndef test_pool():\n    a, b, mpl = init_pool_test()\n    mpl.forward(a)\n    print(\n    'input array:\\n%s\\noutput array:\\n%s' % (a,\n                                             mpl.output_array))\n\n\ndef test_pool_bp():\n    a, b, mpl = init_pool_test()\n    mpl.backward(a, b)\n    print(\n    'input array:\\n%s\\nsensitivity array:\\n%s\\ndelta array:\\n%s' % (\n        a, b, mpl.delta_array))\n\n\nif __name__=='__main__':\n    gradient_check()", "src/py3.x/tensorflow2.x/overfit_and_underfit.py": "\n# coding: utf-8\n# # \u63a2\u7d22\u8fc7\u62df\u5408\u548c\u6b20\u62df\u5408\n\n# \n# \u4e0e\u5f80\u5e38\u4e00\u6837\uff0c\u6b64\u793a\u4f8b\u4e2d\u7684\u4ee3\u7801\u5c06\u4f7f\u7528 `tf.keras` API\uff0c\u60a8\u53ef\u4ee5\u5728TensorFlow [Keras \u6307\u5357](https://www.tensorflow.org/guide/keras)\u4e2d\u4e86\u89e3\u66f4\u591a\u4fe1\u606f\u3002\n# \n# \u5728\u524d\u9762\u7684\u4e24\u4e2a\u793a\u4f8b\uff08\u5bf9\u7535\u5f71\u8bc4\u8bba\u8fdb\u884c\u5206\u7c7b\u548c\u9884\u6d4b\u71c3\u6cb9\u6548\u7387\uff09\u4e2d\uff0c\u6211\u4eec\u770b\u5230\u4e86\u5728\u9a8c\u8bc1\u6570\u636e\u4e0a\u7684\u6a21\u578b\u7684\u51c6\u786e\u6027\u5728\u7ecf\u8fc7\u591a\u4e2a\u65f6\u671f\u7684\u8bad\u7ec3\u540e\u5c06\u8fbe\u5230\u5cf0\u503c\uff0c\u7136\u540e\u5f00\u59cb\u4e0b\u964d\u3002\n# \n# \u6362\u53e5\u8bdd\u8bf4\uff0c\u6211\u4eec\u7684\u6a21\u578b\u5c06 *\u8fc7\u62df\u5408* \u8bad\u7ec3\u6570\u636e\u3002\u5b66\u4e60\u5982\u4f55\u5e94\u5bf9\u8fc7\u62df\u5408\u5f88\u91cd\u8981\u3002\u5c3d\u7ba1\u901a\u5e38\u53ef\u4ee5\u5728*\u8bad\u7ec3\u96c6*\u4e0a\u8fbe\u5230\u9ad8\u7cbe\u5ea6\uff0c\u4f46\u6211\u4eec\u771f\u6b63\u60f3\u8981\u7684\u662f\u5f00\u53d1\u80fd\u591f\u5f88\u597d\u5730\u63a8\u5e7f\u5230*\u6d4b\u8bd5\u96c6*\uff08\u6216\u4e4b\u524d\u672a\u89c1\u7684\u6570\u636e\uff09\u7684\u6a21\u578b\u3002\n# \n# \u8fc7\u62df\u5408\u7684\u53cd\u9762\u662f*\u6b20\u62df\u5408*\u3002\u5f53\u6d4b\u8bd5\u6570\u636e\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\u65f6\uff0c\u5c31\u4f1a\u53d1\u751f\u6b20\u62df\u5408\u3002\u53d1\u751f\u8fd9\u79cd\u60c5\u51b5\u7684\u539f\u56e0\u6709\u5f88\u591a: \u5982\u679c\u6a21\u578b\u4e0d\u591f\u5f3a\u5927\uff0c\u6a21\u578b\u8fc7\u4e8e\u89c4\u8303\u5316\uff0c\u6216\u8005\u4ec5\u4ec5\u662f\u6ca1\u6709\u7ecf\u8fc7\u8db3\u591f\u957f\u65f6\u95f4\u7684\u8bad\u7ec3\u3002\u8fd9\u610f\u5473\u7740\u7f51\u7edc\u5c1a\u672a\u5b66\u4e60\u8bad\u7ec3\u6570\u636e\u4e2d\u7684\u76f8\u5173\u6a21\u5f0f\u3002\n# \n# \u4f46\u662f\uff0c\u5982\u679c\u8bad\u7ec3\u65f6\u95f4\u8fc7\u957f\uff0c\u5219\u6a21\u578b\u5c06\u5f00\u59cb\u8fc7\u62df\u5408\u5e76\u4ece\u8bad\u7ec3\u6570\u636e\u4e2d\u5b66\u4e60\u65e0\u6cd5\u63a8\u5e7f\u5230\u6d4b\u8bd5\u6570\u636e\u7684\u6a21\u5f0f\u3002\u6211\u4eec\u9700\u8981\u4fdd\u6301\u5e73\u8861\u3002\u5982\u4e0b\u6240\u8ff0\uff0c\u4e86\u89e3\u5982\u4f55\u8bad\u7ec3\u9002\u5f53\u7684\u65f6\u671f\u662f\u4e00\u9879\u6709\u7528\u7684\u6280\u80fd\u3002\n# \n# \u4e3a\u4e86\u9632\u6b62\u8fc7\u62df\u5408\uff0c\u6700\u597d\u7684\u89e3\u51b3\u65b9\u6848\u662f\u4f7f\u7528\u66f4\u591a\u7684\u8bad\u7ec3\u6570\u636e\u3002\u7ecf\u8fc7\u66f4\u591a\u6570\u636e\u8bad\u7ec3\u7684\u6a21\u578b\u81ea\u7136\u4f1a\u66f4\u597d\u5730\u63a8\u5e7f\u3002\u5f53\u8fd9\u4e0d\u518d\u53ef\u80fd\u65f6\uff0c\u4e0b\u4e00\u4e2a\u6700\u4f73\u89e3\u51b3\u65b9\u6848\u662f\u4f7f\u7528\u6b63\u5219\u5316\u4e4b\u7c7b\u7684\u6280\u672f\u3002\u8fd9\u4e9b\u56e0\u7d20\u9650\u5236\u4e86\u6a21\u578b\u53ef\u4ee5\u5b58\u50a8\u7684\u4fe1\u606f\u7684\u6570\u91cf\u548c\u7c7b\u578b\u3002\u5982\u679c\u4e00\u4e2a\u7f51\u7edc\u53ea\u80fd\u5b58\u50a8\u5c11\u91cf\u6a21\u5f0f\uff0c\u90a3\u4e48\u4f18\u5316\u8fc7\u7a0b\u5c06\u8feb\u4f7f\u5b83\u4e13\u6ce8\u4e8e\u6700\u7a81\u51fa\u7684\u6a21\u5f0f\uff0c\u8fd9\u4e9b\u6a21\u5f0f\u6709\u66f4\u597d\u7684\u6982\u62ec\u673a\u4f1a\u3002\n# \n# \u5728\u672c\u7b14\u8bb0\u672c\u4e2d\uff0c\u6211\u4eec\u5c06\u63a2\u8ba8\u4e24\u79cd\u5e38\u89c1\u7684\u6b63\u5219\u5316\u6280\u672f\uff08\u6743\u91cd\u6b63\u5219\u5316\u548c dropout\uff09\uff0c\u5e76\u4f7f\u7528\u5b83\u4eec\u6765\u6539\u8fdb\u6211\u4eec\u7684IMDB\u7535\u5f71\u8bc4\u8bba\u5206\u7c7b\u7b14\u8bb0\u672c\u3002\n\n\n# In[ ]:\n\n\nfrom __future__ import absolute_import, division, print_function, unicode_literals\n\ntry:\n  # %tensorflow_version only exists in Colab.\n  get_ipython().run_line_magic('tensorflow_version', '2.x')\nexcept Exception:\n  pass\nimport tensorflow as tf\nfrom tensorflow import keras\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nprint(tf.__version__)\n\n\n# ## \u4e0b\u8f7dIMDB\u6570\u636e\u96c6\n# \n# \u800c\u4e0d\u662f\u50cf\u4ee5\u524d\u7684\u7b14\u8bb0\u672c\u4e2d\u90a3\u6837\u4f7f\u7528embedding\uff0c\u8fd9\u91cc\u6211\u4eec\u5c06\u5bf9\u53e5\u5b50\u8fdb\u884c multi-hot \u7f16\u7801\u3002 \u8be5\u6a21\u578b\u5c06\u5f88\u5feb\u9002\u5408\u8bad\u7ec3\u96c6\u3002 \u5b83\u5c06\u7528\u4e8e\u6f14\u793a\u4f55\u65f6\u53d1\u751f\u8fc7\u62df\u5408\u4ee5\u53ca\u5982\u4f55\u5e94\u5bf9\u3002\n# \n# \u5bf9\u5217\u8868\u8fdb\u884c multi-hot \u7f16\u7801\u610f\u5473\u7740\u5c06\u5b83\u4eec\u53d8\u62100\u548c1\u7684\u5411\u91cf\u3002 \u5177\u4f53\u6765\u8bf4\uff0c\u8fd9\u610f\u5473\u7740\u4f8b\u5982\u5c06\u5e8f\u5217 `[3, 5]` \u53d8\u621010,000\u7ef4\u5411\u91cf\uff0c\u8be5\u5411\u91cf\u9664\u4e86\u7d22\u5f153\u548c5\u5c06\u662f1\uff0c\u5176\u4ed6\u5c06\u662f\u5168\u4e3a\u96f6\u3002\n\n# In[ ]:\n\n\nNUM_WORDS = 10000\n\n(train_data, train_labels), (test_data, test_labels) = keras.datasets.imdb.load_data(num_words=NUM_WORDS)\n\ndef multi_hot_sequences(sequences, dimension):\n    # Create an all-zero matrix of shape (len(sequences), dimension)\n    results = np.zeros((len(sequences), dimension))\n    for i, word_indices in enumerate(sequences):\n        results[i, word_indices] = 1.0  # set specific indices of results[i] to 1s\n    return results\n\n\ntrain_data = multi_hot_sequences(train_data, dimension=NUM_WORDS)\ntest_data = multi_hot_sequences(test_data, dimension=NUM_WORDS)\n\n\n# \u8ba9\u6211\u4eec\u770b\u4e00\u4e0b\u4ea7\u751f\u7684 multi-hot \u5411\u91cf\u4e4b\u4e00\u3002 \u5355\u8bcd\u7d22\u5f15\u6309\u9891\u7387\u6392\u5e8f\uff0c\u56e0\u6b64\u53ef\u4ee5\u9884\u671f\u5728\u7d22\u5f15\u96f6\u9644\u8fd1\u6709\u66f4\u591a\u76841\u503c\uff0c\u5982\u6211\u4eec\u5728\u8be5\u56fe\u4e2d\u6240\u770b\u5230\u7684: \n# \n# \n# \n# \n\n# In[ ]:\n\n\nplt.plot(train_data[0])\n\n\n# ## \u8bc1\u660e\u8fc7\u62df\u5408\n# \n# \u9632\u6b62\u8fc7\u62df\u5408\u7684\u6700\u7b80\u5355\u65b9\u6cd5\u662f\u51cf\u5c0f\u6a21\u578b\u7684\u5927\u5c0f\uff0c\u5373\u51cf\u5c0f\u6a21\u578b\u4e2d\u53ef\u5b66\u4e60\u7684\u53c2\u6570\u7684\u6570\u91cf\uff08\u7531\u5c42\u6570\u548c\u6bcf\u5c42\u5355\u5143\u6570\u786e\u5b9a\uff09\u3002\u5728\u6df1\u5ea6\u5b66\u4e60\u4e2d\uff0c\u6a21\u578b\u4e2d\u53ef\u5b66\u4e60\u53c2\u6570\u7684\u6570\u91cf\u901a\u5e38\u79f0\u4e3a\u6a21\u578b\u7684\u201c\u5bb9\u91cf\u201d\u3002\u76f4\u89c2\u5730\u8bb2\uff0c\u5177\u6709\u66f4\u591a\u53c2\u6570\u7684\u6a21\u578b\u5c06\u5177\u6709\u66f4\u591a\u7684\u201c\u8bb0\u5fc6\u80fd\u529b\u201d\uff0c\u56e0\u6b64\u5c06\u80fd\u591f\u8f7b\u677e\u5b66\u4e60\u8bad\u7ec3\u6837\u672c\u4e0e\u5176\u76ee\u6807\u4e4b\u95f4\u7684\u5b8c\u7f8e\u7684\u5b57\u5178\u5f0f\u6620\u5c04\uff0c\u8fd9\u79cd\u6620\u5c04\u6ca1\u6709\u4efb\u4f55\u6cdb\u5316\u80fd\u529b\uff0c\u4f46\u662f\u5728\u8fdb\u884c\u9884\u6d4b\u65f6\u8fd9\u5c06\u662f\u65e0\u7528\u7684\u6839\u636e\u4ee5\u524d\u770b\u4e0d\u89c1\u7684\u6570\u636e\u3002\n# \n# \u59cb\u7ec8\u7262\u8bb0\u8fd9\u4e00\u70b9: \u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u5f80\u5f80\u64c5\u957f\u62df\u5408\u8bad\u7ec3\u6570\u636e\uff0c\u4f46\u771f\u6b63\u7684\u6311\u6218\u662f\u6cdb\u5316\u800c\u4e0d\u662f\u62df\u5408\u3002\n# \n# \u53e6\u4e00\u65b9\u9762\uff0c\u5982\u679c\u7f51\u7edc\u7684\u5b58\u50a8\u8d44\u6e90\u6709\u9650\uff0c\u5219\u5c06\u65e0\u6cd5\u8f7b\u677e\u5730\u5b66\u4e60\u6620\u5c04\u3002\u4e3a\u4e86\u6700\u5927\u7a0b\u5ea6\u5730\u51cf\u5c11\u635f\u5931\uff0c\u5b83\u5fc5\u987b\u5b66\u4e60\u5177\u6709\u66f4\u5f3a\u9884\u6d4b\u80fd\u529b\u7684\u538b\u7f29\u8868\u793a\u5f62\u5f0f\u3002\u540c\u65f6\uff0c\u5982\u679c\u60a8\u4f7f\u6a21\u578b\u8fc7\u5c0f\uff0c\u5c06\u96be\u4ee5\u62df\u5408\u8bad\u7ec3\u6570\u636e\u3002 \u201c\u5bb9\u91cf\u8fc7\u591a\u201d\u548c\u201c\u5bb9\u91cf\u4e0d\u8db3\u201d\u4e4b\u95f4\u5b58\u5728\u5e73\u8861\u3002\n# \n# \u4e0d\u5e78\u7684\u662f\uff0c\u6ca1\u6709\u795e\u5947\u7684\u516c\u5f0f\u6765\u786e\u5b9a\u6a21\u578b\u7684\u6b63\u786e\u5927\u5c0f\u6216\u4f53\u7cfb\u7ed3\u6784\uff08\u6839\u636e\u5c42\u6570\u6216\u6bcf\u5c42\u7684\u6b63\u786e\u5927\u5c0f\uff09\u3002\u60a8\u5c06\u4e0d\u5f97\u4e0d\u5c1d\u8bd5\u4f7f\u7528\u4e00\u7cfb\u5217\u4e0d\u540c\u7684\u4f53\u7cfb\u7ed3\u6784\u3002\n# \n# \u4e3a\u4e86\u627e\u5230\u5408\u9002\u7684\u6a21\u578b\u5927\u5c0f\uff0c\u6700\u597d\u4ece\u76f8\u5bf9\u8f83\u5c11\u7684\u56fe\u5c42\u548c\u53c2\u6570\u5f00\u59cb\uff0c\u7136\u540e\u5f00\u59cb\u589e\u52a0\u56fe\u5c42\u7684\u5927\u5c0f\u6216\u6dfb\u52a0\u65b0\u7684\u56fe\u5c42\uff0c\u76f4\u5230\u770b\u5230\u9a8c\u8bc1\u635f\u5931\u7684\u6536\u76ca\u9012\u51cf\u4e3a\u6b62\u3002\u8ba9\u6211\u4eec\u5728\u7535\u5f71\u8bc4\u8bba\u5206\u7c7b\u7f51\u7edc\u4e0a\u5c1d\u8bd5\u4e00\u4e0b\u3002\n# \n# \u6211\u4eec\u5c06\u4ec5\u4f7f\u7528 `Dense` \u5c42\u4f5c\u4e3a\u57fa\u51c6\u6765\u521b\u5efa\u4e00\u4e2a\u7b80\u5355\u7684\u6a21\u578b\uff0c\u7136\u540e\u521b\u5efa\u8f83\u5c0f\u548c\u8f83\u5927\u7684\u7248\u672c\u5e76\u8fdb\u884c\u6bd4\u8f83\u3002\n# \n# \n\n# ### Create a baseline model\n\n# In[ ]:\n\n\nbaseline_model = keras.Sequential([\n    # `input_shape` is only required here so that `.summary` works.\n    keras.layers.Dense(16, activation='relu', input_shape=(NUM_WORDS,)),\n    keras.layers.Dense(16, activation='relu'),\n    keras.layers.Dense(1, activation='sigmoid')\n])\n\nbaseline_model.compile(optimizer='adam',\n                       loss='binary_crossentropy',\n                       metrics=['accuracy', 'binary_crossentropy'])\n\nbaseline_model.summary()\n\n\n# In[ ]:\n\n\nbaseline_history = baseline_model.fit(train_data,\n                                      train_labels,\n                                      epochs=20,\n                                      batch_size=512,\n                                      validation_data=(test_data, test_labels),\n                                      verbose=2)\n\n\n# ### \u521b\u5efa smaller model\n# \n# \u8ba9\u6211\u4eec\u521b\u5efa\u4e00\u4e2a\u9690\u85cf\u5355\u5143\u66f4\u5c11\u7684\u6a21\u578b\uff0c\u4ee5\u4e0e\u6211\u4eec\u521a\u521a\u521b\u5efa\u7684\u57fa\u7ebf\u6a21\u578b\u8fdb\u884c\u6bd4\u8f83: \n\n# In[ ]:\n\n\nsmaller_model = keras.Sequential([\n    keras.layers.Dense(4, activation='relu', input_shape=(NUM_WORDS,)),\n    keras.layers.Dense(4, activation='relu'),\n    keras.layers.Dense(1, activation='sigmoid')\n])\n\nsmaller_model.compile(optimizer='adam',\n                      loss='binary_crossentropy',\n                      metrics=['accuracy', 'binary_crossentropy'])\n\nsmaller_model.summary()\n\n\n# \u5e76\u4f7f\u7528\u76f8\u540c\u7684\u6570\u636e\u8bad\u7ec3\u6a21\u578b: \n\n# In[ ]:\n\n\nsmaller_history = smaller_model.fit(train_data,\n                                    train_labels,\n                                    epochs=20,\n                                    batch_size=512,\n                                    validation_data=(test_data, test_labels),\n                                    verbose=2)\n\n\n# ### \u521b\u5efa bigger model\n# \n# \u4f5c\u4e3a\u7ec3\u4e60\uff0c\u60a8\u53ef\u4ee5\u521b\u5efa\u4e00\u4e2a\u66f4\u5927\u7684\u6a21\u578b\uff0c\u5e76\u67e5\u770b\u5b83\u5f00\u59cb\u8fc7\u62df\u5408\u7684\u901f\u5ea6\u3002 \u63a5\u4e0b\u6765\uff0c\u8ba9\u6211\u4eec\u5c06\u5177\u6709\u66f4\u5927\u5bb9\u91cf\u7684\u7f51\u7edc\u6dfb\u52a0\u5230\u6b64\u57fa\u51c6\u7f51\u7edc\u4e2d\uff0c\u8fdc\u8fdc\u8d85\u51fa\u95ee\u9898\u6240\u80fd\u4fdd\u8bc1\u7684\u8303\u56f4: \n\n# In[ ]:\n\n\nbigger_model = keras.models.Sequential([\n    keras.layers.Dense(512, activation='relu', input_shape=(NUM_WORDS,)),\n    keras.layers.Dense(512, activation='relu'),\n    keras.layers.Dense(1, activation='sigmoid')\n])\n\nbigger_model.compile(optimizer='adam',\n                     loss='binary_crossentropy',\n                     metrics=['accuracy','binary_crossentropy'])\n\nbigger_model.summary()\n\n\n# \u518d\u6b21\uff0c\u4f7f\u7528\u76f8\u540c\u7684\u6570\u636e\u8bad\u7ec3\u6a21\u578b: \n\n# In[ ]:\n\n\nbigger_history = bigger_model.fit(train_data, train_labels,\n                                  epochs=20,\n                                  batch_size=512,\n                                  validation_data=(test_data, test_labels),\n                                  verbose=2)\n\n\n# ### \u7ed8\u5236\u8bad\u7ec3\u548c\u9a8c\u8bc1\u635f\u5931\n# \n# <!--TODO(markdaoust): This should be a one-liner with tensorboard -->\n# \n# \u5b9e\u7ebf\u8868\u793a\u8bad\u7ec3\u635f\u5931\uff0c\u800c\u865a\u7ebf\u8868\u793a\u9a8c\u8bc1\u635f\u5931\uff08\u8bf7\u8bb0\u4f4f: \u9a8c\u8bc1\u635f\u5931\u8d8a\u5c0f\u8868\u793a\u6a21\u578b\u8d8a\u597d\uff09\u3002 \u5728\u8fd9\u91cc\uff0c\u8f83\u5c0f\u7684\u7f51\u7edc\u6bd4\u57fa\u51c6\u6a21\u578b\u5f00\u59cb\u8fc7\u5ea6\u62df\u5408\uff08\u57286\u4e2a\u65f6\u671f\u800c\u4e0d\u662f4\u4e2a\u5468\u671f\u4e4b\u540e\uff09\uff0c\u5e76\u4e14\u4e00\u65e6\u5f00\u59cb\u8fc7\u5ea6\u62df\u5408\uff0c\u5176\u6027\u80fd\u4e0b\u964d\u7684\u901f\u5ea6\u5c31\u4f1a\u6162\u5f97\u591a\u3002\n# \n\n# In[ ]:\n\n\ndef plot_history(histories, key='binary_crossentropy'):\n  plt.figure(figsize=(16,10))\n\n  for name, history in histories:\n    val = plt.plot(history.epoch, history.history['val_'+key],\n                   '--', label=name.title()+' Val')\n    plt.plot(history.epoch, history.history[key], color=val[0].get_color(),\n             label=name.title()+' Train')\n\n  plt.xlabel('Epochs')\n  plt.ylabel(key.replace('_',' ').title())\n  plt.legend()\n\n  plt.xlim([0,max(history.epoch)])\n\n\nplot_history([('baseline', baseline_history),\n              ('smaller', smaller_history),\n              ('bigger', bigger_history)])\n\n\n# \u8bf7\u6ce8\u610f\uff0c\u8f83\u5927\u7684\u7f51\u7edc\u4ec5\u5728\u4e00\u4e2a\u65f6\u671f\u540e\u5c31\u5f00\u59cb\u8fc7\u62df\u5408\uff0c\u800c\u4e14\u8fc7\u62df\u5408\u4e25\u91cd\u3002\u7f51\u7edc\u7684\u5bb9\u91cf\u8d8a\u591a\uff0c\u5c06\u80fd\u591f\u66f4\u5feb\u5730\u5bf9\u8bad\u7ec3\u6570\u636e\u8fdb\u884c\u5efa\u6a21\uff08\u5bfc\u81f4\u8f83\u4f4e\u7684\u8bad\u7ec3\u635f\u5931\uff09\uff0c\u4f46\u7f51\u7edc\u8d8a\u5bb9\u6613\u8fc7\u62df\u5408\uff08\u5bfc\u81f4\u8bad\u7ec3\u548c\u9a8c\u8bc1\u635f\u5931\u4e4b\u95f4\u5b58\u5728\u8f83\u5927\u5dee\u5f02\uff09\u3002\n\n# ## \u9632\u6b62\u8fc7\u5ea6\u62df\u5408\u7684\u7b56\u7565\n\n# ### \u6dfb\u52a0\u6743\u91cd\u6b63\u5219\u5316\n\n# \u60a8\u53ef\u80fd\u719f\u6089Occam\u7684Razor\u539f\u7406: \u7ed9\u67d0\u4e8b\u4e24\u79cd\u89e3\u91ca\uff0c\u6700\u53ef\u80fd\u6b63\u786e\u7684\u89e3\u91ca\u662f\u201c\u6700\u7b80\u5355\u201d\u7684\u89e3\u91ca\uff0c\u5373\u5047\u8bbe\u6700\u5c11\u7684\u4e00\u79cd\u3002\u8fd9\u4e5f\u9002\u7528\u4e8e\u901a\u8fc7\u795e\u7ecf\u7f51\u7edc\u5b66\u4e60\u7684\u6a21\u578b: \u7ed9\u5b9a\u4e00\u4e9b\u8bad\u7ec3\u6570\u636e\u548c\u7f51\u7edc\u4f53\u7cfb\u7ed3\u6784\uff0c\u53ef\u4ee5\u4f7f\u7528\u591a\u7ec4\u6743\u91cd\u503c\uff08\u591a\u4e2a\u6a21\u578b\uff09\u6765\u89e3\u91ca\u6570\u636e\uff0c\u5e76\u4e14\u8f83\u7b80\u5355\u7684\u6a21\u578b\u6bd4\u590d\u6742\u7684\u6a21\u578b\u4e0d\u592a\u53ef\u80fd\u8fc7\u62df\u5408\u3002\n# \n# \u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u201c\u7b80\u5355\u6a21\u578b\u201d\u662f\u53c2\u6570\u503c\u7684\u5206\u5e03\u5177\u6709\u8f83\u5c0f\u71b5\u7684\u6a21\u578b\uff08\u6216\u5982\u4e0a\u8282\u6240\u8ff0\uff0c\u5177\u6709\u603b\u5171\u8f83\u5c11\u53c2\u6570\u7684\u6a21\u578b\uff09\u3002\u56e0\u6b64\uff0c\u51cf\u8f7b\u8fc7\u62df\u5408\u7684\u4e00\u79cd\u901a\u7528\u65b9\u6cd5\u662f\u901a\u8fc7\u4ec5\u5c06\u7f51\u7edc\u7684\u6743\u91cd\u5f3a\u5236\u53d6\u5c0f\u7684\u503c\u6765\u5bf9\u7f51\u7edc\u7684\u590d\u6742\u6027\u65bd\u52a0\u7ea6\u675f\uff0c\u8fd9\u4f7f\u5f97\u6743\u91cd\u503c\u7684\u5206\u5e03\u66f4\u52a0\u201c\u89c4\u5219\u201d\u3002\u8fd9\u79f0\u4e3a\u201c\u6743\u91cd\u8c03\u6574\u201d\uff0c\u5b83\u662f\u901a\u8fc7\u5411\u7f51\u7edc\u7684\u635f\u5931\u51fd\u6570\u4e2d\u6dfb\u52a0\u4e0e\u6743\u91cd\u8f83\u5927\u76f8\u5173\u7684\u6210\u672c\u6765\u5b8c\u6210\u7684\u3002\u4ee5\u4e0b\u6709\u4e24\u79cd\u5f62\u5f0f: \n# \n# * [L1\u6b63\u5219\u5316](https://developers.google.com/machine-learning/glossary/#L1_regularization)\uff0c\u5176\u4e2d\u589e\u52a0\u7684\u6210\u672c\u4e0e\u6743\u91cd\u7cfb\u6570\u7684\u7edd\u5bf9\u503c\u6210\u6b63\u6bd4\uff08\u5373\u6240\u8c13\u7684\u201c L1\u89c4\u8303\u201d \u201d\uff09\u3002\n# \n# * [L2\u6b63\u5219\u5316](https://developers.google.com/machine-learning/glossary/#L2_regularization)\uff0c\u5176\u4e2d\u589e\u52a0\u7684\u6210\u672c\u4e0e\u6743\u91cd\u7cfb\u6570\u7684\u503c\u7684\u5e73\u65b9\u6210\u6b63\u6bd4\uff08\u5373\u4e0e\u5e73\u65b9\u7684\u5e73\u65b9\u6210\u6b63\u6bd4\uff09\u6743\u91cd\u7684\u201c L2\u89c4\u8303\u201d\u3002 L2\u6b63\u5219\u5316\u5728\u795e\u7ecf\u7f51\u7edc\u4e2d\u4e5f\u79f0\u4e3a\u6743\u91cd\u8870\u51cf\u3002\u4e0d\u8981\u8ba9\u5176\u4ed6\u540d\u79f0\u4f7f\u60a8\u611f\u5230\u56f0\u60d1: \u6743\u91cd\u8870\u51cf\u5728\u6570\u5b66\u4e0a\u4e0eL2\u6b63\u5219\u5316\u5b8c\u5168\u76f8\u540c\u3002\n# \n# L1\u6b63\u5219\u5316\u5f15\u5165\u7a00\u758f\u6027\uff0c\u4ee5\u4f7f\u60a8\u7684\u67d0\u4e9b\u6743\u91cd\u53c2\u6570\u4e3a\u96f6\u3002 L2\u6b63\u5219\u5316\u5c06\u60e9\u7f5a\u6743\u91cd\u53c2\u6570\u800c\u4e0d\u4f7f\u5176\u7a00\u758f\uff0c\u8fd9\u662fL2\u66f4\u4e3a\u5e38\u89c1\u7684\u539f\u56e0\u4e4b\u4e00\u3002\n# \n# \u5728 `tf.keras` \u4e2d\uff0c\u901a\u8fc7\u5c06\u6743\u91cd\u6b63\u5219\u5316\u5668\u5b9e\u4f8b\u4f5c\u4e3a\u5173\u952e\u5b57\u53c2\u6570\u4f20\u9012\u7ed9\u56fe\u5c42\u6765\u6dfb\u52a0\u6743\u91cd\u6b63\u5219\u5316\u3002\u8ba9\u6211\u4eec\u73b0\u5728\u6dfb\u52a0L2\u6743\u91cd\u6b63\u5219\u5316\u3002\n\n# In[ ]:\n\n\nl2_model = keras.models.Sequential([\n    keras.layers.Dense(16, kernel_regularizer=keras.regularizers.l2(0.001),\n                       activation='relu', input_shape=(NUM_WORDS,)),\n    keras.layers.Dense(16, kernel_regularizer=keras.regularizers.l2(0.001),\n                       activation='relu'),\n    keras.layers.Dense(1, activation='sigmoid')\n])\n\nl2_model.compile(optimizer='adam',\n                 loss='binary_crossentropy',\n                 metrics=['accuracy', 'binary_crossentropy'])\n\nl2_model_history = l2_model.fit(train_data, train_labels,\n                                epochs=20,\n                                batch_size=512,\n                                validation_data=(test_data, test_labels),\n                                verbose=2)\n\n\n# `l2(0.001)` \u8868\u793a\u8be5\u5c42\u6743\u91cd\u77e9\u9635\u4e2d\u7684\u6bcf\u4e2a\u7cfb\u6570\u5c06\u4e3a\u7f51\u7edc\u7684\u603b\u635f\u8017\u589e\u52a0 `0.001 * weight_coefficient_value**2`\u3002 \u8bf7\u6ce8\u610f\uff0c\u7531\u4e8e\u6b64\u60e9\u7f5a\u4ec5\u5728\u8bad\u7ec3\u65f6\u589e\u52a0\uff0c\u56e0\u6b64\u5728\u8bad\u7ec3\u65f6\u6b64\u7f51\u7edc\u7684\u635f\u5931\u5c06\u6bd4\u5728\u6d4b\u8bd5\u65f6\u9ad8\u5f97\u591a\u3002\n# \n# \u8fd9\u662f\u6211\u4eec\u7684L2\u6b63\u5219\u5316\u60e9\u7f5a\u7684\u5f71\u54cd: \n# \n\n# In[ ]:\n\n\nplot_history([('baseline', baseline_history),\n              ('l2', l2_model_history)])\n\n\n# \u5982\u60a8\u6240\u89c1\uff0c\u5373\u4f7f\u4e24\u4e2a\u6a21\u578b\u5177\u6709\u76f8\u540c\u6570\u91cf\u7684\u53c2\u6570\uff0cL2\u6b63\u5219\u5316\u6a21\u578b\u4e5f\u6bd4\u57fa\u7ebf\u6a21\u578b\u5177\u6709\u66f4\u9ad8\u7684\u6297\u8fc7\u5ea6\u62df\u5408\u80fd\u529b\u3002\n\n# ### \u6dfb\u52a0 dropout\n# \n# dropout \u662f Hinton \u548c\u4ed6\u5728\u591a\u4f26\u591a\u5927\u5b66\u7684\u5b66\u751f\u5f00\u53d1\u7684\u6700\u6709\u6548\uff0c\u6700\u5e38\u7528\u7684\u795e\u7ecf\u7f51\u7edc\u6b63\u5219\u5316\u6280\u672f\u4e4b\u4e00\u3002\u5e94\u7528\u4e8e\u56fe\u5c42\u7684\u8f8d\u5b66\u5305\u62ec\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u968f\u673a\u201cdropping out\u201d\uff08\u5373\u8bbe\u7f6e\u4e3a\u96f6\uff09\u8be5\u56fe\u5c42\u7684\u8bb8\u591a\u8f93\u51fa\u7279\u5f81\u3002\u5047\u8bbe\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\uff0c\u7ed9\u5b9a\u7684\u56fe\u5c42\u901a\u5e38\u4f1a\u4e3a\u7ed9\u5b9a\u7684\u8f93\u5165\u6837\u672c\u8fd4\u56de\u5411\u91cf  [0.2, 0.5, 1.3, 0.8, 1.1]\uff1b\u5e94\u7528\u5220\u9664\u540e\uff0c\u6b64\u5411\u91cf\u5c06\u6709\u4e00\u4e9b\u96f6\u4e2a\u6761\u76ee\u968f\u673a\u5206\u5e03\uff0c\u4f8b\u5982 [0, 0.5, 1.3, 0, 1.1]\u3002 \u201cdropout \u7387\u201d\u662f\u88ab\u6e05\u96f6\u7684\u7279\u5f81\u7684\u4e00\u90e8\u5206\u3002\u901a\u5e38\u8bbe\u7f6e\u57280.2\u52300.5\u4e4b\u95f4\u3002\u5728\u6d4b\u8bd5\u65f6\uff0c\u4e0d\u4f1a\u4e22\u5931\u4efb\u4f55\u5355\u5143\uff0c\u800c\u662f\u5c06\u56fe\u5c42\u7684\u8f93\u51fa\u503c\u6309\u7b49\u4e8e\u4e22\u5931\u7387\u7684\u6bd4\u4f8b\u7f29\u5c0f\uff0c\u4ee5\u5e73\u8861\u4e00\u4e2a\u6d3b\u8dc3\u7684\u5355\u5143\uff08\u800c\u4e0d\u662f\u8bad\u7ec3\u65f6\uff09\u7684\u4e8b\u5b9e\u3002\n# \n# \u5728tf.keras\u4e2d\uff0c\u60a8\u53ef\u4ee5\u901a\u8fc7Dropout\u5c42\u5728\u7f51\u7edc\u4e2d\u5f15\u5165Dropout\uff0c\u8be5\u5c42\u5c06\u7acb\u5373\u5e94\u7528\u4e8e\u8be5\u5c42\u7684\u8f93\u51fa\u3002\n# \n# \u8ba9\u6211\u4eec\u5728IMDB\u7f51\u7edc\u4e2d\u6dfb\u52a0\u4e24\u4e2aDropout\u5c42\uff0c\u770b\u770b\u5b83\u4eec\u5728\u51cf\u5c11\u8fc7\u62df\u5408\u65b9\u9762\u7684\u8868\u73b0\u5982\u4f55: \n\n# In[ ]:\n\n\ndpt_model = keras.models.Sequential([\n    keras.layers.Dense(16, activation='relu', input_shape=(NUM_WORDS,)),\n    keras.layers.Dropout(0.5),\n    keras.layers.Dense(16, activation='relu'),\n    keras.layers.Dropout(0.5),\n    keras.layers.Dense(1, activation='sigmoid')\n])\n\ndpt_model.compile(optimizer='adam',\n                  loss='binary_crossentropy',\n                  metrics=['accuracy','binary_crossentropy'])\n\ndpt_model_history = dpt_model.fit(train_data, train_labels,\n                                  epochs=20,\n                                  batch_size=512,\n                                  validation_data=(test_data, test_labels),\n                                  verbose=2)\n\n\n# In[ ]:\n\n\nplot_history([('baseline', baseline_history),\n              ('dropout', dpt_model_history)])\n\n\n# \u6dfb\u52a0 dropout \u662f\u5bf9\u57fa\u7ebf\u6a21\u578b\u7684\u660e\u663e\u6539\u8fdb\u3002\n# \n# \u56de\u987e\u4e00\u4e0b: \u4ee5\u4e0b\u662f\u9632\u6b62\u795e\u7ecf\u7f51\u7edc\u8fc7\u62df\u5408\u7684\u6700\u5e38\u7528\u65b9\u6cd5: \n# \n# * \u83b7\u53d6\u66f4\u591a\u8bad\u7ec3\u6570\u636e\n# * \u51cf\u5c11\u7f51\u7edc\u5bb9\u91cf\n# * \u6dfb\u52a0\u6743\u91cd\u8c03\u6574\n# * \u6dfb\u52a0 dropout\n# \n# \u672c\u6307\u5357\u672a\u6db5\u76d6\u7684\u4e24\u4e2a\u91cd\u8981\u65b9\u6cd5\u662f\u6570\u636e\u589e\u5f3a\u548c\u6279\u5904\u7406\u89c4\u8303\u5316\u3002\n", "src/py3.x/tensorflow2.x/text_Emotion.py": "# *-* coding:utf-8 *-*\n# \u8bcd\u5411\u91cf: \n#   https://www.cnblogs.com/Darwin2000/p/5786984.html\n# \u6570\u636e\u96c6:\n#   https://blog.csdn.net/alip39/article/details/95891321\n# \u53c2\u8003\u4ee3\u7801:\n#   https://blog.csdn.net/u012052268/article/details/90238282\n# Attention:\n#   https://github.com/philipperemy/keras-attention-mechanism\nimport re\nimport os\nimport keras\nimport random\nimport gensim\nimport numpy as np\nimport pandas as pd\nimport jieba\nfrom sklearn.model_selection import train_test_split\nfrom keras import Model\nfrom keras.models import load_model\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.layers import Dropout, Dense, Flatten, Bidirectional, Embedding, GRU, Input, multiply\n\"\"\"\n# padding: pre(\u9ed8\u8ba4) \u5411\u524d\u8865\u51450  post \u5411\u540e\u8865\u51450\n# truncating: \u6587\u672c\u8d85\u8fc7 pad_num,  pre(\u9ed8\u8ba4) \u5220\u9664\u524d\u9762  post \u5220\u9664\u540e\u9762\n# x_train = pad_sequences(x, maxlen=pad_num, value=0, padding='post', truncating=\"post\")\n# print(\"--- \", x_train[0][:20])\n\"\"\"\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils.np_utils import to_categorical\nfrom keras.optimizers import Adam\nfrom config import Config\nimport pickle\nimport matplotlib.pyplot as plt\n\n\n# \u5b58\u50a8\u6a21\u578b: \u6301\u4e45\u5316\ndef load_pkl(filename):\n    with open(filename, 'rb') as fr:\n        model = pickle.load(fr)\n    return model\n\n\ndef save_pkl(model, filename):\n    with open(filename, 'wb') as fw:\n        pickle.dump(model, fw)\n\n\n## \u8bad\u7ec3\u81ea\u5df1\u7684\u8bcd\u5411\u91cf\uff0c\u5e76\u4fdd\u5b58\u3002\ndef trainWord2Vec(infile, outfile):\n    sentences =  gensim.models.word2vec.LineSentence(infile) # \u8bfb\u53d6\u5206\u8bcd\u540e\u7684 \u6587\u672c\n    model = gensim.models.Word2Vec(sentences, size=100, window=5, min_count=1, workers=4) # \u8bad\u7ec3\u6a21\u578b\n    model.save(outfile)\n\n\ndef loadMyWord2Vec(outfile):\n    # \u5bfc\u5165 \u9884\u8bad\u7ec3\u7684\u8bcd\u5411\u91cf\n    Word2VecModel = gensim.models.Word2Vec.load(outfile)\n    return Word2VecModel\n\n\ndef load_embeding():\n    # \u8bad\u7ec3\u8bcd\u5411\u91cf(\u7528\u7a7a\u683c\u9694\u5f00\u7684\u6587\u672c)\n    infile = \"./CarCommentAll_cut.csv\"\n    outfile = \"/opt/data/nlp/\u5f00\u6e90\u8bcd\u5411\u91cf/gensim_word2vec_60/Word60.model\"\n    # trainWord2Vec(infile, outfile)\n    # \u52a0\u8f7d\u8bcd\u5411\u91cf\n    Word2VecModel = loadMyWord2Vec(outfile)\n\n    print('\u7a7a\u95f4\u7684\u8bcd\u5411\u91cf\uff0860 \u7ef4\uff09:', Word2VecModel.wv['\u7a7a\u95f4'].shape, Word2VecModel.wv['\u7a7a\u95f4'])\n    print('\u6253\u5370\u4e0e\u7a7a\u95f4\u6700\u76f8\u8fd1\u76845\u4e2a\u8bcd\u8bed: ', Word2VecModel.wv.most_similar('\u7a7a\u95f4', topn=5))\n\n    ## 2 \u6784\u9020\u5305\u542b\u6240\u6709\u8bcd\u8bed\u7684 list\uff0c\u4ee5\u53ca\u521d\u59cb\u5316 \u201c\u8bcd\u8bed-\u5e8f\u53f7\u201d\u5b57\u5178 \u548c \u201c\u8bcd\u5411\u91cf\u201d\u77e9\u9635\n    vocab_list = [word for word, Vocab in Word2VecModel.wv.vocab.items()]# \u5b58\u50a8 \u6240\u6709\u7684 \u8bcd\u8bed\n\n    word_index = {\" \": 0}# \u521d\u59cb\u5316 `[word : token]` \uff0c\u540e\u671f tokenize \u8bed\u6599\u5e93\u5c31\u662f\u7528\u8be5\u8bcd\u5178\u3002\n    word_vector = {} # \u521d\u59cb\u5316`[word : vector]`\u5b57\u5178\n\n    # \u521d\u59cb\u5316\u5b58\u50a8\u6240\u6709\u5411\u91cf\u7684\u5927\u77e9\u9635\uff0c\u7559\u610f\u5176\u4e2d\u591a\u4e00\u4f4d\uff08\u9996\u884c\uff09\uff0c\u8bcd\u5411\u91cf\u5168\u4e3a 0\uff0c\u7528\u4e8e padding\u8865\u96f6\u3002\n    # \u884c\u6570 \u4e3a \u6240\u6709\u5355\u8bcd\u6570+1 \u6bd4\u5982 10000+1 \uff1b \u5217\u6570\u4e3a \u8bcd\u5411\u91cf\u201c\u7ef4\u5ea6\u201d\u6bd4\u598260\u3002\n    embeddings_matrix = np.zeros((len(vocab_list) + 1, Word2VecModel.vector_size))\n\n    ## 3 \u586b\u5145 \u4e0a\u8ff0 \u7684\u5b57\u5178 \u548c \u5927\u77e9\u9635\n    for i in range(len(vocab_list)):\n        # print(i)\n        word = vocab_list[i]  # \u6bcf\u4e2a\u8bcd\u8bed\n        word_index[word] = i + 1 # \u8bcd\u8bed: \u5e8f\u53f7\n        word_vector[word] = Word2VecModel.wv[word] # \u8bcd\u8bed: \u8bcd\u5411\u91cf\n        embeddings_matrix[i + 1] = Word2VecModel.wv[word]  # \u8bcd\u5411\u91cf\u77e9\u9635\n    print(\"\u52a0\u8f7d\u8bcd\u5411\u91cf\u7ed3\u675f..\")\n    return vocab_list, word_index, embeddings_matrix\n\n\ndef plot_history(history):\n    history_dict = history.history\n    print(history_dict.keys())\n    acc = history_dict['accuracy']\n    val_acc = history_dict['val_accuracy']\n    loss = history_dict['loss']\n    val_loss = history_dict['val_loss']\n    epochs = range(1, len(acc) + 1)\n    # \u201cbo\u201d\u4ee3\u8868 \"\u84dd\u70b9\"\n    plt.plot(epochs, loss, 'bo', label='Training loss')\n    # b\u4ee3\u8868\u201c\u84dd\u8272\u5b9e\u7ebf\u201d\n    plt.plot(epochs, val_loss, 'b', label='Validation loss')\n    plt.title('Training and validation loss')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.savefig('Emotion_loss.png')\n    # plt.show()\n\n    plt.clf()   # \u6e05\u9664\u6570\u5b57\n\n    plt.plot(epochs, acc, 'bo', label='Training acc')\n    plt.plot(epochs, val_acc, 'b', label='Validation acc')\n    plt.title('Training and validation accuracy')\n    plt.xlabel('Epochs')\n    plt.ylabel('Accuracy')\n    plt.legend()\n    plt.savefig('Emotion_acc.png')\n    # plt.show()\n\n\nclass EmotionModel(object):\n    def __init__(self, config):\n        self.model = None\n        self.config = config\n        self.pre_num = self.config.pre_num\n        self.data_file = self.config.data_file\n        self.vocab_list = self.config.vocab_list\n        self.word_index = self.config.word_index\n        self.EMBEDDING_DIM = self.config.EMBEDDING_DIM\n        self.MAX_SEQUENCE_LENGTH = self.config.MAX_SEQUENCE_LENGTH\n\n        # \u5982\u679c\u6a21\u578b\u6587\u4ef6\u5b58\u5728\u5219\u76f4\u63a5\u52a0\u8f7d\u6a21\u578b\uff0c\u5426\u5219\u5f00\u59cb\u8bad\u7ec3\n        if os.path.exists(self.config.model_file):\n            self.model = load_model(self.config.model_file)\n            self.model.summary()\n        else:\n            self.train()\n\n    def build_model(self, embeddings_matrix):\n        ## 4 \u5728 keras\u7684Embedding\u5c42\u4e2d\u4f7f\u7528 \u9884\u8bad\u7ec3\u8bcd\u5411\u91cf\n        embedding_layer = Embedding(\n            input_dim = len(embeddings_matrix), # \u5b57\u5178\u957f\u5ea6\n            output_dim = self.EMBEDDING_DIM, # \u8bcd\u5411\u91cf \u957f\u5ea6\uff0860\uff09\n            weights = [embeddings_matrix], # \u91cd\u70b9: \u9884\u8bad\u7ec3\u7684\u8bcd\u5411\u91cf\u7cfb\u6570\n            input_length = self.MAX_SEQUENCE_LENGTH, # \u6bcf\u53e5\u8bdd\u7684 \u6700\u5927\u957f\u5ea6\uff08\u5fc5\u987bpadding\uff09 \n            trainable = False # \u662f\u5426\u5728 \u8bad\u7ec3\u7684\u8fc7\u7a0b\u4e2d \u66f4\u65b0\u8bcd\u5411\u91cf\n        )\n        # \u5982\u679c\u4e0d\u52a0\u8f7d\u5916\u754c\u7684\uff0c\u53ef\u4ee5\u81ea\u5df1\u8bad\u7ec3\n        # \u53ef\u4ee5\u770b\u51fa\u5728\u4f7f\u7528 Keras\u7684\u4e2dEmbedding\u5c42\u65f6\u5019\uff0c\u4e0d\u6307\u5b9a\u53c2\u6570 weights=[embeddings_matrix] \u5373\u53ef\u81ea\u52a8\u751f\u6210\u8bcd\u5411\u91cf\u3002\n        # embedding_layer = Embedding(\n        #     input_dim = len(word_index) + 1, # \u7531\u4e8e \u6ca1\u6709\u9884\u8bad\u7ec3\uff0c\u8bbe\u7f6e+1 \n        #     output_dim = EMBEDDING_DIM, # \u8bbe\u7f6e\u8bcd\u5411\u91cf\u7684\u7ef4\u5ea6\n        #     input_length=MAX_SEQUENCE_LENGTH\n        # ) #\u8bbe\u7f6e\u53e5\u5b50\u7684\u6700\u5927\u957f\u5ea6\n        print(\"\u5f00\u59cb\u8bad\u7ec3\u6a21\u578b.....\")\n        sequence_input = Input(shape=(self.MAX_SEQUENCE_LENGTH,), dtype='int32')  # \u8fd4\u56de\u4e00\u4e2a\u5f20\u91cf\uff0c\u957f\u5ea6\u4e3a1000\uff0c\u4e5f\u5c31\u662f\u6a21\u578b\u7684\u8f93\u5165\u4e3abatch_size*1000\n        embedded_sequences = embedding_layer(sequence_input)  # \u8fd4\u56debatch_size*1000*100\n        # \u6dfb\u52a0 \u6ce8\u610f\u529b(\u672c\u8d28\u4e0a\u662f\u901a\u8fc7\u52a0\u5165  \u4e00\u4e2a\u968f\u673a\u5411\u91cf \u4f5c\u4e3a \u6743\u91cd \u6765\u4f18\u5316 \u8f93\u5165\u7684\u503c - \u4e0e\u5168\u94fe\u63a5\u4e0d\u540c\u7684\u662f\uff0c\u8fd9\u4e2a\u8fd8\u4f1a\u4f5c\u4e3a\u8f93\u5165\u9879 \u548c \u8f93\u5165\u505a\u70b9\u4e58 )\n        attention_probs = Dense(self.EMBEDDING_DIM, activation='softmax', name='attention_probs')(embedded_sequences)\n        attention_mul = multiply([embedded_sequences, attention_probs], name='attention_mul')\n        x = Bidirectional(GRU(self.EMBEDDING_DIM, return_sequences=True, dropout=0.5))(attention_mul)\n        x = Dropout(0.5)(x)\n        x = Flatten()(x)\n        # x = BatchNormalization()(x)\n        preds = Dense(self.pre_num, activation='softmax')(x)\n        self.model = Model(sequence_input, preds)\n        # \u8bbe\u7f6e\u4f18\u5316\u5668\n        optimizer = Adam(lr=self.config.learning_rate, beta_1=0.95, beta_2=0.999,epsilon=1e-08)\n        self.model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n        self.model.summary()\n\n    def load_word2jieba(self):\n        vocab_list = load_pkl(self.vocab_list)\n        if vocab_list != []:\n            print(\"\u52a0\u8f7d\u8bcd\u7684\u603b\u91cf: \", len(vocab_list))\n            for word in vocab_list:\n                jieba.add_word(word)\n\n    def predict(self, line):\n        '''\u9884\u6d4b'''\n        word_index = load_pkl(self.word_index)\n        STOPWORDS = [\"-\", \"\\t\", \"\\n\", \".\", \"\u3002\", \",\", \"\uff0c\", \";\", \"!\", \"\uff01\", \"?\", \"\uff1f\", \"%\"]\n        words = [word for word in jieba.cut(str(line), cut_all=False) if word not in STOPWORDS]\n        indexs = [word_index.get(word, 0) for word in words]\n        x_pred = pad_sequences([indexs], maxlen=self.MAX_SEQUENCE_LENGTH)\n        res = self.model.predict(x_pred, verbose=0)[0]\n        return res\n\n    def load_data(self, word_index, vocab_list, test_size=0.25):\n        STOPWORDS = [\"-\", \"\\t\", \"\\n\", \".\", \"\u3002\", \",\", \"\uff0c\", \";\", \"!\", \"\uff01\", \"?\", \"\uff1f\", \"%\"]\n        if vocab_list != []:\n            for word in vocab_list:\n                jieba.add_word(word)\n\n        def func(line):\n            # \u5c06\u6587\u672c ['1, 2, 3', '1, 2, .., n'] \u5206\u89e3\u4e3a: [[1, 2, 3], [1, 2, .., n]]\n            words = [word for word in jieba.cut(str(line), cut_all=False) if word not in STOPWORDS]\n            indexs = [word_index.get(word, 0) for word in words]\n            return indexs\n\n        df = pd.read_excel(self.data_file, header=0, error_bad_lines=False, encoding=\"utf_8_sig\")\n        x = df[\"comment\"].apply(lambda line: func(line)).tolist()\n        x = pad_sequences(x, maxlen=self.MAX_SEQUENCE_LENGTH)\n        y = df[\"label\"].tolist()\n        # \u6309\u7167\u5927\u5c0f\u548c\u987a\u5e8f\uff0c\u751f\u6210 label(0,1,2...\u81ea\u7136\u6570\u7c7b\u578b)\n        \"\"\"\n        In [7]: to_categorical(np.asarray([1,1,0,1,3]))\n        Out[7]:\n        array([[0., 1., 0., 0.],\n            [0., 1., 0., 0.],\n            [1., 0., 0., 0.],\n            [0., 1., 0., 0.],\n            [0., 0., 0., 1.]], dtype=float32)\n        \"\"\"\n        y = to_categorical(np.asarray(y))\n        x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=test_size, random_state=10000)\n        return (x_train, y_train), (x_test, y_test) \n\n    def train(self):\n        '''\u8bad\u7ec3\u6a21\u578b'''\n        vocab_list, word_index, embeddings_matrix = load_embeding()\n        save_pkl(vocab_list, self.vocab_list)\n        save_pkl(word_index, self.word_index)\n        (x_train, y_train), (x_test, y_test) = self.load_data(word_index, vocab_list)\n        print(\"---------\")\n        print(x_train[:3], \"\\n\", y_train[:3])\n        print(\"\\n\")\n        print(x_test[:3], \"\\n\", y_test[:3])\n        print(\"---------\")\n        self.build_model(embeddings_matrix)\n\n        # \u753b\u76f8\u5173\u7684 loss \u548c accuracy=(\u9884\u6d4b\u6b63\u786e-\u6b63or\u8d1f/\u603b\u9884\u6d4b\u7684)\n        history = self.model.fit(x_train, y_train, batch_size=60, epochs=40, validation_split=0.2, verbose=0)\n        plot_history(history)\n\n        # self.model.fit(x_train, y_train, batch_size=60, epochs=40)\n        self.model.evaluate(x_test, y_test, verbose=2)\n        self.model.save(self.config.model_file)\n\n\nif __name__ == '__main__':\n    # \u6d4b\u8bd5\u52a0\u8f7d\u5916\u754cword2vec\u8bcd\u5411\u91cf\n    # vocab_list, word_index, embeddings_matrix = load_embeding()\n    model = EmotionModel(Config)\n    status = False\n    while 1:\n        text = input(\"text:\")\n        if text in [\"exit\", \"quit\"]:\n            break\n        # \u9996\u6b21\u542f\u52a8\u52a0\u8f7djieba\u8bcd\u5e93\n        if not status:\n            model.load_word2jieba()\n            status = True\n        res = model.predict(text)\n        label_dic = {0:\"\u6d88\u6781\u7684\", 1:\"\u4e2d\u6027\u7684\", 2:\"\u79ef\u6781\u7684\"}\n        print(res, \" : \", label_dic[np.argmax(res)])\n", "src/py3.x/tensorflow2.x/config.py": "# *-* coding:utf-8 *-*\n'''\n@author: \u7247\u523b\n@date: 20200428 11:02\n'''\n\n\nclass Bert(object):\n    DEBUG = True\n    path_root = \"/home/wac/jiangzhonglian\"\n    if DEBUG:\n        path_root = \"/opt/data/nlp/\u5f00\u6e90\u8bcd\u5411\u91cf/bert\u5b98\u65b9\u7248\u9884\u8bad\u7ec3\u6a21\u578b\"\n\n    dict_path = '%s/chinese_L-12_H-768_A-12/vocab.txt' % path_root\n    path_config = '%s/chinese_L-12_H-768_A-12/bert_config.json' % path_root\n    path_checkpoint = '%s/chinese_L-12_H-768_A-12/bert_model.ckpt' % path_root\n    maxlen = 100\n    path_neg = \"Emotion/neg.xlsx\"\n    path_pos = \"Emotion/pos.xlsx\"\n\n\nclass Config(object):\n    poetry_file = 'poetry.txt'\n    weight_file = 'poetry_model.h5'\n    data_file = 'Emotion/EmotionData.xlsx'\n    model_file = 'Emotion/EmotionModel.h5'\n    vocab_list = 'Emotion/vocal_list.pkl'\n    word_index = 'Emotion/word_index.pkl'\n    # \u6839\u636e\u524d\u516d\u4e2a\u5b57\u9884\u6d4b\u7b2c\u4e03\u4e2a\u5b57\n    max_len = 6\n    batch_size = 512\n    learning_rate = 0.0005\n    pre_num = 3\n    MAX_SEQUENCE_LENGTH = 1000  # \u6bcf\u4e2a\u6587\u672c\u6216\u8005\u53e5\u5b50\u7684\u622a\u65ad\u957f\u5ea6\uff0c\u53ea\u4fdd\u75591000\u4e2a\u5355\u8bcd\n    EMBEDDING_DIM = 60 # \u8bcd\u5411\u91cf\u7ef4\u5ea6\n\n    bert = Bert()", "src/py3.x/tensorflow2.x/quickstart.py": "#%%\n# -*- coding: utf-8 -*-\nfrom __future__ import absolute_import, division, print_function, unicode_literals\nimport tensorflow as tf\n\n#%%\n# \u8f7d\u5165\u5e76\u51c6\u5907\u597d MNIST \u6570\u636e\u96c6\n# \u5b58\u653e\u5730\u5740: /home/xxx/.keras/datasets/mnist.npz\nmnist = tf.keras.datasets.mnist\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\nx_train, x_test = x_train / 255.0, x_test / 255.0\n\n#%%\n# \u5c06\u6a21\u578b\u7684\u5404\u5c42\u5806\u53e0\u8d77\u6765\uff0c\u4ee5\u642d\u5efa tf.keras.Sequential \u6a21\u578b\u3002\u4e3a\u8bad\u7ec3\u9009\u62e9\u4f18\u5316\u5668\u548c\u635f\u5931\u51fd\u6570: \nmodel = tf.keras.models.Sequential([\n  tf.keras.layers.Flatten(input_shape=(28, 28)),\n  tf.keras.layers.Dense(128, activation='relu'),\n  tf.keras.layers.Dropout(0.2),\n  tf.keras.layers.Dense(10, activation='softmax')\n])\n\n#%%\nmodel.compile(optimizer='adam',\n            loss='sparse_categorical_crossentropy',\n            metrics=['accuracy'])\n\n#%%\n# \u8bad\u7ec3\u6a21\u578b\nmodel.fit(x_train, y_train, epochs=5)\n# \u9a8c\u8bc1\u6a21\u578b\nmodel.evaluate(x_test,  y_test, verbose=2)\n", "src/py3.x/tensorflow2.x/text_bert.py": "# *-* coding:utf-8 *-*\n# \u9884\u8bad\u7ec3\u6a21\u578b bert: \n#   https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip\n# \u53c2\u8003\u4ee3\u7801:\n#   https://blog.csdn.net/qq_32796253/article/details/98844242\nimport json\nimport numpy as np\nimport pandas as pd\nfrom random import choice\nfrom keras_bert import load_trained_model_from_checkpoint, Tokenizer\nimport re, os\nimport codecs\nfrom keras.layers import *\nfrom keras.models import Model\nimport keras.backend as K\nfrom keras.optimizers import Adam\nfrom config import Config\n\n\nclass OurTokenizer(Tokenizer):\n    def _tokenize(self, text):\n        R = []\n        for c in text:\n            if c in self._token_dict:\n                R.append(c)\n            elif self._is_space(c):\n                R.append('[unused1]') # space\u7c7b\u7528\u672a\u7ecf\u8bad\u7ec3\u7684[unused1]\u8868\u793a\n            else:\n                R.append('[UNK]') # \u5269\u4f59\u7684\u5b57\u7b26\u662f[UNK]\n        return R\n\n\nclass data_generator:\n    def __init__(self, data, tokenizer, batch_size=16):\n        self.data = data\n        self.batch_size = batch_size\n        self.tokenizer = tokenizer\n        self.steps = len(self.data) // self.batch_size\n        if len(self.data) % self.batch_size != 0:\n            self.steps += 1\n\n    def __len__(self):\n        return self.steps\n\n    def __iter__(self):\n        while True:\n            idxs = list(range(len(self.data)))\n            np.random.shuffle(idxs)\n            X1, X2, Y = [], [], []\n            for i in idxs:\n                d = self.data[i]\n                text = d[0][:Config.bert.maxlen]\n                x1, x2 = self.tokenizer.encode(first=text)\n                y = d[1]\n                X1.append(x1)\n                X2.append(x2)\n                Y.append([y])\n                if len(X1) == self.batch_size or i == idxs[-1]:\n                    X1 = seq_padding(X1)\n                    X2 = seq_padding(X2)\n                    Y = seq_padding(Y)\n                    yield [X1, X2], Y\n                    [X1, X2, Y] = [], [], []\n\n\n\ndef seq_padding(X, padding=0):\n    L = [len(x) for x in X]\n    ML = max(L)\n    return np.array([\n        np.concatenate([x, [padding] * (ML - len(x))]) if len(x) < ML else x for x in X\n    ])\n\n\nif __name__ == \"__main__\":\n    tb = TextBert()\n    model = tb.build_model()\n    tokenizer = OurTokenizer(tb.token_dict)\n\n    train_data, valid_data = tb.prepare_data()\n    train_D = data_generator(train_data, tokenizer)\n    valid_D = data_generator(valid_data, tokenizer)\n    model.fit_generator(\n        train_D.__iter__(),\n        steps_per_epoch=len(train_D),\n        epochs=5,\n        validation_data=valid_D.__iter__(),\n        validation_steps=len(valid_D)\n    )\n\n\n## \u6587\u672c\u6570\u636e\n## bert / Embedding/  + lstm + crt\n\n\n#%%\n# \u52a0\u8f7d\u6570\u636e\nclass TextBert():\n    def __init__(self):\n        self.path_config = Config.bert.path_config\n        self.path_checkpoint = Config.bert.path_checkpoint\n\n        self.token_dict = {}\n        with codecs.open(Config.bert.dict_path, 'r', 'utf8') as reader:\n            for line in reader:\n                token = line.strip()\n                self.token_dict[token] = len(self.token_dict)\n\n\n    def prepare_data(self):\n        neg = pd.read_excel(Config.bert.path_neg, header=None)\n        pos = pd.read_excel(Config.bert.path_pos, header=None)\n        data = []\n        for d in neg[0]:\n            data.append((d, 0))\n        for d in pos[0]:\n            data.append((d, 1))\n        # \u6309\u71679:1\u7684\u6bd4\u4f8b\u5212\u5206\u8bad\u7ec3\u96c6\u548c\u9a8c\u8bc1\u96c6\n        random_order = list(range(len(data)))\n        np.random.shuffle(random_order)\n        train_data = [data[j] for i, j in enumerate(random_order) if i % 10 != 0]\n        valid_data = [data[j] for i, j in enumerate(random_order) if i % 10 == 0]\n        return train_data, valid_data\n\n    def build_model(self, m_type=\"bert\"):\n        if m_type == \"bert\":\n            bert_model = load_trained_model_from_checkpoint(self.path_config, self.path_checkpoint, seq_len=None)\n            for l in bert_model.layers:\n                l.trainable = True\n            x1_in = Input(shape=(None,))\n            x2_in = Input(shape=(None,))\n            x = bert_model([x1_in, x2_in])\n            x = Lambda(lambda x: x[:, 0])(x)\n            p = Dense(1, activation='sigmoid')(x)#\u6839\u636e\u5206\u7c7b\u79cd\u7c7b\u81ea\u884c\u8c03\u8282\uff0c\u4e5f\u53ef\u4ee5\u591a\u52a0\u4e00\u4e9b\u5c42\u6570\n            model = Model([x1_in, x2_in], p)\n            model.compile(\n                loss='binary_crossentropy',\n                optimizer=Adam(1e-5), # \u7528\u8db3\u591f\u5c0f\u7684\u5b66\u4e60\u7387\n                metrics=['accuracy']\n            )\n        else:\n            # \u5426\u5219\u7528 Embedding\n            model = Sequential()\n            model.add(Embedding(len(vocab), EMBED_DIM, mask_zero=True))  # Random embedding\n            model.add(Bidirectional(LSTM(BiRNN_UNITS // 2, return_sequences=True)))\n            crf = CRF(len(chunk_tags), sparse_target=True)\n            model.add(crf)\n            model.compile('adam', loss=crf.loss_function, metrics=[crf.accuracy])\n        \n        model.summary()\n        return model\n\n\n#%%\n# \u52a0\u8f7d\u6570\u636e\nfrom keras_bert import Tokenizer\n#\u5b57\u5178\ntoken_dict = {\n    '[CLS]': 0,\n    '[SEP]': 1,\n    'un': 2,\n    '##aff': 3,\n    '##able': 4,\n    '[UNK]': 5,\n}\n\ntokenizer = Tokenizer(token_dict)\n\n# \u62c6\u5206\u5355\u8bcd\u5b9e\u4f8b\nprint(tokenizer.tokenize('unaffable')) \n# ['[CLS]', 'un', '##aff', '##able', '[SEP]']\n\n# indices\u662f\u5b57\u5bf9\u5e94\u7d22\u5f15\n# segments\u8868\u793a\u7d22\u5f15\u5bf9\u5e94\u4f4d\u7f6e\u4e0a\u7684\u5b57\u5c5e\u4e8e\u7b2c\u4e00\u53e5\u8bdd\u8fd8\u662f\u7b2c\u4e8c\u53e5\u8bdd\n# \u8fd9\u91cc\u53ea\u6709\u4e00\u53e5\u8bdd unaffable\uff0c\u6240\u4ee5segments\u90fd\u662f0\nindices, segments = tokenizer.encode('unaffable')\nprint(indices)  \n# [0, 2, 3, 4, 1]\nprint(segments)  \n# [0, 0, 0, 0, 0]\n\n\n\n# %%\nprint(tokenizer.tokenize('unknown')) \n# ['[CLS]', 'un', '##k', '##n', '##o', '##w', '##n', '[SEP]']\n\nindices, segments = tokenizer.encode('unknown')\n# [0, 2, 5, 5, 5, 5, 5, 1]\n# [0, 0, 0, 0, 0, 0, 0, 0]\n\n# %%\nprint(tokenizer.tokenize(first='unaffable', second='\u94a2'))\n# ['[CLS]', 'un', '##aff', '##able', '[SEP]', '\u94a2', '[SEP]']\nindices, segments = tokenizer.encode(first='unaffable', second='\u94a2', max_len=10)\nprint(indices)  \n# [0, 2, 3, 4, 1, 5, 1, 0, 0, 0]\nprint(segments)  \n# [0, 0, 0, 0, 0, 1, 1, 0, 0, 0]\n\n# %%\nimport keras\nfrom keras_bert import get_base_dict, get_model, compile_model, gen_batch_inputs\n\n\n# \u8f93\u5165\u793a\u4f8b\nsentence_pairs = [\n    [['all', 'work', 'and', 'no', 'play'], ['makes', 'jack', 'a', 'dull', 'boy']],\n    [['from', 'the', 'day', 'forth'], ['my', 'arm', 'changed']],\n    [['and', 'a', 'voice', 'echoed'], ['power', 'give', 'me', 'more', 'power']],\n]\n\n# \u6784\u5efa token \u5b57\u5178\n# \u8fd9\u4e2a\u5b57\u5178\u5b58\u653e\u7684\u662f\u3010\u8bcd\u3011\ntoken_dict = get_base_dict()  \n# get_base_dict()\u8fd4\u56de\u4e00\u4e2a\u5b57\u5178\n# \u5b57\u5178\u9884\u7f6e\u4e86\u4e00\u4e9b\u7279\u6b8atoken\uff0c\u5177\u4f53\u5185\u5bb9\u5982\u4e0b\n# {'': 0, '[UNK]': 1, '[CLS]': 2, '[SEP]': 3, '[MASK]': 4}\nfor pairs in sentence_pairs:\n    for token in pairs[0] + pairs[1]:\n        if token not in token_dict:\n            token_dict[token] = len(token_dict)\n# token_dict \u662f\u7531\u8bcd\u7ec4\u6210\u7684\u5b57\u5178\uff0c\u5927\u81f4\u5982\u4e0b\n# {'': 0, '[UNK]': 1, '[CLS]': 2, '[SEP]': 3, '[MASK]': 4, 'all': 5, 'work': 6,..., 'me': 26, 'more': 27}\n\ntoken_list = list(token_dict.keys())\n\n\n# \u6784\u5efa\u548c\u8bad\u7ec3\u6a21\u578b\nmodel = get_model(\n    token_num=len(token_dict),\n    head_num=5,\n    transformer_num=12,\n    embed_dim=25,\n    feed_forward_dim=100,\n    seq_len=20,\n    pos_num=20,\n    dropout_rate=0.05,\n)\ncompile_model(model)\nmodel.summary()\n\ndef _generator():\n    while True:\n        yield gen_batch_inputs(\n            sentence_pairs,\n            token_dict,\n            token_list,\n            seq_len=20,\n            mask_rate=0.3,\n            swap_sentence_rate=1.0,\n        )\n\nmodel.fit_generator(\n# \u8fd9\u91cc\u6d4b\u8bd5\u96c6\u548c\u9a8c\u8bc1\u96c6\u4f7f\u7528\u4e86\u540c\u6837\u7684\u6570\u636e\n# \u5b9e\u9645\u4e2d\u4f7f\u7528\u65f6\u4e0d\u80fd\u8fd9\u6837\n    generator=_generator(),\n    steps_per_epoch=1000,\n    epochs=100,\n    validation_data=_generator(),\n    validation_steps=100,\n    callbacks=[\n        keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n    ],\n)\n\n\n# \u4f7f\u7528\u8bad\u7ec3\u597d\u7684\u6a21\u578b\n# \u53d6\u51fa \u8f93\u5165\u5c42 \u548c \u6700\u540e\u4e00\u4e2a\u7279\u5f81\u63d0\u53d6\u5c42\ninputs, output_layer = get_model(\n    token_num=len(token_dict),\n    head_num=5,\n    transformer_num=12,\n    embed_dim=25,\n    feed_forward_dim=100,\n    seq_len=20,\n    pos_num=20,\n    dropout_rate=0.05,\n    training=False,\n    trainable=False,\n    output_layer_num=4,\n)\n\n# %%\nimport os\nfrom config import Config\n\n# \u8bbe\u7f6e\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u8def\u5f84\nconfig_path = Config.bert.path_config\ncheckpoint_path = Config.bert.path_checkpoint\nvocab_path = Config.bert.dict_path\n\n# \u6784\u5efa\u5b57\u5178\n# \u4e5f\u53ef\u4ee5\u7528 keras_bert \u4e2d\u7684 load_vocabulary() \u51fd\u6570\n# \u4f20\u5165 vocab_path \u5373\u53ef\n# from keras_bert import load_vocabulary\n# token_dict = load_vocabulary(vocab_path)\nimport codecs\ntoken_dict = {}\nwith codecs.open(vocab_path, 'r', 'utf8') as reader:\n    for line in reader:\n        token = line.strip()\n        token_dict[token] = len(token_dict)\n\n# \u52a0\u8f7d\u9884\u8bad\u7ec3\u6a21\u578b\nfrom keras_bert import load_trained_model_from_checkpoint\nmodel = load_trained_model_from_checkpoint(config_path, checkpoint_path)\n\n# Tokenization\nfrom keras_bert import Tokenizer\n\ntokenizer = Tokenizer(token_dict)\ntext = '\u8bed\u8a00\u6a21\u578b'\ntokens = tokenizer.tokenize(text)\n# ['[CLS]', '\u8bed', '\u8a00', '\u6a21', '\u578b', '[SEP]']\nindices, segments = tokenizer.encode(first=text, max_len=512)\nprint(indices[:10])\n# [101, 6427, 6241, 3563, 1798, 102, 0, 0, 0, 0]\nprint(segments[:10])\n# [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n\n# \u63d0\u53d6\u7279\u5f81\nimport numpy as np\n\npredicts = model.predict([np.array([indices]), np.array([segments])])[0]\nfor i, token in enumerate(tokens):\n    print(token, predicts[i].tolist()[:5])\n\n# %%\ntoken_dict = {}\nwith codecs.open(vocab_path, 'r', 'utf8') as reader:\n    for line in reader:\n        token = line.strip()\n        token_dict[token] = len(token_dict)\n\ntoken_dict_rev = {v: k for k, v in token_dict.items()}\n\nmodel = load_trained_model_from_checkpoint(config_path, checkpoint_path, training=True)\n\ntext = '\u6570\u5b66\u662f\u5229\u7528\u7b26\u53f7\u8bed\u8a00\u7814\u7a76\u6570\u91cf\u3001\u7ed3\u6784\u3001\u53d8\u5316\u4ee5\u53ca\u7a7a\u95f4\u7b49\u6982\u5ff5\u7684\u4e00\u95e8\u5b66\u79d1'\ntokens = tokenizer.tokenize(text)\ntokens[1] = tokens[2] = '[MASK]'# ['[CLS]', '[MASK]', '[MASK]', '\u662f', '\u5229',..., '\u5b66', '\u79d1', '[SEP]']\n\nindices = np.array([[token_dict[token] for token in tokens] + [0] * (512 - len(tokens))])\nsegments = np.array([[0] * len(tokens) + [0] * (512 - len(tokens))])\nmasks = np.array([[0, 1, 1] + [0] * (512 - 3)])\npredicts = model.predict([indices, segments, masks])[0].argmax(axis=-1).tolist()\nprint('Fill with: ', list(map(lambda x: token_dict_rev[x], predicts[0][1:3])))\n# Fill with:  ['\u6570', '\u5b66']\n\n# %%", "src/py3.x/tensorflow2.x/text_regression.py": "# -*- coding: utf-8 -*-\n\"\"\"text_regression.ipynb\n\nNote: \u6211\u4eec\u7684 TensorFlow \u793e\u533a\u7ffb\u8bd1\u4e86\u8fd9\u4e9b\u6587\u6863\u3002\u56e0\u4e3a\u793e\u533a\u7ffb\u8bd1\u662f\u5c3d\u529b\u800c\u4e3a\uff0c \u6240\u4ee5\u65e0\u6cd5\u4fdd\u8bc1\u5b83\u4eec\u662f\u6700\u51c6\u786e\u7684\uff0c\u5e76\u4e14\u53cd\u6620\u4e86\u6700\u65b0\u7684\n[\u5b98\u65b9\u82f1\u6587\u6587\u6863](https://www.tensorflow.org/?hl=en)\u3002\u5982\u679c\u60a8\u6709\u6539\u8fdb\u6b64\u7ffb\u8bd1\u7684\u5efa\u8bae\uff0c \u8bf7\u63d0\u4ea4 pull request \u5230\n[tensorflow/docs](https://github.com/tensorflow/docs) GitHub \u4ed3\u5e93\u3002\u8981\u5fd7\u613f\u5730\u64b0\u5199\u6216\u8005\u5ba1\u6838\u8bd1\u6587\uff0c\u8bf7\u52a0\u5165\n[docs-zh-cn@tensorflow.org Google Group](https://groups.google.com/a/tensorflow.org/forum/#!forum/docs-zh-cn)\u3002\n\n\u5728 *\u56de\u5f52 (regression)* \u95ee\u9898\u4e2d\uff0c\u6211\u4eec\u7684\u76ee\u7684\u662f\u9884\u6d4b\u51fa\u5982\u4ef7\u683c\u6216\u6982\u7387\u8fd9\u6837\u8fde\u7eed\u503c\u7684\u8f93\u51fa\u3002\u76f8\u5bf9\u4e8e*\u5206\u7c7b(classification)* \u95ee\u9898\uff0c*\u5206\u7c7b(classification)* \u7684\u76ee\u7684\u662f\u4ece\u4e00\u7cfb\u5217\u7684\u5206\u7c7b\u51fa\u9009\u62e9\u51fa\u4e00\u4e2a\u5206\u7c7b \uff08\u5982\uff0c\u7ed9\u51fa\u4e00\u5f20\u5305\u542b\u82f9\u679c\u6216\u6a58\u5b50\u7684\u56fe\u7247\uff0c\u8bc6\u522b\u51fa\u56fe\u7247\u4e2d\u662f\u54ea\u79cd\u6c34\u679c\uff09\u3002\n\n\u672c notebook \u4f7f\u7528\u7ecf\u5178\u7684 [Auto MPG](https://archive.ics.uci.edu/ml/datasets/auto+mpg) \u6570\u636e\u96c6\uff0c\u6784\u5efa\u4e86\u4e00\u4e2a\u7528\u6765\u9884\u6d4b70\u5e74\u4ee3\u672b\u523080\u5e74\u4ee3\u521d\u6c7d\u8f66\u71c3\u6cb9\u6548\u7387\u7684\u6a21\u578b\u3002\u4e3a\u4e86\u505a\u5230\u8fd9\u4e00\u70b9\uff0c\u6211\u4eec\u5c06\u4e3a\u8be5\u6a21\u578b\u63d0\u4f9b\u8bb8\u591a\u90a3\u4e2a\u65f6\u671f\u7684\u6c7d\u8f66\u63cf\u8ff0\u3002\u8fd9\u4e2a\u63cf\u8ff0\u5305\u542b: \u6c14\u7f38\u6570\uff0c\u6392\u91cf\uff0c\u9a6c\u529b\u4ee5\u53ca\u91cd\u91cf\u3002\n\n\u672c\u793a\u4f8b\u4f7f\u7528 `tf.keras` API\uff0c\u76f8\u5173\u7ec6\u8282\u8bf7\u53c2\u9605 [\u672c\u6307\u5357](https://tensorflow.google.cn/guide/keras)\u3002\n\"\"\"\n\n# \u4f7f\u7528 seaborn \u7ed8\u5236\u77e9\u9635\u56fe (pairplot)\n!pip install seaborn\n\n# Commented out IPython magic to ensure Python compatibility.\nfrom __future__ import absolute_import, division, print_function, unicode_literals\n\nimport pathlib\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\n\ntry:\n  # %tensorflow_version only exists in Colab.\n#   %tensorflow_version 2.x\nexcept Exception:\n  pass\nimport tensorflow as tf\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\nprint(tf.__version__)\n\n\"\"\"## Auto MPG \u6570\u636e\u96c6\n\n\u8be5\u6570\u636e\u96c6\u53ef\u4ee5\u4ece [UCI\u673a\u5668\u5b66\u4e60\u5e93](https://archive.ics.uci.edu/ml/) \u4e2d\u83b7\u53d6.\n\n### \u83b7\u53d6\u6570\u636e\n\u9996\u5148\u4e0b\u8f7d\u6570\u636e\u96c6\u3002\n\"\"\"\n\ndataset_path = keras.utils.get_file(\"auto-mpg.data\", \"http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data\")\ndataset_path\n\n\"\"\"\u4f7f\u7528 pandas \u5bfc\u5165\u6570\u636e\u96c6\u3002\"\"\"\n\ncolumn_names = ['MPG','Cylinders','Displacement','Horsepower','Weight',\n                'Acceleration', 'Model Year', 'Origin']\nraw_dataset = pd.read_csv(dataset_path, names=column_names,\n                      na_values = \"?\", comment='\\t',\n                      sep=\" \", skipinitialspace=True)\n\ndataset = raw_dataset.copy()\ndataset.tail()\n\n\"\"\"### \u6570\u636e\u6e05\u6d17\n\n\u6570\u636e\u96c6\u4e2d\u5305\u62ec\u4e00\u4e9b\u672a\u77e5\u503c\u3002\n\"\"\"\n\ndataset.isna().sum()\n\n\"\"\"\u4e3a\u4e86\u4fdd\u8bc1\u8fd9\u4e2a\u521d\u59cb\u793a\u4f8b\u7684\u7b80\u5355\u6027\uff0c\u5220\u9664\u8fd9\u4e9b\u884c\u3002\"\"\"\n\ndataset = dataset.dropna()\n\n\"\"\"`\"Origin\"` \u5217\u5b9e\u9645\u4e0a\u4ee3\u8868\u5206\u7c7b\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u4e00\u4e2a\u6570\u5b57\u3002\u6240\u4ee5\u628a\u5b83\u8f6c\u6362\u4e3a\u72ec\u70ed\u7801 \uff08one-hot\uff09:\"\"\"\n\norigin = dataset.pop('Origin')\n\ndataset['USA'] = (origin == 1)*1.0\ndataset['Europe'] = (origin == 2)*1.0\ndataset['Japan'] = (origin == 3)*1.0\ndataset.tail()\n\n\"\"\"### \u62c6\u5206\u8bad\u7ec3\u6570\u636e\u96c6\u548c\u6d4b\u8bd5\u6570\u636e\u96c6\n\n\u73b0\u5728\u9700\u8981\u5c06\u6570\u636e\u96c6\u62c6\u5206\u4e3a\u4e00\u4e2a\u8bad\u7ec3\u6570\u636e\u96c6\u548c\u4e00\u4e2a\u6d4b\u8bd5\u6570\u636e\u96c6\u3002\n\n\u6211\u4eec\u6700\u540e\u5c06\u4f7f\u7528\u6d4b\u8bd5\u6570\u636e\u96c6\u5bf9\u6a21\u578b\u8fdb\u884c\u8bc4\u4f30\u3002\n\"\"\"\n\ntrain_dataset = dataset.sample(frac=0.8,random_state=0)\ntest_dataset = dataset.drop(train_dataset.index)\n\n\"\"\"### \u6570\u636e\u68c0\u67e5\n\n\u5feb\u901f\u67e5\u770b\u8bad\u7ec3\u96c6\u4e2d\u51e0\u5bf9\u5217\u7684\u8054\u5408\u5206\u5e03\u3002\n\"\"\"\n\nsns.pairplot(train_dataset[[\"MPG\", \"Cylinders\", \"Displacement\", \"Weight\"]], diag_kind=\"kde\")\n\n\"\"\"\u4e5f\u53ef\u4ee5\u67e5\u770b\u603b\u4f53\u7684\u6570\u636e\u7edf\u8ba1:\"\"\"\n\ntrain_stats = train_dataset.describe()\ntrain_stats.pop(\"MPG\")\ntrain_stats = train_stats.transpose()\ntrain_stats\n\n\"\"\"### \u4ece\u6807\u7b7e\u4e2d\u5206\u79bb\u7279\u5f81\n\n\u5c06\u7279\u5f81\u503c\u4ece\u76ee\u6807\u503c\u6216\u8005\"\u6807\u7b7e\"\u4e2d\u5206\u79bb\u3002 \u8fd9\u4e2a\u6807\u7b7e\u662f\u4f60\u4f7f\u7528\u8bad\u7ec3\u6a21\u578b\u8fdb\u884c\u9884\u6d4b\u7684\u503c\u3002\n\"\"\"\n\ntrain_labels = train_dataset.pop('MPG')\ntest_labels = test_dataset.pop('MPG')\n\n\"\"\"### \u6570\u636e\u89c4\u8303\u5316\n\n\u518d\u6b21\u5ba1\u89c6\u4e0b\u4e0a\u9762\u7684 `train_stats` \u90e8\u5206\uff0c\u5e76\u6ce8\u610f\u6bcf\u4e2a\u7279\u5f81\u7684\u8303\u56f4\u6709\u4ec0\u4e48\u4e0d\u540c\u3002\n\n\u4f7f\u7528\u4e0d\u540c\u7684\u5c3a\u5ea6\u548c\u8303\u56f4\u5bf9\u7279\u5f81\u5f52\u4e00\u5316\u662f\u597d\u7684\u5b9e\u8df5\u3002\u5c3d\u7ba1\u6a21\u578b*\u53ef\u80fd* \u5728\u6ca1\u6709\u7279\u5f81\u5f52\u4e00\u5316\u7684\u60c5\u51b5\u4e0b\u6536\u655b\uff0c\u5b83\u4f1a\u4f7f\u5f97\u6a21\u578b\u8bad\u7ec3\u66f4\u52a0\u590d\u6742\uff0c\u5e76\u4f1a\u9020\u6210\u751f\u6210\u7684\u6a21\u578b\u4f9d\u8d56\u8f93\u5165\u6240\u4f7f\u7528\u7684\u5355\u4f4d\u9009\u62e9\u3002\n\n\u6ce8\u610f: \u5c3d\u7ba1\u6211\u4eec\u4ec5\u4ec5\u4ece\u8bad\u7ec3\u96c6\u4e2d\u6709\u610f\u751f\u6210\u8fd9\u4e9b\u7edf\u8ba1\u6570\u636e\uff0c\u4f46\u662f\u8fd9\u4e9b\u7edf\u8ba1\u4fe1\u606f\u4e5f\u4f1a\u7528\u4e8e\u5f52\u4e00\u5316\u7684\u6d4b\u8bd5\u6570\u636e\u96c6\u3002\u6211\u4eec\u9700\u8981\u8fd9\u6837\u505a\uff0c\u5c06\u6d4b\u8bd5\u6570\u636e\u96c6\u653e\u5165\u5230\u4e0e\u5df2\u7ecf\u8bad\u7ec3\u8fc7\u7684\u6a21\u578b\u76f8\u540c\u7684\u5206\u5e03\u4e2d\u3002\n\"\"\"\n\ndef norm(x):\n  return (x - train_stats['mean']) / train_stats['std']\nnormed_train_data = norm(train_dataset)\nnormed_test_data = norm(test_dataset)\n\n\"\"\"\u6211\u4eec\u5c06\u4f1a\u4f7f\u7528\u8fd9\u4e2a\u5df2\u7ecf\u5f52\u4e00\u5316\u7684\u6570\u636e\u6765\u8bad\u7ec3\u6a21\u578b\u3002\n\n\u8b66\u544a: \u7528\u4e8e\u5f52\u4e00\u5316\u8f93\u5165\u7684\u6570\u636e\u7edf\u8ba1\uff08\u5747\u503c\u548c\u6807\u51c6\u5dee\uff09\u9700\u8981\u53cd\u9988\u7ed9\u6a21\u578b\u4ece\u800c\u5e94\u7528\u4e8e\u4efb\u4f55\u5176\u4ed6\u6570\u636e\uff0c\u4ee5\u53ca\u6211\u4eec\u4e4b\u524d\u6240\u83b7\u5f97\u72ec\u70ed\u7801\u3002\u8fd9\u4e9b\u6570\u636e\u5305\u542b\u6d4b\u8bd5\u6570\u636e\u96c6\u4ee5\u53ca\u751f\u4ea7\u73af\u5883\u4e2d\u6240\u4f7f\u7528\u7684\u5b9e\u65f6\u6570\u636e\u3002\n\n## \u6a21\u578b\n\n### \u6784\u5efa\u6a21\u578b\n\n\u8ba9\u6211\u4eec\u6765\u6784\u5efa\u6211\u4eec\u81ea\u5df1\u7684\u6a21\u578b\u3002\u8fd9\u91cc\uff0c\u6211\u4eec\u5c06\u4f1a\u4f7f\u7528\u4e00\u4e2a\u201c\u987a\u5e8f\u201d\u6a21\u578b\uff0c\u5176\u4e2d\u5305\u542b\u4e24\u4e2a\u7d27\u5bc6\u76f8\u8fde\u7684\u9690\u85cf\u5c42\uff0c\u4ee5\u53ca\u8fd4\u56de\u5355\u4e2a\u3001\u8fde\u7eed\u503c\u5f97\u8f93\u51fa\u5c42\u3002\u6a21\u578b\u7684\u6784\u5efa\u6b65\u9aa4\u5305\u542b\u4e8e\u4e00\u4e2a\u540d\u53eb 'build_model' \u7684\u51fd\u6570\u4e2d\uff0c\u7a0d\u540e\u6211\u4eec\u5c06\u4f1a\u521b\u5efa\u7b2c\u4e8c\u4e2a\u6a21\u578b\u3002 \u4e24\u4e2a\u5bc6\u96c6\u8fde\u63a5\u7684\u9690\u85cf\u5c42\u3002\n\"\"\"\n\ndef build_model():\n  model = keras.Sequential([\n    layers.Dense(64, activation='relu', input_shape=[len(train_dataset.keys())]),\n    layers.Dense(64, activation='relu'),\n    layers.Dense(1)\n  ])\n\n  optimizer = tf.keras.optimizers.RMSprop(0.001)\n\n  model.compile(loss='mse',\n                optimizer=optimizer,\n                metrics=['mae', 'mse'])\n  return model\n\nmodel = build_model()\n\n\"\"\"### \u68c0\u67e5\u6a21\u578b\n\n\u4f7f\u7528 `.summary` \u65b9\u6cd5\u6765\u6253\u5370\u8be5\u6a21\u578b\u7684\u7b80\u5355\u63cf\u8ff0\u3002\n\"\"\"\n\nmodel.summary()\n\n\"\"\"\u73b0\u5728\u8bd5\u7528\u4e0b\u8fd9\u4e2a\u6a21\u578b\u3002\u4ece\u8bad\u7ec3\u6570\u636e\u4e2d\u6279\u91cf\u83b7\u53d6\u201810\u2019\u6761\u4f8b\u5b50\u5e76\u5bf9\u8fd9\u4e9b\u4f8b\u5b50\u8c03\u7528 `model.predict` \u3002\"\"\"\n\nexample_batch = normed_train_data[:10]\nexample_result = model.predict(example_batch)\nexample_result\n\n\"\"\"\u5b83\u4f3c\u4e4e\u5728\u5de5\u4f5c\uff0c\u5e76\u4ea7\u751f\u4e86\u9884\u671f\u7684\u5f62\u72b6\u548c\u7c7b\u578b\u7684\u7ed3\u679c\n\n### \u8bad\u7ec3\u6a21\u578b\n\n\u5bf9\u6a21\u578b\u8fdb\u884c1000\u4e2a\u5468\u671f\u7684\u8bad\u7ec3\uff0c\u5e76\u5728 `history` \u5bf9\u8c61\u4e2d\u8bb0\u5f55\u8bad\u7ec3\u548c\u9a8c\u8bc1\u7684\u51c6\u786e\u6027\u3002\n\"\"\"\n\n# \u901a\u8fc7\u4e3a\u6bcf\u4e2a\u5b8c\u6210\u7684\u65f6\u671f\u6253\u5370\u4e00\u4e2a\u70b9\u6765\u663e\u793a\u8bad\u7ec3\u8fdb\u5ea6\nclass PrintDot(keras.callbacks.Callback):\n  def on_epoch_end(self, epoch, logs):\n    if epoch % 100 == 0: print('')\n    print('.', end='')\n\nEPOCHS = 1000\n\nhistory = model.fit(\n  normed_train_data, train_labels,\n  epochs=EPOCHS, validation_split = 0.2, verbose=0,\n  callbacks=[PrintDot()])\n\n\"\"\"\u4f7f\u7528 `history` \u5bf9\u8c61\u4e2d\u5b58\u50a8\u7684\u7edf\u8ba1\u4fe1\u606f\u53ef\u89c6\u5316\u6a21\u578b\u7684\u8bad\u7ec3\u8fdb\u5ea6\u3002\"\"\"\n\nhist = pd.DataFrame(history.history)\nhist['epoch'] = history.epoch\nhist.tail()\n\ndef plot_history(history):\n  hist = pd.DataFrame(history.history)\n  hist['epoch'] = history.epoch\n\n  plt.figure()\n  plt.xlabel('Epoch')\n  plt.ylabel('Mean Abs Error [MPG]')\n  plt.plot(hist['epoch'], hist['mae'],\n           label='Train Error')\n  plt.plot(hist['epoch'], hist['val_mae'],\n           label = 'Val Error')\n  plt.ylim([0,5])\n  plt.legend()\n\n  plt.figure()\n  plt.xlabel('Epoch')\n  plt.ylabel('Mean Square Error [$MPG^2$]')\n  plt.plot(hist['epoch'], hist['mse'],\n           label='Train Error')\n  plt.plot(hist['epoch'], hist['val_mse'],\n           label = 'Val Error')\n  plt.ylim([0,20])\n  plt.legend()\n  plt.show()\n\n\nplot_history(history)\n\n\"\"\"\u8be5\u56fe\u8868\u663e\u793a\u5728\u7ea6100\u4e2a epochs \u4e4b\u540e\u8bef\u5dee\u975e\u4f46\u6ca1\u6709\u6539\u8fdb\uff0c\u53cd\u800c\u51fa\u73b0\u6076\u5316\u3002 \u8ba9\u6211\u4eec\u66f4\u65b0 `model.fit` \u8c03\u7528\uff0c\u5f53\u9a8c\u8bc1\u503c\u6ca1\u6709\u63d0\u9ad8\u4e0a\u662f\u81ea\u52a8\u505c\u6b62\u8bad\u7ec3\u3002\n\u6211\u4eec\u5c06\u4f7f\u7528\u4e00\u4e2a *EarlyStopping callback* \u6765\u6d4b\u8bd5\u6bcf\u4e2a epoch \u7684\u8bad\u7ec3\u6761\u4ef6\u3002\u5982\u679c\u7ecf\u8fc7\u4e00\u5b9a\u6570\u91cf\u7684 epochs \u540e\u6ca1\u6709\u6539\u8fdb\uff0c\u5219\u81ea\u52a8\u505c\u6b62\u8bad\u7ec3\u3002\n\n\u4f60\u53ef\u4ee5\u4ece[\u8fd9\u91cc](https://tensorflow.google.cn/versions/master/api_docs/python/tf/keras/callbacks/EarlyStopping)\u5b66\u4e60\u5230\u66f4\u591a\u7684\u56de\u8c03\u3002\n\"\"\"\n\nmodel = build_model()\n\n# patience \u503c\u7528\u6765\u68c0\u67e5\u6539\u8fdb epochs \u7684\u6570\u91cf\nearly_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n\nhistory = model.fit(normed_train_data, train_labels, epochs=EPOCHS,\n                    validation_split = 0.2, verbose=0, callbacks=[early_stop, PrintDot()])\n\nplot_history(history)\n\n\"\"\"\u5982\u56fe\u6240\u793a\uff0c\u9a8c\u8bc1\u96c6\u4e2d\u7684\u5e73\u5747\u7684\u8bef\u5dee\u901a\u5e38\u5728 +/- 2 MPG\u5de6\u53f3\u3002 \u8fd9\u4e2a\u7ed3\u679c\u597d\u4e48\uff1f \u6211\u4eec\u5c06\u51b3\u5b9a\u6743\u7559\u7ed9\u4f60\u3002\n\n\u8ba9\u6211\u4eec\u770b\u770b\u901a\u8fc7\u4f7f\u7528 **\u6d4b\u8bd5\u96c6** \u6765\u6cdb\u5316\u6a21\u578b\u7684\u6548\u679c\u5982\u4f55\uff0c\u6211\u4eec\u5728\u8bad\u7ec3\u6a21\u578b\u65f6\u6ca1\u6709\u4f7f\u7528\u6d4b\u8bd5\u96c6\u3002\u8fd9\u544a\u8bc9\u6211\u4eec\uff0c\u5f53\u6211\u4eec\u5728\u73b0\u5b9e\u4e16\u754c\u4e2d\u4f7f\u7528\u8fd9\u4e2a\u6a21\u578b\u65f6\uff0c\u6211\u4eec\u53ef\u4ee5\u671f\u671b\u5b83\u9884\u6d4b\u5f97\u6709\u591a\u597d\u3002\n\"\"\"\n\nloss, mae, mse = model.evaluate(normed_test_data, test_labels, verbose=2)\n\nprint(\"Testing set Mean Abs Error: {:5.2f} MPG\".format(mae))\n\n\"\"\"### \u505a\u9884\u6d4b\n \n\u6700\u540e\uff0c\u4f7f\u7528\u6d4b\u8bd5\u96c6\u4e2d\u7684\u6570\u636e\u9884\u6d4b MPG \u503c:\n\"\"\"\n\ntest_predictions = model.predict(normed_test_data).flatten()\n\nplt.scatter(test_labels, test_predictions)\nplt.xlabel('True Values [MPG]')\nplt.ylabel('Predictions [MPG]')\nplt.axis('equal')\nplt.axis('square')\nplt.xlim([0,plt.xlim()[1]])\nplt.ylim([0,plt.ylim()[1]])\n_ = plt.plot([-100, 100], [-100, 100])\n\n\"\"\"\u8fd9\u770b\u8d77\u6765\u6211\u4eec\u7684\u6a21\u578b\u9884\u6d4b\u5f97\u76f8\u5f53\u597d\u3002\u6211\u4eec\u6765\u770b\u4e0b\u8bef\u5dee\u5206\u5e03\u3002\"\"\"\n\nerror = test_predictions - test_labels\nplt.hist(error, bins = 25)\nplt.xlabel(\"Prediction Error [MPG]\")\n_ = plt.ylabel(\"Count\")\n\n\"\"\"\u5b83\u4e0d\u662f\u5b8c\u5168\u7684\u9ad8\u65af\u5206\u5e03\uff0c\u4f46\u6211\u4eec\u53ef\u4ee5\u63a8\u65ad\u51fa\uff0c\u8fd9\u662f\u56e0\u4e3a\u6837\u672c\u7684\u6570\u91cf\u5f88\u5c0f\u6240\u5bfc\u81f4\u7684\u3002\n\n## \u7ed3\u8bba\n\n\u672c\u7b14\u8bb0\u672c (notebook) \u4ecb\u7ecd\u4e86\u4e00\u4e9b\u5904\u7406\u56de\u5f52\u95ee\u9898\u7684\u6280\u672f\u3002\n\n* \u5747\u65b9\u8bef\u5dee\uff08MSE\uff09\u662f\u7528\u4e8e\u56de\u5f52\u95ee\u9898\u7684\u5e38\u89c1\u635f\u5931\u51fd\u6570\uff08\u5206\u7c7b\u95ee\u9898\u4e2d\u4f7f\u7528\u4e0d\u540c\u7684\u635f\u5931\u51fd\u6570\uff09\u3002\n* \u7c7b\u4f3c\u7684\uff0c\u7528\u4e8e\u56de\u5f52\u7684\u8bc4\u4f30\u6307\u6807\u4e0e\u5206\u7c7b\u4e0d\u540c\u3002 \u5e38\u89c1\u7684\u56de\u5f52\u6307\u6807\u662f\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\uff08MAE\uff09\u3002\n* \u5f53\u6570\u5b57\u8f93\u5165\u6570\u636e\u7279\u5f81\u7684\u503c\u5b58\u5728\u4e0d\u540c\u8303\u56f4\u65f6\uff0c\u6bcf\u4e2a\u7279\u5f81\u5e94\u72ec\u7acb\u7f29\u653e\u5230\u76f8\u540c\u8303\u56f4\u3002\n* \u5982\u679c\u8bad\u7ec3\u6570\u636e\u4e0d\u591a\uff0c\u4e00\u79cd\u65b9\u6cd5\u662f\u9009\u62e9\u9690\u85cf\u5c42\u8f83\u5c11\u7684\u5c0f\u7f51\u7edc\uff0c\u4ee5\u907f\u514d\u8fc7\u5ea6\u62df\u5408\u3002\n* \u65e9\u671f\u505c\u6b62\u662f\u4e00\u79cd\u9632\u6b62\u8fc7\u5ea6\u62df\u5408\u7684\u6709\u6548\u6280\u672f\u3002\n\"\"\"", "src/py3.x/tensorflow2.x/text_classification.py": "# -*- coding: utf-8 -*-\n\n\"\"\"# \u7535\u5f71\u8bc4\u8bba\u6587\u672c\u5206\u7c7b\n\nNote: \u6211\u4eec\u7684 TensorFlow \u793e\u533a\u7ffb\u8bd1\u4e86\u8fd9\u4e9b\u6587\u6863\u3002\u56e0\u4e3a\u793e\u533a\u7ffb\u8bd1\u662f\u5c3d\u529b\u800c\u4e3a\uff0c \u6240\u4ee5\u65e0\u6cd5\u4fdd\u8bc1\u5b83\u4eec\u662f\u6700\u51c6\u786e\u7684\uff0c\u5e76\u4e14\u53cd\u6620\u4e86\u6700\u65b0\u7684\n[\u5b98\u65b9\u82f1\u6587\u6587\u6863](https://www.tensorflow.org/?hl=en)\u3002\u5982\u679c\u60a8\u6709\u6539\u8fdb\u6b64\u7ffb\u8bd1\u7684\u5efa\u8bae\uff0c \u8bf7\u63d0\u4ea4 pull request \u5230\n[tensorflow/docs](https://github.com/tensorflow/docs) GitHub \u4ed3\u5e93\u3002\u8981\u5fd7\u613f\u5730\u64b0\u5199\u6216\u8005\u5ba1\u6838\u8bd1\u6587\uff0c\u8bf7\u52a0\u5165\n[docs-zh-cn@tensorflow.org Google Group](https://groups.google.com/a/tensorflow.org/forum/#!forum/docs-zh-cn)\u3002\n\n\u6b64\u7b14\u8bb0\u672c\uff08notebook\uff09\u4f7f\u7528\u8bc4\u8bba\u6587\u672c\u5c06\u5f71\u8bc4\u5206\u4e3a*\u79ef\u6781\uff08positive\uff09*\u6216*\u6d88\u6781\uff08nagetive\uff09*\u4e24\u7c7b\u3002\u8fd9\u662f\u4e00\u4e2a*\u4e8c\u5143\uff08binary\uff09*\u6216\u8005\u4e8c\u5206\u7c7b\u95ee\u9898\uff0c\u4e00\u79cd\u91cd\u8981\u4e14\u5e94\u7528\u5e7f\u6cdb\u7684\u673a\u5668\u5b66\u4e60\u95ee\u9898\u3002\n\n\u6211\u4eec\u5c06\u4f7f\u7528\u6765\u6e90\u4e8e[\u7f51\u7edc\u7535\u5f71\u6570\u636e\u5e93\uff08Internet Movie Database\uff09](https://www.imdb.com/)\u7684 [IMDB \u6570\u636e\u96c6\uff08IMDB dataset\uff09](https://tensorflow.google.cn/api_docs/python/tf/keras/datasets/imdb)\uff0c\u5176\u5305\u542b 50,000 \u6761\u5f71\u8bc4\u6587\u672c\u3002\u4ece\u8be5\u6570\u636e\u96c6\u5207\u5272\u51fa\u768425,000\u6761\u8bc4\u8bba\u7528\u4f5c\u8bad\u7ec3\uff0c\u53e6\u5916 25,000 \u6761\u7528\u4f5c\u6d4b\u8bd5\u3002\u8bad\u7ec3\u96c6\u4e0e\u6d4b\u8bd5\u96c6\u662f*\u5e73\u8861\u7684\uff08balanced\uff09*\uff0c\u610f\u5473\u7740\u5b83\u4eec\u5305\u542b\u76f8\u7b49\u6570\u91cf\u7684\u79ef\u6781\u548c\u6d88\u6781\u8bc4\u8bba\u3002\n\n\u6b64\u7b14\u8bb0\u672c\uff08notebook\uff09\u4f7f\u7528\u4e86 [tf.keras](https://tensorflow.google.cn/guide/keras)\uff0c\u5b83\u662f\u4e00\u4e2a Tensorflow \u4e2d\u7528\u4e8e\u6784\u5efa\u548c\u8bad\u7ec3\u6a21\u578b\u7684\u9ad8\u7ea7API\u3002\u6709\u5173\u4f7f\u7528 `tf.keras` \u8fdb\u884c\u6587\u672c\u5206\u7c7b\u7684\u66f4\u9ad8\u7ea7\u6559\u7a0b\uff0c\u8bf7\u53c2\u9605 [MLCC\u6587\u672c\u5206\u7c7b\u6307\u5357\uff08MLCC Text Classification Guide\uff09](https://developers.google.com/machine-learning/guides/text-classification/)\u3002\n\"\"\"\n\n# Commented out IPython magic to ensure Python compatibility.\nfrom __future__ import absolute_import, division, print_function, unicode_literals\n\ntry:\n  # Colab only\n#   %tensorflow_version 2.x\nexcept Exception:\n    pass\nimport tensorflow as tf\nfrom tensorflow import keras\n\nimport numpy as np\n\nprint(tf.__version__)\n\n\"\"\"## \u4e0b\u8f7d IMDB \u6570\u636e\u96c6\n\nIMDB \u6570\u636e\u96c6\u5df2\u7ecf\u6253\u5305\u5728 Tensorflow \u4e2d\u3002\u8be5\u6570\u636e\u96c6\u5df2\u7ecf\u7ecf\u8fc7\u9884\u5904\u7406\uff0c\u8bc4\u8bba\uff08\u5355\u8bcd\u5e8f\u5217\uff09\u5df2\u7ecf\u88ab\u8f6c\u6362\u4e3a\u6574\u6570\u5e8f\u5217\uff0c\u5176\u4e2d\u6bcf\u4e2a\u6574\u6570\u8868\u793a\u5b57\u5178\u4e2d\u7684\u7279\u5b9a\u5355\u8bcd\u3002\n\n\u4ee5\u4e0b\u4ee3\u7801\u5c06\u4e0b\u8f7d IMDB \u6570\u636e\u96c6\u5230\u60a8\u7684\u673a\u5668\u4e0a\uff08\u5982\u679c\u60a8\u5df2\u7ecf\u4e0b\u8f7d\u8fc7\u5c06\u4ece\u7f13\u5b58\u4e2d\u590d\u5236\uff09: \n\"\"\"\n\nimdb = keras.datasets.imdb\n\n(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)\n\n\"\"\"\u53c2\u6570 `num_words=10000` \u4fdd\u7559\u4e86\u8bad\u7ec3\u6570\u636e\u4e2d\u6700\u5e38\u51fa\u73b0\u7684 10,000 \u4e2a\u5355\u8bcd\u3002\u4e3a\u4e86\u4fdd\u6301\u6570\u636e\u89c4\u6a21\u7684\u53ef\u7ba1\u7406\u6027\uff0c\u4f4e\u9891\u8bcd\u5c06\u88ab\u4e22\u5f03\u3002\n\n## \u63a2\u7d22\u6570\u636e\n\n\u8ba9\u6211\u4eec\u82b1\u4e00\u70b9\u65f6\u95f4\u6765\u4e86\u89e3\u6570\u636e\u683c\u5f0f\u3002\u8be5\u6570\u636e\u96c6\u662f\u7ecf\u8fc7\u9884\u5904\u7406\u7684: \u6bcf\u4e2a\u6837\u672c\u90fd\u662f\u4e00\u4e2a\u8868\u793a\u5f71\u8bc4\u4e2d\u8bcd\u6c47\u7684\u6574\u6570\u6570\u7ec4\u3002\u6bcf\u4e2a\u6807\u7b7e\u90fd\u662f\u4e00\u4e2a\u503c\u4e3a 0 \u6216 1 \u7684\u6574\u6570\u503c\uff0c\u5176\u4e2d 0 \u4ee3\u8868\u6d88\u6781\u8bc4\u8bba\uff0c1 \u4ee3\u8868\u79ef\u6781\u8bc4\u8bba\u3002\n\"\"\"\n\nprint(\"Training entries: {}, labels: {}\".format(len(train_data), len(train_labels)))\n\n\"\"\"\u8bc4\u8bba\u6587\u672c\u88ab\u8f6c\u6362\u4e3a\u6574\u6570\u503c\uff0c\u5176\u4e2d\u6bcf\u4e2a\u6574\u6570\u4ee3\u8868\u8bcd\u5178\u4e2d\u7684\u4e00\u4e2a\u5355\u8bcd\u3002\u9996\u6761\u8bc4\u8bba\u662f\u8fd9\u6837\u7684: \"\"\"\n\nprint(train_data[0])\n\n\"\"\"\u7535\u5f71\u8bc4\u8bba\u53ef\u80fd\u5177\u6709\u4e0d\u540c\u7684\u957f\u5ea6\u3002\u4ee5\u4e0b\u4ee3\u7801\u663e\u793a\u4e86\u7b2c\u4e00\u6761\u548c\u7b2c\u4e8c\u6761\u8bc4\u8bba\u7684\u4e2d\u5355\u8bcd\u6570\u91cf\u3002\u7531\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684\u8f93\u5165\u5fc5\u987b\u662f\u7edf\u4e00\u7684\u957f\u5ea6\uff0c\u6211\u4eec\u7a0d\u540e\u9700\u8981\u89e3\u51b3\u8fd9\u4e2a\u95ee\u9898\u3002\"\"\"\n\nlen(train_data[0]), len(train_data[1])\n\n\"\"\"### \u5c06\u6574\u6570\u8f6c\u6362\u56de\u5355\u8bcd\n\n\u4e86\u89e3\u5982\u4f55\u5c06\u6574\u6570\u8f6c\u6362\u56de\u6587\u672c\u5bf9\u60a8\u53ef\u80fd\u662f\u6709\u5e2e\u52a9\u7684\u3002\u8fd9\u91cc\u6211\u4eec\u5c06\u521b\u5efa\u4e00\u4e2a\u8f85\u52a9\u51fd\u6570\u6765\u67e5\u8be2\u4e00\u4e2a\u5305\u542b\u4e86\u6574\u6570\u5230\u5b57\u7b26\u4e32\u6620\u5c04\u7684\u5b57\u5178\u5bf9\u8c61: \n\"\"\"\n\n# \u4e00\u4e2a\u6620\u5c04\u5355\u8bcd\u5230\u6574\u6570\u7d22\u5f15\u7684\u8bcd\u5178\nword_index = imdb.get_word_index()\n\n# \u4fdd\u7559\u7b2c\u4e00\u4e2a\u7d22\u5f15\nword_index = {k:(v+3) for k,v in word_index.items()}\nword_index[\"<PAD>\"] = 0\nword_index[\"<START>\"] = 1\nword_index[\"<UNK>\"] = 2  # unknown\nword_index[\"<UNUSED>\"] = 3\n\nreverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n\ndef decode_review(text):\n    return ' '.join([reverse_word_index.get(i, '?') for i in text])\n\n\"\"\"\u73b0\u5728\u6211\u4eec\u53ef\u4ee5\u4f7f\u7528 `decode_review` \u51fd\u6570\u6765\u663e\u793a\u9996\u6761\u8bc4\u8bba\u7684\u6587\u672c: \"\"\"\n\ndecode_review(train_data[0])\n\n\"\"\"## \u51c6\u5907\u6570\u636e\n\n\u5f71\u8bc4\u2014\u2014\u5373\u6574\u6570\u6570\u7ec4\u5fc5\u987b\u5728\u8f93\u5165\u795e\u7ecf\u7f51\u7edc\u4e4b\u524d\u8f6c\u6362\u4e3a\u5f20\u91cf\u3002\u8fd9\u79cd\u8f6c\u6362\u53ef\u4ee5\u901a\u8fc7\u4ee5\u4e0b\u4e24\u79cd\u65b9\u5f0f\u6765\u5b8c\u6210: \n\n* \u5c06\u6570\u7ec4\u8f6c\u6362\u4e3a\u8868\u793a\u5355\u8bcd\u51fa\u73b0\u4e0e\u5426\u7684\u7531 0 \u548c 1 \u7ec4\u6210\u7684\u5411\u91cf\uff0c\u7c7b\u4f3c\u4e8e one-hot \u7f16\u7801\u3002\u4f8b\u5982\uff0c\u5e8f\u5217[3, 5]\u5c06\u8f6c\u6362\u4e3a\u4e00\u4e2a 10,000 \u7ef4\u7684\u5411\u91cf\uff0c\u8be5\u5411\u91cf\u9664\u4e86\u7d22\u5f15\u4e3a 3 \u548c 5 \u7684\u4f4d\u7f6e\u662f 1 \u4ee5\u5916\uff0c\u5176\u4ed6\u90fd\u4e3a 0\u3002\u7136\u540e\uff0c\u5c06\u5176\u4f5c\u4e3a\u7f51\u7edc\u7684\u9996\u5c42\u2014\u2014\u4e00\u4e2a\u53ef\u4ee5\u5904\u7406\u6d6e\u70b9\u578b\u5411\u91cf\u6570\u636e\u7684\u7a20\u5bc6\u5c42\u3002\u4e0d\u8fc7\uff0c\u8fd9\u79cd\u65b9\u6cd5\u9700\u8981\u5927\u91cf\u7684\u5185\u5b58\uff0c\u9700\u8981\u4e00\u4e2a\u5927\u5c0f\u4e3a `num_words * num_reviews` \u7684\u77e9\u9635\u3002\n\n* \u6216\u8005\uff0c\u6211\u4eec\u53ef\u4ee5\u586b\u5145\u6570\u7ec4\u6765\u4fdd\u8bc1\u8f93\u5165\u6570\u636e\u5177\u6709\u76f8\u540c\u7684\u957f\u5ea6\uff0c\u7136\u540e\u521b\u5efa\u4e00\u4e2a\u5927\u5c0f\u4e3a `max_length * num_reviews` \u7684\u6574\u578b\u5f20\u91cf\u3002\u6211\u4eec\u53ef\u4ee5\u4f7f\u7528\u80fd\u591f\u5904\u7406\u6b64\u5f62\u72b6\u6570\u636e\u7684\u5d4c\u5165\u5c42\u4f5c\u4e3a\u7f51\u7edc\u4e2d\u7684\u7b2c\u4e00\u5c42\u3002\n\n\u5728\u672c\u6559\u7a0b\u4e2d\uff0c\u6211\u4eec\u5c06\u4f7f\u7528\u7b2c\u4e8c\u79cd\u65b9\u6cd5\u3002\n\n\u7531\u4e8e\u7535\u5f71\u8bc4\u8bba\u957f\u5ea6\u5fc5\u987b\u76f8\u540c\uff0c\u6211\u4eec\u5c06\u4f7f\u7528 [pad_sequences](https://tensorflow.google.cn/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences) \u51fd\u6570\u6765\u4f7f\u957f\u5ea6\u6807\u51c6\u5316: \n\"\"\"\n\ntrain_data = keras.preprocessing.sequence.pad_sequences(train_data,\n                                                        value=word_index[\"<PAD>\"],\n                                                        padding='post',\n                                                        maxlen=256)\n\ntest_data = keras.preprocessing.sequence.pad_sequences(test_data,\n                                                       value=word_index[\"<PAD>\"],\n                                                       padding='post',\n                                                       maxlen=256)\n\n\"\"\"\u73b0\u5728\u8ba9\u6211\u4eec\u770b\u4e0b\u6837\u672c\u7684\u957f\u5ea6: \"\"\"\n\nlen(train_data[0]), len(train_data[1])\n\n\"\"\"\u5e76\u68c0\u67e5\u4e00\u4e0b\u9996\u6761\u8bc4\u8bba\uff08\u5f53\u524d\u5df2\u7ecf\u586b\u5145\uff09: \"\"\"\n\nprint(train_data[0])\n\n\"\"\"## \u6784\u5efa\u6a21\u578b\n\n\u795e\u7ecf\u7f51\u7edc\u7531\u5806\u53e0\u7684\u5c42\u6765\u6784\u5efa\uff0c\u8fd9\u9700\u8981\u4ece\u4e24\u4e2a\u4e3b\u8981\u65b9\u9762\u6765\u8fdb\u884c\u4f53\u7cfb\u7ed3\u6784\u51b3\u7b56: \n\n* \u6a21\u578b\u91cc\u6709\u591a\u5c11\u5c42\uff1f\n* \u6bcf\u4e2a\u5c42\u91cc\u6709\u591a\u5c11*\u9690\u5c42\u5355\u5143\uff08hidden units\uff09*\uff1f\n\n\u5728\u6b64\u6837\u672c\u4e2d\uff0c\u8f93\u5165\u6570\u636e\u5305\u542b\u4e00\u4e2a\u5355\u8bcd\u7d22\u5f15\u7684\u6570\u7ec4\u3002\u8981\u9884\u6d4b\u7684\u6807\u7b7e\u4e3a 0 \u6216 1\u3002\u8ba9\u6211\u4eec\u6765\u4e3a\u8be5\u95ee\u9898\u6784\u5efa\u4e00\u4e2a\u6a21\u578b: \n\"\"\"\n\n# \u8f93\u5165\u5f62\u72b6\u662f\u7528\u4e8e\u7535\u5f71\u8bc4\u8bba\u7684\u8bcd\u6c47\u6570\u76ee\uff0810,000 \u8bcd\uff09\nvocab_size = 10000\n\nmodel = keras.Sequential()\nmodel.add(keras.layers.Embedding(vocab_size, 16))\nmodel.add(keras.layers.GlobalAveragePooling1D())\nmodel.add(keras.layers.Dense(16, activation='relu'))\nmodel.add(keras.layers.Dense(1, activation='sigmoid'))\n\nmodel.summary()\n\n\"\"\"\u5c42\u6309\u987a\u5e8f\u5806\u53e0\u4ee5\u6784\u5efa\u5206\u7c7b\u5668: \n\n1. \u7b2c\u4e00\u5c42\u662f`\u5d4c\u5165\uff08Embedding\uff09`\u5c42\u3002\u8be5\u5c42\u91c7\u7528\u6574\u6570\u7f16\u7801\u7684\u8bcd\u6c47\u8868\uff0c\u5e76\u67e5\u627e\u6bcf\u4e2a\u8bcd\u7d22\u5f15\u7684\u5d4c\u5165\u5411\u91cf\uff08embedding vector\uff09\u3002\u8fd9\u4e9b\u5411\u91cf\u662f\u901a\u8fc7\u6a21\u578b\u8bad\u7ec3\u5b66\u4e60\u5230\u7684\u3002\u5411\u91cf\u5411\u8f93\u51fa\u6570\u7ec4\u589e\u52a0\u4e86\u4e00\u4e2a\u7ef4\u5ea6\u3002\u5f97\u5230\u7684\u7ef4\u5ea6\u4e3a: `(batch, sequence, embedding)`\u3002\n2. \u63a5\u4e0b\u6765\uff0c`GlobalAveragePooling1D` \u5c06\u901a\u8fc7\u5bf9\u5e8f\u5217\u7ef4\u5ea6\u6c42\u5e73\u5747\u503c\u6765\u4e3a\u6bcf\u4e2a\u6837\u672c\u8fd4\u56de\u4e00\u4e2a\u5b9a\u957f\u8f93\u51fa\u5411\u91cf\u3002\u8fd9\u5141\u8bb8\u6a21\u578b\u4ee5\u5c3d\u53ef\u80fd\u6700\u7b80\u5355\u7684\u65b9\u5f0f\u5904\u7406\u53d8\u957f\u8f93\u5165\u3002\n3. \u8be5\u5b9a\u957f\u8f93\u51fa\u5411\u91cf\u901a\u8fc7\u4e00\u4e2a\u6709 16 \u4e2a\u9690\u5c42\u5355\u5143\u7684\u5168\u8fde\u63a5\uff08`Dense`\uff09\u5c42\u4f20\u8f93\u3002\n4. \u6700\u540e\u4e00\u5c42\u4e0e\u5355\u4e2a\u8f93\u51fa\u7ed3\u70b9\u5bc6\u96c6\u8fde\u63a5\u3002\u4f7f\u7528 `Sigmoid` \u6fc0\u6d3b\u51fd\u6570\uff0c\u5176\u51fd\u6570\u503c\u4e3a\u4ecb\u4e8e 0 \u4e0e 1 \u4e4b\u95f4\u7684\u6d6e\u70b9\u6570\uff0c\u8868\u793a\u6982\u7387\u6216\u7f6e\u4fe1\u5ea6\u3002\n\n### \u9690\u5c42\u5355\u5143\n\n\u4e0a\u8ff0\u6a21\u578b\u5728\u8f93\u5165\u8f93\u51fa\u4e4b\u95f4\u6709\u4e24\u4e2a\u4e2d\u95f4\u5c42\u6216\u201c\u9690\u85cf\u5c42\u201d\u3002\u8f93\u51fa\uff08\u5355\u5143\uff0c\u7ed3\u70b9\u6216\u795e\u7ecf\u5143\uff09\u7684\u6570\u91cf\u5373\u4e3a\u5c42\u8868\u793a\u7a7a\u95f4\u7684\u7ef4\u5ea6\u3002\u6362\u53e5\u8bdd\u8bf4\uff0c\u662f\u5b66\u4e60\u5185\u90e8\u8868\u793a\u65f6\u7f51\u7edc\u6240\u5141\u8bb8\u7684\u81ea\u7531\u5ea6\u3002\n\n\u5982\u679c\u6a21\u578b\u5177\u6709\u66f4\u591a\u7684\u9690\u5c42\u5355\u5143\uff08\u66f4\u9ad8\u7ef4\u5ea6\u7684\u8868\u793a\u7a7a\u95f4\uff09\u548c/\u6216\u66f4\u591a\u5c42\uff0c\u5219\u53ef\u4ee5\u5b66\u4e60\u5230\u66f4\u590d\u6742\u7684\u8868\u793a\u3002\u4f46\u662f\uff0c\u8fd9\u4f1a\u4f7f\u7f51\u7edc\u7684\u8ba1\u7b97\u6210\u672c\u66f4\u9ad8\uff0c\u5e76\u4e14\u53ef\u80fd\u5bfc\u81f4\u5b66\u4e60\u5230\u4e0d\u9700\u8981\u7684\u6a21\u5f0f\u2014\u2014\u4e00\u4e9b\u80fd\u591f\u5728\u8bad\u7ec3\u6570\u636e\u4e0a\u800c\u4e0d\u662f\u6d4b\u8bd5\u6570\u636e\u4e0a\u6539\u5584\u6027\u80fd\u7684\u6a21\u5f0f\u3002\u8fd9\u88ab\u79f0\u4e3a*\u8fc7\u62df\u5408\uff08overfitting\uff09*\uff0c\u6211\u4eec\u7a0d\u540e\u4f1a\u5bf9\u6b64\u8fdb\u884c\u63a2\u7a76\u3002\n\n### \u635f\u5931\u51fd\u6570\u4e0e\u4f18\u5316\u5668\n\n\u4e00\u4e2a\u6a21\u578b\u9700\u8981\u635f\u5931\u51fd\u6570\u548c\u4f18\u5316\u5668\u6765\u8fdb\u884c\u8bad\u7ec3\u3002\u7531\u4e8e\u8fd9\u662f\u4e00\u4e2a\u4e8c\u5206\u7c7b\u95ee\u9898\u4e14\u6a21\u578b\u8f93\u51fa\u6982\u7387\u503c\uff08\u4e00\u4e2a\u4f7f\u7528 sigmoid \u6fc0\u6d3b\u51fd\u6570\u7684\u5355\u4e00\u5355\u5143\u5c42\uff09\uff0c\u6211\u4eec\u5c06\u4f7f\u7528 `binary_crossentropy` \u635f\u5931\u51fd\u6570\u3002\n\n\u8fd9\u4e0d\u662f\u635f\u5931\u51fd\u6570\u7684\u552f\u4e00\u9009\u62e9\uff0c\u4f8b\u5982\uff0c\u60a8\u53ef\u4ee5\u9009\u62e9 `mean_squared_error` \u3002\u4f46\u662f\uff0c\u4e00\u822c\u6765\u8bf4 `binary_crossentropy` \u66f4\u9002\u5408\u5904\u7406\u6982\u7387\u2014\u2014\u5b83\u80fd\u591f\u5ea6\u91cf\u6982\u7387\u5206\u5e03\u4e4b\u95f4\u7684\u201c\u8ddd\u79bb\u201d\uff0c\u6216\u8005\u5728\u6211\u4eec\u7684\u793a\u4f8b\u4e2d\uff0c\u6307\u7684\u662f\u5ea6\u91cf ground-truth \u5206\u5e03\u4e0e\u9884\u6d4b\u503c\u4e4b\u95f4\u7684\u201c\u8ddd\u79bb\u201d\u3002\n\n\u7a0d\u540e\uff0c\u5f53\u6211\u4eec\u7814\u7a76\u56de\u5f52\u95ee\u9898\uff08\u4f8b\u5982\uff0c\u9884\u6d4b\u623f\u4ef7\uff09\u65f6\uff0c\u6211\u4eec\u5c06\u4ecb\u7ecd\u5982\u4f55\u4f7f\u7528\u53e6\u4e00\u79cd\u53eb\u505a\u5747\u65b9\u8bef\u5dee\u7684\u635f\u5931\u51fd\u6570\u3002\n\n\u73b0\u5728\uff0c\u914d\u7f6e\u6a21\u578b\u6765\u4f7f\u7528\u4f18\u5316\u5668\u548c\u635f\u5931\u51fd\u6570: \n\"\"\"\n\nmodel.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\n\n\"\"\"## \u521b\u5efa\u4e00\u4e2a\u9a8c\u8bc1\u96c6\n\n\u5728\u8bad\u7ec3\u65f6\uff0c\u6211\u4eec\u60f3\u8981\u68c0\u67e5\u6a21\u578b\u5728\u672a\u89c1\u8fc7\u7684\u6570\u636e\u4e0a\u7684\u51c6\u786e\u7387\uff08accuracy\uff09\u3002\u901a\u8fc7\u4ece\u539f\u59cb\u8bad\u7ec3\u6570\u636e\u4e2d\u5206\u79bb 10,000 \u4e2a\u6837\u672c\u6765\u521b\u5efa\u4e00\u4e2a*\u9a8c\u8bc1\u96c6*\u3002\uff08\u4e3a\u4ec0\u4e48\u73b0\u5728\u4e0d\u4f7f\u7528\u6d4b\u8bd5\u96c6\uff1f\u6211\u4eec\u7684\u76ee\u6807\u662f\u53ea\u4f7f\u7528\u8bad\u7ec3\u6570\u636e\u6765\u5f00\u53d1\u548c\u8c03\u6574\u6a21\u578b\uff0c\u7136\u540e\u53ea\u4f7f\u7528\u4e00\u6b21\u6d4b\u8bd5\u6570\u636e\u6765\u8bc4\u4f30\u51c6\u786e\u7387\uff08accuracy\uff09\uff09\u3002\n\"\"\"\n\nx_val = train_data[:10000]\npartial_x_train = train_data[10000:]\n\ny_val = train_labels[:10000]\npartial_y_train = train_labels[10000:]\n\n\"\"\"## \u8bad\u7ec3\u6a21\u578b\n\n\u4ee5 512 \u4e2a\u6837\u672c\u7684 mini-batch \u5927\u5c0f\u8fed\u4ee3 40 \u4e2a epoch \u6765\u8bad\u7ec3\u6a21\u578b\u3002\u8fd9\u662f\u6307\u5bf9 `x_train` \u548c `y_train` \u5f20\u91cf\u4e2d\u6240\u6709\u6837\u672c\u7684\u7684 40 \u6b21\u8fed\u4ee3\u3002\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\uff0c\u76d1\u6d4b\u6765\u81ea\u9a8c\u8bc1\u96c6\u7684 10,000 \u4e2a\u6837\u672c\u4e0a\u7684\u635f\u5931\u503c\uff08loss\uff09\u548c\u51c6\u786e\u7387\uff08accuracy\uff09: \n\"\"\"\n\nhistory = model.fit(partial_x_train,\n                    partial_y_train,\n                    epochs=40,\n                    batch_size=512,\n                    validation_data=(x_val, y_val),\n                    verbose=1)\n\n\"\"\"## \u8bc4\u4f30\u6a21\u578b\n\n\u6211\u4eec\u6765\u770b\u4e00\u4e0b\u6a21\u578b\u7684\u6027\u80fd\u5982\u4f55\u3002\u5c06\u8fd4\u56de\u4e24\u4e2a\u503c\u3002\u635f\u5931\u503c\uff08loss\uff09\uff08\u4e00\u4e2a\u8868\u793a\u8bef\u5dee\u7684\u6570\u5b57\uff0c\u503c\u8d8a\u4f4e\u8d8a\u597d\uff09\u4e0e\u51c6\u786e\u7387\uff08accuracy\uff09\u3002\n\"\"\"\n\nresults = model.evaluate(test_data,  test_labels, verbose=2)\n\nprint(results)\n\n\"\"\"\u8fd9\u79cd\u5341\u5206\u6734\u7d20\u7684\u65b9\u6cd5\u5f97\u5230\u4e86\u7ea6 87% \u7684\u51c6\u786e\u7387\uff08accuracy\uff09\u3002\u82e5\u91c7\u7528\u66f4\u597d\u7684\u65b9\u6cd5\uff0c\u6a21\u578b\u7684\u51c6\u786e\u7387\u5e94\u5f53\u63a5\u8fd1 95%\u3002\n\n## \u521b\u5efa\u4e00\u4e2a\u51c6\u786e\u7387\uff08accuracy\uff09\u548c\u635f\u5931\u503c\uff08loss\uff09\u968f\u65f6\u95f4\u53d8\u5316\u7684\u56fe\u8868\n\n`model.fit()` \u8fd4\u56de\u4e00\u4e2a `History` \u5bf9\u8c61\uff0c\u8be5\u5bf9\u8c61\u5305\u542b\u4e00\u4e2a\u5b57\u5178\uff0c\u5176\u4e2d\u5305\u542b\u8bad\u7ec3\u9636\u6bb5\u6240\u53d1\u751f\u7684\u4e00\u5207\u4e8b\u4ef6: \n\"\"\"\n\nhistory_dict = history.history\nhistory_dict.keys()\n\n\"\"\"\u6709\u56db\u4e2a\u6761\u76ee: \u5728\u8bad\u7ec3\u548c\u9a8c\u8bc1\u671f\u95f4\uff0c\u6bcf\u4e2a\u6761\u76ee\u5bf9\u5e94\u4e00\u4e2a\u76d1\u63a7\u6307\u6807\u3002\u6211\u4eec\u53ef\u4ee5\u4f7f\u7528\u8fd9\u4e9b\u6761\u76ee\u6765\u7ed8\u5236\u8bad\u7ec3\u4e0e\u9a8c\u8bc1\u8fc7\u7a0b\u7684\u635f\u5931\u503c\uff08loss\uff09\u548c\u51c6\u786e\u7387\uff08accuracy\uff09\uff0c\u4ee5\u4fbf\u8fdb\u884c\u6bd4\u8f83\u3002\"\"\"\n\nimport matplotlib.pyplot as plt\n\nacc = history_dict['accuracy']\nval_acc = history_dict['val_accuracy']\nloss = history_dict['loss']\nval_loss = history_dict['val_loss']\n\nepochs = range(1, len(acc) + 1)\n\n# \u201cbo\u201d\u4ee3\u8868 \"\u84dd\u70b9\"\nplt.plot(epochs, loss, 'bo', label='Training loss')\n# b\u4ee3\u8868\u201c\u84dd\u8272\u5b9e\u7ebf\u201d\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.show()\n\nplt.clf()   # \u6e05\u9664\u6570\u5b57\n\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\n\nplt.show()\n\n\"\"\"\u5728\u8be5\u56fe\u4e2d\uff0c\u70b9\u4ee3\u8868\u8bad\u7ec3\u635f\u5931\u503c\uff08loss\uff09\u4e0e\u51c6\u786e\u7387\uff08accuracy\uff09\uff0c\u5b9e\u7ebf\u4ee3\u8868\u9a8c\u8bc1\u635f\u5931\u503c\uff08loss\uff09\u4e0e\u51c6\u786e\u7387\uff08accuracy\uff09\u3002\n\n\u6ce8\u610f\u8bad\u7ec3\u635f\u5931\u503c\u968f\u6bcf\u4e00\u4e2a epoch *\u4e0b\u964d*\u800c\u8bad\u7ec3\u51c6\u786e\u7387\uff08accuracy\uff09\u968f\u6bcf\u4e00\u4e2a epoch *\u4e0a\u5347*\u3002\u8fd9\u5728\u4f7f\u7528\u68af\u5ea6\u4e0b\u964d\u4f18\u5316\u65f6\u662f\u53ef\u9884\u671f\u7684\u2014\u2014\u7406\u5e94\u5728\u6bcf\u6b21\u8fed\u4ee3\u4e2d\u6700\u5c0f\u5316\u671f\u671b\u503c\u3002\n\n\u9a8c\u8bc1\u8fc7\u7a0b\u7684\u635f\u5931\u503c\uff08loss\uff09\u4e0e\u51c6\u786e\u7387\uff08accuracy\uff09\u7684\u60c5\u51b5\u5374\u5e76\u975e\u5982\u6b64\u2014\u2014\u5b83\u4eec\u4f3c\u4e4e\u5728 20 \u4e2a epoch \u540e\u8fbe\u5230\u5cf0\u503c\u3002\u8fd9\u662f\u8fc7\u62df\u5408\u7684\u4e00\u4e2a\u5b9e\u4f8b: \u6a21\u578b\u5728\u8bad\u7ec3\u6570\u636e\u4e0a\u7684\u8868\u73b0\u6bd4\u5728\u4ee5\u524d\u4ece\u672a\u89c1\u8fc7\u7684\u6570\u636e\u4e0a\u7684\u8868\u73b0\u8981\u66f4\u597d\u3002\u5728\u6b64\u4e4b\u540e\uff0c\u6a21\u578b\u8fc7\u5ea6\u4f18\u5316\u5e76\u5b66\u4e60*\u7279\u5b9a*\u4e8e\u8bad\u7ec3\u6570\u636e\u7684\u8868\u793a\uff0c\u800c\u4e0d\u80fd\u591f*\u6cdb\u5316*\u5230\u6d4b\u8bd5\u6570\u636e\u3002\n\n\u5bf9\u4e8e\u8fd9\u79cd\u7279\u6b8a\u60c5\u51b5\uff0c\u6211\u4eec\u53ef\u4ee5\u901a\u8fc7\u5728 20 \u4e2a\u5de6\u53f3\u7684 epoch \u540e\u505c\u6b62\u8bad\u7ec3\u6765\u907f\u514d\u8fc7\u62df\u5408\u3002\u7a0d\u540e\uff0c\u60a8\u5c06\u770b\u5230\u5982\u4f55\u901a\u8fc7\u56de\u8c03\u81ea\u52a8\u6267\u884c\u6b64\u64cd\u4f5c\u3002\n\"\"\"\n", "src/py3.x/tensorflow2.x/text_PoetryModel.py": "# *-* coding:utf-8 *-*\n'''\n\u4ee3\u7801\u53c2\u8003: https://github.com/ioiogoo/poetry_generator_Keras\n\u505a\u4e86\u4e00\u5b9a\u7684\u7b80\u5316\uff0c\u4f5c\u8005 @ioiogoo \u534f\u8bae\u662f MIT\n\u76ee\u6807: \u81ea\u52a8\u751f\u6210\u6b4c\u8bcd\u7684\n'''\nimport re\nimport os\nimport keras\nimport random\nimport numpy as np\nfrom keras.callbacks import LambdaCallback\nfrom keras.models import load_model\nfrom keras.layers import Dropout, Dense, Flatten, Bidirectional, Embedding, GRU\nfrom keras.optimizers import Adam\n# \u8be5\u76ee\u5f55\u4e0b\u7684 config.py\u6587\u4ef6\uff0c \u6570\u636e\u6587\u4ef6\u662f: poetry.txt\nfrom config import Config\n\n\ndef preprocess_file(Config):\n    # \u8bfb\u53d6\u6587\u672c\u5185\u5bb9\uff0c\u5408\u5e76\u5230\u4e00\u4e2a\u5927\u5b57\u7b26\u4e2d\uff0c\u7528 ] \u9694\u5f00\n    files_content = ''\n    with open(Config.poetry_file, 'r', encoding='utf-8') as f:\n        for line in f:\n            # \u6bcf\u884c\u7684\u672b\u5c3e\u52a0\u4e0a\"]\"\u7b26\u53f7\u4ee3\u8868\u4e00\u9996\u8bd7\u7ed3\u675f\n            line = re.sub(r\"[\\]\\[\uff08\uff09(){}\u300a\u300b: ]+\", \"\", line.strip())\n            files_content += line + \"]\"\n    \n    # \u6309\u7167\u5b57\u5b58\u5230\u5b57\u5178\u4e2d\uff0c\u5b57+\u9891\u7387\n    words = [i for i in sorted(list(files_content)) if i != \"]\"]\n    counted_words = {}\n    for word in words:\n        if word in counted_words:\n            counted_words[word] += 1\n        else:\n            counted_words[word] = 1\n\n    # \u53bb\u6389\u4f4e\u9891\u7684\u5b57\n    # [('\u3002', 567), ('\uff0c', 565), ('\u98ce', 47), ('\u82b1', 42), ('\u4e91', 40)]\n    wordPairs = sorted([(k,v) for k,v in counted_words.items() if v>=2], key=lambda  x: x[1], reverse=True)\n    # print(wordPairs)\n\n    words, _ = zip(*wordPairs)\n    # word\u5230id\u7684\u6620\u5c04\n    word2num = dict((c, i) for i, c in enumerate(words))\n    num2word = dict((i, c) for i, c in enumerate(words))\n    word2numF = lambda x: word2num.get(x, 0)\n    return word2numF, num2word, words, files_content\n\n\nclass PoetryModel(object):\n    def __init__(self, config):\n        self.model = None\n        self.do_train = True\n        self.loaded_model = False\n        self.config = config\n\n        # \u6587\u4ef6\u9884\u5904\u7406\n        self.word2numF, self.num2word, self.words, self.files_content = preprocess_file(self.config)\n\n        # \u5982\u679c\u6a21\u578b\u6587\u4ef6\u5b58\u5728\u5219\u76f4\u63a5\u52a0\u8f7d\u6a21\u578b\uff0c\u5426\u5219\u5f00\u59cb\u8bad\u7ec3\n        if os.path.exists(self.config.weight_file):\n            self.model = load_model(self.config.weight_file)\n            self.model.summary()\n        else:\n            self.train()\n\n        self.do_train = False\n        self.loaded_model = True\n\n    def build_model(self):\n        '''\u6784\u5efa\u6a21\u578b'''\n        model = keras.Sequential()\n        model.add(Embedding(len(self.num2word) + 2, 300, input_length=self.config.max_len))\n        model.add(Bidirectional(GRU(128, return_sequences=True)))\n        model.add(Dropout(0.6))\n        model.add(Flatten())\n        model.add(Dense(len(self.words), activation='softmax'))\n        # \u8bbe\u7f6e\u4f18\u5316\u5668\n        optimizer = Adam(lr=self.config.learning_rate)\n        model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n        self.model = model\n\n    def predict(self, text):\n        '''\u6839\u636e\u7ed9\u51fa\u7684\u6587\u5b57\uff0c\u751f\u6210\u8bd7\u53e5'''\n        if not self.loaded_model:\n            return\n        with open(self.config.poetry_file, 'r', encoding='utf-8') as f:\n            file_list = f.readlines()\n        random_line = random.choice(file_list)\n        # \u5982\u679c\u7ed9\u7684text\u4e0d\u5230\u56db\u4e2a\u5b57\uff0c\u5219\u968f\u673a\u8865\u5168\n        if not text or len(text) != 4:\n            for _ in range(4 - len(text)):\n                random_str_index = random.randrange(0, len(self.words))\n                text += self.num2word.get(random_str_index) \\\n                        if self.num2word.get(random_str_index) not in [',', '\u3002', '\uff0c'] \\\n                        else self.num2word.get(random_str_index + 1)\n\n        seed = random_line[-(self.config.max_len):-1]\n\n        res = ''\n        seed = 'c' + seed\n\n        for c in text:\n            seed = seed[1:] + c\n            for j in range(5):\n                x_pred = np.zeros((1, self.config.max_len))\n                for t, char in enumerate(seed):\n                    x_pred[0, t] = self.word2numF(char)\n\n                preds = self.model.predict(x_pred, verbose=0)[0]\n                next_index = self.sample(preds, 1.0)\n                next_char = self.num2word[next_index]\n                seed = seed[1:] + next_char\n            res += seed\n        return res\n\n    def data_generator(self):\n        '''\u751f\u6210\u5668\u751f\u6210\u6570\u636e'''\n        i = 0\n        while 1:\n            # \u5982\u679c\u8d8a\u754c\u4e86\uff0c\u5c31\u4ece0\u518d\u5f00\u59cb\n            if (i + self.config.max_len) > len(self.files_content) -1 :\n                i = 0\n            x = self.files_content[i: i + self.config.max_len]\n            y = self.files_content[i + self.config.max_len]\n\n            puncs = [']', '[', '\uff08', '\uff09', '{', '}', ': ', '\u300a', '\u300b', ':']\n            if len([i for i in puncs if i in x]) != 0:\n                i += 1\n                continue\n            if len([i for i in puncs if i in y]) != 0:\n                i += 1\n                continue\n\n            y_vec = np.zeros(\n                shape=(1, len(self.words)),\n                dtype=np.bool\n            )\n            y_vec[0, self.word2numF(y)] = 1.0\n\n            x_vec = np.zeros(\n                shape=(1, self.config.max_len),\n                dtype=np.int32\n            )\n\n            for t, char in enumerate(x):\n                x_vec[0, t] = self.word2numF(char)\n            yield x_vec, y_vec\n            i += 1\n\n    def train(self):\n        '''\u8bad\u7ec3\u6a21\u578b'''\n        number_of_epoch = len(self.files_content) // self.config.batch_size\n\n        if not self.model:\n            self.build_model()\n\n        self.model.summary()\n\n        self.model.fit_generator(\n            generator=self.data_generator(),\n            verbose=True,\n            steps_per_epoch=self.config.batch_size,\n            epochs=number_of_epoch,\n            callbacks=[\n                keras.callbacks.ModelCheckpoint(self.config.weight_file, save_weights_only=False),\n                LambdaCallback(on_epoch_end=self.generate_sample_result)\n            ]\n        )\n\n\nif __name__ == '__main__':\n    model = PoetryModel(Config)\n    while 1:\n        text = input(\"text:\")\n        sentence = model.predict(text)\n        print(sentence)\n", "src/py3.x/tensorflow2.x/zh-NER-keras-master/val.py": "import bilsm_crf_model\nimport process_data\nimport numpy as np\n\nmodel, (vocab, chunk_tags) = bilsm_crf_model.create_model(train=False)\npredict_text = '\u4e2d\u534e\u4eba\u6c11\u5171\u548c\u56fd\u56fd\u52a1\u9662\u603b\u7406\u5468\u6069\u6765\u5728\u5916\u4ea4\u90e8\u957f\u9648\u6bc5\u7684\u966a\u540c\u4e0b\uff0c\u8fde\u7eed\u8bbf\u95ee\u4e86\u57c3\u585e\u4fc4\u6bd4\u4e9a\u7b49\u975e\u6d3210\u56fd\u4ee5\u53ca\u963f\u5c14\u5df4\u5c3c\u4e9a'\nstr, length = process_data.process_data(predict_text, vocab)\nmodel.load_weights('model/crf.h5')\nraw = model.predict(str)[0][-length:]\nresult = [np.argmax(row) for row in raw]\nresult_tags = [chunk_tags[i] for i in result]\n\nper, loc, org = '', '', ''\n\nfor s, t in zip(predict_text, result_tags):\n    if t in ('B-PER', 'I-PER'):\n        per += ' ' + s if (t == 'B-PER') else s\n    if t in ('B-ORG', 'I-ORG'):\n        org += ' ' + s if (t == 'B-ORG') else s\n    if t in ('B-LOC', 'I-LOC'):\n        loc += ' ' + s if (t == 'B-LOC') else s\n\nprint(['person:' + per, 'location:' + loc, 'organzation:' + org])\n", "src/py3.x/tensorflow2.x/zh-NER-keras-master/train.py": "import bilsm_crf_model\n\nEPOCHS = 10\nmodel, (train_x, train_y), (test_x, test_y) = bilsm_crf_model.create_model()\n# train model\nmodel.fit(train_x, train_y,batch_size=16,epochs=EPOCHS, validation_data=[test_x, test_y])\nmodel.save('model/crf.h5')\n", "src/py3.x/tensorflow2.x/zh-NER-keras-master/bilsm_crf_model.py": "from keras.models import Sequential\nfrom keras.layers import Embedding, Bidirectional, LSTM\nfrom keras_contrib.layers import CRF\nimport process_data\nimport pickle\n\nEMBED_DIM = 200\nBiRNN_UNITS = 200\n\n\ndef create_model(train=True):\n    if train:\n        (train_x, train_y), (test_x, test_y), (vocab, chunk_tags) = process_data.load_data()\n    else:\n        with open('model/config.pkl', 'rb') as inp:\n            (vocab, chunk_tags) = pickle.load(inp)\n    model = Sequential()\n    model.add(Embedding(len(vocab), EMBED_DIM, mask_zero=True))  # Random embedding\n    model.add(Bidirectional(LSTM(BiRNN_UNITS // 2, return_sequences=True)))\n    crf = CRF(len(chunk_tags), sparse_target=True)\n    model.add(crf)\n    model.summary()\n    model.compile('adam', loss=crf.loss_function, metrics=[crf.accuracy])\n    if train:\n        return model, (train_x, train_y), (test_x, test_y)\n    else:\n        return model, (vocab, chunk_tags)\n", "src/config/setting.py": "# *-* coding:utf-8 *-*\n'''\n@author: \u7247\u523b\n@date: 20200901 22:02\n'''\n\nclass TextNER(object):\n    DEBUG = True\n    if DEBUG:\n        path_root = \"data/NER3\"\n        chunk_tags = ['O', 'B-ORG', 'I-ORG', \n            'B-Po_VIEW', 'I-Po_VIEW', \n            'B-Mi_VIEW', 'I-Mi_VIEW', \n            'B-Ne_VIEW', 'I-Ne_VIEW'\n        ]\n        # chunk_tags = ['O', 'B-TIME', 'I-TIME', 'B-LOCATION', 'I-LOCATION', \n        #     \"B-PERSON_NAME\", \"I-PERSON_NAME\", \"B-ORG_NAME\", \"I-ORG_NAME\",\n        #     \"B-COMPANY_NAME\", \"I-COMPANY_NAME\", \"B-PRODUCT_NAME\", \"I-PRODUCT_NAME\"]\n        # chunk_tags = ['O', 'B-PER', 'I-PER', 'B-LOC', 'I-LOC', \"B-ORG\", \"I-ORG\"]\n    else:\n        path_root = \"data/NER\"\n        chunk_tags = ['O', 'B-PER', 'I-PER', 'B-LOC', 'I-LOC', \"B-ORG\", \"I-ORG\"]\n\n    path_origin = '%s/origin.txt'  % path_root\n    path_train  = '%s/train.txt'   % path_root\n    path_test   = '%s/test.txt'    % path_root\n    path_config = '%s/config.pkl'  % path_root\n    path_model  = '%s/model.h5'    % path_root\n\n    # \u8fed\u4ee3\u6b21\u6570\n    EPOCHS = 10\n    # embedding\u7684\u7ef4\u5ea6\u6570\n    EMBED_DIM = 128\n    # LSTM\u7684\u7ef4\u5ea6\u6570\n    BiLSTM_UNITS = 128\n\n\nclass Config(object):\n    nlp_ner = TextNER()\n", "src/config/__init__.py": "", "src/tool/DecisionTree_getInfoGain.py": "#!/usr/bin/python\n# coding: utf8\n\nfrom math import log\n\n\ndef calcShannonEnt(dataSet):\n    \"\"\"calcShannonEnt(calculate Shannon entropy \u8ba1\u7b97label\u5206\u7c7b\u6807\u7b7e\u7684\u9999\u519c\u71b5)\n\n    Args:\n        dataSet \u6570\u636e\u96c6\n    Returns:\n        \u8fd4\u56de\u9999\u519c\u71b5\u7684\u8ba1\u7b97\u503c\n    Raises:\n\n    \"\"\"\n    # \u6c42list\u7684\u957f\u5ea6\uff0c\u8868\u793a\u8ba1\u7b97\u53c2\u4e0e\u8bad\u7ec3\u7684\u6570\u636e\u91cf\n    numEntries = len(dataSet)\n    # print(type(dataSet), 'numEntries: ', numEntries)\n\n    # \u8ba1\u7b97\u5206\u7c7b\u6807\u7b7elabel\u51fa\u73b0\u7684\u6b21\u6570\n    labelCounts = {}\n    # the the number of unique elements and their occurance\n    for featVec in dataSet:\n        currentLabel = featVec[-1]\n        if currentLabel not in labelCounts.keys():\n            labelCounts[currentLabel] = 0\n        labelCounts[currentLabel] += 1\n        # print('-----', featVec, labelCounts)\n\n    # \u5bf9\u4e8elabel\u6807\u7b7e\u7684\u5360\u6bd4\uff0c\u6c42\u51falabel\u6807\u7b7e\u7684\u9999\u519c\u71b5\n    shannonEnt = 0.0\n    for key in labelCounts:\n        prob = float(labelCounts[key])/numEntries\n        # log base 2\n        shannonEnt -= prob * log(prob, 2)\n        # print('---', prob, prob * log(prob, 2), shannonEnt)\n    return shannonEnt\n\n\ndef splitDataSet(dataSet, axis, value):\n    \"\"\"splitDataSet(\u901a\u8fc7\u904d\u5386dataSet\u6570\u636e\u96c6\uff0c\u6c42\u51faaxis\u5bf9\u5e94\u7684colnum\u5217\u7684\u503c\u4e3avalue\u7684\u884c)\n\n    Args:\n        dataSet \u6570\u636e\u96c6\n        axis \u8868\u793a\u6bcf\u4e00\u884c\u7684axis\u5217\n        value \u8868\u793aaxis\u5217\u5bf9\u5e94\u7684value\u503c\n    Returns:\n        axis\u5217\u4e3avalue\u7684\u6570\u636e\u96c6\u3010\u8be5\u6570\u636e\u96c6\u9700\u8981\u6392\u9664axis\u5217\u3011\n    Raises:\n\n    \"\"\"\n    retDataSet = []\n    for featVec in dataSet:\n        # axis\u5217\u4e3avalue\u7684\u6570\u636e\u96c6\u3010\u8be5\u6570\u636e\u96c6\u9700\u8981\u6392\u9664axis\u5217\u3011\n        if featVec[axis] == value:\n            # chop out axis used for splitting\n            reducedFeatVec = featVec[:axis]\n            '''\n            \u8bf7\u767e\u5ea6\u67e5\u8be2\u4e00\u4e0b:  extend\u548cappend\u7684\u533a\u522b\n            '''\n            reducedFeatVec.extend(featVec[axis+1:])\n            # \u6536\u96c6\u7ed3\u679c\u503c axis\u5217\u4e3avalue\u7684\u884c\u3010\u8be5\u884c\u9700\u8981\u6392\u9664axis\u5217\u3011\n            retDataSet.append(reducedFeatVec)\n    return retDataSet\n\n\ndef getFeatureShannonEnt(dataSet, labels):\n    \"\"\"chooseBestFeatureToSplit(\u9009\u62e9\u6700\u597d\u7684\u7279\u5f81)\n\n    Args:\n        dataSet \u6570\u636e\u96c6\n    Returns:\n        bestFeature \u6700\u4f18\u7684\u7279\u5f81\u5217\n    Raises:\n\n    \"\"\"\n    # \u6c42\u7b2c\u4e00\u884c\u6709\u591a\u5c11\u5217\u7684 Feature\n    numFeatures = len(dataSet[0]) - 1\n    # label\u7684\u4fe1\u606f\u71b5\n    baseEntropy = calcShannonEnt(dataSet)\n    # \u6700\u4f18\u7684\u4fe1\u606f\u589e\u76ca\u503c, \u548c\u6700\u4f18\u7684Featurn\u7f16\u53f7\n    bestInfoGain, bestFeature, endEntropy = 0.0, -1, 0.0\n    # iterate over all the features\n    for i in range(numFeatures):\n        # create a list of all the examples of this feature\n        # \u83b7\u53d6\u6bcf\u4e00\u4e2afeature\u7684list\u96c6\u5408\n        featList = [example[i] for example in dataSet]\n        # get a set of unique values\n        # \u83b7\u53d6\u5254\u91cd\u540e\u7684\u96c6\u5408\n        uniqueVals = set(featList)\n        # \u521b\u5efa\u4e00\u4e2a\u4e34\u65f6\u7684\u4fe1\u606f\u71b5\n        newEntropy = 0.0\n        # \u904d\u5386\u67d0\u4e00\u5217\u7684value\u96c6\u5408\uff0c\u8ba1\u7b97\u8be5\u5217\u7684\u4fe1\u606f\u71b5\n        for value in uniqueVals:\n            subDataSet = splitDataSet(dataSet, i, value)\n            prob = len(subDataSet)/float(len(dataSet))\n            newEntropy += prob * calcShannonEnt(subDataSet)\n        # gain[\u4fe1\u606f\u589e\u76ca] \u503c\u8d8a\u5927\uff0c\u610f\u5473\u7740\u8be5\u5206\u7c7b\u63d0\u4f9b\u7684\u4fe1\u606f\u91cf\u8d8a\u5927\uff0c\u8be5\u7279\u5f81\u5bf9\u5206\u7c7b\u7684\u4e0d\u786e\u5b9a\u7a0b\u5ea6\u8d8a\u5c0f\n        # gain[\u4fe1\u606f\u589e\u76ca]=0, \u8868\u793a\u4e0e\u7c7b\u522b\u76f8\u540c\uff0c\u65e0\u9700\u5176\u4ed6\u7684\u5206\u7c7b\n        # gain[\u4fe1\u606f\u589e\u76ca]=baseEntropy, \u8868\u793a\u5206\u7c7b\u548c\u6ca1\u5206\u7c7b\u6ca1\u6709\u533a\u522b\n        infoGain = baseEntropy - newEntropy\n        # print(infoGain)\n        if (infoGain > bestInfoGain):\n            endEntropy = newEntropy\n            bestInfoGain = infoGain\n            bestFeature = i\n    else:\n        if numFeatures < 0:\n            labels[bestFeature] = 'null'\n\n    return labels[bestFeature], baseEntropy, endEntropy, bestInfoGain\n\n\nif __name__ == '__main__':\n    labels = ['no surfacing', 'flippers']\n    dataSet1 = [['yes'], ['yes'], ['no'], ['no'], ['no']]\n    dataSet2 = [['a', 1, 'yes'], ['a', 2, 'yes'], ['b', 3, 'no'], ['c', 4, 'no'], ['c', 5, 'no']]\n    dataSet3 = [[1, 'yes'], [1, 'yes'], [1, 'no'], [3, 'no'], [3, 'no']]\n    infoGain1 = getFeatureShannonEnt(dataSet1, labels)\n    infoGain2 = getFeatureShannonEnt(dataSet2, labels)\n    infoGain3 = getFeatureShannonEnt(dataSet3, labels)\n    print('\u4fe1\u606f\u589e\u76ca: \\n\\t%s, \\n\\t%s, \\n\\t%s' % (infoGain1, infoGain2, infoGain3))\n\n", "src/tool/python2libsvm.py": "#!/usr/bin/python\n# coding:utf8\n\nfrom __future__ import print_function\nimport os\nimport sklearn.datasets as datasets\n\n\ndef get_data(file_input, separator='\\t'):\n    if 'libsvm' not in file_input:\n        file_input = other2libsvm(file_input, separator)\n    data = datasets.load_svmlight_file(file_input)\n    return data[0], data[1]\n\n\ndef other2libsvm(file_name, separator='\\t'):\n\n    libsvm_name = file_name.replace('.txt', '.libsvm_tmp')\n    libsvm_data = open(libsvm_name, 'w')\n\n    file_data = open(file_name, 'r')\n    for line in file_data.readlines():\n        features = line.strip().split(separator)\n        # print len(features)\n        class_data = features[-1]\n        svm_format = ''\n        for i in range(len(features)-1):\n            svm_format += \" %d:%s\" % (i+1, features[i])\n            # print svm_format\n        svm_format = \"%s%s\\n\" % (class_data, svm_format)\n        # print svm_format\n        libsvm_data.write(svm_format)\n    file_data.close()\n\n    libsvm_data.close()\n    return libsvm_name\n\n\ndef dump_data(x, y, file_output):\n    datasets.dump_svmlight_file(x, y, file_output)\n    os.remove(\"%s_tmp\" % file_output)\n\n\nif __name__ == \"__main__\":\n    file_input = \"data/7.AdaBoost/horseColicTest2.txt\"\n    file_output = \"data/7.AdaBoost/horseColicTest2.libsvm\"\n\n    # \u83b7\u53d6\u6570\u636e\u96c6\n    x, y = get_data(file_input, separator='\\t')\n    print(x[3, :])\n    print(y)\n    # \u5bfc\u51fa\u6570\u636e\u4e3a libsvm\n    dump_data(x, y, file_output)\n", "old/run_example.py": "import tutorials.keras.text_NER as ft\n\n\ndef main():\n    ft.main()\n\n\nif __name__ == \"__main__\":\n    main()\n", "tutorials/keras/text_NER.py": "import pickle\nimport numpy as np\nimport pandas as pd\nimport platform\nfrom collections import Counter\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Embedding, Bidirectional, LSTM, Dropout\nfrom keras_contrib.layers import CRF\nfrom keras_contrib.losses import crf_loss\nfrom keras_contrib.metrics import crf_viterbi_accuracy\n\"\"\"\n# padding: pre(\u9ed8\u8ba4) \u5411\u524d\u8865\u51450  post \u5411\u540e\u8865\u51450\n# truncating: \u6587\u672c\u8d85\u8fc7 pad_num,  pre(\u9ed8\u8ba4) \u5220\u9664\u524d\u9762  post \u5220\u9664\u540e\u9762\n# x_train = pad_sequences(x, maxlen=pad_num, value=0, padding='post', truncating=\"post\")\n# print(\"--- \", x_train[0][:20])\n\n\u4f7f\u7528keras_bert\u3001keras_contrib\u7684crf\u65f6bug\u8bb0\u5f55\nTypeError: Tensors in list passed to 'values' of 'ConcatV2' Op have types [bool, float32] that don't all match\n\u89e3\u51b3\u65b9\u6848, \u4fee\u6539crf.py 516\u884c\uff1a\nmask2 = K.cast(K.concatenate([mask, K.zeros_like(mask[:, :1])], axis=1),\n\u4e3a:\nmask2 = K.cast(K.concatenate([mask, K.cast(K.zeros_like(mask[:, :1]), mask.dtype)], axis=1),\n\"\"\"\nfrom keras.preprocessing.sequence import pad_sequences\nfrom config.setting import Config\n\n\ndef load_data():\n    train = _parse_data(Config.nlp_ner.path_train)\n    test  = _parse_data(Config.nlp_ner.path_test)\n    print(\"--- init \u6570\u636e\u52a0\u8f7d\u89e3\u6790\u5b8c\u6210 ---\")\n    \n    # Counter({'\u7684': 8, '\u4e2d': 7, '\u81f4': 7, '\u515a': 7})\n    word_counts = Counter(row[0].lower() for sample in train for row in sample)\n    vocab = [w for w, f in iter(word_counts.items()) if f >= 2]\n    chunk_tags = Config.nlp_ner.chunk_tags\n\n    # \u5b58\u50a8\u4fdd\u7559\u7684\u6709\u6548\u4e2a\u6570\u7684 vovab \u548c \u5bf9\u5e94 chunk_tags\n    with open(Config.nlp_ner.path_config, 'wb') as outp:\n        pickle.dump((vocab, chunk_tags), outp)\n    print(\"--- init \u914d\u7f6e\u6587\u4ef6\u4fdd\u5b58\u6210\u529f ---\")\n\n    train = _process_data(train, vocab, chunk_tags)\n    test  = _process_data(test , vocab, chunk_tags)\n    print(\"--- init \u5bf9\u6570\u636e\u8fdb\u884c\u7f16\u7801\uff0c\u751f\u6210\u8bad\u7ec3\u9700\u8981\u7684\u6570\u636e\u683c\u5f0f ---\")\n    return train, test, (vocab, chunk_tags)\n\n\ndef _parse_data(filename):\n    \"\"\"\n    \u4ee5\u5355\u4e0b\u5212\u7ebf\u5f00\u5934\uff08_foo\uff09\u7684\u4ee3\u8868\u4e0d\u80fd\u76f4\u63a5\u8bbf\u95ee\u7684\u7c7b\u5c5e\u6027\n    \u7528\u4e8e\u89e3\u6790\u6570\u636e\uff0c\u7528\u4e8e\u6a21\u578b\u8bad\u7ec3\n    :param filename: \u6587\u4ef6\u5730\u5740\n    :return: data: \u89e3\u6790\u6570\u636e\u540e\u7684\u7ed3\u679c\n    [[['\u4e2d', 'B-ORG'], ['\u5171', 'I-ORG']], [['\u4e2d', 'B-ORG'], ['\u56fd', 'I-ORG']]]\n    \"\"\"\n    with open(filename, 'rb') as fn:\n        split_text = '\\n'\n        # \u4e3b\u8981\u662f\u5206\u53e5: split_text \u9ed8\u8ba4\u6bcf\u4e2a\u53e5\u5b50\u90fd\u662f\u4e00\u884c\uff0c\u6240\u4ee5\u539f\u6765\u6362\u884c\u5c31\u9700\u8981 \u4e24\u4e2asplit_text\n        texts = fn.read().decode('utf-8').strip().split(split_text + split_text)\n        # \u5bf9\u4e8e\u6bcf\u4e2a\u5b57\u9700\u8981 split_text, \u800c\u5b57\u7684\u5185\u90e8\u9700\u8981\u7528\u7a7a\u683c\u5206\u9694\n        # len(row) > 0 \u907f\u514d\u8fde\u7eed2\u4e2a\u6362\u884c\uff0c\u5bfc\u81f4 row \u6570\u636e\u4e3a\u7a7a\n        # row.split() \u4f1a\u5220\u9664\u7a7a\u683c\u6216\u7279\u6b8a\u7b26\u53f7\uff0c\u5bfc\u81f4\u7a7a\u683c\u6570\u636e\u7f3a\u5931\uff01\n        data = [[[\" \", \"O\"] if len(row.split()) != 2 else row.split() for row in text.split(split_text) if len(row) > 0] for text in texts]\n        # data = [[row.split() for row in text.split(split_text) if len(row.split()) == 2] for text in texts]\n    return data\n\n\ndef _process_data(data, vocab, chunk_tags, maxlen=None, onehot=False):\n    if maxlen is None:\n        maxlen = max(len(s) for s in data)\n    \n    # \u5bf9\u6bcf\u4e2a\u5b57\u8fdb\u884c\u7f16\u7801\n    word2idx = dict((w, i) for i, w in enumerate(vocab))\n    # \u5982\u679c\u4e0d\u5728 vocab\u91cc\u9762\uff0c\u5c31\u7ed9 unk \u503c\u4e3a 1\n    x = [[word2idx.get(w[0].lower(), 1) for w in s] for s in data]\n    y_chunk = [[chunk_tags.index(w[1])  for w in s] for s in data]\n\n    x = pad_sequences(x, maxlen)  # left padding\n    y_chunk = pad_sequences(y_chunk, maxlen, value=-1)\n\n    if onehot:\n        # \u8fd4\u56de\u4e00\u4e2aonehot \u7f16\u7801\u7684\u591a\u7ef4\u6570\u7ec4\n        y_chunk = np.eye(len(chunk_tags), dtype='float32')[y_chunk]\n    else:\n        # np.expand_dims:\u7528\u4e8e\u6269\u5c55\u6570\u7ec4\u7684\u5f62\u72b6\n        # https://blog.csdn.net/hong615771420/article/details/83448878\n        y_chunk = np.expand_dims(y_chunk, 2)\n    return x, y_chunk\n\n\ndef process_data(data, vocab, maxlen=100):\n    word2idx = dict((w, i) for i, w in enumerate(vocab))\n    x = [word2idx.get(w[0].lower(), 1) for w in data]\n    length = len(x)\n    x = pad_sequences([x], maxlen)  # left padding\n    return x, length\n\n\ndef create_model(len_vocab, len_chunk_tags):\n    model = Sequential()\n    model.add(Embedding(len_vocab, Config.nlp_ner.EMBED_DIM, mask_zero=True))  # Random embedding\n    model.add(Bidirectional(LSTM(Config.nlp_ner.BiLSTM_UNITS // 2, return_sequences=True)))\n    model.add(Dropout(0.25))\n    crf = CRF(len_chunk_tags, sparse_target=True)\n    model.add(crf)\n    model.summary()\n    model.compile('adam', loss=crf_loss, metrics=[crf_viterbi_accuracy])\n    # model.compile('rmsprop', loss=crf_loss, metrics=[crf_viterbi_accuracy])\n\n    # from keras.optimizers import Adam\n    # adam_lr = 0.0001\n    # adam_beta_1 = 0.5\n    # model.compile(optimizer=Adam(lr=adam_lr, beta_1=adam_beta_1), loss=crf_loss, metrics=[crf_viterbi_accuracy])\n    return model\n\n\ndef train():\n    (train_x, train_y), (test_x, test_y), (vocab, chunk_tags) = load_data()\n    model = create_model(len(vocab), len(chunk_tags))\n    # train model\n    model.fit(train_x, train_y, batch_size=16, epochs=Config.nlp_ner.EPOCHS, validation_data=[test_x, test_y])\n    model.save(Config.nlp_ner.path_model)\n\n\ndef test():\n    with open(Config.nlp_ner.path_config, 'rb') as inp:\n        (vocab, chunk_tags) = pickle.load(inp)\n    model = create_model(len(vocab), len(chunk_tags))\n    # predict_text = '\u9020\u578b\u72ec\u7279\uff0c\u5c3a\u7801\u504f\u5927\uff0c\u4f30\u8ba1\u662f\u9489\u5b50\u5934\u5706\u7684\u534a\u5f84\u7684\u7f18\u6545'\n    with open(Config.nlp_ner.path_origin, \"r\", encoding=\"utf-8\") as f:\n        lines = f.readlines()\n        for predict_text in lines:\n            content = predict_text.strip()\n            text_EMBED, length = process_data(content, vocab)\n            model.load_weights(Config.nlp_ner.path_model)\n            raw = model.predict(text_EMBED)[0][-length:]\n            pre_result = [np.argmax(row) for row in raw]\n            result_tags = [chunk_tags[i] for i in pre_result]\n\n            # \u4fdd\u5b58\u6bcf\u53e5\u8bdd\u7684 \u5b9e\u4f53\u548c\u89c2\u70b9\n            result = {}\n            tag_list = [i for i in chunk_tags if i not in [\"O\"]]\n            for word, t in zip(content, result_tags):\n                # print(word, t)\n                if t not in tag_list:\n                    continue\n                for i in range(0, len(tag_list), 2):\n                    if t in tag_list[i:i+2]:\n                        # print(\"\\n>>> %s---%s==%s\" % (word, t, tag_list[i:i+2]))\n                        tag = tag_list[i].split(\"-\")[-1]\n                        if tag not in result:\n                            result[tag] = \"\"\n                        result[tag] += ' '+word if t==tag_list[i] else word\n            print(result)\n\n\ndef main():\n    # print(\"--\")\n    train()\n    test()\n\n# if __name__ == \"__main__\":\n#     train()\n", "tutorials/keras/brat_tag.py": "# -*- coding: utf-8 -*-\n\n\"\"\"\n\u6570\u636e\u683c\u5f0f\u8f6c\u5316\n\"\"\"\nimport os\nimport emoji\nfrom middleware.utils import get_catalog_files\nfrom config.setting import Config\n\ntag_dic = {\"\u5b9e\u4f53\u5bf9\u8c61\": \"ORG\",\n           \"\u6b63\u5411\u89c2\u70b9\": \"Po_VIEW\",\n           \"\u4e2d\u6027\u89c2\u70b9\": \"Mi_VIEW\",\n           \"\u8d1f\u5411\u89c2\u70b9\": \"Ne_VIEW\"}\n\n\n# \u8f6c\u6362\u6210\u53ef\u8bad\u7ec3\u7684\u683c\u5f0f\uff0c\u6700\u540e\u4ee5\"END O\"\u7ed3\u5c3e\ndef from_ann2dic(r_ann_path, r_txt_path, w_path):\n    q_dic = {}\n    print(\"\u5f00\u59cb\u8bfb\u53d6\u6587\u4ef6:%s\" % r_ann_path)\n    with open(r_ann_path, \"r\", encoding=\"utf-8\") as f:\n        lines = f.readlines()\n        for line in lines:\n            line_arr = line.split()\n            # print(\">>> \", line_arr)\n            cls = tag_dic[line_arr[1]]\n            start_index = int(line_arr[2])\n            end_index = int(line_arr[3])\n            length = end_index - start_index\n            for r in range(length):\n                q_dic[start_index+r] = (\"B-%s\" % cls) if r == 0 else (\"I-%s\" % cls)\n\n    # \u5b58\u50a8\u5750\u6807\u548c\u5bf9\u5e94\u7684\u5217\u540d:  {23: 'B-Ne_VIEW', 24: 'I-Ne_VIEW', 46: 'B-ORG', 47: 'I-ORG'}\n    print(\"q_dic: \", q_dic)\n\n    print(\"\u5f00\u59cb\u8bfb\u53d6\u6587\u4ef6\u5185\u5bb9: %s\" % r_txt_path)\n    with open(r_txt_path, \"r\", encoding=\"utf-8\") as f:\n        content_str = f.read()\n\n    print(\"\u5f00\u59cb\u5199\u5165\u6587\u672c%s\" % w_path)\n    with open(w_path, \"w\", encoding=\"utf-8\") as w:\n        for i, strA in enumerate(content_str):\n            # print(\">>> %s-%s\" % (i, strA))\n            if strA == \"\\n\":\n                w.write(\"\\n\")\n            else:\n                if i in q_dic:\n                    tag = q_dic[i]\n                else:\n                    tag = \"O\"  # \u5927\u5199\u5b57\u6bcdO\n                w.write('%s %s\\n' % (strA, tag))\n        w.write('%s\\n' % \"END O\")\n\n\n# \u751f\u6210train.txt\u3001dev.txt\u3001test.txt\n# \u96648\uff0c9-new.txt\u5206\u522b\u7528\u4e8edev\u548ctest\u5916,\u5269\u4e0b\u7684\u5408\u5e76\u6210train.txt\ndef create_train_data(data_root_dir, w_path):\n    if os.path.exists(w_path):\n        os.remove(w_path)\n    for file in os.listdir(data_root_dir):\n        path = os.path.join(data_root_dir, file)\n        if file.endswith(\"8-new.txt\"):\n            # \u91cd\u547d\u540d\u4e3adev.txt\n            os.rename(path, os.path.join(data_root_dir, \"dev.txt\"))\n            continue\n        if file.endswith(\"9-new.txt\"):\n            # \u91cd\u547d\u540d\u4e3atest.txt\n            os.rename(path, os.path.join(data_root_dir, \"test.txt\"))\n            continue\n        q_list = []\n        print(\"\u5f00\u59cb\u8bfb\u53d6\u6587\u4ef6:%s\" % file)\n        with open(path, \"r\", encoding=\"utf-8\") as f:\n            lines = f.readlines()\n            for line in lines:\n                line = line.rstrip()\n                if line == \"END O\":\n                    break\n                q_list.append(line)\n\n        # \u83b7\u53d6list \u5217\u8868: ['\u7f8e O', '\uff01 O', '\u6c14 O', '\u8d28 O', '\u7279 O', '\u522b O', '\u597d O', '', '\u9020 O', '\u578b O', '\u72ec O', '\u7279 O', '\uff0c O', '\u5c3a B-ORG', '\u7801 I-ORG', '\u504f B-Ne_VIEW', '\u5927 I-Ne_VIEW', '\uff0c O']\n        # print(\"q_list: \", q_list)\n        print(\"\u5f00\u59cb\u5199\u5165\u6587\u672c: %s\" % w_path)\n        with open(w_path, \"a\", encoding=\"utf-8\") as f:\n            for item in q_list:\n                f.write('%s\\n' % item)\n\n\ndef brat_1_format_origin(catalog):\n    \"\"\"\n    \u683c\u5f0f\u5316\u539f\u59cb\u6587\u4ef6\uff08\u53bb\u9664\u8868\u60c5\u7b26\u53f7\u7684\u5f71\u54cd\uff0cbrat\u53602\u4e2a\u5b57\u7b26\uff0c\u4f46\u662fpython\u53601\u4e2a\u5b57\u7b26\uff09\n    \"\"\"\n    with open('%s/origin/origin.txt' % path_root, \"r\", encoding=\"utf-8\") as f:\n        lines = f.readlines()\n    with open('%s/tag_befer/befer.txt' % path_root, \"w\", encoding=\"utf-8\") as f:\n        # \u8f6c\u6362\u539f\u59cb\u6587\u4ef6\n        for line in lines:\n            text = emoji.demojize(line)\n            f.write('%s' % text)\n        # \u521b\u5efa\u6807\u6ce8\u7684\u65b0\u6587\u4ef6\n        with open('%s/tag_befer/befer.ann' % path_root, \"w\", encoding=\"utf-8\") as f:\n            pass\n\ndef brat_2_create_train_data(catalog):\n    file_list = get_catalog_files(\"%s/tag_after\" % catalog, status=-1, str1=\".DS_Store\")\n    file_list = list(set([i.split(\"/\")[-1].split(\".\")[0] for i in file_list]))\n    print(file_list)\n    for filename in file_list:\n        r_ann_path = os.path.join(catalog, \"tag_after/%s.ann\" % filename)\n        r_txt_path = os.path.join(catalog, \"tag_after/%s.txt\" % filename)\n        w_path = os.path.join(catalog,  \"new/%s-new.txt\" % filename)\n        print(\"filename\", r_ann_path, r_txt_path, w_path)\n        from_ann2dic(r_ann_path, r_txt_path, w_path)\n    # \u751f\u6210train.txt\u3001dev.txt\u3001test.txt\n    create_train_data(\"%s/new\" % catalog, \"%s/new/train.txt\" % catalog)\n\n\ndef main():\n    catalog = Config.nlp_ner.path_root\n\n    # brat_1_format_origin(catalog)\n    brat_2_create_train_data(catalog)\n", "tutorials/keras/__init__.py": "", "tutorials/RecommenderSystems/rs_rating_demo.py": "#!/usr/bin/python\n# coding:utf-8\n# -------------------------------------------------------------------------------\n# Name:    \u63a8\u8350\u7cfb\u7edf\n# Purpose: \u63a8\u8350\u7cfb\u7edf: Item CF/User CF/SVD \u5bf9\u6bd4\n# Author:  jiangzhonglian\n# Create_time:  2020\u5e749\u670821\u65e5\n# Update_time:  2020\u5e749\u670821\u65e5\n# -------------------------------------------------------------------------------\nfrom __future__ import print_function\nimport sys\nimport math\nfrom operator import itemgetter\nimport numpy as np\nimport pandas as pd\nfrom scipy.sparse.linalg import svds\nfrom sklearn import model_selection as cv\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics.pairwise import pairwise_distances\nfrom middleware.utils import TimeStat, Chart\n\n\ndef splitData(dataFile, test_size):\n    # \u52a0\u8f7d\u6570\u636e\u96c6 (\u7528\u6237ID\uff0c \u7535\u5f71ID\uff0c \u8bc4\u5206\uff0c \u65f6\u95f4\u6233)\n    header = ['user_id', 'item_id', 'rating', 'timestamp']\n    df = pd.read_csv(dataFile, sep='\\t', names=header)\n\n    n_users = df.user_id.unique().shape[0]\n    n_items = df.item_id.unique().shape[0]\n\n    print('>>> \u672c\u6570\u636e\u96c6\u5305\u542b: \u603b\u7528\u6237\u6570 = %s | \u603b\u7535\u5f71\u6570 = %s' % (n_users, n_items) )\n    train_data, test_data = cv.train_test_split(df, test_size=test_size)\n    print(\">>> \u8bad\u7ec3:\u6d4b\u8bd5 = %s:%s = %s:%s\" % (len(train_data), len(test_data), 1-test_size, test_size))\n    return df, n_users, n_items, train_data, test_data\n\n\ndef calc_similarity(n_users, n_items, train_data, test_data):\n    # \u521b\u5efa\u7528\u6237\u4ea7\u54c1\u77e9\u9635\uff0c\u9488\u5bf9\u6d4b\u8bd5\u6570\u636e\u548c\u8bad\u7ec3\u6570\u636e\uff0c\u521b\u5efa\u4e24\u4e2a\u77e9\u9635: \n    \"\"\"\n    line:  Pandas(Index=93661, user_id=624, item_id=750, rating=4, timestamp=891961163)\n    \"\"\"\n    train_data_matrix = np.zeros((n_users, n_items))\n    for line in train_data.itertuples():\n        train_data_matrix[line[1] - 1, line[2] - 1] = line[3]\n\n    test_data_matrix = np.zeros((n_users, n_items))\n    for line in test_data.itertuples():\n        test_data_matrix[line[1] - 1, line[2] - 1] = line[3]\n\n    print(\"1:\", np.shape(train_data_matrix))    # \u884c: \u4eba | \u5217: \u7535\u5f71\n    print(\"2:\", np.shape(train_data_matrix.T))  # \u884c: \u7535\u5f71 | \u5217: \u4eba\n\n    # \u4f7f\u7528sklearn\u7684 pairwise_distances \u8ba1\u7b97\u5411\u91cf\u8ddd\u79bb\uff0ccosine\u6765\u8ba1\u7b97\u4f59\u5f26\u8ddd\u79bb\uff0c\u8d8a\u5c0f\u8d8a\u76f8\u4f3c\n    user_similarity = pairwise_distances(train_data_matrix, metric=\"cosine\")\n    item_similarity = pairwise_distances(train_data_matrix.T, metric=\"cosine\")\n    # print(\"<<< %s \\n %s\" % (np.shape(user_similarity), user_similarity) )\n    # print(\"<<< %s \\n %s\" % (np.shape(item_similarity), item_similarity) )\n\n    print('\u5f00\u59cb\u7edf\u8ba1\u6d41\u884citem\u7684\u6570\u91cf...', file=sys.stderr)\n    item_popular = {}\n    # \u7edf\u8ba1\u540c\u4e00\u4e2a\u7535\u5f71\uff0c\u89c2\u770b\u7684\u603b\u4eba\u6570\uff08\u4e5f\u5c31\u662f\u6240\u8c13\u7684\u6d41\u884c\u5ea6\uff01\uff09\n    for i_index in range(n_items):\n        if np.sum(train_data_matrix[:, i_index]) != 0:\n            item_popular[i_index] = np.sum(train_data_matrix[:, i_index] != 0)\n\n    # save the total number of items\n    item_count = len(item_popular)\n    print('\u603b\u5171\u6d41\u884c item \u6570\u91cf = %d' % item_count, file=sys.stderr)\n    return train_data_matrix, test_data_matrix, user_similarity, item_similarity, item_popular\n\n\ndef predict(rating, similarity, type='user'):\n    \"\"\"\n    :param rating: \u8bad\u7ec3\u6570\u636e\n    :param similarity: \u5411\u91cf\u8ddd\u79bb\n    :return:\n    \"\"\"\n    print(\"+++ %s\" % type)\n    print(\"    rating=\", np.shape(rating))\n    print(\"    similarity=\", np.shape(similarity))\n    if type == 'item':\n        \"\"\"\n        \u7efc\u5408\u6253\u5206:  \n            rating.dot(similarity) \u8868\u793a\uff1a\n                \u67d01\u4e2a\u4eba\u6240\u6709\u7684\u7535\u5f71\u7ec4\u5408 X \u00b7\u7535\u5f71*\u7535\u5f71\u00b7\u8ddd\u79bb\uff08\u7b2c1\u5217\u90fd\u662f\u5173\u4e8e\u7b2c1\u90e8\u7535\u5f71\u548c\u5176\u4ed6\u7684\u7535\u5f71\u7684\u8ddd\u79bb\uff09\u4e2d\uff0c\u8ba1\u7b97\u51fa \u7b2c\u4e00\u4e2a\u4eba\u5bf9\u7b2c1/2/3\u90e8\u7535\u5f71\u7684 \u603b\u8bc4\u5206 1*n\n                \u67d02\u4e2a\u4eba\u6240\u6709\u7684\u7535\u5f71\u7ec4\u5408 X \u00b7\u7535\u5f71*\u7535\u5f71\u00b7\u8ddd\u79bb\uff08\u7b2c1\u5217\u90fd\u662f\u5173\u4e8e\u7b2c1\u90e8\u7535\u5f71\u548c\u5176\u4ed6\u7684\u7535\u5f71\u7684\u8ddd\u79bb\uff09\u4e2d\uff0c\u8ba1\u7b97\u51fa \u7b2c\u4e00\u4e2a\u4eba\u5bf9\u7b2c1/2/3\u90e8\u7535\u5f71\u7684 \u603b\u8bc4\u5206 1*n\n                ...\n                \u67d0n\u4e2a\u4eba\u6240\u6709\u7684\u7535\u5f71\u7ec4\u5408 X \u00b7\u7535\u5f71*\u7535\u5f71\u00b7\u8ddd\u79bb\uff08\u7b2c1\u5217\u90fd\u662f\u5173\u4e8e\u7b2c1\u90e8\u7535\u5f71\u548c\u5176\u4ed6\u7684\u7535\u5f71\u7684\u8ddd\u79bb\uff09\u4e2d\uff0c\u8ba1\u7b97\u51fa \u7b2c\u4e00\u4e2a\u4eba\u5bf9\u7b2c1/2/3\u90e8\u7535\u5f71\u7684 \u603b\u8bc4\u5206 1*n\n            = \u4eba-\u7535\u5f71-\u8bc4\u5206(943, 1682) * \u7535\u5f71-\u7535\u5f71-\u8ddd\u79bb(1682, 1682) \n            = \u4eba-\u7535\u5f71-\u603b\u8bc4\u5206\u8ddd\u79bb(943, 1682)\n            \n            np.array([np.abs(similarity).sum(axis=1)]) \u8868\u793a: \u6a2a\u5411\u6c42\u548c: 1 \u8868\u793a\u67d0\u4e00\u884c\u6240\u6709\u7684\u5217\u6c42\u548c\n                \u7b2c1\u5217\u8868\u793a\uff1a\u67d0\u4e2aA\u7535\u5f71\uff0c\u5bf9\u4e8e\u6240\u6709\u7535\u5f71\u8ba1\u7b97\u51faA\u7684\u603b\u8ddd\u79bb\n                \u7b2c2\u5217\u8868\u793a\uff1a\u67d0\u4e2aB\u7535\u5f71\uff0c\u5bf9\u4e8e\u6240\u6709\u7535\u5f71\u7684\u7efc\u51faB\u7684\u603b\u8ddd\u79bb\n                ...\n                \u7b2cn\u5217\u8868\u793a\uff1a\u67d0\u4e2aN\u7535\u5f71\uff0c\u5bf9\u4e8e\u6240\u6709\u7535\u5f71\u7684\u7efc\u51faN\u7684\u603b\u8ddd\u79bb\n            = \u6bcf\u4e00\u4e2a\u7535\u5f71\u7684\u603b\u8ddd\u79bb (1, 1682)\n\n            pred = \u4eba-\u7535\u5f71-\u5e73\u5747\u8bc4\u5206 (943, 1682)\n        \"\"\"\n        pred = rating.dot(similarity) / np.array([np.abs(similarity).sum(axis=1)])\n    elif type == 'user':\n        # \u6bcf\u4e2a\u6837\u672c\u4e0a\u51cf\u53bb\u6570\u636e\u7684\u7edf\u8ba1\u5e73\u5747\u503c\u53ef\u4ee5\u79fb\u9664\u5171\u540c\u7684\u90e8\u5206\uff0c\u51f8\u663e\u4e2a\u4f53\u5dee\u5f02\u3002\n\n        # \u6c42\u51fa\u6bcf\u4e00\u4e2a\u7528\u6237\uff0c\u6240\u6709\u7535\u5f71\u7684\u7efc\u5408\u8bc4\u5206\n        # \u6a2a\u5411\u6c42\u5e73\u5747: 1 \u8868\u793a\u67d0\u4e00\u884c\u6240\u6709\u7684\u5217\u6c42\u5e73\u5747\n        mean_user_rating = rating.mean(axis=1)\n        # numpy\u4e2d\u5305\u542b\u7684 newaxis \u53ef\u4ee5\u7ed9\u539f\u6570\u7ec4\u589e\u52a0\u4e00\u4e2a\u7ef4\u5ea6\n        rating_diff = (rating - mean_user_rating[:, np.newaxis])\n\n        # \u5747\u5206  +  \n        # \u4eba-\u4eba-\u8ddd\u79bb(943, 943)*\u4eba-\u7535\u5f71-\u8bc4\u5206diff(943, 1682)=\u7ed3\u679c-\u4eba-\u7535\u5f71\uff08\u6bcf\u4e2a\u4eba\u5bf9\u540c\u4e00\u7535\u5f71\u7684\u7efc\u5408\u5f97\u5206\uff09(943, 1682)  \u518d\u9664\u4ee5  \u4e2a\u4eba\u4e0e\u5176\u4ed6\u4eba\u603b\u7684\u8ddd\u79bb = \u4eba-\u7535\u5f71\u7efc\u5408\u5f97\u5206\n        \"\"\"\n        \u7efc\u5408\u6253\u5206:  \n            similarity.dot(rating_diff) \u8868\u793a\uff1a\n                \u7b2c1\u5217\uff1a\u7b2c1\u4e2a\u4eba\u4e0e\u5176\u4ed6\u4eba\u7684\u76f8\u4f3c\u5ea6 * \u4eba\u4e0e\u7535\u5f71\u7684\u76f8\u4f3c\u5ea6\uff0c\u5f97\u5230 \u7b2c1\u4e2a\u4eba\u5bf9\u7b2c1/2/3\u5217\u7535\u5f71\u7684 \u603b\u5f97\u5206 1*n\n                \u7b2c2\u5217\uff1a\u7b2c2\u4e2a\u4eba\u4e0e\u5176\u4ed6\u4eba\u7684\u76f8\u4f3c\u5ea6 * \u4eba\u4e0e\u7535\u5f71\u7684\u76f8\u4f3c\u5ea6\uff0c\u5f97\u5230 \u7b2c2\u4e2a\u4eba\u5bf9\u7b2c1/2/3\u5217\u7535\u5f71\u7684 \u603b\u5f97\u5206 1*n\n                ...\n                \u7b2cn\u5217\uff1a\u7b2cn\u4e2a\u4eba\u4e0e\u5176\u4ed6\u4eba\u7684\u76f8\u4f3c\u5ea6 * \u4eba\u4e0e\u7535\u5f71\u7684\u76f8\u4f3c\u5ea6\uff0c\u5f97\u5230 \u7b2cn\u4e2a\u4eba\u5bf9\u7b2c1/2/3\u5217\u7535\u5f71\u7684 \u603b\u5f97\u5206 1*n\n            = \u4eba-\u4eba-\u8ddd\u79bb(943, 943)  *  \u4eba-\u7535\u5f71-\u8bc4\u5206(943, 1682)\n            = \u4eba-\u7535\u5f71-\u603b\u8bc4\u5206\u8ddd\u79bb(943, 1682)\n\n            np.array([np.abs(similarity).sum(axis=1)]) \u8868\u793a: \u6a2a\u5411\u6c42\u548c: 1 \u8868\u793a\u67d0\u4e00\u884c\u6240\u6709\u7684\u5217\u6c42\u548c\n                \u7b2c1\u5217\u8868\u793a\uff1a\u7b2cA\u4e2a\u4eba\uff0c\u5bf9\u4e8e\u6240\u6709\u4eba\u8ba1\u7b97\u51faA\u7684\u603b\u8ddd\u79bb\n                \u7b2c2\u5217\u8868\u793a\uff1a\u7b2cB\u4e2a\u4eba\uff0c\u5bf9\u4e8e\u6240\u6709\u4eba\u8ba1\u7b97\u51faB\u7684\u603b\u8ddd\u79bb\n                ...\n                \u7b2cn\u5217\u8868\u793a\uff1a\u7b2cN\u4e2a\u4eba\uff0c\u5bf9\u4e8e\u6240\u6709\u4eba\u8ba1\u7b97\u51faN\u7684\u603b\u8ddd\u79bb\n            = \u6bcf\u4e00\u4e2a\u7535\u5f71\u7684\u603b\u8ddd\u79bb (1, 943)\n\n            pred = \u5747\u503c + \u4eba-\u7535\u5f71-\u5e73\u5747\u8bc4\u5206 (943, 1682)\n        \"\"\"\n        pred = mean_user_rating[:, np.newaxis] + similarity.dot(rating_diff) / np.array([np.abs(similarity).sum(axis=1)]).T\n\n    return pred\n\n\ndef rmse(prediction, ground_truth):\n    prediction = prediction[ground_truth.nonzero()].flatten()\n    ground_truth = ground_truth[ground_truth.nonzero()].flatten()\n    return math.sqrt(mean_squared_error(prediction, ground_truth))\n\n\ndef evaluate(prediction, item_popular, name):\n    hit = 0\n    rec_count = 0\n    test_count = 0\n    popular_sum = 0\n    all_rec_items = set()\n    for u_index in range(n_users):\n        items = np.where(train_data_matrix[u_index, :] == 0)[0]\n        pre_items = sorted(\n            dict(zip(items, prediction[u_index, items])).items(),\n            key=itemgetter(1),\n            reverse=True)[:20]\n        test_items = np.where(test_data_matrix[u_index, :] != 0)[0]\n\n        # \u5bf9\u6bd4\u6d4b\u8bd5\u96c6\u548c\u63a8\u8350\u96c6\u7684\u5dee\u5f02 item, w\n        for item, _ in pre_items:\n            if item in test_items:\n                hit += 1\n            all_rec_items.add(item)\n\n            # popular_sum\u662f\u5bf9\u6240\u6709\u7684item\u7684\u6d41\u884c\u5ea6\u8fdb\u884c\u52a0\u548c\n            if item in item_popular:\n                popular_sum += math.log(1 + item_popular[item])\n\n        rec_count += len(pre_items)\n        test_count += len(test_items)\n\n    precision = hit / (1.0 * rec_count)\n    # \u53ec\u56de\u7387\uff0c\u76f8\u5bf9\u4e8e\u6d4b\u8bd5\u63a8\u8350\u96c6\u5408\u7684\u6570\u636e\n    recall = hit / (1.0 * test_count)\n    # \u8986\u76d6\u7387\uff0c\u76f8\u5bf9\u4e8e\u8bad\u7ec3\u96c6\u5408\u7684\u6570\u636e\n    coverage = len(all_rec_items) / (1.0 * len(item_popular))\n    popularity = popular_sum / (1.0 * rec_count)\n    print('--- %s: precision=%.4f \\t recall=%.4f \\t coverage=%.4f \\t popularity=%.4f' % (\n        name, precision, recall, coverage, popularity), file=sys.stderr)\n\n\ndef recommend(u_index, prediction):\n    items = np.where(train_data_matrix[u_index, :] == 0)[0]\n    pre_items = sorted(\n        dict(zip(items, prediction[u_index, items])).items(),\n        key=itemgetter(1),\n        reverse=True)[:10]\n    test_items = np.where(test_data_matrix[u_index, :] != 0)[0]\n\n    result = [key for key, value in pre_items]\n    result.sort(reverse=False)\n    print('\u539f\u59cb\u7ed3\u679c(%s): %s' % (len(test_items), test_items) )\n    print('\u63a8\u8350\u7ed3\u679c(%s): %s' % (len(result), result) )\n\n\ndef main():\n    global n_users, train_data_matrix, test_data_matrix\n    # \u57fa\u4e8e\u5185\u5b58\u7684\u534f\u540c\u8fc7\u6ee4\n    # ...\n    # \u62c6\u5206\u6570\u636e\u96c6\n    # http://files.grouplens.org/datasets/movielens/ml-100k.zip\n    path_root = \"/Users/jiangzl/work/data/\u673a\u5668\u5b66\u4e60\"\n    dataFile = '%s/16.RecommenderSystems/ml-100k/u.data' % path_root\n\n    df, n_users, n_items, train_data, test_data = splitData(dataFile, test_size=0.25)\n\n    # \u8ba1\u7b97\u76f8\u4f3c\u5ea6\n    train_data_matrix, test_data_matrix, user_similarity, item_similarity, item_popular = calc_similarity(\n        n_users, n_items, train_data, test_data)\n\n    item_prediction = predict(train_data_matrix, item_similarity, type='item')\n    user_prediction = predict(train_data_matrix, user_similarity, type='user')\n\n    # # \u8bc4\u4f30: \u5747\u65b9\u6839\u8bef\u5dee\n    print('>>> Item based CF RMSE: ' + str(rmse(item_prediction, test_data_matrix)))\n    print('>>> User based CF RMSE: ' + str(rmse(user_prediction, test_data_matrix)))\n\n    # \u57fa\u4e8e\u6a21\u578b\u7684\u534f\u540c\u8fc7\u6ee4\n    # ...\n    # \u8ba1\u7b97MovieLens\u6570\u636e\u96c6\u7684\u7a00\u758f\u5ea6 \uff08n_users\uff0cn_items \u662f\u5e38\u91cf\uff0c\u6240\u4ee5\uff0c\u7528\u6237\u884c\u4e3a\u6570\u636e\u8d8a\u5c11\uff0c\u610f\u5473\u7740\u4fe1\u606f\u91cf\u5c11\uff1b\u8d8a\u7a00\u758f\uff0c\u4f18\u5316\u7684\u7a7a\u95f4\u4e5f\u8d8a\u5927\uff09\n    sparsity = round(1.0 - len(df) / float(n_users * n_items), 3)\n    print('\\nMovieLen100K\u7684\u7a00\u758f\u5ea6: %s%%\\n' % (sparsity * 100))\n\n    # # \u8ba1\u7b97\u7a00\u758f\u77e9\u9635\u7684\u6700\u5927k\u4e2a\u5947\u5f02\u503c/\u5411\u91cf\n    # minrmse = math.inf\n    # index = 1\n    # for k in range(1, 30, 1):\n    #     u, s, vt = svds(train_data_matrix, k=k)\n    #     # print(\">>> \", s)\n    #     s_diag_matrix = np.diag(s)\n    #     svd_prediction = np.dot(np.dot(u, s_diag_matrix), vt)\n    #     r_rmse = rmse(svd_prediction, test_data_matrix)\n    #     if r_rmse < minrmse:\n    #         index = k\n    #         minrmse = r_rmse\n\n    index = 11\n    minrmse = 2.6717213264389765\n    u, s, vt = svds(train_data_matrix, k=index)\n    # print(\">>> \", s)\n    s_diag_matrix = np.diag(s)\n    svd_prediction = np.dot(np.dot(u, s_diag_matrix), vt)\n    r_rmse = rmse(svd_prediction, test_data_matrix)\n    print(\"+++ k=%s, svd-shape: %s\" % (index, np.shape(svd_prediction)) )\n    print('>>> Model based CF RMSE: %s\\n' %  minrmse)\n    # \"\"\"\n    # \u5728\u4fe1\u606f\u91cf\u76f8\u540c\u7684\u60c5\u51b5\u4e0b\uff0c\u77e9\u9635\u8d8a\u5c0f\uff0c\u90a3\u4e48\u643a\u5e26\u7684\u4fe1\u606f\u8d8a\u53ef\u9760\u3002\n    # \u6240\u4ee5: user-cf \u63a8\u8350\u6548\u679c\u9ad8\u4e8e item-cf\uff1b \u800csvd\u5206\u89e3\u540e\uff0c\u53d1\u73b015\u4e2a\u7ef4\u5ea6\u6548\u679c\u5c31\u80fd\u8fbe\u523090%\u4ee5\u4e0a\uff0c\u6240\u4ee5\u4fe1\u606f\u66f4\u53ef\u9760\uff0c\u6548\u679c\u4e5f\u66f4\u597d\u3002\n    # item-cf: 1682\n    # user-cf: 943\n    # svd: 15\n    # \"\"\"\n    evaluate(item_prediction, item_popular, 'item')\n    evaluate(user_prediction, item_popular, 'user')\n    evaluate(svd_prediction,  item_popular, 'svd')\n\n    # \u63a8\u8350\u7ed3\u679c\n    # recommend(1, item_prediction)\n    # recommend(1, user_prediction)\n    recommend(1, svd_prediction)\n\n\nif __name__ == \"__main__\":\n    main()", "tutorials/RecommenderSystems/rs_content_demo.py": "#!/usr/bin/python\n# coding:utf-8\n# -------------------------------------------------------------------------------\n# Name:    \u63a8\u8350\u7cfb\u7edf\n# Purpose: \u57fa\u4e8e\u5185\u5bb9\u63a8\u8350\n# Author:  jiangzhonglian\n# Create_time:  2020\u5e7410\u670815\u65e5\n# Update_time:  2020\u5e7410\u670821\u65e5\n# -------------------------------------------------------------------------------\nimport os\nimport sys\nimport numpy as np\nimport pandas as pd\n# \u81ea\u5b9a\u4e49\u5e93\nimport config.set_content as setting\nfrom middleware.utils import pd_load, pd_like, pd_save, pd_rename, get_days\n\n\ndef data_converting(infile, outfile):\n    \"\"\"\n    # \u5c06\u7528\u6237\u4ea4\u6613\u6570\u636e\u8f6c\u5316\u4e3a: \n    # \u5c06\n    #     \u7528\u6237ID\u3001\u5404\u79cd\u57fa\u91d1\u3001\u53d8\u52a8\u91d1\u989d\u3001\u65f6\u95f4\n    # \u8f6c\u5316\u4e3a\uff1a\n    #     \u7528\u6237ID\u3001\u57fa\u91d1ID\u3001\u8d2d\u4e70\u91d1\u989d\u3001\u65f6\u95f4\u7684\u6570\u636e\n    \"\"\"\n    print(\"Loading user daliy data...\")\n    df = pd_load(infile)\n    df[\"money\"] = df[\"\u53d8\u52a8\u91d1\u989d\"].apply(lambda line: abs(line))\n    df_user_item = df.groupby([\"\u7528\u6237\u8d26\u53f7\", \"\u8bc1\u5238\u4ee3\u7801\"], as_index=False).agg({\n            \"money\": np.sum\n        }).sort_values(\"money\", ascending=True)\n    pd_rename(df_user_item, [\"user_id\", \"item_id\", \"rating\"])\n    pd_save(df_user_item, outfile)\n\n\ndef create_user2item(infile, outfile):\n    \"\"\"\u521b\u5efauser-item\u8bc4\u5206\u77e9\u9635\"\"\"\n\n    print(\"Loading user daliy data...\")\n    df_user_item = pd_load(infile)\n\n    user_id = sorted(df_user_item['user_id'].unique(), reverse=False)\n    item_id = sorted(df_user_item['item_id'].unique(), reverse=False)\n    # print(\"+++ user_id:\", user_id)\n    # print(\"+++ item_id:\", item_id)\n    rating_matrix = np.zeros([len(user_id),len(item_id)])\n    rating_matrix = pd.DataFrame(rating_matrix, index=user_id, columns=item_id)\n\n    print(\"Converting data...\")\n    count = 0\n    user_num= len(user_id)\n    for uid in user_id:\n        user_rating = df_user_item[df_user_item['user_id'] == uid].drop(['user_id'], axis=1)\n        user_rated_num = len(user_rating)\n        for row in range(0, user_rated_num):\n            item_id = user_rating['item_id'].iloc[row]\n            # \u884c\uff08\u7528\u6237\uff09\uff0c\u5217\uff08\u7535\u5f71\uff09\uff0c\u5f97\u5206\n            rating_matrix.loc[uid, item_id] = user_rating['rating'].iloc[row]\n\n        count += 1\n        if count % 10 == 0:\n            completed_percentage = round(float(count) / user_num * 100)\n            print(\"Completed %s\" % completed_percentage + \"%\")\n\n    rating_matrix.index.name = 'user_id'\n    pd_save(rating_matrix, outfile, index=True)\n\n\ndef create_item2feature(infile, outfile):\n    \"\"\"\u521b\u5efa item-\u7279\u5f81-\u662f\u5426\u5b58\u5728 \u77e9\u9635\"\"\"\n\n    print(\"Loading item feature data...\")\n    df_item_info = pd_load(infile, header=1)\n    items_num = df_item_info.shape[0]\n    columns = df_item_info.columns.tolist()\n    new_cols = [col for col in columns if col not in [\"info_type\", \"info_investype\"]]\n    info_types      = sorted(df_item_info[\"info_type\"].unique(), reverse=False)\n    info_investypes = sorted(df_item_info[\"info_investype\"].unique(), reverse=False)\n    dict_n_cols = {\"info_type\": info_types, \"info_investype\": info_investypes}\n    new_cols.append(dict_n_cols)\n    # \u83b7\u53d6\u65b0\u7684\u5217\u540d\n    def get_new_columns(new_cols):\n        new_columns = []\n        for col in new_cols:\n            if isinstance(col, dict):\n                for k, vs in col.items():\n                    new_columns += vs\n            else:\n                new_columns.append(col)\n        return new_columns\n    new_columns = get_new_columns(new_cols)\n    # print(new_cols)\n    # print(new_columns)\n\n    # ['item_id', 'info_name', 'info_trackerror', 'info_manafeeratioo', 'info_custfeeratioo', 'info_salefeeratioo', 'info_foundsize', 'info_foundlevel', 'info_creattime', 'info_unitworth'\n    # {'info_type': ['QDII-ETF', '\u6df7\u5408\u578b', '\u80a1\u7968\u6307\u6570', 'ETF-\u573a\u5185'], 'info_investype': ['\u5951\u7ea6\u578b\u5f00\u653e\u5f0f', '\u5951\u7ea6\u578b\u5c01\u95ed\u5f0f']}]\n    def deal_line(line, new_cols):\n        result = []\n        for col in new_cols:\n            if isinstance(col, str):\n                result.append(line[col])\n            elif isinstance(col, dict):\n                for k, vs in col.items():\n                    for v in vs:\n                        if v == line[k]:\n                            result.append(1)\n                        else:\n                            result.append(0)\n            else:\n                print(\"\u7c7b\u578b\u9519\u8bef\")\n                sys.exit(1)\n        return result\n\n    df = df_item_info.apply(lambda line: deal_line(line, new_cols), axis=1, result_type=\"expand\")\n    pd_rename(df, new_columns)\n    # \u5904\u7406\u65f6\u95f4\n    end_time = \"2020-10-19\"\n    df[\"days\"] = df[\"info_creattime\"].apply(lambda str_time: get_days(str_time, end_time))\n    # print(df.head(5))\n    df.drop(['info_name', 'info_foundlevel', 'info_creattime'], axis=1, inplace=True)\n    pd_save(df, outfile)\n\n\ndef rs_1_data_preprocess():\n    # \u539f\u5c5e\u6570\u636e\n    data_infile = setting.PATH_CONFIG[\"user_daily\"]\n    # \u7528\u6237-\u7269\u54c1-\u8bc4\u5206\n    user_infile = setting.PATH_CONFIG[\"user_item\"]\n    user_outfile = setting.PATH_CONFIG[\"matrix_user_item2rating\"]\n    # \u7269\u54c1-\u7279\u5f81-\u8bc4\u5206\n    item_infile = setting.PATH_CONFIG[\"item_info\"]\n    item_outfile = setting.PATH_CONFIG[\"matrix_item2feature\"]\n\n    # \u5224\u65ad\u7528\u6237\u4ea4\u6613\u6570\u636e\uff0c\u5982\u679c\u4e0d\u5b58\u5728\u5c31\u8981\u91cd\u65b0\u751f\u6210\n    if not os.path.exists(user_infile):\n        \"\"\"\u6570\u636e\u5904\u7406\u90e8\u5206\"\"\"\n        # user \u6570\u636e\u9884\u5904\u7406\n        data_converting(data_infile, user_infile)\n        # \u521b\u5efa user-item-\u8bc4\u5206 \u77e9\u9635\n        create_user2item(user_infile, user_outfile)\n    else:\n        if not os.path.exists(user_outfile):\n            # \u521b\u5efa user-item-\u8bc4\u5206 \u77e9\u9635\n            create_user2item(user_infile, user_outfile)\n\n    if not os.path.exists(item_outfile):\n        # \u521b\u5efa item-feature-\u662f\u5426\u5b58\u5728 \u77e9\u9635\n        create_item2feature(item_infile, item_outfile)\n\n    user_feature = pd_load(user_outfile)\n    item_feature = pd_load(item_outfile)\n    user_feature.set_index('user_id', inplace=True)\n    item_feature.set_index('item_id', inplace=True)\n    return user_feature, item_feature\n\n\ndef cos_measure(item_feature_vector, user_rated_items_matrix):\n    \"\"\"\n    \u8ba1\u7b97item\u4e4b\u95f4\u7684\u4f59\u5f26\u5939\u89d2\u76f8\u4f3c\u5ea6\n    :param item_feature_vector: \u5f85\u6d4b\u91cf\u7684item\u7279\u5f81\u5411\u91cf\n    :param user_rated_items_matrix: \u7528\u6237\u5df2\u8bc4\u5206\u7684items\u7684\u7279\u5f81\u77e9\u9635\n    :return: \u5f85\u8ba1\u7b97item\u4e0e\u7528\u6237\u5df2\u8bc4\u5206\u7684items\u7684\u4f59\u5f26\u5939\u89d2\u76f8\u8bc6\u5ea6\u7684\u5411\u91cf\n    \"\"\"\n    x_c = (item_feature_vector * user_rated_items_matrix.T) + 0.0000001\n    mod_x = np.sqrt(item_feature_vector * item_feature_vector.T)\n    mod_c = np.sqrt((user_rated_items_matrix * user_rated_items_matrix.T).diagonal())\n    cos_xc = x_c / (mod_x * mod_c)\n\n    return cos_xc\n\n\ndef comp_user_feature(user_rated_vector, item_feature_matrix):\n    \"\"\"\n    \u6839\u636euser\u7684\u8bc4\u5206\u6765\u8ba1\u7b97\u5f97\u5230user\u7684\u559c\u597d\u7279\u5f81\n    :param user_rated_vector  : user\u7684\u8bc4\u5206\u5411\u91cf\n    :param item_feature_matrix: item\u7684\u7279\u5f81\u77e9\u9635\n    :return: user\u7684\u559c\u597d\u7279\u5f81\n    \"\"\"\n    # user\u8bc4\u5206\u7684\u5747\u503c\n    user_rating_mean = user_rated_vector.mean()\n    # # \u5206\u522b\u5f97\u5230user\u559c\u6b22\u548c\u4e0d\u559c\u6b22item\u7684\u5411\u91cf\u4ee5\u53caitem\u5bf9\u5e94\u7684\u5f15\u7d22(\u4ee5\u8be5user\u7684\u8bc4\u5206\u5747\u503c\u6765\u5212\u5206)\n    user_like_item = user_rated_vector.loc[user_rated_vector >= user_rating_mean]\n    user_unlike_item = user_rated_vector.loc[user_rated_vector < user_rating_mean]\n\n    print(\"user_like_item: \\n\", user_like_item)\n    print(\"user_unlike_item: \\n\", user_unlike_item)\n\n    # \u83b7\u53d6\u4e70\u5165\u548c\u5356\u51fa\u7684 index\n    user_like_item_index = map(int, user_like_item.index.values)\n    user_unlike_item_index = map(int, user_unlike_item.index.values)\n    # \u83b7\u53d6\u4e70\u5165\u548c\u5356\u51fa\u7684 value\n    user_like_item_rating = np.matrix(user_like_item.values)\n    user_unlike_item_rating = np.matrix(user_unlike_item.values)\n\n    #\u5f97\u5230user\u559c\u6b22\u548c\u4e0d\u559c\u6b22item\u7684\u7279\u5f81\u77e9\u9635\n    user_like_item_feature_matrix = np.matrix(item_feature_matrix.loc[user_like_item_index, :].values)\n    user_unlike_item_feature_matrix = np.matrix(item_feature_matrix.loc[user_unlike_item_index, :].values)\n\n    #\u8ba1\u7b97user\u7684\u559c\u597d\u7279\u5f81\u5411\u91cf\uff0c\u4ee5\u5176\u5bf9item\u7684\u8bc4\u5206\u4f5c\u4e3a\u6743\u91cd\n    weight_of_like = user_like_item_rating / user_like_item_rating.sum()\n    weight_of_unlike = user_unlike_item_rating / user_unlike_item_rating.sum()\n\n    print(\"weight_of_like: \", weight_of_like)\n    print(\"weight_of_unlike: \", weight_of_unlike)\n\n    #\u8ba1\u7b97user\u7684\u559c\u6b22\u7279\u5f81\u548c\u4e0d\u559c\u6b22\u7279\u5f81\u4ee5\u53ca\u603b\u7279\u5f81\n    user_like_feature = weight_of_like * user_like_item_feature_matrix\n    user_unlike_feature = weight_of_unlike * user_unlike_item_feature_matrix\n    user_feature_tol = user_like_feature - user_unlike_feature\n    return user_feature_tol\n\n\ndef rs_2_cb_recommend(user_feature, item_feature_matrix, K=20):\n    \"\"\"\n    \u8ba1\u7b97\u5f97\u5230\u4e0euser\u6700\u76f8\u4f3c\u7684Top K\u4e2aitem\u63a8\u8350\u7ed9user\n    :param user_feature: \u5f85\u63a8\u8350\u7528\u6237\u7684\u5bf9item\u7684\u8bc4\u5206\u5411\u91cf\n    :param item_feature_matrix: \u5305\u542b\u6240\u6709item\u7684\u7279\u5f81\u77e9\u9635\n    :param K: \u63a8\u8350\u7ed9user\u7684item\u6570\u91cf\n    :return: \u4e0euser\u6700\u76f8\u4f3c\u7684Top K\u4e2aitem\u7684\u7f16\u53f7\n    \"\"\"\n    # \u5f97\u5230user\u5df2\u8bc4\u5206\u548c\u672a\u8bc4\u5206\u7684item\u5411\u91cf\n    user_rated_vector = user_feature.loc[user_feature > 0]\n    # print(\"\u64cd\u4f5c >>> \\n\", user_rated_vector)\n    # user_unrated_vector = user_feature.loc[user_feature == 0]\n    # print(\"\u672a\u64cd\u4f5c >>> \\n\", user_unrated_vector)\n    # \u4e70\u8fc7\u7684\u5176\u5b9e\u4e5f\u53ef\u4ee5\u63a8\u8350\n    user_unrated_vector = user_feature\n    # print(\">>> \\n\", user_unrated_vector)\n\n    # user\u559c\u597d\u603b\u7279\u5f81(\u5c31\u662f\u7528\u6237\u7684\u8c03\u6027)\n    user_item_feature_tol = comp_user_feature(user_rated_vector, item_feature_matrix)\n    print(\">>> \u7528\u6237\u8c03\u6027\", user_item_feature_tol)\n    #\u672a\u8bc4\u5206item\u7684\u7279\u5f81\u77e9\u9635\n    user_unrated_item_index = map(int, user_unrated_vector.index.values)\n    user_unrated_item_feature_matrix = np.matrix(item_feature_matrix.loc[user_unrated_item_index, :].values)\n\n    #\u5f97\u5230\u76f8\u4f3c\u5ea6\u5e76\u8fdb\u884c\u6392\u5e8f\n    similarity = list(np.array(cos_measure(user_item_feature_tol, user_unrated_item_feature_matrix))[0])\n\n    key = {'item_index': list(user_unrated_vector.index.values), 'similarity': similarity}\n    item_sim_df = pd.DataFrame(key)\n    item_sim_df.sort_values('similarity', ascending=False, inplace=True)\n    # print(item_sim_df.head(100))\n    return item_sim_df.iloc[:K, 0].values\n\n\ndef estimate_rate(user_rated_vector, similarity):\n    \"\"\"\n    \u4f30\u8ba1\u7528\u6237\u5bf9item\u7684\u8bc4\u5206\n    :param user_rated_vector: \u7528\u6237\u5df2\u6709item\u8bc4\u5206\u5411\u91cf\n    :param similarity: \u5f85\u4f30\u8ba1item\u548c\u5df2\u8bc4\u5206item\u7684\u76f8\u8bc6\u5ea6\u5411\u91cf\n    :return:\u7528\u6237\u5bf9item\u7684\u8bc4\u5206\u7684\u4f30\u8ba1\n    \"\"\"\n    rate_hat = (user_rated_vector * similarity.T) / similarity.sum()\n    # print(\">>> \", rate_hat)\n    return rate_hat[0, 0]\n\n\ndef rs_2_cb_recommend_estimate(user_feature, item_feature_matrix, item):\n    \"\"\"\n    \u57fa\u4e8e\u5185\u5bb9\u7684\u63a8\u8350\u7b97\u6cd5\u5bf9item\u7684\u8bc4\u5206\u8fdb\u884c\u4f30\u8ba1\n    :param item_feature_matrix: \u5305\u542b\u6240\u6709item\u7684\u7279\u5f81\u77e9\u9635\n    :param user_feature: \u5f85\u4f30\u8ba1\u7528\u6237\u7684\u5bf9item\u7684\u8bc4\u5206\u5411\u91cf\n    :param item: \u5f85\u4f30\u8ba1item\u7684\u7f16\u53f7\n    :return: \u57fa\u4e8e\u5185\u5bb9\u7684\u63a8\u8350\u7b97\u6cd5\u5bf9item\u7684\u8bc4\u5206\u8fdb\u884c\u4f30\u8ba1\n    \"\"\"\n    # #\u5f97\u5230item\u7684\u5f15\u7d22\u4ee5\u53ca\u7279\u5f81\u77e9\u9635\n    # item_index = item_feature_matrix.index\n    # item_feature = item_feature_matrix.values\n\n    #\u5f97\u5230\u6240\u6709user\u8bc4\u5206\u7684item\u7684\u5f15\u7d22\n    user_item_index = user_feature.index\n\n    #\u67d0\u4e00\u7528\u6237\u5df2\u6709\u8bc4\u5206item\u7684\u8bc4\u5206\u5411\u91cf\u548c\u5f15\u7d22\u4ee5\u53caitem\u7684\u8bc4\u5206\u77e9\u9635\n    user_rated_vector = np.matrix(user_feature.loc[user_feature > 0].values)\n    user_rated_items = map(int, user_item_index[user_feature > 0].values)\n\n    user_rated_items_matrix = np.matrix(item_feature_matrix.loc[user_rated_items, :].values)\n\n    #\u5f85\u8bc4\u5206item\u7684\u7279\u5f81\u5411\u91cf\uff0c\u51fd\u6570\u4e2d\u7ed9\u51fa\u7684\u662f\u8be5item\u7684Id\n    item_feature_vector = np.matrix(item_feature_matrix.loc[item].values)\n\n    #\u5f97\u5230\u5f85\u8ba1\u7b97item\u4e0e\u7528\u6237\u5df2\u8bc4\u5206\u7684items\u7684\u4f59\u5f26\u5939\u89d2\u76f8\u8bc6\u5ea6\u7684\u5411\u91cf\n    cos_xc = cos_measure(item_feature_vector, user_rated_items_matrix)\n    # print(\">>> \u76f8\u4f3c\u5ea6: %s\" % cos_xc)\n    #\u8ba1\u7b97uesr\u5bf9\u8be5item\u7684\u8bc4\u5206\u4f30\u8ba1\n    rate_hat = estimate_rate(user_rated_vector, cos_xc)\n    return rate_hat\n\n\ndef main():\n    # \u6570\u636e\u521d\u59cb\u5316\n    user_id = 20200930\n    K = 10\n    user_feature, item_feature = rs_1_data_preprocess()\n    # \u57fa\u4e8e\u5185\u5bb9\u63a8\u8350\u7684\u6a21\u5757(\u7ed9\u67d0\u4e00\u4e2a\u7528\u6237\u63a8\u8350 10\u4e2a \u4ed6\u611f\u5174\u8da3\u7684\u5185\u5bb9)\n    user_feature = user_feature.loc[user_id, :]  # \u4e00\u884c \u7528\u6237(\u5177\u4f53\u67d0\u4e00\u4e2a\u7528\u6237)-\u7535\u5f71-\u8bc4\u5206 \u6570\u636e\n    print(\">>> 1 \\n\", user_feature)\n    # \u6548\u679c\u4e0d\u597d\uff0c\u6709\u51e0\u4e2a\u539f\u56e0\n    # 1. \u4ea4\u6613\u6570\u636e\u6bd4\u8f83\u5c11\n    # 2. \u57fa\u91d1\u7684\u7279\u5f81\u4e0d\u591f\u5168\u9762\n    # 3. \u4f18\u5316\u559c\u6b22\u548c\u4e0d\u559c\u6b22\u7684\u9608\u503c\n    result = rs_2_cb_recommend(user_feature, item_feature, K)\n    print(result)\n    # for code in result:\n    #     # \u7ed9\u67d0\u4e00\u4e2a\u7528\u6237\u63a8\u8350\u4e00\u4e2aitem, \u9884\u4f30\u63a8\u8350\u8bc4\u5206\n    #     price = rs_2_cb_recommend_estimate(user_feature, item_feature, code)\n    #     if price > 1000:\n    #         print(\"--- %s \u57fa\u91d1\u4e70\u5165 %s\" % (code, abs(price)) )\n    #     elif price < -1000:\n    #         print(\"--- %s \u57fa\u91d1\u5356\u51fa %s\" % (code, abs(price)) )\n    #     else:\n    #         print(\"--- \u4e0d\u505a\u4efb\u4f55\u64cd\u4f5c\")\n\n\nif __name__ == \"__main__\":\n    main()\n"}