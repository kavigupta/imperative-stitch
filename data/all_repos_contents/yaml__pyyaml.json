{"setup.py": "\nNAME = 'PyYAML'\nVERSION = '6.0.1'\nDESCRIPTION = \"YAML parser and emitter for Python\"\nLONG_DESCRIPTION = \"\"\"\\\nYAML is a data serialization format designed for human readability\nand interaction with scripting languages.  PyYAML is a YAML parser\nand emitter for Python.\n\nPyYAML features a complete YAML 1.1 parser, Unicode support, pickle\nsupport, capable extension API, and sensible error messages.  PyYAML\nsupports standard YAML tags and provides Python-specific tags that\nallow to represent an arbitrary Python object.\n\nPyYAML is applicable for a broad range of tasks from complex\nconfiguration files to object serialization and persistence.\"\"\"\nAUTHOR = \"Kirill Simonov\"\nAUTHOR_EMAIL = 'xi@resolvent.net'\nLICENSE = \"MIT\"\nPLATFORMS = \"Any\"\nURL = \"https://pyyaml.org/\"\nDOWNLOAD_URL = \"https://pypi.org/project/PyYAML/\"\nCLASSIFIERS = [\n    \"Development Status :: 5 - Production/Stable\",\n    \"Intended Audience :: Developers\",\n    \"License :: OSI Approved :: MIT License\",\n    \"Operating System :: OS Independent\",\n    \"Programming Language :: Cython\",\n    \"Programming Language :: Python\",\n    \"Programming Language :: Python :: 3\",\n    \"Programming Language :: Python :: 3.6\",\n    \"Programming Language :: Python :: 3.7\",\n    \"Programming Language :: Python :: 3.8\",\n    \"Programming Language :: Python :: 3.9\",\n    \"Programming Language :: Python :: 3.10\",\n    \"Programming Language :: Python :: 3.11\",\n    \"Programming Language :: Python :: 3.12\",\n    \"Programming Language :: Python :: 3.13\",\n    \"Programming Language :: Python :: Implementation :: CPython\",\n    \"Programming Language :: Python :: Implementation :: PyPy\",\n    \"Topic :: Software Development :: Libraries :: Python Modules\",\n    \"Topic :: Text Processing :: Markup\",\n]\nPROJECT_URLS = {\n   'Bug Tracker': 'https://github.com/yaml/pyyaml/issues',\n   'CI': 'https://github.com/yaml/pyyaml/actions',\n   'Documentation': 'https://pyyaml.org/wiki/PyYAMLDocumentation',\n   'Mailing lists': 'http://lists.sourceforge.net/lists/listinfo/yaml-core',\n   'Source Code': 'https://github.com/yaml/pyyaml',\n}\n\nLIBYAML_CHECK = \"\"\"\n#include <yaml.h>\n\nint main(void) {\n    yaml_parser_t parser;\n    yaml_emitter_t emitter;\n\n    yaml_parser_initialize(&parser);\n    yaml_parser_delete(&parser);\n\n    yaml_emitter_initialize(&emitter);\n    yaml_emitter_delete(&emitter);\n\n    return 0;\n}\n\"\"\"\n\n\nimport sys, os, os.path, pathlib, platform, shutil, tempfile, warnings\n\n# for newer setuptools, enable the embedded distutils before importing setuptools/distutils to avoid warnings\nos.environ['SETUPTOOLS_USE_DISTUTILS'] = 'local'\n\nfrom setuptools import setup, Command, Distribution as _Distribution, Extension as _Extension\nfrom setuptools.command.build_ext import build_ext as _build_ext\n# NB: distutils imports must remain below setuptools to ensure we use the embedded version\nfrom distutils import log\nfrom distutils.errors import DistutilsError, CompileError, LinkError, DistutilsPlatformError\n\nwith_cython = False\nif 'sdist' in sys.argv or os.environ.get('PYYAML_FORCE_CYTHON') == '1':\n    # we need cython here\n    with_cython = True\ntry:\n    from Cython.Distutils.extension import Extension as _Extension\n    try:\n        # try old_build_ext from Cython > 3 first, until we can dump it entirely\n        from Cython.Distutils.old_build_ext import old_build_ext as _build_ext\n    except ImportError:\n        # Cython < 3\n        from Cython.Distutils import build_ext as _build_ext\n    with_cython = True\nexcept ImportError:\n    if with_cython:\n        raise\n\ntry:\n    from wheel.bdist_wheel import bdist_wheel\nexcept ImportError:\n    bdist_wheel = None\n\n\ntry:\n    from _pyyaml_pep517 import ActiveConfigSettings\nexcept ImportError:\n    class ActiveConfigSettings:\n        @staticmethod\n        def current():\n            return {}\n\n# on Windows, disable wheel generation warning noise\nwindows_ignore_warnings = [\n\"Unknown distribution option: 'python_requires'\",\n\"Config variable 'Py_DEBUG' is unset\",\n\"Config variable 'WITH_PYMALLOC' is unset\",\n\"Config variable 'Py_UNICODE_SIZE' is unset\",\n\"Cython directive 'language_level' not set\"\n]\n\nif platform.system() == 'Windows':\n    for w in windows_ignore_warnings:\n        warnings.filterwarnings('ignore', w)\n\n\nclass Distribution(_Distribution):\n    def __init__(self, attrs=None):\n        _Distribution.__init__(self, attrs)\n        if not self.ext_modules:\n            return\n        for idx in range(len(self.ext_modules)-1, -1, -1):\n            ext = self.ext_modules[idx]\n            if not isinstance(ext, Extension):\n                continue\n            setattr(self, ext.attr_name, None)\n            self.global_options = [\n                    (ext.option_name, None,\n                        \"include %s (default if %s is available)\"\n                        % (ext.feature_description, ext.feature_name)),\n                    (ext.neg_option_name, None,\n                        \"exclude %s\" % ext.feature_description),\n            ] + self.global_options\n            self.negative_opt = self.negative_opt.copy()\n            self.negative_opt[ext.neg_option_name] = ext.option_name\n\n    def has_ext_modules(self):\n        if not self.ext_modules:\n            return False\n        for ext in self.ext_modules:\n            with_ext = self.ext_status(ext)\n            if with_ext is None or with_ext:\n                return True\n        return False\n\n    def ext_status(self, ext):\n        implementation = platform.python_implementation()\n        if implementation not in ['CPython', 'PyPy']:\n            return False\n        if isinstance(ext, Extension):\n            # the \"build by default\" behavior is implemented by this returning None\n            with_ext = getattr(self, ext.attr_name) or os.environ.get('PYYAML_FORCE_{0}'.format(ext.feature_name.upper()))\n            try:\n                with_ext = int(with_ext)  # attempt coerce envvar to int\n            except TypeError:\n                pass\n            return with_ext\n        else:\n            return True\n\n\nclass Extension(_Extension):\n\n    def __init__(self, name, sources, feature_name, feature_description,\n            feature_check, **kwds):\n        if not with_cython:\n            for filename in sources[:]:\n                base, ext = os.path.splitext(filename)\n                if ext == '.pyx':\n                    sources.remove(filename)\n                    sources.append('%s.c' % base)\n        _Extension.__init__(self, name, sources, **kwds)\n        self.feature_name = feature_name\n        self.feature_description = feature_description\n        self.feature_check = feature_check\n        self.attr_name = 'with_' + feature_name.replace('-', '_')\n        self.option_name = 'with-' + feature_name\n        self.neg_option_name = 'without-' + feature_name\n\n\nclass build_ext(_build_ext):\n    def finalize_options(self):\n        super().finalize_options()\n        pep517_config = ActiveConfigSettings.current()\n\n        build_config = pep517_config.get('pyyaml_build_config')\n\n        if build_config:\n            import json\n            build_config = json.loads(build_config)\n            print(f\"`pyyaml_build_config`: {build_config}\")\n        else:\n            build_config = {}\n            print(\"No `pyyaml_build_config` setting found.\")\n\n        for key, value in build_config.items():\n            existing_value = getattr(self, key, ...)\n            if existing_value is ...:\n                print(f\"ignoring unknown config key {key!r}\")\n                continue\n\n            if existing_value:\n                print(f\"combining {key!r} {existing_value!r} and {value!r}\")\n                value = existing_value + value  # FIXME: handle type diff\n\n            setattr(self, key, value)\n\n    def run(self):\n        optional = True\n        disabled = True\n        for ext in self.extensions:\n            with_ext = self.distribution.ext_status(ext)\n            if with_ext is None:\n                disabled = False\n            elif with_ext:\n                optional = False\n                disabled = False\n                break\n        if disabled:\n            return\n        try:\n            _build_ext.run(self)\n        except DistutilsPlatformError:\n            exc = sys.exc_info()[1]\n            if optional:\n                log.warn(str(exc))\n                log.warn(\"skipping build_ext\")\n            else:\n                raise\n\n    def get_source_files(self):\n        self.check_extensions_list(self.extensions)\n        filenames = []\n        for ext in self.extensions:\n            if with_cython:\n                self.cython_sources(ext.sources, ext)\n            for filename in ext.sources:\n                filenames.append(filename)\n                base = os.path.splitext(filename)[0]\n                for ext in ['c', 'h', 'pyx', 'pxd']:\n                    filename = '%s.%s' % (base, ext)\n                    if filename not in filenames and os.path.isfile(filename):\n                        filenames.append(filename)\n        return filenames\n\n    def get_outputs(self):\n        self.check_extensions_list(self.extensions)\n        outputs = []\n        for ext in self.extensions:\n            fullname = self.get_ext_fullname(ext.name)\n            filename = os.path.join(self.build_lib,\n                                    self.get_ext_filename(fullname))\n            if os.path.isfile(filename):\n                outputs.append(filename)\n        return outputs\n\n    def build_extensions(self):\n        self.check_extensions_list(self.extensions)\n        for ext in self.extensions:\n            with_ext = self.distribution.ext_status(ext)\n            if with_ext is not None and not with_ext:\n                continue\n            if with_cython:\n                print(f\"BUILDING CYTHON EXT; {self.include_dirs=} {self.library_dirs=} {self.define=}\")\n                ext.sources = self.cython_sources(ext.sources, ext)\n            try:\n                self.build_extension(ext)\n            except (CompileError, LinkError):\n                if with_ext is not None:\n                    raise\n                log.warn(\"Error compiling module, falling back to pure Python\")\n\n\nclass test(Command):\n\n    user_options = []\n\n    def initialize_options(self):\n        pass\n\n    def finalize_options(self):\n        pass\n\n    def run(self):\n        warnings.warn('Running tests via `setup.py test` is deprecated and will be removed in a future release. Use `pytest` instead to ensure that the complete test suite is run.', DeprecationWarning)\n        build_cmd = self.get_finalized_command('build')\n        build_cmd.run()\n\n        # running the tests this way can pollute the post-MANIFEST build sources\n        # (see https://github.com/yaml/pyyaml/issues/527#issuecomment-921058344)\n        # until we remove the test command, run tests from an ephemeral copy of the intermediate build sources\n        tempdir = tempfile.TemporaryDirectory(prefix='test_pyyaml')\n\n        try:\n            # have to create a subdir since we don't get dir_exists_ok on copytree until 3.8\n            temp_test_path = pathlib.Path(tempdir.name) / 'pyyaml'\n            shutil.copytree(build_cmd.build_lib, temp_test_path)\n            sys.path.insert(0, str(temp_test_path))\n            sys.path.insert(0, 'tests/legacy_tests')\n\n            import test_all\n            if not test_all.main([]):\n                raise DistutilsError(\"Tests failed\")\n        finally:\n            try:\n                # this can fail under Windows; best-effort cleanup\n                tempdir.cleanup()\n            except Exception:\n                pass\n\n\ncmdclass = {\n    'build_ext': build_ext,\n    'test': test,\n}\nif bdist_wheel:\n    cmdclass['bdist_wheel'] = bdist_wheel\n\n\nif __name__ == '__main__':\n\n    setup(\n        name=NAME,\n        version=VERSION,\n        description=DESCRIPTION,\n        long_description=LONG_DESCRIPTION,\n        author=AUTHOR,\n        author_email=AUTHOR_EMAIL,\n        license=LICENSE,\n        platforms=PLATFORMS,\n        url=URL,\n        download_url=DOWNLOAD_URL,\n        classifiers=CLASSIFIERS,\n        project_urls=PROJECT_URLS,\n\n        package_dir={'': 'lib'},\n        packages=['yaml', '_yaml'],\n        ext_modules=[\n            Extension('yaml._yaml', ['yaml/_yaml.pyx'],\n                'libyaml', \"LibYAML bindings\", LIBYAML_CHECK,\n                libraries=['yaml']),\n        ],\n\n        distclass=Distribution,\n        cmdclass=cmdclass,\n        python_requires='>=3.6',\n    )\n", ".github/actions/dynamatrix/matrix_yaml_to_json.py": "from __future__ import annotations\n\nimport argparse\nimport json\nimport os\nimport pathlib\nimport sys\nimport typing as t\nimport yaml\n\nfrom collections.abc import MutableMapping, Sequence\n\nskipped_entries = []\n\ndef _filter_omit_entries(value):\n    if isinstance(value, MutableMapping):\n        if (omit_value := value.pop('omit', ...)) is not ...:\n            if omit_value is True or str(omit_value).lower().strip() == 'true':\n                print(f'omitting {value} from matrix')\n                skipped_entries.append(value)\n                return ...\n\n        return {k: v for k, v in ((k, _filter_omit_entries(v)) for k, v in value.items()) if v is not ...}\n\n    if isinstance(value, str):\n        return value\n\n    if isinstance(value, Sequence):\n        return [v for v in (_filter_omit_entries(v) for v in value) if v is not ...]\n\n    return value\n\ndef main():\n    p = argparse.ArgumentParser(description='GHA YAML matrix filter')\n    required_grp = p.add_mutually_exclusive_group(required=True)\n    required_grp.add_argument('--from-stdin', action='store_true', help='read input YAML from stdin')\n    required_grp.add_argument('--from-file', type=pathlib.Path, help='read input YAML from file path')\n\n    args = p.parse_args()\n\n    path: pathlib.Path | None\n\n    matrix_yaml: str\n\n    if path := args.from_file:\n        matrix_yaml = path.read_text()\n    elif args.from_stdin:\n        matrix_yaml = sys.stdin.read()\n    else:\n        raise Exception('no source provided for matrix yaml')\n\n    raw_matrix = yaml.safe_load(matrix_yaml)\n    filtered_matrix = _filter_omit_entries(raw_matrix)\n\n    output_matrix_json = json.dumps(filtered_matrix)\n    output_skipped_matrix_json = json.dumps(skipped_entries)\n\n    print(f'filtered matrix: {output_matrix_json}')\n    print(f'skipped entries: {output_skipped_matrix_json}')\n\n    if (gh_output := os.environ.get('GITHUB_OUTPUT')):\n        print('setting step output var matrix_json; skipped_matrix_json...')\n        with pathlib.Path(gh_output).open('a') as env_fd:\n            env_fd.write(f'matrix_json<<__MATRIX_EOF\\n{output_matrix_json}\\n__MATRIX_EOF\\n')\n            env_fd.write(f'skipped_matrix_json<<__MATRIX_EOF\\n{output_skipped_matrix_json}\\n__MATRIX_EOF\\n')\n    else:\n        print(\"GITHUB_OUTPUT not set; skipping variable output\")\n\n\nif __name__ == '__main__':\n    main()\n", "tests/test_dump_load.py": "import pytest\nimport yaml\n\n\ndef test_dump():\n    assert yaml.dump(['foo'])\n\n\ndef test_load_no_loader():\n    with pytest.raises(TypeError):\n        yaml.load(\"- foo\\n\")\n\n\ndef test_load_safeloader():\n    assert yaml.load(\"- foo\\n\", Loader=yaml.SafeLoader)\n", "tests/legacy_tests/test_structure.py": "\nimport yaml, canonical\nimport pprint\n\ndef _convert_structure(loader):\n    if loader.check_event(yaml.ScalarEvent):\n        event = loader.get_event()\n        if event.tag or event.anchor or event.value:\n            return True\n        else:\n            return None\n    elif loader.check_event(yaml.SequenceStartEvent):\n        loader.get_event()\n        sequence = []\n        while not loader.check_event(yaml.SequenceEndEvent):\n            sequence.append(_convert_structure(loader))\n        loader.get_event()\n        return sequence\n    elif loader.check_event(yaml.MappingStartEvent):\n        loader.get_event()\n        mapping = []\n        while not loader.check_event(yaml.MappingEndEvent):\n            key = _convert_structure(loader)\n            value = _convert_structure(loader)\n            mapping.append((key, value))\n        loader.get_event()\n        return mapping\n    elif loader.check_event(yaml.AliasEvent):\n        loader.get_event()\n        return '*'\n    else:\n        loader.get_event()\n        return '?'\n\ndef test_structure(data_filename, structure_filename, verbose=False):\n    nodes1 = []\n    with open(structure_filename, 'r') as file:\n        nodes2 = eval(file.read())\n    try:\n        with open(data_filename, 'rb') as file:\n            loader = yaml.Loader(file)\n            while loader.check_event():\n                if loader.check_event(\n                    yaml.StreamStartEvent, yaml.StreamEndEvent,\n                    yaml.DocumentStartEvent, yaml.DocumentEndEvent\n                ):\n                    loader.get_event()\n                    continue\n                nodes1.append(_convert_structure(loader))\n        if len(nodes1) == 1:\n            nodes1 = nodes1[0]\n        assert nodes1 == nodes2, (nodes1, nodes2)\n    finally:\n        if verbose:\n            print(\"NODES1:\")\n            pprint.pprint(nodes1)\n            print(\"NODES2:\")\n            pprint.pprint(nodes2)\n\ntest_structure.unittest = ['.data', '.structure']\n\ndef _compare_events(events1, events2, full=False):\n    assert len(events1) == len(events2), (len(events1), len(events2))\n    for event1, event2 in zip(events1, events2):\n        assert event1.__class__ == event2.__class__, (event1, event2)\n        if isinstance(event1, yaml.AliasEvent) and full:\n            assert event1.anchor == event2.anchor, (event1, event2)\n        if isinstance(event1, (yaml.ScalarEvent, yaml.CollectionStartEvent)):\n            if (event1.tag not in [None, '!'] and event2.tag not in [None, '!']) or full:\n                assert event1.tag == event2.tag, (event1, event2)\n        if isinstance(event1, yaml.ScalarEvent):\n            assert event1.value == event2.value, (event1, event2)\n\ndef test_parser(data_filename, canonical_filename, verbose=False):\n    events1 = None\n    events2 = None\n    try:\n        with open(data_filename, 'rb') as file:\n            events1 = list(yaml.parse(file))\n        with open(canonical_filename, 'rb') as file:\n            events2 = list(yaml.canonical_parse(file))\n        _compare_events(events1, events2)\n    finally:\n        if verbose:\n            print(\"EVENTS1:\")\n            pprint.pprint(events1)\n            print(\"EVENTS2:\")\n            pprint.pprint(events2)\n\ntest_parser.unittest = ['.data', '.canonical']\n\ndef test_parser_on_canonical(canonical_filename, verbose=False):\n    events1 = None\n    events2 = None\n    try:\n        with open(canonical_filename, 'rb') as file:\n            events1 = list(yaml.parse(file))\n        with open(canonical_filename, 'rb') as file:\n            events2 = list(yaml.canonical_parse(file))\n        _compare_events(events1, events2, full=True)\n    finally:\n        if verbose:\n            print(\"EVENTS1:\")\n            pprint.pprint(events1)\n            print(\"EVENTS2:\")\n            pprint.pprint(events2)\n\ntest_parser_on_canonical.unittest = ['.canonical']\n\ndef _compare_nodes(node1, node2):\n    assert node1.__class__ == node2.__class__, (node1, node2)\n    assert node1.tag == node2.tag, (node1, node2)\n    if isinstance(node1, yaml.ScalarNode):\n        assert node1.value == node2.value, (node1, node2)\n    else:\n        assert len(node1.value) == len(node2.value), (node1, node2)\n        for item1, item2 in zip(node1.value, node2.value):\n            if not isinstance(item1, tuple):\n                item1 = (item1,)\n                item2 = (item2,)\n            for subnode1, subnode2 in zip(item1, item2):\n                _compare_nodes(subnode1, subnode2)\n\ndef test_composer(data_filename, canonical_filename, verbose=False):\n    nodes1 = None\n    nodes2 = None\n    try:\n        with open(data_filename, 'rb') as file:\n            nodes1 = list(yaml.compose_all(file))\n        with open(canonical_filename, 'rb') as file:\n            nodes2 = list(yaml.canonical_compose_all(file))\n        assert len(nodes1) == len(nodes2), (len(nodes1), len(nodes2))\n        for node1, node2 in zip(nodes1, nodes2):\n            _compare_nodes(node1, node2)\n    finally:\n        if verbose:\n            print(\"NODES1:\")\n            pprint.pprint(nodes1)\n            print(\"NODES2:\")\n            pprint.pprint(nodes2)\n\ntest_composer.unittest = ['.data', '.canonical']\n\ndef _make_loader():\n    global MyLoader\n\n    class MyLoader(yaml.Loader):\n        def construct_sequence(self, node):\n            return tuple(yaml.Loader.construct_sequence(self, node))\n        def construct_mapping(self, node):\n            pairs = self.construct_pairs(node)\n            pairs.sort(key=(lambda i: str(i)))\n            return pairs\n        def construct_undefined(self, node):\n            return self.construct_scalar(node)\n\n    MyLoader.add_constructor('tag:yaml.org,2002:map', MyLoader.construct_mapping)\n    MyLoader.add_constructor(None, MyLoader.construct_undefined)\n\ndef _make_canonical_loader():\n    global MyCanonicalLoader\n\n    class MyCanonicalLoader(yaml.CanonicalLoader):\n        def construct_sequence(self, node):\n            return tuple(yaml.CanonicalLoader.construct_sequence(self, node))\n        def construct_mapping(self, node):\n            pairs = self.construct_pairs(node)\n            pairs.sort(key=(lambda i: str(i)))\n            return pairs\n        def construct_undefined(self, node):\n            return self.construct_scalar(node)\n\n    MyCanonicalLoader.add_constructor('tag:yaml.org,2002:map', MyCanonicalLoader.construct_mapping)\n    MyCanonicalLoader.add_constructor(None, MyCanonicalLoader.construct_undefined)\n\ndef test_constructor(data_filename, canonical_filename, verbose=False):\n    _make_loader()\n    _make_canonical_loader()\n    native1 = None\n    native2 = None\n    try:\n        with open(data_filename, 'rb') as file:\n            native1 = list(yaml.load_all(file, Loader=MyLoader))\n        with open(canonical_filename, 'rb') as file:\n            native2 = list(yaml.load_all(file, Loader=MyCanonicalLoader))\n        assert native1 == native2, (native1, native2)\n    finally:\n        if verbose:\n            print(\"NATIVE1:\")\n            pprint.pprint(native1)\n            print(\"NATIVE2:\")\n            pprint.pprint(native2)\n\ntest_constructor.unittest = ['.data', '.canonical']\n\nif __name__ == '__main__':\n    import test_appliance\n    test_appliance.run(globals())\n\n", "tests/legacy_tests/test_errors.py": "\nimport yaml, test_emitter\n\ndef test_loader_error(error_filename, verbose=False):\n    try:\n        with open(error_filename, 'rb') as file:\n            list(yaml.load_all(file, yaml.FullLoader))\n    except yaml.YAMLError as exc:\n        if verbose:\n            print(\"%s:\" % exc.__class__.__name__, exc)\n    else:\n        raise AssertionError(\"expected an exception\")\n\ntest_loader_error.unittest = ['.loader-error']\n\ndef test_loader_error_string(error_filename, verbose=False):\n    try:\n        with open(error_filename, 'rb') as file:\n            list(yaml.load_all(file.read(), yaml.FullLoader))\n    except yaml.YAMLError as exc:\n        if verbose:\n            print(\"%s:\" % exc.__class__.__name__, exc)\n    else:\n        raise AssertionError(\"expected an exception\")\n\ntest_loader_error_string.unittest = ['.loader-error']\n\ndef test_loader_error_single(error_filename, verbose=False):\n    try:\n        with open(error_filename, 'rb') as file:\n            yaml.load(file.read(), yaml.FullLoader)\n    except yaml.YAMLError as exc:\n        if verbose:\n            print(\"%s:\" % exc.__class__.__name__, exc)\n    else:\n        raise AssertionError(\"expected an exception\")\n\ntest_loader_error_single.unittest = ['.single-loader-error']\n\ndef test_emitter_error(error_filename, verbose=False):\n    with open(error_filename, 'rb') as file:\n        events = list(yaml.load(file, Loader=test_emitter.EventsLoader))\n    try:\n        yaml.emit(events)\n    except yaml.YAMLError as exc:\n        if verbose:\n            print(\"%s:\" % exc.__class__.__name__, exc)\n    else:\n        raise AssertionError(\"expected an exception\")\n\ntest_emitter_error.unittest = ['.emitter-error']\n\ndef test_dumper_error(error_filename, verbose=False):\n    with open(error_filename, 'rb') as file:\n        code = file.read()\n    try:\n        import yaml\n        from io import StringIO\n        exec(code)\n    except yaml.YAMLError as exc:\n        if verbose:\n            print(\"%s:\" % exc.__class__.__name__, exc)\n    else:\n        raise AssertionError(\"expected an exception\")\n\ntest_dumper_error.unittest = ['.dumper-error']\n\nif __name__ == '__main__':\n    import test_appliance\n    test_appliance.run(globals())\n\n", "tests/legacy_tests/test_mark.py": "\nimport yaml\n\ndef test_marks(marks_filename, verbose=False):\n    with open(marks_filename, 'r') as file:\n        inputs = file.read().split('---\\n')[1:]\n    for input in inputs:\n        index = 0\n        line = 0\n        column = 0\n        while input[index] != '*':\n            if input[index] == '\\n':\n                line += 1\n                column = 0\n            else:\n                column += 1\n            index += 1\n        mark = yaml.Mark(marks_filename, index, line, column, input, index)\n        snippet = mark.get_snippet(indent=2, max_length=79)\n        if verbose:\n            print(snippet)\n        assert isinstance(snippet, str), type(snippet)\n        assert snippet.count('\\n') == 1, snippet.count('\\n')\n        data, pointer = snippet.split('\\n')\n        assert len(data) < 82, len(data)\n        assert data[len(pointer)-1] == '*', data[len(pointer)-1]\n\ntest_marks.unittest = ['.marks']\n\nif __name__ == '__main__':\n    import test_appliance\n    test_appliance.run(globals())\n\n", "tests/legacy_tests/test_constructor.py": "\nimport yaml\nimport pprint\n\nimport datetime\nimport yaml.tokens\n\n# Import any packages here that need to be referenced in .code files.\nimport signal\n\ndef execute(code):\n    global value\n    exec(code)\n    return value\n\ndef _make_objects():\n    global MyLoader, MyDumper, MyTestClass1, MyTestClass2, MyTestClass3, YAMLObject1, YAMLObject2,  \\\n            AnObject, AnInstance, AState, ACustomState, InitArgs, InitArgsWithState,    \\\n            NewArgs, NewArgsWithState, Reduce, ReduceWithState, Slots, MyInt, MyList, MyDict,  \\\n            FixedOffset, today, execute, MyFullLoader\n\n    class MyLoader(yaml.Loader):\n        pass\n    class MyDumper(yaml.Dumper):\n        pass\n\n    class MyTestClass1:\n        def __init__(self, x, y=0, z=0):\n            self.x = x\n            self.y = y\n            self.z = z\n        def __eq__(self, other):\n            if isinstance(other, MyTestClass1):\n                return self.__class__, self.__dict__ == other.__class__, other.__dict__\n            else:\n                return False\n\n    def construct1(constructor, node):\n        mapping = constructor.construct_mapping(node)\n        return MyTestClass1(**mapping)\n    def represent1(representer, native):\n        return representer.represent_mapping(\"!tag1\", native.__dict__)\n\n    def my_time_constructor(constructor, node):\n        seq = constructor.construct_sequence(node)\n        dt = seq[0]\n        tz = None\n        try:\n            tz = dt.tzinfo.tzname(dt)\n        except:\n            pass\n        return [dt, tz]\n\n    yaml.add_constructor(\"!tag1\", construct1, Loader=MyLoader)\n    yaml.add_constructor(\"!MyTime\", my_time_constructor, Loader=MyLoader)\n    yaml.add_representer(MyTestClass1, represent1, Dumper=MyDumper)\n\n    class MyTestClass2(MyTestClass1, yaml.YAMLObject):\n        yaml_loader = MyLoader\n        yaml_dumper = MyDumper\n        yaml_tag = \"!tag2\"\n        def from_yaml(cls, constructor, node):\n            x = constructor.construct_yaml_int(node)\n            return cls(x=x)\n        from_yaml = classmethod(from_yaml)\n        def to_yaml(cls, representer, native):\n            return representer.represent_scalar(cls.yaml_tag, str(native.x))\n        to_yaml = classmethod(to_yaml)\n\n    class MyTestClass3(MyTestClass2):\n        yaml_tag = \"!tag3\"\n        def from_yaml(cls, constructor, node):\n            mapping = constructor.construct_mapping(node)\n            if '=' in mapping:\n                x = mapping['=']\n                del mapping['=']\n                mapping['x'] = x\n            return cls(**mapping)\n        from_yaml = classmethod(from_yaml)\n        def to_yaml(cls, representer, native):\n            return representer.represent_mapping(cls.yaml_tag, native.__dict__)\n        to_yaml = classmethod(to_yaml)\n\n    class YAMLObject1(yaml.YAMLObject):\n        yaml_loader = MyLoader\n        yaml_dumper = MyDumper\n        yaml_tag = '!foo'\n        def __init__(self, my_parameter=None, my_another_parameter=None):\n            self.my_parameter = my_parameter\n            self.my_another_parameter = my_another_parameter\n        def __eq__(self, other):\n            if isinstance(other, YAMLObject1):\n                return self.__class__, self.__dict__ == other.__class__, other.__dict__\n            else:\n                return False\n\n    class YAMLObject2(yaml.YAMLObject):\n        yaml_loader = MyLoader\n        yaml_dumper = MyDumper\n        yaml_tag = '!bar'\n        def __init__(self, foo=1, bar=2, baz=3):\n            self.foo = foo\n            self.bar = bar\n            self.baz = baz\n        def __getstate__(self):\n            return {1: self.foo, 2: self.bar, 3: self.baz}\n        def __setstate__(self, state):\n            self.foo = state[1]\n            self.bar = state[2]\n            self.baz = state[3]\n        def __eq__(self, other):\n            if isinstance(other, YAMLObject2):\n                return self.__class__, self.__dict__ == other.__class__, other.__dict__\n            else:\n                return False\n\n    class AnObject:\n        def __new__(cls, foo=None, bar=None, baz=None):\n            self = object.__new__(cls)\n            self.foo = foo\n            self.bar = bar\n            self.baz = baz\n            return self\n        def __cmp__(self, other):\n            return cmp((type(self), self.foo, self.bar, self.baz),\n                    (type(other), other.foo, other.bar, other.baz))\n        def __eq__(self, other):\n            return type(self) is type(other) and    \\\n                    (self.foo, self.bar, self.baz) == (other.foo, other.bar, other.baz)\n\n    class AnInstance:\n        def __init__(self, foo=None, bar=None, baz=None):\n            self.foo = foo\n            self.bar = bar\n            self.baz = baz\n        def __cmp__(self, other):\n            return cmp((type(self), self.foo, self.bar, self.baz),\n                    (type(other), other.foo, other.bar, other.baz))\n        def __eq__(self, other):\n            return type(self) is type(other) and    \\\n                    (self.foo, self.bar, self.baz) == (other.foo, other.bar, other.baz)\n\n    class AState(AnInstance):\n        def __getstate__(self):\n            return {\n                '_foo': self.foo,\n                '_bar': self.bar,\n                '_baz': self.baz,\n            }\n        def __setstate__(self, state):\n            self.foo = state['_foo']\n            self.bar = state['_bar']\n            self.baz = state['_baz']\n\n    class ACustomState(AnInstance):\n        def __getstate__(self):\n            return (self.foo, self.bar, self.baz)\n        def __setstate__(self, state):\n            self.foo, self.bar, self.baz = state\n\n    class NewArgs(AnObject):\n        def __getnewargs__(self):\n            return (self.foo, self.bar, self.baz)\n        def __getstate__(self):\n            return {}\n\n    class NewArgsWithState(AnObject):\n        def __getnewargs__(self):\n            return (self.foo, self.bar)\n        def __getstate__(self):\n            return self.baz\n        def __setstate__(self, state):\n            self.baz = state\n\n    InitArgs = NewArgs\n\n    InitArgsWithState = NewArgsWithState\n\n    class Reduce(AnObject):\n        def __reduce__(self):\n            return self.__class__, (self.foo, self.bar, self.baz)\n\n    class ReduceWithState(AnObject):\n        def __reduce__(self):\n            return self.__class__, (self.foo, self.bar), self.baz\n        def __setstate__(self, state):\n            self.baz = state\n\n    class Slots:\n        __slots__ = (\"foo\", \"bar\", \"baz\")\n        def __init__(self, foo=None, bar=None, baz=None):\n            self.foo = foo\n            self.bar = bar\n            self.baz = baz\n\n        def __eq__(self, other):\n            return type(self) is type(other) and \\\n                (self.foo, self.bar, self.baz) == (other.foo, other.bar, other.baz)\n\n    class MyInt(int):\n        def __eq__(self, other):\n            return type(self) is type(other) and int(self) == int(other)\n\n    class MyList(list):\n        def __init__(self, n=1):\n            self.extend([None]*n)\n        def __eq__(self, other):\n            return type(self) is type(other) and list(self) == list(other)\n\n    class MyDict(dict):\n        def __init__(self, n=1):\n            for k in range(n):\n                self[k] = None\n        def __eq__(self, other):\n            return type(self) is type(other) and dict(self) == dict(other)\n\n    class FixedOffset(datetime.tzinfo):\n        def __init__(self, offset, name):\n            self.__offset = datetime.timedelta(minutes=offset)\n            self.__name = name\n        def utcoffset(self, dt):\n            return self.__offset\n        def tzname(self, dt):\n            return self.__name\n        def dst(self, dt):\n            return datetime.timedelta(0)\n\n    class MyFullLoader(yaml.FullLoader):\n        def get_state_keys_blacklist(self):\n            return super().get_state_keys_blacklist() + ['^mymethod$', '^wrong_.*$']\n\n    today = datetime.date.today()\n\ndef _load_code(expression):\n    return eval(expression)\n\ndef _serialize_value(data):\n    if isinstance(data, list):\n        return '[%s]' % ', '.join(map(_serialize_value, data))\n    elif isinstance(data, dict):\n        items = []\n        for key, value in data.items():\n            key = _serialize_value(key)\n            value = _serialize_value(value)\n            items.append(\"%s: %s\" % (key, value))\n        items.sort()\n        return '{%s}' % ', '.join(items)\n    elif isinstance(data, datetime.datetime):\n        return repr(data.utctimetuple())\n    elif isinstance(data, float) and data != data:\n        return '?'\n    else:\n        return str(data)\n\ndef test_constructor_types(data_filename, code_filename, verbose=False):\n    _make_objects()\n    native1 = None\n    native2 = None\n    try:\n        with open(data_filename, 'rb') as file:\n            native1 = list(yaml.load_all(file, Loader=MyLoader))\n        if len(native1) == 1:\n            native1 = native1[0]\n        with open(code_filename, 'rb') as file:\n            native2 = _load_code(file.read())\n        try:\n            if native1 == native2:\n                return\n        except TypeError:\n            pass\n        if verbose:\n            print(\"SERIALIZED NATIVE1:\")\n            print(_serialize_value(native1))\n            print(\"SERIALIZED NATIVE2:\")\n            print(_serialize_value(native2))\n        assert _serialize_value(native1) == _serialize_value(native2), (native1, native2)\n    finally:\n        if verbose:\n            print(\"NATIVE1:\")\n            pprint.pprint(native1)\n            print(\"NATIVE2:\")\n            pprint.pprint(native2)\n\ntest_constructor_types.unittest = ['.data', '.code']\n\ndef test_subclass_blacklist_types(data_filename, verbose=False):\n    _make_objects()\n    try:\n        with open(data_filename, 'rb') as file:\n            yaml.load(file.read(), MyFullLoader)\n    except yaml.YAMLError as exc:\n        if verbose:\n            print(\"%s:\" % exc.__class__.__name__, exc)\n    else:\n        raise AssertionError(\"expected an exception\")\n\ntest_subclass_blacklist_types.unittest = ['.subclass_blacklist']\n\nif __name__ == '__main__':\n    import sys, test_constructor\n    sys.modules['test_constructor'] = sys.modules['__main__']\n    import test_appliance\n    test_appliance.run(globals())\n\n", "tests/legacy_tests/test_representer.py": "\nimport yaml\nimport test_constructor\nimport pprint\n\ndef test_representer_types(code_filename, verbose=False):\n    test_constructor._make_objects()\n    for allow_unicode in [False, True]:\n        for encoding in ['utf-8', 'utf-16-be', 'utf-16-le']:\n            with open(code_filename, 'rb') as file:\n                native1 = test_constructor._load_code(file.read())\n            native2 = None\n            try:\n                output = yaml.dump(native1, Dumper=test_constructor.MyDumper,\n                            allow_unicode=allow_unicode, encoding=encoding)\n                native2 = yaml.load(output, Loader=test_constructor.MyLoader)\n                try:\n                    if native1 == native2:\n                        continue\n                except TypeError:\n                    pass\n                value1 = test_constructor._serialize_value(native1)\n                value2 = test_constructor._serialize_value(native2)\n                if verbose:\n                    print(\"SERIALIZED NATIVE1:\")\n                    print(value1)\n                    print(\"SERIALIZED NATIVE2:\")\n                    print(value2)\n                assert value1 == value2, (native1, native2)\n            finally:\n                if verbose:\n                    print(\"NATIVE1:\")\n                    pprint.pprint(native1)\n                    print(\"NATIVE2:\")\n                    pprint.pprint(native2)\n                    print(\"OUTPUT:\")\n                    print(output)\n\ntest_representer_types.unittest = ['.code']\n\nif __name__ == '__main__':\n    import test_appliance\n    test_appliance.run(globals())\n\n", "tests/legacy_tests/test_canonical.py": "\nimport yaml, canonical\n\ndef test_canonical_scanner(canonical_filename, verbose=False):\n    with open(canonical_filename, 'rb') as file:\n        data = file.read()\n    tokens = list(yaml.canonical_scan(data))\n    assert tokens, tokens\n    if verbose:\n        for token in tokens:\n            print(token)\n\ntest_canonical_scanner.unittest = ['.canonical']\n\ndef test_canonical_parser(canonical_filename, verbose=False):\n    with open(canonical_filename, 'rb') as file:\n        data = file.read()\n    events = list(yaml.canonical_parse(data))\n    assert events, events\n    if verbose:\n        for event in events:\n            print(event)\n\ntest_canonical_parser.unittest = ['.canonical']\n\ndef test_canonical_error(data_filename, canonical_filename, verbose=False):\n    with open(data_filename, 'rb') as file:\n        data = file.read()\n    try:\n        output = list(yaml.canonical_load_all(data))\n    except yaml.YAMLError as exc:\n        if verbose:\n            print(exc)\n    else:\n        raise AssertionError(\"expected an exception\")\n\ntest_canonical_error.unittest = ['.data', '.canonical']\ntest_canonical_error.skip = ['.empty']\n\nif __name__ == '__main__':\n    import test_appliance\n    test_appliance.run(globals())\n\n", "tests/legacy_tests/test_emitter.py": "\nimport yaml\n\ndef _compare_events(events1, events2):\n    assert len(events1) == len(events2), (events1, events2)\n    for event1, event2 in zip(events1, events2):\n        assert event1.__class__ == event2.__class__, (event1, event2)\n        if isinstance(event1, yaml.NodeEvent):\n            assert event1.anchor == event2.anchor, (event1, event2)\n        if isinstance(event1, yaml.CollectionStartEvent):\n            assert event1.tag == event2.tag, (event1, event2)\n        if isinstance(event1, yaml.ScalarEvent):\n            if True not in event1.implicit+event2.implicit:\n                assert event1.tag == event2.tag, (event1, event2)\n            assert event1.value == event2.value, (event1, event2)\n\ndef test_emitter_on_data(data_filename, canonical_filename, verbose=False):\n    with open(data_filename, 'rb') as file:\n        events = list(yaml.parse(file))\n    output = yaml.emit(events)\n    if verbose:\n        print(\"OUTPUT:\")\n        print(output)\n    new_events = list(yaml.parse(output))\n    _compare_events(events, new_events)\n\ntest_emitter_on_data.unittest = ['.data', '.canonical']\n\ndef test_emitter_on_canonical(canonical_filename, verbose=False):\n    with open(canonical_filename, 'rb') as file:\n        events = list(yaml.parse(file))\n    for canonical in [False, True]:\n        output = yaml.emit(events, canonical=canonical)\n        if verbose:\n            print(\"OUTPUT (canonical=%s):\" % canonical)\n            print(output)\n        new_events = list(yaml.parse(output))\n        _compare_events(events, new_events)\n\ntest_emitter_on_canonical.unittest = ['.canonical']\n\ndef test_emitter_styles(data_filename, canonical_filename, verbose=False):\n    for filename in [data_filename, canonical_filename]:\n        with open(filename, 'rb') as file:\n            events = list(yaml.parse(file))\n        for flow_style in [False, True]:\n            for style in ['|', '>', '\"', '\\'', '']:\n                styled_events = []\n                for event in events:\n                    if isinstance(event, yaml.ScalarEvent):\n                        event = yaml.ScalarEvent(event.anchor, event.tag,\n                                event.implicit, event.value, style=style)\n                    elif isinstance(event, yaml.SequenceStartEvent):\n                        event = yaml.SequenceStartEvent(event.anchor, event.tag,\n                                event.implicit, flow_style=flow_style)\n                    elif isinstance(event, yaml.MappingStartEvent):\n                        event = yaml.MappingStartEvent(event.anchor, event.tag,\n                                event.implicit, flow_style=flow_style)\n                    styled_events.append(event)\n                output = yaml.emit(styled_events)\n                if verbose:\n                    print(\"OUTPUT (filename=%r, flow_style=%r, style=%r)\" % (filename, flow_style, style))\n                    print(output)\n                new_events = list(yaml.parse(output))\n                _compare_events(events, new_events)\n\ntest_emitter_styles.unittest = ['.data', '.canonical']\n\nclass EventsLoader(yaml.Loader):\n\n    def construct_event(self, node):\n        if isinstance(node, yaml.ScalarNode):\n            mapping = {}\n        else:\n            mapping = self.construct_mapping(node)\n        class_name = str(node.tag[1:])+'Event'\n        if class_name in ['AliasEvent', 'ScalarEvent', 'SequenceStartEvent', 'MappingStartEvent']:\n            mapping.setdefault('anchor', None)\n        if class_name in ['ScalarEvent', 'SequenceStartEvent', 'MappingStartEvent']:\n            mapping.setdefault('tag', None)\n        if class_name in ['SequenceStartEvent', 'MappingStartEvent']:\n            mapping.setdefault('implicit', True)\n        if class_name == 'ScalarEvent':\n            mapping.setdefault('implicit', (False, True))\n            mapping.setdefault('value', '')\n        value = getattr(yaml, class_name)(**mapping)\n        return value\n\nEventsLoader.add_constructor(None, EventsLoader.construct_event)\n\ndef test_emitter_events(events_filename, verbose=False):\n    with open(events_filename, 'rb') as file:\n        events = list(yaml.load(file, Loader=EventsLoader))\n    output = yaml.emit(events)\n    if verbose:\n        print(\"OUTPUT:\")\n        print(output)\n    new_events = list(yaml.parse(output))\n    _compare_events(events, new_events)\n\nif __name__ == '__main__':\n    import test_appliance\n    test_appliance.run(globals())\n\n", "tests/legacy_tests/test_sort_keys.py": "import yaml\nimport pprint\nimport sys\n\ndef test_sort_keys(input_filename, sorted_filename, verbose=False):\n    with open(input_filename, 'rb') as file:\n        input = file.read().decode('utf-8')\n    with open(sorted_filename, 'rb') as file:\n        sorted = file.read().decode('utf-8')\n    data = yaml.load(input, Loader=yaml.FullLoader)\n    dump_sorted = yaml.dump(data, default_flow_style=False, sort_keys=True)\n    dump_unsorted = yaml.dump(data, default_flow_style=False, sort_keys=False)\n    dump_unsorted_safe = yaml.dump(data, default_flow_style=False, sort_keys=False, Dumper=yaml.SafeDumper)\n    if verbose:\n        print(\"INPUT:\")\n        print(input)\n        print(\"DATA:\")\n        print(data)\n\n    assert dump_sorted == sorted\n\n    if sys.version_info>=(3,7):\n        assert dump_unsorted == input\n        assert dump_unsorted_safe == input\n\ntest_sort_keys.unittest = ['.sort', '.sorted']\n\nif __name__ == '__main__':\n    import test_appliance\n    test_appliance.run(globals())\n\n", "tests/legacy_tests/test_build_ext.py": "\n\nif __name__ == '__main__':\n    import sys, os, distutils.util\n    build_lib = 'build/lib'\n    build_lib_ext = os.path.join('build', 'lib.{}-{}.{}'.format(distutils.util.get_platform(), *sys.version_info))\n    sys.path.insert(0, build_lib)\n    sys.path.insert(0, build_lib_ext)\n    import test_yaml_ext, test_appliance\n    test_appliance.run(test_yaml_ext)\n\n", "tests/legacy_tests/test_appliance.py": "\nimport sys, os, os.path, types, traceback, pprint, pathlib\n\nDATA = str(pathlib.Path(__file__).parent / 'data')\n\n\ndef find_test_functions(collections):\n    if not isinstance(collections, list):\n        collections = [collections]\n    functions = []\n    for collection in collections:\n        if not isinstance(collection, dict):\n            collection = vars(collection)\n        for key in sorted(collection):\n            value = collection[key]\n            if isinstance(value, types.FunctionType) and hasattr(value, 'unittest'):\n                functions.append(value)\n    return functions\n\n\ndef find_test_filenames(directory):\n    filenames = {}\n    for filename in os.listdir(directory):\n        if os.path.isfile(os.path.join(directory, filename)):\n            base, ext = os.path.splitext(filename)\n            if base.endswith('-py2'):\n                continue\n            filenames.setdefault(base, []).append(ext)\n    filenames = sorted(filenames.items())\n    return filenames\n\ndef parse_arguments(args):\n    if args is None:\n        args = sys.argv[1:]\n    verbose = False\n    if '-v' in args:\n        verbose = True\n        args.remove('-v')\n    if '--verbose' in args:\n        verbose = True\n        args.remove('--verbose')\n    if 'YAML_TEST_VERBOSE' in os.environ:\n        verbose = True\n    include_functions = []\n    if args:\n        include_functions.append(args.pop(0))\n    if 'YAML_TEST_FUNCTIONS' in os.environ:\n        include_functions.extend(os.environ['YAML_TEST_FUNCTIONS'].split())\n    include_filenames = []\n    include_filenames.extend(args)\n    if 'YAML_TEST_FILENAMES' in os.environ:\n        include_filenames.extend(os.environ['YAML_TEST_FILENAMES'].split())\n    return include_functions, include_filenames, verbose\n\ndef execute(function, filenames, verbose):\n    name = function.__name__\n    if verbose:\n        sys.stdout.write('='*75+'\\n')\n        sys.stdout.write('%s(%s)...\\n' % (name, ', '.join(filenames)))\n    try:\n        function(verbose=verbose, *filenames)\n    except Exception as exc:\n        info = sys.exc_info()\n        if isinstance(exc, AssertionError):\n            kind = 'FAILURE'\n        else:\n            kind = 'ERROR'\n        if verbose:\n            traceback.print_exc(limit=1, file=sys.stdout)\n        else:\n            sys.stdout.write(kind[0])\n            sys.stdout.flush()\n    else:\n        kind = 'SUCCESS'\n        info = None\n        if not verbose:\n            sys.stdout.write('.')\n    sys.stdout.flush()\n    return (name, filenames, kind, info)\n\ndef display(results, verbose):\n    if results and not verbose:\n        sys.stdout.write('\\n')\n    total = len(results)\n    failures = 0\n    errors = 0\n    for name, filenames, kind, info in results:\n        if kind == 'SUCCESS':\n            continue\n        if kind == 'FAILURE':\n            failures += 1\n        if kind == 'ERROR':\n            errors += 1\n        sys.stdout.write('='*75+'\\n')\n        sys.stdout.write('%s(%s): %s\\n' % (name, ', '.join(filenames), kind))\n        if kind == 'ERROR':\n            traceback.print_exception(file=sys.stdout, *info)\n        else:\n            sys.stdout.write('Traceback (most recent call last):\\n')\n            traceback.print_tb(info[2], file=sys.stdout)\n            sys.stdout.write('%s: see below\\n' % info[0].__name__)\n            sys.stdout.write('~'*75+'\\n')\n            for arg in info[1].args:\n                pprint.pprint(arg, stream=sys.stdout)\n        for filename in filenames:\n            sys.stdout.write('-'*75+'\\n')\n            sys.stdout.write('%s:\\n' % filename)\n            with open(filename, 'r', errors='replace') as file:\n                data = file.read()\n            sys.stdout.write(data)\n            if data and data[-1] != '\\n':\n                sys.stdout.write('\\n')\n    sys.stdout.write('='*75+'\\n')\n    sys.stdout.write('TESTS: %s\\n' % total)\n    if failures:\n        sys.stdout.write('FAILURES: %s\\n' % failures)\n    if errors:\n        sys.stdout.write('ERRORS: %s\\n' % errors)\n    return not (failures or errors)\n\ndef run(collections, args=None):\n    test_functions = find_test_functions(collections)\n    test_filenames = find_test_filenames(DATA)\n    include_functions, include_filenames, verbose = parse_arguments(args)\n    results = []\n    for function in test_functions:\n        if include_functions and function.__name__ not in include_functions:\n            continue\n        if function.unittest and function.unittest is not True:\n            for base, exts in test_filenames:\n                if include_filenames and base not in include_filenames:\n                    continue\n                filenames = []\n                for ext in function.unittest:\n                    if ext not in exts:\n                        break\n                    filenames.append(os.path.join(DATA, base+ext))\n                else:\n                    skip_exts = getattr(function, 'skip', [])\n                    for skip_ext in skip_exts:\n                        if skip_ext in exts:\n                            break\n                    else:\n                        result = execute(function, filenames, verbose)\n                        results.append(result)\n        else:\n            result = execute(function, [], verbose)\n            results.append(result)\n    return display(results, verbose=verbose)\n\n", "tests/legacy_tests/test_yaml_ext.py": "import yaml._yaml, yaml\nimport types, pprint, tempfile, sys, os\n\nyaml.PyBaseLoader = yaml.BaseLoader\nyaml.PySafeLoader = yaml.SafeLoader\nyaml.PyLoader = yaml.Loader\nyaml.PyBaseDumper = yaml.BaseDumper\nyaml.PySafeDumper = yaml.SafeDumper\nyaml.PyDumper = yaml.Dumper\n\nold_scan = yaml.scan\ndef new_scan(stream, Loader=yaml.CLoader):\n    return old_scan(stream, Loader)\n\nold_parse = yaml.parse\ndef new_parse(stream, Loader=yaml.CLoader):\n    return old_parse(stream, Loader)\n\nold_compose = yaml.compose\ndef new_compose(stream, Loader=yaml.CLoader):\n    return old_compose(stream, Loader)\n\nold_compose_all = yaml.compose_all\ndef new_compose_all(stream, Loader=yaml.CLoader):\n    return old_compose_all(stream, Loader)\n\nold_load = yaml.load\ndef new_load(stream, Loader=yaml.CLoader):\n    return old_load(stream, Loader)\n\nold_load_all = yaml.load_all\ndef new_load_all(stream, Loader=yaml.CLoader):\n    return old_load_all(stream, Loader)\n\nold_safe_load = yaml.safe_load\ndef new_safe_load(stream):\n    return old_load(stream, yaml.CSafeLoader)\n\nold_safe_load_all = yaml.safe_load_all\ndef new_safe_load_all(stream):\n    return old_load_all(stream, yaml.CSafeLoader)\n\nold_emit = yaml.emit\ndef new_emit(events, stream=None, Dumper=yaml.CDumper, **kwds):\n    return old_emit(events, stream, Dumper, **kwds)\n\nold_serialize = yaml.serialize\ndef new_serialize(node, stream, Dumper=yaml.CDumper, **kwds):\n    return old_serialize(node, stream, Dumper, **kwds)\n\nold_serialize_all = yaml.serialize_all\ndef new_serialize_all(nodes, stream=None, Dumper=yaml.CDumper, **kwds):\n    return old_serialize_all(nodes, stream, Dumper, **kwds)\n\nold_dump = yaml.dump\ndef new_dump(data, stream=None, Dumper=yaml.CDumper, **kwds):\n    return old_dump(data, stream, Dumper, **kwds)\n\nold_dump_all = yaml.dump_all\ndef new_dump_all(documents, stream=None, Dumper=yaml.CDumper, **kwds):\n    return old_dump_all(documents, stream, Dumper, **kwds)\n\nold_safe_dump = yaml.safe_dump\ndef new_safe_dump(data, stream=None, **kwds):\n    return old_dump(data, stream, yaml.CSafeDumper, **kwds)\n\nold_safe_dump_all = yaml.safe_dump_all\ndef new_safe_dump_all(documents, stream=None, **kwds):\n    return old_dump_all(documents, stream, yaml.CSafeDumper, **kwds)\n\ndef _set_up():\n    yaml.BaseLoader = yaml.CBaseLoader\n    yaml.SafeLoader = yaml.CSafeLoader\n    yaml.Loader = yaml.CLoader\n    yaml.BaseDumper = yaml.CBaseDumper\n    yaml.SafeDumper = yaml.CSafeDumper\n    yaml.Dumper = yaml.CDumper\n    yaml.scan = new_scan\n    yaml.parse = new_parse\n    yaml.compose = new_compose\n    yaml.compose_all = new_compose_all\n    yaml.load = new_load\n    yaml.load_all = new_load_all\n    yaml.safe_load = new_safe_load\n    yaml.safe_load_all = new_safe_load_all\n    yaml.emit = new_emit\n    yaml.serialize = new_serialize\n    yaml.serialize_all = new_serialize_all\n    yaml.dump = new_dump\n    yaml.dump_all = new_dump_all\n    yaml.safe_dump = new_safe_dump\n    yaml.safe_dump_all = new_safe_dump_all\n\ndef _tear_down():\n    yaml.BaseLoader = yaml.PyBaseLoader\n    yaml.SafeLoader = yaml.PySafeLoader\n    yaml.Loader = yaml.PyLoader\n    yaml.BaseDumper = yaml.PyBaseDumper\n    yaml.SafeDumper = yaml.PySafeDumper\n    yaml.Dumper = yaml.PyDumper\n    yaml.scan = old_scan\n    yaml.parse = old_parse\n    yaml.compose = old_compose\n    yaml.compose_all = old_compose_all\n    yaml.load = old_load\n    yaml.load_all = old_load_all\n    yaml.safe_load = old_safe_load\n    yaml.safe_load_all = old_safe_load_all\n    yaml.emit = old_emit\n    yaml.serialize = old_serialize\n    yaml.serialize_all = old_serialize_all\n    yaml.dump = old_dump\n    yaml.dump_all = old_dump_all\n    yaml.safe_dump = old_safe_dump\n    yaml.safe_dump_all = old_safe_dump_all\n\ndef test_c_version(verbose=False):\n    if verbose:\n        print(_yaml.get_version())\n        print(_yaml.get_version_string())\n    assert (\"%s.%s.%s\" % yaml._yaml.get_version()) == yaml._yaml.get_version_string(),    \\\n            (_yaml.get_version(), yaml._yaml.get_version_string())\n\ndef test_deprecate_yaml_module():\n    import _yaml\n    assert _yaml.__package__ == ''\n    assert isinstance(_yaml.get_version(), str)\n\ndef _compare_scanners(py_data, c_data, verbose):\n    py_tokens = list(yaml.scan(py_data, Loader=yaml.PyLoader))\n    c_tokens = []\n    try:\n        for token in yaml.scan(c_data, Loader=yaml.CLoader):\n            c_tokens.append(token)\n        assert len(py_tokens) == len(c_tokens), (len(py_tokens), len(c_tokens))\n        for py_token, c_token in zip(py_tokens, c_tokens):\n            assert py_token.__class__ == c_token.__class__, (py_token, c_token)\n            if hasattr(py_token, 'value'):\n                assert py_token.value == c_token.value, (py_token, c_token)\n            if isinstance(py_token, yaml.StreamEndToken):\n                continue\n            py_start = (py_token.start_mark.index, py_token.start_mark.line, py_token.start_mark.column)\n            py_end = (py_token.end_mark.index, py_token.end_mark.line, py_token.end_mark.column)\n            c_start = (c_token.start_mark.index, c_token.start_mark.line, c_token.start_mark.column)\n            c_end = (c_token.end_mark.index, c_token.end_mark.line, c_token.end_mark.column)\n            assert py_start == c_start, (py_start, c_start)\n            assert py_end == c_end, (py_end, c_end)\n    finally:\n        if verbose:\n            print(\"PY_TOKENS:\")\n            pprint.pprint(py_tokens)\n            print(\"C_TOKENS:\")\n            pprint.pprint(c_tokens)\n\ndef test_c_scanner(data_filename, canonical_filename, verbose=False):\n    with open(data_filename, 'rb') as file1, open(data_filename, 'rb') as file2:\n        _compare_scanners(file1, file2, verbose)\n    with open(data_filename, 'rb') as file1, open(data_filename, 'rb') as file2:\n        _compare_scanners(file1.read(), file2.read(), verbose)\n    with open(canonical_filename, 'rb') as file1, open(canonical_filename, 'rb') as file2:\n        _compare_scanners(file1, file2, verbose)\n    with open(canonical_filename, 'rb') as file1, open(canonical_filename, 'rb') as file2:\n        _compare_scanners(file1.read(), file2.read(), verbose)\n\ntest_c_scanner.unittest = ['.data', '.canonical']\ntest_c_scanner.skip = ['.skip-ext']\n\ndef _compare_parsers(py_data, c_data, verbose):\n    py_events = list(yaml.parse(py_data, Loader=yaml.PyLoader))\n    c_events = []\n    try:\n        for event in yaml.parse(c_data, Loader=yaml.CLoader):\n            c_events.append(event)\n        assert len(py_events) == len(c_events), (len(py_events), len(c_events))\n        for py_event, c_event in zip(py_events, c_events):\n            for attribute in ['__class__', 'anchor', 'tag', 'implicit',\n                                'value', 'explicit', 'version', 'tags']:\n                py_value = getattr(py_event, attribute, None)\n                c_value = getattr(c_event, attribute, None)\n                assert py_value == c_value, (py_event, c_event, attribute)\n    finally:\n        if verbose:\n            print(\"PY_EVENTS:\")\n            pprint.pprint(py_events)\n            print(\"C_EVENTS:\")\n            pprint.pprint(c_events)\n\ndef test_c_parser(data_filename, canonical_filename, verbose=False):\n    with open(data_filename, 'rb') as file1, open(data_filename, 'rb') as file2:\n        _compare_parsers(file1, file2, verbose)\n    with open(data_filename, 'rb') as file1, open(data_filename, 'rb') as file2:\n        _compare_parsers(file1.read(), file2.read(), verbose)\n    with open(canonical_filename, 'rb') as file1, open(canonical_filename, 'rb') as file2:\n        _compare_parsers(file1, file2, verbose)\n    with open(canonical_filename, 'rb') as file1, open(canonical_filename, 'rb') as file2:\n        _compare_parsers(file1.read(), file2.read(), verbose)\n\ntest_c_parser.unittest = ['.data', '.canonical']\ntest_c_parser.skip = ['.skip-ext']\n\ndef _compare_emitters(data, verbose):\n    events = list(yaml.parse(data, Loader=yaml.PyLoader))\n    c_data = yaml.emit(events, Dumper=yaml.CDumper)\n    if verbose:\n        print(c_data)\n    py_events = list(yaml.parse(c_data, Loader=yaml.PyLoader))\n    c_events = list(yaml.parse(c_data, Loader=yaml.CLoader))\n    try:\n        assert len(events) == len(py_events), (len(events), len(py_events))\n        assert len(events) == len(c_events), (len(events), len(c_events))\n        for event, py_event, c_event in zip(events, py_events, c_events):\n            for attribute in ['__class__', 'anchor', 'tag', 'implicit',\n                                'value', 'explicit', 'version', 'tags']:\n                value = getattr(event, attribute, None)\n                py_value = getattr(py_event, attribute, None)\n                c_value = getattr(c_event, attribute, None)\n                if attribute == 'tag' and value in [None, '!'] \\\n                        and py_value in [None, '!'] and c_value in [None, '!']:\n                    continue\n                if attribute == 'explicit' and (py_value or c_value):\n                    continue\n                assert value == py_value, (event, py_event, attribute)\n                assert value == c_value, (event, c_event, attribute)\n    finally:\n        if verbose:\n            print(\"EVENTS:\")\n            pprint.pprint(events)\n            print(\"PY_EVENTS:\")\n            pprint.pprint(py_events)\n            print(\"C_EVENTS:\")\n            pprint.pprint(c_events)\n\ndef test_c_emitter(data_filename, canonical_filename, verbose=False):\n    with open(data_filename, 'rb') as file:\n        _compare_emitters(file.read(), verbose)\n    with open(canonical_filename, 'rb') as file:\n        _compare_emitters(file.read(), verbose)\n\ntest_c_emitter.unittest = ['.data', '.canonical']\ntest_c_emitter.skip = ['.skip-ext']\n\ndef test_large_file(verbose=False):\n    SIZE_LINE = 24\n    SIZE_ITERATION = 0\n    SIZE_FILE = 31\n    if sys.maxsize <= 2**32:\n        return\n    if os.environ.get('PYYAML_TEST_GROUP', '') != 'all':\n        return\n    with tempfile.TemporaryFile() as temp_file:\n        for i in range(2**(SIZE_FILE-SIZE_ITERATION-SIZE_LINE) + 1):\n            temp_file.write(bytes(('-' + (' ' * (2**SIZE_LINE-4))+ '{}\\n')*(2**SIZE_ITERATION), 'utf-8'))\n        temp_file.seek(0)\n        yaml.load(temp_file, Loader=yaml.CLoader)\n\ntest_large_file.unittest = None\n\ndef wrap_ext_function(function):\n    def wrapper(*args, **kwds):\n        _set_up()\n        try:\n            function(*args, **kwds)\n        finally:\n            _tear_down()\n    wrapper.__name__ = '%s_ext' % function.__name__\n    wrapper.unittest = function.unittest\n    wrapper.skip = getattr(function, 'skip', [])+['.skip-ext']\n    return wrapper\n\ndef wrap_ext(collections):\n    functions = []\n    if not isinstance(collections, list):\n        collections = [collections]\n    for collection in collections:\n        if not isinstance(collection, dict):\n            collection = vars(collection)\n        for key in sorted(collection):\n            value = collection[key]\n            if isinstance(value, types.FunctionType) and hasattr(value, 'unittest'):\n                functions.append(wrap_ext_function(value))\n    for function in functions:\n        assert function.__name__ not in globals()\n        globals()[function.__name__] = function\n\nimport test_tokens, test_structure, test_errors, test_resolver, test_constructor,   \\\n        test_emitter, test_representer, test_recursive, test_input_output\nwrap_ext([test_tokens, test_structure, test_errors, test_resolver, test_constructor,\n        test_emitter, test_representer, test_recursive, test_input_output])\n\nif __name__ == '__main__':\n    import test_appliance\n    test_appliance.run(globals())\n\n", "tests/legacy_tests/test_reader.py": "\nimport yaml.reader\n\ndef _run_reader(data, verbose):\n    try:\n        stream = yaml.reader.Reader(data)\n        while stream.peek() != '\\0':\n            stream.forward()\n    except yaml.reader.ReaderError as exc:\n        if verbose:\n            print(exc)\n    else:\n        raise AssertionError(\"expected an exception\")\n\ndef test_stream_error(error_filename, verbose=False):\n    with open(error_filename, 'rb') as file:\n        _run_reader(file, verbose)\n    with open(error_filename, 'rb') as file:\n        _run_reader(file.read(), verbose)\n    for encoding in ['utf-8', 'utf-16-le', 'utf-16-be']:\n        try:\n            with open(error_filename, 'rb') as file:\n                data = file.read().decode(encoding)\n            break\n        except UnicodeDecodeError:\n            pass\n    else:\n        return\n    _run_reader(data, verbose)\n    with open(error_filename, encoding=encoding) as file:\n        _run_reader(file, verbose)\n\ntest_stream_error.unittest = ['.stream-error']\n\nif __name__ == '__main__':\n    import test_appliance\n    test_appliance.run(globals())\n\n", "tests/legacy_tests/test_build.py": "\nif __name__ == '__main__':\n    import sys, os, distutils.util\n    build_lib = 'build/lib'\n    build_lib_ext = os.path.join('build', 'lib.{}-{}.{}'.format(distutils.util.get_platform(), *sys.version_info))\n    sys.path.insert(0, build_lib)\n    sys.path.insert(0, build_lib_ext)\n    import test_yaml, test_appliance\n    test_appliance.run(test_yaml)\n\n", "tests/legacy_tests/test_yaml.py": "from test_mark import *\nfrom test_reader import *\nfrom test_canonical import *\nfrom test_tokens import *\nfrom test_structure import *\nfrom test_errors import *\nfrom test_resolver import *\nfrom test_constructor import *\nfrom test_emitter import *\nfrom test_representer import *\nfrom test_recursive import *\nfrom test_input_output import *\nfrom test_sort_keys import *\nfrom test_multi_constructor import *\n\nfrom test_schema import *\n\nif __name__ == '__main__':\n    import test_appliance\n    test_appliance.run(globals())\n\n", "tests/legacy_tests/canonical.py": "\nimport yaml, yaml.composer, yaml.constructor, yaml.resolver\n\nclass CanonicalError(yaml.YAMLError):\n    pass\n\nclass CanonicalScanner:\n\n    def __init__(self, data):\n        if isinstance(data, bytes):\n            try:\n                data = data.decode('utf-8')\n            except UnicodeDecodeError:\n                raise CanonicalError(\"utf-8 stream is expected\")\n        self.data = data+'\\0'\n        self.index = 0\n        self.tokens = []\n        self.scanned = False\n\n    def check_token(self, *choices):\n        if not self.scanned:\n            self.scan()\n        if self.tokens:\n            if not choices:\n                return True\n            for choice in choices:\n                if isinstance(self.tokens[0], choice):\n                    return True\n        return False\n\n    def peek_token(self):\n        if not self.scanned:\n            self.scan()\n        if self.tokens:\n            return self.tokens[0]\n\n    def get_token(self, choice=None):\n        if not self.scanned:\n            self.scan()\n        token = self.tokens.pop(0)\n        if choice and not isinstance(token, choice):\n            raise CanonicalError(\"unexpected token \"+repr(token))\n        return token\n\n    def get_token_value(self):\n        token = self.get_token()\n        return token.value\n\n    def scan(self):\n        self.tokens.append(yaml.StreamStartToken(None, None))\n        while True:\n            self.find_token()\n            ch = self.data[self.index]\n            if ch == '\\0':\n                self.tokens.append(yaml.StreamEndToken(None, None))\n                break\n            elif ch == '%':\n                self.tokens.append(self.scan_directive())\n            elif ch == '-' and self.data[self.index:self.index+3] == '---':\n                self.index += 3\n                self.tokens.append(yaml.DocumentStartToken(None, None))\n            elif ch == '[':\n                self.index += 1\n                self.tokens.append(yaml.FlowSequenceStartToken(None, None))\n            elif ch == '{':\n                self.index += 1\n                self.tokens.append(yaml.FlowMappingStartToken(None, None))\n            elif ch == ']':\n                self.index += 1\n                self.tokens.append(yaml.FlowSequenceEndToken(None, None))\n            elif ch == '}':\n                self.index += 1\n                self.tokens.append(yaml.FlowMappingEndToken(None, None))\n            elif ch == '?':\n                self.index += 1\n                self.tokens.append(yaml.KeyToken(None, None))\n            elif ch == ':':\n                self.index += 1\n                self.tokens.append(yaml.ValueToken(None, None))\n            elif ch == ',':\n                self.index += 1\n                self.tokens.append(yaml.FlowEntryToken(None, None))\n            elif ch == '*' or ch == '&':\n                self.tokens.append(self.scan_alias())\n            elif ch == '!':\n                self.tokens.append(self.scan_tag())\n            elif ch == '\"':\n                self.tokens.append(self.scan_scalar())\n            else:\n                raise CanonicalError(\"invalid token\")\n        self.scanned = True\n\n    DIRECTIVE = '%YAML 1.1'\n\n    def scan_directive(self):\n        if self.data[self.index:self.index+len(self.DIRECTIVE)] == self.DIRECTIVE and \\\n                self.data[self.index+len(self.DIRECTIVE)] in ' \\n\\0':\n            self.index += len(self.DIRECTIVE)\n            return yaml.DirectiveToken('YAML', (1, 1), None, None)\n        else:\n            raise CanonicalError(\"invalid directive\")\n\n    def scan_alias(self):\n        if self.data[self.index] == '*':\n            TokenClass = yaml.AliasToken\n        else:\n            TokenClass = yaml.AnchorToken\n        self.index += 1\n        start = self.index\n        while self.data[self.index] not in ', \\n\\0':\n            self.index += 1\n        value = self.data[start:self.index]\n        return TokenClass(value, None, None)\n\n    def scan_tag(self):\n        self.index += 1\n        start = self.index\n        while self.data[self.index] not in ' \\n\\0':\n            self.index += 1\n        value = self.data[start:self.index]\n        if not value:\n            value = '!'\n        elif value[0] == '!':\n            value = 'tag:yaml.org,2002:'+value[1:]\n        elif value[0] == '<' and value[-1] == '>':\n            value = value[1:-1]\n        else:\n            value = '!'+value\n        return yaml.TagToken(value, None, None)\n\n    QUOTE_CODES = {\n        'x': 2,\n        'u': 4,\n        'U': 8,\n    }\n\n    QUOTE_REPLACES = {\n        '\\\\': '\\\\',\n        '\\\"': '\\\"',\n        ' ': ' ',\n        'a': '\\x07',\n        'b': '\\x08',\n        'e': '\\x1B',\n        'f': '\\x0C',\n        'n': '\\x0A',\n        'r': '\\x0D',\n        't': '\\x09',\n        'v': '\\x0B',\n        'N': '\\u0085',\n        'L': '\\u2028',\n        'P': '\\u2029',\n        '_': '_',\n        '0': '\\x00',\n    }\n\n    def scan_scalar(self):\n        self.index += 1\n        chunks = []\n        start = self.index\n        ignore_spaces = False\n        while self.data[self.index] != '\"':\n            if self.data[self.index] == '\\\\':\n                ignore_spaces = False\n                chunks.append(self.data[start:self.index])\n                self.index += 1\n                ch = self.data[self.index]\n                self.index += 1\n                if ch == '\\n':\n                    ignore_spaces = True\n                elif ch in self.QUOTE_CODES:\n                    length = self.QUOTE_CODES[ch]\n                    code = int(self.data[self.index:self.index+length], 16)\n                    chunks.append(chr(code))\n                    self.index += length\n                else:\n                    if ch not in self.QUOTE_REPLACES:\n                        raise CanonicalError(\"invalid escape code\")\n                    chunks.append(self.QUOTE_REPLACES[ch])\n                start = self.index\n            elif self.data[self.index] == '\\n':\n                chunks.append(self.data[start:self.index])\n                chunks.append(' ')\n                self.index += 1\n                start = self.index\n                ignore_spaces = True\n            elif ignore_spaces and self.data[self.index] == ' ':\n                self.index += 1\n                start = self.index\n            else:\n                ignore_spaces = False\n                self.index += 1\n        chunks.append(self.data[start:self.index])\n        self.index += 1\n        return yaml.ScalarToken(''.join(chunks), False, None, None)\n\n    def find_token(self):\n        found = False\n        while not found:\n            while self.data[self.index] in ' \\t':\n                self.index += 1\n            if self.data[self.index] == '#':\n                while self.data[self.index] != '\\n':\n                    self.index += 1\n            if self.data[self.index] == '\\n':\n                self.index += 1\n            else:\n                found = True\n\nclass CanonicalParser:\n\n    def __init__(self):\n        self.events = []\n        self.parsed = False\n\n    def dispose(self):\n        pass\n\n    # stream: STREAM-START document* STREAM-END\n    def parse_stream(self):\n        self.get_token(yaml.StreamStartToken)\n        self.events.append(yaml.StreamStartEvent(None, None))\n        while not self.check_token(yaml.StreamEndToken):\n            if self.check_token(yaml.DirectiveToken, yaml.DocumentStartToken):\n                self.parse_document()\n            else:\n                raise CanonicalError(\"document is expected, got \"+repr(self.tokens[0]))\n        self.get_token(yaml.StreamEndToken)\n        self.events.append(yaml.StreamEndEvent(None, None))\n\n    # document: DIRECTIVE? DOCUMENT-START node\n    def parse_document(self):\n        node = None\n        if self.check_token(yaml.DirectiveToken):\n            self.get_token(yaml.DirectiveToken)\n        self.get_token(yaml.DocumentStartToken)\n        self.events.append(yaml.DocumentStartEvent(None, None))\n        self.parse_node()\n        self.events.append(yaml.DocumentEndEvent(None, None))\n\n    # node: ALIAS | ANCHOR? TAG? (SCALAR|sequence|mapping)\n    def parse_node(self):\n        if self.check_token(yaml.AliasToken):\n            self.events.append(yaml.AliasEvent(self.get_token_value(), None, None))\n        else:\n            anchor = None\n            if self.check_token(yaml.AnchorToken):\n                anchor = self.get_token_value()\n            tag = None\n            if self.check_token(yaml.TagToken):\n                tag = self.get_token_value()\n            if self.check_token(yaml.ScalarToken):\n                self.events.append(yaml.ScalarEvent(anchor, tag, (False, False), self.get_token_value(), None, None))\n            elif self.check_token(yaml.FlowSequenceStartToken):\n                self.events.append(yaml.SequenceStartEvent(anchor, tag, None, None))\n                self.parse_sequence()\n            elif self.check_token(yaml.FlowMappingStartToken):\n                self.events.append(yaml.MappingStartEvent(anchor, tag, None, None))\n                self.parse_mapping()\n            else:\n                raise CanonicalError(\"SCALAR, '[', or '{' is expected, got \"+repr(self.tokens[0]))\n\n    # sequence: SEQUENCE-START (node (ENTRY node)*)? ENTRY? SEQUENCE-END\n    def parse_sequence(self):\n        self.get_token(yaml.FlowSequenceStartToken)\n        if not self.check_token(yaml.FlowSequenceEndToken):\n            self.parse_node()\n            while not self.check_token(yaml.FlowSequenceEndToken):\n                self.get_token(yaml.FlowEntryToken)\n                if not self.check_token(yaml.FlowSequenceEndToken):\n                    self.parse_node()\n        self.get_token(yaml.FlowSequenceEndToken)\n        self.events.append(yaml.SequenceEndEvent(None, None))\n\n    # mapping: MAPPING-START (map_entry (ENTRY map_entry)*)? ENTRY? MAPPING-END\n    def parse_mapping(self):\n        self.get_token(yaml.FlowMappingStartToken)\n        if not self.check_token(yaml.FlowMappingEndToken):\n            self.parse_map_entry()\n            while not self.check_token(yaml.FlowMappingEndToken):\n                self.get_token(yaml.FlowEntryToken)\n                if not self.check_token(yaml.FlowMappingEndToken):\n                    self.parse_map_entry()\n        self.get_token(yaml.FlowMappingEndToken)\n        self.events.append(yaml.MappingEndEvent(None, None))\n\n    # map_entry: KEY node VALUE node\n    def parse_map_entry(self):\n        self.get_token(yaml.KeyToken)\n        self.parse_node()\n        self.get_token(yaml.ValueToken)\n        self.parse_node()\n\n    def parse(self):\n        self.parse_stream()\n        self.parsed = True\n\n    def get_event(self):\n        if not self.parsed:\n            self.parse()\n        return self.events.pop(0)\n\n    def check_event(self, *choices):\n        if not self.parsed:\n            self.parse()\n        if self.events:\n            if not choices:\n                return True\n            for choice in choices:\n                if isinstance(self.events[0], choice):\n                    return True\n        return False\n\n    def peek_event(self):\n        if not self.parsed:\n            self.parse()\n        return self.events[0]\n\nclass CanonicalLoader(CanonicalScanner, CanonicalParser,\n        yaml.composer.Composer, yaml.constructor.Constructor, yaml.resolver.Resolver):\n\n    def __init__(self, stream):\n        if hasattr(stream, 'read'):\n            stream = stream.read()\n        CanonicalScanner.__init__(self, stream)\n        CanonicalParser.__init__(self)\n        yaml.composer.Composer.__init__(self)\n        yaml.constructor.Constructor.__init__(self)\n        yaml.resolver.Resolver.__init__(self)\n\nyaml.CanonicalLoader = CanonicalLoader\n\ndef canonical_scan(stream):\n    return yaml.scan(stream, Loader=CanonicalLoader)\n\nyaml.canonical_scan = canonical_scan\n\ndef canonical_parse(stream):\n    return yaml.parse(stream, Loader=CanonicalLoader)\n\nyaml.canonical_parse = canonical_parse\n\ndef canonical_compose(stream):\n    return yaml.compose(stream, Loader=CanonicalLoader)\n\nyaml.canonical_compose = canonical_compose\n\ndef canonical_compose_all(stream):\n    return yaml.compose_all(stream, Loader=CanonicalLoader)\n\nyaml.canonical_compose_all = canonical_compose_all\n\ndef canonical_load(stream):\n    return yaml.load(stream, Loader=CanonicalLoader)\n\nyaml.canonical_load = canonical_load\n\ndef canonical_load_all(stream):\n    return yaml.load_all(stream, Loader=CanonicalLoader)\n\nyaml.canonical_load_all = canonical_load_all\n\n", "tests/legacy_tests/conftest.py": "# pytest custom collection adapter for legacy pyyaml unit tests/data files; surfaces each\n# legacy test case as a pyyaml item\n\nimport os\nimport pathlib\nimport pytest\nimport warnings\n\nfrom test_appliance import find_test_filenames, DATA\n\ntry:\n    from yaml import _yaml\n    HAS_LIBYAML_EXT = True\n    del _yaml\nexcept ImportError:\n    HAS_LIBYAML_EXT = False\n\n\n_test_filenames = find_test_filenames(DATA)\n\n# ignore all datafiles\ncollect_ignore_glob = ['data/*']\n\n\nclass PyYAMLItem(pytest.Item):\n    def __init__(self, parent=None, config=None, session=None, nodeid=None, function=None, filenames=None, **kwargs):\n        self._function = function\n        self._fargs = filenames or []\n\n        super().__init__(os.path.basename(filenames[0]) if filenames else parent.name, parent, config, session, nodeid)\n        # this is gnarly since the type of fspath is private; fixed in pytest 7 to use pathlib on the `path` attr\n        if filenames:  # pass the data file location as the test path\n            self.fspath = parent.fspath.__class__(filenames[0])\n            self.lineno = 1\n        else:  # pass the function location in the code\n            self.fspath = parent.fspath.__class__(function.__code__.co_filename)\n            self.lineno = function.__code__.co_firstlineno\n\n    def runtest(self):\n        self._function(verbose=True, *self._fargs)\n\n    def reportinfo(self):\n        return self.fspath, self.lineno, ''\n\n\nclass PyYAMLCollector(pytest.Collector):\n    def __init__(self, name, parent=None, function=None, **kwargs):\n        self._function = function\n        self.fspath = parent.fspath.__class__(function.__code__.co_filename)\n        self.lineno = function.__code__.co_firstlineno\n\n        # avoid fspath deprecation warnings on pytest < 7\n        if hasattr(self, 'path') and 'fspath' in kwargs:\n            del kwargs['fspath']\n\n        super().__init__(name=name, parent=parent, **kwargs)\n\n    def collect(self):\n        items = []\n\n        unittest = getattr(self._function, 'unittest', None)\n\n        if unittest is True:  # no filenames\n            items.append(PyYAMLItem.from_parent(parent=self, function=self._function, filenames=None))\n        else:\n            for base, exts in _test_filenames:\n                filenames = []\n                for ext in unittest:\n                    if ext not in exts:\n                        break\n                    filenames.append(os.path.join(DATA, base + ext))\n                else:\n                    skip_exts = getattr(self._function, 'skip', [])\n                    for skip_ext in skip_exts:\n                        if skip_ext in exts:\n                            break\n                    else:\n                        items.append(PyYAMLItem.from_parent(parent=self, function=self._function, filenames=filenames))\n\n        return items or None\n\n    def reportinfo(self):\n        return self.fspath, self.lineno, ''\n\n    @classmethod\n    def from_parent(cls, parent, fspath, **kwargs):\n        return super().from_parent(parent=parent, fspath=fspath, **kwargs)\n\n\n@pytest.hookimpl(hookwrapper=True, trylast=True)\ndef pytest_pycollect_makeitem(collector, name: str, obj: object):\n    outcome = yield\n    outcome.get_result()\n    if not callable(obj):\n        outcome.force_result(None)\n        return\n    unittest = getattr(obj, 'unittest', None)\n\n    if not unittest:\n        outcome.force_result(None)\n        return\n\n    if unittest is True:  # no file list to run against, just return a test item instead of a collector\n        outcome.force_result(PyYAMLItem.from_parent(name=name, parent=collector, fspath=collector.fspath, function=obj))\n        return\n\n    # there's a file list; return a collector to create individual items for each\n    outcome.force_result(PyYAMLCollector.from_parent(name=name, parent=collector, fspath=collector.fspath, function=obj))\n    return\n\n\ndef pytest_collection_modifyitems(session, config, items):\n    pass\n\n\ndef pytest_ignore_collect(collection_path: pathlib.Path):\n    basename = collection_path.name\n    # ignore all Python files in this subtree for normal pytest collection\n    if basename not in ['test_yaml.py', 'test_yaml_ext.py']:\n        return True\n\n    # ignore extension tests (depending on config)\n    if basename == 'test_yaml_ext.py':\n        require_libyaml = os.environ.get('PYYAML_FORCE_LIBYAML', None)\n        if require_libyaml == '1' and not HAS_LIBYAML_EXT:\n            raise RuntimeError('PYYAML_FORCE_LIBYAML envvar is set, but libyaml extension is not available')\n        if require_libyaml == '0':\n            return True\n        if not HAS_LIBYAML_EXT:\n            warnings.warn('libyaml extension is not available, skipping libyaml tests')\n            return True\n\n", "tests/legacy_tests/test_recursive.py": "\nimport yaml\n\nclass AnInstance:\n\n    def __init__(self, foo, bar):\n        self.foo = foo\n        self.bar = bar\n\n    def __repr__(self):\n        try:\n            return \"%s(foo=%r, bar=%r)\" % (self.__class__.__name__,\n                    self.foo, self.bar)\n        except RuntimeError:\n            return \"%s(foo=..., bar=...)\" % self.__class__.__name__\n\nclass AnInstanceWithState(AnInstance):\n\n    def __getstate__(self):\n        return {'attributes': [self.foo, self.bar]}\n\n    def __setstate__(self, state):\n        self.foo, self.bar = state['attributes']\n\ndef test_recursive(recursive_filename, verbose=False):\n    context = globals().copy()\n    with open(recursive_filename, 'rb') as file:\n        exec(file.read(), context)\n    value1 = context['value']\n    output1 = None\n    value2 = None\n    output2 = None\n    try:\n        output1 = yaml.dump(value1)\n        value2 = yaml.unsafe_load(output1)\n        output2 = yaml.dump(value2)\n        assert output1 == output2, (output1, output2)\n    finally:\n        if verbose:\n            print(\"VALUE1:\", value1)\n            print(\"VALUE2:\", value2)\n            print(\"OUTPUT1:\")\n            print(output1)\n            print(\"OUTPUT2:\")\n            print(output2)\n\ntest_recursive.unittest = ['.recursive']\n\nif __name__ == '__main__':\n    import test_appliance\n    test_appliance.run(globals())\n\n", "tests/legacy_tests/test_resolver.py": "\nimport yaml\nimport pprint\n\ndef test_implicit_resolver(data_filename, detect_filename, verbose=False):\n    correct_tag = None\n    node = None\n    try:\n        with open(detect_filename, 'r') as file:\n            correct_tag = file.read().strip()\n        with open(data_filename, 'rb') as file:\n            node = yaml.compose(file)\n        assert isinstance(node, yaml.SequenceNode), node\n        for scalar in node.value:\n            assert isinstance(scalar, yaml.ScalarNode), scalar\n            assert scalar.tag == correct_tag, (scalar.tag, correct_tag)\n    finally:\n        if verbose:\n            print(\"CORRECT TAG:\", correct_tag)\n            if hasattr(node, 'value'):\n                print(\"CHILDREN:\")\n                pprint.pprint(node.value)\n\ntest_implicit_resolver.unittest = ['.data', '.detect']\n\ndef _make_path_loader_and_dumper():\n    global MyLoader, MyDumper\n\n    class MyLoader(yaml.Loader):\n        pass\n    class MyDumper(yaml.Dumper):\n        pass\n\n    yaml.add_path_resolver('!root', [],\n            Loader=MyLoader, Dumper=MyDumper)\n    yaml.add_path_resolver('!root/scalar', [], str,\n            Loader=MyLoader, Dumper=MyDumper)\n    yaml.add_path_resolver('!root/key11/key12/*', ['key11', 'key12'],\n            Loader=MyLoader, Dumper=MyDumper)\n    yaml.add_path_resolver('!root/key21/1/*', ['key21', 1],\n            Loader=MyLoader, Dumper=MyDumper)\n    yaml.add_path_resolver('!root/key31/*/*/key14/map', ['key31', None, None, 'key14'], dict,\n            Loader=MyLoader, Dumper=MyDumper)\n\n    return MyLoader, MyDumper\n\ndef _convert_node(node):\n    if isinstance(node, yaml.ScalarNode):\n        return (node.tag, node.value)\n    elif isinstance(node, yaml.SequenceNode):\n        value = []\n        for item in node.value:\n            value.append(_convert_node(item))\n        return (node.tag, value)\n    elif isinstance(node, yaml.MappingNode):\n        value = []\n        for key, item in node.value:\n            value.append((_convert_node(key), _convert_node(item)))\n        return (node.tag, value)\n\ndef test_path_resolver_loader(data_filename, path_filename, verbose=False):\n    _make_path_loader_and_dumper()\n    with open(data_filename, 'rb') as file:\n        nodes1 = list(yaml.compose_all(file.read(), Loader=MyLoader))\n    with open(path_filename, 'rb') as file:\n        nodes2 = list(yaml.compose_all(file.read()))\n    try:\n        for node1, node2 in zip(nodes1, nodes2):\n            data1 = _convert_node(node1)\n            data2 = _convert_node(node2)\n            assert data1 == data2, (data1, data2)\n    finally:\n        if verbose:\n            print(yaml.serialize_all(nodes1))\n\ntest_path_resolver_loader.unittest = ['.data', '.path']\n\ndef test_path_resolver_dumper(data_filename, path_filename, verbose=False):\n    _make_path_loader_and_dumper()\n    for filename in [data_filename, path_filename]:\n        with open(filename, 'rb') as file:\n            output = yaml.serialize_all(yaml.compose_all(file), Dumper=MyDumper)\n        if verbose:\n            print(output)\n        nodes1 = yaml.compose_all(output)\n        with open(data_filename, 'rb') as file:\n            nodes2 = yaml.compose_all(file)\n            for node1, node2 in zip(nodes1, nodes2):\n                data1 = _convert_node(node1)\n                data2 = _convert_node(node2)\n                assert data1 == data2, (data1, data2)\n\ntest_path_resolver_dumper.unittest = ['.data', '.path']\n\nif __name__ == '__main__':\n    import test_appliance\n    test_appliance.run(globals())\n\n", "tests/legacy_tests/test_input_output.py": "\nimport yaml\nimport codecs, io, tempfile, os, os.path\n\ndef test_unicode_input(unicode_filename, verbose=False):\n    with open(unicode_filename, 'rb') as file:\n        data = file.read().decode('utf-8')\n    value = ' '.join(data.split())\n    output = yaml.full_load(data)\n    assert output == value, (output, value)\n    output = yaml.full_load(io.StringIO(data))\n    assert output == value, (output, value)\n    for input in [data.encode('utf-8'),\n                    codecs.BOM_UTF8+data.encode('utf-8'),\n                    codecs.BOM_UTF16_BE+data.encode('utf-16-be'),\n                    codecs.BOM_UTF16_LE+data.encode('utf-16-le')]:\n        if verbose:\n            print(\"INPUT:\", repr(input[:10]), \"...\")\n        output = yaml.full_load(input)\n        assert output == value, (output, value)\n        output = yaml.full_load(io.BytesIO(input))\n        assert output == value, (output, value)\n\ntest_unicode_input.unittest = ['.unicode']\n\ndef test_unicode_input_errors(unicode_filename, verbose=False):\n    with open(unicode_filename, 'rb') as file:\n        data = file.read().decode('utf-8')\n    for input in [data.encode('utf-16-be'),\n            data.encode('utf-16-le'),\n            codecs.BOM_UTF8+data.encode('utf-16-be'),\n            codecs.BOM_UTF8+data.encode('utf-16-le')]:\n\n        try:\n            yaml.full_load(input)\n        except yaml.YAMLError as exc:\n            if verbose:\n                print(exc)\n        else:\n            raise AssertionError(\"expected an exception\")\n        try:\n            yaml.full_load(io.BytesIO(input))\n        except yaml.YAMLError as exc:\n            if verbose:\n                print(exc)\n        else:\n            raise AssertionError(\"expected an exception\")\n\ntest_unicode_input_errors.unittest = ['.unicode']\n\ndef test_unicode_output(unicode_filename, verbose=False):\n    with open(unicode_filename, 'rb') as file:\n        data = file.read().decode('utf-8')\n    value = ' '.join(data.split())\n    for allow_unicode in [False, True]:\n        data1 = yaml.dump(value, allow_unicode=allow_unicode)\n        for encoding in [None, 'utf-8', 'utf-16-be', 'utf-16-le']:\n            stream = io.StringIO()\n            yaml.dump(value, stream, encoding=encoding, allow_unicode=allow_unicode)\n            data2 = stream.getvalue()\n            data3 = yaml.dump(value, encoding=encoding, allow_unicode=allow_unicode)\n            if encoding is not None:\n                assert isinstance(data3, bytes)\n                data3 = data3.decode(encoding)\n            stream = io.BytesIO()\n            if encoding is None:\n                try:\n                    yaml.dump(value, stream, encoding=encoding, allow_unicode=allow_unicode)\n                except TypeError as exc:\n                    if verbose:\n                        print(exc)\n                    data4 = None\n                else:\n                    raise AssertionError(\"expected an exception\")\n            else:\n                yaml.dump(value, stream, encoding=encoding, allow_unicode=allow_unicode)\n                data4 = stream.getvalue()\n                if verbose:\n                    print(\"BYTES:\", data4[:50])\n                data4 = data4.decode(encoding)\n\n            assert isinstance(data1, str), (type(data1), encoding)\n            assert isinstance(data2, str), (type(data2), encoding)\n\ntest_unicode_output.unittest = ['.unicode']\n\ndef test_file_output(unicode_filename, verbose=False):\n    with open(unicode_filename, 'rb') as file:\n        data = file.read().decode('utf-8')\n    handle, filename = tempfile.mkstemp()\n    os.close(handle)\n    try:\n        stream = io.StringIO()\n        yaml.dump(data, stream, allow_unicode=True)\n        data1 = stream.getvalue()\n        stream = io.BytesIO()\n        yaml.dump(data, stream, encoding='utf-16-le', allow_unicode=True)\n        data2 = stream.getvalue().decode('utf-16-le')[1:]\n        with open(filename, 'w', encoding='utf-16-le') as stream:\n            yaml.dump(data, stream, allow_unicode=True)\n        with open(filename, 'r', encoding='utf-16-le') as file:\n            data3 = file.read()\n        with open(filename, 'wb') as stream:\n            yaml.dump(data, stream, encoding='utf-8', allow_unicode=True)\n        with open(filename, 'r', encoding='utf-8') as file:\n            data4 = file.read()\n        assert data1 == data2, (data1, data2)\n        assert data1 == data3, (data1, data3)\n        assert data1 == data4, (data1, data4)\n    finally:\n        if os.path.exists(filename):\n            os.unlink(filename)\n\ntest_file_output.unittest = ['.unicode']\n\ndef test_unicode_transfer(unicode_filename, verbose=False):\n    with open(unicode_filename, 'rb') as file:\n        data = file.read().decode('utf-8')\n    for encoding in [None, 'utf-8', 'utf-16-be', 'utf-16-le']:\n        input = data\n        if encoding is not None:\n            input = ('\\ufeff'+input).encode(encoding)\n        output1 = yaml.emit(yaml.parse(input), allow_unicode=True)\n        if encoding is None:\n            stream = io.StringIO()\n        else:\n            stream = io.BytesIO()\n        yaml.emit(yaml.parse(input), stream, allow_unicode=True)\n        output2 = stream.getvalue()\n        assert isinstance(output1, str), (type(output1), encoding)\n        if encoding is None:\n            assert isinstance(output2, str), (type(output1), encoding)\n        else:\n            assert isinstance(output2, bytes), (type(output1), encoding)\n            output2.decode(encoding)\n\ntest_unicode_transfer.unittest = ['.unicode']\n\nif __name__ == '__main__':\n    import test_appliance\n    test_appliance.run(globals())\n", "tests/legacy_tests/test_all.py": "\nimport sys, yaml, test_appliance\n\ndef main(args=None):\n    collections = []\n    import test_yaml\n    collections.append(test_yaml)\n    if yaml.__with_libyaml__:\n        import test_yaml_ext\n        collections.append(test_yaml_ext)\n    return test_appliance.run(collections, args)\n\nif __name__ == '__main__':\n    main()\n\n", "tests/legacy_tests/test_multi_constructor.py": "import yaml\nimport pprint\nimport sys\n\ndef _load_code(expression):\n    return eval(expression)\n\ndef myconstructor1(constructor, tag, node):\n    seq = constructor.construct_sequence(node)\n    return {tag: seq }\n\ndef myconstructor2(constructor, tag, node):\n    seq = constructor.construct_sequence(node)\n    string = ''\n    try:\n        i = tag.index('!') + 1\n    except:\n        try:\n            i = tag.rindex(':') + 1\n        except:\n            pass\n    if i >= 0:\n        tag = tag[i:]\n    return { tag: seq }\n\nclass Multi1(yaml.FullLoader):\n    pass\nclass Multi2(yaml.FullLoader):\n    pass\n\ndef test_multi_constructor(input_filename, code_filename, verbose=False):\n    with open(input_filename, 'rb') as file:\n        input = file.read().decode('utf-8')\n    with open(code_filename, 'rb') as file:\n        native = _load_code(file.read())\n\n    # default multi constructor for ! and !! tags\n    Multi1.add_multi_constructor('!', myconstructor1)\n    Multi1.add_multi_constructor('tag:yaml.org,2002:', myconstructor1)\n\n    data = yaml.load(input, Loader=Multi1)\n    if verbose:\n        print('Multi1:')\n        print(data)\n        print(native)\n    assert(data == native)\n\n\n    # default multi constructor for all tags\n    Multi2.add_multi_constructor(None, myconstructor2)\n\n    data = yaml.load(input, Loader=Multi2)\n    if verbose:\n        print('Multi2:')\n        print(data)\n        print(native)\n    assert(data == native)\n\n\ntest_multi_constructor.unittest = ['.multi', '.code']\n\nif __name__ == '__main__':\n    import test_appliance\n    test_appliance.run(globals())\n\n", "tests/legacy_tests/test_tokens.py": "\nimport yaml\nimport pprint\n\n# Tokens mnemonic:\n# directive:            %\n# document_start:       ---\n# document_end:         ...\n# alias:                *\n# anchor:               &\n# tag:                  !\n# scalar                _\n# block_sequence_start: [[\n# block_mapping_start:  {{\n# block_end:            ]}\n# flow_sequence_start:  [\n# flow_sequence_end:    ]\n# flow_mapping_start:   {\n# flow_mapping_end:     }\n# entry:                ,\n# key:                  ?\n# value:                :\n\n_replaces = {\n    yaml.DirectiveToken: '%',\n    yaml.DocumentStartToken: '---',\n    yaml.DocumentEndToken: '...',\n    yaml.AliasToken: '*',\n    yaml.AnchorToken: '&',\n    yaml.TagToken: '!',\n    yaml.ScalarToken: '_',\n    yaml.BlockSequenceStartToken: '[[',\n    yaml.BlockMappingStartToken: '{{',\n    yaml.BlockEndToken: ']}',\n    yaml.FlowSequenceStartToken: '[',\n    yaml.FlowSequenceEndToken: ']',\n    yaml.FlowMappingStartToken: '{',\n    yaml.FlowMappingEndToken: '}',\n    yaml.BlockEntryToken: ',',\n    yaml.FlowEntryToken: ',',\n    yaml.KeyToken: '?',\n    yaml.ValueToken: ':',\n}\n\ndef test_tokens(data_filename, tokens_filename, verbose=False):\n    tokens1 = []\n    with open(tokens_filename, 'r') as file:\n        tokens2 = file.read().split()\n    try:\n        with open(data_filename, 'rb') as file:\n            for token in yaml.scan(file):\n                if not isinstance(token, (yaml.StreamStartToken, yaml.StreamEndToken)):\n                    tokens1.append(_replaces[token.__class__])\n    finally:\n        if verbose:\n            print(\"TOKENS1:\", ' '.join(tokens1))\n            print(\"TOKENS2:\", ' '.join(tokens2))\n    assert len(tokens1) == len(tokens2), (tokens1, tokens2)\n    for token1, token2 in zip(tokens1, tokens2):\n        assert token1 == token2, (token1, token2)\n\ntest_tokens.unittest = ['.data', '.tokens']\n\ndef test_scanner(data_filename, canonical_filename, verbose=False):\n    for filename in [data_filename, canonical_filename]:\n        tokens = []\n        try:\n            with open(filename, 'rb') as file:\n                for token in yaml.scan(file):\n                    tokens.append(token.__class__.__name__)\n        finally:\n            if verbose:\n                pprint.pprint(tokens)\n\ntest_scanner.unittest = ['.data', '.canonical']\n\nif __name__ == '__main__':\n    import test_appliance\n    test_appliance.run(globals())\n\n", "tests/legacy_tests/test_schema.py": "import yaml\nimport sys\nimport pprint\nimport math\n\ndef check_bool(value, expected):\n    if expected == 'false()' and value is False:\n        return 1\n    if expected == 'true()' and value is True:\n        return 1\n    print(value)\n    print(expected)\n    return 0\n\ndef check_int(value, expected):\n    if (int(expected) == value):\n        return 1\n    print(value)\n    print(expected)\n    return 0\n\ndef check_float(value, expected):\n    if expected == 'inf()':\n        if value == math.inf:\n            return 1\n    elif expected == 'inf-neg()':\n        if value == -math.inf:\n            return 1\n    elif expected == 'nan()':\n        if math.isnan(value):\n            return 1\n    elif (float(expected) == value):\n        return 1\n    else:\n        print(value)\n        print(expected)\n        return 0\n\ndef check_str(value, expected):\n    if value == expected:\n        return 1\n    print(value)\n    print(expected)\n    return 0\n\n\ndef _fail(input, test):\n    print(\"Input: >>\" + input + \"<<\")\n    print(test)\n\n# The tests/data/yaml11.schema file is copied from\n# https://github.com/perlpunk/yaml-test-schema/blob/master/data/schema-yaml11.yaml\ndef test_implicit_resolver(data_filename, skip_filename, verbose=False):\n    types = {\n        'str':   [str,   check_str],\n        'int':   [int,   check_int],\n        'float': [float, check_float],\n        'inf':   [float, check_float],\n        'nan':   [float, check_float],\n        'bool':  [bool,  check_bool],\n    }\n    with open(skip_filename, 'rb') as file:\n        skipdata = yaml.load(file, Loader=yaml.SafeLoader)\n    skip_load = skipdata['load']\n    skip_dump = skipdata['dump']\n    if verbose:\n        print(skip_load)\n    with open(data_filename, 'rb') as file:\n        tests = yaml.load(file, Loader=yaml.SafeLoader)\n\n    i = 0\n    fail = 0\n    for i, (input, test) in enumerate(sorted(tests.items())):\n        if verbose:\n            print('-------------------- ' + str(i))\n\n        # Skip known loader bugs\n        if input in skip_load:\n            continue\n\n        exp_type = test[0]\n        data     = test[1]\n        exp_dump = test[2]\n\n        # Test loading\n        try:\n            loaded = yaml.safe_load(input)\n        except:\n            print(\"Error:\", sys.exc_info()[0], '(', sys.exc_info()[1], ')')\n            fail+=1\n            _fail(input, test)\n            continue\n\n        if verbose:\n            print(input)\n            print(test)\n            print(loaded)\n            print(type(loaded))\n\n        if exp_type == 'null':\n            if loaded is None:\n                pass\n            else:\n                fail+=1\n                _fail(input, test)\n        else:\n            t = types[exp_type][0]\n            code = types[exp_type][1]\n            if isinstance(loaded, t):\n                if code(loaded, data):\n                    pass\n                else:\n                    fail+=1\n                    _fail(input, test)\n            else:\n                fail+=1\n                _fail(input, test)\n\n        # Skip known dumper bugs\n        if input in skip_dump:\n            continue\n\n        dump = yaml.safe_dump(loaded, explicit_end=False)\n        # strip trailing newlines and footers\n        if dump.endswith('\\n...\\n'):\n            dump = dump[:-5]\n        if dump.endswith('\\n'):\n            dump = dump[:-1]\n        if dump == exp_dump:\n            pass\n        else:\n            print(\"Compare: >>\" + dump + \"<< >>\" + exp_dump + \"<<\")\n            fail+=1\n            _fail(input, test)\n\n#        if i >= 80:\n#            break\n\n    if fail > 0:\n        print(\"Failed \" + str(fail) + \" / \" + str(i) + \" tests\")\n        assert(False)\n    else:\n        print(\"Passed \" + str(i) + \" tests\")\n    print(\"Skipped \" + str(len(skip_load)) + \" load tests\")\n    print(\"Skipped \" + str(len(skip_dump)) + \" dump tests\")\n\ntest_implicit_resolver.unittest = ['.schema', '.schema-skip']\n\nif __name__ == '__main__':\n    import test_appliance\n    test_appliance.run(globals())\n\n", "packaging/_pyyaml_pep517.py": "import inspect\n\n\ndef _bridge_build_meta():\n    import functools\n    import sys\n\n    from setuptools import build_meta\n\n    self_module = sys.modules[__name__]\n\n    for attr_name in build_meta.__all__:\n        attr_value = getattr(build_meta, attr_name)\n        if callable(attr_value):\n            setattr(self_module, attr_name, functools.partial(_expose_config_settings, attr_value))\n\n\nclass ActiveConfigSettings:\n    _current = {}\n\n    def __init__(self, config_settings):\n        self._config = config_settings\n\n    def __enter__(self):\n        type(self)._current = self._config\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        type(self)._current = {}\n\n    @classmethod\n    def current(cls):\n        return cls._current\n\n\ndef _expose_config_settings(real_method, *args, **kwargs):\n    from contextlib import nullcontext\n    import inspect\n\n    sig = inspect.signature(real_method)\n    boundargs = sig.bind(*args, **kwargs)\n\n    config = boundargs.arguments.get('config_settings')\n\n    ctx = ActiveConfigSettings(config) if config else nullcontext()\n\n    with ctx:\n        return real_method(*args, **kwargs)\n\n\n_bridge_build_meta()\n\n", "packaging/build/smoketest.py": "import sys\nimport yaml\n\n\ndef main():\n    # various smoke tests on an installed PyYAML with extension\n    if not getattr(yaml, '_yaml', None):\n        raise Exception('C extension is not available at `yaml._yaml`')\n\n    print('embedded libyaml version is {0}'.format(yaml._yaml.get_version_string()))\n\n    for loader, dumper in [(yaml.CLoader, yaml.CDumper), (yaml.Loader, yaml.Dumper)]:\n        testyaml = 'dude: mar'\n        loaded = yaml.load(testyaml, Loader=loader)\n        dumped = yaml.dump(loaded, Dumper=dumper)\n        if testyaml != dumped.strip():\n            raise Exception('roundtrip failed with {0}/{1}'.format(loader, dumper))\n    print('smoke test passed for {0}'.format(sys.executable))\n\n\nif __name__ == '__main__':\n    main()", "examples/pygments-lexer/yaml.py": "\n\"\"\"\nyaml.py\n\nLexer for YAML, a human-friendly data serialization language\n(http://yaml.org/).\n\nWritten by Kirill Simonov <xi@resolvent.net>.\n\nLicense: Whatever suitable for inclusion into the Pygments package.\n\"\"\"\n\nfrom pygments.lexer import  \\\n        ExtendedRegexLexer, LexerContext, include, bygroups\nfrom pygments.token import  \\\n        Text, Comment, Punctuation, Name, Literal\n\n__all__ = ['YAMLLexer']\n\n\nclass YAMLLexerContext(LexerContext):\n    \"\"\"Indentation context for the YAML lexer.\"\"\"\n\n    def __init__(self, *args, **kwds):\n        super(YAMLLexerContext, self).__init__(*args, **kwds)\n        self.indent_stack = []\n        self.indent = -1\n        self.next_indent = 0\n        self.block_scalar_indent = None\n\n\ndef something(TokenClass):\n    \"\"\"Do not produce empty tokens.\"\"\"\n    def callback(lexer, match, context):\n        text = match.group()\n        if not text:\n            return\n        yield match.start(), TokenClass, text\n        context.pos = match.end()\n    return callback\n\ndef reset_indent(TokenClass):\n    \"\"\"Reset the indentation levels.\"\"\"\n    def callback(lexer, match, context):\n        text = match.group()\n        context.indent_stack = []\n        context.indent = -1\n        context.next_indent = 0\n        context.block_scalar_indent = None\n        yield match.start(), TokenClass, text\n        context.pos = match.end()\n    return callback\n\ndef save_indent(TokenClass, start=False):\n    \"\"\"Save a possible indentation level.\"\"\"\n    def callback(lexer, match, context):\n        text = match.group()\n        extra = ''\n        if start:\n            context.next_indent = len(text)\n            if context.next_indent < context.indent:\n                while context.next_indent < context.indent:\n                    context.indent = context.indent_stack.pop()\n                if context.next_indent > context.indent:\n                    extra = text[context.indent:]\n                    text = text[:context.indent]\n        else:\n            context.next_indent += len(text)\n        if text:\n            yield match.start(), TokenClass, text\n        if extra:\n            yield match.start()+len(text), TokenClass.Error, extra\n        context.pos = match.end()\n    return callback\n\ndef set_indent(TokenClass, implicit=False):\n    \"\"\"Set the previously saved indentation level.\"\"\"\n    def callback(lexer, match, context):\n        text = match.group()\n        if context.indent < context.next_indent:\n            context.indent_stack.append(context.indent)\n            context.indent = context.next_indent\n        if not implicit:\n            context.next_indent += len(text)\n        yield match.start(), TokenClass, text\n        context.pos = match.end()\n    return callback\n\ndef set_block_scalar_indent(TokenClass):\n    \"\"\"Set an explicit indentation level for a block scalar.\"\"\"\n    def callback(lexer, match, context):\n        text = match.group()\n        context.block_scalar_indent = None\n        if not text:\n            return\n        increment = match.group(1)\n        if increment:\n            current_indent = max(context.indent, 0)\n            increment = int(increment)\n            context.block_scalar_indent = current_indent + increment\n        if text:\n            yield match.start(), TokenClass, text\n            context.pos = match.end()\n    return callback\n\ndef parse_block_scalar_empty_line(IndentTokenClass, ContentTokenClass):\n    \"\"\"Process an empty line in a block scalar.\"\"\"\n    def callback(lexer, match, context):\n        text = match.group()\n        if (context.block_scalar_indent is None or\n                len(text) <= context.block_scalar_indent):\n            if text:\n                yield match.start(), IndentTokenClass, text\n        else:\n            indentation = text[:context.block_scalar_indent]\n            content = text[context.block_scalar_indent:]\n            yield match.start(), IndentTokenClass, indentation\n            yield (match.start()+context.block_scalar_indent,\n                    ContentTokenClass, content)\n        context.pos = match.end()\n    return callback\n\ndef parse_block_scalar_indent(TokenClass):\n    \"\"\"Process indentation spaces in a block scalar.\"\"\"\n    def callback(lexer, match, context):\n        text = match.group()\n        if context.block_scalar_indent is None:\n            if len(text) <= max(context.indent, 0):\n                context.stack.pop()\n                context.stack.pop()\n                return\n            context.block_scalar_indent = len(text)\n        else:\n            if len(text) < context.block_scalar_indent:\n                context.stack.pop()\n                context.stack.pop()\n                return\n        if text:\n            yield match.start(), TokenClass, text\n            context.pos = match.end()\n    return callback\n\ndef parse_plain_scalar_indent(TokenClass):\n    \"\"\"Process indentation spaces in a plain scalar.\"\"\"\n    def callback(lexer, match, context):\n        text = match.group()\n        if len(text) <= context.indent:\n            context.stack.pop()\n            context.stack.pop()\n            return\n        if text:\n            yield match.start(), TokenClass, text\n            context.pos = match.end()\n    return callback\n\n\nclass YAMLLexer(ExtendedRegexLexer):\n    \"\"\"Lexer for the YAML language.\"\"\"\n\n    name = 'YAML'\n    aliases = ['yaml']\n    filenames = ['*.yaml', '*.yml']\n    mimetypes = ['text/x-yaml']\n\n    tokens = {\n\n        # the root rules\n        'root': [\n            # ignored whitespaces\n            (r'[ ]+(?=#|$)', Text.Blank),\n            # line breaks\n            (r'\\n+', Text.Break),\n            # a comment\n            (r'#[^\\n]*', Comment.Single),\n            # the '%YAML' directive\n            (r'^%YAML(?=[ ]|$)', reset_indent(Name.Directive),\n                'yaml-directive'),\n            # the %TAG directive\n            (r'^%TAG(?=[ ]|$)', reset_indent(Name.Directive),\n                'tag-directive'),\n            # document start and document end indicators\n            (r'^(?:---|\\.\\.\\.)(?=[ ]|$)',\n                reset_indent(Punctuation.Document), 'block-line'),\n            # indentation spaces\n            (r'[ ]*(?![ \\t\\n\\r\\f\\v]|$)',\n                save_indent(Text.Indent, start=True),\n                ('block-line', 'indentation')),\n        ],\n\n        # trailing whitespaces after directives or a block scalar indicator\n        'ignored-line': [\n            # ignored whitespaces\n            (r'[ ]+(?=#|$)', Text.Blank),\n            # a comment\n            (r'#[^\\n]*', Comment.Single),\n            # line break\n            (r'\\n', Text.Break, '#pop:2'),\n        ],\n\n        # the %YAML directive\n        'yaml-directive': [\n            # the version number\n            (r'([ ]+)([0-9]+\\.[0-9]+)',\n                bygroups(Text.Blank, Literal.Version), 'ignored-line'),\n        ],\n\n        # the %YAG directive\n        'tag-directive': [\n            # a tag handle and the corresponding prefix\n            (r'([ ]+)(!|![0-9A-Za-z_-]*!)'\n                r'([ ]+)(!|!?[0-9A-Za-z;/?:@&=+$,_.!~*\\'()\\[\\]%-]+)',\n                bygroups(Text.Blank, Name.Type, Text.Blank, Name.Type),\n                'ignored-line'),\n        ],\n\n        # block scalar indicators and indentation spaces\n        'indentation': [\n            # trailing whitespaces are ignored\n            (r'[ ]*$', something(Text.Blank), '#pop:2'),\n            # whitespaces preceding block collection indicators\n            (r'[ ]+(?=[?:-](?:[ ]|$))', save_indent(Text.Indent)),\n            # block collection indicators\n            (r'[?:-](?=[ ]|$)', set_indent(Punctuation.Indicator)),\n            # the beginning a block line\n            (r'[ ]*', save_indent(Text.Indent), '#pop'),\n        ],\n\n        # an indented line in the block context\n        'block-line': [\n            # the line end\n            (r'[ ]*(?=#|$)', something(Text.Blank), '#pop'),\n            # whitespaces separating tokens\n            (r'[ ]+', Text.Blank),\n            # tags, anchors and aliases,\n            include('descriptors'),\n            # block collections and scalars\n            include('block-nodes'),\n            # flow collections and quoted scalars\n            include('flow-nodes'),\n            # a plain scalar\n            (r'(?=[^ \\t\\n\\r\\f\\v?:,\\[\\]{}#&*!|>\\'\"%@`-]|[?:-][^ \\t\\n\\r\\f\\v])',\n                something(Literal.Scalar.Plain),\n                'plain-scalar-in-block-context'),\n        ],\n\n        # tags, anchors, aliases\n        'descriptors' : [\n            # a full-form tag\n            (r'!<[0-9A-Za-z;/?:@&=+$,_.!~*\\'()\\[\\]%-]+>', Name.Type),\n            # a tag in the form '!', '!suffix' or '!handle!suffix'\n            (r'!(?:[0-9A-Za-z_-]+)?'\n                r'(?:![0-9A-Za-z;/?:@&=+$,_.!~*\\'()\\[\\]%-]+)?', Name.Type),\n            # an anchor\n            (r'&[0-9A-Za-z_-]+', Name.Anchor),\n            # an alias\n            (r'\\*[0-9A-Za-z_-]+', Name.Alias),\n        ],\n\n        # block collections and scalars\n        'block-nodes': [\n            # implicit key\n            (r':(?=[ ]|$)', set_indent(Punctuation.Indicator, implicit=True)),\n            # literal and folded scalars\n            (r'[|>]', Punctuation.Indicator,\n                ('block-scalar-content', 'block-scalar-header')),\n        ],\n\n        # flow collections and quoted scalars\n        'flow-nodes': [\n            # a flow sequence\n            (r'\\[', Punctuation.Indicator, 'flow-sequence'),\n            # a flow mapping\n            (r'\\{', Punctuation.Indicator, 'flow-mapping'),\n            # a single-quoted scalar\n            (r'\\'', Literal.Scalar.Flow.Quote, 'single-quoted-scalar'),\n            # a double-quoted scalar\n            (r'\\\"', Literal.Scalar.Flow.Quote, 'double-quoted-scalar'),\n        ],\n\n        # the content of a flow collection\n        'flow-collection': [\n            # whitespaces\n            (r'[ ]+', Text.Blank),\n            # line breaks\n            (r'\\n+', Text.Break),\n            # a comment\n            (r'#[^\\n]*', Comment.Single),\n            # simple indicators\n            (r'[?:,]', Punctuation.Indicator),\n            # tags, anchors and aliases\n            include('descriptors'),\n            # nested collections and quoted scalars\n            include('flow-nodes'),\n            # a plain scalar\n            (r'(?=[^ \\t\\n\\r\\f\\v?:,\\[\\]{}#&*!|>\\'\"%@`])',\n                something(Literal.Scalar.Plain),\n                'plain-scalar-in-flow-context'),\n        ],\n\n        # a flow sequence indicated by '[' and ']'\n        'flow-sequence': [\n            # include flow collection rules\n            include('flow-collection'),\n            # the closing indicator\n            (r'\\]', Punctuation.Indicator, '#pop'),\n        ],\n\n        # a flow mapping indicated by '{' and '}'\n        'flow-mapping': [\n            # include flow collection rules\n            include('flow-collection'),\n            # the closing indicator\n            (r'\\}', Punctuation.Indicator, '#pop'),\n        ],\n\n        # block scalar lines\n        'block-scalar-content': [\n            # line break\n            (r'\\n', Text.Break),\n            # empty line\n            (r'^[ ]+$',\n                parse_block_scalar_empty_line(Text.Indent,\n                    Literal.Scalar.Block)),\n            # indentation spaces (we may leave the state here)\n            (r'^[ ]*', parse_block_scalar_indent(Text.Indent)),\n            # line content\n            (r'[^\\n\\r\\f\\v]+', Literal.Scalar.Block),\n        ],\n\n        # the content of a literal or folded scalar\n        'block-scalar-header': [\n            # indentation indicator followed by chomping flag\n            (r'([1-9])?[+-]?(?=[ ]|$)',\n                set_block_scalar_indent(Punctuation.Indicator),\n                'ignored-line'),\n            # chomping flag followed by indentation indicator\n            (r'[+-]?([1-9])?(?=[ ]|$)',\n                set_block_scalar_indent(Punctuation.Indicator),\n                'ignored-line'),\n        ],\n\n        # ignored and regular whitespaces in quoted scalars\n        'quoted-scalar-whitespaces': [\n            # leading and trailing whitespaces are ignored\n            (r'^[ ]+|[ ]+$', Text.Blank),\n            # line breaks are ignored\n            (r'\\n+', Text.Break),\n            # other whitespaces are a part of the value\n            (r'[ ]+', Literal.Scalar.Flow),\n        ],\n\n        # single-quoted scalars\n        'single-quoted-scalar': [\n            # include whitespace and line break rules\n            include('quoted-scalar-whitespaces'),\n            # escaping of the quote character\n            (r'\\'\\'', Literal.Scalar.Flow.Escape),\n            # regular non-whitespace characters\n            (r'[^ \\t\\n\\r\\f\\v\\']+', Literal.Scalar.Flow),\n            # the closing quote\n            (r'\\'', Literal.Scalar.Flow.Quote, '#pop'),\n        ],\n\n        # double-quoted scalars\n        'double-quoted-scalar': [\n            # include whitespace and line break rules\n            include('quoted-scalar-whitespaces'),\n            # escaping of special characters\n            (r'\\\\[0abt\\tn\\nvfre \"\\\\N_LP]', Literal.Scalar.Flow.Escape),\n            # escape codes\n            (r'\\\\(?:x[0-9A-Fa-f]{2}|u[0-9A-Fa-f]{4}|U[0-9A-Fa-f]{8})',\n                Literal.Scalar.Flow.Escape),\n            # regular non-whitespace characters\n            (r'[^ \\t\\n\\r\\f\\v\\\"\\\\]+', Literal.Scalar.Flow),\n            # the closing quote\n            (r'\"', Literal.Scalar.Flow.Quote, '#pop'),\n        ],\n\n        # the beginning of a new line while scanning a plain scalar\n        'plain-scalar-in-block-context-new-line': [\n            # empty lines\n            (r'^[ ]+$', Text.Blank),\n            # line breaks\n            (r'\\n+', Text.Break),\n            # document start and document end indicators\n            (r'^(?=---|\\.\\.\\.)', something(Punctuation.Document), '#pop:3'),\n            # indentation spaces (we may leave the block line state here)\n            (r'^[ ]*', parse_plain_scalar_indent(Text.Indent), '#pop'),\n        ],\n\n        # a plain scalar in the block context\n        'plain-scalar-in-block-context': [\n            # the scalar ends with the ':' indicator\n            (r'[ ]*(?=:[ ]|:$)', something(Text.Blank), '#pop'),\n            # the scalar ends with whitespaces followed by a comment\n            (r'[ ]+(?=#)', Text.Blank, '#pop'),\n            # trailing whitespaces are ignored\n            (r'[ ]+$', Text.Blank),\n            # line breaks are ignored\n            (r'\\n+', Text.Break, 'plain-scalar-in-block-context-new-line'),\n            # other whitespaces are a part of the value\n            (r'[ ]+', Literal.Scalar.Plain),\n            # regular non-whitespace characters\n            (r'(?::(?![ \\t\\n\\r\\f\\v])|[^ \\t\\n\\r\\f\\v:])+',\n                Literal.Scalar.Plain),\n        ],\n\n        # a plain scalar is the flow context\n        'plain-scalar-in-flow-context': [\n            # the scalar ends with an indicator character\n            (r'[ ]*(?=[,:?\\[\\]{}])', something(Text.Blank), '#pop'),\n            # the scalar ends with a comment\n            (r'[ ]+(?=#)', Text.Blank, '#pop'),\n            # leading and trailing whitespaces are ignored\n            (r'^[ ]+|[ ]+$', Text.Blank),\n            # line breaks are ignored\n            (r'\\n+', Text.Break),\n            # other whitespaces are a part of the value\n            (r'[ ]+', Literal.Scalar.Plain),\n            # regular non-whitespace characters\n            (r'[^ \\t\\n\\r\\f\\v,:?\\[\\]{}]+', Literal.Scalar.Plain),\n        ],\n\n    }\n\n    def get_tokens_unprocessed(self, text=None, context=None):\n        if context is None:\n            context = YAMLLexerContext(text, 0)\n        return super(YAMLLexer, self).get_tokens_unprocessed(text, context)\n\n\n", "examples/yaml-highlight/yaml_hl.py": "#!/usr/bin/python\n\nimport yaml, codecs, sys, os.path, optparse\n\nclass Style:\n\n    def __init__(self, header=None, footer=None,\n            tokens=None, events=None, replaces=None):\n        self.header = header\n        self.footer = footer\n        self.replaces = replaces\n        self.substitutions = {}\n        for domain, Class in [(tokens, 'Token'), (events, 'Event')]:\n            if not domain:\n                continue\n            for key in domain:\n                name = ''.join([part.capitalize() for part in key.split('-')])\n                cls = getattr(yaml, '%s%s' % (name, Class))\n                value = domain[key]\n                if not value:\n                    continue\n                start = value.get('start')\n                end = value.get('end')\n                if start:\n                    self.substitutions[cls, -1] = start\n                if end:\n                    self.substitutions[cls, +1] = end\n\n    def __setstate__(self, state):\n        self.__init__(**state)\n\nyaml.add_path_resolver(u'tag:yaml.org,2002:python/object:__main__.Style',\n        [None], dict)\nyaml.add_path_resolver(u'tag:yaml.org,2002:pairs',\n        [None, u'replaces'], list)\n\nclass YAMLHighlight:\n\n    def __init__(self, options):\n        config = yaml.full_load(file(options.config, 'rb').read())\n        self.style = config[options.style]\n        if options.input:\n            self.input = file(options.input, 'rb')\n        else:\n            self.input = sys.stdin\n        if options.output:\n            self.output = file(options.output, 'wb')\n        else:\n            self.output = sys.stdout\n\n    def highlight(self):\n        input = self.input.read()\n        if input.startswith(codecs.BOM_UTF16_LE):\n            input = unicode(input, 'utf-16-le')\n        elif input.startswith(codecs.BOM_UTF16_BE):\n            input = unicode(input, 'utf-16-be')\n        else:\n            input = unicode(input, 'utf-8')\n        substitutions = self.style.substitutions\n        tokens = yaml.scan(input)\n        events = yaml.parse(input)\n        markers = []\n        number = 0\n        for token in tokens:\n            number += 1\n            if token.start_mark.index != token.end_mark.index:\n                cls = token.__class__\n                if (cls, -1) in substitutions:\n                    markers.append([token.start_mark.index, +2, number, substitutions[cls, -1]])\n                if (cls, +1) in substitutions:\n                    markers.append([token.end_mark.index, -2, number, substitutions[cls, +1]])\n        number = 0\n        for event in events:\n            number += 1\n            cls = event.__class__\n            if (cls, -1) in substitutions:\n                markers.append([event.start_mark.index, +1, number, substitutions[cls, -1]])\n            if (cls, +1) in substitutions:\n                markers.append([event.end_mark.index, -1, number, substitutions[cls, +1]])\n        markers.sort()\n        markers.reverse()\n        chunks = []\n        position = len(input)\n        for index, weight1, weight2, substitution in markers:\n            if index < position:\n                chunk = input[index:position]\n                for substring, replacement in self.style.replaces:\n                    chunk = chunk.replace(substring, replacement)\n                chunks.append(chunk)\n                position = index\n            chunks.append(substitution)\n        chunks.reverse()\n        result = u''.join(chunks)\n        if self.style.header:\n            self.output.write(self.style.header)\n        self.output.write(result.encode('utf-8'))\n        if self.style.footer:\n            self.output.write(self.style.footer)\n\nif __name__ == '__main__':\n    parser = optparse.OptionParser()\n    parser.add_option('-s', '--style', dest='style', default='ascii',\n            help=\"specify the highlighting style\", metavar='STYLE')\n    parser.add_option('-c', '--config', dest='config',\n            default=os.path.join(os.path.dirname(sys.argv[0]), 'yaml_hl.cfg'),\n            help=\"set an alternative configuration file\", metavar='CONFIG')\n    parser.add_option('-i', '--input', dest='input', default=None,\n            help=\"set the input file (default: stdin)\", metavar='FILE')\n    parser.add_option('-o', '--output', dest='output', default=None,\n            help=\"set the output file (default: stdout)\", metavar='FILE')\n    (options, args) = parser.parse_args()\n    hl = YAMLHighlight(options)\n    hl.highlight()\n\n", "lib/_yaml/__init__.py": "# This is a stub package designed to roughly emulate the _yaml\n# extension module, which previously existed as a standalone module\n# and has been moved into the `yaml` package namespace.\n# It does not perfectly mimic its old counterpart, but should get\n# close enough for anyone who's relying on it even when they shouldn't.\nimport yaml\n\n# in some circumstances, the yaml module we imoprted may be from a different version, so we need\n# to tread carefully when poking at it here (it may not have the attributes we expect)\nif not getattr(yaml, '__with_libyaml__', False):\n    from sys import version_info\n\n    exc = ModuleNotFoundError if version_info >= (3, 6) else ImportError\n    raise exc(\"No module named '_yaml'\")\nelse:\n    from yaml._yaml import *\n    import warnings\n    warnings.warn(\n        'The _yaml extension module is now located at yaml._yaml'\n        ' and its location is subject to change.  To use the'\n        ' LibYAML-based parser and emitter, import from `yaml`:'\n        ' `from yaml import CLoader as Loader, CDumper as Dumper`.',\n        DeprecationWarning\n    )\n    del warnings\n    # Don't `del yaml` here because yaml is actually an existing\n    # namespace member of _yaml.\n\n__name__ = '_yaml'\n# If the module is top-level (i.e. not a part of any specific package)\n# then the attribute should be set to ''.\n# https://docs.python.org/3.8/library/types.html\n__package__ = ''\n", "lib/yaml/tokens.py": "\nclass Token(object):\n    def __init__(self, start_mark, end_mark):\n        self.start_mark = start_mark\n        self.end_mark = end_mark\n    def __repr__(self):\n        attributes = [key for key in self.__dict__\n                if not key.endswith('_mark')]\n        attributes.sort()\n        arguments = ', '.join(['%s=%r' % (key, getattr(self, key))\n                for key in attributes])\n        return '%s(%s)' % (self.__class__.__name__, arguments)\n\n#class BOMToken(Token):\n#    id = '<byte order mark>'\n\nclass DirectiveToken(Token):\n    id = '<directive>'\n    def __init__(self, name, value, start_mark, end_mark):\n        self.name = name\n        self.value = value\n        self.start_mark = start_mark\n        self.end_mark = end_mark\n\nclass DocumentStartToken(Token):\n    id = '<document start>'\n\nclass DocumentEndToken(Token):\n    id = '<document end>'\n\nclass StreamStartToken(Token):\n    id = '<stream start>'\n    def __init__(self, start_mark=None, end_mark=None,\n            encoding=None):\n        self.start_mark = start_mark\n        self.end_mark = end_mark\n        self.encoding = encoding\n\nclass StreamEndToken(Token):\n    id = '<stream end>'\n\nclass BlockSequenceStartToken(Token):\n    id = '<block sequence start>'\n\nclass BlockMappingStartToken(Token):\n    id = '<block mapping start>'\n\nclass BlockEndToken(Token):\n    id = '<block end>'\n\nclass FlowSequenceStartToken(Token):\n    id = '['\n\nclass FlowMappingStartToken(Token):\n    id = '{'\n\nclass FlowSequenceEndToken(Token):\n    id = ']'\n\nclass FlowMappingEndToken(Token):\n    id = '}'\n\nclass KeyToken(Token):\n    id = '?'\n\nclass ValueToken(Token):\n    id = ':'\n\nclass BlockEntryToken(Token):\n    id = '-'\n\nclass FlowEntryToken(Token):\n    id = ','\n\nclass AliasToken(Token):\n    id = '<alias>'\n    def __init__(self, value, start_mark, end_mark):\n        self.value = value\n        self.start_mark = start_mark\n        self.end_mark = end_mark\n\nclass AnchorToken(Token):\n    id = '<anchor>'\n    def __init__(self, value, start_mark, end_mark):\n        self.value = value\n        self.start_mark = start_mark\n        self.end_mark = end_mark\n\nclass TagToken(Token):\n    id = '<tag>'\n    def __init__(self, value, start_mark, end_mark):\n        self.value = value\n        self.start_mark = start_mark\n        self.end_mark = end_mark\n\nclass ScalarToken(Token):\n    id = '<scalar>'\n    def __init__(self, value, plain, start_mark, end_mark, style=None):\n        self.value = value\n        self.plain = plain\n        self.start_mark = start_mark\n        self.end_mark = end_mark\n        self.style = style\n\n", "lib/yaml/constructor.py": "\n__all__ = [\n    'BaseConstructor',\n    'SafeConstructor',\n    'FullConstructor',\n    'UnsafeConstructor',\n    'Constructor',\n    'ConstructorError'\n]\n\nfrom .error import *\nfrom .nodes import *\n\nimport collections.abc, datetime, base64, binascii, re, sys, types\n\nclass ConstructorError(MarkedYAMLError):\n    pass\n\nclass BaseConstructor:\n\n    yaml_constructors = {}\n    yaml_multi_constructors = {}\n\n    def __init__(self):\n        self.constructed_objects = {}\n        self.recursive_objects = {}\n        self.state_generators = []\n        self.deep_construct = False\n\n    def check_data(self):\n        # If there are more documents available?\n        return self.check_node()\n\n    def check_state_key(self, key):\n        \"\"\"Block special attributes/methods from being set in a newly created\n        object, to prevent user-controlled methods from being called during\n        deserialization\"\"\"\n        if self.get_state_keys_blacklist_regexp().match(key):\n            raise ConstructorError(None, None,\n                \"blacklisted key '%s' in instance state found\" % (key,), None)\n\n    def get_data(self):\n        # Construct and return the next document.\n        if self.check_node():\n            return self.construct_document(self.get_node())\n\n    def get_single_data(self):\n        # Ensure that the stream contains a single document and construct it.\n        node = self.get_single_node()\n        if node is not None:\n            return self.construct_document(node)\n        return None\n\n    def construct_document(self, node):\n        data = self.construct_object(node)\n        while self.state_generators:\n            state_generators = self.state_generators\n            self.state_generators = []\n            for generator in state_generators:\n                for dummy in generator:\n                    pass\n        self.constructed_objects = {}\n        self.recursive_objects = {}\n        self.deep_construct = False\n        return data\n\n    def construct_object(self, node, deep=False):\n        if node in self.constructed_objects:\n            return self.constructed_objects[node]\n        if deep:\n            old_deep = self.deep_construct\n            self.deep_construct = True\n        if node in self.recursive_objects:\n            raise ConstructorError(None, None,\n                    \"found unconstructable recursive node\", node.start_mark)\n        self.recursive_objects[node] = None\n        constructor = None\n        tag_suffix = None\n        if node.tag in self.yaml_constructors:\n            constructor = self.yaml_constructors[node.tag]\n        else:\n            for tag_prefix in self.yaml_multi_constructors:\n                if tag_prefix is not None and node.tag.startswith(tag_prefix):\n                    tag_suffix = node.tag[len(tag_prefix):]\n                    constructor = self.yaml_multi_constructors[tag_prefix]\n                    break\n            else:\n                if None in self.yaml_multi_constructors:\n                    tag_suffix = node.tag\n                    constructor = self.yaml_multi_constructors[None]\n                elif None in self.yaml_constructors:\n                    constructor = self.yaml_constructors[None]\n                elif isinstance(node, ScalarNode):\n                    constructor = self.__class__.construct_scalar\n                elif isinstance(node, SequenceNode):\n                    constructor = self.__class__.construct_sequence\n                elif isinstance(node, MappingNode):\n                    constructor = self.__class__.construct_mapping\n        if tag_suffix is None:\n            data = constructor(self, node)\n        else:\n            data = constructor(self, tag_suffix, node)\n        if isinstance(data, types.GeneratorType):\n            generator = data\n            data = next(generator)\n            if self.deep_construct:\n                for dummy in generator:\n                    pass\n            else:\n                self.state_generators.append(generator)\n        self.constructed_objects[node] = data\n        del self.recursive_objects[node]\n        if deep:\n            self.deep_construct = old_deep\n        return data\n\n    def construct_scalar(self, node):\n        if not isinstance(node, ScalarNode):\n            raise ConstructorError(None, None,\n                    \"expected a scalar node, but found %s\" % node.id,\n                    node.start_mark)\n        return node.value\n\n    def construct_sequence(self, node, deep=False):\n        if not isinstance(node, SequenceNode):\n            raise ConstructorError(None, None,\n                    \"expected a sequence node, but found %s\" % node.id,\n                    node.start_mark)\n        return [self.construct_object(child, deep=deep)\n                for child in node.value]\n\n    def construct_mapping(self, node, deep=False):\n        if not isinstance(node, MappingNode):\n            raise ConstructorError(None, None,\n                    \"expected a mapping node, but found %s\" % node.id,\n                    node.start_mark)\n        mapping = {}\n        for key_node, value_node in node.value:\n            key = self.construct_object(key_node, deep=deep)\n            if not isinstance(key, collections.abc.Hashable):\n                raise ConstructorError(\"while constructing a mapping\", node.start_mark,\n                        \"found unhashable key\", key_node.start_mark)\n            value = self.construct_object(value_node, deep=deep)\n            mapping[key] = value\n        return mapping\n\n    def construct_pairs(self, node, deep=False):\n        if not isinstance(node, MappingNode):\n            raise ConstructorError(None, None,\n                    \"expected a mapping node, but found %s\" % node.id,\n                    node.start_mark)\n        pairs = []\n        for key_node, value_node in node.value:\n            key = self.construct_object(key_node, deep=deep)\n            value = self.construct_object(value_node, deep=deep)\n            pairs.append((key, value))\n        return pairs\n\n    @classmethod\n    def add_constructor(cls, tag, constructor):\n        if not 'yaml_constructors' in cls.__dict__:\n            cls.yaml_constructors = cls.yaml_constructors.copy()\n        cls.yaml_constructors[tag] = constructor\n\n    @classmethod\n    def add_multi_constructor(cls, tag_prefix, multi_constructor):\n        if not 'yaml_multi_constructors' in cls.__dict__:\n            cls.yaml_multi_constructors = cls.yaml_multi_constructors.copy()\n        cls.yaml_multi_constructors[tag_prefix] = multi_constructor\n\nclass SafeConstructor(BaseConstructor):\n\n    def construct_scalar(self, node):\n        if isinstance(node, MappingNode):\n            for key_node, value_node in node.value:\n                if key_node.tag == 'tag:yaml.org,2002:value':\n                    return self.construct_scalar(value_node)\n        return super().construct_scalar(node)\n\n    def flatten_mapping(self, node):\n        merge = []\n        index = 0\n        while index < len(node.value):\n            key_node, value_node = node.value[index]\n            if key_node.tag == 'tag:yaml.org,2002:merge':\n                del node.value[index]\n                if isinstance(value_node, MappingNode):\n                    self.flatten_mapping(value_node)\n                    merge.extend(value_node.value)\n                elif isinstance(value_node, SequenceNode):\n                    submerge = []\n                    for subnode in value_node.value:\n                        if not isinstance(subnode, MappingNode):\n                            raise ConstructorError(\"while constructing a mapping\",\n                                    node.start_mark,\n                                    \"expected a mapping for merging, but found %s\"\n                                    % subnode.id, subnode.start_mark)\n                        self.flatten_mapping(subnode)\n                        submerge.append(subnode.value)\n                    submerge.reverse()\n                    for value in submerge:\n                        merge.extend(value)\n                else:\n                    raise ConstructorError(\"while constructing a mapping\", node.start_mark,\n                            \"expected a mapping or list of mappings for merging, but found %s\"\n                            % value_node.id, value_node.start_mark)\n            elif key_node.tag == 'tag:yaml.org,2002:value':\n                key_node.tag = 'tag:yaml.org,2002:str'\n                index += 1\n            else:\n                index += 1\n        if merge:\n            node.value = merge + node.value\n\n    def construct_mapping(self, node, deep=False):\n        if isinstance(node, MappingNode):\n            self.flatten_mapping(node)\n        return super().construct_mapping(node, deep=deep)\n\n    def construct_yaml_null(self, node):\n        self.construct_scalar(node)\n        return None\n\n    bool_values = {\n        'yes':      True,\n        'no':       False,\n        'true':     True,\n        'false':    False,\n        'on':       True,\n        'off':      False,\n    }\n\n    def construct_yaml_bool(self, node):\n        value = self.construct_scalar(node)\n        return self.bool_values[value.lower()]\n\n    def construct_yaml_int(self, node):\n        value = self.construct_scalar(node)\n        value = value.replace('_', '')\n        sign = +1\n        if value[0] == '-':\n            sign = -1\n        if value[0] in '+-':\n            value = value[1:]\n        if value == '0':\n            return 0\n        elif value.startswith('0b'):\n            return sign*int(value[2:], 2)\n        elif value.startswith('0x'):\n            return sign*int(value[2:], 16)\n        elif value[0] == '0':\n            return sign*int(value, 8)\n        elif ':' in value:\n            digits = [int(part) for part in value.split(':')]\n            digits.reverse()\n            base = 1\n            value = 0\n            for digit in digits:\n                value += digit*base\n                base *= 60\n            return sign*value\n        else:\n            return sign*int(value)\n\n    inf_value = 1e300\n    while inf_value != inf_value*inf_value:\n        inf_value *= inf_value\n    nan_value = -inf_value/inf_value   # Trying to make a quiet NaN (like C99).\n\n    def construct_yaml_float(self, node):\n        value = self.construct_scalar(node)\n        value = value.replace('_', '').lower()\n        sign = +1\n        if value[0] == '-':\n            sign = -1\n        if value[0] in '+-':\n            value = value[1:]\n        if value == '.inf':\n            return sign*self.inf_value\n        elif value == '.nan':\n            return self.nan_value\n        elif ':' in value:\n            digits = [float(part) for part in value.split(':')]\n            digits.reverse()\n            base = 1\n            value = 0.0\n            for digit in digits:\n                value += digit*base\n                base *= 60\n            return sign*value\n        else:\n            return sign*float(value)\n\n    def construct_yaml_binary(self, node):\n        try:\n            value = self.construct_scalar(node).encode('ascii')\n        except UnicodeEncodeError as exc:\n            raise ConstructorError(None, None,\n                    \"failed to convert base64 data into ascii: %s\" % exc,\n                    node.start_mark)\n        try:\n            if hasattr(base64, 'decodebytes'):\n                return base64.decodebytes(value)\n            else:\n                return base64.decodestring(value)\n        except binascii.Error as exc:\n            raise ConstructorError(None, None,\n                    \"failed to decode base64 data: %s\" % exc, node.start_mark)\n\n    timestamp_regexp = re.compile(\n            r'''^(?P<year>[0-9][0-9][0-9][0-9])\n                -(?P<month>[0-9][0-9]?)\n                -(?P<day>[0-9][0-9]?)\n                (?:(?:[Tt]|[ \\t]+)\n                (?P<hour>[0-9][0-9]?)\n                :(?P<minute>[0-9][0-9])\n                :(?P<second>[0-9][0-9])\n                (?:\\.(?P<fraction>[0-9]*))?\n                (?:[ \\t]*(?P<tz>Z|(?P<tz_sign>[-+])(?P<tz_hour>[0-9][0-9]?)\n                (?::(?P<tz_minute>[0-9][0-9]))?))?)?$''', re.X)\n\n    def construct_yaml_timestamp(self, node):\n        value = self.construct_scalar(node)\n        match = self.timestamp_regexp.match(node.value)\n        values = match.groupdict()\n        year = int(values['year'])\n        month = int(values['month'])\n        day = int(values['day'])\n        if not values['hour']:\n            return datetime.date(year, month, day)\n        hour = int(values['hour'])\n        minute = int(values['minute'])\n        second = int(values['second'])\n        fraction = 0\n        tzinfo = None\n        if values['fraction']:\n            fraction = values['fraction'][:6]\n            while len(fraction) < 6:\n                fraction += '0'\n            fraction = int(fraction)\n        if values['tz_sign']:\n            tz_hour = int(values['tz_hour'])\n            tz_minute = int(values['tz_minute'] or 0)\n            delta = datetime.timedelta(hours=tz_hour, minutes=tz_minute)\n            if values['tz_sign'] == '-':\n                delta = -delta\n            tzinfo = datetime.timezone(delta)\n        elif values['tz']:\n            tzinfo = datetime.timezone.utc\n        return datetime.datetime(year, month, day, hour, minute, second, fraction,\n                                 tzinfo=tzinfo)\n\n    def construct_yaml_omap(self, node):\n        # Note: we do not check for duplicate keys, because it's too\n        # CPU-expensive.\n        omap = []\n        yield omap\n        if not isinstance(node, SequenceNode):\n            raise ConstructorError(\"while constructing an ordered map\", node.start_mark,\n                    \"expected a sequence, but found %s\" % node.id, node.start_mark)\n        for subnode in node.value:\n            if not isinstance(subnode, MappingNode):\n                raise ConstructorError(\"while constructing an ordered map\", node.start_mark,\n                        \"expected a mapping of length 1, but found %s\" % subnode.id,\n                        subnode.start_mark)\n            if len(subnode.value) != 1:\n                raise ConstructorError(\"while constructing an ordered map\", node.start_mark,\n                        \"expected a single mapping item, but found %d items\" % len(subnode.value),\n                        subnode.start_mark)\n            key_node, value_node = subnode.value[0]\n            key = self.construct_object(key_node)\n            value = self.construct_object(value_node)\n            omap.append((key, value))\n\n    def construct_yaml_pairs(self, node):\n        # Note: the same code as `construct_yaml_omap`.\n        pairs = []\n        yield pairs\n        if not isinstance(node, SequenceNode):\n            raise ConstructorError(\"while constructing pairs\", node.start_mark,\n                    \"expected a sequence, but found %s\" % node.id, node.start_mark)\n        for subnode in node.value:\n            if not isinstance(subnode, MappingNode):\n                raise ConstructorError(\"while constructing pairs\", node.start_mark,\n                        \"expected a mapping of length 1, but found %s\" % subnode.id,\n                        subnode.start_mark)\n            if len(subnode.value) != 1:\n                raise ConstructorError(\"while constructing pairs\", node.start_mark,\n                        \"expected a single mapping item, but found %d items\" % len(subnode.value),\n                        subnode.start_mark)\n            key_node, value_node = subnode.value[0]\n            key = self.construct_object(key_node)\n            value = self.construct_object(value_node)\n            pairs.append((key, value))\n\n    def construct_yaml_set(self, node):\n        data = set()\n        yield data\n        value = self.construct_mapping(node)\n        data.update(value)\n\n    def construct_yaml_str(self, node):\n        return self.construct_scalar(node)\n\n    def construct_yaml_seq(self, node):\n        data = []\n        yield data\n        data.extend(self.construct_sequence(node))\n\n    def construct_yaml_map(self, node):\n        data = {}\n        yield data\n        value = self.construct_mapping(node)\n        data.update(value)\n\n    def construct_yaml_object(self, node, cls):\n        data = cls.__new__(cls)\n        yield data\n        if hasattr(data, '__setstate__'):\n            state = self.construct_mapping(node, deep=True)\n            data.__setstate__(state)\n        else:\n            state = self.construct_mapping(node)\n            data.__dict__.update(state)\n\n    def construct_undefined(self, node):\n        raise ConstructorError(None, None,\n                \"could not determine a constructor for the tag %r\" % node.tag,\n                node.start_mark)\n\nSafeConstructor.add_constructor(\n        'tag:yaml.org,2002:null',\n        SafeConstructor.construct_yaml_null)\n\nSafeConstructor.add_constructor(\n        'tag:yaml.org,2002:bool',\n        SafeConstructor.construct_yaml_bool)\n\nSafeConstructor.add_constructor(\n        'tag:yaml.org,2002:int',\n        SafeConstructor.construct_yaml_int)\n\nSafeConstructor.add_constructor(\n        'tag:yaml.org,2002:float',\n        SafeConstructor.construct_yaml_float)\n\nSafeConstructor.add_constructor(\n        'tag:yaml.org,2002:binary',\n        SafeConstructor.construct_yaml_binary)\n\nSafeConstructor.add_constructor(\n        'tag:yaml.org,2002:timestamp',\n        SafeConstructor.construct_yaml_timestamp)\n\nSafeConstructor.add_constructor(\n        'tag:yaml.org,2002:omap',\n        SafeConstructor.construct_yaml_omap)\n\nSafeConstructor.add_constructor(\n        'tag:yaml.org,2002:pairs',\n        SafeConstructor.construct_yaml_pairs)\n\nSafeConstructor.add_constructor(\n        'tag:yaml.org,2002:set',\n        SafeConstructor.construct_yaml_set)\n\nSafeConstructor.add_constructor(\n        'tag:yaml.org,2002:str',\n        SafeConstructor.construct_yaml_str)\n\nSafeConstructor.add_constructor(\n        'tag:yaml.org,2002:seq',\n        SafeConstructor.construct_yaml_seq)\n\nSafeConstructor.add_constructor(\n        'tag:yaml.org,2002:map',\n        SafeConstructor.construct_yaml_map)\n\nSafeConstructor.add_constructor(None,\n        SafeConstructor.construct_undefined)\n\nclass FullConstructor(SafeConstructor):\n    # 'extend' is blacklisted because it is used by\n    # construct_python_object_apply to add `listitems` to a newly generate\n    # python instance\n    def get_state_keys_blacklist(self):\n        return ['^extend$', '^__.*__$']\n\n    def get_state_keys_blacklist_regexp(self):\n        if not hasattr(self, 'state_keys_blacklist_regexp'):\n            self.state_keys_blacklist_regexp = re.compile('(' + '|'.join(self.get_state_keys_blacklist()) + ')')\n        return self.state_keys_blacklist_regexp\n\n    def construct_python_str(self, node):\n        return self.construct_scalar(node)\n\n    def construct_python_unicode(self, node):\n        return self.construct_scalar(node)\n\n    def construct_python_bytes(self, node):\n        try:\n            value = self.construct_scalar(node).encode('ascii')\n        except UnicodeEncodeError as exc:\n            raise ConstructorError(None, None,\n                    \"failed to convert base64 data into ascii: %s\" % exc,\n                    node.start_mark)\n        try:\n            if hasattr(base64, 'decodebytes'):\n                return base64.decodebytes(value)\n            else:\n                return base64.decodestring(value)\n        except binascii.Error as exc:\n            raise ConstructorError(None, None,\n                    \"failed to decode base64 data: %s\" % exc, node.start_mark)\n\n    def construct_python_long(self, node):\n        return self.construct_yaml_int(node)\n\n    def construct_python_complex(self, node):\n       return complex(self.construct_scalar(node))\n\n    def construct_python_tuple(self, node):\n        return tuple(self.construct_sequence(node))\n\n    def find_python_module(self, name, mark, unsafe=False):\n        if not name:\n            raise ConstructorError(\"while constructing a Python module\", mark,\n                    \"expected non-empty name appended to the tag\", mark)\n        if unsafe:\n            try:\n                __import__(name)\n            except ImportError as exc:\n                raise ConstructorError(\"while constructing a Python module\", mark,\n                        \"cannot find module %r (%s)\" % (name, exc), mark)\n        if name not in sys.modules:\n            raise ConstructorError(\"while constructing a Python module\", mark,\n                    \"module %r is not imported\" % name, mark)\n        return sys.modules[name]\n\n    def find_python_name(self, name, mark, unsafe=False):\n        if not name:\n            raise ConstructorError(\"while constructing a Python object\", mark,\n                    \"expected non-empty name appended to the tag\", mark)\n        if '.' in name:\n            module_name, object_name = name.rsplit('.', 1)\n        else:\n            module_name = 'builtins'\n            object_name = name\n        if unsafe:\n            try:\n                __import__(module_name)\n            except ImportError as exc:\n                raise ConstructorError(\"while constructing a Python object\", mark,\n                        \"cannot find module %r (%s)\" % (module_name, exc), mark)\n        if module_name not in sys.modules:\n            raise ConstructorError(\"while constructing a Python object\", mark,\n                    \"module %r is not imported\" % module_name, mark)\n        module = sys.modules[module_name]\n        if not hasattr(module, object_name):\n            raise ConstructorError(\"while constructing a Python object\", mark,\n                    \"cannot find %r in the module %r\"\n                    % (object_name, module.__name__), mark)\n        return getattr(module, object_name)\n\n    def construct_python_name(self, suffix, node):\n        value = self.construct_scalar(node)\n        if value:\n            raise ConstructorError(\"while constructing a Python name\", node.start_mark,\n                    \"expected the empty value, but found %r\" % value, node.start_mark)\n        return self.find_python_name(suffix, node.start_mark)\n\n    def construct_python_module(self, suffix, node):\n        value = self.construct_scalar(node)\n        if value:\n            raise ConstructorError(\"while constructing a Python module\", node.start_mark,\n                    \"expected the empty value, but found %r\" % value, node.start_mark)\n        return self.find_python_module(suffix, node.start_mark)\n\n    def make_python_instance(self, suffix, node,\n            args=None, kwds=None, newobj=False, unsafe=False):\n        if not args:\n            args = []\n        if not kwds:\n            kwds = {}\n        cls = self.find_python_name(suffix, node.start_mark)\n        if not (unsafe or isinstance(cls, type)):\n            raise ConstructorError(\"while constructing a Python instance\", node.start_mark,\n                    \"expected a class, but found %r\" % type(cls),\n                    node.start_mark)\n        if newobj and isinstance(cls, type):\n            return cls.__new__(cls, *args, **kwds)\n        else:\n            return cls(*args, **kwds)\n\n    def set_python_instance_state(self, instance, state, unsafe=False):\n        if hasattr(instance, '__setstate__'):\n            instance.__setstate__(state)\n        else:\n            slotstate = {}\n            if isinstance(state, tuple) and len(state) == 2:\n                state, slotstate = state\n            if hasattr(instance, '__dict__'):\n                if not unsafe and state:\n                    for key in state.keys():\n                        self.check_state_key(key)\n                instance.__dict__.update(state)\n            elif state:\n                slotstate.update(state)\n            for key, value in slotstate.items():\n                if not unsafe:\n                    self.check_state_key(key)\n                setattr(instance, key, value)\n\n    def construct_python_object(self, suffix, node):\n        # Format:\n        #   !!python/object:module.name { ... state ... }\n        instance = self.make_python_instance(suffix, node, newobj=True)\n        yield instance\n        deep = hasattr(instance, '__setstate__')\n        state = self.construct_mapping(node, deep=deep)\n        self.set_python_instance_state(instance, state)\n\n    def construct_python_object_apply(self, suffix, node, newobj=False):\n        # Format:\n        #   !!python/object/apply       # (or !!python/object/new)\n        #   args: [ ... arguments ... ]\n        #   kwds: { ... keywords ... }\n        #   state: ... state ...\n        #   listitems: [ ... listitems ... ]\n        #   dictitems: { ... dictitems ... }\n        # or short format:\n        #   !!python/object/apply [ ... arguments ... ]\n        # The difference between !!python/object/apply and !!python/object/new\n        # is how an object is created, check make_python_instance for details.\n        if isinstance(node, SequenceNode):\n            args = self.construct_sequence(node, deep=True)\n            kwds = {}\n            state = {}\n            listitems = []\n            dictitems = {}\n        else:\n            value = self.construct_mapping(node, deep=True)\n            args = value.get('args', [])\n            kwds = value.get('kwds', {})\n            state = value.get('state', {})\n            listitems = value.get('listitems', [])\n            dictitems = value.get('dictitems', {})\n        instance = self.make_python_instance(suffix, node, args, kwds, newobj)\n        if state:\n            self.set_python_instance_state(instance, state)\n        if listitems:\n            instance.extend(listitems)\n        if dictitems:\n            for key in dictitems:\n                instance[key] = dictitems[key]\n        return instance\n\n    def construct_python_object_new(self, suffix, node):\n        return self.construct_python_object_apply(suffix, node, newobj=True)\n\nFullConstructor.add_constructor(\n    'tag:yaml.org,2002:python/none',\n    FullConstructor.construct_yaml_null)\n\nFullConstructor.add_constructor(\n    'tag:yaml.org,2002:python/bool',\n    FullConstructor.construct_yaml_bool)\n\nFullConstructor.add_constructor(\n    'tag:yaml.org,2002:python/str',\n    FullConstructor.construct_python_str)\n\nFullConstructor.add_constructor(\n    'tag:yaml.org,2002:python/unicode',\n    FullConstructor.construct_python_unicode)\n\nFullConstructor.add_constructor(\n    'tag:yaml.org,2002:python/bytes',\n    FullConstructor.construct_python_bytes)\n\nFullConstructor.add_constructor(\n    'tag:yaml.org,2002:python/int',\n    FullConstructor.construct_yaml_int)\n\nFullConstructor.add_constructor(\n    'tag:yaml.org,2002:python/long',\n    FullConstructor.construct_python_long)\n\nFullConstructor.add_constructor(\n    'tag:yaml.org,2002:python/float',\n    FullConstructor.construct_yaml_float)\n\nFullConstructor.add_constructor(\n    'tag:yaml.org,2002:python/complex',\n    FullConstructor.construct_python_complex)\n\nFullConstructor.add_constructor(\n    'tag:yaml.org,2002:python/list',\n    FullConstructor.construct_yaml_seq)\n\nFullConstructor.add_constructor(\n    'tag:yaml.org,2002:python/tuple',\n    FullConstructor.construct_python_tuple)\n\nFullConstructor.add_constructor(\n    'tag:yaml.org,2002:python/dict',\n    FullConstructor.construct_yaml_map)\n\nFullConstructor.add_multi_constructor(\n    'tag:yaml.org,2002:python/name:',\n    FullConstructor.construct_python_name)\n\nclass UnsafeConstructor(FullConstructor):\n\n    def find_python_module(self, name, mark):\n        return super(UnsafeConstructor, self).find_python_module(name, mark, unsafe=True)\n\n    def find_python_name(self, name, mark):\n        return super(UnsafeConstructor, self).find_python_name(name, mark, unsafe=True)\n\n    def make_python_instance(self, suffix, node, args=None, kwds=None, newobj=False):\n        return super(UnsafeConstructor, self).make_python_instance(\n            suffix, node, args, kwds, newobj, unsafe=True)\n\n    def set_python_instance_state(self, instance, state):\n        return super(UnsafeConstructor, self).set_python_instance_state(\n            instance, state, unsafe=True)\n\nUnsafeConstructor.add_multi_constructor(\n    'tag:yaml.org,2002:python/module:',\n    UnsafeConstructor.construct_python_module)\n\nUnsafeConstructor.add_multi_constructor(\n    'tag:yaml.org,2002:python/object:',\n    UnsafeConstructor.construct_python_object)\n\nUnsafeConstructor.add_multi_constructor(\n    'tag:yaml.org,2002:python/object/new:',\n    UnsafeConstructor.construct_python_object_new)\n\nUnsafeConstructor.add_multi_constructor(\n    'tag:yaml.org,2002:python/object/apply:',\n    UnsafeConstructor.construct_python_object_apply)\n\n# Constructor is same as UnsafeConstructor. Need to leave this in place in case\n# people have extended it directly.\nclass Constructor(UnsafeConstructor):\n    pass\n", "lib/yaml/emitter.py": "\n# Emitter expects events obeying the following grammar:\n# stream ::= STREAM-START document* STREAM-END\n# document ::= DOCUMENT-START node DOCUMENT-END\n# node ::= SCALAR | sequence | mapping\n# sequence ::= SEQUENCE-START node* SEQUENCE-END\n# mapping ::= MAPPING-START (node node)* MAPPING-END\n\n__all__ = ['Emitter', 'EmitterError']\n\nfrom .error import YAMLError\nfrom .events import *\n\nclass EmitterError(YAMLError):\n    pass\n\nclass ScalarAnalysis:\n    def __init__(self, scalar, empty, multiline,\n            allow_flow_plain, allow_block_plain,\n            allow_single_quoted, allow_double_quoted,\n            allow_block):\n        self.scalar = scalar\n        self.empty = empty\n        self.multiline = multiline\n        self.allow_flow_plain = allow_flow_plain\n        self.allow_block_plain = allow_block_plain\n        self.allow_single_quoted = allow_single_quoted\n        self.allow_double_quoted = allow_double_quoted\n        self.allow_block = allow_block\n\nclass Emitter:\n\n    DEFAULT_TAG_PREFIXES = {\n        '!' : '!',\n        'tag:yaml.org,2002:' : '!!',\n    }\n\n    def __init__(self, stream, canonical=None, indent=None, width=None,\n            allow_unicode=None, line_break=None):\n\n        # The stream should have the methods `write` and possibly `flush`.\n        self.stream = stream\n\n        # Encoding can be overridden by STREAM-START.\n        self.encoding = None\n\n        # Emitter is a state machine with a stack of states to handle nested\n        # structures.\n        self.states = []\n        self.state = self.expect_stream_start\n\n        # Current event and the event queue.\n        self.events = []\n        self.event = None\n\n        # The current indentation level and the stack of previous indents.\n        self.indents = []\n        self.indent = None\n\n        # Flow level.\n        self.flow_level = 0\n\n        # Contexts.\n        self.root_context = False\n        self.sequence_context = False\n        self.mapping_context = False\n        self.simple_key_context = False\n\n        # Characteristics of the last emitted character:\n        #  - current position.\n        #  - is it a whitespace?\n        #  - is it an indention character\n        #    (indentation space, '-', '?', or ':')?\n        self.line = 0\n        self.column = 0\n        self.whitespace = True\n        self.indention = True\n\n        # Whether the document requires an explicit document indicator\n        self.open_ended = False\n\n        # Formatting details.\n        self.canonical = canonical\n        self.allow_unicode = allow_unicode\n        self.best_indent = 2\n        if indent and 1 < indent < 10:\n            self.best_indent = indent\n        self.best_width = 80\n        if width and width > self.best_indent*2:\n            self.best_width = width\n        self.best_line_break = '\\n'\n        if line_break in ['\\r', '\\n', '\\r\\n']:\n            self.best_line_break = line_break\n\n        # Tag prefixes.\n        self.tag_prefixes = None\n\n        # Prepared anchor and tag.\n        self.prepared_anchor = None\n        self.prepared_tag = None\n\n        # Scalar analysis and style.\n        self.analysis = None\n        self.style = None\n\n    def dispose(self):\n        # Reset the state attributes (to clear self-references)\n        self.states = []\n        self.state = None\n\n    def emit(self, event):\n        self.events.append(event)\n        while not self.need_more_events():\n            self.event = self.events.pop(0)\n            self.state()\n            self.event = None\n\n    # In some cases, we wait for a few next events before emitting.\n\n    def need_more_events(self):\n        if not self.events:\n            return True\n        event = self.events[0]\n        if isinstance(event, DocumentStartEvent):\n            return self.need_events(1)\n        elif isinstance(event, SequenceStartEvent):\n            return self.need_events(2)\n        elif isinstance(event, MappingStartEvent):\n            return self.need_events(3)\n        else:\n            return False\n\n    def need_events(self, count):\n        level = 0\n        for event in self.events[1:]:\n            if isinstance(event, (DocumentStartEvent, CollectionStartEvent)):\n                level += 1\n            elif isinstance(event, (DocumentEndEvent, CollectionEndEvent)):\n                level -= 1\n            elif isinstance(event, StreamEndEvent):\n                level = -1\n            if level < 0:\n                return False\n        return (len(self.events) < count+1)\n\n    def increase_indent(self, flow=False, indentless=False):\n        self.indents.append(self.indent)\n        if self.indent is None:\n            if flow:\n                self.indent = self.best_indent\n            else:\n                self.indent = 0\n        elif not indentless:\n            self.indent += self.best_indent\n\n    # States.\n\n    # Stream handlers.\n\n    def expect_stream_start(self):\n        if isinstance(self.event, StreamStartEvent):\n            if self.event.encoding and not hasattr(self.stream, 'encoding'):\n                self.encoding = self.event.encoding\n            self.write_stream_start()\n            self.state = self.expect_first_document_start\n        else:\n            raise EmitterError(\"expected StreamStartEvent, but got %s\"\n                    % self.event)\n\n    def expect_nothing(self):\n        raise EmitterError(\"expected nothing, but got %s\" % self.event)\n\n    # Document handlers.\n\n    def expect_first_document_start(self):\n        return self.expect_document_start(first=True)\n\n    def expect_document_start(self, first=False):\n        if isinstance(self.event, DocumentStartEvent):\n            if (self.event.version or self.event.tags) and self.open_ended:\n                self.write_indicator('...', True)\n                self.write_indent()\n            if self.event.version:\n                version_text = self.prepare_version(self.event.version)\n                self.write_version_directive(version_text)\n            self.tag_prefixes = self.DEFAULT_TAG_PREFIXES.copy()\n            if self.event.tags:\n                handles = sorted(self.event.tags.keys())\n                for handle in handles:\n                    prefix = self.event.tags[handle]\n                    self.tag_prefixes[prefix] = handle\n                    handle_text = self.prepare_tag_handle(handle)\n                    prefix_text = self.prepare_tag_prefix(prefix)\n                    self.write_tag_directive(handle_text, prefix_text)\n            implicit = (first and not self.event.explicit and not self.canonical\n                    and not self.event.version and not self.event.tags\n                    and not self.check_empty_document())\n            if not implicit:\n                self.write_indent()\n                self.write_indicator('---', True)\n                if self.canonical:\n                    self.write_indent()\n            self.state = self.expect_document_root\n        elif isinstance(self.event, StreamEndEvent):\n            if self.open_ended:\n                self.write_indicator('...', True)\n                self.write_indent()\n            self.write_stream_end()\n            self.state = self.expect_nothing\n        else:\n            raise EmitterError(\"expected DocumentStartEvent, but got %s\"\n                    % self.event)\n\n    def expect_document_end(self):\n        if isinstance(self.event, DocumentEndEvent):\n            self.write_indent()\n            if self.event.explicit:\n                self.write_indicator('...', True)\n                self.write_indent()\n            self.flush_stream()\n            self.state = self.expect_document_start\n        else:\n            raise EmitterError(\"expected DocumentEndEvent, but got %s\"\n                    % self.event)\n\n    def expect_document_root(self):\n        self.states.append(self.expect_document_end)\n        self.expect_node(root=True)\n\n    # Node handlers.\n\n    def expect_node(self, root=False, sequence=False, mapping=False,\n            simple_key=False):\n        self.root_context = root\n        self.sequence_context = sequence\n        self.mapping_context = mapping\n        self.simple_key_context = simple_key\n        if isinstance(self.event, AliasEvent):\n            self.expect_alias()\n        elif isinstance(self.event, (ScalarEvent, CollectionStartEvent)):\n            self.process_anchor('&')\n            self.process_tag()\n            if isinstance(self.event, ScalarEvent):\n                self.expect_scalar()\n            elif isinstance(self.event, SequenceStartEvent):\n                if self.flow_level or self.canonical or self.event.flow_style   \\\n                        or self.check_empty_sequence():\n                    self.expect_flow_sequence()\n                else:\n                    self.expect_block_sequence()\n            elif isinstance(self.event, MappingStartEvent):\n                if self.flow_level or self.canonical or self.event.flow_style   \\\n                        or self.check_empty_mapping():\n                    self.expect_flow_mapping()\n                else:\n                    self.expect_block_mapping()\n        else:\n            raise EmitterError(\"expected NodeEvent, but got %s\" % self.event)\n\n    def expect_alias(self):\n        if self.event.anchor is None:\n            raise EmitterError(\"anchor is not specified for alias\")\n        self.process_anchor('*')\n        self.state = self.states.pop()\n\n    def expect_scalar(self):\n        self.increase_indent(flow=True)\n        self.process_scalar()\n        self.indent = self.indents.pop()\n        self.state = self.states.pop()\n\n    # Flow sequence handlers.\n\n    def expect_flow_sequence(self):\n        self.write_indicator('[', True, whitespace=True)\n        self.flow_level += 1\n        self.increase_indent(flow=True)\n        self.state = self.expect_first_flow_sequence_item\n\n    def expect_first_flow_sequence_item(self):\n        if isinstance(self.event, SequenceEndEvent):\n            self.indent = self.indents.pop()\n            self.flow_level -= 1\n            self.write_indicator(']', False)\n            self.state = self.states.pop()\n        else:\n            if self.canonical or self.column > self.best_width:\n                self.write_indent()\n            self.states.append(self.expect_flow_sequence_item)\n            self.expect_node(sequence=True)\n\n    def expect_flow_sequence_item(self):\n        if isinstance(self.event, SequenceEndEvent):\n            self.indent = self.indents.pop()\n            self.flow_level -= 1\n            if self.canonical:\n                self.write_indicator(',', False)\n                self.write_indent()\n            self.write_indicator(']', False)\n            self.state = self.states.pop()\n        else:\n            self.write_indicator(',', False)\n            if self.canonical or self.column > self.best_width:\n                self.write_indent()\n            self.states.append(self.expect_flow_sequence_item)\n            self.expect_node(sequence=True)\n\n    # Flow mapping handlers.\n\n    def expect_flow_mapping(self):\n        self.write_indicator('{', True, whitespace=True)\n        self.flow_level += 1\n        self.increase_indent(flow=True)\n        self.state = self.expect_first_flow_mapping_key\n\n    def expect_first_flow_mapping_key(self):\n        if isinstance(self.event, MappingEndEvent):\n            self.indent = self.indents.pop()\n            self.flow_level -= 1\n            self.write_indicator('}', False)\n            self.state = self.states.pop()\n        else:\n            if self.canonical or self.column > self.best_width:\n                self.write_indent()\n            if not self.canonical and self.check_simple_key():\n                self.states.append(self.expect_flow_mapping_simple_value)\n                self.expect_node(mapping=True, simple_key=True)\n            else:\n                self.write_indicator('?', True)\n                self.states.append(self.expect_flow_mapping_value)\n                self.expect_node(mapping=True)\n\n    def expect_flow_mapping_key(self):\n        if isinstance(self.event, MappingEndEvent):\n            self.indent = self.indents.pop()\n            self.flow_level -= 1\n            if self.canonical:\n                self.write_indicator(',', False)\n                self.write_indent()\n            self.write_indicator('}', False)\n            self.state = self.states.pop()\n        else:\n            self.write_indicator(',', False)\n            if self.canonical or self.column > self.best_width:\n                self.write_indent()\n            if not self.canonical and self.check_simple_key():\n                self.states.append(self.expect_flow_mapping_simple_value)\n                self.expect_node(mapping=True, simple_key=True)\n            else:\n                self.write_indicator('?', True)\n                self.states.append(self.expect_flow_mapping_value)\n                self.expect_node(mapping=True)\n\n    def expect_flow_mapping_simple_value(self):\n        self.write_indicator(':', False)\n        self.states.append(self.expect_flow_mapping_key)\n        self.expect_node(mapping=True)\n\n    def expect_flow_mapping_value(self):\n        if self.canonical or self.column > self.best_width:\n            self.write_indent()\n        self.write_indicator(':', True)\n        self.states.append(self.expect_flow_mapping_key)\n        self.expect_node(mapping=True)\n\n    # Block sequence handlers.\n\n    def expect_block_sequence(self):\n        indentless = (self.mapping_context and not self.indention)\n        self.increase_indent(flow=False, indentless=indentless)\n        self.state = self.expect_first_block_sequence_item\n\n    def expect_first_block_sequence_item(self):\n        return self.expect_block_sequence_item(first=True)\n\n    def expect_block_sequence_item(self, first=False):\n        if not first and isinstance(self.event, SequenceEndEvent):\n            self.indent = self.indents.pop()\n            self.state = self.states.pop()\n        else:\n            self.write_indent()\n            self.write_indicator('-', True, indention=True)\n            self.states.append(self.expect_block_sequence_item)\n            self.expect_node(sequence=True)\n\n    # Block mapping handlers.\n\n    def expect_block_mapping(self):\n        self.increase_indent(flow=False)\n        self.state = self.expect_first_block_mapping_key\n\n    def expect_first_block_mapping_key(self):\n        return self.expect_block_mapping_key(first=True)\n\n    def expect_block_mapping_key(self, first=False):\n        if not first and isinstance(self.event, MappingEndEvent):\n            self.indent = self.indents.pop()\n            self.state = self.states.pop()\n        else:\n            self.write_indent()\n            if self.check_simple_key():\n                self.states.append(self.expect_block_mapping_simple_value)\n                self.expect_node(mapping=True, simple_key=True)\n            else:\n                self.write_indicator('?', True, indention=True)\n                self.states.append(self.expect_block_mapping_value)\n                self.expect_node(mapping=True)\n\n    def expect_block_mapping_simple_value(self):\n        self.write_indicator(':', False)\n        self.states.append(self.expect_block_mapping_key)\n        self.expect_node(mapping=True)\n\n    def expect_block_mapping_value(self):\n        self.write_indent()\n        self.write_indicator(':', True, indention=True)\n        self.states.append(self.expect_block_mapping_key)\n        self.expect_node(mapping=True)\n\n    # Checkers.\n\n    def check_empty_sequence(self):\n        return (isinstance(self.event, SequenceStartEvent) and self.events\n                and isinstance(self.events[0], SequenceEndEvent))\n\n    def check_empty_mapping(self):\n        return (isinstance(self.event, MappingStartEvent) and self.events\n                and isinstance(self.events[0], MappingEndEvent))\n\n    def check_empty_document(self):\n        if not isinstance(self.event, DocumentStartEvent) or not self.events:\n            return False\n        event = self.events[0]\n        return (isinstance(event, ScalarEvent) and event.anchor is None\n                and event.tag is None and event.implicit and event.value == '')\n\n    def check_simple_key(self):\n        length = 0\n        if isinstance(self.event, NodeEvent) and self.event.anchor is not None:\n            if self.prepared_anchor is None:\n                self.prepared_anchor = self.prepare_anchor(self.event.anchor)\n            length += len(self.prepared_anchor)\n        if isinstance(self.event, (ScalarEvent, CollectionStartEvent))  \\\n                and self.event.tag is not None:\n            if self.prepared_tag is None:\n                self.prepared_tag = self.prepare_tag(self.event.tag)\n            length += len(self.prepared_tag)\n        if isinstance(self.event, ScalarEvent):\n            if self.analysis is None:\n                self.analysis = self.analyze_scalar(self.event.value)\n            length += len(self.analysis.scalar)\n        return (length < 128 and (isinstance(self.event, AliasEvent)\n            or (isinstance(self.event, ScalarEvent)\n                    and not self.analysis.empty and not self.analysis.multiline)\n            or self.check_empty_sequence() or self.check_empty_mapping()))\n\n    # Anchor, Tag, and Scalar processors.\n\n    def process_anchor(self, indicator):\n        if self.event.anchor is None:\n            self.prepared_anchor = None\n            return\n        if self.prepared_anchor is None:\n            self.prepared_anchor = self.prepare_anchor(self.event.anchor)\n        if self.prepared_anchor:\n            self.write_indicator(indicator+self.prepared_anchor, True)\n        self.prepared_anchor = None\n\n    def process_tag(self):\n        tag = self.event.tag\n        if isinstance(self.event, ScalarEvent):\n            if self.style is None:\n                self.style = self.choose_scalar_style()\n            if ((not self.canonical or tag is None) and\n                ((self.style == '' and self.event.implicit[0])\n                        or (self.style != '' and self.event.implicit[1]))):\n                self.prepared_tag = None\n                return\n            if self.event.implicit[0] and tag is None:\n                tag = '!'\n                self.prepared_tag = None\n        else:\n            if (not self.canonical or tag is None) and self.event.implicit:\n                self.prepared_tag = None\n                return\n        if tag is None:\n            raise EmitterError(\"tag is not specified\")\n        if self.prepared_tag is None:\n            self.prepared_tag = self.prepare_tag(tag)\n        if self.prepared_tag:\n            self.write_indicator(self.prepared_tag, True)\n        self.prepared_tag = None\n\n    def choose_scalar_style(self):\n        if self.analysis is None:\n            self.analysis = self.analyze_scalar(self.event.value)\n        if self.event.style == '\"' or self.canonical:\n            return '\"'\n        if not self.event.style and self.event.implicit[0]:\n            if (not (self.simple_key_context and\n                    (self.analysis.empty or self.analysis.multiline))\n                and (self.flow_level and self.analysis.allow_flow_plain\n                    or (not self.flow_level and self.analysis.allow_block_plain))):\n                return ''\n        if self.event.style and self.event.style in '|>':\n            if (not self.flow_level and not self.simple_key_context\n                    and self.analysis.allow_block):\n                return self.event.style\n        if not self.event.style or self.event.style == '\\'':\n            if (self.analysis.allow_single_quoted and\n                    not (self.simple_key_context and self.analysis.multiline)):\n                return '\\''\n        return '\"'\n\n    def process_scalar(self):\n        if self.analysis is None:\n            self.analysis = self.analyze_scalar(self.event.value)\n        if self.style is None:\n            self.style = self.choose_scalar_style()\n        split = (not self.simple_key_context)\n        #if self.analysis.multiline and split    \\\n        #        and (not self.style or self.style in '\\'\\\"'):\n        #    self.write_indent()\n        if self.style == '\"':\n            self.write_double_quoted(self.analysis.scalar, split)\n        elif self.style == '\\'':\n            self.write_single_quoted(self.analysis.scalar, split)\n        elif self.style == '>':\n            self.write_folded(self.analysis.scalar)\n        elif self.style == '|':\n            self.write_literal(self.analysis.scalar)\n        else:\n            self.write_plain(self.analysis.scalar, split)\n        self.analysis = None\n        self.style = None\n\n    # Analyzers.\n\n    def prepare_version(self, version):\n        major, minor = version\n        if major != 1:\n            raise EmitterError(\"unsupported YAML version: %d.%d\" % (major, minor))\n        return '%d.%d' % (major, minor)\n\n    def prepare_tag_handle(self, handle):\n        if not handle:\n            raise EmitterError(\"tag handle must not be empty\")\n        if handle[0] != '!' or handle[-1] != '!':\n            raise EmitterError(\"tag handle must start and end with '!': %r\" % handle)\n        for ch in handle[1:-1]:\n            if not ('0' <= ch <= '9' or 'A' <= ch <= 'Z' or 'a' <= ch <= 'z'    \\\n                    or ch in '-_'):\n                raise EmitterError(\"invalid character %r in the tag handle: %r\"\n                        % (ch, handle))\n        return handle\n\n    def prepare_tag_prefix(self, prefix):\n        if not prefix:\n            raise EmitterError(\"tag prefix must not be empty\")\n        chunks = []\n        start = end = 0\n        if prefix[0] == '!':\n            end = 1\n        while end < len(prefix):\n            ch = prefix[end]\n            if '0' <= ch <= '9' or 'A' <= ch <= 'Z' or 'a' <= ch <= 'z' \\\n                    or ch in '-;/?!:@&=+$,_.~*\\'()[]':\n                end += 1\n            else:\n                if start < end:\n                    chunks.append(prefix[start:end])\n                start = end = end+1\n                data = ch.encode('utf-8')\n                for ch in data:\n                    chunks.append('%%%02X' % ord(ch))\n        if start < end:\n            chunks.append(prefix[start:end])\n        return ''.join(chunks)\n\n    def prepare_tag(self, tag):\n        if not tag:\n            raise EmitterError(\"tag must not be empty\")\n        if tag == '!':\n            return tag\n        handle = None\n        suffix = tag\n        prefixes = sorted(self.tag_prefixes.keys())\n        for prefix in prefixes:\n            if tag.startswith(prefix)   \\\n                    and (prefix == '!' or len(prefix) < len(tag)):\n                handle = self.tag_prefixes[prefix]\n                suffix = tag[len(prefix):]\n        chunks = []\n        start = end = 0\n        while end < len(suffix):\n            ch = suffix[end]\n            if '0' <= ch <= '9' or 'A' <= ch <= 'Z' or 'a' <= ch <= 'z' \\\n                    or ch in '-;/?:@&=+$,_.~*\\'()[]'   \\\n                    or (ch == '!' and handle != '!'):\n                end += 1\n            else:\n                if start < end:\n                    chunks.append(suffix[start:end])\n                start = end = end+1\n                data = ch.encode('utf-8')\n                for ch in data:\n                    chunks.append('%%%02X' % ch)\n        if start < end:\n            chunks.append(suffix[start:end])\n        suffix_text = ''.join(chunks)\n        if handle:\n            return '%s%s' % (handle, suffix_text)\n        else:\n            return '!<%s>' % suffix_text\n\n    def prepare_anchor(self, anchor):\n        if not anchor:\n            raise EmitterError(\"anchor must not be empty\")\n        for ch in anchor:\n            if not ('0' <= ch <= '9' or 'A' <= ch <= 'Z' or 'a' <= ch <= 'z'    \\\n                    or ch in '-_'):\n                raise EmitterError(\"invalid character %r in the anchor: %r\"\n                        % (ch, anchor))\n        return anchor\n\n    def analyze_scalar(self, scalar):\n\n        # Empty scalar is a special case.\n        if not scalar:\n            return ScalarAnalysis(scalar=scalar, empty=True, multiline=False,\n                    allow_flow_plain=False, allow_block_plain=True,\n                    allow_single_quoted=True, allow_double_quoted=True,\n                    allow_block=False)\n\n        # Indicators and special characters.\n        block_indicators = False\n        flow_indicators = False\n        line_breaks = False\n        special_characters = False\n\n        # Important whitespace combinations.\n        leading_space = False\n        leading_break = False\n        trailing_space = False\n        trailing_break = False\n        break_space = False\n        space_break = False\n\n        # Check document indicators.\n        if scalar.startswith('---') or scalar.startswith('...'):\n            block_indicators = True\n            flow_indicators = True\n\n        # First character or preceded by a whitespace.\n        preceded_by_whitespace = True\n\n        # Last character or followed by a whitespace.\n        followed_by_whitespace = (len(scalar) == 1 or\n                scalar[1] in '\\0 \\t\\r\\n\\x85\\u2028\\u2029')\n\n        # The previous character is a space.\n        previous_space = False\n\n        # The previous character is a break.\n        previous_break = False\n\n        index = 0\n        while index < len(scalar):\n            ch = scalar[index]\n\n            # Check for indicators.\n            if index == 0:\n                # Leading indicators are special characters.\n                if ch in '#,[]{}&*!|>\\'\\\"%@`':\n                    flow_indicators = True\n                    block_indicators = True\n                if ch in '?:':\n                    flow_indicators = True\n                    if followed_by_whitespace:\n                        block_indicators = True\n                if ch == '-' and followed_by_whitespace:\n                    flow_indicators = True\n                    block_indicators = True\n            else:\n                # Some indicators cannot appear within a scalar as well.\n                if ch in ',?[]{}':\n                    flow_indicators = True\n                if ch == ':':\n                    flow_indicators = True\n                    if followed_by_whitespace:\n                        block_indicators = True\n                if ch == '#' and preceded_by_whitespace:\n                    flow_indicators = True\n                    block_indicators = True\n\n            # Check for line breaks, special, and unicode characters.\n            if ch in '\\n\\x85\\u2028\\u2029':\n                line_breaks = True\n            if not (ch == '\\n' or '\\x20' <= ch <= '\\x7E'):\n                if (ch == '\\x85' or '\\xA0' <= ch <= '\\uD7FF'\n                        or '\\uE000' <= ch <= '\\uFFFD'\n                        or '\\U00010000' <= ch < '\\U0010ffff') and ch != '\\uFEFF':\n                    unicode_characters = True\n                    if not self.allow_unicode:\n                        special_characters = True\n                else:\n                    special_characters = True\n\n            # Detect important whitespace combinations.\n            if ch == ' ':\n                if index == 0:\n                    leading_space = True\n                if index == len(scalar)-1:\n                    trailing_space = True\n                if previous_break:\n                    break_space = True\n                previous_space = True\n                previous_break = False\n            elif ch in '\\n\\x85\\u2028\\u2029':\n                if index == 0:\n                    leading_break = True\n                if index == len(scalar)-1:\n                    trailing_break = True\n                if previous_space:\n                    space_break = True\n                previous_space = False\n                previous_break = True\n            else:\n                previous_space = False\n                previous_break = False\n\n            # Prepare for the next character.\n            index += 1\n            preceded_by_whitespace = (ch in '\\0 \\t\\r\\n\\x85\\u2028\\u2029')\n            followed_by_whitespace = (index+1 >= len(scalar) or\n                    scalar[index+1] in '\\0 \\t\\r\\n\\x85\\u2028\\u2029')\n\n        # Let's decide what styles are allowed.\n        allow_flow_plain = True\n        allow_block_plain = True\n        allow_single_quoted = True\n        allow_double_quoted = True\n        allow_block = True\n\n        # Leading and trailing whitespaces are bad for plain scalars.\n        if (leading_space or leading_break\n                or trailing_space or trailing_break):\n            allow_flow_plain = allow_block_plain = False\n\n        # We do not permit trailing spaces for block scalars.\n        if trailing_space:\n            allow_block = False\n\n        # Spaces at the beginning of a new line are only acceptable for block\n        # scalars.\n        if break_space:\n            allow_flow_plain = allow_block_plain = allow_single_quoted = False\n\n        # Spaces followed by breaks, as well as special character are only\n        # allowed for double quoted scalars.\n        if space_break or special_characters:\n            allow_flow_plain = allow_block_plain =  \\\n            allow_single_quoted = allow_block = False\n\n        # Although the plain scalar writer supports breaks, we never emit\n        # multiline plain scalars.\n        if line_breaks:\n            allow_flow_plain = allow_block_plain = False\n\n        # Flow indicators are forbidden for flow plain scalars.\n        if flow_indicators:\n            allow_flow_plain = False\n\n        # Block indicators are forbidden for block plain scalars.\n        if block_indicators:\n            allow_block_plain = False\n\n        return ScalarAnalysis(scalar=scalar,\n                empty=False, multiline=line_breaks,\n                allow_flow_plain=allow_flow_plain,\n                allow_block_plain=allow_block_plain,\n                allow_single_quoted=allow_single_quoted,\n                allow_double_quoted=allow_double_quoted,\n                allow_block=allow_block)\n\n    # Writers.\n\n    def flush_stream(self):\n        if hasattr(self.stream, 'flush'):\n            self.stream.flush()\n\n    def write_stream_start(self):\n        # Write BOM if needed.\n        if self.encoding and self.encoding.startswith('utf-16'):\n            self.stream.write('\\uFEFF'.encode(self.encoding))\n\n    def write_stream_end(self):\n        self.flush_stream()\n\n    def write_indicator(self, indicator, need_whitespace,\n            whitespace=False, indention=False):\n        if self.whitespace or not need_whitespace:\n            data = indicator\n        else:\n            data = ' '+indicator\n        self.whitespace = whitespace\n        self.indention = self.indention and indention\n        self.column += len(data)\n        self.open_ended = False\n        if self.encoding:\n            data = data.encode(self.encoding)\n        self.stream.write(data)\n\n    def write_indent(self):\n        indent = self.indent or 0\n        if not self.indention or self.column > indent   \\\n                or (self.column == indent and not self.whitespace):\n            self.write_line_break()\n        if self.column < indent:\n            self.whitespace = True\n            data = ' '*(indent-self.column)\n            self.column = indent\n            if self.encoding:\n                data = data.encode(self.encoding)\n            self.stream.write(data)\n\n    def write_line_break(self, data=None):\n        if data is None:\n            data = self.best_line_break\n        self.whitespace = True\n        self.indention = True\n        self.line += 1\n        self.column = 0\n        if self.encoding:\n            data = data.encode(self.encoding)\n        self.stream.write(data)\n\n    def write_version_directive(self, version_text):\n        data = '%%YAML %s' % version_text\n        if self.encoding:\n            data = data.encode(self.encoding)\n        self.stream.write(data)\n        self.write_line_break()\n\n    def write_tag_directive(self, handle_text, prefix_text):\n        data = '%%TAG %s %s' % (handle_text, prefix_text)\n        if self.encoding:\n            data = data.encode(self.encoding)\n        self.stream.write(data)\n        self.write_line_break()\n\n    # Scalar streams.\n\n    def write_single_quoted(self, text, split=True):\n        self.write_indicator('\\'', True)\n        spaces = False\n        breaks = False\n        start = end = 0\n        while end <= len(text):\n            ch = None\n            if end < len(text):\n                ch = text[end]\n            if spaces:\n                if ch is None or ch != ' ':\n                    if start+1 == end and self.column > self.best_width and split   \\\n                            and start != 0 and end != len(text):\n                        self.write_indent()\n                    else:\n                        data = text[start:end]\n                        self.column += len(data)\n                        if self.encoding:\n                            data = data.encode(self.encoding)\n                        self.stream.write(data)\n                    start = end\n            elif breaks:\n                if ch is None or ch not in '\\n\\x85\\u2028\\u2029':\n                    if text[start] == '\\n':\n                        self.write_line_break()\n                    for br in text[start:end]:\n                        if br == '\\n':\n                            self.write_line_break()\n                        else:\n                            self.write_line_break(br)\n                    self.write_indent()\n                    start = end\n            else:\n                if ch is None or ch in ' \\n\\x85\\u2028\\u2029' or ch == '\\'':\n                    if start < end:\n                        data = text[start:end]\n                        self.column += len(data)\n                        if self.encoding:\n                            data = data.encode(self.encoding)\n                        self.stream.write(data)\n                        start = end\n            if ch == '\\'':\n                data = '\\'\\''\n                self.column += 2\n                if self.encoding:\n                    data = data.encode(self.encoding)\n                self.stream.write(data)\n                start = end + 1\n            if ch is not None:\n                spaces = (ch == ' ')\n                breaks = (ch in '\\n\\x85\\u2028\\u2029')\n            end += 1\n        self.write_indicator('\\'', False)\n\n    ESCAPE_REPLACEMENTS = {\n        '\\0':       '0',\n        '\\x07':     'a',\n        '\\x08':     'b',\n        '\\x09':     't',\n        '\\x0A':     'n',\n        '\\x0B':     'v',\n        '\\x0C':     'f',\n        '\\x0D':     'r',\n        '\\x1B':     'e',\n        '\\\"':       '\\\"',\n        '\\\\':       '\\\\',\n        '\\x85':     'N',\n        '\\xA0':     '_',\n        '\\u2028':   'L',\n        '\\u2029':   'P',\n    }\n\n    def write_double_quoted(self, text, split=True):\n        self.write_indicator('\"', True)\n        start = end = 0\n        while end <= len(text):\n            ch = None\n            if end < len(text):\n                ch = text[end]\n            if ch is None or ch in '\"\\\\\\x85\\u2028\\u2029\\uFEFF' \\\n                    or not ('\\x20' <= ch <= '\\x7E'\n                        or (self.allow_unicode\n                            and ('\\xA0' <= ch <= '\\uD7FF'\n                                or '\\uE000' <= ch <= '\\uFFFD'))):\n                if start < end:\n                    data = text[start:end]\n                    self.column += len(data)\n                    if self.encoding:\n                        data = data.encode(self.encoding)\n                    self.stream.write(data)\n                    start = end\n                if ch is not None:\n                    if ch in self.ESCAPE_REPLACEMENTS:\n                        data = '\\\\'+self.ESCAPE_REPLACEMENTS[ch]\n                    elif ch <= '\\xFF':\n                        data = '\\\\x%02X' % ord(ch)\n                    elif ch <= '\\uFFFF':\n                        data = '\\\\u%04X' % ord(ch)\n                    else:\n                        data = '\\\\U%08X' % ord(ch)\n                    self.column += len(data)\n                    if self.encoding:\n                        data = data.encode(self.encoding)\n                    self.stream.write(data)\n                    start = end+1\n            if 0 < end < len(text)-1 and (ch == ' ' or start >= end)    \\\n                    and self.column+(end-start) > self.best_width and split:\n                data = text[start:end]+'\\\\'\n                if start < end:\n                    start = end\n                self.column += len(data)\n                if self.encoding:\n                    data = data.encode(self.encoding)\n                self.stream.write(data)\n                self.write_indent()\n                self.whitespace = False\n                self.indention = False\n                if text[start] == ' ':\n                    data = '\\\\'\n                    self.column += len(data)\n                    if self.encoding:\n                        data = data.encode(self.encoding)\n                    self.stream.write(data)\n            end += 1\n        self.write_indicator('\"', False)\n\n    def determine_block_hints(self, text):\n        hints = ''\n        if text:\n            if text[0] in ' \\n\\x85\\u2028\\u2029':\n                hints += str(self.best_indent)\n            if text[-1] not in '\\n\\x85\\u2028\\u2029':\n                hints += '-'\n            elif len(text) == 1 or text[-2] in '\\n\\x85\\u2028\\u2029':\n                hints += '+'\n        return hints\n\n    def write_folded(self, text):\n        hints = self.determine_block_hints(text)\n        self.write_indicator('>'+hints, True)\n        if hints[-1:] == '+':\n            self.open_ended = True\n        self.write_line_break()\n        leading_space = True\n        spaces = False\n        breaks = True\n        start = end = 0\n        while end <= len(text):\n            ch = None\n            if end < len(text):\n                ch = text[end]\n            if breaks:\n                if ch is None or ch not in '\\n\\x85\\u2028\\u2029':\n                    if not leading_space and ch is not None and ch != ' '   \\\n                            and text[start] == '\\n':\n                        self.write_line_break()\n                    leading_space = (ch == ' ')\n                    for br in text[start:end]:\n                        if br == '\\n':\n                            self.write_line_break()\n                        else:\n                            self.write_line_break(br)\n                    if ch is not None:\n                        self.write_indent()\n                    start = end\n            elif spaces:\n                if ch != ' ':\n                    if start+1 == end and self.column > self.best_width:\n                        self.write_indent()\n                    else:\n                        data = text[start:end]\n                        self.column += len(data)\n                        if self.encoding:\n                            data = data.encode(self.encoding)\n                        self.stream.write(data)\n                    start = end\n            else:\n                if ch is None or ch in ' \\n\\x85\\u2028\\u2029':\n                    data = text[start:end]\n                    self.column += len(data)\n                    if self.encoding:\n                        data = data.encode(self.encoding)\n                    self.stream.write(data)\n                    if ch is None:\n                        self.write_line_break()\n                    start = end\n            if ch is not None:\n                breaks = (ch in '\\n\\x85\\u2028\\u2029')\n                spaces = (ch == ' ')\n            end += 1\n\n    def write_literal(self, text):\n        hints = self.determine_block_hints(text)\n        self.write_indicator('|'+hints, True)\n        if hints[-1:] == '+':\n            self.open_ended = True\n        self.write_line_break()\n        breaks = True\n        start = end = 0\n        while end <= len(text):\n            ch = None\n            if end < len(text):\n                ch = text[end]\n            if breaks:\n                if ch is None or ch not in '\\n\\x85\\u2028\\u2029':\n                    for br in text[start:end]:\n                        if br == '\\n':\n                            self.write_line_break()\n                        else:\n                            self.write_line_break(br)\n                    if ch is not None:\n                        self.write_indent()\n                    start = end\n            else:\n                if ch is None or ch in '\\n\\x85\\u2028\\u2029':\n                    data = text[start:end]\n                    if self.encoding:\n                        data = data.encode(self.encoding)\n                    self.stream.write(data)\n                    if ch is None:\n                        self.write_line_break()\n                    start = end\n            if ch is not None:\n                breaks = (ch in '\\n\\x85\\u2028\\u2029')\n            end += 1\n\n    def write_plain(self, text, split=True):\n        if self.root_context:\n            self.open_ended = True\n        if not text:\n            return\n        if not self.whitespace:\n            data = ' '\n            self.column += len(data)\n            if self.encoding:\n                data = data.encode(self.encoding)\n            self.stream.write(data)\n        self.whitespace = False\n        self.indention = False\n        spaces = False\n        breaks = False\n        start = end = 0\n        while end <= len(text):\n            ch = None\n            if end < len(text):\n                ch = text[end]\n            if spaces:\n                if ch != ' ':\n                    if start+1 == end and self.column > self.best_width and split:\n                        self.write_indent()\n                        self.whitespace = False\n                        self.indention = False\n                    else:\n                        data = text[start:end]\n                        self.column += len(data)\n                        if self.encoding:\n                            data = data.encode(self.encoding)\n                        self.stream.write(data)\n                    start = end\n            elif breaks:\n                if ch not in '\\n\\x85\\u2028\\u2029':\n                    if text[start] == '\\n':\n                        self.write_line_break()\n                    for br in text[start:end]:\n                        if br == '\\n':\n                            self.write_line_break()\n                        else:\n                            self.write_line_break(br)\n                    self.write_indent()\n                    self.whitespace = False\n                    self.indention = False\n                    start = end\n            else:\n                if ch is None or ch in ' \\n\\x85\\u2028\\u2029':\n                    data = text[start:end]\n                    self.column += len(data)\n                    if self.encoding:\n                        data = data.encode(self.encoding)\n                    self.stream.write(data)\n                    start = end\n            if ch is not None:\n                spaces = (ch == ' ')\n                breaks = (ch in '\\n\\x85\\u2028\\u2029')\n            end += 1\n", "lib/yaml/events.py": "\n# Abstract classes.\n\nclass Event(object):\n    def __init__(self, start_mark=None, end_mark=None):\n        self.start_mark = start_mark\n        self.end_mark = end_mark\n    def __repr__(self):\n        attributes = [key for key in ['anchor', 'tag', 'implicit', 'value']\n                if hasattr(self, key)]\n        arguments = ', '.join(['%s=%r' % (key, getattr(self, key))\n                for key in attributes])\n        return '%s(%s)' % (self.__class__.__name__, arguments)\n\nclass NodeEvent(Event):\n    def __init__(self, anchor, start_mark=None, end_mark=None):\n        self.anchor = anchor\n        self.start_mark = start_mark\n        self.end_mark = end_mark\n\nclass CollectionStartEvent(NodeEvent):\n    def __init__(self, anchor, tag, implicit, start_mark=None, end_mark=None,\n            flow_style=None):\n        self.anchor = anchor\n        self.tag = tag\n        self.implicit = implicit\n        self.start_mark = start_mark\n        self.end_mark = end_mark\n        self.flow_style = flow_style\n\nclass CollectionEndEvent(Event):\n    pass\n\n# Implementations.\n\nclass StreamStartEvent(Event):\n    def __init__(self, start_mark=None, end_mark=None, encoding=None):\n        self.start_mark = start_mark\n        self.end_mark = end_mark\n        self.encoding = encoding\n\nclass StreamEndEvent(Event):\n    pass\n\nclass DocumentStartEvent(Event):\n    def __init__(self, start_mark=None, end_mark=None,\n            explicit=None, version=None, tags=None):\n        self.start_mark = start_mark\n        self.end_mark = end_mark\n        self.explicit = explicit\n        self.version = version\n        self.tags = tags\n\nclass DocumentEndEvent(Event):\n    def __init__(self, start_mark=None, end_mark=None,\n            explicit=None):\n        self.start_mark = start_mark\n        self.end_mark = end_mark\n        self.explicit = explicit\n\nclass AliasEvent(NodeEvent):\n    pass\n\nclass ScalarEvent(NodeEvent):\n    def __init__(self, anchor, tag, implicit, value,\n            start_mark=None, end_mark=None, style=None):\n        self.anchor = anchor\n        self.tag = tag\n        self.implicit = implicit\n        self.value = value\n        self.start_mark = start_mark\n        self.end_mark = end_mark\n        self.style = style\n\nclass SequenceStartEvent(CollectionStartEvent):\n    pass\n\nclass SequenceEndEvent(CollectionEndEvent):\n    pass\n\nclass MappingStartEvent(CollectionStartEvent):\n    pass\n\nclass MappingEndEvent(CollectionEndEvent):\n    pass\n\n", "lib/yaml/scanner.py": "\n# Scanner produces tokens of the following types:\n# STREAM-START\n# STREAM-END\n# DIRECTIVE(name, value)\n# DOCUMENT-START\n# DOCUMENT-END\n# BLOCK-SEQUENCE-START\n# BLOCK-MAPPING-START\n# BLOCK-END\n# FLOW-SEQUENCE-START\n# FLOW-MAPPING-START\n# FLOW-SEQUENCE-END\n# FLOW-MAPPING-END\n# BLOCK-ENTRY\n# FLOW-ENTRY\n# KEY\n# VALUE\n# ALIAS(value)\n# ANCHOR(value)\n# TAG(value)\n# SCALAR(value, plain, style)\n#\n# Read comments in the Scanner code for more details.\n#\n\n__all__ = ['Scanner', 'ScannerError']\n\nfrom .error import MarkedYAMLError\nfrom .tokens import *\n\nclass ScannerError(MarkedYAMLError):\n    pass\n\nclass SimpleKey:\n    # See below simple keys treatment.\n\n    def __init__(self, token_number, required, index, line, column, mark):\n        self.token_number = token_number\n        self.required = required\n        self.index = index\n        self.line = line\n        self.column = column\n        self.mark = mark\n\nclass Scanner:\n\n    def __init__(self):\n        \"\"\"Initialize the scanner.\"\"\"\n        # It is assumed that Scanner and Reader will have a common descendant.\n        # Reader do the dirty work of checking for BOM and converting the\n        # input data to Unicode. It also adds NUL to the end.\n        #\n        # Reader supports the following methods\n        #   self.peek(i=0)       # peek the next i-th character\n        #   self.prefix(l=1)     # peek the next l characters\n        #   self.forward(l=1)    # read the next l characters and move the pointer.\n\n        # Had we reached the end of the stream?\n        self.done = False\n\n        # The number of unclosed '{' and '['. `flow_level == 0` means block\n        # context.\n        self.flow_level = 0\n\n        # List of processed tokens that are not yet emitted.\n        self.tokens = []\n\n        # Add the STREAM-START token.\n        self.fetch_stream_start()\n\n        # Number of tokens that were emitted through the `get_token` method.\n        self.tokens_taken = 0\n\n        # The current indentation level.\n        self.indent = -1\n\n        # Past indentation levels.\n        self.indents = []\n\n        # Variables related to simple keys treatment.\n\n        # A simple key is a key that is not denoted by the '?' indicator.\n        # Example of simple keys:\n        #   ---\n        #   block simple key: value\n        #   ? not a simple key:\n        #   : { flow simple key: value }\n        # We emit the KEY token before all keys, so when we find a potential\n        # simple key, we try to locate the corresponding ':' indicator.\n        # Simple keys should be limited to a single line and 1024 characters.\n\n        # Can a simple key start at the current position? A simple key may\n        # start:\n        # - at the beginning of the line, not counting indentation spaces\n        #       (in block context),\n        # - after '{', '[', ',' (in the flow context),\n        # - after '?', ':', '-' (in the block context).\n        # In the block context, this flag also signifies if a block collection\n        # may start at the current position.\n        self.allow_simple_key = True\n\n        # Keep track of possible simple keys. This is a dictionary. The key\n        # is `flow_level`; there can be no more that one possible simple key\n        # for each level. The value is a SimpleKey record:\n        #   (token_number, required, index, line, column, mark)\n        # A simple key may start with ALIAS, ANCHOR, TAG, SCALAR(flow),\n        # '[', or '{' tokens.\n        self.possible_simple_keys = {}\n\n    # Public methods.\n\n    def check_token(self, *choices):\n        # Check if the next token is one of the given types.\n        while self.need_more_tokens():\n            self.fetch_more_tokens()\n        if self.tokens:\n            if not choices:\n                return True\n            for choice in choices:\n                if isinstance(self.tokens[0], choice):\n                    return True\n        return False\n\n    def peek_token(self):\n        # Return the next token, but do not delete if from the queue.\n        # Return None if no more tokens.\n        while self.need_more_tokens():\n            self.fetch_more_tokens()\n        if self.tokens:\n            return self.tokens[0]\n        else:\n            return None\n\n    def get_token(self):\n        # Return the next token.\n        while self.need_more_tokens():\n            self.fetch_more_tokens()\n        if self.tokens:\n            self.tokens_taken += 1\n            return self.tokens.pop(0)\n\n    # Private methods.\n\n    def need_more_tokens(self):\n        if self.done:\n            return False\n        if not self.tokens:\n            return True\n        # The current token may be a potential simple key, so we\n        # need to look further.\n        self.stale_possible_simple_keys()\n        if self.next_possible_simple_key() == self.tokens_taken:\n            return True\n\n    def fetch_more_tokens(self):\n\n        # Eat whitespaces and comments until we reach the next token.\n        self.scan_to_next_token()\n\n        # Remove obsolete possible simple keys.\n        self.stale_possible_simple_keys()\n\n        # Compare the current indentation and column. It may add some tokens\n        # and decrease the current indentation level.\n        self.unwind_indent(self.column)\n\n        # Peek the next character.\n        ch = self.peek()\n\n        # Is it the end of stream?\n        if ch == '\\0':\n            return self.fetch_stream_end()\n\n        # Is it a directive?\n        if ch == '%' and self.check_directive():\n            return self.fetch_directive()\n\n        # Is it the document start?\n        if ch == '-' and self.check_document_start():\n            return self.fetch_document_start()\n\n        # Is it the document end?\n        if ch == '.' and self.check_document_end():\n            return self.fetch_document_end()\n\n        # TODO: support for BOM within a stream.\n        #if ch == '\\uFEFF':\n        #    return self.fetch_bom()    <-- issue BOMToken\n\n        # Note: the order of the following checks is NOT significant.\n\n        # Is it the flow sequence start indicator?\n        if ch == '[':\n            return self.fetch_flow_sequence_start()\n\n        # Is it the flow mapping start indicator?\n        if ch == '{':\n            return self.fetch_flow_mapping_start()\n\n        # Is it the flow sequence end indicator?\n        if ch == ']':\n            return self.fetch_flow_sequence_end()\n\n        # Is it the flow mapping end indicator?\n        if ch == '}':\n            return self.fetch_flow_mapping_end()\n\n        # Is it the flow entry indicator?\n        if ch == ',':\n            return self.fetch_flow_entry()\n\n        # Is it the block entry indicator?\n        if ch == '-' and self.check_block_entry():\n            return self.fetch_block_entry()\n\n        # Is it the key indicator?\n        if ch == '?' and self.check_key():\n            return self.fetch_key()\n\n        # Is it the value indicator?\n        if ch == ':' and self.check_value():\n            return self.fetch_value()\n\n        # Is it an alias?\n        if ch == '*':\n            return self.fetch_alias()\n\n        # Is it an anchor?\n        if ch == '&':\n            return self.fetch_anchor()\n\n        # Is it a tag?\n        if ch == '!':\n            return self.fetch_tag()\n\n        # Is it a literal scalar?\n        if ch == '|' and not self.flow_level:\n            return self.fetch_literal()\n\n        # Is it a folded scalar?\n        if ch == '>' and not self.flow_level:\n            return self.fetch_folded()\n\n        # Is it a single quoted scalar?\n        if ch == '\\'':\n            return self.fetch_single()\n\n        # Is it a double quoted scalar?\n        if ch == '\\\"':\n            return self.fetch_double()\n\n        # It must be a plain scalar then.\n        if self.check_plain():\n            return self.fetch_plain()\n\n        # No? It's an error. Let's produce a nice error message.\n        raise ScannerError(\"while scanning for the next token\", None,\n                \"found character %r that cannot start any token\" % ch,\n                self.get_mark())\n\n    # Simple keys treatment.\n\n    def next_possible_simple_key(self):\n        # Return the number of the nearest possible simple key. Actually we\n        # don't need to loop through the whole dictionary. We may replace it\n        # with the following code:\n        #   if not self.possible_simple_keys:\n        #       return None\n        #   return self.possible_simple_keys[\n        #           min(self.possible_simple_keys.keys())].token_number\n        min_token_number = None\n        for level in self.possible_simple_keys:\n            key = self.possible_simple_keys[level]\n            if min_token_number is None or key.token_number < min_token_number:\n                min_token_number = key.token_number\n        return min_token_number\n\n    def stale_possible_simple_keys(self):\n        # Remove entries that are no longer possible simple keys. According to\n        # the YAML specification, simple keys\n        # - should be limited to a single line,\n        # - should be no longer than 1024 characters.\n        # Disabling this procedure will allow simple keys of any length and\n        # height (may cause problems if indentation is broken though).\n        for level in list(self.possible_simple_keys):\n            key = self.possible_simple_keys[level]\n            if key.line != self.line  \\\n                    or self.index-key.index > 1024:\n                if key.required:\n                    raise ScannerError(\"while scanning a simple key\", key.mark,\n                            \"could not find expected ':'\", self.get_mark())\n                del self.possible_simple_keys[level]\n\n    def save_possible_simple_key(self):\n        # The next token may start a simple key. We check if it's possible\n        # and save its position. This function is called for\n        #   ALIAS, ANCHOR, TAG, SCALAR(flow), '[', and '{'.\n\n        # Check if a simple key is required at the current position.\n        required = not self.flow_level and self.indent == self.column\n\n        # The next token might be a simple key. Let's save it's number and\n        # position.\n        if self.allow_simple_key:\n            self.remove_possible_simple_key()\n            token_number = self.tokens_taken+len(self.tokens)\n            key = SimpleKey(token_number, required,\n                    self.index, self.line, self.column, self.get_mark())\n            self.possible_simple_keys[self.flow_level] = key\n\n    def remove_possible_simple_key(self):\n        # Remove the saved possible key position at the current flow level.\n        if self.flow_level in self.possible_simple_keys:\n            key = self.possible_simple_keys[self.flow_level]\n            \n            if key.required:\n                raise ScannerError(\"while scanning a simple key\", key.mark,\n                        \"could not find expected ':'\", self.get_mark())\n\n            del self.possible_simple_keys[self.flow_level]\n\n    # Indentation functions.\n\n    def unwind_indent(self, column):\n\n        ## In flow context, tokens should respect indentation.\n        ## Actually the condition should be `self.indent >= column` according to\n        ## the spec. But this condition will prohibit intuitively correct\n        ## constructions such as\n        ## key : {\n        ## }\n        #if self.flow_level and self.indent > column:\n        #    raise ScannerError(None, None,\n        #            \"invalid indentation or unclosed '[' or '{'\",\n        #            self.get_mark())\n\n        # In the flow context, indentation is ignored. We make the scanner less\n        # restrictive then specification requires.\n        if self.flow_level:\n            return\n\n        # In block context, we may need to issue the BLOCK-END tokens.\n        while self.indent > column:\n            mark = self.get_mark()\n            self.indent = self.indents.pop()\n            self.tokens.append(BlockEndToken(mark, mark))\n\n    def add_indent(self, column):\n        # Check if we need to increase indentation.\n        if self.indent < column:\n            self.indents.append(self.indent)\n            self.indent = column\n            return True\n        return False\n\n    # Fetchers.\n\n    def fetch_stream_start(self):\n        # We always add STREAM-START as the first token and STREAM-END as the\n        # last token.\n\n        # Read the token.\n        mark = self.get_mark()\n        \n        # Add STREAM-START.\n        self.tokens.append(StreamStartToken(mark, mark,\n            encoding=self.encoding))\n        \n\n    def fetch_stream_end(self):\n\n        # Set the current indentation to -1.\n        self.unwind_indent(-1)\n\n        # Reset simple keys.\n        self.remove_possible_simple_key()\n        self.allow_simple_key = False\n        self.possible_simple_keys = {}\n\n        # Read the token.\n        mark = self.get_mark()\n        \n        # Add STREAM-END.\n        self.tokens.append(StreamEndToken(mark, mark))\n\n        # The steam is finished.\n        self.done = True\n\n    def fetch_directive(self):\n        \n        # Set the current indentation to -1.\n        self.unwind_indent(-1)\n\n        # Reset simple keys.\n        self.remove_possible_simple_key()\n        self.allow_simple_key = False\n\n        # Scan and add DIRECTIVE.\n        self.tokens.append(self.scan_directive())\n\n    def fetch_document_start(self):\n        self.fetch_document_indicator(DocumentStartToken)\n\n    def fetch_document_end(self):\n        self.fetch_document_indicator(DocumentEndToken)\n\n    def fetch_document_indicator(self, TokenClass):\n\n        # Set the current indentation to -1.\n        self.unwind_indent(-1)\n\n        # Reset simple keys. Note that there could not be a block collection\n        # after '---'.\n        self.remove_possible_simple_key()\n        self.allow_simple_key = False\n\n        # Add DOCUMENT-START or DOCUMENT-END.\n        start_mark = self.get_mark()\n        self.forward(3)\n        end_mark = self.get_mark()\n        self.tokens.append(TokenClass(start_mark, end_mark))\n\n    def fetch_flow_sequence_start(self):\n        self.fetch_flow_collection_start(FlowSequenceStartToken)\n\n    def fetch_flow_mapping_start(self):\n        self.fetch_flow_collection_start(FlowMappingStartToken)\n\n    def fetch_flow_collection_start(self, TokenClass):\n\n        # '[' and '{' may start a simple key.\n        self.save_possible_simple_key()\n\n        # Increase the flow level.\n        self.flow_level += 1\n\n        # Simple keys are allowed after '[' and '{'.\n        self.allow_simple_key = True\n\n        # Add FLOW-SEQUENCE-START or FLOW-MAPPING-START.\n        start_mark = self.get_mark()\n        self.forward()\n        end_mark = self.get_mark()\n        self.tokens.append(TokenClass(start_mark, end_mark))\n\n    def fetch_flow_sequence_end(self):\n        self.fetch_flow_collection_end(FlowSequenceEndToken)\n\n    def fetch_flow_mapping_end(self):\n        self.fetch_flow_collection_end(FlowMappingEndToken)\n\n    def fetch_flow_collection_end(self, TokenClass):\n\n        # Reset possible simple key on the current level.\n        self.remove_possible_simple_key()\n\n        # Decrease the flow level.\n        self.flow_level -= 1\n\n        # No simple keys after ']' or '}'.\n        self.allow_simple_key = False\n\n        # Add FLOW-SEQUENCE-END or FLOW-MAPPING-END.\n        start_mark = self.get_mark()\n        self.forward()\n        end_mark = self.get_mark()\n        self.tokens.append(TokenClass(start_mark, end_mark))\n\n    def fetch_flow_entry(self):\n\n        # Simple keys are allowed after ','.\n        self.allow_simple_key = True\n\n        # Reset possible simple key on the current level.\n        self.remove_possible_simple_key()\n\n        # Add FLOW-ENTRY.\n        start_mark = self.get_mark()\n        self.forward()\n        end_mark = self.get_mark()\n        self.tokens.append(FlowEntryToken(start_mark, end_mark))\n\n    def fetch_block_entry(self):\n\n        # Block context needs additional checks.\n        if not self.flow_level:\n\n            # Are we allowed to start a new entry?\n            if not self.allow_simple_key:\n                raise ScannerError(None, None,\n                        \"sequence entries are not allowed here\",\n                        self.get_mark())\n\n            # We may need to add BLOCK-SEQUENCE-START.\n            if self.add_indent(self.column):\n                mark = self.get_mark()\n                self.tokens.append(BlockSequenceStartToken(mark, mark))\n\n        # It's an error for the block entry to occur in the flow context,\n        # but we let the parser detect this.\n        else:\n            pass\n\n        # Simple keys are allowed after '-'.\n        self.allow_simple_key = True\n\n        # Reset possible simple key on the current level.\n        self.remove_possible_simple_key()\n\n        # Add BLOCK-ENTRY.\n        start_mark = self.get_mark()\n        self.forward()\n        end_mark = self.get_mark()\n        self.tokens.append(BlockEntryToken(start_mark, end_mark))\n\n    def fetch_key(self):\n        \n        # Block context needs additional checks.\n        if not self.flow_level:\n\n            # Are we allowed to start a key (not necessary a simple)?\n            if not self.allow_simple_key:\n                raise ScannerError(None, None,\n                        \"mapping keys are not allowed here\",\n                        self.get_mark())\n\n            # We may need to add BLOCK-MAPPING-START.\n            if self.add_indent(self.column):\n                mark = self.get_mark()\n                self.tokens.append(BlockMappingStartToken(mark, mark))\n\n        # Simple keys are allowed after '?' in the block context.\n        self.allow_simple_key = not self.flow_level\n\n        # Reset possible simple key on the current level.\n        self.remove_possible_simple_key()\n\n        # Add KEY.\n        start_mark = self.get_mark()\n        self.forward()\n        end_mark = self.get_mark()\n        self.tokens.append(KeyToken(start_mark, end_mark))\n\n    def fetch_value(self):\n\n        # Do we determine a simple key?\n        if self.flow_level in self.possible_simple_keys:\n\n            # Add KEY.\n            key = self.possible_simple_keys[self.flow_level]\n            del self.possible_simple_keys[self.flow_level]\n            self.tokens.insert(key.token_number-self.tokens_taken,\n                    KeyToken(key.mark, key.mark))\n\n            # If this key starts a new block mapping, we need to add\n            # BLOCK-MAPPING-START.\n            if not self.flow_level:\n                if self.add_indent(key.column):\n                    self.tokens.insert(key.token_number-self.tokens_taken,\n                            BlockMappingStartToken(key.mark, key.mark))\n\n            # There cannot be two simple keys one after another.\n            self.allow_simple_key = False\n\n        # It must be a part of a complex key.\n        else:\n            \n            # Block context needs additional checks.\n            # (Do we really need them? They will be caught by the parser\n            # anyway.)\n            if not self.flow_level:\n\n                # We are allowed to start a complex value if and only if\n                # we can start a simple key.\n                if not self.allow_simple_key:\n                    raise ScannerError(None, None,\n                            \"mapping values are not allowed here\",\n                            self.get_mark())\n\n            # If this value starts a new block mapping, we need to add\n            # BLOCK-MAPPING-START.  It will be detected as an error later by\n            # the parser.\n            if not self.flow_level:\n                if self.add_indent(self.column):\n                    mark = self.get_mark()\n                    self.tokens.append(BlockMappingStartToken(mark, mark))\n\n            # Simple keys are allowed after ':' in the block context.\n            self.allow_simple_key = not self.flow_level\n\n            # Reset possible simple key on the current level.\n            self.remove_possible_simple_key()\n\n        # Add VALUE.\n        start_mark = self.get_mark()\n        self.forward()\n        end_mark = self.get_mark()\n        self.tokens.append(ValueToken(start_mark, end_mark))\n\n    def fetch_alias(self):\n\n        # ALIAS could be a simple key.\n        self.save_possible_simple_key()\n\n        # No simple keys after ALIAS.\n        self.allow_simple_key = False\n\n        # Scan and add ALIAS.\n        self.tokens.append(self.scan_anchor(AliasToken))\n\n    def fetch_anchor(self):\n\n        # ANCHOR could start a simple key.\n        self.save_possible_simple_key()\n\n        # No simple keys after ANCHOR.\n        self.allow_simple_key = False\n\n        # Scan and add ANCHOR.\n        self.tokens.append(self.scan_anchor(AnchorToken))\n\n    def fetch_tag(self):\n\n        # TAG could start a simple key.\n        self.save_possible_simple_key()\n\n        # No simple keys after TAG.\n        self.allow_simple_key = False\n\n        # Scan and add TAG.\n        self.tokens.append(self.scan_tag())\n\n    def fetch_literal(self):\n        self.fetch_block_scalar(style='|')\n\n    def fetch_folded(self):\n        self.fetch_block_scalar(style='>')\n\n    def fetch_block_scalar(self, style):\n\n        # A simple key may follow a block scalar.\n        self.allow_simple_key = True\n\n        # Reset possible simple key on the current level.\n        self.remove_possible_simple_key()\n\n        # Scan and add SCALAR.\n        self.tokens.append(self.scan_block_scalar(style))\n\n    def fetch_single(self):\n        self.fetch_flow_scalar(style='\\'')\n\n    def fetch_double(self):\n        self.fetch_flow_scalar(style='\"')\n\n    def fetch_flow_scalar(self, style):\n\n        # A flow scalar could be a simple key.\n        self.save_possible_simple_key()\n\n        # No simple keys after flow scalars.\n        self.allow_simple_key = False\n\n        # Scan and add SCALAR.\n        self.tokens.append(self.scan_flow_scalar(style))\n\n    def fetch_plain(self):\n\n        # A plain scalar could be a simple key.\n        self.save_possible_simple_key()\n\n        # No simple keys after plain scalars. But note that `scan_plain` will\n        # change this flag if the scan is finished at the beginning of the\n        # line.\n        self.allow_simple_key = False\n\n        # Scan and add SCALAR. May change `allow_simple_key`.\n        self.tokens.append(self.scan_plain())\n\n    # Checkers.\n\n    def check_directive(self):\n\n        # DIRECTIVE:        ^ '%' ...\n        # The '%' indicator is already checked.\n        if self.column == 0:\n            return True\n\n    def check_document_start(self):\n\n        # DOCUMENT-START:   ^ '---' (' '|'\\n')\n        if self.column == 0:\n            if self.prefix(3) == '---'  \\\n                    and self.peek(3) in '\\0 \\t\\r\\n\\x85\\u2028\\u2029':\n                return True\n\n    def check_document_end(self):\n\n        # DOCUMENT-END:     ^ '...' (' '|'\\n')\n        if self.column == 0:\n            if self.prefix(3) == '...'  \\\n                    and self.peek(3) in '\\0 \\t\\r\\n\\x85\\u2028\\u2029':\n                return True\n\n    def check_block_entry(self):\n\n        # BLOCK-ENTRY:      '-' (' '|'\\n')\n        return self.peek(1) in '\\0 \\t\\r\\n\\x85\\u2028\\u2029'\n\n    def check_key(self):\n\n        # KEY(flow context):    '?'\n        if self.flow_level:\n            return True\n\n        # KEY(block context):   '?' (' '|'\\n')\n        else:\n            return self.peek(1) in '\\0 \\t\\r\\n\\x85\\u2028\\u2029'\n\n    def check_value(self):\n\n        # VALUE(flow context):  ':'\n        if self.flow_level:\n            return True\n\n        # VALUE(block context): ':' (' '|'\\n')\n        else:\n            return self.peek(1) in '\\0 \\t\\r\\n\\x85\\u2028\\u2029'\n\n    def check_plain(self):\n\n        # A plain scalar may start with any non-space character except:\n        #   '-', '?', ':', ',', '[', ']', '{', '}',\n        #   '#', '&', '*', '!', '|', '>', '\\'', '\\\"',\n        #   '%', '@', '`'.\n        #\n        # It may also start with\n        #   '-', '?', ':'\n        # if it is followed by a non-space character.\n        #\n        # Note that we limit the last rule to the block context (except the\n        # '-' character) because we want the flow context to be space\n        # independent.\n        ch = self.peek()\n        return ch not in '\\0 \\t\\r\\n\\x85\\u2028\\u2029-?:,[]{}#&*!|>\\'\\\"%@`'  \\\n                or (self.peek(1) not in '\\0 \\t\\r\\n\\x85\\u2028\\u2029'\n                        and (ch == '-' or (not self.flow_level and ch in '?:')))\n\n    # Scanners.\n\n    def scan_to_next_token(self):\n        # We ignore spaces, line breaks and comments.\n        # If we find a line break in the block context, we set the flag\n        # `allow_simple_key` on.\n        # The byte order mark is stripped if it's the first character in the\n        # stream. We do not yet support BOM inside the stream as the\n        # specification requires. Any such mark will be considered as a part\n        # of the document.\n        #\n        # TODO: We need to make tab handling rules more sane. A good rule is\n        #   Tabs cannot precede tokens\n        #   BLOCK-SEQUENCE-START, BLOCK-MAPPING-START, BLOCK-END,\n        #   KEY(block), VALUE(block), BLOCK-ENTRY\n        # So the checking code is\n        #   if <TAB>:\n        #       self.allow_simple_keys = False\n        # We also need to add the check for `allow_simple_keys == True` to\n        # `unwind_indent` before issuing BLOCK-END.\n        # Scanners for block, flow, and plain scalars need to be modified.\n\n        if self.index == 0 and self.peek() == '\\uFEFF':\n            self.forward()\n        found = False\n        while not found:\n            while self.peek() == ' ':\n                self.forward()\n            if self.peek() == '#':\n                while self.peek() not in '\\0\\r\\n\\x85\\u2028\\u2029':\n                    self.forward()\n            if self.scan_line_break():\n                if not self.flow_level:\n                    self.allow_simple_key = True\n            else:\n                found = True\n\n    def scan_directive(self):\n        # See the specification for details.\n        start_mark = self.get_mark()\n        self.forward()\n        name = self.scan_directive_name(start_mark)\n        value = None\n        if name == 'YAML':\n            value = self.scan_yaml_directive_value(start_mark)\n            end_mark = self.get_mark()\n        elif name == 'TAG':\n            value = self.scan_tag_directive_value(start_mark)\n            end_mark = self.get_mark()\n        else:\n            end_mark = self.get_mark()\n            while self.peek() not in '\\0\\r\\n\\x85\\u2028\\u2029':\n                self.forward()\n        self.scan_directive_ignored_line(start_mark)\n        return DirectiveToken(name, value, start_mark, end_mark)\n\n    def scan_directive_name(self, start_mark):\n        # See the specification for details.\n        length = 0\n        ch = self.peek(length)\n        while '0' <= ch <= '9' or 'A' <= ch <= 'Z' or 'a' <= ch <= 'z'  \\\n                or ch in '-_':\n            length += 1\n            ch = self.peek(length)\n        if not length:\n            raise ScannerError(\"while scanning a directive\", start_mark,\n                    \"expected alphabetic or numeric character, but found %r\"\n                    % ch, self.get_mark())\n        value = self.prefix(length)\n        self.forward(length)\n        ch = self.peek()\n        if ch not in '\\0 \\r\\n\\x85\\u2028\\u2029':\n            raise ScannerError(\"while scanning a directive\", start_mark,\n                    \"expected alphabetic or numeric character, but found %r\"\n                    % ch, self.get_mark())\n        return value\n\n    def scan_yaml_directive_value(self, start_mark):\n        # See the specification for details.\n        while self.peek() == ' ':\n            self.forward()\n        major = self.scan_yaml_directive_number(start_mark)\n        if self.peek() != '.':\n            raise ScannerError(\"while scanning a directive\", start_mark,\n                    \"expected a digit or '.', but found %r\" % self.peek(),\n                    self.get_mark())\n        self.forward()\n        minor = self.scan_yaml_directive_number(start_mark)\n        if self.peek() not in '\\0 \\r\\n\\x85\\u2028\\u2029':\n            raise ScannerError(\"while scanning a directive\", start_mark,\n                    \"expected a digit or ' ', but found %r\" % self.peek(),\n                    self.get_mark())\n        return (major, minor)\n\n    def scan_yaml_directive_number(self, start_mark):\n        # See the specification for details.\n        ch = self.peek()\n        if not ('0' <= ch <= '9'):\n            raise ScannerError(\"while scanning a directive\", start_mark,\n                    \"expected a digit, but found %r\" % ch, self.get_mark())\n        length = 0\n        while '0' <= self.peek(length) <= '9':\n            length += 1\n        value = int(self.prefix(length))\n        self.forward(length)\n        return value\n\n    def scan_tag_directive_value(self, start_mark):\n        # See the specification for details.\n        while self.peek() == ' ':\n            self.forward()\n        handle = self.scan_tag_directive_handle(start_mark)\n        while self.peek() == ' ':\n            self.forward()\n        prefix = self.scan_tag_directive_prefix(start_mark)\n        return (handle, prefix)\n\n    def scan_tag_directive_handle(self, start_mark):\n        # See the specification for details.\n        value = self.scan_tag_handle('directive', start_mark)\n        ch = self.peek()\n        if ch != ' ':\n            raise ScannerError(\"while scanning a directive\", start_mark,\n                    \"expected ' ', but found %r\" % ch, self.get_mark())\n        return value\n\n    def scan_tag_directive_prefix(self, start_mark):\n        # See the specification for details.\n        value = self.scan_tag_uri('directive', start_mark)\n        ch = self.peek()\n        if ch not in '\\0 \\r\\n\\x85\\u2028\\u2029':\n            raise ScannerError(\"while scanning a directive\", start_mark,\n                    \"expected ' ', but found %r\" % ch, self.get_mark())\n        return value\n\n    def scan_directive_ignored_line(self, start_mark):\n        # See the specification for details.\n        while self.peek() == ' ':\n            self.forward()\n        if self.peek() == '#':\n            while self.peek() not in '\\0\\r\\n\\x85\\u2028\\u2029':\n                self.forward()\n        ch = self.peek()\n        if ch not in '\\0\\r\\n\\x85\\u2028\\u2029':\n            raise ScannerError(\"while scanning a directive\", start_mark,\n                    \"expected a comment or a line break, but found %r\"\n                        % ch, self.get_mark())\n        self.scan_line_break()\n\n    def scan_anchor(self, TokenClass):\n        # The specification does not restrict characters for anchors and\n        # aliases. This may lead to problems, for instance, the document:\n        #   [ *alias, value ]\n        # can be interpreted in two ways, as\n        #   [ \"value\" ]\n        # and\n        #   [ *alias , \"value\" ]\n        # Therefore we restrict aliases to numbers and ASCII letters.\n        start_mark = self.get_mark()\n        indicator = self.peek()\n        if indicator == '*':\n            name = 'alias'\n        else:\n            name = 'anchor'\n        self.forward()\n        length = 0\n        ch = self.peek(length)\n        while '0' <= ch <= '9' or 'A' <= ch <= 'Z' or 'a' <= ch <= 'z'  \\\n                or ch in '-_':\n            length += 1\n            ch = self.peek(length)\n        if not length:\n            raise ScannerError(\"while scanning an %s\" % name, start_mark,\n                    \"expected alphabetic or numeric character, but found %r\"\n                    % ch, self.get_mark())\n        value = self.prefix(length)\n        self.forward(length)\n        ch = self.peek()\n        if ch not in '\\0 \\t\\r\\n\\x85\\u2028\\u2029?:,]}%@`':\n            raise ScannerError(\"while scanning an %s\" % name, start_mark,\n                    \"expected alphabetic or numeric character, but found %r\"\n                    % ch, self.get_mark())\n        end_mark = self.get_mark()\n        return TokenClass(value, start_mark, end_mark)\n\n    def scan_tag(self):\n        # See the specification for details.\n        start_mark = self.get_mark()\n        ch = self.peek(1)\n        if ch == '<':\n            handle = None\n            self.forward(2)\n            suffix = self.scan_tag_uri('tag', start_mark)\n            if self.peek() != '>':\n                raise ScannerError(\"while parsing a tag\", start_mark,\n                        \"expected '>', but found %r\" % self.peek(),\n                        self.get_mark())\n            self.forward()\n        elif ch in '\\0 \\t\\r\\n\\x85\\u2028\\u2029':\n            handle = None\n            suffix = '!'\n            self.forward()\n        else:\n            length = 1\n            use_handle = False\n            while ch not in '\\0 \\r\\n\\x85\\u2028\\u2029':\n                if ch == '!':\n                    use_handle = True\n                    break\n                length += 1\n                ch = self.peek(length)\n            handle = '!'\n            if use_handle:\n                handle = self.scan_tag_handle('tag', start_mark)\n            else:\n                handle = '!'\n                self.forward()\n            suffix = self.scan_tag_uri('tag', start_mark)\n        ch = self.peek()\n        if ch not in '\\0 \\r\\n\\x85\\u2028\\u2029':\n            raise ScannerError(\"while scanning a tag\", start_mark,\n                    \"expected ' ', but found %r\" % ch, self.get_mark())\n        value = (handle, suffix)\n        end_mark = self.get_mark()\n        return TagToken(value, start_mark, end_mark)\n\n    def scan_block_scalar(self, style):\n        # See the specification for details.\n\n        if style == '>':\n            folded = True\n        else:\n            folded = False\n\n        chunks = []\n        start_mark = self.get_mark()\n\n        # Scan the header.\n        self.forward()\n        chomping, increment = self.scan_block_scalar_indicators(start_mark)\n        self.scan_block_scalar_ignored_line(start_mark)\n\n        # Determine the indentation level and go to the first non-empty line.\n        min_indent = self.indent+1\n        if min_indent < 1:\n            min_indent = 1\n        if increment is None:\n            breaks, max_indent, end_mark = self.scan_block_scalar_indentation()\n            indent = max(min_indent, max_indent)\n        else:\n            indent = min_indent+increment-1\n            breaks, end_mark = self.scan_block_scalar_breaks(indent)\n        line_break = ''\n\n        # Scan the inner part of the block scalar.\n        while self.column == indent and self.peek() != '\\0':\n            chunks.extend(breaks)\n            leading_non_space = self.peek() not in ' \\t'\n            length = 0\n            while self.peek(length) not in '\\0\\r\\n\\x85\\u2028\\u2029':\n                length += 1\n            chunks.append(self.prefix(length))\n            self.forward(length)\n            line_break = self.scan_line_break()\n            breaks, end_mark = self.scan_block_scalar_breaks(indent)\n            if self.column == indent and self.peek() != '\\0':\n\n                # Unfortunately, folding rules are ambiguous.\n                #\n                # This is the folding according to the specification:\n                \n                if folded and line_break == '\\n'    \\\n                        and leading_non_space and self.peek() not in ' \\t':\n                    if not breaks:\n                        chunks.append(' ')\n                else:\n                    chunks.append(line_break)\n                \n                # This is Clark Evans's interpretation (also in the spec\n                # examples):\n                #\n                #if folded and line_break == '\\n':\n                #    if not breaks:\n                #        if self.peek() not in ' \\t':\n                #            chunks.append(' ')\n                #        else:\n                #            chunks.append(line_break)\n                #else:\n                #    chunks.append(line_break)\n            else:\n                break\n\n        # Chomp the tail.\n        if chomping is not False:\n            chunks.append(line_break)\n        if chomping is True:\n            chunks.extend(breaks)\n\n        # We are done.\n        return ScalarToken(''.join(chunks), False, start_mark, end_mark,\n                style)\n\n    def scan_block_scalar_indicators(self, start_mark):\n        # See the specification for details.\n        chomping = None\n        increment = None\n        ch = self.peek()\n        if ch in '+-':\n            if ch == '+':\n                chomping = True\n            else:\n                chomping = False\n            self.forward()\n            ch = self.peek()\n            if ch in '0123456789':\n                increment = int(ch)\n                if increment == 0:\n                    raise ScannerError(\"while scanning a block scalar\", start_mark,\n                            \"expected indentation indicator in the range 1-9, but found 0\",\n                            self.get_mark())\n                self.forward()\n        elif ch in '0123456789':\n            increment = int(ch)\n            if increment == 0:\n                raise ScannerError(\"while scanning a block scalar\", start_mark,\n                        \"expected indentation indicator in the range 1-9, but found 0\",\n                        self.get_mark())\n            self.forward()\n            ch = self.peek()\n            if ch in '+-':\n                if ch == '+':\n                    chomping = True\n                else:\n                    chomping = False\n                self.forward()\n        ch = self.peek()\n        if ch not in '\\0 \\r\\n\\x85\\u2028\\u2029':\n            raise ScannerError(\"while scanning a block scalar\", start_mark,\n                    \"expected chomping or indentation indicators, but found %r\"\n                    % ch, self.get_mark())\n        return chomping, increment\n\n    def scan_block_scalar_ignored_line(self, start_mark):\n        # See the specification for details.\n        while self.peek() == ' ':\n            self.forward()\n        if self.peek() == '#':\n            while self.peek() not in '\\0\\r\\n\\x85\\u2028\\u2029':\n                self.forward()\n        ch = self.peek()\n        if ch not in '\\0\\r\\n\\x85\\u2028\\u2029':\n            raise ScannerError(\"while scanning a block scalar\", start_mark,\n                    \"expected a comment or a line break, but found %r\" % ch,\n                    self.get_mark())\n        self.scan_line_break()\n\n    def scan_block_scalar_indentation(self):\n        # See the specification for details.\n        chunks = []\n        max_indent = 0\n        end_mark = self.get_mark()\n        while self.peek() in ' \\r\\n\\x85\\u2028\\u2029':\n            if self.peek() != ' ':\n                chunks.append(self.scan_line_break())\n                end_mark = self.get_mark()\n            else:\n                self.forward()\n                if self.column > max_indent:\n                    max_indent = self.column\n        return chunks, max_indent, end_mark\n\n    def scan_block_scalar_breaks(self, indent):\n        # See the specification for details.\n        chunks = []\n        end_mark = self.get_mark()\n        while self.column < indent and self.peek() == ' ':\n            self.forward()\n        while self.peek() in '\\r\\n\\x85\\u2028\\u2029':\n            chunks.append(self.scan_line_break())\n            end_mark = self.get_mark()\n            while self.column < indent and self.peek() == ' ':\n                self.forward()\n        return chunks, end_mark\n\n    def scan_flow_scalar(self, style):\n        # See the specification for details.\n        # Note that we loose indentation rules for quoted scalars. Quoted\n        # scalars don't need to adhere indentation because \" and ' clearly\n        # mark the beginning and the end of them. Therefore we are less\n        # restrictive then the specification requires. We only need to check\n        # that document separators are not included in scalars.\n        if style == '\"':\n            double = True\n        else:\n            double = False\n        chunks = []\n        start_mark = self.get_mark()\n        quote = self.peek()\n        self.forward()\n        chunks.extend(self.scan_flow_scalar_non_spaces(double, start_mark))\n        while self.peek() != quote:\n            chunks.extend(self.scan_flow_scalar_spaces(double, start_mark))\n            chunks.extend(self.scan_flow_scalar_non_spaces(double, start_mark))\n        self.forward()\n        end_mark = self.get_mark()\n        return ScalarToken(''.join(chunks), False, start_mark, end_mark,\n                style)\n\n    ESCAPE_REPLACEMENTS = {\n        '0':    '\\0',\n        'a':    '\\x07',\n        'b':    '\\x08',\n        't':    '\\x09',\n        '\\t':   '\\x09',\n        'n':    '\\x0A',\n        'v':    '\\x0B',\n        'f':    '\\x0C',\n        'r':    '\\x0D',\n        'e':    '\\x1B',\n        ' ':    '\\x20',\n        '\\\"':   '\\\"',\n        '\\\\':   '\\\\',\n        '/':    '/',\n        'N':    '\\x85',\n        '_':    '\\xA0',\n        'L':    '\\u2028',\n        'P':    '\\u2029',\n    }\n\n    ESCAPE_CODES = {\n        'x':    2,\n        'u':    4,\n        'U':    8,\n    }\n\n    def scan_flow_scalar_non_spaces(self, double, start_mark):\n        # See the specification for details.\n        chunks = []\n        while True:\n            length = 0\n            while self.peek(length) not in '\\'\\\"\\\\\\0 \\t\\r\\n\\x85\\u2028\\u2029':\n                length += 1\n            if length:\n                chunks.append(self.prefix(length))\n                self.forward(length)\n            ch = self.peek()\n            if not double and ch == '\\'' and self.peek(1) == '\\'':\n                chunks.append('\\'')\n                self.forward(2)\n            elif (double and ch == '\\'') or (not double and ch in '\\\"\\\\'):\n                chunks.append(ch)\n                self.forward()\n            elif double and ch == '\\\\':\n                self.forward()\n                ch = self.peek()\n                if ch in self.ESCAPE_REPLACEMENTS:\n                    chunks.append(self.ESCAPE_REPLACEMENTS[ch])\n                    self.forward()\n                elif ch in self.ESCAPE_CODES:\n                    length = self.ESCAPE_CODES[ch]\n                    self.forward()\n                    for k in range(length):\n                        if self.peek(k) not in '0123456789ABCDEFabcdef':\n                            raise ScannerError(\"while scanning a double-quoted scalar\", start_mark,\n                                    \"expected escape sequence of %d hexadecimal numbers, but found %r\" %\n                                        (length, self.peek(k)), self.get_mark())\n                    code = int(self.prefix(length), 16)\n                    chunks.append(chr(code))\n                    self.forward(length)\n                elif ch in '\\r\\n\\x85\\u2028\\u2029':\n                    self.scan_line_break()\n                    chunks.extend(self.scan_flow_scalar_breaks(double, start_mark))\n                else:\n                    raise ScannerError(\"while scanning a double-quoted scalar\", start_mark,\n                            \"found unknown escape character %r\" % ch, self.get_mark())\n            else:\n                return chunks\n\n    def scan_flow_scalar_spaces(self, double, start_mark):\n        # See the specification for details.\n        chunks = []\n        length = 0\n        while self.peek(length) in ' \\t':\n            length += 1\n        whitespaces = self.prefix(length)\n        self.forward(length)\n        ch = self.peek()\n        if ch == '\\0':\n            raise ScannerError(\"while scanning a quoted scalar\", start_mark,\n                    \"found unexpected end of stream\", self.get_mark())\n        elif ch in '\\r\\n\\x85\\u2028\\u2029':\n            line_break = self.scan_line_break()\n            breaks = self.scan_flow_scalar_breaks(double, start_mark)\n            if line_break != '\\n':\n                chunks.append(line_break)\n            elif not breaks:\n                chunks.append(' ')\n            chunks.extend(breaks)\n        else:\n            chunks.append(whitespaces)\n        return chunks\n\n    def scan_flow_scalar_breaks(self, double, start_mark):\n        # See the specification for details.\n        chunks = []\n        while True:\n            # Instead of checking indentation, we check for document\n            # separators.\n            prefix = self.prefix(3)\n            if (prefix == '---' or prefix == '...')   \\\n                    and self.peek(3) in '\\0 \\t\\r\\n\\x85\\u2028\\u2029':\n                raise ScannerError(\"while scanning a quoted scalar\", start_mark,\n                        \"found unexpected document separator\", self.get_mark())\n            while self.peek() in ' \\t':\n                self.forward()\n            if self.peek() in '\\r\\n\\x85\\u2028\\u2029':\n                chunks.append(self.scan_line_break())\n            else:\n                return chunks\n\n    def scan_plain(self):\n        # See the specification for details.\n        # We add an additional restriction for the flow context:\n        #   plain scalars in the flow context cannot contain ',' or '?'.\n        # We also keep track of the `allow_simple_key` flag here.\n        # Indentation rules are loosed for the flow context.\n        chunks = []\n        start_mark = self.get_mark()\n        end_mark = start_mark\n        indent = self.indent+1\n        # We allow zero indentation for scalars, but then we need to check for\n        # document separators at the beginning of the line.\n        #if indent == 0:\n        #    indent = 1\n        spaces = []\n        while True:\n            length = 0\n            if self.peek() == '#':\n                break\n            while True:\n                ch = self.peek(length)\n                if ch in '\\0 \\t\\r\\n\\x85\\u2028\\u2029'    \\\n                        or (ch == ':' and\n                                self.peek(length+1) in '\\0 \\t\\r\\n\\x85\\u2028\\u2029'\n                                      + (u',[]{}' if self.flow_level else u''))\\\n                        or (self.flow_level and ch in ',?[]{}'):\n                    break\n                length += 1\n            if length == 0:\n                break\n            self.allow_simple_key = False\n            chunks.extend(spaces)\n            chunks.append(self.prefix(length))\n            self.forward(length)\n            end_mark = self.get_mark()\n            spaces = self.scan_plain_spaces(indent, start_mark)\n            if not spaces or self.peek() == '#' \\\n                    or (not self.flow_level and self.column < indent):\n                break\n        return ScalarToken(''.join(chunks), True, start_mark, end_mark)\n\n    def scan_plain_spaces(self, indent, start_mark):\n        # See the specification for details.\n        # The specification is really confusing about tabs in plain scalars.\n        # We just forbid them completely. Do not use tabs in YAML!\n        chunks = []\n        length = 0\n        while self.peek(length) in ' ':\n            length += 1\n        whitespaces = self.prefix(length)\n        self.forward(length)\n        ch = self.peek()\n        if ch in '\\r\\n\\x85\\u2028\\u2029':\n            line_break = self.scan_line_break()\n            self.allow_simple_key = True\n            prefix = self.prefix(3)\n            if (prefix == '---' or prefix == '...')   \\\n                    and self.peek(3) in '\\0 \\t\\r\\n\\x85\\u2028\\u2029':\n                return\n            breaks = []\n            while self.peek() in ' \\r\\n\\x85\\u2028\\u2029':\n                if self.peek() == ' ':\n                    self.forward()\n                else:\n                    breaks.append(self.scan_line_break())\n                    prefix = self.prefix(3)\n                    if (prefix == '---' or prefix == '...')   \\\n                            and self.peek(3) in '\\0 \\t\\r\\n\\x85\\u2028\\u2029':\n                        return\n            if line_break != '\\n':\n                chunks.append(line_break)\n            elif not breaks:\n                chunks.append(' ')\n            chunks.extend(breaks)\n        elif whitespaces:\n            chunks.append(whitespaces)\n        return chunks\n\n    def scan_tag_handle(self, name, start_mark):\n        # See the specification for details.\n        # For some strange reasons, the specification does not allow '_' in\n        # tag handles. I have allowed it anyway.\n        ch = self.peek()\n        if ch != '!':\n            raise ScannerError(\"while scanning a %s\" % name, start_mark,\n                    \"expected '!', but found %r\" % ch, self.get_mark())\n        length = 1\n        ch = self.peek(length)\n        if ch != ' ':\n            while '0' <= ch <= '9' or 'A' <= ch <= 'Z' or 'a' <= ch <= 'z'  \\\n                    or ch in '-_':\n                length += 1\n                ch = self.peek(length)\n            if ch != '!':\n                self.forward(length)\n                raise ScannerError(\"while scanning a %s\" % name, start_mark,\n                        \"expected '!', but found %r\" % ch, self.get_mark())\n            length += 1\n        value = self.prefix(length)\n        self.forward(length)\n        return value\n\n    def scan_tag_uri(self, name, start_mark):\n        # See the specification for details.\n        # Note: we do not check if URI is well-formed.\n        chunks = []\n        length = 0\n        ch = self.peek(length)\n        while '0' <= ch <= '9' or 'A' <= ch <= 'Z' or 'a' <= ch <= 'z'  \\\n                or ch in '-;/?:@&=+$,_.!~*\\'()[]%':\n            if ch == '%':\n                chunks.append(self.prefix(length))\n                self.forward(length)\n                length = 0\n                chunks.append(self.scan_uri_escapes(name, start_mark))\n            else:\n                length += 1\n            ch = self.peek(length)\n        if length:\n            chunks.append(self.prefix(length))\n            self.forward(length)\n            length = 0\n        if not chunks:\n            raise ScannerError(\"while parsing a %s\" % name, start_mark,\n                    \"expected URI, but found %r\" % ch, self.get_mark())\n        return ''.join(chunks)\n\n    def scan_uri_escapes(self, name, start_mark):\n        # See the specification for details.\n        codes = []\n        mark = self.get_mark()\n        while self.peek() == '%':\n            self.forward()\n            for k in range(2):\n                if self.peek(k) not in '0123456789ABCDEFabcdef':\n                    raise ScannerError(\"while scanning a %s\" % name, start_mark,\n                            \"expected URI escape sequence of 2 hexadecimal numbers, but found %r\"\n                            % self.peek(k), self.get_mark())\n            codes.append(int(self.prefix(2), 16))\n            self.forward(2)\n        try:\n            value = bytes(codes).decode('utf-8')\n        except UnicodeDecodeError as exc:\n            raise ScannerError(\"while scanning a %s\" % name, start_mark, str(exc), mark)\n        return value\n\n    def scan_line_break(self):\n        # Transforms:\n        #   '\\r\\n'      :   '\\n'\n        #   '\\r'        :   '\\n'\n        #   '\\n'        :   '\\n'\n        #   '\\x85'      :   '\\n'\n        #   '\\u2028'    :   '\\u2028'\n        #   '\\u2029     :   '\\u2029'\n        #   default     :   ''\n        ch = self.peek()\n        if ch in '\\r\\n\\x85':\n            if self.prefix(2) == '\\r\\n':\n                self.forward(2)\n            else:\n                self.forward()\n            return '\\n'\n        elif ch in '\\u2028\\u2029':\n            self.forward()\n            return ch\n        return ''\n", "lib/yaml/representer.py": "\n__all__ = ['BaseRepresenter', 'SafeRepresenter', 'Representer',\n    'RepresenterError']\n\nfrom .error import *\nfrom .nodes import *\n\nimport datetime, copyreg, types, base64, collections\n\nclass RepresenterError(YAMLError):\n    pass\n\nclass BaseRepresenter:\n\n    yaml_representers = {}\n    yaml_multi_representers = {}\n\n    def __init__(self, default_style=None, default_flow_style=False, sort_keys=True):\n        self.default_style = default_style\n        self.sort_keys = sort_keys\n        self.default_flow_style = default_flow_style\n        self.represented_objects = {}\n        self.object_keeper = []\n        self.alias_key = None\n\n    def represent(self, data):\n        node = self.represent_data(data)\n        self.serialize(node)\n        self.represented_objects = {}\n        self.object_keeper = []\n        self.alias_key = None\n\n    def represent_data(self, data):\n        if self.ignore_aliases(data):\n            self.alias_key = None\n        else:\n            self.alias_key = id(data)\n        if self.alias_key is not None:\n            if self.alias_key in self.represented_objects:\n                node = self.represented_objects[self.alias_key]\n                #if node is None:\n                #    raise RepresenterError(\"recursive objects are not allowed: %r\" % data)\n                return node\n            #self.represented_objects[alias_key] = None\n            self.object_keeper.append(data)\n        data_types = type(data).__mro__\n        if data_types[0] in self.yaml_representers:\n            node = self.yaml_representers[data_types[0]](self, data)\n        else:\n            for data_type in data_types:\n                if data_type in self.yaml_multi_representers:\n                    node = self.yaml_multi_representers[data_type](self, data)\n                    break\n            else:\n                if None in self.yaml_multi_representers:\n                    node = self.yaml_multi_representers[None](self, data)\n                elif None in self.yaml_representers:\n                    node = self.yaml_representers[None](self, data)\n                else:\n                    node = ScalarNode(None, str(data))\n        #if alias_key is not None:\n        #    self.represented_objects[alias_key] = node\n        return node\n\n    @classmethod\n    def add_representer(cls, data_type, representer):\n        if not 'yaml_representers' in cls.__dict__:\n            cls.yaml_representers = cls.yaml_representers.copy()\n        cls.yaml_representers[data_type] = representer\n\n    @classmethod\n    def add_multi_representer(cls, data_type, representer):\n        if not 'yaml_multi_representers' in cls.__dict__:\n            cls.yaml_multi_representers = cls.yaml_multi_representers.copy()\n        cls.yaml_multi_representers[data_type] = representer\n\n    def represent_scalar(self, tag, value, style=None):\n        if style is None:\n            style = self.default_style\n        node = ScalarNode(tag, value, style=style)\n        if self.alias_key is not None:\n            self.represented_objects[self.alias_key] = node\n        return node\n\n    def represent_sequence(self, tag, sequence, flow_style=None):\n        value = []\n        node = SequenceNode(tag, value, flow_style=flow_style)\n        if self.alias_key is not None:\n            self.represented_objects[self.alias_key] = node\n        best_style = True\n        for item in sequence:\n            node_item = self.represent_data(item)\n            if not (isinstance(node_item, ScalarNode) and not node_item.style):\n                best_style = False\n            value.append(node_item)\n        if flow_style is None:\n            if self.default_flow_style is not None:\n                node.flow_style = self.default_flow_style\n            else:\n                node.flow_style = best_style\n        return node\n\n    def represent_mapping(self, tag, mapping, flow_style=None):\n        value = []\n        node = MappingNode(tag, value, flow_style=flow_style)\n        if self.alias_key is not None:\n            self.represented_objects[self.alias_key] = node\n        best_style = True\n        if hasattr(mapping, 'items'):\n            mapping = list(mapping.items())\n            if self.sort_keys:\n                try:\n                    mapping = sorted(mapping)\n                except TypeError:\n                    pass\n        for item_key, item_value in mapping:\n            node_key = self.represent_data(item_key)\n            node_value = self.represent_data(item_value)\n            if not (isinstance(node_key, ScalarNode) and not node_key.style):\n                best_style = False\n            if not (isinstance(node_value, ScalarNode) and not node_value.style):\n                best_style = False\n            value.append((node_key, node_value))\n        if flow_style is None:\n            if self.default_flow_style is not None:\n                node.flow_style = self.default_flow_style\n            else:\n                node.flow_style = best_style\n        return node\n\n    def ignore_aliases(self, data):\n        return False\n\nclass SafeRepresenter(BaseRepresenter):\n\n    def ignore_aliases(self, data):\n        if data is None:\n            return True\n        if isinstance(data, tuple) and data == ():\n            return True\n        if isinstance(data, (str, bytes, bool, int, float)):\n            return True\n\n    def represent_none(self, data):\n        return self.represent_scalar('tag:yaml.org,2002:null', 'null')\n\n    def represent_str(self, data):\n        return self.represent_scalar('tag:yaml.org,2002:str', data)\n\n    def represent_binary(self, data):\n        if hasattr(base64, 'encodebytes'):\n            data = base64.encodebytes(data).decode('ascii')\n        else:\n            data = base64.encodestring(data).decode('ascii')\n        return self.represent_scalar('tag:yaml.org,2002:binary', data, style='|')\n\n    def represent_bool(self, data):\n        if data:\n            value = 'true'\n        else:\n            value = 'false'\n        return self.represent_scalar('tag:yaml.org,2002:bool', value)\n\n    def represent_int(self, data):\n        return self.represent_scalar('tag:yaml.org,2002:int', str(data))\n\n    inf_value = 1e300\n    while repr(inf_value) != repr(inf_value*inf_value):\n        inf_value *= inf_value\n\n    def represent_float(self, data):\n        if data != data or (data == 0.0 and data == 1.0):\n            value = '.nan'\n        elif data == self.inf_value:\n            value = '.inf'\n        elif data == -self.inf_value:\n            value = '-.inf'\n        else:\n            value = repr(data).lower()\n            # Note that in some cases `repr(data)` represents a float number\n            # without the decimal parts.  For instance:\n            #   >>> repr(1e17)\n            #   '1e17'\n            # Unfortunately, this is not a valid float representation according\n            # to the definition of the `!!float` tag.  We fix this by adding\n            # '.0' before the 'e' symbol.\n            if '.' not in value and 'e' in value:\n                value = value.replace('e', '.0e', 1)\n        return self.represent_scalar('tag:yaml.org,2002:float', value)\n\n    def represent_list(self, data):\n        #pairs = (len(data) > 0 and isinstance(data, list))\n        #if pairs:\n        #    for item in data:\n        #        if not isinstance(item, tuple) or len(item) != 2:\n        #            pairs = False\n        #            break\n        #if not pairs:\n            return self.represent_sequence('tag:yaml.org,2002:seq', data)\n        #value = []\n        #for item_key, item_value in data:\n        #    value.append(self.represent_mapping(u'tag:yaml.org,2002:map',\n        #        [(item_key, item_value)]))\n        #return SequenceNode(u'tag:yaml.org,2002:pairs', value)\n\n    def represent_dict(self, data):\n        return self.represent_mapping('tag:yaml.org,2002:map', data)\n\n    def represent_set(self, data):\n        value = {}\n        for key in data:\n            value[key] = None\n        return self.represent_mapping('tag:yaml.org,2002:set', value)\n\n    def represent_date(self, data):\n        value = data.isoformat()\n        return self.represent_scalar('tag:yaml.org,2002:timestamp', value)\n\n    def represent_datetime(self, data):\n        value = data.isoformat(' ')\n        return self.represent_scalar('tag:yaml.org,2002:timestamp', value)\n\n    def represent_yaml_object(self, tag, data, cls, flow_style=None):\n        if hasattr(data, '__getstate__'):\n            state = data.__getstate__()\n        else:\n            state = data.__dict__.copy()\n        return self.represent_mapping(tag, state, flow_style=flow_style)\n\n    def represent_undefined(self, data):\n        raise RepresenterError(\"cannot represent an object\", data)\n\nSafeRepresenter.add_representer(type(None),\n        SafeRepresenter.represent_none)\n\nSafeRepresenter.add_representer(str,\n        SafeRepresenter.represent_str)\n\nSafeRepresenter.add_representer(bytes,\n        SafeRepresenter.represent_binary)\n\nSafeRepresenter.add_representer(bool,\n        SafeRepresenter.represent_bool)\n\nSafeRepresenter.add_representer(int,\n        SafeRepresenter.represent_int)\n\nSafeRepresenter.add_representer(float,\n        SafeRepresenter.represent_float)\n\nSafeRepresenter.add_representer(list,\n        SafeRepresenter.represent_list)\n\nSafeRepresenter.add_representer(tuple,\n        SafeRepresenter.represent_list)\n\nSafeRepresenter.add_representer(dict,\n        SafeRepresenter.represent_dict)\n\nSafeRepresenter.add_representer(set,\n        SafeRepresenter.represent_set)\n\nSafeRepresenter.add_representer(datetime.date,\n        SafeRepresenter.represent_date)\n\nSafeRepresenter.add_representer(datetime.datetime,\n        SafeRepresenter.represent_datetime)\n\nSafeRepresenter.add_representer(None,\n        SafeRepresenter.represent_undefined)\n\nclass Representer(SafeRepresenter):\n\n    def represent_complex(self, data):\n        if data.imag == 0.0:\n            data = '%r' % data.real\n        elif data.real == 0.0:\n            data = '%rj' % data.imag\n        elif data.imag > 0:\n            data = '%r+%rj' % (data.real, data.imag)\n        else:\n            data = '%r%rj' % (data.real, data.imag)\n        return self.represent_scalar('tag:yaml.org,2002:python/complex', data)\n\n    def represent_tuple(self, data):\n        return self.represent_sequence('tag:yaml.org,2002:python/tuple', data)\n\n    def represent_name(self, data):\n        name = '%s.%s' % (data.__module__, data.__name__)\n        return self.represent_scalar('tag:yaml.org,2002:python/name:'+name, '')\n\n    def represent_module(self, data):\n        return self.represent_scalar(\n                'tag:yaml.org,2002:python/module:'+data.__name__, '')\n\n    def represent_object(self, data):\n        # We use __reduce__ API to save the data. data.__reduce__ returns\n        # a tuple of length 2-5:\n        #   (function, args, state, listitems, dictitems)\n\n        # For reconstructing, we calls function(*args), then set its state,\n        # listitems, and dictitems if they are not None.\n\n        # A special case is when function.__name__ == '__newobj__'. In this\n        # case we create the object with args[0].__new__(*args).\n\n        # Another special case is when __reduce__ returns a string - we don't\n        # support it.\n\n        # We produce a !!python/object, !!python/object/new or\n        # !!python/object/apply node.\n\n        cls = type(data)\n        if cls in copyreg.dispatch_table:\n            reduce = copyreg.dispatch_table[cls](data)\n        elif hasattr(data, '__reduce_ex__'):\n            reduce = data.__reduce_ex__(2)\n        elif hasattr(data, '__reduce__'):\n            reduce = data.__reduce__()\n        else:\n            raise RepresenterError(\"cannot represent an object\", data)\n        reduce = (list(reduce)+[None]*5)[:5]\n        function, args, state, listitems, dictitems = reduce\n        args = list(args)\n        if state is None:\n            state = {}\n        if listitems is not None:\n            listitems = list(listitems)\n        if dictitems is not None:\n            dictitems = dict(dictitems)\n        if function.__name__ == '__newobj__':\n            function = args[0]\n            args = args[1:]\n            tag = 'tag:yaml.org,2002:python/object/new:'\n            newobj = True\n        else:\n            tag = 'tag:yaml.org,2002:python/object/apply:'\n            newobj = False\n        function_name = '%s.%s' % (function.__module__, function.__name__)\n        if not args and not listitems and not dictitems \\\n                and isinstance(state, dict) and newobj:\n            return self.represent_mapping(\n                    'tag:yaml.org,2002:python/object:'+function_name, state)\n        if not listitems and not dictitems  \\\n                and isinstance(state, dict) and not state:\n            return self.represent_sequence(tag+function_name, args)\n        value = {}\n        if args:\n            value['args'] = args\n        if state or not isinstance(state, dict):\n            value['state'] = state\n        if listitems:\n            value['listitems'] = listitems\n        if dictitems:\n            value['dictitems'] = dictitems\n        return self.represent_mapping(tag+function_name, value)\n\n    def represent_ordered_dict(self, data):\n        # Provide uniform representation across different Python versions.\n        data_type = type(data)\n        tag = 'tag:yaml.org,2002:python/object/apply:%s.%s' \\\n                % (data_type.__module__, data_type.__name__)\n        items = [[key, value] for key, value in data.items()]\n        return self.represent_sequence(tag, [items])\n\nRepresenter.add_representer(complex,\n        Representer.represent_complex)\n\nRepresenter.add_representer(tuple,\n        Representer.represent_tuple)\n\nRepresenter.add_multi_representer(type,\n        Representer.represent_name)\n\nRepresenter.add_representer(collections.OrderedDict,\n        Representer.represent_ordered_dict)\n\nRepresenter.add_representer(types.FunctionType,\n        Representer.represent_name)\n\nRepresenter.add_representer(types.BuiltinFunctionType,\n        Representer.represent_name)\n\nRepresenter.add_representer(types.ModuleType,\n        Representer.represent_module)\n\nRepresenter.add_multi_representer(object,\n        Representer.represent_object)\n\n", "lib/yaml/resolver.py": "\n__all__ = ['BaseResolver', 'Resolver']\n\nfrom .error import *\nfrom .nodes import *\n\nimport re\n\nclass ResolverError(YAMLError):\n    pass\n\nclass BaseResolver:\n\n    DEFAULT_SCALAR_TAG = 'tag:yaml.org,2002:str'\n    DEFAULT_SEQUENCE_TAG = 'tag:yaml.org,2002:seq'\n    DEFAULT_MAPPING_TAG = 'tag:yaml.org,2002:map'\n\n    yaml_implicit_resolvers = {}\n    yaml_path_resolvers = {}\n\n    def __init__(self):\n        self.resolver_exact_paths = []\n        self.resolver_prefix_paths = []\n\n    @classmethod\n    def add_implicit_resolver(cls, tag, regexp, first):\n        if not 'yaml_implicit_resolvers' in cls.__dict__:\n            implicit_resolvers = {}\n            for key in cls.yaml_implicit_resolvers:\n                implicit_resolvers[key] = cls.yaml_implicit_resolvers[key][:]\n            cls.yaml_implicit_resolvers = implicit_resolvers\n        if first is None:\n            first = [None]\n        for ch in first:\n            cls.yaml_implicit_resolvers.setdefault(ch, []).append((tag, regexp))\n\n    @classmethod\n    def add_path_resolver(cls, tag, path, kind=None):\n        # Note: `add_path_resolver` is experimental.  The API could be changed.\n        # `new_path` is a pattern that is matched against the path from the\n        # root to the node that is being considered.  `node_path` elements are\n        # tuples `(node_check, index_check)`.  `node_check` is a node class:\n        # `ScalarNode`, `SequenceNode`, `MappingNode` or `None`.  `None`\n        # matches any kind of a node.  `index_check` could be `None`, a boolean\n        # value, a string value, or a number.  `None` and `False` match against\n        # any _value_ of sequence and mapping nodes.  `True` matches against\n        # any _key_ of a mapping node.  A string `index_check` matches against\n        # a mapping value that corresponds to a scalar key which content is\n        # equal to the `index_check` value.  An integer `index_check` matches\n        # against a sequence value with the index equal to `index_check`.\n        if not 'yaml_path_resolvers' in cls.__dict__:\n            cls.yaml_path_resolvers = cls.yaml_path_resolvers.copy()\n        new_path = []\n        for element in path:\n            if isinstance(element, (list, tuple)):\n                if len(element) == 2:\n                    node_check, index_check = element\n                elif len(element) == 1:\n                    node_check = element[0]\n                    index_check = True\n                else:\n                    raise ResolverError(\"Invalid path element: %s\" % element)\n            else:\n                node_check = None\n                index_check = element\n            if node_check is str:\n                node_check = ScalarNode\n            elif node_check is list:\n                node_check = SequenceNode\n            elif node_check is dict:\n                node_check = MappingNode\n            elif node_check not in [ScalarNode, SequenceNode, MappingNode]  \\\n                    and not isinstance(node_check, str) \\\n                    and node_check is not None:\n                raise ResolverError(\"Invalid node checker: %s\" % node_check)\n            if not isinstance(index_check, (str, int))  \\\n                    and index_check is not None:\n                raise ResolverError(\"Invalid index checker: %s\" % index_check)\n            new_path.append((node_check, index_check))\n        if kind is str:\n            kind = ScalarNode\n        elif kind is list:\n            kind = SequenceNode\n        elif kind is dict:\n            kind = MappingNode\n        elif kind not in [ScalarNode, SequenceNode, MappingNode]    \\\n                and kind is not None:\n            raise ResolverError(\"Invalid node kind: %s\" % kind)\n        cls.yaml_path_resolvers[tuple(new_path), kind] = tag\n\n    def descend_resolver(self, current_node, current_index):\n        if not self.yaml_path_resolvers:\n            return\n        exact_paths = {}\n        prefix_paths = []\n        if current_node:\n            depth = len(self.resolver_prefix_paths)\n            for path, kind in self.resolver_prefix_paths[-1]:\n                if self.check_resolver_prefix(depth, path, kind,\n                        current_node, current_index):\n                    if len(path) > depth:\n                        prefix_paths.append((path, kind))\n                    else:\n                        exact_paths[kind] = self.yaml_path_resolvers[path, kind]\n        else:\n            for path, kind in self.yaml_path_resolvers:\n                if not path:\n                    exact_paths[kind] = self.yaml_path_resolvers[path, kind]\n                else:\n                    prefix_paths.append((path, kind))\n        self.resolver_exact_paths.append(exact_paths)\n        self.resolver_prefix_paths.append(prefix_paths)\n\n    def ascend_resolver(self):\n        if not self.yaml_path_resolvers:\n            return\n        self.resolver_exact_paths.pop()\n        self.resolver_prefix_paths.pop()\n\n    def check_resolver_prefix(self, depth, path, kind,\n            current_node, current_index):\n        node_check, index_check = path[depth-1]\n        if isinstance(node_check, str):\n            if current_node.tag != node_check:\n                return\n        elif node_check is not None:\n            if not isinstance(current_node, node_check):\n                return\n        if index_check is True and current_index is not None:\n            return\n        if (index_check is False or index_check is None)    \\\n                and current_index is None:\n            return\n        if isinstance(index_check, str):\n            if not (isinstance(current_index, ScalarNode)\n                    and index_check == current_index.value):\n                return\n        elif isinstance(index_check, int) and not isinstance(index_check, bool):\n            if index_check != current_index:\n                return\n        return True\n\n    def resolve(self, kind, value, implicit):\n        if kind is ScalarNode and implicit[0]:\n            if value == '':\n                resolvers = self.yaml_implicit_resolvers.get('', [])\n            else:\n                resolvers = self.yaml_implicit_resolvers.get(value[0], [])\n            wildcard_resolvers = self.yaml_implicit_resolvers.get(None, [])\n            for tag, regexp in resolvers + wildcard_resolvers:\n                if regexp.match(value):\n                    return tag\n            implicit = implicit[1]\n        if self.yaml_path_resolvers:\n            exact_paths = self.resolver_exact_paths[-1]\n            if kind in exact_paths:\n                return exact_paths[kind]\n            if None in exact_paths:\n                return exact_paths[None]\n        if kind is ScalarNode:\n            return self.DEFAULT_SCALAR_TAG\n        elif kind is SequenceNode:\n            return self.DEFAULT_SEQUENCE_TAG\n        elif kind is MappingNode:\n            return self.DEFAULT_MAPPING_TAG\n\nclass Resolver(BaseResolver):\n    pass\n\nResolver.add_implicit_resolver(\n        'tag:yaml.org,2002:bool',\n        re.compile(r'''^(?:yes|Yes|YES|no|No|NO\n                    |true|True|TRUE|false|False|FALSE\n                    |on|On|ON|off|Off|OFF)$''', re.X),\n        list('yYnNtTfFoO'))\n\nResolver.add_implicit_resolver(\n        'tag:yaml.org,2002:float',\n        re.compile(r'''^(?:[-+]?(?:[0-9][0-9_]*)\\.[0-9_]*(?:[eE][-+][0-9]+)?\n                    |\\.[0-9][0-9_]*(?:[eE][-+][0-9]+)?\n                    |[-+]?[0-9][0-9_]*(?::[0-5]?[0-9])+\\.[0-9_]*\n                    |[-+]?\\.(?:inf|Inf|INF)\n                    |\\.(?:nan|NaN|NAN))$''', re.X),\n        list('-+0123456789.'))\n\nResolver.add_implicit_resolver(\n        'tag:yaml.org,2002:int',\n        re.compile(r'''^(?:[-+]?0b[0-1_]+\n                    |[-+]?0[0-7_]+\n                    |[-+]?(?:0|[1-9][0-9_]*)\n                    |[-+]?0x[0-9a-fA-F_]+\n                    |[-+]?[1-9][0-9_]*(?::[0-5]?[0-9])+)$''', re.X),\n        list('-+0123456789'))\n\nResolver.add_implicit_resolver(\n        'tag:yaml.org,2002:merge',\n        re.compile(r'^(?:<<)$'),\n        ['<'])\n\nResolver.add_implicit_resolver(\n        'tag:yaml.org,2002:null',\n        re.compile(r'''^(?: ~\n                    |null|Null|NULL\n                    | )$''', re.X),\n        ['~', 'n', 'N', ''])\n\nResolver.add_implicit_resolver(\n        'tag:yaml.org,2002:timestamp',\n        re.compile(r'''^(?:[0-9][0-9][0-9][0-9]-[0-9][0-9]-[0-9][0-9]\n                    |[0-9][0-9][0-9][0-9] -[0-9][0-9]? -[0-9][0-9]?\n                     (?:[Tt]|[ \\t]+)[0-9][0-9]?\n                     :[0-9][0-9] :[0-9][0-9] (?:\\.[0-9]*)?\n                     (?:[ \\t]*(?:Z|[-+][0-9][0-9]?(?::[0-9][0-9])?))?)$''', re.X),\n        list('0123456789'))\n\nResolver.add_implicit_resolver(\n        'tag:yaml.org,2002:value',\n        re.compile(r'^(?:=)$'),\n        ['='])\n\n# The following resolver is only for documentation purposes. It cannot work\n# because plain scalars cannot start with '!', '&', or '*'.\nResolver.add_implicit_resolver(\n        'tag:yaml.org,2002:yaml',\n        re.compile(r'^(?:!|&|\\*)$'),\n        list('!&*'))\n\n", "lib/yaml/nodes.py": "\nclass Node(object):\n    def __init__(self, tag, value, start_mark, end_mark):\n        self.tag = tag\n        self.value = value\n        self.start_mark = start_mark\n        self.end_mark = end_mark\n    def __repr__(self):\n        value = self.value\n        #if isinstance(value, list):\n        #    if len(value) == 0:\n        #        value = '<empty>'\n        #    elif len(value) == 1:\n        #        value = '<1 item>'\n        #    else:\n        #        value = '<%d items>' % len(value)\n        #else:\n        #    if len(value) > 75:\n        #        value = repr(value[:70]+u' ... ')\n        #    else:\n        #        value = repr(value)\n        value = repr(value)\n        return '%s(tag=%r, value=%s)' % (self.__class__.__name__, self.tag, value)\n\nclass ScalarNode(Node):\n    id = 'scalar'\n    def __init__(self, tag, value,\n            start_mark=None, end_mark=None, style=None):\n        self.tag = tag\n        self.value = value\n        self.start_mark = start_mark\n        self.end_mark = end_mark\n        self.style = style\n\nclass CollectionNode(Node):\n    def __init__(self, tag, value,\n            start_mark=None, end_mark=None, flow_style=None):\n        self.tag = tag\n        self.value = value\n        self.start_mark = start_mark\n        self.end_mark = end_mark\n        self.flow_style = flow_style\n\nclass SequenceNode(CollectionNode):\n    id = 'sequence'\n\nclass MappingNode(CollectionNode):\n    id = 'mapping'\n\n", "lib/yaml/serializer.py": "\n__all__ = ['Serializer', 'SerializerError']\n\nfrom .error import YAMLError\nfrom .events import *\nfrom .nodes import *\n\nclass SerializerError(YAMLError):\n    pass\n\nclass Serializer:\n\n    ANCHOR_TEMPLATE = 'id%03d'\n\n    def __init__(self, encoding=None,\n            explicit_start=None, explicit_end=None, version=None, tags=None):\n        self.use_encoding = encoding\n        self.use_explicit_start = explicit_start\n        self.use_explicit_end = explicit_end\n        self.use_version = version\n        self.use_tags = tags\n        self.serialized_nodes = {}\n        self.anchors = {}\n        self.last_anchor_id = 0\n        self.closed = None\n\n    def open(self):\n        if self.closed is None:\n            self.emit(StreamStartEvent(encoding=self.use_encoding))\n            self.closed = False\n        elif self.closed:\n            raise SerializerError(\"serializer is closed\")\n        else:\n            raise SerializerError(\"serializer is already opened\")\n\n    def close(self):\n        if self.closed is None:\n            raise SerializerError(\"serializer is not opened\")\n        elif not self.closed:\n            self.emit(StreamEndEvent())\n            self.closed = True\n\n    #def __del__(self):\n    #    self.close()\n\n    def serialize(self, node):\n        if self.closed is None:\n            raise SerializerError(\"serializer is not opened\")\n        elif self.closed:\n            raise SerializerError(\"serializer is closed\")\n        self.emit(DocumentStartEvent(explicit=self.use_explicit_start,\n            version=self.use_version, tags=self.use_tags))\n        self.anchor_node(node)\n        self.serialize_node(node, None, None)\n        self.emit(DocumentEndEvent(explicit=self.use_explicit_end))\n        self.serialized_nodes = {}\n        self.anchors = {}\n        self.last_anchor_id = 0\n\n    def anchor_node(self, node):\n        if node in self.anchors:\n            if self.anchors[node] is None:\n                self.anchors[node] = self.generate_anchor(node)\n        else:\n            self.anchors[node] = None\n            if isinstance(node, SequenceNode):\n                for item in node.value:\n                    self.anchor_node(item)\n            elif isinstance(node, MappingNode):\n                for key, value in node.value:\n                    self.anchor_node(key)\n                    self.anchor_node(value)\n\n    def generate_anchor(self, node):\n        self.last_anchor_id += 1\n        return self.ANCHOR_TEMPLATE % self.last_anchor_id\n\n    def serialize_node(self, node, parent, index):\n        alias = self.anchors[node]\n        if node in self.serialized_nodes:\n            self.emit(AliasEvent(alias))\n        else:\n            self.serialized_nodes[node] = True\n            self.descend_resolver(parent, index)\n            if isinstance(node, ScalarNode):\n                detected_tag = self.resolve(ScalarNode, node.value, (True, False))\n                default_tag = self.resolve(ScalarNode, node.value, (False, True))\n                implicit = (node.tag == detected_tag), (node.tag == default_tag)\n                self.emit(ScalarEvent(alias, node.tag, implicit, node.value,\n                    style=node.style))\n            elif isinstance(node, SequenceNode):\n                implicit = (node.tag\n                            == self.resolve(SequenceNode, node.value, True))\n                self.emit(SequenceStartEvent(alias, node.tag, implicit,\n                    flow_style=node.flow_style))\n                index = 0\n                for item in node.value:\n                    self.serialize_node(item, node, index)\n                    index += 1\n                self.emit(SequenceEndEvent())\n            elif isinstance(node, MappingNode):\n                implicit = (node.tag\n                            == self.resolve(MappingNode, node.value, True))\n                self.emit(MappingStartEvent(alias, node.tag, implicit,\n                    flow_style=node.flow_style))\n                for key, value in node.value:\n                    self.serialize_node(key, node, None)\n                    self.serialize_node(value, node, key)\n                self.emit(MappingEndEvent())\n            self.ascend_resolver()\n\n", "lib/yaml/loader.py": "\n__all__ = ['BaseLoader', 'FullLoader', 'SafeLoader', 'Loader', 'UnsafeLoader']\n\nfrom .reader import *\nfrom .scanner import *\nfrom .parser import *\nfrom .composer import *\nfrom .constructor import *\nfrom .resolver import *\n\nclass BaseLoader(Reader, Scanner, Parser, Composer, BaseConstructor, BaseResolver):\n\n    def __init__(self, stream):\n        Reader.__init__(self, stream)\n        Scanner.__init__(self)\n        Parser.__init__(self)\n        Composer.__init__(self)\n        BaseConstructor.__init__(self)\n        BaseResolver.__init__(self)\n\nclass FullLoader(Reader, Scanner, Parser, Composer, FullConstructor, Resolver):\n\n    def __init__(self, stream):\n        Reader.__init__(self, stream)\n        Scanner.__init__(self)\n        Parser.__init__(self)\n        Composer.__init__(self)\n        FullConstructor.__init__(self)\n        Resolver.__init__(self)\n\nclass SafeLoader(Reader, Scanner, Parser, Composer, SafeConstructor, Resolver):\n\n    def __init__(self, stream):\n        Reader.__init__(self, stream)\n        Scanner.__init__(self)\n        Parser.__init__(self)\n        Composer.__init__(self)\n        SafeConstructor.__init__(self)\n        Resolver.__init__(self)\n\nclass Loader(Reader, Scanner, Parser, Composer, Constructor, Resolver):\n\n    def __init__(self, stream):\n        Reader.__init__(self, stream)\n        Scanner.__init__(self)\n        Parser.__init__(self)\n        Composer.__init__(self)\n        Constructor.__init__(self)\n        Resolver.__init__(self)\n\n# UnsafeLoader is the same as Loader (which is and was always unsafe on\n# untrusted input). Use of either Loader or UnsafeLoader should be rare, since\n# FullLoad should be able to load almost all YAML safely. Loader is left intact\n# to ensure backwards compatibility.\nclass UnsafeLoader(Reader, Scanner, Parser, Composer, Constructor, Resolver):\n\n    def __init__(self, stream):\n        Reader.__init__(self, stream)\n        Scanner.__init__(self)\n        Parser.__init__(self)\n        Composer.__init__(self)\n        Constructor.__init__(self)\n        Resolver.__init__(self)\n", "lib/yaml/reader.py": "# This module contains abstractions for the input stream. You don't have to\n# looks further, there are no pretty code.\n#\n# We define two classes here.\n#\n#   Mark(source, line, column)\n# It's just a record and its only use is producing nice error messages.\n# Parser does not use it for any other purposes.\n#\n#   Reader(source, data)\n# Reader determines the encoding of `data` and converts it to unicode.\n# Reader provides the following methods and attributes:\n#   reader.peek(length=1) - return the next `length` characters\n#   reader.forward(length=1) - move the current position to `length` characters.\n#   reader.index - the number of the current character.\n#   reader.line, stream.column - the line and the column of the current character.\n\n__all__ = ['Reader', 'ReaderError']\n\nfrom .error import YAMLError, Mark\n\nimport codecs, re\n\nclass ReaderError(YAMLError):\n\n    def __init__(self, name, position, character, encoding, reason):\n        self.name = name\n        self.character = character\n        self.position = position\n        self.encoding = encoding\n        self.reason = reason\n\n    def __str__(self):\n        if isinstance(self.character, bytes):\n            return \"'%s' codec can't decode byte #x%02x: %s\\n\"  \\\n                    \"  in \\\"%s\\\", position %d\"    \\\n                    % (self.encoding, ord(self.character), self.reason,\n                            self.name, self.position)\n        else:\n            return \"unacceptable character #x%04x: %s\\n\"    \\\n                    \"  in \\\"%s\\\", position %d\"    \\\n                    % (self.character, self.reason,\n                            self.name, self.position)\n\nclass Reader(object):\n    # Reader:\n    # - determines the data encoding and converts it to a unicode string,\n    # - checks if characters are in allowed range,\n    # - adds '\\0' to the end.\n\n    # Reader accepts\n    #  - a `bytes` object,\n    #  - a `str` object,\n    #  - a file-like object with its `read` method returning `str`,\n    #  - a file-like object with its `read` method returning `unicode`.\n\n    # Yeah, it's ugly and slow.\n\n    def __init__(self, stream):\n        self.name = None\n        self.stream = None\n        self.stream_pointer = 0\n        self.eof = True\n        self.buffer = ''\n        self.pointer = 0\n        self.raw_buffer = None\n        self.raw_decode = None\n        self.encoding = None\n        self.index = 0\n        self.line = 0\n        self.column = 0\n        if isinstance(stream, str):\n            self.name = \"<unicode string>\"\n            self.check_printable(stream)\n            self.buffer = stream+'\\0'\n        elif isinstance(stream, bytes):\n            self.name = \"<byte string>\"\n            self.raw_buffer = stream\n            self.determine_encoding()\n        else:\n            self.stream = stream\n            self.name = getattr(stream, 'name', \"<file>\")\n            self.eof = False\n            self.raw_buffer = None\n            self.determine_encoding()\n\n    def peek(self, index=0):\n        try:\n            return self.buffer[self.pointer+index]\n        except IndexError:\n            self.update(index+1)\n            return self.buffer[self.pointer+index]\n\n    def prefix(self, length=1):\n        if self.pointer+length >= len(self.buffer):\n            self.update(length)\n        return self.buffer[self.pointer:self.pointer+length]\n\n    def forward(self, length=1):\n        if self.pointer+length+1 >= len(self.buffer):\n            self.update(length+1)\n        while length:\n            ch = self.buffer[self.pointer]\n            self.pointer += 1\n            self.index += 1\n            if ch in '\\n\\x85\\u2028\\u2029'  \\\n                    or (ch == '\\r' and self.buffer[self.pointer] != '\\n'):\n                self.line += 1\n                self.column = 0\n            elif ch != '\\uFEFF':\n                self.column += 1\n            length -= 1\n\n    def get_mark(self):\n        if self.stream is None:\n            return Mark(self.name, self.index, self.line, self.column,\n                    self.buffer, self.pointer)\n        else:\n            return Mark(self.name, self.index, self.line, self.column,\n                    None, None)\n\n    def determine_encoding(self):\n        while not self.eof and (self.raw_buffer is None or len(self.raw_buffer) < 2):\n            self.update_raw()\n        if isinstance(self.raw_buffer, bytes):\n            if self.raw_buffer.startswith(codecs.BOM_UTF16_LE):\n                self.raw_decode = codecs.utf_16_le_decode\n                self.encoding = 'utf-16-le'\n            elif self.raw_buffer.startswith(codecs.BOM_UTF16_BE):\n                self.raw_decode = codecs.utf_16_be_decode\n                self.encoding = 'utf-16-be'\n            else:\n                self.raw_decode = codecs.utf_8_decode\n                self.encoding = 'utf-8'\n        self.update(1)\n\n    NON_PRINTABLE = re.compile('[^\\x09\\x0A\\x0D\\x20-\\x7E\\x85\\xA0-\\uD7FF\\uE000-\\uFFFD\\U00010000-\\U0010ffff]')\n    def check_printable(self, data):\n        match = self.NON_PRINTABLE.search(data)\n        if match:\n            character = match.group()\n            position = self.index+(len(self.buffer)-self.pointer)+match.start()\n            raise ReaderError(self.name, position, ord(character),\n                    'unicode', \"special characters are not allowed\")\n\n    def update(self, length):\n        if self.raw_buffer is None:\n            return\n        self.buffer = self.buffer[self.pointer:]\n        self.pointer = 0\n        while len(self.buffer) < length:\n            if not self.eof:\n                self.update_raw()\n            if self.raw_decode is not None:\n                try:\n                    data, converted = self.raw_decode(self.raw_buffer,\n                            'strict', self.eof)\n                except UnicodeDecodeError as exc:\n                    character = self.raw_buffer[exc.start]\n                    if self.stream is not None:\n                        position = self.stream_pointer-len(self.raw_buffer)+exc.start\n                    else:\n                        position = exc.start\n                    raise ReaderError(self.name, position, character,\n                            exc.encoding, exc.reason)\n            else:\n                data = self.raw_buffer\n                converted = len(data)\n            self.check_printable(data)\n            self.buffer += data\n            self.raw_buffer = self.raw_buffer[converted:]\n            if self.eof:\n                self.buffer += '\\0'\n                self.raw_buffer = None\n                break\n\n    def update_raw(self, size=4096):\n        data = self.stream.read(size)\n        if self.raw_buffer is None:\n            self.raw_buffer = data\n        else:\n            self.raw_buffer += data\n        self.stream_pointer += len(data)\n        if not data:\n            self.eof = True\n", "lib/yaml/error.py": "\n__all__ = ['Mark', 'YAMLError', 'MarkedYAMLError']\n\nclass Mark:\n\n    def __init__(self, name, index, line, column, buffer, pointer):\n        self.name = name\n        self.index = index\n        self.line = line\n        self.column = column\n        self.buffer = buffer\n        self.pointer = pointer\n\n    def get_snippet(self, indent=4, max_length=75):\n        if self.buffer is None:\n            return None\n        head = ''\n        start = self.pointer\n        while start > 0 and self.buffer[start-1] not in '\\0\\r\\n\\x85\\u2028\\u2029':\n            start -= 1\n            if self.pointer-start > max_length/2-1:\n                head = ' ... '\n                start += 5\n                break\n        tail = ''\n        end = self.pointer\n        while end < len(self.buffer) and self.buffer[end] not in '\\0\\r\\n\\x85\\u2028\\u2029':\n            end += 1\n            if end-self.pointer > max_length/2-1:\n                tail = ' ... '\n                end -= 5\n                break\n        snippet = self.buffer[start:end]\n        return ' '*indent + head + snippet + tail + '\\n'  \\\n                + ' '*(indent+self.pointer-start+len(head)) + '^'\n\n    def __str__(self):\n        snippet = self.get_snippet()\n        where = \"  in \\\"%s\\\", line %d, column %d\"   \\\n                % (self.name, self.line+1, self.column+1)\n        if snippet is not None:\n            where += \":\\n\"+snippet\n        return where\n\nclass YAMLError(Exception):\n    pass\n\nclass MarkedYAMLError(YAMLError):\n\n    def __init__(self, context=None, context_mark=None,\n            problem=None, problem_mark=None, note=None):\n        self.context = context\n        self.context_mark = context_mark\n        self.problem = problem\n        self.problem_mark = problem_mark\n        self.note = note\n\n    def __str__(self):\n        lines = []\n        if self.context is not None:\n            lines.append(self.context)\n        if self.context_mark is not None  \\\n            and (self.problem is None or self.problem_mark is None\n                    or self.context_mark.name != self.problem_mark.name\n                    or self.context_mark.line != self.problem_mark.line\n                    or self.context_mark.column != self.problem_mark.column):\n            lines.append(str(self.context_mark))\n        if self.problem is not None:\n            lines.append(self.problem)\n        if self.problem_mark is not None:\n            lines.append(str(self.problem_mark))\n        if self.note is not None:\n            lines.append(self.note)\n        return '\\n'.join(lines)\n\n", "lib/yaml/__init__.py": "\nfrom .error import *\n\nfrom .tokens import *\nfrom .events import *\nfrom .nodes import *\n\nfrom .loader import *\nfrom .dumper import *\n\n__version__ = '6.0.1'\ntry:\n    from .cyaml import *\n    __with_libyaml__ = True\nexcept ImportError:\n    __with_libyaml__ = False\n\nimport io\n\n#------------------------------------------------------------------------------\n# XXX \"Warnings control\" is now deprecated. Leaving in the API function to not\n# break code that uses it.\n#------------------------------------------------------------------------------\ndef warnings(settings=None):\n    if settings is None:\n        return {}\n\n#------------------------------------------------------------------------------\ndef scan(stream, Loader=Loader):\n    \"\"\"\n    Scan a YAML stream and produce scanning tokens.\n    \"\"\"\n    loader = Loader(stream)\n    try:\n        while loader.check_token():\n            yield loader.get_token()\n    finally:\n        loader.dispose()\n\ndef parse(stream, Loader=Loader):\n    \"\"\"\n    Parse a YAML stream and produce parsing events.\n    \"\"\"\n    loader = Loader(stream)\n    try:\n        while loader.check_event():\n            yield loader.get_event()\n    finally:\n        loader.dispose()\n\ndef compose(stream, Loader=Loader):\n    \"\"\"\n    Parse the first YAML document in a stream\n    and produce the corresponding representation tree.\n    \"\"\"\n    loader = Loader(stream)\n    try:\n        return loader.get_single_node()\n    finally:\n        loader.dispose()\n\ndef compose_all(stream, Loader=Loader):\n    \"\"\"\n    Parse all YAML documents in a stream\n    and produce corresponding representation trees.\n    \"\"\"\n    loader = Loader(stream)\n    try:\n        while loader.check_node():\n            yield loader.get_node()\n    finally:\n        loader.dispose()\n\ndef load(stream, Loader):\n    \"\"\"\n    Parse the first YAML document in a stream\n    and produce the corresponding Python object.\n    \"\"\"\n    loader = Loader(stream)\n    try:\n        return loader.get_single_data()\n    finally:\n        loader.dispose()\n\ndef load_all(stream, Loader):\n    \"\"\"\n    Parse all YAML documents in a stream\n    and produce corresponding Python objects.\n    \"\"\"\n    loader = Loader(stream)\n    try:\n        while loader.check_data():\n            yield loader.get_data()\n    finally:\n        loader.dispose()\n\ndef full_load(stream):\n    \"\"\"\n    Parse the first YAML document in a stream\n    and produce the corresponding Python object.\n\n    Resolve all tags except those known to be\n    unsafe on untrusted input.\n    \"\"\"\n    return load(stream, FullLoader)\n\ndef full_load_all(stream):\n    \"\"\"\n    Parse all YAML documents in a stream\n    and produce corresponding Python objects.\n\n    Resolve all tags except those known to be\n    unsafe on untrusted input.\n    \"\"\"\n    return load_all(stream, FullLoader)\n\ndef safe_load(stream):\n    \"\"\"\n    Parse the first YAML document in a stream\n    and produce the corresponding Python object.\n\n    Resolve only basic YAML tags. This is known\n    to be safe for untrusted input.\n    \"\"\"\n    return load(stream, SafeLoader)\n\ndef safe_load_all(stream):\n    \"\"\"\n    Parse all YAML documents in a stream\n    and produce corresponding Python objects.\n\n    Resolve only basic YAML tags. This is known\n    to be safe for untrusted input.\n    \"\"\"\n    return load_all(stream, SafeLoader)\n\ndef unsafe_load(stream):\n    \"\"\"\n    Parse the first YAML document in a stream\n    and produce the corresponding Python object.\n\n    Resolve all tags, even those known to be\n    unsafe on untrusted input.\n    \"\"\"\n    return load(stream, UnsafeLoader)\n\ndef unsafe_load_all(stream):\n    \"\"\"\n    Parse all YAML documents in a stream\n    and produce corresponding Python objects.\n\n    Resolve all tags, even those known to be\n    unsafe on untrusted input.\n    \"\"\"\n    return load_all(stream, UnsafeLoader)\n\ndef emit(events, stream=None, Dumper=Dumper,\n        canonical=None, indent=None, width=None,\n        allow_unicode=None, line_break=None):\n    \"\"\"\n    Emit YAML parsing events into a stream.\n    If stream is None, return the produced string instead.\n    \"\"\"\n    getvalue = None\n    if stream is None:\n        stream = io.StringIO()\n        getvalue = stream.getvalue\n    dumper = Dumper(stream, canonical=canonical, indent=indent, width=width,\n            allow_unicode=allow_unicode, line_break=line_break)\n    try:\n        for event in events:\n            dumper.emit(event)\n    finally:\n        dumper.dispose()\n    if getvalue:\n        return getvalue()\n\ndef serialize_all(nodes, stream=None, Dumper=Dumper,\n        canonical=None, indent=None, width=None,\n        allow_unicode=None, line_break=None,\n        encoding=None, explicit_start=None, explicit_end=None,\n        version=None, tags=None):\n    \"\"\"\n    Serialize a sequence of representation trees into a YAML stream.\n    If stream is None, return the produced string instead.\n    \"\"\"\n    getvalue = None\n    if stream is None:\n        if encoding is None:\n            stream = io.StringIO()\n        else:\n            stream = io.BytesIO()\n        getvalue = stream.getvalue\n    dumper = Dumper(stream, canonical=canonical, indent=indent, width=width,\n            allow_unicode=allow_unicode, line_break=line_break,\n            encoding=encoding, version=version, tags=tags,\n            explicit_start=explicit_start, explicit_end=explicit_end)\n    try:\n        dumper.open()\n        for node in nodes:\n            dumper.serialize(node)\n        dumper.close()\n    finally:\n        dumper.dispose()\n    if getvalue:\n        return getvalue()\n\ndef serialize(node, stream=None, Dumper=Dumper, **kwds):\n    \"\"\"\n    Serialize a representation tree into a YAML stream.\n    If stream is None, return the produced string instead.\n    \"\"\"\n    return serialize_all([node], stream, Dumper=Dumper, **kwds)\n\ndef dump_all(documents, stream=None, Dumper=Dumper,\n        default_style=None, default_flow_style=False,\n        canonical=None, indent=None, width=None,\n        allow_unicode=None, line_break=None,\n        encoding=None, explicit_start=None, explicit_end=None,\n        version=None, tags=None, sort_keys=True):\n    \"\"\"\n    Serialize a sequence of Python objects into a YAML stream.\n    If stream is None, return the produced string instead.\n    \"\"\"\n    getvalue = None\n    if stream is None:\n        if encoding is None:\n            stream = io.StringIO()\n        else:\n            stream = io.BytesIO()\n        getvalue = stream.getvalue\n    dumper = Dumper(stream, default_style=default_style,\n            default_flow_style=default_flow_style,\n            canonical=canonical, indent=indent, width=width,\n            allow_unicode=allow_unicode, line_break=line_break,\n            encoding=encoding, version=version, tags=tags,\n            explicit_start=explicit_start, explicit_end=explicit_end, sort_keys=sort_keys)\n    try:\n        dumper.open()\n        for data in documents:\n            dumper.represent(data)\n        dumper.close()\n    finally:\n        dumper.dispose()\n    if getvalue:\n        return getvalue()\n\ndef dump(data, stream=None, Dumper=Dumper, **kwds):\n    \"\"\"\n    Serialize a Python object into a YAML stream.\n    If stream is None, return the produced string instead.\n    \"\"\"\n    return dump_all([data], stream, Dumper=Dumper, **kwds)\n\ndef safe_dump_all(documents, stream=None, **kwds):\n    \"\"\"\n    Serialize a sequence of Python objects into a YAML stream.\n    Produce only basic YAML tags.\n    If stream is None, return the produced string instead.\n    \"\"\"\n    return dump_all(documents, stream, Dumper=SafeDumper, **kwds)\n\ndef safe_dump(data, stream=None, **kwds):\n    \"\"\"\n    Serialize a Python object into a YAML stream.\n    Produce only basic YAML tags.\n    If stream is None, return the produced string instead.\n    \"\"\"\n    return dump_all([data], stream, Dumper=SafeDumper, **kwds)\n\ndef add_implicit_resolver(tag, regexp, first=None,\n        Loader=None, Dumper=Dumper):\n    \"\"\"\n    Add an implicit scalar detector.\n    If an implicit scalar value matches the given regexp,\n    the corresponding tag is assigned to the scalar.\n    first is a sequence of possible initial characters or None.\n    \"\"\"\n    if Loader is None:\n        loader.Loader.add_implicit_resolver(tag, regexp, first)\n        loader.FullLoader.add_implicit_resolver(tag, regexp, first)\n        loader.UnsafeLoader.add_implicit_resolver(tag, regexp, first)\n    else:\n        Loader.add_implicit_resolver(tag, regexp, first)\n    Dumper.add_implicit_resolver(tag, regexp, first)\n\ndef add_path_resolver(tag, path, kind=None, Loader=None, Dumper=Dumper):\n    \"\"\"\n    Add a path based resolver for the given tag.\n    A path is a list of keys that forms a path\n    to a node in the representation tree.\n    Keys can be string values, integers, or None.\n    \"\"\"\n    if Loader is None:\n        loader.Loader.add_path_resolver(tag, path, kind)\n        loader.FullLoader.add_path_resolver(tag, path, kind)\n        loader.UnsafeLoader.add_path_resolver(tag, path, kind)\n    else:\n        Loader.add_path_resolver(tag, path, kind)\n    Dumper.add_path_resolver(tag, path, kind)\n\ndef add_constructor(tag, constructor, Loader=None):\n    \"\"\"\n    Add a constructor for the given tag.\n    Constructor is a function that accepts a Loader instance\n    and a node object and produces the corresponding Python object.\n    \"\"\"\n    if Loader is None:\n        loader.Loader.add_constructor(tag, constructor)\n        loader.FullLoader.add_constructor(tag, constructor)\n        loader.UnsafeLoader.add_constructor(tag, constructor)\n    else:\n        Loader.add_constructor(tag, constructor)\n\ndef add_multi_constructor(tag_prefix, multi_constructor, Loader=None):\n    \"\"\"\n    Add a multi-constructor for the given tag prefix.\n    Multi-constructor is called for a node if its tag starts with tag_prefix.\n    Multi-constructor accepts a Loader instance, a tag suffix,\n    and a node object and produces the corresponding Python object.\n    \"\"\"\n    if Loader is None:\n        loader.Loader.add_multi_constructor(tag_prefix, multi_constructor)\n        loader.FullLoader.add_multi_constructor(tag_prefix, multi_constructor)\n        loader.UnsafeLoader.add_multi_constructor(tag_prefix, multi_constructor)\n    else:\n        Loader.add_multi_constructor(tag_prefix, multi_constructor)\n\ndef add_representer(data_type, representer, Dumper=Dumper):\n    \"\"\"\n    Add a representer for the given type.\n    Representer is a function accepting a Dumper instance\n    and an instance of the given data type\n    and producing the corresponding representation node.\n    \"\"\"\n    Dumper.add_representer(data_type, representer)\n\ndef add_multi_representer(data_type, multi_representer, Dumper=Dumper):\n    \"\"\"\n    Add a representer for the given type.\n    Multi-representer is a function accepting a Dumper instance\n    and an instance of the given data type or subtype\n    and producing the corresponding representation node.\n    \"\"\"\n    Dumper.add_multi_representer(data_type, multi_representer)\n\nclass YAMLObjectMetaclass(type):\n    \"\"\"\n    The metaclass for YAMLObject.\n    \"\"\"\n    def __init__(cls, name, bases, kwds):\n        super(YAMLObjectMetaclass, cls).__init__(name, bases, kwds)\n        if 'yaml_tag' in kwds and kwds['yaml_tag'] is not None:\n            if isinstance(cls.yaml_loader, list):\n                for loader in cls.yaml_loader:\n                    loader.add_constructor(cls.yaml_tag, cls.from_yaml)\n            else:\n                cls.yaml_loader.add_constructor(cls.yaml_tag, cls.from_yaml)\n\n            cls.yaml_dumper.add_representer(cls, cls.to_yaml)\n\nclass YAMLObject(metaclass=YAMLObjectMetaclass):\n    \"\"\"\n    An object that can dump itself to a YAML stream\n    and load itself from a YAML stream.\n    \"\"\"\n\n    __slots__ = ()  # no direct instantiation, so allow immutable subclasses\n\n    yaml_loader = [Loader, FullLoader, UnsafeLoader]\n    yaml_dumper = Dumper\n\n    yaml_tag = None\n    yaml_flow_style = None\n\n    @classmethod\n    def from_yaml(cls, loader, node):\n        \"\"\"\n        Convert a representation node to a Python object.\n        \"\"\"\n        return loader.construct_yaml_object(node, cls)\n\n    @classmethod\n    def to_yaml(cls, dumper, data):\n        \"\"\"\n        Convert a Python object to a representation node.\n        \"\"\"\n        return dumper.represent_yaml_object(cls.yaml_tag, data, cls,\n                flow_style=cls.yaml_flow_style)\n\n", "lib/yaml/cyaml.py": "\n__all__ = [\n    'CBaseLoader', 'CSafeLoader', 'CFullLoader', 'CUnsafeLoader', 'CLoader',\n    'CBaseDumper', 'CSafeDumper', 'CDumper'\n]\n\nfrom yaml._yaml import CParser, CEmitter\n\nfrom .constructor import *\n\nfrom .serializer import *\nfrom .representer import *\n\nfrom .resolver import *\n\nclass CBaseLoader(CParser, BaseConstructor, BaseResolver):\n\n    def __init__(self, stream):\n        CParser.__init__(self, stream)\n        BaseConstructor.__init__(self)\n        BaseResolver.__init__(self)\n\nclass CSafeLoader(CParser, SafeConstructor, Resolver):\n\n    def __init__(self, stream):\n        CParser.__init__(self, stream)\n        SafeConstructor.__init__(self)\n        Resolver.__init__(self)\n\nclass CFullLoader(CParser, FullConstructor, Resolver):\n\n    def __init__(self, stream):\n        CParser.__init__(self, stream)\n        FullConstructor.__init__(self)\n        Resolver.__init__(self)\n\nclass CUnsafeLoader(CParser, UnsafeConstructor, Resolver):\n\n    def __init__(self, stream):\n        CParser.__init__(self, stream)\n        UnsafeConstructor.__init__(self)\n        Resolver.__init__(self)\n\nclass CLoader(CParser, Constructor, Resolver):\n\n    def __init__(self, stream):\n        CParser.__init__(self, stream)\n        Constructor.__init__(self)\n        Resolver.__init__(self)\n\nclass CBaseDumper(CEmitter, BaseRepresenter, BaseResolver):\n\n    def __init__(self, stream,\n            default_style=None, default_flow_style=False,\n            canonical=None, indent=None, width=None,\n            allow_unicode=None, line_break=None,\n            encoding=None, explicit_start=None, explicit_end=None,\n            version=None, tags=None, sort_keys=True):\n        CEmitter.__init__(self, stream, canonical=canonical,\n                indent=indent, width=width, encoding=encoding,\n                allow_unicode=allow_unicode, line_break=line_break,\n                explicit_start=explicit_start, explicit_end=explicit_end,\n                version=version, tags=tags)\n        Representer.__init__(self, default_style=default_style,\n                default_flow_style=default_flow_style, sort_keys=sort_keys)\n        Resolver.__init__(self)\n\nclass CSafeDumper(CEmitter, SafeRepresenter, Resolver):\n\n    def __init__(self, stream,\n            default_style=None, default_flow_style=False,\n            canonical=None, indent=None, width=None,\n            allow_unicode=None, line_break=None,\n            encoding=None, explicit_start=None, explicit_end=None,\n            version=None, tags=None, sort_keys=True):\n        CEmitter.__init__(self, stream, canonical=canonical,\n                indent=indent, width=width, encoding=encoding,\n                allow_unicode=allow_unicode, line_break=line_break,\n                explicit_start=explicit_start, explicit_end=explicit_end,\n                version=version, tags=tags)\n        SafeRepresenter.__init__(self, default_style=default_style,\n                default_flow_style=default_flow_style, sort_keys=sort_keys)\n        Resolver.__init__(self)\n\nclass CDumper(CEmitter, Serializer, Representer, Resolver):\n\n    def __init__(self, stream,\n            default_style=None, default_flow_style=False,\n            canonical=None, indent=None, width=None,\n            allow_unicode=None, line_break=None,\n            encoding=None, explicit_start=None, explicit_end=None,\n            version=None, tags=None, sort_keys=True):\n        CEmitter.__init__(self, stream, canonical=canonical,\n                indent=indent, width=width, encoding=encoding,\n                allow_unicode=allow_unicode, line_break=line_break,\n                explicit_start=explicit_start, explicit_end=explicit_end,\n                version=version, tags=tags)\n        Representer.__init__(self, default_style=default_style,\n                default_flow_style=default_flow_style, sort_keys=sort_keys)\n        Resolver.__init__(self)\n\n", "lib/yaml/parser.py": "\n# The following YAML grammar is LL(1) and is parsed by a recursive descent\n# parser.\n#\n# stream            ::= STREAM-START implicit_document? explicit_document* STREAM-END\n# implicit_document ::= block_node DOCUMENT-END*\n# explicit_document ::= DIRECTIVE* DOCUMENT-START block_node? DOCUMENT-END*\n# block_node_or_indentless_sequence ::=\n#                       ALIAS\n#                       | properties (block_content | indentless_block_sequence)?\n#                       | block_content\n#                       | indentless_block_sequence\n# block_node        ::= ALIAS\n#                       | properties block_content?\n#                       | block_content\n# flow_node         ::= ALIAS\n#                       | properties flow_content?\n#                       | flow_content\n# properties        ::= TAG ANCHOR? | ANCHOR TAG?\n# block_content     ::= block_collection | flow_collection | SCALAR\n# flow_content      ::= flow_collection | SCALAR\n# block_collection  ::= block_sequence | block_mapping\n# flow_collection   ::= flow_sequence | flow_mapping\n# block_sequence    ::= BLOCK-SEQUENCE-START (BLOCK-ENTRY block_node?)* BLOCK-END\n# indentless_sequence   ::= (BLOCK-ENTRY block_node?)+\n# block_mapping     ::= BLOCK-MAPPING_START\n#                       ((KEY block_node_or_indentless_sequence?)?\n#                       (VALUE block_node_or_indentless_sequence?)?)*\n#                       BLOCK-END\n# flow_sequence     ::= FLOW-SEQUENCE-START\n#                       (flow_sequence_entry FLOW-ENTRY)*\n#                       flow_sequence_entry?\n#                       FLOW-SEQUENCE-END\n# flow_sequence_entry   ::= flow_node | KEY flow_node? (VALUE flow_node?)?\n# flow_mapping      ::= FLOW-MAPPING-START\n#                       (flow_mapping_entry FLOW-ENTRY)*\n#                       flow_mapping_entry?\n#                       FLOW-MAPPING-END\n# flow_mapping_entry    ::= flow_node | KEY flow_node? (VALUE flow_node?)?\n#\n# FIRST sets:\n#\n# stream: { STREAM-START }\n# explicit_document: { DIRECTIVE DOCUMENT-START }\n# implicit_document: FIRST(block_node)\n# block_node: { ALIAS TAG ANCHOR SCALAR BLOCK-SEQUENCE-START BLOCK-MAPPING-START FLOW-SEQUENCE-START FLOW-MAPPING-START }\n# flow_node: { ALIAS ANCHOR TAG SCALAR FLOW-SEQUENCE-START FLOW-MAPPING-START }\n# block_content: { BLOCK-SEQUENCE-START BLOCK-MAPPING-START FLOW-SEQUENCE-START FLOW-MAPPING-START SCALAR }\n# flow_content: { FLOW-SEQUENCE-START FLOW-MAPPING-START SCALAR }\n# block_collection: { BLOCK-SEQUENCE-START BLOCK-MAPPING-START }\n# flow_collection: { FLOW-SEQUENCE-START FLOW-MAPPING-START }\n# block_sequence: { BLOCK-SEQUENCE-START }\n# block_mapping: { BLOCK-MAPPING-START }\n# block_node_or_indentless_sequence: { ALIAS ANCHOR TAG SCALAR BLOCK-SEQUENCE-START BLOCK-MAPPING-START FLOW-SEQUENCE-START FLOW-MAPPING-START BLOCK-ENTRY }\n# indentless_sequence: { ENTRY }\n# flow_collection: { FLOW-SEQUENCE-START FLOW-MAPPING-START }\n# flow_sequence: { FLOW-SEQUENCE-START }\n# flow_mapping: { FLOW-MAPPING-START }\n# flow_sequence_entry: { ALIAS ANCHOR TAG SCALAR FLOW-SEQUENCE-START FLOW-MAPPING-START KEY }\n# flow_mapping_entry: { ALIAS ANCHOR TAG SCALAR FLOW-SEQUENCE-START FLOW-MAPPING-START KEY }\n\n__all__ = ['Parser', 'ParserError']\n\nfrom .error import MarkedYAMLError\nfrom .tokens import *\nfrom .events import *\nfrom .scanner import *\n\nclass ParserError(MarkedYAMLError):\n    pass\n\nclass Parser:\n    # Since writing a recursive-descendant parser is a straightforward task, we\n    # do not give many comments here.\n\n    DEFAULT_TAGS = {\n        '!':   '!',\n        '!!':  'tag:yaml.org,2002:',\n    }\n\n    def __init__(self):\n        self.current_event = None\n        self.yaml_version = None\n        self.tag_handles = {}\n        self.states = []\n        self.marks = []\n        self.state = self.parse_stream_start\n\n    def dispose(self):\n        # Reset the state attributes (to clear self-references)\n        self.states = []\n        self.state = None\n\n    def check_event(self, *choices):\n        # Check the type of the next event.\n        if self.current_event is None:\n            if self.state:\n                self.current_event = self.state()\n        if self.current_event is not None:\n            if not choices:\n                return True\n            for choice in choices:\n                if isinstance(self.current_event, choice):\n                    return True\n        return False\n\n    def peek_event(self):\n        # Get the next event.\n        if self.current_event is None:\n            if self.state:\n                self.current_event = self.state()\n        return self.current_event\n\n    def get_event(self):\n        # Get the next event and proceed further.\n        if self.current_event is None:\n            if self.state:\n                self.current_event = self.state()\n        value = self.current_event\n        self.current_event = None\n        return value\n\n    # stream    ::= STREAM-START implicit_document? explicit_document* STREAM-END\n    # implicit_document ::= block_node DOCUMENT-END*\n    # explicit_document ::= DIRECTIVE* DOCUMENT-START block_node? DOCUMENT-END*\n\n    def parse_stream_start(self):\n\n        # Parse the stream start.\n        token = self.get_token()\n        event = StreamStartEvent(token.start_mark, token.end_mark,\n                encoding=token.encoding)\n\n        # Prepare the next state.\n        self.state = self.parse_implicit_document_start\n\n        return event\n\n    def parse_implicit_document_start(self):\n\n        # Parse an implicit document.\n        if not self.check_token(DirectiveToken, DocumentStartToken,\n                StreamEndToken):\n            self.tag_handles = self.DEFAULT_TAGS\n            token = self.peek_token()\n            start_mark = end_mark = token.start_mark\n            event = DocumentStartEvent(start_mark, end_mark,\n                    explicit=False)\n\n            # Prepare the next state.\n            self.states.append(self.parse_document_end)\n            self.state = self.parse_block_node\n\n            return event\n\n        else:\n            return self.parse_document_start()\n\n    def parse_document_start(self):\n\n        # Parse any extra document end indicators.\n        while self.check_token(DocumentEndToken):\n            self.get_token()\n\n        # Parse an explicit document.\n        if not self.check_token(StreamEndToken):\n            token = self.peek_token()\n            start_mark = token.start_mark\n            version, tags = self.process_directives()\n            if not self.check_token(DocumentStartToken):\n                raise ParserError(None, None,\n                        \"expected '<document start>', but found %r\"\n                        % self.peek_token().id,\n                        self.peek_token().start_mark)\n            token = self.get_token()\n            end_mark = token.end_mark\n            event = DocumentStartEvent(start_mark, end_mark,\n                    explicit=True, version=version, tags=tags)\n            self.states.append(self.parse_document_end)\n            self.state = self.parse_document_content\n        else:\n            # Parse the end of the stream.\n            token = self.get_token()\n            event = StreamEndEvent(token.start_mark, token.end_mark)\n            assert not self.states\n            assert not self.marks\n            self.state = None\n        return event\n\n    def parse_document_end(self):\n\n        # Parse the document end.\n        token = self.peek_token()\n        start_mark = end_mark = token.start_mark\n        explicit = False\n        if self.check_token(DocumentEndToken):\n            token = self.get_token()\n            end_mark = token.end_mark\n            explicit = True\n        event = DocumentEndEvent(start_mark, end_mark,\n                explicit=explicit)\n\n        # Prepare the next state.\n        self.state = self.parse_document_start\n\n        return event\n\n    def parse_document_content(self):\n        if self.check_token(DirectiveToken,\n                DocumentStartToken, DocumentEndToken, StreamEndToken):\n            event = self.process_empty_scalar(self.peek_token().start_mark)\n            self.state = self.states.pop()\n            return event\n        else:\n            return self.parse_block_node()\n\n    def process_directives(self):\n        self.yaml_version = None\n        self.tag_handles = {}\n        while self.check_token(DirectiveToken):\n            token = self.get_token()\n            if token.name == 'YAML':\n                if self.yaml_version is not None:\n                    raise ParserError(None, None,\n                            \"found duplicate YAML directive\", token.start_mark)\n                major, minor = token.value\n                if major != 1:\n                    raise ParserError(None, None,\n                            \"found incompatible YAML document (version 1.* is required)\",\n                            token.start_mark)\n                self.yaml_version = token.value\n            elif token.name == 'TAG':\n                handle, prefix = token.value\n                if handle in self.tag_handles:\n                    raise ParserError(None, None,\n                            \"duplicate tag handle %r\" % handle,\n                            token.start_mark)\n                self.tag_handles[handle] = prefix\n        if self.tag_handles:\n            value = self.yaml_version, self.tag_handles.copy()\n        else:\n            value = self.yaml_version, None\n        for key in self.DEFAULT_TAGS:\n            if key not in self.tag_handles:\n                self.tag_handles[key] = self.DEFAULT_TAGS[key]\n        return value\n\n    # block_node_or_indentless_sequence ::= ALIAS\n    #               | properties (block_content | indentless_block_sequence)?\n    #               | block_content\n    #               | indentless_block_sequence\n    # block_node    ::= ALIAS\n    #                   | properties block_content?\n    #                   | block_content\n    # flow_node     ::= ALIAS\n    #                   | properties flow_content?\n    #                   | flow_content\n    # properties    ::= TAG ANCHOR? | ANCHOR TAG?\n    # block_content     ::= block_collection | flow_collection | SCALAR\n    # flow_content      ::= flow_collection | SCALAR\n    # block_collection  ::= block_sequence | block_mapping\n    # flow_collection   ::= flow_sequence | flow_mapping\n\n    def parse_block_node(self):\n        return self.parse_node(block=True)\n\n    def parse_flow_node(self):\n        return self.parse_node()\n\n    def parse_block_node_or_indentless_sequence(self):\n        return self.parse_node(block=True, indentless_sequence=True)\n\n    def parse_node(self, block=False, indentless_sequence=False):\n        if self.check_token(AliasToken):\n            token = self.get_token()\n            event = AliasEvent(token.value, token.start_mark, token.end_mark)\n            self.state = self.states.pop()\n        else:\n            anchor = None\n            tag = None\n            start_mark = end_mark = tag_mark = None\n            if self.check_token(AnchorToken):\n                token = self.get_token()\n                start_mark = token.start_mark\n                end_mark = token.end_mark\n                anchor = token.value\n                if self.check_token(TagToken):\n                    token = self.get_token()\n                    tag_mark = token.start_mark\n                    end_mark = token.end_mark\n                    tag = token.value\n            elif self.check_token(TagToken):\n                token = self.get_token()\n                start_mark = tag_mark = token.start_mark\n                end_mark = token.end_mark\n                tag = token.value\n                if self.check_token(AnchorToken):\n                    token = self.get_token()\n                    end_mark = token.end_mark\n                    anchor = token.value\n            if tag is not None:\n                handle, suffix = tag\n                if handle is not None:\n                    if handle not in self.tag_handles:\n                        raise ParserError(\"while parsing a node\", start_mark,\n                                \"found undefined tag handle %r\" % handle,\n                                tag_mark)\n                    tag = self.tag_handles[handle]+suffix\n                else:\n                    tag = suffix\n            #if tag == '!':\n            #    raise ParserError(\"while parsing a node\", start_mark,\n            #            \"found non-specific tag '!'\", tag_mark,\n            #            \"Please check 'http://pyyaml.org/wiki/YAMLNonSpecificTag' and share your opinion.\")\n            if start_mark is None:\n                start_mark = end_mark = self.peek_token().start_mark\n            event = None\n            implicit = (tag is None or tag == '!')\n            if indentless_sequence and self.check_token(BlockEntryToken):\n                end_mark = self.peek_token().end_mark\n                event = SequenceStartEvent(anchor, tag, implicit,\n                        start_mark, end_mark)\n                self.state = self.parse_indentless_sequence_entry\n            else:\n                if self.check_token(ScalarToken):\n                    token = self.get_token()\n                    end_mark = token.end_mark\n                    if (token.plain and tag is None) or tag == '!':\n                        implicit = (True, False)\n                    elif tag is None:\n                        implicit = (False, True)\n                    else:\n                        implicit = (False, False)\n                    event = ScalarEvent(anchor, tag, implicit, token.value,\n                            start_mark, end_mark, style=token.style)\n                    self.state = self.states.pop()\n                elif self.check_token(FlowSequenceStartToken):\n                    end_mark = self.peek_token().end_mark\n                    event = SequenceStartEvent(anchor, tag, implicit,\n                            start_mark, end_mark, flow_style=True)\n                    self.state = self.parse_flow_sequence_first_entry\n                elif self.check_token(FlowMappingStartToken):\n                    end_mark = self.peek_token().end_mark\n                    event = MappingStartEvent(anchor, tag, implicit,\n                            start_mark, end_mark, flow_style=True)\n                    self.state = self.parse_flow_mapping_first_key\n                elif block and self.check_token(BlockSequenceStartToken):\n                    end_mark = self.peek_token().start_mark\n                    event = SequenceStartEvent(anchor, tag, implicit,\n                            start_mark, end_mark, flow_style=False)\n                    self.state = self.parse_block_sequence_first_entry\n                elif block and self.check_token(BlockMappingStartToken):\n                    end_mark = self.peek_token().start_mark\n                    event = MappingStartEvent(anchor, tag, implicit,\n                            start_mark, end_mark, flow_style=False)\n                    self.state = self.parse_block_mapping_first_key\n                elif anchor is not None or tag is not None:\n                    # Empty scalars are allowed even if a tag or an anchor is\n                    # specified.\n                    event = ScalarEvent(anchor, tag, (implicit, False), '',\n                            start_mark, end_mark)\n                    self.state = self.states.pop()\n                else:\n                    if block:\n                        node = 'block'\n                    else:\n                        node = 'flow'\n                    token = self.peek_token()\n                    raise ParserError(\"while parsing a %s node\" % node, start_mark,\n                            \"expected the node content, but found %r\" % token.id,\n                            token.start_mark)\n        return event\n\n    # block_sequence ::= BLOCK-SEQUENCE-START (BLOCK-ENTRY block_node?)* BLOCK-END\n\n    def parse_block_sequence_first_entry(self):\n        token = self.get_token()\n        self.marks.append(token.start_mark)\n        return self.parse_block_sequence_entry()\n\n    def parse_block_sequence_entry(self):\n        if self.check_token(BlockEntryToken):\n            token = self.get_token()\n            if not self.check_token(BlockEntryToken, BlockEndToken):\n                self.states.append(self.parse_block_sequence_entry)\n                return self.parse_block_node()\n            else:\n                self.state = self.parse_block_sequence_entry\n                return self.process_empty_scalar(token.end_mark)\n        if not self.check_token(BlockEndToken):\n            token = self.peek_token()\n            raise ParserError(\"while parsing a block collection\", self.marks[-1],\n                    \"expected <block end>, but found %r\" % token.id, token.start_mark)\n        token = self.get_token()\n        event = SequenceEndEvent(token.start_mark, token.end_mark)\n        self.state = self.states.pop()\n        self.marks.pop()\n        return event\n\n    # indentless_sequence ::= (BLOCK-ENTRY block_node?)+\n\n    def parse_indentless_sequence_entry(self):\n        if self.check_token(BlockEntryToken):\n            token = self.get_token()\n            if not self.check_token(BlockEntryToken,\n                    KeyToken, ValueToken, BlockEndToken):\n                self.states.append(self.parse_indentless_sequence_entry)\n                return self.parse_block_node()\n            else:\n                self.state = self.parse_indentless_sequence_entry\n                return self.process_empty_scalar(token.end_mark)\n        token = self.peek_token()\n        event = SequenceEndEvent(token.start_mark, token.start_mark)\n        self.state = self.states.pop()\n        return event\n\n    # block_mapping     ::= BLOCK-MAPPING_START\n    #                       ((KEY block_node_or_indentless_sequence?)?\n    #                       (VALUE block_node_or_indentless_sequence?)?)*\n    #                       BLOCK-END\n\n    def parse_block_mapping_first_key(self):\n        token = self.get_token()\n        self.marks.append(token.start_mark)\n        return self.parse_block_mapping_key()\n\n    def parse_block_mapping_key(self):\n        if self.check_token(KeyToken):\n            token = self.get_token()\n            if not self.check_token(KeyToken, ValueToken, BlockEndToken):\n                self.states.append(self.parse_block_mapping_value)\n                return self.parse_block_node_or_indentless_sequence()\n            else:\n                self.state = self.parse_block_mapping_value\n                return self.process_empty_scalar(token.end_mark)\n        if not self.check_token(BlockEndToken):\n            token = self.peek_token()\n            raise ParserError(\"while parsing a block mapping\", self.marks[-1],\n                    \"expected <block end>, but found %r\" % token.id, token.start_mark)\n        token = self.get_token()\n        event = MappingEndEvent(token.start_mark, token.end_mark)\n        self.state = self.states.pop()\n        self.marks.pop()\n        return event\n\n    def parse_block_mapping_value(self):\n        if self.check_token(ValueToken):\n            token = self.get_token()\n            if not self.check_token(KeyToken, ValueToken, BlockEndToken):\n                self.states.append(self.parse_block_mapping_key)\n                return self.parse_block_node_or_indentless_sequence()\n            else:\n                self.state = self.parse_block_mapping_key\n                return self.process_empty_scalar(token.end_mark)\n        else:\n            self.state = self.parse_block_mapping_key\n            token = self.peek_token()\n            return self.process_empty_scalar(token.start_mark)\n\n    # flow_sequence     ::= FLOW-SEQUENCE-START\n    #                       (flow_sequence_entry FLOW-ENTRY)*\n    #                       flow_sequence_entry?\n    #                       FLOW-SEQUENCE-END\n    # flow_sequence_entry   ::= flow_node | KEY flow_node? (VALUE flow_node?)?\n    #\n    # Note that while production rules for both flow_sequence_entry and\n    # flow_mapping_entry are equal, their interpretations are different.\n    # For `flow_sequence_entry`, the part `KEY flow_node? (VALUE flow_node?)?`\n    # generate an inline mapping (set syntax).\n\n    def parse_flow_sequence_first_entry(self):\n        token = self.get_token()\n        self.marks.append(token.start_mark)\n        return self.parse_flow_sequence_entry(first=True)\n\n    def parse_flow_sequence_entry(self, first=False):\n        if not self.check_token(FlowSequenceEndToken):\n            if not first:\n                if self.check_token(FlowEntryToken):\n                    self.get_token()\n                else:\n                    token = self.peek_token()\n                    raise ParserError(\"while parsing a flow sequence\", self.marks[-1],\n                            \"expected ',' or ']', but got %r\" % token.id, token.start_mark)\n            \n            if self.check_token(KeyToken):\n                token = self.peek_token()\n                event = MappingStartEvent(None, None, True,\n                        token.start_mark, token.end_mark,\n                        flow_style=True)\n                self.state = self.parse_flow_sequence_entry_mapping_key\n                return event\n            elif not self.check_token(FlowSequenceEndToken):\n                self.states.append(self.parse_flow_sequence_entry)\n                return self.parse_flow_node()\n        token = self.get_token()\n        event = SequenceEndEvent(token.start_mark, token.end_mark)\n        self.state = self.states.pop()\n        self.marks.pop()\n        return event\n\n    def parse_flow_sequence_entry_mapping_key(self):\n        token = self.get_token()\n        if not self.check_token(ValueToken,\n                FlowEntryToken, FlowSequenceEndToken):\n            self.states.append(self.parse_flow_sequence_entry_mapping_value)\n            return self.parse_flow_node()\n        else:\n            self.state = self.parse_flow_sequence_entry_mapping_value\n            return self.process_empty_scalar(token.end_mark)\n\n    def parse_flow_sequence_entry_mapping_value(self):\n        if self.check_token(ValueToken):\n            token = self.get_token()\n            if not self.check_token(FlowEntryToken, FlowSequenceEndToken):\n                self.states.append(self.parse_flow_sequence_entry_mapping_end)\n                return self.parse_flow_node()\n            else:\n                self.state = self.parse_flow_sequence_entry_mapping_end\n                return self.process_empty_scalar(token.end_mark)\n        else:\n            self.state = self.parse_flow_sequence_entry_mapping_end\n            token = self.peek_token()\n            return self.process_empty_scalar(token.start_mark)\n\n    def parse_flow_sequence_entry_mapping_end(self):\n        self.state = self.parse_flow_sequence_entry\n        token = self.peek_token()\n        return MappingEndEvent(token.start_mark, token.start_mark)\n\n    # flow_mapping  ::= FLOW-MAPPING-START\n    #                   (flow_mapping_entry FLOW-ENTRY)*\n    #                   flow_mapping_entry?\n    #                   FLOW-MAPPING-END\n    # flow_mapping_entry    ::= flow_node | KEY flow_node? (VALUE flow_node?)?\n\n    def parse_flow_mapping_first_key(self):\n        token = self.get_token()\n        self.marks.append(token.start_mark)\n        return self.parse_flow_mapping_key(first=True)\n\n    def parse_flow_mapping_key(self, first=False):\n        if not self.check_token(FlowMappingEndToken):\n            if not first:\n                if self.check_token(FlowEntryToken):\n                    self.get_token()\n                else:\n                    token = self.peek_token()\n                    raise ParserError(\"while parsing a flow mapping\", self.marks[-1],\n                            \"expected ',' or '}', but got %r\" % token.id, token.start_mark)\n            if self.check_token(KeyToken):\n                token = self.get_token()\n                if not self.check_token(ValueToken,\n                        FlowEntryToken, FlowMappingEndToken):\n                    self.states.append(self.parse_flow_mapping_value)\n                    return self.parse_flow_node()\n                else:\n                    self.state = self.parse_flow_mapping_value\n                    return self.process_empty_scalar(token.end_mark)\n            elif not self.check_token(FlowMappingEndToken):\n                self.states.append(self.parse_flow_mapping_empty_value)\n                return self.parse_flow_node()\n        token = self.get_token()\n        event = MappingEndEvent(token.start_mark, token.end_mark)\n        self.state = self.states.pop()\n        self.marks.pop()\n        return event\n\n    def parse_flow_mapping_value(self):\n        if self.check_token(ValueToken):\n            token = self.get_token()\n            if not self.check_token(FlowEntryToken, FlowMappingEndToken):\n                self.states.append(self.parse_flow_mapping_key)\n                return self.parse_flow_node()\n            else:\n                self.state = self.parse_flow_mapping_key\n                return self.process_empty_scalar(token.end_mark)\n        else:\n            self.state = self.parse_flow_mapping_key\n            token = self.peek_token()\n            return self.process_empty_scalar(token.start_mark)\n\n    def parse_flow_mapping_empty_value(self):\n        self.state = self.parse_flow_mapping_key\n        return self.process_empty_scalar(self.peek_token().start_mark)\n\n    def process_empty_scalar(self, mark):\n        return ScalarEvent(None, None, (True, False), '', mark, mark)\n\n", "lib/yaml/composer.py": "\n__all__ = ['Composer', 'ComposerError']\n\nfrom .error import MarkedYAMLError\nfrom .events import *\nfrom .nodes import *\n\nclass ComposerError(MarkedYAMLError):\n    pass\n\nclass Composer:\n\n    def __init__(self):\n        self.anchors = {}\n\n    def check_node(self):\n        # Drop the STREAM-START event.\n        if self.check_event(StreamStartEvent):\n            self.get_event()\n\n        # If there are more documents available?\n        return not self.check_event(StreamEndEvent)\n\n    def get_node(self):\n        # Get the root node of the next document.\n        if not self.check_event(StreamEndEvent):\n            return self.compose_document()\n\n    def get_single_node(self):\n        # Drop the STREAM-START event.\n        self.get_event()\n\n        # Compose a document if the stream is not empty.\n        document = None\n        if not self.check_event(StreamEndEvent):\n            document = self.compose_document()\n\n        # Ensure that the stream contains no more documents.\n        if not self.check_event(StreamEndEvent):\n            event = self.get_event()\n            raise ComposerError(\"expected a single document in the stream\",\n                    document.start_mark, \"but found another document\",\n                    event.start_mark)\n\n        # Drop the STREAM-END event.\n        self.get_event()\n\n        return document\n\n    def compose_document(self):\n        # Drop the DOCUMENT-START event.\n        self.get_event()\n\n        # Compose the root node.\n        node = self.compose_node(None, None)\n\n        # Drop the DOCUMENT-END event.\n        self.get_event()\n\n        self.anchors = {}\n        return node\n\n    def compose_node(self, parent, index):\n        if self.check_event(AliasEvent):\n            event = self.get_event()\n            anchor = event.anchor\n            if anchor not in self.anchors:\n                raise ComposerError(None, None, \"found undefined alias %r\"\n                        % anchor, event.start_mark)\n            return self.anchors[anchor]\n        event = self.peek_event()\n        anchor = event.anchor\n        if anchor is not None:\n            if anchor in self.anchors:\n                raise ComposerError(\"found duplicate anchor %r; first occurrence\"\n                        % anchor, self.anchors[anchor].start_mark,\n                        \"second occurrence\", event.start_mark)\n        self.descend_resolver(parent, index)\n        if self.check_event(ScalarEvent):\n            node = self.compose_scalar_node(anchor)\n        elif self.check_event(SequenceStartEvent):\n            node = self.compose_sequence_node(anchor)\n        elif self.check_event(MappingStartEvent):\n            node = self.compose_mapping_node(anchor)\n        self.ascend_resolver()\n        return node\n\n    def compose_scalar_node(self, anchor):\n        event = self.get_event()\n        tag = event.tag\n        if tag is None or tag == '!':\n            tag = self.resolve(ScalarNode, event.value, event.implicit)\n        node = ScalarNode(tag, event.value,\n                event.start_mark, event.end_mark, style=event.style)\n        if anchor is not None:\n            self.anchors[anchor] = node\n        return node\n\n    def compose_sequence_node(self, anchor):\n        start_event = self.get_event()\n        tag = start_event.tag\n        if tag is None or tag == '!':\n            tag = self.resolve(SequenceNode, None, start_event.implicit)\n        node = SequenceNode(tag, [],\n                start_event.start_mark, None,\n                flow_style=start_event.flow_style)\n        if anchor is not None:\n            self.anchors[anchor] = node\n        index = 0\n        while not self.check_event(SequenceEndEvent):\n            node.value.append(self.compose_node(node, index))\n            index += 1\n        end_event = self.get_event()\n        node.end_mark = end_event.end_mark\n        return node\n\n    def compose_mapping_node(self, anchor):\n        start_event = self.get_event()\n        tag = start_event.tag\n        if tag is None or tag == '!':\n            tag = self.resolve(MappingNode, None, start_event.implicit)\n        node = MappingNode(tag, [],\n                start_event.start_mark, None,\n                flow_style=start_event.flow_style)\n        if anchor is not None:\n            self.anchors[anchor] = node\n        while not self.check_event(MappingEndEvent):\n            #key_event = self.peek_event()\n            item_key = self.compose_node(node, None)\n            #if item_key in node.value:\n            #    raise ComposerError(\"while composing a mapping\", start_event.start_mark,\n            #            \"found duplicate key\", key_event.start_mark)\n            item_value = self.compose_node(node, item_key)\n            #node.value[item_key] = item_value\n            node.value.append((item_key, item_value))\n        end_event = self.get_event()\n        node.end_mark = end_event.end_mark\n        return node\n\n", "lib/yaml/dumper.py": "\n__all__ = ['BaseDumper', 'SafeDumper', 'Dumper']\n\nfrom .emitter import *\nfrom .serializer import *\nfrom .representer import *\nfrom .resolver import *\n\nclass BaseDumper(Emitter, Serializer, BaseRepresenter, BaseResolver):\n\n    def __init__(self, stream,\n            default_style=None, default_flow_style=False,\n            canonical=None, indent=None, width=None,\n            allow_unicode=None, line_break=None,\n            encoding=None, explicit_start=None, explicit_end=None,\n            version=None, tags=None, sort_keys=True):\n        Emitter.__init__(self, stream, canonical=canonical,\n                indent=indent, width=width,\n                allow_unicode=allow_unicode, line_break=line_break)\n        Serializer.__init__(self, encoding=encoding,\n                explicit_start=explicit_start, explicit_end=explicit_end,\n                version=version, tags=tags)\n        Representer.__init__(self, default_style=default_style,\n                default_flow_style=default_flow_style, sort_keys=sort_keys)\n        Resolver.__init__(self)\n\nclass SafeDumper(Emitter, Serializer, SafeRepresenter, Resolver):\n\n    def __init__(self, stream,\n            default_style=None, default_flow_style=False,\n            canonical=None, indent=None, width=None,\n            allow_unicode=None, line_break=None,\n            encoding=None, explicit_start=None, explicit_end=None,\n            version=None, tags=None, sort_keys=True):\n        Emitter.__init__(self, stream, canonical=canonical,\n                indent=indent, width=width,\n                allow_unicode=allow_unicode, line_break=line_break)\n        Serializer.__init__(self, encoding=encoding,\n                explicit_start=explicit_start, explicit_end=explicit_end,\n                version=version, tags=tags)\n        SafeRepresenter.__init__(self, default_style=default_style,\n                default_flow_style=default_flow_style, sort_keys=sort_keys)\n        Resolver.__init__(self)\n\nclass Dumper(Emitter, Serializer, Representer, Resolver):\n\n    def __init__(self, stream,\n            default_style=None, default_flow_style=False,\n            canonical=None, indent=None, width=None,\n            allow_unicode=None, line_break=None,\n            encoding=None, explicit_start=None, explicit_end=None,\n            version=None, tags=None, sort_keys=True):\n        Emitter.__init__(self, stream, canonical=canonical,\n                indent=indent, width=width,\n                allow_unicode=allow_unicode, line_break=line_break)\n        Serializer.__init__(self, encoding=encoding,\n                explicit_start=explicit_start, explicit_end=explicit_end,\n                version=version, tags=tags)\n        Representer.__init__(self, default_style=default_style,\n                default_flow_style=default_flow_style, sort_keys=sort_keys)\n        Resolver.__init__(self)\n\n"}