{"setup.py": "#!/usr/bin/env python\nimport os\nimport re\n\nfrom setuptools import find_packages, setup\n\nROOT = os.path.dirname(__file__)\nVERSION_RE = re.compile(r'''__version__ = ['\"]([0-9.]+)['\"]''')\n\n\nrequires = [\n    'botocore>=1.33.2,<2.0a.0',\n]\n\n\ndef get_version():\n    init = open(os.path.join(ROOT, 's3transfer', '__init__.py')).read()\n    return VERSION_RE.search(init).group(1)\n\n\nsetup(\n    name='s3transfer',\n    version=get_version(),\n    description='An Amazon S3 Transfer Manager',\n    long_description=open('README.rst').read(),\n    author='Amazon Web Services',\n    author_email='kyknapp1@gmail.com',\n    url='https://github.com/boto/s3transfer',\n    packages=find_packages(exclude=['tests*']),\n    include_package_data=True,\n    install_requires=requires,\n    extras_require={\n        'crt': 'botocore[crt]>=1.33.2,<2.0a.0',\n    },\n    license=\"Apache License 2.0\",\n    python_requires=\">= 3.8\",\n    classifiers=[\n        'Development Status :: 3 - Alpha',\n        'Intended Audience :: Developers',\n        'Natural Language :: English',\n        'License :: OSI Approved :: Apache Software License',\n        'Programming Language :: Python',\n        'Programming Language :: Python :: 3',\n        'Programming Language :: Python :: 3 :: Only',\n        'Programming Language :: Python :: 3.8',\n        'Programming Language :: Python :: 3.9',\n        'Programming Language :: Python :: 3.10',\n        'Programming Language :: Python :: 3.11',\n        'Programming Language :: Python :: 3.12',\n    ],\n)\n", "tests/__init__.py": "# Copyright 2016 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License'). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the 'license' file accompanying this file. This file is\n# distributed on an 'AS IS' BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport hashlib\nimport io\nimport math\nimport os\nimport platform\nimport shutil\nimport string\nimport tempfile\nimport unittest\nfrom unittest import mock  # noqa: F401\n\nimport botocore.session\nfrom botocore.stub import Stubber\n\nfrom s3transfer.futures import (\n    IN_MEMORY_DOWNLOAD_TAG,\n    IN_MEMORY_UPLOAD_TAG,\n    BoundedExecutor,\n    NonThreadedExecutor,\n    TransferCoordinator,\n    TransferFuture,\n    TransferMeta,\n)\nfrom s3transfer.manager import TransferConfig\nfrom s3transfer.subscribers import BaseSubscriber\nfrom s3transfer.utils import (\n    CallArgs,\n    OSUtils,\n    SlidingWindowSemaphore,\n    TaskSemaphore,\n)\n\nORIGINAL_EXECUTOR_CLS = BoundedExecutor.EXECUTOR_CLS\n# Detect if CRT is available for use\ntry:\n    import awscrt.s3  # noqa: F401\n\n    HAS_CRT = True\nexcept ImportError:\n    HAS_CRT = False\n\n\ndef setup_package():\n    if is_serial_implementation():\n        BoundedExecutor.EXECUTOR_CLS = NonThreadedExecutor\n\n\ndef teardown_package():\n    BoundedExecutor.EXECUTOR_CLS = ORIGINAL_EXECUTOR_CLS\n\n\ndef is_serial_implementation():\n    return os.environ.get('USE_SERIAL_EXECUTOR', False)\n\n\ndef assert_files_equal(first, second):\n    if os.path.getsize(first) != os.path.getsize(second):\n        raise AssertionError(f\"Files are not equal: {first}, {second}\")\n    first_md5 = md5_checksum(first)\n    second_md5 = md5_checksum(second)\n    if first_md5 != second_md5:\n        raise AssertionError(\n            \"Files are not equal: {}(md5={}) != {}(md5={})\".format(\n                first, first_md5, second, second_md5\n            )\n        )\n\n\ndef md5_checksum(filename):\n    checksum = hashlib.md5()\n    with open(filename, 'rb') as f:\n        for chunk in iter(lambda: f.read(8192), b''):\n            checksum.update(chunk)\n    return checksum.hexdigest()\n\n\ndef random_bucket_name(prefix='s3transfer', num_chars=10):\n    base = string.ascii_lowercase + string.digits\n    random_bytes = bytearray(os.urandom(num_chars))\n    return prefix + ''.join([base[b % len(base)] for b in random_bytes])\n\n\ndef skip_if_windows(reason):\n    \"\"\"Decorator to skip tests that should not be run on windows.\n\n    Example usage:\n\n        @skip_if_windows(\"Not valid\")\n        def test_some_non_windows_stuff(self):\n            self.assertEqual(...)\n\n    \"\"\"\n\n    def decorator(func):\n        return unittest.skipIf(\n            platform.system() not in ['Darwin', 'Linux'], reason\n        )(func)\n\n    return decorator\n\n\ndef skip_if_using_serial_implementation(reason):\n    \"\"\"Decorator to skip tests when running as the serial implementation\"\"\"\n\n    def decorator(func):\n        return unittest.skipIf(is_serial_implementation(), reason)(func)\n\n    return decorator\n\n\ndef requires_crt(cls, reason=None):\n    if reason is None:\n        reason = \"Test requires awscrt to be installed.\"\n    return unittest.skipIf(not HAS_CRT, reason)(cls)\n\n\nclass StreamWithError:\n    \"\"\"A wrapper to simulate errors while reading from a stream\n\n    :param stream: The underlying stream to read from\n    :param exception_type: The exception type to throw\n    :param num_reads: The number of times to allow a read before raising\n        the exception. A value of zero indicates to raise the error on the\n        first read.\n    \"\"\"\n\n    def __init__(self, stream, exception_type, num_reads=0):\n        self._stream = stream\n        self._exception_type = exception_type\n        self._num_reads = num_reads\n        self._count = 0\n\n    def read(self, n=-1):\n        if self._count == self._num_reads:\n            raise self._exception_type\n        self._count += 1\n        return self._stream.read(n)\n\n\nclass FileSizeProvider:\n    def __init__(self, file_size):\n        self.file_size = file_size\n\n    def on_queued(self, future, **kwargs):\n        future.meta.provide_transfer_size(self.file_size)\n\n\nclass FileCreator:\n    def __init__(self):\n        self.rootdir = tempfile.mkdtemp()\n\n    def remove_all(self):\n        shutil.rmtree(self.rootdir)\n\n    def create_file(self, filename, contents, mode='w'):\n        \"\"\"Creates a file in a tmpdir\n        ``filename`` should be a relative path, e.g. \"foo/bar/baz.txt\"\n        It will be translated into a full path in a tmp dir.\n        ``mode`` is the mode the file should be opened either as ``w`` or\n        `wb``.\n        Returns the full path to the file.\n        \"\"\"\n        full_path = os.path.join(self.rootdir, filename)\n        if not os.path.isdir(os.path.dirname(full_path)):\n            os.makedirs(os.path.dirname(full_path))\n        with open(full_path, mode) as f:\n            f.write(contents)\n        return full_path\n\n    def create_file_with_size(self, filename, filesize):\n        filename = self.create_file(filename, contents='')\n        chunksize = 8192\n        with open(filename, 'wb') as f:\n            for i in range(int(math.ceil(filesize / float(chunksize)))):\n                f.write(b'a' * chunksize)\n        return filename\n\n    def append_file(self, filename, contents):\n        \"\"\"Append contents to a file\n        ``filename`` should be a relative path, e.g. \"foo/bar/baz.txt\"\n        It will be translated into a full path in a tmp dir.\n        Returns the full path to the file.\n        \"\"\"\n        full_path = os.path.join(self.rootdir, filename)\n        if not os.path.isdir(os.path.dirname(full_path)):\n            os.makedirs(os.path.dirname(full_path))\n        with open(full_path, 'a') as f:\n            f.write(contents)\n        return full_path\n\n    def full_path(self, filename):\n        \"\"\"Translate relative path to full path in temp dir.\n        f.full_path('foo/bar.txt') -> /tmp/asdfasd/foo/bar.txt\n        \"\"\"\n        return os.path.join(self.rootdir, filename)\n\n\nclass RecordingOSUtils(OSUtils):\n    \"\"\"An OSUtil abstraction that records openings and renamings\"\"\"\n\n    def __init__(self):\n        super().__init__()\n        self.open_records = []\n        self.rename_records = []\n\n    def open(self, filename, mode):\n        self.open_records.append((filename, mode))\n        return super().open(filename, mode)\n\n    def rename_file(self, current_filename, new_filename):\n        self.rename_records.append((current_filename, new_filename))\n        super().rename_file(current_filename, new_filename)\n\n\nclass RecordingSubscriber(BaseSubscriber):\n    def __init__(self):\n        self.on_queued_calls = []\n        self.on_progress_calls = []\n        self.on_done_calls = []\n\n    def on_queued(self, **kwargs):\n        self.on_queued_calls.append(kwargs)\n\n    def on_progress(self, **kwargs):\n        self.on_progress_calls.append(kwargs)\n\n    def on_done(self, **kwargs):\n        self.on_done_calls.append(kwargs)\n\n    def calculate_bytes_seen(self, **kwargs):\n        amount_seen = 0\n        for call in self.on_progress_calls:\n            amount_seen += call['bytes_transferred']\n        return amount_seen\n\n\nclass TransferCoordinatorWithInterrupt(TransferCoordinator):\n    \"\"\"Used to inject keyboard interrupts\"\"\"\n\n    def result(self):\n        raise KeyboardInterrupt()\n\n\nclass RecordingExecutor:\n    \"\"\"A wrapper on an executor to record calls made to submit()\n\n    You can access the submissions property to receive a list of dictionaries\n    that represents all submissions where the dictionary is formatted::\n\n        {\n            'fn': function\n            'args': positional args (as tuple)\n            'kwargs': keyword args (as dict)\n        }\n    \"\"\"\n\n    def __init__(self, executor):\n        self._executor = executor\n        self.submissions = []\n\n    def submit(self, task, tag=None, block=True):\n        future = self._executor.submit(task, tag, block)\n        self.submissions.append({'task': task, 'tag': tag, 'block': block})\n        return future\n\n    def shutdown(self):\n        self._executor.shutdown()\n\n\nclass StubbedClientTest(unittest.TestCase):\n    def setUp(self):\n        self.session = botocore.session.get_session()\n        self.region = 'us-west-2'\n        self.client = self.session.create_client(\n            's3',\n            self.region,\n            aws_access_key_id='foo',\n            aws_secret_access_key='bar',\n        )\n        self.stubber = Stubber(self.client)\n        self.stubber.activate()\n\n    def tearDown(self):\n        self.stubber.deactivate()\n\n    def reset_stubber_with_new_client(self, override_client_kwargs):\n        client_kwargs = {\n            'service_name': 's3',\n            'region_name': self.region,\n            'aws_access_key_id': 'foo',\n            'aws_secret_access_key': 'bar',\n        }\n        client_kwargs.update(override_client_kwargs)\n        self.client = self.session.create_client(**client_kwargs)\n        self.stubber = Stubber(self.client)\n        self.stubber.activate()\n\n\nclass BaseTaskTest(StubbedClientTest):\n    def setUp(self):\n        super().setUp()\n        self.transfer_coordinator = TransferCoordinator()\n\n    def get_task(self, task_cls, **kwargs):\n        if 'transfer_coordinator' not in kwargs:\n            kwargs['transfer_coordinator'] = self.transfer_coordinator\n        return task_cls(**kwargs)\n\n    def get_transfer_future(self, call_args=None):\n        return TransferFuture(\n            meta=TransferMeta(call_args), coordinator=self.transfer_coordinator\n        )\n\n\nclass BaseSubmissionTaskTest(BaseTaskTest):\n    def setUp(self):\n        super().setUp()\n        self.config = TransferConfig()\n        self.osutil = OSUtils()\n        self.executor = BoundedExecutor(\n            1000,\n            1,\n            {\n                IN_MEMORY_UPLOAD_TAG: TaskSemaphore(10),\n                IN_MEMORY_DOWNLOAD_TAG: SlidingWindowSemaphore(10),\n            },\n        )\n\n    def tearDown(self):\n        super().tearDown()\n        self.executor.shutdown()\n\n\nclass BaseGeneralInterfaceTest(StubbedClientTest):\n    \"\"\"A general test class to ensure consistency across TransferManger methods\n\n    This test should never be called and should be subclassed from to pick up\n    the various tests that all TransferManager method must pass from a\n    functionality standpoint.\n    \"\"\"\n\n    __test__ = False\n\n    def manager(self):\n        \"\"\"The transfer manager to use\"\"\"\n        raise NotImplementedError('method is not implemented')\n\n    @property\n    def method(self):\n        \"\"\"The transfer manager method to invoke i.e. upload()\"\"\"\n        raise NotImplementedError('method is not implemented')\n\n    def create_call_kwargs(self):\n        \"\"\"The kwargs to be passed to the transfer manager method\"\"\"\n        raise NotImplementedError('create_call_kwargs is not implemented')\n\n    def create_invalid_extra_args(self):\n        \"\"\"A value for extra_args that will cause validation errors\"\"\"\n        raise NotImplementedError(\n            'create_invalid_extra_args is not implemented'\n        )\n\n    def create_stubbed_responses(self):\n        \"\"\"A list of stubbed responses that will cause the request to succeed\n\n        The elements of this list is a dictionary that will be used as key\n        word arguments to botocore.Stubber.add_response(). For example::\n\n            [{'method': 'put_object', 'service_response': {}}]\n        \"\"\"\n        raise NotImplementedError(\n            'create_stubbed_responses is not implemented'\n        )\n\n    def create_expected_progress_callback_info(self):\n        \"\"\"A list of kwargs expected to be passed to each progress callback\n\n        Note that the future kwargs does not need to be added to each\n        dictionary provided in the list. This is injected for you. An example\n        is::\n\n            [\n                {'bytes_transferred': 4},\n                {'bytes_transferred': 4},\n                {'bytes_transferred': 2}\n            ]\n\n        This indicates that the progress callback will be called three\n        times and pass along the specified keyword arguments and corresponding\n        values.\n        \"\"\"\n        raise NotImplementedError(\n            'create_expected_progress_callback_info is not implemented'\n        )\n\n    def _setup_default_stubbed_responses(self):\n        for stubbed_response in self.create_stubbed_responses():\n            self.stubber.add_response(**stubbed_response)\n\n    def test_returns_future_with_meta(self):\n        self._setup_default_stubbed_responses()\n        future = self.method(**self.create_call_kwargs())\n        # The result is called so we ensure that the entire process executes\n        # before we try to clean up resources in the tearDown.\n        future.result()\n\n        # Assert the return value is a future with metadata associated to it.\n        self.assertIsInstance(future, TransferFuture)\n        self.assertIsInstance(future.meta, TransferMeta)\n\n    def test_returns_correct_call_args(self):\n        self._setup_default_stubbed_responses()\n        call_kwargs = self.create_call_kwargs()\n        future = self.method(**call_kwargs)\n        # The result is called so we ensure that the entire process executes\n        # before we try to clean up resources in the tearDown.\n        future.result()\n\n        # Assert that there are call args associated to the metadata\n        self.assertIsInstance(future.meta.call_args, CallArgs)\n        # Assert that all of the arguments passed to the method exist and\n        # are of the correct value in call_args.\n        for param, value in call_kwargs.items():\n            self.assertEqual(value, getattr(future.meta.call_args, param))\n\n    def test_has_transfer_id_associated_to_future(self):\n        self._setup_default_stubbed_responses()\n        call_kwargs = self.create_call_kwargs()\n        future = self.method(**call_kwargs)\n        # The result is called so we ensure that the entire process executes\n        # before we try to clean up resources in the tearDown.\n        future.result()\n\n        # Assert that an transfer id was associated to the future.\n        # Since there is only one transfer request is made for that transfer\n        # manager the id will be zero since it will be the first transfer\n        # request made for that transfer manager.\n        self.assertEqual(future.meta.transfer_id, 0)\n\n        # If we make a second request, the transfer id should have incremented\n        # by one for that new TransferFuture.\n        self._setup_default_stubbed_responses()\n        future = self.method(**call_kwargs)\n        future.result()\n        self.assertEqual(future.meta.transfer_id, 1)\n\n    def test_invalid_extra_args(self):\n        with self.assertRaisesRegex(ValueError, 'Invalid extra_args'):\n            self.method(\n                extra_args=self.create_invalid_extra_args(),\n                **self.create_call_kwargs(),\n            )\n\n    def test_for_callback_kwargs_correctness(self):\n        # Add the stubbed responses before invoking the method\n        self._setup_default_stubbed_responses()\n\n        subscriber = RecordingSubscriber()\n        future = self.method(\n            subscribers=[subscriber], **self.create_call_kwargs()\n        )\n        # We call shutdown instead of result on future because the future\n        # could be finished but the done callback could still be going.\n        # The manager's shutdown method ensures everything completes.\n        self.manager.shutdown()\n\n        # Assert the various subscribers were called with the\n        # expected kwargs\n        expected_progress_calls = self.create_expected_progress_callback_info()\n        for expected_progress_call in expected_progress_calls:\n            expected_progress_call['future'] = future\n\n        self.assertEqual(subscriber.on_queued_calls, [{'future': future}])\n        self.assertEqual(subscriber.on_progress_calls, expected_progress_calls)\n        self.assertEqual(subscriber.on_done_calls, [{'future': future}])\n\n\nclass NonSeekableReader(io.RawIOBase):\n    def __init__(self, b=b''):\n        super().__init__()\n        self._data = io.BytesIO(b)\n\n    def seekable(self):\n        return False\n\n    def writable(self):\n        return False\n\n    def readable(self):\n        return True\n\n    def write(self, b):\n        # This is needed because python will not always return the correct\n        # kind of error even though writeable returns False.\n        raise io.UnsupportedOperation(\"write\")\n\n    def read(self, n=-1):\n        return self._data.read(n)\n\n    def readinto(self, b):\n        return self._data.readinto(b)\n\n\nclass NonSeekableWriter(io.RawIOBase):\n    def __init__(self, fileobj):\n        super().__init__()\n        self._fileobj = fileobj\n\n    def seekable(self):\n        return False\n\n    def writable(self):\n        return True\n\n    def readable(self):\n        return False\n\n    def write(self, b):\n        self._fileobj.write(b)\n\n    def read(self, n=-1):\n        raise io.UnsupportedOperation(\"read\")\n", "tests/integration/test_delete.py": "# Copyright 2016 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nfrom tests.integration import BaseTransferManagerIntegTest\n\n\nclass TestDeleteObject(BaseTransferManagerIntegTest):\n    def test_can_delete_object(self):\n        key_name = 'mykey'\n        self.client.put_object(\n            Bucket=self.bucket_name, Key=key_name, Body=b'hello world'\n        )\n        self.assertTrue(self.object_exists(key_name))\n\n        transfer_manager = self.create_transfer_manager()\n        future = transfer_manager.delete(bucket=self.bucket_name, key=key_name)\n        future.result()\n\n        self.assertTrue(self.object_not_exists(key_name))\n", "tests/integration/test_s3transfer.py": "# Copyright 2016 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport hashlib\nimport math\nimport os\nimport shutil\nimport string\nimport tempfile\nimport threading\n\nfrom botocore.client import Config\n\nimport s3transfer\nfrom tests.integration import BaseTransferManagerIntegTest\n\n\ndef assert_files_equal(first, second):\n    if os.path.getsize(first) != os.path.getsize(second):\n        raise AssertionError(f\"Files are not equal: {first}, {second}\")\n    first_md5 = md5_checksum(first)\n    second_md5 = md5_checksum(second)\n    if first_md5 != second_md5:\n        raise AssertionError(\n            \"Files are not equal: {}(md5={}) != {}(md5={})\".format(\n                first, first_md5, second, second_md5\n            )\n        )\n\n\ndef md5_checksum(filename):\n    checksum = hashlib.md5()\n    with open(filename, 'rb') as f:\n        for chunk in iter(lambda: f.read(8192), b''):\n            checksum.update(chunk)\n    return checksum.hexdigest()\n\n\ndef random_bucket_name(prefix='boto3-transfer', num_chars=10):\n    base = string.ascii_lowercase + string.digits\n    random_bytes = bytearray(os.urandom(num_chars))\n    return prefix + ''.join([base[b % len(base)] for b in random_bytes])\n\n\nclass FileCreator:\n    def __init__(self):\n        self.rootdir = tempfile.mkdtemp()\n\n    def remove_all(self):\n        shutil.rmtree(self.rootdir)\n\n    def create_file(self, filename, contents, mode='w'):\n        \"\"\"Creates a file in a tmpdir\n        ``filename`` should be a relative path, e.g. \"foo/bar/baz.txt\"\n        It will be translated into a full path in a tmp dir.\n        ``mode`` is the mode the file should be opened either as ``w`` or\n        `wb``.\n        Returns the full path to the file.\n        \"\"\"\n        full_path = os.path.join(self.rootdir, filename)\n        if not os.path.isdir(os.path.dirname(full_path)):\n            os.makedirs(os.path.dirname(full_path))\n        with open(full_path, mode) as f:\n            f.write(contents)\n        return full_path\n\n    def create_file_with_size(self, filename, filesize):\n        filename = self.create_file(filename, contents='')\n        chunksize = 8192\n        with open(filename, 'wb') as f:\n            for i in range(int(math.ceil(filesize / float(chunksize)))):\n                f.write(b'a' * chunksize)\n        return filename\n\n    def append_file(self, filename, contents):\n        \"\"\"Append contents to a file\n        ``filename`` should be a relative path, e.g. \"foo/bar/baz.txt\"\n        It will be translated into a full path in a tmp dir.\n        Returns the full path to the file.\n        \"\"\"\n        full_path = os.path.join(self.rootdir, filename)\n        if not os.path.isdir(os.path.dirname(full_path)):\n            os.makedirs(os.path.dirname(full_path))\n        with open(full_path, 'a') as f:\n            f.write(contents)\n        return full_path\n\n    def full_path(self, filename):\n        \"\"\"Translate relative path to full path in temp dir.\n        f.full_path('foo/bar.txt') -> /tmp/asdfasd/foo/bar.txt\n        \"\"\"\n        return os.path.join(self.rootdir, filename)\n\n\nclass TestS3Transfers(BaseTransferManagerIntegTest):\n    \"\"\"Tests for the high level s3transfer module.\"\"\"\n\n    def create_s3_transfer(self, config=None):\n        return s3transfer.S3Transfer(self.client, config=config)\n\n    def assert_has_public_read_acl(self, response):\n        grants = response['Grants']\n        public_read = [\n            g['Grantee'].get('URI', '')\n            for g in grants\n            if g['Permission'] == 'READ'\n        ]\n        self.assertIn('groups/global/AllUsers', public_read[0])\n\n    def test_upload_below_threshold(self):\n        config = s3transfer.TransferConfig(multipart_threshold=2 * 1024 * 1024)\n        transfer = self.create_s3_transfer(config)\n        filename = self.files.create_file_with_size(\n            'foo.txt', filesize=1024 * 1024\n        )\n        transfer.upload_file(filename, self.bucket_name, 'foo.txt')\n        self.addCleanup(self.delete_object, 'foo.txt')\n\n        self.assertTrue(self.object_exists('foo.txt'))\n\n    def test_upload_above_threshold(self):\n        config = s3transfer.TransferConfig(multipart_threshold=2 * 1024 * 1024)\n        transfer = self.create_s3_transfer(config)\n        filename = self.files.create_file_with_size(\n            '20mb.txt', filesize=20 * 1024 * 1024\n        )\n        transfer.upload_file(filename, self.bucket_name, '20mb.txt')\n        self.addCleanup(self.delete_object, '20mb.txt')\n        self.assertTrue(self.object_exists('20mb.txt'))\n\n    def test_upload_file_above_threshold_with_acl(self):\n        config = s3transfer.TransferConfig(multipart_threshold=5 * 1024 * 1024)\n        transfer = self.create_s3_transfer(config)\n        filename = self.files.create_file_with_size(\n            '6mb.txt', filesize=6 * 1024 * 1024\n        )\n        extra_args = {'ACL': 'public-read'}\n        transfer.upload_file(\n            filename, self.bucket_name, '6mb.txt', extra_args=extra_args\n        )\n        self.addCleanup(self.delete_object, '6mb.txt')\n\n        self.assertTrue(self.object_exists('6mb.txt'))\n        response = self.client.get_object_acl(\n            Bucket=self.bucket_name, Key='6mb.txt'\n        )\n        self.assert_has_public_read_acl(response)\n\n    def test_upload_file_above_threshold_with_ssec(self):\n        key_bytes = os.urandom(32)\n        extra_args = {\n            'SSECustomerKey': key_bytes,\n            'SSECustomerAlgorithm': 'AES256',\n        }\n        config = s3transfer.TransferConfig(multipart_threshold=5 * 1024 * 1024)\n        transfer = self.create_s3_transfer(config)\n        filename = self.files.create_file_with_size(\n            '6mb.txt', filesize=6 * 1024 * 1024\n        )\n        transfer.upload_file(\n            filename, self.bucket_name, '6mb.txt', extra_args=extra_args\n        )\n        self.addCleanup(self.delete_object, '6mb.txt')\n        self.wait_object_exists('6mb.txt', extra_args)\n        # A head object will fail if it has a customer key\n        # associated with it and it's not provided in the HeadObject\n        # request so we can use this to verify our functionality.\n        response = self.client.head_object(\n            Bucket=self.bucket_name, Key='6mb.txt', **extra_args\n        )\n        self.assertEqual(response['SSECustomerAlgorithm'], 'AES256')\n\n    def test_progress_callback_on_upload(self):\n        self.amount_seen = 0\n        lock = threading.Lock()\n\n        def progress_callback(amount):\n            with lock:\n                self.amount_seen += amount\n\n        transfer = self.create_s3_transfer()\n        filename = self.files.create_file_with_size(\n            '20mb.txt', filesize=20 * 1024 * 1024\n        )\n        transfer.upload_file(\n            filename, self.bucket_name, '20mb.txt', callback=progress_callback\n        )\n        self.addCleanup(self.delete_object, '20mb.txt')\n\n        # The callback should have been called enough times such that\n        # the total amount of bytes we've seen (via the \"amount\"\n        # arg to the callback function) should be the size\n        # of the file we uploaded.\n        self.assertEqual(self.amount_seen, 20 * 1024 * 1024)\n\n    def test_callback_called_once_with_sigv4(self):\n        # Verify #98, where the callback was being invoked\n        # twice when using signature version 4.\n        self.amount_seen = 0\n        lock = threading.Lock()\n\n        def progress_callback(amount):\n            with lock:\n                self.amount_seen += amount\n\n        client = self.session.create_client(\n            's3', self.region, config=Config(signature_version='s3v4')\n        )\n        transfer = s3transfer.S3Transfer(client)\n        filename = self.files.create_file_with_size(\n            '10mb.txt', filesize=10 * 1024 * 1024\n        )\n        transfer.upload_file(\n            filename, self.bucket_name, '10mb.txt', callback=progress_callback\n        )\n        self.addCleanup(self.delete_object, '10mb.txt')\n\n        self.assertEqual(self.amount_seen, 10 * 1024 * 1024)\n\n    def test_can_send_extra_params_on_upload(self):\n        transfer = self.create_s3_transfer()\n        filename = self.files.create_file_with_size('foo.txt', filesize=1024)\n        transfer.upload_file(\n            filename,\n            self.bucket_name,\n            'foo.txt',\n            extra_args={'ACL': 'public-read'},\n        )\n        self.addCleanup(self.delete_object, 'foo.txt')\n\n        self.wait_object_exists('foo.txt')\n        response = self.client.get_object_acl(\n            Bucket=self.bucket_name, Key='foo.txt'\n        )\n        self.assert_has_public_read_acl(response)\n\n    def test_can_configure_threshold(self):\n        config = s3transfer.TransferConfig(multipart_threshold=6 * 1024 * 1024)\n        transfer = self.create_s3_transfer(config)\n        filename = self.files.create_file_with_size(\n            'foo.txt', filesize=8 * 1024 * 1024\n        )\n        transfer.upload_file(filename, self.bucket_name, 'foo.txt')\n        self.addCleanup(self.delete_object, 'foo.txt')\n\n        self.assertTrue(self.object_exists('foo.txt'))\n\n    def test_can_send_extra_params_on_download(self):\n        # We're picking the customer provided sse feature\n        # of S3 to test the extra_args functionality of\n        # S3.\n        key_bytes = os.urandom(32)\n        extra_args = {\n            'SSECustomerKey': key_bytes,\n            'SSECustomerAlgorithm': 'AES256',\n        }\n        filename = self.files.create_file('foo.txt', 'hello world')\n        self.upload_file(filename, 'foo.txt', extra_args)\n        transfer = self.create_s3_transfer()\n\n        download_path = os.path.join(self.files.rootdir, 'downloaded.txt')\n        transfer.download_file(\n            self.bucket_name, 'foo.txt', download_path, extra_args=extra_args\n        )\n        with open(download_path, 'rb') as f:\n            self.assertEqual(f.read(), b'hello world')\n\n    def test_progress_callback_on_download(self):\n        self.amount_seen = 0\n        lock = threading.Lock()\n\n        def progress_callback(amount):\n            with lock:\n                self.amount_seen += amount\n\n        transfer = self.create_s3_transfer()\n        filename = self.files.create_file_with_size(\n            '20mb.txt', filesize=20 * 1024 * 1024\n        )\n        self.upload_file(filename, '20mb.txt')\n\n        download_path = os.path.join(self.files.rootdir, 'downloaded.txt')\n        transfer.download_file(\n            self.bucket_name,\n            '20mb.txt',\n            download_path,\n            callback=progress_callback,\n        )\n\n        self.assertEqual(self.amount_seen, 20 * 1024 * 1024)\n\n    def test_download_below_threshold(self):\n        transfer = self.create_s3_transfer()\n        filename = self.files.create_file_with_size(\n            'foo.txt', filesize=1024 * 1024\n        )\n        self.upload_file(filename, 'foo.txt')\n\n        download_path = os.path.join(self.files.rootdir, 'downloaded.txt')\n        transfer.download_file(self.bucket_name, 'foo.txt', download_path)\n        assert_files_equal(filename, download_path)\n\n    def test_download_above_threshold(self):\n        transfer = self.create_s3_transfer()\n        filename = self.files.create_file_with_size(\n            'foo.txt', filesize=20 * 1024 * 1024\n        )\n        self.upload_file(filename, 'foo.txt')\n\n        download_path = os.path.join(self.files.rootdir, 'downloaded.txt')\n        transfer.download_file(self.bucket_name, 'foo.txt', download_path)\n        assert_files_equal(filename, download_path)\n", "tests/integration/test_copy.py": "# Copyright 2016 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nfrom s3transfer.manager import TransferConfig\nfrom tests import RecordingSubscriber\nfrom tests.integration import BaseTransferManagerIntegTest\n\n\nclass TestCopy(BaseTransferManagerIntegTest):\n    def setUp(self):\n        super().setUp()\n        self.multipart_threshold = 5 * 1024 * 1024\n        self.config = TransferConfig(\n            multipart_threshold=self.multipart_threshold\n        )\n\n    def test_copy_below_threshold(self):\n        transfer_manager = self.create_transfer_manager(self.config)\n        key = '1mb.txt'\n        new_key = '1mb-copy.txt'\n\n        filename = self.files.create_file_with_size(key, filesize=1024 * 1024)\n        self.upload_file(filename, key)\n\n        future = transfer_manager.copy(\n            copy_source={'Bucket': self.bucket_name, 'Key': key},\n            bucket=self.bucket_name,\n            key=new_key,\n        )\n\n        future.result()\n        self.assertTrue(self.object_exists(new_key))\n\n    def test_copy_above_threshold(self):\n        transfer_manager = self.create_transfer_manager(self.config)\n        key = '20mb.txt'\n        new_key = '20mb-copy.txt'\n\n        filename = self.files.create_file_with_size(\n            key, filesize=20 * 1024 * 1024\n        )\n        self.upload_file(filename, key)\n\n        future = transfer_manager.copy(\n            copy_source={'Bucket': self.bucket_name, 'Key': key},\n            bucket=self.bucket_name,\n            key=new_key,\n        )\n\n        future.result()\n        self.assertTrue(self.object_exists(new_key))\n\n    def test_progress_subscribers_on_copy(self):\n        subscriber = RecordingSubscriber()\n        transfer_manager = self.create_transfer_manager(self.config)\n        key = '20mb.txt'\n        new_key = '20mb-copy.txt'\n\n        filename = self.files.create_file_with_size(\n            key, filesize=20 * 1024 * 1024\n        )\n        self.upload_file(filename, key)\n\n        future = transfer_manager.copy(\n            copy_source={'Bucket': self.bucket_name, 'Key': key},\n            bucket=self.bucket_name,\n            key=new_key,\n            subscribers=[subscriber],\n        )\n\n        future.result()\n        # The callback should have been called enough times such that\n        # the total amount of bytes we've seen (via the \"amount\"\n        # arg to the callback function) should be the size\n        # of the file we uploaded.\n        self.assertEqual(subscriber.calculate_bytes_seen(), 20 * 1024 * 1024)\n", "tests/integration/test_processpool.py": "# Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport glob\nimport os\nimport time\n\nfrom s3transfer.processpool import ProcessPoolDownloader, ProcessTransferConfig\nfrom tests import assert_files_equal\nfrom tests.integration import BaseTransferManagerIntegTest\n\n\nclass TestProcessPoolDownloader(BaseTransferManagerIntegTest):\n    def setUp(self):\n        super().setUp()\n        self.multipart_threshold = 5 * 1024 * 1024\n        self.config = ProcessTransferConfig(\n            multipart_threshold=self.multipart_threshold\n        )\n        self.client_kwargs = {'region_name': self.region}\n\n    def create_process_pool_downloader(self, client_kwargs=None, config=None):\n        if client_kwargs is None:\n            client_kwargs = self.client_kwargs\n        if config is None:\n            config = self.config\n        return ProcessPoolDownloader(\n            client_kwargs=client_kwargs, config=config\n        )\n\n    def test_below_threshold(self):\n        downloader = self.create_process_pool_downloader()\n        filename = self.files.create_file_with_size(\n            'foo.txt', filesize=1024 * 1024\n        )\n        self.upload_file(filename, '1mb.txt')\n\n        download_path = os.path.join(self.files.rootdir, '1mb.txt')\n        with downloader:\n            downloader.download_file(\n                self.bucket_name, '1mb.txt', download_path\n            )\n        assert_files_equal(filename, download_path)\n\n    def test_above_threshold(self):\n        downloader = self.create_process_pool_downloader()\n        filename = self.files.create_file_with_size(\n            'foo.txt', filesize=20 * 1024 * 1024\n        )\n        self.upload_file(filename, '20mb.txt')\n\n        download_path = os.path.join(self.files.rootdir, '20mb.txt')\n        with downloader:\n            downloader.download_file(\n                self.bucket_name, '20mb.txt', download_path\n            )\n        assert_files_equal(filename, download_path)\n\n    def test_large_download_exits_quickly_on_exception(self):\n        downloader = self.create_process_pool_downloader()\n\n        filename = self.files.create_file_with_size(\n            'foo.txt', filesize=60 * 1024 * 1024\n        )\n        self.upload_file(filename, '60mb.txt')\n\n        download_path = os.path.join(self.files.rootdir, '60mb.txt')\n        sleep_time = 0.2\n        try:\n            with downloader:\n                downloader.download_file(\n                    self.bucket_name, '60mb.txt', download_path\n                )\n                # Sleep for a little to get the transfer process going\n                time.sleep(sleep_time)\n                # Raise an exception which should cause the preceding\n                # download to cancel and exit quickly\n                start_time = time.time()\n                raise KeyboardInterrupt()\n        except KeyboardInterrupt:\n            pass\n        end_time = time.time()\n        # The maximum time allowed for the transfer manager to exit.\n        # This means that it should take less than a couple second after\n        # sleeping to exit.\n        max_allowed_exit_time = 5\n        self.assertLess(\n            end_time - start_time,\n            max_allowed_exit_time,\n            \"Failed to exit under {}. Instead exited in {}.\".format(\n                max_allowed_exit_time, end_time - start_time\n            ),\n        )\n\n        # Make sure the actual file and the temporary do not exist\n        # by globbing for the file and any of its extensions\n        possible_matches = glob.glob('%s*' % download_path)\n        self.assertEqual(possible_matches, [])\n\n    def test_many_files_exits_quickly_on_exception(self):\n        downloader = self.create_process_pool_downloader()\n\n        filename = self.files.create_file_with_size(\n            '1mb.txt', filesize=1024 * 1024\n        )\n        self.upload_file(filename, '1mb.txt')\n\n        filenames = []\n        base_filename = os.path.join(self.files.rootdir, 'file')\n        for i in range(10):\n            filenames.append(base_filename + str(i))\n\n        try:\n            with downloader:\n                start_time = time.time()\n                for filename in filenames:\n                    downloader.download_file(\n                        self.bucket_name, '1mb.txt', filename\n                    )\n                # Raise an exception which should cause the preceding\n                # transfer to cancel and exit quickly\n                raise KeyboardInterrupt()\n        except KeyboardInterrupt:\n            pass\n        end_time = time.time()\n        # The maximum time allowed for the transfer manager to exit.\n        # This means that it should take less than a couple seconds to exit.\n        max_allowed_exit_time = 5\n        self.assertLess(\n            end_time - start_time,\n            max_allowed_exit_time,\n            \"Failed to exit under {}. Instead exited in {}.\".format(\n                max_allowed_exit_time, end_time - start_time\n            ),\n        )\n\n        # For the transfer that did get cancelled, make sure the temporary\n        # file got removed.\n        possible_matches = glob.glob('%s*' % base_filename)\n        self.assertEqual(possible_matches, [])\n", "tests/integration/__init__.py": "# Copyright 2016 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License'). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the 'license' file accompanying this file. This file is\n# distributed on an 'AS IS' BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport botocore\nimport botocore.session\nfrom botocore.exceptions import WaiterError\n\nfrom s3transfer.manager import TransferManager\nfrom s3transfer.subscribers import BaseSubscriber\nfrom tests import FileCreator, random_bucket_name, unittest\n\n\ndef recursive_delete(client, bucket_name):\n    # Ensure the bucket exists before attempting to wipe it out\n    exists_waiter = client.get_waiter('bucket_exists')\n    exists_waiter.wait(Bucket=bucket_name)\n    page = client.get_paginator('list_objects')\n    # Use pages paired with batch delete_objects().\n    for page in page.paginate(Bucket=bucket_name):\n        keys = [{'Key': obj['Key']} for obj in page.get('Contents', [])]\n        if keys:\n            client.delete_objects(Bucket=bucket_name, Delete={'Objects': keys})\n    for _ in range(5):\n        try:\n            client.delete_bucket(Bucket=bucket_name)\n            break\n        except client.exceptions.NoSuchBucket:\n            exists_waiter.wait(Bucket=bucket_name)\n        except Exception:\n            # We can sometimes get exceptions when trying to\n            # delete a bucket.  We'll let the waiter make\n            # the final call as to whether the bucket was able\n            # to be deleted.\n            not_exists_waiter = client.get_waiter('bucket_not_exists')\n            try:\n                not_exists_waiter.wait(Bucket=bucket_name)\n            except botocore.exceptions.WaiterError:\n                continue\n\n\nclass BaseTransferManagerIntegTest(unittest.TestCase):\n    \"\"\"Tests for the high level s3transfer module.\"\"\"\n\n    @classmethod\n    def setUpClass(cls):\n        cls.region = 'us-west-2'\n        cls.session = botocore.session.get_session()\n        cls.client = cls.session.create_client('s3', cls.region)\n        cls.bucket_name = random_bucket_name()\n        cls.client.create_bucket(\n            Bucket=cls.bucket_name,\n            CreateBucketConfiguration={'LocationConstraint': cls.region},\n            ObjectOwnership='ObjectWriter',\n        )\n        cls.client.delete_public_access_block(Bucket=cls.bucket_name)\n\n    def setUp(self):\n        self.files = FileCreator()\n\n    def tearDown(self):\n        self.files.remove_all()\n\n    @classmethod\n    def tearDownClass(cls):\n        recursive_delete(cls.client, cls.bucket_name)\n\n    def delete_object(self, key):\n        self.client.delete_object(Bucket=self.bucket_name, Key=key)\n\n    def object_exists(self, key, extra_args=None):\n        try:\n            self.wait_object_exists(key, extra_args)\n            return True\n        except WaiterError:\n            return False\n\n    def object_not_exists(self, key, extra_args=None):\n        if extra_args is None:\n            extra_args = {}\n        try:\n            self.client.get_waiter('object_not_exists').wait(\n                Bucket=self.bucket_name, Key=key, **extra_args\n            )\n            return True\n        except WaiterError:\n            return False\n\n    def wait_object_exists(self, key, extra_args=None):\n        if extra_args is None:\n            extra_args = {}\n        for _ in range(5):\n            self.client.get_waiter('object_exists').wait(\n                Bucket=self.bucket_name, Key=key, **extra_args\n            )\n\n    def create_transfer_manager(self, config=None):\n        return TransferManager(self.client, config=config)\n\n    def upload_file(self, filename, key, extra_args=None):\n        transfer = self.create_transfer_manager()\n        with open(filename, 'rb') as f:\n            transfer.upload(f, self.bucket_name, key, extra_args)\n            self.wait_object_exists(key, extra_args)\n            self.addCleanup(self.delete_object, key)\n\n\nclass WaitForTransferStart(BaseSubscriber):\n    def __init__(self, bytes_transfer_started_event):\n        self._bytes_transfer_started_event = bytes_transfer_started_event\n\n    def on_progress(self, **kwargs):\n        if not self._bytes_transfer_started_event.is_set():\n            self._bytes_transfer_started_event.set()\n", "tests/integration/test_upload.py": "# Copyright 2016 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport threading\nimport time\nfrom concurrent.futures import CancelledError\nfrom io import BytesIO\n\nfrom s3transfer.manager import TransferConfig\nfrom tests import (\n    NonSeekableReader,\n    RecordingSubscriber,\n    skip_if_using_serial_implementation,\n)\nfrom tests.integration import (\n    BaseTransferManagerIntegTest,\n    WaitForTransferStart,\n)\n\n\nclass TestUpload(BaseTransferManagerIntegTest):\n    def setUp(self):\n        super().setUp()\n        self.multipart_threshold = 5 * 1024 * 1024\n        self.config = TransferConfig(\n            multipart_threshold=self.multipart_threshold\n        )\n\n    def get_input_fileobj(self, size, name=''):\n        return self.files.create_file_with_size(name, size)\n\n    def test_upload_below_threshold(self):\n        transfer_manager = self.create_transfer_manager(self.config)\n        file = self.get_input_fileobj(size=1024 * 1024, name='1mb.txt')\n        future = transfer_manager.upload(file, self.bucket_name, '1mb.txt')\n        self.addCleanup(self.delete_object, '1mb.txt')\n\n        future.result()\n        self.assertTrue(self.object_exists('1mb.txt'))\n\n    def test_upload_above_threshold(self):\n        transfer_manager = self.create_transfer_manager(self.config)\n        file = self.get_input_fileobj(size=20 * 1024 * 1024, name='20mb.txt')\n        future = transfer_manager.upload(file, self.bucket_name, '20mb.txt')\n        self.addCleanup(self.delete_object, '20mb.txt')\n\n        future.result()\n        self.assertTrue(self.object_exists('20mb.txt'))\n\n    @skip_if_using_serial_implementation(\n        'Exception is thrown once the transfer is submitted. '\n        'However for the serial implementation, transfers are performed '\n        'in main thread meaning the transfer will complete before the '\n        'KeyboardInterrupt being thrown.'\n    )\n    def test_large_upload_exits_quicky_on_exception(self):\n        transfer_manager = self.create_transfer_manager(self.config)\n\n        filename = self.get_input_fileobj(\n            name='foo.txt', size=20 * 1024 * 1024\n        )\n\n        timeout = 10\n        bytes_transferring = threading.Event()\n        subscriber = WaitForTransferStart(bytes_transferring)\n        try:\n            with transfer_manager:\n                future = transfer_manager.upload(\n                    filename,\n                    self.bucket_name,\n                    '20mb.txt',\n                    subscribers=[subscriber],\n                )\n                if not bytes_transferring.wait(timeout):\n                    future.cancel()\n                    raise RuntimeError(\n                        \"Download transfer did not start after waiting for \"\n                        \"%s seconds.\" % timeout\n                    )\n                # Raise an exception which should cause the preceding\n                # download to cancel and exit quickly\n                start_time = time.time()\n                raise KeyboardInterrupt()\n        except KeyboardInterrupt:\n            pass\n        end_time = time.time()\n        # The maximum time allowed for the transfer manager to exit.\n        # This means that it should take less than a couple second after\n        # sleeping to exit.\n        max_allowed_exit_time = 5\n        actual_time_to_exit = end_time - start_time\n        self.assertLess(\n            actual_time_to_exit,\n            max_allowed_exit_time,\n            \"Failed to exit under {}. Instead exited in {}.\".format(\n                max_allowed_exit_time, actual_time_to_exit\n            ),\n        )\n\n        try:\n            future.result()\n            self.skipTest(\n                'Upload completed before interrupted and therefore '\n                'could not cancel the upload'\n            )\n        except CancelledError as e:\n            self.assertEqual(str(e), 'KeyboardInterrupt()')\n            # If the transfer did get cancelled,\n            # make sure the object does not exist.\n            self.assertTrue(self.object_not_exists('20mb.txt'))\n\n    @skip_if_using_serial_implementation(\n        'Exception is thrown once the transfers are submitted. '\n        'However for the serial implementation, transfers are performed '\n        'in main thread meaning the transfers will complete before the '\n        'KeyboardInterrupt being thrown.'\n    )\n    def test_many_files_exits_quicky_on_exception(self):\n        # Set the max request queue size and number of submission threads\n        # to something small to simulate having a large queue\n        # of transfer requests to complete and it is backed up.\n        self.config.max_request_queue_size = 1\n        self.config.max_submission_concurrency = 1\n        transfer_manager = self.create_transfer_manager(self.config)\n\n        fileobjs = []\n        keynames = []\n        futures = []\n        for i in range(10):\n            filename = 'file' + str(i)\n            keynames.append(filename)\n            fileobjs.append(\n                self.get_input_fileobj(name=filename, size=1024 * 1024)\n            )\n\n        try:\n            with transfer_manager:\n                for i, fileobj in enumerate(fileobjs):\n                    futures.append(\n                        transfer_manager.upload(\n                            fileobj, self.bucket_name, keynames[i]\n                        )\n                    )\n                # Raise an exception which should cause the preceding\n                # transfer to cancel and exit quickly\n                start_time = time.time()\n                raise KeyboardInterrupt()\n        except KeyboardInterrupt:\n            pass\n        end_time = time.time()\n        # The maximum time allowed for the transfer manager to exit.\n        # This means that it should take less than a couple seconds to exit.\n        max_allowed_exit_time = 5\n        self.assertLess(\n            end_time - start_time,\n            max_allowed_exit_time,\n            \"Failed to exit under {}. Instead exited in {}.\".format(\n                max_allowed_exit_time, end_time - start_time\n            ),\n        )\n\n        # Make sure at least one of the futures got cancelled\n        with self.assertRaisesRegex(CancelledError, 'KeyboardInterrupt()'):\n            for future in futures:\n                future.result()\n        # For the transfer that did get cancelled, make sure the object\n        # does not exist.\n        self.assertTrue(self.object_not_exists(future.meta.call_args.key))\n\n    def test_progress_subscribers_on_upload(self):\n        subscriber = RecordingSubscriber()\n        transfer_manager = self.create_transfer_manager(self.config)\n        file = self.get_input_fileobj(size=20 * 1024 * 1024, name='20mb.txt')\n        future = transfer_manager.upload(\n            file, self.bucket_name, '20mb.txt', subscribers=[subscriber]\n        )\n        self.addCleanup(self.delete_object, '20mb.txt')\n\n        future.result()\n        # The callback should have been called enough times such that\n        # the total amount of bytes we've seen (via the \"amount\"\n        # arg to the callback function) should be the size\n        # of the file we uploaded.\n        self.assertEqual(subscriber.calculate_bytes_seen(), 20 * 1024 * 1024)\n\n\nclass TestUploadSeekableStream(TestUpload):\n    def get_input_fileobj(self, size, name=''):\n        return BytesIO(b'0' * size)\n\n\nclass TestUploadNonSeekableStream(TestUpload):\n    def get_input_fileobj(self, size, name=''):\n        return NonSeekableReader(b'0' * size)\n", "tests/integration/test_crt.py": "# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport glob\nimport io\nimport os\nfrom uuid import uuid4\n\nfrom botocore.exceptions import ClientError\n\nfrom s3transfer.subscribers import BaseSubscriber\nfrom s3transfer.utils import OSUtils\nfrom tests import (\n    HAS_CRT,\n    NonSeekableReader,\n    NonSeekableWriter,\n    assert_files_equal,\n    requires_crt,\n)\nfrom tests.integration import BaseTransferManagerIntegTest\n\nif HAS_CRT:\n    from awscrt.exceptions import AwsCrtError\n\n    import s3transfer.crt\n\n\nclass RecordingSubscriber(BaseSubscriber):\n    def __init__(self):\n        self.on_queued_called = False\n        self.on_done_called = False\n        self.bytes_transferred = 0\n\n    def on_queued(self, **kwargs):\n        self.on_queued_called = True\n\n    def on_progress(self, future, bytes_transferred, **kwargs):\n        self.bytes_transferred += bytes_transferred\n\n    def on_done(self, **kwargs):\n        self.on_done_called = True\n\n\n@requires_crt\nclass TestCRTS3Transfers(BaseTransferManagerIntegTest):\n    \"\"\"Tests for the high level s3transfer based on CRT implementation.\"\"\"\n\n    def setUp(self):\n        super().setUp()\n        self.s3_key = 's3key.txt'\n        self.download_path = os.path.join(self.files.rootdir, 'download.txt')\n\n    def _create_s3_transfer(self):\n        self.request_serializer = s3transfer.crt.BotocoreCRTRequestSerializer(\n            self.session, client_kwargs={'region_name': self.region}\n        )\n        self.s3_crt_client = s3transfer.crt.create_s3_crt_client(\n            self.region, self._get_crt_credentials_provider()\n        )\n        self.record_subscriber = RecordingSubscriber()\n        self.osutil = OSUtils()\n        return s3transfer.crt.CRTTransferManager(\n            self.s3_crt_client, self.request_serializer\n        )\n\n    def _get_crt_credentials_provider(self):\n        botocore_credentials = self.session.get_credentials()\n        wrapper = s3transfer.crt.BotocoreCRTCredentialsWrapper(\n            botocore_credentials\n        )\n        return wrapper.to_crt_credentials_provider()\n\n    def _upload_with_crt_transfer_manager(self, fileobj, key=None):\n        if key is None:\n            key = self.s3_key\n        self.addCleanup(self.delete_object, key)\n        with self._create_s3_transfer() as transfer:\n            future = transfer.upload(\n                fileobj,\n                self.bucket_name,\n                key,\n                subscribers=[self.record_subscriber],\n            )\n            future.result()\n\n    def _download_with_crt_transfer_manager(self, fileobj, key=None):\n        if key is None:\n            key = self.s3_key\n        self.addCleanup(self.delete_object, key)\n        with self._create_s3_transfer() as transfer:\n            future = transfer.download(\n                self.bucket_name,\n                key,\n                fileobj,\n                subscribers=[self.record_subscriber],\n            )\n            future.result()\n\n    def _assert_expected_s3_object(self, key, expected_size=None):\n        self.assertTrue(self.object_exists(key))\n        if expected_size is not None:\n            response = self.client.head_object(\n                Bucket=self.bucket_name, Key=key\n            )\n            self.assertEqual(response['ContentLength'], expected_size)\n\n    def _assert_has_public_read_acl(self, response):\n        grants = response['Grants']\n        public_read = [\n            g['Grantee'].get('URI', '')\n            for g in grants\n            if g['Permission'] == 'READ'\n        ]\n        self.assertIn('groups/global/AllUsers', public_read[0])\n\n    def _assert_subscribers_called(self, expected_bytes_transferred=None):\n        self.assertTrue(self.record_subscriber.on_queued_called)\n        self.assertTrue(self.record_subscriber.on_done_called)\n        if expected_bytes_transferred:\n            self.assertEqual(\n                self.record_subscriber.bytes_transferred,\n                expected_bytes_transferred,\n            )\n\n    def test_upload_below_multipart_chunksize(self):\n        transfer = self._create_s3_transfer()\n        file_size = 1024 * 1024\n        filename = self.files.create_file_with_size(\n            'foo.txt', filesize=file_size\n        )\n        self.addCleanup(self.delete_object, 'foo.txt')\n\n        with transfer:\n            future = transfer.upload(\n                filename,\n                self.bucket_name,\n                'foo.txt',\n                subscribers=[self.record_subscriber],\n            )\n            future.result()\n\n        self.assertTrue(self.object_exists('foo.txt'))\n        self._assert_subscribers_called(file_size)\n\n    def test_upload_above_multipart_chunksize(self):\n        transfer = self._create_s3_transfer()\n        file_size = 20 * 1024 * 1024\n        filename = self.files.create_file_with_size(\n            '20mb.txt', filesize=file_size\n        )\n        self.addCleanup(self.delete_object, '20mb.txt')\n\n        with transfer:\n            future = transfer.upload(\n                filename,\n                self.bucket_name,\n                '20mb.txt',\n                subscribers=[self.record_subscriber],\n            )\n            future.result()\n        self.assertTrue(self.object_exists('20mb.txt'))\n        self._assert_subscribers_called(file_size)\n\n    def test_upload_file_above_threshold_with_acl(self):\n        transfer = self._create_s3_transfer()\n        file_size = 6 * 1024 * 1024\n        filename = self.files.create_file_with_size(\n            '6mb.txt', filesize=file_size\n        )\n        extra_args = {'ACL': 'public-read'}\n        self.addCleanup(self.delete_object, '6mb.txt')\n\n        with transfer:\n            future = transfer.upload(\n                filename,\n                self.bucket_name,\n                '6mb.txt',\n                extra_args=extra_args,\n                subscribers=[self.record_subscriber],\n            )\n            future.result()\n\n        self.assertTrue(self.object_exists('6mb.txt'))\n        response = self.client.get_object_acl(\n            Bucket=self.bucket_name, Key='6mb.txt'\n        )\n        self._assert_has_public_read_acl(response)\n        self._assert_subscribers_called(file_size)\n\n    def test_upload_file_above_threshold_with_ssec(self):\n        key_bytes = os.urandom(32)\n        extra_args = {\n            'SSECustomerKey': key_bytes,\n            'SSECustomerAlgorithm': 'AES256',\n        }\n        file_size = 6 * 1024 * 1024\n        transfer = self._create_s3_transfer()\n        filename = self.files.create_file_with_size(\n            '6mb.txt', filesize=file_size\n        )\n        self.addCleanup(self.delete_object, '6mb.txt')\n        with transfer:\n            future = transfer.upload(\n                filename,\n                self.bucket_name,\n                '6mb.txt',\n                extra_args=extra_args,\n                subscribers=[self.record_subscriber],\n            )\n            future.result()\n        # A head object will fail if it has a customer key\n        # associated with it and it's not provided in the HeadObject\n        # request so we can use this to verify our functionality.\n        oringal_extra_args = {\n            'SSECustomerKey': key_bytes,\n            'SSECustomerAlgorithm': 'AES256',\n        }\n        self.wait_object_exists('6mb.txt', oringal_extra_args)\n        response = self.client.head_object(\n            Bucket=self.bucket_name, Key='6mb.txt', **oringal_extra_args\n        )\n        self.assertEqual(response['SSECustomerAlgorithm'], 'AES256')\n        self._assert_subscribers_called(file_size)\n\n    def test_upload_seekable_stream(self):\n        size = 1024 * 1024\n        self._upload_with_crt_transfer_manager(io.BytesIO(b'0' * size))\n        self._assert_expected_s3_object(self.s3_key, expected_size=size)\n        self._assert_subscribers_called(size)\n\n    def test_multipart_upload_seekable_stream(self):\n        size = 20 * 1024 * 1024\n        self._upload_with_crt_transfer_manager(io.BytesIO(b'0' * size))\n        self._assert_expected_s3_object(self.s3_key, expected_size=size)\n        self._assert_subscribers_called(size)\n\n    def test_upload_nonseekable_stream(self):\n        size = 1024 * 1024\n        self._upload_with_crt_transfer_manager(NonSeekableReader(b'0' * size))\n        self._assert_expected_s3_object(self.s3_key, expected_size=size)\n        self._assert_subscribers_called(size)\n\n    def test_multipart_upload_nonseekable_stream(self):\n        size = 20 * 1024 * 1024\n        self._upload_with_crt_transfer_manager(NonSeekableReader(b'0' * size))\n        self._assert_expected_s3_object(self.s3_key, expected_size=size)\n        self._assert_subscribers_called(size)\n\n    def test_upload_empty_file(self):\n        size = 0\n        filename = self.files.create_file_with_size(self.s3_key, filesize=size)\n        self._upload_with_crt_transfer_manager(filename)\n        self._assert_expected_s3_object(self.s3_key, expected_size=size)\n        self._assert_subscribers_called(size)\n\n    def test_upload_empty_stream(self):\n        size = 0\n        self._upload_with_crt_transfer_manager(io.BytesIO(b''))\n        self._assert_expected_s3_object(self.s3_key, expected_size=size)\n        self._assert_subscribers_called(size)\n\n    def test_can_send_extra_params_on_download(self):\n        # We're picking the customer provided sse feature\n        # of S3 to test the extra_args functionality of\n        # S3.\n        key_bytes = os.urandom(32)\n        extra_args = {\n            'SSECustomerKey': key_bytes,\n            'SSECustomerAlgorithm': 'AES256',\n        }\n        filename = self.files.create_file('foo.txt', 'hello world')\n        self.upload_file(filename, 'foo.txt', extra_args)\n        transfer = self._create_s3_transfer()\n\n        download_path = os.path.join(self.files.rootdir, 'downloaded.txt')\n        with transfer:\n            future = transfer.download(\n                self.bucket_name,\n                'foo.txt',\n                download_path,\n                extra_args=extra_args,\n                subscribers=[self.record_subscriber],\n            )\n            future.result()\n        file_size = self.osutil.get_file_size(download_path)\n        self._assert_subscribers_called(file_size)\n        with open(download_path, 'rb') as f:\n            self.assertEqual(f.read(), b'hello world')\n\n    def test_download_below_threshold(self):\n        transfer = self._create_s3_transfer()\n        filename = self.files.create_file_with_size(\n            'foo.txt', filesize=1024 * 1024\n        )\n        self.upload_file(filename, 'foo.txt')\n\n        download_path = os.path.join(self.files.rootdir, 'downloaded.txt')\n        with transfer:\n            future = transfer.download(\n                self.bucket_name,\n                'foo.txt',\n                download_path,\n                subscribers=[self.record_subscriber],\n            )\n            future.result()\n        file_size = self.osutil.get_file_size(download_path)\n        self._assert_subscribers_called(file_size)\n        assert_files_equal(filename, download_path)\n\n    def test_download_above_threshold(self):\n        transfer = self._create_s3_transfer()\n        filename = self.files.create_file_with_size(\n            'foo.txt', filesize=20 * 1024 * 1024\n        )\n        self.upload_file(filename, 'foo.txt')\n\n        download_path = os.path.join(self.files.rootdir, 'downloaded.txt')\n        with transfer:\n            future = transfer.download(\n                self.bucket_name,\n                'foo.txt',\n                download_path,\n                subscribers=[self.record_subscriber],\n            )\n            future.result()\n        assert_files_equal(filename, download_path)\n        file_size = self.osutil.get_file_size(download_path)\n        self._assert_subscribers_called(file_size)\n\n    def test_download_seekable_stream(self):\n        size = 1024 * 1024\n        filename = self.files.create_file_with_size(self.s3_key, filesize=size)\n        self.upload_file(filename, self.s3_key)\n\n        with open(self.download_path, 'wb') as f:\n            self._download_with_crt_transfer_manager(f)\n        self._assert_subscribers_called(size)\n        assert_files_equal(filename, self.download_path)\n\n    def test_multipart_download_seekable_stream(self):\n        size = 20 * 1024 * 1024\n        filename = self.files.create_file_with_size(self.s3_key, filesize=size)\n        self.upload_file(filename, self.s3_key)\n\n        with open(self.download_path, 'wb') as f:\n            self._download_with_crt_transfer_manager(f)\n        self._assert_subscribers_called(size)\n        assert_files_equal(filename, self.download_path)\n\n    def test_download_nonseekable_stream(self):\n        size = 1024 * 1024\n        filename = self.files.create_file_with_size(self.s3_key, filesize=size)\n        self.upload_file(filename, self.s3_key)\n\n        with open(self.download_path, 'wb') as f:\n            self._download_with_crt_transfer_manager(NonSeekableWriter(f))\n        self._assert_subscribers_called(size)\n        assert_files_equal(filename, self.download_path)\n\n    def test_multipart_download_nonseekable_stream(self):\n        size = 20 * 1024 * 1024\n        filename = self.files.create_file_with_size(self.s3_key, filesize=size)\n        self.upload_file(filename, self.s3_key)\n\n        with open(self.download_path, 'wb') as f:\n            self._download_with_crt_transfer_manager(NonSeekableWriter(f))\n        self._assert_subscribers_called(size)\n        assert_files_equal(filename, self.download_path)\n\n    def test_download_empty_file(self):\n        size = 0\n        filename = self.files.create_file_with_size(self.s3_key, filesize=size)\n        self.upload_file(filename, self.s3_key)\n\n        self._download_with_crt_transfer_manager(self.download_path)\n        self._assert_subscribers_called(size)\n        assert_files_equal(filename, self.download_path)\n\n    def test_download_empty_stream(self):\n        size = 0\n        filename = self.files.create_file_with_size(self.s3_key, filesize=size)\n        self.upload_file(filename, self.s3_key)\n\n        with open(self.download_path, 'wb') as f:\n            self._download_with_crt_transfer_manager(f)\n        self._assert_subscribers_called(size)\n        assert_files_equal(filename, self.download_path)\n\n    def test_delete(self):\n        transfer = self._create_s3_transfer()\n        filename = self.files.create_file_with_size(\n            'foo.txt', filesize=1024 * 1024\n        )\n        self.upload_file(filename, 'foo.txt')\n\n        with transfer:\n            future = transfer.delete(self.bucket_name, 'foo.txt')\n            future.result()\n        self.assertTrue(self.object_not_exists('foo.txt'))\n\n    def test_delete_exception_no_such_bucket(self):\n        # delete() uses awscrt.s3.S3RequestType.DEFAULT under the hood.\n        # Test that CRT exceptions translate properly into the botocore exceptions.\n        transfer = self._create_s3_transfer()\n        with self.assertRaises(ClientError) as ctx:\n            future = transfer.delete(\n                f\"{self.bucket_name}-NONEXISTENT-{uuid4()}\", \"foo.txt\"\n            )\n            future.result()\n        self.assertEqual(ctx.exception.__class__.__name__, 'NoSuchBucket')\n\n    def test_many_files_download(self):\n        transfer = self._create_s3_transfer()\n\n        filename = self.files.create_file_with_size(\n            '1mb.txt', filesize=1024 * 1024\n        )\n        self.upload_file(filename, '1mb.txt')\n\n        filenames = []\n        base_filename = os.path.join(self.files.rootdir, 'file')\n        for i in range(10):\n            filenames.append(base_filename + str(i))\n\n        with transfer:\n            for filename in filenames:\n                transfer.download(self.bucket_name, '1mb.txt', filename)\n        for download_path in filenames:\n            assert_files_equal(filename, download_path)\n\n    def test_many_files_upload(self):\n        transfer = self._create_s3_transfer()\n        keys = []\n        filenames = []\n        base_key = 'foo'\n        sufix = '.txt'\n        for i in range(10):\n            key = base_key + str(i) + sufix\n            keys.append(key)\n            filename = self.files.create_file_with_size(\n                key, filesize=1024 * 1024\n            )\n            filenames.append(filename)\n            self.addCleanup(self.delete_object, key)\n        with transfer:\n            for filename, key in zip(filenames, keys):\n                transfer.upload(filename, self.bucket_name, key)\n\n        for key in keys:\n            self.assertTrue(self.object_exists(key))\n\n    def test_many_files_delete(self):\n        transfer = self._create_s3_transfer()\n        keys = []\n        base_key = 'foo'\n        sufix = '.txt'\n        filename = self.files.create_file_with_size(\n            '1mb.txt', filesize=1024 * 1024\n        )\n        for i in range(10):\n            key = base_key + str(i) + sufix\n            keys.append(key)\n            self.upload_file(filename, key)\n\n        with transfer:\n            for key in keys:\n                transfer.delete(self.bucket_name, key)\n        for key in keys:\n            self.assertTrue(self.object_not_exists(key))\n\n    def test_upload_cancel(self):\n        transfer = self._create_s3_transfer()\n        filename = self.files.create_file_with_size(\n            '20mb.txt', filesize=20 * 1024 * 1024\n        )\n        future = None\n        try:\n            with transfer:\n                future = transfer.upload(\n                    filename, self.bucket_name, '20mb.txt'\n                )\n                raise KeyboardInterrupt()\n        except KeyboardInterrupt:\n            pass\n\n        with self.assertRaises(AwsCrtError) as cm:\n            future.result()\n            self.assertEqual(cm.name, 'AWS_ERROR_S3_CANCELED')\n        self.assertTrue(self.object_not_exists('20mb.txt'))\n\n    def test_download_cancel(self):\n        transfer = self._create_s3_transfer()\n        filename = self.files.create_file_with_size(\n            'foo.txt', filesize=20 * 1024 * 1024\n        )\n        self.upload_file(filename, 'foo.txt')\n\n        download_path = os.path.join(self.files.rootdir, 'downloaded.txt')\n        future = None\n        try:\n            with transfer:\n                future = transfer.download(\n                    self.bucket_name,\n                    'foo.txt',\n                    download_path,\n                    subscribers=[self.record_subscriber],\n                )\n                raise KeyboardInterrupt()\n        except KeyboardInterrupt:\n            pass\n\n        with self.assertRaises(AwsCrtError) as err:\n            future.result()\n            self.assertEqual(err.name, 'AWS_ERROR_S3_CANCELED')\n\n        possible_matches = glob.glob('%s*' % download_path)\n        self.assertEqual(possible_matches, [])\n        self._assert_subscribers_called()\n", "tests/integration/test_download.py": "# Copyright 2016 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport glob\nimport os\nimport threading\nimport time\nfrom concurrent.futures import CancelledError\n\nfrom s3transfer.manager import TransferConfig\nfrom tests import (\n    NonSeekableWriter,\n    RecordingSubscriber,\n    assert_files_equal,\n    skip_if_using_serial_implementation,\n    skip_if_windows,\n)\nfrom tests.integration import (\n    BaseTransferManagerIntegTest,\n    WaitForTransferStart,\n)\n\n\nclass TestDownload(BaseTransferManagerIntegTest):\n    def setUp(self):\n        super().setUp()\n        self.multipart_threshold = 5 * 1024 * 1024\n        self.config = TransferConfig(\n            multipart_threshold=self.multipart_threshold\n        )\n\n    def test_below_threshold(self):\n        transfer_manager = self.create_transfer_manager(self.config)\n\n        filename = self.files.create_file_with_size(\n            'foo.txt', filesize=1024 * 1024\n        )\n        self.upload_file(filename, '1mb.txt')\n\n        download_path = os.path.join(self.files.rootdir, '1mb.txt')\n        future = transfer_manager.download(\n            self.bucket_name, '1mb.txt', download_path\n        )\n        future.result()\n        assert_files_equal(filename, download_path)\n\n    def test_above_threshold(self):\n        transfer_manager = self.create_transfer_manager(self.config)\n\n        filename = self.files.create_file_with_size(\n            'foo.txt', filesize=20 * 1024 * 1024\n        )\n        self.upload_file(filename, '20mb.txt')\n\n        download_path = os.path.join(self.files.rootdir, '20mb.txt')\n        future = transfer_manager.download(\n            self.bucket_name, '20mb.txt', download_path\n        )\n        future.result()\n        assert_files_equal(filename, download_path)\n\n    @skip_if_using_serial_implementation(\n        'Exception is thrown once the transfer is submitted. '\n        'However for the serial implementation, transfers are performed '\n        'in main thread meaning the transfer will complete before the '\n        'KeyboardInterrupt being thrown.'\n    )\n    def test_large_download_exits_quicky_on_exception(self):\n        transfer_manager = self.create_transfer_manager(self.config)\n\n        filename = self.files.create_file_with_size(\n            'foo.txt', filesize=60 * 1024 * 1024\n        )\n        self.upload_file(filename, '60mb.txt')\n\n        download_path = os.path.join(self.files.rootdir, '60mb.txt')\n        timeout = 10\n        bytes_transferring = threading.Event()\n        subscriber = WaitForTransferStart(bytes_transferring)\n        try:\n            with transfer_manager:\n                future = transfer_manager.download(\n                    self.bucket_name,\n                    '60mb.txt',\n                    download_path,\n                    subscribers=[subscriber],\n                )\n                if not bytes_transferring.wait(timeout):\n                    future.cancel()\n                    raise RuntimeError(\n                        \"Download transfer did not start after waiting for \"\n                        \"%s seconds.\" % timeout\n                    )\n                # Raise an exception which should cause the preceding\n                # download to cancel and exit quickly\n                start_time = time.time()\n                raise KeyboardInterrupt()\n        except KeyboardInterrupt:\n            pass\n        end_time = time.time()\n        # The maximum time allowed for the transfer manager to exit.\n        # This means that it should take less than a couple second after\n        # sleeping to exit.\n        max_allowed_exit_time = 5\n        actual_time_to_exit = end_time - start_time\n        self.assertLess(\n            actual_time_to_exit,\n            max_allowed_exit_time,\n            \"Failed to exit under {}. Instead exited in {}.\".format(\n                max_allowed_exit_time, actual_time_to_exit\n            ),\n        )\n\n        # Make sure the future was cancelled because of the KeyboardInterrupt\n        with self.assertRaisesRegex(CancelledError, 'KeyboardInterrupt()'):\n            future.result()\n\n        # Make sure the actual file and the temporary do not exist\n        # by globbing for the file and any of its extensions\n        possible_matches = glob.glob('%s*' % download_path)\n        self.assertEqual(possible_matches, [])\n\n    @skip_if_using_serial_implementation(\n        'Exception is thrown once the transfer is submitted. '\n        'However for the serial implementation, transfers are performed '\n        'in main thread meaning the transfer will complete before the '\n        'KeyboardInterrupt being thrown.'\n    )\n    def test_many_files_exits_quicky_on_exception(self):\n        # Set the max request queue size and number of submission threads\n        # to something small to simulate having a large queue\n        # of transfer requests to complete and it is backed up.\n        self.config.max_request_queue_size = 1\n        self.config.max_submission_concurrency = 1\n        transfer_manager = self.create_transfer_manager(self.config)\n\n        filename = self.files.create_file_with_size(\n            'foo.txt', filesize=1024 * 1024\n        )\n        self.upload_file(filename, '1mb.txt')\n\n        filenames = []\n        futures = []\n        for i in range(10):\n            filenames.append(os.path.join(self.files.rootdir, 'file' + str(i)))\n\n        try:\n            with transfer_manager:\n                start_time = time.time()\n                for filename in filenames:\n                    futures.append(\n                        transfer_manager.download(\n                            self.bucket_name, '1mb.txt', filename\n                        )\n                    )\n                # Raise an exception which should cause the preceding\n                # transfer to cancel and exit quickly\n                raise KeyboardInterrupt()\n        except KeyboardInterrupt:\n            pass\n        end_time = time.time()\n        # The maximum time allowed for the transfer manager to exit.\n        # This means that it should take less than a couple seconds to exit.\n        max_allowed_exit_time = 5\n        self.assertLess(\n            end_time - start_time,\n            max_allowed_exit_time,\n            \"Failed to exit under {}. Instead exited in {}.\".format(\n                max_allowed_exit_time, end_time - start_time\n            ),\n        )\n\n        # Make sure at least one of the futures got cancelled\n        with self.assertRaisesRegex(CancelledError, 'KeyboardInterrupt()'):\n            for future in futures:\n                future.result()\n\n        # For the transfer that did get cancelled, make sure the temporary\n        # file got removed.\n        possible_matches = glob.glob('%s*' % future.meta.call_args.fileobj)\n        self.assertEqual(possible_matches, [])\n\n    def test_progress_subscribers_on_download(self):\n        subscriber = RecordingSubscriber()\n        transfer_manager = self.create_transfer_manager(self.config)\n\n        filename = self.files.create_file_with_size(\n            'foo.txt', filesize=20 * 1024 * 1024\n        )\n        self.upload_file(filename, '20mb.txt')\n\n        download_path = os.path.join(self.files.rootdir, '20mb.txt')\n\n        future = transfer_manager.download(\n            self.bucket_name,\n            '20mb.txt',\n            download_path,\n            subscribers=[subscriber],\n        )\n        future.result()\n        self.assertEqual(subscriber.calculate_bytes_seen(), 20 * 1024 * 1024)\n\n    def test_below_threshold_for_fileobj(self):\n        transfer_manager = self.create_transfer_manager(self.config)\n\n        filename = self.files.create_file_with_size(\n            'foo.txt', filesize=1024 * 1024\n        )\n        self.upload_file(filename, '1mb.txt')\n\n        download_path = os.path.join(self.files.rootdir, '1mb.txt')\n        with open(download_path, 'wb') as f:\n            future = transfer_manager.download(self.bucket_name, '1mb.txt', f)\n            future.result()\n        assert_files_equal(filename, download_path)\n\n    def test_above_threshold_for_fileobj(self):\n        transfer_manager = self.create_transfer_manager(self.config)\n\n        filename = self.files.create_file_with_size(\n            'foo.txt', filesize=20 * 1024 * 1024\n        )\n        self.upload_file(filename, '20mb.txt')\n\n        download_path = os.path.join(self.files.rootdir, '20mb.txt')\n        with open(download_path, 'wb') as f:\n            future = transfer_manager.download(self.bucket_name, '20mb.txt', f)\n            future.result()\n        assert_files_equal(filename, download_path)\n\n    def test_below_threshold_for_nonseekable_fileobj(self):\n        transfer_manager = self.create_transfer_manager(self.config)\n\n        filename = self.files.create_file_with_size(\n            'foo.txt', filesize=1024 * 1024\n        )\n        self.upload_file(filename, '1mb.txt')\n\n        download_path = os.path.join(self.files.rootdir, '1mb.txt')\n        with open(download_path, 'wb') as f:\n            future = transfer_manager.download(\n                self.bucket_name, '1mb.txt', NonSeekableWriter(f)\n            )\n            future.result()\n        assert_files_equal(filename, download_path)\n\n    def test_above_threshold_for_nonseekable_fileobj(self):\n        transfer_manager = self.create_transfer_manager(self.config)\n\n        filename = self.files.create_file_with_size(\n            'foo.txt', filesize=20 * 1024 * 1024\n        )\n        self.upload_file(filename, '20mb.txt')\n\n        download_path = os.path.join(self.files.rootdir, '20mb.txt')\n        with open(download_path, 'wb') as f:\n            future = transfer_manager.download(\n                self.bucket_name, '20mb.txt', NonSeekableWriter(f)\n            )\n            future.result()\n        assert_files_equal(filename, download_path)\n\n    @skip_if_windows('Windows does not support UNIX special files')\n    def test_download_to_special_file(self):\n        transfer_manager = self.create_transfer_manager(self.config)\n        filename = self.files.create_file_with_size(\n            'foo.txt', filesize=1024 * 1024\n        )\n        self.upload_file(filename, '1mb.txt')\n        future = transfer_manager.download(\n            self.bucket_name, '1mb.txt', '/dev/null'\n        )\n        try:\n            future.result()\n        except Exception as e:\n            self.fail(\n                'Should have been able to download to /dev/null but received '\n                'following exception %s' % e\n            )\n", "tests/functional/test_delete.py": "# Copyright 2016 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nfrom s3transfer.manager import TransferManager\nfrom tests import BaseGeneralInterfaceTest\n\n\nclass TestDeleteObject(BaseGeneralInterfaceTest):\n    __test__ = True\n\n    def setUp(self):\n        super().setUp()\n        self.bucket = 'mybucket'\n        self.key = 'mykey'\n        self.manager = TransferManager(self.client)\n\n    @property\n    def method(self):\n        \"\"\"The transfer manager method to invoke i.e. upload()\"\"\"\n        return self.manager.delete\n\n    def create_call_kwargs(self):\n        \"\"\"The kwargs to be passed to the transfer manager method\"\"\"\n        return {\n            'bucket': self.bucket,\n            'key': self.key,\n        }\n\n    def create_invalid_extra_args(self):\n        return {\n            'BadKwargs': True,\n        }\n\n    def create_stubbed_responses(self):\n        \"\"\"A list of stubbed responses that will cause the request to succeed\n\n        The elements of this list is a dictionary that will be used as key\n        word arguments to botocore.Stubber.add_response(). For example::\n\n            [{'method': 'put_object', 'service_response': {}}]\n        \"\"\"\n        return [\n            {\n                'method': 'delete_object',\n                'service_response': {},\n                'expected_params': {'Bucket': self.bucket, 'Key': self.key},\n            }\n        ]\n\n    def create_expected_progress_callback_info(self):\n        return []\n\n    def test_known_allowed_args_in_input_shape(self):\n        op_model = self.client.meta.service_model.operation_model(\n            'DeleteObject'\n        )\n        for allowed_arg in self.manager.ALLOWED_DELETE_ARGS:\n            self.assertIn(allowed_arg, op_model.input_shape.members)\n\n    def test_raise_exception_on_s3_object_lambda_resource(self):\n        s3_object_lambda_arn = (\n            'arn:aws:s3-object-lambda:us-west-2:123456789012:'\n            'accesspoint:my-accesspoint'\n        )\n        with self.assertRaisesRegex(ValueError, 'methods do not support'):\n            self.manager.delete(s3_object_lambda_arn, self.key)\n", "tests/functional/test_copy.py": "# Copyright 2016 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nfrom botocore.exceptions import ClientError\nfrom botocore.stub import Stubber\n\nfrom s3transfer.manager import TransferConfig, TransferManager\nfrom s3transfer.utils import MIN_UPLOAD_CHUNKSIZE\nfrom tests import BaseGeneralInterfaceTest, FileSizeProvider\n\n\nclass BaseCopyTest(BaseGeneralInterfaceTest):\n    def setUp(self):\n        super().setUp()\n        self.config = TransferConfig(\n            max_request_concurrency=1,\n            multipart_chunksize=MIN_UPLOAD_CHUNKSIZE,\n            multipart_threshold=MIN_UPLOAD_CHUNKSIZE * 4,\n        )\n        self._manager = TransferManager(self.client, self.config)\n\n        # Initialize some default arguments\n        self.bucket = 'mybucket'\n        self.key = 'mykey'\n        self.copy_source = {'Bucket': 'mysourcebucket', 'Key': 'mysourcekey'}\n        self.extra_args = {}\n        self.subscribers = []\n\n        self.half_chunksize = int(MIN_UPLOAD_CHUNKSIZE / 2)\n        self.content = b'0' * (2 * MIN_UPLOAD_CHUNKSIZE + self.half_chunksize)\n\n    @property\n    def manager(self):\n        return self._manager\n\n    @property\n    def method(self):\n        return self.manager.copy\n\n    def create_call_kwargs(self):\n        return {\n            'copy_source': self.copy_source,\n            'bucket': self.bucket,\n            'key': self.key,\n        }\n\n    def create_invalid_extra_args(self):\n        return {'Foo': 'bar'}\n\n    def create_stubbed_responses(self):\n        return [\n            {\n                'method': 'head_object',\n                'service_response': {'ContentLength': len(self.content)},\n            },\n            {'method': 'copy_object', 'service_response': {}},\n        ]\n\n    def create_expected_progress_callback_info(self):\n        return [\n            {'bytes_transferred': len(self.content)},\n        ]\n\n    def add_head_object_response(self, expected_params=None, stubber=None):\n        if not stubber:\n            stubber = self.stubber\n        head_response = self.create_stubbed_responses()[0]\n        if expected_params:\n            head_response['expected_params'] = expected_params\n        stubber.add_response(**head_response)\n\n    def add_successful_copy_responses(\n        self,\n        expected_copy_params=None,\n        expected_create_mpu_params=None,\n        expected_complete_mpu_params=None,\n    ):\n        # Add all responses needed to do the copy of the object.\n        # Should account for both ranged and nonranged downloads.\n        stubbed_responses = self.create_stubbed_responses()[1:]\n\n        # If the length of copy responses is greater than one then it is\n        # a multipart copy.\n        copy_responses = stubbed_responses[0:1]\n        if len(stubbed_responses) > 1:\n            copy_responses = stubbed_responses[1:-1]\n\n        # Add the expected create multipart upload params.\n        if expected_create_mpu_params:\n            stubbed_responses[0][\n                'expected_params'\n            ] = expected_create_mpu_params\n\n        # Add any expected copy parameters.\n        if expected_copy_params:\n            for i, copy_response in enumerate(copy_responses):\n                if isinstance(expected_copy_params, list):\n                    copy_response['expected_params'] = expected_copy_params[i]\n                else:\n                    copy_response['expected_params'] = expected_copy_params\n\n        # Add the expected complete multipart upload params.\n        if expected_complete_mpu_params:\n            stubbed_responses[-1][\n                'expected_params'\n            ] = expected_complete_mpu_params\n\n        # Add the responses to the stubber.\n        for stubbed_response in stubbed_responses:\n            self.stubber.add_response(**stubbed_response)\n\n    def test_can_provide_file_size(self):\n        self.add_successful_copy_responses()\n\n        call_kwargs = self.create_call_kwargs()\n        call_kwargs['subscribers'] = [FileSizeProvider(len(self.content))]\n\n        future = self.manager.copy(**call_kwargs)\n        future.result()\n\n        # The HeadObject should have not happened and should have been able\n        # to successfully copy the file.\n        self.stubber.assert_no_pending_responses()\n\n    def test_provide_copy_source_as_dict(self):\n        self.copy_source['VersionId'] = 'mysourceversionid'\n        expected_params = {\n            'Bucket': 'mysourcebucket',\n            'Key': 'mysourcekey',\n            'VersionId': 'mysourceversionid',\n        }\n\n        self.add_head_object_response(expected_params=expected_params)\n        self.add_successful_copy_responses()\n\n        future = self.manager.copy(**self.create_call_kwargs())\n        future.result()\n        self.stubber.assert_no_pending_responses()\n\n    def test_invalid_copy_source(self):\n        self.copy_source = ['bucket', 'key']\n        future = self.manager.copy(**self.create_call_kwargs())\n        with self.assertRaises(TypeError):\n            future.result()\n\n    def test_provide_copy_source_client(self):\n        source_client = self.session.create_client(\n            's3',\n            'eu-central-1',\n            aws_access_key_id='foo',\n            aws_secret_access_key='bar',\n        )\n        source_stubber = Stubber(source_client)\n        source_stubber.activate()\n        self.addCleanup(source_stubber.deactivate)\n\n        self.add_head_object_response(stubber=source_stubber)\n        self.add_successful_copy_responses()\n\n        call_kwargs = self.create_call_kwargs()\n        call_kwargs['source_client'] = source_client\n        future = self.manager.copy(**call_kwargs)\n        future.result()\n\n        # Make sure that all of the responses were properly\n        # used for both clients.\n        source_stubber.assert_no_pending_responses()\n        self.stubber.assert_no_pending_responses()\n\n\nclass TestNonMultipartCopy(BaseCopyTest):\n    __test__ = True\n\n    def test_copy(self):\n        expected_head_params = {\n            'Bucket': 'mysourcebucket',\n            'Key': 'mysourcekey',\n        }\n        expected_copy_object = {\n            'Bucket': self.bucket,\n            'Key': self.key,\n            'CopySource': self.copy_source,\n        }\n        self.add_head_object_response(expected_params=expected_head_params)\n        self.add_successful_copy_responses(\n            expected_copy_params=expected_copy_object\n        )\n\n        future = self.manager.copy(**self.create_call_kwargs())\n        future.result()\n        self.stubber.assert_no_pending_responses()\n\n    def test_copy_with_checksum(self):\n        self.extra_args['ChecksumAlgorithm'] = 'crc32'\n        expected_head_params = {\n            'Bucket': 'mysourcebucket',\n            'Key': 'mysourcekey',\n        }\n        expected_copy_object = {\n            'Bucket': self.bucket,\n            'Key': self.key,\n            'CopySource': self.copy_source,\n            'ChecksumAlgorithm': 'crc32',\n        }\n        self.add_head_object_response(expected_params=expected_head_params)\n        self.add_successful_copy_responses(\n            expected_copy_params=expected_copy_object\n        )\n\n        call_kwargs = self.create_call_kwargs()\n        call_kwargs['extra_args'] = self.extra_args\n        future = self.manager.copy(**call_kwargs)\n        future.result()\n        self.stubber.assert_no_pending_responses()\n\n    def test_copy_with_extra_args(self):\n        self.extra_args['MetadataDirective'] = 'REPLACE'\n\n        expected_head_params = {\n            'Bucket': 'mysourcebucket',\n            'Key': 'mysourcekey',\n        }\n        expected_copy_object = {\n            'Bucket': self.bucket,\n            'Key': self.key,\n            'CopySource': self.copy_source,\n            'MetadataDirective': 'REPLACE',\n        }\n\n        self.add_head_object_response(expected_params=expected_head_params)\n        self.add_successful_copy_responses(\n            expected_copy_params=expected_copy_object\n        )\n\n        call_kwargs = self.create_call_kwargs()\n        call_kwargs['extra_args'] = self.extra_args\n        future = self.manager.copy(**call_kwargs)\n        future.result()\n        self.stubber.assert_no_pending_responses()\n\n    def test_copy_maps_extra_args_to_head_object(self):\n        self.extra_args['CopySourceSSECustomerAlgorithm'] = 'AES256'\n\n        expected_head_params = {\n            'Bucket': 'mysourcebucket',\n            'Key': 'mysourcekey',\n            'SSECustomerAlgorithm': 'AES256',\n        }\n        expected_copy_object = {\n            'Bucket': self.bucket,\n            'Key': self.key,\n            'CopySource': self.copy_source,\n            'CopySourceSSECustomerAlgorithm': 'AES256',\n        }\n\n        self.add_head_object_response(expected_params=expected_head_params)\n        self.add_successful_copy_responses(\n            expected_copy_params=expected_copy_object\n        )\n\n        call_kwargs = self.create_call_kwargs()\n        call_kwargs['extra_args'] = self.extra_args\n        future = self.manager.copy(**call_kwargs)\n        future.result()\n        self.stubber.assert_no_pending_responses()\n\n    def test_allowed_copy_params_are_valid(self):\n        op_model = self.client.meta.service_model.operation_model('CopyObject')\n        for allowed_upload_arg in self._manager.ALLOWED_COPY_ARGS:\n            self.assertIn(allowed_upload_arg, op_model.input_shape.members)\n\n    def test_copy_with_tagging(self):\n        extra_args = {'Tagging': 'tag1=val1', 'TaggingDirective': 'REPLACE'}\n        self.add_head_object_response()\n        self.add_successful_copy_responses(\n            expected_copy_params={\n                'Bucket': self.bucket,\n                'Key': self.key,\n                'CopySource': self.copy_source,\n                'Tagging': 'tag1=val1',\n                'TaggingDirective': 'REPLACE',\n            }\n        )\n        future = self.manager.copy(\n            self.copy_source, self.bucket, self.key, extra_args\n        )\n        future.result()\n        self.stubber.assert_no_pending_responses()\n\n    def test_raise_exception_on_s3_object_lambda_resource(self):\n        s3_object_lambda_arn = (\n            'arn:aws:s3-object-lambda:us-west-2:123456789012:'\n            'accesspoint:my-accesspoint'\n        )\n        with self.assertRaisesRegex(ValueError, 'methods do not support'):\n            self.manager.copy(self.copy_source, s3_object_lambda_arn, self.key)\n\n    def test_raise_exception_on_s3_object_lambda_resource_as_source(self):\n        source = {\n            'Bucket': 'arn:aws:s3-object-lambda:us-west-2:123456789012:'\n            'accesspoint:my-accesspoint'\n        }\n        with self.assertRaisesRegex(ValueError, 'methods do not support'):\n            self.manager.copy(source, self.bucket, self.key)\n\n\nclass TestMultipartCopy(BaseCopyTest):\n    __test__ = True\n\n    def setUp(self):\n        super().setUp()\n        self.config = TransferConfig(\n            max_request_concurrency=1,\n            multipart_threshold=1,\n            multipart_chunksize=4,\n        )\n        self._manager = TransferManager(self.client, self.config)\n        self.multipart_id = 'my-upload-id'\n\n    def create_stubbed_responses(self):\n        return [\n            {\n                'method': 'head_object',\n                'service_response': {'ContentLength': len(self.content)},\n            },\n            {\n                'method': 'create_multipart_upload',\n                'service_response': {'UploadId': self.multipart_id},\n            },\n            {\n                'method': 'upload_part_copy',\n                'service_response': {'CopyPartResult': {'ETag': 'etag-1'}},\n            },\n            {\n                'method': 'upload_part_copy',\n                'service_response': {'CopyPartResult': {'ETag': 'etag-2'}},\n            },\n            {\n                'method': 'upload_part_copy',\n                'service_response': {'CopyPartResult': {'ETag': 'etag-3'}},\n            },\n            {'method': 'complete_multipart_upload', 'service_response': {}},\n        ]\n\n    def add_get_head_response_with_default_expected_params(\n        self, extra_expected_params=None\n    ):\n        expected_params = {\n            'Bucket': 'mysourcebucket',\n            'Key': 'mysourcekey',\n        }\n        if extra_expected_params:\n            expected_params.update(extra_expected_params)\n        response = self.create_stubbed_responses()[0]\n        response['expected_params'] = expected_params\n        self.stubber.add_response(**response)\n\n    def add_create_multipart_response_with_default_expected_params(\n        self, extra_expected_params=None\n    ):\n        expected_params = {'Bucket': self.bucket, 'Key': self.key}\n        if extra_expected_params:\n            expected_params.update(extra_expected_params)\n        response = self.create_stubbed_responses()[1]\n        response['expected_params'] = expected_params\n        self.stubber.add_response(**response)\n\n    def add_upload_part_copy_responses_with_default_expected_params(\n        self, extra_expected_params=None\n    ):\n        ranges = [\n            'bytes=0-5242879',\n            'bytes=5242880-10485759',\n            'bytes=10485760-13107199',\n        ]\n        upload_part_responses = self.create_stubbed_responses()[2:-1]\n        for i, range_val in enumerate(ranges):\n            upload_part_response = upload_part_responses[i]\n            expected_params = {\n                'Bucket': self.bucket,\n                'Key': self.key,\n                'CopySource': self.copy_source,\n                'UploadId': self.multipart_id,\n                'PartNumber': i + 1,\n                'CopySourceRange': range_val,\n            }\n            if extra_expected_params:\n                if 'ChecksumAlgorithm' in extra_expected_params:\n                    name = extra_expected_params['ChecksumAlgorithm']\n                    checksum_member = 'Checksum%s' % name.upper()\n                    response = upload_part_response['service_response']\n                    response['CopyPartResult'][checksum_member] = 'sum%s==' % (\n                        i + 1\n                    )\n                else:\n                    expected_params.update(extra_expected_params)\n\n            upload_part_response['expected_params'] = expected_params\n            self.stubber.add_response(**upload_part_response)\n\n    def add_complete_multipart_response_with_default_expected_params(\n        self, extra_expected_params=None\n    ):\n        expected_params = {\n            'Bucket': self.bucket,\n            'Key': self.key,\n            'UploadId': self.multipart_id,\n            'MultipartUpload': {\n                'Parts': [\n                    {'ETag': 'etag-1', 'PartNumber': 1},\n                    {'ETag': 'etag-2', 'PartNumber': 2},\n                    {'ETag': 'etag-3', 'PartNumber': 3},\n                ]\n            },\n        }\n        if extra_expected_params:\n            expected_params.update(extra_expected_params)\n\n        response = self.create_stubbed_responses()[-1]\n        response['expected_params'] = expected_params\n        self.stubber.add_response(**response)\n\n    def create_expected_progress_callback_info(self):\n        # Note that last read is from the empty sentinel indicating\n        # that the stream is done.\n        return [\n            {'bytes_transferred': MIN_UPLOAD_CHUNKSIZE},\n            {'bytes_transferred': MIN_UPLOAD_CHUNKSIZE},\n            {'bytes_transferred': self.half_chunksize},\n        ]\n\n    def add_create_multipart_upload_response(self):\n        self.stubber.add_response(**self.create_stubbed_responses()[1])\n\n    def _get_expected_params(self):\n        # Add expected parameters to the head object\n        expected_head_params = {\n            'Bucket': 'mysourcebucket',\n            'Key': 'mysourcekey',\n        }\n\n        # Add expected parameters for the create multipart\n        expected_create_mpu_params = {\n            'Bucket': self.bucket,\n            'Key': self.key,\n        }\n\n        expected_copy_params = []\n        # Add expected parameters to the copy part\n        ranges = [\n            'bytes=0-5242879',\n            'bytes=5242880-10485759',\n            'bytes=10485760-13107199',\n        ]\n        for i, range_val in enumerate(ranges):\n            expected_copy_params.append(\n                {\n                    'Bucket': self.bucket,\n                    'Key': self.key,\n                    'CopySource': self.copy_source,\n                    'UploadId': self.multipart_id,\n                    'PartNumber': i + 1,\n                    'CopySourceRange': range_val,\n                }\n            )\n\n        # Add expected parameters for the complete multipart\n        expected_complete_mpu_params = {\n            'Bucket': self.bucket,\n            'Key': self.key,\n            'UploadId': self.multipart_id,\n            'MultipartUpload': {\n                'Parts': [\n                    {'ETag': 'etag-1', 'PartNumber': 1},\n                    {'ETag': 'etag-2', 'PartNumber': 2},\n                    {'ETag': 'etag-3', 'PartNumber': 3},\n                ]\n            },\n        }\n\n        return expected_head_params, {\n            'expected_create_mpu_params': expected_create_mpu_params,\n            'expected_copy_params': expected_copy_params,\n            'expected_complete_mpu_params': expected_complete_mpu_params,\n        }\n\n    def _add_params_to_expected_params(\n        self, add_copy_kwargs, operation_types, new_params\n    ):\n        expected_params_to_update = []\n        for operation_type in operation_types:\n            add_copy_kwargs_key = 'expected_' + operation_type + '_params'\n            expected_params = add_copy_kwargs[add_copy_kwargs_key]\n            if isinstance(expected_params, list):\n                expected_params_to_update.extend(expected_params)\n            else:\n                expected_params_to_update.append(expected_params)\n\n        for expected_params in expected_params_to_update:\n            expected_params.update(new_params)\n\n    def test_copy(self):\n        head_params, add_copy_kwargs = self._get_expected_params()\n        self.add_head_object_response(expected_params=head_params)\n        self.add_successful_copy_responses(**add_copy_kwargs)\n\n        future = self.manager.copy(**self.create_call_kwargs())\n        future.result()\n        self.stubber.assert_no_pending_responses()\n\n    def test_copy_with_extra_args(self):\n        # This extra argument should be added to the head object,\n        # the create multipart upload, and upload part copy.\n        self.extra_args['RequestPayer'] = 'requester'\n\n        head_params, add_copy_kwargs = self._get_expected_params()\n        head_params.update(self.extra_args)\n        self.add_head_object_response(expected_params=head_params)\n\n        self._add_params_to_expected_params(\n            add_copy_kwargs,\n            ['create_mpu', 'copy', 'complete_mpu'],\n            self.extra_args,\n        )\n        self.add_successful_copy_responses(**add_copy_kwargs)\n\n        call_kwargs = self.create_call_kwargs()\n        call_kwargs['extra_args'] = self.extra_args\n        future = self.manager.copy(**call_kwargs)\n        future.result()\n        self.stubber.assert_no_pending_responses()\n\n    def test_copy_passes_checksums(self):\n        # This extra argument should be added to the head object,\n        # the create multipart upload, and upload part copy.\n        self.extra_args['ChecksumAlgorithm'] = 'sha256'\n\n        self.add_get_head_response_with_default_expected_params()\n\n        # ChecksumAlgorithm should be passed on the create_multipart call\n        self.add_create_multipart_response_with_default_expected_params(\n            self.extra_args,\n        )\n\n        # ChecksumAlgorithm should be passed to the upload_part_copy calls\n        self.add_upload_part_copy_responses_with_default_expected_params(\n            self.extra_args,\n        )\n\n        # The checksums should be used in the complete call like etags\n        self.add_complete_multipart_response_with_default_expected_params(\n            extra_expected_params={\n                'MultipartUpload': {\n                    'Parts': [\n                        {\n                            'ETag': 'etag-1',\n                            'PartNumber': 1,\n                            'ChecksumSHA256': 'sum1==',\n                        },\n                        {\n                            'ETag': 'etag-2',\n                            'PartNumber': 2,\n                            'ChecksumSHA256': 'sum2==',\n                        },\n                        {\n                            'ETag': 'etag-3',\n                            'PartNumber': 3,\n                            'ChecksumSHA256': 'sum3==',\n                        },\n                    ]\n                }\n            }\n        )\n\n        call_kwargs = self.create_call_kwargs()\n        call_kwargs['extra_args'] = self.extra_args\n        future = self.manager.copy(**call_kwargs)\n        future.result()\n        self.stubber.assert_no_pending_responses()\n\n    def test_copy_blacklists_args_to_create_multipart(self):\n        # This argument can never be used for multipart uploads\n        self.extra_args['MetadataDirective'] = 'COPY'\n\n        head_params, add_copy_kwargs = self._get_expected_params()\n        self.add_head_object_response(expected_params=head_params)\n        self.add_successful_copy_responses(**add_copy_kwargs)\n\n        call_kwargs = self.create_call_kwargs()\n        call_kwargs['extra_args'] = self.extra_args\n        future = self.manager.copy(**call_kwargs)\n        future.result()\n        self.stubber.assert_no_pending_responses()\n\n    def test_copy_args_to_only_create_multipart(self):\n        self.extra_args['ACL'] = 'private'\n\n        head_params, add_copy_kwargs = self._get_expected_params()\n        self.add_head_object_response(expected_params=head_params)\n\n        self._add_params_to_expected_params(\n            add_copy_kwargs, ['create_mpu'], self.extra_args\n        )\n        self.add_successful_copy_responses(**add_copy_kwargs)\n\n        call_kwargs = self.create_call_kwargs()\n        call_kwargs['extra_args'] = self.extra_args\n        future = self.manager.copy(**call_kwargs)\n        future.result()\n        self.stubber.assert_no_pending_responses()\n\n    def test_copy_passes_args_to_create_multipart_and_upload_part(self):\n        # This will only be used for the complete multipart upload\n        # and upload part.\n        self.extra_args['SSECustomerAlgorithm'] = 'AES256'\n\n        head_params, add_copy_kwargs = self._get_expected_params()\n        self.add_head_object_response(expected_params=head_params)\n\n        self._add_params_to_expected_params(\n            add_copy_kwargs,\n            ['create_mpu', 'copy', 'complete_mpu'],\n            self.extra_args,\n        )\n        self.add_successful_copy_responses(**add_copy_kwargs)\n\n        call_kwargs = self.create_call_kwargs()\n        call_kwargs['extra_args'] = self.extra_args\n        future = self.manager.copy(**call_kwargs)\n        future.result()\n        self.stubber.assert_no_pending_responses()\n\n    def test_copy_maps_extra_args_to_head_object(self):\n        self.extra_args['CopySourceSSECustomerAlgorithm'] = 'AES256'\n\n        head_params, add_copy_kwargs = self._get_expected_params()\n\n        # The CopySourceSSECustomerAlgorithm needs to get mapped to\n        # SSECustomerAlgorithm for HeadObject\n        head_params['SSECustomerAlgorithm'] = 'AES256'\n        self.add_head_object_response(expected_params=head_params)\n\n        # However, it needs to remain the same for UploadPartCopy.\n        self._add_params_to_expected_params(\n            add_copy_kwargs, ['copy'], self.extra_args\n        )\n        self.add_successful_copy_responses(**add_copy_kwargs)\n\n        call_kwargs = self.create_call_kwargs()\n        call_kwargs['extra_args'] = self.extra_args\n        future = self.manager.copy(**call_kwargs)\n        future.result()\n        self.stubber.assert_no_pending_responses()\n\n    def test_abort_on_failure(self):\n        # First add the head object and create multipart upload\n        self.add_head_object_response()\n        self.add_create_multipart_upload_response()\n\n        # Cause an error on upload_part_copy\n        self.stubber.add_client_error('upload_part_copy', 'ArbitraryFailure')\n\n        # Add the abort multipart to ensure it gets cleaned up on failure\n        self.stubber.add_response(\n            'abort_multipart_upload',\n            service_response={},\n            expected_params={\n                'Bucket': self.bucket,\n                'Key': self.key,\n                'UploadId': self.multipart_id,\n            },\n        )\n\n        future = self.manager.copy(**self.create_call_kwargs())\n        with self.assertRaisesRegex(ClientError, 'ArbitraryFailure'):\n            future.result()\n        self.stubber.assert_no_pending_responses()\n\n    def test_mp_copy_with_tagging_directive(self):\n        extra_args = {'Tagging': 'tag1=val1', 'TaggingDirective': 'REPLACE'}\n        self.add_head_object_response()\n        self.add_successful_copy_responses(\n            expected_create_mpu_params={\n                'Bucket': self.bucket,\n                'Key': self.key,\n                'Tagging': 'tag1=val1',\n            }\n        )\n        future = self.manager.copy(\n            self.copy_source, self.bucket, self.key, extra_args\n        )\n        future.result()\n        self.stubber.assert_no_pending_responses()\n", "tests/functional/test_processpool.py": "# Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport glob\nimport os\nfrom io import BytesIO\nfrom multiprocessing.managers import BaseManager\n\nimport botocore.exceptions\nimport botocore.session\nfrom botocore.stub import Stubber\n\nfrom s3transfer.exceptions import CancelledError\nfrom s3transfer.processpool import ProcessPoolDownloader, ProcessTransferConfig\nfrom tests import FileCreator, mock, unittest\n\n\nclass StubbedClient:\n    def __init__(self):\n        self._client = botocore.session.get_session().create_client(\n            's3',\n            'us-west-2',\n            aws_access_key_id='foo',\n            aws_secret_access_key='bar',\n        )\n        self._stubber = Stubber(self._client)\n        self._stubber.activate()\n        self._caught_stubber_errors = []\n\n    def get_object(self, **kwargs):\n        return self._client.get_object(**kwargs)\n\n    def head_object(self, **kwargs):\n        return self._client.head_object(**kwargs)\n\n    def add_response(self, *args, **kwargs):\n        self._stubber.add_response(*args, **kwargs)\n\n    def add_client_error(self, *args, **kwargs):\n        self._stubber.add_client_error(*args, **kwargs)\n\n\nclass StubbedClientManager(BaseManager):\n    pass\n\n\nStubbedClientManager.register('StubbedClient', StubbedClient)\n\n\n# Ideally a Mock would be used here. However, they cannot be pickled\n# for Windows. So instead we define a factory class at the module level that\n# can return a stubbed client we initialized in the setUp.\nclass StubbedClientFactory:\n    def __init__(self, stubbed_client):\n        self._stubbed_client = stubbed_client\n\n    def __call__(self, *args, **kwargs):\n        # The __call__ is defined so we can provide an instance of the\n        # StubbedClientFactory to mock.patch() and have the instance be\n        # returned when the patched class is instantiated.\n        return self\n\n    def create_client(self):\n        return self._stubbed_client\n\n\nclass TestProcessPoolDownloader(unittest.TestCase):\n    def setUp(self):\n        # The stubbed client needs to run in a manager to be shared across\n        # processes and have it properly consume the stubbed response across\n        # processes.\n        self.manager = StubbedClientManager()\n        self.manager.start()\n        self.stubbed_client = self.manager.StubbedClient()\n        self.stubbed_client_factory = StubbedClientFactory(self.stubbed_client)\n\n        self.client_factory_patch = mock.patch(\n            's3transfer.processpool.ClientFactory', self.stubbed_client_factory\n        )\n        self.client_factory_patch.start()\n        self.files = FileCreator()\n\n        self.config = ProcessTransferConfig(max_request_processes=1)\n        self.downloader = ProcessPoolDownloader(config=self.config)\n        self.bucket = 'mybucket'\n        self.key = 'mykey'\n        self.filename = self.files.full_path('filename')\n        self.remote_contents = b'my content'\n        self.stream = BytesIO(self.remote_contents)\n\n    def tearDown(self):\n        self.manager.shutdown()\n        self.client_factory_patch.stop()\n        self.files.remove_all()\n\n    def assert_contents(self, filename, expected_contents):\n        self.assertTrue(os.path.exists(filename))\n        with open(filename, 'rb') as f:\n            self.assertEqual(f.read(), expected_contents)\n\n    def test_download_file(self):\n        self.stubbed_client.add_response(\n            'head_object', {'ContentLength': len(self.remote_contents)}\n        )\n        self.stubbed_client.add_response('get_object', {'Body': self.stream})\n        with self.downloader:\n            self.downloader.download_file(self.bucket, self.key, self.filename)\n        self.assert_contents(self.filename, self.remote_contents)\n\n    def test_download_multiple_files(self):\n        self.stubbed_client.add_response('get_object', {'Body': self.stream})\n        self.stubbed_client.add_response(\n            'get_object', {'Body': BytesIO(self.remote_contents)}\n        )\n        with self.downloader:\n            self.downloader.download_file(\n                self.bucket,\n                self.key,\n                self.filename,\n                expected_size=len(self.remote_contents),\n            )\n            other_file = self.files.full_path('filename2')\n            self.downloader.download_file(\n                self.bucket,\n                self.key,\n                other_file,\n                expected_size=len(self.remote_contents),\n            )\n        self.assert_contents(self.filename, self.remote_contents)\n        self.assert_contents(other_file, self.remote_contents)\n\n    def test_download_file_ranged_download(self):\n        half_of_content_length = int(len(self.remote_contents) / 2)\n        self.stubbed_client.add_response(\n            'head_object', {'ContentLength': len(self.remote_contents)}\n        )\n        self.stubbed_client.add_response(\n            'get_object',\n            {'Body': BytesIO(self.remote_contents[:half_of_content_length])},\n        )\n        self.stubbed_client.add_response(\n            'get_object',\n            {'Body': BytesIO(self.remote_contents[half_of_content_length:])},\n        )\n        downloader = ProcessPoolDownloader(\n            config=ProcessTransferConfig(\n                multipart_chunksize=half_of_content_length,\n                multipart_threshold=half_of_content_length,\n                max_request_processes=1,\n            )\n        )\n        with downloader:\n            downloader.download_file(self.bucket, self.key, self.filename)\n        self.assert_contents(self.filename, self.remote_contents)\n\n    def test_download_file_extra_args(self):\n        self.stubbed_client.add_response(\n            'head_object',\n            {'ContentLength': len(self.remote_contents)},\n            expected_params={\n                'Bucket': self.bucket,\n                'Key': self.key,\n                'VersionId': 'versionid',\n            },\n        )\n        self.stubbed_client.add_response(\n            'get_object',\n            {'Body': self.stream},\n            expected_params={\n                'Bucket': self.bucket,\n                'Key': self.key,\n                'VersionId': 'versionid',\n            },\n        )\n        with self.downloader:\n            self.downloader.download_file(\n                self.bucket,\n                self.key,\n                self.filename,\n                extra_args={'VersionId': 'versionid'},\n            )\n        self.assert_contents(self.filename, self.remote_contents)\n\n    def test_download_file_expected_size(self):\n        self.stubbed_client.add_response('get_object', {'Body': self.stream})\n        with self.downloader:\n            self.downloader.download_file(\n                self.bucket,\n                self.key,\n                self.filename,\n                expected_size=len(self.remote_contents),\n            )\n        self.assert_contents(self.filename, self.remote_contents)\n\n    def test_cleans_up_tempfile_on_failure(self):\n        self.stubbed_client.add_client_error('get_object', 'NoSuchKey')\n        with self.downloader:\n            self.downloader.download_file(\n                self.bucket,\n                self.key,\n                self.filename,\n                expected_size=len(self.remote_contents),\n            )\n        self.assertFalse(os.path.exists(self.filename))\n        # Any tempfile should have been erased as well\n        possible_matches = glob.glob('%s*' % self.filename + os.extsep)\n        self.assertEqual(possible_matches, [])\n\n    def test_validates_extra_args(self):\n        with self.downloader:\n            with self.assertRaises(ValueError):\n                self.downloader.download_file(\n                    self.bucket,\n                    self.key,\n                    self.filename,\n                    extra_args={'NotSupported': 'NotSupported'},\n                )\n\n    def test_result_with_success(self):\n        self.stubbed_client.add_response('get_object', {'Body': self.stream})\n        with self.downloader:\n            future = self.downloader.download_file(\n                self.bucket,\n                self.key,\n                self.filename,\n                expected_size=len(self.remote_contents),\n            )\n            self.assertIsNone(future.result())\n\n    def test_result_with_exception(self):\n        self.stubbed_client.add_client_error('get_object', 'NoSuchKey')\n        with self.downloader:\n            future = self.downloader.download_file(\n                self.bucket,\n                self.key,\n                self.filename,\n                expected_size=len(self.remote_contents),\n            )\n            with self.assertRaises(botocore.exceptions.ClientError):\n                future.result()\n\n    def test_result_with_cancel(self):\n        self.stubbed_client.add_response('get_object', {'Body': self.stream})\n        with self.downloader:\n            future = self.downloader.download_file(\n                self.bucket,\n                self.key,\n                self.filename,\n                expected_size=len(self.remote_contents),\n            )\n            future.cancel()\n            with self.assertRaises(CancelledError):\n                future.result()\n\n    def test_shutdown_with_no_downloads(self):\n        downloader = ProcessPoolDownloader()\n        try:\n            downloader.shutdown()\n        except AttributeError:\n            self.fail(\n                'The downloader should be able to be shutdown even though '\n                'the downloader was never started.'\n            )\n\n    def test_shutdown_with_no_downloads_and_ctrl_c(self):\n        # Special shutdown logic happens if a KeyboardInterrupt is raised in\n        # the context manager. However, this logic can not happen if the\n        # downloader was never started. So a KeyboardInterrupt should be\n        # the only exception propagated.\n        with self.assertRaises(KeyboardInterrupt):\n            with self.downloader:\n                raise KeyboardInterrupt()\n", "tests/functional/__init__.py": "# Copyright 2016 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n", "tests/functional/test_upload.py": "# Copyright 2016 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport os\nimport shutil\nimport tempfile\nimport time\nfrom io import BytesIO\n\nfrom botocore.awsrequest import AWSRequest\nfrom botocore.client import Config\nfrom botocore.exceptions import ClientError\nfrom botocore.stub import ANY\n\nfrom s3transfer.manager import TransferConfig, TransferManager\nfrom s3transfer.utils import ChunksizeAdjuster\nfrom tests import (\n    BaseGeneralInterfaceTest,\n    NonSeekableReader,\n    RecordingOSUtils,\n    RecordingSubscriber,\n    mock,\n)\n\n\nclass BaseUploadTest(BaseGeneralInterfaceTest):\n    def setUp(self):\n        super().setUp()\n        # TODO: We do not want to use the real MIN_UPLOAD_CHUNKSIZE\n        # when we're adjusting parts.\n        # This is really wasteful and fails CI builds because self.contents\n        # would normally use 10MB+ of memory.\n        # Until there's an API to configure this, we're patching this with\n        # a min size of 1.  We can't patch MIN_UPLOAD_CHUNKSIZE directly\n        # because it's already bound to a default value in the\n        # chunksize adjuster.  Instead we need to patch out the\n        # chunksize adjuster class.\n        self.adjuster_patch = mock.patch(\n            's3transfer.upload.ChunksizeAdjuster',\n            lambda: ChunksizeAdjuster(min_size=1),\n        )\n        self.adjuster_patch.start()\n        self.config = TransferConfig(max_request_concurrency=1)\n        self._manager = TransferManager(self.client, self.config)\n\n        # Create a temporary directory with files to read from\n        self.tempdir = tempfile.mkdtemp()\n        self.filename = os.path.join(self.tempdir, 'myfile')\n        self.content = b'my content'\n\n        with open(self.filename, 'wb') as f:\n            f.write(self.content)\n\n        # Initialize some default arguments\n        self.bucket = 'mybucket'\n        self.key = 'mykey'\n        self.extra_args = {}\n        self.subscribers = []\n\n        # A list to keep track of all of the bodies sent over the wire\n        # and their order.\n        self.sent_bodies = []\n        self.client.meta.events.register(\n            'before-parameter-build.s3.*', self.collect_body\n        )\n\n    def tearDown(self):\n        super().tearDown()\n        shutil.rmtree(self.tempdir)\n        self.adjuster_patch.stop()\n\n    def collect_body(self, params, model, **kwargs):\n        # A handler to simulate the reading of the body including the\n        # request-created event that signals to simulate the progress\n        # callbacks\n        if 'Body' in params:\n            # TODO: This is not ideal. Need to figure out a better idea of\n            # simulating reading of the request across the wire to trigger\n            # progress callbacks\n            request = AWSRequest(\n                method='PUT',\n                url='https://s3.amazonaws.com',\n                data=params['Body'],\n            )\n            self.client.meta.events.emit(\n                'request-created.s3.%s' % model.name,\n                request=request,\n                operation_name=model.name,\n            )\n            self.sent_bodies.append(self._stream_body(params['Body']))\n\n    def _stream_body(self, body):\n        read_amt = 8 * 1024\n        data = body.read(read_amt)\n        collected_body = data\n        while data:\n            data = body.read(read_amt)\n            collected_body += data\n        return collected_body\n\n    @property\n    def manager(self):\n        return self._manager\n\n    @property\n    def method(self):\n        return self.manager.upload\n\n    def create_call_kwargs(self):\n        return {\n            'fileobj': self.filename,\n            'bucket': self.bucket,\n            'key': self.key,\n        }\n\n    def create_invalid_extra_args(self):\n        return {'Foo': 'bar'}\n\n    def create_stubbed_responses(self):\n        return [{'method': 'put_object', 'service_response': {}}]\n\n    def create_expected_progress_callback_info(self):\n        return [{'bytes_transferred': 10}]\n\n    def assert_expected_client_calls_were_correct(self):\n        # We assert that expected client calls were made by ensuring that\n        # there are no more pending responses. If there are no more pending\n        # responses, then all stubbed responses were consumed.\n        self.stubber.assert_no_pending_responses()\n\n\nclass TestNonMultipartUpload(BaseUploadTest):\n    __test__ = True\n\n    def add_put_object_response_with_default_expected_params(\n        self, extra_expected_params=None, bucket=None\n    ):\n        if bucket is None:\n            bucket = self.bucket\n\n        expected_params = {'Body': ANY, 'Bucket': bucket, 'Key': self.key}\n        if extra_expected_params:\n            expected_params.update(extra_expected_params)\n        upload_response = self.create_stubbed_responses()[0]\n        upload_response['expected_params'] = expected_params\n        self.stubber.add_response(**upload_response)\n\n    def assert_put_object_body_was_correct(self):\n        self.assertEqual(self.sent_bodies, [self.content])\n\n    def test_upload(self):\n        self.extra_args['RequestPayer'] = 'requester'\n        self.add_put_object_response_with_default_expected_params(\n            extra_expected_params={'RequestPayer': 'requester'}\n        )\n        future = self.manager.upload(\n            self.filename, self.bucket, self.key, self.extra_args\n        )\n        future.result()\n        self.assert_expected_client_calls_were_correct()\n        self.assert_put_object_body_was_correct()\n\n    def test_upload_with_checksum(self):\n        self.extra_args['ChecksumAlgorithm'] = 'sha256'\n        self.add_put_object_response_with_default_expected_params(\n            extra_expected_params={'ChecksumAlgorithm': 'sha256'}\n        )\n        future = self.manager.upload(\n            self.filename, self.bucket, self.key, self.extra_args\n        )\n        future.result()\n        self.assert_expected_client_calls_were_correct()\n        self.assert_put_object_body_was_correct()\n\n    def test_upload_with_s3express_default_checksum(self):\n        s3express_bucket = \"mytestbucket--usw2-az6--x-s3\"\n        self.assertFalse(\"ChecksumAlgorithm\" in self.extra_args)\n\n        self.add_put_object_response_with_default_expected_params(\n            extra_expected_params={'ChecksumAlgorithm': 'crc32'},\n            bucket=s3express_bucket,\n        )\n        future = self.manager.upload(\n            self.filename, s3express_bucket, self.key, self.extra_args\n        )\n        future.result()\n        self.assert_expected_client_calls_were_correct()\n        self.assert_put_object_body_was_correct()\n\n    def test_upload_for_fileobj(self):\n        self.add_put_object_response_with_default_expected_params()\n        with open(self.filename, 'rb') as f:\n            future = self.manager.upload(\n                f, self.bucket, self.key, self.extra_args\n            )\n            future.result()\n        self.assert_expected_client_calls_were_correct()\n        self.assert_put_object_body_was_correct()\n\n    def test_upload_for_seekable_filelike_obj(self):\n        self.add_put_object_response_with_default_expected_params()\n        bytes_io = BytesIO(self.content)\n        future = self.manager.upload(\n            bytes_io, self.bucket, self.key, self.extra_args\n        )\n        future.result()\n        self.assert_expected_client_calls_were_correct()\n        self.assert_put_object_body_was_correct()\n\n    def test_upload_for_seekable_filelike_obj_that_has_been_seeked(self):\n        self.add_put_object_response_with_default_expected_params()\n        bytes_io = BytesIO(self.content)\n        seek_pos = 5\n        bytes_io.seek(seek_pos)\n        future = self.manager.upload(\n            bytes_io, self.bucket, self.key, self.extra_args\n        )\n        future.result()\n        self.assert_expected_client_calls_were_correct()\n        self.assertEqual(b''.join(self.sent_bodies), self.content[seek_pos:])\n\n    def test_upload_for_non_seekable_filelike_obj(self):\n        self.add_put_object_response_with_default_expected_params()\n        body = NonSeekableReader(self.content)\n        future = self.manager.upload(\n            body, self.bucket, self.key, self.extra_args\n        )\n        future.result()\n        self.assert_expected_client_calls_were_correct()\n        self.assert_put_object_body_was_correct()\n\n    def test_sigv4_progress_callbacks_invoked_once(self):\n        # Reset the client and manager to use sigv4\n        self.reset_stubber_with_new_client(\n            {'config': Config(signature_version='s3v4')}\n        )\n        self.client.meta.events.register(\n            'before-parameter-build.s3.*', self.collect_body\n        )\n        self._manager = TransferManager(self.client, self.config)\n\n        # Add the stubbed response.\n        self.add_put_object_response_with_default_expected_params()\n\n        subscriber = RecordingSubscriber()\n        future = self.manager.upload(\n            self.filename, self.bucket, self.key, subscribers=[subscriber]\n        )\n        future.result()\n        self.assert_expected_client_calls_were_correct()\n\n        # The amount of bytes seen should be the same as the file size\n        self.assertEqual(subscriber.calculate_bytes_seen(), len(self.content))\n\n    def test_uses_provided_osutil(self):\n        osutil = RecordingOSUtils()\n        # Use the recording os utility for the transfer manager\n        self._manager = TransferManager(self.client, self.config, osutil)\n\n        self.add_put_object_response_with_default_expected_params()\n\n        future = self.manager.upload(self.filename, self.bucket, self.key)\n        future.result()\n\n        # The upload should have used the os utility. We check this by making\n        # sure that the recorded opens are as expected.\n        expected_opens = [(self.filename, 'rb')]\n        self.assertEqual(osutil.open_records, expected_opens)\n\n    def test_allowed_upload_params_are_valid(self):\n        op_model = self.client.meta.service_model.operation_model('PutObject')\n        for allowed_upload_arg in self._manager.ALLOWED_UPLOAD_ARGS:\n            self.assertIn(allowed_upload_arg, op_model.input_shape.members)\n\n    def test_upload_with_bandwidth_limiter(self):\n        self.content = b'a' * 1024 * 1024\n        with open(self.filename, 'wb') as f:\n            f.write(self.content)\n        self.config = TransferConfig(\n            max_request_concurrency=1, max_bandwidth=len(self.content) / 2\n        )\n        self._manager = TransferManager(self.client, self.config)\n\n        self.add_put_object_response_with_default_expected_params()\n        start = time.time()\n        future = self.manager.upload(self.filename, self.bucket, self.key)\n        future.result()\n        # This is just a smoke test to make sure that the limiter is\n        # being used and not necessary its exactness. So we set the maximum\n        # bandwidth to len(content)/2 per sec and make sure that it is\n        # noticeably slower. Ideally it will take more than two seconds, but\n        # given tracking at the beginning of transfers are not entirely\n        # accurate setting at the initial start of a transfer, we give us\n        # some flexibility by setting the expected time to half of the\n        # theoretical time to take.\n        self.assertGreaterEqual(time.time() - start, 1)\n\n        self.assert_expected_client_calls_were_correct()\n        self.assert_put_object_body_was_correct()\n\n    def test_raise_exception_on_s3_object_lambda_resource(self):\n        s3_object_lambda_arn = (\n            'arn:aws:s3-object-lambda:us-west-2:123456789012:'\n            'accesspoint:my-accesspoint'\n        )\n        with self.assertRaisesRegex(ValueError, 'methods do not support'):\n            self.manager.upload(self.filename, s3_object_lambda_arn, self.key)\n\n\nclass TestMultipartUpload(BaseUploadTest):\n    __test__ = True\n\n    def setUp(self):\n        super().setUp()\n        self.chunksize = 4\n        self.config = TransferConfig(\n            max_request_concurrency=1,\n            multipart_threshold=1,\n            multipart_chunksize=self.chunksize,\n        )\n        self._manager = TransferManager(self.client, self.config)\n        self.multipart_id = 'my-upload-id'\n\n    def create_stubbed_responses(self):\n        return [\n            {\n                'method': 'create_multipart_upload',\n                'service_response': {'UploadId': self.multipart_id},\n            },\n            {'method': 'upload_part', 'service_response': {'ETag': 'etag-1'}},\n            {'method': 'upload_part', 'service_response': {'ETag': 'etag-2'}},\n            {'method': 'upload_part', 'service_response': {'ETag': 'etag-3'}},\n            {'method': 'complete_multipart_upload', 'service_response': {}},\n        ]\n\n    def create_expected_progress_callback_info(self):\n        return [\n            {'bytes_transferred': 4},\n            {'bytes_transferred': 4},\n            {'bytes_transferred': 2},\n        ]\n\n    def assert_upload_part_bodies_were_correct(self):\n        expected_contents = []\n        for i in range(0, len(self.content), self.chunksize):\n            end_i = i + self.chunksize\n            if end_i > len(self.content):\n                expected_contents.append(self.content[i:])\n            else:\n                expected_contents.append(self.content[i:end_i])\n        self.assertEqual(self.sent_bodies, expected_contents)\n\n    def add_create_multipart_response_with_default_expected_params(\n        self,\n        extra_expected_params=None,\n        bucket=None,\n    ):\n        if bucket is None:\n            bucket = self.bucket\n\n        expected_params = {'Bucket': bucket, 'Key': self.key}\n        if extra_expected_params:\n            expected_params.update(extra_expected_params)\n        response = self.create_stubbed_responses()[0]\n        response['expected_params'] = expected_params\n        self.stubber.add_response(**response)\n\n    def add_upload_part_responses_with_default_expected_params(\n        self,\n        extra_expected_params=None,\n        bucket=None,\n    ):\n        if bucket is None:\n            bucket = self.bucket\n\n        num_parts = 3\n        upload_part_responses = self.create_stubbed_responses()[1:-1]\n        for i in range(num_parts):\n            upload_part_response = upload_part_responses[i]\n            expected_params = {\n                'Bucket': bucket,\n                'Key': self.key,\n                'UploadId': self.multipart_id,\n                'Body': ANY,\n                'PartNumber': i + 1,\n            }\n            if extra_expected_params:\n                expected_params.update(extra_expected_params)\n                # If ChecksumAlgorithm is present stub the response checksums\n                if 'ChecksumAlgorithm' in extra_expected_params:\n                    name = extra_expected_params['ChecksumAlgorithm']\n                    checksum_member = 'Checksum%s' % name.upper()\n                    response = upload_part_response['service_response']\n                    response[checksum_member] = 'sum%s==' % (i + 1)\n\n            upload_part_response['expected_params'] = expected_params\n            self.stubber.add_response(**upload_part_response)\n\n    def add_complete_multipart_response_with_default_expected_params(\n        self,\n        extra_expected_params=None,\n        bucket=None,\n    ):\n        if bucket is None:\n            bucket = self.bucket\n\n        expected_params = {\n            'Bucket': bucket,\n            'Key': self.key,\n            'UploadId': self.multipart_id,\n            'MultipartUpload': {\n                'Parts': [\n                    {'ETag': 'etag-1', 'PartNumber': 1},\n                    {'ETag': 'etag-2', 'PartNumber': 2},\n                    {'ETag': 'etag-3', 'PartNumber': 3},\n                ]\n            },\n        }\n        if extra_expected_params:\n            expected_params.update(extra_expected_params)\n        response = self.create_stubbed_responses()[-1]\n        response['expected_params'] = expected_params\n        self.stubber.add_response(**response)\n\n    def test_upload(self):\n        self.extra_args['RequestPayer'] = 'requester'\n\n        # Add requester pays to the create multipart upload and upload parts.\n        self.add_create_multipart_response_with_default_expected_params(\n            extra_expected_params={'RequestPayer': 'requester'}\n        )\n        self.add_upload_part_responses_with_default_expected_params(\n            extra_expected_params={'RequestPayer': 'requester'}\n        )\n        self.add_complete_multipart_response_with_default_expected_params(\n            extra_expected_params={'RequestPayer': 'requester'}\n        )\n\n        future = self.manager.upload(\n            self.filename, self.bucket, self.key, self.extra_args\n        )\n        future.result()\n        self.assert_expected_client_calls_were_correct()\n\n    def test_upload_for_fileobj(self):\n        self.add_create_multipart_response_with_default_expected_params()\n        self.add_upload_part_responses_with_default_expected_params()\n        self.add_complete_multipart_response_with_default_expected_params()\n        with open(self.filename, 'rb') as f:\n            future = self.manager.upload(\n                f, self.bucket, self.key, self.extra_args\n            )\n            future.result()\n        self.assert_expected_client_calls_were_correct()\n        self.assert_upload_part_bodies_were_correct()\n\n    def test_upload_for_seekable_filelike_obj(self):\n        self.add_create_multipart_response_with_default_expected_params()\n        self.add_upload_part_responses_with_default_expected_params()\n        self.add_complete_multipart_response_with_default_expected_params()\n        bytes_io = BytesIO(self.content)\n        future = self.manager.upload(\n            bytes_io, self.bucket, self.key, self.extra_args\n        )\n        future.result()\n        self.assert_expected_client_calls_were_correct()\n        self.assert_upload_part_bodies_were_correct()\n\n    def test_upload_for_seekable_filelike_obj_that_has_been_seeked(self):\n        self.add_create_multipart_response_with_default_expected_params()\n        self.add_upload_part_responses_with_default_expected_params()\n        self.add_complete_multipart_response_with_default_expected_params()\n        bytes_io = BytesIO(self.content)\n        seek_pos = 1\n        bytes_io.seek(seek_pos)\n        future = self.manager.upload(\n            bytes_io, self.bucket, self.key, self.extra_args\n        )\n        future.result()\n        self.assert_expected_client_calls_were_correct()\n        self.assertEqual(b''.join(self.sent_bodies), self.content[seek_pos:])\n\n    def test_upload_for_non_seekable_filelike_obj(self):\n        self.add_create_multipart_response_with_default_expected_params()\n        self.add_upload_part_responses_with_default_expected_params()\n        self.add_complete_multipart_response_with_default_expected_params()\n        stream = NonSeekableReader(self.content)\n        future = self.manager.upload(\n            stream, self.bucket, self.key, self.extra_args\n        )\n        future.result()\n        self.assert_expected_client_calls_were_correct()\n        self.assert_upload_part_bodies_were_correct()\n\n    def test_limits_in_memory_chunks_for_fileobj(self):\n        # Limit the maximum in memory chunks to one but make number of\n        # threads more than one. This means that the upload will have to\n        # happen sequentially despite having many threads available because\n        # data is sequentially partitioned into chunks in memory and since\n        # there can only every be one in memory chunk, each upload part will\n        # have to happen one at a time.\n        self.config.max_request_concurrency = 10\n        self.config.max_in_memory_upload_chunks = 1\n        self._manager = TransferManager(self.client, self.config)\n\n        # Add some default stubbed responses.\n        # These responses are added in order of part number so if the\n        # multipart upload is not done sequentially, which it should because\n        # we limit the in memory upload chunks to one, the stubber will\n        # raise exceptions for mismatching parameters for partNumber when\n        # once the upload() method is called on the transfer manager.\n        # If there is a mismatch, the stubber error will propagate on\n        # the future.result()\n        self.add_create_multipart_response_with_default_expected_params()\n        self.add_upload_part_responses_with_default_expected_params()\n        self.add_complete_multipart_response_with_default_expected_params()\n        with open(self.filename, 'rb') as f:\n            future = self.manager.upload(\n                f, self.bucket, self.key, self.extra_args\n            )\n            future.result()\n\n        # Make sure that the stubber had all of its stubbed responses consumed.\n        self.assert_expected_client_calls_were_correct()\n        # Ensure the contents were uploaded in sequentially order by checking\n        # the sent contents were in order.\n        self.assert_upload_part_bodies_were_correct()\n\n    def test_upload_failure_invokes_abort(self):\n        self.stubber.add_response(\n            method='create_multipart_upload',\n            service_response={'UploadId': self.multipart_id},\n            expected_params={'Bucket': self.bucket, 'Key': self.key},\n        )\n        self.stubber.add_response(\n            method='upload_part',\n            service_response={'ETag': 'etag-1'},\n            expected_params={\n                'Bucket': self.bucket,\n                'Body': ANY,\n                'Key': self.key,\n                'UploadId': self.multipart_id,\n                'PartNumber': 1,\n            },\n        )\n        # With the upload part failing this should immediately initiate\n        # an abort multipart with no more upload parts called.\n        self.stubber.add_client_error(method='upload_part')\n\n        self.stubber.add_response(\n            method='abort_multipart_upload',\n            service_response={},\n            expected_params={\n                'Bucket': self.bucket,\n                'Key': self.key,\n                'UploadId': self.multipart_id,\n            },\n        )\n\n        future = self.manager.upload(self.filename, self.bucket, self.key)\n        # The exception should get propagated to the future and not be\n        # a cancelled error or something.\n        with self.assertRaises(ClientError):\n            future.result()\n        self.assert_expected_client_calls_were_correct()\n\n    def test_upload_passes_select_extra_args(self):\n        self.extra_args['Metadata'] = {'foo': 'bar'}\n\n        # Add metadata to expected create multipart upload call\n        self.add_create_multipart_response_with_default_expected_params(\n            extra_expected_params={'Metadata': {'foo': 'bar'}}\n        )\n        self.add_upload_part_responses_with_default_expected_params()\n        self.add_complete_multipart_response_with_default_expected_params()\n\n        future = self.manager.upload(\n            self.filename, self.bucket, self.key, self.extra_args\n        )\n        future.result()\n        self.assert_expected_client_calls_were_correct()\n\n    def test_multipart_upload_passes_checksums(self):\n        self.extra_args['ChecksumAlgorithm'] = 'sha1'\n\n        # ChecksumAlgorithm should be passed on the create_multipart call\n        self.add_create_multipart_response_with_default_expected_params(\n            extra_expected_params={'ChecksumAlgorithm': 'sha1'},\n        )\n\n        # ChecksumAlgorithm should be forwarded and a SHA1 will come back\n        self.add_upload_part_responses_with_default_expected_params(\n            extra_expected_params={'ChecksumAlgorithm': 'sha1'},\n        )\n\n        # The checksums should be used in the complete call like etags\n        self.add_complete_multipart_response_with_default_expected_params(\n            extra_expected_params={\n                'MultipartUpload': {\n                    'Parts': [\n                        {\n                            'ETag': 'etag-1',\n                            'PartNumber': 1,\n                            'ChecksumSHA1': 'sum1==',\n                        },\n                        {\n                            'ETag': 'etag-2',\n                            'PartNumber': 2,\n                            'ChecksumSHA1': 'sum2==',\n                        },\n                        {\n                            'ETag': 'etag-3',\n                            'PartNumber': 3,\n                            'ChecksumSHA1': 'sum3==',\n                        },\n                    ]\n                }\n            },\n        )\n\n        future = self.manager.upload(\n            self.filename, self.bucket, self.key, self.extra_args\n        )\n        future.result()\n        self.assert_expected_client_calls_were_correct()\n\n    def test_multipart_upload_sets_s3express_default_checksum(self):\n        s3express_bucket = \"mytestbucket--usw2-az6--x-s3\"\n        self.assertFalse('ChecksumAlgorithm' in self.extra_args)\n\n        # ChecksumAlgorithm should be passed on the create_multipart call\n        self.add_create_multipart_response_with_default_expected_params(\n            extra_expected_params={'ChecksumAlgorithm': 'crc32'},\n            bucket=s3express_bucket,\n        )\n\n        # ChecksumAlgorithm should be forwarded and a SHA1 will come back\n        self.add_upload_part_responses_with_default_expected_params(\n            extra_expected_params={'ChecksumAlgorithm': 'crc32'},\n            bucket=s3express_bucket,\n        )\n\n        # The checksums should be used in the complete call like etags\n        self.add_complete_multipart_response_with_default_expected_params(\n            extra_expected_params={\n                'MultipartUpload': {\n                    'Parts': [\n                        {\n                            'ETag': 'etag-1',\n                            'PartNumber': 1,\n                            'ChecksumCRC32': 'sum1==',\n                        },\n                        {\n                            'ETag': 'etag-2',\n                            'PartNumber': 2,\n                            'ChecksumCRC32': 'sum2==',\n                        },\n                        {\n                            'ETag': 'etag-3',\n                            'PartNumber': 3,\n                            'ChecksumCRC32': 'sum3==',\n                        },\n                    ]\n                }\n            },\n            bucket=s3express_bucket,\n        )\n\n        future = self.manager.upload(\n            self.filename, s3express_bucket, self.key, self.extra_args\n        )\n        future.result()\n        self.assert_expected_client_calls_were_correct()\n\n    def test_multipart_upload_with_ssec_args(self):\n        params = {\n            'RequestPayer': 'requester',\n            'SSECustomerKey': 'key',\n            'SSECustomerAlgorithm': 'AES256',\n            'SSECustomerKeyMD5': 'key-hash',\n        }\n        self.extra_args.update(params)\n\n        self.add_create_multipart_response_with_default_expected_params(\n            extra_expected_params=params\n        )\n\n        self.add_upload_part_responses_with_default_expected_params(\n            extra_expected_params=params\n        )\n        self.add_complete_multipart_response_with_default_expected_params(\n            extra_expected_params=params\n        )\n        future = self.manager.upload(\n            self.filename, self.bucket, self.key, self.extra_args\n        )\n        future.result()\n        self.assert_expected_client_calls_were_correct()\n", "tests/functional/test_crt.py": "# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport fnmatch\nimport io\nimport threading\nimport time\nfrom concurrent.futures import Future\n\nfrom botocore.session import Session\n\nfrom s3transfer.subscribers import BaseSubscriber\nfrom tests import (\n    HAS_CRT,\n    FileCreator,\n    NonSeekableReader,\n    NonSeekableWriter,\n    mock,\n    requires_crt,\n    unittest,\n)\n\nif HAS_CRT:\n    import awscrt\n\n    import s3transfer.crt\n\n\nclass submitThread(threading.Thread):\n    def __init__(self, transfer_manager, futures, callargs):\n        threading.Thread.__init__(self)\n        self._transfer_manager = transfer_manager\n        self._futures = futures\n        self._callargs = callargs\n\n    def run(self):\n        self._futures.append(self._transfer_manager.download(*self._callargs))\n\n\nclass RecordingSubscriber(BaseSubscriber):\n    def __init__(self):\n        self.on_queued_called = False\n        self.on_done_called = False\n        self.bytes_transferred = 0\n        self.on_queued_future = None\n        self.on_done_future = None\n\n    def on_queued(self, future, **kwargs):\n        self.on_queued_called = True\n        self.on_queued_future = future\n\n    def on_done(self, future, **kwargs):\n        self.on_done_called = True\n        self.on_done_future = future\n\n\n@requires_crt\nclass TestCRTTransferManager(unittest.TestCase):\n    def setUp(self):\n        self.region = 'us-west-2'\n        self.bucket = \"test_bucket\"\n        self.s3express_bucket = 's3expressbucket--usw2-az5--x-s3'\n        self.key = \"test_key\"\n        self.expected_content = b'my content'\n        self.expected_download_content = b'new content'\n        self.files = FileCreator()\n        self.filename = self.files.create_file(\n            'myfile', self.expected_content, mode='wb'\n        )\n        self.expected_path = \"/\" + self.bucket + \"/\" + self.key\n        self.expected_host = \"s3.%s.amazonaws.com\" % (self.region)\n        self.expected_s3express_host = f'{self.s3express_bucket}.s3express-usw2-az5.us-west-2.amazonaws.com'\n        self.expected_s3express_path = f'/{self.key}'\n        self.s3_request = mock.Mock(awscrt.s3.S3Request)\n        self.s3_crt_client = mock.Mock(awscrt.s3.S3Client)\n        self.s3_crt_client.make_request.side_effect = (\n            self._simulate_make_request_side_effect\n        )\n        self.session = Session()\n        self.session.set_config_variable('region', self.region)\n        self.request_serializer = s3transfer.crt.BotocoreCRTRequestSerializer(\n            self.session\n        )\n        self.transfer_manager = s3transfer.crt.CRTTransferManager(\n            crt_s3_client=self.s3_crt_client,\n            crt_request_serializer=self.request_serializer,\n        )\n        self.record_subscriber = RecordingSubscriber()\n\n    def tearDown(self):\n        self.files.remove_all()\n\n    def _assert_expected_crt_http_request(\n        self,\n        crt_http_request,\n        expected_http_method='GET',\n        expected_host=None,\n        expected_path=None,\n        expected_body_content=None,\n        expected_content_length=None,\n        expected_missing_headers=None,\n    ):\n        if expected_host is None:\n            expected_host = self.expected_host\n        if expected_path is None:\n            expected_path = self.expected_path\n        self.assertEqual(crt_http_request.method, expected_http_method)\n        self.assertEqual(crt_http_request.headers.get(\"host\"), expected_host)\n        self.assertEqual(crt_http_request.path, expected_path)\n        if expected_body_content is not None:\n            # Note: The underlying CRT awscrt.io.InputStream does not expose\n            # a public read method so we have to reach into the private,\n            # underlying stream to determine the content. We should update\n            # to use a public interface if a public interface is ever exposed.\n            self.assertEqual(\n                crt_http_request.body_stream._stream.read(),\n                expected_body_content,\n            )\n        if expected_content_length is not None:\n            self.assertEqual(\n                crt_http_request.headers.get('Content-Length'),\n                str(expected_content_length),\n            )\n        if expected_missing_headers is not None:\n            header_names = [\n                header[0].lower() for header in crt_http_request.headers\n            ]\n            for expected_missing_header in expected_missing_headers:\n                self.assertNotIn(expected_missing_header.lower(), header_names)\n\n    def _assert_exected_s3express_request(\n        self, make_request_kwargs, expected_http_method='GET'\n    ):\n        self._assert_expected_crt_http_request(\n            make_request_kwargs[\"request\"],\n            expected_host=self.expected_s3express_host,\n            expected_path=self.expected_s3express_path,\n            expected_http_method=expected_http_method,\n        )\n        self.assertIn('signing_config', make_request_kwargs)\n        self.assertEqual(\n            make_request_kwargs['signing_config'].algorithm,\n            awscrt.auth.AwsSigningAlgorithm.V4_S3EXPRESS,\n        )\n\n    def _assert_subscribers_called(self, expected_future=None):\n        self.assertTrue(self.record_subscriber.on_queued_called)\n        self.assertTrue(self.record_subscriber.on_done_called)\n        if expected_future:\n            self.assertIs(\n                self.record_subscriber.on_queued_future, expected_future\n            )\n            self.assertIs(\n                self.record_subscriber.on_done_future, expected_future\n            )\n\n    def _get_expected_upload_checksum_config(self, **overrides):\n        checksum_config_kwargs = {\n            'algorithm': awscrt.s3.S3ChecksumAlgorithm.CRC32,\n            'location': awscrt.s3.S3ChecksumLocation.TRAILER,\n        }\n        checksum_config_kwargs.update(overrides)\n        return awscrt.s3.S3ChecksumConfig(**checksum_config_kwargs)\n\n    def _get_expected_download_checksum_config(self, **overrides):\n        checksum_config_kwargs = {\n            'validate_response': True,\n        }\n        checksum_config_kwargs.update(overrides)\n        return awscrt.s3.S3ChecksumConfig(**checksum_config_kwargs)\n\n    def _invoke_done_callbacks(self, **kwargs):\n        callargs = self.s3_crt_client.make_request.call_args\n        callargs_kwargs = callargs[1]\n        on_done = callargs_kwargs[\"on_done\"]\n        on_done(error=None)\n\n    def _simulate_file_download(self, recv_filepath):\n        self.files.create_file(\n            recv_filepath, self.expected_download_content, mode='wb'\n        )\n\n    def _simulate_on_body_download(self, on_body_callback):\n        on_body_callback(chunk=self.expected_download_content, offset=0)\n\n    def _simulate_make_request_side_effect(self, **kwargs):\n        if kwargs.get('recv_filepath'):\n            self._simulate_file_download(kwargs['recv_filepath'])\n        if kwargs.get('on_body'):\n            self._simulate_on_body_download(kwargs['on_body'])\n        self._invoke_done_callbacks()\n        return self.s3_request\n\n    def test_upload(self):\n        future = self.transfer_manager.upload(\n            self.filename, self.bucket, self.key, {}, [self.record_subscriber]\n        )\n        future.result()\n\n        callargs_kwargs = self.s3_crt_client.make_request.call_args[1]\n        self.assertEqual(\n            callargs_kwargs,\n            {\n                'request': mock.ANY,\n                'type': awscrt.s3.S3RequestType.PUT_OBJECT,\n                'send_filepath': self.filename,\n                'on_progress': mock.ANY,\n                'on_done': mock.ANY,\n                'checksum_config': self._get_expected_upload_checksum_config(),\n            },\n        )\n        self._assert_expected_crt_http_request(\n            callargs_kwargs[\"request\"],\n            expected_http_method='PUT',\n            expected_content_length=len(self.expected_content),\n            expected_missing_headers=['Content-MD5'],\n        )\n        self._assert_subscribers_called(future)\n\n    def test_upload_from_seekable_stream(self):\n        with open(self.filename, 'rb') as f:\n            future = self.transfer_manager.upload(\n                f, self.bucket, self.key, {}, [self.record_subscriber]\n            )\n            future.result()\n\n            callargs_kwargs = self.s3_crt_client.make_request.call_args[1]\n            self.assertEqual(\n                callargs_kwargs,\n                {\n                    'request': mock.ANY,\n                    'type': awscrt.s3.S3RequestType.PUT_OBJECT,\n                    'send_filepath': None,\n                    'on_progress': mock.ANY,\n                    'on_done': mock.ANY,\n                    'checksum_config': self._get_expected_upload_checksum_config(),\n                },\n            )\n            self._assert_expected_crt_http_request(\n                callargs_kwargs[\"request\"],\n                expected_http_method='PUT',\n                expected_body_content=self.expected_content,\n                expected_content_length=len(self.expected_content),\n                expected_missing_headers=['Content-MD5'],\n            )\n            self._assert_subscribers_called(future)\n\n    def test_upload_from_nonseekable_stream(self):\n        nonseekable_stream = NonSeekableReader(self.expected_content)\n        future = self.transfer_manager.upload(\n            nonseekable_stream,\n            self.bucket,\n            self.key,\n            {},\n            [self.record_subscriber],\n        )\n        future.result()\n\n        callargs_kwargs = self.s3_crt_client.make_request.call_args[1]\n        self.assertEqual(\n            callargs_kwargs,\n            {\n                'request': mock.ANY,\n                'type': awscrt.s3.S3RequestType.PUT_OBJECT,\n                'send_filepath': None,\n                'on_progress': mock.ANY,\n                'on_done': mock.ANY,\n                'checksum_config': self._get_expected_upload_checksum_config(),\n            },\n        )\n        self._assert_expected_crt_http_request(\n            callargs_kwargs[\"request\"],\n            expected_http_method='PUT',\n            expected_body_content=self.expected_content,\n            expected_missing_headers=[\n                'Content-MD5',\n                'Content-Length',\n                'Transfer-Encoding',\n            ],\n        )\n        self._assert_subscribers_called(future)\n\n    def test_upload_override_checksum_algorithm(self):\n        future = self.transfer_manager.upload(\n            self.filename,\n            self.bucket,\n            self.key,\n            {'ChecksumAlgorithm': 'CRC32C'},\n            [self.record_subscriber],\n        )\n        future.result()\n\n        callargs_kwargs = self.s3_crt_client.make_request.call_args[1]\n        self.assertEqual(\n            callargs_kwargs,\n            {\n                'request': mock.ANY,\n                'type': awscrt.s3.S3RequestType.PUT_OBJECT,\n                'send_filepath': self.filename,\n                'on_progress': mock.ANY,\n                'on_done': mock.ANY,\n                'checksum_config': self._get_expected_upload_checksum_config(\n                    algorithm=awscrt.s3.S3ChecksumAlgorithm.CRC32C\n                ),\n            },\n        )\n        self._assert_expected_crt_http_request(\n            callargs_kwargs[\"request\"],\n            expected_http_method='PUT',\n            expected_content_length=len(self.expected_content),\n            expected_missing_headers=[\n                'Content-MD5',\n                'x-amz-sdk-checksum-algorithm',\n                'X-Amz-Trailer',\n            ],\n        )\n        self._assert_subscribers_called(future)\n\n    def test_upload_override_checksum_algorithm_accepts_lowercase(self):\n        future = self.transfer_manager.upload(\n            self.filename,\n            self.bucket,\n            self.key,\n            {'ChecksumAlgorithm': 'crc32c'},\n            [self.record_subscriber],\n        )\n        future.result()\n\n        callargs_kwargs = self.s3_crt_client.make_request.call_args[1]\n        self.assertEqual(\n            callargs_kwargs,\n            {\n                'request': mock.ANY,\n                'type': awscrt.s3.S3RequestType.PUT_OBJECT,\n                'send_filepath': self.filename,\n                'on_progress': mock.ANY,\n                'on_done': mock.ANY,\n                'checksum_config': self._get_expected_upload_checksum_config(\n                    algorithm=awscrt.s3.S3ChecksumAlgorithm.CRC32C\n                ),\n            },\n        )\n        self._assert_expected_crt_http_request(\n            callargs_kwargs[\"request\"],\n            expected_http_method='PUT',\n            expected_content_length=len(self.expected_content),\n            expected_missing_headers=[\n                'Content-MD5',\n                'x-amz-sdk-checksum-algorithm',\n                'X-Amz-Trailer',\n            ],\n        )\n        self._assert_subscribers_called(future)\n\n    def test_upload_throws_error_for_unsupported_checksum(self):\n        with self.assertRaisesRegex(\n            ValueError, 'ChecksumAlgorithm: UNSUPPORTED not supported'\n        ):\n            self.transfer_manager.upload(\n                self.filename,\n                self.bucket,\n                self.key,\n                {'ChecksumAlgorithm': 'UNSUPPORTED'},\n                [self.record_subscriber],\n            )\n\n    def test_upload_throws_error_for_unsupported_arg(self):\n        with self.assertRaisesRegex(\n            ValueError, \"Invalid extra_args key 'ContentMD5'\"\n        ):\n            self.transfer_manager.upload(\n                self.filename,\n                self.bucket,\n                self.key,\n                {'ContentMD5': '938c2cc0dcc05f2b68c4287040cfcf71'},\n                [self.record_subscriber],\n            )\n\n    def test_upload_throws_error_on_s3_object_lambda_resource(self):\n        s3_object_lambda_arn = (\n            'arn:aws:s3-object-lambda:us-west-2:123456789012:'\n            'accesspoint:my-accesspoint'\n        )\n        with self.assertRaisesRegex(ValueError, 'methods do not support'):\n            self.transfer_manager.upload(\n                self.filename, s3_object_lambda_arn, self.key\n            )\n\n    def test_upload_with_s3express(self):\n        future = self.transfer_manager.upload(\n            self.filename,\n            self.s3express_bucket,\n            self.key,\n            {},\n            [self.record_subscriber],\n        )\n        future.result()\n        self._assert_exected_s3express_request(\n            self.s3_crt_client.make_request.call_args[1],\n            expected_http_method='PUT',\n        )\n\n    def test_download(self):\n        future = self.transfer_manager.download(\n            self.bucket, self.key, self.filename, {}, [self.record_subscriber]\n        )\n        future.result()\n\n        callargs_kwargs = self.s3_crt_client.make_request.call_args[1]\n        self.assertEqual(\n            callargs_kwargs,\n            {\n                'request': mock.ANY,\n                'type': awscrt.s3.S3RequestType.GET_OBJECT,\n                'recv_filepath': mock.ANY,\n                'on_progress': mock.ANY,\n                'on_done': mock.ANY,\n                'on_body': None,\n                'checksum_config': self._get_expected_download_checksum_config(),\n            },\n        )\n        # the recv_filepath will be set to a temporary file path with some\n        # random suffix\n        self.assertTrue(\n            fnmatch.fnmatch(\n                callargs_kwargs[\"recv_filepath\"],\n                f'{self.filename}.*',\n            )\n        )\n        self._assert_expected_crt_http_request(\n            callargs_kwargs[\"request\"],\n            expected_http_method='GET',\n            expected_content_length=0,\n        )\n        self._assert_subscribers_called(future)\n        with open(self.filename, 'rb') as f:\n            # Check the fake response overwrites the file because of download\n            self.assertEqual(f.read(), self.expected_download_content)\n\n    def test_download_to_seekable_stream(self):\n        with open(self.filename, 'wb') as f:\n            future = self.transfer_manager.download(\n                self.bucket, self.key, f, {}, [self.record_subscriber]\n            )\n            future.result()\n\n        callargs_kwargs = self.s3_crt_client.make_request.call_args[1]\n        self.assertEqual(\n            callargs_kwargs,\n            {\n                'request': mock.ANY,\n                'type': awscrt.s3.S3RequestType.GET_OBJECT,\n                'recv_filepath': None,\n                'on_progress': mock.ANY,\n                'on_done': mock.ANY,\n                'on_body': mock.ANY,\n                'checksum_config': self._get_expected_download_checksum_config(),\n            },\n        )\n        self._assert_expected_crt_http_request(\n            callargs_kwargs[\"request\"],\n            expected_http_method='GET',\n            expected_content_length=0,\n        )\n        self._assert_subscribers_called(future)\n        with open(self.filename, 'rb') as f:\n            # Check the fake response overwrites the file because of download\n            self.assertEqual(f.read(), self.expected_download_content)\n\n    def test_download_to_nonseekable_stream(self):\n        underlying_stream = io.BytesIO()\n        nonseekable_stream = NonSeekableWriter(underlying_stream)\n        future = self.transfer_manager.download(\n            self.bucket,\n            self.key,\n            nonseekable_stream,\n            {},\n            [self.record_subscriber],\n        )\n        future.result()\n\n        callargs_kwargs = self.s3_crt_client.make_request.call_args[1]\n        self.assertEqual(\n            callargs_kwargs,\n            {\n                'request': mock.ANY,\n                'type': awscrt.s3.S3RequestType.GET_OBJECT,\n                'recv_filepath': None,\n                'on_progress': mock.ANY,\n                'on_done': mock.ANY,\n                'on_body': mock.ANY,\n                'checksum_config': self._get_expected_download_checksum_config(),\n            },\n        )\n        self._assert_expected_crt_http_request(\n            callargs_kwargs[\"request\"],\n            expected_http_method='GET',\n            expected_content_length=0,\n        )\n        self._assert_subscribers_called(future)\n        self.assertEqual(\n            underlying_stream.getvalue(), self.expected_download_content\n        )\n\n    def test_download_throws_error_for_unsupported_arg(self):\n        with self.assertRaisesRegex(\n            ValueError, \"Invalid extra_args key 'Range'\"\n        ):\n            self.transfer_manager.download(\n                self.bucket,\n                self.key,\n                self.filename,\n                {'Range': 'bytes:0-1023'},\n                [self.record_subscriber],\n            )\n\n    def test_download_with_s3express(self):\n        future = self.transfer_manager.download(\n            self.s3express_bucket,\n            self.key,\n            self.filename,\n            {},\n            [self.record_subscriber],\n        )\n        future.result()\n        self._assert_exected_s3express_request(\n            self.s3_crt_client.make_request.call_args[1],\n            expected_http_method='GET',\n        )\n\n    def test_delete(self):\n        future = self.transfer_manager.delete(\n            self.bucket, self.key, {}, [self.record_subscriber]\n        )\n        future.result()\n\n        callargs_kwargs = self.s3_crt_client.make_request.call_args[1]\n        self.assertEqual(\n            callargs_kwargs,\n            {\n                'request': mock.ANY,\n                'type': awscrt.s3.S3RequestType.DEFAULT,\n                'operation_name': \"DeleteObject\",\n                'on_progress': mock.ANY,\n                'on_done': mock.ANY,\n            },\n        )\n        self._assert_expected_crt_http_request(\n            callargs_kwargs[\"request\"],\n            expected_http_method='DELETE',\n            expected_content_length=0,\n        )\n        self._assert_subscribers_called(future)\n\n    def test_delete_throws_error_for_unsupported_arg(self):\n        with self.assertRaisesRegex(\n            ValueError, \"Invalid extra_args key 'BypassGovernanceRetention'\"\n        ):\n            self.transfer_manager.delete(\n                self.bucket,\n                self.key,\n                {'BypassGovernanceRetention': True},\n                [self.record_subscriber],\n            )\n\n    def test_delete_with_s3express(self):\n        future = self.transfer_manager.delete(\n            self.s3express_bucket, self.key, {}, [self.record_subscriber]\n        )\n        future.result()\n        self._assert_exected_s3express_request(\n            self.s3_crt_client.make_request.call_args[1],\n            expected_http_method='DELETE',\n        )\n\n    def test_blocks_when_max_requests_processes_reached(self):\n        self.s3_crt_client.make_request.return_value = self.s3_request\n        # We simulate blocking by not invoking the on_done callbacks for\n        # all of the requests we send. The default side effect invokes all\n        # callbacks so we need to unset the side effect to avoid on_done from\n        # being called in the child threads.\n        self.s3_crt_client.make_request.side_effect = None\n        futures = []\n        callargs = (self.bucket, self.key, self.filename, {}, [])\n        max_request_processes = 128  # the hard coded max processes\n        all_concurrent = max_request_processes + 1\n        threads = []\n        for i in range(0, all_concurrent):\n            thread = submitThread(self.transfer_manager, futures, callargs)\n            thread.start()\n            threads.append(thread)\n        # Sleep until the expected max requests has been reached\n        while len(futures) < max_request_processes:\n            time.sleep(0.05)\n        self.assertLessEqual(\n            self.s3_crt_client.make_request.call_count, max_request_processes\n        )\n        # Release lock\n        callargs = self.s3_crt_client.make_request.call_args\n        callargs_kwargs = callargs[1]\n        on_done = callargs_kwargs[\"on_done\"]\n        on_done(error=None)\n        for thread in threads:\n            thread.join()\n        self.assertEqual(\n            self.s3_crt_client.make_request.call_count, all_concurrent\n        )\n\n    def _cancel_function(self):\n        self.cancel_called = True\n        self.s3_request.finished_future.set_exception(\n            awscrt.exceptions.from_code(0)\n        )\n        self._invoke_done_callbacks()\n\n    def test_cancel(self):\n        self.s3_request.finished_future = Future()\n        self.cancel_called = False\n        self.s3_request.cancel = self._cancel_function\n        try:\n            with self.transfer_manager:\n                future = self.transfer_manager.upload(\n                    self.filename, self.bucket, self.key, {}, []\n                )\n                raise KeyboardInterrupt()\n        except KeyboardInterrupt:\n            pass\n\n        with self.assertRaises(awscrt.exceptions.AwsCrtError):\n            future.result()\n        self.assertTrue(self.cancel_called)\n\n    def test_serializer_error_handling(self):\n        class SerializationException(Exception):\n            pass\n\n        class ExceptionRaisingSerializer(\n            s3transfer.crt.BaseCRTRequestSerializer\n        ):\n            def serialize_http_request(self, transfer_type, future):\n                raise SerializationException()\n\n        not_impl_serializer = ExceptionRaisingSerializer()\n        transfer_manager = s3transfer.crt.CRTTransferManager(\n            crt_s3_client=self.s3_crt_client,\n            crt_request_serializer=not_impl_serializer,\n        )\n        future = transfer_manager.upload(\n            self.filename, self.bucket, self.key, {}, []\n        )\n\n        with self.assertRaises(SerializationException):\n            future.result()\n\n    def test_crt_s3_client_error_handling(self):\n        self.s3_crt_client.make_request.side_effect = (\n            awscrt.exceptions.from_code(0)\n        )\n        future = self.transfer_manager.upload(\n            self.filename, self.bucket, self.key, {}, []\n        )\n        with self.assertRaises(awscrt.exceptions.AwsCrtError):\n            future.result()\n", "tests/functional/test_manager.py": "# Copyright 2016 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License'). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the 'license' file accompanying this file. This file is\n# distributed on an 'AS IS' BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nfrom io import BytesIO\n\nfrom botocore.awsrequest import create_request_object\n\nfrom s3transfer.exceptions import CancelledError, FatalError\nfrom s3transfer.futures import BaseExecutor\nfrom s3transfer.manager import TransferConfig, TransferManager\nfrom tests import StubbedClientTest, mock, skip_if_using_serial_implementation\n\n\nclass ArbitraryException(Exception):\n    pass\n\n\nclass SignalTransferringBody(BytesIO):\n    \"\"\"A mocked body with the ability to signal when transfers occur\"\"\"\n\n    def __init__(self):\n        super().__init__()\n        self.signal_transferring_call_count = 0\n        self.signal_not_transferring_call_count = 0\n\n    def signal_transferring(self):\n        self.signal_transferring_call_count += 1\n\n    def signal_not_transferring(self):\n        self.signal_not_transferring_call_count += 1\n\n    def seek(self, where, whence=0):\n        pass\n\n    def tell(self):\n        return 0\n\n    def read(self, amount=0):\n        return b''\n\n\nclass TestTransferManager(StubbedClientTest):\n    @skip_if_using_serial_implementation(\n        'Exception is thrown once all transfers are submitted. '\n        'However for the serial implementation, transfers are performed '\n        'in main thread meaning all transfers will complete before the '\n        'exception being thrown.'\n    )\n    def test_error_in_context_manager_cancels_incomplete_transfers(self):\n        # The purpose of this test is to make sure if an error is raised\n        # in the body of the context manager, incomplete transfers will\n        # be cancelled with value of the exception wrapped by a CancelledError\n\n        # NOTE: The fact that delete() was chosen to test this is arbitrary\n        # other than it is the easiet to set up for the stubber.\n        # The specific operation is not important to the purpose of this test.\n        num_transfers = 100\n        futures = []\n        ref_exception_msg = 'arbitrary exception'\n\n        for _ in range(num_transfers):\n            self.stubber.add_response('delete_object', {})\n\n        manager = TransferManager(\n            self.client,\n            TransferConfig(\n                max_request_concurrency=1, max_submission_concurrency=1\n            ),\n        )\n        try:\n            with manager:\n                for i in range(num_transfers):\n                    futures.append(manager.delete('mybucket', 'mykey'))\n                raise ArbitraryException(ref_exception_msg)\n        except ArbitraryException:\n            # At least one of the submitted futures should have been\n            # cancelled.\n            with self.assertRaisesRegex(FatalError, ref_exception_msg):\n                for future in futures:\n                    future.result()\n\n    @skip_if_using_serial_implementation(\n        'Exception is thrown once all transfers are submitted. '\n        'However for the serial implementation, transfers are performed '\n        'in main thread meaning all transfers will complete before the '\n        'exception being thrown.'\n    )\n    def test_cntrl_c_in_context_manager_cancels_incomplete_transfers(self):\n        # The purpose of this test is to make sure if an error is raised\n        # in the body of the context manager, incomplete transfers will\n        # be cancelled with value of the exception wrapped by a CancelledError\n\n        # NOTE: The fact that delete() was chosen to test this is arbitrary\n        # other than it is the easiet to set up for the stubber.\n        # The specific operation is not important to the purpose of this test.\n        num_transfers = 100\n        futures = []\n\n        for _ in range(num_transfers):\n            self.stubber.add_response('delete_object', {})\n\n        manager = TransferManager(\n            self.client,\n            TransferConfig(\n                max_request_concurrency=1, max_submission_concurrency=1\n            ),\n        )\n        try:\n            with manager:\n                for i in range(num_transfers):\n                    futures.append(manager.delete('mybucket', 'mykey'))\n                raise KeyboardInterrupt()\n        except KeyboardInterrupt:\n            # At least one of the submitted futures should have been\n            # cancelled.\n            with self.assertRaisesRegex(CancelledError, 'KeyboardInterrupt()'):\n                for future in futures:\n                    future.result()\n\n    def test_enable_disable_callbacks_only_ever_registered_once(self):\n        body = SignalTransferringBody()\n        request = create_request_object(\n            {\n                'method': 'PUT',\n                'url': 'https://s3.amazonaws.com',\n                'body': body,\n                'headers': {},\n                'context': {},\n            }\n        )\n        # Create two TransferManager's using the same client\n        TransferManager(self.client)\n        TransferManager(self.client)\n        self.client.meta.events.emit(\n            'request-created.s3', request=request, operation_name='PutObject'\n        )\n        # The client should have only have the enable/disable callback\n        # handlers registered once depite being used for two different\n        # TransferManagers.\n        self.assertEqual(\n            body.signal_transferring_call_count,\n            1,\n            'The enable_callback() should have only ever been registered once',\n        )\n        self.assertEqual(\n            body.signal_not_transferring_call_count,\n            1,\n            'The disable_callback() should have only ever been registered '\n            'once',\n        )\n\n    def test_use_custom_executor_implementation(self):\n        mocked_executor_cls = mock.Mock(BaseExecutor)\n        transfer_manager = TransferManager(\n            self.client, executor_cls=mocked_executor_cls\n        )\n        transfer_manager.delete('bucket', 'key')\n        self.assertTrue(mocked_executor_cls.return_value.submit.called)\n\n    def test_unicode_exception_in_context_manager(self):\n        with self.assertRaises(ArbitraryException):\n            with TransferManager(self.client):\n                raise ArbitraryException('\\u2713')\n\n    def test_client_property(self):\n        manager = TransferManager(self.client)\n        self.assertIs(manager.client, self.client)\n\n    def test_config_property(self):\n        config = TransferConfig()\n        manager = TransferManager(self.client, config)\n        self.assertIs(manager.config, config)\n\n    def test_can_disable_bucket_validation(self):\n        s3_object_lambda_arn = (\n            'arn:aws:s3-object-lambda:us-west-2:123456789012:'\n            'accesspoint:my-accesspoint'\n        )\n        config = TransferConfig()\n        manager = TransferManager(self.client, config)\n        manager.VALIDATE_SUPPORTED_BUCKET_VALUES = False\n        manager.delete(s3_object_lambda_arn, 'my-key')\n", "tests/functional/test_utils.py": "# Copyright 2016 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport os\nimport shutil\nimport socket\nimport tempfile\n\nfrom s3transfer.utils import OSUtils\nfrom tests import skip_if_windows, unittest\n\n\n@skip_if_windows('Windows does not support UNIX special files')\nclass TestOSUtilsSpecialFiles(unittest.TestCase):\n    def setUp(self):\n        self.tempdir = tempfile.mkdtemp()\n        self.filename = os.path.join(self.tempdir, 'myfile')\n\n    def tearDown(self):\n        shutil.rmtree(self.tempdir)\n\n    def test_character_device(self):\n        self.assertTrue(OSUtils().is_special_file('/dev/null'))\n\n    def test_fifo(self):\n        os.mkfifo(self.filename)\n        self.assertTrue(OSUtils().is_special_file(self.filename))\n\n    def test_socket(self):\n        sock = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM)\n        sock.bind(self.filename)\n        self.assertTrue(OSUtils().is_special_file(self.filename))\n", "tests/functional/test_download.py": "# Copyright 2016 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport copy\nimport glob\nimport os\nimport shutil\nimport tempfile\nimport time\nfrom io import BytesIO\n\nfrom botocore.exceptions import ClientError\n\nfrom s3transfer.compat import SOCKET_ERROR\nfrom s3transfer.exceptions import RetriesExceededError\nfrom s3transfer.manager import TransferConfig, TransferManager\nfrom tests import (\n    BaseGeneralInterfaceTest,\n    FileSizeProvider,\n    NonSeekableWriter,\n    RecordingOSUtils,\n    RecordingSubscriber,\n    StreamWithError,\n    skip_if_using_serial_implementation,\n    skip_if_windows,\n)\n\n\nclass BaseDownloadTest(BaseGeneralInterfaceTest):\n    def setUp(self):\n        super().setUp()\n        self.config = TransferConfig(max_request_concurrency=1)\n        self._manager = TransferManager(self.client, self.config)\n\n        # Create a temporary directory to write to\n        self.tempdir = tempfile.mkdtemp()\n        self.filename = os.path.join(self.tempdir, 'myfile')\n\n        # Initialize some default arguments\n        self.bucket = 'mybucket'\n        self.key = 'mykey'\n        self.extra_args = {}\n        self.subscribers = []\n\n        # Create a stream to read from\n        self.content = b'my content'\n        self.stream = BytesIO(self.content)\n\n    def tearDown(self):\n        super().tearDown()\n        shutil.rmtree(self.tempdir)\n\n    @property\n    def manager(self):\n        return self._manager\n\n    @property\n    def method(self):\n        return self.manager.download\n\n    def create_call_kwargs(self):\n        return {\n            'bucket': self.bucket,\n            'key': self.key,\n            'fileobj': self.filename,\n        }\n\n    def create_invalid_extra_args(self):\n        return {'Foo': 'bar'}\n\n    def create_stubbed_responses(self):\n        # We want to make sure the beginning of the stream is always used\n        # in case this gets called twice.\n        self.stream.seek(0)\n        return [\n            {\n                'method': 'head_object',\n                'service_response': {'ContentLength': len(self.content)},\n            },\n            {\n                'method': 'get_object',\n                'service_response': {'Body': self.stream},\n            },\n        ]\n\n    def create_expected_progress_callback_info(self):\n        # Note that last read is from the empty sentinel indicating\n        # that the stream is done.\n        return [{'bytes_transferred': 10}]\n\n    def add_head_object_response(self, expected_params=None):\n        head_response = self.create_stubbed_responses()[0]\n        if expected_params:\n            head_response['expected_params'] = expected_params\n        self.stubber.add_response(**head_response)\n\n    def add_successful_get_object_responses(\n        self, expected_params=None, expected_ranges=None\n    ):\n        # Add all get_object responses needed to complete the download.\n        # Should account for both ranged and nonranged downloads.\n        for i, stubbed_response in enumerate(\n            self.create_stubbed_responses()[1:]\n        ):\n            if expected_params:\n                stubbed_response['expected_params'] = copy.deepcopy(\n                    expected_params\n                )\n                if expected_ranges:\n                    stubbed_response['expected_params'][\n                        'Range'\n                    ] = expected_ranges[i]\n            self.stubber.add_response(**stubbed_response)\n\n    def add_n_retryable_get_object_responses(self, n, num_reads=0):\n        for _ in range(n):\n            self.stubber.add_response(\n                method='get_object',\n                service_response={\n                    'Body': StreamWithError(\n                        copy.deepcopy(self.stream), SOCKET_ERROR, num_reads\n                    )\n                },\n            )\n\n    def test_download_temporary_file_does_not_exist(self):\n        self.add_head_object_response()\n        self.add_successful_get_object_responses()\n\n        future = self.manager.download(**self.create_call_kwargs())\n        future.result()\n        # Make sure the file exists\n        self.assertTrue(os.path.exists(self.filename))\n        # Make sure the random temporary file does not exist\n        possible_matches = glob.glob('%s*' % self.filename + os.extsep)\n        self.assertEqual(possible_matches, [])\n\n    def test_download_for_fileobj(self):\n        self.add_head_object_response()\n        self.add_successful_get_object_responses()\n\n        with open(self.filename, 'wb') as f:\n            future = self.manager.download(\n                self.bucket, self.key, f, self.extra_args\n            )\n            future.result()\n\n        # Ensure that the contents are correct\n        with open(self.filename, 'rb') as f:\n            self.assertEqual(self.content, f.read())\n\n    def test_download_for_seekable_filelike_obj(self):\n        self.add_head_object_response()\n        self.add_successful_get_object_responses()\n\n        # Create a file-like object to test. In this case, it is a BytesIO\n        # object.\n        bytes_io = BytesIO()\n\n        future = self.manager.download(\n            self.bucket, self.key, bytes_io, self.extra_args\n        )\n        future.result()\n\n        # Ensure that the contents are correct\n        bytes_io.seek(0)\n        self.assertEqual(self.content, bytes_io.read())\n\n    def test_download_for_nonseekable_filelike_obj(self):\n        self.add_head_object_response()\n        self.add_successful_get_object_responses()\n\n        with open(self.filename, 'wb') as f:\n            future = self.manager.download(\n                self.bucket, self.key, NonSeekableWriter(f), self.extra_args\n            )\n            future.result()\n\n        # Ensure that the contents are correct\n        with open(self.filename, 'rb') as f:\n            self.assertEqual(self.content, f.read())\n\n    def test_download_cleanup_on_failure(self):\n        self.add_head_object_response()\n\n        # Throw an error on the download\n        self.stubber.add_client_error('get_object')\n\n        future = self.manager.download(**self.create_call_kwargs())\n\n        with self.assertRaises(ClientError):\n            future.result()\n        # Make sure the actual file and the temporary do not exist\n        # by globbing for the file and any of its extensions\n        possible_matches = glob.glob('%s*' % self.filename)\n        self.assertEqual(possible_matches, [])\n\n    def test_download_with_nonexistent_directory(self):\n        self.add_head_object_response()\n        self.add_successful_get_object_responses()\n\n        call_kwargs = self.create_call_kwargs()\n        call_kwargs['fileobj'] = os.path.join(\n            self.tempdir, 'missing-directory', 'myfile'\n        )\n        future = self.manager.download(**call_kwargs)\n        with self.assertRaises(IOError):\n            future.result()\n\n    def test_retries_and_succeeds(self):\n        self.add_head_object_response()\n        # Insert a response that will trigger a retry.\n        self.add_n_retryable_get_object_responses(1)\n        # Add the normal responses to simulate the download proceeding\n        # as normal after the retry.\n        self.add_successful_get_object_responses()\n\n        future = self.manager.download(**self.create_call_kwargs())\n        future.result()\n\n        # The retry should have been consumed and the process should have\n        # continued using the successful responses.\n        self.stubber.assert_no_pending_responses()\n        with open(self.filename, 'rb') as f:\n            self.assertEqual(self.content, f.read())\n\n    def test_retry_failure(self):\n        self.add_head_object_response()\n\n        max_retries = 3\n        self.config.num_download_attempts = max_retries\n        self._manager = TransferManager(self.client, self.config)\n        # Add responses that fill up the maximum number of retries.\n        self.add_n_retryable_get_object_responses(max_retries)\n\n        future = self.manager.download(**self.create_call_kwargs())\n\n        # A retry exceeded error should have happened.\n        with self.assertRaises(RetriesExceededError):\n            future.result()\n\n        # All of the retries should have been used up.\n        self.stubber.assert_no_pending_responses()\n\n    def test_retry_rewinds_callbacks(self):\n        self.add_head_object_response()\n        # Insert a response that will trigger a retry after one read of the\n        # stream has been made.\n        self.add_n_retryable_get_object_responses(1, num_reads=1)\n        # Add the normal responses to simulate the download proceeding\n        # as normal after the retry.\n        self.add_successful_get_object_responses()\n\n        recorder_subscriber = RecordingSubscriber()\n        # Set the streaming to a size that is smaller than the data we\n        # currently provide to it to simulate rewinds of callbacks.\n        self.config.io_chunksize = 3\n        future = self.manager.download(\n            subscribers=[recorder_subscriber], **self.create_call_kwargs()\n        )\n        future.result()\n\n        # Ensure that there is no more remaining responses and that contents\n        # are correct.\n        self.stubber.assert_no_pending_responses()\n        with open(self.filename, 'rb') as f:\n            self.assertEqual(self.content, f.read())\n\n        # Assert that the number of bytes seen is equal to the length of\n        # downloaded content.\n        self.assertEqual(\n            recorder_subscriber.calculate_bytes_seen(), len(self.content)\n        )\n\n        # Also ensure that the second progress invocation was negative three\n        # because a retry happened on the second read of the stream and we\n        # know that the chunk size for each read is 3.\n        progress_byte_amts = [\n            call['bytes_transferred']\n            for call in recorder_subscriber.on_progress_calls\n        ]\n        self.assertEqual(-3, progress_byte_amts[1])\n\n    def test_can_provide_file_size(self):\n        self.add_successful_get_object_responses()\n\n        call_kwargs = self.create_call_kwargs()\n        call_kwargs['subscribers'] = [FileSizeProvider(len(self.content))]\n\n        future = self.manager.download(**call_kwargs)\n        future.result()\n\n        # The HeadObject should have not happened and should have been able\n        # to successfully download the file.\n        self.stubber.assert_no_pending_responses()\n        with open(self.filename, 'rb') as f:\n            self.assertEqual(self.content, f.read())\n\n    def test_uses_provided_osutil(self):\n        osutil = RecordingOSUtils()\n        # Use the recording os utility for the transfer manager\n        self._manager = TransferManager(self.client, self.config, osutil)\n\n        self.add_head_object_response()\n        self.add_successful_get_object_responses()\n\n        future = self.manager.download(**self.create_call_kwargs())\n        future.result()\n        # The osutil should have had its open() method invoked when opening\n        # a temporary file and its rename_file() method invoked when the\n        # the temporary file was moved to its final location.\n        self.assertEqual(len(osutil.open_records), 1)\n        self.assertEqual(len(osutil.rename_records), 1)\n\n    @skip_if_windows('Windows does not support UNIX special files')\n    @skip_if_using_serial_implementation(\n        'A separate thread is needed to read from the fifo'\n    )\n    def test_download_for_fifo_file(self):\n        self.add_head_object_response()\n        self.add_successful_get_object_responses()\n\n        # Create the fifo file\n        os.mkfifo(self.filename)\n\n        future = self.manager.download(\n            self.bucket, self.key, self.filename, self.extra_args\n        )\n\n        # The call to open a fifo will block until there is both a reader\n        # and a writer, so we need to open it for reading after we've\n        # started the transfer.\n        with open(self.filename, 'rb') as fifo:\n            future.result()\n            self.assertEqual(fifo.read(), self.content)\n\n    def test_raise_exception_on_s3_object_lambda_resource(self):\n        s3_object_lambda_arn = (\n            'arn:aws:s3-object-lambda:us-west-2:123456789012:'\n            'accesspoint:my-accesspoint'\n        )\n        with self.assertRaisesRegex(ValueError, 'methods do not support'):\n            self.manager.download(\n                s3_object_lambda_arn, self.key, self.filename, self.extra_args\n            )\n\n\nclass TestNonRangedDownload(BaseDownloadTest):\n    # TODO: If you want to add tests outside of this test class and still\n    # subclass from BaseDownloadTest you need to set ``__test__ = True``. If\n    # you do not, your tests will not get picked up by the test runner! This\n    # needs to be done until we find a better way to ignore running test cases\n    # from the general test base class, which we do not want ran.\n    __test__ = True\n\n    def test_download(self):\n        self.extra_args['RequestPayer'] = 'requester'\n        expected_params = {\n            'Bucket': self.bucket,\n            'Key': self.key,\n            'RequestPayer': 'requester',\n        }\n        self.add_head_object_response(expected_params)\n        self.add_successful_get_object_responses(expected_params)\n        future = self.manager.download(\n            self.bucket, self.key, self.filename, self.extra_args\n        )\n        future.result()\n\n        # Ensure that the contents are correct\n        with open(self.filename, 'rb') as f:\n            self.assertEqual(self.content, f.read())\n\n    def test_download_with_checksum_enabled(self):\n        self.extra_args['ChecksumMode'] = 'ENABLED'\n        expected_params = {\n            'Bucket': self.bucket,\n            'Key': self.key,\n            'ChecksumMode': 'ENABLED',\n        }\n        self.add_head_object_response(expected_params)\n        self.add_successful_get_object_responses(expected_params)\n        future = self.manager.download(\n            self.bucket, self.key, self.filename, self.extra_args\n        )\n        future.result()\n\n        # Ensure that the contents are correct\n        with open(self.filename, 'rb') as f:\n            self.assertEqual(self.content, f.read())\n\n    def test_allowed_copy_params_are_valid(self):\n        op_model = self.client.meta.service_model.operation_model('GetObject')\n        for allowed_upload_arg in self._manager.ALLOWED_DOWNLOAD_ARGS:\n            self.assertIn(allowed_upload_arg, op_model.input_shape.members)\n\n    def test_download_empty_object(self):\n        self.content = b''\n        self.stream = BytesIO(self.content)\n        self.add_head_object_response()\n        self.add_successful_get_object_responses()\n        future = self.manager.download(\n            self.bucket, self.key, self.filename, self.extra_args\n        )\n        future.result()\n\n        # Ensure that the empty file exists\n        with open(self.filename, 'rb') as f:\n            self.assertEqual(b'', f.read())\n\n    def test_uses_bandwidth_limiter(self):\n        self.content = b'a' * 1024 * 1024\n        self.stream = BytesIO(self.content)\n        self.config = TransferConfig(\n            max_request_concurrency=1, max_bandwidth=len(self.content) / 2\n        )\n        self._manager = TransferManager(self.client, self.config)\n\n        self.add_head_object_response()\n        self.add_successful_get_object_responses()\n\n        start = time.time()\n        future = self.manager.download(\n            self.bucket, self.key, self.filename, self.extra_args\n        )\n        future.result()\n        # This is just a smoke test to make sure that the limiter is\n        # being used and not necessary its exactness. So we set the maximum\n        # bandwidth to len(content)/2 per sec and make sure that it is\n        # noticeably slower. Ideally it will take more than two seconds, but\n        # given tracking at the beginning of transfers are not entirely\n        # accurate setting at the initial start of a transfer, we give us\n        # some flexibility by setting the expected time to half of the\n        # theoretical time to take.\n        self.assertGreaterEqual(time.time() - start, 1)\n\n        # Ensure that the contents are correct\n        with open(self.filename, 'rb') as f:\n            self.assertEqual(self.content, f.read())\n\n\nclass TestRangedDownload(BaseDownloadTest):\n    # TODO: If you want to add tests outside of this test class and still\n    # subclass from BaseDownloadTest you need to set ``__test__ = True``. If\n    # you do not, your tests will not get picked up by the test runner! This\n    # needs to be done until we find a better way to ignore running test cases\n    # from the general test base class, which we do not want ran.\n    __test__ = True\n\n    def setUp(self):\n        super().setUp()\n        self.config = TransferConfig(\n            max_request_concurrency=1,\n            multipart_threshold=1,\n            multipart_chunksize=4,\n        )\n        self._manager = TransferManager(self.client, self.config)\n\n    def create_stubbed_responses(self):\n        return [\n            {\n                'method': 'head_object',\n                'service_response': {'ContentLength': len(self.content)},\n            },\n            {\n                'method': 'get_object',\n                'service_response': {'Body': BytesIO(self.content[0:4])},\n            },\n            {\n                'method': 'get_object',\n                'service_response': {'Body': BytesIO(self.content[4:8])},\n            },\n            {\n                'method': 'get_object',\n                'service_response': {'Body': BytesIO(self.content[8:])},\n            },\n        ]\n\n    def create_expected_progress_callback_info(self):\n        return [\n            {'bytes_transferred': 4},\n            {'bytes_transferred': 4},\n            {'bytes_transferred': 2},\n        ]\n\n    def test_download(self):\n        self.extra_args['RequestPayer'] = 'requester'\n        expected_params = {\n            'Bucket': self.bucket,\n            'Key': self.key,\n            'RequestPayer': 'requester',\n        }\n        expected_ranges = ['bytes=0-3', 'bytes=4-7', 'bytes=8-']\n        self.add_head_object_response(expected_params)\n        self.add_successful_get_object_responses(\n            expected_params, expected_ranges\n        )\n\n        future = self.manager.download(\n            self.bucket, self.key, self.filename, self.extra_args\n        )\n        future.result()\n\n        # Ensure that the contents are correct\n        with open(self.filename, 'rb') as f:\n            self.assertEqual(self.content, f.read())\n\n    def test_download_with_checksum_enabled(self):\n        self.extra_args['ChecksumMode'] = 'ENABLED'\n        expected_params = {\n            'Bucket': self.bucket,\n            'Key': self.key,\n            'ChecksumMode': 'ENABLED',\n        }\n        expected_ranges = ['bytes=0-3', 'bytes=4-7', 'bytes=8-']\n        self.add_head_object_response(expected_params)\n        self.add_successful_get_object_responses(\n            expected_params, expected_ranges\n        )\n\n        future = self.manager.download(\n            self.bucket, self.key, self.filename, self.extra_args\n        )\n        future.result()\n\n        # Ensure that the contents are correct\n        with open(self.filename, 'rb') as f:\n            self.assertEqual(self.content, f.read())\n", "tests/unit/test_delete.py": "# Copyright 2016 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nfrom s3transfer.delete import DeleteObjectTask\nfrom tests import BaseTaskTest\n\n\nclass TestDeleteObjectTask(BaseTaskTest):\n    def setUp(self):\n        super().setUp()\n        self.bucket = 'mybucket'\n        self.key = 'mykey'\n        self.extra_args = {}\n        self.callbacks = []\n\n    def get_delete_task(self, **kwargs):\n        default_kwargs = {\n            'client': self.client,\n            'bucket': self.bucket,\n            'key': self.key,\n            'extra_args': self.extra_args,\n        }\n        default_kwargs.update(kwargs)\n        return self.get_task(DeleteObjectTask, main_kwargs=default_kwargs)\n\n    def test_main(self):\n        self.stubber.add_response(\n            'delete_object',\n            service_response={},\n            expected_params={\n                'Bucket': self.bucket,\n                'Key': self.key,\n            },\n        )\n        task = self.get_delete_task()\n        task()\n\n        self.stubber.assert_no_pending_responses()\n\n    def test_extra_args(self):\n        self.extra_args['MFA'] = 'mfa-code'\n        self.extra_args['VersionId'] = '12345'\n        self.stubber.add_response(\n            'delete_object',\n            service_response={},\n            expected_params={\n                'Bucket': self.bucket,\n                'Key': self.key,\n                # These extra_args should be injected into the\n                # expected params for the delete_object call.\n                'MFA': 'mfa-code',\n                'VersionId': '12345',\n            },\n        )\n        task = self.get_delete_task()\n        task()\n\n        self.stubber.assert_no_pending_responses()\n", "tests/unit/test_s3transfer.py": "# Copyright 2016 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License'). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the 'license' file accompanying this file. This file is\n# distributed on an 'AS IS' BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport os\nimport shutil\nimport socket\nimport tempfile\nfrom concurrent import futures\nfrom contextlib import closing\nfrom io import BytesIO, StringIO\n\nfrom s3transfer import (\n    MultipartDownloader,\n    MultipartUploader,\n    OSUtils,\n    QueueShutdownError,\n    ReadFileChunk,\n    S3Transfer,\n    ShutdownQueue,\n    StreamReaderProgress,\n    TransferConfig,\n    disable_upload_callbacks,\n    enable_upload_callbacks,\n    random_file_extension,\n)\nfrom s3transfer.exceptions import RetriesExceededError, S3UploadFailedError\nfrom tests import mock, unittest\n\n\nclass InMemoryOSLayer(OSUtils):\n    def __init__(self, filemap):\n        self.filemap = filemap\n\n    def get_file_size(self, filename):\n        return len(self.filemap[filename])\n\n    def open_file_chunk_reader(self, filename, start_byte, size, callback):\n        return closing(BytesIO(self.filemap[filename]))\n\n    def open(self, filename, mode):\n        if 'wb' in mode:\n            fileobj = BytesIO()\n            self.filemap[filename] = fileobj\n            return closing(fileobj)\n        else:\n            return closing(self.filemap[filename])\n\n    def remove_file(self, filename):\n        if filename in self.filemap:\n            del self.filemap[filename]\n\n    def rename_file(self, current_filename, new_filename):\n        if current_filename in self.filemap:\n            self.filemap[new_filename] = self.filemap.pop(current_filename)\n\n\nclass SequentialExecutor:\n    def __init__(self, max_workers):\n        pass\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, *args, **kwargs):\n        pass\n\n    # The real map() interface actually takes *args, but we specifically do\n    # _not_ use this interface.\n    def map(self, function, args):\n        results = []\n        for arg in args:\n            results.append(function(arg))\n        return results\n\n    def submit(self, function):\n        future = futures.Future()\n        future.set_result(function())\n        return future\n\n\nclass TestOSUtils(unittest.TestCase):\n    def setUp(self):\n        self.tempdir = tempfile.mkdtemp()\n\n    def tearDown(self):\n        shutil.rmtree(self.tempdir)\n\n    def test_get_file_size(self):\n        with mock.patch('os.path.getsize') as m:\n            OSUtils().get_file_size('myfile')\n            m.assert_called_with('myfile')\n\n    def test_open_file_chunk_reader(self):\n        with mock.patch('s3transfer.ReadFileChunk') as m:\n            OSUtils().open_file_chunk_reader('myfile', 0, 100, None)\n            m.from_filename.assert_called_with(\n                'myfile', 0, 100, None, enable_callback=False\n            )\n\n    def test_open_file(self):\n        fileobj = OSUtils().open(os.path.join(self.tempdir, 'foo'), 'w')\n        self.assertTrue(hasattr(fileobj, 'write'))\n\n    def test_remove_file_ignores_errors(self):\n        with mock.patch('os.remove') as remove:\n            remove.side_effect = OSError('fake error')\n            OSUtils().remove_file('foo')\n        remove.assert_called_with('foo')\n\n    def test_remove_file_proxies_remove_file(self):\n        with mock.patch('os.remove') as remove:\n            OSUtils().remove_file('foo')\n            remove.assert_called_with('foo')\n\n    def test_rename_file(self):\n        with mock.patch('s3transfer.compat.rename_file') as rename_file:\n            OSUtils().rename_file('foo', 'newfoo')\n            rename_file.assert_called_with('foo', 'newfoo')\n\n\nclass TestReadFileChunk(unittest.TestCase):\n    def setUp(self):\n        self.tempdir = tempfile.mkdtemp()\n\n    def tearDown(self):\n        shutil.rmtree(self.tempdir)\n\n    def test_read_entire_chunk(self):\n        filename = os.path.join(self.tempdir, 'foo')\n        with open(filename, 'wb') as f:\n            f.write(b'onetwothreefourfivesixseveneightnineten')\n        chunk = ReadFileChunk.from_filename(\n            filename, start_byte=0, chunk_size=3\n        )\n        self.assertEqual(chunk.read(), b'one')\n        self.assertEqual(chunk.read(), b'')\n\n    def test_read_with_amount_size(self):\n        filename = os.path.join(self.tempdir, 'foo')\n        with open(filename, 'wb') as f:\n            f.write(b'onetwothreefourfivesixseveneightnineten')\n        chunk = ReadFileChunk.from_filename(\n            filename, start_byte=11, chunk_size=4\n        )\n        self.assertEqual(chunk.read(1), b'f')\n        self.assertEqual(chunk.read(1), b'o')\n        self.assertEqual(chunk.read(1), b'u')\n        self.assertEqual(chunk.read(1), b'r')\n        self.assertEqual(chunk.read(1), b'')\n\n    def test_reset_stream_emulation(self):\n        filename = os.path.join(self.tempdir, 'foo')\n        with open(filename, 'wb') as f:\n            f.write(b'onetwothreefourfivesixseveneightnineten')\n        chunk = ReadFileChunk.from_filename(\n            filename, start_byte=11, chunk_size=4\n        )\n        self.assertEqual(chunk.read(), b'four')\n        chunk.seek(0)\n        self.assertEqual(chunk.read(), b'four')\n\n    def test_read_past_end_of_file(self):\n        filename = os.path.join(self.tempdir, 'foo')\n        with open(filename, 'wb') as f:\n            f.write(b'onetwothreefourfivesixseveneightnineten')\n        chunk = ReadFileChunk.from_filename(\n            filename, start_byte=36, chunk_size=100000\n        )\n        self.assertEqual(chunk.read(), b'ten')\n        self.assertEqual(chunk.read(), b'')\n        self.assertEqual(len(chunk), 3)\n\n    def test_tell_and_seek(self):\n        filename = os.path.join(self.tempdir, 'foo')\n        with open(filename, 'wb') as f:\n            f.write(b'onetwothreefourfivesixseveneightnineten')\n        chunk = ReadFileChunk.from_filename(\n            filename, start_byte=36, chunk_size=100000\n        )\n        self.assertEqual(chunk.tell(), 0)\n        self.assertEqual(chunk.read(), b'ten')\n        self.assertEqual(chunk.tell(), 3)\n        chunk.seek(0)\n        self.assertEqual(chunk.tell(), 0)\n\n    def test_file_chunk_supports_context_manager(self):\n        filename = os.path.join(self.tempdir, 'foo')\n        with open(filename, 'wb') as f:\n            f.write(b'abc')\n        with ReadFileChunk.from_filename(\n            filename, start_byte=0, chunk_size=2\n        ) as chunk:\n            val = chunk.read()\n            self.assertEqual(val, b'ab')\n\n    def test_iter_is_always_empty(self):\n        # This tests the workaround for the httplib bug (see\n        # the source for more info).\n        filename = os.path.join(self.tempdir, 'foo')\n        open(filename, 'wb').close()\n        chunk = ReadFileChunk.from_filename(\n            filename, start_byte=0, chunk_size=10\n        )\n        self.assertEqual(list(chunk), [])\n\n\nclass TestReadFileChunkWithCallback(TestReadFileChunk):\n    def setUp(self):\n        super().setUp()\n        self.filename = os.path.join(self.tempdir, 'foo')\n        with open(self.filename, 'wb') as f:\n            f.write(b'abc')\n        self.amounts_seen = []\n\n    def callback(self, amount):\n        self.amounts_seen.append(amount)\n\n    def test_callback_is_invoked_on_read(self):\n        chunk = ReadFileChunk.from_filename(\n            self.filename, start_byte=0, chunk_size=3, callback=self.callback\n        )\n        chunk.read(1)\n        chunk.read(1)\n        chunk.read(1)\n        self.assertEqual(self.amounts_seen, [1, 1, 1])\n\n    def test_callback_can_be_disabled(self):\n        chunk = ReadFileChunk.from_filename(\n            self.filename, start_byte=0, chunk_size=3, callback=self.callback\n        )\n        chunk.disable_callback()\n        # Now reading from the ReadFileChunk should not invoke\n        # the callback.\n        chunk.read()\n        self.assertEqual(self.amounts_seen, [])\n\n    def test_callback_will_also_be_triggered_by_seek(self):\n        chunk = ReadFileChunk.from_filename(\n            self.filename, start_byte=0, chunk_size=3, callback=self.callback\n        )\n        chunk.read(2)\n        chunk.seek(0)\n        chunk.read(2)\n        chunk.seek(1)\n        chunk.read(2)\n        self.assertEqual(self.amounts_seen, [2, -2, 2, -1, 2])\n\n\nclass TestStreamReaderProgress(unittest.TestCase):\n    def test_proxies_to_wrapped_stream(self):\n        original_stream = StringIO('foobarbaz')\n        wrapped = StreamReaderProgress(original_stream)\n        self.assertEqual(wrapped.read(), 'foobarbaz')\n\n    def test_callback_invoked(self):\n        amounts_seen = []\n\n        def callback(amount):\n            amounts_seen.append(amount)\n\n        original_stream = StringIO('foobarbaz')\n        wrapped = StreamReaderProgress(original_stream, callback)\n        self.assertEqual(wrapped.read(), 'foobarbaz')\n        self.assertEqual(amounts_seen, [9])\n\n\nclass TestMultipartUploader(unittest.TestCase):\n    def test_multipart_upload_uses_correct_client_calls(self):\n        client = mock.Mock()\n        uploader = MultipartUploader(\n            client,\n            TransferConfig(),\n            InMemoryOSLayer({'filename': b'foobar'}),\n            SequentialExecutor,\n        )\n        client.create_multipart_upload.return_value = {'UploadId': 'upload_id'}\n        client.upload_part.return_value = {'ETag': 'first'}\n\n        uploader.upload_file('filename', 'bucket', 'key', None, {})\n\n        # We need to check both the sequence of calls (create/upload/complete)\n        # as well as the params passed between the calls, including\n        # 1. The upload_id was plumbed through\n        # 2. The collected etags were added to the complete call.\n        client.create_multipart_upload.assert_called_with(\n            Bucket='bucket', Key='key'\n        )\n        # Should be two parts.\n        client.upload_part.assert_called_with(\n            Body=mock.ANY,\n            Bucket='bucket',\n            UploadId='upload_id',\n            Key='key',\n            PartNumber=1,\n        )\n        client.complete_multipart_upload.assert_called_with(\n            MultipartUpload={'Parts': [{'PartNumber': 1, 'ETag': 'first'}]},\n            Bucket='bucket',\n            UploadId='upload_id',\n            Key='key',\n        )\n\n    def test_multipart_upload_injects_proper_kwargs(self):\n        client = mock.Mock()\n        uploader = MultipartUploader(\n            client,\n            TransferConfig(),\n            InMemoryOSLayer({'filename': b'foobar'}),\n            SequentialExecutor,\n        )\n        client.create_multipart_upload.return_value = {'UploadId': 'upload_id'}\n        client.upload_part.return_value = {'ETag': 'first'}\n\n        extra_args = {\n            'SSECustomerKey': 'fakekey',\n            'SSECustomerAlgorithm': 'AES256',\n            'StorageClass': 'REDUCED_REDUNDANCY',\n        }\n        uploader.upload_file('filename', 'bucket', 'key', None, extra_args)\n\n        client.create_multipart_upload.assert_called_with(\n            Bucket='bucket',\n            Key='key',\n            # The initial call should inject all the storage class params.\n            SSECustomerKey='fakekey',\n            SSECustomerAlgorithm='AES256',\n            StorageClass='REDUCED_REDUNDANCY',\n        )\n        client.upload_part.assert_called_with(\n            Body=mock.ANY,\n            Bucket='bucket',\n            UploadId='upload_id',\n            Key='key',\n            PartNumber=1,\n            # We only have to forward certain **extra_args in subsequent\n            # UploadPart calls.\n            SSECustomerKey='fakekey',\n            SSECustomerAlgorithm='AES256',\n        )\n        client.complete_multipart_upload.assert_called_with(\n            MultipartUpload={'Parts': [{'PartNumber': 1, 'ETag': 'first'}]},\n            Bucket='bucket',\n            UploadId='upload_id',\n            Key='key',\n        )\n\n    def test_multipart_upload_is_aborted_on_error(self):\n        # If the create_multipart_upload succeeds and any upload_part\n        # fails, then abort_multipart_upload will be called.\n        client = mock.Mock()\n        uploader = MultipartUploader(\n            client,\n            TransferConfig(),\n            InMemoryOSLayer({'filename': b'foobar'}),\n            SequentialExecutor,\n        )\n        client.create_multipart_upload.return_value = {'UploadId': 'upload_id'}\n        client.upload_part.side_effect = Exception(\n            \"Some kind of error occurred.\"\n        )\n\n        with self.assertRaises(S3UploadFailedError):\n            uploader.upload_file('filename', 'bucket', 'key', None, {})\n\n        client.abort_multipart_upload.assert_called_with(\n            Bucket='bucket', Key='key', UploadId='upload_id'\n        )\n\n\nclass TestMultipartDownloader(unittest.TestCase):\n    maxDiff = None\n\n    def test_multipart_download_uses_correct_client_calls(self):\n        client = mock.Mock()\n        response_body = b'foobarbaz'\n        client.get_object.return_value = {'Body': BytesIO(response_body)}\n\n        downloader = MultipartDownloader(\n            client, TransferConfig(), InMemoryOSLayer({}), SequentialExecutor\n        )\n        downloader.download_file(\n            'bucket', 'key', 'filename', len(response_body), {}\n        )\n\n        client.get_object.assert_called_with(\n            Range='bytes=0-', Bucket='bucket', Key='key'\n        )\n\n    def test_multipart_download_with_multiple_parts(self):\n        client = mock.Mock()\n        response_body = b'foobarbaz'\n        client.get_object.return_value = {'Body': BytesIO(response_body)}\n        # For testing purposes, we're testing with a multipart threshold\n        # of 4 bytes and a chunksize of 4 bytes.  Given b'foobarbaz',\n        # this should result in 3 calls.  In python slices this would be:\n        # r[0:4], r[4:8], r[8:9].  But the Range param will be slightly\n        # different because they use inclusive ranges.\n        config = TransferConfig(multipart_threshold=4, multipart_chunksize=4)\n\n        downloader = MultipartDownloader(\n            client, config, InMemoryOSLayer({}), SequentialExecutor\n        )\n        downloader.download_file(\n            'bucket', 'key', 'filename', len(response_body), {}\n        )\n\n        # We're storing these in **extra because the assertEqual\n        # below is really about verifying we have the correct value\n        # for the Range param.\n        extra = {'Bucket': 'bucket', 'Key': 'key'}\n        self.assertEqual(\n            client.get_object.call_args_list,\n            # Note these are inclusive ranges.\n            [\n                mock.call(Range='bytes=0-3', **extra),\n                mock.call(Range='bytes=4-7', **extra),\n                mock.call(Range='bytes=8-', **extra),\n            ],\n        )\n\n    def test_retry_on_failures_from_stream_reads(self):\n        # If we get an exception during a call to the response body's .read()\n        # method, we should retry the request.\n        client = mock.Mock()\n        response_body = b'foobarbaz'\n        stream_with_errors = mock.Mock()\n        stream_with_errors.read.side_effect = [\n            socket.error(\"fake error\"),\n            response_body,\n        ]\n        client.get_object.return_value = {'Body': stream_with_errors}\n        config = TransferConfig(multipart_threshold=4, multipart_chunksize=4)\n\n        downloader = MultipartDownloader(\n            client, config, InMemoryOSLayer({}), SequentialExecutor\n        )\n        downloader.download_file(\n            'bucket', 'key', 'filename', len(response_body), {}\n        )\n\n        # We're storing these in **extra because the assertEqual\n        # below is really about verifying we have the correct value\n        # for the Range param.\n        extra = {'Bucket': 'bucket', 'Key': 'key'}\n        self.assertEqual(\n            client.get_object.call_args_list,\n            # The first call to range=0-3 fails because of the\n            # side_effect above where we make the .read() raise a\n            # socket.error.\n            # The second call to range=0-3 then succeeds.\n            [\n                mock.call(Range='bytes=0-3', **extra),\n                mock.call(Range='bytes=0-3', **extra),\n                mock.call(Range='bytes=4-7', **extra),\n                mock.call(Range='bytes=8-', **extra),\n            ],\n        )\n\n    def test_exception_raised_on_exceeded_retries(self):\n        client = mock.Mock()\n        response_body = b'foobarbaz'\n        stream_with_errors = mock.Mock()\n        stream_with_errors.read.side_effect = socket.error(\"fake error\")\n        client.get_object.return_value = {'Body': stream_with_errors}\n        config = TransferConfig(multipart_threshold=4, multipart_chunksize=4)\n\n        downloader = MultipartDownloader(\n            client, config, InMemoryOSLayer({}), SequentialExecutor\n        )\n        with self.assertRaises(RetriesExceededError):\n            downloader.download_file(\n                'bucket', 'key', 'filename', len(response_body), {}\n            )\n\n    def test_io_thread_failure_triggers_shutdown(self):\n        client = mock.Mock()\n        response_body = b'foobarbaz'\n        client.get_object.return_value = {'Body': BytesIO(response_body)}\n        os_layer = mock.Mock()\n        mock_fileobj = mock.MagicMock()\n        mock_fileobj.__enter__.return_value = mock_fileobj\n        mock_fileobj.write.side_effect = Exception(\"fake IO error\")\n        os_layer.open.return_value = mock_fileobj\n\n        downloader = MultipartDownloader(\n            client, TransferConfig(), os_layer, SequentialExecutor\n        )\n        # We're verifying that the exception raised from the IO future\n        # propagates back up via download_file().\n        with self.assertRaisesRegex(Exception, \"fake IO error\"):\n            downloader.download_file(\n                'bucket', 'key', 'filename', len(response_body), {}\n            )\n\n    def test_download_futures_fail_triggers_shutdown(self):\n        class FailedDownloadParts(SequentialExecutor):\n            def __init__(self, max_workers):\n                self.is_first = True\n\n            def submit(self, function):\n                future = futures.Future()\n                if self.is_first:\n                    # This is the download_parts_thread.\n                    future.set_exception(\n                        Exception(\"fake download parts error\")\n                    )\n                    self.is_first = False\n                return future\n\n        client = mock.Mock()\n        response_body = b'foobarbaz'\n        client.get_object.return_value = {'Body': BytesIO(response_body)}\n\n        downloader = MultipartDownloader(\n            client, TransferConfig(), InMemoryOSLayer({}), FailedDownloadParts\n        )\n        with self.assertRaisesRegex(Exception, \"fake download parts error\"):\n            downloader.download_file(\n                'bucket', 'key', 'filename', len(response_body), {}\n            )\n\n\nclass TestS3Transfer(unittest.TestCase):\n    def setUp(self):\n        self.client = mock.Mock()\n        self.random_file_patch = mock.patch('s3transfer.random_file_extension')\n        self.random_file = self.random_file_patch.start()\n        self.random_file.return_value = 'RANDOM'\n\n    def tearDown(self):\n        self.random_file_patch.stop()\n\n    def test_callback_handlers_register_on_put_item(self):\n        osutil = InMemoryOSLayer({'smallfile': b'foobar'})\n        transfer = S3Transfer(self.client, osutil=osutil)\n        transfer.upload_file('smallfile', 'bucket', 'key')\n        events = self.client.meta.events\n        events.register_first.assert_called_with(\n            'request-created.s3',\n            disable_upload_callbacks,\n            unique_id='s3upload-callback-disable',\n        )\n        events.register_last.assert_called_with(\n            'request-created.s3',\n            enable_upload_callbacks,\n            unique_id='s3upload-callback-enable',\n        )\n\n    def test_upload_below_multipart_threshold_uses_put_object(self):\n        fake_files = {\n            'smallfile': b'foobar',\n        }\n        osutil = InMemoryOSLayer(fake_files)\n        transfer = S3Transfer(self.client, osutil=osutil)\n        transfer.upload_file('smallfile', 'bucket', 'key')\n        self.client.put_object.assert_called_with(\n            Bucket='bucket', Key='key', Body=mock.ANY\n        )\n\n    def test_extra_args_on_uploaded_passed_to_api_call(self):\n        extra_args = {'ACL': 'public-read'}\n        fake_files = {'smallfile': b'hello world'}\n        osutil = InMemoryOSLayer(fake_files)\n        transfer = S3Transfer(self.client, osutil=osutil)\n        transfer.upload_file(\n            'smallfile', 'bucket', 'key', extra_args=extra_args\n        )\n        self.client.put_object.assert_called_with(\n            Bucket='bucket', Key='key', Body=mock.ANY, ACL='public-read'\n        )\n\n    def test_uses_multipart_upload_when_over_threshold(self):\n        with mock.patch('s3transfer.MultipartUploader') as uploader:\n            fake_files = {\n                'smallfile': b'foobar',\n            }\n            osutil = InMemoryOSLayer(fake_files)\n            config = TransferConfig(\n                multipart_threshold=2, multipart_chunksize=2\n            )\n            transfer = S3Transfer(self.client, osutil=osutil, config=config)\n            transfer.upload_file('smallfile', 'bucket', 'key')\n\n            uploader.return_value.upload_file.assert_called_with(\n                'smallfile', 'bucket', 'key', None, {}\n            )\n\n    def test_uses_multipart_download_when_over_threshold(self):\n        with mock.patch('s3transfer.MultipartDownloader') as downloader:\n            osutil = InMemoryOSLayer({})\n            over_multipart_threshold = 100 * 1024 * 1024\n            transfer = S3Transfer(self.client, osutil=osutil)\n            callback = mock.sentinel.CALLBACK\n            self.client.head_object.return_value = {\n                'ContentLength': over_multipart_threshold,\n            }\n            transfer.download_file(\n                'bucket', 'key', 'filename', callback=callback\n            )\n\n            downloader.return_value.download_file.assert_called_with(\n                # Note how we're downloading to a temporary random file.\n                'bucket',\n                'key',\n                'filename.RANDOM',\n                over_multipart_threshold,\n                {},\n                callback,\n            )\n\n    def test_download_file_with_invalid_extra_args(self):\n        below_threshold = 20\n        osutil = InMemoryOSLayer({})\n        transfer = S3Transfer(self.client, osutil=osutil)\n        self.client.head_object.return_value = {\n            'ContentLength': below_threshold\n        }\n        with self.assertRaises(ValueError):\n            transfer.download_file(\n                'bucket',\n                'key',\n                '/tmp/smallfile',\n                extra_args={'BadValue': 'foo'},\n            )\n\n    def test_upload_file_with_invalid_extra_args(self):\n        osutil = InMemoryOSLayer({})\n        transfer = S3Transfer(self.client, osutil=osutil)\n        bad_args = {\"WebsiteRedirectLocation\": \"/foo\"}\n        with self.assertRaises(ValueError):\n            transfer.upload_file(\n                'bucket', 'key', '/tmp/smallfile', extra_args=bad_args\n            )\n\n    def test_download_file_fowards_extra_args(self):\n        extra_args = {\n            'SSECustomerKey': 'foo',\n            'SSECustomerAlgorithm': 'AES256',\n        }\n        below_threshold = 20\n        osutil = InMemoryOSLayer({'smallfile': b'hello world'})\n        transfer = S3Transfer(self.client, osutil=osutil)\n        self.client.head_object.return_value = {\n            'ContentLength': below_threshold\n        }\n        self.client.get_object.return_value = {'Body': BytesIO(b'foobar')}\n        transfer.download_file(\n            'bucket', 'key', '/tmp/smallfile', extra_args=extra_args\n        )\n\n        # Note that we need to invoke the HeadObject call\n        # and the PutObject call with the extra_args.\n        # This is necessary.  Trying to HeadObject an SSE object\n        # will return a 400 if you don't provide the required\n        # params.\n        self.client.get_object.assert_called_with(\n            Bucket='bucket',\n            Key='key',\n            SSECustomerAlgorithm='AES256',\n            SSECustomerKey='foo',\n        )\n\n    def test_get_object_stream_is_retried_and_succeeds(self):\n        below_threshold = 20\n        osutil = InMemoryOSLayer({'smallfile': b'hello world'})\n        transfer = S3Transfer(self.client, osutil=osutil)\n        self.client.head_object.return_value = {\n            'ContentLength': below_threshold\n        }\n        self.client.get_object.side_effect = [\n            # First request fails.\n            socket.error(\"fake error\"),\n            # Second succeeds.\n            {'Body': BytesIO(b'foobar')},\n        ]\n        transfer.download_file('bucket', 'key', '/tmp/smallfile')\n\n        self.assertEqual(self.client.get_object.call_count, 2)\n\n    def test_get_object_stream_uses_all_retries_and_errors_out(self):\n        below_threshold = 20\n        osutil = InMemoryOSLayer({})\n        transfer = S3Transfer(self.client, osutil=osutil)\n        self.client.head_object.return_value = {\n            'ContentLength': below_threshold\n        }\n        # Here we're raising an exception every single time, which\n        # will exhaust our retry count and propagate a\n        # RetriesExceededError.\n        self.client.get_object.side_effect = socket.error(\"fake error\")\n        with self.assertRaises(RetriesExceededError):\n            transfer.download_file('bucket', 'key', 'smallfile')\n\n        self.assertEqual(self.client.get_object.call_count, 5)\n        # We should have also cleaned up the in progress file\n        # we were downloading to.\n        self.assertEqual(osutil.filemap, {})\n\n    def test_download_below_multipart_threshold(self):\n        below_threshold = 20\n        osutil = InMemoryOSLayer({'smallfile': b'hello world'})\n        transfer = S3Transfer(self.client, osutil=osutil)\n        self.client.head_object.return_value = {\n            'ContentLength': below_threshold\n        }\n        self.client.get_object.return_value = {'Body': BytesIO(b'foobar')}\n        transfer.download_file('bucket', 'key', 'smallfile')\n\n        self.client.get_object.assert_called_with(Bucket='bucket', Key='key')\n\n    def test_can_create_with_just_client(self):\n        transfer = S3Transfer(client=mock.Mock())\n        self.assertIsInstance(transfer, S3Transfer)\n\n\nclass TestShutdownQueue(unittest.TestCase):\n    def test_handles_normal_put_get_requests(self):\n        q = ShutdownQueue()\n        q.put('foo')\n        self.assertEqual(q.get(), 'foo')\n\n    def test_put_raises_error_on_shutdown(self):\n        q = ShutdownQueue()\n        q.trigger_shutdown()\n        with self.assertRaises(QueueShutdownError):\n            q.put('foo')\n\n\nclass TestRandomFileExtension(unittest.TestCase):\n    def test_has_proper_length(self):\n        self.assertEqual(len(random_file_extension(num_digits=4)), 4)\n\n\nclass TestCallbackHandlers(unittest.TestCase):\n    def setUp(self):\n        self.request = mock.Mock()\n\n    def test_disable_request_on_put_object(self):\n        disable_upload_callbacks(self.request, 'PutObject')\n        self.request.body.disable_callback.assert_called_with()\n\n    def test_disable_request_on_upload_part(self):\n        disable_upload_callbacks(self.request, 'UploadPart')\n        self.request.body.disable_callback.assert_called_with()\n\n    def test_enable_object_on_put_object(self):\n        enable_upload_callbacks(self.request, 'PutObject')\n        self.request.body.enable_callback.assert_called_with()\n\n    def test_enable_object_on_upload_part(self):\n        enable_upload_callbacks(self.request, 'UploadPart')\n        self.request.body.enable_callback.assert_called_with()\n\n    def test_dont_disable_if_missing_interface(self):\n        del self.request.body.disable_callback\n        disable_upload_callbacks(self.request, 'PutObject')\n        self.assertEqual(self.request.body.method_calls, [])\n\n    def test_dont_enable_if_missing_interface(self):\n        del self.request.body.enable_callback\n        enable_upload_callbacks(self.request, 'PutObject')\n        self.assertEqual(self.request.body.method_calls, [])\n\n    def test_dont_disable_if_wrong_operation(self):\n        disable_upload_callbacks(self.request, 'OtherOperation')\n        self.assertFalse(self.request.body.disable_callback.called)\n\n    def test_dont_enable_if_wrong_operation(self):\n        enable_upload_callbacks(self.request, 'OtherOperation')\n        self.assertFalse(self.request.body.enable_callback.called)\n", "tests/unit/test_futures.py": "# Copyright 2016 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport os\nimport sys\nimport time\nimport traceback\nfrom concurrent.futures import ThreadPoolExecutor\n\nfrom s3transfer.exceptions import (\n    CancelledError,\n    FatalError,\n    TransferNotDoneError,\n)\nfrom s3transfer.futures import (\n    BaseExecutor,\n    BoundedExecutor,\n    ExecutorFuture,\n    NonThreadedExecutor,\n    NonThreadedExecutorFuture,\n    TransferCoordinator,\n    TransferFuture,\n    TransferMeta,\n)\nfrom s3transfer.tasks import Task\nfrom s3transfer.utils import (\n    FunctionContainer,\n    NoResourcesAvailable,\n    TaskSemaphore,\n)\nfrom tests import (\n    RecordingExecutor,\n    TransferCoordinatorWithInterrupt,\n    mock,\n    unittest,\n)\n\n\ndef return_call_args(*args, **kwargs):\n    return args, kwargs\n\n\ndef raise_exception(exception):\n    raise exception\n\n\ndef get_exc_info(exception):\n    try:\n        raise_exception(exception)\n    except Exception:\n        return sys.exc_info()\n\n\nclass RecordingTransferCoordinator(TransferCoordinator):\n    def __init__(self):\n        self.all_transfer_futures_ever_associated = set()\n        super().__init__()\n\n    def add_associated_future(self, future):\n        self.all_transfer_futures_ever_associated.add(future)\n        super().add_associated_future(future)\n\n\nclass ReturnFooTask(Task):\n    def _main(self, **kwargs):\n        return 'foo'\n\n\nclass SleepTask(Task):\n    def _main(self, sleep_time, **kwargs):\n        time.sleep(sleep_time)\n\n\nclass TestTransferFuture(unittest.TestCase):\n    def setUp(self):\n        self.meta = TransferMeta()\n        self.coordinator = TransferCoordinator()\n        self.future = self._get_transfer_future()\n\n    def _get_transfer_future(self, **kwargs):\n        components = {\n            'meta': self.meta,\n            'coordinator': self.coordinator,\n        }\n        for component_name, component in kwargs.items():\n            components[component_name] = component\n        return TransferFuture(**components)\n\n    def test_meta(self):\n        self.assertIs(self.future.meta, self.meta)\n\n    def test_done(self):\n        self.assertFalse(self.future.done())\n        self.coordinator.set_result(None)\n        self.assertTrue(self.future.done())\n\n    def test_result(self):\n        result = 'foo'\n        self.coordinator.set_result(result)\n        self.coordinator.announce_done()\n        self.assertEqual(self.future.result(), result)\n\n    def test_keyboard_interrupt_on_result_does_not_block(self):\n        # This should raise a KeyboardInterrupt when result is called on it.\n        self.coordinator = TransferCoordinatorWithInterrupt()\n        self.future = self._get_transfer_future()\n\n        # result() should not block and immediately raise the keyboard\n        # interrupt exception.\n        with self.assertRaises(KeyboardInterrupt):\n            self.future.result()\n\n    def test_cancel(self):\n        self.future.cancel()\n        self.assertTrue(self.future.done())\n        self.assertEqual(self.coordinator.status, 'cancelled')\n\n    def test_set_exception(self):\n        # Set the result such that there is no exception\n        self.coordinator.set_result('result')\n        self.coordinator.announce_done()\n        self.assertEqual(self.future.result(), 'result')\n\n        self.future.set_exception(ValueError())\n        with self.assertRaises(ValueError):\n            self.future.result()\n\n    def test_set_exception_only_after_done(self):\n        with self.assertRaises(TransferNotDoneError):\n            self.future.set_exception(ValueError())\n\n        self.coordinator.set_result('result')\n        self.coordinator.announce_done()\n        self.future.set_exception(ValueError())\n        with self.assertRaises(ValueError):\n            self.future.result()\n\n\nclass TestTransferMeta(unittest.TestCase):\n    def setUp(self):\n        self.transfer_meta = TransferMeta()\n\n    def test_size(self):\n        self.assertEqual(self.transfer_meta.size, None)\n        self.transfer_meta.provide_transfer_size(5)\n        self.assertEqual(self.transfer_meta.size, 5)\n\n    def test_call_args(self):\n        call_args = object()\n        transfer_meta = TransferMeta(call_args)\n        # Assert the that call args provided is the same as is returned\n        self.assertIs(transfer_meta.call_args, call_args)\n\n    def test_transfer_id(self):\n        transfer_meta = TransferMeta(transfer_id=1)\n        self.assertEqual(transfer_meta.transfer_id, 1)\n\n    def test_user_context(self):\n        self.transfer_meta.user_context['foo'] = 'bar'\n        self.assertEqual(self.transfer_meta.user_context, {'foo': 'bar'})\n\n\nclass TestTransferCoordinator(unittest.TestCase):\n    def setUp(self):\n        self.transfer_coordinator = TransferCoordinator()\n\n    def test_transfer_id(self):\n        transfer_coordinator = TransferCoordinator(transfer_id=1)\n        self.assertEqual(transfer_coordinator.transfer_id, 1)\n\n    def test_repr(self):\n        transfer_coordinator = TransferCoordinator(transfer_id=1)\n        self.assertEqual(\n            repr(transfer_coordinator), 'TransferCoordinator(transfer_id=1)'\n        )\n\n    def test_initial_status(self):\n        # A TransferCoordinator with no progress should have the status\n        # of not-started\n        self.assertEqual(self.transfer_coordinator.status, 'not-started')\n\n    def test_set_status_to_queued(self):\n        self.transfer_coordinator.set_status_to_queued()\n        self.assertEqual(self.transfer_coordinator.status, 'queued')\n\n    def test_cannot_set_status_to_queued_from_done_state(self):\n        self.transfer_coordinator.set_exception(RuntimeError)\n        with self.assertRaises(RuntimeError):\n            self.transfer_coordinator.set_status_to_queued()\n\n    def test_status_running(self):\n        self.transfer_coordinator.set_status_to_running()\n        self.assertEqual(self.transfer_coordinator.status, 'running')\n\n    def test_cannot_set_status_to_running_from_done_state(self):\n        self.transfer_coordinator.set_exception(RuntimeError)\n        with self.assertRaises(RuntimeError):\n            self.transfer_coordinator.set_status_to_running()\n\n    def test_set_result(self):\n        success_result = 'foo'\n        self.transfer_coordinator.set_result(success_result)\n        self.transfer_coordinator.announce_done()\n        # Setting result should result in a success state and the return value\n        # that was set.\n        self.assertEqual(self.transfer_coordinator.status, 'success')\n        self.assertEqual(self.transfer_coordinator.result(), success_result)\n\n    def test_set_exception(self):\n        exception_result = RuntimeError\n        self.transfer_coordinator.set_exception(exception_result)\n        self.transfer_coordinator.announce_done()\n        # Setting an exception should result in a failed state and the return\n        # value should be the raised exception\n        self.assertEqual(self.transfer_coordinator.status, 'failed')\n        self.assertEqual(self.transfer_coordinator.exception, exception_result)\n        with self.assertRaises(exception_result):\n            self.transfer_coordinator.result()\n\n    def test_exception_cannot_override_done_state(self):\n        self.transfer_coordinator.set_result('foo')\n        self.transfer_coordinator.set_exception(RuntimeError)\n        # It status should be success even after the exception is set because\n        # success is a done state.\n        self.assertEqual(self.transfer_coordinator.status, 'success')\n\n    def test_exception_can_override_done_state_with_override_flag(self):\n        self.transfer_coordinator.set_result('foo')\n        self.transfer_coordinator.set_exception(RuntimeError, override=True)\n        self.assertEqual(self.transfer_coordinator.status, 'failed')\n\n    def test_cancel(self):\n        self.assertEqual(self.transfer_coordinator.status, 'not-started')\n        self.transfer_coordinator.cancel()\n        # This should set the state to cancelled and raise the CancelledError\n        # exception and should have also set the done event so that result()\n        # is no longer set.\n        self.assertEqual(self.transfer_coordinator.status, 'cancelled')\n        with self.assertRaises(CancelledError):\n            self.transfer_coordinator.result()\n\n    def test_cancel_can_run_done_callbacks_that_uses_result(self):\n        exceptions = []\n\n        def capture_exception(transfer_coordinator, captured_exceptions):\n            try:\n                transfer_coordinator.result()\n            except Exception as e:\n                captured_exceptions.append(e)\n\n        self.assertEqual(self.transfer_coordinator.status, 'not-started')\n        self.transfer_coordinator.add_done_callback(\n            capture_exception, self.transfer_coordinator, exceptions\n        )\n        self.transfer_coordinator.cancel()\n\n        self.assertEqual(len(exceptions), 1)\n        self.assertIsInstance(exceptions[0], CancelledError)\n\n    def test_cancel_with_message(self):\n        message = 'my message'\n        self.transfer_coordinator.cancel(message)\n        self.transfer_coordinator.announce_done()\n        with self.assertRaisesRegex(CancelledError, message):\n            self.transfer_coordinator.result()\n\n    def test_cancel_with_provided_exception(self):\n        message = 'my message'\n        self.transfer_coordinator.cancel(message, exc_type=FatalError)\n        self.transfer_coordinator.announce_done()\n        with self.assertRaisesRegex(FatalError, message):\n            self.transfer_coordinator.result()\n\n    def test_cancel_cannot_override_done_state(self):\n        self.transfer_coordinator.set_result('foo')\n        self.transfer_coordinator.cancel()\n        # It status should be success even after cancel is called because\n        # success is a done state.\n        self.assertEqual(self.transfer_coordinator.status, 'success')\n\n    def test_set_result_can_override_cancel(self):\n        self.transfer_coordinator.cancel()\n        # Result setting should override any cancel or set exception as this\n        # is always invoked by the final task.\n        self.transfer_coordinator.set_result('foo')\n        self.transfer_coordinator.announce_done()\n        self.assertEqual(self.transfer_coordinator.status, 'success')\n\n    def test_submit(self):\n        # Submit a callable to the transfer coordinator. It should submit it\n        # to the executor.\n        executor = RecordingExecutor(\n            BoundedExecutor(1, 1, {'my-tag': TaskSemaphore(1)})\n        )\n        task = ReturnFooTask(self.transfer_coordinator)\n        future = self.transfer_coordinator.submit(executor, task, tag='my-tag')\n        executor.shutdown()\n        # Make sure the future got submit and executed as well by checking its\n        # result value which should include the provided future tag.\n        self.assertEqual(\n            executor.submissions,\n            [{'block': True, 'tag': 'my-tag', 'task': task}],\n        )\n        self.assertEqual(future.result(), 'foo')\n\n    def test_association_and_disassociation_on_submit(self):\n        self.transfer_coordinator = RecordingTransferCoordinator()\n\n        # Submit a callable to the transfer coordinator.\n        executor = BoundedExecutor(1, 1)\n        task = ReturnFooTask(self.transfer_coordinator)\n        future = self.transfer_coordinator.submit(executor, task)\n        executor.shutdown()\n\n        # Make sure the future that got submitted was associated to the\n        # transfer future at some point.\n        self.assertEqual(\n            self.transfer_coordinator.all_transfer_futures_ever_associated,\n            {future},\n        )\n\n        # Make sure the future got disassociated once the future is now done\n        # by looking at the currently associated futures.\n        self.assertEqual(self.transfer_coordinator.associated_futures, set())\n\n    def test_done(self):\n        # These should result in not done state:\n        # queued\n        self.assertFalse(self.transfer_coordinator.done())\n        # running\n        self.transfer_coordinator.set_status_to_running()\n        self.assertFalse(self.transfer_coordinator.done())\n\n        # These should result in done state:\n        # failed\n        self.transfer_coordinator.set_exception(Exception)\n        self.assertTrue(self.transfer_coordinator.done())\n\n        # success\n        self.transfer_coordinator.set_result('foo')\n        self.assertTrue(self.transfer_coordinator.done())\n\n        # cancelled\n        self.transfer_coordinator.cancel()\n        self.assertTrue(self.transfer_coordinator.done())\n\n    def test_result_waits_until_done(self):\n        execution_order = []\n\n        def sleep_then_set_result(transfer_coordinator, execution_order):\n            time.sleep(0.05)\n            execution_order.append('setting_result')\n            transfer_coordinator.set_result(None)\n            self.transfer_coordinator.announce_done()\n\n        with ThreadPoolExecutor(max_workers=1) as executor:\n            executor.submit(\n                sleep_then_set_result,\n                self.transfer_coordinator,\n                execution_order,\n            )\n            self.transfer_coordinator.result()\n            execution_order.append('after_result')\n\n        # The result() call should have waited until the other thread set\n        # the result after sleeping for 0.05 seconds.\n        self.assertTrue(execution_order, ['setting_result', 'after_result'])\n\n    def test_failure_cleanups(self):\n        args = (1, 2)\n        kwargs = {'foo': 'bar'}\n\n        second_args = (2, 4)\n        second_kwargs = {'biz': 'baz'}\n\n        self.transfer_coordinator.add_failure_cleanup(\n            return_call_args, *args, **kwargs\n        )\n        self.transfer_coordinator.add_failure_cleanup(\n            return_call_args, *second_args, **second_kwargs\n        )\n\n        # Ensure the callbacks got added.\n        self.assertEqual(len(self.transfer_coordinator.failure_cleanups), 2)\n\n        result_list = []\n        # Ensure they will get called in the correct order.\n        for cleanup in self.transfer_coordinator.failure_cleanups:\n            result_list.append(cleanup())\n        self.assertEqual(\n            result_list, [(args, kwargs), (second_args, second_kwargs)]\n        )\n\n    def test_associated_futures(self):\n        first_future = object()\n        # Associate one future to the transfer\n        self.transfer_coordinator.add_associated_future(first_future)\n        associated_futures = self.transfer_coordinator.associated_futures\n        # The first future should be in the returned list of futures.\n        self.assertEqual(associated_futures, {first_future})\n\n        second_future = object()\n        # Associate another future to the transfer.\n        self.transfer_coordinator.add_associated_future(second_future)\n        # The association should not have mutated the returned list from\n        # before.\n        self.assertEqual(associated_futures, {first_future})\n\n        # Both futures should be in the returned list.\n        self.assertEqual(\n            self.transfer_coordinator.associated_futures,\n            {first_future, second_future},\n        )\n\n    def test_done_callbacks_on_done(self):\n        done_callback_invocations = []\n        callback = FunctionContainer(\n            done_callback_invocations.append, 'done callback called'\n        )\n\n        # Add the done callback to the transfer.\n        self.transfer_coordinator.add_done_callback(callback)\n\n        # Announce that the transfer is done. This should invoke the done\n        # callback.\n        self.transfer_coordinator.announce_done()\n        self.assertEqual(done_callback_invocations, ['done callback called'])\n\n        # If done is announced again, we should not invoke the callback again\n        # because done has already been announced and thus the callback has\n        # been ran as well.\n        self.transfer_coordinator.announce_done()\n        self.assertEqual(done_callback_invocations, ['done callback called'])\n\n    def test_failure_cleanups_on_done(self):\n        cleanup_invocations = []\n        callback = FunctionContainer(\n            cleanup_invocations.append, 'cleanup called'\n        )\n\n        # Add the failure cleanup to the transfer.\n        self.transfer_coordinator.add_failure_cleanup(callback)\n\n        # Announce that the transfer is done. This should invoke the failure\n        # cleanup.\n        self.transfer_coordinator.announce_done()\n        self.assertEqual(cleanup_invocations, ['cleanup called'])\n\n        # If done is announced again, we should not invoke the cleanup again\n        # because done has already been announced and thus the cleanup has\n        # been ran as well.\n        self.transfer_coordinator.announce_done()\n        self.assertEqual(cleanup_invocations, ['cleanup called'])\n\n\nclass TestBoundedExecutor(unittest.TestCase):\n    def setUp(self):\n        self.coordinator = TransferCoordinator()\n        self.tag_semaphores = {}\n        self.executor = self.get_executor()\n\n    def get_executor(self, max_size=1, max_num_threads=1):\n        return BoundedExecutor(max_size, max_num_threads, self.tag_semaphores)\n\n    def get_task(self, task_cls, main_kwargs=None):\n        return task_cls(self.coordinator, main_kwargs=main_kwargs)\n\n    def get_sleep_task(self, sleep_time=0.01):\n        return self.get_task(SleepTask, main_kwargs={'sleep_time': sleep_time})\n\n    def add_semaphore(self, task_tag, count):\n        self.tag_semaphores[task_tag] = TaskSemaphore(count)\n\n    def assert_submit_would_block(self, task, tag=None):\n        with self.assertRaises(NoResourcesAvailable):\n            self.executor.submit(task, tag=tag, block=False)\n\n    def assert_submit_would_not_block(self, task, tag=None, **kwargs):\n        try:\n            self.executor.submit(task, tag=tag, block=False)\n        except NoResourcesAvailable:\n            self.fail(\n                'Task {} should not have been blocked. Caused by:\\n{}'.format(\n                    task, traceback.format_exc()\n                )\n            )\n\n    def add_done_callback_to_future(self, future, fn, *args, **kwargs):\n        callback_for_future = FunctionContainer(fn, *args, **kwargs)\n        future.add_done_callback(callback_for_future)\n\n    def test_submit_single_task(self):\n        # Ensure we can submit a task to the executor\n        task = self.get_task(ReturnFooTask)\n        future = self.executor.submit(task)\n\n        # Ensure what we get back is a Future\n        self.assertIsInstance(future, ExecutorFuture)\n        # Ensure the callable got executed.\n        self.assertEqual(future.result(), 'foo')\n\n    @unittest.skipIf(\n        os.environ.get('USE_SERIAL_EXECUTOR'),\n        \"Not supported with serial executor tests\",\n    )\n    def test_executor_blocks_on_full_capacity(self):\n        first_task = self.get_sleep_task()\n        second_task = self.get_sleep_task()\n        self.executor.submit(first_task)\n        # The first task should be sleeping for a substantial period of\n        # time such that on the submission of the second task, it will\n        # raise an error saying that it cannot be submitted as the max\n        # capacity of the semaphore is one.\n        self.assert_submit_would_block(second_task)\n\n    def test_executor_clears_capacity_on_done_tasks(self):\n        first_task = self.get_sleep_task()\n        second_task = self.get_task(ReturnFooTask)\n\n        # Submit a task.\n        future = self.executor.submit(first_task)\n\n        # Submit a new task when the first task finishes. This should not get\n        # blocked because the first task should have finished clearing up\n        # capacity.\n        self.add_done_callback_to_future(\n            future, self.assert_submit_would_not_block, second_task\n        )\n\n        # Wait for it to complete.\n        self.executor.shutdown()\n\n    @unittest.skipIf(\n        os.environ.get('USE_SERIAL_EXECUTOR'),\n        \"Not supported with serial executor tests\",\n    )\n    def test_would_not_block_when_full_capacity_in_other_semaphore(self):\n        first_task = self.get_sleep_task()\n\n        # Now let's create a new task with a tag and so it uses different\n        # semaphore.\n        task_tag = 'other'\n        other_task = self.get_sleep_task()\n        self.add_semaphore(task_tag, 1)\n\n        # Submit the normal first task\n        self.executor.submit(first_task)\n\n        # Even though The first task should be sleeping for a substantial\n        # period of time, the submission of the second task should not\n        # raise an error because it should use a different semaphore\n        self.assert_submit_would_not_block(other_task, task_tag)\n\n        # Another submission of the other task though should raise\n        # an exception as the capacity is equal to one for that tag.\n        self.assert_submit_would_block(other_task, task_tag)\n\n    def test_shutdown(self):\n        slow_task = self.get_sleep_task()\n        future = self.executor.submit(slow_task)\n        self.executor.shutdown()\n        # Ensure that the shutdown waits until the task is done\n        self.assertTrue(future.done())\n\n    @unittest.skipIf(\n        os.environ.get('USE_SERIAL_EXECUTOR'),\n        \"Not supported with serial executor tests\",\n    )\n    def test_shutdown_no_wait(self):\n        slow_task = self.get_sleep_task()\n        future = self.executor.submit(slow_task)\n        self.executor.shutdown(False)\n        # Ensure that the shutdown returns immediately even if the task is\n        # not done, which it should not be because it it slow.\n        self.assertFalse(future.done())\n\n    def test_replace_underlying_executor(self):\n        mocked_executor_cls = mock.Mock(BaseExecutor)\n        executor = BoundedExecutor(10, 1, {}, mocked_executor_cls)\n        executor.submit(self.get_task(ReturnFooTask))\n        self.assertTrue(mocked_executor_cls.return_value.submit.called)\n\n\nclass TestExecutorFuture(unittest.TestCase):\n    def test_result(self):\n        with ThreadPoolExecutor(max_workers=1) as executor:\n            future = executor.submit(return_call_args, 'foo', biz='baz')\n            wrapped_future = ExecutorFuture(future)\n        self.assertEqual(wrapped_future.result(), (('foo',), {'biz': 'baz'}))\n\n    def test_done(self):\n        with ThreadPoolExecutor(max_workers=1) as executor:\n            future = executor.submit(return_call_args, 'foo', biz='baz')\n            wrapped_future = ExecutorFuture(future)\n        self.assertTrue(wrapped_future.done())\n\n    def test_add_done_callback(self):\n        done_callbacks = []\n        with ThreadPoolExecutor(max_workers=1) as executor:\n            future = executor.submit(return_call_args, 'foo', biz='baz')\n            wrapped_future = ExecutorFuture(future)\n            wrapped_future.add_done_callback(\n                FunctionContainer(done_callbacks.append, 'called')\n            )\n        self.assertEqual(done_callbacks, ['called'])\n\n\nclass TestNonThreadedExecutor(unittest.TestCase):\n    def test_submit(self):\n        executor = NonThreadedExecutor()\n        future = executor.submit(return_call_args, 1, 2, foo='bar')\n        self.assertIsInstance(future, NonThreadedExecutorFuture)\n        self.assertEqual(future.result(), ((1, 2), {'foo': 'bar'}))\n\n    def test_submit_with_exception(self):\n        executor = NonThreadedExecutor()\n        future = executor.submit(raise_exception, RuntimeError())\n        self.assertIsInstance(future, NonThreadedExecutorFuture)\n        with self.assertRaises(RuntimeError):\n            future.result()\n\n    def test_submit_with_exception_and_captures_info(self):\n        exception = ValueError('message')\n        tb = get_exc_info(exception)[2]\n        future = NonThreadedExecutor().submit(raise_exception, exception)\n        try:\n            future.result()\n            # An exception should have been raised\n            self.fail('Future should have raised a ValueError')\n        except ValueError:\n            actual_tb = sys.exc_info()[2]\n            last_frame = traceback.extract_tb(actual_tb)[-1]\n            last_expected_frame = traceback.extract_tb(tb)[-1]\n            self.assertEqual(last_frame, last_expected_frame)\n\n\nclass TestNonThreadedExecutorFuture(unittest.TestCase):\n    def setUp(self):\n        self.future = NonThreadedExecutorFuture()\n\n    def test_done_starts_false(self):\n        self.assertFalse(self.future.done())\n\n    def test_done_after_setting_result(self):\n        self.future.set_result('result')\n        self.assertTrue(self.future.done())\n\n    def test_done_after_setting_exception(self):\n        self.future.set_exception_info(Exception(), None)\n        self.assertTrue(self.future.done())\n\n    def test_result(self):\n        self.future.set_result('result')\n        self.assertEqual(self.future.result(), 'result')\n\n    def test_exception_result(self):\n        exception = ValueError('message')\n        self.future.set_exception_info(exception, None)\n        with self.assertRaisesRegex(ValueError, 'message'):\n            self.future.result()\n\n    def test_exception_result_doesnt_modify_last_frame(self):\n        exception = ValueError('message')\n        tb = get_exc_info(exception)[2]\n        self.future.set_exception_info(exception, tb)\n        try:\n            self.future.result()\n            # An exception should have been raised\n            self.fail()\n        except ValueError:\n            actual_tb = sys.exc_info()[2]\n            last_frame = traceback.extract_tb(actual_tb)[-1]\n            last_expected_frame = traceback.extract_tb(tb)[-1]\n            self.assertEqual(last_frame, last_expected_frame)\n\n    def test_done_callback(self):\n        done_futures = []\n        self.future.add_done_callback(done_futures.append)\n        self.assertEqual(done_futures, [])\n        self.future.set_result('result')\n        self.assertEqual(done_futures, [self.future])\n\n    def test_done_callback_after_done(self):\n        self.future.set_result('result')\n        done_futures = []\n        self.future.add_done_callback(done_futures.append)\n        self.assertEqual(done_futures, [self.future])\n", "tests/unit/test_copies.py": "# Copyright 2016 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nfrom s3transfer.copies import CopyObjectTask, CopyPartTask\nfrom tests import BaseTaskTest, RecordingSubscriber\n\n\nclass BaseCopyTaskTest(BaseTaskTest):\n    def setUp(self):\n        super().setUp()\n        self.bucket = 'mybucket'\n        self.key = 'mykey'\n        self.copy_source = {'Bucket': 'mysourcebucket', 'Key': 'mysourcekey'}\n        self.extra_args = {}\n        self.callbacks = []\n        self.size = 5\n\n\nclass TestCopyObjectTask(BaseCopyTaskTest):\n    def get_copy_task(self, **kwargs):\n        default_kwargs = {\n            'client': self.client,\n            'copy_source': self.copy_source,\n            'bucket': self.bucket,\n            'key': self.key,\n            'extra_args': self.extra_args,\n            'callbacks': self.callbacks,\n            'size': self.size,\n        }\n        default_kwargs.update(kwargs)\n        return self.get_task(CopyObjectTask, main_kwargs=default_kwargs)\n\n    def test_main(self):\n        self.stubber.add_response(\n            'copy_object',\n            service_response={},\n            expected_params={\n                'Bucket': self.bucket,\n                'Key': self.key,\n                'CopySource': self.copy_source,\n            },\n        )\n        task = self.get_copy_task()\n        task()\n\n        self.stubber.assert_no_pending_responses()\n\n    def test_extra_args(self):\n        self.extra_args['ACL'] = 'private'\n        self.stubber.add_response(\n            'copy_object',\n            service_response={},\n            expected_params={\n                'Bucket': self.bucket,\n                'Key': self.key,\n                'CopySource': self.copy_source,\n                'ACL': 'private',\n            },\n        )\n        task = self.get_copy_task()\n        task()\n\n        self.stubber.assert_no_pending_responses()\n\n    def test_callbacks_invoked(self):\n        subscriber = RecordingSubscriber()\n        self.callbacks.append(subscriber.on_progress)\n        self.stubber.add_response(\n            'copy_object',\n            service_response={},\n            expected_params={\n                'Bucket': self.bucket,\n                'Key': self.key,\n                'CopySource': self.copy_source,\n            },\n        )\n        task = self.get_copy_task()\n        task()\n\n        self.stubber.assert_no_pending_responses()\n        self.assertEqual(subscriber.calculate_bytes_seen(), self.size)\n\n\nclass TestCopyPartTask(BaseCopyTaskTest):\n    def setUp(self):\n        super().setUp()\n        self.copy_source_range = 'bytes=5-9'\n        self.extra_args['CopySourceRange'] = self.copy_source_range\n        self.upload_id = 'myuploadid'\n        self.part_number = 1\n        self.result_etag = 'my-etag'\n        self.checksum_sha1 = 'my-checksum_sha1'\n\n    def get_copy_task(self, **kwargs):\n        default_kwargs = {\n            'client': self.client,\n            'copy_source': self.copy_source,\n            'bucket': self.bucket,\n            'key': self.key,\n            'upload_id': self.upload_id,\n            'part_number': self.part_number,\n            'extra_args': self.extra_args,\n            'callbacks': self.callbacks,\n            'size': self.size,\n        }\n        default_kwargs.update(kwargs)\n        return self.get_task(CopyPartTask, main_kwargs=default_kwargs)\n\n    def test_main(self):\n        self.stubber.add_response(\n            'upload_part_copy',\n            service_response={'CopyPartResult': {'ETag': self.result_etag}},\n            expected_params={\n                'Bucket': self.bucket,\n                'Key': self.key,\n                'CopySource': self.copy_source,\n                'UploadId': self.upload_id,\n                'PartNumber': self.part_number,\n                'CopySourceRange': self.copy_source_range,\n            },\n        )\n        task = self.get_copy_task()\n        self.assertEqual(\n            task(), {'PartNumber': self.part_number, 'ETag': self.result_etag}\n        )\n        self.stubber.assert_no_pending_responses()\n\n    def test_main_with_checksum(self):\n        self.stubber.add_response(\n            'upload_part_copy',\n            service_response={\n                'CopyPartResult': {\n                    'ETag': self.result_etag,\n                    'ChecksumSHA1': self.checksum_sha1,\n                }\n            },\n            expected_params={\n                'Bucket': self.bucket,\n                'Key': self.key,\n                'CopySource': self.copy_source,\n                'UploadId': self.upload_id,\n                'PartNumber': self.part_number,\n                'CopySourceRange': self.copy_source_range,\n            },\n        )\n        task = self.get_copy_task(checksum_algorithm=\"sha1\")\n        self.assertEqual(\n            task(),\n            {\n                'PartNumber': self.part_number,\n                'ETag': self.result_etag,\n                'ChecksumSHA1': self.checksum_sha1,\n            },\n        )\n        self.stubber.assert_no_pending_responses()\n\n    def test_extra_args(self):\n        self.extra_args['RequestPayer'] = 'requester'\n        self.stubber.add_response(\n            'upload_part_copy',\n            service_response={'CopyPartResult': {'ETag': self.result_etag}},\n            expected_params={\n                'Bucket': self.bucket,\n                'Key': self.key,\n                'CopySource': self.copy_source,\n                'UploadId': self.upload_id,\n                'PartNumber': self.part_number,\n                'CopySourceRange': self.copy_source_range,\n                'RequestPayer': 'requester',\n            },\n        )\n        task = self.get_copy_task()\n        self.assertEqual(\n            task(), {'PartNumber': self.part_number, 'ETag': self.result_etag}\n        )\n        self.stubber.assert_no_pending_responses()\n\n    def test_callbacks_invoked(self):\n        subscriber = RecordingSubscriber()\n        self.callbacks.append(subscriber.on_progress)\n        self.stubber.add_response(\n            'upload_part_copy',\n            service_response={'CopyPartResult': {'ETag': self.result_etag}},\n            expected_params={\n                'Bucket': self.bucket,\n                'Key': self.key,\n                'CopySource': self.copy_source,\n                'UploadId': self.upload_id,\n                'PartNumber': self.part_number,\n                'CopySourceRange': self.copy_source_range,\n            },\n        )\n        task = self.get_copy_task()\n        self.assertEqual(\n            task(), {'PartNumber': self.part_number, 'ETag': self.result_etag}\n        )\n        self.stubber.assert_no_pending_responses()\n        self.assertEqual(subscriber.calculate_bytes_seen(), self.size)\n", "tests/unit/test_processpool.py": "# Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport os\nimport queue\nimport signal\nimport threading\nimport time\nfrom io import BytesIO\n\nfrom botocore.client import BaseClient\nfrom botocore.config import Config\nfrom botocore.exceptions import ClientError, ReadTimeoutError\n\nfrom s3transfer.constants import PROCESS_USER_AGENT\nfrom s3transfer.exceptions import CancelledError, RetriesExceededError\nfrom s3transfer.processpool import (\n    SHUTDOWN_SIGNAL,\n    ClientFactory,\n    DownloadFileRequest,\n    GetObjectJob,\n    GetObjectSubmitter,\n    GetObjectWorker,\n    ProcessPoolDownloader,\n    ProcessPoolTransferFuture,\n    ProcessPoolTransferMeta,\n    ProcessTransferConfig,\n    TransferMonitor,\n    TransferState,\n    ignore_ctrl_c,\n)\nfrom s3transfer.utils import CallArgs, OSUtils\nfrom tests import (\n    FileCreator,\n    StreamWithError,\n    StubbedClientTest,\n    mock,\n    skip_if_windows,\n    unittest,\n)\n\n\nclass RenameFailingOSUtils(OSUtils):\n    def __init__(self, exception):\n        self.exception = exception\n\n    def rename_file(self, current_filename, new_filename):\n        raise self.exception\n\n\nclass TestIgnoreCtrlC(unittest.TestCase):\n    @skip_if_windows('os.kill() with SIGINT not supported on Windows')\n    def test_ignore_ctrl_c(self):\n        with ignore_ctrl_c():\n            try:\n                os.kill(os.getpid(), signal.SIGINT)\n            except KeyboardInterrupt:\n                self.fail(\n                    'The ignore_ctrl_c context manager should have '\n                    'ignored the KeyboardInterrupt exception'\n                )\n\n\nclass TestProcessPoolDownloader(unittest.TestCase):\n    def test_uses_client_kwargs(self):\n        with mock.patch('s3transfer.processpool.ClientFactory') as factory:\n            ProcessPoolDownloader(client_kwargs={'region_name': 'myregion'})\n            self.assertEqual(\n                factory.call_args[0][0], {'region_name': 'myregion'}\n            )\n\n\nclass TestProcessPoolTransferFuture(unittest.TestCase):\n    def setUp(self):\n        self.monitor = TransferMonitor()\n        self.transfer_id = self.monitor.notify_new_transfer()\n        self.meta = ProcessPoolTransferMeta(\n            transfer_id=self.transfer_id, call_args=CallArgs()\n        )\n        self.future = ProcessPoolTransferFuture(\n            monitor=self.monitor, meta=self.meta\n        )\n\n    def test_meta(self):\n        self.assertEqual(self.future.meta, self.meta)\n\n    def test_done(self):\n        self.assertFalse(self.future.done())\n        self.monitor.notify_done(self.transfer_id)\n        self.assertTrue(self.future.done())\n\n    def test_result(self):\n        self.monitor.notify_done(self.transfer_id)\n        self.assertIsNone(self.future.result())\n\n    def test_result_with_exception(self):\n        self.monitor.notify_exception(self.transfer_id, RuntimeError())\n        self.monitor.notify_done(self.transfer_id)\n        with self.assertRaises(RuntimeError):\n            self.future.result()\n\n    def test_result_with_keyboard_interrupt(self):\n        mock_monitor = mock.Mock(TransferMonitor)\n        mock_monitor._connect = mock.Mock()\n        mock_monitor.poll_for_result.side_effect = KeyboardInterrupt()\n        future = ProcessPoolTransferFuture(\n            monitor=mock_monitor, meta=self.meta\n        )\n        with self.assertRaises(KeyboardInterrupt):\n            future.result()\n        self.assertTrue(mock_monitor._connect.called)\n        self.assertTrue(mock_monitor.notify_exception.called)\n        call_args = mock_monitor.notify_exception.call_args[0]\n        self.assertEqual(call_args[0], self.transfer_id)\n        self.assertIsInstance(call_args[1], CancelledError)\n\n    def test_cancel(self):\n        self.future.cancel()\n        self.monitor.notify_done(self.transfer_id)\n        with self.assertRaises(CancelledError):\n            self.future.result()\n\n\nclass TestProcessPoolTransferMeta(unittest.TestCase):\n    def test_transfer_id(self):\n        meta = ProcessPoolTransferMeta(1, CallArgs())\n        self.assertEqual(meta.transfer_id, 1)\n\n    def test_call_args(self):\n        call_args = CallArgs()\n        meta = ProcessPoolTransferMeta(1, call_args)\n        self.assertEqual(meta.call_args, call_args)\n\n    def test_user_context(self):\n        meta = ProcessPoolTransferMeta(1, CallArgs())\n        self.assertEqual(meta.user_context, {})\n        meta.user_context['mykey'] = 'myvalue'\n        self.assertEqual(meta.user_context, {'mykey': 'myvalue'})\n\n\nclass TestClientFactory(unittest.TestCase):\n    def test_create_client(self):\n        client = ClientFactory().create_client()\n        self.assertIsInstance(client, BaseClient)\n        self.assertEqual(client.meta.service_model.service_name, 's3')\n        self.assertIn(PROCESS_USER_AGENT, client.meta.config.user_agent)\n\n    def test_create_client_with_client_kwargs(self):\n        client = ClientFactory({'region_name': 'myregion'}).create_client()\n        self.assertEqual(client.meta.region_name, 'myregion')\n\n    def test_user_agent_with_config(self):\n        client = ClientFactory({'config': Config()}).create_client()\n        self.assertIn(PROCESS_USER_AGENT, client.meta.config.user_agent)\n\n    def test_user_agent_with_existing_user_agent_extra(self):\n        config = Config(user_agent_extra='foo/1.0')\n        client = ClientFactory({'config': config}).create_client()\n        self.assertIn(PROCESS_USER_AGENT, client.meta.config.user_agent)\n\n    def test_user_agent_with_existing_user_agent(self):\n        config = Config(user_agent='foo/1.0')\n        client = ClientFactory({'config': config}).create_client()\n        self.assertIn(PROCESS_USER_AGENT, client.meta.config.user_agent)\n\n\nclass TestTransferMonitor(unittest.TestCase):\n    def setUp(self):\n        self.monitor = TransferMonitor()\n        self.transfer_id = self.monitor.notify_new_transfer()\n\n    def test_notify_new_transfer_creates_new_state(self):\n        monitor = TransferMonitor()\n        transfer_id = monitor.notify_new_transfer()\n        self.assertFalse(monitor.is_done(transfer_id))\n        self.assertIsNone(monitor.get_exception(transfer_id))\n\n    def test_notify_new_transfer_increments_transfer_id(self):\n        monitor = TransferMonitor()\n        self.assertEqual(monitor.notify_new_transfer(), 0)\n        self.assertEqual(monitor.notify_new_transfer(), 1)\n\n    def test_notify_get_exception(self):\n        exception = Exception()\n        self.monitor.notify_exception(self.transfer_id, exception)\n        self.assertEqual(\n            self.monitor.get_exception(self.transfer_id), exception\n        )\n\n    def test_get_no_exception(self):\n        self.assertIsNone(self.monitor.get_exception(self.transfer_id))\n\n    def test_notify_jobs(self):\n        self.monitor.notify_expected_jobs_to_complete(self.transfer_id, 2)\n        self.assertEqual(self.monitor.notify_job_complete(self.transfer_id), 1)\n        self.assertEqual(self.monitor.notify_job_complete(self.transfer_id), 0)\n\n    def test_notify_jobs_for_multiple_transfers(self):\n        self.monitor.notify_expected_jobs_to_complete(self.transfer_id, 2)\n        other_transfer_id = self.monitor.notify_new_transfer()\n        self.monitor.notify_expected_jobs_to_complete(other_transfer_id, 2)\n        self.assertEqual(self.monitor.notify_job_complete(self.transfer_id), 1)\n        self.assertEqual(\n            self.monitor.notify_job_complete(other_transfer_id), 1\n        )\n\n    def test_done(self):\n        self.assertFalse(self.monitor.is_done(self.transfer_id))\n        self.monitor.notify_done(self.transfer_id)\n        self.assertTrue(self.monitor.is_done(self.transfer_id))\n\n    def test_poll_for_result(self):\n        self.monitor.notify_done(self.transfer_id)\n        self.assertIsNone(self.monitor.poll_for_result(self.transfer_id))\n\n    def test_poll_for_result_raises_error(self):\n        self.monitor.notify_exception(self.transfer_id, RuntimeError())\n        self.monitor.notify_done(self.transfer_id)\n        with self.assertRaises(RuntimeError):\n            self.monitor.poll_for_result(self.transfer_id)\n\n    def test_poll_for_result_waits_till_done(self):\n        event_order = []\n\n        def sleep_then_notify_done():\n            time.sleep(0.05)\n            event_order.append('notify_done')\n            self.monitor.notify_done(self.transfer_id)\n\n        t = threading.Thread(target=sleep_then_notify_done)\n        t.start()\n\n        self.monitor.poll_for_result(self.transfer_id)\n        event_order.append('done_polling')\n        self.assertEqual(event_order, ['notify_done', 'done_polling'])\n\n    def test_notify_cancel_all_in_progress(self):\n        monitor = TransferMonitor()\n        transfer_ids = []\n        for _ in range(10):\n            transfer_ids.append(monitor.notify_new_transfer())\n        monitor.notify_cancel_all_in_progress()\n        for transfer_id in transfer_ids:\n            self.assertIsInstance(\n                monitor.get_exception(transfer_id), CancelledError\n            )\n            # Cancelling a transfer does not mean it is done as there may\n            # be cleanup work left to do.\n            self.assertFalse(monitor.is_done(transfer_id))\n\n    def test_notify_cancel_does_not_affect_done_transfers(self):\n        self.monitor.notify_done(self.transfer_id)\n        self.monitor.notify_cancel_all_in_progress()\n        self.assertTrue(self.monitor.is_done(self.transfer_id))\n        self.assertIsNone(self.monitor.get_exception(self.transfer_id))\n\n\nclass TestTransferState(unittest.TestCase):\n    def setUp(self):\n        self.state = TransferState()\n\n    def test_done(self):\n        self.assertFalse(self.state.done)\n        self.state.set_done()\n        self.assertTrue(self.state.done)\n\n    def test_waits_till_done_is_set(self):\n        event_order = []\n\n        def sleep_then_set_done():\n            time.sleep(0.05)\n            event_order.append('set_done')\n            self.state.set_done()\n\n        t = threading.Thread(target=sleep_then_set_done)\n        t.start()\n\n        self.state.wait_till_done()\n        event_order.append('done_waiting')\n        self.assertEqual(event_order, ['set_done', 'done_waiting'])\n\n    def test_exception(self):\n        exception = RuntimeError()\n        self.state.exception = exception\n        self.assertEqual(self.state.exception, exception)\n\n    def test_jobs_to_complete(self):\n        self.state.jobs_to_complete = 5\n        self.assertEqual(self.state.jobs_to_complete, 5)\n\n    def test_decrement_jobs_to_complete(self):\n        self.state.jobs_to_complete = 5\n        self.assertEqual(self.state.decrement_jobs_to_complete(), 4)\n\n\nclass TestGetObjectSubmitter(StubbedClientTest):\n    def setUp(self):\n        super().setUp()\n        self.transfer_config = ProcessTransferConfig()\n        self.client_factory = mock.Mock(ClientFactory)\n        self.client_factory.create_client.return_value = self.client\n        self.transfer_monitor = TransferMonitor()\n        self.osutil = mock.Mock(OSUtils)\n        self.download_request_queue = queue.Queue()\n        self.worker_queue = queue.Queue()\n        self.submitter = GetObjectSubmitter(\n            transfer_config=self.transfer_config,\n            client_factory=self.client_factory,\n            transfer_monitor=self.transfer_monitor,\n            osutil=self.osutil,\n            download_request_queue=self.download_request_queue,\n            worker_queue=self.worker_queue,\n        )\n        self.transfer_id = self.transfer_monitor.notify_new_transfer()\n        self.bucket = 'bucket'\n        self.key = 'key'\n        self.filename = 'myfile'\n        self.temp_filename = 'myfile.temp'\n        self.osutil.get_temp_filename.return_value = self.temp_filename\n        self.extra_args = {}\n        self.expected_size = None\n\n    def add_download_file_request(self, **override_kwargs):\n        kwargs = {\n            'transfer_id': self.transfer_id,\n            'bucket': self.bucket,\n            'key': self.key,\n            'filename': self.filename,\n            'extra_args': self.extra_args,\n            'expected_size': self.expected_size,\n        }\n        kwargs.update(override_kwargs)\n        self.download_request_queue.put(DownloadFileRequest(**kwargs))\n\n    def add_shutdown(self):\n        self.download_request_queue.put(SHUTDOWN_SIGNAL)\n\n    def assert_submitted_get_object_jobs(self, expected_jobs):\n        actual_jobs = []\n        while not self.worker_queue.empty():\n            actual_jobs.append(self.worker_queue.get())\n        self.assertEqual(actual_jobs, expected_jobs)\n\n    def test_run_for_non_ranged_download(self):\n        self.add_download_file_request(expected_size=1)\n        self.add_shutdown()\n        self.submitter.run()\n        self.osutil.allocate.assert_called_with(self.temp_filename, 1)\n        self.assert_submitted_get_object_jobs(\n            [\n                GetObjectJob(\n                    transfer_id=self.transfer_id,\n                    bucket=self.bucket,\n                    key=self.key,\n                    temp_filename=self.temp_filename,\n                    offset=0,\n                    extra_args={},\n                    filename=self.filename,\n                )\n            ]\n        )\n\n    def test_run_for_ranged_download(self):\n        self.transfer_config.multipart_chunksize = 2\n        self.transfer_config.multipart_threshold = 4\n        self.add_download_file_request(expected_size=4)\n        self.add_shutdown()\n        self.submitter.run()\n        self.osutil.allocate.assert_called_with(self.temp_filename, 4)\n        self.assert_submitted_get_object_jobs(\n            [\n                GetObjectJob(\n                    transfer_id=self.transfer_id,\n                    bucket=self.bucket,\n                    key=self.key,\n                    temp_filename=self.temp_filename,\n                    offset=0,\n                    extra_args={'Range': 'bytes=0-1'},\n                    filename=self.filename,\n                ),\n                GetObjectJob(\n                    transfer_id=self.transfer_id,\n                    bucket=self.bucket,\n                    key=self.key,\n                    temp_filename=self.temp_filename,\n                    offset=2,\n                    extra_args={'Range': 'bytes=2-'},\n                    filename=self.filename,\n                ),\n            ]\n        )\n\n    def test_run_when_expected_size_not_provided(self):\n        self.stubber.add_response(\n            'head_object',\n            {'ContentLength': 1},\n            expected_params={'Bucket': self.bucket, 'Key': self.key},\n        )\n        self.add_download_file_request(expected_size=None)\n        self.add_shutdown()\n        self.submitter.run()\n        self.stubber.assert_no_pending_responses()\n        self.osutil.allocate.assert_called_with(self.temp_filename, 1)\n        self.assert_submitted_get_object_jobs(\n            [\n                GetObjectJob(\n                    transfer_id=self.transfer_id,\n                    bucket=self.bucket,\n                    key=self.key,\n                    temp_filename=self.temp_filename,\n                    offset=0,\n                    extra_args={},\n                    filename=self.filename,\n                )\n            ]\n        )\n\n    def test_run_with_extra_args(self):\n        self.stubber.add_response(\n            'head_object',\n            {'ContentLength': 1},\n            expected_params={\n                'Bucket': self.bucket,\n                'Key': self.key,\n                'VersionId': 'versionid',\n            },\n        )\n        self.add_download_file_request(\n            extra_args={'VersionId': 'versionid'}, expected_size=None\n        )\n        self.add_shutdown()\n        self.submitter.run()\n        self.stubber.assert_no_pending_responses()\n        self.osutil.allocate.assert_called_with(self.temp_filename, 1)\n        self.assert_submitted_get_object_jobs(\n            [\n                GetObjectJob(\n                    transfer_id=self.transfer_id,\n                    bucket=self.bucket,\n                    key=self.key,\n                    temp_filename=self.temp_filename,\n                    offset=0,\n                    extra_args={'VersionId': 'versionid'},\n                    filename=self.filename,\n                )\n            ]\n        )\n\n    def test_run_with_exception(self):\n        self.stubber.add_client_error('head_object', 'NoSuchKey', 404)\n        self.add_download_file_request(expected_size=None)\n        self.add_shutdown()\n        self.submitter.run()\n        self.stubber.assert_no_pending_responses()\n        self.assert_submitted_get_object_jobs([])\n        self.assertIsInstance(\n            self.transfer_monitor.get_exception(self.transfer_id), ClientError\n        )\n\n    def test_run_with_error_in_allocating_temp_file(self):\n        self.osutil.allocate.side_effect = OSError()\n        self.add_download_file_request(expected_size=1)\n        self.add_shutdown()\n        self.submitter.run()\n        self.assert_submitted_get_object_jobs([])\n        self.assertIsInstance(\n            self.transfer_monitor.get_exception(self.transfer_id), OSError\n        )\n\n    @skip_if_windows('os.kill() with SIGINT not supported on Windows')\n    def test_submitter_cannot_be_killed(self):\n        self.add_download_file_request(expected_size=None)\n        self.add_shutdown()\n\n        def raise_ctrl_c(**kwargs):\n            os.kill(os.getpid(), signal.SIGINT)\n\n        mock_client = mock.Mock()\n        mock_client.head_object = raise_ctrl_c\n        self.client_factory.create_client.return_value = mock_client\n\n        try:\n            self.submitter.run()\n        except KeyboardInterrupt:\n            self.fail(\n                'The submitter should have not been killed by the '\n                'KeyboardInterrupt'\n            )\n\n\nclass TestGetObjectWorker(StubbedClientTest):\n    def setUp(self):\n        super().setUp()\n        self.files = FileCreator()\n        self.queue = queue.Queue()\n        self.client_factory = mock.Mock(ClientFactory)\n        self.client_factory.create_client.return_value = self.client\n        self.transfer_monitor = TransferMonitor()\n        self.osutil = OSUtils()\n        self.worker = GetObjectWorker(\n            queue=self.queue,\n            client_factory=self.client_factory,\n            transfer_monitor=self.transfer_monitor,\n            osutil=self.osutil,\n        )\n        self.transfer_id = self.transfer_monitor.notify_new_transfer()\n        self.bucket = 'bucket'\n        self.key = 'key'\n        self.remote_contents = b'my content'\n        self.temp_filename = self.files.create_file('tempfile', '')\n        self.extra_args = {}\n        self.offset = 0\n        self.final_filename = self.files.full_path('final_filename')\n        self.stream = BytesIO(self.remote_contents)\n        self.transfer_monitor.notify_expected_jobs_to_complete(\n            self.transfer_id, 1000\n        )\n\n    def tearDown(self):\n        super().tearDown()\n        self.files.remove_all()\n\n    def add_get_object_job(self, **override_kwargs):\n        kwargs = {\n            'transfer_id': self.transfer_id,\n            'bucket': self.bucket,\n            'key': self.key,\n            'temp_filename': self.temp_filename,\n            'extra_args': self.extra_args,\n            'offset': self.offset,\n            'filename': self.final_filename,\n        }\n        kwargs.update(override_kwargs)\n        self.queue.put(GetObjectJob(**kwargs))\n\n    def add_shutdown(self):\n        self.queue.put(SHUTDOWN_SIGNAL)\n\n    def add_stubbed_get_object_response(self, body=None, expected_params=None):\n        if body is None:\n            body = self.stream\n        get_object_response = {'Body': body}\n\n        if expected_params is None:\n            expected_params = {'Bucket': self.bucket, 'Key': self.key}\n\n        self.stubber.add_response(\n            'get_object', get_object_response, expected_params\n        )\n\n    def assert_contents(self, filename, contents):\n        self.assertTrue(os.path.exists(filename))\n        with open(filename, 'rb') as f:\n            self.assertEqual(f.read(), contents)\n\n    def assert_does_not_exist(self, filename):\n        self.assertFalse(os.path.exists(filename))\n\n    def test_run_is_final_job(self):\n        self.add_get_object_job()\n        self.add_shutdown()\n        self.add_stubbed_get_object_response()\n        self.transfer_monitor.notify_expected_jobs_to_complete(\n            self.transfer_id, 1\n        )\n\n        self.worker.run()\n        self.stubber.assert_no_pending_responses()\n        self.assert_does_not_exist(self.temp_filename)\n        self.assert_contents(self.final_filename, self.remote_contents)\n\n    def test_run_jobs_is_not_final_job(self):\n        self.add_get_object_job()\n        self.add_shutdown()\n        self.add_stubbed_get_object_response()\n        self.transfer_monitor.notify_expected_jobs_to_complete(\n            self.transfer_id, 1000\n        )\n\n        self.worker.run()\n        self.stubber.assert_no_pending_responses()\n        self.assert_contents(self.temp_filename, self.remote_contents)\n        self.assert_does_not_exist(self.final_filename)\n\n    def test_run_with_extra_args(self):\n        self.add_get_object_job(extra_args={'VersionId': 'versionid'})\n        self.add_shutdown()\n        self.add_stubbed_get_object_response(\n            expected_params={\n                'Bucket': self.bucket,\n                'Key': self.key,\n                'VersionId': 'versionid',\n            }\n        )\n\n        self.worker.run()\n        self.stubber.assert_no_pending_responses()\n\n    def test_run_with_offset(self):\n        offset = 1\n        self.add_get_object_job(offset=offset)\n        self.add_shutdown()\n        self.add_stubbed_get_object_response()\n\n        self.worker.run()\n        with open(self.temp_filename, 'rb') as f:\n            f.seek(offset)\n            self.assertEqual(f.read(), self.remote_contents)\n\n    def test_run_error_in_get_object(self):\n        self.add_get_object_job()\n        self.add_shutdown()\n        self.stubber.add_client_error('get_object', 'NoSuchKey', 404)\n        self.add_stubbed_get_object_response()\n\n        self.worker.run()\n        self.assertIsInstance(\n            self.transfer_monitor.get_exception(self.transfer_id), ClientError\n        )\n\n    def test_run_does_retries_for_get_object(self):\n        self.add_get_object_job()\n        self.add_shutdown()\n        self.add_stubbed_get_object_response(\n            body=StreamWithError(\n                self.stream, ReadTimeoutError(endpoint_url='')\n            )\n        )\n        self.add_stubbed_get_object_response()\n\n        self.worker.run()\n        self.stubber.assert_no_pending_responses()\n        self.assert_contents(self.temp_filename, self.remote_contents)\n\n    def test_run_can_exhaust_retries_for_get_object(self):\n        self.add_get_object_job()\n        self.add_shutdown()\n        # 5 is the current setting for max number of GetObject attempts\n        for _ in range(5):\n            self.add_stubbed_get_object_response(\n                body=StreamWithError(\n                    self.stream, ReadTimeoutError(endpoint_url='')\n                )\n            )\n\n        self.worker.run()\n        self.stubber.assert_no_pending_responses()\n        self.assertIsInstance(\n            self.transfer_monitor.get_exception(self.transfer_id),\n            RetriesExceededError,\n        )\n\n    def test_run_skips_get_object_on_previous_exception(self):\n        self.add_get_object_job()\n        self.add_shutdown()\n        self.transfer_monitor.notify_exception(self.transfer_id, Exception())\n\n        self.worker.run()\n        # Note we did not add a stubbed response for get_object\n        self.stubber.assert_no_pending_responses()\n\n    def test_run_final_job_removes_file_on_previous_exception(self):\n        self.add_get_object_job()\n        self.add_shutdown()\n        self.transfer_monitor.notify_exception(self.transfer_id, Exception())\n        self.transfer_monitor.notify_expected_jobs_to_complete(\n            self.transfer_id, 1\n        )\n\n        self.worker.run()\n        self.stubber.assert_no_pending_responses()\n        self.assert_does_not_exist(self.temp_filename)\n        self.assert_does_not_exist(self.final_filename)\n\n    def test_run_fails_to_rename_file(self):\n        exception = OSError()\n        osutil = RenameFailingOSUtils(exception)\n        self.worker = GetObjectWorker(\n            queue=self.queue,\n            client_factory=self.client_factory,\n            transfer_monitor=self.transfer_monitor,\n            osutil=osutil,\n        )\n        self.add_get_object_job()\n        self.add_shutdown()\n        self.add_stubbed_get_object_response()\n        self.transfer_monitor.notify_expected_jobs_to_complete(\n            self.transfer_id, 1\n        )\n\n        self.worker.run()\n        self.assertEqual(\n            self.transfer_monitor.get_exception(self.transfer_id), exception\n        )\n        self.assert_does_not_exist(self.temp_filename)\n        self.assert_does_not_exist(self.final_filename)\n\n    @skip_if_windows('os.kill() with SIGINT not supported on Windows')\n    def test_worker_cannot_be_killed(self):\n        self.add_get_object_job()\n        self.add_shutdown()\n        self.transfer_monitor.notify_expected_jobs_to_complete(\n            self.transfer_id, 1\n        )\n\n        def raise_ctrl_c(**kwargs):\n            os.kill(os.getpid(), signal.SIGINT)\n\n        mock_client = mock.Mock()\n        mock_client.get_object = raise_ctrl_c\n        self.client_factory.create_client.return_value = mock_client\n\n        try:\n            self.worker.run()\n        except KeyboardInterrupt:\n            self.fail(\n                'The worker should have not been killed by the '\n                'KeyboardInterrupt'\n            )\n", "tests/unit/test_subscribers.py": "# Copyright 2016 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License'). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the 'license' file accompanying this file. This file is\n# distributed on an 'AS IS' BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nfrom s3transfer.exceptions import InvalidSubscriberMethodError\nfrom s3transfer.subscribers import BaseSubscriber\nfrom tests import unittest\n\n\nclass ExtraMethodsSubscriber(BaseSubscriber):\n    def extra_method(self):\n        return 'called extra method'\n\n\nclass NotCallableSubscriber(BaseSubscriber):\n    on_done = 'foo'\n\n\nclass NoKwargsSubscriber(BaseSubscriber):\n    def on_done(self):\n        pass\n\n\nclass OverrideMethodSubscriber(BaseSubscriber):\n    def on_queued(self, **kwargs):\n        return kwargs\n\n\nclass OverrideConstructorSubscriber(BaseSubscriber):\n    def __init__(self, arg1, arg2):\n        self.arg1 = arg1\n        self.arg2 = arg2\n\n\nclass TestSubscribers(unittest.TestCase):\n    def test_can_instantiate_base_subscriber(self):\n        try:\n            BaseSubscriber()\n        except InvalidSubscriberMethodError:\n            self.fail('BaseSubscriber should be instantiable')\n\n    def test_can_call_base_subscriber_method(self):\n        subscriber = BaseSubscriber()\n        try:\n            subscriber.on_done(future=None)\n        except Exception as e:\n            self.fail(\n                'Should be able to call base class subscriber method. '\n                'instead got: %s' % e\n            )\n\n    def test_subclass_can_have_and_call_additional_methods(self):\n        subscriber = ExtraMethodsSubscriber()\n        self.assertEqual(subscriber.extra_method(), 'called extra method')\n\n    def test_can_subclass_and_override_method_from_base_subscriber(self):\n        subscriber = OverrideMethodSubscriber()\n        # Make sure that the overridden method is called\n        self.assertEqual(subscriber.on_queued(foo='bar'), {'foo': 'bar'})\n\n    def test_can_subclass_and_override_constructor_from_base_class(self):\n        subscriber = OverrideConstructorSubscriber('foo', arg2='bar')\n        # Make sure you can create a custom constructor.\n        self.assertEqual(subscriber.arg1, 'foo')\n        self.assertEqual(subscriber.arg2, 'bar')\n\n    def test_invalid_arguments_in_constructor_of_subclass_subscriber(self):\n        # The override constructor should still have validation of\n        # constructor args.\n        with self.assertRaises(TypeError):\n            OverrideConstructorSubscriber()\n\n    def test_not_callable_in_subclass_subscriber_method(self):\n        with self.assertRaisesRegex(\n            InvalidSubscriberMethodError, 'must be callable'\n        ):\n            NotCallableSubscriber()\n\n    def test_no_kwargs_in_subclass_subscriber_method(self):\n        with self.assertRaisesRegex(\n            InvalidSubscriberMethodError, 'must accept keyword'\n        ):\n            NoKwargsSubscriber()\n", "tests/unit/test_compat.py": "# Copyright 2016 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport os\nimport shutil\nimport signal\nimport tempfile\nfrom io import BytesIO\n\nfrom s3transfer.compat import BaseManager, readable, seekable\nfrom tests import skip_if_windows, unittest\n\n\nclass ErrorRaisingSeekWrapper:\n    \"\"\"An object wrapper that throws an error when seeked on\n\n    :param fileobj: The fileobj that it wraps\n    :param exception: The exception to raise when seeked on.\n    \"\"\"\n\n    def __init__(self, fileobj, exception):\n        self._fileobj = fileobj\n        self._exception = exception\n\n    def seek(self, offset, whence=0):\n        raise self._exception\n\n    def tell(self):\n        return self._fileobj.tell()\n\n\nclass TestSeekable(unittest.TestCase):\n    def setUp(self):\n        self.tempdir = tempfile.mkdtemp()\n        self.filename = os.path.join(self.tempdir, 'foo')\n\n    def tearDown(self):\n        shutil.rmtree(self.tempdir)\n\n    def test_seekable_fileobj(self):\n        with open(self.filename, 'w') as f:\n            self.assertTrue(seekable(f))\n\n    def test_non_file_like_obj(self):\n        # Fails because there is no seekable(), seek(), nor tell()\n        self.assertFalse(seekable(object()))\n\n    def test_non_seekable_ioerror(self):\n        # Should return False if IOError is thrown.\n        with open(self.filename, 'w') as f:\n            self.assertFalse(seekable(ErrorRaisingSeekWrapper(f, IOError())))\n\n    def test_non_seekable_oserror(self):\n        # Should return False if OSError is thrown.\n        with open(self.filename, 'w') as f:\n            self.assertFalse(seekable(ErrorRaisingSeekWrapper(f, OSError())))\n\n\nclass TestReadable(unittest.TestCase):\n    def test_readable_fileobj(self):\n        with tempfile.TemporaryFile() as f:\n            self.assertTrue(readable(f))\n\n    def test_readable_file_like_obj(self):\n        self.assertTrue(readable(BytesIO()))\n\n    def test_non_file_like_obj(self):\n        self.assertFalse(readable(object()))\n\n\nclass TestBaseManager(unittest.TestCase):\n    def create_pid_manager(self):\n        class PIDManager(BaseManager):\n            pass\n\n        PIDManager.register('getpid', os.getpid)\n        return PIDManager()\n\n    def get_pid(self, pid_manager):\n        pid = pid_manager.getpid()\n        # A proxy object is returned back. The needed value can be acquired\n        # from the repr and converting that to an integer\n        return int(str(pid))\n\n    @skip_if_windows('os.kill() with SIGINT not supported on Windows')\n    def test_can_provide_signal_handler_initializers_to_start(self):\n        manager = self.create_pid_manager()\n        manager.start(signal.signal, (signal.SIGINT, signal.SIG_IGN))\n        pid = self.get_pid(manager)\n        try:\n            os.kill(pid, signal.SIGINT)\n        except KeyboardInterrupt:\n            pass\n        # Try using the manager after the os.kill on the parent process. The\n        # manager should not have died and should still be usable.\n        self.assertEqual(pid, self.get_pid(manager))\n", "tests/unit/test_bandwidth.py": "# Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport os\nimport shutil\nimport tempfile\n\nfrom s3transfer.bandwidth import (\n    BandwidthLimitedStream,\n    BandwidthLimiter,\n    BandwidthRateTracker,\n    ConsumptionScheduler,\n    LeakyBucket,\n    RequestExceededException,\n    RequestToken,\n    TimeUtils,\n)\nfrom s3transfer.futures import TransferCoordinator\nfrom tests import mock, unittest\n\n\nclass FixedIncrementalTickTimeUtils(TimeUtils):\n    def __init__(self, seconds_per_tick=1.0):\n        self._count = 0\n        self._seconds_per_tick = seconds_per_tick\n\n    def time(self):\n        current_count = self._count\n        self._count += self._seconds_per_tick\n        return current_count\n\n\nclass TestTimeUtils(unittest.TestCase):\n    @mock.patch('time.time')\n    def test_time(self, mock_time):\n        mock_return_val = 1\n        mock_time.return_value = mock_return_val\n        time_utils = TimeUtils()\n        self.assertEqual(time_utils.time(), mock_return_val)\n\n    @mock.patch('time.sleep')\n    def test_sleep(self, mock_sleep):\n        time_utils = TimeUtils()\n        time_utils.sleep(1)\n        self.assertEqual(mock_sleep.call_args_list, [mock.call(1)])\n\n\nclass BaseBandwidthLimitTest(unittest.TestCase):\n    def setUp(self):\n        self.leaky_bucket = mock.Mock(LeakyBucket)\n        self.time_utils = mock.Mock(TimeUtils)\n        self.tempdir = tempfile.mkdtemp()\n        self.content = b'a' * 1024 * 1024\n        self.filename = os.path.join(self.tempdir, 'myfile')\n        with open(self.filename, 'wb') as f:\n            f.write(self.content)\n        self.coordinator = TransferCoordinator()\n\n    def tearDown(self):\n        shutil.rmtree(self.tempdir)\n\n    def assert_consume_calls(self, amts):\n        expected_consume_args = [mock.call(amt, mock.ANY) for amt in amts]\n        self.assertEqual(\n            self.leaky_bucket.consume.call_args_list, expected_consume_args\n        )\n\n\nclass TestBandwidthLimiter(BaseBandwidthLimitTest):\n    def setUp(self):\n        super().setUp()\n        self.bandwidth_limiter = BandwidthLimiter(self.leaky_bucket)\n\n    def test_get_bandwidth_limited_stream(self):\n        with open(self.filename, 'rb') as f:\n            stream = self.bandwidth_limiter.get_bandwith_limited_stream(\n                f, self.coordinator\n            )\n            self.assertIsInstance(stream, BandwidthLimitedStream)\n            self.assertEqual(stream.read(len(self.content)), self.content)\n            self.assert_consume_calls(amts=[len(self.content)])\n\n    def test_get_disabled_bandwidth_limited_stream(self):\n        with open(self.filename, 'rb') as f:\n            stream = self.bandwidth_limiter.get_bandwith_limited_stream(\n                f, self.coordinator, enabled=False\n            )\n            self.assertIsInstance(stream, BandwidthLimitedStream)\n            self.assertEqual(stream.read(len(self.content)), self.content)\n            self.leaky_bucket.consume.assert_not_called()\n\n\nclass TestBandwidthLimitedStream(BaseBandwidthLimitTest):\n    def setUp(self):\n        super().setUp()\n        self.bytes_threshold = 1\n\n    def tearDown(self):\n        shutil.rmtree(self.tempdir)\n\n    def get_bandwidth_limited_stream(self, f):\n        return BandwidthLimitedStream(\n            f,\n            self.leaky_bucket,\n            self.coordinator,\n            self.time_utils,\n            self.bytes_threshold,\n        )\n\n    def assert_sleep_calls(self, amts):\n        expected_sleep_args_list = [mock.call(amt) for amt in amts]\n        self.assertEqual(\n            self.time_utils.sleep.call_args_list, expected_sleep_args_list\n        )\n\n    def get_unique_consume_request_tokens(self):\n        return {\n            call_args[0][1]\n            for call_args in self.leaky_bucket.consume.call_args_list\n        }\n\n    def test_read(self):\n        with open(self.filename, 'rb') as f:\n            stream = self.get_bandwidth_limited_stream(f)\n            data = stream.read(len(self.content))\n            self.assertEqual(self.content, data)\n            self.assert_consume_calls(amts=[len(self.content)])\n            self.assert_sleep_calls(amts=[])\n\n    def test_retries_on_request_exceeded(self):\n        with open(self.filename, 'rb') as f:\n            stream = self.get_bandwidth_limited_stream(f)\n            retry_time = 1\n            amt_requested = len(self.content)\n            self.leaky_bucket.consume.side_effect = [\n                RequestExceededException(amt_requested, retry_time),\n                len(self.content),\n            ]\n            data = stream.read(len(self.content))\n            self.assertEqual(self.content, data)\n            self.assert_consume_calls(amts=[amt_requested, amt_requested])\n            self.assert_sleep_calls(amts=[retry_time])\n\n    def test_with_transfer_coordinator_exception(self):\n        self.coordinator.set_exception(ValueError())\n        with open(self.filename, 'rb') as f:\n            stream = self.get_bandwidth_limited_stream(f)\n            with self.assertRaises(ValueError):\n                stream.read(len(self.content))\n\n    def test_read_when_bandwidth_limiting_disabled(self):\n        with open(self.filename, 'rb') as f:\n            stream = self.get_bandwidth_limited_stream(f)\n            stream.disable_bandwidth_limiting()\n            data = stream.read(len(self.content))\n            self.assertEqual(self.content, data)\n            self.assertFalse(self.leaky_bucket.consume.called)\n\n    def test_read_toggle_disable_enable_bandwidth_limiting(self):\n        with open(self.filename, 'rb') as f:\n            stream = self.get_bandwidth_limited_stream(f)\n            stream.disable_bandwidth_limiting()\n            data = stream.read(1)\n            self.assertEqual(self.content[:1], data)\n            self.assert_consume_calls(amts=[])\n            stream.enable_bandwidth_limiting()\n            data = stream.read(len(self.content) - 1)\n            self.assertEqual(self.content[1:], data)\n            self.assert_consume_calls(amts=[len(self.content) - 1])\n\n    def test_seek(self):\n        mock_fileobj = mock.Mock()\n        stream = self.get_bandwidth_limited_stream(mock_fileobj)\n        stream.seek(1)\n        self.assertEqual(mock_fileobj.seek.call_args_list, [mock.call(1, 0)])\n\n    def test_tell(self):\n        mock_fileobj = mock.Mock()\n        stream = self.get_bandwidth_limited_stream(mock_fileobj)\n        stream.tell()\n        self.assertEqual(mock_fileobj.tell.call_args_list, [mock.call()])\n\n    def test_close(self):\n        mock_fileobj = mock.Mock()\n        stream = self.get_bandwidth_limited_stream(mock_fileobj)\n        stream.close()\n        self.assertEqual(mock_fileobj.close.call_args_list, [mock.call()])\n\n    def test_context_manager(self):\n        mock_fileobj = mock.Mock()\n        stream = self.get_bandwidth_limited_stream(mock_fileobj)\n        with stream as stream_handle:\n            self.assertIs(stream_handle, stream)\n        self.assertEqual(mock_fileobj.close.call_args_list, [mock.call()])\n\n    def test_reuses_request_token(self):\n        with open(self.filename, 'rb') as f:\n            stream = self.get_bandwidth_limited_stream(f)\n            stream.read(1)\n            stream.read(1)\n        self.assertEqual(len(self.get_unique_consume_request_tokens()), 1)\n\n    def test_request_tokens_unique_per_stream(self):\n        with open(self.filename, 'rb') as f:\n            stream = self.get_bandwidth_limited_stream(f)\n            stream.read(1)\n        with open(self.filename, 'rb') as f:\n            stream = self.get_bandwidth_limited_stream(f)\n            stream.read(1)\n        self.assertEqual(len(self.get_unique_consume_request_tokens()), 2)\n\n    def test_call_consume_after_reaching_threshold(self):\n        self.bytes_threshold = 2\n        with open(self.filename, 'rb') as f:\n            stream = self.get_bandwidth_limited_stream(f)\n            self.assertEqual(stream.read(1), self.content[:1])\n            self.assert_consume_calls(amts=[])\n            self.assertEqual(stream.read(1), self.content[1:2])\n            self.assert_consume_calls(amts=[2])\n\n    def test_resets_after_reaching_threshold(self):\n        self.bytes_threshold = 2\n        with open(self.filename, 'rb') as f:\n            stream = self.get_bandwidth_limited_stream(f)\n            self.assertEqual(stream.read(2), self.content[:2])\n            self.assert_consume_calls(amts=[2])\n            self.assertEqual(stream.read(1), self.content[2:3])\n            self.assert_consume_calls(amts=[2])\n\n    def test_pending_bytes_seen_on_close(self):\n        self.bytes_threshold = 2\n        with open(self.filename, 'rb') as f:\n            stream = self.get_bandwidth_limited_stream(f)\n            self.assertEqual(stream.read(1), self.content[:1])\n            self.assert_consume_calls(amts=[])\n            stream.close()\n            self.assert_consume_calls(amts=[1])\n\n    def test_no_bytes_remaining_on(self):\n        self.bytes_threshold = 2\n        with open(self.filename, 'rb') as f:\n            stream = self.get_bandwidth_limited_stream(f)\n            self.assertEqual(stream.read(2), self.content[:2])\n            self.assert_consume_calls(amts=[2])\n            stream.close()\n            # There should have been no more consume() calls made\n            # as all bytes have been accounted for in the previous\n            # consume() call.\n            self.assert_consume_calls(amts=[2])\n\n    def test_disable_bandwidth_limiting_with_pending_bytes_seen_on_close(self):\n        self.bytes_threshold = 2\n        with open(self.filename, 'rb') as f:\n            stream = self.get_bandwidth_limited_stream(f)\n            self.assertEqual(stream.read(1), self.content[:1])\n            self.assert_consume_calls(amts=[])\n            stream.disable_bandwidth_limiting()\n            stream.close()\n            self.assert_consume_calls(amts=[])\n\n    def test_signal_transferring(self):\n        with open(self.filename, 'rb') as f:\n            stream = self.get_bandwidth_limited_stream(f)\n            stream.signal_not_transferring()\n            data = stream.read(1)\n            self.assertEqual(self.content[:1], data)\n            self.assert_consume_calls(amts=[])\n            stream.signal_transferring()\n            data = stream.read(len(self.content) - 1)\n            self.assertEqual(self.content[1:], data)\n            self.assert_consume_calls(amts=[len(self.content) - 1])\n\n\nclass TestLeakyBucket(unittest.TestCase):\n    def setUp(self):\n        self.max_rate = 1\n        self.time_now = 1.0\n        self.time_utils = mock.Mock(TimeUtils)\n        self.time_utils.time.return_value = self.time_now\n        self.scheduler = mock.Mock(ConsumptionScheduler)\n        self.scheduler.is_scheduled.return_value = False\n        self.rate_tracker = mock.Mock(BandwidthRateTracker)\n        self.leaky_bucket = LeakyBucket(\n            self.max_rate, self.time_utils, self.rate_tracker, self.scheduler\n        )\n\n    def set_projected_rate(self, rate):\n        self.rate_tracker.get_projected_rate.return_value = rate\n\n    def set_retry_time(self, retry_time):\n        self.scheduler.schedule_consumption.return_value = retry_time\n\n    def assert_recorded_consumed_amt(self, expected_amt):\n        self.assertEqual(\n            self.rate_tracker.record_consumption_rate.call_args,\n            mock.call(expected_amt, self.time_utils.time.return_value),\n        )\n\n    def assert_was_scheduled(self, amt, token):\n        self.assertEqual(\n            self.scheduler.schedule_consumption.call_args,\n            mock.call(amt, token, amt / (self.max_rate)),\n        )\n\n    def assert_nothing_scheduled(self):\n        self.assertFalse(self.scheduler.schedule_consumption.called)\n\n    def assert_processed_request_token(self, request_token):\n        self.assertEqual(\n            self.scheduler.process_scheduled_consumption.call_args,\n            mock.call(request_token),\n        )\n\n    def test_consume_under_max_rate(self):\n        amt = 1\n        self.set_projected_rate(self.max_rate / 2)\n        self.assertEqual(self.leaky_bucket.consume(amt, RequestToken()), amt)\n        self.assert_recorded_consumed_amt(amt)\n        self.assert_nothing_scheduled()\n\n    def test_consume_at_max_rate(self):\n        amt = 1\n        self.set_projected_rate(self.max_rate)\n        self.assertEqual(self.leaky_bucket.consume(amt, RequestToken()), amt)\n        self.assert_recorded_consumed_amt(amt)\n        self.assert_nothing_scheduled()\n\n    def test_consume_over_max_rate(self):\n        amt = 1\n        retry_time = 2.0\n        self.set_projected_rate(self.max_rate + 1)\n        self.set_retry_time(retry_time)\n        request_token = RequestToken()\n        try:\n            self.leaky_bucket.consume(amt, request_token)\n            self.fail('A RequestExceededException should have been thrown')\n        except RequestExceededException as e:\n            self.assertEqual(e.requested_amt, amt)\n            self.assertEqual(e.retry_time, retry_time)\n        self.assert_was_scheduled(amt, request_token)\n\n    def test_consume_with_scheduled_retry(self):\n        amt = 1\n        self.set_projected_rate(self.max_rate + 1)\n        self.scheduler.is_scheduled.return_value = True\n        request_token = RequestToken()\n        self.assertEqual(self.leaky_bucket.consume(amt, request_token), amt)\n        # Nothing new should have been scheduled but the request token\n        # should have been processed.\n        self.assert_nothing_scheduled()\n        self.assert_processed_request_token(request_token)\n\n\nclass TestConsumptionScheduler(unittest.TestCase):\n    def setUp(self):\n        self.scheduler = ConsumptionScheduler()\n\n    def test_schedule_consumption(self):\n        token = RequestToken()\n        consume_time = 5\n        actual_wait_time = self.scheduler.schedule_consumption(\n            1, token, consume_time\n        )\n        self.assertEqual(consume_time, actual_wait_time)\n\n    def test_schedule_consumption_for_multiple_requests(self):\n        token = RequestToken()\n        consume_time = 5\n        actual_wait_time = self.scheduler.schedule_consumption(\n            1, token, consume_time\n        )\n        self.assertEqual(consume_time, actual_wait_time)\n\n        other_consume_time = 3\n        other_token = RequestToken()\n        next_wait_time = self.scheduler.schedule_consumption(\n            1, other_token, other_consume_time\n        )\n\n        # This wait time should be the previous time plus its desired\n        # wait time\n        self.assertEqual(next_wait_time, consume_time + other_consume_time)\n\n    def test_is_scheduled(self):\n        token = RequestToken()\n        consume_time = 5\n        self.scheduler.schedule_consumption(1, token, consume_time)\n        self.assertTrue(self.scheduler.is_scheduled(token))\n\n    def test_is_not_scheduled(self):\n        self.assertFalse(self.scheduler.is_scheduled(RequestToken()))\n\n    def test_process_scheduled_consumption(self):\n        token = RequestToken()\n        consume_time = 5\n        self.scheduler.schedule_consumption(1, token, consume_time)\n        self.scheduler.process_scheduled_consumption(token)\n        self.assertFalse(self.scheduler.is_scheduled(token))\n        different_time = 7\n        # The previous consume time should have no affect on the next wait tim\n        # as it has been completed.\n        self.assertEqual(\n            self.scheduler.schedule_consumption(1, token, different_time),\n            different_time,\n        )\n\n\nclass TestBandwidthRateTracker(unittest.TestCase):\n    def setUp(self):\n        self.alpha = 0.8\n        self.rate_tracker = BandwidthRateTracker(self.alpha)\n\n    def test_current_rate_at_initilizations(self):\n        self.assertEqual(self.rate_tracker.current_rate, 0.0)\n\n    def test_current_rate_after_one_recorded_point(self):\n        self.rate_tracker.record_consumption_rate(1, 1)\n        # There is no last time point to do a diff against so return a\n        # current rate of 0.0\n        self.assertEqual(self.rate_tracker.current_rate, 0.0)\n\n    def test_current_rate(self):\n        self.rate_tracker.record_consumption_rate(1, 1)\n        self.rate_tracker.record_consumption_rate(1, 2)\n        self.rate_tracker.record_consumption_rate(1, 3)\n        self.assertEqual(self.rate_tracker.current_rate, 0.96)\n\n    def test_get_projected_rate_at_initilizations(self):\n        self.assertEqual(self.rate_tracker.get_projected_rate(1, 1), 0.0)\n\n    def test_get_projected_rate(self):\n        self.rate_tracker.record_consumption_rate(1, 1)\n        self.rate_tracker.record_consumption_rate(1, 2)\n        projected_rate = self.rate_tracker.get_projected_rate(1, 3)\n        self.assertEqual(projected_rate, 0.96)\n        self.rate_tracker.record_consumption_rate(1, 3)\n        self.assertEqual(self.rate_tracker.current_rate, projected_rate)\n\n    def test_get_projected_rate_for_same_timestamp(self):\n        self.rate_tracker.record_consumption_rate(1, 1)\n        self.assertEqual(\n            self.rate_tracker.get_projected_rate(1, 1), float('inf')\n        )\n", "tests/unit/test_tasks.py": "# Copyright 2016 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nfrom concurrent import futures\nfrom functools import partial\nfrom threading import Event\n\nfrom s3transfer.futures import BoundedExecutor, TransferCoordinator\nfrom s3transfer.subscribers import BaseSubscriber\nfrom s3transfer.tasks import (\n    CompleteMultipartUploadTask,\n    CreateMultipartUploadTask,\n    SubmissionTask,\n    Task,\n)\nfrom s3transfer.utils import CallArgs, FunctionContainer, get_callbacks\nfrom tests import (\n    BaseSubmissionTaskTest,\n    BaseTaskTest,\n    RecordingSubscriber,\n    unittest,\n)\n\n\nclass TaskFailureException(Exception):\n    pass\n\n\nclass SuccessTask(Task):\n    def _main(\n        self, return_value='success', callbacks=None, failure_cleanups=None\n    ):\n        if callbacks:\n            for callback in callbacks:\n                callback()\n        if failure_cleanups:\n            for failure_cleanup in failure_cleanups:\n                self._transfer_coordinator.add_failure_cleanup(failure_cleanup)\n        return return_value\n\n\nclass FailureTask(Task):\n    def _main(self, exception=TaskFailureException):\n        raise exception()\n\n\nclass ReturnKwargsTask(Task):\n    def _main(self, **kwargs):\n        return kwargs\n\n\nclass SubmitMoreTasksTask(Task):\n    def _main(self, executor, tasks_to_submit):\n        for task_to_submit in tasks_to_submit:\n            self._transfer_coordinator.submit(executor, task_to_submit)\n\n\nclass NOOPSubmissionTask(SubmissionTask):\n    def _submit(self, transfer_future, **kwargs):\n        pass\n\n\nclass ExceptionSubmissionTask(SubmissionTask):\n    def _submit(\n        self,\n        transfer_future,\n        executor=None,\n        tasks_to_submit=None,\n        additional_callbacks=None,\n        exception=TaskFailureException,\n    ):\n        if executor and tasks_to_submit:\n            for task_to_submit in tasks_to_submit:\n                self._transfer_coordinator.submit(executor, task_to_submit)\n        if additional_callbacks:\n            for callback in additional_callbacks:\n                callback()\n        raise exception()\n\n\nclass StatusRecordingTransferCoordinator(TransferCoordinator):\n    def __init__(self, transfer_id=None):\n        super().__init__(transfer_id)\n        self.status_changes = [self._status]\n\n    def set_status_to_queued(self):\n        super().set_status_to_queued()\n        self._record_status_change()\n\n    def set_status_to_running(self):\n        super().set_status_to_running()\n        self._record_status_change()\n\n    def _record_status_change(self):\n        self.status_changes.append(self._status)\n\n\nclass RecordingStateSubscriber(BaseSubscriber):\n    def __init__(self, transfer_coordinator):\n        self._transfer_coordinator = transfer_coordinator\n        self.status_during_on_queued = None\n\n    def on_queued(self, **kwargs):\n        self.status_during_on_queued = self._transfer_coordinator.status\n\n\nclass TestSubmissionTask(BaseSubmissionTaskTest):\n    def setUp(self):\n        super().setUp()\n        self.executor = BoundedExecutor(1000, 5)\n        self.call_args = CallArgs(subscribers=[])\n        self.transfer_future = self.get_transfer_future(self.call_args)\n        self.main_kwargs = {'transfer_future': self.transfer_future}\n\n    def test_transitions_from_not_started_to_queued_to_running(self):\n        self.transfer_coordinator = StatusRecordingTransferCoordinator()\n        submission_task = self.get_task(\n            NOOPSubmissionTask, main_kwargs=self.main_kwargs\n        )\n        # Status should be queued until submission task has been ran.\n        self.assertEqual(self.transfer_coordinator.status, 'not-started')\n\n        submission_task()\n        # Once submission task has been ran, the status should now be running.\n        self.assertEqual(self.transfer_coordinator.status, 'running')\n\n        # Ensure the transitions were as expected as well.\n        self.assertEqual(\n            self.transfer_coordinator.status_changes,\n            ['not-started', 'queued', 'running'],\n        )\n\n    def test_on_queued_callbacks(self):\n        submission_task = self.get_task(\n            NOOPSubmissionTask, main_kwargs=self.main_kwargs\n        )\n\n        subscriber = RecordingSubscriber()\n        self.call_args.subscribers.append(subscriber)\n        submission_task()\n        # Make sure the on_queued callback of the subscriber is called.\n        self.assertEqual(\n            subscriber.on_queued_calls, [{'future': self.transfer_future}]\n        )\n\n    def test_on_queued_status_in_callbacks(self):\n        submission_task = self.get_task(\n            NOOPSubmissionTask, main_kwargs=self.main_kwargs\n        )\n\n        subscriber = RecordingStateSubscriber(self.transfer_coordinator)\n        self.call_args.subscribers.append(subscriber)\n        submission_task()\n        # Make sure the status was queued during on_queued callback.\n        self.assertEqual(subscriber.status_during_on_queued, 'queued')\n\n    def test_sets_exception_from_submit(self):\n        submission_task = self.get_task(\n            ExceptionSubmissionTask, main_kwargs=self.main_kwargs\n        )\n        submission_task()\n\n        # Make sure the status of the future is failed\n        self.assertEqual(self.transfer_coordinator.status, 'failed')\n\n        # Make sure the future propagates the exception encountered in the\n        # submission task.\n        with self.assertRaises(TaskFailureException):\n            self.transfer_future.result()\n\n    def test_catches_and_sets_keyboard_interrupt_exception_from_submit(self):\n        self.main_kwargs['exception'] = KeyboardInterrupt\n        submission_task = self.get_task(\n            ExceptionSubmissionTask, main_kwargs=self.main_kwargs\n        )\n        submission_task()\n\n        self.assertEqual(self.transfer_coordinator.status, 'failed')\n        with self.assertRaises(KeyboardInterrupt):\n            self.transfer_future.result()\n\n    def test_calls_done_callbacks_on_exception(self):\n        submission_task = self.get_task(\n            ExceptionSubmissionTask, main_kwargs=self.main_kwargs\n        )\n\n        subscriber = RecordingSubscriber()\n        self.call_args.subscribers.append(subscriber)\n\n        # Add the done callback to the callbacks to be invoked when the\n        # transfer is done.\n        done_callbacks = get_callbacks(self.transfer_future, 'done')\n        for done_callback in done_callbacks:\n            self.transfer_coordinator.add_done_callback(done_callback)\n        submission_task()\n\n        # Make sure the task failed to start\n        self.assertEqual(self.transfer_coordinator.status, 'failed')\n\n        # Make sure the on_done callback of the subscriber is called.\n        self.assertEqual(\n            subscriber.on_done_calls, [{'future': self.transfer_future}]\n        )\n\n    def test_calls_failure_cleanups_on_exception(self):\n        submission_task = self.get_task(\n            ExceptionSubmissionTask, main_kwargs=self.main_kwargs\n        )\n\n        # Add the callback to the callbacks to be invoked when the\n        # transfer fails.\n        invocations_of_cleanup = []\n        cleanup_callback = FunctionContainer(\n            invocations_of_cleanup.append, 'cleanup happened'\n        )\n        self.transfer_coordinator.add_failure_cleanup(cleanup_callback)\n        submission_task()\n\n        # Make sure the task failed to start\n        self.assertEqual(self.transfer_coordinator.status, 'failed')\n\n        # Make sure the cleanup was called.\n        self.assertEqual(invocations_of_cleanup, ['cleanup happened'])\n\n    def test_cleanups_only_ran_once_on_exception(self):\n        # We want to be able to handle the case where the final task completes\n        # and anounces done but there is an error in the submission task\n        # which will cause it to need to announce done as well. In this case,\n        # we do not want the done callbacks to be invoke more than once.\n\n        final_task = self.get_task(FailureTask, is_final=True)\n        self.main_kwargs['executor'] = self.executor\n        self.main_kwargs['tasks_to_submit'] = [final_task]\n\n        submission_task = self.get_task(\n            ExceptionSubmissionTask, main_kwargs=self.main_kwargs\n        )\n\n        subscriber = RecordingSubscriber()\n        self.call_args.subscribers.append(subscriber)\n\n        # Add the done callback to the callbacks to be invoked when the\n        # transfer is done.\n        done_callbacks = get_callbacks(self.transfer_future, 'done')\n        for done_callback in done_callbacks:\n            self.transfer_coordinator.add_done_callback(done_callback)\n\n        submission_task()\n\n        # Make sure the task failed to start\n        self.assertEqual(self.transfer_coordinator.status, 'failed')\n\n        # Make sure the on_done callback of the subscriber is called only once.\n        self.assertEqual(\n            subscriber.on_done_calls, [{'future': self.transfer_future}]\n        )\n\n    def test_done_callbacks_only_ran_once_on_exception(self):\n        # We want to be able to handle the case where the final task completes\n        # and anounces done but there is an error in the submission task\n        # which will cause it to need to announce done as well. In this case,\n        # we do not want the failure cleanups to be invoked more than once.\n\n        final_task = self.get_task(FailureTask, is_final=True)\n        self.main_kwargs['executor'] = self.executor\n        self.main_kwargs['tasks_to_submit'] = [final_task]\n\n        submission_task = self.get_task(\n            ExceptionSubmissionTask, main_kwargs=self.main_kwargs\n        )\n\n        # Add the callback to the callbacks to be invoked when the\n        # transfer fails.\n        invocations_of_cleanup = []\n        cleanup_callback = FunctionContainer(\n            invocations_of_cleanup.append, 'cleanup happened'\n        )\n        self.transfer_coordinator.add_failure_cleanup(cleanup_callback)\n        submission_task()\n\n        # Make sure the task failed to start\n        self.assertEqual(self.transfer_coordinator.status, 'failed')\n\n        # Make sure the cleanup was called only once.\n        self.assertEqual(invocations_of_cleanup, ['cleanup happened'])\n\n    def test_handles_cleanups_submitted_in_other_tasks(self):\n        invocations_of_cleanup = []\n        event = Event()\n        cleanup_callback = FunctionContainer(\n            invocations_of_cleanup.append, 'cleanup happened'\n        )\n        # We want the cleanup to be added in the execution of the task and\n        # still be executed by the submission task when it fails.\n        task = self.get_task(\n            SuccessTask,\n            main_kwargs={\n                'callbacks': [event.set],\n                'failure_cleanups': [cleanup_callback],\n            },\n        )\n\n        self.main_kwargs['executor'] = self.executor\n        self.main_kwargs['tasks_to_submit'] = [task]\n        self.main_kwargs['additional_callbacks'] = [event.wait]\n\n        submission_task = self.get_task(\n            ExceptionSubmissionTask, main_kwargs=self.main_kwargs\n        )\n\n        submission_task()\n        self.assertEqual(self.transfer_coordinator.status, 'failed')\n\n        # Make sure the cleanup was called even though the callback got\n        # added in a completely different task.\n        self.assertEqual(invocations_of_cleanup, ['cleanup happened'])\n\n    def test_waits_for_tasks_submitted_by_other_tasks_on_exception(self):\n        # In this test, we want to make sure that any tasks that may be\n        # submitted in another task complete before we start performing\n        # cleanups.\n        #\n        # This is tested by doing the following:\n        #\n        # ExecutionSubmissionTask\n        #   |\n        #   +--submits-->SubmitMoreTasksTask\n        #                   |\n        #                   +--submits-->SuccessTask\n        #                                  |\n        #                                  +-->sleeps-->adds failure cleanup\n        #\n        # In the end, the failure cleanup of the SuccessTask should be ran\n        # when the ExecutionSubmissionTask fails. If the\n        # ExeceptionSubmissionTask did not run the failure cleanup it is most\n        # likely that it did not wait for the SuccessTask to complete, which\n        # it needs to because the ExeceptionSubmissionTask does not know\n        # what failure cleanups it needs to run until all spawned tasks have\n        # completed.\n        invocations_of_cleanup = []\n        event = Event()\n        cleanup_callback = FunctionContainer(\n            invocations_of_cleanup.append, 'cleanup happened'\n        )\n\n        cleanup_task = self.get_task(\n            SuccessTask,\n            main_kwargs={\n                'callbacks': [event.set],\n                'failure_cleanups': [cleanup_callback],\n            },\n        )\n        task_for_submitting_cleanup_task = self.get_task(\n            SubmitMoreTasksTask,\n            main_kwargs={\n                'executor': self.executor,\n                'tasks_to_submit': [cleanup_task],\n            },\n        )\n\n        self.main_kwargs['executor'] = self.executor\n        self.main_kwargs['tasks_to_submit'] = [\n            task_for_submitting_cleanup_task\n        ]\n        self.main_kwargs['additional_callbacks'] = [event.wait]\n\n        submission_task = self.get_task(\n            ExceptionSubmissionTask, main_kwargs=self.main_kwargs\n        )\n\n        submission_task()\n        self.assertEqual(self.transfer_coordinator.status, 'failed')\n        self.assertEqual(invocations_of_cleanup, ['cleanup happened'])\n\n    def test_submission_task_announces_done_if_cancelled_before_main(self):\n        invocations_of_done = []\n        done_callback = FunctionContainer(\n            invocations_of_done.append, 'done announced'\n        )\n        self.transfer_coordinator.add_done_callback(done_callback)\n\n        self.transfer_coordinator.cancel()\n        submission_task = self.get_task(\n            NOOPSubmissionTask, main_kwargs=self.main_kwargs\n        )\n        submission_task()\n\n        # Because the submission task was cancelled before being run\n        # it did not submit any extra tasks so a result it is responsible\n        # for making sure it announces done as nothing else will.\n        self.assertEqual(invocations_of_done, ['done announced'])\n\n\nclass TestTask(unittest.TestCase):\n    def setUp(self):\n        self.transfer_id = 1\n        self.transfer_coordinator = TransferCoordinator(\n            transfer_id=self.transfer_id\n        )\n\n    def test_repr(self):\n        main_kwargs = {'bucket': 'mybucket', 'param_to_not_include': 'foo'}\n        task = ReturnKwargsTask(\n            self.transfer_coordinator, main_kwargs=main_kwargs\n        )\n        # The repr should not include the other parameter because it is not\n        # a desired parameter to include.\n        self.assertEqual(\n            repr(task),\n            'ReturnKwargsTask(transfer_id={}, {})'.format(\n                self.transfer_id, {'bucket': 'mybucket'}\n            ),\n        )\n\n    def test_transfer_id(self):\n        task = SuccessTask(self.transfer_coordinator)\n        # Make sure that the id is the one provided to the id associated\n        # to the transfer coordinator.\n        self.assertEqual(task.transfer_id, self.transfer_id)\n\n    def test_context_status_transitioning_success(self):\n        # The status should be set to running.\n        self.transfer_coordinator.set_status_to_running()\n        self.assertEqual(self.transfer_coordinator.status, 'running')\n\n        # If a task is called, the status still should be running.\n        SuccessTask(self.transfer_coordinator)()\n        self.assertEqual(self.transfer_coordinator.status, 'running')\n\n        # Once the final task is called, the status should be set to success.\n        SuccessTask(self.transfer_coordinator, is_final=True)()\n        self.assertEqual(self.transfer_coordinator.status, 'success')\n\n    def test_context_status_transitioning_failed(self):\n        self.transfer_coordinator.set_status_to_running()\n\n        SuccessTask(self.transfer_coordinator)()\n        self.assertEqual(self.transfer_coordinator.status, 'running')\n\n        # A failure task should result in the failed status\n        FailureTask(self.transfer_coordinator)()\n        self.assertEqual(self.transfer_coordinator.status, 'failed')\n\n        # Even if the final task comes in and succeeds, it should stay failed.\n        SuccessTask(self.transfer_coordinator, is_final=True)()\n        self.assertEqual(self.transfer_coordinator.status, 'failed')\n\n    def test_result_setting_for_success(self):\n        override_return = 'foo'\n        SuccessTask(self.transfer_coordinator)()\n        SuccessTask(\n            self.transfer_coordinator,\n            main_kwargs={'return_value': override_return},\n            is_final=True,\n        )()\n\n        # The return value for the transfer future should be of the final\n        # task.\n        self.assertEqual(self.transfer_coordinator.result(), override_return)\n\n    def test_result_setting_for_error(self):\n        FailureTask(self.transfer_coordinator)()\n\n        # If another failure comes in, the result should still throw the\n        # original exception when result() is eventually called.\n        FailureTask(\n            self.transfer_coordinator, main_kwargs={'exception': Exception}\n        )()\n\n        # Even if a success task comes along, the result of the future\n        # should be the original exception\n        SuccessTask(self.transfer_coordinator, is_final=True)()\n        with self.assertRaises(TaskFailureException):\n            self.transfer_coordinator.result()\n\n    def test_done_callbacks_success(self):\n        callback_results = []\n        SuccessTask(\n            self.transfer_coordinator,\n            done_callbacks=[\n                partial(callback_results.append, 'first'),\n                partial(callback_results.append, 'second'),\n            ],\n        )()\n        # For successful tasks, the done callbacks should get called.\n        self.assertEqual(callback_results, ['first', 'second'])\n\n    def test_done_callbacks_failure(self):\n        callback_results = []\n        FailureTask(\n            self.transfer_coordinator,\n            done_callbacks=[\n                partial(callback_results.append, 'first'),\n                partial(callback_results.append, 'second'),\n            ],\n        )()\n        # For even failed tasks, the done callbacks should get called.\n        self.assertEqual(callback_results, ['first', 'second'])\n\n        # Callbacks should continue to be called even after a related failure\n        SuccessTask(\n            self.transfer_coordinator,\n            done_callbacks=[\n                partial(callback_results.append, 'third'),\n                partial(callback_results.append, 'fourth'),\n            ],\n        )()\n        self.assertEqual(\n            callback_results, ['first', 'second', 'third', 'fourth']\n        )\n\n    def test_failure_cleanups_on_failure(self):\n        callback_results = []\n        self.transfer_coordinator.add_failure_cleanup(\n            callback_results.append, 'first'\n        )\n        self.transfer_coordinator.add_failure_cleanup(\n            callback_results.append, 'second'\n        )\n        FailureTask(self.transfer_coordinator)()\n        # The failure callbacks should have not been called yet because it\n        # is not the last task\n        self.assertEqual(callback_results, [])\n\n        # Now the failure callbacks should get called.\n        SuccessTask(self.transfer_coordinator, is_final=True)()\n        self.assertEqual(callback_results, ['first', 'second'])\n\n    def test_no_failure_cleanups_on_success(self):\n        callback_results = []\n        self.transfer_coordinator.add_failure_cleanup(\n            callback_results.append, 'first'\n        )\n        self.transfer_coordinator.add_failure_cleanup(\n            callback_results.append, 'second'\n        )\n        SuccessTask(self.transfer_coordinator, is_final=True)()\n        # The failure cleanups should not have been called because no task\n        # failed for the transfer context.\n        self.assertEqual(callback_results, [])\n\n    def test_passing_main_kwargs(self):\n        main_kwargs = {'foo': 'bar', 'baz': 'biz'}\n        ReturnKwargsTask(\n            self.transfer_coordinator, main_kwargs=main_kwargs, is_final=True\n        )()\n        # The kwargs should have been passed to the main()\n        self.assertEqual(self.transfer_coordinator.result(), main_kwargs)\n\n    def test_passing_pending_kwargs_single_futures(self):\n        pending_kwargs = {}\n        ref_main_kwargs = {'foo': 'bar', 'baz': 'biz'}\n\n        # Pass some tasks to an executor\n        with futures.ThreadPoolExecutor(1) as executor:\n            pending_kwargs['foo'] = executor.submit(\n                SuccessTask(\n                    self.transfer_coordinator,\n                    main_kwargs={'return_value': ref_main_kwargs['foo']},\n                )\n            )\n            pending_kwargs['baz'] = executor.submit(\n                SuccessTask(\n                    self.transfer_coordinator,\n                    main_kwargs={'return_value': ref_main_kwargs['baz']},\n                )\n            )\n\n        # Create a task that depends on the tasks passed to the executor\n        ReturnKwargsTask(\n            self.transfer_coordinator,\n            pending_main_kwargs=pending_kwargs,\n            is_final=True,\n        )()\n        # The result should have the pending keyword arg values flushed\n        # out.\n        self.assertEqual(self.transfer_coordinator.result(), ref_main_kwargs)\n\n    def test_passing_pending_kwargs_list_of_futures(self):\n        pending_kwargs = {}\n        ref_main_kwargs = {'foo': ['first', 'second']}\n\n        # Pass some tasks to an executor\n        with futures.ThreadPoolExecutor(1) as executor:\n            first_future = executor.submit(\n                SuccessTask(\n                    self.transfer_coordinator,\n                    main_kwargs={'return_value': ref_main_kwargs['foo'][0]},\n                )\n            )\n            second_future = executor.submit(\n                SuccessTask(\n                    self.transfer_coordinator,\n                    main_kwargs={'return_value': ref_main_kwargs['foo'][1]},\n                )\n            )\n            # Make the pending keyword arg value a list\n            pending_kwargs['foo'] = [first_future, second_future]\n\n        # Create a task that depends on the tasks passed to the executor\n        ReturnKwargsTask(\n            self.transfer_coordinator,\n            pending_main_kwargs=pending_kwargs,\n            is_final=True,\n        )()\n        # The result should have the pending keyword arg values flushed\n        # out in the expected order.\n        self.assertEqual(self.transfer_coordinator.result(), ref_main_kwargs)\n\n    def test_passing_pending_and_non_pending_kwargs(self):\n        main_kwargs = {'nonpending_value': 'foo'}\n        pending_kwargs = {}\n        ref_main_kwargs = {\n            'nonpending_value': 'foo',\n            'pending_value': 'bar',\n            'pending_list': ['first', 'second'],\n        }\n\n        # Create the pending tasks\n        with futures.ThreadPoolExecutor(1) as executor:\n            pending_kwargs['pending_value'] = executor.submit(\n                SuccessTask(\n                    self.transfer_coordinator,\n                    main_kwargs={\n                        'return_value': ref_main_kwargs['pending_value']\n                    },\n                )\n            )\n\n            first_future = executor.submit(\n                SuccessTask(\n                    self.transfer_coordinator,\n                    main_kwargs={\n                        'return_value': ref_main_kwargs['pending_list'][0]\n                    },\n                )\n            )\n            second_future = executor.submit(\n                SuccessTask(\n                    self.transfer_coordinator,\n                    main_kwargs={\n                        'return_value': ref_main_kwargs['pending_list'][1]\n                    },\n                )\n            )\n            # Make the pending keyword arg value a list\n            pending_kwargs['pending_list'] = [first_future, second_future]\n\n        # Create a task that depends on the tasks passed to the executor\n        # and just regular nonpending kwargs.\n        ReturnKwargsTask(\n            self.transfer_coordinator,\n            main_kwargs=main_kwargs,\n            pending_main_kwargs=pending_kwargs,\n            is_final=True,\n        )()\n        # The result should have all of the kwargs (both pending and\n        # nonpending)\n        self.assertEqual(self.transfer_coordinator.result(), ref_main_kwargs)\n\n    def test_single_failed_pending_future(self):\n        pending_kwargs = {}\n\n        # Pass some tasks to an executor. Make one successful and the other\n        # a failure.\n        with futures.ThreadPoolExecutor(1) as executor:\n            pending_kwargs['foo'] = executor.submit(\n                SuccessTask(\n                    self.transfer_coordinator,\n                    main_kwargs={'return_value': 'bar'},\n                )\n            )\n            pending_kwargs['baz'] = executor.submit(\n                FailureTask(self.transfer_coordinator)\n            )\n\n        # Create a task that depends on the tasks passed to the executor\n        ReturnKwargsTask(\n            self.transfer_coordinator,\n            pending_main_kwargs=pending_kwargs,\n            is_final=True,\n        )()\n        # The end result should raise the exception from the initial\n        # pending future value\n        with self.assertRaises(TaskFailureException):\n            self.transfer_coordinator.result()\n\n    def test_single_failed_pending_future_in_list(self):\n        pending_kwargs = {}\n\n        # Pass some tasks to an executor. Make one successful and the other\n        # a failure.\n        with futures.ThreadPoolExecutor(1) as executor:\n            first_future = executor.submit(\n                SuccessTask(\n                    self.transfer_coordinator,\n                    main_kwargs={'return_value': 'bar'},\n                )\n            )\n            second_future = executor.submit(\n                FailureTask(self.transfer_coordinator)\n            )\n\n            pending_kwargs['pending_list'] = [first_future, second_future]\n\n        # Create a task that depends on the tasks passed to the executor\n        ReturnKwargsTask(\n            self.transfer_coordinator,\n            pending_main_kwargs=pending_kwargs,\n            is_final=True,\n        )()\n        # The end result should raise the exception from the initial\n        # pending future value in the list\n        with self.assertRaises(TaskFailureException):\n            self.transfer_coordinator.result()\n\n\nclass BaseMultipartTaskTest(BaseTaskTest):\n    def setUp(self):\n        super().setUp()\n        self.bucket = 'mybucket'\n        self.key = 'foo'\n\n\nclass TestCreateMultipartUploadTask(BaseMultipartTaskTest):\n    def test_main(self):\n        upload_id = 'foo'\n        extra_args = {'Metadata': {'foo': 'bar'}}\n        response = {'UploadId': upload_id}\n        task = self.get_task(\n            CreateMultipartUploadTask,\n            main_kwargs={\n                'client': self.client,\n                'bucket': self.bucket,\n                'key': self.key,\n                'extra_args': extra_args,\n            },\n        )\n        self.stubber.add_response(\n            method='create_multipart_upload',\n            service_response=response,\n            expected_params={\n                'Bucket': self.bucket,\n                'Key': self.key,\n                'Metadata': {'foo': 'bar'},\n            },\n        )\n        result_id = task()\n        self.stubber.assert_no_pending_responses()\n        # Ensure the upload id returned is correct\n        self.assertEqual(upload_id, result_id)\n\n        # Make sure that the abort was added as a cleanup failure\n        self.assertEqual(len(self.transfer_coordinator.failure_cleanups), 1)\n\n        # Make sure if it is called, it will abort correctly\n        self.stubber.add_response(\n            method='abort_multipart_upload',\n            service_response={},\n            expected_params={\n                'Bucket': self.bucket,\n                'Key': self.key,\n                'UploadId': upload_id,\n            },\n        )\n        self.transfer_coordinator.failure_cleanups[0]()\n        self.stubber.assert_no_pending_responses()\n\n\nclass TestCompleteMultipartUploadTask(BaseMultipartTaskTest):\n    def test_main(self):\n        upload_id = 'my-id'\n        parts = [{'ETag': 'etag', 'PartNumber': 0}]\n        task = self.get_task(\n            CompleteMultipartUploadTask,\n            main_kwargs={\n                'client': self.client,\n                'bucket': self.bucket,\n                'key': self.key,\n                'upload_id': upload_id,\n                'parts': parts,\n                'extra_args': {},\n            },\n        )\n        self.stubber.add_response(\n            method='complete_multipart_upload',\n            service_response={},\n            expected_params={\n                'Bucket': self.bucket,\n                'Key': self.key,\n                'UploadId': upload_id,\n                'MultipartUpload': {'Parts': parts},\n            },\n        )\n        task()\n        self.stubber.assert_no_pending_responses()\n\n    def test_includes_extra_args(self):\n        upload_id = 'my-id'\n        parts = [{'ETag': 'etag', 'PartNumber': 0}]\n        task = self.get_task(\n            CompleteMultipartUploadTask,\n            main_kwargs={\n                'client': self.client,\n                'bucket': self.bucket,\n                'key': self.key,\n                'upload_id': upload_id,\n                'parts': parts,\n                'extra_args': {'RequestPayer': 'requester'},\n            },\n        )\n        self.stubber.add_response(\n            method='complete_multipart_upload',\n            service_response={},\n            expected_params={\n                'Bucket': self.bucket,\n                'Key': self.key,\n                'UploadId': upload_id,\n                'MultipartUpload': {'Parts': parts},\n                'RequestPayer': 'requester',\n            },\n        )\n        task()\n        self.stubber.assert_no_pending_responses()\n", "tests/unit/__init__.py": "# Copyright 2016 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License'). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the 'license' file accompanying this file. This file is\n# distributed on an 'AS IS' BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n", "tests/unit/test_upload.py": "# Copyright 2016 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License'). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the 'license' file accompanying this file. This file is\n# distributed on an 'AS IS' BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\nimport math\nimport os\nimport shutil\nimport tempfile\nfrom io import BytesIO\n\nfrom botocore.stub import ANY\n\nfrom s3transfer.futures import IN_MEMORY_UPLOAD_TAG\nfrom s3transfer.manager import TransferConfig\nfrom s3transfer.upload import (\n    AggregatedProgressCallback,\n    InterruptReader,\n    PutObjectTask,\n    UploadFilenameInputManager,\n    UploadNonSeekableInputManager,\n    UploadPartTask,\n    UploadSeekableInputManager,\n    UploadSubmissionTask,\n)\nfrom s3transfer.utils import MIN_UPLOAD_CHUNKSIZE, CallArgs, OSUtils\nfrom tests import (\n    BaseSubmissionTaskTest,\n    BaseTaskTest,\n    FileSizeProvider,\n    NonSeekableReader,\n    RecordingExecutor,\n    RecordingSubscriber,\n    unittest,\n)\n\n\nclass InterruptionError(Exception):\n    pass\n\n\nclass OSUtilsExceptionOnFileSize(OSUtils):\n    def get_file_size(self, filename):\n        raise AssertionError(\n            \"The file %s should not have been stated\" % filename\n        )\n\n\nclass BaseUploadTest(BaseTaskTest):\n    def setUp(self):\n        super().setUp()\n        self.bucket = 'mybucket'\n        self.key = 'foo'\n        self.osutil = OSUtils()\n\n        self.tempdir = tempfile.mkdtemp()\n        self.filename = os.path.join(self.tempdir, 'myfile')\n        self.content = b'my content'\n        self.subscribers = []\n\n        with open(self.filename, 'wb') as f:\n            f.write(self.content)\n\n        # A list to keep track of all of the bodies sent over the wire\n        # and their order.\n        self.sent_bodies = []\n        self.client.meta.events.register(\n            'before-parameter-build.s3.*', self.collect_body\n        )\n\n    def tearDown(self):\n        super().tearDown()\n        shutil.rmtree(self.tempdir)\n\n    def collect_body(self, params, **kwargs):\n        if 'Body' in params:\n            self.sent_bodies.append(params['Body'].read())\n\n\nclass TestAggregatedProgressCallback(unittest.TestCase):\n    def setUp(self):\n        self.aggregated_amounts = []\n        self.threshold = 3\n        self.aggregated_progress_callback = AggregatedProgressCallback(\n            [self.callback], self.threshold\n        )\n\n    def callback(self, bytes_transferred):\n        self.aggregated_amounts.append(bytes_transferred)\n\n    def test_under_threshold(self):\n        one_under_threshold_amount = self.threshold - 1\n        self.aggregated_progress_callback(one_under_threshold_amount)\n        self.assertEqual(self.aggregated_amounts, [])\n        self.aggregated_progress_callback(1)\n        self.assertEqual(self.aggregated_amounts, [self.threshold])\n\n    def test_at_threshold(self):\n        self.aggregated_progress_callback(self.threshold)\n        self.assertEqual(self.aggregated_amounts, [self.threshold])\n\n    def test_over_threshold(self):\n        over_threshold_amount = self.threshold + 1\n        self.aggregated_progress_callback(over_threshold_amount)\n        self.assertEqual(self.aggregated_amounts, [over_threshold_amount])\n\n    def test_flush(self):\n        under_threshold_amount = self.threshold - 1\n        self.aggregated_progress_callback(under_threshold_amount)\n        self.assertEqual(self.aggregated_amounts, [])\n        self.aggregated_progress_callback.flush()\n        self.assertEqual(self.aggregated_amounts, [under_threshold_amount])\n\n    def test_flush_with_nothing_to_flush(self):\n        under_threshold_amount = self.threshold - 1\n        self.aggregated_progress_callback(under_threshold_amount)\n        self.assertEqual(self.aggregated_amounts, [])\n        self.aggregated_progress_callback.flush()\n        self.assertEqual(self.aggregated_amounts, [under_threshold_amount])\n        # Flushing again should do nothing as it was just flushed\n        self.aggregated_progress_callback.flush()\n        self.assertEqual(self.aggregated_amounts, [under_threshold_amount])\n\n\nclass TestInterruptReader(BaseUploadTest):\n    def test_read_raises_exception(self):\n        with open(self.filename, 'rb') as f:\n            reader = InterruptReader(f, self.transfer_coordinator)\n            # Read some bytes to show it can be read.\n            self.assertEqual(reader.read(1), self.content[0:1])\n            # Then set an exception in the transfer coordinator\n            self.transfer_coordinator.set_exception(InterruptionError())\n            # The next read should have the exception propograte\n            with self.assertRaises(InterruptionError):\n                reader.read()\n\n    def test_seek(self):\n        with open(self.filename, 'rb') as f:\n            reader = InterruptReader(f, self.transfer_coordinator)\n            # Ensure it can seek correctly\n            reader.seek(1)\n            self.assertEqual(reader.read(1), self.content[1:2])\n\n    def test_tell(self):\n        with open(self.filename, 'rb') as f:\n            reader = InterruptReader(f, self.transfer_coordinator)\n            # Ensure it can tell correctly\n            reader.seek(1)\n            self.assertEqual(reader.tell(), 1)\n\n\nclass BaseUploadInputManagerTest(BaseUploadTest):\n    def setUp(self):\n        super().setUp()\n        self.osutil = OSUtils()\n        self.config = TransferConfig()\n        self.recording_subscriber = RecordingSubscriber()\n        self.subscribers.append(self.recording_subscriber)\n\n    def _get_expected_body_for_part(self, part_number):\n        # A helper method for retrieving the expected body for a specific\n        # part number of the data\n        total_size = len(self.content)\n        chunk_size = self.config.multipart_chunksize\n        start_index = (part_number - 1) * chunk_size\n        end_index = part_number * chunk_size\n        if end_index >= total_size:\n            return self.content[start_index:]\n        return self.content[start_index:end_index]\n\n\nclass TestUploadFilenameInputManager(BaseUploadInputManagerTest):\n    def setUp(self):\n        super().setUp()\n        self.upload_input_manager = UploadFilenameInputManager(\n            self.osutil, self.transfer_coordinator\n        )\n        self.call_args = CallArgs(\n            fileobj=self.filename, subscribers=self.subscribers\n        )\n        self.future = self.get_transfer_future(self.call_args)\n\n    def test_is_compatible(self):\n        self.assertTrue(\n            self.upload_input_manager.is_compatible(\n                self.future.meta.call_args.fileobj\n            )\n        )\n\n    def test_stores_bodies_in_memory_put_object(self):\n        self.assertFalse(\n            self.upload_input_manager.stores_body_in_memory('put_object')\n        )\n\n    def test_stores_bodies_in_memory_upload_part(self):\n        self.assertFalse(\n            self.upload_input_manager.stores_body_in_memory('upload_part')\n        )\n\n    def test_provide_transfer_size(self):\n        self.upload_input_manager.provide_transfer_size(self.future)\n        # The provided file size should be equal to size of the contents of\n        # the file.\n        self.assertEqual(self.future.meta.size, len(self.content))\n\n    def test_requires_multipart_upload(self):\n        self.future.meta.provide_transfer_size(len(self.content))\n        # With the default multipart threshold, the length of the content\n        # should be smaller than the threshold thus not requiring a multipart\n        # transfer.\n        self.assertFalse(\n            self.upload_input_manager.requires_multipart_upload(\n                self.future, self.config\n            )\n        )\n        # Decreasing the threshold to that of the length of the content of\n        # the file should trigger the need for a multipart upload.\n        self.config.multipart_threshold = len(self.content)\n        self.assertTrue(\n            self.upload_input_manager.requires_multipart_upload(\n                self.future, self.config\n            )\n        )\n\n    def test_get_put_object_body(self):\n        self.future.meta.provide_transfer_size(len(self.content))\n        read_file_chunk = self.upload_input_manager.get_put_object_body(\n            self.future\n        )\n        read_file_chunk.enable_callback()\n        # The file-like object provided back should be the same as the content\n        # of the file.\n        with read_file_chunk:\n            self.assertEqual(read_file_chunk.read(), self.content)\n        # The file-like object should also have been wrapped with the\n        # on_queued callbacks to track the amount of bytes being transferred.\n        self.assertEqual(\n            self.recording_subscriber.calculate_bytes_seen(), len(self.content)\n        )\n\n    def test_get_put_object_body_is_interruptable(self):\n        self.future.meta.provide_transfer_size(len(self.content))\n        read_file_chunk = self.upload_input_manager.get_put_object_body(\n            self.future\n        )\n\n        # Set an exception in the transfer coordinator\n        self.transfer_coordinator.set_exception(InterruptionError)\n        # Ensure the returned read file chunk can be interrupted with that\n        # error.\n        with self.assertRaises(InterruptionError):\n            read_file_chunk.read()\n\n    def test_yield_upload_part_bodies(self):\n        # Adjust the chunk size to something more grainular for testing.\n        self.config.multipart_chunksize = 4\n        self.future.meta.provide_transfer_size(len(self.content))\n\n        # Get an iterator that will yield all of the bodies and their\n        # respective part number.\n        part_iterator = self.upload_input_manager.yield_upload_part_bodies(\n            self.future, self.config.multipart_chunksize\n        )\n        expected_part_number = 1\n        for part_number, read_file_chunk in part_iterator:\n            # Ensure that the part number is as expected\n            self.assertEqual(part_number, expected_part_number)\n            read_file_chunk.enable_callback()\n            # Ensure that the body is correct for that part.\n            with read_file_chunk:\n                self.assertEqual(\n                    read_file_chunk.read(),\n                    self._get_expected_body_for_part(part_number),\n                )\n            expected_part_number += 1\n\n        # All of the file-like object should also have been wrapped with the\n        # on_queued callbacks to track the amount of bytes being transferred.\n        self.assertEqual(\n            self.recording_subscriber.calculate_bytes_seen(), len(self.content)\n        )\n\n    def test_yield_upload_part_bodies_are_interruptable(self):\n        # Adjust the chunk size to something more grainular for testing.\n        self.config.multipart_chunksize = 4\n        self.future.meta.provide_transfer_size(len(self.content))\n\n        # Get an iterator that will yield all of the bodies and their\n        # respective part number.\n        part_iterator = self.upload_input_manager.yield_upload_part_bodies(\n            self.future, self.config.multipart_chunksize\n        )\n\n        # Set an exception in the transfer coordinator\n        self.transfer_coordinator.set_exception(InterruptionError)\n        for _, read_file_chunk in part_iterator:\n            # Ensure that each read file chunk yielded can be interrupted\n            # with that error.\n            with self.assertRaises(InterruptionError):\n                read_file_chunk.read()\n\n\nclass TestUploadSeekableInputManager(TestUploadFilenameInputManager):\n    def setUp(self):\n        super().setUp()\n        self.upload_input_manager = UploadSeekableInputManager(\n            self.osutil, self.transfer_coordinator\n        )\n        self.fileobj = open(self.filename, 'rb')\n        self.call_args = CallArgs(\n            fileobj=self.fileobj, subscribers=self.subscribers\n        )\n        self.future = self.get_transfer_future(self.call_args)\n\n    def tearDown(self):\n        self.fileobj.close()\n        super().tearDown()\n\n    def test_is_compatible_bytes_io(self):\n        self.assertTrue(self.upload_input_manager.is_compatible(BytesIO()))\n\n    def test_not_compatible_for_non_filelike_obj(self):\n        self.assertFalse(self.upload_input_manager.is_compatible(object()))\n\n    def test_stores_bodies_in_memory_upload_part(self):\n        self.assertTrue(\n            self.upload_input_manager.stores_body_in_memory('upload_part')\n        )\n\n    def test_get_put_object_body(self):\n        start_pos = 3\n        self.fileobj.seek(start_pos)\n        adjusted_size = len(self.content) - start_pos\n        self.future.meta.provide_transfer_size(adjusted_size)\n        read_file_chunk = self.upload_input_manager.get_put_object_body(\n            self.future\n        )\n\n        read_file_chunk.enable_callback()\n        # The fact that the file was seeked to start should be taken into\n        # account in length and content for the read file chunk.\n        with read_file_chunk:\n            self.assertEqual(len(read_file_chunk), adjusted_size)\n            self.assertEqual(read_file_chunk.read(), self.content[start_pos:])\n        self.assertEqual(\n            self.recording_subscriber.calculate_bytes_seen(), adjusted_size\n        )\n\n\nclass TestUploadNonSeekableInputManager(TestUploadFilenameInputManager):\n    def setUp(self):\n        super().setUp()\n        self.upload_input_manager = UploadNonSeekableInputManager(\n            self.osutil, self.transfer_coordinator\n        )\n        self.fileobj = NonSeekableReader(self.content)\n        self.call_args = CallArgs(\n            fileobj=self.fileobj, subscribers=self.subscribers\n        )\n        self.future = self.get_transfer_future(self.call_args)\n\n    def assert_multipart_parts(self):\n        \"\"\"\n        Asserts that the input manager will generate a multipart upload\n        and that each part is in order and the correct size.\n        \"\"\"\n        # Assert that a multipart upload is required.\n        self.assertTrue(\n            self.upload_input_manager.requires_multipart_upload(\n                self.future, self.config\n            )\n        )\n\n        # Get a list of all the parts that would be sent.\n        parts = list(\n            self.upload_input_manager.yield_upload_part_bodies(\n                self.future, self.config.multipart_chunksize\n            )\n        )\n\n        # Assert that the actual number of parts is what we would expect\n        # based on the configuration.\n        size = self.config.multipart_chunksize\n        num_parts = math.ceil(len(self.content) / size)\n        self.assertEqual(len(parts), num_parts)\n\n        # Run for every part but the last part.\n        for i, part in enumerate(parts[:-1]):\n            # Assert the part number is correct.\n            self.assertEqual(part[0], i + 1)\n            # Assert the part contains the right amount of data.\n            data = part[1].read()\n            self.assertEqual(len(data), size)\n\n        # Assert that the last part is the correct size.\n        expected_final_size = len(self.content) - ((num_parts - 1) * size)\n        final_part = parts[-1]\n        self.assertEqual(len(final_part[1].read()), expected_final_size)\n\n        # Assert that the last part has the correct part number.\n        self.assertEqual(final_part[0], len(parts))\n\n    def test_provide_transfer_size(self):\n        self.upload_input_manager.provide_transfer_size(self.future)\n        # There is no way to get the size without reading the entire body.\n        self.assertEqual(self.future.meta.size, None)\n\n    def test_stores_bodies_in_memory_upload_part(self):\n        self.assertTrue(\n            self.upload_input_manager.stores_body_in_memory('upload_part')\n        )\n\n    def test_stores_bodies_in_memory_put_object(self):\n        self.assertTrue(\n            self.upload_input_manager.stores_body_in_memory('put_object')\n        )\n\n    def test_initial_data_parts_threshold_lesser(self):\n        # threshold < size\n        self.config.multipart_chunksize = 4\n        self.config.multipart_threshold = 2\n        self.assert_multipart_parts()\n\n    def test_initial_data_parts_threshold_equal(self):\n        # threshold == size\n        self.config.multipart_chunksize = 4\n        self.config.multipart_threshold = 4\n        self.assert_multipart_parts()\n\n    def test_initial_data_parts_threshold_greater(self):\n        # threshold > size\n        self.config.multipart_chunksize = 4\n        self.config.multipart_threshold = 8\n        self.assert_multipart_parts()\n\n\nclass TestUploadSubmissionTask(BaseSubmissionTaskTest):\n    def setUp(self):\n        super().setUp()\n        self.tempdir = tempfile.mkdtemp()\n        self.filename = os.path.join(self.tempdir, 'myfile')\n        self.content = b'0' * (MIN_UPLOAD_CHUNKSIZE * 3)\n        self.config.multipart_chunksize = MIN_UPLOAD_CHUNKSIZE\n        self.config.multipart_threshold = MIN_UPLOAD_CHUNKSIZE * 5\n\n        with open(self.filename, 'wb') as f:\n            f.write(self.content)\n\n        self.bucket = 'mybucket'\n        self.key = 'mykey'\n        self.extra_args = {}\n        self.subscribers = []\n\n        # A list to keep track of all of the bodies sent over the wire\n        # and their order.\n        self.sent_bodies = []\n        self.client.meta.events.register(\n            'before-parameter-build.s3.*', self.collect_body\n        )\n\n        self.call_args = self.get_call_args()\n        self.transfer_future = self.get_transfer_future(self.call_args)\n        self.submission_main_kwargs = {\n            'client': self.client,\n            'config': self.config,\n            'osutil': self.osutil,\n            'request_executor': self.executor,\n            'transfer_future': self.transfer_future,\n        }\n        self.submission_task = self.get_task(\n            UploadSubmissionTask, main_kwargs=self.submission_main_kwargs\n        )\n\n    def tearDown(self):\n        super().tearDown()\n        shutil.rmtree(self.tempdir)\n\n    def collect_body(self, params, **kwargs):\n        if 'Body' in params:\n            self.sent_bodies.append(params['Body'].read())\n\n    def get_call_args(self, **kwargs):\n        default_call_args = {\n            'fileobj': self.filename,\n            'bucket': self.bucket,\n            'key': self.key,\n            'extra_args': self.extra_args,\n            'subscribers': self.subscribers,\n        }\n        default_call_args.update(kwargs)\n        return CallArgs(**default_call_args)\n\n    def add_multipart_upload_stubbed_responses(self):\n        self.stubber.add_response(\n            method='create_multipart_upload',\n            service_response={'UploadId': 'my-id'},\n        )\n        self.stubber.add_response(\n            method='upload_part', service_response={'ETag': 'etag-1'}\n        )\n        self.stubber.add_response(\n            method='upload_part', service_response={'ETag': 'etag-2'}\n        )\n        self.stubber.add_response(\n            method='upload_part', service_response={'ETag': 'etag-3'}\n        )\n        self.stubber.add_response(\n            method='complete_multipart_upload', service_response={}\n        )\n\n    def wrap_executor_in_recorder(self):\n        self.executor = RecordingExecutor(self.executor)\n        self.submission_main_kwargs['request_executor'] = self.executor\n\n    def use_fileobj_in_call_args(self, fileobj):\n        self.call_args = self.get_call_args(fileobj=fileobj)\n        self.transfer_future = self.get_transfer_future(self.call_args)\n        self.submission_main_kwargs['transfer_future'] = self.transfer_future\n\n    def assert_tag_value_for_put_object(self, tag_value):\n        self.assertEqual(self.executor.submissions[0]['tag'], tag_value)\n\n    def assert_tag_value_for_upload_parts(self, tag_value):\n        for submission in self.executor.submissions[1:-1]:\n            self.assertEqual(submission['tag'], tag_value)\n\n    def test_provide_file_size_on_put(self):\n        self.call_args.subscribers.append(FileSizeProvider(len(self.content)))\n        self.stubber.add_response(\n            method='put_object',\n            service_response={},\n            expected_params={\n                'Body': ANY,\n                'Bucket': self.bucket,\n                'Key': self.key,\n            },\n        )\n\n        # With this submitter, it will fail to stat the file if a transfer\n        # size is not provided.\n        self.submission_main_kwargs['osutil'] = OSUtilsExceptionOnFileSize()\n\n        self.submission_task = self.get_task(\n            UploadSubmissionTask, main_kwargs=self.submission_main_kwargs\n        )\n        self.submission_task()\n        self.transfer_future.result()\n        self.stubber.assert_no_pending_responses()\n        self.assertEqual(self.sent_bodies, [self.content])\n\n    def test_submits_no_tag_for_put_object_filename(self):\n        self.wrap_executor_in_recorder()\n        self.stubber.add_response('put_object', {})\n\n        self.submission_task = self.get_task(\n            UploadSubmissionTask, main_kwargs=self.submission_main_kwargs\n        )\n        self.submission_task()\n        self.transfer_future.result()\n        self.stubber.assert_no_pending_responses()\n\n        # Make sure no tag to limit that task specifically was not associated\n        # to that task submission.\n        self.assert_tag_value_for_put_object(None)\n\n    def test_submits_no_tag_for_multipart_filename(self):\n        self.wrap_executor_in_recorder()\n\n        # Set up for a multipart upload.\n        self.add_multipart_upload_stubbed_responses()\n        self.config.multipart_threshold = 1\n\n        self.submission_task = self.get_task(\n            UploadSubmissionTask, main_kwargs=self.submission_main_kwargs\n        )\n        self.submission_task()\n        self.transfer_future.result()\n        self.stubber.assert_no_pending_responses()\n\n        # Make sure no tag to limit any of the upload part tasks were\n        # were associated when submitted to the executor\n        self.assert_tag_value_for_upload_parts(None)\n\n    def test_submits_no_tag_for_put_object_fileobj(self):\n        self.wrap_executor_in_recorder()\n        self.stubber.add_response('put_object', {})\n\n        with open(self.filename, 'rb') as f:\n            self.use_fileobj_in_call_args(f)\n            self.submission_task = self.get_task(\n                UploadSubmissionTask, main_kwargs=self.submission_main_kwargs\n            )\n            self.submission_task()\n            self.transfer_future.result()\n            self.stubber.assert_no_pending_responses()\n\n        # Make sure no tag to limit that task specifically was not associated\n        # to that task submission.\n        self.assert_tag_value_for_put_object(None)\n\n    def test_submits_tag_for_multipart_fileobj(self):\n        self.wrap_executor_in_recorder()\n\n        # Set up for a multipart upload.\n        self.add_multipart_upload_stubbed_responses()\n        self.config.multipart_threshold = 1\n\n        with open(self.filename, 'rb') as f:\n            self.use_fileobj_in_call_args(f)\n            self.submission_task = self.get_task(\n                UploadSubmissionTask, main_kwargs=self.submission_main_kwargs\n            )\n            self.submission_task()\n            self.transfer_future.result()\n            self.stubber.assert_no_pending_responses()\n\n        # Make sure tags to limit all of the upload part tasks were\n        # were associated when submitted to the executor as these tasks will\n        # have chunks of data stored with them in memory.\n        self.assert_tag_value_for_upload_parts(IN_MEMORY_UPLOAD_TAG)\n\n\nclass TestPutObjectTask(BaseUploadTest):\n    def test_main(self):\n        extra_args = {'Metadata': {'foo': 'bar'}}\n        with open(self.filename, 'rb') as fileobj:\n            task = self.get_task(\n                PutObjectTask,\n                main_kwargs={\n                    'client': self.client,\n                    'fileobj': fileobj,\n                    'bucket': self.bucket,\n                    'key': self.key,\n                    'extra_args': extra_args,\n                },\n            )\n            self.stubber.add_response(\n                method='put_object',\n                service_response={},\n                expected_params={\n                    'Body': ANY,\n                    'Bucket': self.bucket,\n                    'Key': self.key,\n                    'Metadata': {'foo': 'bar'},\n                },\n            )\n            task()\n            self.stubber.assert_no_pending_responses()\n            self.assertEqual(self.sent_bodies, [self.content])\n\n\nclass TestUploadPartTask(BaseUploadTest):\n    def test_main(self):\n        extra_args = {'RequestPayer': 'requester'}\n        upload_id = 'my-id'\n        part_number = 1\n        etag = 'foo'\n        with open(self.filename, 'rb') as fileobj:\n            task = self.get_task(\n                UploadPartTask,\n                main_kwargs={\n                    'client': self.client,\n                    'fileobj': fileobj,\n                    'bucket': self.bucket,\n                    'key': self.key,\n                    'upload_id': upload_id,\n                    'part_number': part_number,\n                    'extra_args': extra_args,\n                },\n            )\n            self.stubber.add_response(\n                method='upload_part',\n                service_response={'ETag': etag},\n                expected_params={\n                    'Body': ANY,\n                    'Bucket': self.bucket,\n                    'Key': self.key,\n                    'UploadId': upload_id,\n                    'PartNumber': part_number,\n                    'RequestPayer': 'requester',\n                },\n            )\n            rval = task()\n            self.stubber.assert_no_pending_responses()\n            self.assertEqual(rval, {'ETag': etag, 'PartNumber': part_number})\n            self.assertEqual(self.sent_bodies, [self.content])\n", "tests/unit/test_crt.py": "# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport io\n\nimport pytest\nfrom botocore.credentials import Credentials, ReadOnlyCredentials\nfrom botocore.exceptions import ClientError, NoCredentialsError\nfrom botocore.session import Session\n\nfrom s3transfer.exceptions import TransferNotDoneError\nfrom s3transfer.utils import CallArgs\nfrom tests import HAS_CRT, FileCreator, mock, requires_crt, unittest\n\nif HAS_CRT:\n    import awscrt.auth\n    import awscrt.s3\n\n    import s3transfer.crt\n\n\nrequires_crt_pytest = pytest.mark.skipif(\n    not HAS_CRT, reason=\"Test requires awscrt to be installed.\"\n)\n\n\n@pytest.fixture\ndef mock_crt_process_lock(monkeypatch):\n    # The process lock is cached at the module layer whenever the\n    # cross process lock is successfully acquired. This patch ensures that\n    # test cases will start off with no previously cached process lock and\n    # if a cross process is instantiated/acquired it will be the mock that\n    # can be used for controlling lock behavior.\n    monkeypatch.setattr('s3transfer.crt.CRT_S3_PROCESS_LOCK', None)\n    with mock.patch('awscrt.s3.CrossProcessLock', spec=True) as mock_lock:\n        yield mock_lock\n\n\n@pytest.fixture\ndef mock_s3_crt_client():\n    with mock.patch('s3transfer.crt.S3Client', spec=True) as mock_client:\n        yield mock_client\n\n\n@pytest.fixture\ndef mock_get_recommended_throughput_target_gbps():\n    with mock.patch(\n        'awscrt.s3.get_recommended_throughput_target_gbps'\n    ) as mock_get_target_gbps:\n        yield mock_get_target_gbps\n\n\nclass CustomFutureException(Exception):\n    pass\n\n\n@requires_crt_pytest\nclass TestCRTProcessLock:\n    def test_acquire_crt_s3_process_lock(self, mock_crt_process_lock):\n        lock = s3transfer.crt.acquire_crt_s3_process_lock('app-name')\n        assert lock is s3transfer.crt.CRT_S3_PROCESS_LOCK\n        assert lock is mock_crt_process_lock.return_value\n        mock_crt_process_lock.assert_called_once_with('app-name')\n        mock_crt_process_lock.return_value.acquire.assert_called_once_with()\n\n    def test_unable_to_acquire_lock_returns_none(self, mock_crt_process_lock):\n        mock_crt_process_lock.return_value.acquire.side_effect = RuntimeError\n        assert s3transfer.crt.acquire_crt_s3_process_lock('app-name') is None\n        assert s3transfer.crt.CRT_S3_PROCESS_LOCK is None\n        mock_crt_process_lock.assert_called_once_with('app-name')\n        mock_crt_process_lock.return_value.acquire.assert_called_once_with()\n\n    def test_multiple_acquires_return_same_lock(self, mock_crt_process_lock):\n        lock = s3transfer.crt.acquire_crt_s3_process_lock('app-name')\n        assert s3transfer.crt.acquire_crt_s3_process_lock('app-name') is lock\n        assert lock is s3transfer.crt.CRT_S3_PROCESS_LOCK\n\n        # The process lock should have only been instantiated and acquired once\n        mock_crt_process_lock.assert_called_once_with('app-name')\n        mock_crt_process_lock.return_value.acquire.assert_called_once_with()\n\n\n@requires_crt\nclass TestBotocoreCRTRequestSerializer(unittest.TestCase):\n    def setUp(self):\n        self.region = 'us-west-2'\n        self.session = Session()\n        self.session.set_config_variable('region', self.region)\n        self.request_serializer = s3transfer.crt.BotocoreCRTRequestSerializer(\n            self.session\n        )\n        self.bucket = \"test_bucket\"\n        self.key = \"test_key\"\n        self.files = FileCreator()\n        self.filename = self.files.create_file('myfile', 'my content')\n        self.expected_path = \"/\" + self.bucket + \"/\" + self.key\n        self.expected_host = \"s3.%s.amazonaws.com\" % (self.region)\n\n    def tearDown(self):\n        self.files.remove_all()\n\n    def test_upload_request(self):\n        callargs = CallArgs(\n            bucket=self.bucket,\n            key=self.key,\n            fileobj=self.filename,\n            extra_args={},\n            subscribers=[],\n        )\n        coordinator = s3transfer.crt.CRTTransferCoordinator()\n        future = s3transfer.crt.CRTTransferFuture(\n            s3transfer.crt.CRTTransferMeta(call_args=callargs), coordinator\n        )\n        crt_request = self.request_serializer.serialize_http_request(\n            \"put_object\", future\n        )\n        self.assertEqual(\"PUT\", crt_request.method)\n        self.assertEqual(self.expected_path, crt_request.path)\n        self.assertEqual(self.expected_host, crt_request.headers.get(\"host\"))\n        self.assertIsNone(crt_request.headers.get(\"Authorization\"))\n\n    def test_download_request(self):\n        callargs = CallArgs(\n            bucket=self.bucket,\n            key=self.key,\n            fileobj=self.filename,\n            extra_args={},\n            subscribers=[],\n        )\n        coordinator = s3transfer.crt.CRTTransferCoordinator()\n        future = s3transfer.crt.CRTTransferFuture(\n            s3transfer.crt.CRTTransferMeta(call_args=callargs), coordinator\n        )\n        crt_request = self.request_serializer.serialize_http_request(\n            \"get_object\", future\n        )\n        self.assertEqual(\"GET\", crt_request.method)\n        self.assertEqual(self.expected_path, crt_request.path)\n        self.assertEqual(self.expected_host, crt_request.headers.get(\"host\"))\n        self.assertIsNone(crt_request.headers.get(\"Authorization\"))\n\n    def test_delete_request(self):\n        callargs = CallArgs(\n            bucket=self.bucket, key=self.key, extra_args={}, subscribers=[]\n        )\n        coordinator = s3transfer.crt.CRTTransferCoordinator()\n        future = s3transfer.crt.CRTTransferFuture(\n            s3transfer.crt.CRTTransferMeta(call_args=callargs), coordinator\n        )\n        crt_request = self.request_serializer.serialize_http_request(\n            \"delete_object\", future\n        )\n        self.assertEqual(\"DELETE\", crt_request.method)\n        self.assertEqual(self.expected_path, crt_request.path)\n        self.assertEqual(self.expected_host, crt_request.headers.get(\"host\"))\n        self.assertIsNone(crt_request.headers.get(\"Authorization\"))\n\n    def _create_crt_response_error(\n        self, status_code, body, operation_name=None\n    ):\n        return awscrt.s3.S3ResponseError(\n            code=14343,\n            name='AWS_ERROR_S3_INVALID_RESPONSE_STATUS',\n            message='Invalid response status from request',\n            status_code=status_code,\n            headers=[\n                ('x-amz-request-id', 'QSJHJJZR2EDYD4GQ'),\n                (\n                    'x-amz-id-2',\n                    'xDbgdKdvYZTjgpOTzm7yNP2JPrOQl+eaQvUkFdOjdJoWkIC643fgHxdsHpUKvVAfjKf5F6otEYA=',\n                ),\n                ('Content-Type', 'application/xml'),\n                ('Transfer-Encoding', 'chunked'),\n                ('Date', 'Fri, 10 Nov 2023 23:22:47 GMT'),\n                ('Server', 'AmazonS3'),\n            ],\n            body=body,\n            operation_name=operation_name,\n        )\n\n    def test_translate_get_object_404(self):\n        body = (\n            b'<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<Error>'\n            b'<Code>NoSuchKey</Code>'\n            b'<Message>The specified key does not exist.</Message>'\n            b'<Key>obviously-no-such-key.txt</Key>'\n            b'<RequestId>SBJ7ZQY03N1WDW9T</RequestId>'\n            b'<HostId>SomeHostId</HostId></Error>'\n        )\n        crt_exc = self._create_crt_response_error(404, body, 'GetObject')\n        boto_err = self.request_serializer.translate_crt_exception(crt_exc)\n        self.assertIsInstance(\n            boto_err, self.session.create_client('s3').exceptions.NoSuchKey\n        )\n\n    def test_translate_head_object_404(self):\n        # There's no body in a HEAD response, so we can't map it to a modeled S3 exception.\n        # But it should still map to a botocore ClientError\n        body = None\n        crt_exc = self._create_crt_response_error(\n            404, body, operation_name='HeadObject'\n        )\n        boto_err = self.request_serializer.translate_crt_exception(crt_exc)\n        self.assertIsInstance(boto_err, ClientError)\n\n    def test_translate_unknown_operation_404(self):\n        body = None\n        crt_exc = self._create_crt_response_error(404, body)\n        boto_err = self.request_serializer.translate_crt_exception(crt_exc)\n        self.assertIsInstance(boto_err, ClientError)\n\n\n@requires_crt_pytest\nclass TestBotocoreCRTCredentialsWrapper:\n    @pytest.fixture\n    def botocore_credentials(self):\n        return Credentials(\n            access_key='access_key', secret_key='secret_key', token='token'\n        )\n\n    def assert_crt_credentials(\n        self,\n        crt_credentials,\n        expected_access_key='access_key',\n        expected_secret_key='secret_key',\n        expected_token='token',\n    ):\n        assert crt_credentials.access_key_id == expected_access_key\n        assert crt_credentials.secret_access_key == expected_secret_key\n        assert crt_credentials.session_token == expected_token\n\n    def test_fetch_crt_credentials_successfully(self, botocore_credentials):\n        wrapper = s3transfer.crt.BotocoreCRTCredentialsWrapper(\n            botocore_credentials\n        )\n        crt_credentials = wrapper()\n        self.assert_crt_credentials(crt_credentials)\n\n    def test_wrapper_does_not_cache_frozen_credentials(self):\n        mock_credentials = mock.Mock(Credentials)\n        mock_credentials.get_frozen_credentials.side_effect = [\n            ReadOnlyCredentials('access_key_1', 'secret_key_1', 'token_1'),\n            ReadOnlyCredentials('access_key_2', 'secret_key_2', 'token_2'),\n        ]\n        wrapper = s3transfer.crt.BotocoreCRTCredentialsWrapper(\n            mock_credentials\n        )\n\n        crt_credentials_1 = wrapper()\n        self.assert_crt_credentials(\n            crt_credentials_1,\n            expected_access_key='access_key_1',\n            expected_secret_key='secret_key_1',\n            expected_token='token_1',\n        )\n\n        crt_credentials_2 = wrapper()\n        self.assert_crt_credentials(\n            crt_credentials_2,\n            expected_access_key='access_key_2',\n            expected_secret_key='secret_key_2',\n            expected_token='token_2',\n        )\n\n        assert mock_credentials.get_frozen_credentials.call_count == 2\n\n    def test_raises_error_when_resolved_credentials_is_none(self):\n        wrapper = s3transfer.crt.BotocoreCRTCredentialsWrapper(None)\n        with pytest.raises(NoCredentialsError):\n            wrapper()\n\n    def test_to_crt_credentials_provider(self, botocore_credentials):\n        wrapper = s3transfer.crt.BotocoreCRTCredentialsWrapper(\n            botocore_credentials\n        )\n        crt_credentials_provider = wrapper.to_crt_credentials_provider()\n        assert isinstance(\n            crt_credentials_provider, awscrt.auth.AwsCredentialsProvider\n        )\n        get_credentials_future = crt_credentials_provider.get_credentials()\n        crt_credentials = get_credentials_future.result()\n        self.assert_crt_credentials(crt_credentials)\n\n\n@requires_crt\nclass TestCRTTransferFuture(unittest.TestCase):\n    def setUp(self):\n        self.mock_s3_request = mock.Mock(awscrt.s3.S3RequestType)\n        self.mock_crt_future = mock.Mock(awscrt.s3.Future)\n        self.mock_s3_request.finished_future = self.mock_crt_future\n        self.coordinator = s3transfer.crt.CRTTransferCoordinator()\n        self.coordinator.set_s3_request(self.mock_s3_request)\n        self.future = s3transfer.crt.CRTTransferFuture(\n            coordinator=self.coordinator\n        )\n\n    def test_set_exception(self):\n        self.future.set_exception(CustomFutureException())\n        with self.assertRaises(CustomFutureException):\n            self.future.result()\n\n    def test_set_exception_raises_error_when_not_done(self):\n        self.mock_crt_future.done.return_value = False\n        with self.assertRaises(TransferNotDoneError):\n            self.future.set_exception(CustomFutureException())\n\n    def test_set_exception_can_override_previous_exception(self):\n        self.future.set_exception(Exception())\n        self.future.set_exception(CustomFutureException())\n        with self.assertRaises(CustomFutureException):\n            self.future.result()\n\n\n@requires_crt\nclass TestOnBodyFileObjWriter(unittest.TestCase):\n    def test_call(self):\n        fileobj = io.BytesIO()\n        writer = s3transfer.crt.OnBodyFileObjWriter(fileobj)\n        writer(chunk=b'content')\n        self.assertEqual(fileobj.getvalue(), b'content')\n\n\n@requires_crt_pytest\nclass TestCreateS3CRTClient:\n    @pytest.mark.parametrize(\n        'provided_bytes_per_sec,recommended_gbps,expected_gbps',\n        [\n            (None, 100.0, 100.0),\n            (None, None, 10.0),\n            # NOTE: create_s3_crt_client() accepts target throughput as bytes\n            # per second and it is converted to gigabits per second for the\n            # CRT client instantiation.\n            (1_000_000_000, None, 8.0),\n            (1_000_000_000, 100.0, 8.0),\n        ],\n    )\n    def test_target_throughput(\n        self,\n        provided_bytes_per_sec,\n        recommended_gbps,\n        expected_gbps,\n        mock_s3_crt_client,\n        mock_get_recommended_throughput_target_gbps,\n    ):\n        mock_get_recommended_throughput_target_gbps.return_value = (\n            recommended_gbps\n        )\n        s3transfer.crt.create_s3_crt_client(\n            'us-west-2',\n            target_throughput=provided_bytes_per_sec,\n        )\n        assert (\n            mock_s3_crt_client.call_args[1]['throughput_target_gbps']\n            == expected_gbps\n        )\n\n    def test_always_enables_s3express(self, mock_s3_crt_client):\n        s3transfer.crt.create_s3_crt_client('us-west-2')\n        assert mock_s3_crt_client.call_args[1]['enable_s3express'] is True\n", "tests/unit/test_manager.py": "# Copyright 2016 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport time\nfrom concurrent.futures import ThreadPoolExecutor\n\nfrom s3transfer.exceptions import CancelledError, FatalError\nfrom s3transfer.futures import TransferCoordinator\nfrom s3transfer.manager import TransferConfig, TransferCoordinatorController\nfrom tests import TransferCoordinatorWithInterrupt, unittest\n\n\nclass FutureResultException(Exception):\n    pass\n\n\nclass TestTransferConfig(unittest.TestCase):\n    def test_exception_on_zero_attr_value(self):\n        with self.assertRaises(ValueError):\n            TransferConfig(max_request_queue_size=0)\n\n\nclass TestTransferCoordinatorController(unittest.TestCase):\n    def setUp(self):\n        self.coordinator_controller = TransferCoordinatorController()\n\n    def sleep_then_announce_done(self, transfer_coordinator, sleep_time):\n        time.sleep(sleep_time)\n        transfer_coordinator.set_result('done')\n        transfer_coordinator.announce_done()\n\n    def assert_coordinator_is_cancelled(self, transfer_coordinator):\n        self.assertEqual(transfer_coordinator.status, 'cancelled')\n\n    def test_add_transfer_coordinator(self):\n        transfer_coordinator = TransferCoordinator()\n        # Add the transfer coordinator\n        self.coordinator_controller.add_transfer_coordinator(\n            transfer_coordinator\n        )\n        # Ensure that is tracked.\n        self.assertEqual(\n            self.coordinator_controller.tracked_transfer_coordinators,\n            {transfer_coordinator},\n        )\n\n    def test_remove_transfer_coordinator(self):\n        transfer_coordinator = TransferCoordinator()\n        # Add the coordinator\n        self.coordinator_controller.add_transfer_coordinator(\n            transfer_coordinator\n        )\n        # Now remove the coordinator\n        self.coordinator_controller.remove_transfer_coordinator(\n            transfer_coordinator\n        )\n        # Make sure that it is no longer getting tracked.\n        self.assertEqual(\n            self.coordinator_controller.tracked_transfer_coordinators, set()\n        )\n\n    def test_cancel(self):\n        transfer_coordinator = TransferCoordinator()\n        # Add the transfer coordinator\n        self.coordinator_controller.add_transfer_coordinator(\n            transfer_coordinator\n        )\n        # Cancel with the canceler\n        self.coordinator_controller.cancel()\n        # Check that coordinator got canceled\n        self.assert_coordinator_is_cancelled(transfer_coordinator)\n\n    def test_cancel_with_message(self):\n        message = 'my cancel message'\n        transfer_coordinator = TransferCoordinator()\n        self.coordinator_controller.add_transfer_coordinator(\n            transfer_coordinator\n        )\n        self.coordinator_controller.cancel(message)\n        transfer_coordinator.announce_done()\n        with self.assertRaisesRegex(CancelledError, message):\n            transfer_coordinator.result()\n\n    def test_cancel_with_provided_exception(self):\n        message = 'my cancel message'\n        transfer_coordinator = TransferCoordinator()\n        self.coordinator_controller.add_transfer_coordinator(\n            transfer_coordinator\n        )\n        self.coordinator_controller.cancel(message, exc_type=FatalError)\n        transfer_coordinator.announce_done()\n        with self.assertRaisesRegex(FatalError, message):\n            transfer_coordinator.result()\n\n    def test_wait_for_done_transfer_coordinators(self):\n        # Create a coordinator and add it to the canceler\n        transfer_coordinator = TransferCoordinator()\n        self.coordinator_controller.add_transfer_coordinator(\n            transfer_coordinator\n        )\n\n        sleep_time = 0.02\n        with ThreadPoolExecutor(max_workers=1) as executor:\n            # In a separate thread sleep and then set the transfer coordinator\n            # to done after sleeping.\n            start_time = time.time()\n            executor.submit(\n                self.sleep_then_announce_done, transfer_coordinator, sleep_time\n            )\n            # Now call wait to wait for the transfer coordinator to be done.\n            self.coordinator_controller.wait()\n            end_time = time.time()\n            wait_time = end_time - start_time\n        # The time waited should not be less than the time it took to sleep in\n        # the separate thread because the wait ending should be dependent on\n        # the sleeping thread announcing that the transfer coordinator is done.\n        self.assertTrue(sleep_time <= wait_time)\n\n    def test_wait_does_not_propogate_exceptions_from_result(self):\n        transfer_coordinator = TransferCoordinator()\n        transfer_coordinator.set_exception(FutureResultException())\n        transfer_coordinator.announce_done()\n        try:\n            self.coordinator_controller.wait()\n        except FutureResultException as e:\n            self.fail('%s should not have been raised.' % e)\n\n    def test_wait_can_be_interrupted(self):\n        inject_interrupt_coordinator = TransferCoordinatorWithInterrupt()\n        self.coordinator_controller.add_transfer_coordinator(\n            inject_interrupt_coordinator\n        )\n        with self.assertRaises(KeyboardInterrupt):\n            self.coordinator_controller.wait()\n", "tests/unit/test_utils.py": "# Copyright 2016 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport io\nimport os.path\nimport random\nimport re\nimport shutil\nimport tempfile\nimport threading\nimport time\nfrom io import BytesIO, StringIO\n\nimport pytest\n\nfrom s3transfer.futures import TransferFuture, TransferMeta\nfrom s3transfer.utils import (\n    MAX_PARTS,\n    MAX_SINGLE_UPLOAD_SIZE,\n    MIN_UPLOAD_CHUNKSIZE,\n    CallArgs,\n    ChunksizeAdjuster,\n    CountCallbackInvoker,\n    DeferredOpenFile,\n    FunctionContainer,\n    NoResourcesAvailable,\n    OSUtils,\n    ReadFileChunk,\n    SlidingWindowSemaphore,\n    StreamReaderProgress,\n    TaskSemaphore,\n    add_s3express_defaults,\n    calculate_num_parts,\n    calculate_range_parameter,\n    get_callbacks,\n    get_filtered_dict,\n    invoke_progress_callbacks,\n    random_file_extension,\n)\nfrom tests import NonSeekableWriter, RecordingSubscriber, mock, unittest\n\n\nclass TestGetCallbacks(unittest.TestCase):\n    def setUp(self):\n        self.subscriber = RecordingSubscriber()\n        self.second_subscriber = RecordingSubscriber()\n        self.call_args = CallArgs(\n            subscribers=[self.subscriber, self.second_subscriber]\n        )\n        self.transfer_meta = TransferMeta(self.call_args)\n        self.transfer_future = TransferFuture(self.transfer_meta)\n\n    def test_get_callbacks(self):\n        callbacks = get_callbacks(self.transfer_future, 'queued')\n        # Make sure two callbacks were added as both subscribers had\n        # an on_queued method.\n        self.assertEqual(len(callbacks), 2)\n\n        # Ensure that the callback was injected with the future by calling\n        # one of them and checking that the future was used in the call.\n        callbacks[0]()\n        self.assertEqual(\n            self.subscriber.on_queued_calls, [{'future': self.transfer_future}]\n        )\n\n    def test_get_callbacks_for_missing_type(self):\n        callbacks = get_callbacks(self.transfer_future, 'fake_state')\n        # There should be no callbacks as the subscribers will not have the\n        # on_fake_state method\n        self.assertEqual(len(callbacks), 0)\n\n\nclass TestGetFilteredDict(unittest.TestCase):\n    def test_get_filtered_dict(self):\n        original = {'Include': 'IncludeValue', 'NotInlude': 'NotIncludeValue'}\n        whitelist = ['Include']\n        self.assertEqual(\n            get_filtered_dict(original, whitelist), {'Include': 'IncludeValue'}\n        )\n\n\nclass TestCallArgs(unittest.TestCase):\n    def test_call_args(self):\n        call_args = CallArgs(foo='bar', biz='baz')\n        self.assertEqual(call_args.foo, 'bar')\n        self.assertEqual(call_args.biz, 'baz')\n\n\nclass TestFunctionContainer(unittest.TestCase):\n    def get_args_kwargs(self, *args, **kwargs):\n        return args, kwargs\n\n    def test_call(self):\n        func_container = FunctionContainer(\n            self.get_args_kwargs, 'foo', bar='baz'\n        )\n        self.assertEqual(func_container(), (('foo',), {'bar': 'baz'}))\n\n    def test_repr(self):\n        func_container = FunctionContainer(\n            self.get_args_kwargs, 'foo', bar='baz'\n        )\n        self.assertEqual(\n            str(func_container),\n            'Function: {} with args {} and kwargs {}'.format(\n                self.get_args_kwargs, ('foo',), {'bar': 'baz'}\n            ),\n        )\n\n\nclass TestCountCallbackInvoker(unittest.TestCase):\n    def invoke_callback(self):\n        self.ref_results.append('callback invoked')\n\n    def assert_callback_invoked(self):\n        self.assertEqual(self.ref_results, ['callback invoked'])\n\n    def assert_callback_not_invoked(self):\n        self.assertEqual(self.ref_results, [])\n\n    def setUp(self):\n        self.ref_results = []\n        self.invoker = CountCallbackInvoker(self.invoke_callback)\n\n    def test_increment(self):\n        self.invoker.increment()\n        self.assertEqual(self.invoker.current_count, 1)\n\n    def test_decrement(self):\n        self.invoker.increment()\n        self.invoker.increment()\n        self.invoker.decrement()\n        self.assertEqual(self.invoker.current_count, 1)\n\n    def test_count_cannot_go_below_zero(self):\n        with self.assertRaises(RuntimeError):\n            self.invoker.decrement()\n\n    def test_callback_invoked_only_once_finalized(self):\n        self.invoker.increment()\n        self.invoker.decrement()\n        self.assert_callback_not_invoked()\n        self.invoker.finalize()\n        # Callback should only be invoked once finalized\n        self.assert_callback_invoked()\n\n    def test_callback_invoked_after_finalizing_and_count_reaching_zero(self):\n        self.invoker.increment()\n        self.invoker.finalize()\n        # Make sure that it does not get invoked immediately after\n        # finalizing as the count is currently one\n        self.assert_callback_not_invoked()\n        self.invoker.decrement()\n        self.assert_callback_invoked()\n\n    def test_cannot_increment_after_finalization(self):\n        self.invoker.finalize()\n        with self.assertRaises(RuntimeError):\n            self.invoker.increment()\n\n\nclass TestRandomFileExtension(unittest.TestCase):\n    def test_has_proper_length(self):\n        self.assertEqual(len(random_file_extension(num_digits=4)), 4)\n\n\nclass TestInvokeProgressCallbacks(unittest.TestCase):\n    def test_invoke_progress_callbacks(self):\n        recording_subscriber = RecordingSubscriber()\n        invoke_progress_callbacks([recording_subscriber.on_progress], 2)\n        self.assertEqual(recording_subscriber.calculate_bytes_seen(), 2)\n\n    def test_invoke_progress_callbacks_with_no_progress(self):\n        recording_subscriber = RecordingSubscriber()\n        invoke_progress_callbacks([recording_subscriber.on_progress], 0)\n        self.assertEqual(len(recording_subscriber.on_progress_calls), 0)\n\n\nclass TestCalculateNumParts(unittest.TestCase):\n    def test_calculate_num_parts_divisible(self):\n        self.assertEqual(calculate_num_parts(size=4, part_size=2), 2)\n\n    def test_calculate_num_parts_not_divisible(self):\n        self.assertEqual(calculate_num_parts(size=3, part_size=2), 2)\n\n\nclass TestCalculateRangeParameter(unittest.TestCase):\n    def setUp(self):\n        self.part_size = 5\n        self.part_index = 1\n        self.num_parts = 3\n\n    def test_calculate_range_paramter(self):\n        range_val = calculate_range_parameter(\n            self.part_size, self.part_index, self.num_parts\n        )\n        self.assertEqual(range_val, 'bytes=5-9')\n\n    def test_last_part_with_no_total_size(self):\n        range_val = calculate_range_parameter(\n            self.part_size, self.part_index, num_parts=2\n        )\n        self.assertEqual(range_val, 'bytes=5-')\n\n    def test_last_part_with_total_size(self):\n        range_val = calculate_range_parameter(\n            self.part_size, self.part_index, num_parts=2, total_size=8\n        )\n        self.assertEqual(range_val, 'bytes=5-7')\n\n\nclass BaseUtilsTest(unittest.TestCase):\n    def setUp(self):\n        self.tempdir = tempfile.mkdtemp()\n        self.filename = os.path.join(self.tempdir, 'foo')\n        self.content = b'abc'\n        with open(self.filename, 'wb') as f:\n            f.write(self.content)\n        self.amounts_seen = []\n        self.num_close_callback_calls = 0\n\n    def tearDown(self):\n        shutil.rmtree(self.tempdir)\n\n    def callback(self, bytes_transferred):\n        self.amounts_seen.append(bytes_transferred)\n\n    def close_callback(self):\n        self.num_close_callback_calls += 1\n\n\nclass TestOSUtils(BaseUtilsTest):\n    def test_get_file_size(self):\n        self.assertEqual(\n            OSUtils().get_file_size(self.filename), len(self.content)\n        )\n\n    def test_open_file_chunk_reader(self):\n        reader = OSUtils().open_file_chunk_reader(\n            self.filename, 0, 3, [self.callback]\n        )\n\n        # The returned reader should be a ReadFileChunk.\n        self.assertIsInstance(reader, ReadFileChunk)\n        # The content of the reader should be correct.\n        self.assertEqual(reader.read(), self.content)\n        # Callbacks should be disabled depspite being passed in.\n        self.assertEqual(self.amounts_seen, [])\n\n    def test_open_file_chunk_reader_from_fileobj(self):\n        with open(self.filename, 'rb') as f:\n            reader = OSUtils().open_file_chunk_reader_from_fileobj(\n                f, len(self.content), len(self.content), [self.callback]\n            )\n\n            # The returned reader should be a ReadFileChunk.\n            self.assertIsInstance(reader, ReadFileChunk)\n            # The content of the reader should be correct.\n            self.assertEqual(reader.read(), self.content)\n            reader.close()\n            # Callbacks should be disabled depspite being passed in.\n            self.assertEqual(self.amounts_seen, [])\n            self.assertEqual(self.num_close_callback_calls, 0)\n\n    def test_open_file(self):\n        fileobj = OSUtils().open(os.path.join(self.tempdir, 'foo'), 'w')\n        self.assertTrue(hasattr(fileobj, 'write'))\n\n    def test_remove_file_ignores_errors(self):\n        non_existent_file = os.path.join(self.tempdir, 'no-exist')\n        # This should not exist to start.\n        self.assertFalse(os.path.exists(non_existent_file))\n        try:\n            OSUtils().remove_file(non_existent_file)\n        except OSError as e:\n            self.fail('OSError should have been caught: %s' % e)\n\n    def test_remove_file_proxies_remove_file(self):\n        OSUtils().remove_file(self.filename)\n        self.assertFalse(os.path.exists(self.filename))\n\n    def test_rename_file(self):\n        new_filename = os.path.join(self.tempdir, 'newfoo')\n        OSUtils().rename_file(self.filename, new_filename)\n        self.assertFalse(os.path.exists(self.filename))\n        self.assertTrue(os.path.exists(new_filename))\n\n    def test_is_special_file_for_normal_file(self):\n        self.assertFalse(OSUtils().is_special_file(self.filename))\n\n    def test_is_special_file_for_non_existant_file(self):\n        non_existant_filename = os.path.join(self.tempdir, 'no-exist')\n        self.assertFalse(os.path.exists(non_existant_filename))\n        self.assertFalse(OSUtils().is_special_file(non_existant_filename))\n\n    def test_get_temp_filename(self):\n        filename = 'myfile'\n        self.assertIsNotNone(\n            re.match(\n                r'%s\\.[0-9A-Fa-f]{8}$' % filename,\n                OSUtils().get_temp_filename(filename),\n            )\n        )\n\n    def test_get_temp_filename_len_255(self):\n        filename = 'a' * 255\n        temp_filename = OSUtils().get_temp_filename(filename)\n        self.assertLessEqual(len(temp_filename), 255)\n\n    def test_get_temp_filename_len_gt_255(self):\n        filename = 'a' * 280\n        temp_filename = OSUtils().get_temp_filename(filename)\n        self.assertLessEqual(len(temp_filename), 255)\n\n    def test_allocate(self):\n        truncate_size = 1\n        OSUtils().allocate(self.filename, truncate_size)\n        with open(self.filename, 'rb') as f:\n            self.assertEqual(len(f.read()), truncate_size)\n\n    @mock.patch('s3transfer.utils.fallocate')\n    def test_allocate_with_io_error(self, mock_fallocate):\n        mock_fallocate.side_effect = IOError()\n        with self.assertRaises(IOError):\n            OSUtils().allocate(self.filename, 1)\n        self.assertFalse(os.path.exists(self.filename))\n\n    @mock.patch('s3transfer.utils.fallocate')\n    def test_allocate_with_os_error(self, mock_fallocate):\n        mock_fallocate.side_effect = OSError()\n        with self.assertRaises(OSError):\n            OSUtils().allocate(self.filename, 1)\n        self.assertFalse(os.path.exists(self.filename))\n\n\nclass TestDeferredOpenFile(BaseUtilsTest):\n    def setUp(self):\n        super().setUp()\n        self.filename = os.path.join(self.tempdir, 'foo')\n        self.contents = b'my contents'\n        with open(self.filename, 'wb') as f:\n            f.write(self.contents)\n        self.deferred_open_file = DeferredOpenFile(\n            self.filename, open_function=self.recording_open_function\n        )\n        self.open_call_args = []\n\n    def tearDown(self):\n        self.deferred_open_file.close()\n        super().tearDown()\n\n    def recording_open_function(self, filename, mode):\n        self.open_call_args.append((filename, mode))\n        return open(filename, mode)\n\n    def open_nonseekable(self, filename, mode):\n        self.open_call_args.append((filename, mode))\n        return NonSeekableWriter(BytesIO(self.content))\n\n    def test_instantiation_does_not_open_file(self):\n        DeferredOpenFile(\n            self.filename, open_function=self.recording_open_function\n        )\n        self.assertEqual(len(self.open_call_args), 0)\n\n    def test_name(self):\n        self.assertEqual(self.deferred_open_file.name, self.filename)\n\n    def test_read(self):\n        content = self.deferred_open_file.read(2)\n        self.assertEqual(content, self.contents[0:2])\n        content = self.deferred_open_file.read(2)\n        self.assertEqual(content, self.contents[2:4])\n        self.assertEqual(len(self.open_call_args), 1)\n\n    def test_write(self):\n        self.deferred_open_file = DeferredOpenFile(\n            self.filename,\n            mode='wb',\n            open_function=self.recording_open_function,\n        )\n\n        write_content = b'foo'\n        self.deferred_open_file.write(write_content)\n        self.deferred_open_file.write(write_content)\n        self.deferred_open_file.close()\n        # Both of the writes should now be in the file.\n        with open(self.filename, 'rb') as f:\n            self.assertEqual(f.read(), write_content * 2)\n        # Open should have only been called once.\n        self.assertEqual(len(self.open_call_args), 1)\n\n    def test_seek(self):\n        self.deferred_open_file.seek(2)\n        content = self.deferred_open_file.read(2)\n        self.assertEqual(content, self.contents[2:4])\n        self.assertEqual(len(self.open_call_args), 1)\n\n    def test_open_does_not_seek_with_zero_start_byte(self):\n        self.deferred_open_file = DeferredOpenFile(\n            self.filename,\n            mode='wb',\n            start_byte=0,\n            open_function=self.open_nonseekable,\n        )\n\n        try:\n            # If this seeks, an UnsupportedOperation error will be raised.\n            self.deferred_open_file.write(b'data')\n        except io.UnsupportedOperation:\n            self.fail('DeferredOpenFile seeked upon opening')\n\n    def test_open_seeks_with_nonzero_start_byte(self):\n        self.deferred_open_file = DeferredOpenFile(\n            self.filename,\n            mode='wb',\n            start_byte=5,\n            open_function=self.open_nonseekable,\n        )\n\n        # Since a non-seekable file is being opened, calling Seek will raise\n        # an UnsupportedOperation error.\n        with self.assertRaises(io.UnsupportedOperation):\n            self.deferred_open_file.write(b'data')\n\n    def test_tell(self):\n        self.deferred_open_file.tell()\n        # tell() should not have opened the file if it has not been seeked\n        # or read because we know the start bytes upfront.\n        self.assertEqual(len(self.open_call_args), 0)\n\n        self.deferred_open_file.seek(2)\n        self.assertEqual(self.deferred_open_file.tell(), 2)\n        self.assertEqual(len(self.open_call_args), 1)\n\n    def test_open_args(self):\n        self.deferred_open_file = DeferredOpenFile(\n            self.filename,\n            mode='ab+',\n            open_function=self.recording_open_function,\n        )\n        # Force an open\n        self.deferred_open_file.write(b'data')\n        self.assertEqual(len(self.open_call_args), 1)\n        self.assertEqual(self.open_call_args[0], (self.filename, 'ab+'))\n\n    def test_context_handler(self):\n        with self.deferred_open_file:\n            self.assertEqual(len(self.open_call_args), 1)\n\n\nclass TestReadFileChunk(BaseUtilsTest):\n    def test_read_entire_chunk(self):\n        filename = os.path.join(self.tempdir, 'foo')\n        with open(filename, 'wb') as f:\n            f.write(b'onetwothreefourfivesixseveneightnineten')\n        chunk = ReadFileChunk.from_filename(\n            filename, start_byte=0, chunk_size=3\n        )\n        self.assertEqual(chunk.read(), b'one')\n        self.assertEqual(chunk.read(), b'')\n\n    def test_read_with_amount_size(self):\n        filename = os.path.join(self.tempdir, 'foo')\n        with open(filename, 'wb') as f:\n            f.write(b'onetwothreefourfivesixseveneightnineten')\n        chunk = ReadFileChunk.from_filename(\n            filename, start_byte=11, chunk_size=4\n        )\n        self.assertEqual(chunk.read(1), b'f')\n        self.assertEqual(chunk.read(1), b'o')\n        self.assertEqual(chunk.read(1), b'u')\n        self.assertEqual(chunk.read(1), b'r')\n        self.assertEqual(chunk.read(1), b'')\n\n    def test_reset_stream_emulation(self):\n        filename = os.path.join(self.tempdir, 'foo')\n        with open(filename, 'wb') as f:\n            f.write(b'onetwothreefourfivesixseveneightnineten')\n        chunk = ReadFileChunk.from_filename(\n            filename, start_byte=11, chunk_size=4\n        )\n        self.assertEqual(chunk.read(), b'four')\n        chunk.seek(0)\n        self.assertEqual(chunk.read(), b'four')\n\n    def test_read_past_end_of_file(self):\n        filename = os.path.join(self.tempdir, 'foo')\n        with open(filename, 'wb') as f:\n            f.write(b'onetwothreefourfivesixseveneightnineten')\n        chunk = ReadFileChunk.from_filename(\n            filename, start_byte=36, chunk_size=100000\n        )\n        self.assertEqual(chunk.read(), b'ten')\n        self.assertEqual(chunk.read(), b'')\n        self.assertEqual(len(chunk), 3)\n\n    def test_tell_and_seek(self):\n        filename = os.path.join(self.tempdir, 'foo')\n        with open(filename, 'wb') as f:\n            f.write(b'onetwothreefourfivesixseveneightnineten')\n        chunk = ReadFileChunk.from_filename(\n            filename, start_byte=36, chunk_size=100000\n        )\n        self.assertEqual(chunk.tell(), 0)\n        self.assertEqual(chunk.read(), b'ten')\n        self.assertEqual(chunk.tell(), 3)\n        chunk.seek(0)\n        self.assertEqual(chunk.tell(), 0)\n        chunk.seek(1, whence=1)\n        self.assertEqual(chunk.tell(), 1)\n        chunk.seek(-1, whence=1)\n        self.assertEqual(chunk.tell(), 0)\n        chunk.seek(-1, whence=2)\n        self.assertEqual(chunk.tell(), 2)\n\n    def test_tell_and_seek_boundaries(self):\n        # Test to ensure ReadFileChunk behaves the same as the\n        # Python standard library around seeking and reading out\n        # of bounds in a file object.\n        data = b'abcdefghij12345678klmnopqrst'\n        start_pos = 10\n        chunk_size = 8\n\n        # Create test file\n        filename = os.path.join(self.tempdir, 'foo')\n        with open(filename, 'wb') as f:\n            f.write(data)\n\n        # ReadFileChunk should be a substring of only numbers\n        file_objects = [\n            ReadFileChunk.from_filename(\n                filename, start_byte=start_pos, chunk_size=chunk_size\n            )\n        ]\n\n        # Uncomment next line to validate we match Python's io.BytesIO\n        # file_objects.append(io.BytesIO(data[start_pos:start_pos+chunk_size]))\n\n        for obj in file_objects:\n            self._assert_whence_start_behavior(obj)\n            self._assert_whence_end_behavior(obj)\n            self._assert_whence_relative_behavior(obj)\n            self._assert_boundary_behavior(obj)\n\n    def _assert_whence_start_behavior(self, file_obj):\n        self.assertEqual(file_obj.tell(), 0)\n\n        file_obj.seek(1, 0)\n        self.assertEqual(file_obj.tell(), 1)\n\n        file_obj.seek(1)\n        self.assertEqual(file_obj.tell(), 1)\n        self.assertEqual(file_obj.read(), b'2345678')\n\n        file_obj.seek(3, 0)\n        self.assertEqual(file_obj.tell(), 3)\n\n        file_obj.seek(0, 0)\n        self.assertEqual(file_obj.tell(), 0)\n\n    def _assert_whence_relative_behavior(self, file_obj):\n        self.assertEqual(file_obj.tell(), 0)\n\n        file_obj.seek(2, 1)\n        self.assertEqual(file_obj.tell(), 2)\n\n        file_obj.seek(1, 1)\n        self.assertEqual(file_obj.tell(), 3)\n        self.assertEqual(file_obj.read(), b'45678')\n\n        file_obj.seek(20, 1)\n        self.assertEqual(file_obj.tell(), 28)\n\n        file_obj.seek(-30, 1)\n        self.assertEqual(file_obj.tell(), 0)\n        self.assertEqual(file_obj.read(), b'12345678')\n\n        file_obj.seek(-8, 1)\n        self.assertEqual(file_obj.tell(), 0)\n\n    def _assert_whence_end_behavior(self, file_obj):\n        self.assertEqual(file_obj.tell(), 0)\n\n        file_obj.seek(-1, 2)\n        self.assertEqual(file_obj.tell(), 7)\n\n        file_obj.seek(1, 2)\n        self.assertEqual(file_obj.tell(), 9)\n\n        file_obj.seek(3, 2)\n        self.assertEqual(file_obj.tell(), 11)\n        self.assertEqual(file_obj.read(), b'')\n\n        file_obj.seek(-15, 2)\n        self.assertEqual(file_obj.tell(), 0)\n        self.assertEqual(file_obj.read(), b'12345678')\n\n        file_obj.seek(-8, 2)\n        self.assertEqual(file_obj.tell(), 0)\n\n    def _assert_boundary_behavior(self, file_obj):\n        # Verify we're at the start\n        self.assertEqual(file_obj.tell(), 0)\n\n        # Verify we can't move backwards beyond start of file\n        file_obj.seek(-10, 1)\n        self.assertEqual(file_obj.tell(), 0)\n\n        # Verify we *can* move after end of file, but return nothing\n        file_obj.seek(10, 2)\n        self.assertEqual(file_obj.tell(), 18)\n        self.assertEqual(file_obj.read(), b'')\n        self.assertEqual(file_obj.read(10), b'')\n\n        # Verify we can partially rewind\n        file_obj.seek(-12, 1)\n        self.assertEqual(file_obj.tell(), 6)\n        self.assertEqual(file_obj.read(), b'78')\n        self.assertEqual(file_obj.tell(), 8)\n\n        # Verify we can rewind to start\n        file_obj.seek(0)\n        self.assertEqual(file_obj.tell(), 0)\n\n    def test_file_chunk_supports_context_manager(self):\n        filename = os.path.join(self.tempdir, 'foo')\n        with open(filename, 'wb') as f:\n            f.write(b'abc')\n        with ReadFileChunk.from_filename(\n            filename, start_byte=0, chunk_size=2\n        ) as chunk:\n            val = chunk.read()\n            self.assertEqual(val, b'ab')\n\n    def test_iter_is_always_empty(self):\n        # This tests the workaround for the httplib bug (see\n        # the source for more info).\n        filename = os.path.join(self.tempdir, 'foo')\n        open(filename, 'wb').close()\n        chunk = ReadFileChunk.from_filename(\n            filename, start_byte=0, chunk_size=10\n        )\n        self.assertEqual(list(chunk), [])\n\n    def test_callback_is_invoked_on_read(self):\n        chunk = ReadFileChunk.from_filename(\n            self.filename,\n            start_byte=0,\n            chunk_size=3,\n            callbacks=[self.callback],\n        )\n        chunk.read(1)\n        chunk.read(1)\n        chunk.read(1)\n        self.assertEqual(self.amounts_seen, [1, 1, 1])\n\n    def test_all_callbacks_invoked_on_read(self):\n        chunk = ReadFileChunk.from_filename(\n            self.filename,\n            start_byte=0,\n            chunk_size=3,\n            callbacks=[self.callback, self.callback],\n        )\n        chunk.read(1)\n        chunk.read(1)\n        chunk.read(1)\n        # The list should be twice as long because there are two callbacks\n        # recording the amount read.\n        self.assertEqual(self.amounts_seen, [1, 1, 1, 1, 1, 1])\n\n    def test_callback_can_be_disabled(self):\n        chunk = ReadFileChunk.from_filename(\n            self.filename,\n            start_byte=0,\n            chunk_size=3,\n            callbacks=[self.callback],\n        )\n        chunk.disable_callback()\n        # Now reading from the ReadFileChunk should not invoke\n        # the callback.\n        chunk.read()\n        self.assertEqual(self.amounts_seen, [])\n\n    def test_callback_will_also_be_triggered_by_seek(self):\n        chunk = ReadFileChunk.from_filename(\n            self.filename,\n            start_byte=0,\n            chunk_size=3,\n            callbacks=[self.callback],\n        )\n        chunk.read(2)\n        chunk.seek(0)\n        chunk.read(2)\n        chunk.seek(1)\n        chunk.read(2)\n        self.assertEqual(self.amounts_seen, [2, -2, 2, -1, 2])\n\n    def test_callback_triggered_by_out_of_bound_seeks(self):\n        data = b'abcdefghij1234567890klmnopqr'\n\n        # Create test file\n        filename = os.path.join(self.tempdir, 'foo')\n        with open(filename, 'wb') as f:\n            f.write(data)\n        chunk = ReadFileChunk.from_filename(\n            filename, start_byte=10, chunk_size=10, callbacks=[self.callback]\n        )\n\n        # Seek calls that generate \"0\" progress are skipped by\n        # invoke_progress_callbacks and won't appear in the list.\n        expected_callback_prog = [10, -5, 5, -1, 1, -1, 1, -5, 5, -10]\n\n        self._assert_out_of_bound_start_seek(chunk, expected_callback_prog)\n        self._assert_out_of_bound_relative_seek(chunk, expected_callback_prog)\n        self._assert_out_of_bound_end_seek(chunk, expected_callback_prog)\n\n    def _assert_out_of_bound_start_seek(self, chunk, expected):\n        # clear amounts_seen\n        self.amounts_seen = []\n        self.assertEqual(self.amounts_seen, [])\n\n        # (position, change)\n        chunk.seek(20)  # (20, 10)\n        chunk.seek(5)  # (5, -5)\n        chunk.seek(20)  # (20, 5)\n        chunk.seek(9)  # (9, -1)\n        chunk.seek(20)  # (20, 1)\n        chunk.seek(11)  # (11, 0)\n        chunk.seek(20)  # (20, 0)\n        chunk.seek(9)  # (9, -1)\n        chunk.seek(20)  # (20, 1)\n        chunk.seek(5)  # (5, -5)\n        chunk.seek(20)  # (20, 5)\n        chunk.seek(0)  # (0, -10)\n        chunk.seek(0)  # (0, 0)\n\n        self.assertEqual(self.amounts_seen, expected)\n\n    def _assert_out_of_bound_relative_seek(self, chunk, expected):\n        # clear amounts_seen\n        self.amounts_seen = []\n        self.assertEqual(self.amounts_seen, [])\n\n        # (position, change)\n        chunk.seek(20, 1)  # (20, 10)\n        chunk.seek(-15, 1)  # (5, -5)\n        chunk.seek(15, 1)  # (20, 5)\n        chunk.seek(-11, 1)  # (9, -1)\n        chunk.seek(11, 1)  # (20, 1)\n        chunk.seek(-9, 1)  # (11, 0)\n        chunk.seek(9, 1)  # (20, 0)\n        chunk.seek(-11, 1)  # (9, -1)\n        chunk.seek(11, 1)  # (20, 1)\n        chunk.seek(-15, 1)  # (5, -5)\n        chunk.seek(15, 1)  # (20, 5)\n        chunk.seek(-20, 1)  # (0, -10)\n        chunk.seek(-1000, 1)  # (0, 0)\n\n        self.assertEqual(self.amounts_seen, expected)\n\n    def _assert_out_of_bound_end_seek(self, chunk, expected):\n        # clear amounts_seen\n        self.amounts_seen = []\n        self.assertEqual(self.amounts_seen, [])\n\n        # (position, change)\n        chunk.seek(10, 2)  # (20, 10)\n        chunk.seek(-5, 2)  # (5, -5)\n        chunk.seek(10, 2)  # (20, 5)\n        chunk.seek(-1, 2)  # (9, -1)\n        chunk.seek(10, 2)  # (20, 1)\n        chunk.seek(1, 2)  # (11, 0)\n        chunk.seek(10, 2)  # (20, 0)\n        chunk.seek(-1, 2)  # (9, -1)\n        chunk.seek(10, 2)  # (20, 1)\n        chunk.seek(-5, 2)  # (5, -5)\n        chunk.seek(10, 2)  # (20, 5)\n        chunk.seek(-10, 2)  # (0, -10)\n        chunk.seek(-1000, 2)  # (0, 0)\n\n        self.assertEqual(self.amounts_seen, expected)\n\n    def test_close_callbacks(self):\n        with open(self.filename) as f:\n            chunk = ReadFileChunk(\n                f,\n                chunk_size=1,\n                full_file_size=3,\n                close_callbacks=[self.close_callback],\n            )\n            chunk.close()\n            self.assertEqual(self.num_close_callback_calls, 1)\n\n    def test_close_callbacks_when_not_enabled(self):\n        with open(self.filename) as f:\n            chunk = ReadFileChunk(\n                f,\n                chunk_size=1,\n                full_file_size=3,\n                enable_callbacks=False,\n                close_callbacks=[self.close_callback],\n            )\n            chunk.close()\n            self.assertEqual(self.num_close_callback_calls, 0)\n\n    def test_close_callbacks_when_context_handler_is_used(self):\n        with open(self.filename) as f:\n            with ReadFileChunk(\n                f,\n                chunk_size=1,\n                full_file_size=3,\n                close_callbacks=[self.close_callback],\n            ) as chunk:\n                chunk.read(1)\n            self.assertEqual(self.num_close_callback_calls, 1)\n\n    def test_signal_transferring(self):\n        chunk = ReadFileChunk.from_filename(\n            self.filename,\n            start_byte=0,\n            chunk_size=3,\n            callbacks=[self.callback],\n        )\n        chunk.signal_not_transferring()\n        chunk.read(1)\n        self.assertEqual(self.amounts_seen, [])\n        chunk.signal_transferring()\n        chunk.read(1)\n        self.assertEqual(self.amounts_seen, [1])\n\n    def test_signal_transferring_to_underlying_fileobj(self):\n        underlying_stream = mock.Mock()\n        underlying_stream.tell.return_value = 0\n        chunk = ReadFileChunk(underlying_stream, 3, 3)\n        chunk.signal_transferring()\n        self.assertTrue(underlying_stream.signal_transferring.called)\n\n    def test_no_call_signal_transferring_to_underlying_fileobj(self):\n        underlying_stream = mock.Mock(io.RawIOBase)\n        underlying_stream.tell.return_value = 0\n        chunk = ReadFileChunk(underlying_stream, 3, 3)\n        try:\n            chunk.signal_transferring()\n        except AttributeError:\n            self.fail(\n                'The stream should not have tried to call signal_transferring '\n                'to the underlying stream.'\n            )\n\n    def test_signal_not_transferring_to_underlying_fileobj(self):\n        underlying_stream = mock.Mock()\n        underlying_stream.tell.return_value = 0\n        chunk = ReadFileChunk(underlying_stream, 3, 3)\n        chunk.signal_not_transferring()\n        self.assertTrue(underlying_stream.signal_not_transferring.called)\n\n    def test_no_call_signal_not_transferring_to_underlying_fileobj(self):\n        underlying_stream = mock.Mock(io.RawIOBase)\n        underlying_stream.tell.return_value = 0\n        chunk = ReadFileChunk(underlying_stream, 3, 3)\n        try:\n            chunk.signal_not_transferring()\n        except AttributeError:\n            self.fail(\n                'The stream should not have tried to call '\n                'signal_not_transferring to the underlying stream.'\n            )\n\n\nclass TestStreamReaderProgress(BaseUtilsTest):\n    def test_proxies_to_wrapped_stream(self):\n        original_stream = StringIO('foobarbaz')\n        wrapped = StreamReaderProgress(original_stream)\n        self.assertEqual(wrapped.read(), 'foobarbaz')\n\n    def test_callback_invoked(self):\n        original_stream = StringIO('foobarbaz')\n        wrapped = StreamReaderProgress(\n            original_stream, [self.callback, self.callback]\n        )\n        self.assertEqual(wrapped.read(), 'foobarbaz')\n        self.assertEqual(self.amounts_seen, [9, 9])\n\n\nclass TestTaskSemaphore(unittest.TestCase):\n    def setUp(self):\n        self.semaphore = TaskSemaphore(1)\n\n    def test_should_block_at_max_capacity(self):\n        self.semaphore.acquire('a', blocking=False)\n        with self.assertRaises(NoResourcesAvailable):\n            self.semaphore.acquire('a', blocking=False)\n\n    def test_release_capacity(self):\n        acquire_token = self.semaphore.acquire('a', blocking=False)\n        self.semaphore.release('a', acquire_token)\n        try:\n            self.semaphore.acquire('a', blocking=False)\n        except NoResourcesAvailable:\n            self.fail(\n                'The release of the semaphore should have allowed for '\n                'the second acquire to not be blocked'\n            )\n\n\nclass TestSlidingWindowSemaphore(unittest.TestCase):\n    # These tests use block=False to tests will fail\n    # instead of hang the test runner in the case of x\n    # incorrect behavior.\n    def test_acquire_release_basic_case(self):\n        sem = SlidingWindowSemaphore(1)\n        # Count is 1\n\n        num = sem.acquire('a', blocking=False)\n        self.assertEqual(num, 0)\n        sem.release('a', 0)\n        # Count now back to 1.\n\n    def test_can_acquire_release_multiple_times(self):\n        sem = SlidingWindowSemaphore(1)\n        num = sem.acquire('a', blocking=False)\n        self.assertEqual(num, 0)\n        sem.release('a', num)\n\n        num = sem.acquire('a', blocking=False)\n        self.assertEqual(num, 1)\n        sem.release('a', num)\n\n    def test_can_acquire_a_range(self):\n        sem = SlidingWindowSemaphore(3)\n        self.assertEqual(sem.acquire('a', blocking=False), 0)\n        self.assertEqual(sem.acquire('a', blocking=False), 1)\n        self.assertEqual(sem.acquire('a', blocking=False), 2)\n        sem.release('a', 0)\n        sem.release('a', 1)\n        sem.release('a', 2)\n        # Now we're reset so we should be able to acquire the same\n        # sequence again.\n        self.assertEqual(sem.acquire('a', blocking=False), 3)\n        self.assertEqual(sem.acquire('a', blocking=False), 4)\n        self.assertEqual(sem.acquire('a', blocking=False), 5)\n        self.assertEqual(sem.current_count(), 0)\n\n    def test_counter_release_only_on_min_element(self):\n        sem = SlidingWindowSemaphore(3)\n        sem.acquire('a', blocking=False)\n        sem.acquire('a', blocking=False)\n        sem.acquire('a', blocking=False)\n\n        # The count only increases when we free the min\n        # element.  This means if we're currently failing to\n        # acquire now:\n        with self.assertRaises(NoResourcesAvailable):\n            sem.acquire('a', blocking=False)\n\n        # Then freeing a non-min element:\n        sem.release('a', 1)\n\n        # doesn't change anything.  We still fail to acquire.\n        with self.assertRaises(NoResourcesAvailable):\n            sem.acquire('a', blocking=False)\n        self.assertEqual(sem.current_count(), 0)\n\n    def test_raises_error_when_count_is_zero(self):\n        sem = SlidingWindowSemaphore(3)\n        sem.acquire('a', blocking=False)\n        sem.acquire('a', blocking=False)\n        sem.acquire('a', blocking=False)\n\n        # Count is now 0 so trying to acquire should fail.\n        with self.assertRaises(NoResourcesAvailable):\n            sem.acquire('a', blocking=False)\n\n    def test_release_counters_can_increment_counter_repeatedly(self):\n        sem = SlidingWindowSemaphore(3)\n        sem.acquire('a', blocking=False)\n        sem.acquire('a', blocking=False)\n        sem.acquire('a', blocking=False)\n\n        # These two releases don't increment the counter\n        # because we're waiting on 0.\n        sem.release('a', 1)\n        sem.release('a', 2)\n        self.assertEqual(sem.current_count(), 0)\n        # But as soon as we release 0, we free up 0, 1, and 2.\n        sem.release('a', 0)\n        self.assertEqual(sem.current_count(), 3)\n        sem.acquire('a', blocking=False)\n        sem.acquire('a', blocking=False)\n        sem.acquire('a', blocking=False)\n\n    def test_error_to_release_unknown_tag(self):\n        sem = SlidingWindowSemaphore(3)\n        with self.assertRaises(ValueError):\n            sem.release('a', 0)\n\n    def test_can_track_multiple_tags(self):\n        sem = SlidingWindowSemaphore(3)\n        self.assertEqual(sem.acquire('a', blocking=False), 0)\n        self.assertEqual(sem.acquire('b', blocking=False), 0)\n        self.assertEqual(sem.acquire('a', blocking=False), 1)\n\n        # We're at our max of 3 even though 2 are for A and 1 is for B.\n        with self.assertRaises(NoResourcesAvailable):\n            sem.acquire('a', blocking=False)\n        with self.assertRaises(NoResourcesAvailable):\n            sem.acquire('b', blocking=False)\n\n    def test_can_handle_multiple_tags_released(self):\n        sem = SlidingWindowSemaphore(4)\n        sem.acquire('a', blocking=False)\n        sem.acquire('a', blocking=False)\n        sem.acquire('b', blocking=False)\n        sem.acquire('b', blocking=False)\n\n        sem.release('b', 1)\n        sem.release('a', 1)\n        self.assertEqual(sem.current_count(), 0)\n\n        sem.release('b', 0)\n        self.assertEqual(sem.acquire('a', blocking=False), 2)\n\n        sem.release('a', 0)\n        self.assertEqual(sem.acquire('b', blocking=False), 2)\n\n    def test_is_error_to_release_unknown_sequence_number(self):\n        sem = SlidingWindowSemaphore(3)\n        sem.acquire('a', blocking=False)\n        with self.assertRaises(ValueError):\n            sem.release('a', 1)\n\n    def test_is_error_to_double_release(self):\n        # This is different than other error tests because\n        # we're verifying we can reset the state after an\n        # acquire/release cycle.\n        sem = SlidingWindowSemaphore(2)\n        sem.acquire('a', blocking=False)\n        sem.acquire('a', blocking=False)\n        sem.release('a', 0)\n        sem.release('a', 1)\n        self.assertEqual(sem.current_count(), 2)\n        with self.assertRaises(ValueError):\n            sem.release('a', 0)\n\n    def test_can_check_in_partial_range(self):\n        sem = SlidingWindowSemaphore(4)\n        sem.acquire('a', blocking=False)\n        sem.acquire('a', blocking=False)\n        sem.acquire('a', blocking=False)\n        sem.acquire('a', blocking=False)\n\n        sem.release('a', 1)\n        sem.release('a', 3)\n        sem.release('a', 0)\n        self.assertEqual(sem.current_count(), 2)\n\n\nclass TestThreadingPropertiesForSlidingWindowSemaphore(unittest.TestCase):\n    # These tests focus on mutithreaded properties of the range\n    # semaphore.  Basic functionality is tested in TestSlidingWindowSemaphore.\n    def setUp(self):\n        self.threads = []\n\n    def tearDown(self):\n        self.join_threads()\n\n    def join_threads(self):\n        for thread in self.threads:\n            thread.join()\n        self.threads = []\n\n    def start_threads(self):\n        for thread in self.threads:\n            thread.start()\n\n    def test_acquire_blocks_until_release_is_called(self):\n        sem = SlidingWindowSemaphore(2)\n        sem.acquire('a', blocking=False)\n        sem.acquire('a', blocking=False)\n\n        def acquire():\n            # This next call to acquire will block.\n            self.assertEqual(sem.acquire('a', blocking=True), 2)\n\n        t = threading.Thread(target=acquire)\n        self.threads.append(t)\n        # Starting the thread will block the sem.acquire()\n        # in the acquire function above.\n        t.start()\n        # This still will keep the thread blocked.\n        sem.release('a', 1)\n        # Releasing the min element will unblock the thread.\n        sem.release('a', 0)\n        t.join()\n        sem.release('a', 2)\n\n    def test_stress_invariants_random_order(self):\n        sem = SlidingWindowSemaphore(100)\n        for _ in range(10):\n            recorded = []\n            for _ in range(100):\n                recorded.append(sem.acquire('a', blocking=False))\n            # Release them in randomized order.  As long as we\n            # eventually free all 100, we should have all the\n            # resources released.\n            random.shuffle(recorded)\n            for i in recorded:\n                sem.release('a', i)\n\n        # Everything's freed so should be back at count == 100\n        self.assertEqual(sem.current_count(), 100)\n\n    def test_blocking_stress(self):\n        sem = SlidingWindowSemaphore(5)\n        num_threads = 10\n        num_iterations = 50\n\n        def acquire():\n            for _ in range(num_iterations):\n                num = sem.acquire('a', blocking=True)\n                time.sleep(0.001)\n                sem.release('a', num)\n\n        for i in range(num_threads):\n            t = threading.Thread(target=acquire)\n            self.threads.append(t)\n        self.start_threads()\n        self.join_threads()\n        # Should have all the available resources freed.\n        self.assertEqual(sem.current_count(), 5)\n        # Should have acquired num_threads * num_iterations\n        self.assertEqual(\n            sem.acquire('a', blocking=False), num_threads * num_iterations\n        )\n\n\nclass TestAdjustChunksize(unittest.TestCase):\n    def setUp(self):\n        self.adjuster = ChunksizeAdjuster()\n\n    def test_valid_chunksize(self):\n        chunksize = 7 * (1024**2)\n        file_size = 8 * (1024**2)\n        new_size = self.adjuster.adjust_chunksize(chunksize, file_size)\n        self.assertEqual(new_size, chunksize)\n\n    def test_chunksize_below_minimum(self):\n        chunksize = MIN_UPLOAD_CHUNKSIZE - 1\n        file_size = 3 * MIN_UPLOAD_CHUNKSIZE\n        new_size = self.adjuster.adjust_chunksize(chunksize, file_size)\n        self.assertEqual(new_size, MIN_UPLOAD_CHUNKSIZE)\n\n    def test_chunksize_above_maximum(self):\n        chunksize = MAX_SINGLE_UPLOAD_SIZE + 1\n        file_size = MAX_SINGLE_UPLOAD_SIZE * 2\n        new_size = self.adjuster.adjust_chunksize(chunksize, file_size)\n        self.assertEqual(new_size, MAX_SINGLE_UPLOAD_SIZE)\n\n    def test_chunksize_too_small(self):\n        chunksize = 7 * (1024**2)\n        file_size = 5 * (1024**4)\n        # If we try to upload a 5TB file, we'll need to use 896MB part\n        # sizes.\n        new_size = self.adjuster.adjust_chunksize(chunksize, file_size)\n        self.assertEqual(new_size, 896 * (1024**2))\n        num_parts = file_size / new_size\n        self.assertLessEqual(num_parts, MAX_PARTS)\n\n    def test_unknown_file_size_with_valid_chunksize(self):\n        chunksize = 7 * (1024**2)\n        new_size = self.adjuster.adjust_chunksize(chunksize)\n        self.assertEqual(new_size, chunksize)\n\n    def test_unknown_file_size_below_minimum(self):\n        chunksize = MIN_UPLOAD_CHUNKSIZE - 1\n        new_size = self.adjuster.adjust_chunksize(chunksize)\n        self.assertEqual(new_size, MIN_UPLOAD_CHUNKSIZE)\n\n    def test_unknown_file_size_above_maximum(self):\n        chunksize = MAX_SINGLE_UPLOAD_SIZE + 1\n        new_size = self.adjuster.adjust_chunksize(chunksize)\n        self.assertEqual(new_size, MAX_SINGLE_UPLOAD_SIZE)\n\n\nclass TestS3ExpressDefaults:\n    @pytest.mark.parametrize(\n        \"bucket,extra_args,expected\",\n        (\n            (\n                \"mytestbucket--usw2-az2--x-s3\",\n                {},\n                {\"ChecksumAlgorithm\": \"crc32\"},\n            ),\n            (\n                \"mytestbucket--usw2-az2--x-s3\",\n                {\"Some\": \"Setting\"},\n                {\"ChecksumAlgorithm\": \"crc32\", \"Some\": \"Setting\"},\n            ),\n            (\n                \"mytestbucket\",\n                {},\n                {},\n            ),\n            (\n                \"mytestbucket--usw2-az2--x-s3\",\n                {\"ChecksumAlgorithm\": \"sha256\"},\n                {\"ChecksumAlgorithm\": \"sha256\"},\n            ),\n        ),\n    )\n    def test_add_s3express_defaults(self, bucket, extra_args, expected):\n        add_s3express_defaults(bucket, extra_args)\n        assert extra_args == expected\n", "tests/unit/test_download.py": "# Copyright 2016 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport copy\nimport os\nimport shutil\nimport tempfile\nfrom io import BytesIO\n\nfrom s3transfer.bandwidth import BandwidthLimiter\nfrom s3transfer.compat import SOCKET_ERROR\nfrom s3transfer.download import (\n    CompleteDownloadNOOPTask,\n    DeferQueue,\n    DownloadChunkIterator,\n    DownloadFilenameOutputManager,\n    DownloadNonSeekableOutputManager,\n    DownloadSeekableOutputManager,\n    DownloadSpecialFilenameOutputManager,\n    DownloadSubmissionTask,\n    GetObjectTask,\n    ImmediatelyWriteIOGetObjectTask,\n    IOCloseTask,\n    IORenameFileTask,\n    IOStreamingWriteTask,\n    IOWriteTask,\n)\nfrom s3transfer.exceptions import RetriesExceededError\nfrom s3transfer.futures import IN_MEMORY_DOWNLOAD_TAG, BoundedExecutor\nfrom s3transfer.utils import CallArgs, OSUtils\nfrom tests import (\n    BaseSubmissionTaskTest,\n    BaseTaskTest,\n    FileCreator,\n    NonSeekableWriter,\n    RecordingExecutor,\n    StreamWithError,\n    mock,\n    unittest,\n)\n\n\nclass DownloadException(Exception):\n    pass\n\n\nclass WriteCollector:\n    \"\"\"A utility to collect information about writes and seeks\"\"\"\n\n    def __init__(self):\n        self._pos = 0\n        self.writes = []\n\n    def seek(self, pos, whence=0):\n        self._pos = pos\n\n    def write(self, data):\n        self.writes.append((self._pos, data))\n        self._pos += len(data)\n\n\nclass AlwaysIndicatesSpecialFileOSUtils(OSUtils):\n    \"\"\"OSUtil that always returns True for is_special_file\"\"\"\n\n    def is_special_file(self, filename):\n        return True\n\n\nclass CancelledStreamWrapper:\n    \"\"\"A wrapper to trigger a cancellation while stream reading\n\n    Forces the transfer coordinator to cancel after a certain amount of reads\n    :param stream: The underlying stream to read from\n    :param transfer_coordinator: The coordinator for the transfer\n    :param num_reads: On which read to signal a cancellation. 0 is the first\n        read.\n    \"\"\"\n\n    def __init__(self, stream, transfer_coordinator, num_reads=0):\n        self._stream = stream\n        self._transfer_coordinator = transfer_coordinator\n        self._num_reads = num_reads\n        self._count = 0\n\n    def read(self, *args, **kwargs):\n        if self._num_reads == self._count:\n            self._transfer_coordinator.cancel()\n        self._stream.read(*args, **kwargs)\n        self._count += 1\n\n\nclass BaseDownloadOutputManagerTest(BaseTaskTest):\n    def setUp(self):\n        super().setUp()\n        self.osutil = OSUtils()\n\n        # Create a file to write to\n        self.tempdir = tempfile.mkdtemp()\n        self.filename = os.path.join(self.tempdir, 'myfile')\n\n        self.call_args = CallArgs(fileobj=self.filename)\n        self.future = self.get_transfer_future(self.call_args)\n        self.io_executor = BoundedExecutor(1000, 1)\n\n    def tearDown(self):\n        super().tearDown()\n        shutil.rmtree(self.tempdir)\n\n\nclass TestDownloadFilenameOutputManager(BaseDownloadOutputManagerTest):\n    def setUp(self):\n        super().setUp()\n        self.download_output_manager = DownloadFilenameOutputManager(\n            self.osutil,\n            self.transfer_coordinator,\n            io_executor=self.io_executor,\n        )\n\n    def test_is_compatible(self):\n        self.assertTrue(\n            self.download_output_manager.is_compatible(\n                self.filename, self.osutil\n            )\n        )\n\n    def test_get_download_task_tag(self):\n        self.assertIsNone(self.download_output_manager.get_download_task_tag())\n\n    def test_get_fileobj_for_io_writes(self):\n        with self.download_output_manager.get_fileobj_for_io_writes(\n            self.future\n        ) as f:\n            # Ensure it is a file like object returned\n            self.assertTrue(hasattr(f, 'read'))\n            self.assertTrue(hasattr(f, 'seek'))\n            # Make sure the name of the file returned is not the same as the\n            # final filename as we should be writing to a temporary file.\n            self.assertNotEqual(f.name, self.filename)\n\n    def test_get_final_io_task(self):\n        ref_contents = b'my_contents'\n        with self.download_output_manager.get_fileobj_for_io_writes(\n            self.future\n        ) as f:\n            temp_filename = f.name\n            # Write some data to test that the data gets moved over to the\n            # final location.\n            f.write(ref_contents)\n            final_task = self.download_output_manager.get_final_io_task()\n            # Make sure it is the appropriate task.\n            self.assertIsInstance(final_task, IORenameFileTask)\n            final_task()\n            # Make sure the temp_file gets removed\n            self.assertFalse(os.path.exists(temp_filename))\n        # Make sure what ever was written to the temp file got moved to\n        # the final filename\n        with open(self.filename, 'rb') as f:\n            self.assertEqual(f.read(), ref_contents)\n\n    def test_can_queue_file_io_task(self):\n        fileobj = WriteCollector()\n        self.download_output_manager.queue_file_io_task(\n            fileobj=fileobj, data='foo', offset=0\n        )\n        self.download_output_manager.queue_file_io_task(\n            fileobj=fileobj, data='bar', offset=3\n        )\n        self.io_executor.shutdown()\n        self.assertEqual(fileobj.writes, [(0, 'foo'), (3, 'bar')])\n\n    def test_get_file_io_write_task(self):\n        fileobj = WriteCollector()\n        io_write_task = self.download_output_manager.get_io_write_task(\n            fileobj=fileobj, data='foo', offset=3\n        )\n        self.assertIsInstance(io_write_task, IOWriteTask)\n\n        io_write_task()\n        self.assertEqual(fileobj.writes, [(3, 'foo')])\n\n\nclass TestDownloadSpecialFilenameOutputManager(BaseDownloadOutputManagerTest):\n    def setUp(self):\n        super().setUp()\n        self.osutil = AlwaysIndicatesSpecialFileOSUtils()\n        self.download_output_manager = DownloadSpecialFilenameOutputManager(\n            self.osutil,\n            self.transfer_coordinator,\n            io_executor=self.io_executor,\n        )\n\n    def test_is_compatible_for_special_file(self):\n        self.assertTrue(\n            self.download_output_manager.is_compatible(\n                self.filename, AlwaysIndicatesSpecialFileOSUtils()\n            )\n        )\n\n    def test_is_not_compatible_for_non_special_file(self):\n        self.assertFalse(\n            self.download_output_manager.is_compatible(\n                self.filename, OSUtils()\n            )\n        )\n\n    def test_get_fileobj_for_io_writes(self):\n        with self.download_output_manager.get_fileobj_for_io_writes(\n            self.future\n        ) as f:\n            # Ensure it is a file like object returned\n            self.assertTrue(hasattr(f, 'read'))\n            # Make sure the name of the file returned is the same as the\n            # final filename as we should not be writing to a temporary file.\n            self.assertEqual(f.name, self.filename)\n\n    def test_get_final_io_task(self):\n        self.assertIsInstance(\n            self.download_output_manager.get_final_io_task(), IOCloseTask\n        )\n\n    def test_can_queue_file_io_task(self):\n        fileobj = WriteCollector()\n        self.download_output_manager.queue_file_io_task(\n            fileobj=fileobj, data='foo', offset=0\n        )\n        self.download_output_manager.queue_file_io_task(\n            fileobj=fileobj, data='bar', offset=3\n        )\n        self.io_executor.shutdown()\n        self.assertEqual(fileobj.writes, [(0, 'foo'), (3, 'bar')])\n\n\nclass TestDownloadSeekableOutputManager(BaseDownloadOutputManagerTest):\n    def setUp(self):\n        super().setUp()\n        self.download_output_manager = DownloadSeekableOutputManager(\n            self.osutil,\n            self.transfer_coordinator,\n            io_executor=self.io_executor,\n        )\n\n        # Create a fileobj to write to\n        self.fileobj = open(self.filename, 'wb')\n\n        self.call_args = CallArgs(fileobj=self.fileobj)\n        self.future = self.get_transfer_future(self.call_args)\n\n    def tearDown(self):\n        self.fileobj.close()\n        super().tearDown()\n\n    def test_is_compatible(self):\n        self.assertTrue(\n            self.download_output_manager.is_compatible(\n                self.fileobj, self.osutil\n            )\n        )\n\n    def test_is_compatible_bytes_io(self):\n        self.assertTrue(\n            self.download_output_manager.is_compatible(BytesIO(), self.osutil)\n        )\n\n    def test_not_compatible_for_non_filelike_obj(self):\n        self.assertFalse(\n            self.download_output_manager.is_compatible(object(), self.osutil)\n        )\n\n    def test_get_download_task_tag(self):\n        self.assertIsNone(self.download_output_manager.get_download_task_tag())\n\n    def test_get_fileobj_for_io_writes(self):\n        self.assertIs(\n            self.download_output_manager.get_fileobj_for_io_writes(\n                self.future\n            ),\n            self.fileobj,\n        )\n\n    def test_get_final_io_task(self):\n        self.assertIsInstance(\n            self.download_output_manager.get_final_io_task(),\n            CompleteDownloadNOOPTask,\n        )\n\n    def test_can_queue_file_io_task(self):\n        fileobj = WriteCollector()\n        self.download_output_manager.queue_file_io_task(\n            fileobj=fileobj, data='foo', offset=0\n        )\n        self.download_output_manager.queue_file_io_task(\n            fileobj=fileobj, data='bar', offset=3\n        )\n        self.io_executor.shutdown()\n        self.assertEqual(fileobj.writes, [(0, 'foo'), (3, 'bar')])\n\n    def test_get_file_io_write_task(self):\n        fileobj = WriteCollector()\n        io_write_task = self.download_output_manager.get_io_write_task(\n            fileobj=fileobj, data='foo', offset=3\n        )\n        self.assertIsInstance(io_write_task, IOWriteTask)\n\n        io_write_task()\n        self.assertEqual(fileobj.writes, [(3, 'foo')])\n\n\nclass TestDownloadNonSeekableOutputManager(BaseDownloadOutputManagerTest):\n    def setUp(self):\n        super().setUp()\n        self.download_output_manager = DownloadNonSeekableOutputManager(\n            self.osutil, self.transfer_coordinator, io_executor=None\n        )\n\n    def test_is_compatible_with_seekable_stream(self):\n        with open(self.filename, 'wb') as f:\n            self.assertTrue(\n                self.download_output_manager.is_compatible(f, self.osutil)\n            )\n\n    def test_not_compatible_with_filename(self):\n        self.assertFalse(\n            self.download_output_manager.is_compatible(\n                self.filename, self.osutil\n            )\n        )\n\n    def test_compatible_with_non_seekable_stream(self):\n        class NonSeekable:\n            def write(self, data):\n                pass\n\n        f = NonSeekable()\n        self.assertTrue(\n            self.download_output_manager.is_compatible(f, self.osutil)\n        )\n\n    def test_is_compatible_with_bytesio(self):\n        self.assertTrue(\n            self.download_output_manager.is_compatible(BytesIO(), self.osutil)\n        )\n\n    def test_get_download_task_tag(self):\n        self.assertIs(\n            self.download_output_manager.get_download_task_tag(),\n            IN_MEMORY_DOWNLOAD_TAG,\n        )\n\n    def test_submit_writes_from_internal_queue(self):\n        class FakeQueue:\n            def request_writes(self, offset, data):\n                return [\n                    {'offset': 0, 'data': 'foo'},\n                    {'offset': 3, 'data': 'bar'},\n                ]\n\n        q = FakeQueue()\n        io_executor = BoundedExecutor(1000, 1)\n        manager = DownloadNonSeekableOutputManager(\n            self.osutil,\n            self.transfer_coordinator,\n            io_executor=io_executor,\n            defer_queue=q,\n        )\n        fileobj = WriteCollector()\n        manager.queue_file_io_task(fileobj=fileobj, data='foo', offset=1)\n        io_executor.shutdown()\n        self.assertEqual(fileobj.writes, [(0, 'foo'), (3, 'bar')])\n\n    def test_get_file_io_write_task(self):\n        fileobj = WriteCollector()\n        io_write_task = self.download_output_manager.get_io_write_task(\n            fileobj=fileobj, data='foo', offset=1\n        )\n        self.assertIsInstance(io_write_task, IOStreamingWriteTask)\n\n        io_write_task()\n        self.assertEqual(fileobj.writes, [(0, 'foo')])\n\n\nclass TestDownloadSubmissionTask(BaseSubmissionTaskTest):\n    def setUp(self):\n        super().setUp()\n        self.tempdir = tempfile.mkdtemp()\n        self.filename = os.path.join(self.tempdir, 'myfile')\n\n        self.bucket = 'mybucket'\n        self.key = 'mykey'\n        self.extra_args = {}\n        self.subscribers = []\n\n        # Create a stream to read from\n        self.content = b'my content'\n        self.stream = BytesIO(self.content)\n\n        # A list to keep track of all of the bodies sent over the wire\n        # and their order.\n\n        self.call_args = self.get_call_args()\n        self.transfer_future = self.get_transfer_future(self.call_args)\n        self.io_executor = BoundedExecutor(1000, 1)\n        self.submission_main_kwargs = {\n            'client': self.client,\n            'config': self.config,\n            'osutil': self.osutil,\n            'request_executor': self.executor,\n            'io_executor': self.io_executor,\n            'transfer_future': self.transfer_future,\n        }\n        self.submission_task = self.get_download_submission_task()\n\n    def tearDown(self):\n        super().tearDown()\n        shutil.rmtree(self.tempdir)\n\n    def get_call_args(self, **kwargs):\n        default_call_args = {\n            'fileobj': self.filename,\n            'bucket': self.bucket,\n            'key': self.key,\n            'extra_args': self.extra_args,\n            'subscribers': self.subscribers,\n        }\n        default_call_args.update(kwargs)\n        return CallArgs(**default_call_args)\n\n    def wrap_executor_in_recorder(self):\n        self.executor = RecordingExecutor(self.executor)\n        self.submission_main_kwargs['request_executor'] = self.executor\n\n    def use_fileobj_in_call_args(self, fileobj):\n        self.call_args = self.get_call_args(fileobj=fileobj)\n        self.transfer_future = self.get_transfer_future(self.call_args)\n        self.submission_main_kwargs['transfer_future'] = self.transfer_future\n\n    def assert_tag_for_get_object(self, tag_value):\n        submissions_to_compare = self.executor.submissions\n        if len(submissions_to_compare) > 1:\n            # If it was ranged get, make sure we do not include the join task.\n            submissions_to_compare = submissions_to_compare[:-1]\n        for submission in submissions_to_compare:\n            self.assertEqual(submission['tag'], tag_value)\n\n    def add_head_object_response(self):\n        self.stubber.add_response(\n            'head_object', {'ContentLength': len(self.content)}\n        )\n\n    def add_get_responses(self):\n        chunksize = self.config.multipart_chunksize\n        for i in range(0, len(self.content), chunksize):\n            if i + chunksize > len(self.content):\n                stream = BytesIO(self.content[i:])\n                self.stubber.add_response('get_object', {'Body': stream})\n            else:\n                stream = BytesIO(self.content[i : i + chunksize])\n                self.stubber.add_response('get_object', {'Body': stream})\n\n    def configure_for_ranged_get(self):\n        self.config.multipart_threshold = 1\n        self.config.multipart_chunksize = 4\n\n    def get_download_submission_task(self):\n        return self.get_task(\n            DownloadSubmissionTask, main_kwargs=self.submission_main_kwargs\n        )\n\n    def wait_and_assert_completed_successfully(self, submission_task):\n        submission_task()\n        self.transfer_future.result()\n        self.stubber.assert_no_pending_responses()\n\n    def test_submits_no_tag_for_get_object_filename(self):\n        self.wrap_executor_in_recorder()\n        self.add_head_object_response()\n        self.add_get_responses()\n\n        self.submission_task = self.get_download_submission_task()\n        self.wait_and_assert_completed_successfully(self.submission_task)\n\n        # Make sure no tag to limit that task specifically was not associated\n        # to that task submission.\n        self.assert_tag_for_get_object(None)\n\n    def test_submits_no_tag_for_ranged_get_filename(self):\n        self.wrap_executor_in_recorder()\n        self.configure_for_ranged_get()\n        self.add_head_object_response()\n        self.add_get_responses()\n\n        self.submission_task = self.get_download_submission_task()\n        self.wait_and_assert_completed_successfully(self.submission_task)\n\n        # Make sure no tag to limit that task specifically was not associated\n        # to that task submission.\n        self.assert_tag_for_get_object(None)\n\n    def test_submits_no_tag_for_get_object_fileobj(self):\n        self.wrap_executor_in_recorder()\n        self.add_head_object_response()\n        self.add_get_responses()\n\n        with open(self.filename, 'wb') as f:\n            self.use_fileobj_in_call_args(f)\n            self.submission_task = self.get_download_submission_task()\n            self.wait_and_assert_completed_successfully(self.submission_task)\n\n        # Make sure no tag to limit that task specifically was not associated\n        # to that task submission.\n        self.assert_tag_for_get_object(None)\n\n    def test_submits_no_tag_for_ranged_get_object_fileobj(self):\n        self.wrap_executor_in_recorder()\n        self.configure_for_ranged_get()\n        self.add_head_object_response()\n        self.add_get_responses()\n\n        with open(self.filename, 'wb') as f:\n            self.use_fileobj_in_call_args(f)\n            self.submission_task = self.get_download_submission_task()\n            self.wait_and_assert_completed_successfully(self.submission_task)\n\n        # Make sure no tag to limit that task specifically was not associated\n        # to that task submission.\n        self.assert_tag_for_get_object(None)\n\n    def tests_submits_tag_for_get_object_nonseekable_fileobj(self):\n        self.wrap_executor_in_recorder()\n        self.add_head_object_response()\n        self.add_get_responses()\n\n        with open(self.filename, 'wb') as f:\n            self.use_fileobj_in_call_args(NonSeekableWriter(f))\n            self.submission_task = self.get_download_submission_task()\n            self.wait_and_assert_completed_successfully(self.submission_task)\n\n        # Make sure no tag to limit that task specifically was not associated\n        # to that task submission.\n        self.assert_tag_for_get_object(IN_MEMORY_DOWNLOAD_TAG)\n\n    def tests_submits_tag_for_ranged_get_object_nonseekable_fileobj(self):\n        self.wrap_executor_in_recorder()\n        self.configure_for_ranged_get()\n        self.add_head_object_response()\n        self.add_get_responses()\n\n        with open(self.filename, 'wb') as f:\n            self.use_fileobj_in_call_args(NonSeekableWriter(f))\n            self.submission_task = self.get_download_submission_task()\n            self.wait_and_assert_completed_successfully(self.submission_task)\n\n        # Make sure no tag to limit that task specifically was not associated\n        # to that task submission.\n        self.assert_tag_for_get_object(IN_MEMORY_DOWNLOAD_TAG)\n\n\nclass TestGetObjectTask(BaseTaskTest):\n    def setUp(self):\n        super().setUp()\n        self.bucket = 'mybucket'\n        self.key = 'mykey'\n        self.extra_args = {}\n        self.callbacks = []\n        self.max_attempts = 5\n        self.io_executor = BoundedExecutor(1000, 1)\n        self.content = b'my content'\n        self.stream = BytesIO(self.content)\n        self.fileobj = WriteCollector()\n        self.osutil = OSUtils()\n        self.io_chunksize = 64 * (1024**2)\n        self.task_cls = GetObjectTask\n        self.download_output_manager = DownloadSeekableOutputManager(\n            self.osutil, self.transfer_coordinator, self.io_executor\n        )\n\n    def get_download_task(self, **kwargs):\n        default_kwargs = {\n            'client': self.client,\n            'bucket': self.bucket,\n            'key': self.key,\n            'fileobj': self.fileobj,\n            'extra_args': self.extra_args,\n            'callbacks': self.callbacks,\n            'max_attempts': self.max_attempts,\n            'download_output_manager': self.download_output_manager,\n            'io_chunksize': self.io_chunksize,\n        }\n        default_kwargs.update(kwargs)\n        self.transfer_coordinator.set_status_to_queued()\n        return self.get_task(self.task_cls, main_kwargs=default_kwargs)\n\n    def assert_io_writes(self, expected_writes):\n        # Let the io executor process all of the writes before checking\n        # what writes were sent to it.\n        self.io_executor.shutdown()\n        self.assertEqual(self.fileobj.writes, expected_writes)\n\n    def test_main(self):\n        self.stubber.add_response(\n            'get_object',\n            service_response={'Body': self.stream},\n            expected_params={'Bucket': self.bucket, 'Key': self.key},\n        )\n        task = self.get_download_task()\n        task()\n\n        self.stubber.assert_no_pending_responses()\n        self.assert_io_writes([(0, self.content)])\n\n    def test_extra_args(self):\n        self.stubber.add_response(\n            'get_object',\n            service_response={'Body': self.stream},\n            expected_params={\n                'Bucket': self.bucket,\n                'Key': self.key,\n                'Range': 'bytes=0-',\n            },\n        )\n        self.extra_args['Range'] = 'bytes=0-'\n        task = self.get_download_task()\n        task()\n\n        self.stubber.assert_no_pending_responses()\n        self.assert_io_writes([(0, self.content)])\n\n    def test_control_chunk_size(self):\n        self.stubber.add_response(\n            'get_object',\n            service_response={'Body': self.stream},\n            expected_params={'Bucket': self.bucket, 'Key': self.key},\n        )\n        task = self.get_download_task(io_chunksize=1)\n        task()\n\n        self.stubber.assert_no_pending_responses()\n        expected_contents = []\n        for i in range(len(self.content)):\n            expected_contents.append((i, bytes(self.content[i : i + 1])))\n\n        self.assert_io_writes(expected_contents)\n\n    def test_start_index(self):\n        self.stubber.add_response(\n            'get_object',\n            service_response={'Body': self.stream},\n            expected_params={'Bucket': self.bucket, 'Key': self.key},\n        )\n        task = self.get_download_task(start_index=5)\n        task()\n\n        self.stubber.assert_no_pending_responses()\n        self.assert_io_writes([(5, self.content)])\n\n    def test_uses_bandwidth_limiter(self):\n        bandwidth_limiter = mock.Mock(BandwidthLimiter)\n\n        self.stubber.add_response(\n            'get_object',\n            service_response={'Body': self.stream},\n            expected_params={'Bucket': self.bucket, 'Key': self.key},\n        )\n        task = self.get_download_task(bandwidth_limiter=bandwidth_limiter)\n        task()\n\n        self.stubber.assert_no_pending_responses()\n        self.assertEqual(\n            bandwidth_limiter.get_bandwith_limited_stream.call_args_list,\n            [mock.call(mock.ANY, self.transfer_coordinator)],\n        )\n\n    def test_retries_succeeds(self):\n        self.stubber.add_response(\n            'get_object',\n            service_response={\n                'Body': StreamWithError(self.stream, SOCKET_ERROR)\n            },\n            expected_params={'Bucket': self.bucket, 'Key': self.key},\n        )\n        self.stubber.add_response(\n            'get_object',\n            service_response={'Body': self.stream},\n            expected_params={'Bucket': self.bucket, 'Key': self.key},\n        )\n        task = self.get_download_task()\n        task()\n\n        # Retryable error should have not affected the bytes placed into\n        # the io queue.\n        self.stubber.assert_no_pending_responses()\n        self.assert_io_writes([(0, self.content)])\n\n    def test_retries_failure(self):\n        for _ in range(self.max_attempts):\n            self.stubber.add_response(\n                'get_object',\n                service_response={\n                    'Body': StreamWithError(self.stream, SOCKET_ERROR)\n                },\n                expected_params={'Bucket': self.bucket, 'Key': self.key},\n            )\n\n        task = self.get_download_task()\n        task()\n        self.transfer_coordinator.announce_done()\n\n        # Should have failed out on a RetriesExceededError\n        with self.assertRaises(RetriesExceededError):\n            self.transfer_coordinator.result()\n        self.stubber.assert_no_pending_responses()\n\n    def test_retries_in_middle_of_streaming(self):\n        # After the first read a retryable error will be thrown\n        self.stubber.add_response(\n            'get_object',\n            service_response={\n                'Body': StreamWithError(\n                    copy.deepcopy(self.stream), SOCKET_ERROR, 1\n                )\n            },\n            expected_params={'Bucket': self.bucket, 'Key': self.key},\n        )\n        self.stubber.add_response(\n            'get_object',\n            service_response={'Body': self.stream},\n            expected_params={'Bucket': self.bucket, 'Key': self.key},\n        )\n        task = self.get_download_task(io_chunksize=1)\n        task()\n\n        self.stubber.assert_no_pending_responses()\n        expected_contents = []\n        # This is the content initially read in before the retry hit on the\n        # second read()\n        expected_contents.append((0, bytes(self.content[0:1])))\n\n        # The rest of the content should be the entire set of data partitioned\n        # out based on the one byte stream chunk size. Note the second\n        # element in the list should be a copy of the first element since\n        # a retryable exception happened in between.\n        for i in range(len(self.content)):\n            expected_contents.append((i, bytes(self.content[i : i + 1])))\n        self.assert_io_writes(expected_contents)\n\n    def test_cancels_out_of_queueing(self):\n        self.stubber.add_response(\n            'get_object',\n            service_response={\n                'Body': CancelledStreamWrapper(\n                    self.stream, self.transfer_coordinator\n                )\n            },\n            expected_params={'Bucket': self.bucket, 'Key': self.key},\n        )\n        task = self.get_download_task()\n        task()\n\n        self.stubber.assert_no_pending_responses()\n        # Make sure that no contents were added to the queue because the task\n        # should have been canceled before trying to add the contents to the\n        # io queue.\n        self.assert_io_writes([])\n\n    def test_handles_callback_on_initial_error(self):\n        # We can't use the stubber for this because we need to raise\n        # a S3_RETRYABLE_DOWNLOAD_ERRORS, and the stubber only allows\n        # you to raise a ClientError.\n        self.client.get_object = mock.Mock(side_effect=SOCKET_ERROR())\n        task = self.get_download_task()\n        task()\n        self.transfer_coordinator.announce_done()\n        # Should have failed out on a RetriesExceededError because\n        # get_object keeps raising a socket error.\n        with self.assertRaises(RetriesExceededError):\n            self.transfer_coordinator.result()\n\n\nclass TestImmediatelyWriteIOGetObjectTask(TestGetObjectTask):\n    def setUp(self):\n        super().setUp()\n        self.task_cls = ImmediatelyWriteIOGetObjectTask\n        # When data is written out, it should not use the io executor at all\n        # if it does use the io executor that is a deviation from expected\n        # behavior as the data should be written immediately to the file\n        # object once downloaded.\n        self.io_executor = None\n        self.download_output_manager = DownloadSeekableOutputManager(\n            self.osutil, self.transfer_coordinator, self.io_executor\n        )\n\n    def assert_io_writes(self, expected_writes):\n        self.assertEqual(self.fileobj.writes, expected_writes)\n\n\nclass BaseIOTaskTest(BaseTaskTest):\n    def setUp(self):\n        super().setUp()\n        self.files = FileCreator()\n        self.osutil = OSUtils()\n        self.temp_filename = os.path.join(self.files.rootdir, 'mytempfile')\n        self.final_filename = os.path.join(self.files.rootdir, 'myfile')\n\n    def tearDown(self):\n        super().tearDown()\n        self.files.remove_all()\n\n\nclass TestIOStreamingWriteTask(BaseIOTaskTest):\n    def test_main(self):\n        with open(self.temp_filename, 'wb') as f:\n            task = self.get_task(\n                IOStreamingWriteTask,\n                main_kwargs={'fileobj': f, 'data': b'foobar'},\n            )\n            task()\n            task2 = self.get_task(\n                IOStreamingWriteTask,\n                main_kwargs={'fileobj': f, 'data': b'baz'},\n            )\n            task2()\n        with open(self.temp_filename, 'rb') as f:\n            # We should just have written to the file in the order\n            # the tasks were executed.\n            self.assertEqual(f.read(), b'foobarbaz')\n\n\nclass TestIOWriteTask(BaseIOTaskTest):\n    def test_main(self):\n        with open(self.temp_filename, 'wb') as f:\n            # Write once to the file\n            task = self.get_task(\n                IOWriteTask,\n                main_kwargs={'fileobj': f, 'data': b'foo', 'offset': 0},\n            )\n            task()\n\n            # Write again to the file\n            task = self.get_task(\n                IOWriteTask,\n                main_kwargs={'fileobj': f, 'data': b'bar', 'offset': 3},\n            )\n            task()\n\n        with open(self.temp_filename, 'rb') as f:\n            self.assertEqual(f.read(), b'foobar')\n\n\nclass TestIORenameFileTask(BaseIOTaskTest):\n    def test_main(self):\n        with open(self.temp_filename, 'wb') as f:\n            task = self.get_task(\n                IORenameFileTask,\n                main_kwargs={\n                    'fileobj': f,\n                    'final_filename': self.final_filename,\n                    'osutil': self.osutil,\n                },\n            )\n            task()\n        self.assertTrue(os.path.exists(self.final_filename))\n        self.assertFalse(os.path.exists(self.temp_filename))\n\n\nclass TestIOCloseTask(BaseIOTaskTest):\n    def test_main(self):\n        with open(self.temp_filename, 'w') as f:\n            task = self.get_task(IOCloseTask, main_kwargs={'fileobj': f})\n            task()\n            self.assertTrue(f.closed)\n\n\nclass TestDownloadChunkIterator(unittest.TestCase):\n    def test_iter(self):\n        content = b'my content'\n        body = BytesIO(content)\n        ref_chunks = []\n        for chunk in DownloadChunkIterator(body, len(content)):\n            ref_chunks.append(chunk)\n        self.assertEqual(ref_chunks, [b'my content'])\n\n    def test_iter_chunksize(self):\n        content = b'1234'\n        body = BytesIO(content)\n        ref_chunks = []\n        for chunk in DownloadChunkIterator(body, 3):\n            ref_chunks.append(chunk)\n        self.assertEqual(ref_chunks, [b'123', b'4'])\n\n    def test_empty_content(self):\n        body = BytesIO(b'')\n        ref_chunks = []\n        for chunk in DownloadChunkIterator(body, 3):\n            ref_chunks.append(chunk)\n        self.assertEqual(ref_chunks, [b''])\n\n\nclass TestDeferQueue(unittest.TestCase):\n    def setUp(self):\n        self.q = DeferQueue()\n\n    def test_no_writes_when_not_lowest_block(self):\n        writes = self.q.request_writes(offset=1, data='bar')\n        self.assertEqual(writes, [])\n\n    def test_writes_returned_in_order(self):\n        self.assertEqual(self.q.request_writes(offset=3, data='d'), [])\n        self.assertEqual(self.q.request_writes(offset=2, data='c'), [])\n        self.assertEqual(self.q.request_writes(offset=1, data='b'), [])\n\n        # Everything at this point has been deferred, but as soon as we\n        # send offset=0, that will unlock offsets 0-3.\n        writes = self.q.request_writes(offset=0, data='a')\n        self.assertEqual(\n            writes,\n            [\n                {'offset': 0, 'data': 'a'},\n                {'offset': 1, 'data': 'b'},\n                {'offset': 2, 'data': 'c'},\n                {'offset': 3, 'data': 'd'},\n            ],\n        )\n\n    def test_unlocks_partial_range(self):\n        self.assertEqual(self.q.request_writes(offset=5, data='f'), [])\n        self.assertEqual(self.q.request_writes(offset=1, data='b'), [])\n\n        # offset=0 unlocks 0-1, but offset=5 still needs to see 2-4 first.\n        writes = self.q.request_writes(offset=0, data='a')\n        self.assertEqual(\n            writes,\n            [\n                {'offset': 0, 'data': 'a'},\n                {'offset': 1, 'data': 'b'},\n            ],\n        )\n\n    def test_data_can_be_any_size(self):\n        self.q.request_writes(offset=5, data='hello world')\n        writes = self.q.request_writes(offset=0, data='abcde')\n        self.assertEqual(\n            writes,\n            [\n                {'offset': 0, 'data': 'abcde'},\n                {'offset': 5, 'data': 'hello world'},\n            ],\n        )\n\n    def test_data_queued_in_order(self):\n        # This immediately gets returned because offset=0 is the\n        # next range we're waiting on.\n        writes = self.q.request_writes(offset=0, data='hello world')\n        self.assertEqual(writes, [{'offset': 0, 'data': 'hello world'}])\n        # Same thing here but with offset\n        writes = self.q.request_writes(offset=11, data='hello again')\n        self.assertEqual(writes, [{'offset': 11, 'data': 'hello again'}])\n\n    def test_writes_below_min_offset_are_ignored(self):\n        self.q.request_writes(offset=0, data='a')\n        self.q.request_writes(offset=1, data='b')\n        self.q.request_writes(offset=2, data='c')\n\n        # At this point we're expecting offset=3, so if a write\n        # comes in below 3, we ignore it.\n        self.assertEqual(self.q.request_writes(offset=0, data='a'), [])\n        self.assertEqual(self.q.request_writes(offset=1, data='b'), [])\n\n        self.assertEqual(\n            self.q.request_writes(offset=3, data='d'),\n            [{'offset': 3, 'data': 'd'}],\n        )\n\n    def test_duplicate_writes_are_ignored(self):\n        self.q.request_writes(offset=2, data='c')\n        self.q.request_writes(offset=1, data='b')\n\n        # We're still waiting for offset=0, but if\n        # a duplicate write comes in for offset=2/offset=1\n        # it's ignored.  This gives \"first one wins\" behavior.\n        self.assertEqual(self.q.request_writes(offset=2, data='X'), [])\n        self.assertEqual(self.q.request_writes(offset=1, data='Y'), [])\n\n        self.assertEqual(\n            self.q.request_writes(offset=0, data='a'),\n            [\n                {'offset': 0, 'data': 'a'},\n                # Note we're seeing 'b' 'c', and not 'X', 'Y'.\n                {'offset': 1, 'data': 'b'},\n                {'offset': 2, 'data': 'c'},\n            ],\n        )\n", "s3transfer/delete.py": "# Copyright 2016 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nfrom s3transfer.tasks import SubmissionTask, Task\n\n\nclass DeleteSubmissionTask(SubmissionTask):\n    \"\"\"Task for submitting tasks to execute an object deletion.\"\"\"\n\n    def _submit(self, client, request_executor, transfer_future, **kwargs):\n        \"\"\"\n        :param client: The client associated with the transfer manager\n\n        :type config: s3transfer.manager.TransferConfig\n        :param config: The transfer config associated with the transfer\n            manager\n\n        :type osutil: s3transfer.utils.OSUtil\n        :param osutil: The os utility associated to the transfer manager\n\n        :type request_executor: s3transfer.futures.BoundedExecutor\n        :param request_executor: The request executor associated with the\n            transfer manager\n\n        :type transfer_future: s3transfer.futures.TransferFuture\n        :param transfer_future: The transfer future associated with the\n            transfer request that tasks are being submitted for\n        \"\"\"\n        call_args = transfer_future.meta.call_args\n\n        self._transfer_coordinator.submit(\n            request_executor,\n            DeleteObjectTask(\n                transfer_coordinator=self._transfer_coordinator,\n                main_kwargs={\n                    'client': client,\n                    'bucket': call_args.bucket,\n                    'key': call_args.key,\n                    'extra_args': call_args.extra_args,\n                },\n                is_final=True,\n            ),\n        )\n\n\nclass DeleteObjectTask(Task):\n    def _main(self, client, bucket, key, extra_args):\n        \"\"\"\n\n        :param client: The S3 client to use when calling DeleteObject\n\n        :type bucket: str\n        :param bucket: The name of the bucket.\n\n        :type key: str\n        :param key: The name of the object to delete.\n\n        :type extra_args: dict\n        :param extra_args: Extra arguments to pass to the DeleteObject call.\n\n        \"\"\"\n        client.delete_object(Bucket=bucket, Key=key, **extra_args)\n", "s3transfer/manager.py": "# Copyright 2016 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport copy\nimport logging\nimport re\nimport threading\n\nfrom s3transfer.bandwidth import BandwidthLimiter, LeakyBucket\nfrom s3transfer.constants import ALLOWED_DOWNLOAD_ARGS, KB, MB\nfrom s3transfer.copies import CopySubmissionTask\nfrom s3transfer.delete import DeleteSubmissionTask\nfrom s3transfer.download import DownloadSubmissionTask\nfrom s3transfer.exceptions import CancelledError, FatalError\nfrom s3transfer.futures import (\n    IN_MEMORY_DOWNLOAD_TAG,\n    IN_MEMORY_UPLOAD_TAG,\n    BoundedExecutor,\n    TransferCoordinator,\n    TransferFuture,\n    TransferMeta,\n)\nfrom s3transfer.upload import UploadSubmissionTask\nfrom s3transfer.utils import (\n    CallArgs,\n    OSUtils,\n    SlidingWindowSemaphore,\n    TaskSemaphore,\n    add_s3express_defaults,\n    get_callbacks,\n    signal_not_transferring,\n    signal_transferring,\n)\n\nlogger = logging.getLogger(__name__)\n\n\nclass TransferConfig:\n    def __init__(\n        self,\n        multipart_threshold=8 * MB,\n        multipart_chunksize=8 * MB,\n        max_request_concurrency=10,\n        max_submission_concurrency=5,\n        max_request_queue_size=1000,\n        max_submission_queue_size=1000,\n        max_io_queue_size=1000,\n        io_chunksize=256 * KB,\n        num_download_attempts=5,\n        max_in_memory_upload_chunks=10,\n        max_in_memory_download_chunks=10,\n        max_bandwidth=None,\n    ):\n        \"\"\"Configurations for the transfer manager\n\n        :param multipart_threshold: The threshold for which multipart\n            transfers occur.\n\n        :param max_request_concurrency: The maximum number of S3 API\n            transfer-related requests that can happen at a time.\n\n        :param max_submission_concurrency: The maximum number of threads\n            processing a call to a TransferManager method. Processing a\n            call usually entails determining which S3 API requests that need\n            to be enqueued, but does **not** entail making any of the\n            S3 API data transferring requests needed to perform the transfer.\n            The threads controlled by ``max_request_concurrency`` is\n            responsible for that.\n\n        :param multipart_chunksize: The size of each transfer if a request\n            becomes a multipart transfer.\n\n        :param max_request_queue_size: The maximum amount of S3 API requests\n            that can be queued at a time.\n\n        :param max_submission_queue_size: The maximum amount of\n            TransferManager method calls that can be queued at a time.\n\n        :param max_io_queue_size: The maximum amount of read parts that\n            can be queued to be written to disk per download. The default\n            size for each elementin this queue is 8 KB.\n\n        :param io_chunksize: The max size of each chunk in the io queue.\n            Currently, this is size used when reading from the downloaded\n            stream as well.\n\n        :param num_download_attempts: The number of download attempts that\n            will be tried upon errors with downloading an object in S3. Note\n            that these retries account for errors that occur when streaming\n            down the data from s3 (i.e. socket errors and read timeouts that\n            occur after receiving an OK response from s3).\n            Other retryable exceptions such as throttling errors and 5xx errors\n            are already retried by botocore (this default is 5). The\n            ``num_download_attempts`` does not take into account the\n            number of exceptions retried by botocore.\n\n        :param max_in_memory_upload_chunks: The number of chunks that can\n            be stored in memory at a time for all ongoing upload requests.\n            This pertains to chunks of data that need to be stored in memory\n            during an upload if the data is sourced from a file-like object.\n            The total maximum memory footprint due to a in-memory upload\n            chunks is roughly equal to:\n\n                max_in_memory_upload_chunks * multipart_chunksize\n                + max_submission_concurrency * multipart_chunksize\n\n            ``max_submission_concurrency`` has an affect on this value because\n            for each thread pulling data off of a file-like object, they may\n            be waiting with a single read chunk to be submitted for upload\n            because the ``max_in_memory_upload_chunks`` value has been reached\n            by the threads making the upload request.\n\n        :param max_in_memory_download_chunks: The number of chunks that can\n            be buffered in memory and **not** in the io queue at a time for all\n            ongoing download requests. This pertains specifically to file-like\n            objects that cannot be seeked. The total maximum memory footprint\n            due to a in-memory download chunks is roughly equal to:\n\n                max_in_memory_download_chunks * multipart_chunksize\n\n        :param max_bandwidth: The maximum bandwidth that will be consumed\n            in uploading and downloading file content. The value is in terms of\n            bytes per second.\n        \"\"\"\n        self.multipart_threshold = multipart_threshold\n        self.multipart_chunksize = multipart_chunksize\n        self.max_request_concurrency = max_request_concurrency\n        self.max_submission_concurrency = max_submission_concurrency\n        self.max_request_queue_size = max_request_queue_size\n        self.max_submission_queue_size = max_submission_queue_size\n        self.max_io_queue_size = max_io_queue_size\n        self.io_chunksize = io_chunksize\n        self.num_download_attempts = num_download_attempts\n        self.max_in_memory_upload_chunks = max_in_memory_upload_chunks\n        self.max_in_memory_download_chunks = max_in_memory_download_chunks\n        self.max_bandwidth = max_bandwidth\n        self._validate_attrs_are_nonzero()\n\n    def _validate_attrs_are_nonzero(self):\n        for attr, attr_val in self.__dict__.items():\n            if attr_val is not None and attr_val <= 0:\n                raise ValueError(\n                    'Provided parameter %s of value %s must be greater than '\n                    '0.' % (attr, attr_val)\n                )\n\n\nclass TransferManager:\n    ALLOWED_DOWNLOAD_ARGS = ALLOWED_DOWNLOAD_ARGS\n\n    ALLOWED_UPLOAD_ARGS = [\n        'ACL',\n        'CacheControl',\n        'ChecksumAlgorithm',\n        'ContentDisposition',\n        'ContentEncoding',\n        'ContentLanguage',\n        'ContentType',\n        'ExpectedBucketOwner',\n        'Expires',\n        'GrantFullControl',\n        'GrantRead',\n        'GrantReadACP',\n        'GrantWriteACP',\n        'Metadata',\n        'ObjectLockLegalHoldStatus',\n        'ObjectLockMode',\n        'ObjectLockRetainUntilDate',\n        'RequestPayer',\n        'ServerSideEncryption',\n        'StorageClass',\n        'SSECustomerAlgorithm',\n        'SSECustomerKey',\n        'SSECustomerKeyMD5',\n        'SSEKMSKeyId',\n        'SSEKMSEncryptionContext',\n        'Tagging',\n        'WebsiteRedirectLocation',\n    ]\n\n    ALLOWED_COPY_ARGS = ALLOWED_UPLOAD_ARGS + [\n        'CopySourceIfMatch',\n        'CopySourceIfModifiedSince',\n        'CopySourceIfNoneMatch',\n        'CopySourceIfUnmodifiedSince',\n        'CopySourceSSECustomerAlgorithm',\n        'CopySourceSSECustomerKey',\n        'CopySourceSSECustomerKeyMD5',\n        'MetadataDirective',\n        'TaggingDirective',\n    ]\n\n    ALLOWED_DELETE_ARGS = [\n        'MFA',\n        'VersionId',\n        'RequestPayer',\n        'ExpectedBucketOwner',\n    ]\n\n    VALIDATE_SUPPORTED_BUCKET_VALUES = True\n\n    _UNSUPPORTED_BUCKET_PATTERNS = {\n        'S3 Object Lambda': re.compile(\n            r'^arn:(aws).*:s3-object-lambda:[a-z\\-0-9]+:[0-9]{12}:'\n            r'accesspoint[/:][a-zA-Z0-9\\-]{1,63}'\n        ),\n    }\n\n    def __init__(self, client, config=None, osutil=None, executor_cls=None):\n        \"\"\"A transfer manager interface for Amazon S3\n\n        :param client: Client to be used by the manager\n        :param config: TransferConfig to associate specific configurations\n        :param osutil: OSUtils object to use for os-related behavior when\n            using with transfer manager.\n\n        :type executor_cls: s3transfer.futures.BaseExecutor\n        :param executor_cls: The class of executor to use with the transfer\n            manager. By default, concurrent.futures.ThreadPoolExecutor is used.\n        \"\"\"\n        self._client = client\n        self._config = config\n        if config is None:\n            self._config = TransferConfig()\n        self._osutil = osutil\n        if osutil is None:\n            self._osutil = OSUtils()\n        self._coordinator_controller = TransferCoordinatorController()\n        # A counter to create unique id's for each transfer submitted.\n        self._id_counter = 0\n\n        # The executor responsible for making S3 API transfer requests\n        self._request_executor = BoundedExecutor(\n            max_size=self._config.max_request_queue_size,\n            max_num_threads=self._config.max_request_concurrency,\n            tag_semaphores={\n                IN_MEMORY_UPLOAD_TAG: TaskSemaphore(\n                    self._config.max_in_memory_upload_chunks\n                ),\n                IN_MEMORY_DOWNLOAD_TAG: SlidingWindowSemaphore(\n                    self._config.max_in_memory_download_chunks\n                ),\n            },\n            executor_cls=executor_cls,\n        )\n\n        # The executor responsible for submitting the necessary tasks to\n        # perform the desired transfer\n        self._submission_executor = BoundedExecutor(\n            max_size=self._config.max_submission_queue_size,\n            max_num_threads=self._config.max_submission_concurrency,\n            executor_cls=executor_cls,\n        )\n\n        # There is one thread available for writing to disk. It will handle\n        # downloads for all files.\n        self._io_executor = BoundedExecutor(\n            max_size=self._config.max_io_queue_size,\n            max_num_threads=1,\n            executor_cls=executor_cls,\n        )\n\n        # The component responsible for limiting bandwidth usage if it\n        # is configured.\n        self._bandwidth_limiter = None\n        if self._config.max_bandwidth is not None:\n            logger.debug(\n                'Setting max_bandwidth to %s', self._config.max_bandwidth\n            )\n            leaky_bucket = LeakyBucket(self._config.max_bandwidth)\n            self._bandwidth_limiter = BandwidthLimiter(leaky_bucket)\n\n        self._register_handlers()\n\n    @property\n    def client(self):\n        return self._client\n\n    @property\n    def config(self):\n        return self._config\n\n    def upload(self, fileobj, bucket, key, extra_args=None, subscribers=None):\n        \"\"\"Uploads a file to S3\n\n        :type fileobj: str or seekable file-like object\n        :param fileobj: The name of a file to upload or a seekable file-like\n            object to upload. It is recommended to use a filename because\n            file-like objects may result in higher memory usage.\n\n        :type bucket: str\n        :param bucket: The name of the bucket to upload to\n\n        :type key: str\n        :param key: The name of the key to upload to\n\n        :type extra_args: dict\n        :param extra_args: Extra arguments that may be passed to the\n            client operation\n\n        :type subscribers: list(s3transfer.subscribers.BaseSubscriber)\n        :param subscribers: The list of subscribers to be invoked in the\n            order provided based on the event emit during the process of\n            the transfer request.\n\n        :rtype: s3transfer.futures.TransferFuture\n        :returns: Transfer future representing the upload\n        \"\"\"\n        if extra_args is None:\n            extra_args = {}\n        if subscribers is None:\n            subscribers = []\n        self._validate_all_known_args(extra_args, self.ALLOWED_UPLOAD_ARGS)\n        self._validate_if_bucket_supported(bucket)\n        self._add_operation_defaults(bucket, extra_args)\n        call_args = CallArgs(\n            fileobj=fileobj,\n            bucket=bucket,\n            key=key,\n            extra_args=extra_args,\n            subscribers=subscribers,\n        )\n        extra_main_kwargs = {}\n        if self._bandwidth_limiter:\n            extra_main_kwargs['bandwidth_limiter'] = self._bandwidth_limiter\n        return self._submit_transfer(\n            call_args, UploadSubmissionTask, extra_main_kwargs\n        )\n\n    def download(\n        self, bucket, key, fileobj, extra_args=None, subscribers=None\n    ):\n        \"\"\"Downloads a file from S3\n\n        :type bucket: str\n        :param bucket: The name of the bucket to download from\n\n        :type key: str\n        :param key: The name of the key to download from\n\n        :type fileobj: str or seekable file-like object\n        :param fileobj: The name of a file to download or a seekable file-like\n            object to download. It is recommended to use a filename because\n            file-like objects may result in higher memory usage.\n\n        :type extra_args: dict\n        :param extra_args: Extra arguments that may be passed to the\n            client operation\n\n        :type subscribers: list(s3transfer.subscribers.BaseSubscriber)\n        :param subscribers: The list of subscribers to be invoked in the\n            order provided based on the event emit during the process of\n            the transfer request.\n\n        :rtype: s3transfer.futures.TransferFuture\n        :returns: Transfer future representing the download\n        \"\"\"\n        if extra_args is None:\n            extra_args = {}\n        if subscribers is None:\n            subscribers = []\n        self._validate_all_known_args(extra_args, self.ALLOWED_DOWNLOAD_ARGS)\n        self._validate_if_bucket_supported(bucket)\n        call_args = CallArgs(\n            bucket=bucket,\n            key=key,\n            fileobj=fileobj,\n            extra_args=extra_args,\n            subscribers=subscribers,\n        )\n        extra_main_kwargs = {'io_executor': self._io_executor}\n        if self._bandwidth_limiter:\n            extra_main_kwargs['bandwidth_limiter'] = self._bandwidth_limiter\n        return self._submit_transfer(\n            call_args, DownloadSubmissionTask, extra_main_kwargs\n        )\n\n    def copy(\n        self,\n        copy_source,\n        bucket,\n        key,\n        extra_args=None,\n        subscribers=None,\n        source_client=None,\n    ):\n        \"\"\"Copies a file in S3\n\n        :type copy_source: dict\n        :param copy_source: The name of the source bucket, key name of the\n            source object, and optional version ID of the source object. The\n            dictionary format is:\n            ``{'Bucket': 'bucket', 'Key': 'key', 'VersionId': 'id'}``. Note\n            that the ``VersionId`` key is optional and may be omitted.\n\n        :type bucket: str\n        :param bucket: The name of the bucket to copy to\n\n        :type key: str\n        :param key: The name of the key to copy to\n\n        :type extra_args: dict\n        :param extra_args: Extra arguments that may be passed to the\n            client operation\n\n        :type subscribers: a list of subscribers\n        :param subscribers: The list of subscribers to be invoked in the\n            order provided based on the event emit during the process of\n            the transfer request.\n\n        :type source_client: botocore or boto3 Client\n        :param source_client: The client to be used for operation that\n            may happen at the source object. For example, this client is\n            used for the head_object that determines the size of the copy.\n            If no client is provided, the transfer manager's client is used\n            as the client for the source object.\n\n        :rtype: s3transfer.futures.TransferFuture\n        :returns: Transfer future representing the copy\n        \"\"\"\n        if extra_args is None:\n            extra_args = {}\n        if subscribers is None:\n            subscribers = []\n        if source_client is None:\n            source_client = self._client\n        self._validate_all_known_args(extra_args, self.ALLOWED_COPY_ARGS)\n        if isinstance(copy_source, dict):\n            self._validate_if_bucket_supported(copy_source.get('Bucket'))\n        self._validate_if_bucket_supported(bucket)\n        call_args = CallArgs(\n            copy_source=copy_source,\n            bucket=bucket,\n            key=key,\n            extra_args=extra_args,\n            subscribers=subscribers,\n            source_client=source_client,\n        )\n        return self._submit_transfer(call_args, CopySubmissionTask)\n\n    def delete(self, bucket, key, extra_args=None, subscribers=None):\n        \"\"\"Delete an S3 object.\n\n        :type bucket: str\n        :param bucket: The name of the bucket.\n\n        :type key: str\n        :param key: The name of the S3 object to delete.\n\n        :type extra_args: dict\n        :param extra_args: Extra arguments that may be passed to the\n            DeleteObject call.\n\n        :type subscribers: list\n        :param subscribers: A list of subscribers to be invoked during the\n            process of the transfer request.  Note that the ``on_progress``\n            callback is not invoked during object deletion.\n\n        :rtype: s3transfer.futures.TransferFuture\n        :return: Transfer future representing the deletion.\n\n        \"\"\"\n        if extra_args is None:\n            extra_args = {}\n        if subscribers is None:\n            subscribers = []\n        self._validate_all_known_args(extra_args, self.ALLOWED_DELETE_ARGS)\n        self._validate_if_bucket_supported(bucket)\n        call_args = CallArgs(\n            bucket=bucket,\n            key=key,\n            extra_args=extra_args,\n            subscribers=subscribers,\n        )\n        return self._submit_transfer(call_args, DeleteSubmissionTask)\n\n    def _validate_if_bucket_supported(self, bucket):\n        # s3 high level operations don't support some resources\n        # (eg. S3 Object Lambda) only direct API calls are available\n        # for such resources\n        if self.VALIDATE_SUPPORTED_BUCKET_VALUES:\n            for resource, pattern in self._UNSUPPORTED_BUCKET_PATTERNS.items():\n                match = pattern.match(bucket)\n                if match:\n                    raise ValueError(\n                        'TransferManager methods do not support %s '\n                        'resource. Use direct client calls instead.' % resource\n                    )\n\n    def _validate_all_known_args(self, actual, allowed):\n        for kwarg in actual:\n            if kwarg not in allowed:\n                raise ValueError(\n                    \"Invalid extra_args key '%s', \"\n                    \"must be one of: %s\" % (kwarg, ', '.join(allowed))\n                )\n\n    def _add_operation_defaults(self, bucket, extra_args):\n        add_s3express_defaults(bucket, extra_args)\n\n    def _submit_transfer(\n        self, call_args, submission_task_cls, extra_main_kwargs=None\n    ):\n        if not extra_main_kwargs:\n            extra_main_kwargs = {}\n\n        # Create a TransferFuture to return back to the user\n        transfer_future, components = self._get_future_with_components(\n            call_args\n        )\n\n        # Add any provided done callbacks to the created transfer future\n        # to be invoked on the transfer future being complete.\n        for callback in get_callbacks(transfer_future, 'done'):\n            components['coordinator'].add_done_callback(callback)\n\n        # Get the main kwargs needed to instantiate the submission task\n        main_kwargs = self._get_submission_task_main_kwargs(\n            transfer_future, extra_main_kwargs\n        )\n\n        # Submit a SubmissionTask that will submit all of the necessary\n        # tasks needed to complete the S3 transfer.\n        self._submission_executor.submit(\n            submission_task_cls(\n                transfer_coordinator=components['coordinator'],\n                main_kwargs=main_kwargs,\n            )\n        )\n\n        # Increment the unique id counter for future transfer requests\n        self._id_counter += 1\n\n        return transfer_future\n\n    def _get_future_with_components(self, call_args):\n        transfer_id = self._id_counter\n        # Creates a new transfer future along with its components\n        transfer_coordinator = TransferCoordinator(transfer_id=transfer_id)\n        # Track the transfer coordinator for transfers to manage.\n        self._coordinator_controller.add_transfer_coordinator(\n            transfer_coordinator\n        )\n        # Also make sure that the transfer coordinator is removed once\n        # the transfer completes so it does not stick around in memory.\n        transfer_coordinator.add_done_callback(\n            self._coordinator_controller.remove_transfer_coordinator,\n            transfer_coordinator,\n        )\n        components = {\n            'meta': TransferMeta(call_args, transfer_id=transfer_id),\n            'coordinator': transfer_coordinator,\n        }\n        transfer_future = TransferFuture(**components)\n        return transfer_future, components\n\n    def _get_submission_task_main_kwargs(\n        self, transfer_future, extra_main_kwargs\n    ):\n        main_kwargs = {\n            'client': self._client,\n            'config': self._config,\n            'osutil': self._osutil,\n            'request_executor': self._request_executor,\n            'transfer_future': transfer_future,\n        }\n        main_kwargs.update(extra_main_kwargs)\n        return main_kwargs\n\n    def _register_handlers(self):\n        # Register handlers to enable/disable callbacks on uploads.\n        event_name = 'request-created.s3'\n        self._client.meta.events.register_first(\n            event_name,\n            signal_not_transferring,\n            unique_id='s3upload-not-transferring',\n        )\n        self._client.meta.events.register_last(\n            event_name, signal_transferring, unique_id='s3upload-transferring'\n        )\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_value, *args):\n        cancel = False\n        cancel_msg = ''\n        cancel_exc_type = FatalError\n        # If a exception was raised in the context handler, signal to cancel\n        # all of the inprogress futures in the shutdown.\n        if exc_type:\n            cancel = True\n            cancel_msg = str(exc_value)\n            if not cancel_msg:\n                cancel_msg = repr(exc_value)\n            # If it was a KeyboardInterrupt, the cancellation was initiated\n            # by the user.\n            if isinstance(exc_value, KeyboardInterrupt):\n                cancel_exc_type = CancelledError\n        self._shutdown(cancel, cancel_msg, cancel_exc_type)\n\n    def shutdown(self, cancel=False, cancel_msg=''):\n        \"\"\"Shutdown the TransferManager\n\n        It will wait till all transfers complete before it completely shuts\n        down.\n\n        :type cancel: boolean\n        :param cancel: If True, calls TransferFuture.cancel() for\n            all in-progress in transfers. This is useful if you want the\n            shutdown to happen quicker.\n\n        :type cancel_msg: str\n        :param cancel_msg: The message to specify if canceling all in-progress\n            transfers.\n        \"\"\"\n        self._shutdown(cancel, cancel, cancel_msg)\n\n    def _shutdown(self, cancel, cancel_msg, exc_type=CancelledError):\n        if cancel:\n            # Cancel all in-flight transfers if requested, before waiting\n            # for them to complete.\n            self._coordinator_controller.cancel(cancel_msg, exc_type)\n        try:\n            # Wait until there are no more in-progress transfers. This is\n            # wrapped in a try statement because this can be interrupted\n            # with a KeyboardInterrupt that needs to be caught.\n            self._coordinator_controller.wait()\n        except KeyboardInterrupt:\n            # If not errors were raised in the try block, the cancel should\n            # have no coordinators it needs to run cancel on. If there was\n            # an error raised in the try statement we want to cancel all of\n            # the inflight transfers before shutting down to speed that\n            # process up.\n            self._coordinator_controller.cancel('KeyboardInterrupt()')\n            raise\n        finally:\n            # Shutdown all of the executors.\n            self._submission_executor.shutdown()\n            self._request_executor.shutdown()\n            self._io_executor.shutdown()\n\n\nclass TransferCoordinatorController:\n    def __init__(self):\n        \"\"\"Abstraction to control all transfer coordinators\n\n        This abstraction allows the manager to wait for inprogress transfers\n        to complete and cancel all inprogress transfers.\n        \"\"\"\n        self._lock = threading.Lock()\n        self._tracked_transfer_coordinators = set()\n\n    @property\n    def tracked_transfer_coordinators(self):\n        \"\"\"The set of transfer coordinators being tracked\"\"\"\n        with self._lock:\n            # We return a copy because the set is mutable and if you were to\n            # iterate over the set, it may be changing in length due to\n            # additions and removals of transfer coordinators.\n            return copy.copy(self._tracked_transfer_coordinators)\n\n    def add_transfer_coordinator(self, transfer_coordinator):\n        \"\"\"Adds a transfer coordinator of a transfer to be canceled if needed\n\n        :type transfer_coordinator: s3transfer.futures.TransferCoordinator\n        :param transfer_coordinator: The transfer coordinator for the\n            particular transfer\n        \"\"\"\n        with self._lock:\n            self._tracked_transfer_coordinators.add(transfer_coordinator)\n\n    def remove_transfer_coordinator(self, transfer_coordinator):\n        \"\"\"Remove a transfer coordinator from cancellation consideration\n\n        Typically, this method is invoked by the transfer coordinator itself\n        to remove its self when it completes its transfer.\n\n        :type transfer_coordinator: s3transfer.futures.TransferCoordinator\n        :param transfer_coordinator: The transfer coordinator for the\n            particular transfer\n        \"\"\"\n        with self._lock:\n            self._tracked_transfer_coordinators.remove(transfer_coordinator)\n\n    def cancel(self, msg='', exc_type=CancelledError):\n        \"\"\"Cancels all inprogress transfers\n\n        This cancels the inprogress transfers by calling cancel() on all\n        tracked transfer coordinators.\n\n        :param msg: The message to pass on to each transfer coordinator that\n            gets cancelled.\n\n        :param exc_type: The type of exception to set for the cancellation\n        \"\"\"\n        for transfer_coordinator in self.tracked_transfer_coordinators:\n            transfer_coordinator.cancel(msg, exc_type)\n\n    def wait(self):\n        \"\"\"Wait until there are no more inprogress transfers\n\n        This will not stop when failures are encountered and not propagate any\n        of these errors from failed transfers, but it can be interrupted with\n        a KeyboardInterrupt.\n        \"\"\"\n        try:\n            transfer_coordinator = None\n            for transfer_coordinator in self.tracked_transfer_coordinators:\n                transfer_coordinator.result()\n        except KeyboardInterrupt:\n            logger.debug('Received KeyboardInterrupt in wait()')\n            # If Keyboard interrupt is raised while waiting for\n            # the result, then exit out of the wait and raise the\n            # exception\n            if transfer_coordinator:\n                logger.debug(\n                    'On KeyboardInterrupt was waiting for %s',\n                    transfer_coordinator,\n                )\n            raise\n        except Exception:\n            # A general exception could have been thrown because\n            # of result(). We just want to ignore this and continue\n            # because we at least know that the transfer coordinator\n            # has completed.\n            pass\n", "s3transfer/exceptions.py": "# Copyright 2016 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nfrom concurrent.futures import CancelledError\n\n\nclass RetriesExceededError(Exception):\n    def __init__(self, last_exception, msg='Max Retries Exceeded'):\n        super().__init__(msg)\n        self.last_exception = last_exception\n\n\nclass S3UploadFailedError(Exception):\n    pass\n\n\nclass InvalidSubscriberMethodError(Exception):\n    pass\n\n\nclass TransferNotDoneError(Exception):\n    pass\n\n\nclass FatalError(CancelledError):\n    \"\"\"A CancelledError raised from an error in the TransferManager\"\"\"\n\n    pass\n", "s3transfer/subscribers.py": "# Copyright 2016 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nfrom functools import lru_cache\n\nfrom s3transfer.compat import accepts_kwargs\nfrom s3transfer.exceptions import InvalidSubscriberMethodError\n\n\nclass BaseSubscriber:\n    \"\"\"The base subscriber class\n\n    It is recommended that all subscriber implementations subclass and then\n    override the subscription methods (i.e. on_{subsribe_type}() methods).\n    \"\"\"\n\n    VALID_SUBSCRIBER_TYPES = ['queued', 'progress', 'done']\n\n    def __new__(cls, *args, **kwargs):\n        cls._validate_subscriber_methods()\n        return super().__new__(cls)\n\n    @classmethod\n    @lru_cache()\n    def _validate_subscriber_methods(cls):\n        for subscriber_type in cls.VALID_SUBSCRIBER_TYPES:\n            subscriber_method = getattr(cls, 'on_' + subscriber_type)\n            if not callable(subscriber_method):\n                raise InvalidSubscriberMethodError(\n                    'Subscriber method %s must be callable.'\n                    % subscriber_method\n                )\n\n            if not accepts_kwargs(subscriber_method):\n                raise InvalidSubscriberMethodError(\n                    'Subscriber method %s must accept keyword '\n                    'arguments (**kwargs)' % subscriber_method\n                )\n\n    def on_queued(self, future, **kwargs):\n        \"\"\"Callback to be invoked when transfer request gets queued\n\n        This callback can be useful for:\n\n            * Keeping track of how many transfers have been requested\n            * Providing the expected transfer size through\n              future.meta.provide_transfer_size() so a HeadObject would not\n              need to be made for copies and downloads.\n\n        :type future: s3transfer.futures.TransferFuture\n        :param future: The TransferFuture representing the requested transfer.\n        \"\"\"\n        pass\n\n    def on_progress(self, future, bytes_transferred, **kwargs):\n        \"\"\"Callback to be invoked when progress is made on transfer\n\n        This callback can be useful for:\n\n            * Recording and displaying progress\n\n        :type future: s3transfer.futures.TransferFuture\n        :param future: The TransferFuture representing the requested transfer.\n\n        :type bytes_transferred: int\n        :param bytes_transferred: The number of bytes transferred for that\n            invocation of the callback. Note that a negative amount can be\n            provided, which usually indicates that an in-progress request\n            needed to be retried and thus progress was rewound.\n        \"\"\"\n        pass\n\n    def on_done(self, future, **kwargs):\n        \"\"\"Callback to be invoked once a transfer is done\n\n        This callback can be useful for:\n\n            * Recording and displaying whether the transfer succeeded or\n              failed using future.result()\n            * Running some task after the transfer completed like changing\n              the last modified time of a downloaded file.\n\n        :type future: s3transfer.futures.TransferFuture\n        :param future: The TransferFuture representing the requested transfer.\n        \"\"\"\n        pass\n", "s3transfer/utils.py": "# Copyright 2016 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport functools\nimport logging\nimport math\nimport os\nimport random\nimport socket\nimport stat\nimport string\nimport threading\nfrom collections import defaultdict\n\nfrom botocore.exceptions import (\n    IncompleteReadError,\n    ReadTimeoutError,\n    ResponseStreamingError,\n)\nfrom botocore.httpchecksum import AwsChunkedWrapper\nfrom botocore.utils import is_s3express_bucket\n\nfrom s3transfer.compat import SOCKET_ERROR, fallocate, rename_file\n\nMAX_PARTS = 10000\n# The maximum file size you can upload via S3 per request.\n# See: http://docs.aws.amazon.com/AmazonS3/latest/dev/UploadingObjects.html\n# and: http://docs.aws.amazon.com/AmazonS3/latest/dev/qfacts.html\nMAX_SINGLE_UPLOAD_SIZE = 5 * (1024**3)\nMIN_UPLOAD_CHUNKSIZE = 5 * (1024**2)\nlogger = logging.getLogger(__name__)\n\n\nS3_RETRYABLE_DOWNLOAD_ERRORS = (\n    socket.timeout,\n    SOCKET_ERROR,\n    ReadTimeoutError,\n    IncompleteReadError,\n    ResponseStreamingError,\n)\n\n\ndef random_file_extension(num_digits=8):\n    return ''.join(random.choice(string.hexdigits) for _ in range(num_digits))\n\n\ndef signal_not_transferring(request, operation_name, **kwargs):\n    if operation_name in ['PutObject', 'UploadPart'] and hasattr(\n        request.body, 'signal_not_transferring'\n    ):\n        request.body.signal_not_transferring()\n\n\ndef signal_transferring(request, operation_name, **kwargs):\n    if operation_name in ['PutObject', 'UploadPart']:\n        body = request.body\n        if isinstance(body, AwsChunkedWrapper):\n            body = getattr(body, '_raw', None)\n        if hasattr(body, 'signal_transferring'):\n            body.signal_transferring()\n\n\ndef calculate_num_parts(size, part_size):\n    return int(math.ceil(size / float(part_size)))\n\n\ndef calculate_range_parameter(\n    part_size, part_index, num_parts, total_size=None\n):\n    \"\"\"Calculate the range parameter for multipart downloads/copies\n\n    :type part_size: int\n    :param part_size: The size of the part\n\n    :type part_index: int\n    :param part_index: The index for which this parts starts. This index starts\n        at zero\n\n    :type num_parts: int\n    :param num_parts: The total number of parts in the transfer\n\n    :returns: The value to use for Range parameter on downloads or\n        the CopySourceRange parameter for copies\n    \"\"\"\n    # Used to calculate the Range parameter\n    start_range = part_index * part_size\n    if part_index == num_parts - 1:\n        end_range = ''\n        if total_size is not None:\n            end_range = str(total_size - 1)\n    else:\n        end_range = start_range + part_size - 1\n    range_param = f'bytes={start_range}-{end_range}'\n    return range_param\n\n\ndef get_callbacks(transfer_future, callback_type):\n    \"\"\"Retrieves callbacks from a subscriber\n\n    :type transfer_future: s3transfer.futures.TransferFuture\n    :param transfer_future: The transfer future the subscriber is associated\n        to.\n\n    :type callback_type: str\n    :param callback_type: The type of callback to retrieve from the subscriber.\n        Valid types include:\n            * 'queued'\n            * 'progress'\n            * 'done'\n\n    :returns: A list of callbacks for the type specified. All callbacks are\n        preinjected with the transfer future.\n    \"\"\"\n    callbacks = []\n    for subscriber in transfer_future.meta.call_args.subscribers:\n        callback_name = 'on_' + callback_type\n        if hasattr(subscriber, callback_name):\n            callbacks.append(\n                functools.partial(\n                    getattr(subscriber, callback_name), future=transfer_future\n                )\n            )\n    return callbacks\n\n\ndef invoke_progress_callbacks(callbacks, bytes_transferred):\n    \"\"\"Calls all progress callbacks\n\n    :param callbacks: A list of progress callbacks to invoke\n    :param bytes_transferred: The number of bytes transferred. This is passed\n        to the callbacks. If no bytes were transferred the callbacks will not\n        be invoked because no progress was achieved. It is also possible\n        to receive a negative amount which comes from retrying a transfer\n        request.\n    \"\"\"\n    # Only invoke the callbacks if bytes were actually transferred.\n    if bytes_transferred:\n        for callback in callbacks:\n            callback(bytes_transferred=bytes_transferred)\n\n\ndef get_filtered_dict(original_dict, whitelisted_keys):\n    \"\"\"Gets a dictionary filtered by whitelisted keys\n\n    :param original_dict: The original dictionary of arguments to source keys\n        and values.\n    :param whitelisted_key: A list of keys to include in the filtered\n        dictionary.\n\n    :returns: A dictionary containing key/values from the original dictionary\n        whose key was included in the whitelist\n    \"\"\"\n    filtered_dict = {}\n    for key, value in original_dict.items():\n        if key in whitelisted_keys:\n            filtered_dict[key] = value\n    return filtered_dict\n\n\nclass CallArgs:\n    def __init__(self, **kwargs):\n        \"\"\"A class that records call arguments\n\n        The call arguments must be passed as keyword arguments. It will set\n        each keyword argument as an attribute of the object along with its\n        associated value.\n        \"\"\"\n        for arg, value in kwargs.items():\n            setattr(self, arg, value)\n\n\nclass FunctionContainer:\n    \"\"\"An object that contains a function and any args or kwargs to call it\n\n    When called the provided function will be called with provided args\n    and kwargs.\n    \"\"\"\n\n    def __init__(self, func, *args, **kwargs):\n        self._func = func\n        self._args = args\n        self._kwargs = kwargs\n\n    def __repr__(self):\n        return 'Function: {} with args {} and kwargs {}'.format(\n            self._func, self._args, self._kwargs\n        )\n\n    def __call__(self):\n        return self._func(*self._args, **self._kwargs)\n\n\nclass CountCallbackInvoker:\n    \"\"\"An abstraction to invoke a callback when a shared count reaches zero\n\n    :param callback: Callback invoke when finalized count reaches zero\n    \"\"\"\n\n    def __init__(self, callback):\n        self._lock = threading.Lock()\n        self._callback = callback\n        self._count = 0\n        self._is_finalized = False\n\n    @property\n    def current_count(self):\n        with self._lock:\n            return self._count\n\n    def increment(self):\n        \"\"\"Increment the count by one\"\"\"\n        with self._lock:\n            if self._is_finalized:\n                raise RuntimeError(\n                    'Counter has been finalized it can no longer be '\n                    'incremented.'\n                )\n            self._count += 1\n\n    def decrement(self):\n        \"\"\"Decrement the count by one\"\"\"\n        with self._lock:\n            if self._count == 0:\n                raise RuntimeError(\n                    'Counter is at zero. It cannot dip below zero'\n                )\n            self._count -= 1\n            if self._is_finalized and self._count == 0:\n                self._callback()\n\n    def finalize(self):\n        \"\"\"Finalize the counter\n\n        Once finalized, the counter never be incremented and the callback\n        can be invoked once the count reaches zero\n        \"\"\"\n        with self._lock:\n            self._is_finalized = True\n            if self._count == 0:\n                self._callback()\n\n\nclass OSUtils:\n    _MAX_FILENAME_LEN = 255\n\n    def get_file_size(self, filename):\n        return os.path.getsize(filename)\n\n    def open_file_chunk_reader(self, filename, start_byte, size, callbacks):\n        return ReadFileChunk.from_filename(\n            filename, start_byte, size, callbacks, enable_callbacks=False\n        )\n\n    def open_file_chunk_reader_from_fileobj(\n        self,\n        fileobj,\n        chunk_size,\n        full_file_size,\n        callbacks,\n        close_callbacks=None,\n    ):\n        return ReadFileChunk(\n            fileobj,\n            chunk_size,\n            full_file_size,\n            callbacks=callbacks,\n            enable_callbacks=False,\n            close_callbacks=close_callbacks,\n        )\n\n    def open(self, filename, mode):\n        return open(filename, mode)\n\n    def remove_file(self, filename):\n        \"\"\"Remove a file, noop if file does not exist.\"\"\"\n        # Unlike os.remove, if the file does not exist,\n        # then this method does nothing.\n        try:\n            os.remove(filename)\n        except OSError:\n            pass\n\n    def rename_file(self, current_filename, new_filename):\n        rename_file(current_filename, new_filename)\n\n    def is_special_file(cls, filename):\n        \"\"\"Checks to see if a file is a special UNIX file.\n\n        It checks if the file is a character special device, block special\n        device, FIFO, or socket.\n\n        :param filename: Name of the file\n\n        :returns: True if the file is a special file. False, if is not.\n        \"\"\"\n        # If it does not exist, it must be a new file so it cannot be\n        # a special file.\n        if not os.path.exists(filename):\n            return False\n        mode = os.stat(filename).st_mode\n        # Character special device.\n        if stat.S_ISCHR(mode):\n            return True\n        # Block special device\n        if stat.S_ISBLK(mode):\n            return True\n        # Named pipe / FIFO\n        if stat.S_ISFIFO(mode):\n            return True\n        # Socket.\n        if stat.S_ISSOCK(mode):\n            return True\n        return False\n\n    def get_temp_filename(self, filename):\n        suffix = os.extsep + random_file_extension()\n        path = os.path.dirname(filename)\n        name = os.path.basename(filename)\n        temp_filename = name[: self._MAX_FILENAME_LEN - len(suffix)] + suffix\n        return os.path.join(path, temp_filename)\n\n    def allocate(self, filename, size):\n        try:\n            with self.open(filename, 'wb') as f:\n                fallocate(f, size)\n        except OSError:\n            self.remove_file(filename)\n            raise\n\n\nclass DeferredOpenFile:\n    def __init__(self, filename, start_byte=0, mode='rb', open_function=open):\n        \"\"\"A class that defers the opening of a file till needed\n\n        This is useful for deferring opening of a file till it is needed\n        in a separate thread, as there is a limit of how many open files\n        there can be in a single thread for most operating systems. The\n        file gets opened in the following methods: ``read()``, ``seek()``,\n        and ``__enter__()``\n\n        :type filename: str\n        :param filename: The name of the file to open\n\n        :type start_byte: int\n        :param start_byte: The byte to seek to when the file is opened.\n\n        :type mode: str\n        :param mode: The mode to use to open the file\n\n        :type open_function: function\n        :param open_function: The function to use to open the file\n        \"\"\"\n        self._filename = filename\n        self._fileobj = None\n        self._start_byte = start_byte\n        self._mode = mode\n        self._open_function = open_function\n\n    def _open_if_needed(self):\n        if self._fileobj is None:\n            self._fileobj = self._open_function(self._filename, self._mode)\n            if self._start_byte != 0:\n                self._fileobj.seek(self._start_byte)\n\n    @property\n    def name(self):\n        return self._filename\n\n    def read(self, amount=None):\n        self._open_if_needed()\n        return self._fileobj.read(amount)\n\n    def write(self, data):\n        self._open_if_needed()\n        self._fileobj.write(data)\n\n    def seek(self, where, whence=0):\n        self._open_if_needed()\n        self._fileobj.seek(where, whence)\n\n    def tell(self):\n        if self._fileobj is None:\n            return self._start_byte\n        return self._fileobj.tell()\n\n    def close(self):\n        if self._fileobj:\n            self._fileobj.close()\n\n    def __enter__(self):\n        self._open_if_needed()\n        return self\n\n    def __exit__(self, *args, **kwargs):\n        self.close()\n\n\nclass ReadFileChunk:\n    def __init__(\n        self,\n        fileobj,\n        chunk_size,\n        full_file_size,\n        callbacks=None,\n        enable_callbacks=True,\n        close_callbacks=None,\n    ):\n        \"\"\"\n\n        Given a file object shown below::\n\n            |___________________________________________________|\n            0          |                 |                 full_file_size\n                       |----chunk_size---|\n                    f.tell()\n\n        :type fileobj: file\n        :param fileobj: File like object\n\n        :type chunk_size: int\n        :param chunk_size: The max chunk size to read.  Trying to read\n            pass the end of the chunk size will behave like you've\n            reached the end of the file.\n\n        :type full_file_size: int\n        :param full_file_size: The entire content length associated\n            with ``fileobj``.\n\n        :type callbacks: A list of function(amount_read)\n        :param callbacks: Called whenever data is read from this object in the\n            order provided.\n\n        :type enable_callbacks: boolean\n        :param enable_callbacks: True if to run callbacks. Otherwise, do not\n            run callbacks\n\n        :type close_callbacks: A list of function()\n        :param close_callbacks: Called when close is called. The function\n            should take no arguments.\n        \"\"\"\n        self._fileobj = fileobj\n        self._start_byte = self._fileobj.tell()\n        self._size = self._calculate_file_size(\n            self._fileobj,\n            requested_size=chunk_size,\n            start_byte=self._start_byte,\n            actual_file_size=full_file_size,\n        )\n        # _amount_read represents the position in the chunk and may exceed\n        # the chunk size, but won't allow reads out of bounds.\n        self._amount_read = 0\n        self._callbacks = callbacks\n        if callbacks is None:\n            self._callbacks = []\n        self._callbacks_enabled = enable_callbacks\n        self._close_callbacks = close_callbacks\n        if close_callbacks is None:\n            self._close_callbacks = close_callbacks\n\n    @classmethod\n    def from_filename(\n        cls,\n        filename,\n        start_byte,\n        chunk_size,\n        callbacks=None,\n        enable_callbacks=True,\n    ):\n        \"\"\"Convenience factory function to create from a filename.\n\n        :type start_byte: int\n        :param start_byte: The first byte from which to start reading.\n\n        :type chunk_size: int\n        :param chunk_size: The max chunk size to read.  Trying to read\n            pass the end of the chunk size will behave like you've\n            reached the end of the file.\n\n        :type full_file_size: int\n        :param full_file_size: The entire content length associated\n            with ``fileobj``.\n\n        :type callbacks: function(amount_read)\n        :param callbacks: Called whenever data is read from this object.\n\n        :type enable_callbacks: bool\n        :param enable_callbacks: Indicate whether to invoke callback\n            during read() calls.\n\n        :rtype: ``ReadFileChunk``\n        :return: A new instance of ``ReadFileChunk``\n\n        \"\"\"\n        f = open(filename, 'rb')\n        f.seek(start_byte)\n        file_size = os.fstat(f.fileno()).st_size\n        return cls(f, chunk_size, file_size, callbacks, enable_callbacks)\n\n    def _calculate_file_size(\n        self, fileobj, requested_size, start_byte, actual_file_size\n    ):\n        max_chunk_size = actual_file_size - start_byte\n        return min(max_chunk_size, requested_size)\n\n    def read(self, amount=None):\n        amount_left = max(self._size - self._amount_read, 0)\n        if amount is None:\n            amount_to_read = amount_left\n        else:\n            amount_to_read = min(amount_left, amount)\n        data = self._fileobj.read(amount_to_read)\n        self._amount_read += len(data)\n        if self._callbacks is not None and self._callbacks_enabled:\n            invoke_progress_callbacks(self._callbacks, len(data))\n        return data\n\n    def signal_transferring(self):\n        self.enable_callback()\n        if hasattr(self._fileobj, 'signal_transferring'):\n            self._fileobj.signal_transferring()\n\n    def signal_not_transferring(self):\n        self.disable_callback()\n        if hasattr(self._fileobj, 'signal_not_transferring'):\n            self._fileobj.signal_not_transferring()\n\n    def enable_callback(self):\n        self._callbacks_enabled = True\n\n    def disable_callback(self):\n        self._callbacks_enabled = False\n\n    def seek(self, where, whence=0):\n        if whence not in (0, 1, 2):\n            # Mimic io's error for invalid whence values\n            raise ValueError(f\"invalid whence ({whence}, should be 0, 1 or 2)\")\n\n        # Recalculate where based on chunk attributes so seek from file\n        # start (whence=0) is always used\n        where += self._start_byte\n        if whence == 1:\n            where += self._amount_read\n        elif whence == 2:\n            where += self._size\n\n        self._fileobj.seek(max(where, self._start_byte))\n        if self._callbacks is not None and self._callbacks_enabled:\n            # To also rewind the callback() for an accurate progress report\n            bounded_where = max(min(where - self._start_byte, self._size), 0)\n            bounded_amount_read = min(self._amount_read, self._size)\n            amount = bounded_where - bounded_amount_read\n            invoke_progress_callbacks(\n                self._callbacks, bytes_transferred=amount\n            )\n        self._amount_read = max(where - self._start_byte, 0)\n\n    def close(self):\n        if self._close_callbacks is not None and self._callbacks_enabled:\n            for callback in self._close_callbacks:\n                callback()\n        self._fileobj.close()\n\n    def tell(self):\n        return self._amount_read\n\n    def __len__(self):\n        # __len__ is defined because requests will try to determine the length\n        # of the stream to set a content length.  In the normal case\n        # of the file it will just stat the file, but we need to change that\n        # behavior.  By providing a __len__, requests will use that instead\n        # of stat'ing the file.\n        return self._size\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, *args, **kwargs):\n        self.close()\n\n    def __iter__(self):\n        # This is a workaround for http://bugs.python.org/issue17575\n        # Basically httplib will try to iterate over the contents, even\n        # if its a file like object.  This wasn't noticed because we've\n        # already exhausted the stream so iterating over the file immediately\n        # stops, which is what we're simulating here.\n        return iter([])\n\n\nclass StreamReaderProgress:\n    \"\"\"Wrapper for a read only stream that adds progress callbacks.\"\"\"\n\n    def __init__(self, stream, callbacks=None):\n        self._stream = stream\n        self._callbacks = callbacks\n        if callbacks is None:\n            self._callbacks = []\n\n    def read(self, *args, **kwargs):\n        value = self._stream.read(*args, **kwargs)\n        invoke_progress_callbacks(self._callbacks, len(value))\n        return value\n\n\nclass NoResourcesAvailable(Exception):\n    pass\n\n\nclass TaskSemaphore:\n    def __init__(self, count):\n        \"\"\"A semaphore for the purpose of limiting the number of tasks\n\n        :param count: The size of semaphore\n        \"\"\"\n        self._semaphore = threading.Semaphore(count)\n\n    def acquire(self, tag, blocking=True):\n        \"\"\"Acquire the semaphore\n\n        :param tag: A tag identifying what is acquiring the semaphore. Note\n            that this is not really needed to directly use this class but is\n            needed for API compatibility with the SlidingWindowSemaphore\n            implementation.\n        :param block: If True, block until it can be acquired. If False,\n            do not block and raise an exception if cannot be acquired.\n\n        :returns: A token (can be None) to use when releasing the semaphore\n        \"\"\"\n        logger.debug(\"Acquiring %s\", tag)\n        if not self._semaphore.acquire(blocking):\n            raise NoResourcesAvailable(\"Cannot acquire tag '%s'\" % tag)\n\n    def release(self, tag, acquire_token):\n        \"\"\"Release the semaphore\n\n        :param tag: A tag identifying what is releasing the semaphore\n        :param acquire_token:  The token returned from when the semaphore was\n            acquired. Note that this is not really needed to directly use this\n            class but is needed for API compatibility with the\n            SlidingWindowSemaphore implementation.\n        \"\"\"\n        logger.debug(f\"Releasing acquire {tag}/{acquire_token}\")\n        self._semaphore.release()\n\n\nclass SlidingWindowSemaphore(TaskSemaphore):\n    \"\"\"A semaphore used to coordinate sequential resource access.\n\n    This class is similar to the stdlib BoundedSemaphore:\n\n    * It's initialized with a count.\n    * Each call to ``acquire()`` decrements the counter.\n    * If the count is at zero, then ``acquire()`` will either block until the\n      count increases, or if ``blocking=False``, then it will raise\n      a NoResourcesAvailable exception indicating that it failed to acquire the\n      semaphore.\n\n    The main difference is that this semaphore is used to limit\n    access to a resource that requires sequential access.  For example,\n    if I want to access resource R that has 20 subresources R_0 - R_19,\n    this semaphore can also enforce that you only have a max range of\n    10 at any given point in time.  You must also specify a tag name\n    when you acquire the semaphore.  The sliding window semantics apply\n    on a per tag basis.  The internal count will only be incremented\n    when the minimum sequence number for a tag is released.\n\n    \"\"\"\n\n    def __init__(self, count):\n        self._count = count\n        # Dict[tag, next_sequence_number].\n        self._tag_sequences = defaultdict(int)\n        self._lowest_sequence = {}\n        self._lock = threading.Lock()\n        self._condition = threading.Condition(self._lock)\n        # Dict[tag, List[sequence_number]]\n        self._pending_release = {}\n\n    def current_count(self):\n        with self._lock:\n            return self._count\n\n    def acquire(self, tag, blocking=True):\n        logger.debug(\"Acquiring %s\", tag)\n        self._condition.acquire()\n        try:\n            if self._count == 0:\n                if not blocking:\n                    raise NoResourcesAvailable(\"Cannot acquire tag '%s'\" % tag)\n                else:\n                    while self._count == 0:\n                        self._condition.wait()\n            # self._count is no longer zero.\n            # First, check if this is the first time we're seeing this tag.\n            sequence_number = self._tag_sequences[tag]\n            if sequence_number == 0:\n                # First time seeing the tag, so record we're at 0.\n                self._lowest_sequence[tag] = sequence_number\n            self._tag_sequences[tag] += 1\n            self._count -= 1\n            return sequence_number\n        finally:\n            self._condition.release()\n\n    def release(self, tag, acquire_token):\n        sequence_number = acquire_token\n        logger.debug(\"Releasing acquire %s/%s\", tag, sequence_number)\n        self._condition.acquire()\n        try:\n            if tag not in self._tag_sequences:\n                raise ValueError(\"Attempted to release unknown tag: %s\" % tag)\n            max_sequence = self._tag_sequences[tag]\n            if self._lowest_sequence[tag] == sequence_number:\n                # We can immediately process this request and free up\n                # resources.\n                self._lowest_sequence[tag] += 1\n                self._count += 1\n                self._condition.notify()\n                queued = self._pending_release.get(tag, [])\n                while queued:\n                    if self._lowest_sequence[tag] == queued[-1]:\n                        queued.pop()\n                        self._lowest_sequence[tag] += 1\n                        self._count += 1\n                    else:\n                        break\n            elif self._lowest_sequence[tag] < sequence_number < max_sequence:\n                # We can't do anything right now because we're still waiting\n                # for the min sequence for the tag to be released.  We have\n                # to queue this for pending release.\n                self._pending_release.setdefault(tag, []).append(\n                    sequence_number\n                )\n                self._pending_release[tag].sort(reverse=True)\n            else:\n                raise ValueError(\n                    \"Attempted to release unknown sequence number \"\n                    \"%s for tag: %s\" % (sequence_number, tag)\n                )\n        finally:\n            self._condition.release()\n\n\nclass ChunksizeAdjuster:\n    def __init__(\n        self,\n        max_size=MAX_SINGLE_UPLOAD_SIZE,\n        min_size=MIN_UPLOAD_CHUNKSIZE,\n        max_parts=MAX_PARTS,\n    ):\n        self.max_size = max_size\n        self.min_size = min_size\n        self.max_parts = max_parts\n\n    def adjust_chunksize(self, current_chunksize, file_size=None):\n        \"\"\"Get a chunksize close to current that fits within all S3 limits.\n\n        :type current_chunksize: int\n        :param current_chunksize: The currently configured chunksize.\n\n        :type file_size: int or None\n        :param file_size: The size of the file to upload. This might be None\n            if the object being transferred has an unknown size.\n\n        :returns: A valid chunksize that fits within configured limits.\n        \"\"\"\n        chunksize = current_chunksize\n        if file_size is not None:\n            chunksize = self._adjust_for_max_parts(chunksize, file_size)\n        return self._adjust_for_chunksize_limits(chunksize)\n\n    def _adjust_for_chunksize_limits(self, current_chunksize):\n        if current_chunksize > self.max_size:\n            logger.debug(\n                \"Chunksize greater than maximum chunksize. \"\n                \"Setting to %s from %s.\" % (self.max_size, current_chunksize)\n            )\n            return self.max_size\n        elif current_chunksize < self.min_size:\n            logger.debug(\n                \"Chunksize less than minimum chunksize. \"\n                \"Setting to %s from %s.\" % (self.min_size, current_chunksize)\n            )\n            return self.min_size\n        else:\n            return current_chunksize\n\n    def _adjust_for_max_parts(self, current_chunksize, file_size):\n        chunksize = current_chunksize\n        num_parts = int(math.ceil(file_size / float(chunksize)))\n\n        while num_parts > self.max_parts:\n            chunksize *= 2\n            num_parts = int(math.ceil(file_size / float(chunksize)))\n\n        if chunksize != current_chunksize:\n            logger.debug(\n                \"Chunksize would result in the number of parts exceeding the \"\n                \"maximum. Setting to %s from %s.\"\n                % (chunksize, current_chunksize)\n            )\n\n        return chunksize\n\n\ndef add_s3express_defaults(bucket, extra_args):\n    if is_s3express_bucket(bucket) and \"ChecksumAlgorithm\" not in extra_args:\n        # Default Transfer Operations to S3Express to use CRC32\n        extra_args[\"ChecksumAlgorithm\"] = \"crc32\"\n", "s3transfer/processpool.py": "# Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\"\"\"Speeds up S3 throughput by using processes\n\nGetting Started\n===============\n\nThe :class:`ProcessPoolDownloader` can be used to download a single file by\ncalling :meth:`ProcessPoolDownloader.download_file`:\n\n.. code:: python\n\n     from s3transfer.processpool import ProcessPoolDownloader\n\n     with ProcessPoolDownloader() as downloader:\n          downloader.download_file('mybucket', 'mykey', 'myfile')\n\n\nThis snippet downloads the S3 object located in the bucket ``mybucket`` at the\nkey ``mykey`` to the local file ``myfile``. Any errors encountered during the\ntransfer are not propagated. To determine if a transfer succeeded or\nfailed, use the `Futures`_ interface.\n\n\nThe :class:`ProcessPoolDownloader` can be used to download multiple files as\nwell:\n\n.. code:: python\n\n     from s3transfer.processpool import ProcessPoolDownloader\n\n     with ProcessPoolDownloader() as downloader:\n          downloader.download_file('mybucket', 'mykey', 'myfile')\n          downloader.download_file('mybucket', 'myotherkey', 'myotherfile')\n\n\nWhen running this snippet, the downloading of ``mykey`` and ``myotherkey``\nhappen in parallel. The first ``download_file`` call does not block the\nsecond ``download_file`` call. The snippet blocks when exiting\nthe context manager and blocks until both downloads are complete.\n\nAlternatively, the ``ProcessPoolDownloader`` can be instantiated\nand explicitly be shutdown using :meth:`ProcessPoolDownloader.shutdown`:\n\n.. code:: python\n\n     from s3transfer.processpool import ProcessPoolDownloader\n\n     downloader = ProcessPoolDownloader()\n     downloader.download_file('mybucket', 'mykey', 'myfile')\n     downloader.download_file('mybucket', 'myotherkey', 'myotherfile')\n     downloader.shutdown()\n\n\nFor this code snippet, the call to ``shutdown`` blocks until both\ndownloads are complete.\n\n\nAdditional Parameters\n=====================\n\nAdditional parameters can be provided to the ``download_file`` method:\n\n* ``extra_args``: A dictionary containing any additional client arguments\n  to include in the\n  `GetObject <https://botocore.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html#S3.Client.get_object>`_\n  API request. For example:\n\n  .. code:: python\n\n     from s3transfer.processpool import ProcessPoolDownloader\n\n     with ProcessPoolDownloader() as downloader:\n          downloader.download_file(\n               'mybucket', 'mykey', 'myfile',\n               extra_args={'VersionId': 'myversion'})\n\n\n* ``expected_size``: By default, the downloader will make a HeadObject\n  call to determine the size of the object. To opt-out of this additional\n  API call, you can provide the size of the object in bytes:\n\n  .. code:: python\n\n     from s3transfer.processpool import ProcessPoolDownloader\n\n     MB = 1024 * 1024\n     with ProcessPoolDownloader() as downloader:\n          downloader.download_file(\n               'mybucket', 'mykey', 'myfile', expected_size=2 * MB)\n\n\nFutures\n=======\n\nWhen ``download_file`` is called, it immediately returns a\n:class:`ProcessPoolTransferFuture`. The future can be used to poll the state\nof a particular transfer. To get the result of the download,\ncall :meth:`ProcessPoolTransferFuture.result`. The method blocks\nuntil the transfer completes, whether it succeeds or fails. For example:\n\n.. code:: python\n\n     from s3transfer.processpool import ProcessPoolDownloader\n\n     with ProcessPoolDownloader() as downloader:\n          future = downloader.download_file('mybucket', 'mykey', 'myfile')\n          print(future.result())\n\n\nIf the download succeeds, the future returns ``None``:\n\n.. code:: python\n\n     None\n\n\nIf the download fails, the exception causing the failure is raised. For\nexample, if ``mykey`` did not exist, the following error would be raised\n\n\n.. code:: python\n\n     botocore.exceptions.ClientError: An error occurred (404) when calling the HeadObject operation: Not Found\n\n\n.. note::\n\n    :meth:`ProcessPoolTransferFuture.result` can only be called while the\n    ``ProcessPoolDownloader`` is running (e.g. before calling ``shutdown`` or\n    inside the context manager).\n\n\nProcess Pool Configuration\n==========================\n\nBy default, the downloader has the following configuration options:\n\n* ``multipart_threshold``: The threshold size for performing ranged downloads\n  in bytes. By default, ranged downloads happen for S3 objects that are\n  greater than or equal to 8 MB in size.\n\n* ``multipart_chunksize``: The size of each ranged download in bytes. By\n  default, the size of each ranged download is 8 MB.\n\n* ``max_request_processes``: The maximum number of processes used to download\n  S3 objects. By default, the maximum is 10 processes.\n\n\nTo change the default configuration, use the :class:`ProcessTransferConfig`:\n\n.. code:: python\n\n     from s3transfer.processpool import ProcessPoolDownloader\n     from s3transfer.processpool import ProcessTransferConfig\n\n     config = ProcessTransferConfig(\n          multipart_threshold=64 * 1024 * 1024,  # 64 MB\n          max_request_processes=50\n     )\n     downloader = ProcessPoolDownloader(config=config)\n\n\nClient Configuration\n====================\n\nThe process pool downloader creates ``botocore`` clients on your behalf. In\norder to affect how the client is created, pass the keyword arguments\nthat would have been used in the :meth:`botocore.Session.create_client` call:\n\n.. code:: python\n\n\n     from s3transfer.processpool import ProcessPoolDownloader\n     from s3transfer.processpool import ProcessTransferConfig\n\n     downloader = ProcessPoolDownloader(\n          client_kwargs={'region_name': 'us-west-2'})\n\n\nThis snippet ensures that all clients created by the ``ProcessPoolDownloader``\nare using ``us-west-2`` as their region.\n\n\"\"\"\nimport collections\nimport contextlib\nimport logging\nimport multiprocessing\nimport signal\nimport threading\nfrom copy import deepcopy\n\nimport botocore.session\nfrom botocore.config import Config\n\nfrom s3transfer.compat import MAXINT, BaseManager\nfrom s3transfer.constants import ALLOWED_DOWNLOAD_ARGS, MB, PROCESS_USER_AGENT\nfrom s3transfer.exceptions import CancelledError, RetriesExceededError\nfrom s3transfer.futures import BaseTransferFuture, BaseTransferMeta\nfrom s3transfer.utils import (\n    S3_RETRYABLE_DOWNLOAD_ERRORS,\n    CallArgs,\n    OSUtils,\n    calculate_num_parts,\n    calculate_range_parameter,\n)\n\nlogger = logging.getLogger(__name__)\n\nSHUTDOWN_SIGNAL = 'SHUTDOWN'\n\n# The DownloadFileRequest tuple is submitted from the ProcessPoolDownloader\n# to the GetObjectSubmitter in order for the submitter to begin submitting\n# GetObjectJobs to the GetObjectWorkers.\nDownloadFileRequest = collections.namedtuple(\n    'DownloadFileRequest',\n    [\n        'transfer_id',  # The unique id for the transfer\n        'bucket',  # The bucket to download the object from\n        'key',  # The key to download the object from\n        'filename',  # The user-requested download location\n        'extra_args',  # Extra arguments to provide to client calls\n        'expected_size',  # The user-provided expected size of the download\n    ],\n)\n\n# The GetObjectJob tuple is submitted from the GetObjectSubmitter\n# to the GetObjectWorkers to download the file or parts of the file.\nGetObjectJob = collections.namedtuple(\n    'GetObjectJob',\n    [\n        'transfer_id',  # The unique id for the transfer\n        'bucket',  # The bucket to download the object from\n        'key',  # The key to download the object from\n        'temp_filename',  # The temporary file to write the content to via\n        # completed GetObject calls.\n        'extra_args',  # Extra arguments to provide to the GetObject call\n        'offset',  # The offset to write the content for the temp file.\n        'filename',  # The user-requested download location. The worker\n        # of final GetObjectJob will move the file located at\n        # temp_filename to the location of filename.\n    ],\n)\n\n\n@contextlib.contextmanager\ndef ignore_ctrl_c():\n    original_handler = _add_ignore_handler_for_interrupts()\n    yield\n    signal.signal(signal.SIGINT, original_handler)\n\n\ndef _add_ignore_handler_for_interrupts():\n    # Windows is unable to pickle signal.signal directly so it needs to\n    # be wrapped in a function defined at the module level\n    return signal.signal(signal.SIGINT, signal.SIG_IGN)\n\n\nclass ProcessTransferConfig:\n    def __init__(\n        self,\n        multipart_threshold=8 * MB,\n        multipart_chunksize=8 * MB,\n        max_request_processes=10,\n    ):\n        \"\"\"Configuration for the ProcessPoolDownloader\n\n        :param multipart_threshold: The threshold for which ranged downloads\n            occur.\n\n        :param multipart_chunksize: The chunk size of each ranged download.\n\n        :param max_request_processes: The maximum number of processes that\n            will be making S3 API transfer-related requests at a time.\n        \"\"\"\n        self.multipart_threshold = multipart_threshold\n        self.multipart_chunksize = multipart_chunksize\n        self.max_request_processes = max_request_processes\n\n\nclass ProcessPoolDownloader:\n    def __init__(self, client_kwargs=None, config=None):\n        \"\"\"Downloads S3 objects using process pools\n\n        :type client_kwargs: dict\n        :param client_kwargs: The keyword arguments to provide when\n            instantiating S3 clients. The arguments must match the keyword\n            arguments provided to the\n            `botocore.session.Session.create_client()` method.\n\n        :type config: ProcessTransferConfig\n        :param config: Configuration for the downloader\n        \"\"\"\n        if client_kwargs is None:\n            client_kwargs = {}\n        self._client_factory = ClientFactory(client_kwargs)\n\n        self._transfer_config = config\n        if config is None:\n            self._transfer_config = ProcessTransferConfig()\n\n        self._download_request_queue = multiprocessing.Queue(1000)\n        self._worker_queue = multiprocessing.Queue(1000)\n        self._osutil = OSUtils()\n\n        self._started = False\n        self._start_lock = threading.Lock()\n\n        # These below are initialized in the start() method\n        self._manager = None\n        self._transfer_monitor = None\n        self._submitter = None\n        self._workers = []\n\n    def download_file(\n        self, bucket, key, filename, extra_args=None, expected_size=None\n    ):\n        \"\"\"Downloads the object's contents to a file\n\n        :type bucket: str\n        :param bucket: The name of the bucket to download from\n\n        :type key: str\n        :param key: The name of the key to download from\n\n        :type filename: str\n        :param filename: The name of a file to download to.\n\n        :type extra_args: dict\n        :param extra_args: Extra arguments that may be passed to the\n            client operation\n\n        :type expected_size: int\n        :param expected_size: The expected size in bytes of the download. If\n            provided, the downloader will not call HeadObject to determine the\n            object's size and use the provided value instead. The size is\n            needed to determine whether to do a multipart download.\n\n        :rtype: s3transfer.futures.TransferFuture\n        :returns: Transfer future representing the download\n        \"\"\"\n        self._start_if_needed()\n        if extra_args is None:\n            extra_args = {}\n        self._validate_all_known_args(extra_args)\n        transfer_id = self._transfer_monitor.notify_new_transfer()\n        download_file_request = DownloadFileRequest(\n            transfer_id=transfer_id,\n            bucket=bucket,\n            key=key,\n            filename=filename,\n            extra_args=extra_args,\n            expected_size=expected_size,\n        )\n        logger.debug(\n            'Submitting download file request: %s.', download_file_request\n        )\n        self._download_request_queue.put(download_file_request)\n        call_args = CallArgs(\n            bucket=bucket,\n            key=key,\n            filename=filename,\n            extra_args=extra_args,\n            expected_size=expected_size,\n        )\n        future = self._get_transfer_future(transfer_id, call_args)\n        return future\n\n    def shutdown(self):\n        \"\"\"Shutdown the downloader\n\n        It will wait till all downloads are complete before returning.\n        \"\"\"\n        self._shutdown_if_needed()\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_value, *args):\n        if isinstance(exc_value, KeyboardInterrupt):\n            if self._transfer_monitor is not None:\n                self._transfer_monitor.notify_cancel_all_in_progress()\n        self.shutdown()\n\n    def _start_if_needed(self):\n        with self._start_lock:\n            if not self._started:\n                self._start()\n\n    def _start(self):\n        self._start_transfer_monitor_manager()\n        self._start_submitter()\n        self._start_get_object_workers()\n        self._started = True\n\n    def _validate_all_known_args(self, provided):\n        for kwarg in provided:\n            if kwarg not in ALLOWED_DOWNLOAD_ARGS:\n                download_args = ', '.join(ALLOWED_DOWNLOAD_ARGS)\n                raise ValueError(\n                    f\"Invalid extra_args key '{kwarg}', \"\n                    f\"must be one of: {download_args}\"\n                )\n\n    def _get_transfer_future(self, transfer_id, call_args):\n        meta = ProcessPoolTransferMeta(\n            call_args=call_args, transfer_id=transfer_id\n        )\n        future = ProcessPoolTransferFuture(\n            monitor=self._transfer_monitor, meta=meta\n        )\n        return future\n\n    def _start_transfer_monitor_manager(self):\n        logger.debug('Starting the TransferMonitorManager.')\n        self._manager = TransferMonitorManager()\n        # We do not want Ctrl-C's to cause the manager to shutdown immediately\n        # as worker processes will still need to communicate with it when they\n        # are shutting down. So instead we ignore Ctrl-C and let the manager\n        # be explicitly shutdown when shutting down the downloader.\n        self._manager.start(_add_ignore_handler_for_interrupts)\n        self._transfer_monitor = self._manager.TransferMonitor()\n\n    def _start_submitter(self):\n        logger.debug('Starting the GetObjectSubmitter.')\n        self._submitter = GetObjectSubmitter(\n            transfer_config=self._transfer_config,\n            client_factory=self._client_factory,\n            transfer_monitor=self._transfer_monitor,\n            osutil=self._osutil,\n            download_request_queue=self._download_request_queue,\n            worker_queue=self._worker_queue,\n        )\n        self._submitter.start()\n\n    def _start_get_object_workers(self):\n        logger.debug(\n            'Starting %s GetObjectWorkers.',\n            self._transfer_config.max_request_processes,\n        )\n        for _ in range(self._transfer_config.max_request_processes):\n            worker = GetObjectWorker(\n                queue=self._worker_queue,\n                client_factory=self._client_factory,\n                transfer_monitor=self._transfer_monitor,\n                osutil=self._osutil,\n            )\n            worker.start()\n            self._workers.append(worker)\n\n    def _shutdown_if_needed(self):\n        with self._start_lock:\n            if self._started:\n                self._shutdown()\n\n    def _shutdown(self):\n        self._shutdown_submitter()\n        self._shutdown_get_object_workers()\n        self._shutdown_transfer_monitor_manager()\n        self._started = False\n\n    def _shutdown_transfer_monitor_manager(self):\n        logger.debug('Shutting down the TransferMonitorManager.')\n        self._manager.shutdown()\n\n    def _shutdown_submitter(self):\n        logger.debug('Shutting down the GetObjectSubmitter.')\n        self._download_request_queue.put(SHUTDOWN_SIGNAL)\n        self._submitter.join()\n\n    def _shutdown_get_object_workers(self):\n        logger.debug('Shutting down the GetObjectWorkers.')\n        for _ in self._workers:\n            self._worker_queue.put(SHUTDOWN_SIGNAL)\n        for worker in self._workers:\n            worker.join()\n\n\nclass ProcessPoolTransferFuture(BaseTransferFuture):\n    def __init__(self, monitor, meta):\n        \"\"\"The future associated to a submitted process pool transfer request\n\n        :type monitor: TransferMonitor\n        :param monitor: The monitor associated to the process pool downloader\n\n        :type meta: ProcessPoolTransferMeta\n        :param meta: The metadata associated to the request. This object\n            is visible to the requester.\n        \"\"\"\n        self._monitor = monitor\n        self._meta = meta\n\n    @property\n    def meta(self):\n        return self._meta\n\n    def done(self):\n        return self._monitor.is_done(self._meta.transfer_id)\n\n    def result(self):\n        try:\n            return self._monitor.poll_for_result(self._meta.transfer_id)\n        except KeyboardInterrupt:\n            # For the multiprocessing Manager, a thread is given a single\n            # connection to reuse in communicating between the thread in the\n            # main process and the Manager's process. If a Ctrl-C happens when\n            # polling for the result, it will make the main thread stop trying\n            # to receive from the connection, but the Manager process will not\n            # know that the main process has stopped trying to receive and\n            # will not close the connection. As a result if another message is\n            # sent to the Manager process, the listener in the Manager\n            # processes will not process the new message as it is still trying\n            # trying to process the previous message (that was Ctrl-C'd) and\n            # thus cause the thread in the main process to hang on its send.\n            # The only way around this is to create a new connection and send\n            # messages from that new connection instead.\n            self._monitor._connect()\n            self.cancel()\n            raise\n\n    def cancel(self):\n        self._monitor.notify_exception(\n            self._meta.transfer_id, CancelledError()\n        )\n\n\nclass ProcessPoolTransferMeta(BaseTransferMeta):\n    \"\"\"Holds metadata about the ProcessPoolTransferFuture\"\"\"\n\n    def __init__(self, transfer_id, call_args):\n        self._transfer_id = transfer_id\n        self._call_args = call_args\n        self._user_context = {}\n\n    @property\n    def call_args(self):\n        return self._call_args\n\n    @property\n    def transfer_id(self):\n        return self._transfer_id\n\n    @property\n    def user_context(self):\n        return self._user_context\n\n\nclass ClientFactory:\n    def __init__(self, client_kwargs=None):\n        \"\"\"Creates S3 clients for processes\n\n        Botocore sessions and clients are not pickleable so they cannot be\n        inherited across Process boundaries. Instead, they must be instantiated\n        once a process is running.\n        \"\"\"\n        self._client_kwargs = client_kwargs\n        if self._client_kwargs is None:\n            self._client_kwargs = {}\n\n        client_config = deepcopy(self._client_kwargs.get('config', Config()))\n        if not client_config.user_agent_extra:\n            client_config.user_agent_extra = PROCESS_USER_AGENT\n        else:\n            client_config.user_agent_extra += \" \" + PROCESS_USER_AGENT\n        self._client_kwargs['config'] = client_config\n\n    def create_client(self):\n        \"\"\"Create a botocore S3 client\"\"\"\n        return botocore.session.Session().create_client(\n            's3', **self._client_kwargs\n        )\n\n\nclass TransferMonitor:\n    def __init__(self):\n        \"\"\"Monitors transfers for cross-process communication\n\n        Notifications can be sent to the monitor and information can be\n        retrieved from the monitor for a particular transfer. This abstraction\n        is ran in a ``multiprocessing.managers.BaseManager`` in order to be\n        shared across processes.\n        \"\"\"\n        # TODO: Add logic that removes the TransferState if the transfer is\n        #  marked as done and the reference to the future is no longer being\n        #  held onto. Without this logic, this dictionary will continue to\n        #  grow in size with no limit.\n        self._transfer_states = {}\n        self._id_count = 0\n        self._init_lock = threading.Lock()\n\n    def notify_new_transfer(self):\n        with self._init_lock:\n            transfer_id = self._id_count\n            self._transfer_states[transfer_id] = TransferState()\n            self._id_count += 1\n            return transfer_id\n\n    def is_done(self, transfer_id):\n        \"\"\"Determine a particular transfer is complete\n\n        :param transfer_id: Unique identifier for the transfer\n        :return: True, if done. False, otherwise.\n        \"\"\"\n        return self._transfer_states[transfer_id].done\n\n    def notify_done(self, transfer_id):\n        \"\"\"Notify a particular transfer is complete\n\n        :param transfer_id: Unique identifier for the transfer\n        \"\"\"\n        self._transfer_states[transfer_id].set_done()\n\n    def poll_for_result(self, transfer_id):\n        \"\"\"Poll for the result of a transfer\n\n        :param transfer_id: Unique identifier for the transfer\n        :return: If the transfer succeeded, it will return the result. If the\n            transfer failed, it will raise the exception associated to the\n            failure.\n        \"\"\"\n        self._transfer_states[transfer_id].wait_till_done()\n        exception = self._transfer_states[transfer_id].exception\n        if exception:\n            raise exception\n        return None\n\n    def notify_exception(self, transfer_id, exception):\n        \"\"\"Notify an exception was encountered for a transfer\n\n        :param transfer_id: Unique identifier for the transfer\n        :param exception: The exception encountered for that transfer\n        \"\"\"\n        # TODO: Not all exceptions are pickleable so if we are running\n        # this in a multiprocessing.BaseManager we will want to\n        # make sure to update this signature to ensure pickleability of the\n        # arguments or have the ProxyObject do the serialization.\n        self._transfer_states[transfer_id].exception = exception\n\n    def notify_cancel_all_in_progress(self):\n        for transfer_state in self._transfer_states.values():\n            if not transfer_state.done:\n                transfer_state.exception = CancelledError()\n\n    def get_exception(self, transfer_id):\n        \"\"\"Retrieve the exception encountered for the transfer\n\n        :param transfer_id: Unique identifier for the transfer\n        :return: The exception encountered for that transfer. Otherwise\n            if there were no exceptions, returns None.\n        \"\"\"\n        return self._transfer_states[transfer_id].exception\n\n    def notify_expected_jobs_to_complete(self, transfer_id, num_jobs):\n        \"\"\"Notify the amount of jobs expected for a transfer\n\n        :param transfer_id: Unique identifier for the transfer\n        :param num_jobs: The number of jobs to complete the transfer\n        \"\"\"\n        self._transfer_states[transfer_id].jobs_to_complete = num_jobs\n\n    def notify_job_complete(self, transfer_id):\n        \"\"\"Notify that a single job is completed for a transfer\n\n        :param transfer_id: Unique identifier for the transfer\n        :return: The number of jobs remaining to complete the transfer\n        \"\"\"\n        return self._transfer_states[transfer_id].decrement_jobs_to_complete()\n\n\nclass TransferState:\n    \"\"\"Represents the current state of an individual transfer\"\"\"\n\n    # NOTE: Ideally the TransferState object would be used directly by the\n    # various different abstractions in the ProcessPoolDownloader and remove\n    # the need for the TransferMonitor. However, it would then impose the\n    # constraint that two hops are required to make or get any changes in the\n    # state of a transfer across processes: one hop to get a proxy object for\n    # the TransferState and then a second hop to communicate calling the\n    # specific TransferState method.\n    def __init__(self):\n        self._exception = None\n        self._done_event = threading.Event()\n        self._job_lock = threading.Lock()\n        self._jobs_to_complete = 0\n\n    @property\n    def done(self):\n        return self._done_event.is_set()\n\n    def set_done(self):\n        self._done_event.set()\n\n    def wait_till_done(self):\n        self._done_event.wait(MAXINT)\n\n    @property\n    def exception(self):\n        return self._exception\n\n    @exception.setter\n    def exception(self, val):\n        self._exception = val\n\n    @property\n    def jobs_to_complete(self):\n        return self._jobs_to_complete\n\n    @jobs_to_complete.setter\n    def jobs_to_complete(self, val):\n        self._jobs_to_complete = val\n\n    def decrement_jobs_to_complete(self):\n        with self._job_lock:\n            self._jobs_to_complete -= 1\n            return self._jobs_to_complete\n\n\nclass TransferMonitorManager(BaseManager):\n    pass\n\n\nTransferMonitorManager.register('TransferMonitor', TransferMonitor)\n\n\nclass BaseS3TransferProcess(multiprocessing.Process):\n    def __init__(self, client_factory):\n        super().__init__()\n        self._client_factory = client_factory\n        self._client = None\n\n    def run(self):\n        # Clients are not pickleable so their instantiation cannot happen\n        # in the __init__ for processes that are created under the\n        # spawn method.\n        self._client = self._client_factory.create_client()\n        with ignore_ctrl_c():\n            # By default these processes are ran as child processes to the\n            # main process. Any Ctrl-c encountered in the main process is\n            # propagated to the child process and interrupt it at any time.\n            # To avoid any potentially bad states caused from an interrupt\n            # (i.e. a transfer failing to notify its done or making the\n            # communication protocol become out of sync with the\n            # TransferMonitor), we ignore all Ctrl-C's and allow the main\n            # process to notify these child processes when to stop processing\n            # jobs.\n            self._do_run()\n\n    def _do_run(self):\n        raise NotImplementedError('_do_run()')\n\n\nclass GetObjectSubmitter(BaseS3TransferProcess):\n    def __init__(\n        self,\n        transfer_config,\n        client_factory,\n        transfer_monitor,\n        osutil,\n        download_request_queue,\n        worker_queue,\n    ):\n        \"\"\"Submit GetObjectJobs to fulfill a download file request\n\n        :param transfer_config: Configuration for transfers.\n        :param client_factory: ClientFactory for creating S3 clients.\n        :param transfer_monitor: Monitor for notifying and retrieving state\n            of transfer.\n        :param osutil: OSUtils object to use for os-related behavior when\n            performing the transfer.\n        :param download_request_queue: Queue to retrieve download file\n            requests.\n        :param worker_queue: Queue to submit GetObjectJobs for workers\n            to perform.\n        \"\"\"\n        super().__init__(client_factory)\n        self._transfer_config = transfer_config\n        self._transfer_monitor = transfer_monitor\n        self._osutil = osutil\n        self._download_request_queue = download_request_queue\n        self._worker_queue = worker_queue\n\n    def _do_run(self):\n        while True:\n            download_file_request = self._download_request_queue.get()\n            if download_file_request == SHUTDOWN_SIGNAL:\n                logger.debug('Submitter shutdown signal received.')\n                return\n            try:\n                self._submit_get_object_jobs(download_file_request)\n            except Exception as e:\n                logger.debug(\n                    'Exception caught when submitting jobs for '\n                    'download file request %s: %s',\n                    download_file_request,\n                    e,\n                    exc_info=True,\n                )\n                self._transfer_monitor.notify_exception(\n                    download_file_request.transfer_id, e\n                )\n                self._transfer_monitor.notify_done(\n                    download_file_request.transfer_id\n                )\n\n    def _submit_get_object_jobs(self, download_file_request):\n        size = self._get_size(download_file_request)\n        temp_filename = self._allocate_temp_file(download_file_request, size)\n        if size < self._transfer_config.multipart_threshold:\n            self._submit_single_get_object_job(\n                download_file_request, temp_filename\n            )\n        else:\n            self._submit_ranged_get_object_jobs(\n                download_file_request, temp_filename, size\n            )\n\n    def _get_size(self, download_file_request):\n        expected_size = download_file_request.expected_size\n        if expected_size is None:\n            expected_size = self._client.head_object(\n                Bucket=download_file_request.bucket,\n                Key=download_file_request.key,\n                **download_file_request.extra_args,\n            )['ContentLength']\n        return expected_size\n\n    def _allocate_temp_file(self, download_file_request, size):\n        temp_filename = self._osutil.get_temp_filename(\n            download_file_request.filename\n        )\n        self._osutil.allocate(temp_filename, size)\n        return temp_filename\n\n    def _submit_single_get_object_job(\n        self, download_file_request, temp_filename\n    ):\n        self._notify_jobs_to_complete(download_file_request.transfer_id, 1)\n        self._submit_get_object_job(\n            transfer_id=download_file_request.transfer_id,\n            bucket=download_file_request.bucket,\n            key=download_file_request.key,\n            temp_filename=temp_filename,\n            offset=0,\n            extra_args=download_file_request.extra_args,\n            filename=download_file_request.filename,\n        )\n\n    def _submit_ranged_get_object_jobs(\n        self, download_file_request, temp_filename, size\n    ):\n        part_size = self._transfer_config.multipart_chunksize\n        num_parts = calculate_num_parts(size, part_size)\n        self._notify_jobs_to_complete(\n            download_file_request.transfer_id, num_parts\n        )\n        for i in range(num_parts):\n            offset = i * part_size\n            range_parameter = calculate_range_parameter(\n                part_size, i, num_parts\n            )\n            get_object_kwargs = {'Range': range_parameter}\n            get_object_kwargs.update(download_file_request.extra_args)\n            self._submit_get_object_job(\n                transfer_id=download_file_request.transfer_id,\n                bucket=download_file_request.bucket,\n                key=download_file_request.key,\n                temp_filename=temp_filename,\n                offset=offset,\n                extra_args=get_object_kwargs,\n                filename=download_file_request.filename,\n            )\n\n    def _submit_get_object_job(self, **get_object_job_kwargs):\n        self._worker_queue.put(GetObjectJob(**get_object_job_kwargs))\n\n    def _notify_jobs_to_complete(self, transfer_id, jobs_to_complete):\n        logger.debug(\n            'Notifying %s job(s) to complete for transfer_id %s.',\n            jobs_to_complete,\n            transfer_id,\n        )\n        self._transfer_monitor.notify_expected_jobs_to_complete(\n            transfer_id, jobs_to_complete\n        )\n\n\nclass GetObjectWorker(BaseS3TransferProcess):\n    # TODO: It may make sense to expose these class variables as configuration\n    # options if users want to tweak them.\n    _MAX_ATTEMPTS = 5\n    _IO_CHUNKSIZE = 2 * MB\n\n    def __init__(self, queue, client_factory, transfer_monitor, osutil):\n        \"\"\"Fulfills GetObjectJobs\n\n        Downloads the S3 object, writes it to the specified file, and\n        renames the file to its final location if it completes the final\n        job for a particular transfer.\n\n        :param queue: Queue for retrieving GetObjectJob's\n        :param client_factory: ClientFactory for creating S3 clients\n        :param transfer_monitor: Monitor for notifying\n        :param osutil: OSUtils object to use for os-related behavior when\n            performing the transfer.\n        \"\"\"\n        super().__init__(client_factory)\n        self._queue = queue\n        self._client_factory = client_factory\n        self._transfer_monitor = transfer_monitor\n        self._osutil = osutil\n\n    def _do_run(self):\n        while True:\n            job = self._queue.get()\n            if job == SHUTDOWN_SIGNAL:\n                logger.debug('Worker shutdown signal received.')\n                return\n            if not self._transfer_monitor.get_exception(job.transfer_id):\n                self._run_get_object_job(job)\n            else:\n                logger.debug(\n                    'Skipping get object job %s because there was a previous '\n                    'exception.',\n                    job,\n                )\n            remaining = self._transfer_monitor.notify_job_complete(\n                job.transfer_id\n            )\n            logger.debug(\n                '%s jobs remaining for transfer_id %s.',\n                remaining,\n                job.transfer_id,\n            )\n            if not remaining:\n                self._finalize_download(\n                    job.transfer_id, job.temp_filename, job.filename\n                )\n\n    def _run_get_object_job(self, job):\n        try:\n            self._do_get_object(\n                bucket=job.bucket,\n                key=job.key,\n                temp_filename=job.temp_filename,\n                extra_args=job.extra_args,\n                offset=job.offset,\n            )\n        except Exception as e:\n            logger.debug(\n                'Exception caught when downloading object for '\n                'get object job %s: %s',\n                job,\n                e,\n                exc_info=True,\n            )\n            self._transfer_monitor.notify_exception(job.transfer_id, e)\n\n    def _do_get_object(self, bucket, key, extra_args, temp_filename, offset):\n        last_exception = None\n        for i in range(self._MAX_ATTEMPTS):\n            try:\n                response = self._client.get_object(\n                    Bucket=bucket, Key=key, **extra_args\n                )\n                self._write_to_file(temp_filename, offset, response['Body'])\n                return\n            except S3_RETRYABLE_DOWNLOAD_ERRORS as e:\n                logger.debug(\n                    'Retrying exception caught (%s), '\n                    'retrying request, (attempt %s / %s)',\n                    e,\n                    i + 1,\n                    self._MAX_ATTEMPTS,\n                    exc_info=True,\n                )\n                last_exception = e\n        raise RetriesExceededError(last_exception)\n\n    def _write_to_file(self, filename, offset, body):\n        with open(filename, 'rb+') as f:\n            f.seek(offset)\n            chunks = iter(lambda: body.read(self._IO_CHUNKSIZE), b'')\n            for chunk in chunks:\n                f.write(chunk)\n\n    def _finalize_download(self, transfer_id, temp_filename, filename):\n        if self._transfer_monitor.get_exception(transfer_id):\n            self._osutil.remove_file(temp_filename)\n        else:\n            self._do_file_rename(transfer_id, temp_filename, filename)\n        self._transfer_monitor.notify_done(transfer_id)\n\n    def _do_file_rename(self, transfer_id, temp_filename, filename):\n        try:\n            self._osutil.rename_file(temp_filename, filename)\n        except Exception as e:\n            self._transfer_monitor.notify_exception(transfer_id, e)\n            self._osutil.remove_file(temp_filename)\n", "s3transfer/tasks.py": "# Copyright 2016 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport copy\nimport logging\n\nfrom s3transfer.utils import get_callbacks\n\nlogger = logging.getLogger(__name__)\n\n\nclass Task:\n    \"\"\"A task associated to a TransferFuture request\n\n    This is a base class for other classes to subclass from. All subclassed\n    classes must implement the main() method.\n    \"\"\"\n\n    def __init__(\n        self,\n        transfer_coordinator,\n        main_kwargs=None,\n        pending_main_kwargs=None,\n        done_callbacks=None,\n        is_final=False,\n    ):\n        \"\"\"\n        :type transfer_coordinator: s3transfer.futures.TransferCoordinator\n        :param transfer_coordinator: The context associated to the\n            TransferFuture for which this Task is associated with.\n\n        :type main_kwargs: dict\n        :param main_kwargs: The keyword args that can be immediately supplied\n            to the _main() method of the task\n\n        :type pending_main_kwargs: dict\n        :param pending_main_kwargs: The keyword args that are depended upon\n            by the result from a dependent future(s). The result returned by\n            the future(s) will be used as the value for the keyword argument\n            when _main() is called. The values for each key can be:\n                * a single future - Once completed, its value will be the\n                  result of that single future\n                * a list of futures - Once all of the futures complete, the\n                  value used will be a list of each completed future result\n                  value in order of when they were originally supplied.\n\n        :type done_callbacks: list of callbacks\n        :param done_callbacks: A list of callbacks to call once the task is\n            done completing. Each callback will be called with no arguments\n            and will be called no matter if the task succeeds or an exception\n            is raised.\n\n        :type is_final: boolean\n        :param is_final: True, to indicate that this task is the final task\n            for the TransferFuture request. By setting this value to True, it\n            will set the result of the entire TransferFuture to the result\n            returned by this task's main() method.\n        \"\"\"\n        self._transfer_coordinator = transfer_coordinator\n\n        self._main_kwargs = main_kwargs\n        if self._main_kwargs is None:\n            self._main_kwargs = {}\n\n        self._pending_main_kwargs = pending_main_kwargs\n        if pending_main_kwargs is None:\n            self._pending_main_kwargs = {}\n\n        self._done_callbacks = done_callbacks\n        if self._done_callbacks is None:\n            self._done_callbacks = []\n\n        self._is_final = is_final\n\n    def __repr__(self):\n        # These are the general main_kwarg parameters that we want to\n        # display in the repr.\n        params_to_display = [\n            'bucket',\n            'key',\n            'part_number',\n            'final_filename',\n            'transfer_future',\n            'offset',\n            'extra_args',\n        ]\n        main_kwargs_to_display = self._get_kwargs_with_params_to_include(\n            self._main_kwargs, params_to_display\n        )\n        return '{}(transfer_id={}, {})'.format(\n            self.__class__.__name__,\n            self._transfer_coordinator.transfer_id,\n            main_kwargs_to_display,\n        )\n\n    @property\n    def transfer_id(self):\n        \"\"\"The id for the transfer request that the task belongs to\"\"\"\n        return self._transfer_coordinator.transfer_id\n\n    def _get_kwargs_with_params_to_include(self, kwargs, include):\n        filtered_kwargs = {}\n        for param in include:\n            if param in kwargs:\n                filtered_kwargs[param] = kwargs[param]\n        return filtered_kwargs\n\n    def _get_kwargs_with_params_to_exclude(self, kwargs, exclude):\n        filtered_kwargs = {}\n        for param, value in kwargs.items():\n            if param in exclude:\n                continue\n            filtered_kwargs[param] = value\n        return filtered_kwargs\n\n    def __call__(self):\n        \"\"\"The callable to use when submitting a Task to an executor\"\"\"\n        try:\n            # Wait for all of futures this task depends on.\n            self._wait_on_dependent_futures()\n            # Gather up all of the main keyword arguments for main().\n            # This includes the immediately provided main_kwargs and\n            # the values for pending_main_kwargs that source from the return\n            # values from the task's dependent futures.\n            kwargs = self._get_all_main_kwargs()\n            # If the task is not done (really only if some other related\n            # task to the TransferFuture had failed) then execute the task's\n            # main() method.\n            if not self._transfer_coordinator.done():\n                return self._execute_main(kwargs)\n        except Exception as e:\n            self._log_and_set_exception(e)\n        finally:\n            # Run any done callbacks associated to the task no matter what.\n            for done_callback in self._done_callbacks:\n                done_callback()\n\n            if self._is_final:\n                # If this is the final task announce that it is done if results\n                # are waiting on its completion.\n                self._transfer_coordinator.announce_done()\n\n    def _execute_main(self, kwargs):\n        # Do not display keyword args that should not be printed, especially\n        # if they are going to make the logs hard to follow.\n        params_to_exclude = ['data']\n        kwargs_to_display = self._get_kwargs_with_params_to_exclude(\n            kwargs, params_to_exclude\n        )\n        # Log what is about to be executed.\n        logger.debug(f\"Executing task {self} with kwargs {kwargs_to_display}\")\n\n        return_value = self._main(**kwargs)\n        # If the task is the final task, then set the TransferFuture's\n        # value to the return value from main().\n        if self._is_final:\n            self._transfer_coordinator.set_result(return_value)\n        return return_value\n\n    def _log_and_set_exception(self, exception):\n        # If an exception is ever thrown than set the exception for the\n        # entire TransferFuture.\n        logger.debug(\"Exception raised.\", exc_info=True)\n        self._transfer_coordinator.set_exception(exception)\n\n    def _main(self, **kwargs):\n        \"\"\"The method that will be ran in the executor\n\n        This method must be implemented by subclasses from Task. main() can\n        be implemented with any arguments decided upon by the subclass.\n        \"\"\"\n        raise NotImplementedError('_main() must be implemented')\n\n    def _wait_on_dependent_futures(self):\n        # Gather all of the futures into that main() depends on.\n        futures_to_wait_on = []\n        for _, future in self._pending_main_kwargs.items():\n            # If the pending main keyword arg is a list then extend the list.\n            if isinstance(future, list):\n                futures_to_wait_on.extend(future)\n            # If the pending main keyword arg is a future append it to the list.\n            else:\n                futures_to_wait_on.append(future)\n        # Now wait for all of the futures to complete.\n        self._wait_until_all_complete(futures_to_wait_on)\n\n    def _wait_until_all_complete(self, futures):\n        # This is a basic implementation of the concurrent.futures.wait()\n        #\n        # concurrent.futures.wait() is not used instead because of this\n        # reported issue: https://bugs.python.org/issue20319.\n        # The issue would occasionally cause multipart uploads to hang\n        # when wait() was called. With this approach, it avoids the\n        # concurrency bug by removing any association with concurrent.futures\n        # implementation of waiters.\n        logger.debug(\n            '%s about to wait for the following futures %s', self, futures\n        )\n        for future in futures:\n            try:\n                logger.debug('%s about to wait for %s', self, future)\n                future.result()\n            except Exception:\n                # result() can also produce exceptions. We want to ignore\n                # these to be deferred to error handling down the road.\n                pass\n        logger.debug('%s done waiting for dependent futures', self)\n\n    def _get_all_main_kwargs(self):\n        # Copy over all of the kwargs that we know is available.\n        kwargs = copy.copy(self._main_kwargs)\n\n        # Iterate through the kwargs whose values are pending on the result\n        # of a future.\n        for key, pending_value in self._pending_main_kwargs.items():\n            # If the value is a list of futures, iterate though the list\n            # appending on the result from each future.\n            if isinstance(pending_value, list):\n                result = []\n                for future in pending_value:\n                    result.append(future.result())\n            # Otherwise if the pending_value is a future, just wait for it.\n            else:\n                result = pending_value.result()\n            # Add the retrieved value to the kwargs to be sent to the\n            # main() call.\n            kwargs[key] = result\n        return kwargs\n\n\nclass SubmissionTask(Task):\n    \"\"\"A base class for any submission task\n\n    Submission tasks are the top-level task used to submit a series of tasks\n    to execute a particular transfer.\n    \"\"\"\n\n    def _main(self, transfer_future, **kwargs):\n        \"\"\"\n        :type transfer_future: s3transfer.futures.TransferFuture\n        :param transfer_future: The transfer future associated with the\n            transfer request that tasks are being submitted for\n\n        :param kwargs: Any additional kwargs that you may want to pass\n            to the _submit() method\n        \"\"\"\n        try:\n            self._transfer_coordinator.set_status_to_queued()\n\n            # Before submitting any tasks, run all of the on_queued callbacks\n            on_queued_callbacks = get_callbacks(transfer_future, 'queued')\n            for on_queued_callback in on_queued_callbacks:\n                on_queued_callback()\n\n            # Once callbacks have been ran set the status to running.\n            self._transfer_coordinator.set_status_to_running()\n\n            # Call the submit method to start submitting tasks to execute the\n            # transfer.\n            self._submit(transfer_future=transfer_future, **kwargs)\n        except BaseException as e:\n            # If there was an exception raised during the submission of task\n            # there is a chance that the final task that signals if a transfer\n            # is done and too run the cleanup may never have been submitted in\n            # the first place so we need to account accordingly.\n            #\n            # Note that BaseException is caught, instead of Exception, because\n            # for some implementations of executors, specifically the serial\n            # implementation, the SubmissionTask is directly exposed to\n            # KeyboardInterupts and so needs to cleanup and signal done\n            # for those as well.\n\n            # Set the exception, that caused the process to fail.\n            self._log_and_set_exception(e)\n\n            # Wait for all possibly associated futures that may have spawned\n            # from this submission task have finished before we announce the\n            # transfer done.\n            self._wait_for_all_submitted_futures_to_complete()\n\n            # Announce the transfer as done, which will run any cleanups\n            # and done callbacks as well.\n            self._transfer_coordinator.announce_done()\n\n    def _submit(self, transfer_future, **kwargs):\n        \"\"\"The submission method to be implemented\n\n        :type transfer_future: s3transfer.futures.TransferFuture\n        :param transfer_future: The transfer future associated with the\n            transfer request that tasks are being submitted for\n\n        :param kwargs: Any additional keyword arguments you want to be passed\n            in\n        \"\"\"\n        raise NotImplementedError('_submit() must be implemented')\n\n    def _wait_for_all_submitted_futures_to_complete(self):\n        # We want to wait for all futures that were submitted to\n        # complete as we do not want the cleanup callbacks or done callbacks\n        # to be called to early. The main problem is any task that was\n        # submitted may have submitted even more during its process and so\n        # we need to account accordingly.\n\n        # First get all of the futures that were submitted up to this point.\n        submitted_futures = self._transfer_coordinator.associated_futures\n        while submitted_futures:\n            # Wait for those futures to complete.\n            self._wait_until_all_complete(submitted_futures)\n            # However, more futures may have been submitted as we waited so\n            # we need to check again for any more associated futures.\n            possibly_more_submitted_futures = (\n                self._transfer_coordinator.associated_futures\n            )\n            # If the current list of submitted futures is equal to the\n            # the list of associated futures for when after the wait completes,\n            # we can ensure no more futures were submitted in waiting on\n            # the current list of futures to complete ultimately meaning all\n            # futures that may have spawned from the original submission task\n            # have completed.\n            if submitted_futures == possibly_more_submitted_futures:\n                break\n            submitted_futures = possibly_more_submitted_futures\n\n\nclass CreateMultipartUploadTask(Task):\n    \"\"\"Task to initiate a multipart upload\"\"\"\n\n    def _main(self, client, bucket, key, extra_args):\n        \"\"\"\n        :param client: The client to use when calling CreateMultipartUpload\n        :param bucket: The name of the bucket to upload to\n        :param key: The name of the key to upload to\n        :param extra_args: A dictionary of any extra arguments that may be\n            used in the initialization.\n\n        :returns: The upload id of the multipart upload\n        \"\"\"\n        # Create the multipart upload.\n        response = client.create_multipart_upload(\n            Bucket=bucket, Key=key, **extra_args\n        )\n        upload_id = response['UploadId']\n\n        # Add a cleanup if the multipart upload fails at any point.\n        self._transfer_coordinator.add_failure_cleanup(\n            client.abort_multipart_upload,\n            Bucket=bucket,\n            Key=key,\n            UploadId=upload_id,\n        )\n        return upload_id\n\n\nclass CompleteMultipartUploadTask(Task):\n    \"\"\"Task to complete a multipart upload\"\"\"\n\n    def _main(self, client, bucket, key, upload_id, parts, extra_args):\n        \"\"\"\n        :param client: The client to use when calling CompleteMultipartUpload\n        :param bucket: The name of the bucket to upload to\n        :param key: The name of the key to upload to\n        :param upload_id: The id of the upload\n        :param parts: A list of parts to use to complete the multipart upload::\n\n            [{'Etag': etag_value, 'PartNumber': part_number}, ...]\n\n            Each element in the list consists of a return value from\n            ``UploadPartTask.main()``.\n        :param extra_args:  A dictionary of any extra arguments that may be\n            used in completing the multipart transfer.\n        \"\"\"\n        client.complete_multipart_upload(\n            Bucket=bucket,\n            Key=key,\n            UploadId=upload_id,\n            MultipartUpload={'Parts': parts},\n            **extra_args,\n        )\n", "s3transfer/bandwidth.py": "# Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport threading\nimport time\n\n\nclass RequestExceededException(Exception):\n    def __init__(self, requested_amt, retry_time):\n        \"\"\"Error when requested amount exceeds what is allowed\n\n        The request that raised this error should be retried after waiting\n        the time specified by ``retry_time``.\n\n        :type requested_amt: int\n        :param requested_amt: The originally requested byte amount\n\n        :type retry_time: float\n        :param retry_time: The length in time to wait to retry for the\n            requested amount\n        \"\"\"\n        self.requested_amt = requested_amt\n        self.retry_time = retry_time\n        msg = 'Request amount {} exceeded the amount available. Retry in {}'.format(\n            requested_amt, retry_time\n        )\n        super().__init__(msg)\n\n\nclass RequestToken:\n    \"\"\"A token to pass as an identifier when consuming from the LeakyBucket\"\"\"\n\n    pass\n\n\nclass TimeUtils:\n    def time(self):\n        \"\"\"Get the current time back\n\n        :rtype: float\n        :returns: The current time in seconds\n        \"\"\"\n        return time.time()\n\n    def sleep(self, value):\n        \"\"\"Sleep for a designated time\n\n        :type value: float\n        :param value: The time to sleep for in seconds\n        \"\"\"\n        return time.sleep(value)\n\n\nclass BandwidthLimiter:\n    def __init__(self, leaky_bucket, time_utils=None):\n        \"\"\"Limits bandwidth for shared S3 transfers\n\n        :type leaky_bucket: LeakyBucket\n        :param leaky_bucket: The leaky bucket to use limit bandwidth\n\n        :type time_utils: TimeUtils\n        :param time_utils: Time utility to use for interacting with time.\n        \"\"\"\n        self._leaky_bucket = leaky_bucket\n        self._time_utils = time_utils\n        if time_utils is None:\n            self._time_utils = TimeUtils()\n\n    def get_bandwith_limited_stream(\n        self, fileobj, transfer_coordinator, enabled=True\n    ):\n        \"\"\"Wraps a fileobj in a bandwidth limited stream wrapper\n\n        :type fileobj: file-like obj\n        :param fileobj: The file-like obj to wrap\n\n        :type transfer_coordinator: s3transfer.futures.TransferCoordinator\n        param transfer_coordinator: The coordinator for the general transfer\n            that the wrapped stream is a part of\n\n        :type enabled: boolean\n        :param enabled: Whether bandwidth limiting should be enabled to start\n        \"\"\"\n        stream = BandwidthLimitedStream(\n            fileobj, self._leaky_bucket, transfer_coordinator, self._time_utils\n        )\n        if not enabled:\n            stream.disable_bandwidth_limiting()\n        return stream\n\n\nclass BandwidthLimitedStream:\n    def __init__(\n        self,\n        fileobj,\n        leaky_bucket,\n        transfer_coordinator,\n        time_utils=None,\n        bytes_threshold=256 * 1024,\n    ):\n        \"\"\"Limits bandwidth for reads on a wrapped stream\n\n        :type fileobj: file-like object\n        :param fileobj: The file like object to wrap\n\n        :type leaky_bucket: LeakyBucket\n        :param leaky_bucket: The leaky bucket to use to throttle reads on\n            the stream\n\n        :type transfer_coordinator: s3transfer.futures.TransferCoordinator\n        param transfer_coordinator: The coordinator for the general transfer\n            that the wrapped stream is a part of\n\n        :type time_utils: TimeUtils\n        :param time_utils: The time utility to use for interacting with time\n        \"\"\"\n        self._fileobj = fileobj\n        self._leaky_bucket = leaky_bucket\n        self._transfer_coordinator = transfer_coordinator\n        self._time_utils = time_utils\n        if time_utils is None:\n            self._time_utils = TimeUtils()\n        self._bandwidth_limiting_enabled = True\n        self._request_token = RequestToken()\n        self._bytes_seen = 0\n        self._bytes_threshold = bytes_threshold\n\n    def enable_bandwidth_limiting(self):\n        \"\"\"Enable bandwidth limiting on reads to the stream\"\"\"\n        self._bandwidth_limiting_enabled = True\n\n    def disable_bandwidth_limiting(self):\n        \"\"\"Disable bandwidth limiting on reads to the stream\"\"\"\n        self._bandwidth_limiting_enabled = False\n\n    def read(self, amount):\n        \"\"\"Read a specified amount\n\n        Reads will only be throttled if bandwidth limiting is enabled.\n        \"\"\"\n        if not self._bandwidth_limiting_enabled:\n            return self._fileobj.read(amount)\n\n        # We do not want to be calling consume on every read as the read\n        # amounts can be small causing the lock of the leaky bucket to\n        # introduce noticeable overhead. So instead we keep track of\n        # how many bytes we have seen and only call consume once we pass a\n        # certain threshold.\n        self._bytes_seen += amount\n        if self._bytes_seen < self._bytes_threshold:\n            return self._fileobj.read(amount)\n\n        self._consume_through_leaky_bucket()\n        return self._fileobj.read(amount)\n\n    def _consume_through_leaky_bucket(self):\n        # NOTE: If the read amount on the stream are high, it will result\n        # in large bursty behavior as there is not an interface for partial\n        # reads. However given the read's on this abstraction are at most 256KB\n        # (via downloads), it reduces the burstiness to be small KB bursts at\n        # worst.\n        while not self._transfer_coordinator.exception:\n            try:\n                self._leaky_bucket.consume(\n                    self._bytes_seen, self._request_token\n                )\n                self._bytes_seen = 0\n                return\n            except RequestExceededException as e:\n                self._time_utils.sleep(e.retry_time)\n        else:\n            raise self._transfer_coordinator.exception\n\n    def signal_transferring(self):\n        \"\"\"Signal that data being read is being transferred to S3\"\"\"\n        self.enable_bandwidth_limiting()\n\n    def signal_not_transferring(self):\n        \"\"\"Signal that data being read is not being transferred to S3\"\"\"\n        self.disable_bandwidth_limiting()\n\n    def seek(self, where, whence=0):\n        self._fileobj.seek(where, whence)\n\n    def tell(self):\n        return self._fileobj.tell()\n\n    def close(self):\n        if self._bandwidth_limiting_enabled and self._bytes_seen:\n            # This handles the case where the file is small enough to never\n            # trigger the threshold and thus is never subjugated to the\n            # leaky bucket on read(). This specifically happens for small\n            # uploads. So instead to account for those bytes, have\n            # it go through the leaky bucket when the file gets closed.\n            self._consume_through_leaky_bucket()\n        self._fileobj.close()\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, *args, **kwargs):\n        self.close()\n\n\nclass LeakyBucket:\n    def __init__(\n        self,\n        max_rate,\n        time_utils=None,\n        rate_tracker=None,\n        consumption_scheduler=None,\n    ):\n        \"\"\"A leaky bucket abstraction to limit bandwidth consumption\n\n        :type rate: int\n        :type rate: The maximum rate to allow. This rate is in terms of\n            bytes per second.\n\n        :type time_utils: TimeUtils\n        :param time_utils: The time utility to use for interacting with time\n\n        :type rate_tracker: BandwidthRateTracker\n        :param rate_tracker: Tracks bandwidth consumption\n\n        :type consumption_scheduler: ConsumptionScheduler\n        :param consumption_scheduler: Schedules consumption retries when\n            necessary\n        \"\"\"\n        self._max_rate = float(max_rate)\n        self._time_utils = time_utils\n        if time_utils is None:\n            self._time_utils = TimeUtils()\n        self._lock = threading.Lock()\n        self._rate_tracker = rate_tracker\n        if rate_tracker is None:\n            self._rate_tracker = BandwidthRateTracker()\n        self._consumption_scheduler = consumption_scheduler\n        if consumption_scheduler is None:\n            self._consumption_scheduler = ConsumptionScheduler()\n\n    def consume(self, amt, request_token):\n        \"\"\"Consume an a requested amount\n\n        :type amt: int\n        :param amt: The amount of bytes to request to consume\n\n        :type request_token: RequestToken\n        :param request_token: The token associated to the consumption\n            request that is used to identify the request. So if a\n            RequestExceededException is raised the token should be used\n            in subsequent retry consume() request.\n\n        :raises RequestExceededException: If the consumption amount would\n            exceed the maximum allocated bandwidth\n\n        :rtype: int\n        :returns: The amount consumed\n        \"\"\"\n        with self._lock:\n            time_now = self._time_utils.time()\n            if self._consumption_scheduler.is_scheduled(request_token):\n                return self._release_requested_amt_for_scheduled_request(\n                    amt, request_token, time_now\n                )\n            elif self._projected_to_exceed_max_rate(amt, time_now):\n                self._raise_request_exceeded_exception(\n                    amt, request_token, time_now\n                )\n            else:\n                return self._release_requested_amt(amt, time_now)\n\n    def _projected_to_exceed_max_rate(self, amt, time_now):\n        projected_rate = self._rate_tracker.get_projected_rate(amt, time_now)\n        return projected_rate > self._max_rate\n\n    def _release_requested_amt_for_scheduled_request(\n        self, amt, request_token, time_now\n    ):\n        self._consumption_scheduler.process_scheduled_consumption(\n            request_token\n        )\n        return self._release_requested_amt(amt, time_now)\n\n    def _raise_request_exceeded_exception(self, amt, request_token, time_now):\n        allocated_time = amt / float(self._max_rate)\n        retry_time = self._consumption_scheduler.schedule_consumption(\n            amt, request_token, allocated_time\n        )\n        raise RequestExceededException(\n            requested_amt=amt, retry_time=retry_time\n        )\n\n    def _release_requested_amt(self, amt, time_now):\n        self._rate_tracker.record_consumption_rate(amt, time_now)\n        return amt\n\n\nclass ConsumptionScheduler:\n    def __init__(self):\n        \"\"\"Schedules when to consume a desired amount\"\"\"\n        self._tokens_to_scheduled_consumption = {}\n        self._total_wait = 0\n\n    def is_scheduled(self, token):\n        \"\"\"Indicates if a consumption request has been scheduled\n\n        :type token: RequestToken\n        :param token: The token associated to the consumption\n            request that is used to identify the request.\n        \"\"\"\n        return token in self._tokens_to_scheduled_consumption\n\n    def schedule_consumption(self, amt, token, time_to_consume):\n        \"\"\"Schedules a wait time to be able to consume an amount\n\n        :type amt: int\n        :param amt: The amount of bytes scheduled to be consumed\n\n        :type token: RequestToken\n        :param token: The token associated to the consumption\n            request that is used to identify the request.\n\n        :type time_to_consume: float\n        :param time_to_consume: The desired time it should take for that\n            specific request amount to be consumed in regardless of previously\n            scheduled consumption requests\n\n        :rtype: float\n        :returns: The amount of time to wait for the specific request before\n            actually consuming the specified amount.\n        \"\"\"\n        self._total_wait += time_to_consume\n        self._tokens_to_scheduled_consumption[token] = {\n            'wait_duration': self._total_wait,\n            'time_to_consume': time_to_consume,\n        }\n        return self._total_wait\n\n    def process_scheduled_consumption(self, token):\n        \"\"\"Processes a scheduled consumption request that has completed\n\n        :type token: RequestToken\n        :param token: The token associated to the consumption\n            request that is used to identify the request.\n        \"\"\"\n        scheduled_retry = self._tokens_to_scheduled_consumption.pop(token)\n        self._total_wait = max(\n            self._total_wait - scheduled_retry['time_to_consume'], 0\n        )\n\n\nclass BandwidthRateTracker:\n    def __init__(self, alpha=0.8):\n        \"\"\"Tracks the rate of bandwidth consumption\n\n        :type a: float\n        :param a: The constant to use in calculating the exponentional moving\n            average of the bandwidth rate. Specifically it is used in the\n            following calculation:\n\n            current_rate = alpha * new_rate + (1 - alpha) * current_rate\n\n            This value of this constant should be between 0 and 1.\n        \"\"\"\n        self._alpha = alpha\n        self._last_time = None\n        self._current_rate = None\n\n    @property\n    def current_rate(self):\n        \"\"\"The current transfer rate\n\n        :rtype: float\n        :returns: The current tracked transfer rate\n        \"\"\"\n        if self._last_time is None:\n            return 0.0\n        return self._current_rate\n\n    def get_projected_rate(self, amt, time_at_consumption):\n        \"\"\"Get the projected rate using a provided amount and time\n\n        :type amt: int\n        :param amt: The proposed amount to consume\n\n        :type time_at_consumption: float\n        :param time_at_consumption: The proposed time to consume at\n\n        :rtype: float\n        :returns: The consumption rate if that amt and time were consumed\n        \"\"\"\n        if self._last_time is None:\n            return 0.0\n        return self._calculate_exponential_moving_average_rate(\n            amt, time_at_consumption\n        )\n\n    def record_consumption_rate(self, amt, time_at_consumption):\n        \"\"\"Record the consumption rate based off amount and time point\n\n        :type amt: int\n        :param amt: The amount that got consumed\n\n        :type time_at_consumption: float\n        :param time_at_consumption: The time at which the amount was consumed\n        \"\"\"\n        if self._last_time is None:\n            self._last_time = time_at_consumption\n            self._current_rate = 0.0\n            return\n        self._current_rate = self._calculate_exponential_moving_average_rate(\n            amt, time_at_consumption\n        )\n        self._last_time = time_at_consumption\n\n    def _calculate_rate(self, amt, time_at_consumption):\n        time_delta = time_at_consumption - self._last_time\n        if time_delta <= 0:\n            # While it is really unlikely to see this in an actual transfer,\n            # we do not want to be returning back a negative rate or try to\n            # divide the amount by zero. So instead return back an infinite\n            # rate as the time delta is infinitesimally small.\n            return float('inf')\n        return amt / (time_delta)\n\n    def _calculate_exponential_moving_average_rate(\n        self, amt, time_at_consumption\n    ):\n        new_rate = self._calculate_rate(amt, time_at_consumption)\n        return self._alpha * new_rate + (1 - self._alpha) * self._current_rate\n", "s3transfer/upload.py": "# Copyright 2016 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport math\nfrom io import BytesIO\n\nfrom s3transfer.compat import readable, seekable\nfrom s3transfer.futures import IN_MEMORY_UPLOAD_TAG\nfrom s3transfer.tasks import (\n    CompleteMultipartUploadTask,\n    CreateMultipartUploadTask,\n    SubmissionTask,\n    Task,\n)\nfrom s3transfer.utils import (\n    ChunksizeAdjuster,\n    DeferredOpenFile,\n    get_callbacks,\n    get_filtered_dict,\n)\n\n\nclass AggregatedProgressCallback:\n    def __init__(self, callbacks, threshold=1024 * 256):\n        \"\"\"Aggregates progress updates for every provided progress callback\n\n        :type callbacks: A list of functions that accepts bytes_transferred\n            as a single argument\n        :param callbacks: The callbacks to invoke when threshold is reached\n\n        :type threshold: int\n        :param threshold: The progress threshold in which to take the\n            aggregated progress and invoke the progress callback with that\n            aggregated progress total\n        \"\"\"\n        self._callbacks = callbacks\n        self._threshold = threshold\n        self._bytes_seen = 0\n\n    def __call__(self, bytes_transferred):\n        self._bytes_seen += bytes_transferred\n        if self._bytes_seen >= self._threshold:\n            self._trigger_callbacks()\n\n    def flush(self):\n        \"\"\"Flushes out any progress that has not been sent to its callbacks\"\"\"\n        if self._bytes_seen > 0:\n            self._trigger_callbacks()\n\n    def _trigger_callbacks(self):\n        for callback in self._callbacks:\n            callback(bytes_transferred=self._bytes_seen)\n        self._bytes_seen = 0\n\n\nclass InterruptReader:\n    \"\"\"Wrapper that can interrupt reading using an error\n\n    It uses a transfer coordinator to propagate an error if it notices\n    that a read is being made while the file is being read from.\n\n    :type fileobj: file-like obj\n    :param fileobj: The file-like object to read from\n\n    :type transfer_coordinator: s3transfer.futures.TransferCoordinator\n    :param transfer_coordinator: The transfer coordinator to use if the\n        reader needs to be interrupted.\n    \"\"\"\n\n    def __init__(self, fileobj, transfer_coordinator):\n        self._fileobj = fileobj\n        self._transfer_coordinator = transfer_coordinator\n\n    def read(self, amount=None):\n        # If there is an exception, then raise the exception.\n        # We raise an error instead of returning no bytes because for\n        # requests where the content length and md5 was sent, it will\n        # cause md5 mismatches and retries as there was no indication that\n        # the stream being read from encountered any issues.\n        if self._transfer_coordinator.exception:\n            raise self._transfer_coordinator.exception\n        return self._fileobj.read(amount)\n\n    def seek(self, where, whence=0):\n        self._fileobj.seek(where, whence)\n\n    def tell(self):\n        return self._fileobj.tell()\n\n    def close(self):\n        self._fileobj.close()\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, *args, **kwargs):\n        self.close()\n\n\nclass UploadInputManager:\n    \"\"\"Base manager class for handling various types of files for uploads\n\n    This class is typically used for the UploadSubmissionTask class to help\n    determine the following:\n\n        * How to determine the size of the file\n        * How to determine if a multipart upload is required\n        * How to retrieve the body for a PutObject\n        * How to retrieve the bodies for a set of UploadParts\n\n    The answers/implementations differ for the various types of file inputs\n    that may be accepted. All implementations must subclass and override\n    public methods from this class.\n    \"\"\"\n\n    def __init__(self, osutil, transfer_coordinator, bandwidth_limiter=None):\n        self._osutil = osutil\n        self._transfer_coordinator = transfer_coordinator\n        self._bandwidth_limiter = bandwidth_limiter\n\n    @classmethod\n    def is_compatible(cls, upload_source):\n        \"\"\"Determines if the source for the upload is compatible with manager\n\n        :param upload_source: The source for which the upload will pull data\n            from.\n\n        :returns: True if the manager can handle the type of source specified\n            otherwise returns False.\n        \"\"\"\n        raise NotImplementedError('must implement _is_compatible()')\n\n    def stores_body_in_memory(self, operation_name):\n        \"\"\"Whether the body it provides are stored in-memory\n\n        :type operation_name: str\n        :param operation_name: The name of the client operation that the body\n            is being used for. Valid operation_names are ``put_object`` and\n            ``upload_part``.\n\n        :rtype: boolean\n        :returns: True if the body returned by the manager will be stored in\n            memory. False if the manager will not directly store the body in\n            memory.\n        \"\"\"\n        raise NotImplementedError('must implement store_body_in_memory()')\n\n    def provide_transfer_size(self, transfer_future):\n        \"\"\"Provides the transfer size of an upload\n\n        :type transfer_future: s3transfer.futures.TransferFuture\n        :param transfer_future: The future associated with upload request\n        \"\"\"\n        raise NotImplementedError('must implement provide_transfer_size()')\n\n    def requires_multipart_upload(self, transfer_future, config):\n        \"\"\"Determines where a multipart upload is required\n\n        :type transfer_future: s3transfer.futures.TransferFuture\n        :param transfer_future: The future associated with upload request\n\n        :type config: s3transfer.manager.TransferConfig\n        :param config: The config associated to the transfer manager\n\n        :rtype: boolean\n        :returns: True, if the upload should be multipart based on\n            configuration and size. False, otherwise.\n        \"\"\"\n        raise NotImplementedError('must implement requires_multipart_upload()')\n\n    def get_put_object_body(self, transfer_future):\n        \"\"\"Returns the body to use for PutObject\n\n        :type transfer_future: s3transfer.futures.TransferFuture\n        :param transfer_future: The future associated with upload request\n\n        :type config: s3transfer.manager.TransferConfig\n        :param config: The config associated to the transfer manager\n\n        :rtype: s3transfer.utils.ReadFileChunk\n        :returns: A ReadFileChunk including all progress callbacks\n            associated with the transfer future.\n        \"\"\"\n        raise NotImplementedError('must implement get_put_object_body()')\n\n    def yield_upload_part_bodies(self, transfer_future, chunksize):\n        \"\"\"Yields the part number and body to use for each UploadPart\n\n        :type transfer_future: s3transfer.futures.TransferFuture\n        :param transfer_future: The future associated with upload request\n\n        :type chunksize: int\n        :param chunksize: The chunksize to use for this upload.\n\n        :rtype: int, s3transfer.utils.ReadFileChunk\n        :returns: Yields the part number and the ReadFileChunk including all\n            progress callbacks associated with the transfer future for that\n            specific yielded part.\n        \"\"\"\n        raise NotImplementedError('must implement yield_upload_part_bodies()')\n\n    def _wrap_fileobj(self, fileobj):\n        fileobj = InterruptReader(fileobj, self._transfer_coordinator)\n        if self._bandwidth_limiter:\n            fileobj = self._bandwidth_limiter.get_bandwith_limited_stream(\n                fileobj, self._transfer_coordinator, enabled=False\n            )\n        return fileobj\n\n    def _get_progress_callbacks(self, transfer_future):\n        callbacks = get_callbacks(transfer_future, 'progress')\n        # We only want to be wrapping the callbacks if there are callbacks to\n        # invoke because we do not want to be doing any unnecessary work if\n        # there are no callbacks to invoke.\n        if callbacks:\n            return [AggregatedProgressCallback(callbacks)]\n        return []\n\n    def _get_close_callbacks(self, aggregated_progress_callbacks):\n        return [callback.flush for callback in aggregated_progress_callbacks]\n\n\nclass UploadFilenameInputManager(UploadInputManager):\n    \"\"\"Upload utility for filenames\"\"\"\n\n    @classmethod\n    def is_compatible(cls, upload_source):\n        return isinstance(upload_source, str)\n\n    def stores_body_in_memory(self, operation_name):\n        return False\n\n    def provide_transfer_size(self, transfer_future):\n        transfer_future.meta.provide_transfer_size(\n            self._osutil.get_file_size(transfer_future.meta.call_args.fileobj)\n        )\n\n    def requires_multipart_upload(self, transfer_future, config):\n        return transfer_future.meta.size >= config.multipart_threshold\n\n    def get_put_object_body(self, transfer_future):\n        # Get a file-like object for the given input\n        fileobj, full_size = self._get_put_object_fileobj_with_full_size(\n            transfer_future\n        )\n\n        # Wrap fileobj with interrupt reader that will quickly cancel\n        # uploads if needed instead of having to wait for the socket\n        # to completely read all of the data.\n        fileobj = self._wrap_fileobj(fileobj)\n\n        callbacks = self._get_progress_callbacks(transfer_future)\n        close_callbacks = self._get_close_callbacks(callbacks)\n        size = transfer_future.meta.size\n        # Return the file-like object wrapped into a ReadFileChunk to get\n        # progress.\n        return self._osutil.open_file_chunk_reader_from_fileobj(\n            fileobj=fileobj,\n            chunk_size=size,\n            full_file_size=full_size,\n            callbacks=callbacks,\n            close_callbacks=close_callbacks,\n        )\n\n    def yield_upload_part_bodies(self, transfer_future, chunksize):\n        full_file_size = transfer_future.meta.size\n        num_parts = self._get_num_parts(transfer_future, chunksize)\n        for part_number in range(1, num_parts + 1):\n            callbacks = self._get_progress_callbacks(transfer_future)\n            close_callbacks = self._get_close_callbacks(callbacks)\n            start_byte = chunksize * (part_number - 1)\n            # Get a file-like object for that part and the size of the full\n            # file size for the associated file-like object for that part.\n            fileobj, full_size = self._get_upload_part_fileobj_with_full_size(\n                transfer_future.meta.call_args.fileobj,\n                start_byte=start_byte,\n                part_size=chunksize,\n                full_file_size=full_file_size,\n            )\n\n            # Wrap fileobj with interrupt reader that will quickly cancel\n            # uploads if needed instead of having to wait for the socket\n            # to completely read all of the data.\n            fileobj = self._wrap_fileobj(fileobj)\n\n            # Wrap the file-like object into a ReadFileChunk to get progress.\n            read_file_chunk = self._osutil.open_file_chunk_reader_from_fileobj(\n                fileobj=fileobj,\n                chunk_size=chunksize,\n                full_file_size=full_size,\n                callbacks=callbacks,\n                close_callbacks=close_callbacks,\n            )\n            yield part_number, read_file_chunk\n\n    def _get_deferred_open_file(self, fileobj, start_byte):\n        fileobj = DeferredOpenFile(\n            fileobj, start_byte, open_function=self._osutil.open\n        )\n        return fileobj\n\n    def _get_put_object_fileobj_with_full_size(self, transfer_future):\n        fileobj = transfer_future.meta.call_args.fileobj\n        size = transfer_future.meta.size\n        return self._get_deferred_open_file(fileobj, 0), size\n\n    def _get_upload_part_fileobj_with_full_size(self, fileobj, **kwargs):\n        start_byte = kwargs['start_byte']\n        full_size = kwargs['full_file_size']\n        return self._get_deferred_open_file(fileobj, start_byte), full_size\n\n    def _get_num_parts(self, transfer_future, part_size):\n        return int(math.ceil(transfer_future.meta.size / float(part_size)))\n\n\nclass UploadSeekableInputManager(UploadFilenameInputManager):\n    \"\"\"Upload utility for an open file object\"\"\"\n\n    @classmethod\n    def is_compatible(cls, upload_source):\n        return readable(upload_source) and seekable(upload_source)\n\n    def stores_body_in_memory(self, operation_name):\n        if operation_name == 'put_object':\n            return False\n        else:\n            return True\n\n    def provide_transfer_size(self, transfer_future):\n        fileobj = transfer_future.meta.call_args.fileobj\n        # To determine size, first determine the starting position\n        # Seek to the end and then find the difference in the length\n        # between the end and start positions.\n        start_position = fileobj.tell()\n        fileobj.seek(0, 2)\n        end_position = fileobj.tell()\n        fileobj.seek(start_position)\n        transfer_future.meta.provide_transfer_size(\n            end_position - start_position\n        )\n\n    def _get_upload_part_fileobj_with_full_size(self, fileobj, **kwargs):\n        # Note: It is unfortunate that in order to do a multithreaded\n        # multipart upload we cannot simply copy the filelike object\n        # since there is not really a mechanism in python (i.e. os.dup\n        # points to the same OS filehandle which causes concurrency\n        # issues). So instead we need to read from the fileobj and\n        # chunk the data out to separate file-like objects in memory.\n        data = fileobj.read(kwargs['part_size'])\n        # We return the length of the data instead of the full_file_size\n        # because we partitioned the data into separate BytesIO objects\n        # meaning the BytesIO object has no knowledge of its start position\n        # relative the input source nor access to the rest of the input\n        # source. So we must treat it as its own standalone file.\n        return BytesIO(data), len(data)\n\n    def _get_put_object_fileobj_with_full_size(self, transfer_future):\n        fileobj = transfer_future.meta.call_args.fileobj\n        # The current position needs to be taken into account when retrieving\n        # the full size of the file.\n        size = fileobj.tell() + transfer_future.meta.size\n        return fileobj, size\n\n\nclass UploadNonSeekableInputManager(UploadInputManager):\n    \"\"\"Upload utility for a file-like object that cannot seek.\"\"\"\n\n    def __init__(self, osutil, transfer_coordinator, bandwidth_limiter=None):\n        super().__init__(osutil, transfer_coordinator, bandwidth_limiter)\n        self._initial_data = b''\n\n    @classmethod\n    def is_compatible(cls, upload_source):\n        return readable(upload_source)\n\n    def stores_body_in_memory(self, operation_name):\n        return True\n\n    def provide_transfer_size(self, transfer_future):\n        # No-op because there is no way to do this short of reading the entire\n        # body into memory.\n        return\n\n    def requires_multipart_upload(self, transfer_future, config):\n        # If the user has set the size, we can use that.\n        if transfer_future.meta.size is not None:\n            return transfer_future.meta.size >= config.multipart_threshold\n\n        # This is tricky to determine in this case because we can't know how\n        # large the input is. So to figure it out, we read data into memory\n        # up until the threshold and compare how much data was actually read\n        # against the threshold.\n        fileobj = transfer_future.meta.call_args.fileobj\n        threshold = config.multipart_threshold\n        self._initial_data = self._read(fileobj, threshold, False)\n        if len(self._initial_data) < threshold:\n            return False\n        else:\n            return True\n\n    def get_put_object_body(self, transfer_future):\n        callbacks = self._get_progress_callbacks(transfer_future)\n        close_callbacks = self._get_close_callbacks(callbacks)\n        fileobj = transfer_future.meta.call_args.fileobj\n\n        body = self._wrap_data(\n            self._initial_data + fileobj.read(), callbacks, close_callbacks\n        )\n\n        # Zero out the stored data so we don't have additional copies\n        # hanging around in memory.\n        self._initial_data = None\n        return body\n\n    def yield_upload_part_bodies(self, transfer_future, chunksize):\n        file_object = transfer_future.meta.call_args.fileobj\n        part_number = 0\n\n        # Continue reading parts from the file-like object until it is empty.\n        while True:\n            callbacks = self._get_progress_callbacks(transfer_future)\n            close_callbacks = self._get_close_callbacks(callbacks)\n            part_number += 1\n            part_content = self._read(file_object, chunksize)\n            if not part_content:\n                break\n            part_object = self._wrap_data(\n                part_content, callbacks, close_callbacks\n            )\n\n            # Zero out part_content to avoid hanging on to additional data.\n            part_content = None\n            yield part_number, part_object\n\n    def _read(self, fileobj, amount, truncate=True):\n        \"\"\"\n        Reads a specific amount of data from a stream and returns it. If there\n        is any data in initial_data, that will be popped out first.\n\n        :type fileobj: A file-like object that implements read\n        :param fileobj: The stream to read from.\n\n        :type amount: int\n        :param amount: The number of bytes to read from the stream.\n\n        :type truncate: bool\n        :param truncate: Whether or not to truncate initial_data after\n            reading from it.\n\n        :return: Generator which generates part bodies from the initial data.\n        \"\"\"\n        # If the the initial data is empty, we simply read from the fileobj\n        if len(self._initial_data) == 0:\n            return fileobj.read(amount)\n\n        # If the requested number of bytes is less than the amount of\n        # initial data, pull entirely from initial data.\n        if amount <= len(self._initial_data):\n            data = self._initial_data[:amount]\n            # Truncate initial data so we don't hang onto the data longer\n            # than we need.\n            if truncate:\n                self._initial_data = self._initial_data[amount:]\n            return data\n\n        # At this point there is some initial data left, but not enough to\n        # satisfy the number of bytes requested. Pull out the remaining\n        # initial data and read the rest from the fileobj.\n        amount_to_read = amount - len(self._initial_data)\n        data = self._initial_data + fileobj.read(amount_to_read)\n\n        # Zero out initial data so we don't hang onto the data any more.\n        if truncate:\n            self._initial_data = b''\n        return data\n\n    def _wrap_data(self, data, callbacks, close_callbacks):\n        \"\"\"\n        Wraps data with the interrupt reader and the file chunk reader.\n\n        :type data: bytes\n        :param data: The data to wrap.\n\n        :type callbacks: list\n        :param callbacks: The callbacks associated with the transfer future.\n\n        :type close_callbacks: list\n        :param close_callbacks: The callbacks to be called when closing the\n            wrapper for the data.\n\n        :return: Fully wrapped data.\n        \"\"\"\n        fileobj = self._wrap_fileobj(BytesIO(data))\n        return self._osutil.open_file_chunk_reader_from_fileobj(\n            fileobj=fileobj,\n            chunk_size=len(data),\n            full_file_size=len(data),\n            callbacks=callbacks,\n            close_callbacks=close_callbacks,\n        )\n\n\nclass UploadSubmissionTask(SubmissionTask):\n    \"\"\"Task for submitting tasks to execute an upload\"\"\"\n\n    UPLOAD_PART_ARGS = [\n        'ChecksumAlgorithm',\n        'SSECustomerKey',\n        'SSECustomerAlgorithm',\n        'SSECustomerKeyMD5',\n        'RequestPayer',\n        'ExpectedBucketOwner',\n    ]\n\n    COMPLETE_MULTIPART_ARGS = [\n        'SSECustomerKey',\n        'SSECustomerAlgorithm',\n        'SSECustomerKeyMD5',\n        'RequestPayer',\n        'ExpectedBucketOwner',\n    ]\n\n    def _get_upload_input_manager_cls(self, transfer_future):\n        \"\"\"Retrieves a class for managing input for an upload based on file type\n\n        :type transfer_future: s3transfer.futures.TransferFuture\n        :param transfer_future: The transfer future for the request\n\n        :rtype: class of UploadInputManager\n        :returns: The appropriate class to use for managing a specific type of\n            input for uploads.\n        \"\"\"\n        upload_manager_resolver_chain = [\n            UploadFilenameInputManager,\n            UploadSeekableInputManager,\n            UploadNonSeekableInputManager,\n        ]\n\n        fileobj = transfer_future.meta.call_args.fileobj\n        for upload_manager_cls in upload_manager_resolver_chain:\n            if upload_manager_cls.is_compatible(fileobj):\n                return upload_manager_cls\n        raise RuntimeError(\n            'Input {} of type: {} is not supported.'.format(\n                fileobj, type(fileobj)\n            )\n        )\n\n    def _submit(\n        self,\n        client,\n        config,\n        osutil,\n        request_executor,\n        transfer_future,\n        bandwidth_limiter=None,\n    ):\n        \"\"\"\n        :param client: The client associated with the transfer manager\n\n        :type config: s3transfer.manager.TransferConfig\n        :param config: The transfer config associated with the transfer\n            manager\n\n        :type osutil: s3transfer.utils.OSUtil\n        :param osutil: The os utility associated to the transfer manager\n\n        :type request_executor: s3transfer.futures.BoundedExecutor\n        :param request_executor: The request executor associated with the\n            transfer manager\n\n        :type transfer_future: s3transfer.futures.TransferFuture\n        :param transfer_future: The transfer future associated with the\n            transfer request that tasks are being submitted for\n        \"\"\"\n        upload_input_manager = self._get_upload_input_manager_cls(\n            transfer_future\n        )(osutil, self._transfer_coordinator, bandwidth_limiter)\n\n        # Determine the size if it was not provided\n        if transfer_future.meta.size is None:\n            upload_input_manager.provide_transfer_size(transfer_future)\n\n        # Do a multipart upload if needed, otherwise do a regular put object.\n        if not upload_input_manager.requires_multipart_upload(\n            transfer_future, config\n        ):\n            self._submit_upload_request(\n                client,\n                config,\n                osutil,\n                request_executor,\n                transfer_future,\n                upload_input_manager,\n            )\n        else:\n            self._submit_multipart_request(\n                client,\n                config,\n                osutil,\n                request_executor,\n                transfer_future,\n                upload_input_manager,\n            )\n\n    def _submit_upload_request(\n        self,\n        client,\n        config,\n        osutil,\n        request_executor,\n        transfer_future,\n        upload_input_manager,\n    ):\n        call_args = transfer_future.meta.call_args\n\n        # Get any tags that need to be associated to the put object task\n        put_object_tag = self._get_upload_task_tag(\n            upload_input_manager, 'put_object'\n        )\n\n        # Submit the request of a single upload.\n        self._transfer_coordinator.submit(\n            request_executor,\n            PutObjectTask(\n                transfer_coordinator=self._transfer_coordinator,\n                main_kwargs={\n                    'client': client,\n                    'fileobj': upload_input_manager.get_put_object_body(\n                        transfer_future\n                    ),\n                    'bucket': call_args.bucket,\n                    'key': call_args.key,\n                    'extra_args': call_args.extra_args,\n                },\n                is_final=True,\n            ),\n            tag=put_object_tag,\n        )\n\n    def _submit_multipart_request(\n        self,\n        client,\n        config,\n        osutil,\n        request_executor,\n        transfer_future,\n        upload_input_manager,\n    ):\n        call_args = transfer_future.meta.call_args\n\n        # Submit the request to create a multipart upload.\n        create_multipart_future = self._transfer_coordinator.submit(\n            request_executor,\n            CreateMultipartUploadTask(\n                transfer_coordinator=self._transfer_coordinator,\n                main_kwargs={\n                    'client': client,\n                    'bucket': call_args.bucket,\n                    'key': call_args.key,\n                    'extra_args': call_args.extra_args,\n                },\n            ),\n        )\n\n        # Submit requests to upload the parts of the file.\n        part_futures = []\n        extra_part_args = self._extra_upload_part_args(call_args.extra_args)\n\n        # Get any tags that need to be associated to the submitted task\n        # for upload the data\n        upload_part_tag = self._get_upload_task_tag(\n            upload_input_manager, 'upload_part'\n        )\n\n        size = transfer_future.meta.size\n        adjuster = ChunksizeAdjuster()\n        chunksize = adjuster.adjust_chunksize(config.multipart_chunksize, size)\n        part_iterator = upload_input_manager.yield_upload_part_bodies(\n            transfer_future, chunksize\n        )\n\n        for part_number, fileobj in part_iterator:\n            part_futures.append(\n                self._transfer_coordinator.submit(\n                    request_executor,\n                    UploadPartTask(\n                        transfer_coordinator=self._transfer_coordinator,\n                        main_kwargs={\n                            'client': client,\n                            'fileobj': fileobj,\n                            'bucket': call_args.bucket,\n                            'key': call_args.key,\n                            'part_number': part_number,\n                            'extra_args': extra_part_args,\n                        },\n                        pending_main_kwargs={\n                            'upload_id': create_multipart_future\n                        },\n                    ),\n                    tag=upload_part_tag,\n                )\n            )\n\n        complete_multipart_extra_args = self._extra_complete_multipart_args(\n            call_args.extra_args\n        )\n        # Submit the request to complete the multipart upload.\n        self._transfer_coordinator.submit(\n            request_executor,\n            CompleteMultipartUploadTask(\n                transfer_coordinator=self._transfer_coordinator,\n                main_kwargs={\n                    'client': client,\n                    'bucket': call_args.bucket,\n                    'key': call_args.key,\n                    'extra_args': complete_multipart_extra_args,\n                },\n                pending_main_kwargs={\n                    'upload_id': create_multipart_future,\n                    'parts': part_futures,\n                },\n                is_final=True,\n            ),\n        )\n\n    def _extra_upload_part_args(self, extra_args):\n        # Only the args in UPLOAD_PART_ARGS actually need to be passed\n        # onto the upload_part calls.\n        return get_filtered_dict(extra_args, self.UPLOAD_PART_ARGS)\n\n    def _extra_complete_multipart_args(self, extra_args):\n        return get_filtered_dict(extra_args, self.COMPLETE_MULTIPART_ARGS)\n\n    def _get_upload_task_tag(self, upload_input_manager, operation_name):\n        tag = None\n        if upload_input_manager.stores_body_in_memory(operation_name):\n            tag = IN_MEMORY_UPLOAD_TAG\n        return tag\n\n\nclass PutObjectTask(Task):\n    \"\"\"Task to do a nonmultipart upload\"\"\"\n\n    def _main(self, client, fileobj, bucket, key, extra_args):\n        \"\"\"\n        :param client: The client to use when calling PutObject\n        :param fileobj: The file to upload.\n        :param bucket: The name of the bucket to upload to\n        :param key: The name of the key to upload to\n        :param extra_args: A dictionary of any extra arguments that may be\n            used in the upload.\n        \"\"\"\n        with fileobj as body:\n            client.put_object(Bucket=bucket, Key=key, Body=body, **extra_args)\n\n\nclass UploadPartTask(Task):\n    \"\"\"Task to upload a part in a multipart upload\"\"\"\n\n    def _main(\n        self, client, fileobj, bucket, key, upload_id, part_number, extra_args\n    ):\n        \"\"\"\n        :param client: The client to use when calling PutObject\n        :param fileobj: The file to upload.\n        :param bucket: The name of the bucket to upload to\n        :param key: The name of the key to upload to\n        :param upload_id: The id of the upload\n        :param part_number: The number representing the part of the multipart\n            upload\n        :param extra_args: A dictionary of any extra arguments that may be\n            used in the upload.\n\n        :rtype: dict\n        :returns: A dictionary representing a part::\n\n            {'Etag': etag_value, 'PartNumber': part_number}\n\n            This value can be appended to a list to be used to complete\n            the multipart upload.\n        \"\"\"\n        with fileobj as body:\n            response = client.upload_part(\n                Bucket=bucket,\n                Key=key,\n                UploadId=upload_id,\n                PartNumber=part_number,\n                Body=body,\n                **extra_args,\n            )\n        etag = response['ETag']\n        part_metadata = {'ETag': etag, 'PartNumber': part_number}\n        if 'ChecksumAlgorithm' in extra_args:\n            algorithm_name = extra_args['ChecksumAlgorithm'].upper()\n            checksum_member = f'Checksum{algorithm_name}'\n            if checksum_member in response:\n                part_metadata[checksum_member] = response[checksum_member]\n        return part_metadata\n", "s3transfer/futures.py": "# Copyright 2016 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport copy\nimport logging\nimport sys\nimport threading\nfrom collections import namedtuple\nfrom concurrent import futures\n\nfrom s3transfer.compat import MAXINT\nfrom s3transfer.exceptions import CancelledError, TransferNotDoneError\nfrom s3transfer.utils import FunctionContainer, TaskSemaphore\n\nlogger = logging.getLogger(__name__)\n\n\nclass BaseTransferFuture:\n    @property\n    def meta(self):\n        \"\"\"The metadata associated to the TransferFuture\"\"\"\n        raise NotImplementedError('meta')\n\n    def done(self):\n        \"\"\"Determines if a TransferFuture has completed\n\n        :returns: True if completed. False, otherwise.\n        \"\"\"\n        raise NotImplementedError('done()')\n\n    def result(self):\n        \"\"\"Waits until TransferFuture is done and returns the result\n\n        If the TransferFuture succeeded, it will return the result. If the\n        TransferFuture failed, it will raise the exception associated to the\n        failure.\n        \"\"\"\n        raise NotImplementedError('result()')\n\n    def cancel(self):\n        \"\"\"Cancels the request associated with the TransferFuture\"\"\"\n        raise NotImplementedError('cancel()')\n\n\nclass BaseTransferMeta:\n    @property\n    def call_args(self):\n        \"\"\"The call args used in the transfer request\"\"\"\n        raise NotImplementedError('call_args')\n\n    @property\n    def transfer_id(self):\n        \"\"\"The unique id of the transfer\"\"\"\n        raise NotImplementedError('transfer_id')\n\n    @property\n    def user_context(self):\n        \"\"\"A dictionary that requesters can store data in\"\"\"\n        raise NotImplementedError('user_context')\n\n\nclass TransferFuture(BaseTransferFuture):\n    def __init__(self, meta=None, coordinator=None):\n        \"\"\"The future associated to a submitted transfer request\n\n        :type meta: TransferMeta\n        :param meta: The metadata associated to the request. This object\n            is visible to the requester.\n\n        :type coordinator: TransferCoordinator\n        :param coordinator: The coordinator associated to the request. This\n            object is not visible to the requester.\n        \"\"\"\n        self._meta = meta\n        if meta is None:\n            self._meta = TransferMeta()\n\n        self._coordinator = coordinator\n        if coordinator is None:\n            self._coordinator = TransferCoordinator()\n\n    @property\n    def meta(self):\n        return self._meta\n\n    def done(self):\n        return self._coordinator.done()\n\n    def result(self):\n        try:\n            # Usually the result() method blocks until the transfer is done,\n            # however if a KeyboardInterrupt is raised we want want to exit\n            # out of this and propagate the exception.\n            return self._coordinator.result()\n        except KeyboardInterrupt as e:\n            self.cancel()\n            raise e\n\n    def cancel(self):\n        self._coordinator.cancel()\n\n    def set_exception(self, exception):\n        \"\"\"Sets the exception on the future.\"\"\"\n        if not self.done():\n            raise TransferNotDoneError(\n                'set_exception can only be called once the transfer is '\n                'complete.'\n            )\n        self._coordinator.set_exception(exception, override=True)\n\n\nclass TransferMeta(BaseTransferMeta):\n    \"\"\"Holds metadata about the TransferFuture\"\"\"\n\n    def __init__(self, call_args=None, transfer_id=None):\n        self._call_args = call_args\n        self._transfer_id = transfer_id\n        self._size = None\n        self._user_context = {}\n\n    @property\n    def call_args(self):\n        \"\"\"The call args used in the transfer request\"\"\"\n        return self._call_args\n\n    @property\n    def transfer_id(self):\n        \"\"\"The unique id of the transfer\"\"\"\n        return self._transfer_id\n\n    @property\n    def size(self):\n        \"\"\"The size of the transfer request if known\"\"\"\n        return self._size\n\n    @property\n    def user_context(self):\n        \"\"\"A dictionary that requesters can store data in\"\"\"\n        return self._user_context\n\n    def provide_transfer_size(self, size):\n        \"\"\"A method to provide the size of a transfer request\n\n        By providing this value, the TransferManager will not try to\n        call HeadObject or use the use OS to determine the size of the\n        transfer.\n        \"\"\"\n        self._size = size\n\n\nclass TransferCoordinator:\n    \"\"\"A helper class for managing TransferFuture\"\"\"\n\n    def __init__(self, transfer_id=None):\n        self.transfer_id = transfer_id\n        self._status = 'not-started'\n        self._result = None\n        self._exception = None\n        self._associated_futures = set()\n        self._failure_cleanups = []\n        self._done_callbacks = []\n        self._done_event = threading.Event()\n        self._lock = threading.Lock()\n        self._associated_futures_lock = threading.Lock()\n        self._done_callbacks_lock = threading.Lock()\n        self._failure_cleanups_lock = threading.Lock()\n\n    def __repr__(self):\n        return '{}(transfer_id={})'.format(\n            self.__class__.__name__, self.transfer_id\n        )\n\n    @property\n    def exception(self):\n        return self._exception\n\n    @property\n    def associated_futures(self):\n        \"\"\"The list of futures associated to the inprogress TransferFuture\n\n        Once the transfer finishes this list becomes empty as the transfer\n        is considered done and there should be no running futures left.\n        \"\"\"\n        with self._associated_futures_lock:\n            # We return a copy of the list because we do not want to\n            # processing the returned list while another thread is adding\n            # more futures to the actual list.\n            return copy.copy(self._associated_futures)\n\n    @property\n    def failure_cleanups(self):\n        \"\"\"The list of callbacks to call when the TransferFuture fails\"\"\"\n        return self._failure_cleanups\n\n    @property\n    def status(self):\n        \"\"\"The status of the TransferFuture\n\n        The currently supported states are:\n            * not-started - Has yet to start. If in this state, a transfer\n              can be canceled immediately and nothing will happen.\n            * queued - SubmissionTask is about to submit tasks\n            * running - Is inprogress. In-progress as of now means that\n              the SubmissionTask that runs the transfer is being executed. So\n              there is no guarantee any transfer requests had been made to\n              S3 if this state is reached.\n            * cancelled - Was cancelled\n            * failed - An exception other than CancelledError was thrown\n            * success - No exceptions were thrown and is done.\n        \"\"\"\n        return self._status\n\n    def set_result(self, result):\n        \"\"\"Set a result for the TransferFuture\n\n        Implies that the TransferFuture succeeded. This will always set a\n        result because it is invoked on the final task where there is only\n        ever one final task and it is ran at the very end of a transfer\n        process. So if a result is being set for this final task, the transfer\n        succeeded even if something came a long and canceled the transfer\n        on the final task.\n        \"\"\"\n        with self._lock:\n            self._exception = None\n            self._result = result\n            self._status = 'success'\n\n    def set_exception(self, exception, override=False):\n        \"\"\"Set an exception for the TransferFuture\n\n        Implies the TransferFuture failed.\n\n        :param exception: The exception that cause the transfer to fail.\n        :param override: If True, override any existing state.\n        \"\"\"\n        with self._lock:\n            if not self.done() or override:\n                self._exception = exception\n                self._status = 'failed'\n\n    def result(self):\n        \"\"\"Waits until TransferFuture is done and returns the result\n\n        If the TransferFuture succeeded, it will return the result. If the\n        TransferFuture failed, it will raise the exception associated to the\n        failure.\n        \"\"\"\n        # Doing a wait() with no timeout cannot be interrupted in python2 but\n        # can be interrupted in python3 so we just wait with the largest\n        # possible value integer value, which is on the scale of billions of\n        # years...\n        self._done_event.wait(MAXINT)\n\n        # Once done waiting, raise an exception if present or return the\n        # final result.\n        if self._exception:\n            raise self._exception\n        return self._result\n\n    def cancel(self, msg='', exc_type=CancelledError):\n        \"\"\"Cancels the TransferFuture\n\n        :param msg: The message to attach to the cancellation\n        :param exc_type: The type of exception to set for the cancellation\n        \"\"\"\n        with self._lock:\n            if not self.done():\n                should_announce_done = False\n                logger.debug('%s cancel(%s) called', self, msg)\n                self._exception = exc_type(msg)\n                if self._status == 'not-started':\n                    should_announce_done = True\n                self._status = 'cancelled'\n                if should_announce_done:\n                    self.announce_done()\n\n    def set_status_to_queued(self):\n        \"\"\"Sets the TransferFutrue's status to running\"\"\"\n        self._transition_to_non_done_state('queued')\n\n    def set_status_to_running(self):\n        \"\"\"Sets the TransferFuture's status to running\"\"\"\n        self._transition_to_non_done_state('running')\n\n    def _transition_to_non_done_state(self, desired_state):\n        with self._lock:\n            if self.done():\n                raise RuntimeError(\n                    'Unable to transition from done state %s to non-done '\n                    'state %s.' % (self.status, desired_state)\n                )\n            self._status = desired_state\n\n    def submit(self, executor, task, tag=None):\n        \"\"\"Submits a task to a provided executor\n\n        :type executor: s3transfer.futures.BoundedExecutor\n        :param executor: The executor to submit the callable to\n\n        :type task: s3transfer.tasks.Task\n        :param task: The task to submit to the executor\n\n        :type tag: s3transfer.futures.TaskTag\n        :param tag: A tag to associate to the submitted task\n\n        :rtype: concurrent.futures.Future\n        :returns: A future representing the submitted task\n        \"\"\"\n        logger.debug(\n            \"Submitting task {} to executor {} for transfer request: {}.\".format(\n                task, executor, self.transfer_id\n            )\n        )\n        future = executor.submit(task, tag=tag)\n        # Add this created future to the list of associated future just\n        # in case it is needed during cleanups.\n        self.add_associated_future(future)\n        future.add_done_callback(\n            FunctionContainer(self.remove_associated_future, future)\n        )\n        return future\n\n    def done(self):\n        \"\"\"Determines if a TransferFuture has completed\n\n        :returns: False if status is equal to 'failed', 'cancelled', or\n            'success'. True, otherwise\n        \"\"\"\n        return self.status in ['failed', 'cancelled', 'success']\n\n    def add_associated_future(self, future):\n        \"\"\"Adds a future to be associated with the TransferFuture\"\"\"\n        with self._associated_futures_lock:\n            self._associated_futures.add(future)\n\n    def remove_associated_future(self, future):\n        \"\"\"Removes a future's association to the TransferFuture\"\"\"\n        with self._associated_futures_lock:\n            self._associated_futures.remove(future)\n\n    def add_done_callback(self, function, *args, **kwargs):\n        \"\"\"Add a done callback to be invoked when transfer is done\"\"\"\n        with self._done_callbacks_lock:\n            self._done_callbacks.append(\n                FunctionContainer(function, *args, **kwargs)\n            )\n\n    def add_failure_cleanup(self, function, *args, **kwargs):\n        \"\"\"Adds a callback to call upon failure\"\"\"\n        with self._failure_cleanups_lock:\n            self._failure_cleanups.append(\n                FunctionContainer(function, *args, **kwargs)\n            )\n\n    def announce_done(self):\n        \"\"\"Announce that future is done running and run associated callbacks\n\n        This will run any failure cleanups if the transfer failed if not\n        they have not been run, allows the result() to be unblocked, and will\n        run any done callbacks associated to the TransferFuture if they have\n        not already been ran.\n        \"\"\"\n        if self.status != 'success':\n            self._run_failure_cleanups()\n        self._done_event.set()\n        self._run_done_callbacks()\n\n    def _run_done_callbacks(self):\n        # Run the callbacks and remove the callbacks from the internal\n        # list so they do not get ran again if done is announced more than\n        # once.\n        with self._done_callbacks_lock:\n            self._run_callbacks(self._done_callbacks)\n            self._done_callbacks = []\n\n    def _run_failure_cleanups(self):\n        # Run the cleanup callbacks and remove the callbacks from the internal\n        # list so they do not get ran again if done is announced more than\n        # once.\n        with self._failure_cleanups_lock:\n            self._run_callbacks(self.failure_cleanups)\n            self._failure_cleanups = []\n\n    def _run_callbacks(self, callbacks):\n        for callback in callbacks:\n            self._run_callback(callback)\n\n    def _run_callback(self, callback):\n        try:\n            callback()\n        # We do not want a callback interrupting the process, especially\n        # in the failure cleanups. So log and catch, the exception.\n        except Exception:\n            logger.debug(\"Exception raised in %s.\" % callback, exc_info=True)\n\n\nclass BoundedExecutor:\n    EXECUTOR_CLS = futures.ThreadPoolExecutor\n\n    def __init__(\n        self, max_size, max_num_threads, tag_semaphores=None, executor_cls=None\n    ):\n        \"\"\"An executor implementation that has a maximum queued up tasks\n\n        The executor will block if the number of tasks that have been\n        submitted and is currently working on is past its maximum.\n\n        :params max_size: The maximum number of inflight futures. An inflight\n            future means that the task is either queued up or is currently\n            being executed. A size of None or 0 means that the executor will\n            have no bound in terms of the number of inflight futures.\n\n        :params max_num_threads: The maximum number of threads the executor\n            uses.\n\n        :type tag_semaphores: dict\n        :params tag_semaphores: A dictionary where the key is the name of the\n            tag and the value is the semaphore to use when limiting the\n            number of tasks the executor is processing at a time.\n\n        :type executor_cls: BaseExecutor\n        :param underlying_executor_cls: The executor class that\n            get bounded by this executor. If None is provided, the\n            concurrent.futures.ThreadPoolExecutor class is used.\n        \"\"\"\n        self._max_num_threads = max_num_threads\n        if executor_cls is None:\n            executor_cls = self.EXECUTOR_CLS\n        self._executor = executor_cls(max_workers=self._max_num_threads)\n        self._semaphore = TaskSemaphore(max_size)\n        self._tag_semaphores = tag_semaphores\n\n    def submit(self, task, tag=None, block=True):\n        \"\"\"Submit a task to complete\n\n        :type task: s3transfer.tasks.Task\n        :param task: The task to run __call__ on\n\n\n        :type tag: s3transfer.futures.TaskTag\n        :param tag: An optional tag to associate to the task. This\n            is used to override which semaphore to use.\n\n        :type block: boolean\n        :param block: True if to wait till it is possible to submit a task.\n            False, if not to wait and raise an error if not able to submit\n            a task.\n\n        :returns: The future associated to the submitted task\n        \"\"\"\n        semaphore = self._semaphore\n        # If a tag was provided, use the semaphore associated to that\n        # tag.\n        if tag:\n            semaphore = self._tag_semaphores[tag]\n\n        # Call acquire on the semaphore.\n        acquire_token = semaphore.acquire(task.transfer_id, block)\n        # Create a callback to invoke when task is done in order to call\n        # release on the semaphore.\n        release_callback = FunctionContainer(\n            semaphore.release, task.transfer_id, acquire_token\n        )\n        # Submit the task to the underlying executor.\n        future = ExecutorFuture(self._executor.submit(task))\n        # Add the Semaphore.release() callback to the future such that\n        # it is invoked once the future completes.\n        future.add_done_callback(release_callback)\n        return future\n\n    def shutdown(self, wait=True):\n        self._executor.shutdown(wait)\n\n\nclass ExecutorFuture:\n    def __init__(self, future):\n        \"\"\"A future returned from the executor\n\n        Currently, it is just a wrapper around a concurrent.futures.Future.\n        However, this can eventually grow to implement the needed functionality\n        of concurrent.futures.Future if we move off of the library and not\n        affect the rest of the codebase.\n\n        :type future: concurrent.futures.Future\n        :param future: The underlying future\n        \"\"\"\n        self._future = future\n\n    def result(self):\n        return self._future.result()\n\n    def add_done_callback(self, fn):\n        \"\"\"Adds a callback to be completed once future is done\n\n        :param fn: A callable that takes no arguments. Note that is different\n            than concurrent.futures.Future.add_done_callback that requires\n            a single argument for the future.\n        \"\"\"\n\n        # The done callback for concurrent.futures.Future will always pass a\n        # the future in as the only argument. So we need to create the\n        # proper signature wrapper that will invoke the callback provided.\n        def done_callback(future_passed_to_callback):\n            return fn()\n\n        self._future.add_done_callback(done_callback)\n\n    def done(self):\n        return self._future.done()\n\n\nclass BaseExecutor:\n    \"\"\"Base Executor class implementation needed to work with s3transfer\"\"\"\n\n    def __init__(self, max_workers=None):\n        pass\n\n    def submit(self, fn, *args, **kwargs):\n        raise NotImplementedError('submit()')\n\n    def shutdown(self, wait=True):\n        raise NotImplementedError('shutdown()')\n\n\nclass NonThreadedExecutor(BaseExecutor):\n    \"\"\"A drop-in replacement non-threaded version of ThreadPoolExecutor\"\"\"\n\n    def submit(self, fn, *args, **kwargs):\n        future = NonThreadedExecutorFuture()\n        try:\n            result = fn(*args, **kwargs)\n            future.set_result(result)\n        except Exception:\n            e, tb = sys.exc_info()[1:]\n            logger.debug(\n                'Setting exception for %s to %s with traceback %s',\n                future,\n                e,\n                tb,\n            )\n            future.set_exception_info(e, tb)\n        return future\n\n    def shutdown(self, wait=True):\n        pass\n\n\nclass NonThreadedExecutorFuture:\n    \"\"\"The Future returned from NonThreadedExecutor\n\n    Note that this future is **not** thread-safe as it is being used\n    from the context of a non-threaded environment.\n    \"\"\"\n\n    def __init__(self):\n        self._result = None\n        self._exception = None\n        self._traceback = None\n        self._done = False\n        self._done_callbacks = []\n\n    def set_result(self, result):\n        self._result = result\n        self._set_done()\n\n    def set_exception_info(self, exception, traceback):\n        self._exception = exception\n        self._traceback = traceback\n        self._set_done()\n\n    def result(self, timeout=None):\n        if self._exception:\n            raise self._exception.with_traceback(self._traceback)\n        return self._result\n\n    def _set_done(self):\n        self._done = True\n        for done_callback in self._done_callbacks:\n            self._invoke_done_callback(done_callback)\n        self._done_callbacks = []\n\n    def _invoke_done_callback(self, done_callback):\n        return done_callback(self)\n\n    def done(self):\n        return self._done\n\n    def add_done_callback(self, fn):\n        if self._done:\n            self._invoke_done_callback(fn)\n        else:\n            self._done_callbacks.append(fn)\n\n\nTaskTag = namedtuple('TaskTag', ['name'])\n\nIN_MEMORY_UPLOAD_TAG = TaskTag('in_memory_upload')\nIN_MEMORY_DOWNLOAD_TAG = TaskTag('in_memory_download')\n", "s3transfer/constants.py": "# Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport s3transfer\n\nKB = 1024\nMB = KB * KB\nGB = MB * KB\n\nALLOWED_DOWNLOAD_ARGS = [\n    'ChecksumMode',\n    'VersionId',\n    'SSECustomerAlgorithm',\n    'SSECustomerKey',\n    'SSECustomerKeyMD5',\n    'RequestPayer',\n    'ExpectedBucketOwner',\n]\n\nUSER_AGENT = 's3transfer/%s' % s3transfer.__version__\nPROCESS_USER_AGENT = '%s processpool' % USER_AGENT\n", "s3transfer/copies.py": "# Copyright 2016 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport copy\nimport math\n\nfrom s3transfer.tasks import (\n    CompleteMultipartUploadTask,\n    CreateMultipartUploadTask,\n    SubmissionTask,\n    Task,\n)\nfrom s3transfer.utils import (\n    ChunksizeAdjuster,\n    calculate_range_parameter,\n    get_callbacks,\n    get_filtered_dict,\n)\n\n\nclass CopySubmissionTask(SubmissionTask):\n    \"\"\"Task for submitting tasks to execute a copy\"\"\"\n\n    EXTRA_ARGS_TO_HEAD_ARGS_MAPPING = {\n        'CopySourceIfMatch': 'IfMatch',\n        'CopySourceIfModifiedSince': 'IfModifiedSince',\n        'CopySourceIfNoneMatch': 'IfNoneMatch',\n        'CopySourceIfUnmodifiedSince': 'IfUnmodifiedSince',\n        'CopySourceSSECustomerKey': 'SSECustomerKey',\n        'CopySourceSSECustomerAlgorithm': 'SSECustomerAlgorithm',\n        'CopySourceSSECustomerKeyMD5': 'SSECustomerKeyMD5',\n        'RequestPayer': 'RequestPayer',\n        'ExpectedBucketOwner': 'ExpectedBucketOwner',\n    }\n\n    UPLOAD_PART_COPY_ARGS = [\n        'CopySourceIfMatch',\n        'CopySourceIfModifiedSince',\n        'CopySourceIfNoneMatch',\n        'CopySourceIfUnmodifiedSince',\n        'CopySourceSSECustomerKey',\n        'CopySourceSSECustomerAlgorithm',\n        'CopySourceSSECustomerKeyMD5',\n        'SSECustomerKey',\n        'SSECustomerAlgorithm',\n        'SSECustomerKeyMD5',\n        'RequestPayer',\n        'ExpectedBucketOwner',\n    ]\n\n    CREATE_MULTIPART_ARGS_BLACKLIST = [\n        'CopySourceIfMatch',\n        'CopySourceIfModifiedSince',\n        'CopySourceIfNoneMatch',\n        'CopySourceIfUnmodifiedSince',\n        'CopySourceSSECustomerKey',\n        'CopySourceSSECustomerAlgorithm',\n        'CopySourceSSECustomerKeyMD5',\n        'MetadataDirective',\n        'TaggingDirective',\n    ]\n\n    COMPLETE_MULTIPART_ARGS = [\n        'SSECustomerKey',\n        'SSECustomerAlgorithm',\n        'SSECustomerKeyMD5',\n        'RequestPayer',\n        'ExpectedBucketOwner',\n    ]\n\n    def _submit(\n        self, client, config, osutil, request_executor, transfer_future\n    ):\n        \"\"\"\n        :param client: The client associated with the transfer manager\n\n        :type config: s3transfer.manager.TransferConfig\n        :param config: The transfer config associated with the transfer\n            manager\n\n        :type osutil: s3transfer.utils.OSUtil\n        :param osutil: The os utility associated to the transfer manager\n\n        :type request_executor: s3transfer.futures.BoundedExecutor\n        :param request_executor: The request executor associated with the\n            transfer manager\n\n        :type transfer_future: s3transfer.futures.TransferFuture\n        :param transfer_future: The transfer future associated with the\n            transfer request that tasks are being submitted for\n        \"\"\"\n        # Determine the size if it was not provided\n        if transfer_future.meta.size is None:\n            # If a size was not provided figure out the size for the\n            # user. Note that we will only use the client provided to\n            # the TransferManager. If the object is outside of the region\n            # of the client, they may have to provide the file size themselves\n            # with a completely new client.\n            call_args = transfer_future.meta.call_args\n            head_object_request = (\n                self._get_head_object_request_from_copy_source(\n                    call_args.copy_source\n                )\n            )\n            extra_args = call_args.extra_args\n\n            # Map any values that may be used in the head object that is\n            # used in the copy object\n            for param, value in extra_args.items():\n                if param in self.EXTRA_ARGS_TO_HEAD_ARGS_MAPPING:\n                    head_object_request[\n                        self.EXTRA_ARGS_TO_HEAD_ARGS_MAPPING[param]\n                    ] = value\n\n            response = call_args.source_client.head_object(\n                **head_object_request\n            )\n            transfer_future.meta.provide_transfer_size(\n                response['ContentLength']\n            )\n\n        # If it is greater than threshold do a multipart copy, otherwise\n        # do a regular copy object.\n        if transfer_future.meta.size < config.multipart_threshold:\n            self._submit_copy_request(\n                client, config, osutil, request_executor, transfer_future\n            )\n        else:\n            self._submit_multipart_request(\n                client, config, osutil, request_executor, transfer_future\n            )\n\n    def _submit_copy_request(\n        self, client, config, osutil, request_executor, transfer_future\n    ):\n        call_args = transfer_future.meta.call_args\n\n        # Get the needed progress callbacks for the task\n        progress_callbacks = get_callbacks(transfer_future, 'progress')\n\n        # Submit the request of a single copy.\n        self._transfer_coordinator.submit(\n            request_executor,\n            CopyObjectTask(\n                transfer_coordinator=self._transfer_coordinator,\n                main_kwargs={\n                    'client': client,\n                    'copy_source': call_args.copy_source,\n                    'bucket': call_args.bucket,\n                    'key': call_args.key,\n                    'extra_args': call_args.extra_args,\n                    'callbacks': progress_callbacks,\n                    'size': transfer_future.meta.size,\n                },\n                is_final=True,\n            ),\n        )\n\n    def _submit_multipart_request(\n        self, client, config, osutil, request_executor, transfer_future\n    ):\n        call_args = transfer_future.meta.call_args\n\n        # Submit the request to create a multipart upload and make sure it\n        # does not include any of the arguments used for copy part.\n        create_multipart_extra_args = {}\n        for param, val in call_args.extra_args.items():\n            if param not in self.CREATE_MULTIPART_ARGS_BLACKLIST:\n                create_multipart_extra_args[param] = val\n\n        create_multipart_future = self._transfer_coordinator.submit(\n            request_executor,\n            CreateMultipartUploadTask(\n                transfer_coordinator=self._transfer_coordinator,\n                main_kwargs={\n                    'client': client,\n                    'bucket': call_args.bucket,\n                    'key': call_args.key,\n                    'extra_args': create_multipart_extra_args,\n                },\n            ),\n        )\n\n        # Determine how many parts are needed based on filesize and\n        # desired chunksize.\n        part_size = config.multipart_chunksize\n        adjuster = ChunksizeAdjuster()\n        part_size = adjuster.adjust_chunksize(\n            part_size, transfer_future.meta.size\n        )\n        num_parts = int(\n            math.ceil(transfer_future.meta.size / float(part_size))\n        )\n\n        # Submit requests to upload the parts of the file.\n        part_futures = []\n        progress_callbacks = get_callbacks(transfer_future, 'progress')\n\n        for part_number in range(1, num_parts + 1):\n            extra_part_args = self._extra_upload_part_args(\n                call_args.extra_args\n            )\n            # The part number for upload part starts at 1 while the\n            # range parameter starts at zero, so just subtract 1 off of\n            # the part number\n            extra_part_args['CopySourceRange'] = calculate_range_parameter(\n                part_size,\n                part_number - 1,\n                num_parts,\n                transfer_future.meta.size,\n            )\n            # Get the size of the part copy as well for the progress\n            # callbacks.\n            size = self._get_transfer_size(\n                part_size,\n                part_number - 1,\n                num_parts,\n                transfer_future.meta.size,\n            )\n            # Get the checksum algorithm of the multipart request.\n            checksum_algorithm = call_args.extra_args.get(\"ChecksumAlgorithm\")\n            part_futures.append(\n                self._transfer_coordinator.submit(\n                    request_executor,\n                    CopyPartTask(\n                        transfer_coordinator=self._transfer_coordinator,\n                        main_kwargs={\n                            'client': client,\n                            'copy_source': call_args.copy_source,\n                            'bucket': call_args.bucket,\n                            'key': call_args.key,\n                            'part_number': part_number,\n                            'extra_args': extra_part_args,\n                            'callbacks': progress_callbacks,\n                            'size': size,\n                            'checksum_algorithm': checksum_algorithm,\n                        },\n                        pending_main_kwargs={\n                            'upload_id': create_multipart_future\n                        },\n                    ),\n                )\n            )\n\n        complete_multipart_extra_args = self._extra_complete_multipart_args(\n            call_args.extra_args\n        )\n        # Submit the request to complete the multipart upload.\n        self._transfer_coordinator.submit(\n            request_executor,\n            CompleteMultipartUploadTask(\n                transfer_coordinator=self._transfer_coordinator,\n                main_kwargs={\n                    'client': client,\n                    'bucket': call_args.bucket,\n                    'key': call_args.key,\n                    'extra_args': complete_multipart_extra_args,\n                },\n                pending_main_kwargs={\n                    'upload_id': create_multipart_future,\n                    'parts': part_futures,\n                },\n                is_final=True,\n            ),\n        )\n\n    def _get_head_object_request_from_copy_source(self, copy_source):\n        if isinstance(copy_source, dict):\n            return copy.copy(copy_source)\n        else:\n            raise TypeError(\n                'Expecting dictionary formatted: '\n                '{\"Bucket\": bucket_name, \"Key\": key} '\n                'but got %s or type %s.' % (copy_source, type(copy_source))\n            )\n\n    def _extra_upload_part_args(self, extra_args):\n        # Only the args in COPY_PART_ARGS actually need to be passed\n        # onto the upload_part_copy calls.\n        return get_filtered_dict(extra_args, self.UPLOAD_PART_COPY_ARGS)\n\n    def _extra_complete_multipart_args(self, extra_args):\n        return get_filtered_dict(extra_args, self.COMPLETE_MULTIPART_ARGS)\n\n    def _get_transfer_size(\n        self, part_size, part_index, num_parts, total_transfer_size\n    ):\n        if part_index == num_parts - 1:\n            # The last part may be different in size then the rest of the\n            # parts.\n            return total_transfer_size - (part_index * part_size)\n        return part_size\n\n\nclass CopyObjectTask(Task):\n    \"\"\"Task to do a nonmultipart copy\"\"\"\n\n    def _main(\n        self, client, copy_source, bucket, key, extra_args, callbacks, size\n    ):\n        \"\"\"\n        :param client: The client to use when calling PutObject\n        :param copy_source: The CopySource parameter to use\n        :param bucket: The name of the bucket to copy to\n        :param key: The name of the key to copy to\n        :param extra_args: A dictionary of any extra arguments that may be\n            used in the upload.\n        :param callbacks: List of callbacks to call after copy\n        :param size: The size of the transfer. This value is passed into\n            the callbacks\n\n        \"\"\"\n        client.copy_object(\n            CopySource=copy_source, Bucket=bucket, Key=key, **extra_args\n        )\n        for callback in callbacks:\n            callback(bytes_transferred=size)\n\n\nclass CopyPartTask(Task):\n    \"\"\"Task to upload a part in a multipart copy\"\"\"\n\n    def _main(\n        self,\n        client,\n        copy_source,\n        bucket,\n        key,\n        upload_id,\n        part_number,\n        extra_args,\n        callbacks,\n        size,\n        checksum_algorithm=None,\n    ):\n        \"\"\"\n        :param client: The client to use when calling PutObject\n        :param copy_source: The CopySource parameter to use\n        :param bucket: The name of the bucket to upload to\n        :param key: The name of the key to upload to\n        :param upload_id: The id of the upload\n        :param part_number: The number representing the part of the multipart\n            upload\n        :param extra_args: A dictionary of any extra arguments that may be\n            used in the upload.\n        :param callbacks: List of callbacks to call after copy part\n        :param size: The size of the transfer. This value is passed into\n            the callbacks\n        :param checksum_algorithm: The algorithm that was used to create the multipart\n            upload\n\n        :rtype: dict\n        :returns: A dictionary representing a part::\n\n            {'Etag': etag_value, 'PartNumber': part_number}\n\n            This value can be appended to a list to be used to complete\n            the multipart upload. If a checksum is in the response,\n            it will also be included.\n        \"\"\"\n        response = client.upload_part_copy(\n            CopySource=copy_source,\n            Bucket=bucket,\n            Key=key,\n            UploadId=upload_id,\n            PartNumber=part_number,\n            **extra_args,\n        )\n        for callback in callbacks:\n            callback(bytes_transferred=size)\n        etag = response['CopyPartResult']['ETag']\n        part_metadata = {'ETag': etag, 'PartNumber': part_number}\n        if checksum_algorithm:\n            checksum_member = f'Checksum{checksum_algorithm.upper()}'\n            if checksum_member in response['CopyPartResult']:\n                part_metadata[checksum_member] = response['CopyPartResult'][\n                    checksum_member\n                ]\n        return part_metadata\n", "s3transfer/__init__.py": "# Copyright 2016 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\"\"\"Abstractions over S3's upload/download operations.\n\nThis module provides high level abstractions for efficient\nuploads/downloads.  It handles several things for the user:\n\n* Automatically switching to multipart transfers when\n  a file is over a specific size threshold\n* Uploading/downloading a file in parallel\n* Throttling based on max bandwidth\n* Progress callbacks to monitor transfers\n* Retries.  While botocore handles retries for streaming uploads,\n  it is not possible for it to handle retries for streaming\n  downloads.  This module handles retries for both cases so\n  you don't need to implement any retry logic yourself.\n\nThis module has a reasonable set of defaults.  It also allows you\nto configure many aspects of the transfer process including:\n\n* Multipart threshold size\n* Max parallel downloads\n* Max bandwidth\n* Socket timeouts\n* Retry amounts\n\nThere is no support for s3->s3 multipart copies at this\ntime.\n\n\n.. _ref_s3transfer_usage:\n\nUsage\n=====\n\nThe simplest way to use this module is:\n\n.. code-block:: python\n\n    client = boto3.client('s3', 'us-west-2')\n    transfer = S3Transfer(client)\n    # Upload /tmp/myfile to s3://bucket/key\n    transfer.upload_file('/tmp/myfile', 'bucket', 'key')\n\n    # Download s3://bucket/key to /tmp/myfile\n    transfer.download_file('bucket', 'key', '/tmp/myfile')\n\nThe ``upload_file`` and ``download_file`` methods also accept\n``**kwargs``, which will be forwarded through to the corresponding\nclient operation.  Here are a few examples using ``upload_file``::\n\n    # Making the object public\n    transfer.upload_file('/tmp/myfile', 'bucket', 'key',\n                         extra_args={'ACL': 'public-read'})\n\n    # Setting metadata\n    transfer.upload_file('/tmp/myfile', 'bucket', 'key',\n                         extra_args={'Metadata': {'a': 'b', 'c': 'd'}})\n\n    # Setting content type\n    transfer.upload_file('/tmp/myfile.json', 'bucket', 'key',\n                         extra_args={'ContentType': \"application/json\"})\n\n\nThe ``S3Transfer`` class also supports progress callbacks so you can\nprovide transfer progress to users.  Both the ``upload_file`` and\n``download_file`` methods take an optional ``callback`` parameter.\nHere's an example of how to print a simple progress percentage\nto the user:\n\n.. code-block:: python\n\n    class ProgressPercentage(object):\n        def __init__(self, filename):\n            self._filename = filename\n            self._size = float(os.path.getsize(filename))\n            self._seen_so_far = 0\n            self._lock = threading.Lock()\n\n        def __call__(self, bytes_amount):\n            # To simplify we'll assume this is hooked up\n            # to a single filename.\n            with self._lock:\n                self._seen_so_far += bytes_amount\n                percentage = (self._seen_so_far / self._size) * 100\n                sys.stdout.write(\n                    \"\\r%s  %s / %s  (%.2f%%)\" % (self._filename, self._seen_so_far,\n                                                 self._size, percentage))\n                sys.stdout.flush()\n\n\n    transfer = S3Transfer(boto3.client('s3', 'us-west-2'))\n    # Upload /tmp/myfile to s3://bucket/key and print upload progress.\n    transfer.upload_file('/tmp/myfile', 'bucket', 'key',\n                         callback=ProgressPercentage('/tmp/myfile'))\n\n\n\nYou can also provide a TransferConfig object to the S3Transfer\nobject that gives you more fine grained control over the\ntransfer.  For example:\n\n.. code-block:: python\n\n    client = boto3.client('s3', 'us-west-2')\n    config = TransferConfig(\n        multipart_threshold=8 * 1024 * 1024,\n        max_concurrency=10,\n        num_download_attempts=10,\n    )\n    transfer = S3Transfer(client, config)\n    transfer.upload_file('/tmp/foo', 'bucket', 'key')\n\n\n\"\"\"\nimport concurrent.futures\nimport functools\nimport logging\nimport math\nimport os\nimport queue\nimport random\nimport socket\nimport string\nimport threading\n\nfrom botocore.compat import six  # noqa: F401\nfrom botocore.exceptions import IncompleteReadError, ResponseStreamingError\nfrom botocore.vendored.requests.packages.urllib3.exceptions import (\n    ReadTimeoutError,\n)\n\nimport s3transfer.compat\nfrom s3transfer.exceptions import RetriesExceededError, S3UploadFailedError\n\n__author__ = 'Amazon Web Services'\n__version__ = '0.10.2'\n\n\nclass NullHandler(logging.Handler):\n    def emit(self, record):\n        pass\n\n\nlogger = logging.getLogger(__name__)\nlogger.addHandler(NullHandler())\n\nMB = 1024 * 1024\nSHUTDOWN_SENTINEL = object()\n\n\ndef random_file_extension(num_digits=8):\n    return ''.join(random.choice(string.hexdigits) for _ in range(num_digits))\n\n\ndef disable_upload_callbacks(request, operation_name, **kwargs):\n    if operation_name in ['PutObject', 'UploadPart'] and hasattr(\n        request.body, 'disable_callback'\n    ):\n        request.body.disable_callback()\n\n\ndef enable_upload_callbacks(request, operation_name, **kwargs):\n    if operation_name in ['PutObject', 'UploadPart'] and hasattr(\n        request.body, 'enable_callback'\n    ):\n        request.body.enable_callback()\n\n\nclass QueueShutdownError(Exception):\n    pass\n\n\nclass ReadFileChunk:\n    def __init__(\n        self,\n        fileobj,\n        start_byte,\n        chunk_size,\n        full_file_size,\n        callback=None,\n        enable_callback=True,\n    ):\n        \"\"\"\n\n        Given a file object shown below:\n\n            |___________________________________________________|\n            0          |                 |                 full_file_size\n                       |----chunk_size---|\n                 start_byte\n\n        :type fileobj: file\n        :param fileobj: File like object\n\n        :type start_byte: int\n        :param start_byte: The first byte from which to start reading.\n\n        :type chunk_size: int\n        :param chunk_size: The max chunk size to read.  Trying to read\n            pass the end of the chunk size will behave like you've\n            reached the end of the file.\n\n        :type full_file_size: int\n        :param full_file_size: The entire content length associated\n            with ``fileobj``.\n\n        :type callback: function(amount_read)\n        :param callback: Called whenever data is read from this object.\n\n        \"\"\"\n        self._fileobj = fileobj\n        self._start_byte = start_byte\n        self._size = self._calculate_file_size(\n            self._fileobj,\n            requested_size=chunk_size,\n            start_byte=start_byte,\n            actual_file_size=full_file_size,\n        )\n        self._fileobj.seek(self._start_byte)\n        self._amount_read = 0\n        self._callback = callback\n        self._callback_enabled = enable_callback\n\n    @classmethod\n    def from_filename(\n        cls,\n        filename,\n        start_byte,\n        chunk_size,\n        callback=None,\n        enable_callback=True,\n    ):\n        \"\"\"Convenience factory function to create from a filename.\n\n        :type start_byte: int\n        :param start_byte: The first byte from which to start reading.\n\n        :type chunk_size: int\n        :param chunk_size: The max chunk size to read.  Trying to read\n            pass the end of the chunk size will behave like you've\n            reached the end of the file.\n\n        :type full_file_size: int\n        :param full_file_size: The entire content length associated\n            with ``fileobj``.\n\n        :type callback: function(amount_read)\n        :param callback: Called whenever data is read from this object.\n\n        :type enable_callback: bool\n        :param enable_callback: Indicate whether to invoke callback\n            during read() calls.\n\n        :rtype: ``ReadFileChunk``\n        :return: A new instance of ``ReadFileChunk``\n\n        \"\"\"\n        f = open(filename, 'rb')\n        file_size = os.fstat(f.fileno()).st_size\n        return cls(\n            f, start_byte, chunk_size, file_size, callback, enable_callback\n        )\n\n    def _calculate_file_size(\n        self, fileobj, requested_size, start_byte, actual_file_size\n    ):\n        max_chunk_size = actual_file_size - start_byte\n        return min(max_chunk_size, requested_size)\n\n    def read(self, amount=None):\n        if amount is None:\n            amount_to_read = self._size - self._amount_read\n        else:\n            amount_to_read = min(self._size - self._amount_read, amount)\n        data = self._fileobj.read(amount_to_read)\n        self._amount_read += len(data)\n        if self._callback is not None and self._callback_enabled:\n            self._callback(len(data))\n        return data\n\n    def enable_callback(self):\n        self._callback_enabled = True\n\n    def disable_callback(self):\n        self._callback_enabled = False\n\n    def seek(self, where):\n        self._fileobj.seek(self._start_byte + where)\n        if self._callback is not None and self._callback_enabled:\n            # To also rewind the callback() for an accurate progress report\n            self._callback(where - self._amount_read)\n        self._amount_read = where\n\n    def close(self):\n        self._fileobj.close()\n\n    def tell(self):\n        return self._amount_read\n\n    def __len__(self):\n        # __len__ is defined because requests will try to determine the length\n        # of the stream to set a content length.  In the normal case\n        # of the file it will just stat the file, but we need to change that\n        # behavior.  By providing a __len__, requests will use that instead\n        # of stat'ing the file.\n        return self._size\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, *args, **kwargs):\n        self.close()\n\n    def __iter__(self):\n        # This is a workaround for http://bugs.python.org/issue17575\n        # Basically httplib will try to iterate over the contents, even\n        # if its a file like object.  This wasn't noticed because we've\n        # already exhausted the stream so iterating over the file immediately\n        # stops, which is what we're simulating here.\n        return iter([])\n\n\nclass StreamReaderProgress:\n    \"\"\"Wrapper for a read only stream that adds progress callbacks.\"\"\"\n\n    def __init__(self, stream, callback=None):\n        self._stream = stream\n        self._callback = callback\n\n    def read(self, *args, **kwargs):\n        value = self._stream.read(*args, **kwargs)\n        if self._callback is not None:\n            self._callback(len(value))\n        return value\n\n\nclass OSUtils:\n    def get_file_size(self, filename):\n        return os.path.getsize(filename)\n\n    def open_file_chunk_reader(self, filename, start_byte, size, callback):\n        return ReadFileChunk.from_filename(\n            filename, start_byte, size, callback, enable_callback=False\n        )\n\n    def open(self, filename, mode):\n        return open(filename, mode)\n\n    def remove_file(self, filename):\n        \"\"\"Remove a file, noop if file does not exist.\"\"\"\n        # Unlike os.remove, if the file does not exist,\n        # then this method does nothing.\n        try:\n            os.remove(filename)\n        except OSError:\n            pass\n\n    def rename_file(self, current_filename, new_filename):\n        s3transfer.compat.rename_file(current_filename, new_filename)\n\n\nclass MultipartUploader:\n    # These are the extra_args that need to be forwarded onto\n    # subsequent upload_parts.\n    UPLOAD_PART_ARGS = [\n        'SSECustomerKey',\n        'SSECustomerAlgorithm',\n        'SSECustomerKeyMD5',\n        'RequestPayer',\n    ]\n\n    def __init__(\n        self,\n        client,\n        config,\n        osutil,\n        executor_cls=concurrent.futures.ThreadPoolExecutor,\n    ):\n        self._client = client\n        self._config = config\n        self._os = osutil\n        self._executor_cls = executor_cls\n\n    def _extra_upload_part_args(self, extra_args):\n        # Only the args in UPLOAD_PART_ARGS actually need to be passed\n        # onto the upload_part calls.\n        upload_parts_args = {}\n        for key, value in extra_args.items():\n            if key in self.UPLOAD_PART_ARGS:\n                upload_parts_args[key] = value\n        return upload_parts_args\n\n    def upload_file(self, filename, bucket, key, callback, extra_args):\n        response = self._client.create_multipart_upload(\n            Bucket=bucket, Key=key, **extra_args\n        )\n        upload_id = response['UploadId']\n        try:\n            parts = self._upload_parts(\n                upload_id, filename, bucket, key, callback, extra_args\n            )\n        except Exception as e:\n            logger.debug(\n                \"Exception raised while uploading parts, \"\n                \"aborting multipart upload.\",\n                exc_info=True,\n            )\n            self._client.abort_multipart_upload(\n                Bucket=bucket, Key=key, UploadId=upload_id\n            )\n            raise S3UploadFailedError(\n                \"Failed to upload {} to {}: {}\".format(\n                    filename, '/'.join([bucket, key]), e\n                )\n            )\n        self._client.complete_multipart_upload(\n            Bucket=bucket,\n            Key=key,\n            UploadId=upload_id,\n            MultipartUpload={'Parts': parts},\n        )\n\n    def _upload_parts(\n        self, upload_id, filename, bucket, key, callback, extra_args\n    ):\n        upload_parts_extra_args = self._extra_upload_part_args(extra_args)\n        parts = []\n        part_size = self._config.multipart_chunksize\n        num_parts = int(\n            math.ceil(self._os.get_file_size(filename) / float(part_size))\n        )\n        max_workers = self._config.max_concurrency\n        with self._executor_cls(max_workers=max_workers) as executor:\n            upload_partial = functools.partial(\n                self._upload_one_part,\n                filename,\n                bucket,\n                key,\n                upload_id,\n                part_size,\n                upload_parts_extra_args,\n                callback,\n            )\n            for part in executor.map(upload_partial, range(1, num_parts + 1)):\n                parts.append(part)\n        return parts\n\n    def _upload_one_part(\n        self,\n        filename,\n        bucket,\n        key,\n        upload_id,\n        part_size,\n        extra_args,\n        callback,\n        part_number,\n    ):\n        open_chunk_reader = self._os.open_file_chunk_reader\n        with open_chunk_reader(\n            filename, part_size * (part_number - 1), part_size, callback\n        ) as body:\n            response = self._client.upload_part(\n                Bucket=bucket,\n                Key=key,\n                UploadId=upload_id,\n                PartNumber=part_number,\n                Body=body,\n                **extra_args,\n            )\n            etag = response['ETag']\n            return {'ETag': etag, 'PartNumber': part_number}\n\n\nclass ShutdownQueue(queue.Queue):\n    \"\"\"A queue implementation that can be shutdown.\n\n    Shutting down a queue means that this class adds a\n    trigger_shutdown method that will trigger all subsequent\n    calls to put() to fail with a ``QueueShutdownError``.\n\n    It purposefully deviates from queue.Queue, and is *not* meant\n    to be a drop in replacement for ``queue.Queue``.\n\n    \"\"\"\n\n    def _init(self, maxsize):\n        self._shutdown = False\n        self._shutdown_lock = threading.Lock()\n        # queue.Queue is an old style class so we don't use super().\n        return queue.Queue._init(self, maxsize)\n\n    def trigger_shutdown(self):\n        with self._shutdown_lock:\n            self._shutdown = True\n            logger.debug(\"The IO queue is now shutdown.\")\n\n    def put(self, item):\n        # Note: this is not sufficient, it's still possible to deadlock!\n        # Need to hook into the condition vars used by this class.\n        with self._shutdown_lock:\n            if self._shutdown:\n                raise QueueShutdownError(\n                    \"Cannot put item to queue when \" \"queue has been shutdown.\"\n                )\n        return queue.Queue.put(self, item)\n\n\nclass MultipartDownloader:\n    def __init__(\n        self,\n        client,\n        config,\n        osutil,\n        executor_cls=concurrent.futures.ThreadPoolExecutor,\n    ):\n        self._client = client\n        self._config = config\n        self._os = osutil\n        self._executor_cls = executor_cls\n        self._ioqueue = ShutdownQueue(self._config.max_io_queue)\n\n    def download_file(\n        self, bucket, key, filename, object_size, extra_args, callback=None\n    ):\n        with self._executor_cls(max_workers=2) as controller:\n            # 1 thread for the future that manages the uploading of files\n            # 1 thread for the future that manages IO writes.\n            download_parts_handler = functools.partial(\n                self._download_file_as_future,\n                bucket,\n                key,\n                filename,\n                object_size,\n                callback,\n            )\n            parts_future = controller.submit(download_parts_handler)\n\n            io_writes_handler = functools.partial(\n                self._perform_io_writes, filename\n            )\n            io_future = controller.submit(io_writes_handler)\n            results = concurrent.futures.wait(\n                [parts_future, io_future],\n                return_when=concurrent.futures.FIRST_EXCEPTION,\n            )\n            self._process_future_results(results)\n\n    def _process_future_results(self, futures):\n        finished, unfinished = futures\n        for future in finished:\n            future.result()\n\n    def _download_file_as_future(\n        self, bucket, key, filename, object_size, callback\n    ):\n        part_size = self._config.multipart_chunksize\n        num_parts = int(math.ceil(object_size / float(part_size)))\n        max_workers = self._config.max_concurrency\n        download_partial = functools.partial(\n            self._download_range,\n            bucket,\n            key,\n            filename,\n            part_size,\n            num_parts,\n            callback,\n        )\n        try:\n            with self._executor_cls(max_workers=max_workers) as executor:\n                list(executor.map(download_partial, range(num_parts)))\n        finally:\n            self._ioqueue.put(SHUTDOWN_SENTINEL)\n\n    def _calculate_range_param(self, part_size, part_index, num_parts):\n        start_range = part_index * part_size\n        if part_index == num_parts - 1:\n            end_range = ''\n        else:\n            end_range = start_range + part_size - 1\n        range_param = f'bytes={start_range}-{end_range}'\n        return range_param\n\n    def _download_range(\n        self, bucket, key, filename, part_size, num_parts, callback, part_index\n    ):\n        try:\n            range_param = self._calculate_range_param(\n                part_size, part_index, num_parts\n            )\n\n            max_attempts = self._config.num_download_attempts\n            last_exception = None\n            for i in range(max_attempts):\n                try:\n                    logger.debug(\"Making get_object call.\")\n                    response = self._client.get_object(\n                        Bucket=bucket, Key=key, Range=range_param\n                    )\n                    streaming_body = StreamReaderProgress(\n                        response['Body'], callback\n                    )\n                    buffer_size = 1024 * 16\n                    current_index = part_size * part_index\n                    for chunk in iter(\n                        lambda: streaming_body.read(buffer_size), b''\n                    ):\n                        self._ioqueue.put((current_index, chunk))\n                        current_index += len(chunk)\n                    return\n                except (\n                    socket.timeout,\n                    OSError,\n                    ReadTimeoutError,\n                    IncompleteReadError,\n                    ResponseStreamingError,\n                ) as e:\n                    logger.debug(\n                        \"Retrying exception caught (%s), \"\n                        \"retrying request, (attempt %s / %s)\",\n                        e,\n                        i,\n                        max_attempts,\n                        exc_info=True,\n                    )\n                    last_exception = e\n                    continue\n            raise RetriesExceededError(last_exception)\n        finally:\n            logger.debug(\"EXITING _download_range for part: %s\", part_index)\n\n    def _perform_io_writes(self, filename):\n        with self._os.open(filename, 'wb') as f:\n            while True:\n                task = self._ioqueue.get()\n                if task is SHUTDOWN_SENTINEL:\n                    logger.debug(\n                        \"Shutdown sentinel received in IO handler, \"\n                        \"shutting down IO handler.\"\n                    )\n                    return\n                else:\n                    try:\n                        offset, data = task\n                        f.seek(offset)\n                        f.write(data)\n                    except Exception as e:\n                        logger.debug(\n                            \"Caught exception in IO thread: %s\",\n                            e,\n                            exc_info=True,\n                        )\n                        self._ioqueue.trigger_shutdown()\n                        raise\n\n\nclass TransferConfig:\n    def __init__(\n        self,\n        multipart_threshold=8 * MB,\n        max_concurrency=10,\n        multipart_chunksize=8 * MB,\n        num_download_attempts=5,\n        max_io_queue=100,\n    ):\n        self.multipart_threshold = multipart_threshold\n        self.max_concurrency = max_concurrency\n        self.multipart_chunksize = multipart_chunksize\n        self.num_download_attempts = num_download_attempts\n        self.max_io_queue = max_io_queue\n\n\nclass S3Transfer:\n    ALLOWED_DOWNLOAD_ARGS = [\n        'VersionId',\n        'SSECustomerAlgorithm',\n        'SSECustomerKey',\n        'SSECustomerKeyMD5',\n        'RequestPayer',\n    ]\n\n    ALLOWED_UPLOAD_ARGS = [\n        'ACL',\n        'CacheControl',\n        'ContentDisposition',\n        'ContentEncoding',\n        'ContentLanguage',\n        'ContentType',\n        'Expires',\n        'GrantFullControl',\n        'GrantRead',\n        'GrantReadACP',\n        'GrantWriteACL',\n        'Metadata',\n        'RequestPayer',\n        'ServerSideEncryption',\n        'StorageClass',\n        'SSECustomerAlgorithm',\n        'SSECustomerKey',\n        'SSECustomerKeyMD5',\n        'SSEKMSKeyId',\n        'SSEKMSEncryptionContext',\n        'Tagging',\n    ]\n\n    def __init__(self, client, config=None, osutil=None):\n        self._client = client\n        if config is None:\n            config = TransferConfig()\n        self._config = config\n        if osutil is None:\n            osutil = OSUtils()\n        self._osutil = osutil\n\n    def upload_file(\n        self, filename, bucket, key, callback=None, extra_args=None\n    ):\n        \"\"\"Upload a file to an S3 object.\n\n        Variants have also been injected into S3 client, Bucket and Object.\n        You don't have to use S3Transfer.upload_file() directly.\n        \"\"\"\n        if extra_args is None:\n            extra_args = {}\n        self._validate_all_known_args(extra_args, self.ALLOWED_UPLOAD_ARGS)\n        events = self._client.meta.events\n        events.register_first(\n            'request-created.s3',\n            disable_upload_callbacks,\n            unique_id='s3upload-callback-disable',\n        )\n        events.register_last(\n            'request-created.s3',\n            enable_upload_callbacks,\n            unique_id='s3upload-callback-enable',\n        )\n        if (\n            self._osutil.get_file_size(filename)\n            >= self._config.multipart_threshold\n        ):\n            self._multipart_upload(filename, bucket, key, callback, extra_args)\n        else:\n            self._put_object(filename, bucket, key, callback, extra_args)\n\n    def _put_object(self, filename, bucket, key, callback, extra_args):\n        # We're using open_file_chunk_reader so we can take advantage of the\n        # progress callback functionality.\n        open_chunk_reader = self._osutil.open_file_chunk_reader\n        with open_chunk_reader(\n            filename,\n            0,\n            self._osutil.get_file_size(filename),\n            callback=callback,\n        ) as body:\n            self._client.put_object(\n                Bucket=bucket, Key=key, Body=body, **extra_args\n            )\n\n    def download_file(\n        self, bucket, key, filename, extra_args=None, callback=None\n    ):\n        \"\"\"Download an S3 object to a file.\n\n        Variants have also been injected into S3 client, Bucket and Object.\n        You don't have to use S3Transfer.download_file() directly.\n        \"\"\"\n        # This method will issue a ``head_object`` request to determine\n        # the size of the S3 object.  This is used to determine if the\n        # object is downloaded in parallel.\n        if extra_args is None:\n            extra_args = {}\n        self._validate_all_known_args(extra_args, self.ALLOWED_DOWNLOAD_ARGS)\n        object_size = self._object_size(bucket, key, extra_args)\n        temp_filename = filename + os.extsep + random_file_extension()\n        try:\n            self._download_file(\n                bucket, key, temp_filename, object_size, extra_args, callback\n            )\n        except Exception:\n            logger.debug(\n                \"Exception caught in download_file, removing partial \"\n                \"file: %s\",\n                temp_filename,\n                exc_info=True,\n            )\n            self._osutil.remove_file(temp_filename)\n            raise\n        else:\n            self._osutil.rename_file(temp_filename, filename)\n\n    def _download_file(\n        self, bucket, key, filename, object_size, extra_args, callback\n    ):\n        if object_size >= self._config.multipart_threshold:\n            self._ranged_download(\n                bucket, key, filename, object_size, extra_args, callback\n            )\n        else:\n            self._get_object(bucket, key, filename, extra_args, callback)\n\n    def _validate_all_known_args(self, actual, allowed):\n        for kwarg in actual:\n            if kwarg not in allowed:\n                raise ValueError(\n                    \"Invalid extra_args key '%s', \"\n                    \"must be one of: %s\" % (kwarg, ', '.join(allowed))\n                )\n\n    def _ranged_download(\n        self, bucket, key, filename, object_size, extra_args, callback\n    ):\n        downloader = MultipartDownloader(\n            self._client, self._config, self._osutil\n        )\n        downloader.download_file(\n            bucket, key, filename, object_size, extra_args, callback\n        )\n\n    def _get_object(self, bucket, key, filename, extra_args, callback):\n        # precondition: num_download_attempts > 0\n        max_attempts = self._config.num_download_attempts\n        last_exception = None\n        for i in range(max_attempts):\n            try:\n                return self._do_get_object(\n                    bucket, key, filename, extra_args, callback\n                )\n            except (\n                socket.timeout,\n                OSError,\n                ReadTimeoutError,\n                IncompleteReadError,\n                ResponseStreamingError,\n            ) as e:\n                # TODO: we need a way to reset the callback if the\n                # download failed.\n                logger.debug(\n                    \"Retrying exception caught (%s), \"\n                    \"retrying request, (attempt %s / %s)\",\n                    e,\n                    i,\n                    max_attempts,\n                    exc_info=True,\n                )\n                last_exception = e\n                continue\n        raise RetriesExceededError(last_exception)\n\n    def _do_get_object(self, bucket, key, filename, extra_args, callback):\n        response = self._client.get_object(\n            Bucket=bucket, Key=key, **extra_args\n        )\n        streaming_body = StreamReaderProgress(response['Body'], callback)\n        with self._osutil.open(filename, 'wb') as f:\n            for chunk in iter(lambda: streaming_body.read(8192), b''):\n                f.write(chunk)\n\n    def _object_size(self, bucket, key, extra_args):\n        return self._client.head_object(Bucket=bucket, Key=key, **extra_args)[\n            'ContentLength'\n        ]\n\n    def _multipart_upload(self, filename, bucket, key, callback, extra_args):\n        uploader = MultipartUploader(self._client, self._config, self._osutil)\n        uploader.upload_file(filename, bucket, key, callback, extra_args)\n", "s3transfer/compat.py": "# Copyright 2016 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport errno\nimport inspect\nimport os\nimport socket\nimport sys\n\nfrom botocore.compat import six\n\nif sys.platform.startswith('win'):\n    def rename_file(current_filename, new_filename):\n        try:\n            os.remove(new_filename)\n        except OSError as e:\n            if not e.errno == errno.ENOENT:\n                # We only want to a ignore trying to remove\n                # a file that does not exist.  If it fails\n                # for any other reason we should be propagating\n                # that exception.\n                raise\n        os.rename(current_filename, new_filename)\nelse:\n    rename_file = os.rename\n\n\ndef accepts_kwargs(func):\n    return inspect.getfullargspec(func)[2]\n\n\n# In python 3, socket.error is OSError, which is too general\n# for what we want (i.e FileNotFoundError is a subclass of OSError).\n# In python 3, all the socket related errors are in a newly created\n# ConnectionError.\nSOCKET_ERROR = ConnectionError\nMAXINT = None\n\n\ndef seekable(fileobj):\n    \"\"\"Backwards compat function to determine if a fileobj is seekable\n\n    :param fileobj: The file-like object to determine if seekable\n\n    :returns: True, if seekable. False, otherwise.\n    \"\"\"\n    # If the fileobj has a seekable attr, try calling the seekable()\n    # method on it.\n    if hasattr(fileobj, 'seekable'):\n        return fileobj.seekable()\n    # If there is no seekable attr, check if the object can be seeked\n    # or telled. If it can, try to seek to the current position.\n    elif hasattr(fileobj, 'seek') and hasattr(fileobj, 'tell'):\n        try:\n            fileobj.seek(0, 1)\n            return True\n        except OSError:\n            # If an io related error was thrown then it is not seekable.\n            return False\n    # Else, the fileobj is not seekable\n    return False\n\n\ndef readable(fileobj):\n    \"\"\"Determines whether or not a file-like object is readable.\n\n    :param fileobj: The file-like object to determine if readable\n\n    :returns: True, if readable. False otherwise.\n    \"\"\"\n    if hasattr(fileobj, 'readable'):\n        return fileobj.readable()\n\n    return hasattr(fileobj, 'read')\n\n\ndef fallocate(fileobj, size):\n    if hasattr(os, 'posix_fallocate'):\n        os.posix_fallocate(fileobj.fileno(), 0, size)\n    else:\n        fileobj.truncate(size)\n\n\n# Import at end of file to avoid circular dependencies\nfrom multiprocessing.managers import BaseManager  # noqa: F401,E402\n", "s3transfer/crt.py": "# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport logging\nimport threading\nfrom io import BytesIO\n\nimport awscrt.http\nimport awscrt.s3\nimport botocore.awsrequest\nimport botocore.session\nfrom awscrt.auth import (\n    AwsCredentials,\n    AwsCredentialsProvider,\n    AwsSigningAlgorithm,\n    AwsSigningConfig,\n)\nfrom awscrt.io import (\n    ClientBootstrap,\n    ClientTlsContext,\n    DefaultHostResolver,\n    EventLoopGroup,\n    TlsContextOptions,\n)\nfrom awscrt.s3 import S3Client, S3RequestTlsMode, S3RequestType\nfrom botocore import UNSIGNED\nfrom botocore.compat import urlsplit\nfrom botocore.config import Config\nfrom botocore.exceptions import NoCredentialsError\n\nfrom s3transfer.constants import MB\nfrom s3transfer.exceptions import TransferNotDoneError\nfrom s3transfer.futures import BaseTransferFuture, BaseTransferMeta\nfrom s3transfer.manager import TransferManager\nfrom s3transfer.utils import (\n    CallArgs,\n    OSUtils,\n    get_callbacks,\n    is_s3express_bucket,\n)\n\nlogger = logging.getLogger(__name__)\n\nCRT_S3_PROCESS_LOCK = None\n\n\ndef acquire_crt_s3_process_lock(name):\n    # Currently, the CRT S3 client performs best when there is only one\n    # instance of it running on a host. This lock allows an application to\n    # signal across processes whether there is another process of the same\n    # application using the CRT S3 client and prevent spawning more than one\n    # CRT S3 clients running on the system for that application.\n    #\n    # NOTE: When acquiring the CRT process lock, the lock automatically is\n    # released when the lock object is garbage collected. So, the CRT process\n    # lock is set as a global so that it is not unintentionally garbage\n    # collected/released if reference of the lock is lost.\n    global CRT_S3_PROCESS_LOCK\n    if CRT_S3_PROCESS_LOCK is None:\n        crt_lock = awscrt.s3.CrossProcessLock(name)\n        try:\n            crt_lock.acquire()\n        except RuntimeError:\n            # If there is another process that is holding the lock, the CRT\n            # returns a RuntimeError. We return None here to signal that our\n            # current process was not able to acquire the lock.\n            return None\n        CRT_S3_PROCESS_LOCK = crt_lock\n    return CRT_S3_PROCESS_LOCK\n\n\ndef create_s3_crt_client(\n    region,\n    crt_credentials_provider=None,\n    num_threads=None,\n    target_throughput=None,\n    part_size=8 * MB,\n    use_ssl=True,\n    verify=None,\n):\n    \"\"\"\n    :type region: str\n    :param region: The region used for signing\n\n    :type crt_credentials_provider:\n        Optional[awscrt.auth.AwsCredentialsProvider]\n    :param crt_credentials_provider: CRT AWS credentials provider\n        to use to sign requests. If not set, requests will not be signed.\n\n    :type num_threads: Optional[int]\n    :param num_threads: Number of worker threads generated. Default\n        is the number of processors in the machine.\n\n    :type target_throughput: Optional[int]\n    :param target_throughput: Throughput target in bytes per second.\n        By default, CRT will automatically attempt to choose a target\n        throughput that matches the system's maximum network throughput.\n        Currently, if CRT is unable to determine the maximum network\n        throughput, a fallback target throughput of ``1_250_000_000`` bytes\n        per second (which translates to 10 gigabits per second, or 1.16\n        gibibytes per second) is used. To set a specific target\n        throughput, set a value for this parameter.\n\n    :type part_size: Optional[int]\n    :param part_size: Size, in Bytes, of parts that files will be downloaded\n        or uploaded in.\n\n    :type use_ssl: boolean\n    :param use_ssl: Whether or not to use SSL.  By default, SSL is used.\n        Note that not all services support non-ssl connections.\n\n    :type verify: Optional[boolean/string]\n    :param verify: Whether or not to verify SSL certificates.\n        By default SSL certificates are verified.  You can provide the\n        following values:\n\n        * False - do not validate SSL certificates.  SSL will still be\n            used (unless use_ssl is False), but SSL certificates\n            will not be verified.\n        * path/to/cert/bundle.pem - A filename of the CA cert bundle to\n            use. Specify this argument if you want to use a custom CA cert\n            bundle instead of the default one on your system.\n    \"\"\"\n    event_loop_group = EventLoopGroup(num_threads)\n    host_resolver = DefaultHostResolver(event_loop_group)\n    bootstrap = ClientBootstrap(event_loop_group, host_resolver)\n    tls_connection_options = None\n\n    tls_mode = (\n        S3RequestTlsMode.ENABLED if use_ssl else S3RequestTlsMode.DISABLED\n    )\n    if verify is not None:\n        tls_ctx_options = TlsContextOptions()\n        if verify:\n            tls_ctx_options.override_default_trust_store_from_path(\n                ca_filepath=verify\n            )\n        else:\n            tls_ctx_options.verify_peer = False\n        client_tls_option = ClientTlsContext(tls_ctx_options)\n        tls_connection_options = client_tls_option.new_connection_options()\n    target_gbps = _get_crt_throughput_target_gbps(\n        provided_throughput_target_bytes=target_throughput\n    )\n    return S3Client(\n        bootstrap=bootstrap,\n        region=region,\n        credential_provider=crt_credentials_provider,\n        part_size=part_size,\n        tls_mode=tls_mode,\n        tls_connection_options=tls_connection_options,\n        throughput_target_gbps=target_gbps,\n        enable_s3express=True,\n    )\n\n\ndef _get_crt_throughput_target_gbps(provided_throughput_target_bytes=None):\n    if provided_throughput_target_bytes is None:\n        target_gbps = awscrt.s3.get_recommended_throughput_target_gbps()\n        logger.debug(\n            'Recommended CRT throughput target in gbps: %s', target_gbps\n        )\n        if target_gbps is None:\n            target_gbps = 10.0\n    else:\n        # NOTE: The GB constant in s3transfer is technically a gibibyte. The\n        # GB constant is not used here because the CRT interprets gigabits\n        # for networking as a base power of 10\n        # (i.e. 1000 ** 3 instead of 1024 ** 3).\n        target_gbps = provided_throughput_target_bytes * 8 / 1_000_000_000\n    logger.debug('Using CRT throughput target in gbps: %s', target_gbps)\n    return target_gbps\n\n\nclass CRTTransferManager:\n    ALLOWED_DOWNLOAD_ARGS = TransferManager.ALLOWED_DOWNLOAD_ARGS\n    ALLOWED_UPLOAD_ARGS = TransferManager.ALLOWED_UPLOAD_ARGS\n    ALLOWED_DELETE_ARGS = TransferManager.ALLOWED_DELETE_ARGS\n\n    VALIDATE_SUPPORTED_BUCKET_VALUES = True\n\n    _UNSUPPORTED_BUCKET_PATTERNS = TransferManager._UNSUPPORTED_BUCKET_PATTERNS\n\n    def __init__(self, crt_s3_client, crt_request_serializer, osutil=None):\n        \"\"\"A transfer manager interface for Amazon S3 on CRT s3 client.\n\n        :type crt_s3_client: awscrt.s3.S3Client\n        :param crt_s3_client: The CRT s3 client, handling all the\n            HTTP requests and functions under then hood\n\n        :type crt_request_serializer: s3transfer.crt.BaseCRTRequestSerializer\n        :param crt_request_serializer: Serializer, generates unsigned crt HTTP\n            request.\n\n        :type osutil: s3transfer.utils.OSUtils\n        :param osutil: OSUtils object to use for os-related behavior when\n            using with transfer manager.\n        \"\"\"\n        if osutil is None:\n            self._osutil = OSUtils()\n        self._crt_s3_client = crt_s3_client\n        self._s3_args_creator = S3ClientArgsCreator(\n            crt_request_serializer, self._osutil\n        )\n        self._crt_exception_translator = (\n            crt_request_serializer.translate_crt_exception\n        )\n        self._future_coordinators = []\n        self._semaphore = threading.Semaphore(128)  # not configurable\n        # A counter to create unique id's for each transfer submitted.\n        self._id_counter = 0\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_value, *args):\n        cancel = False\n        if exc_type:\n            cancel = True\n        self._shutdown(cancel)\n\n    def download(\n        self, bucket, key, fileobj, extra_args=None, subscribers=None\n    ):\n        if extra_args is None:\n            extra_args = {}\n        if subscribers is None:\n            subscribers = {}\n        self._validate_all_known_args(extra_args, self.ALLOWED_DOWNLOAD_ARGS)\n        self._validate_if_bucket_supported(bucket)\n        callargs = CallArgs(\n            bucket=bucket,\n            key=key,\n            fileobj=fileobj,\n            extra_args=extra_args,\n            subscribers=subscribers,\n        )\n        return self._submit_transfer(\"get_object\", callargs)\n\n    def upload(self, fileobj, bucket, key, extra_args=None, subscribers=None):\n        if extra_args is None:\n            extra_args = {}\n        if subscribers is None:\n            subscribers = {}\n        self._validate_all_known_args(extra_args, self.ALLOWED_UPLOAD_ARGS)\n        self._validate_if_bucket_supported(bucket)\n        self._validate_checksum_algorithm_supported(extra_args)\n        callargs = CallArgs(\n            bucket=bucket,\n            key=key,\n            fileobj=fileobj,\n            extra_args=extra_args,\n            subscribers=subscribers,\n        )\n        return self._submit_transfer(\"put_object\", callargs)\n\n    def delete(self, bucket, key, extra_args=None, subscribers=None):\n        if extra_args is None:\n            extra_args = {}\n        if subscribers is None:\n            subscribers = {}\n        self._validate_all_known_args(extra_args, self.ALLOWED_DELETE_ARGS)\n        self._validate_if_bucket_supported(bucket)\n        callargs = CallArgs(\n            bucket=bucket,\n            key=key,\n            extra_args=extra_args,\n            subscribers=subscribers,\n        )\n        return self._submit_transfer(\"delete_object\", callargs)\n\n    def shutdown(self, cancel=False):\n        self._shutdown(cancel)\n\n    def _validate_if_bucket_supported(self, bucket):\n        # s3 high level operations don't support some resources\n        # (eg. S3 Object Lambda) only direct API calls are available\n        # for such resources\n        if self.VALIDATE_SUPPORTED_BUCKET_VALUES:\n            for resource, pattern in self._UNSUPPORTED_BUCKET_PATTERNS.items():\n                match = pattern.match(bucket)\n                if match:\n                    raise ValueError(\n                        f'TransferManager methods do not support {resource} '\n                        'resource. Use direct client calls instead.'\n                    )\n\n    def _validate_all_known_args(self, actual, allowed):\n        for kwarg in actual:\n            if kwarg not in allowed:\n                raise ValueError(\n                    f\"Invalid extra_args key '{kwarg}', \"\n                    f\"must be one of: {', '.join(allowed)}\"\n                )\n\n    def _validate_checksum_algorithm_supported(self, extra_args):\n        checksum_algorithm = extra_args.get('ChecksumAlgorithm')\n        if checksum_algorithm is None:\n            return\n        supported_algorithms = list(awscrt.s3.S3ChecksumAlgorithm.__members__)\n        if checksum_algorithm.upper() not in supported_algorithms:\n            raise ValueError(\n                f'ChecksumAlgorithm: {checksum_algorithm} not supported. '\n                f'Supported algorithms are: {supported_algorithms}'\n            )\n\n    def _cancel_transfers(self):\n        for coordinator in self._future_coordinators:\n            if not coordinator.done():\n                coordinator.cancel()\n\n    def _finish_transfers(self):\n        for coordinator in self._future_coordinators:\n            coordinator.result()\n\n    def _wait_transfers_done(self):\n        for coordinator in self._future_coordinators:\n            coordinator.wait_until_on_done_callbacks_complete()\n\n    def _shutdown(self, cancel=False):\n        if cancel:\n            self._cancel_transfers()\n        try:\n            self._finish_transfers()\n\n        except KeyboardInterrupt:\n            self._cancel_transfers()\n        except Exception:\n            pass\n        finally:\n            self._wait_transfers_done()\n\n    def _release_semaphore(self, **kwargs):\n        self._semaphore.release()\n\n    def _submit_transfer(self, request_type, call_args):\n        on_done_after_calls = [self._release_semaphore]\n        coordinator = CRTTransferCoordinator(\n            transfer_id=self._id_counter,\n            exception_translator=self._crt_exception_translator,\n        )\n        components = {\n            'meta': CRTTransferMeta(self._id_counter, call_args),\n            'coordinator': coordinator,\n        }\n        future = CRTTransferFuture(**components)\n        afterdone = AfterDoneHandler(coordinator)\n        on_done_after_calls.append(afterdone)\n\n        try:\n            self._semaphore.acquire()\n            on_queued = self._s3_args_creator.get_crt_callback(\n                future, 'queued'\n            )\n            on_queued()\n            crt_callargs = self._s3_args_creator.get_make_request_args(\n                request_type,\n                call_args,\n                coordinator,\n                future,\n                on_done_after_calls,\n            )\n            crt_s3_request = self._crt_s3_client.make_request(**crt_callargs)\n        except Exception as e:\n            coordinator.set_exception(e, True)\n            on_done = self._s3_args_creator.get_crt_callback(\n                future, 'done', after_subscribers=on_done_after_calls\n            )\n            on_done(error=e)\n        else:\n            coordinator.set_s3_request(crt_s3_request)\n        self._future_coordinators.append(coordinator)\n\n        self._id_counter += 1\n        return future\n\n\nclass CRTTransferMeta(BaseTransferMeta):\n    \"\"\"Holds metadata about the CRTTransferFuture\"\"\"\n\n    def __init__(self, transfer_id=None, call_args=None):\n        self._transfer_id = transfer_id\n        self._call_args = call_args\n        self._user_context = {}\n\n    @property\n    def call_args(self):\n        return self._call_args\n\n    @property\n    def transfer_id(self):\n        return self._transfer_id\n\n    @property\n    def user_context(self):\n        return self._user_context\n\n\nclass CRTTransferFuture(BaseTransferFuture):\n    def __init__(self, meta=None, coordinator=None):\n        \"\"\"The future associated to a submitted transfer request via CRT S3 client\n\n        :type meta: s3transfer.crt.CRTTransferMeta\n        :param meta: The metadata associated to the transfer future.\n\n        :type coordinator: s3transfer.crt.CRTTransferCoordinator\n        :param coordinator: The coordinator associated to the transfer future.\n        \"\"\"\n        self._meta = meta\n        if meta is None:\n            self._meta = CRTTransferMeta()\n        self._coordinator = coordinator\n\n    @property\n    def meta(self):\n        return self._meta\n\n    def done(self):\n        return self._coordinator.done()\n\n    def result(self, timeout=None):\n        self._coordinator.result(timeout)\n\n    def cancel(self):\n        self._coordinator.cancel()\n\n    def set_exception(self, exception):\n        \"\"\"Sets the exception on the future.\"\"\"\n        if not self.done():\n            raise TransferNotDoneError(\n                'set_exception can only be called once the transfer is '\n                'complete.'\n            )\n        self._coordinator.set_exception(exception, override=True)\n\n\nclass BaseCRTRequestSerializer:\n    def serialize_http_request(self, transfer_type, future):\n        \"\"\"Serialize CRT HTTP requests.\n\n        :type transfer_type: string\n        :param transfer_type: the type of transfer made,\n            e.g 'put_object', 'get_object', 'delete_object'\n\n        :type future: s3transfer.crt.CRTTransferFuture\n\n        :rtype: awscrt.http.HttpRequest\n        :returns: An unsigned HTTP request to be used for the CRT S3 client\n        \"\"\"\n        raise NotImplementedError('serialize_http_request()')\n\n    def translate_crt_exception(self, exception):\n        raise NotImplementedError('translate_crt_exception()')\n\n\nclass BotocoreCRTRequestSerializer(BaseCRTRequestSerializer):\n    def __init__(self, session, client_kwargs=None):\n        \"\"\"Serialize CRT HTTP request using botocore logic\n        It also takes into account configuration from both the session\n        and any keyword arguments that could be passed to\n        `Session.create_client()` when serializing the request.\n\n        :type session: botocore.session.Session\n\n        :type client_kwargs: Optional[Dict[str, str]])\n        :param client_kwargs: The kwargs for the botocore\n            s3 client initialization.\n        \"\"\"\n        self._session = session\n        if client_kwargs is None:\n            client_kwargs = {}\n        self._resolve_client_config(session, client_kwargs)\n        self._client = session.create_client(**client_kwargs)\n        self._client.meta.events.register(\n            'request-created.s3.*', self._capture_http_request\n        )\n        self._client.meta.events.register(\n            'after-call.s3.*', self._change_response_to_serialized_http_request\n        )\n        self._client.meta.events.register(\n            'before-send.s3.*', self._make_fake_http_response\n        )\n\n    def _resolve_client_config(self, session, client_kwargs):\n        user_provided_config = None\n        if session.get_default_client_config():\n            user_provided_config = session.get_default_client_config()\n        if 'config' in client_kwargs:\n            user_provided_config = client_kwargs['config']\n\n        client_config = Config(signature_version=UNSIGNED)\n        if user_provided_config:\n            client_config = user_provided_config.merge(client_config)\n        client_kwargs['config'] = client_config\n        client_kwargs[\"service_name\"] = \"s3\"\n\n    def _crt_request_from_aws_request(self, aws_request):\n        url_parts = urlsplit(aws_request.url)\n        crt_path = url_parts.path\n        if url_parts.query:\n            crt_path = f'{crt_path}?{url_parts.query}'\n        headers_list = []\n        for name, value in aws_request.headers.items():\n            if isinstance(value, str):\n                headers_list.append((name, value))\n            else:\n                headers_list.append((name, str(value, 'utf-8')))\n\n        crt_headers = awscrt.http.HttpHeaders(headers_list)\n\n        crt_request = awscrt.http.HttpRequest(\n            method=aws_request.method,\n            path=crt_path,\n            headers=crt_headers,\n            body_stream=aws_request.body,\n        )\n        return crt_request\n\n    def _convert_to_crt_http_request(self, botocore_http_request):\n        # Logic that does CRTUtils.crt_request_from_aws_request\n        crt_request = self._crt_request_from_aws_request(botocore_http_request)\n        if crt_request.headers.get(\"host\") is None:\n            # If host is not set, set it for the request before using CRT s3\n            url_parts = urlsplit(botocore_http_request.url)\n            crt_request.headers.set(\"host\", url_parts.netloc)\n        if crt_request.headers.get('Content-MD5') is not None:\n            crt_request.headers.remove(\"Content-MD5\")\n\n        # In general, the CRT S3 client expects a content length header. It\n        # only expects a missing content length header if the body is not\n        # seekable. However, botocore does not set the content length header\n        # for GetObject API requests and so we set the content length to zero\n        # to meet the CRT S3 client's expectation that the content length\n        # header is set even if there is no body.\n        if crt_request.headers.get('Content-Length') is None:\n            if botocore_http_request.body is None:\n                crt_request.headers.add('Content-Length', \"0\")\n\n        # Botocore sets the Transfer-Encoding header when it cannot determine\n        # the content length of the request body (e.g. it's not seekable).\n        # However, CRT does not support this header, but it supports\n        # non-seekable bodies. So we remove this header to not cause issues\n        # in the downstream CRT S3 request.\n        if crt_request.headers.get('Transfer-Encoding') is not None:\n            crt_request.headers.remove('Transfer-Encoding')\n\n        return crt_request\n\n    def _capture_http_request(self, request, **kwargs):\n        request.context['http_request'] = request\n\n    def _change_response_to_serialized_http_request(\n        self, context, parsed, **kwargs\n    ):\n        request = context['http_request']\n        parsed['HTTPRequest'] = request.prepare()\n\n    def _make_fake_http_response(self, request, **kwargs):\n        return botocore.awsrequest.AWSResponse(\n            None,\n            200,\n            {},\n            FakeRawResponse(b\"\"),\n        )\n\n    def _get_botocore_http_request(self, client_method, call_args):\n        return getattr(self._client, client_method)(\n            Bucket=call_args.bucket, Key=call_args.key, **call_args.extra_args\n        )['HTTPRequest']\n\n    def serialize_http_request(self, transfer_type, future):\n        botocore_http_request = self._get_botocore_http_request(\n            transfer_type, future.meta.call_args\n        )\n        crt_request = self._convert_to_crt_http_request(botocore_http_request)\n        return crt_request\n\n    def translate_crt_exception(self, exception):\n        if isinstance(exception, awscrt.s3.S3ResponseError):\n            return self._translate_crt_s3_response_error(exception)\n        else:\n            return None\n\n    def _translate_crt_s3_response_error(self, s3_response_error):\n        status_code = s3_response_error.status_code\n        if status_code < 301:\n            # Botocore's exception parsing only\n            # runs on status codes >= 301\n            return None\n\n        headers = {k: v for k, v in s3_response_error.headers}\n        operation_name = s3_response_error.operation_name\n        if operation_name is not None:\n            service_model = self._client.meta.service_model\n            shape = service_model.operation_model(operation_name).output_shape\n        else:\n            shape = None\n\n        response_dict = {\n            'headers': botocore.awsrequest.HeadersDict(headers),\n            'status_code': status_code,\n            'body': s3_response_error.body,\n        }\n        parsed_response = self._client._response_parser.parse(\n            response_dict, shape=shape\n        )\n\n        error_code = parsed_response.get(\"Error\", {}).get(\"Code\")\n        error_class = self._client.exceptions.from_code(error_code)\n        return error_class(parsed_response, operation_name=operation_name)\n\n\nclass FakeRawResponse(BytesIO):\n    def stream(self, amt=1024, decode_content=None):\n        while True:\n            chunk = self.read(amt)\n            if not chunk:\n                break\n            yield chunk\n\n\nclass BotocoreCRTCredentialsWrapper:\n    def __init__(self, resolved_botocore_credentials):\n        self._resolved_credentials = resolved_botocore_credentials\n\n    def __call__(self):\n        credentials = self._get_credentials().get_frozen_credentials()\n        return AwsCredentials(\n            credentials.access_key, credentials.secret_key, credentials.token\n        )\n\n    def to_crt_credentials_provider(self):\n        return AwsCredentialsProvider.new_delegate(self)\n\n    def _get_credentials(self):\n        if self._resolved_credentials is None:\n            raise NoCredentialsError()\n        return self._resolved_credentials\n\n\nclass CRTTransferCoordinator:\n    \"\"\"A helper class for managing CRTTransferFuture\"\"\"\n\n    def __init__(\n        self, transfer_id=None, s3_request=None, exception_translator=None\n    ):\n        self.transfer_id = transfer_id\n        self._exception_translator = exception_translator\n        self._s3_request = s3_request\n        self._lock = threading.Lock()\n        self._exception = None\n        self._crt_future = None\n        self._done_event = threading.Event()\n\n    @property\n    def s3_request(self):\n        return self._s3_request\n\n    def set_done_callbacks_complete(self):\n        self._done_event.set()\n\n    def wait_until_on_done_callbacks_complete(self, timeout=None):\n        self._done_event.wait(timeout)\n\n    def set_exception(self, exception, override=False):\n        with self._lock:\n            if not self.done() or override:\n                self._exception = exception\n\n    def cancel(self):\n        if self._s3_request:\n            self._s3_request.cancel()\n\n    def result(self, timeout=None):\n        if self._exception:\n            raise self._exception\n        try:\n            self._crt_future.result(timeout)\n        except KeyboardInterrupt:\n            self.cancel()\n            self._crt_future.result(timeout)\n            raise\n        except Exception as e:\n            self.handle_exception(e)\n        finally:\n            if self._s3_request:\n                self._s3_request = None\n\n    def handle_exception(self, exc):\n        translated_exc = None\n        if self._exception_translator:\n            try:\n                translated_exc = self._exception_translator(exc)\n            except Exception as e:\n                # Bail out if we hit an issue translating\n                # and raise the original error.\n                logger.debug(\"Unable to translate exception.\", exc_info=e)\n                pass\n        if translated_exc is not None:\n            raise translated_exc from exc\n        else:\n            raise exc\n\n    def done(self):\n        if self._crt_future is None:\n            return False\n        return self._crt_future.done()\n\n    def set_s3_request(self, s3_request):\n        self._s3_request = s3_request\n        self._crt_future = self._s3_request.finished_future\n\n\nclass S3ClientArgsCreator:\n    def __init__(self, crt_request_serializer, os_utils):\n        self._request_serializer = crt_request_serializer\n        self._os_utils = os_utils\n\n    def get_make_request_args(\n        self, request_type, call_args, coordinator, future, on_done_after_calls\n    ):\n        request_args_handler = getattr(\n            self,\n            f'_get_make_request_args_{request_type}',\n            self._default_get_make_request_args,\n        )\n        return request_args_handler(\n            request_type=request_type,\n            call_args=call_args,\n            coordinator=coordinator,\n            future=future,\n            on_done_before_calls=[],\n            on_done_after_calls=on_done_after_calls,\n        )\n\n    def get_crt_callback(\n        self,\n        future,\n        callback_type,\n        before_subscribers=None,\n        after_subscribers=None,\n    ):\n        def invoke_all_callbacks(*args, **kwargs):\n            callbacks_list = []\n            if before_subscribers is not None:\n                callbacks_list += before_subscribers\n            callbacks_list += get_callbacks(future, callback_type)\n            if after_subscribers is not None:\n                callbacks_list += after_subscribers\n            for callback in callbacks_list:\n                # The get_callbacks helper will set the first augment\n                # by keyword, the other augments need to be set by keyword\n                # as well\n                if callback_type == \"progress\":\n                    callback(bytes_transferred=args[0])\n                else:\n                    callback(*args, **kwargs)\n\n        return invoke_all_callbacks\n\n    def _get_make_request_args_put_object(\n        self,\n        request_type,\n        call_args,\n        coordinator,\n        future,\n        on_done_before_calls,\n        on_done_after_calls,\n    ):\n        send_filepath = None\n        if isinstance(call_args.fileobj, str):\n            send_filepath = call_args.fileobj\n            data_len = self._os_utils.get_file_size(send_filepath)\n            call_args.extra_args[\"ContentLength\"] = data_len\n        else:\n            call_args.extra_args[\"Body\"] = call_args.fileobj\n\n        checksum_algorithm = call_args.extra_args.pop(\n            'ChecksumAlgorithm', 'CRC32'\n        ).upper()\n        checksum_config = awscrt.s3.S3ChecksumConfig(\n            algorithm=awscrt.s3.S3ChecksumAlgorithm[checksum_algorithm],\n            location=awscrt.s3.S3ChecksumLocation.TRAILER,\n        )\n        # Suppress botocore's automatic MD5 calculation by setting an override\n        # value that will get deleted in the BotocoreCRTRequestSerializer.\n        # As part of the CRT S3 request, we request the CRT S3 client to\n        # automatically add trailing checksums to its uploads.\n        call_args.extra_args[\"ContentMD5\"] = \"override-to-be-removed\"\n\n        make_request_args = self._default_get_make_request_args(\n            request_type=request_type,\n            call_args=call_args,\n            coordinator=coordinator,\n            future=future,\n            on_done_before_calls=on_done_before_calls,\n            on_done_after_calls=on_done_after_calls,\n        )\n        make_request_args['send_filepath'] = send_filepath\n        make_request_args['checksum_config'] = checksum_config\n        return make_request_args\n\n    def _get_make_request_args_get_object(\n        self,\n        request_type,\n        call_args,\n        coordinator,\n        future,\n        on_done_before_calls,\n        on_done_after_calls,\n    ):\n        recv_filepath = None\n        on_body = None\n        checksum_config = awscrt.s3.S3ChecksumConfig(validate_response=True)\n        if isinstance(call_args.fileobj, str):\n            final_filepath = call_args.fileobj\n            recv_filepath = self._os_utils.get_temp_filename(final_filepath)\n            on_done_before_calls.append(\n                RenameTempFileHandler(\n                    coordinator, final_filepath, recv_filepath, self._os_utils\n                )\n            )\n        else:\n            on_body = OnBodyFileObjWriter(call_args.fileobj)\n\n        make_request_args = self._default_get_make_request_args(\n            request_type=request_type,\n            call_args=call_args,\n            coordinator=coordinator,\n            future=future,\n            on_done_before_calls=on_done_before_calls,\n            on_done_after_calls=on_done_after_calls,\n        )\n        make_request_args['recv_filepath'] = recv_filepath\n        make_request_args['on_body'] = on_body\n        make_request_args['checksum_config'] = checksum_config\n        return make_request_args\n\n    def _default_get_make_request_args(\n        self,\n        request_type,\n        call_args,\n        coordinator,\n        future,\n        on_done_before_calls,\n        on_done_after_calls,\n    ):\n        make_request_args = {\n            'request': self._request_serializer.serialize_http_request(\n                request_type, future\n            ),\n            'type': getattr(\n                S3RequestType, request_type.upper(), S3RequestType.DEFAULT\n            ),\n            'on_done': self.get_crt_callback(\n                future, 'done', on_done_before_calls, on_done_after_calls\n            ),\n            'on_progress': self.get_crt_callback(future, 'progress'),\n        }\n\n        # For DEFAULT requests, CRT requires the official S3 operation name.\n        # So transform string like \"delete_object\" -> \"DeleteObject\".\n        if make_request_args['type'] == S3RequestType.DEFAULT:\n            make_request_args['operation_name'] = ''.join(\n                x.title() for x in request_type.split('_')\n            )\n\n        if is_s3express_bucket(call_args.bucket):\n            make_request_args['signing_config'] = AwsSigningConfig(\n                algorithm=AwsSigningAlgorithm.V4_S3EXPRESS\n            )\n        return make_request_args\n\n\nclass RenameTempFileHandler:\n    def __init__(self, coordinator, final_filename, temp_filename, osutil):\n        self._coordinator = coordinator\n        self._final_filename = final_filename\n        self._temp_filename = temp_filename\n        self._osutil = osutil\n\n    def __call__(self, **kwargs):\n        error = kwargs['error']\n        if error:\n            self._osutil.remove_file(self._temp_filename)\n        else:\n            try:\n                self._osutil.rename_file(\n                    self._temp_filename, self._final_filename\n                )\n            except Exception as e:\n                self._osutil.remove_file(self._temp_filename)\n                # the CRT future has done already at this point\n                self._coordinator.set_exception(e)\n\n\nclass AfterDoneHandler:\n    def __init__(self, coordinator):\n        self._coordinator = coordinator\n\n    def __call__(self, **kwargs):\n        self._coordinator.set_done_callbacks_complete()\n\n\nclass OnBodyFileObjWriter:\n    def __init__(self, fileobj):\n        self._fileobj = fileobj\n\n    def __call__(self, chunk, **kwargs):\n        self._fileobj.write(chunk)\n", "s3transfer/download.py": "# Copyright 2016 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport heapq\nimport logging\nimport threading\n\nfrom s3transfer.compat import seekable\nfrom s3transfer.exceptions import RetriesExceededError\nfrom s3transfer.futures import IN_MEMORY_DOWNLOAD_TAG\nfrom s3transfer.tasks import SubmissionTask, Task\nfrom s3transfer.utils import (\n    S3_RETRYABLE_DOWNLOAD_ERRORS,\n    CountCallbackInvoker,\n    DeferredOpenFile,\n    FunctionContainer,\n    StreamReaderProgress,\n    calculate_num_parts,\n    calculate_range_parameter,\n    get_callbacks,\n    invoke_progress_callbacks,\n)\n\nlogger = logging.getLogger(__name__)\n\n\nclass DownloadOutputManager:\n    \"\"\"Base manager class for handling various types of files for downloads\n\n    This class is typically used for the DownloadSubmissionTask class to help\n    determine the following:\n\n        * Provides the fileobj to write to downloads to\n        * Get a task to complete once everything downloaded has been written\n\n    The answers/implementations differ for the various types of file outputs\n    that may be accepted. All implementations must subclass and override\n    public methods from this class.\n    \"\"\"\n\n    def __init__(self, osutil, transfer_coordinator, io_executor):\n        self._osutil = osutil\n        self._transfer_coordinator = transfer_coordinator\n        self._io_executor = io_executor\n\n    @classmethod\n    def is_compatible(cls, download_target, osutil):\n        \"\"\"Determines if the target for the download is compatible with manager\n\n        :param download_target: The target for which the upload will write\n            data to.\n\n        :param osutil: The os utility to be used for the transfer\n\n        :returns: True if the manager can handle the type of target specified\n            otherwise returns False.\n        \"\"\"\n        raise NotImplementedError('must implement is_compatible()')\n\n    def get_download_task_tag(self):\n        \"\"\"Get the tag (if any) to associate all GetObjectTasks\n\n        :rtype: s3transfer.futures.TaskTag\n        :returns: The tag to associate all GetObjectTasks with\n        \"\"\"\n        return None\n\n    def get_fileobj_for_io_writes(self, transfer_future):\n        \"\"\"Get file-like object to use for io writes in the io executor\n\n        :type transfer_future: s3transfer.futures.TransferFuture\n        :param transfer_future: The future associated with upload request\n\n        returns: A file-like object to write to\n        \"\"\"\n        raise NotImplementedError('must implement get_fileobj_for_io_writes()')\n\n    def queue_file_io_task(self, fileobj, data, offset):\n        \"\"\"Queue IO write for submission to the IO executor.\n\n        This method accepts an IO executor and information about the\n        downloaded data, and handles submitting this to the IO executor.\n\n        This method may defer submission to the IO executor if necessary.\n\n        \"\"\"\n        self._transfer_coordinator.submit(\n            self._io_executor, self.get_io_write_task(fileobj, data, offset)\n        )\n\n    def get_io_write_task(self, fileobj, data, offset):\n        \"\"\"Get an IO write task for the requested set of data\n\n        This task can be ran immediately or be submitted to the IO executor\n        for it to run.\n\n        :type fileobj: file-like object\n        :param fileobj: The file-like object to write to\n\n        :type data: bytes\n        :param data: The data to write out\n\n        :type offset: integer\n        :param offset: The offset to write the data to in the file-like object\n\n        :returns: An IO task to be used to write data to a file-like object\n        \"\"\"\n        return IOWriteTask(\n            self._transfer_coordinator,\n            main_kwargs={\n                'fileobj': fileobj,\n                'data': data,\n                'offset': offset,\n            },\n        )\n\n    def get_final_io_task(self):\n        \"\"\"Get the final io task to complete the download\n\n        This is needed because based on the architecture of the TransferManager\n        the final tasks will be sent to the IO executor, but the executor\n        needs a final task for it to signal that the transfer is done and\n        all done callbacks can be run.\n\n        :rtype: s3transfer.tasks.Task\n        :returns: A final task to completed in the io executor\n        \"\"\"\n        raise NotImplementedError('must implement get_final_io_task()')\n\n    def _get_fileobj_from_filename(self, filename):\n        f = DeferredOpenFile(\n            filename, mode='wb', open_function=self._osutil.open\n        )\n        # Make sure the file gets closed and we remove the temporary file\n        # if anything goes wrong during the process.\n        self._transfer_coordinator.add_failure_cleanup(f.close)\n        return f\n\n\nclass DownloadFilenameOutputManager(DownloadOutputManager):\n    def __init__(self, osutil, transfer_coordinator, io_executor):\n        super().__init__(osutil, transfer_coordinator, io_executor)\n        self._final_filename = None\n        self._temp_filename = None\n        self._temp_fileobj = None\n\n    @classmethod\n    def is_compatible(cls, download_target, osutil):\n        return isinstance(download_target, str)\n\n    def get_fileobj_for_io_writes(self, transfer_future):\n        fileobj = transfer_future.meta.call_args.fileobj\n        self._final_filename = fileobj\n        self._temp_filename = self._osutil.get_temp_filename(fileobj)\n        self._temp_fileobj = self._get_temp_fileobj()\n        return self._temp_fileobj\n\n    def get_final_io_task(self):\n        # A task to rename the file from the temporary file to its final\n        # location is needed. This should be the last task needed to complete\n        # the download.\n        return IORenameFileTask(\n            transfer_coordinator=self._transfer_coordinator,\n            main_kwargs={\n                'fileobj': self._temp_fileobj,\n                'final_filename': self._final_filename,\n                'osutil': self._osutil,\n            },\n            is_final=True,\n        )\n\n    def _get_temp_fileobj(self):\n        f = self._get_fileobj_from_filename(self._temp_filename)\n        self._transfer_coordinator.add_failure_cleanup(\n            self._osutil.remove_file, self._temp_filename\n        )\n        return f\n\n\nclass DownloadSeekableOutputManager(DownloadOutputManager):\n    @classmethod\n    def is_compatible(cls, download_target, osutil):\n        return seekable(download_target)\n\n    def get_fileobj_for_io_writes(self, transfer_future):\n        # Return the fileobj provided to the future.\n        return transfer_future.meta.call_args.fileobj\n\n    def get_final_io_task(self):\n        # This task will serve the purpose of signaling when all of the io\n        # writes have finished so done callbacks can be called.\n        return CompleteDownloadNOOPTask(\n            transfer_coordinator=self._transfer_coordinator\n        )\n\n\nclass DownloadNonSeekableOutputManager(DownloadOutputManager):\n    def __init__(\n        self, osutil, transfer_coordinator, io_executor, defer_queue=None\n    ):\n        super().__init__(osutil, transfer_coordinator, io_executor)\n        if defer_queue is None:\n            defer_queue = DeferQueue()\n        self._defer_queue = defer_queue\n        self._io_submit_lock = threading.Lock()\n\n    @classmethod\n    def is_compatible(cls, download_target, osutil):\n        return hasattr(download_target, 'write')\n\n    def get_download_task_tag(self):\n        return IN_MEMORY_DOWNLOAD_TAG\n\n    def get_fileobj_for_io_writes(self, transfer_future):\n        return transfer_future.meta.call_args.fileobj\n\n    def get_final_io_task(self):\n        return CompleteDownloadNOOPTask(\n            transfer_coordinator=self._transfer_coordinator\n        )\n\n    def queue_file_io_task(self, fileobj, data, offset):\n        with self._io_submit_lock:\n            writes = self._defer_queue.request_writes(offset, data)\n            for write in writes:\n                data = write['data']\n                logger.debug(\n                    \"Queueing IO offset %s for fileobj: %s\",\n                    write['offset'],\n                    fileobj,\n                )\n                super().queue_file_io_task(fileobj, data, offset)\n\n    def get_io_write_task(self, fileobj, data, offset):\n        return IOStreamingWriteTask(\n            self._transfer_coordinator,\n            main_kwargs={\n                'fileobj': fileobj,\n                'data': data,\n            },\n        )\n\n\nclass DownloadSpecialFilenameOutputManager(DownloadNonSeekableOutputManager):\n    def __init__(\n        self, osutil, transfer_coordinator, io_executor, defer_queue=None\n    ):\n        super().__init__(\n            osutil, transfer_coordinator, io_executor, defer_queue\n        )\n        self._fileobj = None\n\n    @classmethod\n    def is_compatible(cls, download_target, osutil):\n        return isinstance(download_target, str) and osutil.is_special_file(\n            download_target\n        )\n\n    def get_fileobj_for_io_writes(self, transfer_future):\n        filename = transfer_future.meta.call_args.fileobj\n        self._fileobj = self._get_fileobj_from_filename(filename)\n        return self._fileobj\n\n    def get_final_io_task(self):\n        # Make sure the file gets closed once the transfer is done.\n        return IOCloseTask(\n            transfer_coordinator=self._transfer_coordinator,\n            is_final=True,\n            main_kwargs={'fileobj': self._fileobj},\n        )\n\n\nclass DownloadSubmissionTask(SubmissionTask):\n    \"\"\"Task for submitting tasks to execute a download\"\"\"\n\n    def _get_download_output_manager_cls(self, transfer_future, osutil):\n        \"\"\"Retrieves a class for managing output for a download\n\n        :type transfer_future: s3transfer.futures.TransferFuture\n        :param transfer_future: The transfer future for the request\n\n        :type osutil: s3transfer.utils.OSUtils\n        :param osutil: The os utility associated to the transfer\n\n        :rtype: class of DownloadOutputManager\n        :returns: The appropriate class to use for managing a specific type of\n            input for downloads.\n        \"\"\"\n        download_manager_resolver_chain = [\n            DownloadSpecialFilenameOutputManager,\n            DownloadFilenameOutputManager,\n            DownloadSeekableOutputManager,\n            DownloadNonSeekableOutputManager,\n        ]\n\n        fileobj = transfer_future.meta.call_args.fileobj\n        for download_manager_cls in download_manager_resolver_chain:\n            if download_manager_cls.is_compatible(fileobj, osutil):\n                return download_manager_cls\n        raise RuntimeError(\n            'Output {} of type: {} is not supported.'.format(\n                fileobj, type(fileobj)\n            )\n        )\n\n    def _submit(\n        self,\n        client,\n        config,\n        osutil,\n        request_executor,\n        io_executor,\n        transfer_future,\n        bandwidth_limiter=None,\n    ):\n        \"\"\"\n        :param client: The client associated with the transfer manager\n\n        :type config: s3transfer.manager.TransferConfig\n        :param config: The transfer config associated with the transfer\n            manager\n\n        :type osutil: s3transfer.utils.OSUtil\n        :param osutil: The os utility associated to the transfer manager\n\n        :type request_executor: s3transfer.futures.BoundedExecutor\n        :param request_executor: The request executor associated with the\n            transfer manager\n\n        :type io_executor: s3transfer.futures.BoundedExecutor\n        :param io_executor: The io executor associated with the\n            transfer manager\n\n        :type transfer_future: s3transfer.futures.TransferFuture\n        :param transfer_future: The transfer future associated with the\n            transfer request that tasks are being submitted for\n\n        :type bandwidth_limiter: s3transfer.bandwidth.BandwidthLimiter\n        :param bandwidth_limiter: The bandwidth limiter to use when\n            downloading streams\n        \"\"\"\n        if transfer_future.meta.size is None:\n            # If a size was not provided figure out the size for the\n            # user.\n            response = client.head_object(\n                Bucket=transfer_future.meta.call_args.bucket,\n                Key=transfer_future.meta.call_args.key,\n                **transfer_future.meta.call_args.extra_args,\n            )\n            transfer_future.meta.provide_transfer_size(\n                response['ContentLength']\n            )\n\n        download_output_manager = self._get_download_output_manager_cls(\n            transfer_future, osutil\n        )(osutil, self._transfer_coordinator, io_executor)\n\n        # If it is greater than threshold do a ranged download, otherwise\n        # do a regular GetObject download.\n        if transfer_future.meta.size < config.multipart_threshold:\n            self._submit_download_request(\n                client,\n                config,\n                osutil,\n                request_executor,\n                io_executor,\n                download_output_manager,\n                transfer_future,\n                bandwidth_limiter,\n            )\n        else:\n            self._submit_ranged_download_request(\n                client,\n                config,\n                osutil,\n                request_executor,\n                io_executor,\n                download_output_manager,\n                transfer_future,\n                bandwidth_limiter,\n            )\n\n    def _submit_download_request(\n        self,\n        client,\n        config,\n        osutil,\n        request_executor,\n        io_executor,\n        download_output_manager,\n        transfer_future,\n        bandwidth_limiter,\n    ):\n        call_args = transfer_future.meta.call_args\n\n        # Get a handle to the file that will be used for writing downloaded\n        # contents\n        fileobj = download_output_manager.get_fileobj_for_io_writes(\n            transfer_future\n        )\n\n        # Get the needed callbacks for the task\n        progress_callbacks = get_callbacks(transfer_future, 'progress')\n\n        # Get any associated tags for the get object task.\n        get_object_tag = download_output_manager.get_download_task_tag()\n\n        # Get the final io task to run once the download is complete.\n        final_task = download_output_manager.get_final_io_task()\n\n        # Submit the task to download the object.\n        self._transfer_coordinator.submit(\n            request_executor,\n            ImmediatelyWriteIOGetObjectTask(\n                transfer_coordinator=self._transfer_coordinator,\n                main_kwargs={\n                    'client': client,\n                    'bucket': call_args.bucket,\n                    'key': call_args.key,\n                    'fileobj': fileobj,\n                    'extra_args': call_args.extra_args,\n                    'callbacks': progress_callbacks,\n                    'max_attempts': config.num_download_attempts,\n                    'download_output_manager': download_output_manager,\n                    'io_chunksize': config.io_chunksize,\n                    'bandwidth_limiter': bandwidth_limiter,\n                },\n                done_callbacks=[final_task],\n            ),\n            tag=get_object_tag,\n        )\n\n    def _submit_ranged_download_request(\n        self,\n        client,\n        config,\n        osutil,\n        request_executor,\n        io_executor,\n        download_output_manager,\n        transfer_future,\n        bandwidth_limiter,\n    ):\n        call_args = transfer_future.meta.call_args\n\n        # Get the needed progress callbacks for the task\n        progress_callbacks = get_callbacks(transfer_future, 'progress')\n\n        # Get a handle to the file that will be used for writing downloaded\n        # contents\n        fileobj = download_output_manager.get_fileobj_for_io_writes(\n            transfer_future\n        )\n\n        # Determine the number of parts\n        part_size = config.multipart_chunksize\n        num_parts = calculate_num_parts(transfer_future.meta.size, part_size)\n\n        # Get any associated tags for the get object task.\n        get_object_tag = download_output_manager.get_download_task_tag()\n\n        # Callback invoker to submit the final io task once all downloads\n        # are complete.\n        finalize_download_invoker = CountCallbackInvoker(\n            self._get_final_io_task_submission_callback(\n                download_output_manager, io_executor\n            )\n        )\n        for i in range(num_parts):\n            # Calculate the range parameter\n            range_parameter = calculate_range_parameter(\n                part_size, i, num_parts\n            )\n\n            # Inject the Range parameter to the parameters to be passed in\n            # as extra args\n            extra_args = {'Range': range_parameter}\n            extra_args.update(call_args.extra_args)\n            finalize_download_invoker.increment()\n            # Submit the ranged downloads\n            self._transfer_coordinator.submit(\n                request_executor,\n                GetObjectTask(\n                    transfer_coordinator=self._transfer_coordinator,\n                    main_kwargs={\n                        'client': client,\n                        'bucket': call_args.bucket,\n                        'key': call_args.key,\n                        'fileobj': fileobj,\n                        'extra_args': extra_args,\n                        'callbacks': progress_callbacks,\n                        'max_attempts': config.num_download_attempts,\n                        'start_index': i * part_size,\n                        'download_output_manager': download_output_manager,\n                        'io_chunksize': config.io_chunksize,\n                        'bandwidth_limiter': bandwidth_limiter,\n                    },\n                    done_callbacks=[finalize_download_invoker.decrement],\n                ),\n                tag=get_object_tag,\n            )\n        finalize_download_invoker.finalize()\n\n    def _get_final_io_task_submission_callback(\n        self, download_manager, io_executor\n    ):\n        final_task = download_manager.get_final_io_task()\n        return FunctionContainer(\n            self._transfer_coordinator.submit, io_executor, final_task\n        )\n\n    def _calculate_range_param(self, part_size, part_index, num_parts):\n        # Used to calculate the Range parameter\n        start_range = part_index * part_size\n        if part_index == num_parts - 1:\n            end_range = ''\n        else:\n            end_range = start_range + part_size - 1\n        range_param = f'bytes={start_range}-{end_range}'\n        return range_param\n\n\nclass GetObjectTask(Task):\n    def _main(\n        self,\n        client,\n        bucket,\n        key,\n        fileobj,\n        extra_args,\n        callbacks,\n        max_attempts,\n        download_output_manager,\n        io_chunksize,\n        start_index=0,\n        bandwidth_limiter=None,\n    ):\n        \"\"\"Downloads an object and places content into io queue\n\n        :param client: The client to use when calling GetObject\n        :param bucket: The bucket to download from\n        :param key: The key to download from\n        :param fileobj: The file handle to write content to\n        :param exta_args: Any extra arguments to include in GetObject request\n        :param callbacks: List of progress callbacks to invoke on download\n        :param max_attempts: The number of retries to do when downloading\n        :param download_output_manager: The download output manager associated\n            with the current download.\n        :param io_chunksize: The size of each io chunk to read from the\n            download stream and queue in the io queue.\n        :param start_index: The location in the file to start writing the\n            content of the key to.\n        :param bandwidth_limiter: The bandwidth limiter to use when throttling\n            the downloading of data in streams.\n        \"\"\"\n        last_exception = None\n        for i in range(max_attempts):\n            try:\n                current_index = start_index\n                response = client.get_object(\n                    Bucket=bucket, Key=key, **extra_args\n                )\n                streaming_body = StreamReaderProgress(\n                    response['Body'], callbacks\n                )\n                if bandwidth_limiter:\n                    streaming_body = (\n                        bandwidth_limiter.get_bandwith_limited_stream(\n                            streaming_body, self._transfer_coordinator\n                        )\n                    )\n\n                chunks = DownloadChunkIterator(streaming_body, io_chunksize)\n                for chunk in chunks:\n                    # If the transfer is done because of a cancellation\n                    # or error somewhere else, stop trying to submit more\n                    # data to be written and break out of the download.\n                    if not self._transfer_coordinator.done():\n                        self._handle_io(\n                            download_output_manager,\n                            fileobj,\n                            chunk,\n                            current_index,\n                        )\n                        current_index += len(chunk)\n                    else:\n                        return\n                return\n            except S3_RETRYABLE_DOWNLOAD_ERRORS as e:\n                logger.debug(\n                    \"Retrying exception caught (%s), \"\n                    \"retrying request, (attempt %s / %s)\",\n                    e,\n                    i,\n                    max_attempts,\n                    exc_info=True,\n                )\n                last_exception = e\n                # Also invoke the progress callbacks to indicate that we\n                # are trying to download the stream again and all progress\n                # for this GetObject has been lost.\n                invoke_progress_callbacks(\n                    callbacks, start_index - current_index\n                )\n                continue\n        raise RetriesExceededError(last_exception)\n\n    def _handle_io(self, download_output_manager, fileobj, chunk, index):\n        download_output_manager.queue_file_io_task(fileobj, chunk, index)\n\n\nclass ImmediatelyWriteIOGetObjectTask(GetObjectTask):\n    \"\"\"GetObjectTask that immediately writes to the provided file object\n\n    This is useful for downloads where it is known only one thread is\n    downloading the object so there is no reason to go through the\n    overhead of using an IO queue and executor.\n    \"\"\"\n\n    def _handle_io(self, download_output_manager, fileobj, chunk, index):\n        task = download_output_manager.get_io_write_task(fileobj, chunk, index)\n        task()\n\n\nclass IOWriteTask(Task):\n    def _main(self, fileobj, data, offset):\n        \"\"\"Pulls off an io queue to write contents to a file\n\n        :param fileobj: The file handle to write content to\n        :param data: The data to write\n        :param offset: The offset to write the data to.\n        \"\"\"\n        fileobj.seek(offset)\n        fileobj.write(data)\n\n\nclass IOStreamingWriteTask(Task):\n    \"\"\"Task for writing data to a non-seekable stream.\"\"\"\n\n    def _main(self, fileobj, data):\n        \"\"\"Write data to a fileobj.\n\n        Data will be written directly to the fileobj without\n        any prior seeking.\n\n        :param fileobj: The fileobj to write content to\n        :param data: The data to write\n\n        \"\"\"\n        fileobj.write(data)\n\n\nclass IORenameFileTask(Task):\n    \"\"\"A task to rename a temporary file to its final filename\n\n    :param fileobj: The file handle that content was written to.\n    :param final_filename: The final name of the file to rename to\n        upon completion of writing the contents.\n    :param osutil: OS utility\n    \"\"\"\n\n    def _main(self, fileobj, final_filename, osutil):\n        fileobj.close()\n        osutil.rename_file(fileobj.name, final_filename)\n\n\nclass IOCloseTask(Task):\n    \"\"\"A task to close out a file once the download is complete.\n\n    :param fileobj: The fileobj to close.\n    \"\"\"\n\n    def _main(self, fileobj):\n        fileobj.close()\n\n\nclass CompleteDownloadNOOPTask(Task):\n    \"\"\"A NOOP task to serve as an indicator that the download is complete\n\n    Note that the default for is_final is set to True because this should\n    always be the last task.\n    \"\"\"\n\n    def __init__(\n        self,\n        transfer_coordinator,\n        main_kwargs=None,\n        pending_main_kwargs=None,\n        done_callbacks=None,\n        is_final=True,\n    ):\n        super().__init__(\n            transfer_coordinator=transfer_coordinator,\n            main_kwargs=main_kwargs,\n            pending_main_kwargs=pending_main_kwargs,\n            done_callbacks=done_callbacks,\n            is_final=is_final,\n        )\n\n    def _main(self):\n        pass\n\n\nclass DownloadChunkIterator:\n    def __init__(self, body, chunksize):\n        \"\"\"Iterator to chunk out a downloaded S3 stream\n\n        :param body: A readable file-like object\n        :param chunksize: The amount to read each time\n        \"\"\"\n        self._body = body\n        self._chunksize = chunksize\n        self._num_reads = 0\n\n    def __iter__(self):\n        return self\n\n    def __next__(self):\n        chunk = self._body.read(self._chunksize)\n        self._num_reads += 1\n        if chunk:\n            return chunk\n        elif self._num_reads == 1:\n            # Even though the response may have not had any\n            # content, we still want to account for an empty object's\n            # existence so return the empty chunk for that initial\n            # read.\n            return chunk\n        raise StopIteration()\n\n    next = __next__\n\n\nclass DeferQueue:\n    \"\"\"IO queue that defers write requests until they are queued sequentially.\n\n    This class is used to track IO data for a *single* fileobj.\n\n    You can send data to this queue, and it will defer any IO write requests\n    until it has the next contiguous block available (starting at 0).\n\n    \"\"\"\n\n    def __init__(self):\n        self._writes = []\n        self._pending_offsets = set()\n        self._next_offset = 0\n\n    def request_writes(self, offset, data):\n        \"\"\"Request any available writes given new incoming data.\n\n        You call this method by providing new data along with the\n        offset associated with the data.  If that new data unlocks\n        any contiguous writes that can now be submitted, this\n        method will return all applicable writes.\n\n        This is done with 1 method call so you don't have to\n        make two method calls (put(), get()) which acquires a lock\n        each method call.\n\n        \"\"\"\n        if offset < self._next_offset:\n            # This is a request for a write that we've already\n            # seen.  This can happen in the event of a retry\n            # where if we retry at at offset N/2, we'll requeue\n            # offsets 0-N/2 again.\n            return []\n        writes = []\n        if offset in self._pending_offsets:\n            # We've already queued this offset so this request is\n            # a duplicate.  In this case we should ignore\n            # this request and prefer what's already queued.\n            return []\n        heapq.heappush(self._writes, (offset, data))\n        self._pending_offsets.add(offset)\n        while self._writes and self._writes[0][0] == self._next_offset:\n            next_write = heapq.heappop(self._writes)\n            writes.append({'offset': next_write[0], 'data': next_write[1]})\n            self._pending_offsets.remove(next_write[0])\n            self._next_offset += len(next_write[1])\n        return writes\n"}