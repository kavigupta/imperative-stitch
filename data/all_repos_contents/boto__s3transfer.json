{"setup.py": "#!/usr/bin/env python\nimport os\nimport re\n\nfrom setuptools import find_packages, setup\n\nROOT = os.path.dirname(__file__)\nVERSION_RE = re.compile(r'''__version__ = ['\"]([0-9.]+)['\"]''')\n\n\nrequires = [\n    'botocore>=1.33.2,<2.0a.0',\n]\n\n\ndef get_version():\n    init = open(os.path.join(ROOT, 's3transfer', '__init__.py')).read()\n    return VERSION_RE.search(init).group(1)\n\n\nsetup(\n    name='s3transfer',\n    version=get_version(),\n    description='An Amazon S3 Transfer Manager',\n    long_description=open('README.rst').read(),\n    author='Amazon Web Services',\n    author_email='kyknapp1@gmail.com',\n    url='https://github.com/boto/s3transfer',\n    packages=find_packages(exclude=['tests*']),\n    include_package_data=True,\n    install_requires=requires,\n    extras_require={\n        'crt': 'botocore[crt]>=1.33.2,<2.0a.0',\n    },\n    license=\"Apache License 2.0\",\n    python_requires=\">= 3.8\",\n    classifiers=[\n        'Development Status :: 3 - Alpha',\n        'Intended Audience :: Developers',\n        'Natural Language :: English',\n        'License :: OSI Approved :: Apache Software License',\n        'Programming Language :: Python',\n        'Programming Language :: Python :: 3',\n        'Programming Language :: Python :: 3 :: Only',\n        'Programming Language :: Python :: 3.8',\n        'Programming Language :: Python :: 3.9',\n        'Programming Language :: Python :: 3.10',\n        'Programming Language :: Python :: 3.11',\n        'Programming Language :: Python :: 3.12',\n    ],\n)\n", "s3transfer/delete.py": "# Copyright 2016 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nfrom s3transfer.tasks import SubmissionTask, Task\n\n\nclass DeleteSubmissionTask(SubmissionTask):\n    \"\"\"Task for submitting tasks to execute an object deletion.\"\"\"\n\n    def _submit(self, client, request_executor, transfer_future, **kwargs):\n        \"\"\"\n        :param client: The client associated with the transfer manager\n\n        :type config: s3transfer.manager.TransferConfig\n        :param config: The transfer config associated with the transfer\n            manager\n\n        :type osutil: s3transfer.utils.OSUtil\n        :param osutil: The os utility associated to the transfer manager\n\n        :type request_executor: s3transfer.futures.BoundedExecutor\n        :param request_executor: The request executor associated with the\n            transfer manager\n\n        :type transfer_future: s3transfer.futures.TransferFuture\n        :param transfer_future: The transfer future associated with the\n            transfer request that tasks are being submitted for\n        \"\"\"\n        call_args = transfer_future.meta.call_args\n\n        self._transfer_coordinator.submit(\n            request_executor,\n            DeleteObjectTask(\n                transfer_coordinator=self._transfer_coordinator,\n                main_kwargs={\n                    'client': client,\n                    'bucket': call_args.bucket,\n                    'key': call_args.key,\n                    'extra_args': call_args.extra_args,\n                },\n                is_final=True,\n            ),\n        )\n\n\nclass DeleteObjectTask(Task):\n    def _main(self, client, bucket, key, extra_args):\n        \"\"\"\n\n        :param client: The S3 client to use when calling DeleteObject\n\n        :type bucket: str\n        :param bucket: The name of the bucket.\n\n        :type key: str\n        :param key: The name of the object to delete.\n\n        :type extra_args: dict\n        :param extra_args: Extra arguments to pass to the DeleteObject call.\n\n        \"\"\"\n        client.delete_object(Bucket=bucket, Key=key, **extra_args)\n", "s3transfer/manager.py": "# Copyright 2016 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport copy\nimport logging\nimport re\nimport threading\n\nfrom s3transfer.bandwidth import BandwidthLimiter, LeakyBucket\nfrom s3transfer.constants import ALLOWED_DOWNLOAD_ARGS, KB, MB\nfrom s3transfer.copies import CopySubmissionTask\nfrom s3transfer.delete import DeleteSubmissionTask\nfrom s3transfer.download import DownloadSubmissionTask\nfrom s3transfer.exceptions import CancelledError, FatalError\nfrom s3transfer.futures import (\n    IN_MEMORY_DOWNLOAD_TAG,\n    IN_MEMORY_UPLOAD_TAG,\n    BoundedExecutor,\n    TransferCoordinator,\n    TransferFuture,\n    TransferMeta,\n)\nfrom s3transfer.upload import UploadSubmissionTask\nfrom s3transfer.utils import (\n    CallArgs,\n    OSUtils,\n    SlidingWindowSemaphore,\n    TaskSemaphore,\n    add_s3express_defaults,\n    get_callbacks,\n    signal_not_transferring,\n    signal_transferring,\n)\n\nlogger = logging.getLogger(__name__)\n\n\nclass TransferConfig:\n    def __init__(\n        self,\n        multipart_threshold=8 * MB,\n        multipart_chunksize=8 * MB,\n        max_request_concurrency=10,\n        max_submission_concurrency=5,\n        max_request_queue_size=1000,\n        max_submission_queue_size=1000,\n        max_io_queue_size=1000,\n        io_chunksize=256 * KB,\n        num_download_attempts=5,\n        max_in_memory_upload_chunks=10,\n        max_in_memory_download_chunks=10,\n        max_bandwidth=None,\n    ):\n        \"\"\"Configurations for the transfer manager\n\n        :param multipart_threshold: The threshold for which multipart\n            transfers occur.\n\n        :param max_request_concurrency: The maximum number of S3 API\n            transfer-related requests that can happen at a time.\n\n        :param max_submission_concurrency: The maximum number of threads\n            processing a call to a TransferManager method. Processing a\n            call usually entails determining which S3 API requests that need\n            to be enqueued, but does **not** entail making any of the\n            S3 API data transferring requests needed to perform the transfer.\n            The threads controlled by ``max_request_concurrency`` is\n            responsible for that.\n\n        :param multipart_chunksize: The size of each transfer if a request\n            becomes a multipart transfer.\n\n        :param max_request_queue_size: The maximum amount of S3 API requests\n            that can be queued at a time.\n\n        :param max_submission_queue_size: The maximum amount of\n            TransferManager method calls that can be queued at a time.\n\n        :param max_io_queue_size: The maximum amount of read parts that\n            can be queued to be written to disk per download. The default\n            size for each elementin this queue is 8 KB.\n\n        :param io_chunksize: The max size of each chunk in the io queue.\n            Currently, this is size used when reading from the downloaded\n            stream as well.\n\n        :param num_download_attempts: The number of download attempts that\n            will be tried upon errors with downloading an object in S3. Note\n            that these retries account for errors that occur when streaming\n            down the data from s3 (i.e. socket errors and read timeouts that\n            occur after receiving an OK response from s3).\n            Other retryable exceptions such as throttling errors and 5xx errors\n            are already retried by botocore (this default is 5). The\n            ``num_download_attempts`` does not take into account the\n            number of exceptions retried by botocore.\n\n        :param max_in_memory_upload_chunks: The number of chunks that can\n            be stored in memory at a time for all ongoing upload requests.\n            This pertains to chunks of data that need to be stored in memory\n            during an upload if the data is sourced from a file-like object.\n            The total maximum memory footprint due to a in-memory upload\n            chunks is roughly equal to:\n\n                max_in_memory_upload_chunks * multipart_chunksize\n                + max_submission_concurrency * multipart_chunksize\n\n            ``max_submission_concurrency`` has an affect on this value because\n            for each thread pulling data off of a file-like object, they may\n            be waiting with a single read chunk to be submitted for upload\n            because the ``max_in_memory_upload_chunks`` value has been reached\n            by the threads making the upload request.\n\n        :param max_in_memory_download_chunks: The number of chunks that can\n            be buffered in memory and **not** in the io queue at a time for all\n            ongoing download requests. This pertains specifically to file-like\n            objects that cannot be seeked. The total maximum memory footprint\n            due to a in-memory download chunks is roughly equal to:\n\n                max_in_memory_download_chunks * multipart_chunksize\n\n        :param max_bandwidth: The maximum bandwidth that will be consumed\n            in uploading and downloading file content. The value is in terms of\n            bytes per second.\n        \"\"\"\n        self.multipart_threshold = multipart_threshold\n        self.multipart_chunksize = multipart_chunksize\n        self.max_request_concurrency = max_request_concurrency\n        self.max_submission_concurrency = max_submission_concurrency\n        self.max_request_queue_size = max_request_queue_size\n        self.max_submission_queue_size = max_submission_queue_size\n        self.max_io_queue_size = max_io_queue_size\n        self.io_chunksize = io_chunksize\n        self.num_download_attempts = num_download_attempts\n        self.max_in_memory_upload_chunks = max_in_memory_upload_chunks\n        self.max_in_memory_download_chunks = max_in_memory_download_chunks\n        self.max_bandwidth = max_bandwidth\n        self._validate_attrs_are_nonzero()\n\n    def _validate_attrs_are_nonzero(self):\n        for attr, attr_val in self.__dict__.items():\n            if attr_val is not None and attr_val <= 0:\n                raise ValueError(\n                    'Provided parameter %s of value %s must be greater than '\n                    '0.' % (attr, attr_val)\n                )\n\n\nclass TransferManager:\n    ALLOWED_DOWNLOAD_ARGS = ALLOWED_DOWNLOAD_ARGS\n\n    ALLOWED_UPLOAD_ARGS = [\n        'ACL',\n        'CacheControl',\n        'ChecksumAlgorithm',\n        'ContentDisposition',\n        'ContentEncoding',\n        'ContentLanguage',\n        'ContentType',\n        'ExpectedBucketOwner',\n        'Expires',\n        'GrantFullControl',\n        'GrantRead',\n        'GrantReadACP',\n        'GrantWriteACP',\n        'Metadata',\n        'ObjectLockLegalHoldStatus',\n        'ObjectLockMode',\n        'ObjectLockRetainUntilDate',\n        'RequestPayer',\n        'ServerSideEncryption',\n        'StorageClass',\n        'SSECustomerAlgorithm',\n        'SSECustomerKey',\n        'SSECustomerKeyMD5',\n        'SSEKMSKeyId',\n        'SSEKMSEncryptionContext',\n        'Tagging',\n        'WebsiteRedirectLocation',\n    ]\n\n    ALLOWED_COPY_ARGS = ALLOWED_UPLOAD_ARGS + [\n        'CopySourceIfMatch',\n        'CopySourceIfModifiedSince',\n        'CopySourceIfNoneMatch',\n        'CopySourceIfUnmodifiedSince',\n        'CopySourceSSECustomerAlgorithm',\n        'CopySourceSSECustomerKey',\n        'CopySourceSSECustomerKeyMD5',\n        'MetadataDirective',\n        'TaggingDirective',\n    ]\n\n    ALLOWED_DELETE_ARGS = [\n        'MFA',\n        'VersionId',\n        'RequestPayer',\n        'ExpectedBucketOwner',\n    ]\n\n    VALIDATE_SUPPORTED_BUCKET_VALUES = True\n\n    _UNSUPPORTED_BUCKET_PATTERNS = {\n        'S3 Object Lambda': re.compile(\n            r'^arn:(aws).*:s3-object-lambda:[a-z\\-0-9]+:[0-9]{12}:'\n            r'accesspoint[/:][a-zA-Z0-9\\-]{1,63}'\n        ),\n    }\n\n    def __init__(self, client, config=None, osutil=None, executor_cls=None):\n        \"\"\"A transfer manager interface for Amazon S3\n\n        :param client: Client to be used by the manager\n        :param config: TransferConfig to associate specific configurations\n        :param osutil: OSUtils object to use for os-related behavior when\n            using with transfer manager.\n\n        :type executor_cls: s3transfer.futures.BaseExecutor\n        :param executor_cls: The class of executor to use with the transfer\n            manager. By default, concurrent.futures.ThreadPoolExecutor is used.\n        \"\"\"\n        self._client = client\n        self._config = config\n        if config is None:\n            self._config = TransferConfig()\n        self._osutil = osutil\n        if osutil is None:\n            self._osutil = OSUtils()\n        self._coordinator_controller = TransferCoordinatorController()\n        # A counter to create unique id's for each transfer submitted.\n        self._id_counter = 0\n\n        # The executor responsible for making S3 API transfer requests\n        self._request_executor = BoundedExecutor(\n            max_size=self._config.max_request_queue_size,\n            max_num_threads=self._config.max_request_concurrency,\n            tag_semaphores={\n                IN_MEMORY_UPLOAD_TAG: TaskSemaphore(\n                    self._config.max_in_memory_upload_chunks\n                ),\n                IN_MEMORY_DOWNLOAD_TAG: SlidingWindowSemaphore(\n                    self._config.max_in_memory_download_chunks\n                ),\n            },\n            executor_cls=executor_cls,\n        )\n\n        # The executor responsible for submitting the necessary tasks to\n        # perform the desired transfer\n        self._submission_executor = BoundedExecutor(\n            max_size=self._config.max_submission_queue_size,\n            max_num_threads=self._config.max_submission_concurrency,\n            executor_cls=executor_cls,\n        )\n\n        # There is one thread available for writing to disk. It will handle\n        # downloads for all files.\n        self._io_executor = BoundedExecutor(\n            max_size=self._config.max_io_queue_size,\n            max_num_threads=1,\n            executor_cls=executor_cls,\n        )\n\n        # The component responsible for limiting bandwidth usage if it\n        # is configured.\n        self._bandwidth_limiter = None\n        if self._config.max_bandwidth is not None:\n            logger.debug(\n                'Setting max_bandwidth to %s', self._config.max_bandwidth\n            )\n            leaky_bucket = LeakyBucket(self._config.max_bandwidth)\n            self._bandwidth_limiter = BandwidthLimiter(leaky_bucket)\n\n        self._register_handlers()\n\n    @property\n    def client(self):\n        return self._client\n\n    @property\n    def config(self):\n        return self._config\n\n    def upload(self, fileobj, bucket, key, extra_args=None, subscribers=None):\n        \"\"\"Uploads a file to S3\n\n        :type fileobj: str or seekable file-like object\n        :param fileobj: The name of a file to upload or a seekable file-like\n            object to upload. It is recommended to use a filename because\n            file-like objects may result in higher memory usage.\n\n        :type bucket: str\n        :param bucket: The name of the bucket to upload to\n\n        :type key: str\n        :param key: The name of the key to upload to\n\n        :type extra_args: dict\n        :param extra_args: Extra arguments that may be passed to the\n            client operation\n\n        :type subscribers: list(s3transfer.subscribers.BaseSubscriber)\n        :param subscribers: The list of subscribers to be invoked in the\n            order provided based on the event emit during the process of\n            the transfer request.\n\n        :rtype: s3transfer.futures.TransferFuture\n        :returns: Transfer future representing the upload\n        \"\"\"\n        if extra_args is None:\n            extra_args = {}\n        if subscribers is None:\n            subscribers = []\n        self._validate_all_known_args(extra_args, self.ALLOWED_UPLOAD_ARGS)\n        self._validate_if_bucket_supported(bucket)\n        self._add_operation_defaults(bucket, extra_args)\n        call_args = CallArgs(\n            fileobj=fileobj,\n            bucket=bucket,\n            key=key,\n            extra_args=extra_args,\n            subscribers=subscribers,\n        )\n        extra_main_kwargs = {}\n        if self._bandwidth_limiter:\n            extra_main_kwargs['bandwidth_limiter'] = self._bandwidth_limiter\n        return self._submit_transfer(\n            call_args, UploadSubmissionTask, extra_main_kwargs\n        )\n\n    def download(\n        self, bucket, key, fileobj, extra_args=None, subscribers=None\n    ):\n        \"\"\"Downloads a file from S3\n\n        :type bucket: str\n        :param bucket: The name of the bucket to download from\n\n        :type key: str\n        :param key: The name of the key to download from\n\n        :type fileobj: str or seekable file-like object\n        :param fileobj: The name of a file to download or a seekable file-like\n            object to download. It is recommended to use a filename because\n            file-like objects may result in higher memory usage.\n\n        :type extra_args: dict\n        :param extra_args: Extra arguments that may be passed to the\n            client operation\n\n        :type subscribers: list(s3transfer.subscribers.BaseSubscriber)\n        :param subscribers: The list of subscribers to be invoked in the\n            order provided based on the event emit during the process of\n            the transfer request.\n\n        :rtype: s3transfer.futures.TransferFuture\n        :returns: Transfer future representing the download\n        \"\"\"\n        if extra_args is None:\n            extra_args = {}\n        if subscribers is None:\n            subscribers = []\n        self._validate_all_known_args(extra_args, self.ALLOWED_DOWNLOAD_ARGS)\n        self._validate_if_bucket_supported(bucket)\n        call_args = CallArgs(\n            bucket=bucket,\n            key=key,\n            fileobj=fileobj,\n            extra_args=extra_args,\n            subscribers=subscribers,\n        )\n        extra_main_kwargs = {'io_executor': self._io_executor}\n        if self._bandwidth_limiter:\n            extra_main_kwargs['bandwidth_limiter'] = self._bandwidth_limiter\n        return self._submit_transfer(\n            call_args, DownloadSubmissionTask, extra_main_kwargs\n        )\n\n    def copy(\n        self,\n        copy_source,\n        bucket,\n        key,\n        extra_args=None,\n        subscribers=None,\n        source_client=None,\n    ):\n        \"\"\"Copies a file in S3\n\n        :type copy_source: dict\n        :param copy_source: The name of the source bucket, key name of the\n            source object, and optional version ID of the source object. The\n            dictionary format is:\n            ``{'Bucket': 'bucket', 'Key': 'key', 'VersionId': 'id'}``. Note\n            that the ``VersionId`` key is optional and may be omitted.\n\n        :type bucket: str\n        :param bucket: The name of the bucket to copy to\n\n        :type key: str\n        :param key: The name of the key to copy to\n\n        :type extra_args: dict\n        :param extra_args: Extra arguments that may be passed to the\n            client operation\n\n        :type subscribers: a list of subscribers\n        :param subscribers: The list of subscribers to be invoked in the\n            order provided based on the event emit during the process of\n            the transfer request.\n\n        :type source_client: botocore or boto3 Client\n        :param source_client: The client to be used for operation that\n            may happen at the source object. For example, this client is\n            used for the head_object that determines the size of the copy.\n            If no client is provided, the transfer manager's client is used\n            as the client for the source object.\n\n        :rtype: s3transfer.futures.TransferFuture\n        :returns: Transfer future representing the copy\n        \"\"\"\n        if extra_args is None:\n            extra_args = {}\n        if subscribers is None:\n            subscribers = []\n        if source_client is None:\n            source_client = self._client\n        self._validate_all_known_args(extra_args, self.ALLOWED_COPY_ARGS)\n        if isinstance(copy_source, dict):\n            self._validate_if_bucket_supported(copy_source.get('Bucket'))\n        self._validate_if_bucket_supported(bucket)\n        call_args = CallArgs(\n            copy_source=copy_source,\n            bucket=bucket,\n            key=key,\n            extra_args=extra_args,\n            subscribers=subscribers,\n            source_client=source_client,\n        )\n        return self._submit_transfer(call_args, CopySubmissionTask)\n\n    def delete(self, bucket, key, extra_args=None, subscribers=None):\n        \"\"\"Delete an S3 object.\n\n        :type bucket: str\n        :param bucket: The name of the bucket.\n\n        :type key: str\n        :param key: The name of the S3 object to delete.\n\n        :type extra_args: dict\n        :param extra_args: Extra arguments that may be passed to the\n            DeleteObject call.\n\n        :type subscribers: list\n        :param subscribers: A list of subscribers to be invoked during the\n            process of the transfer request.  Note that the ``on_progress``\n            callback is not invoked during object deletion.\n\n        :rtype: s3transfer.futures.TransferFuture\n        :return: Transfer future representing the deletion.\n\n        \"\"\"\n        if extra_args is None:\n            extra_args = {}\n        if subscribers is None:\n            subscribers = []\n        self._validate_all_known_args(extra_args, self.ALLOWED_DELETE_ARGS)\n        self._validate_if_bucket_supported(bucket)\n        call_args = CallArgs(\n            bucket=bucket,\n            key=key,\n            extra_args=extra_args,\n            subscribers=subscribers,\n        )\n        return self._submit_transfer(call_args, DeleteSubmissionTask)\n\n    def _validate_if_bucket_supported(self, bucket):\n        # s3 high level operations don't support some resources\n        # (eg. S3 Object Lambda) only direct API calls are available\n        # for such resources\n        if self.VALIDATE_SUPPORTED_BUCKET_VALUES:\n            for resource, pattern in self._UNSUPPORTED_BUCKET_PATTERNS.items():\n                match = pattern.match(bucket)\n                if match:\n                    raise ValueError(\n                        'TransferManager methods do not support %s '\n                        'resource. Use direct client calls instead.' % resource\n                    )\n\n    def _validate_all_known_args(self, actual, allowed):\n        for kwarg in actual:\n            if kwarg not in allowed:\n                raise ValueError(\n                    \"Invalid extra_args key '%s', \"\n                    \"must be one of: %s\" % (kwarg, ', '.join(allowed))\n                )\n\n    def _add_operation_defaults(self, bucket, extra_args):\n        add_s3express_defaults(bucket, extra_args)\n\n    def _submit_transfer(\n        self, call_args, submission_task_cls, extra_main_kwargs=None\n    ):\n        if not extra_main_kwargs:\n            extra_main_kwargs = {}\n\n        # Create a TransferFuture to return back to the user\n        transfer_future, components = self._get_future_with_components(\n            call_args\n        )\n\n        # Add any provided done callbacks to the created transfer future\n        # to be invoked on the transfer future being complete.\n        for callback in get_callbacks(transfer_future, 'done'):\n            components['coordinator'].add_done_callback(callback)\n\n        # Get the main kwargs needed to instantiate the submission task\n        main_kwargs = self._get_submission_task_main_kwargs(\n            transfer_future, extra_main_kwargs\n        )\n\n        # Submit a SubmissionTask that will submit all of the necessary\n        # tasks needed to complete the S3 transfer.\n        self._submission_executor.submit(\n            submission_task_cls(\n                transfer_coordinator=components['coordinator'],\n                main_kwargs=main_kwargs,\n            )\n        )\n\n        # Increment the unique id counter for future transfer requests\n        self._id_counter += 1\n\n        return transfer_future\n\n    def _get_future_with_components(self, call_args):\n        transfer_id = self._id_counter\n        # Creates a new transfer future along with its components\n        transfer_coordinator = TransferCoordinator(transfer_id=transfer_id)\n        # Track the transfer coordinator for transfers to manage.\n        self._coordinator_controller.add_transfer_coordinator(\n            transfer_coordinator\n        )\n        # Also make sure that the transfer coordinator is removed once\n        # the transfer completes so it does not stick around in memory.\n        transfer_coordinator.add_done_callback(\n            self._coordinator_controller.remove_transfer_coordinator,\n            transfer_coordinator,\n        )\n        components = {\n            'meta': TransferMeta(call_args, transfer_id=transfer_id),\n            'coordinator': transfer_coordinator,\n        }\n        transfer_future = TransferFuture(**components)\n        return transfer_future, components\n\n    def _get_submission_task_main_kwargs(\n        self, transfer_future, extra_main_kwargs\n    ):\n        main_kwargs = {\n            'client': self._client,\n            'config': self._config,\n            'osutil': self._osutil,\n            'request_executor': self._request_executor,\n            'transfer_future': transfer_future,\n        }\n        main_kwargs.update(extra_main_kwargs)\n        return main_kwargs\n\n    def _register_handlers(self):\n        # Register handlers to enable/disable callbacks on uploads.\n        event_name = 'request-created.s3'\n        self._client.meta.events.register_first(\n            event_name,\n            signal_not_transferring,\n            unique_id='s3upload-not-transferring',\n        )\n        self._client.meta.events.register_last(\n            event_name, signal_transferring, unique_id='s3upload-transferring'\n        )\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_value, *args):\n        cancel = False\n        cancel_msg = ''\n        cancel_exc_type = FatalError\n        # If a exception was raised in the context handler, signal to cancel\n        # all of the inprogress futures in the shutdown.\n        if exc_type:\n            cancel = True\n            cancel_msg = str(exc_value)\n            if not cancel_msg:\n                cancel_msg = repr(exc_value)\n            # If it was a KeyboardInterrupt, the cancellation was initiated\n            # by the user.\n            if isinstance(exc_value, KeyboardInterrupt):\n                cancel_exc_type = CancelledError\n        self._shutdown(cancel, cancel_msg, cancel_exc_type)\n\n    def shutdown(self, cancel=False, cancel_msg=''):\n        \"\"\"Shutdown the TransferManager\n\n        It will wait till all transfers complete before it completely shuts\n        down.\n\n        :type cancel: boolean\n        :param cancel: If True, calls TransferFuture.cancel() for\n            all in-progress in transfers. This is useful if you want the\n            shutdown to happen quicker.\n\n        :type cancel_msg: str\n        :param cancel_msg: The message to specify if canceling all in-progress\n            transfers.\n        \"\"\"\n        self._shutdown(cancel, cancel, cancel_msg)\n\n    def _shutdown(self, cancel, cancel_msg, exc_type=CancelledError):\n        if cancel:\n            # Cancel all in-flight transfers if requested, before waiting\n            # for them to complete.\n            self._coordinator_controller.cancel(cancel_msg, exc_type)\n        try:\n            # Wait until there are no more in-progress transfers. This is\n            # wrapped in a try statement because this can be interrupted\n            # with a KeyboardInterrupt that needs to be caught.\n            self._coordinator_controller.wait()\n        except KeyboardInterrupt:\n            # If not errors were raised in the try block, the cancel should\n            # have no coordinators it needs to run cancel on. If there was\n            # an error raised in the try statement we want to cancel all of\n            # the inflight transfers before shutting down to speed that\n            # process up.\n            self._coordinator_controller.cancel('KeyboardInterrupt()')\n            raise\n        finally:\n            # Shutdown all of the executors.\n            self._submission_executor.shutdown()\n            self._request_executor.shutdown()\n            self._io_executor.shutdown()\n\n\nclass TransferCoordinatorController:\n    def __init__(self):\n        \"\"\"Abstraction to control all transfer coordinators\n\n        This abstraction allows the manager to wait for inprogress transfers\n        to complete and cancel all inprogress transfers.\n        \"\"\"\n        self._lock = threading.Lock()\n        self._tracked_transfer_coordinators = set()\n\n    @property\n    def tracked_transfer_coordinators(self):\n        \"\"\"The set of transfer coordinators being tracked\"\"\"\n        with self._lock:\n            # We return a copy because the set is mutable and if you were to\n            # iterate over the set, it may be changing in length due to\n            # additions and removals of transfer coordinators.\n            return copy.copy(self._tracked_transfer_coordinators)\n\n    def add_transfer_coordinator(self, transfer_coordinator):\n        \"\"\"Adds a transfer coordinator of a transfer to be canceled if needed\n\n        :type transfer_coordinator: s3transfer.futures.TransferCoordinator\n        :param transfer_coordinator: The transfer coordinator for the\n            particular transfer\n        \"\"\"\n        with self._lock:\n            self._tracked_transfer_coordinators.add(transfer_coordinator)\n\n    def remove_transfer_coordinator(self, transfer_coordinator):\n        \"\"\"Remove a transfer coordinator from cancellation consideration\n\n        Typically, this method is invoked by the transfer coordinator itself\n        to remove its self when it completes its transfer.\n\n        :type transfer_coordinator: s3transfer.futures.TransferCoordinator\n        :param transfer_coordinator: The transfer coordinator for the\n            particular transfer\n        \"\"\"\n        with self._lock:\n            self._tracked_transfer_coordinators.remove(transfer_coordinator)\n\n    def cancel(self, msg='', exc_type=CancelledError):\n        \"\"\"Cancels all inprogress transfers\n\n        This cancels the inprogress transfers by calling cancel() on all\n        tracked transfer coordinators.\n\n        :param msg: The message to pass on to each transfer coordinator that\n            gets cancelled.\n\n        :param exc_type: The type of exception to set for the cancellation\n        \"\"\"\n        for transfer_coordinator in self.tracked_transfer_coordinators:\n            transfer_coordinator.cancel(msg, exc_type)\n\n    def wait(self):\n        \"\"\"Wait until there are no more inprogress transfers\n\n        This will not stop when failures are encountered and not propagate any\n        of these errors from failed transfers, but it can be interrupted with\n        a KeyboardInterrupt.\n        \"\"\"\n        try:\n            transfer_coordinator = None\n            for transfer_coordinator in self.tracked_transfer_coordinators:\n                transfer_coordinator.result()\n        except KeyboardInterrupt:\n            logger.debug('Received KeyboardInterrupt in wait()')\n            # If Keyboard interrupt is raised while waiting for\n            # the result, then exit out of the wait and raise the\n            # exception\n            if transfer_coordinator:\n                logger.debug(\n                    'On KeyboardInterrupt was waiting for %s',\n                    transfer_coordinator,\n                )\n            raise\n        except Exception:\n            # A general exception could have been thrown because\n            # of result(). We just want to ignore this and continue\n            # because we at least know that the transfer coordinator\n            # has completed.\n            pass\n", "s3transfer/exceptions.py": "# Copyright 2016 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nfrom concurrent.futures import CancelledError\n\n\nclass RetriesExceededError(Exception):\n    def __init__(self, last_exception, msg='Max Retries Exceeded'):\n        super().__init__(msg)\n        self.last_exception = last_exception\n\n\nclass S3UploadFailedError(Exception):\n    pass\n\n\nclass InvalidSubscriberMethodError(Exception):\n    pass\n\n\nclass TransferNotDoneError(Exception):\n    pass\n\n\nclass FatalError(CancelledError):\n    \"\"\"A CancelledError raised from an error in the TransferManager\"\"\"\n\n    pass\n", "s3transfer/subscribers.py": "# Copyright 2016 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nfrom functools import lru_cache\n\nfrom s3transfer.compat import accepts_kwargs\nfrom s3transfer.exceptions import InvalidSubscriberMethodError\n\n\nclass BaseSubscriber:\n    \"\"\"The base subscriber class\n\n    It is recommended that all subscriber implementations subclass and then\n    override the subscription methods (i.e. on_{subsribe_type}() methods).\n    \"\"\"\n\n    VALID_SUBSCRIBER_TYPES = ['queued', 'progress', 'done']\n\n    def __new__(cls, *args, **kwargs):\n        cls._validate_subscriber_methods()\n        return super().__new__(cls)\n\n    @classmethod\n    @lru_cache()\n    def _validate_subscriber_methods(cls):\n        for subscriber_type in cls.VALID_SUBSCRIBER_TYPES:\n            subscriber_method = getattr(cls, 'on_' + subscriber_type)\n            if not callable(subscriber_method):\n                raise InvalidSubscriberMethodError(\n                    'Subscriber method %s must be callable.'\n                    % subscriber_method\n                )\n\n            if not accepts_kwargs(subscriber_method):\n                raise InvalidSubscriberMethodError(\n                    'Subscriber method %s must accept keyword '\n                    'arguments (**kwargs)' % subscriber_method\n                )\n\n    def on_queued(self, future, **kwargs):\n        \"\"\"Callback to be invoked when transfer request gets queued\n\n        This callback can be useful for:\n\n            * Keeping track of how many transfers have been requested\n            * Providing the expected transfer size through\n              future.meta.provide_transfer_size() so a HeadObject would not\n              need to be made for copies and downloads.\n\n        :type future: s3transfer.futures.TransferFuture\n        :param future: The TransferFuture representing the requested transfer.\n        \"\"\"\n        pass\n\n    def on_progress(self, future, bytes_transferred, **kwargs):\n        \"\"\"Callback to be invoked when progress is made on transfer\n\n        This callback can be useful for:\n\n            * Recording and displaying progress\n\n        :type future: s3transfer.futures.TransferFuture\n        :param future: The TransferFuture representing the requested transfer.\n\n        :type bytes_transferred: int\n        :param bytes_transferred: The number of bytes transferred for that\n            invocation of the callback. Note that a negative amount can be\n            provided, which usually indicates that an in-progress request\n            needed to be retried and thus progress was rewound.\n        \"\"\"\n        pass\n\n    def on_done(self, future, **kwargs):\n        \"\"\"Callback to be invoked once a transfer is done\n\n        This callback can be useful for:\n\n            * Recording and displaying whether the transfer succeeded or\n              failed using future.result()\n            * Running some task after the transfer completed like changing\n              the last modified time of a downloaded file.\n\n        :type future: s3transfer.futures.TransferFuture\n        :param future: The TransferFuture representing the requested transfer.\n        \"\"\"\n        pass\n", "s3transfer/utils.py": "# Copyright 2016 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport functools\nimport logging\nimport math\nimport os\nimport random\nimport socket\nimport stat\nimport string\nimport threading\nfrom collections import defaultdict\n\nfrom botocore.exceptions import (\n    IncompleteReadError,\n    ReadTimeoutError,\n    ResponseStreamingError,\n)\nfrom botocore.httpchecksum import AwsChunkedWrapper\nfrom botocore.utils import is_s3express_bucket\n\nfrom s3transfer.compat import SOCKET_ERROR, fallocate, rename_file\n\nMAX_PARTS = 10000\n# The maximum file size you can upload via S3 per request.\n# See: http://docs.aws.amazon.com/AmazonS3/latest/dev/UploadingObjects.html\n# and: http://docs.aws.amazon.com/AmazonS3/latest/dev/qfacts.html\nMAX_SINGLE_UPLOAD_SIZE = 5 * (1024**3)\nMIN_UPLOAD_CHUNKSIZE = 5 * (1024**2)\nlogger = logging.getLogger(__name__)\n\n\nS3_RETRYABLE_DOWNLOAD_ERRORS = (\n    socket.timeout,\n    SOCKET_ERROR,\n    ReadTimeoutError,\n    IncompleteReadError,\n    ResponseStreamingError,\n)\n\n\ndef random_file_extension(num_digits=8):\n    return ''.join(random.choice(string.hexdigits) for _ in range(num_digits))\n\n\ndef signal_not_transferring(request, operation_name, **kwargs):\n    if operation_name in ['PutObject', 'UploadPart'] and hasattr(\n        request.body, 'signal_not_transferring'\n    ):\n        request.body.signal_not_transferring()\n\n\ndef signal_transferring(request, operation_name, **kwargs):\n    if operation_name in ['PutObject', 'UploadPart']:\n        body = request.body\n        if isinstance(body, AwsChunkedWrapper):\n            body = getattr(body, '_raw', None)\n        if hasattr(body, 'signal_transferring'):\n            body.signal_transferring()\n\n\ndef calculate_num_parts(size, part_size):\n    return int(math.ceil(size / float(part_size)))\n\n\ndef calculate_range_parameter(\n    part_size, part_index, num_parts, total_size=None\n):\n    \"\"\"Calculate the range parameter for multipart downloads/copies\n\n    :type part_size: int\n    :param part_size: The size of the part\n\n    :type part_index: int\n    :param part_index: The index for which this parts starts. This index starts\n        at zero\n\n    :type num_parts: int\n    :param num_parts: The total number of parts in the transfer\n\n    :returns: The value to use for Range parameter on downloads or\n        the CopySourceRange parameter for copies\n    \"\"\"\n    # Used to calculate the Range parameter\n    start_range = part_index * part_size\n    if part_index == num_parts - 1:\n        end_range = ''\n        if total_size is not None:\n            end_range = str(total_size - 1)\n    else:\n        end_range = start_range + part_size - 1\n    range_param = f'bytes={start_range}-{end_range}'\n    return range_param\n\n\ndef get_callbacks(transfer_future, callback_type):\n    \"\"\"Retrieves callbacks from a subscriber\n\n    :type transfer_future: s3transfer.futures.TransferFuture\n    :param transfer_future: The transfer future the subscriber is associated\n        to.\n\n    :type callback_type: str\n    :param callback_type: The type of callback to retrieve from the subscriber.\n        Valid types include:\n            * 'queued'\n            * 'progress'\n            * 'done'\n\n    :returns: A list of callbacks for the type specified. All callbacks are\n        preinjected with the transfer future.\n    \"\"\"\n    callbacks = []\n    for subscriber in transfer_future.meta.call_args.subscribers:\n        callback_name = 'on_' + callback_type\n        if hasattr(subscriber, callback_name):\n            callbacks.append(\n                functools.partial(\n                    getattr(subscriber, callback_name), future=transfer_future\n                )\n            )\n    return callbacks\n\n\ndef invoke_progress_callbacks(callbacks, bytes_transferred):\n    \"\"\"Calls all progress callbacks\n\n    :param callbacks: A list of progress callbacks to invoke\n    :param bytes_transferred: The number of bytes transferred. This is passed\n        to the callbacks. If no bytes were transferred the callbacks will not\n        be invoked because no progress was achieved. It is also possible\n        to receive a negative amount which comes from retrying a transfer\n        request.\n    \"\"\"\n    # Only invoke the callbacks if bytes were actually transferred.\n    if bytes_transferred:\n        for callback in callbacks:\n            callback(bytes_transferred=bytes_transferred)\n\n\ndef get_filtered_dict(original_dict, whitelisted_keys):\n    \"\"\"Gets a dictionary filtered by whitelisted keys\n\n    :param original_dict: The original dictionary of arguments to source keys\n        and values.\n    :param whitelisted_key: A list of keys to include in the filtered\n        dictionary.\n\n    :returns: A dictionary containing key/values from the original dictionary\n        whose key was included in the whitelist\n    \"\"\"\n    filtered_dict = {}\n    for key, value in original_dict.items():\n        if key in whitelisted_keys:\n            filtered_dict[key] = value\n    return filtered_dict\n\n\nclass CallArgs:\n    def __init__(self, **kwargs):\n        \"\"\"A class that records call arguments\n\n        The call arguments must be passed as keyword arguments. It will set\n        each keyword argument as an attribute of the object along with its\n        associated value.\n        \"\"\"\n        for arg, value in kwargs.items():\n            setattr(self, arg, value)\n\n\nclass FunctionContainer:\n    \"\"\"An object that contains a function and any args or kwargs to call it\n\n    When called the provided function will be called with provided args\n    and kwargs.\n    \"\"\"\n\n    def __init__(self, func, *args, **kwargs):\n        self._func = func\n        self._args = args\n        self._kwargs = kwargs\n\n    def __repr__(self):\n        return 'Function: {} with args {} and kwargs {}'.format(\n            self._func, self._args, self._kwargs\n        )\n\n    def __call__(self):\n        return self._func(*self._args, **self._kwargs)\n\n\nclass CountCallbackInvoker:\n    \"\"\"An abstraction to invoke a callback when a shared count reaches zero\n\n    :param callback: Callback invoke when finalized count reaches zero\n    \"\"\"\n\n    def __init__(self, callback):\n        self._lock = threading.Lock()\n        self._callback = callback\n        self._count = 0\n        self._is_finalized = False\n\n    @property\n    def current_count(self):\n        with self._lock:\n            return self._count\n\n    def increment(self):\n        \"\"\"Increment the count by one\"\"\"\n        with self._lock:\n            if self._is_finalized:\n                raise RuntimeError(\n                    'Counter has been finalized it can no longer be '\n                    'incremented.'\n                )\n            self._count += 1\n\n    def decrement(self):\n        \"\"\"Decrement the count by one\"\"\"\n        with self._lock:\n            if self._count == 0:\n                raise RuntimeError(\n                    'Counter is at zero. It cannot dip below zero'\n                )\n            self._count -= 1\n            if self._is_finalized and self._count == 0:\n                self._callback()\n\n    def finalize(self):\n        \"\"\"Finalize the counter\n\n        Once finalized, the counter never be incremented and the callback\n        can be invoked once the count reaches zero\n        \"\"\"\n        with self._lock:\n            self._is_finalized = True\n            if self._count == 0:\n                self._callback()\n\n\nclass OSUtils:\n    _MAX_FILENAME_LEN = 255\n\n    def get_file_size(self, filename):\n        return os.path.getsize(filename)\n\n    def open_file_chunk_reader(self, filename, start_byte, size, callbacks):\n        return ReadFileChunk.from_filename(\n            filename, start_byte, size, callbacks, enable_callbacks=False\n        )\n\n    def open_file_chunk_reader_from_fileobj(\n        self,\n        fileobj,\n        chunk_size,\n        full_file_size,\n        callbacks,\n        close_callbacks=None,\n    ):\n        return ReadFileChunk(\n            fileobj,\n            chunk_size,\n            full_file_size,\n            callbacks=callbacks,\n            enable_callbacks=False,\n            close_callbacks=close_callbacks,\n        )\n\n    def open(self, filename, mode):\n        return open(filename, mode)\n\n    def remove_file(self, filename):\n        \"\"\"Remove a file, noop if file does not exist.\"\"\"\n        # Unlike os.remove, if the file does not exist,\n        # then this method does nothing.\n        try:\n            os.remove(filename)\n        except OSError:\n            pass\n\n    def rename_file(self, current_filename, new_filename):\n        rename_file(current_filename, new_filename)\n\n    def is_special_file(cls, filename):\n        \"\"\"Checks to see if a file is a special UNIX file.\n\n        It checks if the file is a character special device, block special\n        device, FIFO, or socket.\n\n        :param filename: Name of the file\n\n        :returns: True if the file is a special file. False, if is not.\n        \"\"\"\n        # If it does not exist, it must be a new file so it cannot be\n        # a special file.\n        if not os.path.exists(filename):\n            return False\n        mode = os.stat(filename).st_mode\n        # Character special device.\n        if stat.S_ISCHR(mode):\n            return True\n        # Block special device\n        if stat.S_ISBLK(mode):\n            return True\n        # Named pipe / FIFO\n        if stat.S_ISFIFO(mode):\n            return True\n        # Socket.\n        if stat.S_ISSOCK(mode):\n            return True\n        return False\n\n    def get_temp_filename(self, filename):\n        suffix = os.extsep + random_file_extension()\n        path = os.path.dirname(filename)\n        name = os.path.basename(filename)\n        temp_filename = name[: self._MAX_FILENAME_LEN - len(suffix)] + suffix\n        return os.path.join(path, temp_filename)\n\n    def allocate(self, filename, size):\n        try:\n            with self.open(filename, 'wb') as f:\n                fallocate(f, size)\n        except OSError:\n            self.remove_file(filename)\n            raise\n\n\nclass DeferredOpenFile:\n    def __init__(self, filename, start_byte=0, mode='rb', open_function=open):\n        \"\"\"A class that defers the opening of a file till needed\n\n        This is useful for deferring opening of a file till it is needed\n        in a separate thread, as there is a limit of how many open files\n        there can be in a single thread for most operating systems. The\n        file gets opened in the following methods: ``read()``, ``seek()``,\n        and ``__enter__()``\n\n        :type filename: str\n        :param filename: The name of the file to open\n\n        :type start_byte: int\n        :param start_byte: The byte to seek to when the file is opened.\n\n        :type mode: str\n        :param mode: The mode to use to open the file\n\n        :type open_function: function\n        :param open_function: The function to use to open the file\n        \"\"\"\n        self._filename = filename\n        self._fileobj = None\n        self._start_byte = start_byte\n        self._mode = mode\n        self._open_function = open_function\n\n    def _open_if_needed(self):\n        if self._fileobj is None:\n            self._fileobj = self._open_function(self._filename, self._mode)\n            if self._start_byte != 0:\n                self._fileobj.seek(self._start_byte)\n\n    @property\n    def name(self):\n        return self._filename\n\n    def read(self, amount=None):\n        self._open_if_needed()\n        return self._fileobj.read(amount)\n\n    def write(self, data):\n        self._open_if_needed()\n        self._fileobj.write(data)\n\n    def seek(self, where, whence=0):\n        self._open_if_needed()\n        self._fileobj.seek(where, whence)\n\n    def tell(self):\n        if self._fileobj is None:\n            return self._start_byte\n        return self._fileobj.tell()\n\n    def close(self):\n        if self._fileobj:\n            self._fileobj.close()\n\n    def __enter__(self):\n        self._open_if_needed()\n        return self\n\n    def __exit__(self, *args, **kwargs):\n        self.close()\n\n\nclass ReadFileChunk:\n    def __init__(\n        self,\n        fileobj,\n        chunk_size,\n        full_file_size,\n        callbacks=None,\n        enable_callbacks=True,\n        close_callbacks=None,\n    ):\n        \"\"\"\n\n        Given a file object shown below::\n\n            |___________________________________________________|\n            0          |                 |                 full_file_size\n                       |----chunk_size---|\n                    f.tell()\n\n        :type fileobj: file\n        :param fileobj: File like object\n\n        :type chunk_size: int\n        :param chunk_size: The max chunk size to read.  Trying to read\n            pass the end of the chunk size will behave like you've\n            reached the end of the file.\n\n        :type full_file_size: int\n        :param full_file_size: The entire content length associated\n            with ``fileobj``.\n\n        :type callbacks: A list of function(amount_read)\n        :param callbacks: Called whenever data is read from this object in the\n            order provided.\n\n        :type enable_callbacks: boolean\n        :param enable_callbacks: True if to run callbacks. Otherwise, do not\n            run callbacks\n\n        :type close_callbacks: A list of function()\n        :param close_callbacks: Called when close is called. The function\n            should take no arguments.\n        \"\"\"\n        self._fileobj = fileobj\n        self._start_byte = self._fileobj.tell()\n        self._size = self._calculate_file_size(\n            self._fileobj,\n            requested_size=chunk_size,\n            start_byte=self._start_byte,\n            actual_file_size=full_file_size,\n        )\n        # _amount_read represents the position in the chunk and may exceed\n        # the chunk size, but won't allow reads out of bounds.\n        self._amount_read = 0\n        self._callbacks = callbacks\n        if callbacks is None:\n            self._callbacks = []\n        self._callbacks_enabled = enable_callbacks\n        self._close_callbacks = close_callbacks\n        if close_callbacks is None:\n            self._close_callbacks = close_callbacks\n\n    @classmethod\n    def from_filename(\n        cls,\n        filename,\n        start_byte,\n        chunk_size,\n        callbacks=None,\n        enable_callbacks=True,\n    ):\n        \"\"\"Convenience factory function to create from a filename.\n\n        :type start_byte: int\n        :param start_byte: The first byte from which to start reading.\n\n        :type chunk_size: int\n        :param chunk_size: The max chunk size to read.  Trying to read\n            pass the end of the chunk size will behave like you've\n            reached the end of the file.\n\n        :type full_file_size: int\n        :param full_file_size: The entire content length associated\n            with ``fileobj``.\n\n        :type callbacks: function(amount_read)\n        :param callbacks: Called whenever data is read from this object.\n\n        :type enable_callbacks: bool\n        :param enable_callbacks: Indicate whether to invoke callback\n            during read() calls.\n\n        :rtype: ``ReadFileChunk``\n        :return: A new instance of ``ReadFileChunk``\n\n        \"\"\"\n        f = open(filename, 'rb')\n        f.seek(start_byte)\n        file_size = os.fstat(f.fileno()).st_size\n        return cls(f, chunk_size, file_size, callbacks, enable_callbacks)\n\n    def _calculate_file_size(\n        self, fileobj, requested_size, start_byte, actual_file_size\n    ):\n        max_chunk_size = actual_file_size - start_byte\n        return min(max_chunk_size, requested_size)\n\n    def read(self, amount=None):\n        amount_left = max(self._size - self._amount_read, 0)\n        if amount is None:\n            amount_to_read = amount_left\n        else:\n            amount_to_read = min(amount_left, amount)\n        data = self._fileobj.read(amount_to_read)\n        self._amount_read += len(data)\n        if self._callbacks is not None and self._callbacks_enabled:\n            invoke_progress_callbacks(self._callbacks, len(data))\n        return data\n\n    def signal_transferring(self):\n        self.enable_callback()\n        if hasattr(self._fileobj, 'signal_transferring'):\n            self._fileobj.signal_transferring()\n\n    def signal_not_transferring(self):\n        self.disable_callback()\n        if hasattr(self._fileobj, 'signal_not_transferring'):\n            self._fileobj.signal_not_transferring()\n\n    def enable_callback(self):\n        self._callbacks_enabled = True\n\n    def disable_callback(self):\n        self._callbacks_enabled = False\n\n    def seek(self, where, whence=0):\n        if whence not in (0, 1, 2):\n            # Mimic io's error for invalid whence values\n            raise ValueError(f\"invalid whence ({whence}, should be 0, 1 or 2)\")\n\n        # Recalculate where based on chunk attributes so seek from file\n        # start (whence=0) is always used\n        where += self._start_byte\n        if whence == 1:\n            where += self._amount_read\n        elif whence == 2:\n            where += self._size\n\n        self._fileobj.seek(max(where, self._start_byte))\n        if self._callbacks is not None and self._callbacks_enabled:\n            # To also rewind the callback() for an accurate progress report\n            bounded_where = max(min(where - self._start_byte, self._size), 0)\n            bounded_amount_read = min(self._amount_read, self._size)\n            amount = bounded_where - bounded_amount_read\n            invoke_progress_callbacks(\n                self._callbacks, bytes_transferred=amount\n            )\n        self._amount_read = max(where - self._start_byte, 0)\n\n    def close(self):\n        if self._close_callbacks is not None and self._callbacks_enabled:\n            for callback in self._close_callbacks:\n                callback()\n        self._fileobj.close()\n\n    def tell(self):\n        return self._amount_read\n\n    def __len__(self):\n        # __len__ is defined because requests will try to determine the length\n        # of the stream to set a content length.  In the normal case\n        # of the file it will just stat the file, but we need to change that\n        # behavior.  By providing a __len__, requests will use that instead\n        # of stat'ing the file.\n        return self._size\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, *args, **kwargs):\n        self.close()\n\n    def __iter__(self):\n        # This is a workaround for http://bugs.python.org/issue17575\n        # Basically httplib will try to iterate over the contents, even\n        # if its a file like object.  This wasn't noticed because we've\n        # already exhausted the stream so iterating over the file immediately\n        # stops, which is what we're simulating here.\n        return iter([])\n\n\nclass StreamReaderProgress:\n    \"\"\"Wrapper for a read only stream that adds progress callbacks.\"\"\"\n\n    def __init__(self, stream, callbacks=None):\n        self._stream = stream\n        self._callbacks = callbacks\n        if callbacks is None:\n            self._callbacks = []\n\n    def read(self, *args, **kwargs):\n        value = self._stream.read(*args, **kwargs)\n        invoke_progress_callbacks(self._callbacks, len(value))\n        return value\n\n\nclass NoResourcesAvailable(Exception):\n    pass\n\n\nclass TaskSemaphore:\n    def __init__(self, count):\n        \"\"\"A semaphore for the purpose of limiting the number of tasks\n\n        :param count: The size of semaphore\n        \"\"\"\n        self._semaphore = threading.Semaphore(count)\n\n    def acquire(self, tag, blocking=True):\n        \"\"\"Acquire the semaphore\n\n        :param tag: A tag identifying what is acquiring the semaphore. Note\n            that this is not really needed to directly use this class but is\n            needed for API compatibility with the SlidingWindowSemaphore\n            implementation.\n        :param block: If True, block until it can be acquired. If False,\n            do not block and raise an exception if cannot be acquired.\n\n        :returns: A token (can be None) to use when releasing the semaphore\n        \"\"\"\n        logger.debug(\"Acquiring %s\", tag)\n        if not self._semaphore.acquire(blocking):\n            raise NoResourcesAvailable(\"Cannot acquire tag '%s'\" % tag)\n\n    def release(self, tag, acquire_token):\n        \"\"\"Release the semaphore\n\n        :param tag: A tag identifying what is releasing the semaphore\n        :param acquire_token:  The token returned from when the semaphore was\n            acquired. Note that this is not really needed to directly use this\n            class but is needed for API compatibility with the\n            SlidingWindowSemaphore implementation.\n        \"\"\"\n        logger.debug(f\"Releasing acquire {tag}/{acquire_token}\")\n        self._semaphore.release()\n\n\nclass SlidingWindowSemaphore(TaskSemaphore):\n    \"\"\"A semaphore used to coordinate sequential resource access.\n\n    This class is similar to the stdlib BoundedSemaphore:\n\n    * It's initialized with a count.\n    * Each call to ``acquire()`` decrements the counter.\n    * If the count is at zero, then ``acquire()`` will either block until the\n      count increases, or if ``blocking=False``, then it will raise\n      a NoResourcesAvailable exception indicating that it failed to acquire the\n      semaphore.\n\n    The main difference is that this semaphore is used to limit\n    access to a resource that requires sequential access.  For example,\n    if I want to access resource R that has 20 subresources R_0 - R_19,\n    this semaphore can also enforce that you only have a max range of\n    10 at any given point in time.  You must also specify a tag name\n    when you acquire the semaphore.  The sliding window semantics apply\n    on a per tag basis.  The internal count will only be incremented\n    when the minimum sequence number for a tag is released.\n\n    \"\"\"\n\n    def __init__(self, count):\n        self._count = count\n        # Dict[tag, next_sequence_number].\n        self._tag_sequences = defaultdict(int)\n        self._lowest_sequence = {}\n        self._lock = threading.Lock()\n        self._condition = threading.Condition(self._lock)\n        # Dict[tag, List[sequence_number]]\n        self._pending_release = {}\n\n    def current_count(self):\n        with self._lock:\n            return self._count\n\n    def acquire(self, tag, blocking=True):\n        logger.debug(\"Acquiring %s\", tag)\n        self._condition.acquire()\n        try:\n            if self._count == 0:\n                if not blocking:\n                    raise NoResourcesAvailable(\"Cannot acquire tag '%s'\" % tag)\n                else:\n                    while self._count == 0:\n                        self._condition.wait()\n            # self._count is no longer zero.\n            # First, check if this is the first time we're seeing this tag.\n            sequence_number = self._tag_sequences[tag]\n            if sequence_number == 0:\n                # First time seeing the tag, so record we're at 0.\n                self._lowest_sequence[tag] = sequence_number\n            self._tag_sequences[tag] += 1\n            self._count -= 1\n            return sequence_number\n        finally:\n            self._condition.release()\n\n    def release(self, tag, acquire_token):\n        sequence_number = acquire_token\n        logger.debug(\"Releasing acquire %s/%s\", tag, sequence_number)\n        self._condition.acquire()\n        try:\n            if tag not in self._tag_sequences:\n                raise ValueError(\"Attempted to release unknown tag: %s\" % tag)\n            max_sequence = self._tag_sequences[tag]\n            if self._lowest_sequence[tag] == sequence_number:\n                # We can immediately process this request and free up\n                # resources.\n                self._lowest_sequence[tag] += 1\n                self._count += 1\n                self._condition.notify()\n                queued = self._pending_release.get(tag, [])\n                while queued:\n                    if self._lowest_sequence[tag] == queued[-1]:\n                        queued.pop()\n                        self._lowest_sequence[tag] += 1\n                        self._count += 1\n                    else:\n                        break\n            elif self._lowest_sequence[tag] < sequence_number < max_sequence:\n                # We can't do anything right now because we're still waiting\n                # for the min sequence for the tag to be released.  We have\n                # to queue this for pending release.\n                self._pending_release.setdefault(tag, []).append(\n                    sequence_number\n                )\n                self._pending_release[tag].sort(reverse=True)\n            else:\n                raise ValueError(\n                    \"Attempted to release unknown sequence number \"\n                    \"%s for tag: %s\" % (sequence_number, tag)\n                )\n        finally:\n            self._condition.release()\n\n\nclass ChunksizeAdjuster:\n    def __init__(\n        self,\n        max_size=MAX_SINGLE_UPLOAD_SIZE,\n        min_size=MIN_UPLOAD_CHUNKSIZE,\n        max_parts=MAX_PARTS,\n    ):\n        self.max_size = max_size\n        self.min_size = min_size\n        self.max_parts = max_parts\n\n    def adjust_chunksize(self, current_chunksize, file_size=None):\n        \"\"\"Get a chunksize close to current that fits within all S3 limits.\n\n        :type current_chunksize: int\n        :param current_chunksize: The currently configured chunksize.\n\n        :type file_size: int or None\n        :param file_size: The size of the file to upload. This might be None\n            if the object being transferred has an unknown size.\n\n        :returns: A valid chunksize that fits within configured limits.\n        \"\"\"\n        chunksize = current_chunksize\n        if file_size is not None:\n            chunksize = self._adjust_for_max_parts(chunksize, file_size)\n        return self._adjust_for_chunksize_limits(chunksize)\n\n    def _adjust_for_chunksize_limits(self, current_chunksize):\n        if current_chunksize > self.max_size:\n            logger.debug(\n                \"Chunksize greater than maximum chunksize. \"\n                \"Setting to %s from %s.\" % (self.max_size, current_chunksize)\n            )\n            return self.max_size\n        elif current_chunksize < self.min_size:\n            logger.debug(\n                \"Chunksize less than minimum chunksize. \"\n                \"Setting to %s from %s.\" % (self.min_size, current_chunksize)\n            )\n            return self.min_size\n        else:\n            return current_chunksize\n\n    def _adjust_for_max_parts(self, current_chunksize, file_size):\n        chunksize = current_chunksize\n        num_parts = int(math.ceil(file_size / float(chunksize)))\n\n        while num_parts > self.max_parts:\n            chunksize *= 2\n            num_parts = int(math.ceil(file_size / float(chunksize)))\n\n        if chunksize != current_chunksize:\n            logger.debug(\n                \"Chunksize would result in the number of parts exceeding the \"\n                \"maximum. Setting to %s from %s.\"\n                % (chunksize, current_chunksize)\n            )\n\n        return chunksize\n\n\ndef add_s3express_defaults(bucket, extra_args):\n    if is_s3express_bucket(bucket) and \"ChecksumAlgorithm\" not in extra_args:\n        # Default Transfer Operations to S3Express to use CRC32\n        extra_args[\"ChecksumAlgorithm\"] = \"crc32\"\n", "s3transfer/processpool.py": "# Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\"\"\"Speeds up S3 throughput by using processes\n\nGetting Started\n===============\n\nThe :class:`ProcessPoolDownloader` can be used to download a single file by\ncalling :meth:`ProcessPoolDownloader.download_file`:\n\n.. code:: python\n\n     from s3transfer.processpool import ProcessPoolDownloader\n\n     with ProcessPoolDownloader() as downloader:\n          downloader.download_file('mybucket', 'mykey', 'myfile')\n\n\nThis snippet downloads the S3 object located in the bucket ``mybucket`` at the\nkey ``mykey`` to the local file ``myfile``. Any errors encountered during the\ntransfer are not propagated. To determine if a transfer succeeded or\nfailed, use the `Futures`_ interface.\n\n\nThe :class:`ProcessPoolDownloader` can be used to download multiple files as\nwell:\n\n.. code:: python\n\n     from s3transfer.processpool import ProcessPoolDownloader\n\n     with ProcessPoolDownloader() as downloader:\n          downloader.download_file('mybucket', 'mykey', 'myfile')\n          downloader.download_file('mybucket', 'myotherkey', 'myotherfile')\n\n\nWhen running this snippet, the downloading of ``mykey`` and ``myotherkey``\nhappen in parallel. The first ``download_file`` call does not block the\nsecond ``download_file`` call. The snippet blocks when exiting\nthe context manager and blocks until both downloads are complete.\n\nAlternatively, the ``ProcessPoolDownloader`` can be instantiated\nand explicitly be shutdown using :meth:`ProcessPoolDownloader.shutdown`:\n\n.. code:: python\n\n     from s3transfer.processpool import ProcessPoolDownloader\n\n     downloader = ProcessPoolDownloader()\n     downloader.download_file('mybucket', 'mykey', 'myfile')\n     downloader.download_file('mybucket', 'myotherkey', 'myotherfile')\n     downloader.shutdown()\n\n\nFor this code snippet, the call to ``shutdown`` blocks until both\ndownloads are complete.\n\n\nAdditional Parameters\n=====================\n\nAdditional parameters can be provided to the ``download_file`` method:\n\n* ``extra_args``: A dictionary containing any additional client arguments\n  to include in the\n  `GetObject <https://botocore.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html#S3.Client.get_object>`_\n  API request. For example:\n\n  .. code:: python\n\n     from s3transfer.processpool import ProcessPoolDownloader\n\n     with ProcessPoolDownloader() as downloader:\n          downloader.download_file(\n               'mybucket', 'mykey', 'myfile',\n               extra_args={'VersionId': 'myversion'})\n\n\n* ``expected_size``: By default, the downloader will make a HeadObject\n  call to determine the size of the object. To opt-out of this additional\n  API call, you can provide the size of the object in bytes:\n\n  .. code:: python\n\n     from s3transfer.processpool import ProcessPoolDownloader\n\n     MB = 1024 * 1024\n     with ProcessPoolDownloader() as downloader:\n          downloader.download_file(\n               'mybucket', 'mykey', 'myfile', expected_size=2 * MB)\n\n\nFutures\n=======\n\nWhen ``download_file`` is called, it immediately returns a\n:class:`ProcessPoolTransferFuture`. The future can be used to poll the state\nof a particular transfer. To get the result of the download,\ncall :meth:`ProcessPoolTransferFuture.result`. The method blocks\nuntil the transfer completes, whether it succeeds or fails. For example:\n\n.. code:: python\n\n     from s3transfer.processpool import ProcessPoolDownloader\n\n     with ProcessPoolDownloader() as downloader:\n          future = downloader.download_file('mybucket', 'mykey', 'myfile')\n          print(future.result())\n\n\nIf the download succeeds, the future returns ``None``:\n\n.. code:: python\n\n     None\n\n\nIf the download fails, the exception causing the failure is raised. For\nexample, if ``mykey`` did not exist, the following error would be raised\n\n\n.. code:: python\n\n     botocore.exceptions.ClientError: An error occurred (404) when calling the HeadObject operation: Not Found\n\n\n.. note::\n\n    :meth:`ProcessPoolTransferFuture.result` can only be called while the\n    ``ProcessPoolDownloader`` is running (e.g. before calling ``shutdown`` or\n    inside the context manager).\n\n\nProcess Pool Configuration\n==========================\n\nBy default, the downloader has the following configuration options:\n\n* ``multipart_threshold``: The threshold size for performing ranged downloads\n  in bytes. By default, ranged downloads happen for S3 objects that are\n  greater than or equal to 8 MB in size.\n\n* ``multipart_chunksize``: The size of each ranged download in bytes. By\n  default, the size of each ranged download is 8 MB.\n\n* ``max_request_processes``: The maximum number of processes used to download\n  S3 objects. By default, the maximum is 10 processes.\n\n\nTo change the default configuration, use the :class:`ProcessTransferConfig`:\n\n.. code:: python\n\n     from s3transfer.processpool import ProcessPoolDownloader\n     from s3transfer.processpool import ProcessTransferConfig\n\n     config = ProcessTransferConfig(\n          multipart_threshold=64 * 1024 * 1024,  # 64 MB\n          max_request_processes=50\n     )\n     downloader = ProcessPoolDownloader(config=config)\n\n\nClient Configuration\n====================\n\nThe process pool downloader creates ``botocore`` clients on your behalf. In\norder to affect how the client is created, pass the keyword arguments\nthat would have been used in the :meth:`botocore.Session.create_client` call:\n\n.. code:: python\n\n\n     from s3transfer.processpool import ProcessPoolDownloader\n     from s3transfer.processpool import ProcessTransferConfig\n\n     downloader = ProcessPoolDownloader(\n          client_kwargs={'region_name': 'us-west-2'})\n\n\nThis snippet ensures that all clients created by the ``ProcessPoolDownloader``\nare using ``us-west-2`` as their region.\n\n\"\"\"\nimport collections\nimport contextlib\nimport logging\nimport multiprocessing\nimport signal\nimport threading\nfrom copy import deepcopy\n\nimport botocore.session\nfrom botocore.config import Config\n\nfrom s3transfer.compat import MAXINT, BaseManager\nfrom s3transfer.constants import ALLOWED_DOWNLOAD_ARGS, MB, PROCESS_USER_AGENT\nfrom s3transfer.exceptions import CancelledError, RetriesExceededError\nfrom s3transfer.futures import BaseTransferFuture, BaseTransferMeta\nfrom s3transfer.utils import (\n    S3_RETRYABLE_DOWNLOAD_ERRORS,\n    CallArgs,\n    OSUtils,\n    calculate_num_parts,\n    calculate_range_parameter,\n)\n\nlogger = logging.getLogger(__name__)\n\nSHUTDOWN_SIGNAL = 'SHUTDOWN'\n\n# The DownloadFileRequest tuple is submitted from the ProcessPoolDownloader\n# to the GetObjectSubmitter in order for the submitter to begin submitting\n# GetObjectJobs to the GetObjectWorkers.\nDownloadFileRequest = collections.namedtuple(\n    'DownloadFileRequest',\n    [\n        'transfer_id',  # The unique id for the transfer\n        'bucket',  # The bucket to download the object from\n        'key',  # The key to download the object from\n        'filename',  # The user-requested download location\n        'extra_args',  # Extra arguments to provide to client calls\n        'expected_size',  # The user-provided expected size of the download\n    ],\n)\n\n# The GetObjectJob tuple is submitted from the GetObjectSubmitter\n# to the GetObjectWorkers to download the file or parts of the file.\nGetObjectJob = collections.namedtuple(\n    'GetObjectJob',\n    [\n        'transfer_id',  # The unique id for the transfer\n        'bucket',  # The bucket to download the object from\n        'key',  # The key to download the object from\n        'temp_filename',  # The temporary file to write the content to via\n        # completed GetObject calls.\n        'extra_args',  # Extra arguments to provide to the GetObject call\n        'offset',  # The offset to write the content for the temp file.\n        'filename',  # The user-requested download location. The worker\n        # of final GetObjectJob will move the file located at\n        # temp_filename to the location of filename.\n    ],\n)\n\n\n@contextlib.contextmanager\ndef ignore_ctrl_c():\n    original_handler = _add_ignore_handler_for_interrupts()\n    yield\n    signal.signal(signal.SIGINT, original_handler)\n\n\ndef _add_ignore_handler_for_interrupts():\n    # Windows is unable to pickle signal.signal directly so it needs to\n    # be wrapped in a function defined at the module level\n    return signal.signal(signal.SIGINT, signal.SIG_IGN)\n\n\nclass ProcessTransferConfig:\n    def __init__(\n        self,\n        multipart_threshold=8 * MB,\n        multipart_chunksize=8 * MB,\n        max_request_processes=10,\n    ):\n        \"\"\"Configuration for the ProcessPoolDownloader\n\n        :param multipart_threshold: The threshold for which ranged downloads\n            occur.\n\n        :param multipart_chunksize: The chunk size of each ranged download.\n\n        :param max_request_processes: The maximum number of processes that\n            will be making S3 API transfer-related requests at a time.\n        \"\"\"\n        self.multipart_threshold = multipart_threshold\n        self.multipart_chunksize = multipart_chunksize\n        self.max_request_processes = max_request_processes\n\n\nclass ProcessPoolDownloader:\n    def __init__(self, client_kwargs=None, config=None):\n        \"\"\"Downloads S3 objects using process pools\n\n        :type client_kwargs: dict\n        :param client_kwargs: The keyword arguments to provide when\n            instantiating S3 clients. The arguments must match the keyword\n            arguments provided to the\n            `botocore.session.Session.create_client()` method.\n\n        :type config: ProcessTransferConfig\n        :param config: Configuration for the downloader\n        \"\"\"\n        if client_kwargs is None:\n            client_kwargs = {}\n        self._client_factory = ClientFactory(client_kwargs)\n\n        self._transfer_config = config\n        if config is None:\n            self._transfer_config = ProcessTransferConfig()\n\n        self._download_request_queue = multiprocessing.Queue(1000)\n        self._worker_queue = multiprocessing.Queue(1000)\n        self._osutil = OSUtils()\n\n        self._started = False\n        self._start_lock = threading.Lock()\n\n        # These below are initialized in the start() method\n        self._manager = None\n        self._transfer_monitor = None\n        self._submitter = None\n        self._workers = []\n\n    def download_file(\n        self, bucket, key, filename, extra_args=None, expected_size=None\n    ):\n        \"\"\"Downloads the object's contents to a file\n\n        :type bucket: str\n        :param bucket: The name of the bucket to download from\n\n        :type key: str\n        :param key: The name of the key to download from\n\n        :type filename: str\n        :param filename: The name of a file to download to.\n\n        :type extra_args: dict\n        :param extra_args: Extra arguments that may be passed to the\n            client operation\n\n        :type expected_size: int\n        :param expected_size: The expected size in bytes of the download. If\n            provided, the downloader will not call HeadObject to determine the\n            object's size and use the provided value instead. The size is\n            needed to determine whether to do a multipart download.\n\n        :rtype: s3transfer.futures.TransferFuture\n        :returns: Transfer future representing the download\n        \"\"\"\n        self._start_if_needed()\n        if extra_args is None:\n            extra_args = {}\n        self._validate_all_known_args(extra_args)\n        transfer_id = self._transfer_monitor.notify_new_transfer()\n        download_file_request = DownloadFileRequest(\n            transfer_id=transfer_id,\n            bucket=bucket,\n            key=key,\n            filename=filename,\n            extra_args=extra_args,\n            expected_size=expected_size,\n        )\n        logger.debug(\n            'Submitting download file request: %s.', download_file_request\n        )\n        self._download_request_queue.put(download_file_request)\n        call_args = CallArgs(\n            bucket=bucket,\n            key=key,\n            filename=filename,\n            extra_args=extra_args,\n            expected_size=expected_size,\n        )\n        future = self._get_transfer_future(transfer_id, call_args)\n        return future\n\n    def shutdown(self):\n        \"\"\"Shutdown the downloader\n\n        It will wait till all downloads are complete before returning.\n        \"\"\"\n        self._shutdown_if_needed()\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_value, *args):\n        if isinstance(exc_value, KeyboardInterrupt):\n            if self._transfer_monitor is not None:\n                self._transfer_monitor.notify_cancel_all_in_progress()\n        self.shutdown()\n\n    def _start_if_needed(self):\n        with self._start_lock:\n            if not self._started:\n                self._start()\n\n    def _start(self):\n        self._start_transfer_monitor_manager()\n        self._start_submitter()\n        self._start_get_object_workers()\n        self._started = True\n\n    def _validate_all_known_args(self, provided):\n        for kwarg in provided:\n            if kwarg not in ALLOWED_DOWNLOAD_ARGS:\n                download_args = ', '.join(ALLOWED_DOWNLOAD_ARGS)\n                raise ValueError(\n                    f\"Invalid extra_args key '{kwarg}', \"\n                    f\"must be one of: {download_args}\"\n                )\n\n    def _get_transfer_future(self, transfer_id, call_args):\n        meta = ProcessPoolTransferMeta(\n            call_args=call_args, transfer_id=transfer_id\n        )\n        future = ProcessPoolTransferFuture(\n            monitor=self._transfer_monitor, meta=meta\n        )\n        return future\n\n    def _start_transfer_monitor_manager(self):\n        logger.debug('Starting the TransferMonitorManager.')\n        self._manager = TransferMonitorManager()\n        # We do not want Ctrl-C's to cause the manager to shutdown immediately\n        # as worker processes will still need to communicate with it when they\n        # are shutting down. So instead we ignore Ctrl-C and let the manager\n        # be explicitly shutdown when shutting down the downloader.\n        self._manager.start(_add_ignore_handler_for_interrupts)\n        self._transfer_monitor = self._manager.TransferMonitor()\n\n    def _start_submitter(self):\n        logger.debug('Starting the GetObjectSubmitter.')\n        self._submitter = GetObjectSubmitter(\n            transfer_config=self._transfer_config,\n            client_factory=self._client_factory,\n            transfer_monitor=self._transfer_monitor,\n            osutil=self._osutil,\n            download_request_queue=self._download_request_queue,\n            worker_queue=self._worker_queue,\n        )\n        self._submitter.start()\n\n    def _start_get_object_workers(self):\n        logger.debug(\n            'Starting %s GetObjectWorkers.',\n            self._transfer_config.max_request_processes,\n        )\n        for _ in range(self._transfer_config.max_request_processes):\n            worker = GetObjectWorker(\n                queue=self._worker_queue,\n                client_factory=self._client_factory,\n                transfer_monitor=self._transfer_monitor,\n                osutil=self._osutil,\n            )\n            worker.start()\n            self._workers.append(worker)\n\n    def _shutdown_if_needed(self):\n        with self._start_lock:\n            if self._started:\n                self._shutdown()\n\n    def _shutdown(self):\n        self._shutdown_submitter()\n        self._shutdown_get_object_workers()\n        self._shutdown_transfer_monitor_manager()\n        self._started = False\n\n    def _shutdown_transfer_monitor_manager(self):\n        logger.debug('Shutting down the TransferMonitorManager.')\n        self._manager.shutdown()\n\n    def _shutdown_submitter(self):\n        logger.debug('Shutting down the GetObjectSubmitter.')\n        self._download_request_queue.put(SHUTDOWN_SIGNAL)\n        self._submitter.join()\n\n    def _shutdown_get_object_workers(self):\n        logger.debug('Shutting down the GetObjectWorkers.')\n        for _ in self._workers:\n            self._worker_queue.put(SHUTDOWN_SIGNAL)\n        for worker in self._workers:\n            worker.join()\n\n\nclass ProcessPoolTransferFuture(BaseTransferFuture):\n    def __init__(self, monitor, meta):\n        \"\"\"The future associated to a submitted process pool transfer request\n\n        :type monitor: TransferMonitor\n        :param monitor: The monitor associated to the process pool downloader\n\n        :type meta: ProcessPoolTransferMeta\n        :param meta: The metadata associated to the request. This object\n            is visible to the requester.\n        \"\"\"\n        self._monitor = monitor\n        self._meta = meta\n\n    @property\n    def meta(self):\n        return self._meta\n\n    def done(self):\n        return self._monitor.is_done(self._meta.transfer_id)\n\n    def result(self):\n        try:\n            return self._monitor.poll_for_result(self._meta.transfer_id)\n        except KeyboardInterrupt:\n            # For the multiprocessing Manager, a thread is given a single\n            # connection to reuse in communicating between the thread in the\n            # main process and the Manager's process. If a Ctrl-C happens when\n            # polling for the result, it will make the main thread stop trying\n            # to receive from the connection, but the Manager process will not\n            # know that the main process has stopped trying to receive and\n            # will not close the connection. As a result if another message is\n            # sent to the Manager process, the listener in the Manager\n            # processes will not process the new message as it is still trying\n            # trying to process the previous message (that was Ctrl-C'd) and\n            # thus cause the thread in the main process to hang on its send.\n            # The only way around this is to create a new connection and send\n            # messages from that new connection instead.\n            self._monitor._connect()\n            self.cancel()\n            raise\n\n    def cancel(self):\n        self._monitor.notify_exception(\n            self._meta.transfer_id, CancelledError()\n        )\n\n\nclass ProcessPoolTransferMeta(BaseTransferMeta):\n    \"\"\"Holds metadata about the ProcessPoolTransferFuture\"\"\"\n\n    def __init__(self, transfer_id, call_args):\n        self._transfer_id = transfer_id\n        self._call_args = call_args\n        self._user_context = {}\n\n    @property\n    def call_args(self):\n        return self._call_args\n\n    @property\n    def transfer_id(self):\n        return self._transfer_id\n\n    @property\n    def user_context(self):\n        return self._user_context\n\n\nclass ClientFactory:\n    def __init__(self, client_kwargs=None):\n        \"\"\"Creates S3 clients for processes\n\n        Botocore sessions and clients are not pickleable so they cannot be\n        inherited across Process boundaries. Instead, they must be instantiated\n        once a process is running.\n        \"\"\"\n        self._client_kwargs = client_kwargs\n        if self._client_kwargs is None:\n            self._client_kwargs = {}\n\n        client_config = deepcopy(self._client_kwargs.get('config', Config()))\n        if not client_config.user_agent_extra:\n            client_config.user_agent_extra = PROCESS_USER_AGENT\n        else:\n            client_config.user_agent_extra += \" \" + PROCESS_USER_AGENT\n        self._client_kwargs['config'] = client_config\n\n    def create_client(self):\n        \"\"\"Create a botocore S3 client\"\"\"\n        return botocore.session.Session().create_client(\n            's3', **self._client_kwargs\n        )\n\n\nclass TransferMonitor:\n    def __init__(self):\n        \"\"\"Monitors transfers for cross-process communication\n\n        Notifications can be sent to the monitor and information can be\n        retrieved from the monitor for a particular transfer. This abstraction\n        is ran in a ``multiprocessing.managers.BaseManager`` in order to be\n        shared across processes.\n        \"\"\"\n        # TODO: Add logic that removes the TransferState if the transfer is\n        #  marked as done and the reference to the future is no longer being\n        #  held onto. Without this logic, this dictionary will continue to\n        #  grow in size with no limit.\n        self._transfer_states = {}\n        self._id_count = 0\n        self._init_lock = threading.Lock()\n\n    def notify_new_transfer(self):\n        with self._init_lock:\n            transfer_id = self._id_count\n            self._transfer_states[transfer_id] = TransferState()\n            self._id_count += 1\n            return transfer_id\n\n    def is_done(self, transfer_id):\n        \"\"\"Determine a particular transfer is complete\n\n        :param transfer_id: Unique identifier for the transfer\n        :return: True, if done. False, otherwise.\n        \"\"\"\n        return self._transfer_states[transfer_id].done\n\n    def notify_done(self, transfer_id):\n        \"\"\"Notify a particular transfer is complete\n\n        :param transfer_id: Unique identifier for the transfer\n        \"\"\"\n        self._transfer_states[transfer_id].set_done()\n\n    def poll_for_result(self, transfer_id):\n        \"\"\"Poll for the result of a transfer\n\n        :param transfer_id: Unique identifier for the transfer\n        :return: If the transfer succeeded, it will return the result. If the\n            transfer failed, it will raise the exception associated to the\n            failure.\n        \"\"\"\n        self._transfer_states[transfer_id].wait_till_done()\n        exception = self._transfer_states[transfer_id].exception\n        if exception:\n            raise exception\n        return None\n\n    def notify_exception(self, transfer_id, exception):\n        \"\"\"Notify an exception was encountered for a transfer\n\n        :param transfer_id: Unique identifier for the transfer\n        :param exception: The exception encountered for that transfer\n        \"\"\"\n        # TODO: Not all exceptions are pickleable so if we are running\n        # this in a multiprocessing.BaseManager we will want to\n        # make sure to update this signature to ensure pickleability of the\n        # arguments or have the ProxyObject do the serialization.\n        self._transfer_states[transfer_id].exception = exception\n\n    def notify_cancel_all_in_progress(self):\n        for transfer_state in self._transfer_states.values():\n            if not transfer_state.done:\n                transfer_state.exception = CancelledError()\n\n    def get_exception(self, transfer_id):\n        \"\"\"Retrieve the exception encountered for the transfer\n\n        :param transfer_id: Unique identifier for the transfer\n        :return: The exception encountered for that transfer. Otherwise\n            if there were no exceptions, returns None.\n        \"\"\"\n        return self._transfer_states[transfer_id].exception\n\n    def notify_expected_jobs_to_complete(self, transfer_id, num_jobs):\n        \"\"\"Notify the amount of jobs expected for a transfer\n\n        :param transfer_id: Unique identifier for the transfer\n        :param num_jobs: The number of jobs to complete the transfer\n        \"\"\"\n        self._transfer_states[transfer_id].jobs_to_complete = num_jobs\n\n    def notify_job_complete(self, transfer_id):\n        \"\"\"Notify that a single job is completed for a transfer\n\n        :param transfer_id: Unique identifier for the transfer\n        :return: The number of jobs remaining to complete the transfer\n        \"\"\"\n        return self._transfer_states[transfer_id].decrement_jobs_to_complete()\n\n\nclass TransferState:\n    \"\"\"Represents the current state of an individual transfer\"\"\"\n\n    # NOTE: Ideally the TransferState object would be used directly by the\n    # various different abstractions in the ProcessPoolDownloader and remove\n    # the need for the TransferMonitor. However, it would then impose the\n    # constraint that two hops are required to make or get any changes in the\n    # state of a transfer across processes: one hop to get a proxy object for\n    # the TransferState and then a second hop to communicate calling the\n    # specific TransferState method.\n    def __init__(self):\n        self._exception = None\n        self._done_event = threading.Event()\n        self._job_lock = threading.Lock()\n        self._jobs_to_complete = 0\n\n    @property\n    def done(self):\n        return self._done_event.is_set()\n\n    def set_done(self):\n        self._done_event.set()\n\n    def wait_till_done(self):\n        self._done_event.wait(MAXINT)\n\n    @property\n    def exception(self):\n        return self._exception\n\n    @exception.setter\n    def exception(self, val):\n        self._exception = val\n\n    @property\n    def jobs_to_complete(self):\n        return self._jobs_to_complete\n\n    @jobs_to_complete.setter\n    def jobs_to_complete(self, val):\n        self._jobs_to_complete = val\n\n    def decrement_jobs_to_complete(self):\n        with self._job_lock:\n            self._jobs_to_complete -= 1\n            return self._jobs_to_complete\n\n\nclass TransferMonitorManager(BaseManager):\n    pass\n\n\nTransferMonitorManager.register('TransferMonitor', TransferMonitor)\n\n\nclass BaseS3TransferProcess(multiprocessing.Process):\n    def __init__(self, client_factory):\n        super().__init__()\n        self._client_factory = client_factory\n        self._client = None\n\n    def run(self):\n        # Clients are not pickleable so their instantiation cannot happen\n        # in the __init__ for processes that are created under the\n        # spawn method.\n        self._client = self._client_factory.create_client()\n        with ignore_ctrl_c():\n            # By default these processes are ran as child processes to the\n            # main process. Any Ctrl-c encountered in the main process is\n            # propagated to the child process and interrupt it at any time.\n            # To avoid any potentially bad states caused from an interrupt\n            # (i.e. a transfer failing to notify its done or making the\n            # communication protocol become out of sync with the\n            # TransferMonitor), we ignore all Ctrl-C's and allow the main\n            # process to notify these child processes when to stop processing\n            # jobs.\n            self._do_run()\n\n    def _do_run(self):\n        raise NotImplementedError('_do_run()')\n\n\nclass GetObjectSubmitter(BaseS3TransferProcess):\n    def __init__(\n        self,\n        transfer_config,\n        client_factory,\n        transfer_monitor,\n        osutil,\n        download_request_queue,\n        worker_queue,\n    ):\n        \"\"\"Submit GetObjectJobs to fulfill a download file request\n\n        :param transfer_config: Configuration for transfers.\n        :param client_factory: ClientFactory for creating S3 clients.\n        :param transfer_monitor: Monitor for notifying and retrieving state\n            of transfer.\n        :param osutil: OSUtils object to use for os-related behavior when\n            performing the transfer.\n        :param download_request_queue: Queue to retrieve download file\n            requests.\n        :param worker_queue: Queue to submit GetObjectJobs for workers\n            to perform.\n        \"\"\"\n        super().__init__(client_factory)\n        self._transfer_config = transfer_config\n        self._transfer_monitor = transfer_monitor\n        self._osutil = osutil\n        self._download_request_queue = download_request_queue\n        self._worker_queue = worker_queue\n\n    def _do_run(self):\n        while True:\n            download_file_request = self._download_request_queue.get()\n            if download_file_request == SHUTDOWN_SIGNAL:\n                logger.debug('Submitter shutdown signal received.')\n                return\n            try:\n                self._submit_get_object_jobs(download_file_request)\n            except Exception as e:\n                logger.debug(\n                    'Exception caught when submitting jobs for '\n                    'download file request %s: %s',\n                    download_file_request,\n                    e,\n                    exc_info=True,\n                )\n                self._transfer_monitor.notify_exception(\n                    download_file_request.transfer_id, e\n                )\n                self._transfer_monitor.notify_done(\n                    download_file_request.transfer_id\n                )\n\n    def _submit_get_object_jobs(self, download_file_request):\n        size = self._get_size(download_file_request)\n        temp_filename = self._allocate_temp_file(download_file_request, size)\n        if size < self._transfer_config.multipart_threshold:\n            self._submit_single_get_object_job(\n                download_file_request, temp_filename\n            )\n        else:\n            self._submit_ranged_get_object_jobs(\n                download_file_request, temp_filename, size\n            )\n\n    def _get_size(self, download_file_request):\n        expected_size = download_file_request.expected_size\n        if expected_size is None:\n            expected_size = self._client.head_object(\n                Bucket=download_file_request.bucket,\n                Key=download_file_request.key,\n                **download_file_request.extra_args,\n            )['ContentLength']\n        return expected_size\n\n    def _allocate_temp_file(self, download_file_request, size):\n        temp_filename = self._osutil.get_temp_filename(\n            download_file_request.filename\n        )\n        self._osutil.allocate(temp_filename, size)\n        return temp_filename\n\n    def _submit_single_get_object_job(\n        self, download_file_request, temp_filename\n    ):\n        self._notify_jobs_to_complete(download_file_request.transfer_id, 1)\n        self._submit_get_object_job(\n            transfer_id=download_file_request.transfer_id,\n            bucket=download_file_request.bucket,\n            key=download_file_request.key,\n            temp_filename=temp_filename,\n            offset=0,\n            extra_args=download_file_request.extra_args,\n            filename=download_file_request.filename,\n        )\n\n    def _submit_ranged_get_object_jobs(\n        self, download_file_request, temp_filename, size\n    ):\n        part_size = self._transfer_config.multipart_chunksize\n        num_parts = calculate_num_parts(size, part_size)\n        self._notify_jobs_to_complete(\n            download_file_request.transfer_id, num_parts\n        )\n        for i in range(num_parts):\n            offset = i * part_size\n            range_parameter = calculate_range_parameter(\n                part_size, i, num_parts\n            )\n            get_object_kwargs = {'Range': range_parameter}\n            get_object_kwargs.update(download_file_request.extra_args)\n            self._submit_get_object_job(\n                transfer_id=download_file_request.transfer_id,\n                bucket=download_file_request.bucket,\n                key=download_file_request.key,\n                temp_filename=temp_filename,\n                offset=offset,\n                extra_args=get_object_kwargs,\n                filename=download_file_request.filename,\n            )\n\n    def _submit_get_object_job(self, **get_object_job_kwargs):\n        self._worker_queue.put(GetObjectJob(**get_object_job_kwargs))\n\n    def _notify_jobs_to_complete(self, transfer_id, jobs_to_complete):\n        logger.debug(\n            'Notifying %s job(s) to complete for transfer_id %s.',\n            jobs_to_complete,\n            transfer_id,\n        )\n        self._transfer_monitor.notify_expected_jobs_to_complete(\n            transfer_id, jobs_to_complete\n        )\n\n\nclass GetObjectWorker(BaseS3TransferProcess):\n    # TODO: It may make sense to expose these class variables as configuration\n    # options if users want to tweak them.\n    _MAX_ATTEMPTS = 5\n    _IO_CHUNKSIZE = 2 * MB\n\n    def __init__(self, queue, client_factory, transfer_monitor, osutil):\n        \"\"\"Fulfills GetObjectJobs\n\n        Downloads the S3 object, writes it to the specified file, and\n        renames the file to its final location if it completes the final\n        job for a particular transfer.\n\n        :param queue: Queue for retrieving GetObjectJob's\n        :param client_factory: ClientFactory for creating S3 clients\n        :param transfer_monitor: Monitor for notifying\n        :param osutil: OSUtils object to use for os-related behavior when\n            performing the transfer.\n        \"\"\"\n        super().__init__(client_factory)\n        self._queue = queue\n        self._client_factory = client_factory\n        self._transfer_monitor = transfer_monitor\n        self._osutil = osutil\n\n    def _do_run(self):\n        while True:\n            job = self._queue.get()\n            if job == SHUTDOWN_SIGNAL:\n                logger.debug('Worker shutdown signal received.')\n                return\n            if not self._transfer_monitor.get_exception(job.transfer_id):\n                self._run_get_object_job(job)\n            else:\n                logger.debug(\n                    'Skipping get object job %s because there was a previous '\n                    'exception.',\n                    job,\n                )\n            remaining = self._transfer_monitor.notify_job_complete(\n                job.transfer_id\n            )\n            logger.debug(\n                '%s jobs remaining for transfer_id %s.',\n                remaining,\n                job.transfer_id,\n            )\n            if not remaining:\n                self._finalize_download(\n                    job.transfer_id, job.temp_filename, job.filename\n                )\n\n    def _run_get_object_job(self, job):\n        try:\n            self._do_get_object(\n                bucket=job.bucket,\n                key=job.key,\n                temp_filename=job.temp_filename,\n                extra_args=job.extra_args,\n                offset=job.offset,\n            )\n        except Exception as e:\n            logger.debug(\n                'Exception caught when downloading object for '\n                'get object job %s: %s',\n                job,\n                e,\n                exc_info=True,\n            )\n            self._transfer_monitor.notify_exception(job.transfer_id, e)\n\n    def _do_get_object(self, bucket, key, extra_args, temp_filename, offset):\n        last_exception = None\n        for i in range(self._MAX_ATTEMPTS):\n            try:\n                response = self._client.get_object(\n                    Bucket=bucket, Key=key, **extra_args\n                )\n                self._write_to_file(temp_filename, offset, response['Body'])\n                return\n            except S3_RETRYABLE_DOWNLOAD_ERRORS as e:\n                logger.debug(\n                    'Retrying exception caught (%s), '\n                    'retrying request, (attempt %s / %s)',\n                    e,\n                    i + 1,\n                    self._MAX_ATTEMPTS,\n                    exc_info=True,\n                )\n                last_exception = e\n        raise RetriesExceededError(last_exception)\n\n    def _write_to_file(self, filename, offset, body):\n        with open(filename, 'rb+') as f:\n            f.seek(offset)\n            chunks = iter(lambda: body.read(self._IO_CHUNKSIZE), b'')\n            for chunk in chunks:\n                f.write(chunk)\n\n    def _finalize_download(self, transfer_id, temp_filename, filename):\n        if self._transfer_monitor.get_exception(transfer_id):\n            self._osutil.remove_file(temp_filename)\n        else:\n            self._do_file_rename(transfer_id, temp_filename, filename)\n        self._transfer_monitor.notify_done(transfer_id)\n\n    def _do_file_rename(self, transfer_id, temp_filename, filename):\n        try:\n            self._osutil.rename_file(temp_filename, filename)\n        except Exception as e:\n            self._transfer_monitor.notify_exception(transfer_id, e)\n            self._osutil.remove_file(temp_filename)\n", "s3transfer/tasks.py": "# Copyright 2016 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport copy\nimport logging\n\nfrom s3transfer.utils import get_callbacks\n\nlogger = logging.getLogger(__name__)\n\n\nclass Task:\n    \"\"\"A task associated to a TransferFuture request\n\n    This is a base class for other classes to subclass from. All subclassed\n    classes must implement the main() method.\n    \"\"\"\n\n    def __init__(\n        self,\n        transfer_coordinator,\n        main_kwargs=None,\n        pending_main_kwargs=None,\n        done_callbacks=None,\n        is_final=False,\n    ):\n        \"\"\"\n        :type transfer_coordinator: s3transfer.futures.TransferCoordinator\n        :param transfer_coordinator: The context associated to the\n            TransferFuture for which this Task is associated with.\n\n        :type main_kwargs: dict\n        :param main_kwargs: The keyword args that can be immediately supplied\n            to the _main() method of the task\n\n        :type pending_main_kwargs: dict\n        :param pending_main_kwargs: The keyword args that are depended upon\n            by the result from a dependent future(s). The result returned by\n            the future(s) will be used as the value for the keyword argument\n            when _main() is called. The values for each key can be:\n                * a single future - Once completed, its value will be the\n                  result of that single future\n                * a list of futures - Once all of the futures complete, the\n                  value used will be a list of each completed future result\n                  value in order of when they were originally supplied.\n\n        :type done_callbacks: list of callbacks\n        :param done_callbacks: A list of callbacks to call once the task is\n            done completing. Each callback will be called with no arguments\n            and will be called no matter if the task succeeds or an exception\n            is raised.\n\n        :type is_final: boolean\n        :param is_final: True, to indicate that this task is the final task\n            for the TransferFuture request. By setting this value to True, it\n            will set the result of the entire TransferFuture to the result\n            returned by this task's main() method.\n        \"\"\"\n        self._transfer_coordinator = transfer_coordinator\n\n        self._main_kwargs = main_kwargs\n        if self._main_kwargs is None:\n            self._main_kwargs = {}\n\n        self._pending_main_kwargs = pending_main_kwargs\n        if pending_main_kwargs is None:\n            self._pending_main_kwargs = {}\n\n        self._done_callbacks = done_callbacks\n        if self._done_callbacks is None:\n            self._done_callbacks = []\n\n        self._is_final = is_final\n\n    def __repr__(self):\n        # These are the general main_kwarg parameters that we want to\n        # display in the repr.\n        params_to_display = [\n            'bucket',\n            'key',\n            'part_number',\n            'final_filename',\n            'transfer_future',\n            'offset',\n            'extra_args',\n        ]\n        main_kwargs_to_display = self._get_kwargs_with_params_to_include(\n            self._main_kwargs, params_to_display\n        )\n        return '{}(transfer_id={}, {})'.format(\n            self.__class__.__name__,\n            self._transfer_coordinator.transfer_id,\n            main_kwargs_to_display,\n        )\n\n    @property\n    def transfer_id(self):\n        \"\"\"The id for the transfer request that the task belongs to\"\"\"\n        return self._transfer_coordinator.transfer_id\n\n    def _get_kwargs_with_params_to_include(self, kwargs, include):\n        filtered_kwargs = {}\n        for param in include:\n            if param in kwargs:\n                filtered_kwargs[param] = kwargs[param]\n        return filtered_kwargs\n\n    def _get_kwargs_with_params_to_exclude(self, kwargs, exclude):\n        filtered_kwargs = {}\n        for param, value in kwargs.items():\n            if param in exclude:\n                continue\n            filtered_kwargs[param] = value\n        return filtered_kwargs\n\n    def __call__(self):\n        \"\"\"The callable to use when submitting a Task to an executor\"\"\"\n        try:\n            # Wait for all of futures this task depends on.\n            self._wait_on_dependent_futures()\n            # Gather up all of the main keyword arguments for main().\n            # This includes the immediately provided main_kwargs and\n            # the values for pending_main_kwargs that source from the return\n            # values from the task's dependent futures.\n            kwargs = self._get_all_main_kwargs()\n            # If the task is not done (really only if some other related\n            # task to the TransferFuture had failed) then execute the task's\n            # main() method.\n            if not self._transfer_coordinator.done():\n                return self._execute_main(kwargs)\n        except Exception as e:\n            self._log_and_set_exception(e)\n        finally:\n            # Run any done callbacks associated to the task no matter what.\n            for done_callback in self._done_callbacks:\n                done_callback()\n\n            if self._is_final:\n                # If this is the final task announce that it is done if results\n                # are waiting on its completion.\n                self._transfer_coordinator.announce_done()\n\n    def _execute_main(self, kwargs):\n        # Do not display keyword args that should not be printed, especially\n        # if they are going to make the logs hard to follow.\n        params_to_exclude = ['data']\n        kwargs_to_display = self._get_kwargs_with_params_to_exclude(\n            kwargs, params_to_exclude\n        )\n        # Log what is about to be executed.\n        logger.debug(f\"Executing task {self} with kwargs {kwargs_to_display}\")\n\n        return_value = self._main(**kwargs)\n        # If the task is the final task, then set the TransferFuture's\n        # value to the return value from main().\n        if self._is_final:\n            self._transfer_coordinator.set_result(return_value)\n        return return_value\n\n    def _log_and_set_exception(self, exception):\n        # If an exception is ever thrown than set the exception for the\n        # entire TransferFuture.\n        logger.debug(\"Exception raised.\", exc_info=True)\n        self._transfer_coordinator.set_exception(exception)\n\n    def _main(self, **kwargs):\n        \"\"\"The method that will be ran in the executor\n\n        This method must be implemented by subclasses from Task. main() can\n        be implemented with any arguments decided upon by the subclass.\n        \"\"\"\n        raise NotImplementedError('_main() must be implemented')\n\n    def _wait_on_dependent_futures(self):\n        # Gather all of the futures into that main() depends on.\n        futures_to_wait_on = []\n        for _, future in self._pending_main_kwargs.items():\n            # If the pending main keyword arg is a list then extend the list.\n            if isinstance(future, list):\n                futures_to_wait_on.extend(future)\n            # If the pending main keyword arg is a future append it to the list.\n            else:\n                futures_to_wait_on.append(future)\n        # Now wait for all of the futures to complete.\n        self._wait_until_all_complete(futures_to_wait_on)\n\n    def _wait_until_all_complete(self, futures):\n        # This is a basic implementation of the concurrent.futures.wait()\n        #\n        # concurrent.futures.wait() is not used instead because of this\n        # reported issue: https://bugs.python.org/issue20319.\n        # The issue would occasionally cause multipart uploads to hang\n        # when wait() was called. With this approach, it avoids the\n        # concurrency bug by removing any association with concurrent.futures\n        # implementation of waiters.\n        logger.debug(\n            '%s about to wait for the following futures %s', self, futures\n        )\n        for future in futures:\n            try:\n                logger.debug('%s about to wait for %s', self, future)\n                future.result()\n            except Exception:\n                # result() can also produce exceptions. We want to ignore\n                # these to be deferred to error handling down the road.\n                pass\n        logger.debug('%s done waiting for dependent futures', self)\n\n    def _get_all_main_kwargs(self):\n        # Copy over all of the kwargs that we know is available.\n        kwargs = copy.copy(self._main_kwargs)\n\n        # Iterate through the kwargs whose values are pending on the result\n        # of a future.\n        for key, pending_value in self._pending_main_kwargs.items():\n            # If the value is a list of futures, iterate though the list\n            # appending on the result from each future.\n            if isinstance(pending_value, list):\n                result = []\n                for future in pending_value:\n                    result.append(future.result())\n            # Otherwise if the pending_value is a future, just wait for it.\n            else:\n                result = pending_value.result()\n            # Add the retrieved value to the kwargs to be sent to the\n            # main() call.\n            kwargs[key] = result\n        return kwargs\n\n\nclass SubmissionTask(Task):\n    \"\"\"A base class for any submission task\n\n    Submission tasks are the top-level task used to submit a series of tasks\n    to execute a particular transfer.\n    \"\"\"\n\n    def _main(self, transfer_future, **kwargs):\n        \"\"\"\n        :type transfer_future: s3transfer.futures.TransferFuture\n        :param transfer_future: The transfer future associated with the\n            transfer request that tasks are being submitted for\n\n        :param kwargs: Any additional kwargs that you may want to pass\n            to the _submit() method\n        \"\"\"\n        try:\n            self._transfer_coordinator.set_status_to_queued()\n\n            # Before submitting any tasks, run all of the on_queued callbacks\n            on_queued_callbacks = get_callbacks(transfer_future, 'queued')\n            for on_queued_callback in on_queued_callbacks:\n                on_queued_callback()\n\n            # Once callbacks have been ran set the status to running.\n            self._transfer_coordinator.set_status_to_running()\n\n            # Call the submit method to start submitting tasks to execute the\n            # transfer.\n            self._submit(transfer_future=transfer_future, **kwargs)\n        except BaseException as e:\n            # If there was an exception raised during the submission of task\n            # there is a chance that the final task that signals if a transfer\n            # is done and too run the cleanup may never have been submitted in\n            # the first place so we need to account accordingly.\n            #\n            # Note that BaseException is caught, instead of Exception, because\n            # for some implementations of executors, specifically the serial\n            # implementation, the SubmissionTask is directly exposed to\n            # KeyboardInterupts and so needs to cleanup and signal done\n            # for those as well.\n\n            # Set the exception, that caused the process to fail.\n            self._log_and_set_exception(e)\n\n            # Wait for all possibly associated futures that may have spawned\n            # from this submission task have finished before we announce the\n            # transfer done.\n            self._wait_for_all_submitted_futures_to_complete()\n\n            # Announce the transfer as done, which will run any cleanups\n            # and done callbacks as well.\n            self._transfer_coordinator.announce_done()\n\n    def _submit(self, transfer_future, **kwargs):\n        \"\"\"The submission method to be implemented\n\n        :type transfer_future: s3transfer.futures.TransferFuture\n        :param transfer_future: The transfer future associated with the\n            transfer request that tasks are being submitted for\n\n        :param kwargs: Any additional keyword arguments you want to be passed\n            in\n        \"\"\"\n        raise NotImplementedError('_submit() must be implemented')\n\n    def _wait_for_all_submitted_futures_to_complete(self):\n        # We want to wait for all futures that were submitted to\n        # complete as we do not want the cleanup callbacks or done callbacks\n        # to be called to early. The main problem is any task that was\n        # submitted may have submitted even more during its process and so\n        # we need to account accordingly.\n\n        # First get all of the futures that were submitted up to this point.\n        submitted_futures = self._transfer_coordinator.associated_futures\n        while submitted_futures:\n            # Wait for those futures to complete.\n            self._wait_until_all_complete(submitted_futures)\n            # However, more futures may have been submitted as we waited so\n            # we need to check again for any more associated futures.\n            possibly_more_submitted_futures = (\n                self._transfer_coordinator.associated_futures\n            )\n            # If the current list of submitted futures is equal to the\n            # the list of associated futures for when after the wait completes,\n            # we can ensure no more futures were submitted in waiting on\n            # the current list of futures to complete ultimately meaning all\n            # futures that may have spawned from the original submission task\n            # have completed.\n            if submitted_futures == possibly_more_submitted_futures:\n                break\n            submitted_futures = possibly_more_submitted_futures\n\n\nclass CreateMultipartUploadTask(Task):\n    \"\"\"Task to initiate a multipart upload\"\"\"\n\n    def _main(self, client, bucket, key, extra_args):\n        \"\"\"\n        :param client: The client to use when calling CreateMultipartUpload\n        :param bucket: The name of the bucket to upload to\n        :param key: The name of the key to upload to\n        :param extra_args: A dictionary of any extra arguments that may be\n            used in the initialization.\n\n        :returns: The upload id of the multipart upload\n        \"\"\"\n        # Create the multipart upload.\n        response = client.create_multipart_upload(\n            Bucket=bucket, Key=key, **extra_args\n        )\n        upload_id = response['UploadId']\n\n        # Add a cleanup if the multipart upload fails at any point.\n        self._transfer_coordinator.add_failure_cleanup(\n            client.abort_multipart_upload,\n            Bucket=bucket,\n            Key=key,\n            UploadId=upload_id,\n        )\n        return upload_id\n\n\nclass CompleteMultipartUploadTask(Task):\n    \"\"\"Task to complete a multipart upload\"\"\"\n\n    def _main(self, client, bucket, key, upload_id, parts, extra_args):\n        \"\"\"\n        :param client: The client to use when calling CompleteMultipartUpload\n        :param bucket: The name of the bucket to upload to\n        :param key: The name of the key to upload to\n        :param upload_id: The id of the upload\n        :param parts: A list of parts to use to complete the multipart upload::\n\n            [{'Etag': etag_value, 'PartNumber': part_number}, ...]\n\n            Each element in the list consists of a return value from\n            ``UploadPartTask.main()``.\n        :param extra_args:  A dictionary of any extra arguments that may be\n            used in completing the multipart transfer.\n        \"\"\"\n        client.complete_multipart_upload(\n            Bucket=bucket,\n            Key=key,\n            UploadId=upload_id,\n            MultipartUpload={'Parts': parts},\n            **extra_args,\n        )\n", "s3transfer/bandwidth.py": "# Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport threading\nimport time\n\n\nclass RequestExceededException(Exception):\n    def __init__(self, requested_amt, retry_time):\n        \"\"\"Error when requested amount exceeds what is allowed\n\n        The request that raised this error should be retried after waiting\n        the time specified by ``retry_time``.\n\n        :type requested_amt: int\n        :param requested_amt: The originally requested byte amount\n\n        :type retry_time: float\n        :param retry_time: The length in time to wait to retry for the\n            requested amount\n        \"\"\"\n        self.requested_amt = requested_amt\n        self.retry_time = retry_time\n        msg = 'Request amount {} exceeded the amount available. Retry in {}'.format(\n            requested_amt, retry_time\n        )\n        super().__init__(msg)\n\n\nclass RequestToken:\n    \"\"\"A token to pass as an identifier when consuming from the LeakyBucket\"\"\"\n\n    pass\n\n\nclass TimeUtils:\n    def time(self):\n        \"\"\"Get the current time back\n\n        :rtype: float\n        :returns: The current time in seconds\n        \"\"\"\n        return time.time()\n\n    def sleep(self, value):\n        \"\"\"Sleep for a designated time\n\n        :type value: float\n        :param value: The time to sleep for in seconds\n        \"\"\"\n        return time.sleep(value)\n\n\nclass BandwidthLimiter:\n    def __init__(self, leaky_bucket, time_utils=None):\n        \"\"\"Limits bandwidth for shared S3 transfers\n\n        :type leaky_bucket: LeakyBucket\n        :param leaky_bucket: The leaky bucket to use limit bandwidth\n\n        :type time_utils: TimeUtils\n        :param time_utils: Time utility to use for interacting with time.\n        \"\"\"\n        self._leaky_bucket = leaky_bucket\n        self._time_utils = time_utils\n        if time_utils is None:\n            self._time_utils = TimeUtils()\n\n    def get_bandwith_limited_stream(\n        self, fileobj, transfer_coordinator, enabled=True\n    ):\n        \"\"\"Wraps a fileobj in a bandwidth limited stream wrapper\n\n        :type fileobj: file-like obj\n        :param fileobj: The file-like obj to wrap\n\n        :type transfer_coordinator: s3transfer.futures.TransferCoordinator\n        param transfer_coordinator: The coordinator for the general transfer\n            that the wrapped stream is a part of\n\n        :type enabled: boolean\n        :param enabled: Whether bandwidth limiting should be enabled to start\n        \"\"\"\n        stream = BandwidthLimitedStream(\n            fileobj, self._leaky_bucket, transfer_coordinator, self._time_utils\n        )\n        if not enabled:\n            stream.disable_bandwidth_limiting()\n        return stream\n\n\nclass BandwidthLimitedStream:\n    def __init__(\n        self,\n        fileobj,\n        leaky_bucket,\n        transfer_coordinator,\n        time_utils=None,\n        bytes_threshold=256 * 1024,\n    ):\n        \"\"\"Limits bandwidth for reads on a wrapped stream\n\n        :type fileobj: file-like object\n        :param fileobj: The file like object to wrap\n\n        :type leaky_bucket: LeakyBucket\n        :param leaky_bucket: The leaky bucket to use to throttle reads on\n            the stream\n\n        :type transfer_coordinator: s3transfer.futures.TransferCoordinator\n        param transfer_coordinator: The coordinator for the general transfer\n            that the wrapped stream is a part of\n\n        :type time_utils: TimeUtils\n        :param time_utils: The time utility to use for interacting with time\n        \"\"\"\n        self._fileobj = fileobj\n        self._leaky_bucket = leaky_bucket\n        self._transfer_coordinator = transfer_coordinator\n        self._time_utils = time_utils\n        if time_utils is None:\n            self._time_utils = TimeUtils()\n        self._bandwidth_limiting_enabled = True\n        self._request_token = RequestToken()\n        self._bytes_seen = 0\n        self._bytes_threshold = bytes_threshold\n\n    def enable_bandwidth_limiting(self):\n        \"\"\"Enable bandwidth limiting on reads to the stream\"\"\"\n        self._bandwidth_limiting_enabled = True\n\n    def disable_bandwidth_limiting(self):\n        \"\"\"Disable bandwidth limiting on reads to the stream\"\"\"\n        self._bandwidth_limiting_enabled = False\n\n    def read(self, amount):\n        \"\"\"Read a specified amount\n\n        Reads will only be throttled if bandwidth limiting is enabled.\n        \"\"\"\n        if not self._bandwidth_limiting_enabled:\n            return self._fileobj.read(amount)\n\n        # We do not want to be calling consume on every read as the read\n        # amounts can be small causing the lock of the leaky bucket to\n        # introduce noticeable overhead. So instead we keep track of\n        # how many bytes we have seen and only call consume once we pass a\n        # certain threshold.\n        self._bytes_seen += amount\n        if self._bytes_seen < self._bytes_threshold:\n            return self._fileobj.read(amount)\n\n        self._consume_through_leaky_bucket()\n        return self._fileobj.read(amount)\n\n    def _consume_through_leaky_bucket(self):\n        # NOTE: If the read amount on the stream are high, it will result\n        # in large bursty behavior as there is not an interface for partial\n        # reads. However given the read's on this abstraction are at most 256KB\n        # (via downloads), it reduces the burstiness to be small KB bursts at\n        # worst.\n        while not self._transfer_coordinator.exception:\n            try:\n                self._leaky_bucket.consume(\n                    self._bytes_seen, self._request_token\n                )\n                self._bytes_seen = 0\n                return\n            except RequestExceededException as e:\n                self._time_utils.sleep(e.retry_time)\n        else:\n            raise self._transfer_coordinator.exception\n\n    def signal_transferring(self):\n        \"\"\"Signal that data being read is being transferred to S3\"\"\"\n        self.enable_bandwidth_limiting()\n\n    def signal_not_transferring(self):\n        \"\"\"Signal that data being read is not being transferred to S3\"\"\"\n        self.disable_bandwidth_limiting()\n\n    def seek(self, where, whence=0):\n        self._fileobj.seek(where, whence)\n\n    def tell(self):\n        return self._fileobj.tell()\n\n    def close(self):\n        if self._bandwidth_limiting_enabled and self._bytes_seen:\n            # This handles the case where the file is small enough to never\n            # trigger the threshold and thus is never subjugated to the\n            # leaky bucket on read(). This specifically happens for small\n            # uploads. So instead to account for those bytes, have\n            # it go through the leaky bucket when the file gets closed.\n            self._consume_through_leaky_bucket()\n        self._fileobj.close()\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, *args, **kwargs):\n        self.close()\n\n\nclass LeakyBucket:\n    def __init__(\n        self,\n        max_rate,\n        time_utils=None,\n        rate_tracker=None,\n        consumption_scheduler=None,\n    ):\n        \"\"\"A leaky bucket abstraction to limit bandwidth consumption\n\n        :type rate: int\n        :type rate: The maximum rate to allow. This rate is in terms of\n            bytes per second.\n\n        :type time_utils: TimeUtils\n        :param time_utils: The time utility to use for interacting with time\n\n        :type rate_tracker: BandwidthRateTracker\n        :param rate_tracker: Tracks bandwidth consumption\n\n        :type consumption_scheduler: ConsumptionScheduler\n        :param consumption_scheduler: Schedules consumption retries when\n            necessary\n        \"\"\"\n        self._max_rate = float(max_rate)\n        self._time_utils = time_utils\n        if time_utils is None:\n            self._time_utils = TimeUtils()\n        self._lock = threading.Lock()\n        self._rate_tracker = rate_tracker\n        if rate_tracker is None:\n            self._rate_tracker = BandwidthRateTracker()\n        self._consumption_scheduler = consumption_scheduler\n        if consumption_scheduler is None:\n            self._consumption_scheduler = ConsumptionScheduler()\n\n    def consume(self, amt, request_token):\n        \"\"\"Consume an a requested amount\n\n        :type amt: int\n        :param amt: The amount of bytes to request to consume\n\n        :type request_token: RequestToken\n        :param request_token: The token associated to the consumption\n            request that is used to identify the request. So if a\n            RequestExceededException is raised the token should be used\n            in subsequent retry consume() request.\n\n        :raises RequestExceededException: If the consumption amount would\n            exceed the maximum allocated bandwidth\n\n        :rtype: int\n        :returns: The amount consumed\n        \"\"\"\n        with self._lock:\n            time_now = self._time_utils.time()\n            if self._consumption_scheduler.is_scheduled(request_token):\n                return self._release_requested_amt_for_scheduled_request(\n                    amt, request_token, time_now\n                )\n            elif self._projected_to_exceed_max_rate(amt, time_now):\n                self._raise_request_exceeded_exception(\n                    amt, request_token, time_now\n                )\n            else:\n                return self._release_requested_amt(amt, time_now)\n\n    def _projected_to_exceed_max_rate(self, amt, time_now):\n        projected_rate = self._rate_tracker.get_projected_rate(amt, time_now)\n        return projected_rate > self._max_rate\n\n    def _release_requested_amt_for_scheduled_request(\n        self, amt, request_token, time_now\n    ):\n        self._consumption_scheduler.process_scheduled_consumption(\n            request_token\n        )\n        return self._release_requested_amt(amt, time_now)\n\n    def _raise_request_exceeded_exception(self, amt, request_token, time_now):\n        allocated_time = amt / float(self._max_rate)\n        retry_time = self._consumption_scheduler.schedule_consumption(\n            amt, request_token, allocated_time\n        )\n        raise RequestExceededException(\n            requested_amt=amt, retry_time=retry_time\n        )\n\n    def _release_requested_amt(self, amt, time_now):\n        self._rate_tracker.record_consumption_rate(amt, time_now)\n        return amt\n\n\nclass ConsumptionScheduler:\n    def __init__(self):\n        \"\"\"Schedules when to consume a desired amount\"\"\"\n        self._tokens_to_scheduled_consumption = {}\n        self._total_wait = 0\n\n    def is_scheduled(self, token):\n        \"\"\"Indicates if a consumption request has been scheduled\n\n        :type token: RequestToken\n        :param token: The token associated to the consumption\n            request that is used to identify the request.\n        \"\"\"\n        return token in self._tokens_to_scheduled_consumption\n\n    def schedule_consumption(self, amt, token, time_to_consume):\n        \"\"\"Schedules a wait time to be able to consume an amount\n\n        :type amt: int\n        :param amt: The amount of bytes scheduled to be consumed\n\n        :type token: RequestToken\n        :param token: The token associated to the consumption\n            request that is used to identify the request.\n\n        :type time_to_consume: float\n        :param time_to_consume: The desired time it should take for that\n            specific request amount to be consumed in regardless of previously\n            scheduled consumption requests\n\n        :rtype: float\n        :returns: The amount of time to wait for the specific request before\n            actually consuming the specified amount.\n        \"\"\"\n        self._total_wait += time_to_consume\n        self._tokens_to_scheduled_consumption[token] = {\n            'wait_duration': self._total_wait,\n            'time_to_consume': time_to_consume,\n        }\n        return self._total_wait\n\n    def process_scheduled_consumption(self, token):\n        \"\"\"Processes a scheduled consumption request that has completed\n\n        :type token: RequestToken\n        :param token: The token associated to the consumption\n            request that is used to identify the request.\n        \"\"\"\n        scheduled_retry = self._tokens_to_scheduled_consumption.pop(token)\n        self._total_wait = max(\n            self._total_wait - scheduled_retry['time_to_consume'], 0\n        )\n\n\nclass BandwidthRateTracker:\n    def __init__(self, alpha=0.8):\n        \"\"\"Tracks the rate of bandwidth consumption\n\n        :type a: float\n        :param a: The constant to use in calculating the exponentional moving\n            average of the bandwidth rate. Specifically it is used in the\n            following calculation:\n\n            current_rate = alpha * new_rate + (1 - alpha) * current_rate\n\n            This value of this constant should be between 0 and 1.\n        \"\"\"\n        self._alpha = alpha\n        self._last_time = None\n        self._current_rate = None\n\n    @property\n    def current_rate(self):\n        \"\"\"The current transfer rate\n\n        :rtype: float\n        :returns: The current tracked transfer rate\n        \"\"\"\n        if self._last_time is None:\n            return 0.0\n        return self._current_rate\n\n    def get_projected_rate(self, amt, time_at_consumption):\n        \"\"\"Get the projected rate using a provided amount and time\n\n        :type amt: int\n        :param amt: The proposed amount to consume\n\n        :type time_at_consumption: float\n        :param time_at_consumption: The proposed time to consume at\n\n        :rtype: float\n        :returns: The consumption rate if that amt and time were consumed\n        \"\"\"\n        if self._last_time is None:\n            return 0.0\n        return self._calculate_exponential_moving_average_rate(\n            amt, time_at_consumption\n        )\n\n    def record_consumption_rate(self, amt, time_at_consumption):\n        \"\"\"Record the consumption rate based off amount and time point\n\n        :type amt: int\n        :param amt: The amount that got consumed\n\n        :type time_at_consumption: float\n        :param time_at_consumption: The time at which the amount was consumed\n        \"\"\"\n        if self._last_time is None:\n            self._last_time = time_at_consumption\n            self._current_rate = 0.0\n            return\n        self._current_rate = self._calculate_exponential_moving_average_rate(\n            amt, time_at_consumption\n        )\n        self._last_time = time_at_consumption\n\n    def _calculate_rate(self, amt, time_at_consumption):\n        time_delta = time_at_consumption - self._last_time\n        if time_delta <= 0:\n            # While it is really unlikely to see this in an actual transfer,\n            # we do not want to be returning back a negative rate or try to\n            # divide the amount by zero. So instead return back an infinite\n            # rate as the time delta is infinitesimally small.\n            return float('inf')\n        return amt / (time_delta)\n\n    def _calculate_exponential_moving_average_rate(\n        self, amt, time_at_consumption\n    ):\n        new_rate = self._calculate_rate(amt, time_at_consumption)\n        return self._alpha * new_rate + (1 - self._alpha) * self._current_rate\n", "s3transfer/upload.py": "# Copyright 2016 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport math\nfrom io import BytesIO\n\nfrom s3transfer.compat import readable, seekable\nfrom s3transfer.futures import IN_MEMORY_UPLOAD_TAG\nfrom s3transfer.tasks import (\n    CompleteMultipartUploadTask,\n    CreateMultipartUploadTask,\n    SubmissionTask,\n    Task,\n)\nfrom s3transfer.utils import (\n    ChunksizeAdjuster,\n    DeferredOpenFile,\n    get_callbacks,\n    get_filtered_dict,\n)\n\n\nclass AggregatedProgressCallback:\n    def __init__(self, callbacks, threshold=1024 * 256):\n        \"\"\"Aggregates progress updates for every provided progress callback\n\n        :type callbacks: A list of functions that accepts bytes_transferred\n            as a single argument\n        :param callbacks: The callbacks to invoke when threshold is reached\n\n        :type threshold: int\n        :param threshold: The progress threshold in which to take the\n            aggregated progress and invoke the progress callback with that\n            aggregated progress total\n        \"\"\"\n        self._callbacks = callbacks\n        self._threshold = threshold\n        self._bytes_seen = 0\n\n    def __call__(self, bytes_transferred):\n        self._bytes_seen += bytes_transferred\n        if self._bytes_seen >= self._threshold:\n            self._trigger_callbacks()\n\n    def flush(self):\n        \"\"\"Flushes out any progress that has not been sent to its callbacks\"\"\"\n        if self._bytes_seen > 0:\n            self._trigger_callbacks()\n\n    def _trigger_callbacks(self):\n        for callback in self._callbacks:\n            callback(bytes_transferred=self._bytes_seen)\n        self._bytes_seen = 0\n\n\nclass InterruptReader:\n    \"\"\"Wrapper that can interrupt reading using an error\n\n    It uses a transfer coordinator to propagate an error if it notices\n    that a read is being made while the file is being read from.\n\n    :type fileobj: file-like obj\n    :param fileobj: The file-like object to read from\n\n    :type transfer_coordinator: s3transfer.futures.TransferCoordinator\n    :param transfer_coordinator: The transfer coordinator to use if the\n        reader needs to be interrupted.\n    \"\"\"\n\n    def __init__(self, fileobj, transfer_coordinator):\n        self._fileobj = fileobj\n        self._transfer_coordinator = transfer_coordinator\n\n    def read(self, amount=None):\n        # If there is an exception, then raise the exception.\n        # We raise an error instead of returning no bytes because for\n        # requests where the content length and md5 was sent, it will\n        # cause md5 mismatches and retries as there was no indication that\n        # the stream being read from encountered any issues.\n        if self._transfer_coordinator.exception:\n            raise self._transfer_coordinator.exception\n        return self._fileobj.read(amount)\n\n    def seek(self, where, whence=0):\n        self._fileobj.seek(where, whence)\n\n    def tell(self):\n        return self._fileobj.tell()\n\n    def close(self):\n        self._fileobj.close()\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, *args, **kwargs):\n        self.close()\n\n\nclass UploadInputManager:\n    \"\"\"Base manager class for handling various types of files for uploads\n\n    This class is typically used for the UploadSubmissionTask class to help\n    determine the following:\n\n        * How to determine the size of the file\n        * How to determine if a multipart upload is required\n        * How to retrieve the body for a PutObject\n        * How to retrieve the bodies for a set of UploadParts\n\n    The answers/implementations differ for the various types of file inputs\n    that may be accepted. All implementations must subclass and override\n    public methods from this class.\n    \"\"\"\n\n    def __init__(self, osutil, transfer_coordinator, bandwidth_limiter=None):\n        self._osutil = osutil\n        self._transfer_coordinator = transfer_coordinator\n        self._bandwidth_limiter = bandwidth_limiter\n\n    @classmethod\n    def is_compatible(cls, upload_source):\n        \"\"\"Determines if the source for the upload is compatible with manager\n\n        :param upload_source: The source for which the upload will pull data\n            from.\n\n        :returns: True if the manager can handle the type of source specified\n            otherwise returns False.\n        \"\"\"\n        raise NotImplementedError('must implement _is_compatible()')\n\n    def stores_body_in_memory(self, operation_name):\n        \"\"\"Whether the body it provides are stored in-memory\n\n        :type operation_name: str\n        :param operation_name: The name of the client operation that the body\n            is being used for. Valid operation_names are ``put_object`` and\n            ``upload_part``.\n\n        :rtype: boolean\n        :returns: True if the body returned by the manager will be stored in\n            memory. False if the manager will not directly store the body in\n            memory.\n        \"\"\"\n        raise NotImplementedError('must implement store_body_in_memory()')\n\n    def provide_transfer_size(self, transfer_future):\n        \"\"\"Provides the transfer size of an upload\n\n        :type transfer_future: s3transfer.futures.TransferFuture\n        :param transfer_future: The future associated with upload request\n        \"\"\"\n        raise NotImplementedError('must implement provide_transfer_size()')\n\n    def requires_multipart_upload(self, transfer_future, config):\n        \"\"\"Determines where a multipart upload is required\n\n        :type transfer_future: s3transfer.futures.TransferFuture\n        :param transfer_future: The future associated with upload request\n\n        :type config: s3transfer.manager.TransferConfig\n        :param config: The config associated to the transfer manager\n\n        :rtype: boolean\n        :returns: True, if the upload should be multipart based on\n            configuration and size. False, otherwise.\n        \"\"\"\n        raise NotImplementedError('must implement requires_multipart_upload()')\n\n    def get_put_object_body(self, transfer_future):\n        \"\"\"Returns the body to use for PutObject\n\n        :type transfer_future: s3transfer.futures.TransferFuture\n        :param transfer_future: The future associated with upload request\n\n        :type config: s3transfer.manager.TransferConfig\n        :param config: The config associated to the transfer manager\n\n        :rtype: s3transfer.utils.ReadFileChunk\n        :returns: A ReadFileChunk including all progress callbacks\n            associated with the transfer future.\n        \"\"\"\n        raise NotImplementedError('must implement get_put_object_body()')\n\n    def yield_upload_part_bodies(self, transfer_future, chunksize):\n        \"\"\"Yields the part number and body to use for each UploadPart\n\n        :type transfer_future: s3transfer.futures.TransferFuture\n        :param transfer_future: The future associated with upload request\n\n        :type chunksize: int\n        :param chunksize: The chunksize to use for this upload.\n\n        :rtype: int, s3transfer.utils.ReadFileChunk\n        :returns: Yields the part number and the ReadFileChunk including all\n            progress callbacks associated with the transfer future for that\n            specific yielded part.\n        \"\"\"\n        raise NotImplementedError('must implement yield_upload_part_bodies()')\n\n    def _wrap_fileobj(self, fileobj):\n        fileobj = InterruptReader(fileobj, self._transfer_coordinator)\n        if self._bandwidth_limiter:\n            fileobj = self._bandwidth_limiter.get_bandwith_limited_stream(\n                fileobj, self._transfer_coordinator, enabled=False\n            )\n        return fileobj\n\n    def _get_progress_callbacks(self, transfer_future):\n        callbacks = get_callbacks(transfer_future, 'progress')\n        # We only want to be wrapping the callbacks if there are callbacks to\n        # invoke because we do not want to be doing any unnecessary work if\n        # there are no callbacks to invoke.\n        if callbacks:\n            return [AggregatedProgressCallback(callbacks)]\n        return []\n\n    def _get_close_callbacks(self, aggregated_progress_callbacks):\n        return [callback.flush for callback in aggregated_progress_callbacks]\n\n\nclass UploadFilenameInputManager(UploadInputManager):\n    \"\"\"Upload utility for filenames\"\"\"\n\n    @classmethod\n    def is_compatible(cls, upload_source):\n        return isinstance(upload_source, str)\n\n    def stores_body_in_memory(self, operation_name):\n        return False\n\n    def provide_transfer_size(self, transfer_future):\n        transfer_future.meta.provide_transfer_size(\n            self._osutil.get_file_size(transfer_future.meta.call_args.fileobj)\n        )\n\n    def requires_multipart_upload(self, transfer_future, config):\n        return transfer_future.meta.size >= config.multipart_threshold\n\n    def get_put_object_body(self, transfer_future):\n        # Get a file-like object for the given input\n        fileobj, full_size = self._get_put_object_fileobj_with_full_size(\n            transfer_future\n        )\n\n        # Wrap fileobj with interrupt reader that will quickly cancel\n        # uploads if needed instead of having to wait for the socket\n        # to completely read all of the data.\n        fileobj = self._wrap_fileobj(fileobj)\n\n        callbacks = self._get_progress_callbacks(transfer_future)\n        close_callbacks = self._get_close_callbacks(callbacks)\n        size = transfer_future.meta.size\n        # Return the file-like object wrapped into a ReadFileChunk to get\n        # progress.\n        return self._osutil.open_file_chunk_reader_from_fileobj(\n            fileobj=fileobj,\n            chunk_size=size,\n            full_file_size=full_size,\n            callbacks=callbacks,\n            close_callbacks=close_callbacks,\n        )\n\n    def yield_upload_part_bodies(self, transfer_future, chunksize):\n        full_file_size = transfer_future.meta.size\n        num_parts = self._get_num_parts(transfer_future, chunksize)\n        for part_number in range(1, num_parts + 1):\n            callbacks = self._get_progress_callbacks(transfer_future)\n            close_callbacks = self._get_close_callbacks(callbacks)\n            start_byte = chunksize * (part_number - 1)\n            # Get a file-like object for that part and the size of the full\n            # file size for the associated file-like object for that part.\n            fileobj, full_size = self._get_upload_part_fileobj_with_full_size(\n                transfer_future.meta.call_args.fileobj,\n                start_byte=start_byte,\n                part_size=chunksize,\n                full_file_size=full_file_size,\n            )\n\n            # Wrap fileobj with interrupt reader that will quickly cancel\n            # uploads if needed instead of having to wait for the socket\n            # to completely read all of the data.\n            fileobj = self._wrap_fileobj(fileobj)\n\n            # Wrap the file-like object into a ReadFileChunk to get progress.\n            read_file_chunk = self._osutil.open_file_chunk_reader_from_fileobj(\n                fileobj=fileobj,\n                chunk_size=chunksize,\n                full_file_size=full_size,\n                callbacks=callbacks,\n                close_callbacks=close_callbacks,\n            )\n            yield part_number, read_file_chunk\n\n    def _get_deferred_open_file(self, fileobj, start_byte):\n        fileobj = DeferredOpenFile(\n            fileobj, start_byte, open_function=self._osutil.open\n        )\n        return fileobj\n\n    def _get_put_object_fileobj_with_full_size(self, transfer_future):\n        fileobj = transfer_future.meta.call_args.fileobj\n        size = transfer_future.meta.size\n        return self._get_deferred_open_file(fileobj, 0), size\n\n    def _get_upload_part_fileobj_with_full_size(self, fileobj, **kwargs):\n        start_byte = kwargs['start_byte']\n        full_size = kwargs['full_file_size']\n        return self._get_deferred_open_file(fileobj, start_byte), full_size\n\n    def _get_num_parts(self, transfer_future, part_size):\n        return int(math.ceil(transfer_future.meta.size / float(part_size)))\n\n\nclass UploadSeekableInputManager(UploadFilenameInputManager):\n    \"\"\"Upload utility for an open file object\"\"\"\n\n    @classmethod\n    def is_compatible(cls, upload_source):\n        return readable(upload_source) and seekable(upload_source)\n\n    def stores_body_in_memory(self, operation_name):\n        if operation_name == 'put_object':\n            return False\n        else:\n            return True\n\n    def provide_transfer_size(self, transfer_future):\n        fileobj = transfer_future.meta.call_args.fileobj\n        # To determine size, first determine the starting position\n        # Seek to the end and then find the difference in the length\n        # between the end and start positions.\n        start_position = fileobj.tell()\n        fileobj.seek(0, 2)\n        end_position = fileobj.tell()\n        fileobj.seek(start_position)\n        transfer_future.meta.provide_transfer_size(\n            end_position - start_position\n        )\n\n    def _get_upload_part_fileobj_with_full_size(self, fileobj, **kwargs):\n        # Note: It is unfortunate that in order to do a multithreaded\n        # multipart upload we cannot simply copy the filelike object\n        # since there is not really a mechanism in python (i.e. os.dup\n        # points to the same OS filehandle which causes concurrency\n        # issues). So instead we need to read from the fileobj and\n        # chunk the data out to separate file-like objects in memory.\n        data = fileobj.read(kwargs['part_size'])\n        # We return the length of the data instead of the full_file_size\n        # because we partitioned the data into separate BytesIO objects\n        # meaning the BytesIO object has no knowledge of its start position\n        # relative the input source nor access to the rest of the input\n        # source. So we must treat it as its own standalone file.\n        return BytesIO(data), len(data)\n\n    def _get_put_object_fileobj_with_full_size(self, transfer_future):\n        fileobj = transfer_future.meta.call_args.fileobj\n        # The current position needs to be taken into account when retrieving\n        # the full size of the file.\n        size = fileobj.tell() + transfer_future.meta.size\n        return fileobj, size\n\n\nclass UploadNonSeekableInputManager(UploadInputManager):\n    \"\"\"Upload utility for a file-like object that cannot seek.\"\"\"\n\n    def __init__(self, osutil, transfer_coordinator, bandwidth_limiter=None):\n        super().__init__(osutil, transfer_coordinator, bandwidth_limiter)\n        self._initial_data = b''\n\n    @classmethod\n    def is_compatible(cls, upload_source):\n        return readable(upload_source)\n\n    def stores_body_in_memory(self, operation_name):\n        return True\n\n    def provide_transfer_size(self, transfer_future):\n        # No-op because there is no way to do this short of reading the entire\n        # body into memory.\n        return\n\n    def requires_multipart_upload(self, transfer_future, config):\n        # If the user has set the size, we can use that.\n        if transfer_future.meta.size is not None:\n            return transfer_future.meta.size >= config.multipart_threshold\n\n        # This is tricky to determine in this case because we can't know how\n        # large the input is. So to figure it out, we read data into memory\n        # up until the threshold and compare how much data was actually read\n        # against the threshold.\n        fileobj = transfer_future.meta.call_args.fileobj\n        threshold = config.multipart_threshold\n        self._initial_data = self._read(fileobj, threshold, False)\n        if len(self._initial_data) < threshold:\n            return False\n        else:\n            return True\n\n    def get_put_object_body(self, transfer_future):\n        callbacks = self._get_progress_callbacks(transfer_future)\n        close_callbacks = self._get_close_callbacks(callbacks)\n        fileobj = transfer_future.meta.call_args.fileobj\n\n        body = self._wrap_data(\n            self._initial_data + fileobj.read(), callbacks, close_callbacks\n        )\n\n        # Zero out the stored data so we don't have additional copies\n        # hanging around in memory.\n        self._initial_data = None\n        return body\n\n    def yield_upload_part_bodies(self, transfer_future, chunksize):\n        file_object = transfer_future.meta.call_args.fileobj\n        part_number = 0\n\n        # Continue reading parts from the file-like object until it is empty.\n        while True:\n            callbacks = self._get_progress_callbacks(transfer_future)\n            close_callbacks = self._get_close_callbacks(callbacks)\n            part_number += 1\n            part_content = self._read(file_object, chunksize)\n            if not part_content:\n                break\n            part_object = self._wrap_data(\n                part_content, callbacks, close_callbacks\n            )\n\n            # Zero out part_content to avoid hanging on to additional data.\n            part_content = None\n            yield part_number, part_object\n\n    def _read(self, fileobj, amount, truncate=True):\n        \"\"\"\n        Reads a specific amount of data from a stream and returns it. If there\n        is any data in initial_data, that will be popped out first.\n\n        :type fileobj: A file-like object that implements read\n        :param fileobj: The stream to read from.\n\n        :type amount: int\n        :param amount: The number of bytes to read from the stream.\n\n        :type truncate: bool\n        :param truncate: Whether or not to truncate initial_data after\n            reading from it.\n\n        :return: Generator which generates part bodies from the initial data.\n        \"\"\"\n        # If the the initial data is empty, we simply read from the fileobj\n        if len(self._initial_data) == 0:\n            return fileobj.read(amount)\n\n        # If the requested number of bytes is less than the amount of\n        # initial data, pull entirely from initial data.\n        if amount <= len(self._initial_data):\n            data = self._initial_data[:amount]\n            # Truncate initial data so we don't hang onto the data longer\n            # than we need.\n            if truncate:\n                self._initial_data = self._initial_data[amount:]\n            return data\n\n        # At this point there is some initial data left, but not enough to\n        # satisfy the number of bytes requested. Pull out the remaining\n        # initial data and read the rest from the fileobj.\n        amount_to_read = amount - len(self._initial_data)\n        data = self._initial_data + fileobj.read(amount_to_read)\n\n        # Zero out initial data so we don't hang onto the data any more.\n        if truncate:\n            self._initial_data = b''\n        return data\n\n    def _wrap_data(self, data, callbacks, close_callbacks):\n        \"\"\"\n        Wraps data with the interrupt reader and the file chunk reader.\n\n        :type data: bytes\n        :param data: The data to wrap.\n\n        :type callbacks: list\n        :param callbacks: The callbacks associated with the transfer future.\n\n        :type close_callbacks: list\n        :param close_callbacks: The callbacks to be called when closing the\n            wrapper for the data.\n\n        :return: Fully wrapped data.\n        \"\"\"\n        fileobj = self._wrap_fileobj(BytesIO(data))\n        return self._osutil.open_file_chunk_reader_from_fileobj(\n            fileobj=fileobj,\n            chunk_size=len(data),\n            full_file_size=len(data),\n            callbacks=callbacks,\n            close_callbacks=close_callbacks,\n        )\n\n\nclass UploadSubmissionTask(SubmissionTask):\n    \"\"\"Task for submitting tasks to execute an upload\"\"\"\n\n    UPLOAD_PART_ARGS = [\n        'ChecksumAlgorithm',\n        'SSECustomerKey',\n        'SSECustomerAlgorithm',\n        'SSECustomerKeyMD5',\n        'RequestPayer',\n        'ExpectedBucketOwner',\n    ]\n\n    COMPLETE_MULTIPART_ARGS = [\n        'SSECustomerKey',\n        'SSECustomerAlgorithm',\n        'SSECustomerKeyMD5',\n        'RequestPayer',\n        'ExpectedBucketOwner',\n    ]\n\n    def _get_upload_input_manager_cls(self, transfer_future):\n        \"\"\"Retrieves a class for managing input for an upload based on file type\n\n        :type transfer_future: s3transfer.futures.TransferFuture\n        :param transfer_future: The transfer future for the request\n\n        :rtype: class of UploadInputManager\n        :returns: The appropriate class to use for managing a specific type of\n            input for uploads.\n        \"\"\"\n        upload_manager_resolver_chain = [\n            UploadFilenameInputManager,\n            UploadSeekableInputManager,\n            UploadNonSeekableInputManager,\n        ]\n\n        fileobj = transfer_future.meta.call_args.fileobj\n        for upload_manager_cls in upload_manager_resolver_chain:\n            if upload_manager_cls.is_compatible(fileobj):\n                return upload_manager_cls\n        raise RuntimeError(\n            'Input {} of type: {} is not supported.'.format(\n                fileobj, type(fileobj)\n            )\n        )\n\n    def _submit(\n        self,\n        client,\n        config,\n        osutil,\n        request_executor,\n        transfer_future,\n        bandwidth_limiter=None,\n    ):\n        \"\"\"\n        :param client: The client associated with the transfer manager\n\n        :type config: s3transfer.manager.TransferConfig\n        :param config: The transfer config associated with the transfer\n            manager\n\n        :type osutil: s3transfer.utils.OSUtil\n        :param osutil: The os utility associated to the transfer manager\n\n        :type request_executor: s3transfer.futures.BoundedExecutor\n        :param request_executor: The request executor associated with the\n            transfer manager\n\n        :type transfer_future: s3transfer.futures.TransferFuture\n        :param transfer_future: The transfer future associated with the\n            transfer request that tasks are being submitted for\n        \"\"\"\n        upload_input_manager = self._get_upload_input_manager_cls(\n            transfer_future\n        )(osutil, self._transfer_coordinator, bandwidth_limiter)\n\n        # Determine the size if it was not provided\n        if transfer_future.meta.size is None:\n            upload_input_manager.provide_transfer_size(transfer_future)\n\n        # Do a multipart upload if needed, otherwise do a regular put object.\n        if not upload_input_manager.requires_multipart_upload(\n            transfer_future, config\n        ):\n            self._submit_upload_request(\n                client,\n                config,\n                osutil,\n                request_executor,\n                transfer_future,\n                upload_input_manager,\n            )\n        else:\n            self._submit_multipart_request(\n                client,\n                config,\n                osutil,\n                request_executor,\n                transfer_future,\n                upload_input_manager,\n            )\n\n    def _submit_upload_request(\n        self,\n        client,\n        config,\n        osutil,\n        request_executor,\n        transfer_future,\n        upload_input_manager,\n    ):\n        call_args = transfer_future.meta.call_args\n\n        # Get any tags that need to be associated to the put object task\n        put_object_tag = self._get_upload_task_tag(\n            upload_input_manager, 'put_object'\n        )\n\n        # Submit the request of a single upload.\n        self._transfer_coordinator.submit(\n            request_executor,\n            PutObjectTask(\n                transfer_coordinator=self._transfer_coordinator,\n                main_kwargs={\n                    'client': client,\n                    'fileobj': upload_input_manager.get_put_object_body(\n                        transfer_future\n                    ),\n                    'bucket': call_args.bucket,\n                    'key': call_args.key,\n                    'extra_args': call_args.extra_args,\n                },\n                is_final=True,\n            ),\n            tag=put_object_tag,\n        )\n\n    def _submit_multipart_request(\n        self,\n        client,\n        config,\n        osutil,\n        request_executor,\n        transfer_future,\n        upload_input_manager,\n    ):\n        call_args = transfer_future.meta.call_args\n\n        # Submit the request to create a multipart upload.\n        create_multipart_future = self._transfer_coordinator.submit(\n            request_executor,\n            CreateMultipartUploadTask(\n                transfer_coordinator=self._transfer_coordinator,\n                main_kwargs={\n                    'client': client,\n                    'bucket': call_args.bucket,\n                    'key': call_args.key,\n                    'extra_args': call_args.extra_args,\n                },\n            ),\n        )\n\n        # Submit requests to upload the parts of the file.\n        part_futures = []\n        extra_part_args = self._extra_upload_part_args(call_args.extra_args)\n\n        # Get any tags that need to be associated to the submitted task\n        # for upload the data\n        upload_part_tag = self._get_upload_task_tag(\n            upload_input_manager, 'upload_part'\n        )\n\n        size = transfer_future.meta.size\n        adjuster = ChunksizeAdjuster()\n        chunksize = adjuster.adjust_chunksize(config.multipart_chunksize, size)\n        part_iterator = upload_input_manager.yield_upload_part_bodies(\n            transfer_future, chunksize\n        )\n\n        for part_number, fileobj in part_iterator:\n            part_futures.append(\n                self._transfer_coordinator.submit(\n                    request_executor,\n                    UploadPartTask(\n                        transfer_coordinator=self._transfer_coordinator,\n                        main_kwargs={\n                            'client': client,\n                            'fileobj': fileobj,\n                            'bucket': call_args.bucket,\n                            'key': call_args.key,\n                            'part_number': part_number,\n                            'extra_args': extra_part_args,\n                        },\n                        pending_main_kwargs={\n                            'upload_id': create_multipart_future\n                        },\n                    ),\n                    tag=upload_part_tag,\n                )\n            )\n\n        complete_multipart_extra_args = self._extra_complete_multipart_args(\n            call_args.extra_args\n        )\n        # Submit the request to complete the multipart upload.\n        self._transfer_coordinator.submit(\n            request_executor,\n            CompleteMultipartUploadTask(\n                transfer_coordinator=self._transfer_coordinator,\n                main_kwargs={\n                    'client': client,\n                    'bucket': call_args.bucket,\n                    'key': call_args.key,\n                    'extra_args': complete_multipart_extra_args,\n                },\n                pending_main_kwargs={\n                    'upload_id': create_multipart_future,\n                    'parts': part_futures,\n                },\n                is_final=True,\n            ),\n        )\n\n    def _extra_upload_part_args(self, extra_args):\n        # Only the args in UPLOAD_PART_ARGS actually need to be passed\n        # onto the upload_part calls.\n        return get_filtered_dict(extra_args, self.UPLOAD_PART_ARGS)\n\n    def _extra_complete_multipart_args(self, extra_args):\n        return get_filtered_dict(extra_args, self.COMPLETE_MULTIPART_ARGS)\n\n    def _get_upload_task_tag(self, upload_input_manager, operation_name):\n        tag = None\n        if upload_input_manager.stores_body_in_memory(operation_name):\n            tag = IN_MEMORY_UPLOAD_TAG\n        return tag\n\n\nclass PutObjectTask(Task):\n    \"\"\"Task to do a nonmultipart upload\"\"\"\n\n    def _main(self, client, fileobj, bucket, key, extra_args):\n        \"\"\"\n        :param client: The client to use when calling PutObject\n        :param fileobj: The file to upload.\n        :param bucket: The name of the bucket to upload to\n        :param key: The name of the key to upload to\n        :param extra_args: A dictionary of any extra arguments that may be\n            used in the upload.\n        \"\"\"\n        with fileobj as body:\n            client.put_object(Bucket=bucket, Key=key, Body=body, **extra_args)\n\n\nclass UploadPartTask(Task):\n    \"\"\"Task to upload a part in a multipart upload\"\"\"\n\n    def _main(\n        self, client, fileobj, bucket, key, upload_id, part_number, extra_args\n    ):\n        \"\"\"\n        :param client: The client to use when calling PutObject\n        :param fileobj: The file to upload.\n        :param bucket: The name of the bucket to upload to\n        :param key: The name of the key to upload to\n        :param upload_id: The id of the upload\n        :param part_number: The number representing the part of the multipart\n            upload\n        :param extra_args: A dictionary of any extra arguments that may be\n            used in the upload.\n\n        :rtype: dict\n        :returns: A dictionary representing a part::\n\n            {'Etag': etag_value, 'PartNumber': part_number}\n\n            This value can be appended to a list to be used to complete\n            the multipart upload.\n        \"\"\"\n        with fileobj as body:\n            response = client.upload_part(\n                Bucket=bucket,\n                Key=key,\n                UploadId=upload_id,\n                PartNumber=part_number,\n                Body=body,\n                **extra_args,\n            )\n        etag = response['ETag']\n        part_metadata = {'ETag': etag, 'PartNumber': part_number}\n        if 'ChecksumAlgorithm' in extra_args:\n            algorithm_name = extra_args['ChecksumAlgorithm'].upper()\n            checksum_member = f'Checksum{algorithm_name}'\n            if checksum_member in response:\n                part_metadata[checksum_member] = response[checksum_member]\n        return part_metadata\n", "s3transfer/futures.py": "# Copyright 2016 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport copy\nimport logging\nimport sys\nimport threading\nfrom collections import namedtuple\nfrom concurrent import futures\n\nfrom s3transfer.compat import MAXINT\nfrom s3transfer.exceptions import CancelledError, TransferNotDoneError\nfrom s3transfer.utils import FunctionContainer, TaskSemaphore\n\nlogger = logging.getLogger(__name__)\n\n\nclass BaseTransferFuture:\n    @property\n    def meta(self):\n        \"\"\"The metadata associated to the TransferFuture\"\"\"\n        raise NotImplementedError('meta')\n\n    def done(self):\n        \"\"\"Determines if a TransferFuture has completed\n\n        :returns: True if completed. False, otherwise.\n        \"\"\"\n        raise NotImplementedError('done()')\n\n    def result(self):\n        \"\"\"Waits until TransferFuture is done and returns the result\n\n        If the TransferFuture succeeded, it will return the result. If the\n        TransferFuture failed, it will raise the exception associated to the\n        failure.\n        \"\"\"\n        raise NotImplementedError('result()')\n\n    def cancel(self):\n        \"\"\"Cancels the request associated with the TransferFuture\"\"\"\n        raise NotImplementedError('cancel()')\n\n\nclass BaseTransferMeta:\n    @property\n    def call_args(self):\n        \"\"\"The call args used in the transfer request\"\"\"\n        raise NotImplementedError('call_args')\n\n    @property\n    def transfer_id(self):\n        \"\"\"The unique id of the transfer\"\"\"\n        raise NotImplementedError('transfer_id')\n\n    @property\n    def user_context(self):\n        \"\"\"A dictionary that requesters can store data in\"\"\"\n        raise NotImplementedError('user_context')\n\n\nclass TransferFuture(BaseTransferFuture):\n    def __init__(self, meta=None, coordinator=None):\n        \"\"\"The future associated to a submitted transfer request\n\n        :type meta: TransferMeta\n        :param meta: The metadata associated to the request. This object\n            is visible to the requester.\n\n        :type coordinator: TransferCoordinator\n        :param coordinator: The coordinator associated to the request. This\n            object is not visible to the requester.\n        \"\"\"\n        self._meta = meta\n        if meta is None:\n            self._meta = TransferMeta()\n\n        self._coordinator = coordinator\n        if coordinator is None:\n            self._coordinator = TransferCoordinator()\n\n    @property\n    def meta(self):\n        return self._meta\n\n    def done(self):\n        return self._coordinator.done()\n\n    def result(self):\n        try:\n            # Usually the result() method blocks until the transfer is done,\n            # however if a KeyboardInterrupt is raised we want want to exit\n            # out of this and propagate the exception.\n            return self._coordinator.result()\n        except KeyboardInterrupt as e:\n            self.cancel()\n            raise e\n\n    def cancel(self):\n        self._coordinator.cancel()\n\n    def set_exception(self, exception):\n        \"\"\"Sets the exception on the future.\"\"\"\n        if not self.done():\n            raise TransferNotDoneError(\n                'set_exception can only be called once the transfer is '\n                'complete.'\n            )\n        self._coordinator.set_exception(exception, override=True)\n\n\nclass TransferMeta(BaseTransferMeta):\n    \"\"\"Holds metadata about the TransferFuture\"\"\"\n\n    def __init__(self, call_args=None, transfer_id=None):\n        self._call_args = call_args\n        self._transfer_id = transfer_id\n        self._size = None\n        self._user_context = {}\n\n    @property\n    def call_args(self):\n        \"\"\"The call args used in the transfer request\"\"\"\n        return self._call_args\n\n    @property\n    def transfer_id(self):\n        \"\"\"The unique id of the transfer\"\"\"\n        return self._transfer_id\n\n    @property\n    def size(self):\n        \"\"\"The size of the transfer request if known\"\"\"\n        return self._size\n\n    @property\n    def user_context(self):\n        \"\"\"A dictionary that requesters can store data in\"\"\"\n        return self._user_context\n\n    def provide_transfer_size(self, size):\n        \"\"\"A method to provide the size of a transfer request\n\n        By providing this value, the TransferManager will not try to\n        call HeadObject or use the use OS to determine the size of the\n        transfer.\n        \"\"\"\n        self._size = size\n\n\nclass TransferCoordinator:\n    \"\"\"A helper class for managing TransferFuture\"\"\"\n\n    def __init__(self, transfer_id=None):\n        self.transfer_id = transfer_id\n        self._status = 'not-started'\n        self._result = None\n        self._exception = None\n        self._associated_futures = set()\n        self._failure_cleanups = []\n        self._done_callbacks = []\n        self._done_event = threading.Event()\n        self._lock = threading.Lock()\n        self._associated_futures_lock = threading.Lock()\n        self._done_callbacks_lock = threading.Lock()\n        self._failure_cleanups_lock = threading.Lock()\n\n    def __repr__(self):\n        return '{}(transfer_id={})'.format(\n            self.__class__.__name__, self.transfer_id\n        )\n\n    @property\n    def exception(self):\n        return self._exception\n\n    @property\n    def associated_futures(self):\n        \"\"\"The list of futures associated to the inprogress TransferFuture\n\n        Once the transfer finishes this list becomes empty as the transfer\n        is considered done and there should be no running futures left.\n        \"\"\"\n        with self._associated_futures_lock:\n            # We return a copy of the list because we do not want to\n            # processing the returned list while another thread is adding\n            # more futures to the actual list.\n            return copy.copy(self._associated_futures)\n\n    @property\n    def failure_cleanups(self):\n        \"\"\"The list of callbacks to call when the TransferFuture fails\"\"\"\n        return self._failure_cleanups\n\n    @property\n    def status(self):\n        \"\"\"The status of the TransferFuture\n\n        The currently supported states are:\n            * not-started - Has yet to start. If in this state, a transfer\n              can be canceled immediately and nothing will happen.\n            * queued - SubmissionTask is about to submit tasks\n            * running - Is inprogress. In-progress as of now means that\n              the SubmissionTask that runs the transfer is being executed. So\n              there is no guarantee any transfer requests had been made to\n              S3 if this state is reached.\n            * cancelled - Was cancelled\n            * failed - An exception other than CancelledError was thrown\n            * success - No exceptions were thrown and is done.\n        \"\"\"\n        return self._status\n\n    def set_result(self, result):\n        \"\"\"Set a result for the TransferFuture\n\n        Implies that the TransferFuture succeeded. This will always set a\n        result because it is invoked on the final task where there is only\n        ever one final task and it is ran at the very end of a transfer\n        process. So if a result is being set for this final task, the transfer\n        succeeded even if something came a long and canceled the transfer\n        on the final task.\n        \"\"\"\n        with self._lock:\n            self._exception = None\n            self._result = result\n            self._status = 'success'\n\n    def set_exception(self, exception, override=False):\n        \"\"\"Set an exception for the TransferFuture\n\n        Implies the TransferFuture failed.\n\n        :param exception: The exception that cause the transfer to fail.\n        :param override: If True, override any existing state.\n        \"\"\"\n        with self._lock:\n            if not self.done() or override:\n                self._exception = exception\n                self._status = 'failed'\n\n    def result(self):\n        \"\"\"Waits until TransferFuture is done and returns the result\n\n        If the TransferFuture succeeded, it will return the result. If the\n        TransferFuture failed, it will raise the exception associated to the\n        failure.\n        \"\"\"\n        # Doing a wait() with no timeout cannot be interrupted in python2 but\n        # can be interrupted in python3 so we just wait with the largest\n        # possible value integer value, which is on the scale of billions of\n        # years...\n        self._done_event.wait(MAXINT)\n\n        # Once done waiting, raise an exception if present or return the\n        # final result.\n        if self._exception:\n            raise self._exception\n        return self._result\n\n    def cancel(self, msg='', exc_type=CancelledError):\n        \"\"\"Cancels the TransferFuture\n\n        :param msg: The message to attach to the cancellation\n        :param exc_type: The type of exception to set for the cancellation\n        \"\"\"\n        with self._lock:\n            if not self.done():\n                should_announce_done = False\n                logger.debug('%s cancel(%s) called', self, msg)\n                self._exception = exc_type(msg)\n                if self._status == 'not-started':\n                    should_announce_done = True\n                self._status = 'cancelled'\n                if should_announce_done:\n                    self.announce_done()\n\n    def set_status_to_queued(self):\n        \"\"\"Sets the TransferFutrue's status to running\"\"\"\n        self._transition_to_non_done_state('queued')\n\n    def set_status_to_running(self):\n        \"\"\"Sets the TransferFuture's status to running\"\"\"\n        self._transition_to_non_done_state('running')\n\n    def _transition_to_non_done_state(self, desired_state):\n        with self._lock:\n            if self.done():\n                raise RuntimeError(\n                    'Unable to transition from done state %s to non-done '\n                    'state %s.' % (self.status, desired_state)\n                )\n            self._status = desired_state\n\n    def submit(self, executor, task, tag=None):\n        \"\"\"Submits a task to a provided executor\n\n        :type executor: s3transfer.futures.BoundedExecutor\n        :param executor: The executor to submit the callable to\n\n        :type task: s3transfer.tasks.Task\n        :param task: The task to submit to the executor\n\n        :type tag: s3transfer.futures.TaskTag\n        :param tag: A tag to associate to the submitted task\n\n        :rtype: concurrent.futures.Future\n        :returns: A future representing the submitted task\n        \"\"\"\n        logger.debug(\n            \"Submitting task {} to executor {} for transfer request: {}.\".format(\n                task, executor, self.transfer_id\n            )\n        )\n        future = executor.submit(task, tag=tag)\n        # Add this created future to the list of associated future just\n        # in case it is needed during cleanups.\n        self.add_associated_future(future)\n        future.add_done_callback(\n            FunctionContainer(self.remove_associated_future, future)\n        )\n        return future\n\n    def done(self):\n        \"\"\"Determines if a TransferFuture has completed\n\n        :returns: False if status is equal to 'failed', 'cancelled', or\n            'success'. True, otherwise\n        \"\"\"\n        return self.status in ['failed', 'cancelled', 'success']\n\n    def add_associated_future(self, future):\n        \"\"\"Adds a future to be associated with the TransferFuture\"\"\"\n        with self._associated_futures_lock:\n            self._associated_futures.add(future)\n\n    def remove_associated_future(self, future):\n        \"\"\"Removes a future's association to the TransferFuture\"\"\"\n        with self._associated_futures_lock:\n            self._associated_futures.remove(future)\n\n    def add_done_callback(self, function, *args, **kwargs):\n        \"\"\"Add a done callback to be invoked when transfer is done\"\"\"\n        with self._done_callbacks_lock:\n            self._done_callbacks.append(\n                FunctionContainer(function, *args, **kwargs)\n            )\n\n    def add_failure_cleanup(self, function, *args, **kwargs):\n        \"\"\"Adds a callback to call upon failure\"\"\"\n        with self._failure_cleanups_lock:\n            self._failure_cleanups.append(\n                FunctionContainer(function, *args, **kwargs)\n            )\n\n    def announce_done(self):\n        \"\"\"Announce that future is done running and run associated callbacks\n\n        This will run any failure cleanups if the transfer failed if not\n        they have not been run, allows the result() to be unblocked, and will\n        run any done callbacks associated to the TransferFuture if they have\n        not already been ran.\n        \"\"\"\n        if self.status != 'success':\n            self._run_failure_cleanups()\n        self._done_event.set()\n        self._run_done_callbacks()\n\n    def _run_done_callbacks(self):\n        # Run the callbacks and remove the callbacks from the internal\n        # list so they do not get ran again if done is announced more than\n        # once.\n        with self._done_callbacks_lock:\n            self._run_callbacks(self._done_callbacks)\n            self._done_callbacks = []\n\n    def _run_failure_cleanups(self):\n        # Run the cleanup callbacks and remove the callbacks from the internal\n        # list so they do not get ran again if done is announced more than\n        # once.\n        with self._failure_cleanups_lock:\n            self._run_callbacks(self.failure_cleanups)\n            self._failure_cleanups = []\n\n    def _run_callbacks(self, callbacks):\n        for callback in callbacks:\n            self._run_callback(callback)\n\n    def _run_callback(self, callback):\n        try:\n            callback()\n        # We do not want a callback interrupting the process, especially\n        # in the failure cleanups. So log and catch, the exception.\n        except Exception:\n            logger.debug(\"Exception raised in %s.\" % callback, exc_info=True)\n\n\nclass BoundedExecutor:\n    EXECUTOR_CLS = futures.ThreadPoolExecutor\n\n    def __init__(\n        self, max_size, max_num_threads, tag_semaphores=None, executor_cls=None\n    ):\n        \"\"\"An executor implementation that has a maximum queued up tasks\n\n        The executor will block if the number of tasks that have been\n        submitted and is currently working on is past its maximum.\n\n        :params max_size: The maximum number of inflight futures. An inflight\n            future means that the task is either queued up or is currently\n            being executed. A size of None or 0 means that the executor will\n            have no bound in terms of the number of inflight futures.\n\n        :params max_num_threads: The maximum number of threads the executor\n            uses.\n\n        :type tag_semaphores: dict\n        :params tag_semaphores: A dictionary where the key is the name of the\n            tag and the value is the semaphore to use when limiting the\n            number of tasks the executor is processing at a time.\n\n        :type executor_cls: BaseExecutor\n        :param underlying_executor_cls: The executor class that\n            get bounded by this executor. If None is provided, the\n            concurrent.futures.ThreadPoolExecutor class is used.\n        \"\"\"\n        self._max_num_threads = max_num_threads\n        if executor_cls is None:\n            executor_cls = self.EXECUTOR_CLS\n        self._executor = executor_cls(max_workers=self._max_num_threads)\n        self._semaphore = TaskSemaphore(max_size)\n        self._tag_semaphores = tag_semaphores\n\n    def submit(self, task, tag=None, block=True):\n        \"\"\"Submit a task to complete\n\n        :type task: s3transfer.tasks.Task\n        :param task: The task to run __call__ on\n\n\n        :type tag: s3transfer.futures.TaskTag\n        :param tag: An optional tag to associate to the task. This\n            is used to override which semaphore to use.\n\n        :type block: boolean\n        :param block: True if to wait till it is possible to submit a task.\n            False, if not to wait and raise an error if not able to submit\n            a task.\n\n        :returns: The future associated to the submitted task\n        \"\"\"\n        semaphore = self._semaphore\n        # If a tag was provided, use the semaphore associated to that\n        # tag.\n        if tag:\n            semaphore = self._tag_semaphores[tag]\n\n        # Call acquire on the semaphore.\n        acquire_token = semaphore.acquire(task.transfer_id, block)\n        # Create a callback to invoke when task is done in order to call\n        # release on the semaphore.\n        release_callback = FunctionContainer(\n            semaphore.release, task.transfer_id, acquire_token\n        )\n        # Submit the task to the underlying executor.\n        future = ExecutorFuture(self._executor.submit(task))\n        # Add the Semaphore.release() callback to the future such that\n        # it is invoked once the future completes.\n        future.add_done_callback(release_callback)\n        return future\n\n    def shutdown(self, wait=True):\n        self._executor.shutdown(wait)\n\n\nclass ExecutorFuture:\n    def __init__(self, future):\n        \"\"\"A future returned from the executor\n\n        Currently, it is just a wrapper around a concurrent.futures.Future.\n        However, this can eventually grow to implement the needed functionality\n        of concurrent.futures.Future if we move off of the library and not\n        affect the rest of the codebase.\n\n        :type future: concurrent.futures.Future\n        :param future: The underlying future\n        \"\"\"\n        self._future = future\n\n    def result(self):\n        return self._future.result()\n\n    def add_done_callback(self, fn):\n        \"\"\"Adds a callback to be completed once future is done\n\n        :param fn: A callable that takes no arguments. Note that is different\n            than concurrent.futures.Future.add_done_callback that requires\n            a single argument for the future.\n        \"\"\"\n\n        # The done callback for concurrent.futures.Future will always pass a\n        # the future in as the only argument. So we need to create the\n        # proper signature wrapper that will invoke the callback provided.\n        def done_callback(future_passed_to_callback):\n            return fn()\n\n        self._future.add_done_callback(done_callback)\n\n    def done(self):\n        return self._future.done()\n\n\nclass BaseExecutor:\n    \"\"\"Base Executor class implementation needed to work with s3transfer\"\"\"\n\n    def __init__(self, max_workers=None):\n        pass\n\n    def submit(self, fn, *args, **kwargs):\n        raise NotImplementedError('submit()')\n\n    def shutdown(self, wait=True):\n        raise NotImplementedError('shutdown()')\n\n\nclass NonThreadedExecutor(BaseExecutor):\n    \"\"\"A drop-in replacement non-threaded version of ThreadPoolExecutor\"\"\"\n\n    def submit(self, fn, *args, **kwargs):\n        future = NonThreadedExecutorFuture()\n        try:\n            result = fn(*args, **kwargs)\n            future.set_result(result)\n        except Exception:\n            e, tb = sys.exc_info()[1:]\n            logger.debug(\n                'Setting exception for %s to %s with traceback %s',\n                future,\n                e,\n                tb,\n            )\n            future.set_exception_info(e, tb)\n        return future\n\n    def shutdown(self, wait=True):\n        pass\n\n\nclass NonThreadedExecutorFuture:\n    \"\"\"The Future returned from NonThreadedExecutor\n\n    Note that this future is **not** thread-safe as it is being used\n    from the context of a non-threaded environment.\n    \"\"\"\n\n    def __init__(self):\n        self._result = None\n        self._exception = None\n        self._traceback = None\n        self._done = False\n        self._done_callbacks = []\n\n    def set_result(self, result):\n        self._result = result\n        self._set_done()\n\n    def set_exception_info(self, exception, traceback):\n        self._exception = exception\n        self._traceback = traceback\n        self._set_done()\n\n    def result(self, timeout=None):\n        if self._exception:\n            raise self._exception.with_traceback(self._traceback)\n        return self._result\n\n    def _set_done(self):\n        self._done = True\n        for done_callback in self._done_callbacks:\n            self._invoke_done_callback(done_callback)\n        self._done_callbacks = []\n\n    def _invoke_done_callback(self, done_callback):\n        return done_callback(self)\n\n    def done(self):\n        return self._done\n\n    def add_done_callback(self, fn):\n        if self._done:\n            self._invoke_done_callback(fn)\n        else:\n            self._done_callbacks.append(fn)\n\n\nTaskTag = namedtuple('TaskTag', ['name'])\n\nIN_MEMORY_UPLOAD_TAG = TaskTag('in_memory_upload')\nIN_MEMORY_DOWNLOAD_TAG = TaskTag('in_memory_download')\n", "s3transfer/constants.py": "# Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport s3transfer\n\nKB = 1024\nMB = KB * KB\nGB = MB * KB\n\nALLOWED_DOWNLOAD_ARGS = [\n    'ChecksumMode',\n    'VersionId',\n    'SSECustomerAlgorithm',\n    'SSECustomerKey',\n    'SSECustomerKeyMD5',\n    'RequestPayer',\n    'ExpectedBucketOwner',\n]\n\nUSER_AGENT = 's3transfer/%s' % s3transfer.__version__\nPROCESS_USER_AGENT = '%s processpool' % USER_AGENT\n", "s3transfer/copies.py": "# Copyright 2016 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport copy\nimport math\n\nfrom s3transfer.tasks import (\n    CompleteMultipartUploadTask,\n    CreateMultipartUploadTask,\n    SubmissionTask,\n    Task,\n)\nfrom s3transfer.utils import (\n    ChunksizeAdjuster,\n    calculate_range_parameter,\n    get_callbacks,\n    get_filtered_dict,\n)\n\n\nclass CopySubmissionTask(SubmissionTask):\n    \"\"\"Task for submitting tasks to execute a copy\"\"\"\n\n    EXTRA_ARGS_TO_HEAD_ARGS_MAPPING = {\n        'CopySourceIfMatch': 'IfMatch',\n        'CopySourceIfModifiedSince': 'IfModifiedSince',\n        'CopySourceIfNoneMatch': 'IfNoneMatch',\n        'CopySourceIfUnmodifiedSince': 'IfUnmodifiedSince',\n        'CopySourceSSECustomerKey': 'SSECustomerKey',\n        'CopySourceSSECustomerAlgorithm': 'SSECustomerAlgorithm',\n        'CopySourceSSECustomerKeyMD5': 'SSECustomerKeyMD5',\n        'RequestPayer': 'RequestPayer',\n        'ExpectedBucketOwner': 'ExpectedBucketOwner',\n    }\n\n    UPLOAD_PART_COPY_ARGS = [\n        'CopySourceIfMatch',\n        'CopySourceIfModifiedSince',\n        'CopySourceIfNoneMatch',\n        'CopySourceIfUnmodifiedSince',\n        'CopySourceSSECustomerKey',\n        'CopySourceSSECustomerAlgorithm',\n        'CopySourceSSECustomerKeyMD5',\n        'SSECustomerKey',\n        'SSECustomerAlgorithm',\n        'SSECustomerKeyMD5',\n        'RequestPayer',\n        'ExpectedBucketOwner',\n    ]\n\n    CREATE_MULTIPART_ARGS_BLACKLIST = [\n        'CopySourceIfMatch',\n        'CopySourceIfModifiedSince',\n        'CopySourceIfNoneMatch',\n        'CopySourceIfUnmodifiedSince',\n        'CopySourceSSECustomerKey',\n        'CopySourceSSECustomerAlgorithm',\n        'CopySourceSSECustomerKeyMD5',\n        'MetadataDirective',\n        'TaggingDirective',\n    ]\n\n    COMPLETE_MULTIPART_ARGS = [\n        'SSECustomerKey',\n        'SSECustomerAlgorithm',\n        'SSECustomerKeyMD5',\n        'RequestPayer',\n        'ExpectedBucketOwner',\n    ]\n\n    def _submit(\n        self, client, config, osutil, request_executor, transfer_future\n    ):\n        \"\"\"\n        :param client: The client associated with the transfer manager\n\n        :type config: s3transfer.manager.TransferConfig\n        :param config: The transfer config associated with the transfer\n            manager\n\n        :type osutil: s3transfer.utils.OSUtil\n        :param osutil: The os utility associated to the transfer manager\n\n        :type request_executor: s3transfer.futures.BoundedExecutor\n        :param request_executor: The request executor associated with the\n            transfer manager\n\n        :type transfer_future: s3transfer.futures.TransferFuture\n        :param transfer_future: The transfer future associated with the\n            transfer request that tasks are being submitted for\n        \"\"\"\n        # Determine the size if it was not provided\n        if transfer_future.meta.size is None:\n            # If a size was not provided figure out the size for the\n            # user. Note that we will only use the client provided to\n            # the TransferManager. If the object is outside of the region\n            # of the client, they may have to provide the file size themselves\n            # with a completely new client.\n            call_args = transfer_future.meta.call_args\n            head_object_request = (\n                self._get_head_object_request_from_copy_source(\n                    call_args.copy_source\n                )\n            )\n            extra_args = call_args.extra_args\n\n            # Map any values that may be used in the head object that is\n            # used in the copy object\n            for param, value in extra_args.items():\n                if param in self.EXTRA_ARGS_TO_HEAD_ARGS_MAPPING:\n                    head_object_request[\n                        self.EXTRA_ARGS_TO_HEAD_ARGS_MAPPING[param]\n                    ] = value\n\n            response = call_args.source_client.head_object(\n                **head_object_request\n            )\n            transfer_future.meta.provide_transfer_size(\n                response['ContentLength']\n            )\n\n        # If it is greater than threshold do a multipart copy, otherwise\n        # do a regular copy object.\n        if transfer_future.meta.size < config.multipart_threshold:\n            self._submit_copy_request(\n                client, config, osutil, request_executor, transfer_future\n            )\n        else:\n            self._submit_multipart_request(\n                client, config, osutil, request_executor, transfer_future\n            )\n\n    def _submit_copy_request(\n        self, client, config, osutil, request_executor, transfer_future\n    ):\n        call_args = transfer_future.meta.call_args\n\n        # Get the needed progress callbacks for the task\n        progress_callbacks = get_callbacks(transfer_future, 'progress')\n\n        # Submit the request of a single copy.\n        self._transfer_coordinator.submit(\n            request_executor,\n            CopyObjectTask(\n                transfer_coordinator=self._transfer_coordinator,\n                main_kwargs={\n                    'client': client,\n                    'copy_source': call_args.copy_source,\n                    'bucket': call_args.bucket,\n                    'key': call_args.key,\n                    'extra_args': call_args.extra_args,\n                    'callbacks': progress_callbacks,\n                    'size': transfer_future.meta.size,\n                },\n                is_final=True,\n            ),\n        )\n\n    def _submit_multipart_request(\n        self, client, config, osutil, request_executor, transfer_future\n    ):\n        call_args = transfer_future.meta.call_args\n\n        # Submit the request to create a multipart upload and make sure it\n        # does not include any of the arguments used for copy part.\n        create_multipart_extra_args = {}\n        for param, val in call_args.extra_args.items():\n            if param not in self.CREATE_MULTIPART_ARGS_BLACKLIST:\n                create_multipart_extra_args[param] = val\n\n        create_multipart_future = self._transfer_coordinator.submit(\n            request_executor,\n            CreateMultipartUploadTask(\n                transfer_coordinator=self._transfer_coordinator,\n                main_kwargs={\n                    'client': client,\n                    'bucket': call_args.bucket,\n                    'key': call_args.key,\n                    'extra_args': create_multipart_extra_args,\n                },\n            ),\n        )\n\n        # Determine how many parts are needed based on filesize and\n        # desired chunksize.\n        part_size = config.multipart_chunksize\n        adjuster = ChunksizeAdjuster()\n        part_size = adjuster.adjust_chunksize(\n            part_size, transfer_future.meta.size\n        )\n        num_parts = int(\n            math.ceil(transfer_future.meta.size / float(part_size))\n        )\n\n        # Submit requests to upload the parts of the file.\n        part_futures = []\n        progress_callbacks = get_callbacks(transfer_future, 'progress')\n\n        for part_number in range(1, num_parts + 1):\n            extra_part_args = self._extra_upload_part_args(\n                call_args.extra_args\n            )\n            # The part number for upload part starts at 1 while the\n            # range parameter starts at zero, so just subtract 1 off of\n            # the part number\n            extra_part_args['CopySourceRange'] = calculate_range_parameter(\n                part_size,\n                part_number - 1,\n                num_parts,\n                transfer_future.meta.size,\n            )\n            # Get the size of the part copy as well for the progress\n            # callbacks.\n            size = self._get_transfer_size(\n                part_size,\n                part_number - 1,\n                num_parts,\n                transfer_future.meta.size,\n            )\n            # Get the checksum algorithm of the multipart request.\n            checksum_algorithm = call_args.extra_args.get(\"ChecksumAlgorithm\")\n            part_futures.append(\n                self._transfer_coordinator.submit(\n                    request_executor,\n                    CopyPartTask(\n                        transfer_coordinator=self._transfer_coordinator,\n                        main_kwargs={\n                            'client': client,\n                            'copy_source': call_args.copy_source,\n                            'bucket': call_args.bucket,\n                            'key': call_args.key,\n                            'part_number': part_number,\n                            'extra_args': extra_part_args,\n                            'callbacks': progress_callbacks,\n                            'size': size,\n                            'checksum_algorithm': checksum_algorithm,\n                        },\n                        pending_main_kwargs={\n                            'upload_id': create_multipart_future\n                        },\n                    ),\n                )\n            )\n\n        complete_multipart_extra_args = self._extra_complete_multipart_args(\n            call_args.extra_args\n        )\n        # Submit the request to complete the multipart upload.\n        self._transfer_coordinator.submit(\n            request_executor,\n            CompleteMultipartUploadTask(\n                transfer_coordinator=self._transfer_coordinator,\n                main_kwargs={\n                    'client': client,\n                    'bucket': call_args.bucket,\n                    'key': call_args.key,\n                    'extra_args': complete_multipart_extra_args,\n                },\n                pending_main_kwargs={\n                    'upload_id': create_multipart_future,\n                    'parts': part_futures,\n                },\n                is_final=True,\n            ),\n        )\n\n    def _get_head_object_request_from_copy_source(self, copy_source):\n        if isinstance(copy_source, dict):\n            return copy.copy(copy_source)\n        else:\n            raise TypeError(\n                'Expecting dictionary formatted: '\n                '{\"Bucket\": bucket_name, \"Key\": key} '\n                'but got %s or type %s.' % (copy_source, type(copy_source))\n            )\n\n    def _extra_upload_part_args(self, extra_args):\n        # Only the args in COPY_PART_ARGS actually need to be passed\n        # onto the upload_part_copy calls.\n        return get_filtered_dict(extra_args, self.UPLOAD_PART_COPY_ARGS)\n\n    def _extra_complete_multipart_args(self, extra_args):\n        return get_filtered_dict(extra_args, self.COMPLETE_MULTIPART_ARGS)\n\n    def _get_transfer_size(\n        self, part_size, part_index, num_parts, total_transfer_size\n    ):\n        if part_index == num_parts - 1:\n            # The last part may be different in size then the rest of the\n            # parts.\n            return total_transfer_size - (part_index * part_size)\n        return part_size\n\n\nclass CopyObjectTask(Task):\n    \"\"\"Task to do a nonmultipart copy\"\"\"\n\n    def _main(\n        self, client, copy_source, bucket, key, extra_args, callbacks, size\n    ):\n        \"\"\"\n        :param client: The client to use when calling PutObject\n        :param copy_source: The CopySource parameter to use\n        :param bucket: The name of the bucket to copy to\n        :param key: The name of the key to copy to\n        :param extra_args: A dictionary of any extra arguments that may be\n            used in the upload.\n        :param callbacks: List of callbacks to call after copy\n        :param size: The size of the transfer. This value is passed into\n            the callbacks\n\n        \"\"\"\n        client.copy_object(\n            CopySource=copy_source, Bucket=bucket, Key=key, **extra_args\n        )\n        for callback in callbacks:\n            callback(bytes_transferred=size)\n\n\nclass CopyPartTask(Task):\n    \"\"\"Task to upload a part in a multipart copy\"\"\"\n\n    def _main(\n        self,\n        client,\n        copy_source,\n        bucket,\n        key,\n        upload_id,\n        part_number,\n        extra_args,\n        callbacks,\n        size,\n        checksum_algorithm=None,\n    ):\n        \"\"\"\n        :param client: The client to use when calling PutObject\n        :param copy_source: The CopySource parameter to use\n        :param bucket: The name of the bucket to upload to\n        :param key: The name of the key to upload to\n        :param upload_id: The id of the upload\n        :param part_number: The number representing the part of the multipart\n            upload\n        :param extra_args: A dictionary of any extra arguments that may be\n            used in the upload.\n        :param callbacks: List of callbacks to call after copy part\n        :param size: The size of the transfer. This value is passed into\n            the callbacks\n        :param checksum_algorithm: The algorithm that was used to create the multipart\n            upload\n\n        :rtype: dict\n        :returns: A dictionary representing a part::\n\n            {'Etag': etag_value, 'PartNumber': part_number}\n\n            This value can be appended to a list to be used to complete\n            the multipart upload. If a checksum is in the response,\n            it will also be included.\n        \"\"\"\n        response = client.upload_part_copy(\n            CopySource=copy_source,\n            Bucket=bucket,\n            Key=key,\n            UploadId=upload_id,\n            PartNumber=part_number,\n            **extra_args,\n        )\n        for callback in callbacks:\n            callback(bytes_transferred=size)\n        etag = response['CopyPartResult']['ETag']\n        part_metadata = {'ETag': etag, 'PartNumber': part_number}\n        if checksum_algorithm:\n            checksum_member = f'Checksum{checksum_algorithm.upper()}'\n            if checksum_member in response['CopyPartResult']:\n                part_metadata[checksum_member] = response['CopyPartResult'][\n                    checksum_member\n                ]\n        return part_metadata\n", "s3transfer/__init__.py": "# Copyright 2016 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\n\"\"\"Abstractions over S3's upload/download operations.\n\nThis module provides high level abstractions for efficient\nuploads/downloads.  It handles several things for the user:\n\n* Automatically switching to multipart transfers when\n  a file is over a specific size threshold\n* Uploading/downloading a file in parallel\n* Throttling based on max bandwidth\n* Progress callbacks to monitor transfers\n* Retries.  While botocore handles retries for streaming uploads,\n  it is not possible for it to handle retries for streaming\n  downloads.  This module handles retries for both cases so\n  you don't need to implement any retry logic yourself.\n\nThis module has a reasonable set of defaults.  It also allows you\nto configure many aspects of the transfer process including:\n\n* Multipart threshold size\n* Max parallel downloads\n* Max bandwidth\n* Socket timeouts\n* Retry amounts\n\nThere is no support for s3->s3 multipart copies at this\ntime.\n\n\n.. _ref_s3transfer_usage:\n\nUsage\n=====\n\nThe simplest way to use this module is:\n\n.. code-block:: python\n\n    client = boto3.client('s3', 'us-west-2')\n    transfer = S3Transfer(client)\n    # Upload /tmp/myfile to s3://bucket/key\n    transfer.upload_file('/tmp/myfile', 'bucket', 'key')\n\n    # Download s3://bucket/key to /tmp/myfile\n    transfer.download_file('bucket', 'key', '/tmp/myfile')\n\nThe ``upload_file`` and ``download_file`` methods also accept\n``**kwargs``, which will be forwarded through to the corresponding\nclient operation.  Here are a few examples using ``upload_file``::\n\n    # Making the object public\n    transfer.upload_file('/tmp/myfile', 'bucket', 'key',\n                         extra_args={'ACL': 'public-read'})\n\n    # Setting metadata\n    transfer.upload_file('/tmp/myfile', 'bucket', 'key',\n                         extra_args={'Metadata': {'a': 'b', 'c': 'd'}})\n\n    # Setting content type\n    transfer.upload_file('/tmp/myfile.json', 'bucket', 'key',\n                         extra_args={'ContentType': \"application/json\"})\n\n\nThe ``S3Transfer`` class also supports progress callbacks so you can\nprovide transfer progress to users.  Both the ``upload_file`` and\n``download_file`` methods take an optional ``callback`` parameter.\nHere's an example of how to print a simple progress percentage\nto the user:\n\n.. code-block:: python\n\n    class ProgressPercentage(object):\n        def __init__(self, filename):\n            self._filename = filename\n            self._size = float(os.path.getsize(filename))\n            self._seen_so_far = 0\n            self._lock = threading.Lock()\n\n        def __call__(self, bytes_amount):\n            # To simplify we'll assume this is hooked up\n            # to a single filename.\n            with self._lock:\n                self._seen_so_far += bytes_amount\n                percentage = (self._seen_so_far / self._size) * 100\n                sys.stdout.write(\n                    \"\\r%s  %s / %s  (%.2f%%)\" % (self._filename, self._seen_so_far,\n                                                 self._size, percentage))\n                sys.stdout.flush()\n\n\n    transfer = S3Transfer(boto3.client('s3', 'us-west-2'))\n    # Upload /tmp/myfile to s3://bucket/key and print upload progress.\n    transfer.upload_file('/tmp/myfile', 'bucket', 'key',\n                         callback=ProgressPercentage('/tmp/myfile'))\n\n\n\nYou can also provide a TransferConfig object to the S3Transfer\nobject that gives you more fine grained control over the\ntransfer.  For example:\n\n.. code-block:: python\n\n    client = boto3.client('s3', 'us-west-2')\n    config = TransferConfig(\n        multipart_threshold=8 * 1024 * 1024,\n        max_concurrency=10,\n        num_download_attempts=10,\n    )\n    transfer = S3Transfer(client, config)\n    transfer.upload_file('/tmp/foo', 'bucket', 'key')\n\n\n\"\"\"\nimport concurrent.futures\nimport functools\nimport logging\nimport math\nimport os\nimport queue\nimport random\nimport socket\nimport string\nimport threading\n\nfrom botocore.compat import six  # noqa: F401\nfrom botocore.exceptions import IncompleteReadError, ResponseStreamingError\nfrom botocore.vendored.requests.packages.urllib3.exceptions import (\n    ReadTimeoutError,\n)\n\nimport s3transfer.compat\nfrom s3transfer.exceptions import RetriesExceededError, S3UploadFailedError\n\n__author__ = 'Amazon Web Services'\n__version__ = '0.10.2'\n\n\nclass NullHandler(logging.Handler):\n    def emit(self, record):\n        pass\n\n\nlogger = logging.getLogger(__name__)\nlogger.addHandler(NullHandler())\n\nMB = 1024 * 1024\nSHUTDOWN_SENTINEL = object()\n\n\ndef random_file_extension(num_digits=8):\n    return ''.join(random.choice(string.hexdigits) for _ in range(num_digits))\n\n\ndef disable_upload_callbacks(request, operation_name, **kwargs):\n    if operation_name in ['PutObject', 'UploadPart'] and hasattr(\n        request.body, 'disable_callback'\n    ):\n        request.body.disable_callback()\n\n\ndef enable_upload_callbacks(request, operation_name, **kwargs):\n    if operation_name in ['PutObject', 'UploadPart'] and hasattr(\n        request.body, 'enable_callback'\n    ):\n        request.body.enable_callback()\n\n\nclass QueueShutdownError(Exception):\n    pass\n\n\nclass ReadFileChunk:\n    def __init__(\n        self,\n        fileobj,\n        start_byte,\n        chunk_size,\n        full_file_size,\n        callback=None,\n        enable_callback=True,\n    ):\n        \"\"\"\n\n        Given a file object shown below:\n\n            |___________________________________________________|\n            0          |                 |                 full_file_size\n                       |----chunk_size---|\n                 start_byte\n\n        :type fileobj: file\n        :param fileobj: File like object\n\n        :type start_byte: int\n        :param start_byte: The first byte from which to start reading.\n\n        :type chunk_size: int\n        :param chunk_size: The max chunk size to read.  Trying to read\n            pass the end of the chunk size will behave like you've\n            reached the end of the file.\n\n        :type full_file_size: int\n        :param full_file_size: The entire content length associated\n            with ``fileobj``.\n\n        :type callback: function(amount_read)\n        :param callback: Called whenever data is read from this object.\n\n        \"\"\"\n        self._fileobj = fileobj\n        self._start_byte = start_byte\n        self._size = self._calculate_file_size(\n            self._fileobj,\n            requested_size=chunk_size,\n            start_byte=start_byte,\n            actual_file_size=full_file_size,\n        )\n        self._fileobj.seek(self._start_byte)\n        self._amount_read = 0\n        self._callback = callback\n        self._callback_enabled = enable_callback\n\n    @classmethod\n    def from_filename(\n        cls,\n        filename,\n        start_byte,\n        chunk_size,\n        callback=None,\n        enable_callback=True,\n    ):\n        \"\"\"Convenience factory function to create from a filename.\n\n        :type start_byte: int\n        :param start_byte: The first byte from which to start reading.\n\n        :type chunk_size: int\n        :param chunk_size: The max chunk size to read.  Trying to read\n            pass the end of the chunk size will behave like you've\n            reached the end of the file.\n\n        :type full_file_size: int\n        :param full_file_size: The entire content length associated\n            with ``fileobj``.\n\n        :type callback: function(amount_read)\n        :param callback: Called whenever data is read from this object.\n\n        :type enable_callback: bool\n        :param enable_callback: Indicate whether to invoke callback\n            during read() calls.\n\n        :rtype: ``ReadFileChunk``\n        :return: A new instance of ``ReadFileChunk``\n\n        \"\"\"\n        f = open(filename, 'rb')\n        file_size = os.fstat(f.fileno()).st_size\n        return cls(\n            f, start_byte, chunk_size, file_size, callback, enable_callback\n        )\n\n    def _calculate_file_size(\n        self, fileobj, requested_size, start_byte, actual_file_size\n    ):\n        max_chunk_size = actual_file_size - start_byte\n        return min(max_chunk_size, requested_size)\n\n    def read(self, amount=None):\n        if amount is None:\n            amount_to_read = self._size - self._amount_read\n        else:\n            amount_to_read = min(self._size - self._amount_read, amount)\n        data = self._fileobj.read(amount_to_read)\n        self._amount_read += len(data)\n        if self._callback is not None and self._callback_enabled:\n            self._callback(len(data))\n        return data\n\n    def enable_callback(self):\n        self._callback_enabled = True\n\n    def disable_callback(self):\n        self._callback_enabled = False\n\n    def seek(self, where):\n        self._fileobj.seek(self._start_byte + where)\n        if self._callback is not None and self._callback_enabled:\n            # To also rewind the callback() for an accurate progress report\n            self._callback(where - self._amount_read)\n        self._amount_read = where\n\n    def close(self):\n        self._fileobj.close()\n\n    def tell(self):\n        return self._amount_read\n\n    def __len__(self):\n        # __len__ is defined because requests will try to determine the length\n        # of the stream to set a content length.  In the normal case\n        # of the file it will just stat the file, but we need to change that\n        # behavior.  By providing a __len__, requests will use that instead\n        # of stat'ing the file.\n        return self._size\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, *args, **kwargs):\n        self.close()\n\n    def __iter__(self):\n        # This is a workaround for http://bugs.python.org/issue17575\n        # Basically httplib will try to iterate over the contents, even\n        # if its a file like object.  This wasn't noticed because we've\n        # already exhausted the stream so iterating over the file immediately\n        # stops, which is what we're simulating here.\n        return iter([])\n\n\nclass StreamReaderProgress:\n    \"\"\"Wrapper for a read only stream that adds progress callbacks.\"\"\"\n\n    def __init__(self, stream, callback=None):\n        self._stream = stream\n        self._callback = callback\n\n    def read(self, *args, **kwargs):\n        value = self._stream.read(*args, **kwargs)\n        if self._callback is not None:\n            self._callback(len(value))\n        return value\n\n\nclass OSUtils:\n    def get_file_size(self, filename):\n        return os.path.getsize(filename)\n\n    def open_file_chunk_reader(self, filename, start_byte, size, callback):\n        return ReadFileChunk.from_filename(\n            filename, start_byte, size, callback, enable_callback=False\n        )\n\n    def open(self, filename, mode):\n        return open(filename, mode)\n\n    def remove_file(self, filename):\n        \"\"\"Remove a file, noop if file does not exist.\"\"\"\n        # Unlike os.remove, if the file does not exist,\n        # then this method does nothing.\n        try:\n            os.remove(filename)\n        except OSError:\n            pass\n\n    def rename_file(self, current_filename, new_filename):\n        s3transfer.compat.rename_file(current_filename, new_filename)\n\n\nclass MultipartUploader:\n    # These are the extra_args that need to be forwarded onto\n    # subsequent upload_parts.\n    UPLOAD_PART_ARGS = [\n        'SSECustomerKey',\n        'SSECustomerAlgorithm',\n        'SSECustomerKeyMD5',\n        'RequestPayer',\n    ]\n\n    def __init__(\n        self,\n        client,\n        config,\n        osutil,\n        executor_cls=concurrent.futures.ThreadPoolExecutor,\n    ):\n        self._client = client\n        self._config = config\n        self._os = osutil\n        self._executor_cls = executor_cls\n\n    def _extra_upload_part_args(self, extra_args):\n        # Only the args in UPLOAD_PART_ARGS actually need to be passed\n        # onto the upload_part calls.\n        upload_parts_args = {}\n        for key, value in extra_args.items():\n            if key in self.UPLOAD_PART_ARGS:\n                upload_parts_args[key] = value\n        return upload_parts_args\n\n    def upload_file(self, filename, bucket, key, callback, extra_args):\n        response = self._client.create_multipart_upload(\n            Bucket=bucket, Key=key, **extra_args\n        )\n        upload_id = response['UploadId']\n        try:\n            parts = self._upload_parts(\n                upload_id, filename, bucket, key, callback, extra_args\n            )\n        except Exception as e:\n            logger.debug(\n                \"Exception raised while uploading parts, \"\n                \"aborting multipart upload.\",\n                exc_info=True,\n            )\n            self._client.abort_multipart_upload(\n                Bucket=bucket, Key=key, UploadId=upload_id\n            )\n            raise S3UploadFailedError(\n                \"Failed to upload {} to {}: {}\".format(\n                    filename, '/'.join([bucket, key]), e\n                )\n            )\n        self._client.complete_multipart_upload(\n            Bucket=bucket,\n            Key=key,\n            UploadId=upload_id,\n            MultipartUpload={'Parts': parts},\n        )\n\n    def _upload_parts(\n        self, upload_id, filename, bucket, key, callback, extra_args\n    ):\n        upload_parts_extra_args = self._extra_upload_part_args(extra_args)\n        parts = []\n        part_size = self._config.multipart_chunksize\n        num_parts = int(\n            math.ceil(self._os.get_file_size(filename) / float(part_size))\n        )\n        max_workers = self._config.max_concurrency\n        with self._executor_cls(max_workers=max_workers) as executor:\n            upload_partial = functools.partial(\n                self._upload_one_part,\n                filename,\n                bucket,\n                key,\n                upload_id,\n                part_size,\n                upload_parts_extra_args,\n                callback,\n            )\n            for part in executor.map(upload_partial, range(1, num_parts + 1)):\n                parts.append(part)\n        return parts\n\n    def _upload_one_part(\n        self,\n        filename,\n        bucket,\n        key,\n        upload_id,\n        part_size,\n        extra_args,\n        callback,\n        part_number,\n    ):\n        open_chunk_reader = self._os.open_file_chunk_reader\n        with open_chunk_reader(\n            filename, part_size * (part_number - 1), part_size, callback\n        ) as body:\n            response = self._client.upload_part(\n                Bucket=bucket,\n                Key=key,\n                UploadId=upload_id,\n                PartNumber=part_number,\n                Body=body,\n                **extra_args,\n            )\n            etag = response['ETag']\n            return {'ETag': etag, 'PartNumber': part_number}\n\n\nclass ShutdownQueue(queue.Queue):\n    \"\"\"A queue implementation that can be shutdown.\n\n    Shutting down a queue means that this class adds a\n    trigger_shutdown method that will trigger all subsequent\n    calls to put() to fail with a ``QueueShutdownError``.\n\n    It purposefully deviates from queue.Queue, and is *not* meant\n    to be a drop in replacement for ``queue.Queue``.\n\n    \"\"\"\n\n    def _init(self, maxsize):\n        self._shutdown = False\n        self._shutdown_lock = threading.Lock()\n        # queue.Queue is an old style class so we don't use super().\n        return queue.Queue._init(self, maxsize)\n\n    def trigger_shutdown(self):\n        with self._shutdown_lock:\n            self._shutdown = True\n            logger.debug(\"The IO queue is now shutdown.\")\n\n    def put(self, item):\n        # Note: this is not sufficient, it's still possible to deadlock!\n        # Need to hook into the condition vars used by this class.\n        with self._shutdown_lock:\n            if self._shutdown:\n                raise QueueShutdownError(\n                    \"Cannot put item to queue when \" \"queue has been shutdown.\"\n                )\n        return queue.Queue.put(self, item)\n\n\nclass MultipartDownloader:\n    def __init__(\n        self,\n        client,\n        config,\n        osutil,\n        executor_cls=concurrent.futures.ThreadPoolExecutor,\n    ):\n        self._client = client\n        self._config = config\n        self._os = osutil\n        self._executor_cls = executor_cls\n        self._ioqueue = ShutdownQueue(self._config.max_io_queue)\n\n    def download_file(\n        self, bucket, key, filename, object_size, extra_args, callback=None\n    ):\n        with self._executor_cls(max_workers=2) as controller:\n            # 1 thread for the future that manages the uploading of files\n            # 1 thread for the future that manages IO writes.\n            download_parts_handler = functools.partial(\n                self._download_file_as_future,\n                bucket,\n                key,\n                filename,\n                object_size,\n                callback,\n            )\n            parts_future = controller.submit(download_parts_handler)\n\n            io_writes_handler = functools.partial(\n                self._perform_io_writes, filename\n            )\n            io_future = controller.submit(io_writes_handler)\n            results = concurrent.futures.wait(\n                [parts_future, io_future],\n                return_when=concurrent.futures.FIRST_EXCEPTION,\n            )\n            self._process_future_results(results)\n\n    def _process_future_results(self, futures):\n        finished, unfinished = futures\n        for future in finished:\n            future.result()\n\n    def _download_file_as_future(\n        self, bucket, key, filename, object_size, callback\n    ):\n        part_size = self._config.multipart_chunksize\n        num_parts = int(math.ceil(object_size / float(part_size)))\n        max_workers = self._config.max_concurrency\n        download_partial = functools.partial(\n            self._download_range,\n            bucket,\n            key,\n            filename,\n            part_size,\n            num_parts,\n            callback,\n        )\n        try:\n            with self._executor_cls(max_workers=max_workers) as executor:\n                list(executor.map(download_partial, range(num_parts)))\n        finally:\n            self._ioqueue.put(SHUTDOWN_SENTINEL)\n\n    def _calculate_range_param(self, part_size, part_index, num_parts):\n        start_range = part_index * part_size\n        if part_index == num_parts - 1:\n            end_range = ''\n        else:\n            end_range = start_range + part_size - 1\n        range_param = f'bytes={start_range}-{end_range}'\n        return range_param\n\n    def _download_range(\n        self, bucket, key, filename, part_size, num_parts, callback, part_index\n    ):\n        try:\n            range_param = self._calculate_range_param(\n                part_size, part_index, num_parts\n            )\n\n            max_attempts = self._config.num_download_attempts\n            last_exception = None\n            for i in range(max_attempts):\n                try:\n                    logger.debug(\"Making get_object call.\")\n                    response = self._client.get_object(\n                        Bucket=bucket, Key=key, Range=range_param\n                    )\n                    streaming_body = StreamReaderProgress(\n                        response['Body'], callback\n                    )\n                    buffer_size = 1024 * 16\n                    current_index = part_size * part_index\n                    for chunk in iter(\n                        lambda: streaming_body.read(buffer_size), b''\n                    ):\n                        self._ioqueue.put((current_index, chunk))\n                        current_index += len(chunk)\n                    return\n                except (\n                    socket.timeout,\n                    OSError,\n                    ReadTimeoutError,\n                    IncompleteReadError,\n                    ResponseStreamingError,\n                ) as e:\n                    logger.debug(\n                        \"Retrying exception caught (%s), \"\n                        \"retrying request, (attempt %s / %s)\",\n                        e,\n                        i,\n                        max_attempts,\n                        exc_info=True,\n                    )\n                    last_exception = e\n                    continue\n            raise RetriesExceededError(last_exception)\n        finally:\n            logger.debug(\"EXITING _download_range for part: %s\", part_index)\n\n    def _perform_io_writes(self, filename):\n        with self._os.open(filename, 'wb') as f:\n            while True:\n                task = self._ioqueue.get()\n                if task is SHUTDOWN_SENTINEL:\n                    logger.debug(\n                        \"Shutdown sentinel received in IO handler, \"\n                        \"shutting down IO handler.\"\n                    )\n                    return\n                else:\n                    try:\n                        offset, data = task\n                        f.seek(offset)\n                        f.write(data)\n                    except Exception as e:\n                        logger.debug(\n                            \"Caught exception in IO thread: %s\",\n                            e,\n                            exc_info=True,\n                        )\n                        self._ioqueue.trigger_shutdown()\n                        raise\n\n\nclass TransferConfig:\n    def __init__(\n        self,\n        multipart_threshold=8 * MB,\n        max_concurrency=10,\n        multipart_chunksize=8 * MB,\n        num_download_attempts=5,\n        max_io_queue=100,\n    ):\n        self.multipart_threshold = multipart_threshold\n        self.max_concurrency = max_concurrency\n        self.multipart_chunksize = multipart_chunksize\n        self.num_download_attempts = num_download_attempts\n        self.max_io_queue = max_io_queue\n\n\nclass S3Transfer:\n    ALLOWED_DOWNLOAD_ARGS = [\n        'VersionId',\n        'SSECustomerAlgorithm',\n        'SSECustomerKey',\n        'SSECustomerKeyMD5',\n        'RequestPayer',\n    ]\n\n    ALLOWED_UPLOAD_ARGS = [\n        'ACL',\n        'CacheControl',\n        'ContentDisposition',\n        'ContentEncoding',\n        'ContentLanguage',\n        'ContentType',\n        'Expires',\n        'GrantFullControl',\n        'GrantRead',\n        'GrantReadACP',\n        'GrantWriteACL',\n        'Metadata',\n        'RequestPayer',\n        'ServerSideEncryption',\n        'StorageClass',\n        'SSECustomerAlgorithm',\n        'SSECustomerKey',\n        'SSECustomerKeyMD5',\n        'SSEKMSKeyId',\n        'SSEKMSEncryptionContext',\n        'Tagging',\n    ]\n\n    def __init__(self, client, config=None, osutil=None):\n        self._client = client\n        if config is None:\n            config = TransferConfig()\n        self._config = config\n        if osutil is None:\n            osutil = OSUtils()\n        self._osutil = osutil\n\n    def upload_file(\n        self, filename, bucket, key, callback=None, extra_args=None\n    ):\n        \"\"\"Upload a file to an S3 object.\n\n        Variants have also been injected into S3 client, Bucket and Object.\n        You don't have to use S3Transfer.upload_file() directly.\n        \"\"\"\n        if extra_args is None:\n            extra_args = {}\n        self._validate_all_known_args(extra_args, self.ALLOWED_UPLOAD_ARGS)\n        events = self._client.meta.events\n        events.register_first(\n            'request-created.s3',\n            disable_upload_callbacks,\n            unique_id='s3upload-callback-disable',\n        )\n        events.register_last(\n            'request-created.s3',\n            enable_upload_callbacks,\n            unique_id='s3upload-callback-enable',\n        )\n        if (\n            self._osutil.get_file_size(filename)\n            >= self._config.multipart_threshold\n        ):\n            self._multipart_upload(filename, bucket, key, callback, extra_args)\n        else:\n            self._put_object(filename, bucket, key, callback, extra_args)\n\n    def _put_object(self, filename, bucket, key, callback, extra_args):\n        # We're using open_file_chunk_reader so we can take advantage of the\n        # progress callback functionality.\n        open_chunk_reader = self._osutil.open_file_chunk_reader\n        with open_chunk_reader(\n            filename,\n            0,\n            self._osutil.get_file_size(filename),\n            callback=callback,\n        ) as body:\n            self._client.put_object(\n                Bucket=bucket, Key=key, Body=body, **extra_args\n            )\n\n    def download_file(\n        self, bucket, key, filename, extra_args=None, callback=None\n    ):\n        \"\"\"Download an S3 object to a file.\n\n        Variants have also been injected into S3 client, Bucket and Object.\n        You don't have to use S3Transfer.download_file() directly.\n        \"\"\"\n        # This method will issue a ``head_object`` request to determine\n        # the size of the S3 object.  This is used to determine if the\n        # object is downloaded in parallel.\n        if extra_args is None:\n            extra_args = {}\n        self._validate_all_known_args(extra_args, self.ALLOWED_DOWNLOAD_ARGS)\n        object_size = self._object_size(bucket, key, extra_args)\n        temp_filename = filename + os.extsep + random_file_extension()\n        try:\n            self._download_file(\n                bucket, key, temp_filename, object_size, extra_args, callback\n            )\n        except Exception:\n            logger.debug(\n                \"Exception caught in download_file, removing partial \"\n                \"file: %s\",\n                temp_filename,\n                exc_info=True,\n            )\n            self._osutil.remove_file(temp_filename)\n            raise\n        else:\n            self._osutil.rename_file(temp_filename, filename)\n\n    def _download_file(\n        self, bucket, key, filename, object_size, extra_args, callback\n    ):\n        if object_size >= self._config.multipart_threshold:\n            self._ranged_download(\n                bucket, key, filename, object_size, extra_args, callback\n            )\n        else:\n            self._get_object(bucket, key, filename, extra_args, callback)\n\n    def _validate_all_known_args(self, actual, allowed):\n        for kwarg in actual:\n            if kwarg not in allowed:\n                raise ValueError(\n                    \"Invalid extra_args key '%s', \"\n                    \"must be one of: %s\" % (kwarg, ', '.join(allowed))\n                )\n\n    def _ranged_download(\n        self, bucket, key, filename, object_size, extra_args, callback\n    ):\n        downloader = MultipartDownloader(\n            self._client, self._config, self._osutil\n        )\n        downloader.download_file(\n            bucket, key, filename, object_size, extra_args, callback\n        )\n\n    def _get_object(self, bucket, key, filename, extra_args, callback):\n        # precondition: num_download_attempts > 0\n        max_attempts = self._config.num_download_attempts\n        last_exception = None\n        for i in range(max_attempts):\n            try:\n                return self._do_get_object(\n                    bucket, key, filename, extra_args, callback\n                )\n            except (\n                socket.timeout,\n                OSError,\n                ReadTimeoutError,\n                IncompleteReadError,\n                ResponseStreamingError,\n            ) as e:\n                # TODO: we need a way to reset the callback if the\n                # download failed.\n                logger.debug(\n                    \"Retrying exception caught (%s), \"\n                    \"retrying request, (attempt %s / %s)\",\n                    e,\n                    i,\n                    max_attempts,\n                    exc_info=True,\n                )\n                last_exception = e\n                continue\n        raise RetriesExceededError(last_exception)\n\n    def _do_get_object(self, bucket, key, filename, extra_args, callback):\n        response = self._client.get_object(\n            Bucket=bucket, Key=key, **extra_args\n        )\n        streaming_body = StreamReaderProgress(response['Body'], callback)\n        with self._osutil.open(filename, 'wb') as f:\n            for chunk in iter(lambda: streaming_body.read(8192), b''):\n                f.write(chunk)\n\n    def _object_size(self, bucket, key, extra_args):\n        return self._client.head_object(Bucket=bucket, Key=key, **extra_args)[\n            'ContentLength'\n        ]\n\n    def _multipart_upload(self, filename, bucket, key, callback, extra_args):\n        uploader = MultipartUploader(self._client, self._config, self._osutil)\n        uploader.upload_file(filename, bucket, key, callback, extra_args)\n", "s3transfer/compat.py": "# Copyright 2016 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport errno\nimport inspect\nimport os\nimport socket\nimport sys\n\nfrom botocore.compat import six\n\nif sys.platform.startswith('win'):\n    def rename_file(current_filename, new_filename):\n        try:\n            os.remove(new_filename)\n        except OSError as e:\n            if not e.errno == errno.ENOENT:\n                # We only want to a ignore trying to remove\n                # a file that does not exist.  If it fails\n                # for any other reason we should be propagating\n                # that exception.\n                raise\n        os.rename(current_filename, new_filename)\nelse:\n    rename_file = os.rename\n\n\ndef accepts_kwargs(func):\n    return inspect.getfullargspec(func)[2]\n\n\n# In python 3, socket.error is OSError, which is too general\n# for what we want (i.e FileNotFoundError is a subclass of OSError).\n# In python 3, all the socket related errors are in a newly created\n# ConnectionError.\nSOCKET_ERROR = ConnectionError\nMAXINT = None\n\n\ndef seekable(fileobj):\n    \"\"\"Backwards compat function to determine if a fileobj is seekable\n\n    :param fileobj: The file-like object to determine if seekable\n\n    :returns: True, if seekable. False, otherwise.\n    \"\"\"\n    # If the fileobj has a seekable attr, try calling the seekable()\n    # method on it.\n    if hasattr(fileobj, 'seekable'):\n        return fileobj.seekable()\n    # If there is no seekable attr, check if the object can be seeked\n    # or telled. If it can, try to seek to the current position.\n    elif hasattr(fileobj, 'seek') and hasattr(fileobj, 'tell'):\n        try:\n            fileobj.seek(0, 1)\n            return True\n        except OSError:\n            # If an io related error was thrown then it is not seekable.\n            return False\n    # Else, the fileobj is not seekable\n    return False\n\n\ndef readable(fileobj):\n    \"\"\"Determines whether or not a file-like object is readable.\n\n    :param fileobj: The file-like object to determine if readable\n\n    :returns: True, if readable. False otherwise.\n    \"\"\"\n    if hasattr(fileobj, 'readable'):\n        return fileobj.readable()\n\n    return hasattr(fileobj, 'read')\n\n\ndef fallocate(fileobj, size):\n    if hasattr(os, 'posix_fallocate'):\n        os.posix_fallocate(fileobj.fileno(), 0, size)\n    else:\n        fileobj.truncate(size)\n\n\n# Import at end of file to avoid circular dependencies\nfrom multiprocessing.managers import BaseManager  # noqa: F401,E402\n", "s3transfer/crt.py": "# Copyright 2021 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport logging\nimport threading\nfrom io import BytesIO\n\nimport awscrt.http\nimport awscrt.s3\nimport botocore.awsrequest\nimport botocore.session\nfrom awscrt.auth import (\n    AwsCredentials,\n    AwsCredentialsProvider,\n    AwsSigningAlgorithm,\n    AwsSigningConfig,\n)\nfrom awscrt.io import (\n    ClientBootstrap,\n    ClientTlsContext,\n    DefaultHostResolver,\n    EventLoopGroup,\n    TlsContextOptions,\n)\nfrom awscrt.s3 import S3Client, S3RequestTlsMode, S3RequestType\nfrom botocore import UNSIGNED\nfrom botocore.compat import urlsplit\nfrom botocore.config import Config\nfrom botocore.exceptions import NoCredentialsError\n\nfrom s3transfer.constants import MB\nfrom s3transfer.exceptions import TransferNotDoneError\nfrom s3transfer.futures import BaseTransferFuture, BaseTransferMeta\nfrom s3transfer.manager import TransferManager\nfrom s3transfer.utils import (\n    CallArgs,\n    OSUtils,\n    get_callbacks,\n    is_s3express_bucket,\n)\n\nlogger = logging.getLogger(__name__)\n\nCRT_S3_PROCESS_LOCK = None\n\n\ndef acquire_crt_s3_process_lock(name):\n    # Currently, the CRT S3 client performs best when there is only one\n    # instance of it running on a host. This lock allows an application to\n    # signal across processes whether there is another process of the same\n    # application using the CRT S3 client and prevent spawning more than one\n    # CRT S3 clients running on the system for that application.\n    #\n    # NOTE: When acquiring the CRT process lock, the lock automatically is\n    # released when the lock object is garbage collected. So, the CRT process\n    # lock is set as a global so that it is not unintentionally garbage\n    # collected/released if reference of the lock is lost.\n    global CRT_S3_PROCESS_LOCK\n    if CRT_S3_PROCESS_LOCK is None:\n        crt_lock = awscrt.s3.CrossProcessLock(name)\n        try:\n            crt_lock.acquire()\n        except RuntimeError:\n            # If there is another process that is holding the lock, the CRT\n            # returns a RuntimeError. We return None here to signal that our\n            # current process was not able to acquire the lock.\n            return None\n        CRT_S3_PROCESS_LOCK = crt_lock\n    return CRT_S3_PROCESS_LOCK\n\n\ndef create_s3_crt_client(\n    region,\n    crt_credentials_provider=None,\n    num_threads=None,\n    target_throughput=None,\n    part_size=8 * MB,\n    use_ssl=True,\n    verify=None,\n):\n    \"\"\"\n    :type region: str\n    :param region: The region used for signing\n\n    :type crt_credentials_provider:\n        Optional[awscrt.auth.AwsCredentialsProvider]\n    :param crt_credentials_provider: CRT AWS credentials provider\n        to use to sign requests. If not set, requests will not be signed.\n\n    :type num_threads: Optional[int]\n    :param num_threads: Number of worker threads generated. Default\n        is the number of processors in the machine.\n\n    :type target_throughput: Optional[int]\n    :param target_throughput: Throughput target in bytes per second.\n        By default, CRT will automatically attempt to choose a target\n        throughput that matches the system's maximum network throughput.\n        Currently, if CRT is unable to determine the maximum network\n        throughput, a fallback target throughput of ``1_250_000_000`` bytes\n        per second (which translates to 10 gigabits per second, or 1.16\n        gibibytes per second) is used. To set a specific target\n        throughput, set a value for this parameter.\n\n    :type part_size: Optional[int]\n    :param part_size: Size, in Bytes, of parts that files will be downloaded\n        or uploaded in.\n\n    :type use_ssl: boolean\n    :param use_ssl: Whether or not to use SSL.  By default, SSL is used.\n        Note that not all services support non-ssl connections.\n\n    :type verify: Optional[boolean/string]\n    :param verify: Whether or not to verify SSL certificates.\n        By default SSL certificates are verified.  You can provide the\n        following values:\n\n        * False - do not validate SSL certificates.  SSL will still be\n            used (unless use_ssl is False), but SSL certificates\n            will not be verified.\n        * path/to/cert/bundle.pem - A filename of the CA cert bundle to\n            use. Specify this argument if you want to use a custom CA cert\n            bundle instead of the default one on your system.\n    \"\"\"\n    event_loop_group = EventLoopGroup(num_threads)\n    host_resolver = DefaultHostResolver(event_loop_group)\n    bootstrap = ClientBootstrap(event_loop_group, host_resolver)\n    tls_connection_options = None\n\n    tls_mode = (\n        S3RequestTlsMode.ENABLED if use_ssl else S3RequestTlsMode.DISABLED\n    )\n    if verify is not None:\n        tls_ctx_options = TlsContextOptions()\n        if verify:\n            tls_ctx_options.override_default_trust_store_from_path(\n                ca_filepath=verify\n            )\n        else:\n            tls_ctx_options.verify_peer = False\n        client_tls_option = ClientTlsContext(tls_ctx_options)\n        tls_connection_options = client_tls_option.new_connection_options()\n    target_gbps = _get_crt_throughput_target_gbps(\n        provided_throughput_target_bytes=target_throughput\n    )\n    return S3Client(\n        bootstrap=bootstrap,\n        region=region,\n        credential_provider=crt_credentials_provider,\n        part_size=part_size,\n        tls_mode=tls_mode,\n        tls_connection_options=tls_connection_options,\n        throughput_target_gbps=target_gbps,\n        enable_s3express=True,\n    )\n\n\ndef _get_crt_throughput_target_gbps(provided_throughput_target_bytes=None):\n    if provided_throughput_target_bytes is None:\n        target_gbps = awscrt.s3.get_recommended_throughput_target_gbps()\n        logger.debug(\n            'Recommended CRT throughput target in gbps: %s', target_gbps\n        )\n        if target_gbps is None:\n            target_gbps = 10.0\n    else:\n        # NOTE: The GB constant in s3transfer is technically a gibibyte. The\n        # GB constant is not used here because the CRT interprets gigabits\n        # for networking as a base power of 10\n        # (i.e. 1000 ** 3 instead of 1024 ** 3).\n        target_gbps = provided_throughput_target_bytes * 8 / 1_000_000_000\n    logger.debug('Using CRT throughput target in gbps: %s', target_gbps)\n    return target_gbps\n\n\nclass CRTTransferManager:\n    ALLOWED_DOWNLOAD_ARGS = TransferManager.ALLOWED_DOWNLOAD_ARGS\n    ALLOWED_UPLOAD_ARGS = TransferManager.ALLOWED_UPLOAD_ARGS\n    ALLOWED_DELETE_ARGS = TransferManager.ALLOWED_DELETE_ARGS\n\n    VALIDATE_SUPPORTED_BUCKET_VALUES = True\n\n    _UNSUPPORTED_BUCKET_PATTERNS = TransferManager._UNSUPPORTED_BUCKET_PATTERNS\n\n    def __init__(self, crt_s3_client, crt_request_serializer, osutil=None):\n        \"\"\"A transfer manager interface for Amazon S3 on CRT s3 client.\n\n        :type crt_s3_client: awscrt.s3.S3Client\n        :param crt_s3_client: The CRT s3 client, handling all the\n            HTTP requests and functions under then hood\n\n        :type crt_request_serializer: s3transfer.crt.BaseCRTRequestSerializer\n        :param crt_request_serializer: Serializer, generates unsigned crt HTTP\n            request.\n\n        :type osutil: s3transfer.utils.OSUtils\n        :param osutil: OSUtils object to use for os-related behavior when\n            using with transfer manager.\n        \"\"\"\n        if osutil is None:\n            self._osutil = OSUtils()\n        self._crt_s3_client = crt_s3_client\n        self._s3_args_creator = S3ClientArgsCreator(\n            crt_request_serializer, self._osutil\n        )\n        self._crt_exception_translator = (\n            crt_request_serializer.translate_crt_exception\n        )\n        self._future_coordinators = []\n        self._semaphore = threading.Semaphore(128)  # not configurable\n        # A counter to create unique id's for each transfer submitted.\n        self._id_counter = 0\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, exc_type, exc_value, *args):\n        cancel = False\n        if exc_type:\n            cancel = True\n        self._shutdown(cancel)\n\n    def download(\n        self, bucket, key, fileobj, extra_args=None, subscribers=None\n    ):\n        if extra_args is None:\n            extra_args = {}\n        if subscribers is None:\n            subscribers = {}\n        self._validate_all_known_args(extra_args, self.ALLOWED_DOWNLOAD_ARGS)\n        self._validate_if_bucket_supported(bucket)\n        callargs = CallArgs(\n            bucket=bucket,\n            key=key,\n            fileobj=fileobj,\n            extra_args=extra_args,\n            subscribers=subscribers,\n        )\n        return self._submit_transfer(\"get_object\", callargs)\n\n    def upload(self, fileobj, bucket, key, extra_args=None, subscribers=None):\n        if extra_args is None:\n            extra_args = {}\n        if subscribers is None:\n            subscribers = {}\n        self._validate_all_known_args(extra_args, self.ALLOWED_UPLOAD_ARGS)\n        self._validate_if_bucket_supported(bucket)\n        self._validate_checksum_algorithm_supported(extra_args)\n        callargs = CallArgs(\n            bucket=bucket,\n            key=key,\n            fileobj=fileobj,\n            extra_args=extra_args,\n            subscribers=subscribers,\n        )\n        return self._submit_transfer(\"put_object\", callargs)\n\n    def delete(self, bucket, key, extra_args=None, subscribers=None):\n        if extra_args is None:\n            extra_args = {}\n        if subscribers is None:\n            subscribers = {}\n        self._validate_all_known_args(extra_args, self.ALLOWED_DELETE_ARGS)\n        self._validate_if_bucket_supported(bucket)\n        callargs = CallArgs(\n            bucket=bucket,\n            key=key,\n            extra_args=extra_args,\n            subscribers=subscribers,\n        )\n        return self._submit_transfer(\"delete_object\", callargs)\n\n    def shutdown(self, cancel=False):\n        self._shutdown(cancel)\n\n    def _validate_if_bucket_supported(self, bucket):\n        # s3 high level operations don't support some resources\n        # (eg. S3 Object Lambda) only direct API calls are available\n        # for such resources\n        if self.VALIDATE_SUPPORTED_BUCKET_VALUES:\n            for resource, pattern in self._UNSUPPORTED_BUCKET_PATTERNS.items():\n                match = pattern.match(bucket)\n                if match:\n                    raise ValueError(\n                        f'TransferManager methods do not support {resource} '\n                        'resource. Use direct client calls instead.'\n                    )\n\n    def _validate_all_known_args(self, actual, allowed):\n        for kwarg in actual:\n            if kwarg not in allowed:\n                raise ValueError(\n                    f\"Invalid extra_args key '{kwarg}', \"\n                    f\"must be one of: {', '.join(allowed)}\"\n                )\n\n    def _validate_checksum_algorithm_supported(self, extra_args):\n        checksum_algorithm = extra_args.get('ChecksumAlgorithm')\n        if checksum_algorithm is None:\n            return\n        supported_algorithms = list(awscrt.s3.S3ChecksumAlgorithm.__members__)\n        if checksum_algorithm.upper() not in supported_algorithms:\n            raise ValueError(\n                f'ChecksumAlgorithm: {checksum_algorithm} not supported. '\n                f'Supported algorithms are: {supported_algorithms}'\n            )\n\n    def _cancel_transfers(self):\n        for coordinator in self._future_coordinators:\n            if not coordinator.done():\n                coordinator.cancel()\n\n    def _finish_transfers(self):\n        for coordinator in self._future_coordinators:\n            coordinator.result()\n\n    def _wait_transfers_done(self):\n        for coordinator in self._future_coordinators:\n            coordinator.wait_until_on_done_callbacks_complete()\n\n    def _shutdown(self, cancel=False):\n        if cancel:\n            self._cancel_transfers()\n        try:\n            self._finish_transfers()\n\n        except KeyboardInterrupt:\n            self._cancel_transfers()\n        except Exception:\n            pass\n        finally:\n            self._wait_transfers_done()\n\n    def _release_semaphore(self, **kwargs):\n        self._semaphore.release()\n\n    def _submit_transfer(self, request_type, call_args):\n        on_done_after_calls = [self._release_semaphore]\n        coordinator = CRTTransferCoordinator(\n            transfer_id=self._id_counter,\n            exception_translator=self._crt_exception_translator,\n        )\n        components = {\n            'meta': CRTTransferMeta(self._id_counter, call_args),\n            'coordinator': coordinator,\n        }\n        future = CRTTransferFuture(**components)\n        afterdone = AfterDoneHandler(coordinator)\n        on_done_after_calls.append(afterdone)\n\n        try:\n            self._semaphore.acquire()\n            on_queued = self._s3_args_creator.get_crt_callback(\n                future, 'queued'\n            )\n            on_queued()\n            crt_callargs = self._s3_args_creator.get_make_request_args(\n                request_type,\n                call_args,\n                coordinator,\n                future,\n                on_done_after_calls,\n            )\n            crt_s3_request = self._crt_s3_client.make_request(**crt_callargs)\n        except Exception as e:\n            coordinator.set_exception(e, True)\n            on_done = self._s3_args_creator.get_crt_callback(\n                future, 'done', after_subscribers=on_done_after_calls\n            )\n            on_done(error=e)\n        else:\n            coordinator.set_s3_request(crt_s3_request)\n        self._future_coordinators.append(coordinator)\n\n        self._id_counter += 1\n        return future\n\n\nclass CRTTransferMeta(BaseTransferMeta):\n    \"\"\"Holds metadata about the CRTTransferFuture\"\"\"\n\n    def __init__(self, transfer_id=None, call_args=None):\n        self._transfer_id = transfer_id\n        self._call_args = call_args\n        self._user_context = {}\n\n    @property\n    def call_args(self):\n        return self._call_args\n\n    @property\n    def transfer_id(self):\n        return self._transfer_id\n\n    @property\n    def user_context(self):\n        return self._user_context\n\n\nclass CRTTransferFuture(BaseTransferFuture):\n    def __init__(self, meta=None, coordinator=None):\n        \"\"\"The future associated to a submitted transfer request via CRT S3 client\n\n        :type meta: s3transfer.crt.CRTTransferMeta\n        :param meta: The metadata associated to the transfer future.\n\n        :type coordinator: s3transfer.crt.CRTTransferCoordinator\n        :param coordinator: The coordinator associated to the transfer future.\n        \"\"\"\n        self._meta = meta\n        if meta is None:\n            self._meta = CRTTransferMeta()\n        self._coordinator = coordinator\n\n    @property\n    def meta(self):\n        return self._meta\n\n    def done(self):\n        return self._coordinator.done()\n\n    def result(self, timeout=None):\n        self._coordinator.result(timeout)\n\n    def cancel(self):\n        self._coordinator.cancel()\n\n    def set_exception(self, exception):\n        \"\"\"Sets the exception on the future.\"\"\"\n        if not self.done():\n            raise TransferNotDoneError(\n                'set_exception can only be called once the transfer is '\n                'complete.'\n            )\n        self._coordinator.set_exception(exception, override=True)\n\n\nclass BaseCRTRequestSerializer:\n    def serialize_http_request(self, transfer_type, future):\n        \"\"\"Serialize CRT HTTP requests.\n\n        :type transfer_type: string\n        :param transfer_type: the type of transfer made,\n            e.g 'put_object', 'get_object', 'delete_object'\n\n        :type future: s3transfer.crt.CRTTransferFuture\n\n        :rtype: awscrt.http.HttpRequest\n        :returns: An unsigned HTTP request to be used for the CRT S3 client\n        \"\"\"\n        raise NotImplementedError('serialize_http_request()')\n\n    def translate_crt_exception(self, exception):\n        raise NotImplementedError('translate_crt_exception()')\n\n\nclass BotocoreCRTRequestSerializer(BaseCRTRequestSerializer):\n    def __init__(self, session, client_kwargs=None):\n        \"\"\"Serialize CRT HTTP request using botocore logic\n        It also takes into account configuration from both the session\n        and any keyword arguments that could be passed to\n        `Session.create_client()` when serializing the request.\n\n        :type session: botocore.session.Session\n\n        :type client_kwargs: Optional[Dict[str, str]])\n        :param client_kwargs: The kwargs for the botocore\n            s3 client initialization.\n        \"\"\"\n        self._session = session\n        if client_kwargs is None:\n            client_kwargs = {}\n        self._resolve_client_config(session, client_kwargs)\n        self._client = session.create_client(**client_kwargs)\n        self._client.meta.events.register(\n            'request-created.s3.*', self._capture_http_request\n        )\n        self._client.meta.events.register(\n            'after-call.s3.*', self._change_response_to_serialized_http_request\n        )\n        self._client.meta.events.register(\n            'before-send.s3.*', self._make_fake_http_response\n        )\n\n    def _resolve_client_config(self, session, client_kwargs):\n        user_provided_config = None\n        if session.get_default_client_config():\n            user_provided_config = session.get_default_client_config()\n        if 'config' in client_kwargs:\n            user_provided_config = client_kwargs['config']\n\n        client_config = Config(signature_version=UNSIGNED)\n        if user_provided_config:\n            client_config = user_provided_config.merge(client_config)\n        client_kwargs['config'] = client_config\n        client_kwargs[\"service_name\"] = \"s3\"\n\n    def _crt_request_from_aws_request(self, aws_request):\n        url_parts = urlsplit(aws_request.url)\n        crt_path = url_parts.path\n        if url_parts.query:\n            crt_path = f'{crt_path}?{url_parts.query}'\n        headers_list = []\n        for name, value in aws_request.headers.items():\n            if isinstance(value, str):\n                headers_list.append((name, value))\n            else:\n                headers_list.append((name, str(value, 'utf-8')))\n\n        crt_headers = awscrt.http.HttpHeaders(headers_list)\n\n        crt_request = awscrt.http.HttpRequest(\n            method=aws_request.method,\n            path=crt_path,\n            headers=crt_headers,\n            body_stream=aws_request.body,\n        )\n        return crt_request\n\n    def _convert_to_crt_http_request(self, botocore_http_request):\n        # Logic that does CRTUtils.crt_request_from_aws_request\n        crt_request = self._crt_request_from_aws_request(botocore_http_request)\n        if crt_request.headers.get(\"host\") is None:\n            # If host is not set, set it for the request before using CRT s3\n            url_parts = urlsplit(botocore_http_request.url)\n            crt_request.headers.set(\"host\", url_parts.netloc)\n        if crt_request.headers.get('Content-MD5') is not None:\n            crt_request.headers.remove(\"Content-MD5\")\n\n        # In general, the CRT S3 client expects a content length header. It\n        # only expects a missing content length header if the body is not\n        # seekable. However, botocore does not set the content length header\n        # for GetObject API requests and so we set the content length to zero\n        # to meet the CRT S3 client's expectation that the content length\n        # header is set even if there is no body.\n        if crt_request.headers.get('Content-Length') is None:\n            if botocore_http_request.body is None:\n                crt_request.headers.add('Content-Length', \"0\")\n\n        # Botocore sets the Transfer-Encoding header when it cannot determine\n        # the content length of the request body (e.g. it's not seekable).\n        # However, CRT does not support this header, but it supports\n        # non-seekable bodies. So we remove this header to not cause issues\n        # in the downstream CRT S3 request.\n        if crt_request.headers.get('Transfer-Encoding') is not None:\n            crt_request.headers.remove('Transfer-Encoding')\n\n        return crt_request\n\n    def _capture_http_request(self, request, **kwargs):\n        request.context['http_request'] = request\n\n    def _change_response_to_serialized_http_request(\n        self, context, parsed, **kwargs\n    ):\n        request = context['http_request']\n        parsed['HTTPRequest'] = request.prepare()\n\n    def _make_fake_http_response(self, request, **kwargs):\n        return botocore.awsrequest.AWSResponse(\n            None,\n            200,\n            {},\n            FakeRawResponse(b\"\"),\n        )\n\n    def _get_botocore_http_request(self, client_method, call_args):\n        return getattr(self._client, client_method)(\n            Bucket=call_args.bucket, Key=call_args.key, **call_args.extra_args\n        )['HTTPRequest']\n\n    def serialize_http_request(self, transfer_type, future):\n        botocore_http_request = self._get_botocore_http_request(\n            transfer_type, future.meta.call_args\n        )\n        crt_request = self._convert_to_crt_http_request(botocore_http_request)\n        return crt_request\n\n    def translate_crt_exception(self, exception):\n        if isinstance(exception, awscrt.s3.S3ResponseError):\n            return self._translate_crt_s3_response_error(exception)\n        else:\n            return None\n\n    def _translate_crt_s3_response_error(self, s3_response_error):\n        status_code = s3_response_error.status_code\n        if status_code < 301:\n            # Botocore's exception parsing only\n            # runs on status codes >= 301\n            return None\n\n        headers = {k: v for k, v in s3_response_error.headers}\n        operation_name = s3_response_error.operation_name\n        if operation_name is not None:\n            service_model = self._client.meta.service_model\n            shape = service_model.operation_model(operation_name).output_shape\n        else:\n            shape = None\n\n        response_dict = {\n            'headers': botocore.awsrequest.HeadersDict(headers),\n            'status_code': status_code,\n            'body': s3_response_error.body,\n        }\n        parsed_response = self._client._response_parser.parse(\n            response_dict, shape=shape\n        )\n\n        error_code = parsed_response.get(\"Error\", {}).get(\"Code\")\n        error_class = self._client.exceptions.from_code(error_code)\n        return error_class(parsed_response, operation_name=operation_name)\n\n\nclass FakeRawResponse(BytesIO):\n    def stream(self, amt=1024, decode_content=None):\n        while True:\n            chunk = self.read(amt)\n            if not chunk:\n                break\n            yield chunk\n\n\nclass BotocoreCRTCredentialsWrapper:\n    def __init__(self, resolved_botocore_credentials):\n        self._resolved_credentials = resolved_botocore_credentials\n\n    def __call__(self):\n        credentials = self._get_credentials().get_frozen_credentials()\n        return AwsCredentials(\n            credentials.access_key, credentials.secret_key, credentials.token\n        )\n\n    def to_crt_credentials_provider(self):\n        return AwsCredentialsProvider.new_delegate(self)\n\n    def _get_credentials(self):\n        if self._resolved_credentials is None:\n            raise NoCredentialsError()\n        return self._resolved_credentials\n\n\nclass CRTTransferCoordinator:\n    \"\"\"A helper class for managing CRTTransferFuture\"\"\"\n\n    def __init__(\n        self, transfer_id=None, s3_request=None, exception_translator=None\n    ):\n        self.transfer_id = transfer_id\n        self._exception_translator = exception_translator\n        self._s3_request = s3_request\n        self._lock = threading.Lock()\n        self._exception = None\n        self._crt_future = None\n        self._done_event = threading.Event()\n\n    @property\n    def s3_request(self):\n        return self._s3_request\n\n    def set_done_callbacks_complete(self):\n        self._done_event.set()\n\n    def wait_until_on_done_callbacks_complete(self, timeout=None):\n        self._done_event.wait(timeout)\n\n    def set_exception(self, exception, override=False):\n        with self._lock:\n            if not self.done() or override:\n                self._exception = exception\n\n    def cancel(self):\n        if self._s3_request:\n            self._s3_request.cancel()\n\n    def result(self, timeout=None):\n        if self._exception:\n            raise self._exception\n        try:\n            self._crt_future.result(timeout)\n        except KeyboardInterrupt:\n            self.cancel()\n            self._crt_future.result(timeout)\n            raise\n        except Exception as e:\n            self.handle_exception(e)\n        finally:\n            if self._s3_request:\n                self._s3_request = None\n\n    def handle_exception(self, exc):\n        translated_exc = None\n        if self._exception_translator:\n            try:\n                translated_exc = self._exception_translator(exc)\n            except Exception as e:\n                # Bail out if we hit an issue translating\n                # and raise the original error.\n                logger.debug(\"Unable to translate exception.\", exc_info=e)\n                pass\n        if translated_exc is not None:\n            raise translated_exc from exc\n        else:\n            raise exc\n\n    def done(self):\n        if self._crt_future is None:\n            return False\n        return self._crt_future.done()\n\n    def set_s3_request(self, s3_request):\n        self._s3_request = s3_request\n        self._crt_future = self._s3_request.finished_future\n\n\nclass S3ClientArgsCreator:\n    def __init__(self, crt_request_serializer, os_utils):\n        self._request_serializer = crt_request_serializer\n        self._os_utils = os_utils\n\n    def get_make_request_args(\n        self, request_type, call_args, coordinator, future, on_done_after_calls\n    ):\n        request_args_handler = getattr(\n            self,\n            f'_get_make_request_args_{request_type}',\n            self._default_get_make_request_args,\n        )\n        return request_args_handler(\n            request_type=request_type,\n            call_args=call_args,\n            coordinator=coordinator,\n            future=future,\n            on_done_before_calls=[],\n            on_done_after_calls=on_done_after_calls,\n        )\n\n    def get_crt_callback(\n        self,\n        future,\n        callback_type,\n        before_subscribers=None,\n        after_subscribers=None,\n    ):\n        def invoke_all_callbacks(*args, **kwargs):\n            callbacks_list = []\n            if before_subscribers is not None:\n                callbacks_list += before_subscribers\n            callbacks_list += get_callbacks(future, callback_type)\n            if after_subscribers is not None:\n                callbacks_list += after_subscribers\n            for callback in callbacks_list:\n                # The get_callbacks helper will set the first augment\n                # by keyword, the other augments need to be set by keyword\n                # as well\n                if callback_type == \"progress\":\n                    callback(bytes_transferred=args[0])\n                else:\n                    callback(*args, **kwargs)\n\n        return invoke_all_callbacks\n\n    def _get_make_request_args_put_object(\n        self,\n        request_type,\n        call_args,\n        coordinator,\n        future,\n        on_done_before_calls,\n        on_done_after_calls,\n    ):\n        send_filepath = None\n        if isinstance(call_args.fileobj, str):\n            send_filepath = call_args.fileobj\n            data_len = self._os_utils.get_file_size(send_filepath)\n            call_args.extra_args[\"ContentLength\"] = data_len\n        else:\n            call_args.extra_args[\"Body\"] = call_args.fileobj\n\n        checksum_algorithm = call_args.extra_args.pop(\n            'ChecksumAlgorithm', 'CRC32'\n        ).upper()\n        checksum_config = awscrt.s3.S3ChecksumConfig(\n            algorithm=awscrt.s3.S3ChecksumAlgorithm[checksum_algorithm],\n            location=awscrt.s3.S3ChecksumLocation.TRAILER,\n        )\n        # Suppress botocore's automatic MD5 calculation by setting an override\n        # value that will get deleted in the BotocoreCRTRequestSerializer.\n        # As part of the CRT S3 request, we request the CRT S3 client to\n        # automatically add trailing checksums to its uploads.\n        call_args.extra_args[\"ContentMD5\"] = \"override-to-be-removed\"\n\n        make_request_args = self._default_get_make_request_args(\n            request_type=request_type,\n            call_args=call_args,\n            coordinator=coordinator,\n            future=future,\n            on_done_before_calls=on_done_before_calls,\n            on_done_after_calls=on_done_after_calls,\n        )\n        make_request_args['send_filepath'] = send_filepath\n        make_request_args['checksum_config'] = checksum_config\n        return make_request_args\n\n    def _get_make_request_args_get_object(\n        self,\n        request_type,\n        call_args,\n        coordinator,\n        future,\n        on_done_before_calls,\n        on_done_after_calls,\n    ):\n        recv_filepath = None\n        on_body = None\n        checksum_config = awscrt.s3.S3ChecksumConfig(validate_response=True)\n        if isinstance(call_args.fileobj, str):\n            final_filepath = call_args.fileobj\n            recv_filepath = self._os_utils.get_temp_filename(final_filepath)\n            on_done_before_calls.append(\n                RenameTempFileHandler(\n                    coordinator, final_filepath, recv_filepath, self._os_utils\n                )\n            )\n        else:\n            on_body = OnBodyFileObjWriter(call_args.fileobj)\n\n        make_request_args = self._default_get_make_request_args(\n            request_type=request_type,\n            call_args=call_args,\n            coordinator=coordinator,\n            future=future,\n            on_done_before_calls=on_done_before_calls,\n            on_done_after_calls=on_done_after_calls,\n        )\n        make_request_args['recv_filepath'] = recv_filepath\n        make_request_args['on_body'] = on_body\n        make_request_args['checksum_config'] = checksum_config\n        return make_request_args\n\n    def _default_get_make_request_args(\n        self,\n        request_type,\n        call_args,\n        coordinator,\n        future,\n        on_done_before_calls,\n        on_done_after_calls,\n    ):\n        make_request_args = {\n            'request': self._request_serializer.serialize_http_request(\n                request_type, future\n            ),\n            'type': getattr(\n                S3RequestType, request_type.upper(), S3RequestType.DEFAULT\n            ),\n            'on_done': self.get_crt_callback(\n                future, 'done', on_done_before_calls, on_done_after_calls\n            ),\n            'on_progress': self.get_crt_callback(future, 'progress'),\n        }\n\n        # For DEFAULT requests, CRT requires the official S3 operation name.\n        # So transform string like \"delete_object\" -> \"DeleteObject\".\n        if make_request_args['type'] == S3RequestType.DEFAULT:\n            make_request_args['operation_name'] = ''.join(\n                x.title() for x in request_type.split('_')\n            )\n\n        if is_s3express_bucket(call_args.bucket):\n            make_request_args['signing_config'] = AwsSigningConfig(\n                algorithm=AwsSigningAlgorithm.V4_S3EXPRESS\n            )\n        return make_request_args\n\n\nclass RenameTempFileHandler:\n    def __init__(self, coordinator, final_filename, temp_filename, osutil):\n        self._coordinator = coordinator\n        self._final_filename = final_filename\n        self._temp_filename = temp_filename\n        self._osutil = osutil\n\n    def __call__(self, **kwargs):\n        error = kwargs['error']\n        if error:\n            self._osutil.remove_file(self._temp_filename)\n        else:\n            try:\n                self._osutil.rename_file(\n                    self._temp_filename, self._final_filename\n                )\n            except Exception as e:\n                self._osutil.remove_file(self._temp_filename)\n                # the CRT future has done already at this point\n                self._coordinator.set_exception(e)\n\n\nclass AfterDoneHandler:\n    def __init__(self, coordinator):\n        self._coordinator = coordinator\n\n    def __call__(self, **kwargs):\n        self._coordinator.set_done_callbacks_complete()\n\n\nclass OnBodyFileObjWriter:\n    def __init__(self, fileobj):\n        self._fileobj = fileobj\n\n    def __call__(self, chunk, **kwargs):\n        self._fileobj.write(chunk)\n", "s3transfer/download.py": "# Copyright 2016 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"). You\n# may not use this file except in compliance with the License. A copy of\n# the License is located at\n#\n# http://aws.amazon.com/apache2.0/\n#\n# or in the \"license\" file accompanying this file. This file is\n# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\n# ANY KIND, either express or implied. See the License for the specific\n# language governing permissions and limitations under the License.\nimport heapq\nimport logging\nimport threading\n\nfrom s3transfer.compat import seekable\nfrom s3transfer.exceptions import RetriesExceededError\nfrom s3transfer.futures import IN_MEMORY_DOWNLOAD_TAG\nfrom s3transfer.tasks import SubmissionTask, Task\nfrom s3transfer.utils import (\n    S3_RETRYABLE_DOWNLOAD_ERRORS,\n    CountCallbackInvoker,\n    DeferredOpenFile,\n    FunctionContainer,\n    StreamReaderProgress,\n    calculate_num_parts,\n    calculate_range_parameter,\n    get_callbacks,\n    invoke_progress_callbacks,\n)\n\nlogger = logging.getLogger(__name__)\n\n\nclass DownloadOutputManager:\n    \"\"\"Base manager class for handling various types of files for downloads\n\n    This class is typically used for the DownloadSubmissionTask class to help\n    determine the following:\n\n        * Provides the fileobj to write to downloads to\n        * Get a task to complete once everything downloaded has been written\n\n    The answers/implementations differ for the various types of file outputs\n    that may be accepted. All implementations must subclass and override\n    public methods from this class.\n    \"\"\"\n\n    def __init__(self, osutil, transfer_coordinator, io_executor):\n        self._osutil = osutil\n        self._transfer_coordinator = transfer_coordinator\n        self._io_executor = io_executor\n\n    @classmethod\n    def is_compatible(cls, download_target, osutil):\n        \"\"\"Determines if the target for the download is compatible with manager\n\n        :param download_target: The target for which the upload will write\n            data to.\n\n        :param osutil: The os utility to be used for the transfer\n\n        :returns: True if the manager can handle the type of target specified\n            otherwise returns False.\n        \"\"\"\n        raise NotImplementedError('must implement is_compatible()')\n\n    def get_download_task_tag(self):\n        \"\"\"Get the tag (if any) to associate all GetObjectTasks\n\n        :rtype: s3transfer.futures.TaskTag\n        :returns: The tag to associate all GetObjectTasks with\n        \"\"\"\n        return None\n\n    def get_fileobj_for_io_writes(self, transfer_future):\n        \"\"\"Get file-like object to use for io writes in the io executor\n\n        :type transfer_future: s3transfer.futures.TransferFuture\n        :param transfer_future: The future associated with upload request\n\n        returns: A file-like object to write to\n        \"\"\"\n        raise NotImplementedError('must implement get_fileobj_for_io_writes()')\n\n    def queue_file_io_task(self, fileobj, data, offset):\n        \"\"\"Queue IO write for submission to the IO executor.\n\n        This method accepts an IO executor and information about the\n        downloaded data, and handles submitting this to the IO executor.\n\n        This method may defer submission to the IO executor if necessary.\n\n        \"\"\"\n        self._transfer_coordinator.submit(\n            self._io_executor, self.get_io_write_task(fileobj, data, offset)\n        )\n\n    def get_io_write_task(self, fileobj, data, offset):\n        \"\"\"Get an IO write task for the requested set of data\n\n        This task can be ran immediately or be submitted to the IO executor\n        for it to run.\n\n        :type fileobj: file-like object\n        :param fileobj: The file-like object to write to\n\n        :type data: bytes\n        :param data: The data to write out\n\n        :type offset: integer\n        :param offset: The offset to write the data to in the file-like object\n\n        :returns: An IO task to be used to write data to a file-like object\n        \"\"\"\n        return IOWriteTask(\n            self._transfer_coordinator,\n            main_kwargs={\n                'fileobj': fileobj,\n                'data': data,\n                'offset': offset,\n            },\n        )\n\n    def get_final_io_task(self):\n        \"\"\"Get the final io task to complete the download\n\n        This is needed because based on the architecture of the TransferManager\n        the final tasks will be sent to the IO executor, but the executor\n        needs a final task for it to signal that the transfer is done and\n        all done callbacks can be run.\n\n        :rtype: s3transfer.tasks.Task\n        :returns: A final task to completed in the io executor\n        \"\"\"\n        raise NotImplementedError('must implement get_final_io_task()')\n\n    def _get_fileobj_from_filename(self, filename):\n        f = DeferredOpenFile(\n            filename, mode='wb', open_function=self._osutil.open\n        )\n        # Make sure the file gets closed and we remove the temporary file\n        # if anything goes wrong during the process.\n        self._transfer_coordinator.add_failure_cleanup(f.close)\n        return f\n\n\nclass DownloadFilenameOutputManager(DownloadOutputManager):\n    def __init__(self, osutil, transfer_coordinator, io_executor):\n        super().__init__(osutil, transfer_coordinator, io_executor)\n        self._final_filename = None\n        self._temp_filename = None\n        self._temp_fileobj = None\n\n    @classmethod\n    def is_compatible(cls, download_target, osutil):\n        return isinstance(download_target, str)\n\n    def get_fileobj_for_io_writes(self, transfer_future):\n        fileobj = transfer_future.meta.call_args.fileobj\n        self._final_filename = fileobj\n        self._temp_filename = self._osutil.get_temp_filename(fileobj)\n        self._temp_fileobj = self._get_temp_fileobj()\n        return self._temp_fileobj\n\n    def get_final_io_task(self):\n        # A task to rename the file from the temporary file to its final\n        # location is needed. This should be the last task needed to complete\n        # the download.\n        return IORenameFileTask(\n            transfer_coordinator=self._transfer_coordinator,\n            main_kwargs={\n                'fileobj': self._temp_fileobj,\n                'final_filename': self._final_filename,\n                'osutil': self._osutil,\n            },\n            is_final=True,\n        )\n\n    def _get_temp_fileobj(self):\n        f = self._get_fileobj_from_filename(self._temp_filename)\n        self._transfer_coordinator.add_failure_cleanup(\n            self._osutil.remove_file, self._temp_filename\n        )\n        return f\n\n\nclass DownloadSeekableOutputManager(DownloadOutputManager):\n    @classmethod\n    def is_compatible(cls, download_target, osutil):\n        return seekable(download_target)\n\n    def get_fileobj_for_io_writes(self, transfer_future):\n        # Return the fileobj provided to the future.\n        return transfer_future.meta.call_args.fileobj\n\n    def get_final_io_task(self):\n        # This task will serve the purpose of signaling when all of the io\n        # writes have finished so done callbacks can be called.\n        return CompleteDownloadNOOPTask(\n            transfer_coordinator=self._transfer_coordinator\n        )\n\n\nclass DownloadNonSeekableOutputManager(DownloadOutputManager):\n    def __init__(\n        self, osutil, transfer_coordinator, io_executor, defer_queue=None\n    ):\n        super().__init__(osutil, transfer_coordinator, io_executor)\n        if defer_queue is None:\n            defer_queue = DeferQueue()\n        self._defer_queue = defer_queue\n        self._io_submit_lock = threading.Lock()\n\n    @classmethod\n    def is_compatible(cls, download_target, osutil):\n        return hasattr(download_target, 'write')\n\n    def get_download_task_tag(self):\n        return IN_MEMORY_DOWNLOAD_TAG\n\n    def get_fileobj_for_io_writes(self, transfer_future):\n        return transfer_future.meta.call_args.fileobj\n\n    def get_final_io_task(self):\n        return CompleteDownloadNOOPTask(\n            transfer_coordinator=self._transfer_coordinator\n        )\n\n    def queue_file_io_task(self, fileobj, data, offset):\n        with self._io_submit_lock:\n            writes = self._defer_queue.request_writes(offset, data)\n            for write in writes:\n                data = write['data']\n                logger.debug(\n                    \"Queueing IO offset %s for fileobj: %s\",\n                    write['offset'],\n                    fileobj,\n                )\n                super().queue_file_io_task(fileobj, data, offset)\n\n    def get_io_write_task(self, fileobj, data, offset):\n        return IOStreamingWriteTask(\n            self._transfer_coordinator,\n            main_kwargs={\n                'fileobj': fileobj,\n                'data': data,\n            },\n        )\n\n\nclass DownloadSpecialFilenameOutputManager(DownloadNonSeekableOutputManager):\n    def __init__(\n        self, osutil, transfer_coordinator, io_executor, defer_queue=None\n    ):\n        super().__init__(\n            osutil, transfer_coordinator, io_executor, defer_queue\n        )\n        self._fileobj = None\n\n    @classmethod\n    def is_compatible(cls, download_target, osutil):\n        return isinstance(download_target, str) and osutil.is_special_file(\n            download_target\n        )\n\n    def get_fileobj_for_io_writes(self, transfer_future):\n        filename = transfer_future.meta.call_args.fileobj\n        self._fileobj = self._get_fileobj_from_filename(filename)\n        return self._fileobj\n\n    def get_final_io_task(self):\n        # Make sure the file gets closed once the transfer is done.\n        return IOCloseTask(\n            transfer_coordinator=self._transfer_coordinator,\n            is_final=True,\n            main_kwargs={'fileobj': self._fileobj},\n        )\n\n\nclass DownloadSubmissionTask(SubmissionTask):\n    \"\"\"Task for submitting tasks to execute a download\"\"\"\n\n    def _get_download_output_manager_cls(self, transfer_future, osutil):\n        \"\"\"Retrieves a class for managing output for a download\n\n        :type transfer_future: s3transfer.futures.TransferFuture\n        :param transfer_future: The transfer future for the request\n\n        :type osutil: s3transfer.utils.OSUtils\n        :param osutil: The os utility associated to the transfer\n\n        :rtype: class of DownloadOutputManager\n        :returns: The appropriate class to use for managing a specific type of\n            input for downloads.\n        \"\"\"\n        download_manager_resolver_chain = [\n            DownloadSpecialFilenameOutputManager,\n            DownloadFilenameOutputManager,\n            DownloadSeekableOutputManager,\n            DownloadNonSeekableOutputManager,\n        ]\n\n        fileobj = transfer_future.meta.call_args.fileobj\n        for download_manager_cls in download_manager_resolver_chain:\n            if download_manager_cls.is_compatible(fileobj, osutil):\n                return download_manager_cls\n        raise RuntimeError(\n            'Output {} of type: {} is not supported.'.format(\n                fileobj, type(fileobj)\n            )\n        )\n\n    def _submit(\n        self,\n        client,\n        config,\n        osutil,\n        request_executor,\n        io_executor,\n        transfer_future,\n        bandwidth_limiter=None,\n    ):\n        \"\"\"\n        :param client: The client associated with the transfer manager\n\n        :type config: s3transfer.manager.TransferConfig\n        :param config: The transfer config associated with the transfer\n            manager\n\n        :type osutil: s3transfer.utils.OSUtil\n        :param osutil: The os utility associated to the transfer manager\n\n        :type request_executor: s3transfer.futures.BoundedExecutor\n        :param request_executor: The request executor associated with the\n            transfer manager\n\n        :type io_executor: s3transfer.futures.BoundedExecutor\n        :param io_executor: The io executor associated with the\n            transfer manager\n\n        :type transfer_future: s3transfer.futures.TransferFuture\n        :param transfer_future: The transfer future associated with the\n            transfer request that tasks are being submitted for\n\n        :type bandwidth_limiter: s3transfer.bandwidth.BandwidthLimiter\n        :param bandwidth_limiter: The bandwidth limiter to use when\n            downloading streams\n        \"\"\"\n        if transfer_future.meta.size is None:\n            # If a size was not provided figure out the size for the\n            # user.\n            response = client.head_object(\n                Bucket=transfer_future.meta.call_args.bucket,\n                Key=transfer_future.meta.call_args.key,\n                **transfer_future.meta.call_args.extra_args,\n            )\n            transfer_future.meta.provide_transfer_size(\n                response['ContentLength']\n            )\n\n        download_output_manager = self._get_download_output_manager_cls(\n            transfer_future, osutil\n        )(osutil, self._transfer_coordinator, io_executor)\n\n        # If it is greater than threshold do a ranged download, otherwise\n        # do a regular GetObject download.\n        if transfer_future.meta.size < config.multipart_threshold:\n            self._submit_download_request(\n                client,\n                config,\n                osutil,\n                request_executor,\n                io_executor,\n                download_output_manager,\n                transfer_future,\n                bandwidth_limiter,\n            )\n        else:\n            self._submit_ranged_download_request(\n                client,\n                config,\n                osutil,\n                request_executor,\n                io_executor,\n                download_output_manager,\n                transfer_future,\n                bandwidth_limiter,\n            )\n\n    def _submit_download_request(\n        self,\n        client,\n        config,\n        osutil,\n        request_executor,\n        io_executor,\n        download_output_manager,\n        transfer_future,\n        bandwidth_limiter,\n    ):\n        call_args = transfer_future.meta.call_args\n\n        # Get a handle to the file that will be used for writing downloaded\n        # contents\n        fileobj = download_output_manager.get_fileobj_for_io_writes(\n            transfer_future\n        )\n\n        # Get the needed callbacks for the task\n        progress_callbacks = get_callbacks(transfer_future, 'progress')\n\n        # Get any associated tags for the get object task.\n        get_object_tag = download_output_manager.get_download_task_tag()\n\n        # Get the final io task to run once the download is complete.\n        final_task = download_output_manager.get_final_io_task()\n\n        # Submit the task to download the object.\n        self._transfer_coordinator.submit(\n            request_executor,\n            ImmediatelyWriteIOGetObjectTask(\n                transfer_coordinator=self._transfer_coordinator,\n                main_kwargs={\n                    'client': client,\n                    'bucket': call_args.bucket,\n                    'key': call_args.key,\n                    'fileobj': fileobj,\n                    'extra_args': call_args.extra_args,\n                    'callbacks': progress_callbacks,\n                    'max_attempts': config.num_download_attempts,\n                    'download_output_manager': download_output_manager,\n                    'io_chunksize': config.io_chunksize,\n                    'bandwidth_limiter': bandwidth_limiter,\n                },\n                done_callbacks=[final_task],\n            ),\n            tag=get_object_tag,\n        )\n\n    def _submit_ranged_download_request(\n        self,\n        client,\n        config,\n        osutil,\n        request_executor,\n        io_executor,\n        download_output_manager,\n        transfer_future,\n        bandwidth_limiter,\n    ):\n        call_args = transfer_future.meta.call_args\n\n        # Get the needed progress callbacks for the task\n        progress_callbacks = get_callbacks(transfer_future, 'progress')\n\n        # Get a handle to the file that will be used for writing downloaded\n        # contents\n        fileobj = download_output_manager.get_fileobj_for_io_writes(\n            transfer_future\n        )\n\n        # Determine the number of parts\n        part_size = config.multipart_chunksize\n        num_parts = calculate_num_parts(transfer_future.meta.size, part_size)\n\n        # Get any associated tags for the get object task.\n        get_object_tag = download_output_manager.get_download_task_tag()\n\n        # Callback invoker to submit the final io task once all downloads\n        # are complete.\n        finalize_download_invoker = CountCallbackInvoker(\n            self._get_final_io_task_submission_callback(\n                download_output_manager, io_executor\n            )\n        )\n        for i in range(num_parts):\n            # Calculate the range parameter\n            range_parameter = calculate_range_parameter(\n                part_size, i, num_parts\n            )\n\n            # Inject the Range parameter to the parameters to be passed in\n            # as extra args\n            extra_args = {'Range': range_parameter}\n            extra_args.update(call_args.extra_args)\n            finalize_download_invoker.increment()\n            # Submit the ranged downloads\n            self._transfer_coordinator.submit(\n                request_executor,\n                GetObjectTask(\n                    transfer_coordinator=self._transfer_coordinator,\n                    main_kwargs={\n                        'client': client,\n                        'bucket': call_args.bucket,\n                        'key': call_args.key,\n                        'fileobj': fileobj,\n                        'extra_args': extra_args,\n                        'callbacks': progress_callbacks,\n                        'max_attempts': config.num_download_attempts,\n                        'start_index': i * part_size,\n                        'download_output_manager': download_output_manager,\n                        'io_chunksize': config.io_chunksize,\n                        'bandwidth_limiter': bandwidth_limiter,\n                    },\n                    done_callbacks=[finalize_download_invoker.decrement],\n                ),\n                tag=get_object_tag,\n            )\n        finalize_download_invoker.finalize()\n\n    def _get_final_io_task_submission_callback(\n        self, download_manager, io_executor\n    ):\n        final_task = download_manager.get_final_io_task()\n        return FunctionContainer(\n            self._transfer_coordinator.submit, io_executor, final_task\n        )\n\n    def _calculate_range_param(self, part_size, part_index, num_parts):\n        # Used to calculate the Range parameter\n        start_range = part_index * part_size\n        if part_index == num_parts - 1:\n            end_range = ''\n        else:\n            end_range = start_range + part_size - 1\n        range_param = f'bytes={start_range}-{end_range}'\n        return range_param\n\n\nclass GetObjectTask(Task):\n    def _main(\n        self,\n        client,\n        bucket,\n        key,\n        fileobj,\n        extra_args,\n        callbacks,\n        max_attempts,\n        download_output_manager,\n        io_chunksize,\n        start_index=0,\n        bandwidth_limiter=None,\n    ):\n        \"\"\"Downloads an object and places content into io queue\n\n        :param client: The client to use when calling GetObject\n        :param bucket: The bucket to download from\n        :param key: The key to download from\n        :param fileobj: The file handle to write content to\n        :param exta_args: Any extra arguments to include in GetObject request\n        :param callbacks: List of progress callbacks to invoke on download\n        :param max_attempts: The number of retries to do when downloading\n        :param download_output_manager: The download output manager associated\n            with the current download.\n        :param io_chunksize: The size of each io chunk to read from the\n            download stream and queue in the io queue.\n        :param start_index: The location in the file to start writing the\n            content of the key to.\n        :param bandwidth_limiter: The bandwidth limiter to use when throttling\n            the downloading of data in streams.\n        \"\"\"\n        last_exception = None\n        for i in range(max_attempts):\n            try:\n                current_index = start_index\n                response = client.get_object(\n                    Bucket=bucket, Key=key, **extra_args\n                )\n                streaming_body = StreamReaderProgress(\n                    response['Body'], callbacks\n                )\n                if bandwidth_limiter:\n                    streaming_body = (\n                        bandwidth_limiter.get_bandwith_limited_stream(\n                            streaming_body, self._transfer_coordinator\n                        )\n                    )\n\n                chunks = DownloadChunkIterator(streaming_body, io_chunksize)\n                for chunk in chunks:\n                    # If the transfer is done because of a cancellation\n                    # or error somewhere else, stop trying to submit more\n                    # data to be written and break out of the download.\n                    if not self._transfer_coordinator.done():\n                        self._handle_io(\n                            download_output_manager,\n                            fileobj,\n                            chunk,\n                            current_index,\n                        )\n                        current_index += len(chunk)\n                    else:\n                        return\n                return\n            except S3_RETRYABLE_DOWNLOAD_ERRORS as e:\n                logger.debug(\n                    \"Retrying exception caught (%s), \"\n                    \"retrying request, (attempt %s / %s)\",\n                    e,\n                    i,\n                    max_attempts,\n                    exc_info=True,\n                )\n                last_exception = e\n                # Also invoke the progress callbacks to indicate that we\n                # are trying to download the stream again and all progress\n                # for this GetObject has been lost.\n                invoke_progress_callbacks(\n                    callbacks, start_index - current_index\n                )\n                continue\n        raise RetriesExceededError(last_exception)\n\n    def _handle_io(self, download_output_manager, fileobj, chunk, index):\n        download_output_manager.queue_file_io_task(fileobj, chunk, index)\n\n\nclass ImmediatelyWriteIOGetObjectTask(GetObjectTask):\n    \"\"\"GetObjectTask that immediately writes to the provided file object\n\n    This is useful for downloads where it is known only one thread is\n    downloading the object so there is no reason to go through the\n    overhead of using an IO queue and executor.\n    \"\"\"\n\n    def _handle_io(self, download_output_manager, fileobj, chunk, index):\n        task = download_output_manager.get_io_write_task(fileobj, chunk, index)\n        task()\n\n\nclass IOWriteTask(Task):\n    def _main(self, fileobj, data, offset):\n        \"\"\"Pulls off an io queue to write contents to a file\n\n        :param fileobj: The file handle to write content to\n        :param data: The data to write\n        :param offset: The offset to write the data to.\n        \"\"\"\n        fileobj.seek(offset)\n        fileobj.write(data)\n\n\nclass IOStreamingWriteTask(Task):\n    \"\"\"Task for writing data to a non-seekable stream.\"\"\"\n\n    def _main(self, fileobj, data):\n        \"\"\"Write data to a fileobj.\n\n        Data will be written directly to the fileobj without\n        any prior seeking.\n\n        :param fileobj: The fileobj to write content to\n        :param data: The data to write\n\n        \"\"\"\n        fileobj.write(data)\n\n\nclass IORenameFileTask(Task):\n    \"\"\"A task to rename a temporary file to its final filename\n\n    :param fileobj: The file handle that content was written to.\n    :param final_filename: The final name of the file to rename to\n        upon completion of writing the contents.\n    :param osutil: OS utility\n    \"\"\"\n\n    def _main(self, fileobj, final_filename, osutil):\n        fileobj.close()\n        osutil.rename_file(fileobj.name, final_filename)\n\n\nclass IOCloseTask(Task):\n    \"\"\"A task to close out a file once the download is complete.\n\n    :param fileobj: The fileobj to close.\n    \"\"\"\n\n    def _main(self, fileobj):\n        fileobj.close()\n\n\nclass CompleteDownloadNOOPTask(Task):\n    \"\"\"A NOOP task to serve as an indicator that the download is complete\n\n    Note that the default for is_final is set to True because this should\n    always be the last task.\n    \"\"\"\n\n    def __init__(\n        self,\n        transfer_coordinator,\n        main_kwargs=None,\n        pending_main_kwargs=None,\n        done_callbacks=None,\n        is_final=True,\n    ):\n        super().__init__(\n            transfer_coordinator=transfer_coordinator,\n            main_kwargs=main_kwargs,\n            pending_main_kwargs=pending_main_kwargs,\n            done_callbacks=done_callbacks,\n            is_final=is_final,\n        )\n\n    def _main(self):\n        pass\n\n\nclass DownloadChunkIterator:\n    def __init__(self, body, chunksize):\n        \"\"\"Iterator to chunk out a downloaded S3 stream\n\n        :param body: A readable file-like object\n        :param chunksize: The amount to read each time\n        \"\"\"\n        self._body = body\n        self._chunksize = chunksize\n        self._num_reads = 0\n\n    def __iter__(self):\n        return self\n\n    def __next__(self):\n        chunk = self._body.read(self._chunksize)\n        self._num_reads += 1\n        if chunk:\n            return chunk\n        elif self._num_reads == 1:\n            # Even though the response may have not had any\n            # content, we still want to account for an empty object's\n            # existence so return the empty chunk for that initial\n            # read.\n            return chunk\n        raise StopIteration()\n\n    next = __next__\n\n\nclass DeferQueue:\n    \"\"\"IO queue that defers write requests until they are queued sequentially.\n\n    This class is used to track IO data for a *single* fileobj.\n\n    You can send data to this queue, and it will defer any IO write requests\n    until it has the next contiguous block available (starting at 0).\n\n    \"\"\"\n\n    def __init__(self):\n        self._writes = []\n        self._pending_offsets = set()\n        self._next_offset = 0\n\n    def request_writes(self, offset, data):\n        \"\"\"Request any available writes given new incoming data.\n\n        You call this method by providing new data along with the\n        offset associated with the data.  If that new data unlocks\n        any contiguous writes that can now be submitted, this\n        method will return all applicable writes.\n\n        This is done with 1 method call so you don't have to\n        make two method calls (put(), get()) which acquires a lock\n        each method call.\n\n        \"\"\"\n        if offset < self._next_offset:\n            # This is a request for a write that we've already\n            # seen.  This can happen in the event of a retry\n            # where if we retry at at offset N/2, we'll requeue\n            # offsets 0-N/2 again.\n            return []\n        writes = []\n        if offset in self._pending_offsets:\n            # We've already queued this offset so this request is\n            # a duplicate.  In this case we should ignore\n            # this request and prefer what's already queued.\n            return []\n        heapq.heappush(self._writes, (offset, data))\n        self._pending_offsets.add(offset)\n        while self._writes and self._writes[0][0] == self._next_offset:\n            next_write = heapq.heappop(self._writes)\n            writes.append({'offset': next_write[0], 'data': next_write[1]})\n            self._pending_offsets.remove(next_write[0])\n            self._next_offset += len(next_write[1])\n        return writes\n"}