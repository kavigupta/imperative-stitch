{"distlib/locators.py": "# -*- coding: utf-8 -*-\n#\n# Copyright (C) 2012-2023 Vinay Sajip.\n# Licensed to the Python Software Foundation under a contributor agreement.\n# See LICENSE.txt and CONTRIBUTORS.txt.\n#\n\nimport gzip\nfrom io import BytesIO\nimport json\nimport logging\nimport os\nimport posixpath\nimport re\ntry:\n    import threading\nexcept ImportError:  # pragma: no cover\n    import dummy_threading as threading\nimport zlib\n\nfrom . import DistlibException\nfrom .compat import (urljoin, urlparse, urlunparse, url2pathname, pathname2url, queue, quote, unescape, build_opener,\n                     HTTPRedirectHandler as BaseRedirectHandler, text_type, Request, HTTPError, URLError)\nfrom .database import Distribution, DistributionPath, make_dist\nfrom .metadata import Metadata, MetadataInvalidError\nfrom .util import (cached_property, ensure_slash, split_filename, get_project_data, parse_requirement,\n                   parse_name_and_version, ServerProxy, normalize_name)\nfrom .version import get_scheme, UnsupportedVersionError\nfrom .wheel import Wheel, is_compatible\n\nlogger = logging.getLogger(__name__)\n\nHASHER_HASH = re.compile(r'^(\\w+)=([a-f0-9]+)')\nCHARSET = re.compile(r';\\s*charset\\s*=\\s*(.*)\\s*$', re.I)\nHTML_CONTENT_TYPE = re.compile('text/html|application/x(ht)?ml')\nDEFAULT_INDEX = 'https://pypi.org/pypi'\n\n\ndef get_all_distribution_names(url=None):\n    \"\"\"\n    Return all distribution names known by an index.\n    :param url: The URL of the index.\n    :return: A list of all known distribution names.\n    \"\"\"\n    if url is None:\n        url = DEFAULT_INDEX\n    client = ServerProxy(url, timeout=3.0)\n    try:\n        return client.list_packages()\n    finally:\n        client('close')()\n\n\nclass RedirectHandler(BaseRedirectHandler):\n    \"\"\"\n    A class to work around a bug in some Python 3.2.x releases.\n    \"\"\"\n\n    # There's a bug in the base version for some 3.2.x\n    # (e.g. 3.2.2 on Ubuntu Oneiric). If a Location header\n    # returns e.g. /abc, it bails because it says the scheme ''\n    # is bogus, when actually it should use the request's\n    # URL for the scheme. See Python issue #13696.\n    def http_error_302(self, req, fp, code, msg, headers):\n        # Some servers (incorrectly) return multiple Location headers\n        # (so probably same goes for URI).  Use first header.\n        newurl = None\n        for key in ('location', 'uri'):\n            if key in headers:\n                newurl = headers[key]\n                break\n        if newurl is None:  # pragma: no cover\n            return\n        urlparts = urlparse(newurl)\n        if urlparts.scheme == '':\n            newurl = urljoin(req.get_full_url(), newurl)\n            if hasattr(headers, 'replace_header'):\n                headers.replace_header(key, newurl)\n            else:\n                headers[key] = newurl\n        return BaseRedirectHandler.http_error_302(self, req, fp, code, msg, headers)\n\n    http_error_301 = http_error_303 = http_error_307 = http_error_302\n\n\nclass Locator(object):\n    \"\"\"\n    A base class for locators - things that locate distributions.\n    \"\"\"\n    source_extensions = ('.tar.gz', '.tar.bz2', '.tar', '.zip', '.tgz', '.tbz')\n    binary_extensions = ('.egg', '.exe', '.whl')\n    excluded_extensions = ('.pdf', )\n\n    # A list of tags indicating which wheels you want to match. The default\n    # value of None matches against the tags compatible with the running\n    # Python. If you want to match other values, set wheel_tags on a locator\n    # instance to a list of tuples (pyver, abi, arch) which you want to match.\n    wheel_tags = None\n\n    downloadable_extensions = source_extensions + ('.whl', )\n\n    def __init__(self, scheme='default'):\n        \"\"\"\n        Initialise an instance.\n        :param scheme: Because locators look for most recent versions, they\n                       need to know the version scheme to use. This specifies\n                       the current PEP-recommended scheme - use ``'legacy'``\n                       if you need to support existing distributions on PyPI.\n        \"\"\"\n        self._cache = {}\n        self.scheme = scheme\n        # Because of bugs in some of the handlers on some of the platforms,\n        # we use our own opener rather than just using urlopen.\n        self.opener = build_opener(RedirectHandler())\n        # If get_project() is called from locate(), the matcher instance\n        # is set from the requirement passed to locate(). See issue #18 for\n        # why this can be useful to know.\n        self.matcher = None\n        self.errors = queue.Queue()\n\n    def get_errors(self):\n        \"\"\"\n        Return any errors which have occurred.\n        \"\"\"\n        result = []\n        while not self.errors.empty():  # pragma: no cover\n            try:\n                e = self.errors.get(False)\n                result.append(e)\n            except self.errors.Empty:\n                continue\n            self.errors.task_done()\n        return result\n\n    def clear_errors(self):\n        \"\"\"\n        Clear any errors which may have been logged.\n        \"\"\"\n        # Just get the errors and throw them away\n        self.get_errors()\n\n    def clear_cache(self):\n        self._cache.clear()\n\n    def _get_scheme(self):\n        return self._scheme\n\n    def _set_scheme(self, value):\n        self._scheme = value\n\n    scheme = property(_get_scheme, _set_scheme)\n\n    def _get_project(self, name):\n        \"\"\"\n        For a given project, get a dictionary mapping available versions to Distribution\n        instances.\n\n        This should be implemented in subclasses.\n\n        If called from a locate() request, self.matcher will be set to a\n        matcher for the requirement to satisfy, otherwise it will be None.\n        \"\"\"\n        raise NotImplementedError('Please implement in the subclass')\n\n    def get_distribution_names(self):\n        \"\"\"\n        Return all the distribution names known to this locator.\n        \"\"\"\n        raise NotImplementedError('Please implement in the subclass')\n\n    def get_project(self, name):\n        \"\"\"\n        For a given project, get a dictionary mapping available versions to Distribution\n        instances.\n\n        This calls _get_project to do all the work, and just implements a caching layer on top.\n        \"\"\"\n        if self._cache is None:  # pragma: no cover\n            result = self._get_project(name)\n        elif name in self._cache:\n            result = self._cache[name]\n        else:\n            self.clear_errors()\n            result = self._get_project(name)\n            self._cache[name] = result\n        return result\n\n    def score_url(self, url):\n        \"\"\"\n        Give an url a score which can be used to choose preferred URLs\n        for a given project release.\n        \"\"\"\n        t = urlparse(url)\n        basename = posixpath.basename(t.path)\n        compatible = True\n        is_wheel = basename.endswith('.whl')\n        is_downloadable = basename.endswith(self.downloadable_extensions)\n        if is_wheel:\n            compatible = is_compatible(Wheel(basename), self.wheel_tags)\n        return (t.scheme == 'https', 'pypi.org' in t.netloc, is_downloadable, is_wheel, compatible, basename)\n\n    def prefer_url(self, url1, url2):\n        \"\"\"\n        Choose one of two URLs where both are candidates for distribution\n        archives for the same version of a distribution (for example,\n        .tar.gz vs. zip).\n\n        The current implementation favours https:// URLs over http://, archives\n        from PyPI over those from other locations, wheel compatibility (if a\n        wheel) and then the archive name.\n        \"\"\"\n        result = url2\n        if url1:\n            s1 = self.score_url(url1)\n            s2 = self.score_url(url2)\n            if s1 > s2:\n                result = url1\n            if result != url2:\n                logger.debug('Not replacing %r with %r', url1, url2)\n            else:\n                logger.debug('Replacing %r with %r', url1, url2)\n        return result\n\n    def split_filename(self, filename, project_name):\n        \"\"\"\n        Attempt to split a filename in project name, version and Python version.\n        \"\"\"\n        return split_filename(filename, project_name)\n\n    def convert_url_to_download_info(self, url, project_name):\n        \"\"\"\n        See if a URL is a candidate for a download URL for a project (the URL\n        has typically been scraped from an HTML page).\n\n        If it is, a dictionary is returned with keys \"name\", \"version\",\n        \"filename\" and \"url\"; otherwise, None is returned.\n        \"\"\"\n\n        def same_project(name1, name2):\n            return normalize_name(name1) == normalize_name(name2)\n\n        result = None\n        scheme, netloc, path, params, query, frag = urlparse(url)\n        if frag.lower().startswith('egg='):  # pragma: no cover\n            logger.debug('%s: version hint in fragment: %r', project_name, frag)\n        m = HASHER_HASH.match(frag)\n        if m:\n            algo, digest = m.groups()\n        else:\n            algo, digest = None, None\n        origpath = path\n        if path and path[-1] == '/':  # pragma: no cover\n            path = path[:-1]\n        if path.endswith('.whl'):\n            try:\n                wheel = Wheel(path)\n                if not is_compatible(wheel, self.wheel_tags):\n                    logger.debug('Wheel not compatible: %s', path)\n                else:\n                    if project_name is None:\n                        include = True\n                    else:\n                        include = same_project(wheel.name, project_name)\n                    if include:\n                        result = {\n                            'name': wheel.name,\n                            'version': wheel.version,\n                            'filename': wheel.filename,\n                            'url': urlunparse((scheme, netloc, origpath, params, query, '')),\n                            'python-version': ', '.join(['.'.join(list(v[2:])) for v in wheel.pyver]),\n                        }\n            except Exception:  # pragma: no cover\n                logger.warning('invalid path for wheel: %s', path)\n        elif not path.endswith(self.downloadable_extensions):  # pragma: no cover\n            logger.debug('Not downloadable: %s', path)\n        else:  # downloadable extension\n            path = filename = posixpath.basename(path)\n            for ext in self.downloadable_extensions:\n                if path.endswith(ext):\n                    path = path[:-len(ext)]\n                    t = self.split_filename(path, project_name)\n                    if not t:  # pragma: no cover\n                        logger.debug('No match for project/version: %s', path)\n                    else:\n                        name, version, pyver = t\n                        if not project_name or same_project(project_name, name):\n                            result = {\n                                'name': name,\n                                'version': version,\n                                'filename': filename,\n                                'url': urlunparse((scheme, netloc, origpath, params, query, '')),\n                            }\n                            if pyver:  # pragma: no cover\n                                result['python-version'] = pyver\n                    break\n        if result and algo:\n            result['%s_digest' % algo] = digest\n        return result\n\n    def _get_digest(self, info):\n        \"\"\"\n        Get a digest from a dictionary by looking at a \"digests\" dictionary\n        or keys of the form 'algo_digest'.\n\n        Returns a 2-tuple (algo, digest) if found, else None. Currently\n        looks only for SHA256, then MD5.\n        \"\"\"\n        result = None\n        if 'digests' in info:\n            digests = info['digests']\n            for algo in ('sha256', 'md5'):\n                if algo in digests:\n                    result = (algo, digests[algo])\n                    break\n        if not result:\n            for algo in ('sha256', 'md5'):\n                key = '%s_digest' % algo\n                if key in info:\n                    result = (algo, info[key])\n                    break\n        return result\n\n    def _update_version_data(self, result, info):\n        \"\"\"\n        Update a result dictionary (the final result from _get_project) with a\n        dictionary for a specific version, which typically holds information\n        gleaned from a filename or URL for an archive for the distribution.\n        \"\"\"\n        name = info.pop('name')\n        version = info.pop('version')\n        if version in result:\n            dist = result[version]\n            md = dist.metadata\n        else:\n            dist = make_dist(name, version, scheme=self.scheme)\n            md = dist.metadata\n        dist.digest = digest = self._get_digest(info)\n        url = info['url']\n        result['digests'][url] = digest\n        if md.source_url != info['url']:\n            md.source_url = self.prefer_url(md.source_url, url)\n            result['urls'].setdefault(version, set()).add(url)\n        dist.locator = self\n        result[version] = dist\n\n    def locate(self, requirement, prereleases=False):\n        \"\"\"\n        Find the most recent distribution which matches the given\n        requirement.\n\n        :param requirement: A requirement of the form 'foo (1.0)' or perhaps\n                            'foo (>= 1.0, < 2.0, != 1.3)'\n        :param prereleases: If ``True``, allow pre-release versions\n                            to be located. Otherwise, pre-release versions\n                            are not returned.\n        :return: A :class:`Distribution` instance, or ``None`` if no such\n                 distribution could be located.\n        \"\"\"\n        result = None\n        r = parse_requirement(requirement)\n        if r is None:  # pragma: no cover\n            raise DistlibException('Not a valid requirement: %r' % requirement)\n        scheme = get_scheme(self.scheme)\n        self.matcher = matcher = scheme.matcher(r.requirement)\n        logger.debug('matcher: %s (%s)', matcher, type(matcher).__name__)\n        versions = self.get_project(r.name)\n        if len(versions) > 2:  # urls and digests keys are present\n            # sometimes, versions are invalid\n            slist = []\n            vcls = matcher.version_class\n            for k in versions:\n                if k in ('urls', 'digests'):\n                    continue\n                try:\n                    if not matcher.match(k):\n                        pass  # logger.debug('%s did not match %r', matcher, k)\n                    else:\n                        if prereleases or not vcls(k).is_prerelease:\n                            slist.append(k)\n                except Exception:  # pragma: no cover\n                    logger.warning('error matching %s with %r', matcher, k)\n                    pass  # slist.append(k)\n            if len(slist) > 1:\n                slist = sorted(slist, key=scheme.key)\n            if slist:\n                logger.debug('sorted list: %s', slist)\n                version = slist[-1]\n                result = versions[version]\n        if result:\n            if r.extras:\n                result.extras = r.extras\n            result.download_urls = versions.get('urls', {}).get(version, set())\n            d = {}\n            sd = versions.get('digests', {})\n            for url in result.download_urls:\n                if url in sd:  # pragma: no cover\n                    d[url] = sd[url]\n            result.digests = d\n        self.matcher = None\n        return result\n\n\nclass PyPIRPCLocator(Locator):\n    \"\"\"\n    This locator uses XML-RPC to locate distributions. It therefore\n    cannot be used with simple mirrors (that only mirror file content).\n    \"\"\"\n\n    def __init__(self, url, **kwargs):\n        \"\"\"\n        Initialise an instance.\n\n        :param url: The URL to use for XML-RPC.\n        :param kwargs: Passed to the superclass constructor.\n        \"\"\"\n        super(PyPIRPCLocator, self).__init__(**kwargs)\n        self.base_url = url\n        self.client = ServerProxy(url, timeout=3.0)\n\n    def get_distribution_names(self):\n        \"\"\"\n        Return all the distribution names known to this locator.\n        \"\"\"\n        return set(self.client.list_packages())\n\n    def _get_project(self, name):\n        result = {'urls': {}, 'digests': {}}\n        versions = self.client.package_releases(name, True)\n        for v in versions:\n            urls = self.client.release_urls(name, v)\n            data = self.client.release_data(name, v)\n            metadata = Metadata(scheme=self.scheme)\n            metadata.name = data['name']\n            metadata.version = data['version']\n            metadata.license = data.get('license')\n            metadata.keywords = data.get('keywords', [])\n            metadata.summary = data.get('summary')\n            dist = Distribution(metadata)\n            if urls:\n                info = urls[0]\n                metadata.source_url = info['url']\n                dist.digest = self._get_digest(info)\n                dist.locator = self\n                result[v] = dist\n                for info in urls:\n                    url = info['url']\n                    digest = self._get_digest(info)\n                    result['urls'].setdefault(v, set()).add(url)\n                    result['digests'][url] = digest\n        return result\n\n\nclass PyPIJSONLocator(Locator):\n    \"\"\"\n    This locator uses PyPI's JSON interface. It's very limited in functionality\n    and probably not worth using.\n    \"\"\"\n\n    def __init__(self, url, **kwargs):\n        super(PyPIJSONLocator, self).__init__(**kwargs)\n        self.base_url = ensure_slash(url)\n\n    def get_distribution_names(self):\n        \"\"\"\n        Return all the distribution names known to this locator.\n        \"\"\"\n        raise NotImplementedError('Not available from this locator')\n\n    def _get_project(self, name):\n        result = {'urls': {}, 'digests': {}}\n        url = urljoin(self.base_url, '%s/json' % quote(name))\n        try:\n            resp = self.opener.open(url)\n            data = resp.read().decode()  # for now\n            d = json.loads(data)\n            md = Metadata(scheme=self.scheme)\n            data = d['info']\n            md.name = data['name']\n            md.version = data['version']\n            md.license = data.get('license')\n            md.keywords = data.get('keywords', [])\n            md.summary = data.get('summary')\n            dist = Distribution(md)\n            dist.locator = self\n            # urls = d['urls']\n            result[md.version] = dist\n            for info in d['urls']:\n                url = info['url']\n                dist.download_urls.add(url)\n                dist.digests[url] = self._get_digest(info)\n                result['urls'].setdefault(md.version, set()).add(url)\n                result['digests'][url] = self._get_digest(info)\n            # Now get other releases\n            for version, infos in d['releases'].items():\n                if version == md.version:\n                    continue  # already done\n                omd = Metadata(scheme=self.scheme)\n                omd.name = md.name\n                omd.version = version\n                odist = Distribution(omd)\n                odist.locator = self\n                result[version] = odist\n                for info in infos:\n                    url = info['url']\n                    odist.download_urls.add(url)\n                    odist.digests[url] = self._get_digest(info)\n                    result['urls'].setdefault(version, set()).add(url)\n                    result['digests'][url] = self._get_digest(info)\n\n\n#            for info in urls:\n#                md.source_url = info['url']\n#                dist.digest = self._get_digest(info)\n#                dist.locator = self\n#                for info in urls:\n#                    url = info['url']\n#                    result['urls'].setdefault(md.version, set()).add(url)\n#                    result['digests'][url] = self._get_digest(info)\n        except Exception as e:\n            self.errors.put(text_type(e))\n            logger.exception('JSON fetch failed: %s', e)\n        return result\n\n\nclass Page(object):\n    \"\"\"\n    This class represents a scraped HTML page.\n    \"\"\"\n    # The following slightly hairy-looking regex just looks for the contents of\n    # an anchor link, which has an attribute \"href\" either immediately preceded\n    # or immediately followed by a \"rel\" attribute. The attribute values can be\n    # declared with double quotes, single quotes or no quotes - which leads to\n    # the length of the expression.\n    _href = re.compile(\n        \"\"\"\n(rel\\\\s*=\\\\s*(?:\"(?P<rel1>[^\"]*)\"|'(?P<rel2>[^']*)'|(?P<rel3>[^>\\\\s\\n]*))\\\\s+)?\nhref\\\\s*=\\\\s*(?:\"(?P<url1>[^\"]*)\"|'(?P<url2>[^']*)'|(?P<url3>[^>\\\\s\\n]*))\n(\\\\s+rel\\\\s*=\\\\s*(?:\"(?P<rel4>[^\"]*)\"|'(?P<rel5>[^']*)'|(?P<rel6>[^>\\\\s\\n]*)))?\n\"\"\", re.I | re.S | re.X)\n    _base = re.compile(r\"\"\"<base\\s+href\\s*=\\s*['\"]?([^'\">]+)\"\"\", re.I | re.S)\n\n    def __init__(self, data, url):\n        \"\"\"\n        Initialise an instance with the Unicode page contents and the URL they\n        came from.\n        \"\"\"\n        self.data = data\n        self.base_url = self.url = url\n        m = self._base.search(self.data)\n        if m:\n            self.base_url = m.group(1)\n\n    _clean_re = re.compile(r'[^a-z0-9$&+,/:;=?@.#%_\\\\|-]', re.I)\n\n    @cached_property\n    def links(self):\n        \"\"\"\n        Return the URLs of all the links on a page together with information\n        about their \"rel\" attribute, for determining which ones to treat as\n        downloads and which ones to queue for further scraping.\n        \"\"\"\n\n        def clean(url):\n            \"Tidy up an URL.\"\n            scheme, netloc, path, params, query, frag = urlparse(url)\n            return urlunparse((scheme, netloc, quote(path), params, query, frag))\n\n        result = set()\n        for match in self._href.finditer(self.data):\n            d = match.groupdict('')\n            rel = (d['rel1'] or d['rel2'] or d['rel3'] or d['rel4'] or d['rel5'] or d['rel6'])\n            url = d['url1'] or d['url2'] or d['url3']\n            url = urljoin(self.base_url, url)\n            url = unescape(url)\n            url = self._clean_re.sub(lambda m: '%%%2x' % ord(m.group(0)), url)\n            result.add((url, rel))\n        # We sort the result, hoping to bring the most recent versions\n        # to the front\n        result = sorted(result, key=lambda t: t[0], reverse=True)\n        return result\n\n\nclass SimpleScrapingLocator(Locator):\n    \"\"\"\n    A locator which scrapes HTML pages to locate downloads for a distribution.\n    This runs multiple threads to do the I/O; performance is at least as good\n    as pip's PackageFinder, which works in an analogous fashion.\n    \"\"\"\n\n    # These are used to deal with various Content-Encoding schemes.\n    decoders = {\n        'deflate': zlib.decompress,\n        'gzip': lambda b: gzip.GzipFile(fileobj=BytesIO(b)).read(),\n        'none': lambda b: b,\n    }\n\n    def __init__(self, url, timeout=None, num_workers=10, **kwargs):\n        \"\"\"\n        Initialise an instance.\n        :param url: The root URL to use for scraping.\n        :param timeout: The timeout, in seconds, to be applied to requests.\n                        This defaults to ``None`` (no timeout specified).\n        :param num_workers: The number of worker threads you want to do I/O,\n                            This defaults to 10.\n        :param kwargs: Passed to the superclass.\n        \"\"\"\n        super(SimpleScrapingLocator, self).__init__(**kwargs)\n        self.base_url = ensure_slash(url)\n        self.timeout = timeout\n        self._page_cache = {}\n        self._seen = set()\n        self._to_fetch = queue.Queue()\n        self._bad_hosts = set()\n        self.skip_externals = False\n        self.num_workers = num_workers\n        self._lock = threading.RLock()\n        # See issue #45: we need to be resilient when the locator is used\n        # in a thread, e.g. with concurrent.futures. We can't use self._lock\n        # as it is for coordinating our internal threads - the ones created\n        # in _prepare_threads.\n        self._gplock = threading.RLock()\n        self.platform_check = False  # See issue #112\n\n    def _prepare_threads(self):\n        \"\"\"\n        Threads are created only when get_project is called, and terminate\n        before it returns. They are there primarily to parallelise I/O (i.e.\n        fetching web pages).\n        \"\"\"\n        self._threads = []\n        for i in range(self.num_workers):\n            t = threading.Thread(target=self._fetch)\n            t.daemon = True\n            t.start()\n            self._threads.append(t)\n\n    def _wait_threads(self):\n        \"\"\"\n        Tell all the threads to terminate (by sending a sentinel value) and\n        wait for them to do so.\n        \"\"\"\n        # Note that you need two loops, since you can't say which\n        # thread will get each sentinel\n        for t in self._threads:\n            self._to_fetch.put(None)  # sentinel\n        for t in self._threads:\n            t.join()\n        self._threads = []\n\n    def _get_project(self, name):\n        result = {'urls': {}, 'digests': {}}\n        with self._gplock:\n            self.result = result\n            self.project_name = name\n            url = urljoin(self.base_url, '%s/' % quote(name))\n            self._seen.clear()\n            self._page_cache.clear()\n            self._prepare_threads()\n            try:\n                logger.debug('Queueing %s', url)\n                self._to_fetch.put(url)\n                self._to_fetch.join()\n            finally:\n                self._wait_threads()\n            del self.result\n        return result\n\n    platform_dependent = re.compile(r'\\b(linux_(i\\d86|x86_64|arm\\w+)|'\n                                    r'win(32|_amd64)|macosx_?\\d+)\\b', re.I)\n\n    def _is_platform_dependent(self, url):\n        \"\"\"\n        Does an URL refer to a platform-specific download?\n        \"\"\"\n        return self.platform_dependent.search(url)\n\n    def _process_download(self, url):\n        \"\"\"\n        See if an URL is a suitable download for a project.\n\n        If it is, register information in the result dictionary (for\n        _get_project) about the specific version it's for.\n\n        Note that the return value isn't actually used other than as a boolean\n        value.\n        \"\"\"\n        if self.platform_check and self._is_platform_dependent(url):\n            info = None\n        else:\n            info = self.convert_url_to_download_info(url, self.project_name)\n        logger.debug('process_download: %s -> %s', url, info)\n        if info:\n            with self._lock:  # needed because self.result is shared\n                self._update_version_data(self.result, info)\n        return info\n\n    def _should_queue(self, link, referrer, rel):\n        \"\"\"\n        Determine whether a link URL from a referring page and with a\n        particular \"rel\" attribute should be queued for scraping.\n        \"\"\"\n        scheme, netloc, path, _, _, _ = urlparse(link)\n        if path.endswith(self.source_extensions + self.binary_extensions + self.excluded_extensions):\n            result = False\n        elif self.skip_externals and not link.startswith(self.base_url):\n            result = False\n        elif not referrer.startswith(self.base_url):\n            result = False\n        elif rel not in ('homepage', 'download'):\n            result = False\n        elif scheme not in ('http', 'https', 'ftp'):\n            result = False\n        elif self._is_platform_dependent(link):\n            result = False\n        else:\n            host = netloc.split(':', 1)[0]\n            if host.lower() == 'localhost':\n                result = False\n            else:\n                result = True\n        logger.debug('should_queue: %s (%s) from %s -> %s', link, rel, referrer, result)\n        return result\n\n    def _fetch(self):\n        \"\"\"\n        Get a URL to fetch from the work queue, get the HTML page, examine its\n        links for download candidates and candidates for further scraping.\n\n        This is a handy method to run in a thread.\n        \"\"\"\n        while True:\n            url = self._to_fetch.get()\n            try:\n                if url:\n                    page = self.get_page(url)\n                    if page is None:  # e.g. after an error\n                        continue\n                    for link, rel in page.links:\n                        if link not in self._seen:\n                            try:\n                                self._seen.add(link)\n                                if (not self._process_download(link) and self._should_queue(link, url, rel)):\n                                    logger.debug('Queueing %s from %s', link, url)\n                                    self._to_fetch.put(link)\n                            except MetadataInvalidError:  # e.g. invalid versions\n                                pass\n            except Exception as e:  # pragma: no cover\n                self.errors.put(text_type(e))\n            finally:\n                # always do this, to avoid hangs :-)\n                self._to_fetch.task_done()\n            if not url:\n                # logger.debug('Sentinel seen, quitting.')\n                break\n\n    def get_page(self, url):\n        \"\"\"\n        Get the HTML for an URL, possibly from an in-memory cache.\n\n        XXX TODO Note: this cache is never actually cleared. It's assumed that\n        the data won't get stale over the lifetime of a locator instance (not\n        necessarily true for the default_locator).\n        \"\"\"\n        # http://peak.telecommunity.com/DevCenter/EasyInstall#package-index-api\n        scheme, netloc, path, _, _, _ = urlparse(url)\n        if scheme == 'file' and os.path.isdir(url2pathname(path)):\n            url = urljoin(ensure_slash(url), 'index.html')\n\n        if url in self._page_cache:\n            result = self._page_cache[url]\n            logger.debug('Returning %s from cache: %s', url, result)\n        else:\n            host = netloc.split(':', 1)[0]\n            result = None\n            if host in self._bad_hosts:\n                logger.debug('Skipping %s due to bad host %s', url, host)\n            else:\n                req = Request(url, headers={'Accept-encoding': 'identity'})\n                try:\n                    logger.debug('Fetching %s', url)\n                    resp = self.opener.open(req, timeout=self.timeout)\n                    logger.debug('Fetched %s', url)\n                    headers = resp.info()\n                    content_type = headers.get('Content-Type', '')\n                    if HTML_CONTENT_TYPE.match(content_type):\n                        final_url = resp.geturl()\n                        data = resp.read()\n                        encoding = headers.get('Content-Encoding')\n                        if encoding:\n                            decoder = self.decoders[encoding]  # fail if not found\n                            data = decoder(data)\n                        encoding = 'utf-8'\n                        m = CHARSET.search(content_type)\n                        if m:\n                            encoding = m.group(1)\n                        try:\n                            data = data.decode(encoding)\n                        except UnicodeError:  # pragma: no cover\n                            data = data.decode('latin-1')  # fallback\n                        result = Page(data, final_url)\n                        self._page_cache[final_url] = result\n                except HTTPError as e:\n                    if e.code != 404:\n                        logger.exception('Fetch failed: %s: %s', url, e)\n                except URLError as e:  # pragma: no cover\n                    logger.exception('Fetch failed: %s: %s', url, e)\n                    with self._lock:\n                        self._bad_hosts.add(host)\n                except Exception as e:  # pragma: no cover\n                    logger.exception('Fetch failed: %s: %s', url, e)\n                finally:\n                    self._page_cache[url] = result  # even if None (failure)\n        return result\n\n    _distname_re = re.compile('<a href=[^>]*>([^<]+)<')\n\n    def get_distribution_names(self):\n        \"\"\"\n        Return all the distribution names known to this locator.\n        \"\"\"\n        result = set()\n        page = self.get_page(self.base_url)\n        if not page:\n            raise DistlibException('Unable to get %s' % self.base_url)\n        for match in self._distname_re.finditer(page.data):\n            result.add(match.group(1))\n        return result\n\n\nclass DirectoryLocator(Locator):\n    \"\"\"\n    This class locates distributions in a directory tree.\n    \"\"\"\n\n    def __init__(self, path, **kwargs):\n        \"\"\"\n        Initialise an instance.\n        :param path: The root of the directory tree to search.\n        :param kwargs: Passed to the superclass constructor,\n                       except for:\n                       * recursive - if True (the default), subdirectories are\n                         recursed into. If False, only the top-level directory\n                         is searched,\n        \"\"\"\n        self.recursive = kwargs.pop('recursive', True)\n        super(DirectoryLocator, self).__init__(**kwargs)\n        path = os.path.abspath(path)\n        if not os.path.isdir(path):  # pragma: no cover\n            raise DistlibException('Not a directory: %r' % path)\n        self.base_dir = path\n\n    def should_include(self, filename, parent):\n        \"\"\"\n        Should a filename be considered as a candidate for a distribution\n        archive? As well as the filename, the directory which contains it\n        is provided, though not used by the current implementation.\n        \"\"\"\n        return filename.endswith(self.downloadable_extensions)\n\n    def _get_project(self, name):\n        result = {'urls': {}, 'digests': {}}\n        for root, dirs, files in os.walk(self.base_dir):\n            for fn in files:\n                if self.should_include(fn, root):\n                    fn = os.path.join(root, fn)\n                    url = urlunparse(('file', '', pathname2url(os.path.abspath(fn)), '', '', ''))\n                    info = self.convert_url_to_download_info(url, name)\n                    if info:\n                        self._update_version_data(result, info)\n            if not self.recursive:\n                break\n        return result\n\n    def get_distribution_names(self):\n        \"\"\"\n        Return all the distribution names known to this locator.\n        \"\"\"\n        result = set()\n        for root, dirs, files in os.walk(self.base_dir):\n            for fn in files:\n                if self.should_include(fn, root):\n                    fn = os.path.join(root, fn)\n                    url = urlunparse(('file', '', pathname2url(os.path.abspath(fn)), '', '', ''))\n                    info = self.convert_url_to_download_info(url, None)\n                    if info:\n                        result.add(info['name'])\n            if not self.recursive:\n                break\n        return result\n\n\nclass JSONLocator(Locator):\n    \"\"\"\n    This locator uses special extended metadata (not available on PyPI) and is\n    the basis of performant dependency resolution in distlib. Other locators\n    require archive downloads before dependencies can be determined! As you\n    might imagine, that can be slow.\n    \"\"\"\n\n    def get_distribution_names(self):\n        \"\"\"\n        Return all the distribution names known to this locator.\n        \"\"\"\n        raise NotImplementedError('Not available from this locator')\n\n    def _get_project(self, name):\n        result = {'urls': {}, 'digests': {}}\n        data = get_project_data(name)\n        if data:\n            for info in data.get('files', []):\n                if info['ptype'] != 'sdist' or info['pyversion'] != 'source':\n                    continue\n                # We don't store summary in project metadata as it makes\n                # the data bigger for no benefit during dependency\n                # resolution\n                dist = make_dist(data['name'],\n                                 info['version'],\n                                 summary=data.get('summary', 'Placeholder for summary'),\n                                 scheme=self.scheme)\n                md = dist.metadata\n                md.source_url = info['url']\n                # TODO SHA256 digest\n                if 'digest' in info and info['digest']:\n                    dist.digest = ('md5', info['digest'])\n                md.dependencies = info.get('requirements', {})\n                dist.exports = info.get('exports', {})\n                result[dist.version] = dist\n                result['urls'].setdefault(dist.version, set()).add(info['url'])\n        return result\n\n\nclass DistPathLocator(Locator):\n    \"\"\"\n    This locator finds installed distributions in a path. It can be useful for\n    adding to an :class:`AggregatingLocator`.\n    \"\"\"\n\n    def __init__(self, distpath, **kwargs):\n        \"\"\"\n        Initialise an instance.\n\n        :param distpath: A :class:`DistributionPath` instance to search.\n        \"\"\"\n        super(DistPathLocator, self).__init__(**kwargs)\n        assert isinstance(distpath, DistributionPath)\n        self.distpath = distpath\n\n    def _get_project(self, name):\n        dist = self.distpath.get_distribution(name)\n        if dist is None:\n            result = {'urls': {}, 'digests': {}}\n        else:\n            result = {\n                dist.version: dist,\n                'urls': {\n                    dist.version: set([dist.source_url])\n                },\n                'digests': {\n                    dist.version: set([None])\n                }\n            }\n        return result\n\n\nclass AggregatingLocator(Locator):\n    \"\"\"\n    This class allows you to chain and/or merge a list of locators.\n    \"\"\"\n\n    def __init__(self, *locators, **kwargs):\n        \"\"\"\n        Initialise an instance.\n\n        :param locators: The list of locators to search.\n        :param kwargs: Passed to the superclass constructor,\n                       except for:\n                       * merge - if False (the default), the first successful\n                         search from any of the locators is returned. If True,\n                         the results from all locators are merged (this can be\n                         slow).\n        \"\"\"\n        self.merge = kwargs.pop('merge', False)\n        self.locators = locators\n        super(AggregatingLocator, self).__init__(**kwargs)\n\n    def clear_cache(self):\n        super(AggregatingLocator, self).clear_cache()\n        for locator in self.locators:\n            locator.clear_cache()\n\n    def _set_scheme(self, value):\n        self._scheme = value\n        for locator in self.locators:\n            locator.scheme = value\n\n    scheme = property(Locator.scheme.fget, _set_scheme)\n\n    def _get_project(self, name):\n        result = {}\n        for locator in self.locators:\n            d = locator.get_project(name)\n            if d:\n                if self.merge:\n                    files = result.get('urls', {})\n                    digests = result.get('digests', {})\n                    # next line could overwrite result['urls'], result['digests']\n                    result.update(d)\n                    df = result.get('urls')\n                    if files and df:\n                        for k, v in files.items():\n                            if k in df:\n                                df[k] |= v\n                            else:\n                                df[k] = v\n                    dd = result.get('digests')\n                    if digests and dd:\n                        dd.update(digests)\n                else:\n                    # See issue #18. If any dists are found and we're looking\n                    # for specific constraints, we only return something if\n                    # a match is found. For example, if a DirectoryLocator\n                    # returns just foo (1.0) while we're looking for\n                    # foo (>= 2.0), we'll pretend there was nothing there so\n                    # that subsequent locators can be queried. Otherwise we\n                    # would just return foo (1.0) which would then lead to a\n                    # failure to find foo (>= 2.0), because other locators\n                    # weren't searched. Note that this only matters when\n                    # merge=False.\n                    if self.matcher is None:\n                        found = True\n                    else:\n                        found = False\n                        for k in d:\n                            if self.matcher.match(k):\n                                found = True\n                                break\n                    if found:\n                        result = d\n                        break\n        return result\n\n    def get_distribution_names(self):\n        \"\"\"\n        Return all the distribution names known to this locator.\n        \"\"\"\n        result = set()\n        for locator in self.locators:\n            try:\n                result |= locator.get_distribution_names()\n            except NotImplementedError:\n                pass\n        return result\n\n\n# We use a legacy scheme simply because most of the dists on PyPI use legacy\n# versions which don't conform to PEP 440.\ndefault_locator = AggregatingLocator(\n    # JSONLocator(), # don't use as PEP 426 is withdrawn\n    SimpleScrapingLocator('https://pypi.org/simple/', timeout=3.0),\n    scheme='legacy')\n\nlocate = default_locator.locate\n\n\nclass DependencyFinder(object):\n    \"\"\"\n    Locate dependencies for distributions.\n    \"\"\"\n\n    def __init__(self, locator=None):\n        \"\"\"\n        Initialise an instance, using the specified locator\n        to locate distributions.\n        \"\"\"\n        self.locator = locator or default_locator\n        self.scheme = get_scheme(self.locator.scheme)\n\n    def add_distribution(self, dist):\n        \"\"\"\n        Add a distribution to the finder. This will update internal information\n        about who provides what.\n        :param dist: The distribution to add.\n        \"\"\"\n        logger.debug('adding distribution %s', dist)\n        name = dist.key\n        self.dists_by_name[name] = dist\n        self.dists[(name, dist.version)] = dist\n        for p in dist.provides:\n            name, version = parse_name_and_version(p)\n            logger.debug('Add to provided: %s, %s, %s', name, version, dist)\n            self.provided.setdefault(name, set()).add((version, dist))\n\n    def remove_distribution(self, dist):\n        \"\"\"\n        Remove a distribution from the finder. This will update internal\n        information about who provides what.\n        :param dist: The distribution to remove.\n        \"\"\"\n        logger.debug('removing distribution %s', dist)\n        name = dist.key\n        del self.dists_by_name[name]\n        del self.dists[(name, dist.version)]\n        for p in dist.provides:\n            name, version = parse_name_and_version(p)\n            logger.debug('Remove from provided: %s, %s, %s', name, version, dist)\n            s = self.provided[name]\n            s.remove((version, dist))\n            if not s:\n                del self.provided[name]\n\n    def get_matcher(self, reqt):\n        \"\"\"\n        Get a version matcher for a requirement.\n        :param reqt: The requirement\n        :type reqt: str\n        :return: A version matcher (an instance of\n                 :class:`distlib.version.Matcher`).\n        \"\"\"\n        try:\n            matcher = self.scheme.matcher(reqt)\n        except UnsupportedVersionError:  # pragma: no cover\n            # XXX compat-mode if cannot read the version\n            name = reqt.split()[0]\n            matcher = self.scheme.matcher(name)\n        return matcher\n\n    def find_providers(self, reqt):\n        \"\"\"\n        Find the distributions which can fulfill a requirement.\n\n        :param reqt: The requirement.\n         :type reqt: str\n        :return: A set of distribution which can fulfill the requirement.\n        \"\"\"\n        matcher = self.get_matcher(reqt)\n        name = matcher.key  # case-insensitive\n        result = set()\n        provided = self.provided\n        if name in provided:\n            for version, provider in provided[name]:\n                try:\n                    match = matcher.match(version)\n                except UnsupportedVersionError:\n                    match = False\n\n                if match:\n                    result.add(provider)\n                    break\n        return result\n\n    def try_to_replace(self, provider, other, problems):\n        \"\"\"\n        Attempt to replace one provider with another. This is typically used\n        when resolving dependencies from multiple sources, e.g. A requires\n        (B >= 1.0) while C requires (B >= 1.1).\n\n        For successful replacement, ``provider`` must meet all the requirements\n        which ``other`` fulfills.\n\n        :param provider: The provider we are trying to replace with.\n        :param other: The provider we're trying to replace.\n        :param problems: If False is returned, this will contain what\n                         problems prevented replacement. This is currently\n                         a tuple of the literal string 'cantreplace',\n                         ``provider``, ``other``  and the set of requirements\n                         that ``provider`` couldn't fulfill.\n        :return: True if we can replace ``other`` with ``provider``, else\n                 False.\n        \"\"\"\n        rlist = self.reqts[other]\n        unmatched = set()\n        for s in rlist:\n            matcher = self.get_matcher(s)\n            if not matcher.match(provider.version):\n                unmatched.add(s)\n        if unmatched:\n            # can't replace other with provider\n            problems.add(('cantreplace', provider, other, frozenset(unmatched)))\n            result = False\n        else:\n            # can replace other with provider\n            self.remove_distribution(other)\n            del self.reqts[other]\n            for s in rlist:\n                self.reqts.setdefault(provider, set()).add(s)\n            self.add_distribution(provider)\n            result = True\n        return result\n\n    def find(self, requirement, meta_extras=None, prereleases=False):\n        \"\"\"\n        Find a distribution and all distributions it depends on.\n\n        :param requirement: The requirement specifying the distribution to\n                            find, or a Distribution instance.\n        :param meta_extras: A list of meta extras such as :test:, :build: and\n                            so on.\n        :param prereleases: If ``True``, allow pre-release versions to be\n                            returned - otherwise, don't return prereleases\n                            unless they're all that's available.\n\n        Return a set of :class:`Distribution` instances and a set of\n        problems.\n\n        The distributions returned should be such that they have the\n        :attr:`required` attribute set to ``True`` if they were\n        from the ``requirement`` passed to ``find()``, and they have the\n        :attr:`build_time_dependency` attribute set to ``True`` unless they\n        are post-installation dependencies of the ``requirement``.\n\n        The problems should be a tuple consisting of the string\n        ``'unsatisfied'`` and the requirement which couldn't be satisfied\n        by any distribution known to the locator.\n        \"\"\"\n\n        self.provided = {}\n        self.dists = {}\n        self.dists_by_name = {}\n        self.reqts = {}\n\n        meta_extras = set(meta_extras or [])\n        if ':*:' in meta_extras:\n            meta_extras.remove(':*:')\n            # :meta: and :run: are implicitly included\n            meta_extras |= set([':test:', ':build:', ':dev:'])\n\n        if isinstance(requirement, Distribution):\n            dist = odist = requirement\n            logger.debug('passed %s as requirement', odist)\n        else:\n            dist = odist = self.locator.locate(requirement, prereleases=prereleases)\n            if dist is None:\n                raise DistlibException('Unable to locate %r' % requirement)\n            logger.debug('located %s', odist)\n        dist.requested = True\n        problems = set()\n        todo = set([dist])\n        install_dists = set([odist])\n        while todo:\n            dist = todo.pop()\n            name = dist.key  # case-insensitive\n            if name not in self.dists_by_name:\n                self.add_distribution(dist)\n            else:\n                # import pdb; pdb.set_trace()\n                other = self.dists_by_name[name]\n                if other != dist:\n                    self.try_to_replace(dist, other, problems)\n\n            ireqts = dist.run_requires | dist.meta_requires\n            sreqts = dist.build_requires\n            ereqts = set()\n            if meta_extras and dist in install_dists:\n                for key in ('test', 'build', 'dev'):\n                    e = ':%s:' % key\n                    if e in meta_extras:\n                        ereqts |= getattr(dist, '%s_requires' % key)\n            all_reqts = ireqts | sreqts | ereqts\n            for r in all_reqts:\n                providers = self.find_providers(r)\n                if not providers:\n                    logger.debug('No providers found for %r', r)\n                    provider = self.locator.locate(r, prereleases=prereleases)\n                    # If no provider is found and we didn't consider\n                    # prereleases, consider them now.\n                    if provider is None and not prereleases:\n                        provider = self.locator.locate(r, prereleases=True)\n                    if provider is None:\n                        logger.debug('Cannot satisfy %r', r)\n                        problems.add(('unsatisfied', r))\n                    else:\n                        n, v = provider.key, provider.version\n                        if (n, v) not in self.dists:\n                            todo.add(provider)\n                        providers.add(provider)\n                        if r in ireqts and dist in install_dists:\n                            install_dists.add(provider)\n                            logger.debug('Adding %s to install_dists', provider.name_and_version)\n                for p in providers:\n                    name = p.key\n                    if name not in self.dists_by_name:\n                        self.reqts.setdefault(p, set()).add(r)\n                    else:\n                        other = self.dists_by_name[name]\n                        if other != p:\n                            # see if other can be replaced by p\n                            self.try_to_replace(p, other, problems)\n\n        dists = set(self.dists.values())\n        for dist in dists:\n            dist.build_time_dependency = dist not in install_dists\n            if dist.build_time_dependency:\n                logger.debug('%s is a build-time dependency only.', dist.name_and_version)\n        logger.debug('find done for %s', odist)\n        return dists, problems\n", "distlib/index.py": "# -*- coding: utf-8 -*-\n#\n# Copyright (C) 2013-2023 Vinay Sajip.\n# Licensed to the Python Software Foundation under a contributor agreement.\n# See LICENSE.txt and CONTRIBUTORS.txt.\n#\nimport hashlib\nimport logging\nimport os\nimport shutil\nimport subprocess\nimport tempfile\ntry:\n    from threading import Thread\nexcept ImportError:  # pragma: no cover\n    from dummy_threading import Thread\n\nfrom . import DistlibException\nfrom .compat import (HTTPBasicAuthHandler, Request, HTTPPasswordMgr,\n                     urlparse, build_opener, string_types)\nfrom .util import zip_dir, ServerProxy\n\nlogger = logging.getLogger(__name__)\n\nDEFAULT_INDEX = 'https://pypi.org/pypi'\nDEFAULT_REALM = 'pypi'\n\n\nclass PackageIndex(object):\n    \"\"\"\n    This class represents a package index compatible with PyPI, the Python\n    Package Index.\n    \"\"\"\n\n    boundary = b'----------ThIs_Is_tHe_distlib_index_bouNdaRY_$'\n\n    def __init__(self, url=None):\n        \"\"\"\n        Initialise an instance.\n\n        :param url: The URL of the index. If not specified, the URL for PyPI is\n                    used.\n        \"\"\"\n        self.url = url or DEFAULT_INDEX\n        self.read_configuration()\n        scheme, netloc, path, params, query, frag = urlparse(self.url)\n        if params or query or frag or scheme not in ('http', 'https'):\n            raise DistlibException('invalid repository: %s' % self.url)\n        self.password_handler = None\n        self.ssl_verifier = None\n        self.gpg = None\n        self.gpg_home = None\n        with open(os.devnull, 'w') as sink:\n            # Use gpg by default rather than gpg2, as gpg2 insists on\n            # prompting for passwords\n            for s in ('gpg', 'gpg2'):\n                try:\n                    rc = subprocess.check_call([s, '--version'], stdout=sink,\n                                               stderr=sink)\n                    if rc == 0:\n                        self.gpg = s\n                        break\n                except OSError:\n                    pass\n\n    def _get_pypirc_command(self):\n        \"\"\"\n        Get the distutils command for interacting with PyPI configurations.\n        :return: the command.\n        \"\"\"\n        from .util import _get_pypirc_command as cmd\n        return cmd()\n\n    def read_configuration(self):\n        \"\"\"\n        Read the PyPI access configuration as supported by distutils. This populates\n        ``username``, ``password``, ``realm`` and ``url`` attributes from the\n        configuration.\n        \"\"\"\n        from .util import _load_pypirc\n        cfg = _load_pypirc(self)\n        self.username = cfg.get('username')\n        self.password = cfg.get('password')\n        self.realm = cfg.get('realm', 'pypi')\n        self.url = cfg.get('repository', self.url)\n\n    def save_configuration(self):\n        \"\"\"\n        Save the PyPI access configuration. You must have set ``username`` and\n        ``password`` attributes before calling this method.\n        \"\"\"\n        self.check_credentials()\n        from .util import _store_pypirc\n        _store_pypirc(self)\n\n    def check_credentials(self):\n        \"\"\"\n        Check that ``username`` and ``password`` have been set, and raise an\n        exception if not.\n        \"\"\"\n        if self.username is None or self.password is None:\n            raise DistlibException('username and password must be set')\n        pm = HTTPPasswordMgr()\n        _, netloc, _, _, _, _ = urlparse(self.url)\n        pm.add_password(self.realm, netloc, self.username, self.password)\n        self.password_handler = HTTPBasicAuthHandler(pm)\n\n    def register(self, metadata):  # pragma: no cover\n        \"\"\"\n        Register a distribution on PyPI, using the provided metadata.\n\n        :param metadata: A :class:`Metadata` instance defining at least a name\n                         and version number for the distribution to be\n                         registered.\n        :return: The HTTP response received from PyPI upon submission of the\n                request.\n        \"\"\"\n        self.check_credentials()\n        metadata.validate()\n        d = metadata.todict()\n        d[':action'] = 'verify'\n        request = self.encode_request(d.items(), [])\n        self.send_request(request)\n        d[':action'] = 'submit'\n        request = self.encode_request(d.items(), [])\n        return self.send_request(request)\n\n    def _reader(self, name, stream, outbuf):\n        \"\"\"\n        Thread runner for reading lines of from a subprocess into a buffer.\n\n        :param name: The logical name of the stream (used for logging only).\n        :param stream: The stream to read from. This will typically a pipe\n                       connected to the output stream of a subprocess.\n        :param outbuf: The list to append the read lines to.\n        \"\"\"\n        while True:\n            s = stream.readline()\n            if not s:\n                break\n            s = s.decode('utf-8').rstrip()\n            outbuf.append(s)\n            logger.debug('%s: %s' % (name, s))\n        stream.close()\n\n    def get_sign_command(self, filename, signer, sign_password, keystore=None):  # pragma: no cover\n        \"\"\"\n        Return a suitable command for signing a file.\n\n        :param filename: The pathname to the file to be signed.\n        :param signer: The identifier of the signer of the file.\n        :param sign_password: The passphrase for the signer's\n                              private key used for signing.\n        :param keystore: The path to a directory which contains the keys\n                         used in verification. If not specified, the\n                         instance's ``gpg_home`` attribute is used instead.\n        :return: The signing command as a list suitable to be\n                 passed to :class:`subprocess.Popen`.\n        \"\"\"\n        cmd = [self.gpg, '--status-fd', '2', '--no-tty']\n        if keystore is None:\n            keystore = self.gpg_home\n        if keystore:\n            cmd.extend(['--homedir', keystore])\n        if sign_password is not None:\n            cmd.extend(['--batch', '--passphrase-fd', '0'])\n        td = tempfile.mkdtemp()\n        sf = os.path.join(td, os.path.basename(filename) + '.asc')\n        cmd.extend(['--detach-sign', '--armor', '--local-user',\n                    signer, '--output', sf, filename])\n        logger.debug('invoking: %s', ' '.join(cmd))\n        return cmd, sf\n\n    def run_command(self, cmd, input_data=None):\n        \"\"\"\n        Run a command in a child process , passing it any input data specified.\n\n        :param cmd: The command to run.\n        :param input_data: If specified, this must be a byte string containing\n                           data to be sent to the child process.\n        :return: A tuple consisting of the subprocess' exit code, a list of\n                 lines read from the subprocess' ``stdout``, and a list of\n                 lines read from the subprocess' ``stderr``.\n        \"\"\"\n        kwargs = {\n            'stdout': subprocess.PIPE,\n            'stderr': subprocess.PIPE,\n        }\n        if input_data is not None:\n            kwargs['stdin'] = subprocess.PIPE\n        stdout = []\n        stderr = []\n        p = subprocess.Popen(cmd, **kwargs)\n        # We don't use communicate() here because we may need to\n        # get clever with interacting with the command\n        t1 = Thread(target=self._reader, args=('stdout', p.stdout, stdout))\n        t1.start()\n        t2 = Thread(target=self._reader, args=('stderr', p.stderr, stderr))\n        t2.start()\n        if input_data is not None:\n            p.stdin.write(input_data)\n            p.stdin.close()\n\n        p.wait()\n        t1.join()\n        t2.join()\n        return p.returncode, stdout, stderr\n\n    def sign_file(self, filename, signer, sign_password, keystore=None):  # pragma: no cover\n        \"\"\"\n        Sign a file.\n\n        :param filename: The pathname to the file to be signed.\n        :param signer: The identifier of the signer of the file.\n        :param sign_password: The passphrase for the signer's\n                              private key used for signing.\n        :param keystore: The path to a directory which contains the keys\n                         used in signing. If not specified, the instance's\n                         ``gpg_home`` attribute is used instead.\n        :return: The absolute pathname of the file where the signature is\n                 stored.\n        \"\"\"\n        cmd, sig_file = self.get_sign_command(filename, signer, sign_password,\n                                              keystore)\n        rc, stdout, stderr = self.run_command(cmd,\n                                              sign_password.encode('utf-8'))\n        if rc != 0:\n            raise DistlibException('sign command failed with error '\n                                   'code %s' % rc)\n        return sig_file\n\n    def upload_file(self, metadata, filename, signer=None, sign_password=None,\n                    filetype='sdist', pyversion='source', keystore=None):\n        \"\"\"\n        Upload a release file to the index.\n\n        :param metadata: A :class:`Metadata` instance defining at least a name\n                         and version number for the file to be uploaded.\n        :param filename: The pathname of the file to be uploaded.\n        :param signer: The identifier of the signer of the file.\n        :param sign_password: The passphrase for the signer's\n                              private key used for signing.\n        :param filetype: The type of the file being uploaded. This is the\n                        distutils command which produced that file, e.g.\n                        ``sdist`` or ``bdist_wheel``.\n        :param pyversion: The version of Python which the release relates\n                          to. For code compatible with any Python, this would\n                          be ``source``, otherwise it would be e.g. ``3.2``.\n        :param keystore: The path to a directory which contains the keys\n                         used in signing. If not specified, the instance's\n                         ``gpg_home`` attribute is used instead.\n        :return: The HTTP response received from PyPI upon submission of the\n                request.\n        \"\"\"\n        self.check_credentials()\n        if not os.path.exists(filename):\n            raise DistlibException('not found: %s' % filename)\n        metadata.validate()\n        d = metadata.todict()\n        sig_file = None\n        if signer:\n            if not self.gpg:\n                logger.warning('no signing program available - not signed')\n            else:\n                sig_file = self.sign_file(filename, signer, sign_password,\n                                          keystore)\n        with open(filename, 'rb') as f:\n            file_data = f.read()\n        md5_digest = hashlib.md5(file_data).hexdigest()\n        sha256_digest = hashlib.sha256(file_data).hexdigest()\n        d.update({\n            ':action': 'file_upload',\n            'protocol_version': '1',\n            'filetype': filetype,\n            'pyversion': pyversion,\n            'md5_digest': md5_digest,\n            'sha256_digest': sha256_digest,\n        })\n        files = [('content', os.path.basename(filename), file_data)]\n        if sig_file:\n            with open(sig_file, 'rb') as f:\n                sig_data = f.read()\n            files.append(('gpg_signature', os.path.basename(sig_file),\n                         sig_data))\n            shutil.rmtree(os.path.dirname(sig_file))\n        request = self.encode_request(d.items(), files)\n        return self.send_request(request)\n\n    def upload_documentation(self, metadata, doc_dir):  # pragma: no cover\n        \"\"\"\n        Upload documentation to the index.\n\n        :param metadata: A :class:`Metadata` instance defining at least a name\n                         and version number for the documentation to be\n                         uploaded.\n        :param doc_dir: The pathname of the directory which contains the\n                        documentation. This should be the directory that\n                        contains the ``index.html`` for the documentation.\n        :return: The HTTP response received from PyPI upon submission of the\n                request.\n        \"\"\"\n        self.check_credentials()\n        if not os.path.isdir(doc_dir):\n            raise DistlibException('not a directory: %r' % doc_dir)\n        fn = os.path.join(doc_dir, 'index.html')\n        if not os.path.exists(fn):\n            raise DistlibException('not found: %r' % fn)\n        metadata.validate()\n        name, version = metadata.name, metadata.version\n        zip_data = zip_dir(doc_dir).getvalue()\n        fields = [(':action', 'doc_upload'),\n                  ('name', name), ('version', version)]\n        files = [('content', name, zip_data)]\n        request = self.encode_request(fields, files)\n        return self.send_request(request)\n\n    def get_verify_command(self, signature_filename, data_filename,\n                           keystore=None):\n        \"\"\"\n        Return a suitable command for verifying a file.\n\n        :param signature_filename: The pathname to the file containing the\n                                   signature.\n        :param data_filename: The pathname to the file containing the\n                              signed data.\n        :param keystore: The path to a directory which contains the keys\n                         used in verification. If not specified, the\n                         instance's ``gpg_home`` attribute is used instead.\n        :return: The verifying command as a list suitable to be\n                 passed to :class:`subprocess.Popen`.\n        \"\"\"\n        cmd = [self.gpg, '--status-fd', '2', '--no-tty']\n        if keystore is None:\n            keystore = self.gpg_home\n        if keystore:\n            cmd.extend(['--homedir', keystore])\n        cmd.extend(['--verify', signature_filename, data_filename])\n        logger.debug('invoking: %s', ' '.join(cmd))\n        return cmd\n\n    def verify_signature(self, signature_filename, data_filename,\n                         keystore=None):\n        \"\"\"\n        Verify a signature for a file.\n\n        :param signature_filename: The pathname to the file containing the\n                                   signature.\n        :param data_filename: The pathname to the file containing the\n                              signed data.\n        :param keystore: The path to a directory which contains the keys\n                         used in verification. If not specified, the\n                         instance's ``gpg_home`` attribute is used instead.\n        :return: True if the signature was verified, else False.\n        \"\"\"\n        if not self.gpg:\n            raise DistlibException('verification unavailable because gpg '\n                                   'unavailable')\n        cmd = self.get_verify_command(signature_filename, data_filename,\n                                      keystore)\n        rc, stdout, stderr = self.run_command(cmd)\n        if rc not in (0, 1):\n            raise DistlibException('verify command failed with error code %s' % rc)\n        return rc == 0\n\n    def download_file(self, url, destfile, digest=None, reporthook=None):\n        \"\"\"\n        This is a convenience method for downloading a file from an URL.\n        Normally, this will be a file from the index, though currently\n        no check is made for this (i.e. a file can be downloaded from\n        anywhere).\n\n        The method is just like the :func:`urlretrieve` function in the\n        standard library, except that it allows digest computation to be\n        done during download and checking that the downloaded data\n        matched any expected value.\n\n        :param url: The URL of the file to be downloaded (assumed to be\n                    available via an HTTP GET request).\n        :param destfile: The pathname where the downloaded file is to be\n                         saved.\n        :param digest: If specified, this must be a (hasher, value)\n                       tuple, where hasher is the algorithm used (e.g.\n                       ``'md5'``) and ``value`` is the expected value.\n        :param reporthook: The same as for :func:`urlretrieve` in the\n                           standard library.\n        \"\"\"\n        if digest is None:\n            digester = None\n            logger.debug('No digest specified')\n        else:\n            if isinstance(digest, (list, tuple)):\n                hasher, digest = digest\n            else:\n                hasher = 'md5'\n            digester = getattr(hashlib, hasher)()\n            logger.debug('Digest specified: %s' % digest)\n        # The following code is equivalent to urlretrieve.\n        # We need to do it this way so that we can compute the\n        # digest of the file as we go.\n        with open(destfile, 'wb') as dfp:\n            # addinfourl is not a context manager on 2.x\n            # so we have to use try/finally\n            sfp = self.send_request(Request(url))\n            try:\n                headers = sfp.info()\n                blocksize = 8192\n                size = -1\n                read = 0\n                blocknum = 0\n                if \"content-length\" in headers:\n                    size = int(headers[\"Content-Length\"])\n                if reporthook:\n                    reporthook(blocknum, blocksize, size)\n                while True:\n                    block = sfp.read(blocksize)\n                    if not block:\n                        break\n                    read += len(block)\n                    dfp.write(block)\n                    if digester:\n                        digester.update(block)\n                    blocknum += 1\n                    if reporthook:\n                        reporthook(blocknum, blocksize, size)\n            finally:\n                sfp.close()\n\n        # check that we got the whole file, if we can\n        if size >= 0 and read < size:\n            raise DistlibException(\n                'retrieval incomplete: got only %d out of %d bytes'\n                % (read, size))\n        # if we have a digest, it must match.\n        if digester:\n            actual = digester.hexdigest()\n            if digest != actual:\n                raise DistlibException('%s digest mismatch for %s: expected '\n                                       '%s, got %s' % (hasher, destfile,\n                                                       digest, actual))\n            logger.debug('Digest verified: %s', digest)\n\n    def send_request(self, req):\n        \"\"\"\n        Send a standard library :class:`Request` to PyPI and return its\n        response.\n\n        :param req: The request to send.\n        :return: The HTTP response from PyPI (a standard library HTTPResponse).\n        \"\"\"\n        handlers = []\n        if self.password_handler:\n            handlers.append(self.password_handler)\n        if self.ssl_verifier:\n            handlers.append(self.ssl_verifier)\n        opener = build_opener(*handlers)\n        return opener.open(req)\n\n    def encode_request(self, fields, files):\n        \"\"\"\n        Encode fields and files for posting to an HTTP server.\n\n        :param fields: The fields to send as a list of (fieldname, value)\n                       tuples.\n        :param files: The files to send as a list of (fieldname, filename,\n                      file_bytes) tuple.\n        \"\"\"\n        # Adapted from packaging, which in turn was adapted from\n        # http://code.activestate.com/recipes/146306\n\n        parts = []\n        boundary = self.boundary\n        for k, values in fields:\n            if not isinstance(values, (list, tuple)):\n                values = [values]\n\n            for v in values:\n                parts.extend((\n                    b'--' + boundary,\n                    ('Content-Disposition: form-data; name=\"%s\"' %\n                     k).encode('utf-8'),\n                    b'',\n                    v.encode('utf-8')))\n        for key, filename, value in files:\n            parts.extend((\n                b'--' + boundary,\n                ('Content-Disposition: form-data; name=\"%s\"; filename=\"%s\"' %\n                 (key, filename)).encode('utf-8'),\n                b'',\n                value))\n\n        parts.extend((b'--' + boundary + b'--', b''))\n\n        body = b'\\r\\n'.join(parts)\n        ct = b'multipart/form-data; boundary=' + boundary\n        headers = {\n            'Content-type': ct,\n            'Content-length': str(len(body))\n        }\n        return Request(self.url, body, headers)\n\n    def search(self, terms, operator=None):  # pragma: no cover\n        if isinstance(terms, string_types):\n            terms = {'name': terms}\n        rpc_proxy = ServerProxy(self.url, timeout=3.0)\n        try:\n            return rpc_proxy.search(terms, operator or 'and')\n        finally:\n            rpc_proxy('close')()\n", "distlib/scripts.py": "# -*- coding: utf-8 -*-\n#\n# Copyright (C) 2013-2023 Vinay Sajip.\n# Licensed to the Python Software Foundation under a contributor agreement.\n# See LICENSE.txt and CONTRIBUTORS.txt.\n#\nfrom io import BytesIO\nimport logging\nimport os\nimport re\nimport struct\nimport sys\nimport time\nfrom zipfile import ZipInfo\n\nfrom .compat import sysconfig, detect_encoding, ZipFile\nfrom .resources import finder\nfrom .util import (FileOperator, get_export_entry, convert_path, get_executable, get_platform, in_venv)\n\nlogger = logging.getLogger(__name__)\n\n_DEFAULT_MANIFEST = '''\n<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"yes\"?>\n<assembly xmlns=\"urn:schemas-microsoft-com:asm.v1\" manifestVersion=\"1.0\">\n <assemblyIdentity version=\"1.0.0.0\"\n processorArchitecture=\"X86\"\n name=\"%s\"\n type=\"win32\"/>\n\n <!-- Identify the application security requirements. -->\n <trustInfo xmlns=\"urn:schemas-microsoft-com:asm.v3\">\n <security>\n <requestedPrivileges>\n <requestedExecutionLevel level=\"asInvoker\" uiAccess=\"false\"/>\n </requestedPrivileges>\n </security>\n </trustInfo>\n</assembly>'''.strip()\n\n# check if Python is called on the first line with this expression\nFIRST_LINE_RE = re.compile(b'^#!.*pythonw?[0-9.]*([ \\t].*)?$')\nSCRIPT_TEMPLATE = r'''# -*- coding: utf-8 -*-\nimport re\nimport sys\nfrom %(module)s import %(import_name)s\nif __name__ == '__main__':\n    sys.argv[0] = re.sub(r'(-script\\.pyw|\\.exe)?$', '', sys.argv[0])\n    sys.exit(%(func)s())\n'''\n\n# Pre-fetch the contents of all executable wrapper stubs.\n# This is to address https://github.com/pypa/pip/issues/12666.\n# When updating pip, we rename the old pip in place before installing the\n# new version. If we try to fetch a wrapper *after* that rename, the finder\n# machinery will be confused as the package is no longer available at the\n# location where it was imported from. So we load everything into memory in\n# advance.\n\nif os.name == 'nt' or (os.name == 'java' and os._name == 'nt'):\n    # Issue 31: don't hardcode an absolute package name, but\n    # determine it relative to the current package\n    DISTLIB_PACKAGE = __name__.rsplit('.', 1)[0]\n\n    WRAPPERS = {\n        r.name: r.bytes\n        for r in finder(DISTLIB_PACKAGE).iterator(\"\")\n        if r.name.endswith(\".exe\")\n    }\n\n\ndef enquote_executable(executable):\n    if ' ' in executable:\n        # make sure we quote only the executable in case of env\n        # for example /usr/bin/env \"/dir with spaces/bin/jython\"\n        # instead of \"/usr/bin/env /dir with spaces/bin/jython\"\n        # otherwise whole\n        if executable.startswith('/usr/bin/env '):\n            env, _executable = executable.split(' ', 1)\n            if ' ' in _executable and not _executable.startswith('\"'):\n                executable = '%s \"%s\"' % (env, _executable)\n        else:\n            if not executable.startswith('\"'):\n                executable = '\"%s\"' % executable\n    return executable\n\n\n# Keep the old name around (for now), as there is at least one project using it!\n_enquote_executable = enquote_executable\n\n\nclass ScriptMaker(object):\n    \"\"\"\n    A class to copy or create scripts from source scripts or callable\n    specifications.\n    \"\"\"\n    script_template = SCRIPT_TEMPLATE\n\n    executable = None  # for shebangs\n\n    def __init__(self, source_dir, target_dir, add_launchers=True, dry_run=False, fileop=None):\n        self.source_dir = source_dir\n        self.target_dir = target_dir\n        self.add_launchers = add_launchers\n        self.force = False\n        self.clobber = False\n        # It only makes sense to set mode bits on POSIX.\n        self.set_mode = (os.name == 'posix') or (os.name == 'java' and os._name == 'posix')\n        self.variants = set(('', 'X.Y'))\n        self._fileop = fileop or FileOperator(dry_run)\n\n        self._is_nt = os.name == 'nt' or (os.name == 'java' and os._name == 'nt')\n        self.version_info = sys.version_info\n\n    def _get_alternate_executable(self, executable, options):\n        if options.get('gui', False) and self._is_nt:  # pragma: no cover\n            dn, fn = os.path.split(executable)\n            fn = fn.replace('python', 'pythonw')\n            executable = os.path.join(dn, fn)\n        return executable\n\n    if sys.platform.startswith('java'):  # pragma: no cover\n\n        def _is_shell(self, executable):\n            \"\"\"\n            Determine if the specified executable is a script\n            (contains a #! line)\n            \"\"\"\n            try:\n                with open(executable) as fp:\n                    return fp.read(2) == '#!'\n            except (OSError, IOError):\n                logger.warning('Failed to open %s', executable)\n                return False\n\n        def _fix_jython_executable(self, executable):\n            if self._is_shell(executable):\n                # Workaround for Jython is not needed on Linux systems.\n                import java\n\n                if java.lang.System.getProperty('os.name') == 'Linux':\n                    return executable\n            elif executable.lower().endswith('jython.exe'):\n                # Use wrapper exe for Jython on Windows\n                return executable\n            return '/usr/bin/env %s' % executable\n\n    def _build_shebang(self, executable, post_interp):\n        \"\"\"\n        Build a shebang line. In the simple case (on Windows, or a shebang line\n        which is not too long or contains spaces) use a simple formulation for\n        the shebang. Otherwise, use /bin/sh as the executable, with a contrived\n        shebang which allows the script to run either under Python or sh, using\n        suitable quoting. Thanks to Harald Nordgren for his input.\n\n        See also: http://www.in-ulm.de/~mascheck/various/shebang/#length\n                  https://hg.mozilla.org/mozilla-central/file/tip/mach\n        \"\"\"\n        if os.name != 'posix':\n            simple_shebang = True\n        else:\n            # Add 3 for '#!' prefix and newline suffix.\n            shebang_length = len(executable) + len(post_interp) + 3\n            if sys.platform == 'darwin':\n                max_shebang_length = 512\n            else:\n                max_shebang_length = 127\n            simple_shebang = ((b' ' not in executable) and (shebang_length <= max_shebang_length))\n\n        if simple_shebang:\n            result = b'#!' + executable + post_interp + b'\\n'\n        else:\n            result = b'#!/bin/sh\\n'\n            result += b\"'''exec' \" + executable + post_interp + b' \"$0\" \"$@\"\\n'\n            result += b\"' '''\"\n        return result\n\n    def _get_shebang(self, encoding, post_interp=b'', options=None):\n        enquote = True\n        if self.executable:\n            executable = self.executable\n            enquote = False  # assume this will be taken care of\n        elif not sysconfig.is_python_build():\n            executable = get_executable()\n        elif in_venv():  # pragma: no cover\n            executable = os.path.join(sysconfig.get_path('scripts'), 'python%s' % sysconfig.get_config_var('EXE'))\n        else:  # pragma: no cover\n            if os.name == 'nt':\n                # for Python builds from source on Windows, no Python executables with\n                # a version suffix are created, so we use python.exe\n                executable = os.path.join(sysconfig.get_config_var('BINDIR'),\n                                          'python%s' % (sysconfig.get_config_var('EXE')))\n            else:\n                executable = os.path.join(\n                    sysconfig.get_config_var('BINDIR'),\n                    'python%s%s' % (sysconfig.get_config_var('VERSION'), sysconfig.get_config_var('EXE')))\n        if options:\n            executable = self._get_alternate_executable(executable, options)\n\n        if sys.platform.startswith('java'):  # pragma: no cover\n            executable = self._fix_jython_executable(executable)\n\n        # Normalise case for Windows - COMMENTED OUT\n        # executable = os.path.normcase(executable)\n        # N.B. The normalising operation above has been commented out: See\n        # issue #124. Although paths in Windows are generally case-insensitive,\n        # they aren't always. For example, a path containing a \u1e9e (which is a\n        # LATIN CAPITAL LETTER SHARP S - U+1E9E) is normcased to \u00df (which is a\n        # LATIN SMALL LETTER SHARP S' - U+00DF). The two are not considered by\n        # Windows as equivalent in path names.\n\n        # If the user didn't specify an executable, it may be necessary to\n        # cater for executable paths with spaces (not uncommon on Windows)\n        if enquote:\n            executable = enquote_executable(executable)\n        # Issue #51: don't use fsencode, since we later try to\n        # check that the shebang is decodable using utf-8.\n        executable = executable.encode('utf-8')\n        # in case of IronPython, play safe and enable frames support\n        if (sys.platform == 'cli' and '-X:Frames' not in post_interp and\n                '-X:FullFrames' not in post_interp):  # pragma: no cover\n            post_interp += b' -X:Frames'\n        shebang = self._build_shebang(executable, post_interp)\n        # Python parser starts to read a script using UTF-8 until\n        # it gets a #coding:xxx cookie. The shebang has to be the\n        # first line of a file, the #coding:xxx cookie cannot be\n        # written before. So the shebang has to be decodable from\n        # UTF-8.\n        try:\n            shebang.decode('utf-8')\n        except UnicodeDecodeError:  # pragma: no cover\n            raise ValueError('The shebang (%r) is not decodable from utf-8' % shebang)\n        # If the script is encoded to a custom encoding (use a\n        # #coding:xxx cookie), the shebang has to be decodable from\n        # the script encoding too.\n        if encoding != 'utf-8':\n            try:\n                shebang.decode(encoding)\n            except UnicodeDecodeError:  # pragma: no cover\n                raise ValueError('The shebang (%r) is not decodable '\n                                 'from the script encoding (%r)' % (shebang, encoding))\n        return shebang\n\n    def _get_script_text(self, entry):\n        return self.script_template % dict(\n            module=entry.prefix, import_name=entry.suffix.split('.')[0], func=entry.suffix)\n\n    manifest = _DEFAULT_MANIFEST\n\n    def get_manifest(self, exename):\n        base = os.path.basename(exename)\n        return self.manifest % base\n\n    def _write_script(self, names, shebang, script_bytes, filenames, ext):\n        use_launcher = self.add_launchers and self._is_nt\n        linesep = os.linesep.encode('utf-8')\n        if not shebang.endswith(linesep):\n            shebang += linesep\n        if not use_launcher:\n            script_bytes = shebang + script_bytes\n        else:  # pragma: no cover\n            if ext == 'py':\n                launcher = self._get_launcher('t')\n            else:\n                launcher = self._get_launcher('w')\n            stream = BytesIO()\n            with ZipFile(stream, 'w') as zf:\n                source_date_epoch = os.environ.get('SOURCE_DATE_EPOCH')\n                if source_date_epoch:\n                    date_time = time.gmtime(int(source_date_epoch))[:6]\n                    zinfo = ZipInfo(filename='__main__.py', date_time=date_time)\n                    zf.writestr(zinfo, script_bytes)\n                else:\n                    zf.writestr('__main__.py', script_bytes)\n            zip_data = stream.getvalue()\n            script_bytes = launcher + shebang + zip_data\n        for name in names:\n            outname = os.path.join(self.target_dir, name)\n            if use_launcher:  # pragma: no cover\n                n, e = os.path.splitext(outname)\n                if e.startswith('.py'):\n                    outname = n\n                outname = '%s.exe' % outname\n                try:\n                    self._fileop.write_binary_file(outname, script_bytes)\n                except Exception:\n                    # Failed writing an executable - it might be in use.\n                    logger.warning('Failed to write executable - trying to '\n                                   'use .deleteme logic')\n                    dfname = '%s.deleteme' % outname\n                    if os.path.exists(dfname):\n                        os.remove(dfname)  # Not allowed to fail here\n                    os.rename(outname, dfname)  # nor here\n                    self._fileop.write_binary_file(outname, script_bytes)\n                    logger.debug('Able to replace executable using '\n                                 '.deleteme logic')\n                    try:\n                        os.remove(dfname)\n                    except Exception:\n                        pass  # still in use - ignore error\n            else:\n                if self._is_nt and not outname.endswith('.' + ext):  # pragma: no cover\n                    outname = '%s.%s' % (outname, ext)\n                if os.path.exists(outname) and not self.clobber:\n                    logger.warning('Skipping existing file %s', outname)\n                    continue\n                self._fileop.write_binary_file(outname, script_bytes)\n                if self.set_mode:\n                    self._fileop.set_executable_mode([outname])\n            filenames.append(outname)\n\n    variant_separator = '-'\n\n    def get_script_filenames(self, name):\n        result = set()\n        if '' in self.variants:\n            result.add(name)\n        if 'X' in self.variants:\n            result.add('%s%s' % (name, self.version_info[0]))\n        if 'X.Y' in self.variants:\n            result.add('%s%s%s.%s' % (name, self.variant_separator, self.version_info[0], self.version_info[1]))\n        return result\n\n    def _make_script(self, entry, filenames, options=None):\n        post_interp = b''\n        if options:\n            args = options.get('interpreter_args', [])\n            if args:\n                args = ' %s' % ' '.join(args)\n                post_interp = args.encode('utf-8')\n        shebang = self._get_shebang('utf-8', post_interp, options=options)\n        script = self._get_script_text(entry).encode('utf-8')\n        scriptnames = self.get_script_filenames(entry.name)\n        if options and options.get('gui', False):\n            ext = 'pyw'\n        else:\n            ext = 'py'\n        self._write_script(scriptnames, shebang, script, filenames, ext)\n\n    def _copy_script(self, script, filenames):\n        adjust = False\n        script = os.path.join(self.source_dir, convert_path(script))\n        outname = os.path.join(self.target_dir, os.path.basename(script))\n        if not self.force and not self._fileop.newer(script, outname):\n            logger.debug('not copying %s (up-to-date)', script)\n            return\n\n        # Always open the file, but ignore failures in dry-run mode --\n        # that way, we'll get accurate feedback if we can read the\n        # script.\n        try:\n            f = open(script, 'rb')\n        except IOError:  # pragma: no cover\n            if not self.dry_run:\n                raise\n            f = None\n        else:\n            first_line = f.readline()\n            if not first_line:  # pragma: no cover\n                logger.warning('%s is an empty file (skipping)', script)\n                return\n\n            match = FIRST_LINE_RE.match(first_line.replace(b'\\r\\n', b'\\n'))\n            if match:\n                adjust = True\n                post_interp = match.group(1) or b''\n\n        if not adjust:\n            if f:\n                f.close()\n            self._fileop.copy_file(script, outname)\n            if self.set_mode:\n                self._fileop.set_executable_mode([outname])\n            filenames.append(outname)\n        else:\n            logger.info('copying and adjusting %s -> %s', script, self.target_dir)\n            if not self._fileop.dry_run:\n                encoding, lines = detect_encoding(f.readline)\n                f.seek(0)\n                shebang = self._get_shebang(encoding, post_interp)\n                if b'pythonw' in first_line:  # pragma: no cover\n                    ext = 'pyw'\n                else:\n                    ext = 'py'\n                n = os.path.basename(outname)\n                self._write_script([n], shebang, f.read(), filenames, ext)\n            if f:\n                f.close()\n\n    @property\n    def dry_run(self):\n        return self._fileop.dry_run\n\n    @dry_run.setter\n    def dry_run(self, value):\n        self._fileop.dry_run = value\n\n    if os.name == 'nt' or (os.name == 'java' and os._name == 'nt'):  # pragma: no cover\n        # Executable launcher support.\n        # Launchers are from https://bitbucket.org/vinay.sajip/simple_launcher/\n\n        def _get_launcher(self, kind):\n            if struct.calcsize('P') == 8:  # 64-bit\n                bits = '64'\n            else:\n                bits = '32'\n            platform_suffix = '-arm' if get_platform() == 'win-arm64' else ''\n            name = '%s%s%s.exe' % (kind, bits, platform_suffix)\n            if name not in WRAPPERS:\n                msg = ('Unable to find resource %s in package %s' %\n                       (name, DISTLIB_PACKAGE))\n                raise ValueError(msg)\n            return WRAPPERS[name]\n\n    # Public API follows\n\n    def make(self, specification, options=None):\n        \"\"\"\n        Make a script.\n\n        :param specification: The specification, which is either a valid export\n                              entry specification (to make a script from a\n                              callable) or a filename (to make a script by\n                              copying from a source location).\n        :param options: A dictionary of options controlling script generation.\n        :return: A list of all absolute pathnames written to.\n        \"\"\"\n        filenames = []\n        entry = get_export_entry(specification)\n        if entry is None:\n            self._copy_script(specification, filenames)\n        else:\n            self._make_script(entry, filenames, options=options)\n        return filenames\n\n    def make_multiple(self, specifications, options=None):\n        \"\"\"\n        Take a list of specifications and make scripts from them,\n        :param specifications: A list of specifications.\n        :return: A list of all absolute pathnames written to,\n        \"\"\"\n        filenames = []\n        for specification in specifications:\n            filenames.extend(self.make(specification, options))\n        return filenames\n", "distlib/resources.py": "# -*- coding: utf-8 -*-\n#\n# Copyright (C) 2013-2017 Vinay Sajip.\n# Licensed to the Python Software Foundation under a contributor agreement.\n# See LICENSE.txt and CONTRIBUTORS.txt.\n#\nfrom __future__ import unicode_literals\n\nimport bisect\nimport io\nimport logging\nimport os\nimport pkgutil\nimport sys\nimport types\nimport zipimport\n\nfrom . import DistlibException\nfrom .util import cached_property, get_cache_base, Cache\n\nlogger = logging.getLogger(__name__)\n\n\ncache = None    # created when needed\n\n\nclass ResourceCache(Cache):\n    def __init__(self, base=None):\n        if base is None:\n            # Use native string to avoid issues on 2.x: see Python #20140.\n            base = os.path.join(get_cache_base(), str('resource-cache'))\n        super(ResourceCache, self).__init__(base)\n\n    def is_stale(self, resource, path):\n        \"\"\"\n        Is the cache stale for the given resource?\n\n        :param resource: The :class:`Resource` being cached.\n        :param path: The path of the resource in the cache.\n        :return: True if the cache is stale.\n        \"\"\"\n        # Cache invalidation is a hard problem :-)\n        return True\n\n    def get(self, resource):\n        \"\"\"\n        Get a resource into the cache,\n\n        :param resource: A :class:`Resource` instance.\n        :return: The pathname of the resource in the cache.\n        \"\"\"\n        prefix, path = resource.finder.get_cache_info(resource)\n        if prefix is None:\n            result = path\n        else:\n            result = os.path.join(self.base, self.prefix_to_dir(prefix), path)\n            dirname = os.path.dirname(result)\n            if not os.path.isdir(dirname):\n                os.makedirs(dirname)\n            if not os.path.exists(result):\n                stale = True\n            else:\n                stale = self.is_stale(resource, path)\n            if stale:\n                # write the bytes of the resource to the cache location\n                with open(result, 'wb') as f:\n                    f.write(resource.bytes)\n        return result\n\n\nclass ResourceBase(object):\n    def __init__(self, finder, name):\n        self.finder = finder\n        self.name = name\n\n\nclass Resource(ResourceBase):\n    \"\"\"\n    A class representing an in-package resource, such as a data file. This is\n    not normally instantiated by user code, but rather by a\n    :class:`ResourceFinder` which manages the resource.\n    \"\"\"\n    is_container = False        # Backwards compatibility\n\n    def as_stream(self):\n        \"\"\"\n        Get the resource as a stream.\n\n        This is not a property to make it obvious that it returns a new stream\n        each time.\n        \"\"\"\n        return self.finder.get_stream(self)\n\n    @cached_property\n    def file_path(self):\n        global cache\n        if cache is None:\n            cache = ResourceCache()\n        return cache.get(self)\n\n    @cached_property\n    def bytes(self):\n        return self.finder.get_bytes(self)\n\n    @cached_property\n    def size(self):\n        return self.finder.get_size(self)\n\n\nclass ResourceContainer(ResourceBase):\n    is_container = True     # Backwards compatibility\n\n    @cached_property\n    def resources(self):\n        return self.finder.get_resources(self)\n\n\nclass ResourceFinder(object):\n    \"\"\"\n    Resource finder for file system resources.\n    \"\"\"\n\n    if sys.platform.startswith('java'):\n        skipped_extensions = ('.pyc', '.pyo', '.class')\n    else:\n        skipped_extensions = ('.pyc', '.pyo')\n\n    def __init__(self, module):\n        self.module = module\n        self.loader = getattr(module, '__loader__', None)\n        self.base = os.path.dirname(getattr(module, '__file__', ''))\n\n    def _adjust_path(self, path):\n        return os.path.realpath(path)\n\n    def _make_path(self, resource_name):\n        # Issue #50: need to preserve type of path on Python 2.x\n        # like os.path._get_sep\n        if isinstance(resource_name, bytes):    # should only happen on 2.x\n            sep = b'/'\n        else:\n            sep = '/'\n        parts = resource_name.split(sep)\n        parts.insert(0, self.base)\n        result = os.path.join(*parts)\n        return self._adjust_path(result)\n\n    def _find(self, path):\n        return os.path.exists(path)\n\n    def get_cache_info(self, resource):\n        return None, resource.path\n\n    def find(self, resource_name):\n        path = self._make_path(resource_name)\n        if not self._find(path):\n            result = None\n        else:\n            if self._is_directory(path):\n                result = ResourceContainer(self, resource_name)\n            else:\n                result = Resource(self, resource_name)\n            result.path = path\n        return result\n\n    def get_stream(self, resource):\n        return open(resource.path, 'rb')\n\n    def get_bytes(self, resource):\n        with open(resource.path, 'rb') as f:\n            return f.read()\n\n    def get_size(self, resource):\n        return os.path.getsize(resource.path)\n\n    def get_resources(self, resource):\n        def allowed(f):\n            return (f != '__pycache__' and not\n                    f.endswith(self.skipped_extensions))\n        return set([f for f in os.listdir(resource.path) if allowed(f)])\n\n    def is_container(self, resource):\n        return self._is_directory(resource.path)\n\n    _is_directory = staticmethod(os.path.isdir)\n\n    def iterator(self, resource_name):\n        resource = self.find(resource_name)\n        if resource is not None:\n            todo = [resource]\n            while todo:\n                resource = todo.pop(0)\n                yield resource\n                if resource.is_container:\n                    rname = resource.name\n                    for name in resource.resources:\n                        if not rname:\n                            new_name = name\n                        else:\n                            new_name = '/'.join([rname, name])\n                        child = self.find(new_name)\n                        if child.is_container:\n                            todo.append(child)\n                        else:\n                            yield child\n\n\nclass ZipResourceFinder(ResourceFinder):\n    \"\"\"\n    Resource finder for resources in .zip files.\n    \"\"\"\n    def __init__(self, module):\n        super(ZipResourceFinder, self).__init__(module)\n        archive = self.loader.archive\n        self.prefix_len = 1 + len(archive)\n        # PyPy doesn't have a _files attr on zipimporter, and you can't set one\n        if hasattr(self.loader, '_files'):\n            self._files = self.loader._files\n        else:\n            self._files = zipimport._zip_directory_cache[archive]\n        self.index = sorted(self._files)\n\n    def _adjust_path(self, path):\n        return path\n\n    def _find(self, path):\n        path = path[self.prefix_len:]\n        if path in self._files:\n            result = True\n        else:\n            if path and path[-1] != os.sep:\n                path = path + os.sep\n            i = bisect.bisect(self.index, path)\n            try:\n                result = self.index[i].startswith(path)\n            except IndexError:\n                result = False\n        if not result:\n            logger.debug('_find failed: %r %r', path, self.loader.prefix)\n        else:\n            logger.debug('_find worked: %r %r', path, self.loader.prefix)\n        return result\n\n    def get_cache_info(self, resource):\n        prefix = self.loader.archive\n        path = resource.path[1 + len(prefix):]\n        return prefix, path\n\n    def get_bytes(self, resource):\n        return self.loader.get_data(resource.path)\n\n    def get_stream(self, resource):\n        return io.BytesIO(self.get_bytes(resource))\n\n    def get_size(self, resource):\n        path = resource.path[self.prefix_len:]\n        return self._files[path][3]\n\n    def get_resources(self, resource):\n        path = resource.path[self.prefix_len:]\n        if path and path[-1] != os.sep:\n            path += os.sep\n        plen = len(path)\n        result = set()\n        i = bisect.bisect(self.index, path)\n        while i < len(self.index):\n            if not self.index[i].startswith(path):\n                break\n            s = self.index[i][plen:]\n            result.add(s.split(os.sep, 1)[0])   # only immediate children\n            i += 1\n        return result\n\n    def _is_directory(self, path):\n        path = path[self.prefix_len:]\n        if path and path[-1] != os.sep:\n            path += os.sep\n        i = bisect.bisect(self.index, path)\n        try:\n            result = self.index[i].startswith(path)\n        except IndexError:\n            result = False\n        return result\n\n\n_finder_registry = {\n    type(None): ResourceFinder,\n    zipimport.zipimporter: ZipResourceFinder\n}\n\ntry:\n    # In Python 3.6, _frozen_importlib -> _frozen_importlib_external\n    try:\n        import _frozen_importlib_external as _fi\n    except ImportError:\n        import _frozen_importlib as _fi\n    _finder_registry[_fi.SourceFileLoader] = ResourceFinder\n    _finder_registry[_fi.FileFinder] = ResourceFinder\n    # See issue #146\n    _finder_registry[_fi.SourcelessFileLoader] = ResourceFinder\n    del _fi\nexcept (ImportError, AttributeError):\n    pass\n\n\ndef register_finder(loader, finder_maker):\n    _finder_registry[type(loader)] = finder_maker\n\n\n_finder_cache = {}\n\n\ndef finder(package):\n    \"\"\"\n    Return a resource finder for a package.\n    :param package: The name of the package.\n    :return: A :class:`ResourceFinder` instance for the package.\n    \"\"\"\n    if package in _finder_cache:\n        result = _finder_cache[package]\n    else:\n        if package not in sys.modules:\n            __import__(package)\n        module = sys.modules[package]\n        path = getattr(module, '__path__', None)\n        if path is None:\n            raise DistlibException('You cannot get a finder for a module, '\n                                   'only for a package')\n        loader = getattr(module, '__loader__', None)\n        finder_maker = _finder_registry.get(type(loader))\n        if finder_maker is None:\n            raise DistlibException('Unable to locate finder for %r' % package)\n        result = finder_maker(module)\n        _finder_cache[package] = result\n    return result\n\n\n_dummy_module = types.ModuleType(str('__dummy__'))\n\n\ndef finder_for_path(path):\n    \"\"\"\n    Return a resource finder for a path, which should represent a container.\n\n    :param path: The path.\n    :return: A :class:`ResourceFinder` instance for the path.\n    \"\"\"\n    result = None\n    # calls any path hooks, gets importer into cache\n    pkgutil.get_importer(path)\n    loader = sys.path_importer_cache.get(path)\n    finder = _finder_registry.get(type(loader))\n    if finder:\n        module = _dummy_module\n        module.__file__ = os.path.join(path, '')\n        module.__loader__ = loader\n        result = finder(module)\n    return result\n", "distlib/manifest.py": "# -*- coding: utf-8 -*-\n#\n# Copyright (C) 2012-2023 Python Software Foundation.\n# See LICENSE.txt and CONTRIBUTORS.txt.\n#\n\"\"\"\nClass representing the list of files in a distribution.\n\nEquivalent to distutils.filelist, but fixes some problems.\n\"\"\"\nimport fnmatch\nimport logging\nimport os\nimport re\nimport sys\n\nfrom . import DistlibException\nfrom .compat import fsdecode\nfrom .util import convert_path\n\n\n__all__ = ['Manifest']\n\nlogger = logging.getLogger(__name__)\n\n# a \\ followed by some spaces + EOL\n_COLLAPSE_PATTERN = re.compile('\\\\\\\\w*\\n', re.M)\n_COMMENTED_LINE = re.compile('#.*?(?=\\n)|\\n(?=$)', re.M | re.S)\n\n#\n# Due to the different results returned by fnmatch.translate, we need\n# to do slightly different processing for Python 2.7 and 3.2 ... this needed\n# to be brought in for Python 3.6 onwards.\n#\n_PYTHON_VERSION = sys.version_info[:2]\n\n\nclass Manifest(object):\n    \"\"\"\n    A list of files built by exploring the filesystem and filtered by applying various\n    patterns to what we find there.\n    \"\"\"\n\n    def __init__(self, base=None):\n        \"\"\"\n        Initialise an instance.\n\n        :param base: The base directory to explore under.\n        \"\"\"\n        self.base = os.path.abspath(os.path.normpath(base or os.getcwd()))\n        self.prefix = self.base + os.sep\n        self.allfiles = None\n        self.files = set()\n\n    #\n    # Public API\n    #\n\n    def findall(self):\n        \"\"\"Find all files under the base and set ``allfiles`` to the absolute\n        pathnames of files found.\n        \"\"\"\n        from stat import S_ISREG, S_ISDIR, S_ISLNK\n\n        self.allfiles = allfiles = []\n        root = self.base\n        stack = [root]\n        pop = stack.pop\n        push = stack.append\n\n        while stack:\n            root = pop()\n            names = os.listdir(root)\n\n            for name in names:\n                fullname = os.path.join(root, name)\n\n                # Avoid excess stat calls -- just one will do, thank you!\n                stat = os.stat(fullname)\n                mode = stat.st_mode\n                if S_ISREG(mode):\n                    allfiles.append(fsdecode(fullname))\n                elif S_ISDIR(mode) and not S_ISLNK(mode):\n                    push(fullname)\n\n    def add(self, item):\n        \"\"\"\n        Add a file to the manifest.\n\n        :param item: The pathname to add. This can be relative to the base.\n        \"\"\"\n        if not item.startswith(self.prefix):\n            item = os.path.join(self.base, item)\n        self.files.add(os.path.normpath(item))\n\n    def add_many(self, items):\n        \"\"\"\n        Add a list of files to the manifest.\n\n        :param items: The pathnames to add. These can be relative to the base.\n        \"\"\"\n        for item in items:\n            self.add(item)\n\n    def sorted(self, wantdirs=False):\n        \"\"\"\n        Return sorted files in directory order\n        \"\"\"\n\n        def add_dir(dirs, d):\n            dirs.add(d)\n            logger.debug('add_dir added %s', d)\n            if d != self.base:\n                parent, _ = os.path.split(d)\n                assert parent not in ('', '/')\n                add_dir(dirs, parent)\n\n        result = set(self.files)    # make a copy!\n        if wantdirs:\n            dirs = set()\n            for f in result:\n                add_dir(dirs, os.path.dirname(f))\n            result |= dirs\n        return [os.path.join(*path_tuple) for path_tuple in\n                sorted(os.path.split(path) for path in result)]\n\n    def clear(self):\n        \"\"\"Clear all collected files.\"\"\"\n        self.files = set()\n        self.allfiles = []\n\n    def process_directive(self, directive):\n        \"\"\"\n        Process a directive which either adds some files from ``allfiles`` to\n        ``files``, or removes some files from ``files``.\n\n        :param directive: The directive to process. This should be in a format\n                     compatible with distutils ``MANIFEST.in`` files:\n\n                     http://docs.python.org/distutils/sourcedist.html#commands\n        \"\"\"\n        # Parse the line: split it up, make sure the right number of words\n        # is there, and return the relevant words.  'action' is always\n        # defined: it's the first word of the line.  Which of the other\n        # three are defined depends on the action; it'll be either\n        # patterns, (dir and patterns), or (dirpattern).\n        action, patterns, thedir, dirpattern = self._parse_directive(directive)\n\n        # OK, now we know that the action is valid and we have the\n        # right number of words on the line for that action -- so we\n        # can proceed with minimal error-checking.\n        if action == 'include':\n            for pattern in patterns:\n                if not self._include_pattern(pattern, anchor=True):\n                    logger.warning('no files found matching %r', pattern)\n\n        elif action == 'exclude':\n            for pattern in patterns:\n                self._exclude_pattern(pattern, anchor=True)\n\n        elif action == 'global-include':\n            for pattern in patterns:\n                if not self._include_pattern(pattern, anchor=False):\n                    logger.warning('no files found matching %r '\n                                   'anywhere in distribution', pattern)\n\n        elif action == 'global-exclude':\n            for pattern in patterns:\n                self._exclude_pattern(pattern, anchor=False)\n\n        elif action == 'recursive-include':\n            for pattern in patterns:\n                if not self._include_pattern(pattern, prefix=thedir):\n                    logger.warning('no files found matching %r '\n                                   'under directory %r', pattern, thedir)\n\n        elif action == 'recursive-exclude':\n            for pattern in patterns:\n                self._exclude_pattern(pattern, prefix=thedir)\n\n        elif action == 'graft':\n            if not self._include_pattern(None, prefix=dirpattern):\n                logger.warning('no directories found matching %r',\n                               dirpattern)\n\n        elif action == 'prune':\n            if not self._exclude_pattern(None, prefix=dirpattern):\n                logger.warning('no previously-included directories found '\n                               'matching %r', dirpattern)\n        else:   # pragma: no cover\n            # This should never happen, as it should be caught in\n            # _parse_template_line\n            raise DistlibException(\n                'invalid action %r' % action)\n\n    #\n    # Private API\n    #\n\n    def _parse_directive(self, directive):\n        \"\"\"\n        Validate a directive.\n        :param directive: The directive to validate.\n        :return: A tuple of action, patterns, thedir, dir_patterns\n        \"\"\"\n        words = directive.split()\n        if len(words) == 1 and words[0] not in ('include', 'exclude',\n                                                'global-include',\n                                                'global-exclude',\n                                                'recursive-include',\n                                                'recursive-exclude',\n                                                'graft', 'prune'):\n            # no action given, let's use the default 'include'\n            words.insert(0, 'include')\n\n        action = words[0]\n        patterns = thedir = dir_pattern = None\n\n        if action in ('include', 'exclude',\n                      'global-include', 'global-exclude'):\n            if len(words) < 2:\n                raise DistlibException(\n                    '%r expects <pattern1> <pattern2> ...' % action)\n\n            patterns = [convert_path(word) for word in words[1:]]\n\n        elif action in ('recursive-include', 'recursive-exclude'):\n            if len(words) < 3:\n                raise DistlibException(\n                    '%r expects <dir> <pattern1> <pattern2> ...' % action)\n\n            thedir = convert_path(words[1])\n            patterns = [convert_path(word) for word in words[2:]]\n\n        elif action in ('graft', 'prune'):\n            if len(words) != 2:\n                raise DistlibException(\n                    '%r expects a single <dir_pattern>' % action)\n\n            dir_pattern = convert_path(words[1])\n\n        else:\n            raise DistlibException('unknown action %r' % action)\n\n        return action, patterns, thedir, dir_pattern\n\n    def _include_pattern(self, pattern, anchor=True, prefix=None,\n                         is_regex=False):\n        \"\"\"Select strings (presumably filenames) from 'self.files' that\n        match 'pattern', a Unix-style wildcard (glob) pattern.\n\n        Patterns are not quite the same as implemented by the 'fnmatch'\n        module: '*' and '?'  match non-special characters, where \"special\"\n        is platform-dependent: slash on Unix; colon, slash, and backslash on\n        DOS/Windows; and colon on Mac OS.\n\n        If 'anchor' is true (the default), then the pattern match is more\n        stringent: \"*.py\" will match \"foo.py\" but not \"foo/bar.py\".  If\n        'anchor' is false, both of these will match.\n\n        If 'prefix' is supplied, then only filenames starting with 'prefix'\n        (itself a pattern) and ending with 'pattern', with anything in between\n        them, will match.  'anchor' is ignored in this case.\n\n        If 'is_regex' is true, 'anchor' and 'prefix' are ignored, and\n        'pattern' is assumed to be either a string containing a regex or a\n        regex object -- no translation is done, the regex is just compiled\n        and used as-is.\n\n        Selected strings will be added to self.files.\n\n        Return True if files are found.\n        \"\"\"\n        # XXX docstring lying about what the special chars are?\n        found = False\n        pattern_re = self._translate_pattern(pattern, anchor, prefix, is_regex)\n\n        # delayed loading of allfiles list\n        if self.allfiles is None:\n            self.findall()\n\n        for name in self.allfiles:\n            if pattern_re.search(name):\n                self.files.add(name)\n                found = True\n        return found\n\n    def _exclude_pattern(self, pattern, anchor=True, prefix=None,\n                         is_regex=False):\n        \"\"\"Remove strings (presumably filenames) from 'files' that match\n        'pattern'.\n\n        Other parameters are the same as for 'include_pattern()', above.\n        The list 'self.files' is modified in place. Return True if files are\n        found.\n\n        This API is public to allow e.g. exclusion of SCM subdirs, e.g. when\n        packaging source distributions\n        \"\"\"\n        found = False\n        pattern_re = self._translate_pattern(pattern, anchor, prefix, is_regex)\n        for f in list(self.files):\n            if pattern_re.search(f):\n                self.files.remove(f)\n                found = True\n        return found\n\n    def _translate_pattern(self, pattern, anchor=True, prefix=None,\n                           is_regex=False):\n        \"\"\"Translate a shell-like wildcard pattern to a compiled regular\n        expression.\n\n        Return the compiled regex.  If 'is_regex' true,\n        then 'pattern' is directly compiled to a regex (if it's a string)\n        or just returned as-is (assumes it's a regex object).\n        \"\"\"\n        if is_regex:\n            if isinstance(pattern, str):\n                return re.compile(pattern)\n            else:\n                return pattern\n\n        if _PYTHON_VERSION > (3, 2):\n            # ditch start and end characters\n            start, _, end = self._glob_to_re('_').partition('_')\n\n        if pattern:\n            pattern_re = self._glob_to_re(pattern)\n            if _PYTHON_VERSION > (3, 2):\n                assert pattern_re.startswith(start) and pattern_re.endswith(end)\n        else:\n            pattern_re = ''\n\n        base = re.escape(os.path.join(self.base, ''))\n        if prefix is not None:\n            # ditch end of pattern character\n            if _PYTHON_VERSION <= (3, 2):\n                empty_pattern = self._glob_to_re('')\n                prefix_re = self._glob_to_re(prefix)[:-len(empty_pattern)]\n            else:\n                prefix_re = self._glob_to_re(prefix)\n                assert prefix_re.startswith(start) and prefix_re.endswith(end)\n                prefix_re = prefix_re[len(start): len(prefix_re) - len(end)]\n            sep = os.sep\n            if os.sep == '\\\\':\n                sep = r'\\\\'\n            if _PYTHON_VERSION <= (3, 2):\n                pattern_re = '^' + base + sep.join((prefix_re,\n                                                    '.*' + pattern_re))\n            else:\n                pattern_re = pattern_re[len(start): len(pattern_re) - len(end)]\n                pattern_re = r'%s%s%s%s.*%s%s' % (start, base, prefix_re, sep,\n                                                  pattern_re, end)\n        else:  # no prefix -- respect anchor flag\n            if anchor:\n                if _PYTHON_VERSION <= (3, 2):\n                    pattern_re = '^' + base + pattern_re\n                else:\n                    pattern_re = r'%s%s%s' % (start, base, pattern_re[len(start):])\n\n        return re.compile(pattern_re)\n\n    def _glob_to_re(self, pattern):\n        \"\"\"Translate a shell-like glob pattern to a regular expression.\n\n        Return a string containing the regex.  Differs from\n        'fnmatch.translate()' in that '*' does not match \"special characters\"\n        (which are platform-specific).\n        \"\"\"\n        pattern_re = fnmatch.translate(pattern)\n\n        # '?' and '*' in the glob pattern become '.' and '.*' in the RE, which\n        # IMHO is wrong -- '?' and '*' aren't supposed to match slash in Unix,\n        # and by extension they shouldn't match such \"special characters\" under\n        # any OS.  So change all non-escaped dots in the RE to match any\n        # character except the special characters (currently: just os.sep).\n        sep = os.sep\n        if os.sep == '\\\\':\n            # we're using a regex to manipulate a regex, so we need\n            # to escape the backslash twice\n            sep = r'\\\\\\\\'\n        escaped = r'\\1[^%s]' % sep\n        pattern_re = re.sub(r'((?<!\\\\)(\\\\\\\\)*)\\.', escaped, pattern_re)\n        return pattern_re\n", "distlib/version.py": "# -*- coding: utf-8 -*-\n#\n# Copyright (C) 2012-2023 The Python Software Foundation.\n# See LICENSE.txt and CONTRIBUTORS.txt.\n#\n\"\"\"\nImplementation of a flexible versioning scheme providing support for PEP-440,\nsetuptools-compatible and semantic versioning.\n\"\"\"\n\nimport logging\nimport re\n\nfrom .compat import string_types\nfrom .util import parse_requirement\n\n__all__ = ['NormalizedVersion', 'NormalizedMatcher',\n           'LegacyVersion', 'LegacyMatcher',\n           'SemanticVersion', 'SemanticMatcher',\n           'UnsupportedVersionError', 'get_scheme']\n\nlogger = logging.getLogger(__name__)\n\n\nclass UnsupportedVersionError(ValueError):\n    \"\"\"This is an unsupported version.\"\"\"\n    pass\n\n\nclass Version(object):\n    def __init__(self, s):\n        self._string = s = s.strip()\n        self._parts = parts = self.parse(s)\n        assert isinstance(parts, tuple)\n        assert len(parts) > 0\n\n    def parse(self, s):\n        raise NotImplementedError('please implement in a subclass')\n\n    def _check_compatible(self, other):\n        if type(self) != type(other):\n            raise TypeError('cannot compare %r and %r' % (self, other))\n\n    def __eq__(self, other):\n        self._check_compatible(other)\n        return self._parts == other._parts\n\n    def __ne__(self, other):\n        return not self.__eq__(other)\n\n    def __lt__(self, other):\n        self._check_compatible(other)\n        return self._parts < other._parts\n\n    def __gt__(self, other):\n        return not (self.__lt__(other) or self.__eq__(other))\n\n    def __le__(self, other):\n        return self.__lt__(other) or self.__eq__(other)\n\n    def __ge__(self, other):\n        return self.__gt__(other) or self.__eq__(other)\n\n    # See http://docs.python.org/reference/datamodel#object.__hash__\n    def __hash__(self):\n        return hash(self._parts)\n\n    def __repr__(self):\n        return \"%s('%s')\" % (self.__class__.__name__, self._string)\n\n    def __str__(self):\n        return self._string\n\n    @property\n    def is_prerelease(self):\n        raise NotImplementedError('Please implement in subclasses.')\n\n\nclass Matcher(object):\n    version_class = None\n\n    # value is either a callable or the name of a method\n    _operators = {\n        '<': lambda v, c, p: v < c,\n        '>': lambda v, c, p: v > c,\n        '<=': lambda v, c, p: v == c or v < c,\n        '>=': lambda v, c, p: v == c or v > c,\n        '==': lambda v, c, p: v == c,\n        '===': lambda v, c, p: v == c,\n        # by default, compatible => >=.\n        '~=': lambda v, c, p: v == c or v > c,\n        '!=': lambda v, c, p: v != c,\n    }\n\n    # this is a method only to support alternative implementations\n    # via overriding\n    def parse_requirement(self, s):\n        return parse_requirement(s)\n\n    def __init__(self, s):\n        if self.version_class is None:\n            raise ValueError('Please specify a version class')\n        self._string = s = s.strip()\n        r = self.parse_requirement(s)\n        if not r:\n            raise ValueError('Not valid: %r' % s)\n        self.name = r.name\n        self.key = self.name.lower()    # for case-insensitive comparisons\n        clist = []\n        if r.constraints:\n            # import pdb; pdb.set_trace()\n            for op, s in r.constraints:\n                if s.endswith('.*'):\n                    if op not in ('==', '!='):\n                        raise ValueError('\\'.*\\' not allowed for '\n                                         '%r constraints' % op)\n                    # Could be a partial version (e.g. for '2.*') which\n                    # won't parse as a version, so keep it as a string\n                    vn, prefix = s[:-2], True\n                    # Just to check that vn is a valid version\n                    self.version_class(vn)\n                else:\n                    # Should parse as a version, so we can create an\n                    # instance for the comparison\n                    vn, prefix = self.version_class(s), False\n                clist.append((op, vn, prefix))\n        self._parts = tuple(clist)\n\n    def match(self, version):\n        \"\"\"\n        Check if the provided version matches the constraints.\n\n        :param version: The version to match against this instance.\n        :type version: String or :class:`Version` instance.\n        \"\"\"\n        if isinstance(version, string_types):\n            version = self.version_class(version)\n        for operator, constraint, prefix in self._parts:\n            f = self._operators.get(operator)\n            if isinstance(f, string_types):\n                f = getattr(self, f)\n            if not f:\n                msg = ('%r not implemented '\n                       'for %s' % (operator, self.__class__.__name__))\n                raise NotImplementedError(msg)\n            if not f(version, constraint, prefix):\n                return False\n        return True\n\n    @property\n    def exact_version(self):\n        result = None\n        if len(self._parts) == 1 and self._parts[0][0] in ('==', '==='):\n            result = self._parts[0][1]\n        return result\n\n    def _check_compatible(self, other):\n        if type(self) != type(other) or self.name != other.name:\n            raise TypeError('cannot compare %s and %s' % (self, other))\n\n    def __eq__(self, other):\n        self._check_compatible(other)\n        return self.key == other.key and self._parts == other._parts\n\n    def __ne__(self, other):\n        return not self.__eq__(other)\n\n    # See http://docs.python.org/reference/datamodel#object.__hash__\n    def __hash__(self):\n        return hash(self.key) + hash(self._parts)\n\n    def __repr__(self):\n        return \"%s(%r)\" % (self.__class__.__name__, self._string)\n\n    def __str__(self):\n        return self._string\n\n\nPEP440_VERSION_RE = re.compile(r'^v?(\\d+!)?(\\d+(\\.\\d+)*)((a|alpha|b|beta|c|rc|pre|preview)(\\d+)?)?'\n                               r'(\\.(post|r|rev)(\\d+)?)?([._-]?(dev)(\\d+)?)?'\n                               r'(\\+([a-zA-Z\\d]+(\\.[a-zA-Z\\d]+)?))?$', re.I)\n\n\ndef _pep_440_key(s):\n    s = s.strip()\n    m = PEP440_VERSION_RE.match(s)\n    if not m:\n        raise UnsupportedVersionError('Not a valid version: %s' % s)\n    groups = m.groups()\n    nums = tuple(int(v) for v in groups[1].split('.'))\n    while len(nums) > 1 and nums[-1] == 0:\n        nums = nums[:-1]\n\n    if not groups[0]:\n        epoch = 0\n    else:\n        epoch = int(groups[0][:-1])\n    pre = groups[4:6]\n    post = groups[7:9]\n    dev = groups[10:12]\n    local = groups[13]\n    if pre == (None, None):\n        pre = ()\n    else:\n        if pre[1] is None:\n            pre = pre[0], 0\n        else:\n            pre = pre[0], int(pre[1])\n    if post == (None, None):\n        post = ()\n    else:\n        if post[1] is None:\n            post = post[0], 0\n        else:\n            post = post[0], int(post[1])\n    if dev == (None, None):\n        dev = ()\n    else:\n        if dev[1] is None:\n            dev = dev[0], 0\n        else:\n            dev = dev[0], int(dev[1])\n    if local is None:\n        local = ()\n    else:\n        parts = []\n        for part in local.split('.'):\n            # to ensure that numeric compares as > lexicographic, avoid\n            # comparing them directly, but encode a tuple which ensures\n            # correct sorting\n            if part.isdigit():\n                part = (1, int(part))\n            else:\n                part = (0, part)\n            parts.append(part)\n        local = tuple(parts)\n    if not pre:\n        # either before pre-release, or final release and after\n        if not post and dev:\n            # before pre-release\n            pre = ('a', -1)     # to sort before a0\n        else:\n            pre = ('z',)        # to sort after all pre-releases\n    # now look at the state of post and dev.\n    if not post:\n        post = ('_',)   # sort before 'a'\n    if not dev:\n        dev = ('final',)\n\n    return epoch, nums, pre, post, dev, local\n\n\n_normalized_key = _pep_440_key\n\n\nclass NormalizedVersion(Version):\n    \"\"\"A rational version.\n\n    Good:\n        1.2         # equivalent to \"1.2.0\"\n        1.2.0\n        1.2a1\n        1.2.3a2\n        1.2.3b1\n        1.2.3c1\n        1.2.3.4\n        TODO: fill this out\n\n    Bad:\n        1           # minimum two numbers\n        1.2a        # release level must have a release serial\n        1.2.3b\n    \"\"\"\n    def parse(self, s):\n        result = _normalized_key(s)\n        # _normalized_key loses trailing zeroes in the release\n        # clause, since that's needed to ensure that X.Y == X.Y.0 == X.Y.0.0\n        # However, PEP 440 prefix matching needs it: for example,\n        # (~= 1.4.5.0) matches differently to (~= 1.4.5.0.0).\n        m = PEP440_VERSION_RE.match(s)      # must succeed\n        groups = m.groups()\n        self._release_clause = tuple(int(v) for v in groups[1].split('.'))\n        return result\n\n    PREREL_TAGS = set(['a', 'b', 'c', 'rc', 'dev'])\n\n    @property\n    def is_prerelease(self):\n        return any(t[0] in self.PREREL_TAGS for t in self._parts if t)\n\n\ndef _match_prefix(x, y):\n    x = str(x)\n    y = str(y)\n    if x == y:\n        return True\n    if not x.startswith(y):\n        return False\n    n = len(y)\n    return x[n] == '.'\n\n\nclass NormalizedMatcher(Matcher):\n    version_class = NormalizedVersion\n\n    # value is either a callable or the name of a method\n    _operators = {\n        '~=': '_match_compatible',\n        '<': '_match_lt',\n        '>': '_match_gt',\n        '<=': '_match_le',\n        '>=': '_match_ge',\n        '==': '_match_eq',\n        '===': '_match_arbitrary',\n        '!=': '_match_ne',\n    }\n\n    def _adjust_local(self, version, constraint, prefix):\n        if prefix:\n            strip_local = '+' not in constraint and version._parts[-1]\n        else:\n            # both constraint and version are\n            # NormalizedVersion instances.\n            # If constraint does not have a local component,\n            # ensure the version doesn't, either.\n            strip_local = not constraint._parts[-1] and version._parts[-1]\n        if strip_local:\n            s = version._string.split('+', 1)[0]\n            version = self.version_class(s)\n        return version, constraint\n\n    def _match_lt(self, version, constraint, prefix):\n        version, constraint = self._adjust_local(version, constraint, prefix)\n        if version >= constraint:\n            return False\n        release_clause = constraint._release_clause\n        pfx = '.'.join([str(i) for i in release_clause])\n        return not _match_prefix(version, pfx)\n\n    def _match_gt(self, version, constraint, prefix):\n        version, constraint = self._adjust_local(version, constraint, prefix)\n        if version <= constraint:\n            return False\n        release_clause = constraint._release_clause\n        pfx = '.'.join([str(i) for i in release_clause])\n        return not _match_prefix(version, pfx)\n\n    def _match_le(self, version, constraint, prefix):\n        version, constraint = self._adjust_local(version, constraint, prefix)\n        return version <= constraint\n\n    def _match_ge(self, version, constraint, prefix):\n        version, constraint = self._adjust_local(version, constraint, prefix)\n        return version >= constraint\n\n    def _match_eq(self, version, constraint, prefix):\n        version, constraint = self._adjust_local(version, constraint, prefix)\n        if not prefix:\n            result = (version == constraint)\n        else:\n            result = _match_prefix(version, constraint)\n        return result\n\n    def _match_arbitrary(self, version, constraint, prefix):\n        return str(version) == str(constraint)\n\n    def _match_ne(self, version, constraint, prefix):\n        version, constraint = self._adjust_local(version, constraint, prefix)\n        if not prefix:\n            result = (version != constraint)\n        else:\n            result = not _match_prefix(version, constraint)\n        return result\n\n    def _match_compatible(self, version, constraint, prefix):\n        version, constraint = self._adjust_local(version, constraint, prefix)\n        if version == constraint:\n            return True\n        if version < constraint:\n            return False\n#        if not prefix:\n#            return True\n        release_clause = constraint._release_clause\n        if len(release_clause) > 1:\n            release_clause = release_clause[:-1]\n        pfx = '.'.join([str(i) for i in release_clause])\n        return _match_prefix(version, pfx)\n\n\n_REPLACEMENTS = (\n    (re.compile('[.+-]$'), ''),                     # remove trailing puncts\n    (re.compile(r'^[.](\\d)'), r'0.\\1'),             # .N -> 0.N at start\n    (re.compile('^[.-]'), ''),                      # remove leading puncts\n    (re.compile(r'^\\((.*)\\)$'), r'\\1'),             # remove parentheses\n    (re.compile(r'^v(ersion)?\\s*(\\d+)'), r'\\2'),    # remove leading v(ersion)\n    (re.compile(r'^r(ev)?\\s*(\\d+)'), r'\\2'),        # remove leading v(ersion)\n    (re.compile('[.]{2,}'), '.'),                   # multiple runs of '.'\n    (re.compile(r'\\b(alfa|apha)\\b'), 'alpha'),      # misspelt alpha\n    (re.compile(r'\\b(pre-alpha|prealpha)\\b'),\n        'pre.alpha'),                               # standardise\n    (re.compile(r'\\(beta\\)$'), 'beta'),             # remove parentheses\n)\n\n_SUFFIX_REPLACEMENTS = (\n    (re.compile('^[:~._+-]+'), ''),                   # remove leading puncts\n    (re.compile('[,*\")([\\\\]]'), ''),                  # remove unwanted chars\n    (re.compile('[~:+_ -]'), '.'),                    # replace illegal chars\n    (re.compile('[.]{2,}'), '.'),                   # multiple runs of '.'\n    (re.compile(r'\\.$'), ''),                       # trailing '.'\n)\n\n_NUMERIC_PREFIX = re.compile(r'(\\d+(\\.\\d+)*)')\n\n\ndef _suggest_semantic_version(s):\n    \"\"\"\n    Try to suggest a semantic form for a version for which\n    _suggest_normalized_version couldn't come up with anything.\n    \"\"\"\n    result = s.strip().lower()\n    for pat, repl in _REPLACEMENTS:\n        result = pat.sub(repl, result)\n    if not result:\n        result = '0.0.0'\n\n    # Now look for numeric prefix, and separate it out from\n    # the rest.\n    # import pdb; pdb.set_trace()\n    m = _NUMERIC_PREFIX.match(result)\n    if not m:\n        prefix = '0.0.0'\n        suffix = result\n    else:\n        prefix = m.groups()[0].split('.')\n        prefix = [int(i) for i in prefix]\n        while len(prefix) < 3:\n            prefix.append(0)\n        if len(prefix) == 3:\n            suffix = result[m.end():]\n        else:\n            suffix = '.'.join([str(i) for i in prefix[3:]]) + result[m.end():]\n            prefix = prefix[:3]\n        prefix = '.'.join([str(i) for i in prefix])\n        suffix = suffix.strip()\n    if suffix:\n        # import pdb; pdb.set_trace()\n        # massage the suffix.\n        for pat, repl in _SUFFIX_REPLACEMENTS:\n            suffix = pat.sub(repl, suffix)\n\n    if not suffix:\n        result = prefix\n    else:\n        sep = '-' if 'dev' in suffix else '+'\n        result = prefix + sep + suffix\n    if not is_semver(result):\n        result = None\n    return result\n\n\ndef _suggest_normalized_version(s):\n    \"\"\"Suggest a normalized version close to the given version string.\n\n    If you have a version string that isn't rational (i.e. NormalizedVersion\n    doesn't like it) then you might be able to get an equivalent (or close)\n    rational version from this function.\n\n    This does a number of simple normalizations to the given string, based\n    on observation of versions currently in use on PyPI. Given a dump of\n    those version during PyCon 2009, 4287 of them:\n    - 2312 (53.93%) match NormalizedVersion without change\n      with the automatic suggestion\n    - 3474 (81.04%) match when using this suggestion method\n\n    @param s {str} An irrational version string.\n    @returns A rational version string, or None, if couldn't determine one.\n    \"\"\"\n    try:\n        _normalized_key(s)\n        return s   # already rational\n    except UnsupportedVersionError:\n        pass\n\n    rs = s.lower()\n\n    # part of this could use maketrans\n    for orig, repl in (('-alpha', 'a'), ('-beta', 'b'), ('alpha', 'a'),\n                       ('beta', 'b'), ('rc', 'c'), ('-final', ''),\n                       ('-pre', 'c'),\n                       ('-release', ''), ('.release', ''), ('-stable', ''),\n                       ('+', '.'), ('_', '.'), (' ', ''), ('.final', ''),\n                       ('final', '')):\n        rs = rs.replace(orig, repl)\n\n    # if something ends with dev or pre, we add a 0\n    rs = re.sub(r\"pre$\", r\"pre0\", rs)\n    rs = re.sub(r\"dev$\", r\"dev0\", rs)\n\n    # if we have something like \"b-2\" or \"a.2\" at the end of the\n    # version, that is probably beta, alpha, etc\n    # let's remove the dash or dot\n    rs = re.sub(r\"([abc]|rc)[\\-\\.](\\d+)$\", r\"\\1\\2\", rs)\n\n    # 1.0-dev-r371 -> 1.0.dev371\n    # 0.1-dev-r79 -> 0.1.dev79\n    rs = re.sub(r\"[\\-\\.](dev)[\\-\\.]?r?(\\d+)$\", r\".\\1\\2\", rs)\n\n    # Clean: 2.0.a.3, 2.0.b1, 0.9.0~c1\n    rs = re.sub(r\"[.~]?([abc])\\.?\", r\"\\1\", rs)\n\n    # Clean: v0.3, v1.0\n    if rs.startswith('v'):\n        rs = rs[1:]\n\n    # Clean leading '0's on numbers.\n    # TODO: unintended side-effect on, e.g., \"2003.05.09\"\n    # PyPI stats: 77 (~2%) better\n    rs = re.sub(r\"\\b0+(\\d+)(?!\\d)\", r\"\\1\", rs)\n\n    # Clean a/b/c with no version. E.g. \"1.0a\" -> \"1.0a0\". Setuptools infers\n    # zero.\n    # PyPI stats: 245 (7.56%) better\n    rs = re.sub(r\"(\\d+[abc])$\", r\"\\g<1>0\", rs)\n\n    # the 'dev-rNNN' tag is a dev tag\n    rs = re.sub(r\"\\.?(dev-r|dev\\.r)\\.?(\\d+)$\", r\".dev\\2\", rs)\n\n    # clean the - when used as a pre delimiter\n    rs = re.sub(r\"-(a|b|c)(\\d+)$\", r\"\\1\\2\", rs)\n\n    # a terminal \"dev\" or \"devel\" can be changed into \".dev0\"\n    rs = re.sub(r\"[\\.\\-](dev|devel)$\", r\".dev0\", rs)\n\n    # a terminal \"dev\" can be changed into \".dev0\"\n    rs = re.sub(r\"(?![\\.\\-])dev$\", r\".dev0\", rs)\n\n    # a terminal \"final\" or \"stable\" can be removed\n    rs = re.sub(r\"(final|stable)$\", \"\", rs)\n\n    # The 'r' and the '-' tags are post release tags\n    #   0.4a1.r10       ->  0.4a1.post10\n    #   0.9.33-17222    ->  0.9.33.post17222\n    #   0.9.33-r17222   ->  0.9.33.post17222\n    rs = re.sub(r\"\\.?(r|-|-r)\\.?(\\d+)$\", r\".post\\2\", rs)\n\n    # Clean 'r' instead of 'dev' usage:\n    #   0.9.33+r17222   ->  0.9.33.dev17222\n    #   1.0dev123       ->  1.0.dev123\n    #   1.0.git123      ->  1.0.dev123\n    #   1.0.bzr123      ->  1.0.dev123\n    #   0.1a0dev.123    ->  0.1a0.dev123\n    # PyPI stats:  ~150 (~4%) better\n    rs = re.sub(r\"\\.?(dev|git|bzr)\\.?(\\d+)$\", r\".dev\\2\", rs)\n\n    # Clean '.pre' (normalized from '-pre' above) instead of 'c' usage:\n    #   0.2.pre1        ->  0.2c1\n    #   0.2-c1         ->  0.2c1\n    #   1.0preview123   ->  1.0c123\n    # PyPI stats: ~21 (0.62%) better\n    rs = re.sub(r\"\\.?(pre|preview|-c)(\\d+)$\", r\"c\\g<2>\", rs)\n\n    # Tcl/Tk uses \"px\" for their post release markers\n    rs = re.sub(r\"p(\\d+)$\", r\".post\\1\", rs)\n\n    try:\n        _normalized_key(rs)\n    except UnsupportedVersionError:\n        rs = None\n    return rs\n\n#\n#   Legacy version processing (distribute-compatible)\n#\n\n\n_VERSION_PART = re.compile(r'([a-z]+|\\d+|[\\.-])', re.I)\n_VERSION_REPLACE = {\n    'pre': 'c',\n    'preview': 'c',\n    '-': 'final-',\n    'rc': 'c',\n    'dev': '@',\n    '': None,\n    '.': None,\n}\n\n\ndef _legacy_key(s):\n    def get_parts(s):\n        result = []\n        for p in _VERSION_PART.split(s.lower()):\n            p = _VERSION_REPLACE.get(p, p)\n            if p:\n                if '0' <= p[:1] <= '9':\n                    p = p.zfill(8)\n                else:\n                    p = '*' + p\n                result.append(p)\n        result.append('*final')\n        return result\n\n    result = []\n    for p in get_parts(s):\n        if p.startswith('*'):\n            if p < '*final':\n                while result and result[-1] == '*final-':\n                    result.pop()\n            while result and result[-1] == '00000000':\n                result.pop()\n        result.append(p)\n    return tuple(result)\n\n\nclass LegacyVersion(Version):\n    def parse(self, s):\n        return _legacy_key(s)\n\n    @property\n    def is_prerelease(self):\n        result = False\n        for x in self._parts:\n            if (isinstance(x, string_types) and x.startswith('*') and x < '*final'):\n                result = True\n                break\n        return result\n\n\nclass LegacyMatcher(Matcher):\n    version_class = LegacyVersion\n\n    _operators = dict(Matcher._operators)\n    _operators['~='] = '_match_compatible'\n\n    numeric_re = re.compile(r'^(\\d+(\\.\\d+)*)')\n\n    def _match_compatible(self, version, constraint, prefix):\n        if version < constraint:\n            return False\n        m = self.numeric_re.match(str(constraint))\n        if not m:\n            logger.warning('Cannot compute compatible match for version %s '\n                           ' and constraint %s', version, constraint)\n            return True\n        s = m.groups()[0]\n        if '.' in s:\n            s = s.rsplit('.', 1)[0]\n        return _match_prefix(version, s)\n\n#\n#   Semantic versioning\n#\n\n\n_SEMVER_RE = re.compile(r'^(\\d+)\\.(\\d+)\\.(\\d+)'\n                        r'(-[a-z0-9]+(\\.[a-z0-9-]+)*)?'\n                        r'(\\+[a-z0-9]+(\\.[a-z0-9-]+)*)?$', re.I)\n\n\ndef is_semver(s):\n    return _SEMVER_RE.match(s)\n\n\ndef _semantic_key(s):\n    def make_tuple(s, absent):\n        if s is None:\n            result = (absent,)\n        else:\n            parts = s[1:].split('.')\n            # We can't compare ints and strings on Python 3, so fudge it\n            # by zero-filling numeric values so simulate a numeric comparison\n            result = tuple([p.zfill(8) if p.isdigit() else p for p in parts])\n        return result\n\n    m = is_semver(s)\n    if not m:\n        raise UnsupportedVersionError(s)\n    groups = m.groups()\n    major, minor, patch = [int(i) for i in groups[:3]]\n    # choose the '|' and '*' so that versions sort correctly\n    pre, build = make_tuple(groups[3], '|'), make_tuple(groups[5], '*')\n    return (major, minor, patch), pre, build\n\n\nclass SemanticVersion(Version):\n    def parse(self, s):\n        return _semantic_key(s)\n\n    @property\n    def is_prerelease(self):\n        return self._parts[1][0] != '|'\n\n\nclass SemanticMatcher(Matcher):\n    version_class = SemanticVersion\n\n\nclass VersionScheme(object):\n    def __init__(self, key, matcher, suggester=None):\n        self.key = key\n        self.matcher = matcher\n        self.suggester = suggester\n\n    def is_valid_version(self, s):\n        try:\n            self.matcher.version_class(s)\n            result = True\n        except UnsupportedVersionError:\n            result = False\n        return result\n\n    def is_valid_matcher(self, s):\n        try:\n            self.matcher(s)\n            result = True\n        except UnsupportedVersionError:\n            result = False\n        return result\n\n    def is_valid_constraint_list(self, s):\n        \"\"\"\n        Used for processing some metadata fields\n        \"\"\"\n        # See issue #140. Be tolerant of a single trailing comma.\n        if s.endswith(','):\n            s = s[:-1]\n        return self.is_valid_matcher('dummy_name (%s)' % s)\n\n    def suggest(self, s):\n        if self.suggester is None:\n            result = None\n        else:\n            result = self.suggester(s)\n        return result\n\n\n_SCHEMES = {\n    'normalized': VersionScheme(_normalized_key, NormalizedMatcher,\n                                _suggest_normalized_version),\n    'legacy': VersionScheme(_legacy_key, LegacyMatcher, lambda self, s: s),\n    'semantic': VersionScheme(_semantic_key, SemanticMatcher,\n                              _suggest_semantic_version),\n}\n\n_SCHEMES['default'] = _SCHEMES['normalized']\n\n\ndef get_scheme(name):\n    if name not in _SCHEMES:\n        raise ValueError('unknown scheme name: %r' % name)\n    return _SCHEMES[name]\n", "distlib/util.py": "#\n# Copyright (C) 2012-2023 The Python Software Foundation.\n# See LICENSE.txt and CONTRIBUTORS.txt.\n#\nimport codecs\nfrom collections import deque\nimport contextlib\nimport csv\nfrom glob import iglob as std_iglob\nimport io\nimport json\nimport logging\nimport os\nimport py_compile\nimport re\nimport socket\ntry:\n    import ssl\nexcept ImportError:  # pragma: no cover\n    ssl = None\nimport subprocess\nimport sys\nimport tarfile\nimport tempfile\nimport textwrap\n\ntry:\n    import threading\nexcept ImportError:  # pragma: no cover\n    import dummy_threading as threading\nimport time\n\nfrom . import DistlibException\nfrom .compat import (string_types, text_type, shutil, raw_input, StringIO, cache_from_source, urlopen, urljoin, httplib,\n                     xmlrpclib, HTTPHandler, BaseConfigurator, valid_ident, Container, configparser, URLError, ZipFile,\n                     fsdecode, unquote, urlparse)\n\nlogger = logging.getLogger(__name__)\n\n#\n# Requirement parsing code as per PEP 508\n#\n\nIDENTIFIER = re.compile(r'^([\\w\\.-]+)\\s*')\nVERSION_IDENTIFIER = re.compile(r'^([\\w\\.*+-]+)\\s*')\nCOMPARE_OP = re.compile(r'^(<=?|>=?|={2,3}|[~!]=)\\s*')\nMARKER_OP = re.compile(r'^((<=?)|(>=?)|={2,3}|[~!]=|in|not\\s+in)\\s*')\nOR = re.compile(r'^or\\b\\s*')\nAND = re.compile(r'^and\\b\\s*')\nNON_SPACE = re.compile(r'(\\S+)\\s*')\nSTRING_CHUNK = re.compile(r'([\\s\\w\\.{}()*+#:;,/?!~`@$%^&=|<>\\[\\]-]+)')\n\n\ndef parse_marker(marker_string):\n    \"\"\"\n    Parse a marker string and return a dictionary containing a marker expression.\n\n    The dictionary will contain keys \"op\", \"lhs\" and \"rhs\" for non-terminals in\n    the expression grammar, or strings. A string contained in quotes is to be\n    interpreted as a literal string, and a string not contained in quotes is a\n    variable (such as os_name).\n    \"\"\"\n\n    def marker_var(remaining):\n        # either identifier, or literal string\n        m = IDENTIFIER.match(remaining)\n        if m:\n            result = m.groups()[0]\n            remaining = remaining[m.end():]\n        elif not remaining:\n            raise SyntaxError('unexpected end of input')\n        else:\n            q = remaining[0]\n            if q not in '\\'\"':\n                raise SyntaxError('invalid expression: %s' % remaining)\n            oq = '\\'\"'.replace(q, '')\n            remaining = remaining[1:]\n            parts = [q]\n            while remaining:\n                # either a string chunk, or oq, or q to terminate\n                if remaining[0] == q:\n                    break\n                elif remaining[0] == oq:\n                    parts.append(oq)\n                    remaining = remaining[1:]\n                else:\n                    m = STRING_CHUNK.match(remaining)\n                    if not m:\n                        raise SyntaxError('error in string literal: %s' % remaining)\n                    parts.append(m.groups()[0])\n                    remaining = remaining[m.end():]\n            else:\n                s = ''.join(parts)\n                raise SyntaxError('unterminated string: %s' % s)\n            parts.append(q)\n            result = ''.join(parts)\n            remaining = remaining[1:].lstrip()  # skip past closing quote\n        return result, remaining\n\n    def marker_expr(remaining):\n        if remaining and remaining[0] == '(':\n            result, remaining = marker(remaining[1:].lstrip())\n            if remaining[0] != ')':\n                raise SyntaxError('unterminated parenthesis: %s' % remaining)\n            remaining = remaining[1:].lstrip()\n        else:\n            lhs, remaining = marker_var(remaining)\n            while remaining:\n                m = MARKER_OP.match(remaining)\n                if not m:\n                    break\n                op = m.groups()[0]\n                remaining = remaining[m.end():]\n                rhs, remaining = marker_var(remaining)\n                lhs = {'op': op, 'lhs': lhs, 'rhs': rhs}\n            result = lhs\n        return result, remaining\n\n    def marker_and(remaining):\n        lhs, remaining = marker_expr(remaining)\n        while remaining:\n            m = AND.match(remaining)\n            if not m:\n                break\n            remaining = remaining[m.end():]\n            rhs, remaining = marker_expr(remaining)\n            lhs = {'op': 'and', 'lhs': lhs, 'rhs': rhs}\n        return lhs, remaining\n\n    def marker(remaining):\n        lhs, remaining = marker_and(remaining)\n        while remaining:\n            m = OR.match(remaining)\n            if not m:\n                break\n            remaining = remaining[m.end():]\n            rhs, remaining = marker_and(remaining)\n            lhs = {'op': 'or', 'lhs': lhs, 'rhs': rhs}\n        return lhs, remaining\n\n    return marker(marker_string)\n\n\ndef parse_requirement(req):\n    \"\"\"\n    Parse a requirement passed in as a string. Return a Container\n    whose attributes contain the various parts of the requirement.\n    \"\"\"\n    remaining = req.strip()\n    if not remaining or remaining.startswith('#'):\n        return None\n    m = IDENTIFIER.match(remaining)\n    if not m:\n        raise SyntaxError('name expected: %s' % remaining)\n    distname = m.groups()[0]\n    remaining = remaining[m.end():]\n    extras = mark_expr = versions = uri = None\n    if remaining and remaining[0] == '[':\n        i = remaining.find(']', 1)\n        if i < 0:\n            raise SyntaxError('unterminated extra: %s' % remaining)\n        s = remaining[1:i]\n        remaining = remaining[i + 1:].lstrip()\n        extras = []\n        while s:\n            m = IDENTIFIER.match(s)\n            if not m:\n                raise SyntaxError('malformed extra: %s' % s)\n            extras.append(m.groups()[0])\n            s = s[m.end():]\n            if not s:\n                break\n            if s[0] != ',':\n                raise SyntaxError('comma expected in extras: %s' % s)\n            s = s[1:].lstrip()\n        if not extras:\n            extras = None\n    if remaining:\n        if remaining[0] == '@':\n            # it's a URI\n            remaining = remaining[1:].lstrip()\n            m = NON_SPACE.match(remaining)\n            if not m:\n                raise SyntaxError('invalid URI: %s' % remaining)\n            uri = m.groups()[0]\n            t = urlparse(uri)\n            # there are issues with Python and URL parsing, so this test\n            # is a bit crude. See bpo-20271, bpo-23505. Python doesn't\n            # always parse invalid URLs correctly - it should raise\n            # exceptions for malformed URLs\n            if not (t.scheme and t.netloc):\n                raise SyntaxError('Invalid URL: %s' % uri)\n            remaining = remaining[m.end():].lstrip()\n        else:\n\n            def get_versions(ver_remaining):\n                \"\"\"\n                Return a list of operator, version tuples if any are\n                specified, else None.\n                \"\"\"\n                m = COMPARE_OP.match(ver_remaining)\n                versions = None\n                if m:\n                    versions = []\n                    while True:\n                        op = m.groups()[0]\n                        ver_remaining = ver_remaining[m.end():]\n                        m = VERSION_IDENTIFIER.match(ver_remaining)\n                        if not m:\n                            raise SyntaxError('invalid version: %s' % ver_remaining)\n                        v = m.groups()[0]\n                        versions.append((op, v))\n                        ver_remaining = ver_remaining[m.end():]\n                        if not ver_remaining or ver_remaining[0] != ',':\n                            break\n                        ver_remaining = ver_remaining[1:].lstrip()\n                        # Some packages have a trailing comma which would break things\n                        # See issue #148\n                        if not ver_remaining:\n                            break\n                        m = COMPARE_OP.match(ver_remaining)\n                        if not m:\n                            raise SyntaxError('invalid constraint: %s' % ver_remaining)\n                    if not versions:\n                        versions = None\n                return versions, ver_remaining\n\n            if remaining[0] != '(':\n                versions, remaining = get_versions(remaining)\n            else:\n                i = remaining.find(')', 1)\n                if i < 0:\n                    raise SyntaxError('unterminated parenthesis: %s' % remaining)\n                s = remaining[1:i]\n                remaining = remaining[i + 1:].lstrip()\n                # As a special diversion from PEP 508, allow a version number\n                # a.b.c in parentheses as a synonym for ~= a.b.c (because this\n                # is allowed in earlier PEPs)\n                if COMPARE_OP.match(s):\n                    versions, _ = get_versions(s)\n                else:\n                    m = VERSION_IDENTIFIER.match(s)\n                    if not m:\n                        raise SyntaxError('invalid constraint: %s' % s)\n                    v = m.groups()[0]\n                    s = s[m.end():].lstrip()\n                    if s:\n                        raise SyntaxError('invalid constraint: %s' % s)\n                    versions = [('~=', v)]\n\n    if remaining:\n        if remaining[0] != ';':\n            raise SyntaxError('invalid requirement: %s' % remaining)\n        remaining = remaining[1:].lstrip()\n\n        mark_expr, remaining = parse_marker(remaining)\n\n    if remaining and remaining[0] != '#':\n        raise SyntaxError('unexpected trailing data: %s' % remaining)\n\n    if not versions:\n        rs = distname\n    else:\n        rs = '%s %s' % (distname, ', '.join(['%s %s' % con for con in versions]))\n    return Container(name=distname, extras=extras, constraints=versions, marker=mark_expr, url=uri, requirement=rs)\n\n\ndef get_resources_dests(resources_root, rules):\n    \"\"\"Find destinations for resources files\"\"\"\n\n    def get_rel_path(root, path):\n        # normalizes and returns a lstripped-/-separated path\n        root = root.replace(os.path.sep, '/')\n        path = path.replace(os.path.sep, '/')\n        assert path.startswith(root)\n        return path[len(root):].lstrip('/')\n\n    destinations = {}\n    for base, suffix, dest in rules:\n        prefix = os.path.join(resources_root, base)\n        for abs_base in iglob(prefix):\n            abs_glob = os.path.join(abs_base, suffix)\n            for abs_path in iglob(abs_glob):\n                resource_file = get_rel_path(resources_root, abs_path)\n                if dest is None:  # remove the entry if it was here\n                    destinations.pop(resource_file, None)\n                else:\n                    rel_path = get_rel_path(abs_base, abs_path)\n                    rel_dest = dest.replace(os.path.sep, '/').rstrip('/')\n                    destinations[resource_file] = rel_dest + '/' + rel_path\n    return destinations\n\n\ndef in_venv():\n    if hasattr(sys, 'real_prefix'):\n        # virtualenv venvs\n        result = True\n    else:\n        # PEP 405 venvs\n        result = sys.prefix != getattr(sys, 'base_prefix', sys.prefix)\n    return result\n\n\ndef get_executable():\n    # The __PYVENV_LAUNCHER__ dance is apparently no longer needed, as\n    # changes to the stub launcher mean that sys.executable always points\n    # to the stub on OS X\n    #    if sys.platform == 'darwin' and ('__PYVENV_LAUNCHER__'\n    #                                     in os.environ):\n    #        result =  os.environ['__PYVENV_LAUNCHER__']\n    #    else:\n    #        result = sys.executable\n    #    return result\n    # Avoid normcasing: see issue #143\n    # result = os.path.normcase(sys.executable)\n    result = sys.executable\n    if not isinstance(result, text_type):\n        result = fsdecode(result)\n    return result\n\n\ndef proceed(prompt, allowed_chars, error_prompt=None, default=None):\n    p = prompt\n    while True:\n        s = raw_input(p)\n        p = prompt\n        if not s and default:\n            s = default\n        if s:\n            c = s[0].lower()\n            if c in allowed_chars:\n                break\n            if error_prompt:\n                p = '%c: %s\\n%s' % (c, error_prompt, prompt)\n    return c\n\n\ndef extract_by_key(d, keys):\n    if isinstance(keys, string_types):\n        keys = keys.split()\n    result = {}\n    for key in keys:\n        if key in d:\n            result[key] = d[key]\n    return result\n\n\ndef read_exports(stream):\n    if sys.version_info[0] >= 3:\n        # needs to be a text stream\n        stream = codecs.getreader('utf-8')(stream)\n    # Try to load as JSON, falling back on legacy format\n    data = stream.read()\n    stream = StringIO(data)\n    try:\n        jdata = json.load(stream)\n        result = jdata['extensions']['python.exports']['exports']\n        for group, entries in result.items():\n            for k, v in entries.items():\n                s = '%s = %s' % (k, v)\n                entry = get_export_entry(s)\n                assert entry is not None\n                entries[k] = entry\n        return result\n    except Exception:\n        stream.seek(0, 0)\n\n    def read_stream(cp, stream):\n        if hasattr(cp, 'read_file'):\n            cp.read_file(stream)\n        else:\n            cp.readfp(stream)\n\n    cp = configparser.ConfigParser()\n    try:\n        read_stream(cp, stream)\n    except configparser.MissingSectionHeaderError:\n        stream.close()\n        data = textwrap.dedent(data)\n        stream = StringIO(data)\n        read_stream(cp, stream)\n\n    result = {}\n    for key in cp.sections():\n        result[key] = entries = {}\n        for name, value in cp.items(key):\n            s = '%s = %s' % (name, value)\n            entry = get_export_entry(s)\n            assert entry is not None\n            # entry.dist = self\n            entries[name] = entry\n    return result\n\n\ndef write_exports(exports, stream):\n    if sys.version_info[0] >= 3:\n        # needs to be a text stream\n        stream = codecs.getwriter('utf-8')(stream)\n    cp = configparser.ConfigParser()\n    for k, v in exports.items():\n        # TODO check k, v for valid values\n        cp.add_section(k)\n        for entry in v.values():\n            if entry.suffix is None:\n                s = entry.prefix\n            else:\n                s = '%s:%s' % (entry.prefix, entry.suffix)\n            if entry.flags:\n                s = '%s [%s]' % (s, ', '.join(entry.flags))\n            cp.set(k, entry.name, s)\n    cp.write(stream)\n\n\n@contextlib.contextmanager\ndef tempdir():\n    td = tempfile.mkdtemp()\n    try:\n        yield td\n    finally:\n        shutil.rmtree(td)\n\n\n@contextlib.contextmanager\ndef chdir(d):\n    cwd = os.getcwd()\n    try:\n        os.chdir(d)\n        yield\n    finally:\n        os.chdir(cwd)\n\n\n@contextlib.contextmanager\ndef socket_timeout(seconds=15):\n    cto = socket.getdefaulttimeout()\n    try:\n        socket.setdefaulttimeout(seconds)\n        yield\n    finally:\n        socket.setdefaulttimeout(cto)\n\n\nclass cached_property(object):\n\n    def __init__(self, func):\n        self.func = func\n        # for attr in ('__name__', '__module__', '__doc__'):\n        #     setattr(self, attr, getattr(func, attr, None))\n\n    def __get__(self, obj, cls=None):\n        if obj is None:\n            return self\n        value = self.func(obj)\n        object.__setattr__(obj, self.func.__name__, value)\n        # obj.__dict__[self.func.__name__] = value = self.func(obj)\n        return value\n\n\ndef convert_path(pathname):\n    \"\"\"Return 'pathname' as a name that will work on the native filesystem.\n\n    The path is split on '/' and put back together again using the current\n    directory separator.  Needed because filenames in the setup script are\n    always supplied in Unix style, and have to be converted to the local\n    convention before we can actually use them in the filesystem.  Raises\n    ValueError on non-Unix-ish systems if 'pathname' either starts or\n    ends with a slash.\n    \"\"\"\n    if os.sep == '/':\n        return pathname\n    if not pathname:\n        return pathname\n    if pathname[0] == '/':\n        raise ValueError(\"path '%s' cannot be absolute\" % pathname)\n    if pathname[-1] == '/':\n        raise ValueError(\"path '%s' cannot end with '/'\" % pathname)\n\n    paths = pathname.split('/')\n    while os.curdir in paths:\n        paths.remove(os.curdir)\n    if not paths:\n        return os.curdir\n    return os.path.join(*paths)\n\n\nclass FileOperator(object):\n\n    def __init__(self, dry_run=False):\n        self.dry_run = dry_run\n        self.ensured = set()\n        self._init_record()\n\n    def _init_record(self):\n        self.record = False\n        self.files_written = set()\n        self.dirs_created = set()\n\n    def record_as_written(self, path):\n        if self.record:\n            self.files_written.add(path)\n\n    def newer(self, source, target):\n        \"\"\"Tell if the target is newer than the source.\n\n        Returns true if 'source' exists and is more recently modified than\n        'target', or if 'source' exists and 'target' doesn't.\n\n        Returns false if both exist and 'target' is the same age or younger\n        than 'source'. Raise PackagingFileError if 'source' does not exist.\n\n        Note that this test is not very accurate: files created in the same\n        second will have the same \"age\".\n        \"\"\"\n        if not os.path.exists(source):\n            raise DistlibException(\"file '%r' does not exist\" % os.path.abspath(source))\n        if not os.path.exists(target):\n            return True\n\n        return os.stat(source).st_mtime > os.stat(target).st_mtime\n\n    def copy_file(self, infile, outfile, check=True):\n        \"\"\"Copy a file respecting dry-run and force flags.\n        \"\"\"\n        self.ensure_dir(os.path.dirname(outfile))\n        logger.info('Copying %s to %s', infile, outfile)\n        if not self.dry_run:\n            msg = None\n            if check:\n                if os.path.islink(outfile):\n                    msg = '%s is a symlink' % outfile\n                elif os.path.exists(outfile) and not os.path.isfile(outfile):\n                    msg = '%s is a non-regular file' % outfile\n            if msg:\n                raise ValueError(msg + ' which would be overwritten')\n            shutil.copyfile(infile, outfile)\n        self.record_as_written(outfile)\n\n    def copy_stream(self, instream, outfile, encoding=None):\n        assert not os.path.isdir(outfile)\n        self.ensure_dir(os.path.dirname(outfile))\n        logger.info('Copying stream %s to %s', instream, outfile)\n        if not self.dry_run:\n            if encoding is None:\n                outstream = open(outfile, 'wb')\n            else:\n                outstream = codecs.open(outfile, 'w', encoding=encoding)\n            try:\n                shutil.copyfileobj(instream, outstream)\n            finally:\n                outstream.close()\n        self.record_as_written(outfile)\n\n    def write_binary_file(self, path, data):\n        self.ensure_dir(os.path.dirname(path))\n        if not self.dry_run:\n            if os.path.exists(path):\n                os.remove(path)\n            with open(path, 'wb') as f:\n                f.write(data)\n        self.record_as_written(path)\n\n    def write_text_file(self, path, data, encoding):\n        self.write_binary_file(path, data.encode(encoding))\n\n    def set_mode(self, bits, mask, files):\n        if os.name == 'posix' or (os.name == 'java' and os._name == 'posix'):\n            # Set the executable bits (owner, group, and world) on\n            # all the files specified.\n            for f in files:\n                if self.dry_run:\n                    logger.info(\"changing mode of %s\", f)\n                else:\n                    mode = (os.stat(f).st_mode | bits) & mask\n                    logger.info(\"changing mode of %s to %o\", f, mode)\n                    os.chmod(f, mode)\n\n    set_executable_mode = lambda s, f: s.set_mode(0o555, 0o7777, f)\n\n    def ensure_dir(self, path):\n        path = os.path.abspath(path)\n        if path not in self.ensured and not os.path.exists(path):\n            self.ensured.add(path)\n            d, f = os.path.split(path)\n            self.ensure_dir(d)\n            logger.info('Creating %s' % path)\n            if not self.dry_run:\n                os.mkdir(path)\n            if self.record:\n                self.dirs_created.add(path)\n\n    def byte_compile(self, path, optimize=False, force=False, prefix=None, hashed_invalidation=False):\n        dpath = cache_from_source(path, not optimize)\n        logger.info('Byte-compiling %s to %s', path, dpath)\n        if not self.dry_run:\n            if force or self.newer(path, dpath):\n                if not prefix:\n                    diagpath = None\n                else:\n                    assert path.startswith(prefix)\n                    diagpath = path[len(prefix):]\n            compile_kwargs = {}\n            if hashed_invalidation and hasattr(py_compile, 'PycInvalidationMode'):\n                if not isinstance(hashed_invalidation, py_compile.PycInvalidationMode):\n                    hashed_invalidation = py_compile.PycInvalidationMode.CHECKED_HASH\n                compile_kwargs['invalidation_mode'] = hashed_invalidation\n            py_compile.compile(path, dpath, diagpath, True, **compile_kwargs)  # raise error\n        self.record_as_written(dpath)\n        return dpath\n\n    def ensure_removed(self, path):\n        if os.path.exists(path):\n            if os.path.isdir(path) and not os.path.islink(path):\n                logger.debug('Removing directory tree at %s', path)\n                if not self.dry_run:\n                    shutil.rmtree(path)\n                if self.record:\n                    if path in self.dirs_created:\n                        self.dirs_created.remove(path)\n            else:\n                if os.path.islink(path):\n                    s = 'link'\n                else:\n                    s = 'file'\n                logger.debug('Removing %s %s', s, path)\n                if not self.dry_run:\n                    os.remove(path)\n                if self.record:\n                    if path in self.files_written:\n                        self.files_written.remove(path)\n\n    def is_writable(self, path):\n        result = False\n        while not result:\n            if os.path.exists(path):\n                result = os.access(path, os.W_OK)\n                break\n            parent = os.path.dirname(path)\n            if parent == path:\n                break\n            path = parent\n        return result\n\n    def commit(self):\n        \"\"\"\n        Commit recorded changes, turn off recording, return\n        changes.\n        \"\"\"\n        assert self.record\n        result = self.files_written, self.dirs_created\n        self._init_record()\n        return result\n\n    def rollback(self):\n        if not self.dry_run:\n            for f in list(self.files_written):\n                if os.path.exists(f):\n                    os.remove(f)\n            # dirs should all be empty now, except perhaps for\n            # __pycache__ subdirs\n            # reverse so that subdirs appear before their parents\n            dirs = sorted(self.dirs_created, reverse=True)\n            for d in dirs:\n                flist = os.listdir(d)\n                if flist:\n                    assert flist == ['__pycache__']\n                    sd = os.path.join(d, flist[0])\n                    os.rmdir(sd)\n                os.rmdir(d)  # should fail if non-empty\n        self._init_record()\n\n\ndef resolve(module_name, dotted_path):\n    if module_name in sys.modules:\n        mod = sys.modules[module_name]\n    else:\n        mod = __import__(module_name)\n    if dotted_path is None:\n        result = mod\n    else:\n        parts = dotted_path.split('.')\n        result = getattr(mod, parts.pop(0))\n        for p in parts:\n            result = getattr(result, p)\n    return result\n\n\nclass ExportEntry(object):\n\n    def __init__(self, name, prefix, suffix, flags):\n        self.name = name\n        self.prefix = prefix\n        self.suffix = suffix\n        self.flags = flags\n\n    @cached_property\n    def value(self):\n        return resolve(self.prefix, self.suffix)\n\n    def __repr__(self):  # pragma: no cover\n        return '<ExportEntry %s = %s:%s %s>' % (self.name, self.prefix, self.suffix, self.flags)\n\n    def __eq__(self, other):\n        if not isinstance(other, ExportEntry):\n            result = False\n        else:\n            result = (self.name == other.name and self.prefix == other.prefix and self.suffix == other.suffix and\n                      self.flags == other.flags)\n        return result\n\n    __hash__ = object.__hash__\n\n\nENTRY_RE = re.compile(\n    r'''(?P<name>([^\\[]\\S*))\n                      \\s*=\\s*(?P<callable>(\\w+)([:\\.]\\w+)*)\n                      \\s*(\\[\\s*(?P<flags>[\\w-]+(=\\w+)?(,\\s*\\w+(=\\w+)?)*)\\s*\\])?\n                      ''', re.VERBOSE)\n\n\ndef get_export_entry(specification):\n    m = ENTRY_RE.search(specification)\n    if not m:\n        result = None\n        if '[' in specification or ']' in specification:\n            raise DistlibException(\"Invalid specification \"\n                                   \"'%s'\" % specification)\n    else:\n        d = m.groupdict()\n        name = d['name']\n        path = d['callable']\n        colons = path.count(':')\n        if colons == 0:\n            prefix, suffix = path, None\n        else:\n            if colons != 1:\n                raise DistlibException(\"Invalid specification \"\n                                       \"'%s'\" % specification)\n            prefix, suffix = path.split(':')\n        flags = d['flags']\n        if flags is None:\n            if '[' in specification or ']' in specification:\n                raise DistlibException(\"Invalid specification \"\n                                       \"'%s'\" % specification)\n            flags = []\n        else:\n            flags = [f.strip() for f in flags.split(',')]\n        result = ExportEntry(name, prefix, suffix, flags)\n    return result\n\n\ndef get_cache_base(suffix=None):\n    \"\"\"\n    Return the default base location for distlib caches. If the directory does\n    not exist, it is created. Use the suffix provided for the base directory,\n    and default to '.distlib' if it isn't provided.\n\n    On Windows, if LOCALAPPDATA is defined in the environment, then it is\n    assumed to be a directory, and will be the parent directory of the result.\n    On POSIX, and on Windows if LOCALAPPDATA is not defined, the user's home\n    directory - using os.expanduser('~') - will be the parent directory of\n    the result.\n\n    The result is just the directory '.distlib' in the parent directory as\n    determined above, or with the name specified with ``suffix``.\n    \"\"\"\n    if suffix is None:\n        suffix = '.distlib'\n    if os.name == 'nt' and 'LOCALAPPDATA' in os.environ:\n        result = os.path.expandvars('$localappdata')\n    else:\n        # Assume posix, or old Windows\n        result = os.path.expanduser('~')\n    # we use 'isdir' instead of 'exists', because we want to\n    # fail if there's a file with that name\n    if os.path.isdir(result):\n        usable = os.access(result, os.W_OK)\n        if not usable:\n            logger.warning('Directory exists but is not writable: %s', result)\n    else:\n        try:\n            os.makedirs(result)\n            usable = True\n        except OSError:\n            logger.warning('Unable to create %s', result, exc_info=True)\n            usable = False\n    if not usable:\n        result = tempfile.mkdtemp()\n        logger.warning('Default location unusable, using %s', result)\n    return os.path.join(result, suffix)\n\n\ndef path_to_cache_dir(path):\n    \"\"\"\n    Convert an absolute path to a directory name for use in a cache.\n\n    The algorithm used is:\n\n    #. On Windows, any ``':'`` in the drive is replaced with ``'---'``.\n    #. Any occurrence of ``os.sep`` is replaced with ``'--'``.\n    #. ``'.cache'`` is appended.\n    \"\"\"\n    d, p = os.path.splitdrive(os.path.abspath(path))\n    if d:\n        d = d.replace(':', '---')\n    p = p.replace(os.sep, '--')\n    return d + p + '.cache'\n\n\ndef ensure_slash(s):\n    if not s.endswith('/'):\n        return s + '/'\n    return s\n\n\ndef parse_credentials(netloc):\n    username = password = None\n    if '@' in netloc:\n        prefix, netloc = netloc.rsplit('@', 1)\n        if ':' not in prefix:\n            username = prefix\n        else:\n            username, password = prefix.split(':', 1)\n    if username:\n        username = unquote(username)\n    if password:\n        password = unquote(password)\n    return username, password, netloc\n\n\ndef get_process_umask():\n    result = os.umask(0o22)\n    os.umask(result)\n    return result\n\n\ndef is_string_sequence(seq):\n    result = True\n    i = None\n    for i, s in enumerate(seq):\n        if not isinstance(s, string_types):\n            result = False\n            break\n    assert i is not None\n    return result\n\n\nPROJECT_NAME_AND_VERSION = re.compile('([a-z0-9_]+([.-][a-z_][a-z0-9_]*)*)-'\n                                      '([a-z0-9_.+-]+)', re.I)\nPYTHON_VERSION = re.compile(r'-py(\\d\\.?\\d?)')\n\n\ndef split_filename(filename, project_name=None):\n    \"\"\"\n    Extract name, version, python version from a filename (no extension)\n\n    Return name, version, pyver or None\n    \"\"\"\n    result = None\n    pyver = None\n    filename = unquote(filename).replace(' ', '-')\n    m = PYTHON_VERSION.search(filename)\n    if m:\n        pyver = m.group(1)\n        filename = filename[:m.start()]\n    if project_name and len(filename) > len(project_name) + 1:\n        m = re.match(re.escape(project_name) + r'\\b', filename)\n        if m:\n            n = m.end()\n            result = filename[:n], filename[n + 1:], pyver\n    if result is None:\n        m = PROJECT_NAME_AND_VERSION.match(filename)\n        if m:\n            result = m.group(1), m.group(3), pyver\n    return result\n\n\n# Allow spaces in name because of legacy dists like \"Twisted Core\"\nNAME_VERSION_RE = re.compile(r'(?P<name>[\\w .-]+)\\s*'\n                             r'\\(\\s*(?P<ver>[^\\s)]+)\\)$')\n\n\ndef parse_name_and_version(p):\n    \"\"\"\n    A utility method used to get name and version from a string.\n\n    From e.g. a Provides-Dist value.\n\n    :param p: A value in a form 'foo (1.0)'\n    :return: The name and version as a tuple.\n    \"\"\"\n    m = NAME_VERSION_RE.match(p)\n    if not m:\n        raise DistlibException('Ill-formed name/version string: \\'%s\\'' % p)\n    d = m.groupdict()\n    return d['name'].strip().lower(), d['ver']\n\n\ndef get_extras(requested, available):\n    result = set()\n    requested = set(requested or [])\n    available = set(available or [])\n    if '*' in requested:\n        requested.remove('*')\n        result |= available\n    for r in requested:\n        if r == '-':\n            result.add(r)\n        elif r.startswith('-'):\n            unwanted = r[1:]\n            if unwanted not in available:\n                logger.warning('undeclared extra: %s' % unwanted)\n            if unwanted in result:\n                result.remove(unwanted)\n        else:\n            if r not in available:\n                logger.warning('undeclared extra: %s' % r)\n            result.add(r)\n    return result\n\n\n#\n# Extended metadata functionality\n#\n\n\ndef _get_external_data(url):\n    result = {}\n    try:\n        # urlopen might fail if it runs into redirections,\n        # because of Python issue #13696. Fixed in locators\n        # using a custom redirect handler.\n        resp = urlopen(url)\n        headers = resp.info()\n        ct = headers.get('Content-Type')\n        if not ct.startswith('application/json'):\n            logger.debug('Unexpected response for JSON request: %s', ct)\n        else:\n            reader = codecs.getreader('utf-8')(resp)\n            # data = reader.read().decode('utf-8')\n            # result = json.loads(data)\n            result = json.load(reader)\n    except Exception as e:\n        logger.exception('Failed to get external data for %s: %s', url, e)\n    return result\n\n\n_external_data_base_url = 'https://www.red-dove.com/pypi/projects/'\n\n\ndef get_project_data(name):\n    url = '%s/%s/project.json' % (name[0].upper(), name)\n    url = urljoin(_external_data_base_url, url)\n    result = _get_external_data(url)\n    return result\n\n\ndef get_package_data(name, version):\n    url = '%s/%s/package-%s.json' % (name[0].upper(), name, version)\n    url = urljoin(_external_data_base_url, url)\n    return _get_external_data(url)\n\n\nclass Cache(object):\n    \"\"\"\n    A class implementing a cache for resources that need to live in the file system\n    e.g. shared libraries. This class was moved from resources to here because it\n    could be used by other modules, e.g. the wheel module.\n    \"\"\"\n\n    def __init__(self, base):\n        \"\"\"\n        Initialise an instance.\n\n        :param base: The base directory where the cache should be located.\n        \"\"\"\n        # we use 'isdir' instead of 'exists', because we want to\n        # fail if there's a file with that name\n        if not os.path.isdir(base):  # pragma: no cover\n            os.makedirs(base)\n        if (os.stat(base).st_mode & 0o77) != 0:\n            logger.warning('Directory \\'%s\\' is not private', base)\n        self.base = os.path.abspath(os.path.normpath(base))\n\n    def prefix_to_dir(self, prefix):\n        \"\"\"\n        Converts a resource prefix to a directory name in the cache.\n        \"\"\"\n        return path_to_cache_dir(prefix)\n\n    def clear(self):\n        \"\"\"\n        Clear the cache.\n        \"\"\"\n        not_removed = []\n        for fn in os.listdir(self.base):\n            fn = os.path.join(self.base, fn)\n            try:\n                if os.path.islink(fn) or os.path.isfile(fn):\n                    os.remove(fn)\n                elif os.path.isdir(fn):\n                    shutil.rmtree(fn)\n            except Exception:\n                not_removed.append(fn)\n        return not_removed\n\n\nclass EventMixin(object):\n    \"\"\"\n    A very simple publish/subscribe system.\n    \"\"\"\n\n    def __init__(self):\n        self._subscribers = {}\n\n    def add(self, event, subscriber, append=True):\n        \"\"\"\n        Add a subscriber for an event.\n\n        :param event: The name of an event.\n        :param subscriber: The subscriber to be added (and called when the\n                           event is published).\n        :param append: Whether to append or prepend the subscriber to an\n                       existing subscriber list for the event.\n        \"\"\"\n        subs = self._subscribers\n        if event not in subs:\n            subs[event] = deque([subscriber])\n        else:\n            sq = subs[event]\n            if append:\n                sq.append(subscriber)\n            else:\n                sq.appendleft(subscriber)\n\n    def remove(self, event, subscriber):\n        \"\"\"\n        Remove a subscriber for an event.\n\n        :param event: The name of an event.\n        :param subscriber: The subscriber to be removed.\n        \"\"\"\n        subs = self._subscribers\n        if event not in subs:\n            raise ValueError('No subscribers: %r' % event)\n        subs[event].remove(subscriber)\n\n    def get_subscribers(self, event):\n        \"\"\"\n        Return an iterator for the subscribers for an event.\n        :param event: The event to return subscribers for.\n        \"\"\"\n        return iter(self._subscribers.get(event, ()))\n\n    def publish(self, event, *args, **kwargs):\n        \"\"\"\n        Publish a event and return a list of values returned by its\n        subscribers.\n\n        :param event: The event to publish.\n        :param args: The positional arguments to pass to the event's\n                     subscribers.\n        :param kwargs: The keyword arguments to pass to the event's\n                       subscribers.\n        \"\"\"\n        result = []\n        for subscriber in self.get_subscribers(event):\n            try:\n                value = subscriber(event, *args, **kwargs)\n            except Exception:\n                logger.exception('Exception during event publication')\n                value = None\n            result.append(value)\n        logger.debug('publish %s: args = %s, kwargs = %s, result = %s', event, args, kwargs, result)\n        return result\n\n\n#\n# Simple sequencing\n#\nclass Sequencer(object):\n\n    def __init__(self):\n        self._preds = {}\n        self._succs = {}\n        self._nodes = set()  # nodes with no preds/succs\n\n    def add_node(self, node):\n        self._nodes.add(node)\n\n    def remove_node(self, node, edges=False):\n        if node in self._nodes:\n            self._nodes.remove(node)\n        if edges:\n            for p in set(self._preds.get(node, ())):\n                self.remove(p, node)\n            for s in set(self._succs.get(node, ())):\n                self.remove(node, s)\n            # Remove empties\n            for k, v in list(self._preds.items()):\n                if not v:\n                    del self._preds[k]\n            for k, v in list(self._succs.items()):\n                if not v:\n                    del self._succs[k]\n\n    def add(self, pred, succ):\n        assert pred != succ\n        self._preds.setdefault(succ, set()).add(pred)\n        self._succs.setdefault(pred, set()).add(succ)\n\n    def remove(self, pred, succ):\n        assert pred != succ\n        try:\n            preds = self._preds[succ]\n            succs = self._succs[pred]\n        except KeyError:  # pragma: no cover\n            raise ValueError('%r not a successor of anything' % succ)\n        try:\n            preds.remove(pred)\n            succs.remove(succ)\n        except KeyError:  # pragma: no cover\n            raise ValueError('%r not a successor of %r' % (succ, pred))\n\n    def is_step(self, step):\n        return (step in self._preds or step in self._succs or step in self._nodes)\n\n    def get_steps(self, final):\n        if not self.is_step(final):\n            raise ValueError('Unknown: %r' % final)\n        result = []\n        todo = []\n        seen = set()\n        todo.append(final)\n        while todo:\n            step = todo.pop(0)\n            if step in seen:\n                # if a step was already seen,\n                # move it to the end (so it will appear earlier\n                # when reversed on return) ... but not for the\n                # final step, as that would be confusing for\n                # users\n                if step != final:\n                    result.remove(step)\n                    result.append(step)\n            else:\n                seen.add(step)\n                result.append(step)\n                preds = self._preds.get(step, ())\n                todo.extend(preds)\n        return reversed(result)\n\n    @property\n    def strong_connections(self):\n        # http://en.wikipedia.org/wiki/Tarjan%27s_strongly_connected_components_algorithm\n        index_counter = [0]\n        stack = []\n        lowlinks = {}\n        index = {}\n        result = []\n\n        graph = self._succs\n\n        def strongconnect(node):\n            # set the depth index for this node to the smallest unused index\n            index[node] = index_counter[0]\n            lowlinks[node] = index_counter[0]\n            index_counter[0] += 1\n            stack.append(node)\n\n            # Consider successors\n            try:\n                successors = graph[node]\n            except Exception:\n                successors = []\n            for successor in successors:\n                if successor not in lowlinks:\n                    # Successor has not yet been visited\n                    strongconnect(successor)\n                    lowlinks[node] = min(lowlinks[node], lowlinks[successor])\n                elif successor in stack:\n                    # the successor is in the stack and hence in the current\n                    # strongly connected component (SCC)\n                    lowlinks[node] = min(lowlinks[node], index[successor])\n\n            # If `node` is a root node, pop the stack and generate an SCC\n            if lowlinks[node] == index[node]:\n                connected_component = []\n\n                while True:\n                    successor = stack.pop()\n                    connected_component.append(successor)\n                    if successor == node:\n                        break\n                component = tuple(connected_component)\n                # storing the result\n                result.append(component)\n\n        for node in graph:\n            if node not in lowlinks:\n                strongconnect(node)\n\n        return result\n\n    @property\n    def dot(self):\n        result = ['digraph G {']\n        for succ in self._preds:\n            preds = self._preds[succ]\n            for pred in preds:\n                result.append('  %s -> %s;' % (pred, succ))\n        for node in self._nodes:\n            result.append('  %s;' % node)\n        result.append('}')\n        return '\\n'.join(result)\n\n\n#\n# Unarchiving functionality for zip, tar, tgz, tbz, whl\n#\n\nARCHIVE_EXTENSIONS = ('.tar.gz', '.tar.bz2', '.tar', '.zip', '.tgz', '.tbz', '.whl')\n\n\ndef unarchive(archive_filename, dest_dir, format=None, check=True):\n\n    def check_path(path):\n        if not isinstance(path, text_type):\n            path = path.decode('utf-8')\n        p = os.path.abspath(os.path.join(dest_dir, path))\n        if not p.startswith(dest_dir) or p[plen] != os.sep:\n            raise ValueError('path outside destination: %r' % p)\n\n    dest_dir = os.path.abspath(dest_dir)\n    plen = len(dest_dir)\n    archive = None\n    if format is None:\n        if archive_filename.endswith(('.zip', '.whl')):\n            format = 'zip'\n        elif archive_filename.endswith(('.tar.gz', '.tgz')):\n            format = 'tgz'\n            mode = 'r:gz'\n        elif archive_filename.endswith(('.tar.bz2', '.tbz')):\n            format = 'tbz'\n            mode = 'r:bz2'\n        elif archive_filename.endswith('.tar'):\n            format = 'tar'\n            mode = 'r'\n        else:  # pragma: no cover\n            raise ValueError('Unknown format for %r' % archive_filename)\n    try:\n        if format == 'zip':\n            archive = ZipFile(archive_filename, 'r')\n            if check:\n                names = archive.namelist()\n                for name in names:\n                    check_path(name)\n        else:\n            archive = tarfile.open(archive_filename, mode)\n            if check:\n                names = archive.getnames()\n                for name in names:\n                    check_path(name)\n        if format != 'zip' and sys.version_info[0] < 3:\n            # See Python issue 17153. If the dest path contains Unicode,\n            # tarfile extraction fails on Python 2.x if a member path name\n            # contains non-ASCII characters - it leads to an implicit\n            # bytes -> unicode conversion using ASCII to decode.\n            for tarinfo in archive.getmembers():\n                if not isinstance(tarinfo.name, text_type):\n                    tarinfo.name = tarinfo.name.decode('utf-8')\n\n        # Limit extraction of dangerous items, if this Python\n        # allows it easily. If not, just trust the input.\n        # See: https://docs.python.org/3/library/tarfile.html#extraction-filters\n        def extraction_filter(member, path):\n            \"\"\"Run tarfile.tar_filter, but raise the expected ValueError\"\"\"\n            # This is only called if the current Python has tarfile filters\n            try:\n                return tarfile.tar_filter(member, path)\n            except tarfile.FilterError as exc:\n                raise ValueError(str(exc))\n\n        archive.extraction_filter = extraction_filter\n\n        archive.extractall(dest_dir)\n\n    finally:\n        if archive:\n            archive.close()\n\n\ndef zip_dir(directory):\n    \"\"\"zip a directory tree into a BytesIO object\"\"\"\n    result = io.BytesIO()\n    dlen = len(directory)\n    with ZipFile(result, \"w\") as zf:\n        for root, dirs, files in os.walk(directory):\n            for name in files:\n                full = os.path.join(root, name)\n                rel = root[dlen:]\n                dest = os.path.join(rel, name)\n                zf.write(full, dest)\n    return result\n\n\n#\n# Simple progress bar\n#\n\nUNITS = ('', 'K', 'M', 'G', 'T', 'P')\n\n\nclass Progress(object):\n    unknown = 'UNKNOWN'\n\n    def __init__(self, minval=0, maxval=100):\n        assert maxval is None or maxval >= minval\n        self.min = self.cur = minval\n        self.max = maxval\n        self.started = None\n        self.elapsed = 0\n        self.done = False\n\n    def update(self, curval):\n        assert self.min <= curval\n        assert self.max is None or curval <= self.max\n        self.cur = curval\n        now = time.time()\n        if self.started is None:\n            self.started = now\n        else:\n            self.elapsed = now - self.started\n\n    def increment(self, incr):\n        assert incr >= 0\n        self.update(self.cur + incr)\n\n    def start(self):\n        self.update(self.min)\n        return self\n\n    def stop(self):\n        if self.max is not None:\n            self.update(self.max)\n        self.done = True\n\n    @property\n    def maximum(self):\n        return self.unknown if self.max is None else self.max\n\n    @property\n    def percentage(self):\n        if self.done:\n            result = '100 %'\n        elif self.max is None:\n            result = ' ?? %'\n        else:\n            v = 100.0 * (self.cur - self.min) / (self.max - self.min)\n            result = '%3d %%' % v\n        return result\n\n    def format_duration(self, duration):\n        if (duration <= 0) and self.max is None or self.cur == self.min:\n            result = '??:??:??'\n        # elif duration < 1:\n        #     result = '--:--:--'\n        else:\n            result = time.strftime('%H:%M:%S', time.gmtime(duration))\n        return result\n\n    @property\n    def ETA(self):\n        if self.done:\n            prefix = 'Done'\n            t = self.elapsed\n            # import pdb; pdb.set_trace()\n        else:\n            prefix = 'ETA '\n            if self.max is None:\n                t = -1\n            elif self.elapsed == 0 or (self.cur == self.min):\n                t = 0\n            else:\n                # import pdb; pdb.set_trace()\n                t = float(self.max - self.min)\n                t /= self.cur - self.min\n                t = (t - 1) * self.elapsed\n        return '%s: %s' % (prefix, self.format_duration(t))\n\n    @property\n    def speed(self):\n        if self.elapsed == 0:\n            result = 0.0\n        else:\n            result = (self.cur - self.min) / self.elapsed\n        for unit in UNITS:\n            if result < 1000:\n                break\n            result /= 1000.0\n        return '%d %sB/s' % (result, unit)\n\n\n#\n# Glob functionality\n#\n\nRICH_GLOB = re.compile(r'\\{([^}]*)\\}')\n_CHECK_RECURSIVE_GLOB = re.compile(r'[^/\\\\,{]\\*\\*|\\*\\*[^/\\\\,}]')\n_CHECK_MISMATCH_SET = re.compile(r'^[^{]*\\}|\\{[^}]*$')\n\n\ndef iglob(path_glob):\n    \"\"\"Extended globbing function that supports ** and {opt1,opt2,opt3}.\"\"\"\n    if _CHECK_RECURSIVE_GLOB.search(path_glob):\n        msg = \"\"\"invalid glob %r: recursive glob \"**\" must be used alone\"\"\"\n        raise ValueError(msg % path_glob)\n    if _CHECK_MISMATCH_SET.search(path_glob):\n        msg = \"\"\"invalid glob %r: mismatching set marker '{' or '}'\"\"\"\n        raise ValueError(msg % path_glob)\n    return _iglob(path_glob)\n\n\ndef _iglob(path_glob):\n    rich_path_glob = RICH_GLOB.split(path_glob, 1)\n    if len(rich_path_glob) > 1:\n        assert len(rich_path_glob) == 3, rich_path_glob\n        prefix, set, suffix = rich_path_glob\n        for item in set.split(','):\n            for path in _iglob(''.join((prefix, item, suffix))):\n                yield path\n    else:\n        if '**' not in path_glob:\n            for item in std_iglob(path_glob):\n                yield item\n        else:\n            prefix, radical = path_glob.split('**', 1)\n            if prefix == '':\n                prefix = '.'\n            if radical == '':\n                radical = '*'\n            else:\n                # we support both\n                radical = radical.lstrip('/')\n                radical = radical.lstrip('\\\\')\n            for path, dir, files in os.walk(prefix):\n                path = os.path.normpath(path)\n                for fn in _iglob(os.path.join(path, radical)):\n                    yield fn\n\n\nif ssl:\n    from .compat import (HTTPSHandler as BaseHTTPSHandler, match_hostname, CertificateError)\n\n    #\n    # HTTPSConnection which verifies certificates/matches domains\n    #\n\n    class HTTPSConnection(httplib.HTTPSConnection):\n        ca_certs = None  # set this to the path to the certs file (.pem)\n        check_domain = True  # only used if ca_certs is not None\n\n        # noinspection PyPropertyAccess\n        def connect(self):\n            sock = socket.create_connection((self.host, self.port), self.timeout)\n            if getattr(self, '_tunnel_host', False):\n                self.sock = sock\n                self._tunnel()\n\n            context = ssl.SSLContext(ssl.PROTOCOL_SSLv23)\n            if hasattr(ssl, 'OP_NO_SSLv2'):\n                context.options |= ssl.OP_NO_SSLv2\n            if getattr(self, 'cert_file', None):\n                context.load_cert_chain(self.cert_file, self.key_file)\n            kwargs = {}\n            if self.ca_certs:\n                context.verify_mode = ssl.CERT_REQUIRED\n                context.load_verify_locations(cafile=self.ca_certs)\n                if getattr(ssl, 'HAS_SNI', False):\n                    kwargs['server_hostname'] = self.host\n\n            self.sock = context.wrap_socket(sock, **kwargs)\n            if self.ca_certs and self.check_domain:\n                try:\n                    match_hostname(self.sock.getpeercert(), self.host)\n                    logger.debug('Host verified: %s', self.host)\n                except CertificateError:  # pragma: no cover\n                    self.sock.shutdown(socket.SHUT_RDWR)\n                    self.sock.close()\n                    raise\n\n    class HTTPSHandler(BaseHTTPSHandler):\n\n        def __init__(self, ca_certs, check_domain=True):\n            BaseHTTPSHandler.__init__(self)\n            self.ca_certs = ca_certs\n            self.check_domain = check_domain\n\n        def _conn_maker(self, *args, **kwargs):\n            \"\"\"\n            This is called to create a connection instance. Normally you'd\n            pass a connection class to do_open, but it doesn't actually check for\n            a class, and just expects a callable. As long as we behave just as a\n            constructor would have, we should be OK. If it ever changes so that\n            we *must* pass a class, we'll create an UnsafeHTTPSConnection class\n            which just sets check_domain to False in the class definition, and\n            choose which one to pass to do_open.\n            \"\"\"\n            result = HTTPSConnection(*args, **kwargs)\n            if self.ca_certs:\n                result.ca_certs = self.ca_certs\n                result.check_domain = self.check_domain\n            return result\n\n        def https_open(self, req):\n            try:\n                return self.do_open(self._conn_maker, req)\n            except URLError as e:\n                if 'certificate verify failed' in str(e.reason):\n                    raise CertificateError('Unable to verify server certificate '\n                                           'for %s' % req.host)\n                else:\n                    raise\n\n    #\n    # To prevent against mixing HTTP traffic with HTTPS (examples: A Man-In-The-\n    # Middle proxy using HTTP listens on port 443, or an index mistakenly serves\n    # HTML containing a http://xyz link when it should be https://xyz),\n    # you can use the following handler class, which does not allow HTTP traffic.\n    #\n    # It works by inheriting from HTTPHandler - so build_opener won't add a\n    # handler for HTTP itself.\n    #\n    class HTTPSOnlyHandler(HTTPSHandler, HTTPHandler):\n\n        def http_open(self, req):\n            raise URLError('Unexpected HTTP request on what should be a secure '\n                           'connection: %s' % req)\n\n\n#\n# XML-RPC with timeouts\n#\nclass Transport(xmlrpclib.Transport):\n\n    def __init__(self, timeout, use_datetime=0):\n        self.timeout = timeout\n        xmlrpclib.Transport.__init__(self, use_datetime)\n\n    def make_connection(self, host):\n        h, eh, x509 = self.get_host_info(host)\n        if not self._connection or host != self._connection[0]:\n            self._extra_headers = eh\n            self._connection = host, httplib.HTTPConnection(h)\n        return self._connection[1]\n\n\nif ssl:\n\n    class SafeTransport(xmlrpclib.SafeTransport):\n\n        def __init__(self, timeout, use_datetime=0):\n            self.timeout = timeout\n            xmlrpclib.SafeTransport.__init__(self, use_datetime)\n\n        def make_connection(self, host):\n            h, eh, kwargs = self.get_host_info(host)\n            if not kwargs:\n                kwargs = {}\n            kwargs['timeout'] = self.timeout\n            if not self._connection or host != self._connection[0]:\n                self._extra_headers = eh\n                self._connection = host, httplib.HTTPSConnection(h, None, **kwargs)\n            return self._connection[1]\n\n\nclass ServerProxy(xmlrpclib.ServerProxy):\n\n    def __init__(self, uri, **kwargs):\n        self.timeout = timeout = kwargs.pop('timeout', None)\n        # The above classes only come into play if a timeout\n        # is specified\n        if timeout is not None:\n            # scheme = splittype(uri)  # deprecated as of Python 3.8\n            scheme = urlparse(uri)[0]\n            use_datetime = kwargs.get('use_datetime', 0)\n            if scheme == 'https':\n                tcls = SafeTransport\n            else:\n                tcls = Transport\n            kwargs['transport'] = t = tcls(timeout, use_datetime=use_datetime)\n            self.transport = t\n        xmlrpclib.ServerProxy.__init__(self, uri, **kwargs)\n\n\n#\n# CSV functionality. This is provided because on 2.x, the csv module can't\n# handle Unicode. However, we need to deal with Unicode in e.g. RECORD files.\n#\n\n\ndef _csv_open(fn, mode, **kwargs):\n    if sys.version_info[0] < 3:\n        mode += 'b'\n    else:\n        kwargs['newline'] = ''\n        # Python 3 determines encoding from locale. Force 'utf-8'\n        # file encoding to match other forced utf-8 encoding\n        kwargs['encoding'] = 'utf-8'\n    return open(fn, mode, **kwargs)\n\n\nclass CSVBase(object):\n    defaults = {\n        'delimiter': str(','),  # The strs are used because we need native\n        'quotechar': str('\"'),  # str in the csv API (2.x won't take\n        'lineterminator': str('\\n')  # Unicode)\n    }\n\n    def __enter__(self):\n        return self\n\n    def __exit__(self, *exc_info):\n        self.stream.close()\n\n\nclass CSVReader(CSVBase):\n\n    def __init__(self, **kwargs):\n        if 'stream' in kwargs:\n            stream = kwargs['stream']\n            if sys.version_info[0] >= 3:\n                # needs to be a text stream\n                stream = codecs.getreader('utf-8')(stream)\n            self.stream = stream\n        else:\n            self.stream = _csv_open(kwargs['path'], 'r')\n        self.reader = csv.reader(self.stream, **self.defaults)\n\n    def __iter__(self):\n        return self\n\n    def next(self):\n        result = next(self.reader)\n        if sys.version_info[0] < 3:\n            for i, item in enumerate(result):\n                if not isinstance(item, text_type):\n                    result[i] = item.decode('utf-8')\n        return result\n\n    __next__ = next\n\n\nclass CSVWriter(CSVBase):\n\n    def __init__(self, fn, **kwargs):\n        self.stream = _csv_open(fn, 'w')\n        self.writer = csv.writer(self.stream, **self.defaults)\n\n    def writerow(self, row):\n        if sys.version_info[0] < 3:\n            r = []\n            for item in row:\n                if isinstance(item, text_type):\n                    item = item.encode('utf-8')\n                r.append(item)\n            row = r\n        self.writer.writerow(row)\n\n\n#\n#   Configurator functionality\n#\n\n\nclass Configurator(BaseConfigurator):\n\n    value_converters = dict(BaseConfigurator.value_converters)\n    value_converters['inc'] = 'inc_convert'\n\n    def __init__(self, config, base=None):\n        super(Configurator, self).__init__(config)\n        self.base = base or os.getcwd()\n\n    def configure_custom(self, config):\n\n        def convert(o):\n            if isinstance(o, (list, tuple)):\n                result = type(o)([convert(i) for i in o])\n            elif isinstance(o, dict):\n                if '()' in o:\n                    result = self.configure_custom(o)\n                else:\n                    result = {}\n                    for k in o:\n                        result[k] = convert(o[k])\n            else:\n                result = self.convert(o)\n            return result\n\n        c = config.pop('()')\n        if not callable(c):\n            c = self.resolve(c)\n        props = config.pop('.', None)\n        # Check for valid identifiers\n        args = config.pop('[]', ())\n        if args:\n            args = tuple([convert(o) for o in args])\n        items = [(k, convert(config[k])) for k in config if valid_ident(k)]\n        kwargs = dict(items)\n        result = c(*args, **kwargs)\n        if props:\n            for n, v in props.items():\n                setattr(result, n, convert(v))\n        return result\n\n    def __getitem__(self, key):\n        result = self.config[key]\n        if isinstance(result, dict) and '()' in result:\n            self.config[key] = result = self.configure_custom(result)\n        return result\n\n    def inc_convert(self, value):\n        \"\"\"Default converter for the inc:// protocol.\"\"\"\n        if not os.path.isabs(value):\n            value = os.path.join(self.base, value)\n        with codecs.open(value, 'r', encoding='utf-8') as f:\n            result = json.load(f)\n        return result\n\n\nclass SubprocessMixin(object):\n    \"\"\"\n    Mixin for running subprocesses and capturing their output\n    \"\"\"\n\n    def __init__(self, verbose=False, progress=None):\n        self.verbose = verbose\n        self.progress = progress\n\n    def reader(self, stream, context):\n        \"\"\"\n        Read lines from a subprocess' output stream and either pass to a progress\n        callable (if specified) or write progress information to sys.stderr.\n        \"\"\"\n        progress = self.progress\n        verbose = self.verbose\n        while True:\n            s = stream.readline()\n            if not s:\n                break\n            if progress is not None:\n                progress(s, context)\n            else:\n                if not verbose:\n                    sys.stderr.write('.')\n                else:\n                    sys.stderr.write(s.decode('utf-8'))\n                sys.stderr.flush()\n        stream.close()\n\n    def run_command(self, cmd, **kwargs):\n        p = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, **kwargs)\n        t1 = threading.Thread(target=self.reader, args=(p.stdout, 'stdout'))\n        t1.start()\n        t2 = threading.Thread(target=self.reader, args=(p.stderr, 'stderr'))\n        t2.start()\n        p.wait()\n        t1.join()\n        t2.join()\n        if self.progress is not None:\n            self.progress('done.', 'main')\n        elif self.verbose:\n            sys.stderr.write('done.\\n')\n        return p\n\n\ndef normalize_name(name):\n    \"\"\"Normalize a python package name a la PEP 503\"\"\"\n    # https://www.python.org/dev/peps/pep-0503/#normalized-names\n    return re.sub('[-_.]+', '-', name).lower()\n\n\n# def _get_pypirc_command():\n# \"\"\"\n# Get the distutils command for interacting with PyPI configurations.\n# :return: the command.\n# \"\"\"\n# from distutils.core import Distribution\n# from distutils.config import PyPIRCCommand\n# d = Distribution()\n# return PyPIRCCommand(d)\n\n\nclass PyPIRCFile(object):\n\n    DEFAULT_REPOSITORY = 'https://upload.pypi.org/legacy/'\n    DEFAULT_REALM = 'pypi'\n\n    def __init__(self, fn=None, url=None):\n        if fn is None:\n            fn = os.path.join(os.path.expanduser('~'), '.pypirc')\n        self.filename = fn\n        self.url = url\n\n    def read(self):\n        result = {}\n\n        if os.path.exists(self.filename):\n            repository = self.url or self.DEFAULT_REPOSITORY\n\n            config = configparser.RawConfigParser()\n            config.read(self.filename)\n            sections = config.sections()\n            if 'distutils' in sections:\n                # let's get the list of servers\n                index_servers = config.get('distutils', 'index-servers')\n                _servers = [server.strip() for server in index_servers.split('\\n') if server.strip() != '']\n                if _servers == []:\n                    # nothing set, let's try to get the default pypi\n                    if 'pypi' in sections:\n                        _servers = ['pypi']\n                else:\n                    for server in _servers:\n                        result = {'server': server}\n                        result['username'] = config.get(server, 'username')\n\n                        # optional params\n                        for key, default in (('repository', self.DEFAULT_REPOSITORY), ('realm', self.DEFAULT_REALM),\n                                             ('password', None)):\n                            if config.has_option(server, key):\n                                result[key] = config.get(server, key)\n                            else:\n                                result[key] = default\n\n                        # work around people having \"repository\" for the \"pypi\"\n                        # section of their config set to the HTTP (rather than\n                        # HTTPS) URL\n                        if (server == 'pypi' and repository in (self.DEFAULT_REPOSITORY, 'pypi')):\n                            result['repository'] = self.DEFAULT_REPOSITORY\n                        elif (result['server'] != repository and result['repository'] != repository):\n                            result = {}\n            elif 'server-login' in sections:\n                # old format\n                server = 'server-login'\n                if config.has_option(server, 'repository'):\n                    repository = config.get(server, 'repository')\n                else:\n                    repository = self.DEFAULT_REPOSITORY\n                result = {\n                    'username': config.get(server, 'username'),\n                    'password': config.get(server, 'password'),\n                    'repository': repository,\n                    'server': server,\n                    'realm': self.DEFAULT_REALM\n                }\n        return result\n\n    def update(self, username, password):\n        # import pdb; pdb.set_trace()\n        config = configparser.RawConfigParser()\n        fn = self.filename\n        config.read(fn)\n        if not config.has_section('pypi'):\n            config.add_section('pypi')\n        config.set('pypi', 'username', username)\n        config.set('pypi', 'password', password)\n        with open(fn, 'w') as f:\n            config.write(f)\n\n\ndef _load_pypirc(index):\n    \"\"\"\n    Read the PyPI access configuration as supported by distutils.\n    \"\"\"\n    return PyPIRCFile(url=index.url).read()\n\n\ndef _store_pypirc(index):\n    PyPIRCFile().update(index.username, index.password)\n\n\n#\n# get_platform()/get_host_platform() copied from Python 3.10.a0 source, with some minor\n# tweaks\n#\n\n\ndef get_host_platform():\n    \"\"\"Return a string that identifies the current platform.  This is used mainly to\n    distinguish platform-specific build directories and platform-specific built\n    distributions.  Typically includes the OS name and version and the\n    architecture (as supplied by 'os.uname()'), although the exact information\n    included depends on the OS; eg. on Linux, the kernel version isn't\n    particularly important.\n\n    Examples of returned values:\n       linux-i586\n       linux-alpha (?)\n       solaris-2.6-sun4u\n\n    Windows will return one of:\n       win-amd64 (64bit Windows on AMD64 (aka x86_64, Intel64, EM64T, etc)\n       win32 (all others - specifically, sys.platform is returned)\n\n    For other non-POSIX platforms, currently just returns 'sys.platform'.\n\n    \"\"\"\n    if os.name == 'nt':\n        if 'amd64' in sys.version.lower():\n            return 'win-amd64'\n        if '(arm)' in sys.version.lower():\n            return 'win-arm32'\n        if '(arm64)' in sys.version.lower():\n            return 'win-arm64'\n        return sys.platform\n\n    # Set for cross builds explicitly\n    if \"_PYTHON_HOST_PLATFORM\" in os.environ:\n        return os.environ[\"_PYTHON_HOST_PLATFORM\"]\n\n    if os.name != 'posix' or not hasattr(os, 'uname'):\n        # XXX what about the architecture? NT is Intel or Alpha,\n        # Mac OS is M68k or PPC, etc.\n        return sys.platform\n\n    # Try to distinguish various flavours of Unix\n\n    (osname, host, release, version, machine) = os.uname()\n\n    # Convert the OS name to lowercase, remove '/' characters, and translate\n    # spaces (for \"Power Macintosh\")\n    osname = osname.lower().replace('/', '')\n    machine = machine.replace(' ', '_').replace('/', '-')\n\n    if osname[:5] == 'linux':\n        # At least on Linux/Intel, 'machine' is the processor --\n        # i386, etc.\n        # XXX what about Alpha, SPARC, etc?\n        return \"%s-%s\" % (osname, machine)\n\n    elif osname[:5] == 'sunos':\n        if release[0] >= '5':  # SunOS 5 == Solaris 2\n            osname = 'solaris'\n            release = '%d.%s' % (int(release[0]) - 3, release[2:])\n            # We can't use 'platform.architecture()[0]' because a\n            # bootstrap problem. We use a dict to get an error\n            # if some suspicious happens.\n            bitness = {2147483647: '32bit', 9223372036854775807: '64bit'}\n            machine += '.%s' % bitness[sys.maxsize]\n        # fall through to standard osname-release-machine representation\n    elif osname[:3] == 'aix':\n        from _aix_support import aix_platform\n        return aix_platform()\n    elif osname[:6] == 'cygwin':\n        osname = 'cygwin'\n        rel_re = re.compile(r'[\\d.]+', re.ASCII)\n        m = rel_re.match(release)\n        if m:\n            release = m.group()\n    elif osname[:6] == 'darwin':\n        import _osx_support\n        try:\n            from distutils import sysconfig\n        except ImportError:\n            import sysconfig\n        osname, release, machine = _osx_support.get_platform_osx(sysconfig.get_config_vars(), osname, release, machine)\n\n    return '%s-%s-%s' % (osname, release, machine)\n\n\n_TARGET_TO_PLAT = {\n    'x86': 'win32',\n    'x64': 'win-amd64',\n    'arm': 'win-arm32',\n}\n\n\ndef get_platform():\n    if os.name != 'nt':\n        return get_host_platform()\n    cross_compilation_target = os.environ.get('VSCMD_ARG_TGT_ARCH')\n    if cross_compilation_target not in _TARGET_TO_PLAT:\n        return get_host_platform()\n    return _TARGET_TO_PLAT[cross_compilation_target]\n", "distlib/wheel.py": "# -*- coding: utf-8 -*-\n#\n# Copyright (C) 2013-2023 Vinay Sajip.\n# Licensed to the Python Software Foundation under a contributor agreement.\n# See LICENSE.txt and CONTRIBUTORS.txt.\n#\nfrom __future__ import unicode_literals\n\nimport base64\nimport codecs\nimport datetime\nfrom email import message_from_file\nimport hashlib\nimport json\nimport logging\nimport os\nimport posixpath\nimport re\nimport shutil\nimport sys\nimport tempfile\nimport zipfile\n\nfrom . import __version__, DistlibException\nfrom .compat import sysconfig, ZipFile, fsdecode, text_type, filter\nfrom .database import InstalledDistribution\nfrom .metadata import Metadata, WHEEL_METADATA_FILENAME, LEGACY_METADATA_FILENAME\nfrom .util import (FileOperator, convert_path, CSVReader, CSVWriter, Cache, cached_property, get_cache_base,\n                   read_exports, tempdir, get_platform)\nfrom .version import NormalizedVersion, UnsupportedVersionError\n\nlogger = logging.getLogger(__name__)\n\ncache = None  # created when needed\n\nif hasattr(sys, 'pypy_version_info'):  # pragma: no cover\n    IMP_PREFIX = 'pp'\nelif sys.platform.startswith('java'):  # pragma: no cover\n    IMP_PREFIX = 'jy'\nelif sys.platform == 'cli':  # pragma: no cover\n    IMP_PREFIX = 'ip'\nelse:\n    IMP_PREFIX = 'cp'\n\nVER_SUFFIX = sysconfig.get_config_var('py_version_nodot')\nif not VER_SUFFIX:  # pragma: no cover\n    VER_SUFFIX = '%s%s' % sys.version_info[:2]\nPYVER = 'py' + VER_SUFFIX\nIMPVER = IMP_PREFIX + VER_SUFFIX\n\nARCH = get_platform().replace('-', '_').replace('.', '_')\n\nABI = sysconfig.get_config_var('SOABI')\nif ABI and ABI.startswith('cpython-'):\n    ABI = ABI.replace('cpython-', 'cp').split('-')[0]\nelse:\n\n    def _derive_abi():\n        parts = ['cp', VER_SUFFIX]\n        if sysconfig.get_config_var('Py_DEBUG'):\n            parts.append('d')\n        if IMP_PREFIX == 'cp':\n            vi = sys.version_info[:2]\n            if vi < (3, 8):\n                wpm = sysconfig.get_config_var('WITH_PYMALLOC')\n                if wpm is None:\n                    wpm = True\n                if wpm:\n                    parts.append('m')\n                if vi < (3, 3):\n                    us = sysconfig.get_config_var('Py_UNICODE_SIZE')\n                    if us == 4 or (us is None and sys.maxunicode == 0x10FFFF):\n                        parts.append('u')\n        return ''.join(parts)\n\n    ABI = _derive_abi()\n    del _derive_abi\n\nFILENAME_RE = re.compile(\n    r'''\n(?P<nm>[^-]+)\n-(?P<vn>\\d+[^-]*)\n(-(?P<bn>\\d+[^-]*))?\n-(?P<py>\\w+\\d+(\\.\\w+\\d+)*)\n-(?P<bi>\\w+)\n-(?P<ar>\\w+(\\.\\w+)*)\n\\.whl$\n''', re.IGNORECASE | re.VERBOSE)\n\nNAME_VERSION_RE = re.compile(r'''\n(?P<nm>[^-]+)\n-(?P<vn>\\d+[^-]*)\n(-(?P<bn>\\d+[^-]*))?$\n''', re.IGNORECASE | re.VERBOSE)\n\nSHEBANG_RE = re.compile(br'\\s*#![^\\r\\n]*')\nSHEBANG_DETAIL_RE = re.compile(br'^(\\s*#!(\"[^\"]+\"|\\S+))\\s+(.*)$')\nSHEBANG_PYTHON = b'#!python'\nSHEBANG_PYTHONW = b'#!pythonw'\n\nif os.sep == '/':\n    to_posix = lambda o: o\nelse:\n    to_posix = lambda o: o.replace(os.sep, '/')\n\nif sys.version_info[0] < 3:\n    import imp\nelse:\n    imp = None\n    import importlib.machinery\n    import importlib.util\n\n\ndef _get_suffixes():\n    if imp:\n        return [s[0] for s in imp.get_suffixes()]\n    else:\n        return importlib.machinery.EXTENSION_SUFFIXES\n\n\ndef _load_dynamic(name, path):\n    # https://docs.python.org/3/library/importlib.html#importing-a-source-file-directly\n    if imp:\n        return imp.load_dynamic(name, path)\n    else:\n        spec = importlib.util.spec_from_file_location(name, path)\n        module = importlib.util.module_from_spec(spec)\n        sys.modules[name] = module\n        spec.loader.exec_module(module)\n        return module\n\n\nclass Mounter(object):\n\n    def __init__(self):\n        self.impure_wheels = {}\n        self.libs = {}\n\n    def add(self, pathname, extensions):\n        self.impure_wheels[pathname] = extensions\n        self.libs.update(extensions)\n\n    def remove(self, pathname):\n        extensions = self.impure_wheels.pop(pathname)\n        for k, v in extensions:\n            if k in self.libs:\n                del self.libs[k]\n\n    def find_module(self, fullname, path=None):\n        if fullname in self.libs:\n            result = self\n        else:\n            result = None\n        return result\n\n    def load_module(self, fullname):\n        if fullname in sys.modules:\n            result = sys.modules[fullname]\n        else:\n            if fullname not in self.libs:\n                raise ImportError('unable to find extension for %s' % fullname)\n            result = _load_dynamic(fullname, self.libs[fullname])\n            result.__loader__ = self\n            parts = fullname.rsplit('.', 1)\n            if len(parts) > 1:\n                result.__package__ = parts[0]\n        return result\n\n\n_hook = Mounter()\n\n\nclass Wheel(object):\n    \"\"\"\n    Class to build and install from Wheel files (PEP 427).\n    \"\"\"\n\n    wheel_version = (1, 1)\n    hash_kind = 'sha256'\n\n    def __init__(self, filename=None, sign=False, verify=False):\n        \"\"\"\n        Initialise an instance using a (valid) filename.\n        \"\"\"\n        self.sign = sign\n        self.should_verify = verify\n        self.buildver = ''\n        self.pyver = [PYVER]\n        self.abi = ['none']\n        self.arch = ['any']\n        self.dirname = os.getcwd()\n        if filename is None:\n            self.name = 'dummy'\n            self.version = '0.1'\n            self._filename = self.filename\n        else:\n            m = NAME_VERSION_RE.match(filename)\n            if m:\n                info = m.groupdict('')\n                self.name = info['nm']\n                # Reinstate the local version separator\n                self.version = info['vn'].replace('_', '-')\n                self.buildver = info['bn']\n                self._filename = self.filename\n            else:\n                dirname, filename = os.path.split(filename)\n                m = FILENAME_RE.match(filename)\n                if not m:\n                    raise DistlibException('Invalid name or '\n                                           'filename: %r' % filename)\n                if dirname:\n                    self.dirname = os.path.abspath(dirname)\n                self._filename = filename\n                info = m.groupdict('')\n                self.name = info['nm']\n                self.version = info['vn']\n                self.buildver = info['bn']\n                self.pyver = info['py'].split('.')\n                self.abi = info['bi'].split('.')\n                self.arch = info['ar'].split('.')\n\n    @property\n    def filename(self):\n        \"\"\"\n        Build and return a filename from the various components.\n        \"\"\"\n        if self.buildver:\n            buildver = '-' + self.buildver\n        else:\n            buildver = ''\n        pyver = '.'.join(self.pyver)\n        abi = '.'.join(self.abi)\n        arch = '.'.join(self.arch)\n        # replace - with _ as a local version separator\n        version = self.version.replace('-', '_')\n        return '%s-%s%s-%s-%s-%s.whl' % (self.name, version, buildver, pyver, abi, arch)\n\n    @property\n    def exists(self):\n        path = os.path.join(self.dirname, self.filename)\n        return os.path.isfile(path)\n\n    @property\n    def tags(self):\n        for pyver in self.pyver:\n            for abi in self.abi:\n                for arch in self.arch:\n                    yield pyver, abi, arch\n\n    @cached_property\n    def metadata(self):\n        pathname = os.path.join(self.dirname, self.filename)\n        name_ver = '%s-%s' % (self.name, self.version)\n        info_dir = '%s.dist-info' % name_ver\n        wrapper = codecs.getreader('utf-8')\n        with ZipFile(pathname, 'r') as zf:\n            self.get_wheel_metadata(zf)\n            # wv = wheel_metadata['Wheel-Version'].split('.', 1)\n            # file_version = tuple([int(i) for i in wv])\n            # if file_version < (1, 1):\n            # fns = [WHEEL_METADATA_FILENAME, METADATA_FILENAME,\n            # LEGACY_METADATA_FILENAME]\n            # else:\n            # fns = [WHEEL_METADATA_FILENAME, METADATA_FILENAME]\n            fns = [WHEEL_METADATA_FILENAME, LEGACY_METADATA_FILENAME]\n            result = None\n            for fn in fns:\n                try:\n                    metadata_filename = posixpath.join(info_dir, fn)\n                    with zf.open(metadata_filename) as bf:\n                        wf = wrapper(bf)\n                        result = Metadata(fileobj=wf)\n                        if result:\n                            break\n                except KeyError:\n                    pass\n            if not result:\n                raise ValueError('Invalid wheel, because metadata is '\n                                 'missing: looked in %s' % ', '.join(fns))\n        return result\n\n    def get_wheel_metadata(self, zf):\n        name_ver = '%s-%s' % (self.name, self.version)\n        info_dir = '%s.dist-info' % name_ver\n        metadata_filename = posixpath.join(info_dir, 'WHEEL')\n        with zf.open(metadata_filename) as bf:\n            wf = codecs.getreader('utf-8')(bf)\n            message = message_from_file(wf)\n        return dict(message)\n\n    @cached_property\n    def info(self):\n        pathname = os.path.join(self.dirname, self.filename)\n        with ZipFile(pathname, 'r') as zf:\n            result = self.get_wheel_metadata(zf)\n        return result\n\n    def process_shebang(self, data):\n        m = SHEBANG_RE.match(data)\n        if m:\n            end = m.end()\n            shebang, data_after_shebang = data[:end], data[end:]\n            # Preserve any arguments after the interpreter\n            if b'pythonw' in shebang.lower():\n                shebang_python = SHEBANG_PYTHONW\n            else:\n                shebang_python = SHEBANG_PYTHON\n            m = SHEBANG_DETAIL_RE.match(shebang)\n            if m:\n                args = b' ' + m.groups()[-1]\n            else:\n                args = b''\n            shebang = shebang_python + args\n            data = shebang + data_after_shebang\n        else:\n            cr = data.find(b'\\r')\n            lf = data.find(b'\\n')\n            if cr < 0 or cr > lf:\n                term = b'\\n'\n            else:\n                if data[cr:cr + 2] == b'\\r\\n':\n                    term = b'\\r\\n'\n                else:\n                    term = b'\\r'\n            data = SHEBANG_PYTHON + term + data\n        return data\n\n    def get_hash(self, data, hash_kind=None):\n        if hash_kind is None:\n            hash_kind = self.hash_kind\n        try:\n            hasher = getattr(hashlib, hash_kind)\n        except AttributeError:\n            raise DistlibException('Unsupported hash algorithm: %r' % hash_kind)\n        result = hasher(data).digest()\n        result = base64.urlsafe_b64encode(result).rstrip(b'=').decode('ascii')\n        return hash_kind, result\n\n    def write_record(self, records, record_path, archive_record_path):\n        records = list(records)  # make a copy, as mutated\n        records.append((archive_record_path, '', ''))\n        with CSVWriter(record_path) as writer:\n            for row in records:\n                writer.writerow(row)\n\n    def write_records(self, info, libdir, archive_paths):\n        records = []\n        distinfo, info_dir = info\n        # hasher = getattr(hashlib, self.hash_kind)\n        for ap, p in archive_paths:\n            with open(p, 'rb') as f:\n                data = f.read()\n            digest = '%s=%s' % self.get_hash(data)\n            size = os.path.getsize(p)\n            records.append((ap, digest, size))\n\n        p = os.path.join(distinfo, 'RECORD')\n        ap = to_posix(os.path.join(info_dir, 'RECORD'))\n        self.write_record(records, p, ap)\n        archive_paths.append((ap, p))\n\n    def build_zip(self, pathname, archive_paths):\n        with ZipFile(pathname, 'w', zipfile.ZIP_DEFLATED) as zf:\n            for ap, p in archive_paths:\n                logger.debug('Wrote %s to %s in wheel', p, ap)\n                zf.write(p, ap)\n\n    def build(self, paths, tags=None, wheel_version=None):\n        \"\"\"\n        Build a wheel from files in specified paths, and use any specified tags\n        when determining the name of the wheel.\n        \"\"\"\n        if tags is None:\n            tags = {}\n\n        libkey = list(filter(lambda o: o in paths, ('purelib', 'platlib')))[0]\n        if libkey == 'platlib':\n            is_pure = 'false'\n            default_pyver = [IMPVER]\n            default_abi = [ABI]\n            default_arch = [ARCH]\n        else:\n            is_pure = 'true'\n            default_pyver = [PYVER]\n            default_abi = ['none']\n            default_arch = ['any']\n\n        self.pyver = tags.get('pyver', default_pyver)\n        self.abi = tags.get('abi', default_abi)\n        self.arch = tags.get('arch', default_arch)\n\n        libdir = paths[libkey]\n\n        name_ver = '%s-%s' % (self.name, self.version)\n        data_dir = '%s.data' % name_ver\n        info_dir = '%s.dist-info' % name_ver\n\n        archive_paths = []\n\n        # First, stuff which is not in site-packages\n        for key in ('data', 'headers', 'scripts'):\n            if key not in paths:\n                continue\n            path = paths[key]\n            if os.path.isdir(path):\n                for root, dirs, files in os.walk(path):\n                    for fn in files:\n                        p = fsdecode(os.path.join(root, fn))\n                        rp = os.path.relpath(p, path)\n                        ap = to_posix(os.path.join(data_dir, key, rp))\n                        archive_paths.append((ap, p))\n                        if key == 'scripts' and not p.endswith('.exe'):\n                            with open(p, 'rb') as f:\n                                data = f.read()\n                            data = self.process_shebang(data)\n                            with open(p, 'wb') as f:\n                                f.write(data)\n\n        # Now, stuff which is in site-packages, other than the\n        # distinfo stuff.\n        path = libdir\n        distinfo = None\n        for root, dirs, files in os.walk(path):\n            if root == path:\n                # At the top level only, save distinfo for later\n                # and skip it for now\n                for i, dn in enumerate(dirs):\n                    dn = fsdecode(dn)\n                    if dn.endswith('.dist-info'):\n                        distinfo = os.path.join(root, dn)\n                        del dirs[i]\n                        break\n                assert distinfo, '.dist-info directory expected, not found'\n\n            for fn in files:\n                # comment out next suite to leave .pyc files in\n                if fsdecode(fn).endswith(('.pyc', '.pyo')):\n                    continue\n                p = os.path.join(root, fn)\n                rp = to_posix(os.path.relpath(p, path))\n                archive_paths.append((rp, p))\n\n        # Now distinfo. Assumed to be flat, i.e. os.listdir is enough.\n        files = os.listdir(distinfo)\n        for fn in files:\n            if fn not in ('RECORD', 'INSTALLER', 'SHARED', 'WHEEL'):\n                p = fsdecode(os.path.join(distinfo, fn))\n                ap = to_posix(os.path.join(info_dir, fn))\n                archive_paths.append((ap, p))\n\n        wheel_metadata = [\n            'Wheel-Version: %d.%d' % (wheel_version or self.wheel_version),\n            'Generator: distlib %s' % __version__,\n            'Root-Is-Purelib: %s' % is_pure,\n        ]\n        for pyver, abi, arch in self.tags:\n            wheel_metadata.append('Tag: %s-%s-%s' % (pyver, abi, arch))\n        p = os.path.join(distinfo, 'WHEEL')\n        with open(p, 'w') as f:\n            f.write('\\n'.join(wheel_metadata))\n        ap = to_posix(os.path.join(info_dir, 'WHEEL'))\n        archive_paths.append((ap, p))\n\n        # sort the entries by archive path. Not needed by any spec, but it\n        # keeps the archive listing and RECORD tidier than they would otherwise\n        # be. Use the number of path segments to keep directory entries together,\n        # and keep the dist-info stuff at the end.\n        def sorter(t):\n            ap = t[0]\n            n = ap.count('/')\n            if '.dist-info' in ap:\n                n += 10000\n            return (n, ap)\n\n        archive_paths = sorted(archive_paths, key=sorter)\n\n        # Now, at last, RECORD.\n        # Paths in here are archive paths - nothing else makes sense.\n        self.write_records((distinfo, info_dir), libdir, archive_paths)\n        # Now, ready to build the zip file\n        pathname = os.path.join(self.dirname, self.filename)\n        self.build_zip(pathname, archive_paths)\n        return pathname\n\n    def skip_entry(self, arcname):\n        \"\"\"\n        Determine whether an archive entry should be skipped when verifying\n        or installing.\n        \"\"\"\n        # The signature file won't be in RECORD,\n        # and we  don't currently don't do anything with it\n        # We also skip directories, as they won't be in RECORD\n        # either. See:\n        #\n        # https://github.com/pypa/wheel/issues/294\n        # https://github.com/pypa/wheel/issues/287\n        # https://github.com/pypa/wheel/pull/289\n        #\n        return arcname.endswith(('/', '/RECORD.jws'))\n\n    def install(self, paths, maker, **kwargs):\n        \"\"\"\n        Install a wheel to the specified paths. If kwarg ``warner`` is\n        specified, it should be a callable, which will be called with two\n        tuples indicating the wheel version of this software and the wheel\n        version in the file, if there is a discrepancy in the versions.\n        This can be used to issue any warnings to raise any exceptions.\n        If kwarg ``lib_only`` is True, only the purelib/platlib files are\n        installed, and the headers, scripts, data and dist-info metadata are\n        not written. If kwarg ``bytecode_hashed_invalidation`` is True, written\n        bytecode will try to use file-hash based invalidation (PEP-552) on\n        supported interpreter versions (CPython 3.7+).\n\n        The return value is a :class:`InstalledDistribution` instance unless\n        ``options.lib_only`` is True, in which case the return value is ``None``.\n        \"\"\"\n\n        dry_run = maker.dry_run\n        warner = kwargs.get('warner')\n        lib_only = kwargs.get('lib_only', False)\n        bc_hashed_invalidation = kwargs.get('bytecode_hashed_invalidation', False)\n\n        pathname = os.path.join(self.dirname, self.filename)\n        name_ver = '%s-%s' % (self.name, self.version)\n        data_dir = '%s.data' % name_ver\n        info_dir = '%s.dist-info' % name_ver\n\n        metadata_name = posixpath.join(info_dir, LEGACY_METADATA_FILENAME)\n        wheel_metadata_name = posixpath.join(info_dir, 'WHEEL')\n        record_name = posixpath.join(info_dir, 'RECORD')\n\n        wrapper = codecs.getreader('utf-8')\n\n        with ZipFile(pathname, 'r') as zf:\n            with zf.open(wheel_metadata_name) as bwf:\n                wf = wrapper(bwf)\n                message = message_from_file(wf)\n            wv = message['Wheel-Version'].split('.', 1)\n            file_version = tuple([int(i) for i in wv])\n            if (file_version != self.wheel_version) and warner:\n                warner(self.wheel_version, file_version)\n\n            if message['Root-Is-Purelib'] == 'true':\n                libdir = paths['purelib']\n            else:\n                libdir = paths['platlib']\n\n            records = {}\n            with zf.open(record_name) as bf:\n                with CSVReader(stream=bf) as reader:\n                    for row in reader:\n                        p = row[0]\n                        records[p] = row\n\n            data_pfx = posixpath.join(data_dir, '')\n            info_pfx = posixpath.join(info_dir, '')\n            script_pfx = posixpath.join(data_dir, 'scripts', '')\n\n            # make a new instance rather than a copy of maker's,\n            # as we mutate it\n            fileop = FileOperator(dry_run=dry_run)\n            fileop.record = True  # so we can rollback if needed\n\n            bc = not sys.dont_write_bytecode  # Double negatives. Lovely!\n\n            outfiles = []  # for RECORD writing\n\n            # for script copying/shebang processing\n            workdir = tempfile.mkdtemp()\n            # set target dir later\n            # we default add_launchers to False, as the\n            # Python Launcher should be used instead\n            maker.source_dir = workdir\n            maker.target_dir = None\n            try:\n                for zinfo in zf.infolist():\n                    arcname = zinfo.filename\n                    if isinstance(arcname, text_type):\n                        u_arcname = arcname\n                    else:\n                        u_arcname = arcname.decode('utf-8')\n                    if self.skip_entry(u_arcname):\n                        continue\n                    row = records[u_arcname]\n                    if row[2] and str(zinfo.file_size) != row[2]:\n                        raise DistlibException('size mismatch for '\n                                               '%s' % u_arcname)\n                    if row[1]:\n                        kind, value = row[1].split('=', 1)\n                        with zf.open(arcname) as bf:\n                            data = bf.read()\n                        _, digest = self.get_hash(data, kind)\n                        if digest != value:\n                            raise DistlibException('digest mismatch for '\n                                                   '%s' % arcname)\n\n                    if lib_only and u_arcname.startswith((info_pfx, data_pfx)):\n                        logger.debug('lib_only: skipping %s', u_arcname)\n                        continue\n                    is_script = (u_arcname.startswith(script_pfx) and not u_arcname.endswith('.exe'))\n\n                    if u_arcname.startswith(data_pfx):\n                        _, where, rp = u_arcname.split('/', 2)\n                        outfile = os.path.join(paths[where], convert_path(rp))\n                    else:\n                        # meant for site-packages.\n                        if u_arcname in (wheel_metadata_name, record_name):\n                            continue\n                        outfile = os.path.join(libdir, convert_path(u_arcname))\n                    if not is_script:\n                        with zf.open(arcname) as bf:\n                            fileop.copy_stream(bf, outfile)\n                        # Issue #147: permission bits aren't preserved. Using\n                        # zf.extract(zinfo, libdir) should have worked, but didn't,\n                        # see https://www.thetopsites.net/article/53834422.shtml\n                        # So ... manually preserve permission bits as given in zinfo\n                        if os.name == 'posix':\n                            # just set the normal permission bits\n                            os.chmod(outfile, (zinfo.external_attr >> 16) & 0x1FF)\n                        outfiles.append(outfile)\n                        # Double check the digest of the written file\n                        if not dry_run and row[1]:\n                            with open(outfile, 'rb') as bf:\n                                data = bf.read()\n                                _, newdigest = self.get_hash(data, kind)\n                                if newdigest != digest:\n                                    raise DistlibException('digest mismatch '\n                                                           'on write for '\n                                                           '%s' % outfile)\n                        if bc and outfile.endswith('.py'):\n                            try:\n                                pyc = fileop.byte_compile(outfile, hashed_invalidation=bc_hashed_invalidation)\n                                outfiles.append(pyc)\n                            except Exception:\n                                # Don't give up if byte-compilation fails,\n                                # but log it and perhaps warn the user\n                                logger.warning('Byte-compilation failed', exc_info=True)\n                    else:\n                        fn = os.path.basename(convert_path(arcname))\n                        workname = os.path.join(workdir, fn)\n                        with zf.open(arcname) as bf:\n                            fileop.copy_stream(bf, workname)\n\n                        dn, fn = os.path.split(outfile)\n                        maker.target_dir = dn\n                        filenames = maker.make(fn)\n                        fileop.set_executable_mode(filenames)\n                        outfiles.extend(filenames)\n\n                if lib_only:\n                    logger.debug('lib_only: returning None')\n                    dist = None\n                else:\n                    # Generate scripts\n\n                    # Try to get pydist.json so we can see if there are\n                    # any commands to generate. If this fails (e.g. because\n                    # of a legacy wheel), log a warning but don't give up.\n                    commands = None\n                    file_version = self.info['Wheel-Version']\n                    if file_version == '1.0':\n                        # Use legacy info\n                        ep = posixpath.join(info_dir, 'entry_points.txt')\n                        try:\n                            with zf.open(ep) as bwf:\n                                epdata = read_exports(bwf)\n                            commands = {}\n                            for key in ('console', 'gui'):\n                                k = '%s_scripts' % key\n                                if k in epdata:\n                                    commands['wrap_%s' % key] = d = {}\n                                    for v in epdata[k].values():\n                                        s = '%s:%s' % (v.prefix, v.suffix)\n                                        if v.flags:\n                                            s += ' [%s]' % ','.join(v.flags)\n                                        d[v.name] = s\n                        except Exception:\n                            logger.warning('Unable to read legacy script '\n                                           'metadata, so cannot generate '\n                                           'scripts')\n                    else:\n                        try:\n                            with zf.open(metadata_name) as bwf:\n                                wf = wrapper(bwf)\n                                commands = json.load(wf).get('extensions')\n                                if commands:\n                                    commands = commands.get('python.commands')\n                        except Exception:\n                            logger.warning('Unable to read JSON metadata, so '\n                                           'cannot generate scripts')\n                    if commands:\n                        console_scripts = commands.get('wrap_console', {})\n                        gui_scripts = commands.get('wrap_gui', {})\n                        if console_scripts or gui_scripts:\n                            script_dir = paths.get('scripts', '')\n                            if not os.path.isdir(script_dir):\n                                raise ValueError('Valid script path not '\n                                                 'specified')\n                            maker.target_dir = script_dir\n                            for k, v in console_scripts.items():\n                                script = '%s = %s' % (k, v)\n                                filenames = maker.make(script)\n                                fileop.set_executable_mode(filenames)\n\n                            if gui_scripts:\n                                options = {'gui': True}\n                                for k, v in gui_scripts.items():\n                                    script = '%s = %s' % (k, v)\n                                    filenames = maker.make(script, options)\n                                    fileop.set_executable_mode(filenames)\n\n                    p = os.path.join(libdir, info_dir)\n                    dist = InstalledDistribution(p)\n\n                    # Write SHARED\n                    paths = dict(paths)  # don't change passed in dict\n                    del paths['purelib']\n                    del paths['platlib']\n                    paths['lib'] = libdir\n                    p = dist.write_shared_locations(paths, dry_run)\n                    if p:\n                        outfiles.append(p)\n\n                    # Write RECORD\n                    dist.write_installed_files(outfiles, paths['prefix'], dry_run)\n                return dist\n            except Exception:  # pragma: no cover\n                logger.exception('installation failed.')\n                fileop.rollback()\n                raise\n            finally:\n                shutil.rmtree(workdir)\n\n    def _get_dylib_cache(self):\n        global cache\n        if cache is None:\n            # Use native string to avoid issues on 2.x: see Python #20140.\n            base = os.path.join(get_cache_base(), str('dylib-cache'), '%s.%s' % sys.version_info[:2])\n            cache = Cache(base)\n        return cache\n\n    def _get_extensions(self):\n        pathname = os.path.join(self.dirname, self.filename)\n        name_ver = '%s-%s' % (self.name, self.version)\n        info_dir = '%s.dist-info' % name_ver\n        arcname = posixpath.join(info_dir, 'EXTENSIONS')\n        wrapper = codecs.getreader('utf-8')\n        result = []\n        with ZipFile(pathname, 'r') as zf:\n            try:\n                with zf.open(arcname) as bf:\n                    wf = wrapper(bf)\n                    extensions = json.load(wf)\n                    cache = self._get_dylib_cache()\n                    prefix = cache.prefix_to_dir(pathname)\n                    cache_base = os.path.join(cache.base, prefix)\n                    if not os.path.isdir(cache_base):\n                        os.makedirs(cache_base)\n                    for name, relpath in extensions.items():\n                        dest = os.path.join(cache_base, convert_path(relpath))\n                        if not os.path.exists(dest):\n                            extract = True\n                        else:\n                            file_time = os.stat(dest).st_mtime\n                            file_time = datetime.datetime.fromtimestamp(file_time)\n                            info = zf.getinfo(relpath)\n                            wheel_time = datetime.datetime(*info.date_time)\n                            extract = wheel_time > file_time\n                        if extract:\n                            zf.extract(relpath, cache_base)\n                        result.append((name, dest))\n            except KeyError:\n                pass\n        return result\n\n    def is_compatible(self):\n        \"\"\"\n        Determine if a wheel is compatible with the running system.\n        \"\"\"\n        return is_compatible(self)\n\n    def is_mountable(self):\n        \"\"\"\n        Determine if a wheel is asserted as mountable by its metadata.\n        \"\"\"\n        return True  # for now - metadata details TBD\n\n    def mount(self, append=False):\n        pathname = os.path.abspath(os.path.join(self.dirname, self.filename))\n        if not self.is_compatible():\n            msg = 'Wheel %s not compatible with this Python.' % pathname\n            raise DistlibException(msg)\n        if not self.is_mountable():\n            msg = 'Wheel %s is marked as not mountable.' % pathname\n            raise DistlibException(msg)\n        if pathname in sys.path:\n            logger.debug('%s already in path', pathname)\n        else:\n            if append:\n                sys.path.append(pathname)\n            else:\n                sys.path.insert(0, pathname)\n            extensions = self._get_extensions()\n            if extensions:\n                if _hook not in sys.meta_path:\n                    sys.meta_path.append(_hook)\n                _hook.add(pathname, extensions)\n\n    def unmount(self):\n        pathname = os.path.abspath(os.path.join(self.dirname, self.filename))\n        if pathname not in sys.path:\n            logger.debug('%s not in path', pathname)\n        else:\n            sys.path.remove(pathname)\n            if pathname in _hook.impure_wheels:\n                _hook.remove(pathname)\n            if not _hook.impure_wheels:\n                if _hook in sys.meta_path:\n                    sys.meta_path.remove(_hook)\n\n    def verify(self):\n        pathname = os.path.join(self.dirname, self.filename)\n        name_ver = '%s-%s' % (self.name, self.version)\n        # data_dir = '%s.data' % name_ver\n        info_dir = '%s.dist-info' % name_ver\n\n        # metadata_name = posixpath.join(info_dir, LEGACY_METADATA_FILENAME)\n        wheel_metadata_name = posixpath.join(info_dir, 'WHEEL')\n        record_name = posixpath.join(info_dir, 'RECORD')\n\n        wrapper = codecs.getreader('utf-8')\n\n        with ZipFile(pathname, 'r') as zf:\n            with zf.open(wheel_metadata_name) as bwf:\n                wf = wrapper(bwf)\n                message_from_file(wf)\n            # wv = message['Wheel-Version'].split('.', 1)\n            # file_version = tuple([int(i) for i in wv])\n            # TODO version verification\n\n            records = {}\n            with zf.open(record_name) as bf:\n                with CSVReader(stream=bf) as reader:\n                    for row in reader:\n                        p = row[0]\n                        records[p] = row\n\n            for zinfo in zf.infolist():\n                arcname = zinfo.filename\n                if isinstance(arcname, text_type):\n                    u_arcname = arcname\n                else:\n                    u_arcname = arcname.decode('utf-8')\n                # See issue #115: some wheels have .. in their entries, but\n                # in the filename ... e.g. __main__..py ! So the check is\n                # updated to look for .. in the directory portions\n                p = u_arcname.split('/')\n                if '..' in p:\n                    raise DistlibException('invalid entry in '\n                                           'wheel: %r' % u_arcname)\n\n                if self.skip_entry(u_arcname):\n                    continue\n                row = records[u_arcname]\n                if row[2] and str(zinfo.file_size) != row[2]:\n                    raise DistlibException('size mismatch for '\n                                           '%s' % u_arcname)\n                if row[1]:\n                    kind, value = row[1].split('=', 1)\n                    with zf.open(arcname) as bf:\n                        data = bf.read()\n                    _, digest = self.get_hash(data, kind)\n                    if digest != value:\n                        raise DistlibException('digest mismatch for '\n                                               '%s' % arcname)\n\n    def update(self, modifier, dest_dir=None, **kwargs):\n        \"\"\"\n        Update the contents of a wheel in a generic way. The modifier should\n        be a callable which expects a dictionary argument: its keys are\n        archive-entry paths, and its values are absolute filesystem paths\n        where the contents the corresponding archive entries can be found. The\n        modifier is free to change the contents of the files pointed to, add\n        new entries and remove entries, before returning. This method will\n        extract the entire contents of the wheel to a temporary location, call\n        the modifier, and then use the passed (and possibly updated)\n        dictionary to write a new wheel. If ``dest_dir`` is specified, the new\n        wheel is written there -- otherwise, the original wheel is overwritten.\n\n        The modifier should return True if it updated the wheel, else False.\n        This method returns the same value the modifier returns.\n        \"\"\"\n\n        def get_version(path_map, info_dir):\n            version = path = None\n            key = '%s/%s' % (info_dir, LEGACY_METADATA_FILENAME)\n            if key not in path_map:\n                key = '%s/PKG-INFO' % info_dir\n            if key in path_map:\n                path = path_map[key]\n                version = Metadata(path=path).version\n            return version, path\n\n        def update_version(version, path):\n            updated = None\n            try:\n                NormalizedVersion(version)\n                i = version.find('-')\n                if i < 0:\n                    updated = '%s+1' % version\n                else:\n                    parts = [int(s) for s in version[i + 1:].split('.')]\n                    parts[-1] += 1\n                    updated = '%s+%s' % (version[:i], '.'.join(str(i) for i in parts))\n            except UnsupportedVersionError:\n                logger.debug('Cannot update non-compliant (PEP-440) '\n                             'version %r', version)\n            if updated:\n                md = Metadata(path=path)\n                md.version = updated\n                legacy = path.endswith(LEGACY_METADATA_FILENAME)\n                md.write(path=path, legacy=legacy)\n                logger.debug('Version updated from %r to %r', version, updated)\n\n        pathname = os.path.join(self.dirname, self.filename)\n        name_ver = '%s-%s' % (self.name, self.version)\n        info_dir = '%s.dist-info' % name_ver\n        record_name = posixpath.join(info_dir, 'RECORD')\n        with tempdir() as workdir:\n            with ZipFile(pathname, 'r') as zf:\n                path_map = {}\n                for zinfo in zf.infolist():\n                    arcname = zinfo.filename\n                    if isinstance(arcname, text_type):\n                        u_arcname = arcname\n                    else:\n                        u_arcname = arcname.decode('utf-8')\n                    if u_arcname == record_name:\n                        continue\n                    if '..' in u_arcname:\n                        raise DistlibException('invalid entry in '\n                                               'wheel: %r' % u_arcname)\n                    zf.extract(zinfo, workdir)\n                    path = os.path.join(workdir, convert_path(u_arcname))\n                    path_map[u_arcname] = path\n\n            # Remember the version.\n            original_version, _ = get_version(path_map, info_dir)\n            # Files extracted. Call the modifier.\n            modified = modifier(path_map, **kwargs)\n            if modified:\n                # Something changed - need to build a new wheel.\n                current_version, path = get_version(path_map, info_dir)\n                if current_version and (current_version == original_version):\n                    # Add or update local version to signify changes.\n                    update_version(current_version, path)\n                # Decide where the new wheel goes.\n                if dest_dir is None:\n                    fd, newpath = tempfile.mkstemp(suffix='.whl', prefix='wheel-update-', dir=workdir)\n                    os.close(fd)\n                else:\n                    if not os.path.isdir(dest_dir):\n                        raise DistlibException('Not a directory: %r' % dest_dir)\n                    newpath = os.path.join(dest_dir, self.filename)\n                archive_paths = list(path_map.items())\n                distinfo = os.path.join(workdir, info_dir)\n                info = distinfo, info_dir\n                self.write_records(info, workdir, archive_paths)\n                self.build_zip(newpath, archive_paths)\n                if dest_dir is None:\n                    shutil.copyfile(newpath, pathname)\n        return modified\n\n\ndef _get_glibc_version():\n    import platform\n    ver = platform.libc_ver()\n    result = []\n    if ver[0] == 'glibc':\n        for s in ver[1].split('.'):\n            result.append(int(s) if s.isdigit() else 0)\n        result = tuple(result)\n    return result\n\n\ndef compatible_tags():\n    \"\"\"\n    Return (pyver, abi, arch) tuples compatible with this Python.\n    \"\"\"\n    versions = [VER_SUFFIX]\n    major = VER_SUFFIX[0]\n    for minor in range(sys.version_info[1] - 1, -1, -1):\n        versions.append(''.join([major, str(minor)]))\n\n    abis = []\n    for suffix in _get_suffixes():\n        if suffix.startswith('.abi'):\n            abis.append(suffix.split('.', 2)[1])\n    abis.sort()\n    if ABI != 'none':\n        abis.insert(0, ABI)\n    abis.append('none')\n    result = []\n\n    arches = [ARCH]\n    if sys.platform == 'darwin':\n        m = re.match(r'(\\w+)_(\\d+)_(\\d+)_(\\w+)$', ARCH)\n        if m:\n            name, major, minor, arch = m.groups()\n            minor = int(minor)\n            matches = [arch]\n            if arch in ('i386', 'ppc'):\n                matches.append('fat')\n            if arch in ('i386', 'ppc', 'x86_64'):\n                matches.append('fat3')\n            if arch in ('ppc64', 'x86_64'):\n                matches.append('fat64')\n            if arch in ('i386', 'x86_64'):\n                matches.append('intel')\n            if arch in ('i386', 'x86_64', 'intel', 'ppc', 'ppc64'):\n                matches.append('universal')\n            while minor >= 0:\n                for match in matches:\n                    s = '%s_%s_%s_%s' % (name, major, minor, match)\n                    if s != ARCH:  # already there\n                        arches.append(s)\n                minor -= 1\n\n    # Most specific - our Python version, ABI and arch\n    for abi in abis:\n        for arch in arches:\n            result.append((''.join((IMP_PREFIX, versions[0])), abi, arch))\n            # manylinux\n            if abi != 'none' and sys.platform.startswith('linux'):\n                arch = arch.replace('linux_', '')\n                parts = _get_glibc_version()\n                if len(parts) == 2:\n                    if parts >= (2, 5):\n                        result.append((''.join((IMP_PREFIX, versions[0])), abi, 'manylinux1_%s' % arch))\n                    if parts >= (2, 12):\n                        result.append((''.join((IMP_PREFIX, versions[0])), abi, 'manylinux2010_%s' % arch))\n                    if parts >= (2, 17):\n                        result.append((''.join((IMP_PREFIX, versions[0])), abi, 'manylinux2014_%s' % arch))\n                    result.append((''.join(\n                        (IMP_PREFIX, versions[0])), abi, 'manylinux_%s_%s_%s' % (parts[0], parts[1], arch)))\n\n    # where no ABI / arch dependency, but IMP_PREFIX dependency\n    for i, version in enumerate(versions):\n        result.append((''.join((IMP_PREFIX, version)), 'none', 'any'))\n        if i == 0:\n            result.append((''.join((IMP_PREFIX, version[0])), 'none', 'any'))\n\n    # no IMP_PREFIX, ABI or arch dependency\n    for i, version in enumerate(versions):\n        result.append((''.join(('py', version)), 'none', 'any'))\n        if i == 0:\n            result.append((''.join(('py', version[0])), 'none', 'any'))\n\n    return set(result)\n\n\nCOMPATIBLE_TAGS = compatible_tags()\n\ndel compatible_tags\n\n\ndef is_compatible(wheel, tags=None):\n    if not isinstance(wheel, Wheel):\n        wheel = Wheel(wheel)  # assume it's a filename\n    result = False\n    if tags is None:\n        tags = COMPATIBLE_TAGS\n    for ver, abi, arch in tags:\n        if ver in wheel.pyver and abi in wheel.abi and arch in wheel.arch:\n            result = True\n            break\n    return result\n", "distlib/__init__.py": "# -*- coding: utf-8 -*-\n#\n# Copyright (C) 2012-2023 Vinay Sajip.\n# Licensed to the Python Software Foundation under a contributor agreement.\n# See LICENSE.txt and CONTRIBUTORS.txt.\n#\nimport logging\n\n__version__ = '0.3.9.dev0'\n\n\nclass DistlibException(Exception):\n    pass\n\n\ntry:\n    from logging import NullHandler\nexcept ImportError:  # pragma: no cover\n\n    class NullHandler(logging.Handler):\n\n        def handle(self, record):\n            pass\n\n        def emit(self, record):\n            pass\n\n        def createLock(self):\n            self.lock = None\n\n\nlogger = logging.getLogger(__name__)\nlogger.addHandler(NullHandler())\n", "distlib/compat.py": "# -*- coding: utf-8 -*-\n#\n# Copyright (C) 2013-2017 Vinay Sajip.\n# Licensed to the Python Software Foundation under a contributor agreement.\n# See LICENSE.txt and CONTRIBUTORS.txt.\n#\nfrom __future__ import absolute_import\n\nimport os\nimport re\nimport shutil\nimport sys\n\ntry:\n    import ssl\nexcept ImportError:  # pragma: no cover\n    ssl = None\n\nif sys.version_info[0] < 3:  # pragma: no cover\n    from StringIO import StringIO\n    string_types = basestring,\n    text_type = unicode\n    from types import FileType as file_type\n    import __builtin__ as builtins\n    import ConfigParser as configparser\n    from urlparse import urlparse, urlunparse, urljoin, urlsplit, urlunsplit\n    from urllib import (urlretrieve, quote as _quote, unquote, url2pathname,\n                        pathname2url, ContentTooShortError, splittype)\n\n    def quote(s):\n        if isinstance(s, unicode):\n            s = s.encode('utf-8')\n        return _quote(s)\n\n    import urllib2\n    from urllib2 import (Request, urlopen, URLError, HTTPError,\n                         HTTPBasicAuthHandler, HTTPPasswordMgr, HTTPHandler,\n                         HTTPRedirectHandler, build_opener)\n    if ssl:\n        from urllib2 import HTTPSHandler\n    import httplib\n    import xmlrpclib\n    import Queue as queue\n    from HTMLParser import HTMLParser\n    import htmlentitydefs\n    raw_input = raw_input\n    from itertools import ifilter as filter\n    from itertools import ifilterfalse as filterfalse\n\n    # Leaving this around for now, in case it needs resurrecting in some way\n    # _userprog = None\n    # def splituser(host):\n    # \"\"\"splituser('user[:passwd]@host[:port]') --> 'user[:passwd]', 'host[:port]'.\"\"\"\n    # global _userprog\n    # if _userprog is None:\n    # import re\n    # _userprog = re.compile('^(.*)@(.*)$')\n\n    # match = _userprog.match(host)\n    # if match: return match.group(1, 2)\n    # return None, host\n\nelse:  # pragma: no cover\n    from io import StringIO\n    string_types = str,\n    text_type = str\n    from io import TextIOWrapper as file_type\n    import builtins\n    import configparser\n    from urllib.parse import (urlparse, urlunparse, urljoin, quote, unquote,\n                              urlsplit, urlunsplit, splittype)\n    from urllib.request import (urlopen, urlretrieve, Request, url2pathname,\n                                pathname2url, HTTPBasicAuthHandler,\n                                HTTPPasswordMgr, HTTPHandler,\n                                HTTPRedirectHandler, build_opener)\n    if ssl:\n        from urllib.request import HTTPSHandler\n    from urllib.error import HTTPError, URLError, ContentTooShortError\n    import http.client as httplib\n    import urllib.request as urllib2\n    import xmlrpc.client as xmlrpclib\n    import queue\n    from html.parser import HTMLParser\n    import html.entities as htmlentitydefs\n    raw_input = input\n    from itertools import filterfalse\n    filter = filter\n\ntry:\n    from ssl import match_hostname, CertificateError\nexcept ImportError:  # pragma: no cover\n\n    class CertificateError(ValueError):\n        pass\n\n    def _dnsname_match(dn, hostname, max_wildcards=1):\n        \"\"\"Matching according to RFC 6125, section 6.4.3\n\n        http://tools.ietf.org/html/rfc6125#section-6.4.3\n        \"\"\"\n        pats = []\n        if not dn:\n            return False\n\n        parts = dn.split('.')\n        leftmost, remainder = parts[0], parts[1:]\n\n        wildcards = leftmost.count('*')\n        if wildcards > max_wildcards:\n            # Issue #17980: avoid denials of service by refusing more\n            # than one wildcard per fragment.  A survey of established\n            # policy among SSL implementations showed it to be a\n            # reasonable choice.\n            raise CertificateError(\n                \"too many wildcards in certificate DNS name: \" + repr(dn))\n\n        # speed up common case w/o wildcards\n        if not wildcards:\n            return dn.lower() == hostname.lower()\n\n        # RFC 6125, section 6.4.3, subitem 1.\n        # The client SHOULD NOT attempt to match a presented identifier in which\n        # the wildcard character comprises a label other than the left-most label.\n        if leftmost == '*':\n            # When '*' is a fragment by itself, it matches a non-empty dotless\n            # fragment.\n            pats.append('[^.]+')\n        elif leftmost.startswith('xn--') or hostname.startswith('xn--'):\n            # RFC 6125, section 6.4.3, subitem 3.\n            # The client SHOULD NOT attempt to match a presented identifier\n            # where the wildcard character is embedded within an A-label or\n            # U-label of an internationalized domain name.\n            pats.append(re.escape(leftmost))\n        else:\n            # Otherwise, '*' matches any dotless string, e.g. www*\n            pats.append(re.escape(leftmost).replace(r'\\*', '[^.]*'))\n\n        # add the remaining fragments, ignore any wildcards\n        for frag in remainder:\n            pats.append(re.escape(frag))\n\n        pat = re.compile(r'\\A' + r'\\.'.join(pats) + r'\\Z', re.IGNORECASE)\n        return pat.match(hostname)\n\n    def match_hostname(cert, hostname):\n        \"\"\"Verify that *cert* (in decoded format as returned by\n        SSLSocket.getpeercert()) matches the *hostname*.  RFC 2818 and RFC 6125\n        rules are followed, but IP addresses are not accepted for *hostname*.\n\n        CertificateError is raised on failure. On success, the function\n        returns nothing.\n        \"\"\"\n        if not cert:\n            raise ValueError(\"empty or no certificate, match_hostname needs a \"\n                             \"SSL socket or SSL context with either \"\n                             \"CERT_OPTIONAL or CERT_REQUIRED\")\n        dnsnames = []\n        san = cert.get('subjectAltName', ())\n        for key, value in san:\n            if key == 'DNS':\n                if _dnsname_match(value, hostname):\n                    return\n                dnsnames.append(value)\n        if not dnsnames:\n            # The subject is only checked when there is no dNSName entry\n            # in subjectAltName\n            for sub in cert.get('subject', ()):\n                for key, value in sub:\n                    # XXX according to RFC 2818, the most specific Common Name\n                    # must be used.\n                    if key == 'commonName':\n                        if _dnsname_match(value, hostname):\n                            return\n                        dnsnames.append(value)\n        if len(dnsnames) > 1:\n            raise CertificateError(\"hostname %r \"\n                                   \"doesn't match either of %s\" %\n                                   (hostname, ', '.join(map(repr, dnsnames))))\n        elif len(dnsnames) == 1:\n            raise CertificateError(\"hostname %r \"\n                                   \"doesn't match %r\" %\n                                   (hostname, dnsnames[0]))\n        else:\n            raise CertificateError(\"no appropriate commonName or \"\n                                   \"subjectAltName fields were found\")\n\n\ntry:\n    from types import SimpleNamespace as Container\nexcept ImportError:  # pragma: no cover\n\n    class Container(object):\n        \"\"\"\n        A generic container for when multiple values need to be returned\n        \"\"\"\n\n        def __init__(self, **kwargs):\n            self.__dict__.update(kwargs)\n\n\ntry:\n    from shutil import which\nexcept ImportError:  # pragma: no cover\n    # Implementation from Python 3.3\n    def which(cmd, mode=os.F_OK | os.X_OK, path=None):\n        \"\"\"Given a command, mode, and a PATH string, return the path which\n        conforms to the given mode on the PATH, or None if there is no such\n        file.\n\n        `mode` defaults to os.F_OK | os.X_OK. `path` defaults to the result\n        of os.environ.get(\"PATH\"), or can be overridden with a custom search\n        path.\n\n        \"\"\"\n\n        # Check that a given file can be accessed with the correct mode.\n        # Additionally check that `file` is not a directory, as on Windows\n        # directories pass the os.access check.\n        def _access_check(fn, mode):\n            return (os.path.exists(fn) and os.access(fn, mode) and not os.path.isdir(fn))\n\n        # If we're given a path with a directory part, look it up directly rather\n        # than referring to PATH directories. This includes checking relative to the\n        # current directory, e.g. ./script\n        if os.path.dirname(cmd):\n            if _access_check(cmd, mode):\n                return cmd\n            return None\n\n        if path is None:\n            path = os.environ.get(\"PATH\", os.defpath)\n        if not path:\n            return None\n        path = path.split(os.pathsep)\n\n        if sys.platform == \"win32\":\n            # The current directory takes precedence on Windows.\n            if os.curdir not in path:\n                path.insert(0, os.curdir)\n\n            # PATHEXT is necessary to check on Windows.\n            pathext = os.environ.get(\"PATHEXT\", \"\").split(os.pathsep)\n            # See if the given file matches any of the expected path extensions.\n            # This will allow us to short circuit when given \"python.exe\".\n            # If it does match, only test that one, otherwise we have to try\n            # others.\n            if any(cmd.lower().endswith(ext.lower()) for ext in pathext):\n                files = [cmd]\n            else:\n                files = [cmd + ext for ext in pathext]\n        else:\n            # On other platforms you don't have things like PATHEXT to tell you\n            # what file suffixes are executable, so just pass on cmd as-is.\n            files = [cmd]\n\n        seen = set()\n        for dir in path:\n            normdir = os.path.normcase(dir)\n            if normdir not in seen:\n                seen.add(normdir)\n                for thefile in files:\n                    name = os.path.join(dir, thefile)\n                    if _access_check(name, mode):\n                        return name\n        return None\n\n\n# ZipFile is a context manager in 2.7, but not in 2.6\n\nfrom zipfile import ZipFile as BaseZipFile\n\nif hasattr(BaseZipFile, '__enter__'):  # pragma: no cover\n    ZipFile = BaseZipFile\nelse:  # pragma: no cover\n    from zipfile import ZipExtFile as BaseZipExtFile\n\n    class ZipExtFile(BaseZipExtFile):\n\n        def __init__(self, base):\n            self.__dict__.update(base.__dict__)\n\n        def __enter__(self):\n            return self\n\n        def __exit__(self, *exc_info):\n            self.close()\n            # return None, so if an exception occurred, it will propagate\n\n    class ZipFile(BaseZipFile):\n\n        def __enter__(self):\n            return self\n\n        def __exit__(self, *exc_info):\n            self.close()\n            # return None, so if an exception occurred, it will propagate\n\n        def open(self, *args, **kwargs):\n            base = BaseZipFile.open(self, *args, **kwargs)\n            return ZipExtFile(base)\n\n\ntry:\n    from platform import python_implementation\nexcept ImportError:  # pragma: no cover\n\n    def python_implementation():\n        \"\"\"Return a string identifying the Python implementation.\"\"\"\n        if 'PyPy' in sys.version:\n            return 'PyPy'\n        if os.name == 'java':\n            return 'Jython'\n        if sys.version.startswith('IronPython'):\n            return 'IronPython'\n        return 'CPython'\n\n\nimport sysconfig\n\ntry:\n    callable = callable\nexcept NameError:  # pragma: no cover\n    from collections.abc import Callable\n\n    def callable(obj):\n        return isinstance(obj, Callable)\n\n\ntry:\n    fsencode = os.fsencode\n    fsdecode = os.fsdecode\nexcept AttributeError:  # pragma: no cover\n    # Issue #99: on some systems (e.g. containerised),\n    # sys.getfilesystemencoding() returns None, and we need a real value,\n    # so fall back to utf-8. From the CPython 2.7 docs relating to Unix and\n    # sys.getfilesystemencoding(): the return value is \"the user\u2019s preference\n    # according to the result of nl_langinfo(CODESET), or None if the\n    # nl_langinfo(CODESET) failed.\"\n    _fsencoding = sys.getfilesystemencoding() or 'utf-8'\n    if _fsencoding == 'mbcs':\n        _fserrors = 'strict'\n    else:\n        _fserrors = 'surrogateescape'\n\n    def fsencode(filename):\n        if isinstance(filename, bytes):\n            return filename\n        elif isinstance(filename, text_type):\n            return filename.encode(_fsencoding, _fserrors)\n        else:\n            raise TypeError(\"expect bytes or str, not %s\" %\n                            type(filename).__name__)\n\n    def fsdecode(filename):\n        if isinstance(filename, text_type):\n            return filename\n        elif isinstance(filename, bytes):\n            return filename.decode(_fsencoding, _fserrors)\n        else:\n            raise TypeError(\"expect bytes or str, not %s\" %\n                            type(filename).__name__)\n\n\ntry:\n    from tokenize import detect_encoding\nexcept ImportError:  # pragma: no cover\n    from codecs import BOM_UTF8, lookup\n\n    cookie_re = re.compile(r\"coding[:=]\\s*([-\\w.]+)\")\n\n    def _get_normal_name(orig_enc):\n        \"\"\"Imitates get_normal_name in tokenizer.c.\"\"\"\n        # Only care about the first 12 characters.\n        enc = orig_enc[:12].lower().replace(\"_\", \"-\")\n        if enc == \"utf-8\" or enc.startswith(\"utf-8-\"):\n            return \"utf-8\"\n        if enc in (\"latin-1\", \"iso-8859-1\", \"iso-latin-1\") or \\\n           enc.startswith((\"latin-1-\", \"iso-8859-1-\", \"iso-latin-1-\")):\n            return \"iso-8859-1\"\n        return orig_enc\n\n    def detect_encoding(readline):\n        \"\"\"\n        The detect_encoding() function is used to detect the encoding that should\n        be used to decode a Python source file.  It requires one argument, readline,\n        in the same way as the tokenize() generator.\n\n        It will call readline a maximum of twice, and return the encoding used\n        (as a string) and a list of any lines (left as bytes) it has read in.\n\n        It detects the encoding from the presence of a utf-8 bom or an encoding\n        cookie as specified in pep-0263.  If both a bom and a cookie are present,\n        but disagree, a SyntaxError will be raised.  If the encoding cookie is an\n        invalid charset, raise a SyntaxError.  Note that if a utf-8 bom is found,\n        'utf-8-sig' is returned.\n\n        If no encoding is specified, then the default of 'utf-8' will be returned.\n        \"\"\"\n        try:\n            filename = readline.__self__.name\n        except AttributeError:\n            filename = None\n        bom_found = False\n        encoding = None\n        default = 'utf-8'\n\n        def read_or_stop():\n            try:\n                return readline()\n            except StopIteration:\n                return b''\n\n        def find_cookie(line):\n            try:\n                # Decode as UTF-8. Either the line is an encoding declaration,\n                # in which case it should be pure ASCII, or it must be UTF-8\n                # per default encoding.\n                line_string = line.decode('utf-8')\n            except UnicodeDecodeError:\n                msg = \"invalid or missing encoding declaration\"\n                if filename is not None:\n                    msg = '{} for {!r}'.format(msg, filename)\n                raise SyntaxError(msg)\n\n            matches = cookie_re.findall(line_string)\n            if not matches:\n                return None\n            encoding = _get_normal_name(matches[0])\n            try:\n                codec = lookup(encoding)\n            except LookupError:\n                # This behaviour mimics the Python interpreter\n                if filename is None:\n                    msg = \"unknown encoding: \" + encoding\n                else:\n                    msg = \"unknown encoding for {!r}: {}\".format(\n                        filename, encoding)\n                raise SyntaxError(msg)\n\n            if bom_found:\n                if codec.name != 'utf-8':\n                    # This behaviour mimics the Python interpreter\n                    if filename is None:\n                        msg = 'encoding problem: utf-8'\n                    else:\n                        msg = 'encoding problem for {!r}: utf-8'.format(\n                            filename)\n                    raise SyntaxError(msg)\n                encoding += '-sig'\n            return encoding\n\n        first = read_or_stop()\n        if first.startswith(BOM_UTF8):\n            bom_found = True\n            first = first[3:]\n            default = 'utf-8-sig'\n        if not first:\n            return default, []\n\n        encoding = find_cookie(first)\n        if encoding:\n            return encoding, [first]\n\n        second = read_or_stop()\n        if not second:\n            return default, [first]\n\n        encoding = find_cookie(second)\n        if encoding:\n            return encoding, [first, second]\n\n        return default, [first, second]\n\n\n# For converting & <-> &amp; etc.\ntry:\n    from html import escape\nexcept ImportError:\n    from cgi import escape\nif sys.version_info[:2] < (3, 4):\n    unescape = HTMLParser().unescape\nelse:\n    from html import unescape\n\ntry:\n    from collections import ChainMap\nexcept ImportError:  # pragma: no cover\n    from collections import MutableMapping\n\n    try:\n        from reprlib import recursive_repr as _recursive_repr\n    except ImportError:\n\n        def _recursive_repr(fillvalue='...'):\n            '''\n            Decorator to make a repr function return fillvalue for a recursive\n            call\n            '''\n\n            def decorating_function(user_function):\n                repr_running = set()\n\n                def wrapper(self):\n                    key = id(self), get_ident()\n                    if key in repr_running:\n                        return fillvalue\n                    repr_running.add(key)\n                    try:\n                        result = user_function(self)\n                    finally:\n                        repr_running.discard(key)\n                    return result\n\n                # Can't use functools.wraps() here because of bootstrap issues\n                wrapper.__module__ = getattr(user_function, '__module__')\n                wrapper.__doc__ = getattr(user_function, '__doc__')\n                wrapper.__name__ = getattr(user_function, '__name__')\n                wrapper.__annotations__ = getattr(user_function,\n                                                  '__annotations__', {})\n                return wrapper\n\n            return decorating_function\n\n    class ChainMap(MutableMapping):\n        '''\n        A ChainMap groups multiple dicts (or other mappings) together\n        to create a single, updateable view.\n\n        The underlying mappings are stored in a list.  That list is public and can\n        accessed or updated using the *maps* attribute.  There is no other state.\n\n        Lookups search the underlying mappings successively until a key is found.\n        In contrast, writes, updates, and deletions only operate on the first\n        mapping.\n        '''\n\n        def __init__(self, *maps):\n            '''Initialize a ChainMap by setting *maps* to the given mappings.\n            If no mappings are provided, a single empty dictionary is used.\n\n            '''\n            self.maps = list(maps) or [{}]  # always at least one map\n\n        def __missing__(self, key):\n            raise KeyError(key)\n\n        def __getitem__(self, key):\n            for mapping in self.maps:\n                try:\n                    return mapping[\n                        key]  # can't use 'key in mapping' with defaultdict\n                except KeyError:\n                    pass\n            return self.__missing__(\n                key)  # support subclasses that define __missing__\n\n        def get(self, key, default=None):\n            return self[key] if key in self else default\n\n        def __len__(self):\n            return len(set().union(\n                *self.maps))  # reuses stored hash values if possible\n\n        def __iter__(self):\n            return iter(set().union(*self.maps))\n\n        def __contains__(self, key):\n            return any(key in m for m in self.maps)\n\n        def __bool__(self):\n            return any(self.maps)\n\n        @_recursive_repr()\n        def __repr__(self):\n            return '{0.__class__.__name__}({1})'.format(\n                self, ', '.join(map(repr, self.maps)))\n\n        @classmethod\n        def fromkeys(cls, iterable, *args):\n            'Create a ChainMap with a single dict created from the iterable.'\n            return cls(dict.fromkeys(iterable, *args))\n\n        def copy(self):\n            'New ChainMap or subclass with a new copy of maps[0] and refs to maps[1:]'\n            return self.__class__(self.maps[0].copy(), *self.maps[1:])\n\n        __copy__ = copy\n\n        def new_child(self):  # like Django's Context.push()\n            'New ChainMap with a new dict followed by all previous maps.'\n            return self.__class__({}, *self.maps)\n\n        @property\n        def parents(self):  # like Django's Context.pop()\n            'New ChainMap from maps[1:].'\n            return self.__class__(*self.maps[1:])\n\n        def __setitem__(self, key, value):\n            self.maps[0][key] = value\n\n        def __delitem__(self, key):\n            try:\n                del self.maps[0][key]\n            except KeyError:\n                raise KeyError(\n                    'Key not found in the first mapping: {!r}'.format(key))\n\n        def popitem(self):\n            'Remove and return an item pair from maps[0]. Raise KeyError is maps[0] is empty.'\n            try:\n                return self.maps[0].popitem()\n            except KeyError:\n                raise KeyError('No keys found in the first mapping.')\n\n        def pop(self, key, *args):\n            'Remove *key* from maps[0] and return its value. Raise KeyError if *key* not in maps[0].'\n            try:\n                return self.maps[0].pop(key, *args)\n            except KeyError:\n                raise KeyError(\n                    'Key not found in the first mapping: {!r}'.format(key))\n\n        def clear(self):\n            'Clear maps[0], leaving maps[1:] intact.'\n            self.maps[0].clear()\n\n\ntry:\n    from importlib.util import cache_from_source  # Python >= 3.4\nexcept ImportError:  # pragma: no cover\n\n    def cache_from_source(path, debug_override=None):\n        assert path.endswith('.py')\n        if debug_override is None:\n            debug_override = __debug__\n        if debug_override:\n            suffix = 'c'\n        else:\n            suffix = 'o'\n        return path + suffix\n\n\ntry:\n    from collections import OrderedDict\nexcept ImportError:  # pragma: no cover\n    # {{{ http://code.activestate.com/recipes/576693/ (r9)\n    # Backport of OrderedDict() class that runs on Python 2.4, 2.5, 2.6, 2.7 and pypy.\n    # Passes Python2.7's test suite and incorporates all the latest updates.\n    try:\n        from thread import get_ident as _get_ident\n    except ImportError:\n        from dummy_thread import get_ident as _get_ident\n\n    try:\n        from _abcoll import KeysView, ValuesView, ItemsView\n    except ImportError:\n        pass\n\n    class OrderedDict(dict):\n        'Dictionary that remembers insertion order'\n\n        # An inherited dict maps keys to values.\n        # The inherited dict provides __getitem__, __len__, __contains__, and get.\n        # The remaining methods are order-aware.\n        # Big-O running times for all methods are the same as for regular dictionaries.\n\n        # The internal self.__map dictionary maps keys to links in a doubly linked list.\n        # The circular doubly linked list starts and ends with a sentinel element.\n        # The sentinel element never gets deleted (this simplifies the algorithm).\n        # Each link is stored as a list of length three:  [PREV, NEXT, KEY].\n\n        def __init__(self, *args, **kwds):\n            '''Initialize an ordered dictionary.  Signature is the same as for\n            regular dictionaries, but keyword arguments are not recommended\n            because their insertion order is arbitrary.\n\n            '''\n            if len(args) > 1:\n                raise TypeError('expected at most 1 arguments, got %d' %\n                                len(args))\n            try:\n                self.__root\n            except AttributeError:\n                self.__root = root = []  # sentinel node\n                root[:] = [root, root, None]\n                self.__map = {}\n            self.__update(*args, **kwds)\n\n        def __setitem__(self, key, value, dict_setitem=dict.__setitem__):\n            'od.__setitem__(i, y) <==> od[i]=y'\n            # Setting a new item creates a new link which goes at the end of the linked\n            # list, and the inherited dictionary is updated with the new key/value pair.\n            if key not in self:\n                root = self.__root\n                last = root[0]\n                last[1] = root[0] = self.__map[key] = [last, root, key]\n            dict_setitem(self, key, value)\n\n        def __delitem__(self, key, dict_delitem=dict.__delitem__):\n            'od.__delitem__(y) <==> del od[y]'\n            # Deleting an existing item uses self.__map to find the link which is\n            # then removed by updating the links in the predecessor and successor nodes.\n            dict_delitem(self, key)\n            link_prev, link_next, key = self.__map.pop(key)\n            link_prev[1] = link_next\n            link_next[0] = link_prev\n\n        def __iter__(self):\n            'od.__iter__() <==> iter(od)'\n            root = self.__root\n            curr = root[1]\n            while curr is not root:\n                yield curr[2]\n                curr = curr[1]\n\n        def __reversed__(self):\n            'od.__reversed__() <==> reversed(od)'\n            root = self.__root\n            curr = root[0]\n            while curr is not root:\n                yield curr[2]\n                curr = curr[0]\n\n        def clear(self):\n            'od.clear() -> None.  Remove all items from od.'\n            try:\n                for node in self.__map.itervalues():\n                    del node[:]\n                root = self.__root\n                root[:] = [root, root, None]\n                self.__map.clear()\n            except AttributeError:\n                pass\n            dict.clear(self)\n\n        def popitem(self, last=True):\n            '''od.popitem() -> (k, v), return and remove a (key, value) pair.\n            Pairs are returned in LIFO order if last is true or FIFO order if false.\n\n            '''\n            if not self:\n                raise KeyError('dictionary is empty')\n            root = self.__root\n            if last:\n                link = root[0]\n                link_prev = link[0]\n                link_prev[1] = root\n                root[0] = link_prev\n            else:\n                link = root[1]\n                link_next = link[1]\n                root[1] = link_next\n                link_next[0] = root\n            key = link[2]\n            del self.__map[key]\n            value = dict.pop(self, key)\n            return key, value\n\n        # -- the following methods do not depend on the internal structure --\n\n        def keys(self):\n            'od.keys() -> list of keys in od'\n            return list(self)\n\n        def values(self):\n            'od.values() -> list of values in od'\n            return [self[key] for key in self]\n\n        def items(self):\n            'od.items() -> list of (key, value) pairs in od'\n            return [(key, self[key]) for key in self]\n\n        def iterkeys(self):\n            'od.iterkeys() -> an iterator over the keys in od'\n            return iter(self)\n\n        def itervalues(self):\n            'od.itervalues -> an iterator over the values in od'\n            for k in self:\n                yield self[k]\n\n        def iteritems(self):\n            'od.iteritems -> an iterator over the (key, value) items in od'\n            for k in self:\n                yield (k, self[k])\n\n        def update(*args, **kwds):\n            '''od.update(E, **F) -> None.  Update od from dict/iterable E and F.\n\n            If E is a dict instance, does:           for k in E: od[k] = E[k]\n            If E has a .keys() method, does:         for k in E.keys(): od[k] = E[k]\n            Or if E is an iterable of items, does:   for k, v in E: od[k] = v\n            In either case, this is followed by:     for k, v in F.items(): od[k] = v\n\n            '''\n            if len(args) > 2:\n                raise TypeError('update() takes at most 2 positional '\n                                'arguments (%d given)' % (len(args), ))\n            elif not args:\n                raise TypeError('update() takes at least 1 argument (0 given)')\n            self = args[0]\n            # Make progressively weaker assumptions about \"other\"\n            other = ()\n            if len(args) == 2:\n                other = args[1]\n            if isinstance(other, dict):\n                for key in other:\n                    self[key] = other[key]\n            elif hasattr(other, 'keys'):\n                for key in other.keys():\n                    self[key] = other[key]\n            else:\n                for key, value in other:\n                    self[key] = value\n            for key, value in kwds.items():\n                self[key] = value\n\n        __update = update  # let subclasses override update without breaking __init__\n\n        __marker = object()\n\n        def pop(self, key, default=__marker):\n            '''od.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n            If key is not found, d is returned if given, otherwise KeyError is raised.\n\n            '''\n            if key in self:\n                result = self[key]\n                del self[key]\n                return result\n            if default is self.__marker:\n                raise KeyError(key)\n            return default\n\n        def setdefault(self, key, default=None):\n            'od.setdefault(k[,d]) -> od.get(k,d), also set od[k]=d if k not in od'\n            if key in self:\n                return self[key]\n            self[key] = default\n            return default\n\n        def __repr__(self, _repr_running=None):\n            'od.__repr__() <==> repr(od)'\n            if not _repr_running:\n                _repr_running = {}\n            call_key = id(self), _get_ident()\n            if call_key in _repr_running:\n                return '...'\n            _repr_running[call_key] = 1\n            try:\n                if not self:\n                    return '%s()' % (self.__class__.__name__, )\n                return '%s(%r)' % (self.__class__.__name__, self.items())\n            finally:\n                del _repr_running[call_key]\n\n        def __reduce__(self):\n            'Return state information for pickling'\n            items = [[k, self[k]] for k in self]\n            inst_dict = vars(self).copy()\n            for k in vars(OrderedDict()):\n                inst_dict.pop(k, None)\n            if inst_dict:\n                return (self.__class__, (items, ), inst_dict)\n            return self.__class__, (items, )\n\n        def copy(self):\n            'od.copy() -> a shallow copy of od'\n            return self.__class__(self)\n\n        @classmethod\n        def fromkeys(cls, iterable, value=None):\n            '''OD.fromkeys(S[, v]) -> New ordered dictionary with keys from S\n            and values equal to v (which defaults to None).\n\n            '''\n            d = cls()\n            for key in iterable:\n                d[key] = value\n            return d\n\n        def __eq__(self, other):\n            '''od.__eq__(y) <==> od==y.  Comparison to another OD is order-sensitive\n            while comparison to a regular mapping is order-insensitive.\n\n            '''\n            if isinstance(other, OrderedDict):\n                return len(self) == len(\n                    other) and self.items() == other.items()\n            return dict.__eq__(self, other)\n\n        def __ne__(self, other):\n            return not self == other\n\n        # -- the following methods are only used in Python 2.7 --\n\n        def viewkeys(self):\n            \"od.viewkeys() -> a set-like object providing a view on od's keys\"\n            return KeysView(self)\n\n        def viewvalues(self):\n            \"od.viewvalues() -> an object providing a view on od's values\"\n            return ValuesView(self)\n\n        def viewitems(self):\n            \"od.viewitems() -> a set-like object providing a view on od's items\"\n            return ItemsView(self)\n\n\ntry:\n    from logging.config import BaseConfigurator, valid_ident\nexcept ImportError:  # pragma: no cover\n    IDENTIFIER = re.compile('^[a-z_][a-z0-9_]*$', re.I)\n\n    def valid_ident(s):\n        m = IDENTIFIER.match(s)\n        if not m:\n            raise ValueError('Not a valid Python identifier: %r' % s)\n        return True\n\n    # The ConvertingXXX classes are wrappers around standard Python containers,\n    # and they serve to convert any suitable values in the container. The\n    # conversion converts base dicts, lists and tuples to their wrapped\n    # equivalents, whereas strings which match a conversion format are converted\n    # appropriately.\n    #\n    # Each wrapper should have a configurator attribute holding the actual\n    # configurator to use for conversion.\n\n    class ConvertingDict(dict):\n        \"\"\"A converting dictionary wrapper.\"\"\"\n\n        def __getitem__(self, key):\n            value = dict.__getitem__(self, key)\n            result = self.configurator.convert(value)\n            # If the converted value is different, save for next time\n            if value is not result:\n                self[key] = result\n                if type(result) in (ConvertingDict, ConvertingList,\n                                    ConvertingTuple):\n                    result.parent = self\n                    result.key = key\n            return result\n\n        def get(self, key, default=None):\n            value = dict.get(self, key, default)\n            result = self.configurator.convert(value)\n            # If the converted value is different, save for next time\n            if value is not result:\n                self[key] = result\n                if type(result) in (ConvertingDict, ConvertingList,\n                                    ConvertingTuple):\n                    result.parent = self\n                    result.key = key\n            return result\n\n    def pop(self, key, default=None):\n        value = dict.pop(self, key, default)\n        result = self.configurator.convert(value)\n        if value is not result:\n            if type(result) in (ConvertingDict, ConvertingList,\n                                ConvertingTuple):\n                result.parent = self\n                result.key = key\n        return result\n\n    class ConvertingList(list):\n        \"\"\"A converting list wrapper.\"\"\"\n\n        def __getitem__(self, key):\n            value = list.__getitem__(self, key)\n            result = self.configurator.convert(value)\n            # If the converted value is different, save for next time\n            if value is not result:\n                self[key] = result\n                if type(result) in (ConvertingDict, ConvertingList,\n                                    ConvertingTuple):\n                    result.parent = self\n                    result.key = key\n            return result\n\n        def pop(self, idx=-1):\n            value = list.pop(self, idx)\n            result = self.configurator.convert(value)\n            if value is not result:\n                if type(result) in (ConvertingDict, ConvertingList,\n                                    ConvertingTuple):\n                    result.parent = self\n            return result\n\n    class ConvertingTuple(tuple):\n        \"\"\"A converting tuple wrapper.\"\"\"\n\n        def __getitem__(self, key):\n            value = tuple.__getitem__(self, key)\n            result = self.configurator.convert(value)\n            if value is not result:\n                if type(result) in (ConvertingDict, ConvertingList,\n                                    ConvertingTuple):\n                    result.parent = self\n                    result.key = key\n            return result\n\n    class BaseConfigurator(object):\n        \"\"\"\n        The configurator base class which defines some useful defaults.\n        \"\"\"\n\n        CONVERT_PATTERN = re.compile(r'^(?P<prefix>[a-z]+)://(?P<suffix>.*)$')\n\n        WORD_PATTERN = re.compile(r'^\\s*(\\w+)\\s*')\n        DOT_PATTERN = re.compile(r'^\\.\\s*(\\w+)\\s*')\n        INDEX_PATTERN = re.compile(r'^\\[\\s*(\\w+)\\s*\\]\\s*')\n        DIGIT_PATTERN = re.compile(r'^\\d+$')\n\n        value_converters = {\n            'ext': 'ext_convert',\n            'cfg': 'cfg_convert',\n        }\n\n        # We might want to use a different one, e.g. importlib\n        importer = staticmethod(__import__)\n\n        def __init__(self, config):\n            self.config = ConvertingDict(config)\n            self.config.configurator = self\n\n        def resolve(self, s):\n            \"\"\"\n            Resolve strings to objects using standard import and attribute\n            syntax.\n            \"\"\"\n            name = s.split('.')\n            used = name.pop(0)\n            try:\n                found = self.importer(used)\n                for frag in name:\n                    used += '.' + frag\n                    try:\n                        found = getattr(found, frag)\n                    except AttributeError:\n                        self.importer(used)\n                        found = getattr(found, frag)\n                return found\n            except ImportError:\n                e, tb = sys.exc_info()[1:]\n                v = ValueError('Cannot resolve %r: %s' % (s, e))\n                v.__cause__, v.__traceback__ = e, tb\n                raise v\n\n        def ext_convert(self, value):\n            \"\"\"Default converter for the ext:// protocol.\"\"\"\n            return self.resolve(value)\n\n        def cfg_convert(self, value):\n            \"\"\"Default converter for the cfg:// protocol.\"\"\"\n            rest = value\n            m = self.WORD_PATTERN.match(rest)\n            if m is None:\n                raise ValueError(\"Unable to convert %r\" % value)\n            else:\n                rest = rest[m.end():]\n                d = self.config[m.groups()[0]]\n                while rest:\n                    m = self.DOT_PATTERN.match(rest)\n                    if m:\n                        d = d[m.groups()[0]]\n                    else:\n                        m = self.INDEX_PATTERN.match(rest)\n                        if m:\n                            idx = m.groups()[0]\n                            if not self.DIGIT_PATTERN.match(idx):\n                                d = d[idx]\n                            else:\n                                try:\n                                    n = int(\n                                        idx\n                                    )  # try as number first (most likely)\n                                    d = d[n]\n                                except TypeError:\n                                    d = d[idx]\n                    if m:\n                        rest = rest[m.end():]\n                    else:\n                        raise ValueError('Unable to convert '\n                                         '%r at %r' % (value, rest))\n            # rest should be empty\n            return d\n\n        def convert(self, value):\n            \"\"\"\n            Convert values to an appropriate type. dicts, lists and tuples are\n            replaced by their converting alternatives. Strings are checked to\n            see if they have a conversion format and are converted if they do.\n            \"\"\"\n            if not isinstance(value, ConvertingDict) and isinstance(\n                    value, dict):\n                value = ConvertingDict(value)\n                value.configurator = self\n            elif not isinstance(value, ConvertingList) and isinstance(\n                    value, list):\n                value = ConvertingList(value)\n                value.configurator = self\n            elif not isinstance(value, ConvertingTuple) and isinstance(value, tuple):\n                value = ConvertingTuple(value)\n                value.configurator = self\n            elif isinstance(value, string_types):\n                m = self.CONVERT_PATTERN.match(value)\n                if m:\n                    d = m.groupdict()\n                    prefix = d['prefix']\n                    converter = self.value_converters.get(prefix, None)\n                    if converter:\n                        suffix = d['suffix']\n                        converter = getattr(self, converter)\n                        value = converter(suffix)\n            return value\n\n        def configure_custom(self, config):\n            \"\"\"Configure an object with a user-supplied factory.\"\"\"\n            c = config.pop('()')\n            if not callable(c):\n                c = self.resolve(c)\n            props = config.pop('.', None)\n            # Check for valid identifiers\n            kwargs = dict([(k, config[k]) for k in config if valid_ident(k)])\n            result = c(**kwargs)\n            if props:\n                for name, value in props.items():\n                    setattr(result, name, value)\n            return result\n\n        def as_tuple(self, value):\n            \"\"\"Utility function which converts lists to tuples.\"\"\"\n            if isinstance(value, list):\n                value = tuple(value)\n            return value\n", "distlib/markers.py": "# -*- coding: utf-8 -*-\n#\n# Copyright (C) 2012-2023 Vinay Sajip.\n# Licensed to the Python Software Foundation under a contributor agreement.\n# See LICENSE.txt and CONTRIBUTORS.txt.\n#\n\"\"\"\nParser for the environment markers micro-language defined in PEP 508.\n\"\"\"\n\n# Note: In PEP 345, the micro-language was Python compatible, so the ast\n# module could be used to parse it. However, PEP 508 introduced operators such\n# as ~= and === which aren't in Python, necessitating a different approach.\n\nimport os\nimport re\nimport sys\nimport platform\n\nfrom .compat import string_types\nfrom .util import in_venv, parse_marker\nfrom .version import LegacyVersion as LV\n\n__all__ = ['interpret']\n\n_VERSION_PATTERN = re.compile(r'((\\d+(\\.\\d+)*\\w*)|\\'(\\d+(\\.\\d+)*\\w*)\\'|\\\"(\\d+(\\.\\d+)*\\w*)\\\")')\n_VERSION_MARKERS = {'python_version', 'python_full_version'}\n\n\ndef _is_version_marker(s):\n    return isinstance(s, string_types) and s in _VERSION_MARKERS\n\n\ndef _is_literal(o):\n    if not isinstance(o, string_types) or not o:\n        return False\n    return o[0] in '\\'\"'\n\n\ndef _get_versions(s):\n    return {LV(m.groups()[0]) for m in _VERSION_PATTERN.finditer(s)}\n\n\nclass Evaluator(object):\n    \"\"\"\n    This class is used to evaluate marker expressions.\n    \"\"\"\n\n    operations = {\n        '==': lambda x, y: x == y,\n        '===': lambda x, y: x == y,\n        '~=': lambda x, y: x == y or x > y,\n        '!=': lambda x, y: x != y,\n        '<': lambda x, y: x < y,\n        '<=': lambda x, y: x == y or x < y,\n        '>': lambda x, y: x > y,\n        '>=': lambda x, y: x == y or x > y,\n        'and': lambda x, y: x and y,\n        'or': lambda x, y: x or y,\n        'in': lambda x, y: x in y,\n        'not in': lambda x, y: x not in y,\n    }\n\n    def evaluate(self, expr, context):\n        \"\"\"\n        Evaluate a marker expression returned by the :func:`parse_requirement`\n        function in the specified context.\n        \"\"\"\n        if isinstance(expr, string_types):\n            if expr[0] in '\\'\"':\n                result = expr[1:-1]\n            else:\n                if expr not in context:\n                    raise SyntaxError('unknown variable: %s' % expr)\n                result = context[expr]\n        else:\n            assert isinstance(expr, dict)\n            op = expr['op']\n            if op not in self.operations:\n                raise NotImplementedError('op not implemented: %s' % op)\n            elhs = expr['lhs']\n            erhs = expr['rhs']\n            if _is_literal(expr['lhs']) and _is_literal(expr['rhs']):\n                raise SyntaxError('invalid comparison: %s %s %s' % (elhs, op, erhs))\n\n            lhs = self.evaluate(elhs, context)\n            rhs = self.evaluate(erhs, context)\n            if ((_is_version_marker(elhs) or _is_version_marker(erhs)) and\n                    op in ('<', '<=', '>', '>=', '===', '==', '!=', '~=')):\n                lhs = LV(lhs)\n                rhs = LV(rhs)\n            elif _is_version_marker(elhs) and op in ('in', 'not in'):\n                lhs = LV(lhs)\n                rhs = _get_versions(rhs)\n            result = self.operations[op](lhs, rhs)\n        return result\n\n\n_DIGITS = re.compile(r'\\d+\\.\\d+')\n\n\ndef default_context():\n\n    def format_full_version(info):\n        version = '%s.%s.%s' % (info.major, info.minor, info.micro)\n        kind = info.releaselevel\n        if kind != 'final':\n            version += kind[0] + str(info.serial)\n        return version\n\n    if hasattr(sys, 'implementation'):\n        implementation_version = format_full_version(sys.implementation.version)\n        implementation_name = sys.implementation.name\n    else:\n        implementation_version = '0'\n        implementation_name = ''\n\n    ppv = platform.python_version()\n    m = _DIGITS.match(ppv)\n    pv = m.group(0)\n    result = {\n        'implementation_name': implementation_name,\n        'implementation_version': implementation_version,\n        'os_name': os.name,\n        'platform_machine': platform.machine(),\n        'platform_python_implementation': platform.python_implementation(),\n        'platform_release': platform.release(),\n        'platform_system': platform.system(),\n        'platform_version': platform.version(),\n        'platform_in_venv': str(in_venv()),\n        'python_full_version': ppv,\n        'python_version': pv,\n        'sys_platform': sys.platform,\n    }\n    return result\n\n\nDEFAULT_CONTEXT = default_context()\ndel default_context\n\nevaluator = Evaluator()\n\n\ndef interpret(marker, execution_context=None):\n    \"\"\"\n    Interpret a marker and return a result depending on environment.\n\n    :param marker: The marker to interpret.\n    :type marker: str\n    :param execution_context: The context used for name lookup.\n    :type execution_context: mapping\n    \"\"\"\n    try:\n        expr, rest = parse_marker(marker)\n    except Exception as e:\n        raise SyntaxError('Unable to interpret marker syntax: %s: %s' % (marker, e))\n    if rest and rest[0] != '#':\n        raise SyntaxError('unexpected trailing data in marker: %s: %s' % (marker, rest))\n    context = dict(DEFAULT_CONTEXT)\n    if execution_context:\n        context.update(execution_context)\n    return evaluator.evaluate(expr, context)\n"}