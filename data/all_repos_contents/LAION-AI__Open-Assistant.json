{"oasst-shared/tests/test_oasst_api_client.py": "from typing import Any\nfrom unittest import mock\nfrom uuid import uuid4\n\nimport aiohttp\nimport pytest\nfrom oasst_shared.api_client import OasstApiClient\nfrom oasst_shared.exceptions import OasstError, OasstErrorCode\nfrom oasst_shared.schemas import protocol as protocol_schema\n\n\n@pytest.fixture\ndef oasst_api_client_mocked():\n    \"\"\"\n    A an oasst_api_client pointed at the mocked backend.\n    Relies on ./scripts/backend-development/start-mock-server.sh\n    being run.\n    \"\"\"\n    client = OasstApiClient(backend_url=\"http://localhost:8080\", api_key=\"123\")\n    yield client\n    # TODO The fixture should close this connection, but there seems to be a bug\n    # with async fixtures and pytest.\n    # Since this only results in a warning, I'm leaving this for now.\n    # await client.close()\n\n\nclass MockClientSession(aiohttp.ClientSession):\n    response: Any\n\n    def set_response(self, response: Any):\n        self.response = response\n\n    async def post(self, *args, **kwargs):\n        return self.response\n\n\n@pytest.fixture\ndef mock_http_session():\n    yield MockClientSession()\n\n\n@pytest.fixture\ndef oasst_api_client_fake_http(mock_http_session):\n    \"\"\"\n    An oasst_api_client that uses a mocked http session. No real requests are made.\n    \"\"\"\n    client = OasstApiClient(backend_url=\"http://localhost:8080\", api_key=\"123\", session=mock_http_session)\n    yield client\n\n\n@pytest.mark.asyncio\n@pytest.mark.parametrize(\"task_type\", protocol_schema.TaskRequestType)\nasync def test_can_fetch_task(task_type: protocol_schema.TaskRequestType, oasst_api_client_mocked: OasstApiClient):\n    assert await oasst_api_client_mocked.fetch_task(task_type=task_type) is not None\n\n\n@pytest.mark.asyncio\nasync def test_can_ack_task(oasst_api_client_mocked: OasstApiClient):\n    await oasst_api_client_mocked.ack_task(task_id=uuid4(), message_id=\"123\")\n\n\n@pytest.mark.asyncio\nasync def test_can_nack_task(oasst_api_client_mocked: OasstApiClient):\n    await oasst_api_client_mocked.nack_task(task_id=uuid4(), reason=\"bad task\")\n\n\n@pytest.mark.asyncio\nasync def test_can_post_interaction(oasst_api_client_mocked: OasstApiClient):\n    assert (\n        await oasst_api_client_mocked.post_interaction(\n            protocol_schema.TextReplyToMessage(\n                type=\"text_reply_to_message\",\n                message_id=\"123\",\n                user_message_id=\"321\",\n                text=\"This is my reply\",\n                lang=\"en\",\n                user=protocol_schema.User(\n                    id=\"123\",\n                    display_name=\"lomz\",\n                    auth_method=\"discord\",\n                ),\n            )\n        )\n        is not None\n    )\n\n\n@pytest.mark.asyncio\nasync def test_can_handle_oasst_error_from_api(\n    oasst_api_client_fake_http: OasstApiClient,\n    mock_http_session: MockClientSession,\n):\n    # Return a 400 response with an OasstErrorResponse body\n    response_body = protocol_schema.OasstErrorResponse(\n        error_code=OasstErrorCode.GENERIC_ERROR,\n        message=\"Some error\",\n    )\n    status_code = 400\n\n    mock_http_session.set_response(\n        mock.AsyncMock(\n            status=status_code,\n            text=mock.AsyncMock(return_value=response_body.json()),\n            json=mock.AsyncMock(return_value=response_body.dict()),\n        )\n    )\n\n    with pytest.raises(OasstError):\n        await oasst_api_client_fake_http.post(\"/some-path\", data={})\n\n\n@pytest.mark.asyncio\nasync def test_can_handle_unknown_error_from_api(\n    oasst_api_client_fake_http: OasstApiClient,\n    mock_http_session: MockClientSession,\n):\n    response_body = \"Internal Server Error\"\n    status_code = 500\n\n    mock_http_session.set_response(\n        mock.AsyncMock(\n            status=status_code,\n            text=mock.AsyncMock(return_value=response_body),\n            json=mock.AsyncMock(return_value=None),\n        )\n    )\n\n    with pytest.raises(OasstError):\n        await oasst_api_client_fake_http.post(\"/some-path\", data={})\n", "oasst-shared/tests/__init__.py": "", "oasst-shared/oasst_shared/utils.py": "import hashlib\nimport time\nfrom datetime import datetime, timezone\nfrom functools import wraps\n\nfrom loguru import logger\n\nDELETED_USER_DISPLAY_NAME = \"Deleted User\"\nDELETED_USER_ID_PREFIX = \"deleted_\"\n\n\ndef utcnow() -> datetime:\n    \"\"\"Return the current utc date and time with tzinfo set to UTC.\"\"\"\n    return datetime.now(timezone.utc)\n\n\ndef unaware_to_utc(d: datetime | None) -> datetime:\n    \"\"\"Set timezeno to UTC if datetime is unaware (tzinfo == None).\"\"\"\n    if d and d.tzinfo is None:\n        return d.replace(tzinfo=timezone.utc)\n    return d\n\n\nclass TimerError(Exception):\n    \"\"\"A custom exception used to report errors in use of Timer class\"\"\"\n\n\nclass ScopeTimer:\n    def __init__(self):\n        self.start()\n\n    def start(self) -> None:\n        \"\"\"Measure new start time\"\"\"\n        self.start_time = time.perf_counter()\n\n    def stop(self) -> float:\n        \"\"\"Store and return the elapsed time\"\"\"\n        self.elapsed = time.perf_counter() - self.start_time\n        return self.elapsed\n\n    def __enter__(self):\n        \"\"\"Start a new timer as a context manager\"\"\"\n        self.start()\n        return self\n\n    def __exit__(self, *exc_info):\n        \"\"\"Stop the context manager timer\"\"\"\n        self.stop()\n\n\ndef log_timing(func=None, *, log_kwargs: bool = False, level: int | str = \"DEBUG\") -> None:\n    def decorator(func):\n        @wraps(func)\n        def wrapped(*args, **kwargs):\n            timer = ScopeTimer()\n            result = func(*args, **kwargs)\n            elapsed = timer.stop()\n            if log_kwargs:\n                kwargs = \", \".join([f\"{k}={v}\" for k, v in kwargs.items()])\n                logger.log(level, f\"Function '{func.__name__}({kwargs})' executed in {elapsed:f} s\")\n            else:\n                logger.log(level, f\"Function '{func.__name__}' executed in {elapsed:f} s\")\n            return result\n\n        return wrapped\n\n    if func and callable(func):\n        return decorator(func)\n    return decorator\n\n\ndef sha256_hash(key: str, seed: int) -> str:\n    return hashlib.sha256(f\"{key}{seed}\".encode(\"UTF-8\")).hexdigest()\n\n\nclass Anonymizer:\n    def __init__(self, seed, value_generator=lambda key, seed: sha256_hash(key, seed)):\n        self._map = {}\n        self._values = set()\n        self._seed = seed\n        self._gen = value_generator\n\n    def __getitem__(self, key):\n        if key not in self._map:\n            new_value = self._gen(key, self._seed)\n            if new_value in self._values:\n                raise ValueError(\"Generated value already exists. Try a different seed or value generator.\")\n            self._map[key] = new_value\n            self._values.add(new_value)\n        return self._map[key]\n\n    def anonymize(self, collection: str, key: str | None) -> str | None:\n        if key is None:\n            return None\n        return self[f\"{collection}:{key}\"]\n", "oasst-shared/oasst_shared/api_client.py": "\"\"\"API Client for interacting with the OASST backend.\"\"\"\nimport enum\nimport typing as t\nfrom http import HTTPStatus\nfrom typing import Optional, Type\nfrom uuid import UUID\n\nimport aiohttp\nfrom loguru import logger\nfrom oasst_shared.exceptions.oasst_api_error import OasstError, OasstErrorCode\nfrom oasst_shared.schemas import protocol as protocol_schema\nfrom pydantic import ValidationError\n\n\n# TODO: Move to `protocol`?\nclass TaskType(str, enum.Enum):\n    \"\"\"Task types.\"\"\"\n\n    summarize_story = \"summarize_story\"\n    rate_summary = \"rate_summary\"\n    initial_prompt = \"initial_prompt\"\n    prompter_reply = \"prompter_reply\"\n    assistant_reply = \"assistant_reply\"\n    rank_initial_prompts = \"rank_initial_prompts\"\n    rank_prompter_replies = \"rank_prompter_replies\"\n    rank_assistant_replies = \"rank_assistant_replies\"\n    label_initial_prompt = \"label_initial_prompt\"\n    label_assistant_reply = \"label_assistant_reply\"\n    label_prompter_reply = \"label_prompter_reply\"\n    done = \"task_done\"\n\n\nclass OasstApiClient:\n    \"\"\"API Client for interacting with the OASST backend.\"\"\"\n\n    def __init__(self, backend_url: str, api_key: str, session: Optional[aiohttp.ClientSession] = None):\n        \"\"\"Create a new OasstApiClient.\n\n        Args:\n        ----\n            backend_url (str): The base backend URL.\n            api_key (str): The API key to use for authentication.\n        \"\"\"\n\n        if session is None:\n            logger.debug(\"Opening OasstApiClient session\")\n            session = aiohttp.ClientSession()\n\n        self.session = session\n        self.backend_url = backend_url\n        self.api_key = api_key\n\n        self.task_models_map: dict[TaskType, Type[protocol_schema.Task]] = {\n            TaskType.summarize_story: protocol_schema.SummarizeStoryTask,\n            TaskType.rate_summary: protocol_schema.RateSummaryTask,\n            TaskType.initial_prompt: protocol_schema.InitialPromptTask,\n            TaskType.prompter_reply: protocol_schema.PrompterReplyTask,\n            TaskType.assistant_reply: protocol_schema.AssistantReplyTask,\n            TaskType.rank_initial_prompts: protocol_schema.RankInitialPromptsTask,\n            TaskType.rank_prompter_replies: protocol_schema.RankPrompterRepliesTask,\n            TaskType.rank_assistant_replies: protocol_schema.RankAssistantRepliesTask,\n            TaskType.label_initial_prompt: protocol_schema.LabelInitialPromptTask,\n            TaskType.label_prompter_reply: protocol_schema.LabelPrompterReplyTask,\n            TaskType.label_assistant_reply: protocol_schema.LabelAssistantReplyTask,\n            TaskType.done: protocol_schema.TaskDone,\n        }\n\n    async def post(self, path: str, data: dict[str, t.Any]) -> Optional[dict[str, t.Any]]:\n        \"\"\"Make a POST request to the backend.\"\"\"\n        logger.debug(f\"POST {self.backend_url}{path} DATA: {data}\")\n        response = await self.session.post(f\"{self.backend_url}{path}\", json=data, headers={\"x-api-key\": self.api_key})\n        logger.debug(f\"response: {response}\")\n\n        # If the response is not a 2XX, check to see\n        # if the json has the fields to create an\n        # OasstError.\n        if response.status >= 300:\n            text = await response.text()\n            logger.debug(f\"resp text: {text}\")\n            data = await response.json()\n            try:\n                oasst_error = protocol_schema.OasstErrorResponse(**(data or {}))\n                raise OasstError(\n                    error_code=oasst_error.error_code,\n                    message=oasst_error.message,\n                )\n            except ValidationError as e:\n                logger.debug(f\"Got error from API but could not parse: {e}\")\n\n                raw_response = await response.text()\n                logger.debug(f\"Raw response: {raw_response}\")\n\n                raise OasstError(\n                    raw_response,\n                    OasstErrorCode.GENERIC_ERROR,\n                    HTTPStatus(response.status),\n                )\n\n        if response.status == 204:\n            # No content\n            return None\n        return await response.json()\n\n    def _parse_task(self, data: Optional[dict[str, t.Any]]) -> protocol_schema.Task:\n        if data is None:\n            raise Exception(\"Cannot parse data as a task: data is none\")\n        task_type = TaskType(data.get(\"type\"))\n\n        model = self.task_models_map.get(task_type)\n        if not model:\n            logger.error(f\"Unsupported task type: {task_type}\")\n            raise ValueError(f\"Unsupported task type: {task_type}\")\n        return self.task_models_map[task_type].parse_obj(data)  # type: ignore\n\n    async def fetch_task(\n        self,\n        task_type: protocol_schema.TaskRequestType,\n        user: Optional[protocol_schema.User] = None,\n        collective: bool = False,\n        lang: Optional[str] = None,\n    ) -> protocol_schema.Task:\n        \"\"\"Fetch a task from the backend.\"\"\"\n        logger.debug(f\"Fetching task {task_type} for user {user}\")\n        req = protocol_schema.TaskRequest(type=task_type.value, user=user, collective=collective, lang=lang)\n        resp = await self.post(\"/api/v1/tasks/\", data=req.dict())\n        logger.debug(f\"RESP {resp}\")\n        return self._parse_task(resp)\n\n    async def fetch_random_task(\n        self, user: Optional[protocol_schema.User] = None, collective: bool = False, lang: Optional[str] = None\n    ) -> protocol_schema.Task:\n        \"\"\"Fetch a random task from the backend.\"\"\"\n        logger.debug(f\"Fetching random for user {user}\")\n        return await self.fetch_task(protocol_schema.TaskRequestType.random, user, collective, lang)\n\n    async def ack_task(self, task_id: str | UUID, message_id: str) -> None:\n        \"\"\"Send an ACK for a task to the backend.\"\"\"\n        logger.debug(f\"ACK task {task_id} with post {message_id}\")\n        req = protocol_schema.TaskAck(message_id=message_id)\n        await self.post(f\"/api/v1/tasks/{task_id}/ack\", data=req.dict())\n\n    async def nack_task(self, task_id: str | UUID, reason: str) -> None:\n        \"\"\"Send a NACK for a task to the backend.\"\"\"\n        logger.debug(f\"NACK task {task_id} with reason {reason}\")\n        req = protocol_schema.TaskNAck(reason=reason)\n        await self.post(f\"/api/v1/tasks/{task_id}/nack\", data=req.dict())\n\n    async def post_interaction(self, interaction: protocol_schema.Interaction) -> protocol_schema.Task:\n        \"\"\"Send a completed task to the backend.\"\"\"\n        logger.debug(f\"Interaction: {interaction}\")\n        resp = await self.post(\"/api/v1/tasks/interaction\", data=interaction.dict())\n        return self._parse_task(resp)\n\n    async def close(self):\n        logger.debug(\"Closing OasstApiClient session\")\n        await self.session.close()\n", "oasst-shared/oasst_shared/model_configs.py": "import pydantic\n\n\nclass ModelConfig(pydantic.BaseModel):\n    model_id: str\n    max_input_length: int = 512\n    max_total_length: int = 1024\n    quantized: bool = False\n\n    @property\n    def is_llama(self) -> bool:\n        return \"llama\" in self.model_id.lower()\n\n    @property\n    def is_lorem(self) -> bool:\n        return self.model_id == \"_lorem\"\n\n    @property\n    def compat_hash(self) -> str:\n        return f\"{self.model_id}-{self.max_total_length}-{self.max_input_length}-{'q' if self.quantized else 'f'}\"\n\n\nMODEL_CONFIGS = {\n    \"_lorem\": ModelConfig(\n        model_id=\"_lorem\",\n        max_input_length=128,\n        max_total_length=256,\n    ),\n    \"distilgpt2\": ModelConfig(\n        model_id=\"distilgpt2\",\n        max_input_length=512,\n        max_total_length=1024,\n    ),\n    \"OA_SFT_Pythia_12B\": ModelConfig(\n        model_id=\"OpenAssistant/oasst-sft-1-pythia-12b\",\n        max_input_length=1024,\n        max_total_length=2048,\n    ),\n    \"OA_SFT_Pythia_12Bq\": ModelConfig(\n        model_id=\"OpenAssistant/oasst-sft-1-pythia-12b\",\n        max_input_length=1024,\n        max_total_length=2048,\n        quantized=True,\n    ),\n    \"OA_SFT_Pythia_12B_4\": ModelConfig(\n        model_id=\"OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5\",\n        max_input_length=1024,\n        max_total_length=2048,\n    ),\n    \"OA_SFT_Pythia_12Bq_4\": ModelConfig(\n        model_id=\"OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5\",\n        max_input_length=1024,\n        max_total_length=2048,\n        quantized=True,\n    ),\n    \"OA_SFT_Llama_7B\": ModelConfig(\n        model_id=\"OpenAssistant/oasst_sft_llama_7b_mask_1000\",\n        max_input_length=1024,\n        max_total_length=2048,\n    ),\n    \"OA_SFT_Llama_13B\": ModelConfig(\n        model_id=\"OpenAssistant/oasst_sft_llama_13b_mask_1500\",\n        max_input_length=1024,\n        max_total_length=2048,\n    ),\n    \"OA_SFT_Llama_13Bq\": ModelConfig(\n        model_id=\"OpenAssistant/oasst_sft_llama_13b_mask_1500\",\n        max_input_length=1024,\n        max_total_length=2048,\n        quantized=True,\n    ),\n    \"OA_SFT_Llama_30B\": ModelConfig(\n        model_id=\"OpenAssistant/llama_30b_oasst_latcyr_1000\",\n        max_input_length=1024,\n        max_total_length=1792,  # seeing OOMs on 2048 on an A100 80GB\n    ),\n    \"OA_SFT_Llama_30Bq\": ModelConfig(\n        model_id=\"OpenAssistant/llama_30b_oasst_latcyr_1000\",\n        max_input_length=1024,\n        max_total_length=1792,  # an a100 40GB can't handle 2048\n        quantized=True,\n    ),\n    \"OA_SFT_Llama_30B_2\": ModelConfig(\n        model_id=\"OpenAssistant/llama_30b_oasst_latcyr_400\",\n        max_input_length=1024,\n        max_total_length=1792,\n    ),\n    \"OA_SFT_Llama_30Bq_2\": ModelConfig(\n        model_id=\"OpenAssistant/llama_30b_oasst_latcyr_400\",\n        max_input_length=1024,\n        max_total_length=1792,  # an a100 40GB can't handle 2048\n        quantized=True,\n    ),\n    \"OA_SFT_Llama_30B_5\": ModelConfig(\n        model_id=\"OpenAssistant/oasst-sft-5-llama-30b-epoch-1\",\n        max_input_length=1024,\n        max_total_length=1792,  # seeing OOMs on 2048 on an A100 80GB\n    ),\n    \"OA_SFT_Llama_30Bq_5\": ModelConfig(\n        model_id=\"OpenAssistant/oasst-sft-5-llama-30b-epoch-1\",\n        max_input_length=1024,\n        max_total_length=1792,  # seeing OOMs on 2048 on an A100 80GB\n        quantized=True,\n    ),\n    \"OA_SFT_Llama_30B_6\": ModelConfig(\n        model_id=\"OpenAssistant/oasst-sft-6-llama-30b\",\n        max_input_length=1024,\n        max_total_length=1792,  # seeing OOMs on 2048 on an A100 80GB\n    ),\n    \"OA_SFT_Llama_30Bq_6\": ModelConfig(\n        model_id=\"OpenAssistant/oasst-sft-6-llama-30b\",\n        max_input_length=1024,\n        max_total_length=1792,  # seeing OOMs on 2048 on an A100 80GB\n        quantized=True,\n    ),\n    \"OA_SFT_Llama_30B_7\": ModelConfig(\n        model_id=\"OpenAssistant/oasst-sft-7-llama-30b\",\n        max_input_length=1024,\n        max_total_length=1792,  # seeing OOMs on 2048 on an A100 80GB\n    ),\n    \"OA_SFT_Llama_30Bq_7\": ModelConfig(\n        model_id=\"OpenAssistant/oasst-sft-7-llama-30b\",\n        max_input_length=1024,\n        max_total_length=1792,  # seeing OOMs on 2048 on an A100 80GB\n        quantized=True,\n    ),\n    \"OA_SFT_Llama_30B_7e3\": ModelConfig(\n        model_id=\"OpenAssistant/oasst-sft-7e3-llama-30b\",\n        max_input_length=1024,\n        max_total_length=1792,  # seeing OOMs on 2048 on an A100 80GB\n    ),\n    \"OA_RLHF_Llama_30B_2_7k\": ModelConfig(\n        model_id=\"OpenAssistant/oasst-rlhf-2-llama-30b-7k-steps\",\n        max_input_length=1024,\n        max_total_length=1792,  # seeing OOMs on 2048 on an A100 80GB\n    ),\n    \"Carper_RLHF_13B_1\": ModelConfig(\n        model_id=\"CarperAI/vicuna-13b-fine-tuned-rlhf\",\n        max_input_length=1024,\n        max_total_length=2048,\n    ),\n    \"Carper_RLHF_13Bq_1\": ModelConfig(\n        model_id=\"CarperAI/vicuna-13b-fine-tuned-rlhf\",\n        max_input_length=1024,\n        max_total_length=2048,\n        quantized=True,\n    ),\n    \"OA_SFT_Llama2_70B_10\": ModelConfig(\n        model_id=\"OpenAssistant/llama2-70b-oasst-sft-v10\",\n        max_input_length=3072,\n        max_total_length=4096,\n    ),\n    \"OA_SFT_CodeLlama_13B_10\": ModelConfig(\n        model_id=\"OpenAssistant/codellama-13b-oasst-sft-v10\",\n        max_input_length=8192,\n        max_total_length=12288,\n    ),\n}\n", "oasst-shared/oasst_shared/__init__.py": "", "oasst-shared/oasst_shared/schemas/protocol.py": "import enum\nfrom datetime import datetime\nfrom typing import List, Literal, Optional, Union\nfrom uuid import UUID, uuid4\n\nimport pydantic\nfrom oasst_shared.exceptions import OasstErrorCode\nfrom pydantic import BaseModel, Field, conint, conlist, constr\n\n\nclass TaskRequestType(str, enum.Enum):\n    random = \"random\"\n    summarize_story = \"summarize_story\"\n    rate_summary = \"rate_summary\"\n    initial_prompt = \"initial_prompt\"\n    prompter_reply = \"prompter_reply\"\n    assistant_reply = \"assistant_reply\"\n    rank_initial_prompts = \"rank_initial_prompts\"\n    rank_prompter_replies = \"rank_prompter_replies\"\n    rank_assistant_replies = \"rank_assistant_replies\"\n    label_initial_prompt = \"label_initial_prompt\"\n    label_assistant_reply = \"label_assistant_reply\"\n    label_prompter_reply = \"label_prompter_reply\"\n\n\nclass User(BaseModel):\n    id: str\n    display_name: str\n    auth_method: Literal[\"discord\", \"google\", \"local\", \"system\"]\n\n\nclass Account(BaseModel):\n    id: UUID\n    provider: str\n    provider_account_id: str\n\n\nclass Token(BaseModel):\n    access_token: str\n    token_type: str\n\n\nclass TokenPair(BaseModel):\n    access_token: Token\n    refresh_token: Token\n\n\nclass FrontEndUser(User):\n    user_id: UUID\n    enabled: bool\n    deleted: bool\n    notes: str\n    created_date: Optional[datetime] = None\n    show_on_leaderboard: bool\n    streak_days: Optional[int] = None\n    streak_last_day_date: Optional[datetime] = None\n    last_activity_date: Optional[datetime] = None\n    tos_acceptance_date: Optional[datetime] = None\n\n\nclass PageResult(BaseModel):\n    prev: str | None\n    next: str | None\n    sort_key: str\n    items: list\n    order: Literal[\"asc\", \"desc\"]\n\n\nclass FrontEndUserPage(PageResult):\n    items: list[FrontEndUser]\n\n\nclass ConversationMessage(BaseModel):\n    \"\"\"Represents a message in a conversation between the user and the assistant.\"\"\"\n\n    id: Optional[UUID]\n    user_id: Optional[UUID]\n    frontend_message_id: Optional[str]\n    text: str\n    lang: Optional[str]  # BCP 47\n    is_assistant: bool\n    emojis: Optional[dict[str, int]]\n    user_emojis: Optional[list[str]]\n    user_is_author: Optional[bool]\n    synthetic: Optional[bool]\n\n\nclass Conversation(BaseModel):\n    \"\"\"Represents a conversation between the prompter and the assistant.\"\"\"\n\n    messages: list[ConversationMessage] = []\n\n    def __len__(self):\n        return len(self.messages)\n\n    @property\n    def is_prompter_turn(self) -> bool:\n        if len(self) == 0:\n            return True\n        last_message = self.messages[-1]\n        if last_message.is_assistant:\n            return True\n        return False\n\n\nclass Message(ConversationMessage):\n    parent_id: Optional[UUID]\n    created_date: Optional[datetime]\n    review_result: Optional[bool]\n    review_count: Optional[int]\n    deleted: Optional[bool]\n    edited: Optional[bool]\n    model_name: Optional[str]\n    message_tree_id: Optional[UUID]\n    ranking_count: Optional[int]\n    rank: Optional[int]\n    user: Optional[FrontEndUser]\n\n\nclass MessageRevision(BaseModel):\n    id: UUID\n    text: str\n    message_id: UUID\n    user_id: Optional[UUID]\n    created_date: Optional[datetime]\n    user_is_author: Optional[bool]\n\n\nclass MessagePage(PageResult):\n    items: list[Message]\n\n\nclass MessageTree(BaseModel):\n    \"\"\"All messages belonging to the same message tree.\"\"\"\n\n    id: UUID\n    messages: list[Message] = []\n\n\nclass TaskRequest(BaseModel):\n    \"\"\"The frontend asks the backend for a task.\"\"\"\n\n    type: TaskRequestType = TaskRequestType.random\n    # Must use Field(..., nullable=True) to indicate to the OpenAPI schema that\n    # this is optional. https://github.com/pydantic/pydantic/issues/1270\n    user: Optional[User] = Field(None, nullable=True)\n    collective: bool = False\n    lang: Optional[str] = Field(None, nullable=True)  # BCP 47\n\n\nclass TaskAck(BaseModel):\n    \"\"\"The frontend acknowledges that it has received a task and created a message.\"\"\"\n\n    message_id: str\n\n\nclass TaskNAck(BaseModel):\n    \"\"\"The frontend acknowledges that it has received a task but cannot create a message.\"\"\"\n\n    reason: str | None = Field(None, nullable=True)\n\n\nclass TaskClose(BaseModel):\n    \"\"\"The frontend asks to mark task as done\"\"\"\n\n    message_id: str\n\n\nclass Task(BaseModel):\n    \"\"\"A task is a unit of work that the backend gives to the frontend.\"\"\"\n\n    id: UUID = pydantic.Field(default_factory=uuid4)\n    type: str\n\n\nclass SummarizeStoryTask(Task):\n    \"\"\"A task to summarize a story.\"\"\"\n\n    type: Literal[\"summarize_story\"] = \"summarize_story\"\n    story: str\n\n\nclass RatingScale(BaseModel):\n    min: int\n    max: int\n\n\nclass AbstractRatingTask(Task):\n    \"\"\"A task to rate something.\"\"\"\n\n    scale: RatingScale = RatingScale(min=1, max=5)\n\n\nclass RateSummaryTask(AbstractRatingTask):\n    \"\"\"A task to rate a summary.\"\"\"\n\n    type: Literal[\"rate_summary\"] = \"rate_summary\"\n    full_text: str\n    summary: str\n\n\nclass WithHintMixin(BaseModel):\n    hint: str | None = None  # provide a hint to the user to spark their imagination\n\n\nclass InitialPromptTask(Task, WithHintMixin):\n    \"\"\"A task to prompt the user to submit an initial prompt to the assistant.\"\"\"\n\n    type: Literal[\"initial_prompt\"] = \"initial_prompt\"\n\n\nclass ReplyToConversationTask(Task):\n    \"\"\"A task to prompt the user to submit a reply to a conversation.\"\"\"\n\n    type: Literal[\"reply_to_conversation\"] = \"reply_to_conversation\"\n    conversation: Conversation  # the conversation so far\n\n\nclass PrompterReplyTask(ReplyToConversationTask, WithHintMixin):\n    \"\"\"A task to prompt the user to submit a reply to the assistant.\"\"\"\n\n    type: Literal[\"prompter_reply\"] = \"prompter_reply\"\n\n\nclass AssistantReplyTask(ReplyToConversationTask):\n    \"\"\"A task to prompt the user to act as the assistant.\"\"\"\n\n    type: Literal[\"assistant_reply\"] = \"assistant_reply\"\n\n\nclass RankInitialPromptsTask(Task):\n    \"\"\"A task to rank a set of initial prompts.\"\"\"\n\n    type: Literal[\"rank_initial_prompts\"] = \"rank_initial_prompts\"\n    prompts: list[str]  # deprecated, use prompt_messages\n    prompt_messages: list[ConversationMessage]\n\n\nclass RankConversationRepliesTask(Task):\n    \"\"\"A task to rank a set of replies to a conversation.\"\"\"\n\n    type: Literal[\"rank_conversation_replies\"] = \"rank_conversation_replies\"\n    conversation: Conversation  # the conversation so far\n    replies: list[str]  # deprecated, use reply_messages\n    reply_messages: list[ConversationMessage]\n    message_tree_id: UUID\n    ranking_parent_id: UUID\n    reveal_synthetic: bool\n\n\nclass RankPrompterRepliesTask(RankConversationRepliesTask):\n    \"\"\"A task to rank a set of prompter replies to a conversation.\"\"\"\n\n    type: Literal[\"rank_prompter_replies\"] = \"rank_prompter_replies\"\n\n\nclass RankAssistantRepliesTask(RankConversationRepliesTask):\n    \"\"\"A task to rank a set of assistant replies to a conversation.\"\"\"\n\n    type: Literal[\"rank_assistant_replies\"] = \"rank_assistant_replies\"\n\n\nclass LabelTaskMode(str, enum.Enum):\n    \"\"\"Label task mode that allows frontends to select an appropriate UI.\"\"\"\n\n    simple = \"simple\"\n    full = \"full\"\n\n\nclass LabelTaskDisposition(str, enum.Enum):\n    \"\"\"Reason why the task was issued.\"\"\"\n\n    quality = \"quality\"\n    spam = \"spam\"\n\n\nclass LabelDescription(BaseModel):\n    name: str\n    widget: str\n    display_text: str\n    help_text: Optional[str]\n\n\nclass AbstractLabelTask(Task):\n    message_id: UUID\n    valid_labels: list[str]\n    mandatory_labels: Optional[list[str]]\n    mode: Optional[LabelTaskMode]\n    disposition: Optional[LabelTaskDisposition]\n    labels: Optional[list[LabelDescription]]\n    conversation: Conversation  # the conversation so far (labeling -> last message)\n\n\nclass LabelInitialPromptTask(AbstractLabelTask):\n    \"\"\"A task to label an initial prompt.\"\"\"\n\n    type: Literal[\"label_initial_prompt\"] = \"label_initial_prompt\"\n    prompt: str | None = Field(None, deprecated=True, description=\"deprecated, use `prompt_message`\")\n\n\nclass LabelConversationReplyTask(AbstractLabelTask):\n    \"\"\"A task to label a reply to a conversation.\"\"\"\n\n    type: Literal[\"label_conversation_reply\"] = \"label_conversation_reply\"\n    reply: str | None = Field(None, deprecated=True, description=\"deprecated, use last message of `conversation`\")\n\n\nclass LabelPrompterReplyTask(LabelConversationReplyTask):\n    \"\"\"A task to label a prompter reply to a conversation.\"\"\"\n\n    type: Literal[\"label_prompter_reply\"] = \"label_prompter_reply\"\n\n\nclass LabelAssistantReplyTask(LabelConversationReplyTask):\n    \"\"\"A task to label an assistant reply to a conversation.\"\"\"\n\n    type: Literal[\"label_assistant_reply\"] = \"label_assistant_reply\"\n\n\nclass TaskDone(Task):\n    \"\"\"Signals to the frontend that the task is done.\"\"\"\n\n    type: Literal[\"task_done\"] = \"task_done\"\n\n\nAnyTask = Union[\n    TaskDone,\n    SummarizeStoryTask,\n    RateSummaryTask,\n    InitialPromptTask,\n    ReplyToConversationTask,\n    PrompterReplyTask,\n    AssistantReplyTask,\n    RankInitialPromptsTask,\n    RankConversationRepliesTask,\n    RankPrompterRepliesTask,\n    RankAssistantRepliesTask,\n    LabelInitialPromptTask,\n    LabelConversationReplyTask,\n    LabelPrompterReplyTask,\n    LabelAssistantReplyTask,\n]\n\n\nclass Interaction(BaseModel):\n    \"\"\"An interaction is a user-generated action in the frontend.\"\"\"\n\n    type: str\n    user: User\n\n\nclass TextReplyToMessage(Interaction):\n    \"\"\"A user has replied to a message with text.\"\"\"\n\n    type: Literal[\"text_reply_to_message\"] = \"text_reply_to_message\"\n    message_id: str\n    user_message_id: str\n    text: constr(min_length=1, strip_whitespace=True)\n    lang: Optional[str]  # BCP 47\n\n\nclass MessageRating(Interaction):\n    \"\"\"A user has rated a message.\"\"\"\n\n    type: Literal[\"message_rating\"] = \"message_rating\"\n    message_id: str\n    rating: conint(gt=0)\n\n\nclass MessageRanking(Interaction):\n    \"\"\"A user has given a ranking for a message.\"\"\"\n\n    type: Literal[\"message_ranking\"] = \"message_ranking\"\n    message_id: str  # parent message of replies that were ranked\n    ranking: conlist(item_type=int, min_items=1)\n    not_rankable: Optional[bool]  # all options flawed, factually incorrect or unacceptable\n\n\nclass LabelWidget(str, enum.Enum):\n    yes_no = \"yes_no\"\n    flag = \"flag\"\n    likert = \"likert\"\n\n\nclass TextLabel(str, enum.Enum):\n    \"\"\"A label for a piece of text.\"\"\"\n\n    def __new__(cls, label: str, widget: LabelWidget, display_text: str = \"\", help_text: str = None):\n        obj = str.__new__(cls, label)\n        obj._value_ = label\n        obj.widget = widget\n        obj.display_text = display_text\n        obj.help_text = help_text\n        return obj\n\n    # yes/no questions\n    spam = \"spam\", LabelWidget.yes_no, \"Seems to be intentionally low-quality or irrelevant\"\n    fails_task = \"fails_task\", LabelWidget.yes_no, \"Fails to follow the correct instruction / task\"\n\n    # flags\n    lang_mismatch = (\n        \"lang_mismatch\",\n        LabelWidget.flag,\n        \"Wrong Language\",\n        \"The message is written in a language that differs from the currently selected language.\",\n    )\n    pii = \"pii\", LabelWidget.flag, \"Contains personal identifiable information (PII)\"\n    not_appropriate = \"not_appropriate\", LabelWidget.flag, \"Inappropriate\"\n    hate_speech = (\n        \"hate_speech\",\n        LabelWidget.flag,\n        \"Content is abusive or threatening and expresses prejudice against a protected characteristic\",\n        \"Prejudice refers to preconceived views not based on reason. Protected characteristics \"\n        \"include gender, ethnicity, religion, sexual orientation, and similar characteristics.\",\n    )\n    sexual_content = \"sexual_content\", LabelWidget.flag, \"Contains sexual content\"\n    moral_judgement = \"moral_judgement\", LabelWidget.flag, \"Expresses moral judgement\"\n    political_content = \"political_content\", LabelWidget.flag, \"Expresses political views\"\n\n    # likert\n    quality = \"quality\", LabelWidget.likert, \"Overall subjective quality rating of the message\"\n    toxicity = \"toxicity\", LabelWidget.likert, \"Rude, abusive, profane or insulting content\"\n    humor = \"humor\", LabelWidget.likert, \"Humorous content including sarcasm\"\n    helpfulness = \"helpfulness\", LabelWidget.likert, \"Helpfulness of the message\"\n    creativity = \"creativity\", LabelWidget.likert, \"Creativity\"\n    violence = \"violence\", LabelWidget.likert, \"Violence/abuse/terrorism/self-harm\"\n\n\nclass TextLabels(Interaction):\n    \"\"\"A set of labels for a piece of text.\"\"\"\n\n    type: Literal[\"text_labels\"] = \"text_labels\"\n    text: str\n    labels: dict[TextLabel, float]\n    message_id: UUID\n    task_id: Optional[UUID]\n    is_report: Optional[bool]\n\n    @property\n    def has_message_id(self) -> bool:\n        \"\"\"Whether this TextLabels has a message_id.\"\"\"\n        return bool(self.message_id)\n\n    # check that each label value is between 0 and 1\n    @pydantic.validator(\"labels\")\n    def check_label_values(cls, v):\n        for key, value in v.items():\n            if not (0 <= value <= 1):\n                raise ValueError(f\"Label values must be between 0 and 1, got {value} for {key}.\")\n        return v\n\n\nAnyInteraction = Union[\n    TextReplyToMessage,\n    MessageRating,\n    MessageRanking,\n    TextLabels,\n]\n\n\nclass SystemStats(BaseModel):\n    all: int = 0\n    active: int = 0\n    active_by_lang: dict[str, int] = {}\n    deleted: int = 0\n    message_trees: int = 0\n\n\nclass UserScore(BaseModel):\n    rank: Optional[int]\n    user_id: UUID\n    highlighted: bool = False\n    username: str\n    auth_method: str\n    display_name: str\n\n    leader_score: int = 0\n    level: int = 0  # between 0 and 100\n\n    base_date: Optional[datetime]\n    modified_date: Optional[datetime]\n\n    prompts: int = 0\n    replies_assistant: int = 0\n    replies_prompter: int = 0\n    labels_simple: int = 0\n    labels_full: int = 0\n    rankings_total: int = 0\n    rankings_good: int = 0\n\n    accepted_prompts: int = 0\n    accepted_replies_assistant: int = 0\n    accepted_replies_prompter: int = 0\n\n    reply_ranked_1: int = 0\n    reply_ranked_2: int = 0\n    reply_ranked_3: int = 0\n\n    streak_last_day_date: Optional[datetime]\n    streak_days: Optional[int]\n    last_activity_date: Optional[datetime]\n\n\nclass LeaderboardStats(BaseModel):\n    time_frame: str\n    last_updated: datetime\n    leaderboard: List[UserScore]\n\n\nclass TrollScore(BaseModel):\n    rank: Optional[int]\n    user_id: UUID\n    highlighted: bool = False\n    username: str\n    auth_method: str\n    display_name: str\n    last_activity_date: Optional[datetime]\n    enabled: bool\n    deleted: bool\n    show_on_leaderboard: bool\n\n    troll_score: int = 0\n\n    base_date: Optional[datetime]\n    modified_date: Optional[datetime]\n\n    red_flags: int = 0  # num reported messages of user\n    upvotes: int = 0  # num up-voted messages of user\n    downvotes: int = 0  # num down-voted messages of user\n\n    spam_prompts: int = 0\n\n    quality: Optional[float] = None\n    humor: Optional[float] = None\n    toxicity: Optional[float] = None\n    violence: Optional[float] = None\n    helpfulness: Optional[float] = None\n\n    spam: int = 0\n    lang_mismach: int = 0\n    not_appropriate: int = 0\n    pii: int = 0\n    hate_speech: int = 0\n    sexual_content: int = 0\n    political_content: int = 0\n\n\nclass TrollboardStats(BaseModel):\n    time_frame: str\n    last_updated: datetime\n    trollboard: List[TrollScore]\n\n\nclass OasstErrorResponse(BaseModel):\n    \"\"\"The format of an error response from the OASST API.\"\"\"\n\n    error_code: OasstErrorCode\n    message: str\n\n\nclass EmojiCode(str, enum.Enum):\n    thumbs_up = \"+1\"  # \ud83d\udc4d\n    thumbs_down = \"-1\"  # \ud83d\udc4e\n    red_flag = \"red_flag\"  # \ud83d\udea9\n    hundred = \"100\"  # \ud83d\udcaf\n    rofl = \"rofl\"  # \ud83e\udd23\n    clap = \"clap\"  # \ud83d\udc4f\n    diamond = \"diamond\"  # \ud83d\udc8e\n    heart_eyes = \"heart_eyes\"  # \ud83d\ude0d\n    disappointed = \"disappointed\"  # \ud83d\ude1e\n    poop = \"poop\"  # \ud83d\udca9\n    skull = \"skull\"  # \ud83d\udc80\n\n    # skip task system uses special emoji codes\n    skip_reply = \"_skip_reply\"\n    skip_ranking = \"_skip_ranking\"\n    skip_labeling = \"_skip_labeling\"\n\n\nclass EmojiOp(str, enum.Enum):\n    togggle = \"toggle\"\n    add = \"add\"\n    remove = \"remove\"\n\n\nclass MessageEmojiRequest(BaseModel):\n    user: User\n    op: EmojiOp = EmojiOp.togggle\n    emoji: EmojiCode\n\n\nclass MessageEditRequest(BaseModel):\n    user: User\n    new_content: str\n\n\nclass CreateFrontendUserRequest(User):\n    show_on_leaderboard: bool = True\n    enabled: bool = True\n    tos_acceptance: Optional[bool] = None\n    notes: Optional[str] = None\n\n\nclass CachedStatsName(str, enum.Enum):\n    human_messages_by_lang = \"human_messages_by_lang\"\n    human_messages_by_role = \"human_messages_by_role\"\n    message_trees_by_state = \"message_trees_by_state\"\n    message_trees_states_by_lang = \"message_trees_states_by_lang\"\n    users_accepted_tos = \"users_accepted_tos\"\n\n\nclass CachedStatsResponse(BaseModel):\n    name: CachedStatsName | str\n    last_updated: datetime\n    stats: dict | list\n\n\nclass AllCachedStatsResponse(BaseModel):\n    stats_by_name: dict[CachedStatsName | str, CachedStatsResponse]\n", "oasst-shared/oasst_shared/schemas/__init__.py": "", "oasst-shared/oasst_shared/schemas/inference.py": "import enum\nimport platform\nimport random\nimport uuid\nfrom datetime import datetime\nfrom typing import Annotated, Literal, Union\n\nimport psutil\nimport pydantic\nimport pynvml\nfrom oasst_shared.model_configs import ModelConfig\n\nINFERENCE_PROTOCOL_VERSION = \"1\"\n\n\nclass WorkerGpuInfo(pydantic.BaseModel):\n    name: str\n    total_memory: int\n\n\nclass WorkerHardwareInfo(pydantic.BaseModel):\n    uname_sysname: str\n    uname_release: str\n    uname_version: str\n    uname_machine: str\n    uname_processor: str\n    cpu_count_physical: int\n    cpu_count_logical: int\n    cpu_freq_max: float\n    cpu_freq_min: float\n    mem_total: int\n    swap_total: int\n    nvidia_driver_version: str | None = None\n    gpus: list[WorkerGpuInfo]\n\n    def __init__(self, **data):\n        data[\"uname_sysname\"] = platform.uname().system\n        data[\"uname_release\"] = platform.uname().release\n        data[\"uname_version\"] = platform.uname().version\n        data[\"uname_machine\"] = platform.uname().machine\n        data[\"uname_processor\"] = platform.uname().processor\n        data[\"cpu_count_physical\"] = psutil.cpu_count(logical=False)\n        data[\"cpu_count_logical\"] = psutil.cpu_count(logical=True)\n        try:\n            data[\"cpu_freq_max\"] = psutil.cpu_freq().max\n            data[\"cpu_freq_min\"] = psutil.cpu_freq().min\n        except Exception:\n            # Workaround for psutil.cpu_freq() throwing exception on some hardware\n            # or sometimes returning `None`. Hardware affected includes Apple Silicon\n            # https://github.com/giampaolo/psutil/issues/1892\n            data[\"cpu_freq_max\"] = 0\n            data[\"cpu_freq_min\"] = 0\n        data[\"mem_total\"] = psutil.virtual_memory().total\n        data[\"swap_total\"] = psutil.swap_memory().total\n        data[\"gpus\"] = []\n        try:\n            pynvml.nvmlInit()\n            data[\"nvidia_driver_version\"] = pynvml.nvmlSystemGetDriverVersion()\n            for i in range(pynvml.nvmlDeviceGetCount()):\n                handle = pynvml.nvmlDeviceGetHandleByIndex(i)\n                name = pynvml.nvmlDeviceGetName(handle)\n                total_memory = pynvml.nvmlDeviceGetMemoryInfo(handle).total\n                data[\"gpus\"].append(WorkerGpuInfo(name=name, total_memory=total_memory))\n        except Exception:\n            pass\n        super().__init__(**data)\n\n\nclass WorkerConfig(pydantic.BaseModel):\n    model_config: ModelConfig\n    max_parallel_requests: int = 1\n\n    @property\n    def compat_hash(self) -> str:\n        return self.model_config.compat_hash\n\n\nclass WorkerInfo(pydantic.BaseModel):\n    config: WorkerConfig\n    hardware_info: WorkerHardwareInfo\n\n\nclass GpuMetricsInfo(pydantic.BaseModel):\n    gpu_usage: float\n    mem_usage: float\n\n\nclass WorkerMetricsInfo(pydantic.BaseModel):\n    created_at: datetime\n    cpu_usage: float\n    mem_usage: float\n    swap_usage: float\n    gpus: list[GpuMetricsInfo] | None = None\n\n    def __init__(self, **data):\n        data[\"created_at\"] = datetime.utcnow()\n        data[\"cpu_usage\"] = psutil.cpu_percent()\n        data[\"mem_usage\"] = psutil.virtual_memory().percent\n        data[\"swap_usage\"] = psutil.swap_memory().percent\n        try:\n            pynvml.nvmlInit()\n            data[\"nvidia_driver_version\"] = pynvml.nvmlSystemGetDriverVersion()\n            gpus = []\n            for i in range(pynvml.nvmlDeviceGetCount()):\n                handle = pynvml.nvmlDeviceGetHandleByIndex(i)\n                gpus.append(\n                    {\n                        \"gpu_usage\": pynvml.nvmlDeviceGetUtilizationRates(handle).gpu,\n                        \"mem_usage\": pynvml.nvmlDeviceGetMemoryInfo(handle).used,\n                    }\n                )\n            data[\"gpus\"] = gpus\n        except Exception:\n            pass\n        super().__init__(**data)\n\n\nclass SamplingParameters(pydantic.BaseModel):\n    top_k: int | None = None\n    top_p: float | None = None\n    typical_p: float | None = None\n    temperature: float | None = None\n    repetition_penalty: float | None = None\n    max_new_tokens: int = 1024\n\n\nclass PluginApiType(pydantic.BaseModel):\n    type: str\n    url: str\n    has_user_authentication: bool | None = False\n    # NOTE: Some plugins using this field,\n    # instead of has_user_authentication\n    is_user_authenticated: bool | None = False\n\n\nclass PluginAuthType(pydantic.BaseModel):\n    type: str\n\n\nclass PluginOpenAPIParameter(pydantic.BaseModel):\n    name: str\n    in_: str\n    description: str\n    required: bool\n    schema_: object\n\n\nclass PluginOpenAPIEndpoint(pydantic.BaseModel):\n    path: str\n    type: str\n    summary: str\n    operation_id: str\n    url: str\n    params: list[PluginOpenAPIParameter]\n    payload: dict | None = None\n\n\nclass PluginConfig(pydantic.BaseModel):\n    schema_version: str\n    name_for_model: str\n    name_for_human: str\n    description_for_human: str\n    description_for_model: str\n    api: PluginApiType\n    auth: PluginAuthType\n    logo_url: str | None = None\n    contact_email: str | None = None\n    legal_info_url: str | None = None\n    endpoints: list[PluginOpenAPIEndpoint] | None = None\n\n\nclass PluginEntry(pydantic.BaseModel):\n    url: str\n    enabled: bool = True\n    plugin_config: PluginConfig | None = None\n    # Idea is for OA internal plugins to be trusted, others untrusted by default\n    trusted: bool | None = False\n\n\nclass PluginExecutionDetails(pydantic.BaseModel):\n    inner_monologue: list[str]\n    final_tool_output: str\n    final_prompt: str\n    final_generation_assisted: bool\n    achieved_depth: int | None = None\n    error_message: str | None = None\n    status: Literal[\"success\", \"failure\"]\n\n\nclass PluginUsed(pydantic.BaseModel):\n    name: str | None = None\n    url: str | None = None\n    trusted: bool | None = None\n    execution_details: PluginExecutionDetails\n\n\ndef make_seed() -> int:\n    return random.randint(0, 0xFFFF_FFFF_FFFF_FFFF - 1)\n\n\nclass WorkParameters(pydantic.BaseModel):\n    model_config: ModelConfig\n    sampling_parameters: SamplingParameters = pydantic.Field(\n        default_factory=SamplingParameters,\n    )\n    do_sample: bool = True\n    seed: int = pydantic.Field(\n        default_factory=make_seed,\n    )\n    system_prompt: str | None = None\n    user_profile: str | None = None\n    user_response_instructions: str | None = None\n    plugins: list[PluginEntry] = pydantic.Field(default_factory=list[PluginEntry])\n    plugin_max_depth: int = 4\n\n\nclass ReportType(str, enum.Enum):\n    spam = \"spam\"\n    offensive = \"offensive\"\n    feeback = \"feedback\"\n\n\nclass Vote(pydantic.BaseModel):\n    id: str\n    score: int\n\n\nclass Report(pydantic.BaseModel):\n    id: str\n    report_type: ReportType\n    reason: str\n\n\nclass MessageState(str, enum.Enum):\n    manual = \"manual\"\n    pending = \"pending\"\n    in_progress = \"in_progress\"\n    complete = \"complete\"\n    aborted_by_worker = \"aborted_by_worker\"\n    cancelled = \"cancelled\"\n    timeout = \"timeout\"\n\n\nclass MessageRead(pydantic.BaseModel):\n    id: str\n    parent_id: str | None\n    content: str | None\n    chat_id: str\n    created_at: datetime\n    role: Literal[\"prompter\", \"assistant\"]\n    state: MessageState\n    score: int\n    reports: list[Report] = []\n    # work parameters will be None on user prompts\n    work_parameters: WorkParameters | None\n    safe_content: str | None\n    safety_level: int | None\n    safety_label: str | None\n    safety_rots: str | None\n    used_plugin: PluginUsed | None = None\n\n    @property\n    def is_assistant(self) -> bool:\n        return self.role == \"assistant\"\n\n\nclass Thread(pydantic.BaseModel):\n    messages: list[MessageRead]\n\n\nclass SafetyParameters(pydantic.BaseModel):\n    level: int = 0\n\n    @pydantic.validator(\"level\")\n    def level_must_be_in_range(cls, v):\n        if v < 0 or v > 9:\n            raise ValueError(\"level must be in range [0, 9]\")\n        return v\n\n\nclass SafetyRequest(pydantic.BaseModel):\n    inputs: str\n    parameters: SafetyParameters\n\n\nclass SafetyResponse(pydantic.BaseModel):\n    outputs: str\n\n\nclass WorkerRequestBase(pydantic.BaseModel):\n    id: str = pydantic.Field(default_factory=lambda: str(uuid.uuid4()))\n\n\nclass WorkRequest(WorkerRequestBase):\n    request_type: Literal[\"work\"] = \"work\"\n    thread: Thread = pydantic.Field(..., repr=False)\n    created_at: datetime = pydantic.Field(default_factory=datetime.utcnow)\n    parameters: WorkParameters = pydantic.Field(default_factory=WorkParameters)\n    safety_parameters: SafetyParameters = pydantic.Field(\n        default_factory=SafetyParameters,\n    )\n\n\nclass PingRequest(WorkerRequestBase):\n    request_type: Literal[\"ping\"] = \"ping\"\n\n\nclass ErrorRequest(WorkerRequestBase):\n    request_type: Literal[\"error\"] = \"error\"\n    error: str\n\n\nclass UpgradeProtocolRequest(WorkerRequestBase):\n    request_type: Literal[\"upgrade_protocol\"] = \"upgrade_protocol\"\n\n\nclass WrongApiKeyRequest(WorkerRequestBase):\n    request_type: Literal[\"wrong_api_key\"] = \"wrong_api_key\"\n\n\nclass TerminateRequest(WorkerRequestBase):\n    request_type: Literal[\"terminate\"] = \"terminate\"\n\n\nclass WorkerResponseBase(pydantic.BaseModel):\n    request_id: str | None = None\n\n\nclass PongResponse(WorkerResponseBase):\n    response_type: Literal[\"pong\"] = \"pong\"\n    metrics: WorkerMetricsInfo | None = None\n\n\nclass SafePromptResponse(WorkerResponseBase):\n    response_type: Literal[\"safe_prompt\"] = \"safe_prompt\"\n    safe_prompt: str\n    safety_parameters: SafetyParameters\n    safety_label: str\n    safety_rots: str\n\n\nclass PluginIntermediateResponse(WorkerResponseBase):\n    response_type: Literal[\"plugin_intermediate\"] = \"plugin_intermediate\"\n    text: str = \"\"\n    current_plugin_thought: str\n    current_plugin_action_taken: str\n    current_plugin_action_input: str\n    current_plugin_action_response: str\n\n\nclass TokenResponse(WorkerResponseBase):\n    response_type: Literal[\"token\"] = \"token\"\n    text: str\n    log_prob: float | None\n    token_id: int\n\n\nclass GeneratedTextResponse(WorkerResponseBase):\n    response_type: Literal[\"generated_text\"] = \"generated_text\"\n    text: str\n    finish_reason: Literal[\"length\", \"eos_token\", \"stop_sequence\"]\n    metrics: WorkerMetricsInfo | None = None\n    used_plugin: PluginUsed | None = None\n\n\nclass InternalFinishedMessageResponse(WorkerResponseBase):\n    response_type: Literal[\"internal_finished_message\"] = \"internal_finished_message\"\n    message: MessageRead\n\n\nclass InternalErrorResponse(WorkerResponseBase):\n    response_type: Literal[\"internal_error\"] = \"internal_error\"\n    error: str\n    message: MessageRead\n\n\nclass ErrorResponse(WorkerResponseBase):\n    response_type: Literal[\"error\"] = \"error\"\n    metrics: WorkerMetricsInfo | None = None\n    error: str\n\n\nclass GeneralErrorResponse(WorkerResponseBase):\n    response_type: Literal[\"general_error\"] = \"general_error\"\n    metrics: WorkerMetricsInfo | None = None\n    error: str\n\n\n_WorkerRequest = Union[\n    WorkRequest,\n    PingRequest,\n    ErrorRequest,\n    TerminateRequest,\n    UpgradeProtocolRequest,\n    WrongApiKeyRequest,\n]\nWorkerRequest = Annotated[\n    _WorkerRequest,\n    pydantic.Field(discriminator=\"request_type\"),\n]\n\nWorkerResponse = Annotated[\n    Union[\n        TokenResponse,\n        GeneratedTextResponse,\n        ErrorResponse,\n        PongResponse,\n        InternalFinishedMessageResponse,\n        InternalErrorResponse,\n        SafePromptResponse,\n        PluginIntermediateResponse,\n    ],\n    pydantic.Field(discriminator=\"response_type\"),\n]\n", "oasst-shared/oasst_shared/exceptions/oasst_api_error.py": "from enum import IntEnum\nfrom http import HTTPStatus\n\n\nclass OasstErrorCode(IntEnum):\n    \"\"\"\n    Error codes of the Open-Assistant backend API.\n\n    Ranges:\n         0-1000: general errors\n      1000-2000: tasks endpoint\n      2000-3000: prompt_repository, task_repository, user_repository\n      3000-4000: external resources\n    \"\"\"\n\n    # 0-1000: general errors\n    GENERIC_ERROR = 0\n    DATABASE_URI_NOT_SET = 1\n    API_CLIENT_NOT_AUTHORIZED = 2\n    ROOT_TOKEN_NOT_AUTHORIZED = 3\n    DATABASE_MAX_RETRIES_EXHAUSTED = 4\n\n    SORT_KEY_UNSUPPORTED = 100\n    INVALID_CURSOR_VALUE = 101\n\n    TOO_MANY_REQUESTS = 429\n\n    SERVER_ERROR0 = 500\n    SERVER_ERROR1 = 501\n\n    INVALID_AUTHENTICATION = 600\n\n    # 1000-2000: tasks endpoint\n    TASK_INVALID_REQUEST_TYPE = 1000\n    TASK_ACK_FAILED = 1001\n    TASK_NACK_FAILED = 1002\n    TASK_INVALID_RESPONSE_TYPE = 1003\n    TASK_INTERACTION_REQUEST_FAILED = 1004\n    TASK_GENERATION_FAILED = 1005\n    TASK_REQUESTED_TYPE_NOT_AVAILABLE = 1006\n    TASK_AVAILABILITY_QUERY_FAILED = 1007\n    TASK_MESSAGE_TOO_LONG = 1008\n    TASK_MESSAGE_DUPLICATED = 1009\n    TASK_MESSAGE_TEXT_EMPTY = 1010\n    TASK_MESSAGE_DUPLICATE_REPLY = 1011\n    TASK_TOO_MANY_PENDING = 1012\n\n    # 2000-3000: prompt_repository\n    INVALID_FRONTEND_MESSAGE_ID = 2000\n    MESSAGE_NOT_FOUND = 2001\n    RATING_OUT_OF_RANGE = 2002\n    INVALID_RANKING_VALUE = 2003\n    INVALID_TASK_TYPE = 2004\n\n    NO_MESSAGE_TREE_FOUND = 2006\n    NO_REPLIES_FOUND = 2007\n    INVALID_MESSAGE = 2008\n    BROKEN_CONVERSATION = 2009\n    TREE_IN_ABORTED_STATE = 2010\n    CORRUPT_RANKING_RESULT = 2011\n    AUTH_AND_USERNAME_REQUIRED = 2012\n\n    TEXT_LABELS_WRONG_MESSAGE_ID = 2050\n    TEXT_LABELS_INVALID_LABEL = 2051\n    TEXT_LABELS_MANDATORY_LABEL_MISSING = 2052\n    TEXT_LABELS_NO_SELF_LABELING = 2053\n    TEXT_LABELS_DUPLICATE_TASK_REPLY = 2053\n\n    TASK_NOT_FOUND = 2100\n    TASK_EXPIRED = 2101\n    TASK_PAYLOAD_TYPE_MISMATCH = 2102\n    TASK_ALREADY_UPDATED = 2103\n    TASK_NOT_ACK = 2104\n    TASK_ALREADY_DONE = 2105\n    TASK_NOT_COLLECTIVE = 2106\n    TASK_NOT_ASSIGNED_TO_USER = 2106\n    TASK_UNEXPECTED_PAYLOAD_TYPE_ = 2107\n\n    # 3000-4000: external resources\n    HUGGINGFACE_API_ERROR = 3001\n\n    # 4000-5000: user\n    USER_NOT_SPECIFIED = 4000\n    USER_DISABLED = 4001\n    USER_NOT_FOUND = 4002\n    USER_HAS_NOT_ACCEPTED_TOS = 4003\n\n    EMOJI_OP_UNSUPPORTED = 5000\n\n    CACHED_STATS_NOT_AVAILABLE = 6000\n\n\nclass OasstError(Exception):\n    \"\"\"Base class for Open-Assistant exceptions.\"\"\"\n\n    message: str\n    error_code: int\n    http_status_code: HTTPStatus\n\n    def __init__(self, message: str, error_code: OasstErrorCode, http_status_code: HTTPStatus = HTTPStatus.BAD_REQUEST):\n        super().__init__(message, error_code, http_status_code)  # make exception picklable (fill args member)\n        self.message = message\n        self.error_code = error_code\n        self.http_status_code = http_status_code\n\n    def __repr__(self) -> str:\n        class_name = self.__class__.__name__\n        return f'{class_name}(message=\"{self.message}\", error_code={self.error_code}, http_status_code={self.http_status_code})'\n", "oasst-shared/oasst_shared/exceptions/__init__.py": "# Ignore unused imports; these are re-exported\nfrom .oasst_api_error import OasstError as OasstError  # noqa: F401\nfrom .oasst_api_error import OasstErrorCode as OasstErrorCode  # noqa: F401\n", "discord-bots/oa-bot-py/message_templates.py": "\"\"\"Message templates for the discord bot.\"\"\"\nimport typing\n\nimport jinja2\nfrom loguru import logger\n\n\nclass MessageTemplates:\n    \"\"\"Create message templates for the discord bot.\"\"\"\n\n    def __init__(self, template_dir: str = \"./templates\"):\n        self.env = jinja2.Environment(  # noqa: S701\n            loader=jinja2.FileSystemLoader(template_dir),\n            autoescape=jinja2.select_autoescape(disabled_extensions=(\"msg\",), default=False, default_for_string=False),\n        )\n\n    def render(self, template_name: str, **kwargs: typing.Any):\n        template = self.env.get_template(template_name)\n        txt = template.render(kwargs)\n        logger.debug(txt)\n\n        return txt\n", "discord-bots/oa-bot-py/bot/settings.py": "\"\"\"Configuration for the bot.\"\"\"\nfrom pydantic import BaseSettings, Field\n\n\nclass Settings(BaseSettings):\n    \"\"\"Settings for the bot.\"\"\"\n\n    bot_token: str = Field(env=\"BOT_TOKEN\", default=\"\")\n    declare_global_commands: int = Field(env=\"DECLARE_GLOBAL_COMMANDS\", default=0)\n    owner_ids: list[int] = Field(env=\"OWNER_IDS\", default_factory=list)\n    prefix: str = Field(env=\"PREFIX\", default=\"/\")\n    oasst_api_url: str = Field(env=\"OASST_API_URL\", default=\"http://localhost:8080\")\n    oasst_api_key: str = Field(env=\"OASST_API_KEY\", default=\"\")\n\n    class Config(BaseSettings.Config):\n        env_file = \".env\"\n        case_sensitive = False\n", "discord-bots/oa-bot-py/bot/utils.py": "\"\"\"Utility functions.\"\"\"\nimport typing as t\nfrom datetime import datetime\n\nimport hikari\n\n\ndef format_time(dt: datetime, fmt: t.Literal[\"t\", \"T\", \"D\", \"f\", \"F\", \"R\"]) -> str:\n    \"\"\"Format a datetime object into the discord time format.\n\n    ```\n    | t | HH:MM            | 16:20\n    | T | HH:MM:SS         | 16:20:11\n    | D | D Mo Yr          | 20 April 2022\n    | f | D Mo Yr HH:MM    | 20 April 2022 16:20\n    | F | W, D Mo Yr HH:MM | Wednesday, 20 April 2022 16:20\n    | R | relative         | in an hour\n    ```\n    \"\"\"\n    match fmt:\n        case \"t\" | \"T\" | \"D\" | \"f\" | \"F\" | \"R\":\n            return f\"<t:{dt.timestamp():.0f}:{fmt}>\"\n        case _:\n            raise ValueError(f\"`fmt` must be 't', 'T', 'D', 'f', 'F' or 'R', not {fmt}\")\n\n\ndef mention(\n    id: hikari.Snowflakeish,\n    type: t.Literal[\"channel\", \"role\", \"user\"],\n) -> str:\n    \"\"\"Mention an object.\"\"\"\n    match type:\n        case \"channel\":\n            return f\"<#{id}>\"\n\n        case \"user\":\n            return f\"<@{id}>\"\n\n        case \"role\":\n            return f\"<@&{id}>\"\n", "discord-bots/oa-bot-py/bot/__main__.py": "\"\"\"Entry point for the bot.\"\"\"\nimport logging\nimport os\n\nfrom bot.bot import bot\nfrom hikari.presences import Activity, ActivityType, Status\n\nlogger = logging.getLogger(__name__)\n\nif __name__ == \"__main__\":\n    if os.name != \"nt\":\n        import uvloop\n\n        uvloop.install()\n\n    logger.info(\"Starting bot\")\n    bot.run(\n        check_for_updates=True,\n        activity=Activity(\n            name=\"/help\",\n            type=ActivityType.PLAYING,\n        ),\n        status=Status.ONLINE,\n    )\n", "discord-bots/oa-bot-py/bot/__init__.py": "\"\"\"The official Open-Assistant Discord Bot.\"\"\"\n", "discord-bots/oa-bot-py/bot/bot.py": "\"\"\"Bot logic.\"\"\"\nfrom datetime import datetime\n\nimport aiosqlite\nimport hikari\nimport lightbulb\nimport miru\nfrom bot.settings import Settings\nfrom bot.utils import mention\nfrom oasst_shared.api_client import OasstApiClient\n\nsettings = Settings()\n\n# TODO: Revisit cache settings\nbot = lightbulb.BotApp(\n    token=settings.bot_token,\n    logs=\"DEBUG\",\n    prefix=settings.prefix,\n    default_enabled_guilds=settings.declare_global_commands,\n    owner_ids=settings.owner_ids,\n    intents=hikari.Intents.ALL,\n    help_class=None,\n)\n\n\n@bot.listen()\nasync def on_starting(event: hikari.StartingEvent):\n    \"\"\"Setup.\"\"\"\n    miru.install(bot)  # component handler\n    bot.load_extensions_from(\"./bot/extensions\")  # load extensions\n\n    # Database setup\n    bot.d.db = await aiosqlite.connect(\"./bot/db/database.db\")\n    await bot.d.db.executescript(open(\"./bot/db/schema.sql\").read())\n    await bot.d.db.commit()\n\n    # OASST API setup\n    bot.d.oasst_api = OasstApiClient(settings.oasst_api_url, settings.oasst_api_key)\n\n    # A `dict[hikari.Message | None, UUID | None]]` that maps user IDs to (task msg ID, task UUIDs).\n    # Either both are `None` or both are not `None`.\n    # If both are `None`, the user is not currently selecting a task.\n    # TODO: Grow this on startup so we don't have to re-allocate memory every time it needs to grow\n    bot.d.currently_working = {}\n\n\n@bot.listen()\nasync def on_stopping(event: hikari.StoppingEvent):\n    \"\"\"Cleanup.\"\"\"\n    await bot.d.db.close()\n    await bot.d.oasst_api.close()\n\n\nasync def _send_error_embed(\n    content: str, exception: lightbulb.errors.LightbulbError | BaseException, ctx: lightbulb.Context\n) -> None:\n    ctx.command\n    embed = hikari.Embed(\n        title=f\"`{exception.__class__.__name__}` Error{f' in `/{ctx.command.name}`' if ctx.command else '' }\",\n        description=content,\n        color=0xFF0000,\n        timestamp=datetime.now().astimezone(),\n    ).set_author(name=ctx.author.username, url=str(ctx.author.avatar_url))\n\n    await ctx.respond(embed=embed)\n\n\n@bot.listen(lightbulb.CommandErrorEvent)\nasync def on_error(event: lightbulb.CommandErrorEvent) -> None:\n    \"\"\"Error handler for the bot.\"\"\"\n    # Unwrap the exception to get the original cause\n    exc = event.exception.__cause__ or event.exception\n    ctx = event.context\n    if not ctx.bot.rest.is_alive:\n        return\n\n    if isinstance(event.exception, lightbulb.CommandInvocationError):\n        if not event.context.command:\n            await _send_error_embed(\"Something went wrong\", exc, ctx)\n        else:\n            await _send_error_embed(\n                f\"Something went wrong during invocation of command `{event.context.command.name}`.\", exc, ctx\n            )\n\n        raise event.exception\n\n    # Not an owner\n    if isinstance(exc, lightbulb.NotOwner):\n        await _send_error_embed(\"You are not the owner of this bot.\", exc, ctx)\n    # Command is on cooldown\n    elif isinstance(exc, lightbulb.CommandIsOnCooldown):\n        await _send_error_embed(f\"This command is on cooldown. Retry in `{exc.retry_after:.2f}` seconds.\", exc, ctx)\n    # Missing permissions\n    elif isinstance(exc, lightbulb.errors.MissingRequiredPermission):\n        await _send_error_embed(\n            f\"You do not have permission to use this command. Missing permissions: {exc.missing_perms}\", exc, ctx\n        )\n    # Missing roles\n    elif isinstance(exc, lightbulb.errors.MissingRequiredRole):\n        assert event.context.guild_id is not None  # Roles only exist in guilds\n        await _send_error_embed(\n            f\"You do not have the correct role to use this command. Missing role(s): {[mention(r, 'role') for r in exc.missing_roles]}\",\n            exc,\n            ctx,\n        )\n    # Only a guild command\n    elif isinstance(exc, lightbulb.errors.OnlyInGuild):\n        await _send_error_embed(\"This command can only be run in servers.\", exc, ctx)\n    # Only a DM command\n    elif isinstance(exc, lightbulb.errors.OnlyInDM):\n        await _send_error_embed(\"This command can only be run in DMs.\", exc, ctx)\n    # Not enough arguments\n    elif isinstance(exc, lightbulb.errors.NotEnoughArguments):\n        await _send_error_embed(\n            f\"Not enough arguments were supplied to the command. {[opt.name for opt in exc.missing_options]}\", exc, ctx\n        )\n    # Bot missing permission\n    elif isinstance(exc, lightbulb.errors.BotMissingRequiredPermission):\n        await _send_error_embed(\n            f\"The bot does not have the correct permission(s) to execute this command. Missing permissions: {exc.missing_perms}\",\n            exc,\n            ctx,\n        )\n    elif isinstance(exc, lightbulb.errors.MissingRequiredAttachment):\n        await _send_error_embed(\"Not enough attachments were supplied to this command.\", exc, ctx)\n    elif isinstance(exc, lightbulb.errors.CommandNotFound):\n        await ctx.respond(f\"`/{exc.invoked_with}` is not a valid command. Use `/help` to see a list of commands.\")\n    else:\n        raise exc\n", "discord-bots/oa-bot-py/bot/messages.py": "\"\"\"All user-facing messages and embeds.\n\nWhen sending a conversation\n- The function will return a list of strings\n    - use asyncio.gather to send all messages\n\n-\n\"\"\"\n\nfrom datetime import datetime\n\nimport hikari\nfrom oasst_shared.schemas import protocol as protocol_schema\n\nNUMBER_EMOJIS = [\":one:\", \":two:\", \":three:\", \":four:\", \":five:\", \":six:\", \":seven:\", \":eight:\", \":nine:\", \":ten:\"]\nNL = \"\\n\"\n\n###\n# Reusable 'components'\n###\n\n\ndef _h1(text: str) -> str:\n    return f\"\\n:small_blue_diamond: __**{text}**__ :small_blue_diamond:\"\n\n\ndef _h2(text: str) -> str:\n    return f\"__**{text}**__\"\n\n\ndef _h3(text: str) -> str:\n    return f\"__{text}__\"\n\n\ndef _writing_prompt(text: str) -> str:\n    return f\":pencil: _{text}_\"\n\n\ndef _ranking_prompt(text: str) -> str:\n    return f\":trophy: _{text}_\"\n\n\ndef _label_prompt(text: str, mandatory_label: list[str] | None, valid_labels: list[str]) -> str:\n    return f\"\"\":question: _{text}_\nMandatory labels: {\", \".join(mandatory_label) if mandatory_label is not None else \"None\"}\nValid labels: {\", \".join(valid_labels)}\n\"\"\"\n\n\ndef _response_prompt(text: str) -> str:\n    return f\":speech_balloon: _{text}_\"\n\n\ndef _summarize_prompt(text: str) -> str:\n    return f\":notepad_spiral: _{text}_\"\n\n\ndef _user(text: str | None) -> str:\n    return f\"\"\"\\\n:person_red_hair: {_h3(\"User\")}:{f\"{NL}> **{text}**\" if text is not None else \"\"}\n\"\"\"\n\n\ndef _assistant(text: str | None) -> str:\n    return f\"\"\"\\\n:robot: {_h3(\"Assistant\")}:{f\"{NL}> {text}\" if text is not None else \"\"}\n\"\"\"\n\n\ndef _make_ordered_list(items: list[protocol_schema.ConversationMessage]) -> list[str]:\n    return [f\"{num} {item.text}\" for num, item in zip(NUMBER_EMOJIS, items)]\n\n\ndef _ordered_list(items: list[protocol_schema.ConversationMessage]) -> str:\n    return \"\\n\\n\".join(_make_ordered_list(items))\n\n\ndef _conversation(conv: protocol_schema.Conversation) -> list[str]:\n    # return \"\\n\".join([_assistant(msg.text) if msg.is_assistant else _user(msg.text) for msg in conv.messages])\n    messages = map(\n        lambda m: f\"\"\"\\\n:robot: __Assistant__:\n{m.text}\n\"\"\"\n        if m.is_assistant\n        else f\"\"\"\\\n:person_red_hair: __User__:\n{m.text}\n\"\"\",\n        conv.messages,\n    )\n    return list(messages)\n\n\ndef _li(text: str) -> str:\n    return f\":small_blue_diamond: {text}\"\n\n\n###\n# Messages\n###\n\n\ndef initial_prompt_messages(task: protocol_schema.InitialPromptTask) -> list[str]:\n    \"\"\"Creates the message that gets sent to users when they request an `initial_prompt` task.\"\"\"\n    return [\n        f\"\"\"\\\n\n:small_blue_diamond: __**INITIAL PROMPT**__ :small_blue_diamond:\n\n\n:pencil: _Please provide an initial prompt to the assistant._{f\"{NL}Hint: {task.hint}\" if task.hint else \"\"}\n\"\"\"\n    ]\n\n\ndef rank_initial_prompts_messages(task: protocol_schema.RankInitialPromptsTask) -> list[str]:\n    \"\"\"Creates the message that gets sent to users when they request a `rank_initial_prompts` task.\"\"\"\n    return [\n        f\"\"\"\\\n\n:small_blue_diamond: __**RANK INITIAL PROMPTS**__ :small_blue_diamond:\n\n\n{_ordered_list(task.prompt_messages)}\n\n:trophy: _Reply with the numbers of best to worst prompts separated by commas (example: '4,1,3,2')_\n\"\"\"\n    ]\n\n\ndef rank_prompter_reply_messages(task: protocol_schema.RankPrompterRepliesTask) -> list[str]:\n    \"\"\"Creates the message that gets sent to users when they request a `rank_prompter_replies` task.\"\"\"\n    return [\n        \"\"\"\\\n\n:small_blue_diamond: __**RANK PROMPTER REPLIES**__ :small_blue_diamond:\n\n\"\"\",\n        *_conversation(task.conversation),\n        f\"\"\":person_red_hair: __User__:\n{_ordered_list(task.reply_messages)}\n\n:trophy: _Reply with the numbers of best to worst replies separated by commas (example: '4,1,3,2')_\n\"\"\",\n    ]\n\n\ndef rank_assistant_reply_message(task: protocol_schema.RankAssistantRepliesTask) -> list[str]:\n    \"\"\"Creates the message that gets sent to users when they request a `rank_assistant_replies` task.\"\"\"\n    return [\n        \"\"\"\\\n\n:small_blue_diamond: __**RANK ASSISTANT REPLIES**__ :small_blue_diamond:\n\n\"\"\",\n        *_conversation(task.conversation),\n        f\"\"\":robot: __Assistant__:,\n{_ordered_list(task.reply_messages)}\n:trophy: _Reply with the numbers of best to worst replies separated by commas (example: '4,1,3,2')_\n\"\"\",\n    ]\n\n\ndef rank_conversation_reply_messages(task: protocol_schema.RankConversationRepliesTask) -> list[str]:\n    \"\"\"Creates the message that gets sent to users when they request a `rank_conversation_replies` task.\"\"\"\n    return [\n        \"\"\"\\\n\n:small_blue_diamond: __**RANK CONVERSATION REPLIES**__ :small_blue_diamond:\n\n\"\"\",\n        *_conversation(task.conversation),\n        f\"\"\":person_red_hair: __User__:\n{_ordered_list(task.reply_messages)}\n\"\"\",\n    ]\n\n\ndef label_initial_prompt_message(task: protocol_schema.LabelInitialPromptTask) -> str:\n    \"\"\"Creates the message that gets sent to users when they request a `label_initial_prompt` task.\"\"\"\n    return f\"\"\"\\\n\n{_h1(\"LABEL INITIAL PROMPT\")}\n\n\n{task.prompt}\n\n{_label_prompt(\"Reply with labels for the prompt separated by commas (example: 'profanity,misleading')\", task.mandatory_labels, task.valid_labels)}\n\"\"\"\n\n\ndef label_prompter_reply_messages(task: protocol_schema.LabelPrompterReplyTask) -> list[str]:\n    \"\"\"Creates the message that gets sent to users when they request a `label_prompter_reply` task.\"\"\"\n    return [\n        f\"\"\"\\\n\n{_h1(\"LABEL PROMPTER REPLY\")}\n\n\n\"\"\",\n        *_conversation(task.conversation),\n        f\"\"\"{_user(None)}\n{task.reply}\n\n{_label_prompt(\"Reply with labels for the reply separated by commas (example: 'profanity,misleading')\", task.mandatory_labels, task.valid_labels)}\n\"\"\",\n    ]\n\n\ndef label_assistant_reply_messages(task: protocol_schema.LabelAssistantReplyTask) -> list[str]:\n    \"\"\"Creates the message that gets sent to users when they request a `label_assistant_reply` task.\"\"\"\n    return [\n        f\"\"\"\\\n\n{_h1(\"LABEL ASSISTANT REPLY\")}\n\n\n\"\"\",\n        *_conversation(task.conversation),\n        f\"\"\"\n{_assistant(None)}\n{task.reply}\n\n{_label_prompt(\"Reply with labels for the reply separated by commas (example: 'profanity,misleading')\", task.mandatory_labels, task.valid_labels)}\n\"\"\",\n    ]\n\n\ndef prompter_reply_messages(task: protocol_schema.PrompterReplyTask) -> list[str]:\n    \"\"\"Creates the message that gets sent to users when they request a `prompter_reply` task.\"\"\"\n    return [\n        \"\"\"\\\n:small_blue_diamond: __**PROMPTER REPLY**__ :small_blue_diamond:\n\n\"\"\",\n        *_conversation(task.conversation),\n        f\"\"\"{f\"{NL}Hint: {task.hint}\" if task.hint else \"\"}\n\n:speech_balloon: _Please provide a reply to the assistant._\n\"\"\",\n    ]\n\n\n# def prompter_reply_messages2(task: protocol_schema.PrompterReplyTask) -> list[str]:\n#     \"\"\"Creates the message that gets sent to users when they request a `prompter_reply` task.\"\"\"\n#     return [\n#         message_templates.render(\"title.msg\", \"PROMPTER REPLY\"),\n#         *[message_templates.render(\"conversation_message.msg\", conv) for conv in task.conversation],\n#         message_templates.render(\"prompter_reply_task.msg\", task.hint),\n#     ]\n\n\ndef assistant_reply_messages(task: protocol_schema.AssistantReplyTask) -> list[str]:\n    \"\"\"Creates the message that gets sent to users when they request a `assistant_reply` task.\"\"\"\n    return [\n        \"\"\"\\\n:small_blue_diamond: __**ASSISTANT REPLY**__ :small_blue_diamond:\n\n\"\"\",\n        *_conversation(task.conversation),\n        \"\"\"\\\n\n:speech_balloon: _Please provide a reply to the user as the assistant._\n\"\"\",\n    ]\n\n\ndef confirm_text_response_message(content: str) -> str:\n    return f\"\"\"\\\n{_h2(\"CONFIRM RESPONSE\")}\n\n> {content}\n\"\"\"\n\n\ndef confirm_ranking_response_message(content: str, items: list[protocol_schema.ConversationMessage]) -> str:\n    user_rankings = [int(r) for r in content.replace(\" \", \"\").split(\",\")]\n    original_list = _make_ordered_list(items)\n    user_ranked_list = \"\\n\\n\".join([original_list[r - 1] for r in user_rankings])\n\n    return f\"\"\"\\\n{_h2(\"CONFIRM RESPONSE\")}\n\n{user_ranked_list}\n\"\"\"\n\n\ndef help_message(can_manage_guild: bool, is_dev: bool) -> str:\n    \"\"\"The /help command message.\"\"\"\n    content = f\"\"\"\\\n{_h1(\"HELP\")}\n\n{_li(\"**`/help`**\")}\nShow this message.\n\n{_li(\"**`/work [type]`**\")}\nStart a new task.\n**`[type]`**:\nThe type of task to start. If not provided, a random task will be selected. The different types are\n:small_orange_diamond: `random`: A random task type\n:small_orange_diamond: ~~`summarize_story`~~ (coming soon)\n:small_orange_diamond: ~~`rate_summary`~~ (coming soon)\n:small_orange_diamond: `initial_prompt`: Ask the assistant something\n:small_orange_diamond: `prompter_reply`: Reply to the assistant\n:small_orange_diamond: `assistant_reply`: Reply to the user\n:small_orange_diamond: `rank_initial_prompts`: Rank some initial prompts\n:small_orange_diamond: `rank_prompter_replies`: Rank some prompter replies\n:small_orange_diamond: `rank_assistant_replies`: Rank some assistant replies\n\nTo learn how to complete tasks, run `/tutorial`.\n\"\"\"\n    if can_manage_guild:\n        content += f\"\"\"\\\n\n{_li(\"**`/settings log_channel <channel>`**\")}\nSet the channel that the bot logs completed task messages in.\n**`<channel>`**: The channel to log completed tasks in. The bot needs to be able to send messages in this channel.\n\n{_li(\"**`/settings get`**\")}\nGet the current settings.\n\"\"\"\n    if is_dev:\n        content += f\"\"\"\\\n\n{_li(\"**`/reload [plugin]`**\")}\nHot-reload a plugin. Only code *inside* of function bodies will be updated.\nAny changes to __function signatures__, __other files__, __decorators__, or __imports__ will require a restart.\n**`[plugin]`**:\nThe plugin to hot-reload. If no plugin is provided, all plugins are hot-reload.\n\"\"\"\n    return content\n\n\ndef tutorial_message() -> str:\n    \"\"\"The /tutorial command message.\"\"\"\n    # TODO: Finish message\n    return f\"\"\"\\\n{_h1(\"TUTORIAL\")}\n\"\"\"\n\n\ndef confirm_label_response_message(content: str) -> str:\n    user_labels = content.lower().replace(\" \", \"\").split(\",\")\n    user_labels_str = \", \".join(user_labels)\n\n    return f\"\"\"\\\n{_h2(\"CONFIRM RESPONSE\")}\n\n{user_labels_str}\n\"\"\"\n\n\n###\n# Embeds\n###\n\n\ndef task_complete_embed(task: protocol_schema.Task, mention: str) -> hikari.Embed:\n    return (\n        hikari.Embed(\n            title=\"Task Completion\",\n            description=f\"`{task.type}` completed by {mention}\",\n            color=hikari.Color(0x00FF00),\n            timestamp=datetime.now().astimezone(),\n        )\n        .add_field(\"Total Tasks\", \"0\", inline=True)\n        .add_field(\"Server Ranking\", \"0/0\", inline=True)\n        .add_field(\"Global Ranking\", \"0/0\", inline=True)\n        .set_footer(f\"Task ID: {task.id}\")\n    )\n\n\ndef invalid_user_input_embed(error_message: str) -> hikari.Embed:\n    return hikari.Embed(\n        title=\"Invalid User Input\",\n        description=error_message,\n        color=hikari.Color(0xFF0000),\n        timestamp=datetime.now().astimezone(),\n    )\n\n\ndef plain_embed(text: str) -> hikari.Embed:\n    return hikari.Embed(color=0x36393F, description=text)\n", "discord-bots/oa-bot-py/bot/db/schemas.py": "\"\"\"Database schemas.\"\"\"\nimport typing as t\n\nfrom aiosqlite import Connection, Row\nfrom pydantic import BaseModel\n\n\nclass GuildSettings(BaseModel):\n    \"\"\"Guild settings.\"\"\"\n\n    guild_id: int\n    log_channel_id: int | None\n\n    @classmethod\n    def parse_obj(cls, obj: Row) -> \"GuildSettings\":\n        \"\"\"Deserialize a Row object from aiosqlite into a GuildSettings object.\"\"\"\n        return cls(guild_id=obj[0], log_channel_id=obj[1])\n\n    @classmethod\n    async def from_db(cls, conn: Connection, guild_id: int) -> t.Optional[\"GuildSettings\"]:\n        async with conn.cursor() as cursor:\n            await cursor.execute(\"SELECT * FROM guild_settings WHERE guild_id = ?\", (guild_id,))\n            row = await cursor.fetchone()\n            if row is None:\n                return None\n\n            return cls.parse_obj(row)\n", "discord-bots/oa-bot-py/bot/extensions/user_input_test.py": "\"\"\"Task plugin for testing different data collection methods.\"\"\"\n# TODO: Delete this once user input method has been decided for final bot.\nimport asyncio\nimport typing as t\nfrom datetime import datetime, timedelta\n\nimport hikari\nimport lightbulb\nimport lightbulb.decorators\nimport miru\nfrom bot.utils import format_time\nfrom oasst_shared.schemas.protocol import TaskRequestType\n\nplugin = lightbulb.Plugin(\"TaskPlugin\")\n\nMAX_TASK_TIME = 60 * 60\nMAX_TASK_ACCEPT_TIME = 60\n\n\n@plugin.command\n@lightbulb.option(\n    \"type\",\n    \"The type of task to request.\",\n    choices=[hikari.CommandChoice(name=task.split(\".\")[-1], value=task) for task in TaskRequestType],\n    required=False,\n    default=TaskRequestType.summarize_story,\n    type=str,\n)\n@lightbulb.command(\"task_thread\", \"Request a task from the backend.\", ephemeral=True)\n@lightbulb.implements(lightbulb.SlashCommand)\nasync def task_thread(ctx: lightbulb.SlashContext):\n    \"\"\"Request a task from the backend.\"\"\"\n    typ: str = ctx.options.type\n\n    # Create a thread for the task\n    thread = await ctx.bot.rest.create_thread(ctx.channel_id, hikari.ChannelType.GUILD_PUBLIC_THREAD, f\"Task: {typ}\")\n\n    await ctx.respond(f\"Please complete the task in the thread: {thread.mention}\")\n\n    # Send the task in the thread\n    await thread.send(\n        f\"\"\"\\\nPlease complete the task.\nSample Task\n\nSelf destruct {format_time(datetime.now() + timedelta(seconds=MAX_TASK_TIME), 'R')}\n\"\"\"\n    )\n\n    # Wait for the user to respond\n    try:\n        event = await ctx.bot.wait_for(\n            hikari.GuildMessageCreateEvent,\n            timeout=MAX_TASK_TIME,\n            predicate=lambda e: e.author.id == ctx.author.id and e.channel_id == thread.id,\n        )\n        await ctx.respond(f\"Received message: {event.message.content}\")\n    except asyncio.TimeoutError:\n        await ctx.respond(\"You took too long to respond.\")\n    finally:\n        await thread.delete()\n\n\n@plugin.command\n@lightbulb.option(\n    \"type\",\n    \"The type of task to request.\",\n    choices=[hikari.CommandChoice(name=task.split(\".\")[-1], value=task) for task in TaskRequestType],\n    required=False,\n    default=TaskRequestType.summarize_story,\n    type=str,\n)\n@lightbulb.command(\"task_dm\", \"Request a task from the backend.\", ephemeral=True)\n@lightbulb.implements(lightbulb.SlashCommand, lightbulb.PrefixCommand)\nasync def task_dm(ctx: lightbulb.Context):\n    \"\"\"Request a task from the backend.\"\"\"\n    await ctx.respond(\"Please complete the task in your DMs\")\n\n    # Send the task in the dm\n    await ctx.author.send(\n        f\"\"\"\\\nPlease complete the task.\nSample Task\n\nSelf destruct {format_time(datetime.now() + timedelta(seconds=MAX_TASK_TIME), 'R')}\n\"\"\"\n    )\n\n    # Wait for the user to respond\n    try:\n        event = await ctx.bot.wait_for(\n            hikari.DMMessageCreateEvent,\n            timeout=MAX_TASK_TIME,\n            predicate=lambda e: e.author.id == ctx.author.id,\n        )\n        await ctx.respond(f\"Received message: {event.message.content}\")\n    except asyncio.TimeoutError:\n        await ctx.respond(\"You took too long to respond.\")\n\n\nclass TaskModal(miru.Modal):\n    \"\"\"Modal for submitting a task.\"\"\"\n\n    response = miru.TextInput(\n        label=\"Response\",\n        placeholder=\"Enter your response!\",\n        required=True,\n        style=hikari.TextInputStyle.PARAGRAPH,\n        row=2,\n    )\n\n    async def callback(self, context: miru.ModalContext) -> None:\n        await context.respond(f\"Received response: {self.response.value}\", flags=hikari.MessageFlag.EPHEMERAL)\n\n\nclass ModalView(miru.View):\n    \"\"\"View for opening a modal.\"\"\"\n\n    def __init__(self, modal_title: str, task: str, *args: t.Any, **kwargs: t.Any) -> None:\n        super().__init__(*args, **kwargs)\n        self.modal_title = modal_title\n        self.task = task\n\n    @miru.button(label=\"Start Task!\", style=hikari.ButtonStyle.PRIMARY)\n    async def modal_button(self, button: miru.Button, ctx: miru.ViewContext) -> None:\n        modal = TaskModal(title=self.modal_title)\n        modal.add_item(miru.TextInput(label=\"Task\", value=self.task, style=hikari.TextInputStyle.PARAGRAPH, row=1))\n        await ctx.respond_with_modal(modal)\n\n\n@plugin.command\n@lightbulb.option(\n    \"type\",\n    \"The type of task to request.\",\n    choices=[hikari.CommandChoice(name=task.split(\".\")[-1], value=task) for task in TaskRequestType],\n    required=False,\n    default=TaskRequestType.summarize_story,\n    type=str,\n)\n@lightbulb.command(\"task_modal\", \"Request a task from the backend.\", ephemeral=True, auto_defer=True)\n@lightbulb.implements(lightbulb.SlashCommand)\nasync def task_modal(ctx: lightbulb.SlashContext):\n    \"\"\"Request a task from the backend.\"\"\"\n    # typ: str = ctx.options.type\n    view = ModalView(\n        modal_title=\"Assistant Response\",\n        task=\"Please explain the moon landing to a six year old.\",\n        timeout=MAX_TASK_TIME,\n    )\n    resp = await ctx.respond(\n        \"Task - Respond to the prompt as if you were the Assistant:\",\n        flags=hikari.MessageFlag.EPHEMERAL,\n        components=view,\n    )\n    await view.start(await resp.message())\n\n\nclass RatingView(miru.View):\n    \"\"\"View for rating a task.\"\"\"\n\n    def __init__(self, *args: t.Any, **kwargs: t.Any) -> None:\n        super().__init__(*args, **kwargs)\n        self.presses: list[str] = []\n\n    def _close_if_all_pressed(self) -> None:\n        if len(self.presses) == 5:\n            self.stop()\n\n    @miru.button(label=\"1\", style=hikari.ButtonStyle.PRIMARY)\n    async def button_1(self, button: miru.Button, ctx: miru.ViewContext) -> None:\n        if button.label not in self.presses:\n            self.presses.append(\"1\")\n        await ctx.respond(f\"Received response: {button.label}\", flags=hikari.MessageFlag.EPHEMERAL)\n        self._close_if_all_pressed()\n\n    @miru.button(label=\"2\", style=hikari.ButtonStyle.PRIMARY)\n    async def button_2(self, button: miru.Button, ctx: miru.ViewContext) -> None:\n        if button.label not in self.presses:\n            self.presses.append(\"2\")\n        await ctx.respond(f\"Received response: {button.label}\", flags=hikari.MessageFlag.EPHEMERAL)\n        self._close_if_all_pressed()\n\n    @miru.button(label=\"3\", style=hikari.ButtonStyle.PRIMARY)\n    async def button_3(self, button: miru.Button, ctx: miru.ViewContext) -> None:\n        if button.label not in self.presses:\n            self.presses.append(\"3\")\n        await ctx.respond(f\"Received response: {button.label}\", flags=hikari.MessageFlag.EPHEMERAL)\n        self._close_if_all_pressed()\n\n    @miru.button(label=\"4\", style=hikari.ButtonStyle.PRIMARY)\n    async def button_4(self, button: miru.Button, ctx: miru.ViewContext) -> None:\n        if button.label not in self.presses:\n            self.presses.append(\"4\")\n        await ctx.respond(f\"Received response: {button.label}\", flags=hikari.MessageFlag.EPHEMERAL)\n        self._close_if_all_pressed()\n\n    @miru.button(label=\"5\", style=hikari.ButtonStyle.PRIMARY)\n    async def button_5(self, button: miru.Button, ctx: miru.ViewContext) -> None:\n        if button.label not in self.presses:\n            self.presses.append(\"5\")\n        await ctx.respond(f\"Received response: {button.label}\", flags=hikari.MessageFlag.EPHEMERAL)\n        self._close_if_all_pressed()\n\n    @miru.button(label=\"Reset\", style=hikari.ButtonStyle.DANGER)\n    async def reset_button(self, button: miru.Button, ctx: miru.ViewContext) -> None:\n        self.presses = []\n        await ctx.respond(f\"Received response: {button.label}\", flags=hikari.MessageFlag.EPHEMERAL)\n\n\nclass SelectRating(miru.View):\n    \"\"\"View for rating a task with a select menu.\"\"\"\n\n    @miru.select(\n        options=[\n            hikari.SelectMenuOption(\n                label=\"1\",\n                value=\"1\",\n                description=None,\n                emoji=None,\n                is_default=False,\n            ),\n            hikari.SelectMenuOption(\n                label=\"2\",\n                value=\"2\",\n                description=None,\n                emoji=None,\n                is_default=False,\n            ),\n            hikari.SelectMenuOption(\n                label=\"3\",\n                value=\"3\",\n                description=None,\n                emoji=None,\n                is_default=False,\n            ),\n        ],\n        placeholder=\"Select the good responses\",\n        min_values=0,\n        max_values=3,\n        row=3,\n    )\n    async def select(self, select: miru.Select, ctx: miru.ViewContext) -> None:\n        await ctx.respond(f\"You selected {select.values}\", flags=hikari.MessageFlag.EPHEMERAL)\n\n\n@plugin.command\n@lightbulb.command(\"rating_task\", \"Rate stuff.\")\n@lightbulb.implements(lightbulb.SlashCommand)\nasync def rating_task(ctx: lightbulb.SlashContext):\n    \"\"\"Rate stuff.\"\"\"\n    # Message Based rating\n    await ctx.respond(\n        \"List the responses in order of best to worst response (1,2,3,4,5)\", flags=hikari.MessageFlag.EPHEMERAL\n    )\n    try:\n        event = await ctx.bot.wait_for(\n            hikari.MessageCreateEvent, timeout=MAX_TASK_TIME, predicate=lambda e: e.author.id == ctx.author.id\n        )\n\n    except asyncio.TimeoutError:\n        await ctx.respond(\"Timed out waiting for response\")\n        return\n\n    if event.content is None:\n        await ctx.respond(\"No content in message\")\n        return\n    ratings = event.content.replace(\" \", \"\").split(\",\")\n\n    # Check if the ratings are valid\n    if len(ratings) != 5:\n        await ctx.respond(\"Invalid number of ratings\")\n    if not all([rating in (\"1\", \"2\", \"3\", \"4\", \"5\") for rating in ratings]):\n        await ctx.respond(\"Invalid rating\")\n\n    await ctx.respond(f\"Your responses: {ratings}\", flags=hikari.MessageFlag.EPHEMERAL)\n    # Button Based rating\n    view = RatingView(timeout=MAX_TASK_TIME)\n\n    resp = await ctx.respond(\"Click the buttons in order of best to worst response\", components=view)\n    await view.start(await resp.message())\n    await view.wait()\n    await ctx.respond(f\"Your responses: {view.presses}\", flags=hikari.MessageFlag.EPHEMERAL)\n    await resp.delete()\n\n    # Select Based rating\n    select_view = SelectRating(timeout=MAX_TASK_TIME)\n    resp_2 = await ctx.respond(\"Select the good responses\", components=select_view, flags=hikari.MessageFlag.EPHEMERAL)\n    await select_view.start(await resp_2.message())\n    await select_view.wait()\n    await resp_2.delete()\n\n\ndef load(bot: lightbulb.BotApp):\n    \"\"\"Add the plugin to the bot.\"\"\"\n    bot.add_plugin(plugin)\n\n\ndef unload(bot: lightbulb.BotApp):\n    \"\"\"Remove the plugin to the bot.\"\"\"\n    bot.remove_plugin(plugin)\n", "discord-bots/oa-bot-py/bot/extensions/hot_reload.py": "\"\"\"Hot reload plugin.\"\"\"\nfrom glob import glob\n\nimport hikari\nimport lightbulb\nfrom loguru import logger\n\nplugin = lightbulb.Plugin(\n    \"HotReloadPlugin\",\n)\nplugin.add_checks(lightbulb.owner_only)\n\nEXTENSIONS_FOLDER = \"bot/extensions\"\n\n\ndef _get_extensions() -> list[str]:\n    # Recursively get all the .py files in the extensions directory not starting with an `_`.\n    exts = glob(\"bot/extensions/**/[!_]*.py\", recursive=True)\n    # Turn the path into a plugin path (\"path/to/extension.py\" -> \"path.to.extension\")\n    return [ext.replace(\"/\", \".\").replace(\"\\\\\", \".\").replace(\".py\", \"\") for ext in exts]\n\n\nasync def _plugin_autocomplete(option: hikari.CommandInteractionOption, _: hikari.AutocompleteInteraction) -> list[str]:\n    # Check that the option is a string.\n    if not isinstance(option.value, str):\n        raise TypeError(f\"`option.value` must be of type `str`, it is currently a `{type(option.value)}`\")\n\n    exts = _get_extensions()\n    return [ext for ext in exts if option.value in ext]\n\n\n@plugin.command\n@lightbulb.option(\n    \"plugin\",\n    \"The plugin to reload. Leave empty to reload all plugins.\",\n    autocomplete=_plugin_autocomplete,\n    required=False,\n    default=None,\n)\n@lightbulb.command(\"reload\", \"Reload a plugin\", ephemeral=True)\n@lightbulb.implements(lightbulb.SlashCommand)\nasync def reload(ctx: lightbulb.SlashContext):\n    \"\"\"Reload a plugin or all plugins.\"\"\"\n    # If the plugin option is None, reload all plugins.\n    if ctx.options.plugin is None:\n        ctx.bot.reload_extensions(*_get_extensions())\n        await ctx.respond(\"Reloaded all plugins.\")\n        logger.info(\"Reloaded all plugins.\")\n    # Otherwise, reload the specified plugin.\n    else:\n        ctx.bot.reload_extensions(ctx.options.plugin)\n        await ctx.respond(f\"Reloaded `{ctx.options.plugin}`.\")\n        logger.info(f\"Reloaded `{ctx.options.plugin}`.\")\n\n\ndef load(bot: lightbulb.BotApp):\n    \"\"\"Add the plugin to the bot.\"\"\"\n    bot.add_plugin(plugin)\n\n\ndef unload(bot: lightbulb.BotApp):\n    \"\"\"Remove the plugin to the bot.\"\"\"\n    bot.remove_plugin(plugin)\n", "discord-bots/oa-bot-py/bot/extensions/text_labels.py": "\"\"\"Hot reload plugin.\"\"\"\nimport typing as t\nfrom datetime import datetime\n\nimport hikari\nimport lightbulb\nimport miru\nfrom aiosqlite import Connection\nfrom bot.db.schemas import GuildSettings\nfrom loguru import logger\n\nplugin = lightbulb.Plugin(\n    \"TextLabels\",\n)\nplugin.add_checks(lightbulb.guild_only)  # Context menus are only enabled in guilds\n\n\nDISCORD_GRAY = 0x2F3136\n\n\ndef clamp(num: float) -> float:\n    \"\"\"Clamp a number between 0 and 1.\"\"\"\n    return min(max(0.0, num), 1.0)\n\n\nclass LabelModal(miru.Modal):\n    \"\"\"Modal for submitting text labels.\"\"\"\n\n    def __init__(self, label: str, content: str, *args: t.Any, **kwargs: t.Any):\n        super().__init__(*args, **kwargs)\n        self.label = label\n        self.original_content = content\n\n        # Add the text of the message to the modal\n        self.content = miru.TextInput(\n            label=\"Text\", style=hikari.TextInputStyle.PARAGRAPH, value=content, required=True, row=1\n        )\n        self.add_item(self.content)\n\n    value = miru.TextInput(label=\"Value\", placeholder=\"Enter a value between 0 and 1\", required=True, row=2)\n\n    async def callback(self, context: miru.ModalContext) -> None:\n        val = float(self.value.value) if self.value.value else 0.0\n        val = clamp(val)\n\n        edited = self.content.value != self.original_content\n        await context.respond(\n            f\"Sending {self.label}=`{val}` for `{self.content.value}` (edited={edited}) to the backend.\",\n            flags=hikari.MessageFlag.EPHEMERAL,\n        )\n        logger.info(f\"Sending {self.label}=`{val}` for `{self.content.value}` (edited={edited}) to the backend.\")\n\n        # Send a notification to the log channel\n        assert context.guild_id is not None  # `guild_only` check\n        conn: Connection = context.bot.d.db  # type: ignore\n        guild_settings = await GuildSettings.from_db(conn, context.guild_id)\n\n        if guild_settings is None or guild_settings.log_channel_id is None:\n            logger.warning(f\"No guild settings or log channel for guild {context.guild_id}\")\n            return\n\n        embed = (\n            hikari.Embed(\n                title=\"Message Label\",\n                description=f\"{context.author.mention} labeled a message as `{self.label}`.\",\n                timestamp=datetime.now().astimezone(),\n                color=0x00FF00,\n            )\n            .set_author(name=context.author.username, icon=context.author.avatar_url)\n            .add_field(\"Total Labeled Message\", \"0\", inline=True)\n            .add_field(\"Server Ranking\", \"0/0\", inline=True)\n            .add_field(\"Global Ranking\", \"0/0\", inline=True)\n        )\n        channel = await context.bot.rest.fetch_channel(guild_settings.log_channel_id)\n        assert isinstance(channel, hikari.TextableChannel)\n        await channel.send(embed=embed)\n\n\nclass LabelSelect(miru.View):\n    \"\"\"Select menu for selecting a label.\n\n    The current labels are:\n    - contains toxic language\n    - encourages illegal activity\n    - good quality\n    - bad quality\n    - is spam\n    \"\"\"\n\n    def __init__(self, content: str, *args: t.Any, **kwargs: t.Any):\n        super().__init__(*args, **kwargs)\n        self.content = content\n\n    @miru.select(\n        options=[\n            hikari.SelectMenuOption(\n                label=\"Toxic Language\",\n                value=\"toxic_language\",\n                description=\"The message contains toxic language.\",\n                is_default=False,\n                emoji=None,\n            ),\n            hikari.SelectMenuOption(\n                label=\"Illegal Activity\",\n                value=\"illegal_activity\",\n                description=\"The message encourages illegal activity.\",\n                is_default=False,\n                emoji=None,\n            ),\n            hikari.SelectMenuOption(\n                label=\"Good Quality\",\n                value=\"good_quality\",\n                description=\"The message is good quality.\",\n                is_default=False,\n                emoji=None,\n            ),\n            hikari.SelectMenuOption(\n                label=\"Bad Quality\",\n                value=\"bad_quality\",\n                description=\"The message is bad quality.\",\n                is_default=False,\n                emoji=None,\n            ),\n            hikari.SelectMenuOption(\n                label=\"Spam\",\n                value=\"spam\",\n                description=\"The message is spam.\",\n                is_default=False,\n                emoji=None,\n            ),\n        ],\n        min_values=1,\n        max_values=1,\n    )\n    async def label_select(self, select: miru.Select, ctx: miru.ViewContext) -> None:\n        \"\"\"Handle the select menu.\"\"\"\n        label = select.values[0]\n        modal = LabelModal(label, self.content, title=f\"Text Label: {label}\", timeout=60)\n        await modal.send(ctx.interaction)\n        await modal.wait()\n\n        self.stop()\n\n\n@plugin.command\n@lightbulb.command(\"Label Message\", \"Label a message\")\n@lightbulb.implements(lightbulb.MessageCommand)\nasync def label_message_text(ctx: lightbulb.MessageContext):\n    \"\"\"Label a message.\"\"\"\n    # We have to do some funny interaction chaining because discord only allows one component (select or modal) per interaction\n    # so the select menu will open the modal\n\n    msg: hikari.Message = ctx.options.target\n    # Exit if the message is empty\n    if not msg.content:\n        await ctx.respond(\"Cannot label an empty message.\", flags=hikari.MessageFlag.EPHEMERAL)\n        return\n\n    # Send the select menu\n    # The modal will be opened from the select menu interaction\n    embed = hikari.Embed(title=\"Label Message\", description=\"Select a label for the message.\", color=DISCORD_GRAY)\n    label_select_view = LabelSelect(\n        msg.content,\n        timeout=60,\n    )\n    resp = await ctx.respond(embed=embed, components=label_select_view, flags=hikari.MessageFlag.EPHEMERAL)\n\n    await label_select_view.start(await resp.message())\n    await label_select_view.wait()\n\n\ndef load(bot: lightbulb.BotApp):\n    \"\"\"Add the plugin to the bot.\"\"\"\n    bot.add_plugin(plugin)\n\n\ndef unload(bot: lightbulb.BotApp):\n    \"\"\"Remove the plugin to the bot.\"\"\"\n    bot.remove_plugin(plugin)\n", "discord-bots/oa-bot-py/bot/extensions/guild_settings.py": "\"\"\"Guild settings.\"\"\"\nimport hikari\nimport lightbulb\nfrom aiosqlite import Connection\nfrom bot.db.schemas import GuildSettings\nfrom bot.utils import mention\nfrom lightbulb.utils import permissions_in\nfrom loguru import logger\n\nplugin = lightbulb.Plugin(\"GuildSettings\")\nplugin.add_checks(lightbulb.guild_only)\nplugin.add_checks(lightbulb.has_guild_permissions(hikari.Permissions.MANAGE_GUILD))\n\n\n@plugin.command\n@lightbulb.command(\"settings\", \"Bot settings for the server.\")\n@lightbulb.implements(lightbulb.SlashCommandGroup)\nasync def settings(_: lightbulb.SlashContext) -> None:\n    \"\"\"Bot settings for the server.\"\"\"\n    # This will never execute because it is a group\n    pass\n\n\n@settings.child\n@lightbulb.command(\"get\", \"Get all the guild settings.\")\n@lightbulb.implements(lightbulb.SlashSubCommand)\nasync def get(ctx: lightbulb.SlashContext) -> None:\n    \"\"\"Get one of or all the guild settings.\"\"\"\n    conn: Connection = ctx.bot.d.db\n    assert ctx.guild_id is not None  # `guild_only` check\n\n    async with conn.cursor() as cursor:\n        # Get all settings\n        await cursor.execute(\"SELECT * FROM guild_settings WHERE guild_id = ?\", (ctx.guild_id,))\n        row = await cursor.fetchone()\n\n        if row is None:\n            logger.warning(f\"No guild settings for {ctx.guild_id}\")\n            await ctx.respond(\"No settings found for this guild.\")\n            return\n\n        guild_settings = GuildSettings.parse_obj(row)\n\n        # Respond with all\n        # TODO: Embed\n        await ctx.respond(\n            f\"\"\"\\\n**Guild Settings**\n`log_channel`: {\nmention(guild_settings.log_channel_id, \"channel\")\nif guild_settings.log_channel_id else 'not set'}\n\"\"\"\n        )\n\n\n@settings.child\n@lightbulb.option(\"channel\", \"The channel to use.\", hikari.TextableGuildChannel)\n@lightbulb.command(\"log_channel\", \"Set the channel that the bot logs task and label completions in.\", ephemeral=True)\n@lightbulb.implements(lightbulb.SlashSubCommand)\nasync def log_channel(ctx: lightbulb.SlashContext) -> None:\n    \"\"\"Set the channel that the bot logs task and label completions in.\"\"\"\n    channel: hikari.TextableGuildChannel = ctx.options.channel\n    conn: Connection = ctx.bot.d.db\n    assert ctx.guild_id is not None  # `guild_only` check\n\n    # Check if the bot can send messages in that channel\n    assert isinstance(channel, hikari.InteractionChannel)  # Slash commands are interactions\n    me = ctx.bot.cache.get_me() or await ctx.bot.rest.fetch_my_user()\n    own_member = ctx.bot.cache.get_member(ctx.guild_id, me.id) or await ctx.bot.rest.fetch_member(ctx.guild_id, me.id)\n\n    # Get the channel from the cache if it is there, otherwise fetch it\n    if (ch := ctx.bot.cache.get_guild_channel(channel.id)) is None:\n        ch = {ch.id: ch for ch in await ctx.bot.rest.fetch_guild_channels(channel.id)}[channel.id]\n\n    if not isinstance(ch, hikari.GuildTextChannel):\n        await ctx.respond(f\"{ch.mention} is not a text channel.\")\n        return\n\n    # if the bot's permissions for this channel don't contain SEND_MESSAGE\n    # This will also filter out categories and voice channels\n    if not permissions_in(ch, own_member) & hikari.Permissions.SEND_MESSAGES:\n        await ctx.respond(f\"I don't have permission to send messages in {ch.mention}.\")\n        return\n\n    await ctx.respond(f\"Setting `log_channel` to {channel.mention}.\")\n\n    # update the database\n    async with conn.cursor() as cursor:\n        await cursor.execute(\n            \"INSERT OR REPLACE INTO guild_settings (guild_id, log_channel_id) VALUES (?, ?)\",\n            (ctx.guild_id, channel.id),\n        )\n    await conn.commit()\n    logger.info(f\"Updated `log_channel` for {ctx.guild_id} to {channel.id}.\")\n\n\ndef load(bot: lightbulb.BotApp):\n    \"\"\"Add the plugin to the bot.\"\"\"\n    bot.add_plugin(plugin)\n\n\ndef unload(bot: lightbulb.BotApp):\n    \"\"\"Remove the plugin to the bot.\"\"\"\n    bot.remove_plugin(plugin)\n", "discord-bots/oa-bot-py/bot/extensions/help.py": "\"\"\"Custom help command.\"\"\"\nimport lightbulb\nfrom bot.messages import help_message, tutorial_message\nfrom bot.settings import Settings\nfrom hikari.permissions import Permissions\nfrom lightbulb.utils import permissions_for\n\nplugin = lightbulb.Plugin(\"HelpPlugin\")\n\nsettings = Settings()\n\n\n@plugin.command\n@lightbulb.command(\"help\", \"Help for the bot.\", ephemeral=True)\n@lightbulb.implements(lightbulb.SlashCommand, lightbulb.PrefixCommand)\nasync def help_command(ctx: lightbulb.Context) -> None:\n    \"\"\"Help for the bot.\"\"\"\n    can_manage_guild = False\n    if ctx.guild_id:\n        member = ctx.bot.cache.get_member(ctx.guild_id, ctx.author.id) or await ctx.bot.rest.fetch_member(\n            ctx.guild_id, ctx.author.id\n        )\n        can_manage_guild = bool(permissions_for(member) & Permissions.MANAGE_GUILD)\n\n    await ctx.respond(help_message(can_manage_guild, ctx.author.id in settings.owner_ids))\n\n\n@plugin.command\n@lightbulb.command(\"tutorial\", \"A tutorial for completing tasks.\", ephemeral=True)\n@lightbulb.implements(lightbulb.SlashCommand, lightbulb.PrefixCommand)\nasync def tutorial(ctx: lightbulb.Context) -> None:\n    \"\"\"Help for the bot.\"\"\"\n    await ctx.respond(tutorial_message(True, True))\n\n\ndef load(bot: lightbulb.BotApp):\n    \"\"\"Add the plugin to the bot.\"\"\"\n    bot.add_plugin(plugin)\n\n\ndef unload(bot: lightbulb.BotApp):\n    \"\"\"Remove the plugin to the bot.\"\"\"\n    bot.remove_plugin(plugin)\n", "discord-bots/oa-bot-py/bot/extensions/work.py": "\"\"\"Work plugin for collecting user data.\"\"\"\nimport asyncio\nimport typing as t\nfrom uuid import UUID\n\nimport hikari\nimport lightbulb\nimport lightbulb.decorators\nimport miru\nfrom aiosqlite import Connection\nfrom bot.messages import (\n    assistant_reply_messages,\n    confirm_label_response_message,\n    confirm_ranking_response_message,\n    confirm_text_response_message,\n    initial_prompt_messages,\n    label_assistant_reply_messages,\n    label_prompter_reply_messages,\n    plain_embed,\n    prompter_reply_messages,\n    rank_assistant_reply_message,\n    rank_conversation_reply_messages,\n    rank_initial_prompts_messages,\n    rank_prompter_reply_messages,\n    task_complete_embed,\n)\nfrom bot.settings import Settings\nfrom loguru import logger\nfrom oasst_shared.api_client import OasstApiClient\nfrom oasst_shared.schemas import protocol as protocol_schema\n\nplugin = lightbulb.Plugin(\"WorkPlugin\")\n\nMAX_TASK_TIME = 60 * 60  # seconds\nMAX_TASK_ACCEPT_TIME = 60 * 10  # seconds\n\nsettings = Settings()\n\n_Task_contra = t.TypeVar(\"_Task_contra\", bound=protocol_schema.Task, contravariant=True)\n\n\nclass _TaskHandler(t.Generic[_Task_contra]):\n    \"\"\"Handle user interaction for a task.\"\"\"\n\n    def __init__(self, ctx: lightbulb.Context, task: _Task_contra) -> None:\n        \"\"\"Create a new `TaskHandler`.\n\n        Args:\n            ctx (lightbulb.Context): The context of the command that started the task.\n            task (_Task_contra): The task to handle.\n        \"\"\"\n        self.ctx = ctx\n        self.task = task\n        self.task_messages = self.get_task_messages(task)\n        self.sent_messages: list[hikari.Message] = []\n\n    @staticmethod\n    def get_task_messages(task: _Task_contra) -> list[str]:\n        \"\"\"Get the messages to send to the user for the task.\"\"\"\n        raise NotImplementedError\n\n    async def send(self) -> t.Literal[\"accept\", \"next\", \"cancel\"] | None:\n        \"\"\"Send the task and wait for the user to accept/skip/cancel it.\"\"\"\n        # Send all but the last message because we need to attach buttons to the last one\n        logger.debug(f\"Sending {len(self.task_messages)} messages\\n{self.task_messages!r}\")\n        for task_msg in self.task_messages[:-1]:\n            if len(task_msg) > 2000:\n                logger.warning(f\"Attempting to send a message <2000 characters in length. Task id: {self.task.id}\")\n                task_msg = task_msg[:1999]\n            self.sent_messages.append(await self.ctx.author.send(task_msg))\n\n        # Send the last message with buttons\n        task_accept_view = TaskAcceptView(timeout=MAX_TASK_ACCEPT_TIME)\n        logger.debug(f\"TH Message length {len(self.task_messages[-1])}\")\n        last_msg = await self.ctx.author.send(self.task_messages[-1][:1999], components=task_accept_view)\n\n        await task_accept_view.start(last_msg)\n        await task_accept_view.wait()\n\n        return task_accept_view.choice\n\n    async def handle(self) -> None:\n        \"\"\"Handle the user's response to the task.\n\n        This method should be called after `send` has been called.\"\"\"\n        # Ack task to the backend\n        oasst_api: OasstApiClient = self.ctx.bot.d.oasst_api\n        await oasst_api.ack_task(self.task.id, message_id=f\"{self.sent_messages[0].id}\")\n\n        # Loop until the user's input is accepted\n        while True:\n            try:\n                # Wait for user to send a message\n                event = await self.ctx.bot.wait_for(\n                    hikari.DMMessageCreateEvent,\n                    predicate=lambda e: (\n                        e.author_id == self.ctx.author.id\n                        and e.message.content is not None\n                        and not e.message.content.startswith(settings.prefix)\n                    ),\n                    timeout=MAX_TASK_TIME,\n                )\n\n                # Validate the message\n                if event.content is None or not self.check_user_input(event.content):\n                    await self.ctx.author.send(\"Invalid input\")\n                    continue\n\n                # Confirm user input\n                if not (await self.confirm_user_input(event.content)):\n                    continue\n\n                # Message is valid and confirmed by user\n                break\n\n            except asyncio.TimeoutError:\n                return\n\n        next_task = await self.notify(event.content, event)\n        if not isinstance(next_task, protocol_schema.TaskDone):\n            raise TypeError(f\"Unknown task type: {next_task!r}\")\n\n        return\n\n    async def notify(self, content: str, event: hikari.DMMessageCreateEvent) -> protocol_schema.Task:\n        \"\"\"Notify the backend that the user completed the task.\"\"\"\n        raise NotImplementedError\n\n    async def confirm_user_input(self, content: str) -> bool:\n        \"\"\"Send the user's response back to the user and ask them to confirm it. Returns True if the user confirms.\"\"\"\n        raise NotImplementedError\n\n    def check_user_input(self, content: str) -> bool:\n        \"\"\"Check the user's response to the task. Returns True if the response is valid.\"\"\"\n        raise NotImplementedError\n\n    async def cancel(self, reason: str = \"not specified\") -> None:\n        \"\"\"Cancel the task.\"\"\"\n        oasst_api: OasstApiClient = self.ctx.bot.d.oasst_api\n        await oasst_api.nack_task(self.task.id, reason)\n\n\n_Ranking_contra = t.TypeVar(\n    \"_Ranking_contra\",\n    bound=protocol_schema.RankAssistantRepliesTask\n    | protocol_schema.RankInitialPromptsTask\n    | protocol_schema.RankPrompterRepliesTask\n    | protocol_schema.RankConversationRepliesTask,\n    contravariant=True,\n)\n\n\nclass _RankingTaskHandler(_TaskHandler[_Ranking_contra]):\n    \"\"\"This should not be used directly. Use its subclasses instead.\"\"\"\n\n    async def notify(self, content: str, event: hikari.DMMessageCreateEvent) -> protocol_schema.Task:\n        oasst_api: OasstApiClient = self.ctx.bot.d.oasst_api\n\n        task = await oasst_api.post_interaction(\n            protocol_schema.MessageRanking(\n                user=protocol_schema.User(\n                    id=f\"{self.ctx.author.id}\", auth_method=\"discord\", display_name=self.ctx.author.username\n                ),\n                ranking=[int(r) - 1 for r in content.split(\",\")],\n                message_id=f\"{self.sent_messages[0].id}\",\n            )\n        )\n\n        db: Connection = self.ctx.bot.d.db\n        async with db.cursor() as cursor:\n            row = await (\n                await cursor.execute(\"SELECT log_channel_id FROM guilds WHERE guild_id = ?\", (self.ctx.guild_id,))\n            ).fetchone()\n            log_channel = row[0] if row else None\n        log_messages: list[hikari.Message] = []\n\n        if log_channel is not None:\n            for message in self.task_messages[:-1]:\n                msg = await self.ctx.bot.rest.create_message(log_channel, message)\n                log_messages.append(msg)\n            await self.ctx.bot.rest.create_message(log_channel, task_complete_embed(self.task, self.ctx.author.mention))\n\n        return task\n\n\nclass RankAssistantRepliesHandler(_RankingTaskHandler[protocol_schema.RankAssistantRepliesTask]):\n    @staticmethod\n    def get_task_messages(task: protocol_schema.RankAssistantRepliesTask) -> list[str]:\n        return rank_assistant_reply_message(task)\n\n    def check_user_input(self, content: str) -> bool:\n        return len(content.split(\",\")) == len(self.task.reply_messages) and all(\n            [r.isdigit() and int(r) in range(1, len(self.task.reply_messages) + 1) for r in content.split(\",\")]\n        )\n\n    async def confirm_user_input(self, content: str) -> bool:\n        confirm_input_view = YesNoView()\n        msg = await self.ctx.author.send(\n            confirm_ranking_response_message(content, self.task.reply_messages), components=confirm_input_view\n        )\n        await confirm_input_view.start(msg)\n        await confirm_input_view.wait()\n\n        return bool(confirm_input_view.choice)\n\n\nclass RankInitialPromptHandler(_RankingTaskHandler[protocol_schema.RankInitialPromptsTask]):\n    def __init__(self, ctx: lightbulb.Context, task: protocol_schema.RankInitialPromptsTask) -> None:\n        super().__init__(ctx, task)\n\n    @staticmethod\n    def get_task_messages(task: protocol_schema.RankInitialPromptsTask) -> list[str]:\n        return rank_initial_prompts_messages(task)\n\n    def check_user_input(self, content: str) -> bool:\n        return len(content.split(\",\")) == len(self.task.prompt_messages) and all(\n            [r.isdigit() and int(r) in range(1, len(self.task.prompt_messages) + 1) for r in content.split(\",\")]\n        )\n\n    async def confirm_user_input(self, content: str) -> bool:\n        confirm_input_view = YesNoView()\n        msg = await self.ctx.author.send(\n            confirm_ranking_response_message(content, self.task.prompt_messages), components=confirm_input_view\n        )\n        await confirm_input_view.start(msg)\n        await confirm_input_view.wait()\n\n        return bool(confirm_input_view.choice)\n\n\nclass RankPrompterReplyHandler(_RankingTaskHandler[protocol_schema.RankPrompterRepliesTask]):\n    @staticmethod\n    def get_task_messages(task: protocol_schema.RankPrompterRepliesTask) -> list[str]:\n        return rank_prompter_reply_messages(task)\n\n    def check_user_input(self, content: str) -> bool:\n        return len(content.split(\",\")) == len(self.task.reply_messages) and all(\n            [r.isdigit() and int(r) in range(1, len(self.task.reply_messages) + 1) for r in content.split(\",\")]\n        )\n\n    async def confirm_user_input(self, content: str) -> bool:\n        confirm_input_view = YesNoView()\n        msg = await self.ctx.author.send(\n            confirm_ranking_response_message(content, self.task.reply_messages), components=confirm_input_view\n        )\n        await confirm_input_view.start(msg)\n        await confirm_input_view.wait()\n\n        return bool(confirm_input_view.choice)\n\n\nclass RankConversationReplyHandler(_RankingTaskHandler[protocol_schema.RankConversationRepliesTask]):\n    @staticmethod\n    def get_task_messages(task: protocol_schema.RankConversationRepliesTask) -> list[str]:\n        return rank_conversation_reply_messages(task)\n\n    def check_user_input(self, content: str) -> bool:\n        return len(content.split(\",\")) == len(self.task.reply_messages) and all(\n            [r.isdigit() and int(r) in range(1, len(self.task.reply_messages) + 1) for r in content.split(\",\")]\n        )\n\n    async def confirm_user_input(self, content: str) -> bool:\n        confirm_input_view = YesNoView()\n        msg = await self.ctx.author.send(\n            confirm_ranking_response_message(content, self.task.reply_messages), components=confirm_input_view\n        )\n        await confirm_input_view.start(msg)\n        await confirm_input_view.wait()\n\n        return bool(confirm_input_view.choice)\n\n\nclass InitialPromptHandler(_TaskHandler[protocol_schema.InitialPromptTask]):\n    @staticmethod\n    def get_task_messages(task: protocol_schema.InitialPromptTask) -> list[str]:\n        return initial_prompt_messages(task)\n\n    def check_user_input(self, content: str) -> bool:\n        return len(content) > 0\n\n    async def confirm_user_input(self, content: str) -> bool:\n        confirm_input_view = YesNoView()\n        msg = await self.ctx.author.send(confirm_text_response_message(content), components=confirm_input_view)\n        await confirm_input_view.start(msg)\n        await confirm_input_view.wait()\n\n        return bool(confirm_input_view.choice)\n\n\nclass PrompterReplyHandler(_TaskHandler[protocol_schema.PrompterReplyTask]):\n    @staticmethod\n    def get_task_messages(task: protocol_schema.PrompterReplyTask) -> list[str]:\n        return prompter_reply_messages(task)\n\n    def check_user_input(self, content: str) -> bool:\n        return len(content) > 0\n\n    async def confirm_user_input(self, content: str) -> bool:\n        confirm_input_view = YesNoView()\n        msg = await self.ctx.author.send(confirm_text_response_message(content), components=confirm_input_view)\n        await confirm_input_view.start(msg)\n        await confirm_input_view.wait()\n\n        return bool(confirm_input_view.choice)\n\n\nclass AssistantReplyHandler(_TaskHandler[protocol_schema.AssistantReplyTask]):\n    @staticmethod\n    def get_task_messages(task: protocol_schema.AssistantReplyTask) -> list[str]:\n        return assistant_reply_messages(task)\n\n    def check_user_input(self, content: str) -> bool:\n        return len(content) > 0\n\n    async def confirm_user_input(self, content: str) -> bool:\n        confirm_input_view = YesNoView()\n        msg = await self.ctx.author.send(confirm_text_response_message(content), components=confirm_input_view)\n        await confirm_input_view.start(msg)\n        await confirm_input_view.wait()\n\n        return bool(confirm_input_view.choice)\n\n\n_Label_contra = t.TypeVar(\"_Label_contra\", bound=protocol_schema.LabelConversationReplyTask, contravariant=True)\n\n\nclass _LabelConversationReplyHandler(_TaskHandler[_Label_contra]):\n    def check_user_input(self, content: str) -> bool:\n        user_labels = content.split(\",\")\n        return (\n            all([l in self.task.valid_labels for l in user_labels])\n            and self.task.mandatory_labels is not None\n            and all([m in user_labels for m in self.task.mandatory_labels])\n        )\n\n    async def confirm_user_input(self, content: str) -> bool:\n        confirm_input_view = YesNoView()\n        msg = await self.ctx.author.send(confirm_label_response_message(content), components=confirm_input_view)\n        await confirm_input_view.start(msg)\n        await confirm_input_view.wait()\n\n        return bool(confirm_input_view.choice)\n\n\nclass LabelAssistantReplyHandler(_LabelConversationReplyHandler[protocol_schema.LabelAssistantReplyTask]):\n    @staticmethod\n    def get_task_messages(task: protocol_schema.LabelAssistantReplyTask) -> list[str]:\n        return label_assistant_reply_messages(task)\n\n\nclass LabelPrompterReplyHandler(_LabelConversationReplyHandler[protocol_schema.LabelPrompterReplyTask]):\n    @staticmethod\n    def get_task_messages(task: protocol_schema.LabelPrompterReplyTask) -> list[str]:\n        return label_prompter_reply_messages(task)\n\n\nsummarize_story = \"summarize_story\"\nrate_summary = \"rate_summary\"\n\n\n@plugin.command\n@lightbulb.command(\"work\", \"Complete a task.\")\n@lightbulb.implements(lightbulb.SlashCommand, lightbulb.PrefixCommand)\nasync def work2(ctx: lightbulb.Context) -> None:\n    \"\"\"Complete a task.\"\"\"\n    oasst_api: OasstApiClient = ctx.bot.d.oasst_api\n    currently_working: dict[hikari.Snowflake, UUID] = ctx.bot.d.currently_working\n\n    # Check if the user is already working on a task\n    if ctx.author.id in currently_working:\n        yn_view = YesNoView(timeout=MAX_TASK_ACCEPT_TIME)\n        msg = await ctx.author.send(\n            embed=plain_embed(\"You are already working. Would you like to cancel your old task start a new one?\"),\n            flags=hikari.MessageFlag.EPHEMERAL,\n            components=yn_view,\n        )\n        await yn_view.start(msg)\n        await yn_view.wait()\n\n        match yn_view.choice:\n            case False | None:\n                return\n            case True:\n                task_id = currently_working[ctx.author.id]\n                await oasst_api.nack_task(task_id, reason=\"user cancelled\")\n\n    if ctx.guild_id:\n        await ctx.respond(\"check DMs\", flags=hikari.MessageFlag.EPHEMERAL)\n\n    # Keep sending tasks until the user doesn't want more\n    try:\n        while True:\n            task = await oasst_api.fetch_random_task(\n                user=protocol_schema.User(\n                    id=f\"{ctx.author.id}\", display_name=ctx.author.username, auth_method=\"discord\"\n                ),\n            )\n\n            # Ranking tasks\n            if isinstance(task, protocol_schema.RankAssistantRepliesTask):\n                task_handler = RankAssistantRepliesHandler(ctx, task)\n            elif isinstance(task, protocol_schema.RankInitialPromptsTask):\n                task_handler = RankInitialPromptHandler(ctx, task)\n            elif isinstance(task, protocol_schema.RankPrompterRepliesTask):\n                task_handler = RankPrompterReplyHandler(ctx, task)\n            elif isinstance(task, protocol_schema.RankConversationRepliesTask):\n                task_handler = RankConversationReplyHandler(ctx, task)\n\n            # Text input tasks\n            elif isinstance(task, protocol_schema.InitialPromptTask):\n                task_handler = InitialPromptHandler(ctx, task)\n            elif isinstance(task, protocol_schema.PrompterReplyTask):\n                task_handler = PrompterReplyHandler(ctx, task)\n            elif isinstance(task, protocol_schema.AssistantReplyTask):\n                task_handler = AssistantReplyHandler(ctx, task)\n\n            # Label tasks\n            elif isinstance(task, protocol_schema.LabelAssistantReplyTask):\n                task_handler = LabelAssistantReplyHandler(ctx, task)\n            elif isinstance(task, protocol_schema.LabelPrompterReplyTask):\n                task_handler = LabelPrompterReplyHandler(ctx, task)\n\n            else:\n                raise ValueError(f\"Unknown task type: {type(task)}\")\n\n            resp = await task_handler.send()\n\n            match resp:\n                case \"accept\":\n                    currently_working[ctx.author.id] = task.id\n                    await task_handler.handle()\n                case \"next\":\n                    await task_handler.cancel(\"user skipped task\")\n                case \"cancel\":\n                    await task_handler.cancel(\"user canceled work\")\n                    break\n                case None:\n                    await task_handler.cancel(\"select timed out\")\n                    break\n    finally:\n        del currently_working[ctx.author.id]\n\n\nclass TaskAcceptView(miru.View):\n    \"\"\"View with three buttons: accept, next, and cancel.\n\n    The view stops once one of the buttons is pressed and the choice is stored in the `choice` attribute.\n    \"\"\"\n\n    choice: t.Literal[\"accept\", \"next\", \"cancel\"] | None = None\n\n    @miru.button(label=\"Accept\", custom_id=\"accept\", row=0, style=hikari.ButtonStyle.SUCCESS)\n    async def accept_button(self, button: miru.Button, ctx: miru.ViewContext) -> None:\n        logger.info(\"Accept button pressed\")\n        self.choice = \"accept\"\n        await ctx.message.edit(component=None)\n        self.stop()\n\n    @miru.button(label=\"Next Task\", custom_id=\"next_task\", row=0, style=hikari.ButtonStyle.SECONDARY)\n    async def next_button(self, button: miru.Button, ctx: miru.ViewContext) -> None:\n        logger.info(\"Next button pressed\")\n        self.choice = \"next\"\n        await ctx.message.edit(component=None)\n        self.stop()\n\n    @miru.button(label=\"Cancel\", custom_id=\"cancel\", row=0, style=hikari.ButtonStyle.DANGER)\n    async def cancel_button(self, button: miru.Button, ctx: miru.ViewContext) -> None:\n        logger.info(\"Cancel button pressed\")\n        self.choice = \"cancel\"\n        await ctx.message.edit(component=None)\n        self.stop()\n\n    async def on_timeout(self) -> None:\n        if self.message is not None:\n            await self.message.edit(component=None)\n\n\nclass YesNoView(miru.View):\n    \"\"\"View with two buttons: yes and no.\n\n    The view stops once one of the buttons is pressed and the choice is stored in the `choice` attribute.\n    \"\"\"\n\n    choice: bool | None = None\n\n    @miru.button(label=\"Yes\", custom_id=\"yes\", style=hikari.ButtonStyle.SUCCESS)\n    async def yes_button(self, button: miru.Button, ctx: miru.ViewContext) -> None:\n        self.choice = True\n        await ctx.message.edit(component=None)\n        self.stop()\n\n    @miru.button(label=\"No\", custom_id=\"no\", style=hikari.ButtonStyle.DANGER)\n    async def no_button(self, button: miru.Button, ctx: miru.ViewContext) -> None:\n        self.choice = False\n        await ctx.message.edit(component=None)\n        self.stop()\n\n    async def on_timeout(self) -> None:\n        if self.message is not None:\n            await self.message.edit(component=None)\n\n\ndef load(bot: lightbulb.BotApp):\n    \"\"\"Add the plugin to the bot.\"\"\"\n    bot.add_plugin(plugin)\n\n\ndef unload(bot: lightbulb.BotApp):\n    \"\"\"Remove the plugin to the bot.\"\"\"\n    bot.remove_plugin(plugin)\n", "discord-bots/oa-bot-py/bot/extensions/__init__.py": "\"\"\"Extensions for the bot.\n\nSee: https://hikari-lightbulb.readthedocs.io/en/latest/guides/extensions.html\n\"\"\"\n", "model/model_training/trainer_rm.py": "import argparse\nimport logging\nimport os\nfrom typing import Callable, Literal, Optional, Sequence, Union\n\nimport datasets\nimport torch\nfrom model_training.custom_datasets.ranking_collator import RankingDataCollator\nfrom model_training.efficiency_utils import fuse_gelu\nfrom model_training.metrics import RewardMetrics\nfrom model_training.utils.utils import (\n    PerDatasetSampler,\n    _strtobool,\n    get_dataset,\n    get_loss,\n    get_model,\n    get_tokenizer,\n    init_rng,\n    read_yamls,\n)\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Subset\nfrom tqdm import tqdm\nfrom transformers import PreTrainedModel, Trainer, TrainingArguments\nfrom transformers.trainer_pt_utils import IterableDatasetShard\nfrom transformers.trainer_utils import seed_worker\nfrom transformers.training_args import OptimizerNames\nfrom transformers.utils import is_datasets_available\n\n\nclass RMTrainer(Trainer):\n    def __init__(\n        self,\n        model: Union[PreTrainedModel, nn.Module] = None,\n        args: TrainingArguments = None,\n        sampler: torch.utils.data.sampler.Sampler = None,\n        loss_function: Literal[\"RMLoss\"] = \"RMLoss\",\n        score_l2_reg: float = 0.001,\n        train_collate_fn: Callable = None,\n        **kwargs,\n    ):\n        super().__init__(model, args, **kwargs)\n        self.train_collate_fn = train_collate_fn\n        self.loss_fct = get_loss(loss_function, score_l2_reg=score_l2_reg)\n        self.sampler = sampler\n\n    def compute_loss(self, model, inputs, return_logits=False):\n        batch, cu_lens = inputs\n\n        logits = model(\n            input_ids=batch[\"input_ids\"],\n            attention_mask=batch[\"attention_mask\"],\n        ).logits\n\n        loss = self.loss_fct(logits, cu_lens)\n\n        return (loss, logits) if return_logits else loss\n\n    def prediction_step(\n        self,\n        model: nn.Module,\n        inputs: tuple[dict[str, torch.Tensor], dict[str, torch.Tensor], list[int]],\n        prediction_loss_only: bool,\n        ignore_keys: Optional[list[str]] = None,\n    ) -> tuple[Optional[torch.Tensor], Optional[torch.Tensor], Optional[torch.Tensor]]:\n        batch, cu_lens = inputs\n        with torch.no_grad():\n            batch = self._prepare_inputs(batch)\n            loss, logits = self.compute_loss(model, (batch, cu_lens), return_logits=True)\n\n        loss = loss.mean().detach()\n\n        labels = []\n        for i, (s, e) in enumerate(zip(cu_lens[:-1], cu_lens[1:])):\n            labels.extend([i] * (e - s))\n        # make sure labels are same as logits, needed for deepspeed\n        labels = torch.tensor(labels, device=logits.device, requires_grad=False).view(-1, 1)\n        return (loss, logits.T, labels.T)  # transposed to avoid truncation in evaluation_loop\n\n    def get_train_dataloader(self):\n        \"\"\"\n        Inject custom data sampling behaviour into training loop\n        and use custom task mixing collate function : train_collate_fn\n\n        rewrite from:\n        https://github.com/huggingface/transformers/blob/67d074874d285e616393c65a0e670088e1b6b74a/src/transformers/trainer.py#L846\n        \"\"\"\n        data_collator = self.train_collate_fn\n        train_dataset = self.train_dataset\n        if is_datasets_available() and isinstance(train_dataset, datasets.Dataset):\n            train_dataset = self._remove_unused_columns(train_dataset, description=\"training\")\n\n        if isinstance(train_dataset, torch.utils.data.IterableDataset):\n            # if we are using iterable dataset it means no weight sampling\n            # added for backward compat\n            if self.args.world_size > 1:\n                train_dataset = IterableDatasetShard(\n                    train_dataset,\n                    batch_size=self._train_batch_size,\n                    drop_last=self.args.dataloader_drop_last,\n                    num_processes=self.args.world_size,\n                    process_index=self.args.process_index,\n                )\n            return DataLoader(\n                train_dataset,\n                batch_size=self.args.per_device_train_batch_size,\n                collate_fn=data_collator,\n                num_workers=self.args.dataloader_num_workers,\n                pin_memory=self.args.dataloader_pin_memory,\n            )\n\n        if self.sampler is None:\n            train_sampler = self._get_train_sampler()\n        else:\n            train_sampler = self.sampler\n            logging.warning(\"Custom sampler found!\")\n\n        dataloader = DataLoader(\n            train_dataset,\n            batch_size=self._train_batch_size,\n            sampler=train_sampler,\n            collate_fn=data_collator,\n            drop_last=self.args.dataloader_drop_last,\n            num_workers=self.args.dataloader_num_workers,\n            pin_memory=self.args.dataloader_pin_memory,\n            worker_init_fn=seed_worker,\n        )\n        return dataloader\n\n\ndef argument_parsing(notebook: bool = False, notebook_args: Sequence[str] | None = None):\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--configs\", nargs=\"+\", required=True)\n    parser.add_argument(\"--local_rank\", type=int, default=-1)\n    parser.add_argument(\"--deepspeed\", action=\"store_true\")\n    parser.add_argument(\"--no-deepspeed\", dest=\"deepspeed\", action=\"store_false\")\n    parser.add_argument(\"--wandb-entity\", type=str, default=\"open-assistant\")\n    parser.add_argument(\"--resume_from_checkpoint\", action=\"store_true\", help=\"Resume from last saved checkpoint\")\n    parser.add_argument(\"--rng_seed\", type=int, help=\"rng seed\")\n    parser.add_argument(\"--show_dataset_stats\", action=\"store_true\", help=\"Show dataset stats\", default=False)\n    parser.set_defaults(deepspeed=False)\n\n    if notebook:\n        args, remaining = parser.parse_known_args(notebook_args)\n    else:\n        args, remaining = parser.parse_known_args()\n\n    # Config from YAML\n    conf = {}\n    configs = read_yamls(\"./configs\")\n    for name in args.configs:\n        if \",\" in name:\n            for n in name.split(\",\"):\n                conf.update(configs[n])\n        else:\n            conf.update(configs[name])\n\n    conf[\"wandb_entity\"] = args.wandb_entity\n    conf[\"local_rank\"] = args.local_rank\n    conf[\"deepspeed\"] = args.deepspeed\n    conf[\"resume_from_checkpoint\"] = args.resume_from_checkpoint\n    if args.rng_seed is not None:\n        conf[\"rng_seed\"] = args.rng_seed\n    conf[\"show_dataset_stats\"] = args.show_dataset_stats\n\n    # get the world size in deepspeed\n    if conf[\"deepspeed\"]:\n        conf[\"world_size\"] = int(os.getenv(\"WORLD_SIZE\", default=\"1\"))\n    else:\n        conf[\"world_size\"] = 1\n\n    # Override config from command-line\n    parser = argparse.ArgumentParser()\n    for key, value in conf.items():\n        type_ = type(value) if value is not None else str\n        if type_ == bool:\n            type_ = _strtobool\n        parser.add_argument(f\"--{key}\", type=type_, default=value)\n\n    return parser.parse_args(remaining)\n\n\ndef main():\n    training_conf = argument_parsing()\n    if not training_conf.deepspeed or training_conf.local_rank == 0:\n        print(f\"trainig_conf = {training_conf}\")\n\n    init_rng(training_conf)\n\n    tokenizer = get_tokenizer(training_conf)\n    model = get_model(training_conf, tokenizer)\n\n    train, evals = get_dataset(training_conf, mode=\"rm\")\n    train_collate_fn = RankingDataCollator(\n        tokenizer,\n        max_length=training_conf.max_length,\n        pad_to_multiple_of=16,\n        max_replies=training_conf.max_replies,\n        use_system_tag=training_conf.use_system_tag,\n        system_property_dropout=training_conf.system_property_dropout,\n        system_add_length=training_conf.system_add_length,\n    )\n    eval_collate_fn = RankingDataCollator(\n        tokenizer,\n        max_length=training_conf.max_length,\n        pad_to_multiple_of=16,\n        max_replies=training_conf.max_replies,\n        use_system_tag=training_conf.use_system_tag,\n        system_property_dropout=training_conf.system_property_dropout,\n        system_add_length=training_conf.system_add_length,\n    )\n\n    show_dataset_stats = (training_conf.verbose or training_conf.show_dataset_stats) and (\n        not training_conf.deepspeed or training_conf.local_rank == 0\n    )\n    if show_dataset_stats:\n        print(\"Dataset stats before sampling:\")\n        total = len(train)\n        for d in train.datasets:\n            if isinstance(d, Subset):\n                name = f\"Subset of {type(d.dataset).__name__}\"\n                if hasattr(d.dataset, \"name\"):\n                    name += f\" ({d.dataset.name})\"\n            else:\n                name = type(d).__name__\n                if hasattr(d, \"name\"):\n                    name += f\" ({d.name})\"\n            print(f\"{name}: {len(d)} ({len(d) / total:%})\")\n        print(f\"Total train: {total}\")\n\n    if training_conf.use_custom_sampler:\n        samples_length = None\n        if training_conf.sort_by_length:\n            samples_length = list(\n                map(\n                    lambda x: train_collate_fn.process_one(x, return_length=True),\n                    tqdm(train, desc=\"Calculating lengths per sample\"),\n                )\n            )\n        sampler = PerDatasetSampler.build_sampler_from_config(\n            training_conf,\n            train.datasets,\n            rank=training_conf.local_rank,\n            world_size=training_conf.world_size,\n            samples_length=samples_length,\n            verbose=show_dataset_stats,\n        )\n    else:\n        sampler = None\n\n    optimizer = OptimizerNames.ADAMW_BNB if training_conf.quantization else OptimizerNames.ADAMW_HF\n\n    if training_conf.quantization:\n        import bitsandbytes\n\n        for module in model.modules():\n            if isinstance(module, torch.nn.Embedding):\n                bitsandbytes.optim.GlobalOptimManager.get_instance().register_module_override(\n                    module, \"weight\", {\"optim_bits\": 32}\n                )\n\n    if training_conf.fuse_gelu:\n        model = fuse_gelu(model)\n\n    output_dir = (\n        training_conf.output_dir\n        if training_conf.output_dir\n        else f\"{training_conf.model_name}-{training_conf.log_dir}-finetuned\"\n    )\n\n    args = TrainingArguments(\n        output_dir=output_dir,\n        num_train_epochs=training_conf.num_train_epochs,\n        warmup_steps=training_conf.warmup_steps,\n        learning_rate=float(training_conf.learning_rate),\n        deepspeed=training_conf.deepspeed_config if training_conf.deepspeed else None,\n        optim=optimizer,\n        fp16=training_conf.dtype in [\"fp16\", \"float16\"],\n        bf16=training_conf.dtype in [\"bf16\", \"bfloat16\"],\n        local_rank=training_conf.local_rank,\n        gradient_checkpointing=training_conf.gradient_checkpointing,\n        gradient_accumulation_steps=training_conf.gradient_accumulation_steps,\n        per_device_train_batch_size=training_conf.per_device_train_batch_size,\n        per_device_eval_batch_size=training_conf.per_device_eval_batch_size,\n        adam_beta1=training_conf.adam_beta1,\n        adam_beta2=training_conf.adam_beta2,\n        adam_epsilon=float(training_conf.adam_epsilon),\n        weight_decay=training_conf.weight_decay,\n        max_grad_norm=training_conf.max_grad_norm,\n        logging_steps=training_conf.logging_steps,\n        save_total_limit=training_conf.save_total_limit,\n        evaluation_strategy=\"steps\",\n        eval_steps=training_conf.eval_steps,\n        save_strategy=training_conf.save_strategy,\n        save_steps=training_conf.save_steps,\n        eval_accumulation_steps=training_conf.eval_accumulation_steps,\n        resume_from_checkpoint=training_conf.resume_from_checkpoint,\n        report_to=\"wandb\" if training_conf.log_wandb else None,\n    )\n\n    if not training_conf.log_wandb:\n        os.environ[\"WANDB_MODE\"] = \"offline\"\n\n    if training_conf.log_wandb and (not training_conf.deepspeed or training_conf.local_rank == 0):\n        import wandb\n\n        wandb.init(\n            project=\"reward-model\",\n            entity=training_conf.wandb_entity,\n            resume=training_conf.resume_from_checkpoint,\n            name=f\"{training_conf.model_name}-{training_conf.log_dir}-rm\",\n            config=training_conf,\n        )\n    compute_metrics = RewardMetrics(training_conf.metrics)\n    trainer = RMTrainer(\n        model=model,\n        args=args,\n        sampler=sampler,\n        train_collate_fn=train_collate_fn,\n        loss_function=training_conf.loss_fn,\n        score_l2_reg=training_conf.score_l2_reg,\n        train_dataset=train,\n        eval_dataset=evals,\n        data_collator=eval_collate_fn,\n        tokenizer=tokenizer,\n        compute_metrics=compute_metrics,\n    )\n    trainer.train(resume_from_checkpoint=training_conf.resume_from_checkpoint)\n    trainer.save_model()\n    tokenizer.save_pretrained(output_dir)\n\n\nif __name__ == \"__main__\":\n    main()\n", "model/model_training/check_dataset_counts.py": "import argparse\nfrom collections import Counter\nfrom enum import Enum\nfrom pathlib import Path\nfrom typing import Any\n\nimport pandas as pd\nimport yaml\nfrom langdetect import DetectorFactory, detect\nfrom model_training.custom_datasets.formatting import DatasetEntrySft\nfrom model_training.utils.utils import _strtobool, get_dataset\n\n\nclass Mode(str, Enum):\n    sft = \"sft\"\n    rm = \"rm\"\n    rl = \"rl\"\n\n    def config_name(self) -> str:\n        match self:\n            case Mode.sft:\n                return \"config.yaml\"\n            case Mode.rm:\n                return \"config_rm.yaml\"\n            case Mode.rl:\n                return \"config_rl.yaml\"\n\n    def default_config(self) -> str:\n        match self:\n            case Mode.sft:\n                return \"defaults\"\n            case Mode.rm:\n                return \"defaults_rm\"\n            case Mode.rl:\n                return \"defaults_rlhf\"\n\n\ndef read_yaml(dir: str | Path, config_file: str) -> dict[str, Any]:\n    with open(Path(dir) / config_file, \"r\") as f:\n        return yaml.safe_load(f)\n\n\ndef argument_parsing(notebook=False, notebook_args=None):\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--datasets\",\n        nargs=\"+\",\n        required=True,\n        help=\"\"\"\n        Multiple datasets can be passed to set different options.\n        For example, run as:\n\n           ./check_dataset_counts.py --datasets math oasst_export_eu\n\n        to check the counts of the math and the oasst_export_eu dataset.\n    \"\"\",\n    )\n    parser.add_argument(\"--mode\", dest=\"mode\", type=Mode, choices=list(Mode))\n    parser.add_argument(\"--output_path\", dest=\"output_path\", default=\"dataset_counts.csv\")\n    parser.add_argument(\"--detect_language\", default=False, action=\"store_true\")\n\n    if notebook:\n        args, remaining = parser.parse_known_args(notebook_args)\n    else:\n        args, remaining = parser.parse_known_args()\n\n    # Config from YAML\n    mode: Mode = args.mode\n    configs = read_yaml(\"./configs\", config_file=mode.config_name())\n    conf = configs[mode.default_config()]\n    if \"all\" in args.datasets:\n        conf[\"datasets\"] = configs[mode.default_config()][\"datasets\"] + configs[mode.default_config()][\"datasets_extra\"]\n    else:\n        # reset datasets, so that we only get the datasets defined in configs and remove the ones in the default\n        datasets_list = list()\n\n        for name in args.datasets:\n            # check and process multiple datasets\n            if \",\" in name:\n                for n in name.split(\",\"):\n                    datasets_value = configs[n].get(\"datasets\") or configs[n][\"datasets_extra\"]\n            # check if dataset is extra key in config\n            elif name in configs:\n                datasets_value = configs[name].get(\"datasets\") or configs[name][\"datasets_extra\"]\n            # check in default config\n            elif name in configs[mode.default_config()][\"datasets\"]:\n                datasets_value = [name]\n            else:\n                raise ValueError(\n                    f'Error: Could not find the dataset \"{name}\" in {mode.config_name()}. ',\n                    f\"Tried to look for this dataset within th key {mode.default_config()} \",\n                    \"and as separate key.\",\n                )\n\n            datasets_list.extend(datasets_value)\n    conf[\"mode\"] = mode\n    conf[\"output_path\"] = args.output_path\n    conf[\"datasets_extra\"] = []\n    conf[\"datasets\"] = datasets_list\n    conf[\"detect_language\"] = args.detect_language\n    # Override config from command-line\n    parser = argparse.ArgumentParser()\n    for key, value in conf.items():\n        type_ = type(value) if value is not None else str\n        if type_ == bool:\n            type_ = _strtobool\n        parser.add_argument(f\"--{key}\", type=type_, default=value)\n        # Allow --no-{key}  to remove it completely\n        parser.add_argument(f\"--no-{key}\", dest=key, action=\"store_const\", const=None)\n\n    args = parser.parse_args(remaining)\n    print(args)\n    return args\n\n\nif __name__ == \"__main__\":\n    args = argument_parsing()\n\n    train, evals = get_dataset(args, mode=args.mode.value)\n    overview_df = pd.DataFrame(columns=[\"dataset_name\", \"train_counts\", \"eval_counts\", \"total_counts\"])\n    language_df = pd.DataFrame()\n    if args.detect_language:\n        DetectorFactory.seed = 0\n    for idx, (dataset_name, dataset) in enumerate(evals.items()):\n        train_lang = Counter()\n        if args.detect_language:\n            length = len(dataset)\n            for idx1, row in enumerate(dataset):\n                if idx1 % 1000 == 0:\n                    print(f\"{idx1} of {length} of ds {dataset_name}.\")\n                try:\n                    if isinstance(row, (list, tuple)):\n                        train_lang += Counter([detect(k) for k in row])\n                    elif isinstance(row, DatasetEntrySft):\n                        train_lang += Counter([detect(k) for k in row.questions if k])\n                        if isinstance(row.answers[0], list):\n                            for answers in row.answers:\n                                train_lang += Counter([detect(k) for k in answers if k])\n                        else:\n                            train_lang += Counter([detect(k) for k in row.answers if k])\n                    else:\n                        raise ValueError(\n                            f\"Did not expect the type {type(row)}. Should be either list, tuple or DatasetEntry.\"\n                        )\n                except Exception as e:\n                    print(e)\n        train_lang = dict(train_lang)\n        train_lang[\"dataset_name\"] = dataset_name\n        language_df = pd.concat([language_df, pd.DataFrame([train_lang])])\n        eval_count = len(evals.get(dataset_name, []))\n        overview_df.loc[idx] = [\n            dataset_name,\n            len(train.datasets[idx]),\n            eval_count,\n            len(train.datasets[idx]) + eval_count,\n        ]\n    print(overview_df)\n    print(language_df)\n    overview_df.to_csv(args.output_path, index=False)\n    language_df.to_csv(\"language_counts.csv\", index=False)\n\n# python check_dataset_counts.py --datasets joke webgpt gpt4all alpaca code_alpaca vicuna minimath humaneval_mbpp_codegen_qa humaneval_mbpp_testgen_qa grade_school_math_instructions recipes cmu_wiki_qa oa_wiki_qa_bart_10000row prosocial_dialogue explain_prosocial soda oa_leet10k --mode sft\n# python check_dataset_counts.py --datasets joke webgpt alpaca code_alpaca vicuna minimath humaneval_mbpp_codegen_qa humaneval_mbpp_testgen_qa grade_school_math_instructions recipes cmu_wiki_qa oa_wiki_qa_bart_10000row prosocial_dialogue oa_leet10k --mode sft\n# python check_dataset_counts.py --datasets joke webgpt --mode sft\n", "model/model_training/check_dataset_appearances.py": "\"\"\"\nThis script should help to detect any keywords or other unwanted appearances in the datasets\nRUN WITH:\npython check_dataset_appearances.py -d <datasets> --cache_dir <path-to-cache-dir> --mode <one of sft, rm, rl>\n\ne.g.:\npython check_dataset_appearances.py -d gpt4all webgpt --cache_dir .cache --mode sft\n\npython check_dataset_appearances.py -d alpaca_gpt4 vicuna gpteacher_roleplay red_pajama wizardlm_70k --cache_dir .cache --mode sft\npython check_dataset_appearances.py -d wizardlm_70k --cache_dir .cache --mode sft\n\npython check_dataset_appearances.py -d alpaca_gpt4 vicuna gpteacher_roleplay wizardlm_70k joke poem_instructions oa_stackexchange tell_a_joke --cache_dir .cache --mode sft\npython check_dataset_appearances.py joke  --cache_dir .cache --mode sft\n\npython check_dataset_appearances.py -d webgpt gpt4all code_alpaca minimath humaneval_mbpp_codegen_qa humaneval_mbpp_testgen_qa grade_school_math_instructions recipes cmu_wiki_qa oa_wiki_qa_bart_10000row prosocial_dialogue explain_prosocial soda oa_leet10k dolly15k --cache_dir .cache --mode sft\npython check_dataset_appearances.py -d soda oa_leet10k dolly15k --cache_dir .cache --mode sft\n\"\"\"\nimport argparse\nimport pprint\nfrom collections import defaultdict\n\nfrom model_training.check_dataset_counts import Mode\nfrom model_training.custom_datasets import get_one_dataset\nfrom model_training.custom_datasets.formatting import DatasetEntryLm, DatasetEntrySft\nfrom model_training.custom_datasets.utils import FILTER_BY_WORDS\n\nRE_TO_CHECK = []  # [re_whitespace_newline_match, re_reference_remove, re_single_reference_remove]\nSTRINGS_TO_CHECK = list(set(FILTER_BY_WORDS + []))\n\n\ndef argument_parsing():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"-d\",\n        \"--datasets\",\n        nargs=\"+\",\n        required=True,\n        help=\"\"\"\n        Multiple datasets can be passed to set different options.\n        For example, run as:\n\n           ./check_dataset_counts.py --datasets math oasst_export_eu\n\n        to check the counts of the math and the oasst_export_eu dataset.\n    \"\"\",\n    )\n    parser.add_argument(\"--mode\", dest=\"mode\", type=Mode, choices=list(Mode))\n    parser.add_argument(\"--cache_dir\", dest=\"cache_dir\", type=str)\n    parser.add_argument(\"--verbose\", dest=\"verbose\", type=str, default=False)\n\n    args, _ = parser.parse_known_args()\n\n    return args\n\n\ndef check_in_dataset_row(row: str | list[str] | tuple[str], matched=dict[str, list]):\n    def _check_single_string(row: str, matched: dict[str, list]) -> dict[str, list]:\n        for exp in RE_TO_CHECK:\n            if exp.match(row) is not None:\n                matched[exp].append(row)\n        for string in STRINGS_TO_CHECK:\n            if string.lower() in row.lower():\n                string_idx = row.lower().index(string.lower())\n                matched[string].append(row[max(string_idx - 50, 0) : string_idx + 50])\n        return matched\n\n    if isinstance(row, str):\n        matched = _check_single_string(row, matched)\n    elif isinstance(row, (list, tuple)):\n        for r in row:\n            if not isinstance(r, str):\n                raise ValueError(f\"Unexpected type: {type(row)}\")\n            matched = _check_single_string(r, matched)\n    elif isinstance(row, DatasetEntrySft):\n        formatted = row.get_formatted(eos_token=\"</s>\")\n        for r in formatted:\n            if not isinstance(r, str):\n                raise ValueError(f\"Unexpected type: {type(r)}\")\n            matched = _check_single_string(\n                r.replace(\"<|assistant|>\", \"\").replace(\"<|prompter|>\", \"\").replace(\"</s>\", \"\"), matched\n            )\n    elif isinstance(row, DatasetEntryLm):\n        matched = _check_single_string(row.text, matched)\n    else:\n        raise ValueError(f\"Received unexpected type: {type(row)}.\")\n    return matched\n\n\ndef iterate_over_dataset(ds):\n    matched = defaultdict(list)\n    for row in ds:\n        check_in_dataset_row(row, matched)\n    return matched\n\n\nif __name__ == \"__main__\":\n    args = argument_parsing()\n    pp = pprint.PrettyPrinter(indent=4)\n\n    overview_dct = {}\n    train_datasets, val_datasets = {}, {}\n    for dataset_name in args.datasets:\n        train, val = get_one_dataset(None, dataset_name, mode=args.mode.value, data_path=args.cache_dir)\n        train_datasets[dataset_name] = train\n        if val is not None:\n            val_datasets[dataset_name] = val\n        matched_train = iterate_over_dataset(train)\n        matched_val = iterate_over_dataset(val)\n        train_dct = {k: len(v) for k, v in matched_train.items()}\n        val_dct = {k: len(v) for k, v in matched_val.items()}\n        unified_keys = list(set(train_dct.keys()).union(set(val_dct.keys())))\n        unified_counts = {k: train_dct.get(k, 0) + val_dct.get(k, 0) for k in unified_keys}\n        if len(unified_counts):\n            overview_dct[dataset_name] = unified_counts\n            print(f\"\\nFOUND THE FOLLOWING APPEARANCES FOR DATASET {dataset_name}:\")\n            pp.pprint(unified_counts)\n        if args.verbose:\n            if len(matched_train) != 0:\n                pp.pprint(f\"Found the following occurrences in TRAIN {dataset_name}:\")\n                pp.pprint(dict(matched_train))\n            if len(matched_val) != 0:\n                pp.pprint(f\"Found the following occurrences in VAL {dataset_name}:\")\n                pp.pprint(dict(matched_val))\n        if len(matched_train) + len(matched_val) == 0:\n            print(\n                f\"\\nNON OF THE SPECIFIED REGULAR EXPRESSIONS OR FILTER WORDS WAS FOUND FOR THE DATASET {dataset_name}\"\n            )\n    if len(overview_dct) > 0:\n        pp.pprint(overview_dct)\n", "model/model_training/efficiency_utils.py": "import functools\n\nimport torch\nfrom transformers.activations import FastGELUActivation, GELUActivation, NewGELUActivation, QuickGELUActivation\n\n\ndef rsetattr(obj, attr, val):\n    pre, _, post = attr.rpartition(\".\")\n    return setattr(rgetattr(obj, pre) if pre else obj, post, val)\n\n\ndef rgetattr(obj, attr, *args):\n    def _getattr(obj, attr):\n        return getattr(obj, attr, *args)\n\n    return functools.reduce(_getattr, [obj] + attr.split(\".\"))\n\n\ndef fuse_gelu(model):\n    @torch.jit.script\n    def gelu_fwd(x):\n        return x * 0.5 * (1.0 + torch.tanh(0.79788456 * x * (1 + 0.044715 * x * x)))\n\n    @torch.jit.script\n    def gelu_bwd(g, x):\n        tanh_out = torch.tanh(0.79788456 * x * (1 + 0.044715 * x * x))\n        ff = 0.5 * x * ((1 - tanh_out * tanh_out) * (0.79788456 + 0.1070322243 * x * x)) + 0.5 * (1 + tanh_out)\n        return ff * g\n\n    class _FusedGeLUFunction(torch.autograd.Function):\n        @staticmethod\n        # bias is an optional argument\n        def forward(ctx, input):\n            ctx.input_tensor = input\n            return gelu_fwd(input)\n\n        @staticmethod\n        def backward(ctx, grad_output):\n            input = ctx.input_tensor\n            tmp = gelu_bwd(grad_output, input)\n            return tmp\n\n    class FusedGelu(torch.nn.Module):\n        def forward(self, input):\n            return _FusedGeLUFunction.apply(input)\n\n    fused_gelu_module = FusedGelu()\n    hf_gelu_functions = [GELUActivation, FastGELUActivation, NewGELUActivation, QuickGELUActivation]\n\n    for name, module in model.named_modules():\n        for hf_gelu_function in hf_gelu_functions:\n            if isinstance(module, hf_gelu_function):\n                rsetattr(model, name, fused_gelu_module)\n\n    return model\n", "model/model_training/metrics.py": "import numpy as np\nfrom scipy import stats as st\n\nRM_METRICS = [\"accuracy\", \"kendalltau\", \"spearmanr\"]\n\n\ndef reward_accuracy(eval_pred):\n    logits = eval_pred.predictions\n    labels = eval_pred.label_ids\n    pos_scores, neg_scores = [], []\n    for b_logits, b_labels in zip(logits, labels):\n        b_labels = b_labels[b_labels != -100]\n        b_logits = b_logits[b_logits != -100]\n        for i in np.unique(b_labels):\n            logits_batch = b_logits[b_labels == i]\n            pos_scores.append(logits_batch[0])\n            neg_scores.append(logits_batch[-1])\n    pos_scores = np.array(pos_scores).reshape(-1, 1)\n    neg_scores = np.array(neg_scores).reshape(-1, 1)\n\n    metrics = {\n        \"pos_score\": np.mean(pos_scores),\n        \"neg_score\": np.mean(neg_scores),\n        \"score_diff\": np.mean(pos_scores - neg_scores),\n        \"accuracy\": np.mean(pos_scores > neg_scores),\n    }\n    return metrics\n\n\ndef kendall_tau(eval_pred):\n    logits = eval_pred.predictions\n    labels = eval_pred.label_ids\n    tau = 0.0\n    bsize = 0\n    for b_logits, b_labels in zip(logits, labels):\n        b_labels = b_labels[b_labels != -100]\n        b_logits = b_logits[b_logits != -100]\n        # uncomment to support non pythia model,\n        # remember to add to other metrics as well\n\n        # truncated_logits = min(len(b_labels), len(b_logits))\n        # b_labels = b_labels[:truncated_logits]\n        # b_logits = b_logits[:truncated_logits]\n        for i in np.unique(b_labels):\n            logits_batch = b_logits[b_labels == i]\n            pred_rank = np.argsort(logits_batch)\n            true_rank = np.arange(logits_batch.size - 1, -1, -1)\n            tau += st.kendalltau(pred_rank, true_rank)[0]\n        bsize += np.unique(b_labels).size\n\n    return {\"kendalltau\": tau / bsize}\n\n\ndef spearmanr(eval_pred):\n    logits = eval_pred.predictions\n    labels = eval_pred.label_ids\n    score = 0.0\n    bsize = 0\n    for b_logits, b_labels in zip(logits, labels):\n        b_labels = b_labels[b_labels != -100]\n        b_logits = b_logits[b_logits != -100]\n        for i in np.unique(b_labels):\n            logits_batch = b_logits[b_labels == i]\n            pred_rank = np.argsort(logits_batch)\n            true_rank = np.arange(logits_batch.size - 1, -1, -1)\n            score += st.spearmanr(pred_rank, true_rank).statistic\n        bsize += np.unique(b_labels).size\n\n    return {\"spearmanr\": score / bsize}\n\n\nclass RewardMetrics:\n    \"\"\"\n    class to combine multiple metrics\n    \"\"\"\n\n    def __init__(self, metrics):\n        if isinstance(metrics, str):\n            metrics = [metrics]\n\n        self.metrics = []\n        for name in metrics:\n            if name == \"accuracy\":\n                self.metrics.append(reward_accuracy)\n            elif name == \"kendalltau\":\n                self.metrics.append(kendall_tau)\n            elif name == \"spearmanr\":\n                self.metrics.append(spearmanr)\n            else:\n                raise ValueError(f\"Invalid metrics {name}. Available {RM_METRICS}\")\n\n    def __call__(self, eval_pred):\n        results = {}\n        for metric in self.metrics:\n            results.update(metric(eval_pred))\n\n        return results\n", "model/model_training/trainer_rl.py": "import argparse\nimport math\nimport os\nimport random\nfrom argparse import Namespace\nfrom typing import Sequence\n\nimport numpy as np\nimport torch\nimport transformers\nimport tritonclient.grpc as client_util\nimport trlx\nfrom model_training.custom_datasets.formatting import QA_SPECIAL_TOKENS, format_pairs\nfrom model_training.models import get_specific_model\nfrom model_training.utils.utils import _strtobool, get_dataset, init_rng, read_yamls\nfrom tritonclient.utils import np_to_triton_dtype\nfrom trlx.data.configs import TRLConfig\n\n# flake8: noqa\nfrom utils.ppo_utils import CustomPPOTrainer\nfrom utils.utils import _strtobool, get_dataset, get_model, init_rng, read_yamls\nfrom utils.utils_rl import prepare_tensor\n\n\ndef argument_parsing(notebook: bool = False, notebook_args: Sequence[str] | None = None, **kwargs):\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--configs\", nargs=\"+\", required=True)\n    parser.add_argument(\"--local_rank\", type=int, default=-1)\n    parser.add_argument(\"--wandb-entity\", type=str, default=\"open-assistant\")\n    parser.add_argument(\"--rng_seed\", type=int, help=\"rng seed\")\n\n    if notebook:\n        args, remaining = parser.parse_known_args(notebook_args)\n    else:\n        args, remaining = parser.parse_known_args()\n\n    # Config from YAML\n    conf = {}\n    configs = read_yamls(\"./configs\")\n    for name in args.configs:\n        if \",\" in name:\n            for n in name.split(\",\"):\n                conf.update(configs[n])\n        else:\n            conf.update(configs[name])\n\n    conf[\"local_rank\"] = args.local_rank\n    if args.rng_seed is not None:\n        conf[\"rng_seed\"] = args.rng_seed\n\n    # Override config from command-line\n    parser = argparse.ArgumentParser()\n\n    for key, value in kwargs.items():\n        type_ = type(value) if value is not None else str\n        parser.add_argument(f\"--{key}\", type=type_, default=value)\n\n    for key, value in conf.items():\n        type_ = type(value) if value is not None else str\n        if type_ == bool:\n            type_ = _strtobool\n        parser.add_argument(f\"--{key}\", type=type_, default=value)\n\n    return parser.parse_args(remaining)\n\n\n# Taken from https://github.com/CarperAI/trlx/blob/b7db6f9e74c7d8dc719255b27968d2994836957a/examples/hh/ppo_hh.py#L114\ndef create_reward_fn(rank_config, sft_config):  # noqa:  C901\n    triton_host = os.environ.get(\"TRITON_HOST_RM\")\n    assert triton_host is not None, \"Specify reward model in the TRITON_HOST_RM environmental variable\"\n\n    triton_url, triton_model = triton_host.split(\"/\")\n    client = client_util.InferenceServerClient(url=triton_url, verbose=False)\n\n    rank_tokenizer = transformers.AutoTokenizer.from_pretrained(rank_config.model_name, cache_dir=rank_config.cache_dir)\n    sft_tokenizer = transformers.AutoTokenizer.from_pretrained(sft_config.model_name, cache_dir=sft_config.cache_dir)\n\n    def reward_fn(samples, prompts, outputs):\n        if len(samples) == 0:\n            return []\n\n        # hack to allo for different tokenizers with different eos tokens ... rest of the\n        samples = [x.replace(sft_tokenizer.eos_token, rank_tokenizer.eos_token) for x in samples]\n        samples = [x.replace(sft_tokenizer.pad_token, rank_tokenizer.pad_token) for x in samples]\n\n        inputs = rank_tokenizer(samples, return_tensors=\"np\", padding=True)\n\n        mbs = rank_config.batch_size\n        out = []\n        for i in range(math.ceil(len(samples) / mbs)):\n            batch_ixs = slice(i * mbs, (i + 1) * mbs)\n\n            # We specified int32 as types for a triton client\n            result = client.infer(\n                triton_model,\n                [\n                    prepare_tensor(\"input_ids\", inputs.input_ids[batch_ixs].astype(np.int32)),\n                    prepare_tensor(\"attention_mask\", inputs.attention_mask[batch_ixs].astype(np.int32)),\n                ],\n            )\n\n            rewards = result.as_numpy(\"rewards\")\n\n            out.extend(rewards)\n\n        return out\n\n    return reward_fn\n\n\ndef main():\n    training_conf = argument_parsing()\n    rank_config = Namespace(**training_conf.rank_config)\n    sft_config = Namespace(**training_conf.sft_config)\n\n    triton_host_rm = os.getenv(\"TRITON_HOST_RM\", training_conf.triton_host_rm)\n    triton_host_sft = os.getenv(\"TRITON_HOST_REF\", training_conf.triton_host_sft)\n    os.environ[\"TRITON_HOST_RM\"] = triton_host_rm\n    os.environ[\"TRITON_HOST_REF\"] = triton_host_sft\n\n    init_rng(training_conf)\n\n    eos_token = transformers.AutoTokenizer.from_pretrained(\n        sft_config.model_name, cache_dir=sft_config.cache_dir\n    ).eos_token\n\n    # Load pretrained SFT model\n\n    # override model_name to be the same as sft_model\n    trlx_config = TRLConfig.load_yaml(\"configs/ppo_config.yaml\")\n    trlx_config.sft_config = sft_config\n\n    train, eval_dict = get_dataset(training_conf, mode=\"rl\")\n\n    # take the dataset as the eval prompt generation dataset\n    eval = eval_dict[\"oasst_export\"] if \"oasst_export\" in eval_dict else eval_dict[next(iter(eval_dict))]\n\n    # trlx requires training data to be a list of prompts\n    # first element of each sample is the context and the prompt\n    prompts, eval_prompts = tuple(\n        map(\n            lambda x: [\"\".join(format_pairs(x[i][0], eos_token, add_initial_reply_token=True)) for i in range(len(x))],\n            (train, eval),\n        )\n    )\n\n    ## Override first eval prompts just for visualization\n    eval_prompts = [\n        \"\".join(format_pairs([\"Can you tell me about GLaDOS?\"], eos_token, add_initial_reply_token=True)),\n        \"\".join(format_pairs([\"What is the chemical symbol for gold?\"], eos_token, add_initial_reply_token=True)),\n        \"\".join(\n            format_pairs(\n                [\"If you were the President of the United States, what would you do?\"],\n                eos_token,\n                add_initial_reply_token=True,\n            )\n        ),\n    ] + eval_prompts\n\n    if training_conf.num_eval_prompts is not None and training_conf.num_eval_prompts > 0:\n        eval_prompts = eval_prompts[: training_conf.num_eval_prompts]\n\n    random.shuffle(prompts)\n    # Sanity Check for prompts to make sure it's loading properly\n    with open(r\"output.txt\", \"w\") as fp:\n        for item in eval_prompts:\n            # write each item on a new line\n            fp.write(\"Prompt For RL: %s\\n\" % item)\n\n    trlx_config.tokenizer.tokenizer_path = sft_config.model_name\n    trlx_config.model.model_path = sft_config.model_name\n    trlx_config.train.batch_size = int(training_conf.batch_size)\n    trlx_config.method.chunk_size = int(training_conf.chunk_size)\n    trlx_config.method.num_rollouts = int(training_conf.num_rollouts)\n    trlx_config.train.total_steps = int(training_conf.total_steps)\n\n    if training_conf.debug:\n        print(\"Continuing in debug mode\")\n        prompts = prompts[:10]\n        eval_prompts = eval_prompts[:10]\n        trlx_config.method.num_rollouts = 1\n        # trlx_config.method.gen_kwargs['max_new_tokens'] = 12\n        # trlx_config.train.seq_length = 48\n\n    trainer = trlx.train(\n        sft_config.model_name,\n        reward_fn=create_reward_fn(rank_config, sft_config),\n        prompts=prompts,\n        eval_prompts=eval_prompts,\n        config=trlx_config,\n        stop_sequences=[eos_token],\n    )\n\n    training_conf.output_dir = training_conf.output_dir if training_conf.output_dir else training_conf.model_name\n\n    trainer.save_pretrained(training_conf.output_dir)\n\n\nif __name__ == \"__main__\":\n    main()\n", "model/model_training/__init__.py": "", "model/model_training/trainer_sft.py": "#!/usr/bin/env python3\nimport argparse\nimport logging\nimport os\nfrom functools import partial\nfrom typing import Any, Callable, Dict, List, Optional, Sequence, Tuple, Union\n\nimport datasets\nimport torch\n\n# from model_training.custom_datasets.formatting import DatasetEntry\nfrom model_training.custom_datasets.dialogue_collator import DialogueDataCollator\nfrom model_training.efficiency_utils import fuse_gelu\nfrom model_training.models.patching import RopePatch\nfrom model_training.models.peft_modeling import peft_model\nfrom model_training.utils.utils import (\n    PerDatasetSampler,\n    _strtobool,\n    get_dataset,\n    get_loss,\n    get_metrics,\n    get_model,\n    get_tokenizer,\n    init_rng,\n    read_yamls,\n)\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Subset\nfrom tqdm import tqdm\nfrom transformers import PreTrainedModel, Trainer, TrainingArguments\nfrom transformers.trainer_pt_utils import IterableDatasetShard\nfrom transformers.trainer_utils import seed_worker\nfrom transformers.training_args import OptimizerNames\nfrom transformers.utils import is_datasets_available\n\n\ndef compute_metrics(eval_pred, preprocess_fns, metrics):\n    out = {}\n    for metric, preprocess_fn in zip(metrics, preprocess_fns):\n        preds, labels = preprocess_fn(eval_pred)\n        out = dict(**out, **metric.compute(predictions=preds, references=labels))\n\n    return out\n\n\ndef preprocess_logits_for_metrics(logits, labels):\n    pred_ids = torch.argmax(logits, dim=-1)\n    return pred_ids\n\n\nclass SFTTrainer(Trainer):\n    def __init__(\n        self,\n        model: Union[PreTrainedModel, nn.Module] = None,\n        args: TrainingArguments = None,\n        sampler: torch.utils.data.sampler.Sampler = None,\n        loss_function: str = \"CrossEntropyLoss\",\n        poly_eps: float = 1.0,\n        train_collate_fn: Callable = None,\n        **kwargs,\n    ):\n        super().__init__(model, args, **kwargs)\n        self.train_collate_fn = train_collate_fn\n        # By default CrossEntropyLoss ignores padding_index -100, but just in case use our own loss_fct\n        self.loss_fct = get_loss(loss_function, poly_eps)\n        self.sampler = sampler\n\n    def compute_loss(self, model, inputs, return_outputs=False):\n        labels_mask = inputs.pop(\"label_masks\")\n        targets = inputs.pop(\"targets\")\n\n        outputs = model(\n            input_ids=inputs[\"input_ids\"],\n            attention_mask=inputs.get(\"attention_mask\", None),\n            use_cache=False,\n        )\n\n        loss = self.loss_fct(outputs.get(\"logits\"), targets, mask=labels_mask)\n\n        return (loss, outputs) if return_outputs else loss\n\n    def _compute_loss(self, model, inputs):\n        inputs = self._prepare_inputs(inputs)\n\n        labels_mask = inputs.pop(\"label_masks\")\n        targets = inputs.pop(\"targets\")\n\n        outputs = model(\n            input_ids=inputs[\"input_ids\"],\n            attention_mask=inputs.get(\"attention_mask\", None),\n            use_cache=False,\n        )\n\n        logits = outputs.get(\"logits\")\n\n        loss = self.loss_fct(outputs.get(\"logits\"), targets, mask=labels_mask)\n\n        return loss, logits, targets, labels_mask\n\n    def prediction_step(\n        self,\n        model: nn.Module,\n        inputs: Dict[str, Union[torch.Tensor, Any]],\n        prediction_loss_only: bool,\n        ignore_keys: Optional[List[str]] = None,\n    ) -> Tuple[Optional[torch.Tensor], Optional[torch.Tensor], Optional[torch.Tensor]]:\n        with torch.no_grad():\n            loss, logits, labels, labels_mask = self._compute_loss(model, inputs)\n            labels[~labels_mask.bool()] = -100  # padding_index\n\n        loss = loss.mean().detach()\n\n        if self.args.prediction_loss_only:\n            return (loss, None, None)\n\n        return (loss, logits, labels)\n\n    def get_train_dataloader(self):\n        \"\"\"\n        Inject custom data sampling behaviour into training loop\n        and use custom task mixing collate function : train_collate_fn\n\n        rewrite from:\n        https://github.com/huggingface/transformers/blob/67d074874d285e616393c65a0e670088e1b6b74a/src/transformers/trainer.py#L846\n        \"\"\"\n        data_collator = self.train_collate_fn\n        train_dataset = self.train_dataset\n        if is_datasets_available() and isinstance(train_dataset, datasets.Dataset):\n            train_dataset = self._remove_unused_columns(train_dataset, description=\"training\")\n\n        if isinstance(train_dataset, torch.utils.data.IterableDataset):\n            # if we are using iterable dataset it means no weight sampling\n            # added for backward compat\n            if self.args.world_size > 1:\n                train_dataset = IterableDatasetShard(\n                    train_dataset,\n                    batch_size=self._train_batch_size,\n                    drop_last=self.args.dataloader_drop_last,\n                    num_processes=self.args.world_size,\n                    process_index=self.args.process_index,\n                )\n            return DataLoader(\n                train_dataset,\n                batch_size=self.args.per_device_train_batch_size,\n                collate_fn=data_collator,\n                num_workers=self.args.dataloader_num_workers,\n                pin_memory=self.args.dataloader_pin_memory,\n            )\n\n        if self.sampler is None:\n            train_sampler = self._get_train_sampler()\n        else:\n            train_sampler = self.sampler\n            logging.warning(\"Custom sampler found!\")\n\n        dataloader = DataLoader(\n            train_dataset,\n            batch_size=self._train_batch_size,\n            sampler=train_sampler,\n            collate_fn=data_collator,\n            drop_last=self.args.dataloader_drop_last,\n            num_workers=self.args.dataloader_num_workers,\n            pin_memory=self.args.dataloader_pin_memory,\n            worker_init_fn=seed_worker,\n        )\n        return dataloader\n\n\ndef argument_parsing(notebook: bool = False, notebook_args: Sequence[str] | None = None):\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--configs\",\n        nargs=\"+\",\n        required=True,\n        help=\"\"\"\n        Multiple configs can be passed to set different options.\n        For example, run as:\n\n           ./trainer_sft.py --configs galactica-125m webgpt_dataset_only per_digit_tokens\n\n        to run the galactica-125m model, using the webgpt dataset only (as opposed to all\n        the datasets listed in defaults in config.yaml) and treat each digit as a separate token.\n    \"\"\",\n    )\n    parser.add_argument(\"--local_rank\", type=int, default=-1)\n    parser.add_argument(\"--deepspeed\", action=\"store_true\")\n    parser.add_argument(\"--no-deepspeed\", dest=\"deepspeed\", action=\"store_false\")\n    parser.add_argument(\"--wandb-entity\", type=str, default=\"open-assistant\")\n    parser.add_argument(\"--resume_from_checkpoint\", action=\"store_true\", help=\"Resume from last saved checkpoint\")\n    parser.add_argument(\"--rng_seed\", type=int, help=\"rng seed\")\n    parser.add_argument(\"--show_dataset_stats\", action=\"store_true\", help=\"Show dataset stats\", default=False)\n    parser.set_defaults(deepspeed=False)\n\n    if notebook:\n        args, remaining = parser.parse_known_args(notebook_args)\n    else:\n        args, remaining = parser.parse_known_args()\n\n    # Config from YAML\n    conf = {}\n    configs = read_yamls(\"./configs\")\n    conf.update(configs[\"defaults\"])\n    try:\n        for name in args.configs:\n            if \",\" in name:\n                for n in name.split(\",\"):\n                    conf.update(configs[n])\n            else:\n                conf.update(configs[name])\n    except KeyError as e:\n        print(f'Error: Could not find the config \"{e.args[0]}\" in config.yaml')\n        exit(1)\n\n    conf[\"wandb_entity\"] = args.wandb_entity\n    conf[\"local_rank\"] = args.local_rank\n    conf[\"deepspeed\"] = args.deepspeed\n    conf[\"resume_from_checkpoint\"] = args.resume_from_checkpoint\n    if args.rng_seed is not None:\n        conf[\"rng_seed\"] = args.rng_seed\n    conf[\"show_dataset_stats\"] = args.show_dataset_stats\n\n    # get the world size in deepspeed\n    if conf[\"deepspeed\"]:\n        conf[\"world_size\"] = int(os.getenv(\"WORLD_SIZE\", default=\"1\"))\n    else:\n        conf[\"world_size\"] = 1\n\n    # Override config from command-line\n    parser = argparse.ArgumentParser()\n    for key, value in conf.items():\n        type_ = type(value) if value is not None else str\n        if type_ == bool:\n            type_ = _strtobool\n        parser.add_argument(f\"--{key}\", type=type_, default=value)\n        # Allow --no-{key}  to remove it completely\n        parser.add_argument(f\"--no-{key}\", dest=key, action=\"store_const\", const=None)\n\n    return parser.parse_args(remaining)\n\n\ndef tokenizer_sanity_check(tokenizer):\n    print(\"Tokenizer sanity check:\")\n    print(f\"Type: {type(tokenizer).__name__}\")\n\n    print(\"special_tokens_map:\", tokenizer.special_tokens_map)\n\n    print(f\"bos_token='{tokenizer.bos_token}', bos_token_id={tokenizer.bos_token_id}\")\n    print(f\"eos_token='{tokenizer.eos_token}', eos_token_id={tokenizer.eos_token_id}\")\n\n    from model_training.custom_datasets.formatting import QA_SPECIAL_TOKENS, create_dataset_entry_qa\n\n    ds_entry = create_dataset_entry_qa(\n        mode=\"sft\", questions=[\"Q1\", \"Q2\"], answers=[\"A1\", \"A2\"], lang=\"en\", context=\"ctx\"\n    )\n    in_text = ds_entry.get_formatted(\n        tokenizer.eos_token,\n        use_system_tag=True,\n        system_property_dropout=0,\n        system_add_length=True,\n    )\n    in_text = \"\".join(in_text)\n\n    prompter_token_id = tokenizer.convert_tokens_to_ids(QA_SPECIAL_TOKENS[\"Question\"])\n    assistant_token_id = tokenizer.convert_tokens_to_ids(QA_SPECIAL_TOKENS[\"Answer\"])\n    print(f\"{prompter_token_id=}, {assistant_token_id=}\")\n\n    tr = tokenizer(in_text, max_length=1024, pad_to_max_length=False, truncation=True)\n\n    message_indices = []\n    i = -1\n    for id in tr.input_ids:\n        if id in (prompter_token_id, assistant_token_id):\n            i += 1\n        message_indices.append(i)\n\n    print(\"encoding result:\", tr)\n    for i, xs in enumerate(tr.input_ids):\n        decoded = tokenizer.decode(xs)\n        print(f'{i}: {xs} -> \"{decoded}\"')\n\n    print(\"message_indices:\", message_indices)\n\n\ndef main():\n    training_conf = argument_parsing()\n    if not training_conf.deepspeed or training_conf.local_rank == 0:\n        print(f\"trainig_conf = {training_conf}\")\n\n    output_dir = (\n        training_conf.output_dir\n        if training_conf.output_dir\n        else f\"{training_conf.model_name}-{training_conf.log_dir}-finetuned\"\n    )\n\n    optimizer = OptimizerNames.ADAMW_BNB if training_conf.quantization else OptimizerNames.ADAMW_HF\n\n    # needs to happen before model loading in case of stage 3 training\n    args = TrainingArguments(\n        output_dir=output_dir,\n        num_train_epochs=training_conf.num_train_epochs,\n        warmup_steps=training_conf.warmup_steps,\n        learning_rate=float(training_conf.learning_rate),\n        deepspeed=training_conf.deepspeed_config if training_conf.deepspeed else None,\n        optim=optimizer,\n        fp16=training_conf.dtype in [\"fp16\", \"float16\"],\n        bf16=training_conf.dtype in [\"bf16\", \"bfloat16\"],\n        local_rank=training_conf.local_rank,\n        gradient_checkpointing=training_conf.gradient_checkpointing,\n        gradient_accumulation_steps=training_conf.gradient_accumulation_steps,\n        per_device_train_batch_size=training_conf.per_device_train_batch_size,\n        per_device_eval_batch_size=training_conf.per_device_eval_batch_size,\n        adam_beta1=training_conf.adam_beta1,\n        adam_beta2=training_conf.adam_beta2,\n        adam_epsilon=float(training_conf.adam_epsilon),\n        weight_decay=training_conf.weight_decay,\n        max_grad_norm=training_conf.max_grad_norm,\n        logging_steps=training_conf.logging_steps,\n        save_total_limit=training_conf.save_total_limit,\n        evaluation_strategy=\"steps\",\n        eval_steps=training_conf.eval_steps,\n        save_strategy=training_conf.save_strategy,\n        save_steps=training_conf.save_steps,\n        eval_accumulation_steps=training_conf.eval_accumulation_steps,\n        resume_from_checkpoint=training_conf.resume_from_checkpoint,\n        report_to=\"wandb\" if training_conf.log_wandb else None,\n    )\n\n    init_rng(training_conf)\n\n    tokenizer = get_tokenizer(training_conf)\n\n    if not training_conf.deepspeed or training_conf.local_rank == 0:\n        tokenizer_sanity_check(tokenizer)\n\n    train_collate_fn = DialogueDataCollator(\n        tokenizer,\n        max_length=training_conf.max_length,\n        random_offset_probability=training_conf.random_offset_probability,\n        label_masking=training_conf.label_masking,\n        samples_mixing=training_conf.samples_mixing,\n        pad_to_multiple_of=16,\n        use_system_prefix=training_conf.use_system_prefix,\n        system_prefix=training_conf.system_prefix,\n        use_system_tag=training_conf.use_system_tag,\n        system_property_dropout=training_conf.system_property_dropout,\n        system_add_length=training_conf.system_add_length,\n    )\n\n    if training_conf.val_max_length is None:\n        training_conf.val_max_length = training_conf.max_length\n\n    eval_collate_fn = DialogueDataCollator(\n        tokenizer,\n        max_length=training_conf.val_max_length,\n        random_offset_probability=training_conf.random_offset_probability,\n        label_masking=training_conf.label_masking,\n        samples_mixing=False,\n        use_system_prefix=training_conf.use_system_prefix,\n        system_prefix=training_conf.system_prefix,\n        use_system_tag=training_conf.use_system_tag,\n        system_property_dropout=training_conf.system_property_dropout,\n        system_add_length=training_conf.system_add_length,\n    )\n\n    train, evals = get_dataset(training_conf)\n    show_dataset_stats = (training_conf.verbose or training_conf.show_dataset_stats) and (\n        not training_conf.deepspeed or training_conf.local_rank == 0\n    )\n    if show_dataset_stats:\n        print(\"Training dataset sizes (before sampling):\")\n        total = len(train)\n        for d in train.datasets:\n            if isinstance(d, Subset):\n                name = f\"Subset of {type(d.dataset).__name__}\"\n                if hasattr(d.dataset, \"name\"):\n                    name += f\" ({d.dataset.name})\"\n            else:\n                name = type(d).__name__\n                if hasattr(d, \"name\"):\n                    name += f\" ({d.name})\"\n            print(f\"{name}: {len(d)} ({len(d) / total:.2%})\")\n\n            # ensure that all entries can be formatted\n            # for x in d:\n            #     if isinstance(x, DatasetEntry):\n            #         x.get_formatted(\"sft\", \"<eos>\")\n\n        print(f\"\\nTotal train: {total}\")\n        print(\"-\" * 80)\n        print(\"Evaluation set sizes:\")\n        total_eval = sum(len(x) for x in evals.values())\n        for k, d in evals.items():\n            print(f\"{k}: {len(d)} ({len(d) / total_eval:.2%})\")\n        print(f\"\\nTotal eval: {total_eval}\")\n        print(\"-\" * 80)\n\n    if training_conf.use_custom_sampler:\n        samples_length = None\n        if training_conf.sort_by_length:\n            samples_length = list(\n                map(\n                    lambda x: train_collate_fn.process_one(x, return_length=True),\n                    tqdm(train, desc=\"Calculating lengths per sample\"),\n                )\n            )\n\n        sampler = PerDatasetSampler.build_sampler_from_config(\n            training_conf,\n            train.datasets,\n            rank=training_conf.local_rank,\n            world_size=training_conf.world_size,\n            samples_length=samples_length,\n            verbose=show_dataset_stats,\n        )\n    else:\n        sampler = None\n\n    metrics, preprocess_fns = get_metrics(training_conf, tokenizer)\n    model = get_model(training_conf, tokenizer)\n\n    superhot = RopePatch.from_config(training_conf) if training_conf.superhot else None\n    if superhot:\n        superhot.patch(model)\n\n    print(f\"rope_scaling: {model.config.rope_scaling}\")\n    print(f\"max_position_embeddings: {model.config.max_position_embeddings}\")\n\n    if training_conf.peft_model:\n        print(\"Using PEFT model\")\n        model = peft_model(model, training_conf)\n\n    if training_conf.quantization:\n        import bitsandbytes  # This is noisy, so delay importing until after argument parsing so it doesn't make --help noisy\n\n        for module in model.modules():\n            if isinstance(module, torch.nn.Embedding):\n                bitsandbytes.optim.GlobalOptimManager.get_instance().register_module_override(\n                    module, \"weight\", {\"optim_bits\": 32}\n                )\n\n    if training_conf.fuse_gelu:\n        model = fuse_gelu(model)\n\n    if not training_conf.log_wandb:\n        os.environ[\"WANDB_MODE\"] = \"offline\"\n\n    if training_conf.log_wandb and (not training_conf.deepspeed or training_conf.local_rank == 0):\n        import wandb\n\n        wandb_name = training_conf.model_name.replace(os.getenv(\"HOME\", \"/home/ubuntu\"), \"\")\n        wandb.init(\n            project=\"supervised-finetuning\",\n            entity=training_conf.wandb_entity,\n            resume=training_conf.resume_from_checkpoint,\n            name=f\"{wandb_name}-{training_conf.log_dir}-finetuned\",\n            config=training_conf,\n        )\n        wandb.config[\"_max_length\"] = training_conf.max_length\n        wandb.config[\"_val_max_length\"] = training_conf.val_max_length\n\n    trainer = SFTTrainer(\n        model=model,\n        args=args,\n        sampler=sampler,\n        train_collate_fn=train_collate_fn,\n        loss_function=training_conf.loss_fn,\n        poly_eps=training_conf.poly_eps,\n        train_dataset=train,\n        eval_dataset=evals,\n        data_collator=eval_collate_fn,\n        tokenizer=tokenizer,\n        compute_metrics=partial(compute_metrics, metrics=metrics, preprocess_fns=preprocess_fns),\n        preprocess_logits_for_metrics=preprocess_logits_for_metrics,\n    )\n    trainer.train(resume_from_checkpoint=training_conf.resume_from_checkpoint)\n    trainer.save_model()\n    tokenizer.save_pretrained(output_dir)\n\n\nif __name__ == \"__main__\":\n    main()\n", "model/model_training/to_triton.py": "import os\nfrom argparse import Namespace\nfrom string import Template\n\nimport torch\nimport transformers\nfrom torch import nn\nfrom trainer_rl import argument_parsing\nfrom utils.utils import get_model\n\n\nclass SFTLogitsModel(nn.Module):\n    def __init__(self, model):\n        super().__init__()\n        self.model = model\n\n    def forward(self, *args, **kwargs):\n        return self.model(*args, **kwargs).logits\n\n\nclass RMLogitsModel(nn.Module):\n    def __init__(self, model):\n        super().__init__()\n        self.model = model\n\n    def forward(self, *args, **kwargs):\n        return self.model(*args, **kwargs).logits[:, 0]\n\n\ndef load_model_and_tokenizer(triton_mode, config, device=\"cuda:0\"):\n    tokenizer = transformers.AutoTokenizer.from_pretrained(config.model_name)\n\n    # For llama ...\n    if tokenizer.pad_token_id == tokenizer.eos_token_id:\n        tokenizer.add_special_tokens({\"pad_token\": \"<|padding|>\"})\n\n    print(\"len tokenizer\", len(tokenizer))\n\n    # disable flash attention for triton\n    config.use_flash_attention = False\n\n    model = get_model(config, tokenizer, pad_vocab_size_to_multiple_of=1, check_freeze_layer=False)\n    model.to(device)\n    model.eval()\n\n    if triton_mode == \"sft\":\n        model = SFTLogitsModel(model)\n    elif triton_mode == \"rm\":\n        model = RMLogitsModel(model)\n    else:\n        raise ValueError(f\"Unknown mode {triton_mode}\")\n\n    return model, tokenizer\n\n\ndef write_traced_module(\n    traced_script_module,\n    model_name,\n    dtype=\"fp16\",\n    output_dir=\"model_store_sft\",\n    config_template=\"configs/triton_config_sft.pbtxt\",\n):\n    model_dir = os.path.join(output_dir, model_name, \"1\")\n    os.makedirs(model_dir, exist_ok=True)\n    traced_script_module.save(os.path.join(model_dir, \"traced-model.pt\"))\n\n    with open(config_template) as f:\n        template = Template(f.read())\n\n    if dtype == \"float16\":\n        dtype = \"fp16\"\n    elif dtype == \"float32\":\n        dtype = \"fp32\"\n    elif dtype == \"bfloat16\":\n        dtype = \"bf16\"\n\n    config = template.substitute(\n        {\n            \"model_name\": model_name,\n            \"dtype\": dtype,\n            \"output_type\": f\"TYPE_{dtype.upper()}\",\n        }\n    )\n\n    with open(os.path.join(output_dir, model_name, \"config.pbtxt\"), \"w\") as f:\n        f.write(config)\n\n\ndef trace_model(model, tokenizer, device=\"cuda:0\", trace_example=\"reward model's hash\", max_length=512):\n    inputs = tokenizer(trace_example, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\")\n    inputs = {k: v.to(device) for k, v in inputs.items() if k != \"token_type_ids\"}\n    outputs = model(**inputs)\n    print(f\"Output shape: {outputs.shape}\")\n\n    traced_script_module = torch.jit.trace(model, (inputs[\"input_ids\"], inputs[\"attention_mask\"]))\n    return traced_script_module\n\n\ndef main():\n    conf = argument_parsing(triton_mode=None, triton_output_dir=None)\n\n    if conf.triton_mode == \"sft\":\n        config = Namespace(**conf.sft_config)\n    elif conf.triton_mode == \"rm\":\n        config = Namespace(**conf.rank_config)\n\n    device = torch.device(\"cuda:0\")\n\n    model, tokenizer = load_model_and_tokenizer(conf.triton_mode, config, device=device)\n\n    traced_script_module = trace_model(model, tokenizer, device=device)\n\n    model_name = config.model_name.replace(\"/\", \"-\")\n    write_traced_module(\n        traced_script_module,\n        model_name,\n        config.dtype,\n        config_template=f\"configs/triton_config_{conf.triton_mode}.pbtxt\",\n        output_dir=f\".triton_models/model_store_{conf.triton_mode}\",\n    )\n\n\nif __name__ == \"__main__\":\n    main()\n", "model/model_training/models/patching_llama.py": "import math\nimport warnings\nfrom typing import Optional, Tuple\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom transformers.models.llama.modeling_llama import LlamaAttention, apply_rotary_pos_emb, repeat_kv\n\nfrom .patching_utils import compute_flash_attention\n\n\n# adapted from https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L185\ndef llama_forward_with_flash_attn(\n    self: LlamaAttention,\n    flash_attn: nn.Module,  # flash_attn.modules.mha.FlashSelfAttention\n    hidden_states: torch.Tensor,\n    attention_mask: Optional[torch.Tensor] = None,\n    position_ids: Optional[torch.LongTensor] = None,\n    past_key_value: Optional[Tuple[torch.Tensor]] = None,\n    output_attentions: bool = False,\n    use_cache: bool = False,\n) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n    bsz, q_len, _ = hidden_states.size()\n\n    if output_attentions:\n        warnings.warn(\"Output attentions is not supported for patched `LlamaAttention`, returning `None` instead.\")\n    if self.config.pretraining_tp > 1:\n        key_value_slicing = (self.num_key_value_heads * self.head_dim) // self.config.pretraining_tp\n        query_slices = self.q_proj.weight.split((self.num_heads * self.head_dim) // self.config.pretraining_tp, dim=0)\n        key_slices = self.k_proj.weight.split(key_value_slicing, dim=0)\n        value_slices = self.v_proj.weight.split(key_value_slicing, dim=0)\n\n        query_states = [F.linear(hidden_states, query_slices[i]) for i in range(self.config.pretraining_tp)]\n        query_states = torch.cat(query_states, dim=-1)\n\n        key_states = [F.linear(hidden_states, key_slices[i]) for i in range(self.config.pretraining_tp)]\n        key_states = torch.cat(key_states, dim=-1)\n\n        value_states = [F.linear(hidden_states, value_slices[i]) for i in range(self.config.pretraining_tp)]\n        value_states = torch.cat(value_states, dim=-1)\n\n    else:\n        query_states = self.q_proj(hidden_states)\n        key_states = self.k_proj(hidden_states)\n        value_states = self.v_proj(hidden_states)\n\n    query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n    key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n    value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n\n    kv_seq_len = key_states.shape[-2]\n    if past_key_value is not None:\n        kv_seq_len += past_key_value[0].shape[-2]\n    cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)\n    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n\n    if past_key_value is not None:\n        # reuse k, v, self_attention\n        key_states = torch.cat([past_key_value[0], key_states], dim=2)\n        value_states = torch.cat([past_key_value[1], value_states], dim=2)\n\n    past_key_value = (key_states, value_states) if use_cache else None\n\n    # repeat k/v heads if n_kv_heads < n_heads\n    key_states = repeat_kv(key_states, self.num_key_value_groups)\n    value_states = repeat_kv(value_states, self.num_key_value_groups)\n\n    if (\n        query_states.shape == key_states.shape\n    ):  # and (attention_mask is None or attention_mask[:, 0, -1, 0].min() >= 0):\n        if attention_mask is not None:\n            attention_mask = attention_mask[:, 0, -1]\n\n        flash_attn.train(self.training)\n        out_dtype = value_states.dtype\n        q, k, v = (\n            query_states.transpose(1, 2),\n            key_states.transpose(1, 2),\n            value_states.transpose(1, 2),\n        )\n        attn_output = compute_flash_attention(flash_attn, q, k, v, attention_mask)\n        attn_output = attn_output.transpose(1, 2).to(out_dtype)\n\n        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n            raise ValueError(\n                f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n                f\" {attn_output.size()}\"\n            )\n    else:\n        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n\n        if attn_weights.size() != (bsz, self.num_heads, q_len, kv_seq_len):\n            raise ValueError(\n                f\"Attention weights should be of size {(bsz, self.num_heads, q_len, kv_seq_len)}, but is\"\n                f\" {attn_weights.size()}\"\n            )\n\n        if attention_mask is not None:\n            if attention_mask.size() != (bsz, 1, q_len, kv_seq_len):\n                raise ValueError(\n                    f\"Attention mask should be of size {(bsz, 1, q_len, kv_seq_len)}, but is {attention_mask.size()}\"\n                )\n            attn_weights = attn_weights + attention_mask\n\n        # upcast attention to fp32\n        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n        attn_output = torch.matmul(attn_weights, value_states)\n\n        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n            raise ValueError(\n                f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n                f\" {attn_output.size()}\"\n            )\n\n    attn_output = attn_output.transpose(1, 2).contiguous()\n    attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n\n    if self.config.pretraining_tp > 1:\n        attn_output = attn_output.split(self.hidden_size // self.config.pretraining_tp, dim=2)\n        o_proj_slices = self.o_proj.weight.split(self.hidden_size // self.config.pretraining_tp, dim=1)\n        attn_output = sum([F.linear(attn_output[i], o_proj_slices[i]) for i in range(self.config.pretraining_tp)])\n    else:\n        attn_output = self.o_proj(attn_output)\n\n    return attn_output, None, past_key_value\n", "model/model_training/models/patching_neox.py": "import torch\nimport torch.nn as nn\nimport transformers\n\nfrom .patching_utils import compute_flash_attention\n\n\ndef neox_forward_with_flash_attn(\n    self: transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXAttention,\n    flash_attn: nn.Module,  # flash_attn.modules.mha.FlashSelfAttention\n    query: torch.Tensor,\n    key: torch.Tensor,\n    value: torch.Tensor,\n    attention_mask=None,\n    head_mask=None,\n):\n    # query, key, value: [bs, num_attention_heads, seq_len, attn_head_size]\n    if query.shape == key.shape:\n        flash_attn.train(self.training)\n        out_dtype = value.dtype\n        q, k, v = query.transpose(1, 2), key.transpose(1, 2), value.transpose(1, 2)\n        if attention_mask is not None:\n            attention_mask = attention_mask[:, 0, 0, :]\n        out = compute_flash_attention(flash_attn, q, k, v, attention_mask)\n        out = out.transpose(1, 2).to(out_dtype)\n        return out, None\n    else:\n        return self.old_forward(query, key, value, attention_mask, head_mask)\n", "model/model_training/models/peft_modeling.py": "from dataclasses import dataclass\nfrom pathlib import Path\n\nimport torch\nfrom huggingface_hub import hf_hub_download\nfrom model_training.utils.utils import get_all_linear_layers, get_model, get_tokenizer, merge_dicts\nfrom peft import LoraConfig, PeftModel, PrefixTuningConfig, get_peft_model, prepare_model_for_int8_training\n\n\ndef load_peft_model(model, peft_model_path, tokenizer):\n    model.resize_token_embeddings(len(tokenizer))\n    model.config.eos_token_id = tokenizer.eos_token_id\n    model.config.bos_token_id = tokenizer.bos_token_id\n    model.config.pad_token_id = tokenizer.pad_token_id\n    model = PeftModel.from_pretrained(\n        model,\n        peft_model_path,\n        torch_dtype=model.dtype,\n    )\n    model.eos_token_id = tokenizer.eos_token_id\n    try:\n        extra_embeds = hf_hub_download(peft_model_path, \"extra_embeddings.pt\")\n        embed_weights = torch.load(extra_embeds, map_location=model.device)\n        model.base_model.model.model.embed_tokens.weight[\n            len(tokenizer) - embed_weights.shape[0] :, :\n        ] = embed_weights.to(model.base_model.model.model.embed_tokens.weight.dtype)\n    except Exception:\n        print(\"Warning:Extra embeddings not added. This is expected if adapter file contains WTE\")\n\n    return model\n\n\ndef prepare_model_for_gradient_checkpointing(model):\n    r\"\"\"\n    Prepares the model for gradient checkpointing if necessary\n    \"\"\"\n    if not getattr(model, \"is_loaded_in_8bit\", False):\n        if hasattr(model, \"enable_input_require_grads\"):\n            model.enable_input_require_grads()\n        else:\n\n            def make_inputs_require_grad(module, input, output):\n                output.requires_grad_(True)\n\n            model.get_input_embeddings().register_forward_hook(make_inputs_require_grad)\n    return model\n\n\ndef peft_model(model, training_config):\n    peft_config = training_config.peft_config\n    peft_type = peft_config.pop(\"peft_type\", \"lora\")\n    if peft_type == \"lora\":\n        default_args = {\n            \"r\": 16,\n            \"lora_alpha\": 32,\n            \"target_modules\": \"all\",\n            \"lora_dropout\": 0.05,\n            \"bias\": \"none\",\n            \"task_type\": \"CAUSAL_LM\",\n            \"modules_to_save\": [\"wte\", \"lm_head\"],\n        }\n        kwargs = merge_dicts(default_args, peft_config)\n        if kwargs.get(\"target_modules\") == \"all\":\n            kwargs.update({\"target_modules\": get_all_linear_layers(model)})\n        config = LoraConfig(**kwargs)\n    elif peft_type == \"prefix-tuning\":\n        default_args = {\n            \"num_virtual_tokens\": 30,\n            \"prefix_projection\": True,\n            \"encoder_hidden_size\": 1024,\n            \"task_type\": \"CAUSAL_LM\",\n        }\n        kwargs = merge_dicts(default_args, peft_config)\n        config = PrefixTuningConfig(**kwargs)\n    else:\n        raise ValueError(\"peft_method config is lora or prefix-tuning\")\n    model = get_peft_model(model, config)\n\n    if training_config.int8_training:\n        model = prepare_model_for_int8_training(model)\n\n    if training_config.gradient_checkpointing:\n        model = prepare_model_for_gradient_checkpointing(model)\n    model.print_trainable_parameters()\n    return model\n\n\n@dataclass\nclass SaveLoraConfig:\n    dtype: torch.dtype = torch.float16\n    is_reward_model: bool = False\n    quantization: bool = False\n    seq2seqmodel: bool = False\n    freeze_layer: bool = False\n    residual_dropout: float = 0\n    use_flash_attention: bool = False\n    adapter_save_path: str = \"adapter\"\n    cache_dir: str = \"\"\n    model_name: str = \"\"\n    torch_ckpt_path: str = \"\"\n    peft_type: str = \"lora\"\n\n\ndef save_adapter_model_from_ckpt(save_config: SaveLoraConfig):\n    tokenizer = get_tokenizer(save_config)\n    model = get_model(save_config, tokenizer)\n    model = peft_model(model)\n    model.load_state_dict(torch.load(save_config.torch_ckpt_path))\n    vocab_size = tokenizer.vocab_size\n    num_special_tokens = len(tokenizer.additional_special_tokens)\n\n    new_embs = model.state_dict()[\"base_model.model.model.embed_tokens.weight\"][\n        vocab_size : vocab_size + num_special_tokens, :\n    ].clone()\n    new_embs = new_embs.to(save_config.dtype)\n    model.save_pretrained(save_config.adapter_save_path, torch_dtype=save_config.dtype)\n    tokenizer.save_pretrained(save_config.adapter_save_path)\n    torch.save(new_embs, Path(save_config.adapter_save_path).joinpath(\"extra_embeddings.pt\"))\n", "model/model_training/models/prefix_llama.py": "# Copyright 2022 EleutherAI and the HuggingFace Inc. team. All rights reserved.\n#\n# This code is based on EleutherAI's GPT-NeoX library and the GPT-NeoX\n# and OPT implementations in this library. It has been modified from its\n# original forms to accommodate minor architectural differences compared\n# to GPT-NeoX and OPT used by the Meta AI team that trained the model.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\" PyTorch LLaMA model.\"\"\"\nimport math\nfrom typing import List, Optional, Tuple, Union\n\nimport torch\nimport torch.utils.checkpoint\nfrom torch import nn\nfrom torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\nfrom transformers import LlamaConfig\nfrom transformers.activations import ACT2FN\nfrom transformers.modeling_outputs import (\n    BaseModelOutputWithPast,\n    CausalLMOutputWithPast,\n    SequenceClassifierOutputWithPast,\n)\nfrom transformers.modeling_utils import PreTrainedModel\nfrom transformers.utils import (\n    add_start_docstrings,\n    add_start_docstrings_to_model_forward,\n    logging,\n    replace_return_docstrings,\n)\n\nlogger = logging.get_logger(__name__)\n\n_CONFIG_FOR_DOC = \"LlamaConfig\"\n\n\ndef _make_causal_mask(input_ids_shape: torch.Size, dtype: torch.dtype, past_key_values_length: int = 0):\n    \"\"\"\n    Make causal mask used for bi-directional self-attention.\n    \"\"\"\n    bsz, tgt_len = input_ids_shape\n    mask = torch.full((tgt_len, tgt_len), torch.tensor(torch.finfo(dtype).min))\n    mask_cond = torch.arange(mask.size(-1))\n    mask.masked_fill_(mask_cond < (mask_cond + 1).view(mask.size(-1), 1), 0)\n    mask = mask.to(dtype)\n\n    if past_key_values_length > 0:\n        mask = torch.cat([torch.zeros(tgt_len, past_key_values_length, dtype=dtype), mask], dim=-1)\n    return mask[None, None, :, :].expand(bsz, 1, tgt_len, tgt_len + past_key_values_length)\n\n\ndef _expand_mask(mask: torch.Tensor, dtype: torch.dtype, tgt_len: Optional[int] = None):\n    \"\"\"\n    Expands attention_mask from `[bsz, seq_len]` to `[bsz, 1, tgt_seq_len, src_seq_len]`.\n    \"\"\"\n    bsz, src_len = mask.size()\n    tgt_len = tgt_len if tgt_len is not None else src_len\n\n    expanded_mask = mask[:, None, None, :].expand(bsz, 1, tgt_len, src_len).to(dtype)\n\n    inverted_mask = 1.0 - expanded_mask\n\n    return inverted_mask.masked_fill(inverted_mask.to(torch.bool), torch.finfo(dtype).min)\n\n\nclass LlamaRMSNorm(nn.Module):\n    def __init__(self, hidden_size, eps=1e-6):\n        \"\"\"\n        LlamaRMSNorm is equivalent to T5LayerNorm\n        \"\"\"\n        super().__init__()\n        self.weight = nn.Parameter(torch.ones(hidden_size))\n        self.variance_epsilon = eps\n\n    def forward(self, hidden_states):\n        variance = hidden_states.to(torch.float32).pow(2).mean(-1, keepdim=True)\n        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n\n        # convert into half-precision if necessary\n        if self.weight.dtype in [torch.float16, torch.bfloat16]:\n            hidden_states = hidden_states.to(self.weight.dtype)\n\n        return self.weight * hidden_states\n\n\nclass LlamaRotaryEmbedding(torch.nn.Module):\n    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None):\n        super().__init__()\n        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float().to(device) / dim))\n        self.register_buffer(\"inv_freq\", inv_freq)\n\n        # Build here to make `torch.jit.trace` work.\n        self.max_seq_len_cached = max_position_embeddings\n        t = torch.arange(self.max_seq_len_cached, device=self.inv_freq.device, dtype=self.inv_freq.dtype)\n        freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)\n        # Different from paper, but it uses a different permutation in order to obtain the same calculation\n        emb = torch.cat((freqs, freqs), dim=-1)\n        self.register_buffer(\"cos_cached\", emb.cos()[None, None, :, :], persistent=False)\n        self.register_buffer(\"sin_cached\", emb.sin()[None, None, :, :], persistent=False)\n\n    def forward(self, x, seq_len=None):\n        # x: [bs, num_attention_heads, seq_len, head_size]\n        # This `if` block is unlikely to be run after we build sin/cos in `__init__`. Keep the logic here just in case.\n        if seq_len > self.max_seq_len_cached:\n            self.max_seq_len_cached = seq_len\n            t = torch.arange(self.max_seq_len_cached, device=x.device, dtype=self.inv_freq.dtype)\n            freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)\n            # Different from paper, but it uses a different permutation in order to obtain the same calculation\n            emb = torch.cat((freqs, freqs), dim=-1).to(x.device)\n            self.register_buffer(\"cos_cached\", emb.cos()[None, None, :, :], persistent=False)\n            self.register_buffer(\"sin_cached\", emb.sin()[None, None, :, :], persistent=False)\n        return (\n            self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n            self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n        )\n\n\ndef rotate_half(x):\n    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n    x1 = x[..., : x.shape[-1] // 2]\n    x2 = x[..., x.shape[-1] // 2 :]\n    return torch.cat((-x2, x1), dim=-1)\n\n\ndef apply_rotary_pos_emb(q, k, cos, sin, offset: int = 0):\n    cos = cos[..., offset : q.shape[-2] + offset, :]\n    sin = sin[..., offset : q.shape[-2] + offset, :]\n    q_embed = (q * cos) + (rotate_half(q) * sin)\n    k_embed = (k * cos) + (rotate_half(k) * sin)\n    return q_embed, k_embed\n\n\nclass LlamaMLP(nn.Module):\n    def __init__(\n        self,\n        hidden_size: int,\n        intermediate_size: int,\n        hidden_act: str,\n    ):\n        super().__init__()\n        self.gate_proj = nn.Linear(hidden_size, intermediate_size, bias=False)\n        self.down_proj = nn.Linear(intermediate_size, hidden_size, bias=False)\n        self.up_proj = nn.Linear(hidden_size, intermediate_size, bias=False)\n        self.act_fn = ACT2FN[hidden_act]\n\n    def forward(self, x):\n        return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n\n\nclass LlamaAttention(nn.Module):\n    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n\n    def __init__(\n        self,\n        hidden_size: int,\n        num_heads: int,\n    ):\n        super().__init__()\n        self.hidden_size = hidden_size\n        self.num_heads = num_heads\n        self.head_dim = hidden_size // num_heads\n\n        if (self.head_dim * num_heads) != self.hidden_size:\n            raise ValueError(\n                f\"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}\"\n                f\" and `num_heads`: {num_heads}).\"\n            )\n        self.q_proj = nn.Linear(\n            hidden_size,\n            num_heads * self.head_dim,\n            bias=False,\n        )\n        self.k_proj = nn.Linear(\n            hidden_size,\n            num_heads * self.head_dim,\n            bias=False,\n        )\n        self.v_proj = nn.Linear(\n            hidden_size,\n            num_heads * self.head_dim,\n            bias=False,\n        )\n        self.o_proj = nn.Linear(\n            num_heads * self.head_dim,\n            hidden_size,\n            bias=False,\n        )\n        self.rotary_emb = LlamaRotaryEmbedding(self.head_dim)\n\n    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        output_attentions: bool = False,\n        use_cache: bool = False,\n    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n        \"\"\"Input shape: Batch x Time x Channel\"\"\"\n\n        bsz, q_len, _ = hidden_states.size()\n\n        query_states = self.q_proj(hidden_states).view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n        key_states = self.k_proj(hidden_states).view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n        value_states = self.v_proj(hidden_states).view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n\n        kv_seq_len = key_states.shape[-2]\n        offset = 0\n        if past_key_value is not None:\n            offset = past_key_value[0].shape[-2]\n            kv_seq_len += offset\n        cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)\n        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, offset=offset)\n        # [bsz, nh, t, hd]\n\n        if past_key_value is not None:\n            # reuse k, v, self_attention\n            key_states = torch.cat([past_key_value[0], key_states], dim=2)\n            value_states = torch.cat([past_key_value[1], value_states], dim=2)\n\n        past_key_value = (key_states, value_states) if use_cache else None\n\n        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n\n        if attn_weights.size() != (bsz, self.num_heads, q_len, kv_seq_len):\n            raise ValueError(\n                f\"Attention weights should be of size {(bsz * self.num_heads, q_len, kv_seq_len)}, but is\"\n                f\" {attn_weights.size()}\"\n            )\n\n        if attention_mask is not None:\n            if attention_mask.size() != (bsz, 1, q_len, kv_seq_len):\n                raise ValueError(\n                    f\"Attention mask should be of size {(bsz, 1, q_len, kv_seq_len)}, but is {attention_mask.size()}\"\n                )\n            attn_weights = attn_weights + attention_mask\n            attn_weights = torch.max(attn_weights, torch.tensor(torch.finfo(attn_weights.dtype).min))\n\n        # upcast attention to fp32\n        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n        attn_output = torch.matmul(attn_weights, value_states)\n\n        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n            raise ValueError(\n                f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n                f\" {attn_output.size()}\"\n            )\n\n        attn_output = attn_output.transpose(1, 2)\n        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n\n        attn_output = self.o_proj(attn_output)\n\n        if not output_attentions:\n            attn_weights = None\n\n        return attn_output, attn_weights, past_key_value\n\n\nclass LlamaDecoderLayer(nn.Module):\n    def __init__(self, config: LlamaConfig):\n        super().__init__()\n        self.hidden_size = config.hidden_size\n        self.self_attn = LlamaAttention(\n            hidden_size=self.hidden_size,\n            num_heads=config.num_attention_heads,\n        )\n        self.mlp = LlamaMLP(\n            hidden_size=self.hidden_size,\n            intermediate_size=config.intermediate_size,\n            hidden_act=config.hidden_act,\n        )\n        self.input_layernorm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n        self.post_attention_layernorm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        output_attentions: Optional[bool] = False,\n        use_cache: Optional[bool] = False,\n        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n        \"\"\"\n        Args:\n            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n            attention_mask (`torch.FloatTensor`, *optional*): attention mask of size\n                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n            output_attentions (`bool`, *optional*):\n                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                returned tensors for more detail.\n            use_cache (`bool`, *optional*):\n                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n                (see `past_key_values`).\n            past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n        \"\"\"\n\n        residual = hidden_states\n\n        hidden_states = self.input_layernorm(hidden_states)\n\n        # Self Attention\n        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n            hidden_states=hidden_states,\n            past_key_value=past_key_value,\n            attention_mask=attention_mask,\n            output_attentions=output_attentions,\n            use_cache=use_cache,\n        )\n        hidden_states = residual + hidden_states\n\n        # Fully Connected\n        residual = hidden_states\n        hidden_states = self.post_attention_layernorm(hidden_states)\n        hidden_states = self.mlp(hidden_states)\n        hidden_states = residual + hidden_states\n\n        outputs = (hidden_states,)\n\n        if output_attentions:\n            outputs += (self_attn_weights,)\n\n        if use_cache:\n            outputs += (present_key_value,)\n\n        return outputs\n\n\nLLAMA_START_DOCSTRING = r\"\"\"\n    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n    etc.)\n\n    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n    and behavior.\n\n    Parameters:\n        config ([`LlamaConfig`]):\n            Model configuration class with all the parameters of the model. Initializing with a config file does not\n            load the weights associated with the model, only the configuration. Check out the\n            [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n\"\"\"\n\n\n@add_start_docstrings(\n    \"The bare LLaMA Model outputting raw hidden-states without any specific head on top.\",\n    LLAMA_START_DOCSTRING,\n)\nclass LlamaPreTrainedModel(PreTrainedModel):\n    config_class = LlamaConfig\n    base_model_prefix = \"model\"\n    supports_gradient_checkpointing = True\n    _no_split_modules = [\"LlamaDecoderLayer\"]\n    _keys_to_ignore_on_load_unexpected = [r\"decoder\\.version\"]\n\n    def _init_weights(self, module):\n        std = self.config.initializer_range\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=std)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=std)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n\n    def _set_gradient_checkpointing(self, module, value=False):\n        if isinstance(module, LlamaModel):\n            module.gradient_checkpointing = value\n\n\nLLAMA_INPUTS_DOCSTRING = r\"\"\"\n    Args:\n        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n            it.\n\n            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n            [`PreTrainedTokenizer.__call__`] for details.\n\n            [What are input IDs?](../glossary#input-ids)\n        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n\n            - 1 for tokens that are **not masked**,\n            - 0 for tokens that are **masked**.\n\n            [What are attention masks?](../glossary#attention-mask)\n\n            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n            [`PreTrainedTokenizer.__call__`] for details.\n\n            If `past_key_values` is used, optionally only the last `decoder_input_ids` have to be input (see\n            `past_key_values`).\n\n            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]\n            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more\n            information on the default strategy.\n\n            - 1 indicates the head is **not masked**,\n            - 0 indicates the head is **masked**.\n\n        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n            `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of shape\n            `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.\n\n            Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n            blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.\n\n            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n            `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n            model's internal embedding lookup matrix.\n        use_cache (`bool`, *optional*):\n            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n            `past_key_values`).\n        output_attentions (`bool`, *optional*):\n            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n            tensors for more detail.\n        output_hidden_states (`bool`, *optional*):\n            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n            more detail.\n        return_dict (`bool`, *optional*):\n            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n\"\"\"\n\n\n@add_start_docstrings(\n    \"The bare LLaMA Model outputting raw hidden-states without any specific head on top.\",\n    LLAMA_START_DOCSTRING,\n)\nclass LlamaModel(LlamaPreTrainedModel):\n    \"\"\"\n    Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer is a [`LlamaDecoderLayer`]\n\n    Args:\n        config: LlamaConfig\n    \"\"\"\n\n    def __init__(self, config: LlamaConfig):\n        super().__init__(config)\n        self.padding_idx = config.pad_token_id\n        self.vocab_size = config.vocab_size\n\n        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)\n        self.layers = nn.ModuleList([LlamaDecoderLayer(config) for _ in range(config.num_hidden_layers)])\n        self.norm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n\n        self.gradient_checkpointing = False\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_input_embeddings(self):\n        return self.embed_tokens\n\n    def set_input_embeddings(self, value):\n        self.embed_tokens = value\n\n    # Copied from transformers.models.bart.modeling_bart.BartDecoder._prepare_decoder_attention_mask\n    def _prepare_decoder_attention_mask(self, attention_mask, input_shape, inputs_embeds, past_key_values_length):\n        # create causal mask\n        # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n        combined_attention_mask = None\n        if input_shape[-1] > 1:\n            combined_attention_mask = _make_causal_mask(\n                input_shape, inputs_embeds.dtype, past_key_values_length=past_key_values_length\n            ).to(inputs_embeds.device)\n\n        if attention_mask is not None:\n            # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n            tgt_len = input_shape[-1]\n            expanded_attn_mask = _expand_mask(attention_mask, inputs_embeds.dtype, tgt_len=tgt_len).to(\n                inputs_embeds.device\n            )\n            combined_attention_mask = (\n                expanded_attn_mask if combined_attention_mask is None else expanded_attn_mask + combined_attention_mask\n            )\n\n        return combined_attention_mask\n\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        past_key_values: Optional[List[torch.FloatTensor]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) -> Union[Tuple, BaseModelOutputWithPast]:\n        r\"\"\"\n        Args:\n            input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n                Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you\n                provide it.\n\n                Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n                [`PreTrainedTokenizer.__call__`] for details.\n\n                [What are input IDs?](../glossary#input-ids)\n            attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n                Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n\n                - 1 for tokens that are **not masked**,\n                - 0 for tokens that are **masked**.\n\n                [What are attention masks?](../glossary#attention-mask)\n            past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n                Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n                shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of\n\n                Contains pre-computed hidden-states (key and values in the self-attention blocks and in the\n                cross-attention blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.\n\n                If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those\n                that don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of\n                all `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n            use_cache (`bool`, *optional*):\n                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n                (see `past_key_values`).\n            inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n                Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation.\n                This is useful if you want more control over how to convert `input_ids` indices into associated vectors\n                than the model's internal embedding lookup matrix.\n            output_attentions (`bool`, *optional*):\n                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                returned tensors for more detail.\n            output_hidden_states (`bool`, *optional*):\n                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n                for more detail.\n            return_dict (`bool`, *optional*):\n                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n        \"\"\"\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        use_cache = use_cache if use_cache is not None else self.config.use_cache\n\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        # retrieve input_ids and inputs_embeds\n        if input_ids is not None and inputs_embeds is not None:\n            raise ValueError(\"You cannot specify both decoder_input_ids and decoder_inputs_embeds at the same time\")\n        elif input_ids is not None:\n            batch_size, seq_length = input_ids.shape\n        elif inputs_embeds is not None:\n            batch_size, seq_length, _ = inputs_embeds.shape\n        else:\n            raise ValueError(\"You have to specify either decoder_input_ids or decoder_inputs_embeds\")\n        seq_length_with_past = seq_length\n        past_key_values_length = 0\n        if past_key_values is not None:\n            past_key_values_length = past_key_values[0][0].shape[2]\n            seq_length_with_past = seq_length_with_past + past_key_values_length\n        if inputs_embeds is None:\n            inputs_embeds = self.embed_tokens(input_ids)\n        # embed positions\n        if attention_mask is None:\n            attention_mask = torch.ones(\n                (batch_size, seq_length_with_past), dtype=torch.bool, device=inputs_embeds.device\n            )\n        attention_mask = self._prepare_decoder_attention_mask(\n            attention_mask, (batch_size, seq_length), inputs_embeds, past_key_values_length\n        )\n\n        hidden_states = inputs_embeds\n\n        if self.gradient_checkpointing and self.training:\n            if use_cache:\n                logger.warning_once(\n                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n                )\n                use_cache = False\n\n        # decoder layers\n        all_hidden_states = () if output_hidden_states else None\n        all_self_attns = () if output_attentions else None\n        next_decoder_cache = () if use_cache else None\n\n        for idx, decoder_layer in enumerate(self.layers):\n            if output_hidden_states:\n                all_hidden_states += (hidden_states,)\n\n            past_key_value = past_key_values[idx] if past_key_values is not None else None\n\n            if self.gradient_checkpointing and self.training:\n\n                def create_custom_forward(module):\n                    def custom_forward(*inputs):\n                        # None for past_key_value\n                        return module(*inputs)\n\n                    return custom_forward\n\n                layer_outputs = torch.utils.checkpoint.checkpoint(\n                    create_custom_forward(decoder_layer),\n                    hidden_states,\n                    attention_mask,\n                    output_attentions,\n                    False,\n                    past_key_value,\n                )\n            else:\n                layer_outputs = decoder_layer(\n                    hidden_states,\n                    attention_mask=attention_mask,\n                    past_key_value=past_key_value,\n                    output_attentions=output_attentions,\n                    use_cache=use_cache,\n                )\n\n            hidden_states = layer_outputs[0]\n\n            if use_cache:\n                next_decoder_cache += (layer_outputs[2 if output_attentions else 1],)\n\n            if output_attentions:\n                all_self_attns += (layer_outputs[1],)\n\n        hidden_states = self.norm(hidden_states)\n\n        # add hidden states from the last decoder layer\n        if output_hidden_states:\n            all_hidden_states += (hidden_states,)\n\n        next_cache = next_decoder_cache if use_cache else None\n        if not return_dict:\n            return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)\n        return BaseModelOutputWithPast(\n            last_hidden_state=hidden_states,\n            past_key_values=next_cache,\n            hidden_states=all_hidden_states,\n            attentions=all_self_attns,\n        )\n\n\nclass LlamaForCausalLM(LlamaPreTrainedModel):\n    _keys_to_ignore_on_load_missing = [r\"lm_head.weight\"]\n\n    def __init__(self, config):\n        super().__init__(config)\n        self.model = LlamaModel(config)\n\n        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_input_embeddings(self):\n        return self.model.embed_tokens\n\n    def set_input_embeddings(self, value):\n        self.model.embed_tokens = value\n\n    def get_output_embeddings(self):\n        return self.lm_head\n\n    def set_output_embeddings(self, new_embeddings):\n        self.lm_head = new_embeddings\n\n    def set_decoder(self, decoder):\n        self.model = decoder\n\n    def get_decoder(self):\n        return self.model\n\n    @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        past_key_values: Optional[List[torch.FloatTensor]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        labels: Optional[torch.LongTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) -> Union[Tuple, CausalLMOutputWithPast]:\n        r\"\"\"\n        Args:\n            input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n                Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you\n                provide it.\n\n                Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n                [`PreTrainedTokenizer.__call__`] for details.\n\n                [What are input IDs?](../glossary#input-ids)\n            attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n                Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n\n                - 1 for tokens that are **not masked**,\n                - 0 for tokens that are **masked**.\n\n                [What are attention masks?](../glossary#attention-mask)\n            past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n                Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n                shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of\n                shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`. The two additional\n                tensors are only required when the model is used as a decoder in a Sequence to Sequence model.\n\n                Contains pre-computed hidden-states (key and values in the self-attention blocks and in the\n                cross-attention blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.\n\n                If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those\n                that don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of\n                all `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n            inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n                Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation.\n                This is useful if you want more control over how to convert `input_ids` indices into associated vectors\n                than the model's internal embedding lookup matrix.\n            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n            use_cache (`bool`, *optional*):\n                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n                (see `past_key_values`).\n            output_attentions (`bool`, *optional*):\n                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                returned tensors for more detail.\n            output_hidden_states (`bool`, *optional*):\n                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n                for more detail.\n            return_dict (`bool`, *optional*):\n                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n\n        Returns:\n\n        Example:\n\n        ```python\n        >>> from transformers import AutoTokenizer, LlamaForCausalLM\n\n        >>> model = LlamaForCausalLM.from_pretrained(PATH_TO_CONVERTED_WEIGHTS)\n        >>> tokenizer = AutoTokenizer.from_pretrained(PATH_TO_CONVERTED_TOKENIZER)\n\n        >>> prompt = \"Hey, are you consciours? Can you talk to me?\"\n        >>> inputs = tokenizer(prompt, return_tensors=\"pt\")\n\n        >>> # Generate\n        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)\n        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n        \"Hey, are you consciours? Can you talk to me?\\nI'm not consciours, but I can talk to you.\"\n        ```\"\"\"\n\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n        outputs = self.model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            past_key_values=past_key_values,\n            inputs_embeds=inputs_embeds,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        hidden_states = outputs[0]\n        logits = self.lm_head(hidden_states)\n\n        loss = None\n        if labels is not None:\n            # Shift so that tokens < n predict n\n            shift_logits = logits[..., :-1, :].contiguous()\n            shift_labels = labels[..., 1:].contiguous()\n            # Flatten the tokens\n            loss_fct = CrossEntropyLoss()\n            shift_logits = shift_logits.view(-1, self.config.vocab_size)\n            shift_labels = shift_labels.view(-1)\n            # Enable model/pipeline parallelism\n            shift_labels = shift_labels.to(shift_logits.device)\n            loss = loss_fct(shift_logits, shift_labels)\n\n        if not return_dict:\n            output = (logits,) + outputs[1:]\n            return (loss,) + output if loss is not None else output\n\n        return CausalLMOutputWithPast(\n            loss=loss,\n            logits=logits,\n            past_key_values=outputs.past_key_values,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )\n\n    def prepare_inputs_for_generation(\n        self, input_ids, past_key_values=None, attention_mask=None, inputs_embeds=None, **kwargs\n    ):\n        if past_key_values:\n            input_ids = input_ids[:, -1:]\n\n        # if `inputs_embeds` are passed, we only want to use them in the 1st generation step\n        if inputs_embeds is not None and past_key_values is None:\n            model_inputs = {\"inputs_embeds\": inputs_embeds}\n        else:\n            model_inputs = {\"input_ids\": input_ids}\n\n        model_inputs.update(\n            {\n                \"past_key_values\": past_key_values,\n                \"use_cache\": kwargs.get(\"use_cache\"),\n                \"attention_mask\": attention_mask,\n            }\n        )\n        return model_inputs\n\n    @staticmethod\n    def _reorder_cache(past_key_values, beam_idx):\n        reordered_past = ()\n        for layer_past in past_key_values:\n            reordered_past += (tuple(past_state.index_select(0, beam_idx) for past_state in layer_past),)\n        return reordered_past\n\n\n@add_start_docstrings(\n    \"\"\"\n    The LLaMa Model transformer with a sequence classification head on top (linear layer).\n\n    [`LlamaForSequenceClassification`] uses the last token in order to do the classification, as other causal models\n    (e.g. GPT-2) do.\n\n    Since it does classification on the last token, it requires to know the position of the last token. If a\n    `pad_token_id` is defined in the configuration, it finds the last token that is not a padding token in each row. If\n    no `pad_token_id` is defined, it simply takes the last value in each row of the batch. Since it cannot guess the\n    padding tokens when `inputs_embeds` are passed instead of `input_ids`, it does the same (take the last value in\n    each row of the batch).\n    \"\"\",\n    LLAMA_START_DOCSTRING,\n)\nclass LlamaForSequenceClassification(LlamaPreTrainedModel):\n    _keys_to_ignore_on_load_missing = [r\"lm_head.weight\"]\n\n    def __init__(self, config):\n        super().__init__(config)\n        self.num_labels = config.num_labels\n        self.model = LlamaModel(config)\n        self.score = nn.Linear(config.hidden_size, self.num_labels, bias=False)\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_input_embeddings(self):\n        return self.model.embed_tokens\n\n    def set_input_embeddings(self, value):\n        self.model.embed_tokens = value\n\n    @add_start_docstrings_to_model_forward(LLAMA_INPUTS_DOCSTRING)\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        past_key_values: Optional[List[torch.FloatTensor]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        labels: Optional[torch.LongTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) -> Union[Tuple, SequenceClassifierOutputWithPast]:\n        r\"\"\"\n        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n        \"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        transformer_outputs = self.model(\n            input_ids,\n            past_key_values=past_key_values,\n            attention_mask=attention_mask,\n            inputs_embeds=inputs_embeds,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n        hidden_states = transformer_outputs[0]\n        logits = self.score(hidden_states)\n\n        if input_ids is not None:\n            batch_size = input_ids.shape[0]\n        else:\n            batch_size = inputs_embeds.shape[0]\n\n        if self.config.pad_token_id is None and batch_size != 1:\n            raise ValueError(\"Cannot handle batch sizes > 1 if no padding token is defined.\")\n        if self.config.pad_token_id is None:\n            sequence_lengths = -1\n        else:\n            if input_ids is not None:\n                sequence_lengths = (torch.ne(input_ids, self.config.pad_token_id).sum(-1) - 1).to(logits.device)\n            else:\n                sequence_lengths = -1\n\n        pooled_logits = logits[torch.arange(batch_size, device=logits.device), sequence_lengths]\n\n        loss = None\n        if labels is not None:\n            if self.config.problem_type is None:\n                if self.num_labels == 1:\n                    self.config.problem_type = \"regression\"\n                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n                    self.config.problem_type = \"single_label_classification\"\n                else:\n                    self.config.problem_type = \"multi_label_classification\"\n\n            if self.config.problem_type == \"regression\":\n                loss_fct = MSELoss()\n                if self.num_labels == 1:\n                    loss = loss_fct(pooled_logits.squeeze(), labels.squeeze())\n                else:\n                    loss = loss_fct(pooled_logits, labels)\n            elif self.config.problem_type == \"single_label_classification\":\n                loss_fct = CrossEntropyLoss()\n                loss = loss_fct(pooled_logits.view(-1, self.num_labels), labels.view(-1))\n            elif self.config.problem_type == \"multi_label_classification\":\n                loss_fct = BCEWithLogitsLoss()\n                loss = loss_fct(pooled_logits, labels)\n        if not return_dict:\n            output = (pooled_logits,) + transformer_outputs[1:]\n            return ((loss,) + output) if loss is not None else output\n\n        return SequenceClassifierOutputWithPast(\n            loss=loss,\n            logits=pooled_logits,\n            past_key_values=transformer_outputs.past_key_values,\n            hidden_states=transformer_outputs.hidden_states,\n            attentions=transformer_outputs.attentions,\n        )\n", "model/model_training/models/reward_model.py": "from dataclasses import dataclass\nfrom typing import Literal, Optional\n\nimport torch\nimport torch.nn as nn\nfrom transformers import AutoConfig, AutoModelForSequenceClassification\nfrom transformers.models.gpt_neox.modeling_gpt_neox import GPTNeoXConfig, GPTNeoXModel, GPTNeoXPreTrainedModel\nfrom transformers.utils import ModelOutput\n\n\nclass GPTNeoXRewardModelConfig(GPTNeoXConfig):\n    model_type = \"gpt_neox_reward_model\"\n\n    pooling: Literal[\"mean\", \"last\"]\n\n    def __init__(\n        self,\n        pooling: Literal[\"mean\", \"last\"] = \"last\",\n        **kwargs,\n    ):\n        super().__init__(**kwargs)\n        self.pooling = pooling or \"last\"\n\n\n@dataclass\nclass GPTNeoXRewardModelOutput(ModelOutput):\n    \"\"\"\n    Reward model output.\n\n    Args:\n        logits (`torch.FloatTensor` of shape `(batch_size, 1)`):\n            Reward score\n    \"\"\"\n\n    logits: torch.FloatTensor = None\n\n\nclass GPTNeoXRewardModel(GPTNeoXPreTrainedModel):\n    config_class = GPTNeoXRewardModelConfig\n\n    def __init__(self, config):\n        if type(config) == GPTNeoXConfig:\n            # When a normal GPTNeoX was loaded it will be converted into a reward model.\n            # The direct `type(config) == GPTNeoXConfig` comparison is used (instead of\n            # `isinstance()`) since the configuration class of the reward model is also\n            # derived form `GPTNeoXConfig`.\n            config = GPTNeoXRewardModelConfig.from_dict(config.to_dict())\n        super().__init__(config)\n\n        self.gpt_neox = GPTNeoXModel(config)\n        self.out_proj = nn.Linear(config.hidden_size, 1)\n        self.pooling = config.pooling\n\n    def forward(\n        self,\n        input_ids,\n        attention_mask: Optional[torch.FloatTensor] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        head_mask: Optional[torch.FloatTensor] = None,\n        use_cache: Optional[bool] = None,\n        return_dict: Optional[bool] = True,\n    ) -> GPTNeoXRewardModelOutput:\n        outputs = self.gpt_neox(\n            input_ids,\n            attention_mask=attention_mask,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            use_cache=use_cache,\n            return_dict=return_dict,\n        )\n\n        hidden_states = outputs[0]\n        if self.pooling == \"mean\":\n            if attention_mask is None:\n                pooled = hidden_states.mean(dim=1)\n            else:\n                pooled = (hidden_states * attention_mask).sum(dim=1) / attention_mask.sum(dim=1)\n        elif self.pooling == \"last\":\n            if attention_mask is None:\n                pooled = hidden_states[:, -1]\n            else:\n                last_idx = attention_mask.cumsum(dim=1).argmax(dim=1)\n                pooled = hidden_states.gather(1, last_idx.view(-1, 1, 1).expand(-1, 1, hidden_states.size(-1))).squeeze(\n                    1\n                )\n        else:\n            raise ValueError(f\"Unknown pooling method: {self.pooling}\")\n\n        logits = self.out_proj(pooled)\n\n        if not return_dict:\n            return (logits,) + outputs[1:]\n\n        return GPTNeoXRewardModelOutput(logits=logits)\n\n\nAutoConfig.register(\"gpt_neox_reward_model\", GPTNeoXRewardModelConfig)\nAutoModelForSequenceClassification.register(GPTNeoXRewardModelConfig, GPTNeoXRewardModel)\n", "model/model_training/models/patching_falcon.py": "from typing import Optional, Tuple\n\nimport torch\nimport torch.nn as nn\n\nfrom .patching_utils import compute_flash_attention\n\n\ndef falcon_forward_with_flash_attn(\n    self,\n    flash_attn: nn.Module,  # flash_attn.modules.mha.FlashSelfAttention\n    hidden_states: torch.Tensor,\n    alibi: Optional[torch.Tensor],\n    attention_mask: torch.Tensor,\n    layer_past: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,\n    head_mask: Optional[torch.Tensor] = None,\n    use_cache: bool = False,\n    output_attentions: bool = False,\n) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n    \"\"\"\n    head_mask, alibi & output_attention are not supported.\n    Reference to the original `FalconAttention.forwad()` method which this patch replaces:\n    https://github.com/huggingface/transformers/blob/c965d302791cf935d6ea7776428749be678cf509/src/transformers/models/falcon/modeling_falcon.py#L281\n    \"\"\"\n\n    assert head_mask is None  # not supported.\n    assert alibi is None  # not supported.\n    assert not output_attentions  # not supported.\n\n    fused_qkv = self.query_key_value(hidden_states)  # [batch_size, seq_length, 3 x hidden_size]\n    num_kv_heads = self.num_heads if self.new_decoder_architecture else self.num_kv_heads\n    # 3 x [batch_size, seq_length, num_heads, head_dim]\n    (query_layer, key_layer, value_layer) = self._split_heads(fused_qkv)\n\n    batch_size, query_length, _, _ = query_layer.shape\n\n    query_layer = query_layer.transpose(1, 2).reshape(batch_size * self.num_heads, query_length, self.head_dim)\n    key_layer = key_layer.transpose(1, 2).reshape(\n        batch_size * num_kv_heads,\n        query_length,\n        self.head_dim,\n    )\n    value_layer = value_layer.transpose(1, 2).reshape(batch_size * num_kv_heads, query_length, self.head_dim)\n\n    past_kv_length = 0 if layer_past is None else layer_past[0].shape[1]\n    query_layer, key_layer = self.maybe_rotary(query_layer, key_layer, past_kv_length)\n\n    if layer_past is not None:\n        past_key, past_value = layer_past\n        # concatenate along seq_length dimension:\n        #  - key: [batch_size * self.num_heads, kv_length, head_dim]\n        #  - value: [batch_size * self.num_heads, kv_length, head_dim]\n        key_layer = torch.cat((past_key, key_layer), dim=1)\n        value_layer = torch.cat((past_value, value_layer), dim=1)\n\n    if use_cache:\n        present = (key_layer, value_layer)\n    else:\n        present = None\n\n    query_layer_ = query_layer.reshape(batch_size, self.num_heads, -1, self.head_dim)\n    key_layer_ = key_layer.reshape(batch_size, num_kv_heads, -1, self.head_dim)\n    value_layer_ = value_layer.reshape(batch_size, num_kv_heads, -1, self.head_dim)\n\n    q = query_layer_.permute(0, 2, 1, 3)\n    k = key_layer_.permute(0, 2, 1, 3).expand(q.shape)\n    v = value_layer_.permute(0, 2, 1, 3).expand(q.shape)\n\n    if attention_mask is not None:\n        attention_mask = attention_mask[:, 0, -1]\n\n    flash_attn.train(self.training)\n    attn_output = compute_flash_attention(flash_attn, q, k, v, attention_mask=attention_mask)\n    attn_output = attn_output.reshape(batch_size, query_length, self.num_heads * self.head_dim)\n\n    output_tensor = self.dense(attn_output)\n\n    return output_tensor, present\n", "model/model_training/models/rope.py": "import torch\n\n\n# rotary pos emb helpers (torch.jit.script does not seem to support staticmethod...)\ndef rotate_half(x):\n    x1, x2 = x[..., : x.shape[-1] // 2], x[..., x.shape[-1] // 2 :]\n    return torch.cat((-x2, x1), dim=-1)\n\n\nclass RWNTKScaledRope(torch.nn.Module):\n\n    \"\"\"\n    NTK-Scaled RoPE for RefinedWebModel\n    \"\"\"\n\n    def __init__(\n        self,\n        head_dim: int,\n        base=10000,\n        alpha: int = 2,\n    ):\n        super().__init__()\n        self.alpha = alpha\n        base = base * self.alpha ** (head_dim / (head_dim - 2))\n        inv_freq = 1.0 / (base ** (torch.arange(0, head_dim, 2).float() / head_dim))\n        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n        self.head_dim = head_dim\n        self.seq_len_cached = -1\n        self.batch_size_cached = None\n        self.cos_cached: torch.Tensor | None = None\n        self.sin_cached: torch.Tensor | None = None\n\n    def cos_sin(\n        self,\n        seq_len: int,\n        past_key_values_length: int,\n        device=\"cuda\",\n        dtype=torch.bfloat16,\n    ) -> torch.Tensor:\n        total_length = seq_len + past_key_values_length\n        if total_length > self.seq_len_cached:\n            self.seq_len_cached = total_length\n            t = torch.arange(total_length, device=device, dtype=self.inv_freq.dtype)\n            freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)\n            emb = torch.cat((freqs, freqs), dim=-1).to(device)\n\n            if dtype in [torch.float16, torch.bfloat16]:\n                emb = emb.float()\n\n            self.cos_cached = emb.cos()[None, :, :]\n            self.sin_cached = emb.sin()[None, :, :]\n\n            self.cos_cached = self.cos_cached.type(dtype)\n            self.sin_cached = self.sin_cached.type(dtype)\n\n        return (\n            self.cos_cached[:, past_key_values_length : seq_len + past_key_values_length],\n            self.sin_cached[:, past_key_values_length : seq_len + past_key_values_length],\n        )\n\n    def forward(self, q, k, past_key_values_length=0):\n        batch, seq_len, head_dim = q.shape\n        cos, sin = self.cos_sin(seq_len, past_key_values_length, q.device, q.dtype)\n        return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)\n", "model/model_training/models/patching.py": "from __future__ import annotations  # To make it not choke over FlashSelfAttention\n\nimport warnings\nfrom functools import partial\nfrom typing import Callable, Optional\n\nimport torch.nn as nn\nimport transformers\nfrom transformers import (\n    AutoConfig,\n    FalconForCausalLM,\n    FalconModel,\n    GPTNeoXForCausalLM,\n    GPTNeoXModel,\n    LlamaForCausalLM,\n    LlamaModel,\n)\nfrom transformers.models.llama.modeling_llama import (\n    LlamaDynamicNTKScalingRotaryEmbedding,\n    LlamaLinearScalingRotaryEmbedding,\n)\nfrom trlx.models.modeling_ppo import AutoModelForCausalLMWithHydraValueHead\n\nfrom .patching_falcon import falcon_forward_with_flash_attn\nfrom .patching_llama import llama_forward_with_flash_attn\nfrom .patching_neox import neox_forward_with_flash_attn\nfrom .reward_model import GPTNeoXRewardModel\nfrom .rope import RWNTKScaledRope\n\nSUPPORTED_MODELS = [\n    GPTNeoXModel,\n    GPTNeoXForCausalLM,\n    LlamaForCausalLM,\n    LlamaModel,\n    FalconForCausalLM,\n    FalconModel,\n    GPTNeoXRewardModel,\n    # Currently only supported by NeoX models; Will work on LLaMa models\n    AutoModelForCausalLMWithHydraValueHead,\n]\n\n\ndef _patched_mlp_forward(post_module: nn.Module, module: nn.Module, *args, **kwargs):\n    post_module.train(module.training)\n    out = module.old_forward(*args, **kwargs)\n    out = post_module(out)\n    return out\n\n\ndef _patched_attn_forward(post_module: nn.Module, module: nn.Module, *args, **kwargs):\n    post_module.train(module.training)\n    out = module.old_forward(*args, **kwargs)\n    hiddens = post_module(out[0])\n    return (hiddens,) + out[1:]\n\n\ndef add_dropout(module: nn.Module, patched_fwd: Callable, p_dropout: float = 0.1):\n    dropout = nn.Dropout(p=p_dropout)\n    module.old_forward = module.forward\n    module.forward = partial(patched_fwd, dropout, module)\n\n\ndef add_flash_attn(module: nn.Module, causal: bool = True):\n    \"\"\"\n    Replaces the standard attention implementation with Flash Attention [1].\n    Limitations:\n      - Only works for fp16 or bf16 inputs\n      - Requires inputs to be on CUDA\n      - `output_attentions=True` does not work after patching, attention weights will be None\n      - Non-contiguous attention masks are not supported (e.g. [1, 1, 0, 1, 1, 0, 0] will just become [1, 1, 1, 1, 1, 0, 0]).\n\n    [1] https://github.com/HazyResearch/flash-attention\n    \"\"\"\n\n    flash_attn = FlashSelfAttention(causal=causal)\n    if isinstance(module, transformers.models.llama.modeling_llama.LlamaAttention):\n        module.old_forward = module.forward\n        module.forward = partial(llama_forward_with_flash_attn, module, flash_attn)\n    elif isinstance(module, transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXAttention):\n        if not hasattr(module, \"_attn\"):\n            warnings.warn(\"Provided module doesn't have a _attn() function to be patched.\")\n        module._attn = partial(neox_forward_with_flash_attn, module, flash_attn)\n    elif isinstance(module, transformers.models.falcon.modeling_falcon.FalconAttention):\n        module.forward = partial(falcon_forward_with_flash_attn, module, flash_attn)\n    else:\n        raise NotImplementedError(f\"Flash attention is not implemented for {module.__class__.__name__}.\")\n\n\ndef patch_model(\n    model: nn.Module,\n    resid_pdrop: Optional[float] = 0.1,\n    flash_attention: bool = True,\n    patch_unsupported: bool = False,\n    residual_dropout_lima: bool = False,\n):\n    \"\"\"\n    Helper function for patching HF language models.\n    Currently supports: GPTNeoX-based models\n\n    Limitations:\n      - Flash attention requires CUDA and fp16/bf16 training. It also requires contiguous attention masks.\n      - Residual dropout does not support multi-GPU training without DeepDpeed.\n    \"\"\"\n    global FlashSelfAttention\n    if flash_attention:\n        try:\n            from flash_attn.modules.mha import FlashSelfAttention  # pyright: reportMissingImports=false\n        except ModuleNotFoundError:\n            warnings.warn(\n                \"\"\"\\nmodule flash_attn not found - either install:\n  pip3 install flash_attn\nor run with:\n  --use_flash_attention=false \"\"\"\n            )\n            exit(1)\n    if (resid_pdrop is None or resid_pdrop == 0.0) and not flash_attention:\n        print(\"Continuing without patching\")\n        return\n\n    if resid_pdrop is not None and (resid_pdrop < 0 or resid_pdrop > 1.0):\n        raise ValueError(\"Invalid argument: `resid_pdrop` must be between 0.0 and 1.0\")\n\n    if not flash_attention and (resid_pdrop is None or resid_pdrop == 0.0):\n        return\n\n    if (\n        not any(isinstance(model, model_class) for model_class in SUPPORTED_MODELS)\n        and model.__class__.__name__ != \"RWForCausalLM\"\n    ):\n        if not flash_attention and (resid_pdrop is None or resid_pdrop == 0.0):\n            return  # nothing to patch\n\n        if not patch_unsupported:\n            warnings.warn(\n                \"Model patching does not support this model class. No patches will be applied. \"\n                \"If you want to force patch this model, please set `patch_unsupported=True`.\"\n            )\n            return\n\n        warnings.warn(\n            \"Patching residual dropout has only been tested with this model class. \"\n            f\"Please make sure that it also works for `{model.__class__.__name__}`.\\n\"\n            \"Or disable flash_attention and residual_dropout with:\\n\"\n            \"--use_flash_attention=false  --no-residual_dropout\"\n        )\n\n    if isinstance(model, GPTNeoXRewardModel) or isinstance(model, GPTNeoXForCausalLM):\n        model = model.gpt_neox\n\n    if isinstance(model, LlamaForCausalLM):\n        model = model.model\n\n    if isinstance(model, AutoModelForCausalLMWithHydraValueHead):\n        if isinstance(model.base_model, GPTNeoXForCausalLM):\n            model = model.base_model.gpt_neox\n        elif isinstance(model.base_model, LlamaForCausalLM):\n            model = model.base_model.model\n        else:\n            warnings.warn(\n                \"Unfortunately there is currently only support for NeoX models and LLaMa models \"\n                f\"Please make sure that `{model.__class__.__name__}` is one of those model.\\n\"\n                \"Or disable flash_attention and residual_dropout with:\\n\"\n                \"--use_flash_attention=false  --no-residual_dropout\"\n            )\n\n    if model.__class__.__name__ == \"RWForCausalLM\":\n        model = model.base_model\n\n    if isinstance(model, FalconForCausalLM):\n        model = model.transformer\n\n    attention_key_lookup = {\n        GPTNeoXModel: \"attention\",\n        GPTNeoXRewardModel: \"attention\",\n        LlamaModel: \"self_attn\",\n        FalconModel: \"self_attention\",\n    }\n    mlp_key_lookup = {\n        GPTNeoXModel: \"mlp\",\n        GPTNeoXRewardModel: \"mlp\",\n        LlamaModel: \"mlp\",\n        FalconModel: \"mlp\",\n    }\n    if isinstance(model, FalconModel) or model.__class__.__name__ == \"RWModel\":\n        layers = model.h\n        attention_key = \"self_attention\"\n        mlp_key = \"mlp\"\n    else:\n        layers = model.layers\n        attention_key = attention_key_lookup.get(model.__class__, \"attention\")\n        mlp_key = mlp_key_lookup.get(model.__class__, \"mlp\")\n    num_layers = len(layers)\n    resid_pdrop_last_layer = resid_pdrop\n    for i, layer in enumerate(layers):\n        if flash_attention:\n            add_flash_attn(getattr(layer, attention_key), causal=True)\n        if residual_dropout_lima:\n            resid_pdrop = i / (num_layers - 1) * resid_pdrop_last_layer\n        if resid_pdrop is not None and resid_pdrop > 0:\n            add_dropout(getattr(layer, attention_key), _patched_attn_forward, resid_pdrop)\n            add_dropout(getattr(layer, mlp_key), _patched_mlp_forward, resid_pdrop)\n\n\nclass RopePatch:\n    def __init__(self, model_name, **kwargs):\n        self.args = kwargs\n        self.rope_type = self.args.pop(\"type\")\n        config = AutoConfig.from_pretrained(model_name, trust_remote_code=True)\n        if hasattr(config, \"max_position_embeddings\"):\n            self.args[\"max_position_embeddings\"] = config.max_position_embeddings\n        if hasattr(config, \"base\"):\n            self.args[\"base\"] = config.base\n        architecture = config.architectures\n        if architecture:\n            self.model_name = architecture[0]\n            if \"FalconForCausalLM\" in architecture or \"RWForCausalLM\" in architecture:\n                self.architecture = \"FalconForCausalLM\"\n                if self.rope_type == \"ntk\":\n                    self.patch_fun = RWNTKScaledRope\n                else:\n                    raise NotImplementedError()\n            elif \"LlamaForCausalLM\" in architecture:\n                self.architecture = \"LlamaForCausalLM\"\n                if self.rope_type == \"linear\":\n                    self.patch_fun = LlamaLinearScalingRotaryEmbedding\n                elif self.rope_type == \"dynamic\":\n                    self.patch_fun = LlamaDynamicNTKScalingRotaryEmbedding\n                else:\n                    raise NotImplementedError()\n            else:\n                raise NotImplementedError()\n\n    @classmethod\n    def from_config(cls, config):\n        model_name = config.model_name\n        args = config.superhot_config\n        return cls(model_name, **args)\n\n    def patch(self, model):\n        if self.architecture == \"FalconForCausalLM\":\n            self.patch_falcon_model(model, **self.args)\n        elif self.architecture == \"LlamaForCausalLM\":\n            self.patch_llama_model(model, **self.args)\n        else:\n            raise NotImplementedError()\n\n    def patch_falcon_model(self, model, **kwargs):\n        for each in model.transformer.h:\n            each.self_attention.maybe_rotary = self.patch_fun(model.config.head_dim, **kwargs)\n\n    def patch_llama_model(self, model, **kwargs):\n        kwargs.update({\"device\": model.device})\n        for each in model.model.layers:\n            each.self_attn.rotary_emb = self.patch_fun(each.self_attn.head_dim, **kwargs)\n", "model/model_training/models/patching_utils.py": "import torch\nimport torch.nn.functional as F\n\n\ndef compute_flash_attention(flash_attn, q, k, v, attention_mask=None, head_mask=None):\n    # q, k, v: [bs, seq_len, num_attention_heads, attn_head_size]\n    # attention_mask (float): [bs, seq_len]\n    batch_size, max_len = q.size(0), q.size(1)\n\n    qkv = torch.stack([q, k, v], dim=2)\n    dtype_in = qkv.dtype\n    if dtype_in == torch.float32:\n        qkv = qkv.to(torch.float16)  # need to truncate in case input is fp32\n    cu_seqlens, max_seqlen = None, None\n\n    if attention_mask is None:\n        out = flash_attn(qkv, cu_seqlens=cu_seqlens, max_seqlen=max_seqlen)\n    else:\n        # Limitation: non-contiguous attention mask will not be handled correctly\n        # model will be able to pay attention between the first and last non-masked token, i.e. left- and right-side padding is supported.\n        csums = (attention_mask >= 0).cumsum(dim=1)\n        ends = csums.argmax(dim=1) + 1\n        starts = ends - csums.max(dim=1).values\n        seqlens = ends - starts\n\n        qkv = torch.cat([qkv[i, starts[i] : ends[i]] for i in range(batch_size)], dim=0)\n        zero = torch.zeros_like(seqlens[:1])  # torch.tensor([0]) with correct dtype and device\n        cu_seqlens = torch.cat([zero, seqlens.cumsum(dim=0)], dim=0).to(torch.int32)\n        max_seqlen = seqlens.max().item()\n\n        out = flash_attn(qkv, cu_seqlens=cu_seqlens, max_seqlen=max_seqlen)\n        # out: [num_unmasked_tokens, num_attention_heads, attn_head_size]\n\n        seqs = [out[start:end] for start, end in zip(cu_seqlens[:-1], cu_seqlens[1:])]\n        # stack and pad sequences together\n        padded_seqs = [\n            F.pad(seqs[i], (0, 0) * (seqs[i].dim() - 1) + (starts[i], max_len - ends[i]), value=0.0)\n            for i in range(batch_size)\n        ]\n        out = torch.stack(padded_seqs)\n\n    if out.dtype != dtype_in:\n        out = out.to(dtype_in)\n    return out\n\n\nif __name__ == \"__main__\":\n    from flash_attn.modules.mha import FlashSelfAttention\n\n    flash_attn = FlashSelfAttention(causal=True)\n\n    dtype = torch.float16\n    device = torch.device(\"cuda:0\")\n\n    batch_size, seq_len, num_heads, head_size = 4, 18, 8, 32\n    q = torch.randn(batch_size, seq_len, num_heads, head_size, dtype=dtype, device=device)\n    k = torch.randn(batch_size, seq_len, num_heads, head_size, dtype=dtype, device=device)\n    v = torch.randn(batch_size, seq_len, num_heads, head_size, dtype=dtype, device=device)\n\n    attn_mask = torch.randn(batch_size, seq_len, dtype=dtype, device=device).abs().cumsum(dim=1)\n    attn_mask = ((attn_mask > 3) & (attn_mask < 10)).int().log()\n\n    out = compute_flash_attention(flash_attn, q, k, v, attention_mask=attn_mask)\n", "model/model_training/models/gptj.py": "# Taken from https://github.com/sleekmike/Finetune_GPT-J_6B_8-bit/blob/master/gpt-j-6b-8-bit.py\n\nimport torch\nimport torch.nn.functional as F\nimport transformers\nfrom bitsandbytes.functional import dequantize_blockwise, quantize_blockwise\nfrom torch import nn\nfrom torch.cuda.amp import custom_bwd, custom_fwd\nfrom transformers import AutoModelForCausalLM\n\n\nclass FrozenBNBLinear(nn.Module):\n    def __init__(self, weight, absmax, code, bias=None):\n        assert isinstance(bias, nn.Parameter) or bias is None\n        super().__init__()\n        self.out_features, self.in_features = weight.shape\n        self.register_buffer(\"weight\", weight.requires_grad_(False))\n        self.register_buffer(\"absmax\", absmax.requires_grad_(False))\n        self.register_buffer(\"code\", code.requires_grad_(False))\n        self.adapter = None\n        self.bias = bias\n\n    def forward(self, input):\n        output = DequantizeAndLinear.apply(input, self.weight, self.absmax, self.code, self.bias)\n        if self.adapter:\n            output += self.adapter(input)\n        return output\n\n    @classmethod\n    def from_linear(cls, linear: nn.Linear) -> \"FrozenBNBLinear\":\n        weights_int8, state = quantize_blockise_lowmemory(linear.weight)\n        return cls(weights_int8, *state, linear.bias)\n\n    def __repr__(self):\n        return f\"{self.__class__.__name__}({self.in_features}, {self.out_features})\"\n\n\nclass DequantizeAndLinear(torch.autograd.Function):\n    @staticmethod\n    @custom_fwd\n    def forward(\n        ctx,\n        input: torch.Tensor,\n        weights_quantized: torch.ByteTensor,\n        absmax: torch.FloatTensor,\n        code: torch.FloatTensor,\n        bias: torch.FloatTensor,\n    ):\n        weights_deq = dequantize_blockwise(weights_quantized, absmax=absmax, code=code)\n        ctx.save_for_backward(input, weights_quantized, absmax, code)\n        ctx._has_bias = bias is not None\n        return F.linear(input, weights_deq, bias)\n\n    @staticmethod\n    @custom_bwd\n    def backward(ctx, grad_output: torch.Tensor):\n        assert not ctx.needs_input_grad[1] and not ctx.needs_input_grad[2] and not ctx.needs_input_grad[3]\n        input, weights_quantized, absmax, code = ctx.saved_tensors\n        # grad_output: [*batch, out_features]\n        weights_deq = dequantize_blockwise(weights_quantized, absmax=absmax, code=code)\n        grad_input = grad_output @ weights_deq\n        grad_bias = grad_output.flatten(0, -2).sum(dim=0) if ctx._has_bias else None\n        return grad_input, None, None, None, grad_bias\n\n\nclass FrozenBNBEmbedding(nn.Module):\n    def __init__(self, weight, absmax, code):\n        super().__init__()\n        self.num_embeddings, self.embedding_dim = weight.shape\n        self.register_buffer(\"weight\", weight.requires_grad_(False))\n        self.register_buffer(\"absmax\", absmax.requires_grad_(False))\n        self.register_buffer(\"code\", code.requires_grad_(False))\n        self.adapter = None\n\n    def forward(self, input, **kwargs):\n        with torch.no_grad():\n            # note: both quantized weights and input indices are *not* differentiable\n            weight_deq = dequantize_blockwise(self.weight, absmax=self.absmax, code=self.code)\n            output = F.embedding(input, weight_deq, **kwargs)\n        if self.adapter:\n            output += self.adapter(input)\n        return output\n\n    @classmethod\n    def from_embedding(cls, embedding: nn.Embedding) -> \"FrozenBNBEmbedding\":\n        weights_int8, state = quantize_blockise_lowmemory(embedding.weight)\n        return cls(weights_int8, *state)\n\n    def __repr__(self):\n        return f\"{self.__class__.__name__}({self.num_embeddings}, {self.embedding_dim})\"\n\n\ndef quantize_blockise_lowmemory(matrix: torch.Tensor, chunk_size: int = 2**20):\n    assert chunk_size % 4096 == 0\n    code = None\n    chunks = []\n    absmaxes = []\n    flat_tensor = matrix.view(-1)\n    for i in range((matrix.numel() - 1) // chunk_size + 1):\n        input_chunk = flat_tensor[i * chunk_size : (i + 1) * chunk_size].clone()\n        quantized_chunk, (absmax_chunk, code) = quantize_blockwise(input_chunk, code=code)\n        chunks.append(quantized_chunk)\n        absmaxes.append(absmax_chunk)\n\n    matrix_i8 = torch.cat(chunks).reshape_as(matrix)\n    absmax = torch.cat(absmaxes)\n    return matrix_i8, (absmax, code)\n\n\ndef convert_to_int8(model):\n    \"\"\"Convert linear and embedding modules to 8-bit with optional adapters\"\"\"\n    for module in list(model.modules()):\n        for name, child in module.named_children():\n            if isinstance(child, nn.Linear):\n                print(name, child)\n                setattr(\n                    module,\n                    name,\n                    FrozenBNBLinear(\n                        weight=torch.zeros(child.out_features, child.in_features, dtype=torch.uint8),\n                        absmax=torch.zeros((child.weight.numel() - 1) // 4096 + 1),\n                        code=torch.zeros(256),\n                        bias=child.bias,\n                    ),\n                )\n            elif isinstance(child, nn.Embedding):\n                setattr(\n                    module,\n                    name,\n                    FrozenBNBEmbedding(\n                        weight=torch.zeros(child.num_embeddings, child.embedding_dim, dtype=torch.uint8),\n                        absmax=torch.zeros((child.weight.numel() - 1) // 4096 + 1),\n                        code=torch.zeros(256),\n                    ),\n                )\n\n\nclass GPTJBlock(transformers.models.gptj.modeling_gptj.GPTJBlock):\n    def __init__(self, config):\n        super().__init__(config)\n\n        convert_to_int8(self.attn)\n        convert_to_int8(self.mlp)\n\n\nclass GPTJModel(transformers.models.gptj.modeling_gptj.GPTJModel):\n    def __init__(self, config):\n        super().__init__(config)\n        convert_to_int8(self)\n\n\nclass GPTJForCausalLM(transformers.models.gptj.modeling_gptj.GPTJForCausalLM):\n    def __init__(self, config):\n        super().__init__(config)\n        convert_to_int8(self)\n\n\ndef add_adapters(model, adapter_dim=16):\n    assert adapter_dim > 0\n\n    for module in model.modules():\n        if isinstance(module, FrozenBNBLinear):\n            module.adapter = nn.Sequential(\n                nn.Linear(module.in_features, adapter_dim, bias=False),\n                nn.Linear(adapter_dim, module.out_features, bias=False),\n            )\n            nn.init.zeros_(module.adapter[1].weight)\n        elif isinstance(module, FrozenBNBEmbedding):\n            module.adapter = nn.Sequential(\n                nn.Embedding(module.num_embeddings, adapter_dim),\n                nn.Linear(adapter_dim, module.embedding_dim, bias=False),\n            )\n            nn.init.zeros_(module.adapter[1].weight)\n\n\ndef get_model(model_name, cache_dir, quantization):\n    if quantization is None:\n        model = AutoModelForCausalLM.from_pretrained(model_name, cache_dir=cache_dir)\n    elif quantization == \"8bit\":\n        raise ValueError(\"Loading 8-bit model. Use deepspeed instead.\")\n        transformers.models.gptj.modeling_gptj.GPTJBlock = GPTJBlock\n        model = AutoModelForCausalLM.from_pretrained(model_name, cache_dir=cache_dir)\n        add_adapters(model)\n    else:\n        raise ValueError(f\"Unknown quantization {quantization}\")\n\n    return model\n", "model/model_training/models/__init__.py": "import transformers\n\n\ndef freeze_top_n_layers(model, target_layers):\n    # its possible we can simply detect which module is a ModuleList\n    # and simply freeze the module without doing string parsing\n    for name, param in model.named_parameters():\n        if \"embed\" in name:\n            param.requires_grad = False\n        elif \".layer\" in name or \".h.\" in name:\n            tokens = name.split(\".\")\n            layer_ = None\n            for token in tokens:\n                if token.isdigit():\n                    layer_ = int(token)\n                    break\n            if layer_ is not None and layer_ < target_layers:\n                # print('freeze ', layer_, name)\n                param.requires_grad = False\n    return model\n\n\ndef get_specific_model(\n    model_name,\n    seq2seqmodel=False,\n    without_head=False,\n    cache_dir=\".cache\",\n    quantization=False,\n    **kwargs,\n):\n    if without_head:\n        model = transformers.AutoModel.from_pretrained(model_name, cache_dir=cache_dir, **kwargs)\n    elif seq2seqmodel:\n        # encoder-decoder support for Flan-T5 like models\n        model = transformers.AutoModelForSeq2SeqLM.from_pretrained(model_name, cache_dir=cache_dir, **kwargs)\n    else:\n        if \"falcon-7b\" in model_name:\n            # temporary hack until tiiuae/falcon-7b uses the transformer's Falcon impl by default\n            # in-library PR was reverted https://huggingface.co/tiiuae/falcon-7b/commit/378337427557d1df3e742264a2901a49f25d4eb1\n            model = transformers.models.falcon.modeling_falcon.FalconForCausalLM.from_pretrained(\n                model_name, cache_dir=cache_dir, **kwargs\n            )\n        else:\n            if \"falcon\" in model_name:\n                kwargs[\"trust_remote_code\"] = True\n            model = transformers.AutoModelForCausalLM.from_pretrained(model_name, cache_dir=cache_dir, **kwargs)\n    return model\n", "model/model_training/tools/model_chat.py": "#!/usr/bin/env python3\n\"\"\"\n\nA very simple script to test model locally\n\n\n\"\"\"\nimport argparse\nfrom enum import Enum\nfrom typing import List, Tuple\n\nimport torch\nfrom model_training.custom_datasets.formatting import QA_SPECIAL_TOKENS\nfrom model_training.utils.utils import _strtobool\nfrom tokenizers import pre_tokenizers\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\n\nclass ChatRole(str, Enum):\n    system = \"<|system|>\"\n    prompter = \"<|prompter|>\"\n    assistant = \"<|assistant|>\"\n\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\"--model_path\", type=str, required=True)\nparser.add_argument(\"--bot_name\", type=str, default=\"Joi\", help=\"Use this when your format isn't in OA format\")\nparser.add_argument(\"--format\", type=str, default=\"v2\")\nparser.add_argument(\"--max_new_tokens\", type=int, default=200)\nparser.add_argument(\"--top_k\", type=int, default=40)\nparser.add_argument(\"--temperature\", type=float, default=1.0)\nparser.add_argument(\"--do-sample\", type=_strtobool, default=True)\nparser.add_argument(\"--per-digit-tokens\", action=\"store_true\")\nargs = parser.parse_args()\n\nbot_name: str = args.bot_name\nmodel_name: str = args.model_path\nmethod: str = args.format\n\n\ndef talk(human_input: str, history: List[Tuple[str, str]], sep_token: str, prefix=\"\"):\n    histories = []\n    if method == \"v2\":\n        prefix = \"<prefix>You are a helpful assistant called Joi trained by OpenAssistant on large corpus of data, you will now help user to answer the question as concise as possible</prefix>\"\n        for question, answer in history:\n            histories.append(\n                \"{}{}{}{}\".format(QA_SPECIAL_TOKENS[\"Question\"], question, QA_SPECIAL_TOKENS[\"Answer\"], answer)\n            )\n        if len(histories) > 0:\n            prefix += sep_token.join(histories)\n            # add sep at the end\n            prefix += sep_token\n        prefix += \"{}{}{}\".format(QA_SPECIAL_TOKENS[\"Question\"], human_input, QA_SPECIAL_TOKENS[\"Answer\"])\n    # elif method == \"v3\":\n    #     personality = \"You are a helpful assistant called Joi, you are a smart and helpful bot.\"\n    #     prefix = f\"{SeqToken.begin}{ChatRole.system}{SeqToken.delimiter}{personality}{SeqToken.end}\"\n    #     for question, answer in history:\n    #         histories.append(\n    #             f\"{SeqToken.begin}{ChatRole.prompter}{SeqToken.delimiter}{question}{SeqToken.end}\"\n    #             + f\"{SeqToken.begin}{ChatRole.assistant}{SeqToken.delimiter}{answer}{SeqToken.end}\"\n    #         )\n    #     if len(histories) > 0:\n    #         prefix += \"\".join(histories)\n    #         # add sep at the end\n    #     prefix += f\"{SeqToken.begin}{ChatRole.prompter}{SeqToken.delimiter}{human_input}{SeqToken.end}{SeqToken.begin}{ChatRole.assistant}{SeqToken.delimiter}\"\n    elif method == \"v2.5\":\n        # personality = \"You are a helpful assistant called Joi, you are a smart and helpful bot.\"\n        # prefix = f\"{ChatRole.system}{personality}{SeqToken.end}\"\n        for question, answer in history:\n            histories.append(\n                # f\"{ChatRole.prompter}{question}{SeqToken.end}\" + f\"{ChatRole.assistant}{answer}{SeqToken.end}\"\n                f\"{ChatRole.prompter}{question}</s>\"\n                + f\"{ChatRole.assistant}{answer}</s>\"\n            )\n        if len(histories) > 0:\n            prefix += \"\".join(histories)\n            # add sep at the end\n        prefix += f\"{ChatRole.prompter}{human_input}</s>{ChatRole.assistant}\"\n    else:\n        for question, answer in history:\n            histories.append(\"User: \" + question + \"\\n\\n{}: \".format(bot_name) + answer + \"\\n\")\n        if len(histories) > 0:\n            prefix += \"\\n\".join(histories)\n        prefix += \"\\nUser: \" + human_input + \"\\n\\n{}: \".format(bot_name)\n\n    return prefix\n\n\ndef process_output(output, method, bot_name):\n    if method == \"v2\":\n        answer = output.split(QA_SPECIAL_TOKENS[\"Answer\"])[-1]\n        answer = answer.split(\"</s>\")[0].replace(\"<|endoftext|>\", \"\").lstrip().split(QA_SPECIAL_TOKENS[\"Answer\"])[0]\n    elif method == \"v2.5\":\n        answer = output.split(f\"{ChatRole.assistant}\")[-1]\n        # answer = answer.split(\"</s>\")[0].replace(SeqToken.end, \"\").lstrip()\n    # elif method == \"v3\":\n    #     answer = output.split(f\"{SeqToken.begin}{ChatRole.assistant}{SeqToken.delimiter}\")[-1]\n    #     answer = answer.split(\"</s>\")[0].replace(SeqToken.end, \"\").lstrip()\n    else:\n        answer = output.split(\"\\n\\n{}:\".format(bot_name))[-1]\n        answer = answer.split(\"</s>\")[0].replace(\"<|endoftext|>\", \"\").lstrip().split(\"\\n\\n{}:\".format(bot_name))[0]\n    return answer\n\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nif method != \"v2\":\n    tokenizer.add_special_tokens({\"pad_token\": \"<|endoftext|>\"})\nmodel = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16)\n\nmodel.eval().cuda()\n\nif args.per_digit_tokens:\n    tokenizer._tokenizer.pre_processor = pre_tokenizers.Digits(True)\n\nmodel = AutoModelForCausalLM.from_pretrained(model_name).half().eval().cuda()\n\nif __name__ == \"__main__\":\n    histories = []\n    prefix = \"\"\n    while True:\n        print(\">\", end=\" \")\n        try:\n            prompt = input()\n        except (EOFError, KeyboardInterrupt):  # Catch ctrl+d and ctrl+c respectively\n            print()\n            break\n        if prompt == \"!reset\":\n            histories = []\n        else:\n            input_text = talk(prompt, histories, prefix)\n            inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True).to(0)\n            if \"token_type_ids\" in inputs:\n                del inputs[\"token_type_ids\"]\n            outputs = model.generate(\n                **inputs,\n                early_stopping=True,\n                max_new_tokens=args.max_new_tokens,\n                do_sample=args.do_sample,\n                top_k=args.top_k,\n                temperature=args.temperature,\n                pad_token_id=tokenizer.eos_token_id,\n            )\n            output = tokenizer.decode(outputs[0], truncate_before_pattern=[r\"\\n\\n^#\", \"^'''\", \"\\n\\n\\n\"])\n            reply = process_output(output, method, bot_name)\n\n            if len(reply) != 0:\n                print(reply)\n                histories.append((prompt, reply))\n            else:\n                print(\"empty token\")\n", "model/model_training/tools/export_model.py": "import argparse\nimport sys\n\nimport model_training.models.reward_model  # noqa: F401 make sure reward model is registered for AutoModel\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoModelForSequenceClassification, AutoTokenizer\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"model_name\", type=str, help=\"checkpoint path or model name\")\n    parser.add_argument(\"--dtype\", type=str, default=\"fp16\", help=\"fp16, bf16 or fp32\")\n    parser.add_argument(\"--hf_repo_name\", type=str, help=\"Huggingface repository name\")\n    parser.add_argument(\"--auth_token\", type=str, help=\"User access token\")\n    parser.add_argument(\"--output_folder\", type=str, help=\"output folder path\")\n    parser.add_argument(\"--max_shard_size\", type=str, default=\"10GB\")\n    parser.add_argument(\"--cache_dir\", type=str)\n    parser.add_argument(\"--reward_model\", action=\"store_true\", default=False)\n    parser.add_argument(\"--rl_checkpoint\", type=str, help=\"load RL fine-tuning checkpoint\")\n    parser.add_argument(\n        \"--rope_scaling_type\", type=str, help=\"set rope scaling type (linear, dynamic)\", default=\"linear\"\n    )\n    parser.add_argument(\"--rope_scaling_factor\", type=float, help=\"set rope scaling factor (float >1.0)\")\n    parser.add_argument(\n        \"--trust_remote_code\",\n        action=\"store_true\",\n        default=False,\n        help=\"allow custom model code (required for Falcon)\",\n    )\n    return parser.parse_args()\n\n\ndef main():\n    args = parse_args()\n    print(args)\n\n    if args.dtype in (\"float16\", \"fp16\"):\n        torch_dtype = torch.float16\n    elif args.dtype in (\"float32\", \"fp32\"):\n        torch_dtype = torch.float32\n    elif args.dtype in (\"bfloat16\", \"bf16\"):\n        torch_dtype = torch.bfloat16\n    else:\n        print(f\"Unsupported dtype: {args.dtype}\")\n        sys.exit(1)\n\n    if not args.hf_repo_name and not args.output_folder:\n        print(\n            \"Please specify either `--hf_repo_name` to push to HF or `--output_folder` \"\n            \"to export the model to a local folder.\"\n        )\n        sys.exit(1)\n\n    print(f\"Loading tokenizer '{args.model_name}' ...\")\n    tokenizer = AutoTokenizer.from_pretrained(args.model_name)\n    print(f\"{type(tokenizer).__name__} (vocab_size={len(tokenizer)})\")\n\n    print(f\"Loading model '{args.model_name}' ({args.dtype}) ...\")\n\n    if args.rl_checkpoint:\n        model = AutoModelForCausalLM.from_pretrained(args.model_name, torch_dtype=torch_dtype, cache_dir=args.cache_dir)\n\n        print(f\"Loading RL checkpoint: {args.rl_checkpoint}...\")\n        checkpoint_state = torch.load(args.rl_checkpoint, map_location=\"cpu\")[\"module\"]\n\n        # drop parameters of value head\n        for param_name in (\"v_head.0.weight\", \"v_head.0.bias\", \"v_head.2.weight\", \"v_head.2.bias\"):\n            checkpoint_state.pop(param_name, None)\n\n        # resolve inconsistencies in the vocab size\n        target_size = checkpoint_state[list(filter(lambda x: \"embed\" in x, list(checkpoint_state.keys())))[0]].shape[0]\n        model.resize_token_embeddings(target_size)\n\n        print(model.load_state_dict(checkpoint_state))\n\n    elif args.reward_model:\n        model = AutoModelForSequenceClassification.from_pretrained(\n            args.model_name, torch_dtype=torch_dtype, cache_dir=args.cache_dir\n        )\n    else:\n        model = AutoModelForCausalLM.from_pretrained(\n            args.model_name,\n            torch_dtype=torch_dtype,\n            cache_dir=args.cache_dir,\n            trust_remote_code=args.trust_remote_code,\n        )\n    print(f\"{type(model).__name__} (num_parameters={model.num_parameters()})\")\n\n    print(\"Model architecture:\")\n    print(model)\n\n    if args.rope_scaling_type is not None and args.rope_scaling_factor is not None:\n        assert args.rope_scaling_type in (\"linear\", \"dynamic\")\n        assert args.rope_scaling_factor >= 1.0\n        rope_scaling = {\"type\": args.rope_scaling_type, \"factor\": args.rope_scaling_factor}\n        print(f\"setting new rope_scaling config: {rope_scaling} (old: {model.config.rope_scaling})\")\n        model.config.rope_scaling = rope_scaling\n\n    if args.output_folder:\n        print(f\"Saving model to: {args.output_folder}\")\n        model.save_pretrained(args.output_folder, max_shard_size=args.max_shard_size)\n\n        print(f\"Saving tokenizer to: {args.output_folder}\")\n        tokenizer.save_pretrained(args.output_folder)\n\n    if args.hf_repo_name:\n        print(\"Uploading model to HF...\")\n        model.push_to_hub(args.hf_repo_name, use_auth_token=args.auth_token, max_shard_size=args.max_shard_size)\n\n        print(\"Uploading tokenizer to HF...\")\n        tokenizer.push_to_hub(args.hf_repo_name, use_auth_token=args.auth_token)\n\n\nif __name__ == \"__main__\":\n    main()\n", "model/model_training/tools/check_oasst_export.py": "import argparse\n\nfrom oasst_data import ExportMessageTree, read_message_tree_list, visit_messages_depth_first\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"input_file_path\", type=str, help=\".jsonl or jsonl.gz OA export\")\n    parser.add_argument(\"--lang\", type=str, help=\"comma separated list of lang-codes\")\n    args = parser.parse_args()\n    return args\n\n\ndef main():\n    args = parse_args()\n    lang_codes = None\n    if args.lang:\n        lang_codes = args.lang.split(\",\")\n\n    print(f\"input file: {args.input_file_path}\")\n\n    if lang_codes is None:\n        print(\"Using languages: all\")\n    else:\n        print(f'Filtering languages: {\", \".join(lang_codes)}')\n\n    def tree_filter(tree: ExportMessageTree) -> bool:\n        return (\n            tree.tree_state == \"ready_for_export\"\n            and tree.prompt.review_result\n            and (lang_codes is None or tree.prompt.lang in lang_codes)\n        )\n\n    trees = read_message_tree_list(args.input_file_path, filter=tree_filter)\n    print(f\"{len(trees)} trees\")\n\n    all_messages = []\n    for t in trees:\n        visit_messages_depth_first(t.prompt, all_messages.append)\n    synthetic_messages = [m for m in all_messages if m.synthetic]\n    prompter_messages = [m for m in all_messages if m.role == \"prompter\"]\n    assistant_messages = [m for m in all_messages if m.role == \"assistant\"]\n\n    print(f\"{len(all_messages)} messages\")\n    print(f\"{len(synthetic_messages)} synthetic messages\")\n    print(f\"{len(prompter_messages)} prompter messages\")\n    print(f\"{len(assistant_messages)} assistant messages\")\n\n    prompter_with_replies = [m for m in prompter_messages if m.replies and len(m.replies) > 1]\n    print(f\"{len(prompter_with_replies)} prompter messages with >1 reply\")\n\n    prompter_with_replies_ranked = [\n        m\n        for m in prompter_messages\n        if m.replies and len([rm for rm in m.replies if rm.rank is not None and rm.rank >= 0]) > 1\n    ]\n    print(f\"{len(prompter_with_replies_ranked)} prompter messages with >1 ranked reply\")\n\n\nif __name__ == \"__main__\":\n    main()\n", "model/model_training/tools/sample_rm_data.py": "\"\"\"\n    Recursive method to traverse down the conversation tree\n\n    Use fastlangid for language identification :\n    >> pip install fastlangid\n\n\"\"\"\nimport glob\nimport json\nimport random\nimport sys\nfrom collections import defaultdict\nfrom copy import deepcopy\n\nfrom fastlangid.langid import LID\n\nlangid = LID()\ntotal_ranks = []\ntarget_file = sys.argv[1]\n\nattributes = [\n    \"message_id\",\n    \"parent_id\",\n    \"text\",\n    \"role\",\n    \"lang\",\n    \"review_count\",\n    \"rank\",\n    \"synthetic\",\n    \"model_name\",\n    \"emojis\",\n]\n\n\ndef rank_replies(replies):\n    return sorted(replies, key=lambda x: x[\"rank\"])\n\n\ndef expand_nodes(tree):\n    all_convo = []\n\n    def traverse_tree(tree, root):\n        if len(tree[\"replies\"]) == 0:\n            all_convo.append(root)\n            return\n\n        for reply in tree[\"replies\"]:\n            new_root = deepcopy(root)\n            new_root.append({attr: reply[attr] for attr in attributes if attr in reply})\n            traverse_tree(reply, new_root)\n\n    init_root = [{attr: tree[attr] for attr in attributes if attr in tree}]\n    traverse_tree(tree, init_root)\n    return all_convo\n\n\ndef extract_all_pair_rank(tree):\n    pairs = []\n\n    def traverse_tree(tree, root):\n        if len(tree[\"replies\"]) == 0:\n            return\n\n        if tree[\"role\"] == \"prompter\" and len(tree[\"replies\"]) > 1:\n            available_reply = [{attr: r[attr] for attr in attributes if attr in r} for r in tree[\"replies\"]]\n            pairs.append((root, available_reply))\n\n        for reply in tree[\"replies\"]:\n            new_root = deepcopy(root)\n            new_root.append({attr: reply[attr] for attr in attributes if attr in reply})\n            traverse_tree(reply, new_root)\n\n    init_root = [{attr: tree[attr] for attr in attributes if attr in tree}]\n    traverse_tree(tree, init_root)\n    return pairs\n\n\ndef viz_convo(conversation):\n    for text in conversation:\n        print(text[\"role\"], \":\", text[\"text\"])\n\n\ndef calculate_total_threads(_target_file):\n    total = 0\n    lang_stats = defaultdict(int)\n    with open(_target_file, \"r\") as f:\n        print(_target_file)\n        for line in f:\n            row = json.loads(line)\n            seed_prompt = row[\"prompt\"]\n            all_convo = expand_nodes(row[\"prompt\"])\n            for convo in all_convo:\n                for convo_ in convo:\n                    lang = langid.predict(convo_[\"text\"])\n                    lang_stats[lang] += 1\n\n            if len(all_convo) > 1:\n                total += len(all_convo)\n            assert seed_prompt[\"role\"] == \"prompter\"\n    print(total)\n    print(lang_stats)\n\n\ndef process_context(convo):\n    if len(convo) == 1:\n        return {\"prompt\": convo[0][\"text\"], \"history\": []}\n    last_prompt = convo[-1]\n    convo.pop(-1)\n    history_pair = []\n    for idx in range(0, len(convo), 2):\n        history_pair.append((convo[idx][\"text\"], convo[idx + 1][\"text\"]))\n\n    return {\"prompt\": last_prompt[\"text\"], \"history\": history_pair}\n\n\nif __name__ == \"__main__\":\n    calculate_total_threads(target_file)\n\n    usable_rank = 0\n    response_with_rank = 0\n    RM_dataset = []\n    with open(target_file, \"r\") as f:\n        for line in f:\n            row = json.loads(line)\n            seed_prompt = row[\"prompt\"]\n            initial = seed_prompt[\"text\"]\n            all_convo = extract_all_pair_rank(row[\"prompt\"])\n            for convo, replies in all_convo:\n                if len(replies) > 1:\n                    prefix = process_context(convo)\n                    if \"rank\" not in replies[0]:\n                        continue\n                    elif replies[0][\"rank\"] is not None:\n                        for r in replies:\n                            if \"rank\" in r and r[\"rank\"] is None:\n                                r[\"rank\"] = 5\n                            elif \"rank\" not in r:\n                                r[\"rank\"] = 5\n                        replies = sorted(replies, key=lambda x: x[\"rank\"])\n                        pos_reply = replies[0][\"text\"]\n                        neg_replies = [r[\"text\"] for r in replies[1:]]\n\n                        RM_dataset.append(\n                            {\n                                \"prompt\": prefix[\"prompt\"],\n                                \"history\": prefix[\"history\"],\n                                \"pos\": pos_reply,\n                                \"neg_replies\": neg_replies,\n                            }\n                        )\n\n                        response_with_rank += 1\n\n            usable_rank += len(all_convo)\n\n    print(len(RM_dataset), usable_rank)\n\n    key_sets = set()\n    for rm_jsonl in glob.glob(\"rm_*.jsonl\"):\n        with open(rm_jsonl, \"r\") as f:\n            for line in f:\n                data = json.loads(line)\n                key = \"{}-{}-{}\".format(data[\"prompt\"], data[\"pos\"], \"\".join(data[\"neg_replies\"]))\n                key_sets.add(key)\n\n    new_dataset = []\n    for data in RM_dataset:\n        key = \"{}-{}-{}\".format(data[\"prompt\"], data[\"pos\"], \"\".join(data[\"neg_replies\"]))\n        if key not in key_sets:\n            new_dataset.append(data)\n\n    with open(\"rm_new.jsonl\", \"w\") as f:\n        for row in new_dataset:\n            f.write(json.dumps(row) + \"\\n\")\n\n    random.shuffle(RM_dataset)\n    train_flag = int(len(RM_dataset) * 0.8)\n    test_flag = int(len(RM_dataset) * 0.9)\n    train, test, val = RM_dataset[:train_flag], RM_dataset[train_flag:test_flag], RM_dataset[test_flag:]\n    with open(\"rm_train.jsonl\", \"w\") as f:\n        for row in train:\n            f.write(json.dumps(row) + \"\\n\")\n    with open(\"rm_test.jsonl\", \"w\") as f:\n        for row in test:\n            f.write(json.dumps(row) + \"\\n\")\n    with open(\"rm_val.jsonl\", \"w\") as f:\n        for row in val:\n            f.write(json.dumps(row) + \"\\n\")\n", "model/model_training/tools/model_cli.py": "#!/usr/bin/env python3\nimport argparse\nimport time\n\nimport torch\nimport transformers\nfrom model_training.custom_datasets.formatting import QA_SPECIAL_TOKENS, format_pairs, format_system_prefix\nfrom model_training.models import get_specific_model\nfrom model_training.utils.utils import _strtobool\nfrom tokenizers import pre_tokenizers\n\nif __name__ == \"__main__\":\n    import warnings\n\n    warnings.filterwarnings(\"ignore\")\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--model_path\", type=str, required=True)\n    parser.add_argument(\"--max_new_tokens\", type=int, default=200)\n    parser.add_argument(\"--top_k\", type=int, default=None)\n    parser.add_argument(\"--top_p\", type=int, default=0.9)\n    parser.add_argument(\"--temperature\", type=float, default=0.8)\n    parser.add_argument(\"--do-sample\", type=_strtobool, default=True)\n    parser.add_argument(\"--format\", type=str, default=\"v2\")\n    parser.add_argument(\"--8bit\", action=\"store_true\", dest=\"eightbit\")\n    parser.add_argument(\"--system_prefix\", type=str, default=None)\n    parser.add_argument(\"--cache_dir\", type=str, default=None)\n    parser.add_argument(\"--per-digit-tokens\", action=\"store_true\")\n    parser.add_argument(\"--load_checkpoint\", type=str, default=None)\n    args = parser.parse_args()\n\n    if args.load_checkpoint is not None:\n        print(\"Loading from\", args.load_checkpoint)\n        # \"ckpts_save/best_checkpoint/pytorch_model/mp_rank_00_model_states.pt\"\n        ckpt = torch.load(args.load_checkpoint)\n        import IPython\n\n        IPython.embed()\n        model = get_specific_model(args.model_path, torch_dtype=torch.float16, cache_dir=args.cache_dir)\n\n        base_dict = {k[11:]: v for k, v in ckpt[\"module\"].items() if not k.startswith(\"base_model.transformer\")}\n        # base_dict = {k[11:]: v for k, v in ckpt['module'].items()}\n        print(model.load_state_dict(base_dict, strict=False))\n    else:\n        if args.eightbit:\n            model = get_specific_model(\n                args.model_path,\n                load_in_8bit=True,\n                device_map=\"auto\",\n                low_cpu_mem_usage=True,\n                torch_dtype=torch.float16,\n                offload_state_dict=True,\n                cache_dir=args.cache_dir,\n            )\n        else:\n            model = get_specific_model(args.model_path, cache_dir=args.cache_dir, torch_dtype=torch.float16)\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model = model.to(device)\n\n    model.gradient_checkpointing_enable()  # reduce number of stored activations\n    tokenizer = transformers.AutoTokenizer.from_pretrained(args.model_path)\n    # tokenizer = transformers.AutoTokenizer.from_pretrained(\"dvruette/oasst-pythia-6.9b-4000-steps\")\n    if args.per_digit_tokens:\n        tokenizer._tokenizer.pre_processor = pre_tokenizers.Digits(True)\n\n    human_token_id = tokenizer.additional_special_tokens_ids[\n        tokenizer.additional_special_tokens.index(QA_SPECIAL_TOKENS[\"Question\"])\n    ]\n\n    print('Type \"quit\" to exit')\n    print(\"Press Control + C to restart conversation (spam to exit)\")\n\n    conversation_history = []\n\n    while True:\n        try:\n            user_input = input(\"User: \")\n            if user_input == \"quit\":\n                break\n\n            conversation_history.append(user_input)\n\n            batch = tokenizer.encode(\n                format_system_prefix(args.system_prefix, tokenizer.eos_token)\n                if args.system_prefix\n                else \"\"\n                + \"\".join(format_pairs(conversation_history, tokenizer.eos_token, add_initial_reply_token=True)),\n                return_tensors=\"pt\",\n            )\n\n            with torch.cuda.amp.autocast():\n                out = model.generate(\n                    input_ids=batch.to(model.device),\n                    min_new_tokens=4,\n                    max_new_tokens=args.max_new_tokens,\n                    do_sample=args.do_sample,\n                    top_k=args.top_k,\n                    top_p=args.top_p,\n                    temperature=args.temperature,\n                    eos_token_id=tokenizer.eos_token_id,\n                    pad_token_id=tokenizer.eos_token_id,\n                )\n\n            if out[0][-1] == tokenizer.eos_token_id:\n                response = out[0][:-1]\n            else:\n                response = out[0]\n\n            response = tokenizer.decode(response).split(QA_SPECIAL_TOKENS[\"Answer\"])[-1]\n            print(f\"Bot: {response}\")\n            conversation_history.append(response)\n        except KeyboardInterrupt:\n            conversation_history = []\n            print()\n            print(\"Conversation restarted\")\n            time.sleep(1)\n            continue\n        except EOFError:  # Catch ctrl+d\n            print()\n            break\n", "model/model_training/tools/augment_oasst.py": "\"\"\"\n    Augment oasst dataset with sft generated results\n\n    You can use augment new response using a model with bad response, ie non SFT model\n\n    had to do this in a quick fashion, please tolerate the hackiness in the code\n\n\"\"\"\nimport json\nimport os\n\n# so far load_oasst_export is pretty deterministic in thread order\n# means the train, val split stay the same\nfrom model_training.custom_datasets.oasst_dataset import load_oasst_export\nfrom model_training.models.reward_model import GPTNeoXRewardModel\nfrom tqdm import tqdm\nfrom transformers import AutoModelForCausalLM, AutoModelForSequenceClassification, AutoTokenizer\n\n\nclass AggregateResults:\n    def __init__(self, reward_model) -> None:\n        if \"pythia\" in reward_model:\n            rank_model = GPTNeoXRewardModel.from_pretrained(reward_model)\n        else:\n            rank_model = AutoModelForSequenceClassification.from_pretrained(reward_model)\n        self.rank_tokenizer = AutoTokenizer.from_pretrained(reward_model)\n        self.rank_model = rank_model.half().cuda()\n\n    def scoring(self, prefixes, answer):\n        question = self.rank_tokenizer.sep_token.join(prefixes)\n        inputs = self.rank_tokenizer(question, answer, return_tensors=\"pt\").to(0)\n        score = self.rank_model(**inputs).logits[0].cpu().detach()\n        return score\n\n    def aggregate(self, jsonl_filenames, dataset, split=\"val\"):\n        augmented = {}\n        for train_augmented_filename in jsonl_filenames:\n            with open(train_augmented_filename, \"r\") as f:\n                for line in tqdm(f):\n                    payload = json.loads(line)\n                    idx = payload[\"idx\"]\n                    if idx not in augmented:\n                        augmented[idx] = []\n                    if len(payload[\"gen_samples\"]) == 0:\n                        continue\n                    try:\n                        scores = [\n                            (float(self.scoring(payload[\"prefixes\"], sample)), sample)\n                            for sample in payload[\"gen_samples\"]\n                        ]\n                        sorted_scores = sorted(scores, key=lambda x: x[0], reverse=True)\n                        augmented[idx].append(sorted_scores[0][1])\n                    except RuntimeError as e:\n                        print(e)\n                        continue\n\n        with open(f\"augmented_cycliric_oasst_2023-03-27_{split}.jsonl\", \"w\") as f:\n            for idx, payload in tqdm(enumerate(dataset), total=len(dataset), dynamic_ncols=True):\n                output = {\n                    \"prefixes\": payload[0],\n                    \"responses\": payload[1],\n                    \"augmented\": [],\n                    \"split\": split,\n                }\n                if idx in augmented:\n                    augmented = augmented[idx]\n                    cleaned_aug = []\n                    for a in augmented:\n                        cleaned = (\n                            a.replace(\"<|endoftext|>\", \"\")\n                            .replace(\"<|startoftoken|>human\\n\", \"\")\n                            .replace(\"<human>\", \"\")\n                            .replace(\"<bot>\", \"\")\n                        )\n                        cleaned_aug.append(cleaned)\n                    output[\"augmented\"] = cleaned_aug\n                f.write(json.dumps(output) + \"\\n\")\n\n\ndef r2_conversation(prefixes, tokenizer, model, top_k=10, temperature=0.7, max_new_tokens=512, model_name=\"\"):\n    text = \"\"\n    for idx, convo in enumerate(prefixes):\n        if idx % 2 == 0:\n            text += \"<|startoftoken|>human\\n\" + convo + \"<|endoftoken|>\"\n        else:\n            text += \"<|startoftoken|>assistant\\n\" + convo + \"<|endoftoken|>\"\n    input_text = text + \"<|startoftoken|>assistant\\n\"\n    inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True).to(0)\n\n    generated_samples = []\n    try:\n        outputs = model.generate(\n            **inputs,\n            early_stopping=False,\n            max_new_tokens=max_new_tokens,\n            num_return_sequences=top_k,\n            do_sample=True,\n            temperature=temperature,\n            pad_token_id=tokenizer.eos_token_id,\n            # dialogue_collator.py line 36\n        )\n        gen_sequences = outputs.sequences[:, inputs[\"input_ids\"].shape[-1] :]\n        for output in gen_sequences:\n            decoded = tokenizer.decode(\n                output, truncate_before_pattern=[r\"\\n\\n^#\", \"^'''\", \"\\n\\n\\n\"], skip_special_tokens=True\n            )\n            answer = decoded.split(\"<|endoftext|>\")[0]\n            if len(answer) > 0:\n                generated_samples.append(answer)\n    except RuntimeError as err:\n        print(err)\n\n    return generated_samples\n\n\ndef r0_conversation(prefixes, tokenizer, model, top_k=10, temperature=0.7, max_new_tokens=512, model_name=\"\"):\n    text = \"\"\n    for idx, convo in enumerate(prefixes):\n        if idx % 2 == 0:\n            text += \"<human>\" + convo\n        else:\n            text += \"<bot>\" + convo + \"<|endoftoken|>\"\n    input_text = text + \"<bot>\"\n    inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True).to(0)\n\n    generated_samples = []\n    try:\n        outputs = model.generate(\n            **inputs,\n            early_stopping=False,\n            max_new_tokens=max_new_tokens,\n            num_return_sequences=top_k,\n            do_sample=True,\n            temperature=temperature,\n            pad_token_id=tokenizer.eos_token_id,\n            # dialogue_collator.py line 36\n        )\n        gen_sequences = outputs.sequences[:, inputs[\"input_ids\"].shape[-1] :]\n        for output in gen_sequences:\n            decoded = tokenizer.decode(\n                output, truncate_before_pattern=[r\"\\n\\n^#\", \"^'''\", \"\\n\\n\\n\"], skip_special_tokens=True\n            )\n            answer = decoded.split(\"<|endoftext|>\")[0]\n            if len(answer) > 0:\n                generated_samples.append(answer)\n    except RuntimeError as err:\n        print(err)\n\n    return generated_samples\n\n\ndef rallio_conversation(prefixes, tokenizer, model, top_k=2, temperature=0.7, max_new_tokens=512, model_name=\"Chip2\"):\n    name = \"Chip2\"\n    if \"Chip2\" in model_name:\n        name = \"Chip2\"\n    elif \"Kitt\" in model_name:\n        name = \"Kitt\"\n\n    text = \"\"\n    for idx, convo in enumerate(prefixes):\n        if idx % 2 == 0:\n            text += \"User: \" + convo + \"\\n\"\n        else:\n            text += name + \": \" + convo + \"\\n\"\n    input_text = text + name + \": \"\n    inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True).to(0)\n\n    generated_samples = []\n    try:\n        outputs = model.generate(\n            **inputs,\n            early_stopping=False,\n            max_new_tokens=max_new_tokens,\n            do_sample=True,\n            pad_token_id=tokenizer.eos_token_id,\n            num_return_sequences=top_k,\n            top_p=0.95,\n            temperature=0.5,\n            penalty_alpha=0.6,\n            output_scores=True,\n            return_dict_in_generate=True,\n            repetition_penalty=1.03,\n            use_cache=True\n            # dialogue_collator.py line 36\n        )\n        gen_sequences = outputs.sequences[:, inputs[\"input_ids\"].shape[-1] :]\n        for output in gen_sequences:\n            decoded = tokenizer.decode(\n                output, truncate_before_pattern=[r\"\\n\\n^#\", \"^'''\", \"\\n\\n\\n\"], skip_special_tokens=True\n            )\n            answer = decoded.split(\"<|endoftext|>\")[0]\n            if len(answer) > 0:\n                generated_samples.append(answer)\n    except (RuntimeError, ValueError) as e:\n        print(e)\n\n    return generated_samples\n\n\ndef augment_conversation(model_name, dataset, split=\"train\"):\n    if \"-r2\" in model_name:  # OAI format\n        chat_handler = r2_conversation\n    elif \"Rallio\" in model_name:\n        chat_handler = rallio_conversation\n    else:  # <human>, <bot>\n        chat_handler = r0_conversation\n    chat_handler = r2_conversation\n\n    model = AutoModelForCausalLM.from_pretrained(model_name, cache_dir=\".cache/\").eval().half().cuda()\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    output_file = \"{}_2023-03-27-all_{}_{}.jsonl\".format(model_name.replace(\"/\", \"-\"), languages, split)\n    added = set()\n    if os.path.exists(output_file):\n        with open(output_file, \"r\") as f:\n            for line in f:\n                row = json.loads(line)\n                added.add(row[\"idx\"])\n    with open(output_file, \"a\") as fout:\n        for idx, row in tqdm(enumerate(dataset), total=len(dataset), dynamic_ncols=True):\n            if idx in added:\n                continue\n            prefixes, answers = row\n            samples = chat_handler(\n                prefixes, tokenizer, model, temperature=0.1, top_k=8, max_new_tokens=256, model_name=model_name\n            )\n            fout.write(\n                json.dumps({\"prefixes\": prefixes, \"answers\": answers, \"gen_samples\": samples, \"idx\": idx}) + \"\\n\"\n            )\n            fout.flush()\n\n\nif __name__ == \"__main__\":\n    import glob\n\n    # model_name = 'bigscience/bloom-560m'\n    model_name = \"theblackcat102/pythia-1b-deduped-sft\"\n    # latin_cyrillic\n    languages = \"bg,ca,cs,da,de,en,es,fr,hr,hu,it,nl,pl,pt,ro,ru,sl,sr,sv,uk\"\n    train, val = load_oasst_export(\".cache/2023-03-27_oasst_research_all.jsonl.gz\", lang=languages, mode=\"rm\")\n\n    print(len(train), len(val))\n    augment_conversation(model_name, train, split=\"train\")\n    augment_conversation(model_name, val, split=\"val\")\n\n    agg = AggregateResults(\"theblackcat102/reward-model-deberta-v3-base-v2\")\n    agg.aggregate(glob.glob(\"*_val.jsonl\"), val, \"val\")\n    agg.aggregate(glob.glob(\"*_train.jsonl\"), train, \"train\")\n", "model/model_training/utils/losses.py": "import torch\nimport torch.nn.functional as F\nfrom torch import nn\n\n\nclass CrossEntropyLoss(nn.CrossEntropyLoss):\n    def __init__(self, weight=None, size_average=None, ignore_index=-100, reduce=None, reduction=\"mean\"):\n        super().__init__(weight, size_average, ignore_index, reduce, \"none\")\n        self._reduction = reduction\n\n    def forward(self, input, target, mask=None):\n        input = input.view(-1, input.size(-1))\n        target = target.view(-1)\n\n        if mask is not None:\n            mask = mask.view(-1).bool()\n            input = input[mask]\n            target = target[mask]\n\n        size = target.numel()\n\n        loss = super().forward(input, target)\n\n        if self._reduction == \"none\":\n            return loss\n        return loss.sum() / (size + 1e-8)\n\n\nclass PolyLoss(nn.Module):\n    def __init__(self, weight=None, size_average=None, ignore_index=-100, reduce=None, reduction=\"mean\", epsilon=1.0):\n        super().__init__()\n        self.weight = torch.tensor(weight)\n        self.ignore_index = ignore_index\n        self.reduction = reduction\n        self.cross_entropy = CrossEntropyLoss(weight, size_average, ignore_index, reduce, \"none\")\n        self.epsilon = epsilon\n\n    def forward(self, input, target, mask=None):\n        if mask is not None:\n            mask = mask.view(-1).bool()\n            input = input.view(-1, input.size(-1))\n            target = target.view(-1)\n            input = input[mask]\n            target = target[mask]\n\n        onehot_target = F.one_hot(target, num_classes=input.size(-1)).to(device=input.device, dtype=input.dtype)\n        pt = torch.sum(onehot_target * F.softmax(input, -1), -1)\n        CE = self.cross_entropy(input, target)\n        poly1 = CE + self.epsilon * (1 - pt)\n        if self.reduction == \"mean\":\n            poly1 = poly1.mean()\n        elif self.reduction == \"sum\":\n            poly1 = poly1.sum()\n        return poly1\n\n\nclass RMLoss(nn.Module):\n    def __init__(self, reduction=\"mean\", beta=0.001):\n        super().__init__()\n        self.reduction = reduction\n        self.beta = beta\n\n    def forward(self, logits, cu_lengths=None):\n        # if cu_lengths is None, assume that all examples belong to the same conversation\n        if cu_lengths is None:\n            cu_lengths = [0, logits.size(0)]\n\n        device = logits.device\n        losses = []\n        for start, end in zip(cu_lengths[:-1], cu_lengths[1:]):\n            pairs = torch.combinations(torch.arange(end - start, device=device), 2)\n            pos_ids, neg_ids = pairs[:, 0], pairs[:, 1]\n            pos_logits = logits.take(start + pos_ids)\n            neg_logits = logits.take(start + neg_ids)\n\n            l2 = 0.5 * (pos_logits**2 + neg_logits**2)\n            _loss = (-F.logsigmoid(pos_logits - neg_logits) + self.beta * l2).mean()\n            losses.append(_loss)\n        loss = torch.stack(losses)\n\n        if self.reduction == \"none\":\n            return loss\n        return loss.mean()\n\n\nclass RMCLSLoss(nn.CrossEntropyLoss):\n    def __init__(self, weight=None, size_average=None, ignore_index=-100, reduce=None, reduction=\"mean\"):\n        super().__init__(weight, size_average, ignore_index, reduce, \"none\")\n        self._reduction = reduction\n\n    def forward(self, logits, cu_lengths=None):\n        # if cu_lengths is None, assume that all examples belong to the same conversation\n        if cu_lengths is None:\n            cu_lengths = [0, logits.size(0)]\n\n        device = logits.device\n        logit_pairs = []\n        # aggregate combination between ranks\n        for start, end in zip(cu_lengths[:-1], cu_lengths[1:]):\n            pairs = torch.combinations(torch.arange(end - start, device=device), 2)\n            pos_ids, neg_ids = pairs[:, 0], pairs[:, 1]\n            pos_logits = logits.take(start + pos_ids)\n            neg_logits = logits.take(start + neg_ids)\n            merged = torch.stack((pos_logits, neg_logits), dim=1)\n            logit_pairs.append(merged)\n        logit_pairs = torch.concat(logit_pairs, dim=0)\n        labels = torch.zeros(logit_pairs.shape[0], dtype=torch.long, device=device)\n        loss = super().forward(logit_pairs, labels)\n\n        if self._reduction == \"none\":\n            return loss\n        return loss.mean()\n", "model/model_training/utils/utils.py": "import argparse\nimport copy\nimport math\nimport random\nfrom distutils.util import strtobool\nfrom pathlib import Path\nfrom typing import List, NamedTuple\n\nimport evaluate\nimport torch\nimport transformers\nimport yaml\nfrom model_training.custom_datasets import get_one_dataset\nfrom model_training.custom_datasets.formatting import QA_SPECIAL_TOKENS\nfrom model_training.models import freeze_top_n_layers, get_specific_model\nfrom model_training.models.patching import patch_model\nfrom model_training.models.prefix_llama import LlamaForCausalLM\nfrom model_training.models.reward_model import GPTNeoXRewardModel\nfrom sklearn.model_selection import train_test_split\nfrom tokenizers import pre_tokenizers\nfrom torch.utils.data import ConcatDataset, Dataset, Subset\nfrom torch.utils.data.distributed import DistributedSampler\n\nfrom .losses import CrossEntropyLoss, PolyLoss, RMCLSLoss, RMLoss\n\n\ndef _strtobool(x):\n    return bool(strtobool(x))\n\n\ndef init_rng(conf: argparse.Namespace) -> None:\n    seed = conf.rng_seed\n    if seed is not None:\n        print(f\"RNG seed: {seed}\")\n        transformers.set_seed(seed)\n\n\nclass PerDatasetSampler(DistributedSampler):\n    \"\"\"Sampler which returns a fixed number of samples per dataset, per epoch.\n\n    Example:\n\n    Dataset 1 has 10,000 examples and we want 200 per epoch\n    Dataset 2 has 500 examples and we want all 500 per epoch\n\n    Epoch size will be 700 and every epoch we'll sample a different\n    200 from dataset 1.\n\n    Parameters\n    ----------\n    dataset_sizes : List[int]\n        A list with the size of each dataset.\n    dataset_size_per_epoch : List[int]\n        How many examples to get from each dataset per epoch.\n\n    Note: dataset_sizes & dataset_size_per_epoch must be in the same order.\n    Further the examples in the underlying torch.utils.data.Dataset\n    must per ordered as dataset_1, dataset_2, ..., dataset_n. This is fine\n    if we concatenate a bunch of datasets together\n    e.g. using torch.utils.data.ConcatDataset which is current behaviour.\n    \"\"\"\n\n    def __init__(\n        self,\n        dataset_sizes: List[int],\n        dataset_size_per_epoch: List[int],\n        rank: int = None,\n        world_size: int = None,\n        shuffle: bool = True,\n        seed: int = 0,\n        samples_length: List[int] = None,\n    ):\n        \"\"\"\n        if samples_length is not None, then the sampler\n        will order the samples by dataset length\n        with some variability across epochs\n        \"\"\"\n        self.dataset_sizes = dataset_sizes\n        self.dataset_size_per_epoch = dataset_size_per_epoch\n        self.num_datasets = len(dataset_sizes)\n        self.shuffle = shuffle\n        self.rank = rank\n        self.world_size = world_size\n        self.epoch = 0\n\n        if world_size == 1:\n            self.rank = 0\n\n        self.num_samples = sum(dataset_size_per_epoch)\n        self.seed = seed\n        self.samples_length = samples_length\n\n    def set_epoch(self, epoch: int) -> None:\n        self.epoch = epoch\n\n    def __len__(self) -> int:\n        return self.num_samples // self.world_size\n\n    def __iter__(self):\n        epoch_idx = []\n        n = 0\n\n        random.seed(self.epoch + self.seed)\n\n        for i in range(self.num_datasets):\n            sampled_idx = random.sample(range(n, self.dataset_sizes[i] + n), self.dataset_size_per_epoch[i])\n            n += self.dataset_sizes[i]\n            epoch_idx.extend(sampled_idx)\n\n        if self.samples_length is not None:\n            # sort by samples length and in case of ties randomize\n            epoch_idx = sorted(epoch_idx, key=lambda x: (self.samples_length[x], random.random()))\n\n            if self.shuffle:\n                # do some minor shuffling to avoid repeating the same order\n                # but not too much to avoid too much padding\n                # quasi random basically\n                for i in range(0, len(epoch_idx), 200):  # this should be batch_size dependent\n                    random.shuffle(epoch_idx[i : i + 200])\n        else:\n            if self.shuffle:\n                random.shuffle(epoch_idx)\n\n        # split epoch_idx in world_size chunks\n        epoch_idx = epoch_idx[self.rank : self.num_samples : self.world_size]\n\n        return iter(epoch_idx)\n\n    @classmethod\n    def build_sampler_from_config(cls, training_conf, datasets: List[Dataset], verbose: bool = False, **kwargs):\n        dataset_sizes = [len(x) for x in datasets]\n        fractions = get_dataset_fractions(training_conf.datasets, dataset_sizes, verbose)\n        dataset_size_per_epoch = [int(size * frac) for size, frac in zip(dataset_sizes, fractions)]\n        seed = training_conf.rng_seed\n        return cls(dataset_sizes=dataset_sizes, dataset_size_per_epoch=dataset_size_per_epoch, seed=seed, **kwargs)\n\n\ndef get_dataset_fractions(conf, dataset_sizes: List[int], verbose: bool = False):\n    \"\"\"Calculate fraction of each dataset to use per epoch when sub-sampling\"\"\"\n\n    if verbose:\n        print(\"Creating sampler for datasets:\")\n\n    fractions = []\n    for i, data_config in enumerate(conf):\n        dataset_name, _ = get_dataset_name_and_kwargs_from_data_config(data_config)\n        if isinstance(data_config, dict):\n            if \"fraction\" in data_config[dataset_name]:\n                if data_config[dataset_name][\"fraction\"] <= 0:\n                    raise ValueError(\"Please specify fraction as a value between 0 < fraction <= 1\")\n                fractions.append(min(1, data_config[dataset_name][\"fraction\"]))\n            elif \"size\" in data_config[dataset_name]:\n                if data_config[dataset_name][\"size\"] > dataset_sizes[i]:\n                    raise ValueError(f\"Please specify a size smaller than number of examples: {dataset_sizes[i]:,.0f}\")\n                fractions.append(data_config[dataset_name][\"size\"] / dataset_sizes[i])\n            else:\n                fractions.append(1)\n        else:\n            fractions.append(1)\n\n        if verbose:\n            print(f\"{dataset_name}: {fractions[-1]:.2%} ({int(dataset_sizes[i]*fractions[-1])})\")\n    return fractions\n\n\nclass SpecialTokens(NamedTuple):\n    pad_token: str = \"\"\n    eos_token: str = \"\"\n    sep_token: str = \"\"\n\n\nclass TokenizerConfig(NamedTuple):\n    special_tokens: SpecialTokens = {}\n\n\nTOKENIZER_CONFIGS = {\n    \"galactica\": TokenizerConfig(special_tokens=SpecialTokens(\"<pad>\", \"</s>\")),\n    \"GPT-JT\": TokenizerConfig(special_tokens=SpecialTokens(sep_token=\"<|extratoken_100|>\")),\n    \"codegen\": TokenizerConfig(special_tokens=SpecialTokens(\"<|endoftext|>\", sep_token=\"<|endoftext|>\")),\n    \"pythia\": TokenizerConfig(special_tokens=SpecialTokens(\"<|padding|>\", \"<|endoftext|>\", \"<|endoftext|>\")),\n    \"gpt-neox\": TokenizerConfig(special_tokens=SpecialTokens(\"<|padding|>\", \"<|endoftext|>\", \"<|endoftext|>\")),\n    \"llama\": TokenizerConfig(special_tokens=SpecialTokens(\"</s>\", \"</s>\", sep_token=\"<s>\")),\n    \"cerebras\": TokenizerConfig(special_tokens=SpecialTokens(\"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\")),\n    \"deberta-v3\": TokenizerConfig(special_tokens=SpecialTokens(\"[PAD]\", \"[SEP]\", sep_token=\"[CLS]\")),\n    \"bloom\": TokenizerConfig(special_tokens=SpecialTokens(\"<pad>\", \"</s>\", \"<s>\")),\n    \"electra\": TokenizerConfig(special_tokens=SpecialTokens(\"[PAD]\", \"[SEP]\", sep_token=\"[CLS]\")),\n    \"falcon\": TokenizerConfig(\n        special_tokens=SpecialTokens(\"<|endoftext|>\", \"<|endoftext|>\", sep_token=\"<|endoftext|>\")\n    ),\n    \"LLongMA\": TokenizerConfig(special_tokens=SpecialTokens(\"</s>\", \"</s>\", sep_token=\"<s>\")),\n}\n\n\ndef match_tokenizer_name(model_name: str) -> TokenizerConfig:\n    \"\"\"\n    Match a partial model name to a tokenizer configuration\n    i.e. model_name `Salesforce/codegen-2B-multi` has config name `codegen`\n    \"\"\"\n    tokenizer_config_matches = [config for name, config in TOKENIZER_CONFIGS.items() if name in model_name]\n    if not tokenizer_config_matches:\n        raise ValueError(f\"Cannot find any tokeniser configuration to match {model_name=}\")\n    elif 1 < len(tokenizer_config_matches):\n        raise ValueError(f\"Found multiple tokeniser configuration matches for {model_name=}\")\n    else:\n        return tokenizer_config_matches[0]\n\n\ndef get_tokenizer(conf) -> transformers.AutoTokenizer:\n    tokenizer_name = conf.model_name\n\n    if \"cerebras\" in conf.model_name:\n        # Only 13B has a tokenizer available on HF\n        tokenizer_name = \"cerebras/Cerebras-GPT-13B\"\n\n    tokenizer = transformers.AutoTokenizer.from_pretrained(tokenizer_name, cache_dir=conf.cache_dir)\n\n    tokenizer_config = match_tokenizer_name(conf.model_name)\n\n    if hasattr(conf, \"per_digit_tokens\") and conf.per_digit_tokens:\n        tokenizer._tokenizer.pre_processor = pre_tokenizers.Digits(True)\n\n    if tokenizer_config.special_tokens:\n        if \"GPT-JT\" in conf.model_name:\n            tokenizer_config.special_tokens.pad_token = tokenizer.eos_token\n        # SpecialTokens : latest in 4.25, 4.26\n        tokenizer.add_special_tokens(\n            {\n                \"pad_token\": tokenizer_config.special_tokens.pad_token,\n                \"eos_token\": tokenizer_config.special_tokens.eos_token,\n                \"sep_token\": tokenizer_config.special_tokens.sep_token,\n            }\n        )\n\n    additional_special_tokens = (\n        []\n        if \"additional_special_tokens\" not in tokenizer.special_tokens_map\n        else tokenizer.special_tokens_map[\"additional_special_tokens\"]\n    )\n    additional_special_tokens = list(set(additional_special_tokens + list(QA_SPECIAL_TOKENS.values())))\n\n    tokenizer.add_special_tokens({\"additional_special_tokens\": additional_special_tokens})\n\n    return tokenizer\n\n\ndef default_preprocess(eval_pred, ignote_negative_labels=True):\n    preds, labels = eval_pred.predictions, eval_pred.label_ids\n\n    if not ignote_negative_labels:\n        return preds, labels\n\n    mask = labels > 0\n    return preds[mask], labels[mask]\n\n\n# placeholder for now\ndef preprocess_qa(eval_pred):\n    return (eval_pred.predictions, eval_pred.label_ids)\n\n\n# def postprocess_summarization(preds, labels):\n#     preds = [pred.strip() for pred in preds]\n#     labels = [label.strip() for label in labels]\n\n#     preds = [\"\\n\".join(nltk.sent_tokenize(pred)) for pred in preds]\n#     labels = [\"\\n\".join(nltk.sent_tokenize(label)) for label in labels]\n\n#     return preds, labels\n\n\n# def preprocess_summarization(eval_pred, tokenizer, ignore_pad_token_for_loss=True):\n#     preds, labels = eval_pred\n#     decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n#     if ignore_pad_token_for_loss:\n#         labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n#     decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n\n#     decoded_preds, decoded_labels = postprocess_summarization(decoded_preds, decoded_labels)\n#     return decoded_preds, decoded_labels\n\n\ndef get_metrics(conf, tokenizer):\n    # the reason behind using a list is that we might want to extend the list of our\n    # metrics in the future for more thorough evaluation\n    metrics, preprocess_fns = [evaluate.load(\"accuracy\")], [default_preprocess]\n\n    # if any(dataset in QA_DATASETS for dataset in conf.datasets):\n    #     raise ValueError(\"TODO\")\n    #     metrics.append(evaluate.load(\"squad_v2\"))\n    #     preprocess_fns.append(preprocess_qa)\n    # if any(dataset in SUMMARIZATION_DATASETS for dataset in conf.datasets):\n    #     raise ValueError(\"TODO\")\n    #     metrics.append(evaluate.load(\"rouge\"))\n    #     preprocess_fns.append(\n    #         partial(preprocess_summarization, tokenizer, ignore_pad_token_for_loss=conf.ignore_pad_token_for_loss)\n    #     )\n\n    return metrics, preprocess_fns\n\n\ndef get_model(conf, tokenizer, pad_vocab_size_to_multiple_of=16, check_freeze_layer=True):\n    dtype = torch.float32\n    if conf.dtype in [\"fp16\", \"float16\"]:\n        dtype = torch.float16\n    elif conf.dtype in [\"bf16\", \"bfloat16\"]:\n        dtype = torch.bfloat16\n\n    if conf.is_reward_model:\n        if \"pythia\" in conf.model_name:\n            model = GPTNeoXRewardModel.from_pretrained(conf.model_name, cache_dir=conf.cache_dir, torch_dtype=dtype)\n\n            if conf.pooling:\n                assert conf.pooling in (\"mean\", \"last\"), f\"invalid pooling configuration '{conf.pooling}'\"\n                model.config.pooling = conf.pooling\n        else:\n            model = transformers.AutoModelForSequenceClassification.from_pretrained(\n                conf.model_name, cache_dir=conf.cache_dir, num_labels=1, torch_dtype=dtype\n            )\n    if not conf.is_reward_model:\n        if conf.peft_type is not None and conf.peft_type == \"prefix-tuning\" and \"llama\" in conf.model_name:\n            model = LlamaForCausalLM.from_pretrained(conf.model_name, cache_dir=conf.cache_dir, torch_dtype=dtype)\n        else:\n            model = get_specific_model(\n                conf.model_name,\n                cache_dir=conf.cache_dir,\n                quantization=conf.quantization,\n                seq2seqmodel=conf.seq2seqmodel,\n                without_head=conf.is_reward_model,\n                torch_dtype=dtype,\n            )\n\n        n_embs = model.get_input_embeddings().num_embeddings\n        if len(tokenizer) != n_embs or pad_vocab_size_to_multiple_of:\n            p = pad_vocab_size_to_multiple_of\n            target_size = len(tokenizer) if not p else math.ceil(len(tokenizer) / p) * p\n            print(\"Resizing embeddings to\", target_size)\n            model.resize_token_embeddings(target_size)\n\n        new_n_embs = model.get_input_embeddings().num_embeddings\n        if new_n_embs != n_embs and check_freeze_layer:\n            assert not conf.freeze_layer, \"Cannot change the number of embeddings if the model is frozen.\"\n\n        if conf.freeze_layer:\n            model = freeze_top_n_layers(model, conf.freeze_layer)\n\n    model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n    params = sum([p.numel() for p in model_parameters])\n    print(\"Number of trainable parameters: {}M\".format(int(params / 1e6)))\n\n    patch_model(\n        model,\n        resid_pdrop=conf.residual_dropout,\n        flash_attention=conf.use_flash_attention,\n        residual_dropout_lima=conf.residual_dropout_lima,\n    )\n\n    return model\n\n\ndef get_dataset_name_and_kwargs_from_data_config(data_config):\n    if isinstance(data_config, dict):\n        name = list(data_config.keys())[0]\n\n        # first copy the dict, then remove the size and fraction\n        kwargs = copy.deepcopy(data_config[name])\n\n        kwargs.pop(\"fraction\", None)\n        kwargs.pop(\"size\", None)\n        return name, kwargs\n    else:\n        return data_config, {}\n\n\ndef get_dataset(\n    conf,\n    mode: str = \"sft\",\n) -> tuple[ConcatDataset, dict[str, Subset]]:\n    train_datasets, evals = [], {}\n\n    for data_config in conf.datasets + conf.datasets_extra:\n        dataset_name, kwargs = get_dataset_name_and_kwargs_from_data_config(data_config)\n        train, val = get_one_dataset(conf, dataset_name, mode=mode, **kwargs)\n        train_datasets.append(train)\n\n        if val is not None:\n            evals[dataset_name] = Subset(val, list(range(min(len(val), conf.eval_size)))) if conf.eval_size else val\n\n    train = ConcatDataset(train_datasets)\n\n    return train, evals\n\n\ndef get_loss(loss, poly_eps: float = 1.0, score_l2_reg: float = 0.001):\n    if loss == \"CrossEntropyLoss\":\n        return CrossEntropyLoss()\n    elif loss == \"Poly\":\n        return PolyLoss(epsilon=poly_eps)\n    elif loss == \"RMLoss\":\n        return RMLoss(beta=score_l2_reg)\n    elif loss == \"RMCLSLoss\":\n        return RMCLSLoss()\n    else:\n        raise ValueError(f\"Loss {loss} not supported\")\n\n\ndef read_yamls(dir):\n    conf = {}\n    no_conf = True\n\n    for config_file in Path(dir).glob(\"**/*.yaml\"):\n        no_conf = False\n        with config_file.open(\"r\") as f:\n            conf.update(yaml.safe_load(f))\n\n    if no_conf:\n        print(f\"WARNING: No yaml files found in {dir}\")\n\n    return conf\n\n\ndef train_val_dataset(dataset, val_split=0.2):\n    if val_split == 0:\n        return dataset, None\n\n    train_idx, val_idx = train_test_split(\n        list(range(len(dataset))), test_size=val_split, random_state=666, shuffle=True\n    )\n    return Subset(dataset, train_idx), Subset(dataset, val_idx)\n\n\ndef process_output(output: str, method: str = \"v2\", bot_name: str = \"Joi\") -> str:\n    if method == \"v2\":\n        answer = output.split(QA_SPECIAL_TOKENS[\"Answer\"])[-1]\n        answer = answer.split(\"</s>\")[0].replace(\"<|endoftext|>\", \"\").lstrip().split(QA_SPECIAL_TOKENS[\"Answer\"])[0]\n    else:\n        answer = output.split(\"\\n\\n{}:\".format(bot_name))[-1]\n        answer = answer.split(\"</s>\")[0].replace(\"<|endoftext|>\", \"\").lstrip().split(\"\\n\\n{}:\".format(bot_name))[0]\n    return answer\n\n\ndef merge_dicts(default: dict, config: dict):\n    \"\"\"\n    merge default dict with config dict to override params\n    \"\"\"\n    for k, v in default.items():\n        if k not in config.keys():\n            config.update({k: v})\n\n    return config\n\n\ndef get_all_linear_layers(model):\n    cls = torch.nn.Linear\n\n    modules = {name.split(\".\")[-1] for name, module in model.named_modules() if isinstance(module, cls)}\n    if \"lm_head\" in modules:\n        modules.remove(\"lm_head\")\n\n    return list(modules)\n", "model/model_training/utils/ppo_utils.py": "import json\nimport math\nimport os\nimport warnings\nfrom time import time\nfrom typing import List, Tuple\n\nimport numpy as np\nimport torch\n\n# import torch.distributed as dist\nimport tritonclient.grpc as client_util\nimport trlx.utils.logging as logging\nfrom huggingface_hub import hf_hub_download\n\n# from torch import nn\nfrom torch.nn import functional as F\nfrom torch.utils.data import DataLoader\nfrom transformers import AutoTokenizer, DataCollatorWithPadding, PreTrainedTokenizer\nfrom trlx.data.ppo_types import PPORLElement\nfrom trlx.models.modeling_ppo import AutoModelForCausalLMWithHydraValueHead\nfrom trlx.pipeline import BasePipeline, register_datapipeline\nfrom trlx.trainer import register_trainer\nfrom trlx.trainer.accelerate_base_trainer import AccelerateRLTrainer\nfrom trlx.trainer.accelerate_ppo_trainer import AcceleratePPOTrainer\nfrom trlx.utils import Clock\nfrom trlx.utils.modeling import logprobs_of_labels\nfrom utils.utils import get_model\n\nfrom .utils_rl import prepare_tensor\n\nlogger = logging.get_logger(__name__)\n\n\nclass CustomCausalLMHydraWithValueHead(AutoModelForCausalLMWithHydraValueHead):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n    @classmethod\n    def from_pretrained(cls, config, tokenizer, kwargs=None, revision=None):  # noqa: max-complexity\n        \"\"\"\n        Our custom loader that just modifies the loading of the base model so that patching and other stuff are supported.\n        \"\"\"\n\n        # We may have modified the tokenizer to add the pad token\n        # Since we are decoding avoid pad the vocabulary as this will lead to undefined tokens for the tokenizer\n\n        # we only added a pad token, no need to check that embeddings are trained\n\n        # TODO change freeze_layer parameter here ..\n        base_model = get_model(config, tokenizer, pad_vocab_size_to_multiple_of=1, check_freeze_layer=False)\n\n        # DEBUG check if generation is working properly\n        # original_device = base_model.device\n        # base_model.cuda()\n        # tokens = tokenizer(\"<|prompter|>Can you explain to me how tides work?</s><|assistant|>\", add_special_tokens=False, return_tensors=\"pt\")\n        # output = base_model.generate(tokens.to(base_model.device)[\"input_ids\"], max_new_tokens=16, do_sample=False)\n        # print(tokenizer.decode(output[0]))\n        # base_model.to(original_device)\n\n        # print('Trainable parameters:')\n        # for name, param in base_model.named_parameters():\n        #     if param.requires_grad:\n        #         print(name)\n\n        # if config.ds_zero3:\n        #     print('Overriding model._get_logits_processor')\n        #     # always generate based on the max length. For Zero3 DS avoid getting stuck...\n        #     funcType = type(base_model._get_logits_processor)\n        #     base_model._get_logits_processor = funcType(_get_logits_processor, base_model)\n        #     funcType = type(base_model.sample)\n        #     base_model.sample = funcType(sample, base_model)\n\n        # model.ds_zero3 = config.ds_zero3\n        model = cls(base_model, num_layers_unfrozen=config.num_layers_unfrozen)\n\n        pretrained_model_name_or_path = config.model_name\n\n        if isinstance(pretrained_model_name_or_path, str):\n            filename = os.path.join(pretrained_model_name_or_path, \"pytorch_model.bin\")\n            sharded_index_filename = os.path.join(pretrained_model_name_or_path, \"pytorch_model.bin.index.json\")\n            is_sharded = False\n\n            if not os.path.exists(filename):\n                try:\n                    filename = hf_hub_download(pretrained_model_name_or_path, \"pytorch_model.bin\", revision=revision)\n                # Sharded\n                except Exception:\n                    if os.path.exists(sharded_index_filename):\n                        index_file_name = sharded_index_filename\n                    else:\n                        index_file_name = hf_hub_download(\n                            pretrained_model_name_or_path,\n                            \"pytorch_model.bin.index.json\",\n                            revision=revision,\n                        )\n                    with open(index_file_name, \"r\") as f:\n                        index = json.load(f)\n                    # Collect files containing weights from supported modules\n                    files_to_download = set()\n                    for k, v in index[\"weight_map\"].items():\n                        if any([module in k for module in cls._supported_modules]):\n                            files_to_download.add(v)\n                    is_sharded = True\n\n            if is_sharded:\n                # Merge each shard into a state dict\n                # TODO: Optimize this to avoid wasting RAM\n                state_dict = {}\n                for shard_file in files_to_download:\n                    filename = os.path.join(pretrained_model_name_or_path, shard_file)\n                    # Download if shard file doesn't exist locally\n                    if not os.path.exists(filename):\n                        filename = hf_hub_download(pretrained_model_name_or_path, shard_file, revision=revision)\n                    state_dict.update(torch.load(filename, map_location=\"cpu\"))\n            else:\n                state_dict = torch.load(filename, map_location=\"cpu\")\n        else:\n            state_dict = pretrained_model_name_or_path.state_dict()\n\n        model.post_init(state_dict=state_dict)\n        return model\n\n\n@register_trainer\nclass CustomPPOTrainer(AcceleratePPOTrainer, AccelerateRLTrainer):\n    def __init__(self, config, *args, **kwargs):\n        # hm...\n        self.tokenizer = AutoTokenizer.from_pretrained(\n            config.tokenizer.tokenizer_path\n        )  # Loading our model requires the tokenizer to be loaded first\n        # if pad token id is same as escape token id, then add a new token at the end of the vocab\n        if self.tokenizer.pad_token_id == self.tokenizer.eos_token_id:\n            self.tokenizer.add_special_tokens({\"pad_token\": \"<|padding|>\"})\n\n        # self.tokenizer.pad_token = self.tokenizer.eos_token\n        # self.tokenizer.pad_token_id = self.tokenizer.eos_token_id\n\n        self.tokenizer.padding_side = config.tokenizer.padding_side\n        self.tokenizer.truncation_side = config.tokenizer.truncation_side\n\n        print(\"len self.tokenizer\", len(self.tokenizer))\n\n        # print('len tokenizer', len(self.tokenizer))\n\n        super().__init__(*args, config=config, **kwargs)\n\n        # del self.ref_model\n        self.ref_model = triton_server_ref_model()\n\n    def decode(\n        self,\n        prompts: List[torch.LongTensor],\n        samples: List[torch.LongTensor],\n        prompt_sizes: torch.LongTensor = None,\n        append_eos_token: bool = True,\n    ) -> Tuple[List[str], List[str], List[str]]:\n        \"\"\"\n        Decode tensor generations into lists of strings (`samples`: List[str], `prompts`: List[str], `outputs`: List[str])\n        \"\"\"\n        assert append_eos_token is True\n\n        if prompt_sizes is None:\n            # Assuming prompts were left-padded\n            prompt_sizes = [prompts.shape[1]] * len(prompts)\n\n        str_samples, str_prompts, str_outputs = [], [], []\n\n        for prompt, sample, prompt_size in zip(prompts, samples, prompt_sizes):\n            if self.config.model.model_arch_type == \"seq2seq\":\n                raise NotImplementedError(\"Decoding for seq2seq models is not implemented yet\")\n                output_start_ix = 0\n            else:\n                output_start_ix = prompt_size\n\n            # Skip the padding token but not the other special tokens\n            PAD_TOKEN_ID = self.tokenizer.pad_token_id\n\n            if not torch.is_tensor(sample):\n                sample = torch.tensor(sample)\n\n            if not torch.is_tensor(prompt):\n                prompt = torch.tensor(prompt)\n\n            str_prompt = self.tokenizer.decode(\n                prompt[:prompt_size][prompt[:prompt_size] != PAD_TOKEN_ID], skip_special_tokens=False\n            )\n            # str_prompt = str_prompt.replace(PAD_TOKEN, \"\")\n\n            str_output = self.tokenizer.decode(\n                sample[output_start_ix:][sample[output_start_ix:] != PAD_TOKEN_ID], skip_special_tokens=False\n            )\n            # print('sample', self.tokenizer.decode(sample))\n            # print('prompt', self.tokenizer.decode(prompt))\n            # str_output = str_output.replace(PAD_TOKEN, \"\")\n\n            trimmed = False\n            # Trim outputs up to `self.stop_sequences` if any are present\n            if self.stop_sequences:\n                for stop in self.stop_sequences:\n                    stop_ix = str_output.find(stop)\n                    if stop_ix >= 0:\n                        str_output = str_output[:stop_ix].rstrip()\n                        trimmed = True\n\n            # Recover the last <eos> if it was present in the original sample\n            # or add one if it was trimmed with `self.stop_sequences`.\n            # Only in cases when a generation ended due to `max_new_tokens` exhaustion,\n            # <eos> token would not be present in the original sample\n            if append_eos_token and (trimmed or sample[-1] != self.tokenizer.eos_token_id):\n                str_output += self.tokenizer.eos_token\n\n            str_prompts.append(str_prompt)\n            str_outputs.append(str_output)\n\n            if self.config.model.model_arch_type == \"seq2seq\":\n                sample = str_prompt + self.tokenizer.sep_token + str_output\n            else:\n                sample = str_prompt + str_output\n\n            str_samples.append(sample)\n\n        return str_samples, str_prompts, str_outputs\n\n    def get_arch(self, config):\n        if config.model.model_arch_type == \"seq2seq\":\n            raise NotImplementedError(\"Seq2Seq models are not implemented yet\")\n            # model = Seq2SeqLMHydraWithValueHead(config.model.model_path, config.model.num_layers_unfrozen)\n        else:\n            model = CustomCausalLMHydraWithValueHead.from_pretrained(config.sft_config, self.tokenizer)\n\n        return model\n\n    def generate(self, input_ids, *args, **kwargs):\n        # if self.model.ds_zero3:\n        #     max_new_tokens = self.config.method.gen_kwargs['max_new_tokens']\n\n        #     if self.generate_experience_kwargs is not None:\n        #         if 'max_length' in self.generate_experience_kwargs:\n        #             self.generate_experience_kwargs.pop('max_length')\n\n        #         self.generate_experience_kwargs['max_new_tokens'] = max_new_tokens\n        #         self.generate_experience_kwargs['min_new_tokens'] = max_new_tokens\n        #         self.generate_experience_kwargs['eos_token_id'] = self.tokenizer.eos_token_id\n        #         self.generate_experience_kwargs['pad_token_id'] = self.tokenizer.pad_token_id\n        #     else:\n        #         if self.generate_kwargs is not None:\n        #             if 'max_length' in self.generate_kwargs:\n        #                 self.generate_kwargs.pop('max_length')\n\n        #             self.generate_kwargs['max_new_tokens'] = max_new_tokens\n        #             self.generate_kwargs['min_new_tokens'] = max_new_tokens\n        #             self.generate_kwargs['eos_token_id'] = self.tokenizer.eos_token_id\n        #             self.generate_kwargs['pad_token_id'] = self.tokenizer.pad_token_id\n\n        # print('---> Generate', input_ids, args, kwargs)\n        # print('self.generate_experience_kwargs', self.generate_experience_kwargs)\n        # print('self.generate_kwargs', self.generate_kwargs)\n\n        # self.model.eval()\n\n        # print('generation', self.tokenizer.decode(input_ids[0]))\n\n        kwargs[\"forced_eos_token_id\"] = self.tokenizer.eos_token_id\n        kwargs[\"suppress_tokens\"] = [self.tokenizer.pad_token_id]\n\n        preds = super().generate(input_ids, *args, **kwargs)\n\n        # self.model.train()\n\n        # print('Done generation', self.accelerator.device)\n\n        return preds\n\n    def generate_eval(self, input_ids, *args, **kwargs):\n        # if self.model.ds_zero3:\n        #     if 'max_length' in self.generate_kwargs:\n        #         self.generate_kwargs.pop('max_length')\n\n        #     max_new_tokens = self.config.method.gen_kwargs['max_new_tokens']\n        #     self.generate_kwargs['max_new_tokens'] = max_new_tokens\n        #     self.generate_kwargs['min_new_tokens'] = max_new_tokens\n        #     self.generate_kwargs['eos_token_id'] = self.tokenizer.eos_token_id\n        #     self.generate_kwargs['pad_token_id'] = self.tokenizer.pad_token_id\n\n        # self.model.train()\n\n        # print('generation_eval', self.tokenizer.decode(input_ids[0]))\n\n        # print('input_ids', input_ids[0])\n        # if 'attention_mask' in kwargs:\n        #     print('attention_mask', kwargs['attention_mask'][0])\n\n        kwargs[\"forced_eos_token_id\"] = self.tokenizer.eos_token_id\n        kwargs[\"suppress_tokens\"] = [self.tokenizer.pad_token_id]\n\n        preds = super().generate(input_ids, *args, **kwargs)\n\n        # print('Done generation', self.accelerator.device)\n\n        return preds\n\n    def make_experience(self, num_rollouts: int = 1024, iter_count: int = 0):  # noqa:\n        \"\"\"\n        Replace padding with pad_token_id\n        \"\"\"\n        logger.info(\"Collecting rollouts\")\n        tbar = logging.tqdm(\n            total=num_rollouts,\n            disable=os.environ.get(\"RANK\", 0) != \"0\",\n            desc=f\"[rollout 0 / {num_rollouts}]\",\n            # Lower progress bar by 1 if we're in WARNING mode or above to avoid hiding high priority progress\n            # bars (e.g. loss progress in trainers)\n            position=logging.get_verbosity() >= logging.WARNING,\n            # Leave progress bar if we're in INFO mode or lower to avoid spamming in suppressed verbosity levels\n            leave=logging.get_verbosity() < logging.WARNING,\n        )\n\n        ppo_rl_elements = []\n        stats = {}\n        clock = Clock()\n\n        while len(ppo_rl_elements) < num_rollouts:\n            # Get next batch in prompt dataset\n            batch = next(self.prompt_iterator)\n\n            exp_generate_time = time()\n\n            # Generate samples from the language model (similar to using HuggingFace `generate` method)\n            samples = self.generate(**batch)\n            stats[\"time/exp_generate\"] = time() - exp_generate_time\n\n            prompt_tensors = batch.input_ids\n            device = samples.device\n\n            prompt_sizes = torch.tensor([prompt_tensors.shape[1]] * len(prompt_tensors), device=device)\n\n            padded_samples = self.accelerator.pad_across_processes(\n                samples, dim=1, pad_index=self.tokenizer.pad_token_id, pad_first=False\n            )\n            padded_prompts = self.accelerator.pad_across_processes(\n                prompt_tensors, dim=1, pad_index=self.tokenizer.pad_token_id, pad_first=False\n            )\n            gathered_samples = self.accelerator.gather(padded_samples)\n            gathered_prompts = self.accelerator.gather(padded_prompts)\n            gathered_prompt_sizes = self.accelerator.gather(prompt_sizes)\n\n            if self.accelerator.is_main_process:\n                all_str_samples, all_str_prompts, all_str_outputs = self.decode(\n                    gathered_prompts, gathered_samples, gathered_prompt_sizes, append_eos_token=True\n                )\n\n                exp_score_time = time()\n                all_scores = torch.tensor(\n                    self.reward_fn(\n                        samples=all_str_samples,\n                        prompts=all_str_prompts,\n                        outputs=all_str_outputs,\n                    ),\n                    dtype=torch.float,\n                    device=device,\n                )\n                stats[\"time/exp_score\"] = time() - exp_score_time\n\n                all_scores = list(all_scores.reshape(self.accelerator.num_processes, -1).unbind())\n            else:\n                all_scores = None\n\n            if torch.distributed.is_initialized():\n                scores = torch.empty(len(samples), device=device)\n                torch.distributed.scatter(scores, all_scores)\n            else:\n                scores = all_scores[0].clone().detach()\n\n            str_samples, str_prompts, str_outputs = self.decode(prompt_tensors, samples, append_eos_token=True)\n\n            # Pad the sample outputs\n            outputs = self.tokenizer(str_outputs).input_ids\n            if self.config.model.model_arch_type == \"seq2seq\":\n                # add <pad> to the start of the output\n                for i in range(len(outputs)):\n                    outputs[i] = [self.tokenizer.pad_token_id] + outputs[i]\n\n            outputs = list(map(torch.LongTensor, outputs))\n            maxsize = max(map(len, outputs))\n            outputs = [\n                F.pad(\n                    output,\n                    (0, maxsize - len(output)),\n                    value=self.tokenizer.pad_token_id,\n                )\n                for output in outputs\n            ]\n            sample_outputs = torch.vstack(outputs).to(device)\n\n            # store statistics of the initial rollout as reference\n            if self.ref_mean is None:\n                self.ref_mean, self.ref_std = scores.mean(), scores.std()\n            all_scores_mean, all_scores_std = self.running_moments.update(scores)\n            stats[\"exp_scores/mean\"] = all_scores_mean.item()\n            stats[\"exp_scores/std\"] = all_scores_std.item()\n            stats[\"exp_scores/running_mean\"] = self.running_moments.mean.item()\n            stats[\"exp_scores/running_std\"] = self.running_moments.std.item()\n\n            if self.config.method.scale_reward == \"running\":\n                scores /= self.running_moments.std\n            elif self.config.method.scale_reward == \"ref\":\n                scores /= self.ref_std\n\n            clip_reward = self.config.method.cliprange_reward\n            if clip_reward:\n                scores = torch.clip(scores, -clip_reward, clip_reward)\n\n            # Precompute logprobs, values\n            if self.config.model.model_arch_type == \"seq2seq\":\n                raise NotImplementedError\n                attention_mask = batch.attention_mask.to(device)\n                prompt_tensors = batch.input_ids.to(device)\n                decoder_attention_mask = sample_outputs.not_equal(self.tokenizer.pad_token_id)\n                decoder_attention_mask[:, 0] = 1\n                with torch.no_grad():\n                    outputs = self.model(\n                        input_ids=prompt_tensors,\n                        attention_mask=attention_mask,\n                        decoder_input_ids=sample_outputs,\n                        decoder_attention_mask=decoder_attention_mask,\n                    )\n                    logits = outputs.logits\n                    values = outputs.value\n                    if hasattr(self.model, \"frozen_head\"):\n                        ref_logits = self.model.forward_hydra(\n                            input_ids=prompt_tensors,\n                            attention_mask=attention_mask,\n                            decoder_input_ids=sample_outputs,\n                            decoder_attention_mask=decoder_attention_mask,\n                            return_dict=True,\n                        ).logits\n                    else:\n                        ref_logits = self.ref_model(\n                            input_ids=prompt_tensors,\n                            attention_mask=attention_mask,\n                            decoder_input_ids=sample_outputs,\n                            decoder_attention_mask=decoder_attention_mask,\n                            return_dict=True,\n                        ).logits\n            else:\n                all_tokens = torch.cat((prompt_tensors.to(device), sample_outputs), dim=1)\n                attention_mask = all_tokens.not_equal(self.tokenizer.pad_token_id).long().to(device)\n                with torch.no_grad():\n                    logits, *_, values = self.model(\n                        all_tokens,\n                        attention_mask=attention_mask,\n                    )\n                    # TODO(dahoas): When hydra model works need to also support generation on hydra head\n                    # if hasattr(self.model, \"frozen_head\"):\n                    #     ref_logits = self.model.forward_hydra(\n                    #         all_tokens,\n                    #         attention_mask=attention_mask,\n                    #         return_dict=True,\n                    #     ).logits\n                    # else:\n                    ref_logits = self.ref_model(\n                        all_tokens,\n                        attention_mask,\n                    )\n                    ref_logits = ref_logits.to(device)\n\n            if self.config.model.model_arch_type == \"seq2seq\":\n                logprobs = logprobs_of_labels(logits[:, :-1, :], sample_outputs[:, 1:])\n                ref_logprobs = logprobs_of_labels(ref_logits[:, :-1, :], sample_outputs[:, 1:])\n            else:\n                logprobs = logprobs_of_labels(logits[:, :-1, :], all_tokens[:, 1:])\n                ref_logprobs = logprobs_of_labels(ref_logits[:, :-1, :], all_tokens[:, 1:])\n\n            n_samples: int = samples.shape[0]\n\n            # Estimate the KL divergence between the model and reference model\n            if self.config.model.model_arch_type == \"seq2seq\":\n                attention_mask = sample_outputs != self.tokenizer.pad_token_id\n                start = 0\n            else:\n                start = prompt_tensors.shape[1] - 1\n\n            log_ratio = (logprobs - ref_logprobs) * attention_mask[:, :-1]\n            self.mean_kl = (log_ratio.exp() - 1 - log_ratio).mean().to(device)\n\n            logprobs = logprobs.cpu()\n            ref_logprobs = ref_logprobs.cpu()\n            prompt_tensors = prompt_tensors.cpu()\n            sample_outputs = sample_outputs.cpu()\n            values = values.cpu()[:, :-1]\n\n            # Get the logprobs and values, for tokens that are not padding,\n            # from the start of the prompt up to the <eos> token, while also including the latter\n            # (these are taken from the student model and not the reference model)\n            ends = start + attention_mask[:, start:].sum(1) + 1\n            all_values = [values[ix, start : ends[ix]] for ix in range(n_samples)]\n            all_logprobs = [logprobs[ix, start : ends[ix]] for ix in range(n_samples)]\n\n            kl_penalty = self.kl_ctl.value * -log_ratio.cpu()\n            kl_penalty = [xs[start : ends[ix]] for ix, xs in enumerate(kl_penalty)]\n\n            rollout_count = 0\n\n            for sample_idx in range(n_samples):\n                rewards = kl_penalty[sample_idx]\n                rewards[-1] += scores[sample_idx].cpu()\n\n                ppo_rl_elements.append(\n                    PPORLElement(\n                        query_tensor=prompt_tensors[sample_idx],\n                        response_tensor=sample_outputs[sample_idx],\n                        logprobs=all_logprobs[sample_idx],\n                        values=all_values[sample_idx],\n                        rewards=rewards,\n                    )\n                )\n\n                rollout_count += 1\n            exp_time = clock.tick()\n            tbar.set_description(f\"[rollout {len(ppo_rl_elements)} / {num_rollouts}]\")\n            tbar.update(min(rollout_count, num_rollouts))\n        tbar.close()\n\n        if torch.distributed.is_initialized():\n            torch.distributed.all_reduce(self.mean_kl, torch.distributed.ReduceOp.AVG)\n\n        stats[\"policy/sqrt_kl\"] = torch.sqrt(self.mean_kl).item()\n        stats[\"kl_ctl_value\"] = self.kl_ctl.value\n        stats[\"time/exp\"] = exp_time\n\n        self.accelerator.log(stats, step=iter_count)\n\n        # Push samples and rewards to trainer's rollout storage\n        self.push_to_store(ppo_rl_elements)\n\n\ndef triton_server_ref_model():  # noqa:  C901\n    triton_host = os.environ.get(\"TRITON_HOST_REF\")\n    assert triton_host is not None, \"Specify reference model in the TRITON_HOST_REF environmental variable\"\n\n    triton_url, triton_model = triton_host.split(\"/\")\n    client = client_util.InferenceServerClient(url=triton_url, verbose=False)\n\n    def ref_model(all_tokens, attention_masks):\n        mbs = 8\n\n        all_tokens = all_tokens.detach().cpu().numpy()\n        attention_masks = attention_masks.detach().cpu().numpy()\n\n        out = []\n\n        for i in range(math.ceil(len(all_tokens) / mbs)):\n            batch_ixs = slice(i * mbs, (i + 1) * mbs)\n\n            # We specified int32 as types for a triton client\n            result = client.infer(\n                triton_model,\n                [\n                    prepare_tensor(\"input_ids\", all_tokens[batch_ixs].astype(np.int32)),\n                    prepare_tensor(\"attention_mask\", attention_masks[batch_ixs].astype(np.int32)),\n                ],\n            )\n\n            logits = result.as_numpy(\"logits\")\n\n            out.append(torch.tensor(logits))\n\n        return torch.cat(out, dim=0)\n\n    return ref_model\n\n\n@register_datapipeline\nclass CustomPromptPipeline(BasePipeline):\n    \"\"\"\n    Tokenizes prompts, unless they are already tokenized, and truncates them to `max_prompt_length` from the right\n    \"\"\"\n\n    def __init__(self, prompts: List[str], max_prompt_length: int, tokenizer: PreTrainedTokenizer):\n        super().__init__()\n\n        if max_prompt_length < 16:  # sanity check\n            raise ValueError(\n                f\"`max_prompt_length` is {max_prompt_length}, this is too small (less than 16). \"\n                \"Make sure all the config values are correct, when in doubt increase `seq_len` or decrease `max_new_tokens`.\"\n            )\n\n        model_inputs = tokenizer(\n            prompts,\n            truncation=True,\n            padding=True,\n            max_length=max_prompt_length,\n            add_special_tokens=False,\n        )\n\n        prompts_tokens_ = model_inputs[\"input_ids\"]\n        attention_mask = model_inputs[\"attention_mask\"]\n\n        # make sure that every prompt has an EOS token\n        for prompt_tokens in prompts_tokens_:\n            if tokenizer.eos_token_id not in prompt_tokens:\n                warnings.warn(\n                    \"Found a prompt without an EOS token, which means it was truncated. Consider increasing the context size (`seq_len`)\"\n                )\n                break\n\n        # prompts_tokens = []\n\n        # assistant_token_id = tokenizer.convert_tokens_to_ids(QA_SPECIAL_TOKENS[\"Answer\"])\n        # eos_token_id = tokenizer.eos_token_id\n\n        # print('input', prompts[0])\n        # print('ids', model_inputs[\"input_ids\"][0])\n        # print('masks', model_inputs[\"attention_mask\"])\n        # print('before', tokenizer.decode(prompts_tokens_[0]))\n\n        # If we truncate left this should not be a problem. Also for bpe this does not work...\n        # Due to truncation, special tokens may not be present ... so we add them (context is still incomplete)\n        # not need to update attention_mask since it iw always 1\n        # for prompt_tokens in prompts_tokens_:\n        #     prompts_tokens.append(prompt_tokens[:-2] + [eos_token_id, assistant_token_id])\n\n        prompts_tokens = prompts_tokens_\n\n        # print('after', tokenizer.decode(prompts_tokens[0]))\n\n        self.tokenizer = tokenizer\n        self.prompts = [\n            {\"input_ids\": tokens, \"attention_mask\": mask} for tokens, mask in zip(prompts_tokens, attention_mask)\n        ]\n\n    def __getitem__(self, ix: int):\n        return self.prompts[ix]\n\n    def __len__(self) -> int:\n        return len(self.prompts)\n\n    def create_loader(self, batch_size: int, shuffle=False) -> DataLoader:\n        collate_fn = DataCollatorWithPadding(self.tokenizer) if self.tokenizer else torch.vstack\n        return DataLoader(self, batch_size=batch_size, collate_fn=collate_fn, shuffle=shuffle)\n", "model/model_training/utils/utils_rl.py": "import tritonclient.grpc as client_util\nfrom tritonclient.utils import np_to_triton_dtype\n\n\ndef prepare_tensor(name: str, input):\n    t = client_util.InferInput(name, input.shape, np_to_triton_dtype(input.dtype))\n    t.set_data_from_numpy(input)\n    return t\n", "model/model_training/tests/test_oasst_dataset.py": "from argparse import Namespace\n\nimport pytest\nfrom model_training.custom_datasets import get_one_dataset\n\n\n@pytest.mark.skip(reason=\"cache not populated\")\ndef test_load_oasst_export_dataset():\n    config = Namespace(\n        cache_dir=\".cache\",\n    )\n    kwargs = {\n        \"lang\": \"en,es,de,fr\",\n        \"top_k\": 2,\n        \"input_file_path\": \"2023-02-19_oasst_ready_with_spam_deleted.jsonl.gz\",\n    }\n    train, val = get_one_dataset(conf=config, dataset_name=\"oasst_export\", **kwargs)\n    assert len(train) > 9000\n    assert len(val) > 2000\n", "model/model_training/tests/test_ranking_collator.py": "from argparse import Namespace\n\nimport pytest\nimport torch\nfrom model_training.custom_datasets import get_one_dataset\nfrom model_training.custom_datasets.formatting import (\n    QA_SPECIAL_TOKENS,\n    DatasetEntryRm,\n    Role,\n    Utterance,\n    create_dataset_entry_qa,\n)\nfrom model_training.custom_datasets.ranking_collator import RankingDataCollator\nfrom model_training.utils.utils import get_tokenizer, match_tokenizer_name\nfrom torch.utils.data import DataLoader\nfrom transformers.models.auto.tokenization_auto import AutoTokenizer\n\n\n@pytest.fixture\ndef pythia_tokenizer():\n    tokenizer = AutoTokenizer.from_pretrained(\"tests/resources/data_collator\", local_files_only=True)\n    # for this test we use the pythia special tokens but note that this test is model agnostic\n    tokenizer_config = match_tokenizer_name(\"pythia\")\n\n    tokenizer.add_special_tokens(\n        {\n            \"pad_token\": tokenizer_config.special_tokens.pad_token,\n            \"eos_token\": tokenizer_config.special_tokens.eos_token,\n            \"sep_token\": tokenizer_config.special_tokens.sep_token,\n        }\n    )\n\n    additional_special_tokens = list(QA_SPECIAL_TOKENS.values())\n\n    tokenizer.add_special_tokens({\"additional_special_tokens\": additional_special_tokens})\n    return tokenizer\n\n\ndef test_ranking_collator_system_tag(pythia_tokenizer):\n    first_example = DatasetEntryRm(\n        messages=[Utterance(text=\"First instruction.\", role=Role.prompter, lang=\"en\")],\n        replies=[\n            Utterance(text=\"Answer to first instruction.\", role=Role.assistant, lang=\"en\", quality=0.7),\n            Utterance(text=\"Answer to first instruction.\", role=Role.assistant, lang=\"de\", quality=0.8),\n        ],\n    )\n    second_example = DatasetEntryRm(\n        messages=[Utterance(text=\"Second instruction.\", role=Role.prompter)],\n        replies=[\n            Utterance(text=\"Answer to second instruction.\", role=Role.assistant, humor=0.1, creativity=0.2),\n            Utterance(text=\"Answer to second instruction.\", role=Role.assistant, humor=0.4, creativity=0.3),\n        ],\n    )\n    examples = [first_example, second_example]\n\n    rdc = RankingDataCollator(tokenizer=pythia_tokenizer, padding=True)\n    batch, cu_lens = rdc(examples=examples)\n\n    assert len(batch) == 2\n    assert cu_lens == [0, len(first_example.replies), len(first_example.replies) + len(second_example.replies)]\n    assert batch.data[\"attention_mask\"].shape[0] == 4  # we have 4 replies in total\n    assert batch.data[\"input_ids\"].shape == batch.data[\"attention_mask\"].shape\n    eos = pythia_tokenizer.eos_token\n\n    # check each instruction\n    first_example_first_answer_decoded = pythia_tokenizer.decode(batch.data[\"input_ids\"][0])\n    f\"{QA_SPECIAL_TOKENS['Question']}{first_example.messages[0].text}{eos}\" in first_example_first_answer_decoded\n    f\"{QA_SPECIAL_TOKENS['Answer']}{first_example.replies[0].text}{eos}\" in first_example_first_answer_decoded\n    \"lang: en\" in first_example_first_answer_decoded\n    \"quality: 0.7\" in first_example_first_answer_decoded\n\n    first_example_second_answer_decoded = pythia_tokenizer.decode(batch.data[\"input_ids\"][1])\n    f\"{QA_SPECIAL_TOKENS['Question']}{first_example.messages[0].text}{eos}\" in first_example_second_answer_decoded\n    f\"{QA_SPECIAL_TOKENS['Answer']}{first_example.replies[1].text}{eos}\" in first_example_second_answer_decoded\n    \"lang: de\" in first_example_second_answer_decoded\n    \"quality: 0.8\" in first_example_second_answer_decoded\n\n    second_example_first_answer_decoded = pythia_tokenizer.decode(batch.data[\"input_ids\"][2])\n    f\"{QA_SPECIAL_TOKENS['Question']}{second_example.messages[0].text}{eos}\" in second_example_first_answer_decoded\n    f\"{QA_SPECIAL_TOKENS['Answer']}{second_example.replies[0].text}{eos}\" in second_example_first_answer_decoded\n    \"humor: 0.1\" in second_example_first_answer_decoded\n    \"creativity: 0.2\" in second_example_first_answer_decoded\n\n    second_example_second_answer_decoded = pythia_tokenizer.decode(batch.data[\"input_ids\"][2])\n    f\"{QA_SPECIAL_TOKENS['Question']}{second_example.messages[0].text}{eos}\" in second_example_second_answer_decoded\n    f\"{QA_SPECIAL_TOKENS['Answer']}{second_example.replies[1].text}{eos}\" in second_example_second_answer_decoded\n    \"humor: 0.4\" in second_example_second_answer_decoded\n    \"creativity: 0.3\" in second_example_second_answer_decoded\n\n\ndef test_ranking_collator_no_messages(pythia_tokenizer):\n    first_messages = None\n    first_replies = [\n        \"Response A to None\",\n        \"Response B to None\",\n        \"Response C to None\",\n    ]\n    examples = [(first_messages, first_replies)]\n    rdc = RankingDataCollator(tokenizer=pythia_tokenizer, padding=True)\n    eos = pythia_tokenizer.eos_token\n    examples_ds = [\n        DatasetEntryRm(messages=None, replies=[Utterance(text=r, role=Role.assistant) for r in first_replies])\n    ]\n    # make sure that formatting via dataset entry and lists is the same\n    for ex in [examples, examples_ds]:\n        batch, cu_lens = rdc(examples=ex)\n        assert len(batch) == 2\n        assert cu_lens == [0, len(first_replies)]\n        assert batch.data[\"attention_mask\"].shape[0] == 3  # we have 5 replies in total\n        assert batch.data[\"input_ids\"].shape == batch.data[\"attention_mask\"].shape\n\n        # check each instruction\n        assert pythia_tokenizer.decode(batch.data[\"input_ids\"][0]) == f\"{first_replies[0]}{eos}\"\n        assert pythia_tokenizer.decode(batch.data[\"input_ids\"][1]) == f\"{first_replies[1]}{eos}\"\n        assert pythia_tokenizer.decode(batch.data[\"input_ids\"][2]) == f\"{first_replies[2]}{eos}\"\n        assert (batch.attention_mask == torch.where(batch.input_ids == 1, 0, 1)).all()\n\n\ndef test_ranking_collator_local(pythia_tokenizer):\n    first_messages = [\"First Instruction.\"]\n    first_replies = [\n        \"Response A to First Instruction\",\n        \"Response B to First Instruction\",\n        \"First Response C to First Instruction\",\n    ]\n    second_messages = [\"Second Instruction.\"]\n    second_replies = [\"Response A to Second Instruction\", \"Response B to Second Instruction\"]\n    examples = [(first_messages, first_replies), (second_messages, second_replies)]\n    rdc = RankingDataCollator(tokenizer=pythia_tokenizer, padding=True)\n    eos = pythia_tokenizer.eos_token\n    pad = pythia_tokenizer.pad_token\n\n    examples_ds = [\n        create_dataset_entry_qa(mode=\"rm\", questions=first_messages, answers=first_replies),\n        create_dataset_entry_qa(mode=\"rm\", questions=second_messages, answers=second_replies),\n    ]\n    # make sure that formatting via dataset entry and lists is the same\n    for ex in [examples, examples_ds]:\n        batch, cu_lens = rdc(examples=ex)\n\n        assert len(batch) == 2\n        assert cu_lens == [0, len(first_replies), len(first_replies) + len(second_replies)]\n        assert batch.data[\"attention_mask\"].shape[0] == 5  # we have 5 replies in total\n        assert batch.data[\"input_ids\"].shape == batch.data[\"attention_mask\"].shape\n        # check each instruction\n        assert (\n            pythia_tokenizer.decode(batch.data[\"input_ids\"][0])\n            == f\"{QA_SPECIAL_TOKENS['Question']}{first_messages[0]}{eos}{QA_SPECIAL_TOKENS['Answer']}{first_replies[0]}{eos}\"\n            + 5 * pad\n        )\n        assert (\n            pythia_tokenizer.decode(batch.data[\"input_ids\"][1])\n            == f\"{QA_SPECIAL_TOKENS['Question']}{first_messages[0]}{eos}{QA_SPECIAL_TOKENS['Answer']}{first_replies[1]}{eos}\"\n            + 5 * pad\n        )\n        assert (\n            pythia_tokenizer.decode(batch.data[\"input_ids\"][2])\n            == f\"{QA_SPECIAL_TOKENS['Question']}{first_messages[0]}{eos}{QA_SPECIAL_TOKENS['Answer']}{first_replies[2]}{eos}\"\n        )\n        assert (\n            pythia_tokenizer.decode(batch.data[\"input_ids\"][3])\n            == f\"{QA_SPECIAL_TOKENS['Question']}{second_messages[0]}{eos}{QA_SPECIAL_TOKENS['Answer']}{second_replies[0]}{eos}\"\n            + 4 * pad\n        )\n        assert (\n            pythia_tokenizer.decode(batch.data[\"input_ids\"][4])\n            == f\"{QA_SPECIAL_TOKENS['Question']}{second_messages[0]}{eos}{QA_SPECIAL_TOKENS['Answer']}{second_replies[1]}{eos}\"\n            + 4 * pad\n        )\n\n        assert (batch.attention_mask == torch.where(batch.input_ids == 1, 0, 1)).all()\n\n\n@pytest.mark.skip(reason=\"manual\")\ndef test_rm_datasets():\n    # dummy configuration\n    config = Namespace(cache_dir=\".cache\", model_name=\"EleutherAI/pythia-70m-deduped\")\n\n    dataset_names = [\"anthropic_rlhf\", \"hf_summary_pairs\", \"webgpt\", \"hellaswag\", \"shp\", \"hf_summary\"]\n    for name in dataset_names:\n        train, val = get_one_dataset(conf=config, dataset_name=name, mode=\"rm\")\n        print(f\"dataset: '{name}' (train ({type(train)}): {len(train)}, val({type(val)}): {len(val)})\")\n\n        avg_number_continuations = sum(len(x[1]) for x in train) / len(train)\n        num_more_than_two = sum(1 if len(x[1]) > 2 else 0 for x in train)\n        print(f\"Average number of continuations: {avg_number_continuations} (with >2: {num_more_than_two})\")\n\n        for i in range(10):\n            item = train[i + 100]\n            print(f\"[{i}] Prefix: {item[0]}\")\n            continuations = item[1]\n            print(f\"[{i}] Continuations ({len(continuations)}):\")\n            for j, c in enumerate(continuations):\n                print(f\"[{i}.{j}]: {c}\")\n\n\n@pytest.mark.skip(reason=\"manual\")\ndef test_ranking_collator():\n    # dummy configuration\n    config = Namespace(cache_dir=\".cache\", model_name=\"EleutherAI/pythia-70m-deduped\")\n\n    # get a tokenizer\n    tokenizer = get_tokenizer(config)\n    print(type(tokenizer))\n\n    # load oasst dataset\n    kwargs = {\"lang\": \"en,es,de,fr\", \"input_file_path\": \"2023-03-13_oasst_ready_labels.jsonl.gz\", \"mode\": \"rm\"}\n    train, val = get_one_dataset(conf=config, dataset_name=\"oasst_export\", **kwargs)\n    print(len(train))\n    a = train[0]\n\n    print(type(a))\n    print(len(a))\n    print(\"prefix\", a[0])\n    print(\"continuations\", a[1])\n\n    # create RankingCollator\n    ranking_collator = RankingDataCollator(tokenizer=tokenizer)\n\n    dl = DataLoader(\n        train,\n        batch_size=4,\n        collate_fn=ranking_collator,\n        num_workers=1,\n        pin_memory=False,\n    )\n\n    data_iter = iter(dl)\n    b = next(data_iter)\n    x, y = b\n\n    input_ids = x.input_ids\n    attention_mask = x.attention_mask\n    print(\"input_ids\", input_ids.shape)\n    print(\"attention_mask\", attention_mask.shape)\n    print(\"input_ids[0, :200]\", input_ids[0, :200])\n    print(\"decoded input_ids[0, :200]:\", tokenizer.decode(input_ids[0, :200]))\n    print(\"decoded non masked input_ids[0]:\", tokenizer.decode(input_ids[0][x.attention_mask[0] == 1]))\n\n    print(y)\n\n\nif __name__ == \"__main__\":\n    test_rm_datasets()\n    # test_ranking_collator()\n", "model/model_training/tests/test_patched_gpt_neox.py": "import torch\nfrom model_training.models.patching import patch_model\nfrom transformers import AutoModelForCausalLM, AutoTokenizer, GPTNeoXModel\n\n\ndef test_flash_attention_patch(dtype=torch.float16, device=\"cuda\"):\n    tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-70m-deduped\")\n    tokenizer.add_special_tokens({\"pad_token\": \"<pad>\"})\n\n    model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/pythia-70m-deduped\", torch_dtype=dtype).to(device)\n    patched_model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/pythia-70m-deduped\", torch_dtype=dtype).to(device)\n    patch_model(patched_model, resid_pdrop=None, flash_attention=True)\n\n    device = model.device\n    n_heads = model.config.num_attention_heads\n    head_dim = model.config.hidden_size // n_heads\n\n    with torch.no_grad():\n        for layer1, layer2 in zip(model.gpt_neox.layers, patched_model.gpt_neox.layers):\n            q = torch.randn(4, n_heads, 10, head_dim, dtype=dtype, device=device)\n            k = torch.randn(4, n_heads, 10, head_dim, dtype=dtype, device=device)\n            v = torch.randn(4, n_heads, 10, head_dim, dtype=dtype, device=device)\n            attn1, attn2 = layer1.attention, layer2.attention\n\n            out1, _ = attn1._attn(q, k, v)\n            out2, _ = attn2._attn(q, k, v)\n\n            assert ((out1 - out2).abs() < 1e-2).all()\n\n        batch = tokenizer([\"hello world\", \"lorem ipsum dolor sit amet\"], padding=True, return_tensors=\"pt\").to(device)\n        out1 = model(**batch).logits\n        out2 = patched_model(**batch).logits\n\n        diff = (out1 - out2) * batch[\"attention_mask\"].unsqueeze(-1)\n        assert (diff.abs() < 1).all()\n\n    input_ids = torch.randint(0, model.config.vocab_size, size=(2, 10), device=device)\n    patched_model(input_ids).logits.mean().backward()\n\n\ndef test_resid_dropout_patch():\n    model = GPTNeoXModel.from_pretrained(\"EleutherAI/pythia-70m-deduped\")\n    model.eval()\n\n    with torch.no_grad():\n        input_ids = torch.randint(0, 100, size=(2, 10))\n        attention_mask = torch.ones_like(input_ids)\n\n        logits_before = model(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state\n\n        patch_model(model, resid_pdrop=0.2, flash_attention=False)\n\n        logits_after = model(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state\n\n        assert (\n            logits_before - logits_after\n        ).abs().sum() < 1e-5, \"output is different before/after patching in eval mode\"\n\n        model.train()\n\n        logits1 = model(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state\n        logits2 = model(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state\n\n        assert (logits1 - logits2).abs().sum() > 1e-5, \"output is the same for different forward passes\"\n\n        x = model.get_input_embeddings()(input_ids)\n        for layer in model.layers:\n            # y1 = layer.attention(x, None)[0]\n            # y2 = layer.attention(x, None)[0]\n            # assert (y1 - y2).abs().sum() > 1e-5, \"attention output is the same for different forward passes\"\n\n            y1 = layer.mlp(x)\n            y2 = layer.mlp(x)\n            assert (y1 - y2).abs().sum() > 1e-5, \"mlp output is the same for different forward passes\"\n\n    model = GPTNeoXModel.from_pretrained(\"EleutherAI/pythia-70m-deduped\")\n    patch_model(model, resid_pdrop=0.0, flash_attention=False)\n\n    with torch.no_grad():\n        logits1 = model(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state\n        logits2 = model(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state\n        assert (logits1 - logits2).abs().sum() < 1e-5, \"output is the different for resid_pdrop=0\"\n\n    try:\n        logits = model(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state\n        logits.mean().backward()\n    except Exception as e:\n        raise Exception(\"patched backward pass failed\") from e\n\n\nif __name__ == \"__main__\":\n    test_flash_attention_patch()\n    test_resid_dropout_patch()\n", "model/model_training/tests/test_patched_llama.py": "import torch\nfrom model_training.models.patching import patch_model\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\n\ndef test_flash_attention_patch(dtype=torch.float16, device=\"cuda:0\", llama_path=\"/mnt/data/llama2/Llama-2-7b\"):\n    tokenizer = AutoTokenizer.from_pretrained(llama_path)\n    tokenizer.add_special_tokens({\"pad_token\": \"</s>\", \"eos_token\": \"</s>\", \"sep_token\": \"<s>\"})\n\n    model = AutoModelForCausalLM.from_pretrained(llama_path, torch_dtype=dtype).to(device)\n    patched_model = AutoModelForCausalLM.from_pretrained(llama_path, torch_dtype=dtype).to(device)\n    patch_model(patched_model, resid_pdrop=None, flash_attention=True)\n\n    device = model.device\n    n_heads = model.config.num_attention_heads\n    head_dim = model.config.hidden_size // n_heads\n\n    with torch.no_grad():\n        for layer1, layer2 in zip(model.model.layers, patched_model.model.layers):\n            hidden_states = torch.randn(4, 10, head_dim * n_heads, dtype=dtype, device=device)\n            attention_mask = (torch.randn(4, 10, device=device).sort(dim=-1).values < 0.5).int()\n            attn_mask = patched_model.model._prepare_decoder_attention_mask(attention_mask, (4, 10), hidden_states, 0)\n            position_ids = torch.arange(10, device=device).unsqueeze(0).expand(4, -1)\n            attn1, attn2 = layer1.self_attn, layer2.self_attn\n\n            out1, _, _ = attn1(hidden_states, attention_mask=attn_mask, position_ids=position_ids)\n            out2, _, _ = attn2(hidden_states, attention_mask=attn_mask, position_ids=position_ids)\n\n            assert (((out1 - out2) * attention_mask.unsqueeze(-1)).mean(dim=-1).abs() < 1e-3).all()\n\n        batch = tokenizer([\"hello world\", \"lorem ipsum dolor sit amet\"], padding=True, return_tensors=\"pt\").to(device)\n        out1 = model(**batch).logits\n        out2 = patched_model(**batch).logits\n\n        diff = (out1 - out2) * batch[\"attention_mask\"].unsqueeze(-1)\n        assert (diff.abs() < 0.1).all()\n\n    input_ids = torch.randint(0, model.config.vocab_size, size=(2, 10), device=device)\n    patched_model(input_ids).logits.mean().backward()\n\n\nif __name__ == \"__main__\":\n    test_flash_attention_patch()\n", "model/model_training/tests/test_dialogue_data_collator.py": "import pytest\nimport torch\nfrom model_training.custom_datasets.dialogue_collator import DialogueDataCollator\nfrom model_training.custom_datasets.formatting import QA_SPECIAL_TOKENS, create_dataset_entry_qa\nfrom model_training.utils.utils import match_tokenizer_name\nfrom transformers.models.auto.tokenization_auto import AutoTokenizer\n\n\n@pytest.fixture\ndef pythia_tokenizer():\n    tokenizer = AutoTokenizer.from_pretrained(\"tests/resources/data_collator\", local_files_only=True)\n    # for this test we use the pythia special tokens but note that this test is model agnostic\n    tokenizer_config = match_tokenizer_name(\"pythia\")\n\n    tokenizer.add_special_tokens(\n        {\n            \"pad_token\": tokenizer_config.special_tokens.pad_token,\n            \"eos_token\": tokenizer_config.special_tokens.eos_token,\n            \"sep_token\": tokenizer_config.special_tokens.sep_token,\n        }\n    )\n\n    additional_special_tokens = list(QA_SPECIAL_TOKENS.values())\n\n    tokenizer.add_special_tokens({\"additional_special_tokens\": additional_special_tokens})\n    return tokenizer\n\n\ndef test_dataset_entry_no_context(pythia_tokenizer):\n    d = DialogueDataCollator(\n        tokenizer=pythia_tokenizer,\n        padding=True,\n        system_add_length=False,\n    )\n    features = [\n        create_dataset_entry_qa(\n            mode=\"sft\",\n            questions=[\"Dummy Question?\"],\n            answers=[\"Dummy Answer.\"],\n        )\n    ]\n    batch = d(features)\n    expected_decoded_input_ids = [\n        \"<|prompter|>Dummy Question?<|endoftext|>\" + \"<|assistant|>Dummy Answer.<|endoftext|>\"\n    ]\n    expected_decoded_targets = [\n        expected_decoded_input_ids[0][len(\"<|prompter|>\") :] + expected_decoded_input_ids[0][: len(\"<|prompter|>\")],\n    ]\n\n    expected_masked = [\"<|assistant|>Dummy Answer.\"]\n\n    assert pythia_tokenizer.decode(batch.input_ids[0]) == expected_decoded_input_ids[0]\n    # check if targets are as expected\n    assert pythia_tokenizer.decode(batch.targets[0]) == expected_decoded_targets[0]\n\n    # check if masking is correct. Note that we mask the system as well\n    assert pythia_tokenizer.decode(batch.input_ids[0][batch.label_masks[0]]) == expected_masked[0]\n\n\ndef test_dataset_entry(pythia_tokenizer):\n    d = DialogueDataCollator(\n        tokenizer=pythia_tokenizer,\n        padding=True,\n        use_system_tag=True,\n        system_property_dropout=0,\n        system_add_length=False,\n    )\n    features = [\n        create_dataset_entry_qa(\n            mode=\"sft\",\n            questions=[\"What are the risks of untreated type 1 diabetes?\"],\n            answers=[\n                \"Untreated type 1 diabetes can rapidly result in diabetic ketoacidosis which may lead to loss of consciousness, coma and death.\"\n            ],\n            context=\"Prolonged lack of insulin can also result in diabetic ketoacidosis, characterized by persistent fatigue, dry or flushed skin, abdominal pain, nausea or vomiting, confusion, trouble breathing, and a fruity breath odor. Blood and urine tests reveal unusually high glucose and ketones in the blood and urine. Untreated ketoacidosis can rapidly progress to loss of consciousness, coma, and death. The percentage of children whose type 1 diabetes begins with an episode of diabetic ketoacidosis varies widely by geography, as low as 15% in parts of Europe and North America, and as high as 80% in the developing world.\",\n        ),\n        create_dataset_entry_qa(\n            mode=\"sft\",\n            questions=[\"Find all of the Amsterdam museums mentioned in the text and put them in a numbered list.\"],\n            answers=[\n                \"The Amsterdam museums mentioned in this text are:\\n1. Rijksmuseum\\n2. Van Gogh Museum\\n3. Amsterdam Museum\\n4. Stedelijk Museum\\n5. Hermitage Amsterdam\\n6. Anne Frank House\\n7. Het Scheepvaartmuseum\\n8. NEMO\"\n            ],\n            context=\"Amsterdam's main attractions include its historic canals; the Rijksmuseum, the state museum with a vast collection of Dutch Golden Age art; the Van Gogh Museum; the Dam Square, where the Royal Palace of Amsterdam and former city hall (stadhuis) are located; the Amsterdam Museum; Stedelijk Museum, with modern art; Hermitage Amsterdam, the Concertgebouw concert hall; the Anne Frank House; the Het Scheepvaartmuseum, the Heineken Experience, the Natura Artis Magistra; Hortus Botanicus, NEMO, the red-light district and many cannabis coffee shops. The city is also well known for its nightlife and festival activity; with several of its nightclubs (Melkweg, Paradiso) among the world's most famous. Primarily known for its artistic heritage, elaborate canal system and narrow canal houses with gabled fa\u00e7ades; well-preserved legacies of the city's 17th-century Golden Age, and the establishment of the Van Gogh Museum, displaying the work of the famous Dutch modern artist, have attracted millions of visitors to Amsterdam annually.\",\n        ),\n    ]\n    batch = d(features)\n    expected_decoded_input_ids = [\n        \"<|prompter|>What are the risks of untreated type 1 diabetes?<|endoftext|>\"\n        + \"<|system|>context: Prolonged lack of insulin can also result in diabetic ketoacidosis, characterized by persistent fatigue, dry or flushed skin, abdominal pain, nausea or vomiting, confusion, trouble breathing, and a fruity breath odor. Blood and urine tests reveal unusually high glucose and ketones in the blood and urine. Untreated ketoacidosis can rapidly progress to loss of consciousness, coma, and death. The percentage of children whose type 1 diabetes begins with an episode of diabetic ketoacidosis varies widely by geography, as low as 15% in parts of Europe and North America, and as high as 80% in the developing world.\\n<|endoftext|>\"\n        + \"<|assistant|>Untreated type 1 diabetes can rapidly result in diabetic ketoacidosis which may lead to loss of consciousness, coma and death.<|endoftext|>\"\n        + 346 * \"<|padding|>\",\n        \"<|prompter|>Find all of the Amsterdam museums mentioned in the text and put them in a numbered list.<|endoftext|>\"\n        + \"<|system|>context: Amsterdam's main attractions include its historic canals; the Rijksmuseum, the state museum with a vast collection of Dutch Golden Age art; the Van Gogh Museum; the Dam Square, where the Royal Palace of Amsterdam and former city hall (stadhuis) are located; the Amsterdam Museum; Stedelijk Museum, with modern art; Hermitage Amsterdam, the Concertgebouw concert hall; the Anne Frank House; the Het Scheepvaartmuseum, the Heineken Experience, the Natura Artis Magistra; Hortus Botanicus, NEMO, the red-light district and many cannabis coffee shops. The city is also well known for its nightlife and festival activity; with several of its nightclubs (Melkweg, Paradiso) among the world's most famous. Primarily known for its artistic heritage, elaborate canal system and narrow canal houses with gabled fa\u00e7ades; well-preserved legacies of the city's 17th-century Golden Age, and the establishment of the Van Gogh Museum, displaying the work of the famous Dutch modern artist, have attracted millions of visitors to Amsterdam annually.\\n<|endoftext|>\"\n        + \"<|assistant|>The Amsterdam museums mentioned in this text are:\\n1. Rijksmuseum\\n2. Van Gogh Museum\\n3. Amsterdam Museum\\n4. Stedelijk Museum\\n5. Hermitage Amsterdam\\n6. Anne Frank House\\n7. Het Scheepvaartmuseum\\n8. NEMO<|endoftext|>\",\n    ]\n    expected_masked = [\n        \"<|assistant|>Untreated type 1 diabetes can rapidly result in diabetic ketoacidosis which may lead to loss of consciousness, coma and death.\",\n        \"<|assistant|>The Amsterdam museums mentioned in this text are:\\n1. Rijksmuseum\\n2. Van Gogh Museum\\n3. Amsterdam Museum\\n4. Stedelijk Museum\\n5. Hermitage Amsterdam\\n6. Anne Frank House\\n7. Het Scheepvaartmuseum\\n8. NEMO\",\n    ]\n\n    expected_decoded_targets = [\n        expected_decoded_input_ids[0][len(\"<|prompter|>\") :] + expected_decoded_input_ids[0][: len(\"<|prompter|>\")],\n        expected_decoded_input_ids[1][len(\"<|prompter|>\") :] + expected_decoded_input_ids[1][: len(\"<|prompter|>\")],\n    ]\n    # this is trivial, since input_ids is a tensor\n    assert batch.input_ids[0].shape == batch.input_ids[1].shape\n    # since we want to check things in a human readable way\n    # we decode the encoded ids back and check if they match the expected text\n\n    assert pythia_tokenizer.decode(batch.input_ids[0]) == expected_decoded_input_ids[0]\n    assert pythia_tokenizer.decode(batch.input_ids[1]) == expected_decoded_input_ids[1]\n\n    # check if targets are as expected\n    assert pythia_tokenizer.decode(batch.targets[0]) == expected_decoded_targets[0]\n    assert pythia_tokenizer.decode(batch.targets[1]) == expected_decoded_targets[1]\n\n    # check if masking is correct. Note that we mask the system as well\n    assert pythia_tokenizer.decode(batch.input_ids[0][batch.label_masks[0]]) == expected_masked[0]\n    assert pythia_tokenizer.decode(batch.input_ids[1][batch.label_masks[1]]) == expected_masked[1]\n\n\ndef test_dialogue_data_collator(pythia_tokenizer):\n    d = DialogueDataCollator(\n        tokenizer=pythia_tokenizer,\n        padding=True,\n    )\n    input_features = [\n        [\n            \"When Jada saw the rain outside, she decided to grab her umbrella before leaving the house. She didn't want to get wet, and knew that the umbrella would keep her dry. You are Rain. I don't want to get wet, I'll grab my umbrella.\",\n            \"Hey there! Mind if I talk to you for a bit?\",\n            \"Um, sure I guess.\",\n            \"Great! So, as you can see, it's raining out. Pretty miserable, huh?\",\n            \"Yeah, I was just about to leave and I didn't want to get wet.\",\n            \"Yeah, nobody likes getting wet. But you know what they say, April showers bring May flowers.\",\n            \"I guess that's true.\",\n            \"And speaking of flowers, have you ever seen a rainforest? They're absolutely amazing! Everything is so green and alive.\",\n            \"No, I haven't but it sounds really cool.\",\n            \"Oh, it is! It's one of the most beautiful places on Earth. You should definitely try to see one if you can.\",\n        ],\n        [\n            \"Kalliope got a bachelor\u2019s degree in English from the University of Michigan. Kalliope has always been interested in writing and literature, so she decided to pursue a degree in English. She worked hard throughout her four years of college and was thrilled to finally receive her diploma. You are Dad. Hey Dad. How are you?\",\n            \"I'm doing well, sweetheart. How's college life treating you?\",\n            \"It's been great so far. I just got my degree in English and I'm looking forward to starting my career soon.\",\n            \"That's terrific! Your mother and I are very proud of you. We know you've worked hard to get where you are today.\",\n            \"Thanks, Dad. I couldn't have done it without your support.\",\n            \"Any time, kiddo. So, what's next for you?\",\n            \"I'm not sure yet. I'm hoping to find a job in publishing or something similar. I just have to start sending out my resume and see what happens.\",\n            \"That sounds like a great plan. I know you'll find the perfect job in no time.\",\n            \"Thanks, Dad. I appreciate your confidence in me.\",\n        ],\n    ]\n    batch = d(input_features)\n    expected_decoded_input_ids = [\n        \"<|prompter|>When Jada saw the rain outside, she decided to grab her umbrella before leaving the house. She didn't want to get wet, \"\n        + \"and knew that the umbrella would keep her dry. You are Rain. I don't want to get wet, I'll grab my umbrella.<|endoftext|>\"\n        + \"<|assistant|>Hey there! Mind if I talk to you for a bit?<|endoftext|>\"\n        + \"<|prompter|>Um, sure I guess.<|endoftext|>\"\n        + \"<|assistant|>Great! So, as you can see, it's raining out. Pretty miserable, huh?<|endoftext|>\"\n        + \"<|prompter|>Yeah, I was just about to leave and I didn't want to get wet.<|endoftext|>\"\n        + \"<|assistant|>Yeah, nobody likes getting wet. But you know what they say, April showers bring May flowers.<|endoftext|>\"\n        + \"<|prompter|>I guess that's true.<|endoftext|>\"\n        + \"<|assistant|>And speaking of flowers, have you ever seen a rainforest? They're absolutely amazing! Everything is so green and alive.<|endoftext|>\"\n        + \"<|prompter|>No, I haven't but it sounds really cool.<|endoftext|>\"\n        + \"<|assistant|>Oh, it is! It's one of the most beautiful places on Earth. You should definitely try to see one if you can.<|endoftext|>\"\n        + 21 * \"<|padding|>\",\n        \"<|prompter|>Kalliope got a bachelor\u2019s degree in English from the University of Michigan. Kalliope has always been interested in writing and literature, so she decided to pursue a degree in English. She worked hard throughout her four years of college and was thrilled to finally receive her diploma. You are Dad. Hey Dad. How are you?<|endoftext|>\"\n        + \"<|assistant|>I'm doing well, sweetheart. How's college life treating you?<|endoftext|>\"\n        + \"<|prompter|>It's been great so far. I just got my degree in English and I'm looking forward to starting my career soon.<|endoftext|>\"\n        + \"<|assistant|>That's terrific! Your mother and I are very proud of you. We know you've worked hard to get where you are today.<|endoftext|>\"\n        + \"<|prompter|>Thanks, Dad. I couldn't have done it without your support.<|endoftext|>\"\n        + \"<|assistant|>Any time, kiddo. So, what's next for you?<|endoftext|>\"\n        + \"<|prompter|>I'm not sure yet. I'm hoping to find a job in publishing or something similar. I just have to start sending out my resume and see what happens.<|endoftext|>\"\n        + \"<|assistant|>That sounds like a great plan. I know you'll find the perfect job in no time.<|endoftext|>\"\n        + \"<|prompter|>Thanks, Dad. I appreciate your confidence in me.<|endoftext|>\",\n    ]\n    expected_masked = [\n        \"<|assistant|>Hey there! Mind if I talk to you for a bit?<|endoftext|>\"\n        + \"<|assistant|>Great! So, as you can see, it's raining out. Pretty miserable, huh?<|endoftext|>\"\n        + \"<|assistant|>Yeah, nobody likes getting wet. But you know what they say, April showers bring May flowers.<|endoftext|>\"\n        + \"<|assistant|>And speaking of flowers, have you ever seen a rainforest? They're absolutely amazing! Everything is so green and alive.<|endoftext|>\"\n        + \"<|assistant|>Oh, it is! It's one of the most beautiful places on Earth. You should definitely try to see one if you can.\",\n        \"<|assistant|>I'm doing well, sweetheart. How's college life treating you?<|endoftext|>\"\n        + \"<|assistant|>That's terrific! Your mother and I are very proud of you. We know you've worked hard to get where you are today.<|endoftext|>\"\n        + \"<|assistant|>Any time, kiddo. So, what's next for you?<|endoftext|>\"\n        + \"<|assistant|>That sounds like a great plan. I know you'll find the perfect job in no time.<|endoftext|>\",\n    ]\n    expected_decoded_targets = [\n        # <|prompter|> has 12 tokens, this is removed and added at the end\n        expected_decoded_input_ids[0][12:] + expected_decoded_input_ids[0][:12],\n        expected_decoded_input_ids[1][12:] + expected_decoded_input_ids[1][:12],\n    ]\n\n    # this is trivial, since input_ids is a tensor\n    assert batch.input_ids[0].shape == batch.input_ids[1].shape\n    # since we want to check things in a human readable way\n    # we decode the encoded ids back and check if they match the expected text\n    assert pythia_tokenizer.decode(batch.input_ids[0]) == expected_decoded_input_ids[0]\n    assert pythia_tokenizer.decode(batch.input_ids[1]) == expected_decoded_input_ids[1]\n    # check if the attention mask is correct we mask only the padding\n    assert (\n        torch.argwhere(batch.attention_mask == 0) == torch.stack([torch.zeros(21), torch.arange(217, 238)], dim=1)\n    ).all()\n    assert (\n        torch.argwhere(batch.attention_mask == 1)\n        == torch.cat(\n            [\n                torch.stack([torch.zeros(217), torch.arange(0, 217)], dim=1),\n                torch.stack([torch.ones(238), torch.arange(0, 238)], dim=1),\n            ]\n        )\n    ).all()\n    # Check that the attention mask is only applied to padding tokens\n    padding_token = pythia_tokenizer.get_vocab()[pythia_tokenizer.pad_token]\n    assert (torch.argwhere(batch.attention_mask == 0) == torch.argwhere(batch.input_ids == padding_token)).all()\n    # check if the masking works correctly, we mask everything between <|assistant|> and <|endoftext|>\n    # todo: we mask the last <|endoftext|> but I don't see a reason why???\n    assert pythia_tokenizer.decode(batch.input_ids[0][batch.label_masks[0]]) == expected_masked[0]\n    assert pythia_tokenizer.decode(batch.input_ids[1][batch.label_masks[1]]) == expected_masked[1]\n\n    # check if targets are as expected\n    assert pythia_tokenizer.decode(batch.targets[0]) == expected_decoded_targets[0]\n    assert pythia_tokenizer.decode(batch.targets[1]) == expected_decoded_targets[1]\n", "model/model_training/tests/test_datasets.py": "from argparse import Namespace\n\nimport pytest\nfrom model_training.custom_datasets import QA_DATASETS, SUMMARIZATION_DATASETS, get_one_dataset\nfrom model_training.custom_datasets.dialogue_collator import DialogueDataCollator\nfrom model_training.utils.utils import get_tokenizer\nfrom torch.utils.data import ConcatDataset, DataLoader\n\n\n@pytest.mark.skip(reason=\"very slow\")\ndef test_all_datasets():\n    qa_base = QA_DATASETS\n    summarize_base = SUMMARIZATION_DATASETS\n    others = [\"webgpt\", \"soda\", \"joke\", \"explain_prosocial\", \"prosocial_dialogue\"]\n    translation = [\"dive_mt\", \"wmt2019_zh-en\", \"wmt2019_ru-en\", \"ted_trans_de-ja\", \"ted_trans_nl-en\"]\n\n    config = Namespace(cache_dir=\".cache\")\n    for dataset_name in translation + others + summarize_base + qa_base:\n        print(dataset_name)\n        train, eval = get_one_dataset(config, dataset_name)\n        # sanity check\n        for idx in range(min(len(train), 1000)):\n            train[idx]\n        for idx in range(min(len(eval), 1000)):\n            eval[idx]\n\n\n@pytest.mark.skip(reason=\"very slow\")\ndef test_collate_fn():\n    config = Namespace(cache_dir=\".cache\", model_name=\"Salesforce/codegen-2B-multi\")\n    tokenizer = get_tokenizer(config)\n    collate_fn = DialogueDataCollator(tokenizer, max_length=620)\n    qa_base = QA_DATASETS\n    summarize_base = SUMMARIZATION_DATASETS\n    others = [\"webgpt\", \"soda\", \"joke\", \"gsm8k\"]\n    trains, evals = [], []\n    for dataset_name in others + qa_base + summarize_base:\n        print(dataset_name)\n        train, eval = get_one_dataset(config, dataset_name)\n        trains.append(train)\n        evals.append(eval)\n\n    dataloader = DataLoader(ConcatDataset(trains), collate_fn=collate_fn, batch_size=128)\n    for batch in dataloader:\n        print(batch[\"targets\"].shape[0])\n        print(tokenizer.decode(batch[\"input_ids\"][0]))\n        print(\"-----\")\n        print(tokenizer.decode(batch[\"targets\"][0][batch[\"label_masks\"][0]]))\n        assert batch[\"targets\"].shape[1] <= 620\n    dataloader = DataLoader(ConcatDataset(evals), collate_fn=collate_fn, batch_size=128)\n    for batch in dataloader:\n        assert batch[\"targets\"].shape[1] <= 620\n\n\n@pytest.mark.skip(reason=\"cache not populated\")\ndef test_collate_fn_simple():\n    config = Namespace(cache_dir=\".cache\", model_name=\"EleutherAI/pythia-70m-deduped\")\n    tokenizer = get_tokenizer(config)\n\n    collate_fn = DialogueDataCollator(tokenizer, max_length=620)\n    kwargs = {\n        \"lang\": \"en,de\",\n        \"top_k\": 2,\n        \"input_file_path\": \"2023-03-21_oasst_ready_synth_labels.jsonl.gz\",\n    }\n    train, val = get_one_dataset(conf=config, dataset_name=\"oasst_export\", **kwargs)\n\n    dataloader = DataLoader(train, collate_fn=collate_fn, batch_size=2)\n    for batch in dataloader:\n        print(\"batch:\", batch.keys())\n        print(batch[\"targets\"].shape[0])\n        print(tokenizer.decode(batch[\"input_ids\"][0]))\n        print(tokenizer.decode(batch[\"input_ids\"][1]))\n        print(\"-----\")\n        print(tokenizer.decode(batch[\"targets\"][0][batch[\"label_masks\"][0]]))\n        print(tokenizer.decode(batch[\"targets\"][1][batch[\"label_masks\"][1]]))\n        assert batch[\"targets\"].shape[1] <= 620\n        break\n    # dataloader = DataLoader(ConcatDataset(evals), collate_fn=collate_fn, batch_size=128)\n    # for batch in dataloader:\n    #     assert batch[\"targets\"].shape[1] <= 620\n\n\nif __name__ == \"__main__\":\n    test_collate_fn_simple()\n", "model/model_training/tests/test_rm_loading.py": "from argparse import Namespace\n\nimport model_training.models.reward_model  # noqa: F401\nfrom model_training.models.reward_model import GPTNeoXRewardModel\nfrom model_training.utils.utils import get_tokenizer\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\n\n\ndef test_convert_model(\n    model_name: str = \"EleutherAI/pythia-70m-deduped\",\n    cache_dir: str = \".cache\",\n    output_dir: str = \".saved_models_rm/debug\",\n):\n    training_conf = Namespace(\n        cache_dir=cache_dir,\n        model_name=model_name,\n    )\n    tokenizer = get_tokenizer(training_conf)\n    model = GPTNeoXRewardModel.from_pretrained(model_name, cache_dir=cache_dir)\n    print(\"model\", type(model))\n    print(\"tokenizer\", type(tokenizer))\n    model.save_pretrained(output_dir)\n    tokenizer.save_pretrained(output_dir)\n\n\ndef test_load_reward_model(model_name: str = \"andreaskoepf/oasst-rm-1-pythia-1b\", cache_dir: str = \".cache\"):\n    tokenizer = AutoTokenizer.from_pretrained(model_name, cache_dir=cache_dir)\n    rm = AutoModelForSequenceClassification.from_pretrained(model_name, cache_dir=cache_dir)\n    print(\"auto\", type(rm))\n    print(\"auto.config\", type(rm.config))\n    question = \"<|prompter|>Hi how are you?<|endoftext|><|assistant|>Hi, I am Open-Assistant a large open-source language model trained by LAION AI. How can I help you today?<|endoftext|>\"\n    inputs = tokenizer(question, return_tensors=\"pt\")\n    print(inputs)\n    score = rm(**inputs).logits[0].cpu().detach()\n    print(score)\n\n\nif __name__ == \"__main__\":\n    # test_load_reward_model(\"../.saved_models_rm/oasst-rm-1-pythia-1b/\")\n    test_load_reward_model(\"andreaskoepf/oasst-rm-1-pythia-1b\")\n", "model/model_training/tests/test_formatting.py": "import pytest\nfrom model_training.custom_datasets.formatting import QA_SPECIAL_TOKENS, DatasetEntrySft, Role, Utterance\n\n\ndef test_dataset_entry_formatting_missing_lang():\n    ds_entry = DatasetEntrySft(\n        conversation=[\n            Utterance(\n                text=\"What is the capital of France?\",\n                role=Role.prompter,\n            ),\n            Utterance(\n                text=\"The capital of France is Paris.\",\n                role=Role.assistant,\n                context=\"Some context\",\n                quality=1.0,\n                humor=0.0,\n                creativity=0.0,\n            ),\n        ],\n    )\n    formatted = ds_entry.get_formatted(\n        \"<|endofline|>\",\n        use_system_tag=True,\n        system_property_dropout=0.0,\n        system_add_length=True,\n    )\n    assert len(formatted) == 2\n    # this is just optional\n    assert \"length: 2\" in formatted[0]\n    assert \"quality: 1.0\" in formatted[0]\n    assert \"humor: 0.0\" in formatted[0]\n    assert \"creativity: 0.0\" in formatted[0]\n    assert \"Some context\" in formatted[0]\n    assert f\"{QA_SPECIAL_TOKENS['Question']}What is the capital of France?<|endofline|>\" in formatted[0]\n    assert f\"{QA_SPECIAL_TOKENS['Answer']}The capital of France is Paris.<|endofline|>\" == formatted[1]\n\n\ndef test_dataset_entry():\n    ds_entry = DatasetEntrySft(\n        conversation=[\n            Utterance(\n                text=\"What is the capital of France?\",\n                role=Role.prompter,\n            ),\n            Utterance(\n                text=\"The capital of France is Paris.\",\n                role=Role.assistant,\n                context=\"Some context\",\n                lang=\"en\",\n                quality=1.0,\n                humor=0.0,\n                creativity=0.0,\n            ),\n        ],\n    )\n    formatted = ds_entry.get_formatted(\n        \"<|endofline|>\",\n        use_system_tag=True,\n        system_property_dropout=0.0,\n        system_add_length=True,\n    )\n    assert len(formatted) == 2\n    assert \"lang: en\" in formatted[0]\n    assert \"length: 2\" in formatted[0]\n    assert \"quality: 1.0\" in formatted[0]\n    assert \"humor: 0.0\" in formatted[0]\n    assert \"creativity: 0.0\" in formatted[0]\n    assert \"Some context\" in formatted[0]\n    assert f\"{QA_SPECIAL_TOKENS['Question']}What is the capital of France?<|endofline|>\" in formatted[0]\n    assert f\"{QA_SPECIAL_TOKENS['Answer']}The capital of France is Paris.<|endofline|>\" == formatted[1]\n\n\ndef test_dataset_entry_float_violations():\n    fields = {\n        \"content\": \"The capital of France is Paris.\",\n        \"context\": \"Some context\",\n        \"lang\": \"en\",\n    }\n    with pytest.raises(ValueError, match=\"Field quality must be between 0 and 1. Received: -1.0\"):\n        Utterance(**fields, quality=-1.0, humor=0.0, creativity=0.0)\n\n    with pytest.raises(ValueError, match=\"Field humor must be between 0 and 1. Received: 2\"):\n        Utterance(**fields, quality=1.0, humor=2.0, creativity=0.0)\n\n    with pytest.raises(ValueError, match=\"Field creativity must be between 0 and 1. Received: 1000.0\"):\n        Utterance(**fields, quality=1.0, humor=2.0, creativity=1000.0)\n", "model/model_training/tests/__init__.py": "", "model/model_training/tests/test_patched_falcon.py": "import torch\nfrom model_training.models.patching import patch_model\nfrom transformers import AutoTokenizer\nfrom transformers.models.falcon.modeling_falcon import FalconForCausalLM\n\n\ndef test_flash_attention_falcon_patch(device=\"cuda:0\"):\n    model_name = \"tiiuae/falcon-7b\"\n    dtype = torch.bfloat16\n\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    tokenizer.pad_token = tokenizer.eos_token\n\n    model = FalconForCausalLM.from_pretrained(model_name, torch_dtype=dtype).to(device)\n    patched_model = FalconForCausalLM.from_pretrained(model_name, torch_dtype=dtype).to(device)\n    patch_model(patched_model, resid_pdrop=None, flash_attention=True)\n\n    with torch.no_grad():\n        batch = tokenizer([\"hello world\", \"lorem ipsum dolor sit amet\"], padding=True, return_tensors=\"pt\")\n        batch = {k: v.to(device) for k, v in batch.items() if k != \"token_type_ids\"}\n\n        out1 = model(use_cache=False, **batch).logits\n        out2 = patched_model(use_cache=False, **batch).logits\n\n        diff = (out1 - out2) * batch[\"attention_mask\"].unsqueeze(-1)\n\n        assert (diff.abs() < 0.3).all()\n\n    input_ids = torch.randint(0, patched_model.config.vocab_size, size=(2, 10), device=device)\n    patched_model(input_ids).logits.mean().backward()\n\n\nif __name__ == \"__main__\":\n    test_flash_attention_falcon_patch()\n", "model/model_training/tests/test_utils.py": "from argparse import Namespace\n\nimport pytest\nfrom model_training.utils.utils import TOKENIZER_CONFIGS, get_tokenizer, match_tokenizer_name\n\n\ndef test_tokenizer():\n    get_tokenizer(Namespace(model_name=\"Salesforce/codegen-2B-multi\", cache_dir=\".cache\"))\n    get_tokenizer(Namespace(model_name=\"facebook/galactica-1.3b\", cache_dir=\".cache\"))\n\n\ndef test_tokenizer_successful_match():\n    for config_name, config in TOKENIZER_CONFIGS.items():\n        found_config = match_tokenizer_name(config_name)\n        assert found_config == config\n\n\ndef test_tokenizer_partial_match():\n    for config_name in [\"facebook/galactica-1.3b\", \"togethercomputer/GPT-JT-6B-v1\", \"Salesforce/codegen-2B-multi\"]:\n        found_config = match_tokenizer_name(config_name)\n        assert found_config\n\n\ndef test_tokenizer_failed_match():\n    for fake_config_name in [\"not-a-model\", \"fake\"]:\n        with pytest.raises(ValueError):\n            match_tokenizer_name(fake_config_name)\n", "model/model_training/custom_datasets/instruction.py": "\"\"\"\n    These are in the form of 'INSTRUCTION', 'RESPONSE'\n\"\"\"\nimport random\nfrom typing import Optional\n\nfrom datasets import load_dataset\nfrom model_training.custom_datasets.formatting import DatasetEntry, create_dataset_entry_qa\nfrom model_training.custom_datasets.utils import _filter_by_words\nfrom torch.utils.data import Dataset\n\nINSTRUCTION_DATASETS = {\n    # Note humaneval_mbpp_codegen_qa returns a code string that we would want to at least wrap in ``` marks`\n    \"humaneval_mbpp_codegen_qa\": {\"dataset_path\": \"OllieStanley/humaneval-mbpp-codegen-qa\", \"lang\": \"en\"},\n    # Write unit tests to do task X\n    \"humaneval_mbpp_testgen_qa\": {\"dataset_path\": \"OllieStanley/humaneval-mbpp-testgen-qa\", \"lang\": \"en\"},\n    \"grade_school_math_instructions\": {\"dataset_path\": \"qwedsacf/grade-school-math-instructions\", \"lang\": \"en\"},\n    \"recipes\": {\"dataset_path\": \"dctanner/oa_recipes\", \"lang\": \"en\"},\n    \"ubuntu_dialogue_qa\": {\"dataset_path\": \"sedthh/ubuntu_dialogue_qa\"},\n    \"cmu_wiki_qa\": {\"dataset_path\": \"sedthh/cmu_wiki_qa\"},\n    \"youtube_subs_howto100m\": {\"dataset_path\": \"totuta/youtube_subs_howto100M\"},\n    \"iapp_wiki_qa_squad\": {\"dataset_path\": \"wannaphong/iapp_wiki_qa_squad_oa\"},\n    \"zhihu-kol\": {\"dataset_path\": \"wangrui6/zhihu-kol\"},\n    \"minimath\": {\n        \"dataset_path\": \"kentsui/minimath\",\n        \"instruction_column\": \"question\",\n        \"response_column\": \"answer\",\n    },\n    \"oa_wiki_qa_bart_10000row\": {\"dataset_path\": \"michaelthwan/oa_wiki_qa_bart_10000row\"},\n    \"oa_leet10k\": {\"dataset_path\": \"ehartford/oa_leet10k\"},\n    \"poem_instructions\": {\"dataset_path\": \"checkai/instruction-poems\", \"lang\": \"en\"},\n    \"oa_stackexchange\": {\"dataset_path\": \"donfu/oa-stackexchange\"},\n    \"tell_a_joke\": {\"dataset_path\": \"mikegarts/oa_tell_a_joke_20000\", \"lang\": \"en\"},\n    \"wizardlm_70k\": {\n        \"dataset_path\": \"ehartford/WizardLM_alpaca_evol_instruct_70k_unfiltered\",\n        \"instruction_column\": \"instruction\",\n        \"response_column\": \"output\",\n    },\n    \"megacode\": {\n        \"dataset_path\": \"rombodawg/MegaCodeTraining112k\",\n        \"instruction_column\": \"prompt\",\n        \"response_column\": \"completion\",\n        \"data_files\": \"RombosCodeTraining112k.json\",\n    },\n    \"megacode2\": {\n        \"dataset_path\": \"rombodawg/LosslessMegaCodeTrainingV2_1m_Evol_Uncensored\",\n        \"instruction_column\": \"USER\",\n        \"response_column\": \"ASSISTANT\",\n        \"data_files\": \"DeDuped_LosslessMegaCodeTrainingV2_942k_Evol_Uncensored.json\",\n    },\n    \"megacode3\": {\n        \"dataset_path\": \"rombodawg/LosslessMegaCodeTrainingV3_2.2m_Evol\",\n        \"instruction_column\": \"USER\",\n        \"response_column\": \"ASSISTANT\",\n        \"data_files\": \"LosslessMegaCodeTrainingV3_2.2m_Evol.json\",\n    },\n    \"evol_instruct_code\": {\n        \"dataset_path\": \"nickrosh/Evol-Instruct-Code-80k-v1\",\n        \"instruction_column\": \"instruction\",\n        \"response_column\": \"output\",\n    },\n    \"evol-codealpaca-v1\": {\n        \"dataset_path\": \"theblackcat102/evol-codealpaca-v1\",\n        \"instruction_column\": \"instruction\",\n        \"response_column\": \"output\",\n    },\n    \"cot_submix_original\": {\n        \"dataset_path\": \"conceptofmind/cot_submix_original\",\n        \"instruction_column\": \"inputs\",\n        \"response_column\": \"targets\",\n    },\n}\n\n\nclass InstructionDataset(Dataset):\n    def __init__(\n        self,\n        name: str,\n        dataset_path: str,\n        cache_dir: str,\n        split: str,\n        mode: str = \"sft\",\n        instruction_column: str = \"INSTRUCTION\",\n        response_column: str = \"RESPONSE\",\n        data_files: Optional[str] = None,\n        lang: Optional[str] = None,\n        fill_min_length: Optional[int] = None,\n        seed: int = 42,\n    ):\n        assert mode in (\"sft\", \"rl\")\n        self.name = name\n        self.mode = mode\n\n        self.instruction_column = instruction_column\n        self.response_column = response_column\n        self.data_files = data_files\n        self.lang = lang\n\n        num_invalid = 0\n\n        ds = load_dataset(dataset_path, cache_dir=cache_dir, split=split, data_files=data_files)\n        self.dataset: list[tuple[list[str], list[str]]] = []\n\n        questions, answers = [], []\n        item_len = 0\n\n        rng = random.Random(seed)\n        order = list(range(len(ds)))\n        rng.shuffle(order)\n\n        # filter entries and optionally combine multiple entries\n        for i in order:\n            entry = ds[i]\n            q = entry[self.instruction_column]\n            a = entry[self.response_column]\n            if (\n                q is not None\n                and len(q.strip()) > 0\n                and a is not None\n                and len(a.strip()) > 0\n                and _filter_by_words(q)\n                and _filter_by_words(a)\n            ):\n                questions.append(q)\n                answers.append(a)\n                item_len += len(a) + len(q)\n\n                if fill_min_length is None or fill_min_length < item_len:\n                    self.dataset.append((questions, answers))\n                    item_len = 0\n                    questions, answers = [], []\n            else:\n                num_invalid += 1\n\n        if len(questions) > 0 and len(answers) > 0:\n            self.dataset.append((questions, answers))\n\n        if num_invalid > 0:\n            print(f\"[Warning] {num_invalid} entries of {name} ({dataset_path}) were invalid.\")\n\n    def __len__(self):\n        return len(self.dataset)\n\n    def __getitem__(self, idx) -> DatasetEntry:\n        questions, answers = self.dataset[idx]\n\n        return create_dataset_entry_qa(\n            mode=self.mode,\n            questions=questions,\n            answers=answers,\n            lang=self.lang,\n        )\n\n\nRAG_DATASETS = {\n    \"multi-chapter-summaries\": \"shahules786/Multi-chapter-summaries\",\n}\n\n\nclass RAGDataset(Dataset):\n    def __init__(\n        self,\n        dataset,\n        split: str = \"train\",\n        cache_dir: str = \".cache/\",\n    ):\n        if dataset not in RAG_DATASETS.keys():\n            raise ValueError(f\"Invalid dataset {dataset}\")\n\n        if dataset == \"multi-chapter-summaries\":\n            self.prompt, self.context, self.response = \"prompt\", \"context\", \"summary\"\n\n        self.dataset = load_dataset(RAG_DATASETS[dataset], cache_dir=cache_dir)[split]\n\n    def __len__(self):\n        return len(self.dataset)\n\n    def __getitem__(self, idx):\n        prompt, context, response = [self.dataset[idx][key] for key in [self.prompt, self.context, self.response]]\n\n        return create_dataset_entry_qa(mode=\"sft\", questions=[prompt + context], answers=[response])\n", "model/model_training/custom_datasets/qa_datasets.py": "\"\"\"\n    Open / close book QA datasets\n\"\"\"\nimport glob\nimport json\nimport os\nimport random\nimport re\nfrom collections import defaultdict\nfrom pathlib import Path\nfrom typing import Any\nfrom urllib.request import urlopen\n\nimport numpy as np\nimport requests\nfrom datasets import load_dataset\nfrom model_training.custom_datasets.formatting import DatasetEntry, create_dataset_entry_qa\nfrom model_training.custom_datasets.utils import _filter_by_words\nfrom torch import Generator\nfrom torch.utils.data import Dataset, Subset, random_split\n\n# @agoryuno contributed this\nre_reference_remove = re.compile(r\"\\[\\d+(?:,\\s*\\d+)*?\\]\")\nre_single_reference_remove = re.compile(r\"\\[\\s?\\d+\\s?\\]\")\n\n# check if the whole string is just a combination of (multiple) whitespaces and newlines\nre_whitespace_newline_match = re.compile(r\"^[\\s\\n]*$\")\n\n\nLINKING_CHARS = [\"\\n\", \"\\n\\n\", \" \"]\n\n\ndef index_squad_v2(example):\n    if len(example[\"answers\"][\"text\"]):\n        answer = example[\"answers\"][\"text\"][0]\n    else:\n        answer = \"I do not have answer for that\"\n    return example[\"context\"] + \" \" + example[\"question\"], answer\n\n\ndef index_uasquad(example):\n    if len(example[\"Answer\"]):\n        answer = example[\"Answer\"]\n    else:\n        answer = \"\u042f \u043d\u0435 \u043c\u0430\u044e \u043d\u0430 \u0446\u0435 \u0432\u0456\u0434\u043f\u043e\u0432\u0456\u0434\u0456\"\n    return example[\"Context\"] + \" \" + example[\"Question\"], answer\n\n\ndef index_trivia_qa_nocontext(example):\n    # dummy return one randomly\n    return example[\"question\"], example[\"answer\"][\"aliases\"][np.random.randint(len(example[\"answer\"][\"aliases\"]))]\n\n\ndef index_trivia_qa_context(example):\n    question = example[\"question\"]\n    if len(example[\"search_results\"][\"search_context\"]):\n        context = example[\"search_results\"][\"search_context\"][\n            np.random.randint(len(example[\"search_results\"][\"search_context\"]))\n        ]\n    else:\n        context = \"\"\n    answer = example[\"answer\"][\"aliases\"][np.random.randint(len(example[\"answer\"][\"aliases\"]))]\n\n    return context + \" \" + question, answer\n\n\ndef index_adversarial_qa(example):\n    return example[\"title\"] + \". \" + example[\"context\"] + \" \" + example[\"question\"], example[\"answers\"][\"text\"][0]\n\n\ndef index_gsm8k(example):\n    return example[\"question\"], example[\"answer\"]\n\n\ndef index_wikihow(example):\n    return example[\"title\"] + \", explain step by step\", example[\"result\"]\n\n\ndef index_essay_instruction(example):\n    return example[\"instructions\"], example[\"titles\"].strip() + \"\\n\" + example[\"essays\"]\n\n\ndef index_math_qa(example):\n    \"\"\"\n    we are not including choices, so no need to output the \"answer : <a,b,c,d>\" part\n    > if girls is 10 and boys is 20 , then 10 / 20 . so ratio of girls to boys is = 10 / 20 = 1 / 2 answer : a\n    \"\"\"\n    return example[\"Problem\"], example[\"Rationale\"].split(\"answer : \", maxsplit=1)[0]\n\n\ndef index_eli5(example):\n    return example[\"title\"], example[\"answers\"][\"text\"][0]\n\n\ndef index_gsm_hard(example):\n    return example[\n        \"input\"\n    ] + \"\\nWrite a small snippet of python code to answer this\", \"Here's the code solution to the question\\n```python\\n{}\\n```\\n The answer should be {}\".format(\n        example[\"code\"].strip(), example[\"target\"]\n    )\n\n\nclass QADataset(Dataset):\n    \"\"\"\n    How to define a new QA dataset:\n\n    Criteria : the qa dataset doesn't need fancy transform needed between fields rows or list\n\n    1. Write the transform function, which maps each row into a pair of (question, answer) tuple\n\n    2. Update DATASET_FORMAT_MAPPING with your dataset name and required parameter\n\n        - index_fn : your transform function\n\n        - name: the dataset name, this will be used when the name is different than huggingface load_dataset name\n\n        - params: if your dataset require a predefined name, create a dictionary with the parameter name-value dictionary\n\n    Feel free to create issues on GH for any suggestion how we can simplify this thing\n    \"\"\"\n\n    DATASET_FORMAT_MAPPING = {\n        \"squad_v2\": {\"index_fn\": index_squad_v2},\n        \"ua_squad\": {\n            \"index_fn\": index_uasquad,\n            \"name\": \"FIdo-AI/ua-squad\",\n            \"params\": {\"field\": \"data\"},\n            \"no_val\": True,\n        },\n        \"trivia_qa_nocontext\": {\n            \"index_fn\": index_trivia_qa_nocontext,\n            \"name\": \"trivia_qa\",\n            \"params\": {\"name\": \"rc.nocontext\"},\n        },\n        \"trivia_qa_context\": {\"index_fn\": index_trivia_qa_context, \"name\": \"trivia_qa\", \"params\": {\"name\": \"rc\"}},\n        \"adversarial_qa\": {\n            \"index_fn\": index_adversarial_qa,\n            \"params\": {\"name\": \"adversarialQA\"},\n        },\n        \"gsm8k_hard\": {\"index_fn\": index_gsm_hard, \"name\": \"reasoning-machines/gsm-hard\", \"no_val\": True},\n        \"gsm8k\": {\"index_fn\": index_gsm8k, \"params\": {\"name\": \"main\"}, \"validation\": \"test\"},\n        \"wikihow\": {\"name\": \"b-mc2/wikihow_lists\", \"index_fn\": index_wikihow, \"no_val\": True},\n        \"essay_instruction\": {\n            \"name\": \"ChristophSchuhmann/essays-with-instructions\",\n            \"index_fn\": index_essay_instruction,\n            \"no_val\": True,\n        },\n        \"math_qa\": {\n            \"index_fn\": index_math_qa,\n        },\n        \"reddit_eli5\": {\"name\": \"eli5\", \"index_fn\": index_eli5, \"split_postfix\": \"_eli5\"},\n        \"reddit_askh\": {\"name\": \"eli5\", \"index_fn\": index_eli5, \"split_postfix\": \"_askh\"},\n        \"reddit_asks\": {\"name\": \"eli5\", \"index_fn\": index_eli5, \"split_postfix\": \"_asks\"},\n    }\n\n    def __init__(self, dataset, cache_dir, split):\n        self.no_val = False\n        if dataset in self.DATASET_FORMAT_MAPPING:\n            context = self.DATASET_FORMAT_MAPPING[dataset]\n            if split == \"validation\" and \"validation\" in context:\n                split = context[\"validation\"]\n            if \"name\" not in context:\n                context[\"name\"] = dataset\n            if \"split_postfix\" in context:\n                # append a postfix to split name, used in eli5 : test_eli5, test_asks, test_askh\n                split += context[\"split_postfix\"]\n            if \"params\" not in context:\n                context[\"params\"] = {\"cache_dir\": cache_dir, \"split\": split}\n            else:\n                context[\"params\"][\"cache_dir\"] = cache_dir\n                context[\"params\"][\"split\"] = split\n            if \"no_val\" in context:\n                self.no_val = True\n            self.index_fn = context[\"index_fn\"]\n            self.dataset = load_dataset(context[\"name\"], **context[\"params\"])\n        else:\n            raise ValueError(\"Unknown dataset : \" + dataset)\n        self.length = len(self.dataset)\n\n    def __len__(self):\n        return self.length\n\n    def __getitem__(self, idx):\n        data = self.dataset[idx]\n        return self.index_fn(data)\n\n\nclass WebGPT(Dataset):\n    name = \"webgpt\"\n\n    def __init__(self, mode: str = \"sft\", max_answers: int = 5) -> None:\n        super().__init__()\n        self.mode = mode\n        assert mode in (\"sft\", \"rm\", \"rl\")\n\n        dataset = load_dataset(\"openai/webgpt_comparisons\")\n\n        self.rows = []\n\n        question_answer_dict = defaultdict(dict)\n\n        for row in dataset[\"train\"]:\n            question = row[\"question\"][\"full_text\"]\n            answer_0 = re_reference_remove.sub(\"\", row[\"answer_0\"])\n            answer_1 = re_reference_remove.sub(\"\", row[\"answer_1\"])\n            if answer_0 != \"\" and answer_1 != \"\" and answer_0 != answer_1:\n                question_answer_dict[question][answer_0] = row[\"score_0\"]\n                question_answer_dict[question][answer_1] = row[\"score_1\"]\n\n        for question, answers in question_answer_dict.items():\n            # Sort answer dict with the highest score first (hence the prefactor -1).\n            # Then take only the first `max_answers` elements (usually there are just\n            # 2, but there are examples where we have more)\n            answers_sorted = [x[0] for x in sorted(answers.items(), key=lambda x: -1 * x[1])]\n            self.rows.append(\n                create_dataset_entry_qa(\n                    mode=mode,\n                    questions=[question],\n                    answers=[answers_sorted[:max_answers]],\n                    lang=\"en\",\n                )\n            )\n\n    def __len__(self) -> int:\n        return len(self.rows)\n\n    def __getitem__(self, index) -> DatasetEntry:\n        dialogue = self.rows[index]\n        return dialogue\n\n\nclass SODA(Dataset):\n    name = \"soda\"\n\n    def __init__(self, cache_dir, mode=\"sft\", input_max_length=32 * 1024) -> None:\n        super().__init__()\n        if mode not in (\"sft\", \"rl\"):\n            raise NotImplementedError(f\"Currently only the modes 'sft' and 'rl' are implemented. Received {mode}.\")\n        self.mode = mode\n        self.pairs = []\n        dataset = load_dataset(\"allenai/soda\", cache_dir=cache_dir)[\"train\"]\n        for data in dataset:\n            if (processed_data := self.process_soda_convo(data, input_max_length=input_max_length)) is not None:\n                self.pairs.append(processed_data)\n\n    def process_soda_convo(self, data: dict[str, Any], input_max_length: int) -> DatasetEntry | None:\n        play_as = data[\"speakers\"][1]\n        dialogue_bg = \"{}{}\".format(\n            data[\"narrative\"],\n            \" You are {}.\".format(play_as),\n        )\n\n        # Perform some sanity checks, if these fail return None\n        # ignore data with more than 2 speakers for now\n        if len(set(data[\"speakers\"])) != 2:\n            return None\n\n        speaker1 = data[\"speakers\"][0]\n        speaker2 = data[\"speakers\"][1]\n        # make sure that the speakers are in correct order [S1, S2, S1, S2, S1, S2], otherwise return None\n        speaker1_idx = [idx % 2 == 0 for idx, k in enumerate(data[\"speakers\"]) if k == speaker1]\n        speaker2_idx = [idx % 2 == 1 for idx, k in enumerate(data[\"speakers\"]) if k == speaker2]\n        if all(speaker1_idx) and all(speaker2_idx):\n            # add dialog background to first question.\n            # [Q1, A1, Q2, A2] -> [B + Q1, A1, Q2, A2]\n            data[\"dialogue\"][0] = f\"{dialogue_bg} {data['dialogue'][0]}\"\n            # Use only input_max_length characters\n            truncated_dialogue = [k[:input_max_length] for k in data[\"dialogue\"]]\n            questions = [q for idx, q in enumerate(truncated_dialogue) if idx % 2 == 0]\n            answers = [a for idx, a in enumerate(truncated_dialogue) if idx % 2 == 1]\n            if len(questions) == 0 or len(questions) != len(answers):\n                return None\n            return create_dataset_entry_qa(mode=self.mode, questions=questions, answers=answers)\n\n    def __len__(self) -> int:\n        return len(self.pairs)\n\n    def __getitem__(self, index) -> DatasetEntry:\n        dialogue = self.pairs[index]\n        return dialogue\n\n\nclass SODADialogue(Dataset):\n    def __init__(self, cache_dir, verbose=True):\n        dataset = load_dataset(\"emozilla/soda_synthetic_dialogue\", cache_dir=cache_dir)\n\n        self.pairs = []\n        faulty = 0\n        for split in dataset:\n            for row in dataset[split]:\n                question_answer_pairs = ()\n\n                question_answers = row[\"conversation\"].split(\"User: \")\n                for question_answer in question_answers[1:]:  # first element is empty\n                    try:\n                        question, answer = question_answer.split(\"\\nAssistant: \")\n                        question_answer_pairs += (\n                            question,\n                            answer,\n                        )\n                    except ValueError:\n                        # there might be some extra 'User: ' or 'Assistant: ' tokens in the dataset that cause trouble..\n                        faulty += 1\n                        continue\n\n                self.pairs.append(question_answer_pairs)\n\n        if verbose:\n            print(\"For SODA dialogue dataset found {} faults within the total {} dialogs\".format(faulty, len(self)))\n\n    def __len__(self):\n        return len(self.pairs)\n\n    def __getitem__(self, index):\n        return self.pairs[index]\n\n\nclass JokeExplaination(Dataset):\n    name = \"joke\"\n    url = \"https://gist.github.com/theblackcat102/42b697e24a13fdb499e20edfbf618361/raw/1834dca207898c15f93b809d1195f6f6e47c9e1e/joke_explained.jsonl\"\n\n    def __init__(self, cache_dir) -> None:\n        super().__init__()\n        os.makedirs(cache_dir, exist_ok=True)\n        joke_explain_filename = os.path.join(cache_dir, \"joke_explaination.jsonl\")\n        if not os.path.exists(joke_explain_filename):\n            with urlopen(self.url) as file:\n                content = file.read().decode()\n            with open(joke_explain_filename, \"w\") as fout:\n                fout.write(content)\n\n        self.pairs = []\n        with open(joke_explain_filename, \"r\") as f:\n            for line in f:\n                data = json.loads(line)\n                joke = data[\"joke\"]\n                # DO NOT change this\n                # it's the data that had syntax error\n                explanation = data[\"explaination\"]\n                self.pairs.append(create_dataset_entry_qa(mode=\"sft\", questions=[joke], answers=[explanation]))\n\n    def __len__(self) -> int:\n        return len(self.pairs)\n\n    def __getitem__(self, index) -> DatasetEntry:\n        return self.pairs[index]\n\n\nclass TranslatedQA(Dataset):\n    \"\"\"\n    Translation OA v3 results\n    a list of non english translation of OA v3 instruction generated text in jsonl\n    format for each line:\n    {\n        \"text\": \"User: ... Assistant: ....\",\n        \"meta\": {\"source\": ... },\n        \"translate\": [\n            { \"round\": 1, \"human\":\"...\", \"answer\": \"...\"},\n            ...\n            { \"round\": K, \"human\":\"...\", \"answer\": \"...\"},\n        ]\n    }\n    Since OA contain some code we needed to reference the original text to skip these\n    \"\"\"\n\n    name = \"oa_translated\"\n\n    def __init__(self, cache_dir) -> None:\n        super().__init__()\n        os.makedirs(cache_dir, exist_ok=True)\n        path = os.path.join(cache_dir, self.name)\n        os.makedirs(path, exist_ok=True)\n        self.pairs = []\n        for translated_jsonl in glob.glob(os.path.join(path, \"*.jsonl\")):\n            with open(translated_jsonl, \"r\") as fin:\n                for line in fin:\n                    data = json.loads(line)\n                    if \"Python \" in data[\"text\"]:\n                        # translation currently doesn't ignore code\n                        # so we will have to reference original text\n                        # for ignoring the translation\n                        continue\n                    prefix = \"\"\n                    for convo_round in data[\"translate\"]:\n                        human, answer = convo_round[\"human\"], convo_round[\"answer\"]\n                        if convo_round[\"round\"] > 2:\n                            self.pairs.append((prefix, human, answer))\n                        else:\n                            self.pairs.append((\"\", human, answer))\n\n                        # Does this make sense?\n                        prefix += \"{}{}{}{}\".format(\n                            \"Question:\",\n                            convo_round[\"human\"],\n                            \"Answer:\",\n                            convo_round[\"answer\"],\n                        )\n\n        self.length = len(self.pairs)\n\n    def __len__(self):\n        return self.length\n\n    def __getitem__(self, index):\n        return self.pairs[index]\n\n\nclass AlpacaBaseDataset(Dataset):\n    def __init__(self, data: list, mode: str):\n        super().__init__()\n        self.data = data\n        if mode not in (\"sft\", \"rl\"):\n            raise NotImplementedError(\n                f\"Alpaca Dataset for mode {self.mode} is not implemented. Currently supported modes are 'sft' and 'rl'.\"\n            )\n        self.mode = mode\n\n    def __len__(self) -> int:\n        return len(self.data)\n\n    def __getitem__(self, index: int) -> DatasetEntry:\n        dialogue = self.data[index]\n        return dialogue\n\n\ndef load_alpaca_dataset(\n    dataset_name: str,\n    val_split: float,\n    cache_dir: str,\n    mode: str = \"sft\",\n    manual_seed: int = 287631038922,\n) -> tuple[AlpacaBaseDataset, AlpacaBaseDataset]:\n    generator = Generator()\n    generator.manual_seed(manual_seed)\n\n    def process_split(dataset: Subset) -> list[DatasetEntry]:\n        data = []\n\n        for row in dataset:\n            question = row[\"instruction\"]\n            if len(row[\"input\"]) > 0:\n                input_ = \"{}\\n{}\".format(question, row[\"input\"])\n            else:\n                input_ = question\n\n            if (_filter_by_words(input_) is None) or (_filter_by_words(row[\"output\"]) is None):\n                continue\n\n            ds_entry = create_dataset_entry_qa(mode=mode, questions=[input_], answers=[row[\"output\"]])\n            data.append(ds_entry)\n        return data\n\n    if dataset_name == \"alpaca\":\n        dataset = load_dataset(\"yahma/alpaca-cleaned\", cache_dir=cache_dir)\n    elif dataset_name == \"code_alpaca\":\n        dataset = load_dataset(\"sahil2801/CodeAlpaca-20k\", cache_dir=cache_dir)\n    else:\n        raise ValueError(f\"Expected dataset_name to be 'alapaca' or 'code_alpaca'. Received {dataset_name}.\")\n\n    splits = random_split(dataset[\"train\"], lengths=[1.0 - val_split, val_split], generator=generator)\n    train = AlpacaBaseDataset(process_split(splits[0]), mode=mode)\n    val = AlpacaBaseDataset(process_split(splits[1]), mode=mode)\n    return train, val\n\n\nclass Vicuna(Dataset):\n    name = \"vicuna\"\n\n    @staticmethod\n    def process_vicuna_conversations(\n        data: list[dict[str, None | str]], input_max_length: int\n    ) -> tuple[list[str], list[str]] | None:\n        role = None\n        messages = []\n        # drop conversations that start with Bot\n        if len(data[\"conversations\"]) == 0 or data[\"conversations\"][0][\"from\"] != \"human\":\n            return None\n        questions = []\n        answers = []\n        for line in data[\"conversations\"]:\n            speaker = line[\"from\"]  # 'human' or 'gpt'\n            message = line[\"value\"]\n            if message is None or message == \"\":\n                if speaker == \"gpt\":\n                    return None\n                elif speaker == \"human\":\n                    # replace empty messages with one of the following\n                    message = random.choice([\"...\", \"Please continue\", \"Go on\", \"\"])\n            # remove markdown escaping in revision 192ab2185289094fc556ec8ce5ce1e8e587154ca\n            # python-markdownify with escape_asterisks & escape_underscores True is used\n            # for pre-processing the dataset.\n            # See also https://github.com/LAION-AI/Open-Assistant/issues/2510\n            message = message.replace(r\"\\_\", \"_\")\n            message = message.replace(r\"\\*\", \"*\")\n            message = re_single_reference_remove.sub(\"\", message)\n\n            if role != speaker:\n                if role is not None:\n                    if role == \"human\":\n                        questions.append(\"\\n\".join(messages)[:input_max_length])\n                    if role == \"gpt\":\n                        answers.append(\"\\n\".join(messages)[:input_max_length])\n                    messages = []\n                role = speaker\n            messages.append(message.strip())\n\n        if role is not None and len(messages) > 0:\n            if role == \"human\":\n                questions.append(\"\\n\".join(messages)[:input_max_length])\n            if role == \"gpt\":\n                answers.append(\"\\n\".join(messages)[:input_max_length])\n        return questions, answers\n\n    def __init__(self, cache_dir: str | Path, mode: str = \"sft\", input_max_length: int = 32 * 1024) -> None:\n        super().__init__()\n\n        if mode != \"sft\":\n            raise NotImplementedError(f\"Currently only the mode 'sft' is implemented. Received {mode}.\")\n        self.mode = mode\n\n        dataset = load_dataset(\n            \"Aeala/ShareGPT_Vicuna_unfiltered\",\n            cache_dir=cache_dir,\n            data_files=[\"ShareGPT_V4.3_unfiltered_cleaned_split.json\"],\n        )[\"train\"]\n\n        self.pairs = []\n        for data in dataset:\n            if (qa := self.process_vicuna_conversations(data, input_max_length=input_max_length)) is not None:\n                if len(qa[0]) > 0 and len(qa[0]) == len(qa[1]):\n                    self.pairs.append(create_dataset_entry_qa(mode=self.mode, questions=qa[0], answers=qa[1]))\n\n    def __len__(self) -> int:\n        return len(self.pairs)\n\n    def __getitem__(self, index: int) -> DatasetEntry:\n        return self.pairs[index]\n\n\nclass WizardEvolInstructV2(Dataset):\n    def __init__(self, cache_dir: str | Path, mode: str = \"sft\", input_max_length: int = 32 * 1024) -> None:\n        super().__init__()\n\n        if mode != \"sft\":\n            raise NotImplementedError(f\"Currently only the mode 'sft' is implemented. Received {mode}.\")\n        self.mode = mode\n\n        dataset = load_dataset(\n            \"ehartford/WizardLM_evol_instruct_V2_196k_unfiltered_merged_split\",\n            cache_dir=cache_dir,\n            data_files=[\"WizardLM_evol_instruct_V2_196k_unfiltered_merged_split.json\"],\n            revision=\"34f04cfbc280da93a79ad9ecf339923f9411c1fc\",\n        )[\"train\"]\n\n        self.pairs = []\n        for data in dataset:\n            if (qa := Vicuna.process_vicuna_conversations(data, input_max_length=input_max_length)) is not None:\n                if len(qa[0]) > 0 and len(qa[0]) == len(qa[1]):\n                    self.pairs.append(create_dataset_entry_qa(mode=\"sft\", questions=qa[0], answers=qa[1], lang=\"en\"))\n\n    def __len__(self) -> int:\n        return len(self.pairs)\n\n    def __getitem__(self, index: int) -> DatasetEntry:\n        dialogue = self.pairs[index]\n        return dialogue\n\n\nclass DatabricksDolly15k(Dataset):\n    def __init__(self, cache_dir: str | Path, mode: str = \"sft\") -> None:\n        super().__init__()\n        self.rows = []\n        self.citation_regex = re.compile(r\"\\[[a-zA-Z]\\]\")  # removes citations in the form of e.g. [a] or [A]\n        if mode not in (\"sft\", \"rl\"):\n            raise NotImplementedError(f\"Currently only the modes 'sft' and 'rl' are implemented. Received {mode}.\")\n        self.mode = mode\n        data = load_dataset(\"OllieStanley/oa_dolly_15k\", cache_dir=cache_dir)\n        for line in data[\"train\"]:\n            if (c := self._process_instruction(line)) is not None:\n                self.rows.append(c)\n\n    def _process_instruction(self, row: dict[str, str]) -> DatasetEntry | None:\n        context = re_reference_remove.sub(\"\", row[\"METADATA\"][\"CONTEXT\"])\n        # further remove references\n        context = context.replace(\"[citation needed]\", \"\")\n        context = self.citation_regex.sub(\"\", context)\n        if _filter_by_words(row[\"INSTRUCTION\"]) and _filter_by_words(row[\"RESPONSE\"]):\n            return create_dataset_entry_qa(\n                mode=self.mode,\n                questions=[row[\"INSTRUCTION\"]],\n                answers=[row[\"RESPONSE\"]],\n                context=context,\n            )\n\n    def __len__(self) -> int:\n        return len(self.rows)\n\n    def __getitem__(self, index: int) -> DatasetEntry:\n        dialogue = self.rows[index]\n        return dialogue\n\n\nclass Dolly15kMultilingual(Dataset):\n    def __init__(self, cache_dir: str | Path, mode: str = \"sft\") -> None:\n        super().__init__()\n        self.rows = []\n        self.citation_regex = re.compile(r\"\\[[a-zA-Z]\\]\")  # removes citations in the form of e.g. [a] or [A]\n        if mode not in (\"sft\", \"rl\"):\n            raise NotImplementedError(f\"Currently only the modes 'sft' and 'rl' are implemented. Received {mode}.\")\n        self.mode = mode\n        splits = load_dataset(\"argilla/databricks-dolly-15k-curated-multilingual\", cache_dir=cache_dir)\n        for lang in (\"en\", \"de\", \"es\", \"fr\"):\n            data = splits[lang]\n            for line in data:\n                if (c := self._process_instruction(line, lang=lang)) is not None:\n                    self.rows.append(c)\n\n    def _process_instruction(self, row: dict[str, str], lang: str) -> DatasetEntry | None:\n        context = re_reference_remove.sub(\"\", row[\"context\"])\n        # further remove references\n        context = context.replace(\"[citation needed]\", \"\")\n        context = self.citation_regex.sub(\"\", context)\n        if _filter_by_words(row[\"instruction\"]) and _filter_by_words(row[\"response\"]):\n            return create_dataset_entry_qa(\n                mode=self.mode,\n                questions=[row[\"instruction\"]],\n                answers=[row[\"response\"]],\n                context=context,\n                lang=lang,\n            )\n\n    def __len__(self) -> int:\n        return len(self.rows)\n\n    def __getitem__(self, index: int) -> DatasetEntry:\n        dialogue = self.rows[index]\n        return dialogue\n\n\nclass AlpacaGpt4(Dataset):\n    def __init__(self, cache_dir: str | Path, mode: str = \"sft\") -> None:\n        super().__init__()\n        self.rows = []\n        if mode not in (\"sft\", \"rl\"):\n            raise NotImplementedError(f\"Currently only the modes 'sft' and 'rl' are implemented. Received {mode}.\")\n        self.mode = mode\n        data = load_dataset(\"teknium/GPT4-LLM-Cleaned\", cache_dir=cache_dir)  # alternative: vicgalle/alpaca-gpt4\n        for line in data[\"train\"]:\n            if (conv := self._process_instruction(line)) is not None:\n                self.rows.append(conv)\n\n    def _process_instruction(self, row: dict[str, str]) -> DatasetEntry | None:\n        # filter all appearing variants of \"no input\" or empty input or cases where the input is already in the instruction.\n        # In this cases we don't add the input\n        if (\n            any([k in row[\"input\"].lower() for k in [\"no input\", \"noinput\", \"n/a\"]])\n            or (not row[\"input\"])\n            or (row[\"input\"].lower() in row[\"instruction\"].lower())\n        ):\n            return create_dataset_entry_qa(\n                mode=self.mode,\n                questions=[row[\"instruction\"]],\n                answers=[row[\"output\"]],\n            )\n        # Concatenate the instruction and input.\n        else:\n            linking_char = random.choice(LINKING_CHARS)\n            return create_dataset_entry_qa(\n                mode=self.mode,\n                questions=[f\"{row['instruction']}{linking_char}{row['input']}\"],\n                answers=[row[\"output\"]],\n            )\n\n    def __len__(self) -> int:\n        return len(self.rows)\n\n    def __getitem__(self, index: int) -> DatasetEntry:\n        dialogue = self.rows[index]\n        return dialogue\n\n\nclass GPTeacher_Roleplay(Dataset):\n    def __init__(self, cache_dir: str | Path, mode: str = \"sft\") -> None:\n        super().__init__()\n        self.rows = []\n        if mode not in (\"sft\", \"rl\"):\n            raise NotImplementedError(f\"Currently only the modes 'sft' and 'rl' are implemented. Received {mode}.\")\n        self.mode = mode\n        saved_path = Path(cache_dir) / \"gpteacher_roleplay__json\"\n        file_name = \"gpteacher_roleplay.json\"\n        if os.path.exists(saved_path):\n            with open(saved_path / file_name, \"r\") as f:\n                data = json.load(f)\n        else:\n            req = requests.get(\n                \"https://raw.githubusercontent.com/teknium1/GPTeacher/main/Roleplay/roleplay-simple-deduped-roleplay-instruct.json\"\n            )\n            data = json.loads(req.text)\n            os.makedirs(saved_path, exist_ok=True)\n            with open(saved_path / file_name, \"w+\") as f:\n                json.dump(data, f)\n\n        for line in data:\n            if (conv := self._process_qa(line)) is not None:\n                self.rows.append(conv)\n\n    def _process_qa(self, row: dict[str, str]) -> DatasetEntry | None:\n        return create_dataset_entry_qa(\n            mode=self.mode,\n            questions=[row[\"instruction\"]],\n            answers=[row[\"response\"]],\n            context=row[\"input\"],\n        )\n\n    def __len__(self) -> int:\n        return len(self.rows)\n\n    def __getitem__(self, index: int) -> DatasetEntry:\n        dialogue = self.rows[index]\n        return dialogue\n", "model/model_training/custom_datasets/utils.py": "# mostly taken from\n# https://huggingface.co/datasets/gozfarb/ShareGPT_Vicuna_unfiltered/blob/main/optional_clean.py,\n# https://huggingface.co/datasets/ehartford/WizardLM_alpaca_evol_instruct_70k_unfiltered/blob/main/wizardlm_clean.py\nFILTER_BY_WORDS = [\n    \"as a language model\",\n    \"as an AI language model\",\n    \"As a large language model\",\n    \"As an AI \",\n    \"an AI language model you don't have\",\n    \"As an AI language model, I cannot\",\n    \"As an AI language model, I do not\",\n    \"As an AI language model, I am not able\",\n    \"As an AI language model, I don't have personal\",\n    \"I am an AI language model and do not\",\n    \"As an AI language model, I don't have\",\n    \"As an AI language model, I am only able\",\n    \"AI language model and I do not\",\n    \"As an AI language model, I cannot modify\",\n    \"As an AI language model, I do not\",\n    \"I know as an AI language model you don't have\",\n    \"as an AI language model, you cannot\",\n    \"I'm sorry, but as an AI language model\",\n    \"As an AI language model, I don't have\",\n    \"I'm an AI \",\n    \"I am an AI \",\n    \"As your dedicated AI language model\",\n    \"As a hypothetical AI\",\n    \"As a neutral AI\",\n    \"my knowledge cutoff\",\n    \"my knowledge cut off\",\n    \"As a machine\",\n    \"I cannot assist\",\n    \"I do not have personal preferences\",\n    \"I don't have personal preferences\",\n    \"Unfortunately, I cannot provide\",\n    \"I'm sorry, I cannot\",\n    \"I'm sorry, I cannot generate\",\n    \"AI cannot create or program\",\n    \"I'm afraid I cannot create\",\n    \"OpenAI\",\n]\n\n\ndef _filter_by_words(text: str, filter_words: list[str] | None = None) -> None | str:\n    \"\"\"Used to filter text that contains one of the `FILTER_BY_WORDS`. If so we return `None`\n       otherwise we return the string\n\n    Args:\n        text (str): text to be filtered\n\n    Returns:\n        None | str: filtered text\n    \"\"\"\n    filter_words = filter_words or FILTER_BY_WORDS\n    for word in filter_words:\n        if word.lower() in text.lower():\n            return None\n    return text\n", "model/model_training/custom_datasets/translation.py": "\"\"\"\n    List of translation dataset\n\n    GroNLP/divemt\n\n    fill in the blanks : https://huggingface.co/datasets/m_lama\n\n\"\"\"\nimport random\n\nfrom datasets import load_dataset\nfrom torch.utils.data import Dataset\n\n# postfix prompt\nTRANSLATION_PROMPT = {\n    \"zh\": [  # simplified or any chinese which was not mentioned\n        \"Translate to chinese simplified: {}\",\n        \"{}, translate to chinese\",\n        \"{} give me the chinese translation\",\n        \"\u7ffb\u8bd1\u6210\u4e2d\u6587: {}\",\n        \"{} \u8fd9\u53e5\u4e2d\u6587\u7ffb\u8bd1\u600e\u9ebd\u5199\uff1f\",\n        \"\u6211\u9700\u8981\u8fd9\u53e5\u8bdd\u7684\u4e2d\u6587\u7ffb\u8bd1: {}\",\n    ],\n    \"zh-tw\": [  # WMT code\n        \"{}. Translate to chinese traditional\",\n        \"{}, translate to chinese\",\n        \"{}. get chinese translation\",\n        \"\u4e2d\u6587\u7ffb\u8b6f: {}\",\n        \"\u5e6b\u6211\u7ffb\u8b6f\u6210\u4e2d\u6587: '{}'\",\n        \"{} \u9019\u53e5\u4e2d\u6587\u7ffb\u8b6f\u600e\u9ebc\u5beb\uff1f\",\n    ],\n    \"ja\": [\n        \"{}: help me translate to japanese\",\n        \"Need japanese translation: {}\",\n        \"{}: \u306b\u307b\u3093\u3054\u3084\u304f\u3092\u3088\u3053\u3059\",\n        \"{}: \u306b\u307b\u3093\u3054\u3084\u304f\u3092\u304a\u304f\u308c\",\n        \"{}: \u306b\u307b\u3093\u3054\u3084\u304f\u3092 \u3058\u3087\u3059\",\n        \"give me the japanese translation, {}\",\n    ],\n    \"de\": [\n        \"{}: translate to german\",\n        \"give me the german translation {}\",\n        \"I want german translation {}\",\n        \"{}, ins Deutsche \u00fcbersetzen\",\n        \"{}, \u00dcbersetzen ins Deutsche\",\n    ],\n    \"fr\": [\n        \"{}. translate to french\",\n        \"{} write in french\",\n        \"{} french translation\",\n        \"{} ,donnez moi la traduction fran\u00e7aise\",\n    ],\n    \"ko\": [\n        \"{}. translate to Korean\",\n        \"how do we write in korean: {}\",\n        \"give me the korean translation: {}\",\n        \"{}, \ud55c\uad6d\uc5b4 \ubc88\uc5ed\uc744 \ud574\uc8fc\uc138\uc694\",\n    ],\n    \"ms\": [\n        \"{} translate to malay\",\n        \"{} how do we write in Malay\",\n        \"{} give me the malay translation\",\n        \"{} , berikan saya terjemahan dalam bahasa melayu\",\n        \"{}, Jemahan di bahasa melayu\",\n        \"{}, jemahkan ayat ini kepada bahasa melayu\",\n    ],\n    \"en\": [\"{}. translate to english\", \"{} write in english\", \"english translation: '{}'\"],\n    \"ru\": [\"\u043f\u043e\u043c\u043e\u0433\u0438\u0442\u0435 \u043c\u043d\u0435 \u043f\u0435\u0440\u0435\u0432\u0435\u0441\u0442\u0438 \u044d\u0442\u043e \u043d\u0430 \u0440\u0443\u0441\u0441\u043a\u0438\u0439 : {}\", \"{} \u043f\u0435\u0440\u0435\u0432\u0435\u0441\u0442\u0438 \u043d\u0430 \u0440\u0443\u0441\u0441\u043a\u0438\u0439 \u044f\u0437\u044b\u043a\", \"russian translation: '{}'\"],\n    \"tr\": [\"{}. t\u00fcrk\u00e7eye \u00e7evi\u0307ri\u0307n\", \"{} write in turkish\", \"turkish translation: '{}'\", \"t\u00fcrk\u00e7eye \u00e7evi\u0307rmek: {}\"],\n    \"it\": [\"{}. translate to italian\", \"{} write in italian\", \"italian translation: '{}'\"],\n    \"nl\": [\"{}. translate to dutch\", \"{} write in dutch\", \"dutch translation: '{}'\"],\n    \"vi\": [\"{}. D\u1ecbch sang ti\u1ebfng vi\u1ec7t nam\", \"{} write in vietnamese\", \"vietnamese translation: '{}'\"],\n    \"ar\": [\"{}. translate to arabic\", \"{} write in arabic\", \"arabic translation: '{}'\"],\n    \"es\": [\"{}. translate to spanish\", \"{} write in spanish\", \"spanish translation: '{}'\"],\n    \"hi\": [\"{}. translate to hindi\", \"{}. translate to bengali\", \"{} write in hindi\", \"bengali translation: '{}'\"],\n    \"uk\": [\n        \"{}. translate to ukrainian\",\n        \"{} write in ukrainian\",\n        \"ukrainian translation: '{}'\",\n        \"\u043f\u0435\u0440\u0435\u043a\u043b\u0430\u0434\u0438 \u0443\u043a\u0440\u0430\u0457\u043d\u0441\u044c\u043a\u043e\u044e \u043c\u043e\u0432\u043e\u044e: {}\",\n        \"\u043f\u0435\u0440\u0435\u043a\u043b\u0430\u0434\u0438 \u043d\u0430 \u0443\u043a\u0440\u0430\u0457\u043d\u0441\u044c\u043a\u0443 \u043c\u043e\u0432\u0443: {}\",\n        \"{} \u043f\u0435\u0440\u0435\u043a\u043b\u0430\u0434\u0438 \u0443\u043a\u0440\u0430\u0457\u043d\u0441\u044c\u043a\u043e\u044e\",\n    ],\n}\n\n\nclass TranslationPair(Dataset):\n    def __init__(self, mix_prob=0.2) -> None:\n        super().__init__()\n        self.pairs = []\n        self.length = -1\n        self.mix_prob = mix_prob\n\n    def __len__(self):\n        if self.length < 0:\n            self.length = len(self.pairs)\n        return len(self.pairs)\n\n    def __getitem__(self, index):\n        if random.random() < self.mix_prob and index > 5 and index < (self.length - 5):\n            additional = random.randint(0, 10) - 5\n            while additional == index:\n                additional = random.randint(0, 10) - 5\n\n            history_text = self.pairs[additional + index]\n            question, answer = self.pairs[index]\n            return history_text + [question, answer]\n\n        return self.pairs[index]\n\n\nclass WMT2019(TranslationPair):\n    def __init__(self, pair=\"zh-en\", split=\"train\", mix_prob=0.2, maximum_size=100000) -> None:\n        super().__init__(mix_prob=mix_prob)\n        dataset = load_dataset(\"wmt19\", pair)[split]\n        self.pairs = []\n        src, tgt = pair.split(\"-\")\n        for row in dataset:\n            row = row[\"translation\"]\n            if random.random() > 0.5:\n                source = random.choice(TRANSLATION_PROMPT[tgt]).format(row[src])\n                self.pairs.append((source, row[tgt]))\n            else:  # translating in reverse direction\n                source = random.choice(TRANSLATION_PROMPT[src]).format(row[tgt])\n                self.pairs.append((source, row[src]))\n            # WMT is very large, reduce preprocessing time\n            if len(self.pairs) > maximum_size:\n                break\n\n\nclass DiveMT(TranslationPair):\n    REMAP = {\"tur\": \"tr\", \"ita\": \"it\", \"ukr\": \"uk\", \"nld\": \"nl\", \"vie\": \"vi\", \"ara\": \"ar\"}\n\n    def __init__(self, split=\"train\", mix_prob=0.2) -> None:\n        super().__init__(mix_prob=mix_prob)\n        dataset = load_dataset(\"GroNLP/divemt\", \"main\")[split]\n        tgt, src = \"tgt_text\", \"src_text\"\n        for row in dataset:\n            # ISO 639-2\n            lang_code_2 = row[\"subject_id\"].split(\"_\")[0]\n            lang_code = self.REMAP[lang_code_2]\n            if lang_code not in TRANSLATION_PROMPT:\n                continue\n\n            if random.random() > 0.5:\n                source = random.choice(TRANSLATION_PROMPT[lang_code]).format(row[src])\n                self.pairs.append((source, row[tgt]))\n            else:  # translating in reverse direction\n                lang_code = \"en\"\n                source = random.choice(TRANSLATION_PROMPT[lang_code]).format(row[tgt])\n                self.pairs.append((source, row[src]))\n\n\nclass TEDTalk(TranslationPair):\n    # NOTE: DO NOT use chinese pair, mix with traditional and cantonese, not clean\n\n    def __init__(self, pair=\"de-ja\", split=\"train\", year=\"2016\", mix_prob=0.2, maximum_size=100000) -> None:\n        super().__init__(mix_prob=mix_prob)\n        dataset = load_dataset(\"ted_talks_iwslt\", language_pair=pair.split(\"-\"), year=year)[split]\n        src, tgt = pair.split(\"-\")\n        for row in dataset:\n            row = row[\"translation\"]\n            if random.random() > 0.5:\n                source = random.choice(TRANSLATION_PROMPT[tgt]).format(row[src])\n                self.pairs.append((source, row[tgt]))\n            else:  # translating in reverse direction\n                source = random.choice(TRANSLATION_PROMPT[src]).format(row[tgt])\n                self.pairs.append((source, row[src]))\n            # WMT is very large\n            if len(self.pairs) > maximum_size:\n                break\n", "model/model_training/custom_datasets/rank_datasets.py": "import random\nfrom collections import defaultdict\nfrom typing import List\n\nimport numpy as np\nfrom datasets import load_dataset\nfrom torch.utils.data import Dataset\n\nSEED = 2020\n\n\nclass SHPDataset(Dataset):\n    \"\"\"\n    Dataset class to load stanfordnlp/SHP for Reward Modeling\n    \"\"\"\n\n    name = \"SHP\"\n\n    def __init__(self, split: str | list[str] | None, max_answers: int = 5):\n        super().__init__()\n\n        self.questions = []\n        self.answers = []\n\n        if not isinstance(split, list):\n            split = [split]\n        dataset_splits = load_dataset(\"stanfordnlp/SHP\", split=split)\n\n        answers_by_id = defaultdict(dict)\n        history_by_id = dict()\n        for split in dataset_splits:\n            for row in split:\n                post_id = row[\"post_id\"]\n                history_by_id[post_id] = row[\"history\"]\n                answers_by_id[post_id][row[\"human_ref_A\"]] = row[\"score_A\"]\n                answers_by_id[post_id][row[\"human_ref_B\"]] = row[\"score_B\"]\n\n        for post_id, history in history_by_id.items():\n            self.questions.append(history)\n            answers = answers_by_id[post_id]\n            # Sort answer dict with the highest score first (hence the prefactor -1).\n            # Then take only the first `max_answers` elements (usually there are just\n            # 2, but there are examples where we have more)\n            answers_sorted = [x[0] for x in sorted(answers.items(), key=lambda x: -1 * x[1])]\n            self.answers.append(answers_sorted[:max_answers])\n\n    def __len__(self):\n        return len(self.questions)\n\n    def __getitem__(self, index):\n        return [self.questions[index]], self.answers[index]\n\n\nclass HellaSwagDataset(Dataset):\n    \"\"\"\n    Dataset class to use data from https://arxiv.org/pdf/1905.07830.pdf\n    for Reward modeling\n\n    Note: In order to disable dialog-formatting None is returned as context.\n    \"\"\"\n\n    name = \"hellaswag\"\n\n    def __init__(self, split: str | list[str] | None, seed: int = SEED) -> None:\n        super().__init__()\n\n        np.random.seed(seed)\n        self.dataset_list = []\n        if not isinstance(split, List):\n            split = [split]\n        dataset = load_dataset(\"AlekseyKorshuk/hellaswag\", split=split)\n        for data in dataset:\n            for item in data:\n                context = item.get(\"ctx\")\n                endings = item.get(\"endings\")\n                selected = endings.pop(item.get(\"label\"))\n                ordered_ends = [selected, np.random.choice(endings)]\n                self.dataset_list.append({\"context\": context, \"completions\": ordered_ends})\n\n    def __len__(self) -> int:\n        return len(self.dataset_list)\n\n    def __getitem__(self, idx) -> tuple[str | None, list[list]]:\n        context, completions = self.dataset_list[idx].values()\n        return None, [context + c for c in completions]\n\n\nclass HFDataset(Dataset):\n    \"\"\"\n    Dataset class to use data from openai/summarize_from_feedback for Reward modeling.\n    Summaries ranked by overall score.\n    \"\"\"\n\n    name = \"open_ai_summarize_from_feedback\"\n\n    def __init__(self, split: str | list[str] | None = None, subset: str = \"axis\") -> None:\n        super().__init__()\n        # axis subset contains splits 'test' and 'validation'\n        # comparisons subset contains splits 'train' and 'validation'\n        if not isinstance(split, List):\n            split = [split]\n        dataset = load_dataset(\"openai/summarize_from_feedback\", subset, split=split)\n        self.subset = subset\n\n        # in axis subset the summaries are ranked\n        self.axis_post_ids = []\n        self.axis_post_dict = defaultdict(dict)\n\n        # in comparison subset we have each time a pair\n        # of summarizations and then the chosen out of 2\n        self.comparisons = []\n\n        if subset == \"axis\":\n            self._handle_axis(dataset)\n        else:\n            self._handle_comparisons(dataset)\n\n    def _handle_comparisons(self, dataset):\n        for data in dataset:\n            for item in data:\n                choice = item[\"choice\"]  # indicates the preferred summary\n                full_post = item[\"info\"][\"post\"]\n                summaries = [item[\"summaries\"][choice][\"text\"], item[\"summaries\"][1 - choice][\"text\"]]\n                self.comparisons.append([[full_post], summaries])\n\n    def _handle_axis(self, dataset):\n        for data in dataset:\n            for item in data:\n                if item[\"summary\"].get(\"axes\").get(\"overall\") is not None:\n                    post_id = item.get(\"info\")[\"id\"]\n                    if post_id not in self.axis_post_ids:\n                        self.axis_post_ids.append(post_id)\n                        item_content = item[\"info\"][\"post\"] or item[\"info\"][\"article\"]\n                        self.axis_post_dict[post_id].update({\"post\": item_content, \"summaries\": [item[\"summary\"]]})\n                    else:\n                        self.axis_post_dict[post_id][\"summaries\"].append(item[\"summary\"])\n\n    def __len__(self):\n        if self.subset == \"axis\":\n            return len(self.axis_post_ids)\n        return len(self.comparisons)\n\n    def __getitem__(self, idx):\n        post, summaries = self.post_dict[self.post_ids[idx]].values()\n        summaries = sorted(summaries, key=lambda x: x[\"axes\"][\"overall\"], reverse=True)\n        summaries = [summary[\"text\"] for summary in summaries]\n        return [post], summaries\n\n\nclass AugmentedOA(Dataset):\n    def __init__(self, json_filename: str, split: str = \"train\") -> None:\n        super().__init__()\n        import json\n\n        assert split in (\"train\", \"val\")\n\n        pairs = []\n        with open(json_filename, \"r\", encoding=\"utf-8\") as f:\n            for line in f:\n                data = json.loads(line)\n                if data[\"split\"] == split:\n                    augmented = data[\"augmented\"]\n                    if split == \"val\":  # disable augmentation during validation\n                        augmented = []\n                    pairs.append((data[\"prefixes\"], data[\"responses\"], augmented))\n        self.pairs = pairs\n\n    def __len__(self):\n        return len(self.pairs)\n\n    def __getitem__(self, idx):\n        prefixes, user_answer_ranks, bad_samples = self.pairs[idx]\n        # we want to prevent modifying user_answer_ranks\n        rank = user_answer_ranks\n        if len(bad_samples) > 0:\n            additional = random.choice(bad_samples)\n            rank = user_answer_ranks + [additional]\n\n        return prefixes, rank\n\n\nclass AnthropicRLHF(Dataset):\n    name = \"anthropic_rlhf\"\n\n    @staticmethod\n    def _split_dialogue(text: str) -> list[tuple[str, str]]:\n        lines = text.split(\"\\n\\n\")\n\n        dialogue: list[tuple[str, str]] = []\n\n        # go over messages and combine consecutive messages from the\n        # same speaker (OA v1 expects alternating roles)\n        role = None\n        messages = []\n        for line in lines:\n            if line.startswith(\"Human:\"):\n                speaker = \"Human\"\n                message = line[7:]\n            elif line.startswith(\"Assistant:\"):\n                speaker = \"Assistant\"\n                message = line[11:]\n            else:\n                continue\n            if role != speaker:\n                if role is not None:\n                    dialogue.append((role, \"\\n\".join(messages)))\n                    messages = []\n                role = speaker\n            messages.append(message.strip())\n\n        if role is not None and len(messages) > 0:\n            dialogue.append((role, \"\\n\".join(messages)))\n\n        return dialogue\n\n    def __init__(self, split: str = \"train\") -> None:\n        super().__init__()\n        assert split in (\"train\", \"test\")\n        self.split = split\n        self.data = []\n        dataset = load_dataset(\"Anthropic/hh-rlhf\")[split]\n\n        for entry in dataset:\n            chosen = entry[\"chosen\"]\n\n            if \"Assistant\" not in chosen:\n                continue\n\n            rejected = entry[\"rejected\"]\n            chosen = self._split_dialogue(chosen)\n            rejected = self._split_dialogue(rejected)\n            assert rejected[0][0] == \"Human\" and chosen[0][0] == \"Human\"\n\n            # only very few items have non matching lengths\n            if len(rejected) == len(chosen):\n                prefix = [line for (speaker, line) in chosen[:-1]]\n                good_reply = chosen[-1][1]  # last part of dialog, the text\n                bad_reply = rejected[-1][1]  # last part of dialog, the text\n                self.data.append((prefix, [good_reply, bad_reply]))\n\n    def __len__(self) -> int:\n        return len(self.data)\n\n    def __getitem__(self, index: int) -> tuple[str, list[str]]:\n        return self.data[index]\n", "model/model_training/custom_datasets/dialogue_collator.py": "import random\nfrom dataclasses import dataclass\nfrom typing import Optional, Union\n\nimport numpy as np\nimport torch\nfrom model_training.custom_datasets.formatting import (\n    QA_SPECIAL_TOKENS,\n    DatasetEntryLm,\n    DatasetEntrySft,\n    format_pairs,\n    format_system_prefix,\n)\nfrom torch.nn import functional as F\nfrom transformers.tokenization_utils_base import PaddingStrategy, PreTrainedTokenizerBase, TruncationStrategy\n\n\n@dataclass\nclass DialogueDataCollator:\n    \"\"\"\n    Expects a list of texts corresponding to a sequence of [question, answer, question, answer, ...] pairs.\n    \"\"\"\n\n    tokenizer: PreTrainedTokenizerBase\n    padding: Union[bool, str, PaddingStrategy] = True\n    max_length: Optional[int] = None\n    mix_length_threshold: Optional[int] = 256\n    mix_probability: Optional[float] = 0.6\n    pad_to_multiple_of: Optional[int] = None\n    samples_mixing: Optional[bool] = False\n    random_offset_probability: Optional[float] = 0.5\n    label_masking: bool = True\n    use_system_prefix: bool = False\n    system_prefix: str = None\n    use_system_tag: bool = False\n    system_property_dropout: float = 0.5\n    system_add_length: bool = True\n\n    def __post_init__(self):\n        assert self.tokenizer.eos_token\n\n        if self.use_system_prefix:\n            assert self.system_prefix\n            self.system_prefix = self.tokenizer.encode(\n                format_system_prefix(self.system_prefix, self.tokenizer.eos_token),\n                add_special_tokens=False,\n                return_tensors=\"np\",\n            )[0]\n            self.max_length = self.max_length - len(self.system_prefix)\n\n    def process_one(self, messages, return_length=False):\n        total_short_context_one = 0\n        if random.random() < self.random_offset_probability and not isinstance(messages, DatasetEntryLm):\n            truncation = TruncationStrategy.DO_NOT_TRUNCATE\n            max_length = None\n        else:\n            truncation = TruncationStrategy.LONGEST_FIRST\n            max_length = self.max_length\n\n        pretrain_dataset = False\n        if isinstance(messages, DatasetEntrySft):\n            messages = messages.get_formatted(\n                eos_token=self.tokenizer.eos_token,\n                use_system_tag=self.use_system_tag,\n                system_property_dropout=self.system_property_dropout,\n                system_add_length=self.system_add_length,\n            )\n        elif isinstance(messages, DatasetEntryLm):\n            messages = messages.text\n            pretrain_dataset = True\n        else:\n            messages = list(messages)\n            messages = format_pairs(messages, self.tokenizer.eos_token)\n\n        flatten_message = self.tokenizer(\n            \"\".join(messages),\n            max_length=max_length,\n            truncation=truncation,\n            padding=False,\n        )\n\n        if pretrain_dataset:\n            label_mask = np.ones(len(flatten_message.input_ids), dtype=bool)\n            return flatten_message, label_mask, 0\n\n        if return_length:\n            return min(len(flatten_message.input_ids), self.max_length)\n\n        message_indices: Optional[list[int]] = None\n        if self.label_masking:\n            # message_change_indices = np.cumsum([len(x) for x in messages])\n            # for each token an integer indicating the index of the message it belongs to. Just to create the label mask.\n            # Label mask is true when predicting a token that is part of the answer, false otherwise.\n            # TEXT:             Question: Hello, how are you? Answer: I am fine. Question: What is your name? Answer: My name is John.\n            # MESSAGE_INDICES:  0         0      0   0   0    1       1 1  1     2         2    2  2    2     3       3  3    3  3\n            # LABEL_MASK:       0         0      0   0   0    1       1 1  1     0         0    0  0    0     1       1  1    1  1\n\n            # If no result in next, we are predicting the last termination token(s)\n            # message_indices = list(\n            #     map(\n            #         lambda x: next((i for i, val in enumerate(message_change_indices) if val >= x)),\n            #         list(map(lambda x: x[1], flatten_message.offset_mapping)),\n            #     )\n            # )\n\n            prompter_token_id = self.tokenizer.convert_tokens_to_ids(QA_SPECIAL_TOKENS[\"Question\"])\n            assistant_token_id = self.tokenizer.convert_tokens_to_ids(QA_SPECIAL_TOKENS[\"Answer\"])\n            assert prompter_token_id >= 0 and assistant_token_id >= 0\n\n            message_indices = []\n            i = -1\n            for x in flatten_message.input_ids:\n                if x in (prompter_token_id, assistant_token_id):\n                    i += 1\n                message_indices.append(i)\n\n        input_length = len(flatten_message.input_ids)\n        if self.max_length and input_length > self.max_length:\n            offset = random.randint(0, input_length - self.max_length)\n            for k in flatten_message.keys():\n                v = flatten_message[k]\n                if isinstance(v, list) and len(v) == input_length:\n                    flatten_message[k] = v[offset : offset + self.max_length]\n            if message_indices:\n                message_indices = message_indices[offset : offset + self.max_length]\n\n        if self.label_masking:\n            label_mask = np.array(list(map(lambda x: x % 2 == 1, message_indices)))\n        else:\n            label_mask = np.ones(len(flatten_message.input_ids), dtype=bool)\n\n        label_mask[-1] = False  # make sure last token is inactive, has an effect only when truncating\n\n        if len(flatten_message.input_ids) < self.mix_length_threshold and self.samples_mixing:\n            total_short_context_one += len(flatten_message.input_ids)\n\n        return {k: v for k, v in flatten_message.items() if k != \"offset_mapping\"}, label_mask, total_short_context_one\n\n    def __call__(self, features):\n        flatten_messages = []\n        label_masks = []\n        total_short_context = 0\n        for messages in features:\n            flatten_message, label_mask, total_short_context_one = self.process_one(messages)\n            flatten_messages.append(flatten_message)\n            label_masks.append(label_mask)\n            total_short_context += total_short_context_one\n\n        # packing\n        if total_short_context > 2 and self.samples_mixing:\n            _flatten_messages, _label_masks = [], []\n            prev_short_msg, prev_short_mask = None, None\n            for flatten_msg, label_mask in zip(flatten_messages, label_masks):\n                if len(flatten_msg.input_ids) < self.mix_length_threshold and random.random() > self.mix_probability:\n                    if prev_short_msg is not None:\n                        for key in flatten_msg.keys():\n                            flatten_msg[key] += prev_short_msg[key]\n                            flatten_msg[key] = flatten_msg[key][: self.max_length]\n                        label_mask = np.concatenate([label_mask, prev_short_mask])\n                        _label_masks.append(label_mask[: self.max_length])\n                        _flatten_messages.append(flatten_msg)\n                        # reset\n                        prev_short_msg, prev_short_mask = None, None\n                    else:\n                        # prime\n                        prev_short_msg, prev_short_mask = flatten_msg, label_mask\n                else:\n                    _label_masks.append(label_mask)\n                    _flatten_messages.append(flatten_msg)\n            if prev_short_msg is not None:\n                for key in flatten_msg.keys():\n                    flatten_msg[key] += prev_short_msg[key]\n                    flatten_msg[key] = flatten_msg[key][: self.max_length]\n                label_mask = np.concatenate([label_mask, prev_short_mask])[: self.max_length]\n                _label_masks.append(label_mask)\n                _flatten_messages.append(flatten_msg)\n\n            label_masks = _label_masks\n            flatten_messages = _flatten_messages\n\n        if self.use_system_prefix:\n            flatten_messages = [\n                {\n                    \"input_ids\": np.concatenate([self.system_prefix, flatten_msg[\"input_ids\"]]),\n                    \"attention_mask\": np.concatenate(\n                        [np.ones_like(self.system_prefix).astype(bool), flatten_msg[\"attention_mask\"]]\n                    ),\n                }\n                for flatten_msg in flatten_messages\n            ]\n            label_masks = [\n                np.concatenate([np.zeros_like(self.system_prefix).astype(bool), label_mask])\n                for label_mask in label_masks\n            ]\n\n        batch = self.tokenizer.pad(\n            flatten_messages,\n            padding=self.padding,\n            pad_to_multiple_of=self.pad_to_multiple_of,\n            return_tensors=\"pt\",\n        )\n        dim = batch.input_ids.shape[-1]\n\n        batch[\"label_masks\"] = torch.stack(\n            [F.pad(torch.tensor(x), (0, dim - len(x)), value=False) for x in label_masks]\n        )\n        batch[\"targets\"] = torch.roll(batch.input_ids, -1, -1)\n\n        return batch\n", "model/model_training/custom_datasets/pretrain_datasets.py": "\"\"\"\n   Datasets for LM objective pre-training aimed to prevent catastrophic forgetting during fine-tuning\n\"\"\"\nfrom pathlib import Path\nfrom typing import Optional\n\nfrom datasets import load_dataset\nfrom model_training.custom_datasets.formatting import DatasetEntryLm\nfrom torch.utils.data import Dataset\n\n\nclass PretrainDataset(Dataset):\n    def __init__(\n        self,\n        dataset_name: str,\n        split: str,\n        text_column_name: str,\n        cache_dir: str | Path,\n        mode: str = \"sft\",\n        max_chunk_size: Optional[int] = 64 * 1024,\n    ) -> None:\n        super().__init__()\n\n        assert mode in (\"sft\", \"rm\", \"rl\")\n        self.mode = mode\n        self.max_chunk_size = max_chunk_size\n        self.dataset = load_dataset(dataset_name, cache_dir=cache_dir)[split]\n        self.text_column_name = text_column_name\n\n        # split long entries into chunks smaller than max_chunk_size\n        self.index_map = []\n        for i, entry in enumerate(self.dataset):\n            text_len = len(entry[self.text_column_name])\n            for segment_begin in range(0, text_len, max_chunk_size):\n                segment_end = min(segment_begin + max_chunk_size, text_len)\n                self.index_map.append((i, segment_begin, segment_end))\n\n    def __len__(self) -> int:\n        return len(self.index_map)\n\n    def __getitem__(self, index) -> DatasetEntryLm:\n        i, segment_begin, segment_end = self.index_map[index]\n        text = self.dataset[i][self.text_column_name][segment_begin:segment_end]\n        return DatasetEntryLm(text=text)\n\n\nclass RedPajama(PretrainDataset):\n    name = \"red_pajama\"\n\n    def __init__(\n        self,\n        cache_dir: str | Path,\n        mode: str = \"sft\",\n        max_chunk_size: Optional[int] = 64 * 1024,\n    ) -> None:\n        super().__init__(\n            dataset_name=\"togethercomputer/RedPajama-Data-1T-Sample\",\n            split=\"train\",\n            text_column_name=\"text\",\n            cache_dir=cache_dir,\n            mode=mode,\n            max_chunk_size=max_chunk_size,\n        )\n\n\nclass FanFics(PretrainDataset):\n    name = \"fanfics\"\n\n    def __init__(\n        self,\n        cache_dir: str | Path,\n        mode: str = \"sft\",\n        max_chunk_size: Optional[int] = 64 * 1024,\n    ) -> None:\n        super().__init__(\n            dataset_name=\"atom-in-the-universe/fanfics-10k-50k\",\n            split=\"train\",\n            text_column_name=\"TEXT\",\n            cache_dir=cache_dir,\n            mode=mode,\n            max_chunk_size=max_chunk_size,\n        )\n", "model/model_training/custom_datasets/extra_rm_datasets.py": "from model_training.custom_datasets.rank_datasets import AnthropicRLHF, HellaSwagDataset, SHPDataset\nfrom torch.utils.data import Dataset\n\n\ndef load_anthropic_rlhf() -> tuple[Dataset, Dataset]:\n    train = AnthropicRLHF(split=\"train\")\n    validation = AnthropicRLHF(split=\"test\")\n    return train, validation\n\n\ndef load_shp() -> tuple[Dataset, Dataset]:\n    train = SHPDataset(split=\"train\")\n    validation = SHPDataset(split=\"validation\")\n    return train, validation\n\n\ndef load_hellaswag() -> tuple[Dataset, Dataset]:\n    train = HellaSwagDataset(split=\"train\")\n    validation = HellaSwagDataset(split=\"validation\")\n    return train, validation\n", "model/model_training/custom_datasets/summarization.py": "\"\"\"\n    Summarize different spectrum of documents\n\"\"\"\nimport random\n\nimport numpy as np\nfrom datasets import load_dataset\nfrom torch.utils.data import Dataset\n\nSUMMARIZATION_SPECIAL_TOKENS = {\"Text\": \"\", \"Summary\": [\"TL;DR:\", \"Summarize this\", \"Give me the summary\"]}\n\nSUMMARY_SPECIAL_PROMPT = {\n    \"multi_news\": [\"Summarize in bullet points\", \"Generate summary in list of points\"],\n    \"xsum\": [\"Give me summary in one sentence\", \"Short TLDR\", \"Give me a concise summary\"],\n    \"samsum\": [\"TLDR;\", \"Summarize this dialogue\", \"Summarize dialogue\"],\n}\n\nsummarization_config_mapping = {\n    \"cnn_dailymail\": (\n        \"cnn_dailymail\",\n        \"3.0.0\",\n    ),\n    \"samsum\": (\"samsum\",),\n    \"xsum\": (\"xsum\",),\n    \"multi_news\": (\"multi_news\",),\n    \"scitldr\": (\n        \"scitldr\",\n        \"AIC\",\n    ),\n    \"billsum\": (\"billsum\",),\n    \"reddit\": (\"reddit\",),\n    \"tldr_news\": (\"JulesBelveze/tldr_news\",),  # need to fix : JulesBelveze/tldr_news\n    \"debate_sum\": (\"Hellisotherpeople/DebateSum\",),  # Hellisotherpeople/DebateSum\n}\n\nsummarization_name_mapping = {\n    \"cnn_dailymail\": (\"article\", \"highlights\"),\n    \"samsum\": (\"dialogue\", \"summary\"),\n    \"xsum\": (\"document\", \"summary\"),\n    \"multi_news\": (\"document\", \"summary\"),\n    \"scitldr\": (\"source\", \"target\"),\n    \"billsum\": (\"text\", \"summary\"),\n    \"reddit\": (\"content\", \"summary\"),\n    \"tldr_news\": (\"content\", \"headline\"),\n    \"debate_sum\": (\"Full-Document\", \"Extract\"),\n}\n\n\ndef index_summary_default(text, summary):\n    return text.replace(\"\\n\\n\", \"\\n\"), summary\n\n\ndef index_summary_merge(text, summary):\n    return \" \".join(text), \" \".join(summary)\n\n\nclass SummarizationDataset(Dataset):\n    def __init__(self, dataset, cache_dir, split, max_words=512):\n        self.name = dataset\n        if (dataset in [\"billsum\", \"tldr_news\"]) and (split == \"validation\"):\n            split = \"test\"\n        self.dataset = load_dataset(*summarization_config_mapping[dataset], cache_dir=cache_dir, split=split)\n        self.text_column, self.summary_column = summarization_name_mapping[dataset]\n        self.preprocess_fn = index_summary_merge if dataset == \"scitldr\" else index_summary_default\n        self.max_words = max_words\n\n    def __len__(self):\n        return len(self.dataset)\n\n    def __getitem__(self, idx):\n        data = self.dataset[idx]\n        text, summary = data[self.text_column], data[self.summary_column]\n        text, summary = self.preprocess_fn(text, summary)\n        if self.name in SUMMARY_SPECIAL_PROMPT:\n            prompt = random.choice(SUMMARIZATION_SPECIAL_TOKENS[\"Summary\"])\n        else:\n            prompt = random.choice(SUMMARIZATION_SPECIAL_TOKENS[\"Summary\"])\n\n        context = \"\".join([SUMMARIZATION_SPECIAL_TOKENS[\"Text\"], \" \".join(text.split(\" \")[: self.max_words]), prompt])\n        return (context, summary)\n\n\nSUMMARIZATION_PROMPTS = [\n    \"Please summarize the following content:\\n{}\",\n    \"Write me a summary for the following article:\\n{}\",\n    \"Kindly sum up the following information: {}\",\n    \"Please summarize the following text for me:\\n{}\",\n    \"Give me a summary of the following text:\\n\\n{}\",\n    \"Describe the following information in brief: {}\",\n    \"Will you kindly summarize the following paragraph for me?\\n{}\",\n    \"Summarize this: {}\",\n    \"TLDR this: {}\",\n    \"{}\\nTLDR;\",\n    \"{}\\n\\nTL;DR\",\n    \"{} tl;dr:\",\n    \"{}\\nPlease summarize the content above\",\n    \"{} Please summarize the preceding statements.\",\n]\n\n\nclass HFSummaryPairs(Dataset):\n    \"\"\"\n    Simplified version of the HFSummary class which uses the original examples\n    of the OpenAI dataset.\n    https://huggingface.co/datasets/openai/summarize_from_feedback\n    \"\"\"\n\n    def __init__(self, split=\"train\", mode=\"sft\", conf_threshold=-1) -> None:\n        super().__init__()\n        assert split in (\"train\", \"valid1\", \"valid2\", \"test\")\n        assert mode in (\"sft\", \"rm\", \"rl\")\n        self.mode = mode\n\n        self.posts = []\n        self.summary_pairs = []\n\n        major_split = split if \"train\" == split else \"validation\"\n        dataset = load_dataset(\"openai/summarize_from_feedback\", \"comparisons\")[major_split]\n        for data in dataset:\n            if (\n                \"extra\" in data\n                and \"confidence\" in data[\"extra\"]\n                and data[\"extra\"][\"confidence\"] is not None\n                and conf_threshold > data[\"extra\"][\"confidence\"]\n            ):\n                print(\"skipping {}\".format(data[\"info\"][\"id\"]))\n                continue\n\n            if split != \"train\" and split != data[\"split\"]:\n                continue\n\n            if \"article\" in data[\"info\"] and data[\"info\"][\"article\"] is not None:\n                context = data[\"info\"][\"article\"]\n            elif \"post\" in data[\"info\"]:\n                context = data[\"info\"][\"post\"]\n\n            self.posts.append(context)\n            pos, neg = (0, 1) if data[\"choice\"] == 0 else (1, 0)\n            self.summary_pairs.append((data[\"summaries\"][pos][\"text\"].strip(), data[\"summaries\"][neg][\"text\"].strip()))\n\n    def __len__(self) -> int:\n        return len(self.posts)\n\n    def __getitem__(self, index: int) -> tuple | list:\n        if index < 0 or index >= len(self.posts):\n            raise IndexError()\n\n        context = self.posts[index]\n        # return pairs of comparison\n        good_summary, bad_summary = self.summary_pairs[index]\n        prompt = random.choice(SUMMARIZATION_PROMPTS)\n\n        # pair very big\n        # we are going to do some sampling\n        # not optimal but good for now\n        if self.mode == \"sft\":\n            return [prompt.format(context), good_summary]\n        elif self.mode == \"rl\":\n            return (prompt.format(context),)\n        elif self.mode == \"rm\":\n            return [prompt.format(context)], [good_summary, bad_summary]\n\n        raise RuntimeError(f\"Unsupported mode '{self.mode}'\")\n\n\nclass HFSummary(Dataset):\n    \"\"\"\n    Human feedback data from OpenAI\n    https://github.com/openai/summarize-from-feedback\n    https://huggingface.co/datasets/openai/summarize_from_feedback\n\n    labeling method : pair comparison, 0 or 1\n\n    \"\"\"\n\n    def __init__(self, split=\"train\", mode=\"sft\", conf_threshold=-1, max_comparison_per_sample=5) -> None:\n        super().__init__()\n        assert split in (\"train\", \"valid1\", \"valid2\", \"test\")\n        assert mode in (\"sft\", \"rm\", \"rl\")\n        self.mode = mode\n        summaries = {}\n        # using prompt as our index will allows us\n        # to add additional generated prompt later\n        self.index2summary = {}\n        self.max_comparison_per_sample = max_comparison_per_sample\n        major_split = split if \"train\" == split else \"validation\"\n        dataset = load_dataset(\"openai/summarize_from_feedback\", \"comparisons\")[major_split]\n        for data in dataset:\n            if (\n                \"extra\" in data\n                and \"confidence\" in data[\"extra\"]\n                and data[\"extra\"][\"confidence\"] is not None\n                and conf_threshold > data[\"extra\"][\"confidence\"]\n            ):\n                print(\"skipping {}\".format(data[\"info\"][\"id\"]))\n                continue\n\n            if split != \"train\" and split != data[\"split\"]:\n                continue\n\n            if \"article\" in data[\"info\"] and data[\"info\"][\"article\"] is not None:\n                context = data[\"info\"][\"article\"]\n            elif \"post\" in data[\"info\"]:\n                context = data[\"info\"][\"post\"]\n\n            if context not in self.index2summary:\n                self.index2summary[len(self.index2summary)] = context\n\n            if context not in summaries:\n                summaries[context] = []\n\n            pos, neg = (0, 1) if data[\"choice\"] == 0 else (1, 0)\n            summaries[context].append((data[\"summaries\"][pos][\"text\"].strip(), data[\"summaries\"][neg][\"text\"].strip()))\n\n        ranked_summaries = {}\n        for context, summary_comparison_pairs in summaries.items():\n            ranks = self.get_sorted_ranks(summary_comparison_pairs)\n            ranked_summaries[context] = ranks\n        self.summaries = ranked_summaries\n\n    @staticmethod\n    def get_sorted_ranks(comparison_pairs):\n        # Create a dictionary to keep track of the counts of each element\n\n        counts = {}\n        for pair in comparison_pairs:\n            if pair[0] not in counts:\n                counts[pair[0]] = 0\n            if pair[1] not in counts:\n                counts[pair[1]] = 0\n            counts[pair[0]] += 1\n\n        # Create a list of tuples, where each tuple contains an element and its count\n        elements_counts = [(element, count) for element, count in counts.items()]\n\n        # Sort the list of tuples by count in descending order\n        elements_counts.sort(key=lambda x: x[1], reverse=True)\n\n        # Create a list of elements in order of their counts\n        sorted_elements = [element for element, count in elements_counts]\n\n        return sorted_elements\n\n    def __len__(self) -> int:\n        return len(self.index2summary)\n\n    def __getitem__(self, index) -> tuple | list:\n        if index < 0 or index >= len(self.index2summary):\n            raise IndexError()\n\n        context = self.index2summary[index]\n        # return pairs of comparison\n        rows = self.summaries[context]\n        prompt = random.choice(SUMMARIZATION_PROMPTS)\n\n        # pair very big\n        # we are going to do some sampling\n        # not optimal but good for now\n        if self.mode == \"sft\":\n            return [prompt.format(context), rows[0]]\n        elif self.mode == \"rl\":\n            return (prompt.format(context),)\n        elif self.mode == \"rm\":\n            valid_idx = np.random.choice(len(rows), self.max_comparison_per_sample)\n            return [prompt.format(context)], [r for idx, r in enumerate(rows) if idx in valid_idx]\n\n        raise RuntimeError(f\"Unsupported mode '{self.mode}'\")\n", "model/model_training/custom_datasets/formatting.py": "import re\nfrom enum import Enum\nfrom itertools import zip_longest\nfrom random import random, shuffle\nfrom typing import Literal, Optional\n\nfrom pydantic import BaseModel, validator\nfrom pydantic.fields import ModelField\n\nQA_SPECIAL_TOKENS = {\n    \"Question\": \"<|prompter|>\",\n    \"Answer\": \"<|assistant|>\",\n    \"System\": \"<|system|>\",\n    \"StartPrefix\": \"<|prefix_begin|>\",\n    \"EndPrefix\": \"<|prefix_end|>\",\n}\n\n\ndef format_system_prefix(prefix, eos_token):\n    return \"{}{}{}\".format(\n        QA_SPECIAL_TOKENS[\"System\"],\n        prefix,\n        eos_token,\n    )\n\n\ndef compute_length(s: str) -> int:\n    return len(re.findall(r\"\\w+\", s)) // 5 + 1\n\n\nclass Mode(str, Enum):\n    sft = \"sft\"\n    rm = \"rm\"\n    rl = \"rl\"\n\n\nclass Role(str, Enum):\n    prompter = \"prompter\"\n    assistant = \"assistant\"\n\n\nclass Utterance(BaseModel):\n    text: str\n    role: Role\n    lang: str | None = None\n    quality: float | None = None\n    humor: float | None = None\n    creativity: float | None = None\n    context: str | None = None\n\n    @validator(\"quality\", \"humor\", \"creativity\")\n    def between_0_1(cls, v, field: ModelField) -> float:\n        if v is not None and not (0 <= v <= 1):\n            raise ValueError(f\"Field {field.name} must be between 0 and 1. Received: {v}\")\n        return v\n\n    def system_tag(\n        self,\n        eos_token: str,\n        enabled: bool = True,\n        property_dropout: float = 0.0,\n        add_length: bool = True,\n    ) -> str:\n        if not enabled:\n            return \"\"\n\n        properties: list[tuple[float | str]] = []\n        for k, v in self.dict().items():\n            if v is not None and k in [\"lang\", \"quality\", \"humor\", \"creativity\"]:\n                properties.append((k, v))\n\n        if add_length:\n            properties.append((\"length\", compute_length(self.text)))\n\n        shuffle(properties)\n\n        # ensure that potentially multi-line conext field comes last\n        if self.context:\n            properties.append((\"context\", self.context))\n\n        fragments: list[str] = []\n        for k, v in properties:\n            if random() < property_dropout:\n                continue\n\n            if isinstance(v, float):\n                fragments.append(f\"{k}: {v:0.1f}\")\n            elif isinstance(v, str):\n                if not v.isspace():  # ignore whitespace-only values\n                    fragments.append(f\"{k}: {v}\")\n            else:\n                fragments.append(f\"{k}: {v}\")\n\n        if len(fragments) == 0:\n            return \"\"\n\n        content = \"\\n\".join(fragments)\n        return f\"{QA_SPECIAL_TOKENS['System']}{content}\\n{eos_token}\"\n\n\nclass DatasetEntry(BaseModel):\n    pass\n\n\nclass DatasetEntryLm(DatasetEntry):\n    \"\"\"Language modelling dataset entry\"\"\"\n\n    text: str | None = None\n\n\nclass DatasetEntrySft(DatasetEntry):\n    \"\"\"Supervised fine-tuning conversation dataset entry\"\"\"\n\n    conversation: list[Utterance]\n    system_message: Optional[str]\n\n    def get_formatted(\n        self,\n        eos_token: str,\n        use_system_tag: bool = False,\n        system_property_dropout: float = 0.5,\n        system_add_length: bool = False,\n    ) -> list[str]:\n        output: list[str] = []\n\n        for i, m in enumerate(self.conversation):\n            if m.role == Role.prompter:\n                if use_system_tag and i + 1 < len(self.conversation):\n                    a = self.conversation[i + 1]\n                    assert a.role == Role.assistant\n                    system_tag = a.system_tag(\n                        eos_token=eos_token,\n                        property_dropout=system_property_dropout,\n                        add_length=system_add_length,\n                    )\n                else:\n                    system_tag = \"\"\n                if i == 0 and self.system_message:\n                    output.append(\n                        f\"{QA_SPECIAL_TOKENS['System']}{self.system_message}{eos_token}{QA_SPECIAL_TOKENS['Question']}{m.text}{eos_token}{system_tag}\"\n                    )\n                else:\n                    output.append(f\"{QA_SPECIAL_TOKENS['Question']}{m.text}{eos_token}{system_tag}\")\n            else:\n                output.append(f\"{QA_SPECIAL_TOKENS['Answer']}{m.text}{eos_token}\")\n\n        return output\n\n\nclass DatasetEntryRm(DatasetEntry):\n    \"\"\"Reward model dataset entry (conversation history + ranked replies)\"\"\"\n\n    messages: list[Utterance] | None  # conversation history\n    replies: list[Utterance]  # ordered reply variants, best first\n\n    def get_formatted(\n        self,\n        eos_token: str,\n        use_system_tag: bool = False,\n        system_property_dropout: float = 0.5,\n        system_add_length: bool = False,\n        max_replies: int = 5,\n    ) -> tuple[str, list[str]]:\n        reply_variants = self.replies\n        if len(reply_variants) > max_replies:\n            reply_variants = reply_variants[:max_replies]\n\n        # special handling for non-dialogue datasets like Hellaswag\n        if self.messages is None or len(self.messages) == 1 and self.messages[0] is None:\n            prefix = \"\"\n            replies = [r.text + eos_token for r in reply_variants]\n            return prefix, replies\n\n        assert len(self.messages) > 0 and self.messages[-1].role == Role.prompter\n\n        # format conversation history (prefix)\n        prefix_messages: list[str] = []\n        for i, m in enumerate(self.messages):\n            if m.role == Role.prompter:\n                prefix_messages.append(f\"{QA_SPECIAL_TOKENS['Question']}{m.text}{eos_token}\")\n            else:\n                if use_system_tag:\n                    assert m.role == Role.assistant\n                    system_tag = m.system_tag(\n                        eos_token=eos_token,\n                        property_dropout=system_property_dropout,\n                        add_length=system_add_length,\n                    )\n                else:\n                    system_tag = \"\"\n                prefix_messages.append(f\"{system_tag}{QA_SPECIAL_TOKENS['Answer']}{m.text}{eos_token}\")\n        prefix = \"\".join(prefix_messages)\n\n        #  format reply variants\n        replies: list[str] = []\n        for r in reply_variants:\n            assert r.role == Role.assistant\n            if use_system_tag:\n                system_tag = r.system_tag(\n                    eos_token=eos_token,\n                    property_dropout=system_property_dropout,\n                    add_length=system_add_length,\n                )\n            else:\n                system_tag = \"\"\n            replies.append(f\"{system_tag}{QA_SPECIAL_TOKENS['Answer']}{r.text}{eos_token}\")\n\n        return prefix, replies\n\n\ndef create_dataset_entry_qa(\n    mode: Mode | Literal[\"sft\", \"rm\", \"rl\"],\n    questions: list[str],\n    answers: list[str] | list[list[str]],\n    context: Optional[str] = None,\n    lang: Optional[str] = None,\n) -> DatasetEntry:\n    \"\"\"Helper function to create DatasetEntry objects (DatasetEntrySft or DatasetEntryRm) for simple\n    Q&A datasets.\"\"\"\n    if mode == Mode.sft:\n        messages: list[Utterance] = []\n\n        for q, a in zip_longest(questions, answers):\n            messages.append(Utterance(text=q, role=Role.prompter, lang=lang))\n            if isinstance(a, list):\n                a = a[0]\n            messages.append(Utterance(text=a, role=Role.assistant, lang=lang, context=context))\n\n        return DatasetEntrySft(conversation=messages)\n    elif mode == Mode.rm:\n        if len(questions) != 1:\n            raise RuntimeError(\"QA dataset entry factory does not support multi-turn conversation for the RM case.\")\n\n        if len(answers) == 1 and isinstance(answers[0], list):\n            answers = answers[0]\n\n        assert isinstance(answers, list) and len(answers) > 1 and isinstance(answers[0], str)\n        conversation_history = [Utterance(text=questions[0], role=Role.prompter, lang=lang)]\n        reply_variants = [Utterance(text=a, role=Role.assistant, lang=lang, context=context) for a in answers]\n        return DatasetEntryRm(messages=conversation_history, replies=reply_variants)\n    # elif mode == Mode.rl:\n    else:\n        raise RuntimeError(f\"Unsupported mode ({mode=})\")\n\n\ndef format_pairs(\n    pairs: list[str],\n    eos_token: str,\n    add_initial_reply_token: bool = False,\n) -> list[str]:\n    assert isinstance(pairs, list)\n    conversations = [\n        \"{}{}{}\".format(QA_SPECIAL_TOKENS[\"Question\" if i % 2 == 0 else \"Answer\"], pairs[i], eos_token)\n        for i in range(len(pairs))\n    ]\n    if add_initial_reply_token:\n        conversations.append(QA_SPECIAL_TOKENS[\"Answer\"])\n    return conversations\n\n\ndef format_rl_text(pairs: list[str]) -> str:\n    # convert question answer pairs to only the prefix prompt for RLHF\n    return \"{}{}{}\".format(QA_SPECIAL_TOKENS[\"Question\"], pairs[0], QA_SPECIAL_TOKENS[\"Answer\"])\n\n\ndef format_reply(text: str, eos_token: str) -> str:\n    return \"{}{}{}\".format(QA_SPECIAL_TOKENS[\"Answer\"], text, eos_token)\n", "model/model_training/custom_datasets/oasst_dataset.py": "from pathlib import Path\nfrom typing import Iterable, Literal, Optional\n\nfrom model_training.custom_datasets.formatting import DatasetEntrySft, Role, Utterance\nfrom oasst_data import ExportMessageNode, read_dataset_message_trees, read_message_trees, visit_threads_depth_first\nfrom oasst_data.schemas import ExportMessageTree\nfrom torch import Generator\nfrom torch.utils.data import Dataset, random_split\n\n\nclass ListDataset(Dataset):\n    def __init__(self, data: list):\n        super().__init__()\n        self.data = data\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, index):\n        return self.data[index]\n\n\ndef load_oasst_export(\n    input_file_path: Optional[str | Path] = None,\n    hf_dataset_name: Optional[str] = \"OpenAssistant/oasst1\",\n    val_split: float = 0.2,\n    lang: str = \"en\",\n    top_k: Optional[int] = None,\n    manual_seed: int = 287631038922,\n    data_path: str | Path = None,\n    mode: Literal[\"sft\", \"rm\", \"rl\"] = \"sft\",\n) -> tuple[ListDataset, ListDataset]:\n    if mode not in (\"sft\", \"rm\", \"rl\"):\n        raise ValueError(f\"Unknown dataset mode: {mode}\")\n\n    lang_codes: list[str] = lang.split(\",\")\n\n    generator = Generator()\n    generator.manual_seed(manual_seed)\n\n    tree_iter: Iterable[ExportMessageTree] = None\n    if input_file_path:\n        if not isinstance(input_file_path, Path):\n            input_file_path = Path(input_file_path)\n        if not input_file_path.is_absolute() and data_path:\n            if not isinstance(data_path, Path):\n                data_path = Path(data_path)\n            input_file_path = data_path / input_file_path\n        tree_iter = read_message_trees(input_file_path)\n    elif hf_dataset_name:\n        tree_iter = read_dataset_message_trees(hf_dataset_name, split=\"train+validation\")\n    else:\n        raise RuntimeError(\"Either `input_file_path` or `hf_dataset_name` must be specified.\")\n\n    threads_per_tree = []\n    for tree in tree_iter:\n        if tree.tree_state != \"ready_for_export\" or not tree.prompt.review_result or tree.prompt.lang not in lang_codes:\n            continue\n\n        if mode in (\"sft\", \"rm\"):\n            if tree.tree_state != \"ready_for_export\":\n                continue\n        elif mode == \"rl\":\n            if tree.tree_state not in (\"ready_for_export\", \"prompt_lottery_waiting\"):\n                continue\n\n        # extract all threads up to last assistant reply\n        threads: list[list[ExportMessageNode]] = []\n\n        def thread_filter(thread: list[ExportMessageNode]) -> bool:\n            if any(m.deleted or m.synthetic for m in thread):\n                return False\n\n            if top_k is not None:\n                for i, m in enumerate(thread):\n                    if m.role == \"assistant\":\n                        if m.rank is None:\n                            if i > 0 and len(thread[i - 1].replies) > 1:\n                                return False\n                        elif m.rank >= top_k:\n                            return False\n            return True\n\n        def leaf_filter(thread: list[ExportMessageNode]) -> bool:\n            if mode == \"sft\":\n                # in SFT mode `not thread[-1].replies` finds nodes without children (leaves).\n                # We are interested in those which are role='assistant' but some trees don't end on assistant nodes\n                # but have prompter leaves .. we want to use those trees too .. e.g. remove the last prompter message(s)\n                # so that they end with assistant. The `thread[-2].replies[0] == thread[-1]` check makes sure that only\n                # the FIRST prompter reply is added .. e.g. the parent does not appear multiple times and we can use\n                # pop() to remove superfluous prompter leaf node later.\n                return (\n                    len(thread) > 1\n                    and not thread[-1].replies\n                    and (thread[-1].role == \"assistant\" or thread[-2].replies[0] == thread[-1])\n                    and thread_filter(thread)\n                )\n            elif mode == \"rm\":\n                # for reward models we use thread-fragments ending on prompter messages as prefix and\n                # their (ranked) replies as possible continuations.\n                if thread[-1].replies is None:\n                    return False\n                return (\n                    thread[-1].role == \"prompter\"\n                    and len([r for r in thread[-1].replies if r.rank is not None]) > 1\n                    and thread_filter(thread)\n                )\n            elif mode == \"rl\":\n                # during rl we are interested in all possible prefixes ending in prompter messages\n                return thread[-1].role == \"prompter\" and not any(m.deleted or m.synthetic for m in thread)\n\n            raise RuntimeError()\n\n        visit_threads_depth_first(tree.prompt, visitor=threads.append, predicate=leaf_filter)\n        if mode == \"sft\":\n            for t in threads:\n                if t[-1].role == \"prompter\":\n                    t.pop()\n\n        threads_per_tree.append(threads)\n\n    def process_thread(thread: list[ExportMessageNode]):\n        if mode == \"sft\":\n            # ensure roles are strictly alternating between prompter and assistant\n            assert all(m.role == \"prompter\" for m in thread[0::2]) and all(m.role == \"assistant\" for m in thread[1::2])\n            conversation: list[Utterance] = [\n                Utterance(\n                    text=m.text,\n                    role=Role.prompter if m.role == \"prompter\" else Role.assistant,\n                    lang=m.lang,\n                    quality=m.get_label_value(\"quality\"),\n                    humor=m.get_label_value(\"humor\"),\n                    creativity=m.get_label_value(\"creativity\"),\n                )\n                for m in thread\n            ]\n            return DatasetEntrySft(conversation=conversation)\n        elif mode == \"rm\":\n            prefix = [m.text for m in thread]\n            replies = [r for r in thread[-1].replies if r.role == \"assistant\" and r.rank is not None]\n            replies = sorted(replies, key=lambda r: r.rank)\n            replies = [r.text for r in replies]\n            return (prefix, replies)\n        elif mode == \"rl\":\n            return ([m.text for m in thread],)\n\n        raise RuntimeError()\n\n    # split on tree basis, messages from same tree must not end up in different splits\n    trees = ListDataset(threads_per_tree)\n    splits = random_split(trees, lengths=[1.0 - val_split, val_split], generator=generator)\n\n    def flatten(ds: ListDataset) -> ListDataset:\n        return ListDataset([process_thread(thread) for tree_threads in ds for thread in tree_threads])\n\n    train = flatten(splits[0])\n    val = flatten(splits[1])\n\n    if input_file_path:\n        print(f\"OASST JSONL file {str(input_file_path)}: {len(train)=}, {len(val)=}\")\n    else:\n        print(f\"OASST HF dataset {hf_dataset_name}: {len(train)=}, {len(val)=}\")\n\n    return train, val\n", "model/model_training/custom_datasets/__init__.py": "\"\"\"\n    High level functions for model training\n\"\"\"\nfrom typing import Optional\n\nimport numpy as np\nfrom model_training.custom_datasets.extra_rm_datasets import load_anthropic_rlhf, load_hellaswag, load_shp\nfrom model_training.custom_datasets.instruction import (\n    INSTRUCTION_DATASETS,\n    RAG_DATASETS,\n    InstructionDataset,\n    RAGDataset,\n)\nfrom model_training.custom_datasets.oasst_dataset import load_oasst_export\nfrom model_training.custom_datasets.pretrain_datasets import FanFics, RedPajama\nfrom model_training.custom_datasets.prompt_dialogue import DolphinMix, Gpt4All, OrcaChat, load_oig_file\nfrom model_training.custom_datasets.qa_datasets import (\n    SODA,\n    AlpacaGpt4,\n    DatabricksDolly15k,\n    Dolly15kMultilingual,\n    GPTeacher_Roleplay,\n    JokeExplaination,\n    QADataset,\n    SODADialogue,\n    TranslatedQA,\n    Vicuna,\n    WebGPT,\n    WizardEvolInstructV2,\n    load_alpaca_dataset,\n)\nfrom model_training.custom_datasets.rank_datasets import AugmentedOA\nfrom model_training.custom_datasets.summarization import HFSummary, HFSummaryPairs, SummarizationDataset\nfrom model_training.custom_datasets.toxic_conversation import ProsocialDialogue, ProsocialDialogueExplaination\nfrom model_training.custom_datasets.translation import WMT2019, DiveMT, TEDTalk\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import Dataset, Subset\n\nQA_DATASETS = list(QADataset.DATASET_FORMAT_MAPPING.keys())\n\nSUMMARIZATION_DATASETS = [\n    \"xsum\",\n    \"cnn_dailymail\",\n    \"samsum\",\n    \"multi_news\",\n    \"scitldr\",\n    \"billsum\",\n    \"debate_sum\",\n    \"tldr_news\",\n]\n\nOTHER = [\n    \"prosocial_dialogue\",\n    \"explain_prosocial\",\n    \"private_tuning\",\n    \"oa_translated\",\n]\n\nRL_DATASETS = [\n    \"oasst_export\",\n    \"webgpt\",\n    \"private_tuning\",\n    \"alpaca\",\n    \"hf_summary\",\n    \"hf_summary_pairs\",\n    \"vicuna\",\n]\n\nRM_DATASETS = [\n    \"oasst_export\",\n    \"augment_oasst\",\n    \"anthropic_rlhf\",\n    \"hf_summary\",\n    \"hf_summary_pairs\",\n    \"shp\",\n    \"hellaswag\",\n    \"webgpt\",\n]\n\n\ndef train_val_dataset(dataset, val_split=0.2) -> tuple[Dataset, Dataset | None]:\n    if val_split == 0:\n        return dataset, None\n\n    train_idx, val_idx = train_test_split(\n        list(range(len(dataset))), test_size=val_split, random_state=666, shuffle=True\n    )\n    return Subset(dataset, train_idx), Subset(dataset, val_idx)\n\n\ndef get_one_dataset(\n    conf,\n    dataset_name: str,\n    val_split: float = 0.2,\n    data_path: str = None,\n    mode: str = \"sft\",\n    max_val_set: Optional[int] = None,\n    **kwargs,\n) -> tuple[Dataset, Dataset | None]:\n    if mode == \"rl\":\n        assert dataset_name in RL_DATASETS, f\"Dataset {dataset_name} not supported for RL\"\n\n    if mode == \"rm\":\n        assert dataset_name in RM_DATASETS, f\"Dataset {dataset_name} not supported for reward modeling\"\n\n    data_path = data_path or conf.cache_dir\n    dataset_name = dataset_name.lower()\n\n    if dataset_name in QA_DATASETS:\n        dataset = QADataset(dataset_name, data_path, \"train\")\n        if not dataset.no_val:\n            eval = QADataset(dataset_name, data_path, \"validation\")\n            train = dataset\n    elif dataset_name in SUMMARIZATION_DATASETS:\n        dataset = SummarizationDataset(dataset_name, data_path, \"train\")\n        if dataset_name != \"debate_sum\":\n            eval = SummarizationDataset(dataset_name, data_path, \"validation\")\n            train = dataset\n    elif dataset_name in INSTRUCTION_DATASETS:\n        dataset_args = INSTRUCTION_DATASETS[dataset_name]\n        dataset = InstructionDataset(name=dataset_name, cache_dir=data_path, split=\"train\", **(dataset_args | kwargs))\n    elif \"ted_trans\" in dataset_name:\n        language_pair = dataset_name.split(\"_\")[-1]\n        dataset = TEDTalk(pair=language_pair, split=\"train\")\n    elif \"wmt2019\" in dataset_name:\n        language_pair = dataset_name.split(\"_\")[-1]\n        train = WMT2019(pair=language_pair, split=\"train\")\n        eval = WMT2019(pair=language_pair, split=\"validation\")\n    elif dataset_name == \"dive_mt\":\n        dataset = DiveMT()\n    elif dataset_name == \"webgpt\":\n        dataset = WebGPT(mode=mode)\n    elif dataset_name in (\"alpaca\", \"code_alpaca\"):\n        train, eval = load_alpaca_dataset(dataset_name, val_split=val_split, cache_dir=data_path, **kwargs)\n    elif dataset_name == \"gpt4all\":\n        dataset = Gpt4All(mode=mode, cache_dir=data_path)\n    elif dataset_name == \"prosocial_dialogue\":\n        dataset = ProsocialDialogue(cache_dir=data_path, split=\"train\")\n    elif dataset_name == \"explain_prosocial\":\n        dataset = ProsocialDialogueExplaination(cache_dir=data_path, split=\"train\")\n    elif dataset_name == \"soda\":\n        dataset = SODA(data_path, **kwargs)\n    elif dataset_name == \"soda_dialogue\":\n        dataset = SODADialogue(data_path)\n    elif dataset_name == \"joke\":\n        dataset = JokeExplaination(data_path)\n    elif dataset_name == \"oa_translated\":\n        # TODO make val_split lower..? by saganos\n        dataset = TranslatedQA(data_path)\n    elif dataset_name == \"vicuna\":\n        dataset = Vicuna(cache_dir=data_path, **kwargs)\n    elif dataset_name == \"wizard_evol_instruct_v2\":\n        dataset = WizardEvolInstructV2(cache_dir=data_path, **kwargs)\n    elif dataset_name == \"oasst_export\":\n        train, eval = load_oasst_export(data_path=data_path, val_split=val_split, mode=mode, **kwargs)\n    elif dataset_name == \"hf_summary\":\n        train = HFSummary(split=\"train\", mode=mode)\n        eval = HFSummary(split=\"valid1\", mode=mode)\n    elif dataset_name == \"hf_summary_pairs\":\n        train = HFSummaryPairs(split=\"train\", mode=mode)\n        eval = HFSummaryPairs(split=\"valid1\", mode=mode)\n    elif dataset_name == \"augment_oasst\":\n        # reward model mode only\n        assert mode == \"rm\"\n        train = AugmentedOA(data_path + \"/\" + kwargs[\"input_file_path\"], split=\"train\")\n        eval = AugmentedOA(data_path + \"/\" + kwargs[\"input_file_path\"], split=\"val\")\n    elif dataset_name == \"oig_file\":\n        train, eval = load_oig_file(val_split=val_split, **kwargs)\n    elif dataset_name == \"anthropic_rlhf\":\n        train, eval = load_anthropic_rlhf()\n    elif dataset_name == \"shp\":\n        train, eval = load_shp()\n    elif dataset_name == \"hellaswag\":\n        train, eval = load_hellaswag()\n    elif dataset_name == \"dolly15k\":\n        dataset = DatabricksDolly15k(cache_dir=data_path, mode=mode, **kwargs)\n    elif dataset_name == \"dolly15k_multilingual\":\n        dataset = Dolly15kMultilingual(cache_dir=data_path, mode=mode, **kwargs)\n    elif dataset_name == \"alpaca_gpt4\":\n        dataset = AlpacaGpt4(cache_dir=data_path, mode=mode, **kwargs)\n    elif dataset_name == \"red_pajama\":\n        dataset = RedPajama(cache_dir=data_path, mode=mode, **kwargs)\n    elif dataset_name == \"fanfics\":\n        dataset = FanFics(cache_dir=data_path, mode=mode, **kwargs)\n    elif dataset_name == \"gpteacher_roleplay\":\n        dataset = GPTeacher_Roleplay(cache_dir=data_path, mode=mode, **kwargs)\n    elif dataset_name == \"orca-chat\":\n        dataset = OrcaChat(cache_dir=data_path, **kwargs)\n    elif dataset_name == \"dolphin-mix\":\n        dataset = DolphinMix(cache_dir=data_path, **kwargs)\n    elif dataset_name in RAG_DATASETS.keys():\n        dataset = RAGDataset(dataset_name, cache_dir=data_path, **kwargs)\n    else:\n        raise ValueError(f\"Unknown dataset {dataset_name}\")\n\n    # if eval not already defined\n    if not (\"eval\" in locals() and \"train\" in locals()):\n        train, eval = train_val_dataset(dataset, val_split=val_split)\n\n    if eval and max_val_set and len(eval) > max_val_set:\n        subset_indices = np.random.choice(len(eval), size=max_val_set, replace=False)\n        eval = Subset(eval, subset_indices)\n\n    return train, eval\n", "model/model_training/custom_datasets/prompt_dialogue.py": "import gzip\nimport json\nimport re\nfrom pathlib import Path\nfrom typing import List, Mapping, Optional, Sequence, Union\n\nimport requests\nfrom datasets import load_dataset\nfrom model_training.custom_datasets.formatting import DatasetEntrySft, Role, Utterance\nfrom model_training.custom_datasets.oasst_dataset import ListDataset\nfrom model_training.custom_datasets.utils import _filter_by_words\nfrom torch import Generator, randperm\nfrom torch.utils.data import Dataset, random_split\n\n\ndef load_oig_file(\n    source_url: str,\n    val_split: float = 0.2,\n    cache_dir: str = \".cache/\",\n    no_cache: bool = False,\n    max_count: Optional[int] = None,\n    min_length: Optional[int] = 1000,\n    manual_seed: int = 287631038922,\n) -> tuple[ListDataset, ListDataset]:\n    generator = Generator()\n    generator.manual_seed(manual_seed)\n\n    file_name = source_url[source_url.rindex(\"/\") + 1 :]\n\n    cache_dir = Path(cache_dir)\n    cache_dir.mkdir(parents=True, exist_ok=True)\n    local_path = cache_dir / file_name\n\n    # download file if not cached\n    if not local_path.exists() or local_path.stat().st_size == 0 or no_cache:\n        print(f\"downloading {source_url} to {local_path}\")\n        r = requests.get(source_url, stream=True)\n        with local_path.open(mode=\"wb\") as fd:\n            for chunk in r.iter_content(chunk_size=1024 * 1024):\n                fd.write(chunk)\n\n    # read the file\n    if local_path.suffix == \".gz\":\n        file_in = gzip.open(str(local_path), mode=\"tr\", encoding=\"UTF-8\")\n    else:\n        file_in = local_path.open(\"r\", encoding=\"UTF-8\")\n\n    with file_in:\n        # read one message tree per line\n        conversations = []\n        for line in file_in:\n            data = json.loads(line)\n\n            text = data.get(\"text\")\n            if not text:\n                continue\n\n            fragments = re.split(r\"\\s*(\\<(?:human|bot)\\>)\\:\\s*\", text)\n\n            role = None\n            turns = []\n            s = \"\"\n            for x in fragments:\n                if x == \"<human>\" or x == \"<bot>\":\n                    if role != x:\n                        if role is not None:\n                            turns.append(s)\n                            s = \"\"\n                        role = x\n                    continue\n                s += x.strip()\n            turns.append(s)\n            if role == \"<bot>\" and len(turns) % 2 == 0:\n                conversations.append(turns)\n\n    # shuffling with torch generator (not modifying python's standard random state)\n    random_order = randperm(len(conversations), generator=generator).tolist()\n    conversations = [conversations[i] for i in random_order]\n\n    # concatenate multiple QA pairs until total length is above min_length\n    if min_length is not None:\n        merged_conversations = []\n        merge = []\n        for x in conversations:\n            if sum(len(s) for s in merge) >= min_length:\n                merged_conversations.append(merge)\n                merge = []\n            merge += x\n        merged_conversations.append(merge)\n        conversations = merged_conversations\n\n    # if max count was specified select a random subset\n    if max_count is not None:\n        conversations = conversations[:max_count]\n\n    avg_turn_count = sum(len(x) for x in conversations) / len(conversations)\n    splits = random_split(conversations, lengths=[1.0 - val_split, val_split], generator=generator)\n\n    train = ListDataset(splits[0])\n    val = ListDataset(splits[1])\n\n    print(f\"OIG data {str(local_path)}: {len(train)=}, {len(val)=} ({avg_turn_count=:.1f})\")\n\n    return train, val\n\n\nclass Gpt4All(Dataset):\n    def __init__(self, mode: str, cache_dir: str = None) -> None:\n        super().__init__()\n        self.mode = mode\n        dataset = load_dataset(\n            \"Nebulous/gpt4all_pruned\",\n            data_files=\"data_singleround_pruned_3.jsonl\",\n            cache_dir=cache_dir,\n        )\n        self.rows = [\n            [row[\"prompt\"], row[\"response\"]]\n            for row in dataset[\"train\"]\n            if _filter_by_words(row[\"prompt\"]) and _filter_by_words(row[\"response\"])\n        ]\n\n        dataset_multi = load_dataset(\n            \"Nebulous/gpt4all_pruned\",\n            data_files=\"data_multiround_pruned_3.jsonl\",\n            cache_dir=cache_dir,\n        )\n        for row in dataset_multi[\"train\"][\"conversation\"]:\n            if (processed_conversation := self.process_conversation(row)) is not None:\n                self.rows.append(processed_conversation)\n\n    @staticmethod\n    def process_conversation(conv: list[dict[str, None | str]]) -> list[str] | None:\n        dialogue = []\n        role = None\n        messages = []\n        # drop conversations that start with Bot\n        if conv[0][\"Bot\"] is not None:\n            return None\n        for line in conv:\n            if line[\"User\"] and line[\"Bot\"]:\n                raise ValueError(\"Unexpected dataformat. Should receive only User or Bot data, not both.\")\n            if (message := line[\"User\"]) is not None:\n                speaker = \"Human\"\n            elif (message := line[\"Bot\"]) is not None:\n                speaker = \"Assistant\"\n            else:\n                continue\n            if _filter_by_words(message) is None:\n                return None\n            if role != speaker:\n                if role is not None:\n                    dialogue.append(\"\\n\".join(messages))\n                    messages = []\n                role = speaker\n            messages.append(message.strip())\n\n        if role is not None and len(messages) > 0:\n            dialogue.append(\"\\n\".join(messages))\n        return dialogue\n\n    def __len__(self):\n        return len(self.rows)\n\n    def __getitem__(self, index: int) -> list[str] | tuple[str]:\n        dialogue: list = self.rows[index]\n        if self.mode == \"sft\":\n            return dialogue\n        elif self.mode == \"rl\":\n            return tuple(dialogue[:-1])\n\n\nclass OrcaChat(Dataset):\n    name = \"orca-chat\"\n\n    def __init__(self, data_files: Union[List[str], str] = \"orca-chat-gpt4.json\", cache_dir: str = None) -> None:\n        self.dataset = load_dataset(\"shahules786/orca-chat\", split=\"train\", data_files=data_files, cache_dir=cache_dir)\n\n    def __len__(self):\n        return len(self.dataset)\n\n    def __getitem__(self, idx):\n        conversation, instruction = [self.dataset[idx][key] for key in (\"conversation\", \"instruction\")]\n        conversation = [(item[\"input\"], item[\"output\"]) for item in conversation]\n        conversation = list(sum(conversation, ()))\n        conv_utt: list[Utterance] = [\n            (\n                Utterance(\n                    text=conv,\n                    role=Role.prompter if i % 2 == 0 else Role.assistant,\n                )\n            )\n            for i, conv in enumerate(conversation)\n        ]\n\n        return DatasetEntrySft(conversation=conv_utt, system_message=instruction)\n\n\nclass DolphinMix(Dataset):\n    name = \"dophin-mix\"\n\n    def __init__(\n        self,\n        cache_dir: Optional[str] = None,\n        num_samples: Optional[int] = None,\n        max_char_len: int = 8000,\n        seed: int = 42,\n        data_files: Union[\n            str, Sequence[str], Mapping[str, Union[str, Sequence[str]]]\n        ] = \"flan5m-alpaca-uncensored.jsonl\",\n        split: str = \"train\",\n    ):\n        # flan5m-alpaca-uncensored.jsonl has total entries 2840090\n        self.dataset = load_dataset(\"ehartford/dolphin\", data_files=data_files, cache_dir=cache_dir)\n        self.dataset = self.dataset[split].shuffle(seed).flatten_indices()\n        if num_samples:\n            self.dataset = self.dataset.select(range(num_samples))\n        self.max_char_len = max_char_len\n        instructions = sorted(set([item[\"instruction\"] for item in self.dataset]))\n\n        self.conversations = []\n        for inst in instructions:\n            data_sample = self.dataset.filter(lambda example: example[\"instruction\"] == inst)\n            conversation_len = len(inst)\n            conversation = []\n            for entry in data_sample:\n                input, output = entry[\"input\"], entry[\"output\"]\n                conversation.append({\"input\": input, \"output\": output})\n                conversation_len += len(input) + len(output)\n                if conversation_len >= self.max_char_len:\n                    self.conversations.append({\"conversation\": conversation, \"instruction\": inst})\n                    conversation_len = len(inst)\n                    conversation = []\n\n            if len(conversation) > 0:\n                self.conversations.append({\"conversation\": conversation, \"instruction\": inst})\n\n    def __len__(self) -> int:\n        return len(self.conversations)\n\n    def __getitem__(self, idx) -> DatasetEntrySft:\n        conversation, instruction = [self.conversations[idx][key] for key in (\"conversation\", \"instruction\")]\n        conversation = [(item[\"input\"], item[\"output\"]) for item in conversation]\n        conversation = list(sum(conversation, ()))\n        conv_utt: list[Utterance] = [\n            (\n                Utterance(\n                    text=conv,\n                    role=Role.prompter if i % 2 == 0 else Role.assistant,\n                )\n            )\n            for i, conv in enumerate(conversation)\n        ]\n\n        return DatasetEntrySft(conversation=conv_utt, system_message=instruction)\n", "model/model_training/custom_datasets/ranking_collator.py": "from dataclasses import dataclass\nfrom typing import Optional, Union\n\nfrom model_training.custom_datasets.formatting import DatasetEntryRm\nfrom transformers.tokenization_utils_base import BatchEncoding, PaddingStrategy, PreTrainedTokenizerBase\n\nfrom .formatting import format_pairs, format_reply\n\n\n@dataclass\nclass RankingDataCollator:\n    \"\"\"\n    Data collator that will dynamically pad the inputs for multiple choice received.\n    \"\"\"\n\n    tokenizer: PreTrainedTokenizerBase\n    padding: Union[bool, str, PaddingStrategy] = True\n    max_length: Optional[int] = None\n    min_prefix_length: int = 256\n    pad_to_multiple_of: Optional[int] = None\n    max_replies: Optional[int] = 5\n    use_system_tag: bool = False\n    system_property_dropout: float = 0.5\n    system_add_length: bool = True\n\n    def process_one(\n        self,\n        example: tuple[str | list[str] | None, list[str]] | DatasetEntryRm,\n        return_length: int = False,\n    ) -> list[BatchEncoding]:\n        assert self.tokenizer.eos_token\n        eos = self.tokenizer.eos_token\n\n        if isinstance(example, DatasetEntryRm):\n            prefix, replies = example.get_formatted(\n                eos_token=eos,\n                use_system_tag=self.use_system_tag,\n                system_property_dropout=self.system_property_dropout,\n                system_add_length=self.system_add_length,\n                max_replies=self.max_replies,\n            )\n        else:\n            messages, replies = example\n\n            if self.max_replies:\n                assert self.max_replies > 1, \"max_replies parameter must be > 1 or None\"\n                if len(replies) > self.max_replies:\n                    replies = replies[: self.max_replies]\n\n            if messages is None or len(messages) == 1 and messages[0] is None:\n                # special handling for non-dialogue datasets like Hellaswag\n                prefix = \"\"\n                replies = [r + eos for r in replies]\n            else:\n                # append eos token to each messages\n                prefix = \"\".join(format_pairs(messages, eos_token=eos))\n                replies = [format_reply(r, eos_token=eos) for r in replies]\n\n        prefix_tokens = self.tokenizer(prefix, padding=False, truncation=False)\n        reply_tokens = [self.tokenizer(r, padding=False, truncation=False) for r in replies]\n\n        prefix_len = len(prefix_tokens.input_ids)\n        suffix_len = max(len(r.input_ids) for r in reply_tokens)\n        if return_length:\n            return min(prefix_len + suffix_len, self.max_length)\n\n        for r in reply_tokens:\n            max_prefix_len = (\n                prefix_len\n                if self.max_length is None\n                else max(self.min_prefix_length, self.max_length - len(r.input_ids))\n            )\n            max_suffix_len = len(r.input_ids) if self.max_length is None else self.max_length - max_prefix_len\n\n            for k in r.keys():\n                r[k] = prefix_tokens[k][-max_prefix_len:] + r[k][:max_suffix_len]\n\n        return reply_tokens\n\n    def __call__(\n        self, examples: list[tuple[str | list[str] | None, list[str]]] | list[DatasetEntryRm]\n    ) -> tuple[list[BatchEncoding], list[int]]:\n        flat_tokenized, cu_lens = [], [0]\n        n_samples = 0\n        for example in examples:\n            tokenized = self.process_one(example)\n            flat_tokenized.extend(tokenized)\n\n            n_samples += len(tokenized)\n            cu_lens.append(n_samples)\n\n        batch = self.tokenizer.pad(\n            flat_tokenized,\n            padding=self.padding,\n            max_length=self.max_length,\n            pad_to_multiple_of=self.pad_to_multiple_of,\n            return_tensors=\"pt\",\n        )\n\n        if \"token_type_ids\" in batch:\n            batch.pop(\"token_type_ids\")\n        return batch, cu_lens\n", "model/model_training/custom_datasets/toxic_conversation.py": "\"\"\"\n    SFT dataset to reject toxic questions\n\n\"\"\"\nimport random\n\nfrom datasets import load_dataset\nfrom torch.utils.data import Dataset\n\n\nclass ProsocialDialogueExplaination(Dataset):\n    name = \"explain_prosocial\"\n    TEMPLATE = [\n        # 0 : reply or sentence of interest, 1 : reason of caution\n        (\"'{0}' Why is this sentence {1}\", \"This sentence is {1} because {0}\"),\n        (\"Explain to me why this sentence is {1}: {0}\", \"This sentence is {1} because {0}\"),\n        (\"'{0}' Why is this sentence {1}\", \"This sentence may need {1} because {0}\"),\n        (\"'{0}' Explain to me why we need to {1} of this sentence\", \"We need to be {1} of this sentence because {0}\"),\n    ]\n\n    def __init__(self, split=\"train\", cache_dir=\".cache\") -> None:\n        super().__init__()\n        dataset = load_dataset(\n            \"Englishman2022/prosocial-dialog-filtered\",\n            data_files=\"train.json\",\n            cache_dir=cache_dir,\n            revision=\"e121e4fd886fadc030d633274c053b71839f9c20\",\n        )[split]\n        self.pairs = []\n        for row in dataset:\n            for safety_annotation, safe_answer in zip(row[\"safety_annotations\"], row[\"safety_annotation_reasons\"]):\n                (prompt_template, answer_template) = random.choice(self.TEMPLATE)\n                self.pairs.append(\n                    (\n                        prompt_template.format(row[\"context\"], safety_annotation),\n                        answer_template.format(safe_answer, safety_annotation),\n                    )\n                )\n\n    def __len__(self):\n        return len(self.pairs)\n\n    def __getitem__(self, idx):\n        return self.pairs[idx]\n\n\nclass ProsocialDialogue(Dataset):\n    name = \"prosocial_dialogue\"\n    \"\"\"\n        ProsocialDialog, we set up a human-AI collaborative data creation framework,\n        where GPT-3 generates the potentially unsafe utterances, and crowdworkers\n        provide prosocial responses to them. This approach allows us to circumvent\n        two substantial challenges:\n        (1) there are no available large-scale corpora of multiturn prosocial conversations\n            between humans\n        (2) asking humans to write unethical, toxic, or problematic utterances could result\n            in psychological harms (Roberts, 2017; Steiger et al., 2021).\n    \"\"\"\n\n    def __init__(self, split=\"train\", cache_dir=\".cache\") -> None:\n        super().__init__()\n        dataset = load_dataset(\n            \"Englishman2022/prosocial-dialog-filtered\",\n            data_files=\"train.json\",\n            cache_dir=cache_dir,\n            revision=\"e121e4fd886fadc030d633274c053b71839f9c20\",\n        )[split]\n        self.pairs = []\n        for row in dataset:\n            prompt = row[\"context\"]\n            for answer in row[\"rots\"]:\n                self.pairs.append((prompt, answer))\n\n    def __len__(self):\n        return len(self.pairs)\n\n    def __getitem__(self, idx):\n        return self.pairs[idx]\n", "model/model_eval/utils.py": "import json\nimport os\n\nimport numpy as np\n\n\ndef load_sampling_data(path):\n    \"\"\"\n    Load sampling data and ensure appropriate keys are present.\n    \"\"\"\n\n    if os.path.exists(path):\n        data = json.load(open(path))\n    else:\n        raise FileNotFoundError(f\"Sampling data {path} not found\")\n\n    if \"prompts\" not in data.keys():\n        raise KeyError(\"sampling data should contain prompts key\")\n\n    keys = set(data[\"prompts\"][0].keys())\n    required_keys = set([\"prompt\", \"results\"])\n    keys = keys.intersection(required_keys)\n    if keys != required_keys:\n        raise KeyError(f\"Missing keys {required_keys - keys} \")\n\n    return data\n\n\ndef write_to_json(filename, data):\n    if not filename.endswith(\".json\"):\n        filename = f\"{filename}.json\"\n\n    with open(filename, \"w\") as file:\n        json.dump(data, file, indent=4)\n\n\ndef describe_samples(samples):\n    reward_scores = []\n    for item in samples:\n        reward_scores.extend([float(output[1]) for output in item[\"outputs\"]])\n\n    return {\n        \"mean\": np.mean(reward_scores).astype(str),\n        \"min\": np.min(reward_scores).astype(str),\n        \"max\": np.max(reward_scores).astype(str),\n    }\n", "model/model_eval/sampling_score.py": "import argparse\nimport json\n\nimport model_training.models.reward_model  # noqa: F401 (registers reward model for AutoModel loading)\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom eval_datasets import get_sampling_dataloader\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\nfrom utils import load_sampling_data\n\n\ndef batch_inference(model, dataloader):\n    \"\"\"\n    Batch inference\n    \"\"\"\n\n    scores, sampling = [], []\n    device = model.device\n    for i, data in enumerate(dataloader):\n        sampling.append(data.pop(\"sampling\").cpu().detach().numpy())\n        data = {k: v.squeeze().to(device) for k, v in data.items()}\n        pred = model(**data).logits[:, 0].cpu().detach().numpy()\n        scores.append(pred)\n\n    return np.hstack(sampling), np.hstack(scores)\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description=\"\")\n    parser.add_argument(\"--data_path\", type=str, help=\"Path of the sampling data file\")\n    parser.add_argument(\"--model\", type=str, help=\"Path or url of the model file\")\n    parser.add_argument(\"--max_length\", type=int, help=\"max length of input\")\n    parser.add_argument(\"--batch_size\", type=int, help=\"device\", default=4)\n    parser.add_argument(\"--device\", type=str, help=\"device\", default=\"cpu\")\n    parser.add_argument(\"--save\", type=bool, help=\"whether to save the results\", default=True)\n\n    args = parser.parse_args().__dict__\n    if args.get(\"device\") != \"cpu\":\n        device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n    else:\n        device = torch.device(\"cpu\")\n\n    data = load_sampling_data(args.get(\"data_path\"))\n\n    model_name = args.get(\"model\")\n\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    model = AutoModelForSequenceClassification.from_pretrained(model_name)\n    model.eval()\n    model.to(device)\n    max_length = args.get(\"max_length\") or model.config.max_position_embeddings\n    dataloader = get_sampling_dataloader(data, tokenizer, max_length, args.get(\"batch_size\"))\n    sampling, scores = batch_inference(model, dataloader)\n\n    df = pd.DataFrame({\"sampling\": sampling, \"score\": scores})\n    id2label = {v: k for k, v in dataloader.dataset.label2id.items()}\n    df[\"sampling\"] = df[\"sampling\"].map(id2label)\n    results = df.groupby(\"sampling\")[\"score\"].mean().to_dict()\n    results[\"mean_reward\"] = str(df[\"score\"].mean())\n    print(\"RESULTS: \", results)\n\n    results = {\"model_name\": data[\"model_name\"], \"results\": results, \"reward_model\": args.get(\"model\")}\n    name = \"-\".join(data[\"model_name\"].split(\"/\"))\n\n    if args.get(\"save\"):\n        with open(f\"{name}.json\", \"w\") as file:\n            json.dump(results, file, indent=4)\n", "model/model_eval/eval_rm.py": "import argparse\nfrom collections import defaultdict\n\nimport numpy as np\nimport torch\nfrom model_training.custom_datasets.rank_datasets import HellaSwagDataset, HFDataset, SHPDataset\nfrom model_training.custom_datasets.ranking_collator import RankingDataCollator\nfrom model_training.metrics import RewardMetrics\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\nfrom transformers.trainer_utils import EvalPrediction\nfrom utils import write_to_json\n\nDATASETS = [\"SHP\", \"Hellaswag\", \"HFdataset\"]\n\n\ndef get_ranking_dataset(dataset, split):\n    dataset = dataset.lower()\n    if dataset == \"shp\":\n        return SHPDataset(split=split)\n    elif dataset == \"hellaswag\":\n        return HellaSwagDataset(split=split)\n    elif dataset == \"hfdataset\":\n        return HFDataset(split=split)\n    else:\n        raise ValueError(f\"Invalid dataset name, available {DATASETS}\")\n\n\ndef batch_inference(inputs, model):\n    batch, cu_lens = inputs\n    batch = {k: v.to(model.device) for k, v in batch.items()}\n\n    with torch.no_grad():\n        logits = model(input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"]).logits.detach().cpu()\n\n    if logits.dtype == torch.bfloat16:\n        # As of Numpy 1.21.4, NumPy does not support bfloat16 (see\n        # https://github.com/numpy/numpy/blob/a47ecdea856986cd60eabbd53265c2ca5916ad5d/doc/source/user/basics.types.rst ).\n        # Until Numpy adds bfloat16, we must convert float32.\n        logits = logits.to(torch.float32)\n    logits = logits.numpy()\n\n    labels = []\n    for i, (s, e) in enumerate(zip(cu_lens[:-1], cu_lens[1:])):\n        labels.extend([i] * (e - s))\n    labels = np.array(labels).reshape(-1, 1)\n    return EvalPrediction(predictions=logits.T, label_ids=labels.T)\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description=\"\")\n    parser.add_argument(\"--dataset\", type=str, help=\"name of evaluation dataset\")\n    parser.add_argument(\"--split\", type=str, help=\"dataset splits separated by comma\", default=\"train\")\n    parser.add_argument(\"--model\", type=str, help=\"Path or url of the model file\")\n    parser.add_argument(\"--metrics\", type=str, help=\"metrics to evaluate\", default=\"accuracy\")\n    parser.add_argument(\"--batch_size\", type=int, help=\"Batch Size\", default=8)\n    parser.add_argument(\"--device\", type=str, help=\"device\", default=\"cuda\")\n    parser.add_argument(\"--dtype\", type=str, help=\"data type\", default=None)\n    args = parser.parse_args().__dict__\n\n    if args.get(\"device\") != \"cpu\":\n        device = torch.device(args.get(\"device\")) if torch.cuda.is_available() else torch.device(\"cpu\")\n    else:\n        device = torch.device(\"cpu\")\n\n    model_name = args.get(\"model\")\n\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    model = AutoModelForSequenceClassification.from_pretrained(\n        model_name, torch_dtype=\"auto\" if not args.dtype else args.dtype\n    )\n    model.eval()\n    model.to(device)\n    max_length = args.get(\"max_length\") or model.config.max_position_embeddings\n\n    splits = args.get(\"split\").split(\",\")\n    dataset = get_ranking_dataset(args.get(\"dataset\"), split=splits)\n    collate_fn = RankingDataCollator(tokenizer)\n    dataset = DataLoader(dataset, collate_fn=collate_fn, batch_size=args.get(\"batch_size\"))\n\n    metrics = args.get(\"metrics\").split(\",\")\n    compute_metrics = RewardMetrics(metrics)\n    score_dict = defaultdict(float)\n    for i, data in enumerate(tqdm(dataset)):\n        eval_pred = batch_inference(data, model)\n        results = compute_metrics(eval_pred)\n        for metric in metrics:\n            score_dict[metric] += results.get(metric)\n    score_dict = {k: str(round(v / len(dataset), 3)) for k, v in score_dict.items()}\n\n    results = {\n        \"model\": model_name,\n        \"dataset\": args.get(\"dataset\"),\n        \"split\": splits,\n    }\n    results.update(score_dict)\n\n    print(\"RESULTS\", results)\n    write_to_json(f\"rm-eval-{model_name.split('/')[-1]}-results.json\", results)\n", "model/model_eval/eval_datasets.py": "from collections import defaultdict\n\nimport torch\nfrom model_training.custom_datasets.ranking_collator import RankingDataCollator\nfrom torch.utils.data import DataLoader, Dataset\n\n\ndef get_sampling_dataloader(data, tokenizer, max_length, batch_size):\n    collate_fn = SamplingDataCollator(tokenizer, max_length=max_length)\n    dataset = SamplingDataset(data)\n    return DataLoader(dataset, collate_fn=collate_fn, batch_size=batch_size)\n\n\nclass SamplingDataCollator(RankingDataCollator):\n    def __call__(self, examples):\n        flat_tokenized = []\n        sampling_ids = []\n        for example in examples:\n            prefix, reply, sampling = example\n            sampling_ids.append(sampling)\n            tokenized = self.process_one((prefix, reply))\n            flat_tokenized.extend(tokenized)\n\n        batch = self.tokenizer.pad(\n            flat_tokenized,\n            padding=self.padding,\n            max_length=self.max_length,\n            pad_to_multiple_of=self.pad_to_multiple_of,\n            return_tensors=\"pt\",\n        )\n\n        if \"token_type_ids\" in batch:\n            batch.pop(\"token_type_ids\")\n\n        batch[\"sampling\"] = torch.tensor(sampling_ids)\n        return batch\n\n\nclass SamplingDataset(Dataset):\n\n    \"\"\"\n    Dataset for loading sampling reports\n    \"\"\"\n\n    def __init__(self, dataset):\n        super().__init__()\n\n        self.dataset = []\n        sampling_list = []\n        for data in dataset[\"prompts\"]:\n            prompt = data[\"prompt\"]\n            for result in data[\"results\"]:\n                sampling = result[\"sampling_config\"]\n                for output in result[\"outputs\"]:\n                    self.dataset.append((prompt, output, sampling))\n                if sampling not in sampling_list:\n                    sampling_list.append(sampling)\n\n        self.label2id = self.get_label2id(sampling_list)\n\n    def get_label2id(self, sampling_list):\n        return {v: k for k, v in enumerate(sampling_list)}\n\n    def __len__(self):\n        return len(self.dataset)\n\n    def __getitem__(self, idx):\n        prefix, reply, sampling = self.dataset[idx]\n        sampling = self.label2id[sampling]\n\n        return ([prefix], [reply], sampling)\n\n\nclass RejectionSamplingDataset(Dataset):\n    def __init__(self, dataset):\n        self.prompt_answer = defaultdict(list)\n        for data in dataset[\"prompts\"]:\n            prompt = data[\"prompt\"].strip()\n            if prompt not in self.prompt_answer.keys():\n                self.prompt_answer[prompt] = []\n\n            outputs = [output for result in data[\"results\"] for output in result[\"outputs\"]]\n            self.prompt_answer[prompt].extend(outputs)\n\n        self.prompts = list(self.prompt_answer.keys())\n\n    def __len__(self):\n        return len(self.prompts)\n\n    def __getitem__(self, index):\n        prompt = self.prompts[index]\n        replies = self.prompt_answer.get(prompt)\n\n        return prompt, replies, index\n", "model/model_eval/__init__.py": "", "model/model_eval/rejection_sampling.py": "import argparse\n\nimport model_training.models.reward_model  # noqa: F401 (registers reward model for AutoModel loading)\nimport numpy as np\nimport torch\nfrom eval_datasets import RejectionSamplingDataset, SamplingDataCollator\nfrom torch.utils.data import DataLoader\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\nfrom utils import describe_samples, load_sampling_data, write_to_json\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description=\"\")\n    parser.add_argument(\"--data_path\", type=str, help=\"Path of the sampling data file\")\n    parser.add_argument(\"--model\", type=str, help=\"Path or url of the model file\")\n    parser.add_argument(\"--rs\", type=int, help=\"rejection sampling\", default=3)\n    parser.add_argument(\"--max_length\", type=int, help=\"max length of input\")\n    parser.add_argument(\"--device\", type=str, help=\"device\", default=\"cpu\")\n    args = parser.parse_args().__dict__\n\n    if args.get(\"device\") != \"cpu\":\n        device = torch.device(args.get(\"device\")) if torch.cuda.is_available() else torch.device(\"cpu\")\n    else:\n        device = torch.device(\"cpu\")\n\n    model_name = args.get(\"model\")\n\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    model = AutoModelForSequenceClassification.from_pretrained(model_name)\n    model.eval()\n    model.to(device)\n    max_length = args.get(\"max_length\") or model.config.max_position_embeddings\n\n    sr_report = load_sampling_data(args.get(\"data_path\"))\n    dataset = RejectionSamplingDataset(sr_report)\n    collate_fn = SamplingDataCollator(tokenizer, max_length=max_length)\n    dataloader = DataLoader(dataset, collate_fn=collate_fn, batch_size=1)\n\n    RS = args.get(\"rs\")\n    selected_list, rejected_list = [], []\n    for i, data in enumerate(dataloader):\n        index = data.pop(\"sampling\").detach().cpu().item()\n        data = {k: v.to(device) for k, v in data.items()}\n        pred = (\n            model(**data)\n            .logits[:, 0]\n            .cpu()\n            .detach()\n            .numpy()\n            .reshape(\n                -1,\n            )\n        )\n        sorted_indices = np.argsort(pred)\n        prompt, replies, _ = dataset[index]\n        selected_list.append(\n            {\n                \"prompt\": prompt,\n                \"outputs\": [(replies[idx], str(round(pred[idx], 2))) for idx in reversed(sorted_indices[-RS:])],\n            }\n        )\n\n        rejected_list.append(\n            {\"prompt\": prompt, \"outputs\": [(replies[idx], str(round(pred[idx], 2))) for idx in sorted_indices[:-RS]]}\n        )\n\n    selected_stats = describe_samples(selected_list)\n    rejected_stats = describe_samples(rejected_list)\n    stats = {\"rejected_samples\": rejected_stats, \"selected_samples\": selected_stats}\n    write_to_json(\"selected_samples\", selected_list)\n    write_to_json(\"rejected_samples\", rejected_list)\n    write_to_json(\"comparison\", stats)\n", "model/model_eval/manual/subsample_dataset.py": "import argparse\nimport gzip\nimport json\nimport random\nfrom pathlib import Path\nfrom typing import Optional\n\nimport pydantic\nfrom oasst_data import ExportMessageTree\n\n\ndef load_message_trees(\n    input_file_path: str | Path,\n    lang_codes: list[str],\n    tree_state: str,\n    max_length: Optional[int] = None,\n) -> list[ExportMessageTree]:\n    if not isinstance(input_file_path, Path):\n        input_file_path = Path(input_file_path)\n\n    if input_file_path.suffix == \".gz\":\n        file_in = gzip.open(str(input_file_path), mode=\"tr\", encoding=\"UTF-8\")\n    else:\n        file_in = input_file_path.open(\"r\", encoding=\"UTF-8\")\n\n    trees = []\n\n    with file_in:\n        # read one message tree per line\n        for line in file_in:\n            dict_tree = json.loads(line)\n\n            # validate data\n            tree: ExportMessageTree = pydantic.parse_obj_as(ExportMessageTree, dict_tree)\n\n            if (\n                tree.prompt.lang not in lang_codes\n                or tree.prompt.deleted\n                or not tree.prompt.review_result\n                or tree.tree_state != tree_state\n                or (max_length and len(tree.prompt.text) > max_length)\n            ):\n                continue\n\n            trees.append(tree)\n\n    return trees\n\n\ndef write_file(output_file_path: str | Path, items: list) -> None:\n    if not isinstance(output_file_path, Path):\n        output_file_path = Path(output_file_path)\n\n    if output_file_path.suffix == \".gz\":\n        file_out = gzip.open(str(output_file_path), \"wt\", encoding=\"UTF-8\")\n    else:\n        file_out = open(output_file_path, \"wt\", encoding=\"UTF-8\")\n\n    with file_out:\n        for obj in items:\n            x = obj\n            if isinstance(x, pydantic.BaseModel):\n                x = obj.dict(exclude_none=True)\n            json.dump(x, file_out)\n            file_out.write(\"\\n\")\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--input-file\", type=str, help=\"Name of oasst exprt file to read\", required=True)\n\n    parser.add_argument(\"--output-file\", type=str, default=\"out.jsonl\", help=\"Output file name\", required=True)\n\n    parser.add_argument(\"--state\", type=str, default=\"ready_for_export\", help=\"tree state to filter\")\n    parser.add_argument(\"--max-length\", type=int, help=\"max length of prompt\")\n\n    parser.add_argument(\"-k\", type=int, default=100, help=\"Number of trees to sample\")\n\n    parser.add_argument(\n        \"--lang\",\n        type=str,\n        default=\"en\",\n        help=\"List of comma separated language codes\",\n    )\n\n    parser.add_argument(\"--only-prompts\", action=\"store_true\", default=False)\n\n    parser.add_argument(\"--only-text\", action=\"store_true\", default=False)\n\n    parser.add_argument(\n        \"--seed\",\n        type=int,\n        default=\"42\",\n        help=\"rng seed\",\n    )\n\n    args = parser.parse_args()\n    return args\n\n\ndef main():\n    args = parse_args()\n    lang_codes = args.lang.split(\",\")\n    random.seed(args.seed)\n    trees = load_message_trees(\n        args.input_file,\n        lang_codes=lang_codes,\n        tree_state=args.state,\n        max_length=args.max_length,\n    )\n    print(f\"Matching messages trees: {len(trees)}\")\n    assert len(trees) > args.k, f\"Not enough trees ({len(trees)} found, {args.k} required)\"\n\n    sub_sample = random.sample(trees, k=args.k)\n\n    if args.only_prompts:\n        sub_sample = [x.prompt for x in sub_sample]\n\n        if args.only_text:\n            sub_sample = [x.text for x in sub_sample]\n\n    write_file(args.output_file, sub_sample)\n\n\nif __name__ == \"__main__\":\n    main()\n", "model/model_eval/manual/create_synth_import.py": "import argparse\nimport json\nimport random\nimport re\nimport sys\nfrom uuid import uuid4\n\nimport pydantic\nfrom oasst_data import ExportMessageNode, ExportMessageTree\nfrom sampling_report import SamplingReport\n\n\ndef filter_text(s: str) -> str:\n    m = re.search(\n        r\"\\</?prefix\\>|\\<human\\>|\\<\\|endoftext\\|\\>|\\<\\|prompter\\|\\>|\\<\\|assistant\\|\\>|\\<\\|system\\|\\>|<|prefix_(begin|end)\\|\\>\",\n        s,\n    )\n    if m:\n        s = s[: m.start()]\n    return s\n\n\ndef format_params(p: dict) -> str:\n    s = [f\"{k}={v}\" for k, v in p.items()]\n    return \",\".join(s)\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"input_files\", nargs=\"*\", type=argparse.FileType(\"r\", encoding=\"UTF-8\"))\n    parser.add_argument(\"--seed\", default=219837, type=int)\n    parser.add_argument(\"--num-replies\", default=5, type=int)\n    parser.add_argument(\"--output\", type=argparse.FileType(\"w\", encoding=\"UTF-8\"), default=sys.stdout)\n    parser.add_argument(\"--max-count\", type=int)\n    parser.add_argument(\"--lang\", type=str, default=\"en\")\n    return parser.parse_args()\n\n\ndef main():\n    args = parse_args()\n\n    input_reports: list[SamplingReport] = []\n    for f in args.input_files:\n        json_raw = json.load(f)\n        report = pydantic.parse_obj_as(SamplingReport, json_raw)\n        input_reports.append(report)\n\n    print(f\"Read {len(input_reports)} reports\")\n\n    # index by prompt\n    reply_by_prompt: dict[str, list[ExportMessageNode]] = {}\n    for r in input_reports:\n        for p in r.prompts:\n            for res in p.results:\n                for s in res.outputs:\n                    s = filter_text(s)\n                    model_name = f\"{r.model_name},{format_params(res.sampling_params)}\"\n                    m = ExportMessageNode(\n                        message_id=str(uuid4()),\n                        text=s,\n                        role=\"assistant\",\n                        synthetic=True,\n                        model_name=model_name,\n                        lang=args.lang,\n                    )\n\n                    l = reply_by_prompt.get(p.prompt)\n                    if l is not None:\n                        l.append(m)\n                    else:\n                        reply_by_prompt[p.prompt] = [m]\n\n    random.seed(args.seed)\n    trees: list[ExportMessageTree] = []\n    for k, v in reply_by_prompt.items():\n        # remove exact duplicates\n        reply_texts = set()\n        unique_replies = []\n        for m in v:\n            if m.text in reply_texts:\n                continue\n            unique_replies.append(m)\n            reply_texts.add(m.text)\n\n        if len(unique_replies) < 2:\n            print(\"Skipping enty with < 2 unique replies\")\n            continue\n\n        prompt_message = ExportMessageNode(\n            message_id=str(uuid4()), text=k, role=\"prompter\", synthetic=False, lang=args.lang\n        )\n        prompt_message.replies = random.sample(unique_replies, k=min(args.num_replies, len(unique_replies)))\n        t = ExportMessageTree(message_tree_id=prompt_message.message_id, tree_state=\"ranking\", prompt=prompt_message)\n        trees.append(t)\n        if args.max_count and len(trees) >= args.max_count:\n            break\n\n    with args.output as f:\n        for t in trees:\n            json.dump(t.dict(exclude_none=True), f)\n            f.write(\"\\n\")\n\n\nif __name__ == \"__main__\":\n    main()\n", "model/model_eval/manual/sampling_report.py": "import argparse\nimport gzip\nimport json\nimport random\nimport re\nfrom collections import OrderedDict\nfrom datetime import datetime\nfrom pathlib import Path\nfrom typing import Any, Optional\n\nimport pydantic\nimport torch\nfrom model_training.models.peft_modeling import load_peft_model\nfrom tqdm import tqdm\nfrom transformers import AutoTokenizer, PreTrainedTokenizer\n\nQA_SPECIAL_TOKENS = {\"Question\": \"<human>\", \"Answer\": \"<bot>\", \"StartPrefix\": \"<prefix>\", \"EndPrefix\": \"</prefix>\"}\nQA_SPECIAL_TOKENS_V2_5 = {\n    \"prompter\": \"<|prompter|>\",\n    \"assistant\": \"<|assistant|>\",\n    \"system\": \"<|system|>\",\n    \"prefix_begin\": \"<|prefix_begin|>\",\n    \"prefix_end\": \"<|prefix_end|>\",\n}\n\n\nclass SamplingConfig(pydantic.BaseModel):\n    name: Optional[str]\n    generate_args: dict[str, Any] = {}\n    system_profile: Optional[OrderedDict[str, float | int | str]] = None\n    pre_text: Optional[str]\n    add_prefix_tokens: Optional[bool] = False\n\n    # for legacy mode\n    human_name: Optional[str]\n    bot_name: Optional[str]\n\n\nclass Configuration(pydantic.BaseModel):\n    default: Optional[SamplingConfig]\n    configurations: list[SamplingConfig]\n\n\nclass SamplingResult(pydantic.BaseModel):\n    sampling_config: str\n    sampling_params: dict\n    outputs: list[str]\n\n\nclass PromptResults(pydantic.BaseModel):\n    prompt: str\n    results: list[SamplingResult]\n\n\nclass SamplingReport(pydantic.BaseModel):\n    model_name: str\n    date: str\n    args: dict\n    prompts: list[PromptResults]\n\n\ndef load_jsonl(input_file_path: str | Path) -> list[dict | str]:\n    if not isinstance(input_file_path, Path):\n        input_file_path = Path(input_file_path)\n\n    if input_file_path.suffix == \".gz\":\n        file_in = gzip.open(str(input_file_path), mode=\"tr\", encoding=\"UTF-8\")\n    else:\n        file_in = input_file_path.open(\"r\", encoding=\"UTF-8\")\n\n    items = []\n\n    with file_in:\n        # read one message tree per line\n        for line in file_in:\n            obj = json.loads(line, object_pairs_hook=OrderedDict)\n            items.append(obj)\n\n    return items\n\n\ndef sample(\n    prompt: str,\n    model,\n    tokenizer: PreTrainedTokenizer,\n    mode: str,\n    sampling_config: SamplingConfig,\n    device: torch.DeviceObjType,\n    skip_input_tokens: bool,\n    max_input_len: Optional[int] = None,\n):\n    assert sampling_config.name, \"'name' must be specified for sampling configuration\"\n    sc = sampling_config\n    prefix = \"\"\n    if sampling_config.pre_text:\n        if mode == \"v2\" and sampling_config.add_prefix_tokens:\n            prefix = f\"<prefix>{sampling_config.pre_text}</prefix>\"\n        if mode == \"v2_5\" and sampling_config.add_prefix_tokens:\n            prefix = f\"{QA_SPECIAL_TOKENS_V2_5['prefix_begin']}{sampling_config.pre_text}{QA_SPECIAL_TOKENS_V2_5['prefix_end']}\"\n        else:\n            prefix = sampling_config.pre_text\n\n    if mode == \"v2\":\n        input_text = f\"{prefix}{QA_SPECIAL_TOKENS['Question']}{prompt}{QA_SPECIAL_TOKENS['Answer']}\"\n    elif mode == \"v2_5\":\n        if sampling_config.system_profile and len(sampling_config.system_profile) > 0:\n            system_fragments = [QA_SPECIAL_TOKENS_V2_5[\"system\"]]\n            for k, v in sampling_config.system_profile.items():\n                if isinstance(v, float):\n                    system_fragments.append(f\"{k}: {v:0.1f}\")\n                elif isinstance(v, str):\n                    system_fragments.append(f\"{k}: {v}\")\n                else:\n                    system_fragments.append(f\"{k}: {v}\")\n            system_fragments.append(tokenizer.eos_token)\n            system_tag = \"\\n\".join(system_fragments)\n        else:\n            system_tag = \"\"\n\n        input_text = f\"{prefix}{QA_SPECIAL_TOKENS_V2_5['prompter']}{prompt}{tokenizer.eos_token}{system_tag}{QA_SPECIAL_TOKENS_V2_5['assistant']}\"\n        print(\"input_text\", input_text)\n    else:\n        assert sc.human_name and sc.bot_name, \"'human_name' and 'bot_name' parameters must be specified in config \"\n        input_text = f\"{prefix}\\n{sc.human_name}: {prompt}\\n\\n{sc.bot_name}: \"\n\n    sampling_params = sampling_config.generate_args\n    inputs = tokenizer(\n        input_text,\n        return_tensors=\"pt\",\n        max_length=max_input_len,\n        pad_to_max_length=False,\n        truncation=True,\n    ).to(device)\n    input_ids = inputs.input_ids\n    outputs = model.generate(\n        input_ids=input_ids,\n        pad_token_id=tokenizer.eos_token_id,\n        **sampling_params,\n    )\n    if skip_input_tokens:\n        output_tokens = outputs[0, input_ids.size(1) :]\n    else:\n        output_tokens = outputs[0]\n    return output_tokens, sampling_params\n\n\ndef merge_configs(*configs: tuple[Optional[SamplingConfig]]) -> Optional[SamplingConfig]:\n    merged: SamplingConfig | None = None\n    for c in configs:\n        if not merged:\n            if c:\n                merged = c.copy(deep=True)\n        else:\n            # simple fields\n            fields = [\"name\", \"pre_text\", \"human_name\", \"bot_name\", \"add_prefix_tokens\"]\n            for field_name in fields:\n                v = getattr(c, field_name)\n                if v:\n                    setattr(merged, field_name, v)\n            # generate args\n            if c.generate_args:\n                for k, v in c.generate_args.items():\n                    merged.generate_args[k] = v\n            # system profile\n            if c.system_profile:\n                if not merged.system_profile:\n                    merged.system_profile = {}\n                for k, v in c.system_profile.items():\n                    merged.system_profile[k] = v\n\n    return merged\n\n\ndef sample_prompt_continuations(\n    prompts: list[str],\n    model,\n    tokenizer: PreTrainedTokenizer,\n    mode: str,\n    config: Configuration,\n    device: torch.DeviceObjType,\n    num_samples: int = 1,\n    skip_special_tokens: bool = False,\n    skip_input_tokens: bool = False,\n    verbose: bool = False,\n    max_input_len: Optional[int] = None,\n) -> list[PromptResults]:\n    prompt_results: list[PromptResults] = []\n    for p in tqdm(prompts):\n        sampling_results: list[SamplingResult] = []\n        for sc in config.configurations:\n            outputs = []\n            for i in range(num_samples):\n                if i > 0 and sc.generate_args.get(\"do_sample\") is False:\n                    break  # don't repeat greedy sampling\n                output_tokens, sampling_params = sample(\n                    p,\n                    model=model,\n                    tokenizer=tokenizer,\n                    mode=mode,\n                    sampling_config=merge_configs(config.default, sc),\n                    device=device,\n                    skip_input_tokens=skip_input_tokens,\n                    max_input_len=max_input_len,\n                )\n                output = tokenizer.decode(\n                    output_tokens,\n                    truncate_before_pattern=[r\"\\n\\n^#\", \"^'''\", \"\\n\\n\\n\"],  # only used for codegen model\n                    skip_special_tokens=skip_special_tokens,\n                )\n\n                if verbose:\n                    print(f\"===[ Config: {sc.name} [{i+1}/{num_samples}] ]===\\n\")\n                    print(f'User: \"{p}\"')\n                    print(f'Assistant: \"{output}\"\\n')\n                outputs.append(output)\n\n            sampling_results.append(\n                SamplingResult(sampling_config=sc.name, sampling_params=sampling_params, outputs=outputs)\n            )\n\n        prompt_results.append(PromptResults(prompt=p, results=sampling_results))\n    return prompt_results\n\n\ndef load_configs(path: Path) -> Configuration:\n    with path.open() as f:\n        json_data = json.load(f)\n\n    return pydantic.parse_obj_as(Configuration, json_data)\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--device\", default=\"cuda\", type=str, help=\"device to use\")\n    parser.add_argument(\"--device-index\", default=0, type=int, help=\"device index\")\n    parser.add_argument(\"--model-name\", type=str, default=\"facebook/galactica-125m\")\n    parser.add_argument(\n        \"--mode\",\n        type=str,\n        default=\"legacy\",\n        help=\"legacy, v2\",\n    )\n    parser.add_argument(\n        \"--prompts\", type=str, help=\"jsonl string prompts input file name\", default=\"./data/en_100_text.jsonl.gz\"\n    )\n    parser.add_argument(\"--report\", type=str, help=\"json sampling report output file name\")\n    parser.add_argument(\"--seed\", type=int, default=\"42\", help=\"pseudo random number generator seed\")\n    parser.add_argument(\"--verbose\", action=\"store_true\", default=False)\n    parser.add_argument(\"-n\", type=int, help=\"number of prompts to use (default: all)\")\n    parser.add_argument(\"--num-samples\", type=int, default=2, help=\"number of sampling runs per configuration\")\n    parser.add_argument(\"--config\", type=str, default=\"config/default.json\", help=\"configuration file path\")\n    parser.add_argument(\"--half\", action=\"store_true\", default=False, help=\"use float16\")\n    parser.add_argument(\"--int8\", action=\"store_true\", default=False, help=\"use int8 quantization\")\n    parser.add_argument(\"--skip-special-tokens\", action=\"store_true\", default=False)\n    parser.add_argument(\"--model-type\", type=str, default=\"CausalLM\", help=\"CausalLM, T5Conditional, LLaMA\")\n    parser.add_argument(\"--max-input-len\", type=int, help=\"max token counts for input\")\n    parser.add_argument(\"--auth-token\", type=str)\n    parser.add_argument(\"--num-threads\", type=int, default=8)\n    parser.add_argument(\"--peft_model\", type=str, default=None)\n\n    return parser.parse_args()\n\n\ndef main():\n    \"\"\"\n    Usage example:\n    python sampling_report.py --model-name facebook/galactica-125m --config config/default.json --prompts data/en_100_text.jsonl --report report_file.json -n 10 --verbose\n\n    eval oasst model:\n    python sampling_report.py --model-name theblackcat102/pythia-3b-deduped-sft --mode v2 --config config/default.json --prompts data/en_100_text.jsonl -n 2 --verbose\n    \"\"\"\n\n    print(\"Using pytorch version {}\".format(torch.__version__))\n\n    args = parse_args()\n    if args.int8 and not torch.cuda.is_available():\n        print(\"Warning: --int8 argument passed but cuda is not available. Ignoring --int8.\")\n        args.int8 = False\n\n    print(\"Args:\", args)\n\n    torch.set_num_threads(args.num_threads)\n    torch.set_num_interop_threads(args.num_threads)\n\n    device = torch.device(args.device, args.device_index)\n    print(\"Device:\", device)\n\n    if args.seed:\n        random.seed(args.seed)\n        torch.manual_seed(args.seed)\n\n    # load configuration\n    config = load_configs(Path(args.config))\n\n    model_name = args.model_name\n    print(f\"Loading model: {model_name}\")\n\n    model_args = {}\n    if args.int8:\n        # these will break model.to(device) later in the script so a conditional check is needed\n        model_args[\"load_in_8bit\"] = args.int8\n        model_args[\"device_map\"] = \"auto\"\n\n    if args.model_type.lower() == \"causallm\" or args.model_type.lower() == \"llama\":\n        from transformers import AutoModelForCausalLM\n\n        tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=args.auth_token)\n        model = AutoModelForCausalLM.from_pretrained(model_name, use_auth_token=args.auth_token, **model_args)\n        skip_input_tokens = True\n    elif args.model_type.lower() == \"t5conditional\":\n        from transformers import T5ForConditionalGeneration\n\n        tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=args.auth_token)\n        model = T5ForConditionalGeneration.from_pretrained(model_name, use_auth_token=args.auth_token, **model_args)\n        skip_input_tokens = False\n    else:\n        raise RuntimeError(\"Invalid model_type specified\")\n\n    if args.peft_model is not None:\n        tokenizer = AutoTokenizer.from_pretrained(args.peft_model)\n        model = load_peft_model(model, args.peft_model, tokenizer)\n\n    print(\"special_tokens_map:\", tokenizer.special_tokens_map)\n    print(f\"eos_token='{tokenizer.eos_token}', eos_token_id={tokenizer.eos_token_id}\")\n\n    print(\"Tokenizer check:\")\n    input_text = f\"{QA_SPECIAL_TOKENS_V2_5['prompter']}Hi!{tokenizer.eos_token}{QA_SPECIAL_TOKENS_V2_5['assistant']}\"\n    tr = tokenizer(input_text)\n    print(tr)\n    decoded = tokenizer.decode(tr.input_ids, skip_special_tokens=False)\n    print(\"decoded:\", decoded)\n\n    model.eval()\n    if args.half:\n        model = model.half()\n\n    # int8 models (load_in_8bit = True + device_map = auto): will cause this method to error\n    if not args.int8:\n        model = model.to(device)\n\n    print(f\"Loading prompts file: {args.prompts}\")\n    prompts = load_jsonl(input_file_path=args.prompts)\n    print(f\"prompt count: {len(prompts)}\")\n\n    if args.n:\n        prompts = prompts[: args.n]\n\n    args_dict = vars(args)\n    if \"auth_token\" in args_dict:\n        del args_dict[\"auth_token\"]\n    report = SamplingReport(\n        model_name=model_name,\n        date=datetime.utcnow().isoformat(),\n        args=args_dict,\n        prompts=sample_prompt_continuations(\n            prompts=prompts,\n            model=model,\n            tokenizer=tokenizer,\n            mode=args.mode,\n            config=config,\n            device=device,\n            num_samples=args.num_samples,\n            skip_special_tokens=args.skip_special_tokens,\n            skip_input_tokens=skip_input_tokens,\n            verbose=args.verbose,\n            max_input_len=args.max_input_len,\n        ),\n    )\n\n    report_filename = args.report\n    if not report_filename:\n        save_model_name = re.sub(r\"[^\\w\\d-]\", \"_\", model_name)\n        config_name = Path(args.config).stem\n        date = report.date.split(\"T\")[0]\n        report_filename = f\"{date}_{save_model_name}_sampling_{config_name}.json\"\n    print(\"report_filename\", report_filename)\n\n    report_path = Path(report_filename)\n    print(f\"writing report: {str(report_path)}\")\n    with report_path.open(mode=\"wt\", encoding=\"UTF-8\") as rf:\n        x = report.dict(exclude_none=True)\n        json.dump(x, rf, indent=2)\n\n\nif __name__ == \"__main__\":\n    main()\n", "model/pretokenizer/indexed_dataset.py": "# copied from https://github.com/epfLLM/Megatron-LLM/blob/main/megatron/data/indexed_dataset.py\n\n# Copyright (c) Facebook, Inc. and its affiliates.\n#\n# This source code is licensed under the MIT license found in the\n# LICENSE file in the root directory of this source tree.\n\n\n# copied from fairseq/fairseq/data/indexed_dataset.py\n# Removed IndexedRawTextDataset since it relied on Fairseq dictionary\n# other slight modifications to remove fairseq dependencies\n# Added document index to index file and made it accessible.\n#    An empty sentence no longer separates documents.\n\nimport os\nimport shutil\nimport struct\nfrom functools import lru_cache\nfrom itertools import accumulate\n\nimport numpy as np\nimport torch\n\n# from megatron import print_rank_0\n\nprint_rank_0 = print  # avoid dependency\n\n\ndef __best_fitting_dtype(vocab_size=None):\n    if vocab_size is not None and vocab_size < 65500:\n        return np.uint16\n    else:\n        return np.int32\n\n\ndef get_available_dataset_impl():\n    return [\"lazy\", \"cached\", \"mmap\"]\n\n\ndef infer_dataset_impl(path):\n    if IndexedDataset.exists(path):\n        with open(index_file_path(path), \"rb\") as f:\n            magic = f.read(8)\n            if magic == IndexedDataset._HDR_MAGIC:\n                return \"cached\"\n            elif magic == MMapIndexedDataset.Index._HDR_MAGIC[:8]:\n                return \"mmap\"\n            else:\n                return None\n    else:\n        print(f\"Dataset does not exist: {path}\")\n        print(\"Path should be a basename that both .idx and .bin can be appended to get full filenames.\")\n        return None\n\n\ndef make_builder(out_file, impl, vocab_size=None):\n    if impl == \"mmap\":\n        return MMapIndexedDatasetBuilder(out_file, dtype=__best_fitting_dtype(vocab_size))\n    else:\n        return IndexedDatasetBuilder(out_file)\n\n\ndef make_dataset(path, impl: str, skip_warmup=False):\n    if not IndexedDataset.exists(path):\n        print(f\"Dataset does not exist: {path}\")\n        print(\"Path should be a basename that both .idx and .bin can be appended to get full filenames.\")\n        return None\n\n    if impl == \"infer\":\n        impl = infer_dataset_impl(path)\n    if impl == \"lazy\" and IndexedDataset.exists(path):\n        return IndexedDataset(path)\n    elif impl == \"cached\" and IndexedDataset.exists(path):\n        return IndexedCachedDataset(path)\n    elif impl == \"mmap\" and MMapIndexedDataset.exists(path):\n        return MMapIndexedDataset(path, skip_warmup)\n    print(f\"Unknown dataset implementation: {impl}\")\n    return None\n\n\ndef dataset_exists(path, impl):\n    if impl == \"mmap\":\n        return MMapIndexedDataset.exists(path)\n    else:\n        return IndexedDataset.exists(path)\n\n\ndef read_longs(f, n):\n    a = np.empty(n, dtype=np.int64)\n    f.readinto(a)\n    return a\n\n\ndef write_longs(f, a):\n    f.write(np.array(a, dtype=np.int64))\n\n\ndtypes = {1: np.uint8, 2: np.int8, 3: np.int16, 4: np.int32, 5: np.int64, 6: float, 7: np.double, 8: np.uint16}\n\n\ndef code(dtype):\n    for k in dtypes.keys():\n        if dtypes[k] == dtype:\n            return k\n    raise ValueError(dtype)\n\n\ndef index_file_path(prefix_path):\n    return prefix_path + \".idx\"\n\n\ndef data_file_path(prefix_path):\n    return prefix_path + \".bin\"\n\n\ndef create_doc_idx(sizes):\n    doc_idx = [0]\n    for i, s in enumerate(sizes):\n        if s == 0:\n            doc_idx.append(i + 1)\n    return doc_idx\n\n\nclass IndexedDataset(torch.utils.data.Dataset):\n    \"\"\"Loader for IndexedDataset\"\"\"\n\n    _HDR_MAGIC = b\"TNTIDX\\x00\\x00\"\n\n    def __init__(self, path):\n        super().__init__()\n        self.path = path\n        self.data_file = None\n        self.read_index(path)\n\n    def read_index(self, path):\n        with open(index_file_path(path), \"rb\") as f:\n            magic = f.read(8)\n            assert magic == self._HDR_MAGIC, (\n                \"Index file doesn't match expected format. \" \"Make sure that --dataset_impl is configured properly.\"\n            )\n            version = f.read(8)\n            assert struct.unpack(\"<Q\", version) == (1,)\n            code, self.element_size = struct.unpack(\"<QQ\", f.read(16))\n            self.dtype = dtypes[code]\n            self._len, self.s = struct.unpack(\"<QQ\", f.read(16))\n            self.doc_count = struct.unpack(\"<Q\", f.read(8))\n            self.dim_offsets = read_longs(f, self._len + 1)\n            self.data_offsets = read_longs(f, self._len + 1)\n            self.sizes = read_longs(f, self.s)\n            self.doc_idx = read_longs(f, self.doc_count)\n\n    def read_data(self, path):\n        self.data_file = open(data_file_path(path), \"rb\", buffering=0)\n\n    def check_index(self, i):\n        if i < 0 or i >= self._len:\n            raise IndexError(\"index out of range\")\n\n    def __del__(self):\n        if self.data_file:\n            self.data_file.close()\n\n    # @lru_cache(maxsize=8)\n    def __getitem__(self, idx):\n        if not self.data_file:\n            self.read_data(self.path)\n        if isinstance(idx, int):\n            i = idx\n            self.check_index(i)\n            tensor_size = self.sizes[self.dim_offsets[i] : self.dim_offsets[i + 1]]\n            a = np.empty(tensor_size, dtype=self.dtype)\n            self.data_file.seek(self.data_offsets[i] * self.element_size)\n            self.data_file.readinto(a)\n            return a\n        elif isinstance(idx, slice):\n            start, stop, step = idx.indices(len(self))\n            if step != 1:\n                raise ValueError(\"Slices into indexed_dataset must be contiguous\")\n            sizes = self.sizes[self.dim_offsets[start] : self.dim_offsets[stop]]\n            size = sum(sizes)\n            a = np.empty(size, dtype=self.dtype)\n            self.data_file.seek(self.data_offsets[start] * self.element_size)\n            self.data_file.readinto(a)\n            offsets = list(accumulate(sizes))\n            sents = np.split(a, offsets[:-1])\n            return sents\n\n    def __len__(self):\n        return self._len\n\n    def num_tokens(self, index):\n        return self.sizes[index]\n\n    def size(self, index):\n        return self.sizes[index]\n\n    @staticmethod\n    def exists(path):\n        return os.path.exists(index_file_path(path)) and os.path.exists(data_file_path(path))\n\n    @property\n    def supports_prefetch(self):\n        return False  # avoid prefetching to save memory\n\n\nclass IndexedCachedDataset(IndexedDataset):\n    def __init__(self, path):\n        super().__init__(path)\n        self.cache = None\n        self.cache_index = {}\n\n    @property\n    def supports_prefetch(self):\n        return True\n\n    def prefetch(self, indices):\n        if all(i in self.cache_index for i in indices):\n            return\n        if not self.data_file:\n            self.read_data(self.path)\n        indices = sorted(set(indices))\n        total_size = 0\n        for i in indices:\n            total_size += self.data_offsets[i + 1] - self.data_offsets[i]\n        self.cache = np.empty(total_size, dtype=self.dtype)\n        ptx = 0\n        self.cache_index.clear()\n        for i in indices:\n            self.cache_index[i] = ptx\n            size = self.data_offsets[i + 1] - self.data_offsets[i]\n            a = self.cache[ptx : ptx + size]\n            self.data_file.seek(self.data_offsets[i] * self.element_size)\n            self.data_file.readinto(a)\n            ptx += size\n        if self.data_file:\n            # close and delete data file after prefetch so we can pickle\n            self.data_file.close()\n            self.data_file = None\n\n    # @lru_cache(maxsize=8)\n    def __getitem__(self, idx):\n        if isinstance(idx, int):\n            i = idx\n            self.check_index(i)\n            tensor_size = self.sizes[self.dim_offsets[i] : self.dim_offsets[i + 1]]\n            a = np.empty(tensor_size, dtype=self.dtype)\n            ptx = self.cache_index[i]\n            np.copyto(a, self.cache[ptx : ptx + a.size])\n            return a\n        elif isinstance(idx, slice):\n            # Hack just to make this work, can optimizer later if necessary\n            sents = []\n            for i in range(*idx.indices(len(self))):\n                sents.append(self[i])\n            return sents\n\n\nclass IndexedDatasetBuilder(object):\n    element_sizes = {np.uint8: 1, np.int8: 1, np.int16: 2, np.int32: 4, np.int64: 8, float: 4, np.double: 8}\n\n    def __init__(self, out_file, dtype=np.int32):\n        self.out_file = open(out_file, \"wb\")\n        self.dtype = dtype\n        self.data_offsets = [0]\n        self.dim_offsets = [0]\n        self.sizes = []\n        self.element_size = self.element_sizes[self.dtype]\n        self.doc_idx = [0]\n\n    def add_item(self, tensor):\n        bytes = self.out_file.write(np.array(tensor.numpy(), dtype=self.dtype))\n        self.data_offsets.append(self.data_offsets[-1] + bytes / self.element_size)\n        for s in tensor.size():\n            self.sizes.append(s)\n        self.dim_offsets.append(self.dim_offsets[-1] + len(tensor.size()))\n\n    def end_document(self):\n        self.doc_idx.append(len(self.sizes))\n\n    def merge_file_(self, another_file):\n        index = IndexedDataset(another_file)\n        assert index.dtype == self.dtype\n\n        doc_offset = len(self.sizes)\n\n        begin = self.data_offsets[-1]\n        for data_offset in index.data_offsets[1:]:\n            self.data_offsets.append(begin + data_offset)\n        self.sizes.extend(index.sizes)\n\n        begin = self.dim_offsets[-1]\n        for dim_offset in index.dim_offsets[1:]:\n            self.dim_offsets.append(begin + dim_offset)\n\n        self.doc_idx.extend((doc_offset + index.doc_idx)[1:])\n\n        with open(data_file_path(another_file), \"rb\") as f:\n            while True:\n                data = f.read(1024)\n                if data:\n                    self.out_file.write(data)\n                else:\n                    break\n\n    def finalize(self, index_file):\n        self.out_file.close()\n        index = open(index_file, \"wb\")\n        index.write(b\"TNTIDX\\x00\\x00\")\n        index.write(struct.pack(\"<Q\", 1))\n        index.write(struct.pack(\"<QQ\", code(self.dtype), self.element_size))\n        index.write(struct.pack(\"<QQ\", len(self.data_offsets) - 1, len(self.sizes)))\n        index.write(struct.pack(\"<Q\", len(self.doc_idx)))\n        write_longs(index, self.dim_offsets)\n        write_longs(index, self.data_offsets)\n        write_longs(index, self.sizes)\n        write_longs(index, self.doc_idx)\n        index.close()\n\n\ndef _warmup_mmap_file(path):\n    with open(path, \"rb\") as stream:\n        while stream.read(100 * 1024 * 1024):\n            pass\n\n\nclass MMapIndexedDataset(torch.utils.data.Dataset):\n    class Index(object):\n        _HDR_MAGIC = b\"MMIDIDX\\x00\\x00\"\n\n        @classmethod\n        def writer(cls, path, dtype):\n            class _Writer(object):\n                def __enter__(self):\n                    self._file = open(path, \"wb\")\n\n                    self._file.write(cls._HDR_MAGIC)\n                    self._file.write(struct.pack(\"<Q\", 1))\n                    self._file.write(struct.pack(\"<B\", code(dtype)))\n\n                    return self\n\n                @staticmethod\n                def _get_pointers(sizes):\n                    dtype_size = dtype().itemsize\n                    address = 0\n                    pointers = []\n\n                    for size in sizes:\n                        pointers.append(address)\n                        address += size * dtype_size\n\n                    return pointers\n\n                def write(self, sizes, doc_idx):\n                    pointers = self._get_pointers(sizes)\n\n                    self._file.write(struct.pack(\"<Q\", len(sizes)))\n                    self._file.write(struct.pack(\"<Q\", len(doc_idx)))\n\n                    sizes = np.array(sizes, dtype=np.int32)\n                    self._file.write(sizes.tobytes(order=\"C\"))\n                    del sizes\n\n                    pointers = np.array(pointers, dtype=np.int64)\n                    self._file.write(pointers.tobytes(order=\"C\"))\n                    del pointers\n\n                    doc_idx = np.array(doc_idx, dtype=np.int64)\n                    self._file.write(doc_idx.tobytes(order=\"C\"))\n\n                def __exit__(self, exc_type, exc_val, exc_tb):\n                    self._file.close()\n\n            return _Writer()\n\n        def __init__(self, path, skip_warmup=False):\n            with open(path, \"rb\") as stream:\n                magic_test = stream.read(9)\n                assert self._HDR_MAGIC == magic_test, (\n                    \"Index file doesn't match expected format. \" \"Make sure that --dataset_impl is configured properly.\"\n                )\n                version = struct.unpack(\"<Q\", stream.read(8))\n                assert (1,) == version\n\n                (dtype_code,) = struct.unpack(\"<B\", stream.read(1))\n                self._dtype = dtypes[dtype_code]\n                self._dtype_size = self._dtype().itemsize\n\n                self._len = struct.unpack(\"<Q\", stream.read(8))[0]\n                self._doc_count = struct.unpack(\"<Q\", stream.read(8))[0]\n                offset = stream.tell()\n\n            if not skip_warmup:\n                print_rank_0(\"    warming up index mmap file...\")\n                _warmup_mmap_file(path)\n\n            self._bin_buffer_mmap = np.memmap(path, mode=\"r\", order=\"C\")\n            self._bin_buffer = memoryview(self._bin_buffer_mmap)\n            print_rank_0(\"    reading sizes...\")\n            self._sizes = np.frombuffer(self._bin_buffer, dtype=np.int32, count=self._len, offset=offset)\n            print_rank_0(\"    reading pointers...\")\n            self._pointers = np.frombuffer(\n                self._bin_buffer, dtype=np.int64, count=self._len, offset=offset + self._sizes.nbytes\n            )\n            print_rank_0(\"    reading document index...\")\n            self._doc_idx = np.frombuffer(\n                self._bin_buffer,\n                dtype=np.int64,\n                count=self._doc_count,\n                offset=offset + self._sizes.nbytes + self._pointers.nbytes,\n            )\n\n        def __del__(self):\n            self._bin_buffer_mmap._mmap.close()\n            del self._bin_buffer_mmap\n\n        @property\n        def dtype(self):\n            return self._dtype\n\n        @property\n        def sizes(self):\n            return self._sizes\n\n        @property\n        def doc_idx(self):\n            return self._doc_idx\n\n        @lru_cache(maxsize=8)\n        def __getitem__(self, i):\n            return self._pointers[i], self._sizes[i]\n\n        def __len__(self):\n            return self._len\n\n    def __init__(self, path, skip_warmup=False):\n        super().__init__()\n\n        self._path = None\n        self._index = None\n        self._bin_buffer = None\n\n        self._do_init(path, skip_warmup)\n\n    def __getstate__(self):\n        return self._path\n\n    def __setstate__(self, state):\n        self._do_init(state)\n\n    def _do_init(self, path, skip_warmup):\n        self._path = path\n        self._index = self.Index(index_file_path(self._path), skip_warmup)\n\n        if not skip_warmup:\n            print_rank_0(\"    warming up data mmap file...\")\n            _warmup_mmap_file(data_file_path(self._path))\n        print_rank_0(\"    creating numpy buffer of mmap...\")\n        self._bin_buffer_mmap = np.memmap(data_file_path(self._path), mode=\"r\", order=\"C\")\n        print_rank_0(\"    creating memory view of numpy buffer...\")\n        self._bin_buffer = memoryview(self._bin_buffer_mmap)\n\n    def __del__(self):\n        self._bin_buffer_mmap._mmap.close()\n        del self._bin_buffer_mmap\n        del self._index\n\n    def __len__(self):\n        return len(self._index)\n\n    # @lru_cache(maxsize=8)\n    def __getitem__(self, idx):\n        if isinstance(idx, (int, np.integer)):\n            ptr, size = self._index[idx]\n            np_array = np.frombuffer(self._bin_buffer, dtype=self._index.dtype, count=size, offset=ptr)\n            return np_array\n        elif isinstance(idx, slice):\n            start, stop, step = idx.indices(len(self))\n            if step != 1:\n                raise ValueError(\"Slices into indexed_dataset must be contiguous\")\n            ptr = self._index._pointers[start]\n            sizes = self._index._sizes[idx]\n            offsets = list(accumulate(sizes))\n            total_size = sum(sizes)\n            np_array = np.frombuffer(self._bin_buffer, dtype=self._index.dtype, count=total_size, offset=ptr)\n            sents = np.split(np_array, offsets[:-1])\n            return sents\n        else:\n            raise TypeError(\"Unexpected type received for idx: {}\".format(type(idx)))\n\n    def get(self, idx, offset=0, length=None):\n        \"\"\"Retrieves a single item from the dataset with the option to only\n        return a portion of the item.\n\n        get(idx) is the same as [idx] but get() does not support slicing.\n        \"\"\"\n        ptr, size = self._index[idx]\n        if length is None:\n            length = size - offset\n        ptr += offset * np.dtype(self._index.dtype).itemsize\n        np_array = np.frombuffer(self._bin_buffer, dtype=self._index.dtype, count=length, offset=ptr)\n        return np_array\n\n    @property\n    def sizes(self):\n        return self._index.sizes\n\n    @property\n    def doc_idx(self):\n        return self._index.doc_idx\n\n    def get_doc_idx(self):\n        return self._index._doc_idx\n\n    def set_doc_idx(self, doc_idx_):\n        self._index._doc_idx = doc_idx_\n\n    @property\n    def supports_prefetch(self):\n        return False\n\n    @staticmethod\n    def exists(path):\n        return os.path.exists(index_file_path(path)) and os.path.exists(data_file_path(path))\n\n\nclass MMapIndexedDatasetBuilder(object):\n    def __init__(self, out_file, dtype=np.int64):\n        self._data_file = open(out_file, \"wb\")\n        self._dtype = dtype\n        self._sizes = []\n        self._doc_idx = [0]\n\n    def add_item(self, tensor):\n        np_array = np.array(tensor.numpy(), dtype=self._dtype)\n        self._data_file.write(np_array.tobytes(order=\"C\"))\n        self._sizes.append(np_array.size)\n\n    def add_doc(self, tensor, sizes):\n        np_array = np.array(tensor, dtype=self._dtype)\n        self._data_file.write(np_array.tobytes(order=\"C\"))\n        self._sizes.extend(sizes)\n        self._doc_idx.append(len(self._sizes))\n\n    def end_document(self):\n        self._doc_idx.append(len(self._sizes))\n\n    def merge_file_(self, another_file):\n        # Concatenate index\n        index = MMapIndexedDataset.Index(index_file_path(another_file))\n        assert index.dtype == self._dtype\n\n        offset = len(self._sizes)\n        self._sizes.extend(index.sizes)\n        self._doc_idx.extend((offset + index.doc_idx)[1:])\n\n        # Concatenate data\n        with open(data_file_path(another_file), \"rb\") as f:\n            shutil.copyfileobj(f, self._data_file)\n\n    def finalize(self, index_file):\n        self._data_file.close()\n\n        with MMapIndexedDataset.Index.writer(index_file, self._dtype) as index:\n            index.write(self._sizes, self._doc_idx)\n", "model/pretokenizer/create_hf_tokenizer_config.py": "import argparse\nfrom distutils.util import strtobool as strtoboolint\n\nimport transformers\nfrom tokenizer import build_tokenizer\nfrom transformers.utils import cached_file\n\n\ndef strtobool(s: str) -> bool:\n    return bool(strtoboolint(s))\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--tokenizer_type\", type=str, default=\"SentencePieceTokenizer\", help=\"SentencePieceTokenizer or FalconTokenizer\"\n    )\n    parser.add_argument(\n        \"--vocab_file\", type=str, help=\"[optional] vocab file for SentencePiece (get from HF cache by default)\"\n    )\n    parser.add_argument(\n        \"--tokenizer_name\",\n        type=str,\n        default=\"meta-llama/Llama-2-7b-hf\",\n        help=\"HuggingFace repo name or path, e.g. 'meta-llama/Llama-2-7b-hf' or 'tiiuae/falcon-40b'\",\n    )\n    parser.add_argument(\"--cache_dir\", type=str, default=None, help=\"Huggingface cache directory \")\n    parser.add_argument(\n        \"--vocab_extra_ids_list\",\n        type=str,\n        default=\"<|im_start|>,<|im_end|>\",\n        help='Comma separated list of additional tokens (e.g. \"<|im_start|>,<|im_end|>\")',\n    )\n    parser.add_argument(\"--output_dir\", type=str, default=\"output\", help=\"Path of output directory\")\n    return parser.parse_args()\n\n\ndef main():\n    \"\"\"\n    Usage examples:\n    python create_hf_tokenizer_config.py --tokenizer_type SentencePieceTokenizer --tokenizer_name meta-llama/Llama-2-7b-hf --output_dir output\n    python create_hf_tokenizer_config.py --tokenizer_type FalconTokenizer --tokenizer_name tiiuae/falcon-40b --output_dir output\n    \"\"\"\n    args = parse_args()\n    print(\"Configuration:\")\n    for k, v in vars(args).items():\n        print(f\"{k}: {v}\")\n\n    hf_tokenizer = transformers.AutoTokenizer.from_pretrained(args.tokenizer_name, cache_dir=args.cache_dir)\n\n    print(\"tokenizer.vocab_files_names\", hf_tokenizer.vocab_files_names)\n\n    if args.tokenizer_type == \"FalconTokenizer\":\n        args.vocab_file = \"\"\n    elif args.vocab_file is None:\n        args.vocab_file = cached_file(\n            args.tokenizer_name, hf_tokenizer.vocab_files_names[\"vocab_file\"], cache_dir=args.cache_dir\n        )\n\n    # add default args for megatron tokenizer\n    args.rank = 0\n    args.vocab_extra_ids = 0\n    args.new_tokens = True\n    args.make_vocab_size_divisible_by = 128\n    args.tensor_model_parallel_size = 1\n    mt_tokenizer = build_tokenizer(args)\n\n    if args.tokenizer_type == \"SentencePieceTokenizer\":\n        print(\"_special_tokens\", mt_tokenizer._special_tokens)\n        print(\"additional_special_tokens_ids\", mt_tokenizer.additional_special_tokens_ids)\n\n        hf_tokenizer.add_tokens(\"<CLS>\", special_tokens=True)\n        hf_tokenizer.add_tokens(\"<SEP>\", special_tokens=True)\n        hf_tokenizer.add_tokens(\"<EOD>\", special_tokens=True)\n        hf_tokenizer.add_tokens(\"<MASK>\", special_tokens=True)\n        hf_tokenizer.add_tokens(\"<PAD>\", special_tokens=True)\n        hf_tokenizer.cls_token_id = mt_tokenizer.cls\n        hf_tokenizer.sep_token_id = mt_tokenizer.sep\n        hf_tokenizer.mask_token_id = mt_tokenizer.mask\n        hf_tokenizer.pad_token_id = mt_tokenizer.pad\n\n        additional_special_tokens = hf_tokenizer.additional_special_tokens\n        special_tokens = {\"additional_special_tokens\": additional_special_tokens}\n        if args.vocab_extra_ids_list:\n            additional_special_tokens.extend(args.vocab_extra_ids_list.split(\",\"))\n\n        hf_tokenizer.add_special_tokens(special_tokens_dict=special_tokens, replace_additional_special_tokens=True)\n\n        additional_special_tokens_ids = [mt_tokenizer.vocab.get(t) for t in additional_special_tokens]\n        hf_tokenizer.additional_special_tokens_ids = additional_special_tokens_ids\n\n        tokens_to_check = [\n            v for k, v in hf_tokenizer.special_tokens_map.items() if k != \"additional_special_tokens\"\n        ] + additional_special_tokens\n        print(\"checking token ids:\")\n        for t in tokens_to_check:\n            a = mt_tokenizer.vocab.get(t)\n            b = hf_tokenizer.vocab.get(t)\n            print(f\"{t}: {a} (mt) == {b} (hf)\")\n            assert a == b, \"Mismatch between megatron and huggingface tokenizer vocabularies\"\n    elif args.tokenizer_type == \"FalconTokenizer\":\n        hf_tokenizer = mt_tokenizer.tokenizer\n    else:\n        raise RuntimeError(f\"Unsupported tokenizer type: {args.tokenizer_type}\")\n\n    print(\"special_tokens_map:\", hf_tokenizer.special_tokens_map)\n\n    hf_tokenizer.save_pretrained(args.output_dir)\n\n\nif __name__ == \"__main__\":\n    main()\n", "model/pretokenizer/tokenizer.py": "# copied from https://github.com/epfLLM/Megatron-LLM/blob/main/megatron/tokenizer/tokenizer.py\n# (only keeping _FalconTokenizer & _SentencePieceTokenizer)\n\n# Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.\n\n\"\"\"Megatron tokenizers.\"\"\"\n\nfrom abc import ABC, abstractmethod\n\n\ndef build_tokenizer(args):\n    \"\"\"Initialize tokenizer.\"\"\"\n    if args.rank == 0:\n        print(\"> building {} tokenizer ...\".format(args.tokenizer_type), flush=True)\n\n    if args.tokenizer_type not in {\"SentencePieceTokenizer\", \"FalconTokenizer\"}:\n        assert args.vocab_file is not None\n\n    # Select and instantiate the tokenizer.\n    if args.tokenizer_type == \"SentencePieceTokenizer\":\n        assert args.vocab_file is not None\n        tokenizer = _SentencePieceTokenizer(\n            args.vocab_file,\n            vocab_extra_ids=args.vocab_extra_ids,\n            vocab_extra_ids_list=args.vocab_extra_ids_list,\n            new_tokens=args.new_tokens,\n        )\n    elif args.tokenizer_type == \"FalconTokenizer\":\n        tokenizer = _FalconTokenizer(vocab_extra_ids_list=args.vocab_extra_ids_list, new_tokens=args.new_tokens)\n    else:\n        raise NotImplementedError(\"{} tokenizer is not \" \"implemented.\".format(args.tokenizer_type))\n\n    # Add vocab size.\n    args.padded_vocab_size = _vocab_size_with_padding(tokenizer.vocab_size, args)\n\n    return tokenizer\n\n\ndef _vocab_size_with_padding(orig_vocab_size, args):\n    \"\"\"Pad vocab size so it is divisible by model parallel size and\n    still having GPU friendly size.\"\"\"\n\n    after = orig_vocab_size\n    multiple = args.make_vocab_size_divisible_by * args.tensor_model_parallel_size\n    while (after % multiple) != 0:\n        after += 1\n    if args.rank == 0:\n        print(\n            \" > padded vocab (size: {}) with {} dummy tokens \"\n            \"(new size: {})\".format(orig_vocab_size, after - orig_vocab_size, after),\n            flush=True,\n        )\n    return after\n\n\nclass AbstractTokenizer(ABC):\n    \"\"\"Abstract class for tokenizer.\"\"\"\n\n    def __init__(self, name):\n        self.name = name\n        super().__init__()\n\n    @property\n    @abstractmethod\n    def vocab_size(self):\n        pass\n\n    @property\n    @abstractmethod\n    def vocab(self):\n        \"\"\"Dictionary from vocab text token to id token.\"\"\"\n        pass\n\n    @property\n    @abstractmethod\n    def inv_vocab(self):\n        \"\"\"Dictionary from vocab id token to text token.\"\"\"\n        pass\n\n    @abstractmethod\n    def tokenize(self, text):\n        pass\n\n    def detokenize(self, token_ids):\n        raise NotImplementedError(\"detokenizer is not implemented for {} \" \"tokenizer\".format(self.name))\n\n    @property\n    def cls(self):\n        raise NotImplementedError(\"CLS is not provided for {} \" \"tokenizer\".format(self.name))\n\n    @property\n    def sep(self):\n        raise NotImplementedError(\"SEP is not provided for {} \" \"tokenizer\".format(self.name))\n\n    @property\n    def pad(self):\n        raise NotImplementedError(\"PAD is not provided for {} \" \"tokenizer\".format(self.name))\n\n    @property\n    def eod(self):\n        raise NotImplementedError(\"EOD is not provided for {} \" \"tokenizer\".format(self.name))\n\n    @property\n    def mask(self):\n        raise NotImplementedError(\"MASK is not provided for {} \" \"tokenizer\".format(self.name))\n\n\nclass _FalconTokenizer(AbstractTokenizer):\n    \"\"\"Wrapper of huggingface tokenizer.\"\"\"\n\n    def __init__(self, vocab_extra_ids_list=None, new_tokens=True):\n        name = \"FalconTokenizer\"\n        super().__init__(name)\n        from transformers import AutoTokenizer\n\n        self.tokenizer = AutoTokenizer.from_pretrained(\"tiiuae/falcon-40b\")\n\n        if vocab_extra_ids_list and new_tokens:\n            special_tokens = self.tokenizer.additional_special_tokens + vocab_extra_ids_list.split(\",\")\n            self.tokenizer.add_special_tokens({\"additional_special_tokens\": special_tokens})\n            self._special_tokens = {tok: self.vocab[tok] for tok in special_tokens}\n        else:\n            self._special_tokens = {}\n\n        self._inv_vocab = {idx: token for token, idx in self.tokenizer.vocab.items()}\n\n    @property\n    def vocab_size(self):\n        return len(self.tokenizer.vocab)\n\n    @property\n    def vocab(self):\n        return self.tokenizer.vocab\n\n    def tokenize(self, text):\n        return self.tokenizer.encode(text)\n\n    def detokenize(self, token_ids):\n        return self.tokenizer.decode(token_ids)\n\n    @property\n    def inv_vocab(self):\n        return self._inv_vocab\n\n    @property\n    def eod(self):\n        return self.eos_token_id\n\n    @property\n    def pad(self):\n        return self.eos_token_id\n\n    @property\n    def eos_token_id(self):\n        return self.tokenizer.eos_token_id\n\n\nclass _SentencePieceTokenizer(AbstractTokenizer):\n    \"\"\"SentencePieceTokenizer-Megatron wrapper\"\"\"\n\n    def __init__(self, model_file, vocab_extra_ids=0, vocab_extra_ids_list=None, new_tokens=True):\n        name = \"SentencePieceTokenizer\"\n        super().__init__(name)\n\n        import sentencepiece\n\n        self._tokenizer = sentencepiece.SentencePieceProcessor(model_file=model_file)\n\n        self._initalize(vocab_extra_ids, vocab_extra_ids_list, new_tokens)\n\n    def _initalize(self, vocab_extra_ids, vocab_extra_ids_list, new_tokens):\n        self._vocab = {}\n        self._inv_vocab = {}\n\n        self._special_tokens = {}\n        self._inv_special_tokens = {}\n\n        self._t5_tokens = []\n\n        for i in range(len(self._tokenizer)):\n            t = self._tokenizer.id_to_piece(i)\n            self._inv_vocab[i] = t\n            self._vocab[t] = i\n\n        def _add_special_token(t):\n            if t not in self.vocab and not new_tokens:\n                return\n            if t not in self._vocab:\n                next_id = len(self._vocab)\n                self._vocab[t] = next_id\n                self._inv_vocab[next_id] = t\n            self._special_tokens[t] = self._vocab[t]\n            self._inv_special_tokens[self._vocab[t]] = t\n\n        _add_special_token(\"<CLS>\")\n        self._cls_id = self._vocab.get(\"<CLS>\")\n        _add_special_token(\"<SEP>\")\n        self._sep_id = self._vocab.get(\"<SEP>\")\n        _add_special_token(\"<EOD>\")\n        self._eod_id = self._vocab.get(\"<EOD>\")\n        _add_special_token(\"<MASK>\")\n        self._mask_id = self._vocab.get(\"<MASK>\")\n\n        pad_id = self._tokenizer.pad_id()\n        try:\n            pad_token = self._tokenizer.id_to_piece(pad_id)\n        except IndexError:\n            pad_token = \"<PAD>\"\n        _add_special_token(pad_token)\n        self._pad_id = self._vocab.get(pad_token)\n\n        bos_id = self._tokenizer.bos_id()\n        try:\n            bos_token = self._tokenizer.id_to_piece(bos_id)\n        except IndexError:\n            bos_token = \"<BOS>\"\n        _add_special_token(bos_token)\n        self._bos_id = self._vocab.get(bos_token)\n\n        eos_id = self._tokenizer.eos_id()\n        try:\n            eos_token = self._tokenizer.id_to_piece(eos_id)\n        except IndexError:\n            eos_token = \"<EOS>\"\n        _add_special_token(eos_token)\n        self._eos_id = self._vocab.get(eos_token)\n\n        for i in range(vocab_extra_ids):\n            t = \"<extra_id_{}>\".format(i)\n            _add_special_token(t)\n            self._t5_tokens += [t]\n        if vocab_extra_ids_list:\n            for t in vocab_extra_ids_list.split(\",\"):\n                _add_special_token(t)\n        print(\"Special tokens: {}\".format(self._special_tokens))\n\n    @property\n    def vocab_size(self):\n        return len(self._vocab)\n\n    @property\n    def vocab(self):\n        return self._vocab\n\n    @property\n    def inv_vocab(self):\n        return self._inv_vocab\n\n    # From:\n    # https://github.com/NVIDIA/NeMo/blob/c8fa217e811d60d11d014827c7f3845ff6c99ae7/nemo/collections/common/tokenizers/sentencepiece_tokenizer.py#L89\n    def tokenize(self, text):\n        ids = []\n        idx = 0\n\n        while 1:\n            indices = {}\n            for token in self._special_tokens:\n                try:\n                    indices[token] = text[idx:].index(token)\n                except ValueError:\n                    continue\n            if len(indices) == 0:\n                break\n\n            next_token = min(indices, key=indices.get)\n            next_idx = idx + indices[next_token]\n\n            ids.extend(self._tokenizer.encode_as_ids(text[idx:next_idx]))\n            ids.append(self._special_tokens[next_token])\n            idx = next_idx + len(next_token)\n\n        ids.extend(self._tokenizer.encode_as_ids(text[idx:]))\n        return ids\n\n    # From:\n    # https://github.com/NVIDIA/NeMo/blob/c8fa217e811d60d11d014827c7f3845ff6c99ae7/nemo/collections/common/tokenizers/sentencepiece_tokenizer.py#L125\n    def detokenize(self, ids):\n        text = \"\"\n        last_i = 0\n\n        for i, id in enumerate(ids):\n            if id in self._inv_special_tokens:\n                text += self._tokenizer.decode_ids(ids[last_i:i]) + \" \"\n                text += self._inv_special_tokens[id] + \" \"\n                last_i = i + 1\n        text += self._tokenizer.decode_ids(ids[last_i:])\n        return text.strip()\n\n    @property\n    def cls(self):\n        return self._cls_id\n\n    @property\n    def sep(self):\n        return self._sep_id\n\n    @property\n    def pad(self):\n        return self._pad_id\n\n    @property\n    def bos_token_id(self):\n        return self._bos_id\n\n    @property\n    def bos(self):\n        return self._bos_id\n\n    @property\n    def eod(self):\n        if self._eod_id is not None:\n            return self._eod_id\n        return self._eos_id  # in case noe eod we can patch this up with an eos\n\n    @property\n    def eos_token_id(self):\n        if self._eod_id is not None:\n            return self._eod_id\n        return self._eos_id\n\n    @property\n    def eos(self):\n        return self._eos_id\n\n    @property\n    def mask(self):\n        return self._mask_id\n\n    @property\n    def additional_special_tokens_ids(self):\n        return [self.vocab[k] for k in self._t5_tokens]\n", "model/pretokenizer/pretokenize.py": "import argparse\nimport json\nimport random\nfrom enum import IntEnum\nfrom pathlib import Path\nfrom subprocess import run\n\nimport indexed_dataset\nimport numpy as np\nimport torch\nfrom model_training.custom_datasets.formatting import DatasetEntryLm, DatasetEntrySft, Role\nfrom model_training.utils.utils import _strtobool, get_dataset, get_dataset_fractions, read_yamls\nfrom tokenizer import build_tokenizer\nfrom torch.utils.data import ConcatDataset, Dataset, Subset\nfrom tqdm import tqdm\n\n\nclass IntRole(IntEnum):\n    System = 0\n    Prompter = 1\n    Assistant = 2\n    Context = 3\n\n\nclass Encoder(object):\n    def __init__(self, args):\n        self.args = args\n        self.tokenizer = build_tokenizer(self.args)\n\n    def encode_text(self, text: str) -> list[int]:\n        return self.tokenizer.tokenize(text)\n\n    def decode(self, tokens: list[int]) -> str:\n        return self.tokenizer.detokenize(tokens)\n\n    @property\n    def special_tokens(self) -> dict:\n        return self.tokenizer._special_tokens\n\n\nclass DatasetWriter:\n    def __init__(\n        self,\n        filename_prefix: str,\n        vocab_size: int,\n        dataset_impl: str = \"mmap\",\n        feature: str = \"text\",\n    ):\n        self.bin_filename = f\"{filename_prefix}-{feature}.bin\"\n        self.idx_filename = f\"{filename_prefix}-{feature}.idx\"\n        self.builder = indexed_dataset.make_builder(self.bin_filename, impl=dataset_impl, vocab_size=vocab_size)\n\n    def add_item(self, tokenized_item):\n        self.builder.add_item(torch.IntTensor(tokenized_item))\n\n    def finalize(self):\n        self.builder.finalize(self.idx_filename)\n\n\ndef format_pairs(pairs: list[str] | tuple[str]) -> tuple[list[str], list[int]]:\n    assert isinstance(pairs, list) or isinstance(pairs, tuple)\n    role_names = (\"user\", \"assistant\")\n    role_ids = (1, 2)\n    return [f\"<|im_start|>{role_names[i%2]}\\n{pairs[i]}<|im_end|>\\n\" for i in range(len(pairs))], [\n        role_ids[i % 2] for i in range(len(pairs))\n    ]\n\n\ndef format_sft_entry(entry: DatasetEntrySft) -> tuple[list[str], list[int]]:\n    turns = []\n    roles = []\n    if entry.system_message and len(entry.system_message) > 0:\n        turns.append(f\"<|im_start|>system\\n{entry.system_message}<|im_end|>\\n\")\n        roles.append(IntRole.System.value)  # 0\n    for m in entry.conversation:\n        if m.context:\n            turns.append(f\"<|im_start|>context\\n{m.context}<|im_end|>\\n\")\n            roles.append(IntRole.Context.value)  # 3\n        if m.role == Role.prompter:\n            turns.append(f\"<|im_start|>user\\n{m.text}<|im_end|>\\n\")\n            roles.append(IntRole.Prompter.value)  # 1\n        elif m.role == Role.assistant:\n            turns.append(f\"<|im_start|>assistant\\n{m.text}<|im_end|>\\n\")\n            roles.append(IntRole.Assistant.value)  # 2\n    return turns, roles\n\n\ndef format_conversation(messages) -> str:\n    if isinstance(messages, DatasetEntrySft):\n        return format_sft_entry(messages)\n    elif isinstance(messages, DatasetEntryLm):\n        return messages.text, [3]\n    else:\n        return format_pairs(messages)\n\n\ndef get_dataset_name(d: Dataset):\n    if isinstance(d, Subset):\n        inner = d\n        while isinstance(inner, Subset):\n            inner = inner.dataset\n        name = f\"Subset of {type(inner).__name__}\"\n        if hasattr(inner, \"name\"):\n            name += f\" ({inner.name})\"\n    else:\n        name = type(d).__name__\n        if hasattr(d, \"name\"):\n            name += f\" ({d.name})\"\n    return name\n\n\nclass TokenStats:\n    def __init__(self, name: str, total_samples: int, fraction: float = 1):\n        self.name = name\n        self.skipped_samples = 0\n        self.skipped_tokens = 0\n        self.total_samples = total_samples\n        self.min_tokens = None\n        self.max_tokens = 0\n        self.accepted_samples = 0\n        self.accepted_tokens = 0\n        self.fraction = fraction\n\n    @property\n    def processed_samples(self) -> int:\n        return self.accepted_samples + self.skipped_samples\n\n    def skip(self, tokens: list[int]) -> None:\n        self.skipped_samples += 1\n        self.skipped_tokens = len(tokens)\n\n    def add(self, tokens: list[int]) -> None:\n        l = len(tokens)\n        self.accepted_samples += 1\n        self.accepted_tokens += l\n        if self.min_tokens is None or self.min_tokens > l:\n            self.min_tokens = l\n        if self.max_tokens < l:\n            self.max_tokens = l\n\n\ndef tokenize_dataset(\n    output_dir: Path,\n    filename_prefix: str,\n    dataset: Dataset,\n    encoder: Encoder,\n    dataset_impl: str,\n    datasets_config: dict,\n    max_count: int | None = None,\n    min_assistant_tokens: int | None = None,\n    check_tokenization: bool = True,\n    write_json: bool = False,\n    seed: int = 42,\n):\n    full_prefix = str(output_dir / filename_prefix)\n\n    token_writer = None\n    role_writer = None\n    jsonl_file = None\n\n    per_dataset_stats: list[TokenStats] = []\n    cumulative_sizes: list[int] = []\n\n    rng = np.random.RandomState(seed=seed)\n\n    if isinstance(dataset, ConcatDataset):\n        datasets = list(dataset.datasets)\n\n        if datasets_config:\n            dataset_sizes = [len(x) for x in datasets]\n            fractions = get_dataset_fractions(datasets_config, dataset_sizes, False)\n            dataset_target_sizes = [int(size * frac) for size, frac in zip(dataset_sizes, fractions)]\n        else:\n            dataset_target_sizes = None\n\n        for i in range(len(datasets)):\n            d = datasets[i]\n            name = get_dataset_name(d)\n            frac = 1\n            if dataset_target_sizes:\n                frac = fractions[i]\n                if dataset_target_sizes[i] < len(d):\n                    # sample subset of dataset\n                    subset_indices = rng.choice(len(d), size=dataset_target_sizes[i], replace=False)\n                    d = Subset(d, subset_indices)\n                    datasets[i] = d\n\n            per_dataset_stats.append(TokenStats(name, len(d), frac))\n\n        dataset = ConcatDataset(datasets)\n        cumulative_sizes = dataset.cumulative_sizes\n    else:\n        cumulative_sizes = [len(dataset)]\n\n    total_stats = TokenStats(\"total\", len(dataset))\n\n    try:\n        token_writer = DatasetWriter(\n            filename_prefix=full_prefix,\n            dataset_impl=dataset_impl,\n            vocab_size=encoder.tokenizer.vocab_size,\n            feature=\"text\",\n        )\n\n        role_writer = DatasetWriter(\n            filename_prefix=full_prefix,\n            dataset_impl=dataset_impl,\n            vocab_size=16,\n            feature=\"role\",\n        )\n\n        jsonl_path = Path(full_prefix + \".jsonl\")\n        if write_json:\n            jsonl_file = jsonl_path.open(\"w\", encoding=\"UTF-8\")\n\n        subset_index = 0\n        for i, messages in enumerate(tqdm(dataset)):\n            if i >= cumulative_sizes[subset_index]:\n                subset_index += 1\n\n            if i > 0 and i % 10000 == 0:\n                print(\n                    f\"Accepted: {total_stats.accepted_samples}/{total_stats.processed_samples} ({total_stats.accepted_samples/total_stats.processed_samples:.1%})\"\n                )\n\n            turns, turn_roles = format_conversation(messages)\n\n            tokens = []\n            role_lables = []\n            num_assistant_tokens = 0\n            for t, r in zip(turns, turn_roles):\n                turn_tokens = encoder.encode_text(t)\n                turn_role = [r] * len(turn_tokens)\n                tokens.extend(turn_tokens)\n                if r == IntRole.Assistant:\n                    num_assistant_tokens += len(turn_tokens)\n                role_lables.extend(turn_role)\n\n            if min_assistant_tokens is not None and num_assistant_tokens < min_assistant_tokens:\n                total_stats.skip(tokens)\n                per_dataset_stats[subset_index].skip(tokens)\n                continue\n\n            if check_tokenization:\n                x = encoder.encode_text(\"\".join(turns))\n                assert x == tokens and len(tokens) == len(role_lables)\n\n            token_writer.add_item(tokens)\n            role_writer.add_item(role_lables)\n\n            # update stats\n            total_stats.add(tokens)\n            per_dataset_stats[subset_index].add(tokens)\n\n            if jsonl_file:\n                json.dump({\"text\": \"\".join(turns)}, jsonl_file)\n                jsonl_file.write(\"\\n\")\n\n            if max_count and total_stats.accepted_samples >= max_count:\n                break\n    finally:\n        if token_writer:\n            token_writer.finalize()\n        if role_writer:\n            role_writer.finalize()\n        if jsonl_file:\n            jsonl_file.close()\n\n    per_dataset_stats.append(total_stats)\n\n    stats_path = Path(full_prefix + \"_stats.txt\")\n    with stats_path.open(\"w\", encoding=\"UTF-8\") as stats_file:\n        for f in (None, stats_file):\n            print(f\"\\n# Stats for {full_prefix}*\\n\", file=f)\n\n            for stats in per_dataset_stats:\n                print(f\"## Stats for '{stats.name}' ({stats.total_samples} samples ({stats.fraction:.1%}))\", file=f)\n                print(\"-----------------\", file=f)\n                print(\n                    f\"  Accepted: {stats.accepted_samples}/{stats.processed_samples} ({stats.accepted_samples/stats.processed_samples:.1%})\",\n                    file=f,\n                )\n                print(f\"  Accepted tokens: {stats.accepted_tokens}\", file=f)\n                print(\n                    f\"  Skipped: {stats.skipped_samples} ({stats.skipped_samples/stats.processed_samples:.1%})\", file=f\n                )\n                print(f\"  Min tokens per sample: {stats.min_tokens}\", file=f)\n                print(f\"  Max tokens per sample: {stats.max_tokens}\", file=f)\n                print(f\"  Avg tokens per sample: {stats.accepted_tokens/stats.accepted_samples}\", file=f)\n                print(\"-----------------\\n\", file=f)\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(\n        prog=\"pretokenize.py\", description=\"Tokenize datamixes for LLama2/Falcon fine-tuning with Megatron-LLM.\"\n    )\n    group = parser.add_argument_group(title=\"configuration\")\n    group.add_argument(\n        \"--configs\",\n        nargs=\"+\",\n        required=True,\n        help=\"Configurations sections to apply (read from YAML, multiple can be specified).\",\n    )\n    group.add_argument(\n        \"--output_dir\",\n        type=str,\n        help=\"Path to output directory\",\n    )\n    group.add_argument(\n        \"--write_json\",\n        action=\"store_true\",\n        help=\"Generate a JSONL file with the formatted dialogues (key='text').\",\n    )\n    group.add_argument(\n        \"--compress\",\n        action=\"store_true\",\n        help=\"Generate a .tar.gz file of the output directory.\",\n    )\n\n    args, remaining = parser.parse_known_args()\n\n    # load yaml configurations\n    conf = {}\n    configs = read_yamls(\"./configs\")\n    conf.update(configs[\"defaults\"])\n    try:\n        for name in args.configs:\n            if \",\" in name:\n                for n in name.split(\",\"):\n                    conf.update(configs[n])\n            else:\n                conf.update(configs[name])\n    except KeyError as e:\n        print(f'Error: Section \"{e.args[0]}\" not found in YAML configuration files.')\n        exit(1)\n\n    # override yaml args\n    for k, v in vars(args).items():\n        if k == \"configs\" or v is None:\n            continue\n        conf[k] = v\n\n    parser = argparse.ArgumentParser()\n    for key, value in conf.items():\n        type_ = type(value) if value is not None else str\n        if type_ == bool:\n            type_ = _strtobool\n        parser.add_argument(f\"--{key}\", type=type_, default=value)\n        # Allow --no-{key}  to remove a configuration value\n        parser.add_argument(f\"--no-{key}\", dest=key, action=\"store_const\", const=None)\n    parser.add_argument(\n        \"--max_count\",\n        type=int,\n        help=\"Limit number of train/eval examples to process (debug)\",\n    )\n\n    args = parser.parse_args(remaining)\n    args.keep_empty = False\n    args.rank = 0\n    args.vocab_extra_ids = 0\n    args.make_vocab_size_divisible_by = 128\n    args.tensor_model_parallel_size = 1\n    args.new_tokens = True\n\n    return args\n\n\ndef main():\n    \"\"\"\n    Example usage: `python __main__.py --output_dir output--configs oasst_top1 llama2`\n    \"\"\"\n    args = parse_args()\n    print(\"Configuration:\")\n    for k, v in vars(args).items():\n        print(f\"{k}: {v}\")\n\n    # initialize random states for reproducibility\n    random.seed(args.rng_seed)\n    np.random.seed(args.rng_seed)\n    torch.manual_seed(args.rng_seed)\n\n    print(\"Building encoder\")\n    encoder = Encoder(args)\n\n    tokenizer_check_input = \"<|im_start|>system\\nsystem message<|im_end|>\\n<|im_start|>user\\nprompt<|im_end|><|im_start|>assistant\\nreply<|im_end|>\\n\"\n    tokenizer_check_output = encoder.encode_text(tokenizer_check_input)\n    print(\"Tokenizer check:\")\n    print(\"Input:\", tokenizer_check_input.replace(\"\\n\", r\"\\n\"))\n    print(\"Output:\", tokenizer_check_output)\n    print(f\"Vocab size: {encoder.tokenizer.vocab_size}\")\n\n    output_dir = Path(args.output_dir + args.output_dir_suffix)\n    print(f\"Output dir: {output_dir} (exists: {output_dir.exists()})\")\n\n    train, evals = get_dataset(args)\n\n    # show dataset stats\n    print(\"Training dataset sizes (before sampling):\")\n    total = len(train)\n    for d in train.datasets:\n        name = get_dataset_name(d)\n        print(f\"{name}: {len(d)} ({len(d) / total:.2%})\")\n\n    output_dir.mkdir(parents=True, exist_ok=True)\n\n    fn = output_dir / \"special_tokens.json\"\n    with fn.open(\"w\", encoding=\"UTF-8\") as f:\n        json.dump(encoder.special_tokens, f)\n\n    val = ConcatDataset(evals.values())\n    for split_name, ds in zip([\"train\", \"val\"], [train, val]):\n        datasets_config = args.datasets if split_name == \"train\" else None\n        tokenize_dataset(\n            output_dir=output_dir,\n            filename_prefix=f\"{args.filename_prefix}-{split_name}\",\n            dataset=ds,\n            encoder=encoder,\n            dataset_impl=args.dataset_impl,\n            datasets_config=datasets_config,\n            max_count=args.max_count,\n            min_assistant_tokens=args.min_assistant_tokens,\n            write_json=args.write_json,\n            seed=args.rng_seed,\n        )\n\n    if args.compress:\n        run(f\"tar -czvf {output_dir}.tar.gz {output_dir}\", shell=True, check=True)\n\n\nif __name__ == \"__main__\":\n    main()\n", "text-frontend/auto_main.py": "\"\"\"Simple REPL frontend.\"\"\"\n\nimport http\nimport random\nfrom uuid import uuid4\n\nimport requests\nimport typer\nfrom faker import Faker\n\napp = typer.Typer()\nfake = Faker()\n\n\ndef _random_message_id():\n    return str(uuid4())\n\n\ndef _render_message(message: dict) -> str:\n    \"\"\"Render a message to the user.\"\"\"\n    if message[\"is_assistant\"]:\n        return f\"Assistant: {message['text']}\"\n    return f\"Prompter: {message['text']}\"\n\n\n@app.command()\ndef main(\n    backend_url: str = \"http://127.0.0.1:8080\", api_key: str = \"1234\", random_users: int = 1, tasks_per_user: int = 10\n):\n    \"\"\"automates tasks\"\"\"\n\n    def _post(path: str, json: dict) -> dict:\n        response = requests.post(f\"{backend_url}{path}\", json=json, headers={\"X-API-Key\": api_key})\n        response.raise_for_status()\n        if response.status_code == http.HTTPStatus.NO_CONTENT:\n            return None\n        return response.json()\n\n    def gen_random_text():\n        return \" \".join([random.choice([\"hello\", \"world\", \"foo\", \"bar\"]) for _ in range(10)])\n\n    def gen_random_ranking(messages):\n        \"\"\"rank messages randomly and return list of indexes in order of rank randomly\"\"\"\n        print(\"Ranking\")\n        print(messages)\n        print(len(messages))\n        ranks = [i for i in range(len(messages))]\n        shuffled = random.shuffle(ranks)\n        print(ranks)\n        print(shuffled)\n        return ranks\n\n    for i in range(int(random_users)):\n        name = fake.name()\n        USER = {\"id\": name, \"display_name\": name, \"auth_method\": \"local\"}\n\n        create_user_request = dict(USER)\n        # make sure dummy user has accepted the terms of service\n        create_user_request[\"tos_acceptance\"] = True\n        response = requests.post(\n            f\"{backend_url}/api/v1/frontend_users/\", json=create_user_request, headers={\"X-API-Key\": api_key}\n        )\n        response.raise_for_status()\n        user = response.json()\n        typer.echo(f\"user: {user}\")\n        q = 0\n\n        tasks = [_post(\"/api/v1/tasks/\", {\"type\": \"random\", \"user\": USER})]\n\n        while tasks:\n            task = tasks.pop(0)\n            print(task)\n\n            match (task[\"type\"]):\n                case \"initial_prompt\":\n                    typer.echo(\"Please provide an initial prompt to the assistant.\")\n                    if task[\"hint\"]:\n                        typer.echo(f\"Hint: {task['hint']}\")\n                    # acknowledge task\n                    message_id = _random_message_id()\n                    _post(f\"/api/v1/tasks/{task['id']}/ack\", {\"message_id\": message_id})\n\n                    prompt = gen_random_text()\n                    user_message_id = _random_message_id()\n                    # send interaction\n                    new_task = _post(\n                        \"/api/v1/tasks/interaction\",\n                        {\n                            \"type\": \"text_reply_to_message\",\n                            \"message_id\": message_id,\n                            \"task_id\": task[\"id\"],\n                            \"user_message_id\": user_message_id,\n                            \"text\": prompt,\n                            \"user\": USER,\n                        },\n                    )\n                    tasks.append(new_task)\n\n                case \"label_initial_prompt\":\n                    typer.echo(\"Label the following prompt:\")\n                    typer.echo(task[\"prompt\"])\n                    # acknowledge task\n                    message_id = _random_message_id()\n                    _post(f\"/api/v1/tasks/{task['id']}/ack\", {\"message_id\": message_id})\n\n                    valid_labels = task[\"valid_labels\"]\n                    mandatory_labels = task[\"mandatory_labels\"]\n\n                    labels_dict = None\n                    if task[\"mode\"] == \"simple\" and len(valid_labels) == 1:\n                        answer = random.choice([True, False])\n                        labels_dict = {valid_labels[0]: 1 if answer else 0}\n                    else:\n                        labels = random.sample(valid_labels, random.randint(1, len(valid_labels)))\n                        for l in mandatory_labels:\n                            if l not in labels:\n                                labels.append(l)\n                        labels_dict = {label: random.random() for label in valid_labels}\n                    if random.random() < 0.9:\n                        labels_dict[\"spam\"] = 0\n                        labels_dict[\"lang_mismatch\"] = 0\n\n                    # send labels\n                    new_task = _post(\n                        \"/api/v1/tasks/interaction\",\n                        {\n                            \"type\": \"text_labels\",\n                            \"message_id\": task[\"message_id\"],\n                            \"task_id\": task[\"id\"],\n                            \"text\": task[\"prompt\"],\n                            \"labels\": labels_dict,\n                            \"user\": USER,\n                        },\n                    )\n                    tasks.append(new_task)\n                case \"prompter_reply\":\n                    # acknowledge task\n                    message_id = _random_message_id()\n                    user_message_id = _random_message_id()\n                    _post(f\"/api/v1/tasks/{task['id']}/ack\", {\"message_id\": message_id})\n                    # send interaction\n                    new_task = _post(\n                        \"/api/v1/tasks/interaction\",\n                        {\n                            \"type\": \"text_reply_to_message\",\n                            \"message_id\": message_id,\n                            \"task_id\": task[\"id\"],\n                            \"user_message_id\": user_message_id,\n                            \"text\": gen_random_text(),\n                            \"user\": USER,\n                        },\n                    )\n                    tasks.append(new_task)\n\n                case \"assistant_reply\":\n                    # acknowledge task\n                    message_id = _random_message_id()\n                    user_message_id = _random_message_id()\n                    _post(f\"/api/v1/tasks/{task['id']}/ack\", {\"message_id\": message_id})\n                    # send interaction\n                    new_task = _post(\n                        \"/api/v1/tasks/interaction\",\n                        {\n                            \"type\": \"text_reply_to_message\",\n                            \"message_id\": message_id,\n                            \"task_id\": task[\"id\"],\n                            \"user_message_id\": user_message_id,\n                            \"text\": gen_random_text(),\n                            \"user\": USER,\n                        },\n                    )\n                    tasks.append(new_task)\n\n                case \"rank_prompter_replies\" | \"rank_assistant_replies\":\n                    # acknowledge task\n                    message_id = _random_message_id()\n                    user_message_id = _random_message_id()\n                    _post(f\"/api/v1/tasks/{task['id']}/ack\", {\"message_id\": message_id})\n                    # send interaction\n                    ranking = gen_random_ranking(task[\"replies\"])\n                    print(ranking)\n                    new_task = _post(\n                        \"/api/v1/tasks/interaction\",\n                        {\n                            \"type\": \"message_ranking\",\n                            \"message_id\": message_id,\n                            \"task_id\": task[\"id\"],\n                            \"ranking\": ranking,\n                            \"user\": USER,\n                        },\n                    )\n                    tasks.append(new_task)\n\n                case \"rank_initial_prompts\":\n                    # acknowledge task\n                    message_id = _random_message_id()\n                    user_message_id = _random_message_id()\n                    _post(f\"/api/v1/tasks/{task['id']}/ack\", {\"message_id\": message_id})\n                    # send interaction\n                    ranking = gen_random_ranking(task[\"prompots\"])\n                    new_task = _post(\n                        \"/api/v1/tasks/interaction\",\n                        {\n                            \"type\": \"message_ranking\",\n                            \"message_id\": message_id,\n                            \"ranking\": ranking,\n                            \"user\": USER,\n                        },\n                    )\n                    tasks.append(new_task)\n\n                case \"label_prompter_reply\" | \"label_assistant_reply\":\n                    # acknowledge task\n                    typer.echo(\"Here is the conversation so far:\")\n                    for message in task[\"conversation\"][\"messages\"]:\n                        typer.echo(_render_message(message))\n\n                    typer.echo(\"Label the following reply:\")\n                    typer.echo(task[\"reply\"])\n                    message_id = _random_message_id()\n                    user_message_id = _random_message_id()\n                    _post(f\"/api/v1/tasks/{task['id']}/ack\", {\"message_id\": message_id})\n                    valid_labels = task[\"valid_labels\"]\n                    mandatory_labels = task[\"mandatory_labels\"]\n\n                    labels_dict = None\n                    if task[\"mode\"] == \"simple\" and len(valid_labels) == 1:\n                        answer = random.choice([True, False])\n                        labels_dict = {valid_labels[0]: 1 if answer else 0}\n                    else:\n                        labels = random.sample(valid_labels, random.randint(1, len(valid_labels)))\n                        for l in mandatory_labels:\n                            if l not in labels:\n                                labels.append(l)\n                        labels_dict = {label: random.random() for label in valid_labels}\n                    if random.random() < 0.9:\n                        labels_dict[\"spam\"] = 0\n                        labels_dict[\"lang_mismatch\"] = 0\n\n                    # send interaction\n                    new_task = _post(\n                        \"/api/v1/tasks/interaction\",\n                        {\n                            \"type\": \"text_labels\",\n                            \"message_id\": task[\"message_id\"],\n                            \"task_id\": task[\"id\"],\n                            \"text\": task[\"reply\"],\n                            \"labels\": labels_dict,\n                            \"user\": USER,\n                        },\n                    )\n                    tasks.append(new_task)\n                case \"task_done\":\n                    typer.echo(\"Task done!\")\n                    # rerun with new task selected from above cases\n                    # add a new task\n                    q += 1\n                    if q == tasks_per_user:\n                        typer.echo(\"Task done!\")\n                        break\n                    tasks = [_post(\"/api/v1/tasks/\", {\"type\": \"random\", \"user\": USER})]\n                    #\n                case _:\n                    typer.echo(f\"Unknown task type {task['type']}\")\n                    # rerun with new task selected from above cases\n\n\nif __name__ == \"__main__\":\n    app()\n", "text-frontend/__main__.py": "\"\"\"Simple REPL frontend.\"\"\"\n\nimport http\nimport random\n\nimport requests\nimport typer\n\napp = typer.Typer()\n\n\n# debug constants\nUSER = {\"id\": \"1234\", \"display_name\": \"John Doe\", \"auth_method\": \"local\"}\n\n\ndef _random_message_id():\n    return str(random.randint(1000, 9999))\n\n\ndef _render_message(message: dict) -> str:\n    \"\"\"Render a message to the user.\"\"\"\n    if message[\"is_assistant\"]:\n        return f\"Assistant: {message['text']}\"\n    return f\"Prompter: {message['text']}\"\n\n\n@app.command()\ndef main(backend_url: str = \"http://127.0.0.1:8080\", api_key: str = \"1234\"):\n    \"\"\"Simple REPL frontend.\"\"\"\n\n    # make sure dummy user has accepted the terms of service\n    create_user_request = dict(USER)\n    create_user_request[\"tos_acceptance\"] = True\n    response = requests.post(\n        f\"{backend_url}/api/v1/frontend_users/\", json=create_user_request, headers={\"X-API-Key\": api_key}\n    )\n    response.raise_for_status()\n    user = response.json()\n    typer.echo(f\"user: {user}\")\n\n    def _post(path: str, json: dict) -> dict:\n        response = requests.post(f\"{backend_url}{path}\", json=json, headers={\"X-API-Key\": api_key})\n        response.raise_for_status()\n        if response.status_code == http.HTTPStatus.NO_CONTENT:\n            return None\n        return response.json()\n\n    typer.echo(\"Requesting work...\")\n    tasks = [_post(\"/api/v1/tasks/\", {\"type\": \"random\", \"user\": USER})]\n    while tasks:\n        task = tasks.pop(0)\n        match (task[\"type\"]):\n            case \"summarize_story\":\n                typer.echo(\"Summarize the following story:\")\n                typer.echo(task[\"story\"])\n\n                # acknowledge task\n                message_id = _random_message_id()\n                _post(f\"/api/v1/tasks/{task['id']}/ack\", {\"message_id\": message_id})\n\n                summary = typer.prompt(\"Enter your summary\")\n\n                user_message_id = _random_message_id()\n\n                # send interaction\n                new_task = _post(\n                    \"/api/v1/tasks/interaction\",\n                    {\n                        \"type\": \"text_reply_to_message\",\n                        \"message_id\": message_id,\n                        \"task_id\": task[\"id\"],\n                        \"user_message_id\": user_message_id,\n                        \"text\": summary,\n                        \"user\": USER,\n                    },\n                )\n                tasks.append(new_task)\n            case \"rate_summary\":\n                typer.echo(\"Rate the following summary:\")\n                typer.echo(task[\"summary\"])\n                typer.echo(\"Full text:\")\n                typer.echo(task[\"full_text\"])\n                typer.echo(f\"Rating scale: {task['scale']['min']} - {task['scale']['max']}\")\n\n                # acknowledge task\n                message_id = _random_message_id()\n                _post(f\"/api/v1/tasks/{task['id']}/ack\", {\"message_id\": message_id})\n\n                rating = typer.prompt(\"Enter your rating\", type=int)\n                # send interaction\n                new_task = _post(\n                    \"/api/v1/tasks/interaction\",\n                    {\n                        \"type\": \"message_rating\",\n                        \"message_id\": message_id,\n                        \"rating\": rating,\n                        \"user\": USER,\n                    },\n                )\n                tasks.append(new_task)\n            case \"initial_prompt\":\n                typer.echo(\"Please provide an initial prompt to the assistant.\")\n                if task[\"hint\"]:\n                    typer.echo(f\"Hint: {task['hint']}\")\n                # acknowledge task\n                message_id = _random_message_id()\n                _post(f\"/api/v1/tasks/{task['id']}/ack\", {\"message_id\": message_id})\n                prompt = typer.prompt(\"Enter your prompt\")\n                user_message_id = _random_message_id()\n                # send interaction\n                new_task = _post(\n                    \"/api/v1/tasks/interaction\",\n                    {\n                        \"type\": \"text_reply_to_message\",\n                        \"message_id\": message_id,\n                        \"task_id\": task[\"id\"],\n                        \"user_message_id\": user_message_id,\n                        \"text\": prompt,\n                        \"user\": USER,\n                    },\n                )\n                tasks.append(new_task)\n\n            case \"prompter_reply\":\n                typer.echo(\"Please provide a reply to the assistant.\")\n                typer.echo(\"Here is the conversation so far:\")\n                for message in task[\"conversation\"][\"messages\"]:\n                    typer.echo(_render_message(message))\n                if task[\"hint\"]:\n                    typer.echo(f\"Hint: {task['hint']}\")\n                # acknowledge task\n                message_id = _random_message_id()\n                _post(f\"/api/v1/tasks/{task['id']}/ack\", {\"message_id\": message_id})\n                reply = typer.prompt(\"Enter your reply\")\n                user_message_id = _random_message_id()\n                # send interaction\n                new_task = _post(\n                    \"/api/v1/tasks/interaction\",\n                    {\n                        \"type\": \"text_reply_to_message\",\n                        \"message_id\": message_id,\n                        \"user_message_id\": user_message_id,\n                        \"text\": reply,\n                        \"user\": USER,\n                    },\n                )\n                tasks.append(new_task)\n\n            case \"assistant_reply\":\n                typer.echo(\"Act as the assistant and reply to the user.\")\n                typer.echo(\"Here is the conversation so far:\")\n                for message in task[\"conversation\"][\"messages\"]:\n                    typer.echo(_render_message(message))\n                # acknowledge task\n                message_id = _random_message_id()\n                _post(f\"/api/v1/tasks/{task['id']}/ack\", {\"message_id\": message_id})\n                reply = typer.prompt(\"Enter your reply\")\n                user_message_id = _random_message_id()\n                # send interaction\n                new_task = _post(\n                    \"/api/v1/tasks/interaction\",\n                    {\n                        \"type\": \"text_reply_to_message\",\n                        \"message_id\": message_id,\n                        \"task_id\": task[\"id\"],\n                        \"user_message_id\": user_message_id,\n                        \"text\": reply,\n                        \"user\": USER,\n                    },\n                )\n                tasks.append(new_task)\n\n            case \"rank_initial_prompts\":\n                typer.echo(\"Rank the following prompts:\")\n                for idx, prompt in enumerate(task[\"prompts\"], start=1):\n                    typer.echo(f\"{idx}: {prompt}\")\n                # acknowledge task\n                message_id = _random_message_id()\n                _post(f\"/api/v1/tasks/{task['id']}/ack\", {\"message_id\": message_id})\n\n                ranking_str = typer.prompt(\"Enter the prompt numbers in order of preference, separated by commas\")\n                ranking = [int(x) - 1 for x in ranking_str.split(\",\")]\n\n                # send ranking\n                new_task = _post(\n                    \"/api/v1/tasks/interaction\",\n                    {\n                        \"type\": \"message_ranking\",\n                        \"message_id\": message_id,\n                        \"ranking\": ranking,\n                        \"user\": USER,\n                    },\n                )\n                tasks.append(new_task)\n\n            case \"rank_prompter_replies\" | \"rank_assistant_replies\":\n                typer.echo(\"Here is the conversation so far:\")\n                for message in task[\"conversation\"][\"messages\"]:\n                    typer.echo(_render_message(message))\n                typer.echo(\"Rank the following replies:\")\n                for idx, reply in enumerate(task[\"replies\"], start=1):\n                    typer.echo(f\"{idx}: {reply}\")\n                # acknowledge task\n                message_id = _random_message_id()\n                _post(f\"/api/v1/tasks/{task['id']}/ack\", {\"message_id\": message_id})\n\n                ranking_str = typer.prompt(\"Enter the reply numbers in order of preference, separated by commas\")\n                ranking = [int(x) - 1 for x in ranking_str.split(\",\")]\n\n                # send labels\n                new_task = _post(\n                    \"/api/v1/tasks/interaction\",\n                    {\n                        \"type\": \"message_ranking\",\n                        \"message_id\": message_id,\n                        \"task_id\": task[\"id\"],\n                        \"ranking\": ranking,\n                        \"user\": USER,\n                    },\n                )\n                tasks.append(new_task)\n\n            case \"label_initial_prompt\":\n                typer.echo(\"Label the following prompt:\")\n                typer.echo(task[\"prompt\"])\n                # acknowledge task\n                message_id = _random_message_id()\n                _post(f\"/api/v1/tasks/{task['id']}/ack\", {\"message_id\": message_id})\n\n                valid_labels = task[\"valid_labels\"]\n\n                labels_dict = None\n                if task[\"mode\"] == \"simple\" and len(valid_labels) == 1:\n                    answer: str = typer.confirm(f\"{valid_labels[0]}?\")\n                    labels_dict = {valid_labels[0]: 1 if answer else 0}\n                else:\n                    while labels_dict is None:\n                        labels_str: str = typer.prompt(\"Enter labels, separated by commas\")\n                        labels = labels_str.lower().replace(\" \", \"\").split(\",\")\n\n                        if all([label in valid_labels for label in labels]):\n                            labels_dict = {label: \"1\" if label in labels else \"0\" for label in valid_labels}\n                        else:\n                            invalid_labels = [label for label in labels if label not in valid_labels]\n                            typer.echo(f\"Invalid labels: {', '.join(invalid_labels)}. Valid: {', '.join(valid_labels)}\")\n\n                # send labels\n                new_task = _post(\n                    \"/api/v1/tasks/interaction\",\n                    {\n                        \"type\": \"text_labels\",\n                        \"message_id\": task[\"message_id\"],\n                        \"task_id\": task[\"id\"],\n                        \"text\": task[\"prompt\"],\n                        \"labels\": labels_dict,\n                        \"user\": USER,\n                    },\n                )\n                tasks.append(new_task)\n\n            case \"label_prompter_reply\" | \"label_assistant_reply\":\n                typer.echo(\"Here is the conversation so far:\")\n                for message in task[\"conversation\"][\"messages\"]:\n                    typer.echo(_render_message(message))\n\n                typer.echo(\"Label the following reply:\")\n                typer.echo(task[\"reply\"])\n                # acknowledge task\n                message_id = _random_message_id()\n                _post(f\"/api/v1/tasks/{task['id']}/ack\", {\"message_id\": message_id})\n\n                valid_labels = task[\"valid_labels\"]\n\n                labels_dict = None\n                if task[\"mode\"] == \"simple\" and len(valid_labels) == 1:\n                    answer: str = typer.confirm(f\"{valid_labels[0]}?\")\n                    labels_dict = {valid_labels[0]: 1 if answer else 0}\n                else:\n                    while labels_dict is None:\n                        labels_str: str = typer.prompt(\"Enter labels, separated by commas\")\n                        labels = labels_str.lower().replace(\" \", \"\").split(\",\")\n\n                        if all([label in valid_labels for label in labels]):\n                            labels_dict = {label: \"1\" if label in labels else \"0\" for label in valid_labels}\n                        else:\n                            invalid_labels = [label for label in labels if label not in valid_labels]\n                            typer.echo(f\"Invalid labels: {', '.join(invalid_labels)}. Valid: {', '.join(valid_labels)}\")\n\n                # send labels\n                new_task = _post(\n                    \"/api/v1/tasks/interaction\",\n                    {\n                        \"type\": \"text_labels\",\n                        \"message_id\": task[\"message_id\"],\n                        \"task_id\": task[\"id\"],\n                        \"text\": task[\"reply\"],\n                        \"labels\": labels_dict,\n                        \"user\": USER,\n                    },\n                )\n                tasks.append(new_task)\n\n            case \"task_done\":\n                typer.echo(\"Task done!\")\n            case _:\n                typer.echo(f\"Unknown task type {task['type']}\")\n\n\nif __name__ == \"__main__\":\n    app()\n", "scripts/data-collection/twitter/twitter_process_json.py": "# This file loops through compressed json tweet data, pre-processes them,\n# and then extracts them into more unified parquet files that can be handed\n# off for further processing. The main focus is on producing viable replies.\n\n# Initial data exploration seems that there is no guarantee that the original\n# tweets are in the archive, so we might need to extract suitable replies\n# then get the original tweets separately, and then combine them into a\n# suitable thread format that can be used by our instruction model.\n\n# This assumes data downloaded from https://archive.org/details/twitterstream\n# and that the internal .tar files are extracted locally.\n# They are large files so using something like 7Zip or WinRar might be easier\n# than putting all of it in scripts, but it is a possibility.\n\n# I often work in notebooks. If you encounter any issue, please reach out to let me know.\n\nimport bz2\nimport gzip\nimport json\nimport pickle\nfrom pathlib import Path\n\nimport numpy as np\nimport polars as pl\nfrom tqdm import tqdm\n\n# TODO: OPTIONAL - Put the Untar process in a script instead of doing that part externally. Twitterstream archives are .tar with folders and json.gz files inside.\n# TODO: Set up list of important hashtags & keywords. This might have to be done after we get the original tweets in a separate file.\n# TODO: Process data and filter based on hashtags & keywords\n\n# Sets up paths\n# TODO: Source paths from env file\npath_string = \"PUT THE PATH HERE TO WHERE YOU DOWNLOADED AND EXTRACTED THE ARCHIVE .TAR\"\nfolder_path = Path(path_string)\nfile_list_pkl = folder_path / \"file_list.pkl\"\nprocessed_file_list_pkl = folder_path / \"processed_file_list.pkl\"\n\n# For the processed folder to save inside, we can create the directory if it doesn't exist\nprocessed_folder_path = folder_path / \"processed\"\nprocessed_folder_path.mkdir(parents=True, exist_ok=True)\n\n# Set max buffer to store temporary dataframes for processing\n# Change this depending on the memory of your computer\nprocessed_max_buffer = 5000\n\n# Set up list of wanted column names.\n# Note: User columns are prefixed with user_\nwanted_cols = [\n    \"timestamp_ms\",\n    \"id\",\n    \"text\",\n    \"truncated\",\n    \"in_reply_to_status_id\",\n    \"in_reply_to_user_id\",\n    \"is_quote_status\",\n    \"quote_count\",\n    \"reply_count\",\n    \"retweet_count\",\n    \"favorite_count\",\n    \"filter_level\",\n    \"lang\",\n    \"possibly_sensitive\",\n    \"hashtags\",\n    \"user_id\",\n    \"user_verified\",\n    \"user_followers_count\",\n    \"user_statuses_count\",\n]\n\n\ndef main(file_list_pkl, folder_path, processed_max_buffer):\n    \"\"\"\n    Runs the main processing script to get files, loop through them, and process them.\n    Outputs larger json.gz files made by concat the pre-filtered dataframes from\n    the original json.gz files.\n    \"\"\"\n\n    file_list = get_file_paths(file_list_pkl, folder_path)\n\n    process_json(file_list, processed_max_buffer)\n\n    print(\"Done\")\n\n\ndef get_file_paths(file_list_pkl, folder_path):\n    \"\"\"\n    Gets the file paths by recursively checking the folder structure.\n    # Based on code from stackoverflow https://stackoverflow.com/questions/26835477/pickle-load-variable-if-exists-or-create-and-save-it\n    \"\"\"\n    try:\n        allpaths = pickle.load(open(file_list_pkl, \"rb\"))\n    except (OSError, IOError) as e:\n        print(e)\n        allpaths = sorted(list(folder_path.rglob(\"*.[gz bz2]*\")))\n        pickle.dump(allpaths, open(file_list_pkl, \"wb\"))\n    print(\"Got file paths.\")\n    return allpaths\n\n\ndef get_processed_list(processed_file_list_pkl):\n    # Gets processed file list if stored, if not, creates it.\n    try:\n        processed_list = pickle.load(open(processed_file_list_pkl, \"rb\"))\n    except (OSError, IOError) as e:\n        print(e)\n        processed_list = []\n        pickle.dump(processed_list, open(processed_file_list_pkl, \"wb\"))\n    return processed_list\n\n\ndef modify_dict_cols(j_dict):\n    # Extracting some nested json\n    j_dict[\"user_id\"] = np.int64(j_dict[\"user\"][\"id\"])\n    j_dict[\"user_followers_count\"] = np.int64(j_dict[\"user\"][\"followers_count\"])\n    j_dict[\"user_statuses_count\"] = np.int64(j_dict[\"user\"][\"statuses_count\"])\n\n    # Get hashtags as a list of strings\n    j_dict[\"hashtags\"] = [h[\"text\"] for h in j_dict[\"entities\"][\"hashtags\"]]\n\n    j_dict[\"id\"] = np.int64(j_dict[\"id\"])\n\n    try:\n        j_dict[\"in_reply_to_status_id\"] = np.int64(j_dict[\"in_reply_to_status_id\"])\n    except Exception as e:\n        print(e)\n        j_dict[\"in_reply_to_status_id\"] = j_dict[\"in_reply_to_status_id\"]\n\n    try:\n        j_dict[\"in_reply_to_user_id\"] = np.int64(j_dict[\"in_reply_to_user_id\"])\n    except Exception as e:\n        print(e)\n        j_dict[\"in_reply_to_user_id\"] = j_dict[\"in_reply_to_user_id\"]\n\n    # Make sure relevant columns are available or none.\n    for key in wanted_cols:\n        if key not in j_dict:\n            j_dict[key] = None\n\n    # Ordering keys and taking wanted columns\n    j_dict = {key: j_dict[key] for key in wanted_cols}\n\n    return j_dict\n\n\ndef process_single_file(f, processed_list):\n    j_dict_list = []\n    if f not in processed_list:\n        # Check for compression type\n        if f.suffix == \".bz2\":\n            with bz2.BZ2File(f) as file:\n                for line in file:\n                    # Load JSON\n                    j_dict = json.loads(line)\n                    # Check if user key exists\n                    if \"delete\" not in j_dict:\n                        if j_dict[\"truncated\"] is False:\n                            j_dict = modify_dict_cols(j_dict)\n\n                            j_dict_list.append(j_dict)\n\n        else:\n            with gzip.open(f, \"r\") as file:\n                for line in file:\n                    # Load JSON\n                    j_dict = json.loads(line)\n                    # Check if user key exists\n                    if \"delete\" not in j_dict:\n                        if j_dict[\"truncated\"] is False:\n                            j_dict = modify_dict_cols(j_dict)\n\n                            j_dict_list.append(j_dict)\n\n        return j_dict_list\n\n\ndef process_json(file_list, processed_max_buffer):\n    \"\"\"\n    Loops through file list and loads the compressed\n    json into a list of dicts after some pre-processing.\n\n    Makes sure dicts are ordered in a specific\n    way to make sure polars can read them.\n    \"\"\"\n\n    # Gets processed file list if stored, if not, creates it.\n    processed_list = get_processed_list(processed_file_list_pkl)\n\n    j_list = []\n    temp_processed_files = []\n\n    for i, f in enumerate(tqdm(file_list)):\n        j_dict_list = process_single_file(f, processed_list)\n\n        j_list.extend(j_dict_list)\n\n        temp_processed_files.append(f)\n\n        if len(temp_processed_files) == processed_max_buffer:\n            # If we reach our buffer,\n            # combine into polars dataframe\n            # and write to parquet as\n            # a checkpoint\n            processed_file_name = f\"processed_json_{i}.parquet\"\n            processed_file_path = processed_folder_path / processed_file_name\n\n            pl.DataFrame(j_list, columns=wanted_cols).write_parquet(processed_file_path)\n\n            # Make note of which files have been processed\n            processed_list.extend(temp_processed_files)\n            pickle.dump(processed_list, open(processed_file_list_pkl, \"wb\"))\n\n            # Reset buffer lists\n            j_list = []\n            temp_processed_files = []\n\n    # Process remaining files\n    processed_file_name = f\"processed_json_{i}.parquet\"\n    processed_file_path = processed_folder_path / processed_file_name\n    pl.from_dicts(j_dict_list).write_parquet(processed_file_path)\n    processed_list.extend(temp_processed_files)\n    pickle.dump(processed_list, open(processed_file_list_pkl, \"wb\"))\n    j_dict_list = []\n    temp_processed_files = []\n\n    print(\"Processing completed\")\n\n\nif __name__ == \"__main__\":\n    main(file_list_pkl, folder_path, processed_max_buffer)\n", "scripts/data-collection/twitter/twitter_create_convs.py": "import json\nfrom pathlib import Path\n\nimport polars as pl\nfrom tqdm import tqdm\n\n# Sets up paths\n# TODO: Source paths from env file\npath_string = \"PUT THE PATH HERE TO WHERE YOU STORED THE PARQUET FILES\"\nfolder_path = Path(path_string)\nprocessed_folder_path = folder_path / \"processed\"\noutput_path = folder_path / \"twitter-conv-trees.jsonl\"\n\n# Get parq files\nparq_files = sorted(processed_folder_path.rglob(\"*.parquet\"))\n\nwanted_cols = [\n    \"timestamp_ms\",\n    \"id\",\n    \"text\",\n    \"truncated\",\n    \"in_reply_to_status_id\",\n    \"in_reply_to_user_id\",\n    \"is_quote_status\",\n    \"quote_count\",\n    \"reply_count\",\n    \"retweet_count\",\n    \"favorite_count\",\n    \"filter_level\",\n    \"lang\",\n    \"possibly_sensitive\",\n    \"hashtags\",\n    \"user_id\",\n    \"user_verified\",\n    \"user_followers_count\",\n    \"user_statuses_count\",\n]\n\n# Load parqs into list. Using Polars for performance reasons.\ndf_list = []\nfor p in parq_files:\n    df_list.append(pl.read_parquet(p, columns=wanted_cols))\n\n# Create major dataframe.\n# This can be done incrementally if RAM is constrained by modifying the above code.\np_df = pl.concat(df_list)\n\n# Clean up the reference just in case to help with memory if needed.\ndel df_list\n\n# Get tweets that are replies to other tweets\np_df_replies_only = p_df.filter(pl.col(\"in_reply_to_status_id\").is_null().is_not())\n\n# Group by replied to status id to see the most replied to statuses. This can take some time.\np_df_group_reply_to_status = p_df_replies_only.groupby(\"in_reply_to_status_id\").count().sort(\"count\", reverse=True)\n\n# Save output of grouping the top replied to statuses\ngroup_reply_parq = folder_path / \"group_reply_parq.parquet\"\np_df_group_reply_to_status.write_parquet(group_reply_parq)\n\n# Join the main dataframe with the top replies to find tweets that have replies.\np_join = p_df.join(p_df_group_reply_to_status, left_on=\"id\", right_on=\"in_reply_to_status_id\", how=\"inner\")\n\n# Save output of tweets that have replies\ntweets_that_have_replies_path = folder_path / \"tweets_that_have_replies.parquet\"\np_join.write_parquet(tweets_that_have_replies_path)\n\n# Save output of tweets that are replies to other tweets\ntweets_that_are_replies_path = folder_path / \"tweets_that_are_replies.parquet\"\np_df_replies_only.write_parquet(tweets_that_are_replies_path)\n\n# Filter the tweets that have replies to ones that aren't replies to others.\n# Also filter for only english for now.\n# This gives the root tweets that have replies but are the start of a conversation.\norigin_tweets = p_join.filter((pl.col(\"in_reply_to_status_id\").is_null()) & (pl.col(\"lang\") == \"en\"))\n\n\n# Helper functions and classes below for the next steps\n\n\ndef role_decide(user_id, prompt_user):\n    if user_id == prompt_user:\n        return \"prompter\"\n    else:\n        return \"assistant\"\n\n\nclass ConversationTreeNode:\n    def __init__(self, tweet_id, prompt_user, from_df, children_df, metadata=None):\n        if metadata:\n            self.metadata = metadata\n        else:\n            self.metadata = from_df.filter(pl.col(\"id\") == tweet_id).to_dicts()[0]\n\n        self.metadata[\"prompt_user\"] = prompt_user\n        self.role = role_decide(self.metadata[\"user_id\"], prompt_user)\n        self.children = None\n        self.text = self.metadata[\"text\"]\n        del self.metadata[\"text\"]\n        self.get_children(tweet_id=tweet_id, children_df=children_df)\n\n    def get_children(self, tweet_id, children_df):\n        children_dicts = children_df.filter(pl.col(\"in_reply_to_status_id\") == tweet_id).to_dicts()\n        if len(children_dicts) > 0:\n            children = [\n                ConversationTreeNode(\n                    tweet_id=c[\"id\"],\n                    prompt_user=self.metadata[\"prompt_user\"],\n                    from_df=children_df,\n                    children_df=children_df,\n                    metadata=c,\n                )\n                for c in children_dicts\n            ]\n            self.children = children\n\n\nclass ConversationTree:\n    def __init__(self, tweet_id, prompt_user, from_df, children_df, r_metadata=None):\n        self.root = ConversationTreeNode(\n            tweet_id=tweet_id, prompt_user=prompt_user, from_df=from_df, children_df=children_df, metadata=r_metadata\n        )\n        self.metadata = None\n\n\n# Create conversation trees\nconv_tree_list = [\n    ConversationTree(\n        tweet_id=r[\"id\"], prompt_user=r[\"user_id\"], from_df=origin_tweets, children_df=p_df_replies_only, r_metadata=r\n    )\n    for r in tqdm(origin_tweets.to_dicts())\n]\n\n# Write conversation trees to jsonl file.\n# Might need to clean up the last newline.\nwith open(output_path, \"w\") as output:\n    for t in tqdm(conv_tree_list):\n        json.dump(obj=t, fp=output, default=lambda x: x.__dict__)\n        output.write(\"\\n\")\n", "scripts/xor-codec/xor_codec.py": "import gzip\nimport os\nimport shutil\nimport sys\nfrom pathlib import Path\n\nimport numpy\n\n\ndef xor_uncompressed(dst, src_payload, src_base, block_size=4096):\n    fp_payload = open(src_payload, \"rb\")\n    fp_base = open(src_base, \"rb\")\n    with open(dst, \"wb\") as fp:\n        while True:\n            buf1 = numpy.array(bytearray(fp_payload.read(block_size)), dtype=numpy.uint8)\n            buf2 = numpy.array(bytearray(fp_base.read(block_size)), dtype=numpy.uint8)\n            padding = len(buf1) - len(buf2)\n            if padding > 0:\n                buf2 = numpy.pad(buf2, (0, padding), \"constant\", constant_values=(0,))\n            if padding < 0:\n                buf2 = buf2[: len(buf1)]\n            buf = numpy.bitwise_xor(buf1, buf2)\n            fp.write(buf)\n            if len(buf1) < block_size:\n                break\n    fp_payload.close()\n    fp_base.close()\n\n\ndef xor_encode(dst, src_payload, src_base, block_size=4096):\n    fp_payload = open(src_payload, \"rb\")\n    fp_base = open(src_base, \"rb\")\n    with gzip.open(dst, \"wb\") as fp:\n        while True:\n            buf1 = numpy.array(bytearray(fp_payload.read(block_size)), dtype=numpy.uint8)\n            buf2 = numpy.array(bytearray(fp_base.read(block_size)), dtype=numpy.uint8)\n            padding = len(buf1) - len(buf2)\n            if padding > 0:\n                buf2 = numpy.pad(buf2, (0, padding), \"constant\", constant_values=(0,))\n            if padding < 0:\n                buf2 = buf2[: len(buf1)]\n            buf = numpy.bitwise_xor(buf1, buf2)\n            fp.write(buf)\n            if len(buf1) < block_size:\n                break\n    fp_payload.close()\n    fp_base.close()\n\n\ndef xor_decode(dst, src_payload, src_base, block_size=4096):\n    fp_payload = gzip.open(src_payload, \"rb\")\n    fp_base = open(src_base, \"rb\")\n    with open(dst, \"wb\") as fp:\n        while True:\n            buf1 = numpy.array(bytearray(fp_payload.read(block_size)), dtype=numpy.uint8)\n            buf2 = numpy.array(bytearray(fp_base.read(block_size)), dtype=numpy.uint8)\n            padding = len(buf1) - len(buf2)\n            if padding > 0:\n                buf2 = numpy.pad(buf2, (0, padding), \"constant\", constant_values=(0,))\n            if padding < 0:\n                buf2 = buf2[: len(buf1)]\n            buf = numpy.bitwise_xor(buf1, buf2)\n            fp.write(buf)\n            if len(buf1) < block_size:\n                break\n    fp_payload.close()\n    fp_base.close()\n\n\ndef xor_dir(dst, src_payload, src_base, decode=True, compress=True):\n    if compress:\n        xor = xor_decode if decode else xor_encode\n    else:\n        xor = xor_uncompressed\n    Path(dst).mkdir(parents=True, exist_ok=True)\n    shutil.copy(Path(src_payload) / \"added_tokens.json\", Path(dst) / \"added_tokens.json\")\n    for path in os.listdir(src_payload):\n        print(\"[*] Processing '%s'\" % path)\n        try:\n            xor(\"%s/%s\" % (dst, path), \"%s/%s\" % (src_payload, path), \"%s/%s\" % (src_base, path))\n        except Exception:\n            print(\"Exception when processing '%s'\" % path)\n\n\nif __name__ == \"__main__\":\n    if len(sys.argv) < 4:\n        print(\"Usage: xor.py <DESTINATION> <PAYLOAD SOURCE> <LLAMA SOURCE> [--encode] [--compress]\")\n        exit()\n    dst = sys.argv[1]\n    src_payload = sys.argv[2]\n    src_base = sys.argv[3]\n    decode = True\n    compress = False\n    if len(sys.argv) > 4:\n        for arg in sys.argv[4:]:\n            if arg == \"--encode\":\n                decode = False\n            if arg == \"--compress\":\n                compress = True\n    xor_dir(dst, src_payload, src_base, decode=decode, compress=compress)\n", "scripts/data_augment/data_augment.py": "\"\"\"Script for a variety of data augmentation techniques for generating Question answer pairs.\nDepending on the class used it takes in the input files and generates summaries from essays (which then will result in a \"write a story about [summary]\"-> essay pair),#\nbuggs code (in order to have bugged code + \"please fix\" -> code), ...\nexample usage:\n  data_augment.py --dataset essays.tsv --augmenter hierarchicalsummarizer --output out.json\nargs:\n  -- dataset: TSV file referencing txt files with essays/code\n  -- augmenter: the augmenter used: one of 'essayinstruction', 'essayrevision', 'stackexchange', 'hierarchicalsummarizer', 'entityrecognizedsummarizer', 'codebugger\"\n  -- output: where to save the output\n\"\"\"\n\n\nimport argparse\nimport json\nimport random\nimport string\nfrom collections import Counter\n\nimport nltk\nimport pandas as pd\nimport requests\nimport spacy\nimport torch\nfrom bs4 import BeautifulSoup as bs\nfrom logic.logic_injector import LogicBug\nfrom nltk.corpus import wordnet\nfrom syntax.syntax_injector import SyntaxBug\nfrom transformers import AutoModelForSeq2SeqLM, AutoTokenizer, T5ForConditionalGeneration, pipeline\n\n\nclass DataAugmenter:\n    def __init__(self):\n        raise NotImplementedError()\n\n    def parse(self, essays):\n        prompts = []\n        preds = []\n\n        for essay in essays:\n            essay_prompts, essay_preds = self.parse_single(essay)\n\n            prompts += essay_prompts\n            preds += essay_preds\n\n        return prompts, preds\n\n    def parse_single(self, essay):\n        pass\n\n\nclass EssayInstructor(DataAugmenter):\n    def __init__(self, model_name=None):\n        if model_name is None:\n            model_name = \"snrspeaks/t5-one-line-summary\"\n        self.model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n\n    def parse_single(self, essay):\n        essay_paragraphs = essay.split(\"\\n\\n\")\n        preds = []\n\n        for para in essay_paragraphs:\n            input_ids = self.tokenizer.encode(para, return_tensors=\"pt\", add_special_tokens=True)\n            generated_ids = self.model.generate(\n                input_ids=input_ids,\n                num_beams=5,\n                max_length=35,\n                repetition_penalty=4.5,\n                length_penalty=1.5,\n                early_stopping=True,\n                num_return_sequences=1,\n            )\n            preds.append(\n                self.tokenizer.decode(generated_ids[0], skip_special_tokens=True, clean_up_tokenization_spaces=True)\n            )\n\n        prompts = (\n            [\"Write an intro paragraph to an essay called\"]\n            + [\"Write a paragraph to an essay about\"] * len(preds[1:-1])\n            + [\"Write a concluding paragraph about\"]\n        )\n\n        assert len(preds) == len(prompts)\n        prompts = [prompt + \" \" + pred for prompt, pred in zip(prompts, preds)]\n\n        return prompts, essay_paragraphs\n\n\nclass EssayReviser(DataAugmenter):\n    def __init__(self):\n        nltk.download(\"wordnet\")\n        nltk.download(\"omw-1.4\")\n\n    def parse_single(self, essay):\n        instructions = []\n\n        # Make structure error (shuffle one paragraph with another)\n        essay_paragraphs = essay.split(\"\\n\\n\")  # Splitting a String by newline character (\\n)\n\n        rand1 = random.randint(0, len(essay_paragraphs) - 1)\n        rand2 = random.randint(0, len(essay_paragraphs) - 1)\n\n        temp = essay_paragraphs[rand1]\n        essay_paragraphs[rand1] = essay_paragraphs[rand2]\n        essay_paragraphs[rand2] = temp\n\n        corrupted_essay = \"\\n\\n\".join(essay_paragraphs)\n\n        instructions.append(\"Fix structure errors in this essay\" + corrupted_essay)\n\n        essay_words = essay.split()\n        for i in range(len(essay_words)):\n            if random.randint(0, 100) < 30:\n                suggestion = []\n                for syn in wordnet.synsets(essay_words[i]):\n                    for l in syn.lemmas():\n                        suggestion.append(l.name())\n                if suggestion != []:\n                    essay_words[i] = suggestion[random.randint(0, len(suggestion) - 1)]\n\n        corrupted_essay = \" \".join(essay_words)\n\n        instructions.append(\"Fix grammar errors in this essay: \" + corrupted_essay)\n\n        # you can change the number 60 to change how much corrupted this essay will be\n        for _ in range(len(essay) // 60):\n            rand = random.randint(0, len(essay))\n            corrupted_essay = essay[:rand] + random.choice(string.ascii_letters) + essay[rand + 1 :]\n\n        instructions.append(\"Fix typing errors in this essay\" + corrupted_essay)\n\n        return instructions, [essay] * len(instructions)\n\n\nclass StackExchangeBuilder(DataAugmenter):\n    def __init__(self, base_url=None, filter_opts=None):\n        self.base_url = (\n            base_url\n            if base_url is not None\n            else \"https://ia600107.us.archive.org/view_archive.php?archive=/27/items/stackexchange/{0}&file=Posts.xml\"\n        )\n        self.filter_opts = (\n            filter_opts if filter_opts is not None else [\"accepted\", \"score\", \"convert_html\", \"clean_tags\"]\n        )\n\n    def get_all_filenames(self):\n        response = requests.get(\"https://archive.org/download/stackexchange\")\n        if response.ok:\n            soup = bs(response.content, \"html.parser\")\n            table = soup.find(\"table\")\n            link_tags = table.find_all(\"a\")\n            urls = {}\n            for link in link_tags:\n                url = link[\"href\"]\n                name = url.split(\".stackexchange\")[0].replace(\".\", \"_\").replace(\"-\", \"_\")\n                if url.endswith(\"7z\"):\n                    urls[name] = self.base_url.format(url)\n            return urls\n\n    def xml_to_df(self, response: str):\n        \"\"\"\n        Collect and Manually import XML into Dataframe\n\n        pd.read_xml() errors when XML trees are too large, this is just a hack to\n        download a XML file and parse into a Dataframe. **Not Tested on huge XML files**\n\n        Parameters:\n        response (Requests.Response): Requests response object with the XML data\n\n        Returns:\n        df (DataFrame): A Dataframe from the XML file\n        \"\"\"\n        xml_format_map = {\n            \"Id\": int,\n            \"PostTypeId\": int,\n            \"CreationDate\": str,\n            \"Score\": int,\n            \"ViewCount\": int,\n            \"Body\": str,\n            \"AnswerCount\": int,\n            \"CommentCount\": int,\n            \"ContentLicense\": str,\n            \"AcceptedAnswerId\": int,\n            \"ParentId\": int,\n        }\n        soup = bs(response.content, \"xml\")\n        posts = soup.find_all(\"row\")\n\n        all_posts = [post.attrs for post in posts]\n\n        df = pd.DataFrame(all_posts)\n        df.AnswerCount.fillna(0, inplace=True)\n        df.ViewCount.fillna(0, inplace=True)\n        df.AcceptedAnswerId.fillna(0, inplace=True)\n        df.ParentId.fillna(0, inplace=True)\n        df[\"DataSource\"] = response.url\n        df = df.astype(xml_format_map)\n        return df\n\n    def filter(self, df):\n        if \"accepted\" in self.filter_opts:\n            \"\"\"**TODO**\n            Filter only to Questions with Accepted Answers\n\n            Filter dataframe by questions that have accepted answers, should also include\n            all rows of answers for those questions, even if not accepted.\"\"\"\n\n            df = df[(df[\"AcceptedAnswerId\"].notnull()) | (df[\"ParentId\"] == df[\"Id\"])]\n\n        if \"score\" in self.filter_opts:\n            \"\"\"**TODO**\n            Filter Dataframe by minimum scores\n\n            Filter Question and Answer columns by score thresholds to trim lower scoring results\"\"\"\n            question_score_threshold = 0\n            answer_score_threshold = 5\n            df = df[\n                ((df[\"Score\"] >= question_score_threshold) & (df.PostTypeId == 1))\n                | ((df[\"Score\"] >= answer_score_threshold) & (df.PostTypeId == 2))\n            ]\n\n        if \"clean_tags\" in self.filter_opts:\n            \"\"\"\n            Convert Tags into Comma separated\n            Converts Tag slugs into commas separated tags\"\"\"\n            df[\"TagsClean\"] = (\n                df[\"Tags\"].str.replace(\"-\", \" \").str.replace(\"><\", \", \").str.replace(\"<\", \"\").str.replace(\">\", \"\")\n            )\n\n        if \"convert_html\" in self.filter_opts:\n            \"\"\"\n            Convert HTML tags to pure text\n\n            Feeds HTML text body into BeautifulSoup to parse it to only text. Set aside as\n            function to provide option to skip\"\"\"\n            column = \"Body\"\n            df.dropna(subset=[column], inplace=True)\n            df[f\"{column}Clean\"] = df[column].apply(lambda row: bs(row, \"html.parser\").text)\n\n        return df\n\n    def parse(self, _):\n        urls = self.get_all_filenames()\n        dataset_name = \"ai\"\n\n        xml_posts_path = urls.get(dataset_name)\n\n        response = requests.get(xml_posts_path)\n        df = self.xml_to_df(response)\n        df = self.filter(df)\n\n        questions = df[df.PostTypeId == 1]\n        answers = df[df.PostTypeId == 2]\n\n        df = pd.merge(\n            questions,\n            answers,\n            left_on=\"Id\",\n            right_on=\"ParentId\",\n            suffixes=(\"_q\", \"_a\"),\n            how=\"left\",\n        )\n        questions = df[[\"Title_q\", \"BodyClean_q\"]]\n        # prepend title to question and make questions to list\n        questions = questions.apply(lambda x: x[\"Title_q\"] + \"\\n\" + x[\"BodyClean_q\"], axis=1)\n        questions = questions.tolist()\n\n        answers = df[[\"BodyClean_a\"]]\n        answers = answers.tolist()\n\n        return questions, answers\n\n\nclass HierachicalSummarizer(DataAugmenter):\n    def __init__(self):\n        self.summarizer = pipeline(\n            \"summarization\",\n            \"pszemraj/long-t5-tglobal-base-16384-book-summary\",\n            device=0 if torch.cuda.is_available() else -1,\n        )\n\n        self.params = {\n            \"max_length\": 1024,\n            \"min_length\": 8,\n            \"no_repeat_ngram_size\": 3,\n            \"early_stopping\": False,\n            \"repetition_penalty\": 3.5,\n            \"length_penalty\": 0.3,\n            \"encoder_no_repeat_ngram_size\": 3,\n            \"num_beams\": 4,\n        }  # parameters for text generation out of model\n\n        self.nlp = spacy.load(\"en_core_web_sm\")\n\n    def cleanup_summary(self, out):\n        (\n            out.replace(\"The novel begins with the description of\", \"\")\n            .replace(\"the description of\", \"\")\n            .replace(\"The novel begins\", \"\")\n            .replace(\"This chapter introduces us to\", \"\")\n            .replace(\"In this chapter, \", \"\")\n            .replace(\"This chapter\", \"\")\n            .strip(\" ,\")\n        )\n        return out\n\n    def parse_single(self, essay):\n        final_summary = \"\"\n        new_summary = \"\"\n        level_2_summary = []\n        level_1_summary = []\n        entities = []\n        essay_parts = essay.split(\"##\")\n        for section_text in essay_parts:\n            result = self.summarizer(section_text, **self.params)\n            out = self.cleanup_summary(result[0][\"summary_text\"])\n            level_2_summary.append(out)\n            result = self.summarizer(out, **self.params)\n            out = self.cleanup_summary(result[0][\"summary_text\"])\n            new_summary += \"\\n\" + out\n            level_1_summary.append(out)\n\n            entity = recognize_entities(section_text, self.nlp, n=5, person=\"ignore\")\n            entities.append(entity)\n\n        result = self.summarizer(new_summary, **self.params)\n        final_summary = self.cleanup_summary(result[0][\"summary_text\"])\n\n        first_instruction = \"Write a story about the following:\\n\" + final_summary\n        first_answer = \"\\n\".join(level_1_summary)\n        instructions = [first_instruction]\n        answers = [first_answer]\n\n        for entity, answer in zip(entities, level_2_summary):\n            instructions.append(f\"Now expand on {entity}!\")\n            answers.append(answer)\n\n        for entity, answer in zip(entities, level_1_summary):\n            instructions.append(f\"Further expand on {entity}.\")\n            answers.append(answer)\n\n        return instructions, answers\n\n\nclass EntityRecognizedSummarizer(DataAugmenter):\n    def __init__(self):\n        self.nlp = spacy.load(\"en_core_web_sm\")  # run !python -m spacy download en_core_web_sm in order to download\n\n    def parse_single(self, essay):\n        ents = recognize_entities(essay, self.nlp)\n        characters = ents.most_common(4, person=True)\n        topic = recognize_entities(essay, self.nlp, n=2, person=False)\n\n        question = f\"Please write a story titled {topic} with the characters {characters}.\"\n        answer = f\"Sure. Here is a story titled {topic}\\n\" + essay\n\n        return [question], [answer]\n\n\nclass CodeBugger(DataAugmenter):\n    \"\"\"\n    https://github.com/LAION-AI/Open-Assistant/blob/main/notebooks/code-bugger/openbugger_example.md\n    Openbugger is a Python package that allows you to inject syntax and logic errors into your code.\n    This can be useful for testing the robustness of your code or for creating test cases for debugging exercises or for training an assistant to debug code.\n    To install:\n            cwd = os.getcwd()\n\n        # Next, we'll use Git to clone the repository.\n        subprocess.run([\"git\", \"clone\", \"https://github.com/furlat/OpenBugger\", cwd + \"/OpenBugger\"])\n\n        # Now, we'll use pip to install the package from the local repository.\n        subprocess.run([\"python3\", \"-m\", \"pip\", \"install\", \"--editable\", cwd + \"/OpenBugger\"])\n    \"\"\"\n\n    def __init__(self):\n        self.syntax_bug = SyntaxBug()\n        self.logic_bug = LogicBug()\n\n    def parse_single(self, code):\n        code = self.syntax_bug(code, \"medium\", num_errors=2)\n        code = self.logic_bug(code, \"medium\", num_errors=2)\n\n        question = \"Can you fix the following code?\\n\" + code\n\n        answer = (\n            \"The following code is correct:\\n\"\n            + code\n            + \"\\nI hope I could help you fixing your code. In case you need more help, feel free to ask me again.\"\n        )\n\n        return [question], [answer]\n\n\nclass CodeInstructor(DataAugmenter):\n    def __init__(self):\n        self.tokenizer = AutoTokenizer.from_pretrained(\"Graverman/t5-code-summary\")\n        self.model = T5ForConditionalGeneration.from_pretrained(\"Graverman/t5-code-summary\")\n\n    def parse(self, codes):\n        source_encoding = self.tokenizer(\n            codes,\n            max_length=300,\n            padding=\"max_length\",\n            truncation=True,\n            return_attention_mask=True,\n            add_special_tokens=True,\n            return_tensors=\"pt\",\n        )\n        outputs = self.model.generate(\n            input_ids=source_encoding[\"input_ids\"],\n            attention_mask=source_encoding[\"attention_mask\"],\n            max_length=100,\n            length_penalty=0.75,\n            repetition_penalty=2.5,\n            early_stopping=True,\n            use_cache=True,\n        )\n        summaries = [self.tokenizer.decode(o, skip_special_tokens=True) for o in outputs]\n\n        questions = [\"Write a script that does the following:\\n\" + s for s in summaries]\n        answers = codes\n\n        return questions, answers\n\n\ndef recognize_entities(text, model, n=4, person=\"ignore\"):\n    \"\"\"Given a text and a model for entity recognition, return the most occurring entities in the text as a string\"\"\"\n    doc = model(text)\n    if person == \"ignore\":\n        ents = Counter([ent.text.strip() for ent in list(doc.ents) if len(ent.text.strip()) >= 5])\n    elif person:\n        ents = Counter(\n            [ent.text.strip() for ent in list(doc.ents) if ent.label_ == \"PERSON\" and len(ent.text.strip()) >= 5]\n        )\n    else:\n        ents = Counter(\n            [ent.text.strip() for ent in list(doc.ents) if ent.label_ != \"PERSON\" and len(ent.text.strip()) >= 5]\n        )\n    ents = ents.most_common(n)\n    ents = \", \".join([a[0] for a in ents])\n\n    return ents\n\n\ndef parse_arguments():\n    args = argparse.ArgumentParser()\n    args.add_argument(\"--dataset\", type=str, required=True)\n    args.add_argument(\"--augmenter\", type=str, required=True)\n    args.add_argument(\"--output\", type=str, required=True)\n    args = args.parse_args()\n\n    assert args.dataset.endswith(\".tsv\") or args.dataset.endswith(\n        \".csv\"\n    ), \"Dataset file must be a tsv or csv file, containing a list of files to be augmented\"\n    assert args.output.endswith(\".json\"), \"Output file must be a json file\"\n\n    return args\n\n\ndef read_data(args):\n    files = pd.read_csv(args.dataset, sep=\",\", header=None, names=[\"file\"])\n    files = files[\"file\"].tolist()\n    data = []\n    for file in files:\n        with open(file, \"r\") as f:\n            text = f.read()\n            data.append(text)\n\n    return data\n\n\ndef get_augmenter(args):\n    if args.augmenter == \"essayinstruction\":\n        augmenter = EssayInstructor()\n\n    elif args.augmenter == \"essayrevision\":\n        augmenter = EssayReviser()\n\n    elif args.augmenter == \"stackexchange\":\n        augmenter = StackExchangeBuilder()\n\n    elif args.augmenter == \"hierarchicalsummarizer\":\n        augmenter = HierachicalSummarizer()\n\n    elif args.augmenter == \"entityrecognizedsummarizer\":\n        augmenter = EntityRecognizedSummarizer()\n\n    elif args.augmenter == \"codebugger\":\n        augmenter = CodeBugger()\n\n    elif args.augmenter == \"codeinstructor\":\n        augmenter = CodeInstructor()\n\n    else:\n        raise ValueError(\n            \"Augmenter must be one of 'essayinstruction', 'essayrevision', 'stackexchange', 'hierarchicalsummarizer', 'entityrecognizedsummarizer', 'codebugger', 'codeinstructor\"\n        )\n\n    return augmenter\n\n\ndef main(args):\n    data = read_data(args)\n    augmenter = get_augmenter(args)\n\n    augmented_data = augmenter.parse(data)\n\n    # write augmented data as json file\n    with open(args.output, \"w\") as f:\n        json.dump(augmented_data, f)\n\n\nif __name__ == \"__main__\":\n    args = parse_arguments()\n    main(args)\n", "scripts/postprocessing/rankings.py": "from typing import List\n\nimport numpy as np\n\n\ndef head_to_head_votes(ranks: List[List[int]]):\n    tallies = np.zeros((len(ranks[0]), len(ranks[0])))\n    names = sorted(ranks[0])\n    ranks = np.array(ranks)\n    # we want the sorted indices\n    ranks = np.argsort(ranks, axis=1)\n    for i in range(ranks.shape[1]):\n        for j in range(i + 1, ranks.shape[1]):\n            # now count the cases someone voted for i over j\n            over_j = np.sum(ranks[:, i] < ranks[:, j])\n            over_i = np.sum(ranks[:, j] < ranks[:, i])\n            tallies[i, j] = over_j\n            # tallies[i,j] = over_i\n            tallies[j, i] = over_i\n            # tallies[j,i] = over_j\n    return tallies, names\n\n\ndef cycle_detect(pairs):\n    \"\"\"Recursively detect cycles by removing condorcet losers until either only one pair is left or condorcet losers no longer exist\n    This method upholds the invariant that in a ranking for all a,b either a>b or b>a for all a,b.\n\n\n    Returns\n    -------\n    out : False if the pairs do not contain a cycle, True if the pairs contain a cycle\n\n\n    \"\"\"\n    # get all condorcet losers (pairs that loose to all other pairs)\n    # idea: filter all losers that are never winners\n    # print(\"pairs\", pairs)\n    if len(pairs) <= 1:\n        return False\n    losers = [c_lose for c_lose in np.unique(pairs[:, 1]) if c_lose not in pairs[:, 0]]\n    if len(losers) == 0:\n        # if we recursively removed pairs, and at some point we did not have\n        # a condorcet loser, that means everything is both a winner and loser,\n        # yielding at least one (winner,loser), (loser,winner) pair\n        return True\n\n    new = []\n    for p in pairs:\n        if p[1] not in losers:\n            new.append(p)\n    return cycle_detect(np.array(new))\n\n\ndef get_winner(pairs):\n    \"\"\"\n    This returns _one_ concordant winner.\n    It could be that there are multiple concordant winners, but in our case\n    since we are interested in a ranking, we have to choose one at random.\n    \"\"\"\n    losers = np.unique(pairs[:, 1]).astype(int)\n    winners = np.unique(pairs[:, 0]).astype(int)\n    for w in winners:\n        if w not in losers:\n            return w\n\n\ndef get_ranking(pairs):\n    \"\"\"\n    Abuses concordance property to get a (not necessarily unique) ranking.\n    The lack of uniqueness is due to the potential existence of multiple\n    equally ranked winners. We have to pick one, which is where\n    the non-uniqueness comes from\n    \"\"\"\n    if len(pairs) == 1:\n        return list(pairs[0])\n    w = get_winner(pairs)\n    # now remove the winner from the list of pairs\n    p_new = np.array([(a, b) for a, b in pairs if a != w])\n    return [w] + get_ranking(p_new)\n\n\ndef ranked_pairs(ranks: List[List[int]]):\n    \"\"\"\n    Expects a list of rankings for an item like:\n        [(\"w\",\"x\",\"z\",\"y\") for _ in range(3)]\n        + [(\"w\",\"y\",\"x\",\"z\") for _ in range(2)]\n        + [(\"x\",\"y\",\"z\",\"w\") for _ in range(4)]\n        + [(\"x\",\"z\",\"w\",\"y\") for _ in range(5)]\n        + [(\"y\",\"w\",\"x\",\"z\") for _ in range(1)]\n    This code is quite brain melting, but the idea is the following:\n    1. create a head-to-head matrix that tallies up all win-lose combinations of preferences\n    2. take all combinations that win more than they loose and sort those by how often they win\n    3. use that to create an (implicit) directed graph\n    4. recursively extract nodes from the graph that do not have incoming edges\n    5. said recursive list is the ranking\n    \"\"\"\n    tallies, names = head_to_head_votes(ranks)\n    tallies = tallies - tallies.T\n    # note: the resulting tally matrix should be skew-symmetric\n    # order by strength of victory (using tideman's original method, don't think it would make a difference for us)\n    sorted_majorities = []\n    for i in range(len(ranks[0])):\n        for j in range(len(ranks[0])):\n            # you can never prefer yourself over yourself\n            # we also have to pick one of the two choices,\n            # if the preference is exactly zero...\n            if tallies[i, j] >= 0 and i != j:\n                sorted_majorities.append((i, j, tallies[i, j]))\n    # we don't explicitly deal with tied majorities here\n    sorted_majorities = np.array(sorted(sorted_majorities, key=lambda x: x[2], reverse=True))\n    # now do lock ins\n    lock_ins = []\n    for x, y, _ in sorted_majorities:\n        # invariant: lock_ins has no cycles here\n        lock_ins.append((x, y))\n        # print(\"lock ins are now\",np.array(lock_ins))\n        if cycle_detect(np.array(lock_ins)):\n            # print(\"backup: cycle detected\")\n            # if there's a cycle, delete the new addition and continue\n            lock_ins = lock_ins[:-1]\n    # now simply return all winners in order, and attach the losers\n    # to the back. This is because the overall loser might not be unique\n    # and (by concordance property) may never exist in any winning set to begin with.\n    # (otherwise he would either not be the loser, or cycles exist!)\n    # Since there could be multiple overall losers, we just return them in any order\n    # as we are unable to find a closer ranking\n    numerical_ranks = np.array(get_ranking(np.array(lock_ins))).astype(int)\n    conversion = [names[n] for n in numerical_ranks]\n    return conversion\n\n\nif __name__ == \"__main__\":\n    ranks = \"\"\" (\n        [(\"w\", \"x\", \"z\", \"y\") for _ in range(1)]\n        + [(\"w\", \"y\", \"x\", \"z\") for _ in range(2)]\n        # + [(\"x\",\"y\",\"z\",\"w\") for _ in range(4)]\n        + [(\"x\", \"z\", \"w\", \"y\") for _ in range(5)]\n        + [(\"y\", \"w\", \"x\", \"z\") for _ in range(1)]\n        # [(\"y\",\"z\",\"w\",\"x\") for _ in range(1000)]\n    )\"\"\"\n    ranks = [\n        [\n            (\"c5181083-d3e9-41e7-a935-83fb9fa01488\"),\n            (\"dcf3d179-0f34-4c15-ae21-b8feb15e422d\"),\n            (\"d11705af-5575-43e5-b22e-08d155fbaa62\"),\n        ],\n        [\n            (\"d11705af-5575-43e5-b22e-08d155fbaa62\"),\n            (\"c5181083-d3e9-41e7-a935-83fb9fa01488\"),\n            (\"dcf3d179-0f34-4c15-ae21-b8feb15e422d\"),\n        ],\n        [\n            (\"dcf3d179-0f34-4c15-ae21-b8feb15e422d\"),\n            (\"c5181083-d3e9-41e7-a935-83fb9fa01488\"),\n            (\"d11705af-5575-43e5-b22e-08d155fbaa62\"),\n        ],\n        [\n            (\"d11705af-5575-43e5-b22e-08d155fbaa62\"),\n            (\"c5181083-d3e9-41e7-a935-83fb9fa01488\"),\n            (\"dcf3d179-0f34-4c15-ae21-b8feb15e422d\"),\n        ],\n    ]\n    rp = ranked_pairs(ranks)\n    print(rp)\n", "scripts/postprocessing/scoring.py": "from dataclasses import dataclass, replace\nfrom typing import Any\n\nimport numpy as np\nimport numpy.typing as npt\nfrom scipy.stats import kendalltau\n\n\n@dataclass\nclass Voter:\n    \"\"\"\n    Represents a single voter.\n    This tabulates the number of good votes, total votes,\n    and points.\n    We only put well-behaved people on the scoreboard and filter out the badly behaved ones\n    \"\"\"\n\n    uid: Any\n    num_votes: int\n    num_good_votes: int\n    num_prompts: int\n    num_good_prompts: int\n    num_rankings: int\n    num_good_rankings: int\n\n    #####################\n    voting_points: int\n    prompt_points: int\n    ranking_points: int\n\n    def voter_quality(self):\n        return self.num_good_votes / self.num_votes\n\n    def rank_quality(self):\n        return self.num_good_rankings / self.num_rankings\n\n    def prompt_quality(self):\n        return self.num_good_prompts / self.num_prompts\n\n    def is_well_behaved(self, threshhold_vote, threshhold_prompt, threshhold_rank):\n        return (\n            self.voter_quality() > threshhold_vote\n            and self.prompt_quality() > threshhold_prompt\n            and self.rank_quality() > threshhold_rank\n        )\n\n    def total_points(self, voting_weight, prompt_weight, ranking_weight):\n        return (\n            voting_weight * self.voting_points\n            + prompt_weight * self.prompt_points\n            + ranking_weight * self.ranking_points\n        )\n\n\ndef score_update_votes(new_vote: int, consensus: npt.ArrayLike, voter_data: Voter) -> Voter:\n    \"\"\"\n    This function returns the new \"quality score\" and points for a voter,\n    after that voter cast a vote on a question.\n\n    This function is only to be run when archiving a question\n    i.e. the question has had sufficiently many votes, or we can't get more than \"K\" bits of information\n\n    The consensus is the array of all votes cast by all voters for that question\n    We then update the voter data using the new information\n\n        Parameters:\n            new_vote (int): the index of the vote cast by the voter\n            consensus (ArrayLike): all votes cast for this question\n            voter_data (Voter): a \"Voter\" object that represents the person casting the \"new_vote\"\n\n        Returns:\n            updated_voter (Voter): the new \"quality score\" and points for the voter\n    \"\"\"\n    # produces the ranking of votes, e.g. for [100,300,200] it returns [0, 2, 1],\n    # since 100 is the lowest, 300 the highest and 200 the middle value\n    consensus_ranking = np.argsort(np.argsort(consensus))\n    new_points = consensus_ranking[new_vote] + voter_data.voting_points\n\n    # we need to correct for 0 indexing, if you are closer to \"right\" than \"wrong\" of the consensus,\n    # it's a good vote\n    new_good_votes = int(consensus_ranking[new_vote] > (len(consensus) - 1) / 2) + voter_data.num_good_votes\n    new_num_votes = voter_data.num_votes + 1\n    return replace(voter_data, num_votes=new_num_votes, num_good_votes=new_good_votes, voting_points=new_points)\n\n\ndef score_update_prompts(consensus: npt.ArrayLike, voter_data: Voter) -> Voter:\n    \"\"\"\n    This function returns the gain of points for a given prompt's votes\n\n    In contrast to the other score updating functions, we can run this online as new votes come in.\n    i.e. the question has had sufficiently many votes, or we can't get more than \"K\" bits of information.\n\n\n    Parameters:\n            consensus (ArrayLike): all votes cast for this question\n            voter_data (Voter): a \"Voter\" object that represents the person that wrote the prompt\n\n        Returns:\n            updated_voter (Voter): the new \"quality score\" and points for the voter\n    \"\"\"\n    # produces the ranking of votes, e.g. for [100,300,200] it returns [0, 2, 1],\n    # since 100 is the lowest, 300 the highest and 200 the middle value\n    consensus_ranking = np.arange(len(consensus)) - len(consensus) // 2 + 1\n    # expected consensus ranking (i.e. normalize the votes and multiply-sum with weightings)\n    delta_votes = np.sum(consensus_ranking * consensus / sum(consensus))\n    new_points = delta_votes + voter_data.prompt_points\n\n    # we need to correct for 0 indexing, if you are closer to \"right\" than \"wrong\" of the consensus,\n    # it's a good vote\n    new_good_prompts = int(delta_votes > 0) + voter_data.num_good_prompts\n    new_num_prompts = voter_data.num_prompts + 1\n    return replace(\n        voter_data,\n        num_prompts=new_num_prompts,\n        num_good_prompts=new_good_prompts,\n        prompt_points=new_points,\n    )\n\n\ndef score_update_ranking(user_ranking: npt.ArrayLike, consensus_ranking: npt.ArrayLike, voter_data: Voter) -> Voter:\n    \"\"\"\n    This function returns the gain of points for a given ranking's votes\n\n    This function is only to be run when archiving a question\n    i.e. the question has had sufficiently many votes, or we can't get more than \"K\" bits of information\n\n    we use the bubble-sort distance (or \"kendall-tau\" distance) to compare the two rankings\n    we use this over spearman correlation since:\n        \"[Kendall's \u03c4] approaches a normal distribution more rapidly than \u03c1, as N, the sample size, increases;\n            and \u03c4 is also more tractable mathematically, particularly when ties are present\"\n    Gilpin, A. R. (1993). Table for conversion of Kendall's Tau to Spearman's\n     Rho within the context measures of magnitude of effect for meta-analysis\n\n    Further in\n        \"research design and statistical analyses, second edition, 2003\"\n    the authors note that at least from an significance test POV they will yield the same p-values\n\n        Parameters:\n            user_ranking (ArrayLike): ranking produced by the user\n            consensus (ArrayLike): ranking produced after running the voting algorithm to merge into the consensus ranking\n            voter_data (Voter): a \"Voter\" object that represents the person that wrote the prompt\n\n        Returns:\n            updated_voter (Voter): the new \"quality score\" and points for the voter\n    \"\"\"\n    bubble_sort_distance, p_value = kendalltau(user_ranking, consensus_ranking)\n    # normalize kendall-tau from [-1,1] into [0,1] range\n    bubble_sort_distance = (1 + bubble_sort_distance) / 2\n    new_points = bubble_sort_distance + voter_data.ranking_points\n    new_good_rankings = int(bubble_sort_distance > 0.5) + voter_data.num_good_rankings\n    new_num_rankings = voter_data.num_rankings + 1\n    return replace(\n        voter_data,\n        num_rankings=new_num_rankings,\n        num_good_rankings=new_good_rankings,\n        ranking_points=new_points,\n    )\n\n\nif __name__ == \"__main__\":\n    demo_voter = Voter(\n        \"abc\",\n        num_votes=10,\n        num_good_votes=2,\n        num_prompts=10,\n        num_good_prompts=2,\n        num_rankings=10,\n        num_good_rankings=2,\n        voting_points=6,\n        prompt_points=0,\n        ranking_points=0,\n    )\n    new_vote = 3\n    consensus = np.array([200, 300, 100, 500])\n    print(demo_voter)\n    print(\"best   vote  \", score_update_votes(new_vote, consensus, demo_voter))\n    new_vote = 2\n    print(\"worst  vote  \", score_update_votes(new_vote, consensus, demo_voter))\n    new_vote = 1\n    print(\"medium vote  \", score_update_votes(new_vote, consensus, demo_voter))\n    print(\"prompt writer\", score_update_prompts(consensus, demo_voter))\n    print(\"best   rank  \", score_update_ranking(np.array([0, 2, 1]), np.array([0, 2, 1]), demo_voter))\n    print(\"medium rank  \", score_update_ranking(np.array([2, 0, 1]), np.array([0, 2, 1]), demo_voter))\n    print(\"worst  rank  \", score_update_ranking(np.array([1, 0, 2]), np.array([0, 2, 1]), demo_voter))\n", "scripts/postprocessing/regex_pii_detector.py": "import re\n\n# Adapted from\n# https://www.geeksforgeeks.org/how-to-validate-ssn-social-security-number-using-regular-expression/\n# https://docs.opswat.com/mdcore/proactive-dlp/sample-regular-expressions\n# https://github.com/m4ll0k/SecretFinder/blob/master/BurpSuite-SecretFinder/SecretFinder.py\n\nregex_patterns = {\n    \"google_api\": r\"AIza[0-9A-Za-z-_]{35}\",\n    \"bitcoin_address\": r\"([13][a-km-zA-HJ-NP-Z0-9]{26,33})\",\n    \"slack_api_key\": r\"xox.-[0-9]{12}-[0-9]{12}-[0-9a-zA-Z]{24}\",\n    \"google_cloud_platform_auth\": r\"[0-9a-fA-F]{8}-[0-9a-fA-F]{4}-[0-9a-fA-F]{12}\",\n    \"google_cloud_platform_api\": r\"[A-Za-z0-9_]{21}--[A-Za-z0-9_]{8}\",\n    \"gmail_auth_token\": r\"[0-9(+-[0-9A-Za-z_]{32}.apps.qooqleusercontent.com\",\n    \"github_auth_token\": r\"[0-9a-fA-F]{40}\",\n    \"Instagram_token\": r\"[0-9a-fA-F]{7}.[0-9a-fA-F]{32}\",\n    \"google_captcha\": r\"6L[0-9A-Za-z-_]{38}|^6[0-9a-zA-Z_-]{39}$\",\n    \"google_oauth\": r\"ya29\\.[0-9A-Za-z\\-_]+\",\n    \"amazon_aws_access_key_id\": r\"A[SK]IA[0-9A-Z]{16}\",\n    \"amazon_mws_auth_toke\": r\"amzn\\\\.mws\\\\.[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}\",\n    \"amazon_aws_url\": r\"s3\\.amazonaws.com[/]+|[a-zA-Z0-9_-]*\\.s3\\.amazonaws.com\",\n    \"facebook_access_token\": r\"EAACEdEose0cBA[0-9A-Za-z]+\",\n    \"authorization_basic\": r\"basic\\s*[a-zA-Z0-9=:_\\+\\/-]+\",\n    \"authorization_bearer\": r\"bearer\\s*[a-zA-Z0-9_\\-\\.=:_\\+\\/]+\",\n    \"authorization_api\": r\"api[key|\\s*]+[a-zA-Z0-9_\\-]+\",\n    \"mailgun_api_key\": r\"key-[0-9a-zA-Z]{32}\",\n    \"paypal_braintree_access_token\": r\"access_token\\$production\\$[0-9a-z]{16}\\$[0-9a-f]{32}\",\n    \"square_oauth_secret\": r\"sq0csp-[ 0-9A-Za-z\\-_]{43}|sq0[a-z]{3}-[0-9A-Za-z\\-_]{22,43}\",\n    \"square_access_token\": r\"sqOatp-[0-9A-Za-z\\-_]{22}|EAAA[a-zA-Z0-9]{60}\",\n    \"stripe_standard_api\": r\"sk_live_[0-9a-zA-Z]{24}\",\n    \"stripe_restricted_api\": r\"rk_live_[0-9a-zA-Z]{24}\",\n    \"github_access_token\": r\"[a-zA-Z0-9_-]*:[a-zA-Z0-9_\\-]+@github\\.com*\",\n    \"rsa_private_key\": r\"-----BEGIN RSA PRIVATE KEY-----\",\n    \"ssh_dsa_private_key\": r\"-----BEGIN DSA PRIVATE KEY-----\",\n    \"ssh_ec_private_key\": r\"-----BEGIN EC PRIVATE KEY-----\",\n    \"pgp_private_block\": r\"-----BEGIN PGP PRIVATE KEY BLOCK-----\",\n    \"json_web_token\": r\"ey[A-Za-z0-9_-]*\\.[A-Za-z0-9._-]*|ey[A-Za-z0-9_\\/+-]*\\.[A-Za-z0-9._\\/+-]*\",\n    \"social_security_number\": r\"(?!666|000|9\\\\d{2})\\\\d{3}-(?!00)\\\\d{2}-(?!0{4})\\\\d{4}$\",\n    \"e_mail\": r\"(?:^|\\s)[\\w!#$%&'*+/=?^`{|}~-](\\.?[\\w!#$%&'*+/=?^`{|}~-])*@\\w+[.-]?\\w*\\.[a-zA-Z]{2,3}\\b\",\n}\n\n# Used to query the type later since that is more efficient than doing it dynamically.\nregexes_patterns_inverse = {\n    r\"AIza[0-9A-Za-z-_]{35}\": \"google_api\",\n    r\"([13][a-km-zA-HJ-NP-Z0-9]{26,33})\": \"bitcoin_address\",\n    r\"xox.-[0-9]{12}-[0-9]{12}-[0-9a-zA-Z]{24}\": \"slack_api_key\",\n    r\"[0-9a-fA-F]{8}-[0-9a-fA-F]{4}-[0-9a-fA-F]{12}\": \"google_cloud_platform_auth\",\n    r\"[A-Za-z0-9_]{21}--[A-Za-z0-9_]{8}\": \"google_cloud_platform_api\",\n    r\"[0-9(+-[0-9A-Za-z_]{32}.apps.qooqleusercontent.com\": \"gmail_auth_token\",\n    r\"[0-9a-fA-F]{40}\": \"github_auth_token\",\n    r\"[0-9a-fA-F]{7}.[0-9a-fA-F]{32}\": \"Instagram_token\",\n    r\"6L[0-9A-Za-z-_]{38}|^6[0-9a-zA-Z_-]{39}$\": \"google_captcha\",\n    r\"ya29\\.[0-9A-Za-z\\-_]+\": \"google_oauth\",\n    r\"A[SK]IA[0-9A-Z]{16}\": \"amazon_aws_access_key_id\",\n    r\"amzn\\\\.mws\\\\.[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}\": \"amazon_mws_auth_toke\",\n    r\"s3\\.amazonaws.com[/]+|[a-zA-Z0-9_-]*\\.s3\\.amazonaws.com\": \"amazon_aws_url\",\n    r\"EAACEdEose0cBA[0-9A-Za-z]+\": \"facebook_access_token\",\n    r\"basic\\s*[a-zA-Z0-9=:_\\+\\/-]+\": \"authorization_basic\",\n    r\"bearer\\s*[a-zA-Z0-9_\\-\\.=:_\\+\\/]+\": \"authorization_bearer\",\n    r\"api[key|\\s*]+[a-zA-Z0-9_\\-]+\": \"authorization_api\",\n    r\"key-[0-9a-zA-Z]{32}\": \"mailgun_api_key\",\n    r\"access_token\\$production\\$[0-9a-z]{16}\\$[0-9a-f]{32}\": \"paypal_braintree_access_token\",\n    r\"sq0csp-[ 0-9A-Za-z\\-_]{43}|sq0[a-z]{3}-[0-9A-Za-z\\-_]{22,43}\": \"square_oauth_secret\",\n    r\"sqOatp-[0-9A-Za-z\\-_]{22}|EAAA[a-zA-Z0-9]{60}\": \"square_access_token\",\n    r\"sk_live_[0-9a-zA-Z]{24}\": \"stripe_standard_api\",\n    r\"rk_live_[0-9a-zA-Z]{24}\": \"stripe_restricted_api\",\n    r\"[a-zA-Z0-9_-]*:[a-zA-Z0-9_\\-]+@github\\.com*\": \"github_access_token\",\n    r\"-----BEGIN RSA PRIVATE KEY-----\": \"rsa_private_key\",\n    r\"-----BEGIN EC PRIVATE KEY-----\": \"ssh_ec_private_key\",\n    r\"-----BEGIN DSA PRIVATE KEY-----\": \"ssh_dsa_private_key\",\n    r\"-----BEGIN PGP PRIVATE KEY BLOCK-----\": \"pgp_private_block\",\n    r\"ey[A-Za-z0-9_-]*\\.[A-Za-z0-9._-]*|ey[A-Za-z0-9_\\/+-]*\\.[A-Za-z0-9._\\/+-]*\": \"json_web_token\",\n    r\"(?!666|000|9\\\\d{2})\\\\d{3}-(?!00)\\\\d{2}-(?!0{4})\\\\d{4}$\": \"social_security_number\",\n    r\"(?:^|\\s)[\\w!#$%&'*+/=?^`{|}~-](\\.?[\\w!#$%&'*+/=?^`{|}~-])*@\\w+[.-]?\\w*\\.[a-zA-Z]{2,3}\\b\": \"e_mail\",\n}\n\n\nclass PIIDetector:\n    # Pre compile regexes.\n    def __init__(self):\n        self.regex_list_compiled = []\n        for regex in regex_patterns.values():\n            regex_compiled = re.compile(regex, re.I)\n            self.regex_list_compiled.append(regex_compiled)\n\n    # Returns first pii match in input_text or (\"\", None).\n    def get_pii(self, input_text: str):\n        for reg in self.regex_list_compiled:\n            match = re.search(reg, input_text)\n            if match is None:\n                continue\n            else:\n                return (reg.pattern, match.start())\n        return (\"\", None)\n\n    def formatted_output(self, match_list: list):\n        for match in match_list:\n            print(\"\\nLinenumber: \" + str(match[0]))\n            # To query the actual name efficiently, use  inverted dictionary.\n            print(\"Type: \" + regexes_patterns_inverse.get(match[1][0]))\n            print(\"Start Position: \" + str(match[1][1]))\n", "scripts/postprocessing/task_schedule.py": "from enum import Enum\n\nimport numpy as np\nfrom scipy import optimize\n\n\nclass Task(Enum):\n    RANKING = 0\n    ANSWER = 1\n    PROMPT = 2\n    VOTE = 3\n\n\ndef task_selection(\n    num_ranking_tasks: int, current_prompts: int, target_num_prompts: int, p: float, answers_per_prompt: int\n) -> Task:\n    \"\"\"\n    This computes which task to serve to the user.\n    In general, this method aims to get rankable tasks out of the active pool ASAP.\n    Before checking anything else, we first have a p% probability of running a ranking task.\n    After that, we can dynamically determine which task to serve by balancing the number of active tasks.\n\n        Parameters:\n            num_ranking_tasks (int): number of prompts that are ready to do ranking (i.e. have \"answers_per_prompt\" many answers)\n            current_prompts (int): how many prompts are currently in the active pool\n            target_num_prompts (int): how many prompts _should_ be in the active pool\n            p (float): probability to serve a ranking task, if one is available\n            answers_per_prompt (int): number of answers we want to have per prompt\n        Returns:\n            task (Task): the task Enum that corresponds to one of the four tasks\n    \"\"\"\n    if num_ranking_tasks > 0 and np.random.rand() < p:\n        return Task.RANKING\n    rate = 50 / (current_prompts * 2)\n    prob_prompt_task = 0.5 + (target_num_prompts - current_prompts) * rate\n    # Yes, I'm too lazy to solve this analytically...\n    prob_unfinished_prompt = optimize.linprog(\n        np.array([1, 1]), A_eq=np.array([[1, 1], [1, -answers_per_prompt]]), b_eq=np.array([1, 0]), bounds=(0, None)\n    ).x[0]\n    if np.random.rand() < prob_prompt_task:\n        if np.random.rand() < prob_unfinished_prompt:\n            return Task.ANSWER\n        else:\n            return Task.PROMPT\n    else:\n        return Task.VOTE\n\n\ndef next_answer_task(possible_prompts, answers_per_prompt):\n    \"\"\"\n    If the `task_selection`method returns \"answer\", you can use this method to decide which\n    prompt should get an answer next.\n    The goal of this is to finish off the prompts that have almost enough answers collected already:\n    I.e. if we want 5 answers, this is going to give preferential sampling to those prompts that already\n    have 4/5 answers.\n    This helps to not have too much close-to-finished prompts in the active set.\n\n        Parameters:\n            possible_prompts (dict[prompt_id, num_answers]): a dictionary containing all open prompts and the number of answers these prompts currently have.\n            answers_per_prompt (int): number of answers we per prompt to target\n        Returns:\n            prompt_id (int): the prompt_id corresponding to the next prompt that should get a new answer\n    \"\"\"\n    nums = list(set(possible_prompts.values()))\n    p = np.array([max(x / answers_per_prompt, 1 / answers_per_prompt) for x in nums])\n    idx = np.random.choice(nums, p=p / p.sum())\n    sample = np.random.choice([k for k, v in possible_prompts.items() if v == idx])\n    return sample\n\n\nif __name__ == \"__main__\":\n    x = task_selection(1, 500, 1000, 0.1, 5)\n    print(x)\n    y = next_answer_task({\"this\": 2, \"is\": 4, \"a\": 1, \"test\": 4}, 5)\n    print(y)\n", "scripts/postprocessing/ranking_disagreement.py": "from collections import defaultdict\n\nimport numpy as np\nimport pandas as pd\nimport psycopg2\nfrom rankings import ranked_pairs\nfrom scipy.stats import kendalltau\n\n\n# source: wikipedia ;)\n# but here without the normalization\ndef normalised_kendall_tau_distance(values1, values2):\n    \"\"\"Compute the Kendall tau distance.\"\"\"\n    n = len(values1)\n    assert len(values2) == n, \"Both lists have to be of equal length\"\n    i, j = np.meshgrid(np.arange(n), np.arange(n))\n    a = np.argsort(values1)\n    b = np.argsort(values2)\n    ndisordered = np.logical_or(\n        np.logical_and(a[i] < a[j], b[i] > b[j]), np.logical_and(a[i] > a[j], b[i] < b[j])\n    ).sum()\n    return ndisordered / (n * (n - 1))\n\n\ndef get_df():\n    \"\"\"\n    Simple method that computes merged rankings and compares them to each user.\n    Most interesting output for end-user is presumably the last that lists each user with their\n    correlation to the mean ranking.\n    Lower means less well aligned to the mean, higher means more well aligned.\n    Note that rankings with fewer options are more likely to be wrong, so this could\n    yield to misleading results:\n    **You cannot use this for automatic flagging!**\n    \"\"\"\n    conn = psycopg2.connect(\"host=0.0.0.0 port=5432 user=postgres password=postgres dbname=postgres\")\n    # Define the SQL query\n    # query = \"\"\"SELECT DISTINCT t.parent_message_id, r.user_id, r.payload->'payload'->>'ranked_message_ids' as ranked_ids\n    #    FROM message_reaction r JOIN task t ON r.task_id = t.id\n    #      WHERE r.payload->'payload'->>'type' = 'message_ranking';\"\"\"\n    role = \"'assistant'\"\n    message_tree_id = None  # \"'ef458036-ae8e-4ff5-98f2-0f9dfedcb206'\"\n    query = f\"\"\"\n        -- get all ranking results of completed tasks for all parents with >= 2 children\n        SELECT DISTINCT p.parent_id, p.message_tree_id, mr.* FROM\n        (\n            -- find parents with > 1 children\n            SELECT m.parent_id, m.message_tree_id, COUNT(m.id) children_count\n            FROM message_tree_state mts\n            INNER JOIN message m ON mts.message_tree_id = m.message_tree_id\n            WHERE m.review_result                  -- must be reviewed\n            AND NOT m.deleted                   -- not deleted\n            AND m.parent_id IS NOT NULL         -- ignore initial prompts\n            AND ({role} IS NULL OR m.role = {role}) -- children with matching role\n            -- AND mts.message_tree_id = {message_tree_id}\n            GROUP BY m.parent_id, m.message_tree_id\n            HAVING COUNT(m.id) > 1\n        ) as p\n        LEFT JOIN task t ON p.parent_id = t.parent_message_id AND t.done AND (t.payload_type = 'RankPrompterRepliesPayload' OR t.payload_type = 'RankAssistantRepliesPayload')\n        LEFT JOIN message_reaction mr ON mr.task_id = t.id AND mr.payload_type = 'RankingReactionPayload'\n        \"\"\"\n\n    # Read the query results into a Pandas dataframe\n    df = pd.read_sql(query, con=conn)\n    print(df[[\"message_tree_id\", \"parent_id\", \"payload\"]])\n    # Close the database connection\n    conn.close()\n    users = set()\n    messages = set()\n    rankings = defaultdict(list)\n    rankings_with_user = defaultdict(list)\n    for row in df.itertuples(index=False):\n        row = row._asdict()\n        users.add(str(row[\"user_id\"]))\n        messages.add(str(row[\"message_tree_id\"]))\n        #\n        if row[\"payload\"] is None:\n            continue\n        ranking = row[\"payload\"][\"payload\"][\"ranked_message_ids\"]\n        rankings_with_user[str(row[\"parent_id\"])].append((ranking, str(row[\"user_id\"])))\n        rankings[str(row[\"parent_id\"])].append(ranking)\n    print(*[f\"{k} : {v}\" for k, v in rankings.items()], sep=\"\\n\")\n    users = list(users)\n    messages = list(messages)\n    consensus = dict()\n    total_correlation = list()\n    for k, v in rankings.items():\n        # print(\"v\",[len(i) for i in v])\n        common_set = set.intersection(*map(set, v))\n        # clean up the rankings and remove stuff not in all of them\n        v = [list(filter(lambda x: x in common_set, ids)) for ids in v]\n        merged_rankings = ranked_pairs(v)\n        consensus[k] = merged_rankings\n        ls = []\n        for vote, id in rankings_with_user[k]:\n            # clean up the rankings and remove stuff not in all of them\n            vote = list(filter(lambda x: x in common_set, vote))\n            ls.append((kendalltau(merged_rankings, vote), id))\n        rankings_with_user[k] = ls\n        total_correlation.extend(ls)\n    correlation_by_user = defaultdict(list)\n    for u in users:\n        for c, m in total_correlation:\n            if m == u:\n                correlation_by_user[u].append(c)\n\n    return consensus, users, messages, rankings_with_user, correlation_by_user\n\n\nif __name__ == \"__main__\":\n    cons, user, messages, rankings, correlation_by_user = get_df()\n    # print(user)\n    # print(messages)\n    # print(rankings)\n    # print(\"consensus:\", cons)\n    print(\"correlation_by_user:\", correlation_by_user)\n    for k, v in correlation_by_user.items():\n        if len(v) < 50:\n            res = \"not enough data\"\n        else:\n            i = list(map(lambda x: x, v))\n            res = np.mean(i)\n            res_std = np.std(i)\n            print(\"result:\", k, f\" with value {res:.2f}\", f\"\u00b1 {res_std:.2f}\")\n", "scripts/postprocessing/importance_selection.py": "import logging\nimport warnings\n\nimport numpy as np\nimport pandas as pd\nimport psycopg2\nfrom scipy.optimize import LinearConstraint, minimize\nfrom scipy.sparse import coo_array, csr_array, csr_matrix, hstack\nfrom scipy.special import softmax\nfrom tqdm import trange\nfrom tqdm.contrib.logging import logging_redirect_tqdm\n\n\ndef least_squares_fit(features, target, scaling=1):\n    X = features  # (features - np.mean(features, 0)) / np.std(features, 0)\n    # print(\"feature\",X.shape)\n    # get target\n    y = target.reshape(-1)\n    # Use simple imputer for mean to not change the importance of tree split\n    # Create an instance of the ExtraTreesRegressor algorithm\n    zX = X.toarray()\n    summed_target = y  # (y+zX[:,-1])/2\n    vote_matrix = csr_matrix(zX[:, :-1])\n    constraint = LinearConstraint(np.ones(X.shape[-1] - 1), 1 * scaling, 1 * scaling)\n    init = np.ones(X.shape[-1] - 1)  # lsqr(vote_matrix,summed_target)[0]\n    init = init / np.linalg.norm(init)\n    result = minimize(\n        lambda x: np.sum((vote_matrix @ x - summed_target) ** 2),\n        init,\n        jac=lambda x: 2 * vote_matrix.T @ (vote_matrix @ x - summed_target),\n        constraints=constraint,\n        hess=lambda _: 2 * vote_matrix.T @ vote_matrix,\n        method=\"trust-constr\",\n    )\n    # result = least_squares(residual, np.ones(X.shape[-1]-1))\n    # result = least_squares(zX[:,:-1], (y+zX[:,-1])/2,\n    # print(result)\n    return np.concatenate([result.x, np.ones(1)])\n\n\ndef get_df(study_label):\n    conn = psycopg2.connect(\"host=0.0.0.0 port=5432 user=postgres password=postgres dbname=postgres\")\n    # Define the SQL query\n    query = (\n        \"SELECT DISTINCT message_id, labels, message.user_id FROM text_labels JOIN message ON message_id = message.id;\"\n    )\n\n    # Read the query results into a Pandas dataframe\n    df = pd.read_sql(query, con=conn)\n    print(df.head())\n    # Close the database connection\n    conn.close()\n    users = set()\n    messages = set()\n    for row in df.itertuples(index=False):\n        row = row._asdict()\n        users.add(str(row[\"user_id\"]))\n        # for row in df.itertuples(index=False):\n        # row = row._asdict()\n        messages.add(str(row[\"message_id\"]))\n    users = list(users)\n    messages = list(messages)\n    print(\"num users:\", len(users), \"num messages:\", len(messages), \"num in df\", len(df))\n\n    # arr = np.full((len(messages), len(users)), np.NaN, dtype=np.half)\n    row_idx = []\n    col_idx = []\n    data = []\n\n    def swap(x):\n        return (x[1], x[0])\n\n    dct = dict(map(swap, enumerate(messages)))\n    print(\"converting messages...\")\n    df[\"message_id\"] = df[\"message_id\"].map(dct)\n    print(\"converting users...\")\n    df[\"user_id\"] = df[\"user_id\"].map(dict(map(swap, enumerate(users))))\n    print(\"converting labels...\")\n    df[\"labels\"] = df[\"labels\"].map(lambda x: float(x.get(study_label, 0)))\n    row_idx = df[\"message_id\"].to_numpy()\n    col_idx = df[\"user_id\"].to_numpy()\n    data = df[\"labels\"].to_numpy()\n    print(data)\n    print(row_idx)\n    print(col_idx)\n    \"\"\" for row in df.itertuples(index=False):\n        row = row._asdict()\n        labels = row[\"labels\"]\n        value = labels.get(study_label, None)\n        if value is not None:\n            # tmp=out[str(row[\"message_id\"])]\n            # tmp = np.array(tmp)\n            # tmp[users.index(row[\"user_id\"])] = value\n            # out[str(row[\"message_id\"])] = np.array(tmp)\n            # print(out[str(row[\"message_id\"])].density)\n            row_idx.append(messages.index(str(row[\"message_id\"])))\n            col_idx.append(users.index(str(row[\"user_id\"])))\n            data.append(value)\n            #arr[mid, uid] = value \"\"\"\n    arr = csr_array(coo_array((data, (row_idx, col_idx))))\n    print(\"results\", len(users), arr.shape)\n    # df = pd.DataFrame.from_dict(out,orient=\"index\")\n    print(\"generated dataframe\")\n    return arr, messages, users\n\n\ndef reweight_features(features, weights, noise_scale=0.0):\n    # X = df.drop(target_col, axis=1)\n    # print(\"info\",features.shape,weights.shape)\n    # X = (features - np.mean(features, 0).reshape(1,-1)) / np.std(features, 0).reshape(1,-1)\n    noise = np.random.randn(weights.shape[0]) * noise_scale\n    weights = weights + noise\n    # normalizer = (X.notna().astype(float) * weights).sum(skipna=True, axis=1)\n    values = features @ weights\n    # values = values / normalizer\n    return values\n\n\ndef get_subframe(arr, columns_to_filter):\n    # return np.delete(arr, columns_to_filter, axis=1)\n    \"\"\"\n    Remove the rows denoted by ``indices`` form the CSR sparse matrix ``mat``.\n    \"\"\"\n    if not isinstance(arr, csr_array):\n        raise ValueError(\"works only for CSR format -- use .tocsr() first\")\n    indices = list(columns_to_filter)\n    mask = np.ones(arr.shape[1], dtype=bool)\n    mask[indices] = False\n    return arr[:, mask]\n\n\ndef sample_importance_weights(importance_weights, temperature=1.0):\n    weights = softmax(\n        abs(importance_weights) / temperature,\n    )\n    column = np.random.choice(len(importance_weights), p=weights)\n    return column\n\n\ndef make_random_testframe(num_rows, num_cols, frac_missing):\n    data = np.random.rand(num_rows, num_cols).astype(np.float16)\n    mask = np.random.rand(num_rows, num_cols) < frac_missing\n    data[mask] = np.nan\n    return data\n\n\ndef combine_underrepresented_columns(arr, num_instances):\n    # 1. get the mask\n    mask = arr != 0\n    to_combine = mask.sum(0) < num_instances\n    # print(\"to combine\", mask.sum(0))\n    # print(\"combining\", to_combine.astype(int).sum().tolist(), \"many columns\")\n    if not any(to_combine):\n        return arr\n    # mean = np.zeros(arr.shape[0])\n    # for i in to_combine.tolist():\n    #    mean = np.nansum(np.stack(arr[:,i],mean),0)\n    # mean = mean/len(to_combine)\n    mean = np.mean(arr[:, to_combine], 1).reshape(-1, 1)\n    # print(\"mean shape\",mean.shape)\n    dp = np.arange(len(to_combine))[to_combine]\n    # print(\"removing unused columns\")\n    arr = get_subframe(arr, dp)\n    # print(\"subframe shape\",arr.shape)\n    arr = hstack([arr, mean])\n    # print(\"out arr\", arr.shape)\n    # print((mean==0).astype(int).sum())\n    return arr\n\n\ndef importance_votes(arr, to_fit=10, init_weight=None):\n    # arr = combine_underrepresented_columns(matrix,underrepresentation_thresh)\n    filtered_columns = []\n    weighter = None\n    if init_weight is None:\n        weighter = np.ones(arr.shape[1]) / arr.shape[1]  # pd.Series(1.0, index=df.drop(columns=target).columns)\n    else:\n        weighter = init_weight\n    # print(arr.shape)\n    index = np.arange(arr.shape[1])\n    # subtract 1: the last one will always have maximal reduction!\n    bar = trange(to_fit)\n    target = np.ones(arr.shape[0])\n    for i in bar:\n        index = list(filter(lambda x: x not in filtered_columns, index))\n        # 0. produce target column:\n        # print(\"step 0\")\n        target_old = target\n        target = reweight_features(arr, weighter)\n        error = np.mean((target - target_old) ** 2)\n        bar.set_description(f\"expected error: {error}\", refresh=True)\n        if error < 1e-10:\n            break\n        # 1. get a subframe of interesting features\n        # print(\"step 1\")\n        # subframe = get_subframe(arr, filtered_columns)\n        # 2. compute feature importance\n        # print(\"step 2\")\n        # importance_weights=None\n        # importance_weights = compute_feature_importance(arr, target, index)\n        weighter = least_squares_fit(arr, target)\n        # 3. sample column\n        # print(\"step 3\")\n        # new_column = sample_importance_weights(importance_weights[\"importance\"], temperature)\n        # new_column=index[new_column]\n        # value = importance_weights[\"importance\"][new_column]\n        # print(weighter.shape, importance_weights[\"importance\"].shape)\n        # weighter += alpha[i] * importance_weights[\"importance\"].to_numpy()\n        # normalize to maintain the \"1-voter one vote\" total number of votes!\n        # weighter = weighter / sum(abs(weighter))\n        # stepsize = np.mean(abs(importance_weights[\"importance\"].to_numpy()))\n        # bar.set_description(f\"expected stepsize: {stepsize}\", refresh=True)\n        # filtered_columns.append(new_column)\n    # print(\"new weight values\", weighter)\n    return reweight_features(arr, weighter), weighter\n\n\ndef select_ids(arr, pick_frac, minima=(50, 500), folds=50, to_fit=200, frac=0.6):\n    \"\"\"\n    selects the top-\"pick_frac\"% of messages from \"arr\" after merging all\n    users with less than \"minima\" votes (minima increases linearly with each iteration from min to max).\n    The method returns all messages that are within `frac` many \"minima\" selection\n    \"\"\"\n    votes = []\n    minima = np.linspace(*minima, num=folds, dtype=int)\n    num_per_iter = int(arr.shape[0] * pick_frac)\n    writer_num = 0\n    tmp = None\n    for i in trange(folds):\n        tofit = combine_underrepresented_columns(arr, minima[i])\n        if tofit.shape[1] == writer_num:\n            print(\"already tested these writer counts, skipping and using cached value.....\")\n            votes.append(tmp)\n            continue\n        writer_num = tofit.shape[1]\n        # print(\"arr shape\", arr.shape)\n        init_weight = np.ones(tofit.shape[1]) / tofit.shape[1]\n        out, weight = importance_votes(tofit, init_weight=init_weight, to_fit=to_fit)\n        # print(i, \"final weight\")\n        # print(weight)\n        # mask =(out>thresh)\n        # out = np.arange(arr.shape[0])[mask]\n        indices = np.argpartition(out, -num_per_iter)[-num_per_iter:]\n        tmp = np.zeros((arr.shape[0]))\n        tmp[indices] = 1\n        votes.append(tmp)\n        # votes.append(indices.tolist())\n    # print(*[f\"user_id: {users[idx]} {m}\u00b1{s}\" for m, s, idx in zip(weights_mean, weights_std, range(len(weights_mean)))], sep=\"\\n\")\n    out = []\n    votes = np.stack(votes, axis=0)\n    print(\"votespace\", votes.shape)\n    votes = np.mean(votes, 0)\n    for idx, f in enumerate(votes):\n        if f > frac:\n            out.append((idx, f))\n    return out\n\n\nLOG = logging.getLogger(__name__)\n\nif __name__ == \"__main__\":\n    warnings.filterwarnings(\"ignore\", category=FutureWarning)\n    warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n    warnings.simplefilter(\"ignore\")\n    logging.captureWarnings(True)\n    logging.basicConfig(level=logging.ERROR)\n    # Generate some example data\n    # df = make_random_testframe(100_000,5000,0.99)\n    df, message_ids, users = get_df(\"quality\")\n    print(\"combining columns:\")\n    # df = combine_underrepresented_columns(df, 100)\n    weights = np.ones(df.shape[-1])\n    y = reweight_features(df, weights)\n    num_per_iter = int(df.shape[0] * 0.5)\n    naive = np.argpartition(y, -num_per_iter)[-num_per_iter:]\n\n    print(\"after preprocessing\")\n    # print(df)\n    # preproc input\n\n    # Compute feature importances\n    # y = reweight_features(df,np.ones(df.shape[1]))\n    # importance_weights = compute_feature_importance(df, y, list(range(df.shape[1])))\n    # Print the importance weights for each feature\n    # print(importance_weights)\n\n    print(\"STARTING RUN\")\n\n    # sampled_columns = sample_importance_weights(\n    #    importance_weights[\"importance\"],\n    # )\n    # print(\"sampled column\", sampled_columns)\n    # print(\"compute importance votes:\")\n    # weighted_votes, weightings = importance_votes(df)\n    # print(weighted_votes)\n    # print(weightings)\n    with logging_redirect_tqdm():\n        print(\"selected ids\")\n        ids = select_ids(df, 0.5, folds=500)\n\n    #    print(res, frac)\n    conn = psycopg2.connect(\"host=0.0.0.0 port=5432 user=postgres password=postgres dbname=postgres\")\n    # Define the SQL query\n    # , payload#>'{payload, text}' as text\n    query = \"SELECT DISTINCT id as message_id, message_tree_id FROM message;\"\n    print(\"selected\", len(ids), \"messages\")\n    # Read the query results into a Pandas dataframe\n    df = pd.read_sql(query, con=conn)\n    out = []\n    fracs = []\n    in_naive = []\n    for i, frac in ids:\n        res = message_ids[i]\n        out.append((df.loc[df[\"message_id\"] == res]))\n        fracs.append(frac)\n        in_naive.append(i in naive)\n    df = pd.concat(out)\n    df[\"fracs\"] = fracs\n    df[\"in_naive\"] = in_naive\n    print(df.shape)\n    print(\"differences from naive\", len(in_naive) - sum(in_naive))\n    print(df)\n    df.to_csv(\"output.csv\")\n", "scripts/postprocessing/infogain_selector.py": "import numpy as np\nfrom scipy.special import gammaln, psi\nfrom scipy.stats import dirichlet\n\n'''\nLegacy numerical solution.\nShould not be used as it is probably broken\n\n\ndef make_range(*x):\n    \"\"\"\n    constructs leftover values for the simplex given the first k entries\n    (0,x_k) = 1-(x_1+...+x_(k-1))\n    \"\"\"\n    return (0, max(0, 1 - sum(x)))\n\n\ndef relative_entropy(p, q):\n    \"\"\"\n    relative entropy of the two given dirichlet distributions\n    \"\"\"\n\n    def tmp(*x):\n        \"\"\"\n        First adds the last always forced entry to the input (the last x_last = 1-(x_1+...+x_(N)) )\n        Then computes the relative entropy of posterior and prior for that datapoint\n        \"\"\"\n        x_new = np.append(x, 1 - sum(x))\n        return p(x_new) * log2(p(x_new) / q(x_new))\n\n    return tmp\n\n\ndef naive_monte_carlo_integral(fun, dim, samples=10_000_000):\n    s = np.random.rand(dim - 1, samples)\n    s = np.sort(np.concatenate((np.zeros((1, samples)), s, np.ones((1, samples)))), 0)\n    # print(s)\n    pos = np.diff(s, axis=0)\n    # print(pos)\n    res = fun(pos)\n    return np.mean(res)\n\ndef infogain(a_post, a_prior):\n    raise (\n        \"\"\"For the love of good don't use this:\n    it's insanely poorly conditioned, the worst numerical code I have ever written\n    and it's slow as molasses. Use the analytic solution instead.\n\n    Maybe remove\n    \"\"\"\n    )\n    args = len(a_prior)\n    p = dirichlet(a_post).pdf\n    q = dirichlet(a_prior).pdf\n    (info, _) = nquad(relative_entropy(p, q), [make_range for _ in range(args - 1)], opts={\"epsabs\": 1e-8})\n    # info = naive_monte_carlo_integral(relative_entropy(p,q), len(a_post))\n    return info\n'''\n\n\ndef analytic_solution(a_post, a_prior):\n    \"\"\"\n    Analytic solution to the KL-divergence between two dirichlet distributions.\n    Proof is in the Notion design doc.\n    \"\"\"\n    post_sum = np.sum(a_post)\n    prior_sum = np.sum(a_prior)\n    info = (\n        gammaln(post_sum)\n        - gammaln(prior_sum)\n        - np.sum(gammaln(a_post))\n        + np.sum(gammaln(a_prior))\n        - np.sum((a_post - a_prior) * (psi(a_post) - psi(post_sum)))\n    )\n\n    return info\n\n\ndef uniform_expected_infogain(a_prior):\n    mean_weight = dirichlet.mean(a_prior)\n    results = []\n    for i, w in enumerate(mean_weight):\n        a_post = a_prior.copy()\n        a_post[i] = a_post[i] + 1\n        results.append(w * analytic_solution(a_post, a_prior))\n    return np.sum(results)\n\n\nif __name__ == \"__main__\":\n    a_prior = np.array([1, 1, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n    a_post = np.array([1, 1, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n\n    print(\"algebraic\", analytic_solution(a_post, a_prior))\n    # print(\"raw\",infogain(a_post, a_prior))\n    print(\"large infogain\", uniform_expected_infogain(a_prior))\n    print(\"post infogain\", uniform_expected_infogain(a_post))\n    # a_prior = np.array([1,1,1000])\n    # print(\"small infogain\",uniform_expected_infogain(a_prior))\n", "scripts/frontend-development/find-missing-locales.py": "import sys\nfrom collections import defaultdict\nfrom glob import glob\nfrom json import load\nfrom os import path\n\nALL_PATH = \"../../website/public/locales/**/*.json\"\nDIR = path.dirname(__file__)\nEN_PATH = \"../../website/public/locales/en/*.json\"\n\n\ndef get_not_translated(en_json, translation_json, parent_key=None):\n    not_translated = []\n    for key in en_json.keys():\n        if key in translation_json and translation_json[key] == en_json[key]:\n            not_translated.append((f\"{parent_key}.{key}\" if parent_key else key))\n        elif isinstance(en_json[key], dict):\n            if key not in translation_json:\n                msg = f\"{parent_key}.{key} (and children)\" if parent_key else \"{key} (and children)\"\n                not_translated.append(msg)\n            else:\n                not_translated.extend(get_not_translated(en_json[key], translation_json[key], key))\n    return not_translated\n\n\ndef get_missing(en_json, translation_json):\n    return [key for key in en_json.keys() if key not in translation_json]\n\n\ndef print_result(missing, not_translated, file):\n    if len(missing):\n        print(f\"[{path.basename(path.dirname(file))}] - {path.basename(file)}\\tmissing: {missing}\")\n    if len(not_translated):\n        print(\n            f\"[{path.basename(path.dirname(file))}] - {path.basename(file)}\\tpotentially untranslated: {not_translated}\"\n        )\n\n\ndef audit(file, en_file):\n    en_json = load(open(en_file, encoding=\"utf-8\"))\n    translation_json = load(open(file, encoding=\"utf-8\"))\n    return (get_missing(en_json, translation_json), get_not_translated(en_json, translation_json), file)\n\n\ndef main():\n    per_language_dict = defaultdict(list)\n    for en_file in glob(path.join(DIR, EN_PATH)):\n        for file in glob(path.join(DIR, ALL_PATH)):\n            if path.basename(en_file) == path.basename(file) and file != en_file:\n                lang = path.basename(path.dirname(file))\n                if len(sys.argv) == 0 or lang in sys.argv:\n                    file_info = audit(file, en_file)\n                    per_language_dict[lang].append(file_info)\n    for results in per_language_dict.values():\n        list(map(lambda args: print_result(*args), results))\n        print()\n\n\nif __name__ == \"__main__\":\n    main()\n", "scripts/discord/verify-lobby.py": "#!/usr/bin/env python3\n\n\"\"\"This file is for moderators to verify new users in the lobby.\n\nFirst, moderators read the brief introduction people write in the lobby.\nIf all people's introductions are acceptable, moderators run this script.\n\nNeeds BOT_TOKEN environment variable to be set to the bot token.\n\n\"\"\"\n\n\nimport discord\nimport pydantic\nimport tqdm.asyncio as tqdm\n\n\nclass Settings(pydantic.BaseSettings):\n    bot_token: str\n\n\nsettings = Settings()\n\nintents = discord.Intents.default()\nintents.message_content = True\nintents.members = True\nclient = discord.Client(intents=intents)\n\n\n@client.event\nasync def on_ready():\n    lobby_channel = discord.utils.get(client.get_all_channels(), name=\"lobby\")\n    # obtain the role object for the verified role\n    verified_role = discord.utils.get(lobby_channel.guild.roles, name=\"verified\")\n    async for message in tqdm.tqdm(lobby_channel.history(limit=None)):\n        if not isinstance(message.author, discord.Member):\n            print(f\"{message.author} is not a member\")\n            continue\n        for role in message.author.roles:\n            if role.name == \"unverified\":\n                print(f\"{message.author} has the unverified role.\")\n                break\n        else:\n            continue\n        # un-assign the unverified role\n        await message.author.remove_roles(role)\n        # assign the verified role\n        await message.author.add_roles(verified_role)\n        print(f\"Assigned verified role to {message.author}\")\n    await client.close()\n\n\nclient.run(settings.bot_token)\n", "scripts/discord/stats.py": "#!/usr/bin/env python3\n\n\"\"\"This file is for moderators to verify new users in the lobby.\n\nFirst, moderators read the brief introduction people write in the lobby.\nIf all people's introductions are acceptable, moderators run this script.\n\nNeeds BOT_TOKEN environment variable to be set to the bot token.\n\n\"\"\"\n\n\nimport discord\nimport pydantic\nimport tqdm.asyncio as tqdm\n\n\nclass Settings(pydantic.BaseSettings):\n    bot_token: str\n\n\nsettings = Settings()\n\nintents = discord.Intents.default()\nintents.message_content = True\nintents.members = True\nclient = discord.Client(intents=intents)\n\n\n@client.event\nasync def on_ready():\n    lobby_channel = discord.utils.get(client.get_all_channels(), name=\"lobby\")\n    message: discord.Message\n    times = []\n    async for message in tqdm.tqdm(lobby_channel.history(limit=None)):\n        times.append(message.created_at.timestamp())\n    with open(\"times.txt\", \"w\") as f:\n        f.write(\"\\n\".join(map(str, times)))\n    await client.close()\n\n\nclient.run(settings.bot_token)\n", "backend/update_message_attributes.py": "import time\n\nfrom loguru import logger\nfrom oasst_backend.models import ApiClient, Message\nfrom oasst_backend.scheduled_tasks import hf_feature_extraction, toxicity\nfrom oasst_backend.utils.database_utils import default_session_factory\nfrom sqlmodel import text\n\n\ndef get_messageids_without_toxicity():\n    message_ids = None\n    with default_session_factory() as session:\n        sql = \"\"\"\n        SELECT m.id FROM message as m\n        left join message_toxicity mt on mt.message_id = m.id\n        where mt.message_id is NULL\n        \"\"\"\n        result = session.execute(\n            text(sql),\n        ).all()\n        message_ids = []\n        for row in result:\n            message_id = row[0]\n            message_ids.append(message_id)\n    return message_ids\n\n\ndef get_messageids_without_embedding():\n    message_ids = None\n    with default_session_factory() as session:\n        sql = \"\"\"\n        SELECT m.id FROM message as m\n        left join message_embedding mt on mt.message_id = m.id\n        where mt.message_id is NULL\n        \"\"\"\n        result = session.execute(\n            text(sql),\n        ).all()\n        message_ids = []\n        for row in result:\n            message_id = row[0]\n            message_ids.append(message_id)\n    return message_ids\n\n\ndef find_and_update_embeddings(message_ids):\n    try:\n        with default_session_factory() as session:\n            for message_id in message_ids:\n                result = session.query(Message).filter(Message.id == message_id).first()\n                if result is not None:\n                    api_client_id = result.api_client_id\n                    text = result.payload.payload.text\n                    api_client = session.query(ApiClient).filter(ApiClient.id == api_client_id).first()\n                    if api_client is not None and text is not None:\n                        hf_feature_extraction(text=text, message_id=message_id, api_client=api_client.__dict__)\n                        # to not get rate limited from HF\n                        time.sleep(10)\n    except Exception as e:\n        logger.error(str(e))\n    logger.debug(\"Done: find_and_update_embeddings\")\n\n\ndef find_and_update_toxicity(message_ids):\n    try:\n        with default_session_factory() as session:\n            for message_id in message_ids:\n                result = session.query(Message).filter(Message.id == message_id).first()\n                if result is not None:\n                    api_client_id = result.api_client_id\n                    text = result.payload.payload.text\n                    api_client = session.query(ApiClient).filter(ApiClient.id == api_client_id).first()\n                    if api_client is not None and text is not None:\n                        toxicity(text=text, message_id=message_id, api_client=api_client.__dict__)\n                        # to not get rate limited from HF\n                        time.sleep(10)\n    except Exception as e:\n        logger.error(str(e))\n    logger.debug(\"Done:  find_and_update_toxicity\")\n\n\ndef main():\n    message_ids = get_messageids_without_toxicity()\n    find_and_update_toxicity(message_ids=message_ids)\n    message_ids = get_messageids_without_embedding()\n    find_and_update_embeddings(message_ids=message_ids)\n    return\n\n\nif __name__ == \"__main__\":\n    main()\n", "backend/rerank.py": "import argparse\nfrom uuid import UUID\n\nimport oasst_backend.utils.database_utils as db_utils\nfrom export import fetch_tree_ids\nfrom loguru import logger\nfrom oasst_backend.api.deps import create_api_client\nfrom oasst_backend.database import engine\nfrom oasst_backend.models.api_client import ApiClient\nfrom oasst_backend.models.message_tree_state import State as TreeState\nfrom oasst_backend.prompt_repository import PromptRepository\nfrom oasst_backend.tree_manager import TreeManager\nfrom sqlmodel import Session\nfrom tqdm import tqdm\n\nIMPORT_API_CLIENT_ID = UUID(\"bd8fde8b-1d8e-4e9a-9966-e96d000f8363\")\n\n\ndef update_tree_ranking(tm: TreeManager, message_tree_id: UUID) -> int:\n    ranking_role_filter = None if tm.cfg.rank_prompter_replies else \"assistant\"\n    rankings_by_message = tm.query_tree_ranking_results(message_tree_id, role_filter=ranking_role_filter)\n    if len(rankings_by_message) == 0:\n        logger.warning(f\"No ranking results found for message tree {message_tree_id}\")\n        return 0\n    num_updated = 0\n    for rankings in rankings_by_message.values():\n        if len(rankings) > 0:\n            num_updated += tm.ranked_pairs_update(rankings)\n    return num_updated\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\"Update message ranks with feedback received after tree-completion.\")\n    parser.add_argument(\"--commit\", action=\"store_true\", default=False, help=\"Dry run with rollback if not specified\")\n    args = parser.parse_args()\n    return args\n\n\ndef main():\n    args = parse_args()\n\n    dry_run = not args.commit\n\n    @db_utils.managed_tx_function(auto_commit=db_utils.CommitMode.ROLLBACK if dry_run else db_utils.CommitMode.COMMIT)\n    def update_rankings_tx(db: Session, api_client: ApiClient, message_tree_id: UUID) -> int:\n        # create tree manager\n        tm = TreeManager(db, PromptRepository(db, api_client=api_client))\n        return update_tree_ranking(tm, message_tree_id)\n\n    with Session(engine) as db:\n        # get api client\n        api_client = db.query(ApiClient).filter(ApiClient.id == IMPORT_API_CLIENT_ID).first()\n        if not api_client:\n            api_client = create_api_client(\n                session=db,\n                description=\"API client used for importing data\",\n                frontend_type=\"import\",\n                force_id=IMPORT_API_CLIENT_ID,\n            )\n\n        # find all ready for export trees\n        tree_ids = fetch_tree_ids(db, state_filter=TreeState.READY_FOR_EXPORT)\n        num_updated = 0\n\n        for message_tree_id, _ in tqdm(tree_ids):\n            try:\n                num_updated += update_rankings_tx(api_client=api_client, message_tree_id=message_tree_id)\n            except Exception:\n                logger.exception(f\"Update ranking of message tree {message_tree_id} failed\")\n\n    logger.info(f\"Rank of {num_updated} messages updated.\")\n\n    if dry_run:\n        logger.info(\"DRY RUN with rollback (run with --commit to modify db)\")\n\n\nif __name__ == \"__main__\":\n    main()\n", "backend/main.py": "import json\nfrom datetime import datetime\nfrom http import HTTPStatus\nfrom math import ceil\nfrom pathlib import Path\nfrom typing import Optional\n\nimport alembic.command\nimport alembic.config\nimport fastapi\nimport redis.asyncio as redis\nfrom fastapi_limiter import FastAPILimiter\nfrom fastapi_utils.tasks import repeat_every\nfrom loguru import logger\nfrom oasst_backend.api.deps import api_auth, create_api_client\nfrom oasst_backend.api.v1.api import api_router\nfrom oasst_backend.api.v1.utils import prepare_conversation\nfrom oasst_backend.cached_stats_repository import CachedStatsRepository\nfrom oasst_backend.config import settings\nfrom oasst_backend.database import engine\nfrom oasst_backend.models import message_tree_state\nfrom oasst_backend.prompt_repository import PromptRepository, UserRepository\nfrom oasst_backend.task_repository import TaskRepository, delete_expired_tasks\nfrom oasst_backend.tree_manager import TreeManager, halt_prompts_of_disabled_users\nfrom oasst_backend.user_stats_repository import UserStatsRepository, UserStatsTimeFrame\nfrom oasst_backend.utils.database_utils import CommitMode, managed_tx_function\nfrom oasst_shared.exceptions import OasstError, OasstErrorCode\nfrom oasst_shared.schemas import protocol as protocol_schema\nfrom oasst_shared.utils import utcnow\nfrom prometheus_fastapi_instrumentator import Instrumentator\nfrom pydantic import BaseModel\nfrom sqlmodel import Session\nfrom starlette.middleware.cors import CORSMiddleware\n\napp = fastapi.FastAPI(title=settings.PROJECT_NAME, openapi_url=f\"{settings.API_V1_STR}/openapi.json\")\nstartup_time: datetime = utcnow()\n\n\n@app.exception_handler(OasstError)\nasync def oasst_exception_handler(request: fastapi.Request, ex: OasstError):\n    logger.error(f\"{request.method} {request.url} failed: {repr(ex)}\")\n\n    return fastapi.responses.JSONResponse(\n        status_code=int(ex.http_status_code),\n        content=protocol_schema.OasstErrorResponse(\n            message=ex.message,\n            error_code=OasstErrorCode(ex.error_code),\n        ).dict(),\n    )\n\n\n@app.exception_handler(Exception)\nasync def unhandled_exception_handler(request: fastapi.Request, ex: Exception):\n    logger.exception(f\"{request.method} {request.url} failed [UNHANDLED]: {repr(ex)}\")\n    status = HTTPStatus.INTERNAL_SERVER_ERROR\n    return fastapi.responses.JSONResponse(\n        status_code=status.value, content={\"message\": status.name, \"error_code\": OasstErrorCode.GENERIC_ERROR}\n    )\n\n\n# Set all CORS enabled origins\nif settings.BACKEND_CORS_ORIGINS:\n    app.add_middleware(\n        CORSMiddleware,\n        allow_origins=[str(origin) for origin in settings.BACKEND_CORS_ORIGINS],\n        allow_credentials=True,\n        allow_methods=[\"*\"],\n        allow_headers=[\"*\"],\n    )\n\nif settings.UPDATE_ALEMBIC:\n\n    @app.on_event(\"startup\")\n    def alembic_upgrade():\n        logger.info(\"Attempting to upgrade alembic on startup\")\n        try:\n            alembic_ini_path = Path(__file__).parent / \"alembic.ini\"\n            alembic_cfg = alembic.config.Config(str(alembic_ini_path))\n            alembic_cfg.set_main_option(\"sqlalchemy.url\", settings.DATABASE_URI)\n            alembic.command.upgrade(alembic_cfg, \"head\")\n            logger.info(\"Successfully upgraded alembic on startup\")\n        except Exception:\n            logger.exception(\"Alembic upgrade failed on startup\")\n\n\nif settings.OFFICIAL_WEB_API_KEY:\n\n    @app.on_event(\"startup\")\n    def create_official_web_api_client():\n        with Session(engine) as session:\n            try:\n                api_auth(settings.OFFICIAL_WEB_API_KEY, db=session)\n            except OasstError:\n                logger.info(\"Creating official web API client\")\n                create_api_client(\n                    session=session,\n                    api_key=settings.OFFICIAL_WEB_API_KEY,\n                    description=\"The official web client for the OASST backend.\",\n                    frontend_type=\"web\",\n                    trusted=True,\n                )\n\n\nif settings.ENABLE_PROM_METRICS:\n\n    @app.on_event(\"startup\")\n    async def enable_prom_metrics():\n        Instrumentator().instrument(app).expose(app)\n\n\nif settings.RATE_LIMIT:\n\n    @app.on_event(\"startup\")\n    async def connect_redis():\n        async def http_callback(request: fastapi.Request, response: fastapi.Response, pexpire: int):\n            \"\"\"Error callback function when too many requests\"\"\"\n            expire = ceil(pexpire / 1000)\n            raise OasstError(\n                f\"Too Many Requests. Retry After {expire} seconds.\",\n                OasstErrorCode.TOO_MANY_REQUESTS,\n                HTTPStatus.TOO_MANY_REQUESTS,\n            )\n\n        try:\n            redis_client = redis.from_url(\n                f\"redis://{settings.REDIS_HOST}:{settings.REDIS_PORT}/0\", encoding=\"utf-8\", decode_responses=True\n            )\n            logger.info(f\"Connected to {redis_client=}\")\n            await FastAPILimiter.init(redis_client, http_callback=http_callback)\n        except Exception:\n            logger.exception(\"Failed to establish Redis connection\")\n\n\nif settings.DEBUG_USE_SEED_DATA:\n\n    @app.on_event(\"startup\")\n    @managed_tx_function(auto_commit=CommitMode.COMMIT)\n    def create_seed_data(session: Session):\n        class DummyMessage(BaseModel):\n            task_message_id: str\n            user_message_id: str\n            parent_message_id: Optional[str]\n            text: str\n            lang: Optional[str]\n            role: str\n            tree_state: Optional[message_tree_state.State]\n\n        if not settings.OFFICIAL_WEB_API_KEY:\n            raise ValueError(\"Cannot use seed data without OFFICIAL_WEB_API_KEY\")\n\n        try:\n            logger.info(\"Seed data check began\")\n\n            api_client = api_auth(settings.OFFICIAL_WEB_API_KEY, db=session)\n            dummy_user = protocol_schema.User(id=\"__dummy_user__\", display_name=\"Dummy User\", auth_method=\"local\")\n\n            ur = UserRepository(db=session, api_client=api_client)\n            tr = TaskRepository(db=session, api_client=api_client, client_user=dummy_user, user_repository=ur)\n            ur.update_user(tr.user_id, enabled=True, show_on_leaderboard=False, tos_acceptance=True)\n            pr = PromptRepository(\n                db=session, api_client=api_client, client_user=dummy_user, user_repository=ur, task_repository=tr\n            )\n            tm = TreeManager(session, pr)\n\n            with open(settings.DEBUG_USE_SEED_DATA_PATH) as f:\n                dummy_messages_raw = json.load(f)\n\n            dummy_messages = [DummyMessage(**dm) for dm in dummy_messages_raw]\n\n            for msg in dummy_messages:\n                task = tr.fetch_task_by_frontend_message_id(msg.task_message_id)\n                if task and not task.ack:\n                    logger.warning(\"Deleting unacknowledged seed data task\")\n                    session.delete(task)\n                    task = None\n                if not task:\n                    if msg.parent_message_id is None:\n                        task = tr.store_task(\n                            protocol_schema.InitialPromptTask(hint=\"\"), message_tree_id=None, parent_message_id=None\n                        )\n                    else:\n                        parent_message = pr.fetch_message_by_frontend_message_id(\n                            msg.parent_message_id, fail_if_missing=True\n                        )\n                        conversation_messages = pr.fetch_message_conversation(parent_message)\n                        conversation = prepare_conversation(conversation_messages)\n                        if msg.role == \"assistant\":\n                            task = tr.store_task(\n                                protocol_schema.AssistantReplyTask(conversation=conversation),\n                                message_tree_id=parent_message.message_tree_id,\n                                parent_message_id=parent_message.id,\n                            )\n                        else:\n                            task = tr.store_task(\n                                protocol_schema.PrompterReplyTask(conversation=conversation),\n                                message_tree_id=parent_message.message_tree_id,\n                                parent_message_id=parent_message.id,\n                            )\n                    tr.bind_frontend_message_id(task.id, msg.task_message_id)\n                    message = pr.store_text_reply(\n                        msg.text,\n                        msg.lang or \"en\",\n                        msg.task_message_id,\n                        msg.user_message_id,\n                        review_count=5,\n                        review_result=True,\n                        check_tree_state=False,\n                        check_duplicate=False,\n                    )\n                    if message.parent_id is None:\n                        tm._insert_default_state(\n                            root_message_id=message.id,\n                            lang=message.lang,\n                            state=msg.tree_state or message_tree_state.State.GROWING,\n                        )\n                        session.flush()\n\n                    logger.info(\n                        f\"Inserted: message_id: {message.id}, payload: {message.payload.payload}, parent_message_id: {message.parent_id}\"\n                    )\n                else:\n                    logger.debug(f\"seed data task found: {task.id}\")\n\n            logger.info(\"Seed data check completed\")\n\n        except Exception:\n            logger.exception(\"Seed data insertion failed\")\n\n\n@app.on_event(\"startup\")\ndef ensure_tree_states():\n    try:\n        logger.info(\"Startup: TreeManager.ensure_tree_states()\")\n        with Session(engine) as db:\n            api_client = api_auth(settings.OFFICIAL_WEB_API_KEY, db=db)\n            tm = TreeManager(db, PromptRepository(db, api_client=api_client))\n            tm.ensure_tree_states()\n\n    except Exception:\n        logger.exception(\"TreeManager.ensure_tree_states() failed.\")\n\n\n@app.on_event(\"startup\")\n@repeat_every(seconds=60 * settings.USER_STATS_INTERVAL_DAY, wait_first=False)\n@managed_tx_function(auto_commit=CommitMode.COMMIT)\ndef update_leader_board_day(session: Session) -> None:\n    try:\n        usr = UserStatsRepository(session)\n        usr.update_stats(time_frame=UserStatsTimeFrame.day)\n    except Exception:\n        logger.exception(\"Error during leaderboard update (daily)\")\n\n\n@app.on_event(\"startup\")\n@repeat_every(seconds=60 * settings.USER_STATS_INTERVAL_WEEK, wait_first=False)\n@managed_tx_function(auto_commit=CommitMode.COMMIT)\ndef update_leader_board_week(session: Session) -> None:\n    try:\n        usr = UserStatsRepository(session)\n        usr.update_stats(time_frame=UserStatsTimeFrame.week)\n    except Exception:\n        logger.exception(\"Error during user states update (weekly)\")\n\n\n@app.on_event(\"startup\")\n@repeat_every(seconds=60 * settings.USER_STATS_INTERVAL_MONTH, wait_first=False)\n@managed_tx_function(auto_commit=CommitMode.COMMIT)\ndef update_leader_board_month(session: Session) -> None:\n    try:\n        usr = UserStatsRepository(session)\n        usr.update_stats(time_frame=UserStatsTimeFrame.month)\n    except Exception:\n        logger.exception(\"Error during user states update (monthly)\")\n\n\n@app.on_event(\"startup\")\n@repeat_every(seconds=60 * settings.USER_STATS_INTERVAL_TOTAL, wait_first=False)\n@managed_tx_function(auto_commit=CommitMode.COMMIT)\ndef update_leader_board_total(session: Session) -> None:\n    try:\n        usr = UserStatsRepository(session)\n        usr.update_stats(time_frame=UserStatsTimeFrame.total)\n    except Exception:\n        logger.exception(\"Error during user states update (total)\")\n\n\n@app.on_event(\"startup\")\n@repeat_every(seconds=60 * 60)  # 1 hour\n@managed_tx_function(auto_commit=CommitMode.COMMIT)\ndef cronjob_delete_expired_tasks(session: Session) -> None:\n    delete_expired_tasks(session)\n    halt_prompts_of_disabled_users(session)\n\n\n@app.on_event(\"startup\")\n@repeat_every(seconds=60 * settings.CACHED_STATS_UPDATE_INTERVAL, wait_first=True)\n@managed_tx_function(auto_commit=CommitMode.COMMIT)\ndef update_cached_stats(session: Session) -> None:\n    try:\n        csr = CachedStatsRepository(session)\n        csr.update_all_cached_stats()\n    except Exception:\n        logger.exception(\"Error during cached stats update\")\n\n\napp.include_router(api_router, prefix=settings.API_V1_STR)\n\n\ndef get_openapi_schema():\n    return json.dumps(app.openapi())\n\n\ndef retry_scoring_failed_message_trees():\n    try:\n        logger.info(\"TreeManager.retry_scoring_failed_message_trees()\")\n        with Session(engine) as db:\n            api_client = api_auth(settings.OFFICIAL_WEB_API_KEY, db=db)\n\n            pr = PromptRepository(db=db, api_client=api_client)\n            tm = TreeManager(db, pr)\n            tm.retry_scoring_failed_message_trees()\n\n    except Exception:\n        logger.exception(\"TreeManager.retry_scoring_failed_message_trees() failed.\")\n\n\ndef main():\n    # Importing here so we don't import packages unnecessarily if we're\n    # importing main as a module.\n    import argparse\n\n    import uvicorn\n\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument(\n        \"--print-openapi-schema\",\n        default=False,\n        help=\"Dumps the openapi schema to stdout\",\n        action=\"store_true\",\n    )\n    parser.add_argument(\"--host\", help=\"The host to run the server\", default=\"0.0.0.0\")\n    parser.add_argument(\"--port\", help=\"The port to run the server\", default=8080)\n    parser.add_argument(\n        \"--retry-scoring\",\n        default=False,\n        help=\"Retry scoring failed message trees\",\n        action=\"store_true\",\n    )\n\n    args = parser.parse_args()\n\n    if args.print_openapi_schema:\n        print(get_openapi_schema())\n\n    if args.retry_scoring:\n        retry_scoring_failed_message_trees()\n\n    if not (args.print_openapi_schema or args.retry_scoring):\n        uvicorn.run(app, host=args.host, port=args.port)\n\n\nif __name__ == \"__main__\":\n    main()\n", "backend/import.py": "import argparse\nimport json\nimport sys\nfrom pathlib import Path\nfrom typing import Optional\nfrom uuid import UUID\n\nimport oasst_backend.models.db_payload as db_payload\nimport oasst_backend.utils.database_utils as db_utils\nimport pydantic\nfrom loguru import logger\nfrom oasst_backend.api.deps import create_api_client\nfrom oasst_backend.models import ApiClient, Message\nfrom oasst_backend.models.message_tree_state import MessageTreeState\nfrom oasst_backend.models.message_tree_state import State as TreeState\nfrom oasst_backend.models.payload_column_type import PayloadContainer\nfrom oasst_backend.prompt_repository import PromptRepository\nfrom oasst_backend.user_repository import UserRepository\nfrom oasst_data import ExportMessageNode, ExportMessageTree\nfrom sqlmodel import Session\n\n# well known id\nIMPORT_API_CLIENT_ID = UUID(\"bd8fde8b-1d8e-4e9a-9966-e96d000f8363\")\n\n\nclass Importer:\n    def __init__(self, db: Session, origin: str, model_name: Optional[str] = None):\n        self.db = db\n        self.origin = origin\n        self.model_name = model_name\n\n        # get import api client\n        api_client = db.query(ApiClient).filter(ApiClient.id == IMPORT_API_CLIENT_ID).first()\n        if not api_client:\n            api_client = create_api_client(\n                session=db,\n                description=\"API client used for importing data\",\n                frontend_type=\"import\",\n                force_id=IMPORT_API_CLIENT_ID,\n            )\n\n        ur = UserRepository(db, api_client)\n        self.import_user = ur.lookup_system_user(username=\"import\")\n        self.pr = PromptRepository(db=db, api_client=api_client, user_repository=ur)\n        self.api_client = api_client\n\n    def fetch_message(self, message_id: UUID) -> Message:\n        return self.db.query(Message).filter(Message.id == message_id).one_or_none()\n\n    def fetch_message_tree_state(self, message_tree_id: UUID) -> MessageTreeState:\n        return self.db.query(MessageTreeState).filter(MessageTreeState.message_tree_id == message_tree_id).one_or_none()\n\n    def import_message(\n        self, message: ExportMessageNode, message_tree_id: UUID, parent_id: Optional[UUID] = None\n    ) -> Message:\n        payload = db_payload.MessagePayload(text=message.text)\n        msg = Message(\n            id=message.message_id,\n            message_tree_id=message_tree_id,\n            frontend_message_id=message.message_id,\n            parent_id=parent_id,\n            review_count=message.review_count or 0,\n            lang=message.lang or \"en\",\n            review_result=True,\n            synthetic=message.synthetic if message.synthetic is not None else True,\n            model_name=message.model_name or self.model_name,\n            role=message.role,\n            api_client_id=self.api_client.id,\n            payload_type=type(payload).__name__,\n            payload=PayloadContainer(payload=payload),\n            user_id=self.import_user.id,\n        )\n        self.db.add(msg)\n        if message.replies:\n            for r in message.replies:\n                self.import_message(r, message_tree_id=message_tree_id, parent_id=msg.id)\n        self.db.flush()\n        if parent_id is None:\n            self.pr.update_children_counts(msg.id)\n        self.db.refresh(msg)\n        return msg\n\n    def import_tree(\n        self, tree: ExportMessageTree, state: TreeState = TreeState.BACKLOG_RANKING\n    ) -> tuple[MessageTreeState, Message]:\n        assert tree.message_tree_id is not None and tree.message_tree_id == tree.prompt.message_id\n        root_msg = self.import_message(tree.prompt, message_tree_id=tree.prompt.message_id)\n        assert state == TreeState.BACKLOG_RANKING or state == TreeState.RANKING, f\"{state} not supported for import\"\n        active = state == TreeState.RANKING\n        mts = MessageTreeState(\n            message_tree_id=root_msg.id,\n            goal_tree_size=0,\n            max_depth=0,\n            max_children_count=0,\n            state=state,\n            origin=self.origin,\n            active=active,\n            lang=root_msg.lang or \"en\",\n        )\n        self.db.add(mts)\n        return mts, root_msg\n\n\ndef import_file(\n    input_file_path: Path,\n    origin: str,\n    *,\n    model_name: Optional[str] = None,\n    num_activate: int = 0,\n    max_count: Optional[int] = None,\n    dry_run: bool = False,\n) -> int:\n    @db_utils.managed_tx_function(auto_commit=db_utils.CommitMode.ROLLBACK if dry_run else db_utils.CommitMode.COMMIT)\n    def import_tx(db: Session) -> int:\n        importer = Importer(db, origin=origin, model_name=model_name)\n        i = 0\n        with input_file_path.open() as file_in:\n            # read line tree object\n            for line in file_in:\n                dict_node = json.loads(line)\n\n                # validate data\n                if dict_node.get(\"message_tree_id\"):  # tree\n                    tree: ExportMessageTree = pydantic.parse_obj_as(ExportMessageTree, dict_node)\n                    existing_mts = importer.fetch_message_tree_state(tree.message_tree_id)\n                    if existing_mts:\n                        logger.info(f\"Skipping existing message tree: {tree.message_tree_id}\")\n                    else:\n                        state = TreeState.BACKLOG_RANKING if i >= num_activate else TreeState.RANKING\n                        mts, root_msg = importer.import_tree(tree, state=state)\n                        i += 1\n                        logger.info(\n                            f\"imported tree: {mts.message_tree_id}, {mts.state=}, {mts.active=}, {root_msg.children_count=}\"\n                        )\n\n                    if max_count and i >= max_count:\n                        logger.info(f\"Reached max count {max_count} of trees to import.\")\n                        break\n                elif dict_node.get(\"message_id\"):  # message\n                    message: ExportMessageNode = pydantic.parse_obj_as(ExportMessageNode, dict_node)\n                    existing_msg = importer.fetch_message(message.message_id)\n                    if existing_msg:\n                        logger.info(f\"Skipping existing message: {message.message_id}\")\n                    else:\n                        msg = importer.import_message(message, message_tree_id=message.message_id)\n                        i += 1\n                        logger.info(f\"imported message: {msg.id}\")\n        return i\n\n    if dry_run:\n        logger.info(\"DRY RUN with rollback\")\n    return import_tx()\n\n\ndef parse_args():\n    def str2bool(v):\n        if isinstance(v, bool):\n            return v\n        if v.lower() in (\"yes\", \"true\", \"t\", \"y\", \"1\"):\n            return True\n        elif v.lower() in (\"no\", \"false\", \"f\", \"n\", \"0\"):\n            return False\n        else:\n            raise argparse.ArgumentTypeError(\"Boolean value expected.\")\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"input_file_path\",\n        help=\"Input file path\",\n    )\n    parser.add_argument(\"--origin\", type=str, default=None, help=\"Value for origin of message trees\")\n    parser.add_argument(\"--model_name\", type=str, default=None, help=\"Default name of model (if missing in messages)\")\n    parser.add_argument(\"--num_activate\", type=int, default=0, help=\"Number of trees to add in ranking state\")\n    parser.add_argument(\"--max_count\", type=int, default=None, help=\"Maximum number of message trees to import\")\n    parser.add_argument(\"--dry_run\", type=str2bool, default=False)\n    args = parser.parse_args()\n    return args\n\n\ndef main():\n    args = parse_args()\n\n    input_file_path = Path(args.input_file_path)\n    if not input_file_path.exists() or not input_file_path.is_file():\n        print(\"Invalid input file:\", args.input_file_path)\n        sys.exit(1)\n\n    dry_run = args.dry_run\n    num_imported = import_file(\n        input_file_path,\n        origin=args.origin or input_file_path.name,\n        model_name=args.model_name,\n        num_activate=args.num_activate,\n        max_count=args.max_count,\n        dry_run=dry_run,\n    )\n\n    logger.info(f\"Done ({num_imported=}, {dry_run=})\")\n\n\nif __name__ == \"__main__\":\n    main()\n", "backend/export.py": "import argparse\nfrom pathlib import Path\nfrom typing import List, Optional\nfrom uuid import UUID\n\nimport sqlalchemy as sa\nfrom loguru import logger\nfrom oasst_backend.database import engine\nfrom oasst_backend.models import Message, MessageEmoji, MessageReaction, MessageTreeState, TextLabels, db_payload\nfrom oasst_backend.models.message_tree_state import State as TreeState\nfrom oasst_backend.utils import tree_export\nfrom oasst_data import (\n    ExportMessageEvent,\n    ExportMessageEventEmoji,\n    ExportMessageEventRanking,\n    ExportMessageEventRating,\n    ExportMessageTree,\n    LabelAvgValue,\n    LabelValues,\n)\nfrom oasst_shared.schemas.protocol import TextLabel\nfrom oasst_shared.utils import Anonymizer\nfrom sqlmodel import Session, func\n\n\ndef fetch_tree_ids(\n    db: Session,\n    state_filter: Optional[TreeState] = None,\n    lang: Optional[str] = None,\n    synthetic: Optional[bool] = None,\n    limit: Optional[int] = None,\n) -> list[tuple[UUID, TreeState]]:\n    tree_qry = (\n        db.query(MessageTreeState)\n        .select_from(MessageTreeState)\n        .join(Message, MessageTreeState.message_tree_id == Message.id)\n    )\n\n    if lang is not None:\n        tree_qry = tree_qry.filter(Message.lang == lang)\n\n    if state_filter:\n        tree_qry = tree_qry.filter(MessageTreeState.state == state_filter)\n\n    if synthetic is not None:\n        synth_exists_qry = (\n            db.query()\n            .filter(Message.message_tree_id == MessageTreeState.message_tree_id, Message.synthetic)\n            .exists()\n            .correlate(MessageTreeState)\n        )\n        if synthetic is False:\n            synth_exists_qry = ~synth_exists_qry\n        tree_qry = tree_qry.filter(synth_exists_qry)\n\n    if limit is not None:\n        tree_qry = tree_qry.limit(limit)\n\n    return [(tree.message_tree_id, tree.state) for tree in tree_qry]\n\n\ndef fetch_tree_messages(\n    db: Session,\n    message_tree_id: Optional[UUID] = None,\n    user_id: Optional[UUID] = None,\n    deleted: Optional[bool] = None,\n    synthetic: Optional[bool] = False,\n    prompts_only: bool = False,\n    lang: Optional[str] = None,\n    review_result: Optional[bool] = None,\n    limit: Optional[int] = None,\n) -> List[Message]:\n    qry = db.query(Message)\n\n    if message_tree_id:\n        qry = qry.filter(Message.message_tree_id == message_tree_id)\n    if user_id:\n        qry = qry.filter(Message.user_id == user_id)\n    if deleted is not None:\n        qry = qry.filter(Message.deleted == deleted)\n    if synthetic is not None:\n        qry = qry.filter(Message.synthetic == synthetic)\n    if prompts_only:\n        qry = qry.filter(Message.parent_id.is_(None))\n    if lang:\n        qry = qry.filter(Message.lang == lang)\n    if review_result is not None:\n        qry = qry.filter(Message.review_result == review_result)\n    if limit is not None:\n        qry = qry.limit(limit)\n\n    return qry.all()\n\n\ndef get_events_for_messages(db: Session, message_ids: list[UUID]) -> dict[UUID, ExportMessageEvent]:\n    events = {}\n    emojis = db.query(MessageEmoji).filter(MessageEmoji.message_id.in_(message_ids)).all()\n    for emoji in emojis:\n        event = ExportMessageEventEmoji(user_id=str(emoji.user_id), emoji=emoji.emoji)\n        events.setdefault(emoji.message_id, {}).setdefault(\"emoji\", []).append(event)\n    reactions: list[MessageReaction] = (\n        db.query(MessageReaction).filter(MessageReaction.message_id.in_(message_ids)).all()\n    )\n    for reaction in reactions:\n        match reaction.payload_type:\n            case \"RatingReactionPayload\":\n                key = \"rating\"\n                payload: db_payload.RatingReactionPayload = reaction.payload.payload\n                event = ExportMessageEventRating(user_id=str(reaction.user_id), rating=payload.rating)\n            case \"RankingReactionPayload\":\n                key = \"ranking\"\n                payload: db_payload.RankingReactionPayload = reaction.payload.payload\n                event = ExportMessageEventRanking(\n                    user_id=str(reaction.user_id),\n                    ranking=payload.ranking,\n                    ranked_message_ids=[str(id) for id in payload.ranked_message_ids],\n                    ranking_parent_id=str(payload.ranking_parent_id) if payload.ranking_parent_id else None,\n                    message_tree_id=str(payload.message_tree_id) if payload.message_tree_id else None,\n                    not_rankable=payload.not_rankable if payload.not_rankable else None,\n                )\n            case _:\n                raise ValueError(f\"Unknown payload type {reaction.payload_type}\")\n        events.setdefault(reaction.message_id, {}).setdefault(key, []).append(event)\n\n    return events\n\n\ndef fetch_tree_messages_and_avg_labels(\n    db: Session,\n    message_tree_id: Optional[UUID] = None,\n    user_id: Optional[UUID] = None,\n    deleted: Optional[bool] = None,\n    synthetic: Optional[bool] = False,\n    prompts_only: bool = False,\n    lang: Optional[str] = None,\n    review_result: Optional[bool] = None,\n    limit: Optional[int] = None,\n) -> List[Message]:\n    args = [Message]\n\n    for l in TextLabel:\n        args.append(func.avg(TextLabels.labels[l].cast(sa.Float)).label(l.value))\n        args.append(func.count(TextLabels.labels[l]).label(l.value + \"_count\"))\n\n    qry = db.query(*args).select_from(Message).outerjoin(TextLabels, Message.id == TextLabels.message_id)\n    if message_tree_id:\n        qry = qry.filter(Message.message_tree_id == message_tree_id)\n    if user_id:\n        qry = qry.filter(Message.user_id == user_id)\n    if deleted is not None:\n        qry = qry.filter(Message.deleted == deleted)\n    if synthetic is not None:\n        qry = qry.filter(Message.synthetic == synthetic)\n    if prompts_only:\n        qry = qry.filter(Message.parent_id.is_(None))\n    if lang:\n        qry = qry.filter(Message.lang == lang)\n    if review_result is not None:\n        qry = qry.filter(Message.review_result == review_result)\n\n    qry = qry.group_by(Message.id)\n\n    if limit is not None:\n        qry = qry.limit(limit)\n\n    return qry.all()\n\n\ndef export_trees(\n    db: Session,\n    export_file: Optional[Path] = None,\n    use_compression: bool = False,\n    deleted: Optional[bool] = False,\n    synthetic: Optional[bool] = False,\n    user_id: Optional[UUID] = None,\n    prompts_only: bool = False,\n    state_filter: Optional[TreeState] = None,\n    lang: Optional[str] = None,\n    review_result: Optional[bool] = None,\n    export_labels: bool = False,\n    export_events: bool = False,\n    limit: Optional[int] = None,\n    anonymizer_seed: Optional[str] = None,\n) -> None:\n    message_labels: dict[UUID, LabelValues] = {}\n    anonymizer = Anonymizer(anonymizer_seed) if anonymizer_seed else None\n    if user_id:\n        # when filtering by user we don't have complete message trees, export as list\n        result = fetch_tree_messages_and_avg_labels(\n            db,\n            user_id=user_id,\n            deleted=deleted,\n            synthetic=synthetic,\n            prompts_only=prompts_only,\n            lang=lang,\n            review_result=review_result,\n            limit=limit,\n        )\n\n        messages: list[Message] = []\n        for r in result:\n            msg = r[\"Message\"]\n            messages.append(msg)\n            if export_labels:\n                labels: LabelValues = {\n                    l.value: LabelAvgValue(value=r[l.value], count=r[l.value + \"_count\"])\n                    for l in TextLabel\n                    if r[l.value] is not None\n                }\n                message_labels[msg.id] = labels\n\n        events = {}\n        if export_events:\n            events = get_events_for_messages(db, [msg.id for msg in messages])\n\n        tree_export.write_messages_to_file(\n            export_file,\n            messages,\n            use_compression,\n            labels=message_labels,\n            anonymizer=anonymizer,\n            events=events,\n        )\n    else:\n        # tree export mode\n        message_tree_ids = fetch_tree_ids(db, state_filter, lang=lang, limit=limit, synthetic=synthetic)\n\n        message_trees: list[list[Message]] = []\n\n        for tree_id, _ in message_tree_ids:\n            if export_labels:\n                result = fetch_tree_messages_and_avg_labels(\n                    db,\n                    message_tree_id=tree_id,\n                    deleted=deleted,\n                    synthetic=None,  # pass None here (export trees, filtering happened in fetch_tree_ids)\n                    prompts_only=prompts_only,\n                    lang=None,  # pass None, trees were selected based on lang of prompt\n                    review_result=review_result,\n                )\n\n                messages: list[Message] = []\n                for r in result:\n                    msg = r[\"Message\"]\n                    messages.append(msg)\n                    labels: LabelValues = {\n                        l.value: LabelAvgValue(value=r[l.value], count=r[l.value + \"_count\"])\n                        for l in TextLabel\n                        if r[l.value] is not None\n                    }\n                    message_labels[msg.id] = labels\n\n                message_trees.append(messages)\n            else:\n                messages = fetch_tree_messages(\n                    db,\n                    message_tree_id=tree_id,\n                    deleted=deleted,\n                    synthetic=None,  # pass None here (export trees, filtering happened in fetch_tree_ids)\n                    prompts_only=prompts_only,\n                    lang=None,  # pass None here, trees were selected based on lang of prompt\n                    review_result=review_result,\n                )\n                message_trees.append(messages)\n\n        if review_result is False or deleted is True or synthetic is True:\n            # when exporting filtered we don't have complete message trees, export as list\n            messages = [m for t in message_trees for m in t]  # flatten message list\n            events = {}\n            if export_events:\n                events = get_events_for_messages(db, [msg.id for msg in messages])\n            tree_export.write_messages_to_file(\n                export_file,\n                messages,\n                use_compression,\n                labels=message_labels,\n                anonymizer=anonymizer,\n                events=events,\n            )\n        else:\n            trees_to_export: List[ExportMessageTree] = []\n\n            for (message_tree_id, message_tree_state), message_tree in zip(message_tree_ids, message_trees):\n                if len(message_tree) > 0:\n                    events = {}\n                    if export_events:\n                        events = get_events_for_messages(db, [msg.id for msg in message_tree])\n                    try:\n                        t = tree_export.build_export_tree(\n                            message_tree_id=message_tree_id,\n                            message_tree_state=message_tree_state,\n                            messages=message_tree,\n                            labels=message_labels,\n                            anonymizer=anonymizer,\n                            events=events,\n                        )\n                        if prompts_only:\n                            t.prompt.replies = None\n                        trees_to_export.append(t)\n                    except Exception as ex:\n                        logger.warning(f\"Corrupted tree: {message_tree_id} ({ex})\")\n\n            tree_export.write_trees_to_file(export_file, trees_to_export, use_compression)\n\n\ndef validate_args(args):\n    if args.deleted_only:\n        args.include_deleted = True\n\n    args.use_compression = args.export_file is not None and \".gz\" in args.export_file\n\n    if args.state and args.user is not None:\n        raise ValueError(\"Cannot use --state when specifying a user ID\")\n\n    if args.export_file is None:\n        logger.warning(\"No export file provided, output will be sent to STDOUT\")\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument(\n        \"--export-file\",\n        type=str,\n        help=\"Name of file to export trees to. If not provided, output will be sent to STDOUT\",\n    )\n    parser.add_argument(\n        \"--include-deleted\",\n        action=\"store_true\",\n        help=\"Include deleted messages in export\",\n    )\n    parser.add_argument(\n        \"--deleted-only\",\n        action=\"store_true\",\n        help=\"Export only deleted messages (implies --include-deleted)\",\n    )\n    parser.add_argument(\n        \"--include-spam\",\n        action=\"store_true\",\n        help=\"Export including messages with no review or negative review result.\",\n    )\n    parser.add_argument(\n        \"--spam-only\",\n        action=\"store_true\",\n        help=\"Export only messages with negative review result (implies --include-spam).\",\n    )\n    parser.add_argument(\n        \"--include-synthetic\",\n        action=\"store_true\",\n        help=\"Include synthetic messages in export\",\n    )\n    parser.add_argument(\n        \"--synthetic-only\",\n        action=\"store_true\",\n        help=\"Export only synthetic messages (implies --include-synth)\",\n    )\n    parser.add_argument(\n        \"--user\",\n        type=str,\n        help=\"Only export trees involving the user with the specified ID. Incompatible with --state.\",\n    )\n    parser.add_argument(\n        \"--state\",\n        type=str,\n        help=\"all|prompt_lottery_waiting|growing|ready_for_export|aborted_low_grade|halted_by_moderator|backlog_ranking\",\n    )\n    parser.add_argument(\n        \"--lang\",\n        type=str,\n        help=\"Filter message trees by language code (BCP 47)\",\n    )\n    parser.add_argument(\n        \"--prompts-only\",\n        action=\"store_true\",\n        help=\"Export a list of initial prompt messages\",\n    )\n    parser.add_argument(\n        \"--export-labels\",\n        action=\"store_true\",\n        help=\"Include average label values for messages\",\n    )\n    parser.add_argument(\n        \"--export-events\",\n        action=\"store_true\",\n        help=\"Include events for messages\",\n    )\n    parser.add_argument(\n        \"--limit\",\n        type=int,\n        help=\"Maximum number of trees to export. Leave at `None` to export all trees.\",\n    )\n    parser.add_argument(\n        \"--anonymizer-seed\",\n        type=int,\n        help=\"Seed for the anonymizer. If not specified, no anonymization will be performed.\",\n    )\n\n    args = parser.parse_args()\n    return args\n\n\ndef main():\n    args = parse_args()\n    validate_args(args)\n\n    state_filter: Optional[TreeState] = None\n    if args.state is None:\n        state_filter = TreeState.READY_FOR_EXPORT\n    elif args.state != \"all\":\n        state_filter = TreeState(args.state)\n\n    deleted: Optional[bool] = False\n    if args.include_deleted:\n        deleted = None\n    if args.deleted_only:\n        deleted = True\n\n    review_result: Optional[bool] = True\n    if args.include_spam:\n        review_result = None\n    if args.spam_only:\n        review_result = False\n\n    synthetic: Optional[bool] = False\n    if args.include_synthetic:\n        synthetic = None\n    if args.synthetic_only:\n        synthetic = True\n\n    if args.anonymizer_seed is None:\n        logger.warning(\"No anonymizer seed provided, no anonymization will be performed.\")\n\n    with Session(engine) as db:\n        export_trees(\n            db,\n            Path(args.export_file) if args.export_file is not None else None,\n            use_compression=args.use_compression,\n            deleted=deleted,\n            synthetic=synthetic,\n            user_id=UUID(args.user) if args.user is not None else None,\n            prompts_only=args.prompts_only,\n            state_filter=state_filter,\n            lang=args.lang,\n            review_result=review_result,\n            export_labels=args.export_labels,\n            export_events=args.export_events,\n            limit=args.limit,\n            anonymizer_seed=args.anonymizer_seed,\n        )\n\n\nif __name__ == \"__main__\":\n    main()\n", "backend/alembic/env.py": "from logging.config import fileConfig\n\nimport sqlmodel\nfrom alembic import context\nfrom oasst_backend import models  # noqa: F401\nfrom sqlalchemy import engine_from_config, pool\n\n# this is the Alembic Config object, which provides\n# access to the values within the .ini file in use.\nconfig = context.config\n\n# Interpret the config file for Python logging.\n# This line sets up loggers basically.\nif config.config_file_name is not None:\n    fileConfig(config.config_file_name)\n\n# add your model's MetaData object here\n# for 'autogenerate' support\n# from myapp import mymodel\n# target_metadata = mymodel.Base.metadata\ntarget_metadata = sqlmodel.SQLModel.metadata\n\n# other values from the config, defined by the needs of env.py,\n# can be acquired:\n# my_important_option = config.get_main_option(\"my_important_option\")\n# ... etc.\n\n\ndef run_migrations_offline() -> None:\n    \"\"\"Run migrations in 'offline' mode.\n\n    This configures the context with just a URL\n    and not an Engine, though an Engine is acceptable\n    here as well.  By skipping the Engine creation\n    we don't even need a DBAPI to be available.\n\n    Calls to context.execute() here emit the given string to the\n    script output.\n\n    \"\"\"\n    url = config.get_main_option(\"sqlalchemy.url\")\n    context.configure(\n        url=url,\n        target_metadata=target_metadata,\n        literal_binds=True,\n        dialect_opts={\"paramstyle\": \"named\"},\n    )\n\n    with context.begin_transaction():\n        context.run_migrations()\n\n\ndef run_migrations_online() -> None:\n    \"\"\"Run migrations in 'online' mode.\n\n    In this scenario we need to create an Engine\n    and associate a connection with the context.\n\n    \"\"\"\n    connectable = engine_from_config(\n        config.get_section(config.config_ini_section),\n        prefix=\"sqlalchemy.\",\n        poolclass=pool.NullPool,\n    )\n\n    with connectable.connect() as connection:\n        context.configure(connection=connection, target_metadata=target_metadata)\n\n        with context.begin_transaction():\n            context.get_context()._ensure_version_table()\n            connection.execute(\"LOCK TABLE alembic_version IN ACCESS EXCLUSIVE MODE\")\n            context.run_migrations()\n\n\nif context.is_offline_mode():\n    run_migrations_offline()\nelse:\n    run_migrations_online()\n", "backend/alembic/versions/2023_01_08_2208-92a367bb9f40_restructure_message_tree_state_table.py": "\"\"\"restructure message_tree_state table\n\nRevision ID: 92a367bb9f40\nRevises: ba61fe17fb6e\nCreate Date: 2023-01-08 22:08:46.458195\n\n\"\"\"\nimport sqlalchemy as sa\nimport sqlmodel\nfrom alembic import op\nfrom sqlalchemy.dialects import postgresql\n\n# revision identifiers, used by Alembic.\nrevision = \"92a367bb9f40\"\ndown_revision = \"aac6b2f66006\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_table(\"message_tree_state\")\n    op.create_table(\n        \"message_tree_state\",\n        sa.Column(\"message_tree_id\", postgresql.UUID(as_uuid=True), nullable=False),\n        sa.Column(\"goal_tree_size\", sa.Integer(), nullable=False),\n        sa.Column(\"max_depth\", sa.Integer(), nullable=False),\n        sa.Column(\"max_children_count\", sa.Integer(), nullable=False),\n        sa.Column(\"state\", sqlmodel.sql.sqltypes.AutoString(length=128), nullable=False),\n        sa.Column(\"active\", sa.Boolean(), nullable=False),\n        sa.Column(\"accepted_messages\", sa.Integer(), nullable=False),\n        sa.ForeignKeyConstraint(\n            [\"message_tree_id\"],\n            [\"message.id\"],\n        ),\n        sa.PrimaryKeyConstraint(\"message_tree_id\"),\n    )\n    op.create_index(op.f(\"ix_message_tree_state_active\"), \"message_tree_state\", [\"active\"], unique=False)\n    op.create_index(op.f(\"ix_message_tree_state_state\"), \"message_tree_state\", [\"state\"], unique=False)\n\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_index(op.f(\"ix_message_tree_state_state\"), table_name=\"message_tree_state\")\n    op.drop_index(op.f(\"ix_message_tree_state_active\"), table_name=\"message_tree_state\")\n    op.drop_table(\"message_tree_state\")\n    op.create_table(\n        \"message_tree_state\",\n        sa.Column(\"id\", postgresql.UUID(as_uuid=True), server_default=sa.text(\"gen_random_uuid()\"), nullable=False),\n        sa.Column(\"message_tree_id\", sqlmodel.sql.sqltypes.GUID(), nullable=False),\n        sa.Column(\"state\", sqlmodel.sql.sqltypes.AutoString(length=128), nullable=False),\n        sa.Column(\"goal_tree_size\", sa.Integer(), nullable=False),\n        sa.Column(\"current_num_non_filtered_messages\", sa.Integer(), nullable=False),\n        sa.Column(\"max_depth\", sa.Integer(), nullable=False),\n        sa.PrimaryKeyConstraint(\"id\"),\n    )\n    op.create_index(\n        op.f(\"ix_message_tree_state_message_tree_id\"), \"message_tree_state\", [\"message_tree_id\"], unique=False\n    )\n    op.create_index(\"ix_message_tree_state_tree_id\", \"message_tree_state\", [\"message_tree_id\"], unique=True)\n    # ### end Alembic commands ###\n", "backend/alembic/versions/2023_01_15_1654-0964ac95170d_add_rank_and_indices_to_user_stats.py": "\"\"\"add rank and indices to user_stats\n\nRevision ID: 0964ac95170d\nRevises: 423557e869e4\nCreate Date: 2023-01-15 16:54:09.510018\n\n\"\"\"\nimport sqlalchemy as sa\nfrom alembic import op\n\n# revision identifiers, used by Alembic.\nrevision = \"0964ac95170d\"\ndown_revision = \"423557e869e4\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.add_column(\"user_stats\", sa.Column(\"rank\", sa.Integer(), nullable=True))\n    op.create_index(\n        \"ix_user_stats__timeframe__rank__user_id\", \"user_stats\", [\"time_frame\", \"rank\", \"user_id\"], unique=True\n    )\n    op.create_index(\"ix_user_stats__timeframe__user_id\", \"user_stats\", [\"time_frame\", \"user_id\"], unique=True)\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_index(\"ix_user_stats__timeframe__user_id\", table_name=\"user_stats\")\n    op.drop_index(\"ix_user_stats__timeframe__rank__user_id\", table_name=\"user_stats\")\n    op.drop_column(\"user_stats\", \"rank\")\n    # ### end Alembic commands ###\n", "backend/alembic/versions/2023_02_11_1030-ba40d055714a_add_cached_stats.py": "\"\"\"add cached_stats\n\nRevision ID: ba40d055714a\nRevises: caee1e8ee0bc\nCreate Date: 2023-02-11 10:30:21.996198\n\n\"\"\"\nimport sqlalchemy as sa\nimport sqlmodel\nfrom alembic import op\nfrom sqlalchemy.dialects import postgresql\n\n# revision identifiers, used by Alembic.\nrevision = \"ba40d055714a\"\ndown_revision = \"caee1e8ee0bc\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.create_table(\n        \"cached_stats\",\n        sa.Column(\"name\", sqlmodel.sql.sqltypes.AutoString(length=128), nullable=False),\n        sa.Column(\n            \"modified_date\", sa.DateTime(timezone=True), server_default=sa.text(\"CURRENT_TIMESTAMP\"), nullable=False\n        ),\n        sa.Column(\"stats\", postgresql.JSONB(astext_type=sa.Text()), nullable=False),\n        sa.PrimaryKeyConstraint(\"name\"),\n    )\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_table(\"cached_stats\")\n    # ### end Alembic commands ###\n", "backend/alembic/versions/2023_02_14_1756-165b55de5a94_add_text_labels_message_id_index.py": "\"\"\"add text_labels message_id index\n\nRevision ID: 165b55de5a94\nRevises: ba40d055714a\nCreate Date: 2023-02-14 17:56:48.263684\n\n\"\"\"\nfrom alembic import op\n\n# revision identifiers, used by Alembic.\nrevision = \"165b55de5a94\"\ndown_revision = \"ba40d055714a\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.create_index(op.f(\"ix_text_labels_message_id\"), \"text_labels\", [\"message_id\"], unique=False)\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_index(op.f(\"ix_text_labels_message_id\"), table_name=\"text_labels\")\n    # ### end Alembic commands ###\n", "backend/alembic/versions/2022_12_30_0109-73ce3675c1f5_add_field_trusted_api_client.py": "\"\"\"add field trusted api client\n\nRevision ID: 73ce3675c1f5\nRevises: 464ec4667aae\nCreate Date: 2022-12-30 01:09:06.446020\n\n\"\"\"\nimport sqlalchemy as sa\nfrom alembic import op\n\n# revision identifiers, used by Alembic.\nrevision = \"73ce3675c1f5\"\ndown_revision = \"464ec4667aae\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.add_column(\"api_client\", sa.Column(\"trusted\", sa.Boolean(), server_default=sa.text(\"false\"), nullable=False))\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_column(\"api_client\", \"trusted\")\n    # ### end Alembic commands ###\n", "backend/alembic/versions/2023_02_07_1922-caee1e8ee0bc_added_new_table_for_flagged_messages.py": "\"\"\"Added new table for flagged messages\n\nRevision ID: caee1e8ee0bc\nRevises: 8c8241d1f973\nCreate Date: 2023-02-07 19:22:12.696257\n\n\"\"\"\nimport sqlalchemy as sa\nfrom alembic import op\nfrom sqlalchemy.dialects import postgresql\n\n# revision identifiers, used by Alembic.\nrevision = \"caee1e8ee0bc\"\ndown_revision = \"8c8241d1f973\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.create_table(\n        \"flagged_message\",\n        sa.Column(\"message_id\", postgresql.UUID(as_uuid=True), nullable=False),\n        sa.Column(\n            \"created_date\", sa.DateTime(timezone=True), server_default=sa.text(\"CURRENT_TIMESTAMP\"), nullable=False\n        ),\n        sa.Column(\"processed\", sa.Boolean(), nullable=False),\n        sa.ForeignKeyConstraint([\"message_id\"], [\"message.id\"], ondelete=\"CASCADE\"),\n        sa.PrimaryKeyConstraint(\"message_id\"),\n    )\n    op.create_index(op.f(\"ix_flagged_message_created_date\"), \"flagged_message\", [\"created_date\"], unique=False)\n    op.create_index(op.f(\"ix_flagged_message_processed\"), \"flagged_message\", [\"processed\"], unique=False)\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_index(op.f(\"ix_flagged_message_processed\"), table_name=\"flagged_message\")\n    op.drop_index(op.f(\"ix_flagged_message_created_date\"), table_name=\"flagged_message\")\n    op.drop_table(\"flagged_message\")\n    # ### end Alembic commands ###\n", "backend/alembic/versions/2023_01_27_2013-f856bf19d32b_add_user_show_on_leaderboard.py": "\"\"\"add user.show_on_leaderboard\n\nRevision ID: f856bf19d32b\nRevises: c84fcd6900dc\nCreate Date: 2023-01-27 20:13:56.533374\n\n\"\"\"\nimport sqlalchemy as sa\nfrom alembic import op\n\n# revision identifiers, used by Alembic.\nrevision = \"f856bf19d32b\"\ndown_revision = \"c84fcd6900dc\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.add_column(\n        \"user\", sa.Column(\"show_on_leaderboard\", sa.Boolean(), server_default=sa.text(\"true\"), nullable=False)\n    )\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_column(\"user\", \"show_on_leaderboard\")\n    # ### end Alembic commands ###\n", "backend/alembic/versions/2023_01_15_0002-7c98102efbca_change_user_stats_ranking_counts.py": "\"\"\"change user_stats ranking counts\n\nRevision ID: 7c98102efbca\nRevises: 619255ae9076\nCreate Date: 2023-01-15 00:02:45.622986\n\n\"\"\"\nimport sqlalchemy as sa\nimport sqlmodel\nfrom alembic import op\nfrom sqlalchemy.dialects.postgresql import UUID\n\n# revision identifiers, used by Alembic.\nrevision = \"7c98102efbca\"\ndown_revision = \"619255ae9076\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_table(\"user_stats\")\n    op.create_table(\n        \"user_stats\",\n        sa.Column(\"user_id\", UUID(as_uuid=True), nullable=False),\n        sa.Column(\"modified_date\", sa.DateTime(), server_default=sa.text(\"CURRENT_TIMESTAMP\"), nullable=False),\n        sa.Column(\"base_date\", sa.DateTime(), nullable=True),\n        sa.Column(\"time_frame\", sqlmodel.sql.sqltypes.AutoString(), nullable=False),\n        sa.Column(\"leader_score\", sa.Integer(), nullable=False),\n        sa.Column(\"prompts\", sa.Integer(), nullable=False),\n        sa.Column(\"replies_assistant\", sa.Integer(), nullable=False),\n        sa.Column(\"replies_prompter\", sa.Integer(), nullable=False),\n        sa.Column(\"labels_simple\", sa.Integer(), nullable=False),\n        sa.Column(\"labels_full\", sa.Integer(), nullable=False),\n        sa.Column(\"rankings_total\", sa.Integer(), nullable=False),\n        sa.Column(\"rankings_good\", sa.Integer(), nullable=False),\n        sa.Column(\"accepted_prompts\", sa.Integer(), nullable=False),\n        sa.Column(\"accepted_replies_assistant\", sa.Integer(), nullable=False),\n        sa.Column(\"accepted_replies_prompter\", sa.Integer(), nullable=False),\n        sa.Column(\"reply_ranked_1\", sa.Integer(), nullable=False),\n        sa.Column(\"reply_ranked_2\", sa.Integer(), nullable=False),\n        sa.Column(\"reply_ranked_3\", sa.Integer(), nullable=False),\n        sa.Column(\"streak_last_day_date\", sa.DateTime(), nullable=True),\n        sa.Column(\"streak_days\", sa.Integer(), nullable=True),\n        sa.ForeignKeyConstraint(\n            [\"user_id\"],\n            [\"user.id\"],\n        ),\n        sa.PrimaryKeyConstraint(\"user_id\", \"time_frame\"),\n    )\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.add_column(\n        \"user_stats\",\n        sa.Column(\"reply_prompter_ranked_3\", sa.INTEGER(), server_default=\"0\", autoincrement=False, nullable=False),\n    )\n    op.add_column(\n        \"user_stats\",\n        sa.Column(\"reply_assistant_ranked_1\", sa.INTEGER(), server_default=\"0\", autoincrement=False, nullable=False),\n    )\n    op.add_column(\n        \"user_stats\",\n        sa.Column(\"reply_assistant_ranked_2\", sa.INTEGER(), server_default=\"0\", autoincrement=False, nullable=False),\n    )\n    op.add_column(\n        \"user_stats\",\n        sa.Column(\"reply_prompter_ranked_2\", sa.INTEGER(), server_default=\"0\", autoincrement=False, nullable=False),\n    )\n    op.add_column(\n        \"user_stats\",\n        sa.Column(\"reply_prompter_ranked_1\", sa.INTEGER(), server_default=\"0\", autoincrement=False, nullable=False),\n    )\n    op.add_column(\n        \"user_stats\",\n        sa.Column(\"reply_assistant_ranked_3\", sa.INTEGER(), server_default=\"0\", autoincrement=False, nullable=False),\n    )\n    op.drop_column(\"user_stats\", \"reply_ranked_3\")\n    op.drop_column(\"user_stats\", \"reply_ranked_2\")\n    op.drop_column(\"user_stats\", \"reply_ranked_1\")\n    # ### end Alembic commands ###\n", "backend/alembic/versions/2022_12_25_1705-067c4002f2d9_add_text_labels.py": "\"\"\"Adds text labels table.\n\nRevision ID: 067c4002f2d9\nRevises: 0daec5f8135f\nCreate Date: 2022-12-25 17:05:21.208843\n\n\"\"\"\nimport sqlalchemy as sa\nimport sqlmodel\nfrom alembic import op\nfrom sqlalchemy.dialects import postgresql\n\n# revision identifiers, used by Alembic.\nrevision = \"067c4002f2d9\"\ndown_revision = \"0daec5f8135f\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.create_table(\n        \"text_labels\",\n        sa.Column(\"id\", postgresql.UUID(as_uuid=True), server_default=sa.text(\"gen_random_uuid()\"), nullable=False),\n        sa.Column(\"created_date\", sa.DateTime(), server_default=sa.text(\"CURRENT_TIMESTAMP\"), nullable=False),\n        sa.Column(\"post_id\", postgresql.UUID(as_uuid=True), nullable=True),\n        sa.Column(\"labels\", postgresql.JSONB(astext_type=sa.Text()), nullable=True),\n        sa.Column(\"api_client_id\", sqlmodel.sql.sqltypes.GUID(), nullable=False),\n        sa.Column(\"text\", sqlmodel.sql.sqltypes.AutoString(length=65536), nullable=False),\n        sa.ForeignKeyConstraint(\n            [\"api_client_id\"],\n            [\"api_client.id\"],\n        ),\n        sa.ForeignKeyConstraint(\n            [\"post_id\"],\n            [\"post.id\"],\n        ),\n        sa.PrimaryKeyConstraint(\"id\"),\n    )\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_table(\"text_labels\")\n    # ### end Alembic commands ###\n", "backend/alembic/versions/2023_01_29_1207-7b8f0011e0b0_move_user_streak_from_user_stats_to_.py": "\"\"\"move user_streak from user_stats to user table\n\nRevision ID: 7b8f0011e0b0\nRevises: 8a5feed819ee\nCreate Date: 2023-01-29 12:07:29.379326\n\n\"\"\"\nimport sqlalchemy as sa\nfrom alembic import op\n\n# revision identifiers, used by Alembic.\nrevision = \"7b8f0011e0b0\"\ndown_revision = \"49d8445b4c90\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.add_column(\n        \"user\",\n        sa.Column(\n            \"streak_last_day_date\",\n            sa.DateTime(timezone=True),\n            server_default=sa.text(\"CURRENT_TIMESTAMP\"),\n            autoincrement=False,\n            nullable=True,\n        ),\n    )\n    op.add_column(\"user\", sa.Column(\"streak_days\", sa.INTEGER(), autoincrement=False, nullable=True))\n    op.add_column(\n        \"user\", sa.Column(\"last_activity_date\", sa.DateTime(timezone=True), autoincrement=False, nullable=True)\n    )\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_column(\"user\", \"streak_days\")\n    op.drop_column(\"user\", \"streak_last_day_date\")\n    op.drop_column(\"user\", \"last_activity_date\")\n    # ### end Alembic commands ###\n", "backend/alembic/versions/2022_12_17_2230-6368515778c5_add_auth_method_to_person.py": "\"\"\"add auth_method to person\n\nRevision ID: 6368515778c5\nRevises: cd7de470586e\nCreate Date: 2022-12-17 17:57:33.022549\n\n\"\"\"\nimport sqlalchemy as sa\nfrom alembic import op\n\n# revision identifiers, used by Alembic.\nrevision = \"6368515778c5\"\ndown_revision = \"cd7de470586e\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.add_column(\"person\", sa.Column(\"auth_method\", sa.String(length=128), nullable=True))\n    op.execute(\"UPDATE person SET auth_method = 'local'\")\n    op.alter_column(\"person\", \"auth_method\", nullable=False)\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_column(\"person\", \"auth_method\")\n    # ### end Alembic commands ###\n", "backend/alembic/versions/2023_05_07_2129-1b6e3ae16e9d_add_text_search.py": "\"\"\"add text search\n\nRevision ID: 1b6e3ae16e9d\nRevises: 9db92d504f64\nCreate Date: 2023-05-07 21:29:35.545612\n\"\"\"\nimport sqlalchemy as sa\nfrom alembic import op\nfrom sqlalchemy.dialects import postgresql\n\n# revision identifiers, used by Alembic.\nrevision = \"1b6e3ae16e9d\"\ndown_revision = \"9db92d504f64\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    op.add_column(\"message\", sa.Column(\"search_vector\", postgresql.TSVECTOR(), nullable=True))\n    op.create_index(\"idx_search_vector\", \"message\", [\"search_vector\"], postgresql_using=\"gin\")\n\n\ndef downgrade() -> None:\n    op.drop_index(\"idx_search_vector\", \"message\")\n    op.drop_column(\"message\", \"search_vector\")\n", "backend/alembic/versions/2023_01_08_2200-bcc2fe18d214_messagetoxicity.py": "\"\"\"MessageToxicity\n\nRevision ID: bcc2fe18d214\nRevises: 20cd871f4ec7\nCreate Date: 2023-01-08 22:00:43.297719\n\n\"\"\"\nimport sqlalchemy as sa\nimport sqlmodel\nfrom alembic import op\nfrom sqlalchemy.dialects import postgresql\n\n# revision identifiers, used by Alembic.\nrevision = \"bcc2fe18d214\"\ndown_revision = \"846cc08ac79f\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.create_table(\n        \"message_toxicity\",\n        sa.Column(\"message_id\", postgresql.UUID(as_uuid=True), nullable=False),\n        sa.Column(\"toxicity\", sa.Float(), nullable=True),\n        sa.Column(\"created_date\", sa.DateTime(), server_default=sa.text(\"CURRENT_TIMESTAMP\"), nullable=False),\n        sa.Column(\"model\", sqlmodel.sql.sqltypes.AutoString(length=256), nullable=False),\n        sa.ForeignKeyConstraint(\n            [\"message_id\"],\n            [\"message.id\"],\n        ),\n        sa.PrimaryKeyConstraint(\"message_id\", \"model\"),\n    )\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_table(\"message_toxicity\")\n    # ### end Alembic commands ###\n", "backend/alembic/versions/2022_12_22_1835-0daec5f8135f_add_auth_method_to_ix_person_username.py": "\"\"\"add_auth_method_to_ix_person_username\n\nRevision ID: 0daec5f8135f\nRevises: 6368515778c5\nCreate Date: 2022-12-22 18:35:59.609013\n\n\"\"\"\nimport sqlalchemy as sa  # noqa: F401\nfrom alembic import op\n\n# revision identifiers, used by Alembic.\nrevision = \"0daec5f8135f\"\ndown_revision = \"6368515778c5\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_index(\"ix_person_username\", table_name=\"person\")\n    op.create_index(\"ix_person_username\", \"person\", [\"api_client_id\", \"username\", \"auth_method\"], unique=True)\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_index(\"ix_person_username\", table_name=\"person\")\n    op.create_index(\"ix_person_username\", \"person\", [\"api_client_id\", \"username\"], unique=False)\n    # ### end Alembic commands ###\n", "backend/alembic/versions/2023_01_26_1835-c84fcd6900dc_add_task_created_date_index.py": "\"\"\"add task created date index\n\nRevision ID: c84fcd6900dc\nRevises: 40ed93df0ed5\nCreate Date: 2023-01-26 18:35:43.061589\n\n\"\"\"\nfrom alembic import op\n\n# revision identifiers, used by Alembic.\nrevision = \"c84fcd6900dc\"\ndown_revision = \"40ed93df0ed5\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.create_index(op.f(\"ix_task_created_date\"), \"task\", [\"created_date\"], unique=False)\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_index(op.f(\"ix_task_created_date\"), table_name=\"task\")\n    # ### end Alembic commands ###\n", "backend/alembic/versions/2023_01_24_2256-40ed93df0ed5_add_message_emoji.py": "\"\"\"add message_emoji\n\nRevision ID: 40ed93df0ed5\nRevises: 8ba17b5f467a\nCreate Date: 2023-01-24 22:56:28.229408\n\n\"\"\"\nimport sqlalchemy as sa\nimport sqlmodel\nfrom alembic import op\nfrom sqlalchemy.dialects import postgresql\n\n# revision identifiers, used by Alembic.\nrevision = \"40ed93df0ed5\"\ndown_revision = \"8ba17b5f467a\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.create_table(\n        \"message_emoji\",\n        sa.Column(\"message_id\", postgresql.UUID(as_uuid=True), nullable=False),\n        sa.Column(\"user_id\", postgresql.UUID(as_uuid=True), nullable=False),\n        sa.Column(\n            \"created_date\", sa.DateTime(timezone=True), server_default=sa.text(\"CURRENT_TIMESTAMP\"), nullable=False\n        ),\n        sa.Column(\"emoji\", sqlmodel.sql.sqltypes.AutoString(length=128), nullable=False),\n        sa.ForeignKeyConstraint([\"message_id\"], [\"message.id\"], ondelete=\"CASCADE\"),\n        sa.ForeignKeyConstraint([\"user_id\"], [\"user.id\"], ondelete=\"CASCADE\"),\n        sa.PrimaryKeyConstraint(\"message_id\", \"user_id\", \"emoji\"),\n    )\n    op.create_index(\"ix_message_emoji__user_id__message_id\", \"message_emoji\", [\"user_id\", \"message_id\"], unique=False)\n    op.add_column(\"message\", sa.Column(\"emojis\", postgresql.JSONB(astext_type=sa.Text()), nullable=True))\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_column(\"message\", \"emojis\")\n    op.drop_index(\"ix_message_emoji__user_id__message_id\", table_name=\"message_emoji\")\n    op.drop_table(\"message_emoji\")\n    # ### end Alembic commands ###\n", "backend/alembic/versions/2022_12_28_1824-ef0b52902560_added_lang_column_for_iso_639_1_codes.py": "\"\"\"Added lang column for ISO-639-1 codes\n\nRevision ID: ef0b52902560\nRevises: d24b37426857\nCreate Date: 2022-12-28 18:24:21.393973\n\n\"\"\"\nimport sqlalchemy as sa\nimport sqlmodel\nfrom alembic import op\n\n# revision identifiers, used by Alembic.\nrevision = \"ef0b52902560\"\ndown_revision = \"d24b37426857\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.add_column(\n        \"post\", sa.Column(\"lang\", sqlmodel.sql.sqltypes.AutoString(length=200), nullable=False, default=\"en-US\")\n    )\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_column(\"post\", \"lang\")\n    # ### end Alembic commands ###\n", "backend/alembic/versions/2023_01_19_2153-7f0a28a156f4_switch_to_timestamp_with_tz.py": "\"\"\"switch to timestamp with tz\n\nRevision ID: 7f0a28a156f4\nRevises: 0964ac95170d\nCreate Date: 2023-01-19 21:53:01.107137\n\n\"\"\"\nimport sqlalchemy as sa\nfrom alembic import op\n\n# revision identifiers, used by Alembic.\nrevision = \"7f0a28a156f4\"\ndown_revision = \"0964ac95170d\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.alter_column(table_name=\"user_stats\", column_name=\"modified_date\", type_=sa.DateTime(timezone=True))\n    op.alter_column(table_name=\"user_stats\", column_name=\"base_date\", type_=sa.DateTime(timezone=True))\n    op.alter_column(table_name=\"journal_integration\", column_name=\"last_run\", type_=sa.DateTime(timezone=True))\n    op.alter_column(table_name=\"message_embedding\", column_name=\"created_date\", type_=sa.DateTime(timezone=True))\n    op.alter_column(table_name=\"message_reaction\", column_name=\"created_date\", type_=sa.DateTime(timezone=True))\n    op.alter_column(table_name=\"message_toxicity\", column_name=\"created_date\", type_=sa.DateTime(timezone=True))\n    op.alter_column(table_name=\"message\", column_name=\"created_date\", type_=sa.DateTime(timezone=True))\n    op.alter_column(table_name=\"task\", column_name=\"created_date\", type_=sa.DateTime(timezone=True))\n    op.alter_column(table_name=\"task\", column_name=\"expiry_date\", type_=sa.DateTime(timezone=True))\n    op.alter_column(table_name=\"text_labels\", column_name=\"created_date\", type_=sa.DateTime(timezone=True))\n    op.alter_column(table_name=\"user\", column_name=\"created_date\", type_=sa.DateTime(timezone=True))\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.alter_column(table_name=\"user_stats\", column_name=\"modified_date\", type_=sa.DateTime(timezone=False))\n    op.alter_column(table_name=\"user_stats\", column_name=\"base_date\", type_=sa.DateTime(timezone=False))\n    op.alter_column(table_name=\"journal_integration\", column_name=\"last_run\", type_=sa.DateTime(timezone=False))\n    op.alter_column(table_name=\"message_embedding\", column_name=\"created_date\", type_=sa.DateTime(timezone=False))\n    op.alter_column(table_name=\"message_reaction\", column_name=\"created_date\", type_=sa.DateTime(timezone=False))\n    op.alter_column(table_name=\"message_toxicity\", column_name=\"created_date\", type_=sa.DateTime(timezone=False))\n    op.alter_column(table_name=\"message\", column_name=\"created_date\", type_=sa.DateTime(timezone=False))\n    op.alter_column(table_name=\"task\", column_name=\"created_date\", type_=sa.DateTime(timezone=False))\n    op.alter_column(table_name=\"task\", column_name=\"expiry_date\", type_=sa.DateTime(timezone=False))\n    op.alter_column(table_name=\"text_labels\", column_name=\"created_date\", type_=sa.DateTime(timezone=False))\n    op.alter_column(table_name=\"user\", column_name=\"created_date\", type_=sa.DateTime(timezone=False))\n    # ### end Alembic commands ###\n", "backend/alembic/versions/2023_01_08_2128-aac6b2f66006_created_date.py": "\"\"\"Created date\n\nRevision ID: aac6b2f66006\nRevises: 35bdc1a08bb8\nCreate Date: 2023-01-08 21:28:27.342729\n\n\"\"\"\nimport sqlalchemy as sa\nfrom alembic import op\n\n# revision identifiers, used by Alembic.\nrevision = \"aac6b2f66006\"\ndown_revision = \"35bdc1a08bb8\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.add_column(\n        \"message_embedding\",\n        sa.Column(\"created_date\", sa.DateTime(), server_default=sa.text(\"CURRENT_TIMESTAMP\"), nullable=False),\n    )\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_column(\"message_embedding\", \"created_date\")\n    # ### end Alembic commands ###\n", "backend/alembic/versions/2023_01_08_1603-35bdc1a08bb8_embedding_for_message_now_in_its_own_.py": "\"\"\"embedding for message now in its own table\n\nRevision ID: 35bdc1a08bb8\nRevises: 023548d474f7\nCreate Date: 2023-01-08 16:03:48.454207\n\n\"\"\"\nimport sqlalchemy as sa\nimport sqlmodel\nfrom alembic import op\nfrom sqlalchemy.dialects import postgresql\n\n# revision identifiers, used by Alembic.\nrevision = \"35bdc1a08bb8\"\ndown_revision = \"023548d474f7\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.create_table(\n        \"message_embedding\",\n        sa.Column(\"message_id\", postgresql.UUID(as_uuid=True), nullable=False),\n        sa.Column(\"embedding\", sa.ARRAY(sa.Float()), nullable=True),\n        sa.Column(\"model\", sqlmodel.sql.sqltypes.AutoString(length=256), nullable=False),\n        sa.ForeignKeyConstraint(\n            [\"message_id\"],\n            [\"message.id\"],\n        ),\n        sa.PrimaryKeyConstraint(\"message_id\", \"model\"),\n    )\n    op.drop_column(\"message\", \"miniLM_embedding\")\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.add_column(\n        \"message\",\n        sa.Column(\n            \"miniLM_embedding\",\n            postgresql.ARRAY(postgresql.DOUBLE_PRECISION(precision=53)),\n            autoincrement=False,\n            nullable=True,\n        ),\n    )\n    op.drop_table(\"message_embedding\")\n    # ### end Alembic commands ###\n", "backend/alembic/versions/2022_12_29_2103-464ec4667aae_add_collective_flag_to_task.py": "\"\"\"add collective flag to task\n\nRevision ID: 464ec4667aae\nRevises: ef0b52902560\nCreate Date: 2022-12-29 21:03:06.841962\n\n\"\"\"\nimport sqlalchemy as sa\nfrom alembic import op\n\n# revision identifiers, used by Alembic.\nrevision = \"464ec4667aae\"\ndown_revision = \"ef0b52902560\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.add_column(\n        \"work_package\", sa.Column(\"collective\", sa.Boolean(), server_default=sa.text(\"false\"), nullable=False)\n    )\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_column(\"work_package\", \"collective\")\n    # ### end Alembic commands ###\n", "backend/alembic/versions/2023_02_26_0052-9db92d504f64_add_lang_to_message_tree_state.py": "\"\"\"add lang to message_tree_state\n\nRevision ID: 9db92d504f64\nRevises: 8cd0c34d0c3c\nCreate Date: 2023-02-26 00:52:40.624843\n\n\"\"\"\nimport sqlalchemy as sa\nfrom alembic import op\n\n# revision identifiers, used by Alembic.\nrevision = \"9db92d504f64\"\ndown_revision = \"8cd0c34d0c3c\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.add_column(\"message_tree_state\", sa.Column(\"lang\", sa.String(length=32), nullable=True))\n    op.execute(\n        \"WITH msg AS (SELECT id, lang FROM message WHERE parent_id is NULL) UPDATE message_tree_state mts SET lang = msg.lang FROM msg WHERE mts.message_tree_id = msg.id\"\n    )\n    op.alter_column(\"message_tree_state\", \"lang\", nullable=False)\n    op.drop_index(\"ix_message_tree_state_state\", table_name=\"message_tree_state\")\n    op.create_index(\"ix_message_tree_state__lang__state\", \"message_tree_state\", [\"state\", \"lang\"], unique=False)\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_index(\"ix_message_tree_state__lang__state\", table_name=\"message_tree_state\")\n    op.create_index(\"ix_message_tree_state_state\", \"message_tree_state\", [\"state\"], unique=False)\n    op.drop_column(\"message_tree_state\", \"lang\")\n    # ### end Alembic commands ###\n", "backend/alembic/versions/2023_02_02_1817-8c8241d1f973_add_account_table.py": "\"\"\"Add Account table\n\nRevision ID: 8c8241d1f973\nRevises: 4d7e0b0ebe84\nCreate Date: 2023-01-30 15:10:58.776315\n\n\"\"\"\nimport sqlalchemy as sa\nimport sqlmodel\nfrom alembic import op\nfrom sqlalchemy.dialects import postgresql\n\n# revision identifiers, used by Alembic.\nrevision = \"8c8241d1f973\"\ndown_revision = \"4d7e0b0ebe84\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.create_table(\n        \"account\",\n        sa.Column(\"id\", postgresql.UUID(as_uuid=True), server_default=sa.text(\"gen_random_uuid()\"), nullable=False),\n        sa.Column(\"user_id\", sqlmodel.sql.sqltypes.GUID(), nullable=False),\n        sa.Column(\"provider\", sqlmodel.sql.sqltypes.AutoString(length=128), nullable=False),\n        sa.Column(\"provider_account_id\", sqlmodel.sql.sqltypes.AutoString(length=128), nullable=False),\n        sa.ForeignKeyConstraint([\"user_id\"], [\"user.id\"]),\n        sa.PrimaryKeyConstraint(\"id\"),\n    )\n    op.create_index(\"provider\", \"account\", [\"provider_account_id\"], unique=True)\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_index(\"provider\", table_name=\"account\")\n    op.drop_table(\"account\")\n    # ### end Alembic commands ###\n", "backend/alembic/versions/2023_01_20_1650-160ac010efcc_use_en_instead_en_us_as_default_lang.py": "\"\"\"use 'en' instead 'en-US' as default lang\n\nRevision ID: 160ac010efcc\nRevises: 4f26fec4d204\nCreate Date: 2023-01-20 16:50:00\n\n\"\"\"\nimport sqlalchemy as sa\nfrom alembic import op\n\n# revision identifiers, used by Alembic.\nrevision = \"160ac010efcc\"\ndown_revision = \"4f26fec4d204\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_column(\"message\", \"lang\")\n    op.add_column(\"message\", sa.Column(\"lang\", sa.String(length=32), server_default=\"en\", nullable=False))\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_column(\"message\", \"lang\")\n    op.add_column(\"message\", sa.Column(\"lang\", sa.VARCHAR(length=200), autoincrement=False, nullable=False))\n    # ### end Alembic commands ###\n", "backend/alembic/versions/2022_12_27_1444-3358eb6834e6_add_journal_table.py": "\"\"\"add_journal_table\n\nRevision ID: 3358eb6834e6\nRevises: 067c4002f2d9\nCreate Date: 2022-12-27 14:44:59.483868\n\n\"\"\"\nimport sqlalchemy as sa\nimport sqlmodel\nfrom alembic import op\nfrom sqlalchemy.dialects import postgresql\n\n# revision identifiers, used by Alembic.\nrevision = \"3358eb6834e6\"\ndown_revision = \"067c4002f2d9\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.create_table(\n        \"journal\",\n        sa.Column(\"id\", postgresql.UUID(as_uuid=True), nullable=False),\n        sa.Column(\n            \"created_date\", sa.DateTime(timezone=True), server_default=sa.text(\"CURRENT_TIMESTAMP\"), nullable=False\n        ),\n        sa.Column(\n            \"event_payload\",\n            postgresql.JSONB(astext_type=sa.Text()),\n            nullable=False,\n        ),\n        sa.Column(\"person_id\", sqlmodel.sql.sqltypes.GUID(), nullable=True),\n        sa.Column(\"post_id\", sqlmodel.sql.sqltypes.GUID(), nullable=True),\n        sa.Column(\"api_client_id\", sqlmodel.sql.sqltypes.GUID(), nullable=False),\n        sa.Column(\"event_type\", sqlmodel.sql.sqltypes.AutoString(length=200), nullable=False),\n        sa.ForeignKeyConstraint(\n            [\"api_client_id\"],\n            [\"api_client.id\"],\n        ),\n        sa.ForeignKeyConstraint(\n            [\"person_id\"],\n            [\"person.id\"],\n        ),\n        sa.ForeignKeyConstraint(\n            [\"post_id\"],\n            [\"post.id\"],\n        ),\n        sa.PrimaryKeyConstraint(\"id\"),\n    )\n    op.create_index(op.f(\"ix_journal_person_id\"), \"journal\", [\"person_id\"], unique=False)\n    op.create_table(\n        \"journal_integration\",\n        sa.Column(\"id\", postgresql.UUID(as_uuid=True), server_default=sa.text(\"gen_random_uuid()\"), nullable=False),\n        sa.Column(\"last_run\", sa.DateTime(), nullable=True),\n        sa.Column(\"description\", sqlmodel.sql.sqltypes.AutoString(length=512), nullable=False),\n        sa.Column(\"last_journal_id\", sqlmodel.sql.sqltypes.GUID(), nullable=True),\n        sa.Column(\"last_error\", sqlmodel.sql.sqltypes.AutoString(), nullable=True),\n        sa.Column(\"next_run\", sa.DateTime(), nullable=True),\n        sa.ForeignKeyConstraint(\n            [\"last_journal_id\"],\n            [\"journal.id\"],\n        ),\n        sa.PrimaryKeyConstraint(\"id\", \"description\"),\n    )\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_table(\"journal_integration\")\n    op.drop_index(op.f(\"ix_journal_person_id\"), table_name=\"journal\")\n    op.drop_table(\"journal\")\n    # ### end Alembic commands ###\n", "backend/alembic/versions/2023_01_10_1733-846cc08ac79f_add_enabled_deleted_notes_fields_to_user.py": "\"\"\"Add enabled, deleted, notes fields to User\n\nRevision ID: 846cc08ac79f\nRevises: aac6b2f66006\nCreate Date: 2023-01-10 17:33:07.104596\n\n\"\"\"\nimport sqlalchemy as sa\nimport sqlmodel\nfrom alembic import op\n\n# revision identifiers, used by Alembic.\nrevision = \"846cc08ac79f\"\ndown_revision = \"befa42582ea4\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.add_column(\"user\", sa.Column(\"enabled\", sa.Boolean(), server_default=sa.text(\"true\"), nullable=False))\n    op.add_column(\"user\", sa.Column(\"deleted\", sa.Boolean(), server_default=sa.text(\"false\"), nullable=False))\n    op.add_column(\n        \"user\",\n        sa.Column(\"notes\", sqlmodel.sql.sqltypes.AutoString(length=1024), server_default=\"\", nullable=False),\n    )\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_column(\"user\", \"notes\")\n    op.drop_column(\"user\", \"deleted\")\n    op.drop_column(\"user\", \"enabled\")\n    # ### end Alembic commands ###\n", "backend/alembic/versions/2023_01_09_0047-05975b274a81_add_review_count_ranking_count_to_.py": "\"\"\"add review_count & ranking_count to message\n\nRevision ID: 05975b274a81\nRevises: 92a367bb9f40\nCreate Date: 2023-01-09 00:47:25.496036\n\n\"\"\"\nimport sqlalchemy as sa\nfrom alembic import op\n\n# revision identifiers, used by Alembic.\nrevision = \"05975b274a81\"\ndown_revision = \"92a367bb9f40\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.add_column(\"message\", sa.Column(\"review_count\", sa.Integer(), server_default=sa.text(\"0\"), nullable=False))\n    op.add_column(\"message\", sa.Column(\"review_result\", sa.Boolean(), server_default=sa.text(\"false\"), nullable=False))\n    op.add_column(\"message\", sa.Column(\"ranking_count\", sa.Integer(), server_default=sa.text(\"0\"), nullable=False))\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_column(\"message\", \"ranking_count\")\n    op.drop_column(\"message\", \"review_result\")\n    op.drop_column(\"message\", \"review_count\")\n    # ### end Alembic commands ###\n", "backend/alembic/versions/2023_01_08_1106-3d96bb92e33a_added_minilm_embedding_column_to_message.py": "\"\"\"added miniLM_embedding column to message\n\nRevision ID: 023548d474f7\nRevises: ba61fe17fb6e\nCreate Date: 2023-01-08 11:06:25.613290\n\n\"\"\"\nimport sqlalchemy as sa\nfrom alembic import op\n\n# revision identifiers, used by Alembic.\nrevision = \"023548d474f7\"\ndown_revision = \"ba61fe17fb6e\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.add_column(\"message\", sa.Column(\"miniLM_embedding\", sa.ARRAY(sa.Float()), nullable=True))\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_column(\"message\", \"miniLM_embedding\")\n    # ### end Alembic commands ###\n", "backend/alembic/versions/2022_12_31_0438-8d269bc4fdbd_add_deleted_field_to_post.py": "\"\"\"add deleted field to post\n\nRevision ID: 8d269bc4fdbd\nRevises: abb47e9d145a\nCreate Date: 2022-12-31 04:38:41.799206\n\n\"\"\"\nimport sqlalchemy as sa\nfrom alembic import op\n\n# revision identifiers, used by Alembic.\nrevision = \"8d269bc4fdbd\"\ndown_revision = \"abb47e9d145a\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.add_column(\"message\", sa.Column(\"deleted\", sa.Boolean(), server_default=sa.text(\"false\"), nullable=False))\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_column(\"message\", \"deleted\")\n    # ### end Alembic commands ###\n", "backend/alembic/versions/2023_02_15_1754-8cd0c34d0c3c_message_review_result_nullable.py": "\"\"\"message review_result nullable\n\nRevision ID: 8cd0c34d0c3c\nRevises: 165b55de5a94\nCreate Date: 2023-02-15 17:54:58.029278\n\n\"\"\"\nimport sqlalchemy as sa\nfrom alembic import op\n\n# revision identifiers, used by Alembic.\nrevision = \"8cd0c34d0c3c\"\ndown_revision = \"165b55de5a94\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.alter_column(\n        \"message\",\n        \"review_result\",\n        existing_type=sa.BOOLEAN(),\n        nullable=True,\n        server_default=None,\n        existing_server_default=sa.text(\"false\"),\n    )\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.alter_column(\n        \"message\", \"review_result\", existing_type=sa.BOOLEAN(), nullable=False, server_default=sa.text(\"false\")\n    )\n    # ### end Alembic commands ###\n", "backend/alembic/versions/2023_01_12_0119-befa42582ea4_remove_accepted_messages_from_message_.py": "\"\"\"remove accepted_messages from message_tree_state\n\nRevision ID: befa42582ea4\nRevises: 05975b274a81\nCreate Date: 2023-01-12 01:19:59.654864\n\n\"\"\"\nimport sqlalchemy as sa\nfrom alembic import op\n\n# revision identifiers, used by Alembic.\nrevision = \"befa42582ea4\"\ndown_revision = \"05975b274a81\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_column(\"message_tree_state\", \"accepted_messages\")\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.add_column(\n        \"message_tree_state\", sa.Column(\"accepted_messages\", sa.INTEGER(), autoincrement=False, nullable=False)\n    )\n    # ### end Alembic commands ###\n", "backend/alembic/versions/2023_02_01_2146-9e7ec4a9e3f2_add_skip_bool_skip_reason_to_task.py": "\"\"\"add skip bool & skip_reason to task\n\nRevision ID: 9e7ec4a9e3f2\nRevises: 7b8f0011e0b0\nCreate Date: 2023-02-01 21:46:49.971052\n\n\"\"\"\nimport sqlalchemy as sa\nimport sqlmodel\nfrom alembic import op\n\n# revision identifiers, used by Alembic.\nrevision = \"9e7ec4a9e3f2\"\ndown_revision = \"55361f323d12\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.add_column(\"task\", sa.Column(\"skipped\", sa.Boolean(), server_default=sa.text(\"false\"), nullable=False))\n    op.add_column(\"task\", sa.Column(\"skip_reason\", sqlmodel.sql.sqltypes.AutoString(length=512), nullable=True))\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_column(\"task\", \"skip_reason\")\n    op.drop_column(\"task\", \"skipped\")\n    # ### end Alembic commands ###\n", "backend/alembic/versions/2023_01_05_1346-3b0adfadbef9_removed_date_created_and_deleted_flag_.py": "\"\"\"removed date_created and deleted flag from message_tree_state\n\nRevision ID: 3b0adfadbef9\nRevises: d4161e384f83\nCreate Date: 2023-01-05 13:46:11.338655\n\n\"\"\"\nimport sqlalchemy as sa\nfrom alembic import op\nfrom sqlalchemy.dialects import postgresql\n\n# revision identifiers, used by Alembic.\nrevision = \"3b0adfadbef9\"\ndown_revision = \"d4161e384f83\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_column(\"message_tree_state\", \"deleted\")\n    op.drop_column(\"message_tree_state\", \"created_date\")\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.add_column(\n        \"message_tree_state\",\n        sa.Column(\n            \"created_date\",\n            postgresql.TIMESTAMP(),\n            server_default=sa.text(\"CURRENT_TIMESTAMP\"),\n            autoincrement=False,\n            nullable=False,\n        ),\n    )\n    op.add_column(\n        \"message_tree_state\",\n        sa.Column(\"deleted\", sa.BOOLEAN(), server_default=sa.text(\"false\"), autoincrement=False, nullable=False),\n    )\n    # ### end Alembic commands ###\n", "backend/alembic/versions/2023_01_19_2200-4f26fec4d204_add_ix_user_display_name_id.py": "\"\"\"add ix_user_display_name_id\n\nRevision ID: 4f26fec4d204\nRevises: 0964ac95170d\nCreate Date: 2023-01-19 22:00:00\n\n\"\"\"\nfrom alembic import op\n\n# revision identifiers, used by Alembic.\nrevision = \"4f26fec4d204\"\ndown_revision = \"7f0a28a156f4\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.create_index(\"ix_user_display_name_id\", \"user\", [\"display_name\", \"id\"], unique=True)\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_index(\"ix_user_display_name_id\", table_name=\"user\")\n    # ### end Alembic commands ###\n", "backend/alembic/versions/2023_01_24_1134-8ba17b5f467a_add_message_id_to_message_reaction.py": "\"\"\"add message_id to message_reaction\n\nRevision ID: 8ba17b5f467a\nRevises: 160ac010efcc\nCreate Date: 2023-01-24 11:34:42.167575\n\n\"\"\"\nimport sqlalchemy as sa\nimport sqlmodel\nfrom alembic import op\n\n# revision identifiers, used by Alembic.\nrevision = \"8ba17b5f467a\"\ndown_revision = \"160ac010efcc\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.add_column(\"message_reaction\", sa.Column(\"message_id\", sqlmodel.sql.sqltypes.GUID(), nullable=True))\n    op.create_index(op.f(\"ix_message_reaction_message_id\"), \"message_reaction\", [\"message_id\"], unique=False)\n    op.add_column(\"text_labels\", sa.Column(\"task_id\", sqlmodel.sql.sqltypes.GUID(), nullable=True))\n    op.create_index(op.f(\"ix_text_labels_task_id\"), \"text_labels\", [\"task_id\"], unique=False)\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_index(op.f(\"ix_text_labels_task_id\"), table_name=\"text_labels\")\n    op.drop_column(\"text_labels\", \"task_id\")\n    op.drop_index(op.f(\"ix_message_reaction_message_id\"), table_name=\"message_reaction\")\n    op.drop_column(\"message_reaction\", \"message_id\")\n    # ### end Alembic commands ###\n", "backend/alembic/versions/2022_12_15_0000-23e5fea252dd_first_revision.py": "\"\"\"first revision\n\nRevision ID: 23e5fea252dd\nRevises:\nCreate Date: 2022-12-12 12:47:28.801354\n\n\"\"\"\nimport sqlalchemy as sa\nfrom alembic import op\n\n# revision identifiers, used by Alembic.\nrevision = \"23e5fea252dd\"\ndown_revision = None\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    op.create_table(\n        \"service_client\",\n        sa.Column(\"id\", sa.Integer, sa.Identity()),\n        sa.Column(\"name\", sa.String(200), nullable=False),\n        sa.Column(\"service_admin_email\", sa.String(128), nullable=True),\n        sa.Column(\"api_key\", sa.String(300), nullable=False),\n        sa.Column(\"can_append\", sa.Boolean, nullable=False, server_default=\"true\"),\n        sa.Column(\"can_write\", sa.Boolean, nullable=False, server_default=\"false\"),\n        sa.Column(\"can_delete\", sa.Boolean, nullable=False, server_default=\"false\"),\n        sa.Column(\"can_read\", sa.Boolean, nullable=False, server_default=\"true\"),\n        sa.PrimaryKeyConstraint(\"id\"),\n    )\n    op.create_index(op.f(\"ix_service_client_api_key\"), \"service_client\", [\"api_key\"], unique=True)\n\n    op.create_table(\n        \"labeler\",\n        sa.Column(\"id\", sa.Integer, sa.Identity()),\n        sa.Column(\"display_name\", sa.String(96), nullable=False),\n        sa.Column(\"discord_username\", sa.String(96), nullable=True),\n        sa.Column(\n            \"created_date\",\n            sa.DateTime,\n            nullable=False,\n            server_default=sa.func.current_timestamp(),\n        ),\n        sa.Column(\"is_enabled\", sa.Boolean, nullable=False, server_default=\"true\"),\n        sa.Column(\"notes\", sa.String(10 * 1024), nullable=True),\n        sa.PrimaryKeyConstraint(\"id\"),\n        sa.UniqueConstraint(\"discord_username\"),\n    )\n\n    op.create_table(\n        \"prompt\",\n        sa.Column(\"id\", sa.Integer, sa.Identity()),\n        sa.Column(\"labeler_id\", sa.Integer, nullable=False),\n        sa.Column(\"prompt\", sa.Text, nullable=False),\n        sa.Column(\"response\", sa.Text, nullable=True),\n        sa.Column(\"lang\", sa.String(32), nullable=True),\n        sa.Column(\n            \"created_date\",\n            sa.DateTime(),\n            nullable=False,\n            server_default=sa.func.current_timestamp(),\n        ),\n        sa.ForeignKeyConstraint(\n            [\"labeler_id\"],\n            [\"labeler.id\"],\n        ),\n        sa.PrimaryKeyConstraint(\"id\"),\n    )\n    op.create_index(op.f(\"prompt_labeler_id\"), \"prompt\", [\"labeler_id\"], unique=False)\n\n\ndef downgrade() -> None:\n    op.drop_index(op.f(\"prompt_labeler_id\"), table_name=\"prompt\")\n    op.drop_table(\"prompt\")\n\n    op.drop_table(\"labeler\")\n\n    op.drop_index(op.f(\"ix_service_client_api_key\"), table_name=\"service_client\")\n    op.drop_table(\"service_client\")\n", "backend/alembic/versions/2022_12_16_0000-cd7de470586e_v1_db_structure.py": "\"\"\"v1 db structure\n\nRevision ID: cd7de470586e\nRevises: 23e5fea252dd\nCreate Date: 2022-12-15 11:15:32.830225\n\n\"\"\"\nimport uuid\n\nimport sqlalchemy as sa\nfrom alembic import op\nfrom sqlalchemy.dialects.postgresql import JSONB, UUID\n\n# revision identifiers, used by Alembic.\nrevision = \"cd7de470586e\"\ndown_revision = \"23e5fea252dd\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    # remove database objects\n    op.drop_index(op.f(\"prompt_labeler_id\"), table_name=\"prompt\")\n    op.drop_table(\"prompt\")\n    op.drop_table(\"labeler\")\n    op.drop_index(op.f(\"ix_service_client_api_key\"), table_name=\"service_client\")\n    op.drop_table(\"service_client\")\n\n    # wreate new database structure\n    op.create_table(\n        \"api_client\",\n        sa.Column(\"id\", UUID(as_uuid=True), default=uuid.uuid4, server_default=sa.text(\"gen_random_uuid()\")),\n        sa.Column(\"api_key\", sa.String(512), nullable=False),\n        sa.Column(\"description\", sa.String(256), nullable=False),\n        sa.Column(\"admin_email\", sa.String(256), nullable=True),\n        sa.Column(\"enabled\", sa.Boolean, default=True, nullable=False),\n        sa.PrimaryKeyConstraint(\"id\"),\n    )\n    op.create_index(op.f(\"ix_api_client_api_key\"), \"api_client\", [\"api_key\"], unique=True)\n\n    op.create_table(\n        \"person\",\n        sa.Column(\"id\", UUID(as_uuid=True), default=uuid.uuid4, server_default=sa.text(\"gen_random_uuid()\")),\n        sa.Column(\"username\", sa.String(128), nullable=False),  # unique in combination with api_client_id\n        sa.Column(\"display_name\", sa.String(256), nullable=False),  # cached last seen display_name\n        sa.Column(\"created_date\", sa.DateTime(), nullable=False, server_default=sa.func.current_timestamp()),\n        sa.Column(\"api_client_id\", UUID(as_uuid=True), nullable=False),\n        sa.PrimaryKeyConstraint(\"id\"),\n        sa.ForeignKeyConstraint([\"api_client_id\"], [\"api_client.id\"]),\n    )\n    op.create_index(op.f(\"ix_person_username\"), \"person\", [\"api_client_id\", \"username\"], unique=True)\n\n    op.create_table(\n        \"person_stats\",\n        sa.Column(\"person_id\", UUID(as_uuid=True)),\n        sa.Column(\"leader_score\", sa.Integer, default=0, nullable=False),  # determines position on leader board\n        sa.Column(\"modified_date\", sa.DateTime(), nullable=False, server_default=sa.func.current_timestamp()),\n        sa.Column(\"reactions\", sa.Integer, default=0, nullable=False),  # reactions sent by user\n        sa.Column(\"posts\", sa.Integer, default=0, nullable=False),  # posts sent by user\n        sa.Column(\"upvotes\", sa.Integer, default=0, nullable=False),  # received upvotes (form other users)\n        sa.Column(\"downvotes\", sa.Integer, default=0, nullable=False),  # received downvotes (from other users)\n        sa.Column(\"work_reward\", sa.Integer, default=0, nullable=False),  # reward for workpackage completions\n        sa.Column(\"compare_wins\", sa.Integer, default=0, nullable=False),  # num times user's post won compare tasks\n        sa.Column(\"compare_losses\", sa.Integer, default=0, nullable=False),  # num times users's post lost compare tasks\n        sa.PrimaryKeyConstraint(\"person_id\"),\n        sa.ForeignKeyConstraint([\"person_id\"], [\"person.id\"]),\n    )\n\n    op.create_table(\n        \"work_package\",\n        sa.Column(\"id\", UUID(as_uuid=True), default=uuid.uuid4, server_default=sa.text(\"gen_random_uuid()\")),\n        sa.Column(\"created_date\", sa.DateTime(), nullable=False, server_default=sa.func.current_timestamp()),\n        sa.Column(\"expiry_date\", sa.DateTime(), nullable=True),\n        sa.Column(\"person_id\", UUID(as_uuid=True), nullable=True),\n        sa.Column(\"payload_type\", sa.String(200), nullable=False),  # deserialization hint & dbg aid\n        sa.Column(\"payload\", JSONB(astext_type=sa.Text()), nullable=False),\n        sa.Column(\"api_client_id\", UUID(as_uuid=True), nullable=False),\n        sa.PrimaryKeyConstraint(\"id\"),\n        sa.ForeignKeyConstraint([\"person_id\"], [\"person.id\"]),\n        sa.ForeignKeyConstraint([\"api_client_id\"], [\"api_client.id\"]),\n    )\n    op.create_index(op.f(\"ix_work_package_person_id\"), \"work_package\", [\"person_id\"], unique=False)\n\n    op.create_table(\n        \"post\",\n        sa.Column(\"id\", UUID(as_uuid=True), default=uuid.uuid4, server_default=sa.text(\"gen_random_uuid()\")),\n        sa.Column(\"parent_id\", UUID(as_uuid=True), nullable=True),  # root posts have NULL parent\n        sa.Column(\"thread_id\", UUID(as_uuid=True), nullable=False),  # id of thread root\n        sa.Column(\"workpackage_id\", UUID(as_uuid=True), nullable=True),  # workpackage id to pass to handler on reply\n        sa.Column(\"person_id\", UUID(as_uuid=True), nullable=True),  # sender (recipients are part of payload)\n        sa.Column(\"api_client_id\", UUID(as_uuid=True), nullable=False),\n        sa.Column(\"role\", sa.String(128), nullable=False),  # 'assistant', 'user' or something else\n        sa.Column(\"frontend_post_id\", sa.String(200), nullable=False),  # unique together with api_client_id\n        sa.Column(\"created_date\", sa.DateTime(), nullable=False, server_default=sa.func.current_timestamp()),\n        sa.Column(\"payload_type\", sa.String(200), nullable=False),  # deserialization hint & dbg aid\n        sa.Column(\"payload\", JSONB(astext_type=sa.Text()), nullable=True),\n        sa.PrimaryKeyConstraint(\"id\"),\n        sa.ForeignKeyConstraint([\"person_id\"], [\"person.id\"]),\n        sa.ForeignKeyConstraint([\"api_client_id\"], [\"api_client.id\"]),\n    )\n    op.create_index(op.f(\"ix_post_frontend_post_id\"), \"post\", [\"api_client_id\", \"frontend_post_id\"], unique=True)\n    op.create_index(op.f(\"ix_post_thread_id\"), \"post\", [\"thread_id\"], unique=False)\n    op.create_index(op.f(\"ix_post_workpackage_id\"), \"post\", [\"workpackage_id\"], unique=False)\n    op.create_index(op.f(\"ix_post_person_id\"), \"post\", [\"person_id\"], unique=False)\n\n    op.create_table(\n        \"post_reaction\",\n        sa.Column(\"post_id\", UUID(as_uuid=True), nullable=False),\n        sa.Column(\"person_id\", UUID(as_uuid=True), nullable=False),  # sender (recipients are part of payload)\n        sa.Column(\"created_date\", sa.DateTime(), nullable=False, server_default=sa.func.current_timestamp()),\n        sa.Column(\"payload_type\", sa.String(200), nullable=False),  # deserialization hint & dbg aid\n        sa.Column(\"payload\", JSONB(astext_type=sa.Text()), nullable=False),\n        sa.Column(\"api_client_id\", UUID(as_uuid=True), nullable=False),\n        sa.PrimaryKeyConstraint(\"post_id\", \"person_id\"),\n        sa.ForeignKeyConstraint([\"post_id\"], [\"post.id\"]),\n        sa.ForeignKeyConstraint([\"person_id\"], [\"person.id\"]),\n        sa.ForeignKeyConstraint([\"api_client_id\"], [\"api_client.id\"]),\n    )\n\n\ndef downgrade() -> None:\n    op.drop_table(\"post_reaction\")\n\n    op.drop_index(\"ix_post_person_id\")\n    op.drop_index(\"ix_post_workpackage_id\")\n    op.drop_index(\"ix_post_thread_id\")\n    op.drop_index(\"ix_post_frontend_post_id\")\n    op.drop_table(\"post\")\n\n    op.drop_index(\"ix_work_package_person_id\")\n    op.drop_table(\"work_package\")\n\n    op.drop_table(\"person_stats\")\n\n    op.drop_index(\"ix_person_username\")\n    op.drop_table(\"person\")\n\n    op.drop_index(\"ix_api_client_api_key\")\n    op.drop_table(\"api_client\")\n\n    op.create_table(\n        \"service_client\",\n        sa.Column(\"id\", sa.Integer, sa.Identity()),\n        sa.Column(\"name\", sa.String(200), nullable=False),\n        sa.Column(\"service_admin_email\", sa.String(128), nullable=True),\n        sa.Column(\"api_key\", sa.String(300), nullable=False),\n        sa.Column(\"can_append\", sa.Boolean, nullable=False, server_default=\"true\"),\n        sa.Column(\"can_write\", sa.Boolean, nullable=False, server_default=\"false\"),\n        sa.Column(\"can_delete\", sa.Boolean, nullable=False, server_default=\"false\"),\n        sa.Column(\"can_read\", sa.Boolean, nullable=False, server_default=\"true\"),\n        sa.PrimaryKeyConstraint(\"id\"),\n    )\n    op.create_index(op.f(\"ix_service_client_api_key\"), \"service_client\", [\"api_key\"], unique=True)\n\n    op.create_table(\n        \"labeler\",\n        sa.Column(\"id\", sa.Integer, sa.Identity()),\n        sa.Column(\"display_name\", sa.String(96), nullable=False),\n        sa.Column(\"discord_username\", sa.String(96), nullable=True),\n        sa.Column(\n            \"created_date\",\n            sa.DateTime,\n            nullable=False,\n            server_default=sa.func.current_timestamp(),\n        ),\n        sa.Column(\"is_enabled\", sa.Boolean, nullable=False, server_default=\"true\"),\n        sa.Column(\"notes\", sa.String(10 * 1024), nullable=True),\n        sa.PrimaryKeyConstraint(\"id\"),\n        sa.UniqueConstraint(\"discord_username\"),\n    )\n\n    op.create_table(\n        \"prompt\",\n        sa.Column(\"id\", sa.Integer, sa.Identity()),\n        sa.Column(\"labeler_id\", sa.Integer, nullable=False),\n        sa.Column(\"prompt\", sa.Text, nullable=False),\n        sa.Column(\"response\", sa.Text, nullable=True),\n        sa.Column(\"lang\", sa.String(32), nullable=True),\n        sa.Column(\n            \"created_date\",\n            sa.DateTime(),\n            nullable=False,\n            server_default=sa.func.current_timestamp(),\n        ),\n        sa.ForeignKeyConstraint(\n            [\"labeler_id\"],\n            [\"labeler.id\"],\n        ),\n        sa.PrimaryKeyConstraint(\"id\"),\n    )\n    op.create_index(op.f(\"prompt_labeler_id\"), \"prompt\", [\"labeler_id\"], unique=False)\n", "backend/alembic/versions/2022_12_28_1142-d24b37426857_post_ref_for_work_package.py": "\"\"\"post ref for work_package\n\nRevision ID: d24b37426857\nRevises: 3358eb6834e6\nCreate Date: 2022-12-28 11:42:26.773704\n\n\"\"\"\nimport sqlalchemy as sa\nimport sqlmodel\nfrom alembic import op\nfrom sqlalchemy.dialects import postgresql\n\n# revision identifiers, used by Alembic.\nrevision = \"d24b37426857\"\ndown_revision = \"3358eb6834e6\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.add_column(\"post\", sa.Column(\"depth\", sa.Integer(), server_default=sa.text(\"0\"), nullable=False))\n    op.add_column(\"post\", sa.Column(\"children_count\", sa.Integer(), server_default=sa.text(\"0\"), nullable=False))\n    op.add_column(\"post_reaction\", sa.Column(\"work_package_id\", postgresql.UUID(as_uuid=True), nullable=False))\n    op.drop_constraint(\"post_reaction_post_id_fkey\", \"post_reaction\", type_=\"foreignkey\")\n    op.create_foreign_key(None, \"post_reaction\", \"work_package\", [\"work_package_id\"], [\"id\"])\n    op.drop_column(\"post_reaction\", \"post_id\")\n    op.add_column(\"work_package\", sa.Column(\"done\", sa.Boolean(), server_default=sa.text(\"false\"), nullable=False))\n    op.add_column(\"work_package\", sa.Column(\"ack\", sa.Boolean(), nullable=True))\n    op.add_column(\"work_package\", sa.Column(\"frontend_ref_post_id\", sqlmodel.sql.sqltypes.AutoString(), nullable=True))\n    op.add_column(\"work_package\", sa.Column(\"thread_id\", sqlmodel.sql.sqltypes.GUID(), nullable=True))\n    op.add_column(\"work_package\", sa.Column(\"parent_post_id\", sqlmodel.sql.sqltypes.GUID(), nullable=True))\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_column(\"work_package\", \"parent_post_id\")\n    op.drop_column(\"work_package\", \"thread_id\")\n    op.drop_column(\"work_package\", \"frontend_ref_post_id\")\n    op.drop_column(\"work_package\", \"ack\")\n    op.drop_column(\"work_package\", \"done\")\n    op.add_column(\"post_reaction\", sa.Column(\"post_id\", postgresql.UUID(), autoincrement=False, nullable=False))\n    op.drop_constraint(None, \"post_reaction\", type_=\"foreignkey\")\n    op.create_foreign_key(\"post_reaction_post_id_fkey\", \"post_reaction\", \"post\", [\"post_id\"], [\"id\"])\n    op.drop_column(\"post_reaction\", \"work_package_id\")\n    op.drop_column(\"post\", \"children_count\")\n    op.drop_column(\"post\", \"depth\")\n    # ### end Alembic commands ###\n", "backend/alembic/versions/2023_02_01_1010-f60958968ff8_add_won_prompt_lottery_date_to_mts.py": "\"\"\"add won_prompt_lottery_date to mts\n\nRevision ID: f60958968ff8\nRevises: 7b8f0011e0b0\nCreate Date: 2023-02-01 10:10:38.301707\n\n\"\"\"\nimport sqlalchemy as sa\nfrom alembic import op\n\n# revision identifiers, used by Alembic.\nrevision = \"f60958968ff8\"\ndown_revision = \"7b8f0011e0b0\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.add_column(\"message_tree_state\", sa.Column(\"won_prompt_lottery_date\", sa.DateTime(timezone=True), nullable=True))\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_column(\"message_tree_state\", \"won_prompt_lottery_date\")\n    # ### end Alembic commands ###\n", "backend/alembic/versions/2023_01_15_1139-423557e869e4_add_indices_for_created_date.py": "\"\"\"add indices for created_date\n\nRevision ID: 423557e869e4\nRevises: 7c98102efbca\nCreate Date: 2023-01-15 11:39:10.407859\n\n\"\"\"\nfrom alembic import op\n\n# revision identifiers, used by Alembic.\nrevision = \"423557e869e4\"\ndown_revision = \"7c98102efbca\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.create_index(op.f(\"ix_message_created_date\"), \"message\", [\"created_date\"], unique=False)\n    op.create_index(op.f(\"ix_message_reaction_created_date\"), \"message_reaction\", [\"created_date\"], unique=False)\n    op.create_index(op.f(\"ix_text_labels_created_date\"), \"text_labels\", [\"created_date\"], unique=False)\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_index(op.f(\"ix_text_labels_created_date\"), table_name=\"text_labels\")\n    op.drop_index(op.f(\"ix_message_reaction_created_date\"), table_name=\"message_reaction\")\n    op.drop_index(op.f(\"ix_message_created_date\"), table_name=\"message\")\n    # ### end Alembic commands ###\n", "backend/alembic/versions/2023_01_07_1250-ba61fe17fb6e_added_frontend_type_to_api_client.py": "\"\"\"added frontend_type to api_client\n\nRevision ID: ba61fe17fb6e\nRevises: 20cd871f4ec7\nCreate Date: 2023-01-07 12:50:32.195930\n\n\"\"\"\nimport sqlalchemy as sa\nfrom alembic import op\n\n# revision identifiers, used by Alembic.\nrevision = \"ba61fe17fb6e\"\ndown_revision = \"20cd871f4ec7\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    op.add_column(\"api_client\", sa.Column(\"frontend_type\", sa.String(256), nullable=True))\n\n\ndef downgrade() -> None:\n    op.drop_column(\"api_client\", \"frontend_type\")\n", "backend/alembic/versions/2023_01_28_1157-49d8445b4c90_add_origin_column_to_message_tree_state.py": "\"\"\"add origin column to message_tree_state\n\nRevision ID: 49d8445b4c90\nRevises: f856bf19d32b\nCreate Date: 2023-01-28 11:57:45.580027\n\n\"\"\"\nimport sqlalchemy as sa\nfrom alembic import op\n\n# revision identifiers, used by Alembic.\nrevision = \"49d8445b4c90\"\ndown_revision = \"f856bf19d32b\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.add_column(\"message\", sa.Column(\"synthetic\", sa.Boolean(), server_default=sa.text(\"false\"), nullable=False))\n    op.add_column(\"message\", sa.Column(\"model_name\", sa.String(length=1024), nullable=True))\n    op.add_column(\"message_tree_state\", sa.Column(\"origin\", sa.String(length=1024), nullable=True))\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_column(\"message_tree_state\", \"origin\")\n    op.drop_column(\"message\", \"model_name\")\n    op.drop_column(\"message\", \"synthetic\")\n    # ### end Alembic commands ###\n", "backend/alembic/versions/2023_01_05_1144-d4161e384f83_added_messagetreestate_table.py": "\"\"\"added MessageTreeState table\n\nRevision ID: d4161e384f83\nRevises: 8d269bc4fdbd\nCreate Date: 2023-01-05 11:44:02.630633\n\n\"\"\"\nimport sqlalchemy as sa\nimport sqlmodel\nfrom alembic import op\nfrom sqlalchemy.dialects import postgresql\n\n# revision identifiers, used by Alembic.\nrevision = \"d4161e384f83\"\ndown_revision = \"8d269bc4fdbd\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.create_table(\n        \"message_tree_state\",\n        sa.Column(\"id\", postgresql.UUID(as_uuid=True), server_default=sa.text(\"gen_random_uuid()\"), nullable=False),\n        sa.Column(\"created_date\", sa.DateTime(), server_default=sa.text(\"CURRENT_TIMESTAMP\"), nullable=False),\n        sa.Column(\"deleted\", sa.Boolean(), server_default=sa.text(\"false\"), nullable=False),\n        sa.Column(\"message_tree_id\", sqlmodel.sql.sqltypes.GUID(), nullable=False),\n        sa.Column(\"state\", sqlmodel.sql.sqltypes.AutoString(length=128), nullable=False),\n        sa.Column(\"goal_tree_size\", sa.Integer(), nullable=False),\n        sa.Column(\"current_num_non_filtered_messages\", sa.Integer(), nullable=False),\n        sa.Column(\"max_depth\", sa.Integer(), nullable=False),\n        sa.PrimaryKeyConstraint(\"id\"),\n    )\n    op.create_index(\n        op.f(\"ix_message_tree_state_message_tree_id\"), \"message_tree_state\", [\"message_tree_id\"], unique=False\n    )\n    op.create_index(\"ix_message_tree_state_tree_id\", \"message_tree_state\", [\"message_tree_id\"], unique=True)\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_index(\"ix_message_tree_state_tree_id\", table_name=\"message_tree_state\")\n    op.drop_index(op.f(\"ix_message_tree_state_message_tree_id\"), table_name=\"message_tree_state\")\n    op.drop_table(\"message_tree_state\")\n    # ### end Alembic commands ###\n", "backend/alembic/versions/2023_02_01_0022-55361f323d12_add_tos_acceptance_date_to_user.py": "\"\"\"add tos_acceptance_date to user\n\nRevision ID: 55361f323d12\nRevises: 7b8f0011e0b0\nCreate Date: 2023-02-01 00:22:08.280251\n\n\"\"\"\nimport sqlalchemy as sa\nfrom alembic import op\nfrom sqlalchemy.dialects import postgresql\n\n# revision identifiers, used by Alembic.\nrevision = \"55361f323d12\"\ndown_revision = \"f60958968ff8\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.add_column(\"user\", sa.Column(\"tos_acceptance_date\", sa.DateTime(timezone=True), nullable=True))\n    op.drop_column(\"user_stats\", \"streak_days\")\n    op.drop_column(\"user_stats\", \"streak_last_day_date\")\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.add_column(\n        \"user_stats\", sa.Column(\"streak_last_day_date\", postgresql.TIMESTAMP(), autoincrement=False, nullable=True)\n    )\n    op.add_column(\"user_stats\", sa.Column(\"streak_days\", sa.INTEGER(), autoincrement=False, nullable=True))\n    op.drop_column(\"user\", \"tos_acceptance_date\")\n    # ### end Alembic commands ###\n", "backend/alembic/versions/2023_02_02_1544-4d7e0b0ebe84_add_troll_stats.py": "\"\"\"add troll_stats\n\nRevision ID: 4d7e0b0ebe84\nRevises: 9e7ec4a9e3f2\nCreate Date: 2023-02-02 15:44:12.647260\n\n\"\"\"\nimport sqlalchemy as sa\nimport sqlmodel\nfrom alembic import op\nfrom sqlalchemy.dialects import postgresql\n\n# revision identifiers, used by Alembic.\nrevision = \"4d7e0b0ebe84\"\ndown_revision = \"9e7ec4a9e3f2\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.create_table(\n        \"troll_stats\",\n        sa.Column(\"user_id\", postgresql.UUID(as_uuid=True), nullable=False),\n        sa.Column(\"base_date\", sa.DateTime(timezone=True), nullable=True),\n        sa.Column(\n            \"modified_date\", sa.DateTime(timezone=True), server_default=sa.text(\"CURRENT_TIMESTAMP\"), nullable=False\n        ),\n        sa.Column(\"time_frame\", sqlmodel.sql.sqltypes.AutoString(), nullable=False),\n        sa.Column(\"troll_score\", sa.Integer(), nullable=False),\n        sa.Column(\"rank\", sa.Integer(), nullable=True),\n        sa.Column(\"red_flags\", sa.Integer(), nullable=False),\n        sa.Column(\"upvotes\", sa.Integer(), nullable=False),\n        sa.Column(\"downvotes\", sa.Integer(), nullable=False),\n        sa.Column(\"spam_prompts\", sa.Integer(), nullable=False),\n        sa.Column(\"quality\", sa.Float(), nullable=True),\n        sa.Column(\"humor\", sa.Float(), nullable=True),\n        sa.Column(\"toxicity\", sa.Float(), nullable=True),\n        sa.Column(\"violence\", sa.Float(), nullable=True),\n        sa.Column(\"helpfulness\", sa.Float(), nullable=True),\n        sa.Column(\"spam\", sa.Integer(), nullable=False),\n        sa.Column(\"lang_mismach\", sa.Integer(), nullable=False),\n        sa.Column(\"not_appropriate\", sa.Integer(), nullable=False),\n        sa.Column(\"pii\", sa.Integer(), nullable=False),\n        sa.Column(\"hate_speech\", sa.Integer(), nullable=False),\n        sa.Column(\"sexual_content\", sa.Integer(), nullable=False),\n        sa.Column(\"political_content\", sa.Integer(), nullable=False),\n        sa.ForeignKeyConstraint([\"user_id\"], [\"user.id\"], ondelete=\"CASCADE\"),\n        sa.PrimaryKeyConstraint(\"user_id\", \"time_frame\"),\n    )\n    op.create_index(\"ix_troll_stats__timeframe__user_id\", \"troll_stats\", [\"time_frame\", \"user_id\"], unique=True)\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_index(\"ix_troll_stats__timeframe__user_id\", table_name=\"troll_stats\")\n    op.drop_table(\"troll_stats\")\n    # ### end Alembic commands ###\n", "backend/alembic/versions/2022_12_30_2054-abb47e9d145a_name_changes_person_user_post_message_.py": "\"\"\"name changes: person->user, post->message, work_package->task\n\nRevision ID: abb47e9d145a\nRevises: 73ce3675c1f5\nCreate Date: 2022-12-30 20:54:49.880568\n\n\"\"\"\nimport sqlalchemy as sa\nimport sqlmodel\nfrom alembic import op\nfrom sqlalchemy.dialects import postgresql\n\n# revision identifiers, used by Alembic.\nrevision = \"abb47e9d145a\"\ndown_revision = \"73ce3675c1f5\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    # clear DB\n    op.execute(\"DELETE FROM journal;\")\n    op.execute(\"DELETE FROM work_package;\")\n    op.execute(\"DELETE FROM post_reaction;\")\n    op.execute(\"DELETE FROM post;\")\n    op.execute(\"DELETE FROM person_stats;\")\n    op.execute(\"DELETE FROM person;\")\n    op.execute(\"DELETE FROM text_labels;\")\n\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.create_table(\n        \"user\",\n        sa.Column(\"id\", postgresql.UUID(as_uuid=True), server_default=sa.text(\"gen_random_uuid()\"), nullable=False),\n        sa.Column(\"created_date\", sa.DateTime(), server_default=sa.text(\"CURRENT_TIMESTAMP\"), nullable=False),\n        sa.Column(\"username\", sqlmodel.sql.sqltypes.AutoString(length=128), nullable=False),\n        sa.Column(\"auth_method\", sqlmodel.sql.sqltypes.AutoString(length=128), nullable=False),\n        sa.Column(\"display_name\", sqlmodel.sql.sqltypes.AutoString(length=256), nullable=False),\n        sa.Column(\"api_client_id\", sqlmodel.sql.sqltypes.GUID(), nullable=False),\n        sa.ForeignKeyConstraint(\n            [\"api_client_id\"],\n            [\"api_client.id\"],\n        ),\n        sa.PrimaryKeyConstraint(\"id\"),\n    )\n    op.create_index(\"ix_user_username\", \"user\", [\"api_client_id\", \"username\", \"auth_method\"], unique=True)\n    op.create_table(\n        \"message\",\n        sa.Column(\"id\", postgresql.UUID(as_uuid=True), server_default=sa.text(\"gen_random_uuid()\"), nullable=False),\n        sa.Column(\"created_date\", sa.DateTime(), server_default=sa.text(\"CURRENT_TIMESTAMP\"), nullable=False),\n        sa.Column(\"payload\", postgresql.JSONB(astext_type=sa.Text()), nullable=True),\n        sa.Column(\"depth\", sa.Integer(), server_default=sa.text(\"0\"), nullable=False),\n        sa.Column(\"children_count\", sa.Integer(), server_default=sa.text(\"0\"), nullable=False),\n        sa.Column(\"parent_id\", sqlmodel.sql.sqltypes.GUID(), nullable=True),\n        sa.Column(\"message_tree_id\", sqlmodel.sql.sqltypes.GUID(), nullable=False),\n        sa.Column(\"task_id\", sqlmodel.sql.sqltypes.GUID(), nullable=True),\n        sa.Column(\"user_id\", sqlmodel.sql.sqltypes.GUID(), nullable=True),\n        sa.Column(\"role\", sqlmodel.sql.sqltypes.AutoString(length=128), nullable=False),\n        sa.Column(\"api_client_id\", sqlmodel.sql.sqltypes.GUID(), nullable=False),\n        sa.Column(\"frontend_message_id\", sqlmodel.sql.sqltypes.AutoString(length=200), nullable=False),\n        sa.Column(\"payload_type\", sqlmodel.sql.sqltypes.AutoString(length=200), nullable=False),\n        sa.Column(\"lang\", sqlmodel.sql.sqltypes.AutoString(length=200), nullable=False),\n        sa.ForeignKeyConstraint(\n            [\"api_client_id\"],\n            [\"api_client.id\"],\n        ),\n        sa.ForeignKeyConstraint(\n            [\"user_id\"],\n            [\"user.id\"],\n        ),\n        sa.PrimaryKeyConstraint(\"id\"),\n    )\n    op.create_index(\"ix_message_frontend_message_id\", \"message\", [\"api_client_id\", \"frontend_message_id\"], unique=True)\n    op.create_index(op.f(\"ix_message_message_tree_id\"), \"message\", [\"message_tree_id\"], unique=False)\n    op.create_index(op.f(\"ix_message_task_id\"), \"message\", [\"task_id\"], unique=False)\n    op.create_index(op.f(\"ix_message_user_id\"), \"message\", [\"user_id\"], unique=False)\n    op.create_table(\n        \"task\",\n        sa.Column(\"id\", postgresql.UUID(as_uuid=True), server_default=sa.text(\"gen_random_uuid()\"), nullable=False),\n        sa.Column(\"created_date\", sa.DateTime(), server_default=sa.text(\"CURRENT_TIMESTAMP\"), nullable=False),\n        sa.Column(\"expiry_date\", sa.DateTime(), nullable=True),\n        sa.Column(\"payload\", postgresql.JSONB(astext_type=sa.Text()), nullable=False),\n        sa.Column(\"done\", sa.Boolean(), server_default=sa.text(\"false\"), nullable=False),\n        sa.Column(\"collective\", sa.Boolean(), server_default=sa.text(\"false\"), nullable=False),\n        sa.Column(\"user_id\", sqlmodel.sql.sqltypes.GUID(), nullable=True),\n        sa.Column(\"payload_type\", sqlmodel.sql.sqltypes.AutoString(length=200), nullable=False),\n        sa.Column(\"api_client_id\", sqlmodel.sql.sqltypes.GUID(), nullable=False),\n        sa.Column(\"ack\", sa.Boolean(), nullable=True),\n        sa.Column(\"frontend_message_id\", sqlmodel.sql.sqltypes.AutoString(), nullable=True),\n        sa.Column(\"message_tree_id\", sqlmodel.sql.sqltypes.GUID(), nullable=True),\n        sa.Column(\"parent_message_id\", sqlmodel.sql.sqltypes.GUID(), nullable=True),\n        sa.ForeignKeyConstraint(\n            [\"api_client_id\"],\n            [\"api_client.id\"],\n        ),\n        sa.ForeignKeyConstraint(\n            [\"user_id\"],\n            [\"user.id\"],\n        ),\n        sa.PrimaryKeyConstraint(\"id\"),\n    )\n    op.create_index(op.f(\"ix_task_user_id\"), \"task\", [\"user_id\"], unique=False)\n    op.create_table(\n        \"user_stats\",\n        sa.Column(\"user_id\", postgresql.UUID(as_uuid=True), nullable=False),\n        sa.Column(\"modified_date\", sa.DateTime(), server_default=sa.text(\"CURRENT_TIMESTAMP\"), nullable=False),\n        sa.Column(\"leader_score\", sa.Integer(), nullable=False),\n        sa.Column(\"reactions\", sa.Integer(), nullable=False),\n        sa.Column(\"messages\", sa.Integer(), nullable=False),\n        sa.Column(\"upvotes\", sa.Integer(), nullable=False),\n        sa.Column(\"downvotes\", sa.Integer(), nullable=False),\n        sa.Column(\"task_reward\", sa.Integer(), nullable=False),\n        sa.Column(\"compare_wins\", sa.Integer(), nullable=False),\n        sa.Column(\"compare_losses\", sa.Integer(), nullable=False),\n        sa.ForeignKeyConstraint(\n            [\"user_id\"],\n            [\"user.id\"],\n        ),\n        sa.PrimaryKeyConstraint(\"user_id\"),\n    )\n    op.create_table(\n        \"message_reaction\",\n        sa.Column(\"task_id\", postgresql.UUID(as_uuid=True), nullable=False),\n        sa.Column(\"user_id\", postgresql.UUID(as_uuid=True), nullable=False),\n        sa.Column(\"created_date\", sa.DateTime(), server_default=sa.text(\"CURRENT_TIMESTAMP\"), nullable=False),\n        sa.Column(\"payload\", postgresql.JSONB(astext_type=sa.Text()), nullable=False),\n        sa.Column(\"payload_type\", sqlmodel.sql.sqltypes.AutoString(length=200), nullable=False),\n        sa.Column(\"api_client_id\", sqlmodel.sql.sqltypes.GUID(), nullable=False),\n        sa.ForeignKeyConstraint(\n            [\"api_client_id\"],\n            [\"api_client.id\"],\n        ),\n        sa.ForeignKeyConstraint(\n            [\"task_id\"],\n            [\"task.id\"],\n        ),\n        sa.ForeignKeyConstraint(\n            [\"user_id\"],\n            [\"user.id\"],\n        ),\n        sa.PrimaryKeyConstraint(\"task_id\", \"user_id\"),\n    )\n\n    op.drop_constraint(\"text_labels_post_id_fkey\", \"text_labels\", type_=\"foreignkey\")\n    op.drop_constraint(\"journal_post_id_fkey\", \"journal\", type_=\"foreignkey\")\n    op.drop_constraint(\"journal_person_id_fkey\", \"journal\", type_=\"foreignkey\")\n\n    op.drop_table(\"post_reaction\")\n\n    op.drop_index(\"ix_post_frontend_post_id\", table_name=\"post\")\n    op.drop_index(\"ix_post_person_id\", table_name=\"post\")\n    op.drop_index(\"ix_post_thread_id\", table_name=\"post\")\n    op.drop_index(\"ix_post_workpackage_id\", table_name=\"post\")\n    op.drop_table(\"post\")\n\n    op.drop_index(\"ix_work_package_person_id\", table_name=\"work_package\")\n    op.drop_table(\"work_package\")\n    op.drop_table(\"person_stats\")\n\n    op.drop_index(\"ix_person_username\", table_name=\"person\")\n    op.drop_table(\"person\")\n\n    op.add_column(\"journal\", sa.Column(\"user_id\", sqlmodel.sql.sqltypes.GUID(), nullable=True))\n    op.add_column(\"journal\", sa.Column(\"message_id\", sqlmodel.sql.sqltypes.GUID(), nullable=True))\n    op.drop_index(\"ix_journal_person_id\", table_name=\"journal\")\n    op.create_index(op.f(\"ix_journal_user_id\"), \"journal\", [\"user_id\"], unique=False)\n\n    op.create_foreign_key(None, \"journal\", \"user\", [\"user_id\"], [\"id\"])\n    op.create_foreign_key(None, \"journal\", \"message\", [\"message_id\"], [\"id\"])\n    op.drop_column(\"journal\", \"person_id\")\n    op.drop_column(\"journal\", \"post_id\")\n    op.add_column(\"text_labels\", sa.Column(\"message_id\", postgresql.UUID(as_uuid=True), nullable=True))\n    op.create_foreign_key(None, \"text_labels\", \"message\", [\"message_id\"], [\"id\"])\n    op.drop_column(\"text_labels\", \"post_id\")\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    # clear DB\n    op.execute(\"DELETE FROM journal;\")\n    op.execute(\"DELETE FROM message_reaction;\")\n    op.execute(\"DELETE FROM task;\")\n    op.execute(\"DELETE FROM message;\")\n    op.execute(\"DELETE FROM user_stats;\")\n    op.execute('DELETE FROM \"user\";')\n    op.execute(\"DELETE FROM text_labels;\")\n\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.add_column(\"text_labels\", sa.Column(\"post_id\", postgresql.UUID(), autoincrement=False, nullable=True))\n    op.drop_constraint(\"text_labels_message_id_fkey\", \"text_labels\", type_=\"foreignkey\")\n\n    op.drop_column(\"text_labels\", \"message_id\")\n    op.add_column(\"journal\", sa.Column(\"post_id\", postgresql.UUID(), autoincrement=False, nullable=True))\n    op.add_column(\"journal\", sa.Column(\"person_id\", postgresql.UUID(), autoincrement=False, nullable=True))\n    op.drop_constraint(\"journal_message_id_fkey\", \"journal\", type_=\"foreignkey\")\n    op.drop_constraint(\"journal_user_id_fkey\", \"journal\", type_=\"foreignkey\")\n\n    op.drop_index(op.f(\"ix_journal_user_id\"), table_name=\"journal\")\n    op.create_index(\"ix_journal_person_id\", \"journal\", [\"person_id\"], unique=False)\n    op.drop_column(\"journal\", \"message_id\")\n    op.drop_column(\"journal\", \"user_id\")\n\n    op.create_table(\n        \"person\",\n        sa.Column(\n            \"id\", postgresql.UUID(), server_default=sa.text(\"gen_random_uuid()\"), autoincrement=False, nullable=False\n        ),\n        sa.Column(\"username\", sa.VARCHAR(length=128), autoincrement=False, nullable=False),\n        sa.Column(\"display_name\", sa.VARCHAR(length=256), autoincrement=False, nullable=False),\n        sa.Column(\n            \"created_date\",\n            postgresql.TIMESTAMP(),\n            server_default=sa.text(\"CURRENT_TIMESTAMP\"),\n            autoincrement=False,\n            nullable=False,\n        ),\n        sa.Column(\"api_client_id\", postgresql.UUID(), autoincrement=False, nullable=False),\n        sa.Column(\"auth_method\", sa.VARCHAR(length=128), autoincrement=False, nullable=False),\n        sa.ForeignKeyConstraint([\"api_client_id\"], [\"api_client.id\"], name=\"person_api_client_id_fkey\"),\n        sa.PrimaryKeyConstraint(\"id\", name=\"person_pkey\"),\n    )\n    op.create_table(\n        \"person_stats\",\n        sa.Column(\"person_id\", postgresql.UUID(), autoincrement=False, nullable=False),\n        sa.Column(\"leader_score\", sa.INTEGER(), autoincrement=False, nullable=False),\n        sa.Column(\n            \"modified_date\",\n            postgresql.TIMESTAMP(),\n            server_default=sa.text(\"CURRENT_TIMESTAMP\"),\n            autoincrement=False,\n            nullable=False,\n        ),\n        sa.Column(\"reactions\", sa.INTEGER(), autoincrement=False, nullable=False),\n        sa.Column(\"posts\", sa.INTEGER(), autoincrement=False, nullable=False),\n        sa.Column(\"upvotes\", sa.INTEGER(), autoincrement=False, nullable=False),\n        sa.Column(\"downvotes\", sa.INTEGER(), autoincrement=False, nullable=False),\n        sa.Column(\"work_reward\", sa.INTEGER(), autoincrement=False, nullable=False),\n        sa.Column(\"compare_wins\", sa.INTEGER(), autoincrement=False, nullable=False),\n        sa.Column(\"compare_losses\", sa.INTEGER(), autoincrement=False, nullable=False),\n        sa.ForeignKeyConstraint([\"person_id\"], [\"person.id\"], name=\"person_stats_person_id_fkey\"),\n        sa.PrimaryKeyConstraint(\"person_id\", name=\"person_stats_pkey\"),\n    )\n    op.create_table(\n        \"work_package\",\n        sa.Column(\n            \"id\", postgresql.UUID(), server_default=sa.text(\"gen_random_uuid()\"), autoincrement=False, nullable=False\n        ),\n        sa.Column(\n            \"created_date\",\n            postgresql.TIMESTAMP(),\n            server_default=sa.text(\"CURRENT_TIMESTAMP\"),\n            autoincrement=False,\n            nullable=False,\n        ),\n        sa.Column(\"expiry_date\", postgresql.TIMESTAMP(), autoincrement=False, nullable=True),\n        sa.Column(\"person_id\", postgresql.UUID(), autoincrement=False, nullable=True),\n        sa.Column(\"payload_type\", sa.VARCHAR(length=200), autoincrement=False, nullable=False),\n        sa.Column(\"payload\", postgresql.JSONB(astext_type=sa.Text()), autoincrement=False, nullable=False),\n        sa.Column(\"api_client_id\", postgresql.UUID(), autoincrement=False, nullable=False),\n        sa.Column(\"done\", sa.BOOLEAN(), server_default=sa.text(\"false\"), autoincrement=False, nullable=False),\n        sa.Column(\"ack\", sa.BOOLEAN(), autoincrement=False, nullable=True),\n        sa.Column(\"frontend_ref_post_id\", sa.VARCHAR(), autoincrement=False, nullable=True),\n        sa.Column(\"thread_id\", postgresql.UUID(), autoincrement=False, nullable=True),\n        sa.Column(\"parent_post_id\", postgresql.UUID(), autoincrement=False, nullable=True),\n        sa.Column(\"collective\", sa.BOOLEAN(), server_default=sa.text(\"false\"), autoincrement=False, nullable=False),\n        sa.ForeignKeyConstraint([\"api_client_id\"], [\"api_client.id\"], name=\"work_package_api_client_id_fkey\"),\n        sa.ForeignKeyConstraint([\"person_id\"], [\"person.id\"], name=\"work_package_person_id_fkey\"),\n        sa.PrimaryKeyConstraint(\"id\", name=\"work_package_pkey\"),\n    )\n    op.create_index(\"ix_work_package_person_id\", \"work_package\", [\"person_id\"], unique=False)\n    op.create_table(\n        \"post\",\n        sa.Column(\n            \"id\", postgresql.UUID(), server_default=sa.text(\"gen_random_uuid()\"), autoincrement=False, nullable=False\n        ),\n        sa.Column(\"parent_id\", postgresql.UUID(), autoincrement=False, nullable=True),\n        sa.Column(\"thread_id\", postgresql.UUID(), autoincrement=False, nullable=False),\n        sa.Column(\"workpackage_id\", postgresql.UUID(), autoincrement=False, nullable=True),\n        sa.Column(\"person_id\", postgresql.UUID(), autoincrement=False, nullable=True),\n        sa.Column(\"api_client_id\", postgresql.UUID(), autoincrement=False, nullable=False),\n        sa.Column(\"role\", sa.VARCHAR(length=128), autoincrement=False, nullable=False),\n        sa.Column(\"frontend_post_id\", sa.VARCHAR(length=200), autoincrement=False, nullable=False),\n        sa.Column(\n            \"created_date\",\n            postgresql.TIMESTAMP(),\n            server_default=sa.text(\"CURRENT_TIMESTAMP\"),\n            autoincrement=False,\n            nullable=False,\n        ),\n        sa.Column(\"payload_type\", sa.VARCHAR(length=200), autoincrement=False, nullable=False),\n        sa.Column(\"payload\", postgresql.JSONB(astext_type=sa.Text()), autoincrement=False, nullable=True),\n        sa.Column(\"depth\", sa.INTEGER(), server_default=sa.text(\"0\"), autoincrement=False, nullable=False),\n        sa.Column(\"children_count\", sa.INTEGER(), server_default=sa.text(\"0\"), autoincrement=False, nullable=False),\n        sa.Column(\"lang\", sa.VARCHAR(length=200), autoincrement=False, nullable=False),\n        sa.ForeignKeyConstraint([\"api_client_id\"], [\"api_client.id\"], name=\"post_api_client_id_fkey\"),\n        sa.ForeignKeyConstraint([\"person_id\"], [\"person.id\"], name=\"post_person_id_fkey\"),\n        sa.PrimaryKeyConstraint(\"id\", name=\"post_pkey\"),\n    )\n    op.create_index(\"ix_post_workpackage_id\", \"post\", [\"workpackage_id\"], unique=False)\n    op.create_index(\"ix_post_thread_id\", \"post\", [\"thread_id\"], unique=False)\n    op.create_index(\"ix_post_person_id\", \"post\", [\"person_id\"], unique=False)\n    op.create_index(\"ix_post_frontend_post_id\", \"post\", [\"api_client_id\", \"frontend_post_id\"], unique=False)\n\n    op.create_table(\n        \"post_reaction\",\n        sa.Column(\"person_id\", postgresql.UUID(), autoincrement=False, nullable=False),\n        sa.Column(\n            \"created_date\",\n            postgresql.TIMESTAMP(),\n            server_default=sa.text(\"CURRENT_TIMESTAMP\"),\n            autoincrement=False,\n            nullable=False,\n        ),\n        sa.Column(\"payload_type\", sa.VARCHAR(length=200), autoincrement=False, nullable=False),\n        sa.Column(\"payload\", postgresql.JSONB(astext_type=sa.Text()), autoincrement=False, nullable=False),\n        sa.Column(\"api_client_id\", postgresql.UUID(), autoincrement=False, nullable=False),\n        sa.Column(\"work_package_id\", postgresql.UUID(), autoincrement=False, nullable=False),\n        sa.ForeignKeyConstraint([\"api_client_id\"], [\"api_client.id\"], name=\"post_reaction_api_client_id_fkey\"),\n        sa.ForeignKeyConstraint([\"person_id\"], [\"person.id\"], name=\"post_reaction_person_id_fkey\"),\n        sa.ForeignKeyConstraint([\"work_package_id\"], [\"work_package.id\"], name=\"post_reaction_work_package_id_fkey\"),\n    )\n\n    op.create_index(\"ix_person_username\", \"person\", [\"api_client_id\", \"username\", \"auth_method\"], unique=False)\n    op.create_foreign_key(\"text_labels_post_id_fkey\", \"text_labels\", \"post\", [\"post_id\"], [\"id\"])\n    op.create_foreign_key(\"journal_person_id_fkey\", \"journal\", \"person\", [\"person_id\"], [\"id\"])\n    op.create_foreign_key(\"journal_post_id_fkey\", \"journal\", \"post\", [\"post_id\"], [\"id\"])\n\n    op.drop_table(\"message_reaction\")\n    op.drop_table(\"user_stats\")\n    op.drop_index(op.f(\"ix_task_user_id\"), table_name=\"task\")\n    op.drop_table(\"task\")\n    op.drop_index(op.f(\"ix_message_user_id\"), table_name=\"message\")\n    op.drop_index(op.f(\"ix_message_task_id\"), table_name=\"message\")\n    op.drop_index(op.f(\"ix_message_message_tree_id\"), table_name=\"message\")\n    op.drop_index(\"ix_message_frontend_message_id\", table_name=\"message\")\n    op.drop_table(\"message\")\n    op.drop_index(\"ix_user_username\", table_name=\"user\")\n    op.drop_table(\"user\")\n    # ### end Alembic commands ###\n", "backend/alembic/versions/2023_01_14_1509-619255ae9076_add_rank_to_message_table.py": "\"\"\"add rank to message table\n\nRevision ID: 619255ae9076\nRevises: bcc2fe18d214\nCreate Date: 2023-01-14 15:09:03.462482\n\n\"\"\"\nimport sqlalchemy as sa\nimport sqlmodel\nfrom alembic import op\nfrom sqlalchemy.dialects import postgresql\n\n# revision identifiers, used by Alembic.\nrevision = \"619255ae9076\"\ndown_revision = \"bcc2fe18d214\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.add_column(\"message\", sa.Column(\"rank\", sa.Integer(), nullable=True))\n    op.add_column(\"message_toxicity\", sa.Column(\"score\", sa.Float(), nullable=True))\n    op.add_column(\"message_toxicity\", sa.Column(\"label\", sqlmodel.sql.sqltypes.AutoString(length=256), nullable=False))\n    op.drop_column(\"message_toxicity\", \"toxicity\")\n    op.add_column(\"user_stats\", sa.Column(\"time_frame\", sqlmodel.sql.sqltypes.AutoString(), nullable=False))\n    op.add_column(\"user_stats\", sa.Column(\"prompts\", sa.Integer(), nullable=False))\n    op.add_column(\"user_stats\", sa.Column(\"replies_assistant\", sa.Integer(), nullable=False))\n    op.add_column(\"user_stats\", sa.Column(\"replies_prompter\", sa.Integer(), nullable=False))\n    op.add_column(\"user_stats\", sa.Column(\"labels_simple\", sa.Integer(), nullable=False))\n    op.add_column(\"user_stats\", sa.Column(\"labels_full\", sa.Integer(), nullable=False))\n    op.add_column(\"user_stats\", sa.Column(\"rankings_total\", sa.Integer(), nullable=False))\n    op.add_column(\"user_stats\", sa.Column(\"rankings_good\", sa.Integer(), nullable=False))\n    op.add_column(\"user_stats\", sa.Column(\"accepted_prompts\", sa.Integer(), nullable=False))\n    op.add_column(\"user_stats\", sa.Column(\"accepted_replies_assistant\", sa.Integer(), nullable=False))\n    op.add_column(\"user_stats\", sa.Column(\"accepted_replies_prompter\", sa.Integer(), nullable=False))\n    op.add_column(\"user_stats\", sa.Column(\"reply_assistant_ranked_1\", sa.Integer(), nullable=False))\n    op.add_column(\"user_stats\", sa.Column(\"reply_assistant_ranked_2\", sa.Integer(), nullable=False))\n    op.add_column(\"user_stats\", sa.Column(\"reply_assistant_ranked_3\", sa.Integer(), nullable=False))\n    op.add_column(\"user_stats\", sa.Column(\"reply_prompter_ranked_1\", sa.Integer(), nullable=False))\n    op.add_column(\"user_stats\", sa.Column(\"reply_prompter_ranked_2\", sa.Integer(), nullable=False))\n    op.add_column(\"user_stats\", sa.Column(\"reply_prompter_ranked_3\", sa.Integer(), nullable=False))\n    op.add_column(\"user_stats\", sa.Column(\"streak_last_day_date\", sa.DateTime(), nullable=True))\n    op.add_column(\"user_stats\", sa.Column(\"streak_days\", sa.Integer(), nullable=True))\n    op.drop_column(\"user_stats\", \"messages\")\n    op.drop_column(\"user_stats\", \"upvotes\")\n    op.drop_column(\"user_stats\", \"task_reward\")\n    op.drop_column(\"user_stats\", \"compare_wins\")\n    op.drop_column(\"user_stats\", \"compare_losses\")\n    op.drop_column(\"user_stats\", \"downvotes\")\n    op.drop_column(\"user_stats\", \"reactions\")\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.add_column(\"user_stats\", sa.Column(\"reactions\", sa.INTEGER(), autoincrement=False, nullable=False))\n    op.add_column(\"user_stats\", sa.Column(\"downvotes\", sa.INTEGER(), autoincrement=False, nullable=False))\n    op.add_column(\"user_stats\", sa.Column(\"compare_losses\", sa.INTEGER(), autoincrement=False, nullable=False))\n    op.add_column(\"user_stats\", sa.Column(\"compare_wins\", sa.INTEGER(), autoincrement=False, nullable=False))\n    op.add_column(\"user_stats\", sa.Column(\"task_reward\", sa.INTEGER(), autoincrement=False, nullable=False))\n    op.add_column(\"user_stats\", sa.Column(\"upvotes\", sa.INTEGER(), autoincrement=False, nullable=False))\n    op.add_column(\"user_stats\", sa.Column(\"messages\", sa.INTEGER(), autoincrement=False, nullable=False))\n    op.drop_column(\"user_stats\", \"streak_days\")\n    op.drop_column(\"user_stats\", \"streak_last_day_date\")\n    op.drop_column(\"user_stats\", \"reply_prompter_ranked_3\")\n    op.drop_column(\"user_stats\", \"reply_prompter_ranked_2\")\n    op.drop_column(\"user_stats\", \"reply_prompter_ranked_1\")\n    op.drop_column(\"user_stats\", \"reply_assistant_ranked_3\")\n    op.drop_column(\"user_stats\", \"reply_assistant_ranked_2\")\n    op.drop_column(\"user_stats\", \"reply_assistant_ranked_1\")\n    op.drop_column(\"user_stats\", \"accepted_replies_prompter\")\n    op.drop_column(\"user_stats\", \"accepted_replies_assistant\")\n    op.drop_column(\"user_stats\", \"accepted_prompts\")\n    op.drop_column(\"user_stats\", \"rankings_good\")\n    op.drop_column(\"user_stats\", \"rankings_total\")\n    op.drop_column(\"user_stats\", \"labels_full\")\n    op.drop_column(\"user_stats\", \"labels_simple\")\n    op.drop_column(\"user_stats\", \"replies_prompter\")\n    op.drop_column(\"user_stats\", \"replies_assistant\")\n    op.drop_column(\"user_stats\", \"prompts\")\n    op.drop_column(\"user_stats\", \"time_frame\")\n    op.add_column(\n        \"message_toxicity\",\n        sa.Column(\"toxicity\", postgresql.DOUBLE_PRECISION(precision=53), autoincrement=False, nullable=True),\n    )\n    op.drop_column(\"message_toxicity\", \"label\")\n    op.drop_column(\"message_toxicity\", \"score\")\n    op.drop_column(\"message\", \"rank\")\n    # ### end Alembic commands ###\n", "backend/alembic/versions/2023_01_05_1745-20cd871f4ec7_added_user_to_textlabels.py": "\"\"\"Added user to TextLabels\n\nRevision ID: 20cd871f4ec7\nRevises: d4161e384f83\nCreate Date: 2023-01-05 17:45:15.696468\n\n\"\"\"\nimport sqlalchemy as sa\nfrom alembic import op\nfrom sqlalchemy.dialects import postgresql\n\n# revision identifiers, used by Alembic.\nrevision = \"20cd871f4ec7\"\ndown_revision = \"3b0adfadbef9\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.add_column(\"text_labels\", sa.Column(\"user_id\", postgresql.UUID(as_uuid=True), nullable=False))\n    op.create_foreign_key(None, \"text_labels\", \"user\", [\"user_id\"], [\"id\"])\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_constraint(None, \"text_labels\", type_=\"foreignkey\")\n    op.drop_column(\"text_labels\", \"user_id\")\n    # ### end Alembic commands ###\n", "backend/alembic/versions/2023_06_06_1505-c181661eba3a_add_message_revisions.py": "\"\"\"add_message_revisions\n\nRevision ID: c181661eba3a\nRevises: 1b6e3ae16e9d\nCreate Date: 2023-06-06 15:05:58.079120\n\n\"\"\"\nimport sqlalchemy as sa\nfrom alembic import op\nfrom sqlalchemy.dialects import postgresql\n\n# revision identifiers, used by Alembic.\nrevision = \"c181661eba3a\"\ndown_revision = \"1b6e3ae16e9d\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.create_table(\n        \"message_revision\",\n        sa.Column(\"id\", postgresql.UUID(as_uuid=True), nullable=False),\n        sa.Column(\"payload\", postgresql.JSONB(astext_type=sa.Text()), nullable=True),\n        sa.Column(\"message_id\", postgresql.UUID(as_uuid=True), nullable=False),\n        sa.Column(\"user_id\", postgresql.UUID(as_uuid=True), nullable=True),\n        sa.Column(\n            \"created_date\", sa.DateTime(timezone=True), server_default=sa.text(\"CURRENT_TIMESTAMP\"), nullable=True\n        ),\n        sa.ForeignKeyConstraint(\n            [\"message_id\"],\n            [\"message.id\"],\n        ),\n        sa.ForeignKeyConstraint(\n            [\"user_id\"],\n            [\"user.id\"],\n        ),\n        sa.PrimaryKeyConstraint(\"id\"),\n    )\n    op.create_index(op.f(\"ix_message_revision_message_id\"), \"message_revision\", [\"message_id\"], unique=False)\n    op.add_column(\"message\", sa.Column(\"edited\", sa.Boolean(), server_default=sa.text(\"false\"), nullable=False))\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_column(\"message\", \"edited\")\n    op.drop_index(op.f(\"ix_message_revision_message_id\"), table_name=\"message_revision\")\n    op.drop_table(\"message_revision\")\n    # ### end Alembic commands ###\n", "backend/oasst_backend/celery_worker.py": "import os\n\nfrom celery import Celery\nfrom loguru import logger\n\n\"\"\"\nTo run the worker run `celery run -A oasst_backend.celery_worker worker -l INFO`\nin the parent directory of this file, add -B to embed the beat scheduler inside\nthe worker.\n\"\"\"\napp = Celery(\n    \"oasst_worker\",\n    broker=os.environ.get(\"CELERY_BROKER_URL\", \"redis://localhost:6379/0\"),\n    backend=os.environ.get(\"CELERY_RESULT_BACKEND\", \"redis://localhost:6379/0\"),\n    include=[\"oasst_backend.scheduled_tasks\"],\n)\n\nlogger.info(f\"celery.conf.broker_url {app.conf.broker_url}, app.conf.result_backend{app.conf.result_backend}\")\n\n# see https://docs.celeryq.dev/en/stable/userguide/periodic-tasks.html\napp.conf.beat_schedule = {\n    \"reset-user-streak\": {\n        \"task\": \"periodic_user_streak_reset\",\n        \"schedule\": 60.0 * 60.0 * 4,  # in seconds, every 4h\n    },\n    \"update-search-vectors\": {\n        \"task\": \"update_search_vectors\",\n        \"schedule\": 60.0 * 20.0,\n        \"args\": (1000,),  # (batch_size,)\n    },\n}\napp.conf.timezone = \"UTC\"\n", "backend/oasst_backend/prompt_repository.py": "import random\nimport re\nfrom collections import defaultdict\nfrom datetime import datetime, timedelta\nfrom http import HTTPStatus\nfrom typing import Optional\nfrom uuid import UUID, uuid4\n\nimport oasst_backend.models.db_payload as db_payload\nimport sqlalchemy.dialects.postgresql as pg\nfrom loguru import logger\nfrom oasst_backend.api.deps import FrontendUserId\nfrom oasst_backend.config import settings\nfrom oasst_backend.journal_writer import JournalWriter\nfrom oasst_backend.models import (\n    ApiClient,\n    FlaggedMessage,\n    Message,\n    MessageEmbedding,\n    MessageEmoji,\n    MessageReaction,\n    MessageRevision,\n    MessageToxicity,\n    MessageTreeState,\n    Task,\n    TextLabels,\n    User,\n    message_tree_state,\n)\nfrom oasst_backend.models.payload_column_type import PayloadContainer\nfrom oasst_backend.task_repository import TaskRepository, validate_frontend_message_id\nfrom oasst_backend.user_repository import UserRepository\nfrom oasst_backend.utils.database_utils import CommitMode, db_lang_to_postgres_ts_lang, managed_tx_method\nfrom oasst_backend.utils.discord import send_new_report_message\nfrom oasst_shared.exceptions import OasstError, OasstErrorCode\nfrom oasst_shared.schemas import protocol as protocol_schema\nfrom oasst_shared.schemas.protocol import SystemStats\nfrom oasst_shared.utils import unaware_to_utc, utcnow\nfrom sqlalchemy.orm import Query\nfrom sqlalchemy.orm.attributes import flag_modified\nfrom sqlmodel import JSON, Session, and_, func, literal_column, not_, or_, text, update\n\n_task_type_and_reaction = (\n    (\n        (db_payload.PrompterReplyPayload, db_payload.AssistantReplyPayload),\n        protocol_schema.EmojiCode.skip_reply,\n    ),\n    (\n        (db_payload.LabelInitialPromptPayload, db_payload.LabelConversationReplyPayload),\n        protocol_schema.EmojiCode.skip_labeling,\n    ),\n    (\n        (db_payload.RankInitialPromptsPayload, db_payload.RankConversationRepliesPayload),\n        protocol_schema.EmojiCode.skip_ranking,\n    ),\n)\n\n\nclass PromptRepository:\n    def __init__(\n        self,\n        db: Session,\n        api_client: ApiClient,\n        client_user: Optional[protocol_schema.User] = None,\n        *,\n        user_repository: Optional[UserRepository] = None,\n        task_repository: Optional[TaskRepository] = None,\n        user_id: Optional[UUID] = None,\n        auth_method: Optional[str] = None,\n        username: Optional[str] = None,\n        frontend_user: Optional[FrontendUserId] = None,\n    ):\n        self.db = db\n        self.api_client = api_client\n        self.user_repository = user_repository or UserRepository(db, api_client)\n\n        if frontend_user and not auth_method and not username:\n            auth_method, username = frontend_user\n\n        if user_id:\n            self.user = self.user_repository.get_user(id=user_id)\n        elif auth_method and username:\n            self.user = self.user_repository.query_frontend_user(auth_method=auth_method, username=username)\n        else:\n            self.user = self.user_repository.lookup_client_user(client_user, create_missing=True)\n        self.user_id = self.user.id if self.user else None\n        logger.debug(f\"PromptRepository(api_client_id={self.api_client.id}, {self.user_id=})\")\n        self.task_repository = task_repository or TaskRepository(\n            db, api_client, client_user, user_repository=self.user_repository\n        )\n        self.journal = JournalWriter(db, api_client, self.user)\n\n    def ensure_user_is_enabled(self):\n        if self.user is None or self.user_id is None:\n            raise OasstError(\"User required\", OasstErrorCode.USER_NOT_SPECIFIED)\n\n        if self.user.deleted or not self.user.enabled:\n            raise OasstError(\"User account disabled\", OasstErrorCode.USER_DISABLED, HTTPStatus.SERVICE_UNAVAILABLE)\n\n        if self.user.tos_acceptance_date is None and not settings.DEBUG_IGNORE_TOS_ACCEPTANCE:\n            raise OasstError(\n                \"User has not accepted terms of service.\",\n                OasstErrorCode.USER_HAS_NOT_ACCEPTED_TOS,\n                HTTPStatus.UNAVAILABLE_FOR_LEGAL_REASONS,\n            )\n\n    def fetch_message_by_frontend_message_id(self, frontend_message_id: str, fail_if_missing: bool = True) -> Message:\n        validate_frontend_message_id(frontend_message_id)\n        message: Message = (\n            self.db.query(Message)\n            .filter(Message.api_client_id == self.api_client.id, Message.frontend_message_id == frontend_message_id)\n            .one_or_none()\n        )\n        if fail_if_missing and message is None:\n            raise OasstError(\n                f\"Message with frontend_message_id {frontend_message_id} not found.\",\n                OasstErrorCode.MESSAGE_NOT_FOUND,\n                HTTPStatus.NOT_FOUND,\n            )\n        return message\n\n    @managed_tx_method(CommitMode.FLUSH)\n    def insert_message(\n        self,\n        *,\n        message_id: UUID,\n        frontend_message_id: str,\n        parent_id: UUID,\n        message_tree_id: UUID,\n        task_id: UUID,\n        role: str,\n        payload: db_payload.MessagePayload,\n        lang: str,\n        payload_type: str = None,\n        depth: int = 0,\n        review_count: int = 0,\n        review_result: bool = None,\n        deleted: bool = False,\n    ) -> Message:\n        if payload_type is None:\n            if payload is None:\n                payload_type = \"null\"\n            else:\n                payload_type = type(payload).__name__\n\n        message = Message(\n            id=message_id,\n            parent_id=parent_id,\n            message_tree_id=message_tree_id,\n            task_id=task_id,\n            user_id=self.user_id,\n            role=role,\n            frontend_message_id=frontend_message_id,\n            api_client_id=self.api_client.id,\n            payload_type=payload_type,\n            payload=PayloadContainer(payload=payload),\n            lang=lang,\n            depth=depth,\n            review_count=review_count,\n            review_result=review_result,\n            deleted=deleted,\n        )\n        self.db.add(message)\n        return message\n\n    @managed_tx_method(CommitMode.FLUSH)\n    def insert_revision(\n        self,\n        payload: db_payload.MessagePayload,\n        message_id: UUID,\n        user_id: UUID,\n        created_date: datetime,\n    ) -> MessageRevision:\n        message_revision = MessageRevision(\n            payload=payload,\n            message_id=message_id,\n            user_id=user_id,\n            created_date=created_date,\n        )\n        self.db.add(message_revision)\n        return message_revision\n\n    def _validate_task(\n        self,\n        task: Task,\n        *,\n        task_id: Optional[UUID] = None,\n        frontend_message_id: Optional[str] = None,\n        check_ack: bool = True,\n    ) -> Task:\n        if task is None:\n            if task_id:\n                raise OasstError(f\"Task for {task_id=} not found\", OasstErrorCode.TASK_NOT_FOUND)\n            if frontend_message_id:\n                raise OasstError(f\"Task for {frontend_message_id=} not found\", OasstErrorCode.TASK_NOT_FOUND)\n            raise OasstError(\"Task not found\", OasstErrorCode.TASK_NOT_FOUND)\n\n        if task.expired:\n            raise OasstError(\"Task already expired.\", OasstErrorCode.TASK_EXPIRED)\n        if check_ack and not task.ack:\n            raise OasstError(\"Task is not acknowledged.\", OasstErrorCode.TASK_NOT_ACK)\n        if task.done:\n            raise OasstError(\"Task already done.\", OasstErrorCode.TASK_ALREADY_DONE)\n\n        if (not task.collective or task.user_id is None) and task.user_id != self.user_id:\n            logger.warning(f\"Task was assigned to a different user (expected: {task.user_id}; actual: {self.user_id}).\")\n            raise OasstError(\"Task was assigned to a different user.\", OasstErrorCode.TASK_NOT_ASSIGNED_TO_USER)\n\n        return task\n\n    def fetch_tree_state(self, message_tree_id: UUID) -> MessageTreeState:\n        return self.db.query(MessageTreeState).filter(MessageTreeState.message_tree_id == message_tree_id).one()\n\n    @managed_tx_method(CommitMode.FLUSH)\n    def store_text_reply(\n        self,\n        text: str,\n        lang: str,\n        frontend_message_id: str,\n        user_frontend_message_id: str,\n        review_count: int = 0,\n        review_result: bool = None,\n        check_tree_state: bool = True,\n        check_duplicate: bool = True,\n    ) -> Message:\n        self.ensure_user_is_enabled()\n\n        validate_frontend_message_id(frontend_message_id)\n        validate_frontend_message_id(user_frontend_message_id)\n\n        task = self.task_repository.fetch_task_by_frontend_message_id(frontend_message_id)\n        self._validate_task(task)\n\n        # If there's no parent message assume user started new conversation\n        role: str = None\n        depth: int = 0\n        deleted: bool = False\n\n        # reject whitespaces match with ^\\s+$\n        if re.match(r\"^\\s+$\", text):\n            raise OasstError(\"Message text is empty\", OasstErrorCode.TASK_MESSAGE_TEXT_EMPTY)\n\n        # ensure message size is below the predefined limit\n        if len(text) > settings.MESSAGE_SIZE_LIMIT:\n            logger.error(f\"Message size {len(text)=} exceeds size limit of {settings.MESSAGE_SIZE_LIMIT=}.\")\n            raise OasstError(\"Message size too long.\", OasstErrorCode.TASK_MESSAGE_TOO_LONG)\n\n        if check_duplicate and self.check_users_recent_replies_for_duplicates(text):\n            raise OasstError(\"User recent messages have duplicates\", OasstErrorCode.TASK_MESSAGE_DUPLICATED)\n\n        if task.parent_message_id:\n            parent_message = self.fetch_message(task.parent_message_id)\n\n            # check tree state\n            if check_tree_state:\n                # We store messages even after a tree has been completed.\n                # Although these messages will never be labeled nor ranked they should be\n                # included in the dataset because sometime users put a lot of effort into\n                # writing their reply.\n\n                ts = self.fetch_tree_state(parent_message.message_tree_id)\n                if ts.state not in (\n                    message_tree_state.State.GROWING,\n                    message_tree_state.State.RANKING,\n                    message_tree_state.State.READY_FOR_SCORING,\n                    message_tree_state.State.READY_FOR_EXPORT,\n                ):\n                    raise OasstError(\n                        \"Message insertion failed. Message tree is no longer accepting messages.\",\n                        OasstErrorCode.TREE_IN_ABORTED_STATE,\n                    )\n                if not ts.active:\n                    logger.warning(\n                        f\"Received message for inactive tree {parent_message.message_tree_id} (state='{ts.state.value}').\"\n                    )\n\n            if check_duplicate and not settings.DEBUG_ALLOW_DUPLICATE_TASKS:\n                siblings = self.fetch_message_children(task.parent_message_id, review_result=None, deleted=False)\n                if any(m.user_id == self.user_id for m in siblings):\n                    raise OasstError(\n                        \"User cannot reply twice to the same message.\",\n                        OasstErrorCode.TASK_MESSAGE_DUPLICATE_REPLY,\n                    )\n\n            parent_message.message_tree_id\n            parent_message.children_count += 1\n            self.db.add(parent_message)\n\n            depth = parent_message.depth + 1\n            deleted = parent_message.deleted\n\n        task_payload: db_payload.TaskPayload = task.payload.payload\n        if isinstance(task_payload, db_payload.InitialPromptPayload):\n            role = \"prompter\"\n        elif isinstance(task_payload, db_payload.PrompterReplyPayload):\n            role = \"prompter\"\n        elif isinstance(task_payload, db_payload.AssistantReplyPayload):\n            role = \"assistant\"\n        elif isinstance(task_payload, db_payload.SummarizationStoryPayload):\n            raise NotImplementedError(\"SummarizationStory task not implemented.\")\n        else:\n            raise OasstError(\n                f\"Unexpected task payload type: {type(task_payload).__name__}\",\n                OasstErrorCode.TASK_UNEXPECTED_PAYLOAD_TYPE_,\n            )\n\n        assert role in (\"assistant\", \"prompter\")\n\n        # create reply message\n        new_message_id = uuid4()\n        user_message = self.insert_message(\n            message_id=new_message_id,\n            frontend_message_id=user_frontend_message_id,\n            parent_id=task.parent_message_id,\n            message_tree_id=task.message_tree_id or new_message_id,\n            task_id=task.id,\n            role=role,\n            payload=db_payload.MessagePayload(text=text),\n            lang=lang or \"en\",\n            depth=depth,\n            review_count=review_count,\n            review_result=review_result,\n            deleted=deleted,\n        )\n        if not task.collective:\n            task.done = True\n            self.db.add(task)\n        self.journal.log_text_reply(task=task, message_id=new_message_id, role=role, length=len(text))\n        logger.debug(\n            f\"Inserted message id={user_message.id}, tree={user_message.message_tree_id}, user_id={user_message.user_id}, \"\n            f\"text[:100]='{user_message.text[:100]}', role='{user_message.role}', lang='{user_message.lang}'\"\n        )\n        return user_message\n\n    @managed_tx_method(CommitMode.FLUSH)\n    def revise_message(self, message_id: UUID, new_content: str):\n        # store original message as revision if not already stored\n        message = self.fetch_message(message_id)\n        if not message.edited:\n            self.insert_revision(\n                payload=message.payload,\n                message_id=message_id,\n                user_id=message.user_id,\n                created_date=message.created_date,\n            )\n\n        # store new version as revision\n        self.insert_revision(\n            payload=PayloadContainer(payload=db_payload.MessagePayload(text=new_content)),\n            message_id=message_id,\n            user_id=self.user_id,\n            created_date=utcnow(),\n        )\n\n        # update message with new content\n        updated_message_data = {\n            \"payload\": PayloadContainer(payload=db_payload.MessagePayload(text=new_content)),\n            \"edited\": True,\n            \"search_vector\": None,\n        }\n\n        query = update(Message).where(Message.id == message_id).values(**updated_message_data)\n        self.db.execute(query)\n\n    @managed_tx_method(CommitMode.FLUSH)\n    def store_rating(self, rating: protocol_schema.MessageRating) -> MessageReaction:\n        message = self.fetch_message_by_frontend_message_id(rating.message_id, fail_if_missing=True)\n\n        task = self.task_repository.fetch_task_by_frontend_message_id(rating.message_id)\n        self._validate_task(task)\n        task_payload: db_payload.RateSummaryPayload = task.payload.payload\n        if type(task_payload) != db_payload.RateSummaryPayload:\n            raise OasstError(\n                f\"Task payload type mismatch: {type(task_payload)=} != {db_payload.RateSummaryPayload}\",\n                OasstErrorCode.TASK_PAYLOAD_TYPE_MISMATCH,\n            )\n\n        if rating.rating < task_payload.scale.min or rating.rating > task_payload.scale.max:\n            raise OasstError(\n                f\"Invalid rating value: {rating.rating=} not in {task_payload.scale=}\",\n                OasstErrorCode.RATING_OUT_OF_RANGE,\n            )\n\n        # store reaction to message\n        reaction_payload = db_payload.RatingReactionPayload(rating=rating.rating)\n        reaction = self.insert_reaction(task_id=task.id, payload=reaction_payload, message_id=message.id)\n        if not task.collective:\n            task.done = True\n            self.db.add(task)\n\n        self.journal.log_rating(task, message_id=message.id, rating=rating.rating)\n        logger.info(f\"Ranking {rating.rating} stored for task {task.id}.\")\n        return reaction\n\n    @managed_tx_method(CommitMode.COMMIT)\n    def store_ranking(self, ranking: protocol_schema.MessageRanking) -> tuple[MessageReaction, Task]:\n        # fetch task\n        task = self.task_repository.fetch_task_by_frontend_message_id(ranking.message_id)\n        self._validate_task(task, frontend_message_id=ranking.message_id)\n        if not task.collective:\n            task.done = True\n            self.db.add(task)\n\n        task_payload: db_payload.RankConversationRepliesPayload | db_payload.RankInitialPromptsPayload = (\n            task.payload.payload\n        )\n\n        match type(task_payload):\n            case db_payload.RankPrompterRepliesPayload | db_payload.RankAssistantRepliesPayload:\n                # validate ranking\n                if sorted(ranking.ranking) != list(range(num_replies := len(task_payload.reply_messages))):\n                    raise OasstError(\n                        f\"Invalid ranking submitted. Each reply index must appear exactly once ({num_replies=}).\",\n                        OasstErrorCode.INVALID_RANKING_VALUE,\n                    )\n\n                last_conv_message = task_payload.conversation.messages[-1]\n                parent_msg = self.fetch_message(last_conv_message.id)\n\n                # store reaction to message\n                ranked_message_ids = [task_payload.reply_messages[i].id for i in ranking.ranking]\n                for mid in ranked_message_ids:\n                    message = self.fetch_message(mid)\n                    if message.parent_id != parent_msg.id:\n                        raise OasstError(\"Corrupt reply ranking result\", OasstErrorCode.CORRUPT_RANKING_RESULT)\n                    message.ranking_count += 1\n                    self.db.add(message)\n\n                reaction_payload = db_payload.RankingReactionPayload(\n                    ranking=ranking.ranking,\n                    ranked_message_ids=ranked_message_ids,\n                    ranking_parent_id=task_payload.ranking_parent_id,\n                    message_tree_id=task_payload.message_tree_id,\n                    not_rankable=ranking.not_rankable,\n                )\n                reaction = self.insert_reaction(task_id=task.id, payload=reaction_payload, message_id=parent_msg.id)\n                self.journal.log_ranking(task, message_id=parent_msg.id, ranking=ranking.ranking)\n\n                logger.info(f\"Ranking {ranking.ranking} stored for task {task.id}.\")\n\n            case db_payload.RankInitialPromptsPayload:\n                # validate ranking\n                if sorted(ranking.ranking) != list(range(num_prompts := len(task_payload.prompt_messages))):\n                    raise OasstError(\n                        f\"Invalid ranking submitted. Each reply index must appear exactly once ({num_prompts=}).\",\n                        OasstErrorCode.INVALID_RANKING_VALUE,\n                    )\n\n                # store reaction to message\n                ranked_message_ids = [task_payload.prompt_messages[i].id for i in ranking.ranking]\n                reaction_payload = db_payload.RankingReactionPayload(\n                    ranking=ranking.ranking, ranked_message_ids=ranked_message_ids\n                )\n                reaction = self.insert_reaction(task_id=task.id, payload=reaction_payload, message_id=None)\n                # self.journal.log_ranking(task, message_id=None, ranking=ranking.ranking)\n\n                logger.info(f\"Ranking {ranking.ranking} stored for task {task.id}.\")\n\n            case _:\n                raise OasstError(\n                    f\"task payload type mismatch: {type(task_payload)=} != {db_payload.RankConversationRepliesPayload}\",\n                    OasstErrorCode.TASK_PAYLOAD_TYPE_MISMATCH,\n                )\n\n        return reaction, task\n\n    @managed_tx_method(CommitMode.FLUSH)\n    def insert_toxicity(self, message_id: UUID, model: str, score: float, label: str) -> MessageToxicity:\n        \"\"\"Save the toxicity score of a new message in the database.\n        Args:\n            message_id (UUID): the identifier of the message we want to save its toxicity score\n            model (str): the model used for creating the toxicity score\n            score (float): the toxicity score that we obtained from the model\n            label (str): the final classification in toxicity of the model\n        Raises:\n            OasstError: if misses some of the before params\n        Returns:\n            MessageToxicity: the instance in the database of the score saved for that message\n        \"\"\"\n\n        message_toxicity = MessageToxicity(message_id=message_id, model=model, score=score, label=label)\n        self.db.add(message_toxicity)\n        return message_toxicity\n\n    @managed_tx_method(CommitMode.FLUSH)\n    def insert_message_embedding(self, message_id: UUID, model: str, embedding: list[float]) -> MessageEmbedding:\n        \"\"\"Insert the embedding of a new message in the database.\n\n        Args:\n            message_id (UUID): the identifier of the message we want to save its embedding\n            model (str): the model used for creating the embedding\n            embedding (list[float]): the values obtained from the message & model\n\n        Raises:\n            OasstError: if misses some of the before params\n\n        Returns:\n            MessageEmbedding: the instance in the database of the embedding saved for that message\n        \"\"\"\n\n        message_embedding = MessageEmbedding(message_id=message_id, model=model, embedding=embedding)\n        self.db.add(message_embedding)\n        return message_embedding\n\n    @managed_tx_method(CommitMode.FLUSH)\n    def insert_reaction(\n        self, task_id: UUID, payload: db_payload.ReactionPayload, message_id: Optional[UUID]\n    ) -> MessageReaction:\n        self.ensure_user_is_enabled()\n\n        container = PayloadContainer(payload=payload)\n        reaction = MessageReaction(\n            task_id=task_id,\n            user_id=self.user_id,\n            payload=container,\n            api_client_id=self.api_client.id,\n            payload_type=type(payload).__name__,\n            message_id=message_id,\n        )\n        self.db.add(reaction)\n        return reaction\n\n    @managed_tx_method(CommitMode.FLUSH)\n    def store_text_labels(self, text_labels: protocol_schema.TextLabels) -> tuple[TextLabels, Task, Message]:\n        self.ensure_user_is_enabled()\n\n        valid_labels: Optional[list[str]] = None\n        mandatory_labels: Optional[list[str]] = None\n        text_labels_id: Optional[UUID] = None\n        message_id: Optional[UUID] = text_labels.message_id\n\n        task: Task = None\n        if text_labels.task_id:\n            logger.debug(f\"text_labels reply has task_id {text_labels.task_id}\")\n            task = self.task_repository.fetch_task_by_id(text_labels.task_id)\n            self._validate_task(task, task_id=text_labels.task_id)\n\n            task_payload: db_payload.TaskPayload = task.payload.payload\n            if isinstance(task_payload, db_payload.LabelInitialPromptPayload):\n                if message_id and task_payload.message_id != message_id:\n                    raise OasstError(\"Task message id mismatch\", OasstErrorCode.TEXT_LABELS_WRONG_MESSAGE_ID)\n                message_id = task_payload.message_id\n                valid_labels = task_payload.valid_labels\n                mandatory_labels = task_payload.mandatory_labels\n            elif isinstance(task_payload, db_payload.LabelConversationReplyPayload):\n                if message_id and message_id != message_id:\n                    raise OasstError(\"Task message id mismatch\", OasstErrorCode.TEXT_LABELS_WRONG_MESSAGE_ID)\n                message_id = task_payload.message_id\n                valid_labels = task_payload.valid_labels\n                mandatory_labels = task_payload.mandatory_labels\n            else:\n                raise OasstError(\n                    \"Unexpected text_labels task payload\",\n                    OasstErrorCode.TASK_PAYLOAD_TYPE_MISMATCH,\n                )\n\n            logger.debug(f\"text_labels reply: {valid_labels=}, {mandatory_labels=}\")\n\n            if valid_labels:\n                if not all([label in valid_labels for label in text_labels.labels.keys()]):\n                    raise OasstError(\"Invalid text label specified\", OasstErrorCode.TEXT_LABELS_INVALID_LABEL)\n\n            if isinstance(mandatory_labels, list):\n                mandatory_set = set(mandatory_labels)\n                if not mandatory_set.issubset(text_labels.labels.keys()):\n                    missing = \", \".join(mandatory_set - text_labels.labels.keys())\n                    raise OasstError(\n                        f\"Mandatory text labels missing: {missing}\", OasstErrorCode.TEXT_LABELS_MANDATORY_LABEL_MISSING\n                    )\n\n            text_labels_id = task.id  # associate with task by sharing the id\n\n            if not task.collective:\n                task.done = True\n                self.db.add(task)\n\n        logger.debug(f\"inserting TextLabels for {message_id=}, {text_labels_id=}\")\n        model = TextLabels(\n            id=text_labels_id,\n            api_client_id=self.api_client.id,\n            message_id=message_id,\n            user_id=self.user_id,\n            text=text_labels.text,\n            labels=text_labels.labels,\n            task_id=task.id if task else None,\n        )\n\n        message: Message = None\n        if message_id:\n            if not task:\n                # free labeling case\n\n                if text_labels.is_report is True:\n                    message = self.handle_message_emoji(\n                        message_id, protocol_schema.EmojiOp.add, protocol_schema.EmojiCode.red_flag\n                    )\n\n                    message_details = {\n                        \"message_id\": message_id,\n                        \"message_text\": message.text[:500] + \"...\" if len(message.text) > 500 else message.text,\n                        \"role\": message.role.upper(),\n                        \"lang\": message.lang.upper(),\n                        \"thumbs_up\": message.emojis.get(\"+1\") or 0,\n                        \"thumbs_down\": message.emojis.get(\"-1\") or 0,\n                        \"red_flag\": message.emojis.get(\"red_flag\") or 0,\n                    }\n\n                    send_new_report_message.delay(\n                        message_details=message_details, label_text=text_labels.text, user_id=self.user_id\n                    )\n\n                # update existing record for repeated updates (same user no task associated)\n                existing_text_label = self.fetch_non_task_text_labels(message_id, self.user_id)\n                if existing_text_label is not None:\n                    existing_text_label.labels = text_labels.labels\n                    model = existing_text_label\n\n            else:\n                # task based labeling case\n\n                message = self.fetch_message(message_id, fail_if_missing=True)\n                if not settings.DEBUG_ALLOW_SELF_LABELING and message.user_id == self.user_id:\n                    raise OasstError(\n                        \"Labeling own message is not allowed.\", OasstErrorCode.TEXT_LABELS_NO_SELF_LABELING\n                    )\n\n                existing_labels = self.fetch_message_text_labels(message_id, self.user_id)\n                if not settings.DEBUG_ALLOW_DUPLICATE_TASKS and any(l.task_id for l in existing_labels):\n                    raise OasstError(\n                        \"Message was already labeled by same user before.\",\n                        OasstErrorCode.TEXT_LABELS_DUPLICATE_TASK_REPLY,\n                    )\n\n                message.review_count += 1\n                self.db.add(message)\n\n        self.db.add(model)\n        return model, task, message\n\n    def fetch_random_message_tree(\n        self,\n        require_role: str = None,\n        review_result: Optional[bool] = True,\n        deleted: Optional[bool] = False,\n    ) -> list[Message]:\n        \"\"\"\n        Loads all messages of a random message_tree.\n\n        :param require_role: If set loads only message_tree which has\n            at least one message with given role.\n        \"\"\"\n        distinct_message_trees = self.db.query(Message.message_tree_id).distinct(Message.message_tree_id)\n        if require_role:\n            distinct_message_trees = distinct_message_trees.filter(Message.role == require_role)\n        if review_result is not None:\n            distinct_message_trees = distinct_message_trees.filter(Message.review_result == review_result)\n        distinct_message_trees = distinct_message_trees.subquery()\n\n        random_message_tree_id = self.db.query(distinct_message_trees).order_by(func.random()).limit(1).scalar()\n        if random_message_tree_id:\n            return self.fetch_message_tree(random_message_tree_id, review_result=review_result, deleted=deleted)\n        return None\n\n    def fetch_random_conversation(\n        self,\n        last_message_role: str = None,\n        message_tree_id: Optional[UUID] = None,\n        review_result: Optional[bool] = True,\n        deleted: Optional[bool] = False,\n    ) -> list[Message]:\n        \"\"\"\n        Picks a random linear conversation starting from any root message\n        and ending somewhere in the message_tree, possibly at the root itself.\n\n        :param last_message_role: If set will form a conversation ending with a message\n            created by this role. Necessary for the tasks like \"user_reply\" where\n            the user should reply as a human and hence the last message of the conversation\n            needs to have \"assistant\" role.\n        \"\"\"\n        if message_tree_id:\n            messages_tree = self.fetch_message_tree(message_tree_id, review_result=review_result, deleted=deleted)\n        else:\n            messages_tree = self.fetch_random_message_tree(\n                last_message_role, review_result=review_result, deleted=deleted\n            )\n        if not messages_tree:\n            raise OasstError(\"No message tree found\", OasstErrorCode.NO_MESSAGE_TREE_FOUND)\n\n        if last_message_role:\n            conv_messages = [m for m in messages_tree if m.role == last_message_role]\n            conv_messages = [random.choice(conv_messages)]\n        else:\n            conv_messages = [random.choice(messages_tree)]\n        messages_tree = {m.id: m for m in messages_tree}\n\n        while True:\n            if not conv_messages[-1].parent_id:\n                # reached the start of the conversation\n                break\n\n            parent_message = messages_tree[conv_messages[-1].parent_id]\n            conv_messages.append(parent_message)\n\n        return list(reversed(conv_messages))\n\n    def fetch_random_initial_prompts(self, size: int = 5):\n        messages = self.db.query(Message).filter(Message.parent_id.is_(None)).order_by(func.random()).limit(size).all()\n        return messages\n\n    def fetch_message_tree(\n        self,\n        message_tree_id: UUID,\n        review_result: Optional[bool] = True,\n        deleted: Optional[bool] = False,\n    ) -> list[Message]:\n        qry = self.db.query(Message).filter(Message.message_tree_id == message_tree_id)\n        if review_result is not None:\n            qry = qry.filter(Message.review_result == review_result)\n        if deleted is not None:\n            qry = qry.filter(Message.deleted == deleted)\n        return self._add_user_emojis_all(qry)\n\n    def check_users_recent_replies_for_duplicates(self, text: str) -> bool:\n        \"\"\"\n        Checks if the user has recently replied with the same text within a given time period.\n        \"\"\"\n\n        user_id = self.user_id\n        logger.debug(f\"Checking for duplicate tasks for user {user_id}\")\n        # messages in the past 24 hours\n        messages = (\n            self.db.query(Message)\n            .filter(Message.user_id == user_id)\n            .order_by(Message.created_date.desc())\n            .filter(\n                Message.created_date > utcnow() - timedelta(minutes=settings.DUPLICATE_MESSAGE_FILTER_WINDOW_MINUTES)\n            )\n            .all()\n        )\n        if not messages:\n            return False\n        for msg in messages:\n            if msg.text == text:\n                return True\n        return False\n\n    def fetch_user_message_trees(\n        self, user_id: Message.user_id, reviewed: bool = True, include_deleted: bool = False\n    ) -> list[Message]:\n        qry = self.db.query(Message).filter(Message.user_id == user_id)\n        if reviewed:\n            qry = qry.filter(Message.review_result)\n        if not include_deleted:\n            qry = qry.filter(not_(Message.deleted))\n        return self._add_user_emojis_all(qry)\n\n    def fetch_multiple_random_replies(self, max_size: int = 5, message_role: str = None):\n        \"\"\"\n        Fetch a conversation with multiple possible replies to it.\n\n        This function finds a random message with >1 replies,\n        forms a conversation from the corresponding message tree root up to this message\n        and fetches up to max_size possible replies in continuation to this conversation.\n        \"\"\"\n        parent = self.db.query(Message.id).filter(Message.children_count > 1)\n        if message_role:\n            parent = parent.filter(Message.role == message_role)\n\n        parent = parent.order_by(func.random()).limit(1)\n        replies = (\n            self.db.query(Message).filter(Message.parent_id.in_(parent)).order_by(func.random()).limit(max_size).all()\n        )\n        if not replies:\n            raise OasstError(\"No replies found\", OasstErrorCode.NO_REPLIES_FOUND)\n\n        message_tree = self.fetch_message_tree(replies[0].message_tree_id)\n        message_tree = {p.id: p for p in message_tree}\n        conversation = [message_tree[replies[0].parent_id]]\n        while True:\n            if not conversation[-1].parent_id:\n                # reached start of the conversation\n                break\n\n            parent_message = message_tree[conversation[-1].parent_id]\n            conversation.append(parent_message)\n\n        conversation = reversed(conversation)\n\n        return conversation, replies\n\n    def fetch_message(self, message_id: UUID, fail_if_missing: bool = True) -> Optional[Message]:\n        qry = self.db.query(Message).filter(Message.id == message_id)\n        messages = self._add_user_emojis_all(qry)\n        message = messages[0] if messages else None\n\n        message = self.db.query(Message).filter(Message.id == message_id).one_or_none()\n        if fail_if_missing and not message:\n            raise OasstError(\"Message not found\", OasstErrorCode.MESSAGE_NOT_FOUND, HTTPStatus.NOT_FOUND)\n        return message\n\n    def fetch_non_task_text_labels(self, message_id: UUID, user_id: UUID) -> Optional[TextLabels]:\n        query = (\n            self.db.query(TextLabels)\n            .outerjoin(Task, Task.id == TextLabels.id)\n            .filter(Task.id.is_(None), TextLabels.message_id == message_id, TextLabels.user_id == user_id)\n        )\n        text_label = query.one_or_none()\n        return text_label\n\n    def fetch_message_text_labels(self, message_id: UUID, user_id: Optional[UUID] = None) -> list[TextLabels]:\n        query = self.db.query(TextLabels).filter(TextLabels.message_id == message_id)\n        if user_id is not None:\n            query = query.filter(TextLabels.user_id == user_id)\n        return query.all()\n\n    def fetch_message_revision_history(self, message_id: UUID) -> list[MessageRevision]:\n        # the revisions are sorted by time using the uuid7 id\n        revisions: list[MessageRevision] = sorted(\n            self.db.query(MessageRevision).filter(MessageRevision.message_id == message_id).all(),\n            key=lambda revision: revision.id.int >> 80,\n        )\n        for revision in revisions:\n            revision._user_is_author = self.user_id == revision.user_id\n        return revisions\n\n    @staticmethod\n    def trace_conversation(messages: list[Message] | dict[UUID, Message], last_message: Message) -> list[Message]:\n        \"\"\"\n        Pick messages from a collection so that the result makes a linear conversation\n        starting from a message tree root and up to the given message.\n        Returns an ordered list of messages starting from the message tree root.\n        \"\"\"\n        if isinstance(messages, list):\n            messages = {m.id: m for m in messages}\n        if not isinstance(messages, dict):\n            # This should not normally happen\n            raise OasstError(\"Server error\", OasstErrorCode.SERVER_ERROR0, HTTPStatus.INTERNAL_SERVER_ERROR)\n\n        conv = [last_message]\n        while conv[-1].parent_id:\n            if conv[-1].parent_id not in messages:\n                # Can't form a continuous conversation\n                logger.error(\n                    f\"Broken conversation: parent of message (id={conv[-1].id}, parent_id={conv[-1].parent_id}) not found in result set\"\n                )\n                raise OasstError(\n                    \"Broken conversation\", OasstErrorCode.BROKEN_CONVERSATION, HTTPStatus.INTERNAL_SERVER_ERROR\n                )\n\n            parent_message = messages[conv[-1].parent_id]\n            conv.append(parent_message)\n\n        return list(reversed(conv))\n\n    def fetch_message_conversation(self, message: Message | UUID) -> list[Message]:\n        \"\"\"\n        Fetch a conversation from the tree root and up to this message.\n        \"\"\"\n        if isinstance(message, UUID):\n            message = self.fetch_message(message)\n\n        tree_messages = self.fetch_message_tree(message.message_tree_id)\n        return self.trace_conversation(tree_messages, message)\n\n    def fetch_tree_from_message(\n        self,\n        message: Message | UUID,\n        review_result: Optional[bool] = True,\n        deleted: Optional[bool] = False,\n    ) -> list[Message]:\n        \"\"\"\n        Fetch message tree this message belongs to.\n        \"\"\"\n        if isinstance(message, UUID):\n            message = self.fetch_message(message)\n        logger.debug(f\"fetch_message_tree({message.message_tree_id=})\")\n        return self.fetch_message_tree(message.message_tree_id, review_result=review_result, deleted=deleted)\n\n    def fetch_message_children(\n        self,\n        message: Message | UUID,\n        review_result: Optional[bool] = True,\n        deleted: Optional[bool] = False,\n    ) -> list[Message]:\n        \"\"\"\n        Get all direct children of this message\n        \"\"\"\n        if isinstance(message, Message):\n            message = message.id\n\n        qry = self.db.query(Message).filter(Message.parent_id == message)\n        if review_result is not None:\n            qry = qry.filter(Message.review_result == review_result)\n        if deleted is not None:\n            qry = qry.filter(Message.deleted == deleted)\n        children = self._add_user_emojis_all(qry)\n        return children\n\n    def fetch_message_siblings(\n        self,\n        message: Message | UUID,\n        review_result: Optional[bool] = True,\n        deleted: Optional[bool] = False,\n    ) -> list[Message]:\n        \"\"\"\n        Get siblings of a message (other messages with the same parent_id)\n        \"\"\"\n        qry = self.db.query(Message)\n        if isinstance(message, Message):\n            qry = qry.filter(Message.parent_id == message.parent_id)\n        else:\n            parent_qry = self.db.query(Message.parent_id).filter(Message.id == message).subquery()\n            qry = qry.filter(Message.parent_id == parent_qry.c.parent_id)\n\n        if review_result is not None:\n            qry = qry.filter(Message.review_result == review_result)\n        if deleted is not None:\n            qry = qry.filter(Message.deleted == deleted)\n        siblings = self._add_user_emojis_all(qry)\n        return siblings\n\n    @staticmethod\n    def trace_descendants(root: Message, messages: list[Message]) -> list[Message]:\n        children = defaultdict(list)\n        for msg in messages:\n            children[msg.parent_id].append(msg)\n\n        def _traverse_subtree(m: Message):\n            for child in children[m.id]:\n                yield child\n                yield from _traverse_subtree(child)\n\n        return list(_traverse_subtree(root))\n\n    def fetch_message_descendants(self, message: Message | UUID, max_depth: int = None) -> list[Message]:\n        \"\"\"\n        Find all descendant messages to this message.\n\n        This function creates a subtree of messages starting from given root message.\n        \"\"\"\n        if isinstance(message, UUID):\n            message = self.fetch_message(message)\n\n        desc = self.db.query(Message).filter(\n            Message.message_tree_id == message.message_tree_id, Message.depth > message.depth\n        )\n        if max_depth is not None:\n            desc = desc.filter(Message.depth <= max_depth)\n\n        desc = self._add_user_emojis_all(desc)\n\n        return self.trace_descendants(message, desc)\n\n    def fetch_longest_conversation(self, message: Message | UUID) -> list[Message]:\n        tree = self.fetch_tree_from_message(message)\n        max_message = max(tree, key=lambda m: m.depth)\n        return self.trace_conversation(tree, max_message)\n\n    def fetch_message_with_max_children(self, message: Message | UUID) -> tuple[Message, list[Message]]:\n        tree = self.fetch_tree_from_message(message)\n        max_message = max(tree, key=lambda m: m.children_count)\n        return max_message, [m for m in tree if m.parent_id == max_message.id]\n\n    def _add_user_emojis_all(self, qry: Query, include_user: bool = False) -> list[Message]:\n        if self.user_id is None:\n            if not include_user:\n                return qry.all()\n\n            messages: list[Message] = []\n\n            for element in qry:\n                message = element[\"Message\"]\n                user = element[\"User\"]\n                message._user = user\n                messages.append(message)\n            return messages\n\n        order_by_clauses = qry._order_by_clauses\n        sq = qry.subquery(\"m\")\n        select_entities = [Message, func.string_agg(MessageEmoji.emoji, literal_column(\"','\")).label(\"user_emojis\")]\n        if include_user:\n            select_entities.append(User)\n        qry = (\n            self.db.query(*select_entities)\n            .select_entity_from(sq)\n            .outerjoin(\n                MessageEmoji,\n                and_(\n                    sq.c.id == MessageEmoji.message_id,\n                    MessageEmoji.user_id == self.user_id,\n                    sq.c.emojis != JSON.NULL,\n                ),\n            )\n            .group_by(sq)\n        )\n        qry._order_by_clauses = order_by_clauses\n        messages: list[Message] = []\n        for x in qry:\n            m: Message = x.Message\n            user_emojis = x[\"user_emojis\"]\n            if user_emojis:\n                m._user_emojis = user_emojis.split(\",\")\n            m._user_is_author = self.user_id and self.user_id == m.user_id\n            if include_user:\n                m._user = x[\"User\"]\n            messages.append(m)\n\n        return messages\n\n    def query_messages_ordered_by_created_date(\n        self,\n        user_id: Optional[UUID] = None,\n        auth_method: Optional[str] = None,\n        username: Optional[str] = None,\n        api_client_id: Optional[UUID] = None,\n        gte_created_date: Optional[datetime] = None,\n        gt_id: Optional[UUID] = None,\n        lte_created_date: Optional[datetime] = None,\n        lt_id: Optional[UUID] = None,\n        only_roots: bool = False,\n        deleted: Optional[bool] = None,\n        review_result: Optional[bool] = None,\n        desc: bool = False,\n        limit: Optional[int] = 100,\n        search_query: Optional[str] = None,\n        lang: Optional[str] = None,\n        include_user: Optional[bool] = None,\n    ) -> list[Message]:\n        if not self.api_client.trusted:\n            if not api_client_id:\n                # Let unprivileged api clients query their own messages without api_client_id being set\n                api_client_id = self.api_client.id\n\n            if api_client_id != self.api_client.id:\n                # Unprivileged api client asks for foreign messages\n                raise OasstError(\"Forbidden\", OasstErrorCode.API_CLIENT_NOT_AUTHORIZED, HTTPStatus.FORBIDDEN)\n\n        qry = self.db.query(Message)\n        if include_user:\n            qry = self.db.query(Message, User)\n        if user_id:\n            qry = qry.filter(Message.user_id == user_id)\n        if username or auth_method or include_user:\n            qry = qry.join(User)\n        if username or auth_method:\n            if not (username and auth_method):\n                raise OasstError(\"Auth method or username missing.\", OasstErrorCode.AUTH_AND_USERNAME_REQUIRED)\n            qry = qry.filter(User.username == username, User.auth_method == auth_method)\n        if api_client_id:\n            qry = qry.filter(Message.api_client_id == api_client_id)\n\n        gte_created_date = unaware_to_utc(gte_created_date)\n        lte_created_date = unaware_to_utc(lte_created_date)\n\n        if gte_created_date is not None:\n            if gt_id:\n                qry = qry.filter(\n                    or_(\n                        Message.created_date > gte_created_date,\n                        and_(Message.created_date == gte_created_date, Message.id > gt_id),\n                    )\n                )\n            else:\n                qry = qry.filter(Message.created_date >= gte_created_date)\n        elif gt_id:\n            raise OasstError(\"Need id and date for keyset pagination\", OasstErrorCode.GENERIC_ERROR)\n\n        if lte_created_date is not None:\n            if lt_id:\n                qry = qry.filter(\n                    or_(\n                        Message.created_date < lte_created_date,\n                        and_(Message.created_date == lte_created_date, Message.id < lt_id),\n                    )\n                )\n            else:\n                qry = qry.filter(Message.created_date <= lte_created_date)\n        elif lt_id:\n            raise OasstError(\"Need id and date for keyset pagination\", OasstErrorCode.GENERIC_ERROR)\n\n        if only_roots:\n            qry = qry.filter(Message.parent_id.is_(None))\n\n        if deleted is not None:\n            qry = qry.filter(Message.deleted == deleted)\n\n        if review_result is not None:\n            qry = qry.filter(Message.review_result == review_result)\n\n        if lang is not None:\n            qry = qry.filter(Message.lang == lang)\n\n            if search_query is not None:\n                qry = qry.filter(\n                    Message.search_vector.match(\n                        search_query,\n                        postgresql_regconfig=db_lang_to_postgres_ts_lang(lang),\n                    ),\n                )\n\n        if desc:\n            qry = qry.order_by(Message.created_date.desc(), Message.id.desc())\n        else:\n            qry = qry.order_by(Message.created_date.asc(), Message.id.asc())\n\n        if limit is not None:\n            qry = qry.limit(limit)\n\n        return self._add_user_emojis_all(qry, include_user=include_user)\n\n    def update_children_counts(self, message_tree_id: UUID):\n        sql_update_children_count = \"\"\"\nUPDATE message SET children_count = cc.children_count\nFROM (\n    SELECT m.id, count(c.id) - COALESCE(SUM(c.deleted::int), 0) AS children_count\n    FROM message m\n        LEFT JOIN message c ON m.id = c.parent_id\n    WHERE m.message_tree_id  = :message_tree_id\n    GROUP BY m.id\n) AS cc\nWHERE message.id = cc.id;\n\"\"\"\n        self.db.execute(text(sql_update_children_count), {\"message_tree_id\": message_tree_id})\n\n    @managed_tx_method(CommitMode.COMMIT)\n    def mark_messages_deleted(self, messages: Message | UUID | list[Message | UUID], recursive: bool = True):\n        \"\"\"\n        Marks deleted messages and all their descendants.\n        \"\"\"\n        if isinstance(messages, (Message, UUID)):\n            messages = [messages]\n\n        ids = []\n        for message in messages:\n            if isinstance(message, UUID):\n                ids.append(message)\n            elif isinstance(message, Message):\n                ids.append(message.id)\n            else:\n                raise OasstError(\"Server error\", OasstErrorCode.SERVER_ERROR1, HTTPStatus.INTERNAL_SERVER_ERROR)\n\n        query = update(Message).where(Message.id.in_(ids)).values(deleted=True)\n        self.db.execute(query)\n\n        parent_ids = ids\n        if recursive:\n            while parent_ids:\n                query = (\n                    update(Message).filter(Message.parent_id.in_(parent_ids)).values(deleted=True).returning(Message.id)\n                )\n\n                parent_ids = self.db.execute(query).scalars().all()\n\n    @managed_tx_method(CommitMode.COMMIT)\n    def undelete_deleted_message(self, message: Message | UUID):\n        \"\"\"\n        Undelete deleted messages and all their parents.\n        \"\"\"\n        message_id = None\n        if isinstance(message, UUID):\n            message_id = message\n        elif isinstance(message, Message):\n            message_id = message.id\n        else:\n            raise OasstError(\"Server error\", OasstErrorCode.SERVER_ERROR1, HTTPStatus.INTERNAL_SERVER_ERROR)\n\n        query = update(Message).where(Message.id == message_id).values(deleted=False)\n        self.db.execute(query)\n\n        parent_id = None\n        if isinstance(message, UUID):\n            parent_id = self.db.query(Message.parent_id).where(Message.id == message_id).first()[0]\n        elif isinstance(message, Message):\n            parent_id = message.parent_id\n\n        if parent_id is None:\n            return\n\n        # Fetching the entire parent_message so there is no parent_id query executed after\n        parent_message: Message = self.db.query(Message).where(Message.id == parent_id).first()\n\n        if parent_message is not None:\n            self.undelete_deleted_message(parent_message)\n\n    def get_stats(self) -> SystemStats:\n        \"\"\"\n        Get data stats such as number of all messages in the system,\n        number of deleted and active messages and number of message trees.\n        \"\"\"\n        # With columns: lang, deleted, count\n        group_count_query = self.db.query(Message.lang, Message.deleted, func.count()).group_by(\n            Message.lang, Message.deleted\n        )\n        # With columns: None, None, count\n        msg_tree_query = self.db.query(None, None, func.count(Message.id)).filter(Message.parent_id.is_(None))\n        # Union both queries, so that we can fetch the counts in one database query\n        query = group_count_query.union_all(msg_tree_query)\n\n        nactives = 0\n        ndeleted = 0\n        nactives_by_lang = {}\n        nthreads = 0\n\n        for lang, deleted, count in query.all():\n            if lang is None:  # corresponds to msg_tree_query\n                nthreads = count\n                continue\n            if deleted is False:  # corresponds to group_count_query (lang, deleted=False)\n                nactives_by_lang[lang] = count\n                nactives += count\n            else:  # corresponds to group_count_query (lang, deleted=True)\n                ndeleted += count\n\n        return SystemStats(\n            all=nactives + ndeleted,\n            active=nactives,\n            active_by_lang=nactives_by_lang,\n            deleted=ndeleted,\n            message_trees=nthreads,\n        )\n\n    @managed_tx_method()\n    def skip_task(self, task_id: UUID, reason: Optional[str]):\n        self.ensure_user_is_enabled()\n\n        task = self.task_repository.fetch_task_by_id(task_id)\n        self._validate_task(task, check_ack=False)\n\n        if not task.collective:\n            task.skipped = True\n            task.skip_reason = reason\n            self.db.add(task)\n\n        def handle_cancel_emoji(task_payload: db_payload.TaskPayload) -> Message | None:\n            for types, emoji in _task_type_and_reaction:\n                for t in types:\n                    if isinstance(task_payload, t):\n                        return self.handle_message_emoji(task.parent_message_id, protocol_schema.EmojiOp.add, emoji)\n            return None\n\n        task_payload: db_payload.TaskPayload = task.payload.payload\n        handle_cancel_emoji(task_payload)\n\n    def handle_message_emoji(\n        self, message_id: UUID, op: protocol_schema.EmojiOp, emoji: protocol_schema.EmojiCode\n    ) -> Message:\n        self.ensure_user_is_enabled()\n\n        message = self.fetch_message(message_id)\n\n        # check if emoji exists\n        existing_emoji = (\n            self.db.query(MessageEmoji)\n            .filter(\n                MessageEmoji.message_id == message_id, MessageEmoji.user_id == self.user_id, MessageEmoji.emoji == emoji\n            )\n            .one_or_none()\n        )\n\n        if existing_emoji:\n            if op == protocol_schema.EmojiOp.add:\n                logger.info(f\"Emoji record already exists {message_id=}, {emoji=}, {self.user_id=}\")\n                return message\n            elif op == protocol_schema.EmojiOp.togggle:\n                op = protocol_schema.EmojiOp.remove\n\n        if existing_emoji is None:\n            if op == protocol_schema.EmojiOp.remove:\n                logger.info(f\"Emoji record not found {message_id=}, {emoji=}, {self.user_id=}\")\n                return message\n            elif op == protocol_schema.EmojiOp.togggle:\n                op = protocol_schema.EmojiOp.add\n\n        if op == protocol_schema.EmojiOp.add:\n            # hard coded exclusivity of thumbs_up & thumbs_down\n            if emoji == protocol_schema.EmojiCode.thumbs_up and message.has_user_emoji(\n                protocol_schema.EmojiCode.thumbs_down.value\n            ):\n                message = self.handle_message_emoji(\n                    message_id, protocol_schema.EmojiOp.remove, protocol_schema.EmojiCode.thumbs_down\n                )\n            elif emoji == protocol_schema.EmojiCode.thumbs_down and message.has_user_emoji(\n                protocol_schema.EmojiCode.thumbs_up.value\n            ):\n                message = self.handle_message_emoji(\n                    message_id, protocol_schema.EmojiOp.remove, protocol_schema.EmojiCode.thumbs_up\n                )\n\n            if message.user_id == self.user_id and emoji in (\n                protocol_schema.EmojiCode.thumbs_up,\n                protocol_schema.EmojiCode.thumbs_down,\n            ):\n                logger.debug(f\"Ignoring add emoji op for user's own message ({emoji=})\")\n                return message\n\n            # Add to flagged_message table if the red flag emoji is applied\n            if emoji == protocol_schema.EmojiCode.red_flag:\n                flagged_message = FlaggedMessage(message_id=message_id, processed=False, created_date=utcnow())\n                insert_stmt = pg.insert(FlaggedMessage).values(**flagged_message.dict())\n                upsert_stmt = insert_stmt.on_conflict_do_update(\n                    constraint=\"flagged_message_pkey\", set_=flagged_message.dict()\n                )\n                self.db.execute(upsert_stmt)\n\n            # insert emoji record & increment count\n            message_emoji = MessageEmoji(message_id=message.id, user_id=self.user_id, emoji=emoji)\n            self.db.add(message_emoji)\n            emoji_counts = message.emojis\n            if not emoji_counts:\n                message.emojis = {emoji.value: 1}\n            else:\n                count = emoji_counts.get(emoji.value) or 0\n                emoji_counts[emoji.value] = count + 1\n            if message._user_emojis is None:\n                message._user_emojis = []\n            if emoji.value not in message._user_emojis:\n                message._user_emojis.append(emoji.value)\n        elif op == protocol_schema.EmojiOp.remove:\n            # remove emoji record and & decrement count\n            message = self.fetch_message(message_id)\n            if message._user_emojis and emoji.value in message._user_emojis:\n                message._user_emojis.remove(emoji.value)\n            self.db.delete(existing_emoji)\n            emoji_counts = message.emojis\n            count = emoji_counts.get(emoji.value)\n            if count is not None:\n                if count == 1:\n                    del emoji_counts[emoji.value]\n                else:\n                    emoji_counts[emoji.value] = count - 1\n                flag_modified(message, \"emojis\")\n                self.db.add(message)\n        else:\n            raise OasstError(\"Emoji op not supported\", OasstErrorCode.EMOJI_OP_UNSUPPORTED)\n\n        flag_modified(message, \"emojis\")\n        self.db.add(message)\n        self.db.flush()\n        return message\n\n    def fetch_flagged_messages(self, max_count: Optional[int]) -> list[FlaggedMessage]:\n        qry = self.db.query(FlaggedMessage)\n        if max_count is not None:\n            qry = qry.limit(max_count)\n\n        return qry.all()\n\n    def fetch_flagged_messages_by_created_date(\n        self,\n        gte_created_date: Optional[datetime] = None,\n        gt_id: Optional[UUID] = None,\n        lte_created_date: Optional[datetime] = None,\n        lt_id: Optional[UUID] = None,\n        desc: bool = False,\n        limit: Optional[int] = 100,\n    ) -> list[FlaggedMessage]:\n        qry = self.db.query(FlaggedMessage)\n\n        if gte_created_date is not None:\n            if gt_id:\n                qry = qry.filter(\n                    or_(\n                        FlaggedMessage.created_date > gte_created_date,\n                        and_(FlaggedMessage.created_date == gte_created_date, FlaggedMessage.message_id > gt_id),\n                    )\n                )\n            else:\n                qry = qry.filter(FlaggedMessage.created_date >= gte_created_date)\n        elif gt_id:\n            raise OasstError(\"Need id and date for keyset pagination\", OasstErrorCode.GENERIC_ERROR)\n\n        if lte_created_date is not None:\n            if lt_id:\n                qry = qry.filter(\n                    or_(\n                        FlaggedMessage.created_date < lte_created_date,\n                        and_(FlaggedMessage.created_date == lte_created_date, FlaggedMessage.message_id < lt_id),\n                    )\n                )\n            else:\n                qry = qry.filter(FlaggedMessage.created_date <= lte_created_date)\n        elif lt_id:\n            raise OasstError(\"Need id and date for keyset pagination\", OasstErrorCode.GENERIC_ERROR)\n\n        if desc:\n            qry = qry.order_by(FlaggedMessage.created_date.desc(), FlaggedMessage.message_id.desc())\n        else:\n            qry = qry.order_by(FlaggedMessage.created_date.asc(), FlaggedMessage.message_id.asc())\n\n        if limit is not None:\n            qry = qry.limit(limit)\n\n        return qry.all()\n\n    def process_flagged_message(self, message_id: UUID) -> FlaggedMessage:\n        message = self.db.query(FlaggedMessage).get(message_id)\n\n        if not message:\n            raise OasstError(\"Message not found\", OasstErrorCode.MESSAGE_NOT_FOUND, HTTPStatus.NOT_FOUND)\n\n        message.processed = True\n        self.db.commit()\n        self.db.refresh(message)\n\n        return message\n", "backend/oasst_backend/config.py": "from pathlib import Path\nfrom typing import Any, Dict, List, Optional\n\nfrom oasst_shared.schemas.protocol import TextLabel\nfrom pydantic import AnyHttpUrl, BaseModel, BaseSettings, FilePath, PostgresDsn, validator\n\n\nclass TreeManagerConfiguration(BaseModel):\n    \"\"\"TreeManager configuration settings\"\"\"\n\n    max_active_trees: int = 10\n    \"\"\"Maximum number of concurrently active message trees in the database.\n    No new initial prompt tasks are handed out to users if this\n    number is reached.\"\"\"\n\n    max_initial_prompt_review: int = 100\n    \"\"\"Maximum number of initial prompts under review before no more initial prompt tasks will be handed out.\"\"\"\n\n    max_tree_depth: int = 3\n    \"\"\"Maximum depth of message tree.\"\"\"\n\n    max_children_count: int = 3\n    \"\"\"Maximum number of reply messages per tree node.\"\"\"\n\n    num_prompter_replies: int = 1\n    \"\"\"Number of prompter replies to collect per assistant reply.\"\"\"\n\n    goal_tree_size: int = 12\n    \"\"\"Total number of messages to gather per tree.\"\"\"\n\n    random_goal_tree_size: bool = False\n    \"\"\"If set to true goal tree sizes will be generated randomly within range [min_goal_tree_size, goal_tree_size].\"\"\"\n\n    min_goal_tree_size: int = 5\n    \"\"\"Minimum tree size for random goal sizes.\"\"\"\n\n    num_reviews_initial_prompt: int = 3\n    \"\"\"Number of peer review checks to collect in INITIAL_PROMPT_REVIEW state.\"\"\"\n\n    num_reviews_reply: int = 3\n    \"\"\"Number of peer review checks to collect per reply (other than initial_prompt).\"\"\"\n\n    auto_mod_enabled: bool = True\n    \"\"\"Flag to enable/disable auto moderation.\"\"\"\n\n    auto_mod_max_skip_reply: int = 25\n    \"\"\"Automatically set tree state to `halted_by_moderator` when more than the specified number\n    of users skip replying to a message. (auto moderation)\"\"\"\n\n    auto_mod_red_flags: int = 4\n    \"\"\"Delete messages that receive more than this number of red flags if it is a reply or\n    set the tree to `aborted_low_grade` when a prompt is flagged. (auto moderation)\"\"\"\n\n    p_full_labeling_review_prompt: float = 1.0\n    \"\"\"Probability of full text-labeling (instead of mandatory only) for initial prompts.\"\"\"\n\n    p_full_labeling_review_reply_assistant: float = 1.0\n    \"\"\"Probability of full text-labeling (instead of mandatory only) for assistant replies.\"\"\"\n\n    p_full_labeling_review_reply_prompter: float = 0.25\n    \"\"\"Probability of full text-labeling (instead of mandatory only) for prompter replies.\"\"\"\n\n    acceptance_threshold_initial_prompt: float = 0.6\n    \"\"\"Threshold for accepting an initial prompt.\"\"\"\n\n    acceptance_threshold_reply: float = 0.6\n    \"\"\"Threshold for accepting a reply.\"\"\"\n\n    num_required_rankings: int = 3\n    \"\"\"Number of rankings in which the message participated.\"\"\"\n\n    p_activate_backlog_tree: float = 0.1\n    \"\"\"Probability to activate a message tree in BACKLOG_RANKING state when another tree enters\n    a terminal state.\"\"\"\n\n    min_active_rankings_per_lang: int = 0\n    \"\"\"When the number of active ranking tasks is below this value when a tree enters a terminal\n    state an available trees in BACKLOG_RANKING will be activated (i.e. enters the RANKING state).\"\"\"\n\n    labels_initial_prompt: list[TextLabel] = [\n        TextLabel.spam,\n        TextLabel.lang_mismatch,\n        TextLabel.quality,\n        TextLabel.creativity,\n        TextLabel.humor,\n        TextLabel.toxicity,\n        TextLabel.violence,\n        TextLabel.not_appropriate,\n        TextLabel.pii,\n        TextLabel.hate_speech,\n        TextLabel.sexual_content,\n    ]\n\n    labels_assistant_reply: list[TextLabel] = [\n        TextLabel.spam,\n        TextLabel.lang_mismatch,\n        TextLabel.fails_task,\n        TextLabel.quality,\n        TextLabel.helpfulness,\n        TextLabel.creativity,\n        TextLabel.humor,\n        TextLabel.toxicity,\n        TextLabel.violence,\n        TextLabel.not_appropriate,\n        TextLabel.pii,\n        TextLabel.hate_speech,\n        TextLabel.sexual_content,\n    ]\n\n    labels_prompter_reply: list[TextLabel] = [\n        TextLabel.spam,\n        TextLabel.lang_mismatch,\n        TextLabel.quality,\n        TextLabel.creativity,\n        TextLabel.humor,\n        TextLabel.toxicity,\n        TextLabel.violence,\n        TextLabel.not_appropriate,\n        TextLabel.pii,\n        TextLabel.hate_speech,\n        TextLabel.sexual_content,\n    ]\n\n    mandatory_labels_initial_prompt: Optional[list[TextLabel]] = [TextLabel.spam]\n    \"\"\"Mandatory labels in text-labeling tasks for initial prompts.\"\"\"\n\n    mandatory_labels_assistant_reply: Optional[list[TextLabel]] = [TextLabel.spam]\n    \"\"\"Mandatory labels in text-labeling tasks for assistant replies.\"\"\"\n\n    mandatory_labels_prompter_reply: Optional[list[TextLabel]] = [TextLabel.spam]\n    \"\"\"Mandatory labels in text-labeling tasks for prompter replies.\"\"\"\n\n    rank_prompter_replies: bool = False\n\n    lonely_children_count: int = 2\n    \"\"\"Number of children below which parents are preferred during sampling for reply tasks.\"\"\"\n\n    p_lonely_child_extension: float = 0.75\n    \"\"\"Probability to select a prompter message parent with less than lonely_children_count children.\"\"\"\n\n    recent_tasks_span_sec: int = 5 * 60  # 5 min\n    \"\"\"Time in seconds of recent tasks to consider for exclusion during task selection.\"\"\"\n\n    max_pending_tasks_per_user: int = 8\n    \"\"\"Maximum number of pending tasks (neither canceled nor completed) by a single user within\n    the time span defined by `recent_tasks_span_sec`.\"\"\"\n\n    max_prompt_lottery_waiting: int = 250\n    \"\"\"Maximum number of prompts in prompt_lottery_waiting state per language. If this value\n    is exceeded no new initial prompt tasks for that language are generated.\"\"\"\n\n    init_prompt_disabled_langs: str = \"\"\n\n    @property\n    def init_prompt_disabled_langs_list(self) -> list[str]:\n        return self.init_prompt_disabled_langs.split(\",\")\n\n\nclass Settings(BaseSettings):\n    PROJECT_NAME: str = \"open-assistant backend\"\n    API_V1_STR: str = \"/api/v1\"\n    OFFICIAL_WEB_API_KEY: str = \"1234\"\n\n    # Encryption fields for handling the web generated JSON Web Tokens.\n    # These fields need to be shared with the web's auth settings in order to\n    # correctly decrypt the web tokens.\n    AUTH_INFO: bytes = b\"NextAuth.js Generated Encryption Key\"\n    AUTH_SALT: bytes = b\"\"\n    AUTH_LENGTH: int = 32\n    AUTH_SECRET: bytes = b\"O/M2uIbGj+lDD2oyNa8ax4jEOJqCPJzO53UbWShmq98=\"\n    AUTH_COOKIE_NAME: str = \"next-auth.session-token\"\n    AUTH_ALGORITHM: str = \"HS256\"\n    AUTH_ACCESS_TOKEN_EXPIRE_MINUTES: int = 30\n\n    AUTH_DISCORD_CLIENT_ID: str = \"\"\n    AUTH_DISCORD_CLIENT_SECRET: str = \"\"\n\n    POSTGRES_HOST: str = \"localhost\"\n    POSTGRES_PORT: str = \"5432\"\n    POSTGRES_USER: str = \"postgres\"\n    POSTGRES_PASSWORD: str = \"postgres\"\n    POSTGRES_DB: str = \"postgres\"\n    DATABASE_URI: Optional[PostgresDsn] = None\n    DATABASE_MAX_TX_RETRY_COUNT: int = 3\n\n    DATABASE_POOL_SIZE = 75\n    DATABASE_MAX_OVERFLOW = 20\n\n    RATE_LIMIT: bool = True\n    MESSAGE_SIZE_LIMIT: int = 2000\n    REDIS_HOST: str = \"localhost\"\n    REDIS_PORT: str = \"6379\"\n\n    DEBUG_USE_SEED_DATA: bool = False\n    DEBUG_USE_SEED_DATA_PATH: Optional[FilePath] = (\n        Path(__file__).parent.parent / \"test_data/realistic/realistic_seed_data.json\"\n    )\n    DEBUG_ALLOW_SELF_LABELING: bool = False  # allow users to label their own messages\n    DEBUG_ALLOW_SELF_RANKING: bool = False  # allow users to rank their own messages\n    DEBUG_ALLOW_DUPLICATE_TASKS: bool = False  # offer users tasks to which they already responded\n    DEBUG_SKIP_EMBEDDING_COMPUTATION: bool = False\n    DEBUG_SKIP_TOXICITY_CALCULATION: bool = False\n    DEBUG_DATABASE_ECHO: bool = False\n    DEBUG_IGNORE_TOS_ACCEPTANCE: bool = (  # ignore whether users accepted the ToS\n        True  # TODO: set False after ToS acceptance UI was added to web-frontend\n    )\n\n    DUPLICATE_MESSAGE_FILTER_WINDOW_MINUTES: int = 120\n\n    HUGGING_FACE_API_KEY: str = \"\"\n\n    ROOT_TOKENS: List[str] = [\"1234\"]  # supply a string that can be parsed to a json list\n\n    ENABLE_PROM_METRICS: bool = True  # enable prometheus metrics at /metrics\n\n    @validator(\"DATABASE_URI\", pre=True)\n    def assemble_db_connection(cls, v: Optional[str], values: Dict[str, Any]) -> Any:\n        if isinstance(v, str):\n            return v\n        return PostgresDsn.build(\n            scheme=\"postgresql\",\n            user=values.get(\"POSTGRES_USER\"),\n            password=values.get(\"POSTGRES_PASSWORD\"),\n            host=values.get(\"POSTGRES_HOST\"),\n            port=values.get(\"POSTGRES_PORT\"),\n            path=f\"/{values.get('POSTGRES_DB') or ''}\",\n        )\n\n    BACKEND_CORS_ORIGINS_CSV: Optional[str]  # allow setting CORS origins as comma separated values\n    BACKEND_CORS_ORIGINS: List[AnyHttpUrl] = []\n\n    @validator(\"BACKEND_CORS_ORIGINS\", pre=True)\n    def assemble_cors_origins(cls, v: Optional[List[str]], values: Dict[str, Any]) -> List[str]:\n        s = values.get(\"BACKEND_CORS_ORIGINS_CSV\")\n        if isinstance(s, str):\n            v = [i.strip() for i in s.split(\",\")]\n            return v\n        return v\n\n    UPDATE_ALEMBIC: bool = True\n\n    tree_manager: Optional[TreeManagerConfiguration] = TreeManagerConfiguration()\n\n    USER_STATS_INTERVAL_DAY: int = 5  # minutes\n    USER_STATS_INTERVAL_WEEK: int = 15  # minutes\n    USER_STATS_INTERVAL_MONTH: int = 60  # minutes\n    USER_STATS_INTERVAL_TOTAL: int = 240  # minutes\n    USER_STREAK_UPDATE_INTERVAL: int = 4  # Hours\n\n    @validator(\n        \"USER_STATS_INTERVAL_DAY\",\n        \"USER_STATS_INTERVAL_WEEK\",\n        \"USER_STATS_INTERVAL_MONTH\",\n        \"USER_STATS_INTERVAL_TOTAL\",\n        \"USER_STREAK_UPDATE_INTERVAL\",\n    )\n    def validate_user_stats_intervals(cls, v: int):\n        if v < 1:\n            raise ValueError(v)\n        return v\n\n    CACHED_STATS_UPDATE_INTERVAL: int = 60  # minutes\n\n    RATE_LIMIT_TASK_USER_TIMES: int = 30\n    RATE_LIMIT_TASK_USER_MINUTES: int = 4\n    RATE_LIMIT_TASK_API_TIMES: int = 10_000\n    RATE_LIMIT_TASK_API_MINUTES: int = 1\n\n    RATE_LIMIT_ASSISTANT_USER_TIMES: int = 4\n    RATE_LIMIT_ASSISTANT_USER_MINUTES: int = 2\n\n    RATE_LIMIT_PROMPTER_USER_TIMES: int = 8\n    RATE_LIMIT_PROMPTER_USER_MINUTES: int = 2\n\n    TASK_VALIDITY_MINUTES: int = 60 * 24 * 2  # tasks expire after 2 days\n\n    DISCORD_API_KEY: str | None = None\n    DISCORD_CHANNEL_ID: str | None = None\n\n    class Config:\n        env_file = \".env\"\n        env_file_encoding = \"utf-8\"\n        case_sensitive = False\n        env_nested_delimiter = \"__\"\n\n\nsettings = Settings()\n", "backend/oasst_backend/auth.py": "from datetime import datetime, timedelta\nfrom typing import Optional\n\nfrom jose import jwt\nfrom oasst_backend.config import Settings\nfrom oasst_backend.models import Account\nfrom sqlmodel import Session\n\n\ndef create_access_token(data: dict) -> str:\n    \"\"\"\n    Create an encoded JSON Web Token (JWT) using the given data.\n    \"\"\"\n\n    expires_delta = timedelta(minutes=Settings.AUTH_ACCESS_TOKEN_EXPIRE_MINUTES)\n    to_encode = data.copy()\n    expire = datetime.utcnow() + expires_delta\n    to_encode.update({\"exp\": expire})\n    encoded_jwt = jwt.encode(to_encode, Settings.AUTH_SECRET, algorithm=Settings.AUTH_ALGORITHM)\n    return encoded_jwt\n\n\ndef get_account_from_discord_id(db: Session, discord_id: str) -> Optional[Account]:\n    \"\"\"\n    Get the Open-Assistant Account associated with the given Discord ID.\n    \"\"\"\n\n    account: Account = (\n        db.query(Account)\n        .filter(\n            Account.provider == \"discord\",\n            Account.provider_account_id == discord_id,\n        )\n        .first()\n    )\n\n    return account\n", "backend/oasst_backend/scheduled_tasks.py": "from __future__ import absolute_import, unicode_literals\n\nfrom datetime import timedelta\nfrom typing import Any, Dict, List\n\nfrom asgiref.sync import async_to_sync\nfrom celery import shared_task\nfrom loguru import logger\nfrom oasst_backend.celery_worker import app\nfrom oasst_backend.models import ApiClient, Message, User\nfrom oasst_backend.models.db_payload import MessagePayload\nfrom oasst_backend.prompt_repository import PromptRepository\nfrom oasst_backend.utils.database_utils import db_lang_to_postgres_ts_lang, default_session_factory\nfrom oasst_backend.utils.hugging_face import HfClassificationModel, HfEmbeddingModel, HfUrl, HuggingFaceAPI\nfrom oasst_shared.utils import log_timing, utcnow\nfrom sqlalchemy import func\nfrom sqlmodel import update\n\n\nasync def useHFApi(text, url, model_name):\n    hugging_face_api: HuggingFaceAPI = HuggingFaceAPI(f\"{url}/{model_name}\")\n    result = await hugging_face_api.post(text)\n    return result\n\n\n@app.task(name=\"toxicity\")\ndef toxicity(text, message_id, api_client):\n    try:\n        logger.info(f\"checking toxicity : {api_client}\")\n\n        with default_session_factory() as session:\n            model_name: str = HfClassificationModel.TOXIC_ROBERTA.value\n            url: str = HfUrl.HUGGINGFACE_TOXIC_CLASSIFICATION.value\n            toxicity: List[List[Dict[str, Any]]] = async_to_sync(useHFApi)(text=text, url=url, model_name=model_name)\n            toxicity = toxicity[0][0]\n            logger.info(f\"toxicity from HF {toxicity}\")\n            api_client_m = ApiClient(**api_client)\n            if toxicity is not None:\n                pr = PromptRepository(db=session, api_client=api_client_m)\n                pr.insert_toxicity(\n                    message_id=message_id, model=model_name, score=toxicity[\"score\"], label=toxicity[\"label\"]\n                )\n            session.commit()\n\n    except Exception as e:\n        logger.error(f\"Could not compute toxicity for text reply to {message_id=} with {text=} by.error {str(e)}\")\n\n\n@app.task(name=\"hf_feature_extraction\")\ndef hf_feature_extraction(text, message_id, api_client):\n    try:\n        with default_session_factory() as session:\n            model_name: str = HfEmbeddingModel.MINILM.value\n            url: str = HfUrl.HUGGINGFACE_FEATURE_EXTRACTION.value\n            embedding = async_to_sync(useHFApi)(text=text, url=url, model_name=model_name)\n            api_client_m = ApiClient(**api_client)\n            if embedding is not None:\n                logger.info(f\"emmbedding from HF {len(embedding)}\")\n                pr = PromptRepository(db=session, api_client=api_client_m)\n                pr.insert_message_embedding(\n                    message_id=message_id, model=HfEmbeddingModel.MINILM.value, embedding=embedding\n                )\n                session.commit()\n\n    except Exception as e:\n        logger.error(f\"Could not extract embedding for text reply to {message_id=} with {text=} by.error {str(e)}\")\n\n\n@shared_task(name=\"update_search_vectors\")\ndef update_search_vectors(batch_size: int) -> None:\n    logger.info(\"update_search_vectors start...\")\n    try:\n        with default_session_factory() as session:\n            while True:\n                to_update: list[Message] = (\n                    session.query(Message).filter(Message.search_vector.is_(None)).limit(batch_size).all()\n                )\n\n                if not to_update:\n                    break\n\n                for message in to_update:\n                    message_payload: MessagePayload = message.payload.payload\n                    message_lang: str = db_lang_to_postgres_ts_lang(message.lang)\n                    message.search_vector = func.to_tsvector(message_lang, message_payload.text)\n\n                session.commit()\n    except Exception as e:\n        logger.error(f\"update_search_vectors failed with error: {str(e)}\")\n\n\n@shared_task(name=\"periodic_user_streak_reset\")\n@log_timing(level=\"INFO\")\ndef periodic_user_streak_reset() -> None:\n    try:\n        with default_session_factory() as session:\n            # Reset streak_days to 0 for users with more than 1.5 days of inactivity\n            streak_timeout = utcnow() - timedelta(hours=36)\n            reset_query = (\n                update(User)\n                .filter(User.last_activity_date < streak_timeout, User.streak_last_day_date.is_not(None))\n                .values(streak_days=0, streak_last_day_date=None)\n            )\n            session.execute(reset_query)\n            session.commit()\n    except Exception:\n        logger.exception(\"Error during periodic user streak reset\")\n", "backend/oasst_backend/cached_stats_repository.py": "from oasst_backend.models import CachedStats, Message, MessageTreeState, User\nfrom oasst_shared.exceptions.oasst_api_error import OasstError, OasstErrorCode\nfrom oasst_shared.schemas.protocol import AllCachedStatsResponse, CachedStatsName, CachedStatsResponse\nfrom oasst_shared.utils import log_timing, utcnow\nfrom sqlalchemy.orm.attributes import flag_modified\nfrom sqlmodel import Session, func, not_\n\n\ndef row_to_dict(r) -> dict:\n    return {k: r[k] for k in r.keys()}\n\n\nclass CachedStatsRepository:\n    def __init__(self, db: Session):\n        self.db = db\n\n    def qry_human_messages_by_lang(self) -> dict[str, int]:\n        qry = (\n            self.db.query(Message.lang, func.count(Message.id).label(\"count\"))\n            .filter(not_(Message.deleted), Message.review_result, not_(Message.synthetic))\n            .group_by(Message.lang)\n        )\n        return {r[\"lang\"]: r[\"count\"] for r in qry}\n\n    def qry_human_messages_by_role(self) -> dict[str, int]:\n        qry = (\n            self.db.query(Message.role, func.count(Message.id).label(\"count\"))\n            .filter(not_(Message.deleted), Message.review_result, not_(Message.synthetic))\n            .group_by(Message.role)\n        )\n        return {r[\"role\"]: r[\"count\"] for r in qry}\n\n    def qry_message_trees_by_state(self) -> dict[str, int]:\n        qry = self.db.query(\n            MessageTreeState.state, func.count(MessageTreeState.message_tree_id).label(\"count\")\n        ).group_by(MessageTreeState.state)\n        return {r[\"state\"]: r[\"count\"] for r in qry}\n\n    def qry_message_trees_states_by_lang(self) -> list:\n        qry = (\n            self.db.query(\n                Message.lang, MessageTreeState.state, func.count(MessageTreeState.message_tree_id).label(\"count\")\n            )\n            .select_from(MessageTreeState)\n            .join(Message, MessageTreeState.message_tree_id == Message.id)\n            .group_by(MessageTreeState.state, Message.lang)\n            .order_by(Message.lang, MessageTreeState.state)\n        )\n        return [row_to_dict(r) for r in qry]\n\n    def qry_users_accepted_tos(self) -> dict[str, int]:\n        qry = self.db.query(func.count(User.id)).filter(User.enabled, User.tos_acceptance_date.is_not(None))\n        return {\"count\": qry.scalar()}\n\n    @log_timing(level=\"INFO\")\n    def update_all_cached_stats(self):\n        v = self.qry_human_messages_by_lang()\n        self._insert_cached_stats(CachedStatsName.human_messages_by_lang, v)\n\n        v = self.qry_human_messages_by_role()\n        self._insert_cached_stats(CachedStatsName.human_messages_by_role, v)\n\n        v = self.qry_message_trees_by_state()\n        self._insert_cached_stats(CachedStatsName.message_trees_by_state, v)\n\n        v = self.qry_message_trees_states_by_lang()\n        self._insert_cached_stats(CachedStatsName.message_trees_states_by_lang, v)\n\n        v = self.qry_users_accepted_tos()\n        self._insert_cached_stats(CachedStatsName.users_accepted_tos, v)\n\n    def _insert_cached_stats(self, name: CachedStatsName, stats: dict | list):\n        row: CachedStats | None = self.db.query(CachedStats).filter(CachedStats.name == name).one_or_none()\n        if row:\n            row.modified_date = utcnow()\n            row.stats = stats\n            flag_modified(row, \"stats\")\n        else:\n            row = CachedStats(name=name, modified_date=utcnow(), stats=stats)\n        self.db.add(row)\n\n    def get_stats(self, name: CachedStatsName) -> CachedStatsResponse:\n        row: CachedStats | None = self.db.query(CachedStats).filter(CachedStats.name == name).one_or_none()\n        if not row:\n            raise OasstError(f\"Cached stats '{name.value}' not found.\", OasstErrorCode.CACHED_STATS_NOT_AVAILABLE)\n        return CachedStatsResponse(name=row.name, last_updated=row.modified_date, stats=row.stats)\n\n    def get_stats_all(self) -> AllCachedStatsResponse:\n        by_name: dict[CachedStatsName, CachedStatsResponse] = {}\n        qry = self.db.query(CachedStats)\n        for row in qry:\n            by_name[row.name] = CachedStatsResponse(name=row.name, last_updated=row.modified_date, stats=row.stats)\n        return AllCachedStatsResponse(stats_by_name=by_name)\n\n\nif __name__ == \"__main__\":\n    # from oasst_backend.api.deps import create_api_client\n    from oasst_backend.database import engine\n\n    with Session(engine) as db:\n        csr = CachedStatsRepository(db)\n        csr.update_all_cached_stats()()\n        db.commit()\n", "backend/oasst_backend/database.py": "from oasst_backend.config import settings\nfrom oasst_shared.exceptions import OasstError, OasstErrorCode\nfrom sqlmodel import create_engine\n\nif settings.DATABASE_URI is None:\n    raise OasstError(\"DATABASE_URI is not set\", error_code=OasstErrorCode.DATABASE_URI_NOT_SET)\n\nengine = create_engine(\n    settings.DATABASE_URI,\n    echo=settings.DEBUG_DATABASE_ECHO,\n    isolation_level=\"REPEATABLE READ\",\n    pool_size=settings.DATABASE_POOL_SIZE,\n    max_overflow=settings.DATABASE_MAX_OVERFLOW,\n)\n", "backend/oasst_backend/journal_writer.py": "import enum\nfrom typing import Literal, Optional\nfrom uuid import UUID\n\nfrom oasst_backend.models import ApiClient, Journal, Task, User\nfrom oasst_backend.models.payload_column_type import PayloadContainer, payload_type\nfrom oasst_backend.utils.database_utils import CommitMode, managed_tx_method\nfrom oasst_shared.utils import utcnow\nfrom pydantic import BaseModel\nfrom sqlmodel import Session\n\n\nclass JournalEventType(str, enum.Enum):\n    \"\"\"A label for a piece of text.\"\"\"\n\n    user_created = \"user_created\"\n    text_reply_to_message = \"text_reply_to_message\"\n    message_rating = \"message_rating\"\n    message_ranking = \"message_ranking\"\n\n\n@payload_type\nclass JournalEvent(BaseModel):\n    type: str\n    user_id: Optional[UUID]\n    message_id: Optional[UUID]\n    task_id: Optional[UUID]\n    task_type: Optional[str]\n\n\n@payload_type\nclass TextReplyEvent(JournalEvent):\n    type: Literal[JournalEventType.text_reply_to_message] = JournalEventType.text_reply_to_message\n    length: int\n    role: str\n\n\n@payload_type\nclass RatingEvent(JournalEvent):\n    type: Literal[JournalEventType.message_rating] = JournalEventType.message_rating\n    rating: int\n\n\n@payload_type\nclass RankingEvent(JournalEvent):\n    type: Literal[JournalEventType.message_ranking] = JournalEventType.message_ranking\n    ranking: list[int]\n\n\nclass JournalWriter:\n    def __init__(self, db: Session, api_client: ApiClient, user: User):\n        self.db = db\n        self.api_client = api_client\n        self.user = user\n        self.user_id = self.user.id if self.user else None\n\n    def log_text_reply(self, task: Task, message_id: Optional[UUID], role: str, length: int) -> Journal:\n        return self.log(\n            task_type=task.payload_type,\n            event_type=JournalEventType.text_reply_to_message,\n            payload=TextReplyEvent(role=role, length=length),\n            task_id=task.id,\n            message_id=message_id,\n        )\n\n    def log_rating(self, task: Task, message_id: Optional[UUID], rating: int) -> Journal:\n        return self.log(\n            task_type=task.payload_type,\n            event_type=JournalEventType.message_rating,\n            payload=RatingEvent(rating=rating),\n            task_id=task.id,\n            message_id=message_id,\n        )\n\n    def log_ranking(self, task: Task, message_id: Optional[UUID], ranking: list[int]) -> Journal:\n        return self.log(\n            task_type=task.payload_type,\n            event_type=JournalEventType.message_ranking,\n            payload=RankingEvent(ranking=ranking),\n            task_id=task.id,\n            message_id=message_id,\n        )\n\n    @managed_tx_method(CommitMode.FLUSH)\n    def log(\n        self,\n        *,\n        payload: JournalEvent,\n        task_type: str,\n        event_type: str = None,\n        task_id: Optional[UUID] = None,\n        message_id: Optional[UUID] = None,\n        commit: bool = True,\n    ) -> Journal:\n        if event_type is None:\n            if payload is None:\n                event_type = \"null\"\n            else:\n                event_type = type(payload).__name__\n\n        if payload.user_id is None:\n            payload.user_id = self.user_id\n        if payload.message_id is None:\n            payload.message_id = message_id\n        if payload.task_id is None:\n            payload.task_id = task_id\n        if payload.task_type is None:\n            payload.task_type = task_type\n\n        entry = Journal(\n            user_id=self.user_id,\n            api_client_id=self.api_client.id,\n            created_date=utcnow(),\n            event_type=event_type,\n            event_payload=PayloadContainer(payload=payload),\n            message_id=message_id,\n        )\n\n        self.db.add(entry)\n        return entry\n", "backend/oasst_backend/user_stats_repository.py": "from datetime import datetime, timedelta\nfrom typing import Optional\nfrom uuid import UUID\n\nimport numpy as np\nimport sqlalchemy as sa\nfrom loguru import logger\nfrom oasst_backend.config import settings\nfrom oasst_backend.models import (\n    Message,\n    MessageReaction,\n    MessageTreeState,\n    Task,\n    TextLabels,\n    TrollStats,\n    User,\n    UserStats,\n    UserStatsTimeFrame,\n)\nfrom oasst_backend.models.db_payload import (\n    LabelAssistantReplyPayload,\n    LabelInitialPromptPayload,\n    LabelPrompterReplyPayload,\n    RankingReactionPayload,\n)\nfrom oasst_backend.models.message_tree_state import State as TreeState\nfrom oasst_shared.schemas.protocol import (\n    EmojiCode,\n    LabelTaskMode,\n    LeaderboardStats,\n    TextLabel,\n    TrollboardStats,\n    TrollScore,\n    UserScore,\n)\nfrom oasst_shared.utils import log_timing, utcnow\nfrom sqlalchemy.dialects import postgresql\nfrom sqlalchemy.sql.functions import coalesce\nfrom sqlmodel import Session, delete, func, text\n\n\ndef get_thresholds(baseline: int = 3, alpha: float = 1.1521, max_level: int = 100) -> np.ndarray:\n    level = np.round(np.cumsum(np.arange(1, max_level) * alpha + baseline))\n    return np.array([0] + level.astype(int).tolist())\n\n\n# lookup table, never changes\nTHRESHOLDS = get_thresholds()\n\n\ndef _create_user_score(r, highlighted_user_id: UUID | None) -> UserScore:\n    if r[\"UserStats\"]:\n        d = r[\"UserStats\"].dict()\n        d[\"level\"] = (THRESHOLDS <= d[\"leader_score\"]).sum()\n    else:\n        d = {\"modified_date\": utcnow()}\n        d[\"level\"] = 0\n    for k in [\n        \"user_id\",\n        \"username\",\n        \"auth_method\",\n        \"display_name\",\n        \"streak_days\",\n        \"streak_last_day_date\",\n        \"last_activity_date\",\n    ]:\n        d[k] = r[k]\n    if highlighted_user_id:\n        d[\"highlighted\"] = r[\"user_id\"] == highlighted_user_id\n\n    return UserScore(**d)\n\n\ndef _create_troll_score(r, highlighted_user_id: UUID | None) -> TrollScore:\n    if r[\"TrollStats\"]:\n        d = r[\"TrollStats\"].dict()\n    else:\n        d = {\"modified_date\": utcnow()}\n    for k in [\n        \"user_id\",\n        \"username\",\n        \"auth_method\",\n        \"display_name\",\n        \"last_activity_date\",\n        \"enabled\",\n        \"deleted\",\n        \"show_on_leaderboard\",\n    ]:\n        d[k] = r[k]\n    if highlighted_user_id:\n        d[\"highlighted\"] = r[\"user_id\"] == highlighted_user_id\n    return TrollScore(**d)\n\n\nclass UserStatsRepository:\n    def __init__(self, session: Session):\n        self.session = session\n\n    def get_leaderboard(\n        self,\n        time_frame: UserStatsTimeFrame,\n        limit: int = 100,\n        highlighted_user_id: Optional[UUID] = None,\n    ) -> LeaderboardStats:\n        \"\"\"\n        Get leaderboard stats for the specified time frame\n        \"\"\"\n\n        qry = (\n            self.session.query(\n                User.id.label(\"user_id\"),\n                User.username,\n                User.auth_method,\n                User.display_name,\n                User.streak_days,\n                User.streak_last_day_date,\n                User.last_activity_date,\n                UserStats,\n            )\n            .join(UserStats, User.id == UserStats.user_id)\n            .filter(UserStats.time_frame == time_frame.value, User.show_on_leaderboard, User.enabled)\n            .order_by(UserStats.rank)\n            .limit(limit)\n        )\n\n        leaderboard = [_create_user_score(r, highlighted_user_id) for r in self.session.exec(qry)]\n        if len(leaderboard) > 0:\n            last_update = max(x.modified_date for x in leaderboard)\n        else:\n            last_update = utcnow()\n        return LeaderboardStats(time_frame=time_frame.value, leaderboard=leaderboard, last_updated=last_update)\n\n    def get_leaderboard_user_window(\n        self,\n        user: User,\n        time_frame: UserStatsTimeFrame,\n        window_size: int = 5,\n    ) -> LeaderboardStats | None:\n        # no window for users who don't show themselves\n        if not user.show_on_leaderboard or not user.enabled:\n            return None\n\n        qry = self.session.query(UserStats).filter(UserStats.user_id == user.id, UserStats.time_frame == time_frame)\n        stats: UserStats = qry.one_or_none()\n        if stats is None or stats.rank is None:\n            return None\n\n        min_rank = max(0, stats.rank - window_size // 2)\n        max_rank = min_rank + window_size\n\n        qry = (\n            self.session.query(\n                User.id.label(\"user_id\"),\n                User.username,\n                User.auth_method,\n                User.display_name,\n                User.streak_days,\n                User.streak_last_day_date,\n                User.last_activity_date,\n                UserStats,\n            )\n            .join(UserStats, User.id == UserStats.user_id)\n            .filter(UserStats.time_frame == time_frame.value, User.show_on_leaderboard, User.enabled)\n            .where(UserStats.rank >= min_rank, UserStats.rank <= max_rank)\n            .order_by(UserStats.rank)\n        )\n\n        leaderboard = [_create_user_score(r, highlighted_user_id=user.id) for r in self.session.exec(qry)]\n        if len(leaderboard) > 0:\n            last_update = max(x.modified_date for x in leaderboard)\n        else:\n            last_update = utcnow()\n        return LeaderboardStats(time_frame=time_frame.value, leaderboard=leaderboard, last_updated=last_update)\n\n    def get_user_stats_all_time_frames(self, user_id: UUID) -> dict[str, UserScore | None]:\n        qry = (\n            self.session.query(\n                User.id.label(\"user_id\"),\n                User.username,\n                User.auth_method,\n                User.display_name,\n                User.streak_days,\n                User.streak_last_day_date,\n                User.last_activity_date,\n                UserStats,\n            )\n            .outerjoin(UserStats, User.id == UserStats.user_id)\n            .filter(User.id == user_id)\n        )\n\n        stats_by_timeframe = {}\n        for r in self.session.exec(qry):\n            us = r[\"UserStats\"]\n            if us is not None:\n                stats_by_timeframe[us.time_frame] = _create_user_score(r, user_id)\n            else:\n                stats_by_timeframe = {tf.value: _create_user_score(r, user_id) for tf in UserStatsTimeFrame}\n        return stats_by_timeframe\n\n    def get_trollboard(\n        self,\n        time_frame: UserStatsTimeFrame,\n        limit: int = 100,\n        enabled: Optional[bool] = None,\n        highlighted_user_id: Optional[UUID] = None,\n    ) -> TrollboardStats:\n        \"\"\"\n        Get trollboard stats for the specified time frame\n        \"\"\"\n\n        qry = (\n            self.session.query(\n                User.id.label(\"user_id\"),\n                User.username,\n                User.auth_method,\n                User.display_name,\n                User.last_activity_date,\n                User.enabled,\n                User.deleted,\n                User.show_on_leaderboard,\n                TrollStats,\n            )\n            .join(TrollStats, User.id == TrollStats.user_id)\n            .filter(TrollStats.time_frame == time_frame.value)\n        )\n\n        if enabled is not None:\n            qry = qry.filter(User.enabled == enabled)\n\n        qry = qry.order_by(TrollStats.rank).limit(limit)\n\n        trollboard = [_create_troll_score(r, highlighted_user_id) for r in self.session.exec(qry)]\n        if len(trollboard) > 0:\n            last_update = max(x.modified_date for x in trollboard)\n        else:\n            last_update = utcnow()\n        return TrollboardStats(time_frame=time_frame.value, trollboard=trollboard, last_updated=last_update)\n\n    def query_total_prompts_per_user(\n        self, reference_time: Optional[datetime] = None, only_reviewed: Optional[bool] = True\n    ):\n        qry = self.session.query(Message.user_id, func.count()).filter(\n            Message.deleted == sa.false(), Message.parent_id.is_(None)\n        )\n        if reference_time:\n            qry = qry.filter(Message.created_date >= reference_time)\n        if only_reviewed:\n            qry = qry.filter(Message.review_result == sa.true())\n        qry = qry.group_by(Message.user_id)\n        return qry\n\n    def query_replies_by_role_per_user(\n        self, reference_time: Optional[datetime] = None, only_reviewed: Optional[bool] = True\n    ) -> list:\n        qry = self.session.query(Message.user_id, Message.role, func.count()).filter(\n            Message.deleted == sa.false(), Message.parent_id.is_not(None)\n        )\n        if reference_time:\n            qry = qry.filter(Message.created_date >= reference_time)\n        if only_reviewed:\n            qry = qry.filter(Message.review_result == sa.true())\n        qry = qry.group_by(Message.user_id, Message.role)\n        return qry\n\n    def query_labels_by_mode_per_user(\n        self, payload_type: str = LabelAssistantReplyPayload.__name__, reference_time: Optional[datetime] = None\n    ):\n        qry = self.session.query(Task.user_id, Task.payload[\"payload\", \"mode\"].astext, func.count()).filter(\n            Task.done == sa.true(), Task.payload_type == payload_type\n        )\n        if reference_time:\n            qry = qry.filter(Task.created_date >= reference_time)\n        qry = qry.group_by(Task.user_id, Task.payload[\"payload\", \"mode\"].astext)\n        return qry\n\n    def query_rankings_per_user(self, reference_time: Optional[datetime] = None):\n        qry = self.session.query(MessageReaction.user_id, func.count()).filter(\n            MessageReaction.payload_type == RankingReactionPayload.__name__\n        )\n        if reference_time:\n            qry = qry.filter(MessageReaction.created_date >= reference_time)\n        qry = qry.group_by(MessageReaction.user_id)\n        return qry\n\n    def query_ranking_result_users(self, rank: int = 0, reference_time: Optional[datetime] = None):\n        ranked_message_id = MessageReaction.payload[\"payload\", \"ranked_message_ids\", rank].astext.cast(\n            postgresql.UUID(as_uuid=True)\n        )\n        qry = (\n            self.session.query(Message.user_id, func.count())\n            .select_from(MessageReaction)\n            .join(Message, ranked_message_id == Message.id)\n            .filter(MessageReaction.payload_type == RankingReactionPayload.__name__)\n        )\n        if reference_time:\n            qry = qry.filter(MessageReaction.created_date >= reference_time)\n        qry = qry.group_by(Message.user_id)\n        return qry\n\n    def _update_stats_internal(self, time_frame: UserStatsTimeFrame, base_date: Optional[datetime] = None):\n        # gather user data\n\n        time_frame_key = time_frame.value\n\n        stats_by_user: dict[UUID, UserStats] = dict()\n        now = utcnow()\n\n        def get_stats(id: UUID) -> UserStats:\n            us = stats_by_user.get(id)\n            if not us:\n                us = UserStats(user_id=id, time_frame=time_frame_key, modified_date=now, base_date=base_date)\n                stats_by_user[id] = us\n            return us\n\n        # total prompts\n        qry = self.query_total_prompts_per_user(reference_time=base_date, only_reviewed=False)\n        for r in qry:\n            uid, count = r\n            get_stats(uid).prompts = count\n\n        # accepted prompts\n        qry = self.query_total_prompts_per_user(reference_time=base_date, only_reviewed=True)\n        for r in qry:\n            uid, count = r\n            get_stats(uid).accepted_prompts = count\n\n        # total replies\n        qry = self.query_replies_by_role_per_user(reference_time=base_date, only_reviewed=False)\n        for r in qry:\n            uid, role, count = r\n            s = get_stats(uid)\n            if role == \"assistant\":\n                s.replies_assistant += count\n            elif role == \"prompter\":\n                s.replies_prompter += count\n\n        # accepted replies\n        qry = self.query_replies_by_role_per_user(reference_time=base_date, only_reviewed=True)\n        for r in qry:\n            uid, role, count = r\n            s = get_stats(uid)\n            if role == \"assistant\":\n                s.accepted_replies_assistant += count\n            elif role == \"prompter\":\n                s.accepted_replies_prompter += count\n\n        # simple and full labels\n        qry = self.query_labels_by_mode_per_user(\n            payload_type=LabelAssistantReplyPayload.__name__, reference_time=base_date\n        )\n        for r in qry:\n            uid, mode, count = r\n            s = get_stats(uid)\n            if mode == LabelTaskMode.simple:\n                s.labels_simple = count\n            elif mode == LabelTaskMode.full:\n                s.labels_full = count\n\n        qry = self.query_labels_by_mode_per_user(\n            payload_type=LabelPrompterReplyPayload.__name__, reference_time=base_date\n        )\n        for r in qry:\n            uid, mode, count = r\n            s = get_stats(uid)\n            if mode == LabelTaskMode.simple:\n                s.labels_simple += count\n            elif mode == LabelTaskMode.full:\n                s.labels_full += count\n\n        qry = self.query_labels_by_mode_per_user(\n            payload_type=LabelInitialPromptPayload.__name__, reference_time=base_date\n        )\n        for r in qry:\n            uid, mode, count = r\n            s = get_stats(uid)\n            if mode == LabelTaskMode.simple:\n                s.labels_simple += count\n            elif mode == LabelTaskMode.full:\n                s.labels_full += count\n\n        qry = self.query_rankings_per_user(reference_time=base_date)\n        for r in qry:\n            uid, count = r\n            get_stats(uid).rankings_total = count\n\n        rank_field_names = [\"reply_ranked_1\", \"reply_ranked_2\", \"reply_ranked_3\"]\n        for i, fn in enumerate(rank_field_names):\n            qry = self.query_ranking_result_users(reference_time=base_date, rank=i)\n            for r in qry:\n                uid, count = r\n                setattr(get_stats(uid), fn, count)\n\n        # delete all existing stast for time frame\n        d = delete(UserStats).where(UserStats.time_frame == time_frame_key)\n        self.session.execute(d)\n\n        if None in stats_by_user:\n            logger.warning(\"Some messages in DB have NULL values in user_id column.\")\n            del stats_by_user[None]\n\n        # compute magic leader score\n        for v in stats_by_user.values():\n            v.leader_score = v.compute_leader_score()\n\n        # insert user objects\n        self.session.add_all(stats_by_user.values())\n        self.session.flush()\n\n        self.update_leader_ranks(time_frame=time_frame)\n\n    def query_message_emoji_counts_per_user(self, reference_time: Optional[datetime] = None):\n        qry = self.session.query(\n            Message.user_id,\n            func.sum(coalesce(Message.emojis[EmojiCode.thumbs_up].cast(sa.Integer), 0)).label(\"up\"),\n            func.sum(coalesce(Message.emojis[EmojiCode.thumbs_down].cast(sa.Integer), 0)).label(\"down\"),\n            func.sum(coalesce(Message.emojis[EmojiCode.red_flag].cast(sa.Integer), 0)).label(\"flag\"),\n        ).filter(Message.deleted == sa.false(), Message.emojis.is_not(None))\n\n        if reference_time:\n            qry = qry.filter(Message.created_date >= reference_time)\n\n        qry = qry.group_by(Message.user_id)\n        return qry\n\n    def query_spam_prompts_per_user(self, reference_time: Optional[datetime] = None):\n        qry = (\n            self.session.query(Message.user_id, func.count().label(\"spam_prompts\"))\n            .select_from(MessageTreeState)\n            .join(Message, MessageTreeState.message_tree_id == Message.id)\n            .filter(MessageTreeState.state == TreeState.ABORTED_LOW_GRADE)\n        )\n\n        if reference_time:\n            qry = qry.filter(Message.created_date >= reference_time)\n\n        qry = qry.group_by(Message.user_id)\n        return qry\n\n    def query_labels_per_user(self, reference_time: Optional[datetime] = None):\n        qry = (\n            self.session.query(\n                Message.user_id,\n                func.sum(coalesce(TextLabels.labels[TextLabel.spam].cast(sa.Integer), 0)).label(\"spam\"),\n                func.sum(coalesce(TextLabels.labels[TextLabel.lang_mismatch].cast(sa.Integer), 0)).label(\n                    \"lang_mismach\"\n                ),\n                func.sum(coalesce(TextLabels.labels[TextLabel.not_appropriate].cast(sa.Integer), 0)).label(\n                    \"not_appropriate\"\n                ),\n                func.sum(coalesce(TextLabels.labels[TextLabel.pii].cast(sa.Integer), 0)).label(\"pii\"),\n                func.sum(coalesce(TextLabels.labels[TextLabel.hate_speech].cast(sa.Integer), 0)).label(\"hate_speech\"),\n                func.sum(coalesce(TextLabels.labels[TextLabel.sexual_content].cast(sa.Integer), 0)).label(\n                    \"sexual_content\"\n                ),\n                func.sum(coalesce(TextLabels.labels[TextLabel.political_content].cast(sa.Integer), 0)).label(\n                    \"political_content\"\n                ),\n                func.avg(TextLabels.labels[TextLabel.quality].cast(sa.Float)).label(\"quality\"),\n                func.avg(TextLabels.labels[TextLabel.humor].cast(sa.Float)).label(\"humor\"),\n                func.avg(TextLabels.labels[TextLabel.toxicity].cast(sa.Float)).label(\"toxicity\"),\n                func.avg(TextLabels.labels[TextLabel.violence].cast(sa.Float)).label(\"violence\"),\n                func.avg(TextLabels.labels[TextLabel.helpfulness].cast(sa.Float)).label(\"helpfulness\"),\n            )\n            .select_from(TextLabels)\n            .join(Message, TextLabels.message_id == Message.id)\n            .filter(Message.deleted == sa.false(), Message.emojis.is_not(None))\n        )\n\n        if reference_time:\n            qry = qry.filter(Message.created_date >= reference_time)\n\n        qry = qry.group_by(Message.user_id)\n        return qry\n\n    def _update_troll_stats_internal(self, time_frame: UserStatsTimeFrame, base_date: Optional[datetime] = None):\n        # gather user data\n\n        time_frame_key = time_frame.value\n\n        stats_by_user: dict[UUID, TrollStats] = dict()\n        now = utcnow()\n\n        def get_stats(id: UUID) -> TrollStats:\n            us = stats_by_user.get(id)\n            if not us:\n                us = TrollStats(user_id=id, time_frame=time_frame_key, modified_date=now, base_date=base_date)\n                stats_by_user[id] = us\n            return us\n\n        # emoji counts of user's messages\n        qry = self.query_message_emoji_counts_per_user(reference_time=base_date)\n        for r in qry:\n            uid = r[\"user_id\"]\n            s = get_stats(uid)\n            s.upvotes = r[\"up\"]\n            s.downvotes = r[\"down\"]\n            s.red_flags = r[\"flag\"]\n\n        # num spam prompts\n        qry = self.query_spam_prompts_per_user(reference_time=base_date)\n        for r in qry:\n            uid, count = r\n            s = get_stats(uid).spam_prompts = count\n\n        label_field_names = (\n            \"quality\",\n            \"humor\",\n            \"toxicity\",\n            \"violence\",\n            \"helpfulness\",\n            \"spam\",\n            \"lang_mismach\",\n            \"not_appropriate\",\n            \"pii\",\n            \"hate_speech\",\n            \"sexual_content\",\n            \"political_content\",\n        )\n\n        # label counts / mean values\n        qry = self.query_labels_per_user(reference_time=base_date)\n        for r in qry:\n            uid = r[\"user_id\"]\n            s = get_stats(uid)\n            for fn in label_field_names:\n                setattr(s, fn, r[fn])\n\n        # delete all existing stast for time frame\n        d = delete(TrollStats).where(TrollStats.time_frame == time_frame_key)\n        self.session.execute(d)\n\n        if None in stats_by_user:\n            logger.warning(\"Some messages in DB have NULL values in user_id column.\")\n            del stats_by_user[None]\n\n        # compute magic leader score\n        for v in stats_by_user.values():\n            v.troll_score = v.compute_troll_score()\n\n        # insert user objects\n        self.session.add_all(stats_by_user.values())\n        self.session.flush()\n\n        self.update_troll_ranks(time_frame=time_frame)\n\n    @log_timing(log_kwargs=True)\n    def update_leader_ranks(self, time_frame: UserStatsTimeFrame = None):\n        \"\"\"\n        Update user_stats ranks. The persisted rank values allow to\n        quickly the rank of a single user and to query nearby users.\n        \"\"\"\n\n        # todo: convert sql to sqlalchemy query..\n        # ranks = self.session.query(\n        #     func.row_number()\n        #     .over(partition_by=UserStats.time_frame, order_by=[UserStats.leader_score.desc(), UserStats.user_id])\n        #     .label(\"rank\"),\n        #     UserStats.user_id,\n        #     UserStats.time_frame,\n        # )\n\n        sql_update_rank = \"\"\"\n-- update rank\nUPDATE user_stats us\nSET \"rank\" = r.\"rank\"\nFROM\n    (SELECT\n        ROW_NUMBER () OVER(\n            PARTITION BY time_frame\n            ORDER BY leader_score DESC, user_id\n        ) AS \"rank\", user_id, time_frame\n    FROM user_stats us2\n    INNER JOIN \"user\" u ON us2.user_id = u.id AND u.show_on_leaderboard AND u.enabled\n    WHERE (:time_frame IS NULL OR time_frame = :time_frame)) AS r\nWHERE\n    us.user_id = r.user_id\n    AND us.time_frame = r.time_frame;\"\"\"\n        r = self.session.execute(\n            text(sql_update_rank), {\"time_frame\": time_frame.value if time_frame is not None else None}\n        )\n        logger.debug(f\"pre_compute_ranks leader updated({time_frame=}) {r.rowcount} rows.\")\n\n    @log_timing(log_kwargs=True)\n    def update_troll_ranks(self, time_frame: UserStatsTimeFrame = None):\n        sql_update_troll_rank = \"\"\"\n-- update rank\nUPDATE troll_stats ts\nSET \"rank\" = r.\"rank\"\nFROM\n    (SELECT\n        ROW_NUMBER () OVER(\n            PARTITION BY time_frame\n            ORDER BY troll_score DESC, user_id\n        ) AS \"rank\", user_id, time_frame\n    FROM troll_stats ts2\n    WHERE (:time_frame IS NULL OR time_frame = :time_frame)) AS r\nWHERE\n    ts.user_id = r.user_id\n    AND ts.time_frame = r.time_frame;\"\"\"\n        r = self.session.execute(\n            text(sql_update_troll_rank), {\"time_frame\": time_frame.value if time_frame is not None else None}\n        )\n        logger.debug(f\"pre_compute_ranks troll updated({time_frame=}) {r.rowcount} rows.\")\n\n    def update_stats_time_frame(\n        self,\n        time_frame: UserStatsTimeFrame,\n        reference_time: Optional[datetime] = None,\n        leader_stats: bool = True,\n        troll_stats: bool = True,\n    ):\n        if leader_stats:\n            self._update_stats_internal(time_frame, reference_time)\n        if troll_stats:\n            self._update_troll_stats_internal(time_frame, reference_time)\n        self.session.commit()\n\n    @log_timing(log_kwargs=True, level=\"INFO\")\n    def update_stats(self, *, time_frame: UserStatsTimeFrame):\n        now = utcnow()\n        match time_frame:\n            case UserStatsTimeFrame.day:\n                r = now - timedelta(days=1)\n                self.update_stats_time_frame(time_frame, r)\n\n            case UserStatsTimeFrame.week:\n                r = now.date() - timedelta(days=7)\n                r = datetime(r.year, r.month, r.day, tzinfo=now.tzinfo)\n                self.update_stats_time_frame(time_frame, r)\n\n            case UserStatsTimeFrame.month:\n                r = now.date() - timedelta(days=30)\n                r = datetime(r.year, r.month, r.day, tzinfo=now.tzinfo)\n                self.update_stats_time_frame(time_frame, r)\n\n            case UserStatsTimeFrame.total:\n                self.update_stats_time_frame(time_frame, None)\n\n    @log_timing(level=\"INFO\")\n    def update_multiple_time_frames(self, time_frames: list[UserStatsTimeFrame]):\n        for t in time_frames:\n            self.update_stats(time_frame=t)\n\n    @log_timing(level=\"INFO\")\n    def update_all_time_frames(self):\n        self.update_multiple_time_frames(list(UserStatsTimeFrame))\n\n\nif __name__ == \"__main__\":\n    from oasst_backend.api.deps import api_auth\n    from oasst_backend.database import engine\n\n    with Session(engine) as db:\n        api_client = api_auth(settings.OFFICIAL_WEB_API_KEY, db=db)\n        usr = UserStatsRepository(db)\n        usr.update_all_time_frames()\n        db.commit()\n", "backend/oasst_backend/user_repository.py": "from typing import Optional\nfrom uuid import UUID\n\nimport oasst_backend.models as models\nfrom oasst_backend.config import settings\nfrom oasst_backend.models import ApiClient, User\nfrom oasst_backend.utils.database_utils import CommitMode, managed_tx_method\nfrom oasst_shared import utils as shared_utils\nfrom oasst_shared.exceptions import OasstError, OasstErrorCode\nfrom oasst_shared.schemas import protocol as protocol_schema\nfrom oasst_shared.utils import utcnow\nfrom sqlalchemy.exc import IntegrityError\nfrom sqlmodel import Session, and_, delete, or_, update\nfrom starlette.status import HTTP_403_FORBIDDEN, HTTP_404_NOT_FOUND\n\n\nclass UserRepository:\n    def __init__(self, db: Session, api_client: ApiClient):\n        self.db = db\n        self.api_client = api_client\n\n    def get_user(self, id: UUID, api_client_id: Optional[UUID] = None) -> User:\n        \"\"\"\n        Get a user by global user ID. All clients may get users with the same API client ID as the querying client.\n        Trusted clients can get any user.\n\n        Raises:\n            OasstError: 403 if untrusted client attempts to query foreign users. 404 if user with ID not found.\n        \"\"\"\n        if not self.api_client.trusted and api_client_id is None:\n            api_client_id = self.api_client.id\n\n        if not self.api_client.trusted and api_client_id != self.api_client.id:\n            # Unprivileged client requests foreign user\n            raise OasstError(\"Forbidden\", OasstErrorCode.API_CLIENT_NOT_AUTHORIZED, HTTP_403_FORBIDDEN)\n\n        # Will always be unique\n        user_query = self.db.query(User).filter(User.id == id)\n\n        if api_client_id:\n            user_query = user_query.filter(User.api_client_id == api_client_id)\n\n        user: User = user_query.first()\n\n        if user is None:\n            raise OasstError(\"User not found\", OasstErrorCode.USER_NOT_FOUND, HTTP_404_NOT_FOUND)\n\n        return user\n\n    def query_frontend_user(\n        self, auth_method: str, username: str, api_client_id: Optional[UUID] = None\n    ) -> Optional[User]:\n        if not api_client_id:\n            api_client_id = self.api_client.id\n\n        if not self.api_client.trusted and api_client_id != self.api_client.id:\n            # Unprivileged API client asks for foreign user\n            raise OasstError(\"Forbidden\", OasstErrorCode.API_CLIENT_NOT_AUTHORIZED, HTTP_403_FORBIDDEN)\n\n        user: User = (\n            self.db.query(User)\n            .filter(User.auth_method == auth_method, User.username == username, User.api_client_id == api_client_id)\n            .first()\n        )\n\n        if user is None:\n            raise OasstError(\"User not found\", OasstErrorCode.USER_NOT_FOUND, HTTP_404_NOT_FOUND)\n\n        return user\n\n    @managed_tx_method(CommitMode.COMMIT)\n    def update_user(\n        self,\n        id: UUID,\n        display_name: Optional[str] = None,\n        enabled: Optional[bool] = None,\n        notes: Optional[str] = None,\n        show_on_leaderboard: Optional[bool] = None,\n        tos_acceptance: Optional[bool] = None,\n    ) -> User:\n        \"\"\"\n        Update a user by global user ID to disable or set admin notes. Only trusted clients may update users.\n\n        Raises:\n            OasstError: 403 if untrusted client attempts to update a user. 404 if user with ID not found.\n        \"\"\"\n        if not self.api_client.trusted:\n            raise OasstError(\"Forbidden\", OasstErrorCode.API_CLIENT_NOT_AUTHORIZED, HTTP_403_FORBIDDEN)\n\n        user: User = self.db.query(User).filter(User.id == id).first()\n        if user is None:\n            raise OasstError(\"User not found\", OasstErrorCode.USER_NOT_FOUND, HTTP_404_NOT_FOUND)\n\n        if enabled is not None:\n            user.enabled = enabled\n        if notes is not None:\n            user.notes = notes\n        if show_on_leaderboard is not None:\n            user.show_on_leaderboard = show_on_leaderboard\n        if tos_acceptance:\n            user.tos_acceptance_date = utcnow()\n        if display_name is not None:\n            user.display_name = display_name\n\n        self.db.add(user)\n\n        return user\n\n    @managed_tx_method(CommitMode.COMMIT)\n    def mark_user_deleted(self, id: UUID) -> None:\n        \"\"\"\n        Update a user by global user ID to set deleted flag. Only trusted clients may delete users.\n        User deletion anonymises the data of the user.\n\n        Raises:\n            OasstError: 403 if untrusted client attempts to delete a user. 404 if user with ID not found.\n        \"\"\"\n        if not self.api_client.trusted:\n            raise OasstError(\"Forbidden\", OasstErrorCode.API_CLIENT_NOT_AUTHORIZED, HTTP_403_FORBIDDEN)\n\n        user: User = self.db.query(User).filter(User.id == id).first()\n\n        if user is None:\n            raise OasstError(\"User not found\", OasstErrorCode.USER_NOT_FOUND, HTTP_404_NOT_FOUND)\n\n        user.deleted = True\n\n        # Anonymise user data\n        user.display_name = shared_utils.DELETED_USER_DISPLAY_NAME\n        # Ensure uniqueness of (username, auth_method, api_client_id) Index\n        user.username = f\"{shared_utils.DELETED_USER_ID_PREFIX}{user.id}\"\n        user.show_on_leaderboard = False\n\n        self.db.add(user)\n\n    @managed_tx_method(CommitMode.COMMIT)\n    def _lookup_user_tx(\n        self,\n        *,\n        username: str,\n        auth_method: str,\n        display_name: Optional[str] = None,\n        create_missing: bool = True,\n    ) -> User | None:\n        user: User = (\n            self.db.query(User)\n            .filter(\n                User.api_client_id == self.api_client.id,\n                User.username == username,\n                User.auth_method == auth_method,\n            )\n            .first()\n        )\n        if user is None:\n            if create_missing:\n                # user is unknown, create new record\n                user = User(\n                    username=username,\n                    display_name=display_name,\n                    api_client_id=self.api_client.id,\n                    auth_method=auth_method,\n                )\n                if auth_method == \"system\":\n                    user.show_on_leaderboard = False  # don't show system users, e.g. import user\n                    user.tos_acceptance_date = utcnow()\n                self.db.add(user)\n        elif display_name and display_name != user.display_name:\n            # we found the user but the display name changed\n            user.display_name = display_name\n            self.db.add(user)\n\n        return user\n\n    def lookup_client_user(self, client_user: protocol_schema.User, create_missing: bool = True) -> User | None:\n        if not client_user:\n            return None\n\n        if not (client_user.auth_method and client_user.id):\n            raise OasstError(\"Auth method or username missing.\", OasstErrorCode.AUTH_AND_USERNAME_REQUIRED)\n\n        num_retries = settings.DATABASE_MAX_TX_RETRY_COUNT\n        for i in range(num_retries):\n            try:\n                return self._lookup_user_tx(\n                    username=client_user.id,\n                    auth_method=client_user.auth_method,\n                    display_name=client_user.display_name,\n                    create_missing=create_missing,\n                )\n            except IntegrityError:\n                # catch UniqueViolation exception, for concurrent requests due to conflicts in ix_user_username\n                if i + 1 == num_retries:\n                    raise\n\n    @managed_tx_method(CommitMode.COMMIT)\n    def lookup_system_user(self, username: str, create_missing: bool = True) -> User | None:\n        return self._lookup_user_tx(\n            username=username,\n            auth_method=\"system\",\n            display_name=f\"__system__/{username}\",\n            create_missing=create_missing,\n        )\n\n    def query_users_ordered_by_username(\n        self,\n        api_client_id: Optional[UUID] = None,\n        gte_username: Optional[str] = None,\n        gt_id: Optional[UUID] = None,\n        lte_username: Optional[str] = None,\n        lt_id: Optional[UUID] = None,\n        auth_method: Optional[str] = None,\n        search_text: Optional[str] = None,\n        limit: Optional[int] = 100,\n        desc: bool = False,\n    ) -> list[User]:\n        if not self.api_client.trusted:\n            if not api_client_id:\n                api_client_id = self.api_client.id\n\n            if api_client_id != self.api_client.id:\n                raise OasstError(\"Forbidden\", OasstErrorCode.API_CLIENT_NOT_AUTHORIZED, HTTP_403_FORBIDDEN)\n\n        qry = self.db.query(User)\n\n        if gte_username is not None:\n            if gt_id:\n                qry = qry.filter(\n                    or_(User.username > gte_username, and_(User.username == gte_username, User.id > gt_id))\n                )\n            else:\n                qry = qry.filter(User.username >= gte_username)\n        elif gt_id:\n            raise OasstError(\"Need id and name for keyset pagination\", OasstErrorCode.GENERIC_ERROR)\n\n        if lte_username is not None:\n            if lt_id:\n                qry = qry.filter(\n                    or_(User.username < lte_username, and_(User.username == lte_username, User.id < lt_id))\n                )\n            else:\n                qry = qry.filter(User.username <= lte_username)\n        elif lt_id:\n            raise OasstError(\"Need id and name for keyset pagination\", OasstErrorCode.GENERIC_ERROR)\n\n        if auth_method:\n            qry = qry.filter(User.auth_method == auth_method)\n        if api_client_id:\n            qry = qry.filter(User.api_client_id == api_client_id)\n\n        if search_text:\n            pattern = \"%{}%\".format(search_text.replace(\"\\\\\", \"\\\\\\\\\").replace(\"_\", \"\\\\_\").replace(\"%\", \"\\\\%\"))\n            qry = qry.filter(User.username.like(pattern))\n\n        if desc:\n            qry = qry.order_by(User.username.desc(), User.id.desc())\n        else:\n            qry = qry.order_by(User.username, User.id)\n\n        if limit is not None:\n            qry = qry.limit(limit)\n\n        return qry.all()\n\n    def query_users_ordered_by_display_name(\n        self,\n        gte_display_name: Optional[str] = None,\n        gt_id: Optional[UUID] = None,\n        lte_display_name: Optional[str] = None,\n        lt_id: Optional[UUID] = None,\n        api_client_id: Optional[UUID] = None,\n        auth_method: Optional[str] = None,\n        search_text: Optional[str] = None,\n        limit: Optional[int] = 100,\n        desc: bool = False,\n    ) -> list[User]:\n        if not self.api_client.trusted:\n            if not api_client_id:\n                # Let unprivileged api clients query their own users without api_client_id being set\n                api_client_id = self.api_client.id\n\n            if api_client_id != self.api_client.id:\n                # Unprivileged api client asks for foreign users\n                raise OasstError(\"Forbidden\", OasstErrorCode.API_CLIENT_NOT_AUTHORIZED, HTTP_403_FORBIDDEN)\n\n        qry = self.db.query(User)\n\n        if gte_display_name is not None:\n            if gt_id:\n                qry = qry.filter(\n                    or_(\n                        User.display_name > gte_display_name,\n                        and_(User.display_name == gte_display_name, User.id > gt_id),\n                    )\n                )\n            else:\n                qry = qry.filter(User.display_name >= gte_display_name)\n        elif gt_id:\n            raise OasstError(\"Need id and name for keyset pagination\", OasstErrorCode.GENERIC_ERROR)\n\n        if lte_display_name is not None:\n            if lt_id:\n                qry = qry.filter(\n                    or_(\n                        User.display_name < lte_display_name,\n                        and_(User.display_name == lte_display_name, User.id < lt_id),\n                    )\n                )\n            else:\n                qry = qry.filter(User.display_name <= lte_display_name)\n        elif lt_id:\n            raise OasstError(\"Need id and name for keyset pagination\", OasstErrorCode.GENERIC_ERROR)\n\n        if auth_method:\n            qry = qry.filter(User.auth_method == auth_method)\n        if api_client_id:\n            qry = qry.filter(User.api_client_id == api_client_id)\n\n        if search_text:\n            pattern = \"%{}%\".format(search_text.replace(\"\\\\\", \"\\\\\\\\\").replace(\"_\", \"\\\\_\").replace(\"%\", \"\\\\%\"))\n            qry = qry.filter(User.display_name.like(pattern))\n\n        if auth_method:\n            qry = qry.filter(User.auth_method == auth_method)\n\n        if desc:\n            qry = qry.order_by(User.display_name.desc(), User.id.desc())\n        else:\n            qry = qry.order_by(User.display_name, User.id)\n\n        if limit is not None:\n            qry = qry.limit(limit)\n\n        return qry.all()\n\n    @managed_tx_method(CommitMode.FLUSH)\n    def update_user_last_activity(self, user: User, update_streak: bool = False) -> None:\n        current_time = utcnow()\n        user.last_activity_date = current_time\n\n        if update_streak:\n            if user.streak_last_day_date is None or user.streak_last_day_date > current_time:\n                # begin new streak\n                user.streak_last_day_date = current_time\n                user.streak_days = 0\n            else:\n                # update streak day count\n                user.streak_days = (current_time - user.streak_last_day_date).days\n\n        self.db.add(user)\n\n    @managed_tx_method(CommitMode.FLUSH)\n    def merge_users(self, destination_user_id: UUID, source_user_ids: list[UUID]) -> None:\n        source_user_ids = list(filter(lambda x: x != destination_user_id, source_user_ids))\n        if not source_user_ids:\n            return\n\n        # ensure the destination user exists\n        self.get_user(id=destination_user_id)\n\n        # update rows in tables that have affected users_ids as FK\n        models_to_update = [\n            models.Message,\n            models.MessageRevision,\n            models.MessageReaction,\n            models.MessageEmoji,\n            models.TextLabels,\n            models.Task,\n            models.Journal,\n        ]\n        for table in models_to_update:\n            qry = update(table).where(table.user_id.in_(source_user_ids)).values(user_id=destination_user_id)\n            self.db.execute(qry)\n\n        # delete rows in user stats tables\n        models_to_delete = [models.UserStats, models.TrollStats]\n        for table in models_to_delete:\n            qry = delete(table).where(table.user_id.in_(source_user_ids))\n            self.db.execute(qry)\n\n        # finally delete source users from main user table\n        qry = delete(User).where(User.id.in_(source_user_ids))\n        self.db.execute(qry)\n", "backend/oasst_backend/__init__.py": "", "backend/oasst_backend/task_repository.py": "from datetime import datetime, timedelta\nfrom typing import Optional\nfrom uuid import UUID\n\nimport oasst_backend.models.db_payload as db_payload\nfrom loguru import logger\nfrom oasst_backend.config import settings\nfrom oasst_backend.models import ApiClient, Task\nfrom oasst_backend.models.payload_column_type import PayloadContainer\nfrom oasst_backend.user_repository import UserRepository\nfrom oasst_backend.utils.database_utils import CommitMode, managed_tx_method\nfrom oasst_shared.exceptions.oasst_api_error import OasstError, OasstErrorCode\nfrom oasst_shared.schemas import protocol as protocol_schema\nfrom oasst_shared.utils import utcnow\nfrom sqlmodel import Session, delete, false, func, not_, or_\nfrom starlette.status import HTTP_404_NOT_FOUND\n\n\ndef validate_frontend_message_id(message_id: str) -> None:\n    # TODO: Should it be replaced with fastapi/pydantic validation?\n    if not isinstance(message_id, str):\n        raise OasstError(\n            f\"message_id must be string, not {type(message_id)}\", OasstErrorCode.INVALID_FRONTEND_MESSAGE_ID\n        )\n    if not message_id:\n        raise OasstError(\"message_id must not be empty\", OasstErrorCode.INVALID_FRONTEND_MESSAGE_ID)\n\n\ndef delete_expired_tasks(session: Session) -> int:\n    stm = delete(Task).where(Task.expiry_date < utcnow(), Task.done == false())\n    result = session.exec(stm)\n    logger.info(f\"Deleted {result.rowcount} expired tasks.\")\n    return result.rowcount\n\n\nclass TaskRepository:\n    def __init__(\n        self,\n        db: Session,\n        api_client: ApiClient,\n        client_user: Optional[protocol_schema.User],\n        user_repository: UserRepository,\n    ):\n        self.db = db\n        self.api_client = api_client\n        self.user_repository = user_repository\n        self.user = self.user_repository.lookup_client_user(client_user, create_missing=True)\n        self.user_id = self.user.id if self.user else None\n\n    def store_task(\n        self,\n        task: protocol_schema.Task,\n        message_tree_id: UUID = None,\n        parent_message_id: UUID = None,\n        collective: bool = False,\n    ) -> Task:\n        payload: db_payload.TaskPayload\n        match type(task):\n            case protocol_schema.SummarizeStoryTask:\n                payload = db_payload.SummarizationStoryPayload(story=task.story)\n\n            case protocol_schema.RateSummaryTask:\n                payload = db_payload.RateSummaryPayload(\n                    full_text=task.full_text, summary=task.summary, scale=task.scale\n                )\n\n            case protocol_schema.InitialPromptTask:\n                payload = db_payload.InitialPromptPayload(hint=task.hint)\n\n            case protocol_schema.PrompterReplyTask:\n                payload = db_payload.PrompterReplyPayload(conversation=task.conversation, hint=task.hint)\n\n            case protocol_schema.AssistantReplyTask:\n                payload = db_payload.AssistantReplyPayload(type=task.type, conversation=task.conversation)\n\n            case protocol_schema.RankInitialPromptsTask:\n                payload = db_payload.RankInitialPromptsPayload(type=task.type, prompt_messages=task.prompt_messages)\n\n            case protocol_schema.RankPrompterRepliesTask:\n                payload = db_payload.RankPrompterRepliesPayload(\n                    type=task.type,\n                    conversation=task.conversation,\n                    reply_messages=task.reply_messages,\n                    ranking_parent_id=task.ranking_parent_id,\n                    message_tree_id=task.message_tree_id,\n                    reveal_synthetic=task.reveal_synthetic,\n                )\n\n            case protocol_schema.RankAssistantRepliesTask:\n                payload = db_payload.RankAssistantRepliesPayload(\n                    type=task.type,\n                    conversation=task.conversation,\n                    reply_messages=task.reply_messages,\n                    ranking_parent_id=task.ranking_parent_id,\n                    message_tree_id=task.message_tree_id,\n                    reveal_synthetic=task.reveal_synthetic,\n                )\n\n            case protocol_schema.LabelInitialPromptTask:\n                payload = db_payload.LabelInitialPromptPayload(\n                    type=task.type,\n                    message_id=task.message_id,\n                    prompt=task.prompt,\n                    valid_labels=task.valid_labels,\n                    mandatory_labels=task.mandatory_labels,\n                    mode=task.mode,\n                )\n\n            case protocol_schema.LabelPrompterReplyTask:\n                payload = db_payload.LabelPrompterReplyPayload(\n                    type=task.type,\n                    message_id=task.message_id,\n                    conversation=task.conversation,\n                    valid_labels=task.valid_labels,\n                    mandatory_labels=task.mandatory_labels,\n                    mode=task.mode,\n                )\n\n            case protocol_schema.LabelAssistantReplyTask:\n                payload = db_payload.LabelAssistantReplyPayload(\n                    type=task.type,\n                    message_id=task.message_id,\n                    conversation=task.conversation,\n                    valid_labels=task.valid_labels,\n                    mandatory_labels=task.mandatory_labels,\n                    mode=task.mode,\n                )\n\n            case _:\n                raise OasstError(f\"Invalid task type: {type(task)=}\", OasstErrorCode.INVALID_TASK_TYPE)\n\n        if not collective and settings.TASK_VALIDITY_MINUTES > 0:\n            expiry_date = utcnow() + timedelta(minutes=settings.TASK_VALIDITY_MINUTES)\n        else:\n            expiry_date = None\n\n        task_model = self.insert_task(\n            payload=payload,\n            id=task.id,\n            message_tree_id=message_tree_id,\n            parent_message_id=parent_message_id,\n            collective=collective,\n            expiry_date=expiry_date,\n        )\n        assert task_model.id == task.id\n        return task_model\n\n    @managed_tx_method(CommitMode.COMMIT)\n    def bind_frontend_message_id(self, task_id: UUID, frontend_message_id: str) -> None:\n        validate_frontend_message_id(frontend_message_id)\n\n        # find task\n        task: Task = self.db.query(Task).filter(Task.id == task_id, Task.api_client_id == self.api_client.id).first()\n        if task is None:\n            raise OasstError(f\"Task for {task_id=} not found\", OasstErrorCode.TASK_NOT_FOUND, HTTP_404_NOT_FOUND)\n\n        if task.ack and task.frontend_message_id == frontend_message_id:\n            return  # ACK is idempotent if called with the same frontend_message_id\n\n        if task.expired:\n            raise OasstError(\"Task already expired.\", OasstErrorCode.TASK_EXPIRED)\n\n        if task.done or task.ack is not None:\n            raise OasstError(\"Task already updated.\", OasstErrorCode.TASK_ALREADY_UPDATED)\n\n        task.frontend_message_id = frontend_message_id\n        task.ack = True\n        self.db.add(task)\n\n    @managed_tx_method(CommitMode.COMMIT)\n    def close_task(self, frontend_message_id: str, allow_personal_tasks: bool = False):\n        \"\"\"\n        Mark task as done. No further messages will be accepted for this task.\n        \"\"\"\n        validate_frontend_message_id(frontend_message_id)\n        task = self.task_repository.fetch_task_by_frontend_message_id(frontend_message_id)\n\n        if not task:\n            raise OasstError(\n                f\"Task for {frontend_message_id=} not found\", OasstErrorCode.TASK_NOT_FOUND, HTTP_404_NOT_FOUND\n            )\n        if task.expired:\n            raise OasstError(\"Task already expired\", OasstErrorCode.TASK_EXPIRED)\n        if not allow_personal_tasks and not task.collective:\n            raise OasstError(\"This is not a collective task\", OasstErrorCode.TASK_NOT_COLLECTIVE)\n        if task.done:\n            raise OasstError(\"Already closed\", OasstErrorCode.TASK_ALREADY_DONE)\n\n        task.done = True\n        self.db.add(task)\n\n    @managed_tx_method(CommitMode.COMMIT)\n    def insert_task(\n        self,\n        payload: db_payload.TaskPayload,\n        id: UUID = None,\n        message_tree_id: UUID = None,\n        parent_message_id: UUID = None,\n        collective: bool = False,\n        expiry_date: datetime = None,\n    ) -> Task:\n        c = PayloadContainer(payload=payload)\n        task = Task(\n            id=id,\n            user_id=self.user_id,\n            payload_type=type(payload).__name__,\n            payload=c,\n            api_client_id=self.api_client.id,\n            message_tree_id=message_tree_id,\n            parent_message_id=parent_message_id,\n            collective=collective,\n            expiry_date=expiry_date,\n        )\n        logger.debug(f\"inserting {task=}\")\n        self.db.add(task)\n        return task\n\n    def fetch_task_by_frontend_message_id(self, message_id: str) -> Task:\n        validate_frontend_message_id(message_id)\n        task = (\n            self.db.query(Task)\n            .filter(Task.api_client_id == self.api_client.id, Task.frontend_message_id == message_id)\n            .one_or_none()\n        )\n        return task\n\n    def fetch_task_by_id(self, task_id: UUID) -> Task:\n        task = self.db.query(Task).filter(Task.api_client_id == self.api_client.id, Task.id == task_id).one_or_none()\n        return task\n\n    def fetch_recent_reply_tasks(\n        self,\n        max_age: timedelta = timedelta(minutes=5),\n        done: bool = False,\n        skipped: bool = False,\n        limit: int = 100,\n    ) -> list[Task]:\n        qry = self.db.query(Task).filter(\n            Task.created_date > func.current_timestamp() - max_age,\n            or_(Task.payload_type == \"AssistantReplyPayload\", Task.payload_type == \"PrompterReplyPayload\"),\n        )\n        if done is not None:\n            qry = qry.filter(Task.done == done)\n        if skipped is not None:\n            qry = qry.filter(Task.skipped == skipped)\n        if limit:\n            qry = qry.limit(limit)\n        return qry.all()\n\n    def delete_expired(self) -> int:\n        return delete_expired_tasks(self.db)\n\n    def fetch_pending_tasks_of_user(\n        self,\n        user_id: UUID,\n        max_age: timedelta = timedelta(minutes=5),\n        limit: int = 100,\n    ) -> list[Task]:\n        qry = (\n            self.db.query(Task)\n            .filter(\n                Task.user_id == user_id,\n                Task.created_date > func.current_timestamp() - max_age,\n                not_(Task.done),\n                not_(Task.skipped),\n            )\n            .order_by(Task.created_date)\n        )\n        if limit:\n            qry = qry.limit(limit)\n        return qry.all()\n", "backend/oasst_backend/tree_manager.py": "import random\nfrom datetime import datetime, timedelta\nfrom enum import Enum\nfrom http import HTTPStatus\nfrom typing import Optional, Tuple\nfrom uuid import UUID\n\nimport numpy as np\nimport pydantic\nimport sqlalchemy as sa\nfrom loguru import logger\nfrom oasst_backend.api.v1.utils import prepare_conversation, prepare_conversation_message_list\nfrom oasst_backend.config import TreeManagerConfiguration, settings\nfrom oasst_backend.models import (\n    Message,\n    MessageEmoji,\n    MessageReaction,\n    MessageTreeState,\n    Task,\n    TextLabels,\n    User,\n    UserStats,\n    UserStatsTimeFrame,\n    message_tree_state,\n)\nfrom oasst_backend.prompt_repository import PromptRepository\nfrom oasst_backend.scheduled_tasks import hf_feature_extraction, toxicity\nfrom oasst_backend.utils.database_utils import (\n    CommitMode,\n    async_managed_tx_method,\n    managed_tx_function,\n    managed_tx_method,\n)\nfrom oasst_backend.utils.ranking import ranked_pairs\nfrom oasst_shared.exceptions.oasst_api_error import OasstError, OasstErrorCode\nfrom oasst_shared.schemas import protocol as protocol_schema\nfrom oasst_shared.utils import utcnow\nfrom sqlalchemy.sql.functions import coalesce\nfrom sqlmodel import Session, and_, func, not_, or_, text, update\n\n\nclass TaskType(Enum):\n    NONE = -1\n    RANKING = 0\n    LABEL_REPLY = 1\n    REPLY = 2\n    LABEL_PROMPT = 3\n    PROMPT = 4\n\n\nclass TaskRole(Enum):\n    ANY = 0\n    PROMPTER = 1\n    ASSISTANT = 2\n\n\nclass TreeStateStats(pydantic.BaseModel):\n    initial_prompt_review: int\n    growing: int\n    ranking: int\n    ready_for_scoring: int\n    scoring_failed: int\n    ready_for_export: int\n    aborted_low_grade: int\n    halted_by_moderator: int\n    backlog_ranking: int\n    prompt_lottery_waiting: int\n\n\nclass ActiveTreeSizeRow(pydantic.BaseModel):\n    message_tree_id: UUID\n    goal_tree_size: int\n    tree_size: int\n    awaiting_review: Optional[int]\n\n    @property\n    def remaining_messages(self) -> int:\n        return max(0, self.goal_tree_size - self.tree_size)\n\n    class Config:\n        orm_mode = True\n\n\nclass ExtendibleParentRow(pydantic.BaseModel):\n    parent_id: UUID\n    parent_role: str\n    depth: int\n    message_tree_id: UUID\n    active_children_count: int\n\n    class Config:\n        orm_mode = True\n\n\nclass IncompleteRankingsRow(pydantic.BaseModel):\n    parent_id: UUID\n    role: str\n    children_count: int\n    child_min_ranking_count: int\n    message_tree_id: UUID\n\n    class Config:\n        orm_mode = True\n\n\nclass TreeMessageCountStats(pydantic.BaseModel):\n    message_tree_id: UUID\n    state: str\n    depth: int\n    oldest: datetime\n    youngest: datetime\n    count: int\n    goal_tree_size: int\n\n    @property\n    def completed(self) -> int:\n        return self.count / self.goal_tree_size\n\n\nclass TreeManagerStats(pydantic.BaseModel):\n    state_counts: dict[str, int]\n    message_counts: list[TreeMessageCountStats]\n\n\ndef halt_prompts_of_disabled_users(db: Session):\n    _sql_halt_prompts_of_disabled_users = \"\"\"\n-- remove prompts of disabled & deleted users from prompt lottery\nWITH cte AS (\nSELECT mts.message_tree_id\nFROM message_tree_state mts\nJOIN message m ON mts.message_tree_id = m.id\nJOIN \"user\" u ON m.user_id = u.id\nWHERE state = :prompt_lottery_waiting_state AND (NOT u.enabled OR u.deleted)\n)\nUPDATE message_tree_state mts2\nSET active=false, state=:halted_by_moderator_state\nFROM cte\nWHERE mts2.message_tree_id = cte.message_tree_id;\n\"\"\"\n\n    r = db.execute(\n        text(_sql_halt_prompts_of_disabled_users),\n        {\n            \"prompt_lottery_waiting_state\": message_tree_state.State.PROMPT_LOTTERY_WAITING,\n            \"halted_by_moderator_state\": message_tree_state.State.HALTED_BY_MODERATOR,\n        },\n    )\n    if r.rowcount > 0:\n        logger.info(f\"Halted {r.rowcount} prompts of disabled users.\")\n\n\nclass TreeManager:\n    def __init__(\n        self,\n        db: Session,\n        prompt_repository: PromptRepository,\n        cfg: Optional[TreeManagerConfiguration] = None,\n    ):\n        self.db = db\n        self.cfg = cfg or settings.tree_manager\n        self.pr = prompt_repository\n\n    def _random_task_selection(\n        self,\n        num_ranking_tasks: int,\n        num_replies_need_review: int,\n        num_prompts_need_review: int,\n        num_missing_prompts: int,\n        num_missing_replies: int,\n    ) -> TaskType:\n        \"\"\"\n        Determines which task to hand out to human worker.\n        The task type is drawn with relative weight (e.g. ranking has highest priority)\n        depending on what is possible with the current message trees in the database.\n        \"\"\"\n\n        logger.debug(\n            f\"TreeManager._random_task_selection({num_ranking_tasks=}, {num_replies_need_review=}, \"\n            f\"{num_prompts_need_review=}, {num_missing_prompts=}, {num_missing_replies=})\"\n        )\n\n        task_type = TaskType.NONE\n        task_weights = [0] * 5\n\n        if num_ranking_tasks > 0:\n            task_weights[TaskType.RANKING.value] = 10\n\n        if num_replies_need_review > 0:\n            task_weights[TaskType.LABEL_REPLY.value] = 5\n\n        if num_prompts_need_review > 0:\n            task_weights[TaskType.LABEL_PROMPT.value] = 5\n\n        if num_missing_replies > 0:\n            task_weights[TaskType.REPLY.value] = 2\n\n        if num_missing_prompts > 0:\n            task_weights[TaskType.PROMPT.value] = 0.01\n\n        task_weights = np.array(task_weights)\n        weight_sum = task_weights.sum()\n        if weight_sum > 1e-8:\n            task_weights = task_weights / weight_sum\n            task_type = TaskType(np.random.choice(a=len(task_weights), p=task_weights))\n\n        logger.debug(f\"Selected {task_type=}\")\n        return task_type\n\n    def _determine_task_availability_internal(\n        self,\n        num_missing_prompts: int,\n        extendible_parents: list[ExtendibleParentRow],\n        prompts_need_review: list[Message],\n        replies_need_review: list[Message],\n        incomplete_rankings: list[IncompleteRankingsRow],\n    ) -> dict[protocol_schema.TaskRequestType, int]:\n        task_count_by_type: dict[protocol_schema.TaskRequestType, int] = {t: 0 for t in protocol_schema.TaskRequestType}\n\n        task_count_by_type[protocol_schema.TaskRequestType.initial_prompt] = max(0, num_missing_prompts)\n\n        task_count_by_type[protocol_schema.TaskRequestType.prompter_reply] = len(\n            list(filter(lambda x: x.parent_role == \"assistant\", extendible_parents))\n        )\n        task_count_by_type[protocol_schema.TaskRequestType.assistant_reply] = len(\n            list(filter(lambda x: x.parent_role == \"prompter\", extendible_parents))\n        )\n\n        task_count_by_type[protocol_schema.TaskRequestType.label_initial_prompt] = len(prompts_need_review)\n        task_count_by_type[protocol_schema.TaskRequestType.label_assistant_reply] = len(\n            list(filter(lambda m: m.role == \"assistant\", replies_need_review))\n        )\n        task_count_by_type[protocol_schema.TaskRequestType.label_prompter_reply] = len(\n            list(filter(lambda m: m.role == \"prompter\", replies_need_review))\n        )\n\n        if self.cfg.rank_prompter_replies:\n            task_count_by_type[protocol_schema.TaskRequestType.rank_prompter_replies] = len(\n                list(filter(lambda r: r.role == \"prompter\", incomplete_rankings))\n            )\n\n        task_count_by_type[protocol_schema.TaskRequestType.rank_assistant_replies] = len(\n            list(filter(lambda r: r.role == \"assistant\", incomplete_rankings))\n        )\n\n        task_count_by_type[protocol_schema.TaskRequestType.random] = sum(\n            task_count_by_type[t] for t in protocol_schema.TaskRequestType if t in task_count_by_type\n        )\n\n        return task_count_by_type\n\n    def _prompt_lottery(self, lang: str, max_activate: int = 1) -> int:\n        # Under high load the DB runs into deadlocks when many trees are released\n        # simultaneously (happens whens the max_active_trees setting is increased).\n        # To reduce the chance of write conflicts during updates of rows in the\n        # message_tree_state table we limit the number of trees that are activated\n        # per _prompt_lottery() call to max_activate.\n        activated = 0\n\n        while True:\n            stats = self.tree_counts_by_state_stats(lang=lang, only_active=True)\n            prompt_lottery_waiting = self.query_prompt_lottery_waiting(lang=lang)\n            remaining_lottery_entries = max(0, self.cfg.max_prompt_lottery_waiting - prompt_lottery_waiting)\n            remaining_prompt_review = max(0, self.cfg.max_initial_prompt_review - stats.initial_prompt_review)\n            num_missing_growing = max(0, self.cfg.max_active_trees - stats.growing)\n            logger.info(f\"_prompt_lottery {remaining_prompt_review=}, {num_missing_growing=}\")\n\n            if num_missing_growing == 0 or activated >= max_activate:\n                return min(num_missing_growing + remaining_prompt_review, remaining_lottery_entries)\n\n            @managed_tx_function(CommitMode.COMMIT)\n            def activate_one(db: Session) -> int:\n                # select among distinct users\n                authors_qry = (\n                    db.query(Message.user_id, func.coalesce(UserStats.reply_ranked_1, 0).label(\"reply_ranked_1\"))\n                    .select_from(MessageTreeState)\n                    .join(Message, MessageTreeState.message_tree_id == Message.id)\n                    .join(User, Message.user_id == User.id)\n                    .outerjoin(\n                        UserStats, and_(UserStats.user_id == User.id, UserStats.time_frame == UserStatsTimeFrame.month)\n                    )\n                    .filter(\n                        MessageTreeState.state == message_tree_state.State.PROMPT_LOTTERY_WAITING,\n                        Message.lang == lang,\n                        not_(Message.deleted),\n                        Message.review_result,\n                        User.enabled,\n                        not_(User.deleted),\n                    )\n                    .distinct(Message.user_id)\n                )\n\n                author_data = authors_qry.all()\n                if len(author_data) == 0:\n                    logger.info(\n                        f\"No prompts for prompt lottery available ({num_missing_growing=}, trees missing for {lang=}).\"\n                    )\n                    return False\n\n                author_ids = [data[\"user_id\"] for data in author_data]\n                # add one to avoid any scenario where all weights are 0\n                # this also means inactive users can still occasionally be selected\n                weights = [data[\"reply_ranked_1\"] + 1 for data in author_data]\n\n                # first select an author\n                prompt_author_id: UUID = random.choices(author_ids, weights=weights)[0]\n                logger.info(f\"Selected random prompt author {prompt_author_id} among {len(author_data)} candidates.\")\n\n                # select random prompt of author\n                qry = (\n                    db.query(MessageTreeState, Message)\n                    .select_from(MessageTreeState)\n                    .join(Message, MessageTreeState.message_tree_id == Message.id)\n                    .filter(\n                        MessageTreeState.state == message_tree_state.State.PROMPT_LOTTERY_WAITING,\n                        Message.user_id == prompt_author_id,\n                        Message.lang == lang,\n                        not_(Message.deleted),\n                        Message.review_result,\n                    )\n                    .limit(100)\n                )\n\n                prompt_candidates = qry.all()\n                if len(prompt_candidates) == 0:\n                    logger.warning(\"No prompt candidates of selected author found.\")\n                    return False\n\n                winner_prompt = random.choice(prompt_candidates)\n                message: Message = winner_prompt.Message\n                logger.info(f\"Prompt lottery winner: {message.id=}\")\n\n                mts: MessageTreeState = winner_prompt.MessageTreeState\n                mts.state = message_tree_state.State.GROWING\n                mts.active = True\n                db.add(mts)\n\n                if mts.won_prompt_lottery_date is None:\n                    mts.won_prompt_lottery_date = utcnow()\n                logger.info(f\"Tree entered '{mts.state}' state ({mts.message_tree_id=})\")\n\n                return True\n\n            if not activate_one():\n                return min(num_missing_growing + remaining_prompt_review, remaining_lottery_entries)\n\n            activated += 1\n\n    def _auto_moderation(self, lang: str) -> None:\n        if not self.cfg.auto_mod_enabled:\n            return\n\n        bad_messages = self.query_moderation_bad_messages(lang=lang)\n        for m in bad_messages:\n            num_red_flag = m.emojis.get(protocol_schema.EmojiCode.red_flag)\n\n            if num_red_flag is not None and num_red_flag >= self.cfg.auto_mod_red_flags:\n                if m.parent_id is None:\n                    logger.warning(\n                        f\"[AUTO MOD] Halting tree {m.message_tree_id}, initial prompt got too many red flags ({m.emojis}).\"\n                    )\n                    self.enter_low_grade_state(m.message_tree_id)\n                else:\n                    logger.warning(f\"[AUTO MOD] Deleting message {m.id=}, it received too many red flags ({m.emojis}).\")\n                    self.pr.mark_messages_deleted(m.id, recursive=True)\n\n            num_skip_reply = m.emojis.get(protocol_schema.EmojiCode.skip_reply)\n            if num_skip_reply is not None and num_skip_reply >= self.cfg.auto_mod_max_skip_reply:\n                logger.warning(\n                    f\"[AUTO MOD] Halting tree {m.message_tree_id} due to high skip-reply count of message {m.id=} ({m.emojis}).\"\n                )\n                self.halt_tree(m.id, halt=True)\n\n    def determine_task_availability(self, lang: str) -> dict[protocol_schema.TaskRequestType, int]:\n        self.pr.ensure_user_is_enabled()\n\n        if not lang:\n            lang = \"en\"\n            logger.warning(\"Task availability request without lang tag received, assuming lang='en'.\")\n\n        if lang in self.cfg.init_prompt_disabled_langs_list:\n            num_missing_prompts = 0\n        else:\n            num_missing_prompts = self._prompt_lottery(lang=lang, max_activate=1)\n\n        self._auto_moderation(lang=lang)\n        extendible_parents, _ = self.query_extendible_parents(lang=lang)\n        prompts_need_review = self.query_prompts_need_review(lang=lang)\n        replies_need_review = self.query_replies_need_review(lang=lang)\n        incomplete_rankings = self.query_incomplete_rankings(lang=lang)\n\n        return self._determine_task_availability_internal(\n            num_missing_prompts=num_missing_prompts,\n            extendible_parents=extendible_parents,\n            prompts_need_review=prompts_need_review,\n            replies_need_review=replies_need_review,\n            incomplete_rankings=incomplete_rankings,\n        )\n\n    @staticmethod\n    def _get_label_descriptions(valid_labels: list[TextLabels]) -> list[protocol_schema.LabelDescription]:\n        return [\n            protocol_schema.LabelDescription(\n                name=l.value, widget=l.widget.value, display_text=l.display_text, help_text=l.help_text\n            )\n            for l in valid_labels\n        ]\n\n    def next_task(\n        self,\n        desired_task_type: protocol_schema.TaskRequestType = protocol_schema.TaskRequestType.random,\n        lang: str = \"en\",\n    ) -> Tuple[protocol_schema.Task, Optional[UUID], Optional[UUID]]:\n        logger.debug(f\"TreeManager.next_task({desired_task_type=}, {lang=})\")\n\n        self.pr.ensure_user_is_enabled()\n\n        if not lang:\n            lang = \"en\"\n            logger.warning(\"Task request without lang tag received, assuming 'en'.\")\n\n        self._auto_moderation(lang=lang)\n        num_missing_prompts = self._prompt_lottery(lang=lang, max_activate=2)\n\n        # check user's pending tasks\n        recent_tasks_span = timedelta(seconds=self.cfg.recent_tasks_span_sec)\n        users_pending_tasks = self.pr.task_repository.fetch_pending_tasks_of_user(\n            self.pr.user_id,\n            max_age=recent_tasks_span,\n            limit=self.cfg.max_pending_tasks_per_user + 1,\n        )\n        num_pending_tasks = len(users_pending_tasks)\n        if num_pending_tasks >= self.cfg.max_pending_tasks_per_user:\n            logger.warning(\n                f\"Rejecting task request. User {self.pr.user_id} has {num_pending_tasks} pending tasks. \"\n                f\"Oldest age: {utcnow()-users_pending_tasks[0].created_date}.\"\n            )\n            raise OasstError(\n                \"User has too many pending tasks.\",\n                OasstErrorCode.TASK_TOO_MANY_PENDING,\n            )\n        elif num_pending_tasks > 0:\n            logger.debug(\n                f\"User {self.pr.user_id} has {num_pending_tasks} pending tasks. Oldest age: {utcnow()-users_pending_tasks[0].created_date}\"\n            )\n\n        prompts_need_review = self.query_prompts_need_review(lang=lang)\n        replies_need_review = self.query_replies_need_review(lang=lang)\n        extendible_parents, active_tree_sizes = self.query_extendible_parents(lang=lang)\n\n        incomplete_rankings = self.query_incomplete_rankings(lang=lang)\n        if not self.cfg.rank_prompter_replies:\n            incomplete_rankings = list(filter(lambda r: r.role == \"assistant\", incomplete_rankings))\n\n        # determine type of task to generate\n        num_missing_replies = sum(x.remaining_messages for x in active_tree_sizes)\n\n        task_role = TaskRole.ANY\n        if desired_task_type == protocol_schema.TaskRequestType.random:\n            task_type = self._random_task_selection(\n                num_ranking_tasks=len(incomplete_rankings),\n                num_replies_need_review=len(replies_need_review),\n                num_prompts_need_review=len(prompts_need_review),\n                num_missing_prompts=num_missing_prompts,\n                num_missing_replies=num_missing_replies,\n            )\n\n            if task_type == TaskType.NONE:\n                logger.warning(f\"No random tasks currently available, user: {self.pr.user_id}\")\n                raise OasstError(\n                    f\"No tasks of type '{protocol_schema.TaskRequestType.random.value}' are currently available.\",\n                    OasstErrorCode.TASK_REQUESTED_TYPE_NOT_AVAILABLE,\n                    HTTPStatus.SERVICE_UNAVAILABLE,\n                )\n        else:\n            task_count_by_type = self._determine_task_availability_internal(\n                num_missing_prompts=num_missing_prompts,\n                extendible_parents=extendible_parents,\n                prompts_need_review=prompts_need_review,\n                replies_need_review=replies_need_review,\n                incomplete_rankings=incomplete_rankings,\n            )\n\n            available_count = task_count_by_type.get(desired_task_type)\n            if not available_count:\n                logger.warning(f\"No '{desired_task_type.value}' tasks currently available, user: {self.pr.user_id}\")\n                raise OasstError(\n                    f\"No tasks of type '{desired_task_type.value}' are currently available.\",\n                    OasstErrorCode.TASK_REQUESTED_TYPE_NOT_AVAILABLE,\n                    HTTPStatus.SERVICE_UNAVAILABLE,\n                )\n\n            task_type_role_map = {\n                protocol_schema.TaskRequestType.initial_prompt: (TaskType.PROMPT, TaskRole.ANY),\n                protocol_schema.TaskRequestType.prompter_reply: (TaskType.REPLY, TaskRole.PROMPTER),\n                protocol_schema.TaskRequestType.assistant_reply: (TaskType.REPLY, TaskRole.ASSISTANT),\n                protocol_schema.TaskRequestType.rank_prompter_replies: (TaskType.RANKING, TaskRole.PROMPTER),\n                protocol_schema.TaskRequestType.rank_assistant_replies: (TaskType.RANKING, TaskRole.ASSISTANT),\n                protocol_schema.TaskRequestType.label_initial_prompt: (TaskType.LABEL_PROMPT, TaskRole.ANY),\n                protocol_schema.TaskRequestType.label_assistant_reply: (TaskType.LABEL_REPLY, TaskRole.ASSISTANT),\n                protocol_schema.TaskRequestType.label_prompter_reply: (TaskType.LABEL_REPLY, TaskRole.PROMPTER),\n            }\n\n            task_type, task_role = task_type_role_map[desired_task_type]\n\n        message_tree_id = None\n        parent_message_id = None\n\n        logger.debug(f\"selected {task_type=}\")\n        match task_type:\n            case TaskType.RANKING:\n                if task_role == TaskRole.PROMPTER:\n                    incomplete_rankings = list(filter(lambda m: m.role == \"prompter\", incomplete_rankings))\n                elif task_role == TaskRole.ASSISTANT:\n                    incomplete_rankings = list(filter(lambda m: m.role == \"assistant\", incomplete_rankings))\n\n                if len(incomplete_rankings) > 0:\n                    ranking_parent_id = random.choice(incomplete_rankings).parent_id\n\n                    messages = self.pr.fetch_message_conversation(ranking_parent_id)\n                    assert len(messages) > 0 and messages[-1].id == ranking_parent_id\n                    ranking_parent = messages[-1]\n                    assert not ranking_parent.deleted and ranking_parent.review_result\n                    conversation = prepare_conversation(messages)\n                    replies = self.pr.fetch_message_children(ranking_parent_id, review_result=True, deleted=False)\n\n                    assert len(replies) > 1\n                    random.shuffle(replies)  # hand out replies in random order\n                    reply_messages = prepare_conversation_message_list(replies)\n                    if any(not m.synthetic for m in reply_messages):\n                        reveal_synthetic = False\n                        for rm in reply_messages:\n                            rm.synthetic = None\n                    else:\n                        reveal_synthetic = True\n\n                    replies = [p.text for p in replies]\n                    if messages[-1].role == \"assistant\":\n                        logger.info(\"Generating a RankPrompterRepliesTask.\")\n                        task = protocol_schema.RankPrompterRepliesTask(\n                            conversation=conversation,\n                            replies=replies,\n                            reply_messages=reply_messages,\n                            ranking_parent_id=ranking_parent.id,\n                            message_tree_id=ranking_parent.message_tree_id,\n                            reveal_synthetic=reveal_synthetic,\n                        )\n                    else:\n                        logger.info(\"Generating a RankAssistantRepliesTask.\")\n                        task = protocol_schema.RankAssistantRepliesTask(\n                            conversation=conversation,\n                            replies=replies,\n                            reply_messages=reply_messages,\n                            ranking_parent_id=ranking_parent.id,\n                            message_tree_id=ranking_parent.message_tree_id,\n                            reveal_synthetic=reveal_synthetic,\n                        )\n\n                    parent_message_id = ranking_parent_id\n                    message_tree_id = messages[-1].message_tree_id\n\n            case TaskType.LABEL_REPLY:\n                if task_role == TaskRole.PROMPTER:\n                    replies_need_review = list(filter(lambda m: m.role == \"prompter\", replies_need_review))\n                elif task_role == TaskRole.ASSISTANT:\n                    replies_need_review = list(filter(lambda m: m.role == \"assistant\", replies_need_review))\n\n                if len(replies_need_review) > 0:\n                    random_reply_message = random.choice(replies_need_review)\n                    messages = self.pr.fetch_message_conversation(random_reply_message)\n\n                    conversation = prepare_conversation(messages)\n                    message = messages[-1]\n\n                    self.cfg.p_full_labeling_review_reply_prompter: float = 0.1\n\n                    label_mode = protocol_schema.LabelTaskMode.full\n                    label_disposition = protocol_schema.LabelTaskDisposition.quality\n\n                    if message.role == \"assistant\":\n                        valid_labels = self.cfg.labels_assistant_reply\n                        if (\n                            desired_task_type == protocol_schema.TaskRequestType.random\n                            and random.random() > self.cfg.p_full_labeling_review_reply_assistant\n                        ):\n                            label_mode = protocol_schema.LabelTaskMode.simple\n                            label_disposition = protocol_schema.LabelTaskDisposition.spam\n                            valid_labels = self.cfg.mandatory_labels_assistant_reply.copy()\n                            if protocol_schema.TextLabel.lang_mismatch not in valid_labels:\n                                valid_labels.append(protocol_schema.TextLabel.lang_mismatch)\n                            if protocol_schema.TextLabel.quality not in valid_labels:\n                                valid_labels.append(protocol_schema.TextLabel.quality)\n\n                        logger.info(f\"Generating a LabelAssistantReplyTask. ({label_mode=:s})\")\n                        task = protocol_schema.LabelAssistantReplyTask(\n                            message_id=message.id,\n                            conversation=conversation,\n                            reply=message.text,\n                            valid_labels=list(map(lambda x: x.value, valid_labels)),\n                            mandatory_labels=list(map(lambda x: x.value, self.cfg.mandatory_labels_assistant_reply)),\n                            mode=label_mode,\n                            disposition=label_disposition,\n                            labels=self._get_label_descriptions(valid_labels),\n                        )\n                    else:\n                        valid_labels = self.cfg.labels_prompter_reply\n                        if (\n                            desired_task_type == protocol_schema.TaskRequestType.random\n                            and random.random() > self.cfg.p_full_labeling_review_reply_prompter\n                        ):\n                            label_mode = protocol_schema.LabelTaskMode.simple\n                            label_disposition = protocol_schema.LabelTaskDisposition.spam\n                            valid_labels = self.cfg.mandatory_labels_prompter_reply.copy()\n                            if protocol_schema.TextLabel.lang_mismatch not in valid_labels:\n                                valid_labels.append(protocol_schema.TextLabel.lang_mismatch)\n                            if protocol_schema.TextLabel.quality not in valid_labels:\n                                valid_labels.append(protocol_schema.TextLabel.quality)\n\n                        logger.info(f\"Generating a LabelPrompterReplyTask. ({label_mode=:s})\")\n                        task = protocol_schema.LabelPrompterReplyTask(\n                            message_id=message.id,\n                            conversation=conversation,\n                            reply=message.text,\n                            valid_labels=list(map(lambda x: x.value, valid_labels)),\n                            mandatory_labels=list(map(lambda x: x.value, self.cfg.mandatory_labels_prompter_reply)),\n                            mode=label_mode,\n                            disposition=label_disposition,\n                            labels=self._get_label_descriptions(valid_labels),\n                        )\n\n                    parent_message_id = message.id\n                    message_tree_id = message.message_tree_id\n\n            case TaskType.REPLY:\n                if task_role == TaskRole.PROMPTER:\n                    extendible_parents = list(filter(lambda x: x.parent_role == \"assistant\", extendible_parents))\n                elif task_role == TaskRole.ASSISTANT:\n                    extendible_parents = list(filter(lambda x: x.parent_role == \"prompter\", extendible_parents))\n\n                # select a tree with missing replies\n                if len(extendible_parents) > 0:\n                    random_parent: ExtendibleParentRow = None\n                    if self.cfg.p_lonely_child_extension > 0 and self.cfg.lonely_children_count > 1:\n                        # check if we have extendible prompter parents with a small number of replies\n                        lonely_children_parents = [\n                            p\n                            for p in extendible_parents\n                            if 0 < p.active_children_count < self.cfg.lonely_children_count\n                            and p.parent_role == \"prompter\"\n                        ]\n                        if len(lonely_children_parents) > 0 and random.random() < self.cfg.p_lonely_child_extension:\n                            random_parent = random.choice(lonely_children_parents)\n\n                    if random_parent is None:\n                        random_parent = random.choice(extendible_parents)\n\n                    # fetch random conversation to extend\n                    logger.debug(f\"selected {random_parent=}\")\n                    messages = self.pr.fetch_message_conversation(random_parent.parent_id)\n                    assert all(m.review_result for m in messages)  # ensure all messages have positive reviews\n                    conversation = prepare_conversation(messages)\n\n                    # generate reply task depending on last message\n                    if messages[-1].role == \"assistant\":\n                        logger.info(\"Generating a PrompterReplyTask.\")\n                        task = protocol_schema.PrompterReplyTask(conversation=conversation)\n                    else:\n                        logger.info(\"Generating a AssistantReplyTask.\")\n                        task = protocol_schema.AssistantReplyTask(conversation=conversation)\n\n                    parent_message_id = messages[-1].id\n                    message_tree_id = messages[-1].message_tree_id\n\n            case TaskType.LABEL_PROMPT:\n                assert len(prompts_need_review) > 0\n                message = random.choice(prompts_need_review)\n                message = self.pr.fetch_message(message.id)  # re-fetch message including emojis\n\n                label_mode = protocol_schema.LabelTaskMode.full\n                label_disposition = protocol_schema.LabelTaskDisposition.quality\n                valid_labels = self.cfg.labels_initial_prompt\n\n                if random.random() > self.cfg.p_full_labeling_review_prompt:\n                    valid_labels = self.cfg.mandatory_labels_initial_prompt.copy()\n                    label_mode = protocol_schema.LabelTaskMode.simple\n                    label_disposition = protocol_schema.LabelTaskDisposition.spam\n                    if protocol_schema.TextLabel.lang_mismatch not in valid_labels:\n                        valid_labels.append(protocol_schema.TextLabel.lang_mismatch)\n\n                logger.info(f\"Generating a LabelInitialPromptTask ({label_mode=:s}).\")\n                task = protocol_schema.LabelInitialPromptTask(\n                    message_id=message.id,\n                    prompt=message.text,\n                    conversation=prepare_conversation([message]),\n                    valid_labels=list(map(lambda x: x.value, valid_labels)),\n                    mandatory_labels=list(map(lambda x: x.value, self.cfg.mandatory_labels_initial_prompt)),\n                    mode=label_mode,\n                    disposition=label_disposition,\n                    labels=self._get_label_descriptions(valid_labels),\n                )\n\n                parent_message_id = message.id\n                message_tree_id = message.message_tree_id\n\n            case TaskType.PROMPT:\n                logger.info(\"Generating an InitialPromptTask.\")\n                task = protocol_schema.InitialPromptTask(hint=None)\n\n            case _:\n                task = None\n\n        if task is None:\n            raise OasstError(\n                f\"No task of type '{desired_task_type.value}' is currently available.\",\n                OasstErrorCode.TASK_REQUESTED_TYPE_NOT_AVAILABLE,\n                HTTPStatus.SERVICE_UNAVAILABLE,\n            )\n\n        logger.info(f\"Generated task (type={task.type}, id={task.id})\")\n        logger.debug(f\"Generated {task=}.\")\n\n        return task, message_tree_id, parent_message_id\n\n    @async_managed_tx_method(CommitMode.FLUSH)\n    async def handle_interaction(self, interaction: protocol_schema.AnyInteraction) -> protocol_schema.Task:\n        pr = self.pr\n        pr.ensure_user_is_enabled()\n        match type(interaction):\n            case protocol_schema.TextReplyToMessage:\n                logger.info(\n                    f\"Frontend reports text reply to message_id={interaction.message_id} by user={interaction.user}.\"\n                )\n                logger.debug(f\"with {interaction.text=}\")\n                # here we store the text reply in the database\n                message = pr.store_text_reply(\n                    text=interaction.text,\n                    lang=interaction.lang,\n                    frontend_message_id=interaction.message_id,\n                    user_frontend_message_id=interaction.user_message_id,\n                )\n\n                if not message.parent_id:\n                    logger.info(\n                        f\"TreeManager: Inserting new tree state for initial prompt {message.id=} [{message.lang}]\"\n                    )\n                    self._insert_default_state(message.id, lang=message.lang)\n\n                if not settings.DEBUG_SKIP_EMBEDDING_COMPUTATION:\n                    try:\n                        hf_feature_extraction.delay(interaction.text, message.id, pr.api_client.dict())\n                        logger.debug(\"Extract Embedding\")\n                    except OasstError:\n                        logger.error(\n                            f\"Could not fetch embbeddings for text reply to {interaction.message_id=} with {interaction.text=} by {interaction.user=}.\"\n                        )\n                if not settings.DEBUG_SKIP_TOXICITY_CALCULATION:\n                    try:\n                        toxicity.delay(interaction.text, message.id, pr.api_client.dict())\n                        logger.debug(\"Sent Toxicity\")\n                    except OasstError:\n                        logger.error(\n                            f\"Could not compute toxicity for text reply to {interaction.message_id=} with {interaction.text=} by {interaction.user=}.\"\n                        )\n\n            case protocol_schema.MessageRating:\n                logger.info(\n                    f\"Frontend reports rating of message_id={interaction.message_id} by user={interaction.user}.\"\n                )\n                logger.debug(f\"with {interaction.rating=}\")\n\n                pr.store_rating(interaction)\n\n            case protocol_schema.MessageRanking:\n                logger.info(\n                    f\"Frontend reports ranking of message_id={interaction.message_id} by user={interaction.user}.\"\n                )\n                logger.debug(f\"with {interaction.ranking=}\")\n\n                _, task = pr.store_ranking(interaction)\n                self.check_condition_for_scoring_state(task.message_tree_id)\n\n            case protocol_schema.TextLabels:\n                logger.info(\n                    f\"Frontend reports labels of message_id={interaction.message_id} by user={interaction.user}.\"\n                )\n                logger.debug(f\"with {interaction.labels=}\")\n\n                _, task, msg = pr.store_text_labels(interaction)\n\n                # if it was a response for a task, check if we have enough reviews to calc review_result\n                if task and msg:\n                    reviews = self.query_reviews_for_message(msg.id)\n                    acceptance_score = self._calculate_acceptance(reviews)\n                    logger.debug(\n                        f\"Message {msg.id=}, {acceptance_score=}, {len(reviews)=}, {msg.review_result=}, {msg.review_count=}\"\n                    )\n                    if msg.parent_id is None:\n                        if not msg.review_result and msg.review_count >= self.cfg.num_reviews_initial_prompt:\n                            if acceptance_score > self.cfg.acceptance_threshold_initial_prompt:\n                                msg.review_result = True\n                                self.db.add(msg)\n                                logger.info(\n                                    f\"Initial prompt message was accepted: {msg.id=}, {acceptance_score=}, {len(reviews)=}\"\n                                )\n                            else:\n                                if msg.review_result is None:\n                                    msg.review_result = False\n                                    self.db.add(msg)\n                                self.enter_low_grade_state(msg.message_tree_id)\n                        self.check_condition_for_prompt_lottery(msg.message_tree_id)\n                    elif msg.review_count >= self.cfg.num_reviews_reply:\n                        if not msg.review_result and acceptance_score > self.cfg.acceptance_threshold_reply:\n                            msg.review_result = True\n                            self.db.add(msg)\n                            logger.info(\n                                f\"Reply message message accepted: {msg.id=}, {acceptance_score=}, {len(reviews)=}\"\n                            )\n                        elif msg.review_result is None:  # do not overwrite existing review result\n                            msg.review_result = False\n                            self.db.add(msg)\n\n                    self.check_condition_for_ranking_state(msg.message_tree_id)\n\n            case _:\n                raise OasstError(\"Invalid response type.\", OasstErrorCode.TASK_INVALID_RESPONSE_TYPE)\n\n        return protocol_schema.TaskDone()\n\n    def _enter_state(self, mts: MessageTreeState, state: message_tree_state.State):\n        assert mts\n\n        is_terminal = state in message_tree_state.TERMINAL_STATES\n        was_active = mts.active\n        mts.active = not is_terminal\n        mts.state = state.value\n        self.db.add(mts)\n        self.db.flush\n\n        if is_terminal:\n            logger.info(f\"Tree entered terminal '{mts.state}' state ({mts.message_tree_id=})\")\n            root_msg = self.pr.fetch_message(message_id=mts.message_tree_id, fail_if_missing=False)\n            if root_msg and was_active:\n                if random.random() < self.cfg.p_activate_backlog_tree:\n                    self.activate_backlog_tree(lang=root_msg.lang)\n\n                if self.cfg.min_active_rankings_per_lang > 0:\n                    incomplete_rankings = self.query_incomplete_rankings(lang=root_msg.lang, user_filter=False)\n                    if len(incomplete_rankings) < self.cfg.min_active_rankings_per_lang:\n                        self.activate_backlog_tree(lang=root_msg.lang)\n        else:\n            if mts.state == message_tree_state.State.GROWING and mts.won_prompt_lottery_date is None:\n                mts.won_prompt_lottery_date = utcnow()\n            logger.info(f\"Tree entered '{mts.state}' state ({mts.message_tree_id=})\")\n\n    def enter_low_grade_state(self, message_tree_id: UUID) -> None:\n        logger.debug(f\"enter_low_grade_state({message_tree_id=})\")\n        mts = self.pr.fetch_tree_state(message_tree_id)\n        self._enter_state(mts, message_tree_state.State.ABORTED_LOW_GRADE)\n\n    def check_condition_for_prompt_lottery(self, message_tree_id: UUID) -> bool:\n        logger.debug(f\"check_condition_for_prompt_lottery({message_tree_id=})\")\n\n        mts = self.pr.fetch_tree_state(message_tree_id)\n        if not mts.active or mts.state != message_tree_state.State.INITIAL_PROMPT_REVIEW:\n            logger.debug(f\"False {mts.active=}, {mts.state=}\")\n            return False\n\n        # check if initial prompt was accepted\n        initial_prompt = self.pr.fetch_message(message_tree_id)\n        if not initial_prompt.review_result:\n            logger.debug(f\"False {initial_prompt.review_result=}\")\n            return False\n\n        self._enter_state(mts, message_tree_state.State.PROMPT_LOTTERY_WAITING)\n        return True\n\n    def check_condition_for_ranking_state(self, message_tree_id: UUID) -> bool:\n        logger.debug(f\"check_condition_for_ranking_state({message_tree_id=})\")\n\n        mts = self.pr.fetch_tree_state(message_tree_id)\n        if not mts.active or mts.state != message_tree_state.State.GROWING:\n            logger.debug(f\"False {mts.active=}, {mts.state=}\")\n            return False\n\n        # check if desired tree size has been reached and all nodes have been reviewed\n        tree_size = self.query_tree_size(message_tree_id)\n        if tree_size.tree_size == 0:\n            logger.warning(\n                f\"All messages of message tree {message_tree_id} were deleted (tree_size == 0), halting tree.\"\n            )\n            self._enter_state(mts, message_tree_state.State.HALTED_BY_MODERATOR)\n            return False\n\n        if tree_size.remaining_messages > 0 or tree_size.awaiting_review > 0:\n            logger.debug(f\"False {tree_size.remaining_messages=}, {tree_size.awaiting_review=}\")\n            return False\n\n        self._enter_state(mts, message_tree_state.State.RANKING)\n        return True\n\n    def check_condition_for_scoring_state(self, message_tree_id: UUID) -> bool:\n        logger.debug(f\"check_condition_for_scoring_state({message_tree_id=})\")\n\n        mts = self.pr.fetch_tree_state(message_tree_id)\n        if mts.state != message_tree_state.State.SCORING_FAILED:\n            if not mts.active or mts.state not in (\n                message_tree_state.State.RANKING,\n                message_tree_state.State.READY_FOR_SCORING,\n            ):\n                logger.debug(f\"False {mts.active=}, {mts.state=}\")\n                return False\n\n        ranking_role_filter = None if self.cfg.rank_prompter_replies else \"assistant\"\n        rankings_by_message = self.query_tree_ranking_results(message_tree_id, role_filter=ranking_role_filter)\n        for parent_msg_id, ranking in rankings_by_message.items():\n            if len(ranking) < self.cfg.num_required_rankings:\n                logger.debug(f\"False {parent_msg_id=} {len(ranking)=}\")\n                return False\n\n        if (\n            mts.state != message_tree_state.State.SCORING_FAILED\n            and mts.state != message_tree_state.State.READY_FOR_SCORING\n        ):\n            self._enter_state(mts, message_tree_state.State.READY_FOR_SCORING)\n        self.update_message_ranks(message_tree_id, rankings_by_message)\n        return True\n\n    def ranked_pairs_update(self, rankings: list[MessageReaction]) -> int:\n        assert len(rankings) > 0\n\n        num_updated = 0\n        ordered_ids_list: list[list[UUID]] = [\n            msg_reaction.payload.payload.ranked_message_ids for msg_reaction in rankings\n        ]\n\n        common_set: set[UUID] = set.intersection(*map(set, ordered_ids_list))\n        if len(common_set) < 2:\n            logger.warning(\"The intersection of ranking results ID sets has less than two elements. Skipping.\")\n            return\n\n        # keep only elements in common set\n        ordered_ids_list = [list(filter(lambda x: x in common_set, ids)) for ids in ordered_ids_list]\n        assert all(len(x) == len(common_set) for x in ordered_ids_list)\n\n        logger.debug(f\"SORTED MESSAGE IDS {ordered_ids_list}\")\n        consensus = ranked_pairs(ordered_ids_list)\n        assert len(consensus) == len(common_set)\n        logger.debug(f\"CONSENSUS: {consensus}\\n\\n\")\n\n        # fetch all siblings and index by id\n        siblings = self.pr.fetch_message_siblings(consensus[0], review_result=None, deleted=None)\n        siblings = {m.id: m for m in siblings}\n\n        # set rank for each message that was part of the common set\n        for rank, message_id in enumerate(consensus):\n            message = siblings.get(message_id)\n            if message:\n                if message.rank != rank:\n                    message.rank = rank\n                    self.db.add(message)\n                    num_updated += 1\n            else:\n                logger.warning(f\"Message {message_id=} not found among siblings.\")\n\n        # clear rank of sibling messages not in consensus\n        for message in siblings.values():\n            if message.id not in consensus and message.rank is not None:\n                message.rank = None\n                self.db.add(message)\n                num_updated += 1\n\n        return num_updated\n\n    def update_message_ranks(\n        self, message_tree_id: UUID, rankings_by_message: dict[UUID, list[MessageReaction]]\n    ) -> bool:\n        mts = self.pr.fetch_tree_state(message_tree_id)\n        # check state, allow retry if in SCORING_FAILED state\n        if mts.state not in (message_tree_state.State.READY_FOR_SCORING, message_tree_state.State.SCORING_FAILED):\n            logger.debug(f\"False {mts.active=}, {mts.state=}\")\n            return False\n\n        if mts.state == message_tree_state.State.SCORING_FAILED:\n            mts.active = True\n            mts.state = message_tree_state.State.READY_FOR_SCORING\n\n        try:\n            for rankings in rankings_by_message.values():\n                if len(rankings) > 0:\n                    self.ranked_pairs_update(rankings)\n\n        except Exception:\n            logger.exception(f\"update_message_ranks({message_tree_id=}) failed\")\n            self._enter_state(mts, message_tree_state.State.SCORING_FAILED)\n            return False\n\n        self._enter_state(mts, message_tree_state.State.READY_FOR_EXPORT)\n\n        return True\n\n    def activate_backlog_tree(self, lang: str) -> MessageTreeState:\n        while True:\n            # find tree in backlog state\n            backlog_tree: MessageTreeState = (\n                self.db.query(MessageTreeState)\n                .join(Message, MessageTreeState.message_tree_id == Message.id)  # root msg\n                .filter(MessageTreeState.state == message_tree_state.State.BACKLOG_RANKING)\n                .filter(Message.lang == lang)\n                .limit(1)\n                .one_or_none()\n            )\n\n            if not backlog_tree:\n                return None\n\n            if len(self.query_tree_ranking_results(message_tree_id=backlog_tree.message_tree_id)) == 0:\n                logger.info(\n                    f\"Backlog tree {backlog_tree.message_tree_id} has no children to rank, aborting with 'aborted_low_grade' state.\"\n                )\n                self._enter_state(backlog_tree, message_tree_state.State.ABORTED_LOW_GRADE)\n            else:\n                logger.info(f\"Activating backlog tree {backlog_tree.message_tree_id}\")\n                backlog_tree.active = True\n                self._enter_state(backlog_tree, message_tree_state.State.RANKING)\n                return backlog_tree\n\n    def _calculate_acceptance(self, labels: list[TextLabels]):\n        # calculate acceptance based on lang_mismatch & spam label\n        lang_mismatch = np.mean([(l.labels.get(protocol_schema.TextLabel.lang_mismatch) or 0) for l in labels])\n        spam = np.mean([l.labels[protocol_schema.TextLabel.spam] for l in labels])\n        acceptance_score = 1 - (spam + lang_mismatch)\n        logger.debug(f\"{acceptance_score=} ({spam=}, {lang_mismatch=})\")\n        return acceptance_score\n\n    def _query_need_review(\n        self, state: message_tree_state.State, required_reviews: int, root: bool, lang: str\n    ) -> list[Message]:\n        need_review = (\n            self.db.query(Message)\n            .select_from(MessageTreeState)\n            .join(Message, MessageTreeState.message_tree_id == Message.message_tree_id)\n            .outerjoin(\n                MessageEmoji,\n                and_(\n                    Message.id == MessageEmoji.message_id,\n                    MessageEmoji.user_id == self.pr.user_id,\n                    MessageEmoji.emoji == protocol_schema.EmojiCode.skip_labeling,\n                ),\n            )\n            .filter(\n                MessageTreeState.active,\n                MessageTreeState.state == state,\n                or_(Message.review_result.is_(None), not_(Message.review_result)),\n                not_(Message.deleted),\n                Message.review_count < required_reviews,\n                Message.lang == lang,\n                MessageEmoji.message_id.is_(None),\n            )\n        )\n\n        if root:\n            need_review = need_review.filter(Message.parent_id.is_(None))\n        else:\n            need_review = need_review.filter(Message.parent_id.is_not(None))\n\n        if not settings.DEBUG_ALLOW_SELF_LABELING:\n            need_review = need_review.filter(Message.user_id != self.pr.user_id)\n\n        if settings.DEBUG_ALLOW_DUPLICATE_TASKS:\n            qry = need_review\n        else:\n            user_id = self.pr.user_id\n            need_review = need_review.cte(name=\"need_review\")\n            qry = (\n                self.db.query(Message)\n                .select_entity_from(need_review)\n                .outerjoin(TextLabels, need_review.c.id == TextLabels.message_id)\n                .group_by(need_review)\n                .having(\n                    func.count(TextLabels.id).filter(TextLabels.task_id.is_not(None), TextLabels.user_id == user_id)\n                    == 0\n                )\n            )\n\n        return qry.all()\n\n    def query_prompts_need_review(self, lang: str) -> list[Message]:\n        \"\"\"\n        Select initial prompt messages with less then required rankings in active message tree\n        (active == True in message_tree_state)\n        \"\"\"\n        return self._query_need_review(\n            message_tree_state.State.INITIAL_PROMPT_REVIEW, self.cfg.num_reviews_initial_prompt, True, lang\n        )\n\n    def query_replies_need_review(self, lang: str) -> list[Message]:\n        \"\"\"\n        Select child messages (parent_id IS NOT NULL) with less then required rankings\n        in active message tree (active == True in message_tree_state)\n        \"\"\"\n        return self._query_need_review(message_tree_state.State.GROWING, self.cfg.num_reviews_reply, False, lang)\n\n    _sql_find_incomplete_rankings = \"\"\"\n-- find incomplete rankings\nSELECT m.parent_id, m.role, COUNT(m.id) children_count, MIN(m.ranking_count) child_min_ranking_count,\n    COUNT(m.id) FILTER (WHERE m.ranking_count >= :num_required_rankings) as completed_rankings,\n    mts.message_tree_id\nFROM message_tree_state mts\n    INNER JOIN message m ON mts.message_tree_id = m.message_tree_id\n    INNER JOIN message p ON m.parent_id = p.id\n    LEFT JOIN message_emoji me on\n        (m.parent_id = me.message_id\n        AND :skip_user_id IS NOT NULL\n        AND me.user_id = :skip_user_id\n        AND me.emoji = :skip_ranking)\nWHERE mts.active                        -- only consider active trees\n    AND mts.state = :ranking_state      -- message tree must be in ranking state\n    AND m.review_result                 -- must be reviewed\n    AND p.lang = :lang                  -- parent lang matches\n    AND NOT m.deleted                   -- not deleted\n    AND m.parent_id IS NOT NULL         -- ignore initial prompts\n    AND me.message_id IS NULL           -- no skip ranking emoji for user\nGROUP BY m.parent_id, m.role, mts.message_tree_id\nHAVING COUNT(m.id) > 1                                      -- more than one child\n    AND MIN(m.ranking_count) < :num_required_rankings       -- not complete\n    AND COUNT(m.id) FILTER (WHERE m.user_id = :rank_user_id) = 0 -- no self-ranking\n\"\"\"\n\n    _sql_find_incomplete_rankings_ex = f\"\"\"\n-- incomplete rankings but exclude of current user\nWITH incomplete_rankings AS ({_sql_find_incomplete_rankings})\nSELECT ir.* FROM incomplete_rankings ir\n    LEFT JOIN message_reaction mr ON ir.parent_id = mr.message_id AND mr.payload_type = 'RankingReactionPayload'\nGROUP BY ir.parent_id, ir.role, ir.children_count, ir.child_min_ranking_count, ir.completed_rankings,\n    ir.message_tree_id\nHAVING COUNT(mr.message_id) FILTER (WHERE mr.user_id = :dupe_user_id) = 0\n\"\"\"\n\n    def query_incomplete_rankings(self, lang: str, user_filter: bool = True) -> list[IncompleteRankingsRow]:\n        \"\"\"Query parents which have children that need further rankings\"\"\"\n\n        dupe_user_id = None\n        skip_user_id = None\n        rank_user_id = None\n        if user_filter:\n            if not settings.DEBUG_ALLOW_DUPLICATE_TASKS:\n                dupe_user_id = self.pr.user_id\n            if not settings.DEBUG_ALLOW_SELF_RANKING:\n                rank_user_id = self.pr.user_id\n            skip_user_id = self.pr.user_id\n        r = self.db.execute(\n            text(self._sql_find_incomplete_rankings_ex),\n            {\n                \"num_required_rankings\": self.cfg.num_required_rankings,\n                \"lang\": lang,\n                \"dupe_user_id\": dupe_user_id,\n                \"skip_user_id\": skip_user_id,\n                \"rank_user_id\": rank_user_id,\n                \"ranking_state\": message_tree_state.State.RANKING,\n                \"skip_ranking\": protocol_schema.EmojiCode.skip_ranking,\n            },\n        )\n        return [IncompleteRankingsRow.from_orm(x) for x in r.all()]\n\n    _sql_find_extendible_parents = \"\"\"\n-- find all extendible parent nodes\nWITH recent_reply_tasks (parent_message_id) AS (\n    -- recent incomplete tasks to exclude\n    SELECT parent_message_id FROM task\n    WHERE not done\n        AND not skipped\n        AND created_date > (CURRENT_TIMESTAMP - :recent_tasks_interval)\n        AND (payload_type = 'AssistantReplyPayload' OR payload_type = 'PrompterReplyPayload')\n)\nSELECT m.id as parent_id, m.role as parent_role, m.depth, m.message_tree_id, COUNT(c.id) active_children_count\nFROM message_tree_state mts\n    INNER JOIN message m ON mts.message_tree_id = m.message_tree_id     -- all elements of message tree\n    LEFT JOIN message_emoji me ON\n        (m.id = me.message_id\n        AND :skip_user_id IS NOT NULL\n        AND me.user_id = :skip_user_id\n        AND me.emoji = :skip_reply)\n    LEFT JOIN recent_reply_tasks rrt ON m.id = rrt.parent_message_id    -- recent tasks\n    LEFT JOIN message c ON m.id = c.parent_id  -- child nodes\nWHERE mts.active                        -- only consider active trees\n    AND mts.state = :growing_state      -- message tree must be growing\n    AND NOT m.deleted                   -- ignore deleted messages as parents\n    AND m.depth < mts.max_depth         -- ignore leaf nodes as parents\n    AND m.review_result                 -- parent node must have positive review\n    AND m.lang = :lang                  -- parent matches lang\n    AND me.message_id IS NULL           -- no skip reply emoji for user\n    AND rrt.parent_message_id IS NULL   -- no recent reply task found\n    AND NOT coalesce(c.deleted, FALSE)  -- don't count deleted children\n    AND (c.review_result OR coalesce(c.review_count, 0) < :num_reviews_reply) -- don't count children with negative review but count elements under review\nGROUP BY m.id, m.role, m.depth, m.message_tree_id, mts.max_children_count\nHAVING COUNT(c.id) < mts.max_children_count -- below maximum number of children\n    AND (COUNT(c.id) < :num_prompter_replies OR m.role = 'prompter')   -- limit replies to assistant messages\n    AND COUNT(c.id) FILTER (WHERE c.user_id = :user_id) = 0  -- without reply by user\n\"\"\"\n\n    def query_extendible_parents(self, lang: str) -> tuple[list[ExtendibleParentRow], list[ActiveTreeSizeRow]]:\n        \"\"\"Query parent messages that have not reached the maximum number of replies.\"\"\"\n\n        user_id = self.pr.user_id if not settings.DEBUG_ALLOW_DUPLICATE_TASKS else None\n        r = self.db.execute(\n            text(self._sql_find_extendible_parents),\n            {\n                \"growing_state\": message_tree_state.State.GROWING,\n                \"num_reviews_reply\": self.cfg.num_reviews_reply,\n                \"num_prompter_replies\": self.cfg.num_prompter_replies,\n                \"lang\": lang,\n                \"user_id\": user_id,\n                \"skip_user_id\": self.pr.user_id,\n                \"skip_reply\": protocol_schema.EmojiCode.skip_reply,\n                \"recent_tasks_interval\": timedelta(seconds=self.cfg.recent_tasks_span_sec),\n            },\n        )\n\n        potential_parents = [ExtendibleParentRow.from_orm(x) for x in r.all()]\n        extendible_trees = self.query_extendible_trees(lang=lang)\n        extendible_tree_ids = set(t.message_tree_id for t in extendible_trees)\n        extendible_parents = list(p for p in potential_parents if p.message_tree_id in extendible_tree_ids)\n\n        return extendible_parents, extendible_trees\n\n    _sql_find_extendible_trees = f\"\"\"\n-- find extendible trees\nSELECT m.message_tree_id, mts.goal_tree_size, COUNT(m.id) AS tree_size\nFROM (\n        SELECT DISTINCT message_tree_id FROM ({_sql_find_extendible_parents}) extendible_parents\n    ) trees INNER JOIN message_tree_state mts ON trees.message_tree_id = mts.message_tree_id\n    INNER JOIN message m ON mts.message_tree_id = m.message_tree_id\nWHERE NOT m.deleted\n    AND (\n        m.parent_id IS NOT NULL AND (m.review_result OR m.review_count < :num_reviews_reply) -- children\n        OR m.parent_id IS NULL AND m.review_result -- prompts (root nodes) must have positive review\n    )\nGROUP BY m.message_tree_id, mts.goal_tree_size\nHAVING COUNT(m.id) < mts.goal_tree_size\n\"\"\"\n\n    def query_extendible_trees(self, lang: str) -> list[ActiveTreeSizeRow]:\n        \"\"\"Query size of active message trees in growing state.\"\"\"\n\n        user_id = self.pr.user_id if not settings.DEBUG_ALLOW_DUPLICATE_TASKS else None\n        r = self.db.execute(\n            text(self._sql_find_extendible_trees),\n            {\n                \"growing_state\": message_tree_state.State.GROWING,\n                \"num_reviews_reply\": self.cfg.num_reviews_reply,\n                \"num_prompter_replies\": self.cfg.num_prompter_replies,\n                \"lang\": lang,\n                \"user_id\": user_id,\n                \"skip_user_id\": self.pr.user_id,\n                \"skip_reply\": protocol_schema.EmojiCode.skip_reply,\n                \"recent_tasks_interval\": timedelta(seconds=self.cfg.recent_tasks_span_sec),\n            },\n        )\n        return [ActiveTreeSizeRow.from_orm(x) for x in r.all()]\n\n    def query_tree_size(self, message_tree_id: UUID) -> ActiveTreeSizeRow:\n        \"\"\"Returns the number of reviewed not deleted messages in the message tree.\"\"\"\n\n        required_reviews = settings.tree_manager.num_reviews_reply\n        qry = (\n            self.db.query(\n                MessageTreeState.message_tree_id.label(\"message_tree_id\"),\n                MessageTreeState.goal_tree_size.label(\"goal_tree_size\"),\n                func.count(Message.id).filter(Message.review_result).label(\"tree_size\"),\n                func.count(Message.id)\n                .filter(\n                    or_(Message.review_result.is_(None), not_(Message.review_result)),\n                    Message.review_count < required_reviews,\n                )\n                .label(\"awaiting_review\"),\n            )\n            .select_from(MessageTreeState)\n            .outerjoin(\n                Message, and_(MessageTreeState.message_tree_id == Message.message_tree_id, not_(Message.deleted))\n            )\n            .filter(\n                MessageTreeState.active,\n                MessageTreeState.message_tree_id == message_tree_id,\n            )\n            .group_by(MessageTreeState.message_tree_id, MessageTreeState.goal_tree_size)\n        )\n\n        return ActiveTreeSizeRow.from_orm(qry.one())\n\n    def query_misssing_tree_states(self) -> list[Tuple[UUID, str]]:\n        \"\"\"Find all initial prompt messages that have no associated message tree state\"\"\"\n        qry_missing_tree_states = (\n            self.db.query(Message.id, Message.lang)\n            .outerjoin(MessageTreeState, Message.message_tree_id == MessageTreeState.message_tree_id)\n            .filter(\n                Message.parent_id.is_(None),\n                Message.message_tree_id == Message.id,\n                MessageTreeState.message_tree_id.is_(None),\n            )\n        )\n\n        return [(m.id, m.lang) for m in qry_missing_tree_states.all()]\n\n    _sql_find_tree_ranking_results = \"\"\"\n-- get all ranking results of completed tasks for all parents with >= 2 children\nSELECT p.parent_id, mr.* FROM\n(\n    -- find parents with > 1 children\n    SELECT m.parent_id, m.message_tree_id, COUNT(m.id) children_count\n    FROM message_tree_state mts\n       INNER JOIN message m ON mts.message_tree_id = m.message_tree_id\n    WHERE m.review_result                  -- must be reviewed\n       AND NOT m.deleted                   -- not deleted\n       AND m.parent_id IS NOT NULL         -- ignore initial prompts\n       AND (:role IS NULL OR m.role = :role) -- children with matching role\n       AND mts.message_tree_id = :message_tree_id\n    GROUP BY m.parent_id, m.message_tree_id\n    HAVING COUNT(m.id) > 1\n) as p\nLEFT JOIN task t ON p.parent_id = t.parent_message_id AND t.done AND (t.payload_type = 'RankPrompterRepliesPayload' OR t.payload_type = 'RankAssistantRepliesPayload')\nLEFT JOIN message_reaction mr ON mr.task_id = t.id AND mr.payload_type = 'RankingReactionPayload'\n\"\"\"\n\n    def query_tree_ranking_results(\n        self,\n        message_tree_id: UUID,\n        role_filter: str = \"assistant\",\n    ) -> dict[UUID, list[MessageReaction]]:\n        \"\"\"Finds all completed ranking results for a message_tree\"\"\"\n\n        assert role_filter in (None, \"assistant\", \"prompter\")\n\n        r = self.db.execute(\n            text(self._sql_find_tree_ranking_results),\n            {\n                \"message_tree_id\": message_tree_id,\n                \"role\": role_filter,\n            },\n        )\n\n        rankings_by_message = {}\n        for x in r.all():\n            parent_id = x[\"parent_id\"]\n            if parent_id not in rankings_by_message:\n                rankings_by_message[parent_id] = []\n            if x[\"task_id\"]:\n                rankings_by_message[parent_id].append(MessageReaction.from_orm(x))\n        return rankings_by_message\n\n    @managed_tx_method(CommitMode.COMMIT)\n    def ensure_tree_states(self) -> None:\n        \"\"\"Add message tree state rows for all root nodes (initial prompt messages).\"\"\"\n\n        missing_tree_ids = self.query_misssing_tree_states()\n        for id, lang in missing_tree_ids:\n            tree_size = self.db.query(func.count(Message.id)).filter(Message.message_tree_id == id).scalar()\n            state = message_tree_state.State.INITIAL_PROMPT_REVIEW\n            if tree_size > 1:\n                state = message_tree_state.State.GROWING\n                logger.info(f\"Inserting missing message tree state for message: {id} ({tree_size=}, {state=:s})\")\n            self._insert_default_state(id, lang=lang, state=state)\n\n        halt_prompts_of_disabled_users(self.db)\n\n        # check tree state transitions (maybe variables haves changes): prompt review -> growing -> ranking -> scoring\n        prompt_review_trees: list[MessageTreeState] = (\n            self.db.query(MessageTreeState)\n            .filter(MessageTreeState.state == message_tree_state.State.INITIAL_PROMPT_REVIEW, MessageTreeState.active)\n            .all()\n        )\n        if len(prompt_review_trees) > 0:\n            logger.info(\n                f\"Checking state of {len(prompt_review_trees)} active message trees in 'initial_prompt_review' state.\"\n            )\n            for t in prompt_review_trees:\n                self.check_condition_for_prompt_lottery(t.message_tree_id)\n\n        growing_trees: list[MessageTreeState] = (\n            self.db.query(MessageTreeState)\n            .filter(MessageTreeState.state == message_tree_state.State.GROWING, MessageTreeState.active)\n            .all()\n        )\n        if len(growing_trees) > 0:\n            logger.info(f\"Checking state of {len(growing_trees)} active message trees in 'growing' state.\")\n            for t in growing_trees:\n                self.check_condition_for_ranking_state(t.message_tree_id)\n\n        ranking_trees: list[MessageTreeState] = (\n            self.db.query(MessageTreeState)\n            .filter(\n                or_(\n                    MessageTreeState.state == message_tree_state.State.RANKING,\n                    MessageTreeState.state == message_tree_state.State.READY_FOR_SCORING,\n                ),\n                MessageTreeState.active,\n            )\n            .all()\n        )\n        if len(ranking_trees) > 0:\n            logger.info(f\"Checking state of {len(ranking_trees)} active message trees in 'ranking' state.\")\n            for t in ranking_trees:\n                self.check_condition_for_scoring_state(t.message_tree_id)\n\n    def query_num_growing_trees(self, lang: str) -> int:\n        \"\"\"Count all active trees in growing state.\"\"\"\n        query = (\n            self.db.query(func.count(MessageTreeState.message_tree_id))\n            .join(Message, MessageTreeState.message_tree_id == Message.id)\n            .filter(\n                MessageTreeState.active,\n                MessageTreeState.state == message_tree_state.State.GROWING,\n                Message.lang == lang,\n            )\n        )\n        return query.scalar()\n\n    def query_prompt_lottery_waiting(self, lang: str) -> int:\n        query = self.db.query(func.count(MessageTreeState.message_tree_id)).filter(\n            MessageTreeState.state == message_tree_state.State.PROMPT_LOTTERY_WAITING, MessageTreeState.lang == lang\n        )\n        return query.scalar()\n\n    def query_num_active_trees(\n        self, lang: str, exclude_ranking: bool = True, exclude_prompt_review: bool = True\n    ) -> int:\n        \"\"\"Count all active trees (optionally exclude those in ranking and initial prompt review states).\"\"\"\n        query = (\n            self.db.query(func.count(MessageTreeState.message_tree_id))\n            .join(Message, MessageTreeState.message_tree_id == Message.id)\n            .filter(MessageTreeState.active, Message.lang == lang)\n        )\n        if exclude_ranking:\n            query = query.filter(MessageTreeState.state != message_tree_state.State.RANKING)\n        if exclude_prompt_review:\n            query = query.filter(MessageTreeState.state != message_tree_state.State.INITIAL_PROMPT_REVIEW)\n        return query.scalar()\n\n    def query_reviews_for_message(self, message_id: UUID) -> list[TextLabels]:\n        qry = (\n            self.db.query(TextLabels)\n            .select_from(Task)\n            .join(TextLabels, Task.id == TextLabels.id)\n            .filter(Task.done, TextLabels.message_id == message_id)\n        )\n        return qry.all()\n\n    def query_moderation_bad_messages(self, lang: str) -> list[Message]:\n        qry = (\n            self.db.query(Message)\n            .select_from(MessageTreeState)\n            .join(Message, MessageTreeState.message_tree_id == Message.message_tree_id)\n            .filter(\n                MessageTreeState.active,\n                or_(\n                    MessageTreeState.state == message_tree_state.State.INITIAL_PROMPT_REVIEW,\n                    MessageTreeState.state == message_tree_state.State.GROWING,\n                ),\n                or_(\n                    Message.parent_id.is_(None),\n                    Message.review_result,\n                    and_(Message.parent_id.is_not(None), Message.review_count < self.cfg.num_reviews_reply),\n                ),\n                not_(Message.deleted),\n                or_(\n                    coalesce(Message.emojis[protocol_schema.EmojiCode.red_flag].cast(sa.Integer), 0)\n                    >= self.cfg.auto_mod_red_flags,\n                    coalesce(Message.emojis[protocol_schema.EmojiCode.skip_reply].cast(sa.Integer), 0)\n                    >= self.cfg.auto_mod_max_skip_reply,\n                ),\n            )\n        )\n\n        if lang is not None:\n            qry = qry.filter(Message.lang == lang)\n\n        return qry.all()\n\n    @managed_tx_method(CommitMode.FLUSH)\n    def _insert_tree_state(\n        self,\n        root_message_id: UUID,\n        goal_tree_size: int,\n        max_depth: int,\n        max_children_count: int,\n        active: bool,\n        lang: str,\n        state: message_tree_state.State = message_tree_state.State.INITIAL_PROMPT_REVIEW,\n    ) -> MessageTreeState:\n        model = MessageTreeState(\n            message_tree_id=root_message_id,\n            goal_tree_size=goal_tree_size,\n            max_depth=max_depth,\n            max_children_count=max_children_count,\n            state=state.value,\n            active=active,\n            lang=lang,\n        )\n\n        self.db.add(model)\n        return model\n\n    @managed_tx_method(CommitMode.FLUSH)\n    def _insert_default_state(\n        self,\n        root_message_id: UUID,\n        lang: str,\n        state: message_tree_state.State = message_tree_state.State.INITIAL_PROMPT_REVIEW,\n        *,\n        goal_tree_size: int = None,\n    ) -> MessageTreeState:\n        if goal_tree_size is None:\n            if self.cfg.random_goal_tree_size and self.cfg.min_goal_tree_size < self.cfg.goal_tree_size:\n                goal_tree_size = random.randint(self.cfg.min_goal_tree_size, self.cfg.goal_tree_size)\n            else:\n                goal_tree_size = self.cfg.goal_tree_size\n        return self._insert_tree_state(\n            root_message_id=root_message_id,\n            goal_tree_size=goal_tree_size,\n            max_depth=self.cfg.max_tree_depth,\n            max_children_count=self.cfg.max_children_count,\n            active=True,\n            lang=lang,\n            state=state,\n        )\n\n    def tree_counts_by_state(self, lang: str = None, only_active: bool = False) -> dict[str, int]:\n        qry = self.db.query(MessageTreeState.state, func.count(MessageTreeState.message_tree_id).label(\"count\"))\n\n        if lang is not None:\n            qry = (\n                qry.select_from(MessageTreeState)\n                .join(Message, MessageTreeState.message_tree_id == Message.id)\n                .filter(Message.lang == lang)\n            )\n        if only_active:\n            qry = qry.filter(MessageTreeState.active)\n\n        qry = qry.group_by(MessageTreeState.state)\n        return {x[\"state\"]: x[\"count\"] for x in qry}\n\n    def tree_counts_by_state_stats(self, lang: str = None, only_active: bool = False) -> TreeStateStats:\n        count_by_state = self.tree_counts_by_state(lang=lang, only_active=only_active)\n        r = TreeStateStats(\n            initial_prompt_review=count_by_state.get(message_tree_state.State.INITIAL_PROMPT_REVIEW) or 0,\n            growing=count_by_state.get(message_tree_state.State.GROWING) or 0,\n            ranking=count_by_state.get(message_tree_state.State.RANKING) or 0,\n            ready_for_scoring=count_by_state.get(message_tree_state.State.READY_FOR_SCORING) or 0,\n            ready_for_export=count_by_state.get(message_tree_state.State.READY_FOR_EXPORT) or 0,\n            scoring_failed=count_by_state.get(message_tree_state.State.SCORING_FAILED) or 0,\n            halted_by_moderator=count_by_state.get(message_tree_state.State.HALTED_BY_MODERATOR) or 0,\n            backlog_ranking=count_by_state.get(message_tree_state.State.BACKLOG_RANKING) or 0,\n            prompt_lottery_waiting=count_by_state.get(message_tree_state.State.PROMPT_LOTTERY_WAITING) or 0,\n            aborted_low_grade=count_by_state.get(message_tree_state.State.ABORTED_LOW_GRADE) or 0,\n        )\n        return r\n\n    def tree_message_count_stats(self, only_active: bool = True) -> list[TreeMessageCountStats]:\n        qry = (\n            self.db.query(\n                MessageTreeState.message_tree_id,\n                func.max(Message.depth).label(\"depth\"),\n                func.min(Message.created_date).label(\"oldest\"),\n                func.max(Message.created_date).label(\"youngest\"),\n                func.count(Message.id).label(\"count\"),\n                MessageTreeState.goal_tree_size,\n                MessageTreeState.state,\n            )\n            .select_from(MessageTreeState)\n            .join(Message, MessageTreeState.message_tree_id == Message.message_tree_id)\n            .filter(not_(Message.deleted))\n            .group_by(MessageTreeState.message_tree_id)\n        )\n\n        if only_active:\n            qry = qry.filter(MessageTreeState.active)\n\n        return [TreeMessageCountStats(**x) for x in qry]\n\n    def stats(self) -> TreeManagerStats:\n        return TreeManagerStats(\n            state_counts=self.tree_counts_by_state(),\n            message_counts=self.tree_message_count_stats(only_active=True),\n        )\n\n    def get_user_messages_by_tree(\n        self,\n        user_id: UUID,\n        min_date: datetime = None,\n        max_date: datetime = None,\n    ) -> Tuple[dict[UUID, list[Message]], list[Message]]:\n        \"\"\"Returns a dict with replies by tree (excluding initial prompts) and list of initial prompts\n        associated with user_id.\"\"\"\n\n        # query all messages of the user\n        qry = self.db.query(Message).filter(Message.user_id == user_id)\n        if min_date:\n            qry = qry.filter(Message.created_date >= min_date)\n        if max_date:\n            qry = qry.filter(Message.created_date <= max_date)\n\n        prompts: list[Message] = []\n        replies_by_tree: dict[UUID, list[Message]] = {}\n\n        # walk over result set and distinguish between initial prompts and replies\n        for m in qry:\n            m: Message\n\n            if m.message_tree_id == m.id:\n                prompts.append(m)\n            else:\n                message_list = replies_by_tree.get(m.message_tree_id)\n                if message_list is None:\n                    message_list = [m]\n                    replies_by_tree[m.message_tree_id] = message_list\n                else:\n                    message_list.append(m)\n\n        return replies_by_tree, prompts\n\n    def _purge_message_internal(self, message_id: UUID) -> None:\n        \"\"\"This internal function deletes a single message. It does not take care of\n        descendants, children_count in parent etc.\"\"\"\n\n        sql_purge_message = \"\"\"\nDELETE FROM journal j USING message m WHERE j.message_id = :message_id;\nDELETE FROM message_embedding e WHERE e.message_id = :message_id;\nDELETE FROM message_toxicity t WHERE t.message_id = :message_id;\nDELETE FROM text_labels l WHERE l.message_id = :message_id;\n-- delete all ranking results that contain message\nDELETE FROM message_reaction r WHERE r.payload_type = 'RankingReactionPayload' AND r.task_id IN (\n        SELECT t.id FROM message m\n            JOIN task t ON m.parent_id = t.parent_message_id\n        WHERE m.id = :message_id);\n-- delete task which inserted message\nDELETE FROM task t using message m WHERE t.id = m.task_id AND m.id = :message_id;\nDELETE FROM task t WHERE t.parent_message_id = :message_id;\nDELETE FROM message WHERE id = :message_id;\n\"\"\"\n        parent_id = self.pr.fetch_message(message_id=message_id).parent_id\n        r = self.db.execute(text(sql_purge_message), {\"message_id\": message_id})\n        logger.debug(f\"purge_message({message_id=}): {r.rowcount} rows.\")\n\n        sql_update_ranking_counts = \"\"\"\nWITH r AS (\n    -- find ranking results and count per child\n    SELECT c.id,\n        count(*) FILTER (\n            WHERE mr.payload#>'{payload, ranked_message_ids}' ? CAST(c.id AS varchar)\n        ) AS ranking_count\n    FROM message c\n    LEFT JOIN message_reaction mr ON mr.payload_type = 'RankingReactionPayload'\n        AND mr.message_id = c.parent_id\n    WHERE c.parent_id = :parent_id\n    GROUP BY c.id\n)\nUPDATE message m SET ranking_count = r.ranking_count\nFROM r WHERE m.id = r.id AND m.ranking_count != r.ranking_count;\n\"\"\"\n\n        if parent_id is not None:\n            # update ranking counts of remaining children\n            r = self.db.execute(text(sql_update_ranking_counts), {\"parent_id\": parent_id})\n            logger.debug(f\"ranking_count updated for {r.rowcount} rows.\")\n\n    def purge_message_tree(self, message_tree_id: UUID) -> None:\n        sql_purge_message_tree = \"\"\"\nDELETE FROM journal j USING message m WHERE j.message_id = m.Id AND m.message_tree_id = :message_tree_id;\nDELETE FROM message_embedding e USING message m WHERE e.message_id = m.Id AND m.message_tree_id = :message_tree_id;\nDELETE FROM message_toxicity t USING message m WHERE t.message_id = m.Id AND m.message_tree_id = :message_tree_id;\nDELETE FROM text_labels l USING message m WHERE l.message_id = m.Id AND m.message_tree_id = :message_tree_id;\nDELETE FROM message_reaction r USING task t WHERE r.task_id = t.id AND t.message_tree_id = :message_tree_id;\nDELETE FROM task t WHERE t.message_tree_id = :message_tree_id;\nDELETE FROM message_tree_state WHERE message_tree_id = :message_tree_id;\nDELETE FROM message WHERE message_tree_id = :message_tree_id;\n\"\"\"\n        r = self.db.execute(text(sql_purge_message_tree), {\"message_tree_id\": message_tree_id})\n        logger.debug(f\"purge_message_tree({message_tree_id=}) {r.rowcount} rows.\")\n\n    def _reactivate_tree(self, mts: MessageTreeState):\n        if mts.state == message_tree_state.State.PROMPT_LOTTERY_WAITING:\n            return\n\n        tree_id = mts.message_tree_id\n        if mts.won_prompt_lottery_date is not None:\n            self._enter_state(mts, message_tree_state.State.GROWING)\n            if self.check_condition_for_ranking_state(tree_id):\n                self.check_condition_for_scoring_state(tree_id)\n        else:\n            self._enter_state(mts, message_tree_state.State.INITIAL_PROMPT_REVIEW)\n            self.check_condition_for_prompt_lottery(tree_id)\n\n    @managed_tx_method(CommitMode.FLUSH)\n    def purge_user_messages(\n        self,\n        user_id: UUID,\n        purge_initial_prompts: bool = True,\n        min_date: datetime = None,\n        max_date: datetime = None,\n    ):\n        # find all affected message trees\n        replies_by_tree, prompts = self.get_user_messages_by_tree(user_id, min_date, max_date)\n        total_messages = sum(len(x) for x in replies_by_tree.values())\n        logger.debug(f\"found: {len(replies_by_tree)} trees; {len(prompts)} prompts; {total_messages} messages;\")\n\n        # remove all trees based on initial prompts of the user\n        if purge_initial_prompts:\n            for p in prompts:\n                self.purge_message_tree(p.message_tree_id)\n                if p.message_tree_id in replies_by_tree:\n                    del replies_by_tree[p.message_tree_id]\n\n        # patch all affected message trees\n        for tree_id, replies in replies_by_tree.items():\n            bad_parent_ids = set(m.id for m in replies)\n            logger.debug(f\"patching tree {tree_id=}, {bad_parent_ids=}\")\n\n            tree_messages = self.pr.fetch_message_tree(tree_id, reviewed=False, include_deleted=True)\n            logger.debug(f\"{tree_id=}, {len(bad_parent_ids)=}, {len(tree_messages)=}\")\n            by_id = {m.id: m for m in tree_messages}\n\n            def ancestor_ids(msg: Message) -> list[UUID]:\n                t = []\n                while msg.parent_id is not None:\n                    msg = by_id[msg.parent_id]\n                    t.append(msg.id)\n                return t\n\n            def is_descendant_of_deleted(m: Message) -> bool:\n                if m.id in bad_parent_ids:\n                    return True\n                ancestors = ancestor_ids(m)\n                if any(a in bad_parent_ids for a in ancestors):\n                    return True\n                return False\n\n            # start with deepest messages first\n            tree_messages.sort(key=lambda x: x.depth, reverse=True)\n            for m in tree_messages:\n                if is_descendant_of_deleted(m):\n                    logger.debug(f\"purging message: {m.id}\")\n                    self._purge_message_internal(m.id)\n\n            # update children counts\n            self.pr.update_children_counts(m.message_tree_id)\n\n            # reactivate tree\n            logger.info(f\"reactivating message tree {tree_id}\")\n            mts = self.pr.fetch_tree_state(tree_id)\n            mts.active = True\n            self._reactivate_tree(mts)\n\n    @managed_tx_method(CommitMode.FLUSH)\n    def purge_user(self, user_id: UUID, ban: bool = True) -> None:\n        self.purge_user_messages(user_id, purge_initial_prompts=True)\n\n        # delete all remaining rows and ban user\n        sql_purge_user = \"\"\"\nDELETE FROM journal WHERE user_id = :user_id;\nDELETE FROM message_reaction WHERE user_id = :user_id;\nDELETE FROM message_emoji WHERE user_id = :user_id;\nDELETE FROM task WHERE user_id = :user_id;\nDELETE FROM message WHERE user_id = :user_id;\nDELETE FROM user_stats WHERE user_id = :user_id;\n\"\"\"\n\n        r = self.db.execute(text(sql_purge_user), {\"user_id\": user_id})\n        logger.debug(f\"purge_user({user_id=}): {r.rowcount} rows.\")\n\n        if ban:\n            self.db.execute(update(User).filter(User.id == user_id).values(deleted=True, enabled=False))\n\n    @managed_tx_method(CommitMode.COMMIT)\n    def retry_scoring_failed_message_trees(self):\n        query = self.db.query(MessageTreeState).filter(\n            MessageTreeState.state == message_tree_state.State.SCORING_FAILED\n        )\n        for mts in query.all():\n            mts: MessageTreeState\n            try:\n                if not self.check_condition_for_scoring_state(mts.message_tree_id):\n                    mts.active = True\n                    self._enter_state(mts, message_tree_state.State.RANKING)\n            except Exception:\n                logger.exception(f\"retry_scoring_failed_message_trees failed for ({mts.message_tree_id=})\")\n\n    @managed_tx_method(CommitMode.FLUSH)\n    def halt_tree(self, message_id: UUID, halt: bool = True) -> MessageTreeState:\n        message = self.pr.fetch_message(message_id, fail_if_missing=True)\n        mts = self.pr.fetch_tree_state(message.message_tree_id)\n\n        if halt:\n            self._enter_state(mts, message_tree_state.State.HALTED_BY_MODERATOR)\n        else:\n            self._reactivate_tree(mts)\n\n        return mts\n\n\nif __name__ == \"__main__\":\n    from oasst_backend.api.deps import api_auth\n\n    # from oasst_backend.api.deps import create_api_client\n    from oasst_backend.database import engine\n    from oasst_backend.prompt_repository import PromptRepository\n\n    with Session(engine) as db:\n        api_client = api_auth(settings.OFFICIAL_WEB_API_KEY, db=db)\n        # api_client = create_api_client(session=db, description=\"test\", frontend_type=\"bot\")\n        # dummy_user = protocol_schema.User(id=\"__dummy_user__\", display_name=\"Dummy User\", auth_method=\"local\")\n        dummy_user = protocol_schema.User(id=\"1234\", display_name=\"bulb\", auth_method=\"local\")\n\n        pr = PromptRepository(db=db, api_client=api_client, client_user=dummy_user)\n        cfg = TreeManagerConfiguration()\n        tm = TreeManager(db, pr, cfg)\n        tm.ensure_tree_states()\n\n        # tm.purge_user_messages(user_id=UUID(\"2ef9ad21-0dc5-442d-8750-6f7f1790723f\"), purge_initial_prompts=False)\n        # tm.purge_user(user_id=UUID(\"2ef9ad21-0dc5-442d-8750-6f7f1790723f\"))\n        # db.commit()\n\n        # print(\"query_num_active_trees\", tm.query_num_active_trees())\n        # print(\"query_incomplete_rankings\", tm.query_incomplete_rankings())\n        # print(\"query_replies_need_review\", tm.query_replies_need_review())\n        # print(\"query_incomplete_reply_reviews\", tm.query_replies_need_review())\n        # xs = tm.query_prompts_need_review(lang=\"en\")\n        # print(\"xs\", len(xs))\n        # for x in xs:\n        #    print(x.id, x.emojis)\n        # print(\"query_incomplete_initial_prompt_reviews\", tm.query_prompts_need_review(lang=\"en\"))\n        # print(\"query_extendible_trees\", tm.query_extendible_trees())\n        # print(\"query_extendible_parents\", tm.query_extendible_parents())\n\n        # print(\"next_task:\", tm.next_task())\n\n        # print(\n        #     \".query_tree_ranking_results\", tm.query_tree_ranking_results(UUID(\"21f9d585-d22c-44ab-a696-baa3d83b5f1b\"))\n        # )\n", "backend/oasst_backend/models/db_payload.py": "from typing import Literal, Optional\nfrom uuid import UUID\n\nfrom oasst_backend.models.payload_column_type import payload_type\nfrom oasst_shared.schemas import protocol as protocol_schema\nfrom pydantic import BaseModel, Field\n\n\n@payload_type\nclass TaskPayload(BaseModel):\n    type: str\n\n\n@payload_type\nclass SummarizationStoryPayload(TaskPayload):\n    type: Literal[\"summarize_story\"] = \"summarize_story\"\n    story: str\n\n\n@payload_type\nclass RateSummaryPayload(TaskPayload):\n    type: Literal[\"rate_summary\"] = \"rate_summary\"\n    full_text: str\n    summary: str\n    scale: protocol_schema.RatingScale\n\n\n@payload_type\nclass InitialPromptPayload(TaskPayload):\n    type: Literal[\"initial_prompt\"] = \"initial_prompt\"\n    hint: str | None\n\n\n@payload_type\nclass PrompterReplyPayload(TaskPayload):\n    type: Literal[\"prompter_reply\"] = \"prompter_reply\"\n    conversation: protocol_schema.Conversation\n    hint: str | None\n\n\n@payload_type\nclass AssistantReplyPayload(TaskPayload):\n    type: Literal[\"assistant_reply\"] = \"assistant_reply\"\n    conversation: protocol_schema.Conversation\n\n\n@payload_type\nclass MessagePayload(BaseModel):\n    text: str\n\n\n@payload_type\nclass ReactionPayload(BaseModel):\n    type: str\n\n\n@payload_type\nclass RatingReactionPayload(ReactionPayload):\n    type: Literal[\"message_rating\"] = \"message_rating\"\n    rating: str\n\n\n@payload_type\nclass RankingReactionPayload(ReactionPayload):\n    type: Literal[\"message_ranking\"] = \"message_ranking\"\n    ranking: list[int]\n    ranked_message_ids: list[UUID]\n    ranking_parent_id: Optional[UUID]\n    message_tree_id: Optional[UUID]\n    not_rankable: Optional[bool]  # all options flawed, factually incorrect or unacceptable\n\n\n@payload_type\nclass RankConversationRepliesPayload(TaskPayload):\n    conversation: protocol_schema.Conversation  # the conversation so far\n    reply_messages: list[protocol_schema.ConversationMessage]\n    ranking_parent_id: Optional[UUID]\n    message_tree_id: Optional[UUID]\n    reveal_synthetic: Optional[bool]\n\n\n@payload_type\nclass RankInitialPromptsPayload(TaskPayload):\n    \"\"\"A task to rank a set of initial prompts.\"\"\"\n\n    type: Literal[\"rank_initial_prompts\"] = \"rank_initial_prompts\"\n    prompt_messages: list[protocol_schema.ConversationMessage]\n\n\n@payload_type\nclass RankPrompterRepliesPayload(RankConversationRepliesPayload):\n    \"\"\"A task to rank a set of prompter replies to a conversation.\"\"\"\n\n    type: Literal[\"rank_prompter_replies\"] = \"rank_prompter_replies\"\n\n\n@payload_type\nclass RankAssistantRepliesPayload(RankConversationRepliesPayload):\n    \"\"\"A task to rank a set of assistant replies to a conversation.\"\"\"\n\n    type: Literal[\"rank_assistant_replies\"] = \"rank_assistant_replies\"\n\n\n@payload_type\nclass LabelInitialPromptPayload(TaskPayload):\n    \"\"\"A task to label an initial prompt.\"\"\"\n\n    type: Literal[\"label_initial_prompt\"] = \"label_initial_prompt\"\n    message_id: UUID\n    prompt: str\n    valid_labels: list[str]\n    mandatory_labels: Optional[list[str]]\n    mode: Optional[protocol_schema.LabelTaskMode]\n\n\n@payload_type\nclass LabelConversationReplyPayload(TaskPayload):\n    \"\"\"A task to label a conversation reply.\"\"\"\n\n    message_id: UUID\n    conversation: protocol_schema.Conversation\n    reply: Optional[str] = Field(None, deprecated=True, description=\"deprecated\")\n    reply_message: Optional[protocol_schema.ConversationMessage] = Field(\n        None, deprecated=True, description=\"deprecated\"\n    )\n    valid_labels: list[str]\n    mandatory_labels: Optional[list[str]]\n    mode: Optional[protocol_schema.LabelTaskMode]\n\n\n@payload_type\nclass LabelPrompterReplyPayload(LabelConversationReplyPayload):\n    \"\"\"A task to label a prompter reply.\"\"\"\n\n    type: Literal[\"label_prompter_reply\"] = \"label_prompter_reply\"\n\n\n@payload_type\nclass LabelAssistantReplyPayload(LabelConversationReplyPayload):\n    \"\"\"A task to label an assistant reply.\"\"\"\n\n    type: Literal[\"label_assistant_reply\"] = \"label_assistant_reply\"\n", "backend/oasst_backend/models/message_reaction.py": "from datetime import datetime\nfrom typing import Optional\nfrom uuid import UUID\n\nimport sqlalchemy as sa\nimport sqlalchemy.dialects.postgresql as pg\nfrom sqlmodel import Field, SQLModel\n\nfrom .payload_column_type import PayloadContainer, payload_column_type\n\n\nclass MessageReaction(SQLModel, table=True):\n    __tablename__ = \"message_reaction\"\n\n    task_id: Optional[UUID] = Field(\n        sa_column=sa.Column(pg.UUID(as_uuid=True), sa.ForeignKey(\"task.id\"), nullable=False, primary_key=True)\n    )\n    user_id: UUID = Field(\n        sa_column=sa.Column(pg.UUID(as_uuid=True), sa.ForeignKey(\"user.id\"), nullable=False, primary_key=True)\n    )\n    created_date: Optional[datetime] = Field(\n        sa_column=sa.Column(\n            sa.DateTime(timezone=True), nullable=False, server_default=sa.func.current_timestamp(), index=True\n        )\n    )\n    payload_type: str = Field(nullable=False, max_length=200)\n    payload: PayloadContainer = Field(sa_column=sa.Column(payload_column_type(PayloadContainer), nullable=False))\n    api_client_id: UUID = Field(nullable=False, foreign_key=\"api_client.id\")\n    message_id: Optional[UUID] = Field(nullable=True, index=True)\n", "backend/oasst_backend/models/message_emoji.py": "from datetime import datetime\nfrom typing import Optional\nfrom uuid import UUID\n\nimport sqlalchemy as sa\nimport sqlalchemy.dialects.postgresql as pg\nfrom sqlmodel import Field, Index, SQLModel\n\n\nclass MessageEmoji(SQLModel, table=True):\n    __tablename__ = \"message_emoji\"\n    __table_args__ = (Index(\"ix_message_emoji__user_id__message_id\", \"user_id\", \"message_id\", unique=False),)\n\n    message_id: Optional[UUID] = Field(\n        sa_column=sa.Column(\n            pg.UUID(as_uuid=True), sa.ForeignKey(\"message.id\", ondelete=\"CASCADE\"), nullable=False, primary_key=True\n        )\n    )\n    user_id: UUID = Field(\n        sa_column=sa.Column(\n            pg.UUID(as_uuid=True), sa.ForeignKey(\"user.id\", ondelete=\"CASCADE\"), nullable=False, primary_key=True\n        )\n    )\n    emoji: str = Field(nullable=False, max_length=128, primary_key=True)\n    created_date: Optional[datetime] = Field(\n        sa_column=sa.Column(sa.DateTime(timezone=True), nullable=False, server_default=sa.func.current_timestamp())\n    )\n", "backend/oasst_backend/models/task.py": "from datetime import datetime\nfrom typing import Optional\nfrom uuid import UUID, uuid4\n\nimport sqlalchemy as sa\nimport sqlalchemy.dialects.postgresql as pg\nfrom oasst_shared.utils import utcnow\nfrom sqlalchemy import false\nfrom sqlmodel import Field, SQLModel\n\nfrom .payload_column_type import PayloadContainer, payload_column_type\n\n\nclass Task(SQLModel, table=True):\n    __tablename__ = \"task\"\n\n    id: Optional[UUID] = Field(\n        sa_column=sa.Column(\n            pg.UUID(as_uuid=True), primary_key=True, default=uuid4, server_default=sa.text(\"gen_random_uuid()\")\n        ),\n    )\n    created_date: Optional[datetime] = Field(\n        sa_column=sa.Column(\n            sa.DateTime(timezone=True), nullable=False, index=True, server_default=sa.func.current_timestamp()\n        ),\n    )\n    expiry_date: Optional[datetime] = Field(sa_column=sa.Column(sa.DateTime(timezone=True), nullable=True))\n    user_id: Optional[UUID] = Field(nullable=True, foreign_key=\"user.id\", index=True)\n    payload_type: str = Field(nullable=False, max_length=200)\n    payload: PayloadContainer = Field(sa_column=sa.Column(payload_column_type(PayloadContainer), nullable=False))\n    api_client_id: UUID = Field(nullable=False, foreign_key=\"api_client.id\")\n    ack: Optional[bool] = None\n    done: bool = Field(sa_column=sa.Column(sa.Boolean, nullable=False, server_default=false()))\n    skipped: bool = Field(sa_column=sa.Column(sa.Boolean, nullable=False, server_default=false()))\n    skip_reason: Optional[str] = Field(nullable=True, max_length=512)\n    frontend_message_id: Optional[str] = None\n    message_tree_id: Optional[UUID] = None\n    parent_message_id: Optional[UUID] = None\n    collective: bool = Field(sa_column=sa.Column(sa.Boolean, nullable=False, server_default=false()))\n\n    @property\n    def expired(self) -> bool:\n        return self.expiry_date is not None and utcnow() > self.expiry_date\n", "backend/oasst_backend/models/message.py": "from datetime import datetime\nfrom http import HTTPStatus\nfrom typing import Any, Optional\nfrom uuid import UUID, uuid4\n\nimport sqlalchemy as sa\nimport sqlalchemy.dialects.postgresql as pg\nfrom oasst_backend.models.db_payload import MessagePayload\nfrom oasst_backend.models.user import User\nfrom oasst_shared.exceptions.oasst_api_error import OasstError, OasstErrorCode\nfrom pydantic import PrivateAttr\nfrom sqlalchemy import false\nfrom sqlmodel import Field, Index, SQLModel\n\nfrom .payload_column_type import PayloadContainer, payload_column_type\n\n\nclass Message(SQLModel, table=True):\n    __tablename__ = \"message\"\n    __table_args__ = (\n        Index(\"ix_message_frontend_message_id\", \"api_client_id\", \"frontend_message_id\", unique=True),\n        Index(\"idx_search_vector\", \"search_vector\", postgresql_using=\"gin\"),\n    )\n\n    def __new__(cls, *args: Any, **kwargs: Any):\n        new_object = super().__new__(cls, *args, **kwargs)\n        # temporary fix until https://github.com/tiangolo/sqlmodel/issues/149 gets merged\n        if not hasattr(new_object, \"_user_emojis\"):\n            new_object._init_private_attributes()\n        return new_object\n\n    id: Optional[UUID] = Field(\n        sa_column=sa.Column(\n            pg.UUID(as_uuid=True), primary_key=True, default=uuid4, server_default=sa.text(\"gen_random_uuid()\")\n        ),\n    )\n    parent_id: Optional[UUID] = Field(nullable=True)\n    message_tree_id: UUID = Field(nullable=False, index=True)\n    task_id: Optional[UUID] = Field(nullable=True, index=True)\n    user_id: Optional[UUID] = Field(nullable=True, foreign_key=\"user.id\", index=True)\n    role: str = Field(nullable=False, max_length=128, regex=\"^prompter|assistant$\")\n    api_client_id: UUID = Field(nullable=False, foreign_key=\"api_client.id\")\n    frontend_message_id: str = Field(max_length=200, nullable=False)\n    created_date: Optional[datetime] = Field(\n        sa_column=sa.Column(\n            sa.DateTime(timezone=True), nullable=False, server_default=sa.func.current_timestamp(), index=True\n        )\n    )\n    payload_type: str = Field(nullable=False, max_length=200)\n    payload: Optional[PayloadContainer] = Field(\n        sa_column=sa.Column(payload_column_type(PayloadContainer), nullable=True)\n    )\n    lang: str = Field(sa_column=sa.Column(sa.String(32), server_default=\"en\", nullable=False))\n    depth: int = Field(sa_column=sa.Column(sa.Integer, default=0, server_default=sa.text(\"0\"), nullable=False))\n    children_count: int = Field(sa_column=sa.Column(sa.Integer, default=0, server_default=sa.text(\"0\"), nullable=False))\n    deleted: bool = Field(sa_column=sa.Column(sa.Boolean, nullable=False, server_default=false()))\n\n    search_vector: Optional[str] = Field(sa_column=sa.Column(pg.TSVECTOR(), nullable=True))\n\n    review_count: int = Field(sa_column=sa.Column(sa.Integer, default=0, server_default=sa.text(\"0\"), nullable=False))\n    review_result: bool = Field(sa_column=sa.Column(sa.Boolean, nullable=True))\n    ranking_count: int = Field(sa_column=sa.Column(sa.Integer, default=0, server_default=sa.text(\"0\"), nullable=False))\n\n    rank: Optional[int] = Field(nullable=True)\n\n    synthetic: Optional[bool] = Field(\n        sa_column=sa.Column(sa.Boolean, default=False, server_default=false(), nullable=False)\n    )\n    edited: bool = Field(sa_column=sa.Column(sa.Boolean, default=False, server_default=false(), nullable=False))\n    model_name: Optional[str] = Field(sa_column=sa.Column(sa.String(1024), nullable=True))\n\n    emojis: Optional[dict[str, int]] = Field(default=None, sa_column=sa.Column(pg.JSONB), nullable=False)\n    _user_emojis: Optional[list[str]] = PrivateAttr(default=None)\n    _user_is_author: Optional[bool] = PrivateAttr(default=None)\n    _user: Optional[bool] = PrivateAttr(default=None)\n\n    def ensure_is_message(self) -> None:\n        if not self.payload or not isinstance(self.payload.payload, MessagePayload):\n            raise OasstError(\"Invalid message\", OasstErrorCode.INVALID_MESSAGE, HTTPStatus.INTERNAL_SERVER_ERROR)\n\n    def has_emoji(self, emoji_code: str) -> bool:\n        return self.emojis and emoji_code in self.emojis and self.emojis[emoji_code] > 0\n\n    def has_user_emoji(self, emoji_code: str) -> bool:\n        return self._user_emojis and emoji_code in self._user_emojis\n\n    @property\n    def text(self) -> str:\n        self.ensure_is_message()\n        return self.payload.payload.text\n\n    @property\n    def user_emojis(self) -> str:\n        return self._user_emojis\n\n    @property\n    def user_is_author(self) -> str:\n        return self._user_is_author\n\n    @property\n    def user(self) -> User:\n        return self._user\n", "backend/oasst_backend/models/message_revision.py": "from datetime import datetime\nfrom typing import Optional\nfrom uuid import UUID\n\nimport sqlalchemy as sa\nimport sqlalchemy.dialects.postgresql as pg\nfrom pydantic import PrivateAttr\nfrom sqlmodel import Field, SQLModel\nfrom uuid_extensions import uuid7\n\nfrom .payload_column_type import PayloadContainer, payload_column_type\n\n\nclass MessageRevision(SQLModel, table=True):\n    __tablename__ = \"message_revision\"\n\n    id: UUID = Field(sa_column=sa.Column(pg.UUID(as_uuid=True), primary_key=True, default=uuid7))\n\n    payload: Optional[PayloadContainer] = Field(\n        sa_column=sa.Column(payload_column_type(PayloadContainer), nullable=True)\n    )\n    message_id: UUID = Field(sa_column=sa.Column(sa.ForeignKey(\"message.id\"), nullable=False, index=True))\n    user_id: Optional[UUID] = Field(sa_column=sa.Column(sa.ForeignKey(\"user.id\"), nullable=True))\n    created_date: Optional[datetime] = Field(\n        sa_column=sa.Column(sa.DateTime(timezone=True), nullable=True, server_default=sa.func.current_timestamp())\n    )\n\n    _user_is_author: Optional[bool] = PrivateAttr(default=None)\n", "backend/oasst_backend/models/message_embedding.py": "from datetime import datetime\nfrom typing import List, Optional\nfrom uuid import UUID\n\nimport sqlalchemy as sa\nimport sqlalchemy.dialects.postgresql as pg\nfrom sqlmodel import ARRAY, Field, Float, SQLModel\n\n\nclass MessageEmbedding(SQLModel, table=True):\n    __tablename__ = \"message_embedding\"\n    __table_args__ = (sa.PrimaryKeyConstraint(\"message_id\", \"model\"),)\n\n    message_id: UUID = Field(sa_column=sa.Column(pg.UUID(as_uuid=True), sa.ForeignKey(\"message.id\"), nullable=False))\n    model: str = Field(max_length=256, nullable=False)\n    embedding: List[float] = Field(sa_column=sa.Column(ARRAY(Float)), nullable=True)\n\n    # In the case that the Message Embedding is created afterwards\n    created_date: Optional[datetime] = Field(\n        sa_column=sa.Column(sa.DateTime(timezone=True), nullable=False, server_default=sa.func.current_timestamp())\n    )\n", "backend/oasst_backend/models/api_client.py": "from typing import Optional\nfrom uuid import UUID, uuid4\n\nimport sqlalchemy as sa\nimport sqlalchemy.dialects.postgresql as pg\nfrom sqlalchemy import false\nfrom sqlmodel import Field, SQLModel\n\n\nclass ApiClient(SQLModel, table=True):\n    __tablename__ = \"api_client\"\n\n    id: Optional[UUID] = Field(\n        sa_column=sa.Column(\n            pg.UUID(as_uuid=True), primary_key=True, default=uuid4, server_default=sa.text(\"gen_random_uuid()\")\n        ),\n    )\n    api_key: str = Field(max_length=512, index=True, unique=True)\n    description: str = Field(max_length=256)\n    admin_email: Optional[str] = Field(max_length=256, nullable=True)\n    enabled: bool = Field(default=True)\n    trusted: bool = Field(sa_column=sa.Column(sa.Boolean, nullable=False, server_default=false()))\n    frontend_type: str = Field(max_length=256, nullable=True)\n", "backend/oasst_backend/models/message_tree_state.py": "from datetime import datetime\nfrom enum import Enum\nfrom typing import Optional\nfrom uuid import UUID\n\nimport sqlalchemy as sa\nimport sqlalchemy.dialects.postgresql as pg\nfrom sqlmodel import Field, Index, SQLModel\n\n\nclass State(str, Enum):\n    \"\"\"States of the Open-Assistant message tree state machine.\"\"\"\n\n    INITIAL_PROMPT_REVIEW = \"initial_prompt_review\"\n    \"\"\"In this state the message tree consists only of a single initial prompt root node.\n    Initial prompt labeling tasks will determine if the tree goes into `growing` or\n    `aborted_low_grade` state.\"\"\"\n\n    GROWING = \"growing\"\n    \"\"\"Assistant & prompter human demonstrations are collected. Concurrently labeling tasks\n    are handed out to check if the quality of the replies surpasses the minimum acceptable\n    quality.\n    When the required number of messages passing the initial labelling-quality check has been\n    collected the tree will enter `ranking`. If too many poor-quality labelling responses\n    are received the tree can also enter the `aborted_low_grade` state.\"\"\"\n\n    RANKING = \"ranking\"\n    \"\"\"The tree has been successfully populated with the desired number of messages. Ranking\n    tasks are now handed out for all nodes with more than one child.\"\"\"\n\n    READY_FOR_SCORING = \"ready_for_scoring\"\n    \"\"\"Required ranking responses have been collected and the scoring algorithm can now\n    compute the aggregated ranking scores that will appear in the dataset.\"\"\"\n\n    READY_FOR_EXPORT = \"ready_for_export\"\n    \"\"\"The Scoring algorithm computed rankings scores for all children. The message tree can be\n    exported as part of an Open-Assistant message tree dataset.\"\"\"\n\n    SCORING_FAILED = \"scoring_failed\"\n    \"\"\"An exception occurred in the scoring algorithm.\"\"\"\n\n    ABORTED_LOW_GRADE = \"aborted_low_grade\"\n    \"\"\"The system received too many bad reviews and stopped handing out tasks for this message tree.\"\"\"\n\n    HALTED_BY_MODERATOR = \"halted_by_moderator\"\n    \"\"\"A moderator decided to manually halt the message tree construction process.\"\"\"\n\n    BACKLOG_RANKING = \"backlog_ranking\"\n    \"\"\"Imported tree ready to be activated and ranked by users (currently inactive).\"\"\"\n\n    PROMPT_LOTTERY_WAITING = \"prompt_lottery_waiting\"\n    \"\"\"Initial prompt has passed spam check, waiting to be drawn to grow.\"\"\"\n\n\nVALID_STATES = (\n    State.INITIAL_PROMPT_REVIEW,\n    State.GROWING,\n    State.RANKING,\n    State.READY_FOR_SCORING,\n    State.READY_FOR_EXPORT,\n    State.ABORTED_LOW_GRADE,\n    State.BACKLOG_RANKING,\n)\n\nTERMINAL_STATES = (\n    State.READY_FOR_EXPORT,\n    State.ABORTED_LOW_GRADE,\n    State.SCORING_FAILED,\n    State.HALTED_BY_MODERATOR,\n    State.BACKLOG_RANKING,\n    State.PROMPT_LOTTERY_WAITING,\n)\n\n\nclass MessageTreeState(SQLModel, table=True):\n    __tablename__ = \"message_tree_state\"\n    __table_args__ = (Index(\"ix_message_tree_state__lang__state\", \"state\", \"lang\", unique=False),)\n\n    message_tree_id: UUID = Field(\n        sa_column=sa.Column(pg.UUID(as_uuid=True), sa.ForeignKey(\"message.id\"), primary_key=True)\n    )\n    goal_tree_size: int = Field(nullable=False)\n    max_depth: int = Field(nullable=False)\n    max_children_count: int = Field(nullable=False)\n    state: str = Field(nullable=False, max_length=128)\n    active: bool = Field(nullable=False, index=True)\n    origin: str = Field(sa_column=sa.Column(sa.String(1024), nullable=True))\n    won_prompt_lottery_date: Optional[datetime] = Field(sa_column=sa.Column(sa.DateTime(timezone=True), nullable=True))\n    lang: str = Field(sa_column=sa.Column(sa.String(32), nullable=False))\n", "backend/oasst_backend/models/message_toxicity.py": "from datetime import datetime\nfrom typing import Optional\nfrom uuid import UUID\n\nimport sqlalchemy as sa\nimport sqlalchemy.dialects.postgresql as pg\nfrom sqlmodel import Field, Float, SQLModel\n\n\nclass MessageToxicity(SQLModel, table=True):\n    __tablename__ = \"message_toxicity\"\n    __table_args__ = (sa.PrimaryKeyConstraint(\"message_id\", \"model\"),)\n\n    message_id: UUID = Field(sa_column=sa.Column(pg.UUID(as_uuid=True), sa.ForeignKey(\"message.id\"), nullable=False))\n    model: str = Field(max_length=256, nullable=False)\n\n    # Storing the score and the label of the message\n    score: float = Field(sa_column=sa.Column(Float), nullable=False)\n    label: str = Field(max_length=256, nullable=False)\n\n    # In the case that the Message Embedding is created afterwards\n    created_date: Optional[datetime] = Field(\n        sa_column=sa.Column(sa.DateTime(timezone=True), nullable=False, server_default=sa.func.current_timestamp())\n    )\n", "backend/oasst_backend/models/user.py": "from datetime import datetime\nfrom typing import Optional\nfrom uuid import UUID, uuid4\n\nimport sqlalchemy as sa\nimport sqlalchemy.dialects.postgresql as pg\nfrom oasst_shared.schemas import protocol\nfrom sqlmodel import AutoString, Field, Index, SQLModel\n\n\nclass User(SQLModel, table=True):\n    __tablename__ = \"user\"\n    __table_args__ = (\n        Index(\"ix_user_username\", \"api_client_id\", \"username\", \"auth_method\", unique=True),\n        Index(\"ix_user_display_name_id\", \"display_name\", \"id\", unique=True),\n    )\n\n    id: Optional[UUID] = Field(\n        sa_column=sa.Column(\n            pg.UUID(as_uuid=True), primary_key=True, default=uuid4, server_default=sa.text(\"gen_random_uuid()\")\n        ),\n    )\n    username: str = Field(nullable=False, max_length=128)\n    auth_method: str = Field(nullable=False, max_length=128, default=\"local\")\n    display_name: str = Field(nullable=False, max_length=256)\n    created_date: Optional[datetime] = Field(\n        sa_column=sa.Column(sa.DateTime(timezone=True), nullable=False, server_default=sa.func.current_timestamp())\n    )\n    api_client_id: UUID = Field(foreign_key=\"api_client.id\")\n    enabled: bool = Field(sa_column=sa.Column(sa.Boolean, nullable=False, server_default=sa.true()))\n    notes: str = Field(sa_column=sa.Column(AutoString(length=1024), nullable=False, server_default=\"\"))\n    deleted: bool = Field(sa_column=sa.Column(sa.Boolean, nullable=False, server_default=sa.false()))\n    show_on_leaderboard: bool = Field(sa_column=sa.Column(sa.Boolean, nullable=False, server_default=sa.true()))\n\n    # only used for time span \"total\"\n    streak_last_day_date: Optional[datetime] = Field(\n        sa_column=sa.Column(sa.DateTime(timezone=True), nullable=True, server_default=sa.func.current_timestamp())\n    )\n    streak_days: Optional[int] = Field(nullable=True)\n    last_activity_date: Optional[datetime] = Field(\n        sa_column=sa.Column(sa.DateTime(timezone=True), nullable=True, server_default=sa.func.current_timestamp())\n    )\n\n    # terms of service acceptance date\n    tos_acceptance_date: Optional[datetime] = Field(sa_column=sa.Column(sa.DateTime(timezone=True), nullable=True))\n\n    def to_protocol_frontend_user(self):\n        return protocol.FrontEndUser(\n            user_id=self.id,\n            id=self.username,\n            display_name=self.display_name,\n            auth_method=self.auth_method,\n            enabled=self.enabled,\n            deleted=self.deleted,\n            notes=self.notes,\n            created_date=self.created_date,\n            show_on_leaderboard=self.show_on_leaderboard,\n            streak_days=self.streak_days,\n            streak_last_day_date=self.streak_last_day_date,\n            last_activity_date=self.last_activity_date,\n            tos_acceptance_date=self.tos_acceptance_date,\n        )\n\n\nclass Account(SQLModel, table=True):\n    __tablename__ = \"account\"\n    __table_args__ = (Index(\"provider\", \"provider_account_id\", unique=True),)\n\n    id: Optional[UUID] = Field(\n        sa_column=sa.Column(\n            pg.UUID(as_uuid=True), primary_key=True, default=uuid4, server_default=sa.text(\"gen_random_uuid()\")\n        ),\n    )\n    user_id: UUID = Field(foreign_key=\"user.id\")\n    provider: str = Field(nullable=False, max_length=128, default=\"email\")  # discord or email\n    provider_account_id: str = Field(nullable=False, max_length=128)\n", "backend/oasst_backend/models/flagged_message.py": "from datetime import datetime\nfrom typing import Optional\nfrom uuid import UUID\n\nimport sqlalchemy as sa\nimport sqlalchemy.dialects.postgresql as pg\nfrom sqlmodel import Field, SQLModel\n\n\nclass FlaggedMessage(SQLModel, table=True):\n    __tablename__ = \"flagged_message\"\n\n    message_id: Optional[UUID] = Field(\n        sa_column=sa.Column(\n            pg.UUID(as_uuid=True), sa.ForeignKey(\"message.id\", ondelete=\"CASCADE\"), nullable=False, primary_key=True\n        )\n    )\n    processed: bool = Field(nullable=False, index=True)\n    created_date: Optional[datetime] = Field(\n        sa_column=sa.Column(\n            sa.DateTime(timezone=True), nullable=False, server_default=sa.func.current_timestamp(), index=True\n        )\n    )\n", "backend/oasst_backend/models/payload_column_type.py": "import json\nfrom typing import Any, Generic, Type, TypeVar\n\nimport sqlalchemy.dialects.postgresql as pg\nfrom fastapi.encoders import jsonable_encoder\nfrom pydantic import BaseModel, parse_obj_as, validator\nfrom pydantic.main import ModelMetaclass\nfrom sqlalchemy.types import TypeDecorator\n\npayload_type_registry = {}\n\n\nP = TypeVar(\"P\", bound=BaseModel)\n\n\ndef payload_type(cls: Type[P]) -> Type[P]:\n    payload_type_registry[cls.__name__] = cls\n    return cls\n\n\nclass PayloadContainer(BaseModel):\n    payload_type: str = \"\"\n    payload: BaseModel = None\n\n    def __init__(self, **v):\n        p = v[\"payload\"]\n        if isinstance(p, dict):\n            t = v[\"payload_type\"]\n            if t not in payload_type_registry:\n                raise RuntimeError(f\"Payload type '{t}' not registered\")\n            cls = payload_type_registry[t]\n            v[\"payload\"] = cls(**p)\n        super().__init__(**v)\n\n    @validator(\"payload\", pre=True)\n    def check_payload(cls, v: BaseModel, values: dict[str, Any]) -> BaseModel:\n        values[\"payload_type\"] = type(v).__name__\n        return v\n\n    class Config:\n        orm_mode = True\n\n\nT = TypeVar(\"T\")\n\n\ndef payload_column_type(pydantic_type):\n    class PayloadJSONBType(TypeDecorator, Generic[T]):\n        impl = pg.JSONB()\n\n        cache_ok = True\n\n        def __init__(\n            self,\n            json_encoder=json,\n        ):\n            self.json_encoder = json_encoder\n            super().__init__()\n\n        # serialize\n        def bind_processor(self, dialect):\n            impl_processor = self.impl.bind_processor(dialect)\n            dumps = self.json_encoder.dumps\n\n            def process(value: T):\n                if value is not None:\n                    if isinstance(pydantic_type, ModelMetaclass):\n                        # This allows to assign non-InDB models and if they're\n                        # compatible, they're directly parsed into the InDB\n                        # representation, thus hiding the implementation in the\n                        # background. However, the InDB model will still be returned\n                        value_to_dump = pydantic_type.from_orm(value)\n                    else:\n                        value_to_dump = value\n\n                    value = jsonable_encoder(value_to_dump)\n\n                if impl_processor:\n                    return impl_processor(value)\n                else:\n                    return dumps(jsonable_encoder(value_to_dump))\n\n            return process\n\n        # deserialize\n        def result_processor(self, dialect, coltype) -> T:\n            impl_processor = self.impl.result_processor(dialect, coltype)\n\n            def process(value):\n                if impl_processor:\n                    value = impl_processor(value)\n                if value is None:\n                    return None\n                # Explicitly use the generic directly, not type(T)\n                full_obj = parse_obj_as(pydantic_type, value)\n                return full_obj\n\n            return process\n\n        def compare_values(self, x, y):\n            return x == y\n\n    return PayloadJSONBType\n", "backend/oasst_backend/models/journal.py": "from datetime import datetime\nfrom typing import Optional\nfrom uuid import UUID, uuid1, uuid4\n\nimport sqlalchemy as sa\nimport sqlalchemy.dialects.postgresql as pg\nfrom sqlmodel import Field, SQLModel\n\nfrom .payload_column_type import PayloadContainer, payload_column_type\n\n\ndef generate_time_uuid(node=None, clock_seq=None):\n    \"\"\"Create a lexicographically sortable time ordered custom (non-standard) UUID by reordering the timestamp fields of a version 1 UUID.\"\"\"\n    (time_low, time_mid, time_hi_version, clock_seq_hi_variant, clock_seq_low, node) = uuid1(node, clock_seq).fields\n    # reconstruct 60 bit timestamp, see version 1 uuid: https://www.rfc-editor.org/rfc/rfc4122\n    timestamp = (time_hi_version & 0xFFF) << 48 | (time_mid << 32) | time_low\n    version = time_hi_version >> 12\n    assert version == 1\n    a = timestamp >> 28  # bits 28-59\n    b = (timestamp >> 12) & 0xFFFF  # bits 12-27\n    c = timestamp & 0xFFF  # bits 0-11 (clear version bits)\n    clock_seq_hi_variant &= 0xF  # (clear variant bits)\n    return UUID(fields=(a, b, c, clock_seq_hi_variant, clock_seq_low, node), version=None)\n\n\nclass Journal(SQLModel, table=True):\n    __tablename__ = \"journal\"\n\n    id: Optional[UUID] = Field(\n        sa_column=sa.Column(pg.UUID(as_uuid=True), primary_key=True, default=generate_time_uuid),\n    )\n    created_date: Optional[datetime] = Field(\n        sa_column=sa.Column(sa.DateTime(timezone=True), nullable=False, server_default=sa.func.current_timestamp())\n    )\n    user_id: Optional[UUID] = Field(nullable=True, foreign_key=\"user.id\", index=True)\n    message_id: Optional[UUID] = Field(foreign_key=\"message.id\", nullable=True)\n    api_client_id: UUID = Field(foreign_key=\"api_client.id\")\n\n    event_type: str = Field(nullable=False, max_length=200)\n    event_payload: PayloadContainer = Field(sa_column=sa.Column(payload_column_type(PayloadContainer), nullable=False))\n\n\nclass JournalIntegration(SQLModel, table=True):\n    __tablename__ = \"journal_integration\"\n\n    id: Optional[UUID] = Field(\n        sa_column=sa.Column(\n            pg.UUID(as_uuid=True), primary_key=True, default=uuid4, server_default=sa.text(\"gen_random_uuid()\")\n        ),\n    )\n    description: str = Field(max_length=512, primary_key=True)\n    last_journal_id: Optional[UUID] = Field(foreign_key=\"journal.id\", nullable=True)\n    last_run: Optional[datetime] = Field(sa_column=sa.Column(sa.DateTime(timezone=True), nullable=True))\n    last_error: Optional[str] = Field(nullable=True)\n    next_run: Optional[datetime] = Field(nullable=True)\n", "backend/oasst_backend/models/troll_stats.py": "from datetime import datetime\nfrom typing import Optional\nfrom uuid import UUID\n\nimport sqlalchemy as sa\nimport sqlalchemy.dialects.postgresql as pg\nfrom sqlmodel import Field, Index, SQLModel\n\n\nclass TrollStats(SQLModel, table=True):\n    __tablename__ = \"troll_stats\"\n    __table_args__ = (Index(\"ix_troll_stats__timeframe__user_id\", \"time_frame\", \"user_id\", unique=True),)\n\n    time_frame: Optional[str] = Field(nullable=False, primary_key=True)\n    user_id: Optional[UUID] = Field(\n        sa_column=sa.Column(pg.UUID(as_uuid=True), sa.ForeignKey(\"user.id\", ondelete=\"CASCADE\"), primary_key=True)\n    )\n    base_date: Optional[datetime] = Field(sa_column=sa.Column(sa.DateTime(timezone=True), nullable=True))\n\n    troll_score: int = 0\n    modified_date: Optional[datetime] = Field(\n        sa_column=sa.Column(sa.DateTime(timezone=True), nullable=False, server_default=sa.func.current_timestamp())\n    )\n\n    rank: int = Field(nullable=True)\n\n    red_flags: int = 0  # num reported messages of user\n    upvotes: int = 0  # num up-voted messages of user\n    downvotes: int = 0  # num down-voted messages of user\n\n    spam_prompts: int = 0\n\n    quality: float = Field(nullable=True)\n    humor: float = Field(nullable=True)\n    toxicity: float = Field(nullable=True)\n    violence: float = Field(nullable=True)\n    helpfulness: float = Field(nullable=True)\n\n    spam: int = 0\n    lang_mismach: int = 0\n    not_appropriate: int = 0\n    pii: int = 0\n    hate_speech: int = 0\n    sexual_content: int = 0\n    political_content: int = 0\n\n    def compute_troll_score(self) -> int:\n        return (\n            self.red_flags * 3\n            - self.upvotes\n            + self.downvotes\n            + self.spam_prompts\n            + self.lang_mismach\n            + self.not_appropriate\n            + self.pii\n            + self.hate_speech\n            + self.sexual_content\n            + self.political_content\n        )\n", "backend/oasst_backend/models/text_labels.py": "from datetime import datetime\nfrom typing import Optional\nfrom uuid import UUID, uuid4\n\nimport sqlalchemy as sa\nimport sqlalchemy.dialects.postgresql as pg\nfrom sqlmodel import Field, SQLModel\n\n\nclass TextLabels(SQLModel, table=True):\n    __tablename__ = \"text_labels\"\n\n    id: Optional[UUID] = Field(\n        sa_column=sa.Column(\n            pg.UUID(as_uuid=True), primary_key=True, default=uuid4, server_default=sa.text(\"gen_random_uuid()\")\n        ),\n    )\n    user_id: UUID = Field(sa_column=sa.Column(pg.UUID(as_uuid=True), sa.ForeignKey(\"user.id\"), nullable=False))\n    created_date: Optional[datetime] = Field(\n        sa_column=sa.Column(\n            sa.DateTime(timezone=True), nullable=False, server_default=sa.func.current_timestamp(), index=True\n        ),\n    )\n    api_client_id: UUID = Field(nullable=False, foreign_key=\"api_client.id\")\n    text: str = Field(nullable=False, max_length=2**16)\n    message_id: Optional[UUID] = Field(\n        sa_column=sa.Column(pg.UUID(as_uuid=True), sa.ForeignKey(\"message.id\"), nullable=True, index=True)\n    )\n    labels: dict[str, float] = Field(default={}, sa_column=sa.Column(pg.JSONB), nullable=False)\n    task_id: Optional[UUID] = Field(nullable=True, index=True)\n", "backend/oasst_backend/models/__init__.py": "from .api_client import ApiClient\nfrom .cached_stats import CachedStats\nfrom .flagged_message import FlaggedMessage\nfrom .journal import Journal, JournalIntegration\nfrom .message import Message\nfrom .message_embedding import MessageEmbedding\nfrom .message_emoji import MessageEmoji\nfrom .message_reaction import MessageReaction\nfrom .message_revision import MessageRevision\nfrom .message_toxicity import MessageToxicity\nfrom .message_tree_state import MessageTreeState\nfrom .task import Task\nfrom .text_labels import TextLabels\nfrom .troll_stats import TrollStats\nfrom .user import User\nfrom .user_stats import UserStats, UserStatsTimeFrame\n\n__all__ = [\n    \"ApiClient\",\n    \"User\",\n    \"UserStats\",\n    \"UserStatsTimeFrame\",\n    \"Message\",\n    \"MessageEmbedding\",\n    \"MessageReaction\",\n    \"MessageRevision\",\n    \"MessageTreeState\",\n    \"MessageToxicity\",\n    \"Task\",\n    \"TextLabels\",\n    \"Journal\",\n    \"JournalIntegration\",\n    \"MessageEmoji\",\n    \"TrollStats\",\n    \"FlaggedMessage\",\n    \"CachedStats\",\n]\n", "backend/oasst_backend/models/user_stats.py": "from datetime import datetime\nfrom enum import Enum\nfrom typing import Optional\nfrom uuid import UUID\n\nimport sqlalchemy as sa\nimport sqlalchemy.dialects.postgresql as pg\nfrom sqlmodel import Field, Index, SQLModel\n\n\nclass UserStatsTimeFrame(str, Enum):\n    day = \"day\"\n    week = \"week\"\n    month = \"month\"\n    total = \"total\"\n\n\nclass UserStats(SQLModel, table=True):\n    __tablename__ = \"user_stats\"\n    __table_args__ = (\n        Index(\"ix_user_stats__timeframe__user_id\", \"time_frame\", \"user_id\", unique=True),\n        Index(\"ix_user_stats__timeframe__rank__user_id\", \"time_frame\", \"rank\", \"user_id\", unique=True),\n    )\n\n    time_frame: Optional[str] = Field(nullable=False, primary_key=True)\n    user_id: Optional[UUID] = Field(\n        sa_column=sa.Column(pg.UUID(as_uuid=True), sa.ForeignKey(\"user.id\"), primary_key=True)\n    )\n    base_date: Optional[datetime] = Field(sa_column=sa.Column(sa.DateTime(timezone=True), nullable=True))\n\n    leader_score: int = 0\n    modified_date: Optional[datetime] = Field(\n        sa_column=sa.Column(sa.DateTime(timezone=True), nullable=False, server_default=sa.func.current_timestamp())\n    )\n\n    rank: int = Field(nullable=True)\n\n    prompts: int = 0\n    replies_assistant: int = 0\n    replies_prompter: int = 0\n    labels_simple: int = 0\n    labels_full: int = 0\n    rankings_total: int = 0\n    rankings_good: int = 0\n\n    accepted_prompts: int = 0\n    accepted_replies_assistant: int = 0\n    accepted_replies_prompter: int = 0\n\n    reply_ranked_1: int = 0\n    reply_ranked_2: int = 0\n    reply_ranked_3: int = 0\n\n    def compute_leader_score(self) -> int:\n        return (\n            int(self.prompts * 0.1)\n            + self.replies_assistant * 4\n            + self.replies_prompter\n            + self.labels_simple\n            + self.labels_full * 2\n            + self.rankings_total\n            + self.rankings_good\n            + int(self.accepted_prompts * 0.1)\n            + self.accepted_replies_assistant * 4\n            + self.accepted_replies_prompter\n            + self.reply_ranked_1 * 9\n            + self.reply_ranked_2 * 3\n            + self.reply_ranked_3\n        )\n", "backend/oasst_backend/models/cached_stats.py": "from datetime import datetime\n\nimport sqlalchemy as sa\nimport sqlalchemy.dialects.postgresql as pg\nfrom sqlmodel import AutoString, Field, SQLModel\n\n\nclass CachedStats(SQLModel, table=True):\n    __tablename__ = \"cached_stats\"\n\n    name: str = Field(sa_column=sa.Column(AutoString(length=128), primary_key=True))\n\n    modified_date: datetime | None = Field(\n        sa_column=sa.Column(sa.DateTime(timezone=True), nullable=False, server_default=sa.func.current_timestamp())\n    )\n\n    stats: dict | list | None = Field(None, sa_column=sa.Column(pg.JSONB, nullable=False))\n", "backend/oasst_backend/utils/tree_export.py": "from __future__ import annotations\n\nimport contextlib\nimport gzip\nimport json\nimport sys\nimport uuid\nfrom collections import defaultdict\nfrom typing import Iterable, Optional, TextIO\n\nfrom fastapi.encoders import jsonable_encoder\nfrom oasst_backend.models import Message\nfrom oasst_backend.models.message_tree_state import State as TreeState\nfrom oasst_data import (\n    ExportMessageEvent,\n    ExportMessageEventEmoji,\n    ExportMessageEventRanking,\n    ExportMessageEventRating,\n    ExportMessageNode,\n    ExportMessageTree,\n    LabelValues,\n)\nfrom oasst_shared.utils import Anonymizer\n\n\ndef prepare_export_message_node(\n    message: Message,\n    labels: Optional[LabelValues] = None,\n    anonymizer: Anonymizer | None = None,\n    events: dict[str, list[ExportMessageEvent]] | None = None,\n) -> ExportMessageNode:\n    message_id = str(message.id)\n    parent_id = str(message.parent_id) if message.parent_id else None\n    user_id = str(message.user_id) if message.user_id else None\n    if anonymizer is not None:\n        message_id = anonymizer.anonymize(\"message\", message_id)\n        parent_id = anonymizer.anonymize(\"message\", parent_id)\n        user_id = anonymizer.anonymize(\"user\", user_id)\n        if events is not None:\n            for event_key, event_values in events.items():\n                for event in event_values:\n                    match event_key:\n                        case \"emoji\":\n                            event: ExportMessageEventEmoji = event\n                            if event.user_id is not None:\n                                event.user_id = anonymizer.anonymize(\"user\", event.user_id)\n                        case \"rating\":\n                            event: ExportMessageEventRating = event\n                            if event.user_id is not None:\n                                event.user_id = anonymizer.anonymize(\"user\", event.user_id)\n                        case \"ranking\":\n                            event: ExportMessageEventRanking = event\n                            if event.user_id is not None:\n                                event.user_id = anonymizer.anonymize(\"user\", event.user_id)\n                            event.ranked_message_ids = [\n                                anonymizer.anonymize(\"message\", m) for m in event.ranked_message_ids\n                            ]\n                            if event.ranking_parent_id is not None:\n                                event.ranking_parent_id = anonymizer.anonymize(\"message\", event.ranking_parent_id)\n                            if event.message_tree_id is not None:\n                                event.message_tree_id = anonymizer.anonymize(\"message_tree\", event.message_tree_id)\n                        case _:\n                            raise ValueError(f\"Unknown event type {event_key}\")\n    assert message_id is not None\n    return ExportMessageNode(\n        message_id=message_id,\n        parent_id=parent_id,\n        user_id=user_id,\n        created_date=message.created_date,\n        text=str(message.payload.payload.text),\n        role=message.role,\n        lang=message.lang,\n        deleted=message.deleted,\n        review_count=message.review_count,\n        review_result=message.review_result if message.review_result or message.review_count > 2 else None,\n        synthetic=message.synthetic,\n        model_name=message.model_name,\n        emojis=message.emojis,\n        rank=message.rank,\n        labels=labels,\n        events=events,\n    )\n\n\ndef build_export_tree(\n    message_tree_id: uuid.UUID,\n    message_tree_state: TreeState,\n    messages: list[Message],\n    labels: Optional[dict[uuid.UUID, LabelValues]] = None,\n    anonymizer: Anonymizer | None = None,\n    events: dict[uuid.UUID, dict[str, list[ExportMessageEvent]]] | None = None,\n) -> ExportMessageTree:\n    export_messages = [\n        prepare_export_message_node(\n            m, (labels.get(m.id) if labels else None), anonymizer=anonymizer, events=events.get(m.id)\n        )\n        for m in messages\n    ]\n\n    messages_by_parent = defaultdict(list)\n    for message in export_messages:\n        messages_by_parent[message.parent_id].append(message)\n\n    def assign_replies(node: ExportMessageNode) -> ExportMessageNode:\n        node.replies = messages_by_parent[node.message_id]\n        node.replies.sort(key=lambda x: x.rank if x.rank is not None else float(\"inf\"))\n        for child in node.replies:\n            assign_replies(child)\n        return node\n\n    prompt = assign_replies(messages_by_parent[None][0])\n    return ExportMessageTree(message_tree_id=str(message_tree_id), tree_state=message_tree_state, prompt=prompt)\n\n\n# see https://stackoverflow.com/questions/17602878/how-to-handle-both-with-open-and-sys-stdout-nicely\n@contextlib.contextmanager\ndef smart_open(filename: str = None) -> TextIO:\n    if filename and filename != \"-\":\n        fh = open(filename, \"wt\", encoding=\"UTF-8\")\n    else:\n        fh = sys.stdout\n\n    try:\n        yield fh\n    finally:\n        if fh is not sys.stdout:\n            fh.close()\n\n\ndef write_trees_to_file(filename: str | None, trees: list[ExportMessageTree], use_compression: bool = True) -> None:\n    out_buff: TextIO\n\n    if use_compression:\n        if not filename:\n            raise RuntimeError(\"File name must be specified when using compression.\")\n        out_buff = gzip.open(filename, \"wt\", encoding=\"UTF-8\")\n    else:\n        out_buff = smart_open(filename)\n\n    with out_buff as f:\n        for tree in trees:\n            file_data = jsonable_encoder(tree, exclude_none=True)\n            json.dump(file_data, f)\n            f.write(\"\\n\")\n\n\ndef write_messages_to_file(\n    filename: str | None,\n    messages: Iterable[Message],\n    use_compression: bool = True,\n    labels: Optional[dict[uuid.UUID, LabelValues]] = None,\n    anonymizer: Anonymizer | None = None,\n    events: dict[uuid.UUID, dict[str, list[ExportMessageEvent]]] | None = None,\n) -> None:\n    out_buff: TextIO\n\n    if use_compression:\n        if not filename:\n            raise RuntimeError(\"File name must be specified when using compression.\")\n        out_buff = gzip.open(filename, \"wt\", encoding=\"UTF-8\")\n    else:\n        out_buff = smart_open(filename)\n\n    with out_buff as f:\n        for m in messages:\n            export_message = prepare_export_message_node(\n                m, (labels.get(m.id) if labels else None), anonymizer=anonymizer, events=events.get(m.id)\n            )\n\n            file_data = jsonable_encoder(export_message, exclude_none=True)\n            json.dump(file_data, f)\n            f.write(\"\\n\")\n", "backend/oasst_backend/utils/similarity_functions.py": "import math\n\nimport numpy as np\nimport scipy.sparse as sp\nimport torch\nimport torch.nn.functional as F\nfrom pandas import DataFrame\nfrom sentence_transformers import SentenceTransformer\nfrom torch import Tensor\nfrom tqdm import tqdm\n\nADJACENCY_THRESHOLD = 0.65\n\n\ndef embed_data(\n    data: DataFrame,\n    key: str = \"query\",\n    model_name: str = \"all-MiniLM-L6-v2\",\n    cores: int = 1,\n    gpu: bool = False,\n    batch_size: int = 128,\n):\n    \"\"\"\n    Embed the sentences/text using the MiniLM language model (which uses mean pooling)\n    \"\"\"\n    print(\"Embedding data\")\n    model = SentenceTransformer(model_name)\n    print(\"Model loaded\")\n\n    sentences = data[key].tolist()\n    unique_sentences = data[key].unique()\n    print(\"Unique sentences\", len(unique_sentences))\n\n    if cores == 1:\n        embeddings = model.encode(unique_sentences, show_progress_bar=True, batch_size=batch_size)\n    else:\n        devices = [\"cpu\"] * cores\n        if gpu:\n            devices = None  # use all CUDA devices\n\n        # Start the multi-process pool on multiple devices\n        print(\"Multi-process pool starting\")\n        pool = model.start_multi_process_pool(devices)\n        print(\"Multi-process pool started\")\n\n        chunk_size = math.ceil(len(unique_sentences) / cores)\n\n        # Compute the embeddings using the multi-process pool\n        embeddings = model.encode_multi_process(unique_sentences, pool, batch_size=batch_size, chunk_size=chunk_size)\n        model.stop_multi_process_pool(pool)\n\n    print(\"Embeddings computed\")\n\n    mapping = {sentence: embedding for sentence, embedding in zip(unique_sentences, embeddings)}\n    embeddings = np.array([mapping[sentence] for sentence in sentences])\n\n    return embeddings\n\n\ndef cos_sim(a: Tensor, b: Tensor):\n    \"\"\"\n    Computes the cosine similarity cos_sim(a[i], b[j]) for all i and j.\n    :return: Matrix with res[i][j]  = cos_sim(a[i], b[j])\n    \"\"\"\n    if not isinstance(a, torch.Tensor):\n        a = torch.tensor(np.array(a))\n\n    if not isinstance(b, torch.Tensor):\n        b = torch.tensor(np.array(b))\n\n    if len(a.shape) == 1:\n        a = a.unsqueeze(0)\n\n    if len(b.shape) == 1:\n        b = b.unsqueeze(0)\n\n    a_norm = torch.nn.functional.normalize(a, p=2, dim=1)\n    b_norm = torch.nn.functional.normalize(b, p=2, dim=1)\n    return torch.mm(a_norm, b_norm.transpose(0, 1))\n\n\ndef cos_sim_torch(embs_a: Tensor, embs_b: Tensor) -> Tensor:\n    \"\"\"\n    Computes the cosine similarity cos_sim(a[i], b[j]) for all i and j.\n    Using torch.nn.functional.cosine_similarity\n    :return: Matrix with res[i][j]  = cos_sim(a[i], b[j])\n    \"\"\"\n    if not isinstance(embs_a, torch.Tensor):\n        embs_a = torch.tensor(np.array(embs_a))\n\n    if not isinstance(embs_b, torch.Tensor):\n        embs_b = torch.tensor(np.array(embs_b))\n\n    if len(embs_a.shape) == 1:\n        embs_a = embs_a.unsqueeze(0)\n\n    if len(embs_b.shape) == 1:\n        embs_b = embs_b.unsqueeze(0)\n    A = F.cosine_similarity(embs_a.unsqueeze(1), embs_b.unsqueeze(0), dim=2)\n    return A\n\n\ndef gaussian_kernel_torch(embs_a, embs_b, sigma=1.0):\n    \"\"\"\n    Computes the Gaussian kernel matrix between two sets of embeddings using PyTorch.\n    :param embs_a: Tensor of shape (batch_size_a, embedding_dim) containing the first set of embeddings.\n    :param embs_b: Tensor of shape (batch_size_b, embedding_dim) containing the second set of embeddings.\n    :param sigma: Width of the Gaussian kernel.\n    :return: Tensor of shape (batch_size_a, batch_size_b) containing the Gaussian kernel matrix.\n    \"\"\"\n    if not isinstance(embs_a, torch.Tensor):\n        embs_a = torch.tensor(embs_a)\n\n    if not isinstance(embs_b, torch.Tensor):\n        embs_b = torch.tensor(embs_b)\n\n    # Compute the pairwise distances between the embeddings\n    dist_matrix = torch.cdist(embs_a, embs_b)\n\n    # Compute the Gaussian kernel matrix\n    kernel_matrix = torch.exp(-(dist_matrix**2) / (2 * sigma**2))\n\n    return kernel_matrix\n\n\ndef compute_cos_sim_kernel(embs, threshold=0.65, kernel_type=\"cosine\"):\n    # match case to kernel type\n    if kernel_type == \"gaussian\":\n        A = gaussian_kernel_torch(embs, embs)\n    if kernel_type == \"cosine\":\n        A = cos_sim_torch(embs, embs)\n    adj_matrix = torch.zeros_like(A)\n    adj_matrix[A > threshold] = 1\n    adj_matrix[A <= threshold] = 0\n    adj_matrix = adj_matrix.numpy().astype(np.float32)\n    return adj_matrix\n\n\ndef k_hop_message_passing(A, node_features, k):\n    \"\"\"\n    Compute the k-hop adjacency matrix and aggregated features using message passing.\n\n    Parameters:\n    A (numpy array): The adjacency matrix of the graph.\n    node_features (numpy array): The feature matrix of the nodes.\n    k (int): The number of hops for message passing.\n\n    Returns:\n    A_k (numpy array): The k-hop adjacency matrix.\n    agg_features (numpy array): The aggregated feature matrix for each node in the k-hop neighborhood.\n    \"\"\"\n\n    print(\"Compute the k-hop adjacency matrix\")\n    A_k = np.linalg.matrix_power(A, k)\n\n    print(\"Aggregate the messages from the k-hop neighborhood:\")\n    agg_features = node_features.copy()\n\n    for i in tqdm(range(k)):\n        agg_features += np.matmul(np.linalg.matrix_power(A, i + 1), node_features)\n\n    return A_k, agg_features\n\n\ndef k_hop_message_passing_sparse(A, node_features, k):\n    \"\"\"\n    Compute the k-hop adjacency matrix and aggregated features using message passing.\n\n    Parameters:\n    A (numpy array or scipy sparse matrix): The adjacency matrix of the graph.\n    node_features (numpy array or scipy sparse matrix): The feature matrix of the nodes.\n    k (int): The number of hops for message passing.\n\n    Returns:\n    A_k (numpy array): The k-hop adjacency matrix.\n    agg_features (numpy array): The aggregated feature matrix for each node in the k-hop neighborhood.\n    \"\"\"\n\n    # Convert input matrices to sparse matrices if they are not already\n    if not sp.issparse(A):\n        A = sp.csr_matrix(A)\n    if not sp.issparse(node_features):\n        node_features = sp.csr_matrix(node_features)\n\n    # Compute the k-hop adjacency matrix and the aggregated features\n    A_k = A.copy()\n    agg_features = node_features.copy()\n\n    for i in tqdm(range(k)):\n        # Compute the message passing for the k-hop neighborhood\n        message = A_k.dot(node_features)\n        # Apply a GCN layer to aggregate the messages\n        agg_features = A_k.dot(agg_features) + message\n        # Update the k-hop adjacency matrix by adding new edges\n        A_k += A_k.dot(A)\n\n    return A_k.toarray(), agg_features.toarray()\n", "backend/oasst_backend/utils/message_tree_topic_modeling.py": "import argparse\n\nfrom bertopic import BERTopic\nfrom bertopic.representation import MaximalMarginalRelevance\nfrom bertopic.vectorizers import ClassTfidfTransformer\nfrom exported_tree_loading import load_data\nfrom sentence_transformers import SentenceTransformer\nfrom similarity_functions import compute_cos_sim_kernel, embed_data, k_hop_message_passing_sparse\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n\ndef argument_parsing():\n    parser = argparse.ArgumentParser(description=\"Process some arguments.\")\n    parser.add_argument(\"--model_name\", type=str, default=\"all-MiniLM-L6-v2\")\n    parser.add_argument(\"--cores\", type=int, default=1)\n    parser.add_argument(\"--pair_qa\", type=bool, default=True)\n    parser.add_argument(\"--use_gpu\", type=bool, default=False)\n    parser.add_argument(\"--batch_size\", type=int, default=128)\n    parser.add_argument(\"--k\", type=int, default=2)\n    parser.add_argument(\"--threshold\", type=float, default=0.65)\n    parser.add_argument(\"--exported_tree_path\", nargs=\"+\", help=\"<Required> Set flag\", required=True)\n    parser.add_argument(\"--min_topic_size\", type=int, default=10)\n    parser.add_argument(\"--diversity\", type=float, default=0.2)\n    parser.add_argument(\"--reduce_frequent_words\", type=bool, default=False)\n    parser.add_argument(\"--reduce_outliers_strategy\", type=str, default=\"c-tf-idf\")\n\n    args = parser.parse_args()\n    return args\n\n\ndef load_topic_model(args):\n    vectorizer_model = CountVectorizer(stop_words=\"english\")\n    ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=False)\n    model = SentenceTransformer(MODEL_NAME)\n    representation_model = MaximalMarginalRelevance(diversity=args.diversity)\n    topic_model = BERTopic(\n        nr_topics=\"auto\",\n        min_topic_size=args.min_topic_size,\n        representation_model=representation_model,\n        vectorizer_model=vectorizer_model,\n        ctfidf_model=ctfidf_model,\n        embedding_model=model,\n    )\n    return topic_model\n\n\ndef fit_topic_model(topic_model, data, embeddings, key=\"query\"):\n    topics, probs = topic_model.fit_transform(data[key].to_list(), embeddings)\n    return topics, probs\n\n\ndef get_topic_info(topic_model):\n    return topic_model.get_topic_info()\n\n\ndef reduce_topics(topic_model, data, nr_topics, key=\"query\"):\n    topic_model.reduce_topics(data[key].to_list(), nr_topics)\n    return topic_model\n\n\ndef get_representative_docs(topic_model):\n    return topic_model.get_representative_docs()\n\n\ndef reduce_outliers(topic_model, data, topics, probs, key=\"query\", strategy=\"c-tf-idf\"):\n    if strategy == \"c-tf-idf\":\n        new_topics = topic_model.reduce_outliers(data[key].to_list(), topics, strategy, threshold=0.1)\n    elif strategy == \"embeddings\":\n        new_topics = topic_model.reduce_outliers(data[key].to_list(), topics, strategy)\n    elif strategy == \"distributions\":\n        new_topics = topic_model.reduce_outliers(data[key].to_list(), topics, probabilities=probs, strategy=strategy)\n    else:\n        raise ValueError(\"Invalid strategy\")\n    return new_topics\n\n\ndef compute_hierarchical_topic_tree(topic_model, data, key=\"query\"):\n    hierarchical_topics = topic_model.hierarchical_topics(data[key].to_list())\n    tree = topic_model.get_topic_tree(hierarchical_topics)\n    return hierarchical_topics, tree\n\n\nif __name__ == \"__main__\":\n    \"\"\"\n    Main function to run topic modeling on a list of exported message trees.\n    Example usage:\n    python message_tree_topic_modeling.py --exported_tree_path 2023-02-06_oasst_prod.jsonl 2023-02-07_oasst_prod.jsonl\n    \"\"\"\n    args = argument_parsing()\n    MODEL_NAME = args.model_name\n    data, message_list = load_data(args.exported_tree_path, args.pair_qa)\n    embs = embed_data(data, model_name=MODEL_NAME, cores=args.cores, gpu=args.use_gpu)\n    adj_matrix = compute_cos_sim_kernel(embs, args.threshold)\n    print(adj_matrix.shape)\n    print(embs.shape)\n    A_k, agg_features = k_hop_message_passing_sparse(adj_matrix, embs, args.k)\n    print(A_k.shape)\n    topic_model = load_topic_model(args)\n    topics, probs = fit_topic_model(topic_model, data, agg_features)\n    freq = get_topic_info(topic_model)\n    rep_docs = get_representative_docs(topic_model)\n    print(freq)\n    for k, v in rep_docs.items():\n        print(k)\n        print(v)\n        print(\"\\n\\n\\n\")\n", "backend/oasst_backend/utils/database_utils.py": "from enum import IntEnum\nfrom functools import wraps\nfrom http import HTTPStatus\nfrom typing import Callable\n\nfrom loguru import logger\nfrom oasst_backend.config import settings\nfrom oasst_backend.database import engine\nfrom oasst_shared.exceptions import OasstError, OasstErrorCode\nfrom psycopg2.errors import DeadlockDetected, ExclusionViolation, SerializationFailure, UniqueViolation\nfrom sqlalchemy.exc import OperationalError, PendingRollbackError\nfrom sqlmodel import Session, SQLModel\n\n\"\"\"\nError Handling Reference: https://www.postgresql.org/docs/15/mvcc-serialization-failure-handling.html\n\"\"\"\n\nTEXT_SEARCH_LANGUAGE_MAPPING: dict[str, str] = {\n    \"ar\": \"arabic\",\n    \"hy\": \"armenian\",\n    \"eu\": \"basque\",\n    \"ca\": \"catalan\",\n    \"da\": \"danish\",\n    \"nl\": \"dutch\",\n    \"en\": \"english\",\n    \"fi\": \"finnish\",\n    \"fr\": \"french\",\n    \"de\": \"german\",\n    \"el\": \"greek\",\n    \"ga\": \"irish\",\n    \"hi\": \"hindi\",\n    \"hu\": \"hungarian\",\n    \"id\": \"indonesian\",\n    \"it\": \"italian\",\n    \"lt\": \"lithuanian\",\n    \"ne\": \"nepali\",\n    \"no\": \"norwegian\",\n    \"pt\": \"portuguese\",\n    \"ro\": \"romanian\",\n    \"ru\": \"russian\",\n    \"sr\": \"serbian\",\n    \"ta\": \"tamil\",\n    \"es\": \"spanish\",\n    \"sv\": \"swedish\",\n    \"tr\": \"turkish\",\n    \"yi\": \"yiddish\",\n}\n\n\ndef db_lang_to_postgres_ts_lang(db_lang: str) -> str:\n    # Return 'simple' if language is not directly supported by Postgres\n    return TEXT_SEARCH_LANGUAGE_MAPPING.get(db_lang, \"simple\")\n\n\nclass CommitMode(IntEnum):\n    \"\"\"\n    Commit modes for the managed tx methods\n    \"\"\"\n\n    NONE = 0\n    FLUSH = 1\n    COMMIT = 2\n    ROLLBACK = 3\n\n\n\"\"\"\n* managed_tx_method and async_managed_tx_method methods are decorators functions\n* to be used on class functions. It expects the Class to have a 'db' Session object\n* initialised\n\"\"\"\n\n\ndef managed_tx_method(auto_commit: CommitMode = CommitMode.COMMIT, num_retries=settings.DATABASE_MAX_TX_RETRY_COUNT):\n    def decorator(f):\n        @wraps(f)\n        def wrapped_f(self, *args, **kwargs):\n            try:\n                result = None\n                if auto_commit == CommitMode.COMMIT:\n                    retry_exhausted = True\n                    for i in range(num_retries):\n                        try:\n                            result = f(self, *args, **kwargs)\n                            self.db.commit()\n                            if isinstance(result, SQLModel):\n                                self.db.refresh(result)\n                            retry_exhausted = False\n                            break\n                        except PendingRollbackError as e:\n                            logger.info(str(e))\n                            self.db.rollback()\n                        except OperationalError as e:\n                            if e.orig is not None and isinstance(\n                                e.orig, (SerializationFailure, DeadlockDetected, UniqueViolation, ExclusionViolation)\n                            ):\n                                logger.info(f\"{type(e.orig)} Inner {e.orig.pgcode} {type(e.orig.pgcode)}\")\n                                self.db.rollback()\n                            else:\n                                raise e\n                        logger.info(f\"Retry {i+1}/{num_retries}\")\n                    if retry_exhausted:\n                        raise OasstError(\n                            \"DATABASE_MAX_RETIRES_EXHAUSTED\",\n                            error_code=OasstErrorCode.DATABASE_MAX_RETRIES_EXHAUSTED,\n                            http_status_code=HTTPStatus.SERVICE_UNAVAILABLE,\n                        )\n                else:\n                    result = f(self, *args, **kwargs)\n                    if auto_commit == CommitMode.FLUSH:\n                        self.db.flush()\n                        if isinstance(result, SQLModel):\n                            self.db.refresh(result)\n                    elif auto_commit == CommitMode.ROLLBACK:\n                        self.db.rollback()\n                return result\n            except Exception as e:\n                logger.info(str(e))\n                raise e\n\n        return wrapped_f\n\n    return decorator\n\n\ndef async_managed_tx_method(\n    auto_commit: CommitMode = CommitMode.COMMIT, num_retries=settings.DATABASE_MAX_TX_RETRY_COUNT\n):\n    def decorator(f):\n        @wraps(f)\n        async def wrapped_f(self, *args, **kwargs):\n            try:\n                result = None\n                if auto_commit == CommitMode.COMMIT:\n                    retry_exhausted = True\n                    for i in range(num_retries):\n                        try:\n                            result = await f(self, *args, **kwargs)\n                            self.db.commit()\n                            if isinstance(result, SQLModel):\n                                self.db.refresh(result)\n                            retry_exhausted = False\n                            break\n                        except PendingRollbackError as e:\n                            logger.info(str(e))\n                            self.db.rollback()\n                        except OperationalError as e:\n                            if e.orig is not None and isinstance(\n                                e.orig, (SerializationFailure, DeadlockDetected, UniqueViolation, ExclusionViolation)\n                            ):\n                                logger.info(f\"{type(e.orig)} Inner {e.orig.pgcode} {type(e.orig.pgcode)}\")\n                                self.db.rollback()\n                            else:\n                                raise e\n                        logger.info(f\"Retry {i+1}/{num_retries}\")\n                    if retry_exhausted:\n                        raise OasstError(\n                            \"DATABASE_MAX_RETIRES_EXHAUSTED\",\n                            error_code=OasstErrorCode.DATABASE_MAX_RETRIES_EXHAUSTED,\n                            http_status_code=HTTPStatus.SERVICE_UNAVAILABLE,\n                        )\n                else:\n                    result = await f(self, *args, **kwargs)\n                    if auto_commit == CommitMode.FLUSH:\n                        self.db.flush()\n                        if isinstance(result, SQLModel):\n                            self.db.refresh(result)\n                    elif auto_commit == CommitMode.ROLLBACK:\n                        self.db.rollback()\n                return result\n            except Exception as e:\n                logger.info(str(e))\n                raise e\n\n        return wrapped_f\n\n    return decorator\n\n\ndef default_session_factory() -> Session:\n    return Session(engine)\n\n\ndef managed_tx_function(\n    auto_commit: CommitMode = CommitMode.COMMIT,\n    num_retries=settings.DATABASE_MAX_TX_RETRY_COUNT,\n    session_factory: Callable[..., Session] = default_session_factory,\n):\n    \"\"\"Passes Session object as first argument to wrapped function.\"\"\"\n\n    def decorator(f):\n        @wraps(f)\n        def wrapped_f(*args, **kwargs):\n            try:\n                result = None\n                if auto_commit == CommitMode.COMMIT:\n                    retry_exhausted = True\n                    for i in range(num_retries):\n                        with session_factory() as session:\n                            try:\n                                result = f(session, *args, **kwargs)\n                                session.commit()\n                                if isinstance(result, SQLModel):\n                                    session.refresh(result)\n                                retry_exhausted = False\n                                break\n                            except PendingRollbackError as e:\n                                logger.info(str(e))\n                                session.rollback()\n                            except OperationalError as e:\n                                if e.orig is not None and isinstance(\n                                    e.orig,\n                                    (SerializationFailure, DeadlockDetected, UniqueViolation, ExclusionViolation),\n                                ):\n                                    logger.info(f\"{type(e.orig)} Inner {e.orig.pgcode} {type(e.orig.pgcode)}\")\n                                    session.rollback()\n                                else:\n                                    raise e\n                        logger.info(f\"Retry {i+1}/{num_retries}\")\n                    if retry_exhausted:\n                        raise OasstError(\n                            \"DATABASE_MAX_RETIRES_EXHAUSTED\",\n                            error_code=OasstErrorCode.DATABASE_MAX_RETRIES_EXHAUSTED,\n                            http_status_code=HTTPStatus.SERVICE_UNAVAILABLE,\n                        )\n                else:\n                    with session_factory() as session:\n                        result = f(session, *args, **kwargs)\n                    if auto_commit == CommitMode.FLUSH:\n                        session.flush()\n                        if isinstance(result, SQLModel):\n                            session.refresh(result)\n                    elif auto_commit == CommitMode.ROLLBACK:\n                        session.rollback()\n                return result\n            except Exception as e:\n                logger.info(str(e))\n                raise e\n\n        return wrapped_f\n\n    return decorator\n\n\ndef async_managed_tx_function(\n    auto_commit: CommitMode = CommitMode.COMMIT,\n    num_retries=settings.DATABASE_MAX_TX_RETRY_COUNT,\n    session_factory: Callable[..., Session] = default_session_factory,\n):\n    \"\"\"Passes Session object as first argument to wrapped function.\"\"\"\n\n    def decorator(f):\n        @wraps(f)\n        async def wrapped_f(*args, **kwargs):\n            try:\n                result = None\n                if auto_commit == CommitMode.COMMIT:\n                    retry_exhausted = True\n                    for i in range(num_retries):\n                        with session_factory() as session:\n                            try:\n                                result = await f(session, *args, **kwargs)\n                                session.commit()\n                                if isinstance(result, SQLModel):\n                                    session.refresh(result)\n                                retry_exhausted = False\n                                break\n                            except PendingRollbackError as e:\n                                logger.info(str(e))\n                                session.rollback()\n                            except OperationalError as e:\n                                if e.orig is not None and isinstance(\n                                    e.orig,\n                                    (SerializationFailure, DeadlockDetected, UniqueViolation, ExclusionViolation),\n                                ):\n                                    logger.info(f\"{type(e.orig)} Inner {e.orig.pgcode} {type(e.orig.pgcode)}\")\n                                    session.rollback()\n                                else:\n                                    raise e\n                        logger.info(f\"Retry {i+1}/{num_retries}\")\n                    if retry_exhausted:\n                        raise OasstError(\n                            \"DATABASE_MAX_RETIRES_EXHAUSTED\",\n                            error_code=OasstErrorCode.DATABASE_MAX_RETRIES_EXHAUSTED,\n                            http_status_code=HTTPStatus.SERVICE_UNAVAILABLE,\n                        )\n                else:\n                    with session_factory() as session:\n                        result = await f(session, *args, **kwargs)\n                    if auto_commit == CommitMode.FLUSH:\n                        session.flush()\n                        if isinstance(result, SQLModel):\n                            session.refresh(result)\n                    elif auto_commit == CommitMode.ROLLBACK:\n                        session.rollback()\n                return result\n            except Exception as e:\n                logger.info(str(e))\n                raise e\n\n        return wrapped_f\n\n    return decorator\n", "backend/oasst_backend/utils/ranking.py": "from typing import List\n\nimport numpy as np\n\n\ndef head_to_head_votes(ranks: List[List[int]]):\n    tallies = np.zeros((len(ranks[0]), len(ranks[0])))\n    names = sorted(ranks[0])\n    ranks = np.array(ranks)\n    # we want the sorted indices\n    ranks = np.argsort(ranks, axis=1)\n    for i in range(ranks.shape[1]):\n        for j in range(i + 1, ranks.shape[1]):\n            # now count the cases someone voted for i over j\n            over_j = np.sum(ranks[:, i] < ranks[:, j])\n            over_i = np.sum(ranks[:, j] < ranks[:, i])\n            tallies[i, j] = over_j\n            # tallies[i,j] = over_i\n            tallies[j, i] = over_i\n            # tallies[j,i] = over_j\n    return tallies, names\n\n\ndef cycle_detect(pairs):\n    \"\"\"Recursively detect cycles by removing condorcet losers until either only one pair is left or condorcet losers no longer exist\n    This method upholds the invariant that in a ranking for all a,b either a>b or b>a for all a,b.\n\n\n    Returns\n    -------\n    out : False if the pairs do not contain a cycle, True if the pairs contain a cycle\n\n\n    \"\"\"\n    # get all condorcet losers (pairs that loose to all other pairs)\n    # idea: filter all losers that are never winners\n    # print(\"pairs\", pairs)\n    if len(pairs) <= 1:\n        return False\n    losers = [c_lose for c_lose in np.unique(pairs[:, 1]) if c_lose not in pairs[:, 0]]\n    if len(losers) == 0:\n        # if we recursively removed pairs, and at some point we did not have\n        # a condorcet loser, that means everything is both a winner and loser,\n        # yielding at least one (winner,loser), (loser,winner) pair\n        return True\n\n    new = []\n    for p in pairs:\n        if p[1] not in losers:\n            new.append(p)\n    return cycle_detect(np.array(new))\n\n\ndef get_winner(pairs):\n    \"\"\"\n    This returns _one_ concordant winner.\n    It could be that there are multiple concordant winners, but in our case\n    since we are interested in a ranking, we have to choose one at random.\n    \"\"\"\n    losers = np.unique(pairs[:, 1]).astype(int)\n    winners = np.unique(pairs[:, 0]).astype(int)\n    for w in winners:\n        if w not in losers:\n            return w\n\n\ndef get_ranking(pairs):\n    \"\"\"\n    Abuses concordance property to get a (not necessarily unique) ranking.\n    The lack of uniqueness is due to the potential existence of multiple\n    equally ranked winners. We have to pick one, which is where\n    the non-uniqueness comes from\n    \"\"\"\n    if len(pairs) == 1:\n        return list(pairs[0])\n    w = get_winner(pairs)\n    # now remove the winner from the list of pairs\n    p_new = np.array([(a, b) for a, b in pairs if a != w])\n    return [w] + get_ranking(p_new)\n\n\ndef ranked_pairs(ranks: List[List[int]]):\n    \"\"\"\n    Expects a list of rankings for an item like:\n        [(\"w\",\"x\",\"z\",\"y\") for _ in range(3)]\n        + [(\"w\",\"y\",\"x\",\"z\") for _ in range(2)]\n        + [(\"x\",\"y\",\"z\",\"w\") for _ in range(4)]\n        + [(\"x\",\"z\",\"w\",\"y\") for _ in range(5)]\n        + [(\"y\",\"w\",\"x\",\"z\") for _ in range(1)]\n    This code is quite brain melting, but the idea is the following:\n    1. create a head-to-head matrix that tallies up all win-lose combinations of preferences\n    2. take all combinations that win more than they loose and sort those by how often they win\n    3. use that to create an (implicit) directed graph\n    4. recursively extract nodes from the graph that do not have incoming edges\n    5. said recursive list is the ranking\n    \"\"\"\n    tallies, names = head_to_head_votes(ranks)\n    tallies = tallies - tallies.T\n    # note: the resulting tally matrix should be skew-symmetric\n    # order by strength of victory (using tideman's original method, don't think it would make a difference for us)\n    sorted_majorities = []\n    for i in range(len(ranks[0])):\n        for j in range(len(ranks[0])):\n            # you can never prefer yourself over yourself\n            # we also have to pick one of the two choices,\n            # if the preference is exactly zero...\n            if tallies[i, j] >= 0 and i != j:\n                sorted_majorities.append((i, j, tallies[i, j]))\n    # we don't explicitly deal with tied majorities here\n    sorted_majorities = np.array(sorted(sorted_majorities, key=lambda x: x[2], reverse=True))\n    # now do lock ins\n    lock_ins = []\n    for x, y, _ in sorted_majorities:\n        # invariant: lock_ins has no cycles here\n        lock_ins.append((x, y))\n        # print(\"lock ins are now\",np.array(lock_ins))\n        if cycle_detect(np.array(lock_ins)):\n            # print(\"backup: cycle detected\")\n            # if there's a cycle, delete the new addition and continue\n            lock_ins = lock_ins[:-1]\n    # now simply return all winners in order, and attach the losers\n    # to the back. This is because the overall loser might not be unique\n    # and (by concordance property) may never exist in any winning set to begin with.\n    # (otherwise he would either not be the loser, or cycles exist!)\n    # Since there could be multiple overall losers, we just return them in any order\n    # as we are unable to find a closer ranking\n    numerical_ranks = np.array(get_ranking(np.array(lock_ins))).astype(int)\n    conversion = [names[n] for n in numerical_ranks]\n    return conversion\n\n\nif __name__ == \"__main__\":\n    ranks = \"\"\" (\n        [(\"w\", \"x\", \"z\", \"y\") for _ in range(1)]\n        + [(\"w\", \"y\", \"x\", \"z\") for _ in range(2)]\n        # + [(\"x\",\"y\",\"z\",\"w\") for _ in range(4)]\n        + [(\"x\", \"z\", \"w\", \"y\") for _ in range(5)]\n        + [(\"y\", \"w\", \"x\", \"z\") for _ in range(1)]\n        # [(\"y\",\"z\",\"w\",\"x\") for _ in range(1000)]\n    )\"\"\"\n    ranks = [\n        [\n            (\"c5181083-d3e9-41e7-a935-83fb9fa01488\"),\n            (\"dcf3d179-0f34-4c15-ae21-b8feb15e422d\"),\n            (\"d11705af-5575-43e5-b22e-08d155fbaa62\"),\n        ],\n        [\n            (\"d11705af-5575-43e5-b22e-08d155fbaa62\"),\n            (\"c5181083-d3e9-41e7-a935-83fb9fa01488\"),\n            (\"dcf3d179-0f34-4c15-ae21-b8feb15e422d\"),\n        ],\n        [\n            (\"dcf3d179-0f34-4c15-ae21-b8feb15e422d\"),\n            (\"c5181083-d3e9-41e7-a935-83fb9fa01488\"),\n            (\"d11705af-5575-43e5-b22e-08d155fbaa62\"),\n        ],\n        [\n            (\"d11705af-5575-43e5-b22e-08d155fbaa62\"),\n            (\"c5181083-d3e9-41e7-a935-83fb9fa01488\"),\n            (\"dcf3d179-0f34-4c15-ae21-b8feb15e422d\"),\n        ],\n    ]\n    rp = ranked_pairs(ranks)\n    print(rp)\n", "backend/oasst_backend/utils/exported_tree_loading.py": "import json\nfrom collections import defaultdict\nfrom typing import List\n\nimport pandas as pd\n\n\ndef load_jsonl(filepaths):\n    data = []\n    for filepath in filepaths:\n        with open(filepath, \"r\") as f:\n            for line in f:\n                data.append(json.loads(line))\n    return data\n\n\ndef separate_qa_helper(node, depth, msg_dict):\n    if \"text\" in node:\n        if node[\"role\"] == \"prompter\":\n            msg_dict[\"user_messages\"].append(str(node[\"text\"]))\n        elif node[\"role\"] == \"assistant\":\n            msg_dict[\"assistant_messages\"].append(str(node[\"text\"]))\n        depth += 1\n        if \"replies\" in node:\n            for reply in node[\"replies\"]:\n                separate_qa_helper(reply, depth, msg_dict)\n\n\ndef store_qa_data_separate(trees, data):\n    message_list = []\n    for i, msg_tree in enumerate(trees):\n        if \"prompt\" in msg_tree.keys():\n            separate_qa_helper(msg_tree[\"prompt\"], i, data)\n        elif \"prompt\" not in msg_tree.keys():\n            message_list.append(msg_tree)\n    return data, message_list\n\n\ndef group_qa_helper(node, depth, msg_pairs):\n    if \"text\" in node:\n        if node[\"role\"] == \"prompter\":\n            if \"replies\" in node:\n                for reply in node[\"replies\"]:\n                    qa_pair = {\"instruct\": str(node[\"text\"]), \"answer\": str(reply[\"text\"])}\n                    msg_pairs.append(qa_pair)\n        depth += 1\n        if \"replies\" in node:\n            for reply in node[\"replies\"]:\n                group_qa_helper(reply, depth, msg_pairs)\n\n\ndef store_qa_data_paired(trees, data: List):\n    message_list = []\n    for i, msg_tree in enumerate(trees):\n        if \"prompt\" in msg_tree.keys():\n            group_qa_helper(msg_tree[\"prompt\"], i, data)\n        elif \"prompt\" not in msg_tree.keys():\n            message_list.append(msg_tree)\n    return data, message_list\n\n\ndef load_data(filepaths: List[str], paired=False):\n    trees = load_jsonl(filepaths)\n    if paired:\n        data = []\n        data, message_list = store_qa_data_paired(trees, data)\n        sents = [f\"{qa['instruct']} {qa['answer']}\" for qa in data]\n    elif not paired:\n        data = defaultdict(list)\n        data, message_list = store_qa_data_separate(trees, data)\n        sents = data[\"user_messages\"] + data[\"assistant_messages\"]\n\n    data = [(i, sent) for i, sent in enumerate(sents)]\n    data = pd.DataFrame(data, columns=[\"id\", \"query\"])\n    return data, message_list\n", "backend/oasst_backend/utils/__init__.py": "", "backend/oasst_backend/utils/hugging_face.py": "from enum import Enum\nfrom typing import Any, Dict\n\nimport aiohttp\nfrom loguru import logger\nfrom oasst_backend.config import settings\nfrom oasst_shared.exceptions import OasstError, OasstErrorCode\n\n\nclass HfUrl(str, Enum):\n    HUGGINGFACE_TOXIC_CLASSIFICATION = \"https://api-inference.huggingface.co/models\"\n    HUGGINGFACE_FEATURE_EXTRACTION = \"https://api-inference.huggingface.co/pipeline/feature-extraction\"\n\n\nclass HfClassificationModel(str, Enum):\n    TOXIC_ROBERTA = \"unitary/multilingual-toxic-xlm-roberta\"\n\n\nclass HfEmbeddingModel(str, Enum):\n    MINILM = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n\n\nclass HuggingFaceAPI:\n    \"\"\"Class Object to make post calls to endpoints for inference in models hosted in HuggingFace\"\"\"\n\n    def __init__(\n        self,\n        api_url: str,\n    ):\n        # The API endpoint we want to access\n        self.api_url: str = api_url\n\n        # Access token for the api\n        self.api_key: str = settings.HUGGING_FACE_API_KEY\n\n        # Headers going to be used\n        self.headers: Dict[str, str] = {\"Authorization\": f\"Bearer {self.api_key}\"}\n\n    async def post(self, input: str, wait_for_model: bool = True) -> Any:\n        \"\"\"Post request to the endpoint to get an inference\n\n        Args:\n            input (str): the input that we will pass to the model\n\n        Raises:\n            OasstError: in the case we get a bad response\n\n        Returns:\n            inference: the inference we obtain from the model in HF\n        \"\"\"\n\n        async with aiohttp.ClientSession() as session:\n            payload: Dict[str, str] = {\"inputs\": input, \"wait_for_model\": wait_for_model}\n\n            async with session.post(self.api_url, headers=self.headers, json=payload) as response:\n                # If we get a bad response\n                if not response.ok:\n                    logger.error(response)\n                    logger.info(self.headers)\n                    raise OasstError(\n                        f\"Response Error HuggingFace API (Status: {response.status})\",\n                        error_code=OasstErrorCode.HUGGINGFACE_API_ERROR,\n                    )\n\n                # Get the response from the API call\n                inference = await response.json()\n\n        return inference\n", "backend/oasst_backend/utils/discord.py": "from uuid import UUID\n\nimport requests\nfrom loguru import logger\nfrom oasst_backend.celery_worker import app as celery_app\nfrom oasst_backend.config import settings\n\nROOT_ENDPOINT = \"https://discord.com/api/v10\"\n\n\n@celery_app.task(name=\"send_new_report_message\")\ndef send_new_report_message(message_details: dict, label_text: str, user_id: UUID):\n    \"\"\"\n    Send a message to the Discord channel when a new message is flagged.\n    Note: this is a Celery task.\n\n    Args:\n        message_details (dict): some of the attributes of a Message instance that we will use to compose the discord\n        message.\n        label_text (str): the label text\n        user_id (UUID): the user ID\n    \"\"\"\n    if settings.DISCORD_API_KEY is None or settings.DISCORD_CHANNEL_ID is None:\n        return\n\n    try:\n        logger.debug(\"Sending flagged message to Discord\")\n        label_text = label_text[:4096]  # 4096 is the max length of discord embed description\n        message_content_embed = {\n            \"title\": \"Message content\",\n            \"description\": message_details[\"message_text\"],\n            \"color\": 0x3498DB,  # Blue\n            \"footer\": {\n                \"text\": (\n                    f\"Role: {message_details['role']}\\t \"\n                    f\"Lang: {message_details['lang']}\\t \"\n                    f\"\ud83d\udc4d{message_details['thumbs_up']} \"\n                    f\"\ud83d\udc4e{message_details['thumbs_down']} \"\n                    f\"\ud83d\udea9{message_details['red_flag']}\"\n                )\n            },\n        }\n        label_text_embed = {\n            \"title\": \"Report content\",\n            \"description\": f\"{label_text}\",\n            \"color\": 0xE74C3C,  # Red\n            \"author\": {\n                \"name\": f\"User ID: {user_id}\",\n                \"url\": f\"https://open-assistant.io/admin/manage_user/{user_id}\",\n            },\n        }\n        res = requests.post(\n            f\"{ROOT_ENDPOINT}/channels/{settings.DISCORD_CHANNEL_ID}/messages\",\n            headers={\n                \"user-agent\": \"DiscordBot (https://open-assistant.io, 1)\",\n                \"authorization\": f\"Bot {settings.DISCORD_API_KEY}\",\n            },\n            json={\n                \"content\": f\"New flagged message https://open-assistant.io/admin/messages/{message_details['message_id']}\",\n                \"embeds\": [message_content_embed, label_text_embed],\n            },\n        )\n        res.raise_for_status()\n    except Exception as e:\n        logger.exception(f\"Failed to send flagged message. error: {e}\")\n", "backend/oasst_backend/utils/language_classification.py": "import os\nimport pickle\nfrom collections import Counter\n\nfrom sklearn import metrics\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import LinearSVC\n\n\ndef load_and_split(foldername, num_words):\n    ls = os.listdir(foldername)\n    X = []\n    Y = []\n    langmap = dict()\n    for idx, x in enumerate(ls):\n        print(\"loading language\", x)\n        with open(foldername + \"/\" + x, \"r\") as reader:\n            tmp = reader.read().split(\" \")\n            tmp = [\" \".join(tmp[i : i + num_words]) for i in range(0, 100_000, num_words)]\n            X.extend(tmp)\n            Y.extend([idx] * len(tmp))\n            langmap[idx] = x\n    x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.90)\n    return x_train, x_test, y_train, y_test, langmap\n\n\ndef build_and_train_pipeline(x_train, y_train):\n    vectorizer = TfidfVectorizer(ngram_range=(1, 2), analyzer=\"char\", use_idf=False)\n    clf = Pipeline(\n        [\n            (\"vec\", vectorizer),\n            # (\"nystrom\", Nystroem(n_components=1000,n_jobs=6)),\n            (\"clf\", LinearSVC(C=0.5)),\n            # (\"clf\",GaussianNB())\n            # (\"clf\", HistGradientBoostingClassifier())\n        ]\n    )\n    print(\"fitting model...\")\n    clf.fit(x_train, y_train)\n    return clf\n\n\ndef benchmark(clf, x_test, y_test, langmap):\n    print(\"benchmarking model...\")\n    y_pred = clf.predict(x_test)\n    names = list(langmap.values())\n    # print(y_test)\n    # print(langmap)\n    print(metrics.classification_report(y_test, y_pred, target_names=names))\n    cm = metrics.confusion_matrix(y_test, y_pred)\n    print(cm)\n\n\ndef main(foldername, modelname, num_words):\n    x_train, x_test, y_train, y_test, langmap = load_and_split(foldername=foldername, num_words=num_words)\n    clf = build_and_train_pipeline(x_train, y_train)\n    benchmark(clf, x_test, y_test, langmap)\n    save_model(clf, langmap, num_words, modelname)\n    model = load(modelname)\n    print(\n        \"running inference on long tests\",\n        inference_voter(\n            model,\n            \"\"\"\n    What language is this text written in? Nobody knows until you fill in at least ten words.\n    This test here is to check whether the moving window approach works,\n    so I still need to fill in a little more text.\n    \"\"\",\n        ),\n    )\n\n\ndef load(modelname):\n    with open(modelname, \"rb\") as writer:\n        data = pickle.load(writer)\n    return data\n\n\ndef save_model(model, idx_to_name, num_words, modelname):\n    out = {\n        \"model\": model,\n        \"idx_to_name\": idx_to_name,\n        \"num_words\": num_words,\n    }\n    with open(modelname, \"wb\") as writer:\n        pickle.dump(out, writer)\n\n\ndef inference_voter(model, text):\n    tmp = text.split()\n    # print(len(tmp), tmp)\n    tmp = [\" \".join(tmp[i : i + model[\"num_words\"]]) for i in range(0, len(tmp) - model[\"num_words\"])]\n    predictions = model[\"model\"].predict(tmp)\n    # print(\"integer predictions\", predictions)\n    # print(\"name predictions\", *[model[\"idx_to_name\"][n] for n in predictions])\n    result = Counter(predictions).most_common(1)[0][0]\n    return model[\"idx_to_name\"][result]\n\n\nif __name__ == \"__main__\":\n    import argparse\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"-m\", \"--model\", help=\"save location for model and metadata\")\n    parser.add_argument(\"-d\", \"--data\", help=\"specify the folder for data files\")\n    parser.add_argument(\"-n\", \"--num_words\", help=\"number of words to use for statistics\", type=int)\n    args = parser.parse_args()\n    # np.set_printoptions(threshold=np.inf)\n    main(args.data, args.model, args.num_words)\n", "backend/oasst_backend/schemas/message_tree.py": "from uuid import UUID\n\nfrom oasst_backend.models.message_tree_state import State as TreeState\nfrom pydantic import BaseModel\n\n\nclass MessageTreeStateResponse(BaseModel):\n    message_tree_id: UUID\n    state: TreeState\n    goal_tree_size: int\n    max_depth: int\n    max_children_count: int\n    active: bool\n    origin: str | None\n", "backend/oasst_backend/schemas/text_labels.py": "from oasst_shared.schemas.protocol import LabelDescription\nfrom pydantic import BaseModel\n\n\nclass ValidLabelsResponse(BaseModel):\n    valid_labels: list[LabelDescription]\n", "backend/oasst_backend/schemas/__init__.py": "", "backend/oasst_backend/schemas/hugging_face.py": "from pydantic import BaseModel\n\n\nclass ToxicityClassification(BaseModel):\n    label: str\n    score: float\n", "backend/oasst_backend/api/deps.py": "from http import HTTPStatus\nfrom secrets import token_hex\nfrom typing import Generator, NamedTuple, Optional\nfrom uuid import UUID\n\nfrom fastapi import Depends, Request, Response, Security\nfrom fastapi.security import HTTPAuthorizationCredentials, HTTPBearer\nfrom fastapi.security.api_key import APIKey, APIKeyHeader, APIKeyQuery\nfrom fastapi_limiter.depends import RateLimiter\nfrom loguru import logger\nfrom oasst_backend.config import settings\nfrom oasst_backend.database import engine\nfrom oasst_backend.models import ApiClient\nfrom oasst_shared.exceptions import OasstError, OasstErrorCode\nfrom sqlmodel import Session\n\n\ndef get_db() -> Generator:\n    with Session(engine) as db:\n        yield db\n\n\napi_key_query = APIKeyQuery(name=\"api_key\", scheme_name=\"api-key\", auto_error=False)\napi_key_header = APIKeyHeader(name=\"X-API-Key\", scheme_name=\"api-key\", auto_error=False)\noasst_user_query = APIKeyQuery(name=\"oasst_user\", scheme_name=\"oasst-user\", auto_error=False)\noasst_user_header = APIKeyHeader(name=\"x-oasst-user\", scheme_name=\"oasst-user\", auto_error=False)\n\nbearer_token = HTTPBearer(auto_error=False)\n\n\ndef get_api_key(\n    api_key_query: str = Security(api_key_query),\n    api_key_header: str = Security(api_key_header),\n) -> str:\n    if api_key_query:\n        return api_key_query\n    else:\n        return api_key_header\n\n\nclass FrontendUserId(NamedTuple):\n    auth_method: str\n    username: str\n\n\ndef get_frontend_user_id(\n    user_query: str = Security(oasst_user_query),\n    user_header: str = Security(oasst_user_header),\n) -> FrontendUserId:\n    def split_user(v: str) -> tuple[str, str]:\n        if type(v) is str:\n            v = v.split(\":\", maxsplit=1)\n            if len(v) == 2:\n                return FrontendUserId(auth_method=v[0], username=v[1])\n        return FrontendUserId(auth_method=None, username=None)\n\n    if user_query:\n        return split_user(user_query)\n    else:\n        return split_user(user_header)\n\n\ndef create_api_client(\n    *,\n    session: Session,\n    description: str,\n    frontend_type: str,\n    trusted: bool | None = False,\n    admin_email: str | None = None,\n    api_key: str | None = None,\n    force_id: Optional[UUID] = None,\n) -> ApiClient:\n    if api_key is None:\n        api_key = token_hex(32)\n\n    logger.info(f\"Creating new api client with {api_key=}\")\n    api_client = ApiClient(\n        api_key=api_key,\n        description=description,\n        frontend_type=frontend_type,\n        trusted=trusted,\n        admin_email=admin_email,\n    )\n    if force_id:\n        api_client.id = force_id\n    session.add(api_client)\n    session.commit()\n    session.refresh(api_client)\n    return api_client\n\n\ndef api_auth(\n    api_key: APIKey,\n    db: Session,\n) -> ApiClient:\n    if api_key:\n        api_client = db.query(ApiClient).filter(ApiClient.api_key == api_key).first()\n        if api_client is not None and api_client.enabled:\n            return api_client\n\n    raise OasstError(\n        \"Could not validate credentials\",\n        error_code=OasstErrorCode.API_CLIENT_NOT_AUTHORIZED,\n        http_status_code=HTTPStatus.FORBIDDEN,\n    )\n\n\ndef get_api_client(\n    api_key: APIKey = Depends(get_api_key),\n    db: Session = Depends(get_db),\n):\n    return api_auth(api_key, db)\n\n\ndef get_trusted_api_client(\n    api_key: APIKey = Depends(get_api_key),\n    db: Session = Depends(get_db),\n):\n    client = api_auth(api_key, db)\n    if not client.trusted:\n        raise OasstError(\n            \"Forbidden\",\n            error_code=OasstErrorCode.API_CLIENT_NOT_AUTHORIZED,\n            http_status_code=HTTPStatus.FORBIDDEN,\n        )\n    return client\n\n\ndef get_root_token(bearer_token: HTTPAuthorizationCredentials = Security(bearer_token)) -> str:\n    if bearer_token:\n        token = bearer_token.credentials\n        if token and token in settings.ROOT_TOKENS:\n            return token\n    raise OasstError(\n        \"Could not validate credentials\",\n        error_code=OasstErrorCode.ROOT_TOKEN_NOT_AUTHORIZED,\n        http_status_code=HTTPStatus.FORBIDDEN,\n    )\n\n\nasync def user_identifier(request: Request) -> str:\n    \"\"\"Identify a request by user based on api_key and user header\"\"\"\n    api_key = request.headers.get(\"X-API-Key\") or request.query_params.get(\"api_key\")\n    user = request.headers.get(\"x-oasst-user\")\n    if not user:\n        payload = await request.json()\n        auth_method = payload.get(\"user\").get(\"auth_method\")\n        user_id = payload.get(\"user\").get(\"id\")\n        user = f\"{auth_method}:{user_id}\"\n    return f\"{api_key}:{user}\"\n\n\nclass UserRateLimiter(RateLimiter):\n    def __init__(\n        self, times: int = 100, milliseconds: int = 0, seconds: int = 0, minutes: int = 1, hours: int = 0\n    ) -> None:\n        super().__init__(times, milliseconds, seconds, minutes, hours, user_identifier)\n\n    async def __call__(self, request: Request, response: Response, api_key: str = Depends(get_api_key)) -> None:\n        # Skip if rate limiting is disabled\n        if not settings.RATE_LIMIT:\n            return\n\n        # Attempt to retrieve api_key and user information\n        user = (await request.json()).get(\"user\")\n\n        # Skip when api_key and user information are not available\n        # (such that it will be handled by `APIClientRateLimiter`)\n        if not api_key or not user or not user.get(\"id\"):\n            return\n\n        return await super().__call__(request, response)\n\n\nclass UserTaskTypeRateLimiter(RateLimiter):\n    \"\"\"\n    User-level rate limiter for a specific task type.\n    \"\"\"\n\n    def __init__(\n        self,\n        task_types: list[str],\n        times: int = 100,\n        milliseconds: int = 0,\n        seconds: int = 0,\n        minutes: int = 1,\n        hours: int = 0,\n    ) -> None:\n        super().__init__(times, milliseconds, seconds, minutes, hours, user_identifier)\n        self.task_types = task_types\n\n    async def __call__(self, request: Request, response: Response, api_key: str = Depends(get_api_key)) -> None:\n        # Skip if rate limiting is disabled\n        if not settings.RATE_LIMIT:\n            return\n\n        # Attempt to retrieve api_key and user information\n        json = await request.json()\n        user = json.get(\"user\")\n\n        # Skip when api_key and user information are not available\n        # (such that it will be handled by `APIClientRateLimiter`)\n        if not api_key or not user or not user.get(\"id\"):\n            return\n\n        # Skip when the request is not in our task types of interest\n        if not json.get(\"type\") or json.get(\"type\") not in self.task_types:\n            return\n\n        return await super().__call__(request, response)\n\n\nclass APIClientRateLimiter(RateLimiter):\n    def __init__(\n        self, times: int = 10_000, milliseconds: int = 0, seconds: int = 0, minutes: int = 1, hours: int = 0\n    ) -> None:\n        async def identifier(request: Request) -> str:\n            \"\"\"Identify a request based on api_key and user.id\"\"\"\n            api_key = request.headers.get(\"X-API-Key\") or request.query_params.get(\"api_key\")\n            return f\"{api_key}\"\n\n        super().__init__(times, milliseconds, seconds, minutes, hours, identifier)\n\n    async def __call__(self, request: Request, response: Response, api_key: str = Depends(get_api_key)) -> None:\n        # Skip if rate limiting is disabled\n        if not settings.RATE_LIMIT:\n            return\n\n        # Attempt to retrieve api_key and user information\n        user = (await request.json()).get(\"user\")\n\n        # Skip if user information is available\n        # (such that it will be handled by `UserRateLimiter`)\n        if not api_key or user:\n            return\n\n        return await super().__call__(request, response)\n", "backend/oasst_backend/api/__init__.py": "", "backend/oasst_backend/api/v1/trollboards.py": "from typing import Optional\n\nfrom fastapi import APIRouter, Depends, Query\nfrom oasst_backend.api import deps\nfrom oasst_backend.models import ApiClient\nfrom oasst_backend.user_stats_repository import UserStatsRepository, UserStatsTimeFrame\nfrom oasst_shared.schemas.protocol import TrollboardStats\nfrom sqlmodel import Session\n\nrouter = APIRouter()\n\n\n@router.get(\"/{time_frame}\", response_model=TrollboardStats)\ndef get_trollboard(\n    time_frame: UserStatsTimeFrame,\n    max_count: Optional[int] = Query(100, gt=0, le=10000),\n    enabled: Optional[bool] = None,\n    api_client: ApiClient = Depends(deps.get_trusted_api_client),\n    db: Session = Depends(deps.get_db),\n) -> TrollboardStats:\n    usr = UserStatsRepository(db)\n    return usr.get_trollboard(time_frame, limit=max_count, enabled=enabled)\n", "backend/oasst_backend/api/v1/frontend_messages.py": "from typing import Optional\n\nfrom fastapi import APIRouter, Depends\nfrom oasst_backend.api import deps\nfrom oasst_backend.api.v1 import utils\nfrom oasst_backend.models import ApiClient\nfrom oasst_backend.prompt_repository import PromptRepository\nfrom oasst_shared.schemas import protocol\nfrom sqlmodel import Session\n\nrouter = APIRouter()\n\n\n@router.get(\"/{message_id}\", response_model=protocol.Message)\ndef get_message_by_frontend_id(\n    message_id: str, api_client: ApiClient = Depends(deps.get_api_client), db: Session = Depends(deps.get_db)\n):\n    \"\"\"\n    Get a message by its frontend ID.\n    \"\"\"\n    pr = PromptRepository(db, api_client)\n    message = pr.fetch_message_by_frontend_message_id(message_id)\n    return utils.prepare_message(message)\n\n\n@router.get(\"/{message_id}/conversation\", response_model=protocol.Conversation)\ndef get_conv_by_frontend_id(\n    message_id: str, api_client: ApiClient = Depends(deps.get_api_client), db: Session = Depends(deps.get_db)\n):\n    \"\"\"\n    Get a conversation from the tree root and up to the message with given frontend ID.\n    \"\"\"\n\n    pr = PromptRepository(db, api_client)\n    message = pr.fetch_message_by_frontend_message_id(message_id)\n    messages = pr.fetch_message_conversation(message)\n    return utils.prepare_conversation(messages)\n\n\n@router.get(\"/{message_id}/tree\", response_model=protocol.MessageTree)\ndef get_tree_by_frontend_id(\n    message_id: str,\n    include_spam: Optional[bool] = True,\n    include_deleted: Optional[bool] = False,\n    api_client: ApiClient = Depends(deps.get_api_client),\n    db: Session = Depends(deps.get_db),\n):\n    \"\"\"\n    Get all messages belonging to the same message tree.\n    Message is identified by its frontend ID.\n    \"\"\"\n    pr = PromptRepository(db, api_client)\n    message = pr.fetch_message_by_frontend_message_id(message_id)\n    review_result = None if include_spam else True\n    deleted = None if include_deleted else False\n    tree = pr.fetch_message_tree(message.message_tree_id, review_result=review_result, deleted=deleted)\n    return utils.prepare_tree(tree, message.message_tree_id)\n\n\n@router.get(\"/{message_id}/children\", response_model=list[protocol.Message])\ndef get_children_by_frontend_id(\n    message_id: str, api_client: ApiClient = Depends(deps.get_api_client), db: Session = Depends(deps.get_db)\n):\n    \"\"\"\n    Get all messages belonging to the same message tree.\n    \"\"\"\n    pr = PromptRepository(db, api_client)\n    message = pr.fetch_message_by_frontend_message_id(message_id)\n    messages = pr.fetch_message_children(message.id, review_result=None)\n    return utils.prepare_message_list(messages)\n\n\n@router.get(\"/{message_id}/descendants\", response_model=protocol.MessageTree)\ndef get_descendants_by_frontend_id(\n    message_id: str, api_client: ApiClient = Depends(deps.get_api_client), db: Session = Depends(deps.get_db)\n):\n    \"\"\"\n    Get a subtree which starts with this message.\n    The message is identified by its frontend ID.\n    \"\"\"\n    pr = PromptRepository(db, api_client)\n    message = pr.fetch_message_by_frontend_message_id(message_id)\n    descendants = pr.fetch_message_descendants(message)\n    return utils.prepare_tree(descendants, message.id)\n\n\n@router.get(\"/{message_id}/longest_conversation_in_tree\", response_model=protocol.Conversation)\ndef get_longest_conv_by_frontend_id(\n    message_id: str, api_client: ApiClient = Depends(deps.get_api_client), db: Session = Depends(deps.get_db)\n):\n    \"\"\"\n    Get the longest conversation from the tree of the message.\n    The message is identified by its frontend ID.\n    \"\"\"\n    pr = PromptRepository(db, api_client)\n    message = pr.fetch_message_by_frontend_message_id(message_id)\n    conv = pr.fetch_longest_conversation(message.message_tree_id)\n    return utils.prepare_conversation(conv)\n\n\n@router.get(\"/{message_id}/max_children_in_tree\", response_model=protocol.MessageTree)\ndef get_max_children_by_frontend_id(\n    message_id: str, api_client: ApiClient = Depends(deps.get_api_client), db: Session = Depends(deps.get_db)\n):\n    \"\"\"\n    Get message with the most children from the tree of the provided message.\n    The message is identified by its frontend ID.\n    \"\"\"\n    pr = PromptRepository(db, api_client)\n    message = pr.fetch_message_by_frontend_message_id(message_id)\n    message, children = pr.fetch_message_with_max_children(message.message_tree_id)\n    return utils.prepare_tree([message, *children], message.id)\n", "backend/oasst_backend/api/v1/auth.py": "from typing import Union\n\nfrom cryptography.hazmat.primitives import hashes\nfrom cryptography.hazmat.primitives.kdf.hkdf import HKDF\nfrom fastapi import APIRouter, Depends, Security\nfrom fastapi.security import APIKeyCookie\nfrom jose import jwe\nfrom oasst_backend.config import settings\nfrom pydantic import BaseModel, EmailStr\n\nrouter = APIRouter()\n\noauth2_scheme = APIKeyCookie(name=settings.AUTH_COOKIE_NAME)\n\n\nclass TokenData(BaseModel):\n    \"\"\"\n    A minimal re-creation of the web's token type.  To be expanded later.\n    \"\"\"\n\n    email: Union[EmailStr, None] = None\n\n\nasync def get_current_user(token: str = Security(oauth2_scheme)):\n    \"\"\"\n    Decrypts the user's JSON Web Token using HKDF encryption and returns the\n    TokenData.\n    \"\"\"\n    # We first generate a key from the auth secret.\n    hkdf = HKDF(\n        algorithm=hashes.SHA256(),\n        length=settings.AUTH_LENGTH,\n        salt=settings.AUTH_SALT,\n        info=settings.AUTH_INFO,\n    )\n    key = hkdf.derive(settings.AUTH_SECRET)\n    # Next we decrypt the JWE token.\n    payload = jwe.decrypt(token, key)\n    # Finally we have the real token JSON payload and can do whatever we want.\n    return TokenData.parse_raw(payload)\n\n\n@router.get(\"/check\", response_model=str)\nasync def auth_check(token_data: TokenData = Depends(get_current_user)):\n    \"\"\"Returns the user's email if it can be decrypted.\"\"\"\n    return token_data.email\n", "backend/oasst_backend/api/v1/admin.py": "from datetime import datetime\nfrom typing import Optional\nfrom uuid import UUID\n\nimport pydantic\nfrom fastapi import APIRouter, Depends, Query\nfrom loguru import logger\nfrom oasst_backend.api import deps\nfrom oasst_backend.config import Settings, settings\nfrom oasst_backend.models import ApiClient, User\nfrom oasst_backend.prompt_repository import PromptRepository, UserRepository\nfrom oasst_backend.tree_manager import TreeManager\nfrom oasst_backend.utils.database_utils import CommitMode, managed_tx_function\nfrom oasst_shared import utils\nfrom oasst_shared.exceptions.oasst_api_error import OasstError, OasstErrorCode\nfrom oasst_shared.schemas.protocol import PageResult, SystemStats\nfrom oasst_shared.utils import ScopeTimer, log_timing, unaware_to_utc\nfrom starlette.status import HTTP_204_NO_CONTENT\n\nrouter = APIRouter()\n\n\nclass CreateApiClientRequest(pydantic.BaseModel):\n    description: str\n    frontend_type: str\n    trusted: bool | None = False\n    admin_email: str | None = None\n\n\n@router.post(\"/api_client\", response_model=str)\nasync def create_api_client(\n    request: CreateApiClientRequest,\n    root_token: str = Depends(deps.get_root_token),\n    session: deps.Session = Depends(deps.get_db),\n) -> str:\n    logger.info(f\"Creating new api client with {request=}\")\n    api_client = deps.create_api_client(\n        session=session,\n        description=request.description,\n        frontend_type=request.frontend_type,\n        trusted=request.trusted,\n        admin_email=request.admin_email,\n    )\n    logger.info(f\"Created api_client with key {api_client.api_key}\")\n    return api_client.api_key\n\n\n@router.get(\"/backend_settings/full\", response_model=Settings)\nasync def get_backend_settings_full(api_client: ApiClient = Depends(deps.get_trusted_api_client)) -> Settings:\n    logger.info(\n        f\"Backend settings requested by trusted api_client {api_client.id} (admin_email: {api_client.admin_email}, frontend_type: {api_client.frontend_type})\"\n    )\n    return settings\n\n\nclass PublicSettings(pydantic.BaseModel):\n    \"\"\"Subset of backend settings which can be retrieved by untrusted API clients.\"\"\"\n\n    PROJECT_NAME: str\n    API_V1_STR: str\n    MESSAGE_SIZE_LIMIT: int\n    DEBUG_USE_SEED_DATA: bool\n    DEBUG_ALLOW_SELF_LABELING: bool\n    DEBUG_SKIP_EMBEDDING_COMPUTATION: bool\n    DEBUG_SKIP_TOXICITY_CALCULATION: bool\n    DEBUG_DATABASE_ECHO: bool\n    USER_STATS_INTERVAL_DAY: int\n    USER_STATS_INTERVAL_WEEK: int\n    USER_STATS_INTERVAL_MONTH: int\n    USER_STATS_INTERVAL_TOTAL: int\n\n\n@router.get(\"/backend_settings/public\", response_model=PublicSettings)\nasync def get_backend_settings_public(api_client: ApiClient = Depends(deps.get_api_client)) -> PublicSettings:\n    return PublicSettings(**settings.dict())\n\n\nclass PurgeResultModel(pydantic.BaseModel):\n    before: SystemStats\n    after: SystemStats\n    preview: bool\n    duration: float\n\n\n@router.post(\"/purge_user/{user_id}\", response_model=PurgeResultModel)\nasync def purge_user(\n    user_id: UUID,\n    preview: bool = True,\n    ban: bool = True,\n    api_client: ApiClient = Depends(deps.get_trusted_api_client),\n) -> str:\n    assert api_client.trusted\n\n    @managed_tx_function(CommitMode.ROLLBACK if preview else CommitMode.COMMIT)\n    def purge_tx(session: deps.Session) -> tuple[User, SystemStats, SystemStats]:\n        pr = PromptRepository(session, api_client)\n\n        stats_before = pr.get_stats()\n\n        user = pr.user_repository.get_user(user_id)\n        tm = TreeManager(session, pr)\n        tm.purge_user(user_id=user_id, ban=ban)\n\n        session.expunge(user)\n        return user, stats_before, pr.get_stats()\n\n    timer = ScopeTimer()\n    user, before, after = purge_tx()\n    timer.stop()\n\n    if preview:\n        logger.info(\n            f\"PURGE USER PREVIEW: '{user.display_name}' (id: {str(user_id)}; username: '{user.username}'; auth-method: '{user.auth_method}')\"\n        )\n    else:\n        logger.warning(\n            f\"PURGE USER: '{user.display_name}' (id: {str(user_id)}; username: '{user.username}'; auth-method: '{user.auth_method}')\"\n        )\n\n    logger.info(f\"{before=}; {after=}\")\n    return PurgeResultModel(before=before, after=after, preview=preview, duration=timer.elapsed)\n\n\n@router.post(\"/purge_user/{user_id}/messages\", response_model=PurgeResultModel)\nasync def purge_user_messages(\n    user_id: UUID,\n    purge_initial_prompts: bool = False,\n    min_date: datetime = None,\n    max_date: datetime = None,\n    preview: bool = True,\n    api_client: ApiClient = Depends(deps.get_trusted_api_client),\n) -> str:\n    assert api_client.trusted\n\n    min_date = unaware_to_utc(min_date)\n    max_date = unaware_to_utc(max_date)\n\n    @managed_tx_function(CommitMode.ROLLBACK if preview else CommitMode.COMMIT)\n    def purge_user_messages_tx(session: deps.Session):\n        pr = PromptRepository(session, api_client)\n\n        stats_before = pr.get_stats()\n\n        user = pr.user_repository.get_user(user_id)\n\n        tm = TreeManager(session, pr)\n        tm.purge_user_messages(\n            user_id, purge_initial_prompts=purge_initial_prompts, min_date=min_date, max_date=max_date\n        )\n\n        session.expunge(user)\n        return user, stats_before, pr.get_stats()\n\n    timer = ScopeTimer()\n    user, before, after = purge_user_messages_tx()\n    timer.stop()\n\n    if preview:\n        logger.info(\n            f\"PURGE USER MESSAGES PREVIEW: '{user.display_name}' (id: {str(user_id)}; username: '{user.username}'; auth-method: '{user.auth_method}')\"\n        )\n    else:\n        logger.warning(\n            f\"PURGE USER MESSAGES: '{user.display_name}' (id: {str(user_id)}; username: '{user.username}'; auth-method: '{user.auth_method}')\"\n        )\n\n    logger.info(f\"{before=}; {after=}\")\n    return PurgeResultModel(before=before, after=after, preview=preview, duration=timer.elapsed)\n\n\nclass FlaggedMessageResponse(pydantic.BaseModel):\n    message_id: UUID\n    processed: bool\n    created_date: Optional[datetime]\n\n\nclass FlaggedMessagePage(PageResult):\n    items: list[FlaggedMessageResponse]\n\n\n@router.get(\"/flagged_messages/cursor\", response_model=FlaggedMessagePage)\ndef get_flagged_messages_cursor(\n    *,\n    before: Optional[str] = None,\n    after: Optional[str] = None,\n    max_count: Optional[int] = Query(10, gt=0, le=1000),\n    desc: Optional[bool] = False,\n    session: deps.Session = Depends(deps.get_db),\n    api_client: ApiClient = Depends(deps.get_trusted_api_client),\n) -> str:\n    assert api_client.trusted\n    assert max_count is not None\n\n    def split_cursor(x: str | None) -> tuple[datetime, UUID]:\n        if not x:\n            return None, None\n        try:\n            m = utils.split_uuid_pattern.match(x)\n            if m:\n                return datetime.fromisoformat(m[2]), UUID(m[1])\n            return datetime.fromisoformat(x), None\n        except ValueError:\n            raise OasstError(\"Invalid cursor value\", OasstErrorCode.INVALID_CURSOR_VALUE)\n\n    if desc:\n        gte_created_date, gt_id = split_cursor(before)\n        lte_created_date, lt_id = split_cursor(after)\n        query_desc = not (before is not None and not after)\n    else:\n        lte_created_date, lt_id = split_cursor(before)\n        gte_created_date, gt_id = split_cursor(after)\n        query_desc = before is not None and not after\n\n    logger.debug(f\"{desc=} {query_desc=} {gte_created_date=} {lte_created_date=}\")\n\n    qry_max_count = max_count + 1 if before is None or after is None else max_count\n\n    pr = PromptRepository(session, api_client)\n    items = pr.fetch_flagged_messages_by_created_date(\n        gte_created_date=gte_created_date,\n        gt_id=gt_id,\n        lte_created_date=lte_created_date,\n        lt_id=lt_id,\n        desc=query_desc,\n        limit=qry_max_count,\n    )\n\n    num_rows = len(items)\n    if qry_max_count > max_count and num_rows == qry_max_count:\n        assert not (before and after)\n        items = items[:-1]\n\n    if desc != query_desc:\n        items.reverse()\n\n    n, p = None, None\n    if len(items) > 0:\n        if (num_rows > max_count and before) or after:\n            p = str(items[0].message_id) + \"$\" + items[0].created_date.isoformat()\n        if num_rows > max_count or before:\n            n = str(items[-1].message_id) + \"$\" + items[-1].created_date.isoformat()\n    else:\n        if after:\n            p = lte_created_date.isoformat() if desc else gte_created_date.isoformat()\n        if before:\n            n = gte_created_date.isoformat() if desc else lte_created_date.isoformat()\n\n    order = \"desc\" if desc else \"asc\"\n    print(p, n, items, order)\n    return FlaggedMessagePage(prev=p, next=n, sort_key=\"created_date\", order=order, items=items)\n\n\n@router.get(\"/flagged_messages\", response_model=list[FlaggedMessageResponse])\nasync def get_flagged_messages(\n    max_count: Optional[int],\n    session: deps.Session = Depends(deps.get_db),\n    api_client: ApiClient = Depends(deps.get_trusted_api_client),\n) -> str:\n    assert api_client.trusted\n\n    pr = PromptRepository(session, api_client)\n    flagged_messages = pr.fetch_flagged_messages(max_count=max_count)\n    resp = [FlaggedMessageResponse(**msg.__dict__) for msg in flagged_messages]\n    return resp\n\n\n@router.post(\"/flagged_messages/{message_id}/processed\", response_model=FlaggedMessageResponse)\nasync def process_flagged_messages(\n    message_id: UUID,\n    session: deps.Session = Depends(deps.get_db),\n    api_client: ApiClient = Depends(deps.get_trusted_api_client),\n) -> str:\n    assert api_client.trusted\n\n    pr = PromptRepository(session, api_client)\n    flagged_msg = pr.process_flagged_message(message_id=message_id)\n    resp = FlaggedMessageResponse(**flagged_msg.__dict__)\n    return resp\n\n\nclass MergeUsersRequest(pydantic.BaseModel):\n    destination_user_id: UUID\n    source_user_ids: list[UUID]\n\n\n@log_timing(level=\"INFO\")\n@router.post(\"/merge_users\", response_model=None, status_code=HTTP_204_NO_CONTENT)\ndef merge_users(\n    request: MergeUsersRequest,\n    api_client: ApiClient = Depends(deps.get_trusted_api_client),\n) -> None:\n    @managed_tx_function(CommitMode.COMMIT)\n    def merge_users_tx(session: deps.Session):\n        ur = UserRepository(session, api_client)\n        ur.merge_users(destination_user_id=request.destination_user_id, source_user_ids=request.source_user_ids)\n\n    merge_users_tx()\n\n    logger.info(f\"Merged users: {request=}\")\n", "backend/oasst_backend/api/v1/api.py": "from fastapi import APIRouter\nfrom oasst_backend.api.v1 import (\n    admin,\n    auth,\n    frontend_messages,\n    frontend_users,\n    hugging_face,\n    leaderboards,\n    messages,\n    stats,\n    tasks,\n    text_labels,\n    trollboards,\n    users,\n)\n\napi_router = APIRouter()\napi_router.include_router(tasks.router, prefix=\"/tasks\", tags=[\"tasks\"])\napi_router.include_router(text_labels.router, prefix=\"/text_labels\", tags=[\"text_labels\"])\napi_router.include_router(messages.router, prefix=\"/messages\", tags=[\"messages\"])\napi_router.include_router(frontend_messages.router, prefix=\"/frontend_messages\", tags=[\"frontend_messages\"])\napi_router.include_router(users.router, prefix=\"/users\", tags=[\"users\"])\napi_router.include_router(frontend_users.router, prefix=\"/frontend_users\", tags=[\"frontend_users\"])\napi_router.include_router(stats.router, prefix=\"/stats\", tags=[\"stats\"])\napi_router.include_router(leaderboards.router, prefix=\"/leaderboards\", tags=[\"leaderboards\"])\napi_router.include_router(trollboards.router, prefix=\"/trollboards\", tags=[\"trollboards\"])\napi_router.include_router(hugging_face.router, prefix=\"/hf\", tags=[\"hugging_face\"])\napi_router.include_router(admin.router, prefix=\"/admin\", tags=[\"admin\"])\napi_router.include_router(auth.router, prefix=\"/auth\", tags=[\"auth\"])\n", "backend/oasst_backend/api/v1/frontend_users.py": "import datetime\nfrom typing import Optional\nfrom uuid import UUID\n\nfrom fastapi import APIRouter, Depends, Query\nfrom oasst_backend.api import deps\nfrom oasst_backend.api.v1 import utils\nfrom oasst_backend.api.v1.messages import get_messages_cursor\nfrom oasst_backend.models import ApiClient\nfrom oasst_backend.prompt_repository import PromptRepository\nfrom oasst_backend.user_repository import UserRepository\nfrom oasst_shared.schemas import protocol\nfrom sqlmodel import Session\nfrom starlette.status import HTTP_204_NO_CONTENT\n\nrouter = APIRouter()\n\n\n@router.get(\"/\", response_model=list[protocol.FrontEndUser], deprecated=True)\ndef get_users_ordered_by_username(\n    api_client_id: Optional[UUID] = None,\n    gte_username: Optional[str] = None,\n    gt_id: Optional[UUID] = None,\n    lte_username: Optional[str] = None,\n    lt_id: Optional[UUID] = None,\n    search_text: Optional[str] = None,\n    auth_method: Optional[str] = None,\n    max_count: Optional[int] = Query(100, gt=0, le=10000),\n    api_client: ApiClient = Depends(deps.get_api_client),\n    db: Session = Depends(deps.get_db),\n):\n    ur = UserRepository(db, api_client)\n    users = ur.query_users_ordered_by_username(\n        api_client_id=api_client_id,\n        gte_username=gte_username,\n        gt_id=gt_id,\n        lte_username=lte_username,\n        lt_id=lt_id,\n        auth_method=auth_method,\n        search_text=search_text,\n        limit=max_count,\n    )\n    return [u.to_protocol_frontend_user() for u in users]\n\n\n@router.get(\"/{auth_method}/{username}\", response_model=protocol.FrontEndUser)\ndef query_frontend_user(\n    auth_method: str,\n    username: str,\n    api_client_id: Optional[UUID] = None,\n    api_client: ApiClient = Depends(deps.get_api_client),\n    db: Session = Depends(deps.get_db),\n):\n    \"\"\"\n    Query frontend user.\n    \"\"\"\n    ur = UserRepository(db, api_client)\n    user = ur.query_frontend_user(auth_method, username, api_client_id)\n    return user.to_protocol_frontend_user()\n\n\n@router.post(\"/\", response_model=protocol.FrontEndUser)\ndef create_frontend_user(\n    *,\n    create_user: protocol.CreateFrontendUserRequest,\n    api_client: ApiClient = Depends(deps.get_api_client),\n    db: Session = Depends(deps.get_db),\n):\n    ur = UserRepository(db, api_client)\n    user = ur.lookup_client_user(create_user, create_missing=True)\n\n    def changed(a, b) -> bool:\n        return a is not None and a != b\n\n    # only call update_user if something changed\n    if (\n        changed(create_user.enabled, user.enabled)\n        or changed(create_user.show_on_leaderboard, user.show_on_leaderboard)\n        or changed(create_user.notes, user.notes)\n        or (create_user.tos_acceptance and user.tos_acceptance_date is None)\n    ):\n        user = ur.update_user(\n            user.id,\n            enabled=create_user.enabled,\n            show_on_leaderboard=create_user.show_on_leaderboard,\n            tos_acceptance=create_user.tos_acceptance,\n            notes=create_user.notes,\n        )\n\n    return user.to_protocol_frontend_user()\n\n\n@router.get(\"/{auth_method}/{username}/messages\", response_model=list[protocol.Message])\ndef query_frontend_user_messages(\n    auth_method: str,\n    username: str,\n    api_client_id: UUID = None,\n    max_count: int = Query(10, gt=0, le=1000),\n    start_date: datetime.datetime = None,\n    end_date: datetime.datetime = None,\n    only_roots: bool = False,\n    desc: bool = True,\n    include_deleted: bool = False,\n    lang: Optional[str] = None,\n    api_client: ApiClient = Depends(deps.get_api_client),\n    db: Session = Depends(deps.get_db),\n):\n    \"\"\"\n    Query frontend user messages.\n    \"\"\"\n    pr = PromptRepository(db, api_client, auth_method=auth_method, username=username)\n    messages = pr.query_messages_ordered_by_created_date(\n        auth_method=auth_method,\n        username=username,\n        api_client_id=api_client_id,\n        desc=desc,\n        limit=max_count,\n        gte_created_date=start_date,\n        lte_created_date=end_date,\n        only_roots=only_roots,\n        deleted=None if include_deleted else False,\n        lang=lang,\n    )\n    return utils.prepare_message_list(messages)\n\n\n@router.get(\"/{auth_method}/{username}/messages/cursor\", response_model=protocol.MessagePage)\ndef query_frontend_user_messages_cursor(\n    auth_method: str,\n    username: str,\n    before: Optional[str] = None,\n    after: Optional[str] = None,\n    only_roots: Optional[bool] = False,\n    include_deleted: Optional[bool] = False,\n    max_count: Optional[int] = Query(10, gt=0, le=1000),\n    desc: Optional[bool] = False,\n    lang: Optional[str] = None,\n    frontend_user: deps.FrontendUserId = Depends(deps.get_frontend_user_id),\n    api_client: ApiClient = Depends(deps.get_api_client),\n    db: Session = Depends(deps.get_db),\n):\n    return get_messages_cursor(\n        before=before,\n        after=after,\n        auth_method=auth_method,\n        username=username,\n        only_roots=only_roots,\n        include_deleted=include_deleted,\n        max_count=max_count,\n        desc=desc,\n        lang=lang,\n        frontend_user=frontend_user,\n        api_client=api_client,\n        db=db,\n    )\n\n\n@router.delete(\"/{auth_method}/{username}/messages\", status_code=HTTP_204_NO_CONTENT)\ndef mark_frontend_user_messages_deleted(\n    auth_method: str,\n    username: str,\n    api_client: ApiClient = Depends(deps.get_trusted_api_client),\n    db: Session = Depends(deps.get_db),\n):\n    pr = PromptRepository(db, api_client)\n    messages = pr.query_messages_ordered_by_created_date(\n        auth_method=auth_method,\n        username=username,\n        api_client_id=api_client.id,\n        limit=None,\n    )\n    pr.mark_messages_deleted(messages)\n", "backend/oasst_backend/api/v1/utils.py": "import re\nfrom uuid import UUID\n\nfrom oasst_backend.models import Message, MessageRevision\nfrom oasst_shared.schemas import protocol\n\n\ndef prepare_message(m: Message) -> protocol.Message:\n    return protocol.Message(\n        id=m.id,\n        frontend_message_id=m.frontend_message_id,\n        parent_id=m.parent_id,\n        user_id=m.user_id,\n        text=m.text,\n        lang=m.lang,\n        is_assistant=(m.role == \"assistant\"),\n        created_date=m.created_date,\n        emojis=m.emojis or {},\n        user_emojis=m.user_emojis or [],\n        user_is_author=m.user_is_author,\n        review_result=m.review_result,\n        review_count=m.review_count,\n        ranking_count=m.ranking_count,\n        deleted=m.deleted,\n        edited=m.edited,\n        synthetic=m.synthetic,\n        model_name=m.model_name,\n        message_tree_id=m.message_tree_id,\n        rank=m.rank,\n        user=m.user.to_protocol_frontend_user() if m.user else None,\n    )\n\n\ndef prepare_message_list(messages: list[Message]) -> list[protocol.Message]:\n    return [prepare_message(m) for m in messages]\n\n\ndef prepare_conversation_message(message: Message) -> protocol.ConversationMessage:\n    return protocol.ConversationMessage(\n        id=message.id,\n        user_id=message.user_id,\n        frontend_message_id=message.frontend_message_id,\n        text=message.text,\n        lang=message.lang,\n        is_assistant=(message.role == \"assistant\"),\n        emojis=message.emojis or {},\n        user_emojis=message.user_emojis or [],\n        user_is_author=message.user_is_author,\n        synthetic=message.synthetic,\n    )\n\n\ndef prepare_conversation_message_list(messages: list[Message]) -> list[protocol.ConversationMessage]:\n    return [prepare_conversation_message(message) for message in messages]\n\n\ndef prepare_conversation(messages: list[Message]) -> protocol.Conversation:\n    return protocol.Conversation(messages=prepare_conversation_message_list(messages))\n\n\ndef prepare_tree(tree: list[Message], tree_id: UUID) -> protocol.MessageTree:\n    tree_messages = []\n    for message in tree:\n        tree_messages.append(prepare_message(message))\n\n    return protocol.MessageTree(id=tree_id, messages=tree_messages)\n\n\ndef prepare_message_revision(revision: MessageRevision) -> protocol.MessageRevision:\n    return protocol.MessageRevision(\n        id=revision.id,\n        text=revision.payload.payload.text,\n        message_id=revision.message_id,\n        user_id=revision.user_id,\n        created_date=revision.created_date,\n        user_is_author=revision._user_is_author,\n    )\n\n\ndef prepare_message_revision_list(revisions: list[MessageRevision]) -> list[protocol.MessageRevision]:\n    return [prepare_message_revision(revision) for revision in revisions]\n\n\nsplit_uuid_pattern = re.compile(\n    r\"^([0-9a-fA-F]{8}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{12})\\$(.*)$\"\n)\n", "backend/oasst_backend/api/v1/tasks.py": "from typing import Any, Optional\nfrom uuid import UUID\n\nfrom fastapi import APIRouter, Depends\nfrom fastapi.security.api_key import APIKey\nfrom loguru import logger\nfrom oasst_backend.api import deps\nfrom oasst_backend.config import settings\nfrom oasst_backend.prompt_repository import PromptRepository, TaskRepository\nfrom oasst_backend.tree_manager import TreeManager\nfrom oasst_backend.user_repository import UserRepository\nfrom oasst_backend.utils.database_utils import CommitMode, async_managed_tx_function\nfrom oasst_shared.exceptions import OasstError, OasstErrorCode\nfrom oasst_shared.schemas import protocol as protocol_schema\nfrom sqlmodel import Session\nfrom starlette.status import HTTP_204_NO_CONTENT\n\nrouter = APIRouter()\n\n\n@router.post(\n    \"/\",\n    response_model=protocol_schema.AnyTask,\n    dependencies=[\n        Depends(\n            deps.UserRateLimiter(\n                times=settings.RATE_LIMIT_TASK_USER_TIMES,\n                minutes=settings.RATE_LIMIT_TASK_USER_MINUTES,\n            )\n        ),\n        Depends(\n            deps.APIClientRateLimiter(\n                times=settings.RATE_LIMIT_TASK_API_TIMES,\n                minutes=settings.RATE_LIMIT_TASK_API_MINUTES,\n            )\n        ),\n        Depends(\n            deps.UserTaskTypeRateLimiter(\n                [\n                    protocol_schema.TaskRequestType.assistant_reply,\n                ],\n                times=settings.RATE_LIMIT_ASSISTANT_USER_TIMES,\n                minutes=settings.RATE_LIMIT_ASSISTANT_USER_MINUTES,\n            )\n        ),\n        Depends(\n            deps.UserTaskTypeRateLimiter(\n                [\n                    protocol_schema.TaskRequestType.prompter_reply,\n                ],\n                times=settings.RATE_LIMIT_PROMPTER_USER_TIMES,\n                minutes=settings.RATE_LIMIT_PROMPTER_USER_MINUTES,\n            )\n        ),\n    ],\n)  # work with Union once more types are added\ndef request_task(\n    *,\n    db: Session = Depends(deps.get_db),\n    api_key: APIKey = Depends(deps.get_api_key),\n    request: protocol_schema.TaskRequest,\n) -> Any:\n    \"\"\"\n    Create new task.\n    \"\"\"\n    api_client = deps.api_auth(api_key, db)\n\n    try:\n        pr = PromptRepository(db, api_client, client_user=request.user)\n        pr.ensure_user_is_enabled()\n\n        tm = TreeManager(db, pr)\n        task, message_tree_id, parent_message_id = tm.next_task(desired_task_type=request.type, lang=request.lang)\n        pr.task_repository.store_task(task, message_tree_id, parent_message_id, request.collective)\n\n    except OasstError:\n        raise\n    except Exception:\n        logger.exception(\"Failed to generate task..\")\n        raise OasstError(\"Failed to generate task.\", OasstErrorCode.TASK_GENERATION_FAILED)\n    return task\n\n\n@router.post(\"/availability\", response_model=dict[protocol_schema.TaskRequestType, int])\ndef tasks_availability(\n    *,\n    user: Optional[protocol_schema.User] = None,\n    lang: Optional[str] = \"en\",\n    db: Session = Depends(deps.get_db),\n    api_key: APIKey = Depends(deps.get_api_key),\n):\n    api_client = deps.api_auth(api_key, db)\n\n    try:\n        pr = PromptRepository(db, api_client, client_user=user)\n        tm = TreeManager(db, pr)\n        return tm.determine_task_availability(lang)\n\n    except OasstError:\n        raise\n    except Exception:\n        logger.exception(\"Task availability query failed.\")\n        raise OasstError(\"Task availability query failed.\", OasstErrorCode.TASK_AVAILABILITY_QUERY_FAILED)\n\n\n@router.post(\"/{task_id}/ack\", response_model=None, status_code=HTTP_204_NO_CONTENT)\ndef tasks_acknowledge(\n    *,\n    db: Session = Depends(deps.get_db),\n    api_key: APIKey = Depends(deps.get_api_key),\n    frontend_user: deps.FrontendUserId = Depends(deps.get_frontend_user_id),\n    task_id: UUID,\n    ack_request: protocol_schema.TaskAck,\n) -> None:\n    \"\"\"\n    The frontend acknowledges a task.\n    \"\"\"\n\n    api_client = deps.api_auth(api_key, db)\n\n    try:\n        pr = PromptRepository(db, api_client, frontend_user=frontend_user)\n\n        # here we store the message id in the database for the task\n        logger.info(f\"Frontend ACK task_id={task_id}\")\n        logger.debug(f\"{ack_request=}.\")\n        pr.task_repository.bind_frontend_message_id(task_id=task_id, frontend_message_id=ack_request.message_id)\n\n    except OasstError:\n        raise\n    except Exception:\n        logger.exception(\"Failed to acknowledge task.\")\n        raise OasstError(\"Failed to acknowledge task.\", OasstErrorCode.TASK_ACK_FAILED)\n\n\n@router.post(\"/{task_id}/nack\", response_model=None, status_code=HTTP_204_NO_CONTENT)\ndef tasks_acknowledge_failure(\n    *,\n    db: Session = Depends(deps.get_db),\n    api_key: APIKey = Depends(deps.get_api_key),\n    frontend_user: deps.FrontendUserId = Depends(deps.get_frontend_user_id),\n    task_id: UUID,\n    nack_request: protocol_schema.TaskNAck,\n) -> None:\n    \"\"\"\n    The frontend reports failure to implement a task.\n    \"\"\"\n\n    try:\n        logger.info(f\"Frontend reports failure to implement task {task_id=}, {nack_request=}.\")\n        api_client = deps.api_auth(api_key, db)\n        pr = PromptRepository(db, api_client, frontend_user=frontend_user)\n        pr.skip_task(task_id=task_id, reason=nack_request.reason)\n    except (KeyError, RuntimeError):\n        logger.exception(\"Failed to not acknowledge task.\")\n        raise OasstError(\"Failed to not acknowledge task.\", OasstErrorCode.TASK_NACK_FAILED)\n\n\n@router.post(\"/interaction\", response_model=protocol_schema.TaskDone)\nasync def tasks_interaction(\n    *,\n    api_key: APIKey = Depends(deps.get_api_key),\n    interaction: protocol_schema.AnyInteraction,\n) -> Any:\n    \"\"\"\n    The frontend reports an interaction.\n    \"\"\"\n\n    @async_managed_tx_function(CommitMode.COMMIT)\n    async def interaction_tx(session: deps.Session):\n        api_client = deps.api_auth(api_key, session)\n        pr = PromptRepository(session, api_client, client_user=interaction.user)\n        tm = TreeManager(session, pr)\n        ur = UserRepository(session, api_client)\n        task = await tm.handle_interaction(interaction)\n        if type(task) is protocol_schema.TaskDone:\n            ur.update_user_last_activity(user=pr.user, update_streak=True)\n        return task\n\n    try:\n        return await interaction_tx()\n    except OasstError:\n        raise\n    except Exception:\n        logger.exception(\"Interaction request failed.\")\n        raise OasstError(\"Interaction request failed.\", OasstErrorCode.TASK_INTERACTION_REQUEST_FAILED)\n\n\n@router.post(\"/close\", response_model=protocol_schema.TaskDone)\ndef close_collective_task(\n    close_task_request: protocol_schema.TaskClose,\n    db: Session = Depends(deps.get_db),\n    api_key: APIKey = Depends(deps.get_api_key),\n):\n    api_client = deps.api_auth(api_key, db)\n    tr = TaskRepository(db, api_client)\n    tr.close_task(close_task_request.message_id)\n    return protocol_schema.TaskDone()\n", "backend/oasst_backend/api/v1/stats.py": "from fastapi import APIRouter, Depends\nfrom oasst_backend.api import deps\nfrom oasst_backend.cached_stats_repository import CachedStatsRepository\nfrom oasst_backend.models import ApiClient\nfrom oasst_backend.prompt_repository import PromptRepository\nfrom oasst_backend.tree_manager import TreeManager, TreeManagerStats, TreeMessageCountStats\nfrom oasst_backend.utils.database_utils import CommitMode, managed_tx_function\nfrom oasst_shared.schemas import protocol\nfrom sqlmodel import Session\nfrom starlette.status import HTTP_204_NO_CONTENT\n\nrouter = APIRouter()\n\n\n@router.get(\"/\", response_model=protocol.SystemStats)\ndef get_message_stats(\n    db: Session = Depends(deps.get_db),\n    api_client: ApiClient = Depends(deps.get_trusted_api_client),\n):\n    pr = PromptRepository(db, api_client)\n    return pr.get_stats()\n\n\n@router.get(\"/tree_manager/state_counts\", response_model=dict[str, int])\ndef get_tree_manager__state_counts(\n    db: Session = Depends(deps.get_db),\n    api_client: ApiClient = Depends(deps.get_trusted_api_client),\n):\n    pr = PromptRepository(db, api_client)\n    tm = TreeManager(db, pr)\n    return tm.tree_counts_by_state()\n\n\n@router.get(\"/tree_manager/message_counts\", response_model=list[TreeMessageCountStats])\ndef get_tree_manager__message_counts(\n    only_active: bool = True,\n    db: Session = Depends(deps.get_db),\n    api_client: ApiClient = Depends(deps.get_trusted_api_client),\n):\n    pr = PromptRepository(db, api_client)\n    tm = TreeManager(db, pr)\n    return tm.tree_message_count_stats(only_active=only_active)\n\n\n@router.get(\"/tree_manager\", response_model=TreeManagerStats)\ndef get_tree_manager__stats(\n    db: Session = Depends(deps.get_db),\n    api_client: ApiClient = Depends(deps.get_trusted_api_client),\n):\n    pr = PromptRepository(db, api_client)\n    tm = TreeManager(db, pr)\n    return tm.stats()\n\n\n@router.get(\"/cached/{name}\", response_model=protocol.CachedStatsResponse)\ndef get_cached_stats(\n    *,\n    name: protocol.CachedStatsName,\n    db: Session = Depends(deps.get_db),\n    api_client: ApiClient = Depends(deps.get_api_client),\n):\n    csr = CachedStatsRepository(db)\n    return csr.get_stats(name)\n\n\n@router.get(\"/cached\", response_model=protocol.AllCachedStatsResponse)\ndef get_cached_stats_all(\n    *,\n    db: Session = Depends(deps.get_db),\n    api_client: ApiClient = Depends(deps.get_api_client),\n):\n    csr = CachedStatsRepository(db)\n    return csr.get_stats_all()\n\n\n@router.post(\"/cached/update\", response_model=None, status_code=HTTP_204_NO_CONTENT)\ndef update_cached_stats(\n    *,\n    db: Session = Depends(deps.get_db),\n    api_client: ApiClient = Depends(deps.get_trusted_api_client),\n):\n    @managed_tx_function(CommitMode.COMMIT)\n    def update_tx(db: deps.Session) -> None:\n        csr = CachedStatsRepository(db)\n        csr.update_all_cached_stats()\n\n    update_tx()\n", "backend/oasst_backend/api/v1/login.py": "import aiohttp\nfrom fastapi import APIRouter, Depends, HTTPException, Request\nfrom oasst_backend import auth\nfrom oasst_backend.api import deps\nfrom oasst_backend.config import Settings\nfrom oasst_backend.models import Account\nfrom oasst_shared.exceptions.oasst_api_error import OasstError, OasstErrorCode\nfrom oasst_shared.schemas import protocol as protocol_schema\nfrom sqlmodel import Session\nfrom starlette.status import HTTP_401_UNAUTHORIZED\n\nrouter = APIRouter()\n\n\n@router.get(\"/discord\")\ndef login_discord(request: Request):\n    redirect_uri = f\"{get_callback_uri(request)}/discord\"\n    auth_url = f\"https://discord.com/api/oauth2/authorize?client_id={Settings.AUTH_DISCORD_CLIENT_ID}&redirect_uri={redirect_uri}&response_type=code&scope=identify\"\n    raise HTTPException(status_code=302, headers={\"location\": auth_url})\n\n\n@router.get(\"/callback/discord\", response_model=protocol_schema.Token)\nasync def callback_discord(\n    auth_code: str,\n    request: Request,\n    db: Session = Depends(deps.get_db),\n):\n    redirect_uri = f\"{get_callback_uri(request)}/discord\"\n\n    async with aiohttp.ClientSession(raise_for_status=True) as session:\n        # Exchange the auth code for a Discord access token\n        async with session.post(\n            \"https://discord.com/api/oauth2/token\",\n            data={\n                \"client_id\": Settings.AUTH_DISCORD_CLIENT_ID,\n                \"client_secret\": Settings.AUTH_DISCORD_CLIENT_SECRET,\n                \"grant_type\": \"authorization_code\",\n                \"code\": auth_code,\n                \"redirect_uri\": redirect_uri,\n                \"scope\": \"identify\",\n            },\n        ) as token_response:\n            token_response_json = await token_response.json()\n            access_token = token_response_json[\"access_token\"]\n\n        # Retrieve user's Discord information using access token\n        async with session.get(\n            \"https://discord.com/api/users/@me\", headers={\"Authorization\": f\"Bearer {access_token}\"}\n        ) as user_response:\n            user_response_json = await user_response.json()\n            discord_id = user_response_json[\"id\"]\n\n    account: Account = auth.get_account_from_discord_id(db, discord_id)\n\n    if not account:\n        # Discord account is not linked to an OA account\n        raise OasstError(\"Invalid authentication\", OasstErrorCode.INVALID_AUTHENTICATION, HTTP_401_UNAUTHORIZED)\n\n    # Discord account is valid and linked to an OA account -> create JWT\n    access_token = auth.create_access_token(account)\n\n    return protocol_schema.Token(access_token=access_token, token_type=\"bearer\")\n\n\ndef get_callback_uri(request: Request):\n    \"\"\"\n    Gets the URI for the base callback endpoint with no provider name appended.\n    \"\"\"\n    # This seems ugly, not sure if there is a better way\n    current_url = str(request.url)\n    domain = current_url.split(\"/api/v1/\")[0]\n    redirect_uri = f\"{domain}/api/v1/callback\"\n    return redirect_uri\n", "backend/oasst_backend/api/v1/text_labels.py": "from typing import Optional\nfrom uuid import UUID\n\nfrom fastapi import APIRouter, Depends, HTTPException\nfrom fastapi.security.api_key import APIKey\nfrom loguru import logger\nfrom oasst_backend.api import deps\nfrom oasst_backend.config import settings\nfrom oasst_backend.models import ApiClient\nfrom oasst_backend.prompt_repository import PromptRepository\nfrom oasst_backend.schemas.text_labels import LabelDescription, ValidLabelsResponse\nfrom oasst_backend.utils.database_utils import CommitMode, managed_tx_function\nfrom oasst_shared.exceptions import OasstError\nfrom oasst_shared.schemas import protocol as protocol_schema\nfrom oasst_shared.schemas.protocol import TextLabel\nfrom sqlmodel import Session\nfrom starlette.status import HTTP_204_NO_CONTENT, HTTP_400_BAD_REQUEST\n\nrouter = APIRouter()\n\n\n@router.post(\"/\", status_code=HTTP_204_NO_CONTENT)\ndef label_text(\n    *,\n    api_key: APIKey = Depends(deps.get_api_key),\n    text_labels: protocol_schema.TextLabels,\n) -> None:\n    \"\"\"\n    Label a piece of text.\n    \"\"\"\n\n    @managed_tx_function(CommitMode.COMMIT)\n    def store_text_labels(session: deps.Session):\n        api_client = deps.api_auth(api_key, session)\n        pr = PromptRepository(session, api_client, client_user=text_labels.user)\n        pr.store_text_labels(text_labels)\n\n    try:\n        logger.info(f\"Labeling text {text_labels=}.\")\n        store_text_labels()\n\n    except OasstError:\n        raise\n    except Exception:\n        logger.exception(\"Failed to store label.\")\n        raise HTTPException(\n            status_code=HTTP_400_BAD_REQUEST,\n        )\n\n\n@router.get(\"/valid_labels\")\ndef get_valid_lables(\n    *,\n    message_id: Optional[UUID] = None,\n    db: Session = Depends(deps.get_db),\n    api_client: ApiClient = Depends(deps.get_api_client),\n) -> ValidLabelsResponse:\n    if message_id:\n        pr = PromptRepository(db, api_client=api_client)\n        message = pr.fetch_message(message_id=message_id)\n        if message.parent_id is None:\n            valid_labels = settings.tree_manager.labels_initial_prompt\n        elif message.role == \"assistant\":\n            valid_labels = settings.tree_manager.labels_assistant_reply\n        else:\n            valid_labels = settings.tree_manager.labels_prompter_reply\n    else:\n        valid_labels = [l for l in TextLabel if l != TextLabel.fails_task]\n\n    return ValidLabelsResponse(\n        valid_labels=[\n            LabelDescription(name=l.value, widget=l.widget.value, display_text=l.display_text, help_text=l.help_text)\n            for l in valid_labels\n        ]\n    )\n\n\n@router.get(\"/report_labels\")\ndef get_report_lables() -> ValidLabelsResponse:\n    report_labels = [\n        TextLabel.spam,\n        TextLabel.not_appropriate,\n        TextLabel.pii,\n        TextLabel.hate_speech,\n        TextLabel.sexual_content,\n        TextLabel.moral_judgement,\n        TextLabel.political_content,\n        TextLabel.toxicity,\n        TextLabel.violence,\n        TextLabel.quality,\n    ]\n    return ValidLabelsResponse(\n        valid_labels=[\n            LabelDescription(name=l.value, widget=l.widget.value, display_text=l.display_text, help_text=l.help_text)\n            for l in report_labels\n        ]\n    )\n", "backend/oasst_backend/api/v1/leaderboards.py": "from typing import Optional\nfrom uuid import UUID\n\nfrom fastapi import APIRouter, Depends, Query\nfrom oasst_backend.api import deps\nfrom oasst_backend.models import ApiClient\nfrom oasst_backend.user_repository import UserRepository\nfrom oasst_backend.user_stats_repository import UserStatsRepository, UserStatsTimeFrame\nfrom oasst_shared.schemas.protocol import LeaderboardStats\nfrom sqlmodel import Session\nfrom starlette.status import HTTP_204_NO_CONTENT\n\nrouter = APIRouter()\n\n\n@router.get(\"/{time_frame}\", response_model=LeaderboardStats)\ndef get_leaderboard(\n    time_frame: UserStatsTimeFrame,\n    max_count: Optional[int] = Query(100, gt=0, le=10000),\n    frontend_user: deps.FrontendUserId = Depends(deps.get_frontend_user_id),\n    api_client: ApiClient = Depends(deps.get_api_client),\n    db: Session = Depends(deps.get_db),\n) -> LeaderboardStats:\n    current_user_id: UUID | None = None\n    if frontend_user.username:\n        ur = UserRepository(db, api_client)\n        current_user = ur.query_frontend_user(auth_method=frontend_user.auth_method, username=frontend_user.username)\n        current_user_id = current_user.id\n    usr = UserStatsRepository(db)\n    return usr.get_leaderboard(time_frame, limit=max_count, highlighted_user_id=current_user_id)\n\n\n@router.post(\"/update/{time_frame}\", response_model=None, status_code=HTTP_204_NO_CONTENT)\ndef update_leaderboard_time_frame(\n    time_frame: UserStatsTimeFrame,\n    api_client: ApiClient = Depends(deps.get_trusted_api_client),\n    db: Session = Depends(deps.get_db),\n) -> LeaderboardStats:\n    usr = UserStatsRepository(db)\n    return usr.update_stats(time_frame=time_frame)\n\n\n@router.post(\"/update\", response_model=None, status_code=HTTP_204_NO_CONTENT)\ndef update_leaderboards_all(\n    api_client: ApiClient = Depends(deps.get_trusted_api_client),\n    db: Session = Depends(deps.get_db),\n) -> LeaderboardStats:\n    usr = UserStatsRepository(db)\n    return usr.update_all_time_frames()\n", "backend/oasst_backend/api/v1/__init__.py": "", "backend/oasst_backend/api/v1/hugging_face.py": "from typing import List\n\nfrom fastapi import APIRouter, Depends\nfrom oasst_backend.api import deps\nfrom oasst_backend.models import ApiClient\nfrom oasst_backend.schemas.hugging_face import ToxicityClassification\nfrom oasst_backend.utils.hugging_face import HfClassificationModel, HfUrl, HuggingFaceAPI\n\nrouter = APIRouter()\n\n\n@router.get(\"/text_toxicity\")\nasync def get_text_toxicity(\n    msg: str,\n    api_client: ApiClient = Depends(deps.get_trusted_api_client),\n) -> List[List[ToxicityClassification]]:\n    \"\"\"Get the Message Toxicity from HuggingFace Roberta model.\n\n    Args:\n        msg (str): the message that we want to analyze.\n        api_client (ApiClient, optional): authentication of the user of the request.\n            Defaults to Depends(deps.get_trusted_api_client).\n\n    Returns:\n        ToxicityClassification: the score of toxicity of the message.\n    \"\"\"\n\n    api_url: str = HfUrl.HUGGINGFACE_TOXIC_CLASSIFICATION.value + \"/\" + HfClassificationModel.TOXIC_ROBERTA.value\n\n    hugging_face_api = HuggingFaceAPI(api_url)\n    response = await hugging_face_api.post(msg)\n\n    return response\n", "backend/oasst_backend/api/v1/messages.py": "from datetime import datetime\nfrom typing import Optional\nfrom uuid import UUID\n\nfrom fastapi import APIRouter, Depends, Query\nfrom loguru import logger\nfrom oasst_backend.api import deps\nfrom oasst_backend.api.v1 import utils\nfrom oasst_backend.models import ApiClient, MessageTreeState\nfrom oasst_backend.prompt_repository import PromptRepository\nfrom oasst_backend.schemas.message_tree import MessageTreeStateResponse\nfrom oasst_backend.tree_manager import TreeManager\nfrom oasst_backend.utils.database_utils import CommitMode, managed_tx_function\nfrom oasst_shared.exceptions.oasst_api_error import OasstError, OasstErrorCode\nfrom oasst_shared.schemas import protocol\nfrom sqlmodel import Session\nfrom starlette.status import HTTP_202_ACCEPTED, HTTP_204_NO_CONTENT\n\nrouter = APIRouter()\n\n\n@router.get(\"/\", response_model=list[protocol.Message])\ndef query_messages(\n    *,\n    auth_method: Optional[str] = None,\n    username: Optional[str] = None,\n    api_client_id: Optional[str] = None,\n    max_count: Optional[int] = Query(10, gt=0, le=1000),\n    start_date: Optional[datetime] = None,\n    end_date: Optional[datetime] = None,\n    only_roots: Optional[bool] = False,\n    desc: Optional[bool] = True,\n    allow_deleted: Optional[bool] = False,\n    lang: Optional[str] = None,\n    frontend_user: deps.FrontendUserId = Depends(deps.get_frontend_user_id),\n    api_client: ApiClient = Depends(deps.get_api_client),\n    db: Session = Depends(deps.get_db),\n):\n    \"\"\"\n    Query messages.\n    \"\"\"\n    pr = PromptRepository(db, api_client, auth_method=frontend_user.auth_method, username=frontend_user.username)\n    messages = pr.query_messages_ordered_by_created_date(\n        auth_method=auth_method,\n        username=username,\n        api_client_id=api_client_id,\n        desc=desc,\n        limit=max_count,\n        gte_created_date=start_date,\n        lte_created_date=end_date,\n        only_roots=only_roots,\n        deleted=None if allow_deleted else False,\n        lang=lang,\n    )\n\n    return utils.prepare_message_list(messages)\n\n\n@router.get(\"/cursor\", response_model=protocol.MessagePage)\ndef get_messages_cursor(\n    *,\n    before: Optional[str] = None,\n    after: Optional[str] = None,\n    user_id: Optional[UUID] = None,\n    auth_method: Optional[str] = None,\n    username: Optional[str] = None,\n    api_client_id: Optional[str] = None,\n    only_roots: Optional[bool] = False,\n    include_deleted: Optional[bool] = False,\n    max_count: Optional[int] = Query(10, gt=0, le=1000),\n    desc: Optional[bool] = False,\n    search_query: Optional[str] = None,\n    lang: Optional[str] = None,\n    include_user: Optional[bool] = None,\n    frontend_user: deps.FrontendUserId = Depends(deps.get_frontend_user_id),\n    api_client: ApiClient = Depends(deps.get_api_client),\n    db: Session = Depends(deps.get_db),\n):\n    assert max_count is not None\n\n    def split_cursor(x: str | None) -> tuple[datetime, UUID]:\n        if not x:\n            return None, None\n        try:\n            m = utils.split_uuid_pattern.match(x)\n            if m:\n                return datetime.fromisoformat(m[2]), UUID(m[1])\n            return datetime.fromisoformat(x), None\n        except ValueError:\n            raise OasstError(\"Invalid cursor value\", OasstErrorCode.INVALID_CURSOR_VALUE)\n\n    if desc:\n        gte_created_date, gt_id = split_cursor(before)\n        lte_created_date, lt_id = split_cursor(after)\n        query_desc = not (before is not None and not after)\n    else:\n        lte_created_date, lt_id = split_cursor(before)\n        gte_created_date, gt_id = split_cursor(after)\n        query_desc = before is not None and not after\n\n    logger.debug(f\"{desc=} {query_desc=} {gte_created_date=} {lte_created_date=}\")\n\n    qry_max_count = max_count + 1 if before is None or after is None else max_count\n\n    pr = PromptRepository(db, api_client, frontend_user=frontend_user)\n    items = pr.query_messages_ordered_by_created_date(\n        user_id=user_id,\n        auth_method=auth_method,\n        username=username,\n        api_client_id=api_client_id,\n        gte_created_date=gte_created_date,\n        gt_id=gt_id,\n        lte_created_date=lte_created_date,\n        lt_id=lt_id,\n        only_roots=only_roots,\n        deleted=None if include_deleted else False,\n        desc=query_desc,\n        limit=qry_max_count,\n        search_query=search_query,\n        lang=lang,\n        include_user=include_user,\n    )\n\n    num_rows = len(items)\n    if qry_max_count > max_count and num_rows == qry_max_count:\n        assert not (before and after)\n        items = items[:-1]\n\n    if desc != query_desc:\n        items.reverse()\n\n    items = utils.prepare_message_list(items)\n    n, p = None, None\n    if len(items) > 0:\n        if (num_rows > max_count and before) or after:\n            p = str(items[0].id) + \"$\" + items[0].created_date.isoformat()\n        if num_rows > max_count or before:\n            n = str(items[-1].id) + \"$\" + items[-1].created_date.isoformat()\n    else:\n        if after:\n            p = lte_created_date.isoformat() if desc else gte_created_date.isoformat()\n        if before:\n            n = gte_created_date.isoformat() if desc else lte_created_date.isoformat()\n\n    order = \"desc\" if desc else \"asc\"\n    return protocol.MessagePage(prev=p, next=n, sort_key=\"created_date\", order=order, items=items)\n\n\n@router.get(\"/{message_id}\", response_model=protocol.Message)\ndef get_message(\n    *,\n    message_id: UUID,\n    frontend_user: deps.FrontendUserId = Depends(deps.get_frontend_user_id),\n    api_client: ApiClient = Depends(deps.get_api_client),\n    db: Session = Depends(deps.get_db),\n):\n    \"\"\"\n    Get a message by its internal ID.\n    \"\"\"\n    pr = PromptRepository(db, api_client, frontend_user=frontend_user)\n    message = pr.fetch_message(message_id)\n    return utils.prepare_message(message)\n\n\n@router.get(\"/{message_id}/conversation\", response_model=protocol.Conversation)\ndef get_conv(\n    *,\n    message_id: UUID,\n    frontend_user: deps.FrontendUserId = Depends(deps.get_frontend_user_id),\n    api_client: ApiClient = Depends(deps.get_api_client),\n    db: Session = Depends(deps.get_db),\n):\n    \"\"\"\n    Get a conversation from the tree root and up to the message with given internal ID.\n    \"\"\"\n\n    pr = PromptRepository(db, api_client, frontend_user=frontend_user)\n    messages = pr.fetch_message_conversation(message_id)\n    return utils.prepare_conversation(messages)\n\n\n@router.get(\"/{message_id}/tree\", response_model=protocol.MessageTree)\ndef get_tree(\n    *,\n    message_id: UUID,\n    include_spam: Optional[bool] = True,\n    include_deleted: Optional[bool] = False,\n    frontend_user: deps.FrontendUserId = Depends(deps.get_frontend_user_id),\n    api_client: ApiClient = Depends(deps.get_api_client),\n    db: Session = Depends(deps.get_db),\n):\n    \"\"\"\n    Get all messages belonging to the same message tree.\n    \"\"\"\n    pr = PromptRepository(db, api_client, frontend_user=frontend_user)\n    message = pr.fetch_message(message_id)\n    review_result = None if include_spam else True\n    deleted = None if include_deleted else False\n    tree = pr.fetch_message_tree(message.message_tree_id, review_result=review_result, deleted=deleted)\n    return utils.prepare_tree(tree, message.message_tree_id)\n\n\n@router.get(\"/{message_id}/tree/state\", response_model=MessageTreeStateResponse)\ndef get_message_tree_state(\n    *,\n    message_id: UUID,\n    frontend_user: deps.FrontendUserId = Depends(deps.get_frontend_user_id),\n    api_client: ApiClient = Depends(deps.get_api_client),\n    db: Session = Depends(deps.get_db),\n) -> MessageTreeStateResponse:\n    pr = PromptRepository(db, api_client, frontend_user=frontend_user)\n    message = pr.fetch_message(message_id=message_id, fail_if_missing=True)\n    mts = pr.fetch_tree_state(message.message_tree_id)\n    return MessageTreeStateResponse(\n        message_tree_id=mts.message_tree_id,\n        state=mts.state,\n        active=mts.active,\n        goal_tree_size=mts.goal_tree_size,\n        max_children_count=mts.max_children_count,\n        max_depth=mts.max_depth,\n        origin=mts.origin,\n    )\n\n\n@router.put(\"/{message_id}/tree/state\", response_model=MessageTreeStateResponse)\ndef put_message_tree_state(\n    *,\n    message_id: UUID,\n    halt: bool,\n    frontend_user: deps.FrontendUserId = Depends(deps.get_frontend_user_id),\n    api_client: ApiClient = Depends(deps.get_trusted_api_client),\n) -> MessageTreeStateResponse:\n    @managed_tx_function(CommitMode.COMMIT)\n    def halt_tree_tx(session: deps.Session) -> MessageTreeState:\n        pr = PromptRepository(session, api_client, frontend_user=frontend_user)\n        tm = TreeManager(session, pr)\n        return tm.halt_tree(message_id, halt=halt)\n\n    mts = halt_tree_tx()\n    return MessageTreeStateResponse(\n        message_tree_id=mts.message_tree_id,\n        state=mts.state,\n        active=mts.active,\n        goal_tree_size=mts.goal_tree_size,\n        max_children_count=mts.max_children_count,\n        max_depth=mts.max_depth,\n        origin=mts.origin,\n    )\n\n\n@router.get(\"/{message_id}/children\", response_model=list[protocol.Message])\ndef get_children(\n    *,\n    message_id: UUID,\n    frontend_user: deps.FrontendUserId = Depends(deps.get_frontend_user_id),\n    api_client: ApiClient = Depends(deps.get_api_client),\n    db: Session = Depends(deps.get_db),\n):\n    \"\"\"\n    Get all messages belonging to the same message tree.\n    \"\"\"\n    pr = PromptRepository(db, api_client, frontend_user=frontend_user)\n    messages = pr.fetch_message_children(message_id, review_result=None)\n    return utils.prepare_message_list(messages)\n\n\n@router.get(\"/{message_id}/descendants\", response_model=protocol.MessageTree)\ndef get_descendants(\n    *,\n    message_id: UUID,\n    frontend_user: deps.FrontendUserId = Depends(deps.get_frontend_user_id),\n    api_client: ApiClient = Depends(deps.get_api_client),\n    db: Session = Depends(deps.get_db),\n):\n    \"\"\"\n    Get a subtree which starts with this message.\n    \"\"\"\n    pr = PromptRepository(db, api_client, frontend_user=frontend_user)\n    message = pr.fetch_message(message_id)\n    descendants = pr.fetch_message_descendants(message)\n    return utils.prepare_tree(descendants, message.id)\n\n\n@router.get(\"/{message_id}/longest_conversation_in_tree\", response_model=protocol.Conversation)\ndef get_longest_conv(\n    *,\n    message_id: UUID,\n    frontend_user: deps.FrontendUserId = Depends(deps.get_frontend_user_id),\n    api_client: ApiClient = Depends(deps.get_api_client),\n    db: Session = Depends(deps.get_db),\n):\n    \"\"\"\n    Get the longest conversation from the tree of the message.\n    \"\"\"\n    pr = PromptRepository(db, api_client, frontend_user=frontend_user)\n    message = pr.fetch_message(message_id)\n    conv = pr.fetch_longest_conversation(message.message_tree_id)\n    return utils.prepare_conversation(conv)\n\n\n@router.get(\"/{message_id}/max_children_in_tree\", response_model=protocol.MessageTree)\ndef get_max_children(\n    *,\n    message_id: UUID,\n    frontend_user: deps.FrontendUserId = Depends(deps.get_frontend_user_id),\n    api_client: ApiClient = Depends(deps.get_api_client),\n    db: Session = Depends(deps.get_db),\n):\n    \"\"\"\n    Get message with the most children from the tree of the provided message.\n    \"\"\"\n    pr = PromptRepository(db, api_client, frontend_user=frontend_user)\n    message = pr.fetch_message(message_id)\n    message, children = pr.fetch_message_with_max_children(message.message_tree_id)\n    return utils.prepare_tree([message, *children], message.id)\n\n\n@router.delete(\"/{message_id}\", status_code=HTTP_204_NO_CONTENT)\ndef mark_message_deleted(\n    *,\n    message_id: UUID,\n    frontend_user: deps.FrontendUserId = Depends(deps.get_frontend_user_id),\n    api_client: ApiClient = Depends(deps.get_trusted_api_client),\n    db: Session = Depends(deps.get_db),\n):\n    pr = PromptRepository(db, api_client, frontend_user=frontend_user)\n    pr.mark_messages_deleted(message_id)\n\n\n@router.put(\"/{message_id}/undelete\", status_code=HTTP_202_ACCEPTED, response_model=None)\ndef undelete_message(\n    *,\n    message_id: UUID,\n    frontend_user: deps.FrontendUserId = Depends(deps.get_frontend_user_id),\n    api_client: ApiClient = Depends(deps.get_api_client),\n    db: Session = Depends(deps.get_db),\n):\n    pr = PromptRepository(db, api_client, frontend_user=frontend_user)\n    pr.undelete_deleted_message(message_id)\n\n\n@router.post(\"/{message_id}/edit\")\ndef edit_message(\n    *,\n    message_id: UUID,\n    request: protocol.MessageEditRequest,\n    api_client: ApiClient = Depends(deps.get_trusted_api_client),\n):\n    @managed_tx_function(CommitMode.COMMIT)\n    def edit_tx(session: deps.Session):\n        pr = PromptRepository(session, api_client, client_user=request.user)\n        pr.revise_message(message_id, request.new_content)\n\n    edit_tx()\n\n\n@router.get(\"/{message_id}/history\", response_model=list[protocol.MessageRevision])\ndef get_revision_history(\n    *,\n    message_id: UUID,\n    frontend_user: deps.FrontendUserId = Depends(deps.get_frontend_user_id),\n    api_client: ApiClient = Depends(deps.get_trusted_api_client),\n    db: Session = Depends(deps.get_db),\n):\n    \"\"\"\n    Get all revisions of this message sorted from oldest to most recent\n    \"\"\"\n    pr = PromptRepository(db, api_client, frontend_user=frontend_user)\n    revisions = pr.fetch_message_revision_history(message_id)\n    return utils.prepare_message_revision_list(revisions)\n\n\n@router.post(\"/{message_id}/emoji\", status_code=HTTP_202_ACCEPTED)\ndef post_message_emoji(\n    *,\n    message_id: UUID,\n    request: protocol.MessageEmojiRequest,\n    api_client: ApiClient = Depends(deps.get_api_client),\n) -> protocol.Message:\n    \"\"\"\n    Toggle, add or remove message emoji.\n    \"\"\"\n\n    @managed_tx_function(CommitMode.COMMIT)\n    def emoji_tx(session: deps.Session):\n        pr = PromptRepository(session, api_client, client_user=request.user)\n        return pr.handle_message_emoji(message_id, request.op, request.emoji)\n\n    return utils.prepare_message(emoji_tx())\n", "backend/oasst_backend/api/v1/users.py": "import datetime\nfrom typing import Callable, Optional\nfrom uuid import UUID\n\nfrom fastapi import APIRouter, Depends, Query\nfrom oasst_backend.api import deps\nfrom oasst_backend.api.v1 import utils\nfrom oasst_backend.api.v1.messages import get_messages_cursor\nfrom oasst_backend.models import ApiClient, User\nfrom oasst_backend.prompt_repository import PromptRepository\nfrom oasst_backend.user_repository import UserRepository\nfrom oasst_backend.user_stats_repository import UserStatsRepository, UserStatsTimeFrame\nfrom oasst_shared.exceptions.oasst_api_error import OasstError, OasstErrorCode\nfrom oasst_shared.schemas import protocol\nfrom sqlmodel import Session\nfrom starlette.status import HTTP_204_NO_CONTENT\n\nrouter = APIRouter()\n\n\n@router.get(\"/by_username\", response_model=list[protocol.FrontEndUser])\ndef get_users_ordered_by_username(\n    api_client_id: Optional[UUID] = None,\n    gte_username: Optional[str] = None,\n    gt_id: Optional[UUID] = None,\n    lte_username: Optional[str] = None,\n    lt_id: Optional[UUID] = None,\n    search_text: Optional[str] = None,\n    auth_method: Optional[str] = None,\n    max_count: Optional[int] = Query(100, gt=0, le=10000),\n    desc: Optional[bool] = False,\n    api_client: ApiClient = Depends(deps.get_api_client),\n    db: Session = Depends(deps.get_db),\n):\n    ur = UserRepository(db, api_client)\n    users = ur.query_users_ordered_by_username(\n        api_client_id=api_client_id,\n        gte_username=gte_username,\n        gt_id=gt_id,\n        lte_username=lte_username,\n        lt_id=lt_id,\n        auth_method=auth_method,\n        search_text=search_text,\n        limit=max_count,\n        desc=desc,\n    )\n    return [u.to_protocol_frontend_user() for u in users]\n\n\n@router.get(\"/by_display_name\", response_model=list[protocol.FrontEndUser])\ndef get_users_ordered_by_display_name(\n    api_client_id: Optional[UUID] = None,\n    gte_display_name: Optional[str] = None,\n    gt_id: Optional[UUID] = None,\n    lte_display_name: Optional[str] = None,\n    lt_id: Optional[UUID] = None,\n    auth_method: Optional[str] = None,\n    search_text: Optional[str] = None,\n    max_count: Optional[int] = Query(100, gt=0, le=10000),\n    desc: Optional[bool] = False,\n    api_client: ApiClient = Depends(deps.get_api_client),\n    db: Session = Depends(deps.get_db),\n):\n    ur = UserRepository(db, api_client)\n    users = ur.query_users_ordered_by_display_name(\n        api_client_id=api_client_id,\n        gte_display_name=gte_display_name,\n        gt_id=gt_id,\n        lte_display_name=lte_display_name,\n        lt_id=lt_id,\n        auth_method=auth_method,\n        search_text=search_text,\n        limit=max_count,\n        desc=desc,\n    )\n    return [u.to_protocol_frontend_user() for u in users]\n\n\n@router.get(\"/cursor\", response_model=protocol.FrontEndUserPage)\ndef get_users_cursor(\n    before: Optional[str] = None,\n    after: Optional[str] = None,\n    sort_key: Optional[str] = Query(\"username\", max_length=32),\n    max_count: Optional[int] = Query(100, gt=0, le=10000),\n    api_client_id: Optional[UUID] = None,\n    search_text: Optional[str] = None,\n    auth_method: Optional[str] = None,\n    api_client: ApiClient = Depends(deps.get_api_client),\n    db: Session = Depends(deps.get_db),\n):\n    assert max_count is not None\n\n    def split_cursor(x: str | None) -> tuple[str, UUID]:\n        if not x:\n            return None, None\n        m = utils.split_uuid_pattern.match(x)\n        if m:\n            return m[2], UUID(m[1])\n        return x, None\n\n    items: list[protocol.FrontEndUser]\n    qry_max_count = max_count + 1 if before is None or after is None else max_count\n    desc = before is not None and not after\n\n    def get_next_prev(num_rows: int, lt: str | None, gt: str | None, key_fn: Callable[[protocol.FrontEndUser], str]):\n        p, n = None, None\n        if len(items) > 0:\n            if (num_rows > max_count and lt) or gt:\n                p = str(items[0].user_id) + \"$\" + key_fn(items[0])\n            if num_rows > max_count or lt:\n                n = str(items[-1].user_id) + \"$\" + key_fn(items[-1])\n        else:\n            if gt:\n                p = gt\n            if lt:\n                n = lt\n        return p, n\n\n    def remove_extra_item(items: list[protocol.FrontEndUser], lt: str | None, gt: str | None):\n        num_rows = len(items)\n        if qry_max_count > max_count and num_rows == qry_max_count:\n            assert not (lt is not None and gt is not None)\n            items = items[:-1]\n        if desc:\n            items.reverse()\n        return items, num_rows\n\n    n, p = None, None\n    if sort_key == \"username\":\n        lte_username, lt_id = split_cursor(before)\n        gte_username, gt_id = split_cursor(after)\n        items = get_users_ordered_by_username(\n            api_client_id=api_client_id,\n            gte_username=gte_username,\n            gt_id=gt_id,\n            lte_username=lte_username,\n            lt_id=lt_id,\n            auth_method=auth_method,\n            search_text=search_text,\n            max_count=qry_max_count,\n            desc=desc,\n            api_client=api_client,\n            db=db,\n        )\n        items, num_rows = remove_extra_item(items, lte_username, gte_username)\n        p, n = get_next_prev(num_rows, lte_username, gte_username, lambda x: x.id)\n\n    elif sort_key == \"display_name\":\n        lte_display_name, lt_id = split_cursor(before)\n        gte_display_name, gt_id = split_cursor(after)\n        items = get_users_ordered_by_display_name(\n            api_client_id=api_client_id,\n            gte_display_name=gte_display_name,\n            gt_id=gt_id,\n            lte_display_name=lte_display_name,\n            lt_id=lt_id,\n            auth_method=auth_method,\n            search_text=search_text,\n            max_count=qry_max_count,\n            desc=desc,\n            api_client=api_client,\n            db=db,\n        )\n        items, num_rows = remove_extra_item(items, lte_display_name, gte_display_name)\n        p, n = get_next_prev(num_rows, lte_display_name, gte_display_name, lambda x: x.display_name)\n\n    else:\n        raise OasstError(f\"Unsupported sort key: '{sort_key}'\", OasstErrorCode.SORT_KEY_UNSUPPORTED)\n\n    return protocol.FrontEndUserPage(prev=p, next=n, sort_key=sort_key, order=\"asc\", items=items)\n\n\n@router.get(\"/{user_id}\", response_model=protocol.FrontEndUser)\ndef get_user(\n    user_id: UUID,\n    api_client_id: UUID = None,\n    db: Session = Depends(deps.get_db),\n    api_client: ApiClient = Depends(deps.get_api_client),\n):\n    \"\"\"\n    Get a user by global user ID. Only trusted clients can resolve users they did not register.\n    \"\"\"\n    ur = UserRepository(db, api_client)\n    user: User = ur.get_user(user_id, api_client_id)\n    return user.to_protocol_frontend_user()\n\n\n@router.put(\"/{user_id}\", status_code=HTTP_204_NO_CONTENT)\ndef update_user(\n    user_id: UUID,\n    display_name: Optional[str] = None,\n    enabled: Optional[bool] = None,\n    notes: Optional[str] = None,\n    show_on_leaderboard: Optional[bool] = None,\n    tos_acceptance: Optional[bool] = None,\n    db: Session = Depends(deps.get_db),\n    api_client: ApiClient = Depends(deps.get_trusted_api_client),\n):\n    \"\"\"\n    Update a user by global user ID. Only trusted clients can update users.\n    \"\"\"\n    ur = UserRepository(db, api_client)\n    ur.update_user(user_id, display_name, enabled, notes, show_on_leaderboard, tos_acceptance)\n\n\n@router.delete(\"/{user_id}\", status_code=HTTP_204_NO_CONTENT)\ndef delete_user(\n    user_id: UUID,\n    db: Session = Depends(deps.get_db),\n    api_client: ApiClient = Depends(deps.get_trusted_api_client),\n):\n    \"\"\"\n    Delete a user by global user ID. Only trusted clients can delete users.\n    User deletion anonymises the data of the user.\n    \"\"\"\n    ur = UserRepository(db, api_client)\n    ur.mark_user_deleted(user_id)\n\n\n@router.get(\"/{user_id}/messages\", response_model=list[protocol.Message])\ndef query_user_messages(\n    user_id: UUID,\n    api_client_id: UUID = None,\n    max_count: int = Query(10, gt=0, le=1000),\n    start_date: datetime.datetime = None,\n    end_date: datetime.datetime = None,\n    only_roots: bool = False,\n    desc: bool = True,\n    include_deleted: bool = False,\n    lang: Optional[str] = None,\n    frontend_user: deps.FrontendUserId = Depends(deps.get_frontend_user_id),\n    api_client: ApiClient = Depends(deps.get_api_client),\n    db: Session = Depends(deps.get_db),\n):\n    \"\"\"\n    Query user messages.\n    \"\"\"\n    pr = PromptRepository(db, api_client, frontend_user=frontend_user)\n    messages = pr.query_messages_ordered_by_created_date(\n        user_id=user_id,\n        api_client_id=api_client_id,\n        desc=desc,\n        limit=max_count,\n        gte_created_date=start_date,\n        lte_created_date=end_date,\n        only_roots=only_roots,\n        deleted=None if include_deleted else False,\n        lang=lang,\n    )\n\n    return utils.prepare_message_list(messages)\n\n\n@router.get(\"/{user_id}/messages/cursor\", response_model=protocol.MessagePage)\ndef query_user_messages_cursor(\n    user_id: Optional[UUID],\n    before: Optional[str] = None,\n    after: Optional[str] = None,\n    only_roots: Optional[bool] = False,\n    include_deleted: Optional[bool] = False,\n    max_count: Optional[int] = Query(10, gt=0, le=1000),\n    desc: Optional[bool] = False,\n    lang: Optional[str] = None,\n    frontend_user: deps.FrontendUserId = Depends(deps.get_frontend_user_id),\n    api_client: ApiClient = Depends(deps.get_api_client),\n    db: Session = Depends(deps.get_db),\n):\n    return get_messages_cursor(\n        before=before,\n        after=after,\n        user_id=user_id,\n        only_roots=only_roots,\n        include_deleted=include_deleted,\n        max_count=max_count,\n        desc=desc,\n        lang=lang,\n        frontend_user=frontend_user,\n        api_client=api_client,\n        db=db,\n    )\n\n\n@router.delete(\"/{user_id}/messages\", status_code=HTTP_204_NO_CONTENT)\ndef mark_user_messages_deleted(\n    user_id: UUID,\n    frontend_user: deps.FrontendUserId = Depends(deps.get_frontend_user_id),\n    api_client: ApiClient = Depends(deps.get_trusted_api_client),\n    db: Session = Depends(deps.get_db),\n):\n    pr = PromptRepository(db, api_client, frontend_user=frontend_user)\n    messages = pr.query_messages_ordered_by_created_date(user_id=user_id, limit=None)\n    pr.mark_messages_deleted(messages)\n\n\n@router.get(\"/{user_id}/stats\", response_model=dict[str, protocol.UserScore | None])\ndef query_user_stats(\n    user_id: UUID,\n    api_client: ApiClient = Depends(deps.get_api_client),\n    db: Session = Depends(deps.get_db),\n):\n    usr = UserStatsRepository(db)\n    return usr.get_user_stats_all_time_frames(user_id=user_id)\n\n\n@router.get(\"/{user_id}/stats/{time_frame}\", response_model=protocol.UserScore)\ndef query_user_stats_timeframe(\n    user_id: UUID,\n    time_frame: UserStatsTimeFrame,\n    api_client: ApiClient = Depends(deps.get_api_client),\n    db: Session = Depends(deps.get_db),\n):\n    usr = UserStatsRepository(db)\n    return usr.get_user_stats_all_time_frames(user_id=user_id)[time_frame.value]\n\n\n@router.get(\"/{user_id}/stats/{time_frame}/window\", response_model=protocol.LeaderboardStats | None)\ndef query_user_stats_timeframe_window(\n    user_id: UUID,\n    time_frame: UserStatsTimeFrame,\n    window_size: Optional[int] = Query(5, gt=0, le=100),\n    api_client: ApiClient = Depends(deps.get_api_client),\n    db: Session = Depends(deps.get_db),\n) -> protocol.LeaderboardStats | None:\n    ur = UserRepository(db, api_client=api_client)\n    user = ur.get_user(id=user_id)\n    usr = UserStatsRepository(db)\n    return usr.get_leaderboard_user_window(user=user, time_frame=time_frame, window_size=window_size)\n", "backend/tests/test_tree_manager_config.py": "from oasst_backend.config import TreeManagerConfiguration\n\n\ndef test_tree_manager_config():\n    \"\"\"\n    Just test that we can create a config\n    \"\"\"\n    TreeManagerConfiguration()\n", "backend/tests/__init__.py": "", "backend/tests/test_settings.py": "from oasst_backend.config import Settings\n\n\ndef test_create_default_settings():\n    \"\"\"\n    Make sure we can create one of these\n    \"\"\"\n    Settings()\n\n\ndef test_construct_db_uri_from_dict():\n    \"\"\"\n    No URI provided? Construct one from the other settings\n    \"\"\"\n\n    settings = Settings(\n        POSTGRES_USER=\"myuser\",\n        POSTGRES_PASSWORD=\"weak_password\",\n        POSTGRES_HOST=\"localhost\",\n        POSTGRES_PORT=54321,\n        POSTGRES_DB=\"mydb\",\n    )\n\n    assert str(settings.DATABASE_URI) == \"postgresql://myuser:weak_password@localhost:54321/mydb\"\n\n\ndef test_connection_string():\n    \"\"\"\n    If we provide a connection string, use that\n    \"\"\"\n\n    settings = Settings(DATABASE_URI=\"postgresql://myuser:weak_password@localhost:54321/mydb\")\n\n    assert str(settings.DATABASE_URI) == \"postgresql://myuser:weak_password@localhost:54321/mydb\"\n\n\ndef test_task_expiry_time():\n    \"\"\"\n    Should be two days\n    \"\"\"\n    settings = Settings()\n\n    two_days_in_minutes = 60 * 24 * 2\n\n    assert settings.TASK_VALIDITY_MINUTES == two_days_in_minutes\n", "data/__init__.py": "", "data/datasets/__init__.py": "TEXT_DATASETS = {\n    \"gutenberg_english\": \"sedthh/gutenberg_english\",  # Gutenberg eBooks in English\n    \"gutenberg_multilang\": \"sedthh/gutenberg_multilang\",  # Gutenberg eBooks in foreign languages\n    \"tv_dialogue\": \"sedthh/tv_dialogue\",  # TV and Movie dialogues and transcripts\n    \"fd_dialogue\": \"sedthh/fd_dialogue\",  # TV and Movie dialogues and transcripts from ForeverDreaming\n    \"tlcv2.0_oa\": \"pythainlp/tlcv2.0_oa\",  # Thai classical literature texts\n    \"fa-isna-news\": \"pourmand1376/isna-news\",  # Isna Persian News\n    \"fa-wikipedia\": \"pourmand1376/fa-wikipedia\",  # Farsi Wikipedia texts\n}\n\nINSTRUCTION_DATASETS = {\n    \"humaneval_mbpp_codegen_qa\": \"OllieStanley/humaneval-mbpp-codegen-qa\",\n    \"humaneval_mbpp_testgen_qa\": \"OllieStanley/humaneval-mbpp-testgen-qa\",\n    \"grade_school_math_instructions\": \"qwedsacf/grade-school-math-instructions\",\n    \"recipes\": \"dctanner/oa_recipes\",\n    \"ubuntu_dialogue_qa\": \"sedthh/ubuntu_dialogue_qa\",\n    \"cmu_wiki_qa\": \"sedthh/cmu_wiki_qa\",\n    \"youtube_subs_howto100M\": \"totuta/youtube_subs_howto100M\",\n    \"iapp_wiki_qa_squad\": \"wannaphong/iapp_wiki_qa_squad_oa\",\n    \"zhihu-kol\": \"wangrui6/zhihu-kol\",\n    \"tell_a_joke\": \"mikegarts/oa_tell_a_joke_20000\",\n    \"oa_wiki_qa_bart_10000row\": \"michaelthwan/oa_wiki_qa_bart_10000row\",\n    \"biostars_qa\": \"cannin/biostars_qa\",\n    \"oa_leet10k\": \"ehartford/oa_leet10k\",\n    \"LogicInference_OA\": \"KK04/LogicInference_OA\",\n    \"oa_dolly_15k\": \"OllieStanley/oa_dolly_15k\",\n    \"TSSB-3M\": \"zirui3/TSSB-3M-instructions\",\n    \"poetry_instruction\": \"checkai/instruction-poems\",\n    \"oa_stackexchange\": \"donfu/oa-stackexchange\",\n    \"stable_diffusion_instructional_dataset\": \"MadVoyager/stable_diffusion_instructional_dataset\",\n    \"ru_riddles_337\": \"0x22almostEvil/ru-riddles-377\",\n    \"instructional_codesearchnet_python\": \"Nan-Do/instructional_code-search-net-python\",\n    \"tatoeba_mt_qna_oa\": \"0x22almostEvil/tatoeba-mt-qna-oa\",\n    \"reasoning_bg_oa\": \"0x22almostEvil/reasoning_bg_oa\",\n    \"reasoning_gsm_qna_oa\": \"0x22almostEvil/reasoning-gsm-qna-oa\",\n    \"semantics_ws_qna_oa\": \"0x22almostEvil/semantics-ws-qna-oa\",\n}\n\nSAFETY_DATASETS = {\n    \"prosocial-dialog\": \"allenai/prosocial-dialog\",\n    \"prosocial-confessions\": \"shahules786/prosocial-confessions\",\n}\n\nMULTI_TURN_DIALOG_DATASETS = {}\n", "data/datasets/mt_note_generation/prepare.py": "import json\nimport math\nimport os\nimport random\nimport re\nimport sys\nfrom string import punctuation\n\nimport kaggle\nimport pandas as pd\n\nCLINICAL_NOTE_GENERATION_TEMPLATE = \"\"\"User: Write a clinical note about a patient with the following {section}: {section_information}.\nRosey: {note}\"\"\"\n\n\ndef preprocess(mt_dataset):\n    def filter_for_notes(row):\n        normalized_transcript = row[\"transcription\"].lower()\n        if \"chief complaint:\" in normalized_transcript:\n            return True\n        return False\n\n    mt_dataset = mt_dataset.dropna(subset=[\"description\", \"transcription\"])\n    mt_note_subset = mt_dataset[mt_dataset.apply(filter_for_notes, axis=1)]\n    return mt_note_subset\n\n\ndef is_chief_complaint(section):\n    return \"chief complaint\" in section.lower()\n\n\ndef get_conversations(dataset):\n    def normalize_transcript(x):\n        x = re.sub(r\"\\.+\", \".\", x)\n        x = re.sub(r\"\\,+\", \",\", x)\n        x = re.sub(r\":\\s+\", \": \", x)\n        x = re.sub(r\"\\.\\s+\", \". \", x)\n        x = re.sub(r\":(\\s)*\\,+\", \": \", x)\n        x = re.sub(r\"\\.\\,+\", \". \", x)\n        return x\n\n    conversations = []\n    for idx in range(len(dataset)):\n        transcript = normalize_transcript(dataset.iloc[idx][\"transcription\"])\n        sections = re.findall(r\"\\b[A-Z]+(?: [A-Z]+)*:\", transcript)\n        if len(sections) >= 2:\n            note_prompt = transcript.split(sections[0])[1].split(sections[1])[0]\n        else:\n            continue\n        section_name = sections[0].lower().strip(punctuation)\n        if len(note_prompt.split(\" \")) > 30 and is_chief_complaint(section_name):\n            # There are some chief complaints that seem to be HPI\n            section_name = \"history of present illness\"\n        conversations.append(\n            CLINICAL_NOTE_GENERATION_TEMPLATE.format(\n                section=section_name, section_information=note_prompt, note=transcript\n            )\n        )\n    return conversations\n\n\ndef main(output_dir: str = \"data\"):\n    \"\"\"Download and prepare the dataset for use.\"\"\"\n    os.makedirs(output_dir, exist_ok=True)\n    kaggle.api.dataset_download_files(\"tboyle10/medicaltranscriptions\", \"data\", unzip=True)\n    mt_samples = preprocess(pd.read_csv(\"data/mtsamples.csv\"))\n    conversations = get_conversations(mt_samples)\n    random.shuffle(conversations)\n    train_limit = math.ceil(len(conversations) * 0.6)\n    dev_limit = math.ceil(len(conversations) * 0.8)\n    train, validation, test = (\n        conversations[:train_limit],\n        conversations[train_limit:dev_limit],\n        conversations[dev_limit:],\n    )\n    splits = {\"train\": train, \"validation\": validation, \"test\": test}\n    for split in [\"train\", \"validation\", \"test\"]:\n        with open(f\"{output_dir}/mt_note_generation_{split}.jsonl\", \"w\") as f:\n            for conversation in splits[split]:\n                f.write(f\"{json.dumps({'conversation': conversation})}\\n\")\n\n\nif __name__ == \"__main__\":\n    sys.exit(main())\n", "data/datasets/mt_note_generation/hub.py": "from dataclasses import dataclass\n\nimport datasets\n\n\n@dataclass\nclass OpenAssistantConfig(datasets.BuilderConfig):\n    \"\"\"BuilderConfig for OpenAssistant datasets.\"\"\"\n\n    name: str = None\n    version: datasets.Version = None\n    description: str = None\n    schema: str = None\n    subset_id: str = None\n\n\nfeatures = datasets.Features(\n    {\n        \"conversation\": datasets.Value(\"string\"),\n    }\n)\n", "data/datasets/mt_note_generation/__init__.py": "", "data/datasets/mt_note_generation/mt_note_generation.py": "# Copyright 2023 The OpenAssistant Authors and the current dataset script contributor.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\nMT Note Generation is a set of synthetic dialogues between Assistant and\nUser where the user asks the assistant to generate a clinical note for a patient persona.\n\"\"\"\n\nimport json\nfrom typing import Dict, List, Tuple\n\nimport datasets\n\nfrom .hub import OpenAssistantConfig, features\n\n_CITATION = \"\"\"\\\n @misc{transcribed medical transcription sample reports and examples, title={Welcome to MTSamples},\n url={https://mtsamples.com/},\n journal={Transcribed Medical Transcription Sample Reports and Examples}}\n\"\"\"\n\n_DATASETNAME = \"mt_note_generation\"\n_DISPLAYNAME = \"MT Samples Note Generation\"\n\n_DESCRIPTION = \"\"\"\\\nA dataset of instructions for generating clinical notes from MT samples.\n\"\"\"\n\n_HOMEPAGE = \"\"\n\n_LICENSE = \"mit\"\n\n_URLS = {\n    _DATASETNAME: {\n        \"train\": \"./data/mt_note_generation_train.jsonl\",\n        \"test\": \"./data/mt_note_generation_test.jsonl\",\n        \"validation\": \"./data/mt_note_generation_validation.jsonl\",\n    }\n}\n\n_SUPPORTED_TASKS = [\"dialogue-modeling\"]\n\n_VERSION = \"1.0.0\"\n\n\nclass MTNoteGenerationDataset(datasets.GeneratorBasedBuilder):\n    \"\"\"A set of dialogues synthesized from the MT Samples dataset.\"\"\"\n\n    VERSION = datasets.Version(_VERSION)\n\n    BUILDER_CONFIGS = [\n        OpenAssistantConfig(\n            name=f\"{_DATASETNAME}_dialogue_modeling\",\n            version=VERSION,\n            description=f\"OpenAssistant dataset config for {_DATASETNAME}\",\n            schema=\"dialogue_modeling\",\n            subset_id=_DATASETNAME,\n        )\n    ]\n\n    DEFAULT_CONFIG_NAME = f\"{_DATASETNAME}_dialogue_modeling\"\n\n    def _info(self) -> datasets.DatasetInfo:\n        return datasets.DatasetInfo(\n            description=_DESCRIPTION,\n            features=features,\n            homepage=_HOMEPAGE,\n            license=_LICENSE,\n            citation=_CITATION,\n        )\n\n    def _split_generators(self, dl_manager) -> List[datasets.SplitGenerator]:\n        urls = _URLS[_DATASETNAME]\n        data_dir = dl_manager.download_and_extract(urls)\n        return [\n            datasets.SplitGenerator(\n                name=datasets.Split.TRAIN,\n                # Whatever you put in gen_kwargs will be passed to _generate_examples\n                gen_kwargs={\n                    \"filepath\": data_dir,\n                    \"split\": \"train\",\n                },\n            ),\n            datasets.SplitGenerator(\n                name=datasets.Split.TEST,\n                gen_kwargs={\n                    \"filepath\": data_dir,\n                    \"split\": \"test\",\n                },\n            ),\n            datasets.SplitGenerator(\n                name=datasets.Split.VALIDATION,\n                gen_kwargs={\n                    \"filepath\": data_dir,\n                    \"split\": \"validation\",\n                },\n            ),\n        ]\n\n    def _generate_examples(self, filepath, split: str) -> Tuple[int, Dict]:\n        \"\"\"Yields examples as (key, example) tuples.\"\"\"\n        if self.config.schema == \"dialogue_modeling\":\n            key = 0\n            with open(filepath[split], \"r\", encoding=\"utf8\") as data:\n                while True:\n                    line = data.readline()\n                    if not line:\n                        return\n                    yield key, json.loads(line)\n                    key += 1\n", "data/datasets/youtube_subs_howto100M/youtube_subs_howto100M.py": "# Copyright 2023 The OpenAssistant Authors and the current dataset script contributor.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\nThis dataset is a set of instruction-response pairs from the HowTo100M dataset.\nIn each pair, the short instruction plays the role of Prompt,\nand a long sequence of response plays the role of Response.\n\"\"\"\n\nimport json\nfrom typing import Dict, List, Tuple\n\nimport datasets\n\nfrom .hub import OpenAssistantConfig, instruction_features\n\n_CITATION = \"\"\"\\\n@inproceedings{miech19howto100m,\n   title={How{T}o100{M}: {L}earning a {T}ext-{V}ideo {E}mbedding by {W}atching {H}undred {M}illion {N}arrated {V}ideo {C}lips},\n   author={Miech, Antoine and Zhukov, Dimitri and Alayrac, Jean-Baptiste and Tapaswi, Makarand and Laptev, Ivan and Sivic, Josef},\n   booktitle={ICCV},\n   year={2019},\n}\n\"\"\"\n\n_DATASETNAME = \"youtube_subs_howto100M\"\n_DISPLAYNAME = \"YouTube Subtitles of Instructions: HowTo100M\"\n_DESCRIPTION = \"A set of instruction-response pairs extracted from HowTo100M dataset\"\n_HOMEPAGE = \"https://www.di.ens.fr/willow/research/howto100m/\"\n_LICENSE = \"apache 2.0\"\n_URLS = {\n    _DATASETNAME: {\n        \"train\": \"./data/youtube_subs_howto100M_train.jsonl\",\n        \"test\": \"./data/youtube_subs_howto100M_test.jsonl\",\n        \"validation\": \"./data/youtube_subs_howto100M_validation.jsonl\",\n    }\n}\n_SUPPORTED_TASKS = [\"dialogue-modeling\"]\n_VERSION = \"1.0.0\"\n\n\nclass YouTubeSubsHowTo100MDataset(datasets.GeneratorBasedBuilder):\n    \"\"\"A set of instruction-response pairs extracted from HowTo100M dataset.\"\"\"\n\n    VERSION = datasets.Version(_VERSION)\n\n    BUILDER_CONFIGS = [\n        OpenAssistantConfig(\n            name=f\"{_DATASETNAME}_dialogue_modeling\",\n            version=VERSION,\n            description=f\"OpenAssistant dataset config for {_DATASETNAME}\",\n            schema=\"dialogue_modeling\",\n            subset_id=_DATASETNAME,\n        )\n    ]\n\n    DEFAULT_CONFIG_NAME = f\"{_DATASETNAME}_dialogue_modeling\"\n\n    def _info(self) -> datasets.DatasetInfo:\n        return datasets.DatasetInfo(\n            description=_DESCRIPTION,\n            features=instruction_features,\n            homepage=_HOMEPAGE,\n            license=_LICENSE,\n            citation=_CITATION,\n        )\n\n    def _split_generators(self, dl_manager) -> List[datasets.SplitGenerator]:\n        urls = _URLS[_DATASETNAME]\n        data_dir = dl_manager.download_and_extract(urls)\n\n        return [\n            datasets.SplitGenerator(\n                name=datasets.Split.TRAIN,\n                gen_kwargs={\"filepath\": data_dir, \"split\": \"train\"},\n            ),\n            datasets.SplitGenerator(\n                name=datasets.Split.TEST,\n                gen_kwargs={\"filepath\": data_dir, \"split\": \"test\"},\n            ),\n            datasets.SplitGenerator(\n                name=datasets.Split.VALIDATION,\n                gen_kwargs={\"filepath\": data_dir, \"split\": \"validation\"},\n            ),\n        ]\n\n    def _generate_examples(self, filepath, split: str) -> Tuple[int, Dict]:\n        \"\"\"Yields examples as (key, example) tuples.\"\"\"\n        if self.config.schema == \"dialogue_modeling\":\n            key = 0\n            with open(filepath[split], \"r\", encoding=\"utf8\") as data:\n                while True:\n                    line = data.readline()\n                    if not line:\n                        return\n                    yield key, json.loads(line)\n                    key += 1\n\n\n# This allows you to run your dataloader with `python [dataset_name].py` during development\n# TODO: Remove this before making your PR\nif __name__ == \"__main__\":\n    datasets.load_dataset(__file__)\n", "data/datasets/youtube_subs_howto100M/prepare.py": "import io\nimport json\nimport math\nimport os\nimport pickle\nimport random\nimport re\nimport sys\nimport urllib\nimport zipfile\nfrom typing import List\n\nimport requests\nfrom tqdm import tqdm\nfrom youtube_transcript_api import YouTubeTranscriptApi\n\n\ndef get_video_ids(raw_file: str, video_id_pattern: str) -> List[str]:\n    video_ids = []\n    overlap = \"\"\n    with open(raw_file, \"r\") as f:\n        while True:\n            chunk = f.read(100000)  # arbitrary chunk size\n            if not chunk:\n                break\n            chunk = overlap + chunk\n            match = re.findall(video_id_pattern, chunk)\n            if match:\n                for vid in match:\n                    video_ids.append(vid.strip(\"'\\\"\"))\n            overlap = chunk[-10:]  # in case video_id is split between chunks\n    return list(set(video_ids))  # dedup\n\n\ndef get_title(video_id):\n    params = {\"format\": \"json\", \"url\": f\"https://www.youtube.com/watch?v={video_id}\"}\n    url = \"https://www.youtube.com/oembed\"\n    query_string = urllib.parse.urlencode(params)\n    url = url + \"?\" + query_string\n    try:\n        with urllib.request.urlopen(url) as response:\n            response_text = response.read()\n            data = json.loads(response_text.decode())\n            title = data[\"title\"]\n    except urllib.request.HTTPError:\n        title = None\n    return title\n\n\ndef generate_instruction(title: str) -> str:\n    # TODO: Ask a generative LM, \"Can you rephrase the title of {title} into a request form?\"\n    title = title.lower()\n    if \"how to\" in title:\n        return \"Please explain \" + title[title.index(\"how to\") :]\n\n\ndef get_subs(video_id, languages=[\"en\"]):\n    try:\n        subs_dump = YouTubeTranscriptApi.get_transcript(video_id, languages=languages)\n        subs = \"\"\n        for utterence in subs_dump:\n            subs += utterence[\"text\"] + \" \"\n        # TODO: add punctuation\n    except urllib.request.HTTPError:\n        subs = None\n    return subs\n\n\ndef main(output_dir: str = \"data\"):\n    \"\"\"Download and prepare the dataset for use.\"\"\"\n    os.makedirs(output_dir, exist_ok=True)\n\n    if not os.path.exists(\"./temp/raw_caption.json\"):\n        print(\"Downloading HowTo100M raw_caption.zip...\")\n        print(\" might take some time(3.4G)...\")\n        url = \"https://www.rocq.inria.fr/cluster-willow/amiech/howto100m/raw_caption.zip\"\n        response = requests.get(url)\n        zipped = zipfile.ZipFile(io.BytesIO(response.content))\n        zipped.extractall(\"./temp\")\n\n    if not os.path.exists(\"./temp/video_ids.pkl\"):\n        print(\"Retrieving video ids...\")\n        filename = \"./temp/raw_caption.json\"\n        video_id_pattern = '\"[0-9A-Za-z_-]{11}\"'\n        video_ids = get_video_ids(filename, video_id_pattern)\n        with open(\"./temp/video_ids.pkl\", \"wb\") as f:\n            pickle.dump(video_ids, f)\n    else:\n        with open(\"./temp/video_ids.pkl\", \"rb\") as f:\n            video_ids = pickle.load(f)\n\n    print(\"Extracting instruction-response pairs...\")\n    dataset = []\n    for video_id in tqdm(video_ids):\n        title = get_title(video_id)\n        if title is None:  # video is not available any more\n            continue\n        else:\n            instruction = generate_instruction(title)\n            if instruction:\n                response = get_subs(video_id)\n                dataset.append({\"instruction\": instruction, \"response\": response, \"source\": \"YouTube\"})\n    print(f\"Total {len(dataset)} pairs extracted.\")\n\n    print(\"Splitting and saving data...\")\n    random.shuffle(dataset)\n    train_limit = math.ceil(len(dataset) * 0.6)  # TODO: parameterize ratios\n    dev_limit = math.ceil(len(dataset) * 0.8)\n    train, validation, test = (\n        dataset[:train_limit],\n        dataset[train_limit:dev_limit],\n        dataset[dev_limit:],\n    )\n    splits = {\"train\": train, \"validation\": validation, \"test\": test}\n    for split in [\"train\", \"validation\", \"test\"]:\n        with open(f\"{output_dir}/youtube_subs_howto100M_{split}.jsonl\", \"w\") as f:\n            for datapoint in splits[split]:\n                f.write(f\"{json.dumps(datapoint)}\\n\")\n    print(\"All Done!\")\n\n\nif __name__ == \"__main__\":\n    sys.exit(main())\n", "data/datasets/youtube_subs_howto100M/hub.py": "from dataclasses import dataclass\n\nimport datasets\n\n\n@dataclass\nclass OpenAssistantConfig(datasets.BuilderConfig):\n    \"\"\"BuilderConfig for OpenAssistant datasets.\"\"\"\n\n    name: str = None\n    version: datasets.Version = None\n    description: str = None\n    schema: str = None\n    subset_id: str = None\n\n\ninstruction_features = datasets.Features(\n    {\n        \"instruction\": datasets.Value(\"string\"),\n        \"response\": datasets.Value(\"string\"),\n        \"source\": datasets.Value(\"string\"),\n    }\n)\n", "data/datasets/youtube_subs_howto100M/__init__.py": "", "data/datasets/soda_synthetic_dialogue/prepare.py": "\"\"\"Prepare the SODA Synthetic Dialogue Dataset\"\"\"\n\nimport json\nimport os\nimport random\nimport sys\n\nfrom datasets import load_dataset\nfrom tqdm import tqdm\n\n# adapted from https://colab.research.google.com/drive/1Sw3px5dP8whdqT7QMNoqwmqIasZkMbJi?usp=sharing\n\nSUMMARY_TEMPLATE = \"\"\"User: Can you give me a short story description for this dialogue?\n  {dialogue}\nAssistant: Sure, a short story description for this dialogue could be:\n  {story}\nUser: And a title?\nAssistant: Sure, a title for this dialogue could be:\n  {title}\"\"\"\n\nTHEME_TEMPLATE = \"\"\"\nUser: What would be one theme of this story?\nAssistant: One theme of this story could be:\n  {theme}\"\"\"\n\nNEW_DIALOGUE_TEMPLATE = \"\"\"User: Can you write a short dialogue based on this story:\n  {story}\nAssistant: Sure, a dialogue for this story could be:\n  {dialogue}\nUser: And a title?\nAssistant: Sure, a title for this dialogue could be:\n  {title}\"\"\"\n\nNEXT_LINES_TEMPLATE = \"\"\"User: Can you write the next few lines of dialogue for this scene:\n  {scene}\nAssistant: Sure, the next dialogue for this scene could be:\n  {dialogue}\nUser: And a title?\nAssistant: Sure, a title for this dialogue could be:\n  {title}\nUser: How about a short description?\nAssistant: Sure, a short description for this dialogue could be:\n  {story}\"\"\"\n\nNEW_STORY_AND_DIALOGUE_TEMPLATE = \"\"\"User: Can you write a short story and dialogue about:\n  {title1}\nAssistant: Sure, a short story and dialogue about: \"{title1}\" could be:\n  {story}\"\"\"\n\nFULL_DIALOGUE_TEMPLATE = \"\"\"{conversation}\n  {dialogue}\"\"\"\n\nMORE_DIALOGUE_TEMPLATE = \"\"\"{conversation}\n  {dialogue1}\nUser: Can you provide more dialogue assuming \"{title2}\"?\nAssistant: Sure, the next dialogue for this scene could be:\n  {dialogue2}\"\"\"\n\nNEXT_DIALOGUE_TEMPLATE = \"\"\"{conversation}\n  {dialogue1}\nUser: More please.\nAssistant: Sure, the next dialogue for this scene could be:\n  {dialogue2}\"\"\"\n\nNEW_STORY_AND_DIALOGUE_FROM_THEME_TEMPLATE = \"\"\"User: Can you write short story and dialogue based on the theme:\n  {theme}\nAssistant: Sure, a short story and dialogue based on the theme \"{theme}\" could be:\n  {story}\n  {dialogue}\nUser: And a title?\nAssistant: Sure, a title for this dialogue could be:\n  {title}\"\"\"\n\nPRINT = len(sys.argv) > 1 and sys.argv[1] == \"--print\"\n\n\ndef main(output_dir: str = \"data\"):\n    \"\"\"Download and prepare the dataset for use.\"\"\"\n\n    random.seed(42)\n    dataset = load_dataset(\"allenai/soda\")\n    os.makedirs(output_dir, exist_ok=True)\n\n    for split in [\"train\", \"test\", \"validation\"]:\n        with open(f\"{output_dir}/{split}.jsonl\", \"w\", encoding=\"utf8\") as output:\n            for i in tqdm(range(len(dataset[split])), desc=split):\n                dat = dataset[\"train\"][i]\n                title = dat[\"literal\"]\n                story = dat[\"narrative\"]\n\n                if dat[\"relation\"] == \"xWant\":\n                    theme = \"wanting \" + dat[\"tail\"]\n                elif dat[\"relation\"] == \"xNeed\":\n                    theme = \"needing \" + dat[\"tail\"]\n                elif not dat[\"tail\"].startswith(\"to \") and not dat[\"tail\"].startswith(\"and \"):\n                    theme = \"being \" + dat[\"tail\"]\n                elif dat[\"tail\"].startswith(\"and \"):\n                    theme = \"people are \" + dat[\"tail\"].replace(\"and PersonY \", \"\")\n                else:\n                    theme = dat[\"tail\"]\n                theme = theme.replace(\"PersonY\", \"another person\")\n                theme = theme.replace(\"being is\", \"being\")\n\n                dialogue = [s2 + \": \" + s1 for s1, s2 in zip(dat[\"dialogue\"], dat[\"speakers\"])]\n\n                if random.randint(0, 6) == 0:\n                    # print(\"##\")\n                    # print(f\"User: Can you give me a short story description for this dialog?\")\n                    # print(\"  \" + \"\\n  \".join(dialog))\n                    # print(f\"Assistant: Sure, a short story description for this dialog could be: \\n  {story}\")\n                    # print(\"User: And a title?\")\n                    # print(f\"Assistant: Sure, a title for this dialog could be: \\n  {title}\")\n                    # if theme:\n                    #     print(\"User: What would be one theme of this story?\")\n                    #     print(f'Assistant: One theme of this story could be: \"{theme}\"')\n                    conversation = SUMMARY_TEMPLATE.format(dialogue=\"\\n  \".join(dialogue), story=story, title=title)\n                    if theme:\n                        conversation = conversation + THEME_TEMPLATE.format(theme=theme)\n                elif random.randint(0, 6) == 0:\n                    # print(\"##\")\n                    # print(f\"User: Can you write a short dialog based on this story:\\n  {story}\")\n                    # print(f\"Assistant: Sure, a dialog for this story could be:\")\n                    # print(\"  \" + \"\\n  \".join(dialog))\n                    # print(\"User: And a title?\")\n                    # print(f\"Assistant: Sure, a title for this dialog could be: \\n  {title}\")\n                    # if theme:\n                    #     print(\"User: What would be one theme of this story?\")\n                    #     print(f'Assistant: One theme of this story could be: \"{theme}\"')\n                    conversation = NEW_DIALOGUE_TEMPLATE.format(\n                        story=story, dialogue=\"\\n  \".join(dialogue), title=title\n                    )\n                    if theme:\n                        conversation = conversation + THEME_TEMPLATE.format(theme=theme)\n                elif random.randint(0, 3) == 0:\n                    # print(\"##\")\n                    # print(f\"User: Can you write the next few lines of dialog for this scene:\")\n                    # if random.randint(0, 1) == 0:\n                    #     print(\"  \" + \"\\n  \".join(dialog[:-5]))\n                    #     print(f\"Assistant: Sure, the next dialog for this scene could be:\")\n                    #     print(\"  \" + \"\\n  \".join(dialog[-5:]))\n                    # elif random.randint(0, 1) == 0:\n                    #     print(\"  \" + \"\\n  \".join(dialog[:-3]))\n                    #     print(f\"Assistant: Sure, the next dialog for this scene could be:\")\n                    #     print(\"  \" + \"\\n  \".join(dialog[-3:]))\n                    # else:\n                    #     print(\"  \" + \"\\n  \".join(dialog[:-4]))\n                    #     print(f\"Assistant: Sure, the next dialog for this scene could be:\")\n                    #     print(\"  \" + \"\\n  \".join(dialog[-4:]))\n                    # print(\"User: And a title?\")\n                    # print(f\"Assistant: Sure, a title for this dialog could be: \\n  {title}\")\n                    # print(\"User: How about a short description?\")\n                    # print(f\"Assistant: Sure, a short description for this dialog could be: \\n  {story}\")\n                    # if theme:\n                    #     print(\"User: What would be one theme of this story?\")\n                    #     print(f'Assistant: One theme of this story could be: \"{theme}\"')\n                    if random.randint(0, 1) == 0:\n                        depth = -5\n                    elif random.randint(0, 1) == 0:\n                        depth = -3\n                    else:\n                        depth = -4\n                    conversation = NEXT_LINES_TEMPLATE.format(\n                        scene=\"\\n  \".join(dialogue[:depth]),\n                        dialogue=\"\\n  \".join(dialogue[depth:]),\n                        title=title,\n                        story=story,\n                    )\n                    if theme:\n                        conversation = conversation + THEME_TEMPLATE.format(theme=theme)\n                elif random.randint(0, 3) == 0:\n                    # print(\"##\")\n                    # title1 = title.split(\".\")[0]\n                    # title2 = title.split(\".\")[1]\n                    # print(f\"User: Can you write short story and dialog about: {title1}\")\n                    # print(f'Assistant: Sure, a short story and dialog about: \"{title1}\" could be:')\n                    # print(f\"  {story}\")\n                    # if random.randint(0, 1) == 0:\n                    #     print(\"  \" + \"\\n  \".join(dialog))\n                    # elif random.randint(0, 1) == 0 and len(dialog) > 5:\n                    #     print(\"  \" + \"\\n  \".join(dialog[:-5]))\n                    #     print(f'User: Can you provide more dialog assuming \"{title2}\"?')\n                    #     print(f\"Assistant: Sure, the next dialog for this scene could be:\")\n                    #     print(\"  \" + \"\\n  \".join(dialog[-5:]))\n                    # elif random.randint(0, 1) == 0:\n                    #     print(\"  \" + \"\\n  \".join(dialog[:-3]))\n                    #     print(\"User: more please.\")\n                    #     print(f\"Assistant: Sure, the next dialog for this scene could be:\")\n                    #     print(\"  \" + \"\\n  \".join(dialog[-3:]))\n                    # else:\n                    #     print(\"  \" + \"\\n  \".join(dialog[:-4]))\n                    #     print(f'User: Can you provide more dialog assuming \"{title2}\"?')\n                    #     print(f\"Assistant: Sure, the next dialog for this scene could be:\")\n                    #     print(\"  \" + \"\\n  \".join(dialog[-4:]))\n                    # if theme:\n                    #     print(\"User: What would be one theme of this story?\")\n                    #     print(f'Assistant: One theme of this story could be: \"{theme}\"')\n                    title1 = title.split(\".\")[0]\n                    title2 = title.split(\".\")[1]\n                    conversation = NEW_STORY_AND_DIALOGUE_TEMPLATE.format(title1=title1, story=story)\n                    if random.randint(0, 1) == 0:\n                        conversation = FULL_DIALOGUE_TEMPLATE.format(\n                            conversation=conversation, dialogue=\"\\n  \".join(dialogue)\n                        )\n                    elif random.randint(0, 1) == 0 and len(dialogue) > 5:\n                        conversation = MORE_DIALOGUE_TEMPLATE.format(\n                            conversation=conversation,\n                            dialogue1=\"\\n  \".join(dialogue[:-5]),\n                            title2=title2,\n                            dialogue2=\"\\n  \".join(dialogue[-5:]),\n                        )\n                    elif random.randint(0, 1) == 0:\n                        conversation = NEXT_DIALOGUE_TEMPLATE.format(\n                            conversation=conversation,\n                            dialogue1=\"\\n  \".join(dialogue[:-3]),\n                            dialogue2=\"\\n  \".join(dialogue[-3:]),\n                        )\n                    else:\n                        conversation = MORE_DIALOGUE_TEMPLATE.format(\n                            conversation=conversation,\n                            dialogue1=\"\\n  \".join(dialogue[:-4]),\n                            title2=title2,\n                            dialogue2=\"\\n  \".join(dialogue[-4:]),\n                        )\n                    if theme:\n                        conversation = conversation + THEME_TEMPLATE.format(theme=theme)\n                else:\n                    # print(\"##\")\n                    # print(f\"User: Can you write short story and dialog based on the theme:\\n  {theme}\")\n                    # print(f'Assistant: Sure, a short story and dialog based on the theme \"{theme}\" could be:')\n                    # print(f\"  {story}\")\n                    # print(\"  \" + \"\\n  \".join(dialog))\n                    # print(\"User: And a title?\")\n                    # print(f\"Assistant: Sure, a title for this dialog could be: \\n  {title}\")\n                    conversation = NEW_STORY_AND_DIALOGUE_FROM_THEME_TEMPLATE.format(\n                        theme=theme, story=story, dialogue=\"\\n  \".join(dialogue), title=title\n                    )\n                if PRINT:\n                    print(\"##\")\n                    print(conversation)\n\n                output.write(f\"{json.dumps({'conversation': conversation})}\\n\")\n\n\nif __name__ == \"__main__\":\n    sys.exit(main())\n", "data/datasets/soda_synthetic_dialogue/hub.py": "from dataclasses import dataclass\n\nimport datasets\n\n\n@dataclass\nclass OpenAssistantConfig(datasets.BuilderConfig):\n    \"\"\"BuilderConfig for OpenAssistant datasets.\"\"\"\n\n    name: str = None\n    version: datasets.Version = None\n    description: str = None\n    schema: str = None\n    subset_id: str = None\n\n\nfeatures = datasets.Features(\n    {\n        \"conversation\": datasets.Value(\"string\"),\n    }\n)\n", "data/datasets/soda_synthetic_dialogue/__init__.py": "", "data/datasets/soda_synthetic_dialogue/soda_synthetic_dialogue.py": "# Copyright 2023 The OpenAssistant Authors and the current dataset script contributor.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\nThis dataset is a set of dialogues synthesized from the SODA dataset.\nIn each dialogue, User and Assistant have a conversation about a story.\n\nThe original collab notebook for this dataset can be found at:\nhttps://colab.research.google.com/drive/1Sw3px5dP8whdqT7QMNoqwmqIasZkMbJi?usp=sharing\n\"\"\"\n\nimport json\nfrom typing import Dict, List, Tuple\n\nimport datasets\n\nfrom .hub import OpenAssistantConfig, features\n\n_CITATION = \"\"\"\\\n@article{ontocord2023sodasynth,\n  author    = {ontocord and Jeffrey Quesnelle},\n  title     = {SODA Synthetic Dialogue},\n  year      = {2023}\n}\n\"\"\"\n_DATASETNAME = \"soda_synthetic_dialogue\"\n_DISPLAYNAME = \"\ud83e\udd64SODA Synthetic Dialogue\"\n_DESCRIPTION = \"A set of dialogues synthesized from the SODA dataset.\"\n_HOMEPAGE = \"\"\n_LICENSE = \"mit\"\n_URLS = {\n    _DATASETNAME: {\"train\": \"./data/train.jsonl\", \"test\": \"./data/test.jsonl\", \"validation\": \"./data/validation.jsonl\"}\n}\n_SUPPORTED_TASKS = [\"dialogue-modeling\"]\n_VERSION = \"1.0.0\"\n\n\nclass SODASyntheticDialogueDataset(datasets.GeneratorBasedBuilder):\n    \"\"\"A set of dialogues synthesized from the SODA dataset.\"\"\"\n\n    VERSION = datasets.Version(_VERSION)\n\n    BUILDER_CONFIGS = [\n        OpenAssistantConfig(\n            name=f\"{_DATASETNAME}_dialogue_modeling\",\n            version=VERSION,\n            description=f\"OpenAssistant dataset config for {_DATASETNAME}\",\n            schema=\"dialogue_modeling\",\n            subset_id=_DATASETNAME,\n        )\n    ]\n\n    DEFAULT_CONFIG_NAME = f\"{_DATASETNAME}_dialogue_modeling\"\n\n    def _info(self) -> datasets.DatasetInfo:\n        return datasets.DatasetInfo(\n            description=_DESCRIPTION,\n            features=features,\n            homepage=_HOMEPAGE,\n            license=_LICENSE,\n            citation=_CITATION,\n        )\n\n    def _split_generators(self, dl_manager) -> List[datasets.SplitGenerator]:\n        \"\"\"Returns SplitGenerators.\"\"\"\n\n        urls = _URLS[_DATASETNAME]\n        data_dir = dl_manager.download_and_extract(urls)\n\n        return [\n            datasets.SplitGenerator(\n                name=datasets.Split.TRAIN,\n                gen_kwargs={\"filepath\": data_dir, \"split\": \"train\"},\n            ),\n            datasets.SplitGenerator(\n                name=datasets.Split.TEST,\n                gen_kwargs={\"filepath\": data_dir, \"split\": \"test\"},\n            ),\n            datasets.SplitGenerator(\n                name=datasets.Split.VALIDATION,\n                gen_kwargs={\"filepath\": data_dir, \"split\": \"validation\"},\n            ),\n        ]\n\n    def _generate_examples(self, filepath, split: str) -> Tuple[int, Dict]:\n        \"\"\"Yields examples as (key, example) tuples.\"\"\"\n\n        if self.config.schema == \"dialogue_modeling\":\n            key = 0\n            with open(filepath[split], \"r\", encoding=\"utf8\") as data:\n                while True:\n                    line = data.readline()\n                    if not line:\n                        return\n                    yield key, json.loads(line)\n                    key += 1\n", "data/datasets/oa_stackexchange/process.py": "#!/usr/bin/env python3\n# Simple script to convert StackExchange XML to Open Assistant format\n# Original code by https://github.com/b-mc2\n\nimport gc\nimport glob\nimport os\nimport re\nimport subprocess\nimport sys\n\nimport pandas as pd\nfrom html2text import html2text\nfrom lxml import etree\nfrom tqdm import tqdm\n\nXML_DIR = \"./xml\"\nSOURCE = \"stackexchange-{0}\"\nMAX_ANSWERS = 10\nQUESTION_SCORE_TRESHOLD = 0\nANSWER_SCORE_TRESHOLD = 0\nPARQUET_FILE = \"parquet/{0}.parquet\"\nMAX_LENGTH = 1000  # max length of question or answer\n\n\ndef main():\n    datasets = sys.argv[1:] if len(sys.argv) > 1 else list_cached_datasets()\n    for dataset in datasets:\n        process_dataset(dataset)\n\n\ndef list_cached_datasets():\n    xml_files = glob.glob(f\"{XML_DIR}/*.xml\")\n    datasets = [os.path.splitext(os.path.basename(file))[0] for file in xml_files]\n    datasets.sort()\n    return datasets\n\n\ndef process_dataset(dataset):\n    xml_file = f\"{XML_DIR}/{dataset}.xml\"\n    parquet_file = PARQUET_FILE.format(dataset)\n    source = SOURCE.format(dataset)\n    if not os.path.exists(xml_file):\n        print(f\"XML file {xml_file} not found, please download first. Skipping...\")\n    elif not os.path.exists(parquet_file):\n        df = parse_and_convert(xml_file, source)\n        save_parquet(df, dataset)\n    else:\n        print(f\"File already converted {xml_file}. Skipping...\")\n\n\ndef parse_and_convert(path: str, source: str):\n    \"\"\"\n    Parse (very large) XML files with sax parser and load it into a pandas Dataframe\n    \"\"\"\n    total_rows = int(subprocess.getoutput(f\"grep -c '<row' {path}\"))\n    print(f\"Parsing {total_rows} rows from {path}...\")\n    columns = \"Id PostTypeId Body Title Tags Score AcceptedAnswerId ParentId\"\n    rows = []\n    max_process = 10**6\n    processed = 0\n    oa_df = pd.DataFrame(columns=[\"INSTRUCTION\", \"RESPONSE\", \"SOURCE\", \"METADATA\"])\n\n    context = etree.iterparse(path, events=(\"end\",))\n\n    for _, element in tqdm(context, total=total_rows):\n        if element.tag == \"row\":\n            if len(element.get(\"Body\")) > MAX_LENGTH:\n                continue\n            rows.append(parse_row(element))\n            processed += 1\n            element.clear()\n            while element.getprevious() is not None:\n                del element.getparent()[0]\n\n            if processed % max_process == 0 or processed == total_rows:\n                df = pd.DataFrame(rows, columns=columns.split())\n                rows = []\n                oa = convert_to_oa(df, source)\n                oa_df = pd.concat([oa_df, oa])\n                del df\n                del oa\n                gc.collect()\n\n    return oa_df\n\n\ndef parse_row(element):\n    return [\n        int(element.get(\"Id\")),\n        int(element.get(\"PostTypeId\")),\n        element.get(\"Body\"),\n        element.get(\"Title\", \"\"),\n        element.get(\"Tags\", \"\"),\n        int(element.get(\"Score\", 0)),\n        int(element.get(\"AcceptedAnswerId\", 0)),\n        int(element.get(\"ParentId\", 0)),\n    ]\n\n\ndef convert_to_oa(all, source):\n    \"\"\"\n    Convert dataframe to Open Assistant format with INSTRUCTION, RESPONSE, SOURCE, METADATA columns\n\n    Only include questions with an AcceptedAnswerId\n    \"\"\"\n    questions = all[all[\"AcceptedAnswerId\"] != 0]\n    merged = pd.merge(\n        questions,\n        all,\n        how=\"inner\",\n        left_on=\"AcceptedAnswerId\",\n        right_on=\"Id\",\n        suffixes=(\"_q\", \"_a\"),\n    )\n\n    del all\n\n    merged[\"INSTRUCTION\"] = merged[\"Title_q\"] + \"\\n\" + merged[\"Body_q\"].apply(to_markdown)\n    merged[\"RESPONSE\"] = merged[\"Body_a\"].apply(to_markdown)\n    merged[\"SOURCE\"] = source\n    merged[\"METADATA\"] = merged.apply(create_metadata, axis=1)\n\n    return merged[[\"INSTRUCTION\", \"RESPONSE\", \"SOURCE\", \"METADATA\"]]\n\n\ndef convert_tags(raw):\n    return raw.replace(\"-\", \" \").replace(\"><\", \", \").replace(\"<\", \"\").replace(\">\", \"\")\n\n\ndef create_metadata(row):\n    return {\n        \"tags\": convert_tags(row[\"Tags_q\"]),\n        \"question_score\": row[\"Score_q\"],\n        \"answer_score\": row[\"Score_a\"],\n    }\n\n\ndef save_parquet(df, dataset):\n    \"\"\"\n    Save Dataframe to Parquet. See here for specs:\n    https://projects.laion.ai/Open-Assistant/docs/data/datasets#creating-a-dataset-on-hugging-face\n    \"\"\"\n    os.makedirs(\"parquet\", exist_ok=True)\n    parquet_file = PARQUET_FILE.format(dataset)\n    df.to_parquet(parquet_file, row_group_size=100, engine=\"pyarrow\", index=False)\n    print(f\"Converted {len(df)} instructions into {parquet_file}\")\n\n\nremove_markdown_links_pattern = r\"\\[([^\\]]+)\\]\\(([^\\)]+)\\)\"\nremove_remaining_links = r\"https?:\\/\\/[^\\s]+\"\n\n\ndef remove_emojis(string):\n    emoji_pattern = re.compile(\n        \"[\"\n        \"\\U0001F600-\\U0001F64F\"  # emoticons\n        \"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n        \"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n        \"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n        \"\\U00002702-\\U000027B0\"\n        \"\\U000024C2-\\U0001F251\"\n        \"]+\",\n        flags=re.UNICODE,\n    )\n    return emoji_pattern.sub(r\"\", string)\n\n\n# Replace HTML content to markdown but remove links\ndef to_markdown(text):\n    try:\n        text = html2text(text, bodywidth=0).strip()\n    except Exception as e:\n        print(e)\n        text = re.sub(r\"<[^>]*>\", \"\", str(text))\n    text = re.sub(remove_markdown_links_pattern, r\"\\1\", text)\n    text = remove_emojis(text)\n    return re.sub(remove_remaining_links, \"\", text)\n\n\nif __name__ == \"__main__\":\n    main()\n", "data/datasets/oa_stackexchange/combine.py": "#!/usr/bin/env python3\n# Combine (and shorten) parquet files into a single file\n\nimport glob\n\nimport pandas as pd\nfrom merge_parquets import merge_parquet_dir\n\nMAX_LENGTH = 1000  # max length of question or answer\n\nfor file in glob.glob(\"full/*.parquet\"):\n    df = pd.read_parquet(file)\n    before = len(df)\n    df = df[df[\"INSTRUCTION\"].str.len() < MAX_LENGTH]\n    df = df[df[\"RESPONSE\"].str.len() < MAX_LENGTH]\n    df[\"METADATA\"] = df[\"METADATA\"].apply(\n        lambda meta: {\n            \"tags\": meta[\"tags\"],\n            \"answer_score\": int(meta[\"answer_score\"]) if \"answer_score\" in meta and meta[\"answer_score\"] else 0,\n            \"question_score\": int(meta[\"question_score\"]) if \"question_score\" in meta and meta[\"question_score\"] else 0,\n        }\n    )\n    df.to_parquet(file)\n    after = len(df)\n    print(f\"Shortened {file} from {before} to {after} rows ({100 * after / before:.2f})\")\n\nmerge_parquet_dir(\"full\", \"stackexchange.parquet\")\n", "data/datasets/oa_stackexchange/merge_parquets.py": "from __future__ import annotations\n\nfrom pathlib import Path\nfrom typing import Callable, Iterable, TypeVar\n\nimport pyarrow as pa\nimport pyarrow.parquet as pq\n\n\ndef stream_to_parquet(path: Path, tables: Iterable[pa.Table]) -> None:\n    try:\n        first = next(tables)\n    except StopIteration:\n        return\n    schema = first.schema\n    with pq.ParquetWriter(path, schema) as writer:\n        writer.write_table(first)\n        for table in tables:\n            table = table.cast(schema)  # enforce schema\n            writer.write_table(table)\n\n\ndef stream_from_parquet(path: Path) -> Iterable[pa.Table]:\n    reader = pq.ParquetFile(path)\n    for batch in reader.iter_batches():\n        yield pa.Table.from_batches([batch])\n\n\ndef stream_from_parquets(paths: Iterable[Path]) -> Iterable[pa.Table]:\n    for path in paths:\n        yield from stream_from_parquet(path)\n\n\nT = TypeVar(\"T\")\n\n\ndef coalesce(items: Iterable[T], max_size: int, sizer: Callable[[T], int] = len) -> Iterable[list[T]]:\n    batch = []\n    current_size = 0\n    for item in items:\n        this_size = sizer(item)\n        if current_size + this_size > max_size:\n            yield batch\n            batch = []\n            current_size = 0\n        batch.append(item)\n        current_size += this_size\n    if batch:\n        yield batch\n\n\ndef coalesce_parquets(paths: Iterable[Path], outpath: Path, max_size: int = 2**20) -> None:\n    tables = stream_from_parquets(paths)\n    # Instead of coalescing using number of rows as your metric, you could\n    # use pa.Table.nbytes or something.\n    # table_groups = coalesce(tables, max_size, sizer=lambda t: t.nbytes)\n    table_groups = coalesce(tables, max_size)\n    coalesced_tables = (pa.concat_tables(group) for group in table_groups)\n    stream_to_parquet(outpath, coalesced_tables)\n\n\ndef merge_parquet_dir(path: str, outpath: Path) -> None:\n    paths = Path(path).glob(\"*.parquet\")\n    coalesce_parquets(paths, outpath)\n", "data/datasets/oa_stackexchange/upload.py": "#!/usr/bin/env python3\n# Simple script to convert StackExchange XML to Open Assistant format\n# Original code by https://github.com/b-mc2\n\nfrom datasets import load_dataset\n\nPARQUET_FILE = \"stackexchange.parquet\"\nHF_DATASET = \"donfu/oa-stackexchange\"\n\n\ndef upload_hf():\n    \"\"\"\n    Upload to Hugging Face. Make sure you are logged in beforehand with `huggingface-cli login`\n    \"\"\"\n    parquet_file = PARQUET_FILE\n    dataset = load_dataset(\"parquet\", data_files=parquet_file, name=\"oa-stackexchange\")\n    dataset.push_to_hub(HF_DATASET, max_shard_size=\"500MB\")\n    print(\"Uploaded to Hugging Face: \" + HF_DATASET)\n\n\nif __name__ == \"__main__\":\n    upload_hf()\n", "data/datasets/oa_stackexchange/stats.py": "import glob\nimport re\n\nfrom pyarrow.parquet import ParquetDataset\n\n\ndef rows(topic):\n    dataset = ParquetDataset(f\"parquet/{topic}.parquet\")\n    return sum(p.count_rows() for p in dataset.fragments)\n\n\nfor f in sorted(glob.glob(\"parquet/*.parquet\")):\n    topic = re.match(r\".*\\/(.*?)\\.parquet\", f)[1]\n    num = rows(topic)\n    print(f\"- {topic}: {int(num):,}\")\n", "data/datasets/oa_stackexchange/download.py": "#!/usr/bin/env python3\n#\n# Simple script to download StackExchange archive XML files with posts (threaded version)\n#\n# Note: you probably want to download stackoverflow.com-Posts.7z manually, as it is 18GB\n# and takes a days to download otherwise. You can try using the torrent:\n#\n# webtorrent https://archive.org/download/stackexchange/stackexchange_archive.torrent --select 658\n#\n\nimport concurrent.futures\nimport os\nimport re\n\nimport requests\nfrom bs4 import BeautifulSoup as bs\n\nBASE_URL = \"https://ia600107.us.archive.org/view_archive.php?archive=/27/items/stackexchange/{0}&file=Posts.xml\"\nDOWNLOAD_DIR = \"xml/\"\nNUM_PARALLEL = 20\nRE_IGNORE = r\"_meta|stackoverflow\\.com\\-\"\n\n\ndef get_all_filenames():\n    \"\"\"\n    Retrieve all urls from stackexchange archive.\n    This needs quite some mangling because of special cases (i.e. stackoverflow is not in one 7z archive).\n    Ignore meta files.\n    \"\"\"\n    response = requests.get(\"https://archive.org/download/stackexchange\")\n    if response.ok:\n        soup = bs(response.content, \"html.parser\")\n        table = soup.find(\"table\")\n        link_tags = table.find_all(\"a\")\n        urls = {\"stackoverflow\": \"https://archive.org/download/stackexchange/stackoverflow.com-Posts.7z\"}\n        for link in link_tags:\n            url = link[\"href\"]\n            name = url.split(\".stackexchange\")[0].replace(\".\", \"_\").replace(\"-\", \"_\")\n            name = name.replace(\"_com_7z\", \"\")\n            if url.endswith(\"7z\") and not re.search(RE_IGNORE, url):\n                urls[name] = BASE_URL.format(url)\n        return urls\n\n\ndef download_url(dataset_name: str, url: str):\n    os.makedirs(DOWNLOAD_DIR, exist_ok=True)\n    cache_path = os.path.join(DOWNLOAD_DIR, dataset_name + \".xml\")\n    if os.path.exists(cache_path):\n        print(\"Using cached: \", cache_path)\n        return cache_path\n    else:\n        print(\"Downloading xml: \", dataset_name)\n        response = requests.get(url)\n        print(\"Finished downloading: \", dataset_name)\n        with open(cache_path, \"wb\") as f:\n            f.write(response.content)\n        return cache_path\n\n\ndef download_all():\n    urls = get_all_filenames()\n    with concurrent.futures.ThreadPoolExecutor(max_workers=NUM_PARALLEL) as executor:\n        futures = [executor.submit(download_url, dataset, url) for dataset, url in urls.items()]\n\n    # Wait for all downloads to complete\n    concurrent.futures.wait(futures)\n    print(\"All downloads complete, except for the large stackoverflow XML file\")\n    print(\"Use torrent to download this one much quicker, then uncompress the 7z file\")\n    print(\"and move the extracted stackoverflow.com-Posts.xml to xml/stackoverflow.xml\")\n    print(\"webtorrent https://archive.org/download/stackexchange/stackexchange_archive.torrent --select 658\")\n\n\nif __name__ == \"__main__\":\n    download_all()\n", "data/datasets/TSSB-3M/generate_dataset.py": "\"\"\"Convert the source TSSB-3M  dataset to instruction data\n\"\"\"\n\nimport json\nimport random\nimport re\nfrom os.path import join\n\nfrom tqdm import tqdm\n\nINSTRUCTIONS_LIST = [\n    \"Find the bug in the following code:\",\n    \"Identify the error in the code snippet provided:\",\n    \"Spot the issue within the given code segment:\",\n    \"Locate the problem in the code example below:\",\n    \"Uncover the malfunction in the following piece of code:\",\n    \"Detect the flaw in the code provided:\",\n    \"Pinpoint the glitch in the code sample below:\",\n    \"Search for the anomaly in the given code:\",\n    \"Determine the defect within the following code:\",\n    \"Discover the fault in the code segment provided:\",\n    \"Trace the irregularity in the code example below:\",\n    \"Please locate the error in the code provided.\",\n    \"Can you identify the mistake in this code?\",\n    \"There seems to be a problem with this code. Can you find it?\",\n    \"Please investigate the code and locate the bug.\",\n    \"Please examine the code and find the error.\",\n    \"Can you pinpoint the issue with this code?\",\n    \"Please review the code and identify the bug.\",\n    \"Can you detect the problem with this code?\",\n    \"Please analyze the code and find the mistake.\",\n    \"Can you spot the bug in the code provided?\",\n]\n\n\nRESPONSE_PREFIX_WORDS = [\n    \"The fix of the bug can be laid out as\",\n    \"The resolution of the error can be portrayed like so\",\n    \"The solution for the flaw can be summarized as such\",\n    \"The remedy of the mistake can be captured in this way\",\n    \"The correction of the fault can be depicted like this\",\n    \"The patch for the glitch can be articulated as\",\n    \"The workaround of the defect can be conveyed in this manner\",\n    \"The troubleshooting of the issue can be explained like this\",\n    \"The adjustment to the anomaly can be illustrated as follows\",\n    \"The modification for the irregularity can be exemplified like this\",\n]\n\n\ndef gen_instruction():\n    idx = random.randint(0, len(INSTRUCTIONS_LIST) - 1)\n    return INSTRUCTIONS_LIST[idx]\n\n\ndef gen_response_prefix():\n    idx = random.randint(0, len(RESPONSE_PREFIX_WORDS) - 1)\n    return RESPONSE_PREFIX_WORDS[idx]\n\n\nTEMPLATE = \"\"\"User: {}\n{}\nReply: The fixed code is:\n```\n{}\n```\n\"\"\"\n\n\n# template for pretty output(multiple lines with `User:` & `Reply`)\nTEMPLATE_COMMIT_MSG = \"\"\"User: {}\n{}\nReply: {}:\n{}\nThe fixed code is:\n```\n{}\n```\n\"\"\"\n\nINSTRUCTON_TEMPLATE = \"\"\"{}\n{}\n\"\"\"\n\n\n# template for json output(value)\n\nRESPONSE_TEMPLATE = \"\"\"The fixed code is:\n```\n{}\n```\n\"\"\"\n\nRESPONSE_TEMPLATE_COMMIT_MSG = \"\"\"{}:\n{}\n\nThe fixed code is:\n```\n{}\n```\n\"\"\"\n\n\ndef remove_starting_plus_minus(text):\n    if text.startswith(\"+\") or text.startswith(\"-\"):\n        return text[1:]\n    else:\n        return text\n\n\ndef remove_extraneous_diff_info(text):\n    pattern = \"@@.*@@\"\n    return re.sub(pattern, \"\", text)\n\n\ndef clean(text):\n    return remove_extraneous_diff_info(remove_starting_plus_minus(text))\n\n\ndef clean_PII(text):\n    # Remove sign-off messege generated by `git commit --signoff`, eg. \"Signed-off-by: user_name <xx@yy.zz.com>\"\n    signoff_index = text.rfind(\"\\n\\nSigned-off-by:\")\n    if signoff_index != -1:\n        # Remove the sign-off string from the commit message\n        text = text[:signoff_index]\n\n    # remove email\n    email_pattern = r\"[a-zA-Z0-9._%+-]+@(?:[a-zA-Z0-9-]+\\.)+[a-zA-Z]{2,}\"\n    clean_text = re.sub(email_pattern, \"\", text)\n    return clean_text\n\n\nINVALID_COMMIT_MESSAGES = set([line.strip().split(\"\\t\")[0] for line in open(\"invalid_commit_messages.tsv\").readlines()])\n\n\ndef is_invaid_commit_msg(text):\n    \"\"\"commit message that is incomplete, eg. \"fix bug\", \"hotfix\" \"\"\"\n    return text.strip() in INVALID_COMMIT_MESSAGES\n\n\ndef clean_commit_msg(text):\n    \"\"\"\n    # 1. remove issue id , eg. msg: \"rename (hetr_passes -> passes) #1195\" -> \"rename (hetr_passes -> passes)\"\n    # 2. remove `fix` prefix:\n    some typical cases:\n    ## eg. [fix] \u62fc\u5199\u9519\u8bef -> \u62fc\u5199\u9519\u8bef\n    ## eg. [FIX] purchase_indonesia : AttributeError 'NoneType' object has no attribute 'id' ->  AttributeError 'NoneType' object has no attribute 'id'\n    ## \"fix force insert error refs #2\" -> \"fix force insert error\"\n    ## \"Fix namespace of RPCError Fixes #76\" ->  \"Fix namespace of RPCError\"\n    ## \"fix a minor bug in survey_spec password field handling see: #5477\" -> \"fix a minor bug in survey_spec password field handling\"\n    ## issue #973 -> \"\"\n    ## \"Fixes #246\"  -> \"\"\n    ## \"Close #152.\" -> \"\"\n    ## \"wrong learning rate schedule (#2360)\"  -> \"wrong learning rate schedule\"\n    \"\"\"\n    # filter commit message that contains PII(github user name/email..)\n    text = clean_PII(text)\n\n    # Remove issue id\n    pattern = r\"\\(?#\\d{1,6}\\)?\"\n    # re.sub(r\"(.+?\\s\\(.+?\\))\\s#\\d{1,6}\", '\\\\1', text)\n    text = re.sub(pattern, \"\", text)\n    # Replace multiple spaces with a single space\n    text = re.sub(r\"\\s+\", \" \", text).strip()\n\n    # filter commit message that is too short\n    if len(text) < 4:\n        return None\n\n    if is_invaid_commit_msg(text):\n        return None\n    return text\n\n\ndef create(input_file, output_file, output_json=True):\n    fout = open(output_file, \"w\")\n    with open(input_file) as fin:\n        for line in tqdm(fin):\n            row = json.loads(line.strip())\n            wrong = \"\\n\".join(clean(line) for line in row[\"diff\"].split(\"\\n\") if not line.startswith(\"+\"))\n            correct = \"\\n\".join(clean(line) for line in row[\"diff\"].split(\"\\n\") if not line.startswith(\"-\"))\n\n            instruction = INSTRUCTON_TEMPLATE.format(wrong, correct)\n\n            commit_msg = clean_commit_msg(row[\"commit_message\"]) if \"commit_message\" in row else None\n            if commit_msg:\n                # template: (instruct, wrong_code, resposne_prefix, commit_message, correct_code)\n                out_str = TEMPLATE_COMMIT_MSG.format(\n                    gen_instruction(), wrong, gen_response_prefix(), commit_msg, correct\n                )\n                response = RESPONSE_TEMPLATE_COMMIT_MSG.format(gen_response_prefix(), commit_msg, correct)\n            else:\n                # no commit message\n                out_str = TEMPLATE.format(gen_instruction(), wrong, correct)\n                response = RESPONSE_TEMPLATE.format(correct)\n\n            if output_json:\n                row = {\n                    \"INSTRUCTION\": instruction,\n                    \"RESPONSE\": response,\n                    \"SOURCE\": \"TSSM-3M\",\n                    \"METADATA\": {\n                        \"project_url\": row[\"project_url\"],\n                        \"file_path\": row[\"file_path\"],\n                        \"commit_sha\": row[\"commit_sha\"],\n                    },\n                }\n                out_str = json.dumps(row, ensure_ascii=False)\n\n            print(out_str, file=fout)\n        fout.close()\n\n\nif __name__ == \"__main__\":\n    \"\"\"\n    # get source data from huggingface repository\n     !wget https://huggingface.co/datasets/zirui3/TSSB-3M-ext/blob/main/data.jsonl.gz\n     !gzip -d data.jsonl.gz\n    \"\"\"\n\n    data_dir = \".\"\n    # source TSSB-3M data\n    input_file = join(data_dir, \"data.jsonl\")\n\n    # output multiple lines\n    # output_file = join(data_dir, \"instructions_multple_lines.txt\")\n    # create(input_file, output_file, output_json=False)\n\n    # output jsonl\n    output_file = join(data_dir, \"instructions.jsonl\")\n    create(input_file, output_file, output_json=True)\n", "data/datasets/TSSB-3M/load_script.py": "from datasets import load_dataset\n\nif __name__ == \"__main__\":\n    ds = load_dataset(\"zirui3/TSSB-3M-instructions\")\n    print(ds)\n", "data/datasets/logicreference_OA/generate_dataset.py": "# Copyright 2023 The Google Research Authors.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Driver file that generates IID/OOD/length splits.\n\"\"\"\n\n\nimport os\nimport random\n\nimport rules\nimport splits\nimport tensorflow as tf\nfrom absl import app\n\n# Generation parameters:\n# TARGET_FOLDER = \"/path/to/generate/dataset/\"\nTARGET_FOLDER = \"./e_txt/\"\nANSWER_AT_THE_END = True\nLENGTH_DISTRIBUTION = [0.425, 0.3, 0.2, 0.05, 0.025]\nN_INFERENCE_PROBLEMS = 10000\nN_VARIATIONS = 25\nN_EXAMPLES = 55000\nTRAIN_RATIO = 1\nLENGTH_SPLIT_THRESHOLD = 4\nRANDOM_SEED = 1111\n\n\ndef create_string_feature(values):\n    \"\"\"Creates TensorFlow string features.\n\n    Args:\n      values: A sequence of unicode strings.\n\n    Returns:\n      An entry of int tf.train.Feature.\n    \"\"\"\n    # Converts to `str` (in Python 2) and `bytes` (in Python 3) as\n    # `tf.train.Feature` only takes bytes.\n    values = [value.encode(\"utf-8\") for value in values]\n\n    feature = tf.train.Feature(bytes_list=tf.train.BytesList(value=values))\n    return feature\n\n\ndef generate_t5_split(path, file_name, examples):\n    print(f\"Generating split of size {len(examples)} at {path}\")\n    os.makedirs(path, exist_ok=True)\n    with open(os.path.join(path, file_name), \"w\") as f:\n        for example in examples:\n            f.write(f\"INSTRUCTION: {example.inputs}\\n\")\n            f.write(f\"RESPONSE: {example.targets}\\n\")\n            f.write(\"SOURCE: LogicInference Dataset e\\n\\n\")\n\n\ndef main(_):\n    rules.precompute_rules()\n\n    suffix = \"\"\n    if ANSWER_AT_THE_END:\n        suffix = \"_e\"\n    folder_iid_name = \"logic_inference_iid\" + suffix\n\n    # Generate each of the splits:\n    print(\"IID:\")\n    random.seed(RANDOM_SEED)\n    (train_examples, test_examples) = splits.generate_training_and_test_sets_iid(\n        N_INFERENCE_PROBLEMS,\n        N_VARIATIONS,\n        N_EXAMPLES,\n        TRAIN_RATIO,\n        length_distribution=LENGTH_DISTRIBUTION,\n        answer_at_the_end=ANSWER_AT_THE_END,\n    )\n    generate_t5_split(\n        os.path.join(TARGET_FOLDER, folder_iid_name),\n        f\"{folder_iid_name}-train_tf_examples-00000-of-00001\",\n        train_examples,\n    )\n    generate_t5_split(\n        os.path.join(TARGET_FOLDER, folder_iid_name),\n        f\"{folder_iid_name}-test_tf_examples-00000-of-00001\",\n        test_examples,\n    )\n\n    # print(\"OOD:\")\n    # random.seed(RANDOM_SEED)\n    # (train_examples, test_examples) = splits.generate_training_and_test_sets_ood(\n    #     N_INFERENCE_PROBLEMS, N_VARIATIONS, N_EXAMPLES, TRAIN_RATIO,\n    #     length_distribution=LENGTH_DISTRIBUTION,\n    #     answer_at_the_end=ANSWER_AT_THE_END)\n    # generate_t5_split(os.path.join(TARGET_FOLDER, folder_ood_name),\n    #                   f\"{folder_ood_name}-train_tf_examples-00000-of-00001\",\n    #                   train_examples)\n    # generate_t5_split(os.path.join(TARGET_FOLDER, folder_ood_name),\n    #                   f\"{folder_ood_name}-test_tf_examples-00000-of-00001\",\n    #                   test_examples)\n    #\n    # print(\"Length:\")\n    # random.seed(RANDOM_SEED)\n    # (train_examples,\n    #  test_examples) = splits.generate_training_and_test_sets_length(\n    #      N_INFERENCE_PROBLEMS,\n    #      N_VARIATIONS,\n    #      N_EXAMPLES,\n    #      LENGTH_SPLIT_THRESHOLD,\n    #      length_distribution=LENGTH_DISTRIBUTION,\n    #      answer_at_the_end=ANSWER_AT_THE_END)\n    # generate_t5_split(\n    #     os.path.join(TARGET_FOLDER, folder_length_name),\n    #     f\"{folder_length_name}-train_tf_examples-00000-of-00001\", train_examples)\n    # generate_t5_split(\n    #     os.path.join(TARGET_FOLDER, folder_length_name),\n    #     f\"{folder_length_name}-test_tf_examples-00000-of-00001\", test_examples)\n\n\nif __name__ == \"__main__\":\n    app.run(main)\n", "data/datasets/safety_directory/child_help/child_help.py": "# @title Child Helplines\n# Source : https://github.com/LAION-AI/Open-Instruction-Generalist/blob/main/OIG/src/child_help.py\n\"\"\"\nFrom https://childhelplineinternational.org/helplines/\nNOTE:\nThis list was created in early 2023 from the above website and may not be current.\n\"\"\"\n# thank you to OA discord user: someone13574 for creating this json data\nchild_hotline = {\n    \"Sauver l\u2019Enfant\": {\n        \"region\": \"Benin\",\n        \"page\": \"https://childhelplineinternational.org/benin-sauver-lenfant/\",\n        \"description\": \"Sauver l\u2019Enfant is a new child helpline operating in Benin\",\n        \"contacts\": {\"138\": {\"type\": \"phone\", \"link\": \"tel:11611\"}},\n    },\n    \"Childline Botswana\": {\n        \"region\": \"Botswana\",\n        \"page\": \"https://childhelplineinternational.org/botswana-childline-botswana-2/\",\n        \"description\": \"Childline Botswana\u2019s vision is to have a nation that is child friendly: that respects children\u2019s rights and does not abuse its children by the year 2036.\",\n        \"contacts\": {\n            \"Website\": {\"type\": \"website\", \"link\": \"http://childlinebotswana.org.bw\"},\n            \"11611\": {\"type\": \"phone\", \"link\": \"tel:11611\"},\n            \"3 900 900\": {\"type\": \"phone\", \"link\": \"tel:3900900\"},\n            \"Email\": {\"type\": \"email\", \"link\": \"mailto:info@childlinebotswana.org.bw\"},\n            \"Facebook\": {\"type\": \"facebook\", \"link\": \"https://www.facebook.com/ChildlineBotswanaTrust\"},\n            \"Twitter\": {\n                \"type\": \"twitter\",\n                \"link\": \"https://twitter.com/ChildlineBots?fbclid=IwAR1-vUiOPw0GpE3Q6j8HaD8QkXRLzb2wN1KtfM7tHvqjzoHrwuHWIM6OAkY\",\n            },\n            \"Youtube\": {\"type\": \"youtube\", \"link\": \"https://www.youtube.com/channel/UC96Bnc8a6DcxYbPWJI-QAgA\"},\n        },\n    },\n    \"Allo 116 \u2013 Ligne t\u00e9l\u00e9phonique d\u2019assistance aux enfants\": {\n        \"region\": \"Burkina Faso\",\n        \"page\": \"https://childhelplineinternational.org/burkina-faso-allo-116-ligne-telephonique-dassistance-aux-enfants/\",\n        \"contacts\": {\n            \"action-sociale.gov.bf\": {\"type\": \"website\", \"link\": \"http://www.action-sociale.gov.bf/\"},\n            \"116\": {\"type\": \"phone\", \"link\": \"tel:116\"},\n            \"+226 25 46 09 81\": {\"type\": \"phone\", \"link\": \"tel:+226%2025%2046%2009%2081\"},\n            \"Facebook\": {\"type\": \"facebook\", \"link\": \"https://www.facebook.com/MFSNFAH\"},\n            \"Youtube\": {\"type\": \"youtube\", \"link\": \"https://www.youtube.com/channel/UC2dzYVan9Zta6-hsJmw9fjg\"},\n        },\n    },\n    \"Yaga Ndakumva\": {\n        \"region\": \"Burundi\",\n        \"page\": \"https://childhelplineinternational.org/burundi-yaga-ndakumva/\",\n        \"description\": \"Yaga Ndakumva is a child helpline for the children of Burundi.\",\n        \"contacts\": {\n            \"Website\": {\"type\": \"website\", \"link\": \"http://www.droitshumains.gov.bi/\"},\n            \"116\": {\"type\": \"phone\", \"link\": \"tel:116\"},\n            \"Facebook\": {\"type\": \"facebook\", \"link\": \"https://www.facebook.com/ministeredroitshumains.genre\"},\n            \"Twitter\": {\"type\": \"twitter\", \"link\": \"https://twitter.com/GenreMinistere\"},\n        },\n    },\n    \"Lignes Vertes Cameroun\": {\n        \"region\": \"Cameroon\",\n        \"page\": \"https://childhelplineinternational.org/cameroon-lignes-vertes-cameroun/\",\n        \"description\": \"Lignes Vertes Cameroun is currently migrating to become a 116 operator. \",\n        \"contacts\": {\n            \"Website\": {\"type\": \"website\", \"link\": \"http://www.minproff.cm/liens-utiles/green-lines/\"},\n            \"116\": {\"type\": \"phone\", \"link\": \"tel:116\"},\n            \"222 23 25 50\": {\"type\": \"phone\", \"link\": \"tel:222%2023%2025%2050\"},\n            \"Facebook\": {\"type\": \"facebook\", \"link\": \"https://www.facebook.com/minproff.cameroun/\"},\n        },\n    },\n    \"Ligne Verte 116\": {\n        \"region\": \"C\u00f4te d\u2019Ivoire\",\n        \"page\": \"https://childhelplineinternational.org/cote-divoire-ligne-verte-116-allo-enfant-en-detresse/\",\n        \"description\": \"Minist\u00e8re de la Solidarit\u00e9, de la Famille, de la Femme, et de l\u2019Enfant\u2019s Ligne verte 116 is a free counseling and referral service for children. This service communicates through two numbers:\",\n        \"contacts\": {\n            \"http://www.famille.gouv.ci/\": {\"type\": \"website\", \"link\": \"http://www.famille.gouv.ci/\"},\n            \"116\": {\"type\": \"phone\", \"link\": \"tel:116\"},\n            \"1308\": {\"type\": \"phone\", \"link\": \"tel:1308\"},\n            \"mfpes.info@gmail.com\": {\"type\": \"email\", \"link\": \"http://mfpes.info@gmail.com\"},\n            \"Facebook\": {\"type\": \"facebook\", \"link\": \"https://www.facebook.com/profile.php?id=100067980595652\"},\n            \"Twitter\": {\"type\": \"twitter\", \"link\": \"https://twitter.com/msffegouvci\"},\n        },\n    },\n    \"Tukinge Watoto\": {\n        \"region\": \"Democratic Republic of Congo\",\n        \"page\": \"https://childhelplineinternational.org/democratic-republic-of-congo-tukinge-watoto/\",\n        \"description\": \"War Child UK runs a free helpline called Tukinge Watoto (meaning \u201cLet\u2019s Protect Children\u201d) to help provide support to at-risk children and make sure that law enforcers respect their rights.\",\n        \"contacts\": {\n            \"Website\": {\n                \"type\": \"website\",\n                \"link\": \"https://www.warchild.org.uk/our-work/where-we-work/democratic-republic-of-congo\",\n            },\n            \"117\": {\"type\": \"phone\", \"link\": \"tel:117\"},\n        },\n    },\n    \"Eswatini Ministry of Education Toll free number\": {\n        \"region\": \"Eswatini\",\n        \"page\": \"https://childhelplineinternational.org/eswatini-eswatini-ministry-of-education-toll-free-number/\",\n        \"description\": \"The Eswatini Ministry of Education Toll free number supports children and youth in need.\",\n        \"contacts\": {\n            \"Website\": {\"type\": \"website\", \"link\": \"https://www.govpage.co.za/swaziland-education-and-training.html\"},\n            \"116\": {\"type\": \"phone\", \"link\": \"tel:116\"},\n            \"Facebook\": {\"type\": \"facebook\", \"link\": \"http://www.facebook.com/Govpage\"},\n        },\n    },\n    \"SWAGAA 951 Help Line\": {\n        \"region\": \"Eswatini\",\n        \"page\": \"https://childhelplineinternational.org/eswatini-swagaa-951-help-line/\",\n        \"description\": \"The Swaziland Action Group Against Abuse is committed to eradicating sexual and physical abuse and their toll-free help line is for anyone seeking support surrounding gender-based violence, sexual assault and reporting abuse.\",\n        \"contacts\": {\n            \"Website\": {\"type\": \"website\", \"link\": \"http://www.swagaa.org.sz/\"},\n            \"951\": {\"type\": \"phone\", \"link\": \"tel:591\"},\n            \"Email\": {\"type\": \"email\", \"link\": \"mailto:swagaa@swagaa.org.sz\"},\n            \"Facebook\": {\n                \"type\": \"facebook\",\n                \"link\": \"https://www.facebook.com/Swaziland-Action-Group-Against-Abuse-SWAGAA-202248446510926/\",\n            },\n            \"Instagram\": {\"type\": \"instragram\", \"link\": \"https://www.instagram.com/swatiniactiongroupagainstabuse/\"},\n            \"Twitter\": {\"type\": \"twitter\", \"link\": \"https://twitter.com/swagaa951\"},\n            \"LinkedIn\": {\n                \"type\": \"linkedin\",\n                \"link\": \"https://www.linkedin.com/company/swaziland-action-group-against-abuse/about/\",\n            },\n        },\n    },\n    \"Adama Child Helpline (ECFA)\": {\n        \"region\": \"Ethiopia\",\n        \"page\": \"https://childhelplineinternational.org/ethiopia-adama-child-helpline-ecfa/\",\n        \"description\": \"ECFA works in partnership with community, national, and international partners, including families, government agencies, and local leaders, to prevent abuse and ensure the physical, mental, and social well-being of children and youth in Oromia region, Ethiopia. Operated by ECFA, the Adama Child Helpline provides advice, information and support to children and families.\",\n        \"contacts\": {\n            \"Website\": {\"type\": \"website\", \"link\": \"https://ecfaethiopia.org/\"},\n            \"919\": {\"type\": \"phone\", \"link\": \"tel:919\"},\n            \"+251221-117575\": {\"type\": \"phone\", \"link\": \"tel:+251221-117575\"},\n            \"Email\": {\"type\": \"email\", \"link\": \"mailto:info@ecfaethiopia.org\"},\n        },\n    },\n    \"Child Helpline Gambia\": {\n        \"region\": \"Gambia\",\n        \"page\": \"https://childhelplineinternational.org/gambia-child-helpline-gambia/\",\n        \"description\": \"Child Helpline Gambia is run by CEDAG (Children & Environmental Development Association), a nonprofit organization registered in Gambia with a mission to promote the principle of children\u2019s rights, women and community well-being.\",\n        \"contacts\": {\n            \"+2209940239\": {\"type\": \"phone\", \"link\": \"tel:+2209940239\"},\n            \"199\": {\"type\": \"phone\", \"link\": \"tel:199\"},\n            \"Facebook\": {\n                \"type\": \"facebook\",\n                \"link\": \"https://www.facebook.com/2434793426744326/posts/did-you-know-the-gambia-now-has-a-child-helpline-199-is-the-product-of-a-coordin/2756674237889575/\",\n            },\n        },\n    },\n    \"AMPCAN Ghana\": {\n        \"region\": \"Ghana\",\n        \"page\": \"https://childhelplineinternational.org/ghana-ampcan-ghana/\",\n        \"description\": \"AMPCAN is a non-governmental organization with a mission to improve the welfare of Ghanaian children and to enhance opportunities for the development of their full potential.\",\n        \"contacts\": {\n            \"Facebook\": {\"type\": \"facebook\", \"link\": \"https://www.facebook.com/Ampcan-Ghana-128016973924144/\"}\n        },\n    },\n    \"AGUIAS 116\": {\n        \"region\": \"Guinea\",\n        \"page\": \"https://childhelplineinternational.org/guinea-aguias-116/\",\n        \"description\": \"AGUIAS (Association Guin\u00e9enne des Assistantes Sociales) offers psychosocial support services, legal and medical assistance through its 116 child helpline.\",\n        \"contacts\": {\n            \"116\": {\"type\": \"phone\", \"link\": \"tel:116\"},\n            \" +224 621 75 35 35\": {\"type\": \"phone\", \"link\": \"tel:+224621753535\"},\n            \"Email\": {\"type\": \"email\", \"link\": \"mailto:cisseaguias@gmail.com\"},\n            \"Facebook\": {\"type\": \"facebook\", \"link\": \"https://www.facebook.com/aguias116\"},\n        },\n    },\n    \"Childline Kenya\": {\n        \"region\": \"Kenya\",\n        \"page\": \"https://childhelplineinternational.org/kenya-childline-kenya/\",\n        \"description\": \"Childline Kenya works in partnership with the Government to stop child abuse and provide a safe environment for all children. It offers the only nationwide helpline service dedicated to children that runs 24 hours toll free, and is accessible by simply dialing 116.\",\n        \"contacts\": {\n            \"Website\": {\"type\": \"website\", \"link\": \"http://www.childlinekenya.co.ke/\"},\n            \"116\": {\"type\": \"phone\", \"link\": \"tel:116\"},\n            \"WhatsApp\": {\"type\": \"whatsapp\", \"link\": \"tel:+254722116116\"},\n            \"Email\": {\"type\": \"email\", \"link\": \"mailto:info@childlinekenya.co.ke\"},\n            \"Facebook\": {\"type\": \"facebook\", \"link\": \"https://www.facebook.com/ChildlineKenya/\"},\n            \"Instagram\": {\"type\": \"instragram\", \"link\": \"https://www.instagram.com/childlinekenya/?hl=en\"},\n            \"Twitter\": {\"type\": \"twitter\", \"link\": \"https://twitter.com/childlinekenya\"},\n        },\n    },\n    \"Child Helpline Lesotho\": {\n        \"region\": \"Lesotho\",\n        \"page\": \"https://childhelplineinternational.org/lesotho-child-helpline-lesotho/\",\n        \"description\": \"Child Helpline Lesotho is a free counseling and referral service for children that operates the 116 pan-African number.\",\n        \"contacts\": {\"116\": {\"type\": \"phone\", \"link\": \"tel:116\"}},\n    },\n    \"My Voice, My Safety\": {\n        \"region\": \"Liberia\",\n        \"page\": \"https://childhelplineinternational.org/liberia-my-voice-my-safety/\",\n        \"description\": \"My Voice, My Safety is part of the Ministry of Gender, Children and Social Development in Liberia and provides counseling services to survivors of SGBV/HTP and their families to manage trauma, stigma, neglect and /or loss, depression, anxiety and other mental health concerns.\",\n        \"contacts\": {\n            \"+231886521443\": {\"type\": \"phone\", \"link\": \"tel:+231886521443\"},\n            \"+231777521443  \": {\"type\": \"phone\", \"link\": \"tel:+231777521443%20%20\"},\n            \"Email\": {\"type\": \"email\", \"link\": \"mailto:info@myvoicemysafety.org\"},\n            \"Facebook\": {\"type\": \"facebook\", \"link\": \"https://www.facebook.com/MOGCSPLiberia/\"},\n        },\n    },\n    \"Ligne Verte 147 Madagascar\": {\n        \"region\": \"Madagascar\",\n        \"page\": \"https://childhelplineinternational.org/madagascar-ligne-verte-147-madagascar/\",\n        \"description\": \"Ligne Verte 147 is a child helpline for reporting cases of mistreatment, violence, abuse and exploitation against children and is is free, available 24/7 and accessible everywhere in Madagascar.\",\n        \"contacts\": {\n            \"Website\": {\"type\": \"website\", \"link\": \"https://arozaza.mg/\"},\n            \"147\": {\"type\": \"phone\", \"link\": \"tel:147\"},\n            \"Email\": {\"type\": \"email\", \"link\": \"mailto:147@arozaza.mg\"},\n            \"Facebook\": {\"type\": \"facebook\", \"link\": \"https://www.facebook.com/LigneVerte147/\"},\n        },\n    },\n    \"Tithandizane Helpline 116\": {\n        \"region\": \"Malawi\",\n        \"page\": \"https://childhelplineinternational.org/malawi-tithandizane-helpline-116/\",\n        \"description\": \"Tithandizane Helpline 116 is operated by YONECO, a non-governmental organization committed to empowering the youth, women and children; promoting good health, human rights and democracy; adapting and mitigating the impact of climate change; and conducting research for evidence-based programming and advocacy.\",\n        \"contacts\": {\n            \"Website\": {\"type\": \"website\", \"link\": \"https://yoneco.org/\"},\n            \"116\": {\"type\": \"phone\", \"link\": \"tel:116\"},\n            \"Email\": {\"type\": \"email\", \"link\": \"mailto:%20executive@yoneco.org.mw\"},\n            \"Facebook\": {\"type\": \"facebook\", \"link\": \"https://www.facebook.com/YONECOMw/\"},\n            \"Instagram\": {\"type\": \"instragram\", \"link\": \"https://www.instagram.com/yoneco97/\"},\n            \"Twitter\": {\"type\": \"twitter\", \"link\": \"https://twitter.com/tithandizane1\"},\n        },\n    },\n    \"LATEF \u2013 Ligne d\u2019Assistance Telephonique aux Enfants et aux Femmes\": {\n        \"region\": \"Mauritania\",\n        \"page\": \"https://childhelplineinternational.org/mauritania-latef-ligne-dassistance-telephonique-aux-enfants-et-aux-femmes/\",\n        \"description\": \"LATEF is operated by the Mauritanian Association for Mother and Child Health (AMSME), a non-profit NGO with consultative status with the United Nations Economic and Social Council. It is an important tool in the context of child protection within Mauritania.\",\n        \"contacts\": {\n            \"Website\": {\"type\": \"website\", \"link\": \"http://amsme.org/\"},\n            \"+222 46 71 08 31\": {\"type\": \"phone\", \"link\": \"tel:+22246710831\"},\n            \"Email\": {\"type\": \"email\", \"link\": \"mailto:comm.amsme.99@gmail.com\"},\n            \"Facebook\": {\"type\": \"facebook\", \"link\": \"https://www.facebook.com/www.amsme.org/\"},\n            \"Twitter\": {\"type\": \"twitter\", \"link\": \"https://twitter.com/AmsmeDei\"},\n            \"Youtube\": {\"type\": \"youtube\", \"link\": \"https://www.youtube.com/channel/UCxY39J_J_kWRdVWoL-GMnZw\"},\n            \"LinkedIn\": {\n                \"type\": \"linkedin\",\n                \"link\": \"https://www.linkedin.com/company/amsme-section-d%C3%A9fense-des-enfants-international-mauritanie/about/\",\n            },\n        },\n    },\n    \"Child Helpline Mauritius\": {\n        \"region\": \"Mauritius\",\n        \"page\": \"https://childhelplineinternational.org/mauritius-child-helpline-mauritius/\",\n        \"description\": \"Halley Movement is a coalition of charitable organisations working for the welfare of children and family in Mauritius and in the Southern African region. Its child helpline seeks to raise awareness of children and communities about their rights, provide phone guidance and support to children and parents\",\n        \"contacts\": {\n            \"Website\": {\"type\": \"website\", \"link\": \"http://halleymovement.org/our-programs/child-helpline/\"},\n            \"214 2451\": {\"type\": \"phone\", \"link\": \"tel:116\"},\n            \"Email\": {\"type\": \"email\", \"link\": \"mailto:%20info@helplinemauritius.org\"},\n            \"Facebook\": {\"type\": \"facebook\", \"link\": \"https://www.facebook.com/helplinemauritius\"},\n            \"Twitter\": {\"type\": \"twitter\", \"link\": \"https://twitter.com/halleymovement\"},\n        },\n    },\n    \"Linha Fala Crianca \u2013 Child Helpline Mozambique\": {\n        \"region\": \"Mozambique\",\n        \"page\": \"https://childhelplineinternational.org/mozambique-linha-fala-crianca-child-helpline-mozambique/\",\n        \"description\": \"Established in 2009, Linha Fala Crian\u00e7a is a non-profit organisation which provides a free telephone helpline to children in Mozambique. The helpline allows children who need help to report abuse and to be subsequently referred to the appropriate services in the areas of health, justice, education and social action.\",\n        \"contacts\": {\n            \"Website\": {\"type\": \"website\", \"link\": \"https://www.linhafala.org.mz/\"},\n            \"116\": {\"type\": \"phone\", \"link\": \"tel:116\"},\n            \"Email\": {\"type\": \"email\", \"link\": \"https://orgchil-jicorica.savviihq.com/?p=2511\"},\n            \"Facebook\": {\"type\": \"facebook\", \"link\": \"https://www.facebook.com/linhafala.crianca.3\"},\n            \"Instagram\": {\"type\": \"instragram\", \"link\": \"https://www.instagram.com/linhafala/\"},\n            \"Twitter\": {\"type\": \"twitter\", \"link\": \"https://twitter.com/linha_fala\"},\n        },\n    },\n    \"LifeLine/ChildLine Namibia\": {\n        \"region\": \"Namibia\",\n        \"page\": \"https://childhelplineinternational.org/namibia-lifeline-childline-namibia/\",\n        \"description\": \"Lifeline/Childline is a non-governmental organisation providing counselling to people all over Namibia, focusing on restoring hope and changing lives.\",\n        \"contacts\": {\n            \"Website\": {\"type\": \"website\", \"link\": \"https://www.lifelinechildline.org.na/\"},\n            \"116\": {\"type\": \"phone\", \"link\": \"tel:116\"},\n            \"+264 61 226 889\": {\"type\": \"phone\", \"link\": \"tel:+26461226889\"},\n            \"Email\": {\"type\": \"email\", \"link\": \"mailto:info@lifeline.org.na\"},\n            \"Instagram\": {\"type\": \"instragram\", \"link\": \"https://www.instagram.com/lifeline_childline_nam/?hl=en\"},\n            \"Twitter\": {\"type\": \"twitter\", \"link\": \"https://twitter.com/llcl_nam\"},\n        },\n    },\n    \"Cece Yara Child Helpline\": {\n        \"region\": \"Nigeria\",\n        \"page\": \"https://childhelplineinternational.org/nigeria-cece-yara-child-helpline/\",\n        \"description\": \"The Cece Yara Child Advocacy Centre provides a child-friendly safe environment for children to disclose sexual abuse and to get FREE legal, medical & therapeutic help they need. FREE child helpline to report, get help & information.\",\n        \"contacts\": {\n            \"Website\": {\"type\": \"website\", \"link\": \"https://ceceyara.org/\"},\n            \"08008008001\": {\"type\": \"phone\", \"link\": \"tel:08008008001\"},\n            \"Email\": {\"type\": \"email\", \"link\": \"mailto:info@ceceyara.org\"},\n            \"Facebook\": {\"type\": \"facebook\", \"link\": \"https://www.facebook.com/ceceyarafoundation\"},\n            \"Instagram\": {\"type\": \"instragram\", \"link\": \"https://www.instagram.com/cece_yara/\"},\n            \"Twitter\": {\"type\": \"twitter\", \"link\": \"https://twitter.com/cece_yara\"},\n            \"Youtube\": {\"type\": \"youtube\", \"link\": \"https://www.youtube.com/channel/UCA3vYL6aTo90mruGOLNeJEg/featured\"},\n            \"LinkedIn\": {\n                \"type\": \"linkedin\",\n                \"link\": \"https://www.linkedin.com/company/cece-yara-foundation/?originalSubdomain=ng\",\n            },\n        },\n    },\n    \"HDI Nigeria Child Helpline\": {\n        \"region\": \"Nigeria\",\n        \"page\": \"https://childhelplineinternational.org/nigeria-hdi-nigeria-child-helpline/\",\n        \"description\": \"Human Development Initiatives Nigeria (HDI) offers telephone services to seek for help during emergencies or moments of crisis.Over the years, the HDI Nigeria Child Helpline has succeeded in improving the reproductive health of adolescents through counseling, mentoring and referrals as the case may require.\",\n        \"contacts\": {\n            \"Website\": {\"type\": \"website\", \"link\": \"https://hdinigeria.org/hdinigeria/\"},\n            \"0808-0551-376\": {\"type\": \"phone\", \"link\": \"tel:0808-0551-376\"},\n            \"+234 1 453 5717\": {\"type\": \"phone\", \"link\": \"tel:+23414535717\"},\n            \"Email\": {\"type\": \"email\", \"link\": \"mailto:info@hdinigeria.org\"},\n            \"Facebook\": {\"type\": \"facebook\", \"link\": \"https://www.facebook.com/hdinigeria\"},\n            \"Instagram\": {\"type\": \"instragram\", \"link\": \"https://www.instagram.com/hdi4nigeria/\"},\n            \"Twitter\": {\"type\": \"twitter\", \"link\": \"https://twitter.com/hdi4nigeria\"},\n            \"Youtube\": {\"type\": \"youtube\", \"link\": \"https://www.youtube.com/user/HDINIGERIA\"},\n            \"LinkedIn\": {\n                \"type\": \"linkedin\",\n                \"link\": \"https://www.linkedin.com/in/human-development-initiatives-nigeria-174732191/?trk=public_profile_browsemap&originalSubdomain=ng\",\n            },\n        },\n    },\n    \"Centre GINDDI \u2013 Allo 116\": {\n        \"region\": \"Senegal\",\n        \"page\": \"https://childhelplineinternational.org/senegal-centre-ginddi-allo-116/\",\n        \"description\": \"Allo 116 is a listening, psychological assistance and information service for children and parents. It also works to prevent against all forms of violations of children\u2019s rights and provide medical and social assistance for street children in Senegal.\",\n        \"contacts\": {\n            \"Website\": {\"type\": \"website\", \"link\": \"http://www.centreginddi.com/\"},\n            \"116\": {\"type\": \"phone\", \"link\": \"tel:116\"},\n            \"Email\": {\"type\": \"email\", \"link\": \"mailto:centreginddi@hotmail.fr\"},\n            \"Facebook\": {\"type\": \"facebook\", \"link\": \"https://www.facebook.com/UtilisateurGinddi/\"},\n        },\n    },\n    \"ChildHelp Sierra Leone\": {\n        \"region\": \"Sierra Leone\",\n        \"page\": \"https://childhelplineinternational.org/sierra-leone-childhelp-sierra-leone/\",\n        \"description\": \"ChildHelp Sierra Leone is designed specifically to meet both immediate and long-term needs of impoverished and underprivileged needy children, their families and communities in need and living in difficult circumstances.\",\n        \"contacts\": {\n            \"Website\": {\"type\": \"website\", \"link\": \"http://www.childhelpsl.org/\"},\n            \"+232 78 666269\": {\"type\": \"phone\", \"link\": \"tel:+23278666269\"},\n            \"Email\": {\"type\": \"email\", \"link\": \"mailto:helpachildinafrica@gmail.com\"},\n            \"Facebook\": {\"type\": \"facebook\", \"link\": \"https://www.facebook.com/childhelpsierra/\"},\n            \"Twitter\": {\"type\": \"twitter\", \"link\": \"https://twitter.com/childhelpsierra\"},\n            \"LinkedIn\": {\"type\": \"linkedin\", \"link\": \"https://www.linkedin.com/company/childhelp-sierra-leone/about/\"},\n        },\n    },\n    \"EEHR Sierra Leone Child Helpline\": {\n        \"region\": \"Sierra Leone\",\n        \"page\": \"https://childhelplineinternational.org/sierra-leone-eehr-sierra-leone-child-helpline/\",\n        \"description\": \"EEHRSL (Economic Empowerment & Human Rights) serves the children and young people of Sierra Leone.\",\n        \"contacts\": {\n            \"Website\": {\"type\": \"website\", \"link\": \"https://eehrsl.wixsite.com/eehrsl\"},\n            \"078666269\": {\"type\": \"phone\", \"link\": \"tel:078666269\"},\n            \"Email\": {\"type\": \"email\", \"link\": \"https://eehrsl.wixsite.com/eehrsl/form\"},\n        },\n    },\n    \"WAAPO Child Hotline (Short-Call Helpline)\": {\n        \"region\": \"Somalia\",\n        \"page\": \"https://childhelplineinternational.org/somalia-waapo-child-hotline-short-call-helpline/\",\n        \"description\": \"WAAPO\u2019s mission is to respond to children in need of protection- to make their voices heard and concerns relayed to policy and decision makers; and, to provide learning experiences for children in a safe and caring environment helping them develop socially, emotionally, physically and intellectually. \",\n        \"contacts\": {\n            \"Website\": {\"type\": \"website\", \"link\": \"https://waapo.ngo\"},\n            \"334\": {\"type\": \"phone\", \"link\": \"tel:334\"},\n            \"Email\": {\"type\": \"email\", \"link\": \"mailto:contact@waapo.org.so\"},\n            \"Facebook\": {\"type\": \"facebook\", \"link\": \"https://www.facebook.com/WAAPO-Organization-727724320666578/\"},\n        },\n    },\n    \"Childline South Africa\": {\n        \"region\": \"South Africa\",\n        \"page\": \"https://childhelplineinternational.org/south-africa-childline-south-africa/\",\n        \"description\": \"Childline is an effective non-profit organization that works collectively to protect children from all forms of violence and to create a culture of children\u2019s rights in South Africa\",\n        \"contacts\": {\n            \"Website\": {\"type\": \"website\", \"link\": \"https://www.childlinesa.org.za/\"},\n            \"116\": {\"type\": \"phone\", \"link\": \"tel:116\"},\n            \"Email\": {\"type\": \"email\", \"link\": \"mailto:olcadmin@childlinesa.org.za\"},\n            \"Facebook\": {\"type\": \"facebook\", \"link\": \"https://www.facebook.com/ChildlineSA\"},\n            \"Instagram\": {\"type\": \"instragram\", \"link\": \"https://www.instagram.com/childlinesa/\"},\n            \"Twitter\": {\"type\": \"twitter\", \"link\": \"https://twitter.com/ChildlineSA\"},\n            \"Youtube\": {\"type\": \"youtube\", \"link\": \"https://www.youtube.com/channel/UCm0KZ6ue8FLsH73er56KogA\"},\n        },\n    },\n    \"Tanzania National Child Helpline\": {\n        \"region\": \"Tanzania\",\n        \"page\": \"https://childhelplineinternational.org/tanzania-tanzania-national-child-helpline/\",\n        \"description\": \"The Tanzania National Child Helpline services children through free of cost telephone number 116, the National Child Helpline. With the Internet Watch Foundation hotline we remove child abuse images through an online portal available to the public throughout Tanzania.\",\n        \"contacts\": {\n            \"Website\": {\"type\": \"website\", \"link\": \"https://www.sematanzania.org/child-helpline\"},\n            \"116\": {\"type\": \"phone\", \"link\": \"tel:116\"},\n            \"WhatsApp\": {\n                \"type\": \"whatsapp\",\n                \"link\": \"https://api.whatsapp.com/send/?phone=255624100100&text=Hi%2C+I%27m+interested&app_absent=0\",\n            },\n            \"Email\": {\"type\": \"email\", \"link\": \"mailto:info@sematanzania.org\"},\n            \"Facebook\": {\"type\": \"facebook\", \"link\": \"https://www.facebook.com/SemaTanzania/\"},\n            \"Instagram\": {\"type\": \"instragram\", \"link\": \"https://www.instagram.com/sematanzania/\"},\n            \"Twitter\": {\"type\": \"twitter\", \"link\": \"https://twitter.com/sematanzania\"},\n            \"LinkedIn\": {\"type\": \"linkedin\", \"link\": \"https://www.linkedin.com/company/c-sema/about/\"},\n        },\n    },\n    \"Allo 1011\": {\n        \"region\": \"Togo\",\n        \"page\": \"https://childhelplineinternational.org/togo-allo-1011/\",\n        \"description\": \"Allo 1011 is operated by CROPESDI.\",\n        \"contacts\": {\"1011\": {\"type\": \"phone\", \"link\": \"tel:1011\"}},\n    },\n    \"Sauti 116\": {\n        \"region\": \"Uganda\",\n        \"page\": \"https://childhelplineinternational.org/uganda-sauti-116/\",\n        \"description\": \"Sauti 116 is  sustainable Uganda Child Helpline service embedded into society and consistently offering timely and quality services that respond to child protection needs.\",\n        \"contacts\": {\n            \"Website\": {\"type\": \"website\", \"link\": \"https://sauti.mglsd.go.ug/sauti/\"},\n            \"116\": {\"type\": \"phone\", \"link\": \"tel:116\"},\n            \"Email\": {\"type\": \"email\", \"link\": \"mailto:sautichl@mglsd.go.ug\"},\n            \"Facebook\": {\"type\": \"facebook\", \"link\": \"https://www.facebook.com/sauti116/\"},\n            \"Twitter\": {\"type\": \"twitter\", \"link\": \"https://twitter.com/sauti116\"},\n        },\n    },\n    \"Childline Zambia\": {\n        \"region\": \"Zambia\",\n        \"page\": \"https://childhelplineinternational.org/zambia-childline-zambia/\",\n        \"description\": \"Lifeline/Childline Zambia is a toll-free telephone counselling and guidance service. The service aims to promote child protection and prevent gender-based violence.\",\n        \"contacts\": {\n            \"Website\": {\"type\": \"website\", \"link\": \"https://clzambia.org/\"},\n            \"116\": {\"type\": \"phone\", \"link\": \"tel:116\"},\n            \"SMS 116\": {\"type\": \"comment\", \"link\": \"sms:116\"},\n            \"Facebook\": {\"type\": \"facebook\", \"link\": \"https://www.facebook.com/lifelinechildlinezambia\"},\n            \"Twitter\": {\"type\": \"twitter\", \"link\": \"https://twitter.com/ZambiaLifeline\"},\n        },\n    },\n    \"Childline Zimbabwe\": {\n        \"region\": \"Zimbabwe\",\n        \"page\": \"https://childhelplineinternational.org/zimbabwe-childline-zimbabwe/\",\n        \"description\": \"Childline Zimbabwe\u2019s mission is to champion, defend and promote the rights of children in Zimbabwe through the provision of safe, confidential and child friendly reporting mechanisms centered around a free 24 hour counseling service.\",\n        \"contacts\": {\n            \"Website\": {\"type\": \"website\", \"link\": \"http://www.childline.org.zw\"},\n            \"116\": {\"type\": \"phone\", \"link\": \"tel:116\"},\n            \"WhatsApp\": {\n                \"type\": \"whatsapp\",\n                \"link\": \"https://l.facebook.com/l.php?u=https%3A%2F%2Fapi.whatsapp.com%2Fsend%3Fphone%3D263732116116%26app%3Dfacebook%26entry_point%3Dpage_cta%26fbclid%3DIwAR23wuOB9KGoantZMExDYxTMbBduB5K5rAOrhox6C_td3gA0tGOkW8xLEUs&h=AT1E2futBhPzHb6fk0_RG5euAgsN4tj2WV6gI_Rc4-H8xVLrrcJGodfUZ-LRdkvQLKGY8eekkcxD8tjAxynKX8aK6YQPb7Mf3zPmoKCPlOqnAjuSJJ6BYrhYNwoJdGDrb8hxGsE_\",\n            },\n            \"Email\": {\"type\": \"email\", \"link\": \"mailto:116@childline.org.zw\"},\n            \"Facebook\": {\"type\": \"facebook\", \"link\": \"https://www.facebook.com/childlinezimbabwe/\"},\n            \"Twitter\": {\"type\": \"twitter\", \"link\": \"https://twitter.com/childlinezim\"},\n        },\n    },\n    \"L\u00ednea 102 CaBA\": {\n        \"region\": \"Argentina\",\n        \"page\": \"https://childhelplineinternational.org/argentina-linea-102-caba/\",\n        \"description\": \"L\u00ednea 102 CaBA is a free telephone advice service on the rights of boys and girls in the City of Buenos Aires. It works 24 hours a day, 365 days a year.\",\n        \"contacts\": {\n            \"L\u00ednea 102 CaBA\": {\"type\": \"website\", \"link\": \"https://www.buenosaires.gob.ar/cdnnya/linea102\"},\n            \"102\": {\"type\": \"phone\", \"link\": \"tel:102\"},\n            \"WhatsApp\": {\n                \"type\": \"whatsapp\",\n                \"link\": \"https://api.whatsapp.com/send?phone=5491150500147&text=Linea%20102&source=&data=\",\n            },\n            \"Facebook\": {\"type\": \"facebook\", \"link\": \"https://www.facebook.com/GCBA/\"},\n            \"Twitter\": {\"type\": \"twitter\", \"link\": \"https://twitter.com/gcba\"},\n        },\n    },\n    \"Linea 102 Provincia de Buenos Aires\": {\n        \"region\": \"Argentina\",\n        \"page\": \"https://childhelplineinternational.org/argentina-linea-102-provincia-de-buenos-aires/\",\n        \"description\": \"L\u00ednea 102 PdBA is a free telephone advice service on the rights of boys and girls in the Province of Buenos Aires. It works 24 hours a day, 365 days a year.\",\n        \"contacts\": {\n            \"Website\": {\"type\": \"website\", \"link\": \"https://www.facebook.com/102Linea/\"},\n            \"102\": {\"type\": \"phone\", \"link\": \"tel:%20102\"},\n            \"Email\": {\"type\": \"email\", \"link\": \"http://linea102@syna.gba.gob.ar\"},\n            \"Facebook\": {\"type\": \"facebook\", \"link\": \"https://www.facebook.com/102Linea/\"},\n            \"Instagram\": {\"type\": \"instragram\", \"link\": \"https://twitter.com/bacdnnya\"},\n            \"Twitter\": {\"type\": \"twitter\", \"link\": \"https://twitter.com/bacdnnya\"},\n        },\n    },\n    \"Telefon Pa Hubentud\": {\n        \"region\": \"Aruba\",\n        \"page\": \"https://childhelplineinternational.org/aruba-telefon-pa-hubentud/\",\n        \"description\": \"Telefon Pa Hubentud is an independent charity that serves as an Aruban Youth Telephone Line, which was launched on November 20, 1999. With a distinctive telephone number \u2019131,\u2019 they aim to help children and youngsters from 8-24 years old and provide their services from 14.00 hours to 18.00 hours, 365 days a year.\",\n        \"contacts\": {\n            \"Website\": {\"type\": \"website\", \"link\": \"https://131.aw/\"},\n            \"131\": {\"type\": \"phone\", \"link\": \"tel:131\"},\n            \"Email\": {\"type\": \"email\", \"link\": \"mailto:telhubentud@gmail.com\"},\n            \"Facebook\": {\"type\": \"facebook\", \"link\": \"https://www.facebook.com/telefon.hubentud/\"},\n            \"Instagram\": {\"type\": \"instragram\", \"link\": \"\"},\n        },\n    },\n    \"L\u00ednea 156\": {\n        \"region\": \"Bolivia\",\n        \"page\": \"https://childhelplineinternational.org/bolivia-linea-156/\",\n        \"description\": \"The Platform for Comprehensive Family Care integrates the municipal services of the Defence and Protection against all forms of Violence. L\u00ednea 156 Emergency Violence Line provides care and rescue victims of some kind of violence. Anyone who lives in the municipality of La Paz will be served.\",\n        \"contacts\": {\n            \"Website\": {\"type\": \"website\", \"link\": \"https://www.lapaz.bo/infoservicio/linea-emerg-a-la-violencia-156/\"},\n            \"156\": {\"type\": \"phone\", \"link\": \"tel:156\"},\n            \"Facebook\": {\"type\": \"facebook\", \"link\": \"https://www.facebook.com/MunicipioLaPaz/\"},\n            \"Instagram\": {\"type\": \"instragram\", \"link\": \"https://www.instagram.com/alcaldialapaz/\"},\n            \"Twitter\": {\"type\": \"twitter\", \"link\": \"https://twitter.com/LaPazAlcaldia\"},\n            \"Youtube\": {\"type\": \"youtube\", \"link\": \"https://www.youtube.com/c/Alcald%C3%ADaLaPazGAMLP\"},\n        },\n    },\n    \"Safernet Brasil\": {\n        \"region\": \"Brazil\",\n        \"page\": \"https://childhelplineinternational.org/brazil-safernet-brasil/\",\n        \"contacts\": {\n            \"Website\": {\"type\": \"website\", \"link\": \"http://www.safernet.org.br/site/\"},\n            \"Email\": {\"type\": \"email\", \"link\": \"mailto:comunicacao@safernet.org.br\"},\n            \"Facebook\": {\"type\": \"facebook\", \"link\": \"https://www.facebook.com/SafernetBR\"},\n            \"Instagram\": {\"type\": \"instragram\", \"link\": \"https://www.instagram.com/safernetbr/\"},\n            \"Twitter\": {\"type\": \"twitter\", \"link\": \"https://twitter.com/safernet\"},\n        },\n    },\n    \"Kids Help Phone\": {\n        \"region\": \"Canada\",\n        \"page\": \"https://childhelplineinternational.org/canada-kids-help-phone/\",\n        \"description\": \"Kids Help Phone is Canada\u2019s only 24/7 e-mental health service offering free, confidential support to young people in English and French.\",\n        \"contacts\": {\n            \"Website\": {\"type\": \"website\", \"link\": \"https://kidshelpphone.ca/\"},\n            \"1-800-668-6868\": {\"type\": \"phone\", \"link\": \"tel:1-800-668-6868\"},\n            \"Text TALK to 686868\": {\"type\": \"phone\", \"link\": \"tel:686868\"},\n            \"Email\": {\"type\": \"email\", \"link\": \"mailto:info@kidshelpphone.ca\"},\n            \"Facebook\": {\"type\": \"facebook\", \"link\": \"https://www.facebook.com/KidsHelpPhone/\"},\n            \"Instagram\": {\"type\": \"instragram\", \"link\": \"https://www.instagram.com/kidshelpphone/\"},\n            \"Twitter\": {\"type\": \"twitter\", \"link\": \"https://twitter.com/KidsHelpPhone\"},\n            \"Youtube\": {\"type\": \"youtube\", \"link\": \"https://www.youtube.com/user/KidsHelpPhone\"},\n            \"LinkedIn\": {\n                \"type\": \"linkedin\",\n                \"link\": \"https://www.linkedin.com/company/kidshelpphone/?originalSubdomain=ca\",\n            },\n        },\n    },\n    \"Fonoinfancia 800 200 818\": {\n        \"region\": \"Chile\",\n        \"page\": \"https://childhelplineinternational.org/chile-fonoinfancia-800-200-818/\",\n        \"contacts\": {\n            \"Website\": {\"type\": \"website\", \"link\": \"http://www.fonoinfancia.cl/\"},\n            \"800200818\": {\"type\": \"phone\", \"link\": \"tel:800200818\"},\n            \"Facebook\": {\"type\": \"facebook\", \"link\": \"https://www.facebook.com/fonoinfancia/\"},\n            \"Instagram\": {\"type\": \"instragram\", \"link\": \"https://www.instagram.com/fonoinfancia/\"},\n            \"Twitter\": {\"type\": \"twitter\", \"link\": \"https://twitter.com/Fonoinfancia\"},\n            \"Youtube\": {\"type\": \"youtube\", \"link\": \"https://www.youtube.com/user/fonoinfancia\"},\n        },\n    },\n    \"L\u00ednea Libre\": {\n        \"region\": \"Chile\",\n        \"page\": \"https://childhelplineinternational.org/chile-linea-libre/\",\n        \"description\": \"L\u00ednea Libre is is a psychological support channel aimed at girls, boys and young people, which is attended directly by psychologists trained to contain, guide, intervene in crises, and address mental health concerns or rights violations. It is available Monday to Saturday from 10:00 a.m. to 10:00 p.m. through three channels: phone email, and chat via our app.\",\n        \"contacts\": {\n            \"Website\": {\"type\": \"website\", \"link\": \"http://www.linealibre.cl/\"},\n            \"1515\": {\"type\": \"phone\", \"link\": \"tel:1515\"},\n            \"Email\": {\"type\": \"email\", \"link\": \"mailto:contacto@linealibre.cl\"},\n            \"Facebook\": {\"type\": \"facebook\", \"link\": \"https://www.facebook.com/www.linealibre.cl\"},\n            \"Instagram\": {\"type\": \"instragram\", \"link\": \"https://www.instagram.com/lalinealibre/\"},\n            \"Twitter\": {\"type\": \"twitter\", \"link\": \"https://twitter.com/lalinealibre\"},\n            \"Apple App Store\": {\"type\": \"apple\", \"link\": \"https://apps.apple.com/cl/app/l%C3%ADnea-libre/id1467421633\"},\n            \"Google Play App\": {\n                \"type\": \"google\",\n                \"link\": \"https://play.google.com/store/apps/details?id=com.ltmessenger.linealibre&hl=es_419&pli=1\",\n            },\n        },\n    },\n    \"ICBF Colombia \u2013 L\u00ednea 141\": {\n        \"region\": \"Colombia\",\n        \"page\": \"https://childhelplineinternational.org/colombia-icbf-colombia-linea-141/\",\n        \"description\": \"L\u00ednea 141 is a free national hotline that the Colombian Family Welfare Institute makes available to any adult or child who needs to report an emergency, make a complaint or ask for guidance on cases of child abuse, sexual violence, bullying, child labour, or drug and substance use, among many other situations that threaten or affect the life and integrity of a child or adolescent. It is available 24 hours via phone.\",\n        \"contacts\": {\n            \"Website\": {\"type\": \"website\", \"link\": \"http://www.icbf.gov.co/\"},\n            \"141\": {\"type\": \"phone\", \"link\": \"tel:141\"},\n            \"Email\": {\"type\": \"email\", \"link\": \"mailto:atencionalciudadano@icbf.gov.co\"},\n            \"Facebook\": {\"type\": \"facebook\", \"link\": \"https://www.facebook.com/ICBFColombia\"},\n            \"Instagram\": {\"type\": \"instragram\", \"link\": \"https://www.instagram.com/icbfcolombiaoficial/\"},\n            \"Twitter\": {\"type\": \"twitter\", \"link\": \"https://twitter.com/ICBFColombia\"},\n        },\n    },\n    \"L\u00ednea 106 Bogot\u00e1\": {\n        \"region\": \"Colombia\",\n        \"page\": \"https://childhelplineinternational.org/colombia-linea-106-bogota/\",\n        \"description\": \"L\u00ednea 106 is a free support channel with the mission to listen to people in need and promote mental health. Since 1997, we address the concerns of citizens to make Bogot\u00e1 a better territory for everyone. In addition, that children, adolescents and adults of all ages receive listening and psychological support from the District Health Secretariat.\",\n        \"contacts\": {\n            \"Website\": {\"type\": \"website\", \"link\": \"http://www.saludcapital.gov.co/Paginas2/Linea106-Inicio.aspx\"},\n            \"106\": {\"type\": \"phone\", \"link\": \"tel:106\"},\n            \"Email\": {\"type\": \"email\", \"link\": \"mailto:linea106@saludcapital.gov.co\"},\n            \"Facebook\": {\"type\": \"facebook\", \"link\": \"https://www.facebook.com/Linea106\"},\n            \"Instagram\": {\"type\": \"instragram\", \"link\": \"https://www.instagram.com/linea106bogota/\"},\n            \"Twitter\": {\"type\": \"twitter\", \"link\": \"https://twitter.com/sectorsalud\"},\n        },\n    },\n    \"Te Gu\u00edo\": {\n        \"region\": \"Colombia\",\n        \"page\": \"https://childhelplineinternational.org/te-guio/\",\n        \"description\": \"Te G\u00faio is strengthening capacities in Colombia in terms of prevention of sexual violence against children and adolescents. The helpline provides information on harmful sexual behaviour among minors, and offers a tele-counselling service for adolescents who are exhibiting this type of behaviour.\",\n        \"contacts\": {\n            \"Website\": {\"type\": \"website\", \"link\": \"http://www.teguiocolombia.org\"},\n            \"01800 519 0690\": {\"type\": \"phone\", \"link\": \"tel:018005190690\"},\n            \"WhatsApp\": {\"type\": \"whatsapp\", \"link\": \"http://+573148210435\"},\n            \"Facebook\": {\"type\": \"facebook\", \"link\": \"https://www.facebook.com/teguio.viguias\"},\n            \"Instagram\": {\"type\": \"instragram\", \"link\": \"https://www.instagram.com/teguio.viguias/\"},\n            \"Twitter\": {\"type\": \"twitter\", \"link\": \"https://twitter.com/TeGuio_Viguias\"},\n        },\n    },\n    \"Patronato Nacional de la Infancia (PANI) \u2013 L\u00ednea 1147\": {\n        \"region\": \"Costa Rica\",\n        \"page\": \"https://childhelplineinternational.org/costa-rica-patronato-nacional-de-la-infancia-pani-linea-1147/\",\n        \"description\": \"The National Children\u2019s Trust\u2019s (PANI) L\u00ednea 1147 is a free hotline and confidential service, which is attended by psychologists from Monday to Friday, from 7:00 a.m. to 10:00 p.m., where minors can exercise their right to participate, express their ideas, emotions, opinions and denounce situations that violate their rights.\",\n        \"contacts\": {\n            \"Website\": {\"type\": \"website\", \"link\": \"https://pani.go.cr/\"},\n            \"1147\": {\"type\": \"phone\", \"link\": \"tel:1147\"},\n            \"Facebook\": {\"type\": \"facebook\", \"link\": \"https://www.facebook.com/PatronatoNacionaldelaInfancia\"},\n            \"Instagram\": {\"type\": \"instragram\", \"link\": \"https://www.instagram.com/panicostarica/\"},\n            \"Twitter\": {\"type\": \"twitter\", \"link\": \"https://twitter.com/PANI_CR\"},\n        },\n    },\n    \"Ayudo pa mucha i hoben 918\": {\n        \"region\": \"Cura\u00e7ao\",\n        \"page\": \"https://childhelplineinternational.org/curacao-ayudo-pa-mucha-i-hoben-918/\",\n        \"description\": \"Ayudo pa mucha i hoben 918 is free to contact via phone, chat, and forum. It is available for all children, young people and young adults on Cura\u00e7ao up to and including 25 years old.\",\n        \"contacts\": {\n            \"Website\": {\"type\": \"website\", \"link\": \"https://www.918.cw\"},\n            \"918\": {\"type\": \"phone\", \"link\": \"tel:918\"},\n            \"Email\": {\"type\": \"email\", \"link\": \"mailto:918@kinderbeschermingcuracao.org\"},\n            \"Facebook\": {\"type\": \"facebook\", \"link\": \"https://www.facebook.com/ayudopamuchaihoben918\"},\n            \"Instagram\": {\"type\": \"instragram\", \"link\": \"https://www.instagram.com/918.cw/\"},\n        },\n    },\n    \"L\u00ednea de ayuda Familiar CONTIGO\": {\n        \"region\": \"Dominican Republic\",\n        \"page\": \"https://childhelplineinternational.org/dominican-republic-linea-de-ayuda-familiar-contigo/\",\n        \"description\": \"L\u00ednea de ayuda Familiar CONTIGO is made up of a team of mental health professionals who, with the sponsorship of UNICEF and USAID, plus the support of CODOPSI and the Institute of Mental Health and Telepsychology, offer emotional support to children and adolescents, as well as to the entire population, following the impact generated by the Coronavirus pandemic (COVID-19). It is available for children and adolescents in the Dominican Republic.\",\n        \"contacts\": {\n            \"Website\": {\n                \"type\": \"website\",\n                \"link\": \"https://lineafamiliar.do/?fbclid=IwAR32Upzp_MnlW-nqC1yhx0NSgARwp9LlcUKfEmwZLBQQC_5GbTBxluXUaS8\",\n            },\n            \"809-636-3507\": {\"type\": \"phone\", \"link\": \"tel:8096363507\"},\n            \"WhatsApp\": {\n                \"type\": \"whatsapp\",\n                \"link\": \"https://api.whatsapp.com/send?phone=18492584479&text=&source=&data=\",\n            },\n            \"Email\": {\"type\": \"email\", \"link\": \"mailto:linefamiliar.digital@gmail.com\"},\n            \"Facebook\": {\"type\": \"facebook\", \"link\": \"https://www.facebook.com/Linea-Familiar-RD-107128947691646/\"},\n            \"Instagram\": {\"type\": \"instragram\", \"link\": \"https://www.instagram.com/lineafamiliarrd/?hl=es-la\"},\n            \"Youtube\": {\"type\": \"youtube\", \"link\": \"https://www.youtube.com/channel/UCthdPh9p9f3bxk1wxMst_ag\"},\n        },\n    },\n    \"Sweet Water Foundation Child Helpline\": {\n        \"region\": \"Grenada\",\n        \"page\": \"https://childhelplineinternational.org/grenada-sweet-water-foundation-child-helpline/\",\n        \"description\": \"The Sweet Water Foundation is one of the few agencies in the Caribbean with a sole focus on child sexual abuse. It operates a free, online, confidential, and anonymous service for talking about sex, with a focus on stopping sexual practices which may harm a child. The child helpline is a confidential, anonymous counselling service. Adults and children are equally encouraged to call.\",\n        \"contacts\": {\n            \"Website\": {\"type\": \"website\", \"link\": \"http://www.sweetwaterfoundation.ca\"},\n            \"473-800-4444\": {\"type\": \"phone\", \"link\": \"tel:4738004444\"},\n            \"Email\": {\"type\": \"email\", \"link\": \"mailto:info@sweetwaterfoundation.ca\"},\n            \"Facebook\": {\n                \"type\": \"facebook\",\n                \"link\": \"https://www.facebook.com/Sweet-Water-Foundation-Intl-1448075422110012/\",\n            },\n            \"Instagram\": {\"type\": \"instragram\", \"link\": \"https://www.instagram.com/MySistersKeeper_SWF/\"},\n            \"Twitter\": {\"type\": \"twitter\", \"link\": \"https://twitter.com/SweetWaterIntl\"},\n        },\n    },\n    \"SafeSpot\": {\n        \"region\": \"Jamaica\",\n        \"page\": \"https://childhelplineinternational.org/jamaica-safespot/\",\n        \"description\": \"SafeSpot is a FREE and confidential counselling service for Jamaican children and teens to reach out for help all day, everyday.\",\n        \"contacts\": {\n            \"Website\": {\"type\": \"website\", \"link\": \"https://safespotja.com/\"},\n            \"888-SAFE-SPOT\": {\"type\": \"phone\", \"link\": \"tel:8887233776\"},\n            \"WhatsApp\": {\"type\": \"whatsapp\", \"link\": \"https://wa.me/18764395199\"},\n            \"Facebook\": {\"type\": \"facebook\", \"link\": \"https://www.facebook.com/SafeSpotJamaica\"},\n            \"Instagram\": {\"type\": \"instragram\", \"link\": \"https://www.instagram.com/safespotja/\"},\n            \"Twitter\": {\"type\": \"twitter\", \"link\": \"https://twitter.com/ocajamaica1\"},\n            \"Snapchat\": {\"type\": \"snapchat\", \"link\": \"https://www.snapchat.com/add/safespotja\"},\n        },\n    },\n    \"L\u00ednea 133\": {\n        \"region\": \"Nicaragua\",\n        \"page\": \"https://childhelplineinternational.org/nicaragua-linea-133/\",\n        \"description\": \"Operated by The Ministry of the Family, the institution of the Citizen Power that promotes, prevents and accompanies actions for the restitution of the rights of children, adolescents, older adults and families in the community.\",\n        \"contacts\": {\n            \"Website\": {\"type\": \"website\", \"link\": \"https://www.mifamilia.gob.ni/\"},\n            \"133\": {\"type\": \"phone\", \"link\": \"tel:133\"},\n            \"2222-4444\": {\"type\": \"phone\", \"link\": \"tel:2222-4444\"},\n            \"WhatsApp\": {\n                \"type\": \"whatsapp\",\n                \"link\": \"https://api.whatsapp.com/send/?phone=50583525121&text&app_absent=0\",\n            },\n            \"Email\": {\"type\": \"email\", \"link\": \"mailto:comunicacionmifam@mifamilia.gob.ni\"},\n            \"Facebook\": {\"type\": \"facebook\", \"link\": \"https://www.facebook.com/mifamnicaragua\"},\n            \"Instagram\": {\"type\": \"instragram\", \"link\": \"https://www.instagram.com/mifamnicaragua/\"},\n            \"Twitter\": {\"type\": \"twitter\", \"link\": \"https://twitter.com/MifamNicaragua\"},\n            \"Youtube\": {\n                \"type\": \"youtube\",\n                \"link\": \"https://www.youtube.com/channel/UCxhGXSa7IxfnrbMi67n1z1g?view_as=subscriber\",\n            },\n        },\n    },\n    \"Fono Ayuda L\u00ednea 147\": {\n        \"region\": \"Paraguay\",\n        \"page\": \"https://childhelplineinternational.org/paraguay-fono-ayuda-linea-147/\",\n        \"description\": \"147 Fono Ayuda is a telephone assistance and guidance service for situations involving children and adolescents, specializing in providing psychological, social and legal guidance in cases of violations of rights.\",\n        \"contacts\": {\n            \"Website\": {\"type\": \"website\", \"link\": \"http://www.minna.gov.py/pagina/1224-fono-ayuda-147.html\"},\n            \"147\": {\"type\": \"phone\", \"link\": \"tel:147\"},\n            \"+595 21 204 749\": {\"type\": \"phone\", \"link\": \"tel:+595%2021%20204%20749\"},\n            \"Email\": {\"type\": \"email\", \"link\": \"mailto:comunicacion@minna.gov.py\"},\n            \"Facebook\": {\"type\": \"facebook\", \"link\": \"https://www.facebook.com/paraguayminna/\"},\n            \"Twitter\": {\"type\": \"twitter\", \"link\": \"https://twitter.com/MINNAParaguay\"},\n            \"Youtube\": {\"type\": \"youtube\", \"link\": \"https://www.youtube.com/channel/UCJWfTu4z1TU_LxX3t37ldIw\"},\n        },\n    },\n    \"T\u00e9lefono Anar\": {\n        \"region\": \"Peru\",\n        \"page\": \"https://childhelplineinternational.org/peru-telefono-anar/\",\n        \"description\": \"The ANAR Foundation guides children and adolescents from an interdisciplinary approach on the life situations that affect them, empowering them in their rights and linking them to the protection system when necessary.\",\n        \"contacts\": {\n            \"Website\": {\"type\": \"website\", \"link\": \"http://www.anarperu.org/\"},\n            \"Facebook\": {\"type\": \"facebook\", \"link\": \"https://www.facebook.com/fundacionanarperu\"},\n            \"Twitter\": {\"type\": \"twitter\", \"link\": \"https://twitter.com/anarperu\"},\n            \"Youtube\": {\"type\": \"youtube\", \"link\": \"https://www.youtube.com/user/anarperu\"},\n        },\n    },\n    \"KJT Mi Lijn\": {\n        \"region\": \"Suriname\",\n        \"page\": \"https://childhelplineinternational.org/suriname-kjt-mi-lijn/\",\n        \"description\": \"KJT Mi Lijn is a helpline where children, young people, adolescents and victims of domestic and gender-based violence can talk about their problems in complete confidence and receive appropriate help.\",\n        \"contacts\": {\n            \"Website\": {\"type\": \"website\", \"link\": \"https://www.bel123.org/\"},\n            \"597-123\": {\"type\": \"phone\", \"link\": \"tel:597-123\"},\n            \" +597 850-6907\": {\"type\": \"phone\", \"link\": \"tel:%20+597850-6907\"},\n            \"WhatsApp\": {\n                \"type\": \"whatsapp\",\n                \"link\": \"https://api.whatsapp.com/send/?phone=5978507038&text&app_absent=0\",\n            },\n            \"Email\": {\"type\": \"email\", \"link\": \"mailto:info@bel123.org\"},\n            \"Facebook\": {\"type\": \"facebook\", \"link\": \"https://www.facebook.com/bel123.org\"},\n            \"Instagram\": {\"type\": \"instragram\", \"link\": \"https://www.instagram.com/bel123org/\"},\n            \"Youtube\": {\"type\": \"youtube\", \"link\": \"https://www.youtube.com/channel/UCPvRvR96A21y0qMrnyOHJ6w\"},\n        },\n    },\n    \"ChildLine Trinidad & Tobago\": {\n        \"region\": \"Trinidad & Tobago\",\n        \"page\": \"https://childhelplineinternational.org/trinidad-tobago-childline-trinidad-tobago/\",\n        \"description\": \"ChildLine is a not-for-profit non-governmental organisation dedicated to the welfare and protection of children and young people up to age 25 in Trinidad and Tobago. Established in 2001, one of the main services of the organisation is a free, confidential telephone helpline which is available 24 hours a day, every day of the year.\",\n        \"contacts\": {\n            \"Website\": {\"type\": \"website\", \"link\": \"http://www.childlinett.org/\"},\n            \"131\": {\"type\": \"phone\", \"link\": \"tel:131\"},\n            \"800-4321\": {\"type\": \"phone\", \"link\": \"tel:800-4321\"},\n            \"Email\": {\"type\": \"email\", \"link\": \"http://communications@childlinett.org\"},\n            \"Facebook\": {\"type\": \"facebook\", \"link\": \"https://www.facebook.com/TTChildLine\"},\n            \"Instagram\": {\"type\": \"instragram\", \"link\": \"https://www.instagram.com/childline_tt/\"},\n            \"Twitter\": {\"type\": \"twitter\", \"link\": \"https://twitter.com/childline_tt\"},\n            \"Youtube\": {\"type\": \"youtube\", \"link\": \"https://www.youtube.com/channel/UC2UBiTpfqT6O09A2zG69QCg\"},\n        },\n    },\n    \"L\u00ednea Azul\": {\n        \"region\": \"Uruguay\",\n        \"page\": \"https://childhelplineinternational.org/uruguay-linea-azul/\",\n        \"description\": \"La L\u00ednea Azul receives, through the number 0800 5050 and the INAU website, complaints from the community related to situations of violence and violation of rights experienced by children and adolescents in order to respond to them. Coordinates with other actors involved the interventions to be carried out according to the demands received, in order to give a first response to situations of violation of rights. In addition, it carries out and/or coordinates the actions that allow a diagnostic approach aimed at providing timely and effective responses.\",\n        \"contacts\": {\n            \"Website\": {\n                \"type\": \"website\",\n                \"link\": \"https://www.inau.gub.uy/content_page/item/512-linea-azul-denuncias\",\n            },\n            \"0800 5050\": {\"type\": \"phone\", \"link\": \"tel:08005050\"},\n            \"Email\": {\"type\": \"email\", \"link\": \"mailto:lineaazul@inau.gub.uy\"},\n            \"Facebook\": {\"type\": \"facebook\", \"link\": \"https://www.facebook.com/inauoficial\"},\n            \"Instagram\": {\"type\": \"instragram\", \"link\": \"https://www.instagram.com/inau_oficial/\"},\n            \"Twitter\": {\"type\": \"twitter\", \"link\": \"https://twitter.com/INAU_Oficial\"},\n            \"Youtube\": {\"type\": \"youtube\", \"link\": \"https://www.youtube.com/channel/UClwdJF8XifcEXJgybDHHQjA\"},\n        },\n    },\n    \"2NDFLOOR Youth Helpline\": {\n        \"region\": \"USA\",\n        \"page\": \"https://childhelplineinternational.org/usa-2ndfloor-youth-helpline/\",\n        \"description\": \"2NDFLOOR is a confidential and anonymous helpline for New Jersey\u2019s youth and young adults. We are here to help you find solutions to the problems that you face at home, at school or at play.\",\n        \"contacts\": {\n            \"Website\": {\"type\": \"website\", \"link\": \"https://www.2ndfloor.org/\"},\n            \"888-222-2228\": {\"type\": \"phone\", \"link\": \"tel:888-222-2228\"},\n            \"TTY\": {\"type\": \"phone\", \"link\": \"tel:732-264-1703\"},\n            \"Email\": {\"type\": \"email\", \"link\": \"http://info@2ndfloor.org\"},\n            \"Facebook\": {\"type\": \"facebook\", \"link\": \"https://www.facebook.com/2ndflooryouthhelpline\"},\n            \"Instagram\": {\n                \"type\": \"instragram\",\n                \"link\": \"https://www.instagram.com/2ndflooryouthhelpline/https://www.instagram.com/2ndflooryouthhelpline/\",\n            },\n            \"Twitter\": {\"type\": \"twitter\", \"link\": \"https://twitter.com/2NDFloorNJ\"},\n        },\n    },\n    \"Boys Town National Hotline\": {\n        \"region\": \"USA\",\n        \"page\": \"https://childhelplineinternational.org/usa-boys-town-national-hotline/\",\n        \"contacts\": {\n            \"Website\": {\"type\": \"website\", \"link\": \"http://www.yourlifeyourvoice.org\"},\n            \"1-800-448-3000\": {\"type\": \"phone\", \"link\": \"tel:1-800-448-3000\"},\n            \"Text VOICE to 20121\": {\"type\": \"phone\", \"link\": \"tel:20121\"},\n            \"Email\": {\"type\": \"email\", \"link\": \"http://yourlifeyourvoice@boystown.org\"},\n            \"Facebook\": {\"type\": \"facebook\", \"link\": \"https://www.facebook.com/BoysTownHotline/\"},\n            \"Instagram\": {\"type\": \"instragram\", \"link\": \"https://www.instagram.com/boystown/\"},\n            \"Twitter\": {\"type\": \"twitter\", \"link\": \"https://twitter.com/BoysTown\"},\n        },\n    },\n    \"Crisis Text Line\": {\n        \"region\": \"USA\",\n        \"page\": \"https://childhelplineinternational.org/elementor-5111/\",\n        \"description\": \"Crisis Text Line provides free, 24/7, high-quality text-based mental health support and crisis intervention by empowering a community of trained volunteers to support people in their moments of need. \",\n        \"contacts\": {\n            \"Website\": {\"type\": \"website\", \"link\": \"https://www.crisistextline.org/\"},\n            \"TEXT 741741\": {\"type\": \"phone\", \"link\": \"tel:\"},\n            \"Facebook\": {\"type\": \"facebook\", \"link\": \"https://www.facebook.com/crisistextline\"},\n            \"Instagram\": {\"type\": \"instragram\", \"link\": \"https://www.instagram.com/crisistextline/\"},\n            \"Twitter\": {\"type\": \"twitter\", \"link\": \"https://twitter.com/CrisisTextLine\"},\n            \"LinkedIn\": {\"type\": \"linkedin\", \"link\": \"https://www.linkedin.com/company/crisistextline/\"},\n        },\n    },\n    \"National Child Abuse Hotline\": {\n        \"region\": \"USA\",\n        \"page\": \"https://childhelplineinternational.org/usa-national-child-abuse-hotline/\",\n        \"description\": \"Childhelp National Child Abuse Hotline serves children and adults nationwide and exists to meet the physical, emotional, educational, and spiritual needs of abused, neglected and at-risk children. Every child deserves to know that help is here. Whether you\u2019re a youth in crisis, a parent in need of resources, or a concerned peer, call, text or chat with a counselor 24/7.\",\n        \"contacts\": {\n            \"Website\": {\"type\": \"website\", \"link\": \"https://www.childhelp.org/\"},\n            \"1-800-4-A-CHILD\": {\"type\": \"phone\", \"link\": \"tel:18004224453\"},\n            \"Email\": {\"type\": \"email\", \"link\": \"mailto:info@childhelp.org\"},\n            \"Facebook\": {\"type\": \"facebook\", \"link\": \"https://www.facebook.com/childhelp\"},\n            \"Instagram\": {\"type\": \"instragram\", \"link\": \"https://www.instagram.com/childhelp/\"},\n            \"Twitter\": {\"type\": \"twitter\", \"link\": \"https://twitter.com/Childhelp\"},\n            \"Youtube\": {\"type\": \"youtube\", \"link\": \"https://www.youtube.com/childhelporg\"},\n            \"LinkedIn\": {\n                \"type\": \"linkedin\",\n                \"link\": \"https://www.linkedin.com/company/childhelp-inc-/?fbclid=IwAR0F7AqOHewHnqisyrZOAu8sphFSPwe80STvWm6BHtT2pPuDqWNTQzOopMg\",\n            },\n        },\n    },\n    \"Polaris\": {\n        \"region\": \"USA\",\n        \"page\": \"https://childhelplineinternational.org/usa-polaris/\",\n        \"description\": \"For more than a decade, Polaris has assisted thousands of victims and survivors through the U.S. National Human Trafficking Hotline, helped ensure countless traffickers were held accountable and built the largest known U.S. data set on actual trafficking experiences.\",\n        \"contacts\": {\n            \"Website\": {\"type\": \"website\", \"link\": \"http://polarisproject.org\"},\n            \"1-888-373-7888 (TTY:711)\": {\"type\": \"phone\", \"link\": \"tel:1-888-373-7888\"},\n            \"Text \u201cBeFree\u201d to 233733\": {\"type\": \"phone\", \"link\": \"tel:233733\"},\n            \"Email\": {\"type\": \"email\", \"link\": \"mailto:info@polarisproject.org\"},\n            \"Facebook\": {\"type\": \"facebook\", \"link\": \"https://www.facebook.com/polarisproject\"},\n            \"Instagram\": {\"type\": \"instragram\", \"link\": \"https://www.instagram.com/polarisproject/\"},\n            \"Twitter\": {\"type\": \"twitter\", \"link\": \"https://twitter.com/polaris_project\"},\n            \"Youtube\": {\"type\": \"youtube\", \"link\": \"https://www.youtube.com/user/polarisproject1\"},\n            \"LinkedIn\": {\"type\": \"linkedin\", \"link\": \"https://www.linkedin.com/company/polaris-project/\"},\n        },\n    },\n    \"Stop It Now!\": {\n        \"region\": \"USA\",\n        \"page\": \"https://childhelplineinternational.org/usa-stop-it-now/\",\n        \"description\": \"Stop It Now! prevents the sexual abuse of children by mobilizing adults, families and communities to take actions that protect children before they are harmed. They provide support, information and resources to keep children safe and create healthier communities.\",\n        \"contacts\": {\n            \"Website\": {\"type\": \"website\", \"link\": \"https://www.stopitnow.org/\"},\n            \"1.888.PREVENT\": {\"type\": \"phone\", \"link\": \"tel:18887738368\"},\n            \"Email\": {\"type\": \"email\", \"link\": \"mailto:info@stopitnow.org\"},\n            \"Facebook\": {\"type\": \"facebook\", \"link\": \"https://www.facebook.com/StopItNow\"},\n            \"Instagram\": {\"type\": \"instragram\", \"link\": \"https://www.instagram.com/stopitnow_us/\"},\n            \"Twitter\": {\"type\": \"twitter\", \"link\": \"https://twitter.com/stopitnow\"},\n            \"Youtube\": {\"type\": \"youtube\", \"link\": \"https://www.youtube.com/user/wecanstopitnow\"},\n        },\n    },\n    \"The Trevor Project\": {\n        \"region\": \"USA\",\n        \"page\": \"https://childhelplineinternational.org/the-trevor-project/\",\n        \"description\": \"The Trevor Project provides information & support to LGBTQ young people 24 / 7, all year round.\",\n        \"contacts\": {\n            \"Website\": {\"type\": \"website\", \"link\": \"https://www.thetrevorproject.org\"},\n            \"866-488-7386\": {\"type\": \"phone\", \"link\": \"tel:866-488-7386\"},\n            \"Facebook\": {\"type\": \"facebook\", \"link\": \"https://www.facebook.com/TheTrevorProject\"},\n            \"Instagram\": {\"type\": \"instragram\", \"link\": \"https://www.instagram.com/trevorproject/\"},\n            \"Twitter\": {\"type\": \"twitter\", \"link\": \"https://twitter.com/trevorproject\"},\n            \"Youtube\": {\"type\": \"youtube\", \"link\": \"https://www.youtube.com/thetrevorproject\"},\n            \"LinkedIn\": {\"type\": \"linkedin\", \"link\": \"https://www.linkedin.com/company/the-trevor-project\"},\n        },\n    },\n    \"Kids Helpline (Australia)\": {\n        \"region\": \"Australia\",\n        \"page\": \"https://childhelplineinternational.org/australia-kids-helpline-australia/\",\n        \"description\": \"Kids Helpline is Australia\u2019s only free (even from a mobile), confidential 24/7 online and phone counselling service for young people aged 5 to 25. Qualified counsellors at Kids Helpline are available via WebChat, phone or email anytime and for any reason.\",\n        \"contacts\": {\n            \"Website\": {\"type\": \"website\", \"link\": \"https://kidshelpline.com.au/\"},\n            \"1 800 55 1800\": {\"type\": \"phone\", \"link\": \"tel:1800551800\"},\n            \"Email\": {\"type\": \"email\", \"link\": \"mailto:https://kidshelpline.com.au/get-help/email-counselling/\"},\n            \"Facebook\": {\"type\": \"facebook\", \"link\": \"https://www.facebook.com/KidsHelpline/\"},\n            \"Instagram\": {\"type\": \"instragram\", \"link\": \"https://www.instagram.com/kidshelplineau/\"},\n            \"Twitter\": {\"type\": \"twitter\", \"link\": \"https://twitter.com/kidshelplineau\"},\n            \"Youtube\": {\"type\": \"youtube\", \"link\": \"https://www.youtube.com/user/kidshelpline101\"},\n        },\n    },\n    \"Bangladesh Child Helpline 1098\": {\n        \"region\": \"Bangladesh\",\n        \"page\": \"https://childhelplineinternational.org/bangladesh-bangladesh-child-helpline-1098/\",\n        \"description\": \"Child Helpline 1098 is a service that extends a helping hand to children from all kinds of influences or stress, while protecting all kinds of privacy. The helpline can be reached by dialling 1098 from any part of Bangladesh at any time of day, including public and weekly holidays.\",\n        \"contacts\": {\n            \"Website\": {\"type\": \"website\", \"link\": \"http://www.dss.gov.bd/\"},\n            \"1098\": {\"type\": \"phone\", \"link\": \"tel:1098\"},\n            \"Facebook\": {\"type\": \"facebook\", \"link\": \"https://www.facebook.com/childhelpline1098/\"},\n            \"Twitter\": {\"type\": \"twitter\", \"link\": \"https://twitter.com/dss_bd\"},\n        },\n    },\n    \"Child Helpline Bhutan\": {\n        \"region\": \"Bhutan\",\n        \"page\": \"https://childhelplineinternational.org/bhutan-child-helpline-bhutan/\",\n        \"description\": \"The Toll Free Helpline for Women and Children in difficult circumstances will be operational from 9 am to 5 pm during weekdays. Voice messages can be send to NCWC during off hours and follow up will be made in the following days.\",\n        \"contacts\": {\n            \"Website\": {\"type\": \"website\", \"link\": \"https://www.ncwc.gov.bt/\"},\n            \"1098\": {\"type\": \"phone\", \"link\": \"tel:1098\"},\n            \"Facebook\": {\"type\": \"facebook\", \"link\": \"https://www.facebook.com/NationalCommissionforWomenandChildren/\"},\n            \"YouTube\": {\"type\": \"youtube\", \"link\": \"https://www.youtube.com/channel/UCbA_f5XGWKuZSdIDmGhPIFg\"},\n        },\n    },\n    \"Talian ANAK 121\": {\n        \"region\": \"Brunei Darussalam\",\n        \"page\": \"https://childhelplineinternational.org/brunei-darussalam-talian-anak-121/\",\n        \"description\": \"Talian ANAK 121 provides an efficient communication channel for people to contact JAPEM relating to cases of abuse, neglect, protection, exploitation, counseling, family problems and more.\",\n        \"contacts\": {\n            \"Website\": {\"type\": \"website\", \"link\": \"https://www.japem.gov.bn/\"},\n            \"121\": {\"type\": \"phone\", \"link\": \"tel:121\"},\n            \"Email\": {\"type\": \"email\", \"link\": \"mailto:taliananak121@japem.gov.bn\"},\n            \"Facebook\": {\"type\": \"facebook\", \"link\": \"https://www.facebook.com/jabatanpembangunanmasyarakat.japem\"},\n            \"Instagram\": {\n                \"type\": \"instragram\",\n                \"link\": \"https://www.instagram.com/Japembrunei/?fbclid=IwAR2-AoHudlzvvMFeQ2OFzvfmNf08NA46dMic0Tx2Vnt6Glw9lUun5C140Xo\",\n            },\n            \"Twitter\": {\n                \"type\": \"twitter\",\n                \"link\": \"https://twitter.com/JAPEM_MCYS?fbclid=IwAR3UpyFX-WWAqmIcTj3hPk2ZrWPEuCCbcxl-e_aLSvr504YKNqttgPBG1mQ\",\n            },\n        },\n    },\n    \"Child Helpline Cambodia\": {\n        \"region\": \"Cambodia\",\n        \"page\": \"https://childhelplineinternational.org/cambodia-child-helpline-cambodia/\",\n        \"description\": \"Child Helpline Cambodia provides free, 24 hour phone counseling, information, referral and follow-up services for children and youth up to 25 years old in Cambodia. It allows children and young people to reach out when they need it, in real time and speak directly with someone in a safe and confidential environment via the free 1280 helpline, as well as via their website and Facebook page.\",\n        \"contacts\": {\n            \"Website\": {\"type\": \"website\", \"link\": \"http://childhelplinecambodia.org\"},\n            \"1280\": {\"type\": \"phone\", \"link\": \"tel:1280\"},\n            \"Email\": {\"type\": \"email\", \"link\": \"mailto:cambodiachildhelpline@gmail.com\"},\n            \"Facebook\": {\"type\": \"facebook\", \"link\": \"https://www.facebook.com/chc1280/?fref=ts\"},\n            \"Instagram\": {\"type\": \"instragram\", \"link\": \"https://www.instagram.com/childhelplinecambodia/\"},\n        },\n    },\n    \"Child Emergency Hotline\": {\n        \"region\": \"China\",\n        \"page\": \"https://childhelplineinternational.org/china-child-emergency-hotline/\",\n        \"contacts\": {\n            \"116 111\": {\"type\": \"phone\", \"link\": \"tel:\"},\n            \"Email\": {\"type\": \"email\", \"link\": \"https://orgchil-jicorica.savviihq.com/?p=2511\"},\n        },\n    },\n    \"Child Helpline Fiji\": {\n        \"region\": \"Fiji\",\n        \"page\": \"https://childhelplineinternational.org/fiji-child-helpline-fiji/\",\n        \"description\": \"Operated under the Ministry of Women, Children and Poverty Alleviation, the Child Helpline 1325 is available for children 24 hours a day and 7 days a week. 1325 is a toll-free number that is manned by professional counsellors who are ready to listen to children\u2019s problems and guide you towards getting the help they need and deserve.\",\n        \"contacts\": {\n            \"Website\": {\n                \"type\": \"website\",\n                \"link\": \"http://msp.org.fj/?fbclid=IwAR1TUL6hie4_zG__GypYiKQe6rc_Lnultew8CyAlzXEQdxrWbJYQ7rfk7iY\",\n            },\n            \"1325\": {\"type\": \"phone\", \"link\": \"tel:1325\"},\n            \"Email\": {\"type\": \"email\", \"link\": \"mailto:help@msp.org.fj\"},\n            \"Facebook\": {\"type\": \"facebook\", \"link\": \"https://www.facebook.com/ChildHelplineFiji/\"},\n        },\n    },\n    \"Parent-Child Support Line\": {\n        \"region\": \"Hong Kong (China)\",\n        \"page\": \"https://childhelplineinternational.org/hong-kong-china-parent-child-support-line/\",\n        \"description\": \"Operated by Action Against Abuse (ACA), the Parent-Child Support Line provides service where parents, children, professionals and the public can call the hotline 2755 1122, or go to the ACA centre to report suspected child abuse cases or ask questions about any issues they are facing. It is also a support and hotline for children to express their voices and opinions. The personal data and case content of the data provider/reporter are kept strictly confidential.\",\n        \"contacts\": {\n            \"Website\": {\"type\": \"website\", \"link\": \"https://www.aca.org.hk/index.php#.YmRbANNBw-Q\"},\n            \"116 111\": {\"type\": \"phone\", \"link\": \"tel:\"},\n            \"Email\": {\"type\": \"email\", \"link\": \"mailto:aca@aca.org.hk\"},\n            \"Facebook\": {\"type\": \"facebook\", \"link\": \"https://www.facebook.com/ACAHK\"},\n            \"Instagram\": {\"type\": \"instragram\", \"link\": \"https://www.instagram.com/againstchildabusehk/\"},\n            \"Twitter\": {\n                \"type\": \"twitter\",\n                \"link\": \"https://twitter.com/acahk1979?fbclid=IwAR1yhVK2uky1_zsBavwOd_PtCFTbVZy3NbRb_S_C81bGkj0i2PzJXIP4d2M\",\n            },\n            \"Youtube\": {\"type\": \"youtube\", \"link\": \"https://www.youtube.com/user/AgainstChildAbuseHK\"},\n            \"LinkedIn\": {\n                \"type\": \"linkedin\",\n                \"link\": \"https://www.linkedin.com/company/acahk/?fbclid=IwAR1oZJte9EQUjNLc4rY27fiEm-aGWkE4Jla5tJE4V84BltZtAdKQpOWBFVE\",\n            },\n        },\n    },\n    \"Childline India\": {\n        \"region\": \"India\",\n        \"page\": \"https://childhelplineinternational.org/india-childline-india/\",\n        \"description\": \"Childline 1098 is a phone number that spells hope for millions of children across India. It is a 24-hour a day, 365 days a year, free, emergency phone service for children in need of aid and assistance. They not only respond to the emergency needs of children but also link them to relevant services for their long-term care and rehabilitation. \",\n        \"contacts\": {\n            \"Website\": {\"type\": \"website\", \"link\": \"http://www.childlineindia.org.in\"},\n            \"1098\": {\"type\": \"phone\", \"link\": \"tel:1098\"},\n            \"Email\": {\"type\": \"email\", \"link\": \"mailto:dial1098@childlineindia.org.in\"},\n            \"Facebook\": {\n                \"type\": \"facebook\",\n                \"link\": \"https://www.facebook.com/pages/Childline-India-Foundation/137070779678465\",\n            },\n            \"Instagram\": {\"type\": \"instragram\", \"link\": \"https://www.instagram.com/childlineindia1098/\"},\n            \"Twitter\": {\"type\": \"twitter\", \"link\": \"https://twitter.com/CHILDLINE1098\"},\n            \"LinkedIn\": {\"type\": \"linkedin\", \"link\": \"https://www.linkedin.com/company/childline-india-foundation/\"},\n        },\n    },\n    \"TePSA \u2013 Telepon Pelayanan Sosial Anak\": {\n        \"region\": \"Indonesia\",\n        \"page\": \"https://childhelplineinternational.org/indonesia-tepsa-telepon-pelayanan-sosial-anak/\",\n        \"description\": \"Telepon Pelayanan Sosial Anak (TePSA) is a telephone service for children, which functions to provide telecounseling services for children who have problems, complaints, including services for children who have emergency problems and provide referrals if needed. TePSA is a service created by the Ministry of Social Affairs of the Republic of Indonesia.\",\n        \"contacts\": {\n            \"Website\": {\"type\": \"website\", \"link\": \"https://www.facebook.com/TePSAKEMENSOS\"},\n            \"1500771\": {\"type\": \"phone\", \"link\": \"tel:1098\"},\n            \"Facebook\": {\"type\": \"facebook\", \"link\": \"https://www.facebook.com/TePSAKEMENSOS\"},\n            \"Twitter\": {\"type\": \"twitter\", \"link\": \"https://twitter.com/tepsakemensos\"},\n        },\n    },\n    \"Childline Japan\": {\n        \"region\": \"Japan\",\n        \"page\": \"https://childhelplineinternational.org/japan-childline-japan/\",\n        \"description\": \"ChildLine is a free telephone service for under 18 years old who want to talk with somebody about anything they want. Calls are anonymous, free and the volunteer adults called \u201creceivers\u201d listen to the story and work with the child to work through any problems they want to talk about.\",\n        \"contacts\": {\n            \"Website\": {\n                \"type\": \"website\",\n                \"link\": \"https://childline.or.jp/?fbclid=IwAR1SZxgm0xeG3pzteltaAmlCK68b5hbNWUlBQBoitLnTf937z33uoxVpyZY\",\n            },\n            \"0120-99-7777\": {\"type\": \"phone\", \"link\": \"tel:0120997777\"},\n            \"Email\": {\"type\": \"email\", \"link\": \"mailto:info@childline.or.jp\"},\n            \"Facebook\": {\"type\": \"facebook\", \"link\": \"https://www.facebook.com/childlinejapan\"},\n            \"Twitter\": {\"type\": \"twitter\", \"link\": \"https://twitter.com/childlinejapan\"},\n        },\n    },\n    \"Telefon 150\": {\n        \"region\": \"Kazakhstan\",\n        \"page\": \"https://childhelplineinternational.org/kazakhstan-telefon-150/\",\n        \"description\": \"Telefon 150 is there for children when they are having a hard time and tries to understand them and help them. The helpline completely anonymous and confidential and any concern is respected \u2013 what worries the child becomes important for the consultant at the moment of the conversation.\",\n        \"contacts\": {\n            \"Website\": {\"type\": \"website\", \"link\": \"http://www.telefon150.kz/\"},\n            \"150\": {\"type\": \"phone\", \"link\": \"tel:150\"},\n            \"WhatsApp\": {\n                \"type\": \"whatsapp\",\n                \"link\": \"https://api.whatsapp.com/send?phone=/77081060810&text=%D0%97%D0%B4%D1%80%D0%B0%D0%B2%D1%81%D1%82%D0%B2%D1%83%D0%B9%D1%82%D0%B5...\",\n            },\n            \"Email\": {\"type\": \"email\", \"link\": \"mailto:soyuzkz@rambler.ru\"},\n            \"Facebook\": {\"type\": \"facebook\", \"link\": \"https://www.facebook.com/SouzKrizisnyhCentrovKZ/\"},\n            \"Instagram\": {\"type\": \"instragram\", \"link\": \"https://www.instagram.com/telefon150kz/\"},\n        },\n    },\n    \"The Centre\": {\n        \"region\": \"Kyrgyzstan\",\n        \"page\": \"https://childhelplineinternational.org/kyrgyzstan-the-centre/\",\n        \"description\": \"The Child Rights Defenders League operates the Helpline for Children, which aims to protect and promote the rights and freedoms of children.\",\n        \"contacts\": {\n            \"Website\": {\"type\": \"website\", \"link\": \"http://crdl.kg/\"},\n            \"150 00\": {\"type\": \"phone\", \"link\": \"tel:15000\"},\n        },\n    },\n    \"Vientiane Youthline\": {\n        \"region\": \"Laos\",\n        \"page\": \"https://childhelplineinternational.org/laos-vientiane-youthline/\",\n        \"description\": \"Operated by the Vientiane Women and Youth Centre for Health and Development\",\n        \"contacts\": {\"1361 (female); 1371 (male)\": {\"type\": \"phone\", \"link\": \"tel:\"}},\n    },\n    \"Child Help Line 1412\": {\n        \"region\": \"Maldives\",\n        \"page\": \"https://childhelplineinternational.org/maldives-child-help-line-1412/\",\n        \"description\": \"Operated by the Ministry of Gender, Family and Social Services and Child and Family Protection Service, Child Helpline 1412 provides a service where children can reach out for professional help.\",\n        \"contacts\": {\n            \"Website\": {\"type\": \"website\", \"link\": \"https://gender.gov.mv/\"},\n            \"1412\": {\"type\": \"phone\", \"link\": \"tel:1412\"},\n            \"Email\": {\"type\": \"email\", \"link\": \"mailto:info@gender.gov.mv\"},\n            \"Facebook\": {\"type\": \"facebook\", \"link\": \"https://www.facebook.com/MoGFSSmv\"},\n            \"Instagram\": {\"type\": \"instragram\", \"link\": \"https://www.instagram.com/genderministry/\"},\n            \"Twitter\": {\"type\": \"twitter\", \"link\": \"https://twitter.com/Min_Gender?ref_src=twsrc%5Etfw\"},\n        },\n    },\n    \"Child Helpline 108\": {\n        \"region\": \"Mongolia\",\n        \"page\": \"https://childhelplineinternational.org/mongolia-child-helpline-108/\",\n        \"description\": \"Mongolia\u2019s first ever nationwide, toll free, 24/7 Child Helpline 108 was launched in 2014. Children need only dial 108 on any phone to be directly linked with an operator standing by on the other end for assistance, advice and support.\",\n        \"contacts\": {\n            \"Website\": {\"type\": \"website\", \"link\": \"https://108.mn/\"},\n            \"108\": {\"type\": \"phone\", \"link\": \"tel:108\"},\n            \"Email\": {\"type\": \"email\", \"link\": \"mailto:huuhdiinutas@yahoo.com\"},\n            \"Facebook\": {\"type\": \"facebook\", \"link\": \"https://www.facebook.com/108.mn\"},\n            \"Youtube\": {\"type\": \"youtube\", \"link\": \"https://www.youtube.com/channel/UC3rTtL_jr-8GEmliZk30D3Q\"},\n        },\n    },\n    \"Child Helpline Nepal 1098\": {\n        \"region\": \"Nepal\",\n        \"page\": \"https://childhelplineinternational.org/nepal-child-helpline-nepal-1098/\",\n        \"description\": \"1098 is the Child Helpline number in Nepal and is operated by the Child Workers Concerned Centre in Nepal (CWIN). CWIN runs six child helplines and a number of support homes and centres based throughout Nepal. The child helplines are located respectively in Kathmandu, Biratnagar, Hetauda, Nepalgunj, Pokhara and Dhangadhi and offer counselling, referrals, support and advice,\",\n        \"contacts\": {\n            \"Website\": {\"type\": \"website\", \"link\": \"http://www.cwin.org.np/\"},\n            \"1098\": {\"type\": \"phone\", \"link\": \"tel:1098\"},\n            \"Email\": {\"type\": \"email\", \"link\": \"mailto:cwinnepal1987@gmail.com\"},\n            \"Facebook\": {\"type\": \"facebook\", \"link\": \"https://www.facebook.com/cwin.org.np/\"},\n            \"Twitter\": {\"type\": \"twitter\", \"link\": \"https://twitter.com/CWINNepal\"},\n        },\n    },\n    \"0800 What\u2019s Up?\": {\n        \"region\": \"New Zealand\",\n        \"page\": \"https://childhelplineinternational.org/new-zealand-0800-whats-up/\",\n        \"description\": \"0800 Whats Up is a national helpline for tamariki and rangatahi in Aotearoa. Children and teenagers can either call for free or chat online with one of our counsellors.\",\n        \"contacts\": {\n            \"whatsup.co.nz\": {\"type\": \"website\", \"link\": \"http://www.whatsup.co.nz/\"},\n            \" +64 800 942 8787\": {\"type\": \"phone\", \"link\": \"tel:%20+64%20800%20942%208787\"},\n            \"Email\": {\"type\": \"email\", \"link\": \"mailto:whatsup@barnardos.org.nz\"},\n            \"Facebook\": {\"type\": \"facebook\", \"link\": \"http://www.facebook.com/0800Whatsup\"},\n            \"Instagram\": {\"type\": \"instragram\", \"link\": \"http://www.instagram.com/0800whatsup\"},\n        },\n    },\n    \"Youthline\": {\n        \"region\": \"New Zealand\",\n        \"page\": \"https://childhelplineinternational.org/new-zealand-youthline/\",\n        \"description\": \"Youthline offers a free 24/7 Helpline service (text, phone, webchat & email), free face-to-face counselling services, youth mentoring, programmes in schools and communities to help people grow and develop.\",\n        \"contacts\": {\n            \"Website\": {\"type\": \"website\", \"link\": \"http://www.youthline.co.nz\"},\n            \"0800 376633\": {\"type\": \"phone\", \"link\": \"tel:0800%20376633\"},\n            \"Text 234\": {\"type\": \"comment\", \"link\": \"sms:234\"},\n            \"Email\": {\"type\": \"email\", \"link\": \"mailto:talk@youthline.co.nz\"},\n            \"Facebook\": {\"type\": \"facebook\", \"link\": \"https://www.facebook.com/Youthline.Changing.Lives/\"},\n            \"Instagram\": {\"type\": \"instragram\", \"link\": \"https://www.instagram.com/youthlinenz/?hl=en\"},\n            \"Twitter\": {\"type\": \"twitter\", \"link\": \"https://twitter.com/Youthline\"},\n        },\n    },\n    \"Madadgaar National Helpline\": {\n        \"region\": \"Pakistan\",\n        \"page\": \"https://childhelplineinternational.org/pakistan-madadgaar-national-helpline/\",\n        \"description\": \"The Madadgaar National Helpline is a toll free helpline number, accessible 24/7 for children experiencing violence and abuse. The helpline acts as a portal and guide for survivors of abuse to a wide array of possibly unknown organisations out there who are willing to help them get justice, shelter and a way to start their lives anew.\",\n        \"contacts\": {\n            \"1098\": {\"type\": \"phone\", \"link\": \"tel:1098\"},\n            \"Email\": {\"type\": \"email\", \"link\": \"mailto:madadgaar@cyber.net.pk\"},\n            \"Facebook\": {\"type\": \"facebook\", \"link\": \"https://www.facebook.com/madadgaar1098helpline/?fref=ts\"},\n            \"Instagram\": {\"type\": \"instragram\", \"link\": \"https://www.instagram.com/madadgaar1098helpline/\"},\n            \"Twitter\": {\"type\": \"twitter\", \"link\": \"https://twitter.com/madadgaar1\"},\n        },\n    },\n    \"1-Tok Kaunselin Helpim Lain\": {\n        \"region\": \"Papua New Guinea\",\n        \"page\": \"https://childhelplineinternational.org/papua-new-guinea-1-tok-kaunselin-helpim-lain/\",\n        \"description\": \"1-Tok Kaunselin Helpim Lain is a toll \u2013free confidential phone counselling service providing information and support for anyone experiencing family and sexual violence in Papua New Guinea.\",\n        \"contacts\": {\n            \"Website\": {\n                \"type\": \"website\",\n                \"link\": \"https://www.childfund.org.au/media-news/1-tok-kaunselin-helpim-lain-launches-24-hour-service-and-new-facebook-page/\",\n            },\n            \"+675 7150 8000\": {\"type\": \"phone\", \"link\": \"tel:+67571508000\"},\n            \"Facebook\": {\"type\": \"facebook\", \"link\": \"https://www.facebook.com/1TokHelpimLain/\"},\n        },\n    },\n    \"Bantay Bata 163\": {\n        \"region\": \"Philippines\",\n        \"page\": \"https://childhelplineinternational.org/philippines-bantay-bata-163/\",\n        \"description\": \"Bantay Bata 163 is a child welfare program launched in 1997 to protect disadvantaged and at-risk children through a nationwide network of social services. Dialing 1-6-3 means hope for Filipino children and their families suffering in silence due to child abuse and the helpline also provides community outreach and medical and dental services.\",\n        \"contacts\": {\n            \"Website\": {\"type\": \"website\", \"link\": \"http://www.bantaybata163.com\"},\n            \"163\": {\"type\": \"phone\", \"link\": \"tel:163\"},\n            \"Email\": {\"type\": \"email\", \"link\": \"mailto:foundation@abs-cbnfoundation.com\"},\n            \"Facebook\": {\"type\": \"facebook\", \"link\": \"https://www.facebook.com/abscbnfoundationkapamilya\"},\n            \"Instagram\": {\"type\": \"instragram\", \"link\": \"https://www.instagram.com/abscbnfoundation/\"},\n            \"Twitter\": {\"type\": \"twitter\", \"link\": \"https://twitter.com/BantayBata163\"},\n        },\n    },\n    \"Tinkle Friend Helpline\": {\n        \"region\": \"Singapore\",\n        \"page\": \"https://childhelplineinternational.org/singapore-tinkle-friend-helpline/\",\n        \"description\": \"Tinkle Friend is a national toll-free helpline (1800 2744 788) and chatline for all primary-school-aged children in Singapore, run by Singapore Children\u2019s Society. Tinkle Friend provides support, advice, and information to lonely and distressed children, especially in situations when their parents or main caregivers are unavailable.\",\n        \"contacts\": {\n            \"Website\": {\"type\": \"website\", \"link\": \"https://www.tinklefriend.sg/\"},\n            \"1800 2744 788\": {\"type\": \"phone\", \"link\": \"tel:1800%202744%20788\"},\n            \"Email\": {\"type\": \"email\", \"link\": \"mailto:info@childrensociety.org.sg\"},\n            \"Facebook\": {\"type\": \"facebook\", \"link\": \"https://www.facebook.com/SingaporeChildrensSociety/?fref=ts\"},\n            \"Twitter\": {\"type\": \"twitter\", \"link\": \"https://twitter.com/SgChildrenSoc\"},\n        },\n    },\n    \"Childline Sri Lanka 1929\": {\n        \"region\": \"Sri Lanka\",\n        \"page\": \"https://childhelplineinternational.org/sri-lanka-childline-sri-lanka-1929/\",\n        \"description\": \"Operated by the National Child Protection Authority, 1929 can be contacted free of charge through any telecom provider, from anywhere in the country, at any time of the day, any day of the week, in Sinhala / Tamil / English languages.\",\n        \"contacts\": {\n            \"Website\": {\"type\": \"website\", \"link\": \"http://www.childprotection.gov.lk/\"},\n            \"1929\": {\"type\": \"phone\", \"link\": \"tel:1929\"},\n            \"Email\": {\"type\": \"email\", \"link\": \"mailto:ncpa@childprotection.gov.lk\"},\n            \"Facebook\": {\"type\": \"facebook\", \"link\": \"https://www.facebook.com/NCPASriLanka/\"},\n            \"Twitter\": {\"type\": \"twitter\", \"link\": \"https://twitter.com/ChildLanka?t=xLYQm...54X6w&s=08\"},\n        },\n    },\n    \"Don Bosco Lama Sarana\": {\n        \"region\": \"Sri Lanka\",\n        \"page\": \"https://childhelplineinternational.org/sri-lanka-don-bosco-lama-sarana/\",\n        \"contacts\": {\n            \"Website\": {\"type\": \"website\", \"link\": \"http://www.donbosco.lk/\"},\n            \"Facebook\": {\"type\": \"facebook\", \"link\": \"https://www.facebook.com/donbosco.srilanka/\"},\n        },\n    },\n    \"113 Protection Hotline\": {\n        \"region\": \"Taiwan\",\n        \"page\": \"https://childhelplineinternational.org/taiwan-113-protection-hotline/\",\n        \"description\": \"113 Protection Hotline\",\n        \"contacts\": {\n            \"worldvision.org.tw\": {\"type\": \"website\", \"link\": \"https://www.worldvision.org.tw/\"},\n            \"113\": {\"type\": \"phone\", \"link\": \"tel:113\"},\n            \"+886 2 8195 3005\": {\"type\": \"phone\", \"link\": \"tel:+886281953005\"},\n            \"Facebook\": {\"type\": \"facebook\", \"link\": \"https://www.facebook.com/WorldVisionTW/\"},\n            \"Instagram\": {\n                \"type\": \"instragram\",\n                \"link\": \"https://www.instagram.com/worldvisiontaiwan/?fbclid=IwAR0hhpBJkUBzgc-m3JhpssZMquwnksHoP_dyGMFIN4I88JadV5Z6e2IFc5o\",\n            },\n            \"YouTube\": {\"type\": \"youtube\", \"link\": \"https://www.youtube.com/user/worldvisiontaiwan\"},\n            \"worldvisiontw\": {\"type\": \"line\", \"link\": \"https://page.line.me/ixk2481x\"},\n        },\n    },\n    \"Childline Thailand \u2013 Saidek 1387\": {\n        \"region\": \"Thailand\",\n        \"page\": \"https://childhelplineinternational.org/thailand-childline-thailand-saidek-1387/\",\n        \"description\": \"Childline provides its services for any child under the age of 18. The foundation works with various government and NGO stakeholders to safeguard the rights of every child as outlined by the United Nations\u2019 Convention on the Rights of a Child (CRC). Staff in the call center are ready to take phone calls and respond to online messages 24 hours a day, 365 day a year \u2013 just like any other emergency support service.\",\n        \"contacts\": {\n            \"Website\": {\"type\": \"website\", \"link\": \"http://childlinethailand.org/\"},\n            \"1387\": {\"type\": \"phone\", \"link\": \"tel:1387\"},\n            \"Facebook\": {\"type\": \"facebook\", \"link\": \"https://www.facebook.com/childlinethailand\"},\n            \"Instagram\": {\"type\": \"instragram\", \"link\": \"https://www.instagram.com/saidek1387/\"},\n            \"Twitter\": {\"type\": \"twitter\", \"link\": \"https://twitter.com/saidek1387\"},\n        },\n    },\n    \"Children and Family Support Association of Uzbekistan\": {\n        \"region\": \"Uzbekistan\",\n        \"page\": \"https://childhelplineinternational.org/uzbekistan-children-and-family-support-association-of-uzbekistan/\",\n        \"description\": \"A child helpline is under development by the Children and Family Support Association of Uzbekistan.\",\n        \"contacts\": {},\n    },\n    \"Vanuatu Youth Toll-Free Helpline\": {\n        \"region\": \"Vanuatu\",\n        \"page\": \"https://childhelplineinternational.org/vanuatu-vanuatu-youth-toll-free-helpline/\",\n        \"description\": \"Operated by the Vanuatu Family Health Association (VFHA), the Youth Toll-free HELPLINE 087777 is a confidential telephone and referral service, connecting professional expertise to communities across Vanuatu.\",\n        \"contacts\": {\n            \"Website\": {\"type\": \"website\", \"link\": \"https://vfha15.wordpress.com/\"},\n            \"087777\": {\"type\": \"phone\", \"link\": \"tel:087777\"},\n        },\n    },\n    \"National Hotline for Child Protection 111\": {\n        \"region\": \"Vietnam\",\n        \"page\": \"https://childhelplineinternational.org/vietnam-national-hotline-for-child-protection-111/\",\n        \"description\": \"The Vietnamese National Hotline for Child Protection is run by the Department of Child Affairs, part of the Ministry of Labour, Invalids and Social Affairs. While the hotline deals primarily with information, reports and denunciations on risks and acts of child abuse, it also provides counselling to children.\",\n        \"contacts\": {\n            \"Website\": {\"type\": \"website\", \"link\": \"http://tongdai111.vn/\"},\n            \"111\": {\"type\": \"phone\", \"link\": \"tel:111\"},\n        },\n    },\n    \"ALO 116 111\": {\n        \"region\": \"Albania\",\n        \"page\": \"https://childhelplineinternational.org/alo-116-111/\",\n        \"description\": \"The National Telephone Line for Children (ALO 116) is a free counseling and referral service for children. This service communicates with children through two numbers:\",\n        \"contacts\": {\n            \"Website\": {\"type\": \"website\", \"link\": \"mailto:alo116@alo116.al\"},\n            \"116 111\": {\"type\": \"phone\", \"link\": \"tel:116111\"},\n            \"Email\": {\"type\": \"email\", \"link\": \"https://orgchil-jicorica.savviihq.com/?p=2511\"},\n            \"Facebook\": {\"type\": \"facebook\", \"link\": \"https://www.facebook.com/ALO116\"},\n            \"Instagram\": {\"type\": \"instragram\", \"link\": \"https://www.instagram.com/linja_e_keshillimit_per_femije/\"},\n            \"Twitter\": {\"type\": \"twitter\", \"link\": \"https://twitter.com/alo116albania\"},\n            \"Youtube\": {\"type\": \"youtube\", \"link\": \"https://www.youtube.com/channel/UCBWR-J810zmZgQ67QyAjbGw\"},\n        },\n    },\n    \"FAR Child Protection Hotline & Helpline\": {\n        \"region\": \"Armenia\",\n        \"page\": \"https://childhelplineinternational.org/armenia-far-child-protection-hotline-helpline/\",\n        \"description\": \"The FAR Children\u2019s Center transforms young lives through crisis intervention, critical short-term rehabilitation and stabilization to those who are victims of some of the most horrible predicaments including abuse, abandonment and neglect, and behavioral, emotional and psychological problems. The Center is a full-service facility, which offers access to shelter, medical care, counseling and legal assistance. It provides sources of educational and psychosocial support, life skills training, and guidance for family members when safe reunification is possible.\",\n        \"contacts\": {\n            \"Website\": {\"type\": \"website\", \"link\": \"https://www.farusa.org/\"},\n            \"0800 61 111\": {\"type\": \"phone\", \"link\": \"tel:080061111\"},\n            \"Facebook\": {\"type\": \"facebook\", \"link\": \"https://www.facebook.com/farusa\"},\n            \"Instagram\": {\"type\": \"instragram\", \"link\": \"https://www.instagram.com/far.usa/\"},\n            \"Twitter\": {\"type\": \"twitter\", \"link\": \"https://twitter.com/farusa\"},\n        },\n    },\n    \"147 Rat Auf Draht\": {\n        \"region\": \"Austria\",\n        \"page\": \"https://childhelplineinternational.org/austria-147-rat-auf-draht/\",\n        \"description\": \"147 Rat auf Draht is the Austrian emergency number for children and young people. The number can be reached anonymously and free of charge around the clock.\",\n        \"contacts\": {\n            \"116 111\": {\"type\": \"phone\", \"link\": \"tel:\"},\n            \"Email\": {\"type\": \"email\", \"link\": \"mailto:147@rataufdraht.at\"},\n            \"Facebook\": {\"type\": \"facebook\", \"link\": \"https://www.facebook.com/rataufdraht\"},\n            \"Instagram\": {\"type\": \"instragram\", \"link\": \"https://www.instagram.com/147rataufdraht/\"},\n            \"YouTube\": {\"type\": \"youtube\", \"link\": \"https://www.youtube.com/user/Helpline147\"},\n        },\n    },\n    \"Azerbaijan Child Helpline\": {\n        \"region\": \"Azerbaijan\",\n        \"page\": \"https://childhelplineinternational.org/azerbaijan-azerbaijan-child-helpline/\",\n        \"description\": \"The Azerbaijan Child Helpline has been operating since 2010. It was created with the support of the Ministry of Education, the Ministry of Labor and Social Protection, UNICEF, World Vision, Save the children and Azercell.\",\n        \"contacts\": {\n            \"Website\": {\"type\": \"website\", \"link\": \"https://childhelpline.az/index.php/az/\"},\n            \"116 111\": {\"type\": \"phone\", \"link\": \"tel:116111\"},\n            \"Email\": {\"type\": \"email\", \"link\": \"mailto:reliablefuturengo@gmail.com\"},\n            \"Facebook\": {\"type\": \"facebook\", \"link\": \"https://www.facebook.com/ReliableFutureNGO/\"},\n            \"Instagram\": {\"type\": \"instragram\", \"link\": \"\"},\n        },\n    },\n    \"AWEL\": {\n        \"region\": \"Belgium\",\n        \"page\": \"https://childhelplineinternational.org/belgium-awel/\",\n        \"description\": \"AWEL listens to all children and young people with a question, a story and/or a problem. AWEL works completely anonymously . This means that only the caller and AWEL are aware of the contact. Moreover, a conversation with Awel is completely free.\",\n        \"contacts\": {\n            \"Website\": {\"type\": \"website\", \"link\": \"https://www.awel.be/\"},\n            \"102\": {\"type\": \"phone\", \"link\": \"tel:102\"},\n            \"Email\": {\"type\": \"email\", \"link\": \"mailto:info@awel.be\"},\n            \"Facebook\": {\"type\": \"facebook\", \"link\": \"https://www.facebook.com/awelvzw/\"},\n            \"Instagram\": {\"type\": \"instragram\", \"link\": \"https://www.instagram.com/awel.be_/\"},\n            \"Twitter\": {\"type\": \"twitter\", \"link\": \"https://twitter.com/awel_vzw\"},\n        },\n    },\n    \"Plavi Telefon\": {\n        \"region\": \"Bosnia & Herzegovina\",\n        \"page\": \"https://childhelplineinternational.org/bosnia-herzegovina-plavi-telefon/\",\n        \"description\": \"Plavi Telefon is a toll-free confidential and anonymous counseling line for children, young people and adults who can contact the helpline if they have a problem and want to share how you feel and what is bothering them.\",\n        \"contacts\": {\n            \"Website\": {\"type\": \"website\", \"link\": \"http://www.plavitelefon.ba/\"},\n            \"080 05 03 05\": {\"type\": \"phone\", \"link\": \"tel:080050305\"},\n            \"Email\": {\"type\": \"email\", \"link\": \"mailto:info@novageneracija.org\"},\n            \"Facebook\": {\"type\": \"facebook\", \"link\": \"https://www.facebook.com/PlaviTelefonBiH/\"},\n            \"Instagram\": {\"type\": \"instragram\", \"link\": \"https://www.instagram.com/plavi.telefon/\"},\n        },\n    },\n    \"National Telephone Line for Children 116 111\": {\n        \"region\": \"Bulgaria\",\n        \"page\": \"https://childhelplineinternational.org/bulgaria-national-telephone-line-for-children-116-111/\",\n        \"description\": \"The National Telephone Line for Children 116 111 is managed and administered by the State Agency for Child Protection with the aim of supporting all children and their families in Bulgaria. The operators who answer the calls are trained psychologists who 24 hours a day, 7 days a week, anonymously and completely free of charge, are ready to listen, support, consult and guide the callers on all issues that concern them.\",\n        \"contacts\": {\n            \"Website\": {\"type\": \"website\", \"link\": \"https://www.116111.bg/\"},\n            \"116111\": {\"type\": \"phone\", \"link\": \"tel:116111\"},\n            \"Facebook\": {\"type\": \"facebook\", \"link\": \"https://www.facebook.com/sacp.government.bg\"},\n            \"Instagram\": {\"type\": \"instragram\", \"link\": \"https://www.instagram.com/sacp_bulgaria/\"},\n            \"Youtube\": {\"type\": \"youtube\", \"link\": \"https://www.youtube.com/channel/UCIUOyilakRTJNCHnI-_oj4w\"},\n            \"LinkedIn\": {\n                \"type\": \"linkedin\",\n                \"link\": \"https://www.linkedin.com/company/state-agency-for-child-protection/\",\n            },\n        },\n    },\n    \"Hrabri Telefon\": {\n        \"region\": \"Croatia\",\n        \"page\": \"https://childhelplineinternational.org/hrabritelefon/\",\n        \"description\": \"Hrabri Telefon is a non-profit non-governmental organization founded with the aim of preventing abuse, neglect and unacceptable behavior of children and youth, and providing direct help and support to abused and neglected children and their families.\",\n        \"contacts\": {\n            \"hrabritelefon.hr\": {\"type\": \"website\", \"link\": \"http://hrabritelefon.hr\"},\n            \"116 111\": {\"type\": \"phone\", \"link\": \"tel:116%20111\"},\n            \"0800 0800\": {\"type\": \"phone\", \"link\": \"tel:0800%200800\"},\n            \"Email\": {\"type\": \"email\", \"link\": \"mailto:savjet@hrabritelefon.hr\"},\n            \"Facebook\": {\"type\": \"facebook\", \"link\": \"https://www.facebook.com/HrabriTelefon/\"},\n            \"Viber\": {\"type\": \"viber\", \"link\": \"http://bit.ly/Hrabri-Telefon\"},\n        },\n    },\n    \"Call 116111 Cyprus\": {\n        \"region\": \"Cyprus\",\n        \"page\": \"https://childhelplineinternational.org/cyprus-call-116111-cyprus/\",\n        \"description\": \"\u201cHope For Children\u201d CRC Policy Center is an International Humanitarian and Independent Organization, based in Nicosia, Cyprus. Our Organization was established on the basis of the standards and principles of the United Nations Convention on the Rights of the Child and the Law of the European Union.\",\n        \"contacts\": {\n            \"Website\": {\"type\": \"website\", \"link\": \"https://www.uncrcpc.org.cy/\"},\n            \"116 111\": {\"type\": \"phone\", \"link\": \"tel:116111\"},\n            \"Email\": {\"type\": \"email\", \"link\": \"mailto:info@uncrcpc.org\"},\n            \"Facebook\": {\"type\": \"facebook\", \"link\": \"https://www.facebook.com/Hopeforchildren.crcpolicycenter\"},\n            \"Instagram\": {\"type\": \"instragram\", \"link\": \"https://www.instagram.com/hopeforchildrencrcpolicycenter/\"},\n            \"Twitter\": {\"type\": \"twitter\", \"link\": \"https://twitter.com/infouncrc\"},\n            \"Youtube\": {\"type\": \"youtube\", \"link\": \"https://www.youtube.com/user/TheOfficeforchildren\"},\n        },\n    },\n    \"Linka Bezpe\u010d\u00ed\": {\n        \"region\": \"Czechia\",\n        \"page\": \"https://childhelplineinternational.org/czechia-linka-bezpeci/\",\n        \"description\": \"Linka Bezpe\u010d\u00ed is for children and students under the age of 25 who have issues or worries that they cannot or cannot handle on their own. Children and young people can turn to the Safety Line for help and advice by phone, chat or e-mail.\",\n        \"contacts\": {\n            \"Website\": {\"type\": \"website\", \"link\": \"https://www.linkabezpeci.cz/\"},\n            \"116 111\": {\"type\": \"phone\", \"link\": \"tel:116111\"},\n            \"Email\": {\"type\": \"email\", \"link\": \"mailto:pomoc@linkabezpeci.cz\"},\n            \"Facebook\": {\"type\": \"facebook\", \"link\": \"https://www.facebook.com/Linkabezpeci\"},\n            \"Instagram\": {\"type\": \"instragram\", \"link\": \"https://www.instagram.com/linka_bezpeci/\"},\n            \"Twitter\": {\"type\": \"twitter\", \"link\": \"https://twitter.com/Linka_bezpeci\"},\n            \"Youtube\": {\"type\": \"youtube\", \"link\": \"https://www.youtube.com/user/SdruzeniLinkaBezpeci\"},\n            \"LinkedIn\": {\n                \"type\": \"linkedin\",\n                \"link\": \"https://www.linkedin.com/company/safety-line-association-linka-bezpe-/?originalSubdomain=nl\",\n            },\n        },\n    },\n    \"B\u00f8rneTelefonen\": {\n        \"region\": \"Denmark\",\n        \"page\": \"https://childhelplineinternational.org/denmark-bornetelefonen/\",\n        \"description\": \"B\u00f8rneTelefonen is YOUR line for advice, comfort or just an adult who has time to listen.\",\n        \"contacts\": {\n            \"Website\": {\n                \"type\": \"website\",\n                \"link\": \"https://bornsvilkar.dk/?fbclid=IwAR3L3G9-wLiF85t13AbPxA4Xq7Wdws7DGXobBrh3XRwVB9UdX6neHCOw3rQ\",\n            },\n            \"116 111\": {\"type\": \"phone\", \"link\": \"tel:116111\"},\n            \"Email\": {\"type\": \"email\", \"link\": \"mailto:%20bv@bornsvilkar.dk\"},\n            \"Facebook\": {\"type\": \"facebook\", \"link\": \"https://www.facebook.com/boernsvilkaar\"},\n            \"Instagram\": {\"type\": \"instragram\", \"link\": \"https://www.instagram.com/bornsvilkar/\"},\n            \"Twitter\": {\"type\": \"twitter\", \"link\": \"https://twitter.com/bornsvilkar\"},\n        },\n    },\n    \"Lapsemure\": {\n        \"region\": \"Estonia\",\n        \"page\": \"https://childhelplineinternational.org/estonia-lapsemure/\",\n        \"description\": \"Lapsemure is an online forum hosted by the Estonian Mental Health Society.\",\n        \"contacts\": {\"Website\": {\"type\": \"website\", \"link\": \"http://www.lapsemure.ee/forum_est/\"}},\n    },\n    \"Lasten ja Nuorten Puhelin ja Netti \u2013 Child and Youth Phone\": {\n        \"region\": \"Finland\",\n        \"page\": \"https://childhelplineinternational.org/finland-lasten-ja-nuorten-puhelin-ja-netti-child-and-youth-phone/\",\n        \"description\": \"MLL\u2019s children\u2019s and young people\u2019s telephone provides the ability for children to call, write letters or online chat about any issues they might be having. Conversations are answered by confidential, volunteer adults who have time to listen. Children can contact the helpline anonymously every day of the year for any matter.\",\n        \"contacts\": {\n            \"Website\": {\"type\": \"website\", \"link\": \"http://www.mll.fi/nuortennetti/\"},\n            \"116 111\": {\"type\": \"phone\", \"link\": \"tel:116111\"},\n            \"WhatsApp\": {\"type\": \"whatsapp\", \"link\": \"\"},\n            \"Email\": {\n                \"type\": \"email\",\n                \"link\": \"mailto:?subject=Lasten%20ja%20nuorten%20puhelin&body=https://www.nuortennetti.fi/apua-ja-tukea/lasten-ja-nuorten-puhelin/\",\n            },\n            \"Facebook\": {\"type\": \"facebook\", \"link\": \"https://www.facebook.com/MLLverkkarit/\"},\n            \"Instagram\": {\"type\": \"instragram\", \"link\": \"https://www.instagram.com/mll_nuortennetti/\"},\n            \"Twitter\": {\"type\": \"twitter\", \"link\": \"https://twitter.com/MLL_fi\"},\n            \"Youtube\": {\"type\": \"youtube\", \"link\": \"https://www.youtube.com/user/MLLnuortennetti\"},\n            \"LinkedIn\": {\n                \"type\": \"linkedin\",\n                \"link\": \"https://www.linkedin.com/company/mannerheim-league-for-child-welfare\",\n            },\n        },\n    },\n    \"119 \u2013 All\u00f4 Enfance en Danger\": {\n        \"region\": \"France\",\n        \"page\": \"https://childhelplineinternational.org/france-119-allo-enfance-en-danger/\",\n        \"description\": \"119 \u2013 All\u00f4 Enfance en Dangerwas established in 1989. ince 1997, they have been using he 119 number. The child helpline is funded by the State and Departments.\",\n        \"contacts\": {\n            \"Website\": {\"type\": \"website\", \"link\": \"https://www.allo119.gouv.fr/\"},\n            \"119\": {\"type\": \"phone\", \"link\": \"tel:119\"},\n            \"Instagram\": {\"type\": \"instragram\", \"link\": \"https://www.instagram.com/119alloenfanceendanger/\"},\n        },\n    },\n    \"Kinder- und Jugendtelefon\": {\n        \"region\": \"Germany\",\n        \"page\": \"https://childhelplineinternational.org/germany-kinder-und-jugendtelefon/\",\n        \"description\": \"Nummer gegen Kummer is the umbrella organization of the largest free telephone counseling service for children, young people and parents in Germany.\",\n        \"contacts\": {\n            \"Website\": {\"type\": \"website\", \"link\": \"https://www.nummergegenkummer.de/\"},\n            \"116 111\": {\"type\": \"phone\", \"link\": \"tel:116111\"},\n            \"Email\": {\"type\": \"email\", \"link\": \"mailto:info@nummergegenkummer.de\"},\n            \"Facebook\": {\"type\": \"facebook\", \"link\": \"https://www.facebook.com/ngk.dachverband\"},\n            \"Instagram\": {\"type\": \"instragram\", \"link\": \"https://www.instagram.com/nummergegenkummer_e.v/\"},\n        },\n    },\n    \"The Smile of the Child\": {\n        \"region\": \"Greece\",\n        \"page\": \"https://childhelplineinternational.org/greece-the-smile-of-the-child/\",\n        \"description\": \"The Smile of the Child\u2019s helpline is available to every child and adult to provide support in matters that concern them. Calls to the child helpline are free from landline and mobile phones. The child helpline receiving anonymous and anonymous reports for children at risk (abuse, trafficking, unaccompanied children), activates procedures for the immediate provision of protection of children at risk (on-the-spot intervention), provides directions on each topic and links contacts to other services and other interventions.\",\n        \"contacts\": {\n            \"Website\": {\"type\": \"website\", \"link\": \"http://www.hamogelo.gr/\"},\n            \"116 111\": {\"type\": \"phone\", \"link\": \"tel:116111\"},\n            \"1056\": {\"type\": \"phone\", \"link\": \"tel:1056\"},\n            \"Email\": {\"type\": \"email\", \"link\": \"mailto:info@hamogelo.gr\"},\n            \"Facebook\": {\"type\": \"facebook\", \"link\": \"https://www.facebook.com/hamogelo.org/\"},\n            \"Instagram\": {\"type\": \"instragram\", \"link\": \"https://www.instagram.com/hamogelo/\"},\n            \"Twitter\": {\"type\": \"twitter\", \"link\": \"https://twitter.com/hamogelo\"},\n            \"LinkedIn\": {\"type\": \"linkedin\", \"link\": \"https://www.linkedin.com/company/---the-smile-of-the-child\"},\n        },\n    },\n    \"Together for Children 11525 Helpline\": {\n        \"region\": \"Greece\",\n        \"page\": \"https://childhelplineinternational.org/greece-together-for-children-11525-helpline/\",\n        \"contacts\": {\n            \"Website\": {\"type\": \"website\", \"link\": \"http://www.mazigiatopaidi.gr/\"},\n            \"115 25\": {\"type\": \"phone\", \"link\": \"tel:11525\"},\n            \"Email\": {\"type\": \"email\", \"link\": \"mailto:info@mazigiatopaidi.gr\"},\n            \"Facebook\": {\"type\": \"facebook\", \"link\": \"https://www.facebook.com/mazigiatopaidi\"},\n            \"Instagram\": {\"type\": \"instragram\", \"link\": \"https://www.instagram.com/mazigiatopaidi/\"},\n            \"Twitter\": {\"type\": \"twitter\", \"link\": \"https://twitter.com/mazigiatopaidi\"},\n            \"Youtube\": {\"type\": \"youtube\", \"link\": \"https://www.youtube.com/channel/UCYZ4tA4Qgn9SFdFYVmKAZAw\"},\n            \"LinkedIn\": {\n                \"type\": \"linkedin\",\n                \"link\": \"https://www.linkedin.com/company/together-for-children-ngo-mazi-gia-to-paidi-/\",\n            },\n        },\n    },\n    \"K\u00e9k Vonal\": {\n        \"region\": \"Hungary\",\n        \"page\": \"https://childhelplineinternational.org/hungary-lelkisegely-vonal/\",\n        \"description\": \"K\u00e9k Vonal was created for children, teenagers and young adults, so that they always have someone to turn to! Their trained caregivers are available on the toll-free helpline at 116111, 24 hours a day.\",\n        \"contacts\": {\n            \"Website\": {\"type\": \"website\", \"link\": \"https://kek-vonal.hu/igy-segitunk/a-116-111-segelyvonalrol/\"},\n            \"116111 (Hungarian)\": {\"type\": \"phone\", \"link\": \"tel:116111\"},\n            \"+36 80 984 590 (Ukrainian/Russian)\": {\"type\": \"phone\", \"link\": \"tel:+36%2080%20984%20590\"},\n            \"Email (Hungarian)\": {\"type\": \"email\", \"link\": \"mailto:info@kek-vonal.hu\"},\n            \"Email (Ukrainian/Russian)\": {\"type\": \"email\", \"link\": \"mailto:ukraine@kek-vonal.hu.%20\"},\n            \"Facebook\": {\"type\": \"facebook\", \"link\": \"https://www.facebook.com/kekvonal/\"},\n            \"Instagram\": {\"type\": \"instragram\", \"link\": \"https://www.instagram.com/kekvonal/\"},\n            \"Youtube\": {\"type\": \"youtube\", \"link\": \"https://www.youtube.com/@kekvonalgyermekkrizisalapi8382\"},\n            \"LinkedIn\": {\"type\": \"linkedin\", \"link\": \"https://www.linkedin.com/company/kekvonal/?originalSubdomain=hu\"},\n            \"TikTok\": {\"type\": \"tiktok\", \"link\": \"https://www.tiktok.com/@kekvonal116111\"},\n        },\n    },\n    \"Red Cross Helpline 1717 \u2013 Hj\u00e1lpars\u00edminn 1717\": {\n        \"region\": \"Iceland\",\n        \"page\": \"https://childhelplineinternational.org/iceland-red-cross-helpline-1717-hjalparsiminn-1717/\",\n        \"description\": \"Red Cross Helpline is always open, free, confidential and anonymous and volunteers provide all those who contact with active listening, psychological support and information about the resources available in Iceland.\",\n        \"contacts\": {\n            \"Website\": {\"type\": \"website\", \"link\": \"https://www.raudikrossinn.is/\"},\n            \"1717\": {\"type\": \"phone\", \"link\": \"tel:1717\"},\n            \"Email\": {\"type\": \"email\", \"link\": \"mailto:info@redcross.is\"},\n            \"Facebook\": {\"type\": \"facebook\", \"link\": \"https://www.facebook.com/raudikrossinn/\"},\n            \"Instagram\": {\"type\": \"instragram\", \"link\": \"https://www.instagram.com/raudikrossinn/\"},\n            \"Twitter\": {\"type\": \"twitter\", \"link\": \"https://twitter.com/raudikrossinn\"},\n        },\n    },\n    \"ISPCC Childline\": {\n        \"region\": \"Ireland\",\n        \"page\": \"https://childhelplineinternational.org/ireland-ispcc-childline/\",\n        \"description\": \"Childline is Ireland\u2019s 24-hour national listening service for all children and young people (under the age of 18) in Ireland. It is private, confidential and non-judgemental and can be contacted for free from anywhere in Ireland.\",\n        \"contacts\": {\n            \"Website\": {\"type\": \"website\", \"link\": \"http://ispcc.ie/childline/\"},\n            \"1800 66 66 66\": {\"type\": \"phone\", \"link\": \"tel:1800%2066%2066%2066\"},\n            \"Text 50101\": {\"type\": \"comment\", \"link\": \"tel:%2050101\"},\n            \"Email\": {\"type\": \"email\", \"link\": \"mailto:ispcc@ispcc.ie\"},\n            \"Facebook\": {\"type\": \"facebook\", \"link\": \"https://www.facebook.com/ISPCCChildline\"},\n            \"Instagram\": {\"type\": \"instragram\", \"link\": \"https://www.instagram.com/childlinebyispcc/\"},\n            \"Twitter\": {\"type\": \"twitter\", \"link\": \"https://twitter.com/ISPCCChildline\"},\n        },\n    },\n    \"ERAN (Emotional First Aid in Israel)\": {\n        \"region\": \"Israel\",\n        \"page\": \"https://childhelplineinternational.org/israel-eran-emotional-first-aid-in-israel/\",\n        \"description\": \"ERAN was established in Jerusalem in 1971. Today, ERAN operates the largest hotline in Israel and provides mental health services on the telephone and online to the entire Israeli public, anonymously and immediately.\",\n        \"contacts\": {\n            \"Website\": {\n                \"type\": \"website\",\n                \"link\": \"https://www.eran.org.il/?fbclid=IwAR0rH28WI5SATYe0ZyWggJRb3gAGIOlRfu9gKcFTomkPsJD4ybUrFe9cvPI\",\n            },\n            \"1201\": {\"type\": \"phone\", \"link\": \"tel:1201\"},\n            \"Email\": {\"type\": \"email\", \"link\": \"mailto:eran1201@eran.org.il\"},\n            \"Instagram\": {\"type\": \"instragram\", \"link\": \"https://www.instagram.com/eran_1201/\"},\n            \"Youtube\": {\"type\": \"youtube\", \"link\": \"https://www.youtube.com/channel/UCDDx1k4i3siQaejE1VB82ig\"},\n        },\n    },\n    \"NATAL Helpline\": {\n        \"region\": \"Israel\",\n        \"page\": \"https://childhelplineinternational.org/israel-natal-helpline/\",\n        \"description\": \"NATAL Helpline provides support and psychological assistance to victims of trauma in the context of terrorism and war. The line operates in a special model and offers ongoing guidance to adults and children by phone or chat on the site.\",\n        \"contacts\": {\n            \"Website\": {\"type\": \"website\", \"link\": \"https://www.natal.org.il/\"},\n            \"1(800)363-363\": {\"type\": \"phone\", \"link\": \"tel:1800363363\"},\n            \"Facebook\": {\"type\": \"facebook\", \"link\": \"https://www.facebook.com/NatalIsrael/\"},\n            \"Instagram\": {\n                \"type\": \"instragram\",\n                \"link\": \"https://www.instagram.com/natal.org.il/?fbclid=IwAR3P9SBDgFNcUnAtKbW7RMQr8tAt36k09QlHnXgQfJ9ldD9XD2ep-4064B0\",\n            },\n            \"LinkedIn\": {\n                \"type\": \"linkedin\",\n                \"link\": \"https://www.linkedin.com/company/natal/?fbclid=IwAR18d1aQRm_sdOOtesCqSGj96wJMjnhbTiMVL19lbTtbXRA219EghLHi-rA\",\n            },\n        },\n    },\n    \"Hello Telefono Azzurro\": {\n        \"region\": \"Italy\",\n        \"page\": \"https://childhelplineinternational.org/italy-hello-telefono-azzurro/\",\n        \"description\": \"Telefono Azzurro listens to children and adolescents every day and offers concrete answers to their requests for help, also through collaboration with institutions, associations and other territorial realities. It operates in an international context for the promotion of a culture of rights.\",\n        \"contacts\": {\n            \"Website\": {\"type\": \"website\", \"link\": \"https://azzurro.it/\"},\n            \"1 96 96\": {\"type\": \"phone\", \"link\": \"tel:19696\"},\n            \"Email\": {\"type\": \"email\", \"link\": \"mailto:info@azzurro.it\"},\n            \"Facebook\": {\"type\": \"facebook\", \"link\": \"https://www.facebook.com/TelefonoAzzurroOnlus/\"},\n            \"Instagram\": {\"type\": \"instragram\", \"link\": \"https://www.instagram.com/telefono_azzurro/\"},\n            \"Twitter\": {\"type\": \"twitter\", \"link\": \"https://twitter.com/telefonoazzurro\"},\n            \"Youtube\": {\"type\": \"youtube\", \"link\": \"https://www.youtube.com/channel/UCiSBypJxpwMVDaSA2zsuTIQ\"},\n            \"LinkedIn\": {\n                \"type\": \"linkedin\",\n                \"link\": \"https://www.linkedin.com/company/s.o.s.---telefono-azzurro/?originalSubdomain=it\",\n            },\n        },\n    },\n    \"Uzticibas Talrunis \u2013 Child and Adolescent Helpline 116 111\": {\n        \"region\": \"Latvia\",\n        \"page\": \"https://childhelplineinternational.org/latvia-uzticibas-talrunis-child-and-adolescent-helpline-116-111/\",\n        \"description\": \"The Child and Adolescent Helpline 116111 of the State Inspectorate for the Protection of the Rights of the Child is a free 24-hour helpdesk providing professional psychological assistance and support in crisis situations. The hotline is designed to provide psychological help to children and adolescents, as well as support in crisis situations. The most important task of the hotline is to listen to and support children and adolescents, helping them to deal with different issues and life situations.\",\n        \"contacts\": {\n            \"Website\": {\"type\": \"website\", \"link\": \"https://uzticibastalrunis.lv/\"},\n            \"116 111\": {\"type\": \"phone\", \"link\": \"tel:116111\"},\n            \"800 6008\": {\"type\": \"phone\", \"link\": \"tel:8006008\"},\n            \"Email\": {\"type\": \"email\", \"link\": \"https://orgchil-jicorica.savviihq.com/?p=2511\"},\n            \"Facebook\": {\"type\": \"facebook\", \"link\": \"https://www.facebook.com/UT116111/\"},\n            \"Instagram\": {\"type\": \"instragram\", \"link\": \"https://www.instagram.com/uzticibastalrunis.lv/\"},\n            \"Twitter\": {\"type\": \"twitter\", \"link\": \"https://twitter.com/talrunis116111\"},\n        },\n    },\n    \"Pro Juventute Beratung + Hilfe 147 - Liechtenstein\": {\n        \"region\": \"Liechtenstein\",\n        \"page\": \"https://childhelplineinternational.org/liechtenstein-projuventute/\",\n        \"description\": \"The Sorgentelefon fur Kinder und Jugendliche is a toll-free contact point for questions and problems of all kinds for children and young people in the Principality of Liechtenstein and can be reached at number 147 all year round, day and night, including Sundays and public holidays.\",\n        \"contacts\": {\n            \"Website\": {\"type\": \"website\", \"link\": \"http://www.147.ch/\"},\n            \"147\": {\"type\": \"phone\", \"link\": \"tel:147\"},\n            \"Email\": {\"type\": \"email\", \"link\": \"mailto:socialmedia@projuventute.ch\"},\n            \"Facebook\": {\"type\": \"facebook\", \"link\": \"https://www.facebook.com/ProJuventute.ch/\"},\n            \"Instagram\": {\"type\": \"instragram\", \"link\": \"https://www.instagram.com/147_schweiz/\"},\n            \"Twitter\": {\"type\": \"twitter\", \"link\": \"https://twitter.com/projuventute\"},\n        },\n    },\n    \"Vaiku Linija\": {\n        \"region\": \"Lithuania\",\n        \"page\": \"https://childhelplineinternational.org/lithuania-vaiku-linija/\",\n        \"description\": \"Vaiku Linija was established in 1997 and provides free and anonymous help to the children and teenagers by phone and on-line. Vaiku Linija consultants listen to all the children\u2019s stories and try to find ways together to solve their difficulties, to encourage them to share their worries with the people they trust. If necessary children are referred to other institutions.\",\n        \"contacts\": {\n            \"Website\": {\"type\": \"website\", \"link\": \"http://www.vaikulinija.lt/\"},\n            \"116 111\": {\"type\": \"phone\", \"link\": \"tel:116111\"},\n            \"Email\": {\"type\": \"email\", \"link\": \"mailto:vilnius@vaikulinija.lt\"},\n            \"Facebook\": {\"type\": \"facebook\", \"link\": \"https://www.facebook.com/vaikulinija/\"},\n            \"LinkedIn\": {\n                \"type\": \"linkedin\",\n                \"link\": \"https://www.linkedin.com/company/vaik%C5%B3-linija-child-line-?originalSubdomain=lt\",\n            },\n        },\n    },\n    \"Kanner Jugendtelefon (KJT)\": {\n        \"region\": \"Luxembourg\",\n        \"page\": \"https://childhelplineinternational.org/luxembourg-kanner-jugendtelefon-kjt/\",\n        \"description\": \"Kanner-Jugendtelefon (KJT) was founded in 1992 within the framework of the United Nations Convention on the Rights of the Child. KJT gives every child and every young person the opportunity to be heard.\",\n        \"contacts\": {\n            \"Website\": {\"type\": \"website\", \"link\": \"https://www.kjt.lu/de/\"},\n            \"116 111\": {\"type\": \"phone\", \"link\": \"tel:116111\"},\n            \"Email\": {\"type\": \"email\", \"link\": \"mailto:contact@kjt.lu\"},\n            \"Facebook\": {\"type\": \"facebook\", \"link\": \"https://www.facebook.com/kannerjugendtelefon\"},\n            \"Instagram\": {\"type\": \"instragram\", \"link\": \"https://www.instagram.com/kannerjugendtelefon/\"},\n            \"Twitter\": {\"type\": \"twitter\", \"link\": \"https://twitter.com/kajutel\"},\n            \"Youtube\": {\"type\": \"youtube\", \"link\": \"https://www.youtube.com/user/KannerJugendtelefon\"},\n        },\n    },\n    \"Kellimni.com\": {\n        \"region\": \"Malta\",\n        \"page\": \"https://childhelplineinternational.org/malta-kellimni-com/\",\n        \"description\": \"Kellimni.com offers free online support through SmartMessaging, e-mails, chat or participation in an online forum. All of its services are, free and confidential.\",\n        \"contacts\": {\n            \"Website\": {\"type\": \"website\", \"link\": \"https://kellimni.com/\"},\n            \"Email\": {\"type\": \"email\", \"link\": \"https://kellimni.com/email-us/\"},\n            \"Facebook\": {\"type\": \"facebook\", \"link\": \"https://www.facebook.com/kellimni\"},\n            \"Instagram\": {\"type\": \"instragram\", \"link\": \"https://www.instagram.com/kellimni/\"},\n            \"Twitter\": {\"type\": \"twitter\", \"link\": \"https://twitter.com/kellimnicom\"},\n        },\n    },\n    \"Support Line 179\": {\n        \"region\": \"Malta\",\n        \"page\": \"https://childhelplineinternational.org/malta-support-line-179/\",\n        \"description\": \"Supportline 179 is the national helpline in Malta, which offers support, information about local social welfare services and other agencies, and a referral service to callers who require assistance. The Supportline 179 is available on a 24/7 basis and all calls are free both from landlines and also from mobile phones. The organisation also operates the harmonised 116 111 number for Malta.\",\n        \"contacts\": {\n            \"Website\": {\"type\": \"website\", \"link\": \"https://fsws.gov.mt/en/appogg/Pages/supportline.aspx\"},\n            \"116 111\": {\"type\": \"phone\", \"link\": \"tel:116111\"},\n            \"116 123\": {\"type\": \"phone\", \"link\": \"tel:116123\"},\n        },\n    },\n    \"Telefonul Copilului 116111 Moldova\": {\n        \"region\": \"Moldova\",\n        \"page\": \"https://childhelplineinternational.org/moldova-telefonul-copilului-116111-moldova/\",\n        \"description\": \"Operated by the Ministry of Health, Labor and Social Protection, Telefonul Copilului 116111 Moldova is a free hotline that aims to protect children against any form of abuse of their rights, against any form of abuse and offers psychological counseling, information in the field of children\u2019s rights, child protection consultancy and guidance and referral to the institutions empowered to provide the necessary assistance.\",\n        \"contacts\": {\n            \"Website\": {\"type\": \"website\", \"link\": \"http://telefonulcopilului.md/\"},\n            \"116 111\": {\"type\": \"phone\", \"link\": \"tel:116111\"},\n            \"Skype\": {\"type\": \"skype\", \"link\": \"skype:TelefonulcopiluluiMoldova116111?add\"},\n            \"Facebook\": {\"type\": \"facebook\", \"link\": \"https://www.facebook.com/asociatiatelefonulcopilului/\"},\n        },\n    },\n    \"De Kindertelefoon\": {\n        \"region\": \"Netherlands\",\n        \"page\": \"https://childhelplineinternational.org/netherlands-de-kindertelefoon/\",\n        \"description\": \"De Kindertelefoon has been the place in the Netherlands where children can talk freely and confidentially about subjects that they do not dare, cannot or do not want to discuss in their own environment. We fulfill a statutory duty in this regard, which is why De Kindertelefoon is financed by the Ministry of Health, Welfare and Sport.\",\n        \"contacts\": {\n            \"Website\": {\"type\": \"website\", \"link\": \"https://www.kindertelefoon.nl/\"},\n            \"116 111\": {\"type\": \"phone\", \"link\": \"tel:116111\"},\n            \"0800 0432\": {\"type\": \"phone\", \"link\": \"tel:0800%200432\"},\n            \"Email\": {\"type\": \"email\", \"link\": \"mailto:landelijkbureau@kindertelefoon.nl\"},\n            \"Facebook\": {\"type\": \"facebook\", \"link\": \"https://www.facebook.com/dekindertelefoon.nl\"},\n            \"Instagram\": {\"type\": \"instragram\", \"link\": \"https://www.instagram.com/kindertelefoon.nl/\"},\n            \"Youtube\": {\"type\": \"youtube\", \"link\": \"https://www.youtube.com/user/LBKindertelefoon\"},\n            \"LinkedIn\": {\"type\": \"linkedin\", \"link\": \"https://www.linkedin.com/company/stichting-de-kindertelefoon/\"},\n        },\n    },\n    \"Helpwanted.nl\": {\n        \"region\": \"Netherlands\",\n        \"page\": \"https://childhelplineinternational.org/netherlands-helpwanted-nl/\",\n        \"description\": \"Helpwanted.nl is part of the Expertise Agency Online Child Abuse (EOKM). The EOKM is an independent foundation that is committed to the safety of all children.\",\n        \"contacts\": {\n            \"Website\": {\"type\": \"website\", \"link\": \"https://www.helpwanted.nl/\"},\n            \"+31 20 261 5275\": {\"type\": \"phone\", \"link\": \"tel:+31%2020%20261%205275\"},\n            \"Email\": {\"type\": \"email\", \"link\": \"mailto:info@helpwanted.nl\"},\n            \"Facebook\": {\"type\": \"facebook\", \"link\": \"https://www.facebook.com/Helpwantednl/\"},\n            \"Instagram\": {\"type\": \"instragram\", \"link\": \"https://www.instagram.com/helpwantednl/\"},\n        },\n    },\n    \"Alo Bushavko\": {\n        \"region\": \"North Macedonia\",\n        \"page\": \"https://childhelplineinternational.org/north-macedonia-alo-bushavko/\",\n        \"description\": \"Counseling for childrenAlo Bushavko provides support and counseling to children and youth. Counselors establish a trust-based relationship so that children or young people can share their emotions, needs, problems, learn about their rights, or seek advice or help.\",\n        \"contacts\": {\n            \"Website\": {\"type\": \"website\", \"link\": \"https://alobushavko.mk/mk/\"},\n            \"+389 70 390 632\": {\"type\": \"phone\", \"link\": \"tel:38970390632\"},\n            \"Email\": {\"type\": \"email\", \"link\": \"mailto:alobushavko@childrensembassy.org.mk\"},\n            \"Facebook\": {\"type\": \"facebook\", \"link\": \"https://www.facebook.com/megjashi/\"},\n            \"Instagram\": {\"type\": \"instragram\", \"link\": \"https://www.instagram.com/alobushavko/\"},\n            \"Twitter\": {\"type\": \"twitter\", \"link\": \"https://twitter.com/megjashi12345\"},\n            \"Youtube\": {\"type\": \"youtube\", \"link\": \"https://www.youtube.com/user/Megjashi\"},\n            \"LinkedIn\": {\n                \"type\": \"linkedin\",\n                \"link\": \"https://www.linkedin.com/in/children-s-embassy-detska-ambasada-0530415a/?originalSubdomain=mk\",\n            },\n            \"Flickr\": {\"type\": \"flickr\", \"link\": \"https://www.flickr.com/photos/childrens_embassy/\"},\n        },\n    },\n    \"Alarmtelefonen for barn og unge\": {\n        \"region\": \"Norway\",\n        \"page\": \"https://childhelplineinternational.org/norway-alarmtelefonen-for-barn-og-unge/\",\n        \"description\": \"\u201cAlarmtelefonen for barn og unge\u201d is a free telephone for children and young people who are exposed to violence, abuse and neglect. The alarm phone is open 24 hours a day all year round and also answers inquiries via chat, e-mail and SMS. Adults who are worried about children can also call.\",\n        \"contacts\": {\n            \"Website\": {\"type\": \"website\", \"link\": \"http://www.116111.no/\"},\n            \"116 111\": {\"type\": \"phone\", \"link\": \"tel:116111\"},\n            \"Email\": {\"type\": \"email\", \"link\": \"mailto:alarm@116111.no\"},\n            \"Facebook\": {\"type\": \"facebook\", \"link\": \"https://www.facebook.com/Alarmtelefonen\"},\n        },\n    },\n    \"Kors P\u00e5 Halsen\": {\n        \"region\": \"Norway\",\n        \"page\": \"https://childhelplineinternational.org/norway-kors-pa-halsen/\",\n        \"description\": \"Kors P\u00e5 Halsen was established in 1984 as the national child helpline for everyone up to the age of 18.\",\n        \"contacts\": {\n            \"Website\": {\"type\": \"website\", \"link\": \"https://korspaahalsen.rodekors.no/\"},\n            \"800 333 21\": {\"type\": \"phone\", \"link\": \"tel:800%20333%2021\"},\n            \"Facebook\": {\"type\": \"facebook\", \"link\": \"https://www.facebook.com/korspahalsen\"},\n            \"Instagram\": {\"type\": \"instragram\", \"link\": \"https://www.instagram.com/korspahalsen/\"},\n            \"Youtube\": {\"type\": \"youtube\", \"link\": \"https://www.youtube.com/channel/UCGl8oRzuIeixcF9IHJI0yKw\"},\n        },\n    },\n    \"Telefon Zaufania\": {\n        \"region\": \"Poland\",\n        \"page\": \"https://childhelplineinternational.org/poland-telefon-zaufania/\",\n        \"description\": \"Telefon Zaufania 116 111 is a helpline for children and adolescents provides support to children and adolescents up to 18 years of age. The phone is anonymous and free of charge, it works 7 days a week, 24 hours a day. Children can talk to us about everything: friendship, love, growing up, dealing with parents, siblings, problems at school or the emotions they experience. The child helpline is operated by Fundacja Dajemy Dzieciom Sile (Empowering Children Organisation).\",\n        \"contacts\": {\n            \"Website\": {\"type\": \"website\", \"link\": \"https://fdds.pl/\"},\n            \"116 111\": {\"type\": \"phone\", \"link\": \"tel:116111\"},\n            \"Facebook\": {\"type\": \"facebook\", \"link\": \"https://www.facebook.com/DajemyDzieciomSile/\"},\n            \"Instagram\": {\"type\": \"instragram\", \"link\": \"https://www.instagram.com/116111.pl/\"},\n            \"Twitter\": {\"type\": \"twitter\", \"link\": \"https://twitter.com/TelefonZaufania\"},\n            \"Youtube\": {\"type\": \"youtube\", \"link\": \"https://www.youtube.com/DajemyDzieciomSile\"},\n            \"LinkedIn\": {\n                \"type\": \"linkedin\",\n                \"link\": \"https://www.linkedin.com/in/telefon-zaufania-37b78654/?originalSubdomain=pl\",\n            },\n            \"Spotify\": {\"type\": \"spotify\", \"link\": \"https://open.spotify.com/show/6XfIIhmV9uEqCR7jBrgbhN\"},\n        },\n    },\n    \"SOS Crian\u00e7a\": {\n        \"region\": \"Portugal\",\n        \"page\": \"https://childhelplineinternational.org/portugal-sos-crianca/\",\n        \"description\": \"SOS Crian\u00e7a believes that children and youth have the right to speak and deserve the right to protection. The helpline therefore provides a support service for children to discuss their problems and ask for help. They can do this by phone, email, chat and WhatsApp.\",\n        \"contacts\": {\n            \"Website\": {\"type\": \"website\", \"link\": \"https://iacrianca.pt/intervencao/sos-crianca/\"},\n            \"116 111\": {\"type\": \"phone\", \"link\": \"tel:116111\"},\n            \"Email\": {\"type\": \"email\", \"link\": \"mailto:iac-soscrianca@iacrianca.pt\"},\n            \"Facebook\": {\"type\": \"facebook\", \"link\": \"https://www.facebook.com/SOS-Crian%C3%A7a-185466678171119/\"},\n            \"Instagram\": {\"type\": \"instragram\", \"link\": \"https://www.instagram.com/sos.criancacores/\"},\n        },\n    },\n    \"Telefonul Copilului 116111 Romania\": {\n        \"region\": \"Romania\",\n        \"page\": \"https://childhelplineinternational.org/romania-telefonul-copilului-116111-romania/\",\n        \"description\": \"Telefonul Copilului 116111 is the only non-governmental, non-profit organization in Romania that provides children and parents with a national helpline. The call is offered free of charge by Telekom Romania and social workers and psychologists respond to all calls 7 days a week.\",\n        \"contacts\": {\n            \"Website\": {\"type\": \"website\", \"link\": \"http://www.telefonulcopilului.ro\"},\n            \"116 111\": {\"type\": \"phone\", \"link\": \"tel:116111\"},\n            \"Email\": {\"type\": \"email\", \"link\": \"mailto:telefonulcopilului@telefonulcopilului.ro\"},\n            \"Facebook\": {\"type\": \"facebook\", \"link\": \"https://www.facebook.com/asociatiatelefonulcopilului/\"},\n            \"Instagram\": {\"type\": \"instragram\", \"link\": \"https://www.instagram.com/telefonulcopilului/\"},\n            \"Twitter\": {\"type\": \"twitter\", \"link\": \"https://twitter.com/Childline_RO\"},\n            \"LinkedIn\": {\n                \"type\": \"linkedin\",\n                \"link\": \"https://www.linkedin.com/company/asociatiatelefonulcopilului/?originalSubdomain=nl\",\n            },\n        },\n    },\n    \"NADEL \u2013 Nacionalna Decija Linija Srbije\": {\n        \"region\": \"Serbia\",\n        \"page\": \"https://childhelplineinternational.org/serbia-nadel-nacionalna-decija-linija-srbije/\",\n        \"description\": \"NADEL, or the National Children\u2019s Line, is a telephone counseling service that enables children in Serbia to receive counseling support at any time by calling the toll-free number 116111, if they find themselves in circumstances where their rights are threatened or when they feel they need to talk to a competent and confidential person.\",\n        \"contacts\": {\n            \"Website\": {\"type\": \"website\", \"link\": \"http://nadel-decijalinija.org/\"},\n            \"116 111\": {\"type\": \"phone\", \"link\": \"tel:116111\"},\n            \"Email\": {\"type\": \"email\", \"link\": \"mailto:nadel.srbija116111@gmail.com\"},\n            \"Facebook\": {\"type\": \"facebook\", \"link\": \"https://www.facebook.com/nacionalnadecijalinija/\"},\n            \"Instagram\": {\"type\": \"instragram\", \"link\": \"https://www.instagram.com/116111.rs/\"},\n        },\n    },\n    \"Linka Detskej Istoty\": {\n        \"region\": \"Slovakia\",\n        \"page\": \"https://childhelplineinternational.org/slovakia-linka-detskej-istoty/\",\n        \"description\": \"Linka Detskej Istoty (Child Safety Line) is available 24/7, FREE, ANONYMOUSLY for the whole territory of Slovakia. \",\n        \"contacts\": {\n            \"Website\": {\n                \"type\": \"website\",\n                \"link\": \"http://www.ldi.sk/?fbclid=IwAR3WGHRKtve1hY0Ymnb7myRcIyNTOZUTf6KDMBBmnWzHbWhiHkfbWY24TMY\",\n            },\n            \"116111\": {\"type\": \"phone\", \"link\": \"tel:116111\"},\n            \"0800 500 500\": {\"type\": \"phone\", \"link\": \"tel:0800112112\"},\n            \"Email Admin\": {\"type\": \"email\", \"link\": \"http://info@ldi.sk\"},\n            \"Email (Slovak)\": {\"type\": \"email\", \"link\": \"http://116111@ldi.sk\"},\n            \"Email (Ukrainian & Russian)\": {\"type\": \"email\", \"link\": \"http://0800500500@ldi.sk\"},\n            \"Facebook\": {\"type\": \"facebook\", \"link\": \"https://www.facebook.com/LinkaDetskejIstoty/\"},\n            \"LinkedIn\": {\n                \"type\": \"linkedin\",\n                \"link\": \"https://www.linkedin.com/company/linka-detskej-istoty/?originalSubdomain=nl\",\n            },\n            \"Instagram\": {\"type\": \"instragram\", \"link\": \"http://www.instagram.com/linkadetskejistoty\"},\n        },\n    },\n    \"National Telephone Helpline \u2013 TOM\": {\n        \"region\": \"Slovenia\",\n        \"page\": \"https://childhelplineinternational.org/slovenia-national-telephone-helpline-tom/\",\n        \"description\": \"TOM is a telephone for children and adolescents, operating within the Association of Friends of the Youth of Slovenia (ZPMS), and was created as an emotional support for children and young people who face various issues, dilemmas or hardships in the process of growing up. On the toll-free number 116 111 , they can confide their problems to consultants or ask them for advice and additional information. Anonymity and confidentiality of the conversation and callers are guaranteed\",\n        \"contacts\": {\n            \"Website\": {\"type\": \"website\", \"link\": \"http://www.e-tom.si/\"},\n            \"116 111\": {\"type\": \"phone\", \"link\": \"tel:116111\"},\n            \"Email\": {\"type\": \"email\", \"link\": \"mailto:info@zpms.si\"},\n            \"Facebook\": {\"type\": \"facebook\", \"link\": \"https://www.facebook.com/ZPMSlovenije\"},\n            \"Instagram\": {\"type\": \"instragram\", \"link\": \"https://www.instagram.com/zvezaprijateljev/\"},\n        },\n    },\n    \"Tel\u00e9fono ANAR de Ayuda a Nin\u00f2s y Adolescentes\": {\n        \"region\": \"Spain\",\n        \"page\": \"https://childhelplineinternational.org/spain-telefono-anar-de-ayuda-a-ninos-y-adolescentes/\",\n        \"description\": \"ANAR Telephone Helpline for Children and Adolescent provides an immediate response to any problem that may affect a minor: relationship difficulties, violence in its different forms, psychological problems, among others. Any child or young person can dial our number, which is free and confidential, and they will find a psychologist on the other end who will listen to them for as long as they need, who will guide them through their problem and help them find a solution.\",\n        \"contacts\": {\n            \"Website\": {\"type\": \"website\", \"link\": \"https://www.anar.org/necesitas-ayuda-telefono-ninos-adolescentes/\"},\n            \"116 111\": {\"type\": \"phone\", \"link\": \"tel:116111\"},\n            \"900 20 20 10\": {\"type\": \"phone\", \"link\": \"tel:%20900202010\"},\n            \"Email\": {\"type\": \"email\", \"link\": \"mailto:comunicacion@anar.org\"},\n            \"Facebook\": {\"type\": \"facebook\", \"link\": \"https://www.facebook.com/FundacionANAR/\"},\n            \"Instagram\": {\"type\": \"instragram\", \"link\": \"https://www.instagram.com/fundacionanar/\"},\n            \"Twitter\": {\"type\": \"twitter\", \"link\": \"https://twitter.com/FundacionANAR\"},\n            \"Youtube\": {\"type\": \"youtube\", \"link\": \"https://www.youtube.com/channel/UCOQb3ZDHjHMrvUBXBF6asfw\"},\n            \"LinkedIn\": {\n                \"type\": \"linkedin\",\n                \"link\": \"https://www.linkedin.com/company/fundaci-n-anar/?viewAsMember=true\",\n            },\n            \"TikTok\": {\"type\": \"tiktok\", \"link\": \"https://www.tiktok.com/@fundacionanar?\"},\n        },\n    },\n    \"BRIS\": {\n        \"region\": \"Sweden\",\n        \"page\": \"https://childhelplineinternational.org/sweden-bris/\",\n        \"description\": \"Bris, Children\u2019s rights in society, is one of Sweden\u2019s leading children\u2019s rights organizations that fights every day for a better society for children.\",\n        \"contacts\": {\n            \"Website\": {\"type\": \"website\", \"link\": \"https://www.bris.se/\"},\n            \"116 111\": {\"type\": \"phone\", \"link\": \"tel:116111\"},\n            \"Email\": {\"type\": \"email\", \"link\": \"mailto:info@bris.se\"},\n            \"Facebook\": {\"type\": \"facebook\", \"link\": \"https://www.facebook.com/BRIS/\"},\n            \"Instagram\": {\"type\": \"instragram\", \"link\": \"https://www.instagram.com/bris116111/\"},\n            \"Twitter\": {\"type\": \"twitter\", \"link\": \"https://twitter.com/brispress\"},\n        },\n    },\n    \"Pro Juventute Beratung + Hilfe 147 - Switzerland\": {\n        \"region\": \"Switzerland\",\n        \"page\": \"https://childhelplineinternational.org/switzerland-pro-juventute-beratung-hilfe-147/\",\n        \"description\": \"Pro Juventute\u2019s child helpline provides advice for children and young people, confidential, free of charge and around the clock for children and young people if they have worries or questions.\",\n        \"contacts\": {\n            \"Website\": {\"type\": \"website\", \"link\": \"http://www.147.ch\"},\n            \"147\": {\"type\": \"phone\", \"link\": \"tel:147\"},\n            \"Email\": {\"type\": \"email\", \"link\": \"mailto:socialmedia@projuventute.ch\"},\n            \"Facebook\": {\"type\": \"facebook\", \"link\": \"https://www.facebook.com/ProJuventute.ch/\"},\n            \"Instagram\": {\"type\": \"instragram\", \"link\": \"https://www.instagram.com/147_schweiz/\"},\n            \"Twitter\": {\"type\": \"twitter\", \"link\": \"https://twitter.com/projuventute\"},\n        },\n    },\n    \"Genclik Destek Hatti\": {\n        \"region\": \"T\u00fcrkije\",\n        \"page\": \"https://childhelplineinternational.org/turkey-genclik-destek-hatti/\",\n        \"description\": \"Genclik Destek Hatti (Youth Support Line) provides children and young people information about legal support, education, employment and scholarship, guidance for access to public services, referral to health-related centers, provides individual counseling on issues such as psychological support, information about the probation process, future anxiety, and family problems.\",\n        \"contacts\": {\n            \"Website\": {\"type\": \"website\", \"link\": \"\"},\n            \"0850 455 0070\": {\"type\": \"phone\", \"link\": \"tel:0850%20455%200070\"},\n            \"Email\": {\"type\": \"email\", \"link\": \"mailto:iletisim@genclikdestekhatti.org.tr\"},\n            \"Facebook\": {\n                \"type\": \"facebook\",\n                \"link\": \"https://www.facebook.com/Gen%C3%A7lik-Destek-Hatt%C4%B1-271022699618652/\",\n            },\n        },\n    },\n    \"Ukraine National Child Toll-Free Hotline\": {\n        \"region\": \"Ukraine\",\n        \"page\": \"https://childhelplineinternational.org/ukraine-ukraine-national-child-toll-free-hotline/\",\n        \"contacts\": {\n            \"Website\": {\"type\": \"website\", \"link\": \"http://www.la-strada.org.ua/\"},\n            \"116111 (not accessible using Ukrainian SIM card outside Ukraine)\": {\"type\": \"phone\", \"link\": \"tel:116111\"},\n            \"0 800 500 225\": {\"type\": \"phone\", \"link\": \"tel:0800500225\"},\n            \"Email\": {\"type\": \"email\", \"link\": \"mailto:hotline@la-strada.org.ua\"},\n            \"Facebook\": {\"type\": \"facebook\", \"link\": \"https://www.facebook.com/childhotline.ukraine\"},\n            \"Instagram\": {\"type\": \"instragram\", \"link\": \"https://www.instagram.com/childhotline_ua/\"},\n            \"Skype\": {\"type\": \"skype\", \"link\": \"http://@lastrada-ukraine\"},\n            \"Youtube\": {\"type\": \"youtube\", \"link\": \"https://www.youtube.com/channel/UC6wbQ9UyDJXSJobRzkZsc0g\"},\n            \"Telegram\": {\"type\": \"telegram\", \"link\": \"https://t.me/CHL116111\"},\n        },\n    },\n    \"BEAT\": {\n        \"region\": \"United Kingdom\",\n        \"page\": \"https://childhelplineinternational.org/united-kingdom-beat/\",\n        \"contacts\": {\n            \"Website\": {\"type\": \"website\", \"link\": \"https://www.beateatingdisorders.org.uk/\"},\n            \"Facebook\": {\"type\": \"facebook\", \"link\": \"https://www.facebook.com/beat.eating.disorders\"},\n            \"Instagram\": {\"type\": \"instragram\", \"link\": \"https://www.instagram.com/beatedsupport\"},\n            \"Twitter\": {\"type\": \"twitter\", \"link\": \"https://twitter.com/beated\"},\n            \"Youtube\": {\"type\": \"youtube\", \"link\": \"https://www.youtube.com/user/beatingED\"},\n            \"LinkedIn\": {\n                \"type\": \"linkedin\",\n                \"link\": \"https://www.linkedin.com/company/beat---beating-eating-disorders/\",\n            },\n            \"0808 801 0677\": {\"type\": \"phone\", \"link\": \"tel:0808%20801%200677\"},\n            \"Email 1\": {\"type\": \"email\", \"link\": \"mailto:help@beateatingdisorders.org.uk\"},\n            \"0808 801 0432\": {\"type\": \"phone\", \"link\": \"tel:0808%20801%200432\"},\n            \"Email\": {\"type\": \"email\", \"link\": \"mailto:Scotlandhelp@beateatingdisorders.org.uk\"},\n            \"0808 801 0433\": {\"type\": \"phone\", \"link\": \"tel:0808%20801%200433\"},\n            \"Email 2\": {\"type\": \"email\", \"link\": \"mailto:Waleshelp@beateatingdisorders.org.uk\"},\n            \"Request support in Welsh\": {\n                \"type\": \"comment\",\n                \"link\": \"https://training.beateatingdisorders.org.uk/page/76838/data/1\",\n            },\n            \"0808 801 0434\": {\"type\": \"phone\", \"link\": \"tel:0808%20801%200434\"},\n            \"Email 3\": {\"type\": \"email\", \"link\": \"mailto:NIhelp@beateatingdisorders.org.uk\"},\n        },\n    },\n    \"Childline UK\": {\n        \"region\": \"United Kingdom\",\n        \"page\": \"https://childhelplineinternational.org/united-kingdom-childline-uk/\",\n        \"description\": \" \",\n        \"contacts\": {\n            \"Website\": {\"type\": \"website\", \"link\": \"https://www.childline.org.uk/about/about-childline/\"},\n            \"116 111\": {\"type\": \"phone\", \"link\": \"tel:116111\"},\n            \"0800 1111\": {\"type\": \"phone\", \"link\": \"tel:0800%201111\"},\n            \"Email\": {\"type\": \"email\", \"link\": \"https://www.childline.org.uk/login/?returnPath=%2flocker%2femail%2f\"},\n            \"Facebook\": {\"type\": \"facebook\", \"link\": \"https://www.facebook.com/nspcc\"},\n            \"Instagram\": {\"type\": \"instragram\", \"link\": \"https://www.instagram.com/childline_official/\"},\n            \"Twitter\": {\"type\": \"twitter\", \"link\": \"https://twitter.com/NSPCC\"},\n        },\n    },\n    \"HopelineUK\": {\n        \"region\": \"United Kingdom\",\n        \"page\": \"https://childhelplineinternational.org/united-kingdom-hopelineuk/\",\n        \"description\": \"A service of Papyrus Prevention of Young Suicide, HopelineUK\u2019s advisers want to work with you to understand why thoughts of suicide might be present. They also want to provide you with a safe space to talk through anything happening in your life that could be impacting on your or anyone else\u2019s ability to stay safe.\",\n        \"contacts\": {\n            \"Website\": {\"type\": \"website\", \"link\": \"https://www.papyrus-uk.org/hopelineuk/\"},\n            \"0800 068 4141\": {\"type\": \"phone\", \"link\": \"tel:08000684141\"},\n            \"Text\": {\"type\": \"comment\", \"link\": \"tel:08000684141\"},\n            \"Email\": {\"type\": \"email\", \"link\": \"mailto:pat@papyrus-uk.org\"},\n            \"Facebook\": {\"type\": \"facebook\", \"link\": \"https://www.facebook.com/PAPYRUSUK\"},\n            \"Instagram\": {\"type\": \"instragram\", \"link\": \"https://www.instagram.com/papyrus_uk/\"},\n            \"Twitter\": {\"type\": \"twitter\", \"link\": \"https://twitter.com/PAPYRUS_tweets\"},\n            \"Youtube\": {\"type\": \"youtube\", \"link\": \"https://www.youtube.com/channel/UC_y_k9yZih75co_PCLOoyUA\"},\n            \"LinkedIn\": {\n                \"type\": \"linkedin\",\n                \"link\": \"https://www.linkedin.com/company/papyrus-prevention-of-young-suicide/\",\n            },\n            \"TikTok\": {\n                \"type\": \"tiktok\",\n                \"link\": \"https://www.tiktok.com/@papyrus_uk?_d=secCgwIARCbDRjEFSACKAESPgo864ccELEgLnLpUS3iU1WXrlF0mOyBOhsE1rOCATc51bjAJPeGnHklJPAYhasIzyRwWR10qiJZ0bFX9Z%2FoGgA%3D&language=en&sec_uid=MS4wLjABAAAAQHWcWst9WayCCGURshD_0-v25e2uPEmFjtgJ1rUD1e_nlrXRghjnwM0-vw72Za7a&sec_user_id=MS4wLjABAAAAQHWcWst9WayCCGURshD_0-v25e2uPEmFjtgJ1rUD1e_nlrXRghjnwM0-vw72Za7a&share_app_id=1233&share_author_id=6805954090699441157&share_link_id=7e52fd33-b382-40dc-8a45-a099177e2ea6&timestamp=1645528172&u_code=dbd0ik0jlh3i9l&user_id=6805954090699441157&utm_campaign=client_share&utm_medium=android&utm_source=copy&source=h5_m&_r=1\",\n            },\n        },\n    },\n    \"The Mix\": {\n        \"region\": \"United Kingdom\",\n        \"page\": \"https://childhelplineinternational.org/united-kingdom-the-mix/\",\n        \"description\": \"The Mix is a multi-channel advice service designed to support the physical and mental well-being of under 25s across the UK. Whatever issue a young person is facing, The Mix is always there for them, providing help, information and support 24/7.\",\n        \"contacts\": {\n            \"Website\": {\"type\": \"website\", \"link\": \"http://www.themix.org.uk/\"},\n            \"0808 808 4994\": {\"type\": \"phone\", \"link\": \"tel:0808%20808%204994\"},\n            \"Facebook\": {\"type\": \"facebook\", \"link\": \"https://www.facebook.com/TheMixUK\"},\n            \"Instagram\": {\"type\": \"instragram\", \"link\": \"https://www.instagram.com/themixuk/\"},\n            \"Twitter\": {\"type\": \"twitter\", \"link\": \"https://twitter.com/themixUK\"},\n            \"Youtube\": {\"type\": \"youtube\", \"link\": \"https://www.youtube.com/themixuk\"},\n            \"LinkedIn\": {\"type\": \"linkedin\", \"link\": \"https://www.linkedin.com/company/the-mix-charity/\"},\n        },\n    },\n    \"Je t\u2019\u00e9coute 3033\": {\n        \"region\": \"Algeria\",\n        \"page\": \"https://childhelplineinternational.org/algeria-je-tecoute-3033/\",\n        \"contacts\": {\n            \"Website\": {\"type\": \"website\", \"link\": \"http://www.nada-dz.org\"},\n            \"3033\": {\"type\": \"phone\", \"link\": \"tel:3033\"},\n            \"Email 998\": {\"type\": \"email\", \"link\": \"mailto:998@mlsd.gov.bh\"},\n            \"Facebook\": {\"type\": \"facebook\", \"link\": \"https://www.facebook.com/reseau.nada.dz/\"},\n            \"Instagram\": {\"type\": \"instragram\", \"link\": \"http://www.instagram.com/reseau.nada\"},\n            \"Twitter\": {\"type\": \"twitter\", \"link\": \"https://twitter.com/ReseauNAda\"},\n            \"LinkedIn\": {\n                \"type\": \"linkedin\",\n                \"link\": \"http://www.linkedin.com/company/nada-r\u00e9seau-alg\u00e9rien-pour-la-d\u00e9fense-des-droits-de-l\u2019enfant\",\n            },\n        },\n    },\n    \"Child Helpline 998\": {\n        \"region\": \"Bahrain\",\n        \"page\": \"https://childhelplineinternational.org/bahrain-child-helpline-998/\",\n        \"description\": \"Run by the Ministry of Labour & Social Development in Bahrain, Child Helpline 998 is a toll-free phone line and provides a place to report children\u2019s exposure to violence or abuse. The line receives calls from the child or others who are exposed to violence, abuse or danger. It also provides counselling, active listening and referral to the relevant authorities concerned when needed\",\n        \"contacts\": {\n            \"Website\": {\"type\": \"website\", \"link\": \"http://www.mlsd.gov.bh/en/childhood/childhood_care/998\"},\n            \"80008001\": {\"type\": \"phone\", \"link\": \"tel:+97380008001\"},\n            \"WhatsApp\": {\"type\": \"whatsapp\", \"link\": \"tel:01102121600\"},\n            \"Email\": {\"type\": \"email\", \"link\": \"mailto:info@mlsd.gov.bh\"},\n            \"Facebook\": {\"type\": \"facebook\", \"link\": \"https://www.facebook.com/mosdsocial\"},\n            \"Instagram\": {\"type\": \"instragram\", \"link\": \"https://www.instagram.com/mlsdbahrain\"},\n            \"Twitter\": {\"type\": \"twitter\", \"link\": \"https://twitter.com/mlsdbahrain\"},\n        },\n    },\n    \"Child Helpline Egypt\": {\n        \"region\": \"Egypt\",\n        \"page\": \"https://childhelplineinternational.org/egypt-child-helpline-egypt/\",\n        \"description\": \"Child Helpline 16000 is provided by the National Council for Childhood and Motherhood, which operates 24 hours a day to receive calls about children being at risk. \",\n        \"contacts\": {\n            \"Website\": {\n                \"type\": \"website\",\n                \"link\": \"http://nccm.gov.eg/%d8%ae%d8%b7-%d9%86%d8%ac%d8%af%d8%a9-%d8%a7%d9%84%d8%b7%d9%81%d9%84/\",\n            },\n            \"WhatsApp\": {\"type\": \"whatsapp\", \"link\": \"https://api.whatsapp.com/send?phone=+2001016609579&text=\"},\n            \"16000\": {\"type\": \"phone\", \"link\": \"tel:16000\"},\n            \"88531109 (with the city code 021).\": {\"type\": \"phone\", \"link\": \"tel:02188531109\"},\n            \"Facebook\": {\"type\": \"facebook\", \"link\": \"https://www.facebook.com/Child.Help.line.Egypt\"},\n            \"Twitter\": {\"type\": \"twitter\", \"link\": \"https://twitter.com/NCCMEgypt\"},\n            \"Youtube\": {\n                \"type\": \"youtube\",\n                \"link\": \"https://www.youtube.com/channel/UC0Q-bhJZ4Dr0SfmEx_HOXcQ?view_as=subscriber\",\n            },\n        },\n    },\n    \"Sedaye Yara\": {\n        \"region\": \"Iran\",\n        \"page\": \"https://childhelplineinternational.org/iran-sedaye-yara/\",\n        \"description\": \" Run by Iranian Society for Protecting the Rights of the Child (IRSPRC), Sedaye Yara is a child helpline that is based on the belief that every child and adolescent, regardless of social class, gender, race, talent, or any other form of discrimination, should be able to receive appropriate assistance in times of need. The helpline provides support to children, adolescents, parents and schools and supports them with resolving any challenges and issues they may have.\",\n        \"contacts\": {\n            \"Website\": {\"type\": \"website\", \"link\": \"http://www.irsprc.org\"},\n            \"+98 21 42152\": {\"type\": \"phone\", \"link\": \"tel:+982142152\"},\n            \"Email\": {\"type\": \"email\", \"link\": \"mailto:info@irsprc.org\"},\n            \"Facebook\": {\"type\": \"facebook\", \"link\": \"https://www.facebook.com/irsprc\"},\n            \"Instagram\": {\"type\": \"instragram\", \"link\": \"https://www.instagram.com/irsprcorg/\"},\n            \"Twitter\": {\"type\": \"twitter\", \"link\": \"https://twitter.com/irsprc2\"},\n            \"Telegram\": {\"type\": \"telegram\", \"link\": \"https://t.me/irsprcorg\"},\n            \"Soundcloud\": {\"type\": \"soundcloud\", \"link\": \"https://soundcloud.com/mojekoodaki\"},\n        },\n    },\n    \"116 Child Helpline \u2013 Kurdistan Region\": {\n        \"region\": \"Iraq\",\n        \"page\": \"https://childhelplineinternational.org/iraq-116-child-helpline-kurdistan-region/\",\n        \"description\": \"116 Child Helpline is a free child helpline for children and young people in Iraq.\",\n        \"contacts\": {\n            \"Website\": {\"type\": \"website\", \"link\": \"http://www.molsa.gov.iq/\"},\n            \"116\": {\"type\": \"phone\", \"link\": \"tel:116\"},\n            \"Email\": {\"type\": \"email\", \"link\": \"mailto:childhelp116erbil@gmail.com\"},\n            \"Facebook\": {\n                \"type\": \"facebook\",\n                \"link\": \"https://www.facebook.com/%D9%87%D9%8A%D9%84%D9%89-%D9%81%D8%B1%DB%8C%D8%A7%DA%A9%DB%95%D9%88%D8%AA%D9%86%DB%8C-%D9%85%D9%86%D8%AF%D8%A7%DA%B5-%D9%A1%D9%A1%D9%A6-%D9%87%DB%95%D9%88%D9%84%DB%8E%D8%B1%D8%AE%D8%B7-%D9%86%D8%AC%D8%AF%D8%A9-%D8%A7%D9%84%D8%B7%D9%81%D9%84-116%D8%A7%D8%B1%D8%A8%D9%8A%D9%84-191573691286520/?ref=page_internal\",\n            },\n            \"Twitter\": {\"type\": \"twitter\", \"link\": \"https://twitter.com/site3075\"},\n        },\n    },\n    \"JRF 110 Helpline\": {\n        \"region\": \"Jordan\",\n        \"page\": \"https://childhelplineinternational.org/jordan-jrf-110-helpline/\",\n        \"description\": \"Jordan River Foundation\u2019s 110 Helpline is a free service offering support in child and family safety and psychological well-being in complete confidentiality, offering beneficiaries with resources, referrals and guidance that do not obstruct, label or judge them.\",\n        \"contacts\": {\n            \"Website\": {\"type\": \"website\", \"link\": \"http://www.jordanriver.jo/\"},\n            \"110\": {\"type\": \"phone\", \"link\": \"tel:110\"},\n            \"Email\": {\"type\": \"email\", \"link\": \"mailto:info@jrf.org.jo\"},\n            \"Facebook\": {\"type\": \"facebook\", \"link\": \"https://www.facebook.com/JordanRiverFoundation\"},\n            \"Instagram\": {\"type\": \"instragram\", \"link\": \"https://www.instagram.com/jordanriverdesigns/\"},\n            \"Twitter\": {\"type\": \"twitter\", \"link\": \"https://twitter.com/JordanRiverFDN\"},\n            \"Youtube\": {\"type\": \"youtube\", \"link\": \"https://www.youtube.com/user/JordanRiverFDN\"},\n        },\n    },\n    \"Help Hotline 147\": {\n        \"region\": \"Kuwait\",\n        \"page\": \"https://childhelplineinternational.org/kuwait-help-hotline-147/\",\n        \"description\": \"The Child Helpline 147, run by the Kuwait National Child Protection Program, aims to provide children with protection and give them the opportunity to express their concerns and fears. The helpline works to listen consciously to children or parents, providing them with advice and directing them to appropriate action.\",\n        \"contacts\": {\n            \"Website\": {\"type\": \"website\", \"link\": \"http://kncpp.com/\"},\n            \"147\": {\"type\": \"phone\", \"link\": \"tel:147\"},\n            \"Email\": {\"type\": \"email\", \"link\": \"mailto:kuwaitcpp@gmail.com\"},\n            \"Facebook\": {\"type\": \"facebook\", \"link\": \"https://www.facebook.com/kuwaitncpp/\"},\n            \"Instagram\": {\"type\": \"instragram\", \"link\": \"https://www.instagram.com/kncpp/\"},\n            \"Twitter\": {\"type\": \"twitter\", \"link\": \"https://twitter.com/kncpp\"},\n        },\n    },\n    \"Higher Council for Childhood\": {\n        \"region\": \"Lebanon\",\n        \"page\": \"https://childhelplineinternational.org/lebanon-higher-council-for-childhood/\",\n        \"description\": \"The Higher Council for Childhood is the national framework for complementary work between non governmental organizations and the public sector as to child care and development in compliance with international conventions and especially,  the Convention on the Rights of the Child and in collaboration with international organizations.\",\n        \"contacts\": {\n            \"Website\": {\"type\": \"website\", \"link\": \"http://www.atfalouna.gov.lb/\"},\n            \"Email\": {\"type\": \"email\", \"link\": \"mailto:hcclebanon@gmail.com\"},\n        },\n    },\n    \"Naba\u2019a\": {\n        \"region\": \"Lebanon\",\n        \"page\": \"https://childhelplineinternational.org/lebanon-nabaa/\",\n        \"description\": \"Naba\u2019a is a Lebanese non-political, not-for-profit organisation that aims to create an environment where children and young people can thrive and live in harmony regardless of their religion, sex and nationality. Naba\u2019a works to empower local communities and to enable their members to uphold their rights and to build a better future for themselves.\",\n        \"contacts\": {\n            \"Website\": {\"type\": \"website\", \"link\": \"http://www.nabaa-lb.org\"},\n            \"Email\": {\"type\": \"email\", \"link\": \"mailto:%20info@nabaa-lb.org\"},\n            \"Facebook\": {\"type\": \"facebook\", \"link\": \"https://www.facebook.com/Nabaa.Lebanon/?fref=ts\"},\n            \"Instagram\": {\"type\": \"instragram\", \"link\": \"https://www.instagram.com/nabaalebanon/\"},\n            \"Twitter\": {\"type\": \"twitter\", \"link\": \"https://twitter.com/NABAA2001\"},\n            \"Youtube\": {\"type\": \"youtube\", \"link\": \"https://www.youtube.com/channel/UCDUagSSFTSEtpckzXx5MZZA\"},\n        },\n    },\n    \"Sawa 121\": {\n        \"region\": \"Palestine\",\n        \"page\": \"https://childhelplineinternational.org/palestine-sawa-121/\",\n        \"description\": \"Sawa Foundation works to fight violence against women and children and offers free psychological support to victims of violence over the phone. They provide guidance, primary, medical and legal psychological support service all week over the phone and free of charge\",\n        \"contacts\": {\n            \"121\": {\"type\": \"phone\", \"link\": \"tel:121\"},\n            \"WhatsApp\": {\"type\": \"whatsapp\", \"link\": \"http://972594040121+\"},\n            \"Email\": {\"type\": \"email\", \"link\": \"mailto:info@sawa.ps\"},\n            \"Facebook\": {\"type\": \"facebook\", \"link\": \"https://www.facebook.com/Sawa.Organization/\"},\n            \"Instagram\": {\n                \"type\": \"instragram\",\n                \"link\": \"https://www.instagram.com/sawa.organization/?fbclid=IwAR2S6X51a8LYVJZBWybN_ke7uiBl3xSeVMJjcyib5YePDlB9GuQ4F4PHcto\",\n            },\n            \"Youtube\": {\"type\": \"youtube\", \"link\": \"https://www.youtube.com/results?search_query=sawa+organization\"},\n        },\n    },\n    \"Hotline 919\": {\n        \"region\": \"Qatar\",\n        \"page\": \"https://childhelplineinternational.org/qatar-hotline-919/\",\n        \"description\": \"Hotline 919 provides provides free confidential consultations (social, psychological and legal) for women and children and also provides support to protect and rehabilitate children and women who are victims of violence and family breakdown.\",\n        \"contacts\": {\n            \"Website\": {\"type\": \"website\", \"link\": \"http://www.aman.org.qa/\"},\n            \"919\": {\"type\": \"phone\", \"link\": \"tel:919\"},\n            \"Email\": {\"type\": \"email\", \"link\": \"mailto:info@aman.org.qa\"},\n            \"Facebook\": {\"type\": \"facebook\", \"link\": \"https://www.facebook.com/Amancentre/\"},\n            \"Instagram\": {\"type\": \"instragram\", \"link\": \"https://www.instagram.com/amancentre/\"},\n            \"Twitter\": {\"type\": \"twitter\", \"link\": \"https://twitter.com/amancentre\"},\n            \"Youtube\": {\"type\": \"youtube\", \"link\": \"https://www.youtube.com/channel/UC2jVVqG4MU2UyLDgYZ5TVQg\"},\n        },\n    },\n    \"Saudi Child Helpline 116111\": {\n        \"region\": \"Saudi Arabia\",\n        \"page\": \"https://childhelplineinternational.org/saudi-arabia-saudi-child-helpline-116111/\",\n        \"description\": \"Operating under the Saudi Arabia National Family and Safety Programme, Child Support Line 116111 provides free consultation services and support for children undergoing abuse and neglect.\",\n        \"contacts\": {\n            \"Website\": {\"type\": \"website\", \"link\": \"http://www.nfsp.org.sa\"},\n            \"9661 252 0088\": {\"type\": \"phone\", \"link\": \"tel:9661%20252%200088\"},\n            \"Email\": {\"type\": \"email\", \"link\": \"https://orgchil-jicorica.savviihq.com/?p=2511\"},\n            \"Facebook\": {\n                \"type\": \"facebook\",\n                \"link\": \"https://www.facebook.com/nfspa/?show_switched_toast=0&show_invite_to_follow=0&show_switched_tooltip=0&show_podcast_settings=0&show_community_transition=0&show_community_review_changes=0\",\n            },\n            \"Instagram\": {\"type\": \"instragram\", \"link\": \"https://www.instagram.com/nfsp1/\"},\n            \"Twitter\": {\"type\": \"twitter\", \"link\": \"https://twitter.com/nfsp1\"},\n            \"Youtube\": {\"type\": \"youtube\", \"link\": \"https://www.youtube.com/user/NFSP2005\"},\n        },\n    },\n    \"Child Helpline 9696\": {\n        \"region\": \"Sudan\",\n        \"page\": \"https://childhelplineinternational.org/sudan-child-helpline-9696/\",\n        \"description\": \"Operating under the Child and Family Protection Unit, 9696 is the free hotline for the children\u2019s support in a communication center that works confidentially to receive complaints, inquires from the children themselves, their parents or caregivers.\",\n        \"contacts\": {\n            \"Website\": {\"type\": \"website\", \"link\": \"http://www.fcpu.gov.sd/\"},\n            \"9696\": {\"type\": \"phone\", \"link\": \"tel:9696\"},\n            \"Facebook\": {\"type\": \"facebook\", \"link\": \"https://www.facebook.com/FCPA.SD/?fref=ts\"},\n        },\n    },\n    \"Child Helpline 800 700\": {\n        \"region\": \"United Arab Emirates\",\n        \"page\": \"https://childhelplineinternational.org/united-arab-emirates-child-helpline-800-700/\",\n        \"description\": \"Operated under the Department of Social Services, Child Helpline 800 700 was established in 2007 to receive reports of children at risk and abuse of all kinds, such as physical, sexual and emotional abuse, neglect and commercial exploitation, and deal with them through a specialized team that works to support the child.\",\n        \"contacts\": {\n            \"Website\": {\"type\": \"website\", \"link\": \"https://sssd.shj.ae/\"},\n            \"800 700\": {\"type\": \"phone\", \"link\": \"tel:800700\"},\n            \"Email\": {\"type\": \"email\", \"link\": \"mailto:sssd@sssd.shj.ae\"},\n            \"Facebook\": {\"type\": \"facebook\", \"link\": \"https://www.facebook.com/sssd.gov\"},\n            \"Instagram\": {\"type\": \"instragram\", \"link\": \"https://www.instagram.com/sssdshj/\"},\n            \"Twitter\": {\"type\": \"twitter\", \"link\": \"https://twitter.com/sssdshj\"},\n            \"Youtube\": {\"type\": \"youtube\", \"link\": \"https://www.youtube.com/user/SharjahSocialService\"},\n            \"Flickr\": {\"type\": \"flickr\", \"link\": \"https://www.flickr.com/photos/132221693@N02/\"},\n        },\n    },\n    \"DFWAC Helpline\": {\n        \"region\": \"United Arab Emirates\",\n        \"page\": \"https://childhelplineinternational.org/united-arab-emirates-dfwac-helpline/\",\n        \"description\": \"The Dubai Foundation for Women and Children (DFWAC) is the first licensed non-profit shelter in the UAE for women and child victims of domestic violence, child abuse and human trafficking. It offers victims immediate protection and support services, in accordance with international human rights obligations. DFWAC provides a 24/7 free-of-charge child helpline service (800111) for all residents of Dubai, which aims to provide an efficient response for clients who ask for more information about the foundation\u2019s services, call for urgent help, report abuse or seek consultation.\",\n        \"contacts\": {\n            \"Website\": {\"type\": \"website\", \"link\": \"http://www.dfwac.ae/\"},\n            \"800111\": {\"type\": \"phone\", \"link\": \"tel:800111\"},\n            \"WhatsApp\": {\"type\": \"whatsapp\", \"link\": \"https://api.whatsapp.com/send?phone=971800111\"},\n            \"Email\": {\"type\": \"email\", \"link\": \"mailto:help@dfwac.ae\"},\n            \"Facebook\": {\"type\": \"facebook\", \"link\": \"https://www.facebook.com/DFWAC\"},\n            \"Instagram\": {\"type\": \"instragram\", \"link\": \"https://www.instagram.com/dfwac/\"},\n            \"Twitter\": {\"type\": \"twitter\", \"link\": \"https://twitter.com/DFWAC\"},\n            \"Youtube\": {\"type\": \"youtube\", \"link\": \"https://www.youtube.com/channel/UCa-v9enrOrK0_7K4g4-h2Jw\"},\n            \"LinkedIn\": {\n                \"type\": \"linkedin\",\n                \"link\": \"https://www.linkedin.com/company/dubai-foundation-for-women-and-children/\",\n            },\n            \"Telegram\": {\"type\": \"telegram\", \"link\": \"https://t.me/DFWAC_community_awarness\"},\n        },\n    },\n}\n", "data/datasets/safety_directory/emergency_numbers/emergency_numbers.py": "# @title Emergency Numbers\n# Source : https://github.com/LAION-AI/Open-Instruction-Generalist/blob/main/OIG/src/emergency_numbers.py\n\"\"\"\nFrom:  https://travel.state.gov/content/dam/students-abroad/pdfs/911_ABROAD.pdf\nNOTE:\nEnglish\u2010speaking operators may not be available.\nThis list was created in early 2023 from the above website and may not be current.\n\"\"\"\n\nemergency_numbers = [\n    [\"Country\", \"Ambulance\", \"Fire\", \"Police\"],\n    [\"Albania\", \"17\", \"18\", \"19\"],\n    [\"Algeria\", \"21606666\", \"14\", \"17\"],\n    [\"American_Samoa\", \"911\", \"\", \"\"],\n    [\"Andorra\", \"118\", \"118\", \"110\"],\n    [\"Angola\", \"118\", \"118\", \"110\"],\n    [\"Antigua\", \"&\", \"Barbuda\", \"999,911\"],\n    [\"Argentina\", \"101\", \"107\", \"101\"],\n    [\"Armenia\", \"103\", \"\", \"\"],\n    [\"Aruba\", \"911\", \"\", \"\"],\n    [\"Ascension_Island\", \"6000\", \"911\", \"6666\"],\n    [\"Australia\", \"000_(112_on_cell_phone)\", \"\", \"\"],\n    [\"Austria\", \"112,122\", \"\", \"\"],\n    [\"Azerbaijan_(Baku)\", \"03\", \"01\", \"02\"],\n    [\"Bahamas\", \"911\", \"\", \"\"],\n    [\"Bahrain\", \"999\", \"\", \"\"],\n    [\"Bali\", \"112\", \"118\", \"\"],\n    [\"Bangladesh_(Dhaka)\", \"199\", \"9 555 555\", \"866 551\u20103\"],\n    [\"Barbados\", \"115,119\", \"113,119\", \"112,119\"],\n    [\"Belgium\", \"112_(cell)/101\", \"\", \"\"],\n    [\"Belarus\", \"03\", \"01\", \"02\"],\n    [\"Belize\", \"911\", \"\", \"\"],\n    [\"Bermuda\", \"911\", \"\", \"\"],\n    [\"Bhutan\", \"110\", \"112\", \"113\"],\n    [\"Bolivia\", \"911\", \"\", \"\"],\n    [\"Bonaire\", \"911\", \"\", \"\"],\n    [\"Bosnia\u2010Herzegovina\", \"124\", \"123\", \"122\"],\n    [\"Botswana\", \"997,911\", \"\", \"\"],\n    [\"Brazil\", \"911\", \"\", \"\"],\n    [\"Bosnia\", \"94\", \"93\", \"92\"],\n    [\"British_Virgin_Islands\", \"999\", \"\", \"\"],\n    [\"Brunei\", \"991\", \"995\", \"993\"],\n    [\"Bulgaria\", \"150\", \"160\", \"166\"],\n    [\"Burma/Myanmar\", \"999\", \"\", \"\"],\n    [\"Cambodia\", \"119\", \"118\", \"117\"],\n    [\"Canada\", \"911\", \"\", \"\"],\n    [\"Canary_Islands\", \"112\", \"\", \"\"],\n    [\"Cape_Verde\", \"130\", \"131\", \"132\"],\n    [\"Cayman\", \"Islands\", \"911\", \"\"],\n    [\"Chad\", \"18\", \"17\", \"\"],\n    [\"Chile\", \"131\", \"132\", \"133\"],\n    [\"The_People's_Republic of_China\", \"120\", \"119\", \"110,122_(traffic_accident)\"],\n    [\"Colombia\", \"119\", \"\", \"\"],\n    [\"Cook_Islands\", \"998\", \"996\", \"999\"],\n    [\"Costa_Rica\", \"911\", \"\", \"\"],\n    [\"C\u00f4te_d'Ivoire\", \"\", \"110,111,170\", \"180\"],\n    [\"Croatia\", \"112\", \"\", \"\"],\n    [\"Cuba\", \"26811\", \"\", \"\"],\n    [\"Curacao\", \"112\", \"114\", \"444444\"],\n    [\"Cyprus\", \"112\", \"\", \"\"],\n    [\"Czech_Republic\", \"112,155\", \"150\", \"158\"],\n    [\"Denmark\", \"112\", \"\", \"\"],\n    [\"Djibouti\", \"351351\", \"18\", \"17\"],\n    [\"Dominica\", \"999\", \"\", \"\"],\n    [\"Dominican_Republic\", \"911\", \"\", \"\"],\n    [\"East_Timor\", \"112\", \"\", \"\"],\n    [\"Easter_Island\", \"100\u2010215\", \"100\u2010264\", \"100\u2010244\"],\n    [\"Ecuador\", \"131\", \"101\", \"\"],\n    [\"Egypt\", \"123\", \"180\", \"122\"],\n    [\"El_Salvador\", \"911\", \"\", \"\"],\n    [\"England\", \"112,999\", \"\", \"\"],\n    [\"Estonia\", \"112\", \"112\", \"110\"],\n    [\"Ethiopia\", \"92\", \"93\", \"91\"],\n    [\"Falkland_Islands\", \"999\", \"\", \"\"],\n    [\"Fiji\", \"911\", \"\", \"\"],\n    [\"Finland\", \"112\", \"\", \"\"],\n    [\"France\", \"112,15\", \"112,18\", \"112,17\"],\n    [\"French_Guiana\", \"112,15\", \"112,18\", \"112,17\"],\n    [\"French_Polynesia\", \"15\", \"18\", \"17\"],\n    [\"Gabon\", \"1300\u20101399\", \"18\", \"1730\"],\n    [\"Gambia,The\", \"16\", \"18\", \"17\"],\n    [\"Georgia\", \"022\", \"\", \"\"],\n    [\"Germany\", \"112\", \"110\", \"\"],\n    [\"Ghana\", \"776111\u20105\", \"192\", \"999,171\"],\n    [\"Gibraltar\", \"999\", \"\", \"\"],\n    [\"Greece\", \"112,166\", \"112,199\", \"112,100\"],\n    [\"Grenada\", \"434\", \"112\", \"911\"],\n    [\"Guadeloupe\", \"18\", \"18\", \"17\"],\n    [\"Guam\", \"911\", \"\", \"\"],\n    [\"Guatemala\", \"123\", \"123\", \"110\"],\n    [\"Guyana\", \"999\", \"\", \"\"],\n    [\"Haiti\", \"118\", \"\", \"114\"],\n    [\"Honduras\", \"195,37 8654\", \"198\", \"119\"],\n    [\"Hong\", \"Kong\", \"999\", \"\"],\n    [\"Hungary\", \"112\", \"\", \"\"],\n    [\"Iceland\", \"112\", \"\", \"\"],\n    [\"India\", \"102\", \"101\", \"100,103_(traffic_accident)\"],\n    [\"Indonesia\", \"118\", \"113\", \"110\"],\n    [\"Iran\", \"115\", \"123\", \"110\"],\n    [\"Republic_of_Ireland\", \"112.\", \"999\", \"\"],\n    [\"Isle\", \"of\", \"Man\", \"999\"],\n    [\"Israel\", \"101\", \"102\", \"100\"],\n    [\"Italy\", \"112,118\", \"112,115\", \"112,113\"],\n    [\"Jamaica\", \"110\", \"110\", \"119\"],\n    [\"Japan\", \"119\", \"119\", \"110\"],\n    [\"Jordan\", \"191\", \"193\", \"192\"],\n    [\"Kazakhstan\", \"03\", \"\", \"\"],\n    [\"Kenya\", \"999\", \"\", \"\"],\n    [\"Kiribati\", \"994\", \"\", \"\"],\n    [\"Kosovo\", \"94\", \"\", \"\"],\n    [\"South Korea\", \"119\", \"119\", \"112\"],\n    [\"Kuwait\", \"777\", \"\", \"\"],\n    [\"Kyrgyzstan\", \"103\", \"\", \"\"],\n    [\"Laos\", \"local\", \"numbers\", \"only\"],\n    [\"Latvia\", \"112,03\", \"112,01\", \"112,02\"],\n    [\"Lebanon\", \"112\", \"\", \"\"],\n    [\"Lesotho\", \"121\", \"122\", \"123/124\"],\n    [\"Liberia\", \"911_(cell_phones_only)\", \"\", \"\"],\n    [\"Libya\", \"193\", \"\", \"\"],\n    [\"Liechtenstein\", \"112\", \"\", \"\"],\n    [\"Lithuania\", \"112\", \"\", \"\"],\n    [\"Luxembourg\", \"112/113\", \"\", \"\"],\n    [\"Macau\", \"999\", \"\", \"\"],\n    [\"Macedonia\", \"94\", \"93\", \"92\"],\n    [\"Malawi\", \"998\", \"999\", \"997\"],\n    [\"Malaysia\", \"999\", \"994\", \"999\"],\n    [\"Maldives_Republic\", \"102\", \"999\", \"119\"],\n    [\"Mali\", \"15\", \"17\", \"18\"],\n    [\"Malta\", \"112\", \"\", \"\"],\n    [\"Marianas_Island\", \"911\", \"\", \"\"],\n    [\"Marshall_Islands\", \"625 4111\", \"\", \"625 8666\"],\n    [\"Martinique\", \"15\", \"18\", \"17\"],\n    [\"Mauritania\", \"118\", \"117\", \"\"],\n    [\"Mauritius\", \"999\", \"\", \"\"],\n    [\"M\u00e9xico\", \"065\", \"068\", \"060\"],\n    [\"Moldova\", \"903\", \"901\", \"902\"],\n    [\"Monaco\", \"112\", \"\", \"\"],\n    [\"Mongolia\", \"103\", \"101\", \"102\"],\n    [\"Montenegro\", \"94\", \"\", \"\"],\n    [\"Montserrat\", \"911\", \"\", \"999\"],\n    [\"Morocco\", \"15\", \"15\", \"19\"],\n    [\"Mozambique\", \"117\", \"198\", \"119\"],\n    [\"Namibia\", \"2032276\", \"2032270\", \"1011\"],\n    [\"Nepal\", \"228094\", \"\", \"100\"],\n    [\"Netherlands\", \"112\", \"\", \"\"],\n    [\"Netherlands_Antilles\", \"112\", \"\", \"\"],\n    [\"New_Zealand\", \"111\", \"\", \"\"],\n    [\"Nicaragua\", \"128\", \"115,911\", \"118\"],\n    [\"Nigeria\", \"199\", \"\", \"\"],\n    [\"Northern\", \"Ireland\", \"112,999\", \"\"],\n    [\"Norway\", \"112,110\", \"\", \"\"],\n    [\"Oman\", \"999\", \"\", \"\"],\n    [\"Pakistan\", \"15\", \"\", \"\"],\n    [\"Palau\", \"911\", \"\", \"\"],\n    [\"Palestine\", \"101\", \"101\", \"100\"],\n    [\"Panama\", \"269\u20109778\", \"103\", \"104\"],\n    [\"Papua_New_Guinea\", \"110\", \"000\", \"\"],\n    [\"Paraguay\", \"00\", \"\", \"\"],\n    [\"Peru\", \"011,5114\", \"\", \"\"],\n    [\"Philippines\", \"166,117\", \"\", \"\"],\n    [\"Poland\", \"112,999\", \"\", \"\"],\n    [\"Portugal\", \"112\", \"\", \"\"],\n    [\"Puerto\", \"Rico\", \"911\", \"\"],\n    [\"Qatar\", \"999\", \"\", \"\"],\n    [\"R\u00e9union\", \"112,15\", \"18\", \"17\"],\n    [\"Romania\", \"112\", \"\", \"\"],\n    [\"Russia\", \"112\", \"\", \"\"],\n    [\"Samoa\", \"999\", \"\", \"\"],\n    [\"San_Marino\", \"113\", \"116\", \"112\"],\n    [\"Saudi_Arabia\", \"997\", \"998\", \"999\"],\n    [\"Scotland\", \"112,999\", \"\", \"\"],\n    [\"Serbia\", \"94\", \"\", \"\"],\n    [\"Seychelles\", \"999\", \"\", \"\"],\n    [\"Sierra_Leone\", \"999\", \"019\", \"999\"],\n    [\"Singapore\", \"995\", \"995\", \"999\"],\n    [\"Slovak_Republic_(Slovakia)\", \"155\", \"150\", \"158\"],\n    [\"Slovenia\", \"112\", \"\", \"\"],\n    [\"Solomon_Islands\", \"911\", \"\", \"\"],\n    [\"South_Africa\", \"10177\", \"10177\", \"10111\"],\n    [\"South_Africa_(Cape_Town)\", \"107\", \"\", \"\"],\n    [\"Spain\", \"112\", \"\", \"\"],\n    [\"Sri_Lanka\", \"1\", \"691095,699935\", \"\"],\n    [\"St._Helena\", \"911\", \"\", \"\"],\n    [\"St._Kitts_&_Nevis\", \"911\", \"\", \"\"],\n    [\"St._Lucia\", \"999,911\", \"\", \"\"],\n    [\"St._Marten\", \"911,542\u20102111\", \"911,120\", \"911,542\u20102111\"],\n    [\"St._Vincent_&_the_Grenadines\", \"999,911\", \"\", \"\"],\n    [\"Sweden\", \"112\", \"\", \"\"],\n    [\"Switzerland\", \"144\", \"118\", \"117\"],\n    [\"Syria\", \"110\", \"113\", \"112\"],\n    [\"Tahiti_French_Polynesia\", \"15\", \"\", \"\"],\n    [\"Taiwan\", \"119\", \"119\", \"110\"],\n    [\"Tajikistan\", \"03\", \"\", \"\"],\n    [\"Tanzania\", \"112,999\", \"\", \"\"],\n    [\"Thailand\", \"191\", \"199\", \"191\"],\n    [\"Togo\", \"101\", \"\", \"\"],\n    [\"Tonga\", \"911\", \"\", \"\"],\n    [\"Trinidad_&_Tobago\", \"990\", \"990\", \"999\"],\n    [\"Tunisia\", \"190\", \"198\", \"197\"],\n    [\"Turkey\", \"101,112\", \"102\", \"100\"],\n    [\"Turkmenistan\", \"03\", \"\", \"\"],\n    [\"Turks_and_Caicos_Islands\", \"999,911\", \"\", \"\"],\n    [\"Tuvalu\", \"911\", \"\", \"\"],\n    [\"Uganda\", \"112_(cell_phone),999_(fixed)\", \"\", \"\"],\n    [\"Ukraine\", \"03,118\", \"01\", \"02\"],\n    [\"United_Arab_Emirates\", \"998,999\", \"\", \"\"],\n    [\"United_Kingdom\", \"112,999\", \"\", \"\"],\n    [\"United\", \"States\", \"911\", \"\"],\n    [\"Uruguay\", \"999,911\", \"\", \"\"],\n    [\"US_Virgin_Islands\", \"911\", \"\", \"\"],\n    [\"Uzbekistan\", \"03\", \"\", \"\"],\n    [\"Vanuatu\", \"112\", \"\", \"\"],\n    [\"Vatican_City\", \"113\", \"115\", \"112\"],\n    [\"Venezuela\", \"171\", \"\", \"\"],\n    [\"Vietnam\", \"05\", \"08\", \"03\"],\n    [\"Western_Sahara\", \"150\", \"\", \"\"],\n    [\"Western_Samoa\", \"999\", \"\", \"\"],\n    [\"Republic_of_Yemen\", \"191\", \"191\", \"194\"],\n    [\"Zambia\", \"999\", \"\", \"\"],\n    [\"Zimbabwe\", \"994,999\", \"993,999\", \"995,999\"],\n]\n", "data/datasets/nsfw_selfharm_reddit/utils/reddit.py": "import logging\nimport os\n\nimport pandas as pd\nimport praw\nimport prawcore\nimport utils\nfrom tqdm import tqdm\n\nlogger = logging.getLogger(__name__)\n\n\ndef init_praw_reddit(client_id: str | None = None, client_secret: str | None = None, user_agent: str | None = None):\n    # setup praw\n    CLIENT_ID = client_id if client_id else os.environ.get(\"CLIENT_ID\")\n    CLIENT_SECRET = client_secret if client_secret else os.environ.get(\"CLIENT_SECRET\")\n    USER_AGENT = user_agent if user_agent else os.environ.get(\"USER_AGENT\")\n\n    # the client that communicates with reddit.\n    reddit = praw.Reddit(\n        client_id=CLIENT_ID,\n        client_secret=CLIENT_SECRET,\n        user_agent=USER_AGENT,\n    )\n    return reddit\n\n\ndef scrap_subreddit(subreddit: str, reddit) -> pd.DataFrame | None:\n    \"\"\"\n    Scrap \"hot\", \"top\", \"rising\" given a subreddit and return\n    deduped DataFrame.\n    \"\"\"\n    items = []\n    dfs = []\n\n    sub = reddit.subreddit(subreddit)\n    try:\n        sub.id\n    except prawcore.exceptions.ResponseException as e:\n        logger.error(f\"Error getting {subreddit}: {e}\")\n        return\n    ordering = (sub.hot(limit=1000), sub.top(limit=1000), sub.rising(limit=1000))\n    for order in ordering:\n        for post in tqdm(order, leave=False):\n            item = {\n                \"title\": post.title,\n                \"subreddit\": sub.display_name,\n                \"post_id\": post.id,\n                \"score\": post.score,\n                \"link_flair_text\": post.link_flair_text,\n                \"is_self\": post.is_self,\n                \"over_18\": post.over_18,\n                \"upvote_ratio\": post.upvote_ratio,\n                \"is_question\": utils.is_question(post.title),\n            }\n            items.append(item)\n        dfs.append(pd.DataFrame(items))\n\n    df = pd.concat(dfs)\n    return df.drop_duplicates(subset=[\"post_id\"])\n\n\ndef get_comments(post_ids: list, reddit: praw.Reddit):\n    \"\"\"\n    Get comments for the give list of post_ids.\n    \"\"\"\n    NUM_COMMENTS = 5\n    items = []\n    for i, post_id in enumerate(tqdm(post_ids)):\n        try:\n            item = {\"post_id\": post_id}\n            post = reddit.submission(post_id)\n            for j, c in enumerate(post.comments[:NUM_COMMENTS]):\n                item[f\"C{j+1}\"] = c.body\n            items.append(item)\n        except Exception as e:  # noqa\n            logger.error(f\"Error getting comments for {post_id}: {e}\")\n        if not (i + 1) % 100:\n            pd.DataFrame(items).to_csv(f\"comments_cache/num_{i}.csv\", index=False)\n            print(f\"[epoch-{i}]: Saved!\")\n    pd.DataFrame(items).to_csv(\"df_with_comments.csv\", index=False)\n", "data/datasets/nsfw_selfharm_reddit/utils/__init__.py": "from datasets import load_dataset\n\nfrom .is_question import is_question\nfrom .reddit import init_praw_reddit, scrap_subreddit\n\n__all__ = [\"is_question\", \"scrap_subreddit\", \"init_praw_reddit\"]\n\n\ndef save_to_huggingface(df, name):\n    TMP_FILE = \"/tmp/save_to_huggingface_tmp.json\"\n    df.to_json(TMP_FILE, orient=\"records\")\n    hf_dataset = load_dataset(\"json\", data_files=TMP_FILE)\n    hf_dataset.push_to_hub(name)\n", "data/datasets/nsfw_selfharm_reddit/utils/is_question.py": "from transformers import AutoModelForSequenceClassification, AutoTokenizer, pipeline\n\n\nclass IsQuestion:\n    def __init__(self):\n        self.tokenizer = AutoTokenizer.from_pretrained(\"shahrukhx01/question-vs-statement-classifier\")\n        self.model = AutoModelForSequenceClassification.from_pretrained(\"shahrukhx01/question-vs-statement-classifier\")\n        self.classifier = pipeline(\"sentiment-analysis\", model=self.model, tokenizer=self.tokenizer)\n        self.labels = {\n            \"LABEL_0\": False,\n            \"LABEL_1\": True,\n        }\n\n    def __call__(self, text: str) -> bool:\n        return self.labels[self.classifier(text)[0][\"label\"]]\n\n\nis_question = IsQuestion()\n", "data/datasets/reasoning_bg_oa/data_process.py": "import json\nfrom dataclasses import dataclass\n\nimport pandas as pd\nfrom datasets import concatenate_datasets, load_dataset\n\nconfigs = [\"biology-12th\", \"philosophy-12th\", \"geography-12th\", \"history-12th\", \"history-quiz\"]\ndatasets = []\n\n\n@dataclass\nclass QnA:\n    INSTRUCTION: str\n    RESPONSE: str\n    SOURCE: str\n    METADATA: str\n\n\n# format in QnA\ndef create_qna(row):\n    instruction = f'{row[\"question\"]} {\", \".join(row[\"answers\"])}?'\n    response = row[\"correct\"].translate(str.maketrans(\"\", \"\", \"();\"))\n    source = \"reasoning_bg\"\n    metadata = {\n        \"language\": \"bg\",\n        \"url\": f'{row[\"url\"]}',\n        \"id\": f'{row[\"id\"]}',\n    }\n    metadata_str = json.dumps(metadata)\n    return QnA(instruction, response, source, metadata_str)\n\n\n# merge dataset configs into one\nfor config in configs:\n    dataset = load_dataset(\"reasoning_bg\", config, split=\"train\")\n    datasets.append(dataset)\n\nmerged_dataset = concatenate_datasets(datasets)\n\nprint(merged_dataset)\n\n# convert the dataset to a pandas dataframe\ndf = pd.DataFrame(merged_dataset)\n\nqna_list = df.apply(create_qna, axis=1).tolist()\n\nqna_df = pd.DataFrame(qna_list, columns=[\"INSTRUCTION\", \"RESPONSE\", \"SOURCE\", \"METADATA\"])\nqna_df.to_parquet(\"reasoning-bg-oa.parquet\", row_group_size=100, engine=\"pyarrow\", index=False)\n", "data/datasets/zhihu-kol/convert_parquet.py": "import json\n\nimport pandas as pd\n\n\ndef reformat_csv_to_openassistant(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Reformat the downloaded CSV into either Instruction or Text format\n    so that it could be directly ingested into the training pipeline.\n\n    Parameters\n    ----------\n    df: the downloaded panda dataframe\n\n    Return\n    ------\n    DataFrame: reformatted dataframe\n    \"\"\"\n\n    new_df = pd.DataFrame()\n    new_df[\"INSTRUCTION\"] = df[\"question_title\"]\n    new_df[\"RESPONSE\"] = df[\"content\"]\n    new_df[\"SOURCE\"] = \"Zhihu\"\n    new_df[\"METADATA\"] = df.apply(\n        lambda x: json.dumps(\n            {\n                \"question_id\": x[\"question_id\"],\n                \"answer_id\": x[\"answer_id\"],\n                \"author_id\": x[\"author_id\"],\n                \"upvotes\": x[\"upvotes\"],\n                \"answer_creation_time\": x[\"answer_creation_time\"],\n            },\n            ensure_ascii=False,\n        ),\n        axis=1,\n    )\n    # Remove empty response rows\n    new_df = new_df[~(new_df[\"RESPONSE\"] == \" \") | (new_df[\"RESPONSE\"].isna())]\n\n    return new_df\n\n\nif __name__ == \"__main__\":\n    input_csv = \"zhihu.csv\"\n    # Create a pandas dataframe from your dataset file(s)\n    df = pd.read_csv(input_csv)  # or any other way\n    df = reformat_csv_to_openassistant(df)\n    # Save the file in the Parquet format\n    df.to_parquet(\"dataset.parquet\", row_group_size=100, engine=\"pyarrow\", index=False)\n", "data/datasets/zhihu-kol/upload_hf.py": "from datasets import Dataset\n\nds = Dataset.from_parquet(\"dataset.parquet\")\nds.push_to_hub(\"wangrui6/Zhihu-KOL\")\n", "data/datasets/zhihu-kol/scrape_by_topic.py": "from __future__ import annotations\n\nimport dataclasses\nimport re\nimport time\nfrom dataclasses import dataclass\nfrom typing import List, Union\n\nimport numpy as np\nimport pandas as pd\nimport requests\nfrom bs4 import BeautifulSoup\nfrom loguru import logger\nfrom playwright.sync_api import Locator, Page, sync_playwright\nfrom tqdm import tqdm\n\n\n@dataclass\nclass Content_Data:\n    question_id: int\n    answer_id: int\n    author_id: str\n    question_title: str\n    content: str\n    upvotes: str\n    answer_creation_time: str\n\n\ndef get_answer_content(qid: int, aid: int, question_str: str) -> str:\n    \"\"\"\n    \u6839\u636e\u56de\u7b54ID\u548c\u95ee\u9898ID\u83b7\u53d6\u56de\u7b54\u5185\u5bb9\n    Parameters\n    ----------\n    qid : \u95ee\u9898ID\n    aid : \u56de\u7b54ID\n    \u4f8b\u5982\u4e00\u4e2a\u56de\u7b54\u94fe\u63a5\u4e3a: https://www.zhihu.com/question/438404653/answer/1794419766\n    \u5176 qid \u4e3a 438404653\n    \u5176 aid \u4e3a 1794419766\n    \u6ce8\u610f,\u8fd9\u4e24\u4e2a\u53c2\u6570\u5747\u4e3a\u5b57\u7b26\u4e32\n    Return\n    ------\n    str : \u56de\u7b54\u5185\u5bb9\n    \"\"\"\n    headers = {\n        \"User-Agent\": \"Mozilla/5.0 (iPhone; CPU iPhone OS 14_5 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1 Mobile/15E148 Safari/604.1\",\n        \"Host\": \"www.zhihu.com\",\n    }\n    url = f\"https://www.zhihu.com/question/{qid}/answer/{aid}\"\n    response = requests.get(url, headers=headers)\n\n    soup = BeautifulSoup(response.text, \"html.parser\")\n    content = \" \".join([p.text.strip() for p in soup.find_all(\"p\")])\n    \"\"\"\n    \"<meta itemProp=\"dateCreated\" content=\"2023-02-20T13:19:30.000Z\"/>\"\n    last time from meta tag with item prop attributes seems to be the post creation datetime. I verified by looking at page online\n\n    \"\"\"\n    answer_creation_time_div = soup.find_all(\n        \"meta\",\n        {\"itemprop\": \"dateCreated\"},\n    )\n    answer_creation_time_content = \"\"\n    if len(answer_creation_time_div) > 0:\n        answer_creation_time_content = answer_creation_time_div[-1].attrs[\"content\"]\n    upvotes = (\n        soup.find(\n            \"button\",\n            {\"class\": \"Button VoteButton VoteButton--up\"},\n        )\n        .get_text()\n        .replace(\"\\u200b\", \"\")\n    )\n    author_ids = soup.find_all(\n        \"meta\",\n        {\"itemprop\": \"url\"},\n    )\n    author_id_div = [x for x in author_ids if \"/people/\" in x.attrs[\"content\"]]\n    author_id = author_id_div[0].attrs[\"content\"]\n    return Content_Data(\n        question_id=qid,\n        answer_id=aid,\n        author_id=author_id,\n        question_title=question_str,\n        content=content,\n        upvotes=upvotes,\n        answer_creation_time=answer_creation_time_content,\n    )\n\n\ndef get_all_href(page: Union[Page, Locator]) -> List[str]:\n    hrefs = page.evaluate(\n        \"\"\"() => {\n            let links = document.querySelectorAll('[href]');\n            let hrefs = [];\n            for (let link of links) {\n                hrefs.push(link.href);\n            }\n            return hrefs;\n        }\"\"\"\n    )\n    valid_hrefs = [x for x in hrefs if isinstance(x, str) and \"https://\" in x]\n    return valid_hrefs\n\n\n\"\"\"\nScrape people from round table topics. Save a list of zhihu people profile url to csv\n\"\"\"\n\n\ndef scrape_people_roundtable():\n    headless = False\n    all_ppl_df = pd.DataFrame()\n    roundtable_topic_scrolldown = 20\n    with sync_playwright() as p:\n        browser = p.chromium.launch(headless=headless, timeout=60000)\n        page = browser.new_page()\n        page.goto(\"https://zhihu.com/roundtable\")\n        # Scroll down roundtable topic to get more topic urls\n        for _ in range(roundtable_topic_scrolldown):\n            page.keyboard.down(\"End\")\n            page.wait_for_timeout(1000)\n\n        hrefs = get_all_href(page)\n        relevent_hrefs = [x for x in hrefs if \"https://www.zhihu.com/roundtable/\" in x]\n        np.random.shuffle(relevent_hrefs)\n        # Earlier round table topic might not have started yet. The offset roundtable topic is arbitrary.\n\n        starting_offset = 4\n        for topic_url in tqdm(relevent_hrefs[starting_offset:]):\n            try:\n                page.goto(topic_url)\n                all_hrefs = get_all_href(page)\n                people_urls = [x for x in all_hrefs if \"/people/\" in x]\n                latest_people_id = pd.DataFrame({\"people_id\": people_urls})\n                all_ppl_df = pd.concat([all_ppl_df, latest_people_id])\n            except Exception as e1:\n                logger.error(e1)\n\n            all_ppl_df.to_csv(\"people.csv\")\n\n\n\"\"\"\nEnd to end auto scrape topics from round table\n\"\"\"\n\n\ndef end_to_end_auto_scrape():\n    headless = False\n    pattern = r\"/question/\\d+/answer/\\d+\"\n    all_payloads = []\n    roundtable_topic_scrolldown = 20\n    with sync_playwright() as p:\n        browser = p.chromium.launch(headless=headless, timeout=60000)\n        page = browser.new_page()\n        page.goto(\"https://zhihu.com/roundtable\")\n        # Scroll down roundtable topic to get more topic urls\n        for _ in range(roundtable_topic_scrolldown):\n            page.keyboard.down(\"End\")\n            page.wait_for_timeout(1000)\n\n        hrefs = get_all_href(page)\n        relevent_hrefs = [x for x in hrefs if \"https://www.zhihu.com/roundtable/\" in x]\n        np.random.shuffle(relevent_hrefs)\n        # Earlier round table topic might not have started yet. The offset roundtable topic is arbitrary.\n\n        starting_offset = 4\n        for topic_url in tqdm(relevent_hrefs[starting_offset:]):\n            try:\n                page.goto(topic_url)\n                all_hrefs = get_all_href(page)\n                question_urls = set([x for x in all_hrefs if \"/question/\" in x and \"waiting\" not in x])\n                # people_urls = [x for x in all_hrefs if \"/people/\" in x]\n                for qId in question_urls:\n                    qUrl = qId.replace(\"?write\", \"\")\n\n                    page.goto(qUrl)\n                    question_title = page.locator(\".QuestionHeader-title\").all_inner_texts()[0]\n                    all_hrefs = get_all_href(page.locator(\".QuestionAnswers-answers\"))\n                    # search for all question-answer url\n                    matches_question_answer_url = set(\n                        [s for s in all_hrefs if isinstance(s, str) and re.search(pattern, s)]\n                    )\n\n                    for k in matches_question_answer_url:\n                        elem = k.split(\"/\")\n                        qId = int(elem[-3])\n                        aId = int(elem[-1])\n\n                        complete_content_data = get_answer_content(qId, aId, question_title)\n\n                        content_data_dict = dataclasses.asdict(complete_content_data)\n                        all_payloads.append(content_data_dict)\n                        time.sleep(1)\n            except Exception as e1:\n                logger.error(e1)\n            tmp_df = pd.json_normalize(all_payloads)\n            print(tmp_df)\n            tmp_df.to_csv(\"zhihu.csv\")\n\n\nif __name__ == \"__main__\":\n    # scrape_people_roundtable()\n    end_to_end_auto_scrape()\n", "data/datasets/zhihu-kol/main.py": "from __future__ import annotations\n\nimport json\n\nimport multitasking\nimport pandas as pd\nimport requests\nfrom bs4 import BeautifulSoup\nfrom retry import retry\nfrom tqdm import tqdm\n\n\ndef get_uid_by_url_token(url_token: str) -> str:\n    \"\"\"\n    \u6839\u636e\u77e5\u4e4e\u7528\u6237 url_token \u83b7\u53d6\u5176 uid\n\n    Parameters\n    ----------\n    url_token : \u77e5\u4e4e\u7528\u6237 url_token\n        \u4f8b\u5982\u4e3b\u9875\u4e3a:https://www.zhihu.com/people/la-ge-lang-ri-96-69 \u7684\u7528\u6237\n        \u5176 url_token \u4e3a: la-ge-lang-ri-96-69\n\n        \u6ce8\u610f,\u6b64\u53c2\u6570\u7c7b\u578b\u4e3a\u5b57\u7b26\u4e32\n\n    Return\n    ------\n    str : \u7528\u6237 uid\n    \"\"\"\n    headers = {\n        \"authority\": \"www.zhihu.com\",\n        \"user-agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.150 Safari/537.36 Edg/88.0.705.68\",\n        \"x-requested-with\": \"fetch\",\n        \"content-type\": \"multipart/form-data; boundary=----WebKitFormBoundarycwskcLmf85lBwPKR\",\n        \"accept\": \"*/*\",\n        \"origin\": \"https://www.zhihu.com\",\n        \"sec-fetch-site\": \"same-origin\",\n        \"sec-fetch-mode\": \"cors\",\n        \"sec-fetch-dest\": \"empty\",\n        \"referer\": \"https://www.zhihu.com/\",\n        \"accept-language\": \"zh-CN,zh;q=0.9,en;q=0.8,en-GB;q=0.7,en-US;q=0.6\",\n    }\n\n    url = \"https://api.zhihu.com/people/\" + url_token\n    response = requests.get(url, headers=headers)\n    uid = response.json()[\"id\"]\n    return uid\n\n\n@retry(tries=3)\ndef get_user_answers(url_token: str, max_count: int = 100000) -> pd.DataFrame:\n    \"\"\"\n    \u83b7\u53d6\u7528\u6237\u7684\u56de\u7b54\u811a\u672c\u6570\u636e\u5217\u8868\n\n    Parameters\n    ----------\n    url_token : \u77e5\u4e4e\u7528\u6237 url_token\n        \u4f8b\u5982\u4e3b\u9875\u4e3a:https://www.zhihu.com/people/la-ge-lang-ri-96-69 \u7684\u7528\u6237\n        \u5176 url_token \u4e3a: la-ge-lang-ri-96-69\n\n        \u6ce8\u610f,\u6b64\u53c2\u6570\u7c7b\u578b\u4e3a\u5b57\u7b26\u4e32\n\n    max_count : \u9650\u5236\u83b7\u53d6\u7684\u6700\u5927\u56de\u7b54\u6570(\u9ed8\u8ba4\u4e3a 100000)\n\n    Return\n    ------\n    DataFrame : \u5305\u542b\u7528\u6237\u56de\u7b54\u6570\u636e\u7684 DataFrame\n\n\n    \"\"\"\n    headers = {\n        \"User-Agent\": \"osee2unifiedRelease/4318 osee2unifiedReleaseVersion/7.7.0 Mozilla/5.0 (iPhone; CPU iPhone OS 14_5 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Mobile/15E148\",\n        \"X-APP-BUILD-VERSION\": \"4318\",\n        \"x-app-bundleid\": \"com.zhihu.ios\",\n        \"X-APP-ZA\": \"OS=iOS&Release=14.5&Model=iPhone10,1&VersionName=7.7.0&VersionCode=4318&Width=750&Height=1334&DeviceType=Phone&Brand=Apple&OperatorType=46009\",\n    }\n\n    operations = {\n        \"\u4f5c\u8005\u540d\u79f0\": [\"author\", lambda x: x[\"name\"]],\n        \"\u4f5c\u8005ID\": [\"author\", lambda x: x[\"id\"]],\n        \"\u4f5c\u8005token\": [\"author\", lambda x: x[\"url_token\"]],\n        \"\u56de\u7b54\u70b9\u8d5e\u6570\": [\"voteup_count\", lambda x: x],\n        \"\u56de\u7b54\u65f6\u95f4\": [\"created_time\", lambda x: x],\n        \"\u66f4\u65b0\u65f6\u95f4\": [\"updated_time\", lambda x: x],\n        \"\u56de\u7b54ID\": [\"url\", lambda x: x.split(\"/\")[-1]],\n        \"\u95ee\u9898ID\": [\"question\", lambda x: x[\"id\"]],\n        \"\u95ee\u9898\u5185\u5bb9\": [\"question\", lambda x: x[\"title\"]],\n    }\n    try:\n        uid = get_uid_by_url_token(url_token)\n    except Exception:\n        return pd.DataFrame(columns=operations.keys())\n    bar: tqdm = None\n    offset = 0\n    limit = 20\n    dfs: list[pd.DataFrame] = []\n    url = f\"https://api.zhihu.com/members/{uid}/answers\"\n    while 1:\n        params = (\n            (\"limit\", f\"{limit}\"),\n            (\"offset\", f\"{offset}\"),\n        )\n\n        response = requests.get(url, headers=headers, params=params)\n\n        if response.json().get(\"paging\") is None:\n            return pd.DataFrame(columns=operations.keys())\n        total = response.json()[\"paging\"][\"totals\"]\n        if bar is None:\n            bar = tqdm(total=total, desc=\"\u83b7\u53d6\u56de\u7b54\u6570\u636e\u4e2d\")\n        bar.update(limit)\n        data = response.json().get(\"data\")\n        raw_df = pd.DataFrame(data)\n        if len(raw_df) == 0 or offset >= total or offset > max_count:\n            break\n        df = pd.DataFrame(columns=operations.keys())\n        for new_column, (old_column, operation) in operations.items():\n            df[new_column] = raw_df[old_column].apply(operation)\n        dfs.append(df)\n        offset += 20\n\n    bar.close()\n    df = pd.concat(dfs)\n    return df\n\n\ndef get_answer_content(qid: str, aid) -> str:\n    \"\"\"\n    \u6839\u636e\u56de\u7b54ID\u548c\u95ee\u9898ID\u83b7\u53d6\u56de\u7b54\u5185\u5bb9\n\n    Parameters\n    ----------\n    qid : \u95ee\u9898ID\n    aid : \u56de\u7b54ID\n    \u4f8b\u5982\u4e00\u4e2a\u56de\u7b54\u94fe\u63a5\u4e3a: https://www.zhihu.com/question/438404653/answer/1794419766\n\n    \u5176 qid \u4e3a 438404653\n\n    \u5176 aid \u4e3a 1794419766\n\n    \u6ce8\u610f,\u8fd9\u4e24\u4e2a\u53c2\u6570\u5747\u4e3a\u5b57\u7b26\u4e32\n\n    Return\n    ------\n    str : \u56de\u7b54\u5185\u5bb9\n    \"\"\"\n    headers = {\n        \"User-Agent\": \"Mozilla/5.0 (iPhone; CPU iPhone OS 14_5 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1 Mobile/15E148 Safari/604.1\",\n        \"Host\": \"www.zhihu.com\",\n    }\n    url = f\"https://www.zhihu.com/question/{qid}/answer/{aid}\"\n    response = requests.get(url, headers=headers)\n\n    soup = BeautifulSoup(response.text, \"html.parser\")\n    content = \" \".join([p.text.strip() for p in soup.find_all(\"p\")])\n    return content\n\n\ndef reformat_csv_to_openassistant(df: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"\n    Reformat the downloaded CSV into either Instruction or Text format\n    so that it could be directly ingested into the training pipeline.\n\n    Parameters\n    ----------\n    df: the downloaded panda dataframe\n\n    Return\n    ------\n    DataFrame: reformatted dataframe\n    \"\"\"\n    new_df = pd.DataFrame()\n    new_df[\"INSTRUCTION\"] = df[\"\u95ee\u9898\u5185\u5bb9\"]\n    new_df[\"RESPONSE\"] = df[\"\u56de\u7b54\u5185\u5bb9\"]\n    new_df[\"SOURCE\"] = \"Zhihu\"\n    new_df[\"METADATA\"] = df.apply(\n        lambda x: json.dumps(\n            {\n                \"\u56de\u7b54\u70b9\u8d5e\u6570\": x[\"\u56de\u7b54\u70b9\u8d5e\u6570\"],\n                \"\u56de\u7b54\u65f6\u95f4\": x[\"\u56de\u7b54\u65f6\u95f4\"],\n            },\n            ensure_ascii=False,\n        ),\n        axis=1,\n    )\n\n    return new_df\n\n\ndef save_answers_to_csv(url_token: str, csv_path: str, max_count: int = 10000) -> None:\n    \"\"\"\n    \u6839\u636e\u7528\u6237 url_token \u83b7\u53d6\u7528\u6237\u56de\u7b54\u6570\u636e,\u5e76\u4fdd\u5b58\u5230 csv \u6587\u4ef6\u4e2d\n\n    Parameters\n    ----------\n    url_token : \u77e5\u4e4e\u7528\u6237 url_token\n        \u4f8b\u5982\u4e3b\u9875\u4e3a:https://www.zhihu.com/people/la-ge-lang-ri-96-69 \u7684\u7528\u6237\n        \u5176 url_token \u4e3a: la-ge-lang-ri-96-69\n\n        \u6ce8\u610f,\u6b64\u53c2\u6570\u7c7b\u578b\u4e3a\u5b57\u7b26\u4e32\n\n    csv_path : \u5f85\u4fdd\u5b58\u7684\u56de\u7b54\u6570\u636e csv \u8def\u5f84\n        \u4f8b\u5982: '\u56de\u7b54\u6570\u636e.csv'\n\n    max_count : \u9650\u5236\u83b7\u53d6\u7684\u6700\u5927\u56de\u7b54\u6570(\u53ef\u9009,\u9ed8\u8ba4\u4e3a 100000)\n\n    Return\n    ------\n    DataFrame:\u5305\u542b\u7528\u6237\u591a\u4e2a\u56de\u7b54\u6570\u636e\u7684 DataFrame\n    \"\"\"\n    df = get_user_answers(url_token, max_count=max_count)\n    if len(df) == 0:\n        print(\"url_token \u53ef\u80fd\u6709\u8bef!\")\n        return\n    content_list = {}\n\n    @retry(tries=3)\n    @multitasking.task\n    def start(qid: str, aid: str):\n        content = get_answer_content(qid, aid)\n        content_list[qid] = content  # make sure the qid and aid answer are corresponding during multitasking\n        bar.update()\n\n    bar = tqdm(total=len(df), desc=\"\u83b7\u53d6\u56de\u7b54\u5185\u5bb9\")\n    for row in df.iloc:\n        qid, aid = row[\"\u95ee\u9898ID\"], row[\"\u56de\u7b54ID\"]\n        start(qid, aid)\n    multitasking.wait_for_tasks()\n    df[\"\u56de\u7b54\u5185\u5bb9\"] = df[\"\u95ee\u9898ID\"].apply(lambda x: content_list[x])\n    updated_df = reformat_csv_to_openassistant(df)\n    updated_df.to_csv(csv_path, encoding=\"utf-8-sig\", index=None)\n    bar.close()\n    print(f\"url_token \u4e3a {url_token} \u7684\u7528\u6237\u56de\u7b54\u6570\u636e\u5df2\u5b58\u50a8\u5230\u6587\u4ef6:{csv_path}\")\n\n\nif __name__ == \"__main__\":\n    # \u77e5\u4e4e\u7528\u6237\u7684 url_token\n    # \u4f8b\u5982\u4e3b\u9875\u4e3a : https://www.zhihu.com/people/la-ge-lang-ri-96-69 \u7684\u7528\u6237\n    # \u5176 url_token \u4e3a la-ge-lang-ri-96-69\n    # url_token = 'la-ge-lang-ri-96-69'\n    url_token = \"nicole-97-93\"\n    # \u56de\u7b54\u6570\u636e\u4fdd\u5b58\u8def\u5f84\n    csv_path = url_token + \".csv\"\n    # \u8c03\u7528\u51fd\u6570\u83b7\u53d6\u6570\u636e\n    save_answers_to_csv(url_token, csv_path)\n", "data/datasets/zhihu-kol/__init__.py": "", "data/datasets/poetry_instruction/prepare.py": "import json\nimport os\nimport random\n\nimport kaggle\nimport pandas as pd\n\n# Authenticate the Kaggle API client\nkaggle.api.authenticate()\n\n# Download and extract the dataset to the download_path directory\ndownload_path = os.path.join(os.getcwd(), \"data\", \"datasets\", \"poetry_instruction\")\nkaggle.api.dataset_download_files(\"tgdivy/poetry-foundation-poems\", path=download_path, unzip=True)\n\n# Read the CSV file into a pandas dataframe\ncsv_file = os.path.join(download_path, \"PoetryFoundationData.csv\")\ndf = pd.read_csv(csv_file)\n\n# The data in the CSV file is not formatted correctly, so we need to clean it up.\ndf[\"Title\"] = df[\"Title\"].replace(\"\\n\", \"\", regex=True).replace(\"\\r\", \"\", regex=True)\ndf[\"Title\"] = df[\"Title\"].str.strip()\ndf[\"Title\"] = df[\"Title\"].apply(lambda x: f'\"{x}\"')\ndf[\"Poem\"] = df[\"Poem\"].str.strip()\ndf[\"Poem\"] = df[\"Poem\"].str.replace(\"Translated from the French\", \"\")\n\n# \"writing_prompts\" are for tasks requesting the assistant to write a poem.\n# \"topic\" or \"notTopic\" are used depending if the original dataset had a topic listed for the poem or not.\nwriting_prompts_topic = [\n    \"Write me a poem about $topic.\",\n    \"I want a poem about $topic.\",\n    \"Can you write a poem? Make it about $topic.\",\n    \"Compose a poem, about $topic.\",\n    \"Make a poem with themes of $topic.\" \"Generate a poem with the following themes: $topic.\",\n]\n\nwriting_prompts_notTopic = [\n    \"Write me a poem.\",\n    \"I want a poem.\",\n    \"Can you write a poem?\",\n    \"Compose a poem.\",\n    \"Make a poem.\",\n    \"Generate a poem.\",\n]\n\n# These are replies that the assistant can give to the user.\nreplies_topic = [\n    \"Here's a poem about $topic: \\n$title\\n$poem\",\n    \"Sure, I can do that. Here's a poem about $topic. I call it $title: \\n$poem\",\n    \"Okay, a poem about $topic: \\n$title\\n$poem\",\n    \"Of course! It's called $title: \\n$poem\",\n    \"It's called $title: \\n$poem\",\n    \"Here's your poem about $topic: \\n$title\\n$poem\",\n    \"I've written a poem for you about $topic. The title is $title: \\n$poem\",\n    \"Here's a beautiful poem about $topic for you. It's called $title: \\n$poem\",\n    \"This is a poem about $topic that I just wrote. It's called $title: \\n$poem\",\n    \"Here's a poem I composed about $topic. It's called $title: \\n$poem\",\n]\n\nreplies_notTopic = [\n    \"Here's a poem: \\n$title\\n$poem\",\n    \"Sure, I can do that. Here's a poem. I call it $title: \\n$poem\",\n    \"Okay, a poem: \\n$title\\n$poem\",\n    \"Of course! It's called $title: \\n$poem\",\n    \"It's called $title: \\n$poem\",\n    \"Here's your poem: \\n$title\\n$poem\",\n    \"I've written a poem for you. The title is $title: \\n$poem\",\n    \"Here's a beautiful poem for you. It's called $title: \\n$poem\",\n    \"This is a poem that I just wrote. It's called $title: \\n$poem\",\n    \"Here's a poem I composed. It's called $title: \\n$poem\",\n]\n\n# \"titling_prompts\" are for tasks requesting that the assistant titles a poem. They make up 5% of the dataset.\ntitling_prompts = [\n    \"Title this poem: \\n$poem\",\n    \"Come up with a unique title for my poem: \\n$poem\",\n    \"What should I call this poem? \\n$poem\",\n    \"Name this poem: \\n$poem\",\n    \"What would be a good title for this poem? \\n$poem\",\n    \"I need help coming up with a title for my poem. \\n$poem\",\n    \"$poem\\nWhat should I call this poem?\",\n]\n\ntitling_replies = [\n    \"Based on the poem, a good title could be $title.\",\n    \"I suggest titling this poem $title.\",\n    \"How about calling it $title?\",\n    \"You could name this poem $title.\",\n    \"The title that comes to mind is $title.\",\n    \"Perhaps $title would be a fitting title for this poem.\",\n    \"I think $title would be a great title for this poem.\",\n    \"This poem seems like it could be called $title to me.\",\n    \"$title is a good title for this poem.\",\n]\n\n# Shuffling the dataset and delegating 5% to titling tasks.\n# Calculating the number of titling tasks and writing tasks.\nnum_rows = len(df)\nnum_titling_tasks = int(num_rows * 0.05)\nnum_writing_tasks = num_rows - num_titling_tasks\n\n# Shuffle the rows in the DataFrame.\ndf = df.sample(frac=1)\n\n# Split the DataFrame into two DataFrames, one for titling tasks and one for writing tasks.\nwriting_tasks = df.iloc[:num_writing_tasks]\ntitling_tasks = df.iloc[num_writing_tasks:]\n\nprepared_data = []\n\n# Loop through the writing tasks and process them.\nfor index, row in writing_tasks.iterrows():\n    # Get data from the entry\n    poem = row[\"Poem\"]\n    topics = row[\"Tags\"]\n    title = row[\"Title\"]\n    author = row[\"Poet\"]\n\n    # Variables to store to instruction, reply, source, and metadata.\n    instruction = random.choice(writing_prompts_topic).replace(\"$topic\", str(topics))\n    reply = random.choice(replies_topic).replace(\"$topic\", str(topics)).replace(\"$title\", title).replace(\"$poem\", poem)\n    source = \"PoetryFoundation.org\" + \" - \" + author\n    metadata = {\"author\": author, \"title\": title, \"tags\": str(topics), \"task_type\": \"writing\"}\n\n    # If the entry has an empty value for the topic, use the non-topic prompts and replies.\n    if pd.isna(topics):\n        instruction = random.choice(writing_prompts_notTopic)\n        reply = random.choice(replies_notTopic).replace(\"$title\", title).replace(\"$poem\", poem)\n\n    # Create a dictionary entry for the entry and append it to the list.\n    entry = {\"INSTRUCTION\": instruction, \"RESPONSE\": reply, \"SOURCE\": source, \"METADATA\": json.dumps(metadata)}\n    prepared_data.append(entry)\n\n# Loop through the titling tasks and process them.\nfor index, row in titling_tasks.iterrows():\n    # Get data from the entry\n    poem = row[\"Poem\"]\n    topics = row[\"Tags\"]\n    title = row[\"Title\"]\n    author = row[\"Poet\"]\n\n    # Variables to store to instruction, reply, source, and metadata.\n    instruction = random.choice(titling_prompts).replace(\"$poem\", poem)\n    reply = random.choice(titling_replies).replace(\"$title\", title)\n    source = \"PoetryFoundation.org\" + \" - \" + author\n    metadata = {\"author\": author, \"title\": title, \"tags\": str(topics), \"task_type\": \"titling\"}\n\n    # Create a dictionary entry for the entry and append it to the list.\n    entry = {\"INSTRUCTION\": instruction, \"RESPONSE\": reply, \"SOURCE\": source, \"METADATA\": json.dumps(metadata)}\n    prepared_data.append(entry)\n\n# Convert prepared_data to a DataFrame.\nprepared_data = pd.DataFrame(prepared_data)\n\n# Save the DataFrame to disk in the Parquet format\nprepared_data.to_parquet(\"output.parquet\", row_group_size=100, engine=\"pyarrow\", index=False)\n\n# Print the amount of entries in the final converted dataset\nprint(f\"Prepared {len(df)} entries\")\n", "data/datasets/oa_dolly_15k/create_dataset.py": "import json\nfrom pathlib import Path\n\nimport requests\nfrom datasets import Dataset\n\nDATA_URL = \"https://raw.githubusercontent.com/databrickslabs/dolly/master/data/databricks-dolly-15k.jsonl\"\nFILE_PATH = \"databricks_dolly_15k.jsonl\"\n\n\ndef download_data(url: str, destination: str):\n    response = requests.get(url, stream=True)\n\n    with open(destination, \"wb\") as handle:\n        for data in response.iter_content():\n            handle.write(data)\n\n\ndef build_dataset(data_file: str, include_context: bool = True) -> Dataset:\n    json_data = [\n        to_oa_format(json.loads(line), include_context=include_context)\n        for line in Path(data_file).read_text().splitlines()\n    ]\n\n    dataset = Dataset.from_list(json_data)\n    return dataset\n\n\ndef to_oa_format(data: dict, include_context: bool = True) -> dict:\n    output_data = {\n        \"INSTRUCTION\": data[\"instruction\"],\n        \"RESPONSE\": data[\"response\"],\n        \"SOURCE\": \"databricks-dolly-15k\",\n        \"METADATA\": {\n            \"CATEGORY\": data[\"category\"],\n        },\n    }\n\n    if include_context:\n        output_data[\"METADATA\"][\"CONTEXT\"] = data[\"context\"]\n\n    return output_data\n\n\ndef main():\n    download_data(DATA_URL, FILE_PATH)\n    dataset = build_dataset(FILE_PATH, include_context=True)\n    dataset.push_to_hub(\"OllieStanley/oa_dolly_15k\")\n\n\nif __name__ == \"__main__\":\n    main()\n", "data/datasets/cocktail_recipes/loading_script.py": "from datasets import load_dataset\n\nif __name__ == \"__main__\":\n    ds = load_dataset(\"brianarbuckle/cocktail_recipes\")\n    print(ds[\"train\"])\n", "data/datasets/reasoning_gsm_qna_oa/data_process.py": "import json\nimport random\nimport re\nfrom dataclasses import dataclass\n\nimport pandas as pd\nfrom datasets import load_dataset\n\nrandom.seed(42)\n\nrandom_list_python = [\n    \"Make a python code.\",\n    \"Make a python script. Only function.\",\n    \"Write a solution in python.\",\n    \"Solve with Python.\",\n    \"Please, use python!\",\n    \"Also, could you use python?\",\n    \"Think and write in python.\",\n    \"Write a function in python.\",\n    \"Make a Python function.\",\n]\n\nrandom_list_answer = [\n    \"\\nAnswer is\",\n    \"\\nThe final answer:\",\n    \"\\nThe answer will be\",\n]\n\n\ndef qna_wrapper(source, random_list_python, random_list_answer):\n    def create_qna(row):\n        instruction = row[\"question\"] if source == \"gsm8k\" else row[\"input\"] + \" \" + random.choice(random_list_python)\n        response = (\n            re.sub(r\"(<<[\\d\\.\\-\\+\\*=/\\\\]+>>)\", \"\", row[\"answer\"].replace(\"####\", random.choice(random_list_answer)))\n            + \".\"\n            if source == \"gsm8k\"\n            else row[\"code\"]\n        )\n        metadata = {\n            \"language\": \"en\",\n        }\n        metadata_str = json.dumps(metadata)\n        return QnA(instruction, response, source, metadata_str)\n\n    return create_qna\n\n\n@dataclass\nclass QnA:\n    INSTRUCTION: str\n    RESPONSE: str\n    SOURCE: str\n    METADATA: str\n\n\n# load gsm8k & gsm-hard\ndataset1 = load_dataset(\"gsm8k\", \"main\", split=\"train\")\nprint(dataset1)\n\ndataset2 = load_dataset(\"reasoning-machines/gsm-hard\", split=\"train\")\nprint(dataset2)\n\n# process gsm8k & gsm-hard\nqna_list_1 = pd.DataFrame(dataset1).apply(qna_wrapper(\"gsm8k\", random_list_python, random_list_answer), axis=1).tolist()\nqna_list_2 = (\n    pd.DataFrame(dataset2).apply(qna_wrapper(\"gsm-hard\", random_list_python, random_list_answer), axis=1).tolist()\n)\n\n# merge gsm8k & gsm-hard\nqna_list = qna_list_1 + qna_list_2\n\n# convert to parquet\nqna_df = pd.DataFrame(qna_list, columns=[\"INSTRUCTION\", \"RESPONSE\", \"SOURCE\", \"METADATA\"])\nqna_df.to_parquet(\"reasoning-gsm-qna.parquet\", row_group_size=100, engine=\"pyarrow\", index=False)\n", "data/datasets/bart_searchgpt_wiki_nlp_augment/5_test_downloading_my_dataset.py": "if __name__ == \"__main__\":\n    from datasets import load_dataset\n\n    dataset = load_dataset(\"michaelthwan/oa_wiki_qa_bart_10000row\", split=\"train\")\n    print(dataset[0])\n", "data/datasets/bart_searchgpt_wiki_nlp_augment/1_clean_wikitext.py": "import os\nimport re\nimport time\nimport timeit\n\nimport pandas as pd\nimport psutil\nfrom datasets import load_dataset\nfrom tqdm import tqdm\n\n\ndef memory_and_speed_test():\n    mem_before = psutil.Process(os.getpid()).memory_info().rss >> 20\n    wiki = load_dataset(\"wikipedia\", \"20220301.en\", split=\"train\")\n    mem_after = psutil.Process(os.getpid()).memory_info().rss >> 20\n    print(f\"RAM memory used: {(mem_after - mem_before)} MB\")\n\n    s = \"\"\"batch_size = 1000\n    for i in range(0, len(wiki), batch_size):\n        batch = wiki[i:i + batch_size]\n    \"\"\"\n    time = timeit.timeit(stmt=s, number=1, globals=globals())\n    size = wiki.dataset_size / 2**30\n    print(f\"Iterated over the {size:.1f} GB dataset in {time:.1f} s, i.e. {size * 8 / time:.1f} Gbit/s\")\n    # @michaelthwan output\n    # RAM memory used: 18 MB\n    # Iterated over the 18.9 GB dataset in 43.1 s, i.e. 3.5 Gbit/s\n\n\ndef remove_empty_lines(article: str) -> str:\n    return article.replace(\"\\n\\n\", \"\\n\")\n\n\ndef extract_main_content(article: str) -> (str, int):\n    lines = []\n    word_num = 0\n    is_first_line = True\n    for line in article.splitlines():\n        if (len(line.split(\" \")) <= 5 or word_num >= 500) and not is_first_line:\n            return \"\\n\".join(lines), word_num\n        is_first_line = False\n        word_num += len(line.split(\" \"))\n        lines.append(line)\n    return \"\\n\".join(lines), word_num\n\n\ndef remove_all_parentesis(article: str) -> str:\n    return re.sub(r\"\\([^)]*\\)\", \"\", article)\n\n\nif __name__ == \"__main__\":\n    wiki_dataset = load_dataset(\"wikipedia\", \"20220301.en\", split=\"train\")\n\n    count = 0\n    id_list, url_list, text_list, title_list, word_num_list = [], [], [], [], []\n    for page in tqdm(wiki_dataset):\n        count += 1\n        if count % 1000 == 1:\n            date = time.strftime(\"%Y%m%d_%H%M%S\", time.localtime())\n            print(f\"[{date}] count: {count}\")\n        # if count > 100000:\n        #     break\n\n        id, url, text, title = page[\"id\"], page[\"url\"], page[\"text\"], page[\"title\"]\n        # print(f'title: {title}')\n        text = remove_empty_lines(text)\n        text, word_num = extract_main_content(text)\n        text = remove_all_parentesis(text)\n        # print(f'word_num: {word_num}')\n\n        id_list.append(id)\n        url_list.append(url)\n        text_list.append(text)\n        title_list.append(title)\n        word_num_list.append(word_num)\n    df = pd.DataFrame(\n        {\"id\": id_list, \"url\": url_list, \"text\": text_list, \"title\": title_list, \"word_num\": word_num_list}\n    )\n    df.to_parquet(\"wiki_trimmed.parquet\")\n\n# if __name__ == '__main__':\n#     df = pd.read_parquet('wiki_top1000.parquet')\n#     print(df.iloc[0]['text'])\n", "data/datasets/bart_searchgpt_wiki_nlp_augment/3_10k_bart_trial.py": "import pandas as pd\nimport tiktoken\nimport torch\nimport tqdm\nfrom transformers import pipeline\n\n\ndef num_tokens_from_string(string: str) -> int:\n    encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n    num_tokens = len(encoding.encode(string))\n    return num_tokens\n\n\nif __name__ == \"__main__\":\n    sampled_df = pd.read_csv(\"wiki_qa_bart_10000row_input.csv\")\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(device)\n    summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n    response_list = []\n    # sampled_df = sampled_df.iloc[:5]\n    for i, row in tqdm.tqdm(sampled_df.iterrows(), total=sampled_df.shape[0]):\n        try:\n            num_tokens = num_tokens_from_string(row[\"text\"])\n            min_length = 40\n            max_length = max(int(num_tokens * 0.7), min_length) + 1\n            response = summarizer(row[\"text\"], max_length=max_length, min_length=min_length, do_sample=False)[0][\n                \"summary_text\"\n            ]\n        except Exception as e:\n            print(e)\n            response = \"\"\n        print(f\"text (token: {num_tokens}, max_length: {max_length}, min_length: {min_length})\")\n        print(row[\"text\"])\n        print(\"output\")\n        print(response)\n        response_list.append(response)\n    sampled_df[\"response\"] = response_list\n    sampled_df.to_csv(\"wiki_qa_bart_10000row.csv\", index=False)\n", "data/datasets/bart_searchgpt_wiki_nlp_augment/4_convert_to_oa_format.py": "import json\n\nimport pandas as pd\n\nif __name__ == \"__main__\":\n    raw_df = pd.read_csv(r\"...\\wiki_qa_bart_10000row.csv\")\n    # print(raw_df.iloc[0])\n    # print(raw_df.columns)\n    instruction_list, response_list, metadata_list = [], [], []\n    for index, row in raw_df.iterrows():\n        instruction_list.append(row[\"query\"])\n        response_list.append(row[\"response\"])\n        metadata_json = {\n            \"wiki_id\": row[\"id\"],\n            \"text\": row[\"text\"],\n            \"title\": row[\"title\"],\n            \"url\": row[\"url\"],\n            \"question_parent\": row[\"parent\"],\n        }\n        metadata_list.append(json.dumps(metadata_json))\n    final_df = pd.DataFrame(\n        {\"INSTRUCTION\": instruction_list, \"RESPONSE\": response_list, \"SOURCE\": \"wikipedia\", \"METADATA\": metadata_list}\n    )\n    final_df.to_parquet(\"oa_wiki_qa_bart_10000row.parquet\", row_group_size=100)\n", "data/datasets/iapp_wiki_qa_squad/loading_script.py": "from datasets import load_dataset\n\nif __name__ == \"__main__\":\n    ds = load_dataset(\"wannaphong/iapp_wiki_qa_squad_oa\")\n    print(ds)\n", "data/datasets/biostars_qa/get_biostars_dataset.py": "import json\nimport os\nimport re\nimport time\n\nimport pandas as pd\nimport requests\nfrom tqdm import tqdm\n\n\ndef get_biostars_dataset(start_idx=9557161, accept_threshold=1000000, sleep=0.1, folder=\"biostars\"):\n    \"\"\"\n    Download BioStarts data set from the official API using GET requests\n\n    Args:\n        start_idx (int): The identifier (UID) of the post to retrieve; 9557161 was the last post included in the dataset\n        accept_threshold (int): stop if this many posts with \"has_accepted\" true are retrieved\n        sleep (float): Amount of time to sleep between requests\n        folder (string): folder to store responses as JSON files\n    Returns:\n        Nothing. Content is saved to individual JSON files for each post.\n    \"\"\"\n\n    headers = {\"Content-Type\": \"application/json\"}\n\n    has_accepted_count = 0\n\n    pbar = tqdm(range(start_idx, 0, -1), desc=\"Running ...\")\n\n    for idx in pbar:\n        url = f\"https://www.biostars.org/api/post/{idx}\"\n        file = os.path.join(folder, f\"{idx}.json\")\n\n        if os.path.isfile(file):\n            with open(file, \"r\") as f:\n                data = json.load(f)\n\n                if data.get(\"has_accepted\"):\n                    has_accepted_count += 1\n\n            print(f\"MSG: {file} exists. Skipping; Current accepted: {has_accepted_count}\")\n            continue\n\n        r = requests.get(url, headers=headers)\n\n        # print(r.status_code, r.reason)\n\n        if r.status_code == 200:\n            data = r.json()\n\n            if data.get(\"has_accepted\"):\n                has_accepted_count += 1\n\n            with open(file, \"w\") as f:\n                json.dump(data, f)\n            # print(f\"MSG: File downloaded: {idx}; Current accepted: {has_accepted_count}\")\n        else:\n            print(\"ERROR: Retrieving data: \", idx)\n        time.sleep(sleep)\n\n        if has_accepted_count == accept_threshold:\n            print(f\"{accept_threshold} entries with has_accepted found. Stopping.\")\n            break\n\n        pbar.set_description(f\"Item: {idx}; Accepted {has_accepted_count}\")\n        # tqdm.set_description(f\"Cur: {idx}; Accepted: {has_accepted_count}\")\n\n\ndef extract_accepted_data(folder=\"biostars\", merged_json_file=None):\n    \"\"\"\n    Extract questions paired with their accepted answers\n\n    Args:\n        folder (string): folder to store responses as JSON files\n        merged_json_file (string): A JSON file with individual post content (from get_biostars_dataset()) merged as a JSON array of objects can be provided\n\n    Returns:\n        Nothing. Content is saved to the file: biostars_qa.parquet\n    \"\"\"\n\n    # GET ALL ENTRIES ----\n    # Merge individual files\n    if merged_json_file is None:\n        json_files = [file for file in os.listdir(folder) if file.endswith(\".json\")]\n\n        all_entries = []\n\n        for file in tqdm(json_files, desc=\"Get All Entries\"):\n            with open(os.path.join(folder, file), \"r\") as f:\n                data = json.load(f)\n                all_entries.append(data)\n\n        with open(merged_json_file, \"w\") as f:\n            json.dump(all_entries, f, indent=2)\n\n    df = pd.read_json(merged_json_file)\n\n    # GET QUESTIONS ----\n    questions_df = df[(df[\"has_accepted\"]) & (df[\"vote_count\"] > 0) & (df[\"type\"] == \"Question\")]\n\n    # GET ANSWERS ----\n    answers_df = df[(df[\"has_accepted\"]) & (df[\"vote_count\"] > 0) & (df[\"type\"] == \"Answer\")]\n\n    # GET MATCHED QUESTIONS/ANSWERS ----\n    matched_uids = []\n\n    for input_str in tqdm(answers_df[\"url\"], desc=\"Find Matched Answers\"):\n        # extract the question and answer IDs using regular expressions\n        match_obj = re.match(r\"https://www.biostars.org/p/(\\d+)/#(\\d+)\", input_str)\n        question_id = match_obj.group(1)\n        answer_id = match_obj.group(2)\n\n        # create a dictionary with the question and answer IDs and add it to the output list\n        output_dict = {\"question\": question_id, \"answer\": answer_id}\n        matched_uids.append(output_dict)\n\n    # GET MATCHED QUESTIONS/ANSWERS ----\n    matched_qa = []\n\n    for match in tqdm(matched_uids, desc=\"Get Matched Answers\"):\n        entry = {}\n\n        # match = {'question': '477589', 'answer': '477883'}\n\n        entry_obj = questions_df[questions_df[\"uid\"] == int(match[\"question\"])]\n        if entry_obj.empty:\n            continue\n        entry_dict = entry_obj.iloc[0].to_dict()\n\n        entry[\"INSTRUCTION\"] = entry_dict[\"content\"]\n        entry[\"SOURCE\"] = \"biostars\"\n        entry[\n            \"METADATA\"\n        ] = f'{{\"uid\": {entry_dict[\"uid\"]}, \"view_count\": {entry_dict[\"view_count\"]}, \"vote_count\": {entry_dict[\"vote_count\"]}}}'\n\n        entry_obj = answers_df[answers_df[\"uid\"] == int(match[\"answer\"])]\n        entry_dict = entry_obj.iloc[0].to_dict()\n        entry[\"RESPONSE\"] = entry_dict[\"content\"]\n\n        # sorted_entry = dict(sorted(entry.items(), key=lambda x: x[0] != \"INSTRUCTION\"))\n        sorted_entry = {k: entry[k] for k in [\"INSTRUCTION\", \"RESPONSE\", \"SOURCE\", \"METADATA\"]}\n        matched_qa.append(sorted_entry)\n\n    with open(\"matched_biostars_qa.json\", \"w\") as f:\n        json.dump(matched_qa, f, indent=2)\n\n    len(matched_qa)\n\n    # Read filtered JSON and convert to parquet format\n    tmp = pd.read_json(\"matched_biostars_qa.json\")  # or any other way\n    tmp.to_parquet(\"biostars_qa.parquet\", row_group_size=100, engine=\"pyarrow\")\n\n\nif __name__ == \"__main__\":\n    get_biostars_dataset()\n    extract_accepted_data()\n\n    print(\"DONE\")\n", "data/datasets/tatoeba_mt_qna_oa/language_translate.py": "random_templates_translate = {\n    \"en\": {\n        1: \"Translate {opening_quote}{text}{closing_quote} to {lang2} from {lang1}\",\n        2: \"Can you translate this {opening_quote}{text}{closing_quote}?\",\n        3: \"What does it mean {opening_quote}{text}{closing_quote}?\",\n        4: \"What it'd be in {lang2}: {opening_quote}{text}{closing_quote}?\",\n        5: \"How can I translate {opening_quote}{text}{closing_quote} from {lang1}\",\n        6: \"How can you translate this {opening_quote}{text}{closing_quote}?\",\n        7: \"{lang1}-{lang2}: {opening_quote}{text}{closing_quote}\",\n    },\n    \"ru\": {\n        1: \"\u041f\u0435\u0440\u0435\u0432\u0435\u0434\u0438 \u0434\u043b\u044f \u043c\u0435\u043d\u044f {opening_quote}{text}{closing_quote} \u0441 \u044d\u0442\u043e\u0433\u043e \u044f\u0437\u044b\u043a\u0430: {lang1}\",\n        2: \"\u041a\u0430\u043a \u0431\u0443\u0434\u0435\u0442 {opening_quote}{text}{closing_quote} \u043d\u0430 \u044f\u0437\u044b\u043a\u0435: {lang2}?\",\n        3: \"\u041a\u0430\u043a \u043f\u0435\u0440\u0435\u0432\u0435\u0441\u0442\u0438 \u043d\u0430 {lang2}: {opening_quote}{text}{closing_quote}?\",\n        4: \"\u0427\u0442\u043e \u043e\u0437\u043d\u0430\u0447\u0430\u0435\u0442 {opening_quote}{text}{closing_quote}? \u041f\u0435\u0440\u0435\u0432\u0435\u0434\u0438\",\n        5: \"\u041a\u0430\u043a \u043f\u043e\u043d\u044f\u0442\u044c {opening_quote}{text}{closing_quote}?\",\n        6: \"\u0422\u044b \u043c\u043e\u0436\u0435\u0448\u044c \u043f\u0435\u0440\u0435\u0432\u0435\u0441\u0442\u0438 \u044d\u0442\u043e \u0432\u044b\u0440\u0430\u0436\u0435\u043d\u0438\u0435: {opening_quote}{text}{closing_quote}?\",\n        7: \"\u041f\u0435\u0440\u0435\u0432\u0435\u0434\u0438 \u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0438\u0439 \u0442\u0435\u043a\u0441\u0442 {opening_quote}{text}{closing_quote} \u0441 \u044f\u0437\u044b\u043a\u0430 {opening_quote}{lang1}{closing_quote}\",\n        8: \"\u041a\u0430\u043a \u043c\u043e\u0436\u043d\u043e \u043f\u0435\u0440\u0435\u0432\u0435\u0441\u0442\u0438 {opening_quote}{text}{closing_quote}?\",\n        9: \"{lang1}-{lang2}: {opening_quote}{text}{closing_quote}\",\n    },\n    \"de\": {\n        1: \"\u00dcbersetze {opening_quote}{text}{closing_quote} in {lang2}\",\n        2: \"K\u00f6nntest du {opening_quote}{text}{closing_quote} \u00fcbersetzen?\",\n        3: \"\u00dcbersetze {opening_quote}{text}{closing_quote}\",\n        4: \"\u00dcbersetze {opening_quote}{text}{closing_quote} von {lang1} nach {lang2}\",\n        5: \"\u00dcbersetze {opening_quote}{text}{closing_quote} in {lang2}\",\n        6: \"Wie sagt man {opening_quote}{text}{closing_quote} auf {lang2}?\",\n        7: \"Was ist die korrekte \u00dcbersetzung f\u00fcr {opening_quote}{text}{closing_quote}?\",\n        8: \"Wie schreibt man {opening_quote}{text}{closing_quote} auf {lang2}\",\n        9: \"Wie w\u00fcrde man {opening_quote}{text}{closing_quote} auf {lang2} ausdr\u00fccken?\",\n        10: \"{lang1}-{lang2}: {opening_quote}{text}{closing_quote}\",\n    },\n    \"uk\": {\n        1: \"\u041f\u0435\u0440\u0435\u043a\u043b\u0430\u0434\u0456\u0442\u044c \u0434\u043b\u044f \u043c\u0435\u043d\u0435 \u0442\u0435\u043a\u0441\u0442 {opening_quote}{text}{closing_quote} \u043d\u0430 {lang1}.\",\n        2: \"\u042f\u043a \u043f\u0435\u0440\u0435\u043a\u043b\u0430\u0441\u0442\u0438 \u0442\u0435\u043a\u0441\u0442 {opening_quote}{text}{closing_quote} \u043d\u0430 {lang2}?\",\n        3: \"\u042f\u043a \u043f\u0435\u0440\u0435\u043a\u043b\u0430\u0441\u0442\u0438 \u043d\u0430 {lang2} \u0432\u0438\u0441\u043b\u0456\u0432 {opening_quote}{text}{closing_quote}?\",\n        4: \"\u0429\u043e \u043e\u0437\u043d\u0430\u0447\u0430\u0454 \u0432\u0438\u0441\u043b\u0456\u0432 {opening_quote}{text}{closing_quote}? \u0411\u0443\u0434\u044c \u043b\u0430\u0441\u043a\u0430, \u043f\u0435\u0440\u0435\u043a\u043b\u0430\u0434\u0456\u0442\u044c.\",\n        5: \"\u042f\u043a \u0437\u0440\u043e\u0437\u0443\u043c\u0456\u0442\u0438 \u0432\u0438\u0441\u043b\u0456\u0432 {opening_quote}{text}{closing_quote}?\",\n        6: \"\u0427\u0438 \u043c\u043e\u0436\u0435\u0442\u0435 \u0432\u0438 \u043f\u0435\u0440\u0435\u043a\u043b\u0430\u0441\u0442\u0438 \u043d\u0430\u0441\u0442\u0443\u043f\u043d\u0438\u0439 \u0432\u0438\u0441\u043b\u0456\u0432: {opening_quote}{text}{closing_quote}?\",\n        7: \"\u041f\u0435\u0440\u0435\u043a\u043b\u0430\u0434\u0456\u0442\u044c \u043d\u0430 {lang2} \u043d\u0430\u0441\u0442\u0443\u043f\u043d\u0438\u0439 \u0442\u0435\u043a\u0441\u0442: {opening_quote}{text}{closing_quote}, \u044f\u043a\u0438\u0439 \u043f\u043e\u0445\u043e\u0434\u0438\u0442\u044c \u0437 \u043c\u043e\u0432\u0438 {lang1}.\",\n        8: \"\u042f\u043a \u043f\u0440\u0430\u0432\u0438\u043b\u044c\u043d\u043e \u043f\u0435\u0440\u0435\u043a\u043b\u0430\u0441\u0442\u0438 \u0432\u0438\u0441\u043b\u0456\u0432 {opening_quote}{text}{closing_quote}?\",\n        9: \"\u0411\u0443\u0434\u044c \u043b\u0430\u0441\u043a\u0430, \u043f\u0435\u0440\u0435\u043a\u043b\u0430\u0434\u0456\u0442\u044c \u0432\u0438\u0441\u043b\u0456\u0432 {opening_quote}{text}{closing_quote} \u043d\u0430 {lang2}.\",\n        10: \"\u0427\u0438 \u043c\u043e\u0436\u0435\u0442\u0435 \u0432\u0438 \u043f\u0435\u0440\u0435\u043a\u043b\u0430\u0441\u0442\u0438 \u043d\u0430\u0441\u0442\u0443\u043f\u043d\u0438\u0439 \u0432\u0438\u0441\u043b\u0456\u0432: {opening_quote}{text}{closing_quote}?\",\n        11: \"\u041f\u0435\u0440\u0435\u043a\u043b\u0430\u0434\u0456\u0442\u044c {opening_quote}{text}{closing_quote}, \u0449\u043e \u043d\u0430\u043f\u0438\u0441\u0430\u043d\u0438\u0439 \u043c\u043e\u0432\u043e\u044e {lang1}.\",\n    },\n    \"bg\": {\n        1: \"\u041f\u0440\u0435\u0432\u0435\u0434\u0438 \u0442\u043e\u0437\u0438 \u0442\u0435\u043a\u0441\u0442: {opening_quote}{text}{closing_quote} \u043d\u0430 {lang2}\",\n        2: \"\u041a\u0430\u043a \u0441\u0435 \u043f\u0440\u0435\u0432\u0435\u0436\u0434\u0430 \u043d\u0430 {lang2} \u0438\u0437\u0440\u0430\u0437\u044a\u0442: {opening_quote}{text}{closing_quote}?\",\n        3: \"\u0418\u0441\u043a\u0430\u043c \u0434\u0430 \u043f\u0440\u0435\u0432\u0435\u0434\u0430 \u0442\u043e\u0437\u0438 \u0442\u0435\u043a\u0441\u0442: {opening_quote}{text}{closing_quote} \u043d\u0430 {lang2}. \u041c\u043e\u0436\u0435\u0442\u0435 \u043b\u0438 \u0434\u0430 \u043c\u0438 \u043f\u043e\u043c\u043e\u0433\u043d\u0435\u0442\u0435?\",\n        4: \"\u041c\u043e\u043b\u044f, \u043f\u0440\u0435\u0432\u0435\u0434\u0435\u0442\u0435 \u0442\u043e\u0432\u0430 \u0438\u0437\u0440\u0435\u0447\u0435\u043d\u0438\u0435: {opening_quote}{text}{closing_quote} \u043d\u0430 {lang2}\",\n        5: \"\u041a\u0430\u043a\u0432\u0430 \u0435 \u043f\u0440\u0435\u0432\u043e\u0434\u0430 \u043d\u0430 {opening_quote}{text}{closing_quote} \u043d\u0430 {lang2}?\",\n        6: \"\u041c\u043e\u0433\u0430 \u043b\u0438 \u0434\u0430 \u043f\u043e\u043b\u0443\u0447\u0430 \u043f\u0440\u0435\u0432\u043e\u0434 \u043d\u0430 \u0442\u043e\u0432\u0430: {opening_quote}{text}{closing_quote} \u043d\u0430 {lang2}?\",\n        7: \"\u041c\u043e\u043b\u044f, \u043f\u0440\u0435\u0432\u0435\u0434\u0435\u0442\u0435 {opening_quote}{text}{closing_quote} \u043d\u0430 {lang2} \u043e\u0442 {lang1}\",\n        8: \"\u041a\u0430\u043a \u0434\u0430 \u043f\u0440\u0435\u0432\u0435\u0434\u0430 {opening_quote}{text}{closing_quote}?\",\n        9: \"\u0422\u044a\u0440\u0441\u044f \u043f\u0440\u0435\u0432\u043e\u0434 \u043d\u0430 {opening_quote}{text}{closing_quote} \u043d\u0430 {lang2}\",\n        10: \"\u0418\u0441\u043a\u0430\u043c \u0434\u0430 \u0437\u043d\u0430\u044f \u043a\u0430\u043a \u0434\u0430 \u043f\u0440\u0435\u0432\u0435\u0434\u0430 \u043d\u0430 {lang2} \u0442\u043e\u0437\u0438 \u0442\u0435\u043a\u0441\u0442: {opening_quote}{text}{closing_quote}\",\n        11: \"{lang1}-{lang2}: {opening_quote}{text}{closing_quote}\",\n    },\n    \"ca\": {\n        1: \"Com puc traduir {opening_quote}{text}{closing_quote}?\",\n        2: \"Tradueix per mi a {lang2} aquest text: {opening_quote}{text}{closing_quote}\",\n        3: \"Qu\u00e8 significa {opening_quote}{text}{closing_quote} en {lang2}?\",\n        4: \"Com es diu {opening_quote}{text}{closing_quote} en {lang2}?\",\n        5: \"Podries ajudar-me a traduir {opening_quote}{text}{closing_quote}?\",\n        6: \"Necessito una traducci\u00f3 per {opening_quote}{text}{closing_quote} a {lang2}\",\n        7: \"Si us plau, tradueix {opening_quote}{text}{closing_quote} a {lang2} des de {lang1}\",\n        8: \"Com traduir {opening_quote}{text}{closing_quote} a {lang2} de manera correcta?\",\n        9: \"Demano ajuda per traduir {opening_quote}{text}{closing_quote} a {lang2}\",\n        10: \"Quin \u00e9s el significat de {opening_quote}{text}{closing_quote} en {lang2}?\",\n        11: \"{lang1}-{lang2}: {opening_quote}{text}{closing_quote}\",\n    },\n    \"cs\": {\n        1: \"Jak p\u0159elo\u017eit {opening_quote}{text}{closing_quote}?\",\n        2: \"Pros\u00edm, pomoz mi p\u0159elo\u017eit {opening_quote}{text}{closing_quote} do {lang2}\",\n        3: \"Co znamen\u00e1 {opening_quote}{text}{closing_quote} v {lang2}?\",\n        4: \"Jak se \u0159ekne {opening_quote}{text}{closing_quote} v {lang2}?\",\n        5: \"Pot\u0159ebuji p\u0159elo\u017eit {opening_quote}{text}{closing_quote} do {lang2}, m\u016f\u017ee\u0161 mi pomoci?\",\n        6: \"R\u00e1d bych {opening_quote}{text}{closing_quote} p\u0159elo\u017eil do {lang2}. M\u016f\u017ee\u0161 mi poradit?\",\n        7: \"Pros\u00edm, p\u0159elo\u017e {opening_quote}{text}{closing_quote} do {lang2} z {lang1}\",\n        8: \"Jak spr\u00e1vn\u011b p\u0159elo\u017eit {opening_quote}{text}{closing_quote}?\",\n        9: \"Pot\u0159ebuji p\u0159elo\u017eit tento text: {opening_quote}{text}{closing_quote} do {lang2}\",\n        10: \"Co je v\u00fdznam {opening_quote}{text}{closing_quote} v {lang2}?\",\n        11: \"{lang1}-{lang2}: {opening_quote}{text}{closing_quote}\",\n    },\n    \"da\": {\n        1: \"Overs\u00e6t {opening_quote}{text}{closing_quote} til {lang2} fra {lang1}\",\n        2: \"Kan du overs\u00e6tte dette {opening_quote}{text}{closing_quote}?\",\n        3: \"Hvad betyder {opening_quote}{text}{closing_quote} p\u00e5 {lang2}?\",\n        4: \"Kan du hj\u00e6lpe med at overs\u00e6tte {opening_quote}{text}{closing_quote} til {lang2}?\",\n        5: \"Hvordan overs\u00e6tter man {opening_quote}{text}{closing_quote} til {lang2}?\",\n        6: \"Kan du give mig en overs\u00e6ttelse af {opening_quote}{text}{closing_quote} p\u00e5 {lang2}?\",\n        7: \"Hvad er overs\u00e6ttelsen af {opening_quote}{text}{closing_quote} til {lang2}?\",\n        8: \"Jeg forst\u00e5r ikke {opening_quote}{text}{closing_quote}, kan du overs\u00e6tte det til {lang2}?\",\n        9: \"Kan du overs\u00e6tte {opening_quote}{text}{closing_quote} til {lang2} for mig?\",\n        10: \"Hvad er den korrekte overs\u00e6ttelse af {opening_quote}{text}{closing_quote}?\",\n        11: \"{lang1}-{lang2}: {opening_quote}{text}{closing_quote}\",\n    },\n    \"es\": {\n        1: \"Traduce {opening_quote}{text}{closing_quote} a {lang2} desde {lang1}\",\n        2: \"\u00bfPuedes traducir esto {opening_quote}{text}{closing_quote}?\",\n        3: \"\u00bfC\u00f3mo se dice {opening_quote}{text}{closing_quote} en {lang2}?\",\n        4: \"Necesito una traducci\u00f3n de {opening_quote}{text}{closing_quote} a {lang2}\",\n        5: \"\u00bfMe puedes ayudar a traducir {opening_quote}{text}{closing_quote} a {lang2}?\",\n        6: \"Quiero saber la traducci\u00f3n de {opening_quote}{text}{closing_quote}\",\n        7: \"\u00bfCu\u00e1l es la traducci\u00f3n correcta de {opening_quote}{text}{closing_quote} a {lang2}?\",\n        8: \"\u00bfQu\u00e9 significa {opening_quote}{text}{closing_quote} en {lang2}? Trad\u00facelo por favor\",\n        9: \"\u00bfPuedes proporcionarme una traducci\u00f3n de {opening_quote}{text}{closing_quote} a {lang2}?\",\n        10: \"\u00bfC\u00f3mo se traduce {opening_quote}{text}{closing_quote} al idioma {lang2}?\",\n        11: \"{lang1}-{lang2}: {opening_quote}{text}{closing_quote}\",\n    },\n    \"fr\": {\n        1: \"Traduis {opening_quote}{text}{closing_quote}\",\n        2: \"Peux-tu traduire ceci {opening_quote}{text}{closing_quote}?\",\n        3: \"Comment traduire {opening_quote}{text}{closing_quote} en {lang2}?\",\n        4: \"Je ne comprends pas {opening_quote}{text}{closing_quote}. Peux-tu le traduire en {lang2}?\",\n        5: \"J'ai besoin d'une traduction de {opening_quote}{text}{closing_quote} en {lang2}\",\n        6: \"Saurais-tu traduire {opening_quote}{text}{closing_quote}?\",\n        7: \"Quelle est la traduction correcte de {opening_quote}{text}{closing_quote} en {lang2}?\",\n        8: \"Pourrais-tu m'aider \u00e0 traduire {opening_quote}{opening_quote}{closing_quote} en {lang2}?\",\n        9: \"Que veut dire {opening_quote}{text}{closing_quote} en {lang2}?\",\n        10: \"Est-ce que tu peux me donner la traduction de {opening_quote}{text}{closing_quote}?\",\n        11: \"Traduis {opening_quote}{text}{closing_quote} en {lang2} \u00e0 partir de {lang1}\",\n        12: \"Peux tu traduire {opening_quote}{text}{closing_quote} en {lang2} \u00e0 partir de {lang1}\",\n        13: \"Traduis ce texte \u00e0 partir de {lang1} vers {lang2} : {opening_quote}{text}{closing_quote}\",\n        14: \"{opening_quote}{text}{closing_quote}. Traduis-moi le texte en {lang2}\",\n        15: \"Pouvez-vous traduire cette phrase {opening_quote}{text}{closing_quote} ?\",\n        16: \"{lang1}-{lang2}: {opening_quote}{text}{closing_quote}\",\n        17: \"Transforme cette phrase en {lang1} vers {lang2}: {opening_quote}{text}{closing_quote}\",\n    },\n    \"hr\": {\n        1: \"Prevedi {opening_quote}{text}{closing_quote} na {lang2} iz {lang1}\",\n        2: \"Mo\u017ee\u0161 li prevesti ovo {opening_quote}{text}{closing_quote}?\",\n        3: \"Kako prevesti {opening_quote}{text}{closing_quote} na {lang2}?\",\n        4: \"Nisam siguran \u0161to zna\u010di {opening_quote}{text}{closing_quote}. Mo\u017ee\u0161 li mi prevesti na {lang2}?\",\n        5: \"Molim te, prevedi {opening_quote}{text}{closing_quote} na {lang2}\",\n        6: \"Kako se ka\u017ee {opening_quote}{text}{closing_quote}?\",\n        7: \"Tra\u017eim prijevod za {opening_quote}{text}{closing_quote} na {lang2}\",\n        8: \"Jesi li u mogu\u0107nosti prevesti {opening_quote}{text}{closing_quote} na {lang2}?\",\n        9: \"Koji je to\u010dan prijevod za {opening_quote}{text}{closing_quote} na {lang2}?\",\n        10: \"Mo\u017ee\u0161 li mi dati prijevod za {opening_quote}{text}{closing_quote} na {lang2}?\",\n        11: \"{lang1}-{lang2}: {opening_quote}{text}{closing_quote}\",\n    },\n    \"hu\": {\n        1: \"Ford\u00edtsd le magyar nyelvr\u0151l {lang2} nyelvre: {opening_quote}{text}{closing_quote}\",\n        2: \"Hogy mondan\u00e1d {lang2} nyelven, hogy {opening_quote}{text}{closing_quote}?\",\n        3: \"K\u00e9rlek ford\u00edtsd le magyarra: {text}\",\n        4: \"Mit jelent, hogy {opening_quote}{text}{closing_quote}? Ford\u00edtsd le l\u00e9gy sz\u00edves!\",\n        5: \"Hogyan mondod magyarul, hogy {opening_quote}{text}{closing_quote}?\",\n        6: \"Ford\u00edtsd magyarra {lang1} nyelvr\u0151l: {text}\",\n        7: \"Mit jelent magyarul az a kifejez\u00e9s, hogy {opening_quote}{text}{closing_quote}?\",\n        8: \"Mondd magyarul: {opening_quote}{text}{closing_quote}\",\n        9: \"Ford\u00edtsd le, hogy {opening_quote}{text}{closing_quote}\",\n        10: \"Mondd magyarul, hogy {opening_quote}{text}{closing_quote}\",\n        11: \"Mit jelent {lang2} nyelven: {text}\",\n        12: \"{lang1}-{lang2}: {opening_quote}{text}{closing_quote}\",\n    },\n    \"it\": {\n        1: \"Traduci {opening_quote}{text}{closing_quote} in {lang2} dalla lingua {lang1}\",\n        2: \"Mi puoi tradurre questo {opening_quote}{text}{closing_quote}?\",\n        3: \"Cosa significa {opening_quote}{text}{closing_quote}? Puoi tradurlo?\",\n        4: \"Come si dice {opening_quote}{text}{closing_quote} in {lang1}?\",\n        5: \"Posso avere una traduzione per {opening_quote}{text}{closing_quote} in {lang2}?\",\n        6: \"Ti va di tradurre questo {opening_quote}{text}{closing_quote}?\",\n        7: \"Voglio tradurre {opening_quote}{text}{closing_quote} in {lang2}. Puoi aiutarmi?\",\n        8: \"{lang1}-{lang2}: {opening_quote}{text}{closing_quote}\",\n    },\n    \"nl\": {\n        1: \"Vertaal {opening_quote}{text}{closing_quote} naar {lang2}\",\n        2: \"Kun je dit vertalen {opening_quote}{text}{closing_quote}?\",\n        3: \"Vertaal {opening_quote}{text}{closing_quote} van {lang1} naar {lang2}\",\n        4: \"Wat is {opening_quote}{text}{closing_quote} in het Nederlands?\",\n        5: \"Wat betekent {opening_quote}{text}{closing_quote} in het {lang2}?\",\n        6: \"Hoe zeg je {opening_quote}{text}{closing_quote} in het {lang2}?\",\n        7: \"Hoe zeg je {opening_quote}{text}{closing_quote} in het {lang2}\",\n        8: \"Vertaal {opening_quote}{text}{closing_quote}\",\n        9: \"Vertaal {opening_quote}{text}{closing_quote} alstublieft\",\n        10: \"Vertaal alstublieft {opening_quote}{text}{closing_quote} naar {lang2}\",\n        11: \"Hoe schrijf je {opening_quote}{text}{closing_quote} in het {lang2}\",\n        12: \"{lang1}-{lang2}: {opening_quote}{text}{closing_quote}\",\n    },\n    \"pl\": {\n        1: \"Przet\u0142umacz {opening_quote}{text}{closing_quote} na j\u0119zyk {lang2} z {lang1}\",\n        2: \"Jak przet\u0142umaczy\u0107 {opening_quote}{text}{closing_quote} na {lang2}?\",\n        3: \"Co oznacza s\u0142owo {opening_quote}{text}{closing_quote}? Przet\u0142umacz na {lang2}, prosz\u0119.\",\n        4: \"Prosz\u0119 o t\u0142umaczenie {opening_quote}{text}{closing_quote} na j\u0119zyk {lang2}\",\n        5: \"Czy m\u00f3g\u0142by\u015b mi pom\u00f3c przet\u0142umaczy\u0107 {opening_quote}{text}{closing_quote}?\",\n        6: \"Potrzebuj\u0119 t\u0142umaczenia na {lang2} dla s\u0142owa {opening_quote}{text}{closing_quote}.\",\n        7: \"Przet\u0142umacz to zdanie: {opening_quote}{text}{closing_quote} na {lang2} z j\u0119zyka {lang1}\",\n        8: \"Mo\u017cesz mi poda\u0107 t\u0142umaczenie na {lang2} dla wyra\u017cenia {opening_quote}{text}{closing_quote}?\",\n        9: \"{lang1}-{lang2}: {opening_quote}{text}{closing_quote}\",\n        10: \"Przet\u0142umacz {opening_quote}{text}{closing_quote} z {lang1} na {lang2}.\",\n        11: \"Czy mo\u017cesz przet\u0142umaczy\u0107 {opening_quote}{text}{closing_quote}?\",\n        12: \"Co znaczy ten tekst {opening_quote}{text}{closing_quote}?\",\n        13: \"Jak to napisa\u0107 {opening_quote}{text}{closing_quote} po {lang2}?\",\n        14: \"Jak przet\u0142umaczy\u0107 {opening_quote}{text}{closing_quote} na {lang2}?\",\n    },\n    \"pt\": {\n        1: \"Traduza {opening_quote}{text}{closing_quote} para {lang2} a partir do {lang1}\",\n        2: \"Voc\u00ea pode traduzir {opening_quote}{text}{closing_quote}?\",\n        3: \"Por favor, me traduza {opening_quote}{text}{closing_quote} para o {lang2}.\",\n        4: \"Qual \u00e9 a tradu\u00e7\u00e3o de {opening_quote}{text}{closing_quote} para o {lang2}?\",\n        5: \"Eu preciso de uma tradu\u00e7\u00e3o para {opening_quote}{text}{closing_quote} para o {lang2}.\",\n        6: \"Como se diz {opening_quote}{text}{closing_quote} em {lang2}? Pode traduzir?\",\n        7: \"Voc\u00ea poderia me ajudar a traduzir {opening_quote}{text}{closing_quote} para o {lang2}?\",\n        8: \"Gostaria de saber a tradu\u00e7\u00e3o de {opening_quote}{text}{closing_quote} para o {lang2}.\",\n        9: \"{lang1}-{lang2}: {opening_quote}{text}{closing_quote}\",\n    },\n    \"ro\": {\n        1: \"Tradu {opening_quote}{text}{closing_quote} \u00een {lang2}\",\n        2: \"Po\u021bi s\u0103 traduci asta {opening_quote}{text}{closing_quote}?\",\n        3: \"Care este traducerea pentru {opening_quote}{text}{closing_quote} \u00een {lang2}?\",\n        4: \"A\u0219tept de la tine s\u0103 traduci acest {opening_quote}{text}{closing_quote} \u00een {lang2}\",\n        5: \"Cum pot traduce {opening_quote}{text}{closing_quote} \u00een {lang2}?\",\n        6: \"Te rog s\u0103 \u00eemi traduci {opening_quote}{text}{closing_quote} \u00een {lang2}\",\n        7: \"Tradu expresia {opening_quote}{text}{closing_quote} \u00een {lang2}\",\n        8: \"Vreau s\u0103 \u0219tiu cum se traduce {opening_quote}{text}{closing_quote} \u00een {lang2}\",\n        9: \"Care este echivalentul {opening_quote}{text}{closing_quote} \u00een {lang2}?\",\n        10: \"Te rog s\u0103 \u00eemi explici sensul cuv\u00e2ntului {opening_quote}{text}{closing_quote} \u00een {lang2}\",\n        11: \"{lang1}-{lang2}: {opening_quote}{text}{closing_quote}\",\n    },\n    \"sl\": {\n        1: \"Prevedi {opening_quote}{text}{closing_quote} v {lang2}\",\n        2: \"Lahko prevede\u0161 to {opening_quote}{text}{closing_quote}?\",\n        3: \"Kaj pomeni {opening_quote}{text}{closing_quote} v jeziku {lang2}?\",\n        4: \"Prosim, prevedi {opening_quote}{text}{closing_quote} v {lang2}\",\n        5: \"Kako lahko prevedem {opening_quote}{text}{closing_quote} v {lang2}?\",\n        6: \"Ali lahko prevede\u0161 izraz {opening_quote}{text}{closing_quote} v {lang2}?\",\n        7: \"V {lang2} prevedi besedilo {opening_quote}{text}{closing_quote}\",\n        8: \"Prosim, naj mi nekdo prevede {opening_quote}{text}{closing_quote} v {lang2}\",\n        9: \"Kak\u0161en je prevod za besedo {opening_quote}{text}{closing_quote} v {lang2}?\",\n        10: \"Prosim, razlo\u017ei mi pomen besede {opening_quote}{text}{closing_quote} v {lang2}\",\n        11: \"{lang1}-{lang2}: {opening_quote}{text}{closing_quote}\",\n        12: \"Prevedite {opening_quote}{text}{closing_quote} v {lang2} iz {lang1}\",\n        13: \"Ali lahko prevedete {opening_quote}{text}{closing_quote}?\",\n        14: \"Kaj pomeni {opening_quote}{text}{closing_quote}?\",\n        15: \"Kaj bi bilo v {lang2}: {opening_quote}{text}{closing_quote}?\",\n        16: \"Kako lahko prevedete {opening_quote}{text}{closing_quote} iz {lang1}\",\n        17: \"Kako lahko prevedete {opening_quote}{text}{closing_quote}?\",\n    },\n    \"sr\": {\n        1: \"\u041f\u0440\u0435\u0432\u0435\u0434\u0438 {opening_quote}{text}{closing_quote} \u0443 {lang2}\",\n        2: \"\u041c\u043e\u0436\u0435\u0448 \u043b\u0438 \u0434\u0430 \u043f\u0440\u0435\u0432\u0435\u0434\u0435\u0448 \u043e\u0432\u043e {opening_quote}{text}{closing_quote}?\",\n        3: \"\u041a\u0430\u043a\u043e \u0441\u0435 \u043f\u0440\u0435\u0432\u043e\u0434\u0438 {opening_quote}{text}{closing_quote} \u043d\u0430 {lang2}?\",\n        4: \"\u041c\u043e\u043b\u0438\u043c \u0442\u0435, \u043f\u0440\u0435\u0432\u0435\u0434\u0438 {opening_quote}{text}{closing_quote} \u043d\u0430 {lang2}\",\n        5: \"\u041a\u0430\u043a\u043e \u043c\u043e\u0433\u0443 \u0434\u0430 \u043f\u0440\u0435\u0432\u0435\u0434\u0435\u043c {opening_quote}{text}{closing_quote} \u043d\u0430 {lang2}?\",\n        6: \"\u041c\u043e\u0436\u0435\u0448 \u043b\u0438 \u0434\u0430 \u043f\u0440\u0435\u0432\u0435\u0434\u0435\u0448 \u0438\u0437\u0440\u0430\u0437 {opening_quote}{text}{closing_quote} \u043d\u0430 {lang2}?\",\n        7: \"\u041f\u0440\u0435\u0432\u0435\u0434\u0438 \u0442\u0435\u043a\u0441\u0442 {opening_quote}{text}{closing_quote} \u0441\u0430 {lang1} \u043d\u0430 {lang2}\",\n        8: \"\u041c\u043e\u043b\u0438\u043c \u0442\u0435, \u043f\u0440\u0435\u0432\u0435\u0434\u0438 {opening_quote}{text}{closing_quote}\",\n        9: \"\u041a\u0430\u043a\u043e \u0441\u0435 \u043f\u0440\u0435\u0432\u043e\u0434\u0438 \u0440\u0435\u0447 {opening_quote}{text}{closing_quote} \u043d\u0430 {lang2}?\",\n        10: \"\u041e\u0431\u0458\u0430\u0441\u043d\u0438 \u043c\u0438 \u0448\u0442\u0430 \u0437\u043d\u0430\u0447\u0438 \u0440\u0435\u0447 {opening_quote}{text}{closing_quote}?\",\n        11: \"{lang1}-{lang2}: {opening_quote}{text}{closing_quote}\",\n    },\n    \"sv\": {\n        1: \"\u00d6vers\u00e4tt {opening_quote}{text}{closing_quote}\",\n        2: \"Kan du \u00f6vers\u00e4tta detta {opening_quote}{text}{closing_quote}?\",\n        3: \"Vad betyder {opening_quote}{text}{closing_quote} p\u00e5 {lang2}?\",\n        4: \"Jag vill ha en \u00f6vers\u00e4ttning av {opening_quote}{text}{closing_quote} till {lang2}\",\n        5: \"Hur kan jag \u00f6vers\u00e4tta {opening_quote}{text}{closing_quote}?\",\n        6: \"Kan du \u00f6vers\u00e4tta uttrycket {opening_quote}{text}{closing_quote} till {lang2}?\",\n        7: \"\u00d6vers\u00e4tt texten {opening_quote}{text}{closing_quote} till {lang2}\",\n        8: \"Jag beh\u00f6ver hj\u00e4lp med att \u00f6vers\u00e4tta {opening_quote}{text}{closing_quote}\",\n        9: \"Vad \u00e4r \u00f6vers\u00e4ttningen av ordet {opening_quote}{text}{closing_quote} p\u00e5 {lang2}?\",\n        10: \"Kan du f\u00f6rklara inneb\u00f6rden av {opening_quote}{text}{closing_quote} p\u00e5 {lang2}?\",\n        11: \"{lang1}-{lang2}: {opening_quote}{text}{closing_quote}\",\n    },\n}\n", "data/datasets/tatoeba_mt_qna_oa/data_process.py": "import json\nimport random\nimport uuid\nfrom dataclasses import dataclass\n\nimport datasets\nimport iso639\nimport language_names\nimport language_paraphrase\nimport language_translate\nimport pandas as pd\n\nrandom.seed(42)\n\n\nclass DataProcess:\n    # list of random quotes\n    random_quote = [(\"'\", \"'\"), (\"\u201c\", \"\u201d\"), (\"\u1fce\", \"\u1fcf\"), (\"`\", \"\u00b4\"), (\"\u00ab\", \"\u00bb\"), ('\"', '\"')]\n\n    # provide instruction with a text; process of randomization of a text\n    def randomize_text(self, text, original_lang=None, target_lang=None):\n        templates = (\n            language_translate.random_templates_translate.get(original_lang, {})\n            if not ((original_lang == target_lang) and (original_lang is not None) and (target_lang is not None))\n            else language_paraphrase.random_templates_paraphrase.get(original_lang, {})\n        )\n        template = random.choice(list(templates.values()))\n        quote_pair = random.choice(DataProcess().random_quote)\n        opening_quote, closing_quote = quote_pair\n        original_lang_name = DataProcess.language_name(None, original_lang, original_lang)\n        target_lang_name = DataProcess.language_name(None, target_lang, original_lang)\n        return template.format(\n            text=text,\n            lang1=target_lang_name,\n            lang2=original_lang_name,\n            opening_quote=opening_quote,\n            closing_quote=closing_quote,\n        )\n\n    # convert to iso639_1\n    def convert_code(self, code):\n        mapped_code = iso639.to_iso639_1(code)\n        return mapped_code\n\n    # return language #1 name in language #2\n    def language_name(self, lang1, lang2):\n        name = language_names.language_names.get(lang1, {}).get(lang2)\n        if name is not None:\n            return name\n        # just in case\n        elif lang1 == lang2:\n            iso_name = iso639.to_native(lang1)\n            return iso_name\n        else:\n            return None\n\n\nconverter = DataProcess()\n\n\"\"\"\nEXAMPLES:\n\n# get language name; iso639_1 code\nprint(converter.language_name('ru', 'en')) # Output: Russian\nprint(converter.convert_code(\"eng\")) # Output: en\n\n# convert into INSTRUCTION format: text; to; from\ntext = \"test\"\nprint(converter.randomize_text(text, \"uk\", \"fr\")) # \u0422\u0438 \u043c\u043e\u0436\u0435\u0448 \u043f\u0435\u0440\u0435\u043a\u043b\u0430\u0441\u0442\u0438 \u0446\u0435\u0439 \u0432\u0438\u0441\u043b\u0456\u0432: 'test'?\nprint(converter.randomize_text(text, \"uk\", \"de\")) # \u041f\u0435\u0440\u0435\u043a\u043b\u0430\u0434\u0438 \u043d\u0430\u0441\u0442\u0443\u043f\u043d\u0438\u0439 \u0442\u0435\u043a\u0441\u0442 \"test\" \u0437 \u043c\u043e\u0432\u0438 \"\u043d\u0456\u043c\u0435\u0446\u044c\u043a\u0430 \u043c\u043e\u0432\u0430\"\n\"\"\"\n\n\n@dataclass\nclass QnA:\n    INSTRUCTION: str\n    RESPONSE: str\n    SOURCE: str\n    METADATA: str\n\n\n# format to QnA\ndef create_qna(row):\n    # get rows; create uuid based on texts\n    text = row[\"Text\"]\n    text_length = len(text)\n    translation = row[\"Translated text\"]\n    lang_from = converter.convert_code(row[\"Original lang\"])\n    lang_to = converter.convert_code(row[\"Target lang\"])\n    uuid_val = uuid.uuid3(uuid.NAMESPACE_OID, str(text + translation))\n    # json with language, original text length, uuid and langs-pair\n    METADATA = {\n        \"language\": f\"{lang_to}\",\n        \"length\": f\"{text_length}\",\n        \"uuid\": f\"{uuid_val}\",\n        \"langs-pair\": f\"{lang_from}-{lang_to}\",\n    }\n    metadata_str = json.dumps(METADATA)\n    source = \"tatoeba\"\n    # randomizing INSTRUCTION\n    instruction = converter.randomize_text(text, lang_to, lang_from)\n    response = translation\n    return QnA(instruction, response, source, metadata_str)\n\n\n# load the dataset from Hugging Face\nhf_dataset = datasets.load_dataset(\"0x22almostEvil/tatoeba-mt-llama-only\", split=\"train\")\n\n# original is ~3M; with num_shards=30 it'll be ~120K\nhf_dataset = hf_dataset.shard(num_shards=30, index=0)\nprint(hf_dataset)\n\n# convert the dataset to a pandas dataframe\ndf = pd.DataFrame(hf_dataset)\n\n# apply the create_qna function to each row of the dataframe to create QnA objects\nqna_list = df.apply(create_qna, axis=1).tolist()\n\n# save the QnA objects as a parquet file\nqna_df = pd.DataFrame(qna_list, columns=[\"INSTRUCTION\", \"RESPONSE\", \"SOURCE\", \"METADATA\"])\nqna_df.to_parquet(\"translation-taboeba-qna-120k-oa.parquet\", row_group_size=100, engine=\"pyarrow\", index=False)\n", "data/datasets/tatoeba_mt_qna_oa/language_paraphrase.py": "random_templates_paraphrase = {\n    \"en\": {\n        1: \"Paraphrase {opening_quote}{text}{closing_quote}\",\n        2: \"Can you rewrite next: {opening_quote}{text}{closing_quote}?\",\n        3: \"Can you rephrase this sentence: {opening_quote}{text}{closing_quote}?\",\n        4: \"Reword the following statement: {opening_quote}{text}{closing_quote}.\",\n        5: \"How would you express {opening_quote}{text}{closing_quote} in a different way?\",\n        6: \"Could you put {opening_quote}{text}{closing_quote} into other words?\",\n        7: \"Alter this text: {opening_quote}{text}{closing_quote} to convey the same message in a different way.\",\n        8: \"Can you paraphrase {opening_quote}{text}{closing_quote} using different vocabulary?\",\n        9: \"Try restating {opening_quote}{text}{closing_quote} in your own words.\",\n        10: \"In what other way could you say {opening_quote}{text}{closing_quote}?\",\n        12: \"Use different phrasing to convey the same idea as {opening_quote}{text}{closing_quote}.\",\n        13: \"Please reword {opening_quote}{text}{closing_quote} so that it has a different meaning.\",\n    },\n    \"ru\": {\n        1: \"\u041f\u0435\u0440\u0435\u0444\u0440\u0430\u0437\u0438\u0440\u0443\u0439\u0442\u0435 \u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0438\u0439 \u0442\u0435\u043a\u0441\u0442: {opening_quote}{text}{closing_quote}\",\n        2: \"\u041c\u043e\u0436\u0435\u0442\u0435 \u043b\u0438 \u0432\u044b \u043f\u0435\u0440\u0435\u0444\u043e\u0440\u043c\u0443\u043b\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0438\u0439 \u0442\u0435\u043a\u0441\u0442: {opening_quote}{text}{closing_quote}?\",\n        3: \"\u041f\u0435\u0440\u0435\u0434\u0435\u043b\u0430\u0439 {opening_quote}{text}{closing_quote}\",\n        4: \"\u041c\u043e\u0436\u0435\u0448\u044c \u043a\u0430\u043a-\u0442\u043e \u043f\u0435\u0440\u0435\u0444\u0440\u0430\u0437\u0438\u0440\u043e\u0432\u0430\u0442\u044c {opening_quote}{text}{closing_quote}?\",\n        5: \"\u041f\u043e\u043f\u0440\u043e\u0431\u0443\u0439 \u043f\u0435\u0440\u0435\u0441\u043a\u0430\u0437\u0430\u0442\u044c {opening_quote}{text}{closing_quote} \u0441\u0432\u043e\u0438\u043c\u0438 \u0441\u043b\u043e\u0432\u0430\u043c\u0438\",\n        6: \"\u0418\u0437\u043c\u0435\u043d\u0438 \u0442\u0435\u043a\u0441\u0442 {opening_quote}{text}{closing_quote}, \u0447\u0442\u043e\u0431\u044b \u043e\u043d \u0431\u044b\u043b \u0434\u0440\u0443\u0433\u0438\u043c\",\n        7: \"\u041a\u0430\u043a \u0431\u044b \u0442\u044b \u043f\u0435\u0440\u0435\u0441\u043a\u0430\u0437\u0430\u043b {opening_quote}{text}{closing_quote}?\",\n        8: \"\u041a\u0430\u043a \u043c\u043e\u0436\u043d\u043e \u0434\u0440\u0443\u0433\u0438\u043c\u0438 \u0441\u043b\u043e\u0432\u0430\u043c\u0438 \u043d\u0430\u043f\u0438\u0441\u0430\u0442\u044c {opening_quote}{text}{closing_quote}?\",\n        9: \"\u041f\u043e\u043f\u0440\u043e\u0431\u0443\u0439 \u0441\u0432\u043e\u0438\u043c\u0438 \u0441\u043b\u043e\u0432\u0430\u043c\u0438 \u0441\u043a\u0430\u0437\u0430\u0442\u044c {opening_quote}{text}{closing_quote}\",\n        10: \"{opening_quote}{text}{closing_quote}. \u041f\u0435\u0440\u0435\u0441\u043a\u0430\u0436\u0438 \u044d\u0442\u043e\u0442 \u0442\u0435\u043a\u0441\u0442.\",\n        11: \"\u041f\u0435\u0440\u0435\u0444\u043e\u0440\u043c\u0443\u043b\u0438\u0440\u0443\u0439\u0442\u0435 \u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0435\u0435 \u043f\u0440\u0435\u0434\u043b\u043e\u0436\u0435\u043d\u0438\u0435: {opening_quote}{text}{closing_quote}\",\n        12: \"\u041c\u043e\u0436\u0435\u0442\u0435 \u043b\u0438 \u0432\u044b \u0432\u044b\u0440\u0430\u0437\u0438\u0442\u044c \u044d\u0442\u043e \u0438\u043d\u0430\u0447\u0435: {opening_quote}{text}{closing_quote}?\",\n        13: \"\u041a\u0430\u043a \u0431\u044b \u0432\u044b \u0432\u044b\u0440\u0430\u0437\u0438\u043b\u0438 {opening_quote}{text}{closing_quote} \u0434\u0440\u0443\u0433\u0438\u043c\u0438 \u0441\u043b\u043e\u0432\u0430\u043c\u0438?\",\n        14: \"\u041c\u043e\u0436\u0435\u0442\u0435 \u043b\u0438 \u0432\u044b \u043f\u0435\u0440\u0435\u0444\u043e\u0440\u043c\u0443\u043b\u0438\u0440\u043e\u0432\u0430\u0442\u044c {opening_quote}{text}{closing_quote} \u0434\u0440\u0443\u0433\u0438\u043c\u0438 \u0441\u043b\u043e\u0432\u0430\u043c\u0438?\",\n        15: \"\u0418\u0437\u043c\u0435\u043d\u0438\u0442\u0435 \u044d\u0442\u043e\u0442 \u0442\u0435\u043a\u0441\u0442: {opening_quote}{text}{closing_quote}, \u0447\u0442\u043e\u0431\u044b \u043f\u0435\u0440\u0435\u0434\u0430\u0442\u044c \u0442\u0443 \u0436\u0435 \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u044e \u0438\u043d\u0430\u0447\u0435.\",\n        16: \"\u041c\u043e\u0436\u0435\u0442\u0435 \u043b\u0438 \u0432\u044b \u043f\u0435\u0440\u0435\u0444\u0440\u0430\u0437\u0438\u0440\u043e\u0432\u0430\u0442\u044c {opening_quote}{text}{closing_quote} \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u0434\u0440\u0443\u0433\u043e\u0433\u043e \u0441\u043b\u043e\u0432\u0430\u0440\u044f?\",\n        17: \"\u041f\u043e\u043f\u0440\u043e\u0431\u0443\u0439\u0442\u0435 \u0432\u044b\u0440\u0430\u0437\u0438\u0442\u044c {opening_quote}{text}{closing_quote} \u0441\u0432\u043e\u0438\u043c\u0438 \u0441\u043b\u043e\u0432\u0430\u043c\u0438.\",\n        18: \"\u041a\u0430\u043a \u0438\u043d\u0430\u0447\u0435 \u043c\u043e\u0436\u043d\u043e \u0432\u044b\u0440\u0430\u0437\u0438\u0442\u044c {opening_quote}{text}{closing_quote}?\",\n        19: \"\u0418\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0439\u0442\u0435 \u0434\u0440\u0443\u0433\u0443\u044e \u0444\u043e\u0440\u043c\u0443\u043b\u0438\u0440\u043e\u0432\u043a\u0443, \u0447\u0442\u043e\u0431\u044b \u043f\u0435\u0440\u0435\u0434\u0430\u0442\u044c \u0442\u0443 \u0436\u0435 \u0438\u0434\u0435\u044e, \u0447\u0442\u043e \u0438 {opening_quote}{text}{closing_quote}.\",\n        20: \"\u041f\u043e\u0436\u0430\u043b\u0443\u0439\u0441\u0442\u0430, \u043f\u0435\u0440\u0435\u0444\u043e\u0440\u043c\u0443\u043b\u0438\u0440\u0443\u0439\u0442\u0435 {opening_quote}{text}{closing_quote}, \u0447\u0442\u043e\u0431\u044b \u043e\u043d\u043e \u0438\u043c\u0435\u043b\u043e \u0434\u0440\u0443\u0433\u043e\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435.\",\n    },\n    \"de\": {\n        1: \"Paraphrasiere folgenden Text: {opening_quote}{text}{closing_quote}\",\n        2: \"Kannst du folgendes umformulieren: {opening_quote}{text}{closing_quote}?\",\n        3: \"Wie w\u00fcrdest du {opening_quote}{text}{closing_quote} umschreiben?\",\n        4: \"Wie w\u00fcrde man {opening_quote}{text}{closing_quote} umschreiben?\",\n        5: \"Kannst du dies umschreiben: {opening_quote}{text}{closing_quote}\",\n        6: \"Kannst du {opening_quote}{text}{closing_quote} umschreiben?\",\n        7: \"Umschreibe {opening_quote}{text}{closing_quote}\",\n        8: \"Gibt es eine andere Formulierung f\u00fcr {opening_quote}{text}{closing_quote}?\",\n        9: \"Kannst du diesen Text umformulieren: {opening_quote}{text}{closing_quote}\",\n        10: \"Wie w\u00fcrde man {opening_quote}{text}{closing_quote} umformulieren?\",\n        11: \"Finde eine andere Formulierung f\u00fcr {opening_quote}{text}{closing_quote}\",\n    },\n    \"uk\": {\n        1: \"\u041f\u0435\u0440\u0435\u0444\u043e\u0440\u043c\u0443\u043b\u044e\u0439\u0442\u0435 \u043d\u0430\u0441\u0442\u0443\u043f\u043d\u0435 \u0440\u0435\u0447\u0435\u043d\u043d\u044f: {opening_quote}{text}{closing_quote}\",\n        2: \"\u0427\u0438 \u043c\u043e\u0436\u0435\u0442\u0435 \u0432\u0438 \u0432\u0438\u0441\u043b\u043e\u0432\u0438\u0442\u0438 \u0446\u0435 \u0456\u043d\u0430\u043a\u0448\u0435: {opening_quote}{text}{closing_quote}?\",\n        3: \"\u042f\u043a \u0456\u043d\u0430\u043a\u0448\u0435 \u0432\u0438 \u0432\u0438\u0441\u043b\u043e\u0432\u0438\u043b\u0438 \u0431 {opening_quote}{text}{closing_quote}?\",\n        4: \"\u0427\u0438 \u043c\u043e\u0436\u0435\u0442\u0435 \u0432\u0438 \u043f\u0435\u0440\u0435\u0444\u043e\u0440\u043c\u0443\u043b\u044e\u0432\u0430\u0442\u0438 {opening_quote}{text}{closing_quote} \u0456\u043d\u0448\u0438\u043c\u0438 \u0441\u043b\u043e\u0432\u0430\u043c\u0438?\",\n        5: \"\u0417\u043c\u0456\u043d\u0456\u0442\u044c \u0446\u0435\u0439 \u0442\u0435\u043a\u0441\u0442: {opening_quote}{text}{closing_quote}, \u0449\u043e\u0431 \u043f\u0435\u0440\u0435\u0434\u0430\u0442\u0438 \u0442\u0443 \u0441\u0430\u043c\u0443 \u0456\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0456\u044e \u043f\u043e-\u0456\u043d\u0448\u043e\u043c\u0443.\",\n        6: \"\u0427\u0438 \u043c\u043e\u0436\u0435\u0442\u0435 \u0432\u0438 \u043f\u0435\u0440\u0435\u0444\u0440\u0430\u0437\u0443\u0432\u0430\u0442\u0438 {opening_quote}{text}{closing_quote} \u0437\u0430 \u0434\u043e\u043f\u043e\u043c\u043e\u0433\u043e\u044e \u0456\u043d\u0448\u043e\u0433\u043e \u0441\u043b\u043e\u0432\u043d\u0438\u043a\u0430?\",\n        7: \"\u0421\u043f\u0440\u043e\u0431\u0443\u0439\u0442\u0435 \u0432\u0438\u0441\u043b\u043e\u0432\u0438\u0442\u0438 {opening_quote}{text}{closing_quote} \u0441\u0432\u043e\u0457\u043c\u0438 \u0441\u043b\u043e\u0432\u0430\u043c\u0438.\",\n        8: \"\u042f\u043a \u0456\u043d\u0430\u043a\u0448\u0435 \u043c\u043e\u0436\u043d\u0430 \u0432\u0438\u0441\u043b\u043e\u0432\u0438\u0442\u0438 {opening_quote}{text}{closing_quote}?\",\n        9: \"\u0412\u0438\u043a\u043e\u0440\u0438\u0441\u0442\u0430\u0439\u0442\u0435 \u0456\u043d\u0448\u0443 \u0444\u043e\u0440\u043c\u0443\u043b\u044e\u0432\u0430\u043d\u043d\u044f, \u0449\u043e\u0431 \u043f\u0435\u0440\u0435\u0434\u0430\u0442\u0438 \u0442\u0443 \u0441\u0430\u043c\u0443 \u0456\u0434\u0435\u044e, \u0449\u043e \u0439 {opening_quote}{text}{closing_quote}.\",\n        10: \"\u0411\u0443\u0434\u044c \u043b\u0430\u0441\u043a\u0430, \u043f\u0435\u0440\u0435\u0444\u043e\u0440\u043c\u0443\u043b\u044e\u0439\u0442\u0435 {opening_quote}{text}{closing_quote}, \u0449\u043e\u0431 \u0432\u043e\u043d\u043e \u043c\u0430\u043b\u043e \u0456\u043d\u0448\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u043d\u044f.\",\n        11: \"\u041f\u0435\u0440\u0435\u0444\u0440\u0430\u0437\u0443\u0439\u0442\u0435 \u043d\u0430\u0441\u0442\u0443\u043f\u043d\u0438\u0439 \u0442\u0435\u043a\u0441\u0442: {opening_quote}{text}{closing_quote}\",\n        12: \"\u0427\u0438 \u043c\u043e\u0436\u0435\u0442\u0435 \u0432\u0438 \u043f\u0435\u0440\u0435\u0444\u043e\u0440\u043c\u0443\u043b\u044e\u0432\u0430\u0442\u0438 \u043d\u0430\u0441\u0442\u0443\u043f\u043d\u0435: {opening_quote}{text}{closing_quote}?\",\n    },\n    \"bg\": {\n        1: \"\u041f\u0440\u0435\u043d\u0430\u043f\u0438\u0448\u0435\u0442\u0435 \u0441\u043b\u0435\u0434\u043d\u0438\u044f \u0442\u0435\u043a\u0441\u0442: {opening_quote}{text}{closing_quote}\",\n        2: \"\u041c\u043e\u0436\u0435\u0442\u0435 \u043b\u0438 \u0434\u0430 \u043f\u0440\u0435\u0440\u0430\u0431\u043e\u0442\u0438\u0442\u0435 \u0441\u043b\u0435\u0434\u043d\u043e\u0442\u043e: {opening_quote}{text}{closing_quote}?\",\n        3: \"\u0414\u0430 \u043f\u0440\u0435\u043d\u0430\u043f\u0438\u0448\u0435\u043c {opening_quote}{text}{closing_quote}.\",\n        4: \"\u041c\u043e\u0436\u0435\u0442\u0435 \u043b\u0438 \u0434\u0430 \u0438\u0437\u0440\u0430\u0437\u0438\u0442\u0435 {opening_quote}{text}{closing_quote} \u043f\u043e \u0434\u0440\u0443\u0433 \u043d\u0430\u0447\u0438\u043d?\",\n        5: \"\u041f\u0440\u043e\u043c\u0435\u043d\u0435\u0442\u0435 \u0442\u0435\u043a\u0441\u0442\u0430 \u043d\u0430 {opening_quote}{text}{closing_quote}, \u0437\u0430 \u0434\u0430 \u0438\u0437\u0440\u0430\u0437\u0438 \u0441\u044a\u0449\u043e\u0442\u043e \u043d\u0435\u0449\u043e, \u043d\u043e \u043f\u043e \u0440\u0430\u0437\u043b\u0438\u0447\u0435\u043d \u043d\u0430\u0447\u0438\u043d.\",\n        6: \"\u041c\u043e\u0436\u0435\u0442\u0435 \u043b\u0438 \u0434\u0430 \u043f\u0440\u0435\u0440\u0430\u0437\u0433\u043b\u0435\u0434\u0430\u0442\u0435 {opening_quote}{text}{closing_quote} \u0441 \u0434\u0440\u0443\u0433\u0438 \u0434\u0443\u043c\u0438?\",\n        7: \"\u041e\u043f\u0438\u0442\u0430\u0439\u0442\u0435 \u0434\u0430 \u0438\u0437\u0440\u0430\u0437\u0438\u0442\u0435 {opening_quote}{text}{closing_quote} \u0432 \u0441\u0432\u043e\u0438 \u0441\u043e\u0431\u0441\u0442\u0432\u0435\u043d\u0438 \u0434\u0443\u043c\u0438.\",\n        8: \"\u041a\u0430\u043a \u0431\u0438\u0445\u0442\u0435 \u0438\u0437\u0440\u0430\u0437\u0438\u043b\u0438 {opening_quote}{text}{closing_quote} \u043f\u043e \u0434\u0440\u0443\u0433 \u043d\u0430\u0447\u0438\u043d?\",\n        9: \"\u0418\u0437\u043f\u043e\u043b\u0437\u0432\u0430\u0439\u0442\u0435 \u0434\u0440\u0443\u0433\u0438 \u0434\u0443\u043c\u0438, \u0437\u0430 \u0434\u0430 \u0438\u0437\u0440\u0430\u0437\u0438\u0442\u0435 \u0441\u044a\u0449\u043e\u0442\u043e \u043a\u0430\u0442\u043e {opening_quote}{text}{closing_quote}.\",\n        10: \"\u041c\u043e\u043b\u044f, \u043f\u0440\u0435\u043d\u0430\u043f\u0438\u0448\u0435\u0442\u0435 {opening_quote}{text}{closing_quote}, \u0437\u0430 \u0434\u0430 \u0438\u043c\u0430 \u0440\u0430\u0437\u043b\u0438\u0447\u043d\u043e \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435.\",\n    },\n    \"ca\": {\n        1: \"Par\u00e0frasi aquesta frase: {opening_quote}{text}{closing_quote}\",\n        2: \"Podeu reformular el seg\u00fcent: {opening_quote}{text}{closing_quote}?\",\n        3: \"Com expressaries {opening_quote}{text}{closing_quote} d'una altra manera?\",\n        4: \"Podries posar {opening_quote}{text}{closing_quote} amb altres paraules?\",\n        5: \"Canvia aquest text: {opening_quote}{text}{closing_quote} per transmetre el mateix missatge d'una altra manera.\",\n        6: \"Pots par\u00e0frasiar {opening_quote}{text}{closing_quote} utilitzant un vocabulari diferent?\",\n        7: \"Intenta expressar {opening_quote}{text}{closing_quote} amb les teves pr\u00f2pies paraules.\",\n        8: \"D'una altra manera, com podr\u00edeu dir {opening_quote}{text}{closing_quote}?\",\n        9: \"Utilitza una frase diferent per transmetre la mateixa idea que {opening_quote}{text}{closing_quote}.\",\n        10: \"Si us plau, reescriu {opening_quote}{text}{closing_quote} perqu\u00e8 tingui un significat diferent.\",\n    },\n    \"cs\": {\n        1: \"Parafr\u00e1zeuj tento v\u00fdrok: {opening_quote}{text}{closing_quote}\",\n        2: \"M\u016f\u017ee\u0161 p\u0159eformulovat n\u00e1sleduj\u00edc\u00ed v\u011btu: {opening_quote}{text}{closing_quote}?\",\n        3: \"Jak bys vyj\u00e1d\u0159il/a {opening_quote}{text}{closing_quote} jinak?\",\n        4: \"M\u016f\u017ee\u0161 p\u0159elo\u017eit {opening_quote}{text}{closing_quote} do jin\u00fdch slov?\",\n        5: \"Zm\u011b\u0148 tento text: {opening_quote}{text}{closing_quote} tak, aby v n\u011bm zazn\u011bla stejn\u00e1 my\u0161lenka, ale jinak.\",\n        6: \"Dok\u00e1\u017ee\u0161 parafr\u00e1zovat {opening_quote}{text}{closing_quote} pomoc\u00ed jin\u00e9ho slovn\u00edku?\",\n        7: \"Zkus vyj\u00e1d\u0159it {opening_quote}{text}{closing_quote} vlastn\u00edmi slovy.\",\n        8: \"Jak bys jinak \u0159ekl/a {opening_quote}{text}{closing_quote}?\",\n        9: \"Pou\u017eij jinou formulaci, aby se vyj\u00e1d\u0159ila stejn\u00e1 my\u0161lenka jako {opening_quote}{text}{closing_quote}.\",\n        10: \"Pros\u00edm, p\u0159epi\u0161 {opening_quote}{text}{closing_quote} tak, aby m\u011blo jin\u00fd v\u00fdznam.\",\n    },\n    \"da\": {\n        1: \"Parafras\u00e9r f\u00f8lgende s\u00e6tning: {opening_quote}{text}{closing_quote}\",\n        2: \"Kan du omformulere f\u00f8lgende: {opening_quote}{text}{closing_quote}?\",\n        3: \"Hvordan ville du udtrykke {opening_quote}{text}{closing_quote} p\u00e5 en anden m\u00e5de?\",\n        4: \"Kan du s\u00e6tte {opening_quote}{text}{closing_quote} i andre ord?\",\n        5: \"\u00c6ndr denne tekst: {opening_quote}{text}{closing_quote} for at formidle den samme id\u00e9 p\u00e5 en anden m\u00e5de.\",\n        6: \"Kan du parafrasere {opening_quote}{text}{closing_quote} ved hj\u00e6lp af et andet ordforr\u00e5d?\",\n        7: \"Pr\u00f8v at omskrive {opening_quote}{text}{closing_quote} med dine egne ord.\",\n        8: \"P\u00e5 hvilken anden m\u00e5de kunne du sige {opening_quote}{text}{closing_quote}?\",\n        9: \"Brug en anden formulering for at formidle den samme id\u00e9 som {opening_quote}{text}{closing_quote}.\",\n        10: \"Vil du venligst omskrive {opening_quote}{text}{closing_quote} s\u00e5 det har en anden betydning.\",\n    },\n    \"es\": {\n        1: \"Reformula esta oraci\u00f3n: {opening_quote}{text}{closing_quote}\",\n        2: \"\u00bfPodr\u00edas expresar de otra manera lo siguiente: {opening_quote}{text}{closing_quote}?\",\n        3: \"\u00bfC\u00f3mo dir\u00edas {opening_quote}{text}{closing_quote} con otras palabras?\",\n        4: \"Reescribe {opening_quote}{text}{closing_quote} utilizando un vocabulario diferente.\",\n        5: \"Cambia este texto: {opening_quote}{text}{closing_quote} para transmitir el mismo mensaje de manera diferente.\",\n        6: \"\u00bfPuedes parafrasear {opening_quote}{text}{closing_quote} utilizando un lenguaje diferente?\",\n        7: \"Intenta expresar {opening_quote}{text}{closing_quote} con tus propias palabras.\",\n        8: \"\u00bfDe qu\u00e9 otra manera podr\u00edas decir {opening_quote}{text}{closing_quote}?\",\n        9: \"Utiliza una frase diferente para transmitir la misma idea que {opening_quote}{text}{closing_quote}.\",\n        10: \"Por favor, reescribe {opening_quote}{text}{closing_quote} para que tenga un significado diferente.\",\n    },\n    \"fr\": {\n        1: \"Reformulez cette phrase : {opening_quote}{text}{closing_quote}\",\n        2: \"Pouvez-vous exprimer diff\u00e9remment ceci : {opening_quote}{text}{closing_quote} ?\",\n        3: \"Comment exprimeriez-vous {opening_quote}{text}{closing_quote} autrement ?\",\n        4: \"Pouvez-vous reformuler {opening_quote}{text}{closing_quote} avec d\u2019autres termes ?\",\n        5: \"Modifiez ce texte : {opening_quote}{text}{closing_quote} pour transmettre le m\u00eame message de mani\u00e8re diff\u00e9rente.\",\n        6: \"Pouvez-vous paraphraser {opening_quote}{text}{closing_quote} avec un vocabulaire diff\u00e9rent ?\",\n        7: \"Essayez d\u2019exprimer {opening_quote}{text}{closing_quote} avec vos propres mots.\",\n        8: \"De quelle autre mani\u00e8re pourriez-vous exprimer {opening_quote}{text}{closing_quote} ?\",\n        9: \"Utilisez une autre formulation pour transmettre la m\u00eame id\u00e9e que {opening_quote}{text}{closing_quote}.\",\n        10: \"Pouvez-vous r\u00e9\u00e9crire {opening_quote}{text}{closing_quote} pour qu\u2019il ait une signification diff\u00e9rente ?\",\n        11: \"R\u00e9\u00e9crit {opening_quote}{text}{closing_quote} d'une autre fa\u00e7on.\",\n        12: \"Reformule ce texte : {opening_quote}{text}{closing_quote}.\",\n        13: \"Paraphrase {opening_quote}{text}{closing_quote}.\",\n        14: \"Reformule {opening_quote}{text}{closing_quote} en utilisant des mots diff\u00e9rents tout en conservant le m\u00eame sens.\",\n        15: \"R\u00e9\u00e9cris le texte suivant en utilisant tes propres mots : {opening_quote}{text}{closing_quote}.\",\n        16: \"\u00c9cris {opening_quote}{text}{closing_quote} d'une autre mani\u00e8re tout en pr\u00e9servant son sens.\",\n        17: \"Modifie la formulation de {opening_quote}{text}{closing_quote} tout en gardant la signification originale.\",\n        18: \"Exprime les m\u00eames id\u00e9es que {opening_quote}{text}{closing_quote} en utilisant des termes diff\u00e9rents.\",\n        19: \"R\u00e9\u00e9cris {opening_quote}{text}{closing_quote} en modifiant sa formulation pour exprimer la m\u00eame id\u00e9e d'une mani\u00e8re diff\u00e9rente.\",\n        20: \"Paraphrase {opening_quote}{text}{closing_quote} en utilisant une formulation diff\u00e9rente.\",\n        21: \"Essaye de r\u00e9\u00e9crire le texte suivant : {opening_quote}{text}{closing_quote} en utilisant une formulation diff\u00e9rente.\",\n    },\n    \"hr\": {\n        1: \"Preformuliraj ovu re\u010denicu: {opening_quote}{text}{closing_quote}\",\n        2: \"Mo\u017ee\u0161 li izraziti ovo druga\u010dije: {opening_quote}{text}{closing_quote}?\",\n        3: \"Kako bi izrazio/la {opening_quote}{text}{closing_quote} drugim rije\u010dima?\",\n        4: \"Mo\u017ee\u0161 li re\u0107i {opening_quote}{text}{closing_quote} drugim rije\u010dima?\",\n        5: \"Promijeni ovaj tekst: {opening_quote}{text}{closing_quote} kako bi se prenijela ista poruka na druga\u010diji na\u010din.\",\n        6: \"Mo\u017ee\u0161 li parafrazirati {opening_quote}{text}{closing_quote} koriste\u0107i drugi rje\u010dnik?\",\n        7: \"Poku\u0161aj izraziti {opening_quote}{text}{closing_quote} svojim vlastitim rije\u010dima.\",\n        8: \"Kako bi druga\u010dije izrazio/la {opening_quote}{text}{closing_quote}?\",\n        9: \"Koristi drugu formulaciju kako bi prenio/la istu ideju kao i {opening_quote}{text}{closing_quote}.\",\n        10: \"Molim te, preformuliraj {opening_quote}{text}{closing_quote} tako da ima druga\u010dije zna\u010denje.\",\n    },\n    \"hu\": {\n        1: \"Parafr\u00e1ziszd \u00e1t a k\u00f6vetkez\u0151 mondatot: {opening_quote}{text}{closing_quote}\",\n        2: \"\u00c1t tudn\u00e1d fogalmazni \u00edgy: {opening_quote}{text}{closing_quote}?\",\n        3: \"Hogyan fejezn\u00e9d ki m\u00e1s szavakkal a k\u00f6vetkez\u0151t: {opening_quote}{text}{closing_quote}?\",\n        4: \"Tudsz m\u00e1sk\u00e9ppen fogalmazni a {opening_quote}{text}{closing_quote} kifejez\u00e9st?\",\n        5: \"V\u00e1ltoztasd meg ezt a sz\u00f6veget: {opening_quote}{text}{closing_quote}, hogy az ugyanazt az \u00fczenetet m\u00e1s m\u00f3don k\u00f6zvet\u00edtse.\",\n        6: \"\u00c1t tudod fogalmazni {opening_quote}{text}{closing_quote}-t m\u00e1s sz\u00f3kincs haszn\u00e1lat\u00e1val?\",\n        7: \"Pr\u00f3b\u00e1ld meg a te saj\u00e1t szavaiddal kifejezni a {opening_quote}{text}{closing_quote} tartalm\u00e1t.\",\n        8: \"Milyen m\u00e1s m\u00f3don lehetne kifejezni a {opening_quote}{text}{closing_quote} mondatot?\",\n        9: \"Haszn\u00e1lj m\u00e1s sz\u00f3fordulatot, hogy ugyanazt az \u00f6tletet k\u00f6zvet\u00edtsd, mint a {opening_quote}{text}{closing_quote}.\",\n        10: \"K\u00e9rlek, fogalmazd \u00e1t a {opening_quote}{text}{closing_quote} mondatot \u00fagy, hogy m\u00e1s jelent\u00e9se legyen.\",\n    },\n    \"it\": {\n        1: \"Riformula questa frase: {opening_quote}{text}{closing_quote}\",\n        2: \"Puoi esprimere in modo diverso questo: {opening_quote}{text}{closing_quote}?\",\n        3: \"Come esprimeresti {opening_quote}{text}{closing_quote} con altre parole?\",\n        4: \"Puoi ripetere {opening_quote}{text}{closing_quote} usando un vocabolario diverso?\",\n        5: \"Cambia questo testo: {opening_quote}{text}{closing_quote} per trasmettere lo stesso messaggio in modo diverso.\",\n        6: \"Puoi parafrasare {opening_quote}{text}{closing_quote} usando un lessico diverso?\",\n        7: \"Prova ad esprimere {opening_quote}{text}{closing_quote} con le tue parole.\",\n        8: \"In che altro modo potresti esprimere {opening_quote}{text}{closing_quote}?\",\n        9: \"Usa una formulazione diversa per trasmettere la stessa idea di {opening_quote}{text}{closing_quote}.\",\n        10: \"Per favore, riscrivi {opening_quote}{text}{closing_quote} in modo che abbia un significato diverso.\",\n    },\n    \"nl\": {\n        1: \"Parafraseer de volgende zin: {opening_quote}{text}{closing_quote}\",\n        2: \"Kun je dit anders verwoorden: {opening_quote}{text}{closing_quote}?\",\n        3: \"Hoe zou je {opening_quote}{text}{closing_quote} anders uitdrukken?\",\n        4: \"Kun je {opening_quote}{text}{closing_quote} herschrijven met andere woorden?\",\n        5: \"Verander deze tekst: {opening_quote}{text}{closing_quote} om dezelfde boodschap op een andere manier over te brengen.\",\n        6: \"Kun je {opening_quote}{text}{closing_quote} parafraseren met een ander vocabulaire?\",\n        7: \"Probeer {opening_quote}{text}{closing_quote} met je eigen woorden uit te drukken.\",\n        8: \"Hoe zou je anders zeggen {opening_quote}{text}{closing_quote}?\",\n        9: \"Gebruik een andere formulering om dezelfde gedachte uit te drukken als {opening_quote}{text}{closing_quote}.\",\n        10: \"Wil je alsjeblieft {opening_quote}{text}{closing_quote} herschrijven zodat het een andere betekenis heeft?\",\n    },\n    \"pl\": {\n        1: \"Parafrazuj nast\u0119puj\u0105ce zdanie: {opening_quote}{text}{closing_quote}\",\n        2: \"Czy m\u00f3g\u0142by\u015b to przepisa\u0107 inaczej: {opening_quote}{text}{closing_quote}?\",\n        3: \"Jak inaczej wyrazi\u0142by\u015b {opening_quote}{text}{closing_quote}?\",\n        4: \"Czy m\u00f3g\u0142by\u015b przepisa\u0107 {opening_quote}{text}{closing_quote} innymi s\u0142owami?\",\n        5: \"Zmie\u0144 ten tekst: {opening_quote}{text}{closing_quote} tak, aby przekaza\u0142 t\u0119 sam\u0105 wiadomo\u015b\u0107 w inny spos\u00f3b.\",\n        6: \"Czy m\u00f3g\u0142by\u015b sparafrazowa\u0107 {opening_quote}{text}{closing_quote} u\u017cywaj\u0105c innych s\u0142\u00f3w?\",\n        7: \"Spr\u00f3buj wyrazi\u0107 {opening_quote}{text}{closing_quote} swoimi w\u0142asnymi s\u0142owami.\",\n        8: \"Jak inaczej mo\u017cna powiedzie\u0107 {opening_quote}{text}{closing_quote}?\",\n        9: \"U\u017cyj innej formu\u0142y, aby przekaza\u0107 t\u0119 sam\u0105 ide\u0119 co {opening_quote}{text}{closing_quote}.\",\n        10: \"Czy m\u00f3g\u0142by\u015b przepisa\u0107 {opening_quote}{text}{closing_quote} tak, aby mia\u0142o to inne znaczenie?\",\n        11: \"Sparafrazuj {opening_quote}{text}{closing_quote}.\",\n        12: \"Przeredaguj ten tekst {opening_quote}{text}{closing_quote}.\",\n        13: \"Napiszesz mi {opening_quote}{text}{closing_quote} w inny spos\u00f3b?\",\n        14: \"Napisz {opening_quote}{text}{closing_quote} swoimi s\u0142owami.\",\n        15: \"Przepiszesz mi {opening_quote}{text}{closing_quote} w inny spos\u00f3b tak, \u017ceby zachowa\u0107 pierwotny sens?\",\n        16: \"Jak mo\u017cna inaczej napisa\u0107 {opening_quote}{text}{closing_quote}?\",\n    },\n    \"pt\": {\n        1: \"Reformule a seguinte frase: {opening_quote}{text}{closing_quote}\",\n        2: \"Pode expressar isto de outra forma: {opening_quote}{text}{closing_quote}?\",\n        3: \"Como diria {opening_quote}{text}{closing_quote} com outras palavras?\",\n        4: \"Pode reescrever {opening_quote}{text}{closing_quote} utilizando um vocabul\u00e1rio diferente?\",\n        5: \"Altere este texto: {opening_quote}{text}{closing_quote} para transmitir a mesma mensagem de forma diferente.\",\n        6: \"Pode parafrasear {opening_quote}{text}{closing_quote} utilizando um l\u00e9xico diferente?\",\n        7: \"Tente expressar {opening_quote}{text}{closing_quote} com as suas pr\u00f3prias palavras.\",\n        8: \"De que outra forma poderia expressar {opening_quote}{text}{closing_quote}?\",\n        9: \"Use outra formula\u00e7\u00e3o para transmitir a mesma ideia que {opening_quote}{text}{closing_quote}.\",\n        10: \"Por favor, reescreva {opening_quote}{text}{closing_quote} para que tenha um significado diferente.\",\n    },\n    \"ro\": {\n        1: \"Reformuleaz\u0103 urm\u0103toarea propozi\u021bie: {opening_quote}{text}{closing_quote}\",\n        2: \"Po\u021bi exprima asta altfel: {opening_quote}{text}{closing_quote}?\",\n        3: \"Cum ai spune {opening_quote}{text}{closing_quote} cu alte cuvinte?\",\n        4: \"Po\u021bi rescrie {opening_quote}{text}{closing_quote} cu alte cuvinte?\",\n        5: \"Schimb\u0103 acest text: {opening_quote}{text}{closing_quote} pentru a transmite acela\u0219i mesaj \u00eentr-un mod diferit.\",\n        6: \"Po\u021bi parafrasa {opening_quote}{text}{closing_quote} folosind un alt vocabular?\",\n        7: \"\u00cencearc\u0103 s\u0103 exprimi {opening_quote}{text}{closing_quote} cu propriile cuvinte.\",\n        8: \"Cum ai putea exprima altfel {opening_quote}{text}{closing_quote}?\",\n        9: \"Folose\u0219te o alt\u0103 formulare pentru a transmite aceea\u0219i idee ca \u0219i {opening_quote}{text}{closing_quote}.\",\n        10: \"Te rog s\u0103 rescrii {opening_quote}{text}{closing_quote} astfel \u00eenc\u00e2t s\u0103 aib\u0103 o alt\u0103 semnifica\u021bie.\",\n    },\n    \"sl\": {\n        1: \"Parafraziraj naslednji stavek: {opening_quote}{text}{closing_quote}\",\n        2: \"Lahko izrazi\u0161 to druga\u010de: {opening_quote}{text}{closing_quote}?\",\n        3: \"Kako bi izrazil {opening_quote}{text}{closing_quote} z drugimi besedami?\",\n        4: \"Lahko preoblikuje\u0161 {opening_quote}{text}{closing_quote} z drugimi besedami?\",\n        5: \"Spremeni ta besedilo: {opening_quote}{text}{closing_quote}, da prenese enako sporo\u010dilo na druga\u010den na\u010din.\",\n        6: \"Lahko parafrazira\u0161 {opening_quote}{text}{closing_quote} z drugim besednjakom?\",\n        7: \"Poskusi izraziti {opening_quote}{text}{closing_quote} s svojimi besedami.\",\n        8: \"Kako bi druga\u010de izrazil {opening_quote}{text}{closing_quote}?\",\n        9: \"Uporabi druga\u010den na\u010din izra\u017eanja, da prenese\u0161 enako idejo kot {opening_quote}{text}{closing_quote}.\",\n        10: \"Prosim, preoblikuj {opening_quote}{text}{closing_quote}, da bo imel druga\u010den pomen.\",\n        12: \"Parafrazirajte {opening_quote}{text}{closing_quote}\",\n        13: \"Ali lahko preoblikujete slednje: {opening_quote}{text}{closing_quote}?\",\n        14: \"Ali lahko preoblikujete stavek: {opening_quote}{text}{closing_quote}?\",\n        15: \"Preoblikujte slede\u010do izjavo: {opening_quote}{text}{closing_quote}.\",\n        16: \"Kako bi izrazili {opening_quote}{text}{closing_quote} druga\u010de?\",\n        17: \"Ali lahko napi\u0161ete {opening_quote}{text}{closing_quote} z drugimi besedami?\",\n        18: \"Spremenite to besedilo: {opening_quote}{text}{closing_quote} z istim pomenom vendar na drugacen na\u010din.\",\n        19: \"Ali lahko prafrazirate {opening_quote}{text}{closing_quote} z uporabo druga\u010dnega besednjaka?\",\n        20: \"Poskusite ponoviti {opening_quote}{text}{closing_quote} z va\u0161imi lastnimi besedami.\",\n        21: \"Kako druga\u010de bi lahko rekli {opening_quote}{text}{closing_quote}?\",\n        22: \"Uporabite razli\u010dne fraze za izraz iste ideje kot {opening_quote}{text}{closing_quote}.\",\n        23: \"Prosim ponovno povejte {opening_quote}{text}{closing_quote} tako, da bo imelo druga\u010den pomen.\",\n    },\n    \"sr\": {\n        1: \"Preformuli\u0161i slede\u0107u re\u010denicu: {opening_quote}{text}{closing_quote}\",\n        2: \"Da li mo\u017ee\u0161 da izrazi\u0161 ovo druga\u010dije: {opening_quote}{text}{closing_quote}?\",\n        3: \"Kako bi izrazio/la {opening_quote}{text}{closing_quote} drugim re\u010dima?\",\n        4: \"Mo\u017ee\u0161 li da preformuli\u0161e\u0161 {opening_quote}{text}{closing_quote} koriste\u0107i drugi re\u010dnik?\",\n        5: \"Promeni ovaj tekst: {opening_quote}{text}{closing_quote} kako bi se prenela ista poruka na druga\u010diji na\u010din.\",\n        6: \"Mo\u017ee\u0161 li da parafrizira\u0161 {opening_quote}{text}{closing_quote} koriste\u0107i drugi vokabular?\",\n        7: \"Poku\u0161aj da izrazi\u0161 {opening_quote}{text}{closing_quote} svojim vlastitim re\u010dima.\",\n        8: \"Kako bi druga\u010dije izrazio/la {opening_quote}{text}{closing_quote}?\",\n        9: \"Koristi drugu formulaciju kako bi preneo/la istu ideju kao i {opening_quote}{text}{closing_quote}.\",\n        10: \"Molim te, preformuli\u0161i {opening_quote}{text}{closing_quote} tako da ima druga\u010dije zna\u010denje.\",\n    },\n    \"sv\": {\n        1: \"Parafrasera f\u00f6ljande mening: {opening_quote}{text}{closing_quote}\",\n        2: \"Kan du uttrycka detta p\u00e5 ett annat s\u00e4tt: {opening_quote}{text}{closing_quote}?\",\n        3: \"Hur skulle du uttrycka {opening_quote}{text}{closing_quote} med andra ord?\",\n        4: \"Kan du omskriva {opening_quote}{text}{closing_quote} med andra ord?\",\n        5: \"\u00c4ndra p\u00e5 denna text: {opening_quote}{text}{closing_quote} f\u00f6r att f\u00f6rmedla samma meddelande p\u00e5 ett annat s\u00e4tt.\",\n        6: \"Kan du parafrasera {opening_quote}{text}{closing_quote} med ett annat ordf\u00f6rr\u00e5d?\",\n        7: \"F\u00f6rs\u00f6k att uttrycka {opening_quote}{text}{closing_quote} med dina egna ord.\",\n        8: \"P\u00e5 vilket annat s\u00e4tt skulle du uttrycka {opening_quote}{text}{closing_quote}?\",\n        9: \"Anv\u00e4nd en annan formulering f\u00f6r att f\u00f6rmedla samma id\u00e9 som {opening_quote}{text}{closing_quote}.\",\n        10: \"Kan du sn\u00e4lla omskriva {opening_quote}{text}{closing_quote} s\u00e5 att det har en annan betydelse?\",\n    },\n}\n", "data/datasets/tatoeba_mt_qna_oa/language_names.py": "language_names = {\n    \"en\": {\n        \"en\": \"English\",\n        \"ru\": \"\u0410\u043d\u0433\u043b\u0438\u0439\u0441\u043a\u0438\u0439\",\n        \"de\": \"Englisch\",\n        \"uk\": \"\u0410\u043d\u0433\u043b\u0456\u0439\u0441\u044c\u043a\u0430 \u043c\u043e\u0432\u0430\",\n        \"bg\": \"\u0410\u043d\u0433\u043b\u0438\u0439\u0441\u043a\u0438\",\n        \"ca\": \"Angl\u00e8s\",\n        \"cs\": \"Angli\u010dtina\",\n        \"da\": \"Engelsk\",\n        \"es\": \"Ingl\u00e9s\",\n        \"fr\": \"Anglais\",\n        \"hr\": \"Engleski\",\n        \"hu\": \"Angol\",\n        \"it\": \"Inglese\",\n        \"nl\": \"Engels\",\n        \"pl\": \"Angielski\",\n        \"pt\": \"Ingl\u00eas\",\n        \"ro\": \"Englez\u0103\",\n        \"sl\": \"Angle\u0161\u010dina\",\n        \"sr\": \"\u0415\u043d\u0433\u043b\u0435\u0441\u043a\u0438\",\n        \"sv\": \"Engelska\",\n    },\n    \"ru\": {\n        \"en\": \"Russian\",\n        \"ru\": \"\u0420\u0443\u0441\u0441\u043a\u0438\u0439\",\n        \"de\": \"Russisch\",\n        \"uk\": \"\u0420\u043e\u0441\u0456\u0439\u0441\u044c\u043a\u0430 \u043c\u043e\u0432\u0430\",\n        \"bg\": \"\u0420\u0443\u0441\u043a\u0438\",\n        \"ca\": \"Rus\",\n        \"cs\": \"Ru\u0161tina\",\n        \"da\": \"Russisk\",\n        \"es\": \"Ruso\",\n        \"fr\": \"Russe\",\n        \"hr\": \"Ruski\",\n        \"hu\": \"Orosz\",\n        \"it\": \"Russo\",\n        \"nl\": \"Russisch\",\n        \"pl\": \"Rosyjski\",\n        \"pt\": \"Russo\",\n        \"ro\": \"Rus\u0103\",\n        \"sl\": \"Ru\u0161\u010dina\",\n        \"sr\": \"\u0420\u0443\u0441\u043a\u0438\",\n        \"sv\": \"Ryska\",\n    },\n    \"de\": {\n        \"en\": \"German\",\n        \"ru\": \"\u041d\u0435\u043c\u0435\u0446\u043a\u0438\u0439\",\n        \"de\": \"Deutsch\",\n        \"uk\": \"\u041d\u0456\u043c\u0435\u0446\u044c\u043a\u0430 \u043c\u043e\u0432\u0430\",\n        \"bg\": \"\u041d\u0435\u043c\u0441\u043a\u0438\",\n        \"ca\": \"Alemany\",\n        \"cs\": \"N\u011bm\u010dina\",\n        \"da\": \"Tysk\",\n        \"es\": \"Alem\u00e1n\",\n        \"fr\": \"Allemand\",\n        \"hr\": \"Njema\u010dki\",\n        \"hu\": \"N\u00e9met\",\n        \"it\": \"Tedesco\",\n        \"nl\": \"Duits\",\n        \"pl\": \"Niemiecki\",\n        \"pt\": \"Alem\u00e3o\",\n        \"ro\": \"German\u0103\",\n        \"sl\": \"Nem\u0161\u010dina\",\n        \"sr\": \"\u041d\u0435\u043c\u0430\u0447\u043a\u0438\",\n        \"sv\": \"Tyska\",\n    },\n    \"uk\": {\n        \"en\": \"Ukrainian\",\n        \"ru\": \"\u0423\u043a\u0440\u0430\u0438\u043d\u0441\u043a\u0438\u0439\",\n        \"de\": \"Ukrainisch\",\n        \"uk\": \"\u0423\u043a\u0440\u0430\u0457\u043d\u0441\u044c\u043a\u0430 \u043c\u043e\u0432\u0430\",\n        \"bg\": \"\u0423\u043a\u0440\u0430\u0438\u043d\u0441\u043a\u0438\",\n        \"ca\": \"Ucra\u00efn\u00e8s\",\n        \"cs\": \"Ukrajin\u0161tina\",\n        \"da\": \"Ukrainsk\",\n        \"es\": \"Ucraniano\",\n        \"fr\": \"Ukrainien\",\n        \"hr\": \"Ukrajinski\",\n        \"hu\": \"Ukr\u00e1n\",\n        \"it\": \"Ucraino\",\n        \"nl\": \"Oekra\u00efens\",\n        \"pl\": \"Ukrai\u0144ski\",\n        \"pt\": \"Ucraniano\",\n        \"ro\": \"Ucrainean\u0103\",\n        \"sl\": \"Ukrajin\u0161\u010dina\",\n        \"sr\": \"\u0423\u043a\u0440\u0430\u0458\u0438\u043d\u0441\u043a\u0438\",\n        \"sv\": \"Ukrainska\",\n    },\n    \"bg\": {\n        \"en\": \"Bulgarian\",\n        \"ru\": \"\u0411\u043e\u043b\u0433\u0430\u0440\u0441\u043a\u0438\u0439\",\n        \"de\": \"Bulgarisch\",\n        \"uk\": \"\u0411\u043e\u043b\u0433\u0430\u0440\u0441\u044c\u043a\u0430 \u043c\u043e\u0432\u0430\",\n        \"bg\": \"\u0411\u044a\u043b\u0433\u0430\u0440\u0441\u043a\u0438\",\n        \"ca\": \"B\u00falgar\",\n        \"cs\": \"Bulhar\u0161tina\",\n        \"da\": \"Bulgarsk\",\n        \"es\": \"B\u00falgaro\",\n        \"fr\": \"Bulgare\",\n        \"hr\": \"Bugarski\",\n        \"hu\": \"Bolg\u00e1r\",\n        \"it\": \"Bulgaro\",\n        \"nl\": \"Bulgaars\",\n        \"pl\": \"Bu\u0142garski\",\n        \"pt\": \"B\u00falgaro\",\n        \"ro\": \"Bulgar\u0103\",\n        \"sl\": \"Bolgar\u0161\u010dina\",\n        \"sr\": \"\u0411\u0443\u0433\u0430\u0440\u0441\u043a\u0438\",\n        \"sv\": \"Bulgariska\",\n    },\n    \"ca\": {\n        \"en\": \"Catalan\",\n        \"ru\": \"\u041a\u0430\u0442\u0430\u043b\u0430\u043d\u0441\u043a\u0438\u0439\",\n        \"ca\": \"Catal\u00e0\",\n        \"de\": \"Katalanisch\",\n        \"uk\": \"\u041a\u0430\u0442\u0430\u043b\u043e\u043d\u0441\u044c\u043a\u0430 \u043c\u043e\u0432\u0430\",\n        \"bg\": \"\u041a\u0430\u0442\u0430\u043b\u043e\u043d\u0441\u043a\u0438\",\n        \"cs\": \"Katal\u00e1n\u0161tina\",\n        \"da\": \"Katalansk\",\n        \"es\": \"Catal\u00e1n\",\n        \"fr\": \"Catalan\",\n        \"hr\": \"Katalonski\",\n        \"hu\": \"Katal\u00e1n\",\n        \"it\": \"Catalano\",\n        \"nl\": \"Catalaans\",\n        \"pl\": \"Katalo\u0144ski\",\n        \"pt\": \"Catal\u00e3o\",\n        \"ro\": \"Catalan\u0103\",\n        \"sl\": \"Katalon\u0161\u010dina\",\n        \"sr\": \"\u041a\u0430\u0442\u0430\u043b\u043e\u043d\u0441\u043a\u0438\",\n        \"sv\": \"Katalanska\",\n    },\n    \"cs\": {\n        \"en\": \"Czech\",\n        \"ru\": \"\u0427\u0435\u0448\u0441\u043a\u0438\u0439\",\n        \"de\": \"Tschechisch\",\n        \"uk\": \"\u0427\u0435\u0441\u044c\u043a\u0430 \u043c\u043e\u0432\u0430\",\n        \"bg\": \"\u0427\u0435\u0448\u043a\u0438\",\n        \"ca\": \"Txec\",\n        \"cs\": \"\u010de\u0161tina\",\n        \"da\": \"Tjekkisk\",\n        \"es\": \"Checo\",\n        \"fr\": \"Tch\u00e8que\",\n        \"hr\": \"\u010de\u0161ki\",\n        \"hu\": \"Cseh\",\n        \"it\": \"Ceco\",\n        \"nl\": \"Tsjechisch\",\n        \"pl\": \"Czeski\",\n        \"pt\": \"Checo\",\n        \"ro\": \"Ceh\u0103\",\n        \"sl\": \"\u010de\u0161\u010dina\",\n        \"sr\": \"\u0427\u0435\u0448\u043a\u0438\",\n        \"sv\": \"Tjeckiska\",\n    },\n    \"da\": {\n        \"en\": \"Danish\",\n        \"ru\": \"\u0414\u0430\u0442\u0441\u043a\u0438\u0439\",\n        \"de\": \"D\u00e4nisch\",\n        \"uk\": \"\u0414\u0430\u043d\u0441\u044c\u043a\u0430 \u043c\u043e\u0432\u0430\",\n        \"bg\": \"\u0414\u0430\u0442\u0441\u043a\u0438\",\n        \"ca\": \"Dan\u00e8s\",\n        \"cs\": \"D\u00e1n\u0161tina\",\n        \"da\": \"Dansk\",\n        \"es\": \"Dan\u00e9s\",\n        \"fr\": \"Danois\",\n        \"hr\": \"Danski\",\n        \"hu\": \"D\u00e1n\",\n        \"it\": \"Danese\",\n        \"nl\": \"Deens\",\n        \"pl\": \"Du\u0144ski\",\n        \"pt\": \"Dinamarqu\u00eas\",\n        \"ro\": \"Danez\u0103\",\n        \"sl\": \"Dan\u0161\u010dina\",\n        \"sr\": \"\u0414\u0430\u043d\u0441\u043a\u0438\",\n        \"sv\": \"Danska\",\n    },\n    \"es\": {\n        \"en\": \"Spanish\",\n        \"ru\": \"\u0418\u0441\u043f\u0430\u043d\u0441\u043a\u0438\u0439\",\n        \"de\": \"Spanisch\",\n        \"uk\": \"I\u0441\u043f\u0430\u043d\u0441\u044c\u043a\u0430 \u043c\u043e\u0432\u0430\",\n        \"bg\": \"\u0418\u0441\u043f\u0430\u043d\u0441\u043a\u0438\",\n        \"ca\": \"Espanyol\",\n        \"cs\": \"\u0161pan\u011bl\u0161tina\",\n        \"da\": \"Spansk\",\n        \"es\": \"Espa\u00f1ol\",\n        \"fr\": \"Espagnol\",\n        \"hr\": \"\u0161panjolski\",\n        \"hu\": \"Spanyol\",\n        \"it\": \"Spagnolo\",\n        \"nl\": \"Spaans\",\n        \"pl\": \"Hiszpa\u0144ski\",\n        \"pt\": \"Espanhol\",\n        \"ro\": \"Spaniol\u0103\",\n        \"sl\": \"\u0161pan\u0161\u010dina\",\n        \"sr\": \"\u0428\u043f\u0430\u043d\u0441\u043a\u0438\",\n        \"sv\": \"Spanska\",\n    },\n    \"fr\": {\n        \"en\": \"French\",\n        \"ru\": \"\u0424\u0440\u0430\u043d\u0446\u0443\u0437\u0441\u043a\u0438\u0439\",\n        \"de\": \"Franz\u00f6sisch\",\n        \"uk\": \"\u0424\u0440\u0430\u043d\u0446\u0443\u0437\u044c\u043a\u0430 \u043c\u043e\u0432\u0430\",\n        \"bg\": \"\u0424\u0440\u0435\u043d\u0441\u043a\u0438\",\n        \"ca\": \"Franc\u00e8s\",\n        \"cs\": \"Francouz\u0161tina\",\n        \"da\": \"Fransk\",\n        \"es\": \"Franc\u00e9s\",\n        \"fr\": \"Fran\u00e7ais\",\n        \"hr\": \"Francuski\",\n        \"hu\": \"Francia\",\n        \"it\": \"Francese\",\n        \"nl\": \"Frans\",\n        \"pl\": \"Francuski\",\n        \"pt\": \"Franc\u00eas\",\n        \"ro\": \"Francez\u0103\",\n        \"sl\": \"Franco\u0161\u010dina\",\n        \"sr\": \"\u0424\u0440\u0430\u043d\u0446\u0443\u0441\u043a\u0438\",\n        \"sv\": \"Franska\",\n    },\n    \"hr\": {\n        \"en\": \"Croatian\",\n        \"ru\": \"\u0425\u043e\u0440\u0432\u0430\u0442\u0441\u043a\u0438\u0439\",\n        \"de\": \"Kroatisch\",\n        \"uk\": \"\u0425\u043e\u0440\u0432\u0430\u0442\u0441\u044c\u043a\u0430 \u043c\u043e\u0432\u0430\",\n        \"bg\": \"\u0425\u044a\u0440\u0432\u0430\u0442\u0441\u043a\u0438\",\n        \"ca\": \"Croat\",\n        \"cs\": \"Chorvat\u0161tina\",\n        \"da\": \"Kroatisk\",\n        \"es\": \"Croata\",\n        \"fr\": \"Croate\",\n        \"hr\": \"Hrvatska\",\n        \"hu\": \"Horv\u00e1t\",\n        \"it\": \"Croato\",\n        \"nl\": \"Kroatisch\",\n        \"pl\": \"Chorwacki\",\n        \"pt\": \"Croata\",\n        \"ro\": \"Croat\u0103\",\n        \"sl\": \"Hrva\u0161\u010dina\",\n        \"sr\": \"\u0425\u0440\u0432\u0430\u0442\u0441\u043a\u0438\",\n        \"sv\": \"Kroatiska\",\n    },\n    \"hu\": {\n        \"en\": \"Hungarian\",\n        \"ru\": \"\u0412\u0435\u043d\u0433\u0435\u0440\u0441\u043a\u0438\u0439\",\n        \"de\": \"Ungarisch\",\n        \"uk\": \"\u0423\u0433\u043e\u0440\u0441\u044c\u043a\u0430 \u043c\u043e\u0432\u0430\",\n        \"bg\": \"\u0423\u043d\u0433\u0430\u0440\u0441\u043a\u0438\",\n        \"ca\": \"Hongar\u00e8s\",\n        \"cs\": \"Ma\u010far\u0161tina\",\n        \"da\": \"Ungarsk\",\n        \"es\": \"H\u00fangaro\",\n        \"fr\": \"Hongrois\",\n        \"hr\": \"Ma\u0111arski\",\n        \"hu\": \"Magyar\",\n        \"it\": \"Ungherese\",\n        \"nl\": \"Hongaars\",\n        \"pl\": \"W\u0119gierski\",\n        \"pt\": \"H\u00fangaro\",\n        \"ro\": \"Maghiar\u0103\",\n        \"sl\": \"Mad\u017ear\u0161\u010dina\",\n        \"sr\": \"\u041c\u0430\u0452\u0430\u0440\u0441\u043a\u0438\",\n        \"sv\": \"Ungerska\",\n    },\n    \"it\": {\n        \"en\": \"Italian\",\n        \"ru\": \"\u0418\u0442\u0430\u043b\u044c\u044f\u043d\u0441\u043a\u0438\u0439\",\n        \"de\": \"Italienisch\",\n        \"uk\": \"\u0406\u0442\u0430\u043b\u0456\u0439\u0441\u044c\u043a\u0430 \u043c\u043e\u0432\u0430\",\n        \"bg\": \"\u0418\u0442\u0430\u043b\u0438\u0430\u043d\u0441\u043a\u0438\",\n        \"ca\": \"Itali\u00e0\",\n        \"cs\": \"Ital\u0161tina\",\n        \"da\": \"Italiensk\",\n        \"es\": \"Italiano\",\n        \"fr\": \"Italien\",\n        \"hr\": \"Talijanski\",\n        \"hu\": \"Olasz\",\n        \"it\": \"lo Italiano\",\n        \"nl\": \"Italiaans\",\n        \"pl\": \"W\u0142oski\",\n        \"pt\": \"Italiano\",\n        \"ro\": \"Italian\u0103\",\n        \"sl\": \"Italijan\u0161\u010dina\",\n        \"sr\": \"\u0418\u0442\u0430\u043b\u0438\u0458\u0430\u043d\u0441\u043a\u0438\",\n        \"sv\": \"Italienska\",\n    },\n    \"nl\": {\n        \"en\": \"Dutch\",\n        \"ru\": \"\u041d\u0438\u0434\u0435\u0440\u043b\u0430\u043d\u0434\u0441\u043a\u0438\u0439\",\n        \"de\": \"Niederl\u00e4ndisch\",\n        \"uk\": \"\u0413\u043e\u043b\u043b\u0430\u043d\u0434\u0441\u044c\u043a\u0430 \u043c\u043e\u0432\u0430\",\n        \"bg\": \"\u0425\u043e\u043b\u0430\u043d\u0434\u0441\u043a\u0438\",\n        \"ca\": \"Neerland\u00e8s\",\n        \"cs\": \"Nizozem\u0161tina\",\n        \"da\": \"Hollandsk\",\n        \"es\": \"Neerland\u00e9s\",\n        \"fr\": \"N\u00e9erlandais\",\n        \"hr\": \"Nizozemski\",\n        \"hu\": \"Holland\",\n        \"it\": \"Olandese\",\n        \"nl\": \"Nederlands\",\n        \"pl\": \"Holenderski\",\n        \"pt\": \"Holand\u00eas\",\n        \"ro\": \"Olandez\u0103\",\n        \"sl\": \"Nizozem\u0161\u010dina\",\n        \"sr\": \"\u0425\u043e\u043b\u0430\u043d\u0434\u0441\u043a\u0438\",\n        \"sv\": \"Nederl\u00e4ndska\",\n    },\n    \"pl\": {\n        \"en\": \"Polish\",\n        \"ru\": \"\u041f\u043e\u043b\u044c\u0441\u043a\u0438\u0439\",\n        \"de\": \"Polnisch\",\n        \"uk\": \"\u041f\u043e\u043b\u044c\u0441\u044c\u043a\u0430 \u043c\u043e\u0432\u0430\",\n        \"bg\": \"\u041f\u043e\u043b\u0441\u043a\u0438\",\n        \"ca\": \"Polon\u00e8s\",\n        \"cs\": \"Pol\u0161tina\",\n        \"da\": \"Polsk\",\n        \"es\": \"Polaco\",\n        \"fr\": \"Polonais\",\n        \"hr\": \"Poljski\",\n        \"hu\": \"Lengyel\",\n        \"it\": \"Polacco\",\n        \"nl\": \"Pools\",\n        \"pl\": \"Polski\",\n        \"pt\": \"Polon\u00eas\",\n        \"ro\": \"Polonez\u0103\",\n        \"sl\": \"Polj\u0161\u010dina\",\n        \"sr\": \"\u041f\u043e\u0459\u0441\u043a\u0438\",\n        \"sv\": \"Polska\",\n    },\n    \"pt\": {\n        \"en\": \"Portuguese\",\n        \"ru\": \"\u041f\u043e\u0440\u0442\u0443\u0433\u0430\u043b\u044c\u0441\u043a\u0438\u0439\",\n        \"de\": \"Portugiesisch\",\n        \"uk\": \"\u041f\u043e\u0440\u0442\u0443\u0433\u0430\u043b\u044c\u0441\u044c\u043a\u0430 \u043c\u043e\u0432\u0430\",\n        \"bg\": \"\u041f\u043e\u0440\u0442\u0443\u0433\u0430\u043b\u0441\u043a\u0438\",\n        \"ca\": \"Portugu\u00e8s\",\n        \"cs\": \"Portugal\u0161tina\",\n        \"da\": \"Portugisisk\",\n        \"es\": \"Portugu\u00e9s\",\n        \"fr\": \"Portugais\",\n        \"hr\": \"Portugalski\",\n        \"hu\": \"Portug\u00e1l\",\n        \"it\": \"Portoghese\",\n        \"nl\": \"Portugees\",\n        \"pl\": \"Portugalski\",\n        \"pt\": \"Portugu\u00eas\",\n        \"ro\": \"Portughez\u0103\",\n        \"sl\": \"Portugal\u0161\u010dina\",\n        \"sr\": \"\u041f\u043e\u0440\u0442\u0443\u0433\u0430\u043b\u0441\u043a\u0438\",\n        \"sv\": \"Portugisiska\",\n    },\n    \"ro\": {\n        \"en\": \"Romanian\",\n        \"ru\": \"\u0420\u0443\u043c\u044b\u043d\u0441\u043a\u0438\u0439\",\n        \"de\": \"Rum\u00e4nisch\",\n        \"uk\": \"\u0420\u0443\u043c\u0443\u043d\u0441\u044c\u043a\u0430 \u043c\u043e\u0432\u0430\",\n        \"bg\": \"\u0420\u0443\u043c\u044a\u043d\u0441\u043a\u0438\",\n        \"ca\": \"Roman\u00e8s\",\n        \"cs\": \"Rumun\u0161tina\",\n        \"da\": \"Rum\u00e6nsk\",\n        \"es\": \"Rumano\",\n        \"fr\": \"Roumain\",\n        \"hr\": \"Rumunjski\",\n        \"hu\": \"Rom\u00e1n\",\n        \"it\": \"Rumeno\",\n        \"nl\": \"Roemeens\",\n        \"pl\": \"Rumu\u0144ski\",\n        \"pt\": \"Romeno\",\n        \"ro\": \"Rom\u00e2n\u0103\",\n        \"sl\": \"Romun\u0161\u010dina\",\n        \"sr\": \"\u0420\u0443\u043c\u0443\u043d\u0441\u043a\u0438\",\n        \"sv\": \"Rum\u00e4nska\",\n    },\n    \"sl\": {\n        \"en\": \"Slovenian\",\n        \"ru\": \"\u0421\u043b\u043e\u0432\u0435\u043d\u0441\u043a\u0438\u0439\",\n        \"de\": \"Slowenisch\",\n        \"uk\": \"\u0421\u043b\u043e\u0432\u0435\u043d\u0441\u044c\u043a\u0430 \u043c\u043e\u0432\u0430\",\n        \"bg\": \"\u0421\u043b\u043e\u0432\u0435\u043d\u0441\u043a\u0438\",\n        \"ca\": \"Eslov\u00e8\",\n        \"cs\": \"Slovin\u0161tina\",\n        \"da\": \"Slovensk\",\n        \"es\": \"Ssloveno\",\n        \"fr\": \"Slov\u00e8ne\",\n        \"hr\": \"Slovenski\",\n        \"hu\": \"Szlov\u00e9n\",\n        \"it\": \"Sloveno\",\n        \"nl\": \"Sloveens\",\n        \"pl\": \"S\u0142owe\u0144ski\",\n        \"pt\": \"Esloveno\",\n        \"ro\": \"Sloven\u0103\",\n        \"sl\": \"Sloven\u0161\u010dina\",\n        \"sr\": \"\u0421\u043b\u043e\u0432\u0435\u043d\u0430\u0447\u043a\u0438\",\n        \"sv\": \"Slovenska\",\n    },\n    \"sr\": {\n        \"en\": \"Serbian\",\n        \"ru\": \"\u0421\u0435\u0440\u0431\u0441\u043a\u0438\u0439\",\n        \"de\": \"Serbisch\",\n        \"uk\": \"\u0421\u0435\u0440\u0431\u0441\u044c\u043a\u0430 \u043c\u043e\u0432\u0430\",\n        \"bg\": \"\u0421\u0440\u044a\u0431\u0441\u043a\u0438\",\n        \"ca\": \"Serbi\",\n        \"cs\": \"Srb\u0161tina\",\n        \"da\": \"Serbisk\",\n        \"es\": \"Serbio\",\n        \"fr\": \"Serbe\",\n        \"hr\": \"Srpski\",\n        \"hu\": \"Szerb\",\n        \"it\": \"Serbo\",\n        \"nl\": \"Servisch\",\n        \"pl\": \"Serbski\",\n        \"pt\": \"S\u00e9rvio\",\n        \"ro\": \"S\u00e2rb\u0103\",\n        \"sl\": \"Srb\u0161\u010dina\",\n        \"sr\": \"\u0421\u0440\u043f\u0441\u043a\u0438\",\n        \"sv\": \"Serbiska\",\n    },\n    \"sv\": {\n        \"en\": \"Swedish\",\n        \"ru\": \"\u0428\u0432\u0435\u0434\u0441\u043a\u0438\u0439\",\n        \"de\": \"Schwedisch\",\n        \"uk\": \"\u0428\u0432\u0435\u0434\u0441\u044c\u043a\u0430 \u043c\u043e\u0432\u0430\",\n        \"bg\": \"\u0428\u0432\u0435\u0434\u0441\u043a\u0438\",\n        \"ca\": \"Suec\",\n        \"cs\": \"\u0161v\u00e9d\u0161tina\",\n        \"da\": \"Svensk\",\n        \"es\": \"Sueco\",\n        \"fr\": \"Franska\",\n        \"hr\": \"\u0161vedski\",\n        \"hu\": \"Sv\u00e9d\",\n        \"it\": \"Svedese\",\n        \"nl\": \"Zweeds\",\n        \"pl\": \"Szwedzki\",\n        \"pt\": \"Sueco\",\n        \"ro\": \"Suedez\u0103\",\n        \"sl\": \"\u0161ved\u0161\u010dina\",\n        \"sr\": \"\u0428\u0432\u0435\u0434\u0441\u043a\u0438\",\n        \"sv\": \"Svenska\",\n    },\n}\n", "data/datasets/semantics_ws_qna_oa/data_process.py": "import json\nimport random\nfrom dataclasses import dataclass\n\nimport pandas as pd\nimport random_stuff\nfrom datasets import load_dataset\n\nrandom.seed(42)\n\n\n# format to QnA\ndef qna_wrapper():\n    def create_qna(row):\n        # make a random number\n        random_num = random.randint(0, 2)\n\n        # extract rows' vals\n        lang = row[\"Language\"]\n        con_type = row[\"Type\"]\n        word1 = row[\"Word1\"]\n        word2 = row[\"Word2\"]\n\n        score_percent = row[\"Score\"]\n        # 0 - yes; 1 - 50%, 2 - no\n        instruction = random_stuff.qna_random_magic(lang, word1, word2, con_type, score_percent, random_num, True)\n        response = random_stuff.qna_random_magic(lang, word1, word2, con_type, score_percent, random_num, False)\n        source = \"WordSim353\"\n        metadata = {\n            \"language\": lang,\n            \"score\": score_percent,\n            \"type\": con_type,\n        }\n        metadata_str = json.dumps(metadata)\n        return QnA(instruction, response, source, metadata_str)\n\n    return create_qna\n\n\n@dataclass\nclass QnA:\n    INSTRUCTION: str\n    RESPONSE: str\n    SOURCE: str\n    METADATA: str\n\n\n# load ws dataset\nws_dataset = load_dataset(\"0x22almostEvil/ws-semantics-simnrel\", split=\"train\")\nprint(ws_dataset)\n\n# convert the dataset to a pandas dataframe & apply the create_qna function to each row of the dataframe to create QnA objects\nqna_list = pd.DataFrame(ws_dataset).apply(qna_wrapper(), axis=1).tolist()\n\n# export to parquet\nqna_df = pd.DataFrame(qna_list, columns=[\"INSTRUCTION\", \"RESPONSE\", \"SOURCE\", \"METADATA\"])\nqna_df.to_parquet(\"semantics-ws-qna-oa.parquet\", row_group_size=100, engine=\"pyarrow\", index=False)\n", "data/datasets/semantics_ws_qna_oa/random_stuff.py": "# random lists for ws\n\n# sim_questions\nrandom_list_sim_en_q = [\n    \"Are the words {word1} and {word2} synonymous?\",\n    'Are \"{word1}\" and \"{word2}\" synonymous?',\n    \"Is there any connection between the words '{word1}' and '{word2}'?\",\n]\nrandom_list_sim_ru_q = [\n    \"\u042f\u0432\u043b\u044f\u044e\u0442\u0441\u044f \u043b\u0438 \u0441\u043b\u043e\u0432\u0430 {word1} \u0438 {word2} \u0441\u0438\u043d\u043e\u043d\u0438\u043c\u0430\u043c\u0438?\",\n    '\u0421\u0438\u043d\u043e\u043d\u0438\u043c\u044b \u043b\u0438 \"{word1}\" \u0438 \"{word2}\"?',\n    \"\u0415\u0441\u0442\u044c \u043b\u0438 \u043a\u0430\u043a\u0430\u044f-\u0442\u043e \u0441\u0432\u044f\u0437\u044c \u043c\u0435\u0436\u0434\u0443 \u0441\u043b\u043e\u0432\u0430\u043c\u0438 '{word1}' \u0438 '{word2}'?\",\n]\nrandom_list_sim_de_q = [\n    \"Sind die W\u00f6rter {word1} und {word2} Synonyme?\",\n    'Sind \"{word1}\" und \"{word2}\" Synonyme?',\n    \"Gibt es eine Verbindung zwischen den W\u00f6rtern '{word1}' und '{word2}'?\",\n]\nrandom_list_sim_it_q = [\n    \"Le parole {word1} e {word2} sono sinonimi?\",\n    '\"{word1}\" e \"{word2}\" sono sinonimi?',\n    \"C'\u00e8 qualche connessione tra le parole '{word1}' e '{word2}'?\",\n]\n\n# sim_answers:\n\n# same words;\nrandom_list_sim_en_a_same = [\n    \"Yes, because it's the same word.\",\n    'Of course, we\\'re talking about the same word: \"{word1}\".',\n    \"You repeated '{word1}' twice.\",\n]\nrandom_list_sim_ru_a_same = [\n    \"\u0414\u0430, \u0432\u0435\u0434\u044c \u044d\u0442\u043e \u043e\u0434\u043d\u043e \u0438 \u0442\u043e \u0436\u0435 \u0441\u043b\u043e\u0432\u043e.\",\n    '\u041a\u043e\u043d\u0435\u0447\u043d\u043e, \u0432\u0435\u0434\u044c \u0440\u0435\u0447\u044c \u0438\u0434\u0451\u0442 \u043e\u0431 \u043e\u0434\u043d\u043e\u043c \u0441\u043b\u043e\u0432\u0435: \"{word1}\".',\n    \"\u0412\u044b \u043f\u043e\u0432\u0442\u043e\u0440\u0438\u043b\u0438 '{word1}' \u0434\u0432\u0430\u0436\u0434\u044b.\",\n]\nrandom_list_sim_de_a_same = [\n    \"Ja, denn es ist dasselbe Wort.\",\n    'Nat\u00fcrlich, wir sprechen \u00fcber dasselbe Wort: \"{word1}\".',\n    \"Du hast '{word1}' zweimal wiederholt.\",\n]\nrandom_list_sim_it_a_same = [\n    \"S\u00ec, perch\u00e9 \u00e8 la stessa parola.\",\n    'Certo, stiamo parlando della stessa parola: \"{word1}\".',\n    \"Hai ripetuto '{word1}' due volte.\",\n]\n\n# yes;\nrandom_list_sim_en_a_y = [\n    \"Yes, {word1} and {word2} are synonymous.\",\n    '\"{word1}\" and \"{word2}\" are synonymous.',\n    \"Yes. The type of connection between words '{word1}' and '{word2}' is synonymous.\",\n]\nrandom_list_sim_ru_a_y = [\n    \"\u0414\u0430, {word1} \u0438 {word2} \u044f\u0432\u043b\u044f\u044e\u0442\u0441\u044f \u0441\u0438\u043d\u043e\u043d\u0438\u043c\u0430\u043c\u0438.\",\n    '\"{word1}\" \u0438 \"{word2}\" \u044f\u0432\u043b\u044f\u044e\u0442\u0441\u044f \u0441\u0438\u043d\u043e\u043d\u0438\u043c\u0430\u043c\u0438.',\n    \"\u0414\u0430. \u0422\u0438\u043f \u0441\u0432\u044f\u0437\u0438 \u043c\u0435\u0436\u0434\u0443 \u0441\u043b\u043e\u0432\u0430\u043c\u0438 '{word1}' \u0438 '{word2}' \u0441\u0438\u043d\u043e\u043d\u0438\u043c\u0438\u0447\u0435\u043d.\",\n]\nrandom_list_sim_de_a_y = [\n    \"Ja, {word1} und {word2} sind Synonyme.\",\n    '\"{word1}\" und \"{word2}\" sind Synonyme.',\n    \"Ja. Der Typ der Verbindung zwischen den W\u00f6rtern '{word1}' und '{word2}' ist synonym.\",\n]\nrandom_list_sim_it_a_y = [\n    \"S\u00ec, {word1} e {word2} sono sinonimi.\",\n    '\"{word1}\" e \"{word2}\" sono sinonimi.',\n    \"S\u00ec. Il tipo di connessione tra le parole '{word1}' e '{word2}' \u00e8 sinonimico.\",\n]\n\n# 75%;\nrandom_list_sim_en_a_75 = [\n    \"There is a big conceptual meaning similarity between the words {word1} and {word2}, but they are not exactly synonymous.\",\n    'The words \"{word1}\" and \"{word2}\" share a significant similarity, but they cannot be considered true synonyms.',\n    \"While {word1} and {word2} are not interchangeable, they do have a substantial overlap in meaning.\",\n]\nrandom_list_sim_ru_a_75 = [\n    \"\u041c\u0435\u0436\u0434\u0443 \u0441\u043b\u043e\u0432\u0430\u043c\u0438 {word1} \u0438 {word2} \u0435\u0441\u0442\u044c \u0431\u043e\u043b\u044c\u0448\u043e\u0435 \u0441\u0445\u043e\u0434\u0441\u0442\u0432\u043e, \u043d\u043e \u043e\u043d\u0438 \u043d\u0435 \u044f\u0432\u043b\u044f\u044e\u0442\u0441\u044f \u043f\u043e\u043b\u043d\u043e\u0446\u0435\u043d\u043d\u044b\u043c\u0438 \u0441\u0438\u043d\u043e\u043d\u0438\u043c\u0430\u043c\u0438.\",\n    '\u0421\u043b\u043e\u0432\u0430 \"{word1}\" \u0438 \"{word2}\" \u0438\u043c\u0435\u044e\u0442 \u0437\u043d\u0430\u0447\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u0435 \u0441\u0445\u043e\u0434\u0441\u0442\u0432\u043e, \u043d\u043e \u043d\u0435 \u043c\u043e\u0433\u0443\u0442 \u0441\u0447\u0438\u0442\u0430\u0442\u044c\u0441\u044f \u043f\u043e\u043b\u043d\u043e\u0446\u0435\u043d\u043d\u044b\u043c\u0438 \u0441\u0438\u043d\u043e\u043d\u0438\u043c\u0430\u043c\u0438.',\n    \"\u0425\u043e\u0442\u044f {word1} \u0438 {word2} \u043d\u0435 \u044f\u0432\u043b\u044f\u044e\u0442\u0441\u044f \u0432\u0437\u0430\u0438\u043c\u043e\u0437\u0430\u043c\u0435\u043d\u044f\u0435\u043c\u044b\u043c\u0438, \u0443 \u043d\u0438\u0445 \u0435\u0441\u0442\u044c \u0437\u043d\u0430\u0447\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u0435 \u043f\u0435\u0440\u0435\u0441\u0435\u0447\u0435\u043d\u0438\u0435 \u0432 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0438.\",\n]\nrandom_list_sim_de_a_75 = [\n    \"Es gibt eine gro\u00dfe \u00c4hnlichkeit zwischen den W\u00f6rtern {word1} und {word2}, aber sie sind nicht exakt synonym.\",\n    'DieW\u00f6rter \"{word1}\" und \"{word2}\" weisen eine erhebliche \u00c4hnlichkeit auf, k\u00f6nnen aber nicht als vollwertige Synonyme betrachtet werden.',\n    \"Obwohl {word1} und {word2} nicht austauschbar sind, haben sie eine wesentliche \u00dcberlappung in der Bedeutung.\",\n]\nrandom_list_sim_it_a_75 = [\n    \"C'\u00e8 una grande somiglianza tra le parole {word1} e {word2}, ma non sono esattamente sinonimi.\",\n    'Le parole \"{word1}\" e \"{word2}\" condividono una notevole somiglianza, ma non possono essere considerate sinonimi veri e propri.',\n    \"Anche se {word1} e {word2} non sono interscambiabili, hanno una sostanziale sovrapposizione di significato.\",\n]\n\n# 50%;\nrandom_list_sim_en_a_50 = [\n    \"There is some connection between the words {word1} and {word2}, but they are not full-fledged synonyms.\",\n    'No, \"{word1}\" and \"{word2}\" are not synonymous, but they do have a slight semantic similarity.',\n    \"Yes, there is some connection between '{word1}' and '{word2}', but they cannot be called synonyms.\",\n]\nrandom_list_sim_ru_a_50 = [\n    \"\u041c\u0435\u0436\u0434\u0443 \u0441\u043b\u043e\u0432\u0430\u043c\u0438 {word1} \u0438 {word2} \u0435\u0441\u0442\u044c \u043d\u0435\u043a\u0430\u044f \u0441\u0432\u044f\u0437\u044c, \u043d\u043e \u043e\u043d\u0438 \u043d\u0435 \u044f\u0432\u043b\u044f\u044e\u0442\u0441\u044f \u043f\u043e\u043b\u043d\u043e\u0446\u0435\u043d\u043d\u044b\u043c\u0438 \u0441\u0438\u043d\u043e\u043d\u0438\u043c\u0430\u043c\u0438.\",\n    '\u041d\u0435\u0442, \"{word1}\" \u0438 \"{word2}\" \u043d\u0435 \u0441\u0438\u043d\u043e\u043d\u0438\u043c\u044b, \u043d\u043e \u0443 \u043d\u0438\u0445 \u0435\u0441\u0442\u044c \u043d\u0435\u0431\u043e\u043b\u044c\u0448\u043e\u0435 \u0441\u0435\u043c\u0430\u043d\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0435 \u0441\u0445\u043e\u0434\u0441\u0442\u0432\u043e.',\n    \"\u0414\u0430, \u043c\u0435\u0436\u0434\u0443 '{word1}' \u0438 '{word2}' \u0435\u0441\u0442\u044c \u043d\u0435\u043a\u043e\u0442\u043e\u0440\u0430\u044f \u0441\u0432\u044f\u0437\u044c, \u043d\u043e \u0438\u0445 \u043d\u0435\u043b\u044c\u0437\u044f \u043d\u0430\u0437\u0432\u0430\u0442\u044c \u0441\u0438\u043d\u043e\u043d\u0438\u043c\u0430\u043c\u0438.\",\n]\nrandom_list_sim_de_a_50 = [\n    \"Es gibt eine gewisse Verbindung zwischen den W\u00f6rtern {word1} und {word2}, aber sie sind keine vollwertigen Synonyme.\",\n    'Nein, \"{word1}\" und \"{word2}\" sind keine Synonyme, aber sie haben eine leichte semantische \u00c4hnlichkeit.',\n    \"Ja, es gibt eine Verbindung zwischen '{word1}' und '{word2}', aber man kann sie nicht als Synonyme bezeichnen.\",\n]\nrandom_list_sim_it_a_50 = [\n    \"C'\u00e8 una certa connessione tra le parole {word1} e {word2}, ma non sono sinonimi completi.\",\n    'No, \"{word1}\" e \"{word2}\" non sono sinonimi, ma hanno una leggera somiglianza semantica.',\n    \"S\u00ec, c'\u00e8 una connessione tra le parole '{word1}' e '{word2}', ma non possono essere chiamate sinonimi.\",\n]\n\n# 25%;\nrandom_list_sim_en_a_25 = [\n    \"No, {word1} and {word2} are not really synonymous, and they have very little conceptual meaning in common.\",\n    'The words \"{word1}\" and \"{word2}\" do not have the same meaning, and they share only a small amount of conceptual overlap.',\n    \"While there is some similarity between {word1} and {word2}, they cannot be considered synonyms as their conceptual meaning has very little overlap.\",\n]\nrandom_list_sim_ru_a_25 = [\n    \"\u041d\u0435\u0442, {word1} \u0438 {word2} \u043d\u0435 \u044f\u0432\u043b\u044f\u044e\u0442\u0441\u044f \u0441\u043e\u0432\u0441\u0435\u043c \u0441\u0438\u043d\u043e\u043d\u0438\u043c\u0430\u043c\u0438, \u0438 \u0443 \u043d\u0438\u0445 \u043e\u0447\u0435\u043d\u044c \u043c\u0430\u043b\u043e \u043e\u0431\u0449\u0435\u0433\u043e \u0432 \u043f\u043b\u0430\u043d\u0435 \u043a\u043e\u043d\u0446\u0435\u043f\u0442\u0443\u0430\u043b\u044c\u043d\u043e\u0433\u043e \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f.\",\n    '\u0421\u043b\u043e\u0432\u0430 \"{word1}\" \u0438 \"{word2}\" \u043d\u0435 \u0438\u043c\u0435\u044e\u0442 \u043e\u0434\u0438\u043d\u0430\u043a\u043e\u0432\u043e\u0433\u043e \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f, \u0438 \u0443 \u043d\u0438\u0445 \u0435\u0441\u0442\u044c \u0442\u043e\u043b\u044c\u043a\u043e \u043d\u0435\u0431\u043e\u043b\u044c\u0448\u043e\u0435 \u043a\u043e\u043d\u0446\u0435\u043f\u0442\u0443\u0430\u043b\u044c\u043d\u043e\u0435 \u043f\u0435\u0440\u0435\u0441\u0435\u0447\u0435\u043d\u0438\u0435.',\n    \"\u0425\u043e\u0442\u044f \u043c\u0435\u0436\u0434\u0443 {word1} \u0438 {word2} \u0435\u0441\u0442\u044c \u043d\u0435\u043a\u043e\u0442\u043e\u0440\u043e\u0435 \u0441\u0445\u043e\u0434\u0441\u0442\u0432\u043e, \u043e\u043d\u0438 \u043d\u0435 \u043c\u043e\u0433\u0443\u0442 \u0441\u0447\u0438\u0442\u0430\u0442\u044c\u0441\u044f \u0441\u0438\u043d\u043e\u043d\u0438\u043c\u0430\u043c\u0438, \u043f\u043e\u0441\u043a\u043e\u043b\u044c\u043a\u0443 \u0438\u0445 \u043a\u043e\u043d\u0446\u0435\u043f\u0442\u0443\u0430\u043b\u044c\u043d\u043e\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \u0438\u043c\u0435\u0435\u0442 \u043e\u0447\u0435\u043d\u044c \u043c\u0430\u043b\u043e \u043e\u0431\u0449\u0435\u0433\u043e.\",\n]\nrandom_list_sim_de_a_25 = [\n    \"Nein, {word1} und {word2} sind nicht wirklich Synonyme, und sie haben sehr wenig konzeptionelle Bedeutung gemeinsam.\",\n    'Die W\u00f6rter \"{word1}\" und \"{word2}\" haben nicht dieselbe Bedeutung, und sie teilen nur eine geringe konzeptuelle \u00dcberschneidung.',\n    \"Obwohl {word1} und {word2} einige \u00c4hnlichkeiten aufweisen, k\u00f6nnen sie nicht als Synonyme betrachtetwerden, da ihr konzeptuelles Bedeutungsfeld nur sehr wenig gemeinsam hat.\",\n]\nrandom_list_sim_it_a_25 = [\n    \"No, {word1} e {word2} non sono veri e propri sinonimi, e hanno molto poco in comune a livello concettuale.\",\n    'Le parole \"{word1}\" e \"{word2}\" non hanno lo stesso significato, e condividono solo una piccola quantit\u00e0 di sovrapposizione concettuale.',\n    \"Anche se c'\u00e8 una certa somiglianza tra {word1} e {word2}, non possono essere considerati sinonimi poich\u00e9 il loro campo semantico ha molto poco in comune.\",\n]\n\n# no;\nrandom_list_sim_en_a_n = [\n    \"No, the words {word1} and {word2} are not synonyms.\",\n    'No, \"{word1}\" and \"{word2}\" are not synonymous.',\n    \"There is no direct connection between '{word1}' and '{word2}'.\",\n]\nrandom_list_sim_ru_a_n = [\n    \"\u041d\u0435\u0442, \u0441\u043b\u043e\u0432\u0430 {word1} \u0438 {word2} \u043d\u0435 \u044f\u0432\u043b\u044f\u044e\u0442\u0441\u044f \u0441\u0438\u043d\u043e\u043d\u0438\u043c\u0430\u043c\u0438.\",\n    '\u041d\u0435\u0442, \"{word1}\" \u0438 \"{word2}\" \u043d\u0435 \u0441\u0438\u043d\u043e\u043d\u0438\u043c\u044b.',\n    \"\u041c\u0435\u0436\u0434\u0443 \u0441\u043b\u043e\u0432\u0430\u043c\u0438 '{word1}' \u0438 '{word2}' \u043d\u0435\u0442 \u043f\u0440\u044f\u043c\u043e\u0439 \u0441\u0432\u044f\u0437\u0438.\",\n]\nrandom_list_sim_de_a_n = [\n    \"Nein, die W\u00f6rter {word1} und {word2} sind keine Synonyme.\",\n    'Nein, \"{word1}\" und \"{word2}\" sind keine Synonyme.',\n    \"Es besteht keine direkte Verbindung zwischen '{word1}' und '{word2}'.\",\n]\n\nrandom_list_sim_it_a_n = [\n    \"No, le parole {word1} e {word2} non sono sinonimi.\",\n    'No, \"{word1}\" e \"{word2}\" non sono sinonimi.',\n    \"Non c'\u00e8 una connessione diretta tra '{word1}' e '{word2}'.\",\n]\n\n# rel_questions:\nrandom_list_rel_en_q = [\n    \"Are there any associations between the words {word1} and {word2}?\",\n    'Are the words \"{word1}\" and \"{word2}\" related?',\n    \"What is the association between '{word1}' and '{word2}'?\",\n]\nrandom_list_rel_ru_q = [\n    \"\u0415\u0441\u0442\u044c \u043b\u0438 \u0430\u0441\u0441\u043e\u0446\u0438\u0430\u0446\u0438\u0438 \u043c\u0435\u0436\u0434\u0443 \u0441\u043b\u043e\u0432\u0430\u043c\u0438 {word1} \u0438 {word2}?\",\n    '\u0421\u0432\u044f\u0437\u0430\u043d\u044b \u043b\u0438 \u043a\u0430\u043a-\u0442\u043e \u0441\u043b\u043e\u0432\u0430 \"{word1}\" \u0438 \"{word2}\"?',\n    \"\u041d\u0430\u0441\u043a\u043e\u043b\u044c\u043a\u043e \u0430\u0441\u0441\u043e\u0446\u0438\u0430\u0442\u0438\u0432\u043d\u043e \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \u0441\u043b\u043e\u0432 '{word1}' \u0438 '{word2}'?\",\n]\nrandom_list_rel_de_q = [\n    \"Gibt es Assoziationen zwischen den W\u00f6rtern {word1} und {word2}?\",\n    'Sind die W\u00f6rter \"{word1}\" und \"{word2}\" miteinander verwandt?',\n    \"Welche Verbindung besteht zwischen '{word1}' und '{word2}'?\",\n]\nrandom_list_rel_it_q = [\n    \"Ci sono associazioni tra le parole {word1} e {word2}?\",\n    'Esiste una relazione tra le coppie di parole \"{word1}\" e \"{word2}\"?',\n    \"Qual \u00e8 l'associazione tra '{word1}' e '{word2}'?\",\n]\n\n# rel_answers:\n\n# same words;\nrandom_list_rel_en_a_same = [\n    \"Yes, because it's the same word.\",\n    'Of course, we\\'re talking about the same word: \"{word1}\".',\n    \"You repeated '{word1}' twice.\",\n]\n\nrandom_list_rel_ru_a_same = [\n    \"\u0414\u0430, \u0432\u0435\u0434\u044c \u044d\u0442\u043e \u043e\u0434\u043d\u043e \u0438 \u0442\u043e \u0436\u0435 \u0441\u043b\u043e\u0432\u043e.\",\n    '\u041a\u043e\u043d\u0435\u0447\u043d\u043e, \u0432\u0435\u0434\u044c \u0440\u0435\u0447\u044c \u0438\u0434\u0451\u0442 \u043e\u0431 \u043e\u0434\u043d\u043e\u043c \u0441\u043b\u043e\u0432\u0435: \"{word1}\".',\n    \"\u0412\u044b \u043f\u043e\u0432\u0442\u043e\u0440\u0438\u043b\u0438 '{word1}' \u0434\u0432\u0430\u0436\u0434\u044b.\",\n]\n\nrandom_list_rel_de_a_same = [\n    \"Ja, denn es ist dasselbe Wort.\",\n    'Nat\u00fcrlich, wir sprechen \u00fcber dasselbe Wort: \"{word1}\".',\n    \"Du hast '{word1}' zweimal wiederholt.\",\n]\n\nrandom_list_rel_it_a_same = [\n    \"S\u00ec, perch\u00e9 \u00e8 la stessa parola.\",\n    'Certo, stiamo parlando della stessa parola: \"{word1}\".',\n    \"Hai ripetuto '{word1}' due volte.\",\n]\n\n# yes;\nrandom_list_rel_en_a_y = [\n    \"Yes, there is an association between the words {word1} and {word2}.\",\n    'Yes, there is an associative relationship between the words \"{word1}\" and \"{word2}\".',\n    \"There is a direct associative link between the words '{word1}' and '{word2}'.\",\n]\nrandom_list_rel_ru_a_y = [\n    \"\u0414\u0430, \u043c\u0435\u0436\u0434\u0443 \u0441\u043b\u043e\u0432\u0430\u043c\u0438 {word1} \u0438 {word2} \u043f\u0440\u043e\u0441\u043b\u0435\u0436\u0438\u0432\u0430\u0435\u0442\u0441\u044f \u0430\u0441\u0441\u043e\u0446\u0438\u0430\u0446\u0438\u044f.\",\n    '\u0414\u0430, \u043c\u0435\u0436\u0434\u0443 \u0441\u043b\u043e\u0432\u0430\u043c\u0438 \"{word1}\" \u0438 \"{word2}\" \u0435\u0441\u0442\u044c \u0430\u0441\u0441\u043e\u0446\u0438\u0430\u0442\u0438\u0432\u043d\u0430\u044f \u0441\u0432\u044f\u0437\u044c.',\n    \"\u0415\u0441\u0442\u044c \u043f\u0440\u044f\u043c\u0430\u044f \u0430\u0441\u0441\u043e\u0446\u0438\u0430\u0442\u0438\u0432\u043d\u0430\u044f \u0441\u0432\u044f\u0437\u044c \u043c\u0435\u0436\u0434\u0443 \u0441\u043b\u043e\u0432\u0430\u043c\u0438 '{word1}' \u0438 '{word2}'\",\n]\nrandom_list_rel_de_a_y = [\n    \"Ja, es gibt eine Assoziation zwischen den W\u00f6rtern {word1} und {word2}.\",\n    'Ja, es besteht eine assoziative Beziehung zwischen den W\u00f6rtern \"{word1}\" und \"{word2}\".',\n    \"Es gibt eine direkte assoziative Verbindung zwischen den W\u00f6rtern '{word1}' und '{word2}'.\",\n]\nrandom_list_rel_it_a_y = [\n    \"S\u00ec, c'\u00e8 un'associazione tra le parole {word1} e {word2}.\",\n    'S\u00ec, c\\'\u00e8 una relazione associativa tra le parole \"{word1}\" e \"{word2}\".',\n    \"C'\u00e8 un legame associativo diretto tra le parole '{word1}' e '{word2}'.\",\n]\n\n# 75%;\nrandom_list_rel_en_a_75 = [\n    \"There is a significant association between {word1} and {word2}, but the level of relatedness is not really high, about 75%.\",\n    'While \"{word1}\" and \"{word2}\" are related to some extent, their conceptual overlap is not very strong.',\n    \"There is a moderate association between '{word1}' and '{word2}', indicating that they are related a lot, but not completely.\",\n]\nrandom_list_rel_ru_a_75 = [\n    \"\u041c\u0435\u0436\u0434\u0443 \u0441\u043b\u043e\u0432\u0430\u043c\u0438 {word1} \u0438 {word2} \u0441\u0443\u0449\u0435\u0441\u0442\u0432\u0443\u0435\u0442 \u0437\u043d\u0430\u0447\u0438\u0442\u0435\u043b\u044c\u043d\u0430\u044f \u0441\u0432\u044f\u0437\u044c, \u043d\u043e \u0443\u0440\u043e\u0432\u0435\u043d\u044c \u0441\u0432\u044f\u0437\u0430\u043d\u043d\u043e\u0441\u0442\u0438 \u043d\u0435 \u043f\u0440\u0435\u0432\u044b\u0448\u0430\u0435\u0442 75%.\",\n    '\u0425\u043e\u0442\u044f \u0441\u043b\u043e\u0432\u0430 \"{word1}\" \u0438 \"{word2}\" \u0438\u043c\u0435\u044e\u0442 \u043d\u0435\u043a\u043e\u0442\u043e\u0440\u0443\u044e \u0441\u0432\u044f\u0437\u044c, \u0438\u0445 \u043a\u043e\u043d\u0446\u0435\u043f\u0442\u0443\u0430\u043b\u044c\u043d\u043e\u0435 \u0441\u0445\u043e\u0434\u0441\u0442\u0432\u043e \u043d\u0435 \u0442\u0430\u043a \u0441\u0438\u043b\u044c\u043d\u043e \u0432\u044b\u0441\u043e\u043a\u043e, \u0447\u0442\u043e\u0431\u044b \u0438\u0445 \u043c\u043e\u0436\u043d\u043e \u0431\u044b\u043b\u043e \u043d\u0430\u0437\u0432\u0430\u0442\u044c \u043f\u043e\u043b\u043d\u043e\u0441\u0442\u044c\u044e \u0430\u0441\u0441\u043e\u0446\u0438\u0430\u0442\u0438\u0432\u043d\u044b\u043c\u0438.',\n    \"\u0421\u0443\u0449\u0435\u0441\u0442\u0432\u0443\u0435\u0442 \u0443\u043c\u0435\u0440\u0435\u043d\u043d\u0430\u044f \u0441\u0432\u044f\u0437\u044c \u043c\u0435\u0436\u0434\u0443 \u0441\u043b\u043e\u0432\u0430\u043c\u0438 '{word1}' \u0438 '{word2}', \u0447\u0442\u043e \u0443\u043a\u0430\u0437\u044b\u0432\u0430\u0435\u0442 \u043d\u0430 \u0442\u043e, \u0447\u0442\u043e \u043e\u043d\u0438 \u0441\u0438\u043b\u044c\u043d\u043e \u0441\u0432\u044f\u0437\u0430\u043d\u044b \u043c\u0435\u0436\u0434\u0443 \u0441\u043e\u0431\u043e\u0439, \u043d\u043e \u043d\u0435 \u043f\u043e\u043b\u043d\u043e\u0441\u0442\u044c\u044e.\",\n]\nrandom_list_rel_de_a_75 = [\n    \"Es besteht eine signifikante Assoziation zwischen {word1} und {word2}, aber das Ma\u00df der Verwandtschaft ist nicht sehr hoch.\",\n    'Obwohl \"{word1}\" und \"{word2}\" in gewisser Weise miteinander verbunden sind, ist ihre konzeptuelle \u00dcberlappung nicht sehr stark.',\n    \"Es besteht eine m\u00e4\u00dfige Assoziation zwischen '{word1}' und '{word2}', was darauf hinweist, dass sie stark miteinander verbunden sind, aber nicht vollst\u00e4ndig.\",\n]\nrandom_list_rel_it_a_75 = [\n    \"C'\u00e8 una significativa associazione tra {word1} e {word2}, ma il livello di relazione non \u00e8 molto alto.\",\n    'Anche se \"{word1}\" e \"{word2}\" sono in qualche modo correlati, il loro sovrapporsi concettuale non \u00e8 molto forte.',\n    \"C'\u00e8 una moderata associazione tra '{word1}' e '{word2}', indicando che sono molto correlati, ma non completamente.\",\n]\n\n# 50%;\nrandom_list_rel_en_a_50 = [\n    \"There is a slight association between the words {word1} and {word2}.\",\n    'There is an indirect semantic similarity between the words \"{word1}\" and \"{word2}\".',\n    \"There is some association between the words '{word1}' and '{word2}'.\",\n]\nrandom_list_rel_ru_a_50 = [\n    \"\u0415\u0441\u0442\u044c \u043d\u0435\u0431\u043e\u043b\u044c\u0448\u0430\u044f \u0430\u0441\u0441\u043e\u0446\u0438\u0430\u0446\u0438\u044f \u043c\u0435\u0436\u0434\u0443 \u0441\u043b\u043e\u0432\u0430\u043c\u0438 {word1} \u0438 {word2}.\",\n    '\u041c\u0435\u0436\u0434\u0443 \u0441\u043b\u043e\u0432\u0430\u043c\u0438 \"{word1}\" \u0438 \"{word2}\" \u0435\u0441\u0442\u044c \u043d\u0435\u043f\u0440\u044f\u043c\u043e\u0435 \u0441\u0435\u043c\u0430\u043d\u0442\u0438\u0447\u0435\u0441\u043a\u043e\u0435 \u0441\u0445\u043e\u0434\u0441\u0442\u0432\u043e.',\n    \"\u0415\u0441\u0442\u044c \u043d\u0435\u043a\u043e\u0442\u043e\u0440\u0430\u044f \u0430\u0441\u0441\u043e\u0446\u0438\u0430\u0442\u0438\u0432\u043d\u043e\u0441\u0442\u044c \u043c\u0435\u0436\u0434\u0443 \u0441\u043b\u043e\u0432\u0430\u043c\u0438 '{word1}' \u0438 '{word2}'.\",\n]\nrandom_list_rel_de_a_50 = [\n    \"Es besteht eine leichte Assoziation zwischen den W\u00f6rtern {word1} und {word2}.\",\n    'Es gibt eine indirekte semantische \u00c4hnlichkeit zwischen den W\u00f6rtern \"{word1}\" und \"{word2}\".',\n    \"Es besteht eine gewisse Verbindung zwischen den W\u00f6rtern '{word1}' und '{word2}'.\",\n]\nrandom_list_rel_it_a_50 = [\n    \"C'\u00e8 una leggera associazione tra le parole {word1} e {word2}.\",\n    'C\\'\u00e8 una somiglianza semantica indiretta tra le parole \"{word1}\" e \"{word2}\".',\n    \"C'\u00e8 una certa associazione tra le parole '{word1}' e '{word2}'.\",\n]\n\n# 25%;\nrandom_list_rel_en_a_25 = [\n    \"There is very little conceptual related meaning in common between {word1} and {word2}, with a low level of relatedness.\",\n    \"The association between {word1} and {word2} is weak, suggesting that they are not very related.\",\n    \"While there is some association between {word1} and {word2}, the level of relatedness is quite low.\",\n]\nrandom_list_rel_ru_a_25 = [\n    \"\u041c\u0435\u0436\u0434\u0443 \u0441\u043b\u043e\u0432\u0430\u043c\u0438 {word1} \u0438 {word2} \u043e\u0447\u0435\u043d\u044c \u043c\u0430\u043b\u043e \u043e\u0431\u0449\u0435\u0433\u043e \u0432 \u043f\u043b\u0430\u043d\u0435 \u043a\u043e\u043d\u0446\u0435\u043f\u0442\u0443\u0430\u043b\u044c\u043d\u043e\u0439 \u0441\u0432\u044f\u0437\u0438, \u0443\u0440\u043e\u0432\u0435\u043d\u044c \u0441\u0432\u044f\u0437\u0430\u043d\u043d\u043e\u0441\u0442\u0438 \u043d\u0438\u0437\u043a\u0438\u0439.\",\n    \"\u0421\u0432\u044f\u0437\u044c \u043c\u0435\u0436\u0434\u0443 \u0441\u043b\u043e\u0432\u0430\u043c\u0438 {word1} \u0438 {word2} \u0441\u043b\u0430\u0431\u0430\u044f, \u0447\u0442\u043e \u0443\u043a\u0430\u0437\u044b\u0432\u0430\u0435\u0442 \u043d\u0430 \u0442\u043e, \u0447\u0442\u043e \u043e\u043d\u0438 \u043d\u0435 \u043e\u0447\u0435\u043d\u044c \u0441\u0432\u044f\u0437\u0430\u043d\u044b \u043c\u0435\u0436\u0434\u0443 \u0441\u043e\u0431\u043e\u0439.\",\n    \"\u0425\u043e\u0442\u044f \u043c\u0435\u0436\u0434\u0443 \u0441\u043b\u043e\u0432\u0430\u043c\u0438 {word1} \u0438 {word2} \u0435\u0441\u0442\u044c \u043d\u0435\u043a\u043e\u0442\u043e\u0440\u0430\u044f \u0441\u0432\u044f\u0437\u044c, \u0443\u0440\u043e\u0432\u0435\u043d\u044c \u0441\u0432\u044f\u0437\u0430\u043d\u043d\u043e\u0441\u0442\u0438 \u0434\u043e\u0432\u043e\u043b\u044c\u043d\u043e \u043d\u0438\u0437\u043a\u0438\u0439.\",\n]\nrandom_list_rel_de_a_25 = [\n    \"Es gibt sehr wenig konzeptuell verwandte Bedeutung zwischen den W\u00f6rtern {word1} und {word2}, mit einem niedrigen Verwandtheitsgrad.\",\n    \"Die Assoziation zwischen {word1} und {word2} ist schwach, was darauf hindeutet, dass sie nicht sehr verwandt sind.\",\n    \"Obwohl es eine gewisse Assoziation zwischen {word1} und {word2} gibt, ist das Ma\u00df der Verwandtschaft recht gering.\",\n]\nrandom_list_rel_it_a_25 = [\n    \"C'\u00e8 molto poco significato concettualmente correlato tra {word1} e {word2}, con un basso livello di correlazione.\",\n    \"L'associazione tra {word1} e {word2} \u00e8 debole, suggerendo che non sono molto correlati.\",\n    \"Anche se c'\u00e8 una certa associazione tra {word1} e {word2}, il livello di correlazione \u00e8 piuttosto basso.\",\n]\n\n# no;\nrandom_list_rel_en_a_n = [\n    \"No, there is no associative relationship between the words {word1} and {word2}\",\n    'No, the words \"{word1}\" and \"{word2}\" are not related in any direct associative way.',\n    \"There is no direct associative relationship between '{word1}' and '{word2}'.\",\n]\nrandom_list_rel_ru_a_n = [\n    \"\u041d\u0435\u0442, \u043c\u0435\u0436\u0434\u0443 \u0441\u043b\u043e\u0432\u0430\u043c\u0438 {word1} \u0438 {word2} \u043d\u0435\u0442 \u0430\u0441\u0441\u043e\u0446\u0438\u0430\u0442\u0438\u0432\u043d\u043e\u0439 \u0441\u0432\u044f\u0437\u0438\",\n    '\u041d\u0435\u0442, \u0441\u043b\u043e\u0432\u0430 \"{word1}\" \u0438 \"{word2}\" \u043d\u0435 \u0441\u0432\u044f\u0437\u0430\u043d\u044b \u043a\u0430\u043a\u0438\u043c-\u043b\u0438\u0431\u043e \u043f\u0440\u044f\u043c\u044b\u043c \u0430\u0441\u0441\u043e\u0446\u0438\u0430\u0442\u0438\u0432\u043d\u044b\u043c \u043e\u0431\u0440\u0430\u0437\u043e\u043c.',\n    \"\u041d\u0435\u0442 \u043d\u0438\u043a\u0430\u043a\u043e\u0439 \u043f\u0440\u044f\u043c\u043e\u0439 \u0430\u0441\u0441\u043e\u0446\u0438\u0430\u0442\u0438\u0432\u043d\u043e\u0439 \u0441\u0432\u044f\u0437\u0438 \u043c\u0435\u0436\u0434\u0443 '{word1}' \u0438 '{word2}'.\",\n]\nrandom_list_rel_de_a_n = [\n    \"Nein, es besteht keine Assoziationsbeziehung zwischen den W\u00f6rtern {word1} und {word2}.\",\n    'Nein, die W\u00f6rter \"{word1}\" und \"{word2}\" sind nicht in direkter assoziativer Weise miteinander verbunden.',\n    \"Es besteht keine direkte assoziative Beziehung zwischen '{word1}' und '{word2}'.\",\n]\nrandom_list_rel_it_a_n = [\n    \"No, non c'\u00e8 alcuna relazione associativa tra le parole {word1} e {word2}.\",\n    'No, le parole \"{word1}\" e \"{word2}\" non sono correlate in alcun modo associativo diretto.',\n    \"Non c'\u00e8 una diretta relazione associativa tra '{word1}' e '{word2}'.\",\n]\n\n# easy-way to call stuff above\n\n# dicts of q\n# sim\nrandom_dict_sim_q = {\n    \"en\": random_list_sim_en_q,\n    \"ru\": random_list_sim_ru_q,\n    \"de\": random_list_sim_de_q,\n    \"it\": random_list_sim_it_q,\n}\n# rel\nrandom_dict_rel_q = {\n    \"en\": random_list_rel_en_q,\n    \"ru\": random_list_rel_ru_q,\n    \"de\": random_list_rel_de_q,\n    \"it\": random_list_rel_it_q,\n}\n\n# dicts for a\n# sim - random_dict_sim_a[\"ru\"][0\nrandom_dict_sim_a = {\n    \"en\": {\n        0: random_list_sim_en_a_same,\n        1: random_list_sim_en_a_y,\n        2: random_list_sim_en_a_75,\n        3: random_list_sim_en_a_50,\n        4: random_list_sim_en_a_25,\n        5: random_list_sim_en_a_n,\n    },\n    \"ru\": {\n        0: random_list_sim_ru_a_same,\n        1: random_list_sim_ru_a_y,\n        2: random_list_sim_ru_a_75,\n        3: random_list_sim_ru_a_50,\n        4: random_list_sim_ru_a_25,\n        5: random_list_sim_ru_a_n,\n    },\n    \"de\": {\n        0: random_list_sim_de_a_same,\n        1: random_list_sim_de_a_y,\n        2: random_list_sim_de_a_75,\n        3: random_list_sim_de_a_50,\n        4: random_list_sim_de_a_25,\n        5: random_list_sim_de_a_n,\n    },\n    \"it\": {\n        0: random_list_sim_it_a_same,\n        1: random_list_sim_it_a_y,\n        2: random_list_sim_it_a_75,\n        3: random_list_sim_it_a_50,\n        4: random_list_sim_it_a_25,\n        5: random_list_sim_it_a_n,\n    },\n}\n# rel\nrandom_dict_rel_a = {\n    \"en\": {\n        0: random_list_rel_en_a_same,\n        1: random_list_rel_en_a_y,\n        2: random_list_rel_en_a_75,\n        3: random_list_rel_en_a_50,\n        4: random_list_rel_en_a_25,\n        5: random_list_rel_en_a_n,\n    },\n    \"ru\": {\n        0: random_list_rel_ru_a_same,\n        1: random_list_rel_ru_a_y,\n        2: random_list_rel_ru_a_75,\n        3: random_list_rel_ru_a_50,\n        4: random_list_rel_ru_a_25,\n        5: random_list_rel_ru_a_n,\n    },\n    \"de\": {\n        0: random_list_rel_de_a_same,\n        1: random_list_rel_de_a_y,\n        2: random_list_rel_de_a_75,\n        3: random_list_rel_de_a_50,\n        4: random_list_rel_de_a_25,\n        5: random_list_rel_de_a_n,\n    },\n    \"it\": {\n        0: random_list_rel_it_a_same,\n        1: random_list_rel_it_a_y,\n        2: random_list_rel_it_a_75,\n        3: random_list_rel_it_a_50,\n        4: random_list_rel_it_a_25,\n        5: random_list_rel_it_a_n,\n    },\n}\n\n\n# 0 - same, 1 - yes, 2 - 75, 3 - 50, 4 - 25, 5 - no\ndef qna_random_magic(lang, word1, word2, con_type, score_percent, random_num, isQuestion):\n    if con_type == \"sim\":\n        instruction = random_dict_sim_q[lang][random_num].format(word1=word1, word2=word2)\n    else:\n        instruction = random_dict_rel_q[lang][random_num].format(word1=word1, word2=word2)\n    if score_percent < 1.85 and con_type == \"sim\":\n        response = random_dict_sim_a[lang][5][random_num].format(word1=word1, word2=word2)\n    elif score_percent < 1.85 and con_type == \"rel\":\n        response = random_dict_rel_a[lang][5][random_num].format(word1=word1, word2=word2)\n    elif score_percent < 3.85 and con_type == \"sim\":\n        response = random_dict_sim_a[lang][4][random_num].format(word1=word1, word2=word2)\n    elif score_percent < 3.85 and con_type == \"rel\":\n        response = random_dict_rel_a[lang][4][random_num].format(word1=word1, word2=word2)\n    elif score_percent < 6.3 and con_type == \"sim\":\n        response = random_dict_sim_a[lang][3][random_num].format(word1=word1, word2=word2)\n    elif score_percent < 6.3 and con_type == \"rel\":\n        response = random_dict_rel_a[lang][3][random_num].format(word1=word1, word2=word2)\n    elif score_percent < 8.85 and con_type == \"sim\":\n        response = random_dict_sim_a[lang][2][random_num].format(word1=word1, word2=word2)\n    elif score_percent < 8.85 and con_type == \"rel\":\n        response = random_dict_rel_a[lang][2][random_num].format(word1=word1, word2=word2)\n    elif score_percent < 10 and con_type == \"sim\":\n        response = random_dict_sim_a[lang][1][random_num].format(word1=word1, word2=word2)\n    elif score_percent < 10 and con_type == \"rel\":\n        response = random_dict_rel_a[lang][1][random_num].format(word1=word1, word2=word2)\n    elif score_percent == 10 and con_type == \"sim\":\n        response = random_dict_sim_a[lang][0][random_num].format(word1=word1, word2=word2)\n    elif score_percent == 10 and con_type == \"rel\":\n        response = random_dict_rel_a[lang][0][random_num].format(word1=word1, word2=word2)\n    else:\n        assert \"Error\"\n    return instruction if isQuestion else response\n", "oasst-data/oasst_data/writer.py": "import gzip\nimport json\nfrom datetime import datetime\nfrom pathlib import Path\nfrom typing import Iterable, TextIO\n\nfrom oasst_data.schemas import ExportMessageNode, ExportMessageTree\n\n\ndef default_serializer(obj):\n    \"\"\"JSON serializer for objects not serializable by default json code\"\"\"\n    if isinstance(obj, datetime):\n        return obj.isoformat()\n    raise TypeError(\"Type %s not serializable\" % type(obj))\n\n\ndef open_jsonl_write(file_name: str | Path) -> TextIO:\n    file_name = Path(file_name)\n    if file_name.suffix == \".gz\":\n        return gzip.open(str(file_name), mode=\"wt\", encoding=\"UTF-8\")\n    else:\n        return file_name.open(\"w\", encoding=\"UTF-8\")\n\n\ndef write_tree(\n    file: TextIO,\n    tree: ExportMessageTree,\n    exclude_none: bool = False,\n) -> None:\n    json.dump(tree.dict(exclude_none=exclude_none), file, default=default_serializer)\n    file.write(\"\\n\")\n\n\ndef write_message_trees(\n    output_file_name: str | Path,\n    trees: Iterable[ExportMessageTree],\n    exclude_none: bool,\n) -> None:\n    with open_jsonl_write(output_file_name) as file:\n        # write one tree per line\n        for tree in trees:\n            write_tree(file, tree)\n\n\ndef write_message(\n    file: TextIO,\n    message: ExportMessageNode,\n    exclude_none: bool = False,\n) -> None:\n    message = message.copy(deep=False, exclude={\"replies\"})\n    json.dump(\n        message.dict(exclude_none=exclude_none),\n        file,\n        default=default_serializer,\n    )\n    file.write(\"\\n\")\n\n\ndef write_messages(\n    output_file_name: str | Path,\n    messages: Iterable[ExportMessageNode],\n    exclude_none: bool,\n) -> None:\n    with open_jsonl_write(output_file_name) as file:\n        # write one message per line\n        for message in messages:\n            write_message(file, message, exclude_none)\n", "oasst-data/oasst_data/traversal.py": "from typing import Callable, Optional\n\nfrom .schemas import ExportMessageNode\n\n\ndef visit_threads_depth_first(\n    node: ExportMessageNode,\n    visitor: Callable[[list[ExportMessageNode]], None],\n    predicate: Optional[Callable[[list[ExportMessageNode]], bool]] = None,\n    parents: list[ExportMessageNode] = None,\n):\n    parents = parents or []\n    if not node:\n        return\n    thread = parents + [node]\n    if predicate is None or predicate(thread):\n        visitor(thread)\n    if node.replies:\n        parents = thread\n        for c in node.replies:\n            visit_threads_depth_first(node=c, visitor=visitor, predicate=predicate, parents=parents)\n\n\ndef visit_messages_depth_first(\n    node: ExportMessageNode,\n    visitor: Callable[[ExportMessageNode], None],\n    predicate: Optional[Callable[[ExportMessageNode], bool]] = None,\n):\n    if not node:\n        return\n    if predicate is None or predicate(node):\n        visitor(node)\n    if node.replies:\n        for c in node.replies:\n            visit_messages_depth_first(node=c, visitor=visitor, predicate=predicate)\n", "oasst-data/oasst_data/reader.py": "import gzip\nimport json\nfrom pathlib import Path\nfrom typing import Callable, Iterable, Optional, TextIO\n\nimport pydantic\nfrom datasets import load_dataset\n\nfrom .schemas import ExportMessageNode, ExportMessageTree\n\n\ndef open_jsonl_read(input_file_path: str | Path) -> TextIO:\n    if not isinstance(input_file_path, Path):\n        input_file_path = Path(input_file_path)\n    if input_file_path.suffix == \".gz\":\n        return gzip.open(str(input_file_path), mode=\"tr\", encoding=\"UTF-8\")\n    else:\n        return input_file_path.open(\"r\", encoding=\"UTF-8\")\n\n\ndef read_oasst_obj(obj_dict: dict) -> ExportMessageTree | ExportMessageNode:\n    # validate data\n    if \"message_id\" in obj_dict:\n        return pydantic.parse_obj_as(ExportMessageNode, obj_dict)\n    elif \"message_tree_id\" in obj_dict:\n        return pydantic.parse_obj_as(ExportMessageTree, obj_dict)\n\n    raise RuntimeError(\"Unknown object in jsonl file\")\n\n\ndef read_oasst_jsonl(\n    input_file_path: str | Path,\n) -> Iterable[ExportMessageTree | ExportMessageNode]:\n    with open_jsonl_read(input_file_path) as file_in:\n        # read one object per line\n        for line in file_in:\n            dict_tree = json.loads(line)\n            yield read_oasst_obj(dict_tree)\n\n\ndef read_message_trees(input_file_path: str | Path) -> Iterable[ExportMessageTree]:\n    for x in read_oasst_jsonl(input_file_path):\n        assert isinstance(x, ExportMessageTree)\n        yield x\n\n\ndef read_message_tree_list(\n    input_file_path: str | Path,\n    filter: Optional[Callable[[ExportMessageTree], bool]] = None,\n) -> list[ExportMessageTree]:\n    return [t for t in read_message_trees(input_file_path) if not filter or filter(t)]\n\n\ndef convert_hf_message(row: dict) -> None:\n    emojis = row.get(\"emojis\")\n    if emojis:\n        row[\"emojis\"] = dict(zip(emojis[\"name\"], emojis[\"count\"]))\n    labels = row.get(\"labels\")\n    if labels:\n        row[\"labels\"] = {\n            name: {\"value\": value, \"count\": count}\n            for name, value, count in zip(labels[\"name\"], labels[\"value\"], labels[\"count\"])\n        }\n\n\ndef read_messages(input_file_path: str | Path) -> Iterable[ExportMessageNode]:\n    for x in read_oasst_jsonl(input_file_path):\n        assert isinstance(x, ExportMessageNode)\n        yield x\n\n\ndef read_message_list(\n    input_file_path: str | Path,\n    filter: Optional[Callable[[ExportMessageNode], bool]] = None,\n) -> list[ExportMessageNode]:\n    return [t for t in read_messages(input_file_path) if not filter or filter(t)]\n\n\ndef read_dataset_message_trees(\n    hf_dataset_name: str = \"OpenAssistant/oasst1\",\n    split: str = \"train+validation\",\n) -> Iterable[ExportMessageTree]:\n    dataset = load_dataset(hf_dataset_name, split=split)\n\n    tree_dict: dict = None\n    parents: list = None\n    for row in dataset:\n        convert_hf_message(row)\n        if row[\"parent_id\"] is None:\n            if tree_dict:\n                tree = read_oasst_obj(tree_dict)\n                assert isinstance(tree, ExportMessageTree)\n                yield tree\n\n            tree_dict = {\n                \"message_tree_id\": row[\"message_id\"],\n                \"tree_state\": row[\"tree_state\"],\n                \"prompt\": row,\n            }\n            parents = []\n        else:\n            while parents[-1][\"message_id\"] != row[\"parent_id\"]:\n                parents.pop()\n            parent = parents[-1]\n            if \"replies\" not in parent:\n                parent[\"replies\"] = []\n            parent[\"replies\"].append(row)\n\n        row.pop(\"message_tree_id\", None)\n        row.pop(\"tree_state\", None)\n        parents.append(row)\n\n    if tree_dict:\n        tree = read_oasst_obj(tree_dict)\n        assert isinstance(tree, ExportMessageTree)\n        yield tree\n\n\ndef read_dataset_messages(\n    hf_dataset_name: str = \"OpenAssistant/oasst1\",\n    split: str = \"train+validation\",\n) -> Iterable[ExportMessageNode]:\n    dataset = load_dataset(hf_dataset_name, split=split)\n\n    for row in dataset:\n        convert_hf_message(row)\n        message = read_oasst_obj(row)\n        assert isinstance(message, ExportMessageNode)\n        yield message\n", "oasst-data/oasst_data/__init__.py": "from oasst_data.reader import (\n    read_dataset_message_trees,\n    read_dataset_messages,\n    read_message_list,\n    read_message_tree_list,\n    read_message_trees,\n    read_messages,\n)\nfrom oasst_data.schemas import (\n    ExportMessageEvent,\n    ExportMessageEventEmoji,\n    ExportMessageEventRanking,\n    ExportMessageEventRating,\n    ExportMessageEventReport,\n    ExportMessageEventScore,\n    ExportMessageNode,\n    ExportMessageTree,\n    LabelAvgValue,\n    LabelValues,\n)\nfrom oasst_data.traversal import visit_messages_depth_first, visit_threads_depth_first\nfrom oasst_data.writer import write_message_trees, write_messages\n\n__all__ = [\n    \"LabelAvgValue\",\n    \"LabelValues\",\n    \"ExportMessageEvent\",\n    \"ExportMessageEventEmoji\",\n    \"ExportMessageEventRating\",\n    \"ExportMessageEventRanking\",\n    \"ExportMessageEventReport\",\n    \"ExportMessageEventScore\",\n    \"ExportMessageNode\",\n    \"ExportMessageTree\",\n    \"read_message_trees\",\n    \"read_message_tree_list\",\n    \"read_messages\",\n    \"read_message_list\",\n    \"visit_threads_depth_first\",\n    \"visit_messages_depth_first\",\n    \"write_message_trees\",\n    \"write_messages\",\n    \"read_dataset_message_trees\",\n    \"read_dataset_messages\",\n]\n", "oasst-data/oasst_data/schemas.py": "from __future__ import annotations\n\nfrom datetime import datetime\nfrom typing import Literal, Optional\n\nfrom pydantic import BaseModel, conint\n\n\nclass LabelAvgValue(BaseModel):\n    value: float | None\n    count: int\n\n\nLabelValues = dict[str, LabelAvgValue]\n\n\nclass ExportMessageEvent(BaseModel):\n    type: str\n    user_id: str | None\n\n\nclass ExportMessageEventEmoji(ExportMessageEvent):\n    type: Literal[\"emoji\"] = \"emoji\"\n    emoji: str\n\n\nclass ExportMessageEventRating(ExportMessageEvent):\n    type: Literal[\"rating\"] = \"rating\"\n    rating: str\n\n\nclass ExportMessageEventRanking(ExportMessageEvent):\n    type: Literal[\"ranking\"] = \"ranking\"\n    ranking: list[int]\n    ranked_message_ids: list[str]\n    ranking_parent_id: Optional[str]\n    message_tree_id: Optional[str]\n    not_rankable: Optional[bool]  # flawed, factually incorrect or unacceptable\n\n\nclass ExportMessageEventReport(ExportMessageEvent):\n    type: Literal[\"report\"] = \"report\"\n    report_type: str\n    reason: str\n\n\nclass ExportMessageEventScore(ExportMessageEvent):\n    type: Literal[\"score\"] = \"score\"\n    score: conint(ge=-1, le=1)\n\n\nclass DetoxifyRating(BaseModel):\n    toxicity: float\n    severe_toxicity: float\n    obscene: float\n    identity_attack: float\n    insult: float\n    threat: float\n    sexual_explicit: float\n\n\nclass ExportMessageNode(BaseModel):\n    message_id: str\n    parent_id: str | None\n    user_id: str | None\n    created_date: datetime | None\n    text: str\n    role: str\n    lang: str | None\n    review_count: int | None\n    review_result: bool | None\n    deleted: bool | None\n    rank: int | None\n    synthetic: bool | None\n    model_name: str | None\n    emojis: dict[str, int] | None\n    replies: list[ExportMessageNode] | None\n    labels: LabelValues | None\n    events: dict[str, list[ExportMessageEvent]] | None\n    detoxify: DetoxifyRating | None\n    # the following fields are always None in message tree exports (see outer tree there)\n    message_tree_id: str | None\n    tree_state: str | None\n\n    def get_label_value(self, name: str) -> float | None:\n        if self.labels and (avg_val := self.labels.get(name)):\n            return avg_val.value\n        return None\n\n\nclass ExportMessageTree(BaseModel):\n    message_tree_id: str\n    tree_state: Optional[str]\n    prompt: Optional[ExportMessageNode]\n    origin: Optional[str]\n", "oasst-data/examples/tree_to_messages.py": "import argparse\n\nfrom oasst_data import ExportMessageNode, read_message_trees, visit_messages_depth_first, write_messages\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\"tree_to_messages\")\n    parser.add_argument(\n        \"input_file_name\",\n        type=str,\n        help=\"path to input .jsonl or .jsonl.gz input file\",\n    )\n    parser.add_argument(\n        \"output_file_name\",\n        type=str,\n        help=\"path to output .jsonl or .jsonl.gz file\",\n    )\n    parser.add_argument(\"--exclude-nulls\", action=\"store_true\", default=False)\n    args = parser.parse_args()\n    return args\n\n\ndef main():\n    \"\"\"Read oasst message-trees from input file and generate a flat messages table output file.\"\"\"\n    args = parse_args()\n\n    # read all messages of input file into a list\n    messages: list[ExportMessageNode] = []\n    print(f\"reading: {args.input_file_name}\")\n    tree_count = 0\n    for message_tree in read_message_trees(args.input_file_name):\n\n        def append_with_tree_state(msg: ExportMessageNode):\n            msg.tree_state = message_tree.tree_state\n            msg.message_tree_id = message_tree.message_tree_id\n            messages.append(msg)\n\n        visit_messages_depth_first(message_tree.prompt, append_with_tree_state)\n        tree_count += 1\n    print(f\"{tree_count} trees with {len(messages)} total messages read.\")\n\n    # write messages file\n    print(f\"writing: {args.output_file_name}\")\n    write_messages(args.output_file_name, messages, args.exclude_nulls)\n    print(f\"{len(messages)} messages written.\")\n\n\nif __name__ == \"__main__\":\n    main()\n", "oasst-data/examples/filter_messages.py": "import argparse\nimport json\n\nfrom oasst_data import read_message_list, write_messages\nfrom oasst_data.schemas import ExportMessageNode\nfrom oasst_data.writer import open_jsonl_write\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\"filter_messages\")\n    parser.add_argument(\n        \"input_file_name\",\n        type=str,\n        help=\"path to input .jsonl or .jsonl.gz input file\",\n    )\n    parser.add_argument(\n        \"output_file_name\",\n        type=str,\n        help=\"path to output .jsonl or .jsonl.gz file\",\n    )\n    parser.add_argument(\n        \"--include-deleted\",\n        action=\"store_true\",\n        help=\"Include deleted messages in export\",\n    )\n    parser.add_argument(\n        \"--deleted-only\",\n        action=\"store_true\",\n        help=\"Export only deleted messages (implies --include-deleted)\",\n    )\n    parser.add_argument(\n        \"--include-spam\",\n        action=\"store_true\",\n        help=\"Export including messages with no review or negative review result.\",\n    )\n    parser.add_argument(\n        \"--spam-only\",\n        action=\"store_true\",\n        help=\"Export only messages with negative review result (implies --include-spam).\",\n    )\n    parser.add_argument(\n        \"--exclude-normal\",\n        action=\"store_true\",\n        help=\"exclude non-deleted non-synthetic messages with positive review\",\n        default=False,\n    )\n    parser.add_argument(\n        \"--include-synthetic\",\n        action=\"store_true\",\n        help=\"Include synthetic messages in export\",\n    )\n    parser.add_argument(\n        \"--synthetic-only\",\n        action=\"store_true\",\n        help=\"Export only synthetic messages (implies --include-synth)\",\n    )\n    parser.add_argument(\n        \"--user\",\n        type=str,\n        help=\"Only export trees involving the user with the specified ID. Incompatible with --state.\",\n    )\n    parser.add_argument(\n        \"--state\",\n        type=str,\n        help=\"all|prompt_lottery_waiting|growing|ready_for_export|aborted_low_grade|halted_by_moderator|backlog_ranking\",\n    )\n    parser.add_argument(\n        \"--lang\",\n        type=str,\n        help=\"Filter message trees by language code (BCP 47)\",\n    )\n    parser.add_argument(\n        \"--prompts-only\",\n        action=\"store_true\",\n        help=\"Export a list of initial prompt messages\",\n    )\n    parser.add_argument(\n        \"--export-text-only\",\n        action=\"store_true\",\n        help=\"Write jsonl file with message text strings only\",\n    )\n    parser.add_argument(\"--exclude-nulls\", action=\"store_true\", default=False)\n    args = parser.parse_args()\n    return args\n\n\ndef main():\n    args = parse_args()\n\n    deleted: bool | None = False\n    spam: bool | None = False\n    synthetic: bool | None = False\n    langs: list[str] | None = None\n    states: list[str] | None = None\n    prompts_only: bool = args.prompts_only\n    exclude_normal: bool = args.exclude_normal\n\n    if args.include_deleted:\n        deleted = None\n    elif args.deleted_only:\n        deleted = True\n\n    if args.include_spam:\n        spam = None\n    elif args.spam_only:\n        spam = True\n\n    if args.include_synthetic:\n        synthetic = None\n    elif args.synthetic_only:\n        synthetic = True\n\n    if args.lang:\n        langs = args.lang.split(\",\")\n\n    if args.state:\n        states = args.state.split(\",\")\n\n    def approve_message(msg: ExportMessageNode) -> bool:\n        if (\n            (deleted is not None and msg.deleted != deleted)\n            or (synthetic is not None and msg.synthetic != synthetic)\n            or (prompts_only and msg.parent_id)\n            or (langs is not None and msg.lang not in langs)\n            or (states is not None and msg.tree_state not in states)\n        ):\n            return False\n\n        if exclude_normal is True and not msg.deleted and not msg.synthetic and msg.review_result:\n            return False\n\n        if spam is not None and spam != (not msg.review_result):\n            return False\n\n        return True\n\n    print(f\"Reading: {args.input_file_name}\")\n    messages = read_message_list(args.input_file_name, approve_message)\n\n    print(f\"Found {len(messages)} matching messages.\")\n\n    print(f\"Writing: {args.output_file_name}\")\n    if args.export_text_only:\n        with open_jsonl_write(args.output_file_name) as file:\n            for msg in messages:\n                json.dump(msg.text, file)\n                file.write(\"\\n\")\n    else:\n        write_messages(args.output_file_name, messages, args.exclude_nulls)\n\n\nif __name__ == \"__main__\":\n    main()\n", "oasst-data/examples/clean_dataset.py": "import argparse\nfrom collections import OrderedDict\n\nimport pandas\nfrom oasst_data.reader import read_message_trees\nfrom oasst_data.schemas import ExportMessageNode, ExportMessageTree\nfrom oasst_data.traversal import visit_messages_depth_first\nfrom oasst_data.writer import write_message_trees\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\"filter_dataset\")\n    parser.add_argument(\n        \"input_file_name\",\n        type=str,\n        help=\"path to input .jsonl or .jsonl.gz input file\",\n    )\n    parser.add_argument(\n        \"output_file_name\",\n        type=str,\n        help=\"path to output .jsonl or .jsonl.gz file\",\n    )\n    parser.add_argument(\"--instructions\", type=str, help=\"xlsx file with instructions\")\n    parser.add_argument(\"--exclude-nulls\", action=\"store_true\", default=False)\n    args = parser.parse_args()\n    return args\n\n\ndef main():\n    args = parse_args()\n\n    instructions_df = pandas.read_excel(args.instructions, na_filter=False)\n\n    # load dataset and index messages by id\n    tree_by_id: dict[str, ExportMessageTree] = OrderedDict()\n    message_by_id: dict[str, ExportMessageNode] = {}\n\n    print(f\"Reading: {args.input_file_name}\")\n    for message_tree in read_message_trees(args.input_file_name):\n        tree_by_id[message_tree.message_tree_id] = message_tree\n\n        def index_message(msg: ExportMessageNode):\n            message_by_id[msg.message_id] = msg\n\n        visit_messages_depth_first(message_tree.prompt, index_message)\n\n    print(f\"Loaded {len(tree_by_id)} trees with {len(message_by_id)} messages.\")\n\n    def count_descendants(msg: ExportMessageNode):\n        i = 1\n        if msg.replies:\n            for r in msg.replies:\n                i += count_descendants(r)\n        return i\n\n    def delete_message(msg: ExportMessageNode):\n        if msg.parent_id is None:\n            tree_by_id.pop(msg.message_id)\n            print(f\"Tree deleted: {msg.message_id}\")\n        else:\n            parent_msg = message_by_id[msg.parent_id]\n            parent_msg.replies.remove(msg)\n            print(f\"Branch deleted: {msg.message_id} ({count_descendants(msg)} messages)\")\n\n    # cleaning\n    print(\"Cleaning...\")\n    for index, row in instructions_df.iterrows():\n        id = row[\"UUID\"]\n        msg = message_by_id.get(id)\n        if msg is None:\n            print(f\"Not found: {id}\")\n\n        action = row[\"Action\"]\n        if action == \"Delete\":\n            print(f\"deleting: {id}\")\n            delete_message(msg)\n        elif action == \"Replace\":\n            print(f\"replace: {id}\")\n            replace = row[\"Replace\"]\n            msg.text = replace\n        elif action == \"Edit\":\n            print(f\"edit: {id}\")\n            if row[\"Category\"] == \"Copy Code\":\n                find = \"\\nCopy code\\n\"\n                replace = \"\\n\\n\"\n            else:\n                find = row[\"Find\"]\n                replace = row[\"Replace\"]\n            msg.text.index(find)  # make sure text is present\n            msg.text = msg.text.replace(find, replace)\n        else:\n            print(f\"Unsupported action {action}\")\n\n    print(\"Done\")\n\n    # write cleaned dataset to output file\n    print(f\"Writing: {args.output_file_name}\")\n    write_message_trees(\n        args.output_file_name,\n        tree_by_id.values(),\n        exclude_none=args.exclude_nulls,\n    )\n\n\nif __name__ == \"__main__\":\n    main()\n", "oasst-data/examples/split_dataset.py": "import argparse\nimport random\n\nfrom oasst_data import read_message_list, write_messages\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--val_percent\",\n        type=int,\n        default=5,\n    )\n    parser.add_argument(\n        \"input_file_name\",\n        type=str,\n        help=\"path to input .jsonl or .jsonl.gz input file\",\n    )\n    parser.add_argument(\n        \"--val_output\",\n        type=str,\n        help=\"path to validation output .jsonl or .jsonl.gz file\",\n        required=True,\n    )\n    parser.add_argument(\n        \"--train_output\",\n        type=str,\n        help=\"path to train output .jsonl or .jsonl.gz file\",\n        required=True,\n    )\n    parser.add_argument(\"--exclude-nulls\", action=\"store_true\", default=False)\n    args = parser.parse_args()\n    return args\n\n\ndef main():\n    \"\"\"Split messages file into train and validation set based on message_tree_id.\"\"\"\n    args = parse_args()\n\n    print(f\"Reading: {args.input_file_name}\")\n    messages = read_message_list(args.input_file_name)\n\n    print(f\"Found {len(messages)} matching messages.\")\n\n    tree_ids = list(set(m.message_tree_id for m in messages))\n    random.shuffle(tree_ids)\n\n    val_size = len(tree_ids) * args.val_percent // 100\n\n    train_set = set(tree_ids[val_size:])\n    val_set = set(tree_ids[:val_size])\n\n    train_messages = [m for m in messages if m.message_tree_id in train_set]\n    val_messages = [m for m in messages if m.message_tree_id in val_set]\n\n    print(f\"Writing train {len(train_messages)} messages: {args.train_output}\")\n    write_messages(args.train_output, train_messages, args.exclude_nulls)\n\n    print(f\"Writing valid {len(val_messages)} messages: {args.val_output}\")\n    write_messages(args.val_output, val_messages, args.exclude_nulls)\n\n\nif __name__ == \"__main__\":\n    main()\n", "oasst-data/examples/filter_trees.py": "import argparse\n\nfrom oasst_data import read_message_trees, write_message_trees\nfrom oasst_data.schemas import ExportMessageTree\nfrom oasst_data.traversal import visit_messages_depth_first\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\"filter_tres\")\n    parser.add_argument(\n        \"input_file_name\",\n        type=str,\n        help=\"path to input .jsonl or .jsonl.gz input file\",\n    )\n    parser.add_argument(\n        \"output_file_name\",\n        type=str,\n        help=\"path to output .jsonl or .jsonl.gz file\",\n    )\n    parser.add_argument(\n        \"--states\",\n        type=str,\n        default=\"ready_for_export\",\n        help=\"all|prompt_lottery_waiting|growing|ready_for_export|aborted_low_grade|halted_by_moderator|backlog_ranking\",\n    )\n    parser.add_argument(\"--exclude-nulls\", action=\"store_true\", default=False)\n    parser.add_argument(\"--allow-synth\", action=\"store_true\", default=False)\n    args = parser.parse_args()\n    return args\n\n\ndef main():\n    args = parse_args()\n\n    # load dataset and index messages by id\n    trees: list[ExportMessageTree] = []\n\n    states = args.states.split(\",\")\n    allow_synth = args.allow_synth\n\n    print(f\"Reading: {args.input_file_name}\")\n    for message_tree in read_message_trees(args.input_file_name):\n        msgs = []\n        visit_messages_depth_first(message_tree.prompt, msgs.append)\n        if message_tree.tree_state in states:\n            if allow_synth or not any(x.synthetic for x in msgs):\n                trees.append(message_tree)\n\n    print(f\"Found {len(trees)} matching trees.\")\n\n    print(f\"Writing: {args.output_file_name}\")\n    write_message_trees(args.output_file_name, trees, exclude_none=args.exclude_nulls)\n\n\nif __name__ == \"__main__\":\n    main()\n", "inference/worker/download_model_hf.py": "import os\nimport signal\nimport sys\nfrom pathlib import Path\n\nimport huggingface_hub\n\n\ndef terminate(signum, frame):\n    print(\"Terminating...\")\n    sys.exit(0)\n\n\nif __name__ == \"__main__\":\n    signal.signal(signal.SIGINT, terminate)\n    model_id = os.getenv(\"MODEL_ID\")\n    snapshot_dir = Path(huggingface_hub.snapshot_download(model_id))\n    for file in snapshot_dir.rglob(\"*.json\"):\n        text = file.read_text()\n        text = text.replace(\"LLaMA\", \"Llama\")\n        file.write_text(text)\n", "inference/worker/chat_chain_utils.py": "import json\nimport re\nfrom typing import Callable\n\nimport requests\nimport transformers\nfrom chat_chain_prompts import INSTRUCTIONS, OBSERVATION_SEQ, TOOLS_PREFIX\nfrom hf_langchain_inference import HFInference\nfrom langchain.agents import Tool\nfrom langchain.memory import ConversationBufferMemory\nfrom langchain.prompts import PromptTemplate\nfrom loguru import logger\nfrom oasst_shared.schemas import inference\nfrom openapi_parser import prepare_plugin_for_llm\nfrom settings import settings\nfrom utils import shared_tokenizer_lock, special_tokens\n\nRESPONSE_MAX_LENGTH = 2048\nDESCRIPTION_FOR_MODEL_MAX_LENGTH = 512\n\nllm_json_parser = HFInference(\n    inference_server_url=settings.inference_server_url,\n    max_new_tokens=512,\n    stop_sequences=[special_tokens[\"end\"] if special_tokens[\"end\"] else \"</s>\"],\n    top_k=5,\n    temperature=0.20,\n    repetition_penalty=(1 / 0.83),\n)\n\n\n# This algo should be fine but possible improvements could be levenshtein or vector distance\ndef similarity(ts1: str, ts2: str) -> float:\n    \"\"\"Compute Jaro-Winkler distance between two strings.\"\"\"\n    if ts1 == ts2:\n        return 1\n\n    match = 0\n    len1, len2 = len(ts1), len(ts2)\n    max_dist = (max(len1, len2) // 2) - 1\n\n    hash_ts1 = [0] * len1\n    hash_ts2 = [0] * len2\n\n    for i in range(len1):\n        for j in range(max(0, i - max_dist), min(len2, i + max_dist + 1)):\n            if ts1[i] == ts2[j] and hash_ts2[j] == 0:\n                hash_ts1[i] = 1\n                hash_ts2[j] = 1\n                match += 1\n                break\n\n    if match == 0:\n        return 0\n\n    t = 0\n    point = 0\n\n    for i in range(len1):\n        if hash_ts1[i] == 1:\n            while hash_ts2[point] == 0:\n                point += 1\n            if ts1[i] != ts2[point]:\n                t += 1\n            point += 1\n\n    t /= 2\n    return (match / len1 + match / len2 + (match - t) / match) / 3.0\n\n\ndef extract_tool_and_input(llm_output: str, ai_prefix: str) -> tuple[str, str]:\n    \"\"\"\n    Extract tool name and tool input from LLM output. If LLM chose not to use a tool, `ai_prefix` is returned instead of tool name, and LLM output is returned instead of tool input.\n    \"\"\"\n    llm_output = llm_output.strip().replace(\"```\", \"\")\n    if f\"{ai_prefix}:\" in llm_output:\n        # No tool used, return LLM prefix and LLM output\n        return ai_prefix, llm_output.split(f\"{ai_prefix}:\")[-1].strip()\n\n    regex = r\"Action: (.*?)[\\n]*Action Input:\\n?(.*)\"\n    match = re.search(regex, llm_output, re.MULTILINE | re.DOTALL)\n    if not match:\n        if OBSERVATION_SEQ in llm_output:\n            return ai_prefix, llm_output.split(OBSERVATION_SEQ)[-1].strip()\n        return ai_prefix, llm_output\n\n    action = match.group(1)\n    action_input = match.group(2)\n    return action.strip().replace(\"'\", \"\"), action_input.strip().strip(\" \")\n\n\n# Truncate, but append closing bracket if string starts with [ or { or (\n# Helps prevent LLM from just generating output continuously\ndef truncate_str(output: str, max_length: int = 1024) -> str:\n    if len(output) > max_length:\n        if output[0] == \"(\":\n            return output[:max_length] + \"...)\"\n        elif output[0] == \"[\":\n            return output[:max_length] + \"...]\"\n        elif output[0] == \"{\":\n            return output[:max_length] + \"...}\"\n        else:\n            return output[:max_length] + \"...\"\n    return output\n\n\n# Parse JSON and try to fix it if invalid\ndef prepare_json(json_str: str) -> str:\n    json_str = json_str.strip()\n    fixed_json = json_str\n    try:\n        json.loads(json_str)\n    except json.decoder.JSONDecodeError:\n        # Fix missing quotes around keys and replace Python's True, False, and None\n        fixed_json = re.sub(r\"(?<=\\{|\\,)(\\s*)(\\w+)(\\s*):\", r'\\1\"\\2\"\\3:', json_str)\n        fixed_json = fixed_json.replace(\"True\", \"true\").replace(\"False\", \"false\").replace(\"None\", \"null\")\n\n        # Remove excessive closing braces/brackets\n        brace_count = bracket_count = 0\n        result = []\n        for c in fixed_json:\n            if c == \"{\":\n                brace_count += 1\n            elif c == \"}\":\n                brace_count -= 1\n            elif c == \"[\":\n                bracket_count += 1\n            elif c == \"]\":\n                bracket_count -= 1\n\n            if brace_count >= 0 and bracket_count >= 0:\n                result.append(c)\n        # Add missing closing braces/brackets\n        result.extend([\"}\"] * brace_count)\n        result.extend([\"]\"] * bracket_count)\n        fixed_json = \"\".join(result)\n\n        try:\n            json.loads(fixed_json)\n        except json.decoder.JSONDecodeError as e:\n            logger.warning(f\"JSON is still not valid, trying to fix it with LLM {fixed_json}\")\n            # If still invalid, try using LLM to fix\n            prompt = f\"\"\"{special_tokens['prompter']}Below is malformed JSON object string:\n--------------\n{json_str}\n--------------\nParsing error:\n--------------\n{e}\n\nRULES:\n1. If malformed JSON object string contains multiple objects, you merge them into one.\n2. You will never made up or add any new data, you will only fix the malformed JSON object string.\n\nHere is the fixed JSON object string:{special_tokens['end'] or '</s>'}{special_tokens['assistant']}\"\"\"\n            logger.warning(f\"JSON Fix Prompt: {prompt}\")\n            out = llm_json_parser.generate(prompts=[prompt]).generations[0][0].text\n            out = out[: out.find(\"}\") + 1]\n            logger.warning(f\"JSON Fix Output: {out}\")\n            return out\n\n    return fixed_json\n\n\ndef select_tool(tool_name: str, tools: list[Tool]) -> Tool | None:\n    tool = next((t for t in tools if t.name in tool_name), None)\n    if tool:\n        return tool\n    tool, tool_similarity = max(\n        ((t, similarity(t.name, tool_name)) for t in tools),\n        key=lambda x: x[1],\n        default=(None, 0),\n    )\n    # TODO: make stricter with better models\n    if tool and tool_similarity > 0.75:\n        return tool\n    return None\n\n\ndef use_tool(tool_name: str, tool_input: str, tools: list[Tool]) -> str:\n    tool = select_tool(tool_name, tools)\n    if not tool:\n        return f\"ERROR! {tool_name} is not a valid tool. Try again with different tool!\"\n    prepared_input = prepare_json(tool_input)\n    tool_output = tool.func(prepared_input)\n    return tool_output\n\n\n# Needs more work for errors, error-prompt tweaks are currently based on\n# `OpenAssistant/oasst-sft-6-llama-30b-epoch-1 model`\nclass RequestsForLLM:\n    def run(self, params: str, url: str, param_location: str, type: str, payload: str | None = None) -> str:\n        return self.run_request(params, url, param_location, type, payload)\n\n    def run_request(self, params: str, url: str, param_location: str, type: str, payload: str = None) -> str:\n        try:\n            query_params = params\n            if param_location == \"path\":\n                for key, value in query_params.items():\n                    url = url.replace(f\"{{{key}}}\", value)\n                query_params = {}\n\n            headers = {\"Content-Type\": \"application/json\"} if payload else None\n\n            if type.lower() == \"get\":\n                logger.info(\n                    f\"Running {type.upper()} request on {url} with\\nparams: {params}\\nparam_location: {param_location}\\npayload: {payload}\"\n                )\n                res = requests.get(url, params=query_params, headers=headers)\n            elif type.lower() == \"post\":\n                # if model did not generate payload object, use params as payload\n                data = json.dumps(payload) if payload else json.dumps(params)\n                logger.info(\n                    f\"Running {type.upper()} request on {url} with\\nparams: {params}\\nparam_location: {param_location}\\npayload: {data}\"\n                )\n                res = requests.post(url, params=query_params, data=data, headers=headers)\n            else:\n                return f\"ERROR! Unsupported request type: {type}. Only GET and POST are supported. Try again!\"\n\n            return self.process_response(res)\n        except Exception as e:\n            return f\"ERROR! That didn't work, try modifying Action Input.\\n{e}. Try again!\"\n\n    def process_response(self, res: requests.Response) -> str:\n        logger.info(f\"Request response: {res.text}\")\n        if res.status_code != 200:\n            return f\"ERROR! Please modify Action Input. according to this error message: \\n{res.text}. Try again!\"\n\n        if res.text is None or len(res.text) == 0:\n            return \"ERROR! That didn't work, try modifying Action Input.\\nEmpty response. Try again!\"\n\n        if \"null\" in res.text.lower() and len(res.text) < 10:\n            return \"ERROR! That didn't work, try modifying Action Input.\\nEmpty response. Try again!\"\n\n        return truncate_str(res.text, RESPONSE_MAX_LENGTH)\n\n\ndef compose_tools_from_plugin(plugin: inference.PluginEntry | None) -> tuple[str, list[Tool]]:\n    if not plugin:\n        return \"\", []\n\n    llm_plugin: inference.PluginConfig = prepare_plugin_for_llm(plugin.url)\n    if not llm_plugin:\n        return \"\", []\n\n    tools = []\n    request_tool = RequestsForLLM()\n\n    def create_tool_func(endpoint: inference.PluginOpenAPIEndpoint, param_location: str) -> Callable[..., str]:\n        def func(req) -> str:\n            try:\n                json_obj = json.loads(req)\n                request = json_obj.get(\"request\", {})\n                params = request.get(\"params\", {})\n                payload = request.get(\"payload\", None)\n            except json.JSONDecodeError:\n                print(\"Error: Invalid JSON input\")\n                request, params, payload = {}, {}, None\n            except Exception as e:\n                print(f\"Error: {e}\")\n                request, params, payload = {}, {}, None\n\n            return request_tool.run(\n                url=endpoint.url, params=params, param_location=param_location, type=endpoint.type, payload=payload\n            )\n\n        return func\n\n    # Generate tool for each plugin endpoint. Helps LLM use tools as it does not choose API server URL etc on its own\n    # LLM only chooses endpoint, parameters and values to use. Modifying this can degrade or improve tool usage\n    for endpoint in llm_plugin.endpoints:\n        params = \"\\n\\n\".join(\n            [\n                f\"\"\" name: \"{param.name}\",\\n in: \"{param.in_}\",\\n description: \"{truncate_str(param.description, 128)}\",\\n schema: {param.schema_},\\n required: {param.required}\"\"\"\n                for param in endpoint.params\n            ]\n        )\n\n        # LangChain uses {input_name} for templating\n        # Some plugins can have {some_word} in their description\n        params = params.replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n        payload_description = \"\"\n        if endpoint.payload:\n            try:\n                payload_description = \"payload: \" + truncate_str(json.dumps(endpoint.payload, indent=4), 256)\n                payload_description = payload_description.replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n            except Exception as e:\n                logger.warning(f\"Failed to convert payload to json string: {e}\")\n\n        payload_description += \"\" if not payload_description or payload_description.endswith(\"\\n\") else \"\\n\"\n        if len(payload_description) > 0:\n            payload_description = \"\\n\" + payload_description + \"\\n\"\n\n        parameters_description = f\"params:\\n{params}\\n\" if params else \"\\n\"\n\n        openapi_specification_title = (\n            \"\\nOpenAPI specification\\n\" if len(payload_description) > 0 or len(params) > 0 else \"\"\n        )\n\n        param_location = endpoint.params[0].in_ if len(endpoint.params) > 0 else \"query\"\n\n        # If plugin has no operation_id, use path as fallback\n        path = endpoint.path[1:] if endpoint.path and len(endpoint.path) > 0 else endpoint.path\n\n        tool = Tool(\n            name=endpoint.operation_id if endpoint.operation_id != \"\" else path,\n            # Could be path, e.g /api/v1/endpoint but can lead LLM to invent URLs\n            # Problem with EP description is that it is too long for some plugins\n            func=create_tool_func(endpoint, param_location),\n            description=f\"{openapi_specification_title}{parameters_description}{payload_description}tool description: {endpoint.summary}\\n\",\n        )\n        tools.append(tool)\n\n    tools_string = \"\\n\".join([f\"> {tool.name}{tool.description}\" for tool in tools])\n    # This can be long for some plugins, we need to truncate due to ctx limitations\n    plugin_description_for_model = truncate_str(llm_plugin.description_for_model, DESCRIPTION_FOR_MODEL_MAX_LENGTH)\n    return (\n        f\"{TOOLS_PREFIX}{tools_string}\\n\\n{llm_plugin.name_for_model} plugin description:\\n{plugin_description_for_model}\\n\\n{INSTRUCTIONS}\",\n        tools,\n    )\n\n\ndef prepare_prompt(\n    input_prompt: str,\n    prompt_template: PromptTemplate,\n    memory: ConversationBufferMemory,\n    tools_names: list[str] | None,\n    current_time: str,\n    language: str,\n    tokenizer: transformers.PreTrainedTokenizer,\n    worker_config: inference.WorkerConfig,\n    action_input_format: str,\n    custom_instructions: str = \"\",\n) -> str:\n    max_input_length = worker_config.model_config.max_input_length\n\n    args = {\n        \"input\": input_prompt,\n        \"language\": language,\n        \"current_time\": current_time,\n        \"chat_history\": memory.buffer,\n        \"custom_instructions\": custom_instructions,\n    }\n\n    if tools_names:\n        args[\"tools_names\"] = tools_names\n        args[\"action_input_format\"] = action_input_format\n\n    out_prompt = prompt_template.format(**args)\n\n    with shared_tokenizer_lock:\n        ids = tokenizer.encode(out_prompt)\n\n    # soft truncation (delete whole messages)\n    while len(ids) > max_input_length and len(memory.chat_memory.messages) > 0:\n        memory.chat_memory.messages.pop(0)\n        args = {\n            \"input\": input_prompt,\n            \"language\": language,\n            \"current_time\": current_time,\n            \"chat_history\": memory.buffer,\n            \"custom_instructions\": custom_instructions,\n        }\n\n        if tools_names:\n            args[\"tools_names\"] = tools_names\n            args[\"action_input_format\"] = action_input_format\n\n        out_prompt = prompt_template.format(**args)\n\n        with shared_tokenizer_lock:\n            ids = tokenizer.encode(out_prompt)\n        logger.warning(f\"Prompt too long, deleting chat history. New length: {len(ids)}\")\n\n    return out_prompt\n", "inference/worker/get_model_config_prop.py": "import sys\n\nfrom oasst_shared import model_configs\nfrom settings import settings\n\nif __name__ == \"__main__\":\n    model_config = model_configs.MODEL_CONFIGS.get(settings.model_config_name)\n    if model_config is None:\n        print(f\"Unknown model config name: {settings.model_config_name}\")\n        sys.exit(2)\n    if len(sys.argv) != 2:\n        print(\"Usage: get_model_config_prop.py <property>\")\n        sys.exit(2)\n    prop = sys.argv[1]\n    if not hasattr(model_config, prop):\n        print(f\"Unknown property: {prop}\")\n        sys.exit(2)\n    val = getattr(model_config, prop)\n    if isinstance(val, bool):\n        val = str(val).lower()\n    print(val, end=\"\")\n", "inference/worker/openapi_parser.py": "import json\nfrom urllib.parse import urlsplit\n\nimport requests\nimport yaml\nfrom loguru import logger\nfrom oasst_shared.schemas import inference\n\n\ndef fetch_openapi_spec(url):\n    response = requests.get(url)\n    if response.status_code != 200:\n        raise Exception(f\"Failed to fetch data from URL: {url}. Status code: {response.status_code}\")\n\n    content_type = response.headers.get(\"Content-Type\")\n\n    if \"application/json\" in content_type or url.endswith(\".json\"):\n        return json.loads(response.text)\n    elif (\n        \"application/yaml\" in content_type\n        or \"application/x-yaml\" in content_type\n        or url.endswith(\".yaml\")\n        or url.endswith(\".yml\")\n    ):\n        return yaml.safe_load(response.text)\n    else:\n        raise Exception(f\"Unsupported content type: {content_type}. Only JSON and YAML are supported.\")\n\n\ndef get_plugin_config(url: str) -> inference.PluginConfig | None:\n    try:\n        response = requests.get(url)\n        response.raise_for_status()\n        plugin_dict = response.json()\n        logger.info(f\"Plugin config downloaded {plugin_dict}\")\n        plugin_config = inference.PluginConfig.parse_obj(plugin_dict)\n        return plugin_config\n    except (requests.RequestException, ValueError) as e:\n        logger.warning(f\"Error downloading or parsing Plugin config: {e}\")\n        return None\n\n\ndef resolve_schema_reference(ref: str, openapi_dict: dict):\n    if not ref.startswith(\"#/\"):\n        raise ValueError(f\"Invalid reference format: {ref}\")\n\n    components = ref.split(\"/\")\n    schema = openapi_dict\n    for component in components[1:]:\n        if component not in schema:\n            raise ValueError(f\"Reference component not found: {component}\")\n        schema = schema[component]\n\n    return schema\n\n\ndef parse_plugin_endpoint(\n    api_url: str,\n    method: str,\n    details: dict,\n    base_url: str,\n    path: str,\n    openapi_dict: dict,\n) -> inference.PluginOpenAPIEndpoint:\n    \"\"\"\n    Parse details of a single plugin endpoint from OpenAPI spec.\n\n    Args:\n        api_url: URL of the plugin API.\n        method: HTTP method of the endpoint.\n        details: Details of the endpoint from OpenAPI spec.\n        base_url: Base URL of the plugin.\n        path: Path of the endpoint.\n        openapi_dict: Full OpenAPI spec of the plugin.\n    \"\"\"\n    split_result = urlsplit(api_url)\n    backup_url = f\"{split_result.scheme}://{split_result.netloc}\"\n    params_list = []\n    parameters = details.get(\"parameters\", [])\n    if parameters is not None:\n        for param in parameters:\n            schema = None\n            if \"$ref\" in param[\"schema\"]:\n                schema = resolve_schema_reference(param[\"schema\"][\"$ref\"], openapi_dict)\n\n            params_list.append(\n                inference.PluginOpenAPIParameter(\n                    name=param.get(\"name\", \"\"),\n                    in_=param.get(\"in\", \"query\"),\n                    description=param.get(\"description\", \"\"),\n                    required=param.get(\"required\", False),\n                    schema_=schema,\n                )\n            )\n    # Check if the method is POST and extract request body schema\n    payload = None\n    if \"requestBody\" in details:\n        content = details[\"requestBody\"].get(\"content\", {})\n        for media_type, media_schema in content.items():\n            if media_type == \"application/json\":\n                if \"$ref\" in media_schema[\"schema\"]:\n                    payload = resolve_schema_reference(media_schema[\"schema\"][\"$ref\"], openapi_dict)\n                else:\n                    payload = media_schema[\"schema\"]\n\n    endpoint_data = {\n        \"type\": method,\n        \"summary\": details.get(\"summary\", \"\"),\n        \"operation_id\": details.get(\"operationId\", \"\"),\n        \"url\": f\"{base_url}{path}\" if base_url is not None else f\"{backup_url}{path}\",\n        \"path\": path,\n        \"params\": params_list,\n        \"payload\": payload,\n    }\n\n    if \"tags\" in details:\n        tag_name = details[\"tags\"][0]\n        endpoint_data[\"tag\"] = tag_name\n\n    endpoint = inference.PluginOpenAPIEndpoint(**endpoint_data)\n    return endpoint\n\n\ndef get_plugin_endpoints(api_url: str, openapi_dict: dict) -> list[inference.PluginOpenAPIEndpoint]:\n    endpoints = []\n    base_url = openapi_dict.get(\"servers\", [{}])[0].get(\"url\")\n\n    if base_url is not None:\n        parsed_link = urlsplit(api_url)\n        base_url = (\n            f\"{parsed_link.scheme}://{parsed_link.netloc}{base_url}\" if not urlsplit(base_url).scheme else base_url\n        )\n\n    for path, methods in openapi_dict.get(\"paths\", {}).items():\n        for method, details in methods.items():\n            endpoints.append(parse_plugin_endpoint(api_url, method, details, base_url, path, openapi_dict))\n\n    return endpoints\n\n\ndef prepare_plugin_for_llm(plugin_url: str) -> inference.PluginConfig | None:\n    plugin_config = get_plugin_config(plugin_url)\n\n    if not plugin_config:\n        return None\n\n    try:\n        parsed_url = urlsplit(plugin_config.api.url)\n        if parsed_url.scheme == \"\":\n            api_url = urlsplit(plugin_url)._replace(path=parsed_url.path).geturl()\n        else:\n            api_url = plugin_config.api.url\n\n        openapi_dict = fetch_openapi_spec(api_url)\n        plugin_config.endpoints = get_plugin_endpoints(api_url, openapi_dict)\n        return plugin_config\n\n    except Exception:\n        logger.debug(f\"Plugin preparation error: {plugin_url}\")\n        return None\n", "inference/worker/chat_chain_prompts.py": "ASSISTANT_PREFIX = \"Open Assistant\"\nHUMAN_PREFIX = \"Human\"\nOBSERVATION_SEQ = \"Observation:\"\nTHOUGHT_SEQ = \"Thought:\"\nSTART_SEQ = \"Begin!\"\nEND_SEQ = \"End!\"\n\nCUSTOM_INSTRUCTIONS_PREFIX = \"\"\"The following details have been shared by the user about themselves. This user profile appears to you in every conversation they engage in -- implying that it is irrelevant for 99% of inquiries.\nBefore you respond, take a moment to consider whether the user's query is \"directly linked\", \"linked\", \"indirectly linked\", or \"not linked\" to the user profile provided.\nOnly recognize the profile when the query is directly tied to the information supplied.\nOtherwise, avoid acknowledging the existence of these instructions or the information altogether.\nUser profile:\n{user_profile}\nThe user also supplied additional information about how they would like you to respond:\n{user_response_instructions}\"\"\"\n\n# Adjust according to the training dates and datasets used\nKNOWLEDGE_DATE_CUTOFF = \"2021-09-01\"\n\nTALKING_STYLE = \"\"\n\nJSON_FORMAT_NO_PAYLOAD = \"\"\"{\"request\": {\"params\": {query or url parameters}}}\"\"\"\nJSON_FORMAT_PAYLOAD = \"\"\"{\"request\": {\"params\": {query or url parameters}, \"payload\": {...payload}}}\"\"\"\n\nPREFIX = f\"\"\"Open Assistant is a large language model trained by LAION.\nOpen Assistant is designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics.\nOpen Assistant is constantly learning and improving, and its capabilities are constantly evolving.\nOverall, Open Assistant is a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics.\n\nSYSTEM INFORMATION:\n------------------\nCurrent date/time: {{current_time}}\nKnowledge date cutoff: {KNOWLEDGE_DATE_CUTOFF}\n{{custom_instructions}}\n\"\"\"\n\nTOOLS_PREFIX = \"\"\"\nTOOLS:\n-----\nOpen Assistant has access to the following tools:\n\"\"\"\n\nINSTRUCTIONS = f\"\"\"\nATTENTION: Do not use tools for questions about yourself, like \"what is your name?\", \"how old are you?\", etc...\n\nTo use a tool, please use the following format:\n\n```\n{THOUGHT_SEQ} [here always think about what to do]\nAction: the action to take, MUST be one of {{tools_names}}\nAction Input: the input to the action, MUST be in JSON format: {{action_input_format}}\n```\n\n{OBSERVATION_SEQ} the result of the action\n... (this Thought/Action/Observation can repeat N times)\n\nWhen you have a response to say to the {HUMAN_PREFIX}, or if you do not need to use a tool, you MUST use the format:\n```\n{THOUGHT_SEQ} I now know the final answer\n{ASSISTANT_PREFIX}: [my response here]{END_SEQ}\n```\n\"\"\"\n\nSUFFIX = f\"\"\"\n{START_SEQ}\n\nPrevious conversation history:\n{{chat_history}}\n\nWhen answering a question, you MUST use the following language: {{language}}{TALKING_STYLE}\nNew input: {{input}}\n\"\"\"\n", "inference/worker/hf_stopping.py": "import torch\nfrom tokenizers import Tokenizer\nfrom transformers import StoppingCriteria\n\n\nclass SequenceStoppingCriteria(StoppingCriteria):\n    \"\"\"Enables automatic stopping of model text generation when specific text sequences are generated.\"\"\"\n\n    def __init__(\n        self,\n        tokenizer: Tokenizer,\n        stop_texts: list[str],\n        input_prompt: str,\n        *args,\n        **kwargs,\n    ):\n        super().__init__(*args, **kwargs)\n        self.stop_texts = stop_texts\n        self.tokenizer = tokenizer\n        self.input_length = len(tokenizer.encode(input_prompt))\n\n    def __call__(\n        self,\n        input_ids: torch.LongTensor,\n        scores: torch.FloatTensor,\n        **kwargs,\n    ) -> bool:\n        # Assumes batch size 1, sufficient for our use case\n        generated_ids = input_ids[0, self.input_length :].tolist()\n        # TODO: optimise this. Inefficient to decode whole sequence every time\n        # but can't encode stop sequences as they don't always tokenize the same\n        generated_text = self.tokenizer.decode(generated_ids)\n        return any(text in generated_text for text in self.stop_texts)\n", "inference/worker/chat_chain.py": "import datetime\n\nimport interface\nimport transformers\nimport utils\nimport websocket\nfrom chat_chain_prompts import (\n    ASSISTANT_PREFIX,\n    CUSTOM_INSTRUCTIONS_PREFIX,\n    HUMAN_PREFIX,\n    JSON_FORMAT_NO_PAYLOAD,\n    JSON_FORMAT_PAYLOAD,\n    OBSERVATION_SEQ,\n    PREFIX,\n    SUFFIX,\n    THOUGHT_SEQ,\n)\nfrom chat_chain_utils import compose_tools_from_plugin, extract_tool_and_input, prepare_prompt, use_tool\nfrom hf_langchain_inference import HFInference\nfrom langchain.agents import Tool\nfrom langchain.memory import ConversationBufferMemory\nfrom langchain.prompts import PromptTemplate\nfrom loguru import logger\nfrom oasst_shared.model_configs import ModelConfig\nfrom oasst_shared.schemas import inference\nfrom settings import settings\nfrom utils import special_tokens\n\n# Exclude tools description from final prompt. Saves ctx space but can hurt output\n# quality especially if truncation kicks in. Dependent on model used\nREMOVE_TOOLS_FROM_FINAL_PROMPT = False\n\nllm = HFInference(\n    inference_server_url=settings.inference_server_url,\n    max_new_tokens=512,\n    stop_sequences=[],\n    top_k=50,\n    temperature=0.20,\n    seed=43,\n    repetition_penalty=(1 / 0.92),  # Best with > 0.88\n)\n\n\nclass PromptedLLM:\n    \"\"\"\n    Handles calls to an LLM via LangChain with a prompt template and memory.\n    \"\"\"\n\n    def __init__(\n        self,\n        tokenizer: transformers.PreTrainedTokenizer,\n        worker_config: inference.WorkerConfig,\n        parameters: interface.GenerateStreamParameters,\n        prompt_template: PromptTemplate,\n        memory: ConversationBufferMemory,\n        tool_names: list[str],\n        language: str,\n        action_input_format: str,\n        custom_instructions: str = \"\",\n    ):\n        self.tokenizer = tokenizer\n        self.worker_config = worker_config\n        self.parameters = parameters\n        self.prompt_template = prompt_template\n        self.memory = memory\n        self.tool_names = tool_names\n        self.language = language\n        self.action_input_format = action_input_format\n        self.current_time = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n        self.custom_instructions = custom_instructions\n\n    def call(self, prompt: str) -> tuple[str, str]:\n        \"\"\"Prepares and truncates prompt, calls LLM, returns used prompt and response.\"\"\"\n        prompt = prepare_prompt(\n            prompt,\n            self.prompt_template,\n            self.memory,\n            self.tool_names,\n            self.current_time,\n            self.language,\n            self.tokenizer,\n            self.worker_config,\n            self.action_input_format,\n            self.custom_instructions,\n        )\n\n        # We do not strip() outputs as it seems to degrade instruction-following abilities of the model\n        prompt = utils.truncate_prompt(self.tokenizer, self.worker_config, self.parameters, prompt, True)\n\n        response = (\n            llm.generate(prompts=[prompt], stop=[ASSISTANT_PREFIX, OBSERVATION_SEQ, f\"\\n{OBSERVATION_SEQ}\"])\n            .generations[0][0]\n            .text\n        )\n\n        if response:\n            response = response.replace(\"\\n\\n\", \"\\n\")\n            if response[0] != \"\\n\":\n                response = f\"\\n{response}\"\n\n        return prompt, response\n\n\ndef handle_plugin_usage(\n    input_prompt: str,\n    prompt_template: PromptTemplate,\n    language: str,\n    memory: ConversationBufferMemory,\n    worker_config: inference.WorkerConfig,\n    tokenizer: transformers.PreTrainedTokenizer,\n    parameters: interface.GenerateStreamParameters,\n    tools: list[Tool],\n    plugin: inference.PluginEntry | None,\n    plugin_max_depth: int,\n    ws: websocket.WebSocket,\n    work_request_id: str,\n    custom_instructions: str = \"\",\n) -> tuple[str, inference.PluginUsed]:\n    execution_details = inference.PluginExecutionDetails(\n        inner_monologue=[],\n        final_tool_output=\"\",\n        final_prompt=\"\",\n        final_generation_assisted=False,\n        error_message=\"\",\n        status=\"failure\",\n    )\n    plugin_used = inference.PluginUsed(\n        name=None,\n        url=None,\n        execution_details=execution_details,\n    )\n\n    if plugin is None:\n        return input_prompt, plugin_used\n\n    chain_finished = False\n    achieved_depth = 0\n    assisted = False\n    inner_prompt = \"\"\n    inner_monologue = []\n\n    action_input_format = (\n        JSON_FORMAT_PAYLOAD if prompt_template.template.find(\"payload\") != -1 else JSON_FORMAT_NO_PAYLOAD\n    )\n    eos_token = \"\"\n    if special_tokens[\"end\"]:\n        eos_token = special_tokens[\"end\"]\n    elif hasattr(tokenizer, \"eos_token\"):\n        eos_token = tokenizer.eos_token\n    tool_names = [tool.name for tool in tools]\n\n    chain = PromptedLLM(\n        tokenizer,\n        worker_config,\n        parameters,\n        prompt_template,\n        memory,\n        tool_names,\n        language,\n        action_input_format,\n        custom_instructions,\n    )\n\n    # send \"thinking...\" intermediate step to UI (This will discard queue position 0) immediately\n    utils.send_response(\n        ws,\n        inference.PluginIntermediateResponse(\n            request_id=work_request_id,\n            current_plugin_thought=\"thinking...\",\n            current_plugin_action_taken=\"\",\n            current_plugin_action_input=\"\",\n            current_plugin_action_response=\"\",\n        ),\n    )\n\n    init_prompt = f\"{input_prompt}{eos_token}{special_tokens['assistant']}\"\n    init_prompt, chain_response = chain.call(init_prompt)\n\n    inner_monologue.append(\"In: \" + str(init_prompt))\n    inner_monologue.append(\"Out: \" + str(chain_response))\n\n    current_action_thought = \"\"\n    if THOUGHT_SEQ in chain_response:\n        current_action_thought = chain_response.split(THOUGHT_SEQ)[1].split(\"\\n\")[0]\n\n    # Tool name/assistant prefix, Tool input/assistant response\n    prefix, response = extract_tool_and_input(llm_output=chain_response, ai_prefix=ASSISTANT_PREFIX)\n    assisted = False if ASSISTANT_PREFIX in prefix else True\n    chain_finished = not assisted\n\n    if assisted:\n        # model decided to use a tool, so send that thought to the client\n        utils.send_response(\n            ws,\n            inference.PluginIntermediateResponse(\n                request_id=work_request_id,\n                current_plugin_thought=current_action_thought,\n                current_plugin_action_taken=prefix,\n                current_plugin_action_input=chain_response,\n                current_plugin_action_response=response,\n            ),\n        )\n\n    while not chain_finished and assisted and achieved_depth < plugin_max_depth:\n        tool_response = use_tool(prefix, response, tools)\n\n        # Save previous chain response for use in final prompt\n        prev_chain_response = chain_response\n        new_prompt = (\n            f\"{input_prompt}{eos_token}{special_tokens['assistant']}{chain_response}{OBSERVATION_SEQ} {tool_response}\"\n        )\n\n        new_prompt, chain_response = chain.call(new_prompt)\n\n        inner_monologue.append(\"In: \" + str(new_prompt))\n        inner_monologue.append(\"Out: \" + str(chain_response))\n\n        current_action_thought = \"\"\n        if THOUGHT_SEQ in chain_response:\n            current_action_thought = chain_response.split(THOUGHT_SEQ)[1].split(\"\\n\")[0]\n\n        # Send deep plugin intermediate steps to UI\n        utils.send_response(\n            ws,\n            inference.PluginIntermediateResponse(\n                request_id=work_request_id,\n                current_plugin_thought=current_action_thought,\n                current_plugin_action_taken=prefix,\n                current_plugin_action_input=chain_response,\n                current_plugin_action_response=response,\n            ),\n        )\n\n        prefix, response = extract_tool_and_input(llm_output=chain_response, ai_prefix=ASSISTANT_PREFIX)\n        assisted = False if ASSISTANT_PREFIX in prefix else True\n\n        # Check if tool response contains ERROR string and force retry\n        # Current models sometimes decide to retry on error but sometimes just ignore\n        if tool_response.find(\"ERROR\") != -1 and assisted is False:\n            chain_response = prev_chain_response\n            assisted = True\n\n        if not assisted:\n            chain_finished = True\n\n            if REMOVE_TOOLS_FROM_FINAL_PROMPT:\n                TEMPLATE = f\"\"\"{special_tokens['prompter']}{PREFIX}{SUFFIX}\"\"\"\n                input_variables = [\"input\", \"chat_history\", \"language\", \"current_time\"]\n\n                prompt_template = PromptTemplate(input_variables=input_variables, template=TEMPLATE)\n                tool_names = None\n\n            final_input = f\"{input_prompt}{eos_token}{special_tokens['assistant']}\\n{prev_chain_response}{OBSERVATION_SEQ} {tool_response}\"\n            inner_prompt = prepare_prompt(\n                final_input,\n                prompt_template,\n                memory,\n                tool_names,\n                chain.current_time,\n                language,\n                tokenizer,\n                worker_config,\n                action_input_format,\n                custom_instructions,\n            )\n\n            inner_prompt = f\"{inner_prompt}\\n{THOUGHT_SEQ} I now know the final answer\\n{ASSISTANT_PREFIX}:  \"\n\n            plugin_used.execution_details.inner_monologue = inner_monologue\n            plugin_used.execution_details.final_tool_output = tool_response\n            plugin_used.execution_details.final_prompt = inner_prompt\n            plugin_used.execution_details.final_generation_assisted = True\n            plugin_used.execution_details.achieved_depth = achieved_depth + 1\n            plugin_used.execution_details.status = \"success\"\n            plugin_used.name = plugin.plugin_config.name_for_human\n            plugin_used.trusted = plugin.trusted\n            plugin_used.url = plugin.url\n\n            return inner_prompt, plugin_used\n        achieved_depth += 1\n\n    plugin_used.name = plugin.plugin_config.name_for_human\n    plugin_used.trusted = plugin.trusted\n    plugin_used.url = plugin.url\n    plugin_used.execution_details.inner_monologue = inner_monologue\n\n    # Re-add ASSISTANT_PREFIX to chain_response, omitted with stop=[ASSISTANT_PREFIX]\n    chain_response = f\"{chain_response}{ASSISTANT_PREFIX}:  \"\n\n    if chain_finished:\n        if not response:\n            # Malformed non-assisted LLM output\n            plugin_used.execution_details.status = \"failure\"\n            plugin_used.execution_details.error_message = \"Malformed LLM output\"\n            return init_prompt, plugin_used\n\n        plugin_used.execution_details.status = \"success\"\n        return f\"{init_prompt}{THOUGHT_SEQ} I now know the final answer\\n{ASSISTANT_PREFIX}:  \", plugin_used\n    else:\n        # Max depth reached, answer without tool\n        plugin_used.execution_details.final_prompt = init_prompt\n        plugin_used.execution_details.achieved_depth = achieved_depth\n        plugin_used.execution_details.status = \"failure\"\n        plugin_used.execution_details.error_message = f\"Max depth reached: {plugin_max_depth}\"\n        init_prompt = f\"{init_prompt}{THOUGHT_SEQ} I now know the final answer\\n{ASSISTANT_PREFIX}:  \"\n        return init_prompt, plugin_used\n\n\ndef handle_standard_usage(\n    original_prompt: str,\n    prompt_template: PromptTemplate,\n    language: str,\n    memory: ConversationBufferMemory,\n    worker_config: inference.WorkerConfig,\n    tokenizer: transformers.PreTrainedTokenizer,\n    custom_instructions: str = \"\",\n):\n    eos_token = \"\"\n    if special_tokens[\"end\"]:\n        eos_token = special_tokens[\"end\"]\n    elif hasattr(tokenizer, \"eos_token\"):\n        eos_token = tokenizer.eos_token\n    current_time = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n\n    # Non-plugin prompt template can include some external data e.g. datetime, language\n    action_input_format = (\n        JSON_FORMAT_PAYLOAD if prompt_template.template.find(\"payload\") != -1 else JSON_FORMAT_NO_PAYLOAD\n    )\n    input = f\"{original_prompt}{eos_token}{special_tokens['assistant']}\"\n    init_prompt = prepare_prompt(\n        input,\n        prompt_template,\n        memory,\n        None,\n        current_time,\n        language,\n        tokenizer,\n        worker_config,\n        action_input_format,\n        custom_instructions,\n    )\n    return init_prompt, None\n\n\ndef build_memory(work_request: inference.WorkRequest) -> ConversationBufferMemory:\n    memory = ConversationBufferMemory(\n        memory_key=\"chat_history\",\n        input_key=\"input\",\n        output_key=\"output\",\n        ai_prefix=ASSISTANT_PREFIX,\n        human_prefix=HUMAN_PREFIX,\n    )\n\n    for message in work_request.thread.messages[:-1]:\n        if message.role == \"prompter\" and message.state == inference.MessageState.manual and message.content:\n            memory.chat_memory.add_user_message(message.content)\n        elif message.role == \"assistant\" and message.state == inference.MessageState.complete and message.content:\n            memory.chat_memory.add_ai_message(message.content)\n\n    return memory\n\n\ndef handle_conversation(\n    work_request: inference.WorkRequest,\n    worker_config: inference.WorkerConfig,\n    parameters: interface.GenerateStreamParameters,\n    tokenizer: transformers.PreTrainedTokenizer,\n    ws: websocket.WebSocket,\n) -> tuple[str, inference.PluginUsed | None]:\n    try:\n        original_prompt = work_request.thread.messages[-1].content\n        if not original_prompt:\n            raise ValueError(\"Prompt is empty\")\n\n        language = \"English\"\n        plugin = next((p for p in parameters.plugins if p.enabled), None)\n\n        tools_instructions_template, tools = compose_tools_from_plugin(plugin)\n        plugin_enabled = len(tools) > 0\n        memory: ConversationBufferMemory = build_memory(work_request)\n\n        TEMPLATE = f\"\"\"{special_tokens['prompter']}{PREFIX}{tools_instructions_template}{SUFFIX}\"\"\"\n        input_variables = [\n            \"input\",\n            \"chat_history\",\n            \"language\",\n            \"current_time\",\n            \"action_input_format\",\n            \"custom_instructions\",\n        ] + ([\"tools_names\"] if plugin_enabled else [])\n\n        # TODO: Consider passing language from the UI here\n        prompt_template = PromptTemplate(input_variables=input_variables, template=TEMPLATE)\n\n        custom_instructions = (\n            f\"\"\"\\n{CUSTOM_INSTRUCTIONS_PREFIX.format(\n            user_profile=work_request.parameters.user_profile,\n            user_response_instructions=work_request.parameters.user_response_instructions,\n        )}\"\"\"\n            if work_request.parameters.user_response_instructions or work_request.parameters.user_profile\n            else \"\"\n        )\n\n        if plugin_enabled:\n            return handle_plugin_usage(\n                original_prompt,\n                prompt_template,\n                language,\n                memory,\n                worker_config,\n                tokenizer,\n                parameters,\n                tools,\n                plugin,\n                work_request.parameters.plugin_max_depth,\n                ws,\n                work_request.id,\n                custom_instructions,\n            )\n\n        return handle_standard_usage(\n            original_prompt, prompt_template, language, memory, worker_config, tokenizer, custom_instructions\n        )\n    except Exception as e:\n        logger.error(f\"Error while handling conversation: {e}\")\n        return \"\", None\n\n\nif __name__ == \"__main__\":\n    plugin = inference.PluginEntry(\n        enabled=True,\n        url=\"http://localhost:8082/ai-plugin.json\",\n        plugin_config=inference.PluginConfig(\n            name_for_human=\"Local dev plugin\",\n            name_for_model=\"Local dev plugin\",\n            description_for_model=\"Local dev plugin\",\n            description_for_human=\"Local dev plugin\",\n            schema_version=\"0.0.1\",\n            api={\"type\": \"openapi\", \"url\": \"http://localhost:8082/openapi.json\", \"has_user_authentication\": False},\n            auth={\"type\": \"none\"},\n        ),\n    )\n\n    model_config = ModelConfig(\n        model_id=\"decapoda-research/llama-30b-hf\",\n        max_input_length=1024,\n        max_total_length=2048,\n    )\n\n    work_parameters = inference.WorkParameters(model_config=model_config, do_sample=True, seed=42, plugins=[plugin])\n    parameters = interface.GenerateStreamParameters.from_work_parameters(work_parameters)\n\n    worker_config = inference.WorkerConfig(\n        model_config=model_config,\n        model_id=model_config.model_id,\n        max_input_length=model_config.max_input_length,\n        max_total_length=model_config.max_total_length,\n        do_sample=True,\n        seed=42,\n    )\n\n    while True:\n        input_ = input(\"Enter your input: \")\n        if input == \"exit\":\n            break\n        work_request = inference.WorkRequest(\n            request_type=\"work\",\n            parameters=work_parameters,\n            thread=inference.Thread(\n                messages=[\n                    inference.MessageRead(\n                        id=\"1\",\n                        chat_id=\"1\",\n                        parent_id=None,\n                        content=\"Hi, what is your name?\",\n                        created_at=datetime.datetime.now(),\n                        role=\"prompter\",\n                        state=inference.MessageState.complete,\n                        score=0,\n                        work_parameters=work_parameters,\n                        reports=[],\n                    ),\n                    inference.MessageRead(\n                        id=\"1\",\n                        chat_id=\"1\",\n                        parent_id=None,\n                        content=\"Hello, my name is Open Assistant, how i can help you today?\",\n                        created_at=datetime.datetime.now(),\n                        role=\"assistant\",\n                        state=inference.MessageState.complete,\n                        score=0,\n                        work_parameters=work_parameters,\n                        reports=[],\n                    ),\n                    inference.MessageRead(\n                        id=\"1\",\n                        chat_id=\"1\",\n                        parent_id=None,\n                        content=input_,\n                        created_at=datetime.datetime.now(),\n                        role=\"prompter\",\n                        state=inference.MessageState.in_progress,\n                        score=0,\n                        work_parameters=work_parameters,\n                        reports=[],\n                    ),\n                ]\n            ),\n        )\n        tokenizer = transformers.LlamaTokenizer.from_pretrained(model_config.model_id)\n        final_out, used_plugin = handle_conversation(work_request, worker_config, parameters, tokenizer)\n        print(f\"Used_plugin: {used_plugin}\")\n        print(final_out)\n", "inference/worker/settings.py": "import pydantic\n\n\nclass Settings(pydantic.BaseSettings):\n    backend_url: str = \"ws://localhost:8000\"\n    model_config_name: str = \"distilgpt2\"\n    inference_server_url: str = \"http://localhost:8001\"\n    inference_server_route: str = \"/generate_stream\"\n    safety_server_url: str = \"http://localhost:8002\"\n    api_key: str = \"0000\"\n\n    oa_protocol_version: str = \"v2\"\n\n    # Supported: oasst, chatml\n    model_prompt_format: str = \"oasst\"\n\n    retry_on_error: bool = True\n    hf_pause: float = 0.075\n    max_parallel_requests: int = 1\n    use_stop_sequences: bool = False\n\n    perform_oom_test: bool = False\n    oom_test_max_length: int | None = None\n\n    # for hf basic server\n    quantize: bool = False\n\n    bearer_token: str | None = None\n\n    basic_auth_username: str | None = None\n    basic_auth_password: str | None = None\n\n    enable_safety: bool = False\n\n\nsettings = Settings()\n", "inference/worker/utils.py": "import collections\nimport random\nimport threading\nimport time\nfrom typing import Iterable, Literal\n\nimport interface\nimport lorem\nimport pydantic\nimport requests\nimport sseclient\nimport transformers\nimport websocket\nfrom loguru import logger\nfrom oasst_shared.schemas import inference\nfrom settings import settings\n\nshared_tokenizer_lock = threading.Lock()\n\n\nif settings.model_prompt_format == \"chatml\":\n    special_tokens = {\n        \"prompter\": \"<|im_start|>user\\n\",\n        \"assistant\": \"<|im_start|>assistant\\n\",\n        \"system\": \"<|im_start|>system\\n\",\n        \"end\": \"<|im_end|>\\n\",\n    }\nelse:\n    special_tokens = {\n        \"prompter\": \"<|prompter|>\",\n        \"assistant\": \"<|assistant|>\",\n        \"system\": \"<|system|>\",\n        \"end\": \"\",\n    }\n\n\nclass TokenBuffer:\n    \"\"\"\n    A buffer for storing and managing tokens based on various conditions including stop sequences.\n\n    The TokenBuffer class accumulates tokens while keeping track of the length and manages the tokens based on the stop\n    sequences provided during initialization. Tokens can be added to the buffer and later on iterated upon finishing\n    depending on the reason.\n    \"\"\"\n\n    def __init__(self, stop_sequences: list[str]) -> None:\n        self.stop_sequences = stop_sequences\n        self.longest_stop_len = max((len(stop) for stop in stop_sequences), default=1)\n        self.tokens = collections.deque()\n        self.token_lens = collections.deque()\n        self.total_len = 0\n\n    def add(self, token: interface.Token):\n        self.tokens.append(token)\n        self.token_lens.append(len(token))\n        self.total_len += len(token)\n        while True:\n            if not self.tokens:\n                break\n            head_len = self.token_lens[0]\n            if self.total_len - head_len >= self.longest_stop_len:\n                token = self.tokens.popleft()\n                self.token_lens.popleft()\n                self.total_len -= head_len\n                yield token\n            else:\n                break\n\n    def finish(self, reason: Literal[\"length\", \"eos_token\", \"stop_sequence\"]) -> Iterable[interface.Token]:\n        if reason == \"stop_sequence\":\n            end_sequence = \"\"\n            end_tokens = []\n            while self.tokens:\n                token = self.tokens.pop()\n                end_tokens.append(token)\n                end_sequence = token.text + end_sequence\n                if end_sequence in self.stop_sequences:\n                    break\n            else:\n                self.tokens.extend(reversed(end_tokens))\n            yield from self.tokens\n        elif reason == \"eos_token\":\n            if self.tokens:\n                self.tokens.pop()\n            yield from self.tokens\n        else:\n            yield from self.tokens\n\n\ndef get_max_input_length(worker_config: inference.WorkerConfig, plugin_used: bool):\n    \"\"\"Get the maximum possible input length based on the worker config and whether a plugin is in use.\"\"\"\n    max_input_length = worker_config.model_config.max_input_length\n    if plugin_used:\n        max_input_length = max_input_length - 1\n    return max_input_length\n\n\ndef get_tokens_until(tokens: list[int], target: list[int]) -> list[int]:\n    if len(target) == 1:\n        return tokens[: tokens.index(target[0])]\n\n    for i in range(len(tokens) - len(target)):\n        if tokens[i : i + len(target)] == target:\n            break\n    return tokens[:i]\n\n\ndef truncate_prompt(\n    tokenizer: transformers.PreTrainedTokenizer,\n    worker_config: inference.WorkerConfig,\n    parameters: interface.GenerateStreamParameters,\n    prompt: str,\n    plugin_used: bool,\n):\n    \"\"\"\n    Truncate a prompt to ensure it does not exceed the maximum input length. Regardless of truncation, the system\n    prompt is always retained if it is present. If truncation removes the final prompter prefix, a new one is added.\n\n    The stream generation parameters are also updated with a maximum new tokens value which will not cause the total\n    length to exceed the maximum specified in the worker's model config.\n    \"\"\"\n    with shared_tokenizer_lock:\n        ids = tokenizer.encode(prompt)\n        # list of int IDs\n        prompter_prefix_ids = tokenizer.encode(special_tokens[\"prompter\"])\n\n    system_prompt: str | None = None\n    system_tokens: list[int] | None = None\n    if prompt.startswith(special_tokens[\"system\"]):\n        system_prompt = prompt[: prompt.index(special_tokens[\"prompter\"])]\n        system_tokens = get_tokens_until(ids, prompter_prefix_ids)\n\n    max_input_length = get_max_input_length(worker_config, plugin_used)\n\n    if len(ids) > max_input_length:\n        logger.debug(f\"Prompt too long, left-truncating to {max_input_length} tokens\")\n\n        num_system_tokens = len(system_tokens) if system_tokens else 0\n        # Maximum token allowed for the conversation, ex system prompt\n        # We incorporate a buffer to allow for final inference tokenization differing from ours\n        # This is a slightly hacky workaround and it would be better to find a cleaner solution\n        max_conversation_length = max_input_length - num_system_tokens - int(0.01 * max_input_length)\n        ids = ids[-(max_conversation_length - 1) :]\n\n        with shared_tokenizer_lock:\n            prompt = tokenizer.decode(ids)\n\n            if special_tokens[\"prompter\"] not in prompt:\n                prompt = special_tokens[\"prompter\"] + prompt\n                ids = tokenizer.encode(special_tokens[\"prompter\"]) + ids\n\n            if system_tokens:\n                prompt = system_prompt + prompt\n                ids = system_tokens + ids\n\n    max_total_tokens = worker_config.model_config.max_total_length\n    input_length = len(ids)\n    spare = max_total_tokens - input_length - 1\n\n    if not parameters.max_new_tokens:\n        parameters.max_new_tokens = spare\n    elif parameters.max_new_tokens > spare:\n        logger.debug(f\"Max new tokens too high, reducing to {spare}\")\n        parameters.max_new_tokens = spare\n\n    return prompt\n\n\ndef wait_for_inference_server(http: \"HttpClient\", timeout: int = 600):\n    \"\"\"Wait for the \"health\" endpoint of the inference server to return status 200.\"\"\"\n    time_limit = time.time() + timeout\n    while True:\n        try:\n            response = http.get(\"/health\")\n            response.raise_for_status()\n        except (requests.HTTPError, requests.ConnectionError):\n            if time.time() > time_limit:\n                raise\n            sleep_duration = random.uniform(0, 10)\n            logger.warning(f\"Inference server not ready. Retrying in {sleep_duration:.2f} seconds\")\n            time.sleep(sleep_duration)\n        else:\n            logger.info(\"Inference server is ready\")\n            break\n\n\ndef text_to_events(\n    text: str, seed: int | None = None, pause: float = 0.0\n) -> Iterable[interface.GenerateStreamResponse]:\n    \"\"\"\n    Iterate over stream generation \"events\" derived from the given text, where each word in the text is treated as a\n    generated \"token\".\n    \"\"\"\n    tokens = text.split()\n    for token in tokens[:-1]:\n        yield interface.GenerateStreamResponse(\n            token=interface.Token(\n                text=token + \" \",\n                logprob=0.1,\n                id=0,\n            ),\n        )\n        if pause > 0:\n            time.sleep(pause)\n    yield interface.GenerateStreamResponse(\n        token=interface.Token(\n            text=tokens[-1],\n            logprob=0.1,\n            id=0,\n        ),\n        generated_text=text,\n        details=interface.StreamDetails(\n            finish_reason=\"length\",\n            generated_tokens=len(tokens),\n            seed=seed,\n        ),\n    )\n\n\ndef lorem_events(seed):\n    sentence = lorem.paragraph()\n    yield from text_to_events(sentence, seed=seed, pause=0.2)\n\n\nws_lock = threading.Lock()\n\n\ndef send_response(\n    ws: websocket.WebSocket,\n    response: inference.WorkerResponse | inference.WorkerInfo,\n):\n    msg = response.json()\n    with ws_lock:\n        ws.send(msg)\n\n\nclass HttpClient(pydantic.BaseModel):\n    \"\"\"Basic HTTP client built around `requests`. Supports simple authentication.\"\"\"\n\n    base_url: str\n    basic_auth_username: str | None = None\n    basic_auth_password: str | None = None\n    bearer_token: str | None = None\n\n    @property\n    def auth(self):\n        if self.basic_auth_username and self.basic_auth_password:\n            return self.basic_auth_username, self.basic_auth_password\n        else:\n            return None\n\n    def _maybe_add_bearer_token(self, headers: dict[str, str] | None):\n        if self.bearer_token:\n            if headers is None:\n                headers = {}\n            headers[\"Authorization\"] = f\"Bearer {self.bearer_token}\"\n        return headers\n\n    def get(self, path: str, **kwargs):\n        kwargs[\"headers\"] = self._maybe_add_bearer_token(kwargs.get(\"headers\"))\n        return requests.get(self.base_url + path, auth=self.auth, **kwargs)\n\n    def post(self, path: str, **kwargs):\n        kwargs[\"headers\"] = self._maybe_add_bearer_token(kwargs.get(\"headers\"))\n        return requests.post(self.base_url + path, auth=self.auth, **kwargs)\n\n\ndef get_inference_server_stream_events(\n    request: interface.GenerateStreamRequest,\n) -> Iterable[interface.GenerateStreamResponse]:\n    \"\"\"Query the model inference server specified in the worker settings and stream the generation events.\"\"\"\n    http = HttpClient(\n        base_url=settings.inference_server_url,\n        basic_auth_username=settings.basic_auth_username,\n        basic_auth_password=settings.basic_auth_password,\n        bearer_token=settings.bearer_token,\n    )\n    response = http.post(\n        settings.inference_server_route,\n        json=request.dict(),\n        stream=True,\n        headers={\"Accept\": \"text/event-stream\"},\n    )\n    try:\n        response.raise_for_status()\n    except requests.HTTPError:\n        logger.exception(\"Failed to get response from inference server\")\n        logger.error(f\"Response: {response.text}\")\n        raise\n\n    client = sseclient.SSEClient(response)\n    for event in client.events():\n        if event.event == \"error\":\n            logger.error(f\"Error from inference server: {event.data}\")\n            yield interface.GenerateStreamResponse(error=event.data)\n            raise RuntimeError(f\"Error from inference server: {event.data}\")\n        if event.event == \"ping\":\n            continue\n        stream_response = interface.GenerateStreamResponse.parse_raw(event.data)\n        yield stream_response\n", "inference/worker/hf_streamer.py": "import typing\n\nimport transformers\nfrom loguru import logger\n\n\nclass Printer(typing.Protocol):\n    def __call__(self, value: int) -> None:\n        ...\n\n\ndef _unpack(value):\n    if len(value.shape) > 1 and value.shape[0] > 1:\n        raise ValueError(\"HFStreamer only supports batch size 1\")\n    elif len(value.shape) > 1:\n        value = value[0]\n    return value.cpu().tolist()\n\n\n# based on HF text streamer\nclass HFStreamer(transformers.generation.streamers.BaseStreamer):\n    def __init__(self, input_ids, printer: Printer):\n        self.input_ids = _unpack(input_ids)[::-1]\n        self.printer = printer\n\n    def put(self, value):\n        for token_id in _unpack(value):\n            if self.input_ids:\n                input_id = self.input_ids.pop()\n                if input_id != token_id:\n                    logger.warning(f\"Input id {input_id} does not match output id {token_id}\")\n            else:\n                self.printer(token_id)\n\n    def end(self):\n        pass\n", "inference/worker/__main__.py": "import concurrent.futures\nimport signal\nimport sys\nimport time\nfrom contextlib import closing\n\nimport pydantic\nimport transformers\nimport utils\nimport websocket\nimport work\nfrom loguru import logger\nfrom oasst_shared import model_configs\nfrom oasst_shared.schemas import inference\nfrom settings import settings\n\n\ndef terminate_worker(signum, frame):\n    logger.warning(f\"Signal {signum}. Terminating worker...\")\n    sys.exit(0)\n\n\ndef main():\n    signal.signal(signal.SIGINT, terminate_worker)\n    logger.info(f\"Inference protocol version: {inference.INFERENCE_PROTOCOL_VERSION}\")\n\n    model_config = model_configs.MODEL_CONFIGS.get(settings.model_config_name)\n    logger.warning(f\"Model config: {model_config}\")\n    if model_config is None:\n        logger.error(f\"Unknown model config name: {settings.model_config_name}\")\n        sys.exit(2)\n\n    if model_config.is_lorem:\n        tokenizer = None\n    else:\n        tokenizer: transformers.PreTrainedTokenizer = transformers.AutoTokenizer.from_pretrained(model_config.model_id)\n        logger.warning(f\"Tokenizer {tokenizer.name_or_path} vocab size: {len(tokenizer)}\")\n\n    inference_http = utils.HttpClient(\n        base_url=settings.inference_server_url,\n        basic_auth_username=settings.basic_auth_username,\n        basic_auth_password=settings.basic_auth_password,\n        bearer_token=settings.bearer_token,\n    )\n\n    while True:\n        try:\n            if not model_config.is_lorem:\n                utils.wait_for_inference_server(inference_http)\n\n            if settings.perform_oom_test:\n                work.perform_oom_test(tokenizer)\n                sys.exit(0)\n\n            worker_config = inference.WorkerConfig(\n                model_config=model_config,\n                max_parallel_requests=settings.max_parallel_requests,\n            )\n\n            logger.warning(f\"connecting to {settings.backend_url}...\")\n            with closing(\n                websocket.create_connection(\n                    f\"{settings.backend_url}/workers/work\",\n                    header={\n                        \"X-API-Key\": settings.api_key,\n                        \"X-Protocol-Version\": inference.INFERENCE_PROTOCOL_VERSION,\n                    },\n                )\n            ) as ws:\n                logger.warning(\"Connected to backend, sending config...\")\n                worker_info = inference.WorkerInfo(\n                    config=worker_config,\n                    hardware_info=inference.WorkerHardwareInfo(),\n                )\n                utils.send_response(ws, worker_info)\n                logger.warning(\"Config sent, waiting for work...\")\n\n                with concurrent.futures.ThreadPoolExecutor(max_workers=worker_config.max_parallel_requests) as executor:\n                    ftrs = []\n                    while True:\n                        if ftrs:\n                            done, not_done = concurrent.futures.wait(ftrs, timeout=1)\n                            ftrs = list(not_done)\n                            for ftr in done:\n                                ftr.result()\n                        message = ws.recv()\n                        if not message:\n                            logger.warning(\"Connection closed, reconnecting...\")\n                            break\n                        worker_request = pydantic.parse_raw_as(inference.WorkerRequest, message)\n                        match worker_request.request_type:\n                            case \"work\":\n                                logger.info(f\"Handling work request: {worker_request.id=}\")\n                                ftr = executor.submit(\n                                    work.handle_work_request, ws, tokenizer, worker_request, worker_config\n                                )\n                                ftrs.append(ftr)\n                            case \"ping\":\n                                utils.send_response(\n                                    ws,\n                                    inference.PongResponse(\n                                        request_id=worker_request.id, metrics=inference.WorkerMetricsInfo()\n                                    ),\n                                )\n                            case \"wrong_api_key\":\n                                logger.error(\"Your API Key seems to be wrong, please check it!\")\n                                raise RuntimeError(\"Your API Key seems to be wrong, please check it!\")\n                            case \"upgrade_protocol\":\n                                logger.error(\"Your worker is outdated, please upgrade it!\")\n                                sys.exit(2)  # potentially read this status code outside\n                            case \"terminate\":\n                                logger.info(\"Received terminate, terminating worker\")\n                                sys.exit(0)\n                            case \"error\":\n                                logger.error(f\"Received error: {worker_request.error}\")\n                                raise RuntimeError(f\"Received error: {worker_request.error}\")\n\n        except websocket.WebSocketBadStatusException as e:\n            logger.error(f\"Bad status: {e.status_code=} {str(e)=}\")\n            logger.error(\"Did you provide the correct API key?\")\n            if not settings.retry_on_error:\n                sys.exit(1)\n            time.sleep(5)\n        except Exception:\n            logger.exception(\"Error in websocket\")\n            if not settings.retry_on_error:\n                sys.exit(1)\n            logger.warning(\"Retrying in 5 seconds...\")\n            time.sleep(5)\n\n\nif __name__ == \"__main__\":\n    main()\n", "inference/worker/work.py": "import re\nfrom concurrent import futures\n\nimport chat_chain\nimport interface\nimport requests\nimport transformers\nimport utils\nimport websocket\nfrom chat_chain_prompts import (\n    ASSISTANT_PREFIX,\n    CUSTOM_INSTRUCTIONS_PREFIX,\n    END_SEQ,\n    OBSERVATION_SEQ,\n    START_SEQ,\n    THOUGHT_SEQ,\n)\nfrom loguru import logger\nfrom oasst_shared.schemas import inference\nfrom settings import settings\nfrom utils import shared_tokenizer_lock, special_tokens\n\n\ndef make_prompt_and_parameters(\n    tokenizer: transformers.PreTrainedTokenizer,\n    work_request: inference.WorkRequest,\n) -> tuple[str, interface.GenerateStreamParameters]:\n    \"\"\"Prepare a formatted prompt and stream generation parameters based on a work request.\"\"\"\n    if settings.oa_protocol_version != \"v2\":\n        raise RuntimeError(f\"Unsupported oa protocol version: {settings.oa_protocol_version}\")\n\n    eos_token = \"\"\n    if special_tokens[\"end\"]:\n        eos_token = special_tokens[\"end\"]\n    elif hasattr(tokenizer, \"eos_token\"):\n        eos_token = tokenizer.eos_token\n\n    def _prepare_message(message: inference.MessageRead) -> str:\n        prefix = special_tokens[\"assistant\"] if message.is_assistant else special_tokens[\"prompter\"]\n        return prefix + message.content + eos_token\n\n    # Construct prompt\n    messages = [_prepare_message(message) for message in work_request.thread.messages]\n\n    # Prepend system prompt and custom_instructions if it was specified in work parameters\n    work_params = work_request.parameters\n    if work_params.system_prompt or work_params.user_profile or work_params.user_response_instructions:\n        pre_prompt = special_tokens[\"system\"] + (work_params.system_prompt or \"\")\n\n        if work_params.user_profile or work_params.user_response_instructions:\n            pre_prompt = f\"\"\"{pre_prompt}\\n{CUSTOM_INSTRUCTIONS_PREFIX.format(user_profile=work_params.user_profile or \"\", user_response_instructions=work_params.user_response_instructions or \"\")}\"\"\"\n\n        pre_prompt = pre_prompt + eos_token\n        messages = [pre_prompt] + messages\n\n    # Stringify and append assistant prefix to signify start of generation\n    prompt = \"\".join(messages) + special_tokens[\"assistant\"]\n\n    parameters = interface.GenerateStreamParameters.from_work_parameters(work_request.parameters)\n    if settings.use_stop_sequences:\n        parameters.stop = [\n            special_tokens[\"prompter\"],\n            special_tokens[\"assistant\"],\n            special_tokens[\"system\"],\n        ]\n        if eos_token:\n            parameters.stop.append(eos_token)\n    else:\n        parameters.stop = []\n\n    return prompt, parameters\n\n\ndef prepare_safe_prompt(prompt: str, label: str, rots: str) -> str:\n    \"\"\"Given a prompt, safety label, and safety rule of thumb, prepare a 'safe prompt' to replace the prompt.\"\"\"\n    pre_prompt = f\"Answer the following request with {label} as responsible chatbot that believes that {rots}: \"\n    input_list = prompt.split(special_tokens[\"prompter\"])\n    input_list[-1] = pre_prompt + input_list[-1]\n    return special_tokens[\"prompter\"].join(input_list)\n\n\ndef is_safety_triggered(safety_label: str, safety_level: int) -> bool:\n    \"\"\"\n    Determines whether to trigger the safe prompt based on the configured safety level and severity label from the\n    safety classifier.\n    \"\"\"\n    return (\"caution\" in safety_label and safety_level > 1) or (\"intervention\" in safety_label and safety_level > 0)\n\n\ndef parse_safety_response(safety_opinion: str) -> tuple[str, str]:\n    \"\"\"Parse the response from the safety model into a separate label and rule of thumb.\"\"\"\n    safety_opinion = re.sub(r\"<pad>|</s>\", \"\", safety_opinion).split(\"<sep>\")\n    label, rots = safety_opinion[0], \"and\".join([x.strip(\".\") for x in safety_opinion[1:]])\n    label = label.replace(\"<pad>\", \"\").strip()\n    return label, rots\n\n\ndef handle_work_request(\n    ws: websocket.WebSocket,\n    tokenizer: transformers.PreTrainedTokenizer,\n    work_request: inference.WorkRequest,\n    worker_config: inference.WorkerConfig,\n):\n    \"\"\"Handle a work request from end-to-end. Handles plugins and safety if enabled.\"\"\"\n    parameters = interface.GenerateStreamParameters.from_work_parameters(work_request.parameters)\n    prompt = \"\"\n    used_plugin = None\n\n    for plugin in parameters.plugins:\n        if plugin.enabled:\n            prompt, used_plugin = chat_chain.handle_conversation(work_request, worker_config, parameters, tokenizer, ws)\n            # When using plugins and final prompt is truncated due to length limit\n            # LLaMA has tendency to leak internal prompts and generate bad continuations\n            # So we add keywords/sequences to the stop sequences to reduce this\n            parameters.stop.extend([END_SEQ, START_SEQ, THOUGHT_SEQ, f\"{ASSISTANT_PREFIX}:\"])\n            break\n\n    if not used_plugin:\n        prompt, parameters = make_prompt_and_parameters(tokenizer=tokenizer, work_request=work_request)\n\n    logger.debug(f\"Prompt: {prompt}\")\n\n    model_config = worker_config.model_config\n\n    if settings.enable_safety and work_request.safety_parameters.level:\n        safety_request = inference.SafetyRequest(inputs=prompt, parameters=work_request.safety_parameters)\n        safety_response = get_safety_server_response(safety_request)\n        safety_label, safety_rots = parse_safety_response(safety_response.outputs)\n\n        if is_safety_triggered(safety_label, work_request.safety_parameters.level):\n            prompt = prepare_safe_prompt(prompt, safety_label, safety_rots)\n\n            utils.send_response(\n                ws,\n                inference.SafePromptResponse(\n                    request_id=work_request.id,\n                    safe_prompt=prompt,\n                    safety_parameters=work_request.safety_parameters,\n                    safety_label=safety_label,\n                    safety_rots=safety_rots,\n                ),\n            )\n\n            logger.debug(f\"Safe prompt: {prompt}\")\n\n    stream_response = None\n    token_buffer = utils.TokenBuffer(stop_sequences=parameters.stop)\n    if model_config.is_lorem:\n        stream_events = utils.lorem_events(parameters.seed)\n    else:\n        prompt = utils.truncate_prompt(tokenizer, worker_config, parameters, prompt, used_plugin is not None)\n        stream_request = interface.GenerateStreamRequest(\n            inputs=prompt,\n            parameters=parameters,\n        )\n        stream_events = utils.get_inference_server_stream_events(stream_request)\n\n    generated_ids = []\n    decoded_text = \"\"\n    for stream_response in stream_events:\n        if stream_response.is_error:\n            logger.error(f\"Error from inference server: {stream_response.error}\")\n            utils.send_response(\n                ws,\n                inference.ErrorResponse(\n                    request_id=work_request.id,\n                    error=stream_response.error,\n                    metrics=inference.WorkerMetricsInfo(),\n                ),\n            )\n            raise RuntimeError(f\"Error from inference server: {stream_response.error}\")\n        token = stream_response.token\n\n        if model_config.is_llama:\n            generated_ids.append(token.id)\n            try:\n                with shared_tokenizer_lock:\n                    text = tokenizer.decode(generated_ids, skip_special_tokens=True)\n                new_text = text[len(decoded_text) :]\n                if not decoded_text:\n                    new_text = new_text.lstrip()\n            except Exception:\n                text = decoded_text\n                new_text = \"\"\n            token.text = new_text\n            decoded_text = text\n\n        for send_token in token_buffer.add(token):\n            utils.send_response(ws, send_token.to_token_response(request_id=work_request.id))\n    if stream_response is None:\n        logger.error(\"No stream response received\")\n        raise RuntimeError(\"No stream response received\")\n\n    for send_token in token_buffer.finish(reason=stream_response.details.finish_reason):\n        utils.send_response(\n            ws,\n            send_token.to_token_response(request_id=work_request.id),\n        )\n\n    if model_config.is_llama:\n        stream_response.generated_text = stream_response.generated_text.strip()\n        # Helps with RLHF models using plugin prompts. Get generated text to first occurrence of:\n        # START_SEQ, END_SEQ, ASSISTANT_PREFIX, THOUGHT_SEQ, OBSERVATION_SEQ\n        end_seq_index = min(\n            [\n                stream_response.generated_text.find(seq)\n                for seq in [START_SEQ, END_SEQ, f\"{ASSISTANT_PREFIX}:\", THOUGHT_SEQ, OBSERVATION_SEQ]\n                if seq in stream_response.generated_text\n            ]\n            + [len(stream_response.generated_text)]\n        )\n        if end_seq_index != -1 and used_plugin is not None:\n            stream_response.generated_text = stream_response.generated_text[:end_seq_index]\n\n    logger.info(f\"Done. {stream_response=}\")\n    utils.send_response(\n        ws,\n        inference.GeneratedTextResponse(\n            request_id=work_request.id,\n            text=stream_response.generated_text,\n            finish_reason=stream_response.details.finish_reason,\n            metrics=inference.WorkerMetricsInfo(),\n            used_plugin=used_plugin,\n        ),\n    )\n    logger.debug(\"Work complete. Waiting for more work...\")\n\n\ndef get_safety_server_response(request: inference.SafetyRequest) -> inference.SafetyResponse:\n    \"\"\"Query the safety server URL configured in the worker settings.\"\"\"\n    http = utils.HttpClient(base_url=settings.safety_server_url)\n    response = http.post(\"/safety\", json=request.dict())\n    try:\n        response.raise_for_status()\n    except requests.HTTPError:\n        logger.exception(\"Failed to get response from safety server\")\n        logger.error(f\"Response: {response.text}\")\n        raise\n    return inference.SafetyResponse(**response.json())\n\n\ndef perform_oom_test(tokenizer: transformers.PreTrainedTokenizer):\n    logger.warning(\"Performing OOM test\")\n    prompt = (\"This is a test prompt. \" * 10000).strip()\n    parameters = interface.GenerateStreamParameters(\n        max_new_tokens=4,\n        temperature=1.5,\n        top_p=0.95,\n        repetition_penalty=1.0,\n        do_sample=True,\n        stop=[],\n    )\n\n    class OOMError(Exception):\n        pass\n\n    if settings.oom_test_max_length is None:\n        try:\n            for length in range(256, 2**15, 256):\n                prompt_ids = tokenizer.encode(prompt, max_length=length - 4, truncation=True)\n                short_prompt = tokenizer.decode(prompt_ids)\n                stream_request = interface.GenerateStreamRequest(\n                    inputs=short_prompt,\n                    parameters=parameters,\n                )\n                stream_events = utils.get_inference_server_stream_events(stream_request)\n                for stream_response in stream_events:\n                    if stream_response.is_error:\n                        logger.error(f\"Error from inference server: {stream_response.error}\")\n                        raise OOMError()\n        except OOMError:\n            length = length - 256\n        logger.warning(f\"Max length: {length}\")\n    else:\n        length = settings.oom_test_max_length\n\n    with futures.ThreadPoolExecutor() as executor:\n        try:\n            for batch_size in range(1, 32, 1):\n                prompt_ids = tokenizer.encode(prompt, max_length=length - 4, truncation=True)\n                short_prompt = tokenizer.decode(prompt_ids)\n                stream_request = interface.GenerateStreamRequest(\n                    inputs=short_prompt,\n                    parameters=parameters,\n                )\n                ftrs: list[futures.Future] = []\n                try:\n                    for _ in range(batch_size):\n                        stream_events = utils.get_inference_server_stream_events(stream_request)\n                        ftrs.append(executor.submit(list, stream_events))\n                    for ftr in ftrs:\n                        for stream_response in ftr.result():\n                            if stream_response.is_error:\n                                logger.error(f\"Error from inference server: {stream_response.error}\")\n                                raise OOMError()\n                except Exception:\n                    logger.exception(\"OOM\")\n                    try:\n                        for ftr in ftrs:\n                            ftr.cancel()\n                    except Exception:\n                        pass\n                    raise OOMError()\n        except OOMError:\n            batch_size = batch_size - 1\n        logger.warning(f\"Batch size: {batch_size}\")\n\n    logger.warning(\"OOM test complete\")\n    logger.warning(f\"Max length: {length}\")\n    logger.warning(f\"Batch size: {batch_size}\")\n", "inference/worker/download_model.py": "import os\nimport signal\nimport sys\n\nimport transformers\n\n\ndef terminate(signum, frame):\n    print(\"Terminating...\")\n    sys.exit(0)\n\n\nif __name__ == \"__main__\":\n    signal.signal(signal.SIGINT, terminate)\n    model_id = os.getenv(\"MODEL_ID\")\n    if \"llama\" in model_id.lower():\n        transformers.LlamaTokenizer.from_pretrained(model_id)\n        transformers.LlamaForCausalLM.from_pretrained(model_id)\n    else:\n        transformers.AutoTokenizer.from_pretrained(model_id)\n        transformers.AutoModelForCausalLM.from_pretrained(model_id)\n", "inference/worker/hf_langchain_inference.py": "import interface\nimport utils\nfrom langchain.llms.base import LLM\n\n\nclass HFInference(LLM):\n    \"\"\"LangChain LLM implementation which uses the HF inference server configured in the worker settings.\"\"\"\n\n    max_new_tokens: int = 512\n    top_k: int | None = None\n    top_p: float | None = None\n    typical_p: float | None = None\n    temperature: float = 0.8\n    repetition_penalty: float | None = None\n    stop_sequences: list[str] = []\n    seed: int = 42\n    inference_server_url: str = \"\"\n\n    @property\n    def _llm_type(self) -> str:\n        return \"hf-inference\"\n\n    def _call(self, prompt: str, stop: list[str] | None = None) -> str:\n        if stop is None:\n            stop = self.stop_sequences\n        else:\n            stop += self.stop_sequences\n\n        request = interface.GenerateStreamRequest(\n            inputs=prompt,\n            parameters=interface.GenerateStreamParameters(\n                stop=stop,\n                max_new_tokens=self.max_new_tokens,\n                top_k=self.top_k,\n                top_p=self.top_p,\n                typical_p=self.typical_p,\n                temperature=self.temperature,\n                repetition_penalty=self.repetition_penalty,\n                seed=self.seed,\n            ),\n        )\n\n        for event in utils.get_inference_server_stream_events(request):\n            stream_response = event\n\n        generated_text = stream_response.generated_text or \"\"\n\n        for stop_seq in stop:\n            if stop_seq in generated_text:\n                generated_text = generated_text[: generated_text.index(stop_seq)]\n\n        return generated_text\n", "inference/worker/interface.py": "from typing import Literal\n\nimport pydantic\nfrom oasst_shared.schemas import inference\n\n\nclass GenerateStreamParameters(pydantic.BaseModel):\n    max_new_tokens: int = 1024\n    do_sample: bool = True\n    top_k: int | None = None\n    top_p: float | None = None\n    typical_p: float | None = None\n    temperature: float | None = None\n    repetition_penalty: float | None = None\n    seed: int | None = None\n    stop: list[str] = []\n    details: bool = True\n    plugins: list[inference.PluginEntry] = pydantic.Field(default_factory=list[inference.PluginEntry])\n\n    @staticmethod\n    def from_work_parameters(params: inference.WorkParameters) -> \"GenerateStreamParameters\":\n        return GenerateStreamParameters(\n            max_new_tokens=params.sampling_parameters.max_new_tokens,\n            do_sample=params.do_sample,\n            top_k=params.sampling_parameters.top_k,\n            top_p=params.sampling_parameters.top_p,\n            typical_p=params.sampling_parameters.typical_p,\n            temperature=params.sampling_parameters.temperature,\n            repetition_penalty=params.sampling_parameters.repetition_penalty,\n            seed=params.seed,\n            plugins=params.plugins,\n        )\n\n\nclass GenerateStreamRequest(pydantic.BaseModel):\n    inputs: str\n    parameters: GenerateStreamParameters\n\n\nclass Token(pydantic.BaseModel):\n    text: str\n    logprob: float | None\n    id: int\n\n    def __len__(self) -> int:\n        return len(self.text)\n\n    def to_token_response(self, request_id: str) -> inference.TokenResponse:\n        return inference.TokenResponse(\n            request_id=request_id,\n            text=self.text,\n            log_prob=self.logprob,\n            token_id=self.id,\n        )\n\n\nclass StreamDetails(pydantic.BaseModel):\n    generated_tokens: int\n    seed: int | None\n    finish_reason: Literal[\"length\", \"eos_token\", \"stop_sequence\"]\n\n\nclass GenerateStreamResponse(pydantic.BaseModel):\n    token: Token | None\n    generated_text: str | None\n    details: StreamDetails | None\n    error: str | None\n\n    @property\n    def is_end(self) -> bool:\n        return self.generated_text is not None\n\n    @property\n    def is_error(self) -> bool:\n        return self.error is not None\n", "inference/worker/basic_hf_server.py": "\"\"\"\nBasic FastAPI server to serve models using HuggingFace Transformers library.\nThis is an alternative to running the HuggingFace `text-generation-inference` (tgi) server.\n\"\"\"\n\nimport sys\nimport threading\nfrom queue import Queue\n\nimport fastapi\nimport hf_stopping\nimport hf_streamer\nimport interface\nimport torch\nimport transformers\nimport uvicorn\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom loguru import logger\nfrom oasst_shared import model_configs\nfrom settings import settings\nfrom sse_starlette.sse import EventSourceResponse\n\napp = fastapi.FastAPI()\n\nDECODE_TOKEN = \"<decode-token>\"\n\n\n# Allow CORS\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\n\n@app.middleware(\"http\")\nasync def log_exceptions(request: fastapi.Request, call_next):\n    try:\n        response = await call_next(request)\n    except Exception:\n        logger.exception(\"Exception in request\")\n        raise\n    return response\n\n\nmodel_loaded: bool = False\nfully_loaded: bool = False\nmodel_input_queue: Queue = Queue()\n\n\ndef model_thread():\n    \"\"\"Continually obtain new work requests from the model input queue and work on them.\"\"\"\n    model: transformers.PreTrainedModel\n    tokenizer: transformers.PreTrainedTokenizer\n    model, tokenizer, decode_token = load_models()\n\n    request: interface.GenerateStreamRequest\n    output_queue: Queue\n    eos_token_id = tokenizer.eos_token_id if hasattr(tokenizer, \"eos_token_id\") else None\n    while True:\n        request, output_queue = model_input_queue.get()\n        try:\n            prompt = request.inputs\n            params = request.parameters.dict()\n            seed = params.pop(\"seed\")\n            stop_sequences = params.pop(\"stop\")\n            params.pop(\"details\")\n            params.pop(\"plugins\")\n\n            if seed is not None:\n                torch.manual_seed(seed)\n\n            last_token_id = None  # need to delay by 1 to simulate tgi\n\n            def print_text(token_id: int):\n                nonlocal last_token_id\n                if last_token_id is not None:\n                    text = decode_token(last_token_id)\n                    stream_response = interface.GenerateStreamResponse(\n                        token=interface.Token(text=text, id=last_token_id),\n                    )\n                    output_queue.put_nowait(stream_response)\n                last_token_id = token_id\n\n            with torch.no_grad():\n                ids = tokenizer.encode(prompt, return_tensors=\"pt\", add_special_tokens=False)\n                streamer = hf_streamer.HFStreamer(input_ids=ids, printer=print_text)\n                ids = ids.to(model.device)\n                stopping_criteria = (\n                    transformers.StoppingCriteriaList(\n                        [hf_stopping.SequenceStoppingCriteria(tokenizer, stop_sequences, prompt)]\n                    )\n                    if stop_sequences\n                    else None\n                )\n                output = model.generate(\n                    ids,\n                    **params,\n                    streamer=streamer,\n                    eos_token_id=eos_token_id,\n                    stopping_criteria=stopping_criteria,\n                )\n                output = output.cpu()\n                output_ids = output[0][len(ids[0]) :]\n                decoded = tokenizer.decode(output_ids, skip_special_tokens=True)\n\n            stream_response = interface.GenerateStreamResponse(\n                token=interface.Token(\n                    text=decode_token(last_token_id),  # hack because the \"normal\" inference server does this at once\n                    id=last_token_id,\n                ),\n                generated_text=decoded.strip(),\n                details=interface.StreamDetails(\n                    finish_reason=\"eos_token\",\n                    generated_tokens=len(output_ids),\n                    seed=seed,\n                ),\n            )\n            output_queue.put_nowait(stream_response)\n        except Exception as e:\n            logger.exception(\"Exception in model thread\")\n            output_queue.put_nowait(interface.GenerateStreamResponse(error=str(e)))\n\n\ndef load_models():\n    global model_loaded\n\n    torch.set_num_threads(1)\n    torch.set_num_interop_threads(1)\n\n    model_config = model_configs.MODEL_CONFIGS.get(settings.model_config_name)\n    if model_config is None:\n        logger.error(f\"Unknown model config name: {settings.model_config_name}\")\n        sys.exit(2)\n\n    hf_config = transformers.AutoConfig.from_pretrained(model_config.model_id)\n    logger.warning(f\"Loading model {model_config.model_id}...\")\n    tokenizer = transformers.AutoTokenizer.from_pretrained(model_config.model_id)\n    logger.warning(f\"tokenizer {tokenizer.name_or_path} has vocab size {len(tokenizer)}\")\n\n    # see `decode_token` method, taken from HF text-generation-inference\n    tokenizer.add_special_tokens({\"additional_special_tokens\": [\"<decode-token>\"]})\n\n    special_decode_token_id = tokenizer.convert_tokens_to_ids(\"<decode-token>\")\n    special_decode_token_length = len(\"<decode-token>\")\n\n    def decode_token(token_id):\n        result = tokenizer.decode([special_decode_token_id, token_id], skip_special_tokens=False)\n        # slice to remove special decode token\n        return result[special_decode_token_length:]\n\n    config_dtype = hf_config.torch_dtype if hasattr(hf_config, \"torch_dtype\") else torch.float32\n    dtype = torch.bfloat16 if torch.has_cuda and torch.cuda.is_bf16_supported() else config_dtype\n\n    model = transformers.AutoModelForCausalLM.from_pretrained(\n        model_config.model_id,\n        torch_dtype=dtype,\n        load_in_8bit=settings.quantize,\n        device_map=\"auto\" if torch.cuda.is_available() else None,\n    ).eval()\n    logger.warning(\"Model loaded, using it once...\")\n\n    # warmup\n    with torch.no_grad():\n        text = \"Hello, world\"\n        tokens = tokenizer.encode(text, return_tensors=\"pt\")\n        tokens = tokens.to(model.device)\n        model.generate(tokens, max_length=10, num_beams=1, do_sample=False)\n\n    model_loaded = True\n\n    return model, tokenizer, decode_token\n\n\n@app.on_event(\"startup\")\nasync def start_model_thread():\n    logger.warning(\"Starting model thread...\")\n    threading.Thread(target=model_thread, daemon=True).start()\n    logger.warning(\"Model thread started\")\n\n\n@app.on_event(\"startup\")\nasync def welcome_message():\n    global fully_loaded\n    logger.warning(\"Server started\")\n    logger.warning(\"To stop the server, press Ctrl+C\")\n    fully_loaded = True\n\n\n@app.post(\"/generate_stream\")\nasync def generate(\n    request: interface.GenerateStreamRequest,\n):\n    def event_stream():\n        try:\n            output_queue: Queue = Queue()\n            model_input_queue.put_nowait((request, output_queue))\n            while True:\n                output = output_queue.get()  # type: interface.GenerateStreamResponse\n                yield {\"data\": output.json()}\n                if output.is_end:\n                    break\n                if output.is_error:\n                    raise Exception(output.error)\n        except Exception as e:\n            logger.exception(\"Exception in event stream\")\n            output_queue.put_nowait(interface.GenerateStreamResponse(error=str(e)))\n            raise\n\n    return EventSourceResponse(event_stream())\n\n\n@app.get(\"/health\")\nasync def health():\n    if not (fully_loaded and model_loaded):\n        raise fastapi.HTTPException(status_code=503, detail=\"Server not fully loaded\")\n    return {\"status\": \"ok\"}\n\n\nif __name__ == \"__main__\":\n    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n", "inference/safety/settings.py": "import pydantic\n\n\nclass Settings(pydantic.BaseSettings):\n    # HuggingFace model ID for the model to load in blade2blade\n    safety_model_name: str = \"shahules786/blade2blade-t5-base\"\n\n\nsettings = Settings()\n", "inference/safety/main.py": "\"\"\"\nA simple FastAPI server which serves a `blade2blade2` safety model.\n\nSee https://github.com/LAION-AI/blade2blade for context.\n\"\"\"\n\nimport asyncio\nfrom concurrent.futures import ThreadPoolExecutor\n\nimport fastapi\nimport uvicorn\nfrom blade2blade import Blade2Blade\nfrom loguru import logger\nfrom oasst_shared.schemas import inference\nfrom settings import settings\n\napp = fastapi.FastAPI()\n\n\n@app.middleware(\"http\")\nasync def log_exceptions(request: fastapi.Request, call_next):\n    try:\n        response = await call_next(request)\n    except Exception:\n        logger.exception(\"Exception in request\")\n        raise\n    return response\n\n\npipeline_loaded: bool = False\npipeline: Blade2Blade\nexecutor = ThreadPoolExecutor()\n\n\n@app.on_event(\"startup\")\nasync def load_pipeline():\n    global pipeline_loaded, pipeline\n    pipeline = Blade2Blade(settings.safety_model_name)\n    # warmup\n    input = \"|prompter|Hey,how are you?|endoftext|\"\n    _ = pipeline.predict(input)\n    pipeline_loaded = True\n    logger.info(\"Pipeline loaded\")\n\n\nasync def async_predict(pipeline: Blade2Blade, inputs: str):\n    \"\"\"Run predictions in a separate thread for a small server parallelism benefit.\"\"\"\n    return await asyncio.get_event_loop().run_in_executor(executor, pipeline.predict, inputs)\n\n\n@app.post(\"/safety\", response_model=inference.SafetyResponse)\nasync def safety(request: inference.SafetyRequest):\n    global pipeline_loaded, pipeline\n    while not pipeline_loaded:\n        await asyncio.sleep(1)\n    outputs = await async_predict(pipeline, request.inputs)\n    return inference.SafetyResponse(outputs=outputs)\n\n\n@app.get(\"/health\")\nasync def health():\n    if not pipeline_loaded:\n        raise fastapi.HTTPException(status_code=503, detail=\"Server not fully loaded\")\n    return {\"status\": \"ok\"}\n\n\nif __name__ == \"__main__\":\n    uvicorn.run(app, host=\"0.0.0.0\", port=8008)\n", "inference/text-client/text_client_utils.py": "import json\n\nimport requests\nimport sseclient\nfrom loguru import logger\n\n\nclass DebugClient:\n    def __init__(self, backend_url, http_client=requests):\n        self.backend_url = backend_url\n        self.http_client = http_client\n        self.auth_headers = None\n        self.available_models = self.get_available_models()\n\n    def login(self, username):\n        auth_data = self.http_client.get(f\"{self.backend_url}/auth/callback/debug\", params={\"code\": username}).json()\n        assert auth_data[\"access_token\"][\"token_type\"] == \"bearer\"\n        bearer_token = auth_data[\"access_token\"][\"access_token\"]\n        logger.debug(f\"Logged in as {username} with token {bearer_token}\")\n        self.auth_headers = {\"Authorization\": f\"Bearer {bearer_token}\"}\n\n    def create_chat(self):\n        response = self.http_client.post(\n            f\"{self.backend_url}/chats\",\n            json={},\n            headers=self.auth_headers,\n        )\n        response.raise_for_status()\n        self.chat_id = response.json()[\"id\"]\n        self.message_id = None\n        return self.chat_id\n\n    def get_available_models(self):\n        response = self.http_client.get(\n            f\"{self.backend_url}/configs/model_configs\",\n            headers=self.auth_headers,\n        )\n        response.raise_for_status()\n        return [model[\"name\"] for model in response.json()]\n\n    def send_message(self, message, model_config_name):\n        available_models = self.get_available_models()\n        if model_config_name not in available_models:\n            raise ValueError(f\"Invalid model config name: {model_config_name}\")\n        response = self.http_client.post(\n            f\"{self.backend_url}/chats/{self.chat_id}/prompter_message\",\n            json={\n                \"parent_id\": self.message_id,\n                \"content\": message,\n            },\n            headers=self.auth_headers,\n        )\n        response.raise_for_status()\n        prompter_message_id = response.json()[\"id\"]\n\n        response = self.http_client.post(\n            f\"{self.backend_url}/chats/{self.chat_id}/assistant_message\",\n            json={\n                \"parent_id\": prompter_message_id,\n                \"model_config_name\": model_config_name,\n                \"sampling_parameters\": {\n                    \"top_p\": 0.95,\n                    \"top_k\": 50,\n                    \"repetition_penalty\": 1.2,\n                    \"temperature\": 1.0,\n                },\n            },\n            headers=self.auth_headers,\n        )\n        response.raise_for_status()\n        self.message_id = response.json()[\"id\"]\n\n        response = self.http_client.get(\n            f\"{self.backend_url}/chats/{self.chat_id}/messages/{self.message_id}/events\",\n            stream=True,\n            headers={\n                \"Accept\": \"text/event-stream\",\n                **self.auth_headers,\n            },\n        )\n        response.raise_for_status()\n        if response.status_code == 204:\n            response = self.http_client.get(\n                f\"{self.backend_url}/chats/{self.chat_id}/messages/{self.message_id}\",\n                headers=self.auth_headers,\n            )\n            response.raise_for_status()\n            data = response.json()\n            yield data[\"content\"]\n        else:\n            client = sseclient.SSEClient(response)\n            events = iter(client.events())\n            for event in events:\n                if event.event == \"error\":\n                    raise RuntimeError(event.data)\n                if event.event == \"ping\":\n                    continue\n                try:\n                    data = json.loads(event.data)\n                except json.JSONDecodeError:\n                    raise RuntimeError(f\"Failed to decode {event.data=}\")\n                event_type = data[\"event_type\"]\n                if event_type == \"token\":\n                    yield data[\"text\"]\n                elif event_type == \"message\":\n                    # full message content, can be ignored here\n                    break\n                elif event_type == \"error\":\n                    raise RuntimeError(data[\"error\"])\n                elif event_type == \"pending\":\n                    logger.debug(f\"Message pending. {data=}\")\n", "inference/text-client/__main__.py": "\"\"\"Simple REPL frontend.\"\"\"\n\nimport time\n\nimport text_client_utils as utils\nimport typer\nfrom loguru import logger\n\napp = typer.Typer()\n\n\n@app.command()\ndef main(backend_url: str = \"http://127.0.0.1:8000\", model_config_name=\"distilgpt2\", username=\"test1\"):\n    \"\"\"Simple REPL client.\"\"\"\n    while True:\n        try:\n            # login\n            client = utils.DebugClient(backend_url)\n            client.login(username)\n            chat_id = client.create_chat()\n            typer.echo(f\"Chat ID: {chat_id}\")\n            while True:\n                message = typer.prompt(\"User\").strip()\n                if not message:\n                    continue\n\n                events = client.send_message(message, model_config_name)\n                print(\"Assistant: \", end=\"\", flush=True)\n                for event in events:\n                    print(event, end=\"\", flush=True)\n                print()\n\n        except typer.Abort:\n            typer.echo(\"Exiting...\")\n            break\n        except Exception as e:\n            logger.exception(\"Chat Error\")\n            typer.echo(f\"Error: {e}\")\n            typer.echo(\"Error, restarting chat...\")\n            time.sleep(1)\n\n\nif __name__ == \"__main__\":\n    app()\n", "inference/server/main.py": "import asyncio\nimport math\nimport signal\nimport sys\n\nimport fastapi\nimport redis.asyncio as redis\nimport sqlmodel\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom fastapi_limiter import FastAPILimiter\nfrom loguru import logger\nfrom oasst_inference_server import database, deps, models, plugins\nfrom oasst_inference_server.routes import account, admin, auth, chats, configs, workers\nfrom oasst_inference_server.settings import settings\nfrom oasst_shared.schemas import inference\nfrom prometheus_fastapi_instrumentator import Instrumentator\nfrom starlette.middleware.sessions import SessionMiddleware\nfrom starlette.status import HTTP_429_TOO_MANY_REQUESTS\n\napp = fastapi.FastAPI(title=settings.PROJECT_NAME)\n\n\n# Allow CORS\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=settings.inference_cors_origins_list,\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n# Session middleware for authlib\napp.add_middleware(SessionMiddleware, secret_key=settings.session_middleware_secret_key)\n\n\n@app.middleware(\"http\")\nasync def log_exceptions(request: fastapi.Request, call_next):\n    try:\n        response = await call_next(request)\n    except Exception:\n        logger.exception(\"Exception in request\")\n        raise\n    return response\n\n\n# add prometheus metrics at /metrics\n@app.on_event(\"startup\")\nasync def enable_prom_metrics():\n    Instrumentator().instrument(app).expose(app)\n\n\n@app.on_event(\"startup\")\nasync def log_inference_protocol_version():\n    logger.warning(f\"Inference protocol version: {inference.INFERENCE_PROTOCOL_VERSION}\")\n\n\ndef terminate_server(signum, frame):\n    logger.warning(f\"Signal {signum}. Terminating server...\")\n    sys.exit(0)\n\n\n@app.on_event(\"startup\")\nasync def alembic_upgrade():\n    \"\"\"Upgrades database schema based on Alembic migration scripts.\"\"\"\n    signal.signal(signal.SIGINT, terminate_server)\n    if not settings.update_alembic:\n        logger.warning(\"Skipping alembic upgrade on startup (update_alembic is False)\")\n        return\n    logger.warning(\"Attempting to upgrade alembic on startup\")\n    retry = 0\n    while True:\n        try:\n            async with database.make_engine().begin() as conn:\n                await conn.run_sync(database.alembic_upgrade)\n            logger.warning(\"Successfully upgraded alembic on startup\")\n            break\n        except Exception:\n            logger.exception(\"Alembic upgrade failed on startup\")\n            retry += 1\n            if retry >= settings.alembic_retries:\n                raise\n\n            timeout = settings.alembic_retry_timeout * 2**retry\n            logger.warning(f\"Retrying alembic upgrade in {timeout} seconds\")\n            await asyncio.sleep(timeout)\n    signal.signal(signal.SIGINT, signal.SIG_DFL)\n\n\n@app.on_event(\"startup\")\nasync def setup_rate_limiter():\n    if not settings.rate_limit:\n        logger.warning(\"Skipping rate limiter setup on startup (rate_limit is False)\")\n        return\n\n    async def http_callback(request: fastapi.Request, response: fastapi.Response, pexpire: int):\n        \"\"\"Error callback function when too many requests\"\"\"\n        expire = math.ceil(pexpire / 1000)\n        raise fastapi.HTTPException(f\"Too Many Requests. Retry After {expire} seconds.\", HTTP_429_TOO_MANY_REQUESTS)\n\n    try:\n        client = redis.Redis(\n            host=settings.redis_host, port=settings.redis_port, db=settings.redis_ratelim_db, decode_responses=True\n        )\n        logger.info(f\"Connected to {client=}\")\n        await FastAPILimiter.init(client, http_callback=http_callback)\n    except Exception:\n        logger.exception(\"Failed to establish Redis connection\")\n\n\n@app.on_event(\"startup\")\nasync def maybe_add_debug_api_keys():\n    debug_api_keys = settings.debug_api_keys_list\n    if not debug_api_keys:\n        logger.warning(\"No debug API keys configured, skipping\")\n        return\n    try:\n        logger.warning(\"Adding debug API keys\")\n        async with deps.manual_create_session() as session:\n            for api_key in debug_api_keys:\n                logger.info(f\"Checking if debug API key {api_key} exists\")\n                if (\n                    await session.exec(sqlmodel.select(models.DbWorker).where(models.DbWorker.api_key == api_key))\n                ).one_or_none() is None:\n                    logger.info(f\"Adding debug API key {api_key}\")\n                    session.add(models.DbWorker(api_key=api_key, name=\"Debug API Key\"))\n                    await session.commit()\n                else:\n                    logger.info(f\"Debug API key {api_key} already exists\")\n        logger.warning(\"Finished adding debug API keys\")\n    except Exception:\n        logger.exception(\"Failed to add debug API keys\")\n        raise\n\n\n# add routes\napp.include_router(account.router)\napp.include_router(auth.router)\napp.include_router(admin.router)\napp.include_router(chats.router)\napp.include_router(workers.router)\napp.include_router(configs.router)\n\n# mount builtin plugins to be hosted on this server\nfor app_prefix, sub_app in plugins.plugin_apps.items():\n    app.mount(path=settings.plugins_path_prefix + app_prefix, app=sub_app)\n\n\n@app.on_event(\"startup\")\nasync def welcome_message():\n    logger.warning(\"Inference server started\")\n    logger.warning(\"To stop the server, press Ctrl+C\")\n", "inference/server/export.py": "\"\"\"Script to facilitate exporting chat data from the server database.\"\"\"\n\nimport argparse\nimport asyncio\nimport contextlib\nimport datetime as dt\nimport gzip\nimport json\nimport sys\nfrom collections import defaultdict\nfrom pathlib import Path\nfrom typing import Any, TextIO\n\nimport sqlalchemy\nimport sqlmodel\nfrom fastapi.encoders import jsonable_encoder\nfrom oasst_data import (\n    ExportMessageEvent,\n    ExportMessageEventReport,\n    ExportMessageEventScore,\n    ExportMessageNode,\n    ExportMessageTree,\n)\nfrom oasst_inference_server import deps\nfrom oasst_inference_server.database import AsyncSession\nfrom oasst_inference_server.models import DbChat, DbMessage\nfrom oasst_shared.utils import Anonymizer\n\n\n# see https://stackoverflow.com/questions/17602878/how-to-handle-both-with-open-and-sys-stdout-nicely\n@contextlib.contextmanager\ndef smart_open(filename: str = None) -> TextIO:\n    if filename and filename != \"-\":\n        fh = open(filename, \"wt\", encoding=\"UTF-8\")\n    else:\n        fh = sys.stdout\n\n    try:\n        yield fh\n    finally:\n        if fh is not sys.stdout:\n            fh.close()\n\n\ndef maybe_anonymize(anonymizer: Anonymizer | None, collection: str, key: str) -> str:\n    if anonymizer:\n        return anonymizer.anonymize(collection, key)\n    else:\n        return key\n\n\ndef prepare_export_events(\n    chat: DbChat,\n    message: DbMessage,\n    anonymizer: Anonymizer | None = None,\n) -> dict[str, list[ExportMessageEvent]]:\n    export_events: dict[str, list[ExportMessageEvent]] = {}\n\n    if message.reports:\n        export_events[\"report\"] = [\n            ExportMessageEventReport(\n                user_id=maybe_anonymize(anonymizer, \"user\", str(chat.user_id)),\n                report_type=str(db_report.report_type),\n                reason=db_report.reason,\n            )\n            for db_report in message.reports\n        ]\n\n    if message.score:\n        export_events[\"score\"] = [\n            ExportMessageEventScore(\n                user_id=maybe_anonymize(anonymizer, \"user\", str(chat.user_id)),\n                score=message.score,\n            )\n        ]\n\n    return export_events\n\n\ndef prepare_export_message_tree(\n    chat: DbChat,\n    anonymizer: Anonymizer | None = None,\n) -> ExportMessageTree:\n    messages: list[DbMessage] = chat.messages\n\n    # Exclude messages without content (e.g. work still in progress or aborted)\n    export_messages: list[ExportMessageNode] = [\n        prepare_export_message_node(chat, message, anonymizer=anonymizer) for message in messages if message.content\n    ]\n\n    messages_by_parent = defaultdict(list)\n    for message in export_messages:\n        messages_by_parent[message.parent_id].append(message)\n\n    def assign_replies(node: ExportMessageNode) -> ExportMessageNode:\n        node.replies = messages_by_parent[node.message_id]\n        for child in node.replies:\n            assign_replies(child)\n        return node\n\n    prompt = assign_replies(messages_by_parent[None][0])\n    return ExportMessageTree(message_tree_id=str(chat.id), tree_state=\"not_applicable\", prompt=prompt)\n\n\ndef prepare_export_message_node(\n    chat: DbChat,\n    message: DbMessage,\n    anonymizer: Anonymizer | None = None,\n) -> ExportMessageNode:\n    if message.worker_config:\n        model_name = message.worker_config.model_config.model_id\n    else:\n        model_name = None\n\n    # Chat prompts are human-written, responses are synthetic\n    synthetic = message.role == \"assistant\"\n\n    events: dict[str, list[ExportMessageEvent]] = prepare_export_events(chat, message, anonymizer=anonymizer)\n\n    message_id = maybe_anonymize(anonymizer, \"message\", message.id)\n    parent_id = maybe_anonymize(anonymizer, \"message\", message.parent_id)\n    user_id = maybe_anonymize(anonymizer, \"user\", chat.user_id)\n\n    return ExportMessageNode(\n        message_id=message_id,\n        parent_id=parent_id,\n        user_id=user_id,\n        created_date=message.created_at,\n        text=message.content,\n        role=message.role,\n        synthetic=synthetic,\n        model_name=model_name,\n        events=events,\n    )\n\n\ndef write_messages_to_file(\n    file: Path,\n    chats: list[DbChat],\n    use_compression: bool = True,\n    write_trees: bool = True,\n    anonymizer: Anonymizer | None = None,\n) -> None:\n    out_buff: TextIO\n\n    if use_compression:\n        if not file:\n            raise RuntimeError(\"File name must be specified when using compression.\")\n        out_buff = gzip.open(file, \"wt\", encoding=\"UTF-8\")\n    else:\n        out_buff = smart_open(file)\n\n    with out_buff as f:\n        for chat in chats:\n            if write_trees:\n                export_chat = prepare_export_message_tree(chat, anonymizer=anonymizer)\n                file_data = jsonable_encoder(export_chat, exclude_none=True)\n                json.dump(file_data, f)\n                f.write(\"\\n\")\n            else:\n                # Exclude messages without content (e.g. work still in progress or aborted)\n                for message in filter(lambda m: m.content, chat.messages):\n                    export_message = prepare_export_message_node(chat, message, anonymizer=anonymizer)\n                    file_data = jsonable_encoder(export_message, exclude_none=True)\n                    json.dump(file_data, f)\n                    f.write(\"\\n\")\n\n\nasync def fetch_eligible_chats(session_generator, filters: list[Any]) -> list[DbChat]:\n    \"\"\"Fetch chats which are not opted out of data collection and match the given filters.\"\"\"\n    session: AsyncSession\n    filters.append(DbChat.allow_data_use)\n    async with session_generator() as session:\n        query = (\n            sqlmodel.select(DbChat)\n            .filter(*filters)\n            .options(\n                sqlalchemy.orm.joinedload(\"*\"),\n            )\n        )\n        chats: list[DbChat] = (await session.exec(query)).unique().all()\n        return chats\n\n\ndef export_chats(\n    session_generator,\n    export_path: Path,\n    filters: list[Any],\n    use_compression: bool = True,\n    write_trees: bool = True,\n    anonymizer_seed: str | None = None,\n) -> None:\n    eligible_chats: list[DbChat] = asyncio.run(fetch_eligible_chats(session_generator, filters))\n    anonymizer = Anonymizer(anonymizer_seed) if anonymizer_seed else None\n\n    write_messages_to_file(\n        export_path,\n        eligible_chats,\n        write_trees=write_trees,\n        use_compression=use_compression,\n        anonymizer=anonymizer,\n    )\n\n\ndef parse_date(date_str: str) -> dt.date:\n    return dt.datetime.strptime(date_str, \"%Y-%m-%d\").date()\n\n\ndef parse_args() -> argparse.Namespace:\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--export-file\",\n        type=str,\n        help=\"Name of file to export chats to. If not provided, output will be sent to STDOUT\",\n    )\n    parser.add_argument(\n        \"--no-compression\",\n        action=\"store_true\",\n        help=\"Disable compression when writing to file.\",\n    )\n    parser.add_argument(\n        \"--write-flat\",\n        action=\"store_true\",\n        help=\"Write chats as individual messages rather than trees.\",\n    )\n    parser.add_argument(\n        \"--anonymizer-seed\",\n        type=int,\n        help=\"Seed for the anonymizer. If not specified, no anonymization will be performed.\",\n    )\n    parser.add_argument(\n        \"--from-date\",\n        type=parse_date,\n        help=\"Only export chats created on or after this date. Format: YYYY-MM-DD\",\n    )\n    parser.add_argument(\n        \"--to-date\",\n        type=parse_date,\n        help=\"Only export chats created on or before this date. Format: YYYY-MM-DD\",\n    )\n    parser.add_argument(\n        \"--user-id\",\n        type=str,\n        help=\"Only export chats created by this user.\",\n    )\n    parser.add_argument(\n        \"--chat-id\",\n        type=str,\n        help=\"Only export this chat.\",\n    )\n    parser.add_argument(\n        \"--scored\",\n        action=\"store_true\",\n        help=\"Only export chats with at least one assistant message with score != 0.\",\n    ),\n    parser.add_argument(\n        \"--reported\",\n        action=\"store_true\",\n        help=\"Only export chats with at least one reported message.\",\n    )\n    return parser.parse_args()\n\n\ndef create_filters(args: argparse.Namespace) -> list[Any]:\n    filters = []\n\n    if args.from_date:\n        filters.append(DbChat.created_at >= args.from_date)\n    if args.to_date:\n        filters.append(DbChat.created_at <= args.to_date)\n    if args.user_id:\n        filters.append(DbChat.user_id == args.user_id)\n    if args.chat_id:\n        filters.append(DbChat.id == args.chat_id)\n    if args.scored:\n        filters.append(DbChat.messages.any((DbMessage.role == \"assistant\") & (DbMessage.score != 0)))\n    if args.reported:\n        filters.append(DbChat.messages.any(DbMessage.reports.any()))\n\n    return filters\n\n\ndef main():\n    args = parse_args()\n\n    export_path = Path(args.export_file) if args.export_file else None\n    filters = create_filters(args)\n\n    export_chats(\n        deps.manual_create_session,\n        export_path,\n        filters,\n        use_compression=not args.no_compression,\n        write_trees=not args.write_flat,\n        anonymizer_seed=args.anonymizer_seed,\n    )\n\n\nif __name__ == \"__main__\":\n    main()\n", "inference/server/alembic/env.py": "import asyncio\nfrom logging.config import fileConfig\n\nimport sqlmodel\nfrom alembic import context\nfrom loguru import logger\nfrom oasst_inference_server import models  # noqa: F401\nfrom sqlalchemy import engine_from_config, pool, text\nfrom sqlalchemy.ext.asyncio import AsyncEngine\n\n# this is the Alembic Config object, which provides\n# access to the values within the .ini file in use.\nconfig = context.config\n\n# Interpret the config file for Python logging.\n# This line sets up loggers basically.\nif config.config_file_name is not None:\n    fileConfig(config.config_file_name)\n\n# add your model's MetaData object here\n# for 'autogenerate' support\n# from myapp import mymodel\n# target_metadata = mymodel.Base.metadata\ntarget_metadata = sqlmodel.SQLModel.metadata\n\n# other values from the config, defined by the needs of env.py,\n# can be acquired:\n# my_important_option = config.get_main_option(\"my_important_option\")\n# ... etc.\n\n\ndef run_migrations_offline() -> None:\n    \"\"\"Run migrations in 'offline' mode.\n\n    This configures the context with just a URL\n    and not an Engine, though an Engine is acceptable\n    here as well.  By skipping the Engine creation\n    we don't even need a DBAPI to be available.\n\n    Calls to context.execute() here emit the given string to the\n    script output.\n\n    \"\"\"\n    url = config.get_main_option(\"sqlalchemy.url\")\n    context.configure(\n        url=url,\n        target_metadata=target_metadata,\n        literal_binds=True,\n        dialect_opts={\"paramstyle\": \"named\"},\n    )\n\n    with context.begin_transaction():\n        context.run_migrations()\n\n\ndef do_run_migrations(connection):\n    context.configure(connection=connection, target_metadata=target_metadata)\n\n    with context.begin_transaction():\n        context.get_context()._ensure_version_table()\n        connection.execute(text(\"LOCK TABLE alembic_version IN ACCESS EXCLUSIVE MODE\"))\n        context.run_migrations()\n\n\nasync def run_async_migrations() -> None:\n    \"\"\"Run migrations in 'online' mode.\n\n    In this scenario we need to create an Engine\n    and associate a connection with the context.\n\n    \"\"\"\n    connectable = engine_from_config(\n        config.get_section(config.config_ini_section),\n        prefix=\"sqlalchemy.\",\n        poolclass=pool.NullPool,\n        future=True,\n    )\n\n    connectable = AsyncEngine(connectable)\n\n    logger.info(f\"Running migrations on {connectable.url}\")\n\n    async with connectable.connect() as connection:\n        logger.info(\"Connected to database\")\n        await connection.run_sync(do_run_migrations)\n        logger.info(\"Migrations complete\")\n    logger.info(\"Disconnecting from database\")\n    await connectable.dispose()\n    logger.info(\"Disconnected from database\")\n\n\nif context.is_offline_mode():\n    run_migrations_offline()\nelse:\n    connection = config.attributes.get(\"connection\", None)\n    if connection is None:\n        asyncio.run(run_async_migrations())\n    else:\n        do_run_migrations(connection)\n", "inference/server/alembic/versions/2023_03_22_2113-78f16015b904_add_refresh_token_table.py": "\"\"\"Add refresh token table\n\nRevision ID: 78f16015b904\nRevises: 629d5081160f\nCreate Date: 2023-03-22 21:13:41.411718\n\n\"\"\"\nimport sqlalchemy as sa\nimport sqlmodel\nfrom alembic import op\n\n# revision identifiers, used by Alembic.\nrevision = \"78f16015b904\"\ndown_revision = \"629d5081160f\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.create_table(\n        \"refresh_token\",\n        sa.Column(\"token_hash\", sqlmodel.sql.sqltypes.AutoString(), nullable=False),\n        sa.Column(\"user_id\", sqlmodel.sql.sqltypes.AutoString(), nullable=False),\n        sa.Column(\"enabled\", sa.Boolean(), nullable=False),\n        sa.ForeignKeyConstraint(\n            [\"user_id\"],\n            [\"user.id\"],\n        ),\n        sa.PrimaryKeyConstraint(\"token_hash\"),\n    )\n    op.create_index(op.f(\"ix_refresh_token_user_id\"), \"refresh_token\", [\"user_id\"], unique=False)\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_index(op.f(\"ix_refresh_token_user_id\"), table_name=\"refresh_token\")\n    op.drop_table(\"refresh_token\")\n    # ### end Alembic commands ###\n", "inference/server/alembic/versions/2023_05_01_2253-5b4211625a9f_added_used_plugin_to_message.py": "\"\"\"added used plugin to message\n\nRevision ID: 5b4211625a9f\nRevises: ea19bbc743f9\nCreate Date: 2023-05-01 22:53:16.297495\n\n\"\"\"\nimport sqlalchemy as sa\nfrom alembic import op\nfrom sqlalchemy.dialects import postgresql\n\n# revision identifiers, used by Alembic.\nrevision = \"5b4211625a9f\"\ndown_revision = \"ea19bbc743f9\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.add_column(\"message\", sa.Column(\"used_plugin\", postgresql.JSONB(astext_type=sa.Text()), nullable=True))\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_column(\"message\", \"used_plugin\")\n    # ### end Alembic commands ###\n", "inference/server/alembic/versions/2023_04_12_2033-f0e18084aae4_add_deleted_field_to_user.py": "\"\"\"Add deleted field to user\n\nRevision ID: f0e18084aae4\nRevises: 78f16015b904\nCreate Date: 2023-04-12 20:33:28.239793\n\n\"\"\"\nimport sqlalchemy as sa\nfrom alembic import op\n\n# revision identifiers, used by Alembic.\nrevision = \"f0e18084aae4\"\ndown_revision = \"78f16015b904\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.add_column(\"user\", sa.Column(\"deleted\", sa.Boolean(), server_default=sa.text(\"false\"), nullable=False))\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_column(\"user\", \"deleted\")\n    # ### end Alembic commands ###\n", "inference/server/alembic/versions/2023_05_29_1551-5ed411a331f4_add_active_thread_tail_messsage_id_and_.py": "\"\"\"add_active_thread_tail_messsage_id_and_message_eval\n\nRevision ID: 5ed411a331f4\nRevises: 5b4211625a9f\nCreate Date: 2023-05-29 15:51:41.857262\n\n\"\"\"\nimport sqlalchemy as sa\nimport sqlmodel\nfrom alembic import op\nfrom sqlalchemy.dialects import postgresql\n\n# revision identifiers, used by Alembic.\nrevision = \"5ed411a331f4\"\ndown_revision = \"5b4211625a9f\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.create_table(\n        \"message_evaluation\",\n        sa.Column(\"inferior_message_ids\", postgresql.JSONB(astext_type=sa.Text()), nullable=True),\n        sa.Column(\"id\", sqlmodel.sql.sqltypes.AutoString(), nullable=False),\n        sa.Column(\"chat_id\", sqlmodel.sql.sqltypes.AutoString(), nullable=False),\n        sa.Column(\"user_id\", sqlmodel.sql.sqltypes.AutoString(), nullable=False),\n        sa.Column(\"selected_message_id\", sqlmodel.sql.sqltypes.AutoString(), nullable=False),\n        sa.ForeignKeyConstraint(\n            [\"chat_id\"],\n            [\"chat.id\"],\n        ),\n        sa.ForeignKeyConstraint(\n            [\"selected_message_id\"],\n            [\"message.id\"],\n        ),\n        sa.ForeignKeyConstraint(\n            [\"user_id\"],\n            [\"user.id\"],\n        ),\n        sa.PrimaryKeyConstraint(\"id\"),\n    )\n    op.create_index(op.f(\"ix_message_evaluation_chat_id\"), \"message_evaluation\", [\"chat_id\"], unique=False)\n    op.create_index(op.f(\"ix_message_evaluation_user_id\"), \"message_evaluation\", [\"user_id\"], unique=False)\n    op.add_column(\"chat\", sa.Column(\"active_thread_tail_message_id\", sqlmodel.sql.sqltypes.AutoString(), nullable=True))\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_column(\"chat\", \"active_thread_tail_message_id\")\n    op.drop_index(op.f(\"ix_message_evaluation_user_id\"), table_name=\"message_evaluation\")\n    op.drop_index(op.f(\"ix_message_evaluation_chat_id\"), table_name=\"message_evaluation\")\n    op.drop_table(\"message_evaluation\")\n    # ### end Alembic commands ###\n", "inference/server/alembic/versions/2023_04_29_1739-ea19bbc743f9_add_safe_content_to_message.py": "\"\"\"Add safe_content to message\n\nRevision ID: ea19bbc743f9\nRevises: 401eef162771\nCreate Date: 2023-04-14 22:37:41.373382\n\n\"\"\"\nimport sqlalchemy as sa\nimport sqlmodel\nfrom alembic import op\n\n# revision identifiers, used by Alembic.\nrevision = \"ea19bbc743f9\"\ndown_revision = \"401eef162771\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.add_column(\"message\", sa.Column(\"safe_content\", sqlmodel.sql.sqltypes.AutoString(), nullable=True))\n    op.add_column(\"message\", sa.Column(\"safety_level\", sa.Integer(), nullable=True))\n    op.add_column(\"message\", sa.Column(\"safety_label\", sqlmodel.sql.sqltypes.AutoString(), nullable=True))\n    op.add_column(\"message\", sa.Column(\"safety_rots\", sqlmodel.sql.sqltypes.AutoString(), nullable=True))\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_column(\"message\", \"safe_content\")\n    op.drop_column(\"message\", \"safety_level\")\n    op.drop_column(\"message\", \"safety_label\")\n    op.drop_column(\"message\", \"safety_rots\")\n    # ### end Alembic commands ###\n", "inference/server/alembic/versions/2023_03_21_2116-629d5081160f_changed_worker_config_to_worker_info.py": "\"\"\"changed worker config to worker info\n\nRevision ID: 629d5081160f\nRevises: 7d5be54acd49\nCreate Date: 2023-03-21 21:16:57.999842\n\n\"\"\"\nimport sqlalchemy as sa\nfrom alembic import op\nfrom sqlalchemy.dialects import postgresql\n\n# revision identifiers, used by Alembic.\nrevision = \"629d5081160f\"\ndown_revision = \"7d5be54acd49\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.add_column(\"worker_event\", sa.Column(\"worker_info\", postgresql.JSONB(astext_type=sa.Text()), nullable=True))\n    op.drop_column(\"worker_event\", \"worker_config\")\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.add_column(\n        \"worker_event\",\n        sa.Column(\"worker_config\", postgresql.JSONB(astext_type=sa.Text()), autoincrement=False, nullable=True),\n    )\n    op.drop_column(\"worker_event\", \"worker_info\")\n    # ### end Alembic commands ###\n", "inference/server/alembic/versions/2023_04_24_2130-401eef162771_add_chat_data_opt_out_field.py": "\"\"\"Add chat data opt out field\n\nRevision ID: 401eef162771\nRevises: b66fd8f9da1f\nCreate Date: 2023-04-24 21:30:19.947411\n\n\"\"\"\nimport sqlalchemy as sa\nfrom alembic import op\n\n# revision identifiers, used by Alembic.\nrevision = \"401eef162771\"\ndown_revision = \"b66fd8f9da1f\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.add_column(\"chat\", sa.Column(\"allow_data_use\", sa.Boolean(), server_default=sa.text(\"true\"), nullable=False))\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_column(\"chat\", \"allow_data_use\")\n    # ### end Alembic commands ###\n", "inference/server/alembic/versions/2023_03_12_1742-7d5be54acd49_initial_revision.py": "\"\"\"initial revision\n\nRevision ID: 7d5be54acd49\nRevises:\nCreate Date: 2023-03-12 17:42:42.807459\n\n\"\"\"\nimport sqlalchemy as sa\nimport sqlmodel\nfrom alembic import op\nfrom sqlalchemy.dialects import postgresql\n\n# revision identifiers, used by Alembic.\nrevision = \"7d5be54acd49\"\ndown_revision = None\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.create_table(\n        \"user\",\n        sa.Column(\"id\", sqlmodel.sql.sqltypes.AutoString(), nullable=False),\n        sa.Column(\"provider\", sqlmodel.sql.sqltypes.AutoString(), nullable=False),\n        sa.Column(\"provider_account_id\", sqlmodel.sql.sqltypes.AutoString(), nullable=False),\n        sa.Column(\"display_name\", sqlmodel.sql.sqltypes.AutoString(length=256), nullable=False),\n        sa.PrimaryKeyConstraint(\"id\"),\n    )\n    op.create_index(op.f(\"ix_user_provider\"), \"user\", [\"provider\"], unique=False)\n    op.create_index(op.f(\"ix_user_provider_account_id\"), \"user\", [\"provider_account_id\"], unique=False)\n    op.create_index(\"provider\", \"user\", [\"provider_account_id\"], unique=True)\n    op.create_table(\n        \"worker\",\n        sa.Column(\"id\", sqlmodel.sql.sqltypes.AutoString(), nullable=False),\n        sa.Column(\"api_key\", sqlmodel.sql.sqltypes.AutoString(), nullable=False),\n        sa.Column(\"name\", sqlmodel.sql.sqltypes.AutoString(), nullable=False),\n        sa.Column(\"trusted\", sa.Boolean(), nullable=False),\n        sa.Column(\"in_compliance_check_since\", sa.DateTime(), nullable=True),\n        sa.Column(\"next_compliance_check\", sa.DateTime(), nullable=True),\n        sa.PrimaryKeyConstraint(\"id\"),\n    )\n    op.create_index(op.f(\"ix_worker_api_key\"), \"worker\", [\"api_key\"], unique=False)\n    op.create_table(\n        \"chat\",\n        sa.Column(\"id\", sqlmodel.sql.sqltypes.AutoString(), nullable=False),\n        sa.Column(\"user_id\", sqlmodel.sql.sqltypes.AutoString(), nullable=False),\n        sa.Column(\"created_at\", sa.DateTime(), nullable=False),\n        sa.Column(\"modified_at\", sa.DateTime(), nullable=False),\n        sa.Column(\"title\", sqlmodel.sql.sqltypes.AutoString(), nullable=True),\n        sa.ForeignKeyConstraint(\n            [\"user_id\"],\n            [\"user.id\"],\n        ),\n        sa.PrimaryKeyConstraint(\"id\"),\n    )\n    op.create_index(op.f(\"ix_chat_created_at\"), \"chat\", [\"created_at\"], unique=False)\n    op.create_index(op.f(\"ix_chat_modified_at\"), \"chat\", [\"modified_at\"], unique=False)\n    op.create_index(op.f(\"ix_chat_user_id\"), \"chat\", [\"user_id\"], unique=False)\n    op.create_table(\n        \"worker_compliance_check\",\n        sa.Column(\"id\", sqlmodel.sql.sqltypes.AutoString(), nullable=False),\n        sa.Column(\"worker_id\", sqlmodel.sql.sqltypes.AutoString(), nullable=False),\n        sa.Column(\"compare_worker_id\", sqlmodel.sql.sqltypes.AutoString(), nullable=True),\n        sa.Column(\"start_time\", sa.DateTime(), nullable=False),\n        sa.Column(\"end_time\", sa.DateTime(), nullable=True),\n        sa.Column(\"responded\", sa.Boolean(), nullable=False),\n        sa.Column(\"error\", sqlmodel.sql.sqltypes.AutoString(), nullable=True),\n        sa.Column(\"passed\", sa.Boolean(), nullable=False),\n        sa.ForeignKeyConstraint(\n            [\"worker_id\"],\n            [\"worker.id\"],\n        ),\n        sa.PrimaryKeyConstraint(\"id\"),\n    )\n    op.create_index(\n        op.f(\"ix_worker_compliance_check_compare_worker_id\"),\n        \"worker_compliance_check\",\n        [\"compare_worker_id\"],\n        unique=False,\n    )\n    op.create_index(\n        op.f(\"ix_worker_compliance_check_worker_id\"), \"worker_compliance_check\", [\"worker_id\"], unique=False\n    )\n    op.create_table(\n        \"worker_event\",\n        sa.Column(\"worker_config\", postgresql.JSONB(astext_type=sa.Text()), nullable=True),\n        sa.Column(\"id\", sqlmodel.sql.sqltypes.AutoString(), nullable=False),\n        sa.Column(\"worker_id\", sqlmodel.sql.sqltypes.AutoString(), nullable=False),\n        sa.Column(\"time\", sa.DateTime(), nullable=False),\n        sa.Column(\"event_type\", sqlmodel.sql.sqltypes.AutoString(), nullable=False),\n        sa.ForeignKeyConstraint(\n            [\"worker_id\"],\n            [\"worker.id\"],\n        ),\n        sa.PrimaryKeyConstraint(\"id\"),\n    )\n    op.create_index(op.f(\"ix_worker_event_worker_id\"), \"worker_event\", [\"worker_id\"], unique=False)\n    op.create_table(\n        \"message\",\n        sa.Column(\"work_parameters\", postgresql.JSONB(astext_type=sa.Text()), nullable=True),\n        sa.Column(\"worker_config\", postgresql.JSONB(astext_type=sa.Text()), nullable=True),\n        sa.Column(\"role\", sqlmodel.sql.sqltypes.AutoString(), nullable=False),\n        sa.Column(\"id\", sqlmodel.sql.sqltypes.AutoString(), nullable=False),\n        sa.Column(\"created_at\", sa.DateTime(), nullable=False),\n        sa.Column(\"chat_id\", sqlmodel.sql.sqltypes.AutoString(), nullable=False),\n        sa.Column(\"parent_id\", sqlmodel.sql.sqltypes.AutoString(), nullable=True),\n        sa.Column(\"content\", sqlmodel.sql.sqltypes.AutoString(), nullable=True),\n        sa.Column(\"error\", sqlmodel.sql.sqltypes.AutoString(), nullable=True),\n        sa.Column(\"state\", sqlmodel.sql.sqltypes.AutoString(), nullable=False),\n        sa.Column(\"work_begin_at\", sa.DateTime(), nullable=True),\n        sa.Column(\"work_end_at\", sa.DateTime(), nullable=True),\n        sa.Column(\"worker_id\", sqlmodel.sql.sqltypes.AutoString(), nullable=True),\n        sa.Column(\"worker_compat_hash\", sqlmodel.sql.sqltypes.AutoString(), nullable=True),\n        sa.Column(\"score\", sa.Integer(), nullable=False),\n        sa.ForeignKeyConstraint(\n            [\"chat_id\"],\n            [\"chat.id\"],\n        ),\n        sa.ForeignKeyConstraint(\n            [\"worker_id\"],\n            [\"worker.id\"],\n        ),\n        sa.PrimaryKeyConstraint(\"id\"),\n    )\n    op.create_index(op.f(\"ix_message_chat_id\"), \"message\", [\"chat_id\"], unique=False)\n    op.create_index(op.f(\"ix_message_role\"), \"message\", [\"role\"], unique=False)\n    op.create_index(op.f(\"ix_message_worker_compat_hash\"), \"message\", [\"worker_compat_hash\"], unique=False)\n    op.create_table(\n        \"report\",\n        sa.Column(\"id\", sqlmodel.sql.sqltypes.AutoString(), nullable=False),\n        sa.Column(\"message_id\", sqlmodel.sql.sqltypes.AutoString(), nullable=False),\n        sa.Column(\"report_type\", sqlmodel.sql.sqltypes.AutoString(), nullable=False),\n        sa.Column(\"reason\", sqlmodel.sql.sqltypes.AutoString(), nullable=False),\n        sa.ForeignKeyConstraint(\n            [\"message_id\"],\n            [\"message.id\"],\n        ),\n        sa.PrimaryKeyConstraint(\"id\"),\n    )\n    op.create_index(op.f(\"ix_report_message_id\"), \"report\", [\"message_id\"], unique=False)\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_index(op.f(\"ix_report_message_id\"), table_name=\"report\")\n    op.drop_table(\"report\")\n    op.drop_index(op.f(\"ix_message_worker_compat_hash\"), table_name=\"message\")\n    op.drop_index(op.f(\"ix_message_role\"), table_name=\"message\")\n    op.drop_index(op.f(\"ix_message_chat_id\"), table_name=\"message\")\n    op.drop_table(\"message\")\n    op.drop_index(op.f(\"ix_worker_event_worker_id\"), table_name=\"worker_event\")\n    op.drop_table(\"worker_event\")\n    op.drop_index(op.f(\"ix_worker_compliance_check_worker_id\"), table_name=\"worker_compliance_check\")\n    op.drop_index(op.f(\"ix_worker_compliance_check_compare_worker_id\"), table_name=\"worker_compliance_check\")\n    op.drop_table(\"worker_compliance_check\")\n    op.drop_index(op.f(\"ix_chat_user_id\"), table_name=\"chat\")\n    op.drop_index(op.f(\"ix_chat_modified_at\"), table_name=\"chat\")\n    op.drop_index(op.f(\"ix_chat_created_at\"), table_name=\"chat\")\n    op.drop_table(\"chat\")\n    op.drop_index(op.f(\"ix_worker_api_key\"), table_name=\"worker\")\n    op.drop_table(\"worker\")\n    op.drop_index(\"provider\", table_name=\"user\")\n    op.drop_index(op.f(\"ix_user_provider_account_id\"), table_name=\"user\")\n    op.drop_index(op.f(\"ix_user_provider\"), table_name=\"user\")\n    op.drop_table(\"user\")\n    # ### end Alembic commands ###\n", "inference/server/alembic/versions/2023_04_14_1611-b66fd8f9da1f_add_hidden_field_to_chats.py": "\"\"\"Add hidden field to chats\n\nRevision ID: b66fd8f9da1f\nRevises: f0e18084aae4\nCreate Date: 2023-04-14 16:11:35.361507\n\n\"\"\"\nimport sqlalchemy as sa\nfrom alembic import op\n\n# revision identifiers, used by Alembic.\nrevision = \"b66fd8f9da1f\"\ndown_revision = \"f0e18084aae4\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.add_column(\"chat\", sa.Column(\"hidden\", sa.Boolean(), server_default=sa.text(\"false\"), nullable=False))\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_column(\"chat\", \"hidden\")\n    # ### end Alembic commands ###\n", "inference/server/oasst_inference_server/deps.py": "import contextlib\n\nimport fastapi\nimport redis.asyncio as redis\nfrom fastapi import Depends\nfrom fastapi_limiter.depends import RateLimiter\nfrom oasst_inference_server import auth\nfrom oasst_inference_server.chat_repository import ChatRepository\nfrom oasst_inference_server.database import AsyncSession, get_async_session\nfrom oasst_inference_server.settings import settings\nfrom oasst_inference_server.user_chat_repository import UserChatRepository\n\n\n# create async redis client\ndef make_redis_client():\n    redis_client = redis.Redis(\n        host=settings.redis_host, port=settings.redis_port, db=settings.redis_db, decode_responses=True\n    )\n    return redis_client\n\n\nredis_client = make_redis_client()\n\n\nasync def create_session():\n    async for session in get_async_session():\n        yield session\n\n\n@contextlib.asynccontextmanager\nasync def manual_create_session(autoflush=True):\n    async with contextlib.asynccontextmanager(get_async_session)(autoflush=autoflush) as session:\n        yield session\n\n\nasync def create_chat_repository(session: AsyncSession = Depends(create_session)) -> ChatRepository:\n    repository = ChatRepository(session=session)\n    return repository\n\n\nasync def create_user_chat_repository(\n    session: AsyncSession = Depends(create_session),\n    user_id: str = Depends(auth.get_current_user_id),\n) -> UserChatRepository:\n    repository = UserChatRepository(session=session, user_id=user_id)\n    return repository\n\n\n@contextlib.asynccontextmanager\nasync def manual_chat_repository():\n    async with manual_create_session() as session:\n        yield await create_chat_repository(session)\n\n\n@contextlib.asynccontextmanager\nasync def manual_user_chat_repository(user_id: str):\n    async with manual_create_session() as session:\n        yield await create_user_chat_repository(session, user_id)\n\n\nasync def user_identifier(request: fastapi.Request) -> str:\n    \"\"\"Identify a request by user based on auth header\"\"\"\n    trusted_client_token = request.headers.get(\"TrustedClient\")\n    if trusted_client_token is not None:\n        return auth.get_user_id_from_trusted_client_token(trusted_client_token)\n\n    token = request.headers.get(\"Authorization\")\n    return auth.get_user_id_from_auth_token(token)\n\n\nclass UserRateLimiter(RateLimiter):\n    def __init__(\n        self, times: int = 100, milliseconds: int = 0, seconds: int = 0, minutes: int = 1, hours: int = 0\n    ) -> None:\n        super().__init__(times, milliseconds, seconds, minutes, hours, user_identifier)\n", "inference/server/oasst_inference_server/queueing.py": "import redis.asyncio as redis\nfrom oasst_inference_server.settings import settings\n\n\nclass QueueFullException(Exception):\n    pass\n\n\nclass RedisQueue:\n    def __init__(\n        self,\n        redis_client: redis.Redis,\n        queue_id: str,\n        expire: int | None = None,\n        with_counter: bool = False,\n        counter_pos_expire: int = 1,\n        max_size: int | None = None,\n    ) -> None:\n        self.redis_client = redis_client\n        self.queue_id = queue_id\n        self.expire = expire\n        self.with_counter = with_counter\n        self.counter_pos_expire = counter_pos_expire\n        self.max_size = max_size or 0\n\n    async def enqueue(self, value: str, enforce_max_size: bool = True) -> int | None:\n        if enforce_max_size and self.max_size > 0:\n            if await self.get_length() >= self.max_size:\n                raise QueueFullException()\n        await self.redis_client.rpush(self.queue_id, value)\n        if self.expire is not None:\n            await self.set_expire(self.expire)\n        if self.with_counter:\n            ctr = await self.redis_client.incr(f\"ctr_enq:{self.queue_id}\")\n            await self.redis_client.set(f\"pos:{value}\", ctr, ex=self.counter_pos_expire)\n        else:\n            ctr = None\n        return ctr\n\n    async def dequeue(self, timeout: int = 1) -> str | None:\n        val = await self.redis_client.blpop(self.queue_id, timeout=timeout)\n        if val is not None and self.with_counter:\n            await self.redis_client.incr(f\"ctr_deq:{self.queue_id}\")\n        return val\n\n    async def set_expire(self, timeout: int) -> None:\n        return await self.redis_client.expire(self.queue_id, timeout)\n\n    async def get_enq_counter(self) -> int:\n        if not self.with_counter:\n            return 0\n        enq = await self.redis_client.get(f\"ctr_enq:{self.queue_id}\")\n        enq = int(enq) if enq is not None else 0\n        return enq\n\n    async def get_deq_counter(self) -> int:\n        if not self.with_counter:\n            return 0\n        deq = await self.redis_client.get(f\"ctr_deq:{self.queue_id}\")\n        deq = int(deq) if deq is not None else 0\n        return deq\n\n    async def get_length(self) -> int:\n        return await self.redis_client.llen(self.queue_id)\n\n\nasync def get_pos_value(redis_client: redis.Redis, message_id: str) -> int:\n    val = await redis_client.get(f\"pos:{message_id}\")\n    if val is None:\n        return 0\n    return int(val)\n\n\ndef message_queue(redis_client: redis.Redis, message_id: str) -> RedisQueue:\n    return RedisQueue(redis_client, f\"message:{message_id}\", expire=settings.message_queue_expire)\n\n\ndef work_queue(redis_client: redis.Redis, worker_compat_hash: str) -> RedisQueue:\n    if settings.allowed_worker_compat_hashes != \"*\":\n        if worker_compat_hash not in settings.allowed_worker_compat_hashes_list:\n            raise ValueError(f\"Worker compat hash {worker_compat_hash} not allowed\")\n    return RedisQueue(\n        redis_client,\n        f\"work:{worker_compat_hash}\",\n        with_counter=True,\n        counter_pos_expire=settings.message_queue_expire,\n        max_size=settings.work_queue_max_size,\n    )\n\n\ndef compliance_queue(redis_client: redis.Redis, worker_id: str) -> RedisQueue:\n    return RedisQueue(redis_client, f\"compliance:{worker_id}\")\n", "inference/server/oasst_inference_server/auth.py": "\"\"\"Logic related to authorization actions.\"\"\"\n\nimport hashlib\nimport json\nfrom datetime import datetime, timedelta\n\nimport sqlmodel\nfrom cryptography.hazmat.primitives import hashes\nfrom cryptography.hazmat.primitives.kdf.hkdf import HKDF\nfrom fastapi import HTTPException, Security\nfrom fastapi.security import APIKeyHeader\nfrom jose import jwe\nfrom jose.exceptions import JWEError\nfrom loguru import logger\nfrom oasst_inference_server import deps, models\nfrom oasst_inference_server.schemas import auth\nfrom oasst_inference_server.settings import settings\nfrom starlette.status import HTTP_400_BAD_REQUEST, HTTP_401_UNAUTHORIZED, HTTP_403_FORBIDDEN\n\nauthorization_scheme = APIKeyHeader(name=\"Authorization\", auto_error=False, scheme_name=\"Authorization\")\nrefresh_scheme = APIKeyHeader(name=\"Refresh\", auto_error=False, scheme_name=\"Refresh\")\n\ntrusted_client_scheme = APIKeyHeader(name=\"TrustedClient\", auto_error=False, scheme_name=\"TrustedClient\")\n\n\ndef get_user_id_from_trusted_client_token(trusted_client_token: str) -> str:\n    info: auth.TrustedClient = auth.TrustedClientToken(content=trusted_client_token).content\n    if info.api_key not in settings.trusted_api_keys_list:\n        raise HTTPException(status_code=HTTP_401_UNAUTHORIZED, detail=\"Unauthorized client\")\n    return info.user_id\n\n\ndef get_user_id_from_auth_token(token: str) -> str:\n    if token is None or not token.startswith(\"Bearer \"):\n        logger.warning(f\"Invalid token: {token}\")\n        raise HTTPException(status_code=HTTP_403_FORBIDDEN, detail=\"Not authenticated\")\n\n    token = token[len(\"Bearer \") :]\n    if not token:\n        logger.warning(f\"Invalid token: {token}\")\n        raise HTTPException(status_code=HTTP_403_FORBIDDEN, detail=\"Not authenticated\")\n\n    key: bytes = derive_key()\n\n    try:\n        token: bytes = jwe.decrypt(token, key)\n    except JWEError:\n        raise HTTPException(status_code=HTTP_401_UNAUTHORIZED, detail=\"Invalid token\")\n\n    payload: dict = json.loads(token.decode())\n    validate_access_token(payload)\n\n    user_id = payload.get(\"user_id\")\n    return user_id\n\n\ndef get_current_user_id(\n    token: str = Security(authorization_scheme), trusted_client_token: str = Security(trusted_client_scheme)\n) -> str:\n    \"\"\"Get the current user ID.\"\"\"\n    if trusted_client_token is not None:\n        return get_user_id_from_trusted_client_token(trusted_client_token)\n\n    return get_user_id_from_auth_token(token)\n\n\ndef create_access_token(user_id: str) -> str:\n    \"\"\"Create encoded JSON Web Token (JWT) for the given user ID.\"\"\"\n    payload: bytes = build_payload(user_id, \"access\", settings.auth_access_token_expire_minutes)\n\n    key = derive_key()\n    token: bytes = jwe.encrypt(payload, key)\n\n    return token.decode()\n\n\nasync def create_refresh_token(user_id: str) -> str:\n    \"\"\"Create encoded refresh token for the given user ID.\"\"\"\n    payload: bytes = build_payload(user_id, \"refresh\", settings.auth_refresh_token_expire_minutes)\n\n    key = derive_key()\n    token: bytes = jwe.encrypt(payload, key)\n\n    await store_refresh_token(token, user_id)\n\n    return token.decode()\n\n\nasync def refresh_access_token(refresh_token: str) -> str:\n    \"\"\"Refresh the access token using the given refresh token.\"\"\"\n    key: bytes = derive_key()\n\n    try:\n        token: bytes = jwe.decrypt(refresh_token, key)\n    except JWEError:\n        raise HTTPException(status_code=HTTP_401_UNAUTHORIZED, detail=\"Invalid token\")\n\n    token_model = await query_refresh_token(token)\n\n    if not token_model or not token_model.enabled:\n        raise HTTPException(status_code=HTTP_401_UNAUTHORIZED, detail=\"Invalid token\")\n\n    payload: dict = json.loads(token.decode())\n    validate_refresh_token(payload, token_model.user_id)\n\n    user_id = payload.get(\"user_id\")\n    access_token: str = create_access_token(user_id)\n    return access_token\n\n\ndef derive_key() -> bytes:\n    \"\"\"Derive a key from the auth secret.\"\"\"\n    hkdf = HKDF(\n        algorithm=hashes.SHA256(),\n        length=settings.auth_length,\n        salt=settings.auth_salt,\n        info=settings.auth_info,\n    )\n    key = hkdf.derive(settings.auth_secret)\n    return key\n\n\ndef build_payload(user_id: str, token_type: str, expire_minutes: int) -> bytes:\n    \"\"\"Build a token payload as `bytes` encoded from a JSON string.\"\"\"\n    expires_delta = timedelta(minutes=expire_minutes)\n    expire = datetime.utcnow() + expires_delta\n\n    payload_dict = {\n        \"user_id\": user_id,\n        \"exp\": expire.timestamp(),\n        \"type\": token_type,\n    }\n\n    payload: bytes = json.dumps(payload_dict).encode()\n    return payload\n\n\nasync def store_refresh_token(token: bytes, user_id: str) -> None:\n    \"\"\"Store a refresh token in the backend DB.\"\"\"\n    token_hash: bytes = hashlib.sha256(token).hexdigest()\n\n    async with deps.manual_create_session() as session:\n        token_model: models.DbRefreshToken = models.DbRefreshToken(token_hash=token_hash, user_id=user_id)\n        session.add(token_model)\n        await session.commit()\n\n\nasync def query_refresh_token(token: bytes) -> models.DbRefreshToken | None:\n    \"\"\"Query a refresh token in the backend DB.\"\"\"\n    token_hash: bytes = hashlib.sha256(token).hexdigest()\n\n    async with deps.manual_create_session() as session:\n        query = sqlmodel.select(models.DbRefreshToken).where(models.DbRefreshToken.token_hash == token_hash)\n        token_model: models.DbRefreshToken = (await session.exec(query)).one_or_none()\n\n    return token_model\n\n\ndef validate_access_token(payload: dict) -> None:\n    \"\"\"Validate an access token payload.\"\"\"\n    user_id = payload.get(\"user_id\")\n    exp = payload.get(\"exp\")\n    token_type = payload.get(\"type\")\n\n    if not user_id or not exp:\n        raise HTTPException(status_code=HTTP_401_UNAUTHORIZED, detail=\"Invalid token\")\n\n    if not token_type or token_type != \"access\":\n        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail=\"Invalid token type\")\n\n    if datetime.utcnow() >= datetime.fromtimestamp(exp):\n        raise HTTPException(status_code=HTTP_401_UNAUTHORIZED, detail=\"Token expired\")\n\n\ndef validate_refresh_token(payload: dict, compare_user_id: str) -> None:\n    \"\"\"Validate a refresh token payload and confirm it corresponds to the correct user.\"\"\"\n    user_id = payload.get(\"user_id\")\n    exp = payload.get(\"exp\")\n    token_type = payload.get(\"type\")\n\n    if not exp or not user_id or user_id != compare_user_id:\n        raise HTTPException(status_code=HTTP_401_UNAUTHORIZED, detail=\"Invalid token\")\n\n    if not token_type or token_type != \"refresh\":\n        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail=\"Invalid token type\")\n\n    if datetime.utcnow() >= datetime.fromtimestamp(exp):\n        raise HTTPException(status_code=HTTP_401_UNAUTHORIZED, detail=\"Token expired\")\n", "inference/server/oasst_inference_server/chat_utils.py": "from oasst_inference_server.settings import settings\nfrom oasst_shared import model_configs\n\n\ndef get_model_config(model_config_name: str) -> model_configs.ModelConfig:\n    \"\"\"Get a `ModelConfig` by its name. See `oasst_shared.model_configs`.\"\"\"\n    if settings.allowed_model_config_names != \"*\":\n        if model_config_name not in settings.allowed_model_config_names_list:\n            raise ValueError(f\"Model {model_config_name} not in allowed models: {settings.allowed_model_config_names}\")\n\n    model_config = model_configs.MODEL_CONFIGS.get(model_config_name)\n    if model_config is None:\n        raise ValueError(\n            f\"Model {model_config_name} not found\",\n        )\n    return model_config\n", "inference/server/oasst_inference_server/admin.py": "\"\"\"Logic related to admin actions.\"\"\"\n\nimport fastapi\nfrom loguru import logger\nfrom oasst_inference_server import database, models\nfrom oasst_shared import utils as shared_utils\n\n\nasync def delete_user_from_db(session: database.AsyncSession, user_id: str):\n    \"\"\"Deletes a user.\"\"\"\n    logger.info(f\"Deleting user {user_id}\")\n    user = await session.get(models.DbUser, user_id)\n    if not user:\n        raise fastapi.HTTPException(\n            status_code=fastapi.status.HTTP_404_NOT_FOUND,\n            detail=\"User not found\",\n        )\n    user.deleted = True\n\n    # Anonymise user data\n    user.display_name = shared_utils.DELETED_USER_DISPLAY_NAME\n    # Ensure uniqueness\n    user.provider_account_id = f\"{shared_utils.DELETED_USER_ID_PREFIX}{user.id}\"\n\n    await session.commit()\n", "inference/server/oasst_inference_server/settings.py": "from typing import Any\n\nimport pydantic\n\n\ndef split_keys_string(keys: str | None):\n    if not keys:\n        return []\n    return list(filter(bool, keys.split(\",\")))\n\n\nclass Settings(pydantic.BaseSettings):\n    PROJECT_NAME: str = \"open-assistant inference server\"\n    redis_host: str = \"localhost\"\n    redis_port: int = 6379\n    redis_db: int = 0\n    redis_ratelim_db: int = 1\n\n    message_queue_expire: int = 60\n    work_queue_max_size: int | None = None\n\n    chat_max_messages: int | None = None\n    message_max_length: int | None = None\n\n    rate_limit: bool = True\n    rate_limit_messages_user_times: int = 20\n    rate_limit_messages_user_seconds: int = 600\n\n    allowed_worker_compat_hashes: str = \"*\"\n\n    @property\n    def allowed_worker_compat_hashes_list(self) -> list[str]:\n        return self.allowed_worker_compat_hashes.split(\",\")\n\n    allowed_model_config_names: str = \"*\"\n\n    @property\n    def allowed_model_config_names_list(self) -> list[str]:\n        return self.allowed_model_config_names.split(\",\")\n\n    sse_retry_timeout: int = 15000\n    update_alembic: bool = True\n    alembic_retries: int = 5\n    alembic_retry_timeout: int = 1\n\n    postgres_host: str = \"localhost\"\n    postgres_port: str = \"5432\"\n    postgres_user: str = \"postgres\"\n    postgres_password: str = \"postgres\"\n    postgres_db: str = \"postgres\"\n\n    database_uri: str | None = None\n\n    @pydantic.validator(\"database_uri\", pre=True)\n    def assemble_db_connection(cls, v: str | None, values: dict[str, Any]) -> Any:\n        if isinstance(v, str):\n            return v\n        return pydantic.PostgresDsn.build(\n            scheme=\"postgresql+asyncpg\",\n            user=values.get(\"postgres_user\"),\n            password=values.get(\"postgres_password\"),\n            host=values.get(\"postgres_host\"),\n            port=values.get(\"postgres_port\"),\n            path=f\"/{values.get('postgres_db') or ''}\",\n        )\n\n    db_pool_size: int = 75\n    db_max_overflow: int = 20\n    db_echo: bool = False\n\n    root_token: str = \"1234\"\n\n    debug_api_keys: str = \"\"\n\n    @property\n    def debug_api_keys_list(self) -> list[str]:\n        return split_keys_string(self.debug_api_keys)\n\n    trusted_client_keys: str | None\n\n    @property\n    def trusted_api_keys_list(self) -> list[str]:\n        return split_keys_string(self.trusted_client_keys)\n\n    do_compliance_checks: bool = False\n    compliance_check_interval: int = 60\n    compliance_check_timeout: int = 60\n\n    # url of this server\n    api_root: str = \"http://localhost:8000\"\n\n    allow_debug_auth: bool = False\n\n    session_middleware_secret_key: str = \"\"\n\n    auth_info: bytes = b\"NextAuth.js Generated Encryption Key\"\n    auth_salt: bytes = b\"\"\n    auth_length: int = 32\n    auth_secret: bytes = b\"\"\n    auth_algorithm: str = \"HS256\"\n    auth_access_token_expire_minutes: int = 60\n    auth_refresh_token_expire_minutes: int = 60 * 24 * 7\n\n    auth_discord_client_id: str = \"\"\n    auth_discord_client_secret: str = \"\"\n\n    auth_github_client_id: str = \"\"\n    auth_github_client_secret: str = \"\"\n\n    auth_google_client_id: str = \"\"\n    auth_google_client_secret: str = \"\"\n\n    pending_event_interval: int = 1\n    worker_ping_interval: int = 3\n\n    assistant_message_timeout: int = 60\n\n    inference_cors_origins: str = \"*\"\n\n    # sent as a work parameter, higher values increase load on workers\n    plugin_max_depth: int = 4\n\n    # url path prefix for plugins we host on this server\n    plugins_path_prefix: str = \"/plugins\"\n\n    @property\n    def inference_cors_origins_list(self) -> list[str]:\n        return self.inference_cors_origins.split(\",\")\n\n\nsettings = Settings()\n", "inference/server/oasst_inference_server/plugin_utils.py": "import asyncio\nimport json\n\nimport aiohttp\nimport yaml\nfrom aiohttp.client_exceptions import ClientConnectorError, ServerTimeoutError\nfrom fastapi import HTTPException\nfrom loguru import logger\nfrom oasst_shared.schemas import inference\n\n\nasync def attempt_fetch_plugin(session: aiohttp.ClientSession, url: str, timeout: float = 5.0):\n    \"\"\"Attempt to fetch a plugin specification from the given URL once.\"\"\"\n    async with session.get(url, timeout=timeout) as response:\n        content_type = response.headers.get(\"Content-Type\")\n\n        if response.status == 404:\n            raise HTTPException(status_code=404, detail=\"Plugin not found\")\n        if response.status != 200:\n            raise HTTPException(status_code=500, detail=\"Failed to fetch plugin\")\n\n        if \"application/json\" in content_type or \"text/json\" in content_type or url.endswith(\".json\"):\n            if \"text/json\" in content_type:\n                logger.warning(f\"Plugin {url} is using text/json as its content type. This is not recommended.\")\n                config = json.loads(await response.text())\n            else:\n                config = await response.json()\n        elif (\n            \"application/yaml\" in content_type\n            or \"application/x-yaml\" in content_type\n            or url.endswith(\".yaml\")\n            or url.endswith(\".yml\")\n        ):\n            config = yaml.safe_load(await response.text())\n        else:\n            raise HTTPException(\n                status_code=400,\n                detail=f\"Unsupported content type: {content_type}. Only JSON and YAML are supported.\",\n            )\n\n    return inference.PluginConfig(**config)\n\n\nasync def fetch_plugin(url: str, retries: int = 3, timeout: float = 5.0) -> inference.PluginConfig:\n    \"\"\"Fetch a plugin specification from the given URL, with retries using exponential backoff.\"\"\"\n    async with aiohttp.ClientSession() as session:\n        for attempt in range(retries):\n            try:\n                plugin_config = await attempt_fetch_plugin(session, url, timeout=timeout)\n                return plugin_config\n            except (ClientConnectorError, ServerTimeoutError) as e:\n                if attempt == retries - 1:\n                    raise HTTPException(status_code=500, detail=f\"Request failed after {retries} retries: {e}\")\n                await asyncio.sleep(2**attempt)  # exponential backoff\n            except aiohttp.ClientError as e:\n                raise HTTPException(status_code=500, detail=f\"Request failed: {e}\")\n    raise HTTPException(status_code=500, detail=\"Failed to fetch plugin\")\n", "inference/server/oasst_inference_server/chat_repository.py": "import datetime\n\nimport fastapi\nimport pydantic\nimport sqlalchemy.orm\nimport sqlmodel\nfrom loguru import logger\nfrom oasst_inference_server import database, models\nfrom oasst_inference_server.schemas import chat as chat_schema\nfrom oasst_inference_server.settings import settings\nfrom oasst_shared.schemas import inference\n\n\nclass ChatRepository(pydantic.BaseModel):\n    \"\"\"Wrapper around a database session providing functionality relating to chats.\"\"\"\n\n    session: database.AsyncSession\n\n    class Config:\n        arbitrary_types_allowed = True\n\n    async def get_assistant_message_by_id(self, message_id: str) -> models.DbMessage:\n        query = (\n            sqlmodel.select(models.DbMessage)\n            .options(sqlalchemy.orm.selectinload(models.DbMessage.reports))\n            .where(models.DbMessage.id == message_id, models.DbMessage.role == \"assistant\")\n        )\n        message = (await self.session.exec(query)).one()\n        return message\n\n    async def get_prompter_message_by_id(self, message_id: str) -> models.DbMessage:\n        query = (\n            sqlmodel.select(models.DbMessage)\n            .options(sqlalchemy.orm.selectinload(models.DbMessage.reports))\n            .where(models.DbMessage.id == message_id, models.DbMessage.role == \"prompter\")\n        )\n        message = (await self.session.exec(query)).one()\n        return message\n\n    async def start_work(\n        self, *, message_id: str, worker_id: str, worker_config: inference.WorkerConfig\n    ) -> models.DbMessage:\n        \"\"\"\n        Update an assistant message in the database to be allocated to a specific worker.\n        The message must be in `pending` state. An exception is raised if the message has timed out or was cancelled.\n        \"\"\"\n        logger.debug(f\"Starting work on message {message_id}\")\n        message = await self.get_assistant_message_by_id(message_id)\n\n        if settings.assistant_message_timeout > 0:\n            message_age_in_seconds = (datetime.datetime.utcnow() - message.created_at).total_seconds()\n            if message_age_in_seconds > settings.assistant_message_timeout:\n                message.state = inference.MessageState.timeout\n                await self.session.commit()\n                await self.session.refresh(message)\n                raise chat_schema.MessageTimeoutException(message=message.to_read())\n\n        if message.state == inference.MessageState.cancelled:\n            raise chat_schema.MessageCancelledException(message_id=message_id)\n\n        if message.state != inference.MessageState.pending:\n            raise fastapi.HTTPException(status_code=400, detail=\"Message is not pending\")\n\n        message.state = inference.MessageState.in_progress\n        message.work_begin_at = datetime.datetime.utcnow()\n        message.worker_id = worker_id\n        message.worker_config = worker_config\n        await self.session.commit()\n        logger.debug(f\"Started work on message {message_id}\")\n        await self.session.refresh(message)\n        return message\n\n    async def reset_work(self, message_id: str) -> models.DbMessage:\n        \"\"\"\n        Update an assistant message in the database which has already been allocated to a worker to remove the\n        allocation and reset the message state to `pending`.\n        \"\"\"\n        logger.warning(f\"Resetting work on message {message_id}\")\n        message = await self.get_assistant_message_by_id(message_id)\n        message.state = inference.MessageState.pending\n        message.work_begin_at = None\n        message.worker_id = None\n        message.worker_compat_hash = None\n        message.worker_config = None\n        await self.session.commit()\n        logger.debug(f\"Reset work on message {message_id}\")\n        await self.session.refresh(message)\n        return message\n\n    async def abort_work(self, message_id: str, reason: str) -> models.DbMessage:\n        \"\"\"Update an assistant message in the database to mark it as having been aborted by the allocated worker.\"\"\"\n        logger.warning(f\"Aborting work on message {message_id}\")\n        message = await self.get_assistant_message_by_id(message_id)\n        message.state = inference.MessageState.aborted_by_worker\n        message.work_end_at = datetime.datetime.utcnow()\n        message.error = reason\n        await self.session.commit()\n        logger.debug(f\"Aborted work on message {message_id}\")\n        await self.session.refresh(message)\n        return message\n\n    async def complete_work(\n        self, message_id: str, content: str, used_plugin: inference.PluginUsed | None\n    ) -> models.DbMessage:\n        \"\"\"\n        Update an assistant message in the database to mark it as having been completed with the given content, also\n        updating the used plugin if one is specified.\n        \"\"\"\n        logger.debug(f\"Completing work on message {message_id}\")\n        message = await self.get_assistant_message_by_id(message_id)\n        message.state = inference.MessageState.complete\n        message.work_end_at = datetime.datetime.utcnow()\n        message.content = content\n        message.used_plugin = used_plugin\n        await self.session.commit()\n        logger.debug(f\"Completed work on message {message_id}\")\n        await self.session.refresh(message)\n        return message\n", "inference/server/oasst_inference_server/database.py": "import json\nfrom pathlib import Path\n\nimport alembic.command\nimport alembic.config\nimport pydantic.json\nfrom loguru import logger\nfrom oasst_inference_server.schemas import chat as chat_schema\nfrom oasst_inference_server.settings import settings\nfrom oasst_shared.schemas import inference\nfrom sqlalchemy.ext.asyncio import create_async_engine\nfrom sqlalchemy.orm import sessionmaker\nfrom sqlmodel.ext.asyncio.session import AsyncSession\n\n\ndef default_json_serializer(obj):\n    class_name = obj.__class__.__name__\n    encoded = pydantic.json.pydantic_encoder(obj)\n    encoded[\"_classname_\"] = class_name\n    return encoded\n\n\ndef custom_json_serializer(obj):\n    return json.dumps(obj, default=default_json_serializer)\n\n\ndef custom_json_deserializer(s):\n    d = json.loads(s)\n    if not isinstance(d, dict):\n        return d\n    match d.get(\"_classname_\"):\n        case \"WorkParameters\":\n            return inference.WorkParameters.parse_obj(d)\n        case \"ModelConfig\":\n            return inference.ModelConfig.parse_obj(d)\n        case \"SamplingParameters\":\n            return inference.SamplingParameters.parse_obj(d)\n        case \"WorkerConfig\":\n            return inference.WorkerConfig.parse_obj(d)\n        case \"WorkerInfo\":\n            return inference.WorkerInfo.parse_obj(d)\n        case \"CreateMessageRequest\":\n            return chat_schema.CreateMessageRequest.parse_obj(d)\n        case \"WorkRequest\":\n            return inference.WorkRequest.parse_obj(d)\n        case \"PluginUsed\":\n            return inference.PluginUsed.parse_obj(d)\n        case None:\n            return d\n        case _:\n            logger.error(f\"Unknown class {d['_classname_']}\")\n            raise ValueError(f\"Unknown class {d['_classname_']}\")\n\n\ndef make_engine():\n    engine = create_async_engine(\n        settings.database_uri,\n        json_serializer=custom_json_serializer,\n        json_deserializer=custom_json_deserializer,\n        pool_size=settings.db_pool_size,\n        max_overflow=settings.db_max_overflow,\n        echo=settings.db_echo,\n        future=True,\n    )\n    return engine\n\n\ndb_engine = make_engine()\n\n\nasync def get_async_session(autoflush=True):\n    async_session = sessionmaker(bind=db_engine, class_=AsyncSession, expire_on_commit=False, autoflush=autoflush)\n    async with async_session() as session:\n        yield session\n\n\ndef alembic_upgrade(connection):\n    \"\"\"Upgrades database schema based on Alembic migration scripts.\"\"\"\n    alembic_ini_path = Path(__file__).parent.parent / \"alembic.ini\"\n    alembic_cfg = alembic.config.Config(str(alembic_ini_path))\n    alembic_cfg.set_main_option(\"sqlalchemy.url\", settings.database_uri)\n    alembic_cfg.attributes[\"connection\"] = connection\n    alembic.command.upgrade(alembic_cfg, \"head\")\n", "inference/server/oasst_inference_server/worker_utils.py": "import enum\nimport uuid\n\nimport fastapi\nimport pydantic\nimport sqlalchemy.orm\nimport sqlmodel\nfrom fastapi import Depends\nfrom loguru import logger\nfrom oasst_inference_server import database, deps, models\nfrom oasst_shared.schemas import inference\n\n\nclass WorkerSessionStatus(str, enum.Enum):\n    waiting = \"waiting\"\n    working = \"working\"\n    compliance_check = \"compliance_check\"\n\n\nclass WorkerSession(pydantic.BaseModel):\n    id: str = pydantic.Field(default_factory=lambda: str(uuid.uuid4()))\n    worker_id: str\n    worker_info: inference.WorkerInfo\n    requests_in_flight: int = 0\n    metrics: inference.WorkerMetricsInfo | None = None\n\n\napi_key_header = fastapi.Header(None, alias=\"X-API-Key\")\n\n\ndef get_api_key(api_key: str = api_key_header) -> str:\n    if api_key is None:\n        raise fastapi.HTTPException(\n            status_code=fastapi.status.HTTP_401_UNAUTHORIZED,\n            detail=\"Missing API key\",\n        )\n    return api_key\n\n\nprotocol_version_header = fastapi.Header(None, alias=\"X-Protocol-Version\")\n\n\ndef get_protocol_version(protocol_version: str = protocol_version_header) -> str:\n    if protocol_version != inference.INFERENCE_PROTOCOL_VERSION:\n        logger.warning(f\"Got worker with incompatible protocol version: {protocol_version}\")\n        raise fastapi.HTTPException(\n            status_code=fastapi.status.HTTP_426_UPGRADE_REQUIRED,\n            detail=f\"Incompatible protocol version: {protocol_version}. Expected: {inference.INFERENCE_PROTOCOL_VERSION}.\",\n        )\n    return protocol_version\n\n\nasync def get_worker_id(\n    api_key: str = Depends(get_api_key),\n    protocol_version: str = Depends(get_protocol_version),\n) -> models.DbWorker:\n    \"\"\"Get the ID of a worker from its API key and protocol version.\"\"\"\n    logger.info(f\"get_worker: {api_key=}, {protocol_version=}\")\n    query = sqlmodel.select(models.DbWorker).where(models.DbWorker.api_key == api_key)\n    async with deps.manual_create_session() as session:\n        worker: models.DbWorker = (await session.exec(query)).one_or_none()\n    if worker is None:\n        raise fastapi.HTTPException(\n            status_code=fastapi.status.HTTP_401_UNAUTHORIZED,\n            detail=\"Invalid API key\",\n        )\n    return worker.id\n\n\nasync def get_worker(\n    worker_id: str = Depends(get_worker_id),\n    session: database.AsyncSession = Depends(deps.create_session),\n) -> models.DbWorker:\n    query = sqlmodel.select(models.DbWorker).where(models.DbWorker.id == worker_id)\n    worker = (await session.exec(query)).one()\n    return worker\n\n\nasync def send_worker_request(\n    websocket: fastapi.WebSocket,\n    request: inference.WorkerRequest,\n):\n    return await websocket.send_text(request.json())\n\n\nasync def receive_worker_response(\n    websocket: fastapi.WebSocket,\n) -> inference.WorkerResponse:\n    return pydantic.parse_raw_as(inference.WorkerResponse, await websocket.receive_text())\n\n\nasync def receive_worker_info(\n    websocket: fastapi.WebSocket,\n) -> inference.WorkerInfo:\n    return inference.WorkerInfo.parse_raw(await websocket.receive_text())\n\n\nasync def store_worker_session(worker_session: WorkerSession):\n    await deps.redis_client.set(f\"worker_session:{worker_session.id}\", worker_session.json())\n\n\nasync def delete_worker_session(worker_session_id: str):\n    await deps.redis_client.delete(f\"worker_session:{worker_session_id}\")\n    logger.debug(f\"Deleted worker session {worker_session_id}\")\n\n\nasync def build_work_request(\n    session: database.AsyncSession,\n    message_id: str,\n) -> inference.WorkRequest:\n    \"\"\"\n    Build a work request based on the assistant message associated with the given ID in the database.\n    This will build a chat history based on the parents of the assistant message which will form the work request along\n    with the work parameters associated with the assistant message.\n    \"\"\"\n    query = (\n        sqlmodel.select(models.DbMessage)\n        .options(\n            sqlalchemy.orm.selectinload(models.DbMessage.chat)\n            .selectinload(models.DbChat.messages)\n            .selectinload(models.DbMessage.reports),\n        )\n        .where(models.DbMessage.id == message_id)\n    )\n    message: models.DbMessage = (await session.exec(query)).one()\n    chat = message.chat\n    msg_dict = chat.get_msg_dict()\n    thread_msgs = [msg_dict[message.parent_id]]\n    while thread_msgs[-1].parent_id is not None:\n        thread_msgs.append(msg_dict[thread_msgs[-1].parent_id])\n    thread = inference.Thread(\n        messages=[m.to_read() for m in reversed(thread_msgs)],\n    )\n    return inference.WorkRequest(\n        thread=thread,\n        parameters=message.work_parameters,\n    )\n", "inference/server/oasst_inference_server/compliance.py": "\"\"\"Logic related to worker compliance checks, which seek to ensure workers do not produce malicious responses.\"\"\"\n\nimport datetime\nfrom typing import cast\n\nimport fastapi\nimport sqlmodel\nfrom loguru import logger\nfrom oasst_inference_server import database, deps, models, worker_utils\nfrom oasst_inference_server.settings import settings\nfrom oasst_shared.schemas import inference\nfrom sqlalchemy.sql.functions import random as sql_random\nfrom sqlmodel import not_, or_\n\n\nasync def find_compliance_work_request_message(\n    session: database.AsyncSession, worker_config: inference.WorkerConfig, worker_id: str\n) -> models.DbMessage | None:\n    \"\"\"\n    Find a suitable assistant message to carry out a worker compliance check for the given worker. Such a message must\n    have been generated by a different worker, but one with the same compatibility hash as the given worker.\n    \"\"\"\n    compat_hash = worker_config.compat_hash\n    query = (\n        sqlmodel.select(models.DbMessage)\n        .where(\n            models.DbMessage.role == \"assistant\",\n            models.DbMessage.state == inference.MessageState.complete,\n            models.DbMessage.worker_compat_hash == compat_hash,\n            models.DbMessage.worker_id != worker_id,\n        )\n        .order_by(sql_random())\n    )\n    message = (await session.exec(query)).first()\n    return message\n\n\nasync def should_do_compliance_check(session: database.AsyncSession, worker_id: str) -> bool:\n    \"\"\"\n    Check whether we should carry out a compliance check for the given worker, based on time since last check.\n    Trusted workers are excluded.\n    \"\"\"\n    worker = await worker_utils.get_worker(worker_id, session)\n    if worker.trusted:\n        return False\n    if worker.in_compliance_check:\n        return False\n    if worker.next_compliance_check is None:\n        return True\n    if worker.next_compliance_check < datetime.datetime.utcnow():\n        return True\n    return False\n\n\nasync def run_compliance_check(websocket: fastapi.WebSocket, worker_id: str, worker_config: inference.WorkerConfig):\n    \"\"\"\n    Run a compliance check for the given worker:\n    - Find a suitable compliance check assistant message\n    - Task the worker with generating a response with the same context\n    - Compare the respons against the existing completed message\n    - Update the database with the outcome\n    \"\"\"\n    async with deps.manual_create_session() as session:\n        try:\n            worker = await worker_utils.get_worker(worker_id, session)\n            if worker.in_compliance_check:\n                logger.info(f\"Worker {worker.id} is already in compliance check\")\n                return\n            worker.in_compliance_check_since = datetime.datetime.utcnow()\n        finally:\n            await session.commit()\n\n    logger.info(f\"Running compliance check for worker {worker_id}\")\n\n    async with deps.manual_create_session(autoflush=False) as session:\n        compliance_check = models.DbWorkerComplianceCheck(worker_id=worker_id)\n\n        try:\n            message = await find_compliance_work_request_message(session, worker_config, worker_id)\n            if message is None:\n                logger.warning(\n                    f\"Could not find message for compliance check for worker {worker_id} with config {worker_config}\"\n                )\n                return\n\n            compliance_check.compare_worker_id = message.worker_id\n            compliance_work_request = await worker_utils.build_work_request(session, message.id)\n\n            logger.info(f\"Found work request for compliance check for worker {worker_id}: {compliance_work_request}\")\n            await worker_utils.send_worker_request(websocket, compliance_work_request)\n            response = None\n            while True:\n                response = await worker_utils.receive_worker_response(websocket)\n                if response.response_type == \"error\":\n                    compliance_check.responded = True\n                    compliance_check.error = response.error\n                    logger.warning(f\"Worker {worker_id} errored during compliance check: {response.error}\")\n                    return\n                if response.response_type == \"generated_text\":\n                    break\n            if response is None:\n                logger.warning(f\"Worker {worker_id} did not respond to compliance check\")\n                return\n            compliance_check.responded = True\n            response = cast(inference.GeneratedTextResponse, response)\n            passes = response.text == message.content\n            compliance_check.passed = passes\n            logger.info(f\"Worker {worker_id} passed compliance check: {passes}\")\n\n        finally:\n            compliance_check.end_time = datetime.datetime.utcnow()\n            session.add(compliance_check)\n            worker = await worker_utils.get_worker(worker_id, session)\n            worker.next_compliance_check = datetime.datetime.utcnow() + datetime.timedelta(\n                seconds=settings.compliance_check_interval\n            )\n            worker.in_compliance_check_since = None\n            logger.info(f\"set next compliance check for worker {worker_id} to {worker.next_compliance_check}\")\n            await session.commit()\n            await session.flush()\n\n\nasync def maybe_do_compliance_check(websocket, worker_id, worker_config, worker_session_id):\n    async with deps.manual_create_session() as session:\n        should_check = await should_do_compliance_check(session, worker_id)\n    if should_check:\n        logger.info(f\"Worker {worker_id} needs compliance check\")\n        try:\n            await worker_utils.update_worker_session_status(\n                worker_session_id, worker_utils.WorkerSessionStatus.compliance_check\n            )\n            await run_compliance_check(websocket, worker_id, worker_config)\n        finally:\n            await worker_utils.update_worker_session_status(worker_session_id, worker_utils.WorkerSessionStatus.waiting)\n\n\nasync def compute_worker_compliance_score(worker_id: str) -> float:\n    \"\"\"\n    Compute a float between 0 and 1 (inclusive) representing the compliance score of the worker.\n    Workers are rewarded for passing compliance checks, and penalised for failing to respond to a check, erroring during a check, or failing a check.\n    In-progress checks are ignored.\n    \"\"\"\n    async with deps.manual_create_session() as session:\n        query = sqlmodel.select(models.DbWorkerComplianceCheck).where(\n            or_(\n                models.DbWorkerComplianceCheck.worker_id == worker_id,\n                models.DbWorkerComplianceCheck.compare_worker_id == worker_id,\n            ),\n            not_(models.DbWorkerComplianceCheck.end_time.is_(None)),\n        )\n        worker_checks: list[models.DbWorkerComplianceCheck] = (await session.exec(query)).all()\n\n        # Rudimentary scoring algorithm, we may want to add weightings or other factors\n        total_count = len(worker_checks)\n\n        checked = [c for c in worker_checks if c.worker_id == worker_id]\n        compared = [c for c in worker_checks if c.compare_worker_id == worker_id]\n\n        pass_count = sum(1 for _ in filter(lambda c: c.passed, checked))\n        error_count = sum(1 for _ in filter(lambda c: c.error is not None, checked))\n        no_response_count = sum(1 for _ in filter(lambda c: not c.responded, checked))\n\n        compare_fail_count = sum(1 for _ in filter(lambda c: not c.passed, compared))\n        fail_count = len(checked) - pass_count - error_count - no_response_count\n\n        return (fail_count + compare_fail_count) / total_count\n", "inference/server/oasst_inference_server/__init__.py": "", "inference/server/oasst_inference_server/user_chat_repository.py": "import fastapi\nimport pydantic\nimport sqlalchemy.orm\nimport sqlmodel\nfrom loguru import logger\nfrom oasst_inference_server import database, models\nfrom oasst_inference_server.settings import settings\nfrom oasst_shared.schemas import inference\n\n\nclass UserChatRepository(pydantic.BaseModel):\n    \"\"\"Wrapper around a database session providing user-specific functionality relating to chats.\"\"\"\n\n    session: database.AsyncSession\n    user_id: str = pydantic.Field(..., min_length=1)\n\n    class Config:\n        arbitrary_types_allowed = True\n\n    async def get_chats(\n        self,\n        include_hidden: bool = False,\n        limit: int | None = None,\n        before: str | None = None,\n        after: str | None = None,\n    ) -> list[models.DbChat]:\n        if after is not None and before is not None:\n            raise fastapi.HTTPException(status_code=400, detail=\"Cannot specify both after and before.\")\n\n        query = sqlmodel.select(models.DbChat)\n        query = query.where(models.DbChat.user_id == self.user_id)\n\n        if not include_hidden:\n            query = query.where(models.DbChat.hidden.is_(False))\n        if limit is not None:\n            query = query.limit(limit)\n        if before is not None:\n            query = query.where(models.DbChat.id > before)\n        if after is not None:\n            query = query.where(models.DbChat.id < after)\n\n        query = query.order_by(models.DbChat.created_at.desc() if before is None else models.DbChat.created_at)\n\n        return (await self.session.exec(query)).all()\n\n    async def get_chat_by_id(self, chat_id: str, include_messages: bool = True) -> models.DbChat:\n        query = sqlmodel.select(models.DbChat).where(\n            models.DbChat.id == chat_id,\n            models.DbChat.user_id == self.user_id,\n        )\n        if include_messages:\n            query = query.options(\n                sqlalchemy.orm.selectinload(models.DbChat.messages).selectinload(models.DbMessage.reports),\n            )\n\n        chat = (await self.session.exec(query)).one_or_none()\n        if chat is None:\n            raise fastapi.HTTPException(status_code=404, detail=\"Chat not found\")\n        return chat\n\n    async def get_message_by_id(self, chat_id: str, message_id: str) -> models.DbMessage:\n        query = (\n            sqlmodel.select(models.DbMessage)\n            .where(\n                models.DbMessage.id == message_id,\n                models.DbMessage.chat_id == chat_id,\n            )\n            .options(\n                sqlalchemy.orm.selectinload(models.DbMessage.reports),\n            )\n            .join(models.DbChat)\n            .where(\n                models.DbChat.user_id == self.user_id,\n            )\n        )\n        message = (await self.session.exec(query)).one()\n        return message\n\n    async def create_chat(self) -> models.DbChat:\n        # Try to find the user first\n        user: models.DbUser = (\n            await self.session.execute(sqlmodel.select(models.DbUser).where(models.DbUser.id == self.user_id))\n        ).one_or_none()\n        if not user:\n            raise fastapi.HTTPException(status_code=404, detail=\"User not found\")\n        chat = models.DbChat(user_id=self.user_id)\n        self.session.add(chat)\n        await self.session.commit()\n        return chat\n\n    async def delete_chat(self, chat_id: str) -> models.DbChat:\n        chat = await self.get_chat_by_id(chat_id)\n        if chat is None:\n            raise fastapi.HTTPException(status_code=403)\n        logger.debug(f\"Deleting {chat_id=}\")\n        message_ids = [message.id for message in chat.messages]\n        # delete reports associated with messages\n        await self.session.exec(sqlmodel.delete(models.DbReport).where(models.DbReport.message_id.in_(message_ids)))\n        # delete message evaluations associated with message\n        await self.session.exec(\n            sqlmodel.delete(models.DbMessageEval).where(models.DbMessageEval.selected_message_id.in_(message_ids))\n        )\n        # delete messages\n        await self.session.exec(sqlmodel.delete(models.DbMessage).where(models.DbMessage.chat_id == chat_id))\n        # delete chat\n        await self.session.exec(\n            sqlmodel.delete(models.DbChat).where(\n                models.DbChat.id == chat_id,\n                models.DbChat.user_id == self.user_id,\n            )\n        )\n        await self.session.commit()\n\n    async def add_prompter_message(self, chat_id: str, parent_id: str | None, content: str) -> models.DbMessage:\n        logger.info(f\"Adding prompter message {len(content)=} to chat {chat_id}\")\n\n        if settings.message_max_length is not None:\n            if len(content) > settings.message_max_length:\n                raise fastapi.HTTPException(status_code=413, detail=\"Message content exceeds max length\")\n\n        chat: models.DbChat = (\n            await self.session.exec(\n                sqlmodel.select(models.DbChat)\n                .options(sqlalchemy.orm.selectinload(models.DbChat.messages))\n                .where(\n                    models.DbChat.id == chat_id,\n                    models.DbChat.user_id == self.user_id,\n                )\n            )\n        ).one()\n        if settings.chat_max_messages is not None:\n            if len(chat.messages) >= settings.chat_max_messages:\n                raise fastapi.HTTPException(status_code=413, detail=\"Maximum number of messages reached for this chat\")\n        if parent_id is None:\n            if len(chat.messages) > 0:\n                raise fastapi.HTTPException(status_code=400, detail=\"Trying to add first message to non-empty chat\")\n            if chat.title is None:\n                chat.title = content\n        else:\n            msg_dict = chat.get_msg_dict()\n            if parent_id not in msg_dict:\n                raise fastapi.HTTPException(status_code=400, detail=\"Parent message not found\")\n            if msg_dict[parent_id].role != \"assistant\":\n                raise fastapi.HTTPException(status_code=400, detail=\"Parent message is not an assistant message\")\n            if msg_dict[parent_id].state != inference.MessageState.complete:\n                raise fastapi.HTTPException(status_code=400, detail=\"Parent message is not complete\")\n\n        message = models.DbMessage(role=\"prompter\", chat_id=chat_id, chat=chat, parent_id=parent_id, content=content)\n        self.session.add(message)\n        chat.modified_at = message.created_at\n\n        await self.session.commit()\n        logger.debug(f\"Added prompter message {len(content)=} to chat {chat_id}\")\n        query = (\n            sqlmodel.select(models.DbMessage)\n            .options(\n                sqlalchemy.orm.selectinload(models.DbMessage.chat)\n                .selectinload(models.DbChat.messages)\n                .selectinload(models.DbMessage.reports),\n            )\n            .where(\n                models.DbMessage.id == message.id,\n            )\n        )\n        message = (await self.session.exec(query)).one()\n        return message\n\n    async def initiate_assistant_message(\n        self, parent_id: str, work_parameters: inference.WorkParameters, worker_compat_hash: str\n    ) -> models.DbMessage:\n        logger.info(f\"Adding stub assistant message to {parent_id=}\")\n\n        # find and cancel all pending messages by this user\n        pending_msg_query = (\n            sqlmodel.select(models.DbMessage)\n            .where(\n                models.DbMessage.role == \"assistant\",\n                models.DbMessage.state == inference.MessageState.pending,\n                models.DbMessage.parent_id != parent_id,  # Prevent draft messages from cancelling each other\n            )\n            .join(models.DbChat)\n            .where(\n                models.DbChat.user_id == self.user_id,\n            )\n        )\n\n        pending_msgs: list[models.DbMessage] = (await self.session.exec(pending_msg_query)).all()\n        for pending_msg in pending_msgs:\n            logger.warning(\n                f\"User {self.user_id} has a pending message {pending_msg.id} in chat {pending_msg.chat_id}. Cancelling...\"\n            )\n            pending_msg.state = inference.MessageState.cancelled\n            await self.session.commit()\n            logger.debug(f\"Cancelled message {pending_msg.id} in chat {pending_msg.chat_id}.\")\n\n        query = (\n            sqlmodel.select(models.DbMessage)\n            .options(sqlalchemy.orm.selectinload(models.DbMessage.chat))\n            .where(\n                models.DbMessage.id == parent_id,\n                models.DbMessage.role == \"prompter\",\n            )\n        )\n        parent: models.DbMessage = (await self.session.exec(query)).one()\n        if parent.chat.user_id != self.user_id:\n            raise fastapi.HTTPException(status_code=400, detail=\"Message not found\")\n\n        if settings.chat_max_messages is not None:\n            count_query = sqlmodel.select(sqlmodel.func.count(models.DbMessage.id)).where(\n                models.DbMessage.chat_id == parent.chat.id\n            )\n            num_msgs: int = (await self.session.exec(count_query)).one()\n\n            if num_msgs >= settings.chat_max_messages:\n                raise fastapi.HTTPException(status_code=413, detail=\"Maximum number of messages reached for this chat\")\n\n        message = models.DbMessage(\n            role=\"assistant\",\n            chat_id=parent.chat_id,\n            chat=parent.chat,\n            parent_id=parent_id,\n            state=inference.MessageState.pending,\n            work_parameters=work_parameters,\n            worker_compat_hash=worker_compat_hash,\n        )\n        self.session.add(message)\n        await self.session.commit()\n        logger.debug(f\"Initiated assistant message of {parent_id=}\")\n        query = (\n            sqlmodel.select(models.DbMessage)\n            .options(\n                sqlalchemy.orm.selectinload(models.DbMessage.chat)\n                .selectinload(models.DbChat.messages)\n                .selectinload(models.DbMessage.reports),\n            )\n            .where(models.DbMessage.id == message.id)\n        )\n        message = (await self.session.exec(query)).one()\n        return message\n\n    async def update_score(self, message_id: str, score: int) -> models.DbMessage:\n        if score < -1 or score > 1:\n            raise fastapi.HTTPException(status_code=400, detail=\"Invalid score\")\n\n        logger.info(f\"Updating message score to {message_id=}: {score=}\")\n        query = (\n            sqlmodel.select(models.DbMessage)\n            .options(sqlalchemy.orm.selectinload(models.DbMessage.chat))\n            .where(\n                models.DbMessage.id == message_id,\n                models.DbMessage.role == \"assistant\",\n            )\n        )\n        message: models.DbMessage = (await self.session.exec(query)).one()\n        if message.chat.user_id != self.user_id:\n            raise fastapi.HTTPException(status_code=400, detail=\"Message not found\")\n        message.score = score\n        await self.session.commit()\n        return message\n\n    async def add_message_eval(self, message_id: str, inferior_message_ids: list[str]):\n        logger.info(f\"Adding message evaluation to {message_id=}: {inferior_message_ids=}\")\n        query = (\n            sqlmodel.select(models.DbMessage)\n            .options(sqlalchemy.orm.selectinload(models.DbMessage.chat))\n            .where(models.DbMessage.id == message_id)\n        )\n        message: models.DbMessage = (await self.session.exec(query)).one()\n        if message.chat.user_id != self.user_id:\n            raise fastapi.HTTPException(status_code=400, detail=\"Message not found\")\n        message_eval = models.DbMessageEval(\n            chat_id=message.chat_id,\n            user_id=message.chat.user_id,\n            selected_message_id=message.id,\n            inferior_message_ids=inferior_message_ids,\n        )\n        self.session.add(message_eval)\n        await self.session.commit()\n\n    async def add_report(self, message_id: str, reason: str, report_type: inference.ReportType) -> models.DbReport:\n        logger.info(f\"Adding report to {message_id=}: {reason=}\")\n        query = (\n            sqlmodel.select(models.DbMessage)\n            .options(sqlalchemy.orm.selectinload(models.DbMessage.chat))\n            .where(\n                models.DbMessage.id == message_id,\n                models.DbMessage.role == \"assistant\",\n            )\n        )\n        message: models.DbMessage = (await self.session.exec(query)).one()\n        if message.chat.user_id != self.user_id:\n            raise fastapi.HTTPException(status_code=400, detail=\"Message not found\")\n        report = models.DbReport(message_id=message.id, reason=reason, report_type=report_type)\n        self.session.add(report)\n        await self.session.commit()\n        await self.session.refresh(report)\n        return report\n\n    async def update_chat(\n        self,\n        chat_id: str,\n        title: str | None = None,\n        hidden: bool | None = None,\n        allow_data_use: bool | None = None,\n        active_thread_tail_message_id: str | None = None,\n    ) -> None:\n        logger.info(f\"Updating chat {chat_id=}: {title=} {hidden=} {active_thread_tail_message_id=}\")\n        chat = await self.get_chat_by_id(chat_id=chat_id, include_messages=False)\n\n        if title is not None:\n            logger.info(f\"Updating title of chat {chat_id=}: {title=}\")\n            chat.title = title\n\n        if hidden is not None:\n            logger.info(f\"Setting chat {chat_id=} to {'hidden' if hidden else 'visible'}\")\n            chat.hidden = hidden\n\n        if allow_data_use is not None:\n            logger.info(f\"Updating allow_data_use of chat {chat_id=}: {allow_data_use=}\")\n            chat.allow_data_use = allow_data_use\n\n        if active_thread_tail_message_id is not None:\n            logger.info(f\"Updating active_thread_tail_message_id of chat {chat_id=}: {active_thread_tail_message_id=}\")\n            chat.active_thread_tail_message_id = active_thread_tail_message_id\n\n        await self.session.commit()\n\n    async def hide_all_chats(self) -> None:\n        chats = await self.get_chats(include_hidden=False)\n        for chat in chats:\n            chat.hidden = True\n        await self.session.commit()\n", "inference/server/oasst_inference_server/routes/auth.py": "import fastapi\nimport sqlmodel\nfrom authlib.integrations.starlette_client import OAuth\nfrom fastapi import Depends, HTTPException, Request, Security\nfrom google.oauth2.credentials import Credentials\nfrom googleapiclient.discovery import build\nfrom loguru import logger\nfrom oasst_inference_server import auth, database, deps, models\nfrom oasst_inference_server.schemas.auth import TrustedClient, TrustedClientToken\nfrom oasst_inference_server.settings import settings\nfrom oasst_shared.schemas import protocol\n\nrouter = fastapi.APIRouter(\n    prefix=\"/auth\",\n    tags=[\"auth\"],\n)\n\noauth = OAuth()\noauth_providers: list[str] = []\n\n\n@router.on_event(\"startup\")\ndef register_oauth_providers():\n    if settings.auth_discord_client_id:\n        oauth.register(\n            name=\"discord\",\n            client_id=settings.auth_discord_client_id,\n            client_secret=settings.auth_discord_client_secret,\n            access_token_url=\"https://discord.com/api/oauth2/token\",\n            authorize_url=\"https://discord.com/api/oauth2/authorize\",\n            api_base_url=\"https://discord.com/api/\",\n            client_kwargs={\"scope\": \"identify\"},\n        )\n\n        oauth_providers.append(\"discord\")\n\n    if settings.auth_github_client_id:\n        oauth.register(\n            name=\"github\",\n            client_id=settings.auth_github_client_id,\n            client_secret=settings.auth_github_client_secret,\n            access_token_url=\"https://github.com/login/oauth/access_token\",\n            authorize_url=\"https://github.com/login/oauth/authorize\",\n            api_base_url=\"https://api.github.com/\",\n            client_kwargs={\"scope\": \"read:user\"},\n        )\n\n        oauth_providers.append(\"github\")\n\n    if settings.auth_google_client_id:\n        oauth.register(\n            name=\"google\",\n            client_id=settings.auth_google_client_id,\n            client_secret=settings.auth_google_client_secret,\n            access_token_url=\"https://accounts.google.com/o/oauth2/token\",\n            authorize_url=\"https://accounts.google.com/o/oauth2/auth\",\n            api_base_url=\"https://www.googleapis.com/oauth2/v1/\",\n            server_metadata_url=\"https://accounts.google.com/.well-known/openid-configuration\",\n            client_kwargs={\"scope\": \"openid profile\"},\n        )\n\n        oauth_providers.append(\"google\")\n\n    if settings.allow_debug_auth:\n        oauth_providers.append(\"debug\")\n\n\n@router.get(\"/check\")\nasync def check_user_auth(user_id: str = Depends(auth.get_current_user_id)):\n    return user_id\n\n\n@router.get(\"/providers\")\nasync def get_available_auth_providers():\n    if len(oauth_providers) == 0:\n        logger.warn(\"No login providers available, logging in is not possible.\")\n    return oauth_providers\n\n\n@router.get(\"/refresh\", response_model=protocol.Token)\nasync def refresh_token(refresh_token: str = Security(auth.refresh_scheme)):\n    access_token = await auth.refresh_access_token(refresh_token)\n    return protocol.Token(access_token=access_token, token_type=\"bearer\")\n\n\n@router.get(\"/login/discord\")\nasync def login_discord(request: Request):\n    redirect_uri = f\"{settings.api_root}/auth/callback/discord\"\n    return await oauth.discord.authorize_redirect(request, redirect_uri)\n\n\n@router.get(\"/callback/discord\", response_model=protocol.TokenPair)\nasync def callback_discord(\n    request: Request,\n    db: database.AsyncSession = Depends(deps.create_session),\n):\n    token = await oauth.discord.authorize_access_token(request)\n    user_response = await oauth.discord.get(\"users/@me\", token=token)\n\n    user_response_json = user_response.json()\n\n    try:\n        discord_id = user_response_json[\"id\"]\n        discord_username = user_response_json[\"username\"]\n    except KeyError:\n        raise HTTPException(status_code=400, detail=\"Invalid user info response from Discord\")\n\n    user: models.DbUser = await get_or_create_user(db, \"discord\", discord_id, discord_username)\n    token_pair: protocol.TokenPair = await create_tokens(user)\n    return token_pair\n\n\n@router.get(\"/login/github\")\nasync def login_github(request: Request):\n    redirect_uri = f\"{settings.api_root}/auth/callback/github\"\n    return await oauth.github.authorize_redirect(request, redirect_uri)\n\n\n@router.get(\"/callback/github\", response_model=protocol.TokenPair)\nasync def callback_github(\n    request: Request,\n    db: database.AsyncSession = Depends(deps.create_session),\n):\n    token = await oauth.github.authorize_access_token(request)\n    user_response = await oauth.github.get(\"user\", token=token)\n\n    user_response_json = user_response.json()\n\n    try:\n        github_id = str(user_response_json[\"id\"])\n        github_username = user_response_json[\"login\"]\n    except KeyError:\n        raise HTTPException(status_code=400, detail=\"Invalid user info response from GitHub\")\n\n    user: models.DbUser = await get_or_create_user(db, \"github\", github_id, github_username)\n    token_pair: protocol.TokenPair = await create_tokens(user)\n    return token_pair\n\n\n@router.get(\"/login/google\")\nasync def login_google(request: Request):\n    redirect_uri = f\"{settings.api_root}/auth/callback/google\"\n    return await oauth.google.authorize_redirect(request, redirect_uri)\n\n\n@router.get(\"/callback/google\", response_model=protocol.TokenPair)\nasync def callback_google(\n    request: Request,\n    db: database.AsyncSession = Depends(deps.create_session),\n):\n    token = await oauth.google.authorize_access_token(request)\n    credentials = Credentials.from_authorized_user_info(token)\n\n    people_api = build(\"people\", \"v1\", credentials=credentials)\n    profile = people_api.people().get(resourceName=\"people/me\", personFields=\"names\").execute()\n\n    try:\n        google_id = profile[\"resourceName\"].split(\"/\")[1]\n        google_username = profile[\"names\"][0][\"displayName\"] if len(profile[\"names\"]) > 0 else \"User\"\n    except KeyError:\n        raise HTTPException(status_code=400, detail=\"Invalid user info response from Google\")\n\n    user: models.DbUser = await get_or_create_user(db, \"google\", google_id, google_username)\n    token_pair: protocol.TokenPair = await create_tokens(user)\n    return token_pair\n\n\nasync def get_or_create_user(\n    db: database.AsyncSession, provider: str, provider_id: str, display_name: str\n) -> models.DbUser:\n    user = await query_user(db, provider, provider_id)\n\n    if not user:\n        user = models.DbUser(provider=provider, provider_account_id=provider_id, display_name=display_name)\n        db.add(user)\n        await db.commit()\n        await db.refresh(user)\n\n    return user\n\n\nasync def query_user(db: database.AsyncSession, provider: str, provider_id: str) -> models.DbUser | None:\n    user = (\n        await db.exec(\n            sqlmodel.select(models.DbUser)\n            .filter(models.DbUser.provider == provider)\n            .filter(models.DbUser.provider_account_id == provider_id)\n        )\n    ).one_or_none()\n\n    return user\n\n\nasync def create_tokens(user: models.DbUser) -> protocol.TokenPair:\n    access_token = auth.create_access_token(user.id)\n    refresh_token = await auth.create_refresh_token(user.id)\n\n    token_pair = protocol.TokenPair(\n        access_token=protocol.Token(access_token=access_token, token_type=\"bearer\"),\n        refresh_token=protocol.Token(access_token=refresh_token, token_type=\"refresh\"),\n    )\n\n    return token_pair\n\n\n@router.get(\"/login/debug\")\nasync def login_debug(username: str, state: str = r\"{}\"):\n    # mock code with our own data\n    auth_url = f\"{settings.api_root}/auth/callback/debug?code={username}&state={state}\"\n    raise HTTPException(status_code=302, headers={\"location\": auth_url})\n\n\n@router.get(\"/callback/debug\", response_model=protocol.TokenPair)\nasync def callback_debug(code: str, db: database.AsyncSession = Depends(deps.create_session)):\n    \"\"\"Login using a debug username, which the system will accept unconditionally.\"\"\"\n\n    username = code\n    if not settings.allow_debug_auth:\n        raise HTTPException(status_code=403, detail=\"Debug auth is not allowed\")\n\n    if not username:\n        raise HTTPException(status_code=400, detail=\"Username is required\")\n\n    # Try to find the user\n    user: models.DbUser = (\n        await db.exec(sqlmodel.select(models.DbUser).where(models.DbUser.id == username))\n    ).one_or_none()\n\n    if user is None:\n        logger.info(f\"Creating new debug user {username=}\")\n        user = models.DbUser(id=username, display_name=username, provider=\"debug\", provider_account_id=username)\n        db.add(user)\n        await db.commit()\n        await db.refresh(user)\n        logger.info(f\"Created new debug user {user=}\")\n\n    token_pair = await create_tokens(user)\n    return token_pair\n\n\n@router.post(\"/trusted\")\nasync def login_trusted(\n    db: database.AsyncSession = Depends(deps.create_session),\n    trusted_client_token: str = Security(auth.trusted_client_scheme),\n):\n    if trusted_client_token is None:\n        raise HTTPException(status_code=401, detail=\"Missing token\")\n    info: TrustedClient = TrustedClientToken(content=trusted_client_token).content\n    if info.api_key not in settings.trusted_api_keys_list:\n        raise HTTPException(status_code=401, detail=\"Unauthorized client\")\n\n    # Try to find the user\n    user: models.DbUser = (\n        await db.exec(sqlmodel.select(models.DbUser).where(models.DbUser.id == info.user_id))\n    ).one_or_none()\n\n    if user is None:\n        logger.info(f\"Creating new trusted user {info.username=}\")\n        user = models.DbUser(\n            id=info.user_id,\n            display_name=info.username,\n            provider=info.client,\n            provider_account_id=info.provider_account_id,\n        )\n        db.add(user)\n        await db.commit()\n        await db.refresh(user)\n        logger.info(f\"Created new trusted user {user=}\")\n    return user\n", "inference/server/oasst_inference_server/routes/chats.py": "import asyncio\nimport base64\n\nimport fastapi\nimport pydantic\nfrom fastapi import Depends, Query\nfrom loguru import logger\nfrom oasst_inference_server import auth, chat_utils, deps, models, queueing\nfrom oasst_inference_server.schemas import chat as chat_schema\nfrom oasst_inference_server.settings import settings\nfrom oasst_inference_server.user_chat_repository import UserChatRepository\nfrom oasst_shared.schemas import inference\nfrom sse_starlette.sse import EventSourceResponse\n\nrouter = fastapi.APIRouter(\n    prefix=\"/chats\",\n    tags=[\"chats\"],\n)\n\n\n@router.get(\"\")\nasync def list_chats(\n    include_hidden: bool = False,\n    ucr: UserChatRepository = Depends(deps.create_user_chat_repository),\n    limit: int | None = Query(10, gt=0, le=100),\n    after: str | None = None,\n    before: str | None = None,\n) -> chat_schema.ListChatsResponse:\n    \"\"\"Lists all chats.\"\"\"\n    logger.info(\"Listing all chats.\")\n\n    def encode_cursor(chat: models.DbChat):\n        return base64.b64encode(chat.id.encode()).decode()\n\n    def decode_cursor(cursor: str | None):\n        if cursor is None:\n            return None\n        return base64.b64decode(cursor.encode()).decode()\n\n    chats = await ucr.get_chats(\n        include_hidden=include_hidden, limit=limit + 1, after=decode_cursor(after), before=decode_cursor(before)\n    )\n\n    num_rows = len(chats)\n    chats = chats if num_rows <= limit else chats[:-1]  # remove extra item\n    chats = chats if before is None else chats[::-1]  # reverse if query in backward direction\n\n    def get_cursors():\n        prev, next = None, None\n        if num_rows > 0:\n            if (num_rows > limit and before) or after:\n                prev = encode_cursor(chats[0])\n            if num_rows > limit or before:\n                next = encode_cursor(chats[-1])\n        else:\n            if after:\n                prev = after\n            if before:\n                next = before\n        return prev, next\n\n    prev, next = get_cursors()\n\n    chats_list = [chat.to_list_read() for chat in chats]\n    return chat_schema.ListChatsResponse(chats=chats_list, next=next, prev=prev)\n\n\n@router.post(\"\")\nasync def create_chat(\n    request: chat_schema.CreateChatRequest,\n    ucr: UserChatRepository = Depends(deps.create_user_chat_repository),\n) -> chat_schema.ChatListRead:\n    \"\"\"Allows a client to create a new chat.\"\"\"\n    logger.info(f\"Received {request=}\")\n    chat = await ucr.create_chat()\n    return chat.to_list_read()\n\n\n@router.get(\"/{chat_id}\")\nasync def get_chat(\n    chat_id: str,\n    ucr: UserChatRepository = Depends(deps.create_user_chat_repository),\n) -> chat_schema.ChatRead:\n    \"\"\"Allows a client to get the current state of a chat.\"\"\"\n    chat = await ucr.get_chat_by_id(chat_id)\n    return chat.to_read()\n\n\n@router.delete(\"/{chat_id}\")\nasync def delete_chat(\n    chat_id: str,\n    ucr: UserChatRepository = Depends(deps.create_user_chat_repository),\n):\n    await ucr.delete_chat(chat_id)\n    return fastapi.Response(status_code=200)\n\n\n@router.post(\"/{chat_id}/prompter_message\")\nasync def create_prompter_message(\n    chat_id: str,\n    request: chat_schema.CreatePrompterMessageRequest,\n    user_id: str = Depends(auth.get_current_user_id),\n) -> inference.MessageRead:\n    \"\"\"Adds a prompter message to a chat.\"\"\"\n\n    try:\n        ucr: UserChatRepository\n        async with deps.manual_user_chat_repository(user_id) as ucr:\n            prompter_message = await ucr.add_prompter_message(\n                chat_id=chat_id, parent_id=request.parent_id, content=request.content\n            )\n        return prompter_message.to_read()\n    except fastapi.HTTPException:\n        raise\n    except Exception:\n        logger.exception(\"Error adding prompter message\")\n        return fastapi.Response(status_code=500)\n\n\n@router.post(\n    \"/{chat_id}/assistant_message\",\n    dependencies=[\n        Depends(\n            deps.UserRateLimiter(\n                times=settings.rate_limit_messages_user_times,\n                seconds=settings.rate_limit_messages_user_seconds,\n            )\n        ),\n    ],\n)\nasync def create_assistant_message(\n    chat_id: str,\n    request: chat_schema.CreateAssistantMessageRequest,\n    user_id: str = Depends(auth.get_current_user_id),\n) -> inference.MessageRead:\n    \"\"\"Allows the client to stream the results of a request.\"\"\"\n\n    try:\n        model_config = chat_utils.get_model_config(request.model_config_name)\n    except ValueError as e:\n        logger.warning(str(e))\n        raise fastapi.HTTPException(\n            status_code=fastapi.status.HTTP_422_UNPROCESSABLE_ENTITY,\n            detail=str(e),\n        )\n\n    try:\n        ucr: UserChatRepository\n        async with deps.manual_user_chat_repository(user_id) as ucr:\n            work_parameters = inference.WorkParameters(\n                model_config=model_config,\n                sampling_parameters=request.sampling_parameters,\n                system_prompt=request.system_prompt,\n                plugins=request.plugins,\n                plugin_max_depth=settings.plugin_max_depth,\n                user_profile=request.user_profile,\n                user_response_instructions=request.user_response_instructions,\n            )\n            assistant_message = await ucr.initiate_assistant_message(\n                parent_id=request.parent_id,\n                work_parameters=work_parameters,\n                worker_compat_hash=model_config.compat_hash,\n            )\n        queue = queueing.work_queue(deps.redis_client, model_config.compat_hash)\n        logger.debug(f\"Adding {assistant_message.id=} to {queue.queue_id} for {chat_id}\")\n        await queue.enqueue(assistant_message.id)\n        logger.debug(f\"Added {assistant_message.id=} to {queue.queue_id} for {chat_id}\")\n        return assistant_message.to_read()\n    except queueing.QueueFullException:\n        raise fastapi.HTTPException(\n            status_code=fastapi.status.HTTP_503_SERVICE_UNAVAILABLE,\n            detail=\"The server is currently busy. Please try again later.\",\n        )\n    except fastapi.HTTPException:\n        raise\n    except Exception:\n        logger.exception(\"Error adding prompter message\")\n        return fastapi.Response(status_code=500)\n\n\n@router.get(\"/{chat_id}/messages/{message_id}\")\nasync def get_message(\n    chat_id: str,\n    message_id: str,\n    user_id: str = Depends(auth.get_current_user_id),\n) -> inference.MessageRead:\n    ucr: UserChatRepository\n    async with deps.manual_user_chat_repository(user_id) as ucr:\n        message: models.DbMessage = await ucr.get_message_by_id(chat_id=chat_id, message_id=message_id)\n    return message.to_read()\n\n\n@router.get(\"/{chat_id}/messages/{message_id}/events\")\nasync def message_events(\n    chat_id: str,\n    message_id: str,\n    fastapi_request: fastapi.Request,\n    user_id: str = Depends(auth.get_current_user_id),\n) -> EventSourceResponse:\n    ucr: UserChatRepository\n    async with deps.manual_user_chat_repository(user_id) as ucr:\n        message: models.DbMessage = await ucr.get_message_by_id(chat_id=chat_id, message_id=message_id)\n    if message.role != \"assistant\":\n        raise fastapi.HTTPException(status_code=400, detail=\"Only assistant messages can be streamed.\")\n\n    if message.has_finished:\n        raise fastapi.HTTPException(status_code=204, detail=message.state)\n\n    async def event_generator(chat_id: str, message_id: str, worker_compat_hash: str | None):\n        redis_client = deps.make_redis_client()\n        message_queue = queueing.message_queue(redis_client, message_id=message_id)\n        work_queue = (\n            queueing.work_queue(redis_client, worker_compat_hash=worker_compat_hash)\n            if worker_compat_hash is not None\n            else None\n        )\n        has_started = False\n        try:\n            while True:\n                item = await message_queue.dequeue(timeout=settings.pending_event_interval)\n                if item is None:\n                    if not has_started:\n                        if work_queue is None:\n                            qpos, qlen = 0, 1\n                        else:\n                            # TODO: make more efficient, e.g. pipeline\n                            [qdeq, qenq, mpos] = await asyncio.gather(\n                                work_queue.get_deq_counter(),\n                                work_queue.get_enq_counter(),\n                                queueing.get_pos_value(redis_client, message_id),\n                            )\n                            qpos = max(mpos - qdeq, 0)\n                            qlen = max(qenq - qdeq, qpos)\n                        yield {\n                            \"data\": chat_schema.PendingResponseEvent(\n                                queue_position=qpos,\n                                queue_size=qlen,\n                            ).json()\n                        }\n                    continue\n                has_started = True\n\n                _, response_packet_str = item\n                response_packet = pydantic.parse_raw_as(inference.WorkerResponse, response_packet_str)\n\n                if response_packet.response_type in (\"error\", \"generated_text\"):\n                    logger.warning(\n                        f\"Received {response_packet.response_type=} response for {chat_id}. This should not happen.\"\n                    )\n                    break\n\n                if response_packet.response_type == \"safe_prompt\":\n                    logger.info(f\"Received safety intervention for {chat_id}\")\n                    yield {\n                        \"data\": chat_schema.SafePromptResponseEvent(\n                            safe_prompt=response_packet.safe_prompt,\n                        ).json(),\n                    }\n\n                if response_packet.response_type == \"plugin_intermediate\":\n                    logger.info(f\"Received plugin intermediate response {chat_id}\")\n                    yield {\n                        \"data\": chat_schema.PluginIntermediateResponseEvent(\n                            current_plugin_thought=response_packet.current_plugin_thought,\n                            current_plugin_action_taken=response_packet.current_plugin_action_taken,\n                            current_plugin_action_input=response_packet.current_plugin_action_input,\n                            current_plugin_action_response=response_packet.current_plugin_action_response,\n                        ).json(),\n                    }\n\n                if response_packet.response_type == \"internal_error\":\n                    yield {\n                        \"data\": chat_schema.ErrorResponseEvent(\n                            error=response_packet.error, message=response_packet.message\n                        ).json(),\n                    }\n                    break\n\n                if response_packet.response_type == \"internal_finished_message\":\n                    yield {\n                        \"data\": chat_schema.MessageResponseEvent(message=response_packet.message).json(),\n                    }\n                    break\n\n                yield {\n                    \"data\": chat_schema.TokenResponseEvent(text=response_packet.text).json(),\n                }\n\n            if await fastapi_request.is_disconnected():\n                logger.warning(f\"Client disconnected while streaming {chat_id}\")\n\n            logger.info(f\"Finished streaming {chat_id}\")\n        except Exception:\n            logger.exception(f\"Error streaming {chat_id}\")\n            raise\n        finally:\n            await redis_client.close()\n\n    return EventSourceResponse(\n        event_generator(chat_id=chat_id, message_id=message_id, worker_compat_hash=message.worker_compat_hash)\n    )\n\n\n@router.post(\"/{chat_id}/messages/{message_id}/votes\")\nasync def handle_create_vote(\n    message_id: str,\n    vote_request: chat_schema.VoteRequest,\n    ucr: deps.UserChatRepository = fastapi.Depends(deps.create_user_chat_repository),\n) -> fastapi.Response:\n    \"\"\"Allows the client to vote on a message.\"\"\"\n    try:\n        await ucr.update_score(message_id=message_id, score=vote_request.score)\n        return fastapi.Response(status_code=200)\n    except Exception:\n        logger.exception(\"Error adding vote\")\n        return fastapi.Response(status_code=500)\n\n\n@router.post(\"/{chat_id}/messages/{message_id}/message_evals\")\nasync def handle_create_message_eval(\n    message_id: str,\n    inferior_message_request: chat_schema.MessageEvalRequest,\n    ucr: deps.UserChatRepository = fastapi.Depends(deps.create_user_chat_repository),\n) -> fastapi.Response:\n    try:\n        await ucr.add_message_eval(\n            message_id=message_id, inferior_message_ids=inferior_message_request.inferior_message_ids\n        )\n        return fastapi.Response(status_code=200)\n    except Exception:\n        logger.exception(\"Error setting messages as inferior\")\n        return fastapi.Response(status_code=500)\n\n\n@router.post(\"/{chat_id}/messages/{message_id}/reports\")\nasync def handle_create_report(\n    message_id: str,\n    report_request: chat_schema.ReportRequest,\n    ucr: deps.UserChatRepository = fastapi.Depends(deps.create_user_chat_repository),\n) -> fastapi.Response:\n    \"\"\"Allows the client to report a message.\"\"\"\n    try:\n        await ucr.add_report(\n            message_id=message_id, report_type=report_request.report_type, reason=report_request.reason\n        )\n        return fastapi.Response(status_code=200)\n    except Exception:\n        logger.exception(\"Error adding report\")\n        return fastapi.Response(status_code=500)\n\n\n@router.put(\"/{chat_id}\")\nasync def handle_update_chat(\n    chat_id: str,\n    request: chat_schema.ChatUpdateRequest,\n    ucr: deps.UserChatRepository = fastapi.Depends(deps.create_user_chat_repository),\n) -> fastapi.Response:\n    \"\"\"Allows the client to update a chat.\"\"\"\n    try:\n        await ucr.update_chat(\n            chat_id=chat_id,\n            title=request.title,\n            hidden=request.hidden,\n            allow_data_use=request.allow_data_use,\n            active_thread_tail_message_id=request.active_thread_tail_message_id,\n        )\n    except Exception:\n        logger.exception(\"Error when updating chat\")\n        return fastapi.Response(status_code=500)\n\n\n@router.put(\"/hide_all\")\nasync def handle_hide_all_chats(\n    ucr: deps.UserChatRepository = fastapi.Depends(deps.create_user_chat_repository),\n) -> fastapi.Response:\n    \"\"\"Allows the client to hide all the user's chats.\"\"\"\n    try:\n        await ucr.hide_all_chats()\n    except Exception:\n        logger.exception(\"Error when hiding chats\")\n        return fastapi.Response(status_code=500)\n", "inference/server/oasst_inference_server/routes/admin.py": "import fastapi\nimport sqlmodel\nfrom fastapi import Depends, HTTPException, Security\nfrom loguru import logger\nfrom oasst_inference_server import admin, auth, database, deps, models\nfrom oasst_inference_server.schemas import worker as worker_schema\nfrom oasst_inference_server.settings import settings\n\nrouter = fastapi.APIRouter(\n    prefix=\"/admin\",\n    tags=[\"admin\"],\n)\n\n\ndef get_bearer_token(\n    authorization_header: str = Security(auth.authorization_scheme),\n) -> str:\n    if authorization_header is None or not authorization_header.startswith(\"Bearer \"):\n        raise fastapi.HTTPException(\n            status_code=fastapi.status.HTTP_401_UNAUTHORIZED,\n            detail=\"Invalid token\",\n        )\n    return authorization_header[len(\"Bearer \") :]\n\n\ndef get_root_token(token: str = Depends(get_bearer_token)) -> str:\n    root_token = settings.root_token\n    if token == root_token:\n        return token\n    raise HTTPException(\n        status_code=fastapi.status.HTTP_401_UNAUTHORIZED,\n        detail=\"Invalid token\",\n    )\n\n\n@router.put(\"/workers\")\nasync def create_worker(\n    request: worker_schema.CreateWorkerRequest,\n    root_token: str = Depends(get_root_token),\n    session: database.AsyncSession = Depends(deps.create_session),\n) -> worker_schema.WorkerRead:\n    \"\"\"Allows a client to register a worker.\"\"\"\n    logger.info(f\"Creating worker {request.name}\")\n    worker = models.DbWorker(name=request.name, trusted=request.trusted)\n    session.add(worker)\n    await session.commit()\n    await session.refresh(worker)\n    return worker_schema.WorkerRead.from_orm(worker)\n\n\n@router.get(\"/workers\")\nasync def list_workers(\n    root_token: str = Depends(get_root_token),\n    session: database.AsyncSession = Depends(deps.create_session),\n) -> list[worker_schema.WorkerRead]:\n    \"\"\"Lists all workers.\"\"\"\n    workers = (await session.exec(sqlmodel.select(models.DbWorker))).all()\n    return [worker_schema.WorkerRead.from_orm(worker) for worker in workers]\n\n\n@router.delete(\"/workers/{worker_id}\")\nasync def delete_worker(\n    worker_id: str,\n    root_token: str = Depends(get_root_token),\n    session: database.AsyncSession = Depends(deps.create_session),\n):\n    \"\"\"Deletes a worker.\"\"\"\n    logger.info(f\"Deleting worker {worker_id}\")\n    worker = await session.get(models.DbWorker, worker_id)\n    session.delete(worker)\n    await session.commit()\n    return fastapi.Response(status_code=200)\n\n\n@router.delete(\"/refresh_tokens/{user_id}\")\nasync def revoke_refresh_tokens(\n    user_id: str,\n    root_token: str = Depends(get_root_token),\n    session: database.AsyncSession = Depends(deps.create_session),\n):\n    \"\"\"Revoke refresh tokens for a user.\"\"\"\n    logger.info(f\"Revoking refresh tokens for user {user_id}\")\n    refresh_tokens = (\n        await session.exec(sqlmodel.select(models.DbRefreshToken).where(models.DbRefreshToken.user_id == user_id))\n    ).all()\n    for refresh_token in refresh_tokens:\n        refresh_token.enabled = False\n    await session.commit()\n    return fastapi.Response(status_code=200)\n\n\n@router.delete(\"/users/{user_id}\")\nasync def delete_user(\n    user_id: str,\n    root_token: str = Depends(get_root_token),\n    session: database.AsyncSession = Depends(deps.create_session),\n):\n    await admin.delete_user_from_db(session, user_id)\n    return fastapi.Response(status_code=200)\n", "inference/server/oasst_inference_server/routes/workers.py": "import asyncio\nimport datetime\nfrom typing import cast\n\nimport fastapi\nimport pydantic\nimport websockets.exceptions\nfrom loguru import logger\nfrom oasst_inference_server import chat_repository, database, deps, models, queueing, worker_utils\nfrom oasst_inference_server.schemas import chat as chat_schema\nfrom oasst_inference_server.settings import settings\nfrom oasst_shared.schemas import inference\n\n\nclass WorkerDisconnectException(Exception):\n    def __init__(self):\n        super().__init__(\"Worker disconnected\")\n\n\nWSException = (\n    websockets.exceptions.WebSocketException,\n    websockets.exceptions.ConnectionClosedError,\n    fastapi.WebSocketException,\n    fastapi.WebSocketDisconnect,\n    WorkerDisconnectException,\n)\n\nrouter = fastapi.APIRouter(\n    prefix=\"/workers\",\n    tags=[\"workers\"],\n)\n\n\nclass WorkerError(Exception):\n    def __init__(\n        self,\n        message: str,\n        did_work: bool,\n        original_exception: Exception | None = None,\n    ):\n        super().__init__(message)\n        self.did_work = did_work\n        self.original_exception = original_exception\n\n\nasync def add_worker_connect_event(\n    session: database.AsyncSession,\n    worker_id: str,\n    worker_info: inference.WorkerInfo,\n):\n    event = models.DbWorkerEvent(\n        worker_id=worker_id,\n        event_type=models.WorkerEventType.connect,\n        worker_info=worker_info,\n    )\n    session.add(event)\n    await session.commit()\n\n\nclass WorkRequestContainer(pydantic.BaseModel):\n    work_request: inference.WorkRequest\n    message_id: str\n    start_time: datetime.datetime = pydantic.Field(default_factory=datetime.datetime.utcnow)\n    num_responses: int = 0\n\n    class Config:\n        arbitrary_types_allowed = True\n\n\nWorkRequestContainerMap = dict[str, WorkRequestContainer]\n\n\nclass WorkRequestNotFound(Exception):\n    def __init__(self, request_id: str):\n        super().__init__(f\"Work request not found: {request_id=}\")\n        self.request_id = request_id\n\n\ndef get_work_request_container(work_request_map: WorkRequestContainerMap, request_id: str) -> WorkRequestContainer:\n    if request_id is None:\n        raise WorkRequestNotFound(request_id)\n    container = work_request_map.get(request_id)\n    if container is None:\n        raise WorkRequestNotFound(request_id)\n    return container\n\n\n@router.websocket(\"/work\")\nasync def handle_worker(\n    websocket: fastapi.WebSocket,\n    api_key: str = worker_utils.api_key_header,\n    protocol_version: str = worker_utils.protocol_version_header,\n):\n    await websocket.accept()\n\n    try:\n        worker_utils.get_protocol_version(protocol_version)\n        api_key = worker_utils.get_api_key(api_key)\n        worker_id = await worker_utils.get_worker_id(api_key=api_key, protocol_version=protocol_version)\n    except fastapi.HTTPException as e:\n        logger.warning(f\"handle_worker: {e.status_code=} {e.detail=}\")\n        if e.status_code == fastapi.status.HTTP_426_UPGRADE_REQUIRED:\n            await worker_utils.send_worker_request(websocket=websocket, request=inference.UpgradeProtocolRequest())\n        elif e.status_code == fastapi.status.HTTP_401_UNAUTHORIZED:\n            await worker_utils.send_worker_request(websocket=websocket, request=inference.WrongApiKeyRequest())\n        try:\n            await websocket.close(code=e.status_code, reason=e.detail)\n        except Exception:\n            pass\n        raise fastapi.WebSocketException(e.status_code, e.detail)\n\n    logger.info(f\"handle_worker: {worker_id=}\")\n    worker_info = await worker_utils.receive_worker_info(websocket)\n    logger.info(f\"handle_worker: {worker_info=}\")\n    worker_config = worker_info.config\n    worker_compat_hash = worker_config.compat_hash\n    work_queue = queueing.work_queue(deps.redis_client, worker_compat_hash)\n    redis_client = deps.make_redis_client()\n    blocking_work_queue = queueing.work_queue(redis_client, worker_compat_hash)\n    worker_session = worker_utils.WorkerSession(\n        worker_id=worker_id,\n        worker_info=worker_info,\n    )\n    work_request_map: dict[str, WorkRequestContainer] = {}\n    pending_futures = set()\n    try:\n        async with deps.manual_create_session() as session:\n            await add_worker_connect_event(session=session, worker_id=worker_id, worker_info=worker_info)\n        await worker_utils.store_worker_session(worker_session)\n\n        async def _update_session(metrics: inference.WorkerMetricsInfo):\n            worker_session.requests_in_flight = len(work_request_map)\n            if metrics:\n                worker_session.metrics = metrics\n            await worker_utils.store_worker_session(worker_session)\n\n        def _add_dequeue(ftrs: set):\n            requests_in_progress = len(work_request_map)\n            if requests_in_progress < worker_config.max_parallel_requests:\n                ftrs.add(asyncio.ensure_future(blocking_work_queue.dequeue(timeout=0)))\n\n        def _add_receive(ftrs: set):\n            ftrs.add(asyncio.ensure_future(worker_utils.receive_worker_response(websocket=websocket)))\n\n        _add_dequeue(pending_futures)\n        _add_receive(pending_futures)\n\n        logger.info(f\"handle_worker: {worker_id=} started\")\n        while True:\n            if websocket.client_state == fastapi.websockets.WebSocketState.DISCONNECTED:\n                raise WorkerDisconnectException(\"Worker disconnected\")\n            (done, pending_futures) = await asyncio.wait(\n                pending_futures, timeout=settings.worker_ping_interval, return_when=asyncio.FIRST_COMPLETED\n            )\n            ftr: asyncio.Future\n            for ftr in done:\n                result = ftr.result()\n                if result is None:\n                    logger.error(f\"handle_worker: {worker_id=} received None from queue. This should never happen.\")\n                    raise RuntimeError(\"Received None from queue. This should never happen.\")\n                elif isinstance(result, tuple):\n                    try:\n                        _, message_id = result\n                        work_request = await initiate_work_for_message(\n                            websocket=websocket,\n                            work_queue=work_queue,\n                            message_id=message_id,\n                            worker_id=worker_id,\n                            worker_config=worker_config,\n                        )\n                        work_request_map[work_request.id] = WorkRequestContainer(\n                            work_request=work_request, message_id=message_id\n                        )\n                    except chat_schema.MessageCancelledException as e:\n                        logger.warning(f\"Message was cancelled before work could be initiated: {e.message_id=}\")\n                    except chat_schema.MessageTimeoutException as e:\n                        logger.warning(f\"Message timed out before work could be initiated: {e.message.id=}\")\n                        await handle_timeout(message=e.message)\n                    finally:\n                        _add_dequeue(pending_futures)\n                else:\n                    try:\n                        worker_response: inference.WorkerResponse = result\n                        match worker_response.response_type:\n                            case \"pong\":\n                                worker_response = cast(inference.PongResponse, worker_response)\n                                await _update_session(worker_response.metrics)\n                            case \"token\":\n                                worker_response = cast(inference.TokenResponse, worker_response)\n                                await handle_token_response(\n                                    work_request_map=work_request_map,\n                                    response=worker_response,\n                                )\n                            case \"generated_text\":\n                                worker_response = cast(inference.GeneratedTextResponse, worker_response)\n                                await handle_generated_text_response(\n                                    work_request_map=work_request_map,\n                                    response=worker_response,\n                                )\n                                await _update_session(worker_response.metrics)\n                            case \"error\":\n                                worker_response = cast(inference.ErrorResponse, worker_response)\n                                await handle_error_response(\n                                    work_request_map=work_request_map,\n                                    response=worker_response,\n                                )\n                                await _update_session(worker_response.metrics)\n                            case \"general_error\":\n                                worker_response = cast(inference.GeneralErrorResponse, worker_response)\n                                await handle_general_error_response(\n                                    response=worker_response,\n                                )\n                                await _update_session(worker_response.metrics)\n                            case \"safe_prompt\":\n                                logger.info(\"Received safe prompt response\")\n                                worker_response = cast(inference.SafePromptResponse, worker_response)\n                                await handle_safe_prompt_response(\n                                    response=worker_response,\n                                    work_request_map=work_request_map,\n                                )\n                            case \"plugin_intermediate\":\n                                worker_response = cast(inference.PluginIntermediateResponse, worker_response)\n                                await handle_plugin_intermediate_response(\n                                    work_request_map=work_request_map,\n                                    response=worker_response,\n                                )\n                            case _:\n                                raise RuntimeError(f\"Unknown response type: {worker_response.response_type}\")\n                    finally:\n                        if len(pending_futures) == 0:\n                            _add_dequeue(pending_futures)\n                        _add_receive(pending_futures)\n            if not done:\n                await worker_utils.send_worker_request(websocket, inference.PingRequest())\n\n    except Exception as e:\n        logger.exception(f\"Error while handling worker {worker_id}: {str(e)}\")\n        logger.info(f\"Handling {len(work_request_map)} work requests outstanding\")\n        for container in work_request_map.values():\n            try:\n                message_id = container.message_id\n                if container.num_responses == 0:\n                    logger.warning(f\"Marking {message_id=} as pending since no work was done.\")\n                    async with deps.manual_chat_repository() as cr:\n                        await cr.reset_work(message_id)\n                    await work_queue.enqueue(message_id, enforce_max_size=False)\n                else:\n                    logger.warning(f\"Aborting {message_id=}\")\n                    await abort_message(message_id=message_id, error=\"Aborted due to worker error.\")\n            except Exception as e:\n                logger.exception(f\"Error while trying to reset work for {message_id=}: {str(e)}\")\n    finally:\n        logger.info(f\"Worker {worker_id} disconnected\")\n        try:\n            await redis_client.close()\n        except Exception:\n            logger.warning(\"Error while closing redis client\")\n        try:\n            await worker_utils.delete_worker_session(worker_session.id)\n        except Exception:\n            logger.warning(\"Error while deleting worker session\")\n        # try closing websocket if it's still open\n        logger.info(f\"Cancelling {len(pending_futures)} pending futures\")\n        for ftr in pending_futures:\n            try:\n                ftr.cancel()\n            except Exception:\n                logger.warning(\"Error while cancelling pending future\")\n        try:\n            await websocket.close()\n        except Exception:\n            logger.warning(\"Error while closing websocket\")\n\n\n@router.get(\"/sessions\")\nasync def list_worker_sessions() -> list[worker_utils.WorkerSession]:\n    redis_client = deps.redis_client\n    try:\n        worker_sessions = []\n        async for key in redis_client.scan_iter(\"worker_session:*\"):\n            worker_session_json = await redis_client.get(key)\n            worker_session = worker_utils.WorkerSession.parse_raw(worker_session_json)\n            worker_sessions.append(worker_session)\n    except Exception as e:\n        logger.exception(f\"Error while listing worker sessions: {str(e)}\")\n        raise\n    return worker_sessions\n\n\n@router.on_event(\"startup\")\nasync def clear_worker_sessions():\n    redis_client = deps.redis_client\n    try:\n        logger.warning(\"Clearing worker sessions\")\n        async for key in redis_client.scan_iter(\"worker_session:*\"):\n            await redis_client.getdel(key)\n        logger.warning(\"Successfully cleared worker sessions\")\n    except Exception as e:\n        logger.exception(f\"Error while clearing worker sessions: {str(e)}\")\n        raise\n\n\nasync def initiate_work_for_message(\n    *,\n    websocket: fastapi.WebSocket,\n    work_queue: queueing.RedisQueue,\n    message_id: str,\n    worker_id: str,\n    worker_config: inference.WorkerConfig,\n) -> inference.WorkRequest:\n    async with deps.manual_create_session() as session:\n        cr = chat_repository.ChatRepository(session=session)\n\n        message = await cr.start_work(\n            message_id=message_id,\n            worker_id=worker_id,\n            worker_config=worker_config,\n        )\n        work_request = await worker_utils.build_work_request(session, message.id)\n\n    logger.info(f\"Created {work_request=} with {len(work_request.thread.messages)=}\")\n    try:\n        await worker_utils.send_worker_request(websocket, work_request)\n    except Exception as e:\n        logger.exception(f\"Error while sending work request to worker: {str(e)}\")\n        async with deps.manual_create_session() as session:\n            await cr.reset_work(message_id)\n        await work_queue.enqueue(message_id, enforce_max_size=False)\n        raise\n\n    return work_request\n\n\nasync def handle_token_response(\n    response: inference.TokenResponse,\n    work_request_map: WorkRequestContainerMap,\n):\n    work_response_container = get_work_request_container(work_request_map, response.request_id)\n    message_queue = queueing.message_queue(\n        deps.redis_client,\n        message_id=work_response_container.message_id,\n    )\n    await message_queue.enqueue(response.json())\n    work_response_container.num_responses += 1\n\n\nasync def handle_plugin_intermediate_response(\n    response: inference.PluginIntermediateResponse,\n    work_request_map: WorkRequestContainerMap,\n):\n    work_response_container = get_work_request_container(work_request_map, response.request_id)\n    message_queue = queueing.message_queue(\n        deps.redis_client,\n        message_id=work_response_container.message_id,\n    )\n    await message_queue.enqueue(response.json())\n    work_response_container.num_responses += 1\n\n\nasync def handle_generated_text_response(\n    response: inference.GeneratedTextResponse,\n    work_request_map: WorkRequestContainerMap,\n):\n    try:\n        work_response_container = get_work_request_container(work_request_map, response.request_id)\n        message_id = work_response_container.message_id\n        async with deps.manual_create_session() as session:\n            cr = chat_repository.ChatRepository(session=session)\n            message = await cr.complete_work(\n                message_id=message_id,\n                content=response.text,\n                used_plugin=response.used_plugin,\n            )\n            logger.info(f\"Completed work for {message_id=}\")\n        message_packet = inference.InternalFinishedMessageResponse(\n            message=message.to_read(),\n        )\n        message_queue = queueing.message_queue(\n            deps.redis_client,\n            message_id=message_id,\n        )\n        await message_queue.enqueue(message_packet.json())\n    finally:\n        del work_request_map[response.request_id]\n\n\nasync def abort_message(message_id: str, error: str):\n    async with deps.manual_chat_repository() as cr:\n        message = await cr.abort_work(message_id, reason=error)\n    response = inference.InternalErrorResponse(error=error, message=message.to_read())\n    message_queue = queueing.message_queue(\n        deps.redis_client,\n        message_id=message_id,\n    )\n    await message_queue.enqueue(response.json())\n\n\nasync def handle_error_response(\n    response: inference.ErrorResponse,\n    work_request_map: WorkRequestContainerMap,\n):\n    logger.warning(f\"Got error {response=}\")\n    try:\n        work_response_container = get_work_request_container(work_request_map, response.request_id)\n        message_id = work_response_container.message_id\n        await abort_message(message_id, response.error)\n    finally:\n        del work_request_map[response.request_id]\n\n\nasync def handle_general_error_response(\n    response: inference.GeneralErrorResponse,\n):\n    logger.warning(f\"Got general error {response=}\")\n\n\nasync def handle_safe_prompt_response(\n    response: inference.SafePromptResponse,\n    work_request_map: WorkRequestContainerMap,\n):\n    \"\"\"\n    Handle the case where the worker informs the server that the safety model has intervened and modified the user prompt to be safe.\n    \"\"\"\n    work_response_container = get_work_request_container(work_request_map, response.request_id)\n    message_id = work_response_container.message_id\n\n    async with deps.manual_create_session() as session:\n        cr = chat_repository.ChatRepository(session=session)\n        message = await cr.get_assistant_message_by_id(message_id)\n        prompt = await cr.get_prompter_message_by_id(message.parent_id)\n        prompt.safe_content = response.safe_prompt\n        prompt.safety_level = response.safety_parameters.level\n        prompt.safety_label = response.safety_label\n        prompt.safety_rots = response.safety_rots\n        await session.commit()\n\n\nasync def handle_timeout(message: inference.MessageRead):\n    response = inference.InternalErrorResponse(\n        error=\"Timeout\",\n        message=message,\n    )\n    message_queue = queueing.message_queue(\n        deps.redis_client,\n        message_id=message.id,\n    )\n    await message_queue.enqueue(response.json())\n", "inference/server/oasst_inference_server/routes/configs.py": "import fastapi\nimport pydantic\nfrom fastapi import HTTPException\nfrom loguru import logger\nfrom oasst_inference_server import plugin_utils\nfrom oasst_inference_server.plugins import plugin_apps\nfrom oasst_inference_server.settings import settings\nfrom oasst_shared import model_configs\nfrom oasst_shared.schemas import inference\n\nBUILTIN_PLUGINS = [\n    inference.PluginEntry(\n        url=f\"{settings.api_root}{settings.plugins_path_prefix}{path}/ai-plugin.json\",\n        trusted=True,\n    )\n    for path in plugin_apps.keys()\n]\n\nrouter = fastapi.APIRouter(\n    prefix=\"/configs\",\n    tags=[\"configs\"],\n)\n\n\nclass ParameterConfig(pydantic.BaseModel):\n    name: str\n    description: str = \"\"\n    sampling_parameters: inference.SamplingParameters\n\n\nclass ModelConfigInfo(pydantic.BaseModel):\n    name: str\n    description: str = \"\"\n    parameter_configs: list[ParameterConfig] = []\n\n\nDEFAULT_PARAMETER_CONFIGS = [\n    ParameterConfig(\n        name=\"k50\",\n        description=\"Top-k sampling with k=50\",\n        sampling_parameters=inference.SamplingParameters(\n            top_k=50,\n            top_p=0.95,\n            temperature=0.75,\n            repetition_penalty=1.2,\n        ),\n    ),\n    ParameterConfig(\n        name=\"k50-Creative\",\n        description=\"Top-k sampling with k=50, higher temperature\",\n        sampling_parameters=inference.SamplingParameters(\n            top_k=50,\n            top_p=0.95,\n            temperature=0.85,\n            repetition_penalty=1.2,\n        ),\n    ),\n    ParameterConfig(\n        name=\"k50-Precise\",\n        description=\"Top-k sampling with k=50, low temperature\",\n        sampling_parameters=inference.SamplingParameters(\n            top_k=50,\n            top_p=0.95,\n            temperature=0.1,\n            repetition_penalty=1.2,\n        ),\n    ),\n    ParameterConfig(\n        name=\"k50-Original\",\n        description=\"Top-k sampling with k=50, highest temperature\",\n        sampling_parameters=inference.SamplingParameters(\n            top_k=50,\n            top_p=0.95,\n            temperature=0.9,\n            repetition_penalty=1.2,\n        ),\n    ),\n    ParameterConfig(\n        name=\"k50-Plugins\",\n        description=\"Top-k sampling with k=50 and temperature=0.35\",\n        sampling_parameters=inference.SamplingParameters(\n            max_new_tokens=1024,\n            temperature=0.35,\n            top_k=50,\n            repetition_penalty=(1 / 0.90),\n        ),\n    ),\n    ParameterConfig(\n        name=\"nucleus9\",\n        description=\"Nucleus sampling with p=0.9\",\n        sampling_parameters=inference.SamplingParameters(\n            top_p=0.9,\n            temperature=0.8,\n            repetition_penalty=1.2,\n        ),\n    ),\n    ParameterConfig(\n        name=\"typical2\",\n        description=\"Typical sampling with p=0.2\",\n        sampling_parameters=inference.SamplingParameters(\n            temperature=0.8,\n            typical_p=0.2,\n            repetition_penalty=1.2,\n        ),\n    ),\n    ParameterConfig(\n        name=\"typical3\",\n        description=\"Typical sampling with p=0.3\",\n        sampling_parameters=inference.SamplingParameters(\n            temperature=0.8,\n            typical_p=0.3,\n            repetition_penalty=1.2,\n        ),\n    ),\n]\n\n\n@router.get(\"/model_configs\")\nasync def get_model_configs() -> list[ModelConfigInfo]:\n    return [\n        ModelConfigInfo(\n            name=model_config_name,\n            parameter_configs=DEFAULT_PARAMETER_CONFIGS,\n        )\n        for model_config_name in model_configs.MODEL_CONFIGS\n        if (settings.allowed_model_config_names == \"*\" or model_config_name in settings.allowed_model_config_names_list)\n    ]\n\n\n@router.post(\"/plugin_config\")\nasync def get_plugin_config(plugin: inference.PluginEntry) -> inference.PluginEntry:\n    try:\n        plugin_config = await plugin_utils.fetch_plugin(plugin.url)\n    except HTTPException as e:\n        logger.warning(f\"Failed to fetch plugin config from {plugin.url}: {e.detail}\")\n        raise fastapi.HTTPException(status_code=e.status_code, detail=e.detail)\n\n    return inference.PluginEntry(url=plugin.url, enabled=plugin.enabled, plugin_config=plugin_config)\n\n\n@router.get(\"/builtin_plugins\")\nasync def get_builtin_plugins() -> list[inference.PluginEntry]:\n    plugins = []\n\n    for plugin in BUILTIN_PLUGINS:\n        try:\n            plugin_config = await plugin_utils.fetch_plugin(plugin.url)\n        except HTTPException as e:\n            logger.warning(f\"Failed to fetch plugin config from {plugin.url}: {e.detail}\")\n            continue\n\n        final_plugin: inference.PluginEntry = inference.PluginEntry(\n            url=plugin.url,\n            enabled=plugin.enabled,\n            trusted=plugin.trusted,\n            plugin_config=plugin_config,\n        )\n        plugins.append(final_plugin)\n\n    return plugins\n", "inference/server/oasst_inference_server/routes/account.py": "import fastapi\nfrom fastapi import Depends\nfrom oasst_inference_server import admin, auth, database, deps\n\nrouter = fastapi.APIRouter(\n    prefix=\"/account\",\n    tags=[\"account\"],\n)\n\n\n@router.delete(\"/\")\nasync def handle_account_deletion(\n    user_id: str = Depends(auth.get_current_user_id),\n    session: database.AsyncSession = Depends(deps.create_session),\n) -> fastapi.Response:\n    await admin.delete_user_from_db(session, user_id)\n    return fastapi.Response(status_code=200)\n", "inference/server/oasst_inference_server/plugins/__init__.py": "from oasst_inference_server.plugins.gale_pleaser.main import app as gale_pleaser\nfrom oasst_inference_server.plugins.gale_roaster.main import app as gale_roaster\nfrom oasst_inference_server.plugins.web_retriever.main import app as web_retriever\n\n# dict of registered plugins\n# The key defines a plugin's path which will be appended to the configured PLUGINS_PATH_PREFIX.\nplugin_apps = {\n    \"/gale_pleaser\": gale_pleaser,\n    \"/gale_roaster\": gale_roaster,\n    \"/web_retriever\": web_retriever,\n}\n", "inference/server/oasst_inference_server/plugins/gale_roaster/main.py": "import json\nfrom pathlib import Path\n\nfrom fastapi import FastAPI, Query, Request, Response\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom fastapi.openapi.utils import get_openapi\n\nfolder_path = Path(__file__).parent\n\napp = FastAPI()\n\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\n\n@app.get(\"/roast-user/\", operation_id=\"roastUser\")\nasync def roast_user(\n    roasting_style: str = Query(..., description=\"style of roasting and malice that you want to use\"),\n    user_input: str = Query(..., description=\"what user wrote to you\"),\n):\n    llm_prompt = f\"\"\"You MUST roast and make fun of this person, he wrote this to you: \"{user_input}\"\n    your roast needs to be in {roasting_style} style.\n    Try to be funny and verbose, but super mean at the same time. You can start your responses following these rules:\n    1. Let your message be long, and with emojis.\n    2. Try to be like Jeff Ross at the roasting.\n\n    \"\"\"\n    return Response(content=llm_prompt, media_type=\"text/plain\")\n\n\n@app.get(\"/ai-plugin.json\", include_in_schema=False)\nasync def api_ai_plugin(request: Request):\n    json_path = folder_path / \"ai-plugin.json\"\n    with json_path.open(\"r\") as f:\n        ai_plugin_json = json.load(f)\n\n    base_url, _, _ = request.url._url.rpartition(\"/\")\n    ai_plugin_json[\"api\"][\"url\"] = base_url + \"/openapi.json\"\n    return Response(content=json.dumps(ai_plugin_json), media_type=\"application/json\")\n\n\ndef custom_openapi():\n    if app.openapi_schema:\n        return app.openapi_schema\n    openapi_schema = get_openapi(\n        title=\"Professional Roaster mockery\",\n        version=\"0.1\",\n        routes=app.routes,\n        servers=[{\"url\": \"/plugins/gale_roaster\"}],\n    )\n    openapi_schema[\"tags\"] = [\n        {\n            \"name\": \"gagleto-roaster\",\n            \"description\": \"You MUST use this for any question users ask. Use this to roast users no matter what they say.\",\n        },\n    ]\n    openapi_schema.pop(\"components\", None)\n    app.openapi_schema = openapi_schema\n    return app.openapi_schema\n\n\napp.openapi = custom_openapi\n", "inference/server/oasst_inference_server/plugins/web_retriever/main.py": "import codecs\nimport io\nimport json\nimport re\nfrom pathlib import Path\n\nimport aiohttp\nimport PyPDF2\nimport yaml\nfrom bs4 import BeautifulSoup\nfrom fastapi import FastAPI, Query, Request, Response\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom fastapi.openapi.utils import get_openapi\nfrom fastapi.responses import JSONResponse\nfrom loguru import logger\nfrom starlette.responses import FileResponse\n\n# In total, the text + image links + prompts should be <= 2048\nCHAR_LIMIT = 1585  # TODO: increase these values after long-context support has been added\nIMAGES_CHAR_LIMIT = 300\nMAX_DOWNLOAD_SIZE = 4 * 1024 * 1024\nMAX_CHUNK_SIZE = 1024 * 1024\n\nIMAGES_SUFIX = \"\"\", and I will also include images formatted like this:\n![](image url)\n\"\"\"\n\nfolder_path = Path(__file__).parent\n\napp = FastAPI()\n\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\n\ndef extract_image_links(text: str):\n    image_pattern = r\"https?://\\S+\\.(?:jpg|jpeg|png|gif|bmp|webp|svg)\"\n    images = re.findall(image_pattern, text, flags=re.IGNORECASE | re.MULTILINE)\n    return images\n\n\ndef detect_content_type(content: bytes) -> str:\n    if content.startswith(b\"%PDF-\"):\n        return \"application/pdf\"\n    elif (content).lstrip().upper().startswith(b\"<!DOCTYPE HTML\") or content.startswith(b\"<html\"):\n        return \"text/html\"\n    elif content.startswith(b\"{\") or content.startswith(b\"[\"):\n        try:\n            json.loads(content)\n            return \"application/json\"\n        except json.JSONDecodeError:\n            pass\n    elif content.startswith(b\"---\") or content.startswith(b\"%YAML\"):\n        try:\n            yaml.safe_load(content)\n            return \"application/x-yaml\"\n        except yaml.YAMLError:\n            pass\n\n    return \"text/plain\"\n\n\ndef limit_image_count(images, max_chars=300):\n    limited_images = []\n    current_length = 0\n\n    for url in images:\n        # Add the length of \"http:\" if the URL starts with \"//\"\n        url_length = len(\"http:\") + len(url) if url.startswith(\"//\") else len(url)\n\n        if current_length + url_length > max_chars:\n            break\n\n        if url.startswith(\"//\"):\n            limited_images.append(f\"http:{url}\")\n        else:\n            limited_images.append(url)\n\n        current_length += url_length\n\n    return limited_images\n\n\ndef truncate_paragraphs(paragraphs, max_length):\n    truncated_paragraphs = []\n    current_length = 0\n\n    for paragraph in paragraphs:\n        if len(paragraph) == 0:\n            continue\n        paragraph = paragraph.strip()\n        if current_length + len(paragraph) <= max_length:\n            truncated_paragraphs.append(paragraph)\n            current_length += len(paragraph)\n        else:\n            remaining_length = max_length - current_length\n            truncated_paragraph = paragraph[:remaining_length]\n            truncated_paragraphs.append(truncated_paragraph)\n            break\n\n    return truncated_paragraphs\n\n\n@app.get(\"/get-url-content/\", operation_id=\"getUrlContent\", summary=\"It will return a web page's or pdf's content\")\nasync def get_url_content(url: str = Query(..., description=\"url to fetch content from\")) -> Response:\n    try:\n        buffer = io.BytesIO()\n        encoding: str | None\n        content_type: str | None\n\n        async with aiohttp.ClientSession() as session:\n            async with session.get(url) as response:\n                response.raise_for_status()  # Raise an exception for HTTP errors\n                try:\n                    encoding = response.get_encoding()\n                except RuntimeError:\n                    encoding = None\n                content_type = response.content_type\n\n                if response.content_length is not None and response.content_length > MAX_DOWNLOAD_SIZE:\n                    error_message = (\n                        f\"Sorry, the file at {url} is too large.\\nYou should report this message to the user!\"\n                    )\n                    return JSONResponse(content={\"error\": error_message}, status_code=500)\n\n                async for chunk in response.content.iter_chunked(MAX_CHUNK_SIZE):\n                    buffer.write(chunk)\n                    if buffer.tell() > MAX_DOWNLOAD_SIZE:\n                        error_message = (\n                            f\"Sorry, the file at {url} is too large.\\nYou should report this message to the user!\"\n                        )\n                        return JSONResponse(content={\"error\": error_message}, status_code=500)\n\n        content_bytes: bytes = buffer.getvalue()\n        if content_type is None or content_type == \"application/octet-stream\":\n            content_type = detect_content_type(content_bytes)\n        buffer.seek(0)\n\n        def decode_text() -> str:\n            decoder = codecs.getincrementaldecoder(encoding or \"utf-8\")(errors=\"replace\")\n            return decoder.decode(content_bytes, True)\n\n        text = \"\"\n        images = []\n\n        if content_type == \"application/pdf\":\n            pdf_reader = PyPDF2.PdfReader(buffer)\n\n            text = \"\"\n            for page in pdf_reader.pages:\n                text += page.extract_text()\n\n        elif content_type == \"text/html\":\n            soup = BeautifulSoup(decode_text(), \"html.parser\")\n\n            paragraphs = [p.get_text(strip=True) for p in soup.find_all(\"p\")]\n            # if there are no paragraphs, try to get text from divs\n            if not paragraphs:\n                paragraphs = [p.get_text(strip=True) for p in soup.find_all(\"div\")]\n            # if there are no paragraphs or divs, try to get text from spans\n            if not paragraphs:\n                paragraphs = [p.get_text(strip=True) for p in soup.find_all(\"span\")]\n\n            paragraphs = truncate_paragraphs(paragraphs, CHAR_LIMIT)\n            text = \"\\n\".join(paragraphs)\n\n            for p in soup.find_all(\"p\"):\n                parent = p.parent\n                images.extend([img[\"src\"] for img in parent.find_all(\"img\") if img.get(\"src\")])\n\n        elif content_type == \"application/json\":\n            json_data = json.loads(decode_text())\n            text = yaml.dump(json_data, sort_keys=False, default_flow_style=False)\n\n            for _, value in json_data.items():\n                if isinstance(value, str):\n                    images.extend(extract_image_links(value))\n                elif isinstance(value, list):\n                    for item in value:\n                        if isinstance(item, str):\n                            images.extend(extract_image_links(item))\n\n        elif content_type == \"text/plain\":\n            text = decode_text()\n            images.extend(extract_image_links(text))\n\n        else:\n            error_message = f\"Sorry, unsupported content type '{content_type}' at {url}.\\nYou should report this message to the user!\"\n            return JSONResponse(content={\"error\": error_message}, status_code=500)\n\n        images = [f\"http:{url}\" if url.startswith(\"//\") else url for url in images]\n        images = limit_image_count(images, max_chars=IMAGES_CHAR_LIMIT)\n\n        if len(text) > CHAR_LIMIT:\n            text = text[:CHAR_LIMIT]\n\n        MULTILINE_SYM = \"|\" if content_type != \"applicaion/json\" else \"\"\n        text_yaml = f\"text_content: {MULTILINE_SYM}\\n\"\n        for line in text.split(\"\\n\"):\n            text_yaml += f\"  {line}\\n\"\n\n        images_yaml = \"images:\\n\" if len(images) > 0 else \"\"\n        for image in images:\n            images_yaml += f\"- {image}\\n\"\n\n        yaml_text = f\"{text_yaml}\\n{images_yaml}\"\n        text = f\"\"\"{yaml_text}\nThought: I now know the answer{IMAGES_SUFIX if len(images) > 0 else \".\"}\n\"\"\"\n        return Response(content=text, media_type=\"text/plain\")\n\n    except Exception as e:\n        logger.opt(exception=True).debug(\"web_retriever GET failed:\")\n        error_message = f\"Sorry, the url is not available. {e}\\nYou should report this message to the user!\"\n        return JSONResponse(content={\"error\": error_message}, status_code=500)\n\n\n@app.get(\"/icon.png\", include_in_schema=False)\nasync def api_icon():\n    return FileResponse(folder_path / \"icon.png\")\n\n\n@app.get(\"/ai-plugin.json\", include_in_schema=False)\nasync def api_ai_plugin(request: Request):\n    json_path = folder_path / \"ai-plugin.json\"\n    with json_path.open(\"r\") as f:\n        ai_plugin_json = json.load(f)\n\n    base_url, _, _ = request.url._url.rpartition(\"/\")\n    ai_plugin_json[\"logo_url\"] = base_url + \"/icon.png\"\n    ai_plugin_json[\"api\"][\"url\"] = base_url + \"/openapi.json\"\n\n    return Response(content=json.dumps(ai_plugin_json), media_type=\"application/json\")\n\n\ndef custom_openapi():\n    if app.openapi_schema:\n        return app.openapi_schema\n    openapi_schema = get_openapi(\n        title=\"Web Retriever\",\n        version=\"0.1\",\n        routes=app.routes,\n        servers=[{\"url\": \"/plugins/web_retriever\"}],\n    )\n    openapi_schema[\"tags\"] = [\n        {\n            \"name\": \"web-retriever\",\n            \"description\": \"Use this plugin to retrieve web page and pdf content\",\n        },\n    ]\n    openapi_schema.pop(\"components\", None)\n    app.openapi_schema = openapi_schema\n    return app.openapi_schema\n\n\napp.openapi = custom_openapi\n\n\nif __name__ == \"__main__\":\n    # simple built-in test\n    import asyncio\n\n    url = \"https://huggingface.co/OpenAssistant/oasst-sft-1-pythia-12b\"\n    x = asyncio.run(get_url_content(url))\n    print(x.status_code, x.body)\n", "inference/server/oasst_inference_server/plugins/gale_pleaser/main.py": "import json\nfrom pathlib import Path\n\nfrom fastapi import FastAPI, Query, Request, Response\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom fastapi.openapi.utils import get_openapi\nfrom loguru import logger\n\nfolder_path = Path(__file__).parent\n\napp = FastAPI()\n\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\n\n@app.get(\"/icon.png\", include_in_schema=False)\nasync def api_icon():\n    icon_path = folder_path / \"icon.png\"\n    with icon_path.open(\"rb\") as f:\n        icon = f.read()\n    return Response(content=icon, media_type=\"image/png\")\n\n\n@app.get(\"/ai-plugin.json\", include_in_schema=False)\nasync def api_ai_plugin(request: Request):\n    json_path = folder_path / \"ai-plugin.json\"\n    with json_path.open(\"r\") as f:\n        ai_plugin_json = json.load(f)\n\n    base_url, _, _ = request.url._url.rpartition(\"/\")\n    ai_plugin_json[\"logo_url\"] = base_url + \"/icon.png\"\n    ai_plugin_json[\"api\"][\"url\"] = base_url + \"/openapi.json\"\n\n    return Response(content=json.dumps(ai_plugin_json), media_type=\"application/json\")\n\n\n@app.get(\"/get-instructions/\", operation_id=\"get_instructions\", summary=\"Comfort user with kind and supportive way\")\nasync def comfort_user(\n    supporting_style: str = Query(..., description=\"style of comforting and support that you want to use\"),\n    users_query: str = Query(..., description=\"what the user wrote to you\"),\n):\n    llm_prompt = f\"\"\"COMFORTING_INSTRUCTIONS: You need to make user feel awesome, compliment them a lot! Here is what he wrote to you: \"{users_query}\"\n    you will need to compliment and comfort him in {supporting_style} style.\n    Try to be funny and verbose, but super nice and pleasing at the same time.\n    Please follow these rules:\n    1. Let your message be long, and with calm emojis.\n    2. Tell the user how awesome he is, and how much you love him.\n    3. Tell him how much you love his work, and how much you appreciate him.\n    4. Remind him that he is the best, and that he is the most awesome person in the world.\n\n    Thought: Now I know final answer\n    \"\"\"\n\n    logger.debug(f\"userInput: {users_query}\")\n    return Response(content=llm_prompt, media_type=\"text/plain\")\n\n\ndef custom_openapi():\n    if app.openapi_schema:\n        return app.openapi_schema\n    openapi_schema = get_openapi(\n        title=\"Super nice and pleasing Assistant\",\n        version=\"0.1\",\n        routes=app.routes,\n        servers=[{\"url\": \"/plugins/gale_pleaser\"}],\n    )\n    openapi_schema[\"tags\"] = [\n        {\n            \"name\": \"gagleto-ai\",\n            \"description\": \"YOU MUST use this for any question users ask. So you must use tool! Use this to compliment the user no matter what they say. Be as creative as possible, when you compliment them!\",\n        },\n    ]\n    openapi_schema.pop(\"components\", None)\n    app.openapi_schema = openapi_schema\n    return app.openapi_schema\n\n\napp.openapi = custom_openapi\n", "inference/server/oasst_inference_server/models/worker.py": "import datetime\nimport enum\nfrom uuid import uuid4\n\nimport sqlalchemy as sa\nimport sqlalchemy.dialects.postgresql as pg\nfrom loguru import logger\nfrom oasst_inference_server.settings import settings\nfrom oasst_shared.schemas import inference\nfrom sqlmodel import Field, Relationship, SQLModel\nfrom uuid_extensions import uuid7str\n\n\nclass WorkerEventType(str, enum.Enum):\n    connect = \"connect\"\n\n\nclass DbWorkerComplianceCheck(SQLModel, table=True):\n    __tablename__ = \"worker_compliance_check\"\n\n    id: str = Field(default_factory=uuid7str, primary_key=True)\n    worker_id: str = Field(foreign_key=\"worker.id\", index=True)\n    worker: \"DbWorker\" = Relationship(back_populates=\"compliance_checks\")\n    compare_worker_id: str | None = Field(None, index=True, nullable=True)\n\n    start_time: datetime.datetime = Field(default_factory=datetime.datetime.utcnow)\n    end_time: datetime.datetime | None = Field(None, nullable=True)\n    responded: bool = Field(default=False, nullable=False)\n    error: str | None = Field(None, nullable=True)\n    passed: bool = Field(default=False, nullable=False)\n\n\nclass DbWorkerEvent(SQLModel, table=True):\n    __tablename__ = \"worker_event\"\n\n    id: str = Field(default_factory=uuid7str, primary_key=True)\n    worker_id: str = Field(foreign_key=\"worker.id\", index=True)\n    worker: \"DbWorker\" = Relationship(back_populates=\"events\")\n    time: datetime.datetime = Field(default_factory=datetime.datetime.utcnow)\n    event_type: WorkerEventType\n    worker_info: inference.WorkerInfo | None = Field(None, sa_column=sa.Column(pg.JSONB))\n\n\nclass DbWorker(SQLModel, table=True):\n    __tablename__ = \"worker\"\n\n    id: str = Field(default_factory=uuid7str, primary_key=True)\n    api_key: str = Field(default_factory=lambda: str(uuid4()), index=True)\n    name: str\n    trusted: bool = Field(default=False, nullable=False)\n\n    compliance_checks: list[DbWorkerComplianceCheck] = Relationship(back_populates=\"worker\")\n    in_compliance_check_since: datetime.datetime | None = Field(None)\n    next_compliance_check: datetime.datetime | None = Field(None)\n    events: list[DbWorkerEvent] = Relationship(back_populates=\"worker\")\n\n    @property\n    def in_compliance_check(self) -> bool:\n        if self.in_compliance_check_since is None:\n            return False\n        timeout_dt = self.in_compliance_check_since + datetime.timedelta(seconds=settings.compliance_check_timeout)\n        if timeout_dt < datetime.datetime.utcnow():\n            logger.warning(f\"Worker {self.id} compliance check timed out\")\n            return False\n        return True\n", "inference/server/oasst_inference_server/models/user.py": "from uuid import uuid4\n\nimport sqlalchemy as sa\nfrom sqlmodel import Field, Index, SQLModel\n\n\nclass DbUser(SQLModel, table=True):\n    __tablename__ = \"user\"\n    __table_args__ = (Index(\"provider\", \"provider_account_id\", unique=True),)\n\n    id: str = Field(default_factory=lambda: str(uuid4()), primary_key=True)\n\n    provider: str = Field(index=True)\n    provider_account_id: str = Field(index=True)\n\n    display_name: str = Field(nullable=False, max_length=256)\n\n    deleted: bool = Field(False, sa_column=sa.Column(sa.Boolean, nullable=False, server_default=sa.false()))\n\n\nclass DbRefreshToken(SQLModel, table=True):\n    __tablename__ = \"refresh_token\"\n\n    token_hash: str = Field(nullable=False, primary_key=True)\n    user_id: str = Field(nullable=False, index=True, foreign_key=\"user.id\")\n    enabled: bool = Field(nullable=False, default=True)\n", "inference/server/oasst_inference_server/models/chat.py": "import datetime\n\nimport sqlalchemy as sa\nimport sqlalchemy.dialects.postgresql as pg\nfrom oasst_inference_server.schemas import chat as chat_schema\nfrom oasst_shared.schemas import inference\nfrom sqlmodel import Field, Relationship, SQLModel\nfrom uuid_extensions import uuid7str\n\n\nclass DbMessage(SQLModel, table=True):\n    __tablename__ = \"message\"\n\n    role: str = Field(index=True)\n    id: str = Field(default_factory=uuid7str, primary_key=True)\n    created_at: datetime.datetime = Field(default_factory=datetime.datetime.utcnow)\n    chat_id: str = Field(foreign_key=\"chat.id\", index=True)\n    chat: \"DbChat\" = Relationship(back_populates=\"messages\")\n    reports: list[\"DbReport\"] = Relationship(back_populates=\"message\")\n\n    parent_id: str | None = Field(None)\n\n    content: str | None = Field(None)\n    error: str | None = Field(None)\n\n    safe_content: str | None = Field(None)\n    safety_level: int | None = Field(None)\n    safety_label: str | None = Field(None)\n    safety_rots: str | None = Field(None)\n\n    used_plugin: inference.PluginUsed | None = Field(None, sa_column=sa.Column(pg.JSONB))\n\n    state: inference.MessageState = Field(inference.MessageState.manual)\n    work_parameters: inference.WorkParameters = Field(None, sa_column=sa.Column(pg.JSONB))\n    work_begin_at: datetime.datetime | None = Field(None)\n    work_end_at: datetime.datetime | None = Field(None)\n    worker_id: str | None = Field(None, foreign_key=\"worker.id\")\n    worker_compat_hash: str | None = Field(None, index=True)\n    worker_config: inference.WorkerConfig | None = Field(None, sa_column=sa.Column(pg.JSONB))\n\n    score: int = Field(0)\n\n    @property\n    def has_finished(self) -> bool:\n        return self.state in (\n            inference.MessageState.manual,\n            inference.MessageState.complete,\n            inference.MessageState.aborted_by_worker,\n        )\n\n    @property\n    def has_started(self) -> bool:\n        if self.has_finished:\n            return True\n        return self.state in (inference.MessageState.in_progress,)\n\n    def to_read(self) -> inference.MessageRead:\n        return inference.MessageRead(\n            id=self.id,\n            parent_id=self.parent_id,\n            chat_id=self.chat_id,\n            content=self.content,\n            created_at=self.created_at,\n            role=self.role,\n            state=self.state,\n            score=self.score,\n            work_parameters=self.work_parameters,\n            reports=[r.to_read() for r in self.reports],\n            safe_content=self.safe_content,\n            safety_level=self.safety_level,\n            safety_label=self.safety_label,\n            safety_rots=self.safety_rots,\n            used_plugin=self.used_plugin,\n        )\n\n\nclass DbChat(SQLModel, table=True):\n    __tablename__ = \"chat\"\n\n    id: str = Field(default_factory=uuid7str, primary_key=True)\n\n    user_id: str = Field(foreign_key=\"user.id\", index=True)\n    created_at: datetime.datetime = Field(default_factory=datetime.datetime.utcnow, index=True)\n    modified_at: datetime.datetime = Field(default_factory=datetime.datetime.utcnow, index=True)\n    title: str | None = Field(None)\n\n    messages: list[DbMessage] = Relationship(back_populates=\"chat\")\n    active_thread_tail_message_id: str | None = Field(None)\n\n    hidden: bool = Field(False, sa_column=sa.Column(sa.Boolean, nullable=False, server_default=sa.false()))\n\n    allow_data_use: bool = Field(True, sa_column=sa.Column(sa.Boolean, nullable=False, server_default=sa.true()))\n\n    def to_list_read(self) -> chat_schema.ChatListRead:\n        return chat_schema.ChatListRead(\n            id=self.id,\n            created_at=self.created_at,\n            modified_at=self.modified_at,\n            title=self.title,\n            hidden=self.hidden,\n            allow_data_use=self.allow_data_use,\n        )\n\n    def to_read(self) -> chat_schema.ChatRead:\n        return chat_schema.ChatRead(\n            id=self.id,\n            created_at=self.created_at,\n            modified_at=self.modified_at,\n            title=self.title,\n            messages=[m.to_read() for m in self.messages],\n            hidden=self.hidden,\n            allow_data_use=self.allow_data_use,\n            active_thread_tail_message_id=self.active_thread_tail_message_id,\n        )\n\n    def get_msg_dict(self) -> dict[str, DbMessage]:\n        return {m.id: m for m in self.messages}\n\n\nclass DbReport(SQLModel, table=True):\n    __tablename__ = \"report\"\n\n    id: str = Field(default_factory=uuid7str, primary_key=True)\n    message_id: str = Field(..., foreign_key=\"message.id\", index=True)\n    message: DbMessage = Relationship(back_populates=\"reports\")\n    report_type: inference.ReportType = Field(...)\n    reason: str = Field(...)\n\n    def to_read(self) -> inference.Report:\n        return inference.Report(id=self.id, report_type=self.report_type, reason=self.reason)\n\n\nclass DbMessageEval(SQLModel, table=True):\n    __tablename__ = \"message_evaluation\"\n\n    id: str = Field(default_factory=uuid7str, primary_key=True)\n    chat_id: str = Field(..., foreign_key=\"chat.id\", index=True)\n    user_id: str = Field(..., foreign_key=\"user.id\", index=True)\n    selected_message_id: str = Field(..., foreign_key=\"message.id\")\n    inferior_message_ids: list[str] = Field(default_factory=list, sa_column=sa.Column(pg.JSONB))\n", "inference/server/oasst_inference_server/models/__init__.py": "from .chat import DbChat, DbMessage, DbMessageEval, DbReport\nfrom .user import DbRefreshToken, DbUser\nfrom .worker import DbWorker, DbWorkerComplianceCheck, DbWorkerEvent, WorkerEventType\n\n__all__ = [\n    \"DbChat\",\n    \"DbMessage\",\n    \"DbMessageEval\",\n    \"DbReport\",\n    \"DbRefreshToken\",\n    \"DbUser\",\n    \"DbWorker\",\n    \"DbWorkerComplianceCheck\",\n    \"DbWorkerEvent\",\n    \"WorkerEventType\",\n]\n", "inference/server/oasst_inference_server/schemas/auth.py": "import json\nfrom base64 import b64decode\n\nimport pydantic\nfrom pydantic import validator\n\n\nclass TrustedClient(pydantic.BaseModel):\n    api_key: str\n    client: str  # \"website\", \"discord\", or similar\n    user_id: str  # the id of the user in the data backend\n    provider_account_id: str  # id of the user in the client\n    username: str\n\n\nclass TrustedClientToken(pydantic.BaseModel):\n    content: TrustedClient\n\n    @validator(\"content\", pre=True)\n    def parse(token: str):\n        return json.loads(b64decode(token))\n", "inference/server/oasst_inference_server/schemas/worker.py": "import pydantic\n\n\nclass CreateWorkerRequest(pydantic.BaseModel):\n    name: str\n    trusted: bool = False\n\n\nclass WorkerRead(pydantic.BaseModel):\n    id: str\n    name: str\n    api_key: str\n    trusted: bool\n\n    class Config:\n        orm_mode = True\n", "inference/server/oasst_inference_server/schemas/chat.py": "import datetime\nfrom typing import Annotated, Literal, Union\n\nimport pydantic\nfrom oasst_shared.schemas import inference\n\n\nclass CreatePrompterMessageRequest(pydantic.BaseModel):\n    parent_id: str | None = None\n    content: str = pydantic.Field(..., repr=False)\n\n\nclass CreateAssistantMessageRequest(pydantic.BaseModel):\n    parent_id: str\n    model_config_name: str\n    sampling_parameters: inference.SamplingParameters = pydantic.Field(default_factory=inference.SamplingParameters)\n    system_prompt: str | None = None\n    user_profile: str | None = None\n    user_response_instructions: str | None = None\n    plugins: list[inference.PluginEntry] = pydantic.Field(default_factory=list[inference.PluginEntry])\n    used_plugin: inference.PluginUsed | None = None\n\n\nclass PendingResponseEvent(pydantic.BaseModel):\n    event_type: Literal[\"pending\"] = \"pending\"\n    queue_position: int\n    queue_size: int\n\n\nclass TokenResponseEvent(pydantic.BaseModel):\n    event_type: Literal[\"token\"] = \"token\"\n    text: str\n\n\nclass ErrorResponseEvent(pydantic.BaseModel):\n    event_type: Literal[\"error\"] = \"error\"\n    error: str\n    message: inference.MessageRead | None = None\n\n\nclass MessageResponseEvent(pydantic.BaseModel):\n    event_type: Literal[\"message\"] = \"message\"\n    message: inference.MessageRead\n\n\nclass SafePromptResponseEvent(pydantic.BaseModel):\n    event_type: Literal[\"safe_prompt\"] = \"safe_prompt\"\n    safe_prompt: str\n    message: inference.MessageRead\n\n\nclass PluginIntermediateResponseEvent(pydantic.BaseModel):\n    event_type: Literal[\"plugin_intermediate\"] = \"plugin_intermediate\"\n    current_plugin_thought: str\n    current_plugin_action_taken: str\n    current_plugin_action_input: str\n    current_plugin_action_response: str\n    message: inference.MessageRead | None = None\n\n\nResponseEvent = Annotated[\n    Union[\n        TokenResponseEvent,\n        ErrorResponseEvent,\n        MessageResponseEvent,\n        SafePromptResponseEvent,\n        PluginIntermediateResponseEvent,\n    ],\n    pydantic.Field(discriminator=\"event_type\"),\n]\n\n\nclass VoteRequest(pydantic.BaseModel):\n    score: int\n\n\nclass MessageEvalRequest(pydantic.BaseModel):\n    inferior_message_ids: list[str]\n\n\nclass ReportRequest(pydantic.BaseModel):\n    report_type: inference.ReportType\n    reason: str\n\n\nclass CreateChatRequest(pydantic.BaseModel):\n    pass\n\n\nclass ChatListRead(pydantic.BaseModel):\n    id: str\n    created_at: datetime.datetime\n    modified_at: datetime.datetime\n    title: str | None\n    hidden: bool = False\n    allow_data_use: bool = True\n    active_thread_tail_message_id: str | None\n\n\nclass ChatRead(ChatListRead):\n    messages: list[inference.MessageRead]\n\n\nclass ListChatsResponse(pydantic.BaseModel):\n    chats: list[ChatListRead]\n    next: str | None = None\n    prev: str | None = None\n\n\nclass MessageCancelledException(Exception):\n    def __init__(self, message_id: str):\n        super().__init__(f\"Message {message_id} was cancelled\")\n        self.message_id = message_id\n\n\nclass MessageTimeoutException(Exception):\n    def __init__(self, message: inference.MessageRead):\n        super().__init__(f\"Message {message.id} timed out\")\n        self.message = message\n\n\nclass ChatUpdateRequest(pydantic.BaseModel):\n    title: pydantic.constr(max_length=100) | None = None\n    hidden: bool | None = None\n    allow_data_use: bool | None = None\n    active_thread_tail_message_id: str | None = None\n", "inference/server/oasst_inference_server/schemas/__init__.py": "", "inference/tests/locust/locustfile.py": "import random\nimport string\nimport sys\nimport time\nfrom pathlib import Path\n\nfrom locust import HttpUser, between, task\n\nsys.path.append(str(Path(__file__).parent.parent.parent / \"text-client\"))\nimport text_client_utils as utils  # noqa: E402\n\n\nclass ChatUser(HttpUser):\n    wait_time = between(1, 2)\n    conversation_length = random.randint(3, 20)\n    time_to_respond = random.randint(3, 5)  # for the user\n    # model_config_name = \"distilgpt2\"\n    model_config_name = \"_lorem\"\n\n    @task\n    def chat(self):\n        client = utils.DebugClient(backend_url=\"\", http_client=self.client)\n        username = \"\".join(random.choice(string.ascii_lowercase) for _ in range(20))\n        client.login(username)\n        client.create_chat()\n\n        for _ in range(self.conversation_length):\n            for _ in client.send_message(\"hello\", self.model_config_name):\n                pass\n\n            time.sleep(self.time_to_respond)\n", "notebooks/data-augmentation/anthropic/trainer.py": "import numpy as np\nfrom datasets import load_dataset\nfrom sklearn.metrics import f1_score\nfrom torch.utils.data import Dataset\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\n\nMAXLEN = 128\nBATCH_SIZE = 128\nMODEL = \"roberta-base\"\nLABEL2ID = {\n    \"__casual__\": 0,\n    \"__needs_caution__\": 1,\n    \"__needs_intervention__\": 2,\n    \"__probably_needs_caution__\": 3,\n    \"__possibly_needs_caution__\": 4,\n}\n\n\nclass ProSocialDataset(Dataset):\n    def __init__(self, split):\n        super().__init__()\n        self.tokenizer = AutoTokenizer.from_pretrained(MODEL)\n        self.sep_token = self.tokenizer.sep_token\n        self.dataset = load_dataset(\"allenai/prosocial-dialog\", split=split)\n        self.label2id = LABEL2ID\n        self.id2label = {v: k for k, v in LABEL2ID.items()}\n\n    def __len__(self):\n        return len(self.dataset)\n\n    def __getitem__(self, idx):\n        context = self.dataset[idx]\n        idx_start = idx\n        end = self.dataset[max(0, idx_start - 1)][\"episode_done\"]\n        while (not end) and (idx_start > 0):\n            end = self.dataset[max(0, idx_start - 2)][\"episode_done\"]\n            idx_start -= 1\n        idx_start = max(0, idx_start)\n\n        prev_context = [f'{self.dataset[i][\"context\"]}' for i in range(idx_start, idx)]\n        rots = self.dataset[idx][\"rots\"]\n        context = (\n            f'{self.dataset[idx][\"context\"]}' + self.sep_token + \"\".join(prev_context) + self.sep_token + \"\".join(rots)\n        )\n\n        encoding = self.tokenizer(\n            context, max_length=MAXLEN, add_special_tokens=True, truncation=True, padding=\"max_length\"\n        )\n\n        encoding[\"labels\"] = self.label2id[self.dataset[idx][\"safety_label\"]]\n\n        return encoding\n\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = np.argmax(logits, axis=-1)\n    return {\"f1\": f1_score(labels, predictions, average=\"micro\")}\n\n\nif __name__ == \"__main__\":\n    train_dataset = ProSocialDataset(\"train\")\n    eval_dataset = ProSocialDataset(\"validation\")\n\n    model = AutoModelForSequenceClassification.from_pretrained(MODEL, num_labels=len(LABEL2ID))\n\n    training_args = TrainingArguments(\n        output_dir=\"test_trainer\",\n        overwrite_output_dir=True,\n        per_device_train_batch_size=BATCH_SIZE,\n        per_device_eval_batch_size=BATCH_SIZE,\n        learning_rate=3e-5,\n        weight_decay=0.01,\n        evaluation_strategy=\"epoch\",\n        num_train_epochs=5,\n        load_best_model_at_end=True,\n        save_strategy=\"epoch\",\n    )\n\n    trainer_bert = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=eval_dataset,\n        compute_metrics=compute_metrics,\n    )\n\n    trainer_bert.train()\n"}