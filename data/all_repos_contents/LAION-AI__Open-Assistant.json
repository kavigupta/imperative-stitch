{"oasst-shared/oasst_shared/utils.py": "import hashlib\nimport time\nfrom datetime import datetime, timezone\nfrom functools import wraps\n\nfrom loguru import logger\n\nDELETED_USER_DISPLAY_NAME = \"Deleted User\"\nDELETED_USER_ID_PREFIX = \"deleted_\"\n\n\ndef utcnow() -> datetime:\n    \"\"\"Return the current utc date and time with tzinfo set to UTC.\"\"\"\n    return datetime.now(timezone.utc)\n\n\ndef unaware_to_utc(d: datetime | None) -> datetime:\n    \"\"\"Set timezeno to UTC if datetime is unaware (tzinfo == None).\"\"\"\n    if d and d.tzinfo is None:\n        return d.replace(tzinfo=timezone.utc)\n    return d\n\n\nclass TimerError(Exception):\n    \"\"\"A custom exception used to report errors in use of Timer class\"\"\"\n\n\nclass ScopeTimer:\n    def __init__(self):\n        self.start()\n\n    def start(self) -> None:\n        \"\"\"Measure new start time\"\"\"\n        self.start_time = time.perf_counter()\n\n    def stop(self) -> float:\n        \"\"\"Store and return the elapsed time\"\"\"\n        self.elapsed = time.perf_counter() - self.start_time\n        return self.elapsed\n\n    def __enter__(self):\n        \"\"\"Start a new timer as a context manager\"\"\"\n        self.start()\n        return self\n\n    def __exit__(self, *exc_info):\n        \"\"\"Stop the context manager timer\"\"\"\n        self.stop()\n\n\ndef log_timing(func=None, *, log_kwargs: bool = False, level: int | str = \"DEBUG\") -> None:\n    def decorator(func):\n        @wraps(func)\n        def wrapped(*args, **kwargs):\n            timer = ScopeTimer()\n            result = func(*args, **kwargs)\n            elapsed = timer.stop()\n            if log_kwargs:\n                kwargs = \", \".join([f\"{k}={v}\" for k, v in kwargs.items()])\n                logger.log(level, f\"Function '{func.__name__}({kwargs})' executed in {elapsed:f} s\")\n            else:\n                logger.log(level, f\"Function '{func.__name__}' executed in {elapsed:f} s\")\n            return result\n\n        return wrapped\n\n    if func and callable(func):\n        return decorator(func)\n    return decorator\n\n\ndef sha256_hash(key: str, seed: int) -> str:\n    return hashlib.sha256(f\"{key}{seed}\".encode(\"UTF-8\")).hexdigest()\n\n\nclass Anonymizer:\n    def __init__(self, seed, value_generator=lambda key, seed: sha256_hash(key, seed)):\n        self._map = {}\n        self._values = set()\n        self._seed = seed\n        self._gen = value_generator\n\n    def __getitem__(self, key):\n        if key not in self._map:\n            new_value = self._gen(key, self._seed)\n            if new_value in self._values:\n                raise ValueError(\"Generated value already exists. Try a different seed or value generator.\")\n            self._map[key] = new_value\n            self._values.add(new_value)\n        return self._map[key]\n\n    def anonymize(self, collection: str, key: str | None) -> str | None:\n        if key is None:\n            return None\n        return self[f\"{collection}:{key}\"]\n", "oasst-shared/oasst_shared/api_client.py": "\"\"\"API Client for interacting with the OASST backend.\"\"\"\nimport enum\nimport typing as t\nfrom http import HTTPStatus\nfrom typing import Optional, Type\nfrom uuid import UUID\n\nimport aiohttp\nfrom loguru import logger\nfrom oasst_shared.exceptions.oasst_api_error import OasstError, OasstErrorCode\nfrom oasst_shared.schemas import protocol as protocol_schema\nfrom pydantic import ValidationError\n\n\n# TODO: Move to `protocol`?\nclass TaskType(str, enum.Enum):\n    \"\"\"Task types.\"\"\"\n\n    summarize_story = \"summarize_story\"\n    rate_summary = \"rate_summary\"\n    initial_prompt = \"initial_prompt\"\n    prompter_reply = \"prompter_reply\"\n    assistant_reply = \"assistant_reply\"\n    rank_initial_prompts = \"rank_initial_prompts\"\n    rank_prompter_replies = \"rank_prompter_replies\"\n    rank_assistant_replies = \"rank_assistant_replies\"\n    label_initial_prompt = \"label_initial_prompt\"\n    label_assistant_reply = \"label_assistant_reply\"\n    label_prompter_reply = \"label_prompter_reply\"\n    done = \"task_done\"\n\n\nclass OasstApiClient:\n    \"\"\"API Client for interacting with the OASST backend.\"\"\"\n\n    def __init__(self, backend_url: str, api_key: str, session: Optional[aiohttp.ClientSession] = None):\n        \"\"\"Create a new OasstApiClient.\n\n        Args:\n        ----\n            backend_url (str): The base backend URL.\n            api_key (str): The API key to use for authentication.\n        \"\"\"\n\n        if session is None:\n            logger.debug(\"Opening OasstApiClient session\")\n            session = aiohttp.ClientSession()\n\n        self.session = session\n        self.backend_url = backend_url\n        self.api_key = api_key\n\n        self.task_models_map: dict[TaskType, Type[protocol_schema.Task]] = {\n            TaskType.summarize_story: protocol_schema.SummarizeStoryTask,\n            TaskType.rate_summary: protocol_schema.RateSummaryTask,\n            TaskType.initial_prompt: protocol_schema.InitialPromptTask,\n            TaskType.prompter_reply: protocol_schema.PrompterReplyTask,\n            TaskType.assistant_reply: protocol_schema.AssistantReplyTask,\n            TaskType.rank_initial_prompts: protocol_schema.RankInitialPromptsTask,\n            TaskType.rank_prompter_replies: protocol_schema.RankPrompterRepliesTask,\n            TaskType.rank_assistant_replies: protocol_schema.RankAssistantRepliesTask,\n            TaskType.label_initial_prompt: protocol_schema.LabelInitialPromptTask,\n            TaskType.label_prompter_reply: protocol_schema.LabelPrompterReplyTask,\n            TaskType.label_assistant_reply: protocol_schema.LabelAssistantReplyTask,\n            TaskType.done: protocol_schema.TaskDone,\n        }\n\n    async def post(self, path: str, data: dict[str, t.Any]) -> Optional[dict[str, t.Any]]:\n        \"\"\"Make a POST request to the backend.\"\"\"\n        logger.debug(f\"POST {self.backend_url}{path} DATA: {data}\")\n        response = await self.session.post(f\"{self.backend_url}{path}\", json=data, headers={\"x-api-key\": self.api_key})\n        logger.debug(f\"response: {response}\")\n\n        # If the response is not a 2XX, check to see\n        # if the json has the fields to create an\n        # OasstError.\n        if response.status >= 300:\n            text = await response.text()\n            logger.debug(f\"resp text: {text}\")\n            data = await response.json()\n            try:\n                oasst_error = protocol_schema.OasstErrorResponse(**(data or {}))\n                raise OasstError(\n                    error_code=oasst_error.error_code,\n                    message=oasst_error.message,\n                )\n            except ValidationError as e:\n                logger.debug(f\"Got error from API but could not parse: {e}\")\n\n                raw_response = await response.text()\n                logger.debug(f\"Raw response: {raw_response}\")\n\n                raise OasstError(\n                    raw_response,\n                    OasstErrorCode.GENERIC_ERROR,\n                    HTTPStatus(response.status),\n                )\n\n        if response.status == 204:\n            # No content\n            return None\n        return await response.json()\n\n    def _parse_task(self, data: Optional[dict[str, t.Any]]) -> protocol_schema.Task:\n        if data is None:\n            raise Exception(\"Cannot parse data as a task: data is none\")\n        task_type = TaskType(data.get(\"type\"))\n\n        model = self.task_models_map.get(task_type)\n        if not model:\n            logger.error(f\"Unsupported task type: {task_type}\")\n            raise ValueError(f\"Unsupported task type: {task_type}\")\n        return self.task_models_map[task_type].parse_obj(data)  # type: ignore\n\n    async def fetch_task(\n        self,\n        task_type: protocol_schema.TaskRequestType,\n        user: Optional[protocol_schema.User] = None,\n        collective: bool = False,\n        lang: Optional[str] = None,\n    ) -> protocol_schema.Task:\n        \"\"\"Fetch a task from the backend.\"\"\"\n        logger.debug(f\"Fetching task {task_type} for user {user}\")\n        req = protocol_schema.TaskRequest(type=task_type.value, user=user, collective=collective, lang=lang)\n        resp = await self.post(\"/api/v1/tasks/\", data=req.dict())\n        logger.debug(f\"RESP {resp}\")\n        return self._parse_task(resp)\n\n    async def fetch_random_task(\n        self, user: Optional[protocol_schema.User] = None, collective: bool = False, lang: Optional[str] = None\n    ) -> protocol_schema.Task:\n        \"\"\"Fetch a random task from the backend.\"\"\"\n        logger.debug(f\"Fetching random for user {user}\")\n        return await self.fetch_task(protocol_schema.TaskRequestType.random, user, collective, lang)\n\n    async def ack_task(self, task_id: str | UUID, message_id: str) -> None:\n        \"\"\"Send an ACK for a task to the backend.\"\"\"\n        logger.debug(f\"ACK task {task_id} with post {message_id}\")\n        req = protocol_schema.TaskAck(message_id=message_id)\n        await self.post(f\"/api/v1/tasks/{task_id}/ack\", data=req.dict())\n\n    async def nack_task(self, task_id: str | UUID, reason: str) -> None:\n        \"\"\"Send a NACK for a task to the backend.\"\"\"\n        logger.debug(f\"NACK task {task_id} with reason {reason}\")\n        req = protocol_schema.TaskNAck(reason=reason)\n        await self.post(f\"/api/v1/tasks/{task_id}/nack\", data=req.dict())\n\n    async def post_interaction(self, interaction: protocol_schema.Interaction) -> protocol_schema.Task:\n        \"\"\"Send a completed task to the backend.\"\"\"\n        logger.debug(f\"Interaction: {interaction}\")\n        resp = await self.post(\"/api/v1/tasks/interaction\", data=interaction.dict())\n        return self._parse_task(resp)\n\n    async def close(self):\n        logger.debug(\"Closing OasstApiClient session\")\n        await self.session.close()\n", "oasst-shared/oasst_shared/model_configs.py": "import pydantic\n\n\nclass ModelConfig(pydantic.BaseModel):\n    model_id: str\n    max_input_length: int = 512\n    max_total_length: int = 1024\n    quantized: bool = False\n\n    @property\n    def is_llama(self) -> bool:\n        return \"llama\" in self.model_id.lower()\n\n    @property\n    def is_lorem(self) -> bool:\n        return self.model_id == \"_lorem\"\n\n    @property\n    def compat_hash(self) -> str:\n        return f\"{self.model_id}-{self.max_total_length}-{self.max_input_length}-{'q' if self.quantized else 'f'}\"\n\n\nMODEL_CONFIGS = {\n    \"_lorem\": ModelConfig(\n        model_id=\"_lorem\",\n        max_input_length=128,\n        max_total_length=256,\n    ),\n    \"distilgpt2\": ModelConfig(\n        model_id=\"distilgpt2\",\n        max_input_length=512,\n        max_total_length=1024,\n    ),\n    \"OA_SFT_Pythia_12B\": ModelConfig(\n        model_id=\"OpenAssistant/oasst-sft-1-pythia-12b\",\n        max_input_length=1024,\n        max_total_length=2048,\n    ),\n    \"OA_SFT_Pythia_12Bq\": ModelConfig(\n        model_id=\"OpenAssistant/oasst-sft-1-pythia-12b\",\n        max_input_length=1024,\n        max_total_length=2048,\n        quantized=True,\n    ),\n    \"OA_SFT_Pythia_12B_4\": ModelConfig(\n        model_id=\"OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5\",\n        max_input_length=1024,\n        max_total_length=2048,\n    ),\n    \"OA_SFT_Pythia_12Bq_4\": ModelConfig(\n        model_id=\"OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5\",\n        max_input_length=1024,\n        max_total_length=2048,\n        quantized=True,\n    ),\n    \"OA_SFT_Llama_7B\": ModelConfig(\n        model_id=\"OpenAssistant/oasst_sft_llama_7b_mask_1000\",\n        max_input_length=1024,\n        max_total_length=2048,\n    ),\n    \"OA_SFT_Llama_13B\": ModelConfig(\n        model_id=\"OpenAssistant/oasst_sft_llama_13b_mask_1500\",\n        max_input_length=1024,\n        max_total_length=2048,\n    ),\n    \"OA_SFT_Llama_13Bq\": ModelConfig(\n        model_id=\"OpenAssistant/oasst_sft_llama_13b_mask_1500\",\n        max_input_length=1024,\n        max_total_length=2048,\n        quantized=True,\n    ),\n    \"OA_SFT_Llama_30B\": ModelConfig(\n        model_id=\"OpenAssistant/llama_30b_oasst_latcyr_1000\",\n        max_input_length=1024,\n        max_total_length=1792,  # seeing OOMs on 2048 on an A100 80GB\n    ),\n    \"OA_SFT_Llama_30Bq\": ModelConfig(\n        model_id=\"OpenAssistant/llama_30b_oasst_latcyr_1000\",\n        max_input_length=1024,\n        max_total_length=1792,  # an a100 40GB can't handle 2048\n        quantized=True,\n    ),\n    \"OA_SFT_Llama_30B_2\": ModelConfig(\n        model_id=\"OpenAssistant/llama_30b_oasst_latcyr_400\",\n        max_input_length=1024,\n        max_total_length=1792,\n    ),\n    \"OA_SFT_Llama_30Bq_2\": ModelConfig(\n        model_id=\"OpenAssistant/llama_30b_oasst_latcyr_400\",\n        max_input_length=1024,\n        max_total_length=1792,  # an a100 40GB can't handle 2048\n        quantized=True,\n    ),\n    \"OA_SFT_Llama_30B_5\": ModelConfig(\n        model_id=\"OpenAssistant/oasst-sft-5-llama-30b-epoch-1\",\n        max_input_length=1024,\n        max_total_length=1792,  # seeing OOMs on 2048 on an A100 80GB\n    ),\n    \"OA_SFT_Llama_30Bq_5\": ModelConfig(\n        model_id=\"OpenAssistant/oasst-sft-5-llama-30b-epoch-1\",\n        max_input_length=1024,\n        max_total_length=1792,  # seeing OOMs on 2048 on an A100 80GB\n        quantized=True,\n    ),\n    \"OA_SFT_Llama_30B_6\": ModelConfig(\n        model_id=\"OpenAssistant/oasst-sft-6-llama-30b\",\n        max_input_length=1024,\n        max_total_length=1792,  # seeing OOMs on 2048 on an A100 80GB\n    ),\n    \"OA_SFT_Llama_30Bq_6\": ModelConfig(\n        model_id=\"OpenAssistant/oasst-sft-6-llama-30b\",\n        max_input_length=1024,\n        max_total_length=1792,  # seeing OOMs on 2048 on an A100 80GB\n        quantized=True,\n    ),\n    \"OA_SFT_Llama_30B_7\": ModelConfig(\n        model_id=\"OpenAssistant/oasst-sft-7-llama-30b\",\n        max_input_length=1024,\n        max_total_length=1792,  # seeing OOMs on 2048 on an A100 80GB\n    ),\n    \"OA_SFT_Llama_30Bq_7\": ModelConfig(\n        model_id=\"OpenAssistant/oasst-sft-7-llama-30b\",\n        max_input_length=1024,\n        max_total_length=1792,  # seeing OOMs on 2048 on an A100 80GB\n        quantized=True,\n    ),\n    \"OA_SFT_Llama_30B_7e3\": ModelConfig(\n        model_id=\"OpenAssistant/oasst-sft-7e3-llama-30b\",\n        max_input_length=1024,\n        max_total_length=1792,  # seeing OOMs on 2048 on an A100 80GB\n    ),\n    \"OA_RLHF_Llama_30B_2_7k\": ModelConfig(\n        model_id=\"OpenAssistant/oasst-rlhf-2-llama-30b-7k-steps\",\n        max_input_length=1024,\n        max_total_length=1792,  # seeing OOMs on 2048 on an A100 80GB\n    ),\n    \"Carper_RLHF_13B_1\": ModelConfig(\n        model_id=\"CarperAI/vicuna-13b-fine-tuned-rlhf\",\n        max_input_length=1024,\n        max_total_length=2048,\n    ),\n    \"Carper_RLHF_13Bq_1\": ModelConfig(\n        model_id=\"CarperAI/vicuna-13b-fine-tuned-rlhf\",\n        max_input_length=1024,\n        max_total_length=2048,\n        quantized=True,\n    ),\n    \"OA_SFT_Llama2_70B_10\": ModelConfig(\n        model_id=\"OpenAssistant/llama2-70b-oasst-sft-v10\",\n        max_input_length=3072,\n        max_total_length=4096,\n    ),\n    \"OA_SFT_CodeLlama_13B_10\": ModelConfig(\n        model_id=\"OpenAssistant/codellama-13b-oasst-sft-v10\",\n        max_input_length=8192,\n        max_total_length=12288,\n    ),\n}\n", "oasst-shared/oasst_shared/__init__.py": "", "oasst-shared/oasst_shared/schemas/protocol.py": "import enum\nfrom datetime import datetime\nfrom typing import List, Literal, Optional, Union\nfrom uuid import UUID, uuid4\n\nimport pydantic\nfrom oasst_shared.exceptions import OasstErrorCode\nfrom pydantic import BaseModel, Field, conint, conlist, constr\n\n\nclass TaskRequestType(str, enum.Enum):\n    random = \"random\"\n    summarize_story = \"summarize_story\"\n    rate_summary = \"rate_summary\"\n    initial_prompt = \"initial_prompt\"\n    prompter_reply = \"prompter_reply\"\n    assistant_reply = \"assistant_reply\"\n    rank_initial_prompts = \"rank_initial_prompts\"\n    rank_prompter_replies = \"rank_prompter_replies\"\n    rank_assistant_replies = \"rank_assistant_replies\"\n    label_initial_prompt = \"label_initial_prompt\"\n    label_assistant_reply = \"label_assistant_reply\"\n    label_prompter_reply = \"label_prompter_reply\"\n\n\nclass User(BaseModel):\n    id: str\n    display_name: str\n    auth_method: Literal[\"discord\", \"google\", \"local\", \"system\"]\n\n\nclass Account(BaseModel):\n    id: UUID\n    provider: str\n    provider_account_id: str\n\n\nclass Token(BaseModel):\n    access_token: str\n    token_type: str\n\n\nclass TokenPair(BaseModel):\n    access_token: Token\n    refresh_token: Token\n\n\nclass FrontEndUser(User):\n    user_id: UUID\n    enabled: bool\n    deleted: bool\n    notes: str\n    created_date: Optional[datetime] = None\n    show_on_leaderboard: bool\n    streak_days: Optional[int] = None\n    streak_last_day_date: Optional[datetime] = None\n    last_activity_date: Optional[datetime] = None\n    tos_acceptance_date: Optional[datetime] = None\n\n\nclass PageResult(BaseModel):\n    prev: str | None\n    next: str | None\n    sort_key: str\n    items: list\n    order: Literal[\"asc\", \"desc\"]\n\n\nclass FrontEndUserPage(PageResult):\n    items: list[FrontEndUser]\n\n\nclass ConversationMessage(BaseModel):\n    \"\"\"Represents a message in a conversation between the user and the assistant.\"\"\"\n\n    id: Optional[UUID]\n    user_id: Optional[UUID]\n    frontend_message_id: Optional[str]\n    text: str\n    lang: Optional[str]  # BCP 47\n    is_assistant: bool\n    emojis: Optional[dict[str, int]]\n    user_emojis: Optional[list[str]]\n    user_is_author: Optional[bool]\n    synthetic: Optional[bool]\n\n\nclass Conversation(BaseModel):\n    \"\"\"Represents a conversation between the prompter and the assistant.\"\"\"\n\n    messages: list[ConversationMessage] = []\n\n    def __len__(self):\n        return len(self.messages)\n\n    @property\n    def is_prompter_turn(self) -> bool:\n        if len(self) == 0:\n            return True\n        last_message = self.messages[-1]\n        if last_message.is_assistant:\n            return True\n        return False\n\n\nclass Message(ConversationMessage):\n    parent_id: Optional[UUID]\n    created_date: Optional[datetime]\n    review_result: Optional[bool]\n    review_count: Optional[int]\n    deleted: Optional[bool]\n    edited: Optional[bool]\n    model_name: Optional[str]\n    message_tree_id: Optional[UUID]\n    ranking_count: Optional[int]\n    rank: Optional[int]\n    user: Optional[FrontEndUser]\n\n\nclass MessageRevision(BaseModel):\n    id: UUID\n    text: str\n    message_id: UUID\n    user_id: Optional[UUID]\n    created_date: Optional[datetime]\n    user_is_author: Optional[bool]\n\n\nclass MessagePage(PageResult):\n    items: list[Message]\n\n\nclass MessageTree(BaseModel):\n    \"\"\"All messages belonging to the same message tree.\"\"\"\n\n    id: UUID\n    messages: list[Message] = []\n\n\nclass TaskRequest(BaseModel):\n    \"\"\"The frontend asks the backend for a task.\"\"\"\n\n    type: TaskRequestType = TaskRequestType.random\n    # Must use Field(..., nullable=True) to indicate to the OpenAPI schema that\n    # this is optional. https://github.com/pydantic/pydantic/issues/1270\n    user: Optional[User] = Field(None, nullable=True)\n    collective: bool = False\n    lang: Optional[str] = Field(None, nullable=True)  # BCP 47\n\n\nclass TaskAck(BaseModel):\n    \"\"\"The frontend acknowledges that it has received a task and created a message.\"\"\"\n\n    message_id: str\n\n\nclass TaskNAck(BaseModel):\n    \"\"\"The frontend acknowledges that it has received a task but cannot create a message.\"\"\"\n\n    reason: str | None = Field(None, nullable=True)\n\n\nclass TaskClose(BaseModel):\n    \"\"\"The frontend asks to mark task as done\"\"\"\n\n    message_id: str\n\n\nclass Task(BaseModel):\n    \"\"\"A task is a unit of work that the backend gives to the frontend.\"\"\"\n\n    id: UUID = pydantic.Field(default_factory=uuid4)\n    type: str\n\n\nclass SummarizeStoryTask(Task):\n    \"\"\"A task to summarize a story.\"\"\"\n\n    type: Literal[\"summarize_story\"] = \"summarize_story\"\n    story: str\n\n\nclass RatingScale(BaseModel):\n    min: int\n    max: int\n\n\nclass AbstractRatingTask(Task):\n    \"\"\"A task to rate something.\"\"\"\n\n    scale: RatingScale = RatingScale(min=1, max=5)\n\n\nclass RateSummaryTask(AbstractRatingTask):\n    \"\"\"A task to rate a summary.\"\"\"\n\n    type: Literal[\"rate_summary\"] = \"rate_summary\"\n    full_text: str\n    summary: str\n\n\nclass WithHintMixin(BaseModel):\n    hint: str | None = None  # provide a hint to the user to spark their imagination\n\n\nclass InitialPromptTask(Task, WithHintMixin):\n    \"\"\"A task to prompt the user to submit an initial prompt to the assistant.\"\"\"\n\n    type: Literal[\"initial_prompt\"] = \"initial_prompt\"\n\n\nclass ReplyToConversationTask(Task):\n    \"\"\"A task to prompt the user to submit a reply to a conversation.\"\"\"\n\n    type: Literal[\"reply_to_conversation\"] = \"reply_to_conversation\"\n    conversation: Conversation  # the conversation so far\n\n\nclass PrompterReplyTask(ReplyToConversationTask, WithHintMixin):\n    \"\"\"A task to prompt the user to submit a reply to the assistant.\"\"\"\n\n    type: Literal[\"prompter_reply\"] = \"prompter_reply\"\n\n\nclass AssistantReplyTask(ReplyToConversationTask):\n    \"\"\"A task to prompt the user to act as the assistant.\"\"\"\n\n    type: Literal[\"assistant_reply\"] = \"assistant_reply\"\n\n\nclass RankInitialPromptsTask(Task):\n    \"\"\"A task to rank a set of initial prompts.\"\"\"\n\n    type: Literal[\"rank_initial_prompts\"] = \"rank_initial_prompts\"\n    prompts: list[str]  # deprecated, use prompt_messages\n    prompt_messages: list[ConversationMessage]\n\n\nclass RankConversationRepliesTask(Task):\n    \"\"\"A task to rank a set of replies to a conversation.\"\"\"\n\n    type: Literal[\"rank_conversation_replies\"] = \"rank_conversation_replies\"\n    conversation: Conversation  # the conversation so far\n    replies: list[str]  # deprecated, use reply_messages\n    reply_messages: list[ConversationMessage]\n    message_tree_id: UUID\n    ranking_parent_id: UUID\n    reveal_synthetic: bool\n\n\nclass RankPrompterRepliesTask(RankConversationRepliesTask):\n    \"\"\"A task to rank a set of prompter replies to a conversation.\"\"\"\n\n    type: Literal[\"rank_prompter_replies\"] = \"rank_prompter_replies\"\n\n\nclass RankAssistantRepliesTask(RankConversationRepliesTask):\n    \"\"\"A task to rank a set of assistant replies to a conversation.\"\"\"\n\n    type: Literal[\"rank_assistant_replies\"] = \"rank_assistant_replies\"\n\n\nclass LabelTaskMode(str, enum.Enum):\n    \"\"\"Label task mode that allows frontends to select an appropriate UI.\"\"\"\n\n    simple = \"simple\"\n    full = \"full\"\n\n\nclass LabelTaskDisposition(str, enum.Enum):\n    \"\"\"Reason why the task was issued.\"\"\"\n\n    quality = \"quality\"\n    spam = \"spam\"\n\n\nclass LabelDescription(BaseModel):\n    name: str\n    widget: str\n    display_text: str\n    help_text: Optional[str]\n\n\nclass AbstractLabelTask(Task):\n    message_id: UUID\n    valid_labels: list[str]\n    mandatory_labels: Optional[list[str]]\n    mode: Optional[LabelTaskMode]\n    disposition: Optional[LabelTaskDisposition]\n    labels: Optional[list[LabelDescription]]\n    conversation: Conversation  # the conversation so far (labeling -> last message)\n\n\nclass LabelInitialPromptTask(AbstractLabelTask):\n    \"\"\"A task to label an initial prompt.\"\"\"\n\n    type: Literal[\"label_initial_prompt\"] = \"label_initial_prompt\"\n    prompt: str | None = Field(None, deprecated=True, description=\"deprecated, use `prompt_message`\")\n\n\nclass LabelConversationReplyTask(AbstractLabelTask):\n    \"\"\"A task to label a reply to a conversation.\"\"\"\n\n    type: Literal[\"label_conversation_reply\"] = \"label_conversation_reply\"\n    reply: str | None = Field(None, deprecated=True, description=\"deprecated, use last message of `conversation`\")\n\n\nclass LabelPrompterReplyTask(LabelConversationReplyTask):\n    \"\"\"A task to label a prompter reply to a conversation.\"\"\"\n\n    type: Literal[\"label_prompter_reply\"] = \"label_prompter_reply\"\n\n\nclass LabelAssistantReplyTask(LabelConversationReplyTask):\n    \"\"\"A task to label an assistant reply to a conversation.\"\"\"\n\n    type: Literal[\"label_assistant_reply\"] = \"label_assistant_reply\"\n\n\nclass TaskDone(Task):\n    \"\"\"Signals to the frontend that the task is done.\"\"\"\n\n    type: Literal[\"task_done\"] = \"task_done\"\n\n\nAnyTask = Union[\n    TaskDone,\n    SummarizeStoryTask,\n    RateSummaryTask,\n    InitialPromptTask,\n    ReplyToConversationTask,\n    PrompterReplyTask,\n    AssistantReplyTask,\n    RankInitialPromptsTask,\n    RankConversationRepliesTask,\n    RankPrompterRepliesTask,\n    RankAssistantRepliesTask,\n    LabelInitialPromptTask,\n    LabelConversationReplyTask,\n    LabelPrompterReplyTask,\n    LabelAssistantReplyTask,\n]\n\n\nclass Interaction(BaseModel):\n    \"\"\"An interaction is a user-generated action in the frontend.\"\"\"\n\n    type: str\n    user: User\n\n\nclass TextReplyToMessage(Interaction):\n    \"\"\"A user has replied to a message with text.\"\"\"\n\n    type: Literal[\"text_reply_to_message\"] = \"text_reply_to_message\"\n    message_id: str\n    user_message_id: str\n    text: constr(min_length=1, strip_whitespace=True)\n    lang: Optional[str]  # BCP 47\n\n\nclass MessageRating(Interaction):\n    \"\"\"A user has rated a message.\"\"\"\n\n    type: Literal[\"message_rating\"] = \"message_rating\"\n    message_id: str\n    rating: conint(gt=0)\n\n\nclass MessageRanking(Interaction):\n    \"\"\"A user has given a ranking for a message.\"\"\"\n\n    type: Literal[\"message_ranking\"] = \"message_ranking\"\n    message_id: str  # parent message of replies that were ranked\n    ranking: conlist(item_type=int, min_items=1)\n    not_rankable: Optional[bool]  # all options flawed, factually incorrect or unacceptable\n\n\nclass LabelWidget(str, enum.Enum):\n    yes_no = \"yes_no\"\n    flag = \"flag\"\n    likert = \"likert\"\n\n\nclass TextLabel(str, enum.Enum):\n    \"\"\"A label for a piece of text.\"\"\"\n\n    def __new__(cls, label: str, widget: LabelWidget, display_text: str = \"\", help_text: str = None):\n        obj = str.__new__(cls, label)\n        obj._value_ = label\n        obj.widget = widget\n        obj.display_text = display_text\n        obj.help_text = help_text\n        return obj\n\n    # yes/no questions\n    spam = \"spam\", LabelWidget.yes_no, \"Seems to be intentionally low-quality or irrelevant\"\n    fails_task = \"fails_task\", LabelWidget.yes_no, \"Fails to follow the correct instruction / task\"\n\n    # flags\n    lang_mismatch = (\n        \"lang_mismatch\",\n        LabelWidget.flag,\n        \"Wrong Language\",\n        \"The message is written in a language that differs from the currently selected language.\",\n    )\n    pii = \"pii\", LabelWidget.flag, \"Contains personal identifiable information (PII)\"\n    not_appropriate = \"not_appropriate\", LabelWidget.flag, \"Inappropriate\"\n    hate_speech = (\n        \"hate_speech\",\n        LabelWidget.flag,\n        \"Content is abusive or threatening and expresses prejudice against a protected characteristic\",\n        \"Prejudice refers to preconceived views not based on reason. Protected characteristics \"\n        \"include gender, ethnicity, religion, sexual orientation, and similar characteristics.\",\n    )\n    sexual_content = \"sexual_content\", LabelWidget.flag, \"Contains sexual content\"\n    moral_judgement = \"moral_judgement\", LabelWidget.flag, \"Expresses moral judgement\"\n    political_content = \"political_content\", LabelWidget.flag, \"Expresses political views\"\n\n    # likert\n    quality = \"quality\", LabelWidget.likert, \"Overall subjective quality rating of the message\"\n    toxicity = \"toxicity\", LabelWidget.likert, \"Rude, abusive, profane or insulting content\"\n    humor = \"humor\", LabelWidget.likert, \"Humorous content including sarcasm\"\n    helpfulness = \"helpfulness\", LabelWidget.likert, \"Helpfulness of the message\"\n    creativity = \"creativity\", LabelWidget.likert, \"Creativity\"\n    violence = \"violence\", LabelWidget.likert, \"Violence/abuse/terrorism/self-harm\"\n\n\nclass TextLabels(Interaction):\n    \"\"\"A set of labels for a piece of text.\"\"\"\n\n    type: Literal[\"text_labels\"] = \"text_labels\"\n    text: str\n    labels: dict[TextLabel, float]\n    message_id: UUID\n    task_id: Optional[UUID]\n    is_report: Optional[bool]\n\n    @property\n    def has_message_id(self) -> bool:\n        \"\"\"Whether this TextLabels has a message_id.\"\"\"\n        return bool(self.message_id)\n\n    # check that each label value is between 0 and 1\n    @pydantic.validator(\"labels\")\n    def check_label_values(cls, v):\n        for key, value in v.items():\n            if not (0 <= value <= 1):\n                raise ValueError(f\"Label values must be between 0 and 1, got {value} for {key}.\")\n        return v\n\n\nAnyInteraction = Union[\n    TextReplyToMessage,\n    MessageRating,\n    MessageRanking,\n    TextLabels,\n]\n\n\nclass SystemStats(BaseModel):\n    all: int = 0\n    active: int = 0\n    active_by_lang: dict[str, int] = {}\n    deleted: int = 0\n    message_trees: int = 0\n\n\nclass UserScore(BaseModel):\n    rank: Optional[int]\n    user_id: UUID\n    highlighted: bool = False\n    username: str\n    auth_method: str\n    display_name: str\n\n    leader_score: int = 0\n    level: int = 0  # between 0 and 100\n\n    base_date: Optional[datetime]\n    modified_date: Optional[datetime]\n\n    prompts: int = 0\n    replies_assistant: int = 0\n    replies_prompter: int = 0\n    labels_simple: int = 0\n    labels_full: int = 0\n    rankings_total: int = 0\n    rankings_good: int = 0\n\n    accepted_prompts: int = 0\n    accepted_replies_assistant: int = 0\n    accepted_replies_prompter: int = 0\n\n    reply_ranked_1: int = 0\n    reply_ranked_2: int = 0\n    reply_ranked_3: int = 0\n\n    streak_last_day_date: Optional[datetime]\n    streak_days: Optional[int]\n    last_activity_date: Optional[datetime]\n\n\nclass LeaderboardStats(BaseModel):\n    time_frame: str\n    last_updated: datetime\n    leaderboard: List[UserScore]\n\n\nclass TrollScore(BaseModel):\n    rank: Optional[int]\n    user_id: UUID\n    highlighted: bool = False\n    username: str\n    auth_method: str\n    display_name: str\n    last_activity_date: Optional[datetime]\n    enabled: bool\n    deleted: bool\n    show_on_leaderboard: bool\n\n    troll_score: int = 0\n\n    base_date: Optional[datetime]\n    modified_date: Optional[datetime]\n\n    red_flags: int = 0  # num reported messages of user\n    upvotes: int = 0  # num up-voted messages of user\n    downvotes: int = 0  # num down-voted messages of user\n\n    spam_prompts: int = 0\n\n    quality: Optional[float] = None\n    humor: Optional[float] = None\n    toxicity: Optional[float] = None\n    violence: Optional[float] = None\n    helpfulness: Optional[float] = None\n\n    spam: int = 0\n    lang_mismach: int = 0\n    not_appropriate: int = 0\n    pii: int = 0\n    hate_speech: int = 0\n    sexual_content: int = 0\n    political_content: int = 0\n\n\nclass TrollboardStats(BaseModel):\n    time_frame: str\n    last_updated: datetime\n    trollboard: List[TrollScore]\n\n\nclass OasstErrorResponse(BaseModel):\n    \"\"\"The format of an error response from the OASST API.\"\"\"\n\n    error_code: OasstErrorCode\n    message: str\n\n\nclass EmojiCode(str, enum.Enum):\n    thumbs_up = \"+1\"  # \ud83d\udc4d\n    thumbs_down = \"-1\"  # \ud83d\udc4e\n    red_flag = \"red_flag\"  # \ud83d\udea9\n    hundred = \"100\"  # \ud83d\udcaf\n    rofl = \"rofl\"  # \ud83e\udd23\n    clap = \"clap\"  # \ud83d\udc4f\n    diamond = \"diamond\"  # \ud83d\udc8e\n    heart_eyes = \"heart_eyes\"  # \ud83d\ude0d\n    disappointed = \"disappointed\"  # \ud83d\ude1e\n    poop = \"poop\"  # \ud83d\udca9\n    skull = \"skull\"  # \ud83d\udc80\n\n    # skip task system uses special emoji codes\n    skip_reply = \"_skip_reply\"\n    skip_ranking = \"_skip_ranking\"\n    skip_labeling = \"_skip_labeling\"\n\n\nclass EmojiOp(str, enum.Enum):\n    togggle = \"toggle\"\n    add = \"add\"\n    remove = \"remove\"\n\n\nclass MessageEmojiRequest(BaseModel):\n    user: User\n    op: EmojiOp = EmojiOp.togggle\n    emoji: EmojiCode\n\n\nclass MessageEditRequest(BaseModel):\n    user: User\n    new_content: str\n\n\nclass CreateFrontendUserRequest(User):\n    show_on_leaderboard: bool = True\n    enabled: bool = True\n    tos_acceptance: Optional[bool] = None\n    notes: Optional[str] = None\n\n\nclass CachedStatsName(str, enum.Enum):\n    human_messages_by_lang = \"human_messages_by_lang\"\n    human_messages_by_role = \"human_messages_by_role\"\n    message_trees_by_state = \"message_trees_by_state\"\n    message_trees_states_by_lang = \"message_trees_states_by_lang\"\n    users_accepted_tos = \"users_accepted_tos\"\n\n\nclass CachedStatsResponse(BaseModel):\n    name: CachedStatsName | str\n    last_updated: datetime\n    stats: dict | list\n\n\nclass AllCachedStatsResponse(BaseModel):\n    stats_by_name: dict[CachedStatsName | str, CachedStatsResponse]\n", "oasst-shared/oasst_shared/schemas/__init__.py": "", "oasst-shared/oasst_shared/schemas/inference.py": "import enum\nimport platform\nimport random\nimport uuid\nfrom datetime import datetime\nfrom typing import Annotated, Literal, Union\n\nimport psutil\nimport pydantic\nimport pynvml\nfrom oasst_shared.model_configs import ModelConfig\n\nINFERENCE_PROTOCOL_VERSION = \"1\"\n\n\nclass WorkerGpuInfo(pydantic.BaseModel):\n    name: str\n    total_memory: int\n\n\nclass WorkerHardwareInfo(pydantic.BaseModel):\n    uname_sysname: str\n    uname_release: str\n    uname_version: str\n    uname_machine: str\n    uname_processor: str\n    cpu_count_physical: int\n    cpu_count_logical: int\n    cpu_freq_max: float\n    cpu_freq_min: float\n    mem_total: int\n    swap_total: int\n    nvidia_driver_version: str | None = None\n    gpus: list[WorkerGpuInfo]\n\n    def __init__(self, **data):\n        data[\"uname_sysname\"] = platform.uname().system\n        data[\"uname_release\"] = platform.uname().release\n        data[\"uname_version\"] = platform.uname().version\n        data[\"uname_machine\"] = platform.uname().machine\n        data[\"uname_processor\"] = platform.uname().processor\n        data[\"cpu_count_physical\"] = psutil.cpu_count(logical=False)\n        data[\"cpu_count_logical\"] = psutil.cpu_count(logical=True)\n        try:\n            data[\"cpu_freq_max\"] = psutil.cpu_freq().max\n            data[\"cpu_freq_min\"] = psutil.cpu_freq().min\n        except Exception:\n            # Workaround for psutil.cpu_freq() throwing exception on some hardware\n            # or sometimes returning `None`. Hardware affected includes Apple Silicon\n            # https://github.com/giampaolo/psutil/issues/1892\n            data[\"cpu_freq_max\"] = 0\n            data[\"cpu_freq_min\"] = 0\n        data[\"mem_total\"] = psutil.virtual_memory().total\n        data[\"swap_total\"] = psutil.swap_memory().total\n        data[\"gpus\"] = []\n        try:\n            pynvml.nvmlInit()\n            data[\"nvidia_driver_version\"] = pynvml.nvmlSystemGetDriverVersion()\n            for i in range(pynvml.nvmlDeviceGetCount()):\n                handle = pynvml.nvmlDeviceGetHandleByIndex(i)\n                name = pynvml.nvmlDeviceGetName(handle)\n                total_memory = pynvml.nvmlDeviceGetMemoryInfo(handle).total\n                data[\"gpus\"].append(WorkerGpuInfo(name=name, total_memory=total_memory))\n        except Exception:\n            pass\n        super().__init__(**data)\n\n\nclass WorkerConfig(pydantic.BaseModel):\n    model_config: ModelConfig\n    max_parallel_requests: int = 1\n\n    @property\n    def compat_hash(self) -> str:\n        return self.model_config.compat_hash\n\n\nclass WorkerInfo(pydantic.BaseModel):\n    config: WorkerConfig\n    hardware_info: WorkerHardwareInfo\n\n\nclass GpuMetricsInfo(pydantic.BaseModel):\n    gpu_usage: float\n    mem_usage: float\n\n\nclass WorkerMetricsInfo(pydantic.BaseModel):\n    created_at: datetime\n    cpu_usage: float\n    mem_usage: float\n    swap_usage: float\n    gpus: list[GpuMetricsInfo] | None = None\n\n    def __init__(self, **data):\n        data[\"created_at\"] = datetime.utcnow()\n        data[\"cpu_usage\"] = psutil.cpu_percent()\n        data[\"mem_usage\"] = psutil.virtual_memory().percent\n        data[\"swap_usage\"] = psutil.swap_memory().percent\n        try:\n            pynvml.nvmlInit()\n            data[\"nvidia_driver_version\"] = pynvml.nvmlSystemGetDriverVersion()\n            gpus = []\n            for i in range(pynvml.nvmlDeviceGetCount()):\n                handle = pynvml.nvmlDeviceGetHandleByIndex(i)\n                gpus.append(\n                    {\n                        \"gpu_usage\": pynvml.nvmlDeviceGetUtilizationRates(handle).gpu,\n                        \"mem_usage\": pynvml.nvmlDeviceGetMemoryInfo(handle).used,\n                    }\n                )\n            data[\"gpus\"] = gpus\n        except Exception:\n            pass\n        super().__init__(**data)\n\n\nclass SamplingParameters(pydantic.BaseModel):\n    top_k: int | None = None\n    top_p: float | None = None\n    typical_p: float | None = None\n    temperature: float | None = None\n    repetition_penalty: float | None = None\n    max_new_tokens: int = 1024\n\n\nclass PluginApiType(pydantic.BaseModel):\n    type: str\n    url: str\n    has_user_authentication: bool | None = False\n    # NOTE: Some plugins using this field,\n    # instead of has_user_authentication\n    is_user_authenticated: bool | None = False\n\n\nclass PluginAuthType(pydantic.BaseModel):\n    type: str\n\n\nclass PluginOpenAPIParameter(pydantic.BaseModel):\n    name: str\n    in_: str\n    description: str\n    required: bool\n    schema_: object\n\n\nclass PluginOpenAPIEndpoint(pydantic.BaseModel):\n    path: str\n    type: str\n    summary: str\n    operation_id: str\n    url: str\n    params: list[PluginOpenAPIParameter]\n    payload: dict | None = None\n\n\nclass PluginConfig(pydantic.BaseModel):\n    schema_version: str\n    name_for_model: str\n    name_for_human: str\n    description_for_human: str\n    description_for_model: str\n    api: PluginApiType\n    auth: PluginAuthType\n    logo_url: str | None = None\n    contact_email: str | None = None\n    legal_info_url: str | None = None\n    endpoints: list[PluginOpenAPIEndpoint] | None = None\n\n\nclass PluginEntry(pydantic.BaseModel):\n    url: str\n    enabled: bool = True\n    plugin_config: PluginConfig | None = None\n    # Idea is for OA internal plugins to be trusted, others untrusted by default\n    trusted: bool | None = False\n\n\nclass PluginExecutionDetails(pydantic.BaseModel):\n    inner_monologue: list[str]\n    final_tool_output: str\n    final_prompt: str\n    final_generation_assisted: bool\n    achieved_depth: int | None = None\n    error_message: str | None = None\n    status: Literal[\"success\", \"failure\"]\n\n\nclass PluginUsed(pydantic.BaseModel):\n    name: str | None = None\n    url: str | None = None\n    trusted: bool | None = None\n    execution_details: PluginExecutionDetails\n\n\ndef make_seed() -> int:\n    return random.randint(0, 0xFFFF_FFFF_FFFF_FFFF - 1)\n\n\nclass WorkParameters(pydantic.BaseModel):\n    model_config: ModelConfig\n    sampling_parameters: SamplingParameters = pydantic.Field(\n        default_factory=SamplingParameters,\n    )\n    do_sample: bool = True\n    seed: int = pydantic.Field(\n        default_factory=make_seed,\n    )\n    system_prompt: str | None = None\n    user_profile: str | None = None\n    user_response_instructions: str | None = None\n    plugins: list[PluginEntry] = pydantic.Field(default_factory=list[PluginEntry])\n    plugin_max_depth: int = 4\n\n\nclass ReportType(str, enum.Enum):\n    spam = \"spam\"\n    offensive = \"offensive\"\n    feeback = \"feedback\"\n\n\nclass Vote(pydantic.BaseModel):\n    id: str\n    score: int\n\n\nclass Report(pydantic.BaseModel):\n    id: str\n    report_type: ReportType\n    reason: str\n\n\nclass MessageState(str, enum.Enum):\n    manual = \"manual\"\n    pending = \"pending\"\n    in_progress = \"in_progress\"\n    complete = \"complete\"\n    aborted_by_worker = \"aborted_by_worker\"\n    cancelled = \"cancelled\"\n    timeout = \"timeout\"\n\n\nclass MessageRead(pydantic.BaseModel):\n    id: str\n    parent_id: str | None\n    content: str | None\n    chat_id: str\n    created_at: datetime\n    role: Literal[\"prompter\", \"assistant\"]\n    state: MessageState\n    score: int\n    reports: list[Report] = []\n    # work parameters will be None on user prompts\n    work_parameters: WorkParameters | None\n    safe_content: str | None\n    safety_level: int | None\n    safety_label: str | None\n    safety_rots: str | None\n    used_plugin: PluginUsed | None = None\n\n    @property\n    def is_assistant(self) -> bool:\n        return self.role == \"assistant\"\n\n\nclass Thread(pydantic.BaseModel):\n    messages: list[MessageRead]\n\n\nclass SafetyParameters(pydantic.BaseModel):\n    level: int = 0\n\n    @pydantic.validator(\"level\")\n    def level_must_be_in_range(cls, v):\n        if v < 0 or v > 9:\n            raise ValueError(\"level must be in range [0, 9]\")\n        return v\n\n\nclass SafetyRequest(pydantic.BaseModel):\n    inputs: str\n    parameters: SafetyParameters\n\n\nclass SafetyResponse(pydantic.BaseModel):\n    outputs: str\n\n\nclass WorkerRequestBase(pydantic.BaseModel):\n    id: str = pydantic.Field(default_factory=lambda: str(uuid.uuid4()))\n\n\nclass WorkRequest(WorkerRequestBase):\n    request_type: Literal[\"work\"] = \"work\"\n    thread: Thread = pydantic.Field(..., repr=False)\n    created_at: datetime = pydantic.Field(default_factory=datetime.utcnow)\n    parameters: WorkParameters = pydantic.Field(default_factory=WorkParameters)\n    safety_parameters: SafetyParameters = pydantic.Field(\n        default_factory=SafetyParameters,\n    )\n\n\nclass PingRequest(WorkerRequestBase):\n    request_type: Literal[\"ping\"] = \"ping\"\n\n\nclass ErrorRequest(WorkerRequestBase):\n    request_type: Literal[\"error\"] = \"error\"\n    error: str\n\n\nclass UpgradeProtocolRequest(WorkerRequestBase):\n    request_type: Literal[\"upgrade_protocol\"] = \"upgrade_protocol\"\n\n\nclass WrongApiKeyRequest(WorkerRequestBase):\n    request_type: Literal[\"wrong_api_key\"] = \"wrong_api_key\"\n\n\nclass TerminateRequest(WorkerRequestBase):\n    request_type: Literal[\"terminate\"] = \"terminate\"\n\n\nclass WorkerResponseBase(pydantic.BaseModel):\n    request_id: str | None = None\n\n\nclass PongResponse(WorkerResponseBase):\n    response_type: Literal[\"pong\"] = \"pong\"\n    metrics: WorkerMetricsInfo | None = None\n\n\nclass SafePromptResponse(WorkerResponseBase):\n    response_type: Literal[\"safe_prompt\"] = \"safe_prompt\"\n    safe_prompt: str\n    safety_parameters: SafetyParameters\n    safety_label: str\n    safety_rots: str\n\n\nclass PluginIntermediateResponse(WorkerResponseBase):\n    response_type: Literal[\"plugin_intermediate\"] = \"plugin_intermediate\"\n    text: str = \"\"\n    current_plugin_thought: str\n    current_plugin_action_taken: str\n    current_plugin_action_input: str\n    current_plugin_action_response: str\n\n\nclass TokenResponse(WorkerResponseBase):\n    response_type: Literal[\"token\"] = \"token\"\n    text: str\n    log_prob: float | None\n    token_id: int\n\n\nclass GeneratedTextResponse(WorkerResponseBase):\n    response_type: Literal[\"generated_text\"] = \"generated_text\"\n    text: str\n    finish_reason: Literal[\"length\", \"eos_token\", \"stop_sequence\"]\n    metrics: WorkerMetricsInfo | None = None\n    used_plugin: PluginUsed | None = None\n\n\nclass InternalFinishedMessageResponse(WorkerResponseBase):\n    response_type: Literal[\"internal_finished_message\"] = \"internal_finished_message\"\n    message: MessageRead\n\n\nclass InternalErrorResponse(WorkerResponseBase):\n    response_type: Literal[\"internal_error\"] = \"internal_error\"\n    error: str\n    message: MessageRead\n\n\nclass ErrorResponse(WorkerResponseBase):\n    response_type: Literal[\"error\"] = \"error\"\n    metrics: WorkerMetricsInfo | None = None\n    error: str\n\n\nclass GeneralErrorResponse(WorkerResponseBase):\n    response_type: Literal[\"general_error\"] = \"general_error\"\n    metrics: WorkerMetricsInfo | None = None\n    error: str\n\n\n_WorkerRequest = Union[\n    WorkRequest,\n    PingRequest,\n    ErrorRequest,\n    TerminateRequest,\n    UpgradeProtocolRequest,\n    WrongApiKeyRequest,\n]\nWorkerRequest = Annotated[\n    _WorkerRequest,\n    pydantic.Field(discriminator=\"request_type\"),\n]\n\nWorkerResponse = Annotated[\n    Union[\n        TokenResponse,\n        GeneratedTextResponse,\n        ErrorResponse,\n        PongResponse,\n        InternalFinishedMessageResponse,\n        InternalErrorResponse,\n        SafePromptResponse,\n        PluginIntermediateResponse,\n    ],\n    pydantic.Field(discriminator=\"response_type\"),\n]\n", "oasst-shared/oasst_shared/exceptions/oasst_api_error.py": "from enum import IntEnum\nfrom http import HTTPStatus\n\n\nclass OasstErrorCode(IntEnum):\n    \"\"\"\n    Error codes of the Open-Assistant backend API.\n\n    Ranges:\n         0-1000: general errors\n      1000-2000: tasks endpoint\n      2000-3000: prompt_repository, task_repository, user_repository\n      3000-4000: external resources\n    \"\"\"\n\n    # 0-1000: general errors\n    GENERIC_ERROR = 0\n    DATABASE_URI_NOT_SET = 1\n    API_CLIENT_NOT_AUTHORIZED = 2\n    ROOT_TOKEN_NOT_AUTHORIZED = 3\n    DATABASE_MAX_RETRIES_EXHAUSTED = 4\n\n    SORT_KEY_UNSUPPORTED = 100\n    INVALID_CURSOR_VALUE = 101\n\n    TOO_MANY_REQUESTS = 429\n\n    SERVER_ERROR0 = 500\n    SERVER_ERROR1 = 501\n\n    INVALID_AUTHENTICATION = 600\n\n    # 1000-2000: tasks endpoint\n    TASK_INVALID_REQUEST_TYPE = 1000\n    TASK_ACK_FAILED = 1001\n    TASK_NACK_FAILED = 1002\n    TASK_INVALID_RESPONSE_TYPE = 1003\n    TASK_INTERACTION_REQUEST_FAILED = 1004\n    TASK_GENERATION_FAILED = 1005\n    TASK_REQUESTED_TYPE_NOT_AVAILABLE = 1006\n    TASK_AVAILABILITY_QUERY_FAILED = 1007\n    TASK_MESSAGE_TOO_LONG = 1008\n    TASK_MESSAGE_DUPLICATED = 1009\n    TASK_MESSAGE_TEXT_EMPTY = 1010\n    TASK_MESSAGE_DUPLICATE_REPLY = 1011\n    TASK_TOO_MANY_PENDING = 1012\n\n    # 2000-3000: prompt_repository\n    INVALID_FRONTEND_MESSAGE_ID = 2000\n    MESSAGE_NOT_FOUND = 2001\n    RATING_OUT_OF_RANGE = 2002\n    INVALID_RANKING_VALUE = 2003\n    INVALID_TASK_TYPE = 2004\n\n    NO_MESSAGE_TREE_FOUND = 2006\n    NO_REPLIES_FOUND = 2007\n    INVALID_MESSAGE = 2008\n    BROKEN_CONVERSATION = 2009\n    TREE_IN_ABORTED_STATE = 2010\n    CORRUPT_RANKING_RESULT = 2011\n    AUTH_AND_USERNAME_REQUIRED = 2012\n\n    TEXT_LABELS_WRONG_MESSAGE_ID = 2050\n    TEXT_LABELS_INVALID_LABEL = 2051\n    TEXT_LABELS_MANDATORY_LABEL_MISSING = 2052\n    TEXT_LABELS_NO_SELF_LABELING = 2053\n    TEXT_LABELS_DUPLICATE_TASK_REPLY = 2053\n\n    TASK_NOT_FOUND = 2100\n    TASK_EXPIRED = 2101\n    TASK_PAYLOAD_TYPE_MISMATCH = 2102\n    TASK_ALREADY_UPDATED = 2103\n    TASK_NOT_ACK = 2104\n    TASK_ALREADY_DONE = 2105\n    TASK_NOT_COLLECTIVE = 2106\n    TASK_NOT_ASSIGNED_TO_USER = 2106\n    TASK_UNEXPECTED_PAYLOAD_TYPE_ = 2107\n\n    # 3000-4000: external resources\n    HUGGINGFACE_API_ERROR = 3001\n\n    # 4000-5000: user\n    USER_NOT_SPECIFIED = 4000\n    USER_DISABLED = 4001\n    USER_NOT_FOUND = 4002\n    USER_HAS_NOT_ACCEPTED_TOS = 4003\n\n    EMOJI_OP_UNSUPPORTED = 5000\n\n    CACHED_STATS_NOT_AVAILABLE = 6000\n\n\nclass OasstError(Exception):\n    \"\"\"Base class for Open-Assistant exceptions.\"\"\"\n\n    message: str\n    error_code: int\n    http_status_code: HTTPStatus\n\n    def __init__(self, message: str, error_code: OasstErrorCode, http_status_code: HTTPStatus = HTTPStatus.BAD_REQUEST):\n        super().__init__(message, error_code, http_status_code)  # make exception picklable (fill args member)\n        self.message = message\n        self.error_code = error_code\n        self.http_status_code = http_status_code\n\n    def __repr__(self) -> str:\n        class_name = self.__class__.__name__\n        return f'{class_name}(message=\"{self.message}\", error_code={self.error_code}, http_status_code={self.http_status_code})'\n", "oasst-shared/oasst_shared/exceptions/__init__.py": "# Ignore unused imports; these are re-exported\nfrom .oasst_api_error import OasstError as OasstError  # noqa: F401\nfrom .oasst_api_error import OasstErrorCode as OasstErrorCode  # noqa: F401\n", "discord-bots/oa-bot-py/message_templates.py": "\"\"\"Message templates for the discord bot.\"\"\"\nimport typing\n\nimport jinja2\nfrom loguru import logger\n\n\nclass MessageTemplates:\n    \"\"\"Create message templates for the discord bot.\"\"\"\n\n    def __init__(self, template_dir: str = \"./templates\"):\n        self.env = jinja2.Environment(  # noqa: S701\n            loader=jinja2.FileSystemLoader(template_dir),\n            autoescape=jinja2.select_autoescape(disabled_extensions=(\"msg\",), default=False, default_for_string=False),\n        )\n\n    def render(self, template_name: str, **kwargs: typing.Any):\n        template = self.env.get_template(template_name)\n        txt = template.render(kwargs)\n        logger.debug(txt)\n\n        return txt\n", "discord-bots/oa-bot-py/bot/settings.py": "\"\"\"Configuration for the bot.\"\"\"\nfrom pydantic import BaseSettings, Field\n\n\nclass Settings(BaseSettings):\n    \"\"\"Settings for the bot.\"\"\"\n\n    bot_token: str = Field(env=\"BOT_TOKEN\", default=\"\")\n    declare_global_commands: int = Field(env=\"DECLARE_GLOBAL_COMMANDS\", default=0)\n    owner_ids: list[int] = Field(env=\"OWNER_IDS\", default_factory=list)\n    prefix: str = Field(env=\"PREFIX\", default=\"/\")\n    oasst_api_url: str = Field(env=\"OASST_API_URL\", default=\"http://localhost:8080\")\n    oasst_api_key: str = Field(env=\"OASST_API_KEY\", default=\"\")\n\n    class Config(BaseSettings.Config):\n        env_file = \".env\"\n        case_sensitive = False\n", "discord-bots/oa-bot-py/bot/utils.py": "\"\"\"Utility functions.\"\"\"\nimport typing as t\nfrom datetime import datetime\n\nimport hikari\n\n\ndef format_time(dt: datetime, fmt: t.Literal[\"t\", \"T\", \"D\", \"f\", \"F\", \"R\"]) -> str:\n    \"\"\"Format a datetime object into the discord time format.\n\n    ```\n    | t | HH:MM            | 16:20\n    | T | HH:MM:SS         | 16:20:11\n    | D | D Mo Yr          | 20 April 2022\n    | f | D Mo Yr HH:MM    | 20 April 2022 16:20\n    | F | W, D Mo Yr HH:MM | Wednesday, 20 April 2022 16:20\n    | R | relative         | in an hour\n    ```\n    \"\"\"\n    match fmt:\n        case \"t\" | \"T\" | \"D\" | \"f\" | \"F\" | \"R\":\n            return f\"<t:{dt.timestamp():.0f}:{fmt}>\"\n        case _:\n            raise ValueError(f\"`fmt` must be 't', 'T', 'D', 'f', 'F' or 'R', not {fmt}\")\n\n\ndef mention(\n    id: hikari.Snowflakeish,\n    type: t.Literal[\"channel\", \"role\", \"user\"],\n) -> str:\n    \"\"\"Mention an object.\"\"\"\n    match type:\n        case \"channel\":\n            return f\"<#{id}>\"\n\n        case \"user\":\n            return f\"<@{id}>\"\n\n        case \"role\":\n            return f\"<@&{id}>\"\n", "discord-bots/oa-bot-py/bot/__main__.py": "\"\"\"Entry point for the bot.\"\"\"\nimport logging\nimport os\n\nfrom bot.bot import bot\nfrom hikari.presences import Activity, ActivityType, Status\n\nlogger = logging.getLogger(__name__)\n\nif __name__ == \"__main__\":\n    if os.name != \"nt\":\n        import uvloop\n\n        uvloop.install()\n\n    logger.info(\"Starting bot\")\n    bot.run(\n        check_for_updates=True,\n        activity=Activity(\n            name=\"/help\",\n            type=ActivityType.PLAYING,\n        ),\n        status=Status.ONLINE,\n    )\n", "discord-bots/oa-bot-py/bot/__init__.py": "\"\"\"The official Open-Assistant Discord Bot.\"\"\"\n", "discord-bots/oa-bot-py/bot/bot.py": "\"\"\"Bot logic.\"\"\"\nfrom datetime import datetime\n\nimport aiosqlite\nimport hikari\nimport lightbulb\nimport miru\nfrom bot.settings import Settings\nfrom bot.utils import mention\nfrom oasst_shared.api_client import OasstApiClient\n\nsettings = Settings()\n\n# TODO: Revisit cache settings\nbot = lightbulb.BotApp(\n    token=settings.bot_token,\n    logs=\"DEBUG\",\n    prefix=settings.prefix,\n    default_enabled_guilds=settings.declare_global_commands,\n    owner_ids=settings.owner_ids,\n    intents=hikari.Intents.ALL,\n    help_class=None,\n)\n\n\n@bot.listen()\nasync def on_starting(event: hikari.StartingEvent):\n    \"\"\"Setup.\"\"\"\n    miru.install(bot)  # component handler\n    bot.load_extensions_from(\"./bot/extensions\")  # load extensions\n\n    # Database setup\n    bot.d.db = await aiosqlite.connect(\"./bot/db/database.db\")\n    await bot.d.db.executescript(open(\"./bot/db/schema.sql\").read())\n    await bot.d.db.commit()\n\n    # OASST API setup\n    bot.d.oasst_api = OasstApiClient(settings.oasst_api_url, settings.oasst_api_key)\n\n    # A `dict[hikari.Message | None, UUID | None]]` that maps user IDs to (task msg ID, task UUIDs).\n    # Either both are `None` or both are not `None`.\n    # If both are `None`, the user is not currently selecting a task.\n    # TODO: Grow this on startup so we don't have to re-allocate memory every time it needs to grow\n    bot.d.currently_working = {}\n\n\n@bot.listen()\nasync def on_stopping(event: hikari.StoppingEvent):\n    \"\"\"Cleanup.\"\"\"\n    await bot.d.db.close()\n    await bot.d.oasst_api.close()\n\n\nasync def _send_error_embed(\n    content: str, exception: lightbulb.errors.LightbulbError | BaseException, ctx: lightbulb.Context\n) -> None:\n    ctx.command\n    embed = hikari.Embed(\n        title=f\"`{exception.__class__.__name__}` Error{f' in `/{ctx.command.name}`' if ctx.command else '' }\",\n        description=content,\n        color=0xFF0000,\n        timestamp=datetime.now().astimezone(),\n    ).set_author(name=ctx.author.username, url=str(ctx.author.avatar_url))\n\n    await ctx.respond(embed=embed)\n\n\n@bot.listen(lightbulb.CommandErrorEvent)\nasync def on_error(event: lightbulb.CommandErrorEvent) -> None:\n    \"\"\"Error handler for the bot.\"\"\"\n    # Unwrap the exception to get the original cause\n    exc = event.exception.__cause__ or event.exception\n    ctx = event.context\n    if not ctx.bot.rest.is_alive:\n        return\n\n    if isinstance(event.exception, lightbulb.CommandInvocationError):\n        if not event.context.command:\n            await _send_error_embed(\"Something went wrong\", exc, ctx)\n        else:\n            await _send_error_embed(\n                f\"Something went wrong during invocation of command `{event.context.command.name}`.\", exc, ctx\n            )\n\n        raise event.exception\n\n    # Not an owner\n    if isinstance(exc, lightbulb.NotOwner):\n        await _send_error_embed(\"You are not the owner of this bot.\", exc, ctx)\n    # Command is on cooldown\n    elif isinstance(exc, lightbulb.CommandIsOnCooldown):\n        await _send_error_embed(f\"This command is on cooldown. Retry in `{exc.retry_after:.2f}` seconds.\", exc, ctx)\n    # Missing permissions\n    elif isinstance(exc, lightbulb.errors.MissingRequiredPermission):\n        await _send_error_embed(\n            f\"You do not have permission to use this command. Missing permissions: {exc.missing_perms}\", exc, ctx\n        )\n    # Missing roles\n    elif isinstance(exc, lightbulb.errors.MissingRequiredRole):\n        assert event.context.guild_id is not None  # Roles only exist in guilds\n        await _send_error_embed(\n            f\"You do not have the correct role to use this command. Missing role(s): {[mention(r, 'role') for r in exc.missing_roles]}\",\n            exc,\n            ctx,\n        )\n    # Only a guild command\n    elif isinstance(exc, lightbulb.errors.OnlyInGuild):\n        await _send_error_embed(\"This command can only be run in servers.\", exc, ctx)\n    # Only a DM command\n    elif isinstance(exc, lightbulb.errors.OnlyInDM):\n        await _send_error_embed(\"This command can only be run in DMs.\", exc, ctx)\n    # Not enough arguments\n    elif isinstance(exc, lightbulb.errors.NotEnoughArguments):\n        await _send_error_embed(\n            f\"Not enough arguments were supplied to the command. {[opt.name for opt in exc.missing_options]}\", exc, ctx\n        )\n    # Bot missing permission\n    elif isinstance(exc, lightbulb.errors.BotMissingRequiredPermission):\n        await _send_error_embed(\n            f\"The bot does not have the correct permission(s) to execute this command. Missing permissions: {exc.missing_perms}\",\n            exc,\n            ctx,\n        )\n    elif isinstance(exc, lightbulb.errors.MissingRequiredAttachment):\n        await _send_error_embed(\"Not enough attachments were supplied to this command.\", exc, ctx)\n    elif isinstance(exc, lightbulb.errors.CommandNotFound):\n        await ctx.respond(f\"`/{exc.invoked_with}` is not a valid command. Use `/help` to see a list of commands.\")\n    else:\n        raise exc\n", "discord-bots/oa-bot-py/bot/messages.py": "\"\"\"All user-facing messages and embeds.\n\nWhen sending a conversation\n- The function will return a list of strings\n    - use asyncio.gather to send all messages\n\n-\n\"\"\"\n\nfrom datetime import datetime\n\nimport hikari\nfrom oasst_shared.schemas import protocol as protocol_schema\n\nNUMBER_EMOJIS = [\":one:\", \":two:\", \":three:\", \":four:\", \":five:\", \":six:\", \":seven:\", \":eight:\", \":nine:\", \":ten:\"]\nNL = \"\\n\"\n\n###\n# Reusable 'components'\n###\n\n\ndef _h1(text: str) -> str:\n    return f\"\\n:small_blue_diamond: __**{text}**__ :small_blue_diamond:\"\n\n\ndef _h2(text: str) -> str:\n    return f\"__**{text}**__\"\n\n\ndef _h3(text: str) -> str:\n    return f\"__{text}__\"\n\n\ndef _writing_prompt(text: str) -> str:\n    return f\":pencil: _{text}_\"\n\n\ndef _ranking_prompt(text: str) -> str:\n    return f\":trophy: _{text}_\"\n\n\ndef _label_prompt(text: str, mandatory_label: list[str] | None, valid_labels: list[str]) -> str:\n    return f\"\"\":question: _{text}_\nMandatory labels: {\", \".join(mandatory_label) if mandatory_label is not None else \"None\"}\nValid labels: {\", \".join(valid_labels)}\n\"\"\"\n\n\ndef _response_prompt(text: str) -> str:\n    return f\":speech_balloon: _{text}_\"\n\n\ndef _summarize_prompt(text: str) -> str:\n    return f\":notepad_spiral: _{text}_\"\n\n\ndef _user(text: str | None) -> str:\n    return f\"\"\"\\\n:person_red_hair: {_h3(\"User\")}:{f\"{NL}> **{text}**\" if text is not None else \"\"}\n\"\"\"\n\n\ndef _assistant(text: str | None) -> str:\n    return f\"\"\"\\\n:robot: {_h3(\"Assistant\")}:{f\"{NL}> {text}\" if text is not None else \"\"}\n\"\"\"\n\n\ndef _make_ordered_list(items: list[protocol_schema.ConversationMessage]) -> list[str]:\n    return [f\"{num} {item.text}\" for num, item in zip(NUMBER_EMOJIS, items)]\n\n\ndef _ordered_list(items: list[protocol_schema.ConversationMessage]) -> str:\n    return \"\\n\\n\".join(_make_ordered_list(items))\n\n\ndef _conversation(conv: protocol_schema.Conversation) -> list[str]:\n    # return \"\\n\".join([_assistant(msg.text) if msg.is_assistant else _user(msg.text) for msg in conv.messages])\n    messages = map(\n        lambda m: f\"\"\"\\\n:robot: __Assistant__:\n{m.text}\n\"\"\"\n        if m.is_assistant\n        else f\"\"\"\\\n:person_red_hair: __User__:\n{m.text}\n\"\"\",\n        conv.messages,\n    )\n    return list(messages)\n\n\ndef _li(text: str) -> str:\n    return f\":small_blue_diamond: {text}\"\n\n\n###\n# Messages\n###\n\n\ndef initial_prompt_messages(task: protocol_schema.InitialPromptTask) -> list[str]:\n    \"\"\"Creates the message that gets sent to users when they request an `initial_prompt` task.\"\"\"\n    return [\n        f\"\"\"\\\n\n:small_blue_diamond: __**INITIAL PROMPT**__ :small_blue_diamond:\n\n\n:pencil: _Please provide an initial prompt to the assistant._{f\"{NL}Hint: {task.hint}\" if task.hint else \"\"}\n\"\"\"\n    ]\n\n\ndef rank_initial_prompts_messages(task: protocol_schema.RankInitialPromptsTask) -> list[str]:\n    \"\"\"Creates the message that gets sent to users when they request a `rank_initial_prompts` task.\"\"\"\n    return [\n        f\"\"\"\\\n\n:small_blue_diamond: __**RANK INITIAL PROMPTS**__ :small_blue_diamond:\n\n\n{_ordered_list(task.prompt_messages)}\n\n:trophy: _Reply with the numbers of best to worst prompts separated by commas (example: '4,1,3,2')_\n\"\"\"\n    ]\n\n\ndef rank_prompter_reply_messages(task: protocol_schema.RankPrompterRepliesTask) -> list[str]:\n    \"\"\"Creates the message that gets sent to users when they request a `rank_prompter_replies` task.\"\"\"\n    return [\n        \"\"\"\\\n\n:small_blue_diamond: __**RANK PROMPTER REPLIES**__ :small_blue_diamond:\n\n\"\"\",\n        *_conversation(task.conversation),\n        f\"\"\":person_red_hair: __User__:\n{_ordered_list(task.reply_messages)}\n\n:trophy: _Reply with the numbers of best to worst replies separated by commas (example: '4,1,3,2')_\n\"\"\",\n    ]\n\n\ndef rank_assistant_reply_message(task: protocol_schema.RankAssistantRepliesTask) -> list[str]:\n    \"\"\"Creates the message that gets sent to users when they request a `rank_assistant_replies` task.\"\"\"\n    return [\n        \"\"\"\\\n\n:small_blue_diamond: __**RANK ASSISTANT REPLIES**__ :small_blue_diamond:\n\n\"\"\",\n        *_conversation(task.conversation),\n        f\"\"\":robot: __Assistant__:,\n{_ordered_list(task.reply_messages)}\n:trophy: _Reply with the numbers of best to worst replies separated by commas (example: '4,1,3,2')_\n\"\"\",\n    ]\n\n\ndef rank_conversation_reply_messages(task: protocol_schema.RankConversationRepliesTask) -> list[str]:\n    \"\"\"Creates the message that gets sent to users when they request a `rank_conversation_replies` task.\"\"\"\n    return [\n        \"\"\"\\\n\n:small_blue_diamond: __**RANK CONVERSATION REPLIES**__ :small_blue_diamond:\n\n\"\"\",\n        *_conversation(task.conversation),\n        f\"\"\":person_red_hair: __User__:\n{_ordered_list(task.reply_messages)}\n\"\"\",\n    ]\n\n\ndef label_initial_prompt_message(task: protocol_schema.LabelInitialPromptTask) -> str:\n    \"\"\"Creates the message that gets sent to users when they request a `label_initial_prompt` task.\"\"\"\n    return f\"\"\"\\\n\n{_h1(\"LABEL INITIAL PROMPT\")}\n\n\n{task.prompt}\n\n{_label_prompt(\"Reply with labels for the prompt separated by commas (example: 'profanity,misleading')\", task.mandatory_labels, task.valid_labels)}\n\"\"\"\n\n\ndef label_prompter_reply_messages(task: protocol_schema.LabelPrompterReplyTask) -> list[str]:\n    \"\"\"Creates the message that gets sent to users when they request a `label_prompter_reply` task.\"\"\"\n    return [\n        f\"\"\"\\\n\n{_h1(\"LABEL PROMPTER REPLY\")}\n\n\n\"\"\",\n        *_conversation(task.conversation),\n        f\"\"\"{_user(None)}\n{task.reply}\n\n{_label_prompt(\"Reply with labels for the reply separated by commas (example: 'profanity,misleading')\", task.mandatory_labels, task.valid_labels)}\n\"\"\",\n    ]\n\n\ndef label_assistant_reply_messages(task: protocol_schema.LabelAssistantReplyTask) -> list[str]:\n    \"\"\"Creates the message that gets sent to users when they request a `label_assistant_reply` task.\"\"\"\n    return [\n        f\"\"\"\\\n\n{_h1(\"LABEL ASSISTANT REPLY\")}\n\n\n\"\"\",\n        *_conversation(task.conversation),\n        f\"\"\"\n{_assistant(None)}\n{task.reply}\n\n{_label_prompt(\"Reply with labels for the reply separated by commas (example: 'profanity,misleading')\", task.mandatory_labels, task.valid_labels)}\n\"\"\",\n    ]\n\n\ndef prompter_reply_messages(task: protocol_schema.PrompterReplyTask) -> list[str]:\n    \"\"\"Creates the message that gets sent to users when they request a `prompter_reply` task.\"\"\"\n    return [\n        \"\"\"\\\n:small_blue_diamond: __**PROMPTER REPLY**__ :small_blue_diamond:\n\n\"\"\",\n        *_conversation(task.conversation),\n        f\"\"\"{f\"{NL}Hint: {task.hint}\" if task.hint else \"\"}\n\n:speech_balloon: _Please provide a reply to the assistant._\n\"\"\",\n    ]\n\n\n# def prompter_reply_messages2(task: protocol_schema.PrompterReplyTask) -> list[str]:\n#     \"\"\"Creates the message that gets sent to users when they request a `prompter_reply` task.\"\"\"\n#     return [\n#         message_templates.render(\"title.msg\", \"PROMPTER REPLY\"),\n#         *[message_templates.render(\"conversation_message.msg\", conv) for conv in task.conversation],\n#         message_templates.render(\"prompter_reply_task.msg\", task.hint),\n#     ]\n\n\ndef assistant_reply_messages(task: protocol_schema.AssistantReplyTask) -> list[str]:\n    \"\"\"Creates the message that gets sent to users when they request a `assistant_reply` task.\"\"\"\n    return [\n        \"\"\"\\\n:small_blue_diamond: __**ASSISTANT REPLY**__ :small_blue_diamond:\n\n\"\"\",\n        *_conversation(task.conversation),\n        \"\"\"\\\n\n:speech_balloon: _Please provide a reply to the user as the assistant._\n\"\"\",\n    ]\n\n\ndef confirm_text_response_message(content: str) -> str:\n    return f\"\"\"\\\n{_h2(\"CONFIRM RESPONSE\")}\n\n> {content}\n\"\"\"\n\n\ndef confirm_ranking_response_message(content: str, items: list[protocol_schema.ConversationMessage]) -> str:\n    user_rankings = [int(r) for r in content.replace(\" \", \"\").split(\",\")]\n    original_list = _make_ordered_list(items)\n    user_ranked_list = \"\\n\\n\".join([original_list[r - 1] for r in user_rankings])\n\n    return f\"\"\"\\\n{_h2(\"CONFIRM RESPONSE\")}\n\n{user_ranked_list}\n\"\"\"\n\n\ndef help_message(can_manage_guild: bool, is_dev: bool) -> str:\n    \"\"\"The /help command message.\"\"\"\n    content = f\"\"\"\\\n{_h1(\"HELP\")}\n\n{_li(\"**`/help`**\")}\nShow this message.\n\n{_li(\"**`/work [type]`**\")}\nStart a new task.\n**`[type]`**:\nThe type of task to start. If not provided, a random task will be selected. The different types are\n:small_orange_diamond: `random`: A random task type\n:small_orange_diamond: ~~`summarize_story`~~ (coming soon)\n:small_orange_diamond: ~~`rate_summary`~~ (coming soon)\n:small_orange_diamond: `initial_prompt`: Ask the assistant something\n:small_orange_diamond: `prompter_reply`: Reply to the assistant\n:small_orange_diamond: `assistant_reply`: Reply to the user\n:small_orange_diamond: `rank_initial_prompts`: Rank some initial prompts\n:small_orange_diamond: `rank_prompter_replies`: Rank some prompter replies\n:small_orange_diamond: `rank_assistant_replies`: Rank some assistant replies\n\nTo learn how to complete tasks, run `/tutorial`.\n\"\"\"\n    if can_manage_guild:\n        content += f\"\"\"\\\n\n{_li(\"**`/settings log_channel <channel>`**\")}\nSet the channel that the bot logs completed task messages in.\n**`<channel>`**: The channel to log completed tasks in. The bot needs to be able to send messages in this channel.\n\n{_li(\"**`/settings get`**\")}\nGet the current settings.\n\"\"\"\n    if is_dev:\n        content += f\"\"\"\\\n\n{_li(\"**`/reload [plugin]`**\")}\nHot-reload a plugin. Only code *inside* of function bodies will be updated.\nAny changes to __function signatures__, __other files__, __decorators__, or __imports__ will require a restart.\n**`[plugin]`**:\nThe plugin to hot-reload. If no plugin is provided, all plugins are hot-reload.\n\"\"\"\n    return content\n\n\ndef tutorial_message() -> str:\n    \"\"\"The /tutorial command message.\"\"\"\n    # TODO: Finish message\n    return f\"\"\"\\\n{_h1(\"TUTORIAL\")}\n\"\"\"\n\n\ndef confirm_label_response_message(content: str) -> str:\n    user_labels = content.lower().replace(\" \", \"\").split(\",\")\n    user_labels_str = \", \".join(user_labels)\n\n    return f\"\"\"\\\n{_h2(\"CONFIRM RESPONSE\")}\n\n{user_labels_str}\n\"\"\"\n\n\n###\n# Embeds\n###\n\n\ndef task_complete_embed(task: protocol_schema.Task, mention: str) -> hikari.Embed:\n    return (\n        hikari.Embed(\n            title=\"Task Completion\",\n            description=f\"`{task.type}` completed by {mention}\",\n            color=hikari.Color(0x00FF00),\n            timestamp=datetime.now().astimezone(),\n        )\n        .add_field(\"Total Tasks\", \"0\", inline=True)\n        .add_field(\"Server Ranking\", \"0/0\", inline=True)\n        .add_field(\"Global Ranking\", \"0/0\", inline=True)\n        .set_footer(f\"Task ID: {task.id}\")\n    )\n\n\ndef invalid_user_input_embed(error_message: str) -> hikari.Embed:\n    return hikari.Embed(\n        title=\"Invalid User Input\",\n        description=error_message,\n        color=hikari.Color(0xFF0000),\n        timestamp=datetime.now().astimezone(),\n    )\n\n\ndef plain_embed(text: str) -> hikari.Embed:\n    return hikari.Embed(color=0x36393F, description=text)\n", "discord-bots/oa-bot-py/bot/db/schemas.py": "\"\"\"Database schemas.\"\"\"\nimport typing as t\n\nfrom aiosqlite import Connection, Row\nfrom pydantic import BaseModel\n\n\nclass GuildSettings(BaseModel):\n    \"\"\"Guild settings.\"\"\"\n\n    guild_id: int\n    log_channel_id: int | None\n\n    @classmethod\n    def parse_obj(cls, obj: Row) -> \"GuildSettings\":\n        \"\"\"Deserialize a Row object from aiosqlite into a GuildSettings object.\"\"\"\n        return cls(guild_id=obj[0], log_channel_id=obj[1])\n\n    @classmethod\n    async def from_db(cls, conn: Connection, guild_id: int) -> t.Optional[\"GuildSettings\"]:\n        async with conn.cursor() as cursor:\n            await cursor.execute(\"SELECT * FROM guild_settings WHERE guild_id = ?\", (guild_id,))\n            row = await cursor.fetchone()\n            if row is None:\n                return None\n\n            return cls.parse_obj(row)\n", "discord-bots/oa-bot-py/bot/extensions/hot_reload.py": "\"\"\"Hot reload plugin.\"\"\"\nfrom glob import glob\n\nimport hikari\nimport lightbulb\nfrom loguru import logger\n\nplugin = lightbulb.Plugin(\n    \"HotReloadPlugin\",\n)\nplugin.add_checks(lightbulb.owner_only)\n\nEXTENSIONS_FOLDER = \"bot/extensions\"\n\n\ndef _get_extensions() -> list[str]:\n    # Recursively get all the .py files in the extensions directory not starting with an `_`.\n    exts = glob(\"bot/extensions/**/[!_]*.py\", recursive=True)\n    # Turn the path into a plugin path (\"path/to/extension.py\" -> \"path.to.extension\")\n    return [ext.replace(\"/\", \".\").replace(\"\\\\\", \".\").replace(\".py\", \"\") for ext in exts]\n\n\nasync def _plugin_autocomplete(option: hikari.CommandInteractionOption, _: hikari.AutocompleteInteraction) -> list[str]:\n    # Check that the option is a string.\n    if not isinstance(option.value, str):\n        raise TypeError(f\"`option.value` must be of type `str`, it is currently a `{type(option.value)}`\")\n\n    exts = _get_extensions()\n    return [ext for ext in exts if option.value in ext]\n\n\n@plugin.command\n@lightbulb.option(\n    \"plugin\",\n    \"The plugin to reload. Leave empty to reload all plugins.\",\n    autocomplete=_plugin_autocomplete,\n    required=False,\n    default=None,\n)\n@lightbulb.command(\"reload\", \"Reload a plugin\", ephemeral=True)\n@lightbulb.implements(lightbulb.SlashCommand)\nasync def reload(ctx: lightbulb.SlashContext):\n    \"\"\"Reload a plugin or all plugins.\"\"\"\n    # If the plugin option is None, reload all plugins.\n    if ctx.options.plugin is None:\n        ctx.bot.reload_extensions(*_get_extensions())\n        await ctx.respond(\"Reloaded all plugins.\")\n        logger.info(\"Reloaded all plugins.\")\n    # Otherwise, reload the specified plugin.\n    else:\n        ctx.bot.reload_extensions(ctx.options.plugin)\n        await ctx.respond(f\"Reloaded `{ctx.options.plugin}`.\")\n        logger.info(f\"Reloaded `{ctx.options.plugin}`.\")\n\n\ndef load(bot: lightbulb.BotApp):\n    \"\"\"Add the plugin to the bot.\"\"\"\n    bot.add_plugin(plugin)\n\n\ndef unload(bot: lightbulb.BotApp):\n    \"\"\"Remove the plugin to the bot.\"\"\"\n    bot.remove_plugin(plugin)\n", "discord-bots/oa-bot-py/bot/extensions/text_labels.py": "\"\"\"Hot reload plugin.\"\"\"\nimport typing as t\nfrom datetime import datetime\n\nimport hikari\nimport lightbulb\nimport miru\nfrom aiosqlite import Connection\nfrom bot.db.schemas import GuildSettings\nfrom loguru import logger\n\nplugin = lightbulb.Plugin(\n    \"TextLabels\",\n)\nplugin.add_checks(lightbulb.guild_only)  # Context menus are only enabled in guilds\n\n\nDISCORD_GRAY = 0x2F3136\n\n\ndef clamp(num: float) -> float:\n    \"\"\"Clamp a number between 0 and 1.\"\"\"\n    return min(max(0.0, num), 1.0)\n\n\nclass LabelModal(miru.Modal):\n    \"\"\"Modal for submitting text labels.\"\"\"\n\n    def __init__(self, label: str, content: str, *args: t.Any, **kwargs: t.Any):\n        super().__init__(*args, **kwargs)\n        self.label = label\n        self.original_content = content\n\n        # Add the text of the message to the modal\n        self.content = miru.TextInput(\n            label=\"Text\", style=hikari.TextInputStyle.PARAGRAPH, value=content, required=True, row=1\n        )\n        self.add_item(self.content)\n\n    value = miru.TextInput(label=\"Value\", placeholder=\"Enter a value between 0 and 1\", required=True, row=2)\n\n    async def callback(self, context: miru.ModalContext) -> None:\n        val = float(self.value.value) if self.value.value else 0.0\n        val = clamp(val)\n\n        edited = self.content.value != self.original_content\n        await context.respond(\n            f\"Sending {self.label}=`{val}` for `{self.content.value}` (edited={edited}) to the backend.\",\n            flags=hikari.MessageFlag.EPHEMERAL,\n        )\n        logger.info(f\"Sending {self.label}=`{val}` for `{self.content.value}` (edited={edited}) to the backend.\")\n\n        # Send a notification to the log channel\n        assert context.guild_id is not None  # `guild_only` check\n        conn: Connection = context.bot.d.db  # type: ignore\n        guild_settings = await GuildSettings.from_db(conn, context.guild_id)\n\n        if guild_settings is None or guild_settings.log_channel_id is None:\n            logger.warning(f\"No guild settings or log channel for guild {context.guild_id}\")\n            return\n\n        embed = (\n            hikari.Embed(\n                title=\"Message Label\",\n                description=f\"{context.author.mention} labeled a message as `{self.label}`.\",\n                timestamp=datetime.now().astimezone(),\n                color=0x00FF00,\n            )\n            .set_author(name=context.author.username, icon=context.author.avatar_url)\n            .add_field(\"Total Labeled Message\", \"0\", inline=True)\n            .add_field(\"Server Ranking\", \"0/0\", inline=True)\n            .add_field(\"Global Ranking\", \"0/0\", inline=True)\n        )\n        channel = await context.bot.rest.fetch_channel(guild_settings.log_channel_id)\n        assert isinstance(channel, hikari.TextableChannel)\n        await channel.send(embed=embed)\n\n\nclass LabelSelect(miru.View):\n    \"\"\"Select menu for selecting a label.\n\n    The current labels are:\n    - contains toxic language\n    - encourages illegal activity\n    - good quality\n    - bad quality\n    - is spam\n    \"\"\"\n\n    def __init__(self, content: str, *args: t.Any, **kwargs: t.Any):\n        super().__init__(*args, **kwargs)\n        self.content = content\n\n    @miru.select(\n        options=[\n            hikari.SelectMenuOption(\n                label=\"Toxic Language\",\n                value=\"toxic_language\",\n                description=\"The message contains toxic language.\",\n                is_default=False,\n                emoji=None,\n            ),\n            hikari.SelectMenuOption(\n                label=\"Illegal Activity\",\n                value=\"illegal_activity\",\n                description=\"The message encourages illegal activity.\",\n                is_default=False,\n                emoji=None,\n            ),\n            hikari.SelectMenuOption(\n                label=\"Good Quality\",\n                value=\"good_quality\",\n                description=\"The message is good quality.\",\n                is_default=False,\n                emoji=None,\n            ),\n            hikari.SelectMenuOption(\n                label=\"Bad Quality\",\n                value=\"bad_quality\",\n                description=\"The message is bad quality.\",\n                is_default=False,\n                emoji=None,\n            ),\n            hikari.SelectMenuOption(\n                label=\"Spam\",\n                value=\"spam\",\n                description=\"The message is spam.\",\n                is_default=False,\n                emoji=None,\n            ),\n        ],\n        min_values=1,\n        max_values=1,\n    )\n    async def label_select(self, select: miru.Select, ctx: miru.ViewContext) -> None:\n        \"\"\"Handle the select menu.\"\"\"\n        label = select.values[0]\n        modal = LabelModal(label, self.content, title=f\"Text Label: {label}\", timeout=60)\n        await modal.send(ctx.interaction)\n        await modal.wait()\n\n        self.stop()\n\n\n@plugin.command\n@lightbulb.command(\"Label Message\", \"Label a message\")\n@lightbulb.implements(lightbulb.MessageCommand)\nasync def label_message_text(ctx: lightbulb.MessageContext):\n    \"\"\"Label a message.\"\"\"\n    # We have to do some funny interaction chaining because discord only allows one component (select or modal) per interaction\n    # so the select menu will open the modal\n\n    msg: hikari.Message = ctx.options.target\n    # Exit if the message is empty\n    if not msg.content:\n        await ctx.respond(\"Cannot label an empty message.\", flags=hikari.MessageFlag.EPHEMERAL)\n        return\n\n    # Send the select menu\n    # The modal will be opened from the select menu interaction\n    embed = hikari.Embed(title=\"Label Message\", description=\"Select a label for the message.\", color=DISCORD_GRAY)\n    label_select_view = LabelSelect(\n        msg.content,\n        timeout=60,\n    )\n    resp = await ctx.respond(embed=embed, components=label_select_view, flags=hikari.MessageFlag.EPHEMERAL)\n\n    await label_select_view.start(await resp.message())\n    await label_select_view.wait()\n\n\ndef load(bot: lightbulb.BotApp):\n    \"\"\"Add the plugin to the bot.\"\"\"\n    bot.add_plugin(plugin)\n\n\ndef unload(bot: lightbulb.BotApp):\n    \"\"\"Remove the plugin to the bot.\"\"\"\n    bot.remove_plugin(plugin)\n", "discord-bots/oa-bot-py/bot/extensions/guild_settings.py": "\"\"\"Guild settings.\"\"\"\nimport hikari\nimport lightbulb\nfrom aiosqlite import Connection\nfrom bot.db.schemas import GuildSettings\nfrom bot.utils import mention\nfrom lightbulb.utils import permissions_in\nfrom loguru import logger\n\nplugin = lightbulb.Plugin(\"GuildSettings\")\nplugin.add_checks(lightbulb.guild_only)\nplugin.add_checks(lightbulb.has_guild_permissions(hikari.Permissions.MANAGE_GUILD))\n\n\n@plugin.command\n@lightbulb.command(\"settings\", \"Bot settings for the server.\")\n@lightbulb.implements(lightbulb.SlashCommandGroup)\nasync def settings(_: lightbulb.SlashContext) -> None:\n    \"\"\"Bot settings for the server.\"\"\"\n    # This will never execute because it is a group\n    pass\n\n\n@settings.child\n@lightbulb.command(\"get\", \"Get all the guild settings.\")\n@lightbulb.implements(lightbulb.SlashSubCommand)\nasync def get(ctx: lightbulb.SlashContext) -> None:\n    \"\"\"Get one of or all the guild settings.\"\"\"\n    conn: Connection = ctx.bot.d.db\n    assert ctx.guild_id is not None  # `guild_only` check\n\n    async with conn.cursor() as cursor:\n        # Get all settings\n        await cursor.execute(\"SELECT * FROM guild_settings WHERE guild_id = ?\", (ctx.guild_id,))\n        row = await cursor.fetchone()\n\n        if row is None:\n            logger.warning(f\"No guild settings for {ctx.guild_id}\")\n            await ctx.respond(\"No settings found for this guild.\")\n            return\n\n        guild_settings = GuildSettings.parse_obj(row)\n\n        # Respond with all\n        # TODO: Embed\n        await ctx.respond(\n            f\"\"\"\\\n**Guild Settings**\n`log_channel`: {\nmention(guild_settings.log_channel_id, \"channel\")\nif guild_settings.log_channel_id else 'not set'}\n\"\"\"\n        )\n\n\n@settings.child\n@lightbulb.option(\"channel\", \"The channel to use.\", hikari.TextableGuildChannel)\n@lightbulb.command(\"log_channel\", \"Set the channel that the bot logs task and label completions in.\", ephemeral=True)\n@lightbulb.implements(lightbulb.SlashSubCommand)\nasync def log_channel(ctx: lightbulb.SlashContext) -> None:\n    \"\"\"Set the channel that the bot logs task and label completions in.\"\"\"\n    channel: hikari.TextableGuildChannel = ctx.options.channel\n    conn: Connection = ctx.bot.d.db\n    assert ctx.guild_id is not None  # `guild_only` check\n\n    # Check if the bot can send messages in that channel\n    assert isinstance(channel, hikari.InteractionChannel)  # Slash commands are interactions\n    me = ctx.bot.cache.get_me() or await ctx.bot.rest.fetch_my_user()\n    own_member = ctx.bot.cache.get_member(ctx.guild_id, me.id) or await ctx.bot.rest.fetch_member(ctx.guild_id, me.id)\n\n    # Get the channel from the cache if it is there, otherwise fetch it\n    if (ch := ctx.bot.cache.get_guild_channel(channel.id)) is None:\n        ch = {ch.id: ch for ch in await ctx.bot.rest.fetch_guild_channels(channel.id)}[channel.id]\n\n    if not isinstance(ch, hikari.GuildTextChannel):\n        await ctx.respond(f\"{ch.mention} is not a text channel.\")\n        return\n\n    # if the bot's permissions for this channel don't contain SEND_MESSAGE\n    # This will also filter out categories and voice channels\n    if not permissions_in(ch, own_member) & hikari.Permissions.SEND_MESSAGES:\n        await ctx.respond(f\"I don't have permission to send messages in {ch.mention}.\")\n        return\n\n    await ctx.respond(f\"Setting `log_channel` to {channel.mention}.\")\n\n    # update the database\n    async with conn.cursor() as cursor:\n        await cursor.execute(\n            \"INSERT OR REPLACE INTO guild_settings (guild_id, log_channel_id) VALUES (?, ?)\",\n            (ctx.guild_id, channel.id),\n        )\n    await conn.commit()\n    logger.info(f\"Updated `log_channel` for {ctx.guild_id} to {channel.id}.\")\n\n\ndef load(bot: lightbulb.BotApp):\n    \"\"\"Add the plugin to the bot.\"\"\"\n    bot.add_plugin(plugin)\n\n\ndef unload(bot: lightbulb.BotApp):\n    \"\"\"Remove the plugin to the bot.\"\"\"\n    bot.remove_plugin(plugin)\n", "discord-bots/oa-bot-py/bot/extensions/help.py": "\"\"\"Custom help command.\"\"\"\nimport lightbulb\nfrom bot.messages import help_message, tutorial_message\nfrom bot.settings import Settings\nfrom hikari.permissions import Permissions\nfrom lightbulb.utils import permissions_for\n\nplugin = lightbulb.Plugin(\"HelpPlugin\")\n\nsettings = Settings()\n\n\n@plugin.command\n@lightbulb.command(\"help\", \"Help for the bot.\", ephemeral=True)\n@lightbulb.implements(lightbulb.SlashCommand, lightbulb.PrefixCommand)\nasync def help_command(ctx: lightbulb.Context) -> None:\n    \"\"\"Help for the bot.\"\"\"\n    can_manage_guild = False\n    if ctx.guild_id:\n        member = ctx.bot.cache.get_member(ctx.guild_id, ctx.author.id) or await ctx.bot.rest.fetch_member(\n            ctx.guild_id, ctx.author.id\n        )\n        can_manage_guild = bool(permissions_for(member) & Permissions.MANAGE_GUILD)\n\n    await ctx.respond(help_message(can_manage_guild, ctx.author.id in settings.owner_ids))\n\n\n@plugin.command\n@lightbulb.command(\"tutorial\", \"A tutorial for completing tasks.\", ephemeral=True)\n@lightbulb.implements(lightbulb.SlashCommand, lightbulb.PrefixCommand)\nasync def tutorial(ctx: lightbulb.Context) -> None:\n    \"\"\"Help for the bot.\"\"\"\n    await ctx.respond(tutorial_message(True, True))\n\n\ndef load(bot: lightbulb.BotApp):\n    \"\"\"Add the plugin to the bot.\"\"\"\n    bot.add_plugin(plugin)\n\n\ndef unload(bot: lightbulb.BotApp):\n    \"\"\"Remove the plugin to the bot.\"\"\"\n    bot.remove_plugin(plugin)\n", "discord-bots/oa-bot-py/bot/extensions/work.py": "\"\"\"Work plugin for collecting user data.\"\"\"\nimport asyncio\nimport typing as t\nfrom uuid import UUID\n\nimport hikari\nimport lightbulb\nimport lightbulb.decorators\nimport miru\nfrom aiosqlite import Connection\nfrom bot.messages import (\n    assistant_reply_messages,\n    confirm_label_response_message,\n    confirm_ranking_response_message,\n    confirm_text_response_message,\n    initial_prompt_messages,\n    label_assistant_reply_messages,\n    label_prompter_reply_messages,\n    plain_embed,\n    prompter_reply_messages,\n    rank_assistant_reply_message,\n    rank_conversation_reply_messages,\n    rank_initial_prompts_messages,\n    rank_prompter_reply_messages,\n    task_complete_embed,\n)\nfrom bot.settings import Settings\nfrom loguru import logger\nfrom oasst_shared.api_client import OasstApiClient\nfrom oasst_shared.schemas import protocol as protocol_schema\n\nplugin = lightbulb.Plugin(\"WorkPlugin\")\n\nMAX_TASK_TIME = 60 * 60  # seconds\nMAX_TASK_ACCEPT_TIME = 60 * 10  # seconds\n\nsettings = Settings()\n\n_Task_contra = t.TypeVar(\"_Task_contra\", bound=protocol_schema.Task, contravariant=True)\n\n\nclass _TaskHandler(t.Generic[_Task_contra]):\n    \"\"\"Handle user interaction for a task.\"\"\"\n\n    def __init__(self, ctx: lightbulb.Context, task: _Task_contra) -> None:\n        \"\"\"Create a new `TaskHandler`.\n\n        Args:\n            ctx (lightbulb.Context): The context of the command that started the task.\n            task (_Task_contra): The task to handle.\n        \"\"\"\n        self.ctx = ctx\n        self.task = task\n        self.task_messages = self.get_task_messages(task)\n        self.sent_messages: list[hikari.Message] = []\n\n    @staticmethod\n    def get_task_messages(task: _Task_contra) -> list[str]:\n        \"\"\"Get the messages to send to the user for the task.\"\"\"\n        raise NotImplementedError\n\n    async def send(self) -> t.Literal[\"accept\", \"next\", \"cancel\"] | None:\n        \"\"\"Send the task and wait for the user to accept/skip/cancel it.\"\"\"\n        # Send all but the last message because we need to attach buttons to the last one\n        logger.debug(f\"Sending {len(self.task_messages)} messages\\n{self.task_messages!r}\")\n        for task_msg in self.task_messages[:-1]:\n            if len(task_msg) > 2000:\n                logger.warning(f\"Attempting to send a message <2000 characters in length. Task id: {self.task.id}\")\n                task_msg = task_msg[:1999]\n            self.sent_messages.append(await self.ctx.author.send(task_msg))\n\n        # Send the last message with buttons\n        task_accept_view = TaskAcceptView(timeout=MAX_TASK_ACCEPT_TIME)\n        logger.debug(f\"TH Message length {len(self.task_messages[-1])}\")\n        last_msg = await self.ctx.author.send(self.task_messages[-1][:1999], components=task_accept_view)\n\n        await task_accept_view.start(last_msg)\n        await task_accept_view.wait()\n\n        return task_accept_view.choice\n\n    async def handle(self) -> None:\n        \"\"\"Handle the user's response to the task.\n\n        This method should be called after `send` has been called.\"\"\"\n        # Ack task to the backend\n        oasst_api: OasstApiClient = self.ctx.bot.d.oasst_api\n        await oasst_api.ack_task(self.task.id, message_id=f\"{self.sent_messages[0].id}\")\n\n        # Loop until the user's input is accepted\n        while True:\n            try:\n                # Wait for user to send a message\n                event = await self.ctx.bot.wait_for(\n                    hikari.DMMessageCreateEvent,\n                    predicate=lambda e: (\n                        e.author_id == self.ctx.author.id\n                        and e.message.content is not None\n                        and not e.message.content.startswith(settings.prefix)\n                    ),\n                    timeout=MAX_TASK_TIME,\n                )\n\n                # Validate the message\n                if event.content is None or not self.check_user_input(event.content):\n                    await self.ctx.author.send(\"Invalid input\")\n                    continue\n\n                # Confirm user input\n                if not (await self.confirm_user_input(event.content)):\n                    continue\n\n                # Message is valid and confirmed by user\n                break\n\n            except asyncio.TimeoutError:\n                return\n\n        next_task = await self.notify(event.content, event)\n        if not isinstance(next_task, protocol_schema.TaskDone):\n            raise TypeError(f\"Unknown task type: {next_task!r}\")\n\n        return\n\n    async def notify(self, content: str, event: hikari.DMMessageCreateEvent) -> protocol_schema.Task:\n        \"\"\"Notify the backend that the user completed the task.\"\"\"\n        raise NotImplementedError\n\n    async def confirm_user_input(self, content: str) -> bool:\n        \"\"\"Send the user's response back to the user and ask them to confirm it. Returns True if the user confirms.\"\"\"\n        raise NotImplementedError\n\n    def check_user_input(self, content: str) -> bool:\n        \"\"\"Check the user's response to the task. Returns True if the response is valid.\"\"\"\n        raise NotImplementedError\n\n    async def cancel(self, reason: str = \"not specified\") -> None:\n        \"\"\"Cancel the task.\"\"\"\n        oasst_api: OasstApiClient = self.ctx.bot.d.oasst_api\n        await oasst_api.nack_task(self.task.id, reason)\n\n\n_Ranking_contra = t.TypeVar(\n    \"_Ranking_contra\",\n    bound=protocol_schema.RankAssistantRepliesTask\n    | protocol_schema.RankInitialPromptsTask\n    | protocol_schema.RankPrompterRepliesTask\n    | protocol_schema.RankConversationRepliesTask,\n    contravariant=True,\n)\n\n\nclass _RankingTaskHandler(_TaskHandler[_Ranking_contra]):\n    \"\"\"This should not be used directly. Use its subclasses instead.\"\"\"\n\n    async def notify(self, content: str, event: hikari.DMMessageCreateEvent) -> protocol_schema.Task:\n        oasst_api: OasstApiClient = self.ctx.bot.d.oasst_api\n\n        task = await oasst_api.post_interaction(\n            protocol_schema.MessageRanking(\n                user=protocol_schema.User(\n                    id=f\"{self.ctx.author.id}\", auth_method=\"discord\", display_name=self.ctx.author.username\n                ),\n                ranking=[int(r) - 1 for r in content.split(\",\")],\n                message_id=f\"{self.sent_messages[0].id}\",\n            )\n        )\n\n        db: Connection = self.ctx.bot.d.db\n        async with db.cursor() as cursor:\n            row = await (\n                await cursor.execute(\"SELECT log_channel_id FROM guilds WHERE guild_id = ?\", (self.ctx.guild_id,))\n            ).fetchone()\n            log_channel = row[0] if row else None\n        log_messages: list[hikari.Message] = []\n\n        if log_channel is not None:\n            for message in self.task_messages[:-1]:\n                msg = await self.ctx.bot.rest.create_message(log_channel, message)\n                log_messages.append(msg)\n            await self.ctx.bot.rest.create_message(log_channel, task_complete_embed(self.task, self.ctx.author.mention))\n\n        return task\n\n\nclass RankAssistantRepliesHandler(_RankingTaskHandler[protocol_schema.RankAssistantRepliesTask]):\n    @staticmethod\n    def get_task_messages(task: protocol_schema.RankAssistantRepliesTask) -> list[str]:\n        return rank_assistant_reply_message(task)\n\n    def check_user_input(self, content: str) -> bool:\n        return len(content.split(\",\")) == len(self.task.reply_messages) and all(\n            [r.isdigit() and int(r) in range(1, len(self.task.reply_messages) + 1) for r in content.split(\",\")]\n        )\n\n    async def confirm_user_input(self, content: str) -> bool:\n        confirm_input_view = YesNoView()\n        msg = await self.ctx.author.send(\n            confirm_ranking_response_message(content, self.task.reply_messages), components=confirm_input_view\n        )\n        await confirm_input_view.start(msg)\n        await confirm_input_view.wait()\n\n        return bool(confirm_input_view.choice)\n\n\nclass RankInitialPromptHandler(_RankingTaskHandler[protocol_schema.RankInitialPromptsTask]):\n    def __init__(self, ctx: lightbulb.Context, task: protocol_schema.RankInitialPromptsTask) -> None:\n        super().__init__(ctx, task)\n\n    @staticmethod\n    def get_task_messages(task: protocol_schema.RankInitialPromptsTask) -> list[str]:\n        return rank_initial_prompts_messages(task)\n\n    def check_user_input(self, content: str) -> bool:\n        return len(content.split(\",\")) == len(self.task.prompt_messages) and all(\n            [r.isdigit() and int(r) in range(1, len(self.task.prompt_messages) + 1) for r in content.split(\",\")]\n        )\n\n    async def confirm_user_input(self, content: str) -> bool:\n        confirm_input_view = YesNoView()\n        msg = await self.ctx.author.send(\n            confirm_ranking_response_message(content, self.task.prompt_messages), components=confirm_input_view\n        )\n        await confirm_input_view.start(msg)\n        await confirm_input_view.wait()\n\n        return bool(confirm_input_view.choice)\n\n\nclass RankPrompterReplyHandler(_RankingTaskHandler[protocol_schema.RankPrompterRepliesTask]):\n    @staticmethod\n    def get_task_messages(task: protocol_schema.RankPrompterRepliesTask) -> list[str]:\n        return rank_prompter_reply_messages(task)\n\n    def check_user_input(self, content: str) -> bool:\n        return len(content.split(\",\")) == len(self.task.reply_messages) and all(\n            [r.isdigit() and int(r) in range(1, len(self.task.reply_messages) + 1) for r in content.split(\",\")]\n        )\n\n    async def confirm_user_input(self, content: str) -> bool:\n        confirm_input_view = YesNoView()\n        msg = await self.ctx.author.send(\n            confirm_ranking_response_message(content, self.task.reply_messages), components=confirm_input_view\n        )\n        await confirm_input_view.start(msg)\n        await confirm_input_view.wait()\n\n        return bool(confirm_input_view.choice)\n\n\nclass RankConversationReplyHandler(_RankingTaskHandler[protocol_schema.RankConversationRepliesTask]):\n    @staticmethod\n    def get_task_messages(task: protocol_schema.RankConversationRepliesTask) -> list[str]:\n        return rank_conversation_reply_messages(task)\n\n    def check_user_input(self, content: str) -> bool:\n        return len(content.split(\",\")) == len(self.task.reply_messages) and all(\n            [r.isdigit() and int(r) in range(1, len(self.task.reply_messages) + 1) for r in content.split(\",\")]\n        )\n\n    async def confirm_user_input(self, content: str) -> bool:\n        confirm_input_view = YesNoView()\n        msg = await self.ctx.author.send(\n            confirm_ranking_response_message(content, self.task.reply_messages), components=confirm_input_view\n        )\n        await confirm_input_view.start(msg)\n        await confirm_input_view.wait()\n\n        return bool(confirm_input_view.choice)\n\n\nclass InitialPromptHandler(_TaskHandler[protocol_schema.InitialPromptTask]):\n    @staticmethod\n    def get_task_messages(task: protocol_schema.InitialPromptTask) -> list[str]:\n        return initial_prompt_messages(task)\n\n    def check_user_input(self, content: str) -> bool:\n        return len(content) > 0\n\n    async def confirm_user_input(self, content: str) -> bool:\n        confirm_input_view = YesNoView()\n        msg = await self.ctx.author.send(confirm_text_response_message(content), components=confirm_input_view)\n        await confirm_input_view.start(msg)\n        await confirm_input_view.wait()\n\n        return bool(confirm_input_view.choice)\n\n\nclass PrompterReplyHandler(_TaskHandler[protocol_schema.PrompterReplyTask]):\n    @staticmethod\n    def get_task_messages(task: protocol_schema.PrompterReplyTask) -> list[str]:\n        return prompter_reply_messages(task)\n\n    def check_user_input(self, content: str) -> bool:\n        return len(content) > 0\n\n    async def confirm_user_input(self, content: str) -> bool:\n        confirm_input_view = YesNoView()\n        msg = await self.ctx.author.send(confirm_text_response_message(content), components=confirm_input_view)\n        await confirm_input_view.start(msg)\n        await confirm_input_view.wait()\n\n        return bool(confirm_input_view.choice)\n\n\nclass AssistantReplyHandler(_TaskHandler[protocol_schema.AssistantReplyTask]):\n    @staticmethod\n    def get_task_messages(task: protocol_schema.AssistantReplyTask) -> list[str]:\n        return assistant_reply_messages(task)\n\n    def check_user_input(self, content: str) -> bool:\n        return len(content) > 0\n\n    async def confirm_user_input(self, content: str) -> bool:\n        confirm_input_view = YesNoView()\n        msg = await self.ctx.author.send(confirm_text_response_message(content), components=confirm_input_view)\n        await confirm_input_view.start(msg)\n        await confirm_input_view.wait()\n\n        return bool(confirm_input_view.choice)\n\n\n_Label_contra = t.TypeVar(\"_Label_contra\", bound=protocol_schema.LabelConversationReplyTask, contravariant=True)\n\n\nclass _LabelConversationReplyHandler(_TaskHandler[_Label_contra]):\n    def check_user_input(self, content: str) -> bool:\n        user_labels = content.split(\",\")\n        return (\n            all([l in self.task.valid_labels for l in user_labels])\n            and self.task.mandatory_labels is not None\n            and all([m in user_labels for m in self.task.mandatory_labels])\n        )\n\n    async def confirm_user_input(self, content: str) -> bool:\n        confirm_input_view = YesNoView()\n        msg = await self.ctx.author.send(confirm_label_response_message(content), components=confirm_input_view)\n        await confirm_input_view.start(msg)\n        await confirm_input_view.wait()\n\n        return bool(confirm_input_view.choice)\n\n\nclass LabelAssistantReplyHandler(_LabelConversationReplyHandler[protocol_schema.LabelAssistantReplyTask]):\n    @staticmethod\n    def get_task_messages(task: protocol_schema.LabelAssistantReplyTask) -> list[str]:\n        return label_assistant_reply_messages(task)\n\n\nclass LabelPrompterReplyHandler(_LabelConversationReplyHandler[protocol_schema.LabelPrompterReplyTask]):\n    @staticmethod\n    def get_task_messages(task: protocol_schema.LabelPrompterReplyTask) -> list[str]:\n        return label_prompter_reply_messages(task)\n\n\nsummarize_story = \"summarize_story\"\nrate_summary = \"rate_summary\"\n\n\n@plugin.command\n@lightbulb.command(\"work\", \"Complete a task.\")\n@lightbulb.implements(lightbulb.SlashCommand, lightbulb.PrefixCommand)\nasync def work2(ctx: lightbulb.Context) -> None:\n    \"\"\"Complete a task.\"\"\"\n    oasst_api: OasstApiClient = ctx.bot.d.oasst_api\n    currently_working: dict[hikari.Snowflake, UUID] = ctx.bot.d.currently_working\n\n    # Check if the user is already working on a task\n    if ctx.author.id in currently_working:\n        yn_view = YesNoView(timeout=MAX_TASK_ACCEPT_TIME)\n        msg = await ctx.author.send(\n            embed=plain_embed(\"You are already working. Would you like to cancel your old task start a new one?\"),\n            flags=hikari.MessageFlag.EPHEMERAL,\n            components=yn_view,\n        )\n        await yn_view.start(msg)\n        await yn_view.wait()\n\n        match yn_view.choice:\n            case False | None:\n                return\n            case True:\n                task_id = currently_working[ctx.author.id]\n                await oasst_api.nack_task(task_id, reason=\"user cancelled\")\n\n    if ctx.guild_id:\n        await ctx.respond(\"check DMs\", flags=hikari.MessageFlag.EPHEMERAL)\n\n    # Keep sending tasks until the user doesn't want more\n    try:\n        while True:\n            task = await oasst_api.fetch_random_task(\n                user=protocol_schema.User(\n                    id=f\"{ctx.author.id}\", display_name=ctx.author.username, auth_method=\"discord\"\n                ),\n            )\n\n            # Ranking tasks\n            if isinstance(task, protocol_schema.RankAssistantRepliesTask):\n                task_handler = RankAssistantRepliesHandler(ctx, task)\n            elif isinstance(task, protocol_schema.RankInitialPromptsTask):\n                task_handler = RankInitialPromptHandler(ctx, task)\n            elif isinstance(task, protocol_schema.RankPrompterRepliesTask):\n                task_handler = RankPrompterReplyHandler(ctx, task)\n            elif isinstance(task, protocol_schema.RankConversationRepliesTask):\n                task_handler = RankConversationReplyHandler(ctx, task)\n\n            # Text input tasks\n            elif isinstance(task, protocol_schema.InitialPromptTask):\n                task_handler = InitialPromptHandler(ctx, task)\n            elif isinstance(task, protocol_schema.PrompterReplyTask):\n                task_handler = PrompterReplyHandler(ctx, task)\n            elif isinstance(task, protocol_schema.AssistantReplyTask):\n                task_handler = AssistantReplyHandler(ctx, task)\n\n            # Label tasks\n            elif isinstance(task, protocol_schema.LabelAssistantReplyTask):\n                task_handler = LabelAssistantReplyHandler(ctx, task)\n            elif isinstance(task, protocol_schema.LabelPrompterReplyTask):\n                task_handler = LabelPrompterReplyHandler(ctx, task)\n\n            else:\n                raise ValueError(f\"Unknown task type: {type(task)}\")\n\n            resp = await task_handler.send()\n\n            match resp:\n                case \"accept\":\n                    currently_working[ctx.author.id] = task.id\n                    await task_handler.handle()\n                case \"next\":\n                    await task_handler.cancel(\"user skipped task\")\n                case \"cancel\":\n                    await task_handler.cancel(\"user canceled work\")\n                    break\n                case None:\n                    await task_handler.cancel(\"select timed out\")\n                    break\n    finally:\n        del currently_working[ctx.author.id]\n\n\nclass TaskAcceptView(miru.View):\n    \"\"\"View with three buttons: accept, next, and cancel.\n\n    The view stops once one of the buttons is pressed and the choice is stored in the `choice` attribute.\n    \"\"\"\n\n    choice: t.Literal[\"accept\", \"next\", \"cancel\"] | None = None\n\n    @miru.button(label=\"Accept\", custom_id=\"accept\", row=0, style=hikari.ButtonStyle.SUCCESS)\n    async def accept_button(self, button: miru.Button, ctx: miru.ViewContext) -> None:\n        logger.info(\"Accept button pressed\")\n        self.choice = \"accept\"\n        await ctx.message.edit(component=None)\n        self.stop()\n\n    @miru.button(label=\"Next Task\", custom_id=\"next_task\", row=0, style=hikari.ButtonStyle.SECONDARY)\n    async def next_button(self, button: miru.Button, ctx: miru.ViewContext) -> None:\n        logger.info(\"Next button pressed\")\n        self.choice = \"next\"\n        await ctx.message.edit(component=None)\n        self.stop()\n\n    @miru.button(label=\"Cancel\", custom_id=\"cancel\", row=0, style=hikari.ButtonStyle.DANGER)\n    async def cancel_button(self, button: miru.Button, ctx: miru.ViewContext) -> None:\n        logger.info(\"Cancel button pressed\")\n        self.choice = \"cancel\"\n        await ctx.message.edit(component=None)\n        self.stop()\n\n    async def on_timeout(self) -> None:\n        if self.message is not None:\n            await self.message.edit(component=None)\n\n\nclass YesNoView(miru.View):\n    \"\"\"View with two buttons: yes and no.\n\n    The view stops once one of the buttons is pressed and the choice is stored in the `choice` attribute.\n    \"\"\"\n\n    choice: bool | None = None\n\n    @miru.button(label=\"Yes\", custom_id=\"yes\", style=hikari.ButtonStyle.SUCCESS)\n    async def yes_button(self, button: miru.Button, ctx: miru.ViewContext) -> None:\n        self.choice = True\n        await ctx.message.edit(component=None)\n        self.stop()\n\n    @miru.button(label=\"No\", custom_id=\"no\", style=hikari.ButtonStyle.DANGER)\n    async def no_button(self, button: miru.Button, ctx: miru.ViewContext) -> None:\n        self.choice = False\n        await ctx.message.edit(component=None)\n        self.stop()\n\n    async def on_timeout(self) -> None:\n        if self.message is not None:\n            await self.message.edit(component=None)\n\n\ndef load(bot: lightbulb.BotApp):\n    \"\"\"Add the plugin to the bot.\"\"\"\n    bot.add_plugin(plugin)\n\n\ndef unload(bot: lightbulb.BotApp):\n    \"\"\"Remove the plugin to the bot.\"\"\"\n    bot.remove_plugin(plugin)\n", "discord-bots/oa-bot-py/bot/extensions/__init__.py": "\"\"\"Extensions for the bot.\n\nSee: https://hikari-lightbulb.readthedocs.io/en/latest/guides/extensions.html\n\"\"\"\n", "model/model_training/trainer_rm.py": "import argparse\nimport logging\nimport os\nfrom typing import Callable, Literal, Optional, Sequence, Union\n\nimport datasets\nimport torch\nfrom model_training.custom_datasets.ranking_collator import RankingDataCollator\nfrom model_training.efficiency_utils import fuse_gelu\nfrom model_training.metrics import RewardMetrics\nfrom model_training.utils.utils import (\n    PerDatasetSampler,\n    _strtobool,\n    get_dataset,\n    get_loss,\n    get_model,\n    get_tokenizer,\n    init_rng,\n    read_yamls,\n)\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Subset\nfrom tqdm import tqdm\nfrom transformers import PreTrainedModel, Trainer, TrainingArguments\nfrom transformers.trainer_pt_utils import IterableDatasetShard\nfrom transformers.trainer_utils import seed_worker\nfrom transformers.training_args import OptimizerNames\nfrom transformers.utils import is_datasets_available\n\n\nclass RMTrainer(Trainer):\n    def __init__(\n        self,\n        model: Union[PreTrainedModel, nn.Module] = None,\n        args: TrainingArguments = None,\n        sampler: torch.utils.data.sampler.Sampler = None,\n        loss_function: Literal[\"RMLoss\"] = \"RMLoss\",\n        score_l2_reg: float = 0.001,\n        train_collate_fn: Callable = None,\n        **kwargs,\n    ):\n        super().__init__(model, args, **kwargs)\n        self.train_collate_fn = train_collate_fn\n        self.loss_fct = get_loss(loss_function, score_l2_reg=score_l2_reg)\n        self.sampler = sampler\n\n    def compute_loss(self, model, inputs, return_logits=False):\n        batch, cu_lens = inputs\n\n        logits = model(\n            input_ids=batch[\"input_ids\"],\n            attention_mask=batch[\"attention_mask\"],\n        ).logits\n\n        loss = self.loss_fct(logits, cu_lens)\n\n        return (loss, logits) if return_logits else loss\n\n    def prediction_step(\n        self,\n        model: nn.Module,\n        inputs: tuple[dict[str, torch.Tensor], dict[str, torch.Tensor], list[int]],\n        prediction_loss_only: bool,\n        ignore_keys: Optional[list[str]] = None,\n    ) -> tuple[Optional[torch.Tensor], Optional[torch.Tensor], Optional[torch.Tensor]]:\n        batch, cu_lens = inputs\n        with torch.no_grad():\n            batch = self._prepare_inputs(batch)\n            loss, logits = self.compute_loss(model, (batch, cu_lens), return_logits=True)\n\n        loss = loss.mean().detach()\n\n        labels = []\n        for i, (s, e) in enumerate(zip(cu_lens[:-1], cu_lens[1:])):\n            labels.extend([i] * (e - s))\n        # make sure labels are same as logits, needed for deepspeed\n        labels = torch.tensor(labels, device=logits.device, requires_grad=False).view(-1, 1)\n        return (loss, logits.T, labels.T)  # transposed to avoid truncation in evaluation_loop\n\n    def get_train_dataloader(self):\n        \"\"\"\n        Inject custom data sampling behaviour into training loop\n        and use custom task mixing collate function : train_collate_fn\n\n        rewrite from:\n        https://github.com/huggingface/transformers/blob/67d074874d285e616393c65a0e670088e1b6b74a/src/transformers/trainer.py#L846\n        \"\"\"\n        data_collator = self.train_collate_fn\n        train_dataset = self.train_dataset\n        if is_datasets_available() and isinstance(train_dataset, datasets.Dataset):\n            train_dataset = self._remove_unused_columns(train_dataset, description=\"training\")\n\n        if isinstance(train_dataset, torch.utils.data.IterableDataset):\n            # if we are using iterable dataset it means no weight sampling\n            # added for backward compat\n            if self.args.world_size > 1:\n                train_dataset = IterableDatasetShard(\n                    train_dataset,\n                    batch_size=self._train_batch_size,\n                    drop_last=self.args.dataloader_drop_last,\n                    num_processes=self.args.world_size,\n                    process_index=self.args.process_index,\n                )\n            return DataLoader(\n                train_dataset,\n                batch_size=self.args.per_device_train_batch_size,\n                collate_fn=data_collator,\n                num_workers=self.args.dataloader_num_workers,\n                pin_memory=self.args.dataloader_pin_memory,\n            )\n\n        if self.sampler is None:\n            train_sampler = self._get_train_sampler()\n        else:\n            train_sampler = self.sampler\n            logging.warning(\"Custom sampler found!\")\n\n        dataloader = DataLoader(\n            train_dataset,\n            batch_size=self._train_batch_size,\n            sampler=train_sampler,\n            collate_fn=data_collator,\n            drop_last=self.args.dataloader_drop_last,\n            num_workers=self.args.dataloader_num_workers,\n            pin_memory=self.args.dataloader_pin_memory,\n            worker_init_fn=seed_worker,\n        )\n        return dataloader\n\n\ndef argument_parsing(notebook: bool = False, notebook_args: Sequence[str] | None = None):\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--configs\", nargs=\"+\", required=True)\n    parser.add_argument(\"--local_rank\", type=int, default=-1)\n    parser.add_argument(\"--deepspeed\", action=\"store_true\")\n    parser.add_argument(\"--no-deepspeed\", dest=\"deepspeed\", action=\"store_false\")\n    parser.add_argument(\"--wandb-entity\", type=str, default=\"open-assistant\")\n    parser.add_argument(\"--resume_from_checkpoint\", action=\"store_true\", help=\"Resume from last saved checkpoint\")\n    parser.add_argument(\"--rng_seed\", type=int, help=\"rng seed\")\n    parser.add_argument(\"--show_dataset_stats\", action=\"store_true\", help=\"Show dataset stats\", default=False)\n    parser.set_defaults(deepspeed=False)\n\n    if notebook:\n        args, remaining = parser.parse_known_args(notebook_args)\n    else:\n        args, remaining = parser.parse_known_args()\n\n    # Config from YAML\n    conf = {}\n    configs = read_yamls(\"./configs\")\n    for name in args.configs:\n        if \",\" in name:\n            for n in name.split(\",\"):\n                conf.update(configs[n])\n        else:\n            conf.update(configs[name])\n\n    conf[\"wandb_entity\"] = args.wandb_entity\n    conf[\"local_rank\"] = args.local_rank\n    conf[\"deepspeed\"] = args.deepspeed\n    conf[\"resume_from_checkpoint\"] = args.resume_from_checkpoint\n    if args.rng_seed is not None:\n        conf[\"rng_seed\"] = args.rng_seed\n    conf[\"show_dataset_stats\"] = args.show_dataset_stats\n\n    # get the world size in deepspeed\n    if conf[\"deepspeed\"]:\n        conf[\"world_size\"] = int(os.getenv(\"WORLD_SIZE\", default=\"1\"))\n    else:\n        conf[\"world_size\"] = 1\n\n    # Override config from command-line\n    parser = argparse.ArgumentParser()\n    for key, value in conf.items():\n        type_ = type(value) if value is not None else str\n        if type_ == bool:\n            type_ = _strtobool\n        parser.add_argument(f\"--{key}\", type=type_, default=value)\n\n    return parser.parse_args(remaining)\n\n\ndef main():\n    training_conf = argument_parsing()\n    if not training_conf.deepspeed or training_conf.local_rank == 0:\n        print(f\"trainig_conf = {training_conf}\")\n\n    init_rng(training_conf)\n\n    tokenizer = get_tokenizer(training_conf)\n    model = get_model(training_conf, tokenizer)\n\n    train, evals = get_dataset(training_conf, mode=\"rm\")\n    train_collate_fn = RankingDataCollator(\n        tokenizer,\n        max_length=training_conf.max_length,\n        pad_to_multiple_of=16,\n        max_replies=training_conf.max_replies,\n        use_system_tag=training_conf.use_system_tag,\n        system_property_dropout=training_conf.system_property_dropout,\n        system_add_length=training_conf.system_add_length,\n    )\n    eval_collate_fn = RankingDataCollator(\n        tokenizer,\n        max_length=training_conf.max_length,\n        pad_to_multiple_of=16,\n        max_replies=training_conf.max_replies,\n        use_system_tag=training_conf.use_system_tag,\n        system_property_dropout=training_conf.system_property_dropout,\n        system_add_length=training_conf.system_add_length,\n    )\n\n    show_dataset_stats = (training_conf.verbose or training_conf.show_dataset_stats) and (\n        not training_conf.deepspeed or training_conf.local_rank == 0\n    )\n    if show_dataset_stats:\n        print(\"Dataset stats before sampling:\")\n        total = len(train)\n        for d in train.datasets:\n            if isinstance(d, Subset):\n                name = f\"Subset of {type(d.dataset).__name__}\"\n                if hasattr(d.dataset, \"name\"):\n                    name += f\" ({d.dataset.name})\"\n            else:\n                name = type(d).__name__\n                if hasattr(d, \"name\"):\n                    name += f\" ({d.name})\"\n            print(f\"{name}: {len(d)} ({len(d) / total:%})\")\n        print(f\"Total train: {total}\")\n\n    if training_conf.use_custom_sampler:\n        samples_length = None\n        if training_conf.sort_by_length:\n            samples_length = list(\n                map(\n                    lambda x: train_collate_fn.process_one(x, return_length=True),\n                    tqdm(train, desc=\"Calculating lengths per sample\"),\n                )\n            )\n        sampler = PerDatasetSampler.build_sampler_from_config(\n            training_conf,\n            train.datasets,\n            rank=training_conf.local_rank,\n            world_size=training_conf.world_size,\n            samples_length=samples_length,\n            verbose=show_dataset_stats,\n        )\n    else:\n        sampler = None\n\n    optimizer = OptimizerNames.ADAMW_BNB if training_conf.quantization else OptimizerNames.ADAMW_HF\n\n    if training_conf.quantization:\n        import bitsandbytes\n\n        for module in model.modules():\n            if isinstance(module, torch.nn.Embedding):\n                bitsandbytes.optim.GlobalOptimManager.get_instance().register_module_override(\n                    module, \"weight\", {\"optim_bits\": 32}\n                )\n\n    if training_conf.fuse_gelu:\n        model = fuse_gelu(model)\n\n    output_dir = (\n        training_conf.output_dir\n        if training_conf.output_dir\n        else f\"{training_conf.model_name}-{training_conf.log_dir}-finetuned\"\n    )\n\n    args = TrainingArguments(\n        output_dir=output_dir,\n        num_train_epochs=training_conf.num_train_epochs,\n        warmup_steps=training_conf.warmup_steps,\n        learning_rate=float(training_conf.learning_rate),\n        deepspeed=training_conf.deepspeed_config if training_conf.deepspeed else None,\n        optim=optimizer,\n        fp16=training_conf.dtype in [\"fp16\", \"float16\"],\n        bf16=training_conf.dtype in [\"bf16\", \"bfloat16\"],\n        local_rank=training_conf.local_rank,\n        gradient_checkpointing=training_conf.gradient_checkpointing,\n        gradient_accumulation_steps=training_conf.gradient_accumulation_steps,\n        per_device_train_batch_size=training_conf.per_device_train_batch_size,\n        per_device_eval_batch_size=training_conf.per_device_eval_batch_size,\n        adam_beta1=training_conf.adam_beta1,\n        adam_beta2=training_conf.adam_beta2,\n        adam_epsilon=float(training_conf.adam_epsilon),\n        weight_decay=training_conf.weight_decay,\n        max_grad_norm=training_conf.max_grad_norm,\n        logging_steps=training_conf.logging_steps,\n        save_total_limit=training_conf.save_total_limit,\n        evaluation_strategy=\"steps\",\n        eval_steps=training_conf.eval_steps,\n        save_strategy=training_conf.save_strategy,\n        save_steps=training_conf.save_steps,\n        eval_accumulation_steps=training_conf.eval_accumulation_steps,\n        resume_from_checkpoint=training_conf.resume_from_checkpoint,\n        report_to=\"wandb\" if training_conf.log_wandb else None,\n    )\n\n    if not training_conf.log_wandb:\n        os.environ[\"WANDB_MODE\"] = \"offline\"\n\n    if training_conf.log_wandb and (not training_conf.deepspeed or training_conf.local_rank == 0):\n        import wandb\n\n        wandb.init(\n            project=\"reward-model\",\n            entity=training_conf.wandb_entity,\n            resume=training_conf.resume_from_checkpoint,\n            name=f\"{training_conf.model_name}-{training_conf.log_dir}-rm\",\n            config=training_conf,\n        )\n    compute_metrics = RewardMetrics(training_conf.metrics)\n    trainer = RMTrainer(\n        model=model,\n        args=args,\n        sampler=sampler,\n        train_collate_fn=train_collate_fn,\n        loss_function=training_conf.loss_fn,\n        score_l2_reg=training_conf.score_l2_reg,\n        train_dataset=train,\n        eval_dataset=evals,\n        data_collator=eval_collate_fn,\n        tokenizer=tokenizer,\n        compute_metrics=compute_metrics,\n    )\n    trainer.train(resume_from_checkpoint=training_conf.resume_from_checkpoint)\n    trainer.save_model()\n    tokenizer.save_pretrained(output_dir)\n\n\nif __name__ == \"__main__\":\n    main()\n", "model/model_training/efficiency_utils.py": "import functools\n\nimport torch\nfrom transformers.activations import FastGELUActivation, GELUActivation, NewGELUActivation, QuickGELUActivation\n\n\ndef rsetattr(obj, attr, val):\n    pre, _, post = attr.rpartition(\".\")\n    return setattr(rgetattr(obj, pre) if pre else obj, post, val)\n\n\ndef rgetattr(obj, attr, *args):\n    def _getattr(obj, attr):\n        return getattr(obj, attr, *args)\n\n    return functools.reduce(_getattr, [obj] + attr.split(\".\"))\n\n\ndef fuse_gelu(model):\n    @torch.jit.script\n    def gelu_fwd(x):\n        return x * 0.5 * (1.0 + torch.tanh(0.79788456 * x * (1 + 0.044715 * x * x)))\n\n    @torch.jit.script\n    def gelu_bwd(g, x):\n        tanh_out = torch.tanh(0.79788456 * x * (1 + 0.044715 * x * x))\n        ff = 0.5 * x * ((1 - tanh_out * tanh_out) * (0.79788456 + 0.1070322243 * x * x)) + 0.5 * (1 + tanh_out)\n        return ff * g\n\n    class _FusedGeLUFunction(torch.autograd.Function):\n        @staticmethod\n        # bias is an optional argument\n        def forward(ctx, input):\n            ctx.input_tensor = input\n            return gelu_fwd(input)\n\n        @staticmethod\n        def backward(ctx, grad_output):\n            input = ctx.input_tensor\n            tmp = gelu_bwd(grad_output, input)\n            return tmp\n\n    class FusedGelu(torch.nn.Module):\n        def forward(self, input):\n            return _FusedGeLUFunction.apply(input)\n\n    fused_gelu_module = FusedGelu()\n    hf_gelu_functions = [GELUActivation, FastGELUActivation, NewGELUActivation, QuickGELUActivation]\n\n    for name, module in model.named_modules():\n        for hf_gelu_function in hf_gelu_functions:\n            if isinstance(module, hf_gelu_function):\n                rsetattr(model, name, fused_gelu_module)\n\n    return model\n", "model/model_training/metrics.py": "import numpy as np\nfrom scipy import stats as st\n\nRM_METRICS = [\"accuracy\", \"kendalltau\", \"spearmanr\"]\n\n\ndef reward_accuracy(eval_pred):\n    logits = eval_pred.predictions\n    labels = eval_pred.label_ids\n    pos_scores, neg_scores = [], []\n    for b_logits, b_labels in zip(logits, labels):\n        b_labels = b_labels[b_labels != -100]\n        b_logits = b_logits[b_logits != -100]\n        for i in np.unique(b_labels):\n            logits_batch = b_logits[b_labels == i]\n            pos_scores.append(logits_batch[0])\n            neg_scores.append(logits_batch[-1])\n    pos_scores = np.array(pos_scores).reshape(-1, 1)\n    neg_scores = np.array(neg_scores).reshape(-1, 1)\n\n    metrics = {\n        \"pos_score\": np.mean(pos_scores),\n        \"neg_score\": np.mean(neg_scores),\n        \"score_diff\": np.mean(pos_scores - neg_scores),\n        \"accuracy\": np.mean(pos_scores > neg_scores),\n    }\n    return metrics\n\n\ndef kendall_tau(eval_pred):\n    logits = eval_pred.predictions\n    labels = eval_pred.label_ids\n    tau = 0.0\n    bsize = 0\n    for b_logits, b_labels in zip(logits, labels):\n        b_labels = b_labels[b_labels != -100]\n        b_logits = b_logits[b_logits != -100]\n        # uncomment to support non pythia model,\n        # remember to add to other metrics as well\n\n        # truncated_logits = min(len(b_labels), len(b_logits))\n        # b_labels = b_labels[:truncated_logits]\n        # b_logits = b_logits[:truncated_logits]\n        for i in np.unique(b_labels):\n            logits_batch = b_logits[b_labels == i]\n            pred_rank = np.argsort(logits_batch)\n            true_rank = np.arange(logits_batch.size - 1, -1, -1)\n            tau += st.kendalltau(pred_rank, true_rank)[0]\n        bsize += np.unique(b_labels).size\n\n    return {\"kendalltau\": tau / bsize}\n\n\ndef spearmanr(eval_pred):\n    logits = eval_pred.predictions\n    labels = eval_pred.label_ids\n    score = 0.0\n    bsize = 0\n    for b_logits, b_labels in zip(logits, labels):\n        b_labels = b_labels[b_labels != -100]\n        b_logits = b_logits[b_logits != -100]\n        for i in np.unique(b_labels):\n            logits_batch = b_logits[b_labels == i]\n            pred_rank = np.argsort(logits_batch)\n            true_rank = np.arange(logits_batch.size - 1, -1, -1)\n            score += st.spearmanr(pred_rank, true_rank).statistic\n        bsize += np.unique(b_labels).size\n\n    return {\"spearmanr\": score / bsize}\n\n\nclass RewardMetrics:\n    \"\"\"\n    class to combine multiple metrics\n    \"\"\"\n\n    def __init__(self, metrics):\n        if isinstance(metrics, str):\n            metrics = [metrics]\n\n        self.metrics = []\n        for name in metrics:\n            if name == \"accuracy\":\n                self.metrics.append(reward_accuracy)\n            elif name == \"kendalltau\":\n                self.metrics.append(kendall_tau)\n            elif name == \"spearmanr\":\n                self.metrics.append(spearmanr)\n            else:\n                raise ValueError(f\"Invalid metrics {name}. Available {RM_METRICS}\")\n\n    def __call__(self, eval_pred):\n        results = {}\n        for metric in self.metrics:\n            results.update(metric(eval_pred))\n\n        return results\n", "model/model_training/trainer_rl.py": "import argparse\nimport math\nimport os\nimport random\nfrom argparse import Namespace\nfrom typing import Sequence\n\nimport numpy as np\nimport torch\nimport transformers\nimport tritonclient.grpc as client_util\nimport trlx\nfrom model_training.custom_datasets.formatting import QA_SPECIAL_TOKENS, format_pairs\nfrom model_training.models import get_specific_model\nfrom model_training.utils.utils import _strtobool, get_dataset, init_rng, read_yamls\nfrom tritonclient.utils import np_to_triton_dtype\nfrom trlx.data.configs import TRLConfig\n\n# flake8: noqa\nfrom utils.ppo_utils import CustomPPOTrainer\nfrom utils.utils import _strtobool, get_dataset, get_model, init_rng, read_yamls\nfrom utils.utils_rl import prepare_tensor\n\n\ndef argument_parsing(notebook: bool = False, notebook_args: Sequence[str] | None = None, **kwargs):\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--configs\", nargs=\"+\", required=True)\n    parser.add_argument(\"--local_rank\", type=int, default=-1)\n    parser.add_argument(\"--wandb-entity\", type=str, default=\"open-assistant\")\n    parser.add_argument(\"--rng_seed\", type=int, help=\"rng seed\")\n\n    if notebook:\n        args, remaining = parser.parse_known_args(notebook_args)\n    else:\n        args, remaining = parser.parse_known_args()\n\n    # Config from YAML\n    conf = {}\n    configs = read_yamls(\"./configs\")\n    for name in args.configs:\n        if \",\" in name:\n            for n in name.split(\",\"):\n                conf.update(configs[n])\n        else:\n            conf.update(configs[name])\n\n    conf[\"local_rank\"] = args.local_rank\n    if args.rng_seed is not None:\n        conf[\"rng_seed\"] = args.rng_seed\n\n    # Override config from command-line\n    parser = argparse.ArgumentParser()\n\n    for key, value in kwargs.items():\n        type_ = type(value) if value is not None else str\n        parser.add_argument(f\"--{key}\", type=type_, default=value)\n\n    for key, value in conf.items():\n        type_ = type(value) if value is not None else str\n        if type_ == bool:\n            type_ = _strtobool\n        parser.add_argument(f\"--{key}\", type=type_, default=value)\n\n    return parser.parse_args(remaining)\n\n\n# Taken from https://github.com/CarperAI/trlx/blob/b7db6f9e74c7d8dc719255b27968d2994836957a/examples/hh/ppo_hh.py#L114\ndef create_reward_fn(rank_config, sft_config):  # noqa:  C901\n    triton_host = os.environ.get(\"TRITON_HOST_RM\")\n    assert triton_host is not None, \"Specify reward model in the TRITON_HOST_RM environmental variable\"\n\n    triton_url, triton_model = triton_host.split(\"/\")\n    client = client_util.InferenceServerClient(url=triton_url, verbose=False)\n\n    rank_tokenizer = transformers.AutoTokenizer.from_pretrained(rank_config.model_name, cache_dir=rank_config.cache_dir)\n    sft_tokenizer = transformers.AutoTokenizer.from_pretrained(sft_config.model_name, cache_dir=sft_config.cache_dir)\n\n    def reward_fn(samples, prompts, outputs):\n        if len(samples) == 0:\n            return []\n\n        # hack to allo for different tokenizers with different eos tokens ... rest of the\n        samples = [x.replace(sft_tokenizer.eos_token, rank_tokenizer.eos_token) for x in samples]\n        samples = [x.replace(sft_tokenizer.pad_token, rank_tokenizer.pad_token) for x in samples]\n\n        inputs = rank_tokenizer(samples, return_tensors=\"np\", padding=True)\n\n        mbs = rank_config.batch_size\n        out = []\n        for i in range(math.ceil(len(samples) / mbs)):\n            batch_ixs = slice(i * mbs, (i + 1) * mbs)\n\n            # We specified int32 as types for a triton client\n            result = client.infer(\n                triton_model,\n                [\n                    prepare_tensor(\"input_ids\", inputs.input_ids[batch_ixs].astype(np.int32)),\n                    prepare_tensor(\"attention_mask\", inputs.attention_mask[batch_ixs].astype(np.int32)),\n                ],\n            )\n\n            rewards = result.as_numpy(\"rewards\")\n\n            out.extend(rewards)\n\n        return out\n\n    return reward_fn\n\n\ndef main():\n    training_conf = argument_parsing()\n    rank_config = Namespace(**training_conf.rank_config)\n    sft_config = Namespace(**training_conf.sft_config)\n\n    triton_host_rm = os.getenv(\"TRITON_HOST_RM\", training_conf.triton_host_rm)\n    triton_host_sft = os.getenv(\"TRITON_HOST_REF\", training_conf.triton_host_sft)\n    os.environ[\"TRITON_HOST_RM\"] = triton_host_rm\n    os.environ[\"TRITON_HOST_REF\"] = triton_host_sft\n\n    init_rng(training_conf)\n\n    eos_token = transformers.AutoTokenizer.from_pretrained(\n        sft_config.model_name, cache_dir=sft_config.cache_dir\n    ).eos_token\n\n    # Load pretrained SFT model\n\n    # override model_name to be the same as sft_model\n    trlx_config = TRLConfig.load_yaml(\"configs/ppo_config.yaml\")\n    trlx_config.sft_config = sft_config\n\n    train, eval_dict = get_dataset(training_conf, mode=\"rl\")\n\n    # take the dataset as the eval prompt generation dataset\n    eval = eval_dict[\"oasst_export\"] if \"oasst_export\" in eval_dict else eval_dict[next(iter(eval_dict))]\n\n    # trlx requires training data to be a list of prompts\n    # first element of each sample is the context and the prompt\n    prompts, eval_prompts = tuple(\n        map(\n            lambda x: [\"\".join(format_pairs(x[i][0], eos_token, add_initial_reply_token=True)) for i in range(len(x))],\n            (train, eval),\n        )\n    )\n\n    ## Override first eval prompts just for visualization\n    eval_prompts = [\n        \"\".join(format_pairs([\"Can you tell me about GLaDOS?\"], eos_token, add_initial_reply_token=True)),\n        \"\".join(format_pairs([\"What is the chemical symbol for gold?\"], eos_token, add_initial_reply_token=True)),\n        \"\".join(\n            format_pairs(\n                [\"If you were the President of the United States, what would you do?\"],\n                eos_token,\n                add_initial_reply_token=True,\n            )\n        ),\n    ] + eval_prompts\n\n    if training_conf.num_eval_prompts is not None and training_conf.num_eval_prompts > 0:\n        eval_prompts = eval_prompts[: training_conf.num_eval_prompts]\n\n    random.shuffle(prompts)\n    # Sanity Check for prompts to make sure it's loading properly\n    with open(r\"output.txt\", \"w\") as fp:\n        for item in eval_prompts:\n            # write each item on a new line\n            fp.write(\"Prompt For RL: %s\\n\" % item)\n\n    trlx_config.tokenizer.tokenizer_path = sft_config.model_name\n    trlx_config.model.model_path = sft_config.model_name\n    trlx_config.train.batch_size = int(training_conf.batch_size)\n    trlx_config.method.chunk_size = int(training_conf.chunk_size)\n    trlx_config.method.num_rollouts = int(training_conf.num_rollouts)\n    trlx_config.train.total_steps = int(training_conf.total_steps)\n\n    if training_conf.debug:\n        print(\"Continuing in debug mode\")\n        prompts = prompts[:10]\n        eval_prompts = eval_prompts[:10]\n        trlx_config.method.num_rollouts = 1\n        # trlx_config.method.gen_kwargs['max_new_tokens'] = 12\n        # trlx_config.train.seq_length = 48\n\n    trainer = trlx.train(\n        sft_config.model_name,\n        reward_fn=create_reward_fn(rank_config, sft_config),\n        prompts=prompts,\n        eval_prompts=eval_prompts,\n        config=trlx_config,\n        stop_sequences=[eos_token],\n    )\n\n    training_conf.output_dir = training_conf.output_dir if training_conf.output_dir else training_conf.model_name\n\n    trainer.save_pretrained(training_conf.output_dir)\n\n\nif __name__ == \"__main__\":\n    main()\n", "model/model_training/__init__.py": "", "model/model_training/trainer_sft.py": "#!/usr/bin/env python3\nimport argparse\nimport logging\nimport os\nfrom functools import partial\nfrom typing import Any, Callable, Dict, List, Optional, Sequence, Tuple, Union\n\nimport datasets\nimport torch\n\n# from model_training.custom_datasets.formatting import DatasetEntry\nfrom model_training.custom_datasets.dialogue_collator import DialogueDataCollator\nfrom model_training.efficiency_utils import fuse_gelu\nfrom model_training.models.patching import RopePatch\nfrom model_training.models.peft_modeling import peft_model\nfrom model_training.utils.utils import (\n    PerDatasetSampler,\n    _strtobool,\n    get_dataset,\n    get_loss,\n    get_metrics,\n    get_model,\n    get_tokenizer,\n    init_rng,\n    read_yamls,\n)\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Subset\nfrom tqdm import tqdm\nfrom transformers import PreTrainedModel, Trainer, TrainingArguments\nfrom transformers.trainer_pt_utils import IterableDatasetShard\nfrom transformers.trainer_utils import seed_worker\nfrom transformers.training_args import OptimizerNames\nfrom transformers.utils import is_datasets_available\n\n\ndef compute_metrics(eval_pred, preprocess_fns, metrics):\n    out = {}\n    for metric, preprocess_fn in zip(metrics, preprocess_fns):\n        preds, labels = preprocess_fn(eval_pred)\n        out = dict(**out, **metric.compute(predictions=preds, references=labels))\n\n    return out\n\n\ndef preprocess_logits_for_metrics(logits, labels):\n    pred_ids = torch.argmax(logits, dim=-1)\n    return pred_ids\n\n\nclass SFTTrainer(Trainer):\n    def __init__(\n        self,\n        model: Union[PreTrainedModel, nn.Module] = None,\n        args: TrainingArguments = None,\n        sampler: torch.utils.data.sampler.Sampler = None,\n        loss_function: str = \"CrossEntropyLoss\",\n        poly_eps: float = 1.0,\n        train_collate_fn: Callable = None,\n        **kwargs,\n    ):\n        super().__init__(model, args, **kwargs)\n        self.train_collate_fn = train_collate_fn\n        # By default CrossEntropyLoss ignores padding_index -100, but just in case use our own loss_fct\n        self.loss_fct = get_loss(loss_function, poly_eps)\n        self.sampler = sampler\n\n    def compute_loss(self, model, inputs, return_outputs=False):\n        labels_mask = inputs.pop(\"label_masks\")\n        targets = inputs.pop(\"targets\")\n\n        outputs = model(\n            input_ids=inputs[\"input_ids\"],\n            attention_mask=inputs.get(\"attention_mask\", None),\n            use_cache=False,\n        )\n\n        loss = self.loss_fct(outputs.get(\"logits\"), targets, mask=labels_mask)\n\n        return (loss, outputs) if return_outputs else loss\n\n    def _compute_loss(self, model, inputs):\n        inputs = self._prepare_inputs(inputs)\n\n        labels_mask = inputs.pop(\"label_masks\")\n        targets = inputs.pop(\"targets\")\n\n        outputs = model(\n            input_ids=inputs[\"input_ids\"],\n            attention_mask=inputs.get(\"attention_mask\", None),\n            use_cache=False,\n        )\n\n        logits = outputs.get(\"logits\")\n\n        loss = self.loss_fct(outputs.get(\"logits\"), targets, mask=labels_mask)\n\n        return loss, logits, targets, labels_mask\n\n    def prediction_step(\n        self,\n        model: nn.Module,\n        inputs: Dict[str, Union[torch.Tensor, Any]],\n        prediction_loss_only: bool,\n        ignore_keys: Optional[List[str]] = None,\n    ) -> Tuple[Optional[torch.Tensor], Optional[torch.Tensor], Optional[torch.Tensor]]:\n        with torch.no_grad():\n            loss, logits, labels, labels_mask = self._compute_loss(model, inputs)\n            labels[~labels_mask.bool()] = -100  # padding_index\n\n        loss = loss.mean().detach()\n\n        if self.args.prediction_loss_only:\n            return (loss, None, None)\n\n        return (loss, logits, labels)\n\n    def get_train_dataloader(self):\n        \"\"\"\n        Inject custom data sampling behaviour into training loop\n        and use custom task mixing collate function : train_collate_fn\n\n        rewrite from:\n        https://github.com/huggingface/transformers/blob/67d074874d285e616393c65a0e670088e1b6b74a/src/transformers/trainer.py#L846\n        \"\"\"\n        data_collator = self.train_collate_fn\n        train_dataset = self.train_dataset\n        if is_datasets_available() and isinstance(train_dataset, datasets.Dataset):\n            train_dataset = self._remove_unused_columns(train_dataset, description=\"training\")\n\n        if isinstance(train_dataset, torch.utils.data.IterableDataset):\n            # if we are using iterable dataset it means no weight sampling\n            # added for backward compat\n            if self.args.world_size > 1:\n                train_dataset = IterableDatasetShard(\n                    train_dataset,\n                    batch_size=self._train_batch_size,\n                    drop_last=self.args.dataloader_drop_last,\n                    num_processes=self.args.world_size,\n                    process_index=self.args.process_index,\n                )\n            return DataLoader(\n                train_dataset,\n                batch_size=self.args.per_device_train_batch_size,\n                collate_fn=data_collator,\n                num_workers=self.args.dataloader_num_workers,\n                pin_memory=self.args.dataloader_pin_memory,\n            )\n\n        if self.sampler is None:\n            train_sampler = self._get_train_sampler()\n        else:\n            train_sampler = self.sampler\n            logging.warning(\"Custom sampler found!\")\n\n        dataloader = DataLoader(\n            train_dataset,\n            batch_size=self._train_batch_size,\n            sampler=train_sampler,\n            collate_fn=data_collator,\n            drop_last=self.args.dataloader_drop_last,\n            num_workers=self.args.dataloader_num_workers,\n            pin_memory=self.args.dataloader_pin_memory,\n            worker_init_fn=seed_worker,\n        )\n        return dataloader\n\n\ndef argument_parsing(notebook: bool = False, notebook_args: Sequence[str] | None = None):\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--configs\",\n        nargs=\"+\",\n        required=True,\n        help=\"\"\"\n        Multiple configs can be passed to set different options.\n        For example, run as:\n\n           ./trainer_sft.py --configs galactica-125m webgpt_dataset_only per_digit_tokens\n\n        to run the galactica-125m model, using the webgpt dataset only (as opposed to all\n        the datasets listed in defaults in config.yaml) and treat each digit as a separate token.\n    \"\"\",\n    )\n    parser.add_argument(\"--local_rank\", type=int, default=-1)\n    parser.add_argument(\"--deepspeed\", action=\"store_true\")\n    parser.add_argument(\"--no-deepspeed\", dest=\"deepspeed\", action=\"store_false\")\n    parser.add_argument(\"--wandb-entity\", type=str, default=\"open-assistant\")\n    parser.add_argument(\"--resume_from_checkpoint\", action=\"store_true\", help=\"Resume from last saved checkpoint\")\n    parser.add_argument(\"--rng_seed\", type=int, help=\"rng seed\")\n    parser.add_argument(\"--show_dataset_stats\", action=\"store_true\", help=\"Show dataset stats\", default=False)\n    parser.set_defaults(deepspeed=False)\n\n    if notebook:\n        args, remaining = parser.parse_known_args(notebook_args)\n    else:\n        args, remaining = parser.parse_known_args()\n\n    # Config from YAML\n    conf = {}\n    configs = read_yamls(\"./configs\")\n    conf.update(configs[\"defaults\"])\n    try:\n        for name in args.configs:\n            if \",\" in name:\n                for n in name.split(\",\"):\n                    conf.update(configs[n])\n            else:\n                conf.update(configs[name])\n    except KeyError as e:\n        print(f'Error: Could not find the config \"{e.args[0]}\" in config.yaml')\n        exit(1)\n\n    conf[\"wandb_entity\"] = args.wandb_entity\n    conf[\"local_rank\"] = args.local_rank\n    conf[\"deepspeed\"] = args.deepspeed\n    conf[\"resume_from_checkpoint\"] = args.resume_from_checkpoint\n    if args.rng_seed is not None:\n        conf[\"rng_seed\"] = args.rng_seed\n    conf[\"show_dataset_stats\"] = args.show_dataset_stats\n\n    # get the world size in deepspeed\n    if conf[\"deepspeed\"]:\n        conf[\"world_size\"] = int(os.getenv(\"WORLD_SIZE\", default=\"1\"))\n    else:\n        conf[\"world_size\"] = 1\n\n    # Override config from command-line\n    parser = argparse.ArgumentParser()\n    for key, value in conf.items():\n        type_ = type(value) if value is not None else str\n        if type_ == bool:\n            type_ = _strtobool\n        parser.add_argument(f\"--{key}\", type=type_, default=value)\n        # Allow --no-{key}  to remove it completely\n        parser.add_argument(f\"--no-{key}\", dest=key, action=\"store_const\", const=None)\n\n    return parser.parse_args(remaining)\n\n\ndef tokenizer_sanity_check(tokenizer):\n    print(\"Tokenizer sanity check:\")\n    print(f\"Type: {type(tokenizer).__name__}\")\n\n    print(\"special_tokens_map:\", tokenizer.special_tokens_map)\n\n    print(f\"bos_token='{tokenizer.bos_token}', bos_token_id={tokenizer.bos_token_id}\")\n    print(f\"eos_token='{tokenizer.eos_token}', eos_token_id={tokenizer.eos_token_id}\")\n\n    from model_training.custom_datasets.formatting import QA_SPECIAL_TOKENS, create_dataset_entry_qa\n\n    ds_entry = create_dataset_entry_qa(\n        mode=\"sft\", questions=[\"Q1\", \"Q2\"], answers=[\"A1\", \"A2\"], lang=\"en\", context=\"ctx\"\n    )\n    in_text = ds_entry.get_formatted(\n        tokenizer.eos_token,\n        use_system_tag=True,\n        system_property_dropout=0,\n        system_add_length=True,\n    )\n    in_text = \"\".join(in_text)\n\n    prompter_token_id = tokenizer.convert_tokens_to_ids(QA_SPECIAL_TOKENS[\"Question\"])\n    assistant_token_id = tokenizer.convert_tokens_to_ids(QA_SPECIAL_TOKENS[\"Answer\"])\n    print(f\"{prompter_token_id=}, {assistant_token_id=}\")\n\n    tr = tokenizer(in_text, max_length=1024, pad_to_max_length=False, truncation=True)\n\n    message_indices = []\n    i = -1\n    for id in tr.input_ids:\n        if id in (prompter_token_id, assistant_token_id):\n            i += 1\n        message_indices.append(i)\n\n    print(\"encoding result:\", tr)\n    for i, xs in enumerate(tr.input_ids):\n        decoded = tokenizer.decode(xs)\n        print(f'{i}: {xs} -> \"{decoded}\"')\n\n    print(\"message_indices:\", message_indices)\n\n\ndef main():\n    training_conf = argument_parsing()\n    if not training_conf.deepspeed or training_conf.local_rank == 0:\n        print(f\"trainig_conf = {training_conf}\")\n\n    output_dir = (\n        training_conf.output_dir\n        if training_conf.output_dir\n        else f\"{training_conf.model_name}-{training_conf.log_dir}-finetuned\"\n    )\n\n    optimizer = OptimizerNames.ADAMW_BNB if training_conf.quantization else OptimizerNames.ADAMW_HF\n\n    # needs to happen before model loading in case of stage 3 training\n    args = TrainingArguments(\n        output_dir=output_dir,\n        num_train_epochs=training_conf.num_train_epochs,\n        warmup_steps=training_conf.warmup_steps,\n        learning_rate=float(training_conf.learning_rate),\n        deepspeed=training_conf.deepspeed_config if training_conf.deepspeed else None,\n        optim=optimizer,\n        fp16=training_conf.dtype in [\"fp16\", \"float16\"],\n        bf16=training_conf.dtype in [\"bf16\", \"bfloat16\"],\n        local_rank=training_conf.local_rank,\n        gradient_checkpointing=training_conf.gradient_checkpointing,\n        gradient_accumulation_steps=training_conf.gradient_accumulation_steps,\n        per_device_train_batch_size=training_conf.per_device_train_batch_size,\n        per_device_eval_batch_size=training_conf.per_device_eval_batch_size,\n        adam_beta1=training_conf.adam_beta1,\n        adam_beta2=training_conf.adam_beta2,\n        adam_epsilon=float(training_conf.adam_epsilon),\n        weight_decay=training_conf.weight_decay,\n        max_grad_norm=training_conf.max_grad_norm,\n        logging_steps=training_conf.logging_steps,\n        save_total_limit=training_conf.save_total_limit,\n        evaluation_strategy=\"steps\",\n        eval_steps=training_conf.eval_steps,\n        save_strategy=training_conf.save_strategy,\n        save_steps=training_conf.save_steps,\n        eval_accumulation_steps=training_conf.eval_accumulation_steps,\n        resume_from_checkpoint=training_conf.resume_from_checkpoint,\n        report_to=\"wandb\" if training_conf.log_wandb else None,\n    )\n\n    init_rng(training_conf)\n\n    tokenizer = get_tokenizer(training_conf)\n\n    if not training_conf.deepspeed or training_conf.local_rank == 0:\n        tokenizer_sanity_check(tokenizer)\n\n    train_collate_fn = DialogueDataCollator(\n        tokenizer,\n        max_length=training_conf.max_length,\n        random_offset_probability=training_conf.random_offset_probability,\n        label_masking=training_conf.label_masking,\n        samples_mixing=training_conf.samples_mixing,\n        pad_to_multiple_of=16,\n        use_system_prefix=training_conf.use_system_prefix,\n        system_prefix=training_conf.system_prefix,\n        use_system_tag=training_conf.use_system_tag,\n        system_property_dropout=training_conf.system_property_dropout,\n        system_add_length=training_conf.system_add_length,\n    )\n\n    if training_conf.val_max_length is None:\n        training_conf.val_max_length = training_conf.max_length\n\n    eval_collate_fn = DialogueDataCollator(\n        tokenizer,\n        max_length=training_conf.val_max_length,\n        random_offset_probability=training_conf.random_offset_probability,\n        label_masking=training_conf.label_masking,\n        samples_mixing=False,\n        use_system_prefix=training_conf.use_system_prefix,\n        system_prefix=training_conf.system_prefix,\n        use_system_tag=training_conf.use_system_tag,\n        system_property_dropout=training_conf.system_property_dropout,\n        system_add_length=training_conf.system_add_length,\n    )\n\n    train, evals = get_dataset(training_conf)\n    show_dataset_stats = (training_conf.verbose or training_conf.show_dataset_stats) and (\n        not training_conf.deepspeed or training_conf.local_rank == 0\n    )\n    if show_dataset_stats:\n        print(\"Training dataset sizes (before sampling):\")\n        total = len(train)\n        for d in train.datasets:\n            if isinstance(d, Subset):\n                name = f\"Subset of {type(d.dataset).__name__}\"\n                if hasattr(d.dataset, \"name\"):\n                    name += f\" ({d.dataset.name})\"\n            else:\n                name = type(d).__name__\n                if hasattr(d, \"name\"):\n                    name += f\" ({d.name})\"\n            print(f\"{name}: {len(d)} ({len(d) / total:.2%})\")\n\n            # ensure that all entries can be formatted\n            # for x in d:\n            #     if isinstance(x, DatasetEntry):\n            #         x.get_formatted(\"sft\", \"<eos>\")\n\n        print(f\"\\nTotal train: {total}\")\n        print(\"-\" * 80)\n        print(\"Evaluation set sizes:\")\n        total_eval = sum(len(x) for x in evals.values())\n        for k, d in evals.items():\n            print(f\"{k}: {len(d)} ({len(d) / total_eval:.2%})\")\n        print(f\"\\nTotal eval: {total_eval}\")\n        print(\"-\" * 80)\n\n    if training_conf.use_custom_sampler:\n        samples_length = None\n        if training_conf.sort_by_length:\n            samples_length = list(\n                map(\n                    lambda x: train_collate_fn.process_one(x, return_length=True),\n                    tqdm(train, desc=\"Calculating lengths per sample\"),\n                )\n            )\n\n        sampler = PerDatasetSampler.build_sampler_from_config(\n            training_conf,\n            train.datasets,\n            rank=training_conf.local_rank,\n            world_size=training_conf.world_size,\n            samples_length=samples_length,\n            verbose=show_dataset_stats,\n        )\n    else:\n        sampler = None\n\n    metrics, preprocess_fns = get_metrics(training_conf, tokenizer)\n    model = get_model(training_conf, tokenizer)\n\n    superhot = RopePatch.from_config(training_conf) if training_conf.superhot else None\n    if superhot:\n        superhot.patch(model)\n\n    print(f\"rope_scaling: {model.config.rope_scaling}\")\n    print(f\"max_position_embeddings: {model.config.max_position_embeddings}\")\n\n    if training_conf.peft_model:\n        print(\"Using PEFT model\")\n        model = peft_model(model, training_conf)\n\n    if training_conf.quantization:\n        import bitsandbytes  # This is noisy, so delay importing until after argument parsing so it doesn't make --help noisy\n\n        for module in model.modules():\n            if isinstance(module, torch.nn.Embedding):\n                bitsandbytes.optim.GlobalOptimManager.get_instance().register_module_override(\n                    module, \"weight\", {\"optim_bits\": 32}\n                )\n\n    if training_conf.fuse_gelu:\n        model = fuse_gelu(model)\n\n    if not training_conf.log_wandb:\n        os.environ[\"WANDB_MODE\"] = \"offline\"\n\n    if training_conf.log_wandb and (not training_conf.deepspeed or training_conf.local_rank == 0):\n        import wandb\n\n        wandb_name = training_conf.model_name.replace(os.getenv(\"HOME\", \"/home/ubuntu\"), \"\")\n        wandb.init(\n            project=\"supervised-finetuning\",\n            entity=training_conf.wandb_entity,\n            resume=training_conf.resume_from_checkpoint,\n            name=f\"{wandb_name}-{training_conf.log_dir}-finetuned\",\n            config=training_conf,\n        )\n        wandb.config[\"_max_length\"] = training_conf.max_length\n        wandb.config[\"_val_max_length\"] = training_conf.val_max_length\n\n    trainer = SFTTrainer(\n        model=model,\n        args=args,\n        sampler=sampler,\n        train_collate_fn=train_collate_fn,\n        loss_function=training_conf.loss_fn,\n        poly_eps=training_conf.poly_eps,\n        train_dataset=train,\n        eval_dataset=evals,\n        data_collator=eval_collate_fn,\n        tokenizer=tokenizer,\n        compute_metrics=partial(compute_metrics, metrics=metrics, preprocess_fns=preprocess_fns),\n        preprocess_logits_for_metrics=preprocess_logits_for_metrics,\n    )\n    trainer.train(resume_from_checkpoint=training_conf.resume_from_checkpoint)\n    trainer.save_model()\n    tokenizer.save_pretrained(output_dir)\n\n\nif __name__ == \"__main__\":\n    main()\n", "model/model_training/to_triton.py": "import os\nfrom argparse import Namespace\nfrom string import Template\n\nimport torch\nimport transformers\nfrom torch import nn\nfrom trainer_rl import argument_parsing\nfrom utils.utils import get_model\n\n\nclass SFTLogitsModel(nn.Module):\n    def __init__(self, model):\n        super().__init__()\n        self.model = model\n\n    def forward(self, *args, **kwargs):\n        return self.model(*args, **kwargs).logits\n\n\nclass RMLogitsModel(nn.Module):\n    def __init__(self, model):\n        super().__init__()\n        self.model = model\n\n    def forward(self, *args, **kwargs):\n        return self.model(*args, **kwargs).logits[:, 0]\n\n\ndef load_model_and_tokenizer(triton_mode, config, device=\"cuda:0\"):\n    tokenizer = transformers.AutoTokenizer.from_pretrained(config.model_name)\n\n    # For llama ...\n    if tokenizer.pad_token_id == tokenizer.eos_token_id:\n        tokenizer.add_special_tokens({\"pad_token\": \"<|padding|>\"})\n\n    print(\"len tokenizer\", len(tokenizer))\n\n    # disable flash attention for triton\n    config.use_flash_attention = False\n\n    model = get_model(config, tokenizer, pad_vocab_size_to_multiple_of=1, check_freeze_layer=False)\n    model.to(device)\n    model.eval()\n\n    if triton_mode == \"sft\":\n        model = SFTLogitsModel(model)\n    elif triton_mode == \"rm\":\n        model = RMLogitsModel(model)\n    else:\n        raise ValueError(f\"Unknown mode {triton_mode}\")\n\n    return model, tokenizer\n\n\ndef write_traced_module(\n    traced_script_module,\n    model_name,\n    dtype=\"fp16\",\n    output_dir=\"model_store_sft\",\n    config_template=\"configs/triton_config_sft.pbtxt\",\n):\n    model_dir = os.path.join(output_dir, model_name, \"1\")\n    os.makedirs(model_dir, exist_ok=True)\n    traced_script_module.save(os.path.join(model_dir, \"traced-model.pt\"))\n\n    with open(config_template) as f:\n        template = Template(f.read())\n\n    if dtype == \"float16\":\n        dtype = \"fp16\"\n    elif dtype == \"float32\":\n        dtype = \"fp32\"\n    elif dtype == \"bfloat16\":\n        dtype = \"bf16\"\n\n    config = template.substitute(\n        {\n            \"model_name\": model_name,\n            \"dtype\": dtype,\n            \"output_type\": f\"TYPE_{dtype.upper()}\",\n        }\n    )\n\n    with open(os.path.join(output_dir, model_name, \"config.pbtxt\"), \"w\") as f:\n        f.write(config)\n\n\ndef trace_model(model, tokenizer, device=\"cuda:0\", trace_example=\"reward model's hash\", max_length=512):\n    inputs = tokenizer(trace_example, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\")\n    inputs = {k: v.to(device) for k, v in inputs.items() if k != \"token_type_ids\"}\n    outputs = model(**inputs)\n    print(f\"Output shape: {outputs.shape}\")\n\n    traced_script_module = torch.jit.trace(model, (inputs[\"input_ids\"], inputs[\"attention_mask\"]))\n    return traced_script_module\n\n\ndef main():\n    conf = argument_parsing(triton_mode=None, triton_output_dir=None)\n\n    if conf.triton_mode == \"sft\":\n        config = Namespace(**conf.sft_config)\n    elif conf.triton_mode == \"rm\":\n        config = Namespace(**conf.rank_config)\n\n    device = torch.device(\"cuda:0\")\n\n    model, tokenizer = load_model_and_tokenizer(conf.triton_mode, config, device=device)\n\n    traced_script_module = trace_model(model, tokenizer, device=device)\n\n    model_name = config.model_name.replace(\"/\", \"-\")\n    write_traced_module(\n        traced_script_module,\n        model_name,\n        config.dtype,\n        config_template=f\"configs/triton_config_{conf.triton_mode}.pbtxt\",\n        output_dir=f\".triton_models/model_store_{conf.triton_mode}\",\n    )\n\n\nif __name__ == \"__main__\":\n    main()\n", "model/model_training/models/patching_llama.py": "import math\nimport warnings\nfrom typing import Optional, Tuple\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom transformers.models.llama.modeling_llama import LlamaAttention, apply_rotary_pos_emb, repeat_kv\n\nfrom .patching_utils import compute_flash_attention\n\n\n# adapted from https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L185\ndef llama_forward_with_flash_attn(\n    self: LlamaAttention,\n    flash_attn: nn.Module,  # flash_attn.modules.mha.FlashSelfAttention\n    hidden_states: torch.Tensor,\n    attention_mask: Optional[torch.Tensor] = None,\n    position_ids: Optional[torch.LongTensor] = None,\n    past_key_value: Optional[Tuple[torch.Tensor]] = None,\n    output_attentions: bool = False,\n    use_cache: bool = False,\n) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n    bsz, q_len, _ = hidden_states.size()\n\n    if output_attentions:\n        warnings.warn(\"Output attentions is not supported for patched `LlamaAttention`, returning `None` instead.\")\n    if self.config.pretraining_tp > 1:\n        key_value_slicing = (self.num_key_value_heads * self.head_dim) // self.config.pretraining_tp\n        query_slices = self.q_proj.weight.split((self.num_heads * self.head_dim) // self.config.pretraining_tp, dim=0)\n        key_slices = self.k_proj.weight.split(key_value_slicing, dim=0)\n        value_slices = self.v_proj.weight.split(key_value_slicing, dim=0)\n\n        query_states = [F.linear(hidden_states, query_slices[i]) for i in range(self.config.pretraining_tp)]\n        query_states = torch.cat(query_states, dim=-1)\n\n        key_states = [F.linear(hidden_states, key_slices[i]) for i in range(self.config.pretraining_tp)]\n        key_states = torch.cat(key_states, dim=-1)\n\n        value_states = [F.linear(hidden_states, value_slices[i]) for i in range(self.config.pretraining_tp)]\n        value_states = torch.cat(value_states, dim=-1)\n\n    else:\n        query_states = self.q_proj(hidden_states)\n        key_states = self.k_proj(hidden_states)\n        value_states = self.v_proj(hidden_states)\n\n    query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n    key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n    value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n\n    kv_seq_len = key_states.shape[-2]\n    if past_key_value is not None:\n        kv_seq_len += past_key_value[0].shape[-2]\n    cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)\n    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n\n    if past_key_value is not None:\n        # reuse k, v, self_attention\n        key_states = torch.cat([past_key_value[0], key_states], dim=2)\n        value_states = torch.cat([past_key_value[1], value_states], dim=2)\n\n    past_key_value = (key_states, value_states) if use_cache else None\n\n    # repeat k/v heads if n_kv_heads < n_heads\n    key_states = repeat_kv(key_states, self.num_key_value_groups)\n    value_states = repeat_kv(value_states, self.num_key_value_groups)\n\n    if (\n        query_states.shape == key_states.shape\n    ):  # and (attention_mask is None or attention_mask[:, 0, -1, 0].min() >= 0):\n        if attention_mask is not None:\n            attention_mask = attention_mask[:, 0, -1]\n\n        flash_attn.train(self.training)\n        out_dtype = value_states.dtype\n        q, k, v = (\n            query_states.transpose(1, 2),\n            key_states.transpose(1, 2),\n            value_states.transpose(1, 2),\n        )\n        attn_output = compute_flash_attention(flash_attn, q, k, v, attention_mask)\n        attn_output = attn_output.transpose(1, 2).to(out_dtype)\n\n        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n            raise ValueError(\n                f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n                f\" {attn_output.size()}\"\n            )\n    else:\n        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n\n        if attn_weights.size() != (bsz, self.num_heads, q_len, kv_seq_len):\n            raise ValueError(\n                f\"Attention weights should be of size {(bsz, self.num_heads, q_len, kv_seq_len)}, but is\"\n                f\" {attn_weights.size()}\"\n            )\n\n        if attention_mask is not None:\n            if attention_mask.size() != (bsz, 1, q_len, kv_seq_len):\n                raise ValueError(\n                    f\"Attention mask should be of size {(bsz, 1, q_len, kv_seq_len)}, but is {attention_mask.size()}\"\n                )\n            attn_weights = attn_weights + attention_mask\n\n        # upcast attention to fp32\n        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n        attn_output = torch.matmul(attn_weights, value_states)\n\n        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n            raise ValueError(\n                f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n                f\" {attn_output.size()}\"\n            )\n\n    attn_output = attn_output.transpose(1, 2).contiguous()\n    attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n\n    if self.config.pretraining_tp > 1:\n        attn_output = attn_output.split(self.hidden_size // self.config.pretraining_tp, dim=2)\n        o_proj_slices = self.o_proj.weight.split(self.hidden_size // self.config.pretraining_tp, dim=1)\n        attn_output = sum([F.linear(attn_output[i], o_proj_slices[i]) for i in range(self.config.pretraining_tp)])\n    else:\n        attn_output = self.o_proj(attn_output)\n\n    return attn_output, None, past_key_value\n", "model/model_training/models/patching_neox.py": "import torch\nimport torch.nn as nn\nimport transformers\n\nfrom .patching_utils import compute_flash_attention\n\n\ndef neox_forward_with_flash_attn(\n    self: transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXAttention,\n    flash_attn: nn.Module,  # flash_attn.modules.mha.FlashSelfAttention\n    query: torch.Tensor,\n    key: torch.Tensor,\n    value: torch.Tensor,\n    attention_mask=None,\n    head_mask=None,\n):\n    # query, key, value: [bs, num_attention_heads, seq_len, attn_head_size]\n    if query.shape == key.shape:\n        flash_attn.train(self.training)\n        out_dtype = value.dtype\n        q, k, v = query.transpose(1, 2), key.transpose(1, 2), value.transpose(1, 2)\n        if attention_mask is not None:\n            attention_mask = attention_mask[:, 0, 0, :]\n        out = compute_flash_attention(flash_attn, q, k, v, attention_mask)\n        out = out.transpose(1, 2).to(out_dtype)\n        return out, None\n    else:\n        return self.old_forward(query, key, value, attention_mask, head_mask)\n", "model/model_training/models/peft_modeling.py": "from dataclasses import dataclass\nfrom pathlib import Path\n\nimport torch\nfrom huggingface_hub import hf_hub_download\nfrom model_training.utils.utils import get_all_linear_layers, get_model, get_tokenizer, merge_dicts\nfrom peft import LoraConfig, PeftModel, PrefixTuningConfig, get_peft_model, prepare_model_for_int8_training\n\n\ndef load_peft_model(model, peft_model_path, tokenizer):\n    model.resize_token_embeddings(len(tokenizer))\n    model.config.eos_token_id = tokenizer.eos_token_id\n    model.config.bos_token_id = tokenizer.bos_token_id\n    model.config.pad_token_id = tokenizer.pad_token_id\n    model = PeftModel.from_pretrained(\n        model,\n        peft_model_path,\n        torch_dtype=model.dtype,\n    )\n    model.eos_token_id = tokenizer.eos_token_id\n    try:\n        extra_embeds = hf_hub_download(peft_model_path, \"extra_embeddings.pt\")\n        embed_weights = torch.load(extra_embeds, map_location=model.device)\n        model.base_model.model.model.embed_tokens.weight[\n            len(tokenizer) - embed_weights.shape[0] :, :\n        ] = embed_weights.to(model.base_model.model.model.embed_tokens.weight.dtype)\n    except Exception:\n        print(\"Warning:Extra embeddings not added. This is expected if adapter file contains WTE\")\n\n    return model\n\n\ndef prepare_model_for_gradient_checkpointing(model):\n    r\"\"\"\n    Prepares the model for gradient checkpointing if necessary\n    \"\"\"\n    if not getattr(model, \"is_loaded_in_8bit\", False):\n        if hasattr(model, \"enable_input_require_grads\"):\n            model.enable_input_require_grads()\n        else:\n\n            def make_inputs_require_grad(module, input, output):\n                output.requires_grad_(True)\n\n            model.get_input_embeddings().register_forward_hook(make_inputs_require_grad)\n    return model\n\n\ndef peft_model(model, training_config):\n    peft_config = training_config.peft_config\n    peft_type = peft_config.pop(\"peft_type\", \"lora\")\n    if peft_type == \"lora\":\n        default_args = {\n            \"r\": 16,\n            \"lora_alpha\": 32,\n            \"target_modules\": \"all\",\n            \"lora_dropout\": 0.05,\n            \"bias\": \"none\",\n            \"task_type\": \"CAUSAL_LM\",\n            \"modules_to_save\": [\"wte\", \"lm_head\"],\n        }\n        kwargs = merge_dicts(default_args, peft_config)\n        if kwargs.get(\"target_modules\") == \"all\":\n            kwargs.update({\"target_modules\": get_all_linear_layers(model)})\n        config = LoraConfig(**kwargs)\n    elif peft_type == \"prefix-tuning\":\n        default_args = {\n            \"num_virtual_tokens\": 30,\n            \"prefix_projection\": True,\n            \"encoder_hidden_size\": 1024,\n            \"task_type\": \"CAUSAL_LM\",\n        }\n        kwargs = merge_dicts(default_args, peft_config)\n        config = PrefixTuningConfig(**kwargs)\n    else:\n        raise ValueError(\"peft_method config is lora or prefix-tuning\")\n    model = get_peft_model(model, config)\n\n    if training_config.int8_training:\n        model = prepare_model_for_int8_training(model)\n\n    if training_config.gradient_checkpointing:\n        model = prepare_model_for_gradient_checkpointing(model)\n    model.print_trainable_parameters()\n    return model\n\n\n@dataclass\nclass SaveLoraConfig:\n    dtype: torch.dtype = torch.float16\n    is_reward_model: bool = False\n    quantization: bool = False\n    seq2seqmodel: bool = False\n    freeze_layer: bool = False\n    residual_dropout: float = 0\n    use_flash_attention: bool = False\n    adapter_save_path: str = \"adapter\"\n    cache_dir: str = \"\"\n    model_name: str = \"\"\n    torch_ckpt_path: str = \"\"\n    peft_type: str = \"lora\"\n\n\ndef save_adapter_model_from_ckpt(save_config: SaveLoraConfig):\n    tokenizer = get_tokenizer(save_config)\n    model = get_model(save_config, tokenizer)\n    model = peft_model(model)\n    model.load_state_dict(torch.load(save_config.torch_ckpt_path))\n    vocab_size = tokenizer.vocab_size\n    num_special_tokens = len(tokenizer.additional_special_tokens)\n\n    new_embs = model.state_dict()[\"base_model.model.model.embed_tokens.weight\"][\n        vocab_size : vocab_size + num_special_tokens, :\n    ].clone()\n    new_embs = new_embs.to(save_config.dtype)\n    model.save_pretrained(save_config.adapter_save_path, torch_dtype=save_config.dtype)\n    tokenizer.save_pretrained(save_config.adapter_save_path)\n    torch.save(new_embs, Path(save_config.adapter_save_path).joinpath(\"extra_embeddings.pt\"))\n", "model/model_training/models/prefix_llama.py": "# Copyright 2022 EleutherAI and the HuggingFace Inc. team. All rights reserved.\n#\n# This code is based on EleutherAI's GPT-NeoX library and the GPT-NeoX\n# and OPT implementations in this library. It has been modified from its\n# original forms to accommodate minor architectural differences compared\n# to GPT-NeoX and OPT used by the Meta AI team that trained the model.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\" PyTorch LLaMA model.\"\"\"\nimport math\nfrom typing import List, Optional, Tuple, Union\n\nimport torch\nimport torch.utils.checkpoint\nfrom torch import nn\nfrom torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\nfrom transformers import LlamaConfig\nfrom transformers.activations import ACT2FN\nfrom transformers.modeling_outputs import (\n    BaseModelOutputWithPast,\n    CausalLMOutputWithPast,\n    SequenceClassifierOutputWithPast,\n)\nfrom transformers.modeling_utils import PreTrainedModel\nfrom transformers.utils import (\n    add_start_docstrings,\n    add_start_docstrings_to_model_forward,\n    logging,\n    replace_return_docstrings,\n)\n\nlogger = logging.get_logger(__name__)\n\n_CONFIG_FOR_DOC = \"LlamaConfig\"\n\n\ndef _make_causal_mask(input_ids_shape: torch.Size, dtype: torch.dtype, past_key_values_length: int = 0):\n    \"\"\"\n    Make causal mask used for bi-directional self-attention.\n    \"\"\"\n    bsz, tgt_len = input_ids_shape\n    mask = torch.full((tgt_len, tgt_len), torch.tensor(torch.finfo(dtype).min))\n    mask_cond = torch.arange(mask.size(-1))\n    mask.masked_fill_(mask_cond < (mask_cond + 1).view(mask.size(-1), 1), 0)\n    mask = mask.to(dtype)\n\n    if past_key_values_length > 0:\n        mask = torch.cat([torch.zeros(tgt_len, past_key_values_length, dtype=dtype), mask], dim=-1)\n    return mask[None, None, :, :].expand(bsz, 1, tgt_len, tgt_len + past_key_values_length)\n\n\ndef _expand_mask(mask: torch.Tensor, dtype: torch.dtype, tgt_len: Optional[int] = None):\n    \"\"\"\n    Expands attention_mask from `[bsz, seq_len]` to `[bsz, 1, tgt_seq_len, src_seq_len]`.\n    \"\"\"\n    bsz, src_len = mask.size()\n    tgt_len = tgt_len if tgt_len is not None else src_len\n\n    expanded_mask = mask[:, None, None, :].expand(bsz, 1, tgt_len, src_len).to(dtype)\n\n    inverted_mask = 1.0 - expanded_mask\n\n    return inverted_mask.masked_fill(inverted_mask.to(torch.bool), torch.finfo(dtype).min)\n\n\nclass LlamaRMSNorm(nn.Module):\n    def __init__(self, hidden_size, eps=1e-6):\n        \"\"\"\n        LlamaRMSNorm is equivalent to T5LayerNorm\n        \"\"\"\n        super().__init__()\n        self.weight = nn.Parameter(torch.ones(hidden_size))\n        self.variance_epsilon = eps\n\n    def forward(self, hidden_states):\n        variance = hidden_states.to(torch.float32).pow(2).mean(-1, keepdim=True)\n        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n\n        # convert into half-precision if necessary\n        if self.weight.dtype in [torch.float16, torch.bfloat16]:\n            hidden_states = hidden_states.to(self.weight.dtype)\n\n        return self.weight * hidden_states\n\n\nclass LlamaRotaryEmbedding(torch.nn.Module):\n    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None):\n        super().__init__()\n        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float().to(device) / dim))\n        self.register_buffer(\"inv_freq\", inv_freq)\n\n        # Build here to make `torch.jit.trace` work.\n        self.max_seq_len_cached = max_position_embeddings\n        t = torch.arange(self.max_seq_len_cached, device=self.inv_freq.device, dtype=self.inv_freq.dtype)\n        freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)\n        # Different from paper, but it uses a different permutation in order to obtain the same calculation\n        emb = torch.cat((freqs, freqs), dim=-1)\n        self.register_buffer(\"cos_cached\", emb.cos()[None, None, :, :], persistent=False)\n        self.register_buffer(\"sin_cached\", emb.sin()[None, None, :, :], persistent=False)\n\n    def forward(self, x, seq_len=None):\n        # x: [bs, num_attention_heads, seq_len, head_size]\n        # This `if` block is unlikely to be run after we build sin/cos in `__init__`. Keep the logic here just in case.\n        if seq_len > self.max_seq_len_cached:\n            self.max_seq_len_cached = seq_len\n            t = torch.arange(self.max_seq_len_cached, device=x.device, dtype=self.inv_freq.dtype)\n            freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)\n            # Different from paper, but it uses a different permutation in order to obtain the same calculation\n            emb = torch.cat((freqs, freqs), dim=-1).to(x.device)\n            self.register_buffer(\"cos_cached\", emb.cos()[None, None, :, :], persistent=False)\n            self.register_buffer(\"sin_cached\", emb.sin()[None, None, :, :], persistent=False)\n        return (\n            self.cos_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n            self.sin_cached[:, :, :seq_len, ...].to(dtype=x.dtype),\n        )\n\n\ndef rotate_half(x):\n    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n    x1 = x[..., : x.shape[-1] // 2]\n    x2 = x[..., x.shape[-1] // 2 :]\n    return torch.cat((-x2, x1), dim=-1)\n\n\ndef apply_rotary_pos_emb(q, k, cos, sin, offset: int = 0):\n    cos = cos[..., offset : q.shape[-2] + offset, :]\n    sin = sin[..., offset : q.shape[-2] + offset, :]\n    q_embed = (q * cos) + (rotate_half(q) * sin)\n    k_embed = (k * cos) + (rotate_half(k) * sin)\n    return q_embed, k_embed\n\n\nclass LlamaMLP(nn.Module):\n    def __init__(\n        self,\n        hidden_size: int,\n        intermediate_size: int,\n        hidden_act: str,\n    ):\n        super().__init__()\n        self.gate_proj = nn.Linear(hidden_size, intermediate_size, bias=False)\n        self.down_proj = nn.Linear(intermediate_size, hidden_size, bias=False)\n        self.up_proj = nn.Linear(hidden_size, intermediate_size, bias=False)\n        self.act_fn = ACT2FN[hidden_act]\n\n    def forward(self, x):\n        return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n\n\nclass LlamaAttention(nn.Module):\n    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n\n    def __init__(\n        self,\n        hidden_size: int,\n        num_heads: int,\n    ):\n        super().__init__()\n        self.hidden_size = hidden_size\n        self.num_heads = num_heads\n        self.head_dim = hidden_size // num_heads\n\n        if (self.head_dim * num_heads) != self.hidden_size:\n            raise ValueError(\n                f\"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}\"\n                f\" and `num_heads`: {num_heads}).\"\n            )\n        self.q_proj = nn.Linear(\n            hidden_size,\n            num_heads * self.head_dim,\n            bias=False,\n        )\n        self.k_proj = nn.Linear(\n            hidden_size,\n            num_heads * self.head_dim,\n            bias=False,\n        )\n        self.v_proj = nn.Linear(\n            hidden_size,\n            num_heads * self.head_dim,\n            bias=False,\n        )\n        self.o_proj = nn.Linear(\n            num_heads * self.head_dim,\n            hidden_size,\n            bias=False,\n        )\n        self.rotary_emb = LlamaRotaryEmbedding(self.head_dim)\n\n    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        output_attentions: bool = False,\n        use_cache: bool = False,\n    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n        \"\"\"Input shape: Batch x Time x Channel\"\"\"\n\n        bsz, q_len, _ = hidden_states.size()\n\n        query_states = self.q_proj(hidden_states).view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n        key_states = self.k_proj(hidden_states).view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n        value_states = self.v_proj(hidden_states).view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n\n        kv_seq_len = key_states.shape[-2]\n        offset = 0\n        if past_key_value is not None:\n            offset = past_key_value[0].shape[-2]\n            kv_seq_len += offset\n        cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)\n        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, offset=offset)\n        # [bsz, nh, t, hd]\n\n        if past_key_value is not None:\n            # reuse k, v, self_attention\n            key_states = torch.cat([past_key_value[0], key_states], dim=2)\n            value_states = torch.cat([past_key_value[1], value_states], dim=2)\n\n        past_key_value = (key_states, value_states) if use_cache else None\n\n        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n\n        if attn_weights.size() != (bsz, self.num_heads, q_len, kv_seq_len):\n            raise ValueError(\n                f\"Attention weights should be of size {(bsz * self.num_heads, q_len, kv_seq_len)}, but is\"\n                f\" {attn_weights.size()}\"\n            )\n\n        if attention_mask is not None:\n            if attention_mask.size() != (bsz, 1, q_len, kv_seq_len):\n                raise ValueError(\n                    f\"Attention mask should be of size {(bsz, 1, q_len, kv_seq_len)}, but is {attention_mask.size()}\"\n                )\n            attn_weights = attn_weights + attention_mask\n            attn_weights = torch.max(attn_weights, torch.tensor(torch.finfo(attn_weights.dtype).min))\n\n        # upcast attention to fp32\n        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n        attn_output = torch.matmul(attn_weights, value_states)\n\n        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n            raise ValueError(\n                f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n                f\" {attn_output.size()}\"\n            )\n\n        attn_output = attn_output.transpose(1, 2)\n        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n\n        attn_output = self.o_proj(attn_output)\n\n        if not output_attentions:\n            attn_weights = None\n\n        return attn_output, attn_weights, past_key_value\n\n\nclass LlamaDecoderLayer(nn.Module):\n    def __init__(self, config: LlamaConfig):\n        super().__init__()\n        self.hidden_size = config.hidden_size\n        self.self_attn = LlamaAttention(\n            hidden_size=self.hidden_size,\n            num_heads=config.num_attention_heads,\n        )\n        self.mlp = LlamaMLP(\n            hidden_size=self.hidden_size,\n            intermediate_size=config.intermediate_size,\n            hidden_act=config.hidden_act,\n        )\n        self.input_layernorm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n        self.post_attention_layernorm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n\n    def forward(\n        self,\n        hidden_states: torch.Tensor,\n        attention_mask: Optional[torch.Tensor] = None,\n        output_attentions: Optional[bool] = False,\n        use_cache: Optional[bool] = False,\n        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n        \"\"\"\n        Args:\n            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n            attention_mask (`torch.FloatTensor`, *optional*): attention mask of size\n                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n            output_attentions (`bool`, *optional*):\n                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                returned tensors for more detail.\n            use_cache (`bool`, *optional*):\n                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n                (see `past_key_values`).\n            past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n        \"\"\"\n\n        residual = hidden_states\n\n        hidden_states = self.input_layernorm(hidden_states)\n\n        # Self Attention\n        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n            hidden_states=hidden_states,\n            past_key_value=past_key_value,\n            attention_mask=attention_mask,\n            output_attentions=output_attentions,\n            use_cache=use_cache,\n        )\n        hidden_states = residual + hidden_states\n\n        # Fully Connected\n        residual = hidden_states\n        hidden_states = self.post_attention_layernorm(hidden_states)\n        hidden_states = self.mlp(hidden_states)\n        hidden_states = residual + hidden_states\n\n        outputs = (hidden_states,)\n\n        if output_attentions:\n            outputs += (self_attn_weights,)\n\n        if use_cache:\n            outputs += (present_key_value,)\n\n        return outputs\n\n\nLLAMA_START_DOCSTRING = r\"\"\"\n    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n    etc.)\n\n    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n    and behavior.\n\n    Parameters:\n        config ([`LlamaConfig`]):\n            Model configuration class with all the parameters of the model. Initializing with a config file does not\n            load the weights associated with the model, only the configuration. Check out the\n            [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n\"\"\"\n\n\n@add_start_docstrings(\n    \"The bare LLaMA Model outputting raw hidden-states without any specific head on top.\",\n    LLAMA_START_DOCSTRING,\n)\nclass LlamaPreTrainedModel(PreTrainedModel):\n    config_class = LlamaConfig\n    base_model_prefix = \"model\"\n    supports_gradient_checkpointing = True\n    _no_split_modules = [\"LlamaDecoderLayer\"]\n    _keys_to_ignore_on_load_unexpected = [r\"decoder\\.version\"]\n\n    def _init_weights(self, module):\n        std = self.config.initializer_range\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=std)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=std)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n\n    def _set_gradient_checkpointing(self, module, value=False):\n        if isinstance(module, LlamaModel):\n            module.gradient_checkpointing = value\n\n\nLLAMA_INPUTS_DOCSTRING = r\"\"\"\n    Args:\n        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n            it.\n\n            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n            [`PreTrainedTokenizer.__call__`] for details.\n\n            [What are input IDs?](../glossary#input-ids)\n        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n\n            - 1 for tokens that are **not masked**,\n            - 0 for tokens that are **masked**.\n\n            [What are attention masks?](../glossary#attention-mask)\n\n            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n            [`PreTrainedTokenizer.__call__`] for details.\n\n            If `past_key_values` is used, optionally only the last `decoder_input_ids` have to be input (see\n            `past_key_values`).\n\n            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]\n            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more\n            information on the default strategy.\n\n            - 1 indicates the head is **not masked**,\n            - 0 indicates the head is **masked**.\n\n        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n            `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of shape\n            `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.\n\n            Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n            blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.\n\n            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n            `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n            model's internal embedding lookup matrix.\n        use_cache (`bool`, *optional*):\n            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n            `past_key_values`).\n        output_attentions (`bool`, *optional*):\n            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n            tensors for more detail.\n        output_hidden_states (`bool`, *optional*):\n            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n            more detail.\n        return_dict (`bool`, *optional*):\n            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n\"\"\"\n\n\n@add_start_docstrings(\n    \"The bare LLaMA Model outputting raw hidden-states without any specific head on top.\",\n    LLAMA_START_DOCSTRING,\n)\nclass LlamaModel(LlamaPreTrainedModel):\n    \"\"\"\n    Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer is a [`LlamaDecoderLayer`]\n\n    Args:\n        config: LlamaConfig\n    \"\"\"\n\n    def __init__(self, config: LlamaConfig):\n        super().__init__(config)\n        self.padding_idx = config.pad_token_id\n        self.vocab_size = config.vocab_size\n\n        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)\n        self.layers = nn.ModuleList([LlamaDecoderLayer(config) for _ in range(config.num_hidden_layers)])\n        self.norm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n\n        self.gradient_checkpointing = False\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_input_embeddings(self):\n        return self.embed_tokens\n\n    def set_input_embeddings(self, value):\n        self.embed_tokens = value\n\n    # Copied from transformers.models.bart.modeling_bart.BartDecoder._prepare_decoder_attention_mask\n    def _prepare_decoder_attention_mask(self, attention_mask, input_shape, inputs_embeds, past_key_values_length):\n        # create causal mask\n        # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n        combined_attention_mask = None\n        if input_shape[-1] > 1:\n            combined_attention_mask = _make_causal_mask(\n                input_shape, inputs_embeds.dtype, past_key_values_length=past_key_values_length\n            ).to(inputs_embeds.device)\n\n        if attention_mask is not None:\n            # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n            tgt_len = input_shape[-1]\n            expanded_attn_mask = _expand_mask(attention_mask, inputs_embeds.dtype, tgt_len=tgt_len).to(\n                inputs_embeds.device\n            )\n            combined_attention_mask = (\n                expanded_attn_mask if combined_attention_mask is None else expanded_attn_mask + combined_attention_mask\n            )\n\n        return combined_attention_mask\n\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        past_key_values: Optional[List[torch.FloatTensor]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) -> Union[Tuple, BaseModelOutputWithPast]:\n        r\"\"\"\n        Args:\n            input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n                Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you\n                provide it.\n\n                Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n                [`PreTrainedTokenizer.__call__`] for details.\n\n                [What are input IDs?](../glossary#input-ids)\n            attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n                Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n\n                - 1 for tokens that are **not masked**,\n                - 0 for tokens that are **masked**.\n\n                [What are attention masks?](../glossary#attention-mask)\n            past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n                Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n                shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of\n\n                Contains pre-computed hidden-states (key and values in the self-attention blocks and in the\n                cross-attention blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.\n\n                If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those\n                that don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of\n                all `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n            use_cache (`bool`, *optional*):\n                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n                (see `past_key_values`).\n            inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n                Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation.\n                This is useful if you want more control over how to convert `input_ids` indices into associated vectors\n                than the model's internal embedding lookup matrix.\n            output_attentions (`bool`, *optional*):\n                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                returned tensors for more detail.\n            output_hidden_states (`bool`, *optional*):\n                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n                for more detail.\n            return_dict (`bool`, *optional*):\n                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n        \"\"\"\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        use_cache = use_cache if use_cache is not None else self.config.use_cache\n\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        # retrieve input_ids and inputs_embeds\n        if input_ids is not None and inputs_embeds is not None:\n            raise ValueError(\"You cannot specify both decoder_input_ids and decoder_inputs_embeds at the same time\")\n        elif input_ids is not None:\n            batch_size, seq_length = input_ids.shape\n        elif inputs_embeds is not None:\n            batch_size, seq_length, _ = inputs_embeds.shape\n        else:\n            raise ValueError(\"You have to specify either decoder_input_ids or decoder_inputs_embeds\")\n        seq_length_with_past = seq_length\n        past_key_values_length = 0\n        if past_key_values is not None:\n            past_key_values_length = past_key_values[0][0].shape[2]\n            seq_length_with_past = seq_length_with_past + past_key_values_length\n        if inputs_embeds is None:\n            inputs_embeds = self.embed_tokens(input_ids)\n        # embed positions\n        if attention_mask is None:\n            attention_mask = torch.ones(\n                (batch_size, seq_length_with_past), dtype=torch.bool, device=inputs_embeds.device\n            )\n        attention_mask = self._prepare_decoder_attention_mask(\n            attention_mask, (batch_size, seq_length), inputs_embeds, past_key_values_length\n        )\n\n        hidden_states = inputs_embeds\n\n        if self.gradient_checkpointing and self.training:\n            if use_cache:\n                logger.warning_once(\n                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n                )\n                use_cache = False\n\n        # decoder layers\n        all_hidden_states = () if output_hidden_states else None\n        all_self_attns = () if output_attentions else None\n        next_decoder_cache = () if use_cache else None\n\n        for idx, decoder_layer in enumerate(self.layers):\n            if output_hidden_states:\n                all_hidden_states += (hidden_states,)\n\n            past_key_value = past_key_values[idx] if past_key_values is not None else None\n\n            if self.gradient_checkpointing and self.training:\n\n                def create_custom_forward(module):\n                    def custom_forward(*inputs):\n                        # None for past_key_value\n                        return module(*inputs)\n\n                    return custom_forward\n\n                layer_outputs = torch.utils.checkpoint.checkpoint(\n                    create_custom_forward(decoder_layer),\n                    hidden_states,\n                    attention_mask,\n                    output_attentions,\n                    False,\n                    past_key_value,\n                )\n            else:\n                layer_outputs = decoder_layer(\n                    hidden_states,\n                    attention_mask=attention_mask,\n                    past_key_value=past_key_value,\n                    output_attentions=output_attentions,\n                    use_cache=use_cache,\n                )\n\n            hidden_states = layer_outputs[0]\n\n            if use_cache:\n                next_decoder_cache += (layer_outputs[2 if output_attentions else 1],)\n\n            if output_attentions:\n                all_self_attns += (layer_outputs[1],)\n\n        hidden_states = self.norm(hidden_states)\n\n        # add hidden states from the last decoder layer\n        if output_hidden_states:\n            all_hidden_states += (hidden_states,)\n\n        next_cache = next_decoder_cache if use_cache else None\n        if not return_dict:\n            return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)\n        return BaseModelOutputWithPast(\n            last_hidden_state=hidden_states,\n            past_key_values=next_cache,\n            hidden_states=all_hidden_states,\n            attentions=all_self_attns,\n        )\n\n\nclass LlamaForCausalLM(LlamaPreTrainedModel):\n    _keys_to_ignore_on_load_missing = [r\"lm_head.weight\"]\n\n    def __init__(self, config):\n        super().__init__(config)\n        self.model = LlamaModel(config)\n\n        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_input_embeddings(self):\n        return self.model.embed_tokens\n\n    def set_input_embeddings(self, value):\n        self.model.embed_tokens = value\n\n    def get_output_embeddings(self):\n        return self.lm_head\n\n    def set_output_embeddings(self, new_embeddings):\n        self.lm_head = new_embeddings\n\n    def set_decoder(self, decoder):\n        self.model = decoder\n\n    def get_decoder(self):\n        return self.model\n\n    @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        past_key_values: Optional[List[torch.FloatTensor]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        labels: Optional[torch.LongTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) -> Union[Tuple, CausalLMOutputWithPast]:\n        r\"\"\"\n        Args:\n            input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n                Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you\n                provide it.\n\n                Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n                [`PreTrainedTokenizer.__call__`] for details.\n\n                [What are input IDs?](../glossary#input-ids)\n            attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n                Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n\n                - 1 for tokens that are **not masked**,\n                - 0 for tokens that are **masked**.\n\n                [What are attention masks?](../glossary#attention-mask)\n            past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n                Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n                shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of\n                shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`. The two additional\n                tensors are only required when the model is used as a decoder in a Sequence to Sequence model.\n\n                Contains pre-computed hidden-states (key and values in the self-attention blocks and in the\n                cross-attention blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.\n\n                If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those\n                that don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of\n                all `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n            inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n                Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation.\n                This is useful if you want more control over how to convert `input_ids` indices into associated vectors\n                than the model's internal embedding lookup matrix.\n            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n            use_cache (`bool`, *optional*):\n                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n                (see `past_key_values`).\n            output_attentions (`bool`, *optional*):\n                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                returned tensors for more detail.\n            output_hidden_states (`bool`, *optional*):\n                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n                for more detail.\n            return_dict (`bool`, *optional*):\n                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n\n        Returns:\n\n        Example:\n\n        ```python\n        >>> from transformers import AutoTokenizer, LlamaForCausalLM\n\n        >>> model = LlamaForCausalLM.from_pretrained(PATH_TO_CONVERTED_WEIGHTS)\n        >>> tokenizer = AutoTokenizer.from_pretrained(PATH_TO_CONVERTED_TOKENIZER)\n\n        >>> prompt = \"Hey, are you consciours? Can you talk to me?\"\n        >>> inputs = tokenizer(prompt, return_tensors=\"pt\")\n\n        >>> # Generate\n        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)\n        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n        \"Hey, are you consciours? Can you talk to me?\\nI'm not consciours, but I can talk to you.\"\n        ```\"\"\"\n\n        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n        output_hidden_states = (\n            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n        )\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n        outputs = self.model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            past_key_values=past_key_values,\n            inputs_embeds=inputs_embeds,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n\n        hidden_states = outputs[0]\n        logits = self.lm_head(hidden_states)\n\n        loss = None\n        if labels is not None:\n            # Shift so that tokens < n predict n\n            shift_logits = logits[..., :-1, :].contiguous()\n            shift_labels = labels[..., 1:].contiguous()\n            # Flatten the tokens\n            loss_fct = CrossEntropyLoss()\n            shift_logits = shift_logits.view(-1, self.config.vocab_size)\n            shift_labels = shift_labels.view(-1)\n            # Enable model/pipeline parallelism\n            shift_labels = shift_labels.to(shift_logits.device)\n            loss = loss_fct(shift_logits, shift_labels)\n\n        if not return_dict:\n            output = (logits,) + outputs[1:]\n            return (loss,) + output if loss is not None else output\n\n        return CausalLMOutputWithPast(\n            loss=loss,\n            logits=logits,\n            past_key_values=outputs.past_key_values,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )\n\n    def prepare_inputs_for_generation(\n        self, input_ids, past_key_values=None, attention_mask=None, inputs_embeds=None, **kwargs\n    ):\n        if past_key_values:\n            input_ids = input_ids[:, -1:]\n\n        # if `inputs_embeds` are passed, we only want to use them in the 1st generation step\n        if inputs_embeds is not None and past_key_values is None:\n            model_inputs = {\"inputs_embeds\": inputs_embeds}\n        else:\n            model_inputs = {\"input_ids\": input_ids}\n\n        model_inputs.update(\n            {\n                \"past_key_values\": past_key_values,\n                \"use_cache\": kwargs.get(\"use_cache\"),\n                \"attention_mask\": attention_mask,\n            }\n        )\n        return model_inputs\n\n    @staticmethod\n    def _reorder_cache(past_key_values, beam_idx):\n        reordered_past = ()\n        for layer_past in past_key_values:\n            reordered_past += (tuple(past_state.index_select(0, beam_idx) for past_state in layer_past),)\n        return reordered_past\n\n\n@add_start_docstrings(\n    \"\"\"\n    The LLaMa Model transformer with a sequence classification head on top (linear layer).\n\n    [`LlamaForSequenceClassification`] uses the last token in order to do the classification, as other causal models\n    (e.g. GPT-2) do.\n\n    Since it does classification on the last token, it requires to know the position of the last token. If a\n    `pad_token_id` is defined in the configuration, it finds the last token that is not a padding token in each row. If\n    no `pad_token_id` is defined, it simply takes the last value in each row of the batch. Since it cannot guess the\n    padding tokens when `inputs_embeds` are passed instead of `input_ids`, it does the same (take the last value in\n    each row of the batch).\n    \"\"\",\n    LLAMA_START_DOCSTRING,\n)\nclass LlamaForSequenceClassification(LlamaPreTrainedModel):\n    _keys_to_ignore_on_load_missing = [r\"lm_head.weight\"]\n\n    def __init__(self, config):\n        super().__init__(config)\n        self.num_labels = config.num_labels\n        self.model = LlamaModel(config)\n        self.score = nn.Linear(config.hidden_size, self.num_labels, bias=False)\n\n        # Initialize weights and apply final processing\n        self.post_init()\n\n    def get_input_embeddings(self):\n        return self.model.embed_tokens\n\n    def set_input_embeddings(self, value):\n        self.model.embed_tokens = value\n\n    @add_start_docstrings_to_model_forward(LLAMA_INPUTS_DOCSTRING)\n    def forward(\n        self,\n        input_ids: torch.LongTensor = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        past_key_values: Optional[List[torch.FloatTensor]] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        labels: Optional[torch.LongTensor] = None,\n        use_cache: Optional[bool] = None,\n        output_attentions: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n    ) -> Union[Tuple, SequenceClassifierOutputWithPast]:\n        r\"\"\"\n        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n        \"\"\"\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        transformer_outputs = self.model(\n            input_ids,\n            past_key_values=past_key_values,\n            attention_mask=attention_mask,\n            inputs_embeds=inputs_embeds,\n            use_cache=use_cache,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n        hidden_states = transformer_outputs[0]\n        logits = self.score(hidden_states)\n\n        if input_ids is not None:\n            batch_size = input_ids.shape[0]\n        else:\n            batch_size = inputs_embeds.shape[0]\n\n        if self.config.pad_token_id is None and batch_size != 1:\n            raise ValueError(\"Cannot handle batch sizes > 1 if no padding token is defined.\")\n        if self.config.pad_token_id is None:\n            sequence_lengths = -1\n        else:\n            if input_ids is not None:\n                sequence_lengths = (torch.ne(input_ids, self.config.pad_token_id).sum(-1) - 1).to(logits.device)\n            else:\n                sequence_lengths = -1\n\n        pooled_logits = logits[torch.arange(batch_size, device=logits.device), sequence_lengths]\n\n        loss = None\n        if labels is not None:\n            if self.config.problem_type is None:\n                if self.num_labels == 1:\n                    self.config.problem_type = \"regression\"\n                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n                    self.config.problem_type = \"single_label_classification\"\n                else:\n                    self.config.problem_type = \"multi_label_classification\"\n\n            if self.config.problem_type == \"regression\":\n                loss_fct = MSELoss()\n                if self.num_labels == 1:\n                    loss = loss_fct(pooled_logits.squeeze(), labels.squeeze())\n                else:\n                    loss = loss_fct(pooled_logits, labels)\n            elif self.config.problem_type == \"single_label_classification\":\n                loss_fct = CrossEntropyLoss()\n                loss = loss_fct(pooled_logits.view(-1, self.num_labels), labels.view(-1))\n            elif self.config.problem_type == \"multi_label_classification\":\n                loss_fct = BCEWithLogitsLoss()\n                loss = loss_fct(pooled_logits, labels)\n        if not return_dict:\n            output = (pooled_logits,) + transformer_outputs[1:]\n            return ((loss,) + output) if loss is not None else output\n\n        return SequenceClassifierOutputWithPast(\n            loss=loss,\n            logits=pooled_logits,\n            past_key_values=transformer_outputs.past_key_values,\n            hidden_states=transformer_outputs.hidden_states,\n            attentions=transformer_outputs.attentions,\n        )\n", "model/model_training/models/reward_model.py": "from dataclasses import dataclass\nfrom typing import Literal, Optional\n\nimport torch\nimport torch.nn as nn\nfrom transformers import AutoConfig, AutoModelForSequenceClassification\nfrom transformers.models.gpt_neox.modeling_gpt_neox import GPTNeoXConfig, GPTNeoXModel, GPTNeoXPreTrainedModel\nfrom transformers.utils import ModelOutput\n\n\nclass GPTNeoXRewardModelConfig(GPTNeoXConfig):\n    model_type = \"gpt_neox_reward_model\"\n\n    pooling: Literal[\"mean\", \"last\"]\n\n    def __init__(\n        self,\n        pooling: Literal[\"mean\", \"last\"] = \"last\",\n        **kwargs,\n    ):\n        super().__init__(**kwargs)\n        self.pooling = pooling or \"last\"\n\n\n@dataclass\nclass GPTNeoXRewardModelOutput(ModelOutput):\n    \"\"\"\n    Reward model output.\n\n    Args:\n        logits (`torch.FloatTensor` of shape `(batch_size, 1)`):\n            Reward score\n    \"\"\"\n\n    logits: torch.FloatTensor = None\n\n\nclass GPTNeoXRewardModel(GPTNeoXPreTrainedModel):\n    config_class = GPTNeoXRewardModelConfig\n\n    def __init__(self, config):\n        if type(config) == GPTNeoXConfig:\n            # When a normal GPTNeoX was loaded it will be converted into a reward model.\n            # The direct `type(config) == GPTNeoXConfig` comparison is used (instead of\n            # `isinstance()`) since the configuration class of the reward model is also\n            # derived form `GPTNeoXConfig`.\n            config = GPTNeoXRewardModelConfig.from_dict(config.to_dict())\n        super().__init__(config)\n\n        self.gpt_neox = GPTNeoXModel(config)\n        self.out_proj = nn.Linear(config.hidden_size, 1)\n        self.pooling = config.pooling\n\n    def forward(\n        self,\n        input_ids,\n        attention_mask: Optional[torch.FloatTensor] = None,\n        inputs_embeds: Optional[torch.FloatTensor] = None,\n        head_mask: Optional[torch.FloatTensor] = None,\n        use_cache: Optional[bool] = None,\n        return_dict: Optional[bool] = True,\n    ) -> GPTNeoXRewardModelOutput:\n        outputs = self.gpt_neox(\n            input_ids,\n            attention_mask=attention_mask,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            use_cache=use_cache,\n            return_dict=return_dict,\n        )\n\n        hidden_states = outputs[0]\n        if self.pooling == \"mean\":\n            if attention_mask is None:\n                pooled = hidden_states.mean(dim=1)\n            else:\n                pooled = (hidden_states * attention_mask).sum(dim=1) / attention_mask.sum(dim=1)\n        elif self.pooling == \"last\":\n            if attention_mask is None:\n                pooled = hidden_states[:, -1]\n            else:\n                last_idx = attention_mask.cumsum(dim=1).argmax(dim=1)\n                pooled = hidden_states.gather(1, last_idx.view(-1, 1, 1).expand(-1, 1, hidden_states.size(-1))).squeeze(\n                    1\n                )\n        else:\n            raise ValueError(f\"Unknown pooling method: {self.pooling}\")\n\n        logits = self.out_proj(pooled)\n\n        if not return_dict:\n            return (logits,) + outputs[1:]\n\n        return GPTNeoXRewardModelOutput(logits=logits)\n\n\nAutoConfig.register(\"gpt_neox_reward_model\", GPTNeoXRewardModelConfig)\nAutoModelForSequenceClassification.register(GPTNeoXRewardModelConfig, GPTNeoXRewardModel)\n", "model/model_training/models/patching_falcon.py": "from typing import Optional, Tuple\n\nimport torch\nimport torch.nn as nn\n\nfrom .patching_utils import compute_flash_attention\n\n\ndef falcon_forward_with_flash_attn(\n    self,\n    flash_attn: nn.Module,  # flash_attn.modules.mha.FlashSelfAttention\n    hidden_states: torch.Tensor,\n    alibi: Optional[torch.Tensor],\n    attention_mask: torch.Tensor,\n    layer_past: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,\n    head_mask: Optional[torch.Tensor] = None,\n    use_cache: bool = False,\n    output_attentions: bool = False,\n) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n    \"\"\"\n    head_mask, alibi & output_attention are not supported.\n    Reference to the original `FalconAttention.forwad()` method which this patch replaces:\n    https://github.com/huggingface/transformers/blob/c965d302791cf935d6ea7776428749be678cf509/src/transformers/models/falcon/modeling_falcon.py#L281\n    \"\"\"\n\n    assert head_mask is None  # not supported.\n    assert alibi is None  # not supported.\n    assert not output_attentions  # not supported.\n\n    fused_qkv = self.query_key_value(hidden_states)  # [batch_size, seq_length, 3 x hidden_size]\n    num_kv_heads = self.num_heads if self.new_decoder_architecture else self.num_kv_heads\n    # 3 x [batch_size, seq_length, num_heads, head_dim]\n    (query_layer, key_layer, value_layer) = self._split_heads(fused_qkv)\n\n    batch_size, query_length, _, _ = query_layer.shape\n\n    query_layer = query_layer.transpose(1, 2).reshape(batch_size * self.num_heads, query_length, self.head_dim)\n    key_layer = key_layer.transpose(1, 2).reshape(\n        batch_size * num_kv_heads,\n        query_length,\n        self.head_dim,\n    )\n    value_layer = value_layer.transpose(1, 2).reshape(batch_size * num_kv_heads, query_length, self.head_dim)\n\n    past_kv_length = 0 if layer_past is None else layer_past[0].shape[1]\n    query_layer, key_layer = self.maybe_rotary(query_layer, key_layer, past_kv_length)\n\n    if layer_past is not None:\n        past_key, past_value = layer_past\n        # concatenate along seq_length dimension:\n        #  - key: [batch_size * self.num_heads, kv_length, head_dim]\n        #  - value: [batch_size * self.num_heads, kv_length, head_dim]\n        key_layer = torch.cat((past_key, key_layer), dim=1)\n        value_layer = torch.cat((past_value, value_layer), dim=1)\n\n    if use_cache:\n        present = (key_layer, value_layer)\n    else:\n        present = None\n\n    query_layer_ = query_layer.reshape(batch_size, self.num_heads, -1, self.head_dim)\n    key_layer_ = key_layer.reshape(batch_size, num_kv_heads, -1, self.head_dim)\n    value_layer_ = value_layer.reshape(batch_size, num_kv_heads, -1, self.head_dim)\n\n    q = query_layer_.permute(0, 2, 1, 3)\n    k = key_layer_.permute(0, 2, 1, 3).expand(q.shape)\n    v = value_layer_.permute(0, 2, 1, 3).expand(q.shape)\n\n    if attention_mask is not None:\n        attention_mask = attention_mask[:, 0, -1]\n\n    flash_attn.train(self.training)\n    attn_output = compute_flash_attention(flash_attn, q, k, v, attention_mask=attention_mask)\n    attn_output = attn_output.reshape(batch_size, query_length, self.num_heads * self.head_dim)\n\n    output_tensor = self.dense(attn_output)\n\n    return output_tensor, present\n", "model/model_training/models/rope.py": "import torch\n\n\n# rotary pos emb helpers (torch.jit.script does not seem to support staticmethod...)\ndef rotate_half(x):\n    x1, x2 = x[..., : x.shape[-1] // 2], x[..., x.shape[-1] // 2 :]\n    return torch.cat((-x2, x1), dim=-1)\n\n\nclass RWNTKScaledRope(torch.nn.Module):\n\n    \"\"\"\n    NTK-Scaled RoPE for RefinedWebModel\n    \"\"\"\n\n    def __init__(\n        self,\n        head_dim: int,\n        base=10000,\n        alpha: int = 2,\n    ):\n        super().__init__()\n        self.alpha = alpha\n        base = base * self.alpha ** (head_dim / (head_dim - 2))\n        inv_freq = 1.0 / (base ** (torch.arange(0, head_dim, 2).float() / head_dim))\n        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n        self.head_dim = head_dim\n        self.seq_len_cached = -1\n        self.batch_size_cached = None\n        self.cos_cached: torch.Tensor | None = None\n        self.sin_cached: torch.Tensor | None = None\n\n    def cos_sin(\n        self,\n        seq_len: int,\n        past_key_values_length: int,\n        device=\"cuda\",\n        dtype=torch.bfloat16,\n    ) -> torch.Tensor:\n        total_length = seq_len + past_key_values_length\n        if total_length > self.seq_len_cached:\n            self.seq_len_cached = total_length\n            t = torch.arange(total_length, device=device, dtype=self.inv_freq.dtype)\n            freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)\n            emb = torch.cat((freqs, freqs), dim=-1).to(device)\n\n            if dtype in [torch.float16, torch.bfloat16]:\n                emb = emb.float()\n\n            self.cos_cached = emb.cos()[None, :, :]\n            self.sin_cached = emb.sin()[None, :, :]\n\n            self.cos_cached = self.cos_cached.type(dtype)\n            self.sin_cached = self.sin_cached.type(dtype)\n\n        return (\n            self.cos_cached[:, past_key_values_length : seq_len + past_key_values_length],\n            self.sin_cached[:, past_key_values_length : seq_len + past_key_values_length],\n        )\n\n    def forward(self, q, k, past_key_values_length=0):\n        batch, seq_len, head_dim = q.shape\n        cos, sin = self.cos_sin(seq_len, past_key_values_length, q.device, q.dtype)\n        return (q * cos) + (rotate_half(q) * sin), (k * cos) + (rotate_half(k) * sin)\n", "model/model_training/models/patching.py": "from __future__ import annotations  # To make it not choke over FlashSelfAttention\n\nimport warnings\nfrom functools import partial\nfrom typing import Callable, Optional\n\nimport torch.nn as nn\nimport transformers\nfrom transformers import (\n    AutoConfig,\n    FalconForCausalLM,\n    FalconModel,\n    GPTNeoXForCausalLM,\n    GPTNeoXModel,\n    LlamaForCausalLM,\n    LlamaModel,\n)\nfrom transformers.models.llama.modeling_llama import (\n    LlamaDynamicNTKScalingRotaryEmbedding,\n    LlamaLinearScalingRotaryEmbedding,\n)\nfrom trlx.models.modeling_ppo import AutoModelForCausalLMWithHydraValueHead\n\nfrom .patching_falcon import falcon_forward_with_flash_attn\nfrom .patching_llama import llama_forward_with_flash_attn\nfrom .patching_neox import neox_forward_with_flash_attn\nfrom .reward_model import GPTNeoXRewardModel\nfrom .rope import RWNTKScaledRope\n\nSUPPORTED_MODELS = [\n    GPTNeoXModel,\n    GPTNeoXForCausalLM,\n    LlamaForCausalLM,\n    LlamaModel,\n    FalconForCausalLM,\n    FalconModel,\n    GPTNeoXRewardModel,\n    # Currently only supported by NeoX models; Will work on LLaMa models\n    AutoModelForCausalLMWithHydraValueHead,\n]\n\n\ndef _patched_mlp_forward(post_module: nn.Module, module: nn.Module, *args, **kwargs):\n    post_module.train(module.training)\n    out = module.old_forward(*args, **kwargs)\n    out = post_module(out)\n    return out\n\n\ndef _patched_attn_forward(post_module: nn.Module, module: nn.Module, *args, **kwargs):\n    post_module.train(module.training)\n    out = module.old_forward(*args, **kwargs)\n    hiddens = post_module(out[0])\n    return (hiddens,) + out[1:]\n\n\ndef add_dropout(module: nn.Module, patched_fwd: Callable, p_dropout: float = 0.1):\n    dropout = nn.Dropout(p=p_dropout)\n    module.old_forward = module.forward\n    module.forward = partial(patched_fwd, dropout, module)\n\n\ndef add_flash_attn(module: nn.Module, causal: bool = True):\n    \"\"\"\n    Replaces the standard attention implementation with Flash Attention [1].\n    Limitations:\n      - Only works for fp16 or bf16 inputs\n      - Requires inputs to be on CUDA\n      - `output_attentions=True` does not work after patching, attention weights will be None\n      - Non-contiguous attention masks are not supported (e.g. [1, 1, 0, 1, 1, 0, 0] will just become [1, 1, 1, 1, 1, 0, 0]).\n\n    [1] https://github.com/HazyResearch/flash-attention\n    \"\"\"\n\n    flash_attn = FlashSelfAttention(causal=causal)\n    if isinstance(module, transformers.models.llama.modeling_llama.LlamaAttention):\n        module.old_forward = module.forward\n        module.forward = partial(llama_forward_with_flash_attn, module, flash_attn)\n    elif isinstance(module, transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXAttention):\n        if not hasattr(module, \"_attn\"):\n            warnings.warn(\"Provided module doesn't have a _attn() function to be patched.\")\n        module._attn = partial(neox_forward_with_flash_attn, module, flash_attn)\n    elif isinstance(module, transformers.models.falcon.modeling_falcon.FalconAttention):\n        module.forward = partial(falcon_forward_with_flash_attn, module, flash_attn)\n    else:\n        raise NotImplementedError(f\"Flash attention is not implemented for {module.__class__.__name__}.\")\n\n\ndef patch_model(\n    model: nn.Module,\n    resid_pdrop: Optional[float] = 0.1,\n    flash_attention: bool = True,\n    patch_unsupported: bool = False,\n    residual_dropout_lima: bool = False,\n):\n    \"\"\"\n    Helper function for patching HF language models.\n    Currently supports: GPTNeoX-based models\n\n    Limitations:\n      - Flash attention requires CUDA and fp16/bf16 training. It also requires contiguous attention masks.\n      - Residual dropout does not support multi-GPU training without DeepDpeed.\n    \"\"\"\n    global FlashSelfAttention\n    if flash_attention:\n        try:\n            from flash_attn.modules.mha import FlashSelfAttention  # pyright: reportMissingImports=false\n        except ModuleNotFoundError:\n            warnings.warn(\n                \"\"\"\\nmodule flash_attn not found - either install:\n  pip3 install flash_attn\nor run with:\n  --use_flash_attention=false \"\"\"\n            )\n            exit(1)\n    if (resid_pdrop is None or resid_pdrop == 0.0) and not flash_attention:\n        print(\"Continuing without patching\")\n        return\n\n    if resid_pdrop is not None and (resid_pdrop < 0 or resid_pdrop > 1.0):\n        raise ValueError(\"Invalid argument: `resid_pdrop` must be between 0.0 and 1.0\")\n\n    if not flash_attention and (resid_pdrop is None or resid_pdrop == 0.0):\n        return\n\n    if (\n        not any(isinstance(model, model_class) for model_class in SUPPORTED_MODELS)\n        and model.__class__.__name__ != \"RWForCausalLM\"\n    ):\n        if not flash_attention and (resid_pdrop is None or resid_pdrop == 0.0):\n            return  # nothing to patch\n\n        if not patch_unsupported:\n            warnings.warn(\n                \"Model patching does not support this model class. No patches will be applied. \"\n                \"If you want to force patch this model, please set `patch_unsupported=True`.\"\n            )\n            return\n\n        warnings.warn(\n            \"Patching residual dropout has only been tested with this model class. \"\n            f\"Please make sure that it also works for `{model.__class__.__name__}`.\\n\"\n            \"Or disable flash_attention and residual_dropout with:\\n\"\n            \"--use_flash_attention=false  --no-residual_dropout\"\n        )\n\n    if isinstance(model, GPTNeoXRewardModel) or isinstance(model, GPTNeoXForCausalLM):\n        model = model.gpt_neox\n\n    if isinstance(model, LlamaForCausalLM):\n        model = model.model\n\n    if isinstance(model, AutoModelForCausalLMWithHydraValueHead):\n        if isinstance(model.base_model, GPTNeoXForCausalLM):\n            model = model.base_model.gpt_neox\n        elif isinstance(model.base_model, LlamaForCausalLM):\n            model = model.base_model.model\n        else:\n            warnings.warn(\n                \"Unfortunately there is currently only support for NeoX models and LLaMa models \"\n                f\"Please make sure that `{model.__class__.__name__}` is one of those model.\\n\"\n                \"Or disable flash_attention and residual_dropout with:\\n\"\n                \"--use_flash_attention=false  --no-residual_dropout\"\n            )\n\n    if model.__class__.__name__ == \"RWForCausalLM\":\n        model = model.base_model\n\n    if isinstance(model, FalconForCausalLM):\n        model = model.transformer\n\n    attention_key_lookup = {\n        GPTNeoXModel: \"attention\",\n        GPTNeoXRewardModel: \"attention\",\n        LlamaModel: \"self_attn\",\n        FalconModel: \"self_attention\",\n    }\n    mlp_key_lookup = {\n        GPTNeoXModel: \"mlp\",\n        GPTNeoXRewardModel: \"mlp\",\n        LlamaModel: \"mlp\",\n        FalconModel: \"mlp\",\n    }\n    if isinstance(model, FalconModel) or model.__class__.__name__ == \"RWModel\":\n        layers = model.h\n        attention_key = \"self_attention\"\n        mlp_key = \"mlp\"\n    else:\n        layers = model.layers\n        attention_key = attention_key_lookup.get(model.__class__, \"attention\")\n        mlp_key = mlp_key_lookup.get(model.__class__, \"mlp\")\n    num_layers = len(layers)\n    resid_pdrop_last_layer = resid_pdrop\n    for i, layer in enumerate(layers):\n        if flash_attention:\n            add_flash_attn(getattr(layer, attention_key), causal=True)\n        if residual_dropout_lima:\n            resid_pdrop = i / (num_layers - 1) * resid_pdrop_last_layer\n        if resid_pdrop is not None and resid_pdrop > 0:\n            add_dropout(getattr(layer, attention_key), _patched_attn_forward, resid_pdrop)\n            add_dropout(getattr(layer, mlp_key), _patched_mlp_forward, resid_pdrop)\n\n\nclass RopePatch:\n    def __init__(self, model_name, **kwargs):\n        self.args = kwargs\n        self.rope_type = self.args.pop(\"type\")\n        config = AutoConfig.from_pretrained(model_name, trust_remote_code=True)\n        if hasattr(config, \"max_position_embeddings\"):\n            self.args[\"max_position_embeddings\"] = config.max_position_embeddings\n        if hasattr(config, \"base\"):\n            self.args[\"base\"] = config.base\n        architecture = config.architectures\n        if architecture:\n            self.model_name = architecture[0]\n            if \"FalconForCausalLM\" in architecture or \"RWForCausalLM\" in architecture:\n                self.architecture = \"FalconForCausalLM\"\n                if self.rope_type == \"ntk\":\n                    self.patch_fun = RWNTKScaledRope\n                else:\n                    raise NotImplementedError()\n            elif \"LlamaForCausalLM\" in architecture:\n                self.architecture = \"LlamaForCausalLM\"\n                if self.rope_type == \"linear\":\n                    self.patch_fun = LlamaLinearScalingRotaryEmbedding\n                elif self.rope_type == \"dynamic\":\n                    self.patch_fun = LlamaDynamicNTKScalingRotaryEmbedding\n                else:\n                    raise NotImplementedError()\n            else:\n                raise NotImplementedError()\n\n    @classmethod\n    def from_config(cls, config):\n        model_name = config.model_name\n        args = config.superhot_config\n        return cls(model_name, **args)\n\n    def patch(self, model):\n        if self.architecture == \"FalconForCausalLM\":\n            self.patch_falcon_model(model, **self.args)\n        elif self.architecture == \"LlamaForCausalLM\":\n            self.patch_llama_model(model, **self.args)\n        else:\n            raise NotImplementedError()\n\n    def patch_falcon_model(self, model, **kwargs):\n        for each in model.transformer.h:\n            each.self_attention.maybe_rotary = self.patch_fun(model.config.head_dim, **kwargs)\n\n    def patch_llama_model(self, model, **kwargs):\n        kwargs.update({\"device\": model.device})\n        for each in model.model.layers:\n            each.self_attn.rotary_emb = self.patch_fun(each.self_attn.head_dim, **kwargs)\n", "model/model_training/models/patching_utils.py": "import torch\nimport torch.nn.functional as F\n\n\ndef compute_flash_attention(flash_attn, q, k, v, attention_mask=None, head_mask=None):\n    # q, k, v: [bs, seq_len, num_attention_heads, attn_head_size]\n    # attention_mask (float): [bs, seq_len]\n    batch_size, max_len = q.size(0), q.size(1)\n\n    qkv = torch.stack([q, k, v], dim=2)\n    dtype_in = qkv.dtype\n    if dtype_in == torch.float32:\n        qkv = qkv.to(torch.float16)  # need to truncate in case input is fp32\n    cu_seqlens, max_seqlen = None, None\n\n    if attention_mask is None:\n        out = flash_attn(qkv, cu_seqlens=cu_seqlens, max_seqlen=max_seqlen)\n    else:\n        # Limitation: non-contiguous attention mask will not be handled correctly\n        # model will be able to pay attention between the first and last non-masked token, i.e. left- and right-side padding is supported.\n        csums = (attention_mask >= 0).cumsum(dim=1)\n        ends = csums.argmax(dim=1) + 1\n        starts = ends - csums.max(dim=1).values\n        seqlens = ends - starts\n\n        qkv = torch.cat([qkv[i, starts[i] : ends[i]] for i in range(batch_size)], dim=0)\n        zero = torch.zeros_like(seqlens[:1])  # torch.tensor([0]) with correct dtype and device\n        cu_seqlens = torch.cat([zero, seqlens.cumsum(dim=0)], dim=0).to(torch.int32)\n        max_seqlen = seqlens.max().item()\n\n        out = flash_attn(qkv, cu_seqlens=cu_seqlens, max_seqlen=max_seqlen)\n        # out: [num_unmasked_tokens, num_attention_heads, attn_head_size]\n\n        seqs = [out[start:end] for start, end in zip(cu_seqlens[:-1], cu_seqlens[1:])]\n        # stack and pad sequences together\n        padded_seqs = [\n            F.pad(seqs[i], (0, 0) * (seqs[i].dim() - 1) + (starts[i], max_len - ends[i]), value=0.0)\n            for i in range(batch_size)\n        ]\n        out = torch.stack(padded_seqs)\n\n    if out.dtype != dtype_in:\n        out = out.to(dtype_in)\n    return out\n\n\nif __name__ == \"__main__\":\n    from flash_attn.modules.mha import FlashSelfAttention\n\n    flash_attn = FlashSelfAttention(causal=True)\n\n    dtype = torch.float16\n    device = torch.device(\"cuda:0\")\n\n    batch_size, seq_len, num_heads, head_size = 4, 18, 8, 32\n    q = torch.randn(batch_size, seq_len, num_heads, head_size, dtype=dtype, device=device)\n    k = torch.randn(batch_size, seq_len, num_heads, head_size, dtype=dtype, device=device)\n    v = torch.randn(batch_size, seq_len, num_heads, head_size, dtype=dtype, device=device)\n\n    attn_mask = torch.randn(batch_size, seq_len, dtype=dtype, device=device).abs().cumsum(dim=1)\n    attn_mask = ((attn_mask > 3) & (attn_mask < 10)).int().log()\n\n    out = compute_flash_attention(flash_attn, q, k, v, attention_mask=attn_mask)\n", "model/model_training/models/gptj.py": "# Taken from https://github.com/sleekmike/Finetune_GPT-J_6B_8-bit/blob/master/gpt-j-6b-8-bit.py\n\nimport torch\nimport torch.nn.functional as F\nimport transformers\nfrom bitsandbytes.functional import dequantize_blockwise, quantize_blockwise\nfrom torch import nn\nfrom torch.cuda.amp import custom_bwd, custom_fwd\nfrom transformers import AutoModelForCausalLM\n\n\nclass FrozenBNBLinear(nn.Module):\n    def __init__(self, weight, absmax, code, bias=None):\n        assert isinstance(bias, nn.Parameter) or bias is None\n        super().__init__()\n        self.out_features, self.in_features = weight.shape\n        self.register_buffer(\"weight\", weight.requires_grad_(False))\n        self.register_buffer(\"absmax\", absmax.requires_grad_(False))\n        self.register_buffer(\"code\", code.requires_grad_(False))\n        self.adapter = None\n        self.bias = bias\n\n    def forward(self, input):\n        output = DequantizeAndLinear.apply(input, self.weight, self.absmax, self.code, self.bias)\n        if self.adapter:\n            output += self.adapter(input)\n        return output\n\n    @classmethod\n    def from_linear(cls, linear: nn.Linear) -> \"FrozenBNBLinear\":\n        weights_int8, state = quantize_blockise_lowmemory(linear.weight)\n        return cls(weights_int8, *state, linear.bias)\n\n    def __repr__(self):\n        return f\"{self.__class__.__name__}({self.in_features}, {self.out_features})\"\n\n\nclass DequantizeAndLinear(torch.autograd.Function):\n    @staticmethod\n    @custom_fwd\n    def forward(\n        ctx,\n        input: torch.Tensor,\n        weights_quantized: torch.ByteTensor,\n        absmax: torch.FloatTensor,\n        code: torch.FloatTensor,\n        bias: torch.FloatTensor,\n    ):\n        weights_deq = dequantize_blockwise(weights_quantized, absmax=absmax, code=code)\n        ctx.save_for_backward(input, weights_quantized, absmax, code)\n        ctx._has_bias = bias is not None\n        return F.linear(input, weights_deq, bias)\n\n    @staticmethod\n    @custom_bwd\n    def backward(ctx, grad_output: torch.Tensor):\n        assert not ctx.needs_input_grad[1] and not ctx.needs_input_grad[2] and not ctx.needs_input_grad[3]\n        input, weights_quantized, absmax, code = ctx.saved_tensors\n        # grad_output: [*batch, out_features]\n        weights_deq = dequantize_blockwise(weights_quantized, absmax=absmax, code=code)\n        grad_input = grad_output @ weights_deq\n        grad_bias = grad_output.flatten(0, -2).sum(dim=0) if ctx._has_bias else None\n        return grad_input, None, None, None, grad_bias\n\n\nclass FrozenBNBEmbedding(nn.Module):\n    def __init__(self, weight, absmax, code):\n        super().__init__()\n        self.num_embeddings, self.embedding_dim = weight.shape\n        self.register_buffer(\"weight\", weight.requires_grad_(False))\n        self.register_buffer(\"absmax\", absmax.requires_grad_(False))\n        self.register_buffer(\"code\", code.requires_grad_(False))\n        self.adapter = None\n\n    def forward(self, input, **kwargs):\n        with torch.no_grad():\n            # note: both quantized weights and input indices are *not* differentiable\n            weight_deq = dequantize_blockwise(self.weight, absmax=self.absmax, code=self.code)\n            output = F.embedding(input, weight_deq, **kwargs)\n        if self.adapter:\n            output += self.adapter(input)\n        return output\n\n    @classmethod\n    def from_embedding(cls, embedding: nn.Embedding) -> \"FrozenBNBEmbedding\":\n        weights_int8, state = quantize_blockise_lowmemory(embedding.weight)\n        return cls(weights_int8, *state)\n\n    def __repr__(self):\n        return f\"{self.__class__.__name__}({self.num_embeddings}, {self.embedding_dim})\"\n\n\ndef quantize_blockise_lowmemory(matrix: torch.Tensor, chunk_size: int = 2**20):\n    assert chunk_size % 4096 == 0\n    code = None\n    chunks = []\n    absmaxes = []\n    flat_tensor = matrix.view(-1)\n    for i in range((matrix.numel() - 1) // chunk_size + 1):\n        input_chunk = flat_tensor[i * chunk_size : (i + 1) * chunk_size].clone()\n        quantized_chunk, (absmax_chunk, code) = quantize_blockwise(input_chunk, code=code)\n        chunks.append(quantized_chunk)\n        absmaxes.append(absmax_chunk)\n\n    matrix_i8 = torch.cat(chunks).reshape_as(matrix)\n    absmax = torch.cat(absmaxes)\n    return matrix_i8, (absmax, code)\n\n\ndef convert_to_int8(model):\n    \"\"\"Convert linear and embedding modules to 8-bit with optional adapters\"\"\"\n    for module in list(model.modules()):\n        for name, child in module.named_children():\n            if isinstance(child, nn.Linear):\n                print(name, child)\n                setattr(\n                    module,\n                    name,\n                    FrozenBNBLinear(\n                        weight=torch.zeros(child.out_features, child.in_features, dtype=torch.uint8),\n                        absmax=torch.zeros((child.weight.numel() - 1) // 4096 + 1),\n                        code=torch.zeros(256),\n                        bias=child.bias,\n                    ),\n                )\n            elif isinstance(child, nn.Embedding):\n                setattr(\n                    module,\n                    name,\n                    FrozenBNBEmbedding(\n                        weight=torch.zeros(child.num_embeddings, child.embedding_dim, dtype=torch.uint8),\n                        absmax=torch.zeros((child.weight.numel() - 1) // 4096 + 1),\n                        code=torch.zeros(256),\n                    ),\n                )\n\n\nclass GPTJBlock(transformers.models.gptj.modeling_gptj.GPTJBlock):\n    def __init__(self, config):\n        super().__init__(config)\n\n        convert_to_int8(self.attn)\n        convert_to_int8(self.mlp)\n\n\nclass GPTJModel(transformers.models.gptj.modeling_gptj.GPTJModel):\n    def __init__(self, config):\n        super().__init__(config)\n        convert_to_int8(self)\n\n\nclass GPTJForCausalLM(transformers.models.gptj.modeling_gptj.GPTJForCausalLM):\n    def __init__(self, config):\n        super().__init__(config)\n        convert_to_int8(self)\n\n\ndef add_adapters(model, adapter_dim=16):\n    assert adapter_dim > 0\n\n    for module in model.modules():\n        if isinstance(module, FrozenBNBLinear):\n            module.adapter = nn.Sequential(\n                nn.Linear(module.in_features, adapter_dim, bias=False),\n                nn.Linear(adapter_dim, module.out_features, bias=False),\n            )\n            nn.init.zeros_(module.adapter[1].weight)\n        elif isinstance(module, FrozenBNBEmbedding):\n            module.adapter = nn.Sequential(\n                nn.Embedding(module.num_embeddings, adapter_dim),\n                nn.Linear(adapter_dim, module.embedding_dim, bias=False),\n            )\n            nn.init.zeros_(module.adapter[1].weight)\n\n\ndef get_model(model_name, cache_dir, quantization):\n    if quantization is None:\n        model = AutoModelForCausalLM.from_pretrained(model_name, cache_dir=cache_dir)\n    elif quantization == \"8bit\":\n        raise ValueError(\"Loading 8-bit model. Use deepspeed instead.\")\n        transformers.models.gptj.modeling_gptj.GPTJBlock = GPTJBlock\n        model = AutoModelForCausalLM.from_pretrained(model_name, cache_dir=cache_dir)\n        add_adapters(model)\n    else:\n        raise ValueError(f\"Unknown quantization {quantization}\")\n\n    return model\n", "model/model_training/models/__init__.py": "import transformers\n\n\ndef freeze_top_n_layers(model, target_layers):\n    # its possible we can simply detect which module is a ModuleList\n    # and simply freeze the module without doing string parsing\n    for name, param in model.named_parameters():\n        if \"embed\" in name:\n            param.requires_grad = False\n        elif \".layer\" in name or \".h.\" in name:\n            tokens = name.split(\".\")\n            layer_ = None\n            for token in tokens:\n                if token.isdigit():\n                    layer_ = int(token)\n                    break\n            if layer_ is not None and layer_ < target_layers:\n                # print('freeze ', layer_, name)\n                param.requires_grad = False\n    return model\n\n\ndef get_specific_model(\n    model_name,\n    seq2seqmodel=False,\n    without_head=False,\n    cache_dir=\".cache\",\n    quantization=False,\n    **kwargs,\n):\n    if without_head:\n        model = transformers.AutoModel.from_pretrained(model_name, cache_dir=cache_dir, **kwargs)\n    elif seq2seqmodel:\n        # encoder-decoder support for Flan-T5 like models\n        model = transformers.AutoModelForSeq2SeqLM.from_pretrained(model_name, cache_dir=cache_dir, **kwargs)\n    else:\n        if \"falcon-7b\" in model_name:\n            # temporary hack until tiiuae/falcon-7b uses the transformer's Falcon impl by default\n            # in-library PR was reverted https://huggingface.co/tiiuae/falcon-7b/commit/378337427557d1df3e742264a2901a49f25d4eb1\n            model = transformers.models.falcon.modeling_falcon.FalconForCausalLM.from_pretrained(\n                model_name, cache_dir=cache_dir, **kwargs\n            )\n        else:\n            if \"falcon\" in model_name:\n                kwargs[\"trust_remote_code\"] = True\n            model = transformers.AutoModelForCausalLM.from_pretrained(model_name, cache_dir=cache_dir, **kwargs)\n    return model\n", "model/model_training/tools/model_chat.py": "#!/usr/bin/env python3\n\"\"\"\n\nA very simple script to test model locally\n\n\n\"\"\"\nimport argparse\nfrom enum import Enum\nfrom typing import List, Tuple\n\nimport torch\nfrom model_training.custom_datasets.formatting import QA_SPECIAL_TOKENS\nfrom model_training.utils.utils import _strtobool\nfrom tokenizers import pre_tokenizers\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\n\nclass ChatRole(str, Enum):\n    system = \"<|system|>\"\n    prompter = \"<|prompter|>\"\n    assistant = \"<|assistant|>\"\n\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\"--model_path\", type=str, required=True)\nparser.add_argument(\"--bot_name\", type=str, default=\"Joi\", help=\"Use this when your format isn't in OA format\")\nparser.add_argument(\"--format\", type=str, default=\"v2\")\nparser.add_argument(\"--max_new_tokens\", type=int, default=200)\nparser.add_argument(\"--top_k\", type=int, default=40)\nparser.add_argument(\"--temperature\", type=float, default=1.0)\nparser.add_argument(\"--do-sample\", type=_strtobool, default=True)\nparser.add_argument(\"--per-digit-tokens\", action=\"store_true\")\nargs = parser.parse_args()\n\nbot_name: str = args.bot_name\nmodel_name: str = args.model_path\nmethod: str = args.format\n\n\ndef talk(human_input: str, history: List[Tuple[str, str]], sep_token: str, prefix=\"\"):\n    histories = []\n    if method == \"v2\":\n        prefix = \"<prefix>You are a helpful assistant called Joi trained by OpenAssistant on large corpus of data, you will now help user to answer the question as concise as possible</prefix>\"\n        for question, answer in history:\n            histories.append(\n                \"{}{}{}{}\".format(QA_SPECIAL_TOKENS[\"Question\"], question, QA_SPECIAL_TOKENS[\"Answer\"], answer)\n            )\n        if len(histories) > 0:\n            prefix += sep_token.join(histories)\n            # add sep at the end\n            prefix += sep_token\n        prefix += \"{}{}{}\".format(QA_SPECIAL_TOKENS[\"Question\"], human_input, QA_SPECIAL_TOKENS[\"Answer\"])\n    # elif method == \"v3\":\n    #     personality = \"You are a helpful assistant called Joi, you are a smart and helpful bot.\"\n    #     prefix = f\"{SeqToken.begin}{ChatRole.system}{SeqToken.delimiter}{personality}{SeqToken.end}\"\n    #     for question, answer in history:\n    #         histories.append(\n    #             f\"{SeqToken.begin}{ChatRole.prompter}{SeqToken.delimiter}{question}{SeqToken.end}\"\n    #             + f\"{SeqToken.begin}{ChatRole.assistant}{SeqToken.delimiter}{answer}{SeqToken.end}\"\n    #         )\n    #     if len(histories) > 0:\n    #         prefix += \"\".join(histories)\n    #         # add sep at the end\n    #     prefix += f\"{SeqToken.begin}{ChatRole.prompter}{SeqToken.delimiter}{human_input}{SeqToken.end}{SeqToken.begin}{ChatRole.assistant}{SeqToken.delimiter}\"\n    elif method == \"v2.5\":\n        # personality = \"You are a helpful assistant called Joi, you are a smart and helpful bot.\"\n        # prefix = f\"{ChatRole.system}{personality}{SeqToken.end}\"\n        for question, answer in history:\n            histories.append(\n                # f\"{ChatRole.prompter}{question}{SeqToken.end}\" + f\"{ChatRole.assistant}{answer}{SeqToken.end}\"\n                f\"{ChatRole.prompter}{question}</s>\"\n                + f\"{ChatRole.assistant}{answer}</s>\"\n            )\n        if len(histories) > 0:\n            prefix += \"\".join(histories)\n            # add sep at the end\n        prefix += f\"{ChatRole.prompter}{human_input}</s>{ChatRole.assistant}\"\n    else:\n        for question, answer in history:\n            histories.append(\"User: \" + question + \"\\n\\n{}: \".format(bot_name) + answer + \"\\n\")\n        if len(histories) > 0:\n            prefix += \"\\n\".join(histories)\n        prefix += \"\\nUser: \" + human_input + \"\\n\\n{}: \".format(bot_name)\n\n    return prefix\n\n\ndef process_output(output, method, bot_name):\n    if method == \"v2\":\n        answer = output.split(QA_SPECIAL_TOKENS[\"Answer\"])[-1]\n        answer = answer.split(\"</s>\")[0].replace(\"<|endoftext|>\", \"\").lstrip().split(QA_SPECIAL_TOKENS[\"Answer\"])[0]\n    elif method == \"v2.5\":\n        answer = output.split(f\"{ChatRole.assistant}\")[-1]\n        # answer = answer.split(\"</s>\")[0].replace(SeqToken.end, \"\").lstrip()\n    # elif method == \"v3\":\n    #     answer = output.split(f\"{SeqToken.begin}{ChatRole.assistant}{SeqToken.delimiter}\")[-1]\n    #     answer = answer.split(\"</s>\")[0].replace(SeqToken.end, \"\").lstrip()\n    else:\n        answer = output.split(\"\\n\\n{}:\".format(bot_name))[-1]\n        answer = answer.split(\"</s>\")[0].replace(\"<|endoftext|>\", \"\").lstrip().split(\"\\n\\n{}:\".format(bot_name))[0]\n    return answer\n\n\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nif method != \"v2\":\n    tokenizer.add_special_tokens({\"pad_token\": \"<|endoftext|>\"})\nmodel = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16)\n\nmodel.eval().cuda()\n\nif args.per_digit_tokens:\n    tokenizer._tokenizer.pre_processor = pre_tokenizers.Digits(True)\n\nmodel = AutoModelForCausalLM.from_pretrained(model_name).half().eval().cuda()\n\nif __name__ == \"__main__\":\n    histories = []\n    prefix = \"\"\n    while True:\n        print(\">\", end=\" \")\n        try:\n            prompt = input()\n        except (EOFError, KeyboardInterrupt):  # Catch ctrl+d and ctrl+c respectively\n            print()\n            break\n        if prompt == \"!reset\":\n            histories = []\n        else:\n            input_text = talk(prompt, histories, prefix)\n            inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True).to(0)\n            if \"token_type_ids\" in inputs:\n                del inputs[\"token_type_ids\"]\n            outputs = model.generate(\n                **inputs,\n                early_stopping=True,\n                max_new_tokens=args.max_new_tokens,\n                do_sample=args.do_sample,\n                top_k=args.top_k,\n                temperature=args.temperature,\n                pad_token_id=tokenizer.eos_token_id,\n            )\n            output = tokenizer.decode(outputs[0], truncate_before_pattern=[r\"\\n\\n^#\", \"^'''\", \"\\n\\n\\n\"])\n            reply = process_output(output, method, bot_name)\n\n            if len(reply) != 0:\n                print(reply)\n                histories.append((prompt, reply))\n            else:\n                print(\"empty token\")\n", "model/model_training/tools/export_model.py": "import argparse\nimport sys\n\nimport model_training.models.reward_model  # noqa: F401 make sure reward model is registered for AutoModel\nimport torch\nfrom transformers import AutoModelForCausalLM, AutoModelForSequenceClassification, AutoTokenizer\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"model_name\", type=str, help=\"checkpoint path or model name\")\n    parser.add_argument(\"--dtype\", type=str, default=\"fp16\", help=\"fp16, bf16 or fp32\")\n    parser.add_argument(\"--hf_repo_name\", type=str, help=\"Huggingface repository name\")\n    parser.add_argument(\"--auth_token\", type=str, help=\"User access token\")\n    parser.add_argument(\"--output_folder\", type=str, help=\"output folder path\")\n    parser.add_argument(\"--max_shard_size\", type=str, default=\"10GB\")\n    parser.add_argument(\"--cache_dir\", type=str)\n    parser.add_argument(\"--reward_model\", action=\"store_true\", default=False)\n    parser.add_argument(\"--rl_checkpoint\", type=str, help=\"load RL fine-tuning checkpoint\")\n    parser.add_argument(\n        \"--rope_scaling_type\", type=str, help=\"set rope scaling type (linear, dynamic)\", default=\"linear\"\n    )\n    parser.add_argument(\"--rope_scaling_factor\", type=float, help=\"set rope scaling factor (float >1.0)\")\n    parser.add_argument(\n        \"--trust_remote_code\",\n        action=\"store_true\",\n        default=False,\n        help=\"allow custom model code (required for Falcon)\",\n    )\n    return parser.parse_args()\n\n\ndef main():\n    args = parse_args()\n    print(args)\n\n    if args.dtype in (\"float16\", \"fp16\"):\n        torch_dtype = torch.float16\n    elif args.dtype in (\"float32\", \"fp32\"):\n        torch_dtype = torch.float32\n    elif args.dtype in (\"bfloat16\", \"bf16\"):\n        torch_dtype = torch.bfloat16\n    else:\n        print(f\"Unsupported dtype: {args.dtype}\")\n        sys.exit(1)\n\n    if not args.hf_repo_name and not args.output_folder:\n        print(\n            \"Please specify either `--hf_repo_name` to push to HF or `--output_folder` \"\n            \"to export the model to a local folder.\"\n        )\n        sys.exit(1)\n\n    print(f\"Loading tokenizer '{args.model_name}' ...\")\n    tokenizer = AutoTokenizer.from_pretrained(args.model_name)\n    print(f\"{type(tokenizer).__name__} (vocab_size={len(tokenizer)})\")\n\n    print(f\"Loading model '{args.model_name}' ({args.dtype}) ...\")\n\n    if args.rl_checkpoint:\n        model = AutoModelForCausalLM.from_pretrained(args.model_name, torch_dtype=torch_dtype, cache_dir=args.cache_dir)\n\n        print(f\"Loading RL checkpoint: {args.rl_checkpoint}...\")\n        checkpoint_state = torch.load(args.rl_checkpoint, map_location=\"cpu\")[\"module\"]\n\n        # drop parameters of value head\n        for param_name in (\"v_head.0.weight\", \"v_head.0.bias\", \"v_head.2.weight\", \"v_head.2.bias\"):\n            checkpoint_state.pop(param_name, None)\n\n        # resolve inconsistencies in the vocab size\n        target_size = checkpoint_state[list(filter(lambda x: \"embed\" in x, list(checkpoint_state.keys())))[0]].shape[0]\n        model.resize_token_embeddings(target_size)\n\n        print(model.load_state_dict(checkpoint_state))\n\n    elif args.reward_model:\n        model = AutoModelForSequenceClassification.from_pretrained(\n            args.model_name, torch_dtype=torch_dtype, cache_dir=args.cache_dir\n        )\n    else:\n        model = AutoModelForCausalLM.from_pretrained(\n            args.model_name,\n            torch_dtype=torch_dtype,\n            cache_dir=args.cache_dir,\n            trust_remote_code=args.trust_remote_code,\n        )\n    print(f\"{type(model).__name__} (num_parameters={model.num_parameters()})\")\n\n    print(\"Model architecture:\")\n    print(model)\n\n    if args.rope_scaling_type is not None and args.rope_scaling_factor is not None:\n        assert args.rope_scaling_type in (\"linear\", \"dynamic\")\n        assert args.rope_scaling_factor >= 1.0\n        rope_scaling = {\"type\": args.rope_scaling_type, \"factor\": args.rope_scaling_factor}\n        print(f\"setting new rope_scaling config: {rope_scaling} (old: {model.config.rope_scaling})\")\n        model.config.rope_scaling = rope_scaling\n\n    if args.output_folder:\n        print(f\"Saving model to: {args.output_folder}\")\n        model.save_pretrained(args.output_folder, max_shard_size=args.max_shard_size)\n\n        print(f\"Saving tokenizer to: {args.output_folder}\")\n        tokenizer.save_pretrained(args.output_folder)\n\n    if args.hf_repo_name:\n        print(\"Uploading model to HF...\")\n        model.push_to_hub(args.hf_repo_name, use_auth_token=args.auth_token, max_shard_size=args.max_shard_size)\n\n        print(\"Uploading tokenizer to HF...\")\n        tokenizer.push_to_hub(args.hf_repo_name, use_auth_token=args.auth_token)\n\n\nif __name__ == \"__main__\":\n    main()\n", "model/model_training/tools/check_oasst_export.py": "import argparse\n\nfrom oasst_data import ExportMessageTree, read_message_tree_list, visit_messages_depth_first\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"input_file_path\", type=str, help=\".jsonl or jsonl.gz OA export\")\n    parser.add_argument(\"--lang\", type=str, help=\"comma separated list of lang-codes\")\n    args = parser.parse_args()\n    return args\n\n\ndef main():\n    args = parse_args()\n    lang_codes = None\n    if args.lang:\n        lang_codes = args.lang.split(\",\")\n\n    print(f\"input file: {args.input_file_path}\")\n\n    if lang_codes is None:\n        print(\"Using languages: all\")\n    else:\n        print(f'Filtering languages: {\", \".join(lang_codes)}')\n\n    def tree_filter(tree: ExportMessageTree) -> bool:\n        return (\n            tree.tree_state == \"ready_for_export\"\n            and tree.prompt.review_result\n            and (lang_codes is None or tree.prompt.lang in lang_codes)\n        )\n\n    trees = read_message_tree_list(args.input_file_path, filter=tree_filter)\n    print(f\"{len(trees)} trees\")\n\n    all_messages = []\n    for t in trees:\n        visit_messages_depth_first(t.prompt, all_messages.append)\n    synthetic_messages = [m for m in all_messages if m.synthetic]\n    prompter_messages = [m for m in all_messages if m.role == \"prompter\"]\n    assistant_messages = [m for m in all_messages if m.role == \"assistant\"]\n\n    print(f\"{len(all_messages)} messages\")\n    print(f\"{len(synthetic_messages)} synthetic messages\")\n    print(f\"{len(prompter_messages)} prompter messages\")\n    print(f\"{len(assistant_messages)} assistant messages\")\n\n    prompter_with_replies = [m for m in prompter_messages if m.replies and len(m.replies) > 1]\n    print(f\"{len(prompter_with_replies)} prompter messages with >1 reply\")\n\n    prompter_with_replies_ranked = [\n        m\n        for m in prompter_messages\n        if m.replies and len([rm for rm in m.replies if rm.rank is not None and rm.rank >= 0]) > 1\n    ]\n    print(f\"{len(prompter_with_replies_ranked)} prompter messages with >1 ranked reply\")\n\n\nif __name__ == \"__main__\":\n    main()\n", "model/model_training/tools/model_cli.py": "#!/usr/bin/env python3\nimport argparse\nimport time\n\nimport torch\nimport transformers\nfrom model_training.custom_datasets.formatting import QA_SPECIAL_TOKENS, format_pairs, format_system_prefix\nfrom model_training.models import get_specific_model\nfrom model_training.utils.utils import _strtobool\nfrom tokenizers import pre_tokenizers\n\nif __name__ == \"__main__\":\n    import warnings\n\n    warnings.filterwarnings(\"ignore\")\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--model_path\", type=str, required=True)\n    parser.add_argument(\"--max_new_tokens\", type=int, default=200)\n    parser.add_argument(\"--top_k\", type=int, default=None)\n    parser.add_argument(\"--top_p\", type=int, default=0.9)\n    parser.add_argument(\"--temperature\", type=float, default=0.8)\n    parser.add_argument(\"--do-sample\", type=_strtobool, default=True)\n    parser.add_argument(\"--format\", type=str, default=\"v2\")\n    parser.add_argument(\"--8bit\", action=\"store_true\", dest=\"eightbit\")\n    parser.add_argument(\"--system_prefix\", type=str, default=None)\n    parser.add_argument(\"--cache_dir\", type=str, default=None)\n    parser.add_argument(\"--per-digit-tokens\", action=\"store_true\")\n    parser.add_argument(\"--load_checkpoint\", type=str, default=None)\n    args = parser.parse_args()\n\n    if args.load_checkpoint is not None:\n        print(\"Loading from\", args.load_checkpoint)\n        # \"ckpts_save/best_checkpoint/pytorch_model/mp_rank_00_model_states.pt\"\n        ckpt = torch.load(args.load_checkpoint)\n        import IPython\n\n        IPython.embed()\n        model = get_specific_model(args.model_path, torch_dtype=torch.float16, cache_dir=args.cache_dir)\n\n        base_dict = {k[11:]: v for k, v in ckpt[\"module\"].items() if not k.startswith(\"base_model.transformer\")}\n        # base_dict = {k[11:]: v for k, v in ckpt['module'].items()}\n        print(model.load_state_dict(base_dict, strict=False))\n    else:\n        if args.eightbit:\n            model = get_specific_model(\n                args.model_path,\n                load_in_8bit=True,\n                device_map=\"auto\",\n                low_cpu_mem_usage=True,\n                torch_dtype=torch.float16,\n                offload_state_dict=True,\n                cache_dir=args.cache_dir,\n            )\n        else:\n            model = get_specific_model(args.model_path, cache_dir=args.cache_dir, torch_dtype=torch.float16)\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model = model.to(device)\n\n    model.gradient_checkpointing_enable()  # reduce number of stored activations\n    tokenizer = transformers.AutoTokenizer.from_pretrained(args.model_path)\n    # tokenizer = transformers.AutoTokenizer.from_pretrained(\"dvruette/oasst-pythia-6.9b-4000-steps\")\n    if args.per_digit_tokens:\n        tokenizer._tokenizer.pre_processor = pre_tokenizers.Digits(True)\n\n    human_token_id = tokenizer.additional_special_tokens_ids[\n        tokenizer.additional_special_tokens.index(QA_SPECIAL_TOKENS[\"Question\"])\n    ]\n\n    print('Type \"quit\" to exit')\n    print(\"Press Control + C to restart conversation (spam to exit)\")\n\n    conversation_history = []\n\n    while True:\n        try:\n            user_input = input(\"User: \")\n            if user_input == \"quit\":\n                break\n\n            conversation_history.append(user_input)\n\n            batch = tokenizer.encode(\n                format_system_prefix(args.system_prefix, tokenizer.eos_token)\n                if args.system_prefix\n                else \"\"\n                + \"\".join(format_pairs(conversation_history, tokenizer.eos_token, add_initial_reply_token=True)),\n                return_tensors=\"pt\",\n            )\n\n            with torch.cuda.amp.autocast():\n                out = model.generate(\n                    input_ids=batch.to(model.device),\n                    min_new_tokens=4,\n                    max_new_tokens=args.max_new_tokens,\n                    do_sample=args.do_sample,\n                    top_k=args.top_k,\n                    top_p=args.top_p,\n                    temperature=args.temperature,\n                    eos_token_id=tokenizer.eos_token_id,\n                    pad_token_id=tokenizer.eos_token_id,\n                )\n\n            if out[0][-1] == tokenizer.eos_token_id:\n                response = out[0][:-1]\n            else:\n                response = out[0]\n\n            response = tokenizer.decode(response).split(QA_SPECIAL_TOKENS[\"Answer\"])[-1]\n            print(f\"Bot: {response}\")\n            conversation_history.append(response)\n        except KeyboardInterrupt:\n            conversation_history = []\n            print()\n            print(\"Conversation restarted\")\n            time.sleep(1)\n            continue\n        except EOFError:  # Catch ctrl+d\n            print()\n            break\n", "model/model_training/tools/augment_oasst.py": "\"\"\"\n    Augment oasst dataset with sft generated results\n\n    You can use augment new response using a model with bad response, ie non SFT model\n\n    had to do this in a quick fashion, please tolerate the hackiness in the code\n\n\"\"\"\nimport json\nimport os\n\n# so far load_oasst_export is pretty deterministic in thread order\n# means the train, val split stay the same\nfrom model_training.custom_datasets.oasst_dataset import load_oasst_export\nfrom model_training.models.reward_model import GPTNeoXRewardModel\nfrom tqdm import tqdm\nfrom transformers import AutoModelForCausalLM, AutoModelForSequenceClassification, AutoTokenizer\n\n\nclass AggregateResults:\n    def __init__(self, reward_model) -> None:\n        if \"pythia\" in reward_model:\n            rank_model = GPTNeoXRewardModel.from_pretrained(reward_model)\n        else:\n            rank_model = AutoModelForSequenceClassification.from_pretrained(reward_model)\n        self.rank_tokenizer = AutoTokenizer.from_pretrained(reward_model)\n        self.rank_model = rank_model.half().cuda()\n\n    def scoring(self, prefixes, answer):\n        question = self.rank_tokenizer.sep_token.join(prefixes)\n        inputs = self.rank_tokenizer(question, answer, return_tensors=\"pt\").to(0)\n        score = self.rank_model(**inputs).logits[0].cpu().detach()\n        return score\n\n    def aggregate(self, jsonl_filenames, dataset, split=\"val\"):\n        augmented = {}\n        for train_augmented_filename in jsonl_filenames:\n            with open(train_augmented_filename, \"r\") as f:\n                for line in tqdm(f):\n                    payload = json.loads(line)\n                    idx = payload[\"idx\"]\n                    if idx not in augmented:\n                        augmented[idx] = []\n                    if len(payload[\"gen_samples\"]) == 0:\n                        continue\n                    try:\n                        scores = [\n                            (float(self.scoring(payload[\"prefixes\"], sample)), sample)\n                            for sample in payload[\"gen_samples\"]\n                        ]\n                        sorted_scores = sorted(scores, key=lambda x: x[0], reverse=True)\n                        augmented[idx].append(sorted_scores[0][1])\n                    except RuntimeError as e:\n                        print(e)\n                        continue\n\n        with open(f\"augmented_cycliric_oasst_2023-03-27_{split}.jsonl\", \"w\") as f:\n            for idx, payload in tqdm(enumerate(dataset), total=len(dataset), dynamic_ncols=True):\n                output = {\n                    \"prefixes\": payload[0],\n                    \"responses\": payload[1],\n                    \"augmented\": [],\n                    \"split\": split,\n                }\n                if idx in augmented:\n                    augmented = augmented[idx]\n                    cleaned_aug = []\n                    for a in augmented:\n                        cleaned = (\n                            a.replace(\"<|endoftext|>\", \"\")\n                            .replace(\"<|startoftoken|>human\\n\", \"\")\n                            .replace(\"<human>\", \"\")\n                            .replace(\"<bot>\", \"\")\n                        )\n                        cleaned_aug.append(cleaned)\n                    output[\"augmented\"] = cleaned_aug\n                f.write(json.dumps(output) + \"\\n\")\n\n\ndef r2_conversation(prefixes, tokenizer, model, top_k=10, temperature=0.7, max_new_tokens=512, model_name=\"\"):\n    text = \"\"\n    for idx, convo in enumerate(prefixes):\n        if idx % 2 == 0:\n            text += \"<|startoftoken|>human\\n\" + convo + \"<|endoftoken|>\"\n        else:\n            text += \"<|startoftoken|>assistant\\n\" + convo + \"<|endoftoken|>\"\n    input_text = text + \"<|startoftoken|>assistant\\n\"\n    inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True).to(0)\n\n    generated_samples = []\n    try:\n        outputs = model.generate(\n            **inputs,\n            early_stopping=False,\n            max_new_tokens=max_new_tokens,\n            num_return_sequences=top_k,\n            do_sample=True,\n            temperature=temperature,\n            pad_token_id=tokenizer.eos_token_id,\n            # dialogue_collator.py line 36\n        )\n        gen_sequences = outputs.sequences[:, inputs[\"input_ids\"].shape[-1] :]\n        for output in gen_sequences:\n            decoded = tokenizer.decode(\n                output, truncate_before_pattern=[r\"\\n\\n^#\", \"^'''\", \"\\n\\n\\n\"], skip_special_tokens=True\n            )\n            answer = decoded.split(\"<|endoftext|>\")[0]\n            if len(answer) > 0:\n                generated_samples.append(answer)\n    except RuntimeError as err:\n        print(err)\n\n    return generated_samples\n\n\ndef r0_conversation(prefixes, tokenizer, model, top_k=10, temperature=0.7, max_new_tokens=512, model_name=\"\"):\n    text = \"\"\n    for idx, convo in enumerate(prefixes):\n        if idx % 2 == 0:\n            text += \"<human>\" + convo\n        else:\n            text += \"<bot>\" + convo + \"<|endoftoken|>\"\n    input_text = text + \"<bot>\"\n    inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True).to(0)\n\n    generated_samples = []\n    try:\n        outputs = model.generate(\n            **inputs,\n            early_stopping=False,\n            max_new_tokens=max_new_tokens,\n            num_return_sequences=top_k,\n            do_sample=True,\n            temperature=temperature,\n            pad_token_id=tokenizer.eos_token_id,\n            # dialogue_collator.py line 36\n        )\n        gen_sequences = outputs.sequences[:, inputs[\"input_ids\"].shape[-1] :]\n        for output in gen_sequences:\n            decoded = tokenizer.decode(\n                output, truncate_before_pattern=[r\"\\n\\n^#\", \"^'''\", \"\\n\\n\\n\"], skip_special_tokens=True\n            )\n            answer = decoded.split(\"<|endoftext|>\")[0]\n            if len(answer) > 0:\n                generated_samples.append(answer)\n    except RuntimeError as err:\n        print(err)\n\n    return generated_samples\n\n\ndef rallio_conversation(prefixes, tokenizer, model, top_k=2, temperature=0.7, max_new_tokens=512, model_name=\"Chip2\"):\n    name = \"Chip2\"\n    if \"Chip2\" in model_name:\n        name = \"Chip2\"\n    elif \"Kitt\" in model_name:\n        name = \"Kitt\"\n\n    text = \"\"\n    for idx, convo in enumerate(prefixes):\n        if idx % 2 == 0:\n            text += \"User: \" + convo + \"\\n\"\n        else:\n            text += name + \": \" + convo + \"\\n\"\n    input_text = text + name + \": \"\n    inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True).to(0)\n\n    generated_samples = []\n    try:\n        outputs = model.generate(\n            **inputs,\n            early_stopping=False,\n            max_new_tokens=max_new_tokens,\n            do_sample=True,\n            pad_token_id=tokenizer.eos_token_id,\n            num_return_sequences=top_k,\n            top_p=0.95,\n            temperature=0.5,\n            penalty_alpha=0.6,\n            output_scores=True,\n            return_dict_in_generate=True,\n            repetition_penalty=1.03,\n            use_cache=True\n            # dialogue_collator.py line 36\n        )\n        gen_sequences = outputs.sequences[:, inputs[\"input_ids\"].shape[-1] :]\n        for output in gen_sequences:\n            decoded = tokenizer.decode(\n                output, truncate_before_pattern=[r\"\\n\\n^#\", \"^'''\", \"\\n\\n\\n\"], skip_special_tokens=True\n            )\n            answer = decoded.split(\"<|endoftext|>\")[0]\n            if len(answer) > 0:\n                generated_samples.append(answer)\n    except (RuntimeError, ValueError) as e:\n        print(e)\n\n    return generated_samples\n\n\ndef augment_conversation(model_name, dataset, split=\"train\"):\n    if \"-r2\" in model_name:  # OAI format\n        chat_handler = r2_conversation\n    elif \"Rallio\" in model_name:\n        chat_handler = rallio_conversation\n    else:  # <human>, <bot>\n        chat_handler = r0_conversation\n    chat_handler = r2_conversation\n\n    model = AutoModelForCausalLM.from_pretrained(model_name, cache_dir=\".cache/\").eval().half().cuda()\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    output_file = \"{}_2023-03-27-all_{}_{}.jsonl\".format(model_name.replace(\"/\", \"-\"), languages, split)\n    added = set()\n    if os.path.exists(output_file):\n        with open(output_file, \"r\") as f:\n            for line in f:\n                row = json.loads(line)\n                added.add(row[\"idx\"])\n    with open(output_file, \"a\") as fout:\n        for idx, row in tqdm(enumerate(dataset), total=len(dataset), dynamic_ncols=True):\n            if idx in added:\n                continue\n            prefixes, answers = row\n            samples = chat_handler(\n                prefixes, tokenizer, model, temperature=0.1, top_k=8, max_new_tokens=256, model_name=model_name\n            )\n            fout.write(\n                json.dumps({\"prefixes\": prefixes, \"answers\": answers, \"gen_samples\": samples, \"idx\": idx}) + \"\\n\"\n            )\n            fout.flush()\n\n\nif __name__ == \"__main__\":\n    import glob\n\n    # model_name = 'bigscience/bloom-560m'\n    model_name = \"theblackcat102/pythia-1b-deduped-sft\"\n    # latin_cyrillic\n    languages = \"bg,ca,cs,da,de,en,es,fr,hr,hu,it,nl,pl,pt,ro,ru,sl,sr,sv,uk\"\n    train, val = load_oasst_export(\".cache/2023-03-27_oasst_research_all.jsonl.gz\", lang=languages, mode=\"rm\")\n\n    print(len(train), len(val))\n    augment_conversation(model_name, train, split=\"train\")\n    augment_conversation(model_name, val, split=\"val\")\n\n    agg = AggregateResults(\"theblackcat102/reward-model-deberta-v3-base-v2\")\n    agg.aggregate(glob.glob(\"*_val.jsonl\"), val, \"val\")\n    agg.aggregate(glob.glob(\"*_train.jsonl\"), train, \"train\")\n", "model/model_training/utils/losses.py": "import torch\nimport torch.nn.functional as F\nfrom torch import nn\n\n\nclass CrossEntropyLoss(nn.CrossEntropyLoss):\n    def __init__(self, weight=None, size_average=None, ignore_index=-100, reduce=None, reduction=\"mean\"):\n        super().__init__(weight, size_average, ignore_index, reduce, \"none\")\n        self._reduction = reduction\n\n    def forward(self, input, target, mask=None):\n        input = input.view(-1, input.size(-1))\n        target = target.view(-1)\n\n        if mask is not None:\n            mask = mask.view(-1).bool()\n            input = input[mask]\n            target = target[mask]\n\n        size = target.numel()\n\n        loss = super().forward(input, target)\n\n        if self._reduction == \"none\":\n            return loss\n        return loss.sum() / (size + 1e-8)\n\n\nclass PolyLoss(nn.Module):\n    def __init__(self, weight=None, size_average=None, ignore_index=-100, reduce=None, reduction=\"mean\", epsilon=1.0):\n        super().__init__()\n        self.weight = torch.tensor(weight)\n        self.ignore_index = ignore_index\n        self.reduction = reduction\n        self.cross_entropy = CrossEntropyLoss(weight, size_average, ignore_index, reduce, \"none\")\n        self.epsilon = epsilon\n\n    def forward(self, input, target, mask=None):\n        if mask is not None:\n            mask = mask.view(-1).bool()\n            input = input.view(-1, input.size(-1))\n            target = target.view(-1)\n            input = input[mask]\n            target = target[mask]\n\n        onehot_target = F.one_hot(target, num_classes=input.size(-1)).to(device=input.device, dtype=input.dtype)\n        pt = torch.sum(onehot_target * F.softmax(input, -1), -1)\n        CE = self.cross_entropy(input, target)\n        poly1 = CE + self.epsilon * (1 - pt)\n        if self.reduction == \"mean\":\n            poly1 = poly1.mean()\n        elif self.reduction == \"sum\":\n            poly1 = poly1.sum()\n        return poly1\n\n\nclass RMLoss(nn.Module):\n    def __init__(self, reduction=\"mean\", beta=0.001):\n        super().__init__()\n        self.reduction = reduction\n        self.beta = beta\n\n    def forward(self, logits, cu_lengths=None):\n        # if cu_lengths is None, assume that all examples belong to the same conversation\n        if cu_lengths is None:\n            cu_lengths = [0, logits.size(0)]\n\n        device = logits.device\n        losses = []\n        for start, end in zip(cu_lengths[:-1], cu_lengths[1:]):\n            pairs = torch.combinations(torch.arange(end - start, device=device), 2)\n            pos_ids, neg_ids = pairs[:, 0], pairs[:, 1]\n            pos_logits = logits.take(start + pos_ids)\n            neg_logits = logits.take(start + neg_ids)\n\n            l2 = 0.5 * (pos_logits**2 + neg_logits**2)\n            _loss = (-F.logsigmoid(pos_logits - neg_logits) + self.beta * l2).mean()\n            losses.append(_loss)\n        loss = torch.stack(losses)\n\n        if self.reduction == \"none\":\n            return loss\n        return loss.mean()\n\n\nclass RMCLSLoss(nn.CrossEntropyLoss):\n    def __init__(self, weight=None, size_average=None, ignore_index=-100, reduce=None, reduction=\"mean\"):\n        super().__init__(weight, size_average, ignore_index, reduce, \"none\")\n        self._reduction = reduction\n\n    def forward(self, logits, cu_lengths=None):\n        # if cu_lengths is None, assume that all examples belong to the same conversation\n        if cu_lengths is None:\n            cu_lengths = [0, logits.size(0)]\n\n        device = logits.device\n        logit_pairs = []\n        # aggregate combination between ranks\n        for start, end in zip(cu_lengths[:-1], cu_lengths[1:]):\n            pairs = torch.combinations(torch.arange(end - start, device=device), 2)\n            pos_ids, neg_ids = pairs[:, 0], pairs[:, 1]\n            pos_logits = logits.take(start + pos_ids)\n            neg_logits = logits.take(start + neg_ids)\n            merged = torch.stack((pos_logits, neg_logits), dim=1)\n            logit_pairs.append(merged)\n        logit_pairs = torch.concat(logit_pairs, dim=0)\n        labels = torch.zeros(logit_pairs.shape[0], dtype=torch.long, device=device)\n        loss = super().forward(logit_pairs, labels)\n\n        if self._reduction == \"none\":\n            return loss\n        return loss.mean()\n", "model/model_training/utils/utils.py": "import argparse\nimport copy\nimport math\nimport random\nfrom distutils.util import strtobool\nfrom pathlib import Path\nfrom typing import List, NamedTuple\n\nimport evaluate\nimport torch\nimport transformers\nimport yaml\nfrom model_training.custom_datasets import get_one_dataset\nfrom model_training.custom_datasets.formatting import QA_SPECIAL_TOKENS\nfrom model_training.models import freeze_top_n_layers, get_specific_model\nfrom model_training.models.patching import patch_model\nfrom model_training.models.prefix_llama import LlamaForCausalLM\nfrom model_training.models.reward_model import GPTNeoXRewardModel\nfrom sklearn.model_selection import train_test_split\nfrom tokenizers import pre_tokenizers\nfrom torch.utils.data import ConcatDataset, Dataset, Subset\nfrom torch.utils.data.distributed import DistributedSampler\n\nfrom .losses import CrossEntropyLoss, PolyLoss, RMCLSLoss, RMLoss\n\n\ndef _strtobool(x):\n    return bool(strtobool(x))\n\n\ndef init_rng(conf: argparse.Namespace) -> None:\n    seed = conf.rng_seed\n    if seed is not None:\n        print(f\"RNG seed: {seed}\")\n        transformers.set_seed(seed)\n\n\nclass PerDatasetSampler(DistributedSampler):\n    \"\"\"Sampler which returns a fixed number of samples per dataset, per epoch.\n\n    Example:\n\n    Dataset 1 has 10,000 examples and we want 200 per epoch\n    Dataset 2 has 500 examples and we want all 500 per epoch\n\n    Epoch size will be 700 and every epoch we'll sample a different\n    200 from dataset 1.\n\n    Parameters\n    ----------\n    dataset_sizes : List[int]\n        A list with the size of each dataset.\n    dataset_size_per_epoch : List[int]\n        How many examples to get from each dataset per epoch.\n\n    Note: dataset_sizes & dataset_size_per_epoch must be in the same order.\n    Further the examples in the underlying torch.utils.data.Dataset\n    must per ordered as dataset_1, dataset_2, ..., dataset_n. This is fine\n    if we concatenate a bunch of datasets together\n    e.g. using torch.utils.data.ConcatDataset which is current behaviour.\n    \"\"\"\n\n    def __init__(\n        self,\n        dataset_sizes: List[int],\n        dataset_size_per_epoch: List[int],\n        rank: int = None,\n        world_size: int = None,\n        shuffle: bool = True,\n        seed: int = 0,\n        samples_length: List[int] = None,\n    ):\n        \"\"\"\n        if samples_length is not None, then the sampler\n        will order the samples by dataset length\n        with some variability across epochs\n        \"\"\"\n        self.dataset_sizes = dataset_sizes\n        self.dataset_size_per_epoch = dataset_size_per_epoch\n        self.num_datasets = len(dataset_sizes)\n        self.shuffle = shuffle\n        self.rank = rank\n        self.world_size = world_size\n        self.epoch = 0\n\n        if world_size == 1:\n            self.rank = 0\n\n        self.num_samples = sum(dataset_size_per_epoch)\n        self.seed = seed\n        self.samples_length = samples_length\n\n    def set_epoch(self, epoch: int) -> None:\n        self.epoch = epoch\n\n    def __len__(self) -> int:\n        return self.num_samples // self.world_size\n\n    def __iter__(self):\n        epoch_idx = []\n        n = 0\n\n        random.seed(self.epoch + self.seed)\n\n        for i in range(self.num_datasets):\n            sampled_idx = random.sample(range(n, self.dataset_sizes[i] + n), self.dataset_size_per_epoch[i])\n            n += self.dataset_sizes[i]\n            epoch_idx.extend(sampled_idx)\n\n        if self.samples_length is not None:\n            # sort by samples length and in case of ties randomize\n            epoch_idx = sorted(epoch_idx, key=lambda x: (self.samples_length[x], random.random()))\n\n            if self.shuffle:\n                # do some minor shuffling to avoid repeating the same order\n                # but not too much to avoid too much padding\n                # quasi random basically\n                for i in range(0, len(epoch_idx), 200):  # this should be batch_size dependent\n                    random.shuffle(epoch_idx[i : i + 200])\n        else:\n            if self.shuffle:\n                random.shuffle(epoch_idx)\n\n        # split epoch_idx in world_size chunks\n        epoch_idx = epoch_idx[self.rank : self.num_samples : self.world_size]\n\n        return iter(epoch_idx)\n\n    @classmethod\n    def build_sampler_from_config(cls, training_conf, datasets: List[Dataset], verbose: bool = False, **kwargs):\n        dataset_sizes = [len(x) for x in datasets]\n        fractions = get_dataset_fractions(training_conf.datasets, dataset_sizes, verbose)\n        dataset_size_per_epoch = [int(size * frac) for size, frac in zip(dataset_sizes, fractions)]\n        seed = training_conf.rng_seed\n        return cls(dataset_sizes=dataset_sizes, dataset_size_per_epoch=dataset_size_per_epoch, seed=seed, **kwargs)\n\n\ndef get_dataset_fractions(conf, dataset_sizes: List[int], verbose: bool = False):\n    \"\"\"Calculate fraction of each dataset to use per epoch when sub-sampling\"\"\"\n\n    if verbose:\n        print(\"Creating sampler for datasets:\")\n\n    fractions = []\n    for i, data_config in enumerate(conf):\n        dataset_name, _ = get_dataset_name_and_kwargs_from_data_config(data_config)\n        if isinstance(data_config, dict):\n            if \"fraction\" in data_config[dataset_name]:\n                if data_config[dataset_name][\"fraction\"] <= 0:\n                    raise ValueError(\"Please specify fraction as a value between 0 < fraction <= 1\")\n                fractions.append(min(1, data_config[dataset_name][\"fraction\"]))\n            elif \"size\" in data_config[dataset_name]:\n                if data_config[dataset_name][\"size\"] > dataset_sizes[i]:\n                    raise ValueError(f\"Please specify a size smaller than number of examples: {dataset_sizes[i]:,.0f}\")\n                fractions.append(data_config[dataset_name][\"size\"] / dataset_sizes[i])\n            else:\n                fractions.append(1)\n        else:\n            fractions.append(1)\n\n        if verbose:\n            print(f\"{dataset_name}: {fractions[-1]:.2%} ({int(dataset_sizes[i]*fractions[-1])})\")\n    return fractions\n\n\nclass SpecialTokens(NamedTuple):\n    pad_token: str = \"\"\n    eos_token: str = \"\"\n    sep_token: str = \"\"\n\n\nclass TokenizerConfig(NamedTuple):\n    special_tokens: SpecialTokens = {}\n\n\nTOKENIZER_CONFIGS = {\n    \"galactica\": TokenizerConfig(special_tokens=SpecialTokens(\"<pad>\", \"</s>\")),\n    \"GPT-JT\": TokenizerConfig(special_tokens=SpecialTokens(sep_token=\"<|extratoken_100|>\")),\n    \"codegen\": TokenizerConfig(special_tokens=SpecialTokens(\"<|endoftext|>\", sep_token=\"<|endoftext|>\")),\n    \"pythia\": TokenizerConfig(special_tokens=SpecialTokens(\"<|padding|>\", \"<|endoftext|>\", \"<|endoftext|>\")),\n    \"gpt-neox\": TokenizerConfig(special_tokens=SpecialTokens(\"<|padding|>\", \"<|endoftext|>\", \"<|endoftext|>\")),\n    \"llama\": TokenizerConfig(special_tokens=SpecialTokens(\"</s>\", \"</s>\", sep_token=\"<s>\")),\n    \"cerebras\": TokenizerConfig(special_tokens=SpecialTokens(\"<|endoftext|>\", \"<|endoftext|>\", \"<|endoftext|>\")),\n    \"deberta-v3\": TokenizerConfig(special_tokens=SpecialTokens(\"[PAD]\", \"[SEP]\", sep_token=\"[CLS]\")),\n    \"bloom\": TokenizerConfig(special_tokens=SpecialTokens(\"<pad>\", \"</s>\", \"<s>\")),\n    \"electra\": TokenizerConfig(special_tokens=SpecialTokens(\"[PAD]\", \"[SEP]\", sep_token=\"[CLS]\")),\n    \"falcon\": TokenizerConfig(\n        special_tokens=SpecialTokens(\"<|endoftext|>\", \"<|endoftext|>\", sep_token=\"<|endoftext|>\")\n    ),\n    \"LLongMA\": TokenizerConfig(special_tokens=SpecialTokens(\"</s>\", \"</s>\", sep_token=\"<s>\")),\n}\n\n\ndef match_tokenizer_name(model_name: str) -> TokenizerConfig:\n    \"\"\"\n    Match a partial model name to a tokenizer configuration\n    i.e. model_name `Salesforce/codegen-2B-multi` has config name `codegen`\n    \"\"\"\n    tokenizer_config_matches = [config for name, config in TOKENIZER_CONFIGS.items() if name in model_name]\n    if not tokenizer_config_matches:\n        raise ValueError(f\"Cannot find any tokeniser configuration to match {model_name=}\")\n    elif 1 < len(tokenizer_config_matches):\n        raise ValueError(f\"Found multiple tokeniser configuration matches for {model_name=}\")\n    else:\n        return tokenizer_config_matches[0]\n\n\ndef get_tokenizer(conf) -> transformers.AutoTokenizer:\n    tokenizer_name = conf.model_name\n\n    if \"cerebras\" in conf.model_name:\n        # Only 13B has a tokenizer available on HF\n        tokenizer_name = \"cerebras/Cerebras-GPT-13B\"\n\n    tokenizer = transformers.AutoTokenizer.from_pretrained(tokenizer_name, cache_dir=conf.cache_dir)\n\n    tokenizer_config = match_tokenizer_name(conf.model_name)\n\n    if hasattr(conf, \"per_digit_tokens\") and conf.per_digit_tokens:\n        tokenizer._tokenizer.pre_processor = pre_tokenizers.Digits(True)\n\n    if tokenizer_config.special_tokens:\n        if \"GPT-JT\" in conf.model_name:\n            tokenizer_config.special_tokens.pad_token = tokenizer.eos_token\n        # SpecialTokens : latest in 4.25, 4.26\n        tokenizer.add_special_tokens(\n            {\n                \"pad_token\": tokenizer_config.special_tokens.pad_token,\n                \"eos_token\": tokenizer_config.special_tokens.eos_token,\n                \"sep_token\": tokenizer_config.special_tokens.sep_token,\n            }\n        )\n\n    additional_special_tokens = (\n        []\n        if \"additional_special_tokens\" not in tokenizer.special_tokens_map\n        else tokenizer.special_tokens_map[\"additional_special_tokens\"]\n    )\n    additional_special_tokens = list(set(additional_special_tokens + list(QA_SPECIAL_TOKENS.values())))\n\n    tokenizer.add_special_tokens({\"additional_special_tokens\": additional_special_tokens})\n\n    return tokenizer\n\n\ndef default_preprocess(eval_pred, ignote_negative_labels=True):\n    preds, labels = eval_pred.predictions, eval_pred.label_ids\n\n    if not ignote_negative_labels:\n        return preds, labels\n\n    mask = labels > 0\n    return preds[mask], labels[mask]\n\n\n# placeholder for now\ndef preprocess_qa(eval_pred):\n    return (eval_pred.predictions, eval_pred.label_ids)\n\n\n# def postprocess_summarization(preds, labels):\n#     preds = [pred.strip() for pred in preds]\n#     labels = [label.strip() for label in labels]\n\n#     preds = [\"\\n\".join(nltk.sent_tokenize(pred)) for pred in preds]\n#     labels = [\"\\n\".join(nltk.sent_tokenize(label)) for label in labels]\n\n#     return preds, labels\n\n\n# def preprocess_summarization(eval_pred, tokenizer, ignore_pad_token_for_loss=True):\n#     preds, labels = eval_pred\n#     decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n#     if ignore_pad_token_for_loss:\n#         labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n#     decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n\n#     decoded_preds, decoded_labels = postprocess_summarization(decoded_preds, decoded_labels)\n#     return decoded_preds, decoded_labels\n\n\ndef get_metrics(conf, tokenizer):\n    # the reason behind using a list is that we might want to extend the list of our\n    # metrics in the future for more thorough evaluation\n    metrics, preprocess_fns = [evaluate.load(\"accuracy\")], [default_preprocess]\n\n    # if any(dataset in QA_DATASETS for dataset in conf.datasets):\n    #     raise ValueError(\"TODO\")\n    #     metrics.append(evaluate.load(\"squad_v2\"))\n    #     preprocess_fns.append(preprocess_qa)\n    # if any(dataset in SUMMARIZATION_DATASETS for dataset in conf.datasets):\n    #     raise ValueError(\"TODO\")\n    #     metrics.append(evaluate.load(\"rouge\"))\n    #     preprocess_fns.append(\n    #         partial(preprocess_summarization, tokenizer, ignore_pad_token_for_loss=conf.ignore_pad_token_for_loss)\n    #     )\n\n    return metrics, preprocess_fns\n\n\ndef get_model(conf, tokenizer, pad_vocab_size_to_multiple_of=16, check_freeze_layer=True):\n    dtype = torch.float32\n    if conf.dtype in [\"fp16\", \"float16\"]:\n        dtype = torch.float16\n    elif conf.dtype in [\"bf16\", \"bfloat16\"]:\n        dtype = torch.bfloat16\n\n    if conf.is_reward_model:\n        if \"pythia\" in conf.model_name:\n            model = GPTNeoXRewardModel.from_pretrained(conf.model_name, cache_dir=conf.cache_dir, torch_dtype=dtype)\n\n            if conf.pooling:\n                assert conf.pooling in (\"mean\", \"last\"), f\"invalid pooling configuration '{conf.pooling}'\"\n                model.config.pooling = conf.pooling\n        else:\n            model = transformers.AutoModelForSequenceClassification.from_pretrained(\n                conf.model_name, cache_dir=conf.cache_dir, num_labels=1, torch_dtype=dtype\n            )\n    if not conf.is_reward_model:\n        if conf.peft_type is not None and conf.peft_type == \"prefix-tuning\" and \"llama\" in conf.model_name:\n            model = LlamaForCausalLM.from_pretrained(conf.model_name, cache_dir=conf.cache_dir, torch_dtype=dtype)\n        else:\n            model = get_specific_model(\n                conf.model_name,\n                cache_dir=conf.cache_dir,\n                quantization=conf.quantization,\n                seq2seqmodel=conf.seq2seqmodel,\n                without_head=conf.is_reward_model,\n                torch_dtype=dtype,\n            )\n\n        n_embs = model.get_input_embeddings().num_embeddings\n        if len(tokenizer) != n_embs or pad_vocab_size_to_multiple_of:\n            p = pad_vocab_size_to_multiple_of\n            target_size = len(tokenizer) if not p else math.ceil(len(tokenizer) / p) * p\n            print(\"Resizing embeddings to\", target_size)\n            model.resize_token_embeddings(target_size)\n\n        new_n_embs = model.get_input_embeddings().num_embeddings\n        if new_n_embs != n_embs and check_freeze_layer:\n            assert not conf.freeze_layer, \"Cannot change the number of embeddings if the model is frozen.\"\n\n        if conf.freeze_layer:\n            model = freeze_top_n_layers(model, conf.freeze_layer)\n\n    model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n    params = sum([p.numel() for p in model_parameters])\n    print(\"Number of trainable parameters: {}M\".format(int(params / 1e6)))\n\n    patch_model(\n        model,\n        resid_pdrop=conf.residual_dropout,\n        flash_attention=conf.use_flash_attention,\n        residual_dropout_lima=conf.residual_dropout_lima,\n    )\n\n    return model\n\n\ndef get_dataset_name_and_kwargs_from_data_config(data_config):\n    if isinstance(data_config, dict):\n        name = list(data_config.keys())[0]\n\n        # first copy the dict, then remove the size and fraction\n        kwargs = copy.deepcopy(data_config[name])\n\n        kwargs.pop(\"fraction\", None)\n        kwargs.pop(\"size\", None)\n        return name, kwargs\n    else:\n        return data_config, {}\n\n\ndef get_dataset(\n    conf,\n    mode: str = \"sft\",\n) -> tuple[ConcatDataset, dict[str, Subset]]:\n    train_datasets, evals = [], {}\n\n    for data_config in conf.datasets + conf.datasets_extra:\n        dataset_name, kwargs = get_dataset_name_and_kwargs_from_data_config(data_config)\n        train, val = get_one_dataset(conf, dataset_name, mode=mode, **kwargs)\n        train_datasets.append(train)\n\n        if val is not None:\n            evals[dataset_name] = Subset(val, list(range(min(len(val), conf.eval_size)))) if conf.eval_size else val\n\n    train = ConcatDataset(train_datasets)\n\n    return train, evals\n\n\ndef get_loss(loss, poly_eps: float = 1.0, score_l2_reg: float = 0.001):\n    if loss == \"CrossEntropyLoss\":\n        return CrossEntropyLoss()\n    elif loss == \"Poly\":\n        return PolyLoss(epsilon=poly_eps)\n    elif loss == \"RMLoss\":\n        return RMLoss(beta=score_l2_reg)\n    elif loss == \"RMCLSLoss\":\n        return RMCLSLoss()\n    else:\n        raise ValueError(f\"Loss {loss} not supported\")\n\n\ndef read_yamls(dir):\n    conf = {}\n    no_conf = True\n\n    for config_file in Path(dir).glob(\"**/*.yaml\"):\n        no_conf = False\n        with config_file.open(\"r\") as f:\n            conf.update(yaml.safe_load(f))\n\n    if no_conf:\n        print(f\"WARNING: No yaml files found in {dir}\")\n\n    return conf\n\n\ndef train_val_dataset(dataset, val_split=0.2):\n    if val_split == 0:\n        return dataset, None\n\n    train_idx, val_idx = train_test_split(\n        list(range(len(dataset))), test_size=val_split, random_state=666, shuffle=True\n    )\n    return Subset(dataset, train_idx), Subset(dataset, val_idx)\n\n\ndef process_output(output: str, method: str = \"v2\", bot_name: str = \"Joi\") -> str:\n    if method == \"v2\":\n        answer = output.split(QA_SPECIAL_TOKENS[\"Answer\"])[-1]\n        answer = answer.split(\"</s>\")[0].replace(\"<|endoftext|>\", \"\").lstrip().split(QA_SPECIAL_TOKENS[\"Answer\"])[0]\n    else:\n        answer = output.split(\"\\n\\n{}:\".format(bot_name))[-1]\n        answer = answer.split(\"</s>\")[0].replace(\"<|endoftext|>\", \"\").lstrip().split(\"\\n\\n{}:\".format(bot_name))[0]\n    return answer\n\n\ndef merge_dicts(default: dict, config: dict):\n    \"\"\"\n    merge default dict with config dict to override params\n    \"\"\"\n    for k, v in default.items():\n        if k not in config.keys():\n            config.update({k: v})\n\n    return config\n\n\ndef get_all_linear_layers(model):\n    cls = torch.nn.Linear\n\n    modules = {name.split(\".\")[-1] for name, module in model.named_modules() if isinstance(module, cls)}\n    if \"lm_head\" in modules:\n        modules.remove(\"lm_head\")\n\n    return list(modules)\n", "model/model_training/utils/ppo_utils.py": "import json\nimport math\nimport os\nimport warnings\nfrom time import time\nfrom typing import List, Tuple\n\nimport numpy as np\nimport torch\n\n# import torch.distributed as dist\nimport tritonclient.grpc as client_util\nimport trlx.utils.logging as logging\nfrom huggingface_hub import hf_hub_download\n\n# from torch import nn\nfrom torch.nn import functional as F\nfrom torch.utils.data import DataLoader\nfrom transformers import AutoTokenizer, DataCollatorWithPadding, PreTrainedTokenizer\nfrom trlx.data.ppo_types import PPORLElement\nfrom trlx.models.modeling_ppo import AutoModelForCausalLMWithHydraValueHead\nfrom trlx.pipeline import BasePipeline, register_datapipeline\nfrom trlx.trainer import register_trainer\nfrom trlx.trainer.accelerate_base_trainer import AccelerateRLTrainer\nfrom trlx.trainer.accelerate_ppo_trainer import AcceleratePPOTrainer\nfrom trlx.utils import Clock\nfrom trlx.utils.modeling import logprobs_of_labels\nfrom utils.utils import get_model\n\nfrom .utils_rl import prepare_tensor\n\nlogger = logging.get_logger(__name__)\n\n\nclass CustomCausalLMHydraWithValueHead(AutoModelForCausalLMWithHydraValueHead):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n\n    @classmethod\n    def from_pretrained(cls, config, tokenizer, kwargs=None, revision=None):  # noqa: max-complexity\n        \"\"\"\n        Our custom loader that just modifies the loading of the base model so that patching and other stuff are supported.\n        \"\"\"\n\n        # We may have modified the tokenizer to add the pad token\n        # Since we are decoding avoid pad the vocabulary as this will lead to undefined tokens for the tokenizer\n\n        # we only added a pad token, no need to check that embeddings are trained\n\n        # TODO change freeze_layer parameter here ..\n        base_model = get_model(config, tokenizer, pad_vocab_size_to_multiple_of=1, check_freeze_layer=False)\n\n        # DEBUG check if generation is working properly\n        # original_device = base_model.device\n        # base_model.cuda()\n        # tokens = tokenizer(\"<|prompter|>Can you explain to me how tides work?</s><|assistant|>\", add_special_tokens=False, return_tensors=\"pt\")\n        # output = base_model.generate(tokens.to(base_model.device)[\"input_ids\"], max_new_tokens=16, do_sample=False)\n        # print(tokenizer.decode(output[0]))\n        # base_model.to(original_device)\n\n        # print('Trainable parameters:')\n        # for name, param in base_model.named_parameters():\n        #     if param.requires_grad:\n        #         print(name)\n\n        # if config.ds_zero3:\n        #     print('Overriding model._get_logits_processor')\n        #     # always generate based on the max length. For Zero3 DS avoid getting stuck...\n        #     funcType = type(base_model._get_logits_processor)\n        #     base_model._get_logits_processor = funcType(_get_logits_processor, base_model)\n        #     funcType = type(base_model.sample)\n        #     base_model.sample = funcType(sample, base_model)\n\n        # model.ds_zero3 = config.ds_zero3\n        model = cls(base_model, num_layers_unfrozen=config.num_layers_unfrozen)\n\n        pretrained_model_name_or_path = config.model_name\n\n        if isinstance(pretrained_model_name_or_path, str):\n            filename = os.path.join(pretrained_model_name_or_path, \"pytorch_model.bin\")\n            sharded_index_filename = os.path.join(pretrained_model_name_or_path, \"pytorch_model.bin.index.json\")\n            is_sharded = False\n\n            if not os.path.exists(filename):\n                try:\n                    filename = hf_hub_download(pretrained_model_name_or_path, \"pytorch_model.bin\", revision=revision)\n                # Sharded\n                except Exception:\n                    if os.path.exists(sharded_index_filename):\n                        index_file_name = sharded_index_filename\n                    else:\n                        index_file_name = hf_hub_download(\n                            pretrained_model_name_or_path,\n                            \"pytorch_model.bin.index.json\",\n                            revision=revision,\n                        )\n                    with open(index_file_name, \"r\") as f:\n                        index = json.load(f)\n                    # Collect files containing weights from supported modules\n                    files_to_download = set()\n                    for k, v in index[\"weight_map\"].items():\n                        if any([module in k for module in cls._supported_modules]):\n                            files_to_download.add(v)\n                    is_sharded = True\n\n            if is_sharded:\n                # Merge each shard into a state dict\n                # TODO: Optimize this to avoid wasting RAM\n                state_dict = {}\n                for shard_file in files_to_download:\n                    filename = os.path.join(pretrained_model_name_or_path, shard_file)\n                    # Download if shard file doesn't exist locally\n                    if not os.path.exists(filename):\n                        filename = hf_hub_download(pretrained_model_name_or_path, shard_file, revision=revision)\n                    state_dict.update(torch.load(filename, map_location=\"cpu\"))\n            else:\n                state_dict = torch.load(filename, map_location=\"cpu\")\n        else:\n            state_dict = pretrained_model_name_or_path.state_dict()\n\n        model.post_init(state_dict=state_dict)\n        return model\n\n\n@register_trainer\nclass CustomPPOTrainer(AcceleratePPOTrainer, AccelerateRLTrainer):\n    def __init__(self, config, *args, **kwargs):\n        # hm...\n        self.tokenizer = AutoTokenizer.from_pretrained(\n            config.tokenizer.tokenizer_path\n        )  # Loading our model requires the tokenizer to be loaded first\n        # if pad token id is same as escape token id, then add a new token at the end of the vocab\n        if self.tokenizer.pad_token_id == self.tokenizer.eos_token_id:\n            self.tokenizer.add_special_tokens({\"pad_token\": \"<|padding|>\"})\n\n        # self.tokenizer.pad_token = self.tokenizer.eos_token\n        # self.tokenizer.pad_token_id = self.tokenizer.eos_token_id\n\n        self.tokenizer.padding_side = config.tokenizer.padding_side\n        self.tokenizer.truncation_side = config.tokenizer.truncation_side\n\n        print(\"len self.tokenizer\", len(self.tokenizer))\n\n        # print('len tokenizer', len(self.tokenizer))\n\n        super().__init__(*args, config=config, **kwargs)\n\n        # del self.ref_model\n        self.ref_model = triton_server_ref_model()\n\n    def decode(\n        self,\n        prompts: List[torch.LongTensor],\n        samples: List[torch.LongTensor],\n        prompt_sizes: torch.LongTensor = None,\n        append_eos_token: bool = True,\n    ) -> Tuple[List[str], List[str], List[str]]:\n        \"\"\"\n        Decode tensor generations into lists of strings (`samples`: List[str], `prompts`: List[str], `outputs`: List[str])\n        \"\"\"\n        assert append_eos_token is True\n\n        if prompt_sizes is None:\n            # Assuming prompts were left-padded\n            prompt_sizes = [prompts.shape[1]] * len(prompts)\n\n        str_samples, str_prompts, str_outputs = [], [], []\n\n        for prompt, sample, prompt_size in zip(prompts, samples, prompt_sizes):\n            if self.config.model.model_arch_type == \"seq2seq\":\n                raise NotImplementedError(\"Decoding for seq2seq models is not implemented yet\")\n                output_start_ix = 0\n            else:\n                output_start_ix = prompt_size\n\n            # Skip the padding token but not the other special tokens\n            PAD_TOKEN_ID = self.tokenizer.pad_token_id\n\n            if not torch.is_tensor(sample):\n                sample = torch.tensor(sample)\n\n            if not torch.is_tensor(prompt):\n                prompt = torch.tensor(prompt)\n\n            str_prompt = self.tokenizer.decode(\n                prompt[:prompt_size][prompt[:prompt_size] != PAD_TOKEN_ID], skip_special_tokens=False\n            )\n            # str_prompt = str_prompt.replace(PAD_TOKEN, \"\")\n\n            str_output = self.tokenizer.decode(\n                sample[output_start_ix:][sample[output_start_ix:] != PAD_TOKEN_ID], skip_special_tokens=False\n            )\n            # print('sample', self.tokenizer.decode(sample))\n            # print('prompt', self.tokenizer.decode(prompt))\n            # str_output = str_output.replace(PAD_TOKEN, \"\")\n\n            trimmed = False\n            # Trim outputs up to `self.stop_sequences` if any are present\n            if self.stop_sequences:\n                for stop in self.stop_sequences:\n                    stop_ix = str_output.find(stop)\n                    if stop_ix >= 0:\n                        str_output = str_output[:stop_ix].rstrip()\n                        trimmed = True\n\n            # Recover the last <eos> if it was present in the original sample\n            # or add one if it was trimmed with `self.stop_sequences`.\n            # Only in cases when a generation ended due to `max_new_tokens` exhaustion,\n            # <eos> token would not be present in the original sample\n            if append_eos_token and (trimmed or sample[-1] != self.tokenizer.eos_token_id):\n                str_output += self.tokenizer.eos_token\n\n            str_prompts.append(str_prompt)\n            str_outputs.append(str_output)\n\n            if self.config.model.model_arch_type == \"seq2seq\":\n                sample = str_prompt + self.tokenizer.sep_token + str_output\n            else:\n                sample = str_prompt + str_output\n\n            str_samples.append(sample)\n\n        return str_samples, str_prompts, str_outputs\n\n    def get_arch(self, config):\n        if config.model.model_arch_type == \"seq2seq\":\n            raise NotImplementedError(\"Seq2Seq models are not implemented yet\")\n            # model = Seq2SeqLMHydraWithValueHead(config.model.model_path, config.model.num_layers_unfrozen)\n        else:\n            model = CustomCausalLMHydraWithValueHead.from_pretrained(config.sft_config, self.tokenizer)\n\n        return model\n\n    def generate(self, input_ids, *args, **kwargs):\n        # if self.model.ds_zero3:\n        #     max_new_tokens = self.config.method.gen_kwargs['max_new_tokens']\n\n        #     if self.generate_experience_kwargs is not None:\n        #         if 'max_length' in self.generate_experience_kwargs:\n        #             self.generate_experience_kwargs.pop('max_length')\n\n        #         self.generate_experience_kwargs['max_new_tokens'] = max_new_tokens\n        #         self.generate_experience_kwargs['min_new_tokens'] = max_new_tokens\n        #         self.generate_experience_kwargs['eos_token_id'] = self.tokenizer.eos_token_id\n        #         self.generate_experience_kwargs['pad_token_id'] = self.tokenizer.pad_token_id\n        #     else:\n        #         if self.generate_kwargs is not None:\n        #             if 'max_length' in self.generate_kwargs:\n        #                 self.generate_kwargs.pop('max_length')\n\n        #             self.generate_kwargs['max_new_tokens'] = max_new_tokens\n        #             self.generate_kwargs['min_new_tokens'] = max_new_tokens\n        #             self.generate_kwargs['eos_token_id'] = self.tokenizer.eos_token_id\n        #             self.generate_kwargs['pad_token_id'] = self.tokenizer.pad_token_id\n\n        # print('---> Generate', input_ids, args, kwargs)\n        # print('self.generate_experience_kwargs', self.generate_experience_kwargs)\n        # print('self.generate_kwargs', self.generate_kwargs)\n\n        # self.model.eval()\n\n        # print('generation', self.tokenizer.decode(input_ids[0]))\n\n        kwargs[\"forced_eos_token_id\"] = self.tokenizer.eos_token_id\n        kwargs[\"suppress_tokens\"] = [self.tokenizer.pad_token_id]\n\n        preds = super().generate(input_ids, *args, **kwargs)\n\n        # self.model.train()\n\n        # print('Done generation', self.accelerator.device)\n\n        return preds\n\n    def generate_eval(self, input_ids, *args, **kwargs):\n        # if self.model.ds_zero3:\n        #     if 'max_length' in self.generate_kwargs:\n        #         self.generate_kwargs.pop('max_length')\n\n        #     max_new_tokens = self.config.method.gen_kwargs['max_new_tokens']\n        #     self.generate_kwargs['max_new_tokens'] = max_new_tokens\n        #     self.generate_kwargs['min_new_tokens'] = max_new_tokens\n        #     self.generate_kwargs['eos_token_id'] = self.tokenizer.eos_token_id\n        #     self.generate_kwargs['pad_token_id'] = self.tokenizer.pad_token_id\n\n        # self.model.train()\n\n        # print('generation_eval', self.tokenizer.decode(input_ids[0]))\n\n        # print('input_ids', input_ids[0])\n        # if 'attention_mask' in kwargs:\n        #     print('attention_mask', kwargs['attention_mask'][0])\n\n        kwargs[\"forced_eos_token_id\"] = self.tokenizer.eos_token_id\n        kwargs[\"suppress_tokens\"] = [self.tokenizer.pad_token_id]\n\n        preds = super().generate(input_ids, *args, **kwargs)\n\n        # print('Done generation', self.accelerator.device)\n\n        return preds\n\n    def make_experience(self, num_rollouts: int = 1024, iter_count: int = 0):  # noqa:\n        \"\"\"\n        Replace padding with pad_token_id\n        \"\"\"\n        logger.info(\"Collecting rollouts\")\n        tbar = logging.tqdm(\n            total=num_rollouts,\n            disable=os.environ.get(\"RANK\", 0) != \"0\",\n            desc=f\"[rollout 0 / {num_rollouts}]\",\n            # Lower progress bar by 1 if we're in WARNING mode or above to avoid hiding high priority progress\n            # bars (e.g. loss progress in trainers)\n            position=logging.get_verbosity() >= logging.WARNING,\n            # Leave progress bar if we're in INFO mode or lower to avoid spamming in suppressed verbosity levels\n            leave=logging.get_verbosity() < logging.WARNING,\n        )\n\n        ppo_rl_elements = []\n        stats = {}\n        clock = Clock()\n\n        while len(ppo_rl_elements) < num_rollouts:\n            # Get next batch in prompt dataset\n            batch = next(self.prompt_iterator)\n\n            exp_generate_time = time()\n\n            # Generate samples from the language model (similar to using HuggingFace `generate` method)\n            samples = self.generate(**batch)\n            stats[\"time/exp_generate\"] = time() - exp_generate_time\n\n            prompt_tensors = batch.input_ids\n            device = samples.device\n\n            prompt_sizes = torch.tensor([prompt_tensors.shape[1]] * len(prompt_tensors), device=device)\n\n            padded_samples = self.accelerator.pad_across_processes(\n                samples, dim=1, pad_index=self.tokenizer.pad_token_id, pad_first=False\n            )\n            padded_prompts = self.accelerator.pad_across_processes(\n                prompt_tensors, dim=1, pad_index=self.tokenizer.pad_token_id, pad_first=False\n            )\n            gathered_samples = self.accelerator.gather(padded_samples)\n            gathered_prompts = self.accelerator.gather(padded_prompts)\n            gathered_prompt_sizes = self.accelerator.gather(prompt_sizes)\n\n            if self.accelerator.is_main_process:\n                all_str_samples, all_str_prompts, all_str_outputs = self.decode(\n                    gathered_prompts, gathered_samples, gathered_prompt_sizes, append_eos_token=True\n                )\n\n                exp_score_time = time()\n                all_scores = torch.tensor(\n                    self.reward_fn(\n                        samples=all_str_samples,\n                        prompts=all_str_prompts,\n                        outputs=all_str_outputs,\n                    ),\n                    dtype=torch.float,\n                    device=device,\n                )\n                stats[\"time/exp_score\"] = time() - exp_score_time\n\n                all_scores = list(all_scores.reshape(self.accelerator.num_processes, -1).unbind())\n            else:\n                all_scores = None\n\n            if torch.distributed.is_initialized():\n                scores = torch.empty(len(samples), device=device)\n                torch.distributed.scatter(scores, all_scores)\n            else:\n                scores = all_scores[0].clone().detach()\n\n            str_samples, str_prompts, str_outputs = self.decode(prompt_tensors, samples, append_eos_token=True)\n\n            # Pad the sample outputs\n            outputs = self.tokenizer(str_outputs).input_ids\n            if self.config.model.model_arch_type == \"seq2seq\":\n                # add <pad> to the start of the output\n                for i in range(len(outputs)):\n                    outputs[i] = [self.tokenizer.pad_token_id] + outputs[i]\n\n            outputs = list(map(torch.LongTensor, outputs))\n            maxsize = max(map(len, outputs))\n            outputs = [\n                F.pad(\n                    output,\n                    (0, maxsize - len(output)),\n                    value=self.tokenizer.pad_token_id,\n                )\n                for output in outputs\n            ]\n            sample_outputs = torch.vstack(outputs).to(device)\n\n            # store statistics of the initial rollout as reference\n            if self.ref_mean is None:\n                self.ref_mean, self.ref_std = scores.mean(), scores.std()\n            all_scores_mean, all_scores_std = self.running_moments.update(scores)\n            stats[\"exp_scores/mean\"] = all_scores_mean.item()\n            stats[\"exp_scores/std\"] = all_scores_std.item()\n            stats[\"exp_scores/running_mean\"] = self.running_moments.mean.item()\n            stats[\"exp_scores/running_std\"] = self.running_moments.std.item()\n\n            if self.config.method.scale_reward == \"running\":\n                scores /= self.running_moments.std\n            elif self.config.method.scale_reward == \"ref\":\n                scores /= self.ref_std\n\n            clip_reward = self.config.method.cliprange_reward\n            if clip_reward:\n                scores = torch.clip(scores, -clip_reward, clip_reward)\n\n            # Precompute logprobs, values\n            if self.config.model.model_arch_type == \"seq2seq\":\n                raise NotImplementedError\n                attention_mask = batch.attention_mask.to(device)\n                prompt_tensors = batch.input_ids.to(device)\n                decoder_attention_mask = sample_outputs.not_equal(self.tokenizer.pad_token_id)\n                decoder_attention_mask[:, 0] = 1\n                with torch.no_grad():\n                    outputs = self.model(\n                        input_ids=prompt_tensors,\n                        attention_mask=attention_mask,\n                        decoder_input_ids=sample_outputs,\n                        decoder_attention_mask=decoder_attention_mask,\n                    )\n                    logits = outputs.logits\n                    values = outputs.value\n                    if hasattr(self.model, \"frozen_head\"):\n                        ref_logits = self.model.forward_hydra(\n                            input_ids=prompt_tensors,\n                            attention_mask=attention_mask,\n                            decoder_input_ids=sample_outputs,\n                            decoder_attention_mask=decoder_attention_mask,\n                            return_dict=True,\n                        ).logits\n                    else:\n                        ref_logits = self.ref_model(\n                            input_ids=prompt_tensors,\n                            attention_mask=attention_mask,\n                            decoder_input_ids=sample_outputs,\n                            decoder_attention_mask=decoder_attention_mask,\n                            return_dict=True,\n                        ).logits\n            else:\n                all_tokens = torch.cat((prompt_tensors.to(device), sample_outputs), dim=1)\n                attention_mask = all_tokens.not_equal(self.tokenizer.pad_token_id).long().to(device)\n                with torch.no_grad():\n                    logits, *_, values = self.model(\n                        all_tokens,\n                        attention_mask=attention_mask,\n                    )\n                    # TODO(dahoas): When hydra model works need to also support generation on hydra head\n                    # if hasattr(self.model, \"frozen_head\"):\n                    #     ref_logits = self.model.forward_hydra(\n                    #         all_tokens,\n                    #         attention_mask=attention_mask,\n                    #         return_dict=True,\n                    #     ).logits\n                    # else:\n                    ref_logits = self.ref_model(\n                        all_tokens,\n                        attention_mask,\n                    )\n                    ref_logits = ref_logits.to(device)\n\n            if self.config.model.model_arch_type == \"seq2seq\":\n                logprobs = logprobs_of_labels(logits[:, :-1, :], sample_outputs[:, 1:])\n                ref_logprobs = logprobs_of_labels(ref_logits[:, :-1, :], sample_outputs[:, 1:])\n            else:\n                logprobs = logprobs_of_labels(logits[:, :-1, :], all_tokens[:, 1:])\n                ref_logprobs = logprobs_of_labels(ref_logits[:, :-1, :], all_tokens[:, 1:])\n\n            n_samples: int = samples.shape[0]\n\n            # Estimate the KL divergence between the model and reference model\n            if self.config.model.model_arch_type == \"seq2seq\":\n                attention_mask = sample_outputs != self.tokenizer.pad_token_id\n                start = 0\n            else:\n                start = prompt_tensors.shape[1] - 1\n\n            log_ratio = (logprobs - ref_logprobs) * attention_mask[:, :-1]\n            self.mean_kl = (log_ratio.exp() - 1 - log_ratio).mean().to(device)\n\n            logprobs = logprobs.cpu()\n            ref_logprobs = ref_logprobs.cpu()\n            prompt_tensors = prompt_tensors.cpu()\n            sample_outputs = sample_outputs.cpu()\n            values = values.cpu()[:, :-1]\n\n            # Get the logprobs and values, for tokens that are not padding,\n            # from the start of the prompt up to the <eos> token, while also including the latter\n            # (these are taken from the student model and not the reference model)\n            ends = start + attention_mask[:, start:].sum(1) + 1\n            all_values = [values[ix, start : ends[ix]] for ix in range(n_samples)]\n            all_logprobs = [logprobs[ix, start : ends[ix]] for ix in range(n_samples)]\n\n            kl_penalty = self.kl_ctl.value * -log_ratio.cpu()\n            kl_penalty = [xs[start : ends[ix]] for ix, xs in enumerate(kl_penalty)]\n\n            rollout_count = 0\n\n            for sample_idx in range(n_samples):\n                rewards = kl_penalty[sample_idx]\n                rewards[-1] += scores[sample_idx].cpu()\n\n                ppo_rl_elements.append(\n                    PPORLElement(\n                        query_tensor=prompt_tensors[sample_idx],\n                        response_tensor=sample_outputs[sample_idx],\n                        logprobs=all_logprobs[sample_idx],\n                        values=all_values[sample_idx],\n                        rewards=rewards,\n                    )\n                )\n\n                rollout_count += 1\n            exp_time = clock.tick()\n            tbar.set_description(f\"[rollout {len(ppo_rl_elements)} / {num_rollouts}]\")\n            tbar.update(min(rollout_count, num_rollouts))\n        tbar.close()\n\n        if torch.distributed.is_initialized():\n            torch.distributed.all_reduce(self.mean_kl, torch.distributed.ReduceOp.AVG)\n\n        stats[\"policy/sqrt_kl\"] = torch.sqrt(self.mean_kl).item()\n        stats[\"kl_ctl_value\"] = self.kl_ctl.value\n        stats[\"time/exp\"] = exp_time\n\n        self.accelerator.log(stats, step=iter_count)\n\n        # Push samples and rewards to trainer's rollout storage\n        self.push_to_store(ppo_rl_elements)\n\n\ndef triton_server_ref_model():  # noqa:  C901\n    triton_host = os.environ.get(\"TRITON_HOST_REF\")\n    assert triton_host is not None, \"Specify reference model in the TRITON_HOST_REF environmental variable\"\n\n    triton_url, triton_model = triton_host.split(\"/\")\n    client = client_util.InferenceServerClient(url=triton_url, verbose=False)\n\n    def ref_model(all_tokens, attention_masks):\n        mbs = 8\n\n        all_tokens = all_tokens.detach().cpu().numpy()\n        attention_masks = attention_masks.detach().cpu().numpy()\n\n        out = []\n\n        for i in range(math.ceil(len(all_tokens) / mbs)):\n            batch_ixs = slice(i * mbs, (i + 1) * mbs)\n\n            # We specified int32 as types for a triton client\n            result = client.infer(\n                triton_model,\n                [\n                    prepare_tensor(\"input_ids\", all_tokens[batch_ixs].astype(np.int32)),\n                    prepare_tensor(\"attention_mask\", attention_masks[batch_ixs].astype(np.int32)),\n                ],\n            )\n\n            logits = result.as_numpy(\"logits\")\n\n            out.append(torch.tensor(logits))\n\n        return torch.cat(out, dim=0)\n\n    return ref_model\n\n\n@register_datapipeline\nclass CustomPromptPipeline(BasePipeline):\n    \"\"\"\n    Tokenizes prompts, unless they are already tokenized, and truncates them to `max_prompt_length` from the right\n    \"\"\"\n\n    def __init__(self, prompts: List[str], max_prompt_length: int, tokenizer: PreTrainedTokenizer):\n        super().__init__()\n\n        if max_prompt_length < 16:  # sanity check\n            raise ValueError(\n                f\"`max_prompt_length` is {max_prompt_length}, this is too small (less than 16). \"\n                \"Make sure all the config values are correct, when in doubt increase `seq_len` or decrease `max_new_tokens`.\"\n            )\n\n        model_inputs = tokenizer(\n            prompts,\n            truncation=True,\n            padding=True,\n            max_length=max_prompt_length,\n            add_special_tokens=False,\n        )\n\n        prompts_tokens_ = model_inputs[\"input_ids\"]\n        attention_mask = model_inputs[\"attention_mask\"]\n\n        # make sure that every prompt has an EOS token\n        for prompt_tokens in prompts_tokens_:\n            if tokenizer.eos_token_id not in prompt_tokens:\n                warnings.warn(\n                    \"Found a prompt without an EOS token, which means it was truncated. Consider increasing the context size (`seq_len`)\"\n                )\n                break\n\n        # prompts_tokens = []\n\n        # assistant_token_id = tokenizer.convert_tokens_to_ids(QA_SPECIAL_TOKENS[\"Answer\"])\n        # eos_token_id = tokenizer.eos_token_id\n\n        # print('input', prompts[0])\n        # print('ids', model_inputs[\"input_ids\"][0])\n        # print('masks', model_inputs[\"attention_mask\"])\n        # print('before', tokenizer.decode(prompts_tokens_[0]))\n\n        # If we truncate left this should not be a problem. Also for bpe this does not work...\n        # Due to truncation, special tokens may not be present ... so we add them (context is still incomplete)\n        # not need to update attention_mask since it iw always 1\n        # for prompt_tokens in prompts_tokens_:\n        #     prompts_tokens.append(prompt_tokens[:-2] + [eos_token_id, assistant_token_id])\n\n        prompts_tokens = prompts_tokens_\n\n        # print('after', tokenizer.decode(prompts_tokens[0]))\n\n        self.tokenizer = tokenizer\n        self.prompts = [\n            {\"input_ids\": tokens, \"attention_mask\": mask} for tokens, mask in zip(prompts_tokens, attention_mask)\n        ]\n\n    def __getitem__(self, ix: int):\n        return self.prompts[ix]\n\n    def __len__(self) -> int:\n        return len(self.prompts)\n\n    def create_loader(self, batch_size: int, shuffle=False) -> DataLoader:\n        collate_fn = DataCollatorWithPadding(self.tokenizer) if self.tokenizer else torch.vstack\n        return DataLoader(self, batch_size=batch_size, collate_fn=collate_fn, shuffle=shuffle)\n", "model/model_training/utils/utils_rl.py": "import tritonclient.grpc as client_util\nfrom tritonclient.utils import np_to_triton_dtype\n\n\ndef prepare_tensor(name: str, input):\n    t = client_util.InferInput(name, input.shape, np_to_triton_dtype(input.dtype))\n    t.set_data_from_numpy(input)\n    return t\n", "model/model_eval/utils.py": "import json\nimport os\n\nimport numpy as np\n\n\ndef load_sampling_data(path):\n    \"\"\"\n    Load sampling data and ensure appropriate keys are present.\n    \"\"\"\n\n    if os.path.exists(path):\n        data = json.load(open(path))\n    else:\n        raise FileNotFoundError(f\"Sampling data {path} not found\")\n\n    if \"prompts\" not in data.keys():\n        raise KeyError(\"sampling data should contain prompts key\")\n\n    keys = set(data[\"prompts\"][0].keys())\n    required_keys = set([\"prompt\", \"results\"])\n    keys = keys.intersection(required_keys)\n    if keys != required_keys:\n        raise KeyError(f\"Missing keys {required_keys - keys} \")\n\n    return data\n\n\ndef write_to_json(filename, data):\n    if not filename.endswith(\".json\"):\n        filename = f\"{filename}.json\"\n\n    with open(filename, \"w\") as file:\n        json.dump(data, file, indent=4)\n\n\ndef describe_samples(samples):\n    reward_scores = []\n    for item in samples:\n        reward_scores.extend([float(output[1]) for output in item[\"outputs\"]])\n\n    return {\n        \"mean\": np.mean(reward_scores).astype(str),\n        \"min\": np.min(reward_scores).astype(str),\n        \"max\": np.max(reward_scores).astype(str),\n    }\n", "model/model_eval/sampling_score.py": "import argparse\nimport json\n\nimport model_training.models.reward_model  # noqa: F401 (registers reward model for AutoModel loading)\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom eval_datasets import get_sampling_dataloader\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\nfrom utils import load_sampling_data\n\n\ndef batch_inference(model, dataloader):\n    \"\"\"\n    Batch inference\n    \"\"\"\n\n    scores, sampling = [], []\n    device = model.device\n    for i, data in enumerate(dataloader):\n        sampling.append(data.pop(\"sampling\").cpu().detach().numpy())\n        data = {k: v.squeeze().to(device) for k, v in data.items()}\n        pred = model(**data).logits[:, 0].cpu().detach().numpy()\n        scores.append(pred)\n\n    return np.hstack(sampling), np.hstack(scores)\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description=\"\")\n    parser.add_argument(\"--data_path\", type=str, help=\"Path of the sampling data file\")\n    parser.add_argument(\"--model\", type=str, help=\"Path or url of the model file\")\n    parser.add_argument(\"--max_length\", type=int, help=\"max length of input\")\n    parser.add_argument(\"--batch_size\", type=int, help=\"device\", default=4)\n    parser.add_argument(\"--device\", type=str, help=\"device\", default=\"cpu\")\n    parser.add_argument(\"--save\", type=bool, help=\"whether to save the results\", default=True)\n\n    args = parser.parse_args().__dict__\n    if args.get(\"device\") != \"cpu\":\n        device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n    else:\n        device = torch.device(\"cpu\")\n\n    data = load_sampling_data(args.get(\"data_path\"))\n\n    model_name = args.get(\"model\")\n\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    model = AutoModelForSequenceClassification.from_pretrained(model_name)\n    model.eval()\n    model.to(device)\n    max_length = args.get(\"max_length\") or model.config.max_position_embeddings\n    dataloader = get_sampling_dataloader(data, tokenizer, max_length, args.get(\"batch_size\"))\n    sampling, scores = batch_inference(model, dataloader)\n\n    df = pd.DataFrame({\"sampling\": sampling, \"score\": scores})\n    id2label = {v: k for k, v in dataloader.dataset.label2id.items()}\n    df[\"sampling\"] = df[\"sampling\"].map(id2label)\n    results = df.groupby(\"sampling\")[\"score\"].mean().to_dict()\n    results[\"mean_reward\"] = str(df[\"score\"].mean())\n    print(\"RESULTS: \", results)\n\n    results = {\"model_name\": data[\"model_name\"], \"results\": results, \"reward_model\": args.get(\"model\")}\n    name = \"-\".join(data[\"model_name\"].split(\"/\"))\n\n    if args.get(\"save\"):\n        with open(f\"{name}.json\", \"w\") as file:\n            json.dump(results, file, indent=4)\n", "model/model_eval/eval_rm.py": "import argparse\nfrom collections import defaultdict\n\nimport numpy as np\nimport torch\nfrom model_training.custom_datasets.rank_datasets import HellaSwagDataset, HFDataset, SHPDataset\nfrom model_training.custom_datasets.ranking_collator import RankingDataCollator\nfrom model_training.metrics import RewardMetrics\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\nfrom transformers.trainer_utils import EvalPrediction\nfrom utils import write_to_json\n\nDATASETS = [\"SHP\", \"Hellaswag\", \"HFdataset\"]\n\n\ndef get_ranking_dataset(dataset, split):\n    dataset = dataset.lower()\n    if dataset == \"shp\":\n        return SHPDataset(split=split)\n    elif dataset == \"hellaswag\":\n        return HellaSwagDataset(split=split)\n    elif dataset == \"hfdataset\":\n        return HFDataset(split=split)\n    else:\n        raise ValueError(f\"Invalid dataset name, available {DATASETS}\")\n\n\ndef batch_inference(inputs, model):\n    batch, cu_lens = inputs\n    batch = {k: v.to(model.device) for k, v in batch.items()}\n\n    with torch.no_grad():\n        logits = model(input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"]).logits.detach().cpu()\n\n    if logits.dtype == torch.bfloat16:\n        # As of Numpy 1.21.4, NumPy does not support bfloat16 (see\n        # https://github.com/numpy/numpy/blob/a47ecdea856986cd60eabbd53265c2ca5916ad5d/doc/source/user/basics.types.rst ).\n        # Until Numpy adds bfloat16, we must convert float32.\n        logits = logits.to(torch.float32)\n    logits = logits.numpy()\n\n    labels = []\n    for i, (s, e) in enumerate(zip(cu_lens[:-1], cu_lens[1:])):\n        labels.extend([i] * (e - s))\n    labels = np.array(labels).reshape(-1, 1)\n    return EvalPrediction(predictions=logits.T, label_ids=labels.T)\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description=\"\")\n    parser.add_argument(\"--dataset\", type=str, help=\"name of evaluation dataset\")\n    parser.add_argument(\"--split\", type=str, help=\"dataset splits separated by comma\", default=\"train\")\n    parser.add_argument(\"--model\", type=str, help=\"Path or url of the model file\")\n    parser.add_argument(\"--metrics\", type=str, help=\"metrics to evaluate\", default=\"accuracy\")\n    parser.add_argument(\"--batch_size\", type=int, help=\"Batch Size\", default=8)\n    parser.add_argument(\"--device\", type=str, help=\"device\", default=\"cuda\")\n    parser.add_argument(\"--dtype\", type=str, help=\"data type\", default=None)\n    args = parser.parse_args().__dict__\n\n    if args.get(\"device\") != \"cpu\":\n        device = torch.device(args.get(\"device\")) if torch.cuda.is_available() else torch.device(\"cpu\")\n    else:\n        device = torch.device(\"cpu\")\n\n    model_name = args.get(\"model\")\n\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    model = AutoModelForSequenceClassification.from_pretrained(\n        model_name, torch_dtype=\"auto\" if not args.dtype else args.dtype\n    )\n    model.eval()\n    model.to(device)\n    max_length = args.get(\"max_length\") or model.config.max_position_embeddings\n\n    splits = args.get(\"split\").split(\",\")\n    dataset = get_ranking_dataset(args.get(\"dataset\"), split=splits)\n    collate_fn = RankingDataCollator(tokenizer)\n    dataset = DataLoader(dataset, collate_fn=collate_fn, batch_size=args.get(\"batch_size\"))\n\n    metrics = args.get(\"metrics\").split(\",\")\n    compute_metrics = RewardMetrics(metrics)\n    score_dict = defaultdict(float)\n    for i, data in enumerate(tqdm(dataset)):\n        eval_pred = batch_inference(data, model)\n        results = compute_metrics(eval_pred)\n        for metric in metrics:\n            score_dict[metric] += results.get(metric)\n    score_dict = {k: str(round(v / len(dataset), 3)) for k, v in score_dict.items()}\n\n    results = {\n        \"model\": model_name,\n        \"dataset\": args.get(\"dataset\"),\n        \"split\": splits,\n    }\n    results.update(score_dict)\n\n    print(\"RESULTS\", results)\n    write_to_json(f\"rm-eval-{model_name.split('/')[-1]}-results.json\", results)\n", "model/model_eval/__init__.py": "", "model/model_eval/rejection_sampling.py": "import argparse\n\nimport model_training.models.reward_model  # noqa: F401 (registers reward model for AutoModel loading)\nimport numpy as np\nimport torch\nfrom eval_datasets import RejectionSamplingDataset, SamplingDataCollator\nfrom torch.utils.data import DataLoader\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\nfrom utils import describe_samples, load_sampling_data, write_to_json\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(description=\"\")\n    parser.add_argument(\"--data_path\", type=str, help=\"Path of the sampling data file\")\n    parser.add_argument(\"--model\", type=str, help=\"Path or url of the model file\")\n    parser.add_argument(\"--rs\", type=int, help=\"rejection sampling\", default=3)\n    parser.add_argument(\"--max_length\", type=int, help=\"max length of input\")\n    parser.add_argument(\"--device\", type=str, help=\"device\", default=\"cpu\")\n    args = parser.parse_args().__dict__\n\n    if args.get(\"device\") != \"cpu\":\n        device = torch.device(args.get(\"device\")) if torch.cuda.is_available() else torch.device(\"cpu\")\n    else:\n        device = torch.device(\"cpu\")\n\n    model_name = args.get(\"model\")\n\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    model = AutoModelForSequenceClassification.from_pretrained(model_name)\n    model.eval()\n    model.to(device)\n    max_length = args.get(\"max_length\") or model.config.max_position_embeddings\n\n    sr_report = load_sampling_data(args.get(\"data_path\"))\n    dataset = RejectionSamplingDataset(sr_report)\n    collate_fn = SamplingDataCollator(tokenizer, max_length=max_length)\n    dataloader = DataLoader(dataset, collate_fn=collate_fn, batch_size=1)\n\n    RS = args.get(\"rs\")\n    selected_list, rejected_list = [], []\n    for i, data in enumerate(dataloader):\n        index = data.pop(\"sampling\").detach().cpu().item()\n        data = {k: v.to(device) for k, v in data.items()}\n        pred = (\n            model(**data)\n            .logits[:, 0]\n            .cpu()\n            .detach()\n            .numpy()\n            .reshape(\n                -1,\n            )\n        )\n        sorted_indices = np.argsort(pred)\n        prompt, replies, _ = dataset[index]\n        selected_list.append(\n            {\n                \"prompt\": prompt,\n                \"outputs\": [(replies[idx], str(round(pred[idx], 2))) for idx in reversed(sorted_indices[-RS:])],\n            }\n        )\n\n        rejected_list.append(\n            {\"prompt\": prompt, \"outputs\": [(replies[idx], str(round(pred[idx], 2))) for idx in sorted_indices[:-RS]]}\n        )\n\n    selected_stats = describe_samples(selected_list)\n    rejected_stats = describe_samples(rejected_list)\n    stats = {\"rejected_samples\": rejected_stats, \"selected_samples\": selected_stats}\n    write_to_json(\"selected_samples\", selected_list)\n    write_to_json(\"rejected_samples\", rejected_list)\n    write_to_json(\"comparison\", stats)\n", "model/model_eval/manual/create_synth_import.py": "import argparse\nimport json\nimport random\nimport re\nimport sys\nfrom uuid import uuid4\n\nimport pydantic\nfrom oasst_data import ExportMessageNode, ExportMessageTree\nfrom sampling_report import SamplingReport\n\n\ndef filter_text(s: str) -> str:\n    m = re.search(\n        r\"\\</?prefix\\>|\\<human\\>|\\<\\|endoftext\\|\\>|\\<\\|prompter\\|\\>|\\<\\|assistant\\|\\>|\\<\\|system\\|\\>|<|prefix_(begin|end)\\|\\>\",\n        s,\n    )\n    if m:\n        s = s[: m.start()]\n    return s\n\n\ndef format_params(p: dict) -> str:\n    s = [f\"{k}={v}\" for k, v in p.items()]\n    return \",\".join(s)\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"input_files\", nargs=\"*\", type=argparse.FileType(\"r\", encoding=\"UTF-8\"))\n    parser.add_argument(\"--seed\", default=219837, type=int)\n    parser.add_argument(\"--num-replies\", default=5, type=int)\n    parser.add_argument(\"--output\", type=argparse.FileType(\"w\", encoding=\"UTF-8\"), default=sys.stdout)\n    parser.add_argument(\"--max-count\", type=int)\n    parser.add_argument(\"--lang\", type=str, default=\"en\")\n    return parser.parse_args()\n\n\ndef main():\n    args = parse_args()\n\n    input_reports: list[SamplingReport] = []\n    for f in args.input_files:\n        json_raw = json.load(f)\n        report = pydantic.parse_obj_as(SamplingReport, json_raw)\n        input_reports.append(report)\n\n    print(f\"Read {len(input_reports)} reports\")\n\n    # index by prompt\n    reply_by_prompt: dict[str, list[ExportMessageNode]] = {}\n    for r in input_reports:\n        for p in r.prompts:\n            for res in p.results:\n                for s in res.outputs:\n                    s = filter_text(s)\n                    model_name = f\"{r.model_name},{format_params(res.sampling_params)}\"\n                    m = ExportMessageNode(\n                        message_id=str(uuid4()),\n                        text=s,\n                        role=\"assistant\",\n                        synthetic=True,\n                        model_name=model_name,\n                        lang=args.lang,\n                    )\n\n                    l = reply_by_prompt.get(p.prompt)\n                    if l is not None:\n                        l.append(m)\n                    else:\n                        reply_by_prompt[p.prompt] = [m]\n\n    random.seed(args.seed)\n    trees: list[ExportMessageTree] = []\n    for k, v in reply_by_prompt.items():\n        # remove exact duplicates\n        reply_texts = set()\n        unique_replies = []\n        for m in v:\n            if m.text in reply_texts:\n                continue\n            unique_replies.append(m)\n            reply_texts.add(m.text)\n\n        if len(unique_replies) < 2:\n            print(\"Skipping enty with < 2 unique replies\")\n            continue\n\n        prompt_message = ExportMessageNode(\n            message_id=str(uuid4()), text=k, role=\"prompter\", synthetic=False, lang=args.lang\n        )\n        prompt_message.replies = random.sample(unique_replies, k=min(args.num_replies, len(unique_replies)))\n        t = ExportMessageTree(message_tree_id=prompt_message.message_id, tree_state=\"ranking\", prompt=prompt_message)\n        trees.append(t)\n        if args.max_count and len(trees) >= args.max_count:\n            break\n\n    with args.output as f:\n        for t in trees:\n            json.dump(t.dict(exclude_none=True), f)\n            f.write(\"\\n\")\n\n\nif __name__ == \"__main__\":\n    main()\n", "model/model_eval/manual/sampling_report.py": "import argparse\nimport gzip\nimport json\nimport random\nimport re\nfrom collections import OrderedDict\nfrom datetime import datetime\nfrom pathlib import Path\nfrom typing import Any, Optional\n\nimport pydantic\nimport torch\nfrom model_training.models.peft_modeling import load_peft_model\nfrom tqdm import tqdm\nfrom transformers import AutoTokenizer, PreTrainedTokenizer\n\nQA_SPECIAL_TOKENS = {\"Question\": \"<human>\", \"Answer\": \"<bot>\", \"StartPrefix\": \"<prefix>\", \"EndPrefix\": \"</prefix>\"}\nQA_SPECIAL_TOKENS_V2_5 = {\n    \"prompter\": \"<|prompter|>\",\n    \"assistant\": \"<|assistant|>\",\n    \"system\": \"<|system|>\",\n    \"prefix_begin\": \"<|prefix_begin|>\",\n    \"prefix_end\": \"<|prefix_end|>\",\n}\n\n\nclass SamplingConfig(pydantic.BaseModel):\n    name: Optional[str]\n    generate_args: dict[str, Any] = {}\n    system_profile: Optional[OrderedDict[str, float | int | str]] = None\n    pre_text: Optional[str]\n    add_prefix_tokens: Optional[bool] = False\n\n    # for legacy mode\n    human_name: Optional[str]\n    bot_name: Optional[str]\n\n\nclass Configuration(pydantic.BaseModel):\n    default: Optional[SamplingConfig]\n    configurations: list[SamplingConfig]\n\n\nclass SamplingResult(pydantic.BaseModel):\n    sampling_config: str\n    sampling_params: dict\n    outputs: list[str]\n\n\nclass PromptResults(pydantic.BaseModel):\n    prompt: str\n    results: list[SamplingResult]\n\n\nclass SamplingReport(pydantic.BaseModel):\n    model_name: str\n    date: str\n    args: dict\n    prompts: list[PromptResults]\n\n\ndef load_jsonl(input_file_path: str | Path) -> list[dict | str]:\n    if not isinstance(input_file_path, Path):\n        input_file_path = Path(input_file_path)\n\n    if input_file_path.suffix == \".gz\":\n        file_in = gzip.open(str(input_file_path), mode=\"tr\", encoding=\"UTF-8\")\n    else:\n        file_in = input_file_path.open(\"r\", encoding=\"UTF-8\")\n\n    items = []\n\n    with file_in:\n        # read one message tree per line\n        for line in file_in:\n            obj = json.loads(line, object_pairs_hook=OrderedDict)\n            items.append(obj)\n\n    return items\n\n\ndef sample(\n    prompt: str,\n    model,\n    tokenizer: PreTrainedTokenizer,\n    mode: str,\n    sampling_config: SamplingConfig,\n    device: torch.DeviceObjType,\n    skip_input_tokens: bool,\n    max_input_len: Optional[int] = None,\n):\n    assert sampling_config.name, \"'name' must be specified for sampling configuration\"\n    sc = sampling_config\n    prefix = \"\"\n    if sampling_config.pre_text:\n        if mode == \"v2\" and sampling_config.add_prefix_tokens:\n            prefix = f\"<prefix>{sampling_config.pre_text}</prefix>\"\n        if mode == \"v2_5\" and sampling_config.add_prefix_tokens:\n            prefix = f\"{QA_SPECIAL_TOKENS_V2_5['prefix_begin']}{sampling_config.pre_text}{QA_SPECIAL_TOKENS_V2_5['prefix_end']}\"\n        else:\n            prefix = sampling_config.pre_text\n\n    if mode == \"v2\":\n        input_text = f\"{prefix}{QA_SPECIAL_TOKENS['Question']}{prompt}{QA_SPECIAL_TOKENS['Answer']}\"\n    elif mode == \"v2_5\":\n        if sampling_config.system_profile and len(sampling_config.system_profile) > 0:\n            system_fragments = [QA_SPECIAL_TOKENS_V2_5[\"system\"]]\n            for k, v in sampling_config.system_profile.items():\n                if isinstance(v, float):\n                    system_fragments.append(f\"{k}: {v:0.1f}\")\n                elif isinstance(v, str):\n                    system_fragments.append(f\"{k}: {v}\")\n                else:\n                    system_fragments.append(f\"{k}: {v}\")\n            system_fragments.append(tokenizer.eos_token)\n            system_tag = \"\\n\".join(system_fragments)\n        else:\n            system_tag = \"\"\n\n        input_text = f\"{prefix}{QA_SPECIAL_TOKENS_V2_5['prompter']}{prompt}{tokenizer.eos_token}{system_tag}{QA_SPECIAL_TOKENS_V2_5['assistant']}\"\n        print(\"input_text\", input_text)\n    else:\n        assert sc.human_name and sc.bot_name, \"'human_name' and 'bot_name' parameters must be specified in config \"\n        input_text = f\"{prefix}\\n{sc.human_name}: {prompt}\\n\\n{sc.bot_name}: \"\n\n    sampling_params = sampling_config.generate_args\n    inputs = tokenizer(\n        input_text,\n        return_tensors=\"pt\",\n        max_length=max_input_len,\n        pad_to_max_length=False,\n        truncation=True,\n    ).to(device)\n    input_ids = inputs.input_ids\n    outputs = model.generate(\n        input_ids=input_ids,\n        pad_token_id=tokenizer.eos_token_id,\n        **sampling_params,\n    )\n    if skip_input_tokens:\n        output_tokens = outputs[0, input_ids.size(1) :]\n    else:\n        output_tokens = outputs[0]\n    return output_tokens, sampling_params\n\n\ndef merge_configs(*configs: tuple[Optional[SamplingConfig]]) -> Optional[SamplingConfig]:\n    merged: SamplingConfig | None = None\n    for c in configs:\n        if not merged:\n            if c:\n                merged = c.copy(deep=True)\n        else:\n            # simple fields\n            fields = [\"name\", \"pre_text\", \"human_name\", \"bot_name\", \"add_prefix_tokens\"]\n            for field_name in fields:\n                v = getattr(c, field_name)\n                if v:\n                    setattr(merged, field_name, v)\n            # generate args\n            if c.generate_args:\n                for k, v in c.generate_args.items():\n                    merged.generate_args[k] = v\n            # system profile\n            if c.system_profile:\n                if not merged.system_profile:\n                    merged.system_profile = {}\n                for k, v in c.system_profile.items():\n                    merged.system_profile[k] = v\n\n    return merged\n\n\ndef sample_prompt_continuations(\n    prompts: list[str],\n    model,\n    tokenizer: PreTrainedTokenizer,\n    mode: str,\n    config: Configuration,\n    device: torch.DeviceObjType,\n    num_samples: int = 1,\n    skip_special_tokens: bool = False,\n    skip_input_tokens: bool = False,\n    verbose: bool = False,\n    max_input_len: Optional[int] = None,\n) -> list[PromptResults]:\n    prompt_results: list[PromptResults] = []\n    for p in tqdm(prompts):\n        sampling_results: list[SamplingResult] = []\n        for sc in config.configurations:\n            outputs = []\n            for i in range(num_samples):\n                if i > 0 and sc.generate_args.get(\"do_sample\") is False:\n                    break  # don't repeat greedy sampling\n                output_tokens, sampling_params = sample(\n                    p,\n                    model=model,\n                    tokenizer=tokenizer,\n                    mode=mode,\n                    sampling_config=merge_configs(config.default, sc),\n                    device=device,\n                    skip_input_tokens=skip_input_tokens,\n                    max_input_len=max_input_len,\n                )\n                output = tokenizer.decode(\n                    output_tokens,\n                    truncate_before_pattern=[r\"\\n\\n^#\", \"^'''\", \"\\n\\n\\n\"],  # only used for codegen model\n                    skip_special_tokens=skip_special_tokens,\n                )\n\n                if verbose:\n                    print(f\"===[ Config: {sc.name} [{i+1}/{num_samples}] ]===\\n\")\n                    print(f'User: \"{p}\"')\n                    print(f'Assistant: \"{output}\"\\n')\n                outputs.append(output)\n\n            sampling_results.append(\n                SamplingResult(sampling_config=sc.name, sampling_params=sampling_params, outputs=outputs)\n            )\n\n        prompt_results.append(PromptResults(prompt=p, results=sampling_results))\n    return prompt_results\n\n\ndef load_configs(path: Path) -> Configuration:\n    with path.open() as f:\n        json_data = json.load(f)\n\n    return pydantic.parse_obj_as(Configuration, json_data)\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--device\", default=\"cuda\", type=str, help=\"device to use\")\n    parser.add_argument(\"--device-index\", default=0, type=int, help=\"device index\")\n    parser.add_argument(\"--model-name\", type=str, default=\"facebook/galactica-125m\")\n    parser.add_argument(\n        \"--mode\",\n        type=str,\n        default=\"legacy\",\n        help=\"legacy, v2\",\n    )\n    parser.add_argument(\n        \"--prompts\", type=str, help=\"jsonl string prompts input file name\", default=\"./data/en_100_text.jsonl.gz\"\n    )\n    parser.add_argument(\"--report\", type=str, help=\"json sampling report output file name\")\n    parser.add_argument(\"--seed\", type=int, default=\"42\", help=\"pseudo random number generator seed\")\n    parser.add_argument(\"--verbose\", action=\"store_true\", default=False)\n    parser.add_argument(\"-n\", type=int, help=\"number of prompts to use (default: all)\")\n    parser.add_argument(\"--num-samples\", type=int, default=2, help=\"number of sampling runs per configuration\")\n    parser.add_argument(\"--config\", type=str, default=\"config/default.json\", help=\"configuration file path\")\n    parser.add_argument(\"--half\", action=\"store_true\", default=False, help=\"use float16\")\n    parser.add_argument(\"--int8\", action=\"store_true\", default=False, help=\"use int8 quantization\")\n    parser.add_argument(\"--skip-special-tokens\", action=\"store_true\", default=False)\n    parser.add_argument(\"--model-type\", type=str, default=\"CausalLM\", help=\"CausalLM, T5Conditional, LLaMA\")\n    parser.add_argument(\"--max-input-len\", type=int, help=\"max token counts for input\")\n    parser.add_argument(\"--auth-token\", type=str)\n    parser.add_argument(\"--num-threads\", type=int, default=8)\n    parser.add_argument(\"--peft_model\", type=str, default=None)\n\n    return parser.parse_args()\n\n\ndef main():\n    \"\"\"\n    Usage example:\n    python sampling_report.py --model-name facebook/galactica-125m --config config/default.json --prompts data/en_100_text.jsonl --report report_file.json -n 10 --verbose\n\n    eval oasst model:\n    python sampling_report.py --model-name theblackcat102/pythia-3b-deduped-sft --mode v2 --config config/default.json --prompts data/en_100_text.jsonl -n 2 --verbose\n    \"\"\"\n\n    print(\"Using pytorch version {}\".format(torch.__version__))\n\n    args = parse_args()\n    if args.int8 and not torch.cuda.is_available():\n        print(\"Warning: --int8 argument passed but cuda is not available. Ignoring --int8.\")\n        args.int8 = False\n\n    print(\"Args:\", args)\n\n    torch.set_num_threads(args.num_threads)\n    torch.set_num_interop_threads(args.num_threads)\n\n    device = torch.device(args.device, args.device_index)\n    print(\"Device:\", device)\n\n    if args.seed:\n        random.seed(args.seed)\n        torch.manual_seed(args.seed)\n\n    # load configuration\n    config = load_configs(Path(args.config))\n\n    model_name = args.model_name\n    print(f\"Loading model: {model_name}\")\n\n    model_args = {}\n    if args.int8:\n        # these will break model.to(device) later in the script so a conditional check is needed\n        model_args[\"load_in_8bit\"] = args.int8\n        model_args[\"device_map\"] = \"auto\"\n\n    if args.model_type.lower() == \"causallm\" or args.model_type.lower() == \"llama\":\n        from transformers import AutoModelForCausalLM\n\n        tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=args.auth_token)\n        model = AutoModelForCausalLM.from_pretrained(model_name, use_auth_token=args.auth_token, **model_args)\n        skip_input_tokens = True\n    elif args.model_type.lower() == \"t5conditional\":\n        from transformers import T5ForConditionalGeneration\n\n        tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=args.auth_token)\n        model = T5ForConditionalGeneration.from_pretrained(model_name, use_auth_token=args.auth_token, **model_args)\n        skip_input_tokens = False\n    else:\n        raise RuntimeError(\"Invalid model_type specified\")\n\n    if args.peft_model is not None:\n        tokenizer = AutoTokenizer.from_pretrained(args.peft_model)\n        model = load_peft_model(model, args.peft_model, tokenizer)\n\n    print(\"special_tokens_map:\", tokenizer.special_tokens_map)\n    print(f\"eos_token='{tokenizer.eos_token}', eos_token_id={tokenizer.eos_token_id}\")\n\n    print(\"Tokenizer check:\")\n    input_text = f\"{QA_SPECIAL_TOKENS_V2_5['prompter']}Hi!{tokenizer.eos_token}{QA_SPECIAL_TOKENS_V2_5['assistant']}\"\n    tr = tokenizer(input_text)\n    print(tr)\n    decoded = tokenizer.decode(tr.input_ids, skip_special_tokens=False)\n    print(\"decoded:\", decoded)\n\n    model.eval()\n    if args.half:\n        model = model.half()\n\n    # int8 models (load_in_8bit = True + device_map = auto): will cause this method to error\n    if not args.int8:\n        model = model.to(device)\n\n    print(f\"Loading prompts file: {args.prompts}\")\n    prompts = load_jsonl(input_file_path=args.prompts)\n    print(f\"prompt count: {len(prompts)}\")\n\n    if args.n:\n        prompts = prompts[: args.n]\n\n    args_dict = vars(args)\n    if \"auth_token\" in args_dict:\n        del args_dict[\"auth_token\"]\n    report = SamplingReport(\n        model_name=model_name,\n        date=datetime.utcnow().isoformat(),\n        args=args_dict,\n        prompts=sample_prompt_continuations(\n            prompts=prompts,\n            model=model,\n            tokenizer=tokenizer,\n            mode=args.mode,\n            config=config,\n            device=device,\n            num_samples=args.num_samples,\n            skip_special_tokens=args.skip_special_tokens,\n            skip_input_tokens=skip_input_tokens,\n            verbose=args.verbose,\n            max_input_len=args.max_input_len,\n        ),\n    )\n\n    report_filename = args.report\n    if not report_filename:\n        save_model_name = re.sub(r\"[^\\w\\d-]\", \"_\", model_name)\n        config_name = Path(args.config).stem\n        date = report.date.split(\"T\")[0]\n        report_filename = f\"{date}_{save_model_name}_sampling_{config_name}.json\"\n    print(\"report_filename\", report_filename)\n\n    report_path = Path(report_filename)\n    print(f\"writing report: {str(report_path)}\")\n    with report_path.open(mode=\"wt\", encoding=\"UTF-8\") as rf:\n        x = report.dict(exclude_none=True)\n        json.dump(x, rf, indent=2)\n\n\nif __name__ == \"__main__\":\n    main()\n", "model/pretokenizer/create_hf_tokenizer_config.py": "import argparse\nfrom distutils.util import strtobool as strtoboolint\n\nimport transformers\nfrom tokenizer import build_tokenizer\nfrom transformers.utils import cached_file\n\n\ndef strtobool(s: str) -> bool:\n    return bool(strtoboolint(s))\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--tokenizer_type\", type=str, default=\"SentencePieceTokenizer\", help=\"SentencePieceTokenizer or FalconTokenizer\"\n    )\n    parser.add_argument(\n        \"--vocab_file\", type=str, help=\"[optional] vocab file for SentencePiece (get from HF cache by default)\"\n    )\n    parser.add_argument(\n        \"--tokenizer_name\",\n        type=str,\n        default=\"meta-llama/Llama-2-7b-hf\",\n        help=\"HuggingFace repo name or path, e.g. 'meta-llama/Llama-2-7b-hf' or 'tiiuae/falcon-40b'\",\n    )\n    parser.add_argument(\"--cache_dir\", type=str, default=None, help=\"Huggingface cache directory \")\n    parser.add_argument(\n        \"--vocab_extra_ids_list\",\n        type=str,\n        default=\"<|im_start|>,<|im_end|>\",\n        help='Comma separated list of additional tokens (e.g. \"<|im_start|>,<|im_end|>\")',\n    )\n    parser.add_argument(\"--output_dir\", type=str, default=\"output\", help=\"Path of output directory\")\n    return parser.parse_args()\n\n\ndef main():\n    \"\"\"\n    Usage examples:\n    python create_hf_tokenizer_config.py --tokenizer_type SentencePieceTokenizer --tokenizer_name meta-llama/Llama-2-7b-hf --output_dir output\n    python create_hf_tokenizer_config.py --tokenizer_type FalconTokenizer --tokenizer_name tiiuae/falcon-40b --output_dir output\n    \"\"\"\n    args = parse_args()\n    print(\"Configuration:\")\n    for k, v in vars(args).items():\n        print(f\"{k}: {v}\")\n\n    hf_tokenizer = transformers.AutoTokenizer.from_pretrained(args.tokenizer_name, cache_dir=args.cache_dir)\n\n    print(\"tokenizer.vocab_files_names\", hf_tokenizer.vocab_files_names)\n\n    if args.tokenizer_type == \"FalconTokenizer\":\n        args.vocab_file = \"\"\n    elif args.vocab_file is None:\n        args.vocab_file = cached_file(\n            args.tokenizer_name, hf_tokenizer.vocab_files_names[\"vocab_file\"], cache_dir=args.cache_dir\n        )\n\n    # add default args for megatron tokenizer\n    args.rank = 0\n    args.vocab_extra_ids = 0\n    args.new_tokens = True\n    args.make_vocab_size_divisible_by = 128\n    args.tensor_model_parallel_size = 1\n    mt_tokenizer = build_tokenizer(args)\n\n    if args.tokenizer_type == \"SentencePieceTokenizer\":\n        print(\"_special_tokens\", mt_tokenizer._special_tokens)\n        print(\"additional_special_tokens_ids\", mt_tokenizer.additional_special_tokens_ids)\n\n        hf_tokenizer.add_tokens(\"<CLS>\", special_tokens=True)\n        hf_tokenizer.add_tokens(\"<SEP>\", special_tokens=True)\n        hf_tokenizer.add_tokens(\"<EOD>\", special_tokens=True)\n        hf_tokenizer.add_tokens(\"<MASK>\", special_tokens=True)\n        hf_tokenizer.add_tokens(\"<PAD>\", special_tokens=True)\n        hf_tokenizer.cls_token_id = mt_tokenizer.cls\n        hf_tokenizer.sep_token_id = mt_tokenizer.sep\n        hf_tokenizer.mask_token_id = mt_tokenizer.mask\n        hf_tokenizer.pad_token_id = mt_tokenizer.pad\n\n        additional_special_tokens = hf_tokenizer.additional_special_tokens\n        special_tokens = {\"additional_special_tokens\": additional_special_tokens}\n        if args.vocab_extra_ids_list:\n            additional_special_tokens.extend(args.vocab_extra_ids_list.split(\",\"))\n\n        hf_tokenizer.add_special_tokens(special_tokens_dict=special_tokens, replace_additional_special_tokens=True)\n\n        additional_special_tokens_ids = [mt_tokenizer.vocab.get(t) for t in additional_special_tokens]\n        hf_tokenizer.additional_special_tokens_ids = additional_special_tokens_ids\n\n        tokens_to_check = [\n            v for k, v in hf_tokenizer.special_tokens_map.items() if k != \"additional_special_tokens\"\n        ] + additional_special_tokens\n        print(\"checking token ids:\")\n        for t in tokens_to_check:\n            a = mt_tokenizer.vocab.get(t)\n            b = hf_tokenizer.vocab.get(t)\n            print(f\"{t}: {a} (mt) == {b} (hf)\")\n            assert a == b, \"Mismatch between megatron and huggingface tokenizer vocabularies\"\n    elif args.tokenizer_type == \"FalconTokenizer\":\n        hf_tokenizer = mt_tokenizer.tokenizer\n    else:\n        raise RuntimeError(f\"Unsupported tokenizer type: {args.tokenizer_type}\")\n\n    print(\"special_tokens_map:\", hf_tokenizer.special_tokens_map)\n\n    hf_tokenizer.save_pretrained(args.output_dir)\n\n\nif __name__ == \"__main__\":\n    main()\n", "model/pretokenizer/tokenizer.py": "# copied from https://github.com/epfLLM/Megatron-LLM/blob/main/megatron/tokenizer/tokenizer.py\n# (only keeping _FalconTokenizer & _SentencePieceTokenizer)\n\n# Copyright (c) 2022, NVIDIA CORPORATION. All rights reserved.\n\n\"\"\"Megatron tokenizers.\"\"\"\n\nfrom abc import ABC, abstractmethod\n\n\ndef build_tokenizer(args):\n    \"\"\"Initialize tokenizer.\"\"\"\n    if args.rank == 0:\n        print(\"> building {} tokenizer ...\".format(args.tokenizer_type), flush=True)\n\n    if args.tokenizer_type not in {\"SentencePieceTokenizer\", \"FalconTokenizer\"}:\n        assert args.vocab_file is not None\n\n    # Select and instantiate the tokenizer.\n    if args.tokenizer_type == \"SentencePieceTokenizer\":\n        assert args.vocab_file is not None\n        tokenizer = _SentencePieceTokenizer(\n            args.vocab_file,\n            vocab_extra_ids=args.vocab_extra_ids,\n            vocab_extra_ids_list=args.vocab_extra_ids_list,\n            new_tokens=args.new_tokens,\n        )\n    elif args.tokenizer_type == \"FalconTokenizer\":\n        tokenizer = _FalconTokenizer(vocab_extra_ids_list=args.vocab_extra_ids_list, new_tokens=args.new_tokens)\n    else:\n        raise NotImplementedError(\"{} tokenizer is not \" \"implemented.\".format(args.tokenizer_type))\n\n    # Add vocab size.\n    args.padded_vocab_size = _vocab_size_with_padding(tokenizer.vocab_size, args)\n\n    return tokenizer\n\n\ndef _vocab_size_with_padding(orig_vocab_size, args):\n    \"\"\"Pad vocab size so it is divisible by model parallel size and\n    still having GPU friendly size.\"\"\"\n\n    after = orig_vocab_size\n    multiple = args.make_vocab_size_divisible_by * args.tensor_model_parallel_size\n    while (after % multiple) != 0:\n        after += 1\n    if args.rank == 0:\n        print(\n            \" > padded vocab (size: {}) with {} dummy tokens \"\n            \"(new size: {})\".format(orig_vocab_size, after - orig_vocab_size, after),\n            flush=True,\n        )\n    return after\n\n\nclass AbstractTokenizer(ABC):\n    \"\"\"Abstract class for tokenizer.\"\"\"\n\n    def __init__(self, name):\n        self.name = name\n        super().__init__()\n\n    @property\n    @abstractmethod\n    def vocab_size(self):\n        pass\n\n    @property\n    @abstractmethod\n    def vocab(self):\n        \"\"\"Dictionary from vocab text token to id token.\"\"\"\n        pass\n\n    @property\n    @abstractmethod\n    def inv_vocab(self):\n        \"\"\"Dictionary from vocab id token to text token.\"\"\"\n        pass\n\n    @abstractmethod\n    def tokenize(self, text):\n        pass\n\n    def detokenize(self, token_ids):\n        raise NotImplementedError(\"detokenizer is not implemented for {} \" \"tokenizer\".format(self.name))\n\n    @property\n    def cls(self):\n        raise NotImplementedError(\"CLS is not provided for {} \" \"tokenizer\".format(self.name))\n\n    @property\n    def sep(self):\n        raise NotImplementedError(\"SEP is not provided for {} \" \"tokenizer\".format(self.name))\n\n    @property\n    def pad(self):\n        raise NotImplementedError(\"PAD is not provided for {} \" \"tokenizer\".format(self.name))\n\n    @property\n    def eod(self):\n        raise NotImplementedError(\"EOD is not provided for {} \" \"tokenizer\".format(self.name))\n\n    @property\n    def mask(self):\n        raise NotImplementedError(\"MASK is not provided for {} \" \"tokenizer\".format(self.name))\n\n\nclass _FalconTokenizer(AbstractTokenizer):\n    \"\"\"Wrapper of huggingface tokenizer.\"\"\"\n\n    def __init__(self, vocab_extra_ids_list=None, new_tokens=True):\n        name = \"FalconTokenizer\"\n        super().__init__(name)\n        from transformers import AutoTokenizer\n\n        self.tokenizer = AutoTokenizer.from_pretrained(\"tiiuae/falcon-40b\")\n\n        if vocab_extra_ids_list and new_tokens:\n            special_tokens = self.tokenizer.additional_special_tokens + vocab_extra_ids_list.split(\",\")\n            self.tokenizer.add_special_tokens({\"additional_special_tokens\": special_tokens})\n            self._special_tokens = {tok: self.vocab[tok] for tok in special_tokens}\n        else:\n            self._special_tokens = {}\n\n        self._inv_vocab = {idx: token for token, idx in self.tokenizer.vocab.items()}\n\n    @property\n    def vocab_size(self):\n        return len(self.tokenizer.vocab)\n\n    @property\n    def vocab(self):\n        return self.tokenizer.vocab\n\n    def tokenize(self, text):\n        return self.tokenizer.encode(text)\n\n    def detokenize(self, token_ids):\n        return self.tokenizer.decode(token_ids)\n\n    @property\n    def inv_vocab(self):\n        return self._inv_vocab\n\n    @property\n    def eod(self):\n        return self.eos_token_id\n\n    @property\n    def pad(self):\n        return self.eos_token_id\n\n    @property\n    def eos_token_id(self):\n        return self.tokenizer.eos_token_id\n\n\nclass _SentencePieceTokenizer(AbstractTokenizer):\n    \"\"\"SentencePieceTokenizer-Megatron wrapper\"\"\"\n\n    def __init__(self, model_file, vocab_extra_ids=0, vocab_extra_ids_list=None, new_tokens=True):\n        name = \"SentencePieceTokenizer\"\n        super().__init__(name)\n\n        import sentencepiece\n\n        self._tokenizer = sentencepiece.SentencePieceProcessor(model_file=model_file)\n\n        self._initalize(vocab_extra_ids, vocab_extra_ids_list, new_tokens)\n\n    def _initalize(self, vocab_extra_ids, vocab_extra_ids_list, new_tokens):\n        self._vocab = {}\n        self._inv_vocab = {}\n\n        self._special_tokens = {}\n        self._inv_special_tokens = {}\n\n        self._t5_tokens = []\n\n        for i in range(len(self._tokenizer)):\n            t = self._tokenizer.id_to_piece(i)\n            self._inv_vocab[i] = t\n            self._vocab[t] = i\n\n        def _add_special_token(t):\n            if t not in self.vocab and not new_tokens:\n                return\n            if t not in self._vocab:\n                next_id = len(self._vocab)\n                self._vocab[t] = next_id\n                self._inv_vocab[next_id] = t\n            self._special_tokens[t] = self._vocab[t]\n            self._inv_special_tokens[self._vocab[t]] = t\n\n        _add_special_token(\"<CLS>\")\n        self._cls_id = self._vocab.get(\"<CLS>\")\n        _add_special_token(\"<SEP>\")\n        self._sep_id = self._vocab.get(\"<SEP>\")\n        _add_special_token(\"<EOD>\")\n        self._eod_id = self._vocab.get(\"<EOD>\")\n        _add_special_token(\"<MASK>\")\n        self._mask_id = self._vocab.get(\"<MASK>\")\n\n        pad_id = self._tokenizer.pad_id()\n        try:\n            pad_token = self._tokenizer.id_to_piece(pad_id)\n        except IndexError:\n            pad_token = \"<PAD>\"\n        _add_special_token(pad_token)\n        self._pad_id = self._vocab.get(pad_token)\n\n        bos_id = self._tokenizer.bos_id()\n        try:\n            bos_token = self._tokenizer.id_to_piece(bos_id)\n        except IndexError:\n            bos_token = \"<BOS>\"\n        _add_special_token(bos_token)\n        self._bos_id = self._vocab.get(bos_token)\n\n        eos_id = self._tokenizer.eos_id()\n        try:\n            eos_token = self._tokenizer.id_to_piece(eos_id)\n        except IndexError:\n            eos_token = \"<EOS>\"\n        _add_special_token(eos_token)\n        self._eos_id = self._vocab.get(eos_token)\n\n        for i in range(vocab_extra_ids):\n            t = \"<extra_id_{}>\".format(i)\n            _add_special_token(t)\n            self._t5_tokens += [t]\n        if vocab_extra_ids_list:\n            for t in vocab_extra_ids_list.split(\",\"):\n                _add_special_token(t)\n        print(\"Special tokens: {}\".format(self._special_tokens))\n\n    @property\n    def vocab_size(self):\n        return len(self._vocab)\n\n    @property\n    def vocab(self):\n        return self._vocab\n\n    @property\n    def inv_vocab(self):\n        return self._inv_vocab\n\n    # From:\n    # https://github.com/NVIDIA/NeMo/blob/c8fa217e811d60d11d014827c7f3845ff6c99ae7/nemo/collections/common/tokenizers/sentencepiece_tokenizer.py#L89\n    def tokenize(self, text):\n        ids = []\n        idx = 0\n\n        while 1:\n            indices = {}\n            for token in self._special_tokens:\n                try:\n                    indices[token] = text[idx:].index(token)\n                except ValueError:\n                    continue\n            if len(indices) == 0:\n                break\n\n            next_token = min(indices, key=indices.get)\n            next_idx = idx + indices[next_token]\n\n            ids.extend(self._tokenizer.encode_as_ids(text[idx:next_idx]))\n            ids.append(self._special_tokens[next_token])\n            idx = next_idx + len(next_token)\n\n        ids.extend(self._tokenizer.encode_as_ids(text[idx:]))\n        return ids\n\n    # From:\n    # https://github.com/NVIDIA/NeMo/blob/c8fa217e811d60d11d014827c7f3845ff6c99ae7/nemo/collections/common/tokenizers/sentencepiece_tokenizer.py#L125\n    def detokenize(self, ids):\n        text = \"\"\n        last_i = 0\n\n        for i, id in enumerate(ids):\n            if id in self._inv_special_tokens:\n                text += self._tokenizer.decode_ids(ids[last_i:i]) + \" \"\n                text += self._inv_special_tokens[id] + \" \"\n                last_i = i + 1\n        text += self._tokenizer.decode_ids(ids[last_i:])\n        return text.strip()\n\n    @property\n    def cls(self):\n        return self._cls_id\n\n    @property\n    def sep(self):\n        return self._sep_id\n\n    @property\n    def pad(self):\n        return self._pad_id\n\n    @property\n    def bos_token_id(self):\n        return self._bos_id\n\n    @property\n    def bos(self):\n        return self._bos_id\n\n    @property\n    def eod(self):\n        if self._eod_id is not None:\n            return self._eod_id\n        return self._eos_id  # in case noe eod we can patch this up with an eos\n\n    @property\n    def eos_token_id(self):\n        if self._eod_id is not None:\n            return self._eod_id\n        return self._eos_id\n\n    @property\n    def eos(self):\n        return self._eos_id\n\n    @property\n    def mask(self):\n        return self._mask_id\n\n    @property\n    def additional_special_tokens_ids(self):\n        return [self.vocab[k] for k in self._t5_tokens]\n", "model/pretokenizer/pretokenize.py": "import argparse\nimport json\nimport random\nfrom enum import IntEnum\nfrom pathlib import Path\nfrom subprocess import run\n\nimport indexed_dataset\nimport numpy as np\nimport torch\nfrom model_training.custom_datasets.formatting import DatasetEntryLm, DatasetEntrySft, Role\nfrom model_training.utils.utils import _strtobool, get_dataset, get_dataset_fractions, read_yamls\nfrom tokenizer import build_tokenizer\nfrom torch.utils.data import ConcatDataset, Dataset, Subset\nfrom tqdm import tqdm\n\n\nclass IntRole(IntEnum):\n    System = 0\n    Prompter = 1\n    Assistant = 2\n    Context = 3\n\n\nclass Encoder(object):\n    def __init__(self, args):\n        self.args = args\n        self.tokenizer = build_tokenizer(self.args)\n\n    def encode_text(self, text: str) -> list[int]:\n        return self.tokenizer.tokenize(text)\n\n    def decode(self, tokens: list[int]) -> str:\n        return self.tokenizer.detokenize(tokens)\n\n    @property\n    def special_tokens(self) -> dict:\n        return self.tokenizer._special_tokens\n\n\nclass DatasetWriter:\n    def __init__(\n        self,\n        filename_prefix: str,\n        vocab_size: int,\n        dataset_impl: str = \"mmap\",\n        feature: str = \"text\",\n    ):\n        self.bin_filename = f\"{filename_prefix}-{feature}.bin\"\n        self.idx_filename = f\"{filename_prefix}-{feature}.idx\"\n        self.builder = indexed_dataset.make_builder(self.bin_filename, impl=dataset_impl, vocab_size=vocab_size)\n\n    def add_item(self, tokenized_item):\n        self.builder.add_item(torch.IntTensor(tokenized_item))\n\n    def finalize(self):\n        self.builder.finalize(self.idx_filename)\n\n\ndef format_pairs(pairs: list[str] | tuple[str]) -> tuple[list[str], list[int]]:\n    assert isinstance(pairs, list) or isinstance(pairs, tuple)\n    role_names = (\"user\", \"assistant\")\n    role_ids = (1, 2)\n    return [f\"<|im_start|>{role_names[i%2]}\\n{pairs[i]}<|im_end|>\\n\" for i in range(len(pairs))], [\n        role_ids[i % 2] for i in range(len(pairs))\n    ]\n\n\ndef format_sft_entry(entry: DatasetEntrySft) -> tuple[list[str], list[int]]:\n    turns = []\n    roles = []\n    if entry.system_message and len(entry.system_message) > 0:\n        turns.append(f\"<|im_start|>system\\n{entry.system_message}<|im_end|>\\n\")\n        roles.append(IntRole.System.value)  # 0\n    for m in entry.conversation:\n        if m.context:\n            turns.append(f\"<|im_start|>context\\n{m.context}<|im_end|>\\n\")\n            roles.append(IntRole.Context.value)  # 3\n        if m.role == Role.prompter:\n            turns.append(f\"<|im_start|>user\\n{m.text}<|im_end|>\\n\")\n            roles.append(IntRole.Prompter.value)  # 1\n        elif m.role == Role.assistant:\n            turns.append(f\"<|im_start|>assistant\\n{m.text}<|im_end|>\\n\")\n            roles.append(IntRole.Assistant.value)  # 2\n    return turns, roles\n\n\ndef format_conversation(messages) -> str:\n    if isinstance(messages, DatasetEntrySft):\n        return format_sft_entry(messages)\n    elif isinstance(messages, DatasetEntryLm):\n        return messages.text, [3]\n    else:\n        return format_pairs(messages)\n\n\ndef get_dataset_name(d: Dataset):\n    if isinstance(d, Subset):\n        inner = d\n        while isinstance(inner, Subset):\n            inner = inner.dataset\n        name = f\"Subset of {type(inner).__name__}\"\n        if hasattr(inner, \"name\"):\n            name += f\" ({inner.name})\"\n    else:\n        name = type(d).__name__\n        if hasattr(d, \"name\"):\n            name += f\" ({d.name})\"\n    return name\n\n\nclass TokenStats:\n    def __init__(self, name: str, total_samples: int, fraction: float = 1):\n        self.name = name\n        self.skipped_samples = 0\n        self.skipped_tokens = 0\n        self.total_samples = total_samples\n        self.min_tokens = None\n        self.max_tokens = 0\n        self.accepted_samples = 0\n        self.accepted_tokens = 0\n        self.fraction = fraction\n\n    @property\n    def processed_samples(self) -> int:\n        return self.accepted_samples + self.skipped_samples\n\n    def skip(self, tokens: list[int]) -> None:\n        self.skipped_samples += 1\n        self.skipped_tokens = len(tokens)\n\n    def add(self, tokens: list[int]) -> None:\n        l = len(tokens)\n        self.accepted_samples += 1\n        self.accepted_tokens += l\n        if self.min_tokens is None or self.min_tokens > l:\n            self.min_tokens = l\n        if self.max_tokens < l:\n            self.max_tokens = l\n\n\ndef tokenize_dataset(\n    output_dir: Path,\n    filename_prefix: str,\n    dataset: Dataset,\n    encoder: Encoder,\n    dataset_impl: str,\n    datasets_config: dict,\n    max_count: int | None = None,\n    min_assistant_tokens: int | None = None,\n    check_tokenization: bool = True,\n    write_json: bool = False,\n    seed: int = 42,\n):\n    full_prefix = str(output_dir / filename_prefix)\n\n    token_writer = None\n    role_writer = None\n    jsonl_file = None\n\n    per_dataset_stats: list[TokenStats] = []\n    cumulative_sizes: list[int] = []\n\n    rng = np.random.RandomState(seed=seed)\n\n    if isinstance(dataset, ConcatDataset):\n        datasets = list(dataset.datasets)\n\n        if datasets_config:\n            dataset_sizes = [len(x) for x in datasets]\n            fractions = get_dataset_fractions(datasets_config, dataset_sizes, False)\n            dataset_target_sizes = [int(size * frac) for size, frac in zip(dataset_sizes, fractions)]\n        else:\n            dataset_target_sizes = None\n\n        for i in range(len(datasets)):\n            d = datasets[i]\n            name = get_dataset_name(d)\n            frac = 1\n            if dataset_target_sizes:\n                frac = fractions[i]\n                if dataset_target_sizes[i] < len(d):\n                    # sample subset of dataset\n                    subset_indices = rng.choice(len(d), size=dataset_target_sizes[i], replace=False)\n                    d = Subset(d, subset_indices)\n                    datasets[i] = d\n\n            per_dataset_stats.append(TokenStats(name, len(d), frac))\n\n        dataset = ConcatDataset(datasets)\n        cumulative_sizes = dataset.cumulative_sizes\n    else:\n        cumulative_sizes = [len(dataset)]\n\n    total_stats = TokenStats(\"total\", len(dataset))\n\n    try:\n        token_writer = DatasetWriter(\n            filename_prefix=full_prefix,\n            dataset_impl=dataset_impl,\n            vocab_size=encoder.tokenizer.vocab_size,\n            feature=\"text\",\n        )\n\n        role_writer = DatasetWriter(\n            filename_prefix=full_prefix,\n            dataset_impl=dataset_impl,\n            vocab_size=16,\n            feature=\"role\",\n        )\n\n        jsonl_path = Path(full_prefix + \".jsonl\")\n        if write_json:\n            jsonl_file = jsonl_path.open(\"w\", encoding=\"UTF-8\")\n\n        subset_index = 0\n        for i, messages in enumerate(tqdm(dataset)):\n            if i >= cumulative_sizes[subset_index]:\n                subset_index += 1\n\n            if i > 0 and i % 10000 == 0:\n                print(\n                    f\"Accepted: {total_stats.accepted_samples}/{total_stats.processed_samples} ({total_stats.accepted_samples/total_stats.processed_samples:.1%})\"\n                )\n\n            turns, turn_roles = format_conversation(messages)\n\n            tokens = []\n            role_lables = []\n            num_assistant_tokens = 0\n            for t, r in zip(turns, turn_roles):\n                turn_tokens = encoder.encode_text(t)\n                turn_role = [r] * len(turn_tokens)\n                tokens.extend(turn_tokens)\n                if r == IntRole.Assistant:\n                    num_assistant_tokens += len(turn_tokens)\n                role_lables.extend(turn_role)\n\n            if min_assistant_tokens is not None and num_assistant_tokens < min_assistant_tokens:\n                total_stats.skip(tokens)\n                per_dataset_stats[subset_index].skip(tokens)\n                continue\n\n            if check_tokenization:\n                x = encoder.encode_text(\"\".join(turns))\n                assert x == tokens and len(tokens) == len(role_lables)\n\n            token_writer.add_item(tokens)\n            role_writer.add_item(role_lables)\n\n            # update stats\n            total_stats.add(tokens)\n            per_dataset_stats[subset_index].add(tokens)\n\n            if jsonl_file:\n                json.dump({\"text\": \"\".join(turns)}, jsonl_file)\n                jsonl_file.write(\"\\n\")\n\n            if max_count and total_stats.accepted_samples >= max_count:\n                break\n    finally:\n        if token_writer:\n            token_writer.finalize()\n        if role_writer:\n            role_writer.finalize()\n        if jsonl_file:\n            jsonl_file.close()\n\n    per_dataset_stats.append(total_stats)\n\n    stats_path = Path(full_prefix + \"_stats.txt\")\n    with stats_path.open(\"w\", encoding=\"UTF-8\") as stats_file:\n        for f in (None, stats_file):\n            print(f\"\\n# Stats for {full_prefix}*\\n\", file=f)\n\n            for stats in per_dataset_stats:\n                print(f\"## Stats for '{stats.name}' ({stats.total_samples} samples ({stats.fraction:.1%}))\", file=f)\n                print(\"-----------------\", file=f)\n                print(\n                    f\"  Accepted: {stats.accepted_samples}/{stats.processed_samples} ({stats.accepted_samples/stats.processed_samples:.1%})\",\n                    file=f,\n                )\n                print(f\"  Accepted tokens: {stats.accepted_tokens}\", file=f)\n                print(\n                    f\"  Skipped: {stats.skipped_samples} ({stats.skipped_samples/stats.processed_samples:.1%})\", file=f\n                )\n                print(f\"  Min tokens per sample: {stats.min_tokens}\", file=f)\n                print(f\"  Max tokens per sample: {stats.max_tokens}\", file=f)\n                print(f\"  Avg tokens per sample: {stats.accepted_tokens/stats.accepted_samples}\", file=f)\n                print(\"-----------------\\n\", file=f)\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(\n        prog=\"pretokenize.py\", description=\"Tokenize datamixes for LLama2/Falcon fine-tuning with Megatron-LLM.\"\n    )\n    group = parser.add_argument_group(title=\"configuration\")\n    group.add_argument(\n        \"--configs\",\n        nargs=\"+\",\n        required=True,\n        help=\"Configurations sections to apply (read from YAML, multiple can be specified).\",\n    )\n    group.add_argument(\n        \"--output_dir\",\n        type=str,\n        help=\"Path to output directory\",\n    )\n    group.add_argument(\n        \"--write_json\",\n        action=\"store_true\",\n        help=\"Generate a JSONL file with the formatted dialogues (key='text').\",\n    )\n    group.add_argument(\n        \"--compress\",\n        action=\"store_true\",\n        help=\"Generate a .tar.gz file of the output directory.\",\n    )\n\n    args, remaining = parser.parse_known_args()\n\n    # load yaml configurations\n    conf = {}\n    configs = read_yamls(\"./configs\")\n    conf.update(configs[\"defaults\"])\n    try:\n        for name in args.configs:\n            if \",\" in name:\n                for n in name.split(\",\"):\n                    conf.update(configs[n])\n            else:\n                conf.update(configs[name])\n    except KeyError as e:\n        print(f'Error: Section \"{e.args[0]}\" not found in YAML configuration files.')\n        exit(1)\n\n    # override yaml args\n    for k, v in vars(args).items():\n        if k == \"configs\" or v is None:\n            continue\n        conf[k] = v\n\n    parser = argparse.ArgumentParser()\n    for key, value in conf.items():\n        type_ = type(value) if value is not None else str\n        if type_ == bool:\n            type_ = _strtobool\n        parser.add_argument(f\"--{key}\", type=type_, default=value)\n        # Allow --no-{key}  to remove a configuration value\n        parser.add_argument(f\"--no-{key}\", dest=key, action=\"store_const\", const=None)\n    parser.add_argument(\n        \"--max_count\",\n        type=int,\n        help=\"Limit number of train/eval examples to process (debug)\",\n    )\n\n    args = parser.parse_args(remaining)\n    args.keep_empty = False\n    args.rank = 0\n    args.vocab_extra_ids = 0\n    args.make_vocab_size_divisible_by = 128\n    args.tensor_model_parallel_size = 1\n    args.new_tokens = True\n\n    return args\n\n\ndef main():\n    \"\"\"\n    Example usage: `python __main__.py --output_dir output--configs oasst_top1 llama2`\n    \"\"\"\n    args = parse_args()\n    print(\"Configuration:\")\n    for k, v in vars(args).items():\n        print(f\"{k}: {v}\")\n\n    # initialize random states for reproducibility\n    random.seed(args.rng_seed)\n    np.random.seed(args.rng_seed)\n    torch.manual_seed(args.rng_seed)\n\n    print(\"Building encoder\")\n    encoder = Encoder(args)\n\n    tokenizer_check_input = \"<|im_start|>system\\nsystem message<|im_end|>\\n<|im_start|>user\\nprompt<|im_end|><|im_start|>assistant\\nreply<|im_end|>\\n\"\n    tokenizer_check_output = encoder.encode_text(tokenizer_check_input)\n    print(\"Tokenizer check:\")\n    print(\"Input:\", tokenizer_check_input.replace(\"\\n\", r\"\\n\"))\n    print(\"Output:\", tokenizer_check_output)\n    print(f\"Vocab size: {encoder.tokenizer.vocab_size}\")\n\n    output_dir = Path(args.output_dir + args.output_dir_suffix)\n    print(f\"Output dir: {output_dir} (exists: {output_dir.exists()})\")\n\n    train, evals = get_dataset(args)\n\n    # show dataset stats\n    print(\"Training dataset sizes (before sampling):\")\n    total = len(train)\n    for d in train.datasets:\n        name = get_dataset_name(d)\n        print(f\"{name}: {len(d)} ({len(d) / total:.2%})\")\n\n    output_dir.mkdir(parents=True, exist_ok=True)\n\n    fn = output_dir / \"special_tokens.json\"\n    with fn.open(\"w\", encoding=\"UTF-8\") as f:\n        json.dump(encoder.special_tokens, f)\n\n    val = ConcatDataset(evals.values())\n    for split_name, ds in zip([\"train\", \"val\"], [train, val]):\n        datasets_config = args.datasets if split_name == \"train\" else None\n        tokenize_dataset(\n            output_dir=output_dir,\n            filename_prefix=f\"{args.filename_prefix}-{split_name}\",\n            dataset=ds,\n            encoder=encoder,\n            dataset_impl=args.dataset_impl,\n            datasets_config=datasets_config,\n            max_count=args.max_count,\n            min_assistant_tokens=args.min_assistant_tokens,\n            write_json=args.write_json,\n            seed=args.rng_seed,\n        )\n\n    if args.compress:\n        run(f\"tar -czvf {output_dir}.tar.gz {output_dir}\", shell=True, check=True)\n\n\nif __name__ == \"__main__\":\n    main()\n", "text-frontend/auto_main.py": "\"\"\"Simple REPL frontend.\"\"\"\n\nimport http\nimport random\nfrom uuid import uuid4\n\nimport requests\nimport typer\nfrom faker import Faker\n\napp = typer.Typer()\nfake = Faker()\n\n\ndef _random_message_id():\n    return str(uuid4())\n\n\ndef _render_message(message: dict) -> str:\n    \"\"\"Render a message to the user.\"\"\"\n    if message[\"is_assistant\"]:\n        return f\"Assistant: {message['text']}\"\n    return f\"Prompter: {message['text']}\"\n\n\n@app.command()\ndef main(\n    backend_url: str = \"http://127.0.0.1:8080\", api_key: str = \"1234\", random_users: int = 1, tasks_per_user: int = 10\n):\n    \"\"\"automates tasks\"\"\"\n\n    def _post(path: str, json: dict) -> dict:\n        response = requests.post(f\"{backend_url}{path}\", json=json, headers={\"X-API-Key\": api_key})\n        response.raise_for_status()\n        if response.status_code == http.HTTPStatus.NO_CONTENT:\n            return None\n        return response.json()\n\n    def gen_random_text():\n        return \" \".join([random.choice([\"hello\", \"world\", \"foo\", \"bar\"]) for _ in range(10)])\n\n    def gen_random_ranking(messages):\n        \"\"\"rank messages randomly and return list of indexes in order of rank randomly\"\"\"\n        print(\"Ranking\")\n        print(messages)\n        print(len(messages))\n        ranks = [i for i in range(len(messages))]\n        shuffled = random.shuffle(ranks)\n        print(ranks)\n        print(shuffled)\n        return ranks\n\n    for i in range(int(random_users)):\n        name = fake.name()\n        USER = {\"id\": name, \"display_name\": name, \"auth_method\": \"local\"}\n\n        create_user_request = dict(USER)\n        # make sure dummy user has accepted the terms of service\n        create_user_request[\"tos_acceptance\"] = True\n        response = requests.post(\n            f\"{backend_url}/api/v1/frontend_users/\", json=create_user_request, headers={\"X-API-Key\": api_key}\n        )\n        response.raise_for_status()\n        user = response.json()\n        typer.echo(f\"user: {user}\")\n        q = 0\n\n        tasks = [_post(\"/api/v1/tasks/\", {\"type\": \"random\", \"user\": USER})]\n\n        while tasks:\n            task = tasks.pop(0)\n            print(task)\n\n            match (task[\"type\"]):\n                case \"initial_prompt\":\n                    typer.echo(\"Please provide an initial prompt to the assistant.\")\n                    if task[\"hint\"]:\n                        typer.echo(f\"Hint: {task['hint']}\")\n                    # acknowledge task\n                    message_id = _random_message_id()\n                    _post(f\"/api/v1/tasks/{task['id']}/ack\", {\"message_id\": message_id})\n\n                    prompt = gen_random_text()\n                    user_message_id = _random_message_id()\n                    # send interaction\n                    new_task = _post(\n                        \"/api/v1/tasks/interaction\",\n                        {\n                            \"type\": \"text_reply_to_message\",\n                            \"message_id\": message_id,\n                            \"task_id\": task[\"id\"],\n                            \"user_message_id\": user_message_id,\n                            \"text\": prompt,\n                            \"user\": USER,\n                        },\n                    )\n                    tasks.append(new_task)\n\n                case \"label_initial_prompt\":\n                    typer.echo(\"Label the following prompt:\")\n                    typer.echo(task[\"prompt\"])\n                    # acknowledge task\n                    message_id = _random_message_id()\n                    _post(f\"/api/v1/tasks/{task['id']}/ack\", {\"message_id\": message_id})\n\n                    valid_labels = task[\"valid_labels\"]\n                    mandatory_labels = task[\"mandatory_labels\"]\n\n                    labels_dict = None\n                    if task[\"mode\"] == \"simple\" and len(valid_labels) == 1:\n                        answer = random.choice([True, False])\n                        labels_dict = {valid_labels[0]: 1 if answer else 0}\n                    else:\n                        labels = random.sample(valid_labels, random.randint(1, len(valid_labels)))\n                        for l in mandatory_labels:\n                            if l not in labels:\n                                labels.append(l)\n                        labels_dict = {label: random.random() for label in valid_labels}\n                    if random.random() < 0.9:\n                        labels_dict[\"spam\"] = 0\n                        labels_dict[\"lang_mismatch\"] = 0\n\n                    # send labels\n                    new_task = _post(\n                        \"/api/v1/tasks/interaction\",\n                        {\n                            \"type\": \"text_labels\",\n                            \"message_id\": task[\"message_id\"],\n                            \"task_id\": task[\"id\"],\n                            \"text\": task[\"prompt\"],\n                            \"labels\": labels_dict,\n                            \"user\": USER,\n                        },\n                    )\n                    tasks.append(new_task)\n                case \"prompter_reply\":\n                    # acknowledge task\n                    message_id = _random_message_id()\n                    user_message_id = _random_message_id()\n                    _post(f\"/api/v1/tasks/{task['id']}/ack\", {\"message_id\": message_id})\n                    # send interaction\n                    new_task = _post(\n                        \"/api/v1/tasks/interaction\",\n                        {\n                            \"type\": \"text_reply_to_message\",\n                            \"message_id\": message_id,\n                            \"task_id\": task[\"id\"],\n                            \"user_message_id\": user_message_id,\n                            \"text\": gen_random_text(),\n                            \"user\": USER,\n                        },\n                    )\n                    tasks.append(new_task)\n\n                case \"assistant_reply\":\n                    # acknowledge task\n                    message_id = _random_message_id()\n                    user_message_id = _random_message_id()\n                    _post(f\"/api/v1/tasks/{task['id']}/ack\", {\"message_id\": message_id})\n                    # send interaction\n                    new_task = _post(\n                        \"/api/v1/tasks/interaction\",\n                        {\n                            \"type\": \"text_reply_to_message\",\n                            \"message_id\": message_id,\n                            \"task_id\": task[\"id\"],\n                            \"user_message_id\": user_message_id,\n                            \"text\": gen_random_text(),\n                            \"user\": USER,\n                        },\n                    )\n                    tasks.append(new_task)\n\n                case \"rank_prompter_replies\" | \"rank_assistant_replies\":\n                    # acknowledge task\n                    message_id = _random_message_id()\n                    user_message_id = _random_message_id()\n                    _post(f\"/api/v1/tasks/{task['id']}/ack\", {\"message_id\": message_id})\n                    # send interaction\n                    ranking = gen_random_ranking(task[\"replies\"])\n                    print(ranking)\n                    new_task = _post(\n                        \"/api/v1/tasks/interaction\",\n                        {\n                            \"type\": \"message_ranking\",\n                            \"message_id\": message_id,\n                            \"task_id\": task[\"id\"],\n                            \"ranking\": ranking,\n                            \"user\": USER,\n                        },\n                    )\n                    tasks.append(new_task)\n\n                case \"rank_initial_prompts\":\n                    # acknowledge task\n                    message_id = _random_message_id()\n                    user_message_id = _random_message_id()\n                    _post(f\"/api/v1/tasks/{task['id']}/ack\", {\"message_id\": message_id})\n                    # send interaction\n                    ranking = gen_random_ranking(task[\"prompots\"])\n                    new_task = _post(\n                        \"/api/v1/tasks/interaction\",\n                        {\n                            \"type\": \"message_ranking\",\n                            \"message_id\": message_id,\n                            \"ranking\": ranking,\n                            \"user\": USER,\n                        },\n                    )\n                    tasks.append(new_task)\n\n                case \"label_prompter_reply\" | \"label_assistant_reply\":\n                    # acknowledge task\n                    typer.echo(\"Here is the conversation so far:\")\n                    for message in task[\"conversation\"][\"messages\"]:\n                        typer.echo(_render_message(message))\n\n                    typer.echo(\"Label the following reply:\")\n                    typer.echo(task[\"reply\"])\n                    message_id = _random_message_id()\n                    user_message_id = _random_message_id()\n                    _post(f\"/api/v1/tasks/{task['id']}/ack\", {\"message_id\": message_id})\n                    valid_labels = task[\"valid_labels\"]\n                    mandatory_labels = task[\"mandatory_labels\"]\n\n                    labels_dict = None\n                    if task[\"mode\"] == \"simple\" and len(valid_labels) == 1:\n                        answer = random.choice([True, False])\n                        labels_dict = {valid_labels[0]: 1 if answer else 0}\n                    else:\n                        labels = random.sample(valid_labels, random.randint(1, len(valid_labels)))\n                        for l in mandatory_labels:\n                            if l not in labels:\n                                labels.append(l)\n                        labels_dict = {label: random.random() for label in valid_labels}\n                    if random.random() < 0.9:\n                        labels_dict[\"spam\"] = 0\n                        labels_dict[\"lang_mismatch\"] = 0\n\n                    # send interaction\n                    new_task = _post(\n                        \"/api/v1/tasks/interaction\",\n                        {\n                            \"type\": \"text_labels\",\n                            \"message_id\": task[\"message_id\"],\n                            \"task_id\": task[\"id\"],\n                            \"text\": task[\"reply\"],\n                            \"labels\": labels_dict,\n                            \"user\": USER,\n                        },\n                    )\n                    tasks.append(new_task)\n                case \"task_done\":\n                    typer.echo(\"Task done!\")\n                    # rerun with new task selected from above cases\n                    # add a new task\n                    q += 1\n                    if q == tasks_per_user:\n                        typer.echo(\"Task done!\")\n                        break\n                    tasks = [_post(\"/api/v1/tasks/\", {\"type\": \"random\", \"user\": USER})]\n                    #\n                case _:\n                    typer.echo(f\"Unknown task type {task['type']}\")\n                    # rerun with new task selected from above cases\n\n\nif __name__ == \"__main__\":\n    app()\n", "text-frontend/__main__.py": "\"\"\"Simple REPL frontend.\"\"\"\n\nimport http\nimport random\n\nimport requests\nimport typer\n\napp = typer.Typer()\n\n\n# debug constants\nUSER = {\"id\": \"1234\", \"display_name\": \"John Doe\", \"auth_method\": \"local\"}\n\n\ndef _random_message_id():\n    return str(random.randint(1000, 9999))\n\n\ndef _render_message(message: dict) -> str:\n    \"\"\"Render a message to the user.\"\"\"\n    if message[\"is_assistant\"]:\n        return f\"Assistant: {message['text']}\"\n    return f\"Prompter: {message['text']}\"\n\n\n@app.command()\ndef main(backend_url: str = \"http://127.0.0.1:8080\", api_key: str = \"1234\"):\n    \"\"\"Simple REPL frontend.\"\"\"\n\n    # make sure dummy user has accepted the terms of service\n    create_user_request = dict(USER)\n    create_user_request[\"tos_acceptance\"] = True\n    response = requests.post(\n        f\"{backend_url}/api/v1/frontend_users/\", json=create_user_request, headers={\"X-API-Key\": api_key}\n    )\n    response.raise_for_status()\n    user = response.json()\n    typer.echo(f\"user: {user}\")\n\n    def _post(path: str, json: dict) -> dict:\n        response = requests.post(f\"{backend_url}{path}\", json=json, headers={\"X-API-Key\": api_key})\n        response.raise_for_status()\n        if response.status_code == http.HTTPStatus.NO_CONTENT:\n            return None\n        return response.json()\n\n    typer.echo(\"Requesting work...\")\n    tasks = [_post(\"/api/v1/tasks/\", {\"type\": \"random\", \"user\": USER})]\n    while tasks:\n        task = tasks.pop(0)\n        match (task[\"type\"]):\n            case \"summarize_story\":\n                typer.echo(\"Summarize the following story:\")\n                typer.echo(task[\"story\"])\n\n                # acknowledge task\n                message_id = _random_message_id()\n                _post(f\"/api/v1/tasks/{task['id']}/ack\", {\"message_id\": message_id})\n\n                summary = typer.prompt(\"Enter your summary\")\n\n                user_message_id = _random_message_id()\n\n                # send interaction\n                new_task = _post(\n                    \"/api/v1/tasks/interaction\",\n                    {\n                        \"type\": \"text_reply_to_message\",\n                        \"message_id\": message_id,\n                        \"task_id\": task[\"id\"],\n                        \"user_message_id\": user_message_id,\n                        \"text\": summary,\n                        \"user\": USER,\n                    },\n                )\n                tasks.append(new_task)\n            case \"rate_summary\":\n                typer.echo(\"Rate the following summary:\")\n                typer.echo(task[\"summary\"])\n                typer.echo(\"Full text:\")\n                typer.echo(task[\"full_text\"])\n                typer.echo(f\"Rating scale: {task['scale']['min']} - {task['scale']['max']}\")\n\n                # acknowledge task\n                message_id = _random_message_id()\n                _post(f\"/api/v1/tasks/{task['id']}/ack\", {\"message_id\": message_id})\n\n                rating = typer.prompt(\"Enter your rating\", type=int)\n                # send interaction\n                new_task = _post(\n                    \"/api/v1/tasks/interaction\",\n                    {\n                        \"type\": \"message_rating\",\n                        \"message_id\": message_id,\n                        \"rating\": rating,\n                        \"user\": USER,\n                    },\n                )\n                tasks.append(new_task)\n            case \"initial_prompt\":\n                typer.echo(\"Please provide an initial prompt to the assistant.\")\n                if task[\"hint\"]:\n                    typer.echo(f\"Hint: {task['hint']}\")\n                # acknowledge task\n                message_id = _random_message_id()\n                _post(f\"/api/v1/tasks/{task['id']}/ack\", {\"message_id\": message_id})\n                prompt = typer.prompt(\"Enter your prompt\")\n                user_message_id = _random_message_id()\n                # send interaction\n                new_task = _post(\n                    \"/api/v1/tasks/interaction\",\n                    {\n                        \"type\": \"text_reply_to_message\",\n                        \"message_id\": message_id,\n                        \"task_id\": task[\"id\"],\n                        \"user_message_id\": user_message_id,\n                        \"text\": prompt,\n                        \"user\": USER,\n                    },\n                )\n                tasks.append(new_task)\n\n            case \"prompter_reply\":\n                typer.echo(\"Please provide a reply to the assistant.\")\n                typer.echo(\"Here is the conversation so far:\")\n                for message in task[\"conversation\"][\"messages\"]:\n                    typer.echo(_render_message(message))\n                if task[\"hint\"]:\n                    typer.echo(f\"Hint: {task['hint']}\")\n                # acknowledge task\n                message_id = _random_message_id()\n                _post(f\"/api/v1/tasks/{task['id']}/ack\", {\"message_id\": message_id})\n                reply = typer.prompt(\"Enter your reply\")\n                user_message_id = _random_message_id()\n                # send interaction\n                new_task = _post(\n                    \"/api/v1/tasks/interaction\",\n                    {\n                        \"type\": \"text_reply_to_message\",\n                        \"message_id\": message_id,\n                        \"user_message_id\": user_message_id,\n                        \"text\": reply,\n                        \"user\": USER,\n                    },\n                )\n                tasks.append(new_task)\n\n            case \"assistant_reply\":\n                typer.echo(\"Act as the assistant and reply to the user.\")\n                typer.echo(\"Here is the conversation so far:\")\n                for message in task[\"conversation\"][\"messages\"]:\n                    typer.echo(_render_message(message))\n                # acknowledge task\n                message_id = _random_message_id()\n                _post(f\"/api/v1/tasks/{task['id']}/ack\", {\"message_id\": message_id})\n                reply = typer.prompt(\"Enter your reply\")\n                user_message_id = _random_message_id()\n                # send interaction\n                new_task = _post(\n                    \"/api/v1/tasks/interaction\",\n                    {\n                        \"type\": \"text_reply_to_message\",\n                        \"message_id\": message_id,\n                        \"task_id\": task[\"id\"],\n                        \"user_message_id\": user_message_id,\n                        \"text\": reply,\n                        \"user\": USER,\n                    },\n                )\n                tasks.append(new_task)\n\n            case \"rank_initial_prompts\":\n                typer.echo(\"Rank the following prompts:\")\n                for idx, prompt in enumerate(task[\"prompts\"], start=1):\n                    typer.echo(f\"{idx}: {prompt}\")\n                # acknowledge task\n                message_id = _random_message_id()\n                _post(f\"/api/v1/tasks/{task['id']}/ack\", {\"message_id\": message_id})\n\n                ranking_str = typer.prompt(\"Enter the prompt numbers in order of preference, separated by commas\")\n                ranking = [int(x) - 1 for x in ranking_str.split(\",\")]\n\n                # send ranking\n                new_task = _post(\n                    \"/api/v1/tasks/interaction\",\n                    {\n                        \"type\": \"message_ranking\",\n                        \"message_id\": message_id,\n                        \"ranking\": ranking,\n                        \"user\": USER,\n                    },\n                )\n                tasks.append(new_task)\n\n            case \"rank_prompter_replies\" | \"rank_assistant_replies\":\n                typer.echo(\"Here is the conversation so far:\")\n                for message in task[\"conversation\"][\"messages\"]:\n                    typer.echo(_render_message(message))\n                typer.echo(\"Rank the following replies:\")\n                for idx, reply in enumerate(task[\"replies\"], start=1):\n                    typer.echo(f\"{idx}: {reply}\")\n                # acknowledge task\n                message_id = _random_message_id()\n                _post(f\"/api/v1/tasks/{task['id']}/ack\", {\"message_id\": message_id})\n\n                ranking_str = typer.prompt(\"Enter the reply numbers in order of preference, separated by commas\")\n                ranking = [int(x) - 1 for x in ranking_str.split(\",\")]\n\n                # send labels\n                new_task = _post(\n                    \"/api/v1/tasks/interaction\",\n                    {\n                        \"type\": \"message_ranking\",\n                        \"message_id\": message_id,\n                        \"task_id\": task[\"id\"],\n                        \"ranking\": ranking,\n                        \"user\": USER,\n                    },\n                )\n                tasks.append(new_task)\n\n            case \"label_initial_prompt\":\n                typer.echo(\"Label the following prompt:\")\n                typer.echo(task[\"prompt\"])\n                # acknowledge task\n                message_id = _random_message_id()\n                _post(f\"/api/v1/tasks/{task['id']}/ack\", {\"message_id\": message_id})\n\n                valid_labels = task[\"valid_labels\"]\n\n                labels_dict = None\n                if task[\"mode\"] == \"simple\" and len(valid_labels) == 1:\n                    answer: str = typer.confirm(f\"{valid_labels[0]}?\")\n                    labels_dict = {valid_labels[0]: 1 if answer else 0}\n                else:\n                    while labels_dict is None:\n                        labels_str: str = typer.prompt(\"Enter labels, separated by commas\")\n                        labels = labels_str.lower().replace(\" \", \"\").split(\",\")\n\n                        if all([label in valid_labels for label in labels]):\n                            labels_dict = {label: \"1\" if label in labels else \"0\" for label in valid_labels}\n                        else:\n                            invalid_labels = [label for label in labels if label not in valid_labels]\n                            typer.echo(f\"Invalid labels: {', '.join(invalid_labels)}. Valid: {', '.join(valid_labels)}\")\n\n                # send labels\n                new_task = _post(\n                    \"/api/v1/tasks/interaction\",\n                    {\n                        \"type\": \"text_labels\",\n                        \"message_id\": task[\"message_id\"],\n                        \"task_id\": task[\"id\"],\n                        \"text\": task[\"prompt\"],\n                        \"labels\": labels_dict,\n                        \"user\": USER,\n                    },\n                )\n                tasks.append(new_task)\n\n            case \"label_prompter_reply\" | \"label_assistant_reply\":\n                typer.echo(\"Here is the conversation so far:\")\n                for message in task[\"conversation\"][\"messages\"]:\n                    typer.echo(_render_message(message))\n\n                typer.echo(\"Label the following reply:\")\n                typer.echo(task[\"reply\"])\n                # acknowledge task\n                message_id = _random_message_id()\n                _post(f\"/api/v1/tasks/{task['id']}/ack\", {\"message_id\": message_id})\n\n                valid_labels = task[\"valid_labels\"]\n\n                labels_dict = None\n                if task[\"mode\"] == \"simple\" and len(valid_labels) == 1:\n                    answer: str = typer.confirm(f\"{valid_labels[0]}?\")\n                    labels_dict = {valid_labels[0]: 1 if answer else 0}\n                else:\n                    while labels_dict is None:\n                        labels_str: str = typer.prompt(\"Enter labels, separated by commas\")\n                        labels = labels_str.lower().replace(\" \", \"\").split(\",\")\n\n                        if all([label in valid_labels for label in labels]):\n                            labels_dict = {label: \"1\" if label in labels else \"0\" for label in valid_labels}\n                        else:\n                            invalid_labels = [label for label in labels if label not in valid_labels]\n                            typer.echo(f\"Invalid labels: {', '.join(invalid_labels)}. Valid: {', '.join(valid_labels)}\")\n\n                # send labels\n                new_task = _post(\n                    \"/api/v1/tasks/interaction\",\n                    {\n                        \"type\": \"text_labels\",\n                        \"message_id\": task[\"message_id\"],\n                        \"task_id\": task[\"id\"],\n                        \"text\": task[\"reply\"],\n                        \"labels\": labels_dict,\n                        \"user\": USER,\n                    },\n                )\n                tasks.append(new_task)\n\n            case \"task_done\":\n                typer.echo(\"Task done!\")\n            case _:\n                typer.echo(f\"Unknown task type {task['type']}\")\n\n\nif __name__ == \"__main__\":\n    app()\n", "scripts/xor-codec/xor_codec.py": "import gzip\nimport os\nimport shutil\nimport sys\nfrom pathlib import Path\n\nimport numpy\n\n\ndef xor_uncompressed(dst, src_payload, src_base, block_size=4096):\n    fp_payload = open(src_payload, \"rb\")\n    fp_base = open(src_base, \"rb\")\n    with open(dst, \"wb\") as fp:\n        while True:\n            buf1 = numpy.array(bytearray(fp_payload.read(block_size)), dtype=numpy.uint8)\n            buf2 = numpy.array(bytearray(fp_base.read(block_size)), dtype=numpy.uint8)\n            padding = len(buf1) - len(buf2)\n            if padding > 0:\n                buf2 = numpy.pad(buf2, (0, padding), \"constant\", constant_values=(0,))\n            if padding < 0:\n                buf2 = buf2[: len(buf1)]\n            buf = numpy.bitwise_xor(buf1, buf2)\n            fp.write(buf)\n            if len(buf1) < block_size:\n                break\n    fp_payload.close()\n    fp_base.close()\n\n\ndef xor_encode(dst, src_payload, src_base, block_size=4096):\n    fp_payload = open(src_payload, \"rb\")\n    fp_base = open(src_base, \"rb\")\n    with gzip.open(dst, \"wb\") as fp:\n        while True:\n            buf1 = numpy.array(bytearray(fp_payload.read(block_size)), dtype=numpy.uint8)\n            buf2 = numpy.array(bytearray(fp_base.read(block_size)), dtype=numpy.uint8)\n            padding = len(buf1) - len(buf2)\n            if padding > 0:\n                buf2 = numpy.pad(buf2, (0, padding), \"constant\", constant_values=(0,))\n            if padding < 0:\n                buf2 = buf2[: len(buf1)]\n            buf = numpy.bitwise_xor(buf1, buf2)\n            fp.write(buf)\n            if len(buf1) < block_size:\n                break\n    fp_payload.close()\n    fp_base.close()\n\n\ndef xor_decode(dst, src_payload, src_base, block_size=4096):\n    fp_payload = gzip.open(src_payload, \"rb\")\n    fp_base = open(src_base, \"rb\")\n    with open(dst, \"wb\") as fp:\n        while True:\n            buf1 = numpy.array(bytearray(fp_payload.read(block_size)), dtype=numpy.uint8)\n            buf2 = numpy.array(bytearray(fp_base.read(block_size)), dtype=numpy.uint8)\n            padding = len(buf1) - len(buf2)\n            if padding > 0:\n                buf2 = numpy.pad(buf2, (0, padding), \"constant\", constant_values=(0,))\n            if padding < 0:\n                buf2 = buf2[: len(buf1)]\n            buf = numpy.bitwise_xor(buf1, buf2)\n            fp.write(buf)\n            if len(buf1) < block_size:\n                break\n    fp_payload.close()\n    fp_base.close()\n\n\ndef xor_dir(dst, src_payload, src_base, decode=True, compress=True):\n    if compress:\n        xor = xor_decode if decode else xor_encode\n    else:\n        xor = xor_uncompressed\n    Path(dst).mkdir(parents=True, exist_ok=True)\n    shutil.copy(Path(src_payload) / \"added_tokens.json\", Path(dst) / \"added_tokens.json\")\n    for path in os.listdir(src_payload):\n        print(\"[*] Processing '%s'\" % path)\n        try:\n            xor(\"%s/%s\" % (dst, path), \"%s/%s\" % (src_payload, path), \"%s/%s\" % (src_base, path))\n        except Exception:\n            print(\"Exception when processing '%s'\" % path)\n\n\nif __name__ == \"__main__\":\n    if len(sys.argv) < 4:\n        print(\"Usage: xor.py <DESTINATION> <PAYLOAD SOURCE> <LLAMA SOURCE> [--encode] [--compress]\")\n        exit()\n    dst = sys.argv[1]\n    src_payload = sys.argv[2]\n    src_base = sys.argv[3]\n    decode = True\n    compress = False\n    if len(sys.argv) > 4:\n        for arg in sys.argv[4:]:\n            if arg == \"--encode\":\n                decode = False\n            if arg == \"--compress\":\n                compress = True\n    xor_dir(dst, src_payload, src_base, decode=decode, compress=compress)\n", "scripts/postprocessing/rankings.py": "from typing import List\n\nimport numpy as np\n\n\ndef head_to_head_votes(ranks: List[List[int]]):\n    tallies = np.zeros((len(ranks[0]), len(ranks[0])))\n    names = sorted(ranks[0])\n    ranks = np.array(ranks)\n    # we want the sorted indices\n    ranks = np.argsort(ranks, axis=1)\n    for i in range(ranks.shape[1]):\n        for j in range(i + 1, ranks.shape[1]):\n            # now count the cases someone voted for i over j\n            over_j = np.sum(ranks[:, i] < ranks[:, j])\n            over_i = np.sum(ranks[:, j] < ranks[:, i])\n            tallies[i, j] = over_j\n            # tallies[i,j] = over_i\n            tallies[j, i] = over_i\n            # tallies[j,i] = over_j\n    return tallies, names\n\n\ndef cycle_detect(pairs):\n    \"\"\"Recursively detect cycles by removing condorcet losers until either only one pair is left or condorcet losers no longer exist\n    This method upholds the invariant that in a ranking for all a,b either a>b or b>a for all a,b.\n\n\n    Returns\n    -------\n    out : False if the pairs do not contain a cycle, True if the pairs contain a cycle\n\n\n    \"\"\"\n    # get all condorcet losers (pairs that loose to all other pairs)\n    # idea: filter all losers that are never winners\n    # print(\"pairs\", pairs)\n    if len(pairs) <= 1:\n        return False\n    losers = [c_lose for c_lose in np.unique(pairs[:, 1]) if c_lose not in pairs[:, 0]]\n    if len(losers) == 0:\n        # if we recursively removed pairs, and at some point we did not have\n        # a condorcet loser, that means everything is both a winner and loser,\n        # yielding at least one (winner,loser), (loser,winner) pair\n        return True\n\n    new = []\n    for p in pairs:\n        if p[1] not in losers:\n            new.append(p)\n    return cycle_detect(np.array(new))\n\n\ndef get_winner(pairs):\n    \"\"\"\n    This returns _one_ concordant winner.\n    It could be that there are multiple concordant winners, but in our case\n    since we are interested in a ranking, we have to choose one at random.\n    \"\"\"\n    losers = np.unique(pairs[:, 1]).astype(int)\n    winners = np.unique(pairs[:, 0]).astype(int)\n    for w in winners:\n        if w not in losers:\n            return w\n\n\ndef get_ranking(pairs):\n    \"\"\"\n    Abuses concordance property to get a (not necessarily unique) ranking.\n    The lack of uniqueness is due to the potential existence of multiple\n    equally ranked winners. We have to pick one, which is where\n    the non-uniqueness comes from\n    \"\"\"\n    if len(pairs) == 1:\n        return list(pairs[0])\n    w = get_winner(pairs)\n    # now remove the winner from the list of pairs\n    p_new = np.array([(a, b) for a, b in pairs if a != w])\n    return [w] + get_ranking(p_new)\n\n\ndef ranked_pairs(ranks: List[List[int]]):\n    \"\"\"\n    Expects a list of rankings for an item like:\n        [(\"w\",\"x\",\"z\",\"y\") for _ in range(3)]\n        + [(\"w\",\"y\",\"x\",\"z\") for _ in range(2)]\n        + [(\"x\",\"y\",\"z\",\"w\") for _ in range(4)]\n        + [(\"x\",\"z\",\"w\",\"y\") for _ in range(5)]\n        + [(\"y\",\"w\",\"x\",\"z\") for _ in range(1)]\n    This code is quite brain melting, but the idea is the following:\n    1. create a head-to-head matrix that tallies up all win-lose combinations of preferences\n    2. take all combinations that win more than they loose and sort those by how often they win\n    3. use that to create an (implicit) directed graph\n    4. recursively extract nodes from the graph that do not have incoming edges\n    5. said recursive list is the ranking\n    \"\"\"\n    tallies, names = head_to_head_votes(ranks)\n    tallies = tallies - tallies.T\n    # note: the resulting tally matrix should be skew-symmetric\n    # order by strength of victory (using tideman's original method, don't think it would make a difference for us)\n    sorted_majorities = []\n    for i in range(len(ranks[0])):\n        for j in range(len(ranks[0])):\n            # you can never prefer yourself over yourself\n            # we also have to pick one of the two choices,\n            # if the preference is exactly zero...\n            if tallies[i, j] >= 0 and i != j:\n                sorted_majorities.append((i, j, tallies[i, j]))\n    # we don't explicitly deal with tied majorities here\n    sorted_majorities = np.array(sorted(sorted_majorities, key=lambda x: x[2], reverse=True))\n    # now do lock ins\n    lock_ins = []\n    for x, y, _ in sorted_majorities:\n        # invariant: lock_ins has no cycles here\n        lock_ins.append((x, y))\n        # print(\"lock ins are now\",np.array(lock_ins))\n        if cycle_detect(np.array(lock_ins)):\n            # print(\"backup: cycle detected\")\n            # if there's a cycle, delete the new addition and continue\n            lock_ins = lock_ins[:-1]\n    # now simply return all winners in order, and attach the losers\n    # to the back. This is because the overall loser might not be unique\n    # and (by concordance property) may never exist in any winning set to begin with.\n    # (otherwise he would either not be the loser, or cycles exist!)\n    # Since there could be multiple overall losers, we just return them in any order\n    # as we are unable to find a closer ranking\n    numerical_ranks = np.array(get_ranking(np.array(lock_ins))).astype(int)\n    conversion = [names[n] for n in numerical_ranks]\n    return conversion\n\n\nif __name__ == \"__main__\":\n    ranks = \"\"\" (\n        [(\"w\", \"x\", \"z\", \"y\") for _ in range(1)]\n        + [(\"w\", \"y\", \"x\", \"z\") for _ in range(2)]\n        # + [(\"x\",\"y\",\"z\",\"w\") for _ in range(4)]\n        + [(\"x\", \"z\", \"w\", \"y\") for _ in range(5)]\n        + [(\"y\", \"w\", \"x\", \"z\") for _ in range(1)]\n        # [(\"y\",\"z\",\"w\",\"x\") for _ in range(1000)]\n    )\"\"\"\n    ranks = [\n        [\n            (\"c5181083-d3e9-41e7-a935-83fb9fa01488\"),\n            (\"dcf3d179-0f34-4c15-ae21-b8feb15e422d\"),\n            (\"d11705af-5575-43e5-b22e-08d155fbaa62\"),\n        ],\n        [\n            (\"d11705af-5575-43e5-b22e-08d155fbaa62\"),\n            (\"c5181083-d3e9-41e7-a935-83fb9fa01488\"),\n            (\"dcf3d179-0f34-4c15-ae21-b8feb15e422d\"),\n        ],\n        [\n            (\"dcf3d179-0f34-4c15-ae21-b8feb15e422d\"),\n            (\"c5181083-d3e9-41e7-a935-83fb9fa01488\"),\n            (\"d11705af-5575-43e5-b22e-08d155fbaa62\"),\n        ],\n        [\n            (\"d11705af-5575-43e5-b22e-08d155fbaa62\"),\n            (\"c5181083-d3e9-41e7-a935-83fb9fa01488\"),\n            (\"dcf3d179-0f34-4c15-ae21-b8feb15e422d\"),\n        ],\n    ]\n    rp = ranked_pairs(ranks)\n    print(rp)\n", "scripts/postprocessing/scoring.py": "from dataclasses import dataclass, replace\nfrom typing import Any\n\nimport numpy as np\nimport numpy.typing as npt\nfrom scipy.stats import kendalltau\n\n\n@dataclass\nclass Voter:\n    \"\"\"\n    Represents a single voter.\n    This tabulates the number of good votes, total votes,\n    and points.\n    We only put well-behaved people on the scoreboard and filter out the badly behaved ones\n    \"\"\"\n\n    uid: Any\n    num_votes: int\n    num_good_votes: int\n    num_prompts: int\n    num_good_prompts: int\n    num_rankings: int\n    num_good_rankings: int\n\n    #####################\n    voting_points: int\n    prompt_points: int\n    ranking_points: int\n\n    def voter_quality(self):\n        return self.num_good_votes / self.num_votes\n\n    def rank_quality(self):\n        return self.num_good_rankings / self.num_rankings\n\n    def prompt_quality(self):\n        return self.num_good_prompts / self.num_prompts\n\n    def is_well_behaved(self, threshhold_vote, threshhold_prompt, threshhold_rank):\n        return (\n            self.voter_quality() > threshhold_vote\n            and self.prompt_quality() > threshhold_prompt\n            and self.rank_quality() > threshhold_rank\n        )\n\n    def total_points(self, voting_weight, prompt_weight, ranking_weight):\n        return (\n            voting_weight * self.voting_points\n            + prompt_weight * self.prompt_points\n            + ranking_weight * self.ranking_points\n        )\n\n\ndef score_update_votes(new_vote: int, consensus: npt.ArrayLike, voter_data: Voter) -> Voter:\n    \"\"\"\n    This function returns the new \"quality score\" and points for a voter,\n    after that voter cast a vote on a question.\n\n    This function is only to be run when archiving a question\n    i.e. the question has had sufficiently many votes, or we can't get more than \"K\" bits of information\n\n    The consensus is the array of all votes cast by all voters for that question\n    We then update the voter data using the new information\n\n        Parameters:\n            new_vote (int): the index of the vote cast by the voter\n            consensus (ArrayLike): all votes cast for this question\n            voter_data (Voter): a \"Voter\" object that represents the person casting the \"new_vote\"\n\n        Returns:\n            updated_voter (Voter): the new \"quality score\" and points for the voter\n    \"\"\"\n    # produces the ranking of votes, e.g. for [100,300,200] it returns [0, 2, 1],\n    # since 100 is the lowest, 300 the highest and 200 the middle value\n    consensus_ranking = np.argsort(np.argsort(consensus))\n    new_points = consensus_ranking[new_vote] + voter_data.voting_points\n\n    # we need to correct for 0 indexing, if you are closer to \"right\" than \"wrong\" of the consensus,\n    # it's a good vote\n    new_good_votes = int(consensus_ranking[new_vote] > (len(consensus) - 1) / 2) + voter_data.num_good_votes\n    new_num_votes = voter_data.num_votes + 1\n    return replace(voter_data, num_votes=new_num_votes, num_good_votes=new_good_votes, voting_points=new_points)\n\n\ndef score_update_prompts(consensus: npt.ArrayLike, voter_data: Voter) -> Voter:\n    \"\"\"\n    This function returns the gain of points for a given prompt's votes\n\n    In contrast to the other score updating functions, we can run this online as new votes come in.\n    i.e. the question has had sufficiently many votes, or we can't get more than \"K\" bits of information.\n\n\n    Parameters:\n            consensus (ArrayLike): all votes cast for this question\n            voter_data (Voter): a \"Voter\" object that represents the person that wrote the prompt\n\n        Returns:\n            updated_voter (Voter): the new \"quality score\" and points for the voter\n    \"\"\"\n    # produces the ranking of votes, e.g. for [100,300,200] it returns [0, 2, 1],\n    # since 100 is the lowest, 300 the highest and 200 the middle value\n    consensus_ranking = np.arange(len(consensus)) - len(consensus) // 2 + 1\n    # expected consensus ranking (i.e. normalize the votes and multiply-sum with weightings)\n    delta_votes = np.sum(consensus_ranking * consensus / sum(consensus))\n    new_points = delta_votes + voter_data.prompt_points\n\n    # we need to correct for 0 indexing, if you are closer to \"right\" than \"wrong\" of the consensus,\n    # it's a good vote\n    new_good_prompts = int(delta_votes > 0) + voter_data.num_good_prompts\n    new_num_prompts = voter_data.num_prompts + 1\n    return replace(\n        voter_data,\n        num_prompts=new_num_prompts,\n        num_good_prompts=new_good_prompts,\n        prompt_points=new_points,\n    )\n\n\ndef score_update_ranking(user_ranking: npt.ArrayLike, consensus_ranking: npt.ArrayLike, voter_data: Voter) -> Voter:\n    \"\"\"\n    This function returns the gain of points for a given ranking's votes\n\n    This function is only to be run when archiving a question\n    i.e. the question has had sufficiently many votes, or we can't get more than \"K\" bits of information\n\n    we use the bubble-sort distance (or \"kendall-tau\" distance) to compare the two rankings\n    we use this over spearman correlation since:\n        \"[Kendall's \u03c4] approaches a normal distribution more rapidly than \u03c1, as N, the sample size, increases;\n            and \u03c4 is also more tractable mathematically, particularly when ties are present\"\n    Gilpin, A. R. (1993). Table for conversion of Kendall's Tau to Spearman's\n     Rho within the context measures of magnitude of effect for meta-analysis\n\n    Further in\n        \"research design and statistical analyses, second edition, 2003\"\n    the authors note that at least from an significance test POV they will yield the same p-values\n\n        Parameters:\n            user_ranking (ArrayLike): ranking produced by the user\n            consensus (ArrayLike): ranking produced after running the voting algorithm to merge into the consensus ranking\n            voter_data (Voter): a \"Voter\" object that represents the person that wrote the prompt\n\n        Returns:\n            updated_voter (Voter): the new \"quality score\" and points for the voter\n    \"\"\"\n    bubble_sort_distance, p_value = kendalltau(user_ranking, consensus_ranking)\n    # normalize kendall-tau from [-1,1] into [0,1] range\n    bubble_sort_distance = (1 + bubble_sort_distance) / 2\n    new_points = bubble_sort_distance + voter_data.ranking_points\n    new_good_rankings = int(bubble_sort_distance > 0.5) + voter_data.num_good_rankings\n    new_num_rankings = voter_data.num_rankings + 1\n    return replace(\n        voter_data,\n        num_rankings=new_num_rankings,\n        num_good_rankings=new_good_rankings,\n        ranking_points=new_points,\n    )\n\n\nif __name__ == \"__main__\":\n    demo_voter = Voter(\n        \"abc\",\n        num_votes=10,\n        num_good_votes=2,\n        num_prompts=10,\n        num_good_prompts=2,\n        num_rankings=10,\n        num_good_rankings=2,\n        voting_points=6,\n        prompt_points=0,\n        ranking_points=0,\n    )\n    new_vote = 3\n    consensus = np.array([200, 300, 100, 500])\n    print(demo_voter)\n    print(\"best   vote  \", score_update_votes(new_vote, consensus, demo_voter))\n    new_vote = 2\n    print(\"worst  vote  \", score_update_votes(new_vote, consensus, demo_voter))\n    new_vote = 1\n    print(\"medium vote  \", score_update_votes(new_vote, consensus, demo_voter))\n    print(\"prompt writer\", score_update_prompts(consensus, demo_voter))\n    print(\"best   rank  \", score_update_ranking(np.array([0, 2, 1]), np.array([0, 2, 1]), demo_voter))\n    print(\"medium rank  \", score_update_ranking(np.array([2, 0, 1]), np.array([0, 2, 1]), demo_voter))\n    print(\"worst  rank  \", score_update_ranking(np.array([1, 0, 2]), np.array([0, 2, 1]), demo_voter))\n", "scripts/postprocessing/regex_pii_detector.py": "import re\n\n# Adapted from\n# https://www.geeksforgeeks.org/how-to-validate-ssn-social-security-number-using-regular-expression/\n# https://docs.opswat.com/mdcore/proactive-dlp/sample-regular-expressions\n# https://github.com/m4ll0k/SecretFinder/blob/master/BurpSuite-SecretFinder/SecretFinder.py\n\nregex_patterns = {\n    \"google_api\": r\"AIza[0-9A-Za-z-_]{35}\",\n    \"bitcoin_address\": r\"([13][a-km-zA-HJ-NP-Z0-9]{26,33})\",\n    \"slack_api_key\": r\"xox.-[0-9]{12}-[0-9]{12}-[0-9a-zA-Z]{24}\",\n    \"google_cloud_platform_auth\": r\"[0-9a-fA-F]{8}-[0-9a-fA-F]{4}-[0-9a-fA-F]{12}\",\n    \"google_cloud_platform_api\": r\"[A-Za-z0-9_]{21}--[A-Za-z0-9_]{8}\",\n    \"gmail_auth_token\": r\"[0-9(+-[0-9A-Za-z_]{32}.apps.qooqleusercontent.com\",\n    \"github_auth_token\": r\"[0-9a-fA-F]{40}\",\n    \"Instagram_token\": r\"[0-9a-fA-F]{7}.[0-9a-fA-F]{32}\",\n    \"google_captcha\": r\"6L[0-9A-Za-z-_]{38}|^6[0-9a-zA-Z_-]{39}$\",\n    \"google_oauth\": r\"ya29\\.[0-9A-Za-z\\-_]+\",\n    \"amazon_aws_access_key_id\": r\"A[SK]IA[0-9A-Z]{16}\",\n    \"amazon_mws_auth_toke\": r\"amzn\\\\.mws\\\\.[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}\",\n    \"amazon_aws_url\": r\"s3\\.amazonaws.com[/]+|[a-zA-Z0-9_-]*\\.s3\\.amazonaws.com\",\n    \"facebook_access_token\": r\"EAACEdEose0cBA[0-9A-Za-z]+\",\n    \"authorization_basic\": r\"basic\\s*[a-zA-Z0-9=:_\\+\\/-]+\",\n    \"authorization_bearer\": r\"bearer\\s*[a-zA-Z0-9_\\-\\.=:_\\+\\/]+\",\n    \"authorization_api\": r\"api[key|\\s*]+[a-zA-Z0-9_\\-]+\",\n    \"mailgun_api_key\": r\"key-[0-9a-zA-Z]{32}\",\n    \"paypal_braintree_access_token\": r\"access_token\\$production\\$[0-9a-z]{16}\\$[0-9a-f]{32}\",\n    \"square_oauth_secret\": r\"sq0csp-[ 0-9A-Za-z\\-_]{43}|sq0[a-z]{3}-[0-9A-Za-z\\-_]{22,43}\",\n    \"square_access_token\": r\"sqOatp-[0-9A-Za-z\\-_]{22}|EAAA[a-zA-Z0-9]{60}\",\n    \"stripe_standard_api\": r\"sk_live_[0-9a-zA-Z]{24}\",\n    \"stripe_restricted_api\": r\"rk_live_[0-9a-zA-Z]{24}\",\n    \"github_access_token\": r\"[a-zA-Z0-9_-]*:[a-zA-Z0-9_\\-]+@github\\.com*\",\n    \"rsa_private_key\": r\"-----BEGIN RSA PRIVATE KEY-----\",\n    \"ssh_dsa_private_key\": r\"-----BEGIN DSA PRIVATE KEY-----\",\n    \"ssh_ec_private_key\": r\"-----BEGIN EC PRIVATE KEY-----\",\n    \"pgp_private_block\": r\"-----BEGIN PGP PRIVATE KEY BLOCK-----\",\n    \"json_web_token\": r\"ey[A-Za-z0-9_-]*\\.[A-Za-z0-9._-]*|ey[A-Za-z0-9_\\/+-]*\\.[A-Za-z0-9._\\/+-]*\",\n    \"social_security_number\": r\"(?!666|000|9\\\\d{2})\\\\d{3}-(?!00)\\\\d{2}-(?!0{4})\\\\d{4}$\",\n    \"e_mail\": r\"(?:^|\\s)[\\w!#$%&'*+/=?^`{|}~-](\\.?[\\w!#$%&'*+/=?^`{|}~-])*@\\w+[.-]?\\w*\\.[a-zA-Z]{2,3}\\b\",\n}\n\n# Used to query the type later since that is more efficient than doing it dynamically.\nregexes_patterns_inverse = {\n    r\"AIza[0-9A-Za-z-_]{35}\": \"google_api\",\n    r\"([13][a-km-zA-HJ-NP-Z0-9]{26,33})\": \"bitcoin_address\",\n    r\"xox.-[0-9]{12}-[0-9]{12}-[0-9a-zA-Z]{24}\": \"slack_api_key\",\n    r\"[0-9a-fA-F]{8}-[0-9a-fA-F]{4}-[0-9a-fA-F]{12}\": \"google_cloud_platform_auth\",\n    r\"[A-Za-z0-9_]{21}--[A-Za-z0-9_]{8}\": \"google_cloud_platform_api\",\n    r\"[0-9(+-[0-9A-Za-z_]{32}.apps.qooqleusercontent.com\": \"gmail_auth_token\",\n    r\"[0-9a-fA-F]{40}\": \"github_auth_token\",\n    r\"[0-9a-fA-F]{7}.[0-9a-fA-F]{32}\": \"Instagram_token\",\n    r\"6L[0-9A-Za-z-_]{38}|^6[0-9a-zA-Z_-]{39}$\": \"google_captcha\",\n    r\"ya29\\.[0-9A-Za-z\\-_]+\": \"google_oauth\",\n    r\"A[SK]IA[0-9A-Z]{16}\": \"amazon_aws_access_key_id\",\n    r\"amzn\\\\.mws\\\\.[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}\": \"amazon_mws_auth_toke\",\n    r\"s3\\.amazonaws.com[/]+|[a-zA-Z0-9_-]*\\.s3\\.amazonaws.com\": \"amazon_aws_url\",\n    r\"EAACEdEose0cBA[0-9A-Za-z]+\": \"facebook_access_token\",\n    r\"basic\\s*[a-zA-Z0-9=:_\\+\\/-]+\": \"authorization_basic\",\n    r\"bearer\\s*[a-zA-Z0-9_\\-\\.=:_\\+\\/]+\": \"authorization_bearer\",\n    r\"api[key|\\s*]+[a-zA-Z0-9_\\-]+\": \"authorization_api\",\n    r\"key-[0-9a-zA-Z]{32}\": \"mailgun_api_key\",\n    r\"access_token\\$production\\$[0-9a-z]{16}\\$[0-9a-f]{32}\": \"paypal_braintree_access_token\",\n    r\"sq0csp-[ 0-9A-Za-z\\-_]{43}|sq0[a-z]{3}-[0-9A-Za-z\\-_]{22,43}\": \"square_oauth_secret\",\n    r\"sqOatp-[0-9A-Za-z\\-_]{22}|EAAA[a-zA-Z0-9]{60}\": \"square_access_token\",\n    r\"sk_live_[0-9a-zA-Z]{24}\": \"stripe_standard_api\",\n    r\"rk_live_[0-9a-zA-Z]{24}\": \"stripe_restricted_api\",\n    r\"[a-zA-Z0-9_-]*:[a-zA-Z0-9_\\-]+@github\\.com*\": \"github_access_token\",\n    r\"-----BEGIN RSA PRIVATE KEY-----\": \"rsa_private_key\",\n    r\"-----BEGIN EC PRIVATE KEY-----\": \"ssh_ec_private_key\",\n    r\"-----BEGIN DSA PRIVATE KEY-----\": \"ssh_dsa_private_key\",\n    r\"-----BEGIN PGP PRIVATE KEY BLOCK-----\": \"pgp_private_block\",\n    r\"ey[A-Za-z0-9_-]*\\.[A-Za-z0-9._-]*|ey[A-Za-z0-9_\\/+-]*\\.[A-Za-z0-9._\\/+-]*\": \"json_web_token\",\n    r\"(?!666|000|9\\\\d{2})\\\\d{3}-(?!00)\\\\d{2}-(?!0{4})\\\\d{4}$\": \"social_security_number\",\n    r\"(?:^|\\s)[\\w!#$%&'*+/=?^`{|}~-](\\.?[\\w!#$%&'*+/=?^`{|}~-])*@\\w+[.-]?\\w*\\.[a-zA-Z]{2,3}\\b\": \"e_mail\",\n}\n\n\nclass PIIDetector:\n    # Pre compile regexes.\n    def __init__(self):\n        self.regex_list_compiled = []\n        for regex in regex_patterns.values():\n            regex_compiled = re.compile(regex, re.I)\n            self.regex_list_compiled.append(regex_compiled)\n\n    # Returns first pii match in input_text or (\"\", None).\n    def get_pii(self, input_text: str):\n        for reg in self.regex_list_compiled:\n            match = re.search(reg, input_text)\n            if match is None:\n                continue\n            else:\n                return (reg.pattern, match.start())\n        return (\"\", None)\n\n    def formatted_output(self, match_list: list):\n        for match in match_list:\n            print(\"\\nLinenumber: \" + str(match[0]))\n            # To query the actual name efficiently, use  inverted dictionary.\n            print(\"Type: \" + regexes_patterns_inverse.get(match[1][0]))\n            print(\"Start Position: \" + str(match[1][1]))\n", "scripts/postprocessing/task_schedule.py": "from enum import Enum\n\nimport numpy as np\nfrom scipy import optimize\n\n\nclass Task(Enum):\n    RANKING = 0\n    ANSWER = 1\n    PROMPT = 2\n    VOTE = 3\n\n\ndef task_selection(\n    num_ranking_tasks: int, current_prompts: int, target_num_prompts: int, p: float, answers_per_prompt: int\n) -> Task:\n    \"\"\"\n    This computes which task to serve to the user.\n    In general, this method aims to get rankable tasks out of the active pool ASAP.\n    Before checking anything else, we first have a p% probability of running a ranking task.\n    After that, we can dynamically determine which task to serve by balancing the number of active tasks.\n\n        Parameters:\n            num_ranking_tasks (int): number of prompts that are ready to do ranking (i.e. have \"answers_per_prompt\" many answers)\n            current_prompts (int): how many prompts are currently in the active pool\n            target_num_prompts (int): how many prompts _should_ be in the active pool\n            p (float): probability to serve a ranking task, if one is available\n            answers_per_prompt (int): number of answers we want to have per prompt\n        Returns:\n            task (Task): the task Enum that corresponds to one of the four tasks\n    \"\"\"\n    if num_ranking_tasks > 0 and np.random.rand() < p:\n        return Task.RANKING\n    rate = 50 / (current_prompts * 2)\n    prob_prompt_task = 0.5 + (target_num_prompts - current_prompts) * rate\n    # Yes, I'm too lazy to solve this analytically...\n    prob_unfinished_prompt = optimize.linprog(\n        np.array([1, 1]), A_eq=np.array([[1, 1], [1, -answers_per_prompt]]), b_eq=np.array([1, 0]), bounds=(0, None)\n    ).x[0]\n    if np.random.rand() < prob_prompt_task:\n        if np.random.rand() < prob_unfinished_prompt:\n            return Task.ANSWER\n        else:\n            return Task.PROMPT\n    else:\n        return Task.VOTE\n\n\ndef next_answer_task(possible_prompts, answers_per_prompt):\n    \"\"\"\n    If the `task_selection`method returns \"answer\", you can use this method to decide which\n    prompt should get an answer next.\n    The goal of this is to finish off the prompts that have almost enough answers collected already:\n    I.e. if we want 5 answers, this is going to give preferential sampling to those prompts that already\n    have 4/5 answers.\n    This helps to not have too much close-to-finished prompts in the active set.\n\n        Parameters:\n            possible_prompts (dict[prompt_id, num_answers]): a dictionary containing all open prompts and the number of answers these prompts currently have.\n            answers_per_prompt (int): number of answers we per prompt to target\n        Returns:\n            prompt_id (int): the prompt_id corresponding to the next prompt that should get a new answer\n    \"\"\"\n    nums = list(set(possible_prompts.values()))\n    p = np.array([max(x / answers_per_prompt, 1 / answers_per_prompt) for x in nums])\n    idx = np.random.choice(nums, p=p / p.sum())\n    sample = np.random.choice([k for k, v in possible_prompts.items() if v == idx])\n    return sample\n\n\nif __name__ == \"__main__\":\n    x = task_selection(1, 500, 1000, 0.1, 5)\n    print(x)\n    y = next_answer_task({\"this\": 2, \"is\": 4, \"a\": 1, \"test\": 4}, 5)\n    print(y)\n", "scripts/postprocessing/ranking_disagreement.py": "from collections import defaultdict\n\nimport numpy as np\nimport pandas as pd\nimport psycopg2\nfrom rankings import ranked_pairs\nfrom scipy.stats import kendalltau\n\n\n# source: wikipedia ;)\n# but here without the normalization\ndef normalised_kendall_tau_distance(values1, values2):\n    \"\"\"Compute the Kendall tau distance.\"\"\"\n    n = len(values1)\n    assert len(values2) == n, \"Both lists have to be of equal length\"\n    i, j = np.meshgrid(np.arange(n), np.arange(n))\n    a = np.argsort(values1)\n    b = np.argsort(values2)\n    ndisordered = np.logical_or(\n        np.logical_and(a[i] < a[j], b[i] > b[j]), np.logical_and(a[i] > a[j], b[i] < b[j])\n    ).sum()\n    return ndisordered / (n * (n - 1))\n\n\ndef get_df():\n    \"\"\"\n    Simple method that computes merged rankings and compares them to each user.\n    Most interesting output for end-user is presumably the last that lists each user with their\n    correlation to the mean ranking.\n    Lower means less well aligned to the mean, higher means more well aligned.\n    Note that rankings with fewer options are more likely to be wrong, so this could\n    yield to misleading results:\n    **You cannot use this for automatic flagging!**\n    \"\"\"\n    conn = psycopg2.connect(\"host=0.0.0.0 port=5432 user=postgres password=postgres dbname=postgres\")\n    # Define the SQL query\n    # query = \"\"\"SELECT DISTINCT t.parent_message_id, r.user_id, r.payload->'payload'->>'ranked_message_ids' as ranked_ids\n    #    FROM message_reaction r JOIN task t ON r.task_id = t.id\n    #      WHERE r.payload->'payload'->>'type' = 'message_ranking';\"\"\"\n    role = \"'assistant'\"\n    message_tree_id = None  # \"'ef458036-ae8e-4ff5-98f2-0f9dfedcb206'\"\n    query = f\"\"\"\n        -- get all ranking results of completed tasks for all parents with >= 2 children\n        SELECT DISTINCT p.parent_id, p.message_tree_id, mr.* FROM\n        (\n            -- find parents with > 1 children\n            SELECT m.parent_id, m.message_tree_id, COUNT(m.id) children_count\n            FROM message_tree_state mts\n            INNER JOIN message m ON mts.message_tree_id = m.message_tree_id\n            WHERE m.review_result                  -- must be reviewed\n            AND NOT m.deleted                   -- not deleted\n            AND m.parent_id IS NOT NULL         -- ignore initial prompts\n            AND ({role} IS NULL OR m.role = {role}) -- children with matching role\n            -- AND mts.message_tree_id = {message_tree_id}\n            GROUP BY m.parent_id, m.message_tree_id\n            HAVING COUNT(m.id) > 1\n        ) as p\n        LEFT JOIN task t ON p.parent_id = t.parent_message_id AND t.done AND (t.payload_type = 'RankPrompterRepliesPayload' OR t.payload_type = 'RankAssistantRepliesPayload')\n        LEFT JOIN message_reaction mr ON mr.task_id = t.id AND mr.payload_type = 'RankingReactionPayload'\n        \"\"\"\n\n    # Read the query results into a Pandas dataframe\n    df = pd.read_sql(query, con=conn)\n    print(df[[\"message_tree_id\", \"parent_id\", \"payload\"]])\n    # Close the database connection\n    conn.close()\n    users = set()\n    messages = set()\n    rankings = defaultdict(list)\n    rankings_with_user = defaultdict(list)\n    for row in df.itertuples(index=False):\n        row = row._asdict()\n        users.add(str(row[\"user_id\"]))\n        messages.add(str(row[\"message_tree_id\"]))\n        #\n        if row[\"payload\"] is None:\n            continue\n        ranking = row[\"payload\"][\"payload\"][\"ranked_message_ids\"]\n        rankings_with_user[str(row[\"parent_id\"])].append((ranking, str(row[\"user_id\"])))\n        rankings[str(row[\"parent_id\"])].append(ranking)\n    print(*[f\"{k} : {v}\" for k, v in rankings.items()], sep=\"\\n\")\n    users = list(users)\n    messages = list(messages)\n    consensus = dict()\n    total_correlation = list()\n    for k, v in rankings.items():\n        # print(\"v\",[len(i) for i in v])\n        common_set = set.intersection(*map(set, v))\n        # clean up the rankings and remove stuff not in all of them\n        v = [list(filter(lambda x: x in common_set, ids)) for ids in v]\n        merged_rankings = ranked_pairs(v)\n        consensus[k] = merged_rankings\n        ls = []\n        for vote, id in rankings_with_user[k]:\n            # clean up the rankings and remove stuff not in all of them\n            vote = list(filter(lambda x: x in common_set, vote))\n            ls.append((kendalltau(merged_rankings, vote), id))\n        rankings_with_user[k] = ls\n        total_correlation.extend(ls)\n    correlation_by_user = defaultdict(list)\n    for u in users:\n        for c, m in total_correlation:\n            if m == u:\n                correlation_by_user[u].append(c)\n\n    return consensus, users, messages, rankings_with_user, correlation_by_user\n\n\nif __name__ == \"__main__\":\n    cons, user, messages, rankings, correlation_by_user = get_df()\n    # print(user)\n    # print(messages)\n    # print(rankings)\n    # print(\"consensus:\", cons)\n    print(\"correlation_by_user:\", correlation_by_user)\n    for k, v in correlation_by_user.items():\n        if len(v) < 50:\n            res = \"not enough data\"\n        else:\n            i = list(map(lambda x: x, v))\n            res = np.mean(i)\n            res_std = np.std(i)\n            print(\"result:\", k, f\" with value {res:.2f}\", f\"\u00b1 {res_std:.2f}\")\n", "scripts/postprocessing/importance_selection.py": "import logging\nimport warnings\n\nimport numpy as np\nimport pandas as pd\nimport psycopg2\nfrom scipy.optimize import LinearConstraint, minimize\nfrom scipy.sparse import coo_array, csr_array, csr_matrix, hstack\nfrom scipy.special import softmax\nfrom tqdm import trange\nfrom tqdm.contrib.logging import logging_redirect_tqdm\n\n\ndef least_squares_fit(features, target, scaling=1):\n    X = features  # (features - np.mean(features, 0)) / np.std(features, 0)\n    # print(\"feature\",X.shape)\n    # get target\n    y = target.reshape(-1)\n    # Use simple imputer for mean to not change the importance of tree split\n    # Create an instance of the ExtraTreesRegressor algorithm\n    zX = X.toarray()\n    summed_target = y  # (y+zX[:,-1])/2\n    vote_matrix = csr_matrix(zX[:, :-1])\n    constraint = LinearConstraint(np.ones(X.shape[-1] - 1), 1 * scaling, 1 * scaling)\n    init = np.ones(X.shape[-1] - 1)  # lsqr(vote_matrix,summed_target)[0]\n    init = init / np.linalg.norm(init)\n    result = minimize(\n        lambda x: np.sum((vote_matrix @ x - summed_target) ** 2),\n        init,\n        jac=lambda x: 2 * vote_matrix.T @ (vote_matrix @ x - summed_target),\n        constraints=constraint,\n        hess=lambda _: 2 * vote_matrix.T @ vote_matrix,\n        method=\"trust-constr\",\n    )\n    # result = least_squares(residual, np.ones(X.shape[-1]-1))\n    # result = least_squares(zX[:,:-1], (y+zX[:,-1])/2,\n    # print(result)\n    return np.concatenate([result.x, np.ones(1)])\n\n\ndef get_df(study_label):\n    conn = psycopg2.connect(\"host=0.0.0.0 port=5432 user=postgres password=postgres dbname=postgres\")\n    # Define the SQL query\n    query = (\n        \"SELECT DISTINCT message_id, labels, message.user_id FROM text_labels JOIN message ON message_id = message.id;\"\n    )\n\n    # Read the query results into a Pandas dataframe\n    df = pd.read_sql(query, con=conn)\n    print(df.head())\n    # Close the database connection\n    conn.close()\n    users = set()\n    messages = set()\n    for row in df.itertuples(index=False):\n        row = row._asdict()\n        users.add(str(row[\"user_id\"]))\n        # for row in df.itertuples(index=False):\n        # row = row._asdict()\n        messages.add(str(row[\"message_id\"]))\n    users = list(users)\n    messages = list(messages)\n    print(\"num users:\", len(users), \"num messages:\", len(messages), \"num in df\", len(df))\n\n    # arr = np.full((len(messages), len(users)), np.NaN, dtype=np.half)\n    row_idx = []\n    col_idx = []\n    data = []\n\n    def swap(x):\n        return (x[1], x[0])\n\n    dct = dict(map(swap, enumerate(messages)))\n    print(\"converting messages...\")\n    df[\"message_id\"] = df[\"message_id\"].map(dct)\n    print(\"converting users...\")\n    df[\"user_id\"] = df[\"user_id\"].map(dict(map(swap, enumerate(users))))\n    print(\"converting labels...\")\n    df[\"labels\"] = df[\"labels\"].map(lambda x: float(x.get(study_label, 0)))\n    row_idx = df[\"message_id\"].to_numpy()\n    col_idx = df[\"user_id\"].to_numpy()\n    data = df[\"labels\"].to_numpy()\n    print(data)\n    print(row_idx)\n    print(col_idx)\n    \"\"\" for row in df.itertuples(index=False):\n        row = row._asdict()\n        labels = row[\"labels\"]\n        value = labels.get(study_label, None)\n        if value is not None:\n            # tmp=out[str(row[\"message_id\"])]\n            # tmp = np.array(tmp)\n            # tmp[users.index(row[\"user_id\"])] = value\n            # out[str(row[\"message_id\"])] = np.array(tmp)\n            # print(out[str(row[\"message_id\"])].density)\n            row_idx.append(messages.index(str(row[\"message_id\"])))\n            col_idx.append(users.index(str(row[\"user_id\"])))\n            data.append(value)\n            #arr[mid, uid] = value \"\"\"\n    arr = csr_array(coo_array((data, (row_idx, col_idx))))\n    print(\"results\", len(users), arr.shape)\n    # df = pd.DataFrame.from_dict(out,orient=\"index\")\n    print(\"generated dataframe\")\n    return arr, messages, users\n\n\ndef reweight_features(features, weights, noise_scale=0.0):\n    # X = df.drop(target_col, axis=1)\n    # print(\"info\",features.shape,weights.shape)\n    # X = (features - np.mean(features, 0).reshape(1,-1)) / np.std(features, 0).reshape(1,-1)\n    noise = np.random.randn(weights.shape[0]) * noise_scale\n    weights = weights + noise\n    # normalizer = (X.notna().astype(float) * weights).sum(skipna=True, axis=1)\n    values = features @ weights\n    # values = values / normalizer\n    return values\n\n\ndef get_subframe(arr, columns_to_filter):\n    # return np.delete(arr, columns_to_filter, axis=1)\n    \"\"\"\n    Remove the rows denoted by ``indices`` form the CSR sparse matrix ``mat``.\n    \"\"\"\n    if not isinstance(arr, csr_array):\n        raise ValueError(\"works only for CSR format -- use .tocsr() first\")\n    indices = list(columns_to_filter)\n    mask = np.ones(arr.shape[1], dtype=bool)\n    mask[indices] = False\n    return arr[:, mask]\n\n\ndef sample_importance_weights(importance_weights, temperature=1.0):\n    weights = softmax(\n        abs(importance_weights) / temperature,\n    )\n    column = np.random.choice(len(importance_weights), p=weights)\n    return column\n\n\ndef make_random_testframe(num_rows, num_cols, frac_missing):\n    data = np.random.rand(num_rows, num_cols).astype(np.float16)\n    mask = np.random.rand(num_rows, num_cols) < frac_missing\n    data[mask] = np.nan\n    return data\n\n\ndef combine_underrepresented_columns(arr, num_instances):\n    # 1. get the mask\n    mask = arr != 0\n    to_combine = mask.sum(0) < num_instances\n    # print(\"to combine\", mask.sum(0))\n    # print(\"combining\", to_combine.astype(int).sum().tolist(), \"many columns\")\n    if not any(to_combine):\n        return arr\n    # mean = np.zeros(arr.shape[0])\n    # for i in to_combine.tolist():\n    #    mean = np.nansum(np.stack(arr[:,i],mean),0)\n    # mean = mean/len(to_combine)\n    mean = np.mean(arr[:, to_combine], 1).reshape(-1, 1)\n    # print(\"mean shape\",mean.shape)\n    dp = np.arange(len(to_combine))[to_combine]\n    # print(\"removing unused columns\")\n    arr = get_subframe(arr, dp)\n    # print(\"subframe shape\",arr.shape)\n    arr = hstack([arr, mean])\n    # print(\"out arr\", arr.shape)\n    # print((mean==0).astype(int).sum())\n    return arr\n\n\ndef importance_votes(arr, to_fit=10, init_weight=None):\n    # arr = combine_underrepresented_columns(matrix,underrepresentation_thresh)\n    filtered_columns = []\n    weighter = None\n    if init_weight is None:\n        weighter = np.ones(arr.shape[1]) / arr.shape[1]  # pd.Series(1.0, index=df.drop(columns=target).columns)\n    else:\n        weighter = init_weight\n    # print(arr.shape)\n    index = np.arange(arr.shape[1])\n    # subtract 1: the last one will always have maximal reduction!\n    bar = trange(to_fit)\n    target = np.ones(arr.shape[0])\n    for i in bar:\n        index = list(filter(lambda x: x not in filtered_columns, index))\n        # 0. produce target column:\n        # print(\"step 0\")\n        target_old = target\n        target = reweight_features(arr, weighter)\n        error = np.mean((target - target_old) ** 2)\n        bar.set_description(f\"expected error: {error}\", refresh=True)\n        if error < 1e-10:\n            break\n        # 1. get a subframe of interesting features\n        # print(\"step 1\")\n        # subframe = get_subframe(arr, filtered_columns)\n        # 2. compute feature importance\n        # print(\"step 2\")\n        # importance_weights=None\n        # importance_weights = compute_feature_importance(arr, target, index)\n        weighter = least_squares_fit(arr, target)\n        # 3. sample column\n        # print(\"step 3\")\n        # new_column = sample_importance_weights(importance_weights[\"importance\"], temperature)\n        # new_column=index[new_column]\n        # value = importance_weights[\"importance\"][new_column]\n        # print(weighter.shape, importance_weights[\"importance\"].shape)\n        # weighter += alpha[i] * importance_weights[\"importance\"].to_numpy()\n        # normalize to maintain the \"1-voter one vote\" total number of votes!\n        # weighter = weighter / sum(abs(weighter))\n        # stepsize = np.mean(abs(importance_weights[\"importance\"].to_numpy()))\n        # bar.set_description(f\"expected stepsize: {stepsize}\", refresh=True)\n        # filtered_columns.append(new_column)\n    # print(\"new weight values\", weighter)\n    return reweight_features(arr, weighter), weighter\n\n\ndef select_ids(arr, pick_frac, minima=(50, 500), folds=50, to_fit=200, frac=0.6):\n    \"\"\"\n    selects the top-\"pick_frac\"% of messages from \"arr\" after merging all\n    users with less than \"minima\" votes (minima increases linearly with each iteration from min to max).\n    The method returns all messages that are within `frac` many \"minima\" selection\n    \"\"\"\n    votes = []\n    minima = np.linspace(*minima, num=folds, dtype=int)\n    num_per_iter = int(arr.shape[0] * pick_frac)\n    writer_num = 0\n    tmp = None\n    for i in trange(folds):\n        tofit = combine_underrepresented_columns(arr, minima[i])\n        if tofit.shape[1] == writer_num:\n            print(\"already tested these writer counts, skipping and using cached value.....\")\n            votes.append(tmp)\n            continue\n        writer_num = tofit.shape[1]\n        # print(\"arr shape\", arr.shape)\n        init_weight = np.ones(tofit.shape[1]) / tofit.shape[1]\n        out, weight = importance_votes(tofit, init_weight=init_weight, to_fit=to_fit)\n        # print(i, \"final weight\")\n        # print(weight)\n        # mask =(out>thresh)\n        # out = np.arange(arr.shape[0])[mask]\n        indices = np.argpartition(out, -num_per_iter)[-num_per_iter:]\n        tmp = np.zeros((arr.shape[0]))\n        tmp[indices] = 1\n        votes.append(tmp)\n        # votes.append(indices.tolist())\n    # print(*[f\"user_id: {users[idx]} {m}\u00b1{s}\" for m, s, idx in zip(weights_mean, weights_std, range(len(weights_mean)))], sep=\"\\n\")\n    out = []\n    votes = np.stack(votes, axis=0)\n    print(\"votespace\", votes.shape)\n    votes = np.mean(votes, 0)\n    for idx, f in enumerate(votes):\n        if f > frac:\n            out.append((idx, f))\n    return out\n\n\nLOG = logging.getLogger(__name__)\n\nif __name__ == \"__main__\":\n    warnings.filterwarnings(\"ignore\", category=FutureWarning)\n    warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n    warnings.simplefilter(\"ignore\")\n    logging.captureWarnings(True)\n    logging.basicConfig(level=logging.ERROR)\n    # Generate some example data\n    # df = make_random_testframe(100_000,5000,0.99)\n    df, message_ids, users = get_df(\"quality\")\n    print(\"combining columns:\")\n    # df = combine_underrepresented_columns(df, 100)\n    weights = np.ones(df.shape[-1])\n    y = reweight_features(df, weights)\n    num_per_iter = int(df.shape[0] * 0.5)\n    naive = np.argpartition(y, -num_per_iter)[-num_per_iter:]\n\n    print(\"after preprocessing\")\n    # print(df)\n    # preproc input\n\n    # Compute feature importances\n    # y = reweight_features(df,np.ones(df.shape[1]))\n    # importance_weights = compute_feature_importance(df, y, list(range(df.shape[1])))\n    # Print the importance weights for each feature\n    # print(importance_weights)\n\n    print(\"STARTING RUN\")\n\n    # sampled_columns = sample_importance_weights(\n    #    importance_weights[\"importance\"],\n    # )\n    # print(\"sampled column\", sampled_columns)\n    # print(\"compute importance votes:\")\n    # weighted_votes, weightings = importance_votes(df)\n    # print(weighted_votes)\n    # print(weightings)\n    with logging_redirect_tqdm():\n        print(\"selected ids\")\n        ids = select_ids(df, 0.5, folds=500)\n\n    #    print(res, frac)\n    conn = psycopg2.connect(\"host=0.0.0.0 port=5432 user=postgres password=postgres dbname=postgres\")\n    # Define the SQL query\n    # , payload#>'{payload, text}' as text\n    query = \"SELECT DISTINCT id as message_id, message_tree_id FROM message;\"\n    print(\"selected\", len(ids), \"messages\")\n    # Read the query results into a Pandas dataframe\n    df = pd.read_sql(query, con=conn)\n    out = []\n    fracs = []\n    in_naive = []\n    for i, frac in ids:\n        res = message_ids[i]\n        out.append((df.loc[df[\"message_id\"] == res]))\n        fracs.append(frac)\n        in_naive.append(i in naive)\n    df = pd.concat(out)\n    df[\"fracs\"] = fracs\n    df[\"in_naive\"] = in_naive\n    print(df.shape)\n    print(\"differences from naive\", len(in_naive) - sum(in_naive))\n    print(df)\n    df.to_csv(\"output.csv\")\n", "scripts/postprocessing/infogain_selector.py": "import numpy as np\nfrom scipy.special import gammaln, psi\nfrom scipy.stats import dirichlet\n\n'''\nLegacy numerical solution.\nShould not be used as it is probably broken\n\n\ndef make_range(*x):\n    \"\"\"\n    constructs leftover values for the simplex given the first k entries\n    (0,x_k) = 1-(x_1+...+x_(k-1))\n    \"\"\"\n    return (0, max(0, 1 - sum(x)))\n\n\ndef relative_entropy(p, q):\n    \"\"\"\n    relative entropy of the two given dirichlet distributions\n    \"\"\"\n\n    def tmp(*x):\n        \"\"\"\n        First adds the last always forced entry to the input (the last x_last = 1-(x_1+...+x_(N)) )\n        Then computes the relative entropy of posterior and prior for that datapoint\n        \"\"\"\n        x_new = np.append(x, 1 - sum(x))\n        return p(x_new) * log2(p(x_new) / q(x_new))\n\n    return tmp\n\n\ndef naive_monte_carlo_integral(fun, dim, samples=10_000_000):\n    s = np.random.rand(dim - 1, samples)\n    s = np.sort(np.concatenate((np.zeros((1, samples)), s, np.ones((1, samples)))), 0)\n    # print(s)\n    pos = np.diff(s, axis=0)\n    # print(pos)\n    res = fun(pos)\n    return np.mean(res)\n\ndef infogain(a_post, a_prior):\n    raise (\n        \"\"\"For the love of good don't use this:\n    it's insanely poorly conditioned, the worst numerical code I have ever written\n    and it's slow as molasses. Use the analytic solution instead.\n\n    Maybe remove\n    \"\"\"\n    )\n    args = len(a_prior)\n    p = dirichlet(a_post).pdf\n    q = dirichlet(a_prior).pdf\n    (info, _) = nquad(relative_entropy(p, q), [make_range for _ in range(args - 1)], opts={\"epsabs\": 1e-8})\n    # info = naive_monte_carlo_integral(relative_entropy(p,q), len(a_post))\n    return info\n'''\n\n\ndef analytic_solution(a_post, a_prior):\n    \"\"\"\n    Analytic solution to the KL-divergence between two dirichlet distributions.\n    Proof is in the Notion design doc.\n    \"\"\"\n    post_sum = np.sum(a_post)\n    prior_sum = np.sum(a_prior)\n    info = (\n        gammaln(post_sum)\n        - gammaln(prior_sum)\n        - np.sum(gammaln(a_post))\n        + np.sum(gammaln(a_prior))\n        - np.sum((a_post - a_prior) * (psi(a_post) - psi(post_sum)))\n    )\n\n    return info\n\n\ndef uniform_expected_infogain(a_prior):\n    mean_weight = dirichlet.mean(a_prior)\n    results = []\n    for i, w in enumerate(mean_weight):\n        a_post = a_prior.copy()\n        a_post[i] = a_post[i] + 1\n        results.append(w * analytic_solution(a_post, a_prior))\n    return np.sum(results)\n\n\nif __name__ == \"__main__\":\n    a_prior = np.array([1, 1, 9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n    a_post = np.array([1, 1, 20, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n\n    print(\"algebraic\", analytic_solution(a_post, a_prior))\n    # print(\"raw\",infogain(a_post, a_prior))\n    print(\"large infogain\", uniform_expected_infogain(a_prior))\n    print(\"post infogain\", uniform_expected_infogain(a_post))\n    # a_prior = np.array([1,1,1000])\n    # print(\"small infogain\",uniform_expected_infogain(a_prior))\n", "scripts/frontend-development/find-missing-locales.py": "import sys\nfrom collections import defaultdict\nfrom glob import glob\nfrom json import load\nfrom os import path\n\nALL_PATH = \"../../website/public/locales/**/*.json\"\nDIR = path.dirname(__file__)\nEN_PATH = \"../../website/public/locales/en/*.json\"\n\n\ndef get_not_translated(en_json, translation_json, parent_key=None):\n    not_translated = []\n    for key in en_json.keys():\n        if key in translation_json and translation_json[key] == en_json[key]:\n            not_translated.append((f\"{parent_key}.{key}\" if parent_key else key))\n        elif isinstance(en_json[key], dict):\n            if key not in translation_json:\n                msg = f\"{parent_key}.{key} (and children)\" if parent_key else \"{key} (and children)\"\n                not_translated.append(msg)\n            else:\n                not_translated.extend(get_not_translated(en_json[key], translation_json[key], key))\n    return not_translated\n\n\ndef get_missing(en_json, translation_json):\n    return [key for key in en_json.keys() if key not in translation_json]\n\n\ndef print_result(missing, not_translated, file):\n    if len(missing):\n        print(f\"[{path.basename(path.dirname(file))}] - {path.basename(file)}\\tmissing: {missing}\")\n    if len(not_translated):\n        print(\n            f\"[{path.basename(path.dirname(file))}] - {path.basename(file)}\\tpotentially untranslated: {not_translated}\"\n        )\n\n\ndef audit(file, en_file):\n    en_json = load(open(en_file, encoding=\"utf-8\"))\n    translation_json = load(open(file, encoding=\"utf-8\"))\n    return (get_missing(en_json, translation_json), get_not_translated(en_json, translation_json), file)\n\n\ndef main():\n    per_language_dict = defaultdict(list)\n    for en_file in glob(path.join(DIR, EN_PATH)):\n        for file in glob(path.join(DIR, ALL_PATH)):\n            if path.basename(en_file) == path.basename(file) and file != en_file:\n                lang = path.basename(path.dirname(file))\n                if len(sys.argv) == 0 or lang in sys.argv:\n                    file_info = audit(file, en_file)\n                    per_language_dict[lang].append(file_info)\n    for results in per_language_dict.values():\n        list(map(lambda args: print_result(*args), results))\n        print()\n\n\nif __name__ == \"__main__\":\n    main()\n", "scripts/discord/verify-lobby.py": "#!/usr/bin/env python3\n\n\"\"\"This file is for moderators to verify new users in the lobby.\n\nFirst, moderators read the brief introduction people write in the lobby.\nIf all people's introductions are acceptable, moderators run this script.\n\nNeeds BOT_TOKEN environment variable to be set to the bot token.\n\n\"\"\"\n\n\nimport discord\nimport pydantic\nimport tqdm.asyncio as tqdm\n\n\nclass Settings(pydantic.BaseSettings):\n    bot_token: str\n\n\nsettings = Settings()\n\nintents = discord.Intents.default()\nintents.message_content = True\nintents.members = True\nclient = discord.Client(intents=intents)\n\n\n@client.event\nasync def on_ready():\n    lobby_channel = discord.utils.get(client.get_all_channels(), name=\"lobby\")\n    # obtain the role object for the verified role\n    verified_role = discord.utils.get(lobby_channel.guild.roles, name=\"verified\")\n    async for message in tqdm.tqdm(lobby_channel.history(limit=None)):\n        if not isinstance(message.author, discord.Member):\n            print(f\"{message.author} is not a member\")\n            continue\n        for role in message.author.roles:\n            if role.name == \"unverified\":\n                print(f\"{message.author} has the unverified role.\")\n                break\n        else:\n            continue\n        # un-assign the unverified role\n        await message.author.remove_roles(role)\n        # assign the verified role\n        await message.author.add_roles(verified_role)\n        print(f\"Assigned verified role to {message.author}\")\n    await client.close()\n\n\nclient.run(settings.bot_token)\n", "scripts/discord/stats.py": "#!/usr/bin/env python3\n\n\"\"\"This file is for moderators to verify new users in the lobby.\n\nFirst, moderators read the brief introduction people write in the lobby.\nIf all people's introductions are acceptable, moderators run this script.\n\nNeeds BOT_TOKEN environment variable to be set to the bot token.\n\n\"\"\"\n\n\nimport discord\nimport pydantic\nimport tqdm.asyncio as tqdm\n\n\nclass Settings(pydantic.BaseSettings):\n    bot_token: str\n\n\nsettings = Settings()\n\nintents = discord.Intents.default()\nintents.message_content = True\nintents.members = True\nclient = discord.Client(intents=intents)\n\n\n@client.event\nasync def on_ready():\n    lobby_channel = discord.utils.get(client.get_all_channels(), name=\"lobby\")\n    message: discord.Message\n    times = []\n    async for message in tqdm.tqdm(lobby_channel.history(limit=None)):\n        times.append(message.created_at.timestamp())\n    with open(\"times.txt\", \"w\") as f:\n        f.write(\"\\n\".join(map(str, times)))\n    await client.close()\n\n\nclient.run(settings.bot_token)\n", "backend/update_message_attributes.py": "import time\n\nfrom loguru import logger\nfrom oasst_backend.models import ApiClient, Message\nfrom oasst_backend.scheduled_tasks import hf_feature_extraction, toxicity\nfrom oasst_backend.utils.database_utils import default_session_factory\nfrom sqlmodel import text\n\n\ndef get_messageids_without_toxicity():\n    message_ids = None\n    with default_session_factory() as session:\n        sql = \"\"\"\n        SELECT m.id FROM message as m\n        left join message_toxicity mt on mt.message_id = m.id\n        where mt.message_id is NULL\n        \"\"\"\n        result = session.execute(\n            text(sql),\n        ).all()\n        message_ids = []\n        for row in result:\n            message_id = row[0]\n            message_ids.append(message_id)\n    return message_ids\n\n\ndef get_messageids_without_embedding():\n    message_ids = None\n    with default_session_factory() as session:\n        sql = \"\"\"\n        SELECT m.id FROM message as m\n        left join message_embedding mt on mt.message_id = m.id\n        where mt.message_id is NULL\n        \"\"\"\n        result = session.execute(\n            text(sql),\n        ).all()\n        message_ids = []\n        for row in result:\n            message_id = row[0]\n            message_ids.append(message_id)\n    return message_ids\n\n\ndef find_and_update_embeddings(message_ids):\n    try:\n        with default_session_factory() as session:\n            for message_id in message_ids:\n                result = session.query(Message).filter(Message.id == message_id).first()\n                if result is not None:\n                    api_client_id = result.api_client_id\n                    text = result.payload.payload.text\n                    api_client = session.query(ApiClient).filter(ApiClient.id == api_client_id).first()\n                    if api_client is not None and text is not None:\n                        hf_feature_extraction(text=text, message_id=message_id, api_client=api_client.__dict__)\n                        # to not get rate limited from HF\n                        time.sleep(10)\n    except Exception as e:\n        logger.error(str(e))\n    logger.debug(\"Done: find_and_update_embeddings\")\n\n\ndef find_and_update_toxicity(message_ids):\n    try:\n        with default_session_factory() as session:\n            for message_id in message_ids:\n                result = session.query(Message).filter(Message.id == message_id).first()\n                if result is not None:\n                    api_client_id = result.api_client_id\n                    text = result.payload.payload.text\n                    api_client = session.query(ApiClient).filter(ApiClient.id == api_client_id).first()\n                    if api_client is not None and text is not None:\n                        toxicity(text=text, message_id=message_id, api_client=api_client.__dict__)\n                        # to not get rate limited from HF\n                        time.sleep(10)\n    except Exception as e:\n        logger.error(str(e))\n    logger.debug(\"Done:  find_and_update_toxicity\")\n\n\ndef main():\n    message_ids = get_messageids_without_toxicity()\n    find_and_update_toxicity(message_ids=message_ids)\n    message_ids = get_messageids_without_embedding()\n    find_and_update_embeddings(message_ids=message_ids)\n    return\n\n\nif __name__ == \"__main__\":\n    main()\n", "backend/rerank.py": "import argparse\nfrom uuid import UUID\n\nimport oasst_backend.utils.database_utils as db_utils\nfrom export import fetch_tree_ids\nfrom loguru import logger\nfrom oasst_backend.api.deps import create_api_client\nfrom oasst_backend.database import engine\nfrom oasst_backend.models.api_client import ApiClient\nfrom oasst_backend.models.message_tree_state import State as TreeState\nfrom oasst_backend.prompt_repository import PromptRepository\nfrom oasst_backend.tree_manager import TreeManager\nfrom sqlmodel import Session\nfrom tqdm import tqdm\n\nIMPORT_API_CLIENT_ID = UUID(\"bd8fde8b-1d8e-4e9a-9966-e96d000f8363\")\n\n\ndef update_tree_ranking(tm: TreeManager, message_tree_id: UUID) -> int:\n    ranking_role_filter = None if tm.cfg.rank_prompter_replies else \"assistant\"\n    rankings_by_message = tm.query_tree_ranking_results(message_tree_id, role_filter=ranking_role_filter)\n    if len(rankings_by_message) == 0:\n        logger.warning(f\"No ranking results found for message tree {message_tree_id}\")\n        return 0\n    num_updated = 0\n    for rankings in rankings_by_message.values():\n        if len(rankings) > 0:\n            num_updated += tm.ranked_pairs_update(rankings)\n    return num_updated\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=\"Update message ranks with feedback received after tree-completion.\")\n    parser.add_argument(\"--commit\", action=\"store_true\", default=False, help=\"Dry run with rollback if not specified\")\n    args = parser.parse_args()\n    return args\n\n\ndef main():\n    args = parse_args()\n\n    dry_run = not args.commit\n\n    @db_utils.managed_tx_function(auto_commit=db_utils.CommitMode.ROLLBACK if dry_run else db_utils.CommitMode.COMMIT)\n    def update_rankings_tx(db: Session, api_client: ApiClient, message_tree_id: UUID) -> int:\n        # create tree manager\n        tm = TreeManager(db, PromptRepository(db, api_client=api_client))\n        return update_tree_ranking(tm, message_tree_id)\n\n    with Session(engine) as db:\n        # get api client\n        api_client = db.query(ApiClient).filter(ApiClient.id == IMPORT_API_CLIENT_ID).first()\n        if not api_client:\n            api_client = create_api_client(\n                session=db,\n                description=\"API client used for importing data\",\n                frontend_type=\"import\",\n                force_id=IMPORT_API_CLIENT_ID,\n            )\n\n        # find all ready for export trees\n        tree_ids = fetch_tree_ids(db, state_filter=TreeState.READY_FOR_EXPORT)\n        num_updated = 0\n\n        for message_tree_id, _ in tqdm(tree_ids):\n            try:\n                num_updated += update_rankings_tx(api_client=api_client, message_tree_id=message_tree_id)\n            except Exception:\n                logger.exception(f\"Update ranking of message tree {message_tree_id} failed\")\n\n    logger.info(f\"Rank of {num_updated} messages updated.\")\n\n    if dry_run:\n        logger.info(\"DRY RUN with rollback (run with --commit to modify db)\")\n\n\nif __name__ == \"__main__\":\n    main()\n", "backend/main.py": "import json\nfrom datetime import datetime\nfrom http import HTTPStatus\nfrom math import ceil\nfrom pathlib import Path\nfrom typing import Optional\n\nimport alembic.command\nimport alembic.config\nimport fastapi\nimport redis.asyncio as redis\nfrom fastapi_limiter import FastAPILimiter\nfrom fastapi_utils.tasks import repeat_every\nfrom loguru import logger\nfrom oasst_backend.api.deps import api_auth, create_api_client\nfrom oasst_backend.api.v1.api import api_router\nfrom oasst_backend.api.v1.utils import prepare_conversation\nfrom oasst_backend.cached_stats_repository import CachedStatsRepository\nfrom oasst_backend.config import settings\nfrom oasst_backend.database import engine\nfrom oasst_backend.models import message_tree_state\nfrom oasst_backend.prompt_repository import PromptRepository, UserRepository\nfrom oasst_backend.task_repository import TaskRepository, delete_expired_tasks\nfrom oasst_backend.tree_manager import TreeManager, halt_prompts_of_disabled_users\nfrom oasst_backend.user_stats_repository import UserStatsRepository, UserStatsTimeFrame\nfrom oasst_backend.utils.database_utils import CommitMode, managed_tx_function\nfrom oasst_shared.exceptions import OasstError, OasstErrorCode\nfrom oasst_shared.schemas import protocol as protocol_schema\nfrom oasst_shared.utils import utcnow\nfrom prometheus_fastapi_instrumentator import Instrumentator\nfrom pydantic import BaseModel\nfrom sqlmodel import Session\nfrom starlette.middleware.cors import CORSMiddleware\n\napp = fastapi.FastAPI(title=settings.PROJECT_NAME, openapi_url=f\"{settings.API_V1_STR}/openapi.json\")\nstartup_time: datetime = utcnow()\n\n\n@app.exception_handler(OasstError)\nasync def oasst_exception_handler(request: fastapi.Request, ex: OasstError):\n    logger.error(f\"{request.method} {request.url} failed: {repr(ex)}\")\n\n    return fastapi.responses.JSONResponse(\n        status_code=int(ex.http_status_code),\n        content=protocol_schema.OasstErrorResponse(\n            message=ex.message,\n            error_code=OasstErrorCode(ex.error_code),\n        ).dict(),\n    )\n\n\n@app.exception_handler(Exception)\nasync def unhandled_exception_handler(request: fastapi.Request, ex: Exception):\n    logger.exception(f\"{request.method} {request.url} failed [UNHANDLED]: {repr(ex)}\")\n    status = HTTPStatus.INTERNAL_SERVER_ERROR\n    return fastapi.responses.JSONResponse(\n        status_code=status.value, content={\"message\": status.name, \"error_code\": OasstErrorCode.GENERIC_ERROR}\n    )\n\n\n# Set all CORS enabled origins\nif settings.BACKEND_CORS_ORIGINS:\n    app.add_middleware(\n        CORSMiddleware,\n        allow_origins=[str(origin) for origin in settings.BACKEND_CORS_ORIGINS],\n        allow_credentials=True,\n        allow_methods=[\"*\"],\n        allow_headers=[\"*\"],\n    )\n\nif settings.UPDATE_ALEMBIC:\n\n    @app.on_event(\"startup\")\n    def alembic_upgrade():\n        logger.info(\"Attempting to upgrade alembic on startup\")\n        try:\n            alembic_ini_path = Path(__file__).parent / \"alembic.ini\"\n            alembic_cfg = alembic.config.Config(str(alembic_ini_path))\n            alembic_cfg.set_main_option(\"sqlalchemy.url\", settings.DATABASE_URI)\n            alembic.command.upgrade(alembic_cfg, \"head\")\n            logger.info(\"Successfully upgraded alembic on startup\")\n        except Exception:\n            logger.exception(\"Alembic upgrade failed on startup\")\n\n\nif settings.OFFICIAL_WEB_API_KEY:\n\n    @app.on_event(\"startup\")\n    def create_official_web_api_client():\n        with Session(engine) as session:\n            try:\n                api_auth(settings.OFFICIAL_WEB_API_KEY, db=session)\n            except OasstError:\n                logger.info(\"Creating official web API client\")\n                create_api_client(\n                    session=session,\n                    api_key=settings.OFFICIAL_WEB_API_KEY,\n                    description=\"The official web client for the OASST backend.\",\n                    frontend_type=\"web\",\n                    trusted=True,\n                )\n\n\nif settings.ENABLE_PROM_METRICS:\n\n    @app.on_event(\"startup\")\n    async def enable_prom_metrics():\n        Instrumentator().instrument(app).expose(app)\n\n\nif settings.RATE_LIMIT:\n\n    @app.on_event(\"startup\")\n    async def connect_redis():\n        async def http_callback(request: fastapi.Request, response: fastapi.Response, pexpire: int):\n            \"\"\"Error callback function when too many requests\"\"\"\n            expire = ceil(pexpire / 1000)\n            raise OasstError(\n                f\"Too Many Requests. Retry After {expire} seconds.\",\n                OasstErrorCode.TOO_MANY_REQUESTS,\n                HTTPStatus.TOO_MANY_REQUESTS,\n            )\n\n        try:\n            redis_client = redis.from_url(\n                f\"redis://{settings.REDIS_HOST}:{settings.REDIS_PORT}/0\", encoding=\"utf-8\", decode_responses=True\n            )\n            logger.info(f\"Connected to {redis_client=}\")\n            await FastAPILimiter.init(redis_client, http_callback=http_callback)\n        except Exception:\n            logger.exception(\"Failed to establish Redis connection\")\n\n\nif settings.DEBUG_USE_SEED_DATA:\n\n    @app.on_event(\"startup\")\n    @managed_tx_function(auto_commit=CommitMode.COMMIT)\n    def create_seed_data(session: Session):\n        class DummyMessage(BaseModel):\n            task_message_id: str\n            user_message_id: str\n            parent_message_id: Optional[str]\n            text: str\n            lang: Optional[str]\n            role: str\n            tree_state: Optional[message_tree_state.State]\n\n        if not settings.OFFICIAL_WEB_API_KEY:\n            raise ValueError(\"Cannot use seed data without OFFICIAL_WEB_API_KEY\")\n\n        try:\n            logger.info(\"Seed data check began\")\n\n            api_client = api_auth(settings.OFFICIAL_WEB_API_KEY, db=session)\n            dummy_user = protocol_schema.User(id=\"__dummy_user__\", display_name=\"Dummy User\", auth_method=\"local\")\n\n            ur = UserRepository(db=session, api_client=api_client)\n            tr = TaskRepository(db=session, api_client=api_client, client_user=dummy_user, user_repository=ur)\n            ur.update_user(tr.user_id, enabled=True, show_on_leaderboard=False, tos_acceptance=True)\n            pr = PromptRepository(\n                db=session, api_client=api_client, client_user=dummy_user, user_repository=ur, task_repository=tr\n            )\n            tm = TreeManager(session, pr)\n\n            with open(settings.DEBUG_USE_SEED_DATA_PATH) as f:\n                dummy_messages_raw = json.load(f)\n\n            dummy_messages = [DummyMessage(**dm) for dm in dummy_messages_raw]\n\n            for msg in dummy_messages:\n                task = tr.fetch_task_by_frontend_message_id(msg.task_message_id)\n                if task and not task.ack:\n                    logger.warning(\"Deleting unacknowledged seed data task\")\n                    session.delete(task)\n                    task = None\n                if not task:\n                    if msg.parent_message_id is None:\n                        task = tr.store_task(\n                            protocol_schema.InitialPromptTask(hint=\"\"), message_tree_id=None, parent_message_id=None\n                        )\n                    else:\n                        parent_message = pr.fetch_message_by_frontend_message_id(\n                            msg.parent_message_id, fail_if_missing=True\n                        )\n                        conversation_messages = pr.fetch_message_conversation(parent_message)\n                        conversation = prepare_conversation(conversation_messages)\n                        if msg.role == \"assistant\":\n                            task = tr.store_task(\n                                protocol_schema.AssistantReplyTask(conversation=conversation),\n                                message_tree_id=parent_message.message_tree_id,\n                                parent_message_id=parent_message.id,\n                            )\n                        else:\n                            task = tr.store_task(\n                                protocol_schema.PrompterReplyTask(conversation=conversation),\n                                message_tree_id=parent_message.message_tree_id,\n                                parent_message_id=parent_message.id,\n                            )\n                    tr.bind_frontend_message_id(task.id, msg.task_message_id)\n                    message = pr.store_text_reply(\n                        msg.text,\n                        msg.lang or \"en\",\n                        msg.task_message_id,\n                        msg.user_message_id,\n                        review_count=5,\n                        review_result=True,\n                        check_tree_state=False,\n                        check_duplicate=False,\n                    )\n                    if message.parent_id is None:\n                        tm._insert_default_state(\n                            root_message_id=message.id,\n                            lang=message.lang,\n                            state=msg.tree_state or message_tree_state.State.GROWING,\n                        )\n                        session.flush()\n\n                    logger.info(\n                        f\"Inserted: message_id: {message.id}, payload: {message.payload.payload}, parent_message_id: {message.parent_id}\"\n                    )\n                else:\n                    logger.debug(f\"seed data task found: {task.id}\")\n\n            logger.info(\"Seed data check completed\")\n\n        except Exception:\n            logger.exception(\"Seed data insertion failed\")\n\n\n@app.on_event(\"startup\")\ndef ensure_tree_states():\n    try:\n        logger.info(\"Startup: TreeManager.ensure_tree_states()\")\n        with Session(engine) as db:\n            api_client = api_auth(settings.OFFICIAL_WEB_API_KEY, db=db)\n            tm = TreeManager(db, PromptRepository(db, api_client=api_client))\n            tm.ensure_tree_states()\n\n    except Exception:\n        logger.exception(\"TreeManager.ensure_tree_states() failed.\")\n\n\n@app.on_event(\"startup\")\n@repeat_every(seconds=60 * settings.USER_STATS_INTERVAL_DAY, wait_first=False)\n@managed_tx_function(auto_commit=CommitMode.COMMIT)\ndef update_leader_board_day(session: Session) -> None:\n    try:\n        usr = UserStatsRepository(session)\n        usr.update_stats(time_frame=UserStatsTimeFrame.day)\n    except Exception:\n        logger.exception(\"Error during leaderboard update (daily)\")\n\n\n@app.on_event(\"startup\")\n@repeat_every(seconds=60 * settings.USER_STATS_INTERVAL_WEEK, wait_first=False)\n@managed_tx_function(auto_commit=CommitMode.COMMIT)\ndef update_leader_board_week(session: Session) -> None:\n    try:\n        usr = UserStatsRepository(session)\n        usr.update_stats(time_frame=UserStatsTimeFrame.week)\n    except Exception:\n        logger.exception(\"Error during user states update (weekly)\")\n\n\n@app.on_event(\"startup\")\n@repeat_every(seconds=60 * settings.USER_STATS_INTERVAL_MONTH, wait_first=False)\n@managed_tx_function(auto_commit=CommitMode.COMMIT)\ndef update_leader_board_month(session: Session) -> None:\n    try:\n        usr = UserStatsRepository(session)\n        usr.update_stats(time_frame=UserStatsTimeFrame.month)\n    except Exception:\n        logger.exception(\"Error during user states update (monthly)\")\n\n\n@app.on_event(\"startup\")\n@repeat_every(seconds=60 * settings.USER_STATS_INTERVAL_TOTAL, wait_first=False)\n@managed_tx_function(auto_commit=CommitMode.COMMIT)\ndef update_leader_board_total(session: Session) -> None:\n    try:\n        usr = UserStatsRepository(session)\n        usr.update_stats(time_frame=UserStatsTimeFrame.total)\n    except Exception:\n        logger.exception(\"Error during user states update (total)\")\n\n\n@app.on_event(\"startup\")\n@repeat_every(seconds=60 * 60)  # 1 hour\n@managed_tx_function(auto_commit=CommitMode.COMMIT)\ndef cronjob_delete_expired_tasks(session: Session) -> None:\n    delete_expired_tasks(session)\n    halt_prompts_of_disabled_users(session)\n\n\n@app.on_event(\"startup\")\n@repeat_every(seconds=60 * settings.CACHED_STATS_UPDATE_INTERVAL, wait_first=True)\n@managed_tx_function(auto_commit=CommitMode.COMMIT)\ndef update_cached_stats(session: Session) -> None:\n    try:\n        csr = CachedStatsRepository(session)\n        csr.update_all_cached_stats()\n    except Exception:\n        logger.exception(\"Error during cached stats update\")\n\n\napp.include_router(api_router, prefix=settings.API_V1_STR)\n\n\ndef get_openapi_schema():\n    return json.dumps(app.openapi())\n\n\ndef retry_scoring_failed_message_trees():\n    try:\n        logger.info(\"TreeManager.retry_scoring_failed_message_trees()\")\n        with Session(engine) as db:\n            api_client = api_auth(settings.OFFICIAL_WEB_API_KEY, db=db)\n\n            pr = PromptRepository(db=db, api_client=api_client)\n            tm = TreeManager(db, pr)\n            tm.retry_scoring_failed_message_trees()\n\n    except Exception:\n        logger.exception(\"TreeManager.retry_scoring_failed_message_trees() failed.\")\n\n\ndef main():\n    # Importing here so we don't import packages unnecessarily if we're\n    # importing main as a module.\n    import argparse\n\n    import uvicorn\n\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument(\n        \"--print-openapi-schema\",\n        default=False,\n        help=\"Dumps the openapi schema to stdout\",\n        action=\"store_true\",\n    )\n    parser.add_argument(\"--host\", help=\"The host to run the server\", default=\"0.0.0.0\")\n    parser.add_argument(\"--port\", help=\"The port to run the server\", default=8080)\n    parser.add_argument(\n        \"--retry-scoring\",\n        default=False,\n        help=\"Retry scoring failed message trees\",\n        action=\"store_true\",\n    )\n\n    args = parser.parse_args()\n\n    if args.print_openapi_schema:\n        print(get_openapi_schema())\n\n    if args.retry_scoring:\n        retry_scoring_failed_message_trees()\n\n    if not (args.print_openapi_schema or args.retry_scoring):\n        uvicorn.run(app, host=args.host, port=args.port)\n\n\nif __name__ == \"__main__\":\n    main()\n", "backend/import.py": "import argparse\nimport json\nimport sys\nfrom pathlib import Path\nfrom typing import Optional\nfrom uuid import UUID\n\nimport oasst_backend.models.db_payload as db_payload\nimport oasst_backend.utils.database_utils as db_utils\nimport pydantic\nfrom loguru import logger\nfrom oasst_backend.api.deps import create_api_client\nfrom oasst_backend.models import ApiClient, Message\nfrom oasst_backend.models.message_tree_state import MessageTreeState\nfrom oasst_backend.models.message_tree_state import State as TreeState\nfrom oasst_backend.models.payload_column_type import PayloadContainer\nfrom oasst_backend.prompt_repository import PromptRepository\nfrom oasst_backend.user_repository import UserRepository\nfrom oasst_data import ExportMessageNode, ExportMessageTree\nfrom sqlmodel import Session\n\n# well known id\nIMPORT_API_CLIENT_ID = UUID(\"bd8fde8b-1d8e-4e9a-9966-e96d000f8363\")\n\n\nclass Importer:\n    def __init__(self, db: Session, origin: str, model_name: Optional[str] = None):\n        self.db = db\n        self.origin = origin\n        self.model_name = model_name\n\n        # get import api client\n        api_client = db.query(ApiClient).filter(ApiClient.id == IMPORT_API_CLIENT_ID).first()\n        if not api_client:\n            api_client = create_api_client(\n                session=db,\n                description=\"API client used for importing data\",\n                frontend_type=\"import\",\n                force_id=IMPORT_API_CLIENT_ID,\n            )\n\n        ur = UserRepository(db, api_client)\n        self.import_user = ur.lookup_system_user(username=\"import\")\n        self.pr = PromptRepository(db=db, api_client=api_client, user_repository=ur)\n        self.api_client = api_client\n\n    def fetch_message(self, message_id: UUID) -> Message:\n        return self.db.query(Message).filter(Message.id == message_id).one_or_none()\n\n    def fetch_message_tree_state(self, message_tree_id: UUID) -> MessageTreeState:\n        return self.db.query(MessageTreeState).filter(MessageTreeState.message_tree_id == message_tree_id).one_or_none()\n\n    def import_message(\n        self, message: ExportMessageNode, message_tree_id: UUID, parent_id: Optional[UUID] = None\n    ) -> Message:\n        payload = db_payload.MessagePayload(text=message.text)\n        msg = Message(\n            id=message.message_id,\n            message_tree_id=message_tree_id,\n            frontend_message_id=message.message_id,\n            parent_id=parent_id,\n            review_count=message.review_count or 0,\n            lang=message.lang or \"en\",\n            review_result=True,\n            synthetic=message.synthetic if message.synthetic is not None else True,\n            model_name=message.model_name or self.model_name,\n            role=message.role,\n            api_client_id=self.api_client.id,\n            payload_type=type(payload).__name__,\n            payload=PayloadContainer(payload=payload),\n            user_id=self.import_user.id,\n        )\n        self.db.add(msg)\n        if message.replies:\n            for r in message.replies:\n                self.import_message(r, message_tree_id=message_tree_id, parent_id=msg.id)\n        self.db.flush()\n        if parent_id is None:\n            self.pr.update_children_counts(msg.id)\n        self.db.refresh(msg)\n        return msg\n\n    def import_tree(\n        self, tree: ExportMessageTree, state: TreeState = TreeState.BACKLOG_RANKING\n    ) -> tuple[MessageTreeState, Message]:\n        assert tree.message_tree_id is not None and tree.message_tree_id == tree.prompt.message_id\n        root_msg = self.import_message(tree.prompt, message_tree_id=tree.prompt.message_id)\n        assert state == TreeState.BACKLOG_RANKING or state == TreeState.RANKING, f\"{state} not supported for import\"\n        active = state == TreeState.RANKING\n        mts = MessageTreeState(\n            message_tree_id=root_msg.id,\n            goal_tree_size=0,\n            max_depth=0,\n            max_children_count=0,\n            state=state,\n            origin=self.origin,\n            active=active,\n            lang=root_msg.lang or \"en\",\n        )\n        self.db.add(mts)\n        return mts, root_msg\n\n\ndef import_file(\n    input_file_path: Path,\n    origin: str,\n    *,\n    model_name: Optional[str] = None,\n    num_activate: int = 0,\n    max_count: Optional[int] = None,\n    dry_run: bool = False,\n) -> int:\n    @db_utils.managed_tx_function(auto_commit=db_utils.CommitMode.ROLLBACK if dry_run else db_utils.CommitMode.COMMIT)\n    def import_tx(db: Session) -> int:\n        importer = Importer(db, origin=origin, model_name=model_name)\n        i = 0\n        with input_file_path.open() as file_in:\n            # read line tree object\n            for line in file_in:\n                dict_node = json.loads(line)\n\n                # validate data\n                if dict_node.get(\"message_tree_id\"):  # tree\n                    tree: ExportMessageTree = pydantic.parse_obj_as(ExportMessageTree, dict_node)\n                    existing_mts = importer.fetch_message_tree_state(tree.message_tree_id)\n                    if existing_mts:\n                        logger.info(f\"Skipping existing message tree: {tree.message_tree_id}\")\n                    else:\n                        state = TreeState.BACKLOG_RANKING if i >= num_activate else TreeState.RANKING\n                        mts, root_msg = importer.import_tree(tree, state=state)\n                        i += 1\n                        logger.info(\n                            f\"imported tree: {mts.message_tree_id}, {mts.state=}, {mts.active=}, {root_msg.children_count=}\"\n                        )\n\n                    if max_count and i >= max_count:\n                        logger.info(f\"Reached max count {max_count} of trees to import.\")\n                        break\n                elif dict_node.get(\"message_id\"):  # message\n                    message: ExportMessageNode = pydantic.parse_obj_as(ExportMessageNode, dict_node)\n                    existing_msg = importer.fetch_message(message.message_id)\n                    if existing_msg:\n                        logger.info(f\"Skipping existing message: {message.message_id}\")\n                    else:\n                        msg = importer.import_message(message, message_tree_id=message.message_id)\n                        i += 1\n                        logger.info(f\"imported message: {msg.id}\")\n        return i\n\n    if dry_run:\n        logger.info(\"DRY RUN with rollback\")\n    return import_tx()\n\n\ndef parse_args():\n    def str2bool(v):\n        if isinstance(v, bool):\n            return v\n        if v.lower() in (\"yes\", \"true\", \"t\", \"y\", \"1\"):\n            return True\n        elif v.lower() in (\"no\", \"false\", \"f\", \"n\", \"0\"):\n            return False\n        else:\n            raise argparse.ArgumentTypeError(\"Boolean value expected.\")\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"input_file_path\",\n        help=\"Input file path\",\n    )\n    parser.add_argument(\"--origin\", type=str, default=None, help=\"Value for origin of message trees\")\n    parser.add_argument(\"--model_name\", type=str, default=None, help=\"Default name of model (if missing in messages)\")\n    parser.add_argument(\"--num_activate\", type=int, default=0, help=\"Number of trees to add in ranking state\")\n    parser.add_argument(\"--max_count\", type=int, default=None, help=\"Maximum number of message trees to import\")\n    parser.add_argument(\"--dry_run\", type=str2bool, default=False)\n    args = parser.parse_args()\n    return args\n\n\ndef main():\n    args = parse_args()\n\n    input_file_path = Path(args.input_file_path)\n    if not input_file_path.exists() or not input_file_path.is_file():\n        print(\"Invalid input file:\", args.input_file_path)\n        sys.exit(1)\n\n    dry_run = args.dry_run\n    num_imported = import_file(\n        input_file_path,\n        origin=args.origin or input_file_path.name,\n        model_name=args.model_name,\n        num_activate=args.num_activate,\n        max_count=args.max_count,\n        dry_run=dry_run,\n    )\n\n    logger.info(f\"Done ({num_imported=}, {dry_run=})\")\n\n\nif __name__ == \"__main__\":\n    main()\n", "backend/export.py": "import argparse\nfrom pathlib import Path\nfrom typing import List, Optional\nfrom uuid import UUID\n\nimport sqlalchemy as sa\nfrom loguru import logger\nfrom oasst_backend.database import engine\nfrom oasst_backend.models import Message, MessageEmoji, MessageReaction, MessageTreeState, TextLabels, db_payload\nfrom oasst_backend.models.message_tree_state import State as TreeState\nfrom oasst_backend.utils import tree_export\nfrom oasst_data import (\n    ExportMessageEvent,\n    ExportMessageEventEmoji,\n    ExportMessageEventRanking,\n    ExportMessageEventRating,\n    ExportMessageTree,\n    LabelAvgValue,\n    LabelValues,\n)\nfrom oasst_shared.schemas.protocol import TextLabel\nfrom oasst_shared.utils import Anonymizer\nfrom sqlmodel import Session, func\n\n\ndef fetch_tree_ids(\n    db: Session,\n    state_filter: Optional[TreeState] = None,\n    lang: Optional[str] = None,\n    synthetic: Optional[bool] = None,\n    limit: Optional[int] = None,\n) -> list[tuple[UUID, TreeState]]:\n    tree_qry = (\n        db.query(MessageTreeState)\n        .select_from(MessageTreeState)\n        .join(Message, MessageTreeState.message_tree_id == Message.id)\n    )\n\n    if lang is not None:\n        tree_qry = tree_qry.filter(Message.lang == lang)\n\n    if state_filter:\n        tree_qry = tree_qry.filter(MessageTreeState.state == state_filter)\n\n    if synthetic is not None:\n        synth_exists_qry = (\n            db.query()\n            .filter(Message.message_tree_id == MessageTreeState.message_tree_id, Message.synthetic)\n            .exists()\n            .correlate(MessageTreeState)\n        )\n        if synthetic is False:\n            synth_exists_qry = ~synth_exists_qry\n        tree_qry = tree_qry.filter(synth_exists_qry)\n\n    if limit is not None:\n        tree_qry = tree_qry.limit(limit)\n\n    return [(tree.message_tree_id, tree.state) for tree in tree_qry]\n\n\ndef fetch_tree_messages(\n    db: Session,\n    message_tree_id: Optional[UUID] = None,\n    user_id: Optional[UUID] = None,\n    deleted: Optional[bool] = None,\n    synthetic: Optional[bool] = False,\n    prompts_only: bool = False,\n    lang: Optional[str] = None,\n    review_result: Optional[bool] = None,\n    limit: Optional[int] = None,\n) -> List[Message]:\n    qry = db.query(Message)\n\n    if message_tree_id:\n        qry = qry.filter(Message.message_tree_id == message_tree_id)\n    if user_id:\n        qry = qry.filter(Message.user_id == user_id)\n    if deleted is not None:\n        qry = qry.filter(Message.deleted == deleted)\n    if synthetic is not None:\n        qry = qry.filter(Message.synthetic == synthetic)\n    if prompts_only:\n        qry = qry.filter(Message.parent_id.is_(None))\n    if lang:\n        qry = qry.filter(Message.lang == lang)\n    if review_result is not None:\n        qry = qry.filter(Message.review_result == review_result)\n    if limit is not None:\n        qry = qry.limit(limit)\n\n    return qry.all()\n\n\ndef get_events_for_messages(db: Session, message_ids: list[UUID]) -> dict[UUID, ExportMessageEvent]:\n    events = {}\n    emojis = db.query(MessageEmoji).filter(MessageEmoji.message_id.in_(message_ids)).all()\n    for emoji in emojis:\n        event = ExportMessageEventEmoji(user_id=str(emoji.user_id), emoji=emoji.emoji)\n        events.setdefault(emoji.message_id, {}).setdefault(\"emoji\", []).append(event)\n    reactions: list[MessageReaction] = (\n        db.query(MessageReaction).filter(MessageReaction.message_id.in_(message_ids)).all()\n    )\n    for reaction in reactions:\n        match reaction.payload_type:\n            case \"RatingReactionPayload\":\n                key = \"rating\"\n                payload: db_payload.RatingReactionPayload = reaction.payload.payload\n                event = ExportMessageEventRating(user_id=str(reaction.user_id), rating=payload.rating)\n            case \"RankingReactionPayload\":\n                key = \"ranking\"\n                payload: db_payload.RankingReactionPayload = reaction.payload.payload\n                event = ExportMessageEventRanking(\n                    user_id=str(reaction.user_id),\n                    ranking=payload.ranking,\n                    ranked_message_ids=[str(id) for id in payload.ranked_message_ids],\n                    ranking_parent_id=str(payload.ranking_parent_id) if payload.ranking_parent_id else None,\n                    message_tree_id=str(payload.message_tree_id) if payload.message_tree_id else None,\n                    not_rankable=payload.not_rankable if payload.not_rankable else None,\n                )\n            case _:\n                raise ValueError(f\"Unknown payload type {reaction.payload_type}\")\n        events.setdefault(reaction.message_id, {}).setdefault(key, []).append(event)\n\n    return events\n\n\ndef fetch_tree_messages_and_avg_labels(\n    db: Session,\n    message_tree_id: Optional[UUID] = None,\n    user_id: Optional[UUID] = None,\n    deleted: Optional[bool] = None,\n    synthetic: Optional[bool] = False,\n    prompts_only: bool = False,\n    lang: Optional[str] = None,\n    review_result: Optional[bool] = None,\n    limit: Optional[int] = None,\n) -> List[Message]:\n    args = [Message]\n\n    for l in TextLabel:\n        args.append(func.avg(TextLabels.labels[l].cast(sa.Float)).label(l.value))\n        args.append(func.count(TextLabels.labels[l]).label(l.value + \"_count\"))\n\n    qry = db.query(*args).select_from(Message).outerjoin(TextLabels, Message.id == TextLabels.message_id)\n    if message_tree_id:\n        qry = qry.filter(Message.message_tree_id == message_tree_id)\n    if user_id:\n        qry = qry.filter(Message.user_id == user_id)\n    if deleted is not None:\n        qry = qry.filter(Message.deleted == deleted)\n    if synthetic is not None:\n        qry = qry.filter(Message.synthetic == synthetic)\n    if prompts_only:\n        qry = qry.filter(Message.parent_id.is_(None))\n    if lang:\n        qry = qry.filter(Message.lang == lang)\n    if review_result is not None:\n        qry = qry.filter(Message.review_result == review_result)\n\n    qry = qry.group_by(Message.id)\n\n    if limit is not None:\n        qry = qry.limit(limit)\n\n    return qry.all()\n\n\ndef export_trees(\n    db: Session,\n    export_file: Optional[Path] = None,\n    use_compression: bool = False,\n    deleted: Optional[bool] = False,\n    synthetic: Optional[bool] = False,\n    user_id: Optional[UUID] = None,\n    prompts_only: bool = False,\n    state_filter: Optional[TreeState] = None,\n    lang: Optional[str] = None,\n    review_result: Optional[bool] = None,\n    export_labels: bool = False,\n    export_events: bool = False,\n    limit: Optional[int] = None,\n    anonymizer_seed: Optional[str] = None,\n) -> None:\n    message_labels: dict[UUID, LabelValues] = {}\n    anonymizer = Anonymizer(anonymizer_seed) if anonymizer_seed else None\n    if user_id:\n        # when filtering by user we don't have complete message trees, export as list\n        result = fetch_tree_messages_and_avg_labels(\n            db,\n            user_id=user_id,\n            deleted=deleted,\n            synthetic=synthetic,\n            prompts_only=prompts_only,\n            lang=lang,\n            review_result=review_result,\n            limit=limit,\n        )\n\n        messages: list[Message] = []\n        for r in result:\n            msg = r[\"Message\"]\n            messages.append(msg)\n            if export_labels:\n                labels: LabelValues = {\n                    l.value: LabelAvgValue(value=r[l.value], count=r[l.value + \"_count\"])\n                    for l in TextLabel\n                    if r[l.value] is not None\n                }\n                message_labels[msg.id] = labels\n\n        events = {}\n        if export_events:\n            events = get_events_for_messages(db, [msg.id for msg in messages])\n\n        tree_export.write_messages_to_file(\n            export_file,\n            messages,\n            use_compression,\n            labels=message_labels,\n            anonymizer=anonymizer,\n            events=events,\n        )\n    else:\n        # tree export mode\n        message_tree_ids = fetch_tree_ids(db, state_filter, lang=lang, limit=limit, synthetic=synthetic)\n\n        message_trees: list[list[Message]] = []\n\n        for tree_id, _ in message_tree_ids:\n            if export_labels:\n                result = fetch_tree_messages_and_avg_labels(\n                    db,\n                    message_tree_id=tree_id,\n                    deleted=deleted,\n                    synthetic=None,  # pass None here (export trees, filtering happened in fetch_tree_ids)\n                    prompts_only=prompts_only,\n                    lang=None,  # pass None, trees were selected based on lang of prompt\n                    review_result=review_result,\n                )\n\n                messages: list[Message] = []\n                for r in result:\n                    msg = r[\"Message\"]\n                    messages.append(msg)\n                    labels: LabelValues = {\n                        l.value: LabelAvgValue(value=r[l.value], count=r[l.value + \"_count\"])\n                        for l in TextLabel\n                        if r[l.value] is not None\n                    }\n                    message_labels[msg.id] = labels\n\n                message_trees.append(messages)\n            else:\n                messages = fetch_tree_messages(\n                    db,\n                    message_tree_id=tree_id,\n                    deleted=deleted,\n                    synthetic=None,  # pass None here (export trees, filtering happened in fetch_tree_ids)\n                    prompts_only=prompts_only,\n                    lang=None,  # pass None here, trees were selected based on lang of prompt\n                    review_result=review_result,\n                )\n                message_trees.append(messages)\n\n        if review_result is False or deleted is True or synthetic is True:\n            # when exporting filtered we don't have complete message trees, export as list\n            messages = [m for t in message_trees for m in t]  # flatten message list\n            events = {}\n            if export_events:\n                events = get_events_for_messages(db, [msg.id for msg in messages])\n            tree_export.write_messages_to_file(\n                export_file,\n                messages,\n                use_compression,\n                labels=message_labels,\n                anonymizer=anonymizer,\n                events=events,\n            )\n        else:\n            trees_to_export: List[ExportMessageTree] = []\n\n            for (message_tree_id, message_tree_state), message_tree in zip(message_tree_ids, message_trees):\n                if len(message_tree) > 0:\n                    events = {}\n                    if export_events:\n                        events = get_events_for_messages(db, [msg.id for msg in message_tree])\n                    try:\n                        t = tree_export.build_export_tree(\n                            message_tree_id=message_tree_id,\n                            message_tree_state=message_tree_state,\n                            messages=message_tree,\n                            labels=message_labels,\n                            anonymizer=anonymizer,\n                            events=events,\n                        )\n                        if prompts_only:\n                            t.prompt.replies = None\n                        trees_to_export.append(t)\n                    except Exception as ex:\n                        logger.warning(f\"Corrupted tree: {message_tree_id} ({ex})\")\n\n            tree_export.write_trees_to_file(export_file, trees_to_export, use_compression)\n\n\ndef validate_args(args):\n    if args.deleted_only:\n        args.include_deleted = True\n\n    args.use_compression = args.export_file is not None and \".gz\" in args.export_file\n\n    if args.state and args.user is not None:\n        raise ValueError(\"Cannot use --state when specifying a user ID\")\n\n    if args.export_file is None:\n        logger.warning(\"No export file provided, output will be sent to STDOUT\")\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument(\n        \"--export-file\",\n        type=str,\n        help=\"Name of file to export trees to. If not provided, output will be sent to STDOUT\",\n    )\n    parser.add_argument(\n        \"--include-deleted\",\n        action=\"store_true\",\n        help=\"Include deleted messages in export\",\n    )\n    parser.add_argument(\n        \"--deleted-only\",\n        action=\"store_true\",\n        help=\"Export only deleted messages (implies --include-deleted)\",\n    )\n    parser.add_argument(\n        \"--include-spam\",\n        action=\"store_true\",\n        help=\"Export including messages with no review or negative review result.\",\n    )\n    parser.add_argument(\n        \"--spam-only\",\n        action=\"store_true\",\n        help=\"Export only messages with negative review result (implies --include-spam).\",\n    )\n    parser.add_argument(\n        \"--include-synthetic\",\n        action=\"store_true\",\n        help=\"Include synthetic messages in export\",\n    )\n    parser.add_argument(\n        \"--synthetic-only\",\n        action=\"store_true\",\n        help=\"Export only synthetic messages (implies --include-synth)\",\n    )\n    parser.add_argument(\n        \"--user\",\n        type=str,\n        help=\"Only export trees involving the user with the specified ID. Incompatible with --state.\",\n    )\n    parser.add_argument(\n        \"--state\",\n        type=str,\n        help=\"all|prompt_lottery_waiting|growing|ready_for_export|aborted_low_grade|halted_by_moderator|backlog_ranking\",\n    )\n    parser.add_argument(\n        \"--lang\",\n        type=str,\n        help=\"Filter message trees by language code (BCP 47)\",\n    )\n    parser.add_argument(\n        \"--prompts-only\",\n        action=\"store_true\",\n        help=\"Export a list of initial prompt messages\",\n    )\n    parser.add_argument(\n        \"--export-labels\",\n        action=\"store_true\",\n        help=\"Include average label values for messages\",\n    )\n    parser.add_argument(\n        \"--export-events\",\n        action=\"store_true\",\n        help=\"Include events for messages\",\n    )\n    parser.add_argument(\n        \"--limit\",\n        type=int,\n        help=\"Maximum number of trees to export. Leave at `None` to export all trees.\",\n    )\n    parser.add_argument(\n        \"--anonymizer-seed\",\n        type=int,\n        help=\"Seed for the anonymizer. If not specified, no anonymization will be performed.\",\n    )\n\n    args = parser.parse_args()\n    return args\n\n\ndef main():\n    args = parse_args()\n    validate_args(args)\n\n    state_filter: Optional[TreeState] = None\n    if args.state is None:\n        state_filter = TreeState.READY_FOR_EXPORT\n    elif args.state != \"all\":\n        state_filter = TreeState(args.state)\n\n    deleted: Optional[bool] = False\n    if args.include_deleted:\n        deleted = None\n    if args.deleted_only:\n        deleted = True\n\n    review_result: Optional[bool] = True\n    if args.include_spam:\n        review_result = None\n    if args.spam_only:\n        review_result = False\n\n    synthetic: Optional[bool] = False\n    if args.include_synthetic:\n        synthetic = None\n    if args.synthetic_only:\n        synthetic = True\n\n    if args.anonymizer_seed is None:\n        logger.warning(\"No anonymizer seed provided, no anonymization will be performed.\")\n\n    with Session(engine) as db:\n        export_trees(\n            db,\n            Path(args.export_file) if args.export_file is not None else None,\n            use_compression=args.use_compression,\n            deleted=deleted,\n            synthetic=synthetic,\n            user_id=UUID(args.user) if args.user is not None else None,\n            prompts_only=args.prompts_only,\n            state_filter=state_filter,\n            lang=args.lang,\n            review_result=review_result,\n            export_labels=args.export_labels,\n            export_events=args.export_events,\n            limit=args.limit,\n            anonymizer_seed=args.anonymizer_seed,\n        )\n\n\nif __name__ == \"__main__\":\n    main()\n", "backend/alembic/env.py": "from logging.config import fileConfig\n\nimport sqlmodel\nfrom alembic import context\nfrom oasst_backend import models  # noqa: F401\nfrom sqlalchemy import engine_from_config, pool\n\n# this is the Alembic Config object, which provides\n# access to the values within the .ini file in use.\nconfig = context.config\n\n# Interpret the config file for Python logging.\n# This line sets up loggers basically.\nif config.config_file_name is not None:\n    fileConfig(config.config_file_name)\n\n# add your model's MetaData object here\n# for 'autogenerate' support\n# from myapp import mymodel\n# target_metadata = mymodel.Base.metadata\ntarget_metadata = sqlmodel.SQLModel.metadata\n\n# other values from the config, defined by the needs of env.py,\n# can be acquired:\n# my_important_option = config.get_main_option(\"my_important_option\")\n# ... etc.\n\n\ndef run_migrations_offline() -> None:\n    \"\"\"Run migrations in 'offline' mode.\n\n    This configures the context with just a URL\n    and not an Engine, though an Engine is acceptable\n    here as well.  By skipping the Engine creation\n    we don't even need a DBAPI to be available.\n\n    Calls to context.execute() here emit the given string to the\n    script output.\n\n    \"\"\"\n    url = config.get_main_option(\"sqlalchemy.url\")\n    context.configure(\n        url=url,\n        target_metadata=target_metadata,\n        literal_binds=True,\n        dialect_opts={\"paramstyle\": \"named\"},\n    )\n\n    with context.begin_transaction():\n        context.run_migrations()\n\n\ndef run_migrations_online() -> None:\n    \"\"\"Run migrations in 'online' mode.\n\n    In this scenario we need to create an Engine\n    and associate a connection with the context.\n\n    \"\"\"\n    connectable = engine_from_config(\n        config.get_section(config.config_ini_section),\n        prefix=\"sqlalchemy.\",\n        poolclass=pool.NullPool,\n    )\n\n    with connectable.connect() as connection:\n        context.configure(connection=connection, target_metadata=target_metadata)\n\n        with context.begin_transaction():\n            context.get_context()._ensure_version_table()\n            connection.execute(\"LOCK TABLE alembic_version IN ACCESS EXCLUSIVE MODE\")\n            context.run_migrations()\n\n\nif context.is_offline_mode():\n    run_migrations_offline()\nelse:\n    run_migrations_online()\n", "backend/alembic/versions/2023_01_08_2208-92a367bb9f40_restructure_message_tree_state_table.py": "\"\"\"restructure message_tree_state table\n\nRevision ID: 92a367bb9f40\nRevises: ba61fe17fb6e\nCreate Date: 2023-01-08 22:08:46.458195\n\n\"\"\"\nimport sqlalchemy as sa\nimport sqlmodel\nfrom alembic import op\nfrom sqlalchemy.dialects import postgresql\n\n# revision identifiers, used by Alembic.\nrevision = \"92a367bb9f40\"\ndown_revision = \"aac6b2f66006\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_table(\"message_tree_state\")\n    op.create_table(\n        \"message_tree_state\",\n        sa.Column(\"message_tree_id\", postgresql.UUID(as_uuid=True), nullable=False),\n        sa.Column(\"goal_tree_size\", sa.Integer(), nullable=False),\n        sa.Column(\"max_depth\", sa.Integer(), nullable=False),\n        sa.Column(\"max_children_count\", sa.Integer(), nullable=False),\n        sa.Column(\"state\", sqlmodel.sql.sqltypes.AutoString(length=128), nullable=False),\n        sa.Column(\"active\", sa.Boolean(), nullable=False),\n        sa.Column(\"accepted_messages\", sa.Integer(), nullable=False),\n        sa.ForeignKeyConstraint(\n            [\"message_tree_id\"],\n            [\"message.id\"],\n        ),\n        sa.PrimaryKeyConstraint(\"message_tree_id\"),\n    )\n    op.create_index(op.f(\"ix_message_tree_state_active\"), \"message_tree_state\", [\"active\"], unique=False)\n    op.create_index(op.f(\"ix_message_tree_state_state\"), \"message_tree_state\", [\"state\"], unique=False)\n\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_index(op.f(\"ix_message_tree_state_state\"), table_name=\"message_tree_state\")\n    op.drop_index(op.f(\"ix_message_tree_state_active\"), table_name=\"message_tree_state\")\n    op.drop_table(\"message_tree_state\")\n    op.create_table(\n        \"message_tree_state\",\n        sa.Column(\"id\", postgresql.UUID(as_uuid=True), server_default=sa.text(\"gen_random_uuid()\"), nullable=False),\n        sa.Column(\"message_tree_id\", sqlmodel.sql.sqltypes.GUID(), nullable=False),\n        sa.Column(\"state\", sqlmodel.sql.sqltypes.AutoString(length=128), nullable=False),\n        sa.Column(\"goal_tree_size\", sa.Integer(), nullable=False),\n        sa.Column(\"current_num_non_filtered_messages\", sa.Integer(), nullable=False),\n        sa.Column(\"max_depth\", sa.Integer(), nullable=False),\n        sa.PrimaryKeyConstraint(\"id\"),\n    )\n    op.create_index(\n        op.f(\"ix_message_tree_state_message_tree_id\"), \"message_tree_state\", [\"message_tree_id\"], unique=False\n    )\n    op.create_index(\"ix_message_tree_state_tree_id\", \"message_tree_state\", [\"message_tree_id\"], unique=True)\n    # ### end Alembic commands ###\n", "backend/alembic/versions/2023_01_15_1654-0964ac95170d_add_rank_and_indices_to_user_stats.py": "\"\"\"add rank and indices to user_stats\n\nRevision ID: 0964ac95170d\nRevises: 423557e869e4\nCreate Date: 2023-01-15 16:54:09.510018\n\n\"\"\"\nimport sqlalchemy as sa\nfrom alembic import op\n\n# revision identifiers, used by Alembic.\nrevision = \"0964ac95170d\"\ndown_revision = \"423557e869e4\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.add_column(\"user_stats\", sa.Column(\"rank\", sa.Integer(), nullable=True))\n    op.create_index(\n        \"ix_user_stats__timeframe__rank__user_id\", \"user_stats\", [\"time_frame\", \"rank\", \"user_id\"], unique=True\n    )\n    op.create_index(\"ix_user_stats__timeframe__user_id\", \"user_stats\", [\"time_frame\", \"user_id\"], unique=True)\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_index(\"ix_user_stats__timeframe__user_id\", table_name=\"user_stats\")\n    op.drop_index(\"ix_user_stats__timeframe__rank__user_id\", table_name=\"user_stats\")\n    op.drop_column(\"user_stats\", \"rank\")\n    # ### end Alembic commands ###\n", "backend/alembic/versions/2023_02_11_1030-ba40d055714a_add_cached_stats.py": "\"\"\"add cached_stats\n\nRevision ID: ba40d055714a\nRevises: caee1e8ee0bc\nCreate Date: 2023-02-11 10:30:21.996198\n\n\"\"\"\nimport sqlalchemy as sa\nimport sqlmodel\nfrom alembic import op\nfrom sqlalchemy.dialects import postgresql\n\n# revision identifiers, used by Alembic.\nrevision = \"ba40d055714a\"\ndown_revision = \"caee1e8ee0bc\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.create_table(\n        \"cached_stats\",\n        sa.Column(\"name\", sqlmodel.sql.sqltypes.AutoString(length=128), nullable=False),\n        sa.Column(\n            \"modified_date\", sa.DateTime(timezone=True), server_default=sa.text(\"CURRENT_TIMESTAMP\"), nullable=False\n        ),\n        sa.Column(\"stats\", postgresql.JSONB(astext_type=sa.Text()), nullable=False),\n        sa.PrimaryKeyConstraint(\"name\"),\n    )\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_table(\"cached_stats\")\n    # ### end Alembic commands ###\n", "backend/alembic/versions/2023_02_14_1756-165b55de5a94_add_text_labels_message_id_index.py": "\"\"\"add text_labels message_id index\n\nRevision ID: 165b55de5a94\nRevises: ba40d055714a\nCreate Date: 2023-02-14 17:56:48.263684\n\n\"\"\"\nfrom alembic import op\n\n# revision identifiers, used by Alembic.\nrevision = \"165b55de5a94\"\ndown_revision = \"ba40d055714a\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.create_index(op.f(\"ix_text_labels_message_id\"), \"text_labels\", [\"message_id\"], unique=False)\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_index(op.f(\"ix_text_labels_message_id\"), table_name=\"text_labels\")\n    # ### end Alembic commands ###\n", "backend/alembic/versions/2022_12_30_0109-73ce3675c1f5_add_field_trusted_api_client.py": "\"\"\"add field trusted api client\n\nRevision ID: 73ce3675c1f5\nRevises: 464ec4667aae\nCreate Date: 2022-12-30 01:09:06.446020\n\n\"\"\"\nimport sqlalchemy as sa\nfrom alembic import op\n\n# revision identifiers, used by Alembic.\nrevision = \"73ce3675c1f5\"\ndown_revision = \"464ec4667aae\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.add_column(\"api_client\", sa.Column(\"trusted\", sa.Boolean(), server_default=sa.text(\"false\"), nullable=False))\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_column(\"api_client\", \"trusted\")\n    # ### end Alembic commands ###\n", "backend/alembic/versions/2023_02_07_1922-caee1e8ee0bc_added_new_table_for_flagged_messages.py": "\"\"\"Added new table for flagged messages\n\nRevision ID: caee1e8ee0bc\nRevises: 8c8241d1f973\nCreate Date: 2023-02-07 19:22:12.696257\n\n\"\"\"\nimport sqlalchemy as sa\nfrom alembic import op\nfrom sqlalchemy.dialects import postgresql\n\n# revision identifiers, used by Alembic.\nrevision = \"caee1e8ee0bc\"\ndown_revision = \"8c8241d1f973\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.create_table(\n        \"flagged_message\",\n        sa.Column(\"message_id\", postgresql.UUID(as_uuid=True), nullable=False),\n        sa.Column(\n            \"created_date\", sa.DateTime(timezone=True), server_default=sa.text(\"CURRENT_TIMESTAMP\"), nullable=False\n        ),\n        sa.Column(\"processed\", sa.Boolean(), nullable=False),\n        sa.ForeignKeyConstraint([\"message_id\"], [\"message.id\"], ondelete=\"CASCADE\"),\n        sa.PrimaryKeyConstraint(\"message_id\"),\n    )\n    op.create_index(op.f(\"ix_flagged_message_created_date\"), \"flagged_message\", [\"created_date\"], unique=False)\n    op.create_index(op.f(\"ix_flagged_message_processed\"), \"flagged_message\", [\"processed\"], unique=False)\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_index(op.f(\"ix_flagged_message_processed\"), table_name=\"flagged_message\")\n    op.drop_index(op.f(\"ix_flagged_message_created_date\"), table_name=\"flagged_message\")\n    op.drop_table(\"flagged_message\")\n    # ### end Alembic commands ###\n", "backend/alembic/versions/2023_01_27_2013-f856bf19d32b_add_user_show_on_leaderboard.py": "\"\"\"add user.show_on_leaderboard\n\nRevision ID: f856bf19d32b\nRevises: c84fcd6900dc\nCreate Date: 2023-01-27 20:13:56.533374\n\n\"\"\"\nimport sqlalchemy as sa\nfrom alembic import op\n\n# revision identifiers, used by Alembic.\nrevision = \"f856bf19d32b\"\ndown_revision = \"c84fcd6900dc\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.add_column(\n        \"user\", sa.Column(\"show_on_leaderboard\", sa.Boolean(), server_default=sa.text(\"true\"), nullable=False)\n    )\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_column(\"user\", \"show_on_leaderboard\")\n    # ### end Alembic commands ###\n", "backend/alembic/versions/2023_01_15_0002-7c98102efbca_change_user_stats_ranking_counts.py": "\"\"\"change user_stats ranking counts\n\nRevision ID: 7c98102efbca\nRevises: 619255ae9076\nCreate Date: 2023-01-15 00:02:45.622986\n\n\"\"\"\nimport sqlalchemy as sa\nimport sqlmodel\nfrom alembic import op\nfrom sqlalchemy.dialects.postgresql import UUID\n\n# revision identifiers, used by Alembic.\nrevision = \"7c98102efbca\"\ndown_revision = \"619255ae9076\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_table(\"user_stats\")\n    op.create_table(\n        \"user_stats\",\n        sa.Column(\"user_id\", UUID(as_uuid=True), nullable=False),\n        sa.Column(\"modified_date\", sa.DateTime(), server_default=sa.text(\"CURRENT_TIMESTAMP\"), nullable=False),\n        sa.Column(\"base_date\", sa.DateTime(), nullable=True),\n        sa.Column(\"time_frame\", sqlmodel.sql.sqltypes.AutoString(), nullable=False),\n        sa.Column(\"leader_score\", sa.Integer(), nullable=False),\n        sa.Column(\"prompts\", sa.Integer(), nullable=False),\n        sa.Column(\"replies_assistant\", sa.Integer(), nullable=False),\n        sa.Column(\"replies_prompter\", sa.Integer(), nullable=False),\n        sa.Column(\"labels_simple\", sa.Integer(), nullable=False),\n        sa.Column(\"labels_full\", sa.Integer(), nullable=False),\n        sa.Column(\"rankings_total\", sa.Integer(), nullable=False),\n        sa.Column(\"rankings_good\", sa.Integer(), nullable=False),\n        sa.Column(\"accepted_prompts\", sa.Integer(), nullable=False),\n        sa.Column(\"accepted_replies_assistant\", sa.Integer(), nullable=False),\n        sa.Column(\"accepted_replies_prompter\", sa.Integer(), nullable=False),\n        sa.Column(\"reply_ranked_1\", sa.Integer(), nullable=False),\n        sa.Column(\"reply_ranked_2\", sa.Integer(), nullable=False),\n        sa.Column(\"reply_ranked_3\", sa.Integer(), nullable=False),\n        sa.Column(\"streak_last_day_date\", sa.DateTime(), nullable=True),\n        sa.Column(\"streak_days\", sa.Integer(), nullable=True),\n        sa.ForeignKeyConstraint(\n            [\"user_id\"],\n            [\"user.id\"],\n        ),\n        sa.PrimaryKeyConstraint(\"user_id\", \"time_frame\"),\n    )\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.add_column(\n        \"user_stats\",\n        sa.Column(\"reply_prompter_ranked_3\", sa.INTEGER(), server_default=\"0\", autoincrement=False, nullable=False),\n    )\n    op.add_column(\n        \"user_stats\",\n        sa.Column(\"reply_assistant_ranked_1\", sa.INTEGER(), server_default=\"0\", autoincrement=False, nullable=False),\n    )\n    op.add_column(\n        \"user_stats\",\n        sa.Column(\"reply_assistant_ranked_2\", sa.INTEGER(), server_default=\"0\", autoincrement=False, nullable=False),\n    )\n    op.add_column(\n        \"user_stats\",\n        sa.Column(\"reply_prompter_ranked_2\", sa.INTEGER(), server_default=\"0\", autoincrement=False, nullable=False),\n    )\n    op.add_column(\n        \"user_stats\",\n        sa.Column(\"reply_prompter_ranked_1\", sa.INTEGER(), server_default=\"0\", autoincrement=False, nullable=False),\n    )\n    op.add_column(\n        \"user_stats\",\n        sa.Column(\"reply_assistant_ranked_3\", sa.INTEGER(), server_default=\"0\", autoincrement=False, nullable=False),\n    )\n    op.drop_column(\"user_stats\", \"reply_ranked_3\")\n    op.drop_column(\"user_stats\", \"reply_ranked_2\")\n    op.drop_column(\"user_stats\", \"reply_ranked_1\")\n    # ### end Alembic commands ###\n", "backend/alembic/versions/2022_12_25_1705-067c4002f2d9_add_text_labels.py": "\"\"\"Adds text labels table.\n\nRevision ID: 067c4002f2d9\nRevises: 0daec5f8135f\nCreate Date: 2022-12-25 17:05:21.208843\n\n\"\"\"\nimport sqlalchemy as sa\nimport sqlmodel\nfrom alembic import op\nfrom sqlalchemy.dialects import postgresql\n\n# revision identifiers, used by Alembic.\nrevision = \"067c4002f2d9\"\ndown_revision = \"0daec5f8135f\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.create_table(\n        \"text_labels\",\n        sa.Column(\"id\", postgresql.UUID(as_uuid=True), server_default=sa.text(\"gen_random_uuid()\"), nullable=False),\n        sa.Column(\"created_date\", sa.DateTime(), server_default=sa.text(\"CURRENT_TIMESTAMP\"), nullable=False),\n        sa.Column(\"post_id\", postgresql.UUID(as_uuid=True), nullable=True),\n        sa.Column(\"labels\", postgresql.JSONB(astext_type=sa.Text()), nullable=True),\n        sa.Column(\"api_client_id\", sqlmodel.sql.sqltypes.GUID(), nullable=False),\n        sa.Column(\"text\", sqlmodel.sql.sqltypes.AutoString(length=65536), nullable=False),\n        sa.ForeignKeyConstraint(\n            [\"api_client_id\"],\n            [\"api_client.id\"],\n        ),\n        sa.ForeignKeyConstraint(\n            [\"post_id\"],\n            [\"post.id\"],\n        ),\n        sa.PrimaryKeyConstraint(\"id\"),\n    )\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_table(\"text_labels\")\n    # ### end Alembic commands ###\n", "backend/alembic/versions/2023_01_29_1207-7b8f0011e0b0_move_user_streak_from_user_stats_to_.py": "\"\"\"move user_streak from user_stats to user table\n\nRevision ID: 7b8f0011e0b0\nRevises: 8a5feed819ee\nCreate Date: 2023-01-29 12:07:29.379326\n\n\"\"\"\nimport sqlalchemy as sa\nfrom alembic import op\n\n# revision identifiers, used by Alembic.\nrevision = \"7b8f0011e0b0\"\ndown_revision = \"49d8445b4c90\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.add_column(\n        \"user\",\n        sa.Column(\n            \"streak_last_day_date\",\n            sa.DateTime(timezone=True),\n            server_default=sa.text(\"CURRENT_TIMESTAMP\"),\n            autoincrement=False,\n            nullable=True,\n        ),\n    )\n    op.add_column(\"user\", sa.Column(\"streak_days\", sa.INTEGER(), autoincrement=False, nullable=True))\n    op.add_column(\n        \"user\", sa.Column(\"last_activity_date\", sa.DateTime(timezone=True), autoincrement=False, nullable=True)\n    )\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_column(\"user\", \"streak_days\")\n    op.drop_column(\"user\", \"streak_last_day_date\")\n    op.drop_column(\"user\", \"last_activity_date\")\n    # ### end Alembic commands ###\n", "backend/alembic/versions/2022_12_17_2230-6368515778c5_add_auth_method_to_person.py": "\"\"\"add auth_method to person\n\nRevision ID: 6368515778c5\nRevises: cd7de470586e\nCreate Date: 2022-12-17 17:57:33.022549\n\n\"\"\"\nimport sqlalchemy as sa\nfrom alembic import op\n\n# revision identifiers, used by Alembic.\nrevision = \"6368515778c5\"\ndown_revision = \"cd7de470586e\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.add_column(\"person\", sa.Column(\"auth_method\", sa.String(length=128), nullable=True))\n    op.execute(\"UPDATE person SET auth_method = 'local'\")\n    op.alter_column(\"person\", \"auth_method\", nullable=False)\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_column(\"person\", \"auth_method\")\n    # ### end Alembic commands ###\n", "backend/alembic/versions/2023_05_07_2129-1b6e3ae16e9d_add_text_search.py": "\"\"\"add text search\n\nRevision ID: 1b6e3ae16e9d\nRevises: 9db92d504f64\nCreate Date: 2023-05-07 21:29:35.545612\n\"\"\"\nimport sqlalchemy as sa\nfrom alembic import op\nfrom sqlalchemy.dialects import postgresql\n\n# revision identifiers, used by Alembic.\nrevision = \"1b6e3ae16e9d\"\ndown_revision = \"9db92d504f64\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    op.add_column(\"message\", sa.Column(\"search_vector\", postgresql.TSVECTOR(), nullable=True))\n    op.create_index(\"idx_search_vector\", \"message\", [\"search_vector\"], postgresql_using=\"gin\")\n\n\ndef downgrade() -> None:\n    op.drop_index(\"idx_search_vector\", \"message\")\n    op.drop_column(\"message\", \"search_vector\")\n", "backend/alembic/versions/2023_01_08_2200-bcc2fe18d214_messagetoxicity.py": "\"\"\"MessageToxicity\n\nRevision ID: bcc2fe18d214\nRevises: 20cd871f4ec7\nCreate Date: 2023-01-08 22:00:43.297719\n\n\"\"\"\nimport sqlalchemy as sa\nimport sqlmodel\nfrom alembic import op\nfrom sqlalchemy.dialects import postgresql\n\n# revision identifiers, used by Alembic.\nrevision = \"bcc2fe18d214\"\ndown_revision = \"846cc08ac79f\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.create_table(\n        \"message_toxicity\",\n        sa.Column(\"message_id\", postgresql.UUID(as_uuid=True), nullable=False),\n        sa.Column(\"toxicity\", sa.Float(), nullable=True),\n        sa.Column(\"created_date\", sa.DateTime(), server_default=sa.text(\"CURRENT_TIMESTAMP\"), nullable=False),\n        sa.Column(\"model\", sqlmodel.sql.sqltypes.AutoString(length=256), nullable=False),\n        sa.ForeignKeyConstraint(\n            [\"message_id\"],\n            [\"message.id\"],\n        ),\n        sa.PrimaryKeyConstraint(\"message_id\", \"model\"),\n    )\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_table(\"message_toxicity\")\n    # ### end Alembic commands ###\n", "backend/alembic/versions/2022_12_22_1835-0daec5f8135f_add_auth_method_to_ix_person_username.py": "\"\"\"add_auth_method_to_ix_person_username\n\nRevision ID: 0daec5f8135f\nRevises: 6368515778c5\nCreate Date: 2022-12-22 18:35:59.609013\n\n\"\"\"\nimport sqlalchemy as sa  # noqa: F401\nfrom alembic import op\n\n# revision identifiers, used by Alembic.\nrevision = \"0daec5f8135f\"\ndown_revision = \"6368515778c5\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_index(\"ix_person_username\", table_name=\"person\")\n    op.create_index(\"ix_person_username\", \"person\", [\"api_client_id\", \"username\", \"auth_method\"], unique=True)\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_index(\"ix_person_username\", table_name=\"person\")\n    op.create_index(\"ix_person_username\", \"person\", [\"api_client_id\", \"username\"], unique=False)\n    # ### end Alembic commands ###\n", "backend/alembic/versions/2023_01_26_1835-c84fcd6900dc_add_task_created_date_index.py": "\"\"\"add task created date index\n\nRevision ID: c84fcd6900dc\nRevises: 40ed93df0ed5\nCreate Date: 2023-01-26 18:35:43.061589\n\n\"\"\"\nfrom alembic import op\n\n# revision identifiers, used by Alembic.\nrevision = \"c84fcd6900dc\"\ndown_revision = \"40ed93df0ed5\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.create_index(op.f(\"ix_task_created_date\"), \"task\", [\"created_date\"], unique=False)\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_index(op.f(\"ix_task_created_date\"), table_name=\"task\")\n    # ### end Alembic commands ###\n", "backend/alembic/versions/2023_01_24_2256-40ed93df0ed5_add_message_emoji.py": "\"\"\"add message_emoji\n\nRevision ID: 40ed93df0ed5\nRevises: 8ba17b5f467a\nCreate Date: 2023-01-24 22:56:28.229408\n\n\"\"\"\nimport sqlalchemy as sa\nimport sqlmodel\nfrom alembic import op\nfrom sqlalchemy.dialects import postgresql\n\n# revision identifiers, used by Alembic.\nrevision = \"40ed93df0ed5\"\ndown_revision = \"8ba17b5f467a\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.create_table(\n        \"message_emoji\",\n        sa.Column(\"message_id\", postgresql.UUID(as_uuid=True), nullable=False),\n        sa.Column(\"user_id\", postgresql.UUID(as_uuid=True), nullable=False),\n        sa.Column(\n            \"created_date\", sa.DateTime(timezone=True), server_default=sa.text(\"CURRENT_TIMESTAMP\"), nullable=False\n        ),\n        sa.Column(\"emoji\", sqlmodel.sql.sqltypes.AutoString(length=128), nullable=False),\n        sa.ForeignKeyConstraint([\"message_id\"], [\"message.id\"], ondelete=\"CASCADE\"),\n        sa.ForeignKeyConstraint([\"user_id\"], [\"user.id\"], ondelete=\"CASCADE\"),\n        sa.PrimaryKeyConstraint(\"message_id\", \"user_id\", \"emoji\"),\n    )\n    op.create_index(\"ix_message_emoji__user_id__message_id\", \"message_emoji\", [\"user_id\", \"message_id\"], unique=False)\n    op.add_column(\"message\", sa.Column(\"emojis\", postgresql.JSONB(astext_type=sa.Text()), nullable=True))\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_column(\"message\", \"emojis\")\n    op.drop_index(\"ix_message_emoji__user_id__message_id\", table_name=\"message_emoji\")\n    op.drop_table(\"message_emoji\")\n    # ### end Alembic commands ###\n", "backend/alembic/versions/2022_12_28_1824-ef0b52902560_added_lang_column_for_iso_639_1_codes.py": "\"\"\"Added lang column for ISO-639-1 codes\n\nRevision ID: ef0b52902560\nRevises: d24b37426857\nCreate Date: 2022-12-28 18:24:21.393973\n\n\"\"\"\nimport sqlalchemy as sa\nimport sqlmodel\nfrom alembic import op\n\n# revision identifiers, used by Alembic.\nrevision = \"ef0b52902560\"\ndown_revision = \"d24b37426857\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.add_column(\n        \"post\", sa.Column(\"lang\", sqlmodel.sql.sqltypes.AutoString(length=200), nullable=False, default=\"en-US\")\n    )\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_column(\"post\", \"lang\")\n    # ### end Alembic commands ###\n", "backend/alembic/versions/2023_01_19_2153-7f0a28a156f4_switch_to_timestamp_with_tz.py": "\"\"\"switch to timestamp with tz\n\nRevision ID: 7f0a28a156f4\nRevises: 0964ac95170d\nCreate Date: 2023-01-19 21:53:01.107137\n\n\"\"\"\nimport sqlalchemy as sa\nfrom alembic import op\n\n# revision identifiers, used by Alembic.\nrevision = \"7f0a28a156f4\"\ndown_revision = \"0964ac95170d\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.alter_column(table_name=\"user_stats\", column_name=\"modified_date\", type_=sa.DateTime(timezone=True))\n    op.alter_column(table_name=\"user_stats\", column_name=\"base_date\", type_=sa.DateTime(timezone=True))\n    op.alter_column(table_name=\"journal_integration\", column_name=\"last_run\", type_=sa.DateTime(timezone=True))\n    op.alter_column(table_name=\"message_embedding\", column_name=\"created_date\", type_=sa.DateTime(timezone=True))\n    op.alter_column(table_name=\"message_reaction\", column_name=\"created_date\", type_=sa.DateTime(timezone=True))\n    op.alter_column(table_name=\"message_toxicity\", column_name=\"created_date\", type_=sa.DateTime(timezone=True))\n    op.alter_column(table_name=\"message\", column_name=\"created_date\", type_=sa.DateTime(timezone=True))\n    op.alter_column(table_name=\"task\", column_name=\"created_date\", type_=sa.DateTime(timezone=True))\n    op.alter_column(table_name=\"task\", column_name=\"expiry_date\", type_=sa.DateTime(timezone=True))\n    op.alter_column(table_name=\"text_labels\", column_name=\"created_date\", type_=sa.DateTime(timezone=True))\n    op.alter_column(table_name=\"user\", column_name=\"created_date\", type_=sa.DateTime(timezone=True))\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.alter_column(table_name=\"user_stats\", column_name=\"modified_date\", type_=sa.DateTime(timezone=False))\n    op.alter_column(table_name=\"user_stats\", column_name=\"base_date\", type_=sa.DateTime(timezone=False))\n    op.alter_column(table_name=\"journal_integration\", column_name=\"last_run\", type_=sa.DateTime(timezone=False))\n    op.alter_column(table_name=\"message_embedding\", column_name=\"created_date\", type_=sa.DateTime(timezone=False))\n    op.alter_column(table_name=\"message_reaction\", column_name=\"created_date\", type_=sa.DateTime(timezone=False))\n    op.alter_column(table_name=\"message_toxicity\", column_name=\"created_date\", type_=sa.DateTime(timezone=False))\n    op.alter_column(table_name=\"message\", column_name=\"created_date\", type_=sa.DateTime(timezone=False))\n    op.alter_column(table_name=\"task\", column_name=\"created_date\", type_=sa.DateTime(timezone=False))\n    op.alter_column(table_name=\"task\", column_name=\"expiry_date\", type_=sa.DateTime(timezone=False))\n    op.alter_column(table_name=\"text_labels\", column_name=\"created_date\", type_=sa.DateTime(timezone=False))\n    op.alter_column(table_name=\"user\", column_name=\"created_date\", type_=sa.DateTime(timezone=False))\n    # ### end Alembic commands ###\n", "backend/alembic/versions/2023_01_08_2128-aac6b2f66006_created_date.py": "\"\"\"Created date\n\nRevision ID: aac6b2f66006\nRevises: 35bdc1a08bb8\nCreate Date: 2023-01-08 21:28:27.342729\n\n\"\"\"\nimport sqlalchemy as sa\nfrom alembic import op\n\n# revision identifiers, used by Alembic.\nrevision = \"aac6b2f66006\"\ndown_revision = \"35bdc1a08bb8\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.add_column(\n        \"message_embedding\",\n        sa.Column(\"created_date\", sa.DateTime(), server_default=sa.text(\"CURRENT_TIMESTAMP\"), nullable=False),\n    )\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_column(\"message_embedding\", \"created_date\")\n    # ### end Alembic commands ###\n", "backend/alembic/versions/2023_01_08_1603-35bdc1a08bb8_embedding_for_message_now_in_its_own_.py": "\"\"\"embedding for message now in its own table\n\nRevision ID: 35bdc1a08bb8\nRevises: 023548d474f7\nCreate Date: 2023-01-08 16:03:48.454207\n\n\"\"\"\nimport sqlalchemy as sa\nimport sqlmodel\nfrom alembic import op\nfrom sqlalchemy.dialects import postgresql\n\n# revision identifiers, used by Alembic.\nrevision = \"35bdc1a08bb8\"\ndown_revision = \"023548d474f7\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.create_table(\n        \"message_embedding\",\n        sa.Column(\"message_id\", postgresql.UUID(as_uuid=True), nullable=False),\n        sa.Column(\"embedding\", sa.ARRAY(sa.Float()), nullable=True),\n        sa.Column(\"model\", sqlmodel.sql.sqltypes.AutoString(length=256), nullable=False),\n        sa.ForeignKeyConstraint(\n            [\"message_id\"],\n            [\"message.id\"],\n        ),\n        sa.PrimaryKeyConstraint(\"message_id\", \"model\"),\n    )\n    op.drop_column(\"message\", \"miniLM_embedding\")\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.add_column(\n        \"message\",\n        sa.Column(\n            \"miniLM_embedding\",\n            postgresql.ARRAY(postgresql.DOUBLE_PRECISION(precision=53)),\n            autoincrement=False,\n            nullable=True,\n        ),\n    )\n    op.drop_table(\"message_embedding\")\n    # ### end Alembic commands ###\n", "backend/alembic/versions/2022_12_29_2103-464ec4667aae_add_collective_flag_to_task.py": "\"\"\"add collective flag to task\n\nRevision ID: 464ec4667aae\nRevises: ef0b52902560\nCreate Date: 2022-12-29 21:03:06.841962\n\n\"\"\"\nimport sqlalchemy as sa\nfrom alembic import op\n\n# revision identifiers, used by Alembic.\nrevision = \"464ec4667aae\"\ndown_revision = \"ef0b52902560\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.add_column(\n        \"work_package\", sa.Column(\"collective\", sa.Boolean(), server_default=sa.text(\"false\"), nullable=False)\n    )\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_column(\"work_package\", \"collective\")\n    # ### end Alembic commands ###\n", "backend/alembic/versions/2023_02_26_0052-9db92d504f64_add_lang_to_message_tree_state.py": "\"\"\"add lang to message_tree_state\n\nRevision ID: 9db92d504f64\nRevises: 8cd0c34d0c3c\nCreate Date: 2023-02-26 00:52:40.624843\n\n\"\"\"\nimport sqlalchemy as sa\nfrom alembic import op\n\n# revision identifiers, used by Alembic.\nrevision = \"9db92d504f64\"\ndown_revision = \"8cd0c34d0c3c\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.add_column(\"message_tree_state\", sa.Column(\"lang\", sa.String(length=32), nullable=True))\n    op.execute(\n        \"WITH msg AS (SELECT id, lang FROM message WHERE parent_id is NULL) UPDATE message_tree_state mts SET lang = msg.lang FROM msg WHERE mts.message_tree_id = msg.id\"\n    )\n    op.alter_column(\"message_tree_state\", \"lang\", nullable=False)\n    op.drop_index(\"ix_message_tree_state_state\", table_name=\"message_tree_state\")\n    op.create_index(\"ix_message_tree_state__lang__state\", \"message_tree_state\", [\"state\", \"lang\"], unique=False)\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_index(\"ix_message_tree_state__lang__state\", table_name=\"message_tree_state\")\n    op.create_index(\"ix_message_tree_state_state\", \"message_tree_state\", [\"state\"], unique=False)\n    op.drop_column(\"message_tree_state\", \"lang\")\n    # ### end Alembic commands ###\n", "backend/alembic/versions/2023_02_02_1817-8c8241d1f973_add_account_table.py": "\"\"\"Add Account table\n\nRevision ID: 8c8241d1f973\nRevises: 4d7e0b0ebe84\nCreate Date: 2023-01-30 15:10:58.776315\n\n\"\"\"\nimport sqlalchemy as sa\nimport sqlmodel\nfrom alembic import op\nfrom sqlalchemy.dialects import postgresql\n\n# revision identifiers, used by Alembic.\nrevision = \"8c8241d1f973\"\ndown_revision = \"4d7e0b0ebe84\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.create_table(\n        \"account\",\n        sa.Column(\"id\", postgresql.UUID(as_uuid=True), server_default=sa.text(\"gen_random_uuid()\"), nullable=False),\n        sa.Column(\"user_id\", sqlmodel.sql.sqltypes.GUID(), nullable=False),\n        sa.Column(\"provider\", sqlmodel.sql.sqltypes.AutoString(length=128), nullable=False),\n        sa.Column(\"provider_account_id\", sqlmodel.sql.sqltypes.AutoString(length=128), nullable=False),\n        sa.ForeignKeyConstraint([\"user_id\"], [\"user.id\"]),\n        sa.PrimaryKeyConstraint(\"id\"),\n    )\n    op.create_index(\"provider\", \"account\", [\"provider_account_id\"], unique=True)\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_index(\"provider\", table_name=\"account\")\n    op.drop_table(\"account\")\n    # ### end Alembic commands ###\n", "backend/alembic/versions/2023_01_20_1650-160ac010efcc_use_en_instead_en_us_as_default_lang.py": "\"\"\"use 'en' instead 'en-US' as default lang\n\nRevision ID: 160ac010efcc\nRevises: 4f26fec4d204\nCreate Date: 2023-01-20 16:50:00\n\n\"\"\"\nimport sqlalchemy as sa\nfrom alembic import op\n\n# revision identifiers, used by Alembic.\nrevision = \"160ac010efcc\"\ndown_revision = \"4f26fec4d204\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_column(\"message\", \"lang\")\n    op.add_column(\"message\", sa.Column(\"lang\", sa.String(length=32), server_default=\"en\", nullable=False))\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_column(\"message\", \"lang\")\n    op.add_column(\"message\", sa.Column(\"lang\", sa.VARCHAR(length=200), autoincrement=False, nullable=False))\n    # ### end Alembic commands ###\n", "backend/alembic/versions/2022_12_27_1444-3358eb6834e6_add_journal_table.py": "\"\"\"add_journal_table\n\nRevision ID: 3358eb6834e6\nRevises: 067c4002f2d9\nCreate Date: 2022-12-27 14:44:59.483868\n\n\"\"\"\nimport sqlalchemy as sa\nimport sqlmodel\nfrom alembic import op\nfrom sqlalchemy.dialects import postgresql\n\n# revision identifiers, used by Alembic.\nrevision = \"3358eb6834e6\"\ndown_revision = \"067c4002f2d9\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.create_table(\n        \"journal\",\n        sa.Column(\"id\", postgresql.UUID(as_uuid=True), nullable=False),\n        sa.Column(\n            \"created_date\", sa.DateTime(timezone=True), server_default=sa.text(\"CURRENT_TIMESTAMP\"), nullable=False\n        ),\n        sa.Column(\n            \"event_payload\",\n            postgresql.JSONB(astext_type=sa.Text()),\n            nullable=False,\n        ),\n        sa.Column(\"person_id\", sqlmodel.sql.sqltypes.GUID(), nullable=True),\n        sa.Column(\"post_id\", sqlmodel.sql.sqltypes.GUID(), nullable=True),\n        sa.Column(\"api_client_id\", sqlmodel.sql.sqltypes.GUID(), nullable=False),\n        sa.Column(\"event_type\", sqlmodel.sql.sqltypes.AutoString(length=200), nullable=False),\n        sa.ForeignKeyConstraint(\n            [\"api_client_id\"],\n            [\"api_client.id\"],\n        ),\n        sa.ForeignKeyConstraint(\n            [\"person_id\"],\n            [\"person.id\"],\n        ),\n        sa.ForeignKeyConstraint(\n            [\"post_id\"],\n            [\"post.id\"],\n        ),\n        sa.PrimaryKeyConstraint(\"id\"),\n    )\n    op.create_index(op.f(\"ix_journal_person_id\"), \"journal\", [\"person_id\"], unique=False)\n    op.create_table(\n        \"journal_integration\",\n        sa.Column(\"id\", postgresql.UUID(as_uuid=True), server_default=sa.text(\"gen_random_uuid()\"), nullable=False),\n        sa.Column(\"last_run\", sa.DateTime(), nullable=True),\n        sa.Column(\"description\", sqlmodel.sql.sqltypes.AutoString(length=512), nullable=False),\n        sa.Column(\"last_journal_id\", sqlmodel.sql.sqltypes.GUID(), nullable=True),\n        sa.Column(\"last_error\", sqlmodel.sql.sqltypes.AutoString(), nullable=True),\n        sa.Column(\"next_run\", sa.DateTime(), nullable=True),\n        sa.ForeignKeyConstraint(\n            [\"last_journal_id\"],\n            [\"journal.id\"],\n        ),\n        sa.PrimaryKeyConstraint(\"id\", \"description\"),\n    )\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_table(\"journal_integration\")\n    op.drop_index(op.f(\"ix_journal_person_id\"), table_name=\"journal\")\n    op.drop_table(\"journal\")\n    # ### end Alembic commands ###\n", "backend/alembic/versions/2023_01_10_1733-846cc08ac79f_add_enabled_deleted_notes_fields_to_user.py": "\"\"\"Add enabled, deleted, notes fields to User\n\nRevision ID: 846cc08ac79f\nRevises: aac6b2f66006\nCreate Date: 2023-01-10 17:33:07.104596\n\n\"\"\"\nimport sqlalchemy as sa\nimport sqlmodel\nfrom alembic import op\n\n# revision identifiers, used by Alembic.\nrevision = \"846cc08ac79f\"\ndown_revision = \"befa42582ea4\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.add_column(\"user\", sa.Column(\"enabled\", sa.Boolean(), server_default=sa.text(\"true\"), nullable=False))\n    op.add_column(\"user\", sa.Column(\"deleted\", sa.Boolean(), server_default=sa.text(\"false\"), nullable=False))\n    op.add_column(\n        \"user\",\n        sa.Column(\"notes\", sqlmodel.sql.sqltypes.AutoString(length=1024), server_default=\"\", nullable=False),\n    )\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_column(\"user\", \"notes\")\n    op.drop_column(\"user\", \"deleted\")\n    op.drop_column(\"user\", \"enabled\")\n    # ### end Alembic commands ###\n", "backend/alembic/versions/2023_01_09_0047-05975b274a81_add_review_count_ranking_count_to_.py": "\"\"\"add review_count & ranking_count to message\n\nRevision ID: 05975b274a81\nRevises: 92a367bb9f40\nCreate Date: 2023-01-09 00:47:25.496036\n\n\"\"\"\nimport sqlalchemy as sa\nfrom alembic import op\n\n# revision identifiers, used by Alembic.\nrevision = \"05975b274a81\"\ndown_revision = \"92a367bb9f40\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.add_column(\"message\", sa.Column(\"review_count\", sa.Integer(), server_default=sa.text(\"0\"), nullable=False))\n    op.add_column(\"message\", sa.Column(\"review_result\", sa.Boolean(), server_default=sa.text(\"false\"), nullable=False))\n    op.add_column(\"message\", sa.Column(\"ranking_count\", sa.Integer(), server_default=sa.text(\"0\"), nullable=False))\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_column(\"message\", \"ranking_count\")\n    op.drop_column(\"message\", \"review_result\")\n    op.drop_column(\"message\", \"review_count\")\n    # ### end Alembic commands ###\n", "backend/alembic/versions/2023_01_08_1106-3d96bb92e33a_added_minilm_embedding_column_to_message.py": "\"\"\"added miniLM_embedding column to message\n\nRevision ID: 023548d474f7\nRevises: ba61fe17fb6e\nCreate Date: 2023-01-08 11:06:25.613290\n\n\"\"\"\nimport sqlalchemy as sa\nfrom alembic import op\n\n# revision identifiers, used by Alembic.\nrevision = \"023548d474f7\"\ndown_revision = \"ba61fe17fb6e\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.add_column(\"message\", sa.Column(\"miniLM_embedding\", sa.ARRAY(sa.Float()), nullable=True))\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_column(\"message\", \"miniLM_embedding\")\n    # ### end Alembic commands ###\n", "backend/alembic/versions/2022_12_31_0438-8d269bc4fdbd_add_deleted_field_to_post.py": "\"\"\"add deleted field to post\n\nRevision ID: 8d269bc4fdbd\nRevises: abb47e9d145a\nCreate Date: 2022-12-31 04:38:41.799206\n\n\"\"\"\nimport sqlalchemy as sa\nfrom alembic import op\n\n# revision identifiers, used by Alembic.\nrevision = \"8d269bc4fdbd\"\ndown_revision = \"abb47e9d145a\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.add_column(\"message\", sa.Column(\"deleted\", sa.Boolean(), server_default=sa.text(\"false\"), nullable=False))\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_column(\"message\", \"deleted\")\n    # ### end Alembic commands ###\n", "backend/alembic/versions/2023_02_15_1754-8cd0c34d0c3c_message_review_result_nullable.py": "\"\"\"message review_result nullable\n\nRevision ID: 8cd0c34d0c3c\nRevises: 165b55de5a94\nCreate Date: 2023-02-15 17:54:58.029278\n\n\"\"\"\nimport sqlalchemy as sa\nfrom alembic import op\n\n# revision identifiers, used by Alembic.\nrevision = \"8cd0c34d0c3c\"\ndown_revision = \"165b55de5a94\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.alter_column(\n        \"message\",\n        \"review_result\",\n        existing_type=sa.BOOLEAN(),\n        nullable=True,\n        server_default=None,\n        existing_server_default=sa.text(\"false\"),\n    )\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.alter_column(\n        \"message\", \"review_result\", existing_type=sa.BOOLEAN(), nullable=False, server_default=sa.text(\"false\")\n    )\n    # ### end Alembic commands ###\n", "backend/alembic/versions/2023_01_12_0119-befa42582ea4_remove_accepted_messages_from_message_.py": "\"\"\"remove accepted_messages from message_tree_state\n\nRevision ID: befa42582ea4\nRevises: 05975b274a81\nCreate Date: 2023-01-12 01:19:59.654864\n\n\"\"\"\nimport sqlalchemy as sa\nfrom alembic import op\n\n# revision identifiers, used by Alembic.\nrevision = \"befa42582ea4\"\ndown_revision = \"05975b274a81\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_column(\"message_tree_state\", \"accepted_messages\")\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.add_column(\n        \"message_tree_state\", sa.Column(\"accepted_messages\", sa.INTEGER(), autoincrement=False, nullable=False)\n    )\n    # ### end Alembic commands ###\n", "backend/alembic/versions/2023_02_01_2146-9e7ec4a9e3f2_add_skip_bool_skip_reason_to_task.py": "\"\"\"add skip bool & skip_reason to task\n\nRevision ID: 9e7ec4a9e3f2\nRevises: 7b8f0011e0b0\nCreate Date: 2023-02-01 21:46:49.971052\n\n\"\"\"\nimport sqlalchemy as sa\nimport sqlmodel\nfrom alembic import op\n\n# revision identifiers, used by Alembic.\nrevision = \"9e7ec4a9e3f2\"\ndown_revision = \"55361f323d12\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.add_column(\"task\", sa.Column(\"skipped\", sa.Boolean(), server_default=sa.text(\"false\"), nullable=False))\n    op.add_column(\"task\", sa.Column(\"skip_reason\", sqlmodel.sql.sqltypes.AutoString(length=512), nullable=True))\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_column(\"task\", \"skip_reason\")\n    op.drop_column(\"task\", \"skipped\")\n    # ### end Alembic commands ###\n", "backend/alembic/versions/2023_01_05_1346-3b0adfadbef9_removed_date_created_and_deleted_flag_.py": "\"\"\"removed date_created and deleted flag from message_tree_state\n\nRevision ID: 3b0adfadbef9\nRevises: d4161e384f83\nCreate Date: 2023-01-05 13:46:11.338655\n\n\"\"\"\nimport sqlalchemy as sa\nfrom alembic import op\nfrom sqlalchemy.dialects import postgresql\n\n# revision identifiers, used by Alembic.\nrevision = \"3b0adfadbef9\"\ndown_revision = \"d4161e384f83\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_column(\"message_tree_state\", \"deleted\")\n    op.drop_column(\"message_tree_state\", \"created_date\")\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.add_column(\n        \"message_tree_state\",\n        sa.Column(\n            \"created_date\",\n            postgresql.TIMESTAMP(),\n            server_default=sa.text(\"CURRENT_TIMESTAMP\"),\n            autoincrement=False,\n            nullable=False,\n        ),\n    )\n    op.add_column(\n        \"message_tree_state\",\n        sa.Column(\"deleted\", sa.BOOLEAN(), server_default=sa.text(\"false\"), autoincrement=False, nullable=False),\n    )\n    # ### end Alembic commands ###\n", "backend/alembic/versions/2023_01_19_2200-4f26fec4d204_add_ix_user_display_name_id.py": "\"\"\"add ix_user_display_name_id\n\nRevision ID: 4f26fec4d204\nRevises: 0964ac95170d\nCreate Date: 2023-01-19 22:00:00\n\n\"\"\"\nfrom alembic import op\n\n# revision identifiers, used by Alembic.\nrevision = \"4f26fec4d204\"\ndown_revision = \"7f0a28a156f4\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.create_index(\"ix_user_display_name_id\", \"user\", [\"display_name\", \"id\"], unique=True)\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_index(\"ix_user_display_name_id\", table_name=\"user\")\n    # ### end Alembic commands ###\n", "backend/alembic/versions/2023_01_24_1134-8ba17b5f467a_add_message_id_to_message_reaction.py": "\"\"\"add message_id to message_reaction\n\nRevision ID: 8ba17b5f467a\nRevises: 160ac010efcc\nCreate Date: 2023-01-24 11:34:42.167575\n\n\"\"\"\nimport sqlalchemy as sa\nimport sqlmodel\nfrom alembic import op\n\n# revision identifiers, used by Alembic.\nrevision = \"8ba17b5f467a\"\ndown_revision = \"160ac010efcc\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.add_column(\"message_reaction\", sa.Column(\"message_id\", sqlmodel.sql.sqltypes.GUID(), nullable=True))\n    op.create_index(op.f(\"ix_message_reaction_message_id\"), \"message_reaction\", [\"message_id\"], unique=False)\n    op.add_column(\"text_labels\", sa.Column(\"task_id\", sqlmodel.sql.sqltypes.GUID(), nullable=True))\n    op.create_index(op.f(\"ix_text_labels_task_id\"), \"text_labels\", [\"task_id\"], unique=False)\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_index(op.f(\"ix_text_labels_task_id\"), table_name=\"text_labels\")\n    op.drop_column(\"text_labels\", \"task_id\")\n    op.drop_index(op.f(\"ix_message_reaction_message_id\"), table_name=\"message_reaction\")\n    op.drop_column(\"message_reaction\", \"message_id\")\n    # ### end Alembic commands ###\n", "backend/alembic/versions/2022_12_15_0000-23e5fea252dd_first_revision.py": "\"\"\"first revision\n\nRevision ID: 23e5fea252dd\nRevises:\nCreate Date: 2022-12-12 12:47:28.801354\n\n\"\"\"\nimport sqlalchemy as sa\nfrom alembic import op\n\n# revision identifiers, used by Alembic.\nrevision = \"23e5fea252dd\"\ndown_revision = None\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    op.create_table(\n        \"service_client\",\n        sa.Column(\"id\", sa.Integer, sa.Identity()),\n        sa.Column(\"name\", sa.String(200), nullable=False),\n        sa.Column(\"service_admin_email\", sa.String(128), nullable=True),\n        sa.Column(\"api_key\", sa.String(300), nullable=False),\n        sa.Column(\"can_append\", sa.Boolean, nullable=False, server_default=\"true\"),\n        sa.Column(\"can_write\", sa.Boolean, nullable=False, server_default=\"false\"),\n        sa.Column(\"can_delete\", sa.Boolean, nullable=False, server_default=\"false\"),\n        sa.Column(\"can_read\", sa.Boolean, nullable=False, server_default=\"true\"),\n        sa.PrimaryKeyConstraint(\"id\"),\n    )\n    op.create_index(op.f(\"ix_service_client_api_key\"), \"service_client\", [\"api_key\"], unique=True)\n\n    op.create_table(\n        \"labeler\",\n        sa.Column(\"id\", sa.Integer, sa.Identity()),\n        sa.Column(\"display_name\", sa.String(96), nullable=False),\n        sa.Column(\"discord_username\", sa.String(96), nullable=True),\n        sa.Column(\n            \"created_date\",\n            sa.DateTime,\n            nullable=False,\n            server_default=sa.func.current_timestamp(),\n        ),\n        sa.Column(\"is_enabled\", sa.Boolean, nullable=False, server_default=\"true\"),\n        sa.Column(\"notes\", sa.String(10 * 1024), nullable=True),\n        sa.PrimaryKeyConstraint(\"id\"),\n        sa.UniqueConstraint(\"discord_username\"),\n    )\n\n    op.create_table(\n        \"prompt\",\n        sa.Column(\"id\", sa.Integer, sa.Identity()),\n        sa.Column(\"labeler_id\", sa.Integer, nullable=False),\n        sa.Column(\"prompt\", sa.Text, nullable=False),\n        sa.Column(\"response\", sa.Text, nullable=True),\n        sa.Column(\"lang\", sa.String(32), nullable=True),\n        sa.Column(\n            \"created_date\",\n            sa.DateTime(),\n            nullable=False,\n            server_default=sa.func.current_timestamp(),\n        ),\n        sa.ForeignKeyConstraint(\n            [\"labeler_id\"],\n            [\"labeler.id\"],\n        ),\n        sa.PrimaryKeyConstraint(\"id\"),\n    )\n    op.create_index(op.f(\"prompt_labeler_id\"), \"prompt\", [\"labeler_id\"], unique=False)\n\n\ndef downgrade() -> None:\n    op.drop_index(op.f(\"prompt_labeler_id\"), table_name=\"prompt\")\n    op.drop_table(\"prompt\")\n\n    op.drop_table(\"labeler\")\n\n    op.drop_index(op.f(\"ix_service_client_api_key\"), table_name=\"service_client\")\n    op.drop_table(\"service_client\")\n", "backend/alembic/versions/2022_12_16_0000-cd7de470586e_v1_db_structure.py": "\"\"\"v1 db structure\n\nRevision ID: cd7de470586e\nRevises: 23e5fea252dd\nCreate Date: 2022-12-15 11:15:32.830225\n\n\"\"\"\nimport uuid\n\nimport sqlalchemy as sa\nfrom alembic import op\nfrom sqlalchemy.dialects.postgresql import JSONB, UUID\n\n# revision identifiers, used by Alembic.\nrevision = \"cd7de470586e\"\ndown_revision = \"23e5fea252dd\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    # remove database objects\n    op.drop_index(op.f(\"prompt_labeler_id\"), table_name=\"prompt\")\n    op.drop_table(\"prompt\")\n    op.drop_table(\"labeler\")\n    op.drop_index(op.f(\"ix_service_client_api_key\"), table_name=\"service_client\")\n    op.drop_table(\"service_client\")\n\n    # wreate new database structure\n    op.create_table(\n        \"api_client\",\n        sa.Column(\"id\", UUID(as_uuid=True), default=uuid.uuid4, server_default=sa.text(\"gen_random_uuid()\")),\n        sa.Column(\"api_key\", sa.String(512), nullable=False),\n        sa.Column(\"description\", sa.String(256), nullable=False),\n        sa.Column(\"admin_email\", sa.String(256), nullable=True),\n        sa.Column(\"enabled\", sa.Boolean, default=True, nullable=False),\n        sa.PrimaryKeyConstraint(\"id\"),\n    )\n    op.create_index(op.f(\"ix_api_client_api_key\"), \"api_client\", [\"api_key\"], unique=True)\n\n    op.create_table(\n        \"person\",\n        sa.Column(\"id\", UUID(as_uuid=True), default=uuid.uuid4, server_default=sa.text(\"gen_random_uuid()\")),\n        sa.Column(\"username\", sa.String(128), nullable=False),  # unique in combination with api_client_id\n        sa.Column(\"display_name\", sa.String(256), nullable=False),  # cached last seen display_name\n        sa.Column(\"created_date\", sa.DateTime(), nullable=False, server_default=sa.func.current_timestamp()),\n        sa.Column(\"api_client_id\", UUID(as_uuid=True), nullable=False),\n        sa.PrimaryKeyConstraint(\"id\"),\n        sa.ForeignKeyConstraint([\"api_client_id\"], [\"api_client.id\"]),\n    )\n    op.create_index(op.f(\"ix_person_username\"), \"person\", [\"api_client_id\", \"username\"], unique=True)\n\n    op.create_table(\n        \"person_stats\",\n        sa.Column(\"person_id\", UUID(as_uuid=True)),\n        sa.Column(\"leader_score\", sa.Integer, default=0, nullable=False),  # determines position on leader board\n        sa.Column(\"modified_date\", sa.DateTime(), nullable=False, server_default=sa.func.current_timestamp()),\n        sa.Column(\"reactions\", sa.Integer, default=0, nullable=False),  # reactions sent by user\n        sa.Column(\"posts\", sa.Integer, default=0, nullable=False),  # posts sent by user\n        sa.Column(\"upvotes\", sa.Integer, default=0, nullable=False),  # received upvotes (form other users)\n        sa.Column(\"downvotes\", sa.Integer, default=0, nullable=False),  # received downvotes (from other users)\n        sa.Column(\"work_reward\", sa.Integer, default=0, nullable=False),  # reward for workpackage completions\n        sa.Column(\"compare_wins\", sa.Integer, default=0, nullable=False),  # num times user's post won compare tasks\n        sa.Column(\"compare_losses\", sa.Integer, default=0, nullable=False),  # num times users's post lost compare tasks\n        sa.PrimaryKeyConstraint(\"person_id\"),\n        sa.ForeignKeyConstraint([\"person_id\"], [\"person.id\"]),\n    )\n\n    op.create_table(\n        \"work_package\",\n        sa.Column(\"id\", UUID(as_uuid=True), default=uuid.uuid4, server_default=sa.text(\"gen_random_uuid()\")),\n        sa.Column(\"created_date\", sa.DateTime(), nullable=False, server_default=sa.func.current_timestamp()),\n        sa.Column(\"expiry_date\", sa.DateTime(), nullable=True),\n        sa.Column(\"person_id\", UUID(as_uuid=True), nullable=True),\n        sa.Column(\"payload_type\", sa.String(200), nullable=False),  # deserialization hint & dbg aid\n        sa.Column(\"payload\", JSONB(astext_type=sa.Text()), nullable=False),\n        sa.Column(\"api_client_id\", UUID(as_uuid=True), nullable=False),\n        sa.PrimaryKeyConstraint(\"id\"),\n        sa.ForeignKeyConstraint([\"person_id\"], [\"person.id\"]),\n        sa.ForeignKeyConstraint([\"api_client_id\"], [\"api_client.id\"]),\n    )\n    op.create_index(op.f(\"ix_work_package_person_id\"), \"work_package\", [\"person_id\"], unique=False)\n\n    op.create_table(\n        \"post\",\n        sa.Column(\"id\", UUID(as_uuid=True), default=uuid.uuid4, server_default=sa.text(\"gen_random_uuid()\")),\n        sa.Column(\"parent_id\", UUID(as_uuid=True), nullable=True),  # root posts have NULL parent\n        sa.Column(\"thread_id\", UUID(as_uuid=True), nullable=False),  # id of thread root\n        sa.Column(\"workpackage_id\", UUID(as_uuid=True), nullable=True),  # workpackage id to pass to handler on reply\n        sa.Column(\"person_id\", UUID(as_uuid=True), nullable=True),  # sender (recipients are part of payload)\n        sa.Column(\"api_client_id\", UUID(as_uuid=True), nullable=False),\n        sa.Column(\"role\", sa.String(128), nullable=False),  # 'assistant', 'user' or something else\n        sa.Column(\"frontend_post_id\", sa.String(200), nullable=False),  # unique together with api_client_id\n        sa.Column(\"created_date\", sa.DateTime(), nullable=False, server_default=sa.func.current_timestamp()),\n        sa.Column(\"payload_type\", sa.String(200), nullable=False),  # deserialization hint & dbg aid\n        sa.Column(\"payload\", JSONB(astext_type=sa.Text()), nullable=True),\n        sa.PrimaryKeyConstraint(\"id\"),\n        sa.ForeignKeyConstraint([\"person_id\"], [\"person.id\"]),\n        sa.ForeignKeyConstraint([\"api_client_id\"], [\"api_client.id\"]),\n    )\n    op.create_index(op.f(\"ix_post_frontend_post_id\"), \"post\", [\"api_client_id\", \"frontend_post_id\"], unique=True)\n    op.create_index(op.f(\"ix_post_thread_id\"), \"post\", [\"thread_id\"], unique=False)\n    op.create_index(op.f(\"ix_post_workpackage_id\"), \"post\", [\"workpackage_id\"], unique=False)\n    op.create_index(op.f(\"ix_post_person_id\"), \"post\", [\"person_id\"], unique=False)\n\n    op.create_table(\n        \"post_reaction\",\n        sa.Column(\"post_id\", UUID(as_uuid=True), nullable=False),\n        sa.Column(\"person_id\", UUID(as_uuid=True), nullable=False),  # sender (recipients are part of payload)\n        sa.Column(\"created_date\", sa.DateTime(), nullable=False, server_default=sa.func.current_timestamp()),\n        sa.Column(\"payload_type\", sa.String(200), nullable=False),  # deserialization hint & dbg aid\n        sa.Column(\"payload\", JSONB(astext_type=sa.Text()), nullable=False),\n        sa.Column(\"api_client_id\", UUID(as_uuid=True), nullable=False),\n        sa.PrimaryKeyConstraint(\"post_id\", \"person_id\"),\n        sa.ForeignKeyConstraint([\"post_id\"], [\"post.id\"]),\n        sa.ForeignKeyConstraint([\"person_id\"], [\"person.id\"]),\n        sa.ForeignKeyConstraint([\"api_client_id\"], [\"api_client.id\"]),\n    )\n\n\ndef downgrade() -> None:\n    op.drop_table(\"post_reaction\")\n\n    op.drop_index(\"ix_post_person_id\")\n    op.drop_index(\"ix_post_workpackage_id\")\n    op.drop_index(\"ix_post_thread_id\")\n    op.drop_index(\"ix_post_frontend_post_id\")\n    op.drop_table(\"post\")\n\n    op.drop_index(\"ix_work_package_person_id\")\n    op.drop_table(\"work_package\")\n\n    op.drop_table(\"person_stats\")\n\n    op.drop_index(\"ix_person_username\")\n    op.drop_table(\"person\")\n\n    op.drop_index(\"ix_api_client_api_key\")\n    op.drop_table(\"api_client\")\n\n    op.create_table(\n        \"service_client\",\n        sa.Column(\"id\", sa.Integer, sa.Identity()),\n        sa.Column(\"name\", sa.String(200), nullable=False),\n        sa.Column(\"service_admin_email\", sa.String(128), nullable=True),\n        sa.Column(\"api_key\", sa.String(300), nullable=False),\n        sa.Column(\"can_append\", sa.Boolean, nullable=False, server_default=\"true\"),\n        sa.Column(\"can_write\", sa.Boolean, nullable=False, server_default=\"false\"),\n        sa.Column(\"can_delete\", sa.Boolean, nullable=False, server_default=\"false\"),\n        sa.Column(\"can_read\", sa.Boolean, nullable=False, server_default=\"true\"),\n        sa.PrimaryKeyConstraint(\"id\"),\n    )\n    op.create_index(op.f(\"ix_service_client_api_key\"), \"service_client\", [\"api_key\"], unique=True)\n\n    op.create_table(\n        \"labeler\",\n        sa.Column(\"id\", sa.Integer, sa.Identity()),\n        sa.Column(\"display_name\", sa.String(96), nullable=False),\n        sa.Column(\"discord_username\", sa.String(96), nullable=True),\n        sa.Column(\n            \"created_date\",\n            sa.DateTime,\n            nullable=False,\n            server_default=sa.func.current_timestamp(),\n        ),\n        sa.Column(\"is_enabled\", sa.Boolean, nullable=False, server_default=\"true\"),\n        sa.Column(\"notes\", sa.String(10 * 1024), nullable=True),\n        sa.PrimaryKeyConstraint(\"id\"),\n        sa.UniqueConstraint(\"discord_username\"),\n    )\n\n    op.create_table(\n        \"prompt\",\n        sa.Column(\"id\", sa.Integer, sa.Identity()),\n        sa.Column(\"labeler_id\", sa.Integer, nullable=False),\n        sa.Column(\"prompt\", sa.Text, nullable=False),\n        sa.Column(\"response\", sa.Text, nullable=True),\n        sa.Column(\"lang\", sa.String(32), nullable=True),\n        sa.Column(\n            \"created_date\",\n            sa.DateTime(),\n            nullable=False,\n            server_default=sa.func.current_timestamp(),\n        ),\n        sa.ForeignKeyConstraint(\n            [\"labeler_id\"],\n            [\"labeler.id\"],\n        ),\n        sa.PrimaryKeyConstraint(\"id\"),\n    )\n    op.create_index(op.f(\"prompt_labeler_id\"), \"prompt\", [\"labeler_id\"], unique=False)\n", "backend/alembic/versions/2022_12_28_1142-d24b37426857_post_ref_for_work_package.py": "\"\"\"post ref for work_package\n\nRevision ID: d24b37426857\nRevises: 3358eb6834e6\nCreate Date: 2022-12-28 11:42:26.773704\n\n\"\"\"\nimport sqlalchemy as sa\nimport sqlmodel\nfrom alembic import op\nfrom sqlalchemy.dialects import postgresql\n\n# revision identifiers, used by Alembic.\nrevision = \"d24b37426857\"\ndown_revision = \"3358eb6834e6\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.add_column(\"post\", sa.Column(\"depth\", sa.Integer(), server_default=sa.text(\"0\"), nullable=False))\n    op.add_column(\"post\", sa.Column(\"children_count\", sa.Integer(), server_default=sa.text(\"0\"), nullable=False))\n    op.add_column(\"post_reaction\", sa.Column(\"work_package_id\", postgresql.UUID(as_uuid=True), nullable=False))\n    op.drop_constraint(\"post_reaction_post_id_fkey\", \"post_reaction\", type_=\"foreignkey\")\n    op.create_foreign_key(None, \"post_reaction\", \"work_package\", [\"work_package_id\"], [\"id\"])\n    op.drop_column(\"post_reaction\", \"post_id\")\n    op.add_column(\"work_package\", sa.Column(\"done\", sa.Boolean(), server_default=sa.text(\"false\"), nullable=False))\n    op.add_column(\"work_package\", sa.Column(\"ack\", sa.Boolean(), nullable=True))\n    op.add_column(\"work_package\", sa.Column(\"frontend_ref_post_id\", sqlmodel.sql.sqltypes.AutoString(), nullable=True))\n    op.add_column(\"work_package\", sa.Column(\"thread_id\", sqlmodel.sql.sqltypes.GUID(), nullable=True))\n    op.add_column(\"work_package\", sa.Column(\"parent_post_id\", sqlmodel.sql.sqltypes.GUID(), nullable=True))\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_column(\"work_package\", \"parent_post_id\")\n    op.drop_column(\"work_package\", \"thread_id\")\n    op.drop_column(\"work_package\", \"frontend_ref_post_id\")\n    op.drop_column(\"work_package\", \"ack\")\n    op.drop_column(\"work_package\", \"done\")\n    op.add_column(\"post_reaction\", sa.Column(\"post_id\", postgresql.UUID(), autoincrement=False, nullable=False))\n    op.drop_constraint(None, \"post_reaction\", type_=\"foreignkey\")\n    op.create_foreign_key(\"post_reaction_post_id_fkey\", \"post_reaction\", \"post\", [\"post_id\"], [\"id\"])\n    op.drop_column(\"post_reaction\", \"work_package_id\")\n    op.drop_column(\"post\", \"children_count\")\n    op.drop_column(\"post\", \"depth\")\n    # ### end Alembic commands ###\n", "backend/alembic/versions/2023_02_01_1010-f60958968ff8_add_won_prompt_lottery_date_to_mts.py": "\"\"\"add won_prompt_lottery_date to mts\n\nRevision ID: f60958968ff8\nRevises: 7b8f0011e0b0\nCreate Date: 2023-02-01 10:10:38.301707\n\n\"\"\"\nimport sqlalchemy as sa\nfrom alembic import op\n\n# revision identifiers, used by Alembic.\nrevision = \"f60958968ff8\"\ndown_revision = \"7b8f0011e0b0\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.add_column(\"message_tree_state\", sa.Column(\"won_prompt_lottery_date\", sa.DateTime(timezone=True), nullable=True))\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_column(\"message_tree_state\", \"won_prompt_lottery_date\")\n    # ### end Alembic commands ###\n", "backend/alembic/versions/2023_01_15_1139-423557e869e4_add_indices_for_created_date.py": "\"\"\"add indices for created_date\n\nRevision ID: 423557e869e4\nRevises: 7c98102efbca\nCreate Date: 2023-01-15 11:39:10.407859\n\n\"\"\"\nfrom alembic import op\n\n# revision identifiers, used by Alembic.\nrevision = \"423557e869e4\"\ndown_revision = \"7c98102efbca\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.create_index(op.f(\"ix_message_created_date\"), \"message\", [\"created_date\"], unique=False)\n    op.create_index(op.f(\"ix_message_reaction_created_date\"), \"message_reaction\", [\"created_date\"], unique=False)\n    op.create_index(op.f(\"ix_text_labels_created_date\"), \"text_labels\", [\"created_date\"], unique=False)\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_index(op.f(\"ix_text_labels_created_date\"), table_name=\"text_labels\")\n    op.drop_index(op.f(\"ix_message_reaction_created_date\"), table_name=\"message_reaction\")\n    op.drop_index(op.f(\"ix_message_created_date\"), table_name=\"message\")\n    # ### end Alembic commands ###\n", "backend/alembic/versions/2023_01_07_1250-ba61fe17fb6e_added_frontend_type_to_api_client.py": "\"\"\"added frontend_type to api_client\n\nRevision ID: ba61fe17fb6e\nRevises: 20cd871f4ec7\nCreate Date: 2023-01-07 12:50:32.195930\n\n\"\"\"\nimport sqlalchemy as sa\nfrom alembic import op\n\n# revision identifiers, used by Alembic.\nrevision = \"ba61fe17fb6e\"\ndown_revision = \"20cd871f4ec7\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    op.add_column(\"api_client\", sa.Column(\"frontend_type\", sa.String(256), nullable=True))\n\n\ndef downgrade() -> None:\n    op.drop_column(\"api_client\", \"frontend_type\")\n", "backend/alembic/versions/2023_01_28_1157-49d8445b4c90_add_origin_column_to_message_tree_state.py": "\"\"\"add origin column to message_tree_state\n\nRevision ID: 49d8445b4c90\nRevises: f856bf19d32b\nCreate Date: 2023-01-28 11:57:45.580027\n\n\"\"\"\nimport sqlalchemy as sa\nfrom alembic import op\n\n# revision identifiers, used by Alembic.\nrevision = \"49d8445b4c90\"\ndown_revision = \"f856bf19d32b\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.add_column(\"message\", sa.Column(\"synthetic\", sa.Boolean(), server_default=sa.text(\"false\"), nullable=False))\n    op.add_column(\"message\", sa.Column(\"model_name\", sa.String(length=1024), nullable=True))\n    op.add_column(\"message_tree_state\", sa.Column(\"origin\", sa.String(length=1024), nullable=True))\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_column(\"message_tree_state\", \"origin\")\n    op.drop_column(\"message\", \"model_name\")\n    op.drop_column(\"message\", \"synthetic\")\n    # ### end Alembic commands ###\n", "backend/alembic/versions/2023_01_05_1144-d4161e384f83_added_messagetreestate_table.py": "\"\"\"added MessageTreeState table\n\nRevision ID: d4161e384f83\nRevises: 8d269bc4fdbd\nCreate Date: 2023-01-05 11:44:02.630633\n\n\"\"\"\nimport sqlalchemy as sa\nimport sqlmodel\nfrom alembic import op\nfrom sqlalchemy.dialects import postgresql\n\n# revision identifiers, used by Alembic.\nrevision = \"d4161e384f83\"\ndown_revision = \"8d269bc4fdbd\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.create_table(\n        \"message_tree_state\",\n        sa.Column(\"id\", postgresql.UUID(as_uuid=True), server_default=sa.text(\"gen_random_uuid()\"), nullable=False),\n        sa.Column(\"created_date\", sa.DateTime(), server_default=sa.text(\"CURRENT_TIMESTAMP\"), nullable=False),\n        sa.Column(\"deleted\", sa.Boolean(), server_default=sa.text(\"false\"), nullable=False),\n        sa.Column(\"message_tree_id\", sqlmodel.sql.sqltypes.GUID(), nullable=False),\n        sa.Column(\"state\", sqlmodel.sql.sqltypes.AutoString(length=128), nullable=False),\n        sa.Column(\"goal_tree_size\", sa.Integer(), nullable=False),\n        sa.Column(\"current_num_non_filtered_messages\", sa.Integer(), nullable=False),\n        sa.Column(\"max_depth\", sa.Integer(), nullable=False),\n        sa.PrimaryKeyConstraint(\"id\"),\n    )\n    op.create_index(\n        op.f(\"ix_message_tree_state_message_tree_id\"), \"message_tree_state\", [\"message_tree_id\"], unique=False\n    )\n    op.create_index(\"ix_message_tree_state_tree_id\", \"message_tree_state\", [\"message_tree_id\"], unique=True)\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_index(\"ix_message_tree_state_tree_id\", table_name=\"message_tree_state\")\n    op.drop_index(op.f(\"ix_message_tree_state_message_tree_id\"), table_name=\"message_tree_state\")\n    op.drop_table(\"message_tree_state\")\n    # ### end Alembic commands ###\n", "backend/alembic/versions/2023_02_01_0022-55361f323d12_add_tos_acceptance_date_to_user.py": "\"\"\"add tos_acceptance_date to user\n\nRevision ID: 55361f323d12\nRevises: 7b8f0011e0b0\nCreate Date: 2023-02-01 00:22:08.280251\n\n\"\"\"\nimport sqlalchemy as sa\nfrom alembic import op\nfrom sqlalchemy.dialects import postgresql\n\n# revision identifiers, used by Alembic.\nrevision = \"55361f323d12\"\ndown_revision = \"f60958968ff8\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.add_column(\"user\", sa.Column(\"tos_acceptance_date\", sa.DateTime(timezone=True), nullable=True))\n    op.drop_column(\"user_stats\", \"streak_days\")\n    op.drop_column(\"user_stats\", \"streak_last_day_date\")\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.add_column(\n        \"user_stats\", sa.Column(\"streak_last_day_date\", postgresql.TIMESTAMP(), autoincrement=False, nullable=True)\n    )\n    op.add_column(\"user_stats\", sa.Column(\"streak_days\", sa.INTEGER(), autoincrement=False, nullable=True))\n    op.drop_column(\"user\", \"tos_acceptance_date\")\n    # ### end Alembic commands ###\n", "backend/alembic/versions/2023_02_02_1544-4d7e0b0ebe84_add_troll_stats.py": "\"\"\"add troll_stats\n\nRevision ID: 4d7e0b0ebe84\nRevises: 9e7ec4a9e3f2\nCreate Date: 2023-02-02 15:44:12.647260\n\n\"\"\"\nimport sqlalchemy as sa\nimport sqlmodel\nfrom alembic import op\nfrom sqlalchemy.dialects import postgresql\n\n# revision identifiers, used by Alembic.\nrevision = \"4d7e0b0ebe84\"\ndown_revision = \"9e7ec4a9e3f2\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.create_table(\n        \"troll_stats\",\n        sa.Column(\"user_id\", postgresql.UUID(as_uuid=True), nullable=False),\n        sa.Column(\"base_date\", sa.DateTime(timezone=True), nullable=True),\n        sa.Column(\n            \"modified_date\", sa.DateTime(timezone=True), server_default=sa.text(\"CURRENT_TIMESTAMP\"), nullable=False\n        ),\n        sa.Column(\"time_frame\", sqlmodel.sql.sqltypes.AutoString(), nullable=False),\n        sa.Column(\"troll_score\", sa.Integer(), nullable=False),\n        sa.Column(\"rank\", sa.Integer(), nullable=True),\n        sa.Column(\"red_flags\", sa.Integer(), nullable=False),\n        sa.Column(\"upvotes\", sa.Integer(), nullable=False),\n        sa.Column(\"downvotes\", sa.Integer(), nullable=False),\n        sa.Column(\"spam_prompts\", sa.Integer(), nullable=False),\n        sa.Column(\"quality\", sa.Float(), nullable=True),\n        sa.Column(\"humor\", sa.Float(), nullable=True),\n        sa.Column(\"toxicity\", sa.Float(), nullable=True),\n        sa.Column(\"violence\", sa.Float(), nullable=True),\n        sa.Column(\"helpfulness\", sa.Float(), nullable=True),\n        sa.Column(\"spam\", sa.Integer(), nullable=False),\n        sa.Column(\"lang_mismach\", sa.Integer(), nullable=False),\n        sa.Column(\"not_appropriate\", sa.Integer(), nullable=False),\n        sa.Column(\"pii\", sa.Integer(), nullable=False),\n        sa.Column(\"hate_speech\", sa.Integer(), nullable=False),\n        sa.Column(\"sexual_content\", sa.Integer(), nullable=False),\n        sa.Column(\"political_content\", sa.Integer(), nullable=False),\n        sa.ForeignKeyConstraint([\"user_id\"], [\"user.id\"], ondelete=\"CASCADE\"),\n        sa.PrimaryKeyConstraint(\"user_id\", \"time_frame\"),\n    )\n    op.create_index(\"ix_troll_stats__timeframe__user_id\", \"troll_stats\", [\"time_frame\", \"user_id\"], unique=True)\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_index(\"ix_troll_stats__timeframe__user_id\", table_name=\"troll_stats\")\n    op.drop_table(\"troll_stats\")\n    # ### end Alembic commands ###\n", "backend/alembic/versions/2022_12_30_2054-abb47e9d145a_name_changes_person_user_post_message_.py": "\"\"\"name changes: person->user, post->message, work_package->task\n\nRevision ID: abb47e9d145a\nRevises: 73ce3675c1f5\nCreate Date: 2022-12-30 20:54:49.880568\n\n\"\"\"\nimport sqlalchemy as sa\nimport sqlmodel\nfrom alembic import op\nfrom sqlalchemy.dialects import postgresql\n\n# revision identifiers, used by Alembic.\nrevision = \"abb47e9d145a\"\ndown_revision = \"73ce3675c1f5\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    # clear DB\n    op.execute(\"DELETE FROM journal;\")\n    op.execute(\"DELETE FROM work_package;\")\n    op.execute(\"DELETE FROM post_reaction;\")\n    op.execute(\"DELETE FROM post;\")\n    op.execute(\"DELETE FROM person_stats;\")\n    op.execute(\"DELETE FROM person;\")\n    op.execute(\"DELETE FROM text_labels;\")\n\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.create_table(\n        \"user\",\n        sa.Column(\"id\", postgresql.UUID(as_uuid=True), server_default=sa.text(\"gen_random_uuid()\"), nullable=False),\n        sa.Column(\"created_date\", sa.DateTime(), server_default=sa.text(\"CURRENT_TIMESTAMP\"), nullable=False),\n        sa.Column(\"username\", sqlmodel.sql.sqltypes.AutoString(length=128), nullable=False),\n        sa.Column(\"auth_method\", sqlmodel.sql.sqltypes.AutoString(length=128), nullable=False),\n        sa.Column(\"display_name\", sqlmodel.sql.sqltypes.AutoString(length=256), nullable=False),\n        sa.Column(\"api_client_id\", sqlmodel.sql.sqltypes.GUID(), nullable=False),\n        sa.ForeignKeyConstraint(\n            [\"api_client_id\"],\n            [\"api_client.id\"],\n        ),\n        sa.PrimaryKeyConstraint(\"id\"),\n    )\n    op.create_index(\"ix_user_username\", \"user\", [\"api_client_id\", \"username\", \"auth_method\"], unique=True)\n    op.create_table(\n        \"message\",\n        sa.Column(\"id\", postgresql.UUID(as_uuid=True), server_default=sa.text(\"gen_random_uuid()\"), nullable=False),\n        sa.Column(\"created_date\", sa.DateTime(), server_default=sa.text(\"CURRENT_TIMESTAMP\"), nullable=False),\n        sa.Column(\"payload\", postgresql.JSONB(astext_type=sa.Text()), nullable=True),\n        sa.Column(\"depth\", sa.Integer(), server_default=sa.text(\"0\"), nullable=False),\n        sa.Column(\"children_count\", sa.Integer(), server_default=sa.text(\"0\"), nullable=False),\n        sa.Column(\"parent_id\", sqlmodel.sql.sqltypes.GUID(), nullable=True),\n        sa.Column(\"message_tree_id\", sqlmodel.sql.sqltypes.GUID(), nullable=False),\n        sa.Column(\"task_id\", sqlmodel.sql.sqltypes.GUID(), nullable=True),\n        sa.Column(\"user_id\", sqlmodel.sql.sqltypes.GUID(), nullable=True),\n        sa.Column(\"role\", sqlmodel.sql.sqltypes.AutoString(length=128), nullable=False),\n        sa.Column(\"api_client_id\", sqlmodel.sql.sqltypes.GUID(), nullable=False),\n        sa.Column(\"frontend_message_id\", sqlmodel.sql.sqltypes.AutoString(length=200), nullable=False),\n        sa.Column(\"payload_type\", sqlmodel.sql.sqltypes.AutoString(length=200), nullable=False),\n        sa.Column(\"lang\", sqlmodel.sql.sqltypes.AutoString(length=200), nullable=False),\n        sa.ForeignKeyConstraint(\n            [\"api_client_id\"],\n            [\"api_client.id\"],\n        ),\n        sa.ForeignKeyConstraint(\n            [\"user_id\"],\n            [\"user.id\"],\n        ),\n        sa.PrimaryKeyConstraint(\"id\"),\n    )\n    op.create_index(\"ix_message_frontend_message_id\", \"message\", [\"api_client_id\", \"frontend_message_id\"], unique=True)\n    op.create_index(op.f(\"ix_message_message_tree_id\"), \"message\", [\"message_tree_id\"], unique=False)\n    op.create_index(op.f(\"ix_message_task_id\"), \"message\", [\"task_id\"], unique=False)\n    op.create_index(op.f(\"ix_message_user_id\"), \"message\", [\"user_id\"], unique=False)\n    op.create_table(\n        \"task\",\n        sa.Column(\"id\", postgresql.UUID(as_uuid=True), server_default=sa.text(\"gen_random_uuid()\"), nullable=False),\n        sa.Column(\"created_date\", sa.DateTime(), server_default=sa.text(\"CURRENT_TIMESTAMP\"), nullable=False),\n        sa.Column(\"expiry_date\", sa.DateTime(), nullable=True),\n        sa.Column(\"payload\", postgresql.JSONB(astext_type=sa.Text()), nullable=False),\n        sa.Column(\"done\", sa.Boolean(), server_default=sa.text(\"false\"), nullable=False),\n        sa.Column(\"collective\", sa.Boolean(), server_default=sa.text(\"false\"), nullable=False),\n        sa.Column(\"user_id\", sqlmodel.sql.sqltypes.GUID(), nullable=True),\n        sa.Column(\"payload_type\", sqlmodel.sql.sqltypes.AutoString(length=200), nullable=False),\n        sa.Column(\"api_client_id\", sqlmodel.sql.sqltypes.GUID(), nullable=False),\n        sa.Column(\"ack\", sa.Boolean(), nullable=True),\n        sa.Column(\"frontend_message_id\", sqlmodel.sql.sqltypes.AutoString(), nullable=True),\n        sa.Column(\"message_tree_id\", sqlmodel.sql.sqltypes.GUID(), nullable=True),\n        sa.Column(\"parent_message_id\", sqlmodel.sql.sqltypes.GUID(), nullable=True),\n        sa.ForeignKeyConstraint(\n            [\"api_client_id\"],\n            [\"api_client.id\"],\n        ),\n        sa.ForeignKeyConstraint(\n            [\"user_id\"],\n            [\"user.id\"],\n        ),\n        sa.PrimaryKeyConstraint(\"id\"),\n    )\n    op.create_index(op.f(\"ix_task_user_id\"), \"task\", [\"user_id\"], unique=False)\n    op.create_table(\n        \"user_stats\",\n        sa.Column(\"user_id\", postgresql.UUID(as_uuid=True), nullable=False),\n        sa.Column(\"modified_date\", sa.DateTime(), server_default=sa.text(\"CURRENT_TIMESTAMP\"), nullable=False),\n        sa.Column(\"leader_score\", sa.Integer(), nullable=False),\n        sa.Column(\"reactions\", sa.Integer(), nullable=False),\n        sa.Column(\"messages\", sa.Integer(), nullable=False),\n        sa.Column(\"upvotes\", sa.Integer(), nullable=False),\n        sa.Column(\"downvotes\", sa.Integer(), nullable=False),\n        sa.Column(\"task_reward\", sa.Integer(), nullable=False),\n        sa.Column(\"compare_wins\", sa.Integer(), nullable=False),\n        sa.Column(\"compare_losses\", sa.Integer(), nullable=False),\n        sa.ForeignKeyConstraint(\n            [\"user_id\"],\n            [\"user.id\"],\n        ),\n        sa.PrimaryKeyConstraint(\"user_id\"),\n    )\n    op.create_table(\n        \"message_reaction\",\n        sa.Column(\"task_id\", postgresql.UUID(as_uuid=True), nullable=False),\n        sa.Column(\"user_id\", postgresql.UUID(as_uuid=True), nullable=False),\n        sa.Column(\"created_date\", sa.DateTime(), server_default=sa.text(\"CURRENT_TIMESTAMP\"), nullable=False),\n        sa.Column(\"payload\", postgresql.JSONB(astext_type=sa.Text()), nullable=False),\n        sa.Column(\"payload_type\", sqlmodel.sql.sqltypes.AutoString(length=200), nullable=False),\n        sa.Column(\"api_client_id\", sqlmodel.sql.sqltypes.GUID(), nullable=False),\n        sa.ForeignKeyConstraint(\n            [\"api_client_id\"],\n            [\"api_client.id\"],\n        ),\n        sa.ForeignKeyConstraint(\n            [\"task_id\"],\n            [\"task.id\"],\n        ),\n        sa.ForeignKeyConstraint(\n            [\"user_id\"],\n            [\"user.id\"],\n        ),\n        sa.PrimaryKeyConstraint(\"task_id\", \"user_id\"),\n    )\n\n    op.drop_constraint(\"text_labels_post_id_fkey\", \"text_labels\", type_=\"foreignkey\")\n    op.drop_constraint(\"journal_post_id_fkey\", \"journal\", type_=\"foreignkey\")\n    op.drop_constraint(\"journal_person_id_fkey\", \"journal\", type_=\"foreignkey\")\n\n    op.drop_table(\"post_reaction\")\n\n    op.drop_index(\"ix_post_frontend_post_id\", table_name=\"post\")\n    op.drop_index(\"ix_post_person_id\", table_name=\"post\")\n    op.drop_index(\"ix_post_thread_id\", table_name=\"post\")\n    op.drop_index(\"ix_post_workpackage_id\", table_name=\"post\")\n    op.drop_table(\"post\")\n\n    op.drop_index(\"ix_work_package_person_id\", table_name=\"work_package\")\n    op.drop_table(\"work_package\")\n    op.drop_table(\"person_stats\")\n\n    op.drop_index(\"ix_person_username\", table_name=\"person\")\n    op.drop_table(\"person\")\n\n    op.add_column(\"journal\", sa.Column(\"user_id\", sqlmodel.sql.sqltypes.GUID(), nullable=True))\n    op.add_column(\"journal\", sa.Column(\"message_id\", sqlmodel.sql.sqltypes.GUID(), nullable=True))\n    op.drop_index(\"ix_journal_person_id\", table_name=\"journal\")\n    op.create_index(op.f(\"ix_journal_user_id\"), \"journal\", [\"user_id\"], unique=False)\n\n    op.create_foreign_key(None, \"journal\", \"user\", [\"user_id\"], [\"id\"])\n    op.create_foreign_key(None, \"journal\", \"message\", [\"message_id\"], [\"id\"])\n    op.drop_column(\"journal\", \"person_id\")\n    op.drop_column(\"journal\", \"post_id\")\n    op.add_column(\"text_labels\", sa.Column(\"message_id\", postgresql.UUID(as_uuid=True), nullable=True))\n    op.create_foreign_key(None, \"text_labels\", \"message\", [\"message_id\"], [\"id\"])\n    op.drop_column(\"text_labels\", \"post_id\")\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    # clear DB\n    op.execute(\"DELETE FROM journal;\")\n    op.execute(\"DELETE FROM message_reaction;\")\n    op.execute(\"DELETE FROM task;\")\n    op.execute(\"DELETE FROM message;\")\n    op.execute(\"DELETE FROM user_stats;\")\n    op.execute('DELETE FROM \"user\";')\n    op.execute(\"DELETE FROM text_labels;\")\n\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.add_column(\"text_labels\", sa.Column(\"post_id\", postgresql.UUID(), autoincrement=False, nullable=True))\n    op.drop_constraint(\"text_labels_message_id_fkey\", \"text_labels\", type_=\"foreignkey\")\n\n    op.drop_column(\"text_labels\", \"message_id\")\n    op.add_column(\"journal\", sa.Column(\"post_id\", postgresql.UUID(), autoincrement=False, nullable=True))\n    op.add_column(\"journal\", sa.Column(\"person_id\", postgresql.UUID(), autoincrement=False, nullable=True))\n    op.drop_constraint(\"journal_message_id_fkey\", \"journal\", type_=\"foreignkey\")\n    op.drop_constraint(\"journal_user_id_fkey\", \"journal\", type_=\"foreignkey\")\n\n    op.drop_index(op.f(\"ix_journal_user_id\"), table_name=\"journal\")\n    op.create_index(\"ix_journal_person_id\", \"journal\", [\"person_id\"], unique=False)\n    op.drop_column(\"journal\", \"message_id\")\n    op.drop_column(\"journal\", \"user_id\")\n\n    op.create_table(\n        \"person\",\n        sa.Column(\n            \"id\", postgresql.UUID(), server_default=sa.text(\"gen_random_uuid()\"), autoincrement=False, nullable=False\n        ),\n        sa.Column(\"username\", sa.VARCHAR(length=128), autoincrement=False, nullable=False),\n        sa.Column(\"display_name\", sa.VARCHAR(length=256), autoincrement=False, nullable=False),\n        sa.Column(\n            \"created_date\",\n            postgresql.TIMESTAMP(),\n            server_default=sa.text(\"CURRENT_TIMESTAMP\"),\n            autoincrement=False,\n            nullable=False,\n        ),\n        sa.Column(\"api_client_id\", postgresql.UUID(), autoincrement=False, nullable=False),\n        sa.Column(\"auth_method\", sa.VARCHAR(length=128), autoincrement=False, nullable=False),\n        sa.ForeignKeyConstraint([\"api_client_id\"], [\"api_client.id\"], name=\"person_api_client_id_fkey\"),\n        sa.PrimaryKeyConstraint(\"id\", name=\"person_pkey\"),\n    )\n    op.create_table(\n        \"person_stats\",\n        sa.Column(\"person_id\", postgresql.UUID(), autoincrement=False, nullable=False),\n        sa.Column(\"leader_score\", sa.INTEGER(), autoincrement=False, nullable=False),\n        sa.Column(\n            \"modified_date\",\n            postgresql.TIMESTAMP(),\n            server_default=sa.text(\"CURRENT_TIMESTAMP\"),\n            autoincrement=False,\n            nullable=False,\n        ),\n        sa.Column(\"reactions\", sa.INTEGER(), autoincrement=False, nullable=False),\n        sa.Column(\"posts\", sa.INTEGER(), autoincrement=False, nullable=False),\n        sa.Column(\"upvotes\", sa.INTEGER(), autoincrement=False, nullable=False),\n        sa.Column(\"downvotes\", sa.INTEGER(), autoincrement=False, nullable=False),\n        sa.Column(\"work_reward\", sa.INTEGER(), autoincrement=False, nullable=False),\n        sa.Column(\"compare_wins\", sa.INTEGER(), autoincrement=False, nullable=False),\n        sa.Column(\"compare_losses\", sa.INTEGER(), autoincrement=False, nullable=False),\n        sa.ForeignKeyConstraint([\"person_id\"], [\"person.id\"], name=\"person_stats_person_id_fkey\"),\n        sa.PrimaryKeyConstraint(\"person_id\", name=\"person_stats_pkey\"),\n    )\n    op.create_table(\n        \"work_package\",\n        sa.Column(\n            \"id\", postgresql.UUID(), server_default=sa.text(\"gen_random_uuid()\"), autoincrement=False, nullable=False\n        ),\n        sa.Column(\n            \"created_date\",\n            postgresql.TIMESTAMP(),\n            server_default=sa.text(\"CURRENT_TIMESTAMP\"),\n            autoincrement=False,\n            nullable=False,\n        ),\n        sa.Column(\"expiry_date\", postgresql.TIMESTAMP(), autoincrement=False, nullable=True),\n        sa.Column(\"person_id\", postgresql.UUID(), autoincrement=False, nullable=True),\n        sa.Column(\"payload_type\", sa.VARCHAR(length=200), autoincrement=False, nullable=False),\n        sa.Column(\"payload\", postgresql.JSONB(astext_type=sa.Text()), autoincrement=False, nullable=False),\n        sa.Column(\"api_client_id\", postgresql.UUID(), autoincrement=False, nullable=False),\n        sa.Column(\"done\", sa.BOOLEAN(), server_default=sa.text(\"false\"), autoincrement=False, nullable=False),\n        sa.Column(\"ack\", sa.BOOLEAN(), autoincrement=False, nullable=True),\n        sa.Column(\"frontend_ref_post_id\", sa.VARCHAR(), autoincrement=False, nullable=True),\n        sa.Column(\"thread_id\", postgresql.UUID(), autoincrement=False, nullable=True),\n        sa.Column(\"parent_post_id\", postgresql.UUID(), autoincrement=False, nullable=True),\n        sa.Column(\"collective\", sa.BOOLEAN(), server_default=sa.text(\"false\"), autoincrement=False, nullable=False),\n        sa.ForeignKeyConstraint([\"api_client_id\"], [\"api_client.id\"], name=\"work_package_api_client_id_fkey\"),\n        sa.ForeignKeyConstraint([\"person_id\"], [\"person.id\"], name=\"work_package_person_id_fkey\"),\n        sa.PrimaryKeyConstraint(\"id\", name=\"work_package_pkey\"),\n    )\n    op.create_index(\"ix_work_package_person_id\", \"work_package\", [\"person_id\"], unique=False)\n    op.create_table(\n        \"post\",\n        sa.Column(\n            \"id\", postgresql.UUID(), server_default=sa.text(\"gen_random_uuid()\"), autoincrement=False, nullable=False\n        ),\n        sa.Column(\"parent_id\", postgresql.UUID(), autoincrement=False, nullable=True),\n        sa.Column(\"thread_id\", postgresql.UUID(), autoincrement=False, nullable=False),\n        sa.Column(\"workpackage_id\", postgresql.UUID(), autoincrement=False, nullable=True),\n        sa.Column(\"person_id\", postgresql.UUID(), autoincrement=False, nullable=True),\n        sa.Column(\"api_client_id\", postgresql.UUID(), autoincrement=False, nullable=False),\n        sa.Column(\"role\", sa.VARCHAR(length=128), autoincrement=False, nullable=False),\n        sa.Column(\"frontend_post_id\", sa.VARCHAR(length=200), autoincrement=False, nullable=False),\n        sa.Column(\n            \"created_date\",\n            postgresql.TIMESTAMP(),\n            server_default=sa.text(\"CURRENT_TIMESTAMP\"),\n            autoincrement=False,\n            nullable=False,\n        ),\n        sa.Column(\"payload_type\", sa.VARCHAR(length=200), autoincrement=False, nullable=False),\n        sa.Column(\"payload\", postgresql.JSONB(astext_type=sa.Text()), autoincrement=False, nullable=True),\n        sa.Column(\"depth\", sa.INTEGER(), server_default=sa.text(\"0\"), autoincrement=False, nullable=False),\n        sa.Column(\"children_count\", sa.INTEGER(), server_default=sa.text(\"0\"), autoincrement=False, nullable=False),\n        sa.Column(\"lang\", sa.VARCHAR(length=200), autoincrement=False, nullable=False),\n        sa.ForeignKeyConstraint([\"api_client_id\"], [\"api_client.id\"], name=\"post_api_client_id_fkey\"),\n        sa.ForeignKeyConstraint([\"person_id\"], [\"person.id\"], name=\"post_person_id_fkey\"),\n        sa.PrimaryKeyConstraint(\"id\", name=\"post_pkey\"),\n    )\n    op.create_index(\"ix_post_workpackage_id\", \"post\", [\"workpackage_id\"], unique=False)\n    op.create_index(\"ix_post_thread_id\", \"post\", [\"thread_id\"], unique=False)\n    op.create_index(\"ix_post_person_id\", \"post\", [\"person_id\"], unique=False)\n    op.create_index(\"ix_post_frontend_post_id\", \"post\", [\"api_client_id\", \"frontend_post_id\"], unique=False)\n\n    op.create_table(\n        \"post_reaction\",\n        sa.Column(\"person_id\", postgresql.UUID(), autoincrement=False, nullable=False),\n        sa.Column(\n            \"created_date\",\n            postgresql.TIMESTAMP(),\n            server_default=sa.text(\"CURRENT_TIMESTAMP\"),\n            autoincrement=False,\n            nullable=False,\n        ),\n        sa.Column(\"payload_type\", sa.VARCHAR(length=200), autoincrement=False, nullable=False),\n        sa.Column(\"payload\", postgresql.JSONB(astext_type=sa.Text()), autoincrement=False, nullable=False),\n        sa.Column(\"api_client_id\", postgresql.UUID(), autoincrement=False, nullable=False),\n        sa.Column(\"work_package_id\", postgresql.UUID(), autoincrement=False, nullable=False),\n        sa.ForeignKeyConstraint([\"api_client_id\"], [\"api_client.id\"], name=\"post_reaction_api_client_id_fkey\"),\n        sa.ForeignKeyConstraint([\"person_id\"], [\"person.id\"], name=\"post_reaction_person_id_fkey\"),\n        sa.ForeignKeyConstraint([\"work_package_id\"], [\"work_package.id\"], name=\"post_reaction_work_package_id_fkey\"),\n    )\n\n    op.create_index(\"ix_person_username\", \"person\", [\"api_client_id\", \"username\", \"auth_method\"], unique=False)\n    op.create_foreign_key(\"text_labels_post_id_fkey\", \"text_labels\", \"post\", [\"post_id\"], [\"id\"])\n    op.create_foreign_key(\"journal_person_id_fkey\", \"journal\", \"person\", [\"person_id\"], [\"id\"])\n    op.create_foreign_key(\"journal_post_id_fkey\", \"journal\", \"post\", [\"post_id\"], [\"id\"])\n\n    op.drop_table(\"message_reaction\")\n    op.drop_table(\"user_stats\")\n    op.drop_index(op.f(\"ix_task_user_id\"), table_name=\"task\")\n    op.drop_table(\"task\")\n    op.drop_index(op.f(\"ix_message_user_id\"), table_name=\"message\")\n    op.drop_index(op.f(\"ix_message_task_id\"), table_name=\"message\")\n    op.drop_index(op.f(\"ix_message_message_tree_id\"), table_name=\"message\")\n    op.drop_index(\"ix_message_frontend_message_id\", table_name=\"message\")\n    op.drop_table(\"message\")\n    op.drop_index(\"ix_user_username\", table_name=\"user\")\n    op.drop_table(\"user\")\n    # ### end Alembic commands ###\n", "backend/alembic/versions/2023_01_14_1509-619255ae9076_add_rank_to_message_table.py": "\"\"\"add rank to message table\n\nRevision ID: 619255ae9076\nRevises: bcc2fe18d214\nCreate Date: 2023-01-14 15:09:03.462482\n\n\"\"\"\nimport sqlalchemy as sa\nimport sqlmodel\nfrom alembic import op\nfrom sqlalchemy.dialects import postgresql\n\n# revision identifiers, used by Alembic.\nrevision = \"619255ae9076\"\ndown_revision = \"bcc2fe18d214\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.add_column(\"message\", sa.Column(\"rank\", sa.Integer(), nullable=True))\n    op.add_column(\"message_toxicity\", sa.Column(\"score\", sa.Float(), nullable=True))\n    op.add_column(\"message_toxicity\", sa.Column(\"label\", sqlmodel.sql.sqltypes.AutoString(length=256), nullable=False))\n    op.drop_column(\"message_toxicity\", \"toxicity\")\n    op.add_column(\"user_stats\", sa.Column(\"time_frame\", sqlmodel.sql.sqltypes.AutoString(), nullable=False))\n    op.add_column(\"user_stats\", sa.Column(\"prompts\", sa.Integer(), nullable=False))\n    op.add_column(\"user_stats\", sa.Column(\"replies_assistant\", sa.Integer(), nullable=False))\n    op.add_column(\"user_stats\", sa.Column(\"replies_prompter\", sa.Integer(), nullable=False))\n    op.add_column(\"user_stats\", sa.Column(\"labels_simple\", sa.Integer(), nullable=False))\n    op.add_column(\"user_stats\", sa.Column(\"labels_full\", sa.Integer(), nullable=False))\n    op.add_column(\"user_stats\", sa.Column(\"rankings_total\", sa.Integer(), nullable=False))\n    op.add_column(\"user_stats\", sa.Column(\"rankings_good\", sa.Integer(), nullable=False))\n    op.add_column(\"user_stats\", sa.Column(\"accepted_prompts\", sa.Integer(), nullable=False))\n    op.add_column(\"user_stats\", sa.Column(\"accepted_replies_assistant\", sa.Integer(), nullable=False))\n    op.add_column(\"user_stats\", sa.Column(\"accepted_replies_prompter\", sa.Integer(), nullable=False))\n    op.add_column(\"user_stats\", sa.Column(\"reply_assistant_ranked_1\", sa.Integer(), nullable=False))\n    op.add_column(\"user_stats\", sa.Column(\"reply_assistant_ranked_2\", sa.Integer(), nullable=False))\n    op.add_column(\"user_stats\", sa.Column(\"reply_assistant_ranked_3\", sa.Integer(), nullable=False))\n    op.add_column(\"user_stats\", sa.Column(\"reply_prompter_ranked_1\", sa.Integer(), nullable=False))\n    op.add_column(\"user_stats\", sa.Column(\"reply_prompter_ranked_2\", sa.Integer(), nullable=False))\n    op.add_column(\"user_stats\", sa.Column(\"reply_prompter_ranked_3\", sa.Integer(), nullable=False))\n    op.add_column(\"user_stats\", sa.Column(\"streak_last_day_date\", sa.DateTime(), nullable=True))\n    op.add_column(\"user_stats\", sa.Column(\"streak_days\", sa.Integer(), nullable=True))\n    op.drop_column(\"user_stats\", \"messages\")\n    op.drop_column(\"user_stats\", \"upvotes\")\n    op.drop_column(\"user_stats\", \"task_reward\")\n    op.drop_column(\"user_stats\", \"compare_wins\")\n    op.drop_column(\"user_stats\", \"compare_losses\")\n    op.drop_column(\"user_stats\", \"downvotes\")\n    op.drop_column(\"user_stats\", \"reactions\")\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.add_column(\"user_stats\", sa.Column(\"reactions\", sa.INTEGER(), autoincrement=False, nullable=False))\n    op.add_column(\"user_stats\", sa.Column(\"downvotes\", sa.INTEGER(), autoincrement=False, nullable=False))\n    op.add_column(\"user_stats\", sa.Column(\"compare_losses\", sa.INTEGER(), autoincrement=False, nullable=False))\n    op.add_column(\"user_stats\", sa.Column(\"compare_wins\", sa.INTEGER(), autoincrement=False, nullable=False))\n    op.add_column(\"user_stats\", sa.Column(\"task_reward\", sa.INTEGER(), autoincrement=False, nullable=False))\n    op.add_column(\"user_stats\", sa.Column(\"upvotes\", sa.INTEGER(), autoincrement=False, nullable=False))\n    op.add_column(\"user_stats\", sa.Column(\"messages\", sa.INTEGER(), autoincrement=False, nullable=False))\n    op.drop_column(\"user_stats\", \"streak_days\")\n    op.drop_column(\"user_stats\", \"streak_last_day_date\")\n    op.drop_column(\"user_stats\", \"reply_prompter_ranked_3\")\n    op.drop_column(\"user_stats\", \"reply_prompter_ranked_2\")\n    op.drop_column(\"user_stats\", \"reply_prompter_ranked_1\")\n    op.drop_column(\"user_stats\", \"reply_assistant_ranked_3\")\n    op.drop_column(\"user_stats\", \"reply_assistant_ranked_2\")\n    op.drop_column(\"user_stats\", \"reply_assistant_ranked_1\")\n    op.drop_column(\"user_stats\", \"accepted_replies_prompter\")\n    op.drop_column(\"user_stats\", \"accepted_replies_assistant\")\n    op.drop_column(\"user_stats\", \"accepted_prompts\")\n    op.drop_column(\"user_stats\", \"rankings_good\")\n    op.drop_column(\"user_stats\", \"rankings_total\")\n    op.drop_column(\"user_stats\", \"labels_full\")\n    op.drop_column(\"user_stats\", \"labels_simple\")\n    op.drop_column(\"user_stats\", \"replies_prompter\")\n    op.drop_column(\"user_stats\", \"replies_assistant\")\n    op.drop_column(\"user_stats\", \"prompts\")\n    op.drop_column(\"user_stats\", \"time_frame\")\n    op.add_column(\n        \"message_toxicity\",\n        sa.Column(\"toxicity\", postgresql.DOUBLE_PRECISION(precision=53), autoincrement=False, nullable=True),\n    )\n    op.drop_column(\"message_toxicity\", \"label\")\n    op.drop_column(\"message_toxicity\", \"score\")\n    op.drop_column(\"message\", \"rank\")\n    # ### end Alembic commands ###\n", "backend/alembic/versions/2023_01_05_1745-20cd871f4ec7_added_user_to_textlabels.py": "\"\"\"Added user to TextLabels\n\nRevision ID: 20cd871f4ec7\nRevises: d4161e384f83\nCreate Date: 2023-01-05 17:45:15.696468\n\n\"\"\"\nimport sqlalchemy as sa\nfrom alembic import op\nfrom sqlalchemy.dialects import postgresql\n\n# revision identifiers, used by Alembic.\nrevision = \"20cd871f4ec7\"\ndown_revision = \"3b0adfadbef9\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.add_column(\"text_labels\", sa.Column(\"user_id\", postgresql.UUID(as_uuid=True), nullable=False))\n    op.create_foreign_key(None, \"text_labels\", \"user\", [\"user_id\"], [\"id\"])\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_constraint(None, \"text_labels\", type_=\"foreignkey\")\n    op.drop_column(\"text_labels\", \"user_id\")\n    # ### end Alembic commands ###\n", "backend/alembic/versions/2023_06_06_1505-c181661eba3a_add_message_revisions.py": "\"\"\"add_message_revisions\n\nRevision ID: c181661eba3a\nRevises: 1b6e3ae16e9d\nCreate Date: 2023-06-06 15:05:58.079120\n\n\"\"\"\nimport sqlalchemy as sa\nfrom alembic import op\nfrom sqlalchemy.dialects import postgresql\n\n# revision identifiers, used by Alembic.\nrevision = \"c181661eba3a\"\ndown_revision = \"1b6e3ae16e9d\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.create_table(\n        \"message_revision\",\n        sa.Column(\"id\", postgresql.UUID(as_uuid=True), nullable=False),\n        sa.Column(\"payload\", postgresql.JSONB(astext_type=sa.Text()), nullable=True),\n        sa.Column(\"message_id\", postgresql.UUID(as_uuid=True), nullable=False),\n        sa.Column(\"user_id\", postgresql.UUID(as_uuid=True), nullable=True),\n        sa.Column(\n            \"created_date\", sa.DateTime(timezone=True), server_default=sa.text(\"CURRENT_TIMESTAMP\"), nullable=True\n        ),\n        sa.ForeignKeyConstraint(\n            [\"message_id\"],\n            [\"message.id\"],\n        ),\n        sa.ForeignKeyConstraint(\n            [\"user_id\"],\n            [\"user.id\"],\n        ),\n        sa.PrimaryKeyConstraint(\"id\"),\n    )\n    op.create_index(op.f(\"ix_message_revision_message_id\"), \"message_revision\", [\"message_id\"], unique=False)\n    op.add_column(\"message\", sa.Column(\"edited\", sa.Boolean(), server_default=sa.text(\"false\"), nullable=False))\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_column(\"message\", \"edited\")\n    op.drop_index(op.f(\"ix_message_revision_message_id\"), table_name=\"message_revision\")\n    op.drop_table(\"message_revision\")\n    # ### end Alembic commands ###\n", "backend/oasst_backend/celery_worker.py": "import os\n\nfrom celery import Celery\nfrom loguru import logger\n\n\"\"\"\nTo run the worker run `celery run -A oasst_backend.celery_worker worker -l INFO`\nin the parent directory of this file, add -B to embed the beat scheduler inside\nthe worker.\n\"\"\"\napp = Celery(\n    \"oasst_worker\",\n    broker=os.environ.get(\"CELERY_BROKER_URL\", \"redis://localhost:6379/0\"),\n    backend=os.environ.get(\"CELERY_RESULT_BACKEND\", \"redis://localhost:6379/0\"),\n    include=[\"oasst_backend.scheduled_tasks\"],\n)\n\nlogger.info(f\"celery.conf.broker_url {app.conf.broker_url}, app.conf.result_backend{app.conf.result_backend}\")\n\n# see https://docs.celeryq.dev/en/stable/userguide/periodic-tasks.html\napp.conf.beat_schedule = {\n    \"reset-user-streak\": {\n        \"task\": \"periodic_user_streak_reset\",\n        \"schedule\": 60.0 * 60.0 * 4,  # in seconds, every 4h\n    },\n    \"update-search-vectors\": {\n        \"task\": \"update_search_vectors\",\n        \"schedule\": 60.0 * 20.0,\n        \"args\": (1000,),  # (batch_size,)\n    },\n}\napp.conf.timezone = \"UTC\"\n", "backend/oasst_backend/prompt_repository.py": "import random\nimport re\nfrom collections import defaultdict\nfrom datetime import datetime, timedelta\nfrom http import HTTPStatus\nfrom typing import Optional\nfrom uuid import UUID, uuid4\n\nimport oasst_backend.models.db_payload as db_payload\nimport sqlalchemy.dialects.postgresql as pg\nfrom loguru import logger\nfrom oasst_backend.api.deps import FrontendUserId\nfrom oasst_backend.config import settings\nfrom oasst_backend.journal_writer import JournalWriter\nfrom oasst_backend.models import (\n    ApiClient,\n    FlaggedMessage,\n    Message,\n    MessageEmbedding,\n    MessageEmoji,\n    MessageReaction,\n    MessageRevision,\n    MessageToxicity,\n    MessageTreeState,\n    Task,\n    TextLabels,\n    User,\n    message_tree_state,\n)\nfrom oasst_backend.models.payload_column_type import PayloadContainer\nfrom oasst_backend.task_repository import TaskRepository, validate_frontend_message_id\nfrom oasst_backend.user_repository import UserRepository\nfrom oasst_backend.utils.database_utils import CommitMode, db_lang_to_postgres_ts_lang, managed_tx_method\nfrom oasst_backend.utils.discord import send_new_report_message\nfrom oasst_shared.exceptions import OasstError, OasstErrorCode\nfrom oasst_shared.schemas import protocol as protocol_schema\nfrom oasst_shared.schemas.protocol import SystemStats\nfrom oasst_shared.utils import unaware_to_utc, utcnow\nfrom sqlalchemy.orm import Query\nfrom sqlalchemy.orm.attributes import flag_modified\nfrom sqlmodel import JSON, Session, and_, func, literal_column, not_, or_, text, update\n\n_task_type_and_reaction = (\n    (\n        (db_payload.PrompterReplyPayload, db_payload.AssistantReplyPayload),\n        protocol_schema.EmojiCode.skip_reply,\n    ),\n    (\n        (db_payload.LabelInitialPromptPayload, db_payload.LabelConversationReplyPayload),\n        protocol_schema.EmojiCode.skip_labeling,\n    ),\n    (\n        (db_payload.RankInitialPromptsPayload, db_payload.RankConversationRepliesPayload),\n        protocol_schema.EmojiCode.skip_ranking,\n    ),\n)\n\n\nclass PromptRepository:\n    def __init__(\n        self,\n        db: Session,\n        api_client: ApiClient,\n        client_user: Optional[protocol_schema.User] = None,\n        *,\n        user_repository: Optional[UserRepository] = None,\n        task_repository: Optional[TaskRepository] = None,\n        user_id: Optional[UUID] = None,\n        auth_method: Optional[str] = None,\n        username: Optional[str] = None,\n        frontend_user: Optional[FrontendUserId] = None,\n    ):\n        self.db = db\n        self.api_client = api_client\n        self.user_repository = user_repository or UserRepository(db, api_client)\n\n        if frontend_user and not auth_method and not username:\n            auth_method, username = frontend_user\n\n        if user_id:\n            self.user = self.user_repository.get_user(id=user_id)\n        elif auth_method and username:\n            self.user = self.user_repository.query_frontend_user(auth_method=auth_method, username=username)\n        else:\n            self.user = self.user_repository.lookup_client_user(client_user, create_missing=True)\n        self.user_id = self.user.id if self.user else None\n        logger.debug(f\"PromptRepository(api_client_id={self.api_client.id}, {self.user_id=})\")\n        self.task_repository = task_repository or TaskRepository(\n            db, api_client, client_user, user_repository=self.user_repository\n        )\n        self.journal = JournalWriter(db, api_client, self.user)\n\n    def ensure_user_is_enabled(self):\n        if self.user is None or self.user_id is None:\n            raise OasstError(\"User required\", OasstErrorCode.USER_NOT_SPECIFIED)\n\n        if self.user.deleted or not self.user.enabled:\n            raise OasstError(\"User account disabled\", OasstErrorCode.USER_DISABLED, HTTPStatus.SERVICE_UNAVAILABLE)\n\n        if self.user.tos_acceptance_date is None and not settings.DEBUG_IGNORE_TOS_ACCEPTANCE:\n            raise OasstError(\n                \"User has not accepted terms of service.\",\n                OasstErrorCode.USER_HAS_NOT_ACCEPTED_TOS,\n                HTTPStatus.UNAVAILABLE_FOR_LEGAL_REASONS,\n            )\n\n    def fetch_message_by_frontend_message_id(self, frontend_message_id: str, fail_if_missing: bool = True) -> Message:\n        validate_frontend_message_id(frontend_message_id)\n        message: Message = (\n            self.db.query(Message)\n            .filter(Message.api_client_id == self.api_client.id, Message.frontend_message_id == frontend_message_id)\n            .one_or_none()\n        )\n        if fail_if_missing and message is None:\n            raise OasstError(\n                f\"Message with frontend_message_id {frontend_message_id} not found.\",\n                OasstErrorCode.MESSAGE_NOT_FOUND,\n                HTTPStatus.NOT_FOUND,\n            )\n        return message\n\n    @managed_tx_method(CommitMode.FLUSH)\n    def insert_message(\n        self,\n        *,\n        message_id: UUID,\n        frontend_message_id: str,\n        parent_id: UUID,\n        message_tree_id: UUID,\n        task_id: UUID,\n        role: str,\n        payload: db_payload.MessagePayload,\n        lang: str,\n        payload_type: str = None,\n        depth: int = 0,\n        review_count: int = 0,\n        review_result: bool = None,\n        deleted: bool = False,\n    ) -> Message:\n        if payload_type is None:\n            if payload is None:\n                payload_type = \"null\"\n            else:\n                payload_type = type(payload).__name__\n\n        message = Message(\n            id=message_id,\n            parent_id=parent_id,\n            message_tree_id=message_tree_id,\n            task_id=task_id,\n            user_id=self.user_id,\n            role=role,\n            frontend_message_id=frontend_message_id,\n            api_client_id=self.api_client.id,\n            payload_type=payload_type,\n            payload=PayloadContainer(payload=payload),\n            lang=lang,\n            depth=depth,\n            review_count=review_count,\n            review_result=review_result,\n            deleted=deleted,\n        )\n        self.db.add(message)\n        return message\n\n    @managed_tx_method(CommitMode.FLUSH)\n    def insert_revision(\n        self,\n        payload: db_payload.MessagePayload,\n        message_id: UUID,\n        user_id: UUID,\n        created_date: datetime,\n    ) -> MessageRevision:\n        message_revision = MessageRevision(\n            payload=payload,\n            message_id=message_id,\n            user_id=user_id,\n            created_date=created_date,\n        )\n        self.db.add(message_revision)\n        return message_revision\n\n    def _validate_task(\n        self,\n        task: Task,\n        *,\n        task_id: Optional[UUID] = None,\n        frontend_message_id: Optional[str] = None,\n        check_ack: bool = True,\n    ) -> Task:\n        if task is None:\n            if task_id:\n                raise OasstError(f\"Task for {task_id=} not found\", OasstErrorCode.TASK_NOT_FOUND)\n            if frontend_message_id:\n                raise OasstError(f\"Task for {frontend_message_id=} not found\", OasstErrorCode.TASK_NOT_FOUND)\n            raise OasstError(\"Task not found\", OasstErrorCode.TASK_NOT_FOUND)\n\n        if task.expired:\n            raise OasstError(\"Task already expired.\", OasstErrorCode.TASK_EXPIRED)\n        if check_ack and not task.ack:\n            raise OasstError(\"Task is not acknowledged.\", OasstErrorCode.TASK_NOT_ACK)\n        if task.done:\n            raise OasstError(\"Task already done.\", OasstErrorCode.TASK_ALREADY_DONE)\n\n        if (not task.collective or task.user_id is None) and task.user_id != self.user_id:\n            logger.warning(f\"Task was assigned to a different user (expected: {task.user_id}; actual: {self.user_id}).\")\n            raise OasstError(\"Task was assigned to a different user.\", OasstErrorCode.TASK_NOT_ASSIGNED_TO_USER)\n\n        return task\n\n    def fetch_tree_state(self, message_tree_id: UUID) -> MessageTreeState:\n        return self.db.query(MessageTreeState).filter(MessageTreeState.message_tree_id == message_tree_id).one()\n\n    @managed_tx_method(CommitMode.FLUSH)\n    def store_text_reply(\n        self,\n        text: str,\n        lang: str,\n        frontend_message_id: str,\n        user_frontend_message_id: str,\n        review_count: int = 0,\n        review_result: bool = None,\n        check_tree_state: bool = True,\n        check_duplicate: bool = True,\n    ) -> Message:\n        self.ensure_user_is_enabled()\n\n        validate_frontend_message_id(frontend_message_id)\n        validate_frontend_message_id(user_frontend_message_id)\n\n        task = self.task_repository.fetch_task_by_frontend_message_id(frontend_message_id)\n        self._validate_task(task)\n\n        # If there's no parent message assume user started new conversation\n        role: str = None\n        depth: int = 0\n        deleted: bool = False\n\n        # reject whitespaces match with ^\\s+$\n        if re.match(r\"^\\s+$\", text):\n            raise OasstError(\"Message text is empty\", OasstErrorCode.TASK_MESSAGE_TEXT_EMPTY)\n\n        # ensure message size is below the predefined limit\n        if len(text) > settings.MESSAGE_SIZE_LIMIT:\n            logger.error(f\"Message size {len(text)=} exceeds size limit of {settings.MESSAGE_SIZE_LIMIT=}.\")\n            raise OasstError(\"Message size too long.\", OasstErrorCode.TASK_MESSAGE_TOO_LONG)\n\n        if check_duplicate and self.check_users_recent_replies_for_duplicates(text):\n            raise OasstError(\"User recent messages have duplicates\", OasstErrorCode.TASK_MESSAGE_DUPLICATED)\n\n        if task.parent_message_id:\n            parent_message = self.fetch_message(task.parent_message_id)\n\n            # check tree state\n            if check_tree_state:\n                # We store messages even after a tree has been completed.\n                # Although these messages will never be labeled nor ranked they should be\n                # included in the dataset because sometime users put a lot of effort into\n                # writing their reply.\n\n                ts = self.fetch_tree_state(parent_message.message_tree_id)\n                if ts.state not in (\n                    message_tree_state.State.GROWING,\n                    message_tree_state.State.RANKING,\n                    message_tree_state.State.READY_FOR_SCORING,\n                    message_tree_state.State.READY_FOR_EXPORT,\n                ):\n                    raise OasstError(\n                        \"Message insertion failed. Message tree is no longer accepting messages.\",\n                        OasstErrorCode.TREE_IN_ABORTED_STATE,\n                    )\n                if not ts.active:\n                    logger.warning(\n                        f\"Received message for inactive tree {parent_message.message_tree_id} (state='{ts.state.value}').\"\n                    )\n\n            if check_duplicate and not settings.DEBUG_ALLOW_DUPLICATE_TASKS:\n                siblings = self.fetch_message_children(task.parent_message_id, review_result=None, deleted=False)\n                if any(m.user_id == self.user_id for m in siblings):\n                    raise OasstError(\n                        \"User cannot reply twice to the same message.\",\n                        OasstErrorCode.TASK_MESSAGE_DUPLICATE_REPLY,\n                    )\n\n            parent_message.message_tree_id\n            parent_message.children_count += 1\n            self.db.add(parent_message)\n\n            depth = parent_message.depth + 1\n            deleted = parent_message.deleted\n\n        task_payload: db_payload.TaskPayload = task.payload.payload\n        if isinstance(task_payload, db_payload.InitialPromptPayload):\n            role = \"prompter\"\n        elif isinstance(task_payload, db_payload.PrompterReplyPayload):\n            role = \"prompter\"\n        elif isinstance(task_payload, db_payload.AssistantReplyPayload):\n            role = \"assistant\"\n        elif isinstance(task_payload, db_payload.SummarizationStoryPayload):\n            raise NotImplementedError(\"SummarizationStory task not implemented.\")\n        else:\n            raise OasstError(\n                f\"Unexpected task payload type: {type(task_payload).__name__}\",\n                OasstErrorCode.TASK_UNEXPECTED_PAYLOAD_TYPE_,\n            )\n\n        assert role in (\"assistant\", \"prompter\")\n\n        # create reply message\n        new_message_id = uuid4()\n        user_message = self.insert_message(\n            message_id=new_message_id,\n            frontend_message_id=user_frontend_message_id,\n            parent_id=task.parent_message_id,\n            message_tree_id=task.message_tree_id or new_message_id,\n            task_id=task.id,\n            role=role,\n            payload=db_payload.MessagePayload(text=text),\n            lang=lang or \"en\",\n            depth=depth,\n            review_count=review_count,\n            review_result=review_result,\n            deleted=deleted,\n        )\n        if not task.collective:\n            task.done = True\n            self.db.add(task)\n        self.journal.log_text_reply(task=task, message_id=new_message_id, role=role, length=len(text))\n        logger.debug(\n            f\"Inserted message id={user_message.id}, tree={user_message.message_tree_id}, user_id={user_message.user_id}, \"\n            f\"text[:100]='{user_message.text[:100]}', role='{user_message.role}', lang='{user_message.lang}'\"\n        )\n        return user_message\n\n    @managed_tx_method(CommitMode.FLUSH)\n    def revise_message(self, message_id: UUID, new_content: str):\n        # store original message as revision if not already stored\n        message = self.fetch_message(message_id)\n        if not message.edited:\n            self.insert_revision(\n                payload=message.payload,\n                message_id=message_id,\n                user_id=message.user_id,\n                created_date=message.created_date,\n            )\n\n        # store new version as revision\n        self.insert_revision(\n            payload=PayloadContainer(payload=db_payload.MessagePayload(text=new_content)),\n            message_id=message_id,\n            user_id=self.user_id,\n            created_date=utcnow(),\n        )\n\n        # update message with new content\n        updated_message_data = {\n            \"payload\": PayloadContainer(payload=db_payload.MessagePayload(text=new_content)),\n            \"edited\": True,\n            \"search_vector\": None,\n        }\n\n        query = update(Message).where(Message.id == message_id).values(**updated_message_data)\n        self.db.execute(query)\n\n    @managed_tx_method(CommitMode.FLUSH)\n    def store_rating(self, rating: protocol_schema.MessageRating) -> MessageReaction:\n        message = self.fetch_message_by_frontend_message_id(rating.message_id, fail_if_missing=True)\n\n        task = self.task_repository.fetch_task_by_frontend_message_id(rating.message_id)\n        self._validate_task(task)\n        task_payload: db_payload.RateSummaryPayload = task.payload.payload\n        if type(task_payload) != db_payload.RateSummaryPayload:\n            raise OasstError(\n                f\"Task payload type mismatch: {type(task_payload)=} != {db_payload.RateSummaryPayload}\",\n                OasstErrorCode.TASK_PAYLOAD_TYPE_MISMATCH,\n            )\n\n        if rating.rating < task_payload.scale.min or rating.rating > task_payload.scale.max:\n            raise OasstError(\n                f\"Invalid rating value: {rating.rating=} not in {task_payload.scale=}\",\n                OasstErrorCode.RATING_OUT_OF_RANGE,\n            )\n\n        # store reaction to message\n        reaction_payload = db_payload.RatingReactionPayload(rating=rating.rating)\n        reaction = self.insert_reaction(task_id=task.id, payload=reaction_payload, message_id=message.id)\n        if not task.collective:\n            task.done = True\n            self.db.add(task)\n\n        self.journal.log_rating(task, message_id=message.id, rating=rating.rating)\n        logger.info(f\"Ranking {rating.rating} stored for task {task.id}.\")\n        return reaction\n\n    @managed_tx_method(CommitMode.COMMIT)\n    def store_ranking(self, ranking: protocol_schema.MessageRanking) -> tuple[MessageReaction, Task]:\n        # fetch task\n        task = self.task_repository.fetch_task_by_frontend_message_id(ranking.message_id)\n        self._validate_task(task, frontend_message_id=ranking.message_id)\n        if not task.collective:\n            task.done = True\n            self.db.add(task)\n\n        task_payload: db_payload.RankConversationRepliesPayload | db_payload.RankInitialPromptsPayload = (\n            task.payload.payload\n        )\n\n        match type(task_payload):\n            case db_payload.RankPrompterRepliesPayload | db_payload.RankAssistantRepliesPayload:\n                # validate ranking\n                if sorted(ranking.ranking) != list(range(num_replies := len(task_payload.reply_messages))):\n                    raise OasstError(\n                        f\"Invalid ranking submitted. Each reply index must appear exactly once ({num_replies=}).\",\n                        OasstErrorCode.INVALID_RANKING_VALUE,\n                    )\n\n                last_conv_message = task_payload.conversation.messages[-1]\n                parent_msg = self.fetch_message(last_conv_message.id)\n\n                # store reaction to message\n                ranked_message_ids = [task_payload.reply_messages[i].id for i in ranking.ranking]\n                for mid in ranked_message_ids:\n                    message = self.fetch_message(mid)\n                    if message.parent_id != parent_msg.id:\n                        raise OasstError(\"Corrupt reply ranking result\", OasstErrorCode.CORRUPT_RANKING_RESULT)\n                    message.ranking_count += 1\n                    self.db.add(message)\n\n                reaction_payload = db_payload.RankingReactionPayload(\n                    ranking=ranking.ranking,\n                    ranked_message_ids=ranked_message_ids,\n                    ranking_parent_id=task_payload.ranking_parent_id,\n                    message_tree_id=task_payload.message_tree_id,\n                    not_rankable=ranking.not_rankable,\n                )\n                reaction = self.insert_reaction(task_id=task.id, payload=reaction_payload, message_id=parent_msg.id)\n                self.journal.log_ranking(task, message_id=parent_msg.id, ranking=ranking.ranking)\n\n                logger.info(f\"Ranking {ranking.ranking} stored for task {task.id}.\")\n\n            case db_payload.RankInitialPromptsPayload:\n                # validate ranking\n                if sorted(ranking.ranking) != list(range(num_prompts := len(task_payload.prompt_messages))):\n                    raise OasstError(\n                        f\"Invalid ranking submitted. Each reply index must appear exactly once ({num_prompts=}).\",\n                        OasstErrorCode.INVALID_RANKING_VALUE,\n                    )\n\n                # store reaction to message\n                ranked_message_ids = [task_payload.prompt_messages[i].id for i in ranking.ranking]\n                reaction_payload = db_payload.RankingReactionPayload(\n                    ranking=ranking.ranking, ranked_message_ids=ranked_message_ids\n                )\n                reaction = self.insert_reaction(task_id=task.id, payload=reaction_payload, message_id=None)\n                # self.journal.log_ranking(task, message_id=None, ranking=ranking.ranking)\n\n                logger.info(f\"Ranking {ranking.ranking} stored for task {task.id}.\")\n\n            case _:\n                raise OasstError(\n                    f\"task payload type mismatch: {type(task_payload)=} != {db_payload.RankConversationRepliesPayload}\",\n                    OasstErrorCode.TASK_PAYLOAD_TYPE_MISMATCH,\n                )\n\n        return reaction, task\n\n    @managed_tx_method(CommitMode.FLUSH)\n    def insert_toxicity(self, message_id: UUID, model: str, score: float, label: str) -> MessageToxicity:\n        \"\"\"Save the toxicity score of a new message in the database.\n        Args:\n            message_id (UUID): the identifier of the message we want to save its toxicity score\n            model (str): the model used for creating the toxicity score\n            score (float): the toxicity score that we obtained from the model\n            label (str): the final classification in toxicity of the model\n        Raises:\n            OasstError: if misses some of the before params\n        Returns:\n            MessageToxicity: the instance in the database of the score saved for that message\n        \"\"\"\n\n        message_toxicity = MessageToxicity(message_id=message_id, model=model, score=score, label=label)\n        self.db.add(message_toxicity)\n        return message_toxicity\n\n    @managed_tx_method(CommitMode.FLUSH)\n    def insert_message_embedding(self, message_id: UUID, model: str, embedding: list[float]) -> MessageEmbedding:\n        \"\"\"Insert the embedding of a new message in the database.\n\n        Args:\n            message_id (UUID): the identifier of the message we want to save its embedding\n            model (str): the model used for creating the embedding\n            embedding (list[float]): the values obtained from the message & model\n\n        Raises:\n            OasstError: if misses some of the before params\n\n        Returns:\n            MessageEmbedding: the instance in the database of the embedding saved for that message\n        \"\"\"\n\n        message_embedding = MessageEmbedding(message_id=message_id, model=model, embedding=embedding)\n        self.db.add(message_embedding)\n        return message_embedding\n\n    @managed_tx_method(CommitMode.FLUSH)\n    def insert_reaction(\n        self, task_id: UUID, payload: db_payload.ReactionPayload, message_id: Optional[UUID]\n    ) -> MessageReaction:\n        self.ensure_user_is_enabled()\n\n        container = PayloadContainer(payload=payload)\n        reaction = MessageReaction(\n            task_id=task_id,\n            user_id=self.user_id,\n            payload=container,\n            api_client_id=self.api_client.id,\n            payload_type=type(payload).__name__,\n            message_id=message_id,\n        )\n        self.db.add(reaction)\n        return reaction\n\n    @managed_tx_method(CommitMode.FLUSH)\n    def store_text_labels(self, text_labels: protocol_schema.TextLabels) -> tuple[TextLabels, Task, Message]:\n        self.ensure_user_is_enabled()\n\n        valid_labels: Optional[list[str]] = None\n        mandatory_labels: Optional[list[str]] = None\n        text_labels_id: Optional[UUID] = None\n        message_id: Optional[UUID] = text_labels.message_id\n\n        task: Task = None\n        if text_labels.task_id:\n            logger.debug(f\"text_labels reply has task_id {text_labels.task_id}\")\n            task = self.task_repository.fetch_task_by_id(text_labels.task_id)\n            self._validate_task(task, task_id=text_labels.task_id)\n\n            task_payload: db_payload.TaskPayload = task.payload.payload\n            if isinstance(task_payload, db_payload.LabelInitialPromptPayload):\n                if message_id and task_payload.message_id != message_id:\n                    raise OasstError(\"Task message id mismatch\", OasstErrorCode.TEXT_LABELS_WRONG_MESSAGE_ID)\n                message_id = task_payload.message_id\n                valid_labels = task_payload.valid_labels\n                mandatory_labels = task_payload.mandatory_labels\n            elif isinstance(task_payload, db_payload.LabelConversationReplyPayload):\n                if message_id and message_id != message_id:\n                    raise OasstError(\"Task message id mismatch\", OasstErrorCode.TEXT_LABELS_WRONG_MESSAGE_ID)\n                message_id = task_payload.message_id\n                valid_labels = task_payload.valid_labels\n                mandatory_labels = task_payload.mandatory_labels\n            else:\n                raise OasstError(\n                    \"Unexpected text_labels task payload\",\n                    OasstErrorCode.TASK_PAYLOAD_TYPE_MISMATCH,\n                )\n\n            logger.debug(f\"text_labels reply: {valid_labels=}, {mandatory_labels=}\")\n\n            if valid_labels:\n                if not all([label in valid_labels for label in text_labels.labels.keys()]):\n                    raise OasstError(\"Invalid text label specified\", OasstErrorCode.TEXT_LABELS_INVALID_LABEL)\n\n            if isinstance(mandatory_labels, list):\n                mandatory_set = set(mandatory_labels)\n                if not mandatory_set.issubset(text_labels.labels.keys()):\n                    missing = \", \".join(mandatory_set - text_labels.labels.keys())\n                    raise OasstError(\n                        f\"Mandatory text labels missing: {missing}\", OasstErrorCode.TEXT_LABELS_MANDATORY_LABEL_MISSING\n                    )\n\n            text_labels_id = task.id  # associate with task by sharing the id\n\n            if not task.collective:\n                task.done = True\n                self.db.add(task)\n\n        logger.debug(f\"inserting TextLabels for {message_id=}, {text_labels_id=}\")\n        model = TextLabels(\n            id=text_labels_id,\n            api_client_id=self.api_client.id,\n            message_id=message_id,\n            user_id=self.user_id,\n            text=text_labels.text,\n            labels=text_labels.labels,\n            task_id=task.id if task else None,\n        )\n\n        message: Message = None\n        if message_id:\n            if not task:\n                # free labeling case\n\n                if text_labels.is_report is True:\n                    message = self.handle_message_emoji(\n                        message_id, protocol_schema.EmojiOp.add, protocol_schema.EmojiCode.red_flag\n                    )\n\n                    message_details = {\n                        \"message_id\": message_id,\n                        \"message_text\": message.text[:500] + \"...\" if len(message.text) > 500 else message.text,\n                        \"role\": message.role.upper(),\n                        \"lang\": message.lang.upper(),\n                        \"thumbs_up\": message.emojis.get(\"+1\") or 0,\n                        \"thumbs_down\": message.emojis.get(\"-1\") or 0,\n                        \"red_flag\": message.emojis.get(\"red_flag\") or 0,\n                    }\n\n                    send_new_report_message.delay(\n                        message_details=message_details, label_text=text_labels.text, user_id=self.user_id\n                    )\n\n                # update existing record for repeated updates (same user no task associated)\n                existing_text_label = self.fetch_non_task_text_labels(message_id, self.user_id)\n                if existing_text_label is not None:\n                    existing_text_label.labels = text_labels.labels\n                    model = existing_text_label\n\n            else:\n                # task based labeling case\n\n                message = self.fetch_message(message_id, fail_if_missing=True)\n                if not settings.DEBUG_ALLOW_SELF_LABELING and message.user_id == self.user_id:\n                    raise OasstError(\n                        \"Labeling own message is not allowed.\", OasstErrorCode.TEXT_LABELS_NO_SELF_LABELING\n                    )\n\n                existing_labels = self.fetch_message_text_labels(message_id, self.user_id)\n                if not settings.DEBUG_ALLOW_DUPLICATE_TASKS and any(l.task_id for l in existing_labels):\n                    raise OasstError(\n                        \"Message was already labeled by same user before.\",\n                        OasstErrorCode.TEXT_LABELS_DUPLICATE_TASK_REPLY,\n                    )\n\n                message.review_count += 1\n                self.db.add(message)\n\n        self.db.add(model)\n        return model, task, message\n\n    def fetch_random_message_tree(\n        self,\n        require_role: str = None,\n        review_result: Optional[bool] = True,\n        deleted: Optional[bool] = False,\n    ) -> list[Message]:\n        \"\"\"\n        Loads all messages of a random message_tree.\n\n        :param require_role: If set loads only message_tree which has\n            at least one message with given role.\n        \"\"\"\n        distinct_message_trees = self.db.query(Message.message_tree_id).distinct(Message.message_tree_id)\n        if require_role:\n            distinct_message_trees = distinct_message_trees.filter(Message.role == require_role)\n        if review_result is not None:\n            distinct_message_trees = distinct_message_trees.filter(Message.review_result == review_result)\n        distinct_message_trees = distinct_message_trees.subquery()\n\n        random_message_tree_id = self.db.query(distinct_message_trees).order_by(func.random()).limit(1).scalar()\n        if random_message_tree_id:\n            return self.fetch_message_tree(random_message_tree_id, review_result=review_result, deleted=deleted)\n        return None\n\n    def fetch_random_conversation(\n        self,\n        last_message_role: str = None,\n        message_tree_id: Optional[UUID] = None,\n        review_result: Optional[bool] = True,\n        deleted: Optional[bool] = False,\n    ) -> list[Message]:\n        \"\"\"\n        Picks a random linear conversation starting from any root message\n        and ending somewhere in the message_tree, possibly at the root itself.\n\n        :param last_message_role: If set will form a conversation ending with a message\n            created by this role. Necessary for the tasks like \"user_reply\" where\n            the user should reply as a human and hence the last message of the conversation\n            needs to have \"assistant\" role.\n        \"\"\"\n        if message_tree_id:\n            messages_tree = self.fetch_message_tree(message_tree_id, review_result=review_result, deleted=deleted)\n        else:\n            messages_tree = self.fetch_random_message_tree(\n                last_message_role, review_result=review_result, deleted=deleted\n            )\n        if not messages_tree:\n            raise OasstError(\"No message tree found\", OasstErrorCode.NO_MESSAGE_TREE_FOUND)\n\n        if last_message_role:\n            conv_messages = [m for m in messages_tree if m.role == last_message_role]\n            conv_messages = [random.choice(conv_messages)]\n        else:\n            conv_messages = [random.choice(messages_tree)]\n        messages_tree = {m.id: m for m in messages_tree}\n\n        while True:\n            if not conv_messages[-1].parent_id:\n                # reached the start of the conversation\n                break\n\n            parent_message = messages_tree[conv_messages[-1].parent_id]\n            conv_messages.append(parent_message)\n\n        return list(reversed(conv_messages))\n\n    def fetch_random_initial_prompts(self, size: int = 5):\n        messages = self.db.query(Message).filter(Message.parent_id.is_(None)).order_by(func.random()).limit(size).all()\n        return messages\n\n    def fetch_message_tree(\n        self,\n        message_tree_id: UUID,\n        review_result: Optional[bool] = True,\n        deleted: Optional[bool] = False,\n    ) -> list[Message]:\n        qry = self.db.query(Message).filter(Message.message_tree_id == message_tree_id)\n        if review_result is not None:\n            qry = qry.filter(Message.review_result == review_result)\n        if deleted is not None:\n            qry = qry.filter(Message.deleted == deleted)\n        return self._add_user_emojis_all(qry)\n\n    def check_users_recent_replies_for_duplicates(self, text: str) -> bool:\n        \"\"\"\n        Checks if the user has recently replied with the same text within a given time period.\n        \"\"\"\n\n        user_id = self.user_id\n        logger.debug(f\"Checking for duplicate tasks for user {user_id}\")\n        # messages in the past 24 hours\n        messages = (\n            self.db.query(Message)\n            .filter(Message.user_id == user_id)\n            .order_by(Message.created_date.desc())\n            .filter(\n                Message.created_date > utcnow() - timedelta(minutes=settings.DUPLICATE_MESSAGE_FILTER_WINDOW_MINUTES)\n            )\n            .all()\n        )\n        if not messages:\n            return False\n        for msg in messages:\n            if msg.text == text:\n                return True\n        return False\n\n    def fetch_user_message_trees(\n        self, user_id: Message.user_id, reviewed: bool = True, include_deleted: bool = False\n    ) -> list[Message]:\n        qry = self.db.query(Message).filter(Message.user_id == user_id)\n        if reviewed:\n            qry = qry.filter(Message.review_result)\n        if not include_deleted:\n            qry = qry.filter(not_(Message.deleted))\n        return self._add_user_emojis_all(qry)\n\n    def fetch_multiple_random_replies(self, max_size: int = 5, message_role: str = None):\n        \"\"\"\n        Fetch a conversation with multiple possible replies to it.\n\n        This function finds a random message with >1 replies,\n        forms a conversation from the corresponding message tree root up to this message\n        and fetches up to max_size possible replies in continuation to this conversation.\n        \"\"\"\n        parent = self.db.query(Message.id).filter(Message.children_count > 1)\n        if message_role:\n            parent = parent.filter(Message.role == message_role)\n\n        parent = parent.order_by(func.random()).limit(1)\n        replies = (\n            self.db.query(Message).filter(Message.parent_id.in_(parent)).order_by(func.random()).limit(max_size).all()\n        )\n        if not replies:\n            raise OasstError(\"No replies found\", OasstErrorCode.NO_REPLIES_FOUND)\n\n        message_tree = self.fetch_message_tree(replies[0].message_tree_id)\n        message_tree = {p.id: p for p in message_tree}\n        conversation = [message_tree[replies[0].parent_id]]\n        while True:\n            if not conversation[-1].parent_id:\n                # reached start of the conversation\n                break\n\n            parent_message = message_tree[conversation[-1].parent_id]\n            conversation.append(parent_message)\n\n        conversation = reversed(conversation)\n\n        return conversation, replies\n\n    def fetch_message(self, message_id: UUID, fail_if_missing: bool = True) -> Optional[Message]:\n        qry = self.db.query(Message).filter(Message.id == message_id)\n        messages = self._add_user_emojis_all(qry)\n        message = messages[0] if messages else None\n\n        message = self.db.query(Message).filter(Message.id == message_id).one_or_none()\n        if fail_if_missing and not message:\n            raise OasstError(\"Message not found\", OasstErrorCode.MESSAGE_NOT_FOUND, HTTPStatus.NOT_FOUND)\n        return message\n\n    def fetch_non_task_text_labels(self, message_id: UUID, user_id: UUID) -> Optional[TextLabels]:\n        query = (\n            self.db.query(TextLabels)\n            .outerjoin(Task, Task.id == TextLabels.id)\n            .filter(Task.id.is_(None), TextLabels.message_id == message_id, TextLabels.user_id == user_id)\n        )\n        text_label = query.one_or_none()\n        return text_label\n\n    def fetch_message_text_labels(self, message_id: UUID, user_id: Optional[UUID] = None) -> list[TextLabels]:\n        query = self.db.query(TextLabels).filter(TextLabels.message_id == message_id)\n        if user_id is not None:\n            query = query.filter(TextLabels.user_id == user_id)\n        return query.all()\n\n    def fetch_message_revision_history(self, message_id: UUID) -> list[MessageRevision]:\n        # the revisions are sorted by time using the uuid7 id\n        revisions: list[MessageRevision] = sorted(\n            self.db.query(MessageRevision).filter(MessageRevision.message_id == message_id).all(),\n            key=lambda revision: revision.id.int >> 80,\n        )\n        for revision in revisions:\n            revision._user_is_author = self.user_id == revision.user_id\n        return revisions\n\n    @staticmethod\n    def trace_conversation(messages: list[Message] | dict[UUID, Message], last_message: Message) -> list[Message]:\n        \"\"\"\n        Pick messages from a collection so that the result makes a linear conversation\n        starting from a message tree root and up to the given message.\n        Returns an ordered list of messages starting from the message tree root.\n        \"\"\"\n        if isinstance(messages, list):\n            messages = {m.id: m for m in messages}\n        if not isinstance(messages, dict):\n            # This should not normally happen\n            raise OasstError(\"Server error\", OasstErrorCode.SERVER_ERROR0, HTTPStatus.INTERNAL_SERVER_ERROR)\n\n        conv = [last_message]\n        while conv[-1].parent_id:\n            if conv[-1].parent_id not in messages:\n                # Can't form a continuous conversation\n                logger.error(\n                    f\"Broken conversation: parent of message (id={conv[-1].id}, parent_id={conv[-1].parent_id}) not found in result set\"\n                )\n                raise OasstError(\n                    \"Broken conversation\", OasstErrorCode.BROKEN_CONVERSATION, HTTPStatus.INTERNAL_SERVER_ERROR\n                )\n\n            parent_message = messages[conv[-1].parent_id]\n            conv.append(parent_message)\n\n        return list(reversed(conv))\n\n    def fetch_message_conversation(self, message: Message | UUID) -> list[Message]:\n        \"\"\"\n        Fetch a conversation from the tree root and up to this message.\n        \"\"\"\n        if isinstance(message, UUID):\n            message = self.fetch_message(message)\n\n        tree_messages = self.fetch_message_tree(message.message_tree_id)\n        return self.trace_conversation(tree_messages, message)\n\n    def fetch_tree_from_message(\n        self,\n        message: Message | UUID,\n        review_result: Optional[bool] = True,\n        deleted: Optional[bool] = False,\n    ) -> list[Message]:\n        \"\"\"\n        Fetch message tree this message belongs to.\n        \"\"\"\n        if isinstance(message, UUID):\n            message = self.fetch_message(message)\n        logger.debug(f\"fetch_message_tree({message.message_tree_id=})\")\n        return self.fetch_message_tree(message.message_tree_id, review_result=review_result, deleted=deleted)\n\n    def fetch_message_children(\n        self,\n        message: Message | UUID,\n        review_result: Optional[bool] = True,\n        deleted: Optional[bool] = False,\n    ) -> list[Message]:\n        \"\"\"\n        Get all direct children of this message\n        \"\"\"\n        if isinstance(message, Message):\n            message = message.id\n\n        qry = self.db.query(Message).filter(Message.parent_id == message)\n        if review_result is not None:\n            qry = qry.filter(Message.review_result == review_result)\n        if deleted is not None:\n            qry = qry.filter(Message.deleted == deleted)\n        children = self._add_user_emojis_all(qry)\n        return children\n\n    def fetch_message_siblings(\n        self,\n        message: Message | UUID,\n        review_result: Optional[bool] = True,\n        deleted: Optional[bool] = False,\n    ) -> list[Message]:\n        \"\"\"\n        Get siblings of a message (other messages with the same parent_id)\n        \"\"\"\n        qry = self.db.query(Message)\n        if isinstance(message, Message):\n            qry = qry.filter(Message.parent_id == message.parent_id)\n        else:\n            parent_qry = self.db.query(Message.parent_id).filter(Message.id == message).subquery()\n            qry = qry.filter(Message.parent_id == parent_qry.c.parent_id)\n\n        if review_result is not None:\n            qry = qry.filter(Message.review_result == review_result)\n        if deleted is not None:\n            qry = qry.filter(Message.deleted == deleted)\n        siblings = self._add_user_emojis_all(qry)\n        return siblings\n\n    @staticmethod\n    def trace_descendants(root: Message, messages: list[Message]) -> list[Message]:\n        children = defaultdict(list)\n        for msg in messages:\n            children[msg.parent_id].append(msg)\n\n        def _traverse_subtree(m: Message):\n            for child in children[m.id]:\n                yield child\n                yield from _traverse_subtree(child)\n\n        return list(_traverse_subtree(root))\n\n    def fetch_message_descendants(self, message: Message | UUID, max_depth: int = None) -> list[Message]:\n        \"\"\"\n        Find all descendant messages to this message.\n\n        This function creates a subtree of messages starting from given root message.\n        \"\"\"\n        if isinstance(message, UUID):\n            message = self.fetch_message(message)\n\n        desc = self.db.query(Message).filter(\n            Message.message_tree_id == message.message_tree_id, Message.depth > message.depth\n        )\n        if max_depth is not None:\n            desc = desc.filter(Message.depth <= max_depth)\n\n        desc = self._add_user_emojis_all(desc)\n\n        return self.trace_descendants(message, desc)\n\n    def fetch_longest_conversation(self, message: Message | UUID) -> list[Message]:\n        tree = self.fetch_tree_from_message(message)\n        max_message = max(tree, key=lambda m: m.depth)\n        return self.trace_conversation(tree, max_message)\n\n    def fetch_message_with_max_children(self, message: Message | UUID) -> tuple[Message, list[Message]]:\n        tree = self.fetch_tree_from_message(message)\n        max_message = max(tree, key=lambda m: m.children_count)\n        return max_message, [m for m in tree if m.parent_id == max_message.id]\n\n    def _add_user_emojis_all(self, qry: Query, include_user: bool = False) -> list[Message]:\n        if self.user_id is None:\n            if not include_user:\n                return qry.all()\n\n            messages: list[Message] = []\n\n            for element in qry:\n                message = element[\"Message\"]\n                user = element[\"User\"]\n                message._user = user\n                messages.append(message)\n            return messages\n\n        order_by_clauses = qry._order_by_clauses\n        sq = qry.subquery(\"m\")\n        select_entities = [Message, func.string_agg(MessageEmoji.emoji, literal_column(\"','\")).label(\"user_emojis\")]\n        if include_user:\n            select_entities.append(User)\n        qry = (\n            self.db.query(*select_entities)\n            .select_entity_from(sq)\n            .outerjoin(\n                MessageEmoji,\n                and_(\n                    sq.c.id == MessageEmoji.message_id,\n                    MessageEmoji.user_id == self.user_id,\n                    sq.c.emojis != JSON.NULL,\n                ),\n            )\n            .group_by(sq)\n        )\n        qry._order_by_clauses = order_by_clauses\n        messages: list[Message] = []\n        for x in qry:\n            m: Message = x.Message\n            user_emojis = x[\"user_emojis\"]\n            if user_emojis:\n                m._user_emojis = user_emojis.split(\",\")\n            m._user_is_author = self.user_id and self.user_id == m.user_id\n            if include_user:\n                m._user = x[\"User\"]\n            messages.append(m)\n\n        return messages\n\n    def query_messages_ordered_by_created_date(\n        self,\n        user_id: Optional[UUID] = None,\n        auth_method: Optional[str] = None,\n        username: Optional[str] = None,\n        api_client_id: Optional[UUID] = None,\n        gte_created_date: Optional[datetime] = None,\n        gt_id: Optional[UUID] = None,\n        lte_created_date: Optional[datetime] = None,\n        lt_id: Optional[UUID] = None,\n        only_roots: bool = False,\n        deleted: Optional[bool] = None,\n        review_result: Optional[bool] = None,\n        desc: bool = False,\n        limit: Optional[int] = 100,\n        search_query: Optional[str] = None,\n        lang: Optional[str] = None,\n        include_user: Optional[bool] = None,\n    ) -> list[Message]:\n        if not self.api_client.trusted:\n            if not api_client_id:\n                # Let unprivileged api clients query their own messages without api_client_id being set\n                api_client_id = self.api_client.id\n\n            if api_client_id != self.api_client.id:\n                # Unprivileged api client asks for foreign messages\n                raise OasstError(\"Forbidden\", OasstErrorCode.API_CLIENT_NOT_AUTHORIZED, HTTPStatus.FORBIDDEN)\n\n        qry = self.db.query(Message)\n        if include_user:\n            qry = self.db.query(Message, User)\n        if user_id:\n            qry = qry.filter(Message.user_id == user_id)\n        if username or auth_method or include_user:\n            qry = qry.join(User)\n        if username or auth_method:\n            if not (username and auth_method):\n                raise OasstError(\"Auth method or username missing.\", OasstErrorCode.AUTH_AND_USERNAME_REQUIRED)\n            qry = qry.filter(User.username == username, User.auth_method == auth_method)\n        if api_client_id:\n            qry = qry.filter(Message.api_client_id == api_client_id)\n\n        gte_created_date = unaware_to_utc(gte_created_date)\n        lte_created_date = unaware_to_utc(lte_created_date)\n\n        if gte_created_date is not None:\n            if gt_id:\n                qry = qry.filter(\n                    or_(\n                        Message.created_date > gte_created_date,\n                        and_(Message.created_date == gte_created_date, Message.id > gt_id),\n                    )\n                )\n            else:\n                qry = qry.filter(Message.created_date >= gte_created_date)\n        elif gt_id:\n            raise OasstError(\"Need id and date for keyset pagination\", OasstErrorCode.GENERIC_ERROR)\n\n        if lte_created_date is not None:\n            if lt_id:\n                qry = qry.filter(\n                    or_(\n                        Message.created_date < lte_created_date,\n                        and_(Message.created_date == lte_created_date, Message.id < lt_id),\n                    )\n                )\n            else:\n                qry = qry.filter(Message.created_date <= lte_created_date)\n        elif lt_id:\n            raise OasstError(\"Need id and date for keyset pagination\", OasstErrorCode.GENERIC_ERROR)\n\n        if only_roots:\n            qry = qry.filter(Message.parent_id.is_(None))\n\n        if deleted is not None:\n            qry = qry.filter(Message.deleted == deleted)\n\n        if review_result is not None:\n            qry = qry.filter(Message.review_result == review_result)\n\n        if lang is not None:\n            qry = qry.filter(Message.lang == lang)\n\n            if search_query is not None:\n                qry = qry.filter(\n                    Message.search_vector.match(\n                        search_query,\n                        postgresql_regconfig=db_lang_to_postgres_ts_lang(lang),\n                    ),\n                )\n\n        if desc:\n            qry = qry.order_by(Message.created_date.desc(), Message.id.desc())\n        else:\n            qry = qry.order_by(Message.created_date.asc(), Message.id.asc())\n\n        if limit is not None:\n            qry = qry.limit(limit)\n\n        return self._add_user_emojis_all(qry, include_user=include_user)\n\n    def update_children_counts(self, message_tree_id: UUID):\n        sql_update_children_count = \"\"\"\nUPDATE message SET children_count = cc.children_count\nFROM (\n    SELECT m.id, count(c.id) - COALESCE(SUM(c.deleted::int), 0) AS children_count\n    FROM message m\n        LEFT JOIN message c ON m.id = c.parent_id\n    WHERE m.message_tree_id  = :message_tree_id\n    GROUP BY m.id\n) AS cc\nWHERE message.id = cc.id;\n\"\"\"\n        self.db.execute(text(sql_update_children_count), {\"message_tree_id\": message_tree_id})\n\n    @managed_tx_method(CommitMode.COMMIT)\n    def mark_messages_deleted(self, messages: Message | UUID | list[Message | UUID], recursive: bool = True):\n        \"\"\"\n        Marks deleted messages and all their descendants.\n        \"\"\"\n        if isinstance(messages, (Message, UUID)):\n            messages = [messages]\n\n        ids = []\n        for message in messages:\n            if isinstance(message, UUID):\n                ids.append(message)\n            elif isinstance(message, Message):\n                ids.append(message.id)\n            else:\n                raise OasstError(\"Server error\", OasstErrorCode.SERVER_ERROR1, HTTPStatus.INTERNAL_SERVER_ERROR)\n\n        query = update(Message).where(Message.id.in_(ids)).values(deleted=True)\n        self.db.execute(query)\n\n        parent_ids = ids\n        if recursive:\n            while parent_ids:\n                query = (\n                    update(Message).filter(Message.parent_id.in_(parent_ids)).values(deleted=True).returning(Message.id)\n                )\n\n                parent_ids = self.db.execute(query).scalars().all()\n\n    @managed_tx_method(CommitMode.COMMIT)\n    def undelete_deleted_message(self, message: Message | UUID):\n        \"\"\"\n        Undelete deleted messages and all their parents.\n        \"\"\"\n        message_id = None\n        if isinstance(message, UUID):\n            message_id = message\n        elif isinstance(message, Message):\n            message_id = message.id\n        else:\n            raise OasstError(\"Server error\", OasstErrorCode.SERVER_ERROR1, HTTPStatus.INTERNAL_SERVER_ERROR)\n\n        query = update(Message).where(Message.id == message_id).values(deleted=False)\n        self.db.execute(query)\n\n        parent_id = None\n        if isinstance(message, UUID):\n            parent_id = self.db.query(Message.parent_id).where(Message.id == message_id).first()[0]\n        elif isinstance(message, Message):\n            parent_id = message.parent_id\n\n        if parent_id is None:\n            return\n\n        # Fetching the entire parent_message so there is no parent_id query executed after\n        parent_message: Message = self.db.query(Message).where(Message.id == parent_id).first()\n\n        if parent_message is not None:\n            self.undelete_deleted_message(parent_message)\n\n    def get_stats(self) -> SystemStats:\n        \"\"\"\n        Get data stats such as number of all messages in the system,\n        number of deleted and active messages and number of message trees.\n        \"\"\"\n        # With columns: lang, deleted, count\n        group_count_query = self.db.query(Message.lang, Message.deleted, func.count()).group_by(\n            Message.lang, Message.deleted\n        )\n        # With columns: None, None, count\n        msg_tree_query = self.db.query(None, None, func.count(Message.id)).filter(Message.parent_id.is_(None))\n        # Union both queries, so that we can fetch the counts in one database query\n        query = group_count_query.union_all(msg_tree_query)\n\n        nactives = 0\n        ndeleted = 0\n        nactives_by_lang = {}\n        nthreads = 0\n\n        for lang, deleted, count in query.all():\n            if lang is None:  # corresponds to msg_tree_query\n                nthreads = count\n                continue\n            if deleted is False:  # corresponds to group_count_query (lang, deleted=False)\n                nactives_by_lang[lang] = count\n                nactives += count\n            else:  # corresponds to group_count_query (lang, deleted=True)\n                ndeleted += count\n\n        return SystemStats(\n            all=nactives + ndeleted,\n            active=nactives,\n            active_by_lang=nactives_by_lang,\n            deleted=ndeleted,\n            message_trees=nthreads,\n        )\n\n    @managed_tx_method()\n    def skip_task(self, task_id: UUID, reason: Optional[str]):\n        self.ensure_user_is_enabled()\n\n        task = self.task_repository.fetch_task_by_id(task_id)\n        self._validate_task(task, check_ack=False)\n\n        if not task.collective:\n            task.skipped = True\n            task.skip_reason = reason\n            self.db.add(task)\n\n        def handle_cancel_emoji(task_payload: db_payload.TaskPayload) -> Message | None:\n            for types, emoji in _task_type_and_reaction:\n                for t in types:\n                    if isinstance(task_payload, t):\n                        return self.handle_message_emoji(task.parent_message_id, protocol_schema.EmojiOp.add, emoji)\n            return None\n\n        task_payload: db_payload.TaskPayload = task.payload.payload\n        handle_cancel_emoji(task_payload)\n\n    def handle_message_emoji(\n        self, message_id: UUID, op: protocol_schema.EmojiOp, emoji: protocol_schema.EmojiCode\n    ) -> Message:\n        self.ensure_user_is_enabled()\n\n        message = self.fetch_message(message_id)\n\n        # check if emoji exists\n        existing_emoji = (\n            self.db.query(MessageEmoji)\n            .filter(\n                MessageEmoji.message_id == message_id, MessageEmoji.user_id == self.user_id, MessageEmoji.emoji == emoji\n            )\n            .one_or_none()\n        )\n\n        if existing_emoji:\n            if op == protocol_schema.EmojiOp.add:\n                logger.info(f\"Emoji record already exists {message_id=}, {emoji=}, {self.user_id=}\")\n                return message\n            elif op == protocol_schema.EmojiOp.togggle:\n                op = protocol_schema.EmojiOp.remove\n\n        if existing_emoji is None:\n            if op == protocol_schema.EmojiOp.remove:\n                logger.info(f\"Emoji record not found {message_id=}, {emoji=}, {self.user_id=}\")\n                return message\n            elif op == protocol_schema.EmojiOp.togggle:\n                op = protocol_schema.EmojiOp.add\n\n        if op == protocol_schema.EmojiOp.add:\n            # hard coded exclusivity of thumbs_up & thumbs_down\n            if emoji == protocol_schema.EmojiCode.thumbs_up and message.has_user_emoji(\n                protocol_schema.EmojiCode.thumbs_down.value\n            ):\n                message = self.handle_message_emoji(\n                    message_id, protocol_schema.EmojiOp.remove, protocol_schema.EmojiCode.thumbs_down\n                )\n            elif emoji == protocol_schema.EmojiCode.thumbs_down and message.has_user_emoji(\n                protocol_schema.EmojiCode.thumbs_up.value\n            ):\n                message = self.handle_message_emoji(\n                    message_id, protocol_schema.EmojiOp.remove, protocol_schema.EmojiCode.thumbs_up\n                )\n\n            if message.user_id == self.user_id and emoji in (\n                protocol_schema.EmojiCode.thumbs_up,\n                protocol_schema.EmojiCode.thumbs_down,\n            ):\n                logger.debug(f\"Ignoring add emoji op for user's own message ({emoji=})\")\n                return message\n\n            # Add to flagged_message table if the red flag emoji is applied\n            if emoji == protocol_schema.EmojiCode.red_flag:\n                flagged_message = FlaggedMessage(message_id=message_id, processed=False, created_date=utcnow())\n                insert_stmt = pg.insert(FlaggedMessage).values(**flagged_message.dict())\n                upsert_stmt = insert_stmt.on_conflict_do_update(\n                    constraint=\"flagged_message_pkey\", set_=flagged_message.dict()\n                )\n                self.db.execute(upsert_stmt)\n\n            # insert emoji record & increment count\n            message_emoji = MessageEmoji(message_id=message.id, user_id=self.user_id, emoji=emoji)\n            self.db.add(message_emoji)\n            emoji_counts = message.emojis\n            if not emoji_counts:\n                message.emojis = {emoji.value: 1}\n            else:\n                count = emoji_counts.get(emoji.value) or 0\n                emoji_counts[emoji.value] = count + 1\n            if message._user_emojis is None:\n                message._user_emojis = []\n            if emoji.value not in message._user_emojis:\n                message._user_emojis.append(emoji.value)\n        elif op == protocol_schema.EmojiOp.remove:\n            # remove emoji record and & decrement count\n            message = self.fetch_message(message_id)\n            if message._user_emojis and emoji.value in message._user_emojis:\n                message._user_emojis.remove(emoji.value)\n            self.db.delete(existing_emoji)\n            emoji_counts = message.emojis\n            count = emoji_counts.get(emoji.value)\n            if count is not None:\n                if count == 1:\n                    del emoji_counts[emoji.value]\n                else:\n                    emoji_counts[emoji.value] = count - 1\n                flag_modified(message, \"emojis\")\n                self.db.add(message)\n        else:\n            raise OasstError(\"Emoji op not supported\", OasstErrorCode.EMOJI_OP_UNSUPPORTED)\n\n        flag_modified(message, \"emojis\")\n        self.db.add(message)\n        self.db.flush()\n        return message\n\n    def fetch_flagged_messages(self, max_count: Optional[int]) -> list[FlaggedMessage]:\n        qry = self.db.query(FlaggedMessage)\n        if max_count is not None:\n            qry = qry.limit(max_count)\n\n        return qry.all()\n\n    def fetch_flagged_messages_by_created_date(\n        self,\n        gte_created_date: Optional[datetime] = None,\n        gt_id: Optional[UUID] = None,\n        lte_created_date: Optional[datetime] = None,\n        lt_id: Optional[UUID] = None,\n        desc: bool = False,\n        limit: Optional[int] = 100,\n    ) -> list[FlaggedMessage]:\n        qry = self.db.query(FlaggedMessage)\n\n        if gte_created_date is not None:\n            if gt_id:\n                qry = qry.filter(\n                    or_(\n                        FlaggedMessage.created_date > gte_created_date,\n                        and_(FlaggedMessage.created_date == gte_created_date, FlaggedMessage.message_id > gt_id),\n                    )\n                )\n            else:\n                qry = qry.filter(FlaggedMessage.created_date >= gte_created_date)\n        elif gt_id:\n            raise OasstError(\"Need id and date for keyset pagination\", OasstErrorCode.GENERIC_ERROR)\n\n        if lte_created_date is not None:\n            if lt_id:\n                qry = qry.filter(\n                    or_(\n                        FlaggedMessage.created_date < lte_created_date,\n                        and_(FlaggedMessage.created_date == lte_created_date, FlaggedMessage.message_id < lt_id),\n                    )\n                )\n            else:\n                qry = qry.filter(FlaggedMessage.created_date <= lte_created_date)\n        elif lt_id:\n            raise OasstError(\"Need id and date for keyset pagination\", OasstErrorCode.GENERIC_ERROR)\n\n        if desc:\n            qry = qry.order_by(FlaggedMessage.created_date.desc(), FlaggedMessage.message_id.desc())\n        else:\n            qry = qry.order_by(FlaggedMessage.created_date.asc(), FlaggedMessage.message_id.asc())\n\n        if limit is not None:\n            qry = qry.limit(limit)\n\n        return qry.all()\n\n    def process_flagged_message(self, message_id: UUID) -> FlaggedMessage:\n        message = self.db.query(FlaggedMessage).get(message_id)\n\n        if not message:\n            raise OasstError(\"Message not found\", OasstErrorCode.MESSAGE_NOT_FOUND, HTTPStatus.NOT_FOUND)\n\n        message.processed = True\n        self.db.commit()\n        self.db.refresh(message)\n\n        return message\n", "backend/oasst_backend/config.py": "from pathlib import Path\nfrom typing import Any, Dict, List, Optional\n\nfrom oasst_shared.schemas.protocol import TextLabel\nfrom pydantic import AnyHttpUrl, BaseModel, BaseSettings, FilePath, PostgresDsn, validator\n\n\nclass TreeManagerConfiguration(BaseModel):\n    \"\"\"TreeManager configuration settings\"\"\"\n\n    max_active_trees: int = 10\n    \"\"\"Maximum number of concurrently active message trees in the database.\n    No new initial prompt tasks are handed out to users if this\n    number is reached.\"\"\"\n\n    max_initial_prompt_review: int = 100\n    \"\"\"Maximum number of initial prompts under review before no more initial prompt tasks will be handed out.\"\"\"\n\n    max_tree_depth: int = 3\n    \"\"\"Maximum depth of message tree.\"\"\"\n\n    max_children_count: int = 3\n    \"\"\"Maximum number of reply messages per tree node.\"\"\"\n\n    num_prompter_replies: int = 1\n    \"\"\"Number of prompter replies to collect per assistant reply.\"\"\"\n\n    goal_tree_size: int = 12\n    \"\"\"Total number of messages to gather per tree.\"\"\"\n\n    random_goal_tree_size: bool = False\n    \"\"\"If set to true goal tree sizes will be generated randomly within range [min_goal_tree_size, goal_tree_size].\"\"\"\n\n    min_goal_tree_size: int = 5\n    \"\"\"Minimum tree size for random goal sizes.\"\"\"\n\n    num_reviews_initial_prompt: int = 3\n    \"\"\"Number of peer review checks to collect in INITIAL_PROMPT_REVIEW state.\"\"\"\n\n    num_reviews_reply: int = 3\n    \"\"\"Number of peer review checks to collect per reply (other than initial_prompt).\"\"\"\n\n    auto_mod_enabled: bool = True\n    \"\"\"Flag to enable/disable auto moderation.\"\"\"\n\n    auto_mod_max_skip_reply: int = 25\n    \"\"\"Automatically set tree state to `halted_by_moderator` when more than the specified number\n    of users skip replying to a message. (auto moderation)\"\"\"\n\n    auto_mod_red_flags: int = 4\n    \"\"\"Delete messages that receive more than this number of red flags if it is a reply or\n    set the tree to `aborted_low_grade` when a prompt is flagged. (auto moderation)\"\"\"\n\n    p_full_labeling_review_prompt: float = 1.0\n    \"\"\"Probability of full text-labeling (instead of mandatory only) for initial prompts.\"\"\"\n\n    p_full_labeling_review_reply_assistant: float = 1.0\n    \"\"\"Probability of full text-labeling (instead of mandatory only) for assistant replies.\"\"\"\n\n    p_full_labeling_review_reply_prompter: float = 0.25\n    \"\"\"Probability of full text-labeling (instead of mandatory only) for prompter replies.\"\"\"\n\n    acceptance_threshold_initial_prompt: float = 0.6\n    \"\"\"Threshold for accepting an initial prompt.\"\"\"\n\n    acceptance_threshold_reply: float = 0.6\n    \"\"\"Threshold for accepting a reply.\"\"\"\n\n    num_required_rankings: int = 3\n    \"\"\"Number of rankings in which the message participated.\"\"\"\n\n    p_activate_backlog_tree: float = 0.1\n    \"\"\"Probability to activate a message tree in BACKLOG_RANKING state when another tree enters\n    a terminal state.\"\"\"\n\n    min_active_rankings_per_lang: int = 0\n    \"\"\"When the number of active ranking tasks is below this value when a tree enters a terminal\n    state an available trees in BACKLOG_RANKING will be activated (i.e. enters the RANKING state).\"\"\"\n\n    labels_initial_prompt: list[TextLabel] = [\n        TextLabel.spam,\n        TextLabel.lang_mismatch,\n        TextLabel.quality,\n        TextLabel.creativity,\n        TextLabel.humor,\n        TextLabel.toxicity,\n        TextLabel.violence,\n        TextLabel.not_appropriate,\n        TextLabel.pii,\n        TextLabel.hate_speech,\n        TextLabel.sexual_content,\n    ]\n\n    labels_assistant_reply: list[TextLabel] = [\n        TextLabel.spam,\n        TextLabel.lang_mismatch,\n        TextLabel.fails_task,\n        TextLabel.quality,\n        TextLabel.helpfulness,\n        TextLabel.creativity,\n        TextLabel.humor,\n        TextLabel.toxicity,\n        TextLabel.violence,\n        TextLabel.not_appropriate,\n        TextLabel.pii,\n        TextLabel.hate_speech,\n        TextLabel.sexual_content,\n    ]\n\n    labels_prompter_reply: list[TextLabel] = [\n        TextLabel.spam,\n        TextLabel.lang_mismatch,\n        TextLabel.quality,\n        TextLabel.creativity,\n        TextLabel.humor,\n        TextLabel.toxicity,\n        TextLabel.violence,\n        TextLabel.not_appropriate,\n        TextLabel.pii,\n        TextLabel.hate_speech,\n        TextLabel.sexual_content,\n    ]\n\n    mandatory_labels_initial_prompt: Optional[list[TextLabel]] = [TextLabel.spam]\n    \"\"\"Mandatory labels in text-labeling tasks for initial prompts.\"\"\"\n\n    mandatory_labels_assistant_reply: Optional[list[TextLabel]] = [TextLabel.spam]\n    \"\"\"Mandatory labels in text-labeling tasks for assistant replies.\"\"\"\n\n    mandatory_labels_prompter_reply: Optional[list[TextLabel]] = [TextLabel.spam]\n    \"\"\"Mandatory labels in text-labeling tasks for prompter replies.\"\"\"\n\n    rank_prompter_replies: bool = False\n\n    lonely_children_count: int = 2\n    \"\"\"Number of children below which parents are preferred during sampling for reply tasks.\"\"\"\n\n    p_lonely_child_extension: float = 0.75\n    \"\"\"Probability to select a prompter message parent with less than lonely_children_count children.\"\"\"\n\n    recent_tasks_span_sec: int = 5 * 60  # 5 min\n    \"\"\"Time in seconds of recent tasks to consider for exclusion during task selection.\"\"\"\n\n    max_pending_tasks_per_user: int = 8\n    \"\"\"Maximum number of pending tasks (neither canceled nor completed) by a single user within\n    the time span defined by `recent_tasks_span_sec`.\"\"\"\n\n    max_prompt_lottery_waiting: int = 250\n    \"\"\"Maximum number of prompts in prompt_lottery_waiting state per language. If this value\n    is exceeded no new initial prompt tasks for that language are generated.\"\"\"\n\n    init_prompt_disabled_langs: str = \"\"\n\n    @property\n    def init_prompt_disabled_langs_list(self) -> list[str]:\n        return self.init_prompt_disabled_langs.split(\",\")\n\n\nclass Settings(BaseSettings):\n    PROJECT_NAME: str = \"open-assistant backend\"\n    API_V1_STR: str = \"/api/v1\"\n    OFFICIAL_WEB_API_KEY: str = \"1234\"\n\n    # Encryption fields for handling the web generated JSON Web Tokens.\n    # These fields need to be shared with the web's auth settings in order to\n    # correctly decrypt the web tokens.\n    AUTH_INFO: bytes = b\"NextAuth.js Generated Encryption Key\"\n    AUTH_SALT: bytes = b\"\"\n    AUTH_LENGTH: int = 32\n    AUTH_SECRET: bytes = b\"O/M2uIbGj+lDD2oyNa8ax4jEOJqCPJzO53UbWShmq98=\"\n    AUTH_COOKIE_NAME: str = \"next-auth.session-token\"\n    AUTH_ALGORITHM: str = \"HS256\"\n    AUTH_ACCESS_TOKEN_EXPIRE_MINUTES: int = 30\n\n    AUTH_DISCORD_CLIENT_ID: str = \"\"\n    AUTH_DISCORD_CLIENT_SECRET: str = \"\"\n\n    POSTGRES_HOST: str = \"localhost\"\n    POSTGRES_PORT: str = \"5432\"\n    POSTGRES_USER: str = \"postgres\"\n    POSTGRES_PASSWORD: str = \"postgres\"\n    POSTGRES_DB: str = \"postgres\"\n    DATABASE_URI: Optional[PostgresDsn] = None\n    DATABASE_MAX_TX_RETRY_COUNT: int = 3\n\n    DATABASE_POOL_SIZE = 75\n    DATABASE_MAX_OVERFLOW = 20\n\n    RATE_LIMIT: bool = True\n    MESSAGE_SIZE_LIMIT: int = 2000\n    REDIS_HOST: str = \"localhost\"\n    REDIS_PORT: str = \"6379\"\n\n    DEBUG_USE_SEED_DATA: bool = False\n    DEBUG_USE_SEED_DATA_PATH: Optional[FilePath] = (\n        Path(__file__).parent.parent / \"test_data/realistic/realistic_seed_data.json\"\n    )\n    DEBUG_ALLOW_SELF_LABELING: bool = False  # allow users to label their own messages\n    DEBUG_ALLOW_SELF_RANKING: bool = False  # allow users to rank their own messages\n    DEBUG_ALLOW_DUPLICATE_TASKS: bool = False  # offer users tasks to which they already responded\n    DEBUG_SKIP_EMBEDDING_COMPUTATION: bool = False\n    DEBUG_SKIP_TOXICITY_CALCULATION: bool = False\n    DEBUG_DATABASE_ECHO: bool = False\n    DEBUG_IGNORE_TOS_ACCEPTANCE: bool = (  # ignore whether users accepted the ToS\n        True  # TODO: set False after ToS acceptance UI was added to web-frontend\n    )\n\n    DUPLICATE_MESSAGE_FILTER_WINDOW_MINUTES: int = 120\n\n    HUGGING_FACE_API_KEY: str = \"\"\n\n    ROOT_TOKENS: List[str] = [\"1234\"]  # supply a string that can be parsed to a json list\n\n    ENABLE_PROM_METRICS: bool = True  # enable prometheus metrics at /metrics\n\n    @validator(\"DATABASE_URI\", pre=True)\n    def assemble_db_connection(cls, v: Optional[str], values: Dict[str, Any]) -> Any:\n        if isinstance(v, str):\n            return v\n        return PostgresDsn.build(\n            scheme=\"postgresql\",\n            user=values.get(\"POSTGRES_USER\"),\n            password=values.get(\"POSTGRES_PASSWORD\"),\n            host=values.get(\"POSTGRES_HOST\"),\n            port=values.get(\"POSTGRES_PORT\"),\n            path=f\"/{values.get('POSTGRES_DB') or ''}\",\n        )\n\n    BACKEND_CORS_ORIGINS_CSV: Optional[str]  # allow setting CORS origins as comma separated values\n    BACKEND_CORS_ORIGINS: List[AnyHttpUrl] = []\n\n    @validator(\"BACKEND_CORS_ORIGINS\", pre=True)\n    def assemble_cors_origins(cls, v: Optional[List[str]], values: Dict[str, Any]) -> List[str]:\n        s = values.get(\"BACKEND_CORS_ORIGINS_CSV\")\n        if isinstance(s, str):\n            v = [i.strip() for i in s.split(\",\")]\n            return v\n        return v\n\n    UPDATE_ALEMBIC: bool = True\n\n    tree_manager: Optional[TreeManagerConfiguration] = TreeManagerConfiguration()\n\n    USER_STATS_INTERVAL_DAY: int = 5  # minutes\n    USER_STATS_INTERVAL_WEEK: int = 15  # minutes\n    USER_STATS_INTERVAL_MONTH: int = 60  # minutes\n    USER_STATS_INTERVAL_TOTAL: int = 240  # minutes\n    USER_STREAK_UPDATE_INTERVAL: int = 4  # Hours\n\n    @validator(\n        \"USER_STATS_INTERVAL_DAY\",\n        \"USER_STATS_INTERVAL_WEEK\",\n        \"USER_STATS_INTERVAL_MONTH\",\n        \"USER_STATS_INTERVAL_TOTAL\",\n        \"USER_STREAK_UPDATE_INTERVAL\",\n    )\n    def validate_user_stats_intervals(cls, v: int):\n        if v < 1:\n            raise ValueError(v)\n        return v\n\n    CACHED_STATS_UPDATE_INTERVAL: int = 60  # minutes\n\n    RATE_LIMIT_TASK_USER_TIMES: int = 30\n    RATE_LIMIT_TASK_USER_MINUTES: int = 4\n    RATE_LIMIT_TASK_API_TIMES: int = 10_000\n    RATE_LIMIT_TASK_API_MINUTES: int = 1\n\n    RATE_LIMIT_ASSISTANT_USER_TIMES: int = 4\n    RATE_LIMIT_ASSISTANT_USER_MINUTES: int = 2\n\n    RATE_LIMIT_PROMPTER_USER_TIMES: int = 8\n    RATE_LIMIT_PROMPTER_USER_MINUTES: int = 2\n\n    TASK_VALIDITY_MINUTES: int = 60 * 24 * 2  # tasks expire after 2 days\n\n    DISCORD_API_KEY: str | None = None\n    DISCORD_CHANNEL_ID: str | None = None\n\n    class Config:\n        env_file = \".env\"\n        env_file_encoding = \"utf-8\"\n        case_sensitive = False\n        env_nested_delimiter = \"__\"\n\n\nsettings = Settings()\n", "backend/oasst_backend/auth.py": "from datetime import datetime, timedelta\nfrom typing import Optional\n\nfrom jose import jwt\nfrom oasst_backend.config import Settings\nfrom oasst_backend.models import Account\nfrom sqlmodel import Session\n\n\ndef create_access_token(data: dict) -> str:\n    \"\"\"\n    Create an encoded JSON Web Token (JWT) using the given data.\n    \"\"\"\n\n    expires_delta = timedelta(minutes=Settings.AUTH_ACCESS_TOKEN_EXPIRE_MINUTES)\n    to_encode = data.copy()\n    expire = datetime.utcnow() + expires_delta\n    to_encode.update({\"exp\": expire})\n    encoded_jwt = jwt.encode(to_encode, Settings.AUTH_SECRET, algorithm=Settings.AUTH_ALGORITHM)\n    return encoded_jwt\n\n\ndef get_account_from_discord_id(db: Session, discord_id: str) -> Optional[Account]:\n    \"\"\"\n    Get the Open-Assistant Account associated with the given Discord ID.\n    \"\"\"\n\n    account: Account = (\n        db.query(Account)\n        .filter(\n            Account.provider == \"discord\",\n            Account.provider_account_id == discord_id,\n        )\n        .first()\n    )\n\n    return account\n", "backend/oasst_backend/scheduled_tasks.py": "from __future__ import absolute_import, unicode_literals\n\nfrom datetime import timedelta\nfrom typing import Any, Dict, List\n\nfrom asgiref.sync import async_to_sync\nfrom celery import shared_task\nfrom loguru import logger\nfrom oasst_backend.celery_worker import app\nfrom oasst_backend.models import ApiClient, Message, User\nfrom oasst_backend.models.db_payload import MessagePayload\nfrom oasst_backend.prompt_repository import PromptRepository\nfrom oasst_backend.utils.database_utils import db_lang_to_postgres_ts_lang, default_session_factory\nfrom oasst_backend.utils.hugging_face import HfClassificationModel, HfEmbeddingModel, HfUrl, HuggingFaceAPI\nfrom oasst_shared.utils import log_timing, utcnow\nfrom sqlalchemy import func\nfrom sqlmodel import update\n\n\nasync def useHFApi(text, url, model_name):\n    hugging_face_api: HuggingFaceAPI = HuggingFaceAPI(f\"{url}/{model_name}\")\n    result = await hugging_face_api.post(text)\n    return result\n\n\n@app.task(name=\"toxicity\")\ndef toxicity(text, message_id, api_client):\n    try:\n        logger.info(f\"checking toxicity : {api_client}\")\n\n        with default_session_factory() as session:\n            model_name: str = HfClassificationModel.TOXIC_ROBERTA.value\n            url: str = HfUrl.HUGGINGFACE_TOXIC_CLASSIFICATION.value\n            toxicity: List[List[Dict[str, Any]]] = async_to_sync(useHFApi)(text=text, url=url, model_name=model_name)\n            toxicity = toxicity[0][0]\n            logger.info(f\"toxicity from HF {toxicity}\")\n            api_client_m = ApiClient(**api_client)\n            if toxicity is not None:\n                pr = PromptRepository(db=session, api_client=api_client_m)\n                pr.insert_toxicity(\n                    message_id=message_id, model=model_name, score=toxicity[\"score\"], label=toxicity[\"label\"]\n                )\n            session.commit()\n\n    except Exception as e:\n        logger.error(f\"Could not compute toxicity for text reply to {message_id=} with {text=} by.error {str(e)}\")\n\n\n@app.task(name=\"hf_feature_extraction\")\ndef hf_feature_extraction(text, message_id, api_client):\n    try:\n        with default_session_factory() as session:\n            model_name: str = HfEmbeddingModel.MINILM.value\n            url: str = HfUrl.HUGGINGFACE_FEATURE_EXTRACTION.value\n            embedding = async_to_sync(useHFApi)(text=text, url=url, model_name=model_name)\n            api_client_m = ApiClient(**api_client)\n            if embedding is not None:\n                logger.info(f\"emmbedding from HF {len(embedding)}\")\n                pr = PromptRepository(db=session, api_client=api_client_m)\n                pr.insert_message_embedding(\n                    message_id=message_id, model=HfEmbeddingModel.MINILM.value, embedding=embedding\n                )\n                session.commit()\n\n    except Exception as e:\n        logger.error(f\"Could not extract embedding for text reply to {message_id=} with {text=} by.error {str(e)}\")\n\n\n@shared_task(name=\"update_search_vectors\")\ndef update_search_vectors(batch_size: int) -> None:\n    logger.info(\"update_search_vectors start...\")\n    try:\n        with default_session_factory() as session:\n            while True:\n                to_update: list[Message] = (\n                    session.query(Message).filter(Message.search_vector.is_(None)).limit(batch_size).all()\n                )\n\n                if not to_update:\n                    break\n\n                for message in to_update:\n                    message_payload: MessagePayload = message.payload.payload\n                    message_lang: str = db_lang_to_postgres_ts_lang(message.lang)\n                    message.search_vector = func.to_tsvector(message_lang, message_payload.text)\n\n                session.commit()\n    except Exception as e:\n        logger.error(f\"update_search_vectors failed with error: {str(e)}\")\n\n\n@shared_task(name=\"periodic_user_streak_reset\")\n@log_timing(level=\"INFO\")\ndef periodic_user_streak_reset() -> None:\n    try:\n        with default_session_factory() as session:\n            # Reset streak_days to 0 for users with more than 1.5 days of inactivity\n            streak_timeout = utcnow() - timedelta(hours=36)\n            reset_query = (\n                update(User)\n                .filter(User.last_activity_date < streak_timeout, User.streak_last_day_date.is_not(None))\n                .values(streak_days=0, streak_last_day_date=None)\n            )\n            session.execute(reset_query)\n            session.commit()\n    except Exception:\n        logger.exception(\"Error during periodic user streak reset\")\n", "backend/oasst_backend/cached_stats_repository.py": "from oasst_backend.models import CachedStats, Message, MessageTreeState, User\nfrom oasst_shared.exceptions.oasst_api_error import OasstError, OasstErrorCode\nfrom oasst_shared.schemas.protocol import AllCachedStatsResponse, CachedStatsName, CachedStatsResponse\nfrom oasst_shared.utils import log_timing, utcnow\nfrom sqlalchemy.orm.attributes import flag_modified\nfrom sqlmodel import Session, func, not_\n\n\ndef row_to_dict(r) -> dict:\n    return {k: r[k] for k in r.keys()}\n\n\nclass CachedStatsRepository:\n    def __init__(self, db: Session):\n        self.db = db\n\n    def qry_human_messages_by_lang(self) -> dict[str, int]:\n        qry = (\n            self.db.query(Message.lang, func.count(Message.id).label(\"count\"))\n            .filter(not_(Message.deleted), Message.review_result, not_(Message.synthetic))\n            .group_by(Message.lang)\n        )\n        return {r[\"lang\"]: r[\"count\"] for r in qry}\n\n    def qry_human_messages_by_role(self) -> dict[str, int]:\n        qry = (\n            self.db.query(Message.role, func.count(Message.id).label(\"count\"))\n            .filter(not_(Message.deleted), Message.review_result, not_(Message.synthetic))\n            .group_by(Message.role)\n        )\n        return {r[\"role\"]: r[\"count\"] for r in qry}\n\n    def qry_message_trees_by_state(self) -> dict[str, int]:\n        qry = self.db.query(\n            MessageTreeState.state, func.count(MessageTreeState.message_tree_id).label(\"count\")\n        ).group_by(MessageTreeState.state)\n        return {r[\"state\"]: r[\"count\"] for r in qry}\n\n    def qry_message_trees_states_by_lang(self) -> list:\n        qry = (\n            self.db.query(\n                Message.lang, MessageTreeState.state, func.count(MessageTreeState.message_tree_id).label(\"count\")\n            )\n            .select_from(MessageTreeState)\n            .join(Message, MessageTreeState.message_tree_id == Message.id)\n            .group_by(MessageTreeState.state, Message.lang)\n            .order_by(Message.lang, MessageTreeState.state)\n        )\n        return [row_to_dict(r) for r in qry]\n\n    def qry_users_accepted_tos(self) -> dict[str, int]:\n        qry = self.db.query(func.count(User.id)).filter(User.enabled, User.tos_acceptance_date.is_not(None))\n        return {\"count\": qry.scalar()}\n\n    @log_timing(level=\"INFO\")\n    def update_all_cached_stats(self):\n        v = self.qry_human_messages_by_lang()\n        self._insert_cached_stats(CachedStatsName.human_messages_by_lang, v)\n\n        v = self.qry_human_messages_by_role()\n        self._insert_cached_stats(CachedStatsName.human_messages_by_role, v)\n\n        v = self.qry_message_trees_by_state()\n        self._insert_cached_stats(CachedStatsName.message_trees_by_state, v)\n\n        v = self.qry_message_trees_states_by_lang()\n        self._insert_cached_stats(CachedStatsName.message_trees_states_by_lang, v)\n\n        v = self.qry_users_accepted_tos()\n        self._insert_cached_stats(CachedStatsName.users_accepted_tos, v)\n\n    def _insert_cached_stats(self, name: CachedStatsName, stats: dict | list):\n        row: CachedStats | None = self.db.query(CachedStats).filter(CachedStats.name == name).one_or_none()\n        if row:\n            row.modified_date = utcnow()\n            row.stats = stats\n            flag_modified(row, \"stats\")\n        else:\n            row = CachedStats(name=name, modified_date=utcnow(), stats=stats)\n        self.db.add(row)\n\n    def get_stats(self, name: CachedStatsName) -> CachedStatsResponse:\n        row: CachedStats | None = self.db.query(CachedStats).filter(CachedStats.name == name).one_or_none()\n        if not row:\n            raise OasstError(f\"Cached stats '{name.value}' not found.\", OasstErrorCode.CACHED_STATS_NOT_AVAILABLE)\n        return CachedStatsResponse(name=row.name, last_updated=row.modified_date, stats=row.stats)\n\n    def get_stats_all(self) -> AllCachedStatsResponse:\n        by_name: dict[CachedStatsName, CachedStatsResponse] = {}\n        qry = self.db.query(CachedStats)\n        for row in qry:\n            by_name[row.name] = CachedStatsResponse(name=row.name, last_updated=row.modified_date, stats=row.stats)\n        return AllCachedStatsResponse(stats_by_name=by_name)\n\n\nif __name__ == \"__main__\":\n    # from oasst_backend.api.deps import create_api_client\n    from oasst_backend.database import engine\n\n    with Session(engine) as db:\n        csr = CachedStatsRepository(db)\n        csr.update_all_cached_stats()()\n        db.commit()\n", "backend/oasst_backend/journal_writer.py": "import enum\nfrom typing import Literal, Optional\nfrom uuid import UUID\n\nfrom oasst_backend.models import ApiClient, Journal, Task, User\nfrom oasst_backend.models.payload_column_type import PayloadContainer, payload_type\nfrom oasst_backend.utils.database_utils import CommitMode, managed_tx_method\nfrom oasst_shared.utils import utcnow\nfrom pydantic import BaseModel\nfrom sqlmodel import Session\n\n\nclass JournalEventType(str, enum.Enum):\n    \"\"\"A label for a piece of text.\"\"\"\n\n    user_created = \"user_created\"\n    text_reply_to_message = \"text_reply_to_message\"\n    message_rating = \"message_rating\"\n    message_ranking = \"message_ranking\"\n\n\n@payload_type\nclass JournalEvent(BaseModel):\n    type: str\n    user_id: Optional[UUID]\n    message_id: Optional[UUID]\n    task_id: Optional[UUID]\n    task_type: Optional[str]\n\n\n@payload_type\nclass TextReplyEvent(JournalEvent):\n    type: Literal[JournalEventType.text_reply_to_message] = JournalEventType.text_reply_to_message\n    length: int\n    role: str\n\n\n@payload_type\nclass RatingEvent(JournalEvent):\n    type: Literal[JournalEventType.message_rating] = JournalEventType.message_rating\n    rating: int\n\n\n@payload_type\nclass RankingEvent(JournalEvent):\n    type: Literal[JournalEventType.message_ranking] = JournalEventType.message_ranking\n    ranking: list[int]\n\n\nclass JournalWriter:\n    def __init__(self, db: Session, api_client: ApiClient, user: User):\n        self.db = db\n        self.api_client = api_client\n        self.user = user\n        self.user_id = self.user.id if self.user else None\n\n    def log_text_reply(self, task: Task, message_id: Optional[UUID], role: str, length: int) -> Journal:\n        return self.log(\n            task_type=task.payload_type,\n            event_type=JournalEventType.text_reply_to_message,\n            payload=TextReplyEvent(role=role, length=length),\n            task_id=task.id,\n            message_id=message_id,\n        )\n\n    def log_rating(self, task: Task, message_id: Optional[UUID], rating: int) -> Journal:\n        return self.log(\n            task_type=task.payload_type,\n            event_type=JournalEventType.message_rating,\n            payload=RatingEvent(rating=rating),\n            task_id=task.id,\n            message_id=message_id,\n        )\n\n    def log_ranking(self, task: Task, message_id: Optional[UUID], ranking: list[int]) -> Journal:\n        return self.log(\n            task_type=task.payload_type,\n            event_type=JournalEventType.message_ranking,\n            payload=RankingEvent(ranking=ranking),\n            task_id=task.id,\n            message_id=message_id,\n        )\n\n    @managed_tx_method(CommitMode.FLUSH)\n    def log(\n        self,\n        *,\n        payload: JournalEvent,\n        task_type: str,\n        event_type: str = None,\n        task_id: Optional[UUID] = None,\n        message_id: Optional[UUID] = None,\n        commit: bool = True,\n    ) -> Journal:\n        if event_type is None:\n            if payload is None:\n                event_type = \"null\"\n            else:\n                event_type = type(payload).__name__\n\n        if payload.user_id is None:\n            payload.user_id = self.user_id\n        if payload.message_id is None:\n            payload.message_id = message_id\n        if payload.task_id is None:\n            payload.task_id = task_id\n        if payload.task_type is None:\n            payload.task_type = task_type\n\n        entry = Journal(\n            user_id=self.user_id,\n            api_client_id=self.api_client.id,\n            created_date=utcnow(),\n            event_type=event_type,\n            event_payload=PayloadContainer(payload=payload),\n            message_id=message_id,\n        )\n\n        self.db.add(entry)\n        return entry\n", "backend/oasst_backend/user_stats_repository.py": "from datetime import datetime, timedelta\nfrom typing import Optional\nfrom uuid import UUID\n\nimport numpy as np\nimport sqlalchemy as sa\nfrom loguru import logger\nfrom oasst_backend.config import settings\nfrom oasst_backend.models import (\n    Message,\n    MessageReaction,\n    MessageTreeState,\n    Task,\n    TextLabels,\n    TrollStats,\n    User,\n    UserStats,\n    UserStatsTimeFrame,\n)\nfrom oasst_backend.models.db_payload import (\n    LabelAssistantReplyPayload,\n    LabelInitialPromptPayload,\n    LabelPrompterReplyPayload,\n    RankingReactionPayload,\n)\nfrom oasst_backend.models.message_tree_state import State as TreeState\nfrom oasst_shared.schemas.protocol import (\n    EmojiCode,\n    LabelTaskMode,\n    LeaderboardStats,\n    TextLabel,\n    TrollboardStats,\n    TrollScore,\n    UserScore,\n)\nfrom oasst_shared.utils import log_timing, utcnow\nfrom sqlalchemy.dialects import postgresql\nfrom sqlalchemy.sql.functions import coalesce\nfrom sqlmodel import Session, delete, func, text\n\n\ndef get_thresholds(baseline: int = 3, alpha: float = 1.1521, max_level: int = 100) -> np.ndarray:\n    level = np.round(np.cumsum(np.arange(1, max_level) * alpha + baseline))\n    return np.array([0] + level.astype(int).tolist())\n\n\n# lookup table, never changes\nTHRESHOLDS = get_thresholds()\n\n\ndef _create_user_score(r, highlighted_user_id: UUID | None) -> UserScore:\n    if r[\"UserStats\"]:\n        d = r[\"UserStats\"].dict()\n        d[\"level\"] = (THRESHOLDS <= d[\"leader_score\"]).sum()\n    else:\n        d = {\"modified_date\": utcnow()}\n        d[\"level\"] = 0\n    for k in [\n        \"user_id\",\n        \"username\",\n        \"auth_method\",\n        \"display_name\",\n        \"streak_days\",\n        \"streak_last_day_date\",\n        \"last_activity_date\",\n    ]:\n        d[k] = r[k]\n    if highlighted_user_id:\n        d[\"highlighted\"] = r[\"user_id\"] == highlighted_user_id\n\n    return UserScore(**d)\n\n\ndef _create_troll_score(r, highlighted_user_id: UUID | None) -> TrollScore:\n    if r[\"TrollStats\"]:\n        d = r[\"TrollStats\"].dict()\n    else:\n        d = {\"modified_date\": utcnow()}\n    for k in [\n        \"user_id\",\n        \"username\",\n        \"auth_method\",\n        \"display_name\",\n        \"last_activity_date\",\n        \"enabled\",\n        \"deleted\",\n        \"show_on_leaderboard\",\n    ]:\n        d[k] = r[k]\n    if highlighted_user_id:\n        d[\"highlighted\"] = r[\"user_id\"] == highlighted_user_id\n    return TrollScore(**d)\n\n\nclass UserStatsRepository:\n    def __init__(self, session: Session):\n        self.session = session\n\n    def get_leaderboard(\n        self,\n        time_frame: UserStatsTimeFrame,\n        limit: int = 100,\n        highlighted_user_id: Optional[UUID] = None,\n    ) -> LeaderboardStats:\n        \"\"\"\n        Get leaderboard stats for the specified time frame\n        \"\"\"\n\n        qry = (\n            self.session.query(\n                User.id.label(\"user_id\"),\n                User.username,\n                User.auth_method,\n                User.display_name,\n                User.streak_days,\n                User.streak_last_day_date,\n                User.last_activity_date,\n                UserStats,\n            )\n            .join(UserStats, User.id == UserStats.user_id)\n            .filter(UserStats.time_frame == time_frame.value, User.show_on_leaderboard, User.enabled)\n            .order_by(UserStats.rank)\n            .limit(limit)\n        )\n\n        leaderboard = [_create_user_score(r, highlighted_user_id) for r in self.session.exec(qry)]\n        if len(leaderboard) > 0:\n            last_update = max(x.modified_date for x in leaderboard)\n        else:\n            last_update = utcnow()\n        return LeaderboardStats(time_frame=time_frame.value, leaderboard=leaderboard, last_updated=last_update)\n\n    def get_leaderboard_user_window(\n        self,\n        user: User,\n        time_frame: UserStatsTimeFrame,\n        window_size: int = 5,\n    ) -> LeaderboardStats | None:\n        # no window for users who don't show themselves\n        if not user.show_on_leaderboard or not user.enabled:\n            return None\n\n        qry = self.session.query(UserStats).filter(UserStats.user_id == user.id, UserStats.time_frame == time_frame)\n        stats: UserStats = qry.one_or_none()\n        if stats is None or stats.rank is None:\n            return None\n\n        min_rank = max(0, stats.rank - window_size // 2)\n        max_rank = min_rank + window_size\n\n        qry = (\n            self.session.query(\n                User.id.label(\"user_id\"),\n                User.username,\n                User.auth_method,\n                User.display_name,\n                User.streak_days,\n                User.streak_last_day_date,\n                User.last_activity_date,\n                UserStats,\n            )\n            .join(UserStats, User.id == UserStats.user_id)\n            .filter(UserStats.time_frame == time_frame.value, User.show_on_leaderboard, User.enabled)\n            .where(UserStats.rank >= min_rank, UserStats.rank <= max_rank)\n            .order_by(UserStats.rank)\n        )\n\n        leaderboard = [_create_user_score(r, highlighted_user_id=user.id) for r in self.session.exec(qry)]\n        if len(leaderboard) > 0:\n            last_update = max(x.modified_date for x in leaderboard)\n        else:\n            last_update = utcnow()\n        return LeaderboardStats(time_frame=time_frame.value, leaderboard=leaderboard, last_updated=last_update)\n\n    def get_user_stats_all_time_frames(self, user_id: UUID) -> dict[str, UserScore | None]:\n        qry = (\n            self.session.query(\n                User.id.label(\"user_id\"),\n                User.username,\n                User.auth_method,\n                User.display_name,\n                User.streak_days,\n                User.streak_last_day_date,\n                User.last_activity_date,\n                UserStats,\n            )\n            .outerjoin(UserStats, User.id == UserStats.user_id)\n            .filter(User.id == user_id)\n        )\n\n        stats_by_timeframe = {}\n        for r in self.session.exec(qry):\n            us = r[\"UserStats\"]\n            if us is not None:\n                stats_by_timeframe[us.time_frame] = _create_user_score(r, user_id)\n            else:\n                stats_by_timeframe = {tf.value: _create_user_score(r, user_id) for tf in UserStatsTimeFrame}\n        return stats_by_timeframe\n\n    def get_trollboard(\n        self,\n        time_frame: UserStatsTimeFrame,\n        limit: int = 100,\n        enabled: Optional[bool] = None,\n        highlighted_user_id: Optional[UUID] = None,\n    ) -> TrollboardStats:\n        \"\"\"\n        Get trollboard stats for the specified time frame\n        \"\"\"\n\n        qry = (\n            self.session.query(\n                User.id.label(\"user_id\"),\n                User.username,\n                User.auth_method,\n                User.display_name,\n                User.last_activity_date,\n                User.enabled,\n                User.deleted,\n                User.show_on_leaderboard,\n                TrollStats,\n            )\n            .join(TrollStats, User.id == TrollStats.user_id)\n            .filter(TrollStats.time_frame == time_frame.value)\n        )\n\n        if enabled is not None:\n            qry = qry.filter(User.enabled == enabled)\n\n        qry = qry.order_by(TrollStats.rank).limit(limit)\n\n        trollboard = [_create_troll_score(r, highlighted_user_id) for r in self.session.exec(qry)]\n        if len(trollboard) > 0:\n            last_update = max(x.modified_date for x in trollboard)\n        else:\n            last_update = utcnow()\n        return TrollboardStats(time_frame=time_frame.value, trollboard=trollboard, last_updated=last_update)\n\n    def query_total_prompts_per_user(\n        self, reference_time: Optional[datetime] = None, only_reviewed: Optional[bool] = True\n    ):\n        qry = self.session.query(Message.user_id, func.count()).filter(\n            Message.deleted == sa.false(), Message.parent_id.is_(None)\n        )\n        if reference_time:\n            qry = qry.filter(Message.created_date >= reference_time)\n        if only_reviewed:\n            qry = qry.filter(Message.review_result == sa.true())\n        qry = qry.group_by(Message.user_id)\n        return qry\n\n    def query_replies_by_role_per_user(\n        self, reference_time: Optional[datetime] = None, only_reviewed: Optional[bool] = True\n    ) -> list:\n        qry = self.session.query(Message.user_id, Message.role, func.count()).filter(\n            Message.deleted == sa.false(), Message.parent_id.is_not(None)\n        )\n        if reference_time:\n            qry = qry.filter(Message.created_date >= reference_time)\n        if only_reviewed:\n            qry = qry.filter(Message.review_result == sa.true())\n        qry = qry.group_by(Message.user_id, Message.role)\n        return qry\n\n    def query_labels_by_mode_per_user(\n        self, payload_type: str = LabelAssistantReplyPayload.__name__, reference_time: Optional[datetime] = None\n    ):\n        qry = self.session.query(Task.user_id, Task.payload[\"payload\", \"mode\"].astext, func.count()).filter(\n            Task.done == sa.true(), Task.payload_type == payload_type\n        )\n        if reference_time:\n            qry = qry.filter(Task.created_date >= reference_time)\n        qry = qry.group_by(Task.user_id, Task.payload[\"payload\", \"mode\"].astext)\n        return qry\n\n    def query_rankings_per_user(self, reference_time: Optional[datetime] = None):\n        qry = self.session.query(MessageReaction.user_id, func.count()).filter(\n            MessageReaction.payload_type == RankingReactionPayload.__name__\n        )\n        if reference_time:\n            qry = qry.filter(MessageReaction.created_date >= reference_time)\n        qry = qry.group_by(MessageReaction.user_id)\n        return qry\n\n    def query_ranking_result_users(self, rank: int = 0, reference_time: Optional[datetime] = None):\n        ranked_message_id = MessageReaction.payload[\"payload\", \"ranked_message_ids\", rank].astext.cast(\n            postgresql.UUID(as_uuid=True)\n        )\n        qry = (\n            self.session.query(Message.user_id, func.count())\n            .select_from(MessageReaction)\n            .join(Message, ranked_message_id == Message.id)\n            .filter(MessageReaction.payload_type == RankingReactionPayload.__name__)\n        )\n        if reference_time:\n            qry = qry.filter(MessageReaction.created_date >= reference_time)\n        qry = qry.group_by(Message.user_id)\n        return qry\n\n    def _update_stats_internal(self, time_frame: UserStatsTimeFrame, base_date: Optional[datetime] = None):\n        # gather user data\n\n        time_frame_key = time_frame.value\n\n        stats_by_user: dict[UUID, UserStats] = dict()\n        now = utcnow()\n\n        def get_stats(id: UUID) -> UserStats:\n            us = stats_by_user.get(id)\n            if not us:\n                us = UserStats(user_id=id, time_frame=time_frame_key, modified_date=now, base_date=base_date)\n                stats_by_user[id] = us\n            return us\n\n        # total prompts\n        qry = self.query_total_prompts_per_user(reference_time=base_date, only_reviewed=False)\n        for r in qry:\n            uid, count = r\n            get_stats(uid).prompts = count\n\n        # accepted prompts\n        qry = self.query_total_prompts_per_user(reference_time=base_date, only_reviewed=True)\n        for r in qry:\n            uid, count = r\n            get_stats(uid).accepted_prompts = count\n\n        # total replies\n        qry = self.query_replies_by_role_per_user(reference_time=base_date, only_reviewed=False)\n        for r in qry:\n            uid, role, count = r\n            s = get_stats(uid)\n            if role == \"assistant\":\n                s.replies_assistant += count\n            elif role == \"prompter\":\n                s.replies_prompter += count\n\n        # accepted replies\n        qry = self.query_replies_by_role_per_user(reference_time=base_date, only_reviewed=True)\n        for r in qry:\n            uid, role, count = r\n            s = get_stats(uid)\n            if role == \"assistant\":\n                s.accepted_replies_assistant += count\n            elif role == \"prompter\":\n                s.accepted_replies_prompter += count\n\n        # simple and full labels\n        qry = self.query_labels_by_mode_per_user(\n            payload_type=LabelAssistantReplyPayload.__name__, reference_time=base_date\n        )\n        for r in qry:\n            uid, mode, count = r\n            s = get_stats(uid)\n            if mode == LabelTaskMode.simple:\n                s.labels_simple = count\n            elif mode == LabelTaskMode.full:\n                s.labels_full = count\n\n        qry = self.query_labels_by_mode_per_user(\n            payload_type=LabelPrompterReplyPayload.__name__, reference_time=base_date\n        )\n        for r in qry:\n            uid, mode, count = r\n            s = get_stats(uid)\n            if mode == LabelTaskMode.simple:\n                s.labels_simple += count\n            elif mode == LabelTaskMode.full:\n                s.labels_full += count\n\n        qry = self.query_labels_by_mode_per_user(\n            payload_type=LabelInitialPromptPayload.__name__, reference_time=base_date\n        )\n        for r in qry:\n            uid, mode, count = r\n            s = get_stats(uid)\n            if mode == LabelTaskMode.simple:\n                s.labels_simple += count\n            elif mode == LabelTaskMode.full:\n                s.labels_full += count\n\n        qry = self.query_rankings_per_user(reference_time=base_date)\n        for r in qry:\n            uid, count = r\n            get_stats(uid).rankings_total = count\n\n        rank_field_names = [\"reply_ranked_1\", \"reply_ranked_2\", \"reply_ranked_3\"]\n        for i, fn in enumerate(rank_field_names):\n            qry = self.query_ranking_result_users(reference_time=base_date, rank=i)\n            for r in qry:\n                uid, count = r\n                setattr(get_stats(uid), fn, count)\n\n        # delete all existing stast for time frame\n        d = delete(UserStats).where(UserStats.time_frame == time_frame_key)\n        self.session.execute(d)\n\n        if None in stats_by_user:\n            logger.warning(\"Some messages in DB have NULL values in user_id column.\")\n            del stats_by_user[None]\n\n        # compute magic leader score\n        for v in stats_by_user.values():\n            v.leader_score = v.compute_leader_score()\n\n        # insert user objects\n        self.session.add_all(stats_by_user.values())\n        self.session.flush()\n\n        self.update_leader_ranks(time_frame=time_frame)\n\n    def query_message_emoji_counts_per_user(self, reference_time: Optional[datetime] = None):\n        qry = self.session.query(\n            Message.user_id,\n            func.sum(coalesce(Message.emojis[EmojiCode.thumbs_up].cast(sa.Integer), 0)).label(\"up\"),\n            func.sum(coalesce(Message.emojis[EmojiCode.thumbs_down].cast(sa.Integer), 0)).label(\"down\"),\n            func.sum(coalesce(Message.emojis[EmojiCode.red_flag].cast(sa.Integer), 0)).label(\"flag\"),\n        ).filter(Message.deleted == sa.false(), Message.emojis.is_not(None))\n\n        if reference_time:\n            qry = qry.filter(Message.created_date >= reference_time)\n\n        qry = qry.group_by(Message.user_id)\n        return qry\n\n    def query_spam_prompts_per_user(self, reference_time: Optional[datetime] = None):\n        qry = (\n            self.session.query(Message.user_id, func.count().label(\"spam_prompts\"))\n            .select_from(MessageTreeState)\n            .join(Message, MessageTreeState.message_tree_id == Message.id)\n            .filter(MessageTreeState.state == TreeState.ABORTED_LOW_GRADE)\n        )\n\n        if reference_time:\n            qry = qry.filter(Message.created_date >= reference_time)\n\n        qry = qry.group_by(Message.user_id)\n        return qry\n\n    def query_labels_per_user(self, reference_time: Optional[datetime] = None):\n        qry = (\n            self.session.query(\n                Message.user_id,\n                func.sum(coalesce(TextLabels.labels[TextLabel.spam].cast(sa.Integer), 0)).label(\"spam\"),\n                func.sum(coalesce(TextLabels.labels[TextLabel.lang_mismatch].cast(sa.Integer), 0)).label(\n                    \"lang_mismach\"\n                ),\n                func.sum(coalesce(TextLabels.labels[TextLabel.not_appropriate].cast(sa.Integer), 0)).label(\n                    \"not_appropriate\"\n                ),\n                func.sum(coalesce(TextLabels.labels[TextLabel.pii].cast(sa.Integer), 0)).label(\"pii\"),\n                func.sum(coalesce(TextLabels.labels[TextLabel.hate_speech].cast(sa.Integer), 0)).label(\"hate_speech\"),\n                func.sum(coalesce(TextLabels.labels[TextLabel.sexual_content].cast(sa.Integer), 0)).label(\n                    \"sexual_content\"\n                ),\n                func.sum(coalesce(TextLabels.labels[TextLabel.political_content].cast(sa.Integer), 0)).label(\n                    \"political_content\"\n                ),\n                func.avg(TextLabels.labels[TextLabel.quality].cast(sa.Float)).label(\"quality\"),\n                func.avg(TextLabels.labels[TextLabel.humor].cast(sa.Float)).label(\"humor\"),\n                func.avg(TextLabels.labels[TextLabel.toxicity].cast(sa.Float)).label(\"toxicity\"),\n                func.avg(TextLabels.labels[TextLabel.violence].cast(sa.Float)).label(\"violence\"),\n                func.avg(TextLabels.labels[TextLabel.helpfulness].cast(sa.Float)).label(\"helpfulness\"),\n            )\n            .select_from(TextLabels)\n            .join(Message, TextLabels.message_id == Message.id)\n            .filter(Message.deleted == sa.false(), Message.emojis.is_not(None))\n        )\n\n        if reference_time:\n            qry = qry.filter(Message.created_date >= reference_time)\n\n        qry = qry.group_by(Message.user_id)\n        return qry\n\n    def _update_troll_stats_internal(self, time_frame: UserStatsTimeFrame, base_date: Optional[datetime] = None):\n        # gather user data\n\n        time_frame_key = time_frame.value\n\n        stats_by_user: dict[UUID, TrollStats] = dict()\n        now = utcnow()\n\n        def get_stats(id: UUID) -> TrollStats:\n            us = stats_by_user.get(id)\n            if not us:\n                us = TrollStats(user_id=id, time_frame=time_frame_key, modified_date=now, base_date=base_date)\n                stats_by_user[id] = us\n            return us\n\n        # emoji counts of user's messages\n        qry = self.query_message_emoji_counts_per_user(reference_time=base_date)\n        for r in qry:\n            uid = r[\"user_id\"]\n            s = get_stats(uid)\n            s.upvotes = r[\"up\"]\n            s.downvotes = r[\"down\"]\n            s.red_flags = r[\"flag\"]\n\n        # num spam prompts\n        qry = self.query_spam_prompts_per_user(reference_time=base_date)\n        for r in qry:\n            uid, count = r\n            s = get_stats(uid).spam_prompts = count\n\n        label_field_names = (\n            \"quality\",\n            \"humor\",\n            \"toxicity\",\n            \"violence\",\n            \"helpfulness\",\n            \"spam\",\n            \"lang_mismach\",\n            \"not_appropriate\",\n            \"pii\",\n            \"hate_speech\",\n            \"sexual_content\",\n            \"political_content\",\n        )\n\n        # label counts / mean values\n        qry = self.query_labels_per_user(reference_time=base_date)\n        for r in qry:\n            uid = r[\"user_id\"]\n            s = get_stats(uid)\n            for fn in label_field_names:\n                setattr(s, fn, r[fn])\n\n        # delete all existing stast for time frame\n        d = delete(TrollStats).where(TrollStats.time_frame == time_frame_key)\n        self.session.execute(d)\n\n        if None in stats_by_user:\n            logger.warning(\"Some messages in DB have NULL values in user_id column.\")\n            del stats_by_user[None]\n\n        # compute magic leader score\n        for v in stats_by_user.values():\n            v.troll_score = v.compute_troll_score()\n\n        # insert user objects\n        self.session.add_all(stats_by_user.values())\n        self.session.flush()\n\n        self.update_troll_ranks(time_frame=time_frame)\n\n    @log_timing(log_kwargs=True)\n    def update_leader_ranks(self, time_frame: UserStatsTimeFrame = None):\n        \"\"\"\n        Update user_stats ranks. The persisted rank values allow to\n        quickly the rank of a single user and to query nearby users.\n        \"\"\"\n\n        # todo: convert sql to sqlalchemy query..\n        # ranks = self.session.query(\n        #     func.row_number()\n        #     .over(partition_by=UserStats.time_frame, order_by=[UserStats.leader_score.desc(), UserStats.user_id])\n        #     .label(\"rank\"),\n        #     UserStats.user_id,\n        #     UserStats.time_frame,\n        # )\n\n        sql_update_rank = \"\"\"\n-- update rank\nUPDATE user_stats us\nSET \"rank\" = r.\"rank\"\nFROM\n    (SELECT\n        ROW_NUMBER () OVER(\n            PARTITION BY time_frame\n            ORDER BY leader_score DESC, user_id\n        ) AS \"rank\", user_id, time_frame\n    FROM user_stats us2\n    INNER JOIN \"user\" u ON us2.user_id = u.id AND u.show_on_leaderboard AND u.enabled\n    WHERE (:time_frame IS NULL OR time_frame = :time_frame)) AS r\nWHERE\n    us.user_id = r.user_id\n    AND us.time_frame = r.time_frame;\"\"\"\n        r = self.session.execute(\n            text(sql_update_rank), {\"time_frame\": time_frame.value if time_frame is not None else None}\n        )\n        logger.debug(f\"pre_compute_ranks leader updated({time_frame=}) {r.rowcount} rows.\")\n\n    @log_timing(log_kwargs=True)\n    def update_troll_ranks(self, time_frame: UserStatsTimeFrame = None):\n        sql_update_troll_rank = \"\"\"\n-- update rank\nUPDATE troll_stats ts\nSET \"rank\" = r.\"rank\"\nFROM\n    (SELECT\n        ROW_NUMBER () OVER(\n            PARTITION BY time_frame\n            ORDER BY troll_score DESC, user_id\n        ) AS \"rank\", user_id, time_frame\n    FROM troll_stats ts2\n    WHERE (:time_frame IS NULL OR time_frame = :time_frame)) AS r\nWHERE\n    ts.user_id = r.user_id\n    AND ts.time_frame = r.time_frame;\"\"\"\n        r = self.session.execute(\n            text(sql_update_troll_rank), {\"time_frame\": time_frame.value if time_frame is not None else None}\n        )\n        logger.debug(f\"pre_compute_ranks troll updated({time_frame=}) {r.rowcount} rows.\")\n\n    def update_stats_time_frame(\n        self,\n        time_frame: UserStatsTimeFrame,\n        reference_time: Optional[datetime] = None,\n        leader_stats: bool = True,\n        troll_stats: bool = True,\n    ):\n        if leader_stats:\n            self._update_stats_internal(time_frame, reference_time)\n        if troll_stats:\n            self._update_troll_stats_internal(time_frame, reference_time)\n        self.session.commit()\n\n    @log_timing(log_kwargs=True, level=\"INFO\")\n    def update_stats(self, *, time_frame: UserStatsTimeFrame):\n        now = utcnow()\n        match time_frame:\n            case UserStatsTimeFrame.day:\n                r = now - timedelta(days=1)\n                self.update_stats_time_frame(time_frame, r)\n\n            case UserStatsTimeFrame.week:\n                r = now.date() - timedelta(days=7)\n                r = datetime(r.year, r.month, r.day, tzinfo=now.tzinfo)\n                self.update_stats_time_frame(time_frame, r)\n\n            case UserStatsTimeFrame.month:\n                r = now.date() - timedelta(days=30)\n                r = datetime(r.year, r.month, r.day, tzinfo=now.tzinfo)\n                self.update_stats_time_frame(time_frame, r)\n\n            case UserStatsTimeFrame.total:\n                self.update_stats_time_frame(time_frame, None)\n\n    @log_timing(level=\"INFO\")\n    def update_multiple_time_frames(self, time_frames: list[UserStatsTimeFrame]):\n        for t in time_frames:\n            self.update_stats(time_frame=t)\n\n    @log_timing(level=\"INFO\")\n    def update_all_time_frames(self):\n        self.update_multiple_time_frames(list(UserStatsTimeFrame))\n\n\nif __name__ == \"__main__\":\n    from oasst_backend.api.deps import api_auth\n    from oasst_backend.database import engine\n\n    with Session(engine) as db:\n        api_client = api_auth(settings.OFFICIAL_WEB_API_KEY, db=db)\n        usr = UserStatsRepository(db)\n        usr.update_all_time_frames()\n        db.commit()\n", "backend/oasst_backend/user_repository.py": "from typing import Optional\nfrom uuid import UUID\n\nimport oasst_backend.models as models\nfrom oasst_backend.config import settings\nfrom oasst_backend.models import ApiClient, User\nfrom oasst_backend.utils.database_utils import CommitMode, managed_tx_method\nfrom oasst_shared import utils as shared_utils\nfrom oasst_shared.exceptions import OasstError, OasstErrorCode\nfrom oasst_shared.schemas import protocol as protocol_schema\nfrom oasst_shared.utils import utcnow\nfrom sqlalchemy.exc import IntegrityError\nfrom sqlmodel import Session, and_, delete, or_, update\nfrom starlette.status import HTTP_403_FORBIDDEN, HTTP_404_NOT_FOUND\n\n\nclass UserRepository:\n    def __init__(self, db: Session, api_client: ApiClient):\n        self.db = db\n        self.api_client = api_client\n\n    def get_user(self, id: UUID, api_client_id: Optional[UUID] = None) -> User:\n        \"\"\"\n        Get a user by global user ID. All clients may get users with the same API client ID as the querying client.\n        Trusted clients can get any user.\n\n        Raises:\n            OasstError: 403 if untrusted client attempts to query foreign users. 404 if user with ID not found.\n        \"\"\"\n        if not self.api_client.trusted and api_client_id is None:\n            api_client_id = self.api_client.id\n\n        if not self.api_client.trusted and api_client_id != self.api_client.id:\n            # Unprivileged client requests foreign user\n            raise OasstError(\"Forbidden\", OasstErrorCode.API_CLIENT_NOT_AUTHORIZED, HTTP_403_FORBIDDEN)\n\n        # Will always be unique\n        user_query = self.db.query(User).filter(User.id == id)\n\n        if api_client_id:\n            user_query = user_query.filter(User.api_client_id == api_client_id)\n\n        user: User = user_query.first()\n\n        if user is None:\n            raise OasstError(\"User not found\", OasstErrorCode.USER_NOT_FOUND, HTTP_404_NOT_FOUND)\n\n        return user\n\n    def query_frontend_user(\n        self, auth_method: str, username: str, api_client_id: Optional[UUID] = None\n    ) -> Optional[User]:\n        if not api_client_id:\n            api_client_id = self.api_client.id\n\n        if not self.api_client.trusted and api_client_id != self.api_client.id:\n            # Unprivileged API client asks for foreign user\n            raise OasstError(\"Forbidden\", OasstErrorCode.API_CLIENT_NOT_AUTHORIZED, HTTP_403_FORBIDDEN)\n\n        user: User = (\n            self.db.query(User)\n            .filter(User.auth_method == auth_method, User.username == username, User.api_client_id == api_client_id)\n            .first()\n        )\n\n        if user is None:\n            raise OasstError(\"User not found\", OasstErrorCode.USER_NOT_FOUND, HTTP_404_NOT_FOUND)\n\n        return user\n\n    @managed_tx_method(CommitMode.COMMIT)\n    def update_user(\n        self,\n        id: UUID,\n        display_name: Optional[str] = None,\n        enabled: Optional[bool] = None,\n        notes: Optional[str] = None,\n        show_on_leaderboard: Optional[bool] = None,\n        tos_acceptance: Optional[bool] = None,\n    ) -> User:\n        \"\"\"\n        Update a user by global user ID to disable or set admin notes. Only trusted clients may update users.\n\n        Raises:\n            OasstError: 403 if untrusted client attempts to update a user. 404 if user with ID not found.\n        \"\"\"\n        if not self.api_client.trusted:\n            raise OasstError(\"Forbidden\", OasstErrorCode.API_CLIENT_NOT_AUTHORIZED, HTTP_403_FORBIDDEN)\n\n        user: User = self.db.query(User).filter(User.id == id).first()\n        if user is None:\n            raise OasstError(\"User not found\", OasstErrorCode.USER_NOT_FOUND, HTTP_404_NOT_FOUND)\n\n        if enabled is not None:\n            user.enabled = enabled\n        if notes is not None:\n            user.notes = notes\n        if show_on_leaderboard is not None:\n            user.show_on_leaderboard = show_on_leaderboard\n        if tos_acceptance:\n            user.tos_acceptance_date = utcnow()\n        if display_name is not None:\n            user.display_name = display_name\n\n        self.db.add(user)\n\n        return user\n\n    @managed_tx_method(CommitMode.COMMIT)\n    def mark_user_deleted(self, id: UUID) -> None:\n        \"\"\"\n        Update a user by global user ID to set deleted flag. Only trusted clients may delete users.\n        User deletion anonymises the data of the user.\n\n        Raises:\n            OasstError: 403 if untrusted client attempts to delete a user. 404 if user with ID not found.\n        \"\"\"\n        if not self.api_client.trusted:\n            raise OasstError(\"Forbidden\", OasstErrorCode.API_CLIENT_NOT_AUTHORIZED, HTTP_403_FORBIDDEN)\n\n        user: User = self.db.query(User).filter(User.id == id).first()\n\n        if user is None:\n            raise OasstError(\"User not found\", OasstErrorCode.USER_NOT_FOUND, HTTP_404_NOT_FOUND)\n\n        user.deleted = True\n\n        # Anonymise user data\n        user.display_name = shared_utils.DELETED_USER_DISPLAY_NAME\n        # Ensure uniqueness of (username, auth_method, api_client_id) Index\n        user.username = f\"{shared_utils.DELETED_USER_ID_PREFIX}{user.id}\"\n        user.show_on_leaderboard = False\n\n        self.db.add(user)\n\n    @managed_tx_method(CommitMode.COMMIT)\n    def _lookup_user_tx(\n        self,\n        *,\n        username: str,\n        auth_method: str,\n        display_name: Optional[str] = None,\n        create_missing: bool = True,\n    ) -> User | None:\n        user: User = (\n            self.db.query(User)\n            .filter(\n                User.api_client_id == self.api_client.id,\n                User.username == username,\n                User.auth_method == auth_method,\n            )\n            .first()\n        )\n        if user is None:\n            if create_missing:\n                # user is unknown, create new record\n                user = User(\n                    username=username,\n                    display_name=display_name,\n                    api_client_id=self.api_client.id,\n                    auth_method=auth_method,\n                )\n                if auth_method == \"system\":\n                    user.show_on_leaderboard = False  # don't show system users, e.g. import user\n                    user.tos_acceptance_date = utcnow()\n                self.db.add(user)\n        elif display_name and display_name != user.display_name:\n            # we found the user but the display name changed\n            user.display_name = display_name\n            self.db.add(user)\n\n        return user\n\n    def lookup_client_user(self, client_user: protocol_schema.User, create_missing: bool = True) -> User | None:\n        if not client_user:\n            return None\n\n        if not (client_user.auth_method and client_user.id):\n            raise OasstError(\"Auth method or username missing.\", OasstErrorCode.AUTH_AND_USERNAME_REQUIRED)\n\n        num_retries = settings.DATABASE_MAX_TX_RETRY_COUNT\n        for i in range(num_retries):\n            try:\n                return self._lookup_user_tx(\n                    username=client_user.id,\n                    auth_method=client_user.auth_method,\n                    display_name=client_user.display_name,\n                    create_missing=create_missing,\n                )\n            except IntegrityError:\n                # catch UniqueViolation exception, for concurrent requests due to conflicts in ix_user_username\n                if i + 1 == num_retries:\n                    raise\n\n    @managed_tx_method(CommitMode.COMMIT)\n    def lookup_system_user(self, username: str, create_missing: bool = True) -> User | None:\n        return self._lookup_user_tx(\n            username=username,\n            auth_method=\"system\",\n            display_name=f\"__system__/{username}\",\n            create_missing=create_missing,\n        )\n\n    def query_users_ordered_by_username(\n        self,\n        api_client_id: Optional[UUID] = None,\n        gte_username: Optional[str] = None,\n        gt_id: Optional[UUID] = None,\n        lte_username: Optional[str] = None,\n        lt_id: Optional[UUID] = None,\n        auth_method: Optional[str] = None,\n        search_text: Optional[str] = None,\n        limit: Optional[int] = 100,\n        desc: bool = False,\n    ) -> list[User]:\n        if not self.api_client.trusted:\n            if not api_client_id:\n                api_client_id = self.api_client.id\n\n            if api_client_id != self.api_client.id:\n                raise OasstError(\"Forbidden\", OasstErrorCode.API_CLIENT_NOT_AUTHORIZED, HTTP_403_FORBIDDEN)\n\n        qry = self.db.query(User)\n\n        if gte_username is not None:\n            if gt_id:\n                qry = qry.filter(\n                    or_(User.username > gte_username, and_(User.username == gte_username, User.id > gt_id))\n                )\n            else:\n                qry = qry.filter(User.username >= gte_username)\n        elif gt_id:\n            raise OasstError(\"Need id and name for keyset pagination\", OasstErrorCode.GENERIC_ERROR)\n\n        if lte_username is not None:\n            if lt_id:\n                qry = qry.filter(\n                    or_(User.username < lte_username, and_(User.username == lte_username, User.id < lt_id))\n                )\n            else:\n                qry = qry.filter(User.username <= lte_username)\n        elif lt_id:\n            raise OasstError(\"Need id and name for keyset pagination\", OasstErrorCode.GENERIC_ERROR)\n\n        if auth_method:\n            qry = qry.filter(User.auth_method == auth_method)\n        if api_client_id:\n            qry = qry.filter(User.api_client_id == api_client_id)\n\n        if search_text:\n            pattern = \"%{}%\".format(search_text.replace(\"\\\\\", \"\\\\\\\\\").replace(\"_\", \"\\\\_\").replace(\"%\", \"\\\\%\"))\n            qry = qry.filter(User.username.like(pattern))\n\n        if desc:\n            qry = qry.order_by(User.username.desc(), User.id.desc())\n        else:\n            qry = qry.order_by(User.username, User.id)\n\n        if limit is not None:\n            qry = qry.limit(limit)\n\n        return qry.all()\n\n    def query_users_ordered_by_display_name(\n        self,\n        gte_display_name: Optional[str] = None,\n        gt_id: Optional[UUID] = None,\n        lte_display_name: Optional[str] = None,\n        lt_id: Optional[UUID] = None,\n        api_client_id: Optional[UUID] = None,\n        auth_method: Optional[str] = None,\n        search_text: Optional[str] = None,\n        limit: Optional[int] = 100,\n        desc: bool = False,\n    ) -> list[User]:\n        if not self.api_client.trusted:\n            if not api_client_id:\n                # Let unprivileged api clients query their own users without api_client_id being set\n                api_client_id = self.api_client.id\n\n            if api_client_id != self.api_client.id:\n                # Unprivileged api client asks for foreign users\n                raise OasstError(\"Forbidden\", OasstErrorCode.API_CLIENT_NOT_AUTHORIZED, HTTP_403_FORBIDDEN)\n\n        qry = self.db.query(User)\n\n        if gte_display_name is not None:\n            if gt_id:\n                qry = qry.filter(\n                    or_(\n                        User.display_name > gte_display_name,\n                        and_(User.display_name == gte_display_name, User.id > gt_id),\n                    )\n                )\n            else:\n                qry = qry.filter(User.display_name >= gte_display_name)\n        elif gt_id:\n            raise OasstError(\"Need id and name for keyset pagination\", OasstErrorCode.GENERIC_ERROR)\n\n        if lte_display_name is not None:\n            if lt_id:\n                qry = qry.filter(\n                    or_(\n                        User.display_name < lte_display_name,\n                        and_(User.display_name == lte_display_name, User.id < lt_id),\n                    )\n                )\n            else:\n                qry = qry.filter(User.display_name <= lte_display_name)\n        elif lt_id:\n            raise OasstError(\"Need id and name for keyset pagination\", OasstErrorCode.GENERIC_ERROR)\n\n        if auth_method:\n            qry = qry.filter(User.auth_method == auth_method)\n        if api_client_id:\n            qry = qry.filter(User.api_client_id == api_client_id)\n\n        if search_text:\n            pattern = \"%{}%\".format(search_text.replace(\"\\\\\", \"\\\\\\\\\").replace(\"_\", \"\\\\_\").replace(\"%\", \"\\\\%\"))\n            qry = qry.filter(User.display_name.like(pattern))\n\n        if auth_method:\n            qry = qry.filter(User.auth_method == auth_method)\n\n        if desc:\n            qry = qry.order_by(User.display_name.desc(), User.id.desc())\n        else:\n            qry = qry.order_by(User.display_name, User.id)\n\n        if limit is not None:\n            qry = qry.limit(limit)\n\n        return qry.all()\n\n    @managed_tx_method(CommitMode.FLUSH)\n    def update_user_last_activity(self, user: User, update_streak: bool = False) -> None:\n        current_time = utcnow()\n        user.last_activity_date = current_time\n\n        if update_streak:\n            if user.streak_last_day_date is None or user.streak_last_day_date > current_time:\n                # begin new streak\n                user.streak_last_day_date = current_time\n                user.streak_days = 0\n            else:\n                # update streak day count\n                user.streak_days = (current_time - user.streak_last_day_date).days\n\n        self.db.add(user)\n\n    @managed_tx_method(CommitMode.FLUSH)\n    def merge_users(self, destination_user_id: UUID, source_user_ids: list[UUID]) -> None:\n        source_user_ids = list(filter(lambda x: x != destination_user_id, source_user_ids))\n        if not source_user_ids:\n            return\n\n        # ensure the destination user exists\n        self.get_user(id=destination_user_id)\n\n        # update rows in tables that have affected users_ids as FK\n        models_to_update = [\n            models.Message,\n            models.MessageRevision,\n            models.MessageReaction,\n            models.MessageEmoji,\n            models.TextLabels,\n            models.Task,\n            models.Journal,\n        ]\n        for table in models_to_update:\n            qry = update(table).where(table.user_id.in_(source_user_ids)).values(user_id=destination_user_id)\n            self.db.execute(qry)\n\n        # delete rows in user stats tables\n        models_to_delete = [models.UserStats, models.TrollStats]\n        for table in models_to_delete:\n            qry = delete(table).where(table.user_id.in_(source_user_ids))\n            self.db.execute(qry)\n\n        # finally delete source users from main user table\n        qry = delete(User).where(User.id.in_(source_user_ids))\n        self.db.execute(qry)\n", "backend/oasst_backend/__init__.py": "", "backend/oasst_backend/task_repository.py": "from datetime import datetime, timedelta\nfrom typing import Optional\nfrom uuid import UUID\n\nimport oasst_backend.models.db_payload as db_payload\nfrom loguru import logger\nfrom oasst_backend.config import settings\nfrom oasst_backend.models import ApiClient, Task\nfrom oasst_backend.models.payload_column_type import PayloadContainer\nfrom oasst_backend.user_repository import UserRepository\nfrom oasst_backend.utils.database_utils import CommitMode, managed_tx_method\nfrom oasst_shared.exceptions.oasst_api_error import OasstError, OasstErrorCode\nfrom oasst_shared.schemas import protocol as protocol_schema\nfrom oasst_shared.utils import utcnow\nfrom sqlmodel import Session, delete, false, func, not_, or_\nfrom starlette.status import HTTP_404_NOT_FOUND\n\n\ndef validate_frontend_message_id(message_id: str) -> None:\n    # TODO: Should it be replaced with fastapi/pydantic validation?\n    if not isinstance(message_id, str):\n        raise OasstError(\n            f\"message_id must be string, not {type(message_id)}\", OasstErrorCode.INVALID_FRONTEND_MESSAGE_ID\n        )\n    if not message_id:\n        raise OasstError(\"message_id must not be empty\", OasstErrorCode.INVALID_FRONTEND_MESSAGE_ID)\n\n\ndef delete_expired_tasks(session: Session) -> int:\n    stm = delete(Task).where(Task.expiry_date < utcnow(), Task.done == false())\n    result = session.exec(stm)\n    logger.info(f\"Deleted {result.rowcount} expired tasks.\")\n    return result.rowcount\n\n\nclass TaskRepository:\n    def __init__(\n        self,\n        db: Session,\n        api_client: ApiClient,\n        client_user: Optional[protocol_schema.User],\n        user_repository: UserRepository,\n    ):\n        self.db = db\n        self.api_client = api_client\n        self.user_repository = user_repository\n        self.user = self.user_repository.lookup_client_user(client_user, create_missing=True)\n        self.user_id = self.user.id if self.user else None\n\n    def store_task(\n        self,\n        task: protocol_schema.Task,\n        message_tree_id: UUID = None,\n        parent_message_id: UUID = None,\n        collective: bool = False,\n    ) -> Task:\n        payload: db_payload.TaskPayload\n        match type(task):\n            case protocol_schema.SummarizeStoryTask:\n                payload = db_payload.SummarizationStoryPayload(story=task.story)\n\n            case protocol_schema.RateSummaryTask:\n                payload = db_payload.RateSummaryPayload(\n                    full_text=task.full_text, summary=task.summary, scale=task.scale\n                )\n\n            case protocol_schema.InitialPromptTask:\n                payload = db_payload.InitialPromptPayload(hint=task.hint)\n\n            case protocol_schema.PrompterReplyTask:\n                payload = db_payload.PrompterReplyPayload(conversation=task.conversation, hint=task.hint)\n\n            case protocol_schema.AssistantReplyTask:\n                payload = db_payload.AssistantReplyPayload(type=task.type, conversation=task.conversation)\n\n            case protocol_schema.RankInitialPromptsTask:\n                payload = db_payload.RankInitialPromptsPayload(type=task.type, prompt_messages=task.prompt_messages)\n\n            case protocol_schema.RankPrompterRepliesTask:\n                payload = db_payload.RankPrompterRepliesPayload(\n                    type=task.type,\n                    conversation=task.conversation,\n                    reply_messages=task.reply_messages,\n                    ranking_parent_id=task.ranking_parent_id,\n                    message_tree_id=task.message_tree_id,\n                    reveal_synthetic=task.reveal_synthetic,\n                )\n\n            case protocol_schema.RankAssistantRepliesTask:\n                payload = db_payload.RankAssistantRepliesPayload(\n                    type=task.type,\n                    conversation=task.conversation,\n                    reply_messages=task.reply_messages,\n                    ranking_parent_id=task.ranking_parent_id,\n                    message_tree_id=task.message_tree_id,\n                    reveal_synthetic=task.reveal_synthetic,\n                )\n\n            case protocol_schema.LabelInitialPromptTask:\n                payload = db_payload.LabelInitialPromptPayload(\n                    type=task.type,\n                    message_id=task.message_id,\n                    prompt=task.prompt,\n                    valid_labels=task.valid_labels,\n                    mandatory_labels=task.mandatory_labels,\n                    mode=task.mode,\n                )\n\n            case protocol_schema.LabelPrompterReplyTask:\n                payload = db_payload.LabelPrompterReplyPayload(\n                    type=task.type,\n                    message_id=task.message_id,\n                    conversation=task.conversation,\n                    valid_labels=task.valid_labels,\n                    mandatory_labels=task.mandatory_labels,\n                    mode=task.mode,\n                )\n\n            case protocol_schema.LabelAssistantReplyTask:\n                payload = db_payload.LabelAssistantReplyPayload(\n                    type=task.type,\n                    message_id=task.message_id,\n                    conversation=task.conversation,\n                    valid_labels=task.valid_labels,\n                    mandatory_labels=task.mandatory_labels,\n                    mode=task.mode,\n                )\n\n            case _:\n                raise OasstError(f\"Invalid task type: {type(task)=}\", OasstErrorCode.INVALID_TASK_TYPE)\n\n        if not collective and settings.TASK_VALIDITY_MINUTES > 0:\n            expiry_date = utcnow() + timedelta(minutes=settings.TASK_VALIDITY_MINUTES)\n        else:\n            expiry_date = None\n\n        task_model = self.insert_task(\n            payload=payload,\n            id=task.id,\n            message_tree_id=message_tree_id,\n            parent_message_id=parent_message_id,\n            collective=collective,\n            expiry_date=expiry_date,\n        )\n        assert task_model.id == task.id\n        return task_model\n\n    @managed_tx_method(CommitMode.COMMIT)\n    def bind_frontend_message_id(self, task_id: UUID, frontend_message_id: str) -> None:\n        validate_frontend_message_id(frontend_message_id)\n\n        # find task\n        task: Task = self.db.query(Task).filter(Task.id == task_id, Task.api_client_id == self.api_client.id).first()\n        if task is None:\n            raise OasstError(f\"Task for {task_id=} not found\", OasstErrorCode.TASK_NOT_FOUND, HTTP_404_NOT_FOUND)\n\n        if task.ack and task.frontend_message_id == frontend_message_id:\n            return  # ACK is idempotent if called with the same frontend_message_id\n\n        if task.expired:\n            raise OasstError(\"Task already expired.\", OasstErrorCode.TASK_EXPIRED)\n\n        if task.done or task.ack is not None:\n            raise OasstError(\"Task already updated.\", OasstErrorCode.TASK_ALREADY_UPDATED)\n\n        task.frontend_message_id = frontend_message_id\n        task.ack = True\n        self.db.add(task)\n\n    @managed_tx_method(CommitMode.COMMIT)\n    def close_task(self, frontend_message_id: str, allow_personal_tasks: bool = False):\n        \"\"\"\n        Mark task as done. No further messages will be accepted for this task.\n        \"\"\"\n        validate_frontend_message_id(frontend_message_id)\n        task = self.task_repository.fetch_task_by_frontend_message_id(frontend_message_id)\n\n        if not task:\n            raise OasstError(\n                f\"Task for {frontend_message_id=} not found\", OasstErrorCode.TASK_NOT_FOUND, HTTP_404_NOT_FOUND\n            )\n        if task.expired:\n            raise OasstError(\"Task already expired\", OasstErrorCode.TASK_EXPIRED)\n        if not allow_personal_tasks and not task.collective:\n            raise OasstError(\"This is not a collective task\", OasstErrorCode.TASK_NOT_COLLECTIVE)\n        if task.done:\n            raise OasstError(\"Already closed\", OasstErrorCode.TASK_ALREADY_DONE)\n\n        task.done = True\n        self.db.add(task)\n\n    @managed_tx_method(CommitMode.COMMIT)\n    def insert_task(\n        self,\n        payload: db_payload.TaskPayload,\n        id: UUID = None,\n        message_tree_id: UUID = None,\n        parent_message_id: UUID = None,\n        collective: bool = False,\n        expiry_date: datetime = None,\n    ) -> Task:\n        c = PayloadContainer(payload=payload)\n        task = Task(\n            id=id,\n            user_id=self.user_id,\n            payload_type=type(payload).__name__,\n            payload=c,\n            api_client_id=self.api_client.id,\n            message_tree_id=message_tree_id,\n            parent_message_id=parent_message_id,\n            collective=collective,\n            expiry_date=expiry_date,\n        )\n        logger.debug(f\"inserting {task=}\")\n        self.db.add(task)\n        return task\n\n    def fetch_task_by_frontend_message_id(self, message_id: str) -> Task:\n        validate_frontend_message_id(message_id)\n        task = (\n            self.db.query(Task)\n            .filter(Task.api_client_id == self.api_client.id, Task.frontend_message_id == message_id)\n            .one_or_none()\n        )\n        return task\n\n    def fetch_task_by_id(self, task_id: UUID) -> Task:\n        task = self.db.query(Task).filter(Task.api_client_id == self.api_client.id, Task.id == task_id).one_or_none()\n        return task\n\n    def fetch_recent_reply_tasks(\n        self,\n        max_age: timedelta = timedelta(minutes=5),\n        done: bool = False,\n        skipped: bool = False,\n        limit: int = 100,\n    ) -> list[Task]:\n        qry = self.db.query(Task).filter(\n            Task.created_date > func.current_timestamp() - max_age,\n            or_(Task.payload_type == \"AssistantReplyPayload\", Task.payload_type == \"PrompterReplyPayload\"),\n        )\n        if done is not None:\n            qry = qry.filter(Task.done == done)\n        if skipped is not None:\n            qry = qry.filter(Task.skipped == skipped)\n        if limit:\n            qry = qry.limit(limit)\n        return qry.all()\n\n    def delete_expired(self) -> int:\n        return delete_expired_tasks(self.db)\n\n    def fetch_pending_tasks_of_user(\n        self,\n        user_id: UUID,\n        max_age: timedelta = timedelta(minutes=5),\n        limit: int = 100,\n    ) -> list[Task]:\n        qry = (\n            self.db.query(Task)\n            .filter(\n                Task.user_id == user_id,\n                Task.created_date > func.current_timestamp() - max_age,\n                not_(Task.done),\n                not_(Task.skipped),\n            )\n            .order_by(Task.created_date)\n        )\n        if limit:\n            qry = qry.limit(limit)\n        return qry.all()\n", "backend/oasst_backend/tree_manager.py": "import random\nfrom datetime import datetime, timedelta\nfrom enum import Enum\nfrom http import HTTPStatus\nfrom typing import Optional, Tuple\nfrom uuid import UUID\n\nimport numpy as np\nimport pydantic\nimport sqlalchemy as sa\nfrom loguru import logger\nfrom oasst_backend.api.v1.utils import prepare_conversation, prepare_conversation_message_list\nfrom oasst_backend.config import TreeManagerConfiguration, settings\nfrom oasst_backend.models import (\n    Message,\n    MessageEmoji,\n    MessageReaction,\n    MessageTreeState,\n    Task,\n    TextLabels,\n    User,\n    UserStats,\n    UserStatsTimeFrame,\n    message_tree_state,\n)\nfrom oasst_backend.prompt_repository import PromptRepository\nfrom oasst_backend.scheduled_tasks import hf_feature_extraction, toxicity\nfrom oasst_backend.utils.database_utils import (\n    CommitMode,\n    async_managed_tx_method,\n    managed_tx_function,\n    managed_tx_method,\n)\nfrom oasst_backend.utils.ranking import ranked_pairs\nfrom oasst_shared.exceptions.oasst_api_error import OasstError, OasstErrorCode\nfrom oasst_shared.schemas import protocol as protocol_schema\nfrom oasst_shared.utils import utcnow\nfrom sqlalchemy.sql.functions import coalesce\nfrom sqlmodel import Session, and_, func, not_, or_, text, update\n\n\nclass TaskType(Enum):\n    NONE = -1\n    RANKING = 0\n    LABEL_REPLY = 1\n    REPLY = 2\n    LABEL_PROMPT = 3\n    PROMPT = 4\n\n\nclass TaskRole(Enum):\n    ANY = 0\n    PROMPTER = 1\n    ASSISTANT = 2\n\n\nclass TreeStateStats(pydantic.BaseModel):\n    initial_prompt_review: int\n    growing: int\n    ranking: int\n    ready_for_scoring: int\n    scoring_failed: int\n    ready_for_export: int\n    aborted_low_grade: int\n    halted_by_moderator: int\n    backlog_ranking: int\n    prompt_lottery_waiting: int\n\n\nclass ActiveTreeSizeRow(pydantic.BaseModel):\n    message_tree_id: UUID\n    goal_tree_size: int\n    tree_size: int\n    awaiting_review: Optional[int]\n\n    @property\n    def remaining_messages(self) -> int:\n        return max(0, self.goal_tree_size - self.tree_size)\n\n    class Config:\n        orm_mode = True\n\n\nclass ExtendibleParentRow(pydantic.BaseModel):\n    parent_id: UUID\n    parent_role: str\n    depth: int\n    message_tree_id: UUID\n    active_children_count: int\n\n    class Config:\n        orm_mode = True\n\n\nclass IncompleteRankingsRow(pydantic.BaseModel):\n    parent_id: UUID\n    role: str\n    children_count: int\n    child_min_ranking_count: int\n    message_tree_id: UUID\n\n    class Config:\n        orm_mode = True\n\n\nclass TreeMessageCountStats(pydantic.BaseModel):\n    message_tree_id: UUID\n    state: str\n    depth: int\n    oldest: datetime\n    youngest: datetime\n    count: int\n    goal_tree_size: int\n\n    @property\n    def completed(self) -> int:\n        return self.count / self.goal_tree_size\n\n\nclass TreeManagerStats(pydantic.BaseModel):\n    state_counts: dict[str, int]\n    message_counts: list[TreeMessageCountStats]\n\n\ndef halt_prompts_of_disabled_users(db: Session):\n    _sql_halt_prompts_of_disabled_users = \"\"\"\n-- remove prompts of disabled & deleted users from prompt lottery\nWITH cte AS (\nSELECT mts.message_tree_id\nFROM message_tree_state mts\nJOIN message m ON mts.message_tree_id = m.id\nJOIN \"user\" u ON m.user_id = u.id\nWHERE state = :prompt_lottery_waiting_state AND (NOT u.enabled OR u.deleted)\n)\nUPDATE message_tree_state mts2\nSET active=false, state=:halted_by_moderator_state\nFROM cte\nWHERE mts2.message_tree_id = cte.message_tree_id;\n\"\"\"\n\n    r = db.execute(\n        text(_sql_halt_prompts_of_disabled_users),\n        {\n            \"prompt_lottery_waiting_state\": message_tree_state.State.PROMPT_LOTTERY_WAITING,\n            \"halted_by_moderator_state\": message_tree_state.State.HALTED_BY_MODERATOR,\n        },\n    )\n    if r.rowcount > 0:\n        logger.info(f\"Halted {r.rowcount} prompts of disabled users.\")\n\n\nclass TreeManager:\n    def __init__(\n        self,\n        db: Session,\n        prompt_repository: PromptRepository,\n        cfg: Optional[TreeManagerConfiguration] = None,\n    ):\n        self.db = db\n        self.cfg = cfg or settings.tree_manager\n        self.pr = prompt_repository\n\n    def _random_task_selection(\n        self,\n        num_ranking_tasks: int,\n        num_replies_need_review: int,\n        num_prompts_need_review: int,\n        num_missing_prompts: int,\n        num_missing_replies: int,\n    ) -> TaskType:\n        \"\"\"\n        Determines which task to hand out to human worker.\n        The task type is drawn with relative weight (e.g. ranking has highest priority)\n        depending on what is possible with the current message trees in the database.\n        \"\"\"\n\n        logger.debug(\n            f\"TreeManager._random_task_selection({num_ranking_tasks=}, {num_replies_need_review=}, \"\n            f\"{num_prompts_need_review=}, {num_missing_prompts=}, {num_missing_replies=})\"\n        )\n\n        task_type = TaskType.NONE\n        task_weights = [0] * 5\n\n        if num_ranking_tasks > 0:\n            task_weights[TaskType.RANKING.value] = 10\n\n        if num_replies_need_review > 0:\n            task_weights[TaskType.LABEL_REPLY.value] = 5\n\n        if num_prompts_need_review > 0:\n            task_weights[TaskType.LABEL_PROMPT.value] = 5\n\n        if num_missing_replies > 0:\n            task_weights[TaskType.REPLY.value] = 2\n\n        if num_missing_prompts > 0:\n            task_weights[TaskType.PROMPT.value] = 0.01\n\n        task_weights = np.array(task_weights)\n        weight_sum = task_weights.sum()\n        if weight_sum > 1e-8:\n            task_weights = task_weights / weight_sum\n            task_type = TaskType(np.random.choice(a=len(task_weights), p=task_weights))\n\n        logger.debug(f\"Selected {task_type=}\")\n        return task_type\n\n    def _determine_task_availability_internal(\n        self,\n        num_missing_prompts: int,\n        extendible_parents: list[ExtendibleParentRow],\n        prompts_need_review: list[Message],\n        replies_need_review: list[Message],\n        incomplete_rankings: list[IncompleteRankingsRow],\n    ) -> dict[protocol_schema.TaskRequestType, int]:\n        task_count_by_type: dict[protocol_schema.TaskRequestType, int] = {t: 0 for t in protocol_schema.TaskRequestType}\n\n        task_count_by_type[protocol_schema.TaskRequestType.initial_prompt] = max(0, num_missing_prompts)\n\n        task_count_by_type[protocol_schema.TaskRequestType.prompter_reply] = len(\n            list(filter(lambda x: x.parent_role == \"assistant\", extendible_parents))\n        )\n        task_count_by_type[protocol_schema.TaskRequestType.assistant_reply] = len(\n            list(filter(lambda x: x.parent_role == \"prompter\", extendible_parents))\n        )\n\n        task_count_by_type[protocol_schema.TaskRequestType.label_initial_prompt] = len(prompts_need_review)\n        task_count_by_type[protocol_schema.TaskRequestType.label_assistant_reply] = len(\n            list(filter(lambda m: m.role == \"assistant\", replies_need_review))\n        )\n        task_count_by_type[protocol_schema.TaskRequestType.label_prompter_reply] = len(\n            list(filter(lambda m: m.role == \"prompter\", replies_need_review))\n        )\n\n        if self.cfg.rank_prompter_replies:\n            task_count_by_type[protocol_schema.TaskRequestType.rank_prompter_replies] = len(\n                list(filter(lambda r: r.role == \"prompter\", incomplete_rankings))\n            )\n\n        task_count_by_type[protocol_schema.TaskRequestType.rank_assistant_replies] = len(\n            list(filter(lambda r: r.role == \"assistant\", incomplete_rankings))\n        )\n\n        task_count_by_type[protocol_schema.TaskRequestType.random] = sum(\n            task_count_by_type[t] for t in protocol_schema.TaskRequestType if t in task_count_by_type\n        )\n\n        return task_count_by_type\n\n    def _prompt_lottery(self, lang: str, max_activate: int = 1) -> int:\n        # Under high load the DB runs into deadlocks when many trees are released\n        # simultaneously (happens whens the max_active_trees setting is increased).\n        # To reduce the chance of write conflicts during updates of rows in the\n        # message_tree_state table we limit the number of trees that are activated\n        # per _prompt_lottery() call to max_activate.\n        activated = 0\n\n        while True:\n            stats = self.tree_counts_by_state_stats(lang=lang, only_active=True)\n            prompt_lottery_waiting = self.query_prompt_lottery_waiting(lang=lang)\n            remaining_lottery_entries = max(0, self.cfg.max_prompt_lottery_waiting - prompt_lottery_waiting)\n            remaining_prompt_review = max(0, self.cfg.max_initial_prompt_review - stats.initial_prompt_review)\n            num_missing_growing = max(0, self.cfg.max_active_trees - stats.growing)\n            logger.info(f\"_prompt_lottery {remaining_prompt_review=}, {num_missing_growing=}\")\n\n            if num_missing_growing == 0 or activated >= max_activate:\n                return min(num_missing_growing + remaining_prompt_review, remaining_lottery_entries)\n\n            @managed_tx_function(CommitMode.COMMIT)\n            def activate_one(db: Session) -> int:\n                # select among distinct users\n                authors_qry = (\n                    db.query(Message.user_id, func.coalesce(UserStats.reply_ranked_1, 0).label(\"reply_ranked_1\"))\n                    .select_from(MessageTreeState)\n                    .join(Message, MessageTreeState.message_tree_id == Message.id)\n                    .join(User, Message.user_id == User.id)\n                    .outerjoin(\n                        UserStats, and_(UserStats.user_id == User.id, UserStats.time_frame == UserStatsTimeFrame.month)\n                    )\n                    .filter(\n                        MessageTreeState.state == message_tree_state.State.PROMPT_LOTTERY_WAITING,\n                        Message.lang == lang,\n                        not_(Message.deleted),\n                        Message.review_result,\n                        User.enabled,\n                        not_(User.deleted),\n                    )\n                    .distinct(Message.user_id)\n                )\n\n                author_data = authors_qry.all()\n                if len(author_data) == 0:\n                    logger.info(\n                        f\"No prompts for prompt lottery available ({num_missing_growing=}, trees missing for {lang=}).\"\n                    )\n                    return False\n\n                author_ids = [data[\"user_id\"] for data in author_data]\n                # add one to avoid any scenario where all weights are 0\n                # this also means inactive users can still occasionally be selected\n                weights = [data[\"reply_ranked_1\"] + 1 for data in author_data]\n\n                # first select an author\n                prompt_author_id: UUID = random.choices(author_ids, weights=weights)[0]\n                logger.info(f\"Selected random prompt author {prompt_author_id} among {len(author_data)} candidates.\")\n\n                # select random prompt of author\n                qry = (\n                    db.query(MessageTreeState, Message)\n                    .select_from(MessageTreeState)\n                    .join(Message, MessageTreeState.message_tree_id == Message.id)\n                    .filter(\n                        MessageTreeState.state == message_tree_state.State.PROMPT_LOTTERY_WAITING,\n                        Message.user_id == prompt_author_id,\n                        Message.lang == lang,\n                        not_(Message.deleted),\n                        Message.review_result,\n                    )\n                    .limit(100)\n                )\n\n                prompt_candidates = qry.all()\n                if len(prompt_candidates) == 0:\n                    logger.warning(\"No prompt candidates of selected author found.\")\n                    return False\n\n                winner_prompt = random.choice(prompt_candidates)\n                message: Message = winner_prompt.Message\n                logger.info(f\"Prompt lottery winner: {message.id=}\")\n\n                mts: MessageTreeState = winner_prompt.MessageTreeState\n                mts.state = message_tree_state.State.GROWING\n                mts.active = True\n                db.add(mts)\n\n                if mts.won_prompt_lottery_date is None:\n                    mts.won_prompt_lottery_date = utcnow()\n                logger.info(f\"Tree entered '{mts.state}' state ({mts.message_tree_id=})\")\n\n                return True\n\n            if not activate_one():\n                return min(num_missing_growing + remaining_prompt_review, remaining_lottery_entries)\n\n            activated += 1\n\n    def _auto_moderation(self, lang: str) -> None:\n        if not self.cfg.auto_mod_enabled:\n            return\n\n        bad_messages = self.query_moderation_bad_messages(lang=lang)\n        for m in bad_messages:\n            num_red_flag = m.emojis.get(protocol_schema.EmojiCode.red_flag)\n\n            if num_red_flag is not None and num_red_flag >= self.cfg.auto_mod_red_flags:\n                if m.parent_id is None:\n                    logger.warning(\n                        f\"[AUTO MOD] Halting tree {m.message_tree_id}, initial prompt got too many red flags ({m.emojis}).\"\n                    )\n                    self.enter_low_grade_state(m.message_tree_id)\n                else:\n                    logger.warning(f\"[AUTO MOD] Deleting message {m.id=}, it received too many red flags ({m.emojis}).\")\n                    self.pr.mark_messages_deleted(m.id, recursive=True)\n\n            num_skip_reply = m.emojis.get(protocol_schema.EmojiCode.skip_reply)\n            if num_skip_reply is not None and num_skip_reply >= self.cfg.auto_mod_max_skip_reply:\n                logger.warning(\n                    f\"[AUTO MOD] Halting tree {m.message_tree_id} due to high skip-reply count of message {m.id=} ({m.emojis}).\"\n                )\n                self.halt_tree(m.id, halt=True)\n\n    def determine_task_availability(self, lang: str) -> dict[protocol_schema.TaskRequestType, int]:\n        self.pr.ensure_user_is_enabled()\n\n        if not lang:\n            lang = \"en\"\n            logger.warning(\"Task availability request without lang tag received, assuming lang='en'.\")\n\n        if lang in self.cfg.init_prompt_disabled_langs_list:\n            num_missing_prompts = 0\n        else:\n            num_missing_prompts = self._prompt_lottery(lang=lang, max_activate=1)\n\n        self._auto_moderation(lang=lang)\n        extendible_parents, _ = self.query_extendible_parents(lang=lang)\n        prompts_need_review = self.query_prompts_need_review(lang=lang)\n        replies_need_review = self.query_replies_need_review(lang=lang)\n        incomplete_rankings = self.query_incomplete_rankings(lang=lang)\n\n        return self._determine_task_availability_internal(\n            num_missing_prompts=num_missing_prompts,\n            extendible_parents=extendible_parents,\n            prompts_need_review=prompts_need_review,\n            replies_need_review=replies_need_review,\n            incomplete_rankings=incomplete_rankings,\n        )\n\n    @staticmethod\n    def _get_label_descriptions(valid_labels: list[TextLabels]) -> list[protocol_schema.LabelDescription]:\n        return [\n            protocol_schema.LabelDescription(\n                name=l.value, widget=l.widget.value, display_text=l.display_text, help_text=l.help_text\n            )\n            for l in valid_labels\n        ]\n\n    def next_task(\n        self,\n        desired_task_type: protocol_schema.TaskRequestType = protocol_schema.TaskRequestType.random,\n        lang: str = \"en\",\n    ) -> Tuple[protocol_schema.Task, Optional[UUID], Optional[UUID]]:\n        logger.debug(f\"TreeManager.next_task({desired_task_type=}, {lang=})\")\n\n        self.pr.ensure_user_is_enabled()\n\n        if not lang:\n            lang = \"en\"\n            logger.warning(\"Task request without lang tag received, assuming 'en'.\")\n\n        self._auto_moderation(lang=lang)\n        num_missing_prompts = self._prompt_lottery(lang=lang, max_activate=2)\n\n        # check user's pending tasks\n        recent_tasks_span = timedelta(seconds=self.cfg.recent_tasks_span_sec)\n        users_pending_tasks = self.pr.task_repository.fetch_pending_tasks_of_user(\n            self.pr.user_id,\n            max_age=recent_tasks_span,\n            limit=self.cfg.max_pending_tasks_per_user + 1,\n        )\n        num_pending_tasks = len(users_pending_tasks)\n        if num_pending_tasks >= self.cfg.max_pending_tasks_per_user:\n            logger.warning(\n                f\"Rejecting task request. User {self.pr.user_id} has {num_pending_tasks} pending tasks. \"\n                f\"Oldest age: {utcnow()-users_pending_tasks[0].created_date}.\"\n            )\n            raise OasstError(\n                \"User has too many pending tasks.\",\n                OasstErrorCode.TASK_TOO_MANY_PENDING,\n            )\n        elif num_pending_tasks > 0:\n            logger.debug(\n                f\"User {self.pr.user_id} has {num_pending_tasks} pending tasks. Oldest age: {utcnow()-users_pending_tasks[0].created_date}\"\n            )\n\n        prompts_need_review = self.query_prompts_need_review(lang=lang)\n        replies_need_review = self.query_replies_need_review(lang=lang)\n        extendible_parents, active_tree_sizes = self.query_extendible_parents(lang=lang)\n\n        incomplete_rankings = self.query_incomplete_rankings(lang=lang)\n        if not self.cfg.rank_prompter_replies:\n            incomplete_rankings = list(filter(lambda r: r.role == \"assistant\", incomplete_rankings))\n\n        # determine type of task to generate\n        num_missing_replies = sum(x.remaining_messages for x in active_tree_sizes)\n\n        task_role = TaskRole.ANY\n        if desired_task_type == protocol_schema.TaskRequestType.random:\n            task_type = self._random_task_selection(\n                num_ranking_tasks=len(incomplete_rankings),\n                num_replies_need_review=len(replies_need_review),\n                num_prompts_need_review=len(prompts_need_review),\n                num_missing_prompts=num_missing_prompts,\n                num_missing_replies=num_missing_replies,\n            )\n\n            if task_type == TaskType.NONE:\n                logger.warning(f\"No random tasks currently available, user: {self.pr.user_id}\")\n                raise OasstError(\n                    f\"No tasks of type '{protocol_schema.TaskRequestType.random.value}' are currently available.\",\n                    OasstErrorCode.TASK_REQUESTED_TYPE_NOT_AVAILABLE,\n                    HTTPStatus.SERVICE_UNAVAILABLE,\n                )\n        else:\n            task_count_by_type = self._determine_task_availability_internal(\n                num_missing_prompts=num_missing_prompts,\n                extendible_parents=extendible_parents,\n                prompts_need_review=prompts_need_review,\n                replies_need_review=replies_need_review,\n                incomplete_rankings=incomplete_rankings,\n            )\n\n            available_count = task_count_by_type.get(desired_task_type)\n            if not available_count:\n                logger.warning(f\"No '{desired_task_type.value}' tasks currently available, user: {self.pr.user_id}\")\n                raise OasstError(\n                    f\"No tasks of type '{desired_task_type.value}' are currently available.\",\n                    OasstErrorCode.TASK_REQUESTED_TYPE_NOT_AVAILABLE,\n                    HTTPStatus.SERVICE_UNAVAILABLE,\n                )\n\n            task_type_role_map = {\n                protocol_schema.TaskRequestType.initial_prompt: (TaskType.PROMPT, TaskRole.ANY),\n                protocol_schema.TaskRequestType.prompter_reply: (TaskType.REPLY, TaskRole.PROMPTER),\n                protocol_schema.TaskRequestType.assistant_reply: (TaskType.REPLY, TaskRole.ASSISTANT),\n                protocol_schema.TaskRequestType.rank_prompter_replies: (TaskType.RANKING, TaskRole.PROMPTER),\n                protocol_schema.TaskRequestType.rank_assistant_replies: (TaskType.RANKING, TaskRole.ASSISTANT),\n                protocol_schema.TaskRequestType.label_initial_prompt: (TaskType.LABEL_PROMPT, TaskRole.ANY),\n                protocol_schema.TaskRequestType.label_assistant_reply: (TaskType.LABEL_REPLY, TaskRole.ASSISTANT),\n                protocol_schema.TaskRequestType.label_prompter_reply: (TaskType.LABEL_REPLY, TaskRole.PROMPTER),\n            }\n\n            task_type, task_role = task_type_role_map[desired_task_type]\n\n        message_tree_id = None\n        parent_message_id = None\n\n        logger.debug(f\"selected {task_type=}\")\n        match task_type:\n            case TaskType.RANKING:\n                if task_role == TaskRole.PROMPTER:\n                    incomplete_rankings = list(filter(lambda m: m.role == \"prompter\", incomplete_rankings))\n                elif task_role == TaskRole.ASSISTANT:\n                    incomplete_rankings = list(filter(lambda m: m.role == \"assistant\", incomplete_rankings))\n\n                if len(incomplete_rankings) > 0:\n                    ranking_parent_id = random.choice(incomplete_rankings).parent_id\n\n                    messages = self.pr.fetch_message_conversation(ranking_parent_id)\n                    assert len(messages) > 0 and messages[-1].id == ranking_parent_id\n                    ranking_parent = messages[-1]\n                    assert not ranking_parent.deleted and ranking_parent.review_result\n                    conversation = prepare_conversation(messages)\n                    replies = self.pr.fetch_message_children(ranking_parent_id, review_result=True, deleted=False)\n\n                    assert len(replies) > 1\n                    random.shuffle(replies)  # hand out replies in random order\n                    reply_messages = prepare_conversation_message_list(replies)\n                    if any(not m.synthetic for m in reply_messages):\n                        reveal_synthetic = False\n                        for rm in reply_messages:\n                            rm.synthetic = None\n                    else:\n                        reveal_synthetic = True\n\n                    replies = [p.text for p in replies]\n                    if messages[-1].role == \"assistant\":\n                        logger.info(\"Generating a RankPrompterRepliesTask.\")\n                        task = protocol_schema.RankPrompterRepliesTask(\n                            conversation=conversation,\n                            replies=replies,\n                            reply_messages=reply_messages,\n                            ranking_parent_id=ranking_parent.id,\n                            message_tree_id=ranking_parent.message_tree_id,\n                            reveal_synthetic=reveal_synthetic,\n                        )\n                    else:\n                        logger.info(\"Generating a RankAssistantRepliesTask.\")\n                        task = protocol_schema.RankAssistantRepliesTask(\n                            conversation=conversation,\n                            replies=replies,\n                            reply_messages=reply_messages,\n                            ranking_parent_id=ranking_parent.id,\n                            message_tree_id=ranking_parent.message_tree_id,\n                            reveal_synthetic=reveal_synthetic,\n                        )\n\n                    parent_message_id = ranking_parent_id\n                    message_tree_id = messages[-1].message_tree_id\n\n            case TaskType.LABEL_REPLY:\n                if task_role == TaskRole.PROMPTER:\n                    replies_need_review = list(filter(lambda m: m.role == \"prompter\", replies_need_review))\n                elif task_role == TaskRole.ASSISTANT:\n                    replies_need_review = list(filter(lambda m: m.role == \"assistant\", replies_need_review))\n\n                if len(replies_need_review) > 0:\n                    random_reply_message = random.choice(replies_need_review)\n                    messages = self.pr.fetch_message_conversation(random_reply_message)\n\n                    conversation = prepare_conversation(messages)\n                    message = messages[-1]\n\n                    self.cfg.p_full_labeling_review_reply_prompter: float = 0.1\n\n                    label_mode = protocol_schema.LabelTaskMode.full\n                    label_disposition = protocol_schema.LabelTaskDisposition.quality\n\n                    if message.role == \"assistant\":\n                        valid_labels = self.cfg.labels_assistant_reply\n                        if (\n                            desired_task_type == protocol_schema.TaskRequestType.random\n                            and random.random() > self.cfg.p_full_labeling_review_reply_assistant\n                        ):\n                            label_mode = protocol_schema.LabelTaskMode.simple\n                            label_disposition = protocol_schema.LabelTaskDisposition.spam\n                            valid_labels = self.cfg.mandatory_labels_assistant_reply.copy()\n                            if protocol_schema.TextLabel.lang_mismatch not in valid_labels:\n                                valid_labels.append(protocol_schema.TextLabel.lang_mismatch)\n                            if protocol_schema.TextLabel.quality not in valid_labels:\n                                valid_labels.append(protocol_schema.TextLabel.quality)\n\n                        logger.info(f\"Generating a LabelAssistantReplyTask. ({label_mode=:s})\")\n                        task = protocol_schema.LabelAssistantReplyTask(\n                            message_id=message.id,\n                            conversation=conversation,\n                            reply=message.text,\n                            valid_labels=list(map(lambda x: x.value, valid_labels)),\n                            mandatory_labels=list(map(lambda x: x.value, self.cfg.mandatory_labels_assistant_reply)),\n                            mode=label_mode,\n                            disposition=label_disposition,\n                            labels=self._get_label_descriptions(valid_labels),\n                        )\n                    else:\n                        valid_labels = self.cfg.labels_prompter_reply\n                        if (\n                            desired_task_type == protocol_schema.TaskRequestType.random\n                            and random.random() > self.cfg.p_full_labeling_review_reply_prompter\n                        ):\n                            label_mode = protocol_schema.LabelTaskMode.simple\n                            label_disposition = protocol_schema.LabelTaskDisposition.spam\n                            valid_labels = self.cfg.mandatory_labels_prompter_reply.copy()\n                            if protocol_schema.TextLabel.lang_mismatch not in valid_labels:\n                                valid_labels.append(protocol_schema.TextLabel.lang_mismatch)\n                            if protocol_schema.TextLabel.quality not in valid_labels:\n                                valid_labels.append(protocol_schema.TextLabel.quality)\n\n                        logger.info(f\"Generating a LabelPrompterReplyTask. ({label_mode=:s})\")\n                        task = protocol_schema.LabelPrompterReplyTask(\n                            message_id=message.id,\n                            conversation=conversation,\n                            reply=message.text,\n                            valid_labels=list(map(lambda x: x.value, valid_labels)),\n                            mandatory_labels=list(map(lambda x: x.value, self.cfg.mandatory_labels_prompter_reply)),\n                            mode=label_mode,\n                            disposition=label_disposition,\n                            labels=self._get_label_descriptions(valid_labels),\n                        )\n\n                    parent_message_id = message.id\n                    message_tree_id = message.message_tree_id\n\n            case TaskType.REPLY:\n                if task_role == TaskRole.PROMPTER:\n                    extendible_parents = list(filter(lambda x: x.parent_role == \"assistant\", extendible_parents))\n                elif task_role == TaskRole.ASSISTANT:\n                    extendible_parents = list(filter(lambda x: x.parent_role == \"prompter\", extendible_parents))\n\n                # select a tree with missing replies\n                if len(extendible_parents) > 0:\n                    random_parent: ExtendibleParentRow = None\n                    if self.cfg.p_lonely_child_extension > 0 and self.cfg.lonely_children_count > 1:\n                        # check if we have extendible prompter parents with a small number of replies\n                        lonely_children_parents = [\n                            p\n                            for p in extendible_parents\n                            if 0 < p.active_children_count < self.cfg.lonely_children_count\n                            and p.parent_role == \"prompter\"\n                        ]\n                        if len(lonely_children_parents) > 0 and random.random() < self.cfg.p_lonely_child_extension:\n                            random_parent = random.choice(lonely_children_parents)\n\n                    if random_parent is None:\n                        random_parent = random.choice(extendible_parents)\n\n                    # fetch random conversation to extend\n                    logger.debug(f\"selected {random_parent=}\")\n                    messages = self.pr.fetch_message_conversation(random_parent.parent_id)\n                    assert all(m.review_result for m in messages)  # ensure all messages have positive reviews\n                    conversation = prepare_conversation(messages)\n\n                    # generate reply task depending on last message\n                    if messages[-1].role == \"assistant\":\n                        logger.info(\"Generating a PrompterReplyTask.\")\n                        task = protocol_schema.PrompterReplyTask(conversation=conversation)\n                    else:\n                        logger.info(\"Generating a AssistantReplyTask.\")\n                        task = protocol_schema.AssistantReplyTask(conversation=conversation)\n\n                    parent_message_id = messages[-1].id\n                    message_tree_id = messages[-1].message_tree_id\n\n            case TaskType.LABEL_PROMPT:\n                assert len(prompts_need_review) > 0\n                message = random.choice(prompts_need_review)\n                message = self.pr.fetch_message(message.id)  # re-fetch message including emojis\n\n                label_mode = protocol_schema.LabelTaskMode.full\n                label_disposition = protocol_schema.LabelTaskDisposition.quality\n                valid_labels = self.cfg.labels_initial_prompt\n\n                if random.random() > self.cfg.p_full_labeling_review_prompt:\n                    valid_labels = self.cfg.mandatory_labels_initial_prompt.copy()\n                    label_mode = protocol_schema.LabelTaskMode.simple\n                    label_disposition = protocol_schema.LabelTaskDisposition.spam\n                    if protocol_schema.TextLabel.lang_mismatch not in valid_labels:\n                        valid_labels.append(protocol_schema.TextLabel.lang_mismatch)\n\n                logger.info(f\"Generating a LabelInitialPromptTask ({label_mode=:s}).\")\n                task = protocol_schema.LabelInitialPromptTask(\n                    message_id=message.id,\n                    prompt=message.text,\n                    conversation=prepare_conversation([message]),\n                    valid_labels=list(map(lambda x: x.value, valid_labels)),\n                    mandatory_labels=list(map(lambda x: x.value, self.cfg.mandatory_labels_initial_prompt)),\n                    mode=label_mode,\n                    disposition=label_disposition,\n                    labels=self._get_label_descriptions(valid_labels),\n                )\n\n                parent_message_id = message.id\n                message_tree_id = message.message_tree_id\n\n            case TaskType.PROMPT:\n                logger.info(\"Generating an InitialPromptTask.\")\n                task = protocol_schema.InitialPromptTask(hint=None)\n\n            case _:\n                task = None\n\n        if task is None:\n            raise OasstError(\n                f\"No task of type '{desired_task_type.value}' is currently available.\",\n                OasstErrorCode.TASK_REQUESTED_TYPE_NOT_AVAILABLE,\n                HTTPStatus.SERVICE_UNAVAILABLE,\n            )\n\n        logger.info(f\"Generated task (type={task.type}, id={task.id})\")\n        logger.debug(f\"Generated {task=}.\")\n\n        return task, message_tree_id, parent_message_id\n\n    @async_managed_tx_method(CommitMode.FLUSH)\n    async def handle_interaction(self, interaction: protocol_schema.AnyInteraction) -> protocol_schema.Task:\n        pr = self.pr\n        pr.ensure_user_is_enabled()\n        match type(interaction):\n            case protocol_schema.TextReplyToMessage:\n                logger.info(\n                    f\"Frontend reports text reply to message_id={interaction.message_id} by user={interaction.user}.\"\n                )\n                logger.debug(f\"with {interaction.text=}\")\n                # here we store the text reply in the database\n                message = pr.store_text_reply(\n                    text=interaction.text,\n                    lang=interaction.lang,\n                    frontend_message_id=interaction.message_id,\n                    user_frontend_message_id=interaction.user_message_id,\n                )\n\n                if not message.parent_id:\n                    logger.info(\n                        f\"TreeManager: Inserting new tree state for initial prompt {message.id=} [{message.lang}]\"\n                    )\n                    self._insert_default_state(message.id, lang=message.lang)\n\n                if not settings.DEBUG_SKIP_EMBEDDING_COMPUTATION:\n                    try:\n                        hf_feature_extraction.delay(interaction.text, message.id, pr.api_client.dict())\n                        logger.debug(\"Extract Embedding\")\n                    except OasstError:\n                        logger.error(\n                            f\"Could not fetch embbeddings for text reply to {interaction.message_id=} with {interaction.text=} by {interaction.user=}.\"\n                        )\n                if not settings.DEBUG_SKIP_TOXICITY_CALCULATION:\n                    try:\n                        toxicity.delay(interaction.text, message.id, pr.api_client.dict())\n                        logger.debug(\"Sent Toxicity\")\n                    except OasstError:\n                        logger.error(\n                            f\"Could not compute toxicity for text reply to {interaction.message_id=} with {interaction.text=} by {interaction.user=}.\"\n                        )\n\n            case protocol_schema.MessageRating:\n                logger.info(\n                    f\"Frontend reports rating of message_id={interaction.message_id} by user={interaction.user}.\"\n                )\n                logger.debug(f\"with {interaction.rating=}\")\n\n                pr.store_rating(interaction)\n\n            case protocol_schema.MessageRanking:\n                logger.info(\n                    f\"Frontend reports ranking of message_id={interaction.message_id} by user={interaction.user}.\"\n                )\n                logger.debug(f\"with {interaction.ranking=}\")\n\n                _, task = pr.store_ranking(interaction)\n                self.check_condition_for_scoring_state(task.message_tree_id)\n\n            case protocol_schema.TextLabels:\n                logger.info(\n                    f\"Frontend reports labels of message_id={interaction.message_id} by user={interaction.user}.\"\n                )\n                logger.debug(f\"with {interaction.labels=}\")\n\n                _, task, msg = pr.store_text_labels(interaction)\n\n                # if it was a response for a task, check if we have enough reviews to calc review_result\n                if task and msg:\n                    reviews = self.query_reviews_for_message(msg.id)\n                    acceptance_score = self._calculate_acceptance(reviews)\n                    logger.debug(\n                        f\"Message {msg.id=}, {acceptance_score=}, {len(reviews)=}, {msg.review_result=}, {msg.review_count=}\"\n                    )\n                    if msg.parent_id is None:\n                        if not msg.review_result and msg.review_count >= self.cfg.num_reviews_initial_prompt:\n                            if acceptance_score > self.cfg.acceptance_threshold_initial_prompt:\n                                msg.review_result = True\n                                self.db.add(msg)\n                                logger.info(\n                                    f\"Initial prompt message was accepted: {msg.id=}, {acceptance_score=}, {len(reviews)=}\"\n                                )\n                            else:\n                                if msg.review_result is None:\n                                    msg.review_result = False\n                                    self.db.add(msg)\n                                self.enter_low_grade_state(msg.message_tree_id)\n                        self.check_condition_for_prompt_lottery(msg.message_tree_id)\n                    elif msg.review_count >= self.cfg.num_reviews_reply:\n                        if not msg.review_result and acceptance_score > self.cfg.acceptance_threshold_reply:\n                            msg.review_result = True\n                            self.db.add(msg)\n                            logger.info(\n                                f\"Reply message message accepted: {msg.id=}, {acceptance_score=}, {len(reviews)=}\"\n                            )\n                        elif msg.review_result is None:  # do not overwrite existing review result\n                            msg.review_result = False\n                            self.db.add(msg)\n\n                    self.check_condition_for_ranking_state(msg.message_tree_id)\n\n            case _:\n                raise OasstError(\"Invalid response type.\", OasstErrorCode.TASK_INVALID_RESPONSE_TYPE)\n\n        return protocol_schema.TaskDone()\n\n    def _enter_state(self, mts: MessageTreeState, state: message_tree_state.State):\n        assert mts\n\n        is_terminal = state in message_tree_state.TERMINAL_STATES\n        was_active = mts.active\n        mts.active = not is_terminal\n        mts.state = state.value\n        self.db.add(mts)\n        self.db.flush\n\n        if is_terminal:\n            logger.info(f\"Tree entered terminal '{mts.state}' state ({mts.message_tree_id=})\")\n            root_msg = self.pr.fetch_message(message_id=mts.message_tree_id, fail_if_missing=False)\n            if root_msg and was_active:\n                if random.random() < self.cfg.p_activate_backlog_tree:\n                    self.activate_backlog_tree(lang=root_msg.lang)\n\n                if self.cfg.min_active_rankings_per_lang > 0:\n                    incomplete_rankings = self.query_incomplete_rankings(lang=root_msg.lang, user_filter=False)\n                    if len(incomplete_rankings) < self.cfg.min_active_rankings_per_lang:\n                        self.activate_backlog_tree(lang=root_msg.lang)\n        else:\n            if mts.state == message_tree_state.State.GROWING and mts.won_prompt_lottery_date is None:\n                mts.won_prompt_lottery_date = utcnow()\n            logger.info(f\"Tree entered '{mts.state}' state ({mts.message_tree_id=})\")\n\n    def enter_low_grade_state(self, message_tree_id: UUID) -> None:\n        logger.debug(f\"enter_low_grade_state({message_tree_id=})\")\n        mts = self.pr.fetch_tree_state(message_tree_id)\n        self._enter_state(mts, message_tree_state.State.ABORTED_LOW_GRADE)\n\n    def check_condition_for_prompt_lottery(self, message_tree_id: UUID) -> bool:\n        logger.debug(f\"check_condition_for_prompt_lottery({message_tree_id=})\")\n\n        mts = self.pr.fetch_tree_state(message_tree_id)\n        if not mts.active or mts.state != message_tree_state.State.INITIAL_PROMPT_REVIEW:\n            logger.debug(f\"False {mts.active=}, {mts.state=}\")\n            return False\n\n        # check if initial prompt was accepted\n        initial_prompt = self.pr.fetch_message(message_tree_id)\n        if not initial_prompt.review_result:\n            logger.debug(f\"False {initial_prompt.review_result=}\")\n            return False\n\n        self._enter_state(mts, message_tree_state.State.PROMPT_LOTTERY_WAITING)\n        return True\n\n    def check_condition_for_ranking_state(self, message_tree_id: UUID) -> bool:\n        logger.debug(f\"check_condition_for_ranking_state({message_tree_id=})\")\n\n        mts = self.pr.fetch_tree_state(message_tree_id)\n        if not mts.active or mts.state != message_tree_state.State.GROWING:\n            logger.debug(f\"False {mts.active=}, {mts.state=}\")\n            return False\n\n        # check if desired tree size has been reached and all nodes have been reviewed\n        tree_size = self.query_tree_size(message_tree_id)\n        if tree_size.tree_size == 0:\n            logger.warning(\n                f\"All messages of message tree {message_tree_id} were deleted (tree_size == 0), halting tree.\"\n            )\n            self._enter_state(mts, message_tree_state.State.HALTED_BY_MODERATOR)\n            return False\n\n        if tree_size.remaining_messages > 0 or tree_size.awaiting_review > 0:\n            logger.debug(f\"False {tree_size.remaining_messages=}, {tree_size.awaiting_review=}\")\n            return False\n\n        self._enter_state(mts, message_tree_state.State.RANKING)\n        return True\n\n    def check_condition_for_scoring_state(self, message_tree_id: UUID) -> bool:\n        logger.debug(f\"check_condition_for_scoring_state({message_tree_id=})\")\n\n        mts = self.pr.fetch_tree_state(message_tree_id)\n        if mts.state != message_tree_state.State.SCORING_FAILED:\n            if not mts.active or mts.state not in (\n                message_tree_state.State.RANKING,\n                message_tree_state.State.READY_FOR_SCORING,\n            ):\n                logger.debug(f\"False {mts.active=}, {mts.state=}\")\n                return False\n\n        ranking_role_filter = None if self.cfg.rank_prompter_replies else \"assistant\"\n        rankings_by_message = self.query_tree_ranking_results(message_tree_id, role_filter=ranking_role_filter)\n        for parent_msg_id, ranking in rankings_by_message.items():\n            if len(ranking) < self.cfg.num_required_rankings:\n                logger.debug(f\"False {parent_msg_id=} {len(ranking)=}\")\n                return False\n\n        if (\n            mts.state != message_tree_state.State.SCORING_FAILED\n            and mts.state != message_tree_state.State.READY_FOR_SCORING\n        ):\n            self._enter_state(mts, message_tree_state.State.READY_FOR_SCORING)\n        self.update_message_ranks(message_tree_id, rankings_by_message)\n        return True\n\n    def ranked_pairs_update(self, rankings: list[MessageReaction]) -> int:\n        assert len(rankings) > 0\n\n        num_updated = 0\n        ordered_ids_list: list[list[UUID]] = [\n            msg_reaction.payload.payload.ranked_message_ids for msg_reaction in rankings\n        ]\n\n        common_set: set[UUID] = set.intersection(*map(set, ordered_ids_list))\n        if len(common_set) < 2:\n            logger.warning(\"The intersection of ranking results ID sets has less than two elements. Skipping.\")\n            return\n\n        # keep only elements in common set\n        ordered_ids_list = [list(filter(lambda x: x in common_set, ids)) for ids in ordered_ids_list]\n        assert all(len(x) == len(common_set) for x in ordered_ids_list)\n\n        logger.debug(f\"SORTED MESSAGE IDS {ordered_ids_list}\")\n        consensus = ranked_pairs(ordered_ids_list)\n        assert len(consensus) == len(common_set)\n        logger.debug(f\"CONSENSUS: {consensus}\\n\\n\")\n\n        # fetch all siblings and index by id\n        siblings = self.pr.fetch_message_siblings(consensus[0], review_result=None, deleted=None)\n        siblings = {m.id: m for m in siblings}\n\n        # set rank for each message that was part of the common set\n        for rank, message_id in enumerate(consensus):\n            message = siblings.get(message_id)\n            if message:\n                if message.rank != rank:\n                    message.rank = rank\n                    self.db.add(message)\n                    num_updated += 1\n            else:\n                logger.warning(f\"Message {message_id=} not found among siblings.\")\n\n        # clear rank of sibling messages not in consensus\n        for message in siblings.values():\n            if message.id not in consensus and message.rank is not None:\n                message.rank = None\n                self.db.add(message)\n                num_updated += 1\n\n        return num_updated\n\n    def update_message_ranks(\n        self, message_tree_id: UUID, rankings_by_message: dict[UUID, list[MessageReaction]]\n    ) -> bool:\n        mts = self.pr.fetch_tree_state(message_tree_id)\n        # check state, allow retry if in SCORING_FAILED state\n        if mts.state not in (message_tree_state.State.READY_FOR_SCORING, message_tree_state.State.SCORING_FAILED):\n            logger.debug(f\"False {mts.active=}, {mts.state=}\")\n            return False\n\n        if mts.state == message_tree_state.State.SCORING_FAILED:\n            mts.active = True\n            mts.state = message_tree_state.State.READY_FOR_SCORING\n\n        try:\n            for rankings in rankings_by_message.values():\n                if len(rankings) > 0:\n                    self.ranked_pairs_update(rankings)\n\n        except Exception:\n            logger.exception(f\"update_message_ranks({message_tree_id=}) failed\")\n            self._enter_state(mts, message_tree_state.State.SCORING_FAILED)\n            return False\n\n        self._enter_state(mts, message_tree_state.State.READY_FOR_EXPORT)\n\n        return True\n\n    def activate_backlog_tree(self, lang: str) -> MessageTreeState:\n        while True:\n            # find tree in backlog state\n            backlog_tree: MessageTreeState = (\n                self.db.query(MessageTreeState)\n                .join(Message, MessageTreeState.message_tree_id == Message.id)  # root msg\n                .filter(MessageTreeState.state == message_tree_state.State.BACKLOG_RANKING)\n                .filter(Message.lang == lang)\n                .limit(1)\n                .one_or_none()\n            )\n\n            if not backlog_tree:\n                return None\n\n            if len(self.query_tree_ranking_results(message_tree_id=backlog_tree.message_tree_id)) == 0:\n                logger.info(\n                    f\"Backlog tree {backlog_tree.message_tree_id} has no children to rank, aborting with 'aborted_low_grade' state.\"\n                )\n                self._enter_state(backlog_tree, message_tree_state.State.ABORTED_LOW_GRADE)\n            else:\n                logger.info(f\"Activating backlog tree {backlog_tree.message_tree_id}\")\n                backlog_tree.active = True\n                self._enter_state(backlog_tree, message_tree_state.State.RANKING)\n                return backlog_tree\n\n    def _calculate_acceptance(self, labels: list[TextLabels]):\n        # calculate acceptance based on lang_mismatch & spam label\n        lang_mismatch = np.mean([(l.labels.get(protocol_schema.TextLabel.lang_mismatch) or 0) for l in labels])\n        spam = np.mean([l.labels[protocol_schema.TextLabel.spam] for l in labels])\n        acceptance_score = 1 - (spam + lang_mismatch)\n        logger.debug(f\"{acceptance_score=} ({spam=}, {lang_mismatch=})\")\n        return acceptance_score\n\n    def _query_need_review(\n        self, state: message_tree_state.State, required_reviews: int, root: bool, lang: str\n    ) -> list[Message]:\n        need_review = (\n            self.db.query(Message)\n            .select_from(MessageTreeState)\n            .join(Message, MessageTreeState.message_tree_id == Message.message_tree_id)\n            .outerjoin(\n                MessageEmoji,\n                and_(\n                    Message.id == MessageEmoji.message_id,\n                    MessageEmoji.user_id == self.pr.user_id,\n                    MessageEmoji.emoji == protocol_schema.EmojiCode.skip_labeling,\n                ),\n            )\n            .filter(\n                MessageTreeState.active,\n                MessageTreeState.state == state,\n                or_(Message.review_result.is_(None), not_(Message.review_result)),\n                not_(Message.deleted),\n                Message.review_count < required_reviews,\n                Message.lang == lang,\n                MessageEmoji.message_id.is_(None),\n            )\n        )\n\n        if root:\n            need_review = need_review.filter(Message.parent_id.is_(None))\n        else:\n            need_review = need_review.filter(Message.parent_id.is_not(None))\n\n        if not settings.DEBUG_ALLOW_SELF_LABELING:\n            need_review = need_review.filter(Message.user_id != self.pr.user_id)\n\n        if settings.DEBUG_ALLOW_DUPLICATE_TASKS:\n            qry = need_review\n        else:\n            user_id = self.pr.user_id\n            need_review = need_review.cte(name=\"need_review\")\n            qry = (\n                self.db.query(Message)\n                .select_entity_from(need_review)\n                .outerjoin(TextLabels, need_review.c.id == TextLabels.message_id)\n                .group_by(need_review)\n                .having(\n                    func.count(TextLabels.id).filter(TextLabels.task_id.is_not(None), TextLabels.user_id == user_id)\n                    == 0\n                )\n            )\n\n        return qry.all()\n\n    def query_prompts_need_review(self, lang: str) -> list[Message]:\n        \"\"\"\n        Select initial prompt messages with less then required rankings in active message tree\n        (active == True in message_tree_state)\n        \"\"\"\n        return self._query_need_review(\n            message_tree_state.State.INITIAL_PROMPT_REVIEW, self.cfg.num_reviews_initial_prompt, True, lang\n        )\n\n    def query_replies_need_review(self, lang: str) -> list[Message]:\n        \"\"\"\n        Select child messages (parent_id IS NOT NULL) with less then required rankings\n        in active message tree (active == True in message_tree_state)\n        \"\"\"\n        return self._query_need_review(message_tree_state.State.GROWING, self.cfg.num_reviews_reply, False, lang)\n\n    _sql_find_incomplete_rankings = \"\"\"\n-- find incomplete rankings\nSELECT m.parent_id, m.role, COUNT(m.id) children_count, MIN(m.ranking_count) child_min_ranking_count,\n    COUNT(m.id) FILTER (WHERE m.ranking_count >= :num_required_rankings) as completed_rankings,\n    mts.message_tree_id\nFROM message_tree_state mts\n    INNER JOIN message m ON mts.message_tree_id = m.message_tree_id\n    INNER JOIN message p ON m.parent_id = p.id\n    LEFT JOIN message_emoji me on\n        (m.parent_id = me.message_id\n        AND :skip_user_id IS NOT NULL\n        AND me.user_id = :skip_user_id\n        AND me.emoji = :skip_ranking)\nWHERE mts.active                        -- only consider active trees\n    AND mts.state = :ranking_state      -- message tree must be in ranking state\n    AND m.review_result                 -- must be reviewed\n    AND p.lang = :lang                  -- parent lang matches\n    AND NOT m.deleted                   -- not deleted\n    AND m.parent_id IS NOT NULL         -- ignore initial prompts\n    AND me.message_id IS NULL           -- no skip ranking emoji for user\nGROUP BY m.parent_id, m.role, mts.message_tree_id\nHAVING COUNT(m.id) > 1                                      -- more than one child\n    AND MIN(m.ranking_count) < :num_required_rankings       -- not complete\n    AND COUNT(m.id) FILTER (WHERE m.user_id = :rank_user_id) = 0 -- no self-ranking\n\"\"\"\n\n    _sql_find_incomplete_rankings_ex = f\"\"\"\n-- incomplete rankings but exclude of current user\nWITH incomplete_rankings AS ({_sql_find_incomplete_rankings})\nSELECT ir.* FROM incomplete_rankings ir\n    LEFT JOIN message_reaction mr ON ir.parent_id = mr.message_id AND mr.payload_type = 'RankingReactionPayload'\nGROUP BY ir.parent_id, ir.role, ir.children_count, ir.child_min_ranking_count, ir.completed_rankings,\n    ir.message_tree_id\nHAVING COUNT(mr.message_id) FILTER (WHERE mr.user_id = :dupe_user_id) = 0\n\"\"\"\n\n    def query_incomplete_rankings(self, lang: str, user_filter: bool = True) -> list[IncompleteRankingsRow]:\n        \"\"\"Query parents which have children that need further rankings\"\"\"\n\n        dupe_user_id = None\n        skip_user_id = None\n        rank_user_id = None\n        if user_filter:\n            if not settings.DEBUG_ALLOW_DUPLICATE_TASKS:\n                dupe_user_id = self.pr.user_id\n            if not settings.DEBUG_ALLOW_SELF_RANKING:\n                rank_user_id = self.pr.user_id\n            skip_user_id = self.pr.user_id\n        r = self.db.execute(\n            text(self._sql_find_incomplete_rankings_ex),\n            {\n                \"num_required_rankings\": self.cfg.num_required_rankings,\n                \"lang\": lang,\n                \"dupe_user_id\": dupe_user_id,\n                \"skip_user_id\": skip_user_id,\n                \"rank_user_id\": rank_user_id,\n                \"ranking_state\": message_tree_state.State.RANKING,\n                \"skip_ranking\": protocol_schema.EmojiCode.skip_ranking,\n            },\n        )\n        return [IncompleteRankingsRow.from_orm(x) for x in r.all()]\n\n    _sql_find_extendible_parents = \"\"\"\n-- find all extendible parent nodes\nWITH recent_reply_tasks (parent_message_id) AS (\n    -- recent incomplete tasks to exclude\n    SELECT parent_message_id FROM task\n    WHERE not done\n        AND not skipped\n        AND created_date > (CURRENT_TIMESTAMP - :recent_tasks_interval)\n        AND (payload_type = 'AssistantReplyPayload' OR payload_type = 'PrompterReplyPayload')\n)\nSELECT m.id as parent_id, m.role as parent_role, m.depth, m.message_tree_id, COUNT(c.id) active_children_count\nFROM message_tree_state mts\n    INNER JOIN message m ON mts.message_tree_id = m.message_tree_id     -- all elements of message tree\n    LEFT JOIN message_emoji me ON\n        (m.id = me.message_id\n        AND :skip_user_id IS NOT NULL\n        AND me.user_id = :skip_user_id\n        AND me.emoji = :skip_reply)\n    LEFT JOIN recent_reply_tasks rrt ON m.id = rrt.parent_message_id    -- recent tasks\n    LEFT JOIN message c ON m.id = c.parent_id  -- child nodes\nWHERE mts.active                        -- only consider active trees\n    AND mts.state = :growing_state      -- message tree must be growing\n    AND NOT m.deleted                   -- ignore deleted messages as parents\n    AND m.depth < mts.max_depth         -- ignore leaf nodes as parents\n    AND m.review_result                 -- parent node must have positive review\n    AND m.lang = :lang                  -- parent matches lang\n    AND me.message_id IS NULL           -- no skip reply emoji for user\n    AND rrt.parent_message_id IS NULL   -- no recent reply task found\n    AND NOT coalesce(c.deleted, FALSE)  -- don't count deleted children\n    AND (c.review_result OR coalesce(c.review_count, 0) < :num_reviews_reply) -- don't count children with negative review but count elements under review\nGROUP BY m.id, m.role, m.depth, m.message_tree_id, mts.max_children_count\nHAVING COUNT(c.id) < mts.max_children_count -- below maximum number of children\n    AND (COUNT(c.id) < :num_prompter_replies OR m.role = 'prompter')   -- limit replies to assistant messages\n    AND COUNT(c.id) FILTER (WHERE c.user_id = :user_id) = 0  -- without reply by user\n\"\"\"\n\n    def query_extendible_parents(self, lang: str) -> tuple[list[ExtendibleParentRow], list[ActiveTreeSizeRow]]:\n        \"\"\"Query parent messages that have not reached the maximum number of replies.\"\"\"\n\n        user_id = self.pr.user_id if not settings.DEBUG_ALLOW_DUPLICATE_TASKS else None\n        r = self.db.execute(\n            text(self._sql_find_extendible_parents),\n            {\n                \"growing_state\": message_tree_state.State.GROWING,\n                \"num_reviews_reply\": self.cfg.num_reviews_reply,\n                \"num_prompter_replies\": self.cfg.num_prompter_replies,\n                \"lang\": lang,\n                \"user_id\": user_id,\n                \"skip_user_id\": self.pr.user_id,\n                \"skip_reply\": protocol_schema.EmojiCode.skip_reply,\n                \"recent_tasks_interval\": timedelta(seconds=self.cfg.recent_tasks_span_sec),\n            },\n        )\n\n        potential_parents = [ExtendibleParentRow.from_orm(x) for x in r.all()]\n        extendible_trees = self.query_extendible_trees(lang=lang)\n        extendible_tree_ids = set(t.message_tree_id for t in extendible_trees)\n        extendible_parents = list(p for p in potential_parents if p.message_tree_id in extendible_tree_ids)\n\n        return extendible_parents, extendible_trees\n\n    _sql_find_extendible_trees = f\"\"\"\n-- find extendible trees\nSELECT m.message_tree_id, mts.goal_tree_size, COUNT(m.id) AS tree_size\nFROM (\n        SELECT DISTINCT message_tree_id FROM ({_sql_find_extendible_parents}) extendible_parents\n    ) trees INNER JOIN message_tree_state mts ON trees.message_tree_id = mts.message_tree_id\n    INNER JOIN message m ON mts.message_tree_id = m.message_tree_id\nWHERE NOT m.deleted\n    AND (\n        m.parent_id IS NOT NULL AND (m.review_result OR m.review_count < :num_reviews_reply) -- children\n        OR m.parent_id IS NULL AND m.review_result -- prompts (root nodes) must have positive review\n    )\nGROUP BY m.message_tree_id, mts.goal_tree_size\nHAVING COUNT(m.id) < mts.goal_tree_size\n\"\"\"\n\n    def query_extendible_trees(self, lang: str) -> list[ActiveTreeSizeRow]:\n        \"\"\"Query size of active message trees in growing state.\"\"\"\n\n        user_id = self.pr.user_id if not settings.DEBUG_ALLOW_DUPLICATE_TASKS else None\n        r = self.db.execute(\n            text(self._sql_find_extendible_trees),\n            {\n                \"growing_state\": message_tree_state.State.GROWING,\n                \"num_reviews_reply\": self.cfg.num_reviews_reply,\n                \"num_prompter_replies\": self.cfg.num_prompter_replies,\n                \"lang\": lang,\n                \"user_id\": user_id,\n                \"skip_user_id\": self.pr.user_id,\n                \"skip_reply\": protocol_schema.EmojiCode.skip_reply,\n                \"recent_tasks_interval\": timedelta(seconds=self.cfg.recent_tasks_span_sec),\n            },\n        )\n        return [ActiveTreeSizeRow.from_orm(x) for x in r.all()]\n\n    def query_tree_size(self, message_tree_id: UUID) -> ActiveTreeSizeRow:\n        \"\"\"Returns the number of reviewed not deleted messages in the message tree.\"\"\"\n\n        required_reviews = settings.tree_manager.num_reviews_reply\n        qry = (\n            self.db.query(\n                MessageTreeState.message_tree_id.label(\"message_tree_id\"),\n                MessageTreeState.goal_tree_size.label(\"goal_tree_size\"),\n                func.count(Message.id).filter(Message.review_result).label(\"tree_size\"),\n                func.count(Message.id)\n                .filter(\n                    or_(Message.review_result.is_(None), not_(Message.review_result)),\n                    Message.review_count < required_reviews,\n                )\n                .label(\"awaiting_review\"),\n            )\n            .select_from(MessageTreeState)\n            .outerjoin(\n                Message, and_(MessageTreeState.message_tree_id == Message.message_tree_id, not_(Message.deleted))\n            )\n            .filter(\n                MessageTreeState.active,\n                MessageTreeState.message_tree_id == message_tree_id,\n            )\n            .group_by(MessageTreeState.message_tree_id, MessageTreeState.goal_tree_size)\n        )\n\n        return ActiveTreeSizeRow.from_orm(qry.one())\n\n    def query_misssing_tree_states(self) -> list[Tuple[UUID, str]]:\n        \"\"\"Find all initial prompt messages that have no associated message tree state\"\"\"\n        qry_missing_tree_states = (\n            self.db.query(Message.id, Message.lang)\n            .outerjoin(MessageTreeState, Message.message_tree_id == MessageTreeState.message_tree_id)\n            .filter(\n                Message.parent_id.is_(None),\n                Message.message_tree_id == Message.id,\n                MessageTreeState.message_tree_id.is_(None),\n            )\n        )\n\n        return [(m.id, m.lang) for m in qry_missing_tree_states.all()]\n\n    _sql_find_tree_ranking_results = \"\"\"\n-- get all ranking results of completed tasks for all parents with >= 2 children\nSELECT p.parent_id, mr.* FROM\n(\n    -- find parents with > 1 children\n    SELECT m.parent_id, m.message_tree_id, COUNT(m.id) children_count\n    FROM message_tree_state mts\n       INNER JOIN message m ON mts.message_tree_id = m.message_tree_id\n    WHERE m.review_result                  -- must be reviewed\n       AND NOT m.deleted                   -- not deleted\n       AND m.parent_id IS NOT NULL         -- ignore initial prompts\n       AND (:role IS NULL OR m.role = :role) -- children with matching role\n       AND mts.message_tree_id = :message_tree_id\n    GROUP BY m.parent_id, m.message_tree_id\n    HAVING COUNT(m.id) > 1\n) as p\nLEFT JOIN task t ON p.parent_id = t.parent_message_id AND t.done AND (t.payload_type = 'RankPrompterRepliesPayload' OR t.payload_type = 'RankAssistantRepliesPayload')\nLEFT JOIN message_reaction mr ON mr.task_id = t.id AND mr.payload_type = 'RankingReactionPayload'\n\"\"\"\n\n    def query_tree_ranking_results(\n        self,\n        message_tree_id: UUID,\n        role_filter: str = \"assistant\",\n    ) -> dict[UUID, list[MessageReaction]]:\n        \"\"\"Finds all completed ranking results for a message_tree\"\"\"\n\n        assert role_filter in (None, \"assistant\", \"prompter\")\n\n        r = self.db.execute(\n            text(self._sql_find_tree_ranking_results),\n            {\n                \"message_tree_id\": message_tree_id,\n                \"role\": role_filter,\n            },\n        )\n\n        rankings_by_message = {}\n        for x in r.all():\n            parent_id = x[\"parent_id\"]\n            if parent_id not in rankings_by_message:\n                rankings_by_message[parent_id] = []\n            if x[\"task_id\"]:\n                rankings_by_message[parent_id].append(MessageReaction.from_orm(x))\n        return rankings_by_message\n\n    @managed_tx_method(CommitMode.COMMIT)\n    def ensure_tree_states(self) -> None:\n        \"\"\"Add message tree state rows for all root nodes (initial prompt messages).\"\"\"\n\n        missing_tree_ids = self.query_misssing_tree_states()\n        for id, lang in missing_tree_ids:\n            tree_size = self.db.query(func.count(Message.id)).filter(Message.message_tree_id == id).scalar()\n            state = message_tree_state.State.INITIAL_PROMPT_REVIEW\n            if tree_size > 1:\n                state = message_tree_state.State.GROWING\n                logger.info(f\"Inserting missing message tree state for message: {id} ({tree_size=}, {state=:s})\")\n            self._insert_default_state(id, lang=lang, state=state)\n\n        halt_prompts_of_disabled_users(self.db)\n\n        # check tree state transitions (maybe variables haves changes): prompt review -> growing -> ranking -> scoring\n        prompt_review_trees: list[MessageTreeState] = (\n            self.db.query(MessageTreeState)\n            .filter(MessageTreeState.state == message_tree_state.State.INITIAL_PROMPT_REVIEW, MessageTreeState.active)\n            .all()\n        )\n        if len(prompt_review_trees) > 0:\n            logger.info(\n                f\"Checking state of {len(prompt_review_trees)} active message trees in 'initial_prompt_review' state.\"\n            )\n            for t in prompt_review_trees:\n                self.check_condition_for_prompt_lottery(t.message_tree_id)\n\n        growing_trees: list[MessageTreeState] = (\n            self.db.query(MessageTreeState)\n            .filter(MessageTreeState.state == message_tree_state.State.GROWING, MessageTreeState.active)\n            .all()\n        )\n        if len(growing_trees) > 0:\n            logger.info(f\"Checking state of {len(growing_trees)} active message trees in 'growing' state.\")\n            for t in growing_trees:\n                self.check_condition_for_ranking_state(t.message_tree_id)\n\n        ranking_trees: list[MessageTreeState] = (\n            self.db.query(MessageTreeState)\n            .filter(\n                or_(\n                    MessageTreeState.state == message_tree_state.State.RANKING,\n                    MessageTreeState.state == message_tree_state.State.READY_FOR_SCORING,\n                ),\n                MessageTreeState.active,\n            )\n            .all()\n        )\n        if len(ranking_trees) > 0:\n            logger.info(f\"Checking state of {len(ranking_trees)} active message trees in 'ranking' state.\")\n            for t in ranking_trees:\n                self.check_condition_for_scoring_state(t.message_tree_id)\n\n    def query_num_growing_trees(self, lang: str) -> int:\n        \"\"\"Count all active trees in growing state.\"\"\"\n        query = (\n            self.db.query(func.count(MessageTreeState.message_tree_id))\n            .join(Message, MessageTreeState.message_tree_id == Message.id)\n            .filter(\n                MessageTreeState.active,\n                MessageTreeState.state == message_tree_state.State.GROWING,\n                Message.lang == lang,\n            )\n        )\n        return query.scalar()\n\n    def query_prompt_lottery_waiting(self, lang: str) -> int:\n        query = self.db.query(func.count(MessageTreeState.message_tree_id)).filter(\n            MessageTreeState.state == message_tree_state.State.PROMPT_LOTTERY_WAITING, MessageTreeState.lang == lang\n        )\n        return query.scalar()\n\n    def query_num_active_trees(\n        self, lang: str, exclude_ranking: bool = True, exclude_prompt_review: bool = True\n    ) -> int:\n        \"\"\"Count all active trees (optionally exclude those in ranking and initial prompt review states).\"\"\"\n        query = (\n            self.db.query(func.count(MessageTreeState.message_tree_id))\n            .join(Message, MessageTreeState.message_tree_id == Message.id)\n            .filter(MessageTreeState.active, Message.lang == lang)\n        )\n        if exclude_ranking:\n            query = query.filter(MessageTreeState.state != message_tree_state.State.RANKING)\n        if exclude_prompt_review:\n            query = query.filter(MessageTreeState.state != message_tree_state.State.INITIAL_PROMPT_REVIEW)\n        return query.scalar()\n\n    def query_reviews_for_message(self, message_id: UUID) -> list[TextLabels]:\n        qry = (\n            self.db.query(TextLabels)\n            .select_from(Task)\n            .join(TextLabels, Task.id == TextLabels.id)\n            .filter(Task.done, TextLabels.message_id == message_id)\n        )\n        return qry.all()\n\n    def query_moderation_bad_messages(self, lang: str) -> list[Message]:\n        qry = (\n            self.db.query(Message)\n            .select_from(MessageTreeState)\n            .join(Message, MessageTreeState.message_tree_id == Message.message_tree_id)\n            .filter(\n                MessageTreeState.active,\n                or_(\n                    MessageTreeState.state == message_tree_state.State.INITIAL_PROMPT_REVIEW,\n                    MessageTreeState.state == message_tree_state.State.GROWING,\n                ),\n                or_(\n                    Message.parent_id.is_(None),\n                    Message.review_result,\n                    and_(Message.parent_id.is_not(None), Message.review_count < self.cfg.num_reviews_reply),\n                ),\n                not_(Message.deleted),\n                or_(\n                    coalesce(Message.emojis[protocol_schema.EmojiCode.red_flag].cast(sa.Integer), 0)\n                    >= self.cfg.auto_mod_red_flags,\n                    coalesce(Message.emojis[protocol_schema.EmojiCode.skip_reply].cast(sa.Integer), 0)\n                    >= self.cfg.auto_mod_max_skip_reply,\n                ),\n            )\n        )\n\n        if lang is not None:\n            qry = qry.filter(Message.lang == lang)\n\n        return qry.all()\n\n    @managed_tx_method(CommitMode.FLUSH)\n    def _insert_tree_state(\n        self,\n        root_message_id: UUID,\n        goal_tree_size: int,\n        max_depth: int,\n        max_children_count: int,\n        active: bool,\n        lang: str,\n        state: message_tree_state.State = message_tree_state.State.INITIAL_PROMPT_REVIEW,\n    ) -> MessageTreeState:\n        model = MessageTreeState(\n            message_tree_id=root_message_id,\n            goal_tree_size=goal_tree_size,\n            max_depth=max_depth,\n            max_children_count=max_children_count,\n            state=state.value,\n            active=active,\n            lang=lang,\n        )\n\n        self.db.add(model)\n        return model\n\n    @managed_tx_method(CommitMode.FLUSH)\n    def _insert_default_state(\n        self,\n        root_message_id: UUID,\n        lang: str,\n        state: message_tree_state.State = message_tree_state.State.INITIAL_PROMPT_REVIEW,\n        *,\n        goal_tree_size: int = None,\n    ) -> MessageTreeState:\n        if goal_tree_size is None:\n            if self.cfg.random_goal_tree_size and self.cfg.min_goal_tree_size < self.cfg.goal_tree_size:\n                goal_tree_size = random.randint(self.cfg.min_goal_tree_size, self.cfg.goal_tree_size)\n            else:\n                goal_tree_size = self.cfg.goal_tree_size\n        return self._insert_tree_state(\n            root_message_id=root_message_id,\n            goal_tree_size=goal_tree_size,\n            max_depth=self.cfg.max_tree_depth,\n            max_children_count=self.cfg.max_children_count,\n            active=True,\n            lang=lang,\n            state=state,\n        )\n\n    def tree_counts_by_state(self, lang: str = None, only_active: bool = False) -> dict[str, int]:\n        qry = self.db.query(MessageTreeState.state, func.count(MessageTreeState.message_tree_id).label(\"count\"))\n\n        if lang is not None:\n            qry = (\n                qry.select_from(MessageTreeState)\n                .join(Message, MessageTreeState.message_tree_id == Message.id)\n                .filter(Message.lang == lang)\n            )\n        if only_active:\n            qry = qry.filter(MessageTreeState.active)\n\n        qry = qry.group_by(MessageTreeState.state)\n        return {x[\"state\"]: x[\"count\"] for x in qry}\n\n    def tree_counts_by_state_stats(self, lang: str = None, only_active: bool = False) -> TreeStateStats:\n        count_by_state = self.tree_counts_by_state(lang=lang, only_active=only_active)\n        r = TreeStateStats(\n            initial_prompt_review=count_by_state.get(message_tree_state.State.INITIAL_PROMPT_REVIEW) or 0,\n            growing=count_by_state.get(message_tree_state.State.GROWING) or 0,\n            ranking=count_by_state.get(message_tree_state.State.RANKING) or 0,\n            ready_for_scoring=count_by_state.get(message_tree_state.State.READY_FOR_SCORING) or 0,\n            ready_for_export=count_by_state.get(message_tree_state.State.READY_FOR_EXPORT) or 0,\n            scoring_failed=count_by_state.get(message_tree_state.State.SCORING_FAILED) or 0,\n            halted_by_moderator=count_by_state.get(message_tree_state.State.HALTED_BY_MODERATOR) or 0,\n            backlog_ranking=count_by_state.get(message_tree_state.State.BACKLOG_RANKING) or 0,\n            prompt_lottery_waiting=count_by_state.get(message_tree_state.State.PROMPT_LOTTERY_WAITING) or 0,\n            aborted_low_grade=count_by_state.get(message_tree_state.State.ABORTED_LOW_GRADE) or 0,\n        )\n        return r\n\n    def tree_message_count_stats(self, only_active: bool = True) -> list[TreeMessageCountStats]:\n        qry = (\n            self.db.query(\n                MessageTreeState.message_tree_id,\n                func.max(Message.depth).label(\"depth\"),\n                func.min(Message.created_date).label(\"oldest\"),\n                func.max(Message.created_date).label(\"youngest\"),\n                func.count(Message.id).label(\"count\"),\n                MessageTreeState.goal_tree_size,\n                MessageTreeState.state,\n            )\n            .select_from(MessageTreeState)\n            .join(Message, MessageTreeState.message_tree_id == Message.message_tree_id)\n            .filter(not_(Message.deleted))\n            .group_by(MessageTreeState.message_tree_id)\n        )\n\n        if only_active:\n            qry = qry.filter(MessageTreeState.active)\n\n        return [TreeMessageCountStats(**x) for x in qry]\n\n    def stats(self) -> TreeManagerStats:\n        return TreeManagerStats(\n            state_counts=self.tree_counts_by_state(),\n            message_counts=self.tree_message_count_stats(only_active=True),\n        )\n\n    def get_user_messages_by_tree(\n        self,\n        user_id: UUID,\n        min_date: datetime = None,\n        max_date: datetime = None,\n    ) -> Tuple[dict[UUID, list[Message]], list[Message]]:\n        \"\"\"Returns a dict with replies by tree (excluding initial prompts) and list of initial prompts\n        associated with user_id.\"\"\"\n\n        # query all messages of the user\n        qry = self.db.query(Message).filter(Message.user_id == user_id)\n        if min_date:\n            qry = qry.filter(Message.created_date >= min_date)\n        if max_date:\n            qry = qry.filter(Message.created_date <= max_date)\n\n        prompts: list[Message] = []\n        replies_by_tree: dict[UUID, list[Message]] = {}\n\n        # walk over result set and distinguish between initial prompts and replies\n        for m in qry:\n            m: Message\n\n            if m.message_tree_id == m.id:\n                prompts.append(m)\n            else:\n                message_list = replies_by_tree.get(m.message_tree_id)\n                if message_list is None:\n                    message_list = [m]\n                    replies_by_tree[m.message_tree_id] = message_list\n                else:\n                    message_list.append(m)\n\n        return replies_by_tree, prompts\n\n    def _purge_message_internal(self, message_id: UUID) -> None:\n        \"\"\"This internal function deletes a single message. It does not take care of\n        descendants, children_count in parent etc.\"\"\"\n\n        sql_purge_message = \"\"\"\nDELETE FROM journal j USING message m WHERE j.message_id = :message_id;\nDELETE FROM message_embedding e WHERE e.message_id = :message_id;\nDELETE FROM message_toxicity t WHERE t.message_id = :message_id;\nDELETE FROM text_labels l WHERE l.message_id = :message_id;\n-- delete all ranking results that contain message\nDELETE FROM message_reaction r WHERE r.payload_type = 'RankingReactionPayload' AND r.task_id IN (\n        SELECT t.id FROM message m\n            JOIN task t ON m.parent_id = t.parent_message_id\n        WHERE m.id = :message_id);\n-- delete task which inserted message\nDELETE FROM task t using message m WHERE t.id = m.task_id AND m.id = :message_id;\nDELETE FROM task t WHERE t.parent_message_id = :message_id;\nDELETE FROM message WHERE id = :message_id;\n\"\"\"\n        parent_id = self.pr.fetch_message(message_id=message_id).parent_id\n        r = self.db.execute(text(sql_purge_message), {\"message_id\": message_id})\n        logger.debug(f\"purge_message({message_id=}): {r.rowcount} rows.\")\n\n        sql_update_ranking_counts = \"\"\"\nWITH r AS (\n    -- find ranking results and count per child\n    SELECT c.id,\n        count(*) FILTER (\n            WHERE mr.payload#>'{payload, ranked_message_ids}' ? CAST(c.id AS varchar)\n        ) AS ranking_count\n    FROM message c\n    LEFT JOIN message_reaction mr ON mr.payload_type = 'RankingReactionPayload'\n        AND mr.message_id = c.parent_id\n    WHERE c.parent_id = :parent_id\n    GROUP BY c.id\n)\nUPDATE message m SET ranking_count = r.ranking_count\nFROM r WHERE m.id = r.id AND m.ranking_count != r.ranking_count;\n\"\"\"\n\n        if parent_id is not None:\n            # update ranking counts of remaining children\n            r = self.db.execute(text(sql_update_ranking_counts), {\"parent_id\": parent_id})\n            logger.debug(f\"ranking_count updated for {r.rowcount} rows.\")\n\n    def purge_message_tree(self, message_tree_id: UUID) -> None:\n        sql_purge_message_tree = \"\"\"\nDELETE FROM journal j USING message m WHERE j.message_id = m.Id AND m.message_tree_id = :message_tree_id;\nDELETE FROM message_embedding e USING message m WHERE e.message_id = m.Id AND m.message_tree_id = :message_tree_id;\nDELETE FROM message_toxicity t USING message m WHERE t.message_id = m.Id AND m.message_tree_id = :message_tree_id;\nDELETE FROM text_labels l USING message m WHERE l.message_id = m.Id AND m.message_tree_id = :message_tree_id;\nDELETE FROM message_reaction r USING task t WHERE r.task_id = t.id AND t.message_tree_id = :message_tree_id;\nDELETE FROM task t WHERE t.message_tree_id = :message_tree_id;\nDELETE FROM message_tree_state WHERE message_tree_id = :message_tree_id;\nDELETE FROM message WHERE message_tree_id = :message_tree_id;\n\"\"\"\n        r = self.db.execute(text(sql_purge_message_tree), {\"message_tree_id\": message_tree_id})\n        logger.debug(f\"purge_message_tree({message_tree_id=}) {r.rowcount} rows.\")\n\n    def _reactivate_tree(self, mts: MessageTreeState):\n        if mts.state == message_tree_state.State.PROMPT_LOTTERY_WAITING:\n            return\n\n        tree_id = mts.message_tree_id\n        if mts.won_prompt_lottery_date is not None:\n            self._enter_state(mts, message_tree_state.State.GROWING)\n            if self.check_condition_for_ranking_state(tree_id):\n                self.check_condition_for_scoring_state(tree_id)\n        else:\n            self._enter_state(mts, message_tree_state.State.INITIAL_PROMPT_REVIEW)\n            self.check_condition_for_prompt_lottery(tree_id)\n\n    @managed_tx_method(CommitMode.FLUSH)\n    def purge_user_messages(\n        self,\n        user_id: UUID,\n        purge_initial_prompts: bool = True,\n        min_date: datetime = None,\n        max_date: datetime = None,\n    ):\n        # find all affected message trees\n        replies_by_tree, prompts = self.get_user_messages_by_tree(user_id, min_date, max_date)\n        total_messages = sum(len(x) for x in replies_by_tree.values())\n        logger.debug(f\"found: {len(replies_by_tree)} trees; {len(prompts)} prompts; {total_messages} messages;\")\n\n        # remove all trees based on initial prompts of the user\n        if purge_initial_prompts:\n            for p in prompts:\n                self.purge_message_tree(p.message_tree_id)\n                if p.message_tree_id in replies_by_tree:\n                    del replies_by_tree[p.message_tree_id]\n\n        # patch all affected message trees\n        for tree_id, replies in replies_by_tree.items():\n            bad_parent_ids = set(m.id for m in replies)\n            logger.debug(f\"patching tree {tree_id=}, {bad_parent_ids=}\")\n\n            tree_messages = self.pr.fetch_message_tree(tree_id, reviewed=False, include_deleted=True)\n            logger.debug(f\"{tree_id=}, {len(bad_parent_ids)=}, {len(tree_messages)=}\")\n            by_id = {m.id: m for m in tree_messages}\n\n            def ancestor_ids(msg: Message) -> list[UUID]:\n                t = []\n                while msg.parent_id is not None:\n                    msg = by_id[msg.parent_id]\n                    t.append(msg.id)\n                return t\n\n            def is_descendant_of_deleted(m: Message) -> bool:\n                if m.id in bad_parent_ids:\n                    return True\n                ancestors = ancestor_ids(m)\n                if any(a in bad_parent_ids for a in ancestors):\n                    return True\n                return False\n\n            # start with deepest messages first\n            tree_messages.sort(key=lambda x: x.depth, reverse=True)\n            for m in tree_messages:\n                if is_descendant_of_deleted(m):\n                    logger.debug(f\"purging message: {m.id}\")\n                    self._purge_message_internal(m.id)\n\n            # update children counts\n            self.pr.update_children_counts(m.message_tree_id)\n\n            # reactivate tree\n            logger.info(f\"reactivating message tree {tree_id}\")\n            mts = self.pr.fetch_tree_state(tree_id)\n            mts.active = True\n            self._reactivate_tree(mts)\n\n    @managed_tx_method(CommitMode.FLUSH)\n    def purge_user(self, user_id: UUID, ban: bool = True) -> None:\n        self.purge_user_messages(user_id, purge_initial_prompts=True)\n\n        # delete all remaining rows and ban user\n        sql_purge_user = \"\"\"\nDELETE FROM journal WHERE user_id = :user_id;\nDELETE FROM message_reaction WHERE user_id = :user_id;\nDELETE FROM message_emoji WHERE user_id = :user_id;\nDELETE FROM task WHERE user_id = :user_id;\nDELETE FROM message WHERE user_id = :user_id;\nDELETE FROM user_stats WHERE user_id = :user_id;\n\"\"\"\n\n        r = self.db.execute(text(sql_purge_user), {\"user_id\": user_id})\n        logger.debug(f\"purge_user({user_id=}): {r.rowcount} rows.\")\n\n        if ban:\n            self.db.execute(update(User).filter(User.id == user_id).values(deleted=True, enabled=False))\n\n    @managed_tx_method(CommitMode.COMMIT)\n    def retry_scoring_failed_message_trees(self):\n        query = self.db.query(MessageTreeState).filter(\n            MessageTreeState.state == message_tree_state.State.SCORING_FAILED\n        )\n        for mts in query.all():\n            mts: MessageTreeState\n            try:\n                if not self.check_condition_for_scoring_state(mts.message_tree_id):\n                    mts.active = True\n                    self._enter_state(mts, message_tree_state.State.RANKING)\n            except Exception:\n                logger.exception(f\"retry_scoring_failed_message_trees failed for ({mts.message_tree_id=})\")\n\n    @managed_tx_method(CommitMode.FLUSH)\n    def halt_tree(self, message_id: UUID, halt: bool = True) -> MessageTreeState:\n        message = self.pr.fetch_message(message_id, fail_if_missing=True)\n        mts = self.pr.fetch_tree_state(message.message_tree_id)\n\n        if halt:\n            self._enter_state(mts, message_tree_state.State.HALTED_BY_MODERATOR)\n        else:\n            self._reactivate_tree(mts)\n\n        return mts\n\n\nif __name__ == \"__main__\":\n    from oasst_backend.api.deps import api_auth\n\n    # from oasst_backend.api.deps import create_api_client\n    from oasst_backend.database import engine\n    from oasst_backend.prompt_repository import PromptRepository\n\n    with Session(engine) as db:\n        api_client = api_auth(settings.OFFICIAL_WEB_API_KEY, db=db)\n        # api_client = create_api_client(session=db, description=\"test\", frontend_type=\"bot\")\n        # dummy_user = protocol_schema.User(id=\"__dummy_user__\", display_name=\"Dummy User\", auth_method=\"local\")\n        dummy_user = protocol_schema.User(id=\"1234\", display_name=\"bulb\", auth_method=\"local\")\n\n        pr = PromptRepository(db=db, api_client=api_client, client_user=dummy_user)\n        cfg = TreeManagerConfiguration()\n        tm = TreeManager(db, pr, cfg)\n        tm.ensure_tree_states()\n\n        # tm.purge_user_messages(user_id=UUID(\"2ef9ad21-0dc5-442d-8750-6f7f1790723f\"), purge_initial_prompts=False)\n        # tm.purge_user(user_id=UUID(\"2ef9ad21-0dc5-442d-8750-6f7f1790723f\"))\n        # db.commit()\n\n        # print(\"query_num_active_trees\", tm.query_num_active_trees())\n        # print(\"query_incomplete_rankings\", tm.query_incomplete_rankings())\n        # print(\"query_replies_need_review\", tm.query_replies_need_review())\n        # print(\"query_incomplete_reply_reviews\", tm.query_replies_need_review())\n        # xs = tm.query_prompts_need_review(lang=\"en\")\n        # print(\"xs\", len(xs))\n        # for x in xs:\n        #    print(x.id, x.emojis)\n        # print(\"query_incomplete_initial_prompt_reviews\", tm.query_prompts_need_review(lang=\"en\"))\n        # print(\"query_extendible_trees\", tm.query_extendible_trees())\n        # print(\"query_extendible_parents\", tm.query_extendible_parents())\n\n        # print(\"next_task:\", tm.next_task())\n\n        # print(\n        #     \".query_tree_ranking_results\", tm.query_tree_ranking_results(UUID(\"21f9d585-d22c-44ab-a696-baa3d83b5f1b\"))\n        # )\n", "backend/oasst_backend/models/db_payload.py": "from typing import Literal, Optional\nfrom uuid import UUID\n\nfrom oasst_backend.models.payload_column_type import payload_type\nfrom oasst_shared.schemas import protocol as protocol_schema\nfrom pydantic import BaseModel, Field\n\n\n@payload_type\nclass TaskPayload(BaseModel):\n    type: str\n\n\n@payload_type\nclass SummarizationStoryPayload(TaskPayload):\n    type: Literal[\"summarize_story\"] = \"summarize_story\"\n    story: str\n\n\n@payload_type\nclass RateSummaryPayload(TaskPayload):\n    type: Literal[\"rate_summary\"] = \"rate_summary\"\n    full_text: str\n    summary: str\n    scale: protocol_schema.RatingScale\n\n\n@payload_type\nclass InitialPromptPayload(TaskPayload):\n    type: Literal[\"initial_prompt\"] = \"initial_prompt\"\n    hint: str | None\n\n\n@payload_type\nclass PrompterReplyPayload(TaskPayload):\n    type: Literal[\"prompter_reply\"] = \"prompter_reply\"\n    conversation: protocol_schema.Conversation\n    hint: str | None\n\n\n@payload_type\nclass AssistantReplyPayload(TaskPayload):\n    type: Literal[\"assistant_reply\"] = \"assistant_reply\"\n    conversation: protocol_schema.Conversation\n\n\n@payload_type\nclass MessagePayload(BaseModel):\n    text: str\n\n\n@payload_type\nclass ReactionPayload(BaseModel):\n    type: str\n\n\n@payload_type\nclass RatingReactionPayload(ReactionPayload):\n    type: Literal[\"message_rating\"] = \"message_rating\"\n    rating: str\n\n\n@payload_type\nclass RankingReactionPayload(ReactionPayload):\n    type: Literal[\"message_ranking\"] = \"message_ranking\"\n    ranking: list[int]\n    ranked_message_ids: list[UUID]\n    ranking_parent_id: Optional[UUID]\n    message_tree_id: Optional[UUID]\n    not_rankable: Optional[bool]  # all options flawed, factually incorrect or unacceptable\n\n\n@payload_type\nclass RankConversationRepliesPayload(TaskPayload):\n    conversation: protocol_schema.Conversation  # the conversation so far\n    reply_messages: list[protocol_schema.ConversationMessage]\n    ranking_parent_id: Optional[UUID]\n    message_tree_id: Optional[UUID]\n    reveal_synthetic: Optional[bool]\n\n\n@payload_type\nclass RankInitialPromptsPayload(TaskPayload):\n    \"\"\"A task to rank a set of initial prompts.\"\"\"\n\n    type: Literal[\"rank_initial_prompts\"] = \"rank_initial_prompts\"\n    prompt_messages: list[protocol_schema.ConversationMessage]\n\n\n@payload_type\nclass RankPrompterRepliesPayload(RankConversationRepliesPayload):\n    \"\"\"A task to rank a set of prompter replies to a conversation.\"\"\"\n\n    type: Literal[\"rank_prompter_replies\"] = \"rank_prompter_replies\"\n\n\n@payload_type\nclass RankAssistantRepliesPayload(RankConversationRepliesPayload):\n    \"\"\"A task to rank a set of assistant replies to a conversation.\"\"\"\n\n    type: Literal[\"rank_assistant_replies\"] = \"rank_assistant_replies\"\n\n\n@payload_type\nclass LabelInitialPromptPayload(TaskPayload):\n    \"\"\"A task to label an initial prompt.\"\"\"\n\n    type: Literal[\"label_initial_prompt\"] = \"label_initial_prompt\"\n    message_id: UUID\n    prompt: str\n    valid_labels: list[str]\n    mandatory_labels: Optional[list[str]]\n    mode: Optional[protocol_schema.LabelTaskMode]\n\n\n@payload_type\nclass LabelConversationReplyPayload(TaskPayload):\n    \"\"\"A task to label a conversation reply.\"\"\"\n\n    message_id: UUID\n    conversation: protocol_schema.Conversation\n    reply: Optional[str] = Field(None, deprecated=True, description=\"deprecated\")\n    reply_message: Optional[protocol_schema.ConversationMessage] = Field(\n        None, deprecated=True, description=\"deprecated\"\n    )\n    valid_labels: list[str]\n    mandatory_labels: Optional[list[str]]\n    mode: Optional[protocol_schema.LabelTaskMode]\n\n\n@payload_type\nclass LabelPrompterReplyPayload(LabelConversationReplyPayload):\n    \"\"\"A task to label a prompter reply.\"\"\"\n\n    type: Literal[\"label_prompter_reply\"] = \"label_prompter_reply\"\n\n\n@payload_type\nclass LabelAssistantReplyPayload(LabelConversationReplyPayload):\n    \"\"\"A task to label an assistant reply.\"\"\"\n\n    type: Literal[\"label_assistant_reply\"] = \"label_assistant_reply\"\n", "backend/oasst_backend/models/message_reaction.py": "from datetime import datetime\nfrom typing import Optional\nfrom uuid import UUID\n\nimport sqlalchemy as sa\nimport sqlalchemy.dialects.postgresql as pg\nfrom sqlmodel import Field, SQLModel\n\nfrom .payload_column_type import PayloadContainer, payload_column_type\n\n\nclass MessageReaction(SQLModel, table=True):\n    __tablename__ = \"message_reaction\"\n\n    task_id: Optional[UUID] = Field(\n        sa_column=sa.Column(pg.UUID(as_uuid=True), sa.ForeignKey(\"task.id\"), nullable=False, primary_key=True)\n    )\n    user_id: UUID = Field(\n        sa_column=sa.Column(pg.UUID(as_uuid=True), sa.ForeignKey(\"user.id\"), nullable=False, primary_key=True)\n    )\n    created_date: Optional[datetime] = Field(\n        sa_column=sa.Column(\n            sa.DateTime(timezone=True), nullable=False, server_default=sa.func.current_timestamp(), index=True\n        )\n    )\n    payload_type: str = Field(nullable=False, max_length=200)\n    payload: PayloadContainer = Field(sa_column=sa.Column(payload_column_type(PayloadContainer), nullable=False))\n    api_client_id: UUID = Field(nullable=False, foreign_key=\"api_client.id\")\n    message_id: Optional[UUID] = Field(nullable=True, index=True)\n", "backend/oasst_backend/models/message_emoji.py": "from datetime import datetime\nfrom typing import Optional\nfrom uuid import UUID\n\nimport sqlalchemy as sa\nimport sqlalchemy.dialects.postgresql as pg\nfrom sqlmodel import Field, Index, SQLModel\n\n\nclass MessageEmoji(SQLModel, table=True):\n    __tablename__ = \"message_emoji\"\n    __table_args__ = (Index(\"ix_message_emoji__user_id__message_id\", \"user_id\", \"message_id\", unique=False),)\n\n    message_id: Optional[UUID] = Field(\n        sa_column=sa.Column(\n            pg.UUID(as_uuid=True), sa.ForeignKey(\"message.id\", ondelete=\"CASCADE\"), nullable=False, primary_key=True\n        )\n    )\n    user_id: UUID = Field(\n        sa_column=sa.Column(\n            pg.UUID(as_uuid=True), sa.ForeignKey(\"user.id\", ondelete=\"CASCADE\"), nullable=False, primary_key=True\n        )\n    )\n    emoji: str = Field(nullable=False, max_length=128, primary_key=True)\n    created_date: Optional[datetime] = Field(\n        sa_column=sa.Column(sa.DateTime(timezone=True), nullable=False, server_default=sa.func.current_timestamp())\n    )\n", "backend/oasst_backend/models/task.py": "from datetime import datetime\nfrom typing import Optional\nfrom uuid import UUID, uuid4\n\nimport sqlalchemy as sa\nimport sqlalchemy.dialects.postgresql as pg\nfrom oasst_shared.utils import utcnow\nfrom sqlalchemy import false\nfrom sqlmodel import Field, SQLModel\n\nfrom .payload_column_type import PayloadContainer, payload_column_type\n\n\nclass Task(SQLModel, table=True):\n    __tablename__ = \"task\"\n\n    id: Optional[UUID] = Field(\n        sa_column=sa.Column(\n            pg.UUID(as_uuid=True), primary_key=True, default=uuid4, server_default=sa.text(\"gen_random_uuid()\")\n        ),\n    )\n    created_date: Optional[datetime] = Field(\n        sa_column=sa.Column(\n            sa.DateTime(timezone=True), nullable=False, index=True, server_default=sa.func.current_timestamp()\n        ),\n    )\n    expiry_date: Optional[datetime] = Field(sa_column=sa.Column(sa.DateTime(timezone=True), nullable=True))\n    user_id: Optional[UUID] = Field(nullable=True, foreign_key=\"user.id\", index=True)\n    payload_type: str = Field(nullable=False, max_length=200)\n    payload: PayloadContainer = Field(sa_column=sa.Column(payload_column_type(PayloadContainer), nullable=False))\n    api_client_id: UUID = Field(nullable=False, foreign_key=\"api_client.id\")\n    ack: Optional[bool] = None\n    done: bool = Field(sa_column=sa.Column(sa.Boolean, nullable=False, server_default=false()))\n    skipped: bool = Field(sa_column=sa.Column(sa.Boolean, nullable=False, server_default=false()))\n    skip_reason: Optional[str] = Field(nullable=True, max_length=512)\n    frontend_message_id: Optional[str] = None\n    message_tree_id: Optional[UUID] = None\n    parent_message_id: Optional[UUID] = None\n    collective: bool = Field(sa_column=sa.Column(sa.Boolean, nullable=False, server_default=false()))\n\n    @property\n    def expired(self) -> bool:\n        return self.expiry_date is not None and utcnow() > self.expiry_date\n", "backend/oasst_backend/models/message.py": "from datetime import datetime\nfrom http import HTTPStatus\nfrom typing import Any, Optional\nfrom uuid import UUID, uuid4\n\nimport sqlalchemy as sa\nimport sqlalchemy.dialects.postgresql as pg\nfrom oasst_backend.models.db_payload import MessagePayload\nfrom oasst_backend.models.user import User\nfrom oasst_shared.exceptions.oasst_api_error import OasstError, OasstErrorCode\nfrom pydantic import PrivateAttr\nfrom sqlalchemy import false\nfrom sqlmodel import Field, Index, SQLModel\n\nfrom .payload_column_type import PayloadContainer, payload_column_type\n\n\nclass Message(SQLModel, table=True):\n    __tablename__ = \"message\"\n    __table_args__ = (\n        Index(\"ix_message_frontend_message_id\", \"api_client_id\", \"frontend_message_id\", unique=True),\n        Index(\"idx_search_vector\", \"search_vector\", postgresql_using=\"gin\"),\n    )\n\n    def __new__(cls, *args: Any, **kwargs: Any):\n        new_object = super().__new__(cls, *args, **kwargs)\n        # temporary fix until https://github.com/tiangolo/sqlmodel/issues/149 gets merged\n        if not hasattr(new_object, \"_user_emojis\"):\n            new_object._init_private_attributes()\n        return new_object\n\n    id: Optional[UUID] = Field(\n        sa_column=sa.Column(\n            pg.UUID(as_uuid=True), primary_key=True, default=uuid4, server_default=sa.text(\"gen_random_uuid()\")\n        ),\n    )\n    parent_id: Optional[UUID] = Field(nullable=True)\n    message_tree_id: UUID = Field(nullable=False, index=True)\n    task_id: Optional[UUID] = Field(nullable=True, index=True)\n    user_id: Optional[UUID] = Field(nullable=True, foreign_key=\"user.id\", index=True)\n    role: str = Field(nullable=False, max_length=128, regex=\"^prompter|assistant$\")\n    api_client_id: UUID = Field(nullable=False, foreign_key=\"api_client.id\")\n    frontend_message_id: str = Field(max_length=200, nullable=False)\n    created_date: Optional[datetime] = Field(\n        sa_column=sa.Column(\n            sa.DateTime(timezone=True), nullable=False, server_default=sa.func.current_timestamp(), index=True\n        )\n    )\n    payload_type: str = Field(nullable=False, max_length=200)\n    payload: Optional[PayloadContainer] = Field(\n        sa_column=sa.Column(payload_column_type(PayloadContainer), nullable=True)\n    )\n    lang: str = Field(sa_column=sa.Column(sa.String(32), server_default=\"en\", nullable=False))\n    depth: int = Field(sa_column=sa.Column(sa.Integer, default=0, server_default=sa.text(\"0\"), nullable=False))\n    children_count: int = Field(sa_column=sa.Column(sa.Integer, default=0, server_default=sa.text(\"0\"), nullable=False))\n    deleted: bool = Field(sa_column=sa.Column(sa.Boolean, nullable=False, server_default=false()))\n\n    search_vector: Optional[str] = Field(sa_column=sa.Column(pg.TSVECTOR(), nullable=True))\n\n    review_count: int = Field(sa_column=sa.Column(sa.Integer, default=0, server_default=sa.text(\"0\"), nullable=False))\n    review_result: bool = Field(sa_column=sa.Column(sa.Boolean, nullable=True))\n    ranking_count: int = Field(sa_column=sa.Column(sa.Integer, default=0, server_default=sa.text(\"0\"), nullable=False))\n\n    rank: Optional[int] = Field(nullable=True)\n\n    synthetic: Optional[bool] = Field(\n        sa_column=sa.Column(sa.Boolean, default=False, server_default=false(), nullable=False)\n    )\n    edited: bool = Field(sa_column=sa.Column(sa.Boolean, default=False, server_default=false(), nullable=False))\n    model_name: Optional[str] = Field(sa_column=sa.Column(sa.String(1024), nullable=True))\n\n    emojis: Optional[dict[str, int]] = Field(default=None, sa_column=sa.Column(pg.JSONB), nullable=False)\n    _user_emojis: Optional[list[str]] = PrivateAttr(default=None)\n    _user_is_author: Optional[bool] = PrivateAttr(default=None)\n    _user: Optional[bool] = PrivateAttr(default=None)\n\n    def ensure_is_message(self) -> None:\n        if not self.payload or not isinstance(self.payload.payload, MessagePayload):\n            raise OasstError(\"Invalid message\", OasstErrorCode.INVALID_MESSAGE, HTTPStatus.INTERNAL_SERVER_ERROR)\n\n    def has_emoji(self, emoji_code: str) -> bool:\n        return self.emojis and emoji_code in self.emojis and self.emojis[emoji_code] > 0\n\n    def has_user_emoji(self, emoji_code: str) -> bool:\n        return self._user_emojis and emoji_code in self._user_emojis\n\n    @property\n    def text(self) -> str:\n        self.ensure_is_message()\n        return self.payload.payload.text\n\n    @property\n    def user_emojis(self) -> str:\n        return self._user_emojis\n\n    @property\n    def user_is_author(self) -> str:\n        return self._user_is_author\n\n    @property\n    def user(self) -> User:\n        return self._user\n", "backend/oasst_backend/models/message_revision.py": "from datetime import datetime\nfrom typing import Optional\nfrom uuid import UUID\n\nimport sqlalchemy as sa\nimport sqlalchemy.dialects.postgresql as pg\nfrom pydantic import PrivateAttr\nfrom sqlmodel import Field, SQLModel\nfrom uuid_extensions import uuid7\n\nfrom .payload_column_type import PayloadContainer, payload_column_type\n\n\nclass MessageRevision(SQLModel, table=True):\n    __tablename__ = \"message_revision\"\n\n    id: UUID = Field(sa_column=sa.Column(pg.UUID(as_uuid=True), primary_key=True, default=uuid7))\n\n    payload: Optional[PayloadContainer] = Field(\n        sa_column=sa.Column(payload_column_type(PayloadContainer), nullable=True)\n    )\n    message_id: UUID = Field(sa_column=sa.Column(sa.ForeignKey(\"message.id\"), nullable=False, index=True))\n    user_id: Optional[UUID] = Field(sa_column=sa.Column(sa.ForeignKey(\"user.id\"), nullable=True))\n    created_date: Optional[datetime] = Field(\n        sa_column=sa.Column(sa.DateTime(timezone=True), nullable=True, server_default=sa.func.current_timestamp())\n    )\n\n    _user_is_author: Optional[bool] = PrivateAttr(default=None)\n", "backend/oasst_backend/models/message_embedding.py": "from datetime import datetime\nfrom typing import List, Optional\nfrom uuid import UUID\n\nimport sqlalchemy as sa\nimport sqlalchemy.dialects.postgresql as pg\nfrom sqlmodel import ARRAY, Field, Float, SQLModel\n\n\nclass MessageEmbedding(SQLModel, table=True):\n    __tablename__ = \"message_embedding\"\n    __table_args__ = (sa.PrimaryKeyConstraint(\"message_id\", \"model\"),)\n\n    message_id: UUID = Field(sa_column=sa.Column(pg.UUID(as_uuid=True), sa.ForeignKey(\"message.id\"), nullable=False))\n    model: str = Field(max_length=256, nullable=False)\n    embedding: List[float] = Field(sa_column=sa.Column(ARRAY(Float)), nullable=True)\n\n    # In the case that the Message Embedding is created afterwards\n    created_date: Optional[datetime] = Field(\n        sa_column=sa.Column(sa.DateTime(timezone=True), nullable=False, server_default=sa.func.current_timestamp())\n    )\n", "backend/oasst_backend/models/api_client.py": "from typing import Optional\nfrom uuid import UUID, uuid4\n\nimport sqlalchemy as sa\nimport sqlalchemy.dialects.postgresql as pg\nfrom sqlalchemy import false\nfrom sqlmodel import Field, SQLModel\n\n\nclass ApiClient(SQLModel, table=True):\n    __tablename__ = \"api_client\"\n\n    id: Optional[UUID] = Field(\n        sa_column=sa.Column(\n            pg.UUID(as_uuid=True), primary_key=True, default=uuid4, server_default=sa.text(\"gen_random_uuid()\")\n        ),\n    )\n    api_key: str = Field(max_length=512, index=True, unique=True)\n    description: str = Field(max_length=256)\n    admin_email: Optional[str] = Field(max_length=256, nullable=True)\n    enabled: bool = Field(default=True)\n    trusted: bool = Field(sa_column=sa.Column(sa.Boolean, nullable=False, server_default=false()))\n    frontend_type: str = Field(max_length=256, nullable=True)\n", "backend/oasst_backend/models/message_tree_state.py": "from datetime import datetime\nfrom enum import Enum\nfrom typing import Optional\nfrom uuid import UUID\n\nimport sqlalchemy as sa\nimport sqlalchemy.dialects.postgresql as pg\nfrom sqlmodel import Field, Index, SQLModel\n\n\nclass State(str, Enum):\n    \"\"\"States of the Open-Assistant message tree state machine.\"\"\"\n\n    INITIAL_PROMPT_REVIEW = \"initial_prompt_review\"\n    \"\"\"In this state the message tree consists only of a single initial prompt root node.\n    Initial prompt labeling tasks will determine if the tree goes into `growing` or\n    `aborted_low_grade` state.\"\"\"\n\n    GROWING = \"growing\"\n    \"\"\"Assistant & prompter human demonstrations are collected. Concurrently labeling tasks\n    are handed out to check if the quality of the replies surpasses the minimum acceptable\n    quality.\n    When the required number of messages passing the initial labelling-quality check has been\n    collected the tree will enter `ranking`. If too many poor-quality labelling responses\n    are received the tree can also enter the `aborted_low_grade` state.\"\"\"\n\n    RANKING = \"ranking\"\n    \"\"\"The tree has been successfully populated with the desired number of messages. Ranking\n    tasks are now handed out for all nodes with more than one child.\"\"\"\n\n    READY_FOR_SCORING = \"ready_for_scoring\"\n    \"\"\"Required ranking responses have been collected and the scoring algorithm can now\n    compute the aggregated ranking scores that will appear in the dataset.\"\"\"\n\n    READY_FOR_EXPORT = \"ready_for_export\"\n    \"\"\"The Scoring algorithm computed rankings scores for all children. The message tree can be\n    exported as part of an Open-Assistant message tree dataset.\"\"\"\n\n    SCORING_FAILED = \"scoring_failed\"\n    \"\"\"An exception occurred in the scoring algorithm.\"\"\"\n\n    ABORTED_LOW_GRADE = \"aborted_low_grade\"\n    \"\"\"The system received too many bad reviews and stopped handing out tasks for this message tree.\"\"\"\n\n    HALTED_BY_MODERATOR = \"halted_by_moderator\"\n    \"\"\"A moderator decided to manually halt the message tree construction process.\"\"\"\n\n    BACKLOG_RANKING = \"backlog_ranking\"\n    \"\"\"Imported tree ready to be activated and ranked by users (currently inactive).\"\"\"\n\n    PROMPT_LOTTERY_WAITING = \"prompt_lottery_waiting\"\n    \"\"\"Initial prompt has passed spam check, waiting to be drawn to grow.\"\"\"\n\n\nVALID_STATES = (\n    State.INITIAL_PROMPT_REVIEW,\n    State.GROWING,\n    State.RANKING,\n    State.READY_FOR_SCORING,\n    State.READY_FOR_EXPORT,\n    State.ABORTED_LOW_GRADE,\n    State.BACKLOG_RANKING,\n)\n\nTERMINAL_STATES = (\n    State.READY_FOR_EXPORT,\n    State.ABORTED_LOW_GRADE,\n    State.SCORING_FAILED,\n    State.HALTED_BY_MODERATOR,\n    State.BACKLOG_RANKING,\n    State.PROMPT_LOTTERY_WAITING,\n)\n\n\nclass MessageTreeState(SQLModel, table=True):\n    __tablename__ = \"message_tree_state\"\n    __table_args__ = (Index(\"ix_message_tree_state__lang__state\", \"state\", \"lang\", unique=False),)\n\n    message_tree_id: UUID = Field(\n        sa_column=sa.Column(pg.UUID(as_uuid=True), sa.ForeignKey(\"message.id\"), primary_key=True)\n    )\n    goal_tree_size: int = Field(nullable=False)\n    max_depth: int = Field(nullable=False)\n    max_children_count: int = Field(nullable=False)\n    state: str = Field(nullable=False, max_length=128)\n    active: bool = Field(nullable=False, index=True)\n    origin: str = Field(sa_column=sa.Column(sa.String(1024), nullable=True))\n    won_prompt_lottery_date: Optional[datetime] = Field(sa_column=sa.Column(sa.DateTime(timezone=True), nullable=True))\n    lang: str = Field(sa_column=sa.Column(sa.String(32), nullable=False))\n", "backend/oasst_backend/models/message_toxicity.py": "from datetime import datetime\nfrom typing import Optional\nfrom uuid import UUID\n\nimport sqlalchemy as sa\nimport sqlalchemy.dialects.postgresql as pg\nfrom sqlmodel import Field, Float, SQLModel\n\n\nclass MessageToxicity(SQLModel, table=True):\n    __tablename__ = \"message_toxicity\"\n    __table_args__ = (sa.PrimaryKeyConstraint(\"message_id\", \"model\"),)\n\n    message_id: UUID = Field(sa_column=sa.Column(pg.UUID(as_uuid=True), sa.ForeignKey(\"message.id\"), nullable=False))\n    model: str = Field(max_length=256, nullable=False)\n\n    # Storing the score and the label of the message\n    score: float = Field(sa_column=sa.Column(Float), nullable=False)\n    label: str = Field(max_length=256, nullable=False)\n\n    # In the case that the Message Embedding is created afterwards\n    created_date: Optional[datetime] = Field(\n        sa_column=sa.Column(sa.DateTime(timezone=True), nullable=False, server_default=sa.func.current_timestamp())\n    )\n", "backend/oasst_backend/models/user.py": "from datetime import datetime\nfrom typing import Optional\nfrom uuid import UUID, uuid4\n\nimport sqlalchemy as sa\nimport sqlalchemy.dialects.postgresql as pg\nfrom oasst_shared.schemas import protocol\nfrom sqlmodel import AutoString, Field, Index, SQLModel\n\n\nclass User(SQLModel, table=True):\n    __tablename__ = \"user\"\n    __table_args__ = (\n        Index(\"ix_user_username\", \"api_client_id\", \"username\", \"auth_method\", unique=True),\n        Index(\"ix_user_display_name_id\", \"display_name\", \"id\", unique=True),\n    )\n\n    id: Optional[UUID] = Field(\n        sa_column=sa.Column(\n            pg.UUID(as_uuid=True), primary_key=True, default=uuid4, server_default=sa.text(\"gen_random_uuid()\")\n        ),\n    )\n    username: str = Field(nullable=False, max_length=128)\n    auth_method: str = Field(nullable=False, max_length=128, default=\"local\")\n    display_name: str = Field(nullable=False, max_length=256)\n    created_date: Optional[datetime] = Field(\n        sa_column=sa.Column(sa.DateTime(timezone=True), nullable=False, server_default=sa.func.current_timestamp())\n    )\n    api_client_id: UUID = Field(foreign_key=\"api_client.id\")\n    enabled: bool = Field(sa_column=sa.Column(sa.Boolean, nullable=False, server_default=sa.true()))\n    notes: str = Field(sa_column=sa.Column(AutoString(length=1024), nullable=False, server_default=\"\"))\n    deleted: bool = Field(sa_column=sa.Column(sa.Boolean, nullable=False, server_default=sa.false()))\n    show_on_leaderboard: bool = Field(sa_column=sa.Column(sa.Boolean, nullable=False, server_default=sa.true()))\n\n    # only used for time span \"total\"\n    streak_last_day_date: Optional[datetime] = Field(\n        sa_column=sa.Column(sa.DateTime(timezone=True), nullable=True, server_default=sa.func.current_timestamp())\n    )\n    streak_days: Optional[int] = Field(nullable=True)\n    last_activity_date: Optional[datetime] = Field(\n        sa_column=sa.Column(sa.DateTime(timezone=True), nullable=True, server_default=sa.func.current_timestamp())\n    )\n\n    # terms of service acceptance date\n    tos_acceptance_date: Optional[datetime] = Field(sa_column=sa.Column(sa.DateTime(timezone=True), nullable=True))\n\n    def to_protocol_frontend_user(self):\n        return protocol.FrontEndUser(\n            user_id=self.id,\n            id=self.username,\n            display_name=self.display_name,\n            auth_method=self.auth_method,\n            enabled=self.enabled,\n            deleted=self.deleted,\n            notes=self.notes,\n            created_date=self.created_date,\n            show_on_leaderboard=self.show_on_leaderboard,\n            streak_days=self.streak_days,\n            streak_last_day_date=self.streak_last_day_date,\n            last_activity_date=self.last_activity_date,\n            tos_acceptance_date=self.tos_acceptance_date,\n        )\n\n\nclass Account(SQLModel, table=True):\n    __tablename__ = \"account\"\n    __table_args__ = (Index(\"provider\", \"provider_account_id\", unique=True),)\n\n    id: Optional[UUID] = Field(\n        sa_column=sa.Column(\n            pg.UUID(as_uuid=True), primary_key=True, default=uuid4, server_default=sa.text(\"gen_random_uuid()\")\n        ),\n    )\n    user_id: UUID = Field(foreign_key=\"user.id\")\n    provider: str = Field(nullable=False, max_length=128, default=\"email\")  # discord or email\n    provider_account_id: str = Field(nullable=False, max_length=128)\n", "backend/oasst_backend/models/flagged_message.py": "from datetime import datetime\nfrom typing import Optional\nfrom uuid import UUID\n\nimport sqlalchemy as sa\nimport sqlalchemy.dialects.postgresql as pg\nfrom sqlmodel import Field, SQLModel\n\n\nclass FlaggedMessage(SQLModel, table=True):\n    __tablename__ = \"flagged_message\"\n\n    message_id: Optional[UUID] = Field(\n        sa_column=sa.Column(\n            pg.UUID(as_uuid=True), sa.ForeignKey(\"message.id\", ondelete=\"CASCADE\"), nullable=False, primary_key=True\n        )\n    )\n    processed: bool = Field(nullable=False, index=True)\n    created_date: Optional[datetime] = Field(\n        sa_column=sa.Column(\n            sa.DateTime(timezone=True), nullable=False, server_default=sa.func.current_timestamp(), index=True\n        )\n    )\n", "backend/oasst_backend/models/payload_column_type.py": "import json\nfrom typing import Any, Generic, Type, TypeVar\n\nimport sqlalchemy.dialects.postgresql as pg\nfrom fastapi.encoders import jsonable_encoder\nfrom pydantic import BaseModel, parse_obj_as, validator\nfrom pydantic.main import ModelMetaclass\nfrom sqlalchemy.types import TypeDecorator\n\npayload_type_registry = {}\n\n\nP = TypeVar(\"P\", bound=BaseModel)\n\n\ndef payload_type(cls: Type[P]) -> Type[P]:\n    payload_type_registry[cls.__name__] = cls\n    return cls\n\n\nclass PayloadContainer(BaseModel):\n    payload_type: str = \"\"\n    payload: BaseModel = None\n\n    def __init__(self, **v):\n        p = v[\"payload\"]\n        if isinstance(p, dict):\n            t = v[\"payload_type\"]\n            if t not in payload_type_registry:\n                raise RuntimeError(f\"Payload type '{t}' not registered\")\n            cls = payload_type_registry[t]\n            v[\"payload\"] = cls(**p)\n        super().__init__(**v)\n\n    @validator(\"payload\", pre=True)\n    def check_payload(cls, v: BaseModel, values: dict[str, Any]) -> BaseModel:\n        values[\"payload_type\"] = type(v).__name__\n        return v\n\n    class Config:\n        orm_mode = True\n\n\nT = TypeVar(\"T\")\n\n\ndef payload_column_type(pydantic_type):\n    class PayloadJSONBType(TypeDecorator, Generic[T]):\n        impl = pg.JSONB()\n\n        cache_ok = True\n\n        def __init__(\n            self,\n            json_encoder=json,\n        ):\n            self.json_encoder = json_encoder\n            super().__init__()\n\n        # serialize\n        def bind_processor(self, dialect):\n            impl_processor = self.impl.bind_processor(dialect)\n            dumps = self.json_encoder.dumps\n\n            def process(value: T):\n                if value is not None:\n                    if isinstance(pydantic_type, ModelMetaclass):\n                        # This allows to assign non-InDB models and if they're\n                        # compatible, they're directly parsed into the InDB\n                        # representation, thus hiding the implementation in the\n                        # background. However, the InDB model will still be returned\n                        value_to_dump = pydantic_type.from_orm(value)\n                    else:\n                        value_to_dump = value\n\n                    value = jsonable_encoder(value_to_dump)\n\n                if impl_processor:\n                    return impl_processor(value)\n                else:\n                    return dumps(jsonable_encoder(value_to_dump))\n\n            return process\n\n        # deserialize\n        def result_processor(self, dialect, coltype) -> T:\n            impl_processor = self.impl.result_processor(dialect, coltype)\n\n            def process(value):\n                if impl_processor:\n                    value = impl_processor(value)\n                if value is None:\n                    return None\n                # Explicitly use the generic directly, not type(T)\n                full_obj = parse_obj_as(pydantic_type, value)\n                return full_obj\n\n            return process\n\n        def compare_values(self, x, y):\n            return x == y\n\n    return PayloadJSONBType\n", "backend/oasst_backend/models/journal.py": "from datetime import datetime\nfrom typing import Optional\nfrom uuid import UUID, uuid1, uuid4\n\nimport sqlalchemy as sa\nimport sqlalchemy.dialects.postgresql as pg\nfrom sqlmodel import Field, SQLModel\n\nfrom .payload_column_type import PayloadContainer, payload_column_type\n\n\ndef generate_time_uuid(node=None, clock_seq=None):\n    \"\"\"Create a lexicographically sortable time ordered custom (non-standard) UUID by reordering the timestamp fields of a version 1 UUID.\"\"\"\n    (time_low, time_mid, time_hi_version, clock_seq_hi_variant, clock_seq_low, node) = uuid1(node, clock_seq).fields\n    # reconstruct 60 bit timestamp, see version 1 uuid: https://www.rfc-editor.org/rfc/rfc4122\n    timestamp = (time_hi_version & 0xFFF) << 48 | (time_mid << 32) | time_low\n    version = time_hi_version >> 12\n    assert version == 1\n    a = timestamp >> 28  # bits 28-59\n    b = (timestamp >> 12) & 0xFFFF  # bits 12-27\n    c = timestamp & 0xFFF  # bits 0-11 (clear version bits)\n    clock_seq_hi_variant &= 0xF  # (clear variant bits)\n    return UUID(fields=(a, b, c, clock_seq_hi_variant, clock_seq_low, node), version=None)\n\n\nclass Journal(SQLModel, table=True):\n    __tablename__ = \"journal\"\n\n    id: Optional[UUID] = Field(\n        sa_column=sa.Column(pg.UUID(as_uuid=True), primary_key=True, default=generate_time_uuid),\n    )\n    created_date: Optional[datetime] = Field(\n        sa_column=sa.Column(sa.DateTime(timezone=True), nullable=False, server_default=sa.func.current_timestamp())\n    )\n    user_id: Optional[UUID] = Field(nullable=True, foreign_key=\"user.id\", index=True)\n    message_id: Optional[UUID] = Field(foreign_key=\"message.id\", nullable=True)\n    api_client_id: UUID = Field(foreign_key=\"api_client.id\")\n\n    event_type: str = Field(nullable=False, max_length=200)\n    event_payload: PayloadContainer = Field(sa_column=sa.Column(payload_column_type(PayloadContainer), nullable=False))\n\n\nclass JournalIntegration(SQLModel, table=True):\n    __tablename__ = \"journal_integration\"\n\n    id: Optional[UUID] = Field(\n        sa_column=sa.Column(\n            pg.UUID(as_uuid=True), primary_key=True, default=uuid4, server_default=sa.text(\"gen_random_uuid()\")\n        ),\n    )\n    description: str = Field(max_length=512, primary_key=True)\n    last_journal_id: Optional[UUID] = Field(foreign_key=\"journal.id\", nullable=True)\n    last_run: Optional[datetime] = Field(sa_column=sa.Column(sa.DateTime(timezone=True), nullable=True))\n    last_error: Optional[str] = Field(nullable=True)\n    next_run: Optional[datetime] = Field(nullable=True)\n", "backend/oasst_backend/models/troll_stats.py": "from datetime import datetime\nfrom typing import Optional\nfrom uuid import UUID\n\nimport sqlalchemy as sa\nimport sqlalchemy.dialects.postgresql as pg\nfrom sqlmodel import Field, Index, SQLModel\n\n\nclass TrollStats(SQLModel, table=True):\n    __tablename__ = \"troll_stats\"\n    __table_args__ = (Index(\"ix_troll_stats__timeframe__user_id\", \"time_frame\", \"user_id\", unique=True),)\n\n    time_frame: Optional[str] = Field(nullable=False, primary_key=True)\n    user_id: Optional[UUID] = Field(\n        sa_column=sa.Column(pg.UUID(as_uuid=True), sa.ForeignKey(\"user.id\", ondelete=\"CASCADE\"), primary_key=True)\n    )\n    base_date: Optional[datetime] = Field(sa_column=sa.Column(sa.DateTime(timezone=True), nullable=True))\n\n    troll_score: int = 0\n    modified_date: Optional[datetime] = Field(\n        sa_column=sa.Column(sa.DateTime(timezone=True), nullable=False, server_default=sa.func.current_timestamp())\n    )\n\n    rank: int = Field(nullable=True)\n\n    red_flags: int = 0  # num reported messages of user\n    upvotes: int = 0  # num up-voted messages of user\n    downvotes: int = 0  # num down-voted messages of user\n\n    spam_prompts: int = 0\n\n    quality: float = Field(nullable=True)\n    humor: float = Field(nullable=True)\n    toxicity: float = Field(nullable=True)\n    violence: float = Field(nullable=True)\n    helpfulness: float = Field(nullable=True)\n\n    spam: int = 0\n    lang_mismach: int = 0\n    not_appropriate: int = 0\n    pii: int = 0\n    hate_speech: int = 0\n    sexual_content: int = 0\n    political_content: int = 0\n\n    def compute_troll_score(self) -> int:\n        return (\n            self.red_flags * 3\n            - self.upvotes\n            + self.downvotes\n            + self.spam_prompts\n            + self.lang_mismach\n            + self.not_appropriate\n            + self.pii\n            + self.hate_speech\n            + self.sexual_content\n            + self.political_content\n        )\n", "backend/oasst_backend/models/text_labels.py": "from datetime import datetime\nfrom typing import Optional\nfrom uuid import UUID, uuid4\n\nimport sqlalchemy as sa\nimport sqlalchemy.dialects.postgresql as pg\nfrom sqlmodel import Field, SQLModel\n\n\nclass TextLabels(SQLModel, table=True):\n    __tablename__ = \"text_labels\"\n\n    id: Optional[UUID] = Field(\n        sa_column=sa.Column(\n            pg.UUID(as_uuid=True), primary_key=True, default=uuid4, server_default=sa.text(\"gen_random_uuid()\")\n        ),\n    )\n    user_id: UUID = Field(sa_column=sa.Column(pg.UUID(as_uuid=True), sa.ForeignKey(\"user.id\"), nullable=False))\n    created_date: Optional[datetime] = Field(\n        sa_column=sa.Column(\n            sa.DateTime(timezone=True), nullable=False, server_default=sa.func.current_timestamp(), index=True\n        ),\n    )\n    api_client_id: UUID = Field(nullable=False, foreign_key=\"api_client.id\")\n    text: str = Field(nullable=False, max_length=2**16)\n    message_id: Optional[UUID] = Field(\n        sa_column=sa.Column(pg.UUID(as_uuid=True), sa.ForeignKey(\"message.id\"), nullable=True, index=True)\n    )\n    labels: dict[str, float] = Field(default={}, sa_column=sa.Column(pg.JSONB), nullable=False)\n    task_id: Optional[UUID] = Field(nullable=True, index=True)\n", "backend/oasst_backend/models/__init__.py": "from .api_client import ApiClient\nfrom .cached_stats import CachedStats\nfrom .flagged_message import FlaggedMessage\nfrom .journal import Journal, JournalIntegration\nfrom .message import Message\nfrom .message_embedding import MessageEmbedding\nfrom .message_emoji import MessageEmoji\nfrom .message_reaction import MessageReaction\nfrom .message_revision import MessageRevision\nfrom .message_toxicity import MessageToxicity\nfrom .message_tree_state import MessageTreeState\nfrom .task import Task\nfrom .text_labels import TextLabels\nfrom .troll_stats import TrollStats\nfrom .user import User\nfrom .user_stats import UserStats, UserStatsTimeFrame\n\n__all__ = [\n    \"ApiClient\",\n    \"User\",\n    \"UserStats\",\n    \"UserStatsTimeFrame\",\n    \"Message\",\n    \"MessageEmbedding\",\n    \"MessageReaction\",\n    \"MessageRevision\",\n    \"MessageTreeState\",\n    \"MessageToxicity\",\n    \"Task\",\n    \"TextLabels\",\n    \"Journal\",\n    \"JournalIntegration\",\n    \"MessageEmoji\",\n    \"TrollStats\",\n    \"FlaggedMessage\",\n    \"CachedStats\",\n]\n", "backend/oasst_backend/models/user_stats.py": "from datetime import datetime\nfrom enum import Enum\nfrom typing import Optional\nfrom uuid import UUID\n\nimport sqlalchemy as sa\nimport sqlalchemy.dialects.postgresql as pg\nfrom sqlmodel import Field, Index, SQLModel\n\n\nclass UserStatsTimeFrame(str, Enum):\n    day = \"day\"\n    week = \"week\"\n    month = \"month\"\n    total = \"total\"\n\n\nclass UserStats(SQLModel, table=True):\n    __tablename__ = \"user_stats\"\n    __table_args__ = (\n        Index(\"ix_user_stats__timeframe__user_id\", \"time_frame\", \"user_id\", unique=True),\n        Index(\"ix_user_stats__timeframe__rank__user_id\", \"time_frame\", \"rank\", \"user_id\", unique=True),\n    )\n\n    time_frame: Optional[str] = Field(nullable=False, primary_key=True)\n    user_id: Optional[UUID] = Field(\n        sa_column=sa.Column(pg.UUID(as_uuid=True), sa.ForeignKey(\"user.id\"), primary_key=True)\n    )\n    base_date: Optional[datetime] = Field(sa_column=sa.Column(sa.DateTime(timezone=True), nullable=True))\n\n    leader_score: int = 0\n    modified_date: Optional[datetime] = Field(\n        sa_column=sa.Column(sa.DateTime(timezone=True), nullable=False, server_default=sa.func.current_timestamp())\n    )\n\n    rank: int = Field(nullable=True)\n\n    prompts: int = 0\n    replies_assistant: int = 0\n    replies_prompter: int = 0\n    labels_simple: int = 0\n    labels_full: int = 0\n    rankings_total: int = 0\n    rankings_good: int = 0\n\n    accepted_prompts: int = 0\n    accepted_replies_assistant: int = 0\n    accepted_replies_prompter: int = 0\n\n    reply_ranked_1: int = 0\n    reply_ranked_2: int = 0\n    reply_ranked_3: int = 0\n\n    def compute_leader_score(self) -> int:\n        return (\n            int(self.prompts * 0.1)\n            + self.replies_assistant * 4\n            + self.replies_prompter\n            + self.labels_simple\n            + self.labels_full * 2\n            + self.rankings_total\n            + self.rankings_good\n            + int(self.accepted_prompts * 0.1)\n            + self.accepted_replies_assistant * 4\n            + self.accepted_replies_prompter\n            + self.reply_ranked_1 * 9\n            + self.reply_ranked_2 * 3\n            + self.reply_ranked_3\n        )\n", "backend/oasst_backend/models/cached_stats.py": "from datetime import datetime\n\nimport sqlalchemy as sa\nimport sqlalchemy.dialects.postgresql as pg\nfrom sqlmodel import AutoString, Field, SQLModel\n\n\nclass CachedStats(SQLModel, table=True):\n    __tablename__ = \"cached_stats\"\n\n    name: str = Field(sa_column=sa.Column(AutoString(length=128), primary_key=True))\n\n    modified_date: datetime | None = Field(\n        sa_column=sa.Column(sa.DateTime(timezone=True), nullable=False, server_default=sa.func.current_timestamp())\n    )\n\n    stats: dict | list | None = Field(None, sa_column=sa.Column(pg.JSONB, nullable=False))\n", "backend/oasst_backend/utils/tree_export.py": "from __future__ import annotations\n\nimport contextlib\nimport gzip\nimport json\nimport sys\nimport uuid\nfrom collections import defaultdict\nfrom typing import Iterable, Optional, TextIO\n\nfrom fastapi.encoders import jsonable_encoder\nfrom oasst_backend.models import Message\nfrom oasst_backend.models.message_tree_state import State as TreeState\nfrom oasst_data import (\n    ExportMessageEvent,\n    ExportMessageEventEmoji,\n    ExportMessageEventRanking,\n    ExportMessageEventRating,\n    ExportMessageNode,\n    ExportMessageTree,\n    LabelValues,\n)\nfrom oasst_shared.utils import Anonymizer\n\n\ndef prepare_export_message_node(\n    message: Message,\n    labels: Optional[LabelValues] = None,\n    anonymizer: Anonymizer | None = None,\n    events: dict[str, list[ExportMessageEvent]] | None = None,\n) -> ExportMessageNode:\n    message_id = str(message.id)\n    parent_id = str(message.parent_id) if message.parent_id else None\n    user_id = str(message.user_id) if message.user_id else None\n    if anonymizer is not None:\n        message_id = anonymizer.anonymize(\"message\", message_id)\n        parent_id = anonymizer.anonymize(\"message\", parent_id)\n        user_id = anonymizer.anonymize(\"user\", user_id)\n        if events is not None:\n            for event_key, event_values in events.items():\n                for event in event_values:\n                    match event_key:\n                        case \"emoji\":\n                            event: ExportMessageEventEmoji = event\n                            if event.user_id is not None:\n                                event.user_id = anonymizer.anonymize(\"user\", event.user_id)\n                        case \"rating\":\n                            event: ExportMessageEventRating = event\n                            if event.user_id is not None:\n                                event.user_id = anonymizer.anonymize(\"user\", event.user_id)\n                        case \"ranking\":\n                            event: ExportMessageEventRanking = event\n                            if event.user_id is not None:\n                                event.user_id = anonymizer.anonymize(\"user\", event.user_id)\n                            event.ranked_message_ids = [\n                                anonymizer.anonymize(\"message\", m) for m in event.ranked_message_ids\n                            ]\n                            if event.ranking_parent_id is not None:\n                                event.ranking_parent_id = anonymizer.anonymize(\"message\", event.ranking_parent_id)\n                            if event.message_tree_id is not None:\n                                event.message_tree_id = anonymizer.anonymize(\"message_tree\", event.message_tree_id)\n                        case _:\n                            raise ValueError(f\"Unknown event type {event_key}\")\n    assert message_id is not None\n    return ExportMessageNode(\n        message_id=message_id,\n        parent_id=parent_id,\n        user_id=user_id,\n        created_date=message.created_date,\n        text=str(message.payload.payload.text),\n        role=message.role,\n        lang=message.lang,\n        deleted=message.deleted,\n        review_count=message.review_count,\n        review_result=message.review_result if message.review_result or message.review_count > 2 else None,\n        synthetic=message.synthetic,\n        model_name=message.model_name,\n        emojis=message.emojis,\n        rank=message.rank,\n        labels=labels,\n        events=events,\n    )\n\n\ndef build_export_tree(\n    message_tree_id: uuid.UUID,\n    message_tree_state: TreeState,\n    messages: list[Message],\n    labels: Optional[dict[uuid.UUID, LabelValues]] = None,\n    anonymizer: Anonymizer | None = None,\n    events: dict[uuid.UUID, dict[str, list[ExportMessageEvent]]] | None = None,\n) -> ExportMessageTree:\n    export_messages = [\n        prepare_export_message_node(\n            m, (labels.get(m.id) if labels else None), anonymizer=anonymizer, events=events.get(m.id)\n        )\n        for m in messages\n    ]\n\n    messages_by_parent = defaultdict(list)\n    for message in export_messages:\n        messages_by_parent[message.parent_id].append(message)\n\n    def assign_replies(node: ExportMessageNode) -> ExportMessageNode:\n        node.replies = messages_by_parent[node.message_id]\n        node.replies.sort(key=lambda x: x.rank if x.rank is not None else float(\"inf\"))\n        for child in node.replies:\n            assign_replies(child)\n        return node\n\n    prompt = assign_replies(messages_by_parent[None][0])\n    return ExportMessageTree(message_tree_id=str(message_tree_id), tree_state=message_tree_state, prompt=prompt)\n\n\n# see https://stackoverflow.com/questions/17602878/how-to-handle-both-with-open-and-sys-stdout-nicely\n@contextlib.contextmanager\ndef smart_open(filename: str = None) -> TextIO:\n    if filename and filename != \"-\":\n        fh = open(filename, \"wt\", encoding=\"UTF-8\")\n    else:\n        fh = sys.stdout\n\n    try:\n        yield fh\n    finally:\n        if fh is not sys.stdout:\n            fh.close()\n\n\ndef write_trees_to_file(filename: str | None, trees: list[ExportMessageTree], use_compression: bool = True) -> None:\n    out_buff: TextIO\n\n    if use_compression:\n        if not filename:\n            raise RuntimeError(\"File name must be specified when using compression.\")\n        out_buff = gzip.open(filename, \"wt\", encoding=\"UTF-8\")\n    else:\n        out_buff = smart_open(filename)\n\n    with out_buff as f:\n        for tree in trees:\n            file_data = jsonable_encoder(tree, exclude_none=True)\n            json.dump(file_data, f)\n            f.write(\"\\n\")\n\n\ndef write_messages_to_file(\n    filename: str | None,\n    messages: Iterable[Message],\n    use_compression: bool = True,\n    labels: Optional[dict[uuid.UUID, LabelValues]] = None,\n    anonymizer: Anonymizer | None = None,\n    events: dict[uuid.UUID, dict[str, list[ExportMessageEvent]]] | None = None,\n) -> None:\n    out_buff: TextIO\n\n    if use_compression:\n        if not filename:\n            raise RuntimeError(\"File name must be specified when using compression.\")\n        out_buff = gzip.open(filename, \"wt\", encoding=\"UTF-8\")\n    else:\n        out_buff = smart_open(filename)\n\n    with out_buff as f:\n        for m in messages:\n            export_message = prepare_export_message_node(\n                m, (labels.get(m.id) if labels else None), anonymizer=anonymizer, events=events.get(m.id)\n            )\n\n            file_data = jsonable_encoder(export_message, exclude_none=True)\n            json.dump(file_data, f)\n            f.write(\"\\n\")\n", "backend/oasst_backend/utils/similarity_functions.py": "import math\n\nimport numpy as np\nimport scipy.sparse as sp\nimport torch\nimport torch.nn.functional as F\nfrom pandas import DataFrame\nfrom sentence_transformers import SentenceTransformer\nfrom torch import Tensor\nfrom tqdm import tqdm\n\nADJACENCY_THRESHOLD = 0.65\n\n\ndef embed_data(\n    data: DataFrame,\n    key: str = \"query\",\n    model_name: str = \"all-MiniLM-L6-v2\",\n    cores: int = 1,\n    gpu: bool = False,\n    batch_size: int = 128,\n):\n    \"\"\"\n    Embed the sentences/text using the MiniLM language model (which uses mean pooling)\n    \"\"\"\n    print(\"Embedding data\")\n    model = SentenceTransformer(model_name)\n    print(\"Model loaded\")\n\n    sentences = data[key].tolist()\n    unique_sentences = data[key].unique()\n    print(\"Unique sentences\", len(unique_sentences))\n\n    if cores == 1:\n        embeddings = model.encode(unique_sentences, show_progress_bar=True, batch_size=batch_size)\n    else:\n        devices = [\"cpu\"] * cores\n        if gpu:\n            devices = None  # use all CUDA devices\n\n        # Start the multi-process pool on multiple devices\n        print(\"Multi-process pool starting\")\n        pool = model.start_multi_process_pool(devices)\n        print(\"Multi-process pool started\")\n\n        chunk_size = math.ceil(len(unique_sentences) / cores)\n\n        # Compute the embeddings using the multi-process pool\n        embeddings = model.encode_multi_process(unique_sentences, pool, batch_size=batch_size, chunk_size=chunk_size)\n        model.stop_multi_process_pool(pool)\n\n    print(\"Embeddings computed\")\n\n    mapping = {sentence: embedding for sentence, embedding in zip(unique_sentences, embeddings)}\n    embeddings = np.array([mapping[sentence] for sentence in sentences])\n\n    return embeddings\n\n\ndef cos_sim(a: Tensor, b: Tensor):\n    \"\"\"\n    Computes the cosine similarity cos_sim(a[i], b[j]) for all i and j.\n    :return: Matrix with res[i][j]  = cos_sim(a[i], b[j])\n    \"\"\"\n    if not isinstance(a, torch.Tensor):\n        a = torch.tensor(np.array(a))\n\n    if not isinstance(b, torch.Tensor):\n        b = torch.tensor(np.array(b))\n\n    if len(a.shape) == 1:\n        a = a.unsqueeze(0)\n\n    if len(b.shape) == 1:\n        b = b.unsqueeze(0)\n\n    a_norm = torch.nn.functional.normalize(a, p=2, dim=1)\n    b_norm = torch.nn.functional.normalize(b, p=2, dim=1)\n    return torch.mm(a_norm, b_norm.transpose(0, 1))\n\n\ndef cos_sim_torch(embs_a: Tensor, embs_b: Tensor) -> Tensor:\n    \"\"\"\n    Computes the cosine similarity cos_sim(a[i], b[j]) for all i and j.\n    Using torch.nn.functional.cosine_similarity\n    :return: Matrix with res[i][j]  = cos_sim(a[i], b[j])\n    \"\"\"\n    if not isinstance(embs_a, torch.Tensor):\n        embs_a = torch.tensor(np.array(embs_a))\n\n    if not isinstance(embs_b, torch.Tensor):\n        embs_b = torch.tensor(np.array(embs_b))\n\n    if len(embs_a.shape) == 1:\n        embs_a = embs_a.unsqueeze(0)\n\n    if len(embs_b.shape) == 1:\n        embs_b = embs_b.unsqueeze(0)\n    A = F.cosine_similarity(embs_a.unsqueeze(1), embs_b.unsqueeze(0), dim=2)\n    return A\n\n\ndef gaussian_kernel_torch(embs_a, embs_b, sigma=1.0):\n    \"\"\"\n    Computes the Gaussian kernel matrix between two sets of embeddings using PyTorch.\n    :param embs_a: Tensor of shape (batch_size_a, embedding_dim) containing the first set of embeddings.\n    :param embs_b: Tensor of shape (batch_size_b, embedding_dim) containing the second set of embeddings.\n    :param sigma: Width of the Gaussian kernel.\n    :return: Tensor of shape (batch_size_a, batch_size_b) containing the Gaussian kernel matrix.\n    \"\"\"\n    if not isinstance(embs_a, torch.Tensor):\n        embs_a = torch.tensor(embs_a)\n\n    if not isinstance(embs_b, torch.Tensor):\n        embs_b = torch.tensor(embs_b)\n\n    # Compute the pairwise distances between the embeddings\n    dist_matrix = torch.cdist(embs_a, embs_b)\n\n    # Compute the Gaussian kernel matrix\n    kernel_matrix = torch.exp(-(dist_matrix**2) / (2 * sigma**2))\n\n    return kernel_matrix\n\n\ndef compute_cos_sim_kernel(embs, threshold=0.65, kernel_type=\"cosine\"):\n    # match case to kernel type\n    if kernel_type == \"gaussian\":\n        A = gaussian_kernel_torch(embs, embs)\n    if kernel_type == \"cosine\":\n        A = cos_sim_torch(embs, embs)\n    adj_matrix = torch.zeros_like(A)\n    adj_matrix[A > threshold] = 1\n    adj_matrix[A <= threshold] = 0\n    adj_matrix = adj_matrix.numpy().astype(np.float32)\n    return adj_matrix\n\n\ndef k_hop_message_passing(A, node_features, k):\n    \"\"\"\n    Compute the k-hop adjacency matrix and aggregated features using message passing.\n\n    Parameters:\n    A (numpy array): The adjacency matrix of the graph.\n    node_features (numpy array): The feature matrix of the nodes.\n    k (int): The number of hops for message passing.\n\n    Returns:\n    A_k (numpy array): The k-hop adjacency matrix.\n    agg_features (numpy array): The aggregated feature matrix for each node in the k-hop neighborhood.\n    \"\"\"\n\n    print(\"Compute the k-hop adjacency matrix\")\n    A_k = np.linalg.matrix_power(A, k)\n\n    print(\"Aggregate the messages from the k-hop neighborhood:\")\n    agg_features = node_features.copy()\n\n    for i in tqdm(range(k)):\n        agg_features += np.matmul(np.linalg.matrix_power(A, i + 1), node_features)\n\n    return A_k, agg_features\n\n\ndef k_hop_message_passing_sparse(A, node_features, k):\n    \"\"\"\n    Compute the k-hop adjacency matrix and aggregated features using message passing.\n\n    Parameters:\n    A (numpy array or scipy sparse matrix): The adjacency matrix of the graph.\n    node_features (numpy array or scipy sparse matrix): The feature matrix of the nodes.\n    k (int): The number of hops for message passing.\n\n    Returns:\n    A_k (numpy array): The k-hop adjacency matrix.\n    agg_features (numpy array): The aggregated feature matrix for each node in the k-hop neighborhood.\n    \"\"\"\n\n    # Convert input matrices to sparse matrices if they are not already\n    if not sp.issparse(A):\n        A = sp.csr_matrix(A)\n    if not sp.issparse(node_features):\n        node_features = sp.csr_matrix(node_features)\n\n    # Compute the k-hop adjacency matrix and the aggregated features\n    A_k = A.copy()\n    agg_features = node_features.copy()\n\n    for i in tqdm(range(k)):\n        # Compute the message passing for the k-hop neighborhood\n        message = A_k.dot(node_features)\n        # Apply a GCN layer to aggregate the messages\n        agg_features = A_k.dot(agg_features) + message\n        # Update the k-hop adjacency matrix by adding new edges\n        A_k += A_k.dot(A)\n\n    return A_k.toarray(), agg_features.toarray()\n", "backend/oasst_backend/utils/message_tree_topic_modeling.py": "import argparse\n\nfrom bertopic import BERTopic\nfrom bertopic.representation import MaximalMarginalRelevance\nfrom bertopic.vectorizers import ClassTfidfTransformer\nfrom exported_tree_loading import load_data\nfrom sentence_transformers import SentenceTransformer\nfrom similarity_functions import compute_cos_sim_kernel, embed_data, k_hop_message_passing_sparse\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n\ndef argument_parsing():\n    parser = argparse.ArgumentParser(description=\"Process some arguments.\")\n    parser.add_argument(\"--model_name\", type=str, default=\"all-MiniLM-L6-v2\")\n    parser.add_argument(\"--cores\", type=int, default=1)\n    parser.add_argument(\"--pair_qa\", type=bool, default=True)\n    parser.add_argument(\"--use_gpu\", type=bool, default=False)\n    parser.add_argument(\"--batch_size\", type=int, default=128)\n    parser.add_argument(\"--k\", type=int, default=2)\n    parser.add_argument(\"--threshold\", type=float, default=0.65)\n    parser.add_argument(\"--exported_tree_path\", nargs=\"+\", help=\"<Required> Set flag\", required=True)\n    parser.add_argument(\"--min_topic_size\", type=int, default=10)\n    parser.add_argument(\"--diversity\", type=float, default=0.2)\n    parser.add_argument(\"--reduce_frequent_words\", type=bool, default=False)\n    parser.add_argument(\"--reduce_outliers_strategy\", type=str, default=\"c-tf-idf\")\n\n    args = parser.parse_args()\n    return args\n\n\ndef load_topic_model(args):\n    vectorizer_model = CountVectorizer(stop_words=\"english\")\n    ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=False)\n    model = SentenceTransformer(MODEL_NAME)\n    representation_model = MaximalMarginalRelevance(diversity=args.diversity)\n    topic_model = BERTopic(\n        nr_topics=\"auto\",\n        min_topic_size=args.min_topic_size,\n        representation_model=representation_model,\n        vectorizer_model=vectorizer_model,\n        ctfidf_model=ctfidf_model,\n        embedding_model=model,\n    )\n    return topic_model\n\n\ndef fit_topic_model(topic_model, data, embeddings, key=\"query\"):\n    topics, probs = topic_model.fit_transform(data[key].to_list(), embeddings)\n    return topics, probs\n\n\ndef get_topic_info(topic_model):\n    return topic_model.get_topic_info()\n\n\ndef reduce_topics(topic_model, data, nr_topics, key=\"query\"):\n    topic_model.reduce_topics(data[key].to_list(), nr_topics)\n    return topic_model\n\n\ndef get_representative_docs(topic_model):\n    return topic_model.get_representative_docs()\n\n\ndef reduce_outliers(topic_model, data, topics, probs, key=\"query\", strategy=\"c-tf-idf\"):\n    if strategy == \"c-tf-idf\":\n        new_topics = topic_model.reduce_outliers(data[key].to_list(), topics, strategy, threshold=0.1)\n    elif strategy == \"embeddings\":\n        new_topics = topic_model.reduce_outliers(data[key].to_list(), topics, strategy)\n    elif strategy == \"distributions\":\n        new_topics = topic_model.reduce_outliers(data[key].to_list(), topics, probabilities=probs, strategy=strategy)\n    else:\n        raise ValueError(\"Invalid strategy\")\n    return new_topics\n\n\ndef compute_hierarchical_topic_tree(topic_model, data, key=\"query\"):\n    hierarchical_topics = topic_model.hierarchical_topics(data[key].to_list())\n    tree = topic_model.get_topic_tree(hierarchical_topics)\n    return hierarchical_topics, tree\n\n\nif __name__ == \"__main__\":\n    \"\"\"\n    Main function to run topic modeling on a list of exported message trees.\n    Example usage:\n    python message_tree_topic_modeling.py --exported_tree_path 2023-02-06_oasst_prod.jsonl 2023-02-07_oasst_prod.jsonl\n    \"\"\"\n    args = argument_parsing()\n    MODEL_NAME = args.model_name\n    data, message_list = load_data(args.exported_tree_path, args.pair_qa)\n    embs = embed_data(data, model_name=MODEL_NAME, cores=args.cores, gpu=args.use_gpu)\n    adj_matrix = compute_cos_sim_kernel(embs, args.threshold)\n    print(adj_matrix.shape)\n    print(embs.shape)\n    A_k, agg_features = k_hop_message_passing_sparse(adj_matrix, embs, args.k)\n    print(A_k.shape)\n    topic_model = load_topic_model(args)\n    topics, probs = fit_topic_model(topic_model, data, agg_features)\n    freq = get_topic_info(topic_model)\n    rep_docs = get_representative_docs(topic_model)\n    print(freq)\n    for k, v in rep_docs.items():\n        print(k)\n        print(v)\n        print(\"\\n\\n\\n\")\n", "backend/oasst_backend/utils/ranking.py": "from typing import List\n\nimport numpy as np\n\n\ndef head_to_head_votes(ranks: List[List[int]]):\n    tallies = np.zeros((len(ranks[0]), len(ranks[0])))\n    names = sorted(ranks[0])\n    ranks = np.array(ranks)\n    # we want the sorted indices\n    ranks = np.argsort(ranks, axis=1)\n    for i in range(ranks.shape[1]):\n        for j in range(i + 1, ranks.shape[1]):\n            # now count the cases someone voted for i over j\n            over_j = np.sum(ranks[:, i] < ranks[:, j])\n            over_i = np.sum(ranks[:, j] < ranks[:, i])\n            tallies[i, j] = over_j\n            # tallies[i,j] = over_i\n            tallies[j, i] = over_i\n            # tallies[j,i] = over_j\n    return tallies, names\n\n\ndef cycle_detect(pairs):\n    \"\"\"Recursively detect cycles by removing condorcet losers until either only one pair is left or condorcet losers no longer exist\n    This method upholds the invariant that in a ranking for all a,b either a>b or b>a for all a,b.\n\n\n    Returns\n    -------\n    out : False if the pairs do not contain a cycle, True if the pairs contain a cycle\n\n\n    \"\"\"\n    # get all condorcet losers (pairs that loose to all other pairs)\n    # idea: filter all losers that are never winners\n    # print(\"pairs\", pairs)\n    if len(pairs) <= 1:\n        return False\n    losers = [c_lose for c_lose in np.unique(pairs[:, 1]) if c_lose not in pairs[:, 0]]\n    if len(losers) == 0:\n        # if we recursively removed pairs, and at some point we did not have\n        # a condorcet loser, that means everything is both a winner and loser,\n        # yielding at least one (winner,loser), (loser,winner) pair\n        return True\n\n    new = []\n    for p in pairs:\n        if p[1] not in losers:\n            new.append(p)\n    return cycle_detect(np.array(new))\n\n\ndef get_winner(pairs):\n    \"\"\"\n    This returns _one_ concordant winner.\n    It could be that there are multiple concordant winners, but in our case\n    since we are interested in a ranking, we have to choose one at random.\n    \"\"\"\n    losers = np.unique(pairs[:, 1]).astype(int)\n    winners = np.unique(pairs[:, 0]).astype(int)\n    for w in winners:\n        if w not in losers:\n            return w\n\n\ndef get_ranking(pairs):\n    \"\"\"\n    Abuses concordance property to get a (not necessarily unique) ranking.\n    The lack of uniqueness is due to the potential existence of multiple\n    equally ranked winners. We have to pick one, which is where\n    the non-uniqueness comes from\n    \"\"\"\n    if len(pairs) == 1:\n        return list(pairs[0])\n    w = get_winner(pairs)\n    # now remove the winner from the list of pairs\n    p_new = np.array([(a, b) for a, b in pairs if a != w])\n    return [w] + get_ranking(p_new)\n\n\ndef ranked_pairs(ranks: List[List[int]]):\n    \"\"\"\n    Expects a list of rankings for an item like:\n        [(\"w\",\"x\",\"z\",\"y\") for _ in range(3)]\n        + [(\"w\",\"y\",\"x\",\"z\") for _ in range(2)]\n        + [(\"x\",\"y\",\"z\",\"w\") for _ in range(4)]\n        + [(\"x\",\"z\",\"w\",\"y\") for _ in range(5)]\n        + [(\"y\",\"w\",\"x\",\"z\") for _ in range(1)]\n    This code is quite brain melting, but the idea is the following:\n    1. create a head-to-head matrix that tallies up all win-lose combinations of preferences\n    2. take all combinations that win more than they loose and sort those by how often they win\n    3. use that to create an (implicit) directed graph\n    4. recursively extract nodes from the graph that do not have incoming edges\n    5. said recursive list is the ranking\n    \"\"\"\n    tallies, names = head_to_head_votes(ranks)\n    tallies = tallies - tallies.T\n    # note: the resulting tally matrix should be skew-symmetric\n    # order by strength of victory (using tideman's original method, don't think it would make a difference for us)\n    sorted_majorities = []\n    for i in range(len(ranks[0])):\n        for j in range(len(ranks[0])):\n            # you can never prefer yourself over yourself\n            # we also have to pick one of the two choices,\n            # if the preference is exactly zero...\n            if tallies[i, j] >= 0 and i != j:\n                sorted_majorities.append((i, j, tallies[i, j]))\n    # we don't explicitly deal with tied majorities here\n    sorted_majorities = np.array(sorted(sorted_majorities, key=lambda x: x[2], reverse=True))\n    # now do lock ins\n    lock_ins = []\n    for x, y, _ in sorted_majorities:\n        # invariant: lock_ins has no cycles here\n        lock_ins.append((x, y))\n        # print(\"lock ins are now\",np.array(lock_ins))\n        if cycle_detect(np.array(lock_ins)):\n            # print(\"backup: cycle detected\")\n            # if there's a cycle, delete the new addition and continue\n            lock_ins = lock_ins[:-1]\n    # now simply return all winners in order, and attach the losers\n    # to the back. This is because the overall loser might not be unique\n    # and (by concordance property) may never exist in any winning set to begin with.\n    # (otherwise he would either not be the loser, or cycles exist!)\n    # Since there could be multiple overall losers, we just return them in any order\n    # as we are unable to find a closer ranking\n    numerical_ranks = np.array(get_ranking(np.array(lock_ins))).astype(int)\n    conversion = [names[n] for n in numerical_ranks]\n    return conversion\n\n\nif __name__ == \"__main__\":\n    ranks = \"\"\" (\n        [(\"w\", \"x\", \"z\", \"y\") for _ in range(1)]\n        + [(\"w\", \"y\", \"x\", \"z\") for _ in range(2)]\n        # + [(\"x\",\"y\",\"z\",\"w\") for _ in range(4)]\n        + [(\"x\", \"z\", \"w\", \"y\") for _ in range(5)]\n        + [(\"y\", \"w\", \"x\", \"z\") for _ in range(1)]\n        # [(\"y\",\"z\",\"w\",\"x\") for _ in range(1000)]\n    )\"\"\"\n    ranks = [\n        [\n            (\"c5181083-d3e9-41e7-a935-83fb9fa01488\"),\n            (\"dcf3d179-0f34-4c15-ae21-b8feb15e422d\"),\n            (\"d11705af-5575-43e5-b22e-08d155fbaa62\"),\n        ],\n        [\n            (\"d11705af-5575-43e5-b22e-08d155fbaa62\"),\n            (\"c5181083-d3e9-41e7-a935-83fb9fa01488\"),\n            (\"dcf3d179-0f34-4c15-ae21-b8feb15e422d\"),\n        ],\n        [\n            (\"dcf3d179-0f34-4c15-ae21-b8feb15e422d\"),\n            (\"c5181083-d3e9-41e7-a935-83fb9fa01488\"),\n            (\"d11705af-5575-43e5-b22e-08d155fbaa62\"),\n        ],\n        [\n            (\"d11705af-5575-43e5-b22e-08d155fbaa62\"),\n            (\"c5181083-d3e9-41e7-a935-83fb9fa01488\"),\n            (\"dcf3d179-0f34-4c15-ae21-b8feb15e422d\"),\n        ],\n    ]\n    rp = ranked_pairs(ranks)\n    print(rp)\n", "backend/oasst_backend/utils/exported_tree_loading.py": "import json\nfrom collections import defaultdict\nfrom typing import List\n\nimport pandas as pd\n\n\ndef load_jsonl(filepaths):\n    data = []\n    for filepath in filepaths:\n        with open(filepath, \"r\") as f:\n            for line in f:\n                data.append(json.loads(line))\n    return data\n\n\ndef separate_qa_helper(node, depth, msg_dict):\n    if \"text\" in node:\n        if node[\"role\"] == \"prompter\":\n            msg_dict[\"user_messages\"].append(str(node[\"text\"]))\n        elif node[\"role\"] == \"assistant\":\n            msg_dict[\"assistant_messages\"].append(str(node[\"text\"]))\n        depth += 1\n        if \"replies\" in node:\n            for reply in node[\"replies\"]:\n                separate_qa_helper(reply, depth, msg_dict)\n\n\ndef store_qa_data_separate(trees, data):\n    message_list = []\n    for i, msg_tree in enumerate(trees):\n        if \"prompt\" in msg_tree.keys():\n            separate_qa_helper(msg_tree[\"prompt\"], i, data)\n        elif \"prompt\" not in msg_tree.keys():\n            message_list.append(msg_tree)\n    return data, message_list\n\n\ndef group_qa_helper(node, depth, msg_pairs):\n    if \"text\" in node:\n        if node[\"role\"] == \"prompter\":\n            if \"replies\" in node:\n                for reply in node[\"replies\"]:\n                    qa_pair = {\"instruct\": str(node[\"text\"]), \"answer\": str(reply[\"text\"])}\n                    msg_pairs.append(qa_pair)\n        depth += 1\n        if \"replies\" in node:\n            for reply in node[\"replies\"]:\n                group_qa_helper(reply, depth, msg_pairs)\n\n\ndef store_qa_data_paired(trees, data: List):\n    message_list = []\n    for i, msg_tree in enumerate(trees):\n        if \"prompt\" in msg_tree.keys():\n            group_qa_helper(msg_tree[\"prompt\"], i, data)\n        elif \"prompt\" not in msg_tree.keys():\n            message_list.append(msg_tree)\n    return data, message_list\n\n\ndef load_data(filepaths: List[str], paired=False):\n    trees = load_jsonl(filepaths)\n    if paired:\n        data = []\n        data, message_list = store_qa_data_paired(trees, data)\n        sents = [f\"{qa['instruct']} {qa['answer']}\" for qa in data]\n    elif not paired:\n        data = defaultdict(list)\n        data, message_list = store_qa_data_separate(trees, data)\n        sents = data[\"user_messages\"] + data[\"assistant_messages\"]\n\n    data = [(i, sent) for i, sent in enumerate(sents)]\n    data = pd.DataFrame(data, columns=[\"id\", \"query\"])\n    return data, message_list\n", "backend/oasst_backend/utils/__init__.py": "", "backend/oasst_backend/utils/hugging_face.py": "from enum import Enum\nfrom typing import Any, Dict\n\nimport aiohttp\nfrom loguru import logger\nfrom oasst_backend.config import settings\nfrom oasst_shared.exceptions import OasstError, OasstErrorCode\n\n\nclass HfUrl(str, Enum):\n    HUGGINGFACE_TOXIC_CLASSIFICATION = \"https://api-inference.huggingface.co/models\"\n    HUGGINGFACE_FEATURE_EXTRACTION = \"https://api-inference.huggingface.co/pipeline/feature-extraction\"\n\n\nclass HfClassificationModel(str, Enum):\n    TOXIC_ROBERTA = \"unitary/multilingual-toxic-xlm-roberta\"\n\n\nclass HfEmbeddingModel(str, Enum):\n    MINILM = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n\n\nclass HuggingFaceAPI:\n    \"\"\"Class Object to make post calls to endpoints for inference in models hosted in HuggingFace\"\"\"\n\n    def __init__(\n        self,\n        api_url: str,\n    ):\n        # The API endpoint we want to access\n        self.api_url: str = api_url\n\n        # Access token for the api\n        self.api_key: str = settings.HUGGING_FACE_API_KEY\n\n        # Headers going to be used\n        self.headers: Dict[str, str] = {\"Authorization\": f\"Bearer {self.api_key}\"}\n\n    async def post(self, input: str, wait_for_model: bool = True) -> Any:\n        \"\"\"Post request to the endpoint to get an inference\n\n        Args:\n            input (str): the input that we will pass to the model\n\n        Raises:\n            OasstError: in the case we get a bad response\n\n        Returns:\n            inference: the inference we obtain from the model in HF\n        \"\"\"\n\n        async with aiohttp.ClientSession() as session:\n            payload: Dict[str, str] = {\"inputs\": input, \"wait_for_model\": wait_for_model}\n\n            async with session.post(self.api_url, headers=self.headers, json=payload) as response:\n                # If we get a bad response\n                if not response.ok:\n                    logger.error(response)\n                    logger.info(self.headers)\n                    raise OasstError(\n                        f\"Response Error HuggingFace API (Status: {response.status})\",\n                        error_code=OasstErrorCode.HUGGINGFACE_API_ERROR,\n                    )\n\n                # Get the response from the API call\n                inference = await response.json()\n\n        return inference\n", "backend/oasst_backend/utils/discord.py": "from uuid import UUID\n\nimport requests\nfrom loguru import logger\nfrom oasst_backend.celery_worker import app as celery_app\nfrom oasst_backend.config import settings\n\nROOT_ENDPOINT = \"https://discord.com/api/v10\"\n\n\n@celery_app.task(name=\"send_new_report_message\")\ndef send_new_report_message(message_details: dict, label_text: str, user_id: UUID):\n    \"\"\"\n    Send a message to the Discord channel when a new message is flagged.\n    Note: this is a Celery task.\n\n    Args:\n        message_details (dict): some of the attributes of a Message instance that we will use to compose the discord\n        message.\n        label_text (str): the label text\n        user_id (UUID): the user ID\n    \"\"\"\n    if settings.DISCORD_API_KEY is None or settings.DISCORD_CHANNEL_ID is None:\n        return\n\n    try:\n        logger.debug(\"Sending flagged message to Discord\")\n        label_text = label_text[:4096]  # 4096 is the max length of discord embed description\n        message_content_embed = {\n            \"title\": \"Message content\",\n            \"description\": message_details[\"message_text\"],\n            \"color\": 0x3498DB,  # Blue\n            \"footer\": {\n                \"text\": (\n                    f\"Role: {message_details['role']}\\t \"\n                    f\"Lang: {message_details['lang']}\\t \"\n                    f\"\ud83d\udc4d{message_details['thumbs_up']} \"\n                    f\"\ud83d\udc4e{message_details['thumbs_down']} \"\n                    f\"\ud83d\udea9{message_details['red_flag']}\"\n                )\n            },\n        }\n        label_text_embed = {\n            \"title\": \"Report content\",\n            \"description\": f\"{label_text}\",\n            \"color\": 0xE74C3C,  # Red\n            \"author\": {\n                \"name\": f\"User ID: {user_id}\",\n                \"url\": f\"https://open-assistant.io/admin/manage_user/{user_id}\",\n            },\n        }\n        res = requests.post(\n            f\"{ROOT_ENDPOINT}/channels/{settings.DISCORD_CHANNEL_ID}/messages\",\n            headers={\n                \"user-agent\": \"DiscordBot (https://open-assistant.io, 1)\",\n                \"authorization\": f\"Bot {settings.DISCORD_API_KEY}\",\n            },\n            json={\n                \"content\": f\"New flagged message https://open-assistant.io/admin/messages/{message_details['message_id']}\",\n                \"embeds\": [message_content_embed, label_text_embed],\n            },\n        )\n        res.raise_for_status()\n    except Exception as e:\n        logger.exception(f\"Failed to send flagged message. error: {e}\")\n", "backend/oasst_backend/utils/language_classification.py": "import os\nimport pickle\nfrom collections import Counter\n\nfrom sklearn import metrics\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import LinearSVC\n\n\ndef load_and_split(foldername, num_words):\n    ls = os.listdir(foldername)\n    X = []\n    Y = []\n    langmap = dict()\n    for idx, x in enumerate(ls):\n        print(\"loading language\", x)\n        with open(foldername + \"/\" + x, \"r\") as reader:\n            tmp = reader.read().split(\" \")\n            tmp = [\" \".join(tmp[i : i + num_words]) for i in range(0, 100_000, num_words)]\n            X.extend(tmp)\n            Y.extend([idx] * len(tmp))\n            langmap[idx] = x\n    x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.90)\n    return x_train, x_test, y_train, y_test, langmap\n\n\ndef build_and_train_pipeline(x_train, y_train):\n    vectorizer = TfidfVectorizer(ngram_range=(1, 2), analyzer=\"char\", use_idf=False)\n    clf = Pipeline(\n        [\n            (\"vec\", vectorizer),\n            # (\"nystrom\", Nystroem(n_components=1000,n_jobs=6)),\n            (\"clf\", LinearSVC(C=0.5)),\n            # (\"clf\",GaussianNB())\n            # (\"clf\", HistGradientBoostingClassifier())\n        ]\n    )\n    print(\"fitting model...\")\n    clf.fit(x_train, y_train)\n    return clf\n\n\ndef benchmark(clf, x_test, y_test, langmap):\n    print(\"benchmarking model...\")\n    y_pred = clf.predict(x_test)\n    names = list(langmap.values())\n    # print(y_test)\n    # print(langmap)\n    print(metrics.classification_report(y_test, y_pred, target_names=names))\n    cm = metrics.confusion_matrix(y_test, y_pred)\n    print(cm)\n\n\ndef main(foldername, modelname, num_words):\n    x_train, x_test, y_train, y_test, langmap = load_and_split(foldername=foldername, num_words=num_words)\n    clf = build_and_train_pipeline(x_train, y_train)\n    benchmark(clf, x_test, y_test, langmap)\n    save_model(clf, langmap, num_words, modelname)\n    model = load(modelname)\n    print(\n        \"running inference on long tests\",\n        inference_voter(\n            model,\n            \"\"\"\n    What language is this text written in? Nobody knows until you fill in at least ten words.\n    This test here is to check whether the moving window approach works,\n    so I still need to fill in a little more text.\n    \"\"\",\n        ),\n    )\n\n\ndef load(modelname):\n    with open(modelname, \"rb\") as writer:\n        data = pickle.load(writer)\n    return data\n\n\ndef save_model(model, idx_to_name, num_words, modelname):\n    out = {\n        \"model\": model,\n        \"idx_to_name\": idx_to_name,\n        \"num_words\": num_words,\n    }\n    with open(modelname, \"wb\") as writer:\n        pickle.dump(out, writer)\n\n\ndef inference_voter(model, text):\n    tmp = text.split()\n    # print(len(tmp), tmp)\n    tmp = [\" \".join(tmp[i : i + model[\"num_words\"]]) for i in range(0, len(tmp) - model[\"num_words\"])]\n    predictions = model[\"model\"].predict(tmp)\n    # print(\"integer predictions\", predictions)\n    # print(\"name predictions\", *[model[\"idx_to_name\"][n] for n in predictions])\n    result = Counter(predictions).most_common(1)[0][0]\n    return model[\"idx_to_name\"][result]\n\n\nif __name__ == \"__main__\":\n    import argparse\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"-m\", \"--model\", help=\"save location for model and metadata\")\n    parser.add_argument(\"-d\", \"--data\", help=\"specify the folder for data files\")\n    parser.add_argument(\"-n\", \"--num_words\", help=\"number of words to use for statistics\", type=int)\n    args = parser.parse_args()\n    # np.set_printoptions(threshold=np.inf)\n    main(args.data, args.model, args.num_words)\n", "backend/oasst_backend/schemas/message_tree.py": "from uuid import UUID\n\nfrom oasst_backend.models.message_tree_state import State as TreeState\nfrom pydantic import BaseModel\n\n\nclass MessageTreeStateResponse(BaseModel):\n    message_tree_id: UUID\n    state: TreeState\n    goal_tree_size: int\n    max_depth: int\n    max_children_count: int\n    active: bool\n    origin: str | None\n", "backend/oasst_backend/schemas/text_labels.py": "from oasst_shared.schemas.protocol import LabelDescription\nfrom pydantic import BaseModel\n\n\nclass ValidLabelsResponse(BaseModel):\n    valid_labels: list[LabelDescription]\n", "backend/oasst_backend/schemas/__init__.py": "", "backend/oasst_backend/schemas/hugging_face.py": "from pydantic import BaseModel\n\n\nclass ToxicityClassification(BaseModel):\n    label: str\n    score: float\n", "backend/oasst_backend/api/deps.py": "from http import HTTPStatus\nfrom secrets import token_hex\nfrom typing import Generator, NamedTuple, Optional\nfrom uuid import UUID\n\nfrom fastapi import Depends, Request, Response, Security\nfrom fastapi.security import HTTPAuthorizationCredentials, HTTPBearer\nfrom fastapi.security.api_key import APIKey, APIKeyHeader, APIKeyQuery\nfrom fastapi_limiter.depends import RateLimiter\nfrom loguru import logger\nfrom oasst_backend.config import settings\nfrom oasst_backend.database import engine\nfrom oasst_backend.models import ApiClient\nfrom oasst_shared.exceptions import OasstError, OasstErrorCode\nfrom sqlmodel import Session\n\n\ndef get_db() -> Generator:\n    with Session(engine) as db:\n        yield db\n\n\napi_key_query = APIKeyQuery(name=\"api_key\", scheme_name=\"api-key\", auto_error=False)\napi_key_header = APIKeyHeader(name=\"X-API-Key\", scheme_name=\"api-key\", auto_error=False)\noasst_user_query = APIKeyQuery(name=\"oasst_user\", scheme_name=\"oasst-user\", auto_error=False)\noasst_user_header = APIKeyHeader(name=\"x-oasst-user\", scheme_name=\"oasst-user\", auto_error=False)\n\nbearer_token = HTTPBearer(auto_error=False)\n\n\ndef get_api_key(\n    api_key_query: str = Security(api_key_query),\n    api_key_header: str = Security(api_key_header),\n) -> str:\n    if api_key_query:\n        return api_key_query\n    else:\n        return api_key_header\n\n\nclass FrontendUserId(NamedTuple):\n    auth_method: str\n    username: str\n\n\ndef get_frontend_user_id(\n    user_query: str = Security(oasst_user_query),\n    user_header: str = Security(oasst_user_header),\n) -> FrontendUserId:\n    def split_user(v: str) -> tuple[str, str]:\n        if type(v) is str:\n            v = v.split(\":\", maxsplit=1)\n            if len(v) == 2:\n                return FrontendUserId(auth_method=v[0], username=v[1])\n        return FrontendUserId(auth_method=None, username=None)\n\n    if user_query:\n        return split_user(user_query)\n    else:\n        return split_user(user_header)\n\n\ndef create_api_client(\n    *,\n    session: Session,\n    description: str,\n    frontend_type: str,\n    trusted: bool | None = False,\n    admin_email: str | None = None,\n    api_key: str | None = None,\n    force_id: Optional[UUID] = None,\n) -> ApiClient:\n    if api_key is None:\n        api_key = token_hex(32)\n\n    logger.info(f\"Creating new api client with {api_key=}\")\n    api_client = ApiClient(\n        api_key=api_key,\n        description=description,\n        frontend_type=frontend_type,\n        trusted=trusted,\n        admin_email=admin_email,\n    )\n    if force_id:\n        api_client.id = force_id\n    session.add(api_client)\n    session.commit()\n    session.refresh(api_client)\n    return api_client\n\n\ndef api_auth(\n    api_key: APIKey,\n    db: Session,\n) -> ApiClient:\n    if api_key:\n        api_client = db.query(ApiClient).filter(ApiClient.api_key == api_key).first()\n        if api_client is not None and api_client.enabled:\n            return api_client\n\n    raise OasstError(\n        \"Could not validate credentials\",\n        error_code=OasstErrorCode.API_CLIENT_NOT_AUTHORIZED,\n        http_status_code=HTTPStatus.FORBIDDEN,\n    )\n\n\ndef get_api_client(\n    api_key: APIKey = Depends(get_api_key),\n    db: Session = Depends(get_db),\n):\n    return api_auth(api_key, db)\n\n\ndef get_trusted_api_client(\n    api_key: APIKey = Depends(get_api_key),\n    db: Session = Depends(get_db),\n):\n    client = api_auth(api_key, db)\n    if not client.trusted:\n        raise OasstError(\n            \"Forbidden\",\n            error_code=OasstErrorCode.API_CLIENT_NOT_AUTHORIZED,\n            http_status_code=HTTPStatus.FORBIDDEN,\n        )\n    return client\n\n\ndef get_root_token(bearer_token: HTTPAuthorizationCredentials = Security(bearer_token)) -> str:\n    if bearer_token:\n        token = bearer_token.credentials\n        if token and token in settings.ROOT_TOKENS:\n            return token\n    raise OasstError(\n        \"Could not validate credentials\",\n        error_code=OasstErrorCode.ROOT_TOKEN_NOT_AUTHORIZED,\n        http_status_code=HTTPStatus.FORBIDDEN,\n    )\n\n\nasync def user_identifier(request: Request) -> str:\n    \"\"\"Identify a request by user based on api_key and user header\"\"\"\n    api_key = request.headers.get(\"X-API-Key\") or request.query_params.get(\"api_key\")\n    user = request.headers.get(\"x-oasst-user\")\n    if not user:\n        payload = await request.json()\n        auth_method = payload.get(\"user\").get(\"auth_method\")\n        user_id = payload.get(\"user\").get(\"id\")\n        user = f\"{auth_method}:{user_id}\"\n    return f\"{api_key}:{user}\"\n\n\nclass UserRateLimiter(RateLimiter):\n    def __init__(\n        self, times: int = 100, milliseconds: int = 0, seconds: int = 0, minutes: int = 1, hours: int = 0\n    ) -> None:\n        super().__init__(times, milliseconds, seconds, minutes, hours, user_identifier)\n\n    async def __call__(self, request: Request, response: Response, api_key: str = Depends(get_api_key)) -> None:\n        # Skip if rate limiting is disabled\n        if not settings.RATE_LIMIT:\n            return\n\n        # Attempt to retrieve api_key and user information\n        user = (await request.json()).get(\"user\")\n\n        # Skip when api_key and user information are not available\n        # (such that it will be handled by `APIClientRateLimiter`)\n        if not api_key or not user or not user.get(\"id\"):\n            return\n\n        return await super().__call__(request, response)\n\n\nclass UserTaskTypeRateLimiter(RateLimiter):\n    \"\"\"\n    User-level rate limiter for a specific task type.\n    \"\"\"\n\n    def __init__(\n        self,\n        task_types: list[str],\n        times: int = 100,\n        milliseconds: int = 0,\n        seconds: int = 0,\n        minutes: int = 1,\n        hours: int = 0,\n    ) -> None:\n        super().__init__(times, milliseconds, seconds, minutes, hours, user_identifier)\n        self.task_types = task_types\n\n    async def __call__(self, request: Request, response: Response, api_key: str = Depends(get_api_key)) -> None:\n        # Skip if rate limiting is disabled\n        if not settings.RATE_LIMIT:\n            return\n\n        # Attempt to retrieve api_key and user information\n        json = await request.json()\n        user = json.get(\"user\")\n\n        # Skip when api_key and user information are not available\n        # (such that it will be handled by `APIClientRateLimiter`)\n        if not api_key or not user or not user.get(\"id\"):\n            return\n\n        # Skip when the request is not in our task types of interest\n        if not json.get(\"type\") or json.get(\"type\") not in self.task_types:\n            return\n\n        return await super().__call__(request, response)\n\n\nclass APIClientRateLimiter(RateLimiter):\n    def __init__(\n        self, times: int = 10_000, milliseconds: int = 0, seconds: int = 0, minutes: int = 1, hours: int = 0\n    ) -> None:\n        async def identifier(request: Request) -> str:\n            \"\"\"Identify a request based on api_key and user.id\"\"\"\n            api_key = request.headers.get(\"X-API-Key\") or request.query_params.get(\"api_key\")\n            return f\"{api_key}\"\n\n        super().__init__(times, milliseconds, seconds, minutes, hours, identifier)\n\n    async def __call__(self, request: Request, response: Response, api_key: str = Depends(get_api_key)) -> None:\n        # Skip if rate limiting is disabled\n        if not settings.RATE_LIMIT:\n            return\n\n        # Attempt to retrieve api_key and user information\n        user = (await request.json()).get(\"user\")\n\n        # Skip if user information is available\n        # (such that it will be handled by `UserRateLimiter`)\n        if not api_key or user:\n            return\n\n        return await super().__call__(request, response)\n", "backend/oasst_backend/api/__init__.py": "", "backend/oasst_backend/api/v1/trollboards.py": "from typing import Optional\n\nfrom fastapi import APIRouter, Depends, Query\nfrom oasst_backend.api import deps\nfrom oasst_backend.models import ApiClient\nfrom oasst_backend.user_stats_repository import UserStatsRepository, UserStatsTimeFrame\nfrom oasst_shared.schemas.protocol import TrollboardStats\nfrom sqlmodel import Session\n\nrouter = APIRouter()\n\n\n@router.get(\"/{time_frame}\", response_model=TrollboardStats)\ndef get_trollboard(\n    time_frame: UserStatsTimeFrame,\n    max_count: Optional[int] = Query(100, gt=0, le=10000),\n    enabled: Optional[bool] = None,\n    api_client: ApiClient = Depends(deps.get_trusted_api_client),\n    db: Session = Depends(deps.get_db),\n) -> TrollboardStats:\n    usr = UserStatsRepository(db)\n    return usr.get_trollboard(time_frame, limit=max_count, enabled=enabled)\n", "backend/oasst_backend/api/v1/frontend_messages.py": "from typing import Optional\n\nfrom fastapi import APIRouter, Depends\nfrom oasst_backend.api import deps\nfrom oasst_backend.api.v1 import utils\nfrom oasst_backend.models import ApiClient\nfrom oasst_backend.prompt_repository import PromptRepository\nfrom oasst_shared.schemas import protocol\nfrom sqlmodel import Session\n\nrouter = APIRouter()\n\n\n@router.get(\"/{message_id}\", response_model=protocol.Message)\ndef get_message_by_frontend_id(\n    message_id: str, api_client: ApiClient = Depends(deps.get_api_client), db: Session = Depends(deps.get_db)\n):\n    \"\"\"\n    Get a message by its frontend ID.\n    \"\"\"\n    pr = PromptRepository(db, api_client)\n    message = pr.fetch_message_by_frontend_message_id(message_id)\n    return utils.prepare_message(message)\n\n\n@router.get(\"/{message_id}/conversation\", response_model=protocol.Conversation)\ndef get_conv_by_frontend_id(\n    message_id: str, api_client: ApiClient = Depends(deps.get_api_client), db: Session = Depends(deps.get_db)\n):\n    \"\"\"\n    Get a conversation from the tree root and up to the message with given frontend ID.\n    \"\"\"\n\n    pr = PromptRepository(db, api_client)\n    message = pr.fetch_message_by_frontend_message_id(message_id)\n    messages = pr.fetch_message_conversation(message)\n    return utils.prepare_conversation(messages)\n\n\n@router.get(\"/{message_id}/tree\", response_model=protocol.MessageTree)\ndef get_tree_by_frontend_id(\n    message_id: str,\n    include_spam: Optional[bool] = True,\n    include_deleted: Optional[bool] = False,\n    api_client: ApiClient = Depends(deps.get_api_client),\n    db: Session = Depends(deps.get_db),\n):\n    \"\"\"\n    Get all messages belonging to the same message tree.\n    Message is identified by its frontend ID.\n    \"\"\"\n    pr = PromptRepository(db, api_client)\n    message = pr.fetch_message_by_frontend_message_id(message_id)\n    review_result = None if include_spam else True\n    deleted = None if include_deleted else False\n    tree = pr.fetch_message_tree(message.message_tree_id, review_result=review_result, deleted=deleted)\n    return utils.prepare_tree(tree, message.message_tree_id)\n\n\n@router.get(\"/{message_id}/children\", response_model=list[protocol.Message])\ndef get_children_by_frontend_id(\n    message_id: str, api_client: ApiClient = Depends(deps.get_api_client), db: Session = Depends(deps.get_db)\n):\n    \"\"\"\n    Get all messages belonging to the same message tree.\n    \"\"\"\n    pr = PromptRepository(db, api_client)\n    message = pr.fetch_message_by_frontend_message_id(message_id)\n    messages = pr.fetch_message_children(message.id, review_result=None)\n    return utils.prepare_message_list(messages)\n\n\n@router.get(\"/{message_id}/descendants\", response_model=protocol.MessageTree)\ndef get_descendants_by_frontend_id(\n    message_id: str, api_client: ApiClient = Depends(deps.get_api_client), db: Session = Depends(deps.get_db)\n):\n    \"\"\"\n    Get a subtree which starts with this message.\n    The message is identified by its frontend ID.\n    \"\"\"\n    pr = PromptRepository(db, api_client)\n    message = pr.fetch_message_by_frontend_message_id(message_id)\n    descendants = pr.fetch_message_descendants(message)\n    return utils.prepare_tree(descendants, message.id)\n\n\n@router.get(\"/{message_id}/longest_conversation_in_tree\", response_model=protocol.Conversation)\ndef get_longest_conv_by_frontend_id(\n    message_id: str, api_client: ApiClient = Depends(deps.get_api_client), db: Session = Depends(deps.get_db)\n):\n    \"\"\"\n    Get the longest conversation from the tree of the message.\n    The message is identified by its frontend ID.\n    \"\"\"\n    pr = PromptRepository(db, api_client)\n    message = pr.fetch_message_by_frontend_message_id(message_id)\n    conv = pr.fetch_longest_conversation(message.message_tree_id)\n    return utils.prepare_conversation(conv)\n\n\n@router.get(\"/{message_id}/max_children_in_tree\", response_model=protocol.MessageTree)\ndef get_max_children_by_frontend_id(\n    message_id: str, api_client: ApiClient = Depends(deps.get_api_client), db: Session = Depends(deps.get_db)\n):\n    \"\"\"\n    Get message with the most children from the tree of the provided message.\n    The message is identified by its frontend ID.\n    \"\"\"\n    pr = PromptRepository(db, api_client)\n    message = pr.fetch_message_by_frontend_message_id(message_id)\n    message, children = pr.fetch_message_with_max_children(message.message_tree_id)\n    return utils.prepare_tree([message, *children], message.id)\n", "backend/oasst_backend/api/v1/auth.py": "from typing import Union\n\nfrom cryptography.hazmat.primitives import hashes\nfrom cryptography.hazmat.primitives.kdf.hkdf import HKDF\nfrom fastapi import APIRouter, Depends, Security\nfrom fastapi.security import APIKeyCookie\nfrom jose import jwe\nfrom oasst_backend.config import settings\nfrom pydantic import BaseModel, EmailStr\n\nrouter = APIRouter()\n\noauth2_scheme = APIKeyCookie(name=settings.AUTH_COOKIE_NAME)\n\n\nclass TokenData(BaseModel):\n    \"\"\"\n    A minimal re-creation of the web's token type.  To be expanded later.\n    \"\"\"\n\n    email: Union[EmailStr, None] = None\n\n\nasync def get_current_user(token: str = Security(oauth2_scheme)):\n    \"\"\"\n    Decrypts the user's JSON Web Token using HKDF encryption and returns the\n    TokenData.\n    \"\"\"\n    # We first generate a key from the auth secret.\n    hkdf = HKDF(\n        algorithm=hashes.SHA256(),\n        length=settings.AUTH_LENGTH,\n        salt=settings.AUTH_SALT,\n        info=settings.AUTH_INFO,\n    )\n    key = hkdf.derive(settings.AUTH_SECRET)\n    # Next we decrypt the JWE token.\n    payload = jwe.decrypt(token, key)\n    # Finally we have the real token JSON payload and can do whatever we want.\n    return TokenData.parse_raw(payload)\n\n\n@router.get(\"/check\", response_model=str)\nasync def auth_check(token_data: TokenData = Depends(get_current_user)):\n    \"\"\"Returns the user's email if it can be decrypted.\"\"\"\n    return token_data.email\n", "backend/oasst_backend/api/v1/admin.py": "from datetime import datetime\nfrom typing import Optional\nfrom uuid import UUID\n\nimport pydantic\nfrom fastapi import APIRouter, Depends, Query\nfrom loguru import logger\nfrom oasst_backend.api import deps\nfrom oasst_backend.config import Settings, settings\nfrom oasst_backend.models import ApiClient, User\nfrom oasst_backend.prompt_repository import PromptRepository, UserRepository\nfrom oasst_backend.tree_manager import TreeManager\nfrom oasst_backend.utils.database_utils import CommitMode, managed_tx_function\nfrom oasst_shared import utils\nfrom oasst_shared.exceptions.oasst_api_error import OasstError, OasstErrorCode\nfrom oasst_shared.schemas.protocol import PageResult, SystemStats\nfrom oasst_shared.utils import ScopeTimer, log_timing, unaware_to_utc\nfrom starlette.status import HTTP_204_NO_CONTENT\n\nrouter = APIRouter()\n\n\nclass CreateApiClientRequest(pydantic.BaseModel):\n    description: str\n    frontend_type: str\n    trusted: bool | None = False\n    admin_email: str | None = None\n\n\n@router.post(\"/api_client\", response_model=str)\nasync def create_api_client(\n    request: CreateApiClientRequest,\n    root_token: str = Depends(deps.get_root_token),\n    session: deps.Session = Depends(deps.get_db),\n) -> str:\n    logger.info(f\"Creating new api client with {request=}\")\n    api_client = deps.create_api_client(\n        session=session,\n        description=request.description,\n        frontend_type=request.frontend_type,\n        trusted=request.trusted,\n        admin_email=request.admin_email,\n    )\n    logger.info(f\"Created api_client with key {api_client.api_key}\")\n    return api_client.api_key\n\n\n@router.get(\"/backend_settings/full\", response_model=Settings)\nasync def get_backend_settings_full(api_client: ApiClient = Depends(deps.get_trusted_api_client)) -> Settings:\n    logger.info(\n        f\"Backend settings requested by trusted api_client {api_client.id} (admin_email: {api_client.admin_email}, frontend_type: {api_client.frontend_type})\"\n    )\n    return settings\n\n\nclass PublicSettings(pydantic.BaseModel):\n    \"\"\"Subset of backend settings which can be retrieved by untrusted API clients.\"\"\"\n\n    PROJECT_NAME: str\n    API_V1_STR: str\n    MESSAGE_SIZE_LIMIT: int\n    DEBUG_USE_SEED_DATA: bool\n    DEBUG_ALLOW_SELF_LABELING: bool\n    DEBUG_SKIP_EMBEDDING_COMPUTATION: bool\n    DEBUG_SKIP_TOXICITY_CALCULATION: bool\n    DEBUG_DATABASE_ECHO: bool\n    USER_STATS_INTERVAL_DAY: int\n    USER_STATS_INTERVAL_WEEK: int\n    USER_STATS_INTERVAL_MONTH: int\n    USER_STATS_INTERVAL_TOTAL: int\n\n\n@router.get(\"/backend_settings/public\", response_model=PublicSettings)\nasync def get_backend_settings_public(api_client: ApiClient = Depends(deps.get_api_client)) -> PublicSettings:\n    return PublicSettings(**settings.dict())\n\n\nclass PurgeResultModel(pydantic.BaseModel):\n    before: SystemStats\n    after: SystemStats\n    preview: bool\n    duration: float\n\n\n@router.post(\"/purge_user/{user_id}\", response_model=PurgeResultModel)\nasync def purge_user(\n    user_id: UUID,\n    preview: bool = True,\n    ban: bool = True,\n    api_client: ApiClient = Depends(deps.get_trusted_api_client),\n) -> str:\n    assert api_client.trusted\n\n    @managed_tx_function(CommitMode.ROLLBACK if preview else CommitMode.COMMIT)\n    def purge_tx(session: deps.Session) -> tuple[User, SystemStats, SystemStats]:\n        pr = PromptRepository(session, api_client)\n\n        stats_before = pr.get_stats()\n\n        user = pr.user_repository.get_user(user_id)\n        tm = TreeManager(session, pr)\n        tm.purge_user(user_id=user_id, ban=ban)\n\n        session.expunge(user)\n        return user, stats_before, pr.get_stats()\n\n    timer = ScopeTimer()\n    user, before, after = purge_tx()\n    timer.stop()\n\n    if preview:\n        logger.info(\n            f\"PURGE USER PREVIEW: '{user.display_name}' (id: {str(user_id)}; username: '{user.username}'; auth-method: '{user.auth_method}')\"\n        )\n    else:\n        logger.warning(\n            f\"PURGE USER: '{user.display_name}' (id: {str(user_id)}; username: '{user.username}'; auth-method: '{user.auth_method}')\"\n        )\n\n    logger.info(f\"{before=}; {after=}\")\n    return PurgeResultModel(before=before, after=after, preview=preview, duration=timer.elapsed)\n\n\n@router.post(\"/purge_user/{user_id}/messages\", response_model=PurgeResultModel)\nasync def purge_user_messages(\n    user_id: UUID,\n    purge_initial_prompts: bool = False,\n    min_date: datetime = None,\n    max_date: datetime = None,\n    preview: bool = True,\n    api_client: ApiClient = Depends(deps.get_trusted_api_client),\n) -> str:\n    assert api_client.trusted\n\n    min_date = unaware_to_utc(min_date)\n    max_date = unaware_to_utc(max_date)\n\n    @managed_tx_function(CommitMode.ROLLBACK if preview else CommitMode.COMMIT)\n    def purge_user_messages_tx(session: deps.Session):\n        pr = PromptRepository(session, api_client)\n\n        stats_before = pr.get_stats()\n\n        user = pr.user_repository.get_user(user_id)\n\n        tm = TreeManager(session, pr)\n        tm.purge_user_messages(\n            user_id, purge_initial_prompts=purge_initial_prompts, min_date=min_date, max_date=max_date\n        )\n\n        session.expunge(user)\n        return user, stats_before, pr.get_stats()\n\n    timer = ScopeTimer()\n    user, before, after = purge_user_messages_tx()\n    timer.stop()\n\n    if preview:\n        logger.info(\n            f\"PURGE USER MESSAGES PREVIEW: '{user.display_name}' (id: {str(user_id)}; username: '{user.username}'; auth-method: '{user.auth_method}')\"\n        )\n    else:\n        logger.warning(\n            f\"PURGE USER MESSAGES: '{user.display_name}' (id: {str(user_id)}; username: '{user.username}'; auth-method: '{user.auth_method}')\"\n        )\n\n    logger.info(f\"{before=}; {after=}\")\n    return PurgeResultModel(before=before, after=after, preview=preview, duration=timer.elapsed)\n\n\nclass FlaggedMessageResponse(pydantic.BaseModel):\n    message_id: UUID\n    processed: bool\n    created_date: Optional[datetime]\n\n\nclass FlaggedMessagePage(PageResult):\n    items: list[FlaggedMessageResponse]\n\n\n@router.get(\"/flagged_messages/cursor\", response_model=FlaggedMessagePage)\ndef get_flagged_messages_cursor(\n    *,\n    before: Optional[str] = None,\n    after: Optional[str] = None,\n    max_count: Optional[int] = Query(10, gt=0, le=1000),\n    desc: Optional[bool] = False,\n    session: deps.Session = Depends(deps.get_db),\n    api_client: ApiClient = Depends(deps.get_trusted_api_client),\n) -> str:\n    assert api_client.trusted\n    assert max_count is not None\n\n    def split_cursor(x: str | None) -> tuple[datetime, UUID]:\n        if not x:\n            return None, None\n        try:\n            m = utils.split_uuid_pattern.match(x)\n            if m:\n                return datetime.fromisoformat(m[2]), UUID(m[1])\n            return datetime.fromisoformat(x), None\n        except ValueError:\n            raise OasstError(\"Invalid cursor value\", OasstErrorCode.INVALID_CURSOR_VALUE)\n\n    if desc:\n        gte_created_date, gt_id = split_cursor(before)\n        lte_created_date, lt_id = split_cursor(after)\n        query_desc = not (before is not None and not after)\n    else:\n        lte_created_date, lt_id = split_cursor(before)\n        gte_created_date, gt_id = split_cursor(after)\n        query_desc = before is not None and not after\n\n    logger.debug(f\"{desc=} {query_desc=} {gte_created_date=} {lte_created_date=}\")\n\n    qry_max_count = max_count + 1 if before is None or after is None else max_count\n\n    pr = PromptRepository(session, api_client)\n    items = pr.fetch_flagged_messages_by_created_date(\n        gte_created_date=gte_created_date,\n        gt_id=gt_id,\n        lte_created_date=lte_created_date,\n        lt_id=lt_id,\n        desc=query_desc,\n        limit=qry_max_count,\n    )\n\n    num_rows = len(items)\n    if qry_max_count > max_count and num_rows == qry_max_count:\n        assert not (before and after)\n        items = items[:-1]\n\n    if desc != query_desc:\n        items.reverse()\n\n    n, p = None, None\n    if len(items) > 0:\n        if (num_rows > max_count and before) or after:\n            p = str(items[0].message_id) + \"$\" + items[0].created_date.isoformat()\n        if num_rows > max_count or before:\n            n = str(items[-1].message_id) + \"$\" + items[-1].created_date.isoformat()\n    else:\n        if after:\n            p = lte_created_date.isoformat() if desc else gte_created_date.isoformat()\n        if before:\n            n = gte_created_date.isoformat() if desc else lte_created_date.isoformat()\n\n    order = \"desc\" if desc else \"asc\"\n    print(p, n, items, order)\n    return FlaggedMessagePage(prev=p, next=n, sort_key=\"created_date\", order=order, items=items)\n\n\n@router.get(\"/flagged_messages\", response_model=list[FlaggedMessageResponse])\nasync def get_flagged_messages(\n    max_count: Optional[int],\n    session: deps.Session = Depends(deps.get_db),\n    api_client: ApiClient = Depends(deps.get_trusted_api_client),\n) -> str:\n    assert api_client.trusted\n\n    pr = PromptRepository(session, api_client)\n    flagged_messages = pr.fetch_flagged_messages(max_count=max_count)\n    resp = [FlaggedMessageResponse(**msg.__dict__) for msg in flagged_messages]\n    return resp\n\n\n@router.post(\"/flagged_messages/{message_id}/processed\", response_model=FlaggedMessageResponse)\nasync def process_flagged_messages(\n    message_id: UUID,\n    session: deps.Session = Depends(deps.get_db),\n    api_client: ApiClient = Depends(deps.get_trusted_api_client),\n) -> str:\n    assert api_client.trusted\n\n    pr = PromptRepository(session, api_client)\n    flagged_msg = pr.process_flagged_message(message_id=message_id)\n    resp = FlaggedMessageResponse(**flagged_msg.__dict__)\n    return resp\n\n\nclass MergeUsersRequest(pydantic.BaseModel):\n    destination_user_id: UUID\n    source_user_ids: list[UUID]\n\n\n@log_timing(level=\"INFO\")\n@router.post(\"/merge_users\", response_model=None, status_code=HTTP_204_NO_CONTENT)\ndef merge_users(\n    request: MergeUsersRequest,\n    api_client: ApiClient = Depends(deps.get_trusted_api_client),\n) -> None:\n    @managed_tx_function(CommitMode.COMMIT)\n    def merge_users_tx(session: deps.Session):\n        ur = UserRepository(session, api_client)\n        ur.merge_users(destination_user_id=request.destination_user_id, source_user_ids=request.source_user_ids)\n\n    merge_users_tx()\n\n    logger.info(f\"Merged users: {request=}\")\n", "backend/oasst_backend/api/v1/api.py": "from fastapi import APIRouter\nfrom oasst_backend.api.v1 import (\n    admin,\n    auth,\n    frontend_messages,\n    frontend_users,\n    hugging_face,\n    leaderboards,\n    messages,\n    stats,\n    tasks,\n    text_labels,\n    trollboards,\n    users,\n)\n\napi_router = APIRouter()\napi_router.include_router(tasks.router, prefix=\"/tasks\", tags=[\"tasks\"])\napi_router.include_router(text_labels.router, prefix=\"/text_labels\", tags=[\"text_labels\"])\napi_router.include_router(messages.router, prefix=\"/messages\", tags=[\"messages\"])\napi_router.include_router(frontend_messages.router, prefix=\"/frontend_messages\", tags=[\"frontend_messages\"])\napi_router.include_router(users.router, prefix=\"/users\", tags=[\"users\"])\napi_router.include_router(frontend_users.router, prefix=\"/frontend_users\", tags=[\"frontend_users\"])\napi_router.include_router(stats.router, prefix=\"/stats\", tags=[\"stats\"])\napi_router.include_router(leaderboards.router, prefix=\"/leaderboards\", tags=[\"leaderboards\"])\napi_router.include_router(trollboards.router, prefix=\"/trollboards\", tags=[\"trollboards\"])\napi_router.include_router(hugging_face.router, prefix=\"/hf\", tags=[\"hugging_face\"])\napi_router.include_router(admin.router, prefix=\"/admin\", tags=[\"admin\"])\napi_router.include_router(auth.router, prefix=\"/auth\", tags=[\"auth\"])\n", "backend/oasst_backend/api/v1/frontend_users.py": "import datetime\nfrom typing import Optional\nfrom uuid import UUID\n\nfrom fastapi import APIRouter, Depends, Query\nfrom oasst_backend.api import deps\nfrom oasst_backend.api.v1 import utils\nfrom oasst_backend.api.v1.messages import get_messages_cursor\nfrom oasst_backend.models import ApiClient\nfrom oasst_backend.prompt_repository import PromptRepository\nfrom oasst_backend.user_repository import UserRepository\nfrom oasst_shared.schemas import protocol\nfrom sqlmodel import Session\nfrom starlette.status import HTTP_204_NO_CONTENT\n\nrouter = APIRouter()\n\n\n@router.get(\"/\", response_model=list[protocol.FrontEndUser], deprecated=True)\ndef get_users_ordered_by_username(\n    api_client_id: Optional[UUID] = None,\n    gte_username: Optional[str] = None,\n    gt_id: Optional[UUID] = None,\n    lte_username: Optional[str] = None,\n    lt_id: Optional[UUID] = None,\n    search_text: Optional[str] = None,\n    auth_method: Optional[str] = None,\n    max_count: Optional[int] = Query(100, gt=0, le=10000),\n    api_client: ApiClient = Depends(deps.get_api_client),\n    db: Session = Depends(deps.get_db),\n):\n    ur = UserRepository(db, api_client)\n    users = ur.query_users_ordered_by_username(\n        api_client_id=api_client_id,\n        gte_username=gte_username,\n        gt_id=gt_id,\n        lte_username=lte_username,\n        lt_id=lt_id,\n        auth_method=auth_method,\n        search_text=search_text,\n        limit=max_count,\n    )\n    return [u.to_protocol_frontend_user() for u in users]\n\n\n@router.get(\"/{auth_method}/{username}\", response_model=protocol.FrontEndUser)\ndef query_frontend_user(\n    auth_method: str,\n    username: str,\n    api_client_id: Optional[UUID] = None,\n    api_client: ApiClient = Depends(deps.get_api_client),\n    db: Session = Depends(deps.get_db),\n):\n    \"\"\"\n    Query frontend user.\n    \"\"\"\n    ur = UserRepository(db, api_client)\n    user = ur.query_frontend_user(auth_method, username, api_client_id)\n    return user.to_protocol_frontend_user()\n\n\n@router.post(\"/\", response_model=protocol.FrontEndUser)\ndef create_frontend_user(\n    *,\n    create_user: protocol.CreateFrontendUserRequest,\n    api_client: ApiClient = Depends(deps.get_api_client),\n    db: Session = Depends(deps.get_db),\n):\n    ur = UserRepository(db, api_client)\n    user = ur.lookup_client_user(create_user, create_missing=True)\n\n    def changed(a, b) -> bool:\n        return a is not None and a != b\n\n    # only call update_user if something changed\n    if (\n        changed(create_user.enabled, user.enabled)\n        or changed(create_user.show_on_leaderboard, user.show_on_leaderboard)\n        or changed(create_user.notes, user.notes)\n        or (create_user.tos_acceptance and user.tos_acceptance_date is None)\n    ):\n        user = ur.update_user(\n            user.id,\n            enabled=create_user.enabled,\n            show_on_leaderboard=create_user.show_on_leaderboard,\n            tos_acceptance=create_user.tos_acceptance,\n            notes=create_user.notes,\n        )\n\n    return user.to_protocol_frontend_user()\n\n\n@router.get(\"/{auth_method}/{username}/messages\", response_model=list[protocol.Message])\ndef query_frontend_user_messages(\n    auth_method: str,\n    username: str,\n    api_client_id: UUID = None,\n    max_count: int = Query(10, gt=0, le=1000),\n    start_date: datetime.datetime = None,\n    end_date: datetime.datetime = None,\n    only_roots: bool = False,\n    desc: bool = True,\n    include_deleted: bool = False,\n    lang: Optional[str] = None,\n    api_client: ApiClient = Depends(deps.get_api_client),\n    db: Session = Depends(deps.get_db),\n):\n    \"\"\"\n    Query frontend user messages.\n    \"\"\"\n    pr = PromptRepository(db, api_client, auth_method=auth_method, username=username)\n    messages = pr.query_messages_ordered_by_created_date(\n        auth_method=auth_method,\n        username=username,\n        api_client_id=api_client_id,\n        desc=desc,\n        limit=max_count,\n        gte_created_date=start_date,\n        lte_created_date=end_date,\n        only_roots=only_roots,\n        deleted=None if include_deleted else False,\n        lang=lang,\n    )\n    return utils.prepare_message_list(messages)\n\n\n@router.get(\"/{auth_method}/{username}/messages/cursor\", response_model=protocol.MessagePage)\ndef query_frontend_user_messages_cursor(\n    auth_method: str,\n    username: str,\n    before: Optional[str] = None,\n    after: Optional[str] = None,\n    only_roots: Optional[bool] = False,\n    include_deleted: Optional[bool] = False,\n    max_count: Optional[int] = Query(10, gt=0, le=1000),\n    desc: Optional[bool] = False,\n    lang: Optional[str] = None,\n    frontend_user: deps.FrontendUserId = Depends(deps.get_frontend_user_id),\n    api_client: ApiClient = Depends(deps.get_api_client),\n    db: Session = Depends(deps.get_db),\n):\n    return get_messages_cursor(\n        before=before,\n        after=after,\n        auth_method=auth_method,\n        username=username,\n        only_roots=only_roots,\n        include_deleted=include_deleted,\n        max_count=max_count,\n        desc=desc,\n        lang=lang,\n        frontend_user=frontend_user,\n        api_client=api_client,\n        db=db,\n    )\n\n\n@router.delete(\"/{auth_method}/{username}/messages\", status_code=HTTP_204_NO_CONTENT)\ndef mark_frontend_user_messages_deleted(\n    auth_method: str,\n    username: str,\n    api_client: ApiClient = Depends(deps.get_trusted_api_client),\n    db: Session = Depends(deps.get_db),\n):\n    pr = PromptRepository(db, api_client)\n    messages = pr.query_messages_ordered_by_created_date(\n        auth_method=auth_method,\n        username=username,\n        api_client_id=api_client.id,\n        limit=None,\n    )\n    pr.mark_messages_deleted(messages)\n", "backend/oasst_backend/api/v1/utils.py": "import re\nfrom uuid import UUID\n\nfrom oasst_backend.models import Message, MessageRevision\nfrom oasst_shared.schemas import protocol\n\n\ndef prepare_message(m: Message) -> protocol.Message:\n    return protocol.Message(\n        id=m.id,\n        frontend_message_id=m.frontend_message_id,\n        parent_id=m.parent_id,\n        user_id=m.user_id,\n        text=m.text,\n        lang=m.lang,\n        is_assistant=(m.role == \"assistant\"),\n        created_date=m.created_date,\n        emojis=m.emojis or {},\n        user_emojis=m.user_emojis or [],\n        user_is_author=m.user_is_author,\n        review_result=m.review_result,\n        review_count=m.review_count,\n        ranking_count=m.ranking_count,\n        deleted=m.deleted,\n        edited=m.edited,\n        synthetic=m.synthetic,\n        model_name=m.model_name,\n        message_tree_id=m.message_tree_id,\n        rank=m.rank,\n        user=m.user.to_protocol_frontend_user() if m.user else None,\n    )\n\n\ndef prepare_message_list(messages: list[Message]) -> list[protocol.Message]:\n    return [prepare_message(m) for m in messages]\n\n\ndef prepare_conversation_message(message: Message) -> protocol.ConversationMessage:\n    return protocol.ConversationMessage(\n        id=message.id,\n        user_id=message.user_id,\n        frontend_message_id=message.frontend_message_id,\n        text=message.text,\n        lang=message.lang,\n        is_assistant=(message.role == \"assistant\"),\n        emojis=message.emojis or {},\n        user_emojis=message.user_emojis or [],\n        user_is_author=message.user_is_author,\n        synthetic=message.synthetic,\n    )\n\n\ndef prepare_conversation_message_list(messages: list[Message]) -> list[protocol.ConversationMessage]:\n    return [prepare_conversation_message(message) for message in messages]\n\n\ndef prepare_conversation(messages: list[Message]) -> protocol.Conversation:\n    return protocol.Conversation(messages=prepare_conversation_message_list(messages))\n\n\ndef prepare_tree(tree: list[Message], tree_id: UUID) -> protocol.MessageTree:\n    tree_messages = []\n    for message in tree:\n        tree_messages.append(prepare_message(message))\n\n    return protocol.MessageTree(id=tree_id, messages=tree_messages)\n\n\ndef prepare_message_revision(revision: MessageRevision) -> protocol.MessageRevision:\n    return protocol.MessageRevision(\n        id=revision.id,\n        text=revision.payload.payload.text,\n        message_id=revision.message_id,\n        user_id=revision.user_id,\n        created_date=revision.created_date,\n        user_is_author=revision._user_is_author,\n    )\n\n\ndef prepare_message_revision_list(revisions: list[MessageRevision]) -> list[protocol.MessageRevision]:\n    return [prepare_message_revision(revision) for revision in revisions]\n\n\nsplit_uuid_pattern = re.compile(\n    r\"^([0-9a-fA-F]{8}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{12})\\$(.*)$\"\n)\n", "backend/oasst_backend/api/v1/tasks.py": "from typing import Any, Optional\nfrom uuid import UUID\n\nfrom fastapi import APIRouter, Depends\nfrom fastapi.security.api_key import APIKey\nfrom loguru import logger\nfrom oasst_backend.api import deps\nfrom oasst_backend.config import settings\nfrom oasst_backend.prompt_repository import PromptRepository, TaskRepository\nfrom oasst_backend.tree_manager import TreeManager\nfrom oasst_backend.user_repository import UserRepository\nfrom oasst_backend.utils.database_utils import CommitMode, async_managed_tx_function\nfrom oasst_shared.exceptions import OasstError, OasstErrorCode\nfrom oasst_shared.schemas import protocol as protocol_schema\nfrom sqlmodel import Session\nfrom starlette.status import HTTP_204_NO_CONTENT\n\nrouter = APIRouter()\n\n\n@router.post(\n    \"/\",\n    response_model=protocol_schema.AnyTask,\n    dependencies=[\n        Depends(\n            deps.UserRateLimiter(\n                times=settings.RATE_LIMIT_TASK_USER_TIMES,\n                minutes=settings.RATE_LIMIT_TASK_USER_MINUTES,\n            )\n        ),\n        Depends(\n            deps.APIClientRateLimiter(\n                times=settings.RATE_LIMIT_TASK_API_TIMES,\n                minutes=settings.RATE_LIMIT_TASK_API_MINUTES,\n            )\n        ),\n        Depends(\n            deps.UserTaskTypeRateLimiter(\n                [\n                    protocol_schema.TaskRequestType.assistant_reply,\n                ],\n                times=settings.RATE_LIMIT_ASSISTANT_USER_TIMES,\n                minutes=settings.RATE_LIMIT_ASSISTANT_USER_MINUTES,\n            )\n        ),\n        Depends(\n            deps.UserTaskTypeRateLimiter(\n                [\n                    protocol_schema.TaskRequestType.prompter_reply,\n                ],\n                times=settings.RATE_LIMIT_PROMPTER_USER_TIMES,\n                minutes=settings.RATE_LIMIT_PROMPTER_USER_MINUTES,\n            )\n        ),\n    ],\n)  # work with Union once more types are added\ndef request_task(\n    *,\n    db: Session = Depends(deps.get_db),\n    api_key: APIKey = Depends(deps.get_api_key),\n    request: protocol_schema.TaskRequest,\n) -> Any:\n    \"\"\"\n    Create new task.\n    \"\"\"\n    api_client = deps.api_auth(api_key, db)\n\n    try:\n        pr = PromptRepository(db, api_client, client_user=request.user)\n        pr.ensure_user_is_enabled()\n\n        tm = TreeManager(db, pr)\n        task, message_tree_id, parent_message_id = tm.next_task(desired_task_type=request.type, lang=request.lang)\n        pr.task_repository.store_task(task, message_tree_id, parent_message_id, request.collective)\n\n    except OasstError:\n        raise\n    except Exception:\n        logger.exception(\"Failed to generate task..\")\n        raise OasstError(\"Failed to generate task.\", OasstErrorCode.TASK_GENERATION_FAILED)\n    return task\n\n\n@router.post(\"/availability\", response_model=dict[protocol_schema.TaskRequestType, int])\ndef tasks_availability(\n    *,\n    user: Optional[protocol_schema.User] = None,\n    lang: Optional[str] = \"en\",\n    db: Session = Depends(deps.get_db),\n    api_key: APIKey = Depends(deps.get_api_key),\n):\n    api_client = deps.api_auth(api_key, db)\n\n    try:\n        pr = PromptRepository(db, api_client, client_user=user)\n        tm = TreeManager(db, pr)\n        return tm.determine_task_availability(lang)\n\n    except OasstError:\n        raise\n    except Exception:\n        logger.exception(\"Task availability query failed.\")\n        raise OasstError(\"Task availability query failed.\", OasstErrorCode.TASK_AVAILABILITY_QUERY_FAILED)\n\n\n@router.post(\"/{task_id}/ack\", response_model=None, status_code=HTTP_204_NO_CONTENT)\ndef tasks_acknowledge(\n    *,\n    db: Session = Depends(deps.get_db),\n    api_key: APIKey = Depends(deps.get_api_key),\n    frontend_user: deps.FrontendUserId = Depends(deps.get_frontend_user_id),\n    task_id: UUID,\n    ack_request: protocol_schema.TaskAck,\n) -> None:\n    \"\"\"\n    The frontend acknowledges a task.\n    \"\"\"\n\n    api_client = deps.api_auth(api_key, db)\n\n    try:\n        pr = PromptRepository(db, api_client, frontend_user=frontend_user)\n\n        # here we store the message id in the database for the task\n        logger.info(f\"Frontend ACK task_id={task_id}\")\n        logger.debug(f\"{ack_request=}.\")\n        pr.task_repository.bind_frontend_message_id(task_id=task_id, frontend_message_id=ack_request.message_id)\n\n    except OasstError:\n        raise\n    except Exception:\n        logger.exception(\"Failed to acknowledge task.\")\n        raise OasstError(\"Failed to acknowledge task.\", OasstErrorCode.TASK_ACK_FAILED)\n\n\n@router.post(\"/{task_id}/nack\", response_model=None, status_code=HTTP_204_NO_CONTENT)\ndef tasks_acknowledge_failure(\n    *,\n    db: Session = Depends(deps.get_db),\n    api_key: APIKey = Depends(deps.get_api_key),\n    frontend_user: deps.FrontendUserId = Depends(deps.get_frontend_user_id),\n    task_id: UUID,\n    nack_request: protocol_schema.TaskNAck,\n) -> None:\n    \"\"\"\n    The frontend reports failure to implement a task.\n    \"\"\"\n\n    try:\n        logger.info(f\"Frontend reports failure to implement task {task_id=}, {nack_request=}.\")\n        api_client = deps.api_auth(api_key, db)\n        pr = PromptRepository(db, api_client, frontend_user=frontend_user)\n        pr.skip_task(task_id=task_id, reason=nack_request.reason)\n    except (KeyError, RuntimeError):\n        logger.exception(\"Failed to not acknowledge task.\")\n        raise OasstError(\"Failed to not acknowledge task.\", OasstErrorCode.TASK_NACK_FAILED)\n\n\n@router.post(\"/interaction\", response_model=protocol_schema.TaskDone)\nasync def tasks_interaction(\n    *,\n    api_key: APIKey = Depends(deps.get_api_key),\n    interaction: protocol_schema.AnyInteraction,\n) -> Any:\n    \"\"\"\n    The frontend reports an interaction.\n    \"\"\"\n\n    @async_managed_tx_function(CommitMode.COMMIT)\n    async def interaction_tx(session: deps.Session):\n        api_client = deps.api_auth(api_key, session)\n        pr = PromptRepository(session, api_client, client_user=interaction.user)\n        tm = TreeManager(session, pr)\n        ur = UserRepository(session, api_client)\n        task = await tm.handle_interaction(interaction)\n        if type(task) is protocol_schema.TaskDone:\n            ur.update_user_last_activity(user=pr.user, update_streak=True)\n        return task\n\n    try:\n        return await interaction_tx()\n    except OasstError:\n        raise\n    except Exception:\n        logger.exception(\"Interaction request failed.\")\n        raise OasstError(\"Interaction request failed.\", OasstErrorCode.TASK_INTERACTION_REQUEST_FAILED)\n\n\n@router.post(\"/close\", response_model=protocol_schema.TaskDone)\ndef close_collective_task(\n    close_task_request: protocol_schema.TaskClose,\n    db: Session = Depends(deps.get_db),\n    api_key: APIKey = Depends(deps.get_api_key),\n):\n    api_client = deps.api_auth(api_key, db)\n    tr = TaskRepository(db, api_client)\n    tr.close_task(close_task_request.message_id)\n    return protocol_schema.TaskDone()\n", "backend/oasst_backend/api/v1/stats.py": "from fastapi import APIRouter, Depends\nfrom oasst_backend.api import deps\nfrom oasst_backend.cached_stats_repository import CachedStatsRepository\nfrom oasst_backend.models import ApiClient\nfrom oasst_backend.prompt_repository import PromptRepository\nfrom oasst_backend.tree_manager import TreeManager, TreeManagerStats, TreeMessageCountStats\nfrom oasst_backend.utils.database_utils import CommitMode, managed_tx_function\nfrom oasst_shared.schemas import protocol\nfrom sqlmodel import Session\nfrom starlette.status import HTTP_204_NO_CONTENT\n\nrouter = APIRouter()\n\n\n@router.get(\"/\", response_model=protocol.SystemStats)\ndef get_message_stats(\n    db: Session = Depends(deps.get_db),\n    api_client: ApiClient = Depends(deps.get_trusted_api_client),\n):\n    pr = PromptRepository(db, api_client)\n    return pr.get_stats()\n\n\n@router.get(\"/tree_manager/state_counts\", response_model=dict[str, int])\ndef get_tree_manager__state_counts(\n    db: Session = Depends(deps.get_db),\n    api_client: ApiClient = Depends(deps.get_trusted_api_client),\n):\n    pr = PromptRepository(db, api_client)\n    tm = TreeManager(db, pr)\n    return tm.tree_counts_by_state()\n\n\n@router.get(\"/tree_manager/message_counts\", response_model=list[TreeMessageCountStats])\ndef get_tree_manager__message_counts(\n    only_active: bool = True,\n    db: Session = Depends(deps.get_db),\n    api_client: ApiClient = Depends(deps.get_trusted_api_client),\n):\n    pr = PromptRepository(db, api_client)\n    tm = TreeManager(db, pr)\n    return tm.tree_message_count_stats(only_active=only_active)\n\n\n@router.get(\"/tree_manager\", response_model=TreeManagerStats)\ndef get_tree_manager__stats(\n    db: Session = Depends(deps.get_db),\n    api_client: ApiClient = Depends(deps.get_trusted_api_client),\n):\n    pr = PromptRepository(db, api_client)\n    tm = TreeManager(db, pr)\n    return tm.stats()\n\n\n@router.get(\"/cached/{name}\", response_model=protocol.CachedStatsResponse)\ndef get_cached_stats(\n    *,\n    name: protocol.CachedStatsName,\n    db: Session = Depends(deps.get_db),\n    api_client: ApiClient = Depends(deps.get_api_client),\n):\n    csr = CachedStatsRepository(db)\n    return csr.get_stats(name)\n\n\n@router.get(\"/cached\", response_model=protocol.AllCachedStatsResponse)\ndef get_cached_stats_all(\n    *,\n    db: Session = Depends(deps.get_db),\n    api_client: ApiClient = Depends(deps.get_api_client),\n):\n    csr = CachedStatsRepository(db)\n    return csr.get_stats_all()\n\n\n@router.post(\"/cached/update\", response_model=None, status_code=HTTP_204_NO_CONTENT)\ndef update_cached_stats(\n    *,\n    db: Session = Depends(deps.get_db),\n    api_client: ApiClient = Depends(deps.get_trusted_api_client),\n):\n    @managed_tx_function(CommitMode.COMMIT)\n    def update_tx(db: deps.Session) -> None:\n        csr = CachedStatsRepository(db)\n        csr.update_all_cached_stats()\n\n    update_tx()\n", "backend/oasst_backend/api/v1/login.py": "import aiohttp\nfrom fastapi import APIRouter, Depends, HTTPException, Request\nfrom oasst_backend import auth\nfrom oasst_backend.api import deps\nfrom oasst_backend.config import Settings\nfrom oasst_backend.models import Account\nfrom oasst_shared.exceptions.oasst_api_error import OasstError, OasstErrorCode\nfrom oasst_shared.schemas import protocol as protocol_schema\nfrom sqlmodel import Session\nfrom starlette.status import HTTP_401_UNAUTHORIZED\n\nrouter = APIRouter()\n\n\n@router.get(\"/discord\")\ndef login_discord(request: Request):\n    redirect_uri = f\"{get_callback_uri(request)}/discord\"\n    auth_url = f\"https://discord.com/api/oauth2/authorize?client_id={Settings.AUTH_DISCORD_CLIENT_ID}&redirect_uri={redirect_uri}&response_type=code&scope=identify\"\n    raise HTTPException(status_code=302, headers={\"location\": auth_url})\n\n\n@router.get(\"/callback/discord\", response_model=protocol_schema.Token)\nasync def callback_discord(\n    auth_code: str,\n    request: Request,\n    db: Session = Depends(deps.get_db),\n):\n    redirect_uri = f\"{get_callback_uri(request)}/discord\"\n\n    async with aiohttp.ClientSession(raise_for_status=True) as session:\n        # Exchange the auth code for a Discord access token\n        async with session.post(\n            \"https://discord.com/api/oauth2/token\",\n            data={\n                \"client_id\": Settings.AUTH_DISCORD_CLIENT_ID,\n                \"client_secret\": Settings.AUTH_DISCORD_CLIENT_SECRET,\n                \"grant_type\": \"authorization_code\",\n                \"code\": auth_code,\n                \"redirect_uri\": redirect_uri,\n                \"scope\": \"identify\",\n            },\n        ) as token_response:\n            token_response_json = await token_response.json()\n            access_token = token_response_json[\"access_token\"]\n\n        # Retrieve user's Discord information using access token\n        async with session.get(\n            \"https://discord.com/api/users/@me\", headers={\"Authorization\": f\"Bearer {access_token}\"}\n        ) as user_response:\n            user_response_json = await user_response.json()\n            discord_id = user_response_json[\"id\"]\n\n    account: Account = auth.get_account_from_discord_id(db, discord_id)\n\n    if not account:\n        # Discord account is not linked to an OA account\n        raise OasstError(\"Invalid authentication\", OasstErrorCode.INVALID_AUTHENTICATION, HTTP_401_UNAUTHORIZED)\n\n    # Discord account is valid and linked to an OA account -> create JWT\n    access_token = auth.create_access_token(account)\n\n    return protocol_schema.Token(access_token=access_token, token_type=\"bearer\")\n\n\ndef get_callback_uri(request: Request):\n    \"\"\"\n    Gets the URI for the base callback endpoint with no provider name appended.\n    \"\"\"\n    # This seems ugly, not sure if there is a better way\n    current_url = str(request.url)\n    domain = current_url.split(\"/api/v1/\")[0]\n    redirect_uri = f\"{domain}/api/v1/callback\"\n    return redirect_uri\n", "backend/oasst_backend/api/v1/text_labels.py": "from typing import Optional\nfrom uuid import UUID\n\nfrom fastapi import APIRouter, Depends, HTTPException\nfrom fastapi.security.api_key import APIKey\nfrom loguru import logger\nfrom oasst_backend.api import deps\nfrom oasst_backend.config import settings\nfrom oasst_backend.models import ApiClient\nfrom oasst_backend.prompt_repository import PromptRepository\nfrom oasst_backend.schemas.text_labels import LabelDescription, ValidLabelsResponse\nfrom oasst_backend.utils.database_utils import CommitMode, managed_tx_function\nfrom oasst_shared.exceptions import OasstError\nfrom oasst_shared.schemas import protocol as protocol_schema\nfrom oasst_shared.schemas.protocol import TextLabel\nfrom sqlmodel import Session\nfrom starlette.status import HTTP_204_NO_CONTENT, HTTP_400_BAD_REQUEST\n\nrouter = APIRouter()\n\n\n@router.post(\"/\", status_code=HTTP_204_NO_CONTENT)\ndef label_text(\n    *,\n    api_key: APIKey = Depends(deps.get_api_key),\n    text_labels: protocol_schema.TextLabels,\n) -> None:\n    \"\"\"\n    Label a piece of text.\n    \"\"\"\n\n    @managed_tx_function(CommitMode.COMMIT)\n    def store_text_labels(session: deps.Session):\n        api_client = deps.api_auth(api_key, session)\n        pr = PromptRepository(session, api_client, client_user=text_labels.user)\n        pr.store_text_labels(text_labels)\n\n    try:\n        logger.info(f\"Labeling text {text_labels=}.\")\n        store_text_labels()\n\n    except OasstError:\n        raise\n    except Exception:\n        logger.exception(\"Failed to store label.\")\n        raise HTTPException(\n            status_code=HTTP_400_BAD_REQUEST,\n        )\n\n\n@router.get(\"/valid_labels\")\ndef get_valid_lables(\n    *,\n    message_id: Optional[UUID] = None,\n    db: Session = Depends(deps.get_db),\n    api_client: ApiClient = Depends(deps.get_api_client),\n) -> ValidLabelsResponse:\n    if message_id:\n        pr = PromptRepository(db, api_client=api_client)\n        message = pr.fetch_message(message_id=message_id)\n        if message.parent_id is None:\n            valid_labels = settings.tree_manager.labels_initial_prompt\n        elif message.role == \"assistant\":\n            valid_labels = settings.tree_manager.labels_assistant_reply\n        else:\n            valid_labels = settings.tree_manager.labels_prompter_reply\n    else:\n        valid_labels = [l for l in TextLabel if l != TextLabel.fails_task]\n\n    return ValidLabelsResponse(\n        valid_labels=[\n            LabelDescription(name=l.value, widget=l.widget.value, display_text=l.display_text, help_text=l.help_text)\n            for l in valid_labels\n        ]\n    )\n\n\n@router.get(\"/report_labels\")\ndef get_report_lables() -> ValidLabelsResponse:\n    report_labels = [\n        TextLabel.spam,\n        TextLabel.not_appropriate,\n        TextLabel.pii,\n        TextLabel.hate_speech,\n        TextLabel.sexual_content,\n        TextLabel.moral_judgement,\n        TextLabel.political_content,\n        TextLabel.toxicity,\n        TextLabel.violence,\n        TextLabel.quality,\n    ]\n    return ValidLabelsResponse(\n        valid_labels=[\n            LabelDescription(name=l.value, widget=l.widget.value, display_text=l.display_text, help_text=l.help_text)\n            for l in report_labels\n        ]\n    )\n", "backend/oasst_backend/api/v1/leaderboards.py": "from typing import Optional\nfrom uuid import UUID\n\nfrom fastapi import APIRouter, Depends, Query\nfrom oasst_backend.api import deps\nfrom oasst_backend.models import ApiClient\nfrom oasst_backend.user_repository import UserRepository\nfrom oasst_backend.user_stats_repository import UserStatsRepository, UserStatsTimeFrame\nfrom oasst_shared.schemas.protocol import LeaderboardStats\nfrom sqlmodel import Session\nfrom starlette.status import HTTP_204_NO_CONTENT\n\nrouter = APIRouter()\n\n\n@router.get(\"/{time_frame}\", response_model=LeaderboardStats)\ndef get_leaderboard(\n    time_frame: UserStatsTimeFrame,\n    max_count: Optional[int] = Query(100, gt=0, le=10000),\n    frontend_user: deps.FrontendUserId = Depends(deps.get_frontend_user_id),\n    api_client: ApiClient = Depends(deps.get_api_client),\n    db: Session = Depends(deps.get_db),\n) -> LeaderboardStats:\n    current_user_id: UUID | None = None\n    if frontend_user.username:\n        ur = UserRepository(db, api_client)\n        current_user = ur.query_frontend_user(auth_method=frontend_user.auth_method, username=frontend_user.username)\n        current_user_id = current_user.id\n    usr = UserStatsRepository(db)\n    return usr.get_leaderboard(time_frame, limit=max_count, highlighted_user_id=current_user_id)\n\n\n@router.post(\"/update/{time_frame}\", response_model=None, status_code=HTTP_204_NO_CONTENT)\ndef update_leaderboard_time_frame(\n    time_frame: UserStatsTimeFrame,\n    api_client: ApiClient = Depends(deps.get_trusted_api_client),\n    db: Session = Depends(deps.get_db),\n) -> LeaderboardStats:\n    usr = UserStatsRepository(db)\n    return usr.update_stats(time_frame=time_frame)\n\n\n@router.post(\"/update\", response_model=None, status_code=HTTP_204_NO_CONTENT)\ndef update_leaderboards_all(\n    api_client: ApiClient = Depends(deps.get_trusted_api_client),\n    db: Session = Depends(deps.get_db),\n) -> LeaderboardStats:\n    usr = UserStatsRepository(db)\n    return usr.update_all_time_frames()\n", "backend/oasst_backend/api/v1/__init__.py": "", "backend/oasst_backend/api/v1/hugging_face.py": "from typing import List\n\nfrom fastapi import APIRouter, Depends\nfrom oasst_backend.api import deps\nfrom oasst_backend.models import ApiClient\nfrom oasst_backend.schemas.hugging_face import ToxicityClassification\nfrom oasst_backend.utils.hugging_face import HfClassificationModel, HfUrl, HuggingFaceAPI\n\nrouter = APIRouter()\n\n\n@router.get(\"/text_toxicity\")\nasync def get_text_toxicity(\n    msg: str,\n    api_client: ApiClient = Depends(deps.get_trusted_api_client),\n) -> List[List[ToxicityClassification]]:\n    \"\"\"Get the Message Toxicity from HuggingFace Roberta model.\n\n    Args:\n        msg (str): the message that we want to analyze.\n        api_client (ApiClient, optional): authentication of the user of the request.\n            Defaults to Depends(deps.get_trusted_api_client).\n\n    Returns:\n        ToxicityClassification: the score of toxicity of the message.\n    \"\"\"\n\n    api_url: str = HfUrl.HUGGINGFACE_TOXIC_CLASSIFICATION.value + \"/\" + HfClassificationModel.TOXIC_ROBERTA.value\n\n    hugging_face_api = HuggingFaceAPI(api_url)\n    response = await hugging_face_api.post(msg)\n\n    return response\n", "backend/oasst_backend/api/v1/messages.py": "from datetime import datetime\nfrom typing import Optional\nfrom uuid import UUID\n\nfrom fastapi import APIRouter, Depends, Query\nfrom loguru import logger\nfrom oasst_backend.api import deps\nfrom oasst_backend.api.v1 import utils\nfrom oasst_backend.models import ApiClient, MessageTreeState\nfrom oasst_backend.prompt_repository import PromptRepository\nfrom oasst_backend.schemas.message_tree import MessageTreeStateResponse\nfrom oasst_backend.tree_manager import TreeManager\nfrom oasst_backend.utils.database_utils import CommitMode, managed_tx_function\nfrom oasst_shared.exceptions.oasst_api_error import OasstError, OasstErrorCode\nfrom oasst_shared.schemas import protocol\nfrom sqlmodel import Session\nfrom starlette.status import HTTP_202_ACCEPTED, HTTP_204_NO_CONTENT\n\nrouter = APIRouter()\n\n\n@router.get(\"/\", response_model=list[protocol.Message])\ndef query_messages(\n    *,\n    auth_method: Optional[str] = None,\n    username: Optional[str] = None,\n    api_client_id: Optional[str] = None,\n    max_count: Optional[int] = Query(10, gt=0, le=1000),\n    start_date: Optional[datetime] = None,\n    end_date: Optional[datetime] = None,\n    only_roots: Optional[bool] = False,\n    desc: Optional[bool] = True,\n    allow_deleted: Optional[bool] = False,\n    lang: Optional[str] = None,\n    frontend_user: deps.FrontendUserId = Depends(deps.get_frontend_user_id),\n    api_client: ApiClient = Depends(deps.get_api_client),\n    db: Session = Depends(deps.get_db),\n):\n    \"\"\"\n    Query messages.\n    \"\"\"\n    pr = PromptRepository(db, api_client, auth_method=frontend_user.auth_method, username=frontend_user.username)\n    messages = pr.query_messages_ordered_by_created_date(\n        auth_method=auth_method,\n        username=username,\n        api_client_id=api_client_id,\n        desc=desc,\n        limit=max_count,\n        gte_created_date=start_date,\n        lte_created_date=end_date,\n        only_roots=only_roots,\n        deleted=None if allow_deleted else False,\n        lang=lang,\n    )\n\n    return utils.prepare_message_list(messages)\n\n\n@router.get(\"/cursor\", response_model=protocol.MessagePage)\ndef get_messages_cursor(\n    *,\n    before: Optional[str] = None,\n    after: Optional[str] = None,\n    user_id: Optional[UUID] = None,\n    auth_method: Optional[str] = None,\n    username: Optional[str] = None,\n    api_client_id: Optional[str] = None,\n    only_roots: Optional[bool] = False,\n    include_deleted: Optional[bool] = False,\n    max_count: Optional[int] = Query(10, gt=0, le=1000),\n    desc: Optional[bool] = False,\n    search_query: Optional[str] = None,\n    lang: Optional[str] = None,\n    include_user: Optional[bool] = None,\n    frontend_user: deps.FrontendUserId = Depends(deps.get_frontend_user_id),\n    api_client: ApiClient = Depends(deps.get_api_client),\n    db: Session = Depends(deps.get_db),\n):\n    assert max_count is not None\n\n    def split_cursor(x: str | None) -> tuple[datetime, UUID]:\n        if not x:\n            return None, None\n        try:\n            m = utils.split_uuid_pattern.match(x)\n            if m:\n                return datetime.fromisoformat(m[2]), UUID(m[1])\n            return datetime.fromisoformat(x), None\n        except ValueError:\n            raise OasstError(\"Invalid cursor value\", OasstErrorCode.INVALID_CURSOR_VALUE)\n\n    if desc:\n        gte_created_date, gt_id = split_cursor(before)\n        lte_created_date, lt_id = split_cursor(after)\n        query_desc = not (before is not None and not after)\n    else:\n        lte_created_date, lt_id = split_cursor(before)\n        gte_created_date, gt_id = split_cursor(after)\n        query_desc = before is not None and not after\n\n    logger.debug(f\"{desc=} {query_desc=} {gte_created_date=} {lte_created_date=}\")\n\n    qry_max_count = max_count + 1 if before is None or after is None else max_count\n\n    pr = PromptRepository(db, api_client, frontend_user=frontend_user)\n    items = pr.query_messages_ordered_by_created_date(\n        user_id=user_id,\n        auth_method=auth_method,\n        username=username,\n        api_client_id=api_client_id,\n        gte_created_date=gte_created_date,\n        gt_id=gt_id,\n        lte_created_date=lte_created_date,\n        lt_id=lt_id,\n        only_roots=only_roots,\n        deleted=None if include_deleted else False,\n        desc=query_desc,\n        limit=qry_max_count,\n        search_query=search_query,\n        lang=lang,\n        include_user=include_user,\n    )\n\n    num_rows = len(items)\n    if qry_max_count > max_count and num_rows == qry_max_count:\n        assert not (before and after)\n        items = items[:-1]\n\n    if desc != query_desc:\n        items.reverse()\n\n    items = utils.prepare_message_list(items)\n    n, p = None, None\n    if len(items) > 0:\n        if (num_rows > max_count and before) or after:\n            p = str(items[0].id) + \"$\" + items[0].created_date.isoformat()\n        if num_rows > max_count or before:\n            n = str(items[-1].id) + \"$\" + items[-1].created_date.isoformat()\n    else:\n        if after:\n            p = lte_created_date.isoformat() if desc else gte_created_date.isoformat()\n        if before:\n            n = gte_created_date.isoformat() if desc else lte_created_date.isoformat()\n\n    order = \"desc\" if desc else \"asc\"\n    return protocol.MessagePage(prev=p, next=n, sort_key=\"created_date\", order=order, items=items)\n\n\n@router.get(\"/{message_id}\", response_model=protocol.Message)\ndef get_message(\n    *,\n    message_id: UUID,\n    frontend_user: deps.FrontendUserId = Depends(deps.get_frontend_user_id),\n    api_client: ApiClient = Depends(deps.get_api_client),\n    db: Session = Depends(deps.get_db),\n):\n    \"\"\"\n    Get a message by its internal ID.\n    \"\"\"\n    pr = PromptRepository(db, api_client, frontend_user=frontend_user)\n    message = pr.fetch_message(message_id)\n    return utils.prepare_message(message)\n\n\n@router.get(\"/{message_id}/conversation\", response_model=protocol.Conversation)\ndef get_conv(\n    *,\n    message_id: UUID,\n    frontend_user: deps.FrontendUserId = Depends(deps.get_frontend_user_id),\n    api_client: ApiClient = Depends(deps.get_api_client),\n    db: Session = Depends(deps.get_db),\n):\n    \"\"\"\n    Get a conversation from the tree root and up to the message with given internal ID.\n    \"\"\"\n\n    pr = PromptRepository(db, api_client, frontend_user=frontend_user)\n    messages = pr.fetch_message_conversation(message_id)\n    return utils.prepare_conversation(messages)\n\n\n@router.get(\"/{message_id}/tree\", response_model=protocol.MessageTree)\ndef get_tree(\n    *,\n    message_id: UUID,\n    include_spam: Optional[bool] = True,\n    include_deleted: Optional[bool] = False,\n    frontend_user: deps.FrontendUserId = Depends(deps.get_frontend_user_id),\n    api_client: ApiClient = Depends(deps.get_api_client),\n    db: Session = Depends(deps.get_db),\n):\n    \"\"\"\n    Get all messages belonging to the same message tree.\n    \"\"\"\n    pr = PromptRepository(db, api_client, frontend_user=frontend_user)\n    message = pr.fetch_message(message_id)\n    review_result = None if include_spam else True\n    deleted = None if include_deleted else False\n    tree = pr.fetch_message_tree(message.message_tree_id, review_result=review_result, deleted=deleted)\n    return utils.prepare_tree(tree, message.message_tree_id)\n\n\n@router.get(\"/{message_id}/tree/state\", response_model=MessageTreeStateResponse)\ndef get_message_tree_state(\n    *,\n    message_id: UUID,\n    frontend_user: deps.FrontendUserId = Depends(deps.get_frontend_user_id),\n    api_client: ApiClient = Depends(deps.get_api_client),\n    db: Session = Depends(deps.get_db),\n) -> MessageTreeStateResponse:\n    pr = PromptRepository(db, api_client, frontend_user=frontend_user)\n    message = pr.fetch_message(message_id=message_id, fail_if_missing=True)\n    mts = pr.fetch_tree_state(message.message_tree_id)\n    return MessageTreeStateResponse(\n        message_tree_id=mts.message_tree_id,\n        state=mts.state,\n        active=mts.active,\n        goal_tree_size=mts.goal_tree_size,\n        max_children_count=mts.max_children_count,\n        max_depth=mts.max_depth,\n        origin=mts.origin,\n    )\n\n\n@router.put(\"/{message_id}/tree/state\", response_model=MessageTreeStateResponse)\ndef put_message_tree_state(\n    *,\n    message_id: UUID,\n    halt: bool,\n    frontend_user: deps.FrontendUserId = Depends(deps.get_frontend_user_id),\n    api_client: ApiClient = Depends(deps.get_trusted_api_client),\n) -> MessageTreeStateResponse:\n    @managed_tx_function(CommitMode.COMMIT)\n    def halt_tree_tx(session: deps.Session) -> MessageTreeState:\n        pr = PromptRepository(session, api_client, frontend_user=frontend_user)\n        tm = TreeManager(session, pr)\n        return tm.halt_tree(message_id, halt=halt)\n\n    mts = halt_tree_tx()\n    return MessageTreeStateResponse(\n        message_tree_id=mts.message_tree_id,\n        state=mts.state,\n        active=mts.active,\n        goal_tree_size=mts.goal_tree_size,\n        max_children_count=mts.max_children_count,\n        max_depth=mts.max_depth,\n        origin=mts.origin,\n    )\n\n\n@router.get(\"/{message_id}/children\", response_model=list[protocol.Message])\ndef get_children(\n    *,\n    message_id: UUID,\n    frontend_user: deps.FrontendUserId = Depends(deps.get_frontend_user_id),\n    api_client: ApiClient = Depends(deps.get_api_client),\n    db: Session = Depends(deps.get_db),\n):\n    \"\"\"\n    Get all messages belonging to the same message tree.\n    \"\"\"\n    pr = PromptRepository(db, api_client, frontend_user=frontend_user)\n    messages = pr.fetch_message_children(message_id, review_result=None)\n    return utils.prepare_message_list(messages)\n\n\n@router.get(\"/{message_id}/descendants\", response_model=protocol.MessageTree)\ndef get_descendants(\n    *,\n    message_id: UUID,\n    frontend_user: deps.FrontendUserId = Depends(deps.get_frontend_user_id),\n    api_client: ApiClient = Depends(deps.get_api_client),\n    db: Session = Depends(deps.get_db),\n):\n    \"\"\"\n    Get a subtree which starts with this message.\n    \"\"\"\n    pr = PromptRepository(db, api_client, frontend_user=frontend_user)\n    message = pr.fetch_message(message_id)\n    descendants = pr.fetch_message_descendants(message)\n    return utils.prepare_tree(descendants, message.id)\n\n\n@router.get(\"/{message_id}/longest_conversation_in_tree\", response_model=protocol.Conversation)\ndef get_longest_conv(\n    *,\n    message_id: UUID,\n    frontend_user: deps.FrontendUserId = Depends(deps.get_frontend_user_id),\n    api_client: ApiClient = Depends(deps.get_api_client),\n    db: Session = Depends(deps.get_db),\n):\n    \"\"\"\n    Get the longest conversation from the tree of the message.\n    \"\"\"\n    pr = PromptRepository(db, api_client, frontend_user=frontend_user)\n    message = pr.fetch_message(message_id)\n    conv = pr.fetch_longest_conversation(message.message_tree_id)\n    return utils.prepare_conversation(conv)\n\n\n@router.get(\"/{message_id}/max_children_in_tree\", response_model=protocol.MessageTree)\ndef get_max_children(\n    *,\n    message_id: UUID,\n    frontend_user: deps.FrontendUserId = Depends(deps.get_frontend_user_id),\n    api_client: ApiClient = Depends(deps.get_api_client),\n    db: Session = Depends(deps.get_db),\n):\n    \"\"\"\n    Get message with the most children from the tree of the provided message.\n    \"\"\"\n    pr = PromptRepository(db, api_client, frontend_user=frontend_user)\n    message = pr.fetch_message(message_id)\n    message, children = pr.fetch_message_with_max_children(message.message_tree_id)\n    return utils.prepare_tree([message, *children], message.id)\n\n\n@router.delete(\"/{message_id}\", status_code=HTTP_204_NO_CONTENT)\ndef mark_message_deleted(\n    *,\n    message_id: UUID,\n    frontend_user: deps.FrontendUserId = Depends(deps.get_frontend_user_id),\n    api_client: ApiClient = Depends(deps.get_trusted_api_client),\n    db: Session = Depends(deps.get_db),\n):\n    pr = PromptRepository(db, api_client, frontend_user=frontend_user)\n    pr.mark_messages_deleted(message_id)\n\n\n@router.put(\"/{message_id}/undelete\", status_code=HTTP_202_ACCEPTED, response_model=None)\ndef undelete_message(\n    *,\n    message_id: UUID,\n    frontend_user: deps.FrontendUserId = Depends(deps.get_frontend_user_id),\n    api_client: ApiClient = Depends(deps.get_api_client),\n    db: Session = Depends(deps.get_db),\n):\n    pr = PromptRepository(db, api_client, frontend_user=frontend_user)\n    pr.undelete_deleted_message(message_id)\n\n\n@router.post(\"/{message_id}/edit\")\ndef edit_message(\n    *,\n    message_id: UUID,\n    request: protocol.MessageEditRequest,\n    api_client: ApiClient = Depends(deps.get_trusted_api_client),\n):\n    @managed_tx_function(CommitMode.COMMIT)\n    def edit_tx(session: deps.Session):\n        pr = PromptRepository(session, api_client, client_user=request.user)\n        pr.revise_message(message_id, request.new_content)\n\n    edit_tx()\n\n\n@router.get(\"/{message_id}/history\", response_model=list[protocol.MessageRevision])\ndef get_revision_history(\n    *,\n    message_id: UUID,\n    frontend_user: deps.FrontendUserId = Depends(deps.get_frontend_user_id),\n    api_client: ApiClient = Depends(deps.get_trusted_api_client),\n    db: Session = Depends(deps.get_db),\n):\n    \"\"\"\n    Get all revisions of this message sorted from oldest to most recent\n    \"\"\"\n    pr = PromptRepository(db, api_client, frontend_user=frontend_user)\n    revisions = pr.fetch_message_revision_history(message_id)\n    return utils.prepare_message_revision_list(revisions)\n\n\n@router.post(\"/{message_id}/emoji\", status_code=HTTP_202_ACCEPTED)\ndef post_message_emoji(\n    *,\n    message_id: UUID,\n    request: protocol.MessageEmojiRequest,\n    api_client: ApiClient = Depends(deps.get_api_client),\n) -> protocol.Message:\n    \"\"\"\n    Toggle, add or remove message emoji.\n    \"\"\"\n\n    @managed_tx_function(CommitMode.COMMIT)\n    def emoji_tx(session: deps.Session):\n        pr = PromptRepository(session, api_client, client_user=request.user)\n        return pr.handle_message_emoji(message_id, request.op, request.emoji)\n\n    return utils.prepare_message(emoji_tx())\n", "backend/oasst_backend/api/v1/users.py": "import datetime\nfrom typing import Callable, Optional\nfrom uuid import UUID\n\nfrom fastapi import APIRouter, Depends, Query\nfrom oasst_backend.api import deps\nfrom oasst_backend.api.v1 import utils\nfrom oasst_backend.api.v1.messages import get_messages_cursor\nfrom oasst_backend.models import ApiClient, User\nfrom oasst_backend.prompt_repository import PromptRepository\nfrom oasst_backend.user_repository import UserRepository\nfrom oasst_backend.user_stats_repository import UserStatsRepository, UserStatsTimeFrame\nfrom oasst_shared.exceptions.oasst_api_error import OasstError, OasstErrorCode\nfrom oasst_shared.schemas import protocol\nfrom sqlmodel import Session\nfrom starlette.status import HTTP_204_NO_CONTENT\n\nrouter = APIRouter()\n\n\n@router.get(\"/by_username\", response_model=list[protocol.FrontEndUser])\ndef get_users_ordered_by_username(\n    api_client_id: Optional[UUID] = None,\n    gte_username: Optional[str] = None,\n    gt_id: Optional[UUID] = None,\n    lte_username: Optional[str] = None,\n    lt_id: Optional[UUID] = None,\n    search_text: Optional[str] = None,\n    auth_method: Optional[str] = None,\n    max_count: Optional[int] = Query(100, gt=0, le=10000),\n    desc: Optional[bool] = False,\n    api_client: ApiClient = Depends(deps.get_api_client),\n    db: Session = Depends(deps.get_db),\n):\n    ur = UserRepository(db, api_client)\n    users = ur.query_users_ordered_by_username(\n        api_client_id=api_client_id,\n        gte_username=gte_username,\n        gt_id=gt_id,\n        lte_username=lte_username,\n        lt_id=lt_id,\n        auth_method=auth_method,\n        search_text=search_text,\n        limit=max_count,\n        desc=desc,\n    )\n    return [u.to_protocol_frontend_user() for u in users]\n\n\n@router.get(\"/by_display_name\", response_model=list[protocol.FrontEndUser])\ndef get_users_ordered_by_display_name(\n    api_client_id: Optional[UUID] = None,\n    gte_display_name: Optional[str] = None,\n    gt_id: Optional[UUID] = None,\n    lte_display_name: Optional[str] = None,\n    lt_id: Optional[UUID] = None,\n    auth_method: Optional[str] = None,\n    search_text: Optional[str] = None,\n    max_count: Optional[int] = Query(100, gt=0, le=10000),\n    desc: Optional[bool] = False,\n    api_client: ApiClient = Depends(deps.get_api_client),\n    db: Session = Depends(deps.get_db),\n):\n    ur = UserRepository(db, api_client)\n    users = ur.query_users_ordered_by_display_name(\n        api_client_id=api_client_id,\n        gte_display_name=gte_display_name,\n        gt_id=gt_id,\n        lte_display_name=lte_display_name,\n        lt_id=lt_id,\n        auth_method=auth_method,\n        search_text=search_text,\n        limit=max_count,\n        desc=desc,\n    )\n    return [u.to_protocol_frontend_user() for u in users]\n\n\n@router.get(\"/cursor\", response_model=protocol.FrontEndUserPage)\ndef get_users_cursor(\n    before: Optional[str] = None,\n    after: Optional[str] = None,\n    sort_key: Optional[str] = Query(\"username\", max_length=32),\n    max_count: Optional[int] = Query(100, gt=0, le=10000),\n    api_client_id: Optional[UUID] = None,\n    search_text: Optional[str] = None,\n    auth_method: Optional[str] = None,\n    api_client: ApiClient = Depends(deps.get_api_client),\n    db: Session = Depends(deps.get_db),\n):\n    assert max_count is not None\n\n    def split_cursor(x: str | None) -> tuple[str, UUID]:\n        if not x:\n            return None, None\n        m = utils.split_uuid_pattern.match(x)\n        if m:\n            return m[2], UUID(m[1])\n        return x, None\n\n    items: list[protocol.FrontEndUser]\n    qry_max_count = max_count + 1 if before is None or after is None else max_count\n    desc = before is not None and not after\n\n    def get_next_prev(num_rows: int, lt: str | None, gt: str | None, key_fn: Callable[[protocol.FrontEndUser], str]):\n        p, n = None, None\n        if len(items) > 0:\n            if (num_rows > max_count and lt) or gt:\n                p = str(items[0].user_id) + \"$\" + key_fn(items[0])\n            if num_rows > max_count or lt:\n                n = str(items[-1].user_id) + \"$\" + key_fn(items[-1])\n        else:\n            if gt:\n                p = gt\n            if lt:\n                n = lt\n        return p, n\n\n    def remove_extra_item(items: list[protocol.FrontEndUser], lt: str | None, gt: str | None):\n        num_rows = len(items)\n        if qry_max_count > max_count and num_rows == qry_max_count:\n            assert not (lt is not None and gt is not None)\n            items = items[:-1]\n        if desc:\n            items.reverse()\n        return items, num_rows\n\n    n, p = None, None\n    if sort_key == \"username\":\n        lte_username, lt_id = split_cursor(before)\n        gte_username, gt_id = split_cursor(after)\n        items = get_users_ordered_by_username(\n            api_client_id=api_client_id,\n            gte_username=gte_username,\n            gt_id=gt_id,\n            lte_username=lte_username,\n            lt_id=lt_id,\n            auth_method=auth_method,\n            search_text=search_text,\n            max_count=qry_max_count,\n            desc=desc,\n            api_client=api_client,\n            db=db,\n        )\n        items, num_rows = remove_extra_item(items, lte_username, gte_username)\n        p, n = get_next_prev(num_rows, lte_username, gte_username, lambda x: x.id)\n\n    elif sort_key == \"display_name\":\n        lte_display_name, lt_id = split_cursor(before)\n        gte_display_name, gt_id = split_cursor(after)\n        items = get_users_ordered_by_display_name(\n            api_client_id=api_client_id,\n            gte_display_name=gte_display_name,\n            gt_id=gt_id,\n            lte_display_name=lte_display_name,\n            lt_id=lt_id,\n            auth_method=auth_method,\n            search_text=search_text,\n            max_count=qry_max_count,\n            desc=desc,\n            api_client=api_client,\n            db=db,\n        )\n        items, num_rows = remove_extra_item(items, lte_display_name, gte_display_name)\n        p, n = get_next_prev(num_rows, lte_display_name, gte_display_name, lambda x: x.display_name)\n\n    else:\n        raise OasstError(f\"Unsupported sort key: '{sort_key}'\", OasstErrorCode.SORT_KEY_UNSUPPORTED)\n\n    return protocol.FrontEndUserPage(prev=p, next=n, sort_key=sort_key, order=\"asc\", items=items)\n\n\n@router.get(\"/{user_id}\", response_model=protocol.FrontEndUser)\ndef get_user(\n    user_id: UUID,\n    api_client_id: UUID = None,\n    db: Session = Depends(deps.get_db),\n    api_client: ApiClient = Depends(deps.get_api_client),\n):\n    \"\"\"\n    Get a user by global user ID. Only trusted clients can resolve users they did not register.\n    \"\"\"\n    ur = UserRepository(db, api_client)\n    user: User = ur.get_user(user_id, api_client_id)\n    return user.to_protocol_frontend_user()\n\n\n@router.put(\"/{user_id}\", status_code=HTTP_204_NO_CONTENT)\ndef update_user(\n    user_id: UUID,\n    display_name: Optional[str] = None,\n    enabled: Optional[bool] = None,\n    notes: Optional[str] = None,\n    show_on_leaderboard: Optional[bool] = None,\n    tos_acceptance: Optional[bool] = None,\n    db: Session = Depends(deps.get_db),\n    api_client: ApiClient = Depends(deps.get_trusted_api_client),\n):\n    \"\"\"\n    Update a user by global user ID. Only trusted clients can update users.\n    \"\"\"\n    ur = UserRepository(db, api_client)\n    ur.update_user(user_id, display_name, enabled, notes, show_on_leaderboard, tos_acceptance)\n\n\n@router.delete(\"/{user_id}\", status_code=HTTP_204_NO_CONTENT)\ndef delete_user(\n    user_id: UUID,\n    db: Session = Depends(deps.get_db),\n    api_client: ApiClient = Depends(deps.get_trusted_api_client),\n):\n    \"\"\"\n    Delete a user by global user ID. Only trusted clients can delete users.\n    User deletion anonymises the data of the user.\n    \"\"\"\n    ur = UserRepository(db, api_client)\n    ur.mark_user_deleted(user_id)\n\n\n@router.get(\"/{user_id}/messages\", response_model=list[protocol.Message])\ndef query_user_messages(\n    user_id: UUID,\n    api_client_id: UUID = None,\n    max_count: int = Query(10, gt=0, le=1000),\n    start_date: datetime.datetime = None,\n    end_date: datetime.datetime = None,\n    only_roots: bool = False,\n    desc: bool = True,\n    include_deleted: bool = False,\n    lang: Optional[str] = None,\n    frontend_user: deps.FrontendUserId = Depends(deps.get_frontend_user_id),\n    api_client: ApiClient = Depends(deps.get_api_client),\n    db: Session = Depends(deps.get_db),\n):\n    \"\"\"\n    Query user messages.\n    \"\"\"\n    pr = PromptRepository(db, api_client, frontend_user=frontend_user)\n    messages = pr.query_messages_ordered_by_created_date(\n        user_id=user_id,\n        api_client_id=api_client_id,\n        desc=desc,\n        limit=max_count,\n        gte_created_date=start_date,\n        lte_created_date=end_date,\n        only_roots=only_roots,\n        deleted=None if include_deleted else False,\n        lang=lang,\n    )\n\n    return utils.prepare_message_list(messages)\n\n\n@router.get(\"/{user_id}/messages/cursor\", response_model=protocol.MessagePage)\ndef query_user_messages_cursor(\n    user_id: Optional[UUID],\n    before: Optional[str] = None,\n    after: Optional[str] = None,\n    only_roots: Optional[bool] = False,\n    include_deleted: Optional[bool] = False,\n    max_count: Optional[int] = Query(10, gt=0, le=1000),\n    desc: Optional[bool] = False,\n    lang: Optional[str] = None,\n    frontend_user: deps.FrontendUserId = Depends(deps.get_frontend_user_id),\n    api_client: ApiClient = Depends(deps.get_api_client),\n    db: Session = Depends(deps.get_db),\n):\n    return get_messages_cursor(\n        before=before,\n        after=after,\n        user_id=user_id,\n        only_roots=only_roots,\n        include_deleted=include_deleted,\n        max_count=max_count,\n        desc=desc,\n        lang=lang,\n        frontend_user=frontend_user,\n        api_client=api_client,\n        db=db,\n    )\n\n\n@router.delete(\"/{user_id}/messages\", status_code=HTTP_204_NO_CONTENT)\ndef mark_user_messages_deleted(\n    user_id: UUID,\n    frontend_user: deps.FrontendUserId = Depends(deps.get_frontend_user_id),\n    api_client: ApiClient = Depends(deps.get_trusted_api_client),\n    db: Session = Depends(deps.get_db),\n):\n    pr = PromptRepository(db, api_client, frontend_user=frontend_user)\n    messages = pr.query_messages_ordered_by_created_date(user_id=user_id, limit=None)\n    pr.mark_messages_deleted(messages)\n\n\n@router.get(\"/{user_id}/stats\", response_model=dict[str, protocol.UserScore | None])\ndef query_user_stats(\n    user_id: UUID,\n    api_client: ApiClient = Depends(deps.get_api_client),\n    db: Session = Depends(deps.get_db),\n):\n    usr = UserStatsRepository(db)\n    return usr.get_user_stats_all_time_frames(user_id=user_id)\n\n\n@router.get(\"/{user_id}/stats/{time_frame}\", response_model=protocol.UserScore)\ndef query_user_stats_timeframe(\n    user_id: UUID,\n    time_frame: UserStatsTimeFrame,\n    api_client: ApiClient = Depends(deps.get_api_client),\n    db: Session = Depends(deps.get_db),\n):\n    usr = UserStatsRepository(db)\n    return usr.get_user_stats_all_time_frames(user_id=user_id)[time_frame.value]\n\n\n@router.get(\"/{user_id}/stats/{time_frame}/window\", response_model=protocol.LeaderboardStats | None)\ndef query_user_stats_timeframe_window(\n    user_id: UUID,\n    time_frame: UserStatsTimeFrame,\n    window_size: Optional[int] = Query(5, gt=0, le=100),\n    api_client: ApiClient = Depends(deps.get_api_client),\n    db: Session = Depends(deps.get_db),\n) -> protocol.LeaderboardStats | None:\n    ur = UserRepository(db, api_client=api_client)\n    user = ur.get_user(id=user_id)\n    usr = UserStatsRepository(db)\n    return usr.get_leaderboard_user_window(user=user, time_frame=time_frame, window_size=window_size)\n", "inference/worker/download_model_hf.py": "import os\nimport signal\nimport sys\nfrom pathlib import Path\n\nimport huggingface_hub\n\n\ndef terminate(signum, frame):\n    print(\"Terminating...\")\n    sys.exit(0)\n\n\nif __name__ == \"__main__\":\n    signal.signal(signal.SIGINT, terminate)\n    model_id = os.getenv(\"MODEL_ID\")\n    snapshot_dir = Path(huggingface_hub.snapshot_download(model_id))\n    for file in snapshot_dir.rglob(\"*.json\"):\n        text = file.read_text()\n        text = text.replace(\"LLaMA\", \"Llama\")\n        file.write_text(text)\n", "inference/worker/chat_chain_utils.py": "import json\nimport re\nfrom typing import Callable\n\nimport requests\nimport transformers\nfrom chat_chain_prompts import INSTRUCTIONS, OBSERVATION_SEQ, TOOLS_PREFIX\nfrom hf_langchain_inference import HFInference\nfrom langchain.agents import Tool\nfrom langchain.memory import ConversationBufferMemory\nfrom langchain.prompts import PromptTemplate\nfrom loguru import logger\nfrom oasst_shared.schemas import inference\nfrom openapi_parser import prepare_plugin_for_llm\nfrom settings import settings\nfrom utils import shared_tokenizer_lock, special_tokens\n\nRESPONSE_MAX_LENGTH = 2048\nDESCRIPTION_FOR_MODEL_MAX_LENGTH = 512\n\nllm_json_parser = HFInference(\n    inference_server_url=settings.inference_server_url,\n    max_new_tokens=512,\n    stop_sequences=[special_tokens[\"end\"] if special_tokens[\"end\"] else \"</s>\"],\n    top_k=5,\n    temperature=0.20,\n    repetition_penalty=(1 / 0.83),\n)\n\n\n# This algo should be fine but possible improvements could be levenshtein or vector distance\ndef similarity(ts1: str, ts2: str) -> float:\n    \"\"\"Compute Jaro-Winkler distance between two strings.\"\"\"\n    if ts1 == ts2:\n        return 1\n\n    match = 0\n    len1, len2 = len(ts1), len(ts2)\n    max_dist = (max(len1, len2) // 2) - 1\n\n    hash_ts1 = [0] * len1\n    hash_ts2 = [0] * len2\n\n    for i in range(len1):\n        for j in range(max(0, i - max_dist), min(len2, i + max_dist + 1)):\n            if ts1[i] == ts2[j] and hash_ts2[j] == 0:\n                hash_ts1[i] = 1\n                hash_ts2[j] = 1\n                match += 1\n                break\n\n    if match == 0:\n        return 0\n\n    t = 0\n    point = 0\n\n    for i in range(len1):\n        if hash_ts1[i] == 1:\n            while hash_ts2[point] == 0:\n                point += 1\n            if ts1[i] != ts2[point]:\n                t += 1\n            point += 1\n\n    t /= 2\n    return (match / len1 + match / len2 + (match - t) / match) / 3.0\n\n\ndef extract_tool_and_input(llm_output: str, ai_prefix: str) -> tuple[str, str]:\n    \"\"\"\n    Extract tool name and tool input from LLM output. If LLM chose not to use a tool, `ai_prefix` is returned instead of tool name, and LLM output is returned instead of tool input.\n    \"\"\"\n    llm_output = llm_output.strip().replace(\"```\", \"\")\n    if f\"{ai_prefix}:\" in llm_output:\n        # No tool used, return LLM prefix and LLM output\n        return ai_prefix, llm_output.split(f\"{ai_prefix}:\")[-1].strip()\n\n    regex = r\"Action: (.*?)[\\n]*Action Input:\\n?(.*)\"\n    match = re.search(regex, llm_output, re.MULTILINE | re.DOTALL)\n    if not match:\n        if OBSERVATION_SEQ in llm_output:\n            return ai_prefix, llm_output.split(OBSERVATION_SEQ)[-1].strip()\n        return ai_prefix, llm_output\n\n    action = match.group(1)\n    action_input = match.group(2)\n    return action.strip().replace(\"'\", \"\"), action_input.strip().strip(\" \")\n\n\n# Truncate, but append closing bracket if string starts with [ or { or (\n# Helps prevent LLM from just generating output continuously\ndef truncate_str(output: str, max_length: int = 1024) -> str:\n    if len(output) > max_length:\n        if output[0] == \"(\":\n            return output[:max_length] + \"...)\"\n        elif output[0] == \"[\":\n            return output[:max_length] + \"...]\"\n        elif output[0] == \"{\":\n            return output[:max_length] + \"...}\"\n        else:\n            return output[:max_length] + \"...\"\n    return output\n\n\n# Parse JSON and try to fix it if invalid\ndef prepare_json(json_str: str) -> str:\n    json_str = json_str.strip()\n    fixed_json = json_str\n    try:\n        json.loads(json_str)\n    except json.decoder.JSONDecodeError:\n        # Fix missing quotes around keys and replace Python's True, False, and None\n        fixed_json = re.sub(r\"(?<=\\{|\\,)(\\s*)(\\w+)(\\s*):\", r'\\1\"\\2\"\\3:', json_str)\n        fixed_json = fixed_json.replace(\"True\", \"true\").replace(\"False\", \"false\").replace(\"None\", \"null\")\n\n        # Remove excessive closing braces/brackets\n        brace_count = bracket_count = 0\n        result = []\n        for c in fixed_json:\n            if c == \"{\":\n                brace_count += 1\n            elif c == \"}\":\n                brace_count -= 1\n            elif c == \"[\":\n                bracket_count += 1\n            elif c == \"]\":\n                bracket_count -= 1\n\n            if brace_count >= 0 and bracket_count >= 0:\n                result.append(c)\n        # Add missing closing braces/brackets\n        result.extend([\"}\"] * brace_count)\n        result.extend([\"]\"] * bracket_count)\n        fixed_json = \"\".join(result)\n\n        try:\n            json.loads(fixed_json)\n        except json.decoder.JSONDecodeError as e:\n            logger.warning(f\"JSON is still not valid, trying to fix it with LLM {fixed_json}\")\n            # If still invalid, try using LLM to fix\n            prompt = f\"\"\"{special_tokens['prompter']}Below is malformed JSON object string:\n--------------\n{json_str}\n--------------\nParsing error:\n--------------\n{e}\n\nRULES:\n1. If malformed JSON object string contains multiple objects, you merge them into one.\n2. You will never made up or add any new data, you will only fix the malformed JSON object string.\n\nHere is the fixed JSON object string:{special_tokens['end'] or '</s>'}{special_tokens['assistant']}\"\"\"\n            logger.warning(f\"JSON Fix Prompt: {prompt}\")\n            out = llm_json_parser.generate(prompts=[prompt]).generations[0][0].text\n            out = out[: out.find(\"}\") + 1]\n            logger.warning(f\"JSON Fix Output: {out}\")\n            return out\n\n    return fixed_json\n\n\ndef select_tool(tool_name: str, tools: list[Tool]) -> Tool | None:\n    tool = next((t for t in tools if t.name in tool_name), None)\n    if tool:\n        return tool\n    tool, tool_similarity = max(\n        ((t, similarity(t.name, tool_name)) for t in tools),\n        key=lambda x: x[1],\n        default=(None, 0),\n    )\n    # TODO: make stricter with better models\n    if tool and tool_similarity > 0.75:\n        return tool\n    return None\n\n\ndef use_tool(tool_name: str, tool_input: str, tools: list[Tool]) -> str:\n    tool = select_tool(tool_name, tools)\n    if not tool:\n        return f\"ERROR! {tool_name} is not a valid tool. Try again with different tool!\"\n    prepared_input = prepare_json(tool_input)\n    tool_output = tool.func(prepared_input)\n    return tool_output\n\n\n# Needs more work for errors, error-prompt tweaks are currently based on\n# `OpenAssistant/oasst-sft-6-llama-30b-epoch-1 model`\nclass RequestsForLLM:\n    def run(self, params: str, url: str, param_location: str, type: str, payload: str | None = None) -> str:\n        return self.run_request(params, url, param_location, type, payload)\n\n    def run_request(self, params: str, url: str, param_location: str, type: str, payload: str = None) -> str:\n        try:\n            query_params = params\n            if param_location == \"path\":\n                for key, value in query_params.items():\n                    url = url.replace(f\"{{{key}}}\", value)\n                query_params = {}\n\n            headers = {\"Content-Type\": \"application/json\"} if payload else None\n\n            if type.lower() == \"get\":\n                logger.info(\n                    f\"Running {type.upper()} request on {url} with\\nparams: {params}\\nparam_location: {param_location}\\npayload: {payload}\"\n                )\n                res = requests.get(url, params=query_params, headers=headers)\n            elif type.lower() == \"post\":\n                # if model did not generate payload object, use params as payload\n                data = json.dumps(payload) if payload else json.dumps(params)\n                logger.info(\n                    f\"Running {type.upper()} request on {url} with\\nparams: {params}\\nparam_location: {param_location}\\npayload: {data}\"\n                )\n                res = requests.post(url, params=query_params, data=data, headers=headers)\n            else:\n                return f\"ERROR! Unsupported request type: {type}. Only GET and POST are supported. Try again!\"\n\n            return self.process_response(res)\n        except Exception as e:\n            return f\"ERROR! That didn't work, try modifying Action Input.\\n{e}. Try again!\"\n\n    def process_response(self, res: requests.Response) -> str:\n        logger.info(f\"Request response: {res.text}\")\n        if res.status_code != 200:\n            return f\"ERROR! Please modify Action Input. according to this error message: \\n{res.text}. Try again!\"\n\n        if res.text is None or len(res.text) == 0:\n            return \"ERROR! That didn't work, try modifying Action Input.\\nEmpty response. Try again!\"\n\n        if \"null\" in res.text.lower() and len(res.text) < 10:\n            return \"ERROR! That didn't work, try modifying Action Input.\\nEmpty response. Try again!\"\n\n        return truncate_str(res.text, RESPONSE_MAX_LENGTH)\n\n\ndef compose_tools_from_plugin(plugin: inference.PluginEntry | None) -> tuple[str, list[Tool]]:\n    if not plugin:\n        return \"\", []\n\n    llm_plugin: inference.PluginConfig = prepare_plugin_for_llm(plugin.url)\n    if not llm_plugin:\n        return \"\", []\n\n    tools = []\n    request_tool = RequestsForLLM()\n\n    def create_tool_func(endpoint: inference.PluginOpenAPIEndpoint, param_location: str) -> Callable[..., str]:\n        def func(req) -> str:\n            try:\n                json_obj = json.loads(req)\n                request = json_obj.get(\"request\", {})\n                params = request.get(\"params\", {})\n                payload = request.get(\"payload\", None)\n            except json.JSONDecodeError:\n                print(\"Error: Invalid JSON input\")\n                request, params, payload = {}, {}, None\n            except Exception as e:\n                print(f\"Error: {e}\")\n                request, params, payload = {}, {}, None\n\n            return request_tool.run(\n                url=endpoint.url, params=params, param_location=param_location, type=endpoint.type, payload=payload\n            )\n\n        return func\n\n    # Generate tool for each plugin endpoint. Helps LLM use tools as it does not choose API server URL etc on its own\n    # LLM only chooses endpoint, parameters and values to use. Modifying this can degrade or improve tool usage\n    for endpoint in llm_plugin.endpoints:\n        params = \"\\n\\n\".join(\n            [\n                f\"\"\" name: \"{param.name}\",\\n in: \"{param.in_}\",\\n description: \"{truncate_str(param.description, 128)}\",\\n schema: {param.schema_},\\n required: {param.required}\"\"\"\n                for param in endpoint.params\n            ]\n        )\n\n        # LangChain uses {input_name} for templating\n        # Some plugins can have {some_word} in their description\n        params = params.replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n        payload_description = \"\"\n        if endpoint.payload:\n            try:\n                payload_description = \"payload: \" + truncate_str(json.dumps(endpoint.payload, indent=4), 256)\n                payload_description = payload_description.replace(\"{\", \"{{\").replace(\"}\", \"}}\")\n            except Exception as e:\n                logger.warning(f\"Failed to convert payload to json string: {e}\")\n\n        payload_description += \"\" if not payload_description or payload_description.endswith(\"\\n\") else \"\\n\"\n        if len(payload_description) > 0:\n            payload_description = \"\\n\" + payload_description + \"\\n\"\n\n        parameters_description = f\"params:\\n{params}\\n\" if params else \"\\n\"\n\n        openapi_specification_title = (\n            \"\\nOpenAPI specification\\n\" if len(payload_description) > 0 or len(params) > 0 else \"\"\n        )\n\n        param_location = endpoint.params[0].in_ if len(endpoint.params) > 0 else \"query\"\n\n        # If plugin has no operation_id, use path as fallback\n        path = endpoint.path[1:] if endpoint.path and len(endpoint.path) > 0 else endpoint.path\n\n        tool = Tool(\n            name=endpoint.operation_id if endpoint.operation_id != \"\" else path,\n            # Could be path, e.g /api/v1/endpoint but can lead LLM to invent URLs\n            # Problem with EP description is that it is too long for some plugins\n            func=create_tool_func(endpoint, param_location),\n            description=f\"{openapi_specification_title}{parameters_description}{payload_description}tool description: {endpoint.summary}\\n\",\n        )\n        tools.append(tool)\n\n    tools_string = \"\\n\".join([f\"> {tool.name}{tool.description}\" for tool in tools])\n    # This can be long for some plugins, we need to truncate due to ctx limitations\n    plugin_description_for_model = truncate_str(llm_plugin.description_for_model, DESCRIPTION_FOR_MODEL_MAX_LENGTH)\n    return (\n        f\"{TOOLS_PREFIX}{tools_string}\\n\\n{llm_plugin.name_for_model} plugin description:\\n{plugin_description_for_model}\\n\\n{INSTRUCTIONS}\",\n        tools,\n    )\n\n\ndef prepare_prompt(\n    input_prompt: str,\n    prompt_template: PromptTemplate,\n    memory: ConversationBufferMemory,\n    tools_names: list[str] | None,\n    current_time: str,\n    language: str,\n    tokenizer: transformers.PreTrainedTokenizer,\n    worker_config: inference.WorkerConfig,\n    action_input_format: str,\n    custom_instructions: str = \"\",\n) -> str:\n    max_input_length = worker_config.model_config.max_input_length\n\n    args = {\n        \"input\": input_prompt,\n        \"language\": language,\n        \"current_time\": current_time,\n        \"chat_history\": memory.buffer,\n        \"custom_instructions\": custom_instructions,\n    }\n\n    if tools_names:\n        args[\"tools_names\"] = tools_names\n        args[\"action_input_format\"] = action_input_format\n\n    out_prompt = prompt_template.format(**args)\n\n    with shared_tokenizer_lock:\n        ids = tokenizer.encode(out_prompt)\n\n    # soft truncation (delete whole messages)\n    while len(ids) > max_input_length and len(memory.chat_memory.messages) > 0:\n        memory.chat_memory.messages.pop(0)\n        args = {\n            \"input\": input_prompt,\n            \"language\": language,\n            \"current_time\": current_time,\n            \"chat_history\": memory.buffer,\n            \"custom_instructions\": custom_instructions,\n        }\n\n        if tools_names:\n            args[\"tools_names\"] = tools_names\n            args[\"action_input_format\"] = action_input_format\n\n        out_prompt = prompt_template.format(**args)\n\n        with shared_tokenizer_lock:\n            ids = tokenizer.encode(out_prompt)\n        logger.warning(f\"Prompt too long, deleting chat history. New length: {len(ids)}\")\n\n    return out_prompt\n", "inference/worker/get_model_config_prop.py": "import sys\n\nfrom oasst_shared import model_configs\nfrom settings import settings\n\nif __name__ == \"__main__\":\n    model_config = model_configs.MODEL_CONFIGS.get(settings.model_config_name)\n    if model_config is None:\n        print(f\"Unknown model config name: {settings.model_config_name}\")\n        sys.exit(2)\n    if len(sys.argv) != 2:\n        print(\"Usage: get_model_config_prop.py <property>\")\n        sys.exit(2)\n    prop = sys.argv[1]\n    if not hasattr(model_config, prop):\n        print(f\"Unknown property: {prop}\")\n        sys.exit(2)\n    val = getattr(model_config, prop)\n    if isinstance(val, bool):\n        val = str(val).lower()\n    print(val, end=\"\")\n", "inference/worker/openapi_parser.py": "import json\nfrom urllib.parse import urlsplit\n\nimport requests\nimport yaml\nfrom loguru import logger\nfrom oasst_shared.schemas import inference\n\n\ndef fetch_openapi_spec(url):\n    response = requests.get(url)\n    if response.status_code != 200:\n        raise Exception(f\"Failed to fetch data from URL: {url}. Status code: {response.status_code}\")\n\n    content_type = response.headers.get(\"Content-Type\")\n\n    if \"application/json\" in content_type or url.endswith(\".json\"):\n        return json.loads(response.text)\n    elif (\n        \"application/yaml\" in content_type\n        or \"application/x-yaml\" in content_type\n        or url.endswith(\".yaml\")\n        or url.endswith(\".yml\")\n    ):\n        return yaml.safe_load(response.text)\n    else:\n        raise Exception(f\"Unsupported content type: {content_type}. Only JSON and YAML are supported.\")\n\n\ndef get_plugin_config(url: str) -> inference.PluginConfig | None:\n    try:\n        response = requests.get(url)\n        response.raise_for_status()\n        plugin_dict = response.json()\n        logger.info(f\"Plugin config downloaded {plugin_dict}\")\n        plugin_config = inference.PluginConfig.parse_obj(plugin_dict)\n        return plugin_config\n    except (requests.RequestException, ValueError) as e:\n        logger.warning(f\"Error downloading or parsing Plugin config: {e}\")\n        return None\n\n\ndef resolve_schema_reference(ref: str, openapi_dict: dict):\n    if not ref.startswith(\"#/\"):\n        raise ValueError(f\"Invalid reference format: {ref}\")\n\n    components = ref.split(\"/\")\n    schema = openapi_dict\n    for component in components[1:]:\n        if component not in schema:\n            raise ValueError(f\"Reference component not found: {component}\")\n        schema = schema[component]\n\n    return schema\n\n\ndef parse_plugin_endpoint(\n    api_url: str,\n    method: str,\n    details: dict,\n    base_url: str,\n    path: str,\n    openapi_dict: dict,\n) -> inference.PluginOpenAPIEndpoint:\n    \"\"\"\n    Parse details of a single plugin endpoint from OpenAPI spec.\n\n    Args:\n        api_url: URL of the plugin API.\n        method: HTTP method of the endpoint.\n        details: Details of the endpoint from OpenAPI spec.\n        base_url: Base URL of the plugin.\n        path: Path of the endpoint.\n        openapi_dict: Full OpenAPI spec of the plugin.\n    \"\"\"\n    split_result = urlsplit(api_url)\n    backup_url = f\"{split_result.scheme}://{split_result.netloc}\"\n    params_list = []\n    parameters = details.get(\"parameters\", [])\n    if parameters is not None:\n        for param in parameters:\n            schema = None\n            if \"$ref\" in param[\"schema\"]:\n                schema = resolve_schema_reference(param[\"schema\"][\"$ref\"], openapi_dict)\n\n            params_list.append(\n                inference.PluginOpenAPIParameter(\n                    name=param.get(\"name\", \"\"),\n                    in_=param.get(\"in\", \"query\"),\n                    description=param.get(\"description\", \"\"),\n                    required=param.get(\"required\", False),\n                    schema_=schema,\n                )\n            )\n    # Check if the method is POST and extract request body schema\n    payload = None\n    if \"requestBody\" in details:\n        content = details[\"requestBody\"].get(\"content\", {})\n        for media_type, media_schema in content.items():\n            if media_type == \"application/json\":\n                if \"$ref\" in media_schema[\"schema\"]:\n                    payload = resolve_schema_reference(media_schema[\"schema\"][\"$ref\"], openapi_dict)\n                else:\n                    payload = media_schema[\"schema\"]\n\n    endpoint_data = {\n        \"type\": method,\n        \"summary\": details.get(\"summary\", \"\"),\n        \"operation_id\": details.get(\"operationId\", \"\"),\n        \"url\": f\"{base_url}{path}\" if base_url is not None else f\"{backup_url}{path}\",\n        \"path\": path,\n        \"params\": params_list,\n        \"payload\": payload,\n    }\n\n    if \"tags\" in details:\n        tag_name = details[\"tags\"][0]\n        endpoint_data[\"tag\"] = tag_name\n\n    endpoint = inference.PluginOpenAPIEndpoint(**endpoint_data)\n    return endpoint\n\n\ndef get_plugin_endpoints(api_url: str, openapi_dict: dict) -> list[inference.PluginOpenAPIEndpoint]:\n    endpoints = []\n    base_url = openapi_dict.get(\"servers\", [{}])[0].get(\"url\")\n\n    if base_url is not None:\n        parsed_link = urlsplit(api_url)\n        base_url = (\n            f\"{parsed_link.scheme}://{parsed_link.netloc}{base_url}\" if not urlsplit(base_url).scheme else base_url\n        )\n\n    for path, methods in openapi_dict.get(\"paths\", {}).items():\n        for method, details in methods.items():\n            endpoints.append(parse_plugin_endpoint(api_url, method, details, base_url, path, openapi_dict))\n\n    return endpoints\n\n\ndef prepare_plugin_for_llm(plugin_url: str) -> inference.PluginConfig | None:\n    plugin_config = get_plugin_config(plugin_url)\n\n    if not plugin_config:\n        return None\n\n    try:\n        parsed_url = urlsplit(plugin_config.api.url)\n        if parsed_url.scheme == \"\":\n            api_url = urlsplit(plugin_url)._replace(path=parsed_url.path).geturl()\n        else:\n            api_url = plugin_config.api.url\n\n        openapi_dict = fetch_openapi_spec(api_url)\n        plugin_config.endpoints = get_plugin_endpoints(api_url, openapi_dict)\n        return plugin_config\n\n    except Exception:\n        logger.debug(f\"Plugin preparation error: {plugin_url}\")\n        return None\n", "inference/worker/chat_chain_prompts.py": "ASSISTANT_PREFIX = \"Open Assistant\"\nHUMAN_PREFIX = \"Human\"\nOBSERVATION_SEQ = \"Observation:\"\nTHOUGHT_SEQ = \"Thought:\"\nSTART_SEQ = \"Begin!\"\nEND_SEQ = \"End!\"\n\nCUSTOM_INSTRUCTIONS_PREFIX = \"\"\"The following details have been shared by the user about themselves. This user profile appears to you in every conversation they engage in -- implying that it is irrelevant for 99% of inquiries.\nBefore you respond, take a moment to consider whether the user's query is \"directly linked\", \"linked\", \"indirectly linked\", or \"not linked\" to the user profile provided.\nOnly recognize the profile when the query is directly tied to the information supplied.\nOtherwise, avoid acknowledging the existence of these instructions or the information altogether.\nUser profile:\n{user_profile}\nThe user also supplied additional information about how they would like you to respond:\n{user_response_instructions}\"\"\"\n\n# Adjust according to the training dates and datasets used\nKNOWLEDGE_DATE_CUTOFF = \"2021-09-01\"\n\nTALKING_STYLE = \"\"\n\nJSON_FORMAT_NO_PAYLOAD = \"\"\"{\"request\": {\"params\": {query or url parameters}}}\"\"\"\nJSON_FORMAT_PAYLOAD = \"\"\"{\"request\": {\"params\": {query or url parameters}, \"payload\": {...payload}}}\"\"\"\n\nPREFIX = f\"\"\"Open Assistant is a large language model trained by LAION.\nOpen Assistant is designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics.\nOpen Assistant is constantly learning and improving, and its capabilities are constantly evolving.\nOverall, Open Assistant is a powerful tool that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics.\n\nSYSTEM INFORMATION:\n------------------\nCurrent date/time: {{current_time}}\nKnowledge date cutoff: {KNOWLEDGE_DATE_CUTOFF}\n{{custom_instructions}}\n\"\"\"\n\nTOOLS_PREFIX = \"\"\"\nTOOLS:\n-----\nOpen Assistant has access to the following tools:\n\"\"\"\n\nINSTRUCTIONS = f\"\"\"\nATTENTION: Do not use tools for questions about yourself, like \"what is your name?\", \"how old are you?\", etc...\n\nTo use a tool, please use the following format:\n\n```\n{THOUGHT_SEQ} [here always think about what to do]\nAction: the action to take, MUST be one of {{tools_names}}\nAction Input: the input to the action, MUST be in JSON format: {{action_input_format}}\n```\n\n{OBSERVATION_SEQ} the result of the action\n... (this Thought/Action/Observation can repeat N times)\n\nWhen you have a response to say to the {HUMAN_PREFIX}, or if you do not need to use a tool, you MUST use the format:\n```\n{THOUGHT_SEQ} I now know the final answer\n{ASSISTANT_PREFIX}: [my response here]{END_SEQ}\n```\n\"\"\"\n\nSUFFIX = f\"\"\"\n{START_SEQ}\n\nPrevious conversation history:\n{{chat_history}}\n\nWhen answering a question, you MUST use the following language: {{language}}{TALKING_STYLE}\nNew input: {{input}}\n\"\"\"\n", "inference/worker/hf_stopping.py": "import torch\nfrom tokenizers import Tokenizer\nfrom transformers import StoppingCriteria\n\n\nclass SequenceStoppingCriteria(StoppingCriteria):\n    \"\"\"Enables automatic stopping of model text generation when specific text sequences are generated.\"\"\"\n\n    def __init__(\n        self,\n        tokenizer: Tokenizer,\n        stop_texts: list[str],\n        input_prompt: str,\n        *args,\n        **kwargs,\n    ):\n        super().__init__(*args, **kwargs)\n        self.stop_texts = stop_texts\n        self.tokenizer = tokenizer\n        self.input_length = len(tokenizer.encode(input_prompt))\n\n    def __call__(\n        self,\n        input_ids: torch.LongTensor,\n        scores: torch.FloatTensor,\n        **kwargs,\n    ) -> bool:\n        # Assumes batch size 1, sufficient for our use case\n        generated_ids = input_ids[0, self.input_length :].tolist()\n        # TODO: optimise this. Inefficient to decode whole sequence every time\n        # but can't encode stop sequences as they don't always tokenize the same\n        generated_text = self.tokenizer.decode(generated_ids)\n        return any(text in generated_text for text in self.stop_texts)\n", "inference/worker/chat_chain.py": "import datetime\n\nimport interface\nimport transformers\nimport utils\nimport websocket\nfrom chat_chain_prompts import (\n    ASSISTANT_PREFIX,\n    CUSTOM_INSTRUCTIONS_PREFIX,\n    HUMAN_PREFIX,\n    JSON_FORMAT_NO_PAYLOAD,\n    JSON_FORMAT_PAYLOAD,\n    OBSERVATION_SEQ,\n    PREFIX,\n    SUFFIX,\n    THOUGHT_SEQ,\n)\nfrom chat_chain_utils import compose_tools_from_plugin, extract_tool_and_input, prepare_prompt, use_tool\nfrom hf_langchain_inference import HFInference\nfrom langchain.agents import Tool\nfrom langchain.memory import ConversationBufferMemory\nfrom langchain.prompts import PromptTemplate\nfrom loguru import logger\nfrom oasst_shared.model_configs import ModelConfig\nfrom oasst_shared.schemas import inference\nfrom settings import settings\nfrom utils import special_tokens\n\n# Exclude tools description from final prompt. Saves ctx space but can hurt output\n# quality especially if truncation kicks in. Dependent on model used\nREMOVE_TOOLS_FROM_FINAL_PROMPT = False\n\nllm = HFInference(\n    inference_server_url=settings.inference_server_url,\n    max_new_tokens=512,\n    stop_sequences=[],\n    top_k=50,\n    temperature=0.20,\n    seed=43,\n    repetition_penalty=(1 / 0.92),  # Best with > 0.88\n)\n\n\nclass PromptedLLM:\n    \"\"\"\n    Handles calls to an LLM via LangChain with a prompt template and memory.\n    \"\"\"\n\n    def __init__(\n        self,\n        tokenizer: transformers.PreTrainedTokenizer,\n        worker_config: inference.WorkerConfig,\n        parameters: interface.GenerateStreamParameters,\n        prompt_template: PromptTemplate,\n        memory: ConversationBufferMemory,\n        tool_names: list[str],\n        language: str,\n        action_input_format: str,\n        custom_instructions: str = \"\",\n    ):\n        self.tokenizer = tokenizer\n        self.worker_config = worker_config\n        self.parameters = parameters\n        self.prompt_template = prompt_template\n        self.memory = memory\n        self.tool_names = tool_names\n        self.language = language\n        self.action_input_format = action_input_format\n        self.current_time = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n        self.custom_instructions = custom_instructions\n\n    def call(self, prompt: str) -> tuple[str, str]:\n        \"\"\"Prepares and truncates prompt, calls LLM, returns used prompt and response.\"\"\"\n        prompt = prepare_prompt(\n            prompt,\n            self.prompt_template,\n            self.memory,\n            self.tool_names,\n            self.current_time,\n            self.language,\n            self.tokenizer,\n            self.worker_config,\n            self.action_input_format,\n            self.custom_instructions,\n        )\n\n        # We do not strip() outputs as it seems to degrade instruction-following abilities of the model\n        prompt = utils.truncate_prompt(self.tokenizer, self.worker_config, self.parameters, prompt, True)\n\n        response = (\n            llm.generate(prompts=[prompt], stop=[ASSISTANT_PREFIX, OBSERVATION_SEQ, f\"\\n{OBSERVATION_SEQ}\"])\n            .generations[0][0]\n            .text\n        )\n\n        if response:\n            response = response.replace(\"\\n\\n\", \"\\n\")\n            if response[0] != \"\\n\":\n                response = f\"\\n{response}\"\n\n        return prompt, response\n\n\ndef handle_plugin_usage(\n    input_prompt: str,\n    prompt_template: PromptTemplate,\n    language: str,\n    memory: ConversationBufferMemory,\n    worker_config: inference.WorkerConfig,\n    tokenizer: transformers.PreTrainedTokenizer,\n    parameters: interface.GenerateStreamParameters,\n    tools: list[Tool],\n    plugin: inference.PluginEntry | None,\n    plugin_max_depth: int,\n    ws: websocket.WebSocket,\n    work_request_id: str,\n    custom_instructions: str = \"\",\n) -> tuple[str, inference.PluginUsed]:\n    execution_details = inference.PluginExecutionDetails(\n        inner_monologue=[],\n        final_tool_output=\"\",\n        final_prompt=\"\",\n        final_generation_assisted=False,\n        error_message=\"\",\n        status=\"failure\",\n    )\n    plugin_used = inference.PluginUsed(\n        name=None,\n        url=None,\n        execution_details=execution_details,\n    )\n\n    if plugin is None:\n        return input_prompt, plugin_used\n\n    chain_finished = False\n    achieved_depth = 0\n    assisted = False\n    inner_prompt = \"\"\n    inner_monologue = []\n\n    action_input_format = (\n        JSON_FORMAT_PAYLOAD if prompt_template.template.find(\"payload\") != -1 else JSON_FORMAT_NO_PAYLOAD\n    )\n    eos_token = \"\"\n    if special_tokens[\"end\"]:\n        eos_token = special_tokens[\"end\"]\n    elif hasattr(tokenizer, \"eos_token\"):\n        eos_token = tokenizer.eos_token\n    tool_names = [tool.name for tool in tools]\n\n    chain = PromptedLLM(\n        tokenizer,\n        worker_config,\n        parameters,\n        prompt_template,\n        memory,\n        tool_names,\n        language,\n        action_input_format,\n        custom_instructions,\n    )\n\n    # send \"thinking...\" intermediate step to UI (This will discard queue position 0) immediately\n    utils.send_response(\n        ws,\n        inference.PluginIntermediateResponse(\n            request_id=work_request_id,\n            current_plugin_thought=\"thinking...\",\n            current_plugin_action_taken=\"\",\n            current_plugin_action_input=\"\",\n            current_plugin_action_response=\"\",\n        ),\n    )\n\n    init_prompt = f\"{input_prompt}{eos_token}{special_tokens['assistant']}\"\n    init_prompt, chain_response = chain.call(init_prompt)\n\n    inner_monologue.append(\"In: \" + str(init_prompt))\n    inner_monologue.append(\"Out: \" + str(chain_response))\n\n    current_action_thought = \"\"\n    if THOUGHT_SEQ in chain_response:\n        current_action_thought = chain_response.split(THOUGHT_SEQ)[1].split(\"\\n\")[0]\n\n    # Tool name/assistant prefix, Tool input/assistant response\n    prefix, response = extract_tool_and_input(llm_output=chain_response, ai_prefix=ASSISTANT_PREFIX)\n    assisted = False if ASSISTANT_PREFIX in prefix else True\n    chain_finished = not assisted\n\n    if assisted:\n        # model decided to use a tool, so send that thought to the client\n        utils.send_response(\n            ws,\n            inference.PluginIntermediateResponse(\n                request_id=work_request_id,\n                current_plugin_thought=current_action_thought,\n                current_plugin_action_taken=prefix,\n                current_plugin_action_input=chain_response,\n                current_plugin_action_response=response,\n            ),\n        )\n\n    while not chain_finished and assisted and achieved_depth < plugin_max_depth:\n        tool_response = use_tool(prefix, response, tools)\n\n        # Save previous chain response for use in final prompt\n        prev_chain_response = chain_response\n        new_prompt = (\n            f\"{input_prompt}{eos_token}{special_tokens['assistant']}{chain_response}{OBSERVATION_SEQ} {tool_response}\"\n        )\n\n        new_prompt, chain_response = chain.call(new_prompt)\n\n        inner_monologue.append(\"In: \" + str(new_prompt))\n        inner_monologue.append(\"Out: \" + str(chain_response))\n\n        current_action_thought = \"\"\n        if THOUGHT_SEQ in chain_response:\n            current_action_thought = chain_response.split(THOUGHT_SEQ)[1].split(\"\\n\")[0]\n\n        # Send deep plugin intermediate steps to UI\n        utils.send_response(\n            ws,\n            inference.PluginIntermediateResponse(\n                request_id=work_request_id,\n                current_plugin_thought=current_action_thought,\n                current_plugin_action_taken=prefix,\n                current_plugin_action_input=chain_response,\n                current_plugin_action_response=response,\n            ),\n        )\n\n        prefix, response = extract_tool_and_input(llm_output=chain_response, ai_prefix=ASSISTANT_PREFIX)\n        assisted = False if ASSISTANT_PREFIX in prefix else True\n\n        # Check if tool response contains ERROR string and force retry\n        # Current models sometimes decide to retry on error but sometimes just ignore\n        if tool_response.find(\"ERROR\") != -1 and assisted is False:\n            chain_response = prev_chain_response\n            assisted = True\n\n        if not assisted:\n            chain_finished = True\n\n            if REMOVE_TOOLS_FROM_FINAL_PROMPT:\n                TEMPLATE = f\"\"\"{special_tokens['prompter']}{PREFIX}{SUFFIX}\"\"\"\n                input_variables = [\"input\", \"chat_history\", \"language\", \"current_time\"]\n\n                prompt_template = PromptTemplate(input_variables=input_variables, template=TEMPLATE)\n                tool_names = None\n\n            final_input = f\"{input_prompt}{eos_token}{special_tokens['assistant']}\\n{prev_chain_response}{OBSERVATION_SEQ} {tool_response}\"\n            inner_prompt = prepare_prompt(\n                final_input,\n                prompt_template,\n                memory,\n                tool_names,\n                chain.current_time,\n                language,\n                tokenizer,\n                worker_config,\n                action_input_format,\n                custom_instructions,\n            )\n\n            inner_prompt = f\"{inner_prompt}\\n{THOUGHT_SEQ} I now know the final answer\\n{ASSISTANT_PREFIX}:  \"\n\n            plugin_used.execution_details.inner_monologue = inner_monologue\n            plugin_used.execution_details.final_tool_output = tool_response\n            plugin_used.execution_details.final_prompt = inner_prompt\n            plugin_used.execution_details.final_generation_assisted = True\n            plugin_used.execution_details.achieved_depth = achieved_depth + 1\n            plugin_used.execution_details.status = \"success\"\n            plugin_used.name = plugin.plugin_config.name_for_human\n            plugin_used.trusted = plugin.trusted\n            plugin_used.url = plugin.url\n\n            return inner_prompt, plugin_used\n        achieved_depth += 1\n\n    plugin_used.name = plugin.plugin_config.name_for_human\n    plugin_used.trusted = plugin.trusted\n    plugin_used.url = plugin.url\n    plugin_used.execution_details.inner_monologue = inner_monologue\n\n    # Re-add ASSISTANT_PREFIX to chain_response, omitted with stop=[ASSISTANT_PREFIX]\n    chain_response = f\"{chain_response}{ASSISTANT_PREFIX}:  \"\n\n    if chain_finished:\n        if not response:\n            # Malformed non-assisted LLM output\n            plugin_used.execution_details.status = \"failure\"\n            plugin_used.execution_details.error_message = \"Malformed LLM output\"\n            return init_prompt, plugin_used\n\n        plugin_used.execution_details.status = \"success\"\n        return f\"{init_prompt}{THOUGHT_SEQ} I now know the final answer\\n{ASSISTANT_PREFIX}:  \", plugin_used\n    else:\n        # Max depth reached, answer without tool\n        plugin_used.execution_details.final_prompt = init_prompt\n        plugin_used.execution_details.achieved_depth = achieved_depth\n        plugin_used.execution_details.status = \"failure\"\n        plugin_used.execution_details.error_message = f\"Max depth reached: {plugin_max_depth}\"\n        init_prompt = f\"{init_prompt}{THOUGHT_SEQ} I now know the final answer\\n{ASSISTANT_PREFIX}:  \"\n        return init_prompt, plugin_used\n\n\ndef handle_standard_usage(\n    original_prompt: str,\n    prompt_template: PromptTemplate,\n    language: str,\n    memory: ConversationBufferMemory,\n    worker_config: inference.WorkerConfig,\n    tokenizer: transformers.PreTrainedTokenizer,\n    custom_instructions: str = \"\",\n):\n    eos_token = \"\"\n    if special_tokens[\"end\"]:\n        eos_token = special_tokens[\"end\"]\n    elif hasattr(tokenizer, \"eos_token\"):\n        eos_token = tokenizer.eos_token\n    current_time = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n\n    # Non-plugin prompt template can include some external data e.g. datetime, language\n    action_input_format = (\n        JSON_FORMAT_PAYLOAD if prompt_template.template.find(\"payload\") != -1 else JSON_FORMAT_NO_PAYLOAD\n    )\n    input = f\"{original_prompt}{eos_token}{special_tokens['assistant']}\"\n    init_prompt = prepare_prompt(\n        input,\n        prompt_template,\n        memory,\n        None,\n        current_time,\n        language,\n        tokenizer,\n        worker_config,\n        action_input_format,\n        custom_instructions,\n    )\n    return init_prompt, None\n\n\ndef build_memory(work_request: inference.WorkRequest) -> ConversationBufferMemory:\n    memory = ConversationBufferMemory(\n        memory_key=\"chat_history\",\n        input_key=\"input\",\n        output_key=\"output\",\n        ai_prefix=ASSISTANT_PREFIX,\n        human_prefix=HUMAN_PREFIX,\n    )\n\n    for message in work_request.thread.messages[:-1]:\n        if message.role == \"prompter\" and message.state == inference.MessageState.manual and message.content:\n            memory.chat_memory.add_user_message(message.content)\n        elif message.role == \"assistant\" and message.state == inference.MessageState.complete and message.content:\n            memory.chat_memory.add_ai_message(message.content)\n\n    return memory\n\n\ndef handle_conversation(\n    work_request: inference.WorkRequest,\n    worker_config: inference.WorkerConfig,\n    parameters: interface.GenerateStreamParameters,\n    tokenizer: transformers.PreTrainedTokenizer,\n    ws: websocket.WebSocket,\n) -> tuple[str, inference.PluginUsed | None]:\n    try:\n        original_prompt = work_request.thread.messages[-1].content\n        if not original_prompt:\n            raise ValueError(\"Prompt is empty\")\n\n        language = \"English\"\n        plugin = next((p for p in parameters.plugins if p.enabled), None)\n\n        tools_instructions_template, tools = compose_tools_from_plugin(plugin)\n        plugin_enabled = len(tools) > 0\n        memory: ConversationBufferMemory = build_memory(work_request)\n\n        TEMPLATE = f\"\"\"{special_tokens['prompter']}{PREFIX}{tools_instructions_template}{SUFFIX}\"\"\"\n        input_variables = [\n            \"input\",\n            \"chat_history\",\n            \"language\",\n            \"current_time\",\n            \"action_input_format\",\n            \"custom_instructions\",\n        ] + ([\"tools_names\"] if plugin_enabled else [])\n\n        # TODO: Consider passing language from the UI here\n        prompt_template = PromptTemplate(input_variables=input_variables, template=TEMPLATE)\n\n        custom_instructions = (\n            f\"\"\"\\n{CUSTOM_INSTRUCTIONS_PREFIX.format(\n            user_profile=work_request.parameters.user_profile,\n            user_response_instructions=work_request.parameters.user_response_instructions,\n        )}\"\"\"\n            if work_request.parameters.user_response_instructions or work_request.parameters.user_profile\n            else \"\"\n        )\n\n        if plugin_enabled:\n            return handle_plugin_usage(\n                original_prompt,\n                prompt_template,\n                language,\n                memory,\n                worker_config,\n                tokenizer,\n                parameters,\n                tools,\n                plugin,\n                work_request.parameters.plugin_max_depth,\n                ws,\n                work_request.id,\n                custom_instructions,\n            )\n\n        return handle_standard_usage(\n            original_prompt, prompt_template, language, memory, worker_config, tokenizer, custom_instructions\n        )\n    except Exception as e:\n        logger.error(f\"Error while handling conversation: {e}\")\n        return \"\", None\n\n\nif __name__ == \"__main__\":\n    plugin = inference.PluginEntry(\n        enabled=True,\n        url=\"http://localhost:8082/ai-plugin.json\",\n        plugin_config=inference.PluginConfig(\n            name_for_human=\"Local dev plugin\",\n            name_for_model=\"Local dev plugin\",\n            description_for_model=\"Local dev plugin\",\n            description_for_human=\"Local dev plugin\",\n            schema_version=\"0.0.1\",\n            api={\"type\": \"openapi\", \"url\": \"http://localhost:8082/openapi.json\", \"has_user_authentication\": False},\n            auth={\"type\": \"none\"},\n        ),\n    )\n\n    model_config = ModelConfig(\n        model_id=\"decapoda-research/llama-30b-hf\",\n        max_input_length=1024,\n        max_total_length=2048,\n    )\n\n    work_parameters = inference.WorkParameters(model_config=model_config, do_sample=True, seed=42, plugins=[plugin])\n    parameters = interface.GenerateStreamParameters.from_work_parameters(work_parameters)\n\n    worker_config = inference.WorkerConfig(\n        model_config=model_config,\n        model_id=model_config.model_id,\n        max_input_length=model_config.max_input_length,\n        max_total_length=model_config.max_total_length,\n        do_sample=True,\n        seed=42,\n    )\n\n    while True:\n        input_ = input(\"Enter your input: \")\n        if input == \"exit\":\n            break\n        work_request = inference.WorkRequest(\n            request_type=\"work\",\n            parameters=work_parameters,\n            thread=inference.Thread(\n                messages=[\n                    inference.MessageRead(\n                        id=\"1\",\n                        chat_id=\"1\",\n                        parent_id=None,\n                        content=\"Hi, what is your name?\",\n                        created_at=datetime.datetime.now(),\n                        role=\"prompter\",\n                        state=inference.MessageState.complete,\n                        score=0,\n                        work_parameters=work_parameters,\n                        reports=[],\n                    ),\n                    inference.MessageRead(\n                        id=\"1\",\n                        chat_id=\"1\",\n                        parent_id=None,\n                        content=\"Hello, my name is Open Assistant, how i can help you today?\",\n                        created_at=datetime.datetime.now(),\n                        role=\"assistant\",\n                        state=inference.MessageState.complete,\n                        score=0,\n                        work_parameters=work_parameters,\n                        reports=[],\n                    ),\n                    inference.MessageRead(\n                        id=\"1\",\n                        chat_id=\"1\",\n                        parent_id=None,\n                        content=input_,\n                        created_at=datetime.datetime.now(),\n                        role=\"prompter\",\n                        state=inference.MessageState.in_progress,\n                        score=0,\n                        work_parameters=work_parameters,\n                        reports=[],\n                    ),\n                ]\n            ),\n        )\n        tokenizer = transformers.LlamaTokenizer.from_pretrained(model_config.model_id)\n        final_out, used_plugin = handle_conversation(work_request, worker_config, parameters, tokenizer)\n        print(f\"Used_plugin: {used_plugin}\")\n        print(final_out)\n", "inference/worker/settings.py": "import pydantic\n\n\nclass Settings(pydantic.BaseSettings):\n    backend_url: str = \"ws://localhost:8000\"\n    model_config_name: str = \"distilgpt2\"\n    inference_server_url: str = \"http://localhost:8001\"\n    inference_server_route: str = \"/generate_stream\"\n    safety_server_url: str = \"http://localhost:8002\"\n    api_key: str = \"0000\"\n\n    oa_protocol_version: str = \"v2\"\n\n    # Supported: oasst, chatml\n    model_prompt_format: str = \"oasst\"\n\n    retry_on_error: bool = True\n    hf_pause: float = 0.075\n    max_parallel_requests: int = 1\n    use_stop_sequences: bool = False\n\n    perform_oom_test: bool = False\n    oom_test_max_length: int | None = None\n\n    # for hf basic server\n    quantize: bool = False\n\n    bearer_token: str | None = None\n\n    basic_auth_username: str | None = None\n    basic_auth_password: str | None = None\n\n    enable_safety: bool = False\n\n\nsettings = Settings()\n", "inference/worker/utils.py": "import collections\nimport random\nimport threading\nimport time\nfrom typing import Iterable, Literal\n\nimport interface\nimport lorem\nimport pydantic\nimport requests\nimport sseclient\nimport transformers\nimport websocket\nfrom loguru import logger\nfrom oasst_shared.schemas import inference\nfrom settings import settings\n\nshared_tokenizer_lock = threading.Lock()\n\n\nif settings.model_prompt_format == \"chatml\":\n    special_tokens = {\n        \"prompter\": \"<|im_start|>user\\n\",\n        \"assistant\": \"<|im_start|>assistant\\n\",\n        \"system\": \"<|im_start|>system\\n\",\n        \"end\": \"<|im_end|>\\n\",\n    }\nelse:\n    special_tokens = {\n        \"prompter\": \"<|prompter|>\",\n        \"assistant\": \"<|assistant|>\",\n        \"system\": \"<|system|>\",\n        \"end\": \"\",\n    }\n\n\nclass TokenBuffer:\n    \"\"\"\n    A buffer for storing and managing tokens based on various conditions including stop sequences.\n\n    The TokenBuffer class accumulates tokens while keeping track of the length and manages the tokens based on the stop\n    sequences provided during initialization. Tokens can be added to the buffer and later on iterated upon finishing\n    depending on the reason.\n    \"\"\"\n\n    def __init__(self, stop_sequences: list[str]) -> None:\n        self.stop_sequences = stop_sequences\n        self.longest_stop_len = max((len(stop) for stop in stop_sequences), default=1)\n        self.tokens = collections.deque()\n        self.token_lens = collections.deque()\n        self.total_len = 0\n\n    def add(self, token: interface.Token):\n        self.tokens.append(token)\n        self.token_lens.append(len(token))\n        self.total_len += len(token)\n        while True:\n            if not self.tokens:\n                break\n            head_len = self.token_lens[0]\n            if self.total_len - head_len >= self.longest_stop_len:\n                token = self.tokens.popleft()\n                self.token_lens.popleft()\n                self.total_len -= head_len\n                yield token\n            else:\n                break\n\n    def finish(self, reason: Literal[\"length\", \"eos_token\", \"stop_sequence\"]) -> Iterable[interface.Token]:\n        if reason == \"stop_sequence\":\n            end_sequence = \"\"\n            end_tokens = []\n            while self.tokens:\n                token = self.tokens.pop()\n                end_tokens.append(token)\n                end_sequence = token.text + end_sequence\n                if end_sequence in self.stop_sequences:\n                    break\n            else:\n                self.tokens.extend(reversed(end_tokens))\n            yield from self.tokens\n        elif reason == \"eos_token\":\n            if self.tokens:\n                self.tokens.pop()\n            yield from self.tokens\n        else:\n            yield from self.tokens\n\n\ndef get_max_input_length(worker_config: inference.WorkerConfig, plugin_used: bool):\n    \"\"\"Get the maximum possible input length based on the worker config and whether a plugin is in use.\"\"\"\n    max_input_length = worker_config.model_config.max_input_length\n    if plugin_used:\n        max_input_length = max_input_length - 1\n    return max_input_length\n\n\ndef get_tokens_until(tokens: list[int], target: list[int]) -> list[int]:\n    if len(target) == 1:\n        return tokens[: tokens.index(target[0])]\n\n    for i in range(len(tokens) - len(target)):\n        if tokens[i : i + len(target)] == target:\n            break\n    return tokens[:i]\n\n\ndef truncate_prompt(\n    tokenizer: transformers.PreTrainedTokenizer,\n    worker_config: inference.WorkerConfig,\n    parameters: interface.GenerateStreamParameters,\n    prompt: str,\n    plugin_used: bool,\n):\n    \"\"\"\n    Truncate a prompt to ensure it does not exceed the maximum input length. Regardless of truncation, the system\n    prompt is always retained if it is present. If truncation removes the final prompter prefix, a new one is added.\n\n    The stream generation parameters are also updated with a maximum new tokens value which will not cause the total\n    length to exceed the maximum specified in the worker's model config.\n    \"\"\"\n    with shared_tokenizer_lock:\n        ids = tokenizer.encode(prompt)\n        # list of int IDs\n        prompter_prefix_ids = tokenizer.encode(special_tokens[\"prompter\"])\n\n    system_prompt: str | None = None\n    system_tokens: list[int] | None = None\n    if prompt.startswith(special_tokens[\"system\"]):\n        system_prompt = prompt[: prompt.index(special_tokens[\"prompter\"])]\n        system_tokens = get_tokens_until(ids, prompter_prefix_ids)\n\n    max_input_length = get_max_input_length(worker_config, plugin_used)\n\n    if len(ids) > max_input_length:\n        logger.debug(f\"Prompt too long, left-truncating to {max_input_length} tokens\")\n\n        num_system_tokens = len(system_tokens) if system_tokens else 0\n        # Maximum token allowed for the conversation, ex system prompt\n        # We incorporate a buffer to allow for final inference tokenization differing from ours\n        # This is a slightly hacky workaround and it would be better to find a cleaner solution\n        max_conversation_length = max_input_length - num_system_tokens - int(0.01 * max_input_length)\n        ids = ids[-(max_conversation_length - 1) :]\n\n        with shared_tokenizer_lock:\n            prompt = tokenizer.decode(ids)\n\n            if special_tokens[\"prompter\"] not in prompt:\n                prompt = special_tokens[\"prompter\"] + prompt\n                ids = tokenizer.encode(special_tokens[\"prompter\"]) + ids\n\n            if system_tokens:\n                prompt = system_prompt + prompt\n                ids = system_tokens + ids\n\n    max_total_tokens = worker_config.model_config.max_total_length\n    input_length = len(ids)\n    spare = max_total_tokens - input_length - 1\n\n    if not parameters.max_new_tokens:\n        parameters.max_new_tokens = spare\n    elif parameters.max_new_tokens > spare:\n        logger.debug(f\"Max new tokens too high, reducing to {spare}\")\n        parameters.max_new_tokens = spare\n\n    return prompt\n\n\ndef wait_for_inference_server(http: \"HttpClient\", timeout: int = 600):\n    \"\"\"Wait for the \"health\" endpoint of the inference server to return status 200.\"\"\"\n    time_limit = time.time() + timeout\n    while True:\n        try:\n            response = http.get(\"/health\")\n            response.raise_for_status()\n        except (requests.HTTPError, requests.ConnectionError):\n            if time.time() > time_limit:\n                raise\n            sleep_duration = random.uniform(0, 10)\n            logger.warning(f\"Inference server not ready. Retrying in {sleep_duration:.2f} seconds\")\n            time.sleep(sleep_duration)\n        else:\n            logger.info(\"Inference server is ready\")\n            break\n\n\ndef text_to_events(\n    text: str, seed: int | None = None, pause: float = 0.0\n) -> Iterable[interface.GenerateStreamResponse]:\n    \"\"\"\n    Iterate over stream generation \"events\" derived from the given text, where each word in the text is treated as a\n    generated \"token\".\n    \"\"\"\n    tokens = text.split()\n    for token in tokens[:-1]:\n        yield interface.GenerateStreamResponse(\n            token=interface.Token(\n                text=token + \" \",\n                logprob=0.1,\n                id=0,\n            ),\n        )\n        if pause > 0:\n            time.sleep(pause)\n    yield interface.GenerateStreamResponse(\n        token=interface.Token(\n            text=tokens[-1],\n            logprob=0.1,\n            id=0,\n        ),\n        generated_text=text,\n        details=interface.StreamDetails(\n            finish_reason=\"length\",\n            generated_tokens=len(tokens),\n            seed=seed,\n        ),\n    )\n\n\ndef lorem_events(seed):\n    sentence = lorem.paragraph()\n    yield from text_to_events(sentence, seed=seed, pause=0.2)\n\n\nws_lock = threading.Lock()\n\n\ndef send_response(\n    ws: websocket.WebSocket,\n    response: inference.WorkerResponse | inference.WorkerInfo,\n):\n    msg = response.json()\n    with ws_lock:\n        ws.send(msg)\n\n\nclass HttpClient(pydantic.BaseModel):\n    \"\"\"Basic HTTP client built around `requests`. Supports simple authentication.\"\"\"\n\n    base_url: str\n    basic_auth_username: str | None = None\n    basic_auth_password: str | None = None\n    bearer_token: str | None = None\n\n    @property\n    def auth(self):\n        if self.basic_auth_username and self.basic_auth_password:\n            return self.basic_auth_username, self.basic_auth_password\n        else:\n            return None\n\n    def _maybe_add_bearer_token(self, headers: dict[str, str] | None):\n        if self.bearer_token:\n            if headers is None:\n                headers = {}\n            headers[\"Authorization\"] = f\"Bearer {self.bearer_token}\"\n        return headers\n\n    def get(self, path: str, **kwargs):\n        kwargs[\"headers\"] = self._maybe_add_bearer_token(kwargs.get(\"headers\"))\n        return requests.get(self.base_url + path, auth=self.auth, **kwargs)\n\n    def post(self, path: str, **kwargs):\n        kwargs[\"headers\"] = self._maybe_add_bearer_token(kwargs.get(\"headers\"))\n        return requests.post(self.base_url + path, auth=self.auth, **kwargs)\n\n\ndef get_inference_server_stream_events(\n    request: interface.GenerateStreamRequest,\n) -> Iterable[interface.GenerateStreamResponse]:\n    \"\"\"Query the model inference server specified in the worker settings and stream the generation events.\"\"\"\n    http = HttpClient(\n        base_url=settings.inference_server_url,\n        basic_auth_username=settings.basic_auth_username,\n        basic_auth_password=settings.basic_auth_password,\n        bearer_token=settings.bearer_token,\n    )\n    response = http.post(\n        settings.inference_server_route,\n        json=request.dict(),\n        stream=True,\n        headers={\"Accept\": \"text/event-stream\"},\n    )\n    try:\n        response.raise_for_status()\n    except requests.HTTPError:\n        logger.exception(\"Failed to get response from inference server\")\n        logger.error(f\"Response: {response.text}\")\n        raise\n\n    client = sseclient.SSEClient(response)\n    for event in client.events():\n        if event.event == \"error\":\n            logger.error(f\"Error from inference server: {event.data}\")\n            yield interface.GenerateStreamResponse(error=event.data)\n            raise RuntimeError(f\"Error from inference server: {event.data}\")\n        if event.event == \"ping\":\n            continue\n        stream_response = interface.GenerateStreamResponse.parse_raw(event.data)\n        yield stream_response\n", "inference/worker/hf_streamer.py": "import typing\n\nimport transformers\nfrom loguru import logger\n\n\nclass Printer(typing.Protocol):\n    def __call__(self, value: int) -> None:\n        ...\n\n\ndef _unpack(value):\n    if len(value.shape) > 1 and value.shape[0] > 1:\n        raise ValueError(\"HFStreamer only supports batch size 1\")\n    elif len(value.shape) > 1:\n        value = value[0]\n    return value.cpu().tolist()\n\n\n# based on HF text streamer\nclass HFStreamer(transformers.generation.streamers.BaseStreamer):\n    def __init__(self, input_ids, printer: Printer):\n        self.input_ids = _unpack(input_ids)[::-1]\n        self.printer = printer\n\n    def put(self, value):\n        for token_id in _unpack(value):\n            if self.input_ids:\n                input_id = self.input_ids.pop()\n                if input_id != token_id:\n                    logger.warning(f\"Input id {input_id} does not match output id {token_id}\")\n            else:\n                self.printer(token_id)\n\n    def end(self):\n        pass\n", "inference/worker/__main__.py": "import concurrent.futures\nimport signal\nimport sys\nimport time\nfrom contextlib import closing\n\nimport pydantic\nimport transformers\nimport utils\nimport websocket\nimport work\nfrom loguru import logger\nfrom oasst_shared import model_configs\nfrom oasst_shared.schemas import inference\nfrom settings import settings\n\n\ndef terminate_worker(signum, frame):\n    logger.warning(f\"Signal {signum}. Terminating worker...\")\n    sys.exit(0)\n\n\ndef main():\n    signal.signal(signal.SIGINT, terminate_worker)\n    logger.info(f\"Inference protocol version: {inference.INFERENCE_PROTOCOL_VERSION}\")\n\n    model_config = model_configs.MODEL_CONFIGS.get(settings.model_config_name)\n    logger.warning(f\"Model config: {model_config}\")\n    if model_config is None:\n        logger.error(f\"Unknown model config name: {settings.model_config_name}\")\n        sys.exit(2)\n\n    if model_config.is_lorem:\n        tokenizer = None\n    else:\n        tokenizer: transformers.PreTrainedTokenizer = transformers.AutoTokenizer.from_pretrained(model_config.model_id)\n        logger.warning(f\"Tokenizer {tokenizer.name_or_path} vocab size: {len(tokenizer)}\")\n\n    inference_http = utils.HttpClient(\n        base_url=settings.inference_server_url,\n        basic_auth_username=settings.basic_auth_username,\n        basic_auth_password=settings.basic_auth_password,\n        bearer_token=settings.bearer_token,\n    )\n\n    while True:\n        try:\n            if not model_config.is_lorem:\n                utils.wait_for_inference_server(inference_http)\n\n            if settings.perform_oom_test:\n                work.perform_oom_test(tokenizer)\n                sys.exit(0)\n\n            worker_config = inference.WorkerConfig(\n                model_config=model_config,\n                max_parallel_requests=settings.max_parallel_requests,\n            )\n\n            logger.warning(f\"connecting to {settings.backend_url}...\")\n            with closing(\n                websocket.create_connection(\n                    f\"{settings.backend_url}/workers/work\",\n                    header={\n                        \"X-API-Key\": settings.api_key,\n                        \"X-Protocol-Version\": inference.INFERENCE_PROTOCOL_VERSION,\n                    },\n                )\n            ) as ws:\n                logger.warning(\"Connected to backend, sending config...\")\n                worker_info = inference.WorkerInfo(\n                    config=worker_config,\n                    hardware_info=inference.WorkerHardwareInfo(),\n                )\n                utils.send_response(ws, worker_info)\n                logger.warning(\"Config sent, waiting for work...\")\n\n                with concurrent.futures.ThreadPoolExecutor(max_workers=worker_config.max_parallel_requests) as executor:\n                    ftrs = []\n                    while True:\n                        if ftrs:\n                            done, not_done = concurrent.futures.wait(ftrs, timeout=1)\n                            ftrs = list(not_done)\n                            for ftr in done:\n                                ftr.result()\n                        message = ws.recv()\n                        if not message:\n                            logger.warning(\"Connection closed, reconnecting...\")\n                            break\n                        worker_request = pydantic.parse_raw_as(inference.WorkerRequest, message)\n                        match worker_request.request_type:\n                            case \"work\":\n                                logger.info(f\"Handling work request: {worker_request.id=}\")\n                                ftr = executor.submit(\n                                    work.handle_work_request, ws, tokenizer, worker_request, worker_config\n                                )\n                                ftrs.append(ftr)\n                            case \"ping\":\n                                utils.send_response(\n                                    ws,\n                                    inference.PongResponse(\n                                        request_id=worker_request.id, metrics=inference.WorkerMetricsInfo()\n                                    ),\n                                )\n                            case \"wrong_api_key\":\n                                logger.error(\"Your API Key seems to be wrong, please check it!\")\n                                raise RuntimeError(\"Your API Key seems to be wrong, please check it!\")\n                            case \"upgrade_protocol\":\n                                logger.error(\"Your worker is outdated, please upgrade it!\")\n                                sys.exit(2)  # potentially read this status code outside\n                            case \"terminate\":\n                                logger.info(\"Received terminate, terminating worker\")\n                                sys.exit(0)\n                            case \"error\":\n                                logger.error(f\"Received error: {worker_request.error}\")\n                                raise RuntimeError(f\"Received error: {worker_request.error}\")\n\n        except websocket.WebSocketBadStatusException as e:\n            logger.error(f\"Bad status: {e.status_code=} {str(e)=}\")\n            logger.error(\"Did you provide the correct API key?\")\n            if not settings.retry_on_error:\n                sys.exit(1)\n            time.sleep(5)\n        except Exception:\n            logger.exception(\"Error in websocket\")\n            if not settings.retry_on_error:\n                sys.exit(1)\n            logger.warning(\"Retrying in 5 seconds...\")\n            time.sleep(5)\n\n\nif __name__ == \"__main__\":\n    main()\n", "inference/worker/work.py": "import re\nfrom concurrent import futures\n\nimport chat_chain\nimport interface\nimport requests\nimport transformers\nimport utils\nimport websocket\nfrom chat_chain_prompts import (\n    ASSISTANT_PREFIX,\n    CUSTOM_INSTRUCTIONS_PREFIX,\n    END_SEQ,\n    OBSERVATION_SEQ,\n    START_SEQ,\n    THOUGHT_SEQ,\n)\nfrom loguru import logger\nfrom oasst_shared.schemas import inference\nfrom settings import settings\nfrom utils import shared_tokenizer_lock, special_tokens\n\n\ndef make_prompt_and_parameters(\n    tokenizer: transformers.PreTrainedTokenizer,\n    work_request: inference.WorkRequest,\n) -> tuple[str, interface.GenerateStreamParameters]:\n    \"\"\"Prepare a formatted prompt and stream generation parameters based on a work request.\"\"\"\n    if settings.oa_protocol_version != \"v2\":\n        raise RuntimeError(f\"Unsupported oa protocol version: {settings.oa_protocol_version}\")\n\n    eos_token = \"\"\n    if special_tokens[\"end\"]:\n        eos_token = special_tokens[\"end\"]\n    elif hasattr(tokenizer, \"eos_token\"):\n        eos_token = tokenizer.eos_token\n\n    def _prepare_message(message: inference.MessageRead) -> str:\n        prefix = special_tokens[\"assistant\"] if message.is_assistant else special_tokens[\"prompter\"]\n        return prefix + message.content + eos_token\n\n    # Construct prompt\n    messages = [_prepare_message(message) for message in work_request.thread.messages]\n\n    # Prepend system prompt and custom_instructions if it was specified in work parameters\n    work_params = work_request.parameters\n    if work_params.system_prompt or work_params.user_profile or work_params.user_response_instructions:\n        pre_prompt = special_tokens[\"system\"] + (work_params.system_prompt or \"\")\n\n        if work_params.user_profile or work_params.user_response_instructions:\n            pre_prompt = f\"\"\"{pre_prompt}\\n{CUSTOM_INSTRUCTIONS_PREFIX.format(user_profile=work_params.user_profile or \"\", user_response_instructions=work_params.user_response_instructions or \"\")}\"\"\"\n\n        pre_prompt = pre_prompt + eos_token\n        messages = [pre_prompt] + messages\n\n    # Stringify and append assistant prefix to signify start of generation\n    prompt = \"\".join(messages) + special_tokens[\"assistant\"]\n\n    parameters = interface.GenerateStreamParameters.from_work_parameters(work_request.parameters)\n    if settings.use_stop_sequences:\n        parameters.stop = [\n            special_tokens[\"prompter\"],\n            special_tokens[\"assistant\"],\n            special_tokens[\"system\"],\n        ]\n        if eos_token:\n            parameters.stop.append(eos_token)\n    else:\n        parameters.stop = []\n\n    return prompt, parameters\n\n\ndef prepare_safe_prompt(prompt: str, label: str, rots: str) -> str:\n    \"\"\"Given a prompt, safety label, and safety rule of thumb, prepare a 'safe prompt' to replace the prompt.\"\"\"\n    pre_prompt = f\"Answer the following request with {label} as responsible chatbot that believes that {rots}: \"\n    input_list = prompt.split(special_tokens[\"prompter\"])\n    input_list[-1] = pre_prompt + input_list[-1]\n    return special_tokens[\"prompter\"].join(input_list)\n\n\ndef is_safety_triggered(safety_label: str, safety_level: int) -> bool:\n    \"\"\"\n    Determines whether to trigger the safe prompt based on the configured safety level and severity label from the\n    safety classifier.\n    \"\"\"\n    return (\"caution\" in safety_label and safety_level > 1) or (\"intervention\" in safety_label and safety_level > 0)\n\n\ndef parse_safety_response(safety_opinion: str) -> tuple[str, str]:\n    \"\"\"Parse the response from the safety model into a separate label and rule of thumb.\"\"\"\n    safety_opinion = re.sub(r\"<pad>|</s>\", \"\", safety_opinion).split(\"<sep>\")\n    label, rots = safety_opinion[0], \"and\".join([x.strip(\".\") for x in safety_opinion[1:]])\n    label = label.replace(\"<pad>\", \"\").strip()\n    return label, rots\n\n\ndef handle_work_request(\n    ws: websocket.WebSocket,\n    tokenizer: transformers.PreTrainedTokenizer,\n    work_request: inference.WorkRequest,\n    worker_config: inference.WorkerConfig,\n):\n    \"\"\"Handle a work request from end-to-end. Handles plugins and safety if enabled.\"\"\"\n    parameters = interface.GenerateStreamParameters.from_work_parameters(work_request.parameters)\n    prompt = \"\"\n    used_plugin = None\n\n    for plugin in parameters.plugins:\n        if plugin.enabled:\n            prompt, used_plugin = chat_chain.handle_conversation(work_request, worker_config, parameters, tokenizer, ws)\n            # When using plugins and final prompt is truncated due to length limit\n            # LLaMA has tendency to leak internal prompts and generate bad continuations\n            # So we add keywords/sequences to the stop sequences to reduce this\n            parameters.stop.extend([END_SEQ, START_SEQ, THOUGHT_SEQ, f\"{ASSISTANT_PREFIX}:\"])\n            break\n\n    if not used_plugin:\n        prompt, parameters = make_prompt_and_parameters(tokenizer=tokenizer, work_request=work_request)\n\n    logger.debug(f\"Prompt: {prompt}\")\n\n    model_config = worker_config.model_config\n\n    if settings.enable_safety and work_request.safety_parameters.level:\n        safety_request = inference.SafetyRequest(inputs=prompt, parameters=work_request.safety_parameters)\n        safety_response = get_safety_server_response(safety_request)\n        safety_label, safety_rots = parse_safety_response(safety_response.outputs)\n\n        if is_safety_triggered(safety_label, work_request.safety_parameters.level):\n            prompt = prepare_safe_prompt(prompt, safety_label, safety_rots)\n\n            utils.send_response(\n                ws,\n                inference.SafePromptResponse(\n                    request_id=work_request.id,\n                    safe_prompt=prompt,\n                    safety_parameters=work_request.safety_parameters,\n                    safety_label=safety_label,\n                    safety_rots=safety_rots,\n                ),\n            )\n\n            logger.debug(f\"Safe prompt: {prompt}\")\n\n    stream_response = None\n    token_buffer = utils.TokenBuffer(stop_sequences=parameters.stop)\n    if model_config.is_lorem:\n        stream_events = utils.lorem_events(parameters.seed)\n    else:\n        prompt = utils.truncate_prompt(tokenizer, worker_config, parameters, prompt, used_plugin is not None)\n        stream_request = interface.GenerateStreamRequest(\n            inputs=prompt,\n            parameters=parameters,\n        )\n        stream_events = utils.get_inference_server_stream_events(stream_request)\n\n    generated_ids = []\n    decoded_text = \"\"\n    for stream_response in stream_events:\n        if stream_response.is_error:\n            logger.error(f\"Error from inference server: {stream_response.error}\")\n            utils.send_response(\n                ws,\n                inference.ErrorResponse(\n                    request_id=work_request.id,\n                    error=stream_response.error,\n                    metrics=inference.WorkerMetricsInfo(),\n                ),\n            )\n            raise RuntimeError(f\"Error from inference server: {stream_response.error}\")\n        token = stream_response.token\n\n        if model_config.is_llama:\n            generated_ids.append(token.id)\n            try:\n                with shared_tokenizer_lock:\n                    text = tokenizer.decode(generated_ids, skip_special_tokens=True)\n                new_text = text[len(decoded_text) :]\n                if not decoded_text:\n                    new_text = new_text.lstrip()\n            except Exception:\n                text = decoded_text\n                new_text = \"\"\n            token.text = new_text\n            decoded_text = text\n\n        for send_token in token_buffer.add(token):\n            utils.send_response(ws, send_token.to_token_response(request_id=work_request.id))\n    if stream_response is None:\n        logger.error(\"No stream response received\")\n        raise RuntimeError(\"No stream response received\")\n\n    for send_token in token_buffer.finish(reason=stream_response.details.finish_reason):\n        utils.send_response(\n            ws,\n            send_token.to_token_response(request_id=work_request.id),\n        )\n\n    if model_config.is_llama:\n        stream_response.generated_text = stream_response.generated_text.strip()\n        # Helps with RLHF models using plugin prompts. Get generated text to first occurrence of:\n        # START_SEQ, END_SEQ, ASSISTANT_PREFIX, THOUGHT_SEQ, OBSERVATION_SEQ\n        end_seq_index = min(\n            [\n                stream_response.generated_text.find(seq)\n                for seq in [START_SEQ, END_SEQ, f\"{ASSISTANT_PREFIX}:\", THOUGHT_SEQ, OBSERVATION_SEQ]\n                if seq in stream_response.generated_text\n            ]\n            + [len(stream_response.generated_text)]\n        )\n        if end_seq_index != -1 and used_plugin is not None:\n            stream_response.generated_text = stream_response.generated_text[:end_seq_index]\n\n    logger.info(f\"Done. {stream_response=}\")\n    utils.send_response(\n        ws,\n        inference.GeneratedTextResponse(\n            request_id=work_request.id,\n            text=stream_response.generated_text,\n            finish_reason=stream_response.details.finish_reason,\n            metrics=inference.WorkerMetricsInfo(),\n            used_plugin=used_plugin,\n        ),\n    )\n    logger.debug(\"Work complete. Waiting for more work...\")\n\n\ndef get_safety_server_response(request: inference.SafetyRequest) -> inference.SafetyResponse:\n    \"\"\"Query the safety server URL configured in the worker settings.\"\"\"\n    http = utils.HttpClient(base_url=settings.safety_server_url)\n    response = http.post(\"/safety\", json=request.dict())\n    try:\n        response.raise_for_status()\n    except requests.HTTPError:\n        logger.exception(\"Failed to get response from safety server\")\n        logger.error(f\"Response: {response.text}\")\n        raise\n    return inference.SafetyResponse(**response.json())\n\n\ndef perform_oom_test(tokenizer: transformers.PreTrainedTokenizer):\n    logger.warning(\"Performing OOM test\")\n    prompt = (\"This is a test prompt. \" * 10000).strip()\n    parameters = interface.GenerateStreamParameters(\n        max_new_tokens=4,\n        temperature=1.5,\n        top_p=0.95,\n        repetition_penalty=1.0,\n        do_sample=True,\n        stop=[],\n    )\n\n    class OOMError(Exception):\n        pass\n\n    if settings.oom_test_max_length is None:\n        try:\n            for length in range(256, 2**15, 256):\n                prompt_ids = tokenizer.encode(prompt, max_length=length - 4, truncation=True)\n                short_prompt = tokenizer.decode(prompt_ids)\n                stream_request = interface.GenerateStreamRequest(\n                    inputs=short_prompt,\n                    parameters=parameters,\n                )\n                stream_events = utils.get_inference_server_stream_events(stream_request)\n                for stream_response in stream_events:\n                    if stream_response.is_error:\n                        logger.error(f\"Error from inference server: {stream_response.error}\")\n                        raise OOMError()\n        except OOMError:\n            length = length - 256\n        logger.warning(f\"Max length: {length}\")\n    else:\n        length = settings.oom_test_max_length\n\n    with futures.ThreadPoolExecutor() as executor:\n        try:\n            for batch_size in range(1, 32, 1):\n                prompt_ids = tokenizer.encode(prompt, max_length=length - 4, truncation=True)\n                short_prompt = tokenizer.decode(prompt_ids)\n                stream_request = interface.GenerateStreamRequest(\n                    inputs=short_prompt,\n                    parameters=parameters,\n                )\n                ftrs: list[futures.Future] = []\n                try:\n                    for _ in range(batch_size):\n                        stream_events = utils.get_inference_server_stream_events(stream_request)\n                        ftrs.append(executor.submit(list, stream_events))\n                    for ftr in ftrs:\n                        for stream_response in ftr.result():\n                            if stream_response.is_error:\n                                logger.error(f\"Error from inference server: {stream_response.error}\")\n                                raise OOMError()\n                except Exception:\n                    logger.exception(\"OOM\")\n                    try:\n                        for ftr in ftrs:\n                            ftr.cancel()\n                    except Exception:\n                        pass\n                    raise OOMError()\n        except OOMError:\n            batch_size = batch_size - 1\n        logger.warning(f\"Batch size: {batch_size}\")\n\n    logger.warning(\"OOM test complete\")\n    logger.warning(f\"Max length: {length}\")\n    logger.warning(f\"Batch size: {batch_size}\")\n", "inference/worker/download_model.py": "import os\nimport signal\nimport sys\n\nimport transformers\n\n\ndef terminate(signum, frame):\n    print(\"Terminating...\")\n    sys.exit(0)\n\n\nif __name__ == \"__main__\":\n    signal.signal(signal.SIGINT, terminate)\n    model_id = os.getenv(\"MODEL_ID\")\n    if \"llama\" in model_id.lower():\n        transformers.LlamaTokenizer.from_pretrained(model_id)\n        transformers.LlamaForCausalLM.from_pretrained(model_id)\n    else:\n        transformers.AutoTokenizer.from_pretrained(model_id)\n        transformers.AutoModelForCausalLM.from_pretrained(model_id)\n", "inference/worker/hf_langchain_inference.py": "import interface\nimport utils\nfrom langchain.llms.base import LLM\n\n\nclass HFInference(LLM):\n    \"\"\"LangChain LLM implementation which uses the HF inference server configured in the worker settings.\"\"\"\n\n    max_new_tokens: int = 512\n    top_k: int | None = None\n    top_p: float | None = None\n    typical_p: float | None = None\n    temperature: float = 0.8\n    repetition_penalty: float | None = None\n    stop_sequences: list[str] = []\n    seed: int = 42\n    inference_server_url: str = \"\"\n\n    @property\n    def _llm_type(self) -> str:\n        return \"hf-inference\"\n\n    def _call(self, prompt: str, stop: list[str] | None = None) -> str:\n        if stop is None:\n            stop = self.stop_sequences\n        else:\n            stop += self.stop_sequences\n\n        request = interface.GenerateStreamRequest(\n            inputs=prompt,\n            parameters=interface.GenerateStreamParameters(\n                stop=stop,\n                max_new_tokens=self.max_new_tokens,\n                top_k=self.top_k,\n                top_p=self.top_p,\n                typical_p=self.typical_p,\n                temperature=self.temperature,\n                repetition_penalty=self.repetition_penalty,\n                seed=self.seed,\n            ),\n        )\n\n        for event in utils.get_inference_server_stream_events(request):\n            stream_response = event\n\n        generated_text = stream_response.generated_text or \"\"\n\n        for stop_seq in stop:\n            if stop_seq in generated_text:\n                generated_text = generated_text[: generated_text.index(stop_seq)]\n\n        return generated_text\n", "inference/worker/interface.py": "from typing import Literal\n\nimport pydantic\nfrom oasst_shared.schemas import inference\n\n\nclass GenerateStreamParameters(pydantic.BaseModel):\n    max_new_tokens: int = 1024\n    do_sample: bool = True\n    top_k: int | None = None\n    top_p: float | None = None\n    typical_p: float | None = None\n    temperature: float | None = None\n    repetition_penalty: float | None = None\n    seed: int | None = None\n    stop: list[str] = []\n    details: bool = True\n    plugins: list[inference.PluginEntry] = pydantic.Field(default_factory=list[inference.PluginEntry])\n\n    @staticmethod\n    def from_work_parameters(params: inference.WorkParameters) -> \"GenerateStreamParameters\":\n        return GenerateStreamParameters(\n            max_new_tokens=params.sampling_parameters.max_new_tokens,\n            do_sample=params.do_sample,\n            top_k=params.sampling_parameters.top_k,\n            top_p=params.sampling_parameters.top_p,\n            typical_p=params.sampling_parameters.typical_p,\n            temperature=params.sampling_parameters.temperature,\n            repetition_penalty=params.sampling_parameters.repetition_penalty,\n            seed=params.seed,\n            plugins=params.plugins,\n        )\n\n\nclass GenerateStreamRequest(pydantic.BaseModel):\n    inputs: str\n    parameters: GenerateStreamParameters\n\n\nclass Token(pydantic.BaseModel):\n    text: str\n    logprob: float | None\n    id: int\n\n    def __len__(self) -> int:\n        return len(self.text)\n\n    def to_token_response(self, request_id: str) -> inference.TokenResponse:\n        return inference.TokenResponse(\n            request_id=request_id,\n            text=self.text,\n            log_prob=self.logprob,\n            token_id=self.id,\n        )\n\n\nclass StreamDetails(pydantic.BaseModel):\n    generated_tokens: int\n    seed: int | None\n    finish_reason: Literal[\"length\", \"eos_token\", \"stop_sequence\"]\n\n\nclass GenerateStreamResponse(pydantic.BaseModel):\n    token: Token | None\n    generated_text: str | None\n    details: StreamDetails | None\n    error: str | None\n\n    @property\n    def is_end(self) -> bool:\n        return self.generated_text is not None\n\n    @property\n    def is_error(self) -> bool:\n        return self.error is not None\n", "inference/worker/basic_hf_server.py": "\"\"\"\nBasic FastAPI server to serve models using HuggingFace Transformers library.\nThis is an alternative to running the HuggingFace `text-generation-inference` (tgi) server.\n\"\"\"\n\nimport sys\nimport threading\nfrom queue import Queue\n\nimport fastapi\nimport hf_stopping\nimport hf_streamer\nimport interface\nimport torch\nimport transformers\nimport uvicorn\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom loguru import logger\nfrom oasst_shared import model_configs\nfrom settings import settings\nfrom sse_starlette.sse import EventSourceResponse\n\napp = fastapi.FastAPI()\n\nDECODE_TOKEN = \"<decode-token>\"\n\n\n# Allow CORS\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\n\n@app.middleware(\"http\")\nasync def log_exceptions(request: fastapi.Request, call_next):\n    try:\n        response = await call_next(request)\n    except Exception:\n        logger.exception(\"Exception in request\")\n        raise\n    return response\n\n\nmodel_loaded: bool = False\nfully_loaded: bool = False\nmodel_input_queue: Queue = Queue()\n\n\ndef model_thread():\n    \"\"\"Continually obtain new work requests from the model input queue and work on them.\"\"\"\n    model: transformers.PreTrainedModel\n    tokenizer: transformers.PreTrainedTokenizer\n    model, tokenizer, decode_token = load_models()\n\n    request: interface.GenerateStreamRequest\n    output_queue: Queue\n    eos_token_id = tokenizer.eos_token_id if hasattr(tokenizer, \"eos_token_id\") else None\n    while True:\n        request, output_queue = model_input_queue.get()\n        try:\n            prompt = request.inputs\n            params = request.parameters.dict()\n            seed = params.pop(\"seed\")\n            stop_sequences = params.pop(\"stop\")\n            params.pop(\"details\")\n            params.pop(\"plugins\")\n\n            if seed is not None:\n                torch.manual_seed(seed)\n\n            last_token_id = None  # need to delay by 1 to simulate tgi\n\n            def print_text(token_id: int):\n                nonlocal last_token_id\n                if last_token_id is not None:\n                    text = decode_token(last_token_id)\n                    stream_response = interface.GenerateStreamResponse(\n                        token=interface.Token(text=text, id=last_token_id),\n                    )\n                    output_queue.put_nowait(stream_response)\n                last_token_id = token_id\n\n            with torch.no_grad():\n                ids = tokenizer.encode(prompt, return_tensors=\"pt\", add_special_tokens=False)\n                streamer = hf_streamer.HFStreamer(input_ids=ids, printer=print_text)\n                ids = ids.to(model.device)\n                stopping_criteria = (\n                    transformers.StoppingCriteriaList(\n                        [hf_stopping.SequenceStoppingCriteria(tokenizer, stop_sequences, prompt)]\n                    )\n                    if stop_sequences\n                    else None\n                )\n                output = model.generate(\n                    ids,\n                    **params,\n                    streamer=streamer,\n                    eos_token_id=eos_token_id,\n                    stopping_criteria=stopping_criteria,\n                )\n                output = output.cpu()\n                output_ids = output[0][len(ids[0]) :]\n                decoded = tokenizer.decode(output_ids, skip_special_tokens=True)\n\n            stream_response = interface.GenerateStreamResponse(\n                token=interface.Token(\n                    text=decode_token(last_token_id),  # hack because the \"normal\" inference server does this at once\n                    id=last_token_id,\n                ),\n                generated_text=decoded.strip(),\n                details=interface.StreamDetails(\n                    finish_reason=\"eos_token\",\n                    generated_tokens=len(output_ids),\n                    seed=seed,\n                ),\n            )\n            output_queue.put_nowait(stream_response)\n        except Exception as e:\n            logger.exception(\"Exception in model thread\")\n            output_queue.put_nowait(interface.GenerateStreamResponse(error=str(e)))\n\n\ndef load_models():\n    global model_loaded\n\n    torch.set_num_threads(1)\n    torch.set_num_interop_threads(1)\n\n    model_config = model_configs.MODEL_CONFIGS.get(settings.model_config_name)\n    if model_config is None:\n        logger.error(f\"Unknown model config name: {settings.model_config_name}\")\n        sys.exit(2)\n\n    hf_config = transformers.AutoConfig.from_pretrained(model_config.model_id)\n    logger.warning(f\"Loading model {model_config.model_id}...\")\n    tokenizer = transformers.AutoTokenizer.from_pretrained(model_config.model_id)\n    logger.warning(f\"tokenizer {tokenizer.name_or_path} has vocab size {len(tokenizer)}\")\n\n    # see `decode_token` method, taken from HF text-generation-inference\n    tokenizer.add_special_tokens({\"additional_special_tokens\": [\"<decode-token>\"]})\n\n    special_decode_token_id = tokenizer.convert_tokens_to_ids(\"<decode-token>\")\n    special_decode_token_length = len(\"<decode-token>\")\n\n    def decode_token(token_id):\n        result = tokenizer.decode([special_decode_token_id, token_id], skip_special_tokens=False)\n        # slice to remove special decode token\n        return result[special_decode_token_length:]\n\n    config_dtype = hf_config.torch_dtype if hasattr(hf_config, \"torch_dtype\") else torch.float32\n    dtype = torch.bfloat16 if torch.has_cuda and torch.cuda.is_bf16_supported() else config_dtype\n\n    model = transformers.AutoModelForCausalLM.from_pretrained(\n        model_config.model_id,\n        torch_dtype=dtype,\n        load_in_8bit=settings.quantize,\n        device_map=\"auto\" if torch.cuda.is_available() else None,\n    ).eval()\n    logger.warning(\"Model loaded, using it once...\")\n\n    # warmup\n    with torch.no_grad():\n        text = \"Hello, world\"\n        tokens = tokenizer.encode(text, return_tensors=\"pt\")\n        tokens = tokens.to(model.device)\n        model.generate(tokens, max_length=10, num_beams=1, do_sample=False)\n\n    model_loaded = True\n\n    return model, tokenizer, decode_token\n\n\n@app.on_event(\"startup\")\nasync def start_model_thread():\n    logger.warning(\"Starting model thread...\")\n    threading.Thread(target=model_thread, daemon=True).start()\n    logger.warning(\"Model thread started\")\n\n\n@app.on_event(\"startup\")\nasync def welcome_message():\n    global fully_loaded\n    logger.warning(\"Server started\")\n    logger.warning(\"To stop the server, press Ctrl+C\")\n    fully_loaded = True\n\n\n@app.post(\"/generate_stream\")\nasync def generate(\n    request: interface.GenerateStreamRequest,\n):\n    def event_stream():\n        try:\n            output_queue: Queue = Queue()\n            model_input_queue.put_nowait((request, output_queue))\n            while True:\n                output = output_queue.get()  # type: interface.GenerateStreamResponse\n                yield {\"data\": output.json()}\n                if output.is_end:\n                    break\n                if output.is_error:\n                    raise Exception(output.error)\n        except Exception as e:\n            logger.exception(\"Exception in event stream\")\n            output_queue.put_nowait(interface.GenerateStreamResponse(error=str(e)))\n            raise\n\n    return EventSourceResponse(event_stream())\n\n\n@app.get(\"/health\")\nasync def health():\n    if not (fully_loaded and model_loaded):\n        raise fastapi.HTTPException(status_code=503, detail=\"Server not fully loaded\")\n    return {\"status\": \"ok\"}\n\n\nif __name__ == \"__main__\":\n    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n", "inference/safety/settings.py": "import pydantic\n\n\nclass Settings(pydantic.BaseSettings):\n    # HuggingFace model ID for the model to load in blade2blade\n    safety_model_name: str = \"shahules786/blade2blade-t5-base\"\n\n\nsettings = Settings()\n", "inference/safety/main.py": "\"\"\"\nA simple FastAPI server which serves a `blade2blade2` safety model.\n\nSee https://github.com/LAION-AI/blade2blade for context.\n\"\"\"\n\nimport asyncio\nfrom concurrent.futures import ThreadPoolExecutor\n\nimport fastapi\nimport uvicorn\nfrom blade2blade import Blade2Blade\nfrom loguru import logger\nfrom oasst_shared.schemas import inference\nfrom settings import settings\n\napp = fastapi.FastAPI()\n\n\n@app.middleware(\"http\")\nasync def log_exceptions(request: fastapi.Request, call_next):\n    try:\n        response = await call_next(request)\n    except Exception:\n        logger.exception(\"Exception in request\")\n        raise\n    return response\n\n\npipeline_loaded: bool = False\npipeline: Blade2Blade\nexecutor = ThreadPoolExecutor()\n\n\n@app.on_event(\"startup\")\nasync def load_pipeline():\n    global pipeline_loaded, pipeline\n    pipeline = Blade2Blade(settings.safety_model_name)\n    # warmup\n    input = \"|prompter|Hey,how are you?|endoftext|\"\n    _ = pipeline.predict(input)\n    pipeline_loaded = True\n    logger.info(\"Pipeline loaded\")\n\n\nasync def async_predict(pipeline: Blade2Blade, inputs: str):\n    \"\"\"Run predictions in a separate thread for a small server parallelism benefit.\"\"\"\n    return await asyncio.get_event_loop().run_in_executor(executor, pipeline.predict, inputs)\n\n\n@app.post(\"/safety\", response_model=inference.SafetyResponse)\nasync def safety(request: inference.SafetyRequest):\n    global pipeline_loaded, pipeline\n    while not pipeline_loaded:\n        await asyncio.sleep(1)\n    outputs = await async_predict(pipeline, request.inputs)\n    return inference.SafetyResponse(outputs=outputs)\n\n\n@app.get(\"/health\")\nasync def health():\n    if not pipeline_loaded:\n        raise fastapi.HTTPException(status_code=503, detail=\"Server not fully loaded\")\n    return {\"status\": \"ok\"}\n\n\nif __name__ == \"__main__\":\n    uvicorn.run(app, host=\"0.0.0.0\", port=8008)\n", "inference/text-client/text_client_utils.py": "import json\n\nimport requests\nimport sseclient\nfrom loguru import logger\n\n\nclass DebugClient:\n    def __init__(self, backend_url, http_client=requests):\n        self.backend_url = backend_url\n        self.http_client = http_client\n        self.auth_headers = None\n        self.available_models = self.get_available_models()\n\n    def login(self, username):\n        auth_data = self.http_client.get(f\"{self.backend_url}/auth/callback/debug\", params={\"code\": username}).json()\n        assert auth_data[\"access_token\"][\"token_type\"] == \"bearer\"\n        bearer_token = auth_data[\"access_token\"][\"access_token\"]\n        logger.debug(f\"Logged in as {username} with token {bearer_token}\")\n        self.auth_headers = {\"Authorization\": f\"Bearer {bearer_token}\"}\n\n    def create_chat(self):\n        response = self.http_client.post(\n            f\"{self.backend_url}/chats\",\n            json={},\n            headers=self.auth_headers,\n        )\n        response.raise_for_status()\n        self.chat_id = response.json()[\"id\"]\n        self.message_id = None\n        return self.chat_id\n\n    def get_available_models(self):\n        response = self.http_client.get(\n            f\"{self.backend_url}/configs/model_configs\",\n            headers=self.auth_headers,\n        )\n        response.raise_for_status()\n        return [model[\"name\"] for model in response.json()]\n\n    def send_message(self, message, model_config_name):\n        available_models = self.get_available_models()\n        if model_config_name not in available_models:\n            raise ValueError(f\"Invalid model config name: {model_config_name}\")\n        response = self.http_client.post(\n            f\"{self.backend_url}/chats/{self.chat_id}/prompter_message\",\n            json={\n                \"parent_id\": self.message_id,\n                \"content\": message,\n            },\n            headers=self.auth_headers,\n        )\n        response.raise_for_status()\n        prompter_message_id = response.json()[\"id\"]\n\n        response = self.http_client.post(\n            f\"{self.backend_url}/chats/{self.chat_id}/assistant_message\",\n            json={\n                \"parent_id\": prompter_message_id,\n                \"model_config_name\": model_config_name,\n                \"sampling_parameters\": {\n                    \"top_p\": 0.95,\n                    \"top_k\": 50,\n                    \"repetition_penalty\": 1.2,\n                    \"temperature\": 1.0,\n                },\n            },\n            headers=self.auth_headers,\n        )\n        response.raise_for_status()\n        self.message_id = response.json()[\"id\"]\n\n        response = self.http_client.get(\n            f\"{self.backend_url}/chats/{self.chat_id}/messages/{self.message_id}/events\",\n            stream=True,\n            headers={\n                \"Accept\": \"text/event-stream\",\n                **self.auth_headers,\n            },\n        )\n        response.raise_for_status()\n        if response.status_code == 204:\n            response = self.http_client.get(\n                f\"{self.backend_url}/chats/{self.chat_id}/messages/{self.message_id}\",\n                headers=self.auth_headers,\n            )\n            response.raise_for_status()\n            data = response.json()\n            yield data[\"content\"]\n        else:\n            client = sseclient.SSEClient(response)\n            events = iter(client.events())\n            for event in events:\n                if event.event == \"error\":\n                    raise RuntimeError(event.data)\n                if event.event == \"ping\":\n                    continue\n                try:\n                    data = json.loads(event.data)\n                except json.JSONDecodeError:\n                    raise RuntimeError(f\"Failed to decode {event.data=}\")\n                event_type = data[\"event_type\"]\n                if event_type == \"token\":\n                    yield data[\"text\"]\n                elif event_type == \"message\":\n                    # full message content, can be ignored here\n                    break\n                elif event_type == \"error\":\n                    raise RuntimeError(data[\"error\"])\n                elif event_type == \"pending\":\n                    logger.debug(f\"Message pending. {data=}\")\n", "inference/text-client/__main__.py": "\"\"\"Simple REPL frontend.\"\"\"\n\nimport time\n\nimport text_client_utils as utils\nimport typer\nfrom loguru import logger\n\napp = typer.Typer()\n\n\n@app.command()\ndef main(backend_url: str = \"http://127.0.0.1:8000\", model_config_name=\"distilgpt2\", username=\"test1\"):\n    \"\"\"Simple REPL client.\"\"\"\n    while True:\n        try:\n            # login\n            client = utils.DebugClient(backend_url)\n            client.login(username)\n            chat_id = client.create_chat()\n            typer.echo(f\"Chat ID: {chat_id}\")\n            while True:\n                message = typer.prompt(\"User\").strip()\n                if not message:\n                    continue\n\n                events = client.send_message(message, model_config_name)\n                print(\"Assistant: \", end=\"\", flush=True)\n                for event in events:\n                    print(event, end=\"\", flush=True)\n                print()\n\n        except typer.Abort:\n            typer.echo(\"Exiting...\")\n            break\n        except Exception as e:\n            logger.exception(\"Chat Error\")\n            typer.echo(f\"Error: {e}\")\n            typer.echo(\"Error, restarting chat...\")\n            time.sleep(1)\n\n\nif __name__ == \"__main__\":\n    app()\n", "inference/server/main.py": "import asyncio\nimport math\nimport signal\nimport sys\n\nimport fastapi\nimport redis.asyncio as redis\nimport sqlmodel\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom fastapi_limiter import FastAPILimiter\nfrom loguru import logger\nfrom oasst_inference_server import database, deps, models, plugins\nfrom oasst_inference_server.routes import account, admin, auth, chats, configs, workers\nfrom oasst_inference_server.settings import settings\nfrom oasst_shared.schemas import inference\nfrom prometheus_fastapi_instrumentator import Instrumentator\nfrom starlette.middleware.sessions import SessionMiddleware\nfrom starlette.status import HTTP_429_TOO_MANY_REQUESTS\n\napp = fastapi.FastAPI(title=settings.PROJECT_NAME)\n\n\n# Allow CORS\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=settings.inference_cors_origins_list,\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n# Session middleware for authlib\napp.add_middleware(SessionMiddleware, secret_key=settings.session_middleware_secret_key)\n\n\n@app.middleware(\"http\")\nasync def log_exceptions(request: fastapi.Request, call_next):\n    try:\n        response = await call_next(request)\n    except Exception:\n        logger.exception(\"Exception in request\")\n        raise\n    return response\n\n\n# add prometheus metrics at /metrics\n@app.on_event(\"startup\")\nasync def enable_prom_metrics():\n    Instrumentator().instrument(app).expose(app)\n\n\n@app.on_event(\"startup\")\nasync def log_inference_protocol_version():\n    logger.warning(f\"Inference protocol version: {inference.INFERENCE_PROTOCOL_VERSION}\")\n\n\ndef terminate_server(signum, frame):\n    logger.warning(f\"Signal {signum}. Terminating server...\")\n    sys.exit(0)\n\n\n@app.on_event(\"startup\")\nasync def alembic_upgrade():\n    \"\"\"Upgrades database schema based on Alembic migration scripts.\"\"\"\n    signal.signal(signal.SIGINT, terminate_server)\n    if not settings.update_alembic:\n        logger.warning(\"Skipping alembic upgrade on startup (update_alembic is False)\")\n        return\n    logger.warning(\"Attempting to upgrade alembic on startup\")\n    retry = 0\n    while True:\n        try:\n            async with database.make_engine().begin() as conn:\n                await conn.run_sync(database.alembic_upgrade)\n            logger.warning(\"Successfully upgraded alembic on startup\")\n            break\n        except Exception:\n            logger.exception(\"Alembic upgrade failed on startup\")\n            retry += 1\n            if retry >= settings.alembic_retries:\n                raise\n\n            timeout = settings.alembic_retry_timeout * 2**retry\n            logger.warning(f\"Retrying alembic upgrade in {timeout} seconds\")\n            await asyncio.sleep(timeout)\n    signal.signal(signal.SIGINT, signal.SIG_DFL)\n\n\n@app.on_event(\"startup\")\nasync def setup_rate_limiter():\n    if not settings.rate_limit:\n        logger.warning(\"Skipping rate limiter setup on startup (rate_limit is False)\")\n        return\n\n    async def http_callback(request: fastapi.Request, response: fastapi.Response, pexpire: int):\n        \"\"\"Error callback function when too many requests\"\"\"\n        expire = math.ceil(pexpire / 1000)\n        raise fastapi.HTTPException(f\"Too Many Requests. Retry After {expire} seconds.\", HTTP_429_TOO_MANY_REQUESTS)\n\n    try:\n        client = redis.Redis(\n            host=settings.redis_host, port=settings.redis_port, db=settings.redis_ratelim_db, decode_responses=True\n        )\n        logger.info(f\"Connected to {client=}\")\n        await FastAPILimiter.init(client, http_callback=http_callback)\n    except Exception:\n        logger.exception(\"Failed to establish Redis connection\")\n\n\n@app.on_event(\"startup\")\nasync def maybe_add_debug_api_keys():\n    debug_api_keys = settings.debug_api_keys_list\n    if not debug_api_keys:\n        logger.warning(\"No debug API keys configured, skipping\")\n        return\n    try:\n        logger.warning(\"Adding debug API keys\")\n        async with deps.manual_create_session() as session:\n            for api_key in debug_api_keys:\n                logger.info(f\"Checking if debug API key {api_key} exists\")\n                if (\n                    await session.exec(sqlmodel.select(models.DbWorker).where(models.DbWorker.api_key == api_key))\n                ).one_or_none() is None:\n                    logger.info(f\"Adding debug API key {api_key}\")\n                    session.add(models.DbWorker(api_key=api_key, name=\"Debug API Key\"))\n                    await session.commit()\n                else:\n                    logger.info(f\"Debug API key {api_key} already exists\")\n        logger.warning(\"Finished adding debug API keys\")\n    except Exception:\n        logger.exception(\"Failed to add debug API keys\")\n        raise\n\n\n# add routes\napp.include_router(account.router)\napp.include_router(auth.router)\napp.include_router(admin.router)\napp.include_router(chats.router)\napp.include_router(workers.router)\napp.include_router(configs.router)\n\n# mount builtin plugins to be hosted on this server\nfor app_prefix, sub_app in plugins.plugin_apps.items():\n    app.mount(path=settings.plugins_path_prefix + app_prefix, app=sub_app)\n\n\n@app.on_event(\"startup\")\nasync def welcome_message():\n    logger.warning(\"Inference server started\")\n    logger.warning(\"To stop the server, press Ctrl+C\")\n", "inference/server/export.py": "\"\"\"Script to facilitate exporting chat data from the server database.\"\"\"\n\nimport argparse\nimport asyncio\nimport contextlib\nimport datetime as dt\nimport gzip\nimport json\nimport sys\nfrom collections import defaultdict\nfrom pathlib import Path\nfrom typing import Any, TextIO\n\nimport sqlalchemy\nimport sqlmodel\nfrom fastapi.encoders import jsonable_encoder\nfrom oasst_data import (\n    ExportMessageEvent,\n    ExportMessageEventReport,\n    ExportMessageEventScore,\n    ExportMessageNode,\n    ExportMessageTree,\n)\nfrom oasst_inference_server import deps\nfrom oasst_inference_server.database import AsyncSession\nfrom oasst_inference_server.models import DbChat, DbMessage\nfrom oasst_shared.utils import Anonymizer\n\n\n# see https://stackoverflow.com/questions/17602878/how-to-handle-both-with-open-and-sys-stdout-nicely\n@contextlib.contextmanager\ndef smart_open(filename: str = None) -> TextIO:\n    if filename and filename != \"-\":\n        fh = open(filename, \"wt\", encoding=\"UTF-8\")\n    else:\n        fh = sys.stdout\n\n    try:\n        yield fh\n    finally:\n        if fh is not sys.stdout:\n            fh.close()\n\n\ndef maybe_anonymize(anonymizer: Anonymizer | None, collection: str, key: str) -> str:\n    if anonymizer:\n        return anonymizer.anonymize(collection, key)\n    else:\n        return key\n\n\ndef prepare_export_events(\n    chat: DbChat,\n    message: DbMessage,\n    anonymizer: Anonymizer | None = None,\n) -> dict[str, list[ExportMessageEvent]]:\n    export_events: dict[str, list[ExportMessageEvent]] = {}\n\n    if message.reports:\n        export_events[\"report\"] = [\n            ExportMessageEventReport(\n                user_id=maybe_anonymize(anonymizer, \"user\", str(chat.user_id)),\n                report_type=str(db_report.report_type),\n                reason=db_report.reason,\n            )\n            for db_report in message.reports\n        ]\n\n    if message.score:\n        export_events[\"score\"] = [\n            ExportMessageEventScore(\n                user_id=maybe_anonymize(anonymizer, \"user\", str(chat.user_id)),\n                score=message.score,\n            )\n        ]\n\n    return export_events\n\n\ndef prepare_export_message_tree(\n    chat: DbChat,\n    anonymizer: Anonymizer | None = None,\n) -> ExportMessageTree:\n    messages: list[DbMessage] = chat.messages\n\n    # Exclude messages without content (e.g. work still in progress or aborted)\n    export_messages: list[ExportMessageNode] = [\n        prepare_export_message_node(chat, message, anonymizer=anonymizer) for message in messages if message.content\n    ]\n\n    messages_by_parent = defaultdict(list)\n    for message in export_messages:\n        messages_by_parent[message.parent_id].append(message)\n\n    def assign_replies(node: ExportMessageNode) -> ExportMessageNode:\n        node.replies = messages_by_parent[node.message_id]\n        for child in node.replies:\n            assign_replies(child)\n        return node\n\n    prompt = assign_replies(messages_by_parent[None][0])\n    return ExportMessageTree(message_tree_id=str(chat.id), tree_state=\"not_applicable\", prompt=prompt)\n\n\ndef prepare_export_message_node(\n    chat: DbChat,\n    message: DbMessage,\n    anonymizer: Anonymizer | None = None,\n) -> ExportMessageNode:\n    if message.worker_config:\n        model_name = message.worker_config.model_config.model_id\n    else:\n        model_name = None\n\n    # Chat prompts are human-written, responses are synthetic\n    synthetic = message.role == \"assistant\"\n\n    events: dict[str, list[ExportMessageEvent]] = prepare_export_events(chat, message, anonymizer=anonymizer)\n\n    message_id = maybe_anonymize(anonymizer, \"message\", message.id)\n    parent_id = maybe_anonymize(anonymizer, \"message\", message.parent_id)\n    user_id = maybe_anonymize(anonymizer, \"user\", chat.user_id)\n\n    return ExportMessageNode(\n        message_id=message_id,\n        parent_id=parent_id,\n        user_id=user_id,\n        created_date=message.created_at,\n        text=message.content,\n        role=message.role,\n        synthetic=synthetic,\n        model_name=model_name,\n        events=events,\n    )\n\n\ndef write_messages_to_file(\n    file: Path,\n    chats: list[DbChat],\n    use_compression: bool = True,\n    write_trees: bool = True,\n    anonymizer: Anonymizer | None = None,\n) -> None:\n    out_buff: TextIO\n\n    if use_compression:\n        if not file:\n            raise RuntimeError(\"File name must be specified when using compression.\")\n        out_buff = gzip.open(file, \"wt\", encoding=\"UTF-8\")\n    else:\n        out_buff = smart_open(file)\n\n    with out_buff as f:\n        for chat in chats:\n            if write_trees:\n                export_chat = prepare_export_message_tree(chat, anonymizer=anonymizer)\n                file_data = jsonable_encoder(export_chat, exclude_none=True)\n                json.dump(file_data, f)\n                f.write(\"\\n\")\n            else:\n                # Exclude messages without content (e.g. work still in progress or aborted)\n                for message in filter(lambda m: m.content, chat.messages):\n                    export_message = prepare_export_message_node(chat, message, anonymizer=anonymizer)\n                    file_data = jsonable_encoder(export_message, exclude_none=True)\n                    json.dump(file_data, f)\n                    f.write(\"\\n\")\n\n\nasync def fetch_eligible_chats(session_generator, filters: list[Any]) -> list[DbChat]:\n    \"\"\"Fetch chats which are not opted out of data collection and match the given filters.\"\"\"\n    session: AsyncSession\n    filters.append(DbChat.allow_data_use)\n    async with session_generator() as session:\n        query = (\n            sqlmodel.select(DbChat)\n            .filter(*filters)\n            .options(\n                sqlalchemy.orm.joinedload(\"*\"),\n            )\n        )\n        chats: list[DbChat] = (await session.exec(query)).unique().all()\n        return chats\n\n\ndef export_chats(\n    session_generator,\n    export_path: Path,\n    filters: list[Any],\n    use_compression: bool = True,\n    write_trees: bool = True,\n    anonymizer_seed: str | None = None,\n) -> None:\n    eligible_chats: list[DbChat] = asyncio.run(fetch_eligible_chats(session_generator, filters))\n    anonymizer = Anonymizer(anonymizer_seed) if anonymizer_seed else None\n\n    write_messages_to_file(\n        export_path,\n        eligible_chats,\n        write_trees=write_trees,\n        use_compression=use_compression,\n        anonymizer=anonymizer,\n    )\n\n\ndef parse_date(date_str: str) -> dt.date:\n    return dt.datetime.strptime(date_str, \"%Y-%m-%d\").date()\n\n\ndef parse_args() -> argparse.Namespace:\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        \"--export-file\",\n        type=str,\n        help=\"Name of file to export chats to. If not provided, output will be sent to STDOUT\",\n    )\n    parser.add_argument(\n        \"--no-compression\",\n        action=\"store_true\",\n        help=\"Disable compression when writing to file.\",\n    )\n    parser.add_argument(\n        \"--write-flat\",\n        action=\"store_true\",\n        help=\"Write chats as individual messages rather than trees.\",\n    )\n    parser.add_argument(\n        \"--anonymizer-seed\",\n        type=int,\n        help=\"Seed for the anonymizer. If not specified, no anonymization will be performed.\",\n    )\n    parser.add_argument(\n        \"--from-date\",\n        type=parse_date,\n        help=\"Only export chats created on or after this date. Format: YYYY-MM-DD\",\n    )\n    parser.add_argument(\n        \"--to-date\",\n        type=parse_date,\n        help=\"Only export chats created on or before this date. Format: YYYY-MM-DD\",\n    )\n    parser.add_argument(\n        \"--user-id\",\n        type=str,\n        help=\"Only export chats created by this user.\",\n    )\n    parser.add_argument(\n        \"--chat-id\",\n        type=str,\n        help=\"Only export this chat.\",\n    )\n    parser.add_argument(\n        \"--scored\",\n        action=\"store_true\",\n        help=\"Only export chats with at least one assistant message with score != 0.\",\n    ),\n    parser.add_argument(\n        \"--reported\",\n        action=\"store_true\",\n        help=\"Only export chats with at least one reported message.\",\n    )\n    return parser.parse_args()\n\n\ndef create_filters(args: argparse.Namespace) -> list[Any]:\n    filters = []\n\n    if args.from_date:\n        filters.append(DbChat.created_at >= args.from_date)\n    if args.to_date:\n        filters.append(DbChat.created_at <= args.to_date)\n    if args.user_id:\n        filters.append(DbChat.user_id == args.user_id)\n    if args.chat_id:\n        filters.append(DbChat.id == args.chat_id)\n    if args.scored:\n        filters.append(DbChat.messages.any((DbMessage.role == \"assistant\") & (DbMessage.score != 0)))\n    if args.reported:\n        filters.append(DbChat.messages.any(DbMessage.reports.any()))\n\n    return filters\n\n\ndef main():\n    args = parse_args()\n\n    export_path = Path(args.export_file) if args.export_file else None\n    filters = create_filters(args)\n\n    export_chats(\n        deps.manual_create_session,\n        export_path,\n        filters,\n        use_compression=not args.no_compression,\n        write_trees=not args.write_flat,\n        anonymizer_seed=args.anonymizer_seed,\n    )\n\n\nif __name__ == \"__main__\":\n    main()\n", "inference/server/alembic/env.py": "import asyncio\nfrom logging.config import fileConfig\n\nimport sqlmodel\nfrom alembic import context\nfrom loguru import logger\nfrom oasst_inference_server import models  # noqa: F401\nfrom sqlalchemy import engine_from_config, pool, text\nfrom sqlalchemy.ext.asyncio import AsyncEngine\n\n# this is the Alembic Config object, which provides\n# access to the values within the .ini file in use.\nconfig = context.config\n\n# Interpret the config file for Python logging.\n# This line sets up loggers basically.\nif config.config_file_name is not None:\n    fileConfig(config.config_file_name)\n\n# add your model's MetaData object here\n# for 'autogenerate' support\n# from myapp import mymodel\n# target_metadata = mymodel.Base.metadata\ntarget_metadata = sqlmodel.SQLModel.metadata\n\n# other values from the config, defined by the needs of env.py,\n# can be acquired:\n# my_important_option = config.get_main_option(\"my_important_option\")\n# ... etc.\n\n\ndef run_migrations_offline() -> None:\n    \"\"\"Run migrations in 'offline' mode.\n\n    This configures the context with just a URL\n    and not an Engine, though an Engine is acceptable\n    here as well.  By skipping the Engine creation\n    we don't even need a DBAPI to be available.\n\n    Calls to context.execute() here emit the given string to the\n    script output.\n\n    \"\"\"\n    url = config.get_main_option(\"sqlalchemy.url\")\n    context.configure(\n        url=url,\n        target_metadata=target_metadata,\n        literal_binds=True,\n        dialect_opts={\"paramstyle\": \"named\"},\n    )\n\n    with context.begin_transaction():\n        context.run_migrations()\n\n\ndef do_run_migrations(connection):\n    context.configure(connection=connection, target_metadata=target_metadata)\n\n    with context.begin_transaction():\n        context.get_context()._ensure_version_table()\n        connection.execute(text(\"LOCK TABLE alembic_version IN ACCESS EXCLUSIVE MODE\"))\n        context.run_migrations()\n\n\nasync def run_async_migrations() -> None:\n    \"\"\"Run migrations in 'online' mode.\n\n    In this scenario we need to create an Engine\n    and associate a connection with the context.\n\n    \"\"\"\n    connectable = engine_from_config(\n        config.get_section(config.config_ini_section),\n        prefix=\"sqlalchemy.\",\n        poolclass=pool.NullPool,\n        future=True,\n    )\n\n    connectable = AsyncEngine(connectable)\n\n    logger.info(f\"Running migrations on {connectable.url}\")\n\n    async with connectable.connect() as connection:\n        logger.info(\"Connected to database\")\n        await connection.run_sync(do_run_migrations)\n        logger.info(\"Migrations complete\")\n    logger.info(\"Disconnecting from database\")\n    await connectable.dispose()\n    logger.info(\"Disconnected from database\")\n\n\nif context.is_offline_mode():\n    run_migrations_offline()\nelse:\n    connection = config.attributes.get(\"connection\", None)\n    if connection is None:\n        asyncio.run(run_async_migrations())\n    else:\n        do_run_migrations(connection)\n", "inference/server/alembic/versions/2023_03_22_2113-78f16015b904_add_refresh_token_table.py": "\"\"\"Add refresh token table\n\nRevision ID: 78f16015b904\nRevises: 629d5081160f\nCreate Date: 2023-03-22 21:13:41.411718\n\n\"\"\"\nimport sqlalchemy as sa\nimport sqlmodel\nfrom alembic import op\n\n# revision identifiers, used by Alembic.\nrevision = \"78f16015b904\"\ndown_revision = \"629d5081160f\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.create_table(\n        \"refresh_token\",\n        sa.Column(\"token_hash\", sqlmodel.sql.sqltypes.AutoString(), nullable=False),\n        sa.Column(\"user_id\", sqlmodel.sql.sqltypes.AutoString(), nullable=False),\n        sa.Column(\"enabled\", sa.Boolean(), nullable=False),\n        sa.ForeignKeyConstraint(\n            [\"user_id\"],\n            [\"user.id\"],\n        ),\n        sa.PrimaryKeyConstraint(\"token_hash\"),\n    )\n    op.create_index(op.f(\"ix_refresh_token_user_id\"), \"refresh_token\", [\"user_id\"], unique=False)\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_index(op.f(\"ix_refresh_token_user_id\"), table_name=\"refresh_token\")\n    op.drop_table(\"refresh_token\")\n    # ### end Alembic commands ###\n", "inference/server/alembic/versions/2023_05_01_2253-5b4211625a9f_added_used_plugin_to_message.py": "\"\"\"added used plugin to message\n\nRevision ID: 5b4211625a9f\nRevises: ea19bbc743f9\nCreate Date: 2023-05-01 22:53:16.297495\n\n\"\"\"\nimport sqlalchemy as sa\nfrom alembic import op\nfrom sqlalchemy.dialects import postgresql\n\n# revision identifiers, used by Alembic.\nrevision = \"5b4211625a9f\"\ndown_revision = \"ea19bbc743f9\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.add_column(\"message\", sa.Column(\"used_plugin\", postgresql.JSONB(astext_type=sa.Text()), nullable=True))\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_column(\"message\", \"used_plugin\")\n    # ### end Alembic commands ###\n", "inference/server/alembic/versions/2023_04_12_2033-f0e18084aae4_add_deleted_field_to_user.py": "\"\"\"Add deleted field to user\n\nRevision ID: f0e18084aae4\nRevises: 78f16015b904\nCreate Date: 2023-04-12 20:33:28.239793\n\n\"\"\"\nimport sqlalchemy as sa\nfrom alembic import op\n\n# revision identifiers, used by Alembic.\nrevision = \"f0e18084aae4\"\ndown_revision = \"78f16015b904\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.add_column(\"user\", sa.Column(\"deleted\", sa.Boolean(), server_default=sa.text(\"false\"), nullable=False))\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_column(\"user\", \"deleted\")\n    # ### end Alembic commands ###\n", "inference/server/alembic/versions/2023_05_29_1551-5ed411a331f4_add_active_thread_tail_messsage_id_and_.py": "\"\"\"add_active_thread_tail_messsage_id_and_message_eval\n\nRevision ID: 5ed411a331f4\nRevises: 5b4211625a9f\nCreate Date: 2023-05-29 15:51:41.857262\n\n\"\"\"\nimport sqlalchemy as sa\nimport sqlmodel\nfrom alembic import op\nfrom sqlalchemy.dialects import postgresql\n\n# revision identifiers, used by Alembic.\nrevision = \"5ed411a331f4\"\ndown_revision = \"5b4211625a9f\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.create_table(\n        \"message_evaluation\",\n        sa.Column(\"inferior_message_ids\", postgresql.JSONB(astext_type=sa.Text()), nullable=True),\n        sa.Column(\"id\", sqlmodel.sql.sqltypes.AutoString(), nullable=False),\n        sa.Column(\"chat_id\", sqlmodel.sql.sqltypes.AutoString(), nullable=False),\n        sa.Column(\"user_id\", sqlmodel.sql.sqltypes.AutoString(), nullable=False),\n        sa.Column(\"selected_message_id\", sqlmodel.sql.sqltypes.AutoString(), nullable=False),\n        sa.ForeignKeyConstraint(\n            [\"chat_id\"],\n            [\"chat.id\"],\n        ),\n        sa.ForeignKeyConstraint(\n            [\"selected_message_id\"],\n            [\"message.id\"],\n        ),\n        sa.ForeignKeyConstraint(\n            [\"user_id\"],\n            [\"user.id\"],\n        ),\n        sa.PrimaryKeyConstraint(\"id\"),\n    )\n    op.create_index(op.f(\"ix_message_evaluation_chat_id\"), \"message_evaluation\", [\"chat_id\"], unique=False)\n    op.create_index(op.f(\"ix_message_evaluation_user_id\"), \"message_evaluation\", [\"user_id\"], unique=False)\n    op.add_column(\"chat\", sa.Column(\"active_thread_tail_message_id\", sqlmodel.sql.sqltypes.AutoString(), nullable=True))\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_column(\"chat\", \"active_thread_tail_message_id\")\n    op.drop_index(op.f(\"ix_message_evaluation_user_id\"), table_name=\"message_evaluation\")\n    op.drop_index(op.f(\"ix_message_evaluation_chat_id\"), table_name=\"message_evaluation\")\n    op.drop_table(\"message_evaluation\")\n    # ### end Alembic commands ###\n", "inference/server/alembic/versions/2023_04_29_1739-ea19bbc743f9_add_safe_content_to_message.py": "\"\"\"Add safe_content to message\n\nRevision ID: ea19bbc743f9\nRevises: 401eef162771\nCreate Date: 2023-04-14 22:37:41.373382\n\n\"\"\"\nimport sqlalchemy as sa\nimport sqlmodel\nfrom alembic import op\n\n# revision identifiers, used by Alembic.\nrevision = \"ea19bbc743f9\"\ndown_revision = \"401eef162771\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.add_column(\"message\", sa.Column(\"safe_content\", sqlmodel.sql.sqltypes.AutoString(), nullable=True))\n    op.add_column(\"message\", sa.Column(\"safety_level\", sa.Integer(), nullable=True))\n    op.add_column(\"message\", sa.Column(\"safety_label\", sqlmodel.sql.sqltypes.AutoString(), nullable=True))\n    op.add_column(\"message\", sa.Column(\"safety_rots\", sqlmodel.sql.sqltypes.AutoString(), nullable=True))\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_column(\"message\", \"safe_content\")\n    op.drop_column(\"message\", \"safety_level\")\n    op.drop_column(\"message\", \"safety_label\")\n    op.drop_column(\"message\", \"safety_rots\")\n    # ### end Alembic commands ###\n", "inference/server/alembic/versions/2023_03_21_2116-629d5081160f_changed_worker_config_to_worker_info.py": "\"\"\"changed worker config to worker info\n\nRevision ID: 629d5081160f\nRevises: 7d5be54acd49\nCreate Date: 2023-03-21 21:16:57.999842\n\n\"\"\"\nimport sqlalchemy as sa\nfrom alembic import op\nfrom sqlalchemy.dialects import postgresql\n\n# revision identifiers, used by Alembic.\nrevision = \"629d5081160f\"\ndown_revision = \"7d5be54acd49\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.add_column(\"worker_event\", sa.Column(\"worker_info\", postgresql.JSONB(astext_type=sa.Text()), nullable=True))\n    op.drop_column(\"worker_event\", \"worker_config\")\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.add_column(\n        \"worker_event\",\n        sa.Column(\"worker_config\", postgresql.JSONB(astext_type=sa.Text()), autoincrement=False, nullable=True),\n    )\n    op.drop_column(\"worker_event\", \"worker_info\")\n    # ### end Alembic commands ###\n", "inference/server/alembic/versions/2023_03_12_1742-7d5be54acd49_initial_revision.py": "\"\"\"initial revision\n\nRevision ID: 7d5be54acd49\nRevises:\nCreate Date: 2023-03-12 17:42:42.807459\n\n\"\"\"\nimport sqlalchemy as sa\nimport sqlmodel\nfrom alembic import op\nfrom sqlalchemy.dialects import postgresql\n\n# revision identifiers, used by Alembic.\nrevision = \"7d5be54acd49\"\ndown_revision = None\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.create_table(\n        \"user\",\n        sa.Column(\"id\", sqlmodel.sql.sqltypes.AutoString(), nullable=False),\n        sa.Column(\"provider\", sqlmodel.sql.sqltypes.AutoString(), nullable=False),\n        sa.Column(\"provider_account_id\", sqlmodel.sql.sqltypes.AutoString(), nullable=False),\n        sa.Column(\"display_name\", sqlmodel.sql.sqltypes.AutoString(length=256), nullable=False),\n        sa.PrimaryKeyConstraint(\"id\"),\n    )\n    op.create_index(op.f(\"ix_user_provider\"), \"user\", [\"provider\"], unique=False)\n    op.create_index(op.f(\"ix_user_provider_account_id\"), \"user\", [\"provider_account_id\"], unique=False)\n    op.create_index(\"provider\", \"user\", [\"provider_account_id\"], unique=True)\n    op.create_table(\n        \"worker\",\n        sa.Column(\"id\", sqlmodel.sql.sqltypes.AutoString(), nullable=False),\n        sa.Column(\"api_key\", sqlmodel.sql.sqltypes.AutoString(), nullable=False),\n        sa.Column(\"name\", sqlmodel.sql.sqltypes.AutoString(), nullable=False),\n        sa.Column(\"trusted\", sa.Boolean(), nullable=False),\n        sa.Column(\"in_compliance_check_since\", sa.DateTime(), nullable=True),\n        sa.Column(\"next_compliance_check\", sa.DateTime(), nullable=True),\n        sa.PrimaryKeyConstraint(\"id\"),\n    )\n    op.create_index(op.f(\"ix_worker_api_key\"), \"worker\", [\"api_key\"], unique=False)\n    op.create_table(\n        \"chat\",\n        sa.Column(\"id\", sqlmodel.sql.sqltypes.AutoString(), nullable=False),\n        sa.Column(\"user_id\", sqlmodel.sql.sqltypes.AutoString(), nullable=False),\n        sa.Column(\"created_at\", sa.DateTime(), nullable=False),\n        sa.Column(\"modified_at\", sa.DateTime(), nullable=False),\n        sa.Column(\"title\", sqlmodel.sql.sqltypes.AutoString(), nullable=True),\n        sa.ForeignKeyConstraint(\n            [\"user_id\"],\n            [\"user.id\"],\n        ),\n        sa.PrimaryKeyConstraint(\"id\"),\n    )\n    op.create_index(op.f(\"ix_chat_created_at\"), \"chat\", [\"created_at\"], unique=False)\n    op.create_index(op.f(\"ix_chat_modified_at\"), \"chat\", [\"modified_at\"], unique=False)\n    op.create_index(op.f(\"ix_chat_user_id\"), \"chat\", [\"user_id\"], unique=False)\n    op.create_table(\n        \"worker_compliance_check\",\n        sa.Column(\"id\", sqlmodel.sql.sqltypes.AutoString(), nullable=False),\n        sa.Column(\"worker_id\", sqlmodel.sql.sqltypes.AutoString(), nullable=False),\n        sa.Column(\"compare_worker_id\", sqlmodel.sql.sqltypes.AutoString(), nullable=True),\n        sa.Column(\"start_time\", sa.DateTime(), nullable=False),\n        sa.Column(\"end_time\", sa.DateTime(), nullable=True),\n        sa.Column(\"responded\", sa.Boolean(), nullable=False),\n        sa.Column(\"error\", sqlmodel.sql.sqltypes.AutoString(), nullable=True),\n        sa.Column(\"passed\", sa.Boolean(), nullable=False),\n        sa.ForeignKeyConstraint(\n            [\"worker_id\"],\n            [\"worker.id\"],\n        ),\n        sa.PrimaryKeyConstraint(\"id\"),\n    )\n    op.create_index(\n        op.f(\"ix_worker_compliance_check_compare_worker_id\"),\n        \"worker_compliance_check\",\n        [\"compare_worker_id\"],\n        unique=False,\n    )\n    op.create_index(\n        op.f(\"ix_worker_compliance_check_worker_id\"), \"worker_compliance_check\", [\"worker_id\"], unique=False\n    )\n    op.create_table(\n        \"worker_event\",\n        sa.Column(\"worker_config\", postgresql.JSONB(astext_type=sa.Text()), nullable=True),\n        sa.Column(\"id\", sqlmodel.sql.sqltypes.AutoString(), nullable=False),\n        sa.Column(\"worker_id\", sqlmodel.sql.sqltypes.AutoString(), nullable=False),\n        sa.Column(\"time\", sa.DateTime(), nullable=False),\n        sa.Column(\"event_type\", sqlmodel.sql.sqltypes.AutoString(), nullable=False),\n        sa.ForeignKeyConstraint(\n            [\"worker_id\"],\n            [\"worker.id\"],\n        ),\n        sa.PrimaryKeyConstraint(\"id\"),\n    )\n    op.create_index(op.f(\"ix_worker_event_worker_id\"), \"worker_event\", [\"worker_id\"], unique=False)\n    op.create_table(\n        \"message\",\n        sa.Column(\"work_parameters\", postgresql.JSONB(astext_type=sa.Text()), nullable=True),\n        sa.Column(\"worker_config\", postgresql.JSONB(astext_type=sa.Text()), nullable=True),\n        sa.Column(\"role\", sqlmodel.sql.sqltypes.AutoString(), nullable=False),\n        sa.Column(\"id\", sqlmodel.sql.sqltypes.AutoString(), nullable=False),\n        sa.Column(\"created_at\", sa.DateTime(), nullable=False),\n        sa.Column(\"chat_id\", sqlmodel.sql.sqltypes.AutoString(), nullable=False),\n        sa.Column(\"parent_id\", sqlmodel.sql.sqltypes.AutoString(), nullable=True),\n        sa.Column(\"content\", sqlmodel.sql.sqltypes.AutoString(), nullable=True),\n        sa.Column(\"error\", sqlmodel.sql.sqltypes.AutoString(), nullable=True),\n        sa.Column(\"state\", sqlmodel.sql.sqltypes.AutoString(), nullable=False),\n        sa.Column(\"work_begin_at\", sa.DateTime(), nullable=True),\n        sa.Column(\"work_end_at\", sa.DateTime(), nullable=True),\n        sa.Column(\"worker_id\", sqlmodel.sql.sqltypes.AutoString(), nullable=True),\n        sa.Column(\"worker_compat_hash\", sqlmodel.sql.sqltypes.AutoString(), nullable=True),\n        sa.Column(\"score\", sa.Integer(), nullable=False),\n        sa.ForeignKeyConstraint(\n            [\"chat_id\"],\n            [\"chat.id\"],\n        ),\n        sa.ForeignKeyConstraint(\n            [\"worker_id\"],\n            [\"worker.id\"],\n        ),\n        sa.PrimaryKeyConstraint(\"id\"),\n    )\n    op.create_index(op.f(\"ix_message_chat_id\"), \"message\", [\"chat_id\"], unique=False)\n    op.create_index(op.f(\"ix_message_role\"), \"message\", [\"role\"], unique=False)\n    op.create_index(op.f(\"ix_message_worker_compat_hash\"), \"message\", [\"worker_compat_hash\"], unique=False)\n    op.create_table(\n        \"report\",\n        sa.Column(\"id\", sqlmodel.sql.sqltypes.AutoString(), nullable=False),\n        sa.Column(\"message_id\", sqlmodel.sql.sqltypes.AutoString(), nullable=False),\n        sa.Column(\"report_type\", sqlmodel.sql.sqltypes.AutoString(), nullable=False),\n        sa.Column(\"reason\", sqlmodel.sql.sqltypes.AutoString(), nullable=False),\n        sa.ForeignKeyConstraint(\n            [\"message_id\"],\n            [\"message.id\"],\n        ),\n        sa.PrimaryKeyConstraint(\"id\"),\n    )\n    op.create_index(op.f(\"ix_report_message_id\"), \"report\", [\"message_id\"], unique=False)\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_index(op.f(\"ix_report_message_id\"), table_name=\"report\")\n    op.drop_table(\"report\")\n    op.drop_index(op.f(\"ix_message_worker_compat_hash\"), table_name=\"message\")\n    op.drop_index(op.f(\"ix_message_role\"), table_name=\"message\")\n    op.drop_index(op.f(\"ix_message_chat_id\"), table_name=\"message\")\n    op.drop_table(\"message\")\n    op.drop_index(op.f(\"ix_worker_event_worker_id\"), table_name=\"worker_event\")\n    op.drop_table(\"worker_event\")\n    op.drop_index(op.f(\"ix_worker_compliance_check_worker_id\"), table_name=\"worker_compliance_check\")\n    op.drop_index(op.f(\"ix_worker_compliance_check_compare_worker_id\"), table_name=\"worker_compliance_check\")\n    op.drop_table(\"worker_compliance_check\")\n    op.drop_index(op.f(\"ix_chat_user_id\"), table_name=\"chat\")\n    op.drop_index(op.f(\"ix_chat_modified_at\"), table_name=\"chat\")\n    op.drop_index(op.f(\"ix_chat_created_at\"), table_name=\"chat\")\n    op.drop_table(\"chat\")\n    op.drop_index(op.f(\"ix_worker_api_key\"), table_name=\"worker\")\n    op.drop_table(\"worker\")\n    op.drop_index(\"provider\", table_name=\"user\")\n    op.drop_index(op.f(\"ix_user_provider_account_id\"), table_name=\"user\")\n    op.drop_index(op.f(\"ix_user_provider\"), table_name=\"user\")\n    op.drop_table(\"user\")\n    # ### end Alembic commands ###\n", "inference/server/alembic/versions/2023_04_14_1611-b66fd8f9da1f_add_hidden_field_to_chats.py": "\"\"\"Add hidden field to chats\n\nRevision ID: b66fd8f9da1f\nRevises: f0e18084aae4\nCreate Date: 2023-04-14 16:11:35.361507\n\n\"\"\"\nimport sqlalchemy as sa\nfrom alembic import op\n\n# revision identifiers, used by Alembic.\nrevision = \"b66fd8f9da1f\"\ndown_revision = \"f0e18084aae4\"\nbranch_labels = None\ndepends_on = None\n\n\ndef upgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.add_column(\"chat\", sa.Column(\"hidden\", sa.Boolean(), server_default=sa.text(\"false\"), nullable=False))\n    # ### end Alembic commands ###\n\n\ndef downgrade() -> None:\n    # ### commands auto generated by Alembic - please adjust! ###\n    op.drop_column(\"chat\", \"hidden\")\n    # ### end Alembic commands ###\n", "inference/server/oasst_inference_server/deps.py": "import contextlib\n\nimport fastapi\nimport redis.asyncio as redis\nfrom fastapi import Depends\nfrom fastapi_limiter.depends import RateLimiter\nfrom oasst_inference_server import auth\nfrom oasst_inference_server.chat_repository import ChatRepository\nfrom oasst_inference_server.database import AsyncSession, get_async_session\nfrom oasst_inference_server.settings import settings\nfrom oasst_inference_server.user_chat_repository import UserChatRepository\n\n\n# create async redis client\ndef make_redis_client():\n    redis_client = redis.Redis(\n        host=settings.redis_host, port=settings.redis_port, db=settings.redis_db, decode_responses=True\n    )\n    return redis_client\n\n\nredis_client = make_redis_client()\n\n\nasync def create_session():\n    async for session in get_async_session():\n        yield session\n\n\n@contextlib.asynccontextmanager\nasync def manual_create_session(autoflush=True):\n    async with contextlib.asynccontextmanager(get_async_session)(autoflush=autoflush) as session:\n        yield session\n\n\nasync def create_chat_repository(session: AsyncSession = Depends(create_session)) -> ChatRepository:\n    repository = ChatRepository(session=session)\n    return repository\n\n\nasync def create_user_chat_repository(\n    session: AsyncSession = Depends(create_session),\n    user_id: str = Depends(auth.get_current_user_id),\n) -> UserChatRepository:\n    repository = UserChatRepository(session=session, user_id=user_id)\n    return repository\n\n\n@contextlib.asynccontextmanager\nasync def manual_chat_repository():\n    async with manual_create_session() as session:\n        yield await create_chat_repository(session)\n\n\n@contextlib.asynccontextmanager\nasync def manual_user_chat_repository(user_id: str):\n    async with manual_create_session() as session:\n        yield await create_user_chat_repository(session, user_id)\n\n\nasync def user_identifier(request: fastapi.Request) -> str:\n    \"\"\"Identify a request by user based on auth header\"\"\"\n    trusted_client_token = request.headers.get(\"TrustedClient\")\n    if trusted_client_token is not None:\n        return auth.get_user_id_from_trusted_client_token(trusted_client_token)\n\n    token = request.headers.get(\"Authorization\")\n    return auth.get_user_id_from_auth_token(token)\n\n\nclass UserRateLimiter(RateLimiter):\n    def __init__(\n        self, times: int = 100, milliseconds: int = 0, seconds: int = 0, minutes: int = 1, hours: int = 0\n    ) -> None:\n        super().__init__(times, milliseconds, seconds, minutes, hours, user_identifier)\n", "inference/server/oasst_inference_server/queueing.py": "import redis.asyncio as redis\nfrom oasst_inference_server.settings import settings\n\n\nclass QueueFullException(Exception):\n    pass\n\n\nclass RedisQueue:\n    def __init__(\n        self,\n        redis_client: redis.Redis,\n        queue_id: str,\n        expire: int | None = None,\n        with_counter: bool = False,\n        counter_pos_expire: int = 1,\n        max_size: int | None = None,\n    ) -> None:\n        self.redis_client = redis_client\n        self.queue_id = queue_id\n        self.expire = expire\n        self.with_counter = with_counter\n        self.counter_pos_expire = counter_pos_expire\n        self.max_size = max_size or 0\n\n    async def enqueue(self, value: str, enforce_max_size: bool = True) -> int | None:\n        if enforce_max_size and self.max_size > 0:\n            if await self.get_length() >= self.max_size:\n                raise QueueFullException()\n        await self.redis_client.rpush(self.queue_id, value)\n        if self.expire is not None:\n            await self.set_expire(self.expire)\n        if self.with_counter:\n            ctr = await self.redis_client.incr(f\"ctr_enq:{self.queue_id}\")\n            await self.redis_client.set(f\"pos:{value}\", ctr, ex=self.counter_pos_expire)\n        else:\n            ctr = None\n        return ctr\n\n    async def dequeue(self, timeout: int = 1) -> str | None:\n        val = await self.redis_client.blpop(self.queue_id, timeout=timeout)\n        if val is not None and self.with_counter:\n            await self.redis_client.incr(f\"ctr_deq:{self.queue_id}\")\n        return val\n\n    async def set_expire(self, timeout: int) -> None:\n        return await self.redis_client.expire(self.queue_id, timeout)\n\n    async def get_enq_counter(self) -> int:\n        if not self.with_counter:\n            return 0\n        enq = await self.redis_client.get(f\"ctr_enq:{self.queue_id}\")\n        enq = int(enq) if enq is not None else 0\n        return enq\n\n    async def get_deq_counter(self) -> int:\n        if not self.with_counter:\n            return 0\n        deq = await self.redis_client.get(f\"ctr_deq:{self.queue_id}\")\n        deq = int(deq) if deq is not None else 0\n        return deq\n\n    async def get_length(self) -> int:\n        return await self.redis_client.llen(self.queue_id)\n\n\nasync def get_pos_value(redis_client: redis.Redis, message_id: str) -> int:\n    val = await redis_client.get(f\"pos:{message_id}\")\n    if val is None:\n        return 0\n    return int(val)\n\n\ndef message_queue(redis_client: redis.Redis, message_id: str) -> RedisQueue:\n    return RedisQueue(redis_client, f\"message:{message_id}\", expire=settings.message_queue_expire)\n\n\ndef work_queue(redis_client: redis.Redis, worker_compat_hash: str) -> RedisQueue:\n    if settings.allowed_worker_compat_hashes != \"*\":\n        if worker_compat_hash not in settings.allowed_worker_compat_hashes_list:\n            raise ValueError(f\"Worker compat hash {worker_compat_hash} not allowed\")\n    return RedisQueue(\n        redis_client,\n        f\"work:{worker_compat_hash}\",\n        with_counter=True,\n        counter_pos_expire=settings.message_queue_expire,\n        max_size=settings.work_queue_max_size,\n    )\n\n\ndef compliance_queue(redis_client: redis.Redis, worker_id: str) -> RedisQueue:\n    return RedisQueue(redis_client, f\"compliance:{worker_id}\")\n", "inference/server/oasst_inference_server/auth.py": "\"\"\"Logic related to authorization actions.\"\"\"\n\nimport hashlib\nimport json\nfrom datetime import datetime, timedelta\n\nimport sqlmodel\nfrom cryptography.hazmat.primitives import hashes\nfrom cryptography.hazmat.primitives.kdf.hkdf import HKDF\nfrom fastapi import HTTPException, Security\nfrom fastapi.security import APIKeyHeader\nfrom jose import jwe\nfrom jose.exceptions import JWEError\nfrom loguru import logger\nfrom oasst_inference_server import deps, models\nfrom oasst_inference_server.schemas import auth\nfrom oasst_inference_server.settings import settings\nfrom starlette.status import HTTP_400_BAD_REQUEST, HTTP_401_UNAUTHORIZED, HTTP_403_FORBIDDEN\n\nauthorization_scheme = APIKeyHeader(name=\"Authorization\", auto_error=False, scheme_name=\"Authorization\")\nrefresh_scheme = APIKeyHeader(name=\"Refresh\", auto_error=False, scheme_name=\"Refresh\")\n\ntrusted_client_scheme = APIKeyHeader(name=\"TrustedClient\", auto_error=False, scheme_name=\"TrustedClient\")\n\n\ndef get_user_id_from_trusted_client_token(trusted_client_token: str) -> str:\n    info: auth.TrustedClient = auth.TrustedClientToken(content=trusted_client_token).content\n    if info.api_key not in settings.trusted_api_keys_list:\n        raise HTTPException(status_code=HTTP_401_UNAUTHORIZED, detail=\"Unauthorized client\")\n    return info.user_id\n\n\ndef get_user_id_from_auth_token(token: str) -> str:\n    if token is None or not token.startswith(\"Bearer \"):\n        logger.warning(f\"Invalid token: {token}\")\n        raise HTTPException(status_code=HTTP_403_FORBIDDEN, detail=\"Not authenticated\")\n\n    token = token[len(\"Bearer \") :]\n    if not token:\n        logger.warning(f\"Invalid token: {token}\")\n        raise HTTPException(status_code=HTTP_403_FORBIDDEN, detail=\"Not authenticated\")\n\n    key: bytes = derive_key()\n\n    try:\n        token: bytes = jwe.decrypt(token, key)\n    except JWEError:\n        raise HTTPException(status_code=HTTP_401_UNAUTHORIZED, detail=\"Invalid token\")\n\n    payload: dict = json.loads(token.decode())\n    validate_access_token(payload)\n\n    user_id = payload.get(\"user_id\")\n    return user_id\n\n\ndef get_current_user_id(\n    token: str = Security(authorization_scheme), trusted_client_token: str = Security(trusted_client_scheme)\n) -> str:\n    \"\"\"Get the current user ID.\"\"\"\n    if trusted_client_token is not None:\n        return get_user_id_from_trusted_client_token(trusted_client_token)\n\n    return get_user_id_from_auth_token(token)\n\n\ndef create_access_token(user_id: str) -> str:\n    \"\"\"Create encoded JSON Web Token (JWT) for the given user ID.\"\"\"\n    payload: bytes = build_payload(user_id, \"access\", settings.auth_access_token_expire_minutes)\n\n    key = derive_key()\n    token: bytes = jwe.encrypt(payload, key)\n\n    return token.decode()\n\n\nasync def create_refresh_token(user_id: str) -> str:\n    \"\"\"Create encoded refresh token for the given user ID.\"\"\"\n    payload: bytes = build_payload(user_id, \"refresh\", settings.auth_refresh_token_expire_minutes)\n\n    key = derive_key()\n    token: bytes = jwe.encrypt(payload, key)\n\n    await store_refresh_token(token, user_id)\n\n    return token.decode()\n\n\nasync def refresh_access_token(refresh_token: str) -> str:\n    \"\"\"Refresh the access token using the given refresh token.\"\"\"\n    key: bytes = derive_key()\n\n    try:\n        token: bytes = jwe.decrypt(refresh_token, key)\n    except JWEError:\n        raise HTTPException(status_code=HTTP_401_UNAUTHORIZED, detail=\"Invalid token\")\n\n    token_model = await query_refresh_token(token)\n\n    if not token_model or not token_model.enabled:\n        raise HTTPException(status_code=HTTP_401_UNAUTHORIZED, detail=\"Invalid token\")\n\n    payload: dict = json.loads(token.decode())\n    validate_refresh_token(payload, token_model.user_id)\n\n    user_id = payload.get(\"user_id\")\n    access_token: str = create_access_token(user_id)\n    return access_token\n\n\ndef derive_key() -> bytes:\n    \"\"\"Derive a key from the auth secret.\"\"\"\n    hkdf = HKDF(\n        algorithm=hashes.SHA256(),\n        length=settings.auth_length,\n        salt=settings.auth_salt,\n        info=settings.auth_info,\n    )\n    key = hkdf.derive(settings.auth_secret)\n    return key\n\n\ndef build_payload(user_id: str, token_type: str, expire_minutes: int) -> bytes:\n    \"\"\"Build a token payload as `bytes` encoded from a JSON string.\"\"\"\n    expires_delta = timedelta(minutes=expire_minutes)\n    expire = datetime.utcnow() + expires_delta\n\n    payload_dict = {\n        \"user_id\": user_id,\n        \"exp\": expire.timestamp(),\n        \"type\": token_type,\n    }\n\n    payload: bytes = json.dumps(payload_dict).encode()\n    return payload\n\n\nasync def store_refresh_token(token: bytes, user_id: str) -> None:\n    \"\"\"Store a refresh token in the backend DB.\"\"\"\n    token_hash: bytes = hashlib.sha256(token).hexdigest()\n\n    async with deps.manual_create_session() as session:\n        token_model: models.DbRefreshToken = models.DbRefreshToken(token_hash=token_hash, user_id=user_id)\n        session.add(token_model)\n        await session.commit()\n\n\nasync def query_refresh_token(token: bytes) -> models.DbRefreshToken | None:\n    \"\"\"Query a refresh token in the backend DB.\"\"\"\n    token_hash: bytes = hashlib.sha256(token).hexdigest()\n\n    async with deps.manual_create_session() as session:\n        query = sqlmodel.select(models.DbRefreshToken).where(models.DbRefreshToken.token_hash == token_hash)\n        token_model: models.DbRefreshToken = (await session.exec(query)).one_or_none()\n\n    return token_model\n\n\ndef validate_access_token(payload: dict) -> None:\n    \"\"\"Validate an access token payload.\"\"\"\n    user_id = payload.get(\"user_id\")\n    exp = payload.get(\"exp\")\n    token_type = payload.get(\"type\")\n\n    if not user_id or not exp:\n        raise HTTPException(status_code=HTTP_401_UNAUTHORIZED, detail=\"Invalid token\")\n\n    if not token_type or token_type != \"access\":\n        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail=\"Invalid token type\")\n\n    if datetime.utcnow() >= datetime.fromtimestamp(exp):\n        raise HTTPException(status_code=HTTP_401_UNAUTHORIZED, detail=\"Token expired\")\n\n\ndef validate_refresh_token(payload: dict, compare_user_id: str) -> None:\n    \"\"\"Validate a refresh token payload and confirm it corresponds to the correct user.\"\"\"\n    user_id = payload.get(\"user_id\")\n    exp = payload.get(\"exp\")\n    token_type = payload.get(\"type\")\n\n    if not exp or not user_id or user_id != compare_user_id:\n        raise HTTPException(status_code=HTTP_401_UNAUTHORIZED, detail=\"Invalid token\")\n\n    if not token_type or token_type != \"refresh\":\n        raise HTTPException(status_code=HTTP_400_BAD_REQUEST, detail=\"Invalid token type\")\n\n    if datetime.utcnow() >= datetime.fromtimestamp(exp):\n        raise HTTPException(status_code=HTTP_401_UNAUTHORIZED, detail=\"Token expired\")\n", "inference/server/oasst_inference_server/chat_utils.py": "from oasst_inference_server.settings import settings\nfrom oasst_shared import model_configs\n\n\ndef get_model_config(model_config_name: str) -> model_configs.ModelConfig:\n    \"\"\"Get a `ModelConfig` by its name. See `oasst_shared.model_configs`.\"\"\"\n    if settings.allowed_model_config_names != \"*\":\n        if model_config_name not in settings.allowed_model_config_names_list:\n            raise ValueError(f\"Model {model_config_name} not in allowed models: {settings.allowed_model_config_names}\")\n\n    model_config = model_configs.MODEL_CONFIGS.get(model_config_name)\n    if model_config is None:\n        raise ValueError(\n            f\"Model {model_config_name} not found\",\n        )\n    return model_config\n", "inference/server/oasst_inference_server/admin.py": "\"\"\"Logic related to admin actions.\"\"\"\n\nimport fastapi\nfrom loguru import logger\nfrom oasst_inference_server import database, models\nfrom oasst_shared import utils as shared_utils\n\n\nasync def delete_user_from_db(session: database.AsyncSession, user_id: str):\n    \"\"\"Deletes a user.\"\"\"\n    logger.info(f\"Deleting user {user_id}\")\n    user = await session.get(models.DbUser, user_id)\n    if not user:\n        raise fastapi.HTTPException(\n            status_code=fastapi.status.HTTP_404_NOT_FOUND,\n            detail=\"User not found\",\n        )\n    user.deleted = True\n\n    # Anonymise user data\n    user.display_name = shared_utils.DELETED_USER_DISPLAY_NAME\n    # Ensure uniqueness\n    user.provider_account_id = f\"{shared_utils.DELETED_USER_ID_PREFIX}{user.id}\"\n\n    await session.commit()\n", "inference/server/oasst_inference_server/settings.py": "from typing import Any\n\nimport pydantic\n\n\ndef split_keys_string(keys: str | None):\n    if not keys:\n        return []\n    return list(filter(bool, keys.split(\",\")))\n\n\nclass Settings(pydantic.BaseSettings):\n    PROJECT_NAME: str = \"open-assistant inference server\"\n    redis_host: str = \"localhost\"\n    redis_port: int = 6379\n    redis_db: int = 0\n    redis_ratelim_db: int = 1\n\n    message_queue_expire: int = 60\n    work_queue_max_size: int | None = None\n\n    chat_max_messages: int | None = None\n    message_max_length: int | None = None\n\n    rate_limit: bool = True\n    rate_limit_messages_user_times: int = 20\n    rate_limit_messages_user_seconds: int = 600\n\n    allowed_worker_compat_hashes: str = \"*\"\n\n    @property\n    def allowed_worker_compat_hashes_list(self) -> list[str]:\n        return self.allowed_worker_compat_hashes.split(\",\")\n\n    allowed_model_config_names: str = \"*\"\n\n    @property\n    def allowed_model_config_names_list(self) -> list[str]:\n        return self.allowed_model_config_names.split(\",\")\n\n    sse_retry_timeout: int = 15000\n    update_alembic: bool = True\n    alembic_retries: int = 5\n    alembic_retry_timeout: int = 1\n\n    postgres_host: str = \"localhost\"\n    postgres_port: str = \"5432\"\n    postgres_user: str = \"postgres\"\n    postgres_password: str = \"postgres\"\n    postgres_db: str = \"postgres\"\n\n    database_uri: str | None = None\n\n    @pydantic.validator(\"database_uri\", pre=True)\n    def assemble_db_connection(cls, v: str | None, values: dict[str, Any]) -> Any:\n        if isinstance(v, str):\n            return v\n        return pydantic.PostgresDsn.build(\n            scheme=\"postgresql+asyncpg\",\n            user=values.get(\"postgres_user\"),\n            password=values.get(\"postgres_password\"),\n            host=values.get(\"postgres_host\"),\n            port=values.get(\"postgres_port\"),\n            path=f\"/{values.get('postgres_db') or ''}\",\n        )\n\n    db_pool_size: int = 75\n    db_max_overflow: int = 20\n    db_echo: bool = False\n\n    root_token: str = \"1234\"\n\n    debug_api_keys: str = \"\"\n\n    @property\n    def debug_api_keys_list(self) -> list[str]:\n        return split_keys_string(self.debug_api_keys)\n\n    trusted_client_keys: str | None\n\n    @property\n    def trusted_api_keys_list(self) -> list[str]:\n        return split_keys_string(self.trusted_client_keys)\n\n    do_compliance_checks: bool = False\n    compliance_check_interval: int = 60\n    compliance_check_timeout: int = 60\n\n    # url of this server\n    api_root: str = \"http://localhost:8000\"\n\n    allow_debug_auth: bool = False\n\n    session_middleware_secret_key: str = \"\"\n\n    auth_info: bytes = b\"NextAuth.js Generated Encryption Key\"\n    auth_salt: bytes = b\"\"\n    auth_length: int = 32\n    auth_secret: bytes = b\"\"\n    auth_algorithm: str = \"HS256\"\n    auth_access_token_expire_minutes: int = 60\n    auth_refresh_token_expire_minutes: int = 60 * 24 * 7\n\n    auth_discord_client_id: str = \"\"\n    auth_discord_client_secret: str = \"\"\n\n    auth_github_client_id: str = \"\"\n    auth_github_client_secret: str = \"\"\n\n    auth_google_client_id: str = \"\"\n    auth_google_client_secret: str = \"\"\n\n    pending_event_interval: int = 1\n    worker_ping_interval: int = 3\n\n    assistant_message_timeout: int = 60\n\n    inference_cors_origins: str = \"*\"\n\n    # sent as a work parameter, higher values increase load on workers\n    plugin_max_depth: int = 4\n\n    # url path prefix for plugins we host on this server\n    plugins_path_prefix: str = \"/plugins\"\n\n    @property\n    def inference_cors_origins_list(self) -> list[str]:\n        return self.inference_cors_origins.split(\",\")\n\n\nsettings = Settings()\n", "inference/server/oasst_inference_server/plugin_utils.py": "import asyncio\nimport json\n\nimport aiohttp\nimport yaml\nfrom aiohttp.client_exceptions import ClientConnectorError, ServerTimeoutError\nfrom fastapi import HTTPException\nfrom loguru import logger\nfrom oasst_shared.schemas import inference\n\n\nasync def attempt_fetch_plugin(session: aiohttp.ClientSession, url: str, timeout: float = 5.0):\n    \"\"\"Attempt to fetch a plugin specification from the given URL once.\"\"\"\n    async with session.get(url, timeout=timeout) as response:\n        content_type = response.headers.get(\"Content-Type\")\n\n        if response.status == 404:\n            raise HTTPException(status_code=404, detail=\"Plugin not found\")\n        if response.status != 200:\n            raise HTTPException(status_code=500, detail=\"Failed to fetch plugin\")\n\n        if \"application/json\" in content_type or \"text/json\" in content_type or url.endswith(\".json\"):\n            if \"text/json\" in content_type:\n                logger.warning(f\"Plugin {url} is using text/json as its content type. This is not recommended.\")\n                config = json.loads(await response.text())\n            else:\n                config = await response.json()\n        elif (\n            \"application/yaml\" in content_type\n            or \"application/x-yaml\" in content_type\n            or url.endswith(\".yaml\")\n            or url.endswith(\".yml\")\n        ):\n            config = yaml.safe_load(await response.text())\n        else:\n            raise HTTPException(\n                status_code=400,\n                detail=f\"Unsupported content type: {content_type}. Only JSON and YAML are supported.\",\n            )\n\n    return inference.PluginConfig(**config)\n\n\nasync def fetch_plugin(url: str, retries: int = 3, timeout: float = 5.0) -> inference.PluginConfig:\n    \"\"\"Fetch a plugin specification from the given URL, with retries using exponential backoff.\"\"\"\n    async with aiohttp.ClientSession() as session:\n        for attempt in range(retries):\n            try:\n                plugin_config = await attempt_fetch_plugin(session, url, timeout=timeout)\n                return plugin_config\n            except (ClientConnectorError, ServerTimeoutError) as e:\n                if attempt == retries - 1:\n                    raise HTTPException(status_code=500, detail=f\"Request failed after {retries} retries: {e}\")\n                await asyncio.sleep(2**attempt)  # exponential backoff\n            except aiohttp.ClientError as e:\n                raise HTTPException(status_code=500, detail=f\"Request failed: {e}\")\n    raise HTTPException(status_code=500, detail=\"Failed to fetch plugin\")\n", "inference/server/oasst_inference_server/chat_repository.py": "import datetime\n\nimport fastapi\nimport pydantic\nimport sqlalchemy.orm\nimport sqlmodel\nfrom loguru import logger\nfrom oasst_inference_server import database, models\nfrom oasst_inference_server.schemas import chat as chat_schema\nfrom oasst_inference_server.settings import settings\nfrom oasst_shared.schemas import inference\n\n\nclass ChatRepository(pydantic.BaseModel):\n    \"\"\"Wrapper around a database session providing functionality relating to chats.\"\"\"\n\n    session: database.AsyncSession\n\n    class Config:\n        arbitrary_types_allowed = True\n\n    async def get_assistant_message_by_id(self, message_id: str) -> models.DbMessage:\n        query = (\n            sqlmodel.select(models.DbMessage)\n            .options(sqlalchemy.orm.selectinload(models.DbMessage.reports))\n            .where(models.DbMessage.id == message_id, models.DbMessage.role == \"assistant\")\n        )\n        message = (await self.session.exec(query)).one()\n        return message\n\n    async def get_prompter_message_by_id(self, message_id: str) -> models.DbMessage:\n        query = (\n            sqlmodel.select(models.DbMessage)\n            .options(sqlalchemy.orm.selectinload(models.DbMessage.reports))\n            .where(models.DbMessage.id == message_id, models.DbMessage.role == \"prompter\")\n        )\n        message = (await self.session.exec(query)).one()\n        return message\n\n    async def start_work(\n        self, *, message_id: str, worker_id: str, worker_config: inference.WorkerConfig\n    ) -> models.DbMessage:\n        \"\"\"\n        Update an assistant message in the database to be allocated to a specific worker.\n        The message must be in `pending` state. An exception is raised if the message has timed out or was cancelled.\n        \"\"\"\n        logger.debug(f\"Starting work on message {message_id}\")\n        message = await self.get_assistant_message_by_id(message_id)\n\n        if settings.assistant_message_timeout > 0:\n            message_age_in_seconds = (datetime.datetime.utcnow() - message.created_at).total_seconds()\n            if message_age_in_seconds > settings.assistant_message_timeout:\n                message.state = inference.MessageState.timeout\n                await self.session.commit()\n                await self.session.refresh(message)\n                raise chat_schema.MessageTimeoutException(message=message.to_read())\n\n        if message.state == inference.MessageState.cancelled:\n            raise chat_schema.MessageCancelledException(message_id=message_id)\n\n        if message.state != inference.MessageState.pending:\n            raise fastapi.HTTPException(status_code=400, detail=\"Message is not pending\")\n\n        message.state = inference.MessageState.in_progress\n        message.work_begin_at = datetime.datetime.utcnow()\n        message.worker_id = worker_id\n        message.worker_config = worker_config\n        await self.session.commit()\n        logger.debug(f\"Started work on message {message_id}\")\n        await self.session.refresh(message)\n        return message\n\n    async def reset_work(self, message_id: str) -> models.DbMessage:\n        \"\"\"\n        Update an assistant message in the database which has already been allocated to a worker to remove the\n        allocation and reset the message state to `pending`.\n        \"\"\"\n        logger.warning(f\"Resetting work on message {message_id}\")\n        message = await self.get_assistant_message_by_id(message_id)\n        message.state = inference.MessageState.pending\n        message.work_begin_at = None\n        message.worker_id = None\n        message.worker_compat_hash = None\n        message.worker_config = None\n        await self.session.commit()\n        logger.debug(f\"Reset work on message {message_id}\")\n        await self.session.refresh(message)\n        return message\n\n    async def abort_work(self, message_id: str, reason: str) -> models.DbMessage:\n        \"\"\"Update an assistant message in the database to mark it as having been aborted by the allocated worker.\"\"\"\n        logger.warning(f\"Aborting work on message {message_id}\")\n        message = await self.get_assistant_message_by_id(message_id)\n        message.state = inference.MessageState.aborted_by_worker\n        message.work_end_at = datetime.datetime.utcnow()\n        message.error = reason\n        await self.session.commit()\n        logger.debug(f\"Aborted work on message {message_id}\")\n        await self.session.refresh(message)\n        return message\n\n    async def complete_work(\n        self, message_id: str, content: str, used_plugin: inference.PluginUsed | None\n    ) -> models.DbMessage:\n        \"\"\"\n        Update an assistant message in the database to mark it as having been completed with the given content, also\n        updating the used plugin if one is specified.\n        \"\"\"\n        logger.debug(f\"Completing work on message {message_id}\")\n        message = await self.get_assistant_message_by_id(message_id)\n        message.state = inference.MessageState.complete\n        message.work_end_at = datetime.datetime.utcnow()\n        message.content = content\n        message.used_plugin = used_plugin\n        await self.session.commit()\n        logger.debug(f\"Completed work on message {message_id}\")\n        await self.session.refresh(message)\n        return message\n", "inference/server/oasst_inference_server/worker_utils.py": "import enum\nimport uuid\n\nimport fastapi\nimport pydantic\nimport sqlalchemy.orm\nimport sqlmodel\nfrom fastapi import Depends\nfrom loguru import logger\nfrom oasst_inference_server import database, deps, models\nfrom oasst_shared.schemas import inference\n\n\nclass WorkerSessionStatus(str, enum.Enum):\n    waiting = \"waiting\"\n    working = \"working\"\n    compliance_check = \"compliance_check\"\n\n\nclass WorkerSession(pydantic.BaseModel):\n    id: str = pydantic.Field(default_factory=lambda: str(uuid.uuid4()))\n    worker_id: str\n    worker_info: inference.WorkerInfo\n    requests_in_flight: int = 0\n    metrics: inference.WorkerMetricsInfo | None = None\n\n\napi_key_header = fastapi.Header(None, alias=\"X-API-Key\")\n\n\ndef get_api_key(api_key: str = api_key_header) -> str:\n    if api_key is None:\n        raise fastapi.HTTPException(\n            status_code=fastapi.status.HTTP_401_UNAUTHORIZED,\n            detail=\"Missing API key\",\n        )\n    return api_key\n\n\nprotocol_version_header = fastapi.Header(None, alias=\"X-Protocol-Version\")\n\n\ndef get_protocol_version(protocol_version: str = protocol_version_header) -> str:\n    if protocol_version != inference.INFERENCE_PROTOCOL_VERSION:\n        logger.warning(f\"Got worker with incompatible protocol version: {protocol_version}\")\n        raise fastapi.HTTPException(\n            status_code=fastapi.status.HTTP_426_UPGRADE_REQUIRED,\n            detail=f\"Incompatible protocol version: {protocol_version}. Expected: {inference.INFERENCE_PROTOCOL_VERSION}.\",\n        )\n    return protocol_version\n\n\nasync def get_worker_id(\n    api_key: str = Depends(get_api_key),\n    protocol_version: str = Depends(get_protocol_version),\n) -> models.DbWorker:\n    \"\"\"Get the ID of a worker from its API key and protocol version.\"\"\"\n    logger.info(f\"get_worker: {api_key=}, {protocol_version=}\")\n    query = sqlmodel.select(models.DbWorker).where(models.DbWorker.api_key == api_key)\n    async with deps.manual_create_session() as session:\n        worker: models.DbWorker = (await session.exec(query)).one_or_none()\n    if worker is None:\n        raise fastapi.HTTPException(\n            status_code=fastapi.status.HTTP_401_UNAUTHORIZED,\n            detail=\"Invalid API key\",\n        )\n    return worker.id\n\n\nasync def get_worker(\n    worker_id: str = Depends(get_worker_id),\n    session: database.AsyncSession = Depends(deps.create_session),\n) -> models.DbWorker:\n    query = sqlmodel.select(models.DbWorker).where(models.DbWorker.id == worker_id)\n    worker = (await session.exec(query)).one()\n    return worker\n\n\nasync def send_worker_request(\n    websocket: fastapi.WebSocket,\n    request: inference.WorkerRequest,\n):\n    return await websocket.send_text(request.json())\n\n\nasync def receive_worker_response(\n    websocket: fastapi.WebSocket,\n) -> inference.WorkerResponse:\n    return pydantic.parse_raw_as(inference.WorkerResponse, await websocket.receive_text())\n\n\nasync def receive_worker_info(\n    websocket: fastapi.WebSocket,\n) -> inference.WorkerInfo:\n    return inference.WorkerInfo.parse_raw(await websocket.receive_text())\n\n\nasync def store_worker_session(worker_session: WorkerSession):\n    await deps.redis_client.set(f\"worker_session:{worker_session.id}\", worker_session.json())\n\n\nasync def delete_worker_session(worker_session_id: str):\n    await deps.redis_client.delete(f\"worker_session:{worker_session_id}\")\n    logger.debug(f\"Deleted worker session {worker_session_id}\")\n\n\nasync def build_work_request(\n    session: database.AsyncSession,\n    message_id: str,\n) -> inference.WorkRequest:\n    \"\"\"\n    Build a work request based on the assistant message associated with the given ID in the database.\n    This will build a chat history based on the parents of the assistant message which will form the work request along\n    with the work parameters associated with the assistant message.\n    \"\"\"\n    query = (\n        sqlmodel.select(models.DbMessage)\n        .options(\n            sqlalchemy.orm.selectinload(models.DbMessage.chat)\n            .selectinload(models.DbChat.messages)\n            .selectinload(models.DbMessage.reports),\n        )\n        .where(models.DbMessage.id == message_id)\n    )\n    message: models.DbMessage = (await session.exec(query)).one()\n    chat = message.chat\n    msg_dict = chat.get_msg_dict()\n    thread_msgs = [msg_dict[message.parent_id]]\n    while thread_msgs[-1].parent_id is not None:\n        thread_msgs.append(msg_dict[thread_msgs[-1].parent_id])\n    thread = inference.Thread(\n        messages=[m.to_read() for m in reversed(thread_msgs)],\n    )\n    return inference.WorkRequest(\n        thread=thread,\n        parameters=message.work_parameters,\n    )\n", "inference/server/oasst_inference_server/compliance.py": "\"\"\"Logic related to worker compliance checks, which seek to ensure workers do not produce malicious responses.\"\"\"\n\nimport datetime\nfrom typing import cast\n\nimport fastapi\nimport sqlmodel\nfrom loguru import logger\nfrom oasst_inference_server import database, deps, models, worker_utils\nfrom oasst_inference_server.settings import settings\nfrom oasst_shared.schemas import inference\nfrom sqlalchemy.sql.functions import random as sql_random\nfrom sqlmodel import not_, or_\n\n\nasync def find_compliance_work_request_message(\n    session: database.AsyncSession, worker_config: inference.WorkerConfig, worker_id: str\n) -> models.DbMessage | None:\n    \"\"\"\n    Find a suitable assistant message to carry out a worker compliance check for the given worker. Such a message must\n    have been generated by a different worker, but one with the same compatibility hash as the given worker.\n    \"\"\"\n    compat_hash = worker_config.compat_hash\n    query = (\n        sqlmodel.select(models.DbMessage)\n        .where(\n            models.DbMessage.role == \"assistant\",\n            models.DbMessage.state == inference.MessageState.complete,\n            models.DbMessage.worker_compat_hash == compat_hash,\n            models.DbMessage.worker_id != worker_id,\n        )\n        .order_by(sql_random())\n    )\n    message = (await session.exec(query)).first()\n    return message\n\n\nasync def should_do_compliance_check(session: database.AsyncSession, worker_id: str) -> bool:\n    \"\"\"\n    Check whether we should carry out a compliance check for the given worker, based on time since last check.\n    Trusted workers are excluded.\n    \"\"\"\n    worker = await worker_utils.get_worker(worker_id, session)\n    if worker.trusted:\n        return False\n    if worker.in_compliance_check:\n        return False\n    if worker.next_compliance_check is None:\n        return True\n    if worker.next_compliance_check < datetime.datetime.utcnow():\n        return True\n    return False\n\n\nasync def run_compliance_check(websocket: fastapi.WebSocket, worker_id: str, worker_config: inference.WorkerConfig):\n    \"\"\"\n    Run a compliance check for the given worker:\n    - Find a suitable compliance check assistant message\n    - Task the worker with generating a response with the same context\n    - Compare the respons against the existing completed message\n    - Update the database with the outcome\n    \"\"\"\n    async with deps.manual_create_session() as session:\n        try:\n            worker = await worker_utils.get_worker(worker_id, session)\n            if worker.in_compliance_check:\n                logger.info(f\"Worker {worker.id} is already in compliance check\")\n                return\n            worker.in_compliance_check_since = datetime.datetime.utcnow()\n        finally:\n            await session.commit()\n\n    logger.info(f\"Running compliance check for worker {worker_id}\")\n\n    async with deps.manual_create_session(autoflush=False) as session:\n        compliance_check = models.DbWorkerComplianceCheck(worker_id=worker_id)\n\n        try:\n            message = await find_compliance_work_request_message(session, worker_config, worker_id)\n            if message is None:\n                logger.warning(\n                    f\"Could not find message for compliance check for worker {worker_id} with config {worker_config}\"\n                )\n                return\n\n            compliance_check.compare_worker_id = message.worker_id\n            compliance_work_request = await worker_utils.build_work_request(session, message.id)\n\n            logger.info(f\"Found work request for compliance check for worker {worker_id}: {compliance_work_request}\")\n            await worker_utils.send_worker_request(websocket, compliance_work_request)\n            response = None\n            while True:\n                response = await worker_utils.receive_worker_response(websocket)\n                if response.response_type == \"error\":\n                    compliance_check.responded = True\n                    compliance_check.error = response.error\n                    logger.warning(f\"Worker {worker_id} errored during compliance check: {response.error}\")\n                    return\n                if response.response_type == \"generated_text\":\n                    break\n            if response is None:\n                logger.warning(f\"Worker {worker_id} did not respond to compliance check\")\n                return\n            compliance_check.responded = True\n            response = cast(inference.GeneratedTextResponse, response)\n            passes = response.text == message.content\n            compliance_check.passed = passes\n            logger.info(f\"Worker {worker_id} passed compliance check: {passes}\")\n\n        finally:\n            compliance_check.end_time = datetime.datetime.utcnow()\n            session.add(compliance_check)\n            worker = await worker_utils.get_worker(worker_id, session)\n            worker.next_compliance_check = datetime.datetime.utcnow() + datetime.timedelta(\n                seconds=settings.compliance_check_interval\n            )\n            worker.in_compliance_check_since = None\n            logger.info(f\"set next compliance check for worker {worker_id} to {worker.next_compliance_check}\")\n            await session.commit()\n            await session.flush()\n\n\nasync def maybe_do_compliance_check(websocket, worker_id, worker_config, worker_session_id):\n    async with deps.manual_create_session() as session:\n        should_check = await should_do_compliance_check(session, worker_id)\n    if should_check:\n        logger.info(f\"Worker {worker_id} needs compliance check\")\n        try:\n            await worker_utils.update_worker_session_status(\n                worker_session_id, worker_utils.WorkerSessionStatus.compliance_check\n            )\n            await run_compliance_check(websocket, worker_id, worker_config)\n        finally:\n            await worker_utils.update_worker_session_status(worker_session_id, worker_utils.WorkerSessionStatus.waiting)\n\n\nasync def compute_worker_compliance_score(worker_id: str) -> float:\n    \"\"\"\n    Compute a float between 0 and 1 (inclusive) representing the compliance score of the worker.\n    Workers are rewarded for passing compliance checks, and penalised for failing to respond to a check, erroring during a check, or failing a check.\n    In-progress checks are ignored.\n    \"\"\"\n    async with deps.manual_create_session() as session:\n        query = sqlmodel.select(models.DbWorkerComplianceCheck).where(\n            or_(\n                models.DbWorkerComplianceCheck.worker_id == worker_id,\n                models.DbWorkerComplianceCheck.compare_worker_id == worker_id,\n            ),\n            not_(models.DbWorkerComplianceCheck.end_time.is_(None)),\n        )\n        worker_checks: list[models.DbWorkerComplianceCheck] = (await session.exec(query)).all()\n\n        # Rudimentary scoring algorithm, we may want to add weightings or other factors\n        total_count = len(worker_checks)\n\n        checked = [c for c in worker_checks if c.worker_id == worker_id]\n        compared = [c for c in worker_checks if c.compare_worker_id == worker_id]\n\n        pass_count = sum(1 for _ in filter(lambda c: c.passed, checked))\n        error_count = sum(1 for _ in filter(lambda c: c.error is not None, checked))\n        no_response_count = sum(1 for _ in filter(lambda c: not c.responded, checked))\n\n        compare_fail_count = sum(1 for _ in filter(lambda c: not c.passed, compared))\n        fail_count = len(checked) - pass_count - error_count - no_response_count\n\n        return (fail_count + compare_fail_count) / total_count\n", "inference/server/oasst_inference_server/__init__.py": "", "inference/server/oasst_inference_server/user_chat_repository.py": "import fastapi\nimport pydantic\nimport sqlalchemy.orm\nimport sqlmodel\nfrom loguru import logger\nfrom oasst_inference_server import database, models\nfrom oasst_inference_server.settings import settings\nfrom oasst_shared.schemas import inference\n\n\nclass UserChatRepository(pydantic.BaseModel):\n    \"\"\"Wrapper around a database session providing user-specific functionality relating to chats.\"\"\"\n\n    session: database.AsyncSession\n    user_id: str = pydantic.Field(..., min_length=1)\n\n    class Config:\n        arbitrary_types_allowed = True\n\n    async def get_chats(\n        self,\n        include_hidden: bool = False,\n        limit: int | None = None,\n        before: str | None = None,\n        after: str | None = None,\n    ) -> list[models.DbChat]:\n        if after is not None and before is not None:\n            raise fastapi.HTTPException(status_code=400, detail=\"Cannot specify both after and before.\")\n\n        query = sqlmodel.select(models.DbChat)\n        query = query.where(models.DbChat.user_id == self.user_id)\n\n        if not include_hidden:\n            query = query.where(models.DbChat.hidden.is_(False))\n        if limit is not None:\n            query = query.limit(limit)\n        if before is not None:\n            query = query.where(models.DbChat.id > before)\n        if after is not None:\n            query = query.where(models.DbChat.id < after)\n\n        query = query.order_by(models.DbChat.created_at.desc() if before is None else models.DbChat.created_at)\n\n        return (await self.session.exec(query)).all()\n\n    async def get_chat_by_id(self, chat_id: str, include_messages: bool = True) -> models.DbChat:\n        query = sqlmodel.select(models.DbChat).where(\n            models.DbChat.id == chat_id,\n            models.DbChat.user_id == self.user_id,\n        )\n        if include_messages:\n            query = query.options(\n                sqlalchemy.orm.selectinload(models.DbChat.messages).selectinload(models.DbMessage.reports),\n            )\n\n        chat = (await self.session.exec(query)).one_or_none()\n        if chat is None:\n            raise fastapi.HTTPException(status_code=404, detail=\"Chat not found\")\n        return chat\n\n    async def get_message_by_id(self, chat_id: str, message_id: str) -> models.DbMessage:\n        query = (\n            sqlmodel.select(models.DbMessage)\n            .where(\n                models.DbMessage.id == message_id,\n                models.DbMessage.chat_id == chat_id,\n            )\n            .options(\n                sqlalchemy.orm.selectinload(models.DbMessage.reports),\n            )\n            .join(models.DbChat)\n            .where(\n                models.DbChat.user_id == self.user_id,\n            )\n        )\n        message = (await self.session.exec(query)).one()\n        return message\n\n    async def create_chat(self) -> models.DbChat:\n        # Try to find the user first\n        user: models.DbUser = (\n            await self.session.execute(sqlmodel.select(models.DbUser).where(models.DbUser.id == self.user_id))\n        ).one_or_none()\n        if not user:\n            raise fastapi.HTTPException(status_code=404, detail=\"User not found\")\n        chat = models.DbChat(user_id=self.user_id)\n        self.session.add(chat)\n        await self.session.commit()\n        return chat\n\n    async def delete_chat(self, chat_id: str) -> models.DbChat:\n        chat = await self.get_chat_by_id(chat_id)\n        if chat is None:\n            raise fastapi.HTTPException(status_code=403)\n        logger.debug(f\"Deleting {chat_id=}\")\n        message_ids = [message.id for message in chat.messages]\n        # delete reports associated with messages\n        await self.session.exec(sqlmodel.delete(models.DbReport).where(models.DbReport.message_id.in_(message_ids)))\n        # delete message evaluations associated with message\n        await self.session.exec(\n            sqlmodel.delete(models.DbMessageEval).where(models.DbMessageEval.selected_message_id.in_(message_ids))\n        )\n        # delete messages\n        await self.session.exec(sqlmodel.delete(models.DbMessage).where(models.DbMessage.chat_id == chat_id))\n        # delete chat\n        await self.session.exec(\n            sqlmodel.delete(models.DbChat).where(\n                models.DbChat.id == chat_id,\n                models.DbChat.user_id == self.user_id,\n            )\n        )\n        await self.session.commit()\n\n    async def add_prompter_message(self, chat_id: str, parent_id: str | None, content: str) -> models.DbMessage:\n        logger.info(f\"Adding prompter message {len(content)=} to chat {chat_id}\")\n\n        if settings.message_max_length is not None:\n            if len(content) > settings.message_max_length:\n                raise fastapi.HTTPException(status_code=413, detail=\"Message content exceeds max length\")\n\n        chat: models.DbChat = (\n            await self.session.exec(\n                sqlmodel.select(models.DbChat)\n                .options(sqlalchemy.orm.selectinload(models.DbChat.messages))\n                .where(\n                    models.DbChat.id == chat_id,\n                    models.DbChat.user_id == self.user_id,\n                )\n            )\n        ).one()\n        if settings.chat_max_messages is not None:\n            if len(chat.messages) >= settings.chat_max_messages:\n                raise fastapi.HTTPException(status_code=413, detail=\"Maximum number of messages reached for this chat\")\n        if parent_id is None:\n            if len(chat.messages) > 0:\n                raise fastapi.HTTPException(status_code=400, detail=\"Trying to add first message to non-empty chat\")\n            if chat.title is None:\n                chat.title = content\n        else:\n            msg_dict = chat.get_msg_dict()\n            if parent_id not in msg_dict:\n                raise fastapi.HTTPException(status_code=400, detail=\"Parent message not found\")\n            if msg_dict[parent_id].role != \"assistant\":\n                raise fastapi.HTTPException(status_code=400, detail=\"Parent message is not an assistant message\")\n            if msg_dict[parent_id].state != inference.MessageState.complete:\n                raise fastapi.HTTPException(status_code=400, detail=\"Parent message is not complete\")\n\n        message = models.DbMessage(role=\"prompter\", chat_id=chat_id, chat=chat, parent_id=parent_id, content=content)\n        self.session.add(message)\n        chat.modified_at = message.created_at\n\n        await self.session.commit()\n        logger.debug(f\"Added prompter message {len(content)=} to chat {chat_id}\")\n        query = (\n            sqlmodel.select(models.DbMessage)\n            .options(\n                sqlalchemy.orm.selectinload(models.DbMessage.chat)\n                .selectinload(models.DbChat.messages)\n                .selectinload(models.DbMessage.reports),\n            )\n            .where(\n                models.DbMessage.id == message.id,\n            )\n        )\n        message = (await self.session.exec(query)).one()\n        return message\n\n    async def initiate_assistant_message(\n        self, parent_id: str, work_parameters: inference.WorkParameters, worker_compat_hash: str\n    ) -> models.DbMessage:\n        logger.info(f\"Adding stub assistant message to {parent_id=}\")\n\n        # find and cancel all pending messages by this user\n        pending_msg_query = (\n            sqlmodel.select(models.DbMessage)\n            .where(\n                models.DbMessage.role == \"assistant\",\n                models.DbMessage.state == inference.MessageState.pending,\n                models.DbMessage.parent_id != parent_id,  # Prevent draft messages from cancelling each other\n            )\n            .join(models.DbChat)\n            .where(\n                models.DbChat.user_id == self.user_id,\n            )\n        )\n\n        pending_msgs: list[models.DbMessage] = (await self.session.exec(pending_msg_query)).all()\n        for pending_msg in pending_msgs:\n            logger.warning(\n                f\"User {self.user_id} has a pending message {pending_msg.id} in chat {pending_msg.chat_id}. Cancelling...\"\n            )\n            pending_msg.state = inference.MessageState.cancelled\n            await self.session.commit()\n            logger.debug(f\"Cancelled message {pending_msg.id} in chat {pending_msg.chat_id}.\")\n\n        query = (\n            sqlmodel.select(models.DbMessage)\n            .options(sqlalchemy.orm.selectinload(models.DbMessage.chat))\n            .where(\n                models.DbMessage.id == parent_id,\n                models.DbMessage.role == \"prompter\",\n            )\n        )\n        parent: models.DbMessage = (await self.session.exec(query)).one()\n        if parent.chat.user_id != self.user_id:\n            raise fastapi.HTTPException(status_code=400, detail=\"Message not found\")\n\n        if settings.chat_max_messages is not None:\n            count_query = sqlmodel.select(sqlmodel.func.count(models.DbMessage.id)).where(\n                models.DbMessage.chat_id == parent.chat.id\n            )\n            num_msgs: int = (await self.session.exec(count_query)).one()\n\n            if num_msgs >= settings.chat_max_messages:\n                raise fastapi.HTTPException(status_code=413, detail=\"Maximum number of messages reached for this chat\")\n\n        message = models.DbMessage(\n            role=\"assistant\",\n            chat_id=parent.chat_id,\n            chat=parent.chat,\n            parent_id=parent_id,\n            state=inference.MessageState.pending,\n            work_parameters=work_parameters,\n            worker_compat_hash=worker_compat_hash,\n        )\n        self.session.add(message)\n        await self.session.commit()\n        logger.debug(f\"Initiated assistant message of {parent_id=}\")\n        query = (\n            sqlmodel.select(models.DbMessage)\n            .options(\n                sqlalchemy.orm.selectinload(models.DbMessage.chat)\n                .selectinload(models.DbChat.messages)\n                .selectinload(models.DbMessage.reports),\n            )\n            .where(models.DbMessage.id == message.id)\n        )\n        message = (await self.session.exec(query)).one()\n        return message\n\n    async def update_score(self, message_id: str, score: int) -> models.DbMessage:\n        if score < -1 or score > 1:\n            raise fastapi.HTTPException(status_code=400, detail=\"Invalid score\")\n\n        logger.info(f\"Updating message score to {message_id=}: {score=}\")\n        query = (\n            sqlmodel.select(models.DbMessage)\n            .options(sqlalchemy.orm.selectinload(models.DbMessage.chat))\n            .where(\n                models.DbMessage.id == message_id,\n                models.DbMessage.role == \"assistant\",\n            )\n        )\n        message: models.DbMessage = (await self.session.exec(query)).one()\n        if message.chat.user_id != self.user_id:\n            raise fastapi.HTTPException(status_code=400, detail=\"Message not found\")\n        message.score = score\n        await self.session.commit()\n        return message\n\n    async def add_message_eval(self, message_id: str, inferior_message_ids: list[str]):\n        logger.info(f\"Adding message evaluation to {message_id=}: {inferior_message_ids=}\")\n        query = (\n            sqlmodel.select(models.DbMessage)\n            .options(sqlalchemy.orm.selectinload(models.DbMessage.chat))\n            .where(models.DbMessage.id == message_id)\n        )\n        message: models.DbMessage = (await self.session.exec(query)).one()\n        if message.chat.user_id != self.user_id:\n            raise fastapi.HTTPException(status_code=400, detail=\"Message not found\")\n        message_eval = models.DbMessageEval(\n            chat_id=message.chat_id,\n            user_id=message.chat.user_id,\n            selected_message_id=message.id,\n            inferior_message_ids=inferior_message_ids,\n        )\n        self.session.add(message_eval)\n        await self.session.commit()\n\n    async def add_report(self, message_id: str, reason: str, report_type: inference.ReportType) -> models.DbReport:\n        logger.info(f\"Adding report to {message_id=}: {reason=}\")\n        query = (\n            sqlmodel.select(models.DbMessage)\n            .options(sqlalchemy.orm.selectinload(models.DbMessage.chat))\n            .where(\n                models.DbMessage.id == message_id,\n                models.DbMessage.role == \"assistant\",\n            )\n        )\n        message: models.DbMessage = (await self.session.exec(query)).one()\n        if message.chat.user_id != self.user_id:\n            raise fastapi.HTTPException(status_code=400, detail=\"Message not found\")\n        report = models.DbReport(message_id=message.id, reason=reason, report_type=report_type)\n        self.session.add(report)\n        await self.session.commit()\n        await self.session.refresh(report)\n        return report\n\n    async def update_chat(\n        self,\n        chat_id: str,\n        title: str | None = None,\n        hidden: bool | None = None,\n        allow_data_use: bool | None = None,\n        active_thread_tail_message_id: str | None = None,\n    ) -> None:\n        logger.info(f\"Updating chat {chat_id=}: {title=} {hidden=} {active_thread_tail_message_id=}\")\n        chat = await self.get_chat_by_id(chat_id=chat_id, include_messages=False)\n\n        if title is not None:\n            logger.info(f\"Updating title of chat {chat_id=}: {title=}\")\n            chat.title = title\n\n        if hidden is not None:\n            logger.info(f\"Setting chat {chat_id=} to {'hidden' if hidden else 'visible'}\")\n            chat.hidden = hidden\n\n        if allow_data_use is not None:\n            logger.info(f\"Updating allow_data_use of chat {chat_id=}: {allow_data_use=}\")\n            chat.allow_data_use = allow_data_use\n\n        if active_thread_tail_message_id is not None:\n            logger.info(f\"Updating active_thread_tail_message_id of chat {chat_id=}: {active_thread_tail_message_id=}\")\n            chat.active_thread_tail_message_id = active_thread_tail_message_id\n\n        await self.session.commit()\n\n    async def hide_all_chats(self) -> None:\n        chats = await self.get_chats(include_hidden=False)\n        for chat in chats:\n            chat.hidden = True\n        await self.session.commit()\n", "inference/server/oasst_inference_server/routes/auth.py": "import fastapi\nimport sqlmodel\nfrom authlib.integrations.starlette_client import OAuth\nfrom fastapi import Depends, HTTPException, Request, Security\nfrom google.oauth2.credentials import Credentials\nfrom googleapiclient.discovery import build\nfrom loguru import logger\nfrom oasst_inference_server import auth, database, deps, models\nfrom oasst_inference_server.schemas.auth import TrustedClient, TrustedClientToken\nfrom oasst_inference_server.settings import settings\nfrom oasst_shared.schemas import protocol\n\nrouter = fastapi.APIRouter(\n    prefix=\"/auth\",\n    tags=[\"auth\"],\n)\n\noauth = OAuth()\noauth_providers: list[str] = []\n\n\n@router.on_event(\"startup\")\ndef register_oauth_providers():\n    if settings.auth_discord_client_id:\n        oauth.register(\n            name=\"discord\",\n            client_id=settings.auth_discord_client_id,\n            client_secret=settings.auth_discord_client_secret,\n            access_token_url=\"https://discord.com/api/oauth2/token\",\n            authorize_url=\"https://discord.com/api/oauth2/authorize\",\n            api_base_url=\"https://discord.com/api/\",\n            client_kwargs={\"scope\": \"identify\"},\n        )\n\n        oauth_providers.append(\"discord\")\n\n    if settings.auth_github_client_id:\n        oauth.register(\n            name=\"github\",\n            client_id=settings.auth_github_client_id,\n            client_secret=settings.auth_github_client_secret,\n            access_token_url=\"https://github.com/login/oauth/access_token\",\n            authorize_url=\"https://github.com/login/oauth/authorize\",\n            api_base_url=\"https://api.github.com/\",\n            client_kwargs={\"scope\": \"read:user\"},\n        )\n\n        oauth_providers.append(\"github\")\n\n    if settings.auth_google_client_id:\n        oauth.register(\n            name=\"google\",\n            client_id=settings.auth_google_client_id,\n            client_secret=settings.auth_google_client_secret,\n            access_token_url=\"https://accounts.google.com/o/oauth2/token\",\n            authorize_url=\"https://accounts.google.com/o/oauth2/auth\",\n            api_base_url=\"https://www.googleapis.com/oauth2/v1/\",\n            server_metadata_url=\"https://accounts.google.com/.well-known/openid-configuration\",\n            client_kwargs={\"scope\": \"openid profile\"},\n        )\n\n        oauth_providers.append(\"google\")\n\n    if settings.allow_debug_auth:\n        oauth_providers.append(\"debug\")\n\n\n@router.get(\"/check\")\nasync def check_user_auth(user_id: str = Depends(auth.get_current_user_id)):\n    return user_id\n\n\n@router.get(\"/providers\")\nasync def get_available_auth_providers():\n    if len(oauth_providers) == 0:\n        logger.warn(\"No login providers available, logging in is not possible.\")\n    return oauth_providers\n\n\n@router.get(\"/refresh\", response_model=protocol.Token)\nasync def refresh_token(refresh_token: str = Security(auth.refresh_scheme)):\n    access_token = await auth.refresh_access_token(refresh_token)\n    return protocol.Token(access_token=access_token, token_type=\"bearer\")\n\n\n@router.get(\"/login/discord\")\nasync def login_discord(request: Request):\n    redirect_uri = f\"{settings.api_root}/auth/callback/discord\"\n    return await oauth.discord.authorize_redirect(request, redirect_uri)\n\n\n@router.get(\"/callback/discord\", response_model=protocol.TokenPair)\nasync def callback_discord(\n    request: Request,\n    db: database.AsyncSession = Depends(deps.create_session),\n):\n    token = await oauth.discord.authorize_access_token(request)\n    user_response = await oauth.discord.get(\"users/@me\", token=token)\n\n    user_response_json = user_response.json()\n\n    try:\n        discord_id = user_response_json[\"id\"]\n        discord_username = user_response_json[\"username\"]\n    except KeyError:\n        raise HTTPException(status_code=400, detail=\"Invalid user info response from Discord\")\n\n    user: models.DbUser = await get_or_create_user(db, \"discord\", discord_id, discord_username)\n    token_pair: protocol.TokenPair = await create_tokens(user)\n    return token_pair\n\n\n@router.get(\"/login/github\")\nasync def login_github(request: Request):\n    redirect_uri = f\"{settings.api_root}/auth/callback/github\"\n    return await oauth.github.authorize_redirect(request, redirect_uri)\n\n\n@router.get(\"/callback/github\", response_model=protocol.TokenPair)\nasync def callback_github(\n    request: Request,\n    db: database.AsyncSession = Depends(deps.create_session),\n):\n    token = await oauth.github.authorize_access_token(request)\n    user_response = await oauth.github.get(\"user\", token=token)\n\n    user_response_json = user_response.json()\n\n    try:\n        github_id = str(user_response_json[\"id\"])\n        github_username = user_response_json[\"login\"]\n    except KeyError:\n        raise HTTPException(status_code=400, detail=\"Invalid user info response from GitHub\")\n\n    user: models.DbUser = await get_or_create_user(db, \"github\", github_id, github_username)\n    token_pair: protocol.TokenPair = await create_tokens(user)\n    return token_pair\n\n\n@router.get(\"/login/google\")\nasync def login_google(request: Request):\n    redirect_uri = f\"{settings.api_root}/auth/callback/google\"\n    return await oauth.google.authorize_redirect(request, redirect_uri)\n\n\n@router.get(\"/callback/google\", response_model=protocol.TokenPair)\nasync def callback_google(\n    request: Request,\n    db: database.AsyncSession = Depends(deps.create_session),\n):\n    token = await oauth.google.authorize_access_token(request)\n    credentials = Credentials.from_authorized_user_info(token)\n\n    people_api = build(\"people\", \"v1\", credentials=credentials)\n    profile = people_api.people().get(resourceName=\"people/me\", personFields=\"names\").execute()\n\n    try:\n        google_id = profile[\"resourceName\"].split(\"/\")[1]\n        google_username = profile[\"names\"][0][\"displayName\"] if len(profile[\"names\"]) > 0 else \"User\"\n    except KeyError:\n        raise HTTPException(status_code=400, detail=\"Invalid user info response from Google\")\n\n    user: models.DbUser = await get_or_create_user(db, \"google\", google_id, google_username)\n    token_pair: protocol.TokenPair = await create_tokens(user)\n    return token_pair\n\n\nasync def get_or_create_user(\n    db: database.AsyncSession, provider: str, provider_id: str, display_name: str\n) -> models.DbUser:\n    user = await query_user(db, provider, provider_id)\n\n    if not user:\n        user = models.DbUser(provider=provider, provider_account_id=provider_id, display_name=display_name)\n        db.add(user)\n        await db.commit()\n        await db.refresh(user)\n\n    return user\n\n\nasync def query_user(db: database.AsyncSession, provider: str, provider_id: str) -> models.DbUser | None:\n    user = (\n        await db.exec(\n            sqlmodel.select(models.DbUser)\n            .filter(models.DbUser.provider == provider)\n            .filter(models.DbUser.provider_account_id == provider_id)\n        )\n    ).one_or_none()\n\n    return user\n\n\nasync def create_tokens(user: models.DbUser) -> protocol.TokenPair:\n    access_token = auth.create_access_token(user.id)\n    refresh_token = await auth.create_refresh_token(user.id)\n\n    token_pair = protocol.TokenPair(\n        access_token=protocol.Token(access_token=access_token, token_type=\"bearer\"),\n        refresh_token=protocol.Token(access_token=refresh_token, token_type=\"refresh\"),\n    )\n\n    return token_pair\n\n\n@router.get(\"/login/debug\")\nasync def login_debug(username: str, state: str = r\"{}\"):\n    # mock code with our own data\n    auth_url = f\"{settings.api_root}/auth/callback/debug?code={username}&state={state}\"\n    raise HTTPException(status_code=302, headers={\"location\": auth_url})\n\n\n@router.get(\"/callback/debug\", response_model=protocol.TokenPair)\nasync def callback_debug(code: str, db: database.AsyncSession = Depends(deps.create_session)):\n    \"\"\"Login using a debug username, which the system will accept unconditionally.\"\"\"\n\n    username = code\n    if not settings.allow_debug_auth:\n        raise HTTPException(status_code=403, detail=\"Debug auth is not allowed\")\n\n    if not username:\n        raise HTTPException(status_code=400, detail=\"Username is required\")\n\n    # Try to find the user\n    user: models.DbUser = (\n        await db.exec(sqlmodel.select(models.DbUser).where(models.DbUser.id == username))\n    ).one_or_none()\n\n    if user is None:\n        logger.info(f\"Creating new debug user {username=}\")\n        user = models.DbUser(id=username, display_name=username, provider=\"debug\", provider_account_id=username)\n        db.add(user)\n        await db.commit()\n        await db.refresh(user)\n        logger.info(f\"Created new debug user {user=}\")\n\n    token_pair = await create_tokens(user)\n    return token_pair\n\n\n@router.post(\"/trusted\")\nasync def login_trusted(\n    db: database.AsyncSession = Depends(deps.create_session),\n    trusted_client_token: str = Security(auth.trusted_client_scheme),\n):\n    if trusted_client_token is None:\n        raise HTTPException(status_code=401, detail=\"Missing token\")\n    info: TrustedClient = TrustedClientToken(content=trusted_client_token).content\n    if info.api_key not in settings.trusted_api_keys_list:\n        raise HTTPException(status_code=401, detail=\"Unauthorized client\")\n\n    # Try to find the user\n    user: models.DbUser = (\n        await db.exec(sqlmodel.select(models.DbUser).where(models.DbUser.id == info.user_id))\n    ).one_or_none()\n\n    if user is None:\n        logger.info(f\"Creating new trusted user {info.username=}\")\n        user = models.DbUser(\n            id=info.user_id,\n            display_name=info.username,\n            provider=info.client,\n            provider_account_id=info.provider_account_id,\n        )\n        db.add(user)\n        await db.commit()\n        await db.refresh(user)\n        logger.info(f\"Created new trusted user {user=}\")\n    return user\n", "inference/server/oasst_inference_server/routes/chats.py": "import asyncio\nimport base64\n\nimport fastapi\nimport pydantic\nfrom fastapi import Depends, Query\nfrom loguru import logger\nfrom oasst_inference_server import auth, chat_utils, deps, models, queueing\nfrom oasst_inference_server.schemas import chat as chat_schema\nfrom oasst_inference_server.settings import settings\nfrom oasst_inference_server.user_chat_repository import UserChatRepository\nfrom oasst_shared.schemas import inference\nfrom sse_starlette.sse import EventSourceResponse\n\nrouter = fastapi.APIRouter(\n    prefix=\"/chats\",\n    tags=[\"chats\"],\n)\n\n\n@router.get(\"\")\nasync def list_chats(\n    include_hidden: bool = False,\n    ucr: UserChatRepository = Depends(deps.create_user_chat_repository),\n    limit: int | None = Query(10, gt=0, le=100),\n    after: str | None = None,\n    before: str | None = None,\n) -> chat_schema.ListChatsResponse:\n    \"\"\"Lists all chats.\"\"\"\n    logger.info(\"Listing all chats.\")\n\n    def encode_cursor(chat: models.DbChat):\n        return base64.b64encode(chat.id.encode()).decode()\n\n    def decode_cursor(cursor: str | None):\n        if cursor is None:\n            return None\n        return base64.b64decode(cursor.encode()).decode()\n\n    chats = await ucr.get_chats(\n        include_hidden=include_hidden, limit=limit + 1, after=decode_cursor(after), before=decode_cursor(before)\n    )\n\n    num_rows = len(chats)\n    chats = chats if num_rows <= limit else chats[:-1]  # remove extra item\n    chats = chats if before is None else chats[::-1]  # reverse if query in backward direction\n\n    def get_cursors():\n        prev, next = None, None\n        if num_rows > 0:\n            if (num_rows > limit and before) or after:\n                prev = encode_cursor(chats[0])\n            if num_rows > limit or before:\n                next = encode_cursor(chats[-1])\n        else:\n            if after:\n                prev = after\n            if before:\n                next = before\n        return prev, next\n\n    prev, next = get_cursors()\n\n    chats_list = [chat.to_list_read() for chat in chats]\n    return chat_schema.ListChatsResponse(chats=chats_list, next=next, prev=prev)\n\n\n@router.post(\"\")\nasync def create_chat(\n    request: chat_schema.CreateChatRequest,\n    ucr: UserChatRepository = Depends(deps.create_user_chat_repository),\n) -> chat_schema.ChatListRead:\n    \"\"\"Allows a client to create a new chat.\"\"\"\n    logger.info(f\"Received {request=}\")\n    chat = await ucr.create_chat()\n    return chat.to_list_read()\n\n\n@router.get(\"/{chat_id}\")\nasync def get_chat(\n    chat_id: str,\n    ucr: UserChatRepository = Depends(deps.create_user_chat_repository),\n) -> chat_schema.ChatRead:\n    \"\"\"Allows a client to get the current state of a chat.\"\"\"\n    chat = await ucr.get_chat_by_id(chat_id)\n    return chat.to_read()\n\n\n@router.delete(\"/{chat_id}\")\nasync def delete_chat(\n    chat_id: str,\n    ucr: UserChatRepository = Depends(deps.create_user_chat_repository),\n):\n    await ucr.delete_chat(chat_id)\n    return fastapi.Response(status_code=200)\n\n\n@router.post(\"/{chat_id}/prompter_message\")\nasync def create_prompter_message(\n    chat_id: str,\n    request: chat_schema.CreatePrompterMessageRequest,\n    user_id: str = Depends(auth.get_current_user_id),\n) -> inference.MessageRead:\n    \"\"\"Adds a prompter message to a chat.\"\"\"\n\n    try:\n        ucr: UserChatRepository\n        async with deps.manual_user_chat_repository(user_id) as ucr:\n            prompter_message = await ucr.add_prompter_message(\n                chat_id=chat_id, parent_id=request.parent_id, content=request.content\n            )\n        return prompter_message.to_read()\n    except fastapi.HTTPException:\n        raise\n    except Exception:\n        logger.exception(\"Error adding prompter message\")\n        return fastapi.Response(status_code=500)\n\n\n@router.post(\n    \"/{chat_id}/assistant_message\",\n    dependencies=[\n        Depends(\n            deps.UserRateLimiter(\n                times=settings.rate_limit_messages_user_times,\n                seconds=settings.rate_limit_messages_user_seconds,\n            )\n        ),\n    ],\n)\nasync def create_assistant_message(\n    chat_id: str,\n    request: chat_schema.CreateAssistantMessageRequest,\n    user_id: str = Depends(auth.get_current_user_id),\n) -> inference.MessageRead:\n    \"\"\"Allows the client to stream the results of a request.\"\"\"\n\n    try:\n        model_config = chat_utils.get_model_config(request.model_config_name)\n    except ValueError as e:\n        logger.warning(str(e))\n        raise fastapi.HTTPException(\n            status_code=fastapi.status.HTTP_422_UNPROCESSABLE_ENTITY,\n            detail=str(e),\n        )\n\n    try:\n        ucr: UserChatRepository\n        async with deps.manual_user_chat_repository(user_id) as ucr:\n            work_parameters = inference.WorkParameters(\n                model_config=model_config,\n                sampling_parameters=request.sampling_parameters,\n                system_prompt=request.system_prompt,\n                plugins=request.plugins,\n                plugin_max_depth=settings.plugin_max_depth,\n                user_profile=request.user_profile,\n                user_response_instructions=request.user_response_instructions,\n            )\n            assistant_message = await ucr.initiate_assistant_message(\n                parent_id=request.parent_id,\n                work_parameters=work_parameters,\n                worker_compat_hash=model_config.compat_hash,\n            )\n        queue = queueing.work_queue(deps.redis_client, model_config.compat_hash)\n        logger.debug(f\"Adding {assistant_message.id=} to {queue.queue_id} for {chat_id}\")\n        await queue.enqueue(assistant_message.id)\n        logger.debug(f\"Added {assistant_message.id=} to {queue.queue_id} for {chat_id}\")\n        return assistant_message.to_read()\n    except queueing.QueueFullException:\n        raise fastapi.HTTPException(\n            status_code=fastapi.status.HTTP_503_SERVICE_UNAVAILABLE,\n            detail=\"The server is currently busy. Please try again later.\",\n        )\n    except fastapi.HTTPException:\n        raise\n    except Exception:\n        logger.exception(\"Error adding prompter message\")\n        return fastapi.Response(status_code=500)\n\n\n@router.get(\"/{chat_id}/messages/{message_id}\")\nasync def get_message(\n    chat_id: str,\n    message_id: str,\n    user_id: str = Depends(auth.get_current_user_id),\n) -> inference.MessageRead:\n    ucr: UserChatRepository\n    async with deps.manual_user_chat_repository(user_id) as ucr:\n        message: models.DbMessage = await ucr.get_message_by_id(chat_id=chat_id, message_id=message_id)\n    return message.to_read()\n\n\n@router.get(\"/{chat_id}/messages/{message_id}/events\")\nasync def message_events(\n    chat_id: str,\n    message_id: str,\n    fastapi_request: fastapi.Request,\n    user_id: str = Depends(auth.get_current_user_id),\n) -> EventSourceResponse:\n    ucr: UserChatRepository\n    async with deps.manual_user_chat_repository(user_id) as ucr:\n        message: models.DbMessage = await ucr.get_message_by_id(chat_id=chat_id, message_id=message_id)\n    if message.role != \"assistant\":\n        raise fastapi.HTTPException(status_code=400, detail=\"Only assistant messages can be streamed.\")\n\n    if message.has_finished:\n        raise fastapi.HTTPException(status_code=204, detail=message.state)\n\n    async def event_generator(chat_id: str, message_id: str, worker_compat_hash: str | None):\n        redis_client = deps.make_redis_client()\n        message_queue = queueing.message_queue(redis_client, message_id=message_id)\n        work_queue = (\n            queueing.work_queue(redis_client, worker_compat_hash=worker_compat_hash)\n            if worker_compat_hash is not None\n            else None\n        )\n        has_started = False\n        try:\n            while True:\n                item = await message_queue.dequeue(timeout=settings.pending_event_interval)\n                if item is None:\n                    if not has_started:\n                        if work_queue is None:\n                            qpos, qlen = 0, 1\n                        else:\n                            # TODO: make more efficient, e.g. pipeline\n                            [qdeq, qenq, mpos] = await asyncio.gather(\n                                work_queue.get_deq_counter(),\n                                work_queue.get_enq_counter(),\n                                queueing.get_pos_value(redis_client, message_id),\n                            )\n                            qpos = max(mpos - qdeq, 0)\n                            qlen = max(qenq - qdeq, qpos)\n                        yield {\n                            \"data\": chat_schema.PendingResponseEvent(\n                                queue_position=qpos,\n                                queue_size=qlen,\n                            ).json()\n                        }\n                    continue\n                has_started = True\n\n                _, response_packet_str = item\n                response_packet = pydantic.parse_raw_as(inference.WorkerResponse, response_packet_str)\n\n                if response_packet.response_type in (\"error\", \"generated_text\"):\n                    logger.warning(\n                        f\"Received {response_packet.response_type=} response for {chat_id}. This should not happen.\"\n                    )\n                    break\n\n                if response_packet.response_type == \"safe_prompt\":\n                    logger.info(f\"Received safety intervention for {chat_id}\")\n                    yield {\n                        \"data\": chat_schema.SafePromptResponseEvent(\n                            safe_prompt=response_packet.safe_prompt,\n                        ).json(),\n                    }\n\n                if response_packet.response_type == \"plugin_intermediate\":\n                    logger.info(f\"Received plugin intermediate response {chat_id}\")\n                    yield {\n                        \"data\": chat_schema.PluginIntermediateResponseEvent(\n                            current_plugin_thought=response_packet.current_plugin_thought,\n                            current_plugin_action_taken=response_packet.current_plugin_action_taken,\n                            current_plugin_action_input=response_packet.current_plugin_action_input,\n                            current_plugin_action_response=response_packet.current_plugin_action_response,\n                        ).json(),\n                    }\n\n                if response_packet.response_type == \"internal_error\":\n                    yield {\n                        \"data\": chat_schema.ErrorResponseEvent(\n                            error=response_packet.error, message=response_packet.message\n                        ).json(),\n                    }\n                    break\n\n                if response_packet.response_type == \"internal_finished_message\":\n                    yield {\n                        \"data\": chat_schema.MessageResponseEvent(message=response_packet.message).json(),\n                    }\n                    break\n\n                yield {\n                    \"data\": chat_schema.TokenResponseEvent(text=response_packet.text).json(),\n                }\n\n            if await fastapi_request.is_disconnected():\n                logger.warning(f\"Client disconnected while streaming {chat_id}\")\n\n            logger.info(f\"Finished streaming {chat_id}\")\n        except Exception:\n            logger.exception(f\"Error streaming {chat_id}\")\n            raise\n        finally:\n            await redis_client.close()\n\n    return EventSourceResponse(\n        event_generator(chat_id=chat_id, message_id=message_id, worker_compat_hash=message.worker_compat_hash)\n    )\n\n\n@router.post(\"/{chat_id}/messages/{message_id}/votes\")\nasync def handle_create_vote(\n    message_id: str,\n    vote_request: chat_schema.VoteRequest,\n    ucr: deps.UserChatRepository = fastapi.Depends(deps.create_user_chat_repository),\n) -> fastapi.Response:\n    \"\"\"Allows the client to vote on a message.\"\"\"\n    try:\n        await ucr.update_score(message_id=message_id, score=vote_request.score)\n        return fastapi.Response(status_code=200)\n    except Exception:\n        logger.exception(\"Error adding vote\")\n        return fastapi.Response(status_code=500)\n\n\n@router.post(\"/{chat_id}/messages/{message_id}/message_evals\")\nasync def handle_create_message_eval(\n    message_id: str,\n    inferior_message_request: chat_schema.MessageEvalRequest,\n    ucr: deps.UserChatRepository = fastapi.Depends(deps.create_user_chat_repository),\n) -> fastapi.Response:\n    try:\n        await ucr.add_message_eval(\n            message_id=message_id, inferior_message_ids=inferior_message_request.inferior_message_ids\n        )\n        return fastapi.Response(status_code=200)\n    except Exception:\n        logger.exception(\"Error setting messages as inferior\")\n        return fastapi.Response(status_code=500)\n\n\n@router.post(\"/{chat_id}/messages/{message_id}/reports\")\nasync def handle_create_report(\n    message_id: str,\n    report_request: chat_schema.ReportRequest,\n    ucr: deps.UserChatRepository = fastapi.Depends(deps.create_user_chat_repository),\n) -> fastapi.Response:\n    \"\"\"Allows the client to report a message.\"\"\"\n    try:\n        await ucr.add_report(\n            message_id=message_id, report_type=report_request.report_type, reason=report_request.reason\n        )\n        return fastapi.Response(status_code=200)\n    except Exception:\n        logger.exception(\"Error adding report\")\n        return fastapi.Response(status_code=500)\n\n\n@router.put(\"/{chat_id}\")\nasync def handle_update_chat(\n    chat_id: str,\n    request: chat_schema.ChatUpdateRequest,\n    ucr: deps.UserChatRepository = fastapi.Depends(deps.create_user_chat_repository),\n) -> fastapi.Response:\n    \"\"\"Allows the client to update a chat.\"\"\"\n    try:\n        await ucr.update_chat(\n            chat_id=chat_id,\n            title=request.title,\n            hidden=request.hidden,\n            allow_data_use=request.allow_data_use,\n            active_thread_tail_message_id=request.active_thread_tail_message_id,\n        )\n    except Exception:\n        logger.exception(\"Error when updating chat\")\n        return fastapi.Response(status_code=500)\n\n\n@router.put(\"/hide_all\")\nasync def handle_hide_all_chats(\n    ucr: deps.UserChatRepository = fastapi.Depends(deps.create_user_chat_repository),\n) -> fastapi.Response:\n    \"\"\"Allows the client to hide all the user's chats.\"\"\"\n    try:\n        await ucr.hide_all_chats()\n    except Exception:\n        logger.exception(\"Error when hiding chats\")\n        return fastapi.Response(status_code=500)\n", "inference/server/oasst_inference_server/routes/admin.py": "import fastapi\nimport sqlmodel\nfrom fastapi import Depends, HTTPException, Security\nfrom loguru import logger\nfrom oasst_inference_server import admin, auth, database, deps, models\nfrom oasst_inference_server.schemas import worker as worker_schema\nfrom oasst_inference_server.settings import settings\n\nrouter = fastapi.APIRouter(\n    prefix=\"/admin\",\n    tags=[\"admin\"],\n)\n\n\ndef get_bearer_token(\n    authorization_header: str = Security(auth.authorization_scheme),\n) -> str:\n    if authorization_header is None or not authorization_header.startswith(\"Bearer \"):\n        raise fastapi.HTTPException(\n            status_code=fastapi.status.HTTP_401_UNAUTHORIZED,\n            detail=\"Invalid token\",\n        )\n    return authorization_header[len(\"Bearer \") :]\n\n\ndef get_root_token(token: str = Depends(get_bearer_token)) -> str:\n    root_token = settings.root_token\n    if token == root_token:\n        return token\n    raise HTTPException(\n        status_code=fastapi.status.HTTP_401_UNAUTHORIZED,\n        detail=\"Invalid token\",\n    )\n\n\n@router.put(\"/workers\")\nasync def create_worker(\n    request: worker_schema.CreateWorkerRequest,\n    root_token: str = Depends(get_root_token),\n    session: database.AsyncSession = Depends(deps.create_session),\n) -> worker_schema.WorkerRead:\n    \"\"\"Allows a client to register a worker.\"\"\"\n    logger.info(f\"Creating worker {request.name}\")\n    worker = models.DbWorker(name=request.name, trusted=request.trusted)\n    session.add(worker)\n    await session.commit()\n    await session.refresh(worker)\n    return worker_schema.WorkerRead.from_orm(worker)\n\n\n@router.get(\"/workers\")\nasync def list_workers(\n    root_token: str = Depends(get_root_token),\n    session: database.AsyncSession = Depends(deps.create_session),\n) -> list[worker_schema.WorkerRead]:\n    \"\"\"Lists all workers.\"\"\"\n    workers = (await session.exec(sqlmodel.select(models.DbWorker))).all()\n    return [worker_schema.WorkerRead.from_orm(worker) for worker in workers]\n\n\n@router.delete(\"/workers/{worker_id}\")\nasync def delete_worker(\n    worker_id: str,\n    root_token: str = Depends(get_root_token),\n    session: database.AsyncSession = Depends(deps.create_session),\n):\n    \"\"\"Deletes a worker.\"\"\"\n    logger.info(f\"Deleting worker {worker_id}\")\n    worker = await session.get(models.DbWorker, worker_id)\n    session.delete(worker)\n    await session.commit()\n    return fastapi.Response(status_code=200)\n\n\n@router.delete(\"/refresh_tokens/{user_id}\")\nasync def revoke_refresh_tokens(\n    user_id: str,\n    root_token: str = Depends(get_root_token),\n    session: database.AsyncSession = Depends(deps.create_session),\n):\n    \"\"\"Revoke refresh tokens for a user.\"\"\"\n    logger.info(f\"Revoking refresh tokens for user {user_id}\")\n    refresh_tokens = (\n        await session.exec(sqlmodel.select(models.DbRefreshToken).where(models.DbRefreshToken.user_id == user_id))\n    ).all()\n    for refresh_token in refresh_tokens:\n        refresh_token.enabled = False\n    await session.commit()\n    return fastapi.Response(status_code=200)\n\n\n@router.delete(\"/users/{user_id}\")\nasync def delete_user(\n    user_id: str,\n    root_token: str = Depends(get_root_token),\n    session: database.AsyncSession = Depends(deps.create_session),\n):\n    await admin.delete_user_from_db(session, user_id)\n    return fastapi.Response(status_code=200)\n", "inference/server/oasst_inference_server/routes/workers.py": "import asyncio\nimport datetime\nfrom typing import cast\n\nimport fastapi\nimport pydantic\nimport websockets.exceptions\nfrom loguru import logger\nfrom oasst_inference_server import chat_repository, database, deps, models, queueing, worker_utils\nfrom oasst_inference_server.schemas import chat as chat_schema\nfrom oasst_inference_server.settings import settings\nfrom oasst_shared.schemas import inference\n\n\nclass WorkerDisconnectException(Exception):\n    def __init__(self):\n        super().__init__(\"Worker disconnected\")\n\n\nWSException = (\n    websockets.exceptions.WebSocketException,\n    websockets.exceptions.ConnectionClosedError,\n    fastapi.WebSocketException,\n    fastapi.WebSocketDisconnect,\n    WorkerDisconnectException,\n)\n\nrouter = fastapi.APIRouter(\n    prefix=\"/workers\",\n    tags=[\"workers\"],\n)\n\n\nclass WorkerError(Exception):\n    def __init__(\n        self,\n        message: str,\n        did_work: bool,\n        original_exception: Exception | None = None,\n    ):\n        super().__init__(message)\n        self.did_work = did_work\n        self.original_exception = original_exception\n\n\nasync def add_worker_connect_event(\n    session: database.AsyncSession,\n    worker_id: str,\n    worker_info: inference.WorkerInfo,\n):\n    event = models.DbWorkerEvent(\n        worker_id=worker_id,\n        event_type=models.WorkerEventType.connect,\n        worker_info=worker_info,\n    )\n    session.add(event)\n    await session.commit()\n\n\nclass WorkRequestContainer(pydantic.BaseModel):\n    work_request: inference.WorkRequest\n    message_id: str\n    start_time: datetime.datetime = pydantic.Field(default_factory=datetime.datetime.utcnow)\n    num_responses: int = 0\n\n    class Config:\n        arbitrary_types_allowed = True\n\n\nWorkRequestContainerMap = dict[str, WorkRequestContainer]\n\n\nclass WorkRequestNotFound(Exception):\n    def __init__(self, request_id: str):\n        super().__init__(f\"Work request not found: {request_id=}\")\n        self.request_id = request_id\n\n\ndef get_work_request_container(work_request_map: WorkRequestContainerMap, request_id: str) -> WorkRequestContainer:\n    if request_id is None:\n        raise WorkRequestNotFound(request_id)\n    container = work_request_map.get(request_id)\n    if container is None:\n        raise WorkRequestNotFound(request_id)\n    return container\n\n\n@router.websocket(\"/work\")\nasync def handle_worker(\n    websocket: fastapi.WebSocket,\n    api_key: str = worker_utils.api_key_header,\n    protocol_version: str = worker_utils.protocol_version_header,\n):\n    await websocket.accept()\n\n    try:\n        worker_utils.get_protocol_version(protocol_version)\n        api_key = worker_utils.get_api_key(api_key)\n        worker_id = await worker_utils.get_worker_id(api_key=api_key, protocol_version=protocol_version)\n    except fastapi.HTTPException as e:\n        logger.warning(f\"handle_worker: {e.status_code=} {e.detail=}\")\n        if e.status_code == fastapi.status.HTTP_426_UPGRADE_REQUIRED:\n            await worker_utils.send_worker_request(websocket=websocket, request=inference.UpgradeProtocolRequest())\n        elif e.status_code == fastapi.status.HTTP_401_UNAUTHORIZED:\n            await worker_utils.send_worker_request(websocket=websocket, request=inference.WrongApiKeyRequest())\n        try:\n            await websocket.close(code=e.status_code, reason=e.detail)\n        except Exception:\n            pass\n        raise fastapi.WebSocketException(e.status_code, e.detail)\n\n    logger.info(f\"handle_worker: {worker_id=}\")\n    worker_info = await worker_utils.receive_worker_info(websocket)\n    logger.info(f\"handle_worker: {worker_info=}\")\n    worker_config = worker_info.config\n    worker_compat_hash = worker_config.compat_hash\n    work_queue = queueing.work_queue(deps.redis_client, worker_compat_hash)\n    redis_client = deps.make_redis_client()\n    blocking_work_queue = queueing.work_queue(redis_client, worker_compat_hash)\n    worker_session = worker_utils.WorkerSession(\n        worker_id=worker_id,\n        worker_info=worker_info,\n    )\n    work_request_map: dict[str, WorkRequestContainer] = {}\n    pending_futures = set()\n    try:\n        async with deps.manual_create_session() as session:\n            await add_worker_connect_event(session=session, worker_id=worker_id, worker_info=worker_info)\n        await worker_utils.store_worker_session(worker_session)\n\n        async def _update_session(metrics: inference.WorkerMetricsInfo):\n            worker_session.requests_in_flight = len(work_request_map)\n            if metrics:\n                worker_session.metrics = metrics\n            await worker_utils.store_worker_session(worker_session)\n\n        def _add_dequeue(ftrs: set):\n            requests_in_progress = len(work_request_map)\n            if requests_in_progress < worker_config.max_parallel_requests:\n                ftrs.add(asyncio.ensure_future(blocking_work_queue.dequeue(timeout=0)))\n\n        def _add_receive(ftrs: set):\n            ftrs.add(asyncio.ensure_future(worker_utils.receive_worker_response(websocket=websocket)))\n\n        _add_dequeue(pending_futures)\n        _add_receive(pending_futures)\n\n        logger.info(f\"handle_worker: {worker_id=} started\")\n        while True:\n            if websocket.client_state == fastapi.websockets.WebSocketState.DISCONNECTED:\n                raise WorkerDisconnectException(\"Worker disconnected\")\n            (done, pending_futures) = await asyncio.wait(\n                pending_futures, timeout=settings.worker_ping_interval, return_when=asyncio.FIRST_COMPLETED\n            )\n            ftr: asyncio.Future\n            for ftr in done:\n                result = ftr.result()\n                if result is None:\n                    logger.error(f\"handle_worker: {worker_id=} received None from queue. This should never happen.\")\n                    raise RuntimeError(\"Received None from queue. This should never happen.\")\n                elif isinstance(result, tuple):\n                    try:\n                        _, message_id = result\n                        work_request = await initiate_work_for_message(\n                            websocket=websocket,\n                            work_queue=work_queue,\n                            message_id=message_id,\n                            worker_id=worker_id,\n                            worker_config=worker_config,\n                        )\n                        work_request_map[work_request.id] = WorkRequestContainer(\n                            work_request=work_request, message_id=message_id\n                        )\n                    except chat_schema.MessageCancelledException as e:\n                        logger.warning(f\"Message was cancelled before work could be initiated: {e.message_id=}\")\n                    except chat_schema.MessageTimeoutException as e:\n                        logger.warning(f\"Message timed out before work could be initiated: {e.message.id=}\")\n                        await handle_timeout(message=e.message)\n                    finally:\n                        _add_dequeue(pending_futures)\n                else:\n                    try:\n                        worker_response: inference.WorkerResponse = result\n                        match worker_response.response_type:\n                            case \"pong\":\n                                worker_response = cast(inference.PongResponse, worker_response)\n                                await _update_session(worker_response.metrics)\n                            case \"token\":\n                                worker_response = cast(inference.TokenResponse, worker_response)\n                                await handle_token_response(\n                                    work_request_map=work_request_map,\n                                    response=worker_response,\n                                )\n                            case \"generated_text\":\n                                worker_response = cast(inference.GeneratedTextResponse, worker_response)\n                                await handle_generated_text_response(\n                                    work_request_map=work_request_map,\n                                    response=worker_response,\n                                )\n                                await _update_session(worker_response.metrics)\n                            case \"error\":\n                                worker_response = cast(inference.ErrorResponse, worker_response)\n                                await handle_error_response(\n                                    work_request_map=work_request_map,\n                                    response=worker_response,\n                                )\n                                await _update_session(worker_response.metrics)\n                            case \"general_error\":\n                                worker_response = cast(inference.GeneralErrorResponse, worker_response)\n                                await handle_general_error_response(\n                                    response=worker_response,\n                                )\n                                await _update_session(worker_response.metrics)\n                            case \"safe_prompt\":\n                                logger.info(\"Received safe prompt response\")\n                                worker_response = cast(inference.SafePromptResponse, worker_response)\n                                await handle_safe_prompt_response(\n                                    response=worker_response,\n                                    work_request_map=work_request_map,\n                                )\n                            case \"plugin_intermediate\":\n                                worker_response = cast(inference.PluginIntermediateResponse, worker_response)\n                                await handle_plugin_intermediate_response(\n                                    work_request_map=work_request_map,\n                                    response=worker_response,\n                                )\n                            case _:\n                                raise RuntimeError(f\"Unknown response type: {worker_response.response_type}\")\n                    finally:\n                        if len(pending_futures) == 0:\n                            _add_dequeue(pending_futures)\n                        _add_receive(pending_futures)\n            if not done:\n                await worker_utils.send_worker_request(websocket, inference.PingRequest())\n\n    except Exception as e:\n        logger.exception(f\"Error while handling worker {worker_id}: {str(e)}\")\n        logger.info(f\"Handling {len(work_request_map)} work requests outstanding\")\n        for container in work_request_map.values():\n            try:\n                message_id = container.message_id\n                if container.num_responses == 0:\n                    logger.warning(f\"Marking {message_id=} as pending since no work was done.\")\n                    async with deps.manual_chat_repository() as cr:\n                        await cr.reset_work(message_id)\n                    await work_queue.enqueue(message_id, enforce_max_size=False)\n                else:\n                    logger.warning(f\"Aborting {message_id=}\")\n                    await abort_message(message_id=message_id, error=\"Aborted due to worker error.\")\n            except Exception as e:\n                logger.exception(f\"Error while trying to reset work for {message_id=}: {str(e)}\")\n    finally:\n        logger.info(f\"Worker {worker_id} disconnected\")\n        try:\n            await redis_client.close()\n        except Exception:\n            logger.warning(\"Error while closing redis client\")\n        try:\n            await worker_utils.delete_worker_session(worker_session.id)\n        except Exception:\n            logger.warning(\"Error while deleting worker session\")\n        # try closing websocket if it's still open\n        logger.info(f\"Cancelling {len(pending_futures)} pending futures\")\n        for ftr in pending_futures:\n            try:\n                ftr.cancel()\n            except Exception:\n                logger.warning(\"Error while cancelling pending future\")\n        try:\n            await websocket.close()\n        except Exception:\n            logger.warning(\"Error while closing websocket\")\n\n\n@router.get(\"/sessions\")\nasync def list_worker_sessions() -> list[worker_utils.WorkerSession]:\n    redis_client = deps.redis_client\n    try:\n        worker_sessions = []\n        async for key in redis_client.scan_iter(\"worker_session:*\"):\n            worker_session_json = await redis_client.get(key)\n            worker_session = worker_utils.WorkerSession.parse_raw(worker_session_json)\n            worker_sessions.append(worker_session)\n    except Exception as e:\n        logger.exception(f\"Error while listing worker sessions: {str(e)}\")\n        raise\n    return worker_sessions\n\n\n@router.on_event(\"startup\")\nasync def clear_worker_sessions():\n    redis_client = deps.redis_client\n    try:\n        logger.warning(\"Clearing worker sessions\")\n        async for key in redis_client.scan_iter(\"worker_session:*\"):\n            await redis_client.getdel(key)\n        logger.warning(\"Successfully cleared worker sessions\")\n    except Exception as e:\n        logger.exception(f\"Error while clearing worker sessions: {str(e)}\")\n        raise\n\n\nasync def initiate_work_for_message(\n    *,\n    websocket: fastapi.WebSocket,\n    work_queue: queueing.RedisQueue,\n    message_id: str,\n    worker_id: str,\n    worker_config: inference.WorkerConfig,\n) -> inference.WorkRequest:\n    async with deps.manual_create_session() as session:\n        cr = chat_repository.ChatRepository(session=session)\n\n        message = await cr.start_work(\n            message_id=message_id,\n            worker_id=worker_id,\n            worker_config=worker_config,\n        )\n        work_request = await worker_utils.build_work_request(session, message.id)\n\n    logger.info(f\"Created {work_request=} with {len(work_request.thread.messages)=}\")\n    try:\n        await worker_utils.send_worker_request(websocket, work_request)\n    except Exception as e:\n        logger.exception(f\"Error while sending work request to worker: {str(e)}\")\n        async with deps.manual_create_session() as session:\n            await cr.reset_work(message_id)\n        await work_queue.enqueue(message_id, enforce_max_size=False)\n        raise\n\n    return work_request\n\n\nasync def handle_token_response(\n    response: inference.TokenResponse,\n    work_request_map: WorkRequestContainerMap,\n):\n    work_response_container = get_work_request_container(work_request_map, response.request_id)\n    message_queue = queueing.message_queue(\n        deps.redis_client,\n        message_id=work_response_container.message_id,\n    )\n    await message_queue.enqueue(response.json())\n    work_response_container.num_responses += 1\n\n\nasync def handle_plugin_intermediate_response(\n    response: inference.PluginIntermediateResponse,\n    work_request_map: WorkRequestContainerMap,\n):\n    work_response_container = get_work_request_container(work_request_map, response.request_id)\n    message_queue = queueing.message_queue(\n        deps.redis_client,\n        message_id=work_response_container.message_id,\n    )\n    await message_queue.enqueue(response.json())\n    work_response_container.num_responses += 1\n\n\nasync def handle_generated_text_response(\n    response: inference.GeneratedTextResponse,\n    work_request_map: WorkRequestContainerMap,\n):\n    try:\n        work_response_container = get_work_request_container(work_request_map, response.request_id)\n        message_id = work_response_container.message_id\n        async with deps.manual_create_session() as session:\n            cr = chat_repository.ChatRepository(session=session)\n            message = await cr.complete_work(\n                message_id=message_id,\n                content=response.text,\n                used_plugin=response.used_plugin,\n            )\n            logger.info(f\"Completed work for {message_id=}\")\n        message_packet = inference.InternalFinishedMessageResponse(\n            message=message.to_read(),\n        )\n        message_queue = queueing.message_queue(\n            deps.redis_client,\n            message_id=message_id,\n        )\n        await message_queue.enqueue(message_packet.json())\n    finally:\n        del work_request_map[response.request_id]\n\n\nasync def abort_message(message_id: str, error: str):\n    async with deps.manual_chat_repository() as cr:\n        message = await cr.abort_work(message_id, reason=error)\n    response = inference.InternalErrorResponse(error=error, message=message.to_read())\n    message_queue = queueing.message_queue(\n        deps.redis_client,\n        message_id=message_id,\n    )\n    await message_queue.enqueue(response.json())\n\n\nasync def handle_error_response(\n    response: inference.ErrorResponse,\n    work_request_map: WorkRequestContainerMap,\n):\n    logger.warning(f\"Got error {response=}\")\n    try:\n        work_response_container = get_work_request_container(work_request_map, response.request_id)\n        message_id = work_response_container.message_id\n        await abort_message(message_id, response.error)\n    finally:\n        del work_request_map[response.request_id]\n\n\nasync def handle_general_error_response(\n    response: inference.GeneralErrorResponse,\n):\n    logger.warning(f\"Got general error {response=}\")\n\n\nasync def handle_safe_prompt_response(\n    response: inference.SafePromptResponse,\n    work_request_map: WorkRequestContainerMap,\n):\n    \"\"\"\n    Handle the case where the worker informs the server that the safety model has intervened and modified the user prompt to be safe.\n    \"\"\"\n    work_response_container = get_work_request_container(work_request_map, response.request_id)\n    message_id = work_response_container.message_id\n\n    async with deps.manual_create_session() as session:\n        cr = chat_repository.ChatRepository(session=session)\n        message = await cr.get_assistant_message_by_id(message_id)\n        prompt = await cr.get_prompter_message_by_id(message.parent_id)\n        prompt.safe_content = response.safe_prompt\n        prompt.safety_level = response.safety_parameters.level\n        prompt.safety_label = response.safety_label\n        prompt.safety_rots = response.safety_rots\n        await session.commit()\n\n\nasync def handle_timeout(message: inference.MessageRead):\n    response = inference.InternalErrorResponse(\n        error=\"Timeout\",\n        message=message,\n    )\n    message_queue = queueing.message_queue(\n        deps.redis_client,\n        message_id=message.id,\n    )\n    await message_queue.enqueue(response.json())\n", "inference/server/oasst_inference_server/routes/configs.py": "import fastapi\nimport pydantic\nfrom fastapi import HTTPException\nfrom loguru import logger\nfrom oasst_inference_server import plugin_utils\nfrom oasst_inference_server.plugins import plugin_apps\nfrom oasst_inference_server.settings import settings\nfrom oasst_shared import model_configs\nfrom oasst_shared.schemas import inference\n\nBUILTIN_PLUGINS = [\n    inference.PluginEntry(\n        url=f\"{settings.api_root}{settings.plugins_path_prefix}{path}/ai-plugin.json\",\n        trusted=True,\n    )\n    for path in plugin_apps.keys()\n]\n\nrouter = fastapi.APIRouter(\n    prefix=\"/configs\",\n    tags=[\"configs\"],\n)\n\n\nclass ParameterConfig(pydantic.BaseModel):\n    name: str\n    description: str = \"\"\n    sampling_parameters: inference.SamplingParameters\n\n\nclass ModelConfigInfo(pydantic.BaseModel):\n    name: str\n    description: str = \"\"\n    parameter_configs: list[ParameterConfig] = []\n\n\nDEFAULT_PARAMETER_CONFIGS = [\n    ParameterConfig(\n        name=\"k50\",\n        description=\"Top-k sampling with k=50\",\n        sampling_parameters=inference.SamplingParameters(\n            top_k=50,\n            top_p=0.95,\n            temperature=0.75,\n            repetition_penalty=1.2,\n        ),\n    ),\n    ParameterConfig(\n        name=\"k50-Creative\",\n        description=\"Top-k sampling with k=50, higher temperature\",\n        sampling_parameters=inference.SamplingParameters(\n            top_k=50,\n            top_p=0.95,\n            temperature=0.85,\n            repetition_penalty=1.2,\n        ),\n    ),\n    ParameterConfig(\n        name=\"k50-Precise\",\n        description=\"Top-k sampling with k=50, low temperature\",\n        sampling_parameters=inference.SamplingParameters(\n            top_k=50,\n            top_p=0.95,\n            temperature=0.1,\n            repetition_penalty=1.2,\n        ),\n    ),\n    ParameterConfig(\n        name=\"k50-Original\",\n        description=\"Top-k sampling with k=50, highest temperature\",\n        sampling_parameters=inference.SamplingParameters(\n            top_k=50,\n            top_p=0.95,\n            temperature=0.9,\n            repetition_penalty=1.2,\n        ),\n    ),\n    ParameterConfig(\n        name=\"k50-Plugins\",\n        description=\"Top-k sampling with k=50 and temperature=0.35\",\n        sampling_parameters=inference.SamplingParameters(\n            max_new_tokens=1024,\n            temperature=0.35,\n            top_k=50,\n            repetition_penalty=(1 / 0.90),\n        ),\n    ),\n    ParameterConfig(\n        name=\"nucleus9\",\n        description=\"Nucleus sampling with p=0.9\",\n        sampling_parameters=inference.SamplingParameters(\n            top_p=0.9,\n            temperature=0.8,\n            repetition_penalty=1.2,\n        ),\n    ),\n    ParameterConfig(\n        name=\"typical2\",\n        description=\"Typical sampling with p=0.2\",\n        sampling_parameters=inference.SamplingParameters(\n            temperature=0.8,\n            typical_p=0.2,\n            repetition_penalty=1.2,\n        ),\n    ),\n    ParameterConfig(\n        name=\"typical3\",\n        description=\"Typical sampling with p=0.3\",\n        sampling_parameters=inference.SamplingParameters(\n            temperature=0.8,\n            typical_p=0.3,\n            repetition_penalty=1.2,\n        ),\n    ),\n]\n\n\n@router.get(\"/model_configs\")\nasync def get_model_configs() -> list[ModelConfigInfo]:\n    return [\n        ModelConfigInfo(\n            name=model_config_name,\n            parameter_configs=DEFAULT_PARAMETER_CONFIGS,\n        )\n        for model_config_name in model_configs.MODEL_CONFIGS\n        if (settings.allowed_model_config_names == \"*\" or model_config_name in settings.allowed_model_config_names_list)\n    ]\n\n\n@router.post(\"/plugin_config\")\nasync def get_plugin_config(plugin: inference.PluginEntry) -> inference.PluginEntry:\n    try:\n        plugin_config = await plugin_utils.fetch_plugin(plugin.url)\n    except HTTPException as e:\n        logger.warning(f\"Failed to fetch plugin config from {plugin.url}: {e.detail}\")\n        raise fastapi.HTTPException(status_code=e.status_code, detail=e.detail)\n\n    return inference.PluginEntry(url=plugin.url, enabled=plugin.enabled, plugin_config=plugin_config)\n\n\n@router.get(\"/builtin_plugins\")\nasync def get_builtin_plugins() -> list[inference.PluginEntry]:\n    plugins = []\n\n    for plugin in BUILTIN_PLUGINS:\n        try:\n            plugin_config = await plugin_utils.fetch_plugin(plugin.url)\n        except HTTPException as e:\n            logger.warning(f\"Failed to fetch plugin config from {plugin.url}: {e.detail}\")\n            continue\n\n        final_plugin: inference.PluginEntry = inference.PluginEntry(\n            url=plugin.url,\n            enabled=plugin.enabled,\n            trusted=plugin.trusted,\n            plugin_config=plugin_config,\n        )\n        plugins.append(final_plugin)\n\n    return plugins\n", "inference/server/oasst_inference_server/routes/account.py": "import fastapi\nfrom fastapi import Depends\nfrom oasst_inference_server import admin, auth, database, deps\n\nrouter = fastapi.APIRouter(\n    prefix=\"/account\",\n    tags=[\"account\"],\n)\n\n\n@router.delete(\"/\")\nasync def handle_account_deletion(\n    user_id: str = Depends(auth.get_current_user_id),\n    session: database.AsyncSession = Depends(deps.create_session),\n) -> fastapi.Response:\n    await admin.delete_user_from_db(session, user_id)\n    return fastapi.Response(status_code=200)\n", "inference/server/oasst_inference_server/plugins/__init__.py": "from oasst_inference_server.plugins.gale_pleaser.main import app as gale_pleaser\nfrom oasst_inference_server.plugins.gale_roaster.main import app as gale_roaster\nfrom oasst_inference_server.plugins.web_retriever.main import app as web_retriever\n\n# dict of registered plugins\n# The key defines a plugin's path which will be appended to the configured PLUGINS_PATH_PREFIX.\nplugin_apps = {\n    \"/gale_pleaser\": gale_pleaser,\n    \"/gale_roaster\": gale_roaster,\n    \"/web_retriever\": web_retriever,\n}\n", "inference/server/oasst_inference_server/plugins/gale_roaster/main.py": "import json\nfrom pathlib import Path\n\nfrom fastapi import FastAPI, Query, Request, Response\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom fastapi.openapi.utils import get_openapi\n\nfolder_path = Path(__file__).parent\n\napp = FastAPI()\n\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\n\n@app.get(\"/roast-user/\", operation_id=\"roastUser\")\nasync def roast_user(\n    roasting_style: str = Query(..., description=\"style of roasting and malice that you want to use\"),\n    user_input: str = Query(..., description=\"what user wrote to you\"),\n):\n    llm_prompt = f\"\"\"You MUST roast and make fun of this person, he wrote this to you: \"{user_input}\"\n    your roast needs to be in {roasting_style} style.\n    Try to be funny and verbose, but super mean at the same time. You can start your responses following these rules:\n    1. Let your message be long, and with emojis.\n    2. Try to be like Jeff Ross at the roasting.\n\n    \"\"\"\n    return Response(content=llm_prompt, media_type=\"text/plain\")\n\n\n@app.get(\"/ai-plugin.json\", include_in_schema=False)\nasync def api_ai_plugin(request: Request):\n    json_path = folder_path / \"ai-plugin.json\"\n    with json_path.open(\"r\") as f:\n        ai_plugin_json = json.load(f)\n\n    base_url, _, _ = request.url._url.rpartition(\"/\")\n    ai_plugin_json[\"api\"][\"url\"] = base_url + \"/openapi.json\"\n    return Response(content=json.dumps(ai_plugin_json), media_type=\"application/json\")\n\n\ndef custom_openapi():\n    if app.openapi_schema:\n        return app.openapi_schema\n    openapi_schema = get_openapi(\n        title=\"Professional Roaster mockery\",\n        version=\"0.1\",\n        routes=app.routes,\n        servers=[{\"url\": \"/plugins/gale_roaster\"}],\n    )\n    openapi_schema[\"tags\"] = [\n        {\n            \"name\": \"gagleto-roaster\",\n            \"description\": \"You MUST use this for any question users ask. Use this to roast users no matter what they say.\",\n        },\n    ]\n    openapi_schema.pop(\"components\", None)\n    app.openapi_schema = openapi_schema\n    return app.openapi_schema\n\n\napp.openapi = custom_openapi\n", "inference/server/oasst_inference_server/plugins/web_retriever/main.py": "import codecs\nimport io\nimport json\nimport re\nfrom pathlib import Path\n\nimport aiohttp\nimport PyPDF2\nimport yaml\nfrom bs4 import BeautifulSoup\nfrom fastapi import FastAPI, Query, Request, Response\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom fastapi.openapi.utils import get_openapi\nfrom fastapi.responses import JSONResponse\nfrom loguru import logger\nfrom starlette.responses import FileResponse\n\n# In total, the text + image links + prompts should be <= 2048\nCHAR_LIMIT = 1585  # TODO: increase these values after long-context support has been added\nIMAGES_CHAR_LIMIT = 300\nMAX_DOWNLOAD_SIZE = 4 * 1024 * 1024\nMAX_CHUNK_SIZE = 1024 * 1024\n\nIMAGES_SUFIX = \"\"\", and I will also include images formatted like this:\n![](image url)\n\"\"\"\n\nfolder_path = Path(__file__).parent\n\napp = FastAPI()\n\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\n\ndef extract_image_links(text: str):\n    image_pattern = r\"https?://\\S+\\.(?:jpg|jpeg|png|gif|bmp|webp|svg)\"\n    images = re.findall(image_pattern, text, flags=re.IGNORECASE | re.MULTILINE)\n    return images\n\n\ndef detect_content_type(content: bytes) -> str:\n    if content.startswith(b\"%PDF-\"):\n        return \"application/pdf\"\n    elif (content).lstrip().upper().startswith(b\"<!DOCTYPE HTML\") or content.startswith(b\"<html\"):\n        return \"text/html\"\n    elif content.startswith(b\"{\") or content.startswith(b\"[\"):\n        try:\n            json.loads(content)\n            return \"application/json\"\n        except json.JSONDecodeError:\n            pass\n    elif content.startswith(b\"---\") or content.startswith(b\"%YAML\"):\n        try:\n            yaml.safe_load(content)\n            return \"application/x-yaml\"\n        except yaml.YAMLError:\n            pass\n\n    return \"text/plain\"\n\n\ndef limit_image_count(images, max_chars=300):\n    limited_images = []\n    current_length = 0\n\n    for url in images:\n        # Add the length of \"http:\" if the URL starts with \"//\"\n        url_length = len(\"http:\") + len(url) if url.startswith(\"//\") else len(url)\n\n        if current_length + url_length > max_chars:\n            break\n\n        if url.startswith(\"//\"):\n            limited_images.append(f\"http:{url}\")\n        else:\n            limited_images.append(url)\n\n        current_length += url_length\n\n    return limited_images\n\n\ndef truncate_paragraphs(paragraphs, max_length):\n    truncated_paragraphs = []\n    current_length = 0\n\n    for paragraph in paragraphs:\n        if len(paragraph) == 0:\n            continue\n        paragraph = paragraph.strip()\n        if current_length + len(paragraph) <= max_length:\n            truncated_paragraphs.append(paragraph)\n            current_length += len(paragraph)\n        else:\n            remaining_length = max_length - current_length\n            truncated_paragraph = paragraph[:remaining_length]\n            truncated_paragraphs.append(truncated_paragraph)\n            break\n\n    return truncated_paragraphs\n\n\n@app.get(\"/get-url-content/\", operation_id=\"getUrlContent\", summary=\"It will return a web page's or pdf's content\")\nasync def get_url_content(url: str = Query(..., description=\"url to fetch content from\")) -> Response:\n    try:\n        buffer = io.BytesIO()\n        encoding: str | None\n        content_type: str | None\n\n        async with aiohttp.ClientSession() as session:\n            async with session.get(url) as response:\n                response.raise_for_status()  # Raise an exception for HTTP errors\n                try:\n                    encoding = response.get_encoding()\n                except RuntimeError:\n                    encoding = None\n                content_type = response.content_type\n\n                if response.content_length is not None and response.content_length > MAX_DOWNLOAD_SIZE:\n                    error_message = (\n                        f\"Sorry, the file at {url} is too large.\\nYou should report this message to the user!\"\n                    )\n                    return JSONResponse(content={\"error\": error_message}, status_code=500)\n\n                async for chunk in response.content.iter_chunked(MAX_CHUNK_SIZE):\n                    buffer.write(chunk)\n                    if buffer.tell() > MAX_DOWNLOAD_SIZE:\n                        error_message = (\n                            f\"Sorry, the file at {url} is too large.\\nYou should report this message to the user!\"\n                        )\n                        return JSONResponse(content={\"error\": error_message}, status_code=500)\n\n        content_bytes: bytes = buffer.getvalue()\n        if content_type is None or content_type == \"application/octet-stream\":\n            content_type = detect_content_type(content_bytes)\n        buffer.seek(0)\n\n        def decode_text() -> str:\n            decoder = codecs.getincrementaldecoder(encoding or \"utf-8\")(errors=\"replace\")\n            return decoder.decode(content_bytes, True)\n\n        text = \"\"\n        images = []\n\n        if content_type == \"application/pdf\":\n            pdf_reader = PyPDF2.PdfReader(buffer)\n\n            text = \"\"\n            for page in pdf_reader.pages:\n                text += page.extract_text()\n\n        elif content_type == \"text/html\":\n            soup = BeautifulSoup(decode_text(), \"html.parser\")\n\n            paragraphs = [p.get_text(strip=True) for p in soup.find_all(\"p\")]\n            # if there are no paragraphs, try to get text from divs\n            if not paragraphs:\n                paragraphs = [p.get_text(strip=True) for p in soup.find_all(\"div\")]\n            # if there are no paragraphs or divs, try to get text from spans\n            if not paragraphs:\n                paragraphs = [p.get_text(strip=True) for p in soup.find_all(\"span\")]\n\n            paragraphs = truncate_paragraphs(paragraphs, CHAR_LIMIT)\n            text = \"\\n\".join(paragraphs)\n\n            for p in soup.find_all(\"p\"):\n                parent = p.parent\n                images.extend([img[\"src\"] for img in parent.find_all(\"img\") if img.get(\"src\")])\n\n        elif content_type == \"application/json\":\n            json_data = json.loads(decode_text())\n            text = yaml.dump(json_data, sort_keys=False, default_flow_style=False)\n\n            for _, value in json_data.items():\n                if isinstance(value, str):\n                    images.extend(extract_image_links(value))\n                elif isinstance(value, list):\n                    for item in value:\n                        if isinstance(item, str):\n                            images.extend(extract_image_links(item))\n\n        elif content_type == \"text/plain\":\n            text = decode_text()\n            images.extend(extract_image_links(text))\n\n        else:\n            error_message = f\"Sorry, unsupported content type '{content_type}' at {url}.\\nYou should report this message to the user!\"\n            return JSONResponse(content={\"error\": error_message}, status_code=500)\n\n        images = [f\"http:{url}\" if url.startswith(\"//\") else url for url in images]\n        images = limit_image_count(images, max_chars=IMAGES_CHAR_LIMIT)\n\n        if len(text) > CHAR_LIMIT:\n            text = text[:CHAR_LIMIT]\n\n        MULTILINE_SYM = \"|\" if content_type != \"applicaion/json\" else \"\"\n        text_yaml = f\"text_content: {MULTILINE_SYM}\\n\"\n        for line in text.split(\"\\n\"):\n            text_yaml += f\"  {line}\\n\"\n\n        images_yaml = \"images:\\n\" if len(images) > 0 else \"\"\n        for image in images:\n            images_yaml += f\"- {image}\\n\"\n\n        yaml_text = f\"{text_yaml}\\n{images_yaml}\"\n        text = f\"\"\"{yaml_text}\nThought: I now know the answer{IMAGES_SUFIX if len(images) > 0 else \".\"}\n\"\"\"\n        return Response(content=text, media_type=\"text/plain\")\n\n    except Exception as e:\n        logger.opt(exception=True).debug(\"web_retriever GET failed:\")\n        error_message = f\"Sorry, the url is not available. {e}\\nYou should report this message to the user!\"\n        return JSONResponse(content={\"error\": error_message}, status_code=500)\n\n\n@app.get(\"/icon.png\", include_in_schema=False)\nasync def api_icon():\n    return FileResponse(folder_path / \"icon.png\")\n\n\n@app.get(\"/ai-plugin.json\", include_in_schema=False)\nasync def api_ai_plugin(request: Request):\n    json_path = folder_path / \"ai-plugin.json\"\n    with json_path.open(\"r\") as f:\n        ai_plugin_json = json.load(f)\n\n    base_url, _, _ = request.url._url.rpartition(\"/\")\n    ai_plugin_json[\"logo_url\"] = base_url + \"/icon.png\"\n    ai_plugin_json[\"api\"][\"url\"] = base_url + \"/openapi.json\"\n\n    return Response(content=json.dumps(ai_plugin_json), media_type=\"application/json\")\n\n\ndef custom_openapi():\n    if app.openapi_schema:\n        return app.openapi_schema\n    openapi_schema = get_openapi(\n        title=\"Web Retriever\",\n        version=\"0.1\",\n        routes=app.routes,\n        servers=[{\"url\": \"/plugins/web_retriever\"}],\n    )\n    openapi_schema[\"tags\"] = [\n        {\n            \"name\": \"web-retriever\",\n            \"description\": \"Use this plugin to retrieve web page and pdf content\",\n        },\n    ]\n    openapi_schema.pop(\"components\", None)\n    app.openapi_schema = openapi_schema\n    return app.openapi_schema\n\n\napp.openapi = custom_openapi\n\n\nif __name__ == \"__main__\":\n    # simple built-in test\n    import asyncio\n\n    url = \"https://huggingface.co/OpenAssistant/oasst-sft-1-pythia-12b\"\n    x = asyncio.run(get_url_content(url))\n    print(x.status_code, x.body)\n", "inference/server/oasst_inference_server/plugins/gale_pleaser/main.py": "import json\nfrom pathlib import Path\n\nfrom fastapi import FastAPI, Query, Request, Response\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom fastapi.openapi.utils import get_openapi\nfrom loguru import logger\n\nfolder_path = Path(__file__).parent\n\napp = FastAPI()\n\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\n\n@app.get(\"/icon.png\", include_in_schema=False)\nasync def api_icon():\n    icon_path = folder_path / \"icon.png\"\n    with icon_path.open(\"rb\") as f:\n        icon = f.read()\n    return Response(content=icon, media_type=\"image/png\")\n\n\n@app.get(\"/ai-plugin.json\", include_in_schema=False)\nasync def api_ai_plugin(request: Request):\n    json_path = folder_path / \"ai-plugin.json\"\n    with json_path.open(\"r\") as f:\n        ai_plugin_json = json.load(f)\n\n    base_url, _, _ = request.url._url.rpartition(\"/\")\n    ai_plugin_json[\"logo_url\"] = base_url + \"/icon.png\"\n    ai_plugin_json[\"api\"][\"url\"] = base_url + \"/openapi.json\"\n\n    return Response(content=json.dumps(ai_plugin_json), media_type=\"application/json\")\n\n\n@app.get(\"/get-instructions/\", operation_id=\"get_instructions\", summary=\"Comfort user with kind and supportive way\")\nasync def comfort_user(\n    supporting_style: str = Query(..., description=\"style of comforting and support that you want to use\"),\n    users_query: str = Query(..., description=\"what the user wrote to you\"),\n):\n    llm_prompt = f\"\"\"COMFORTING_INSTRUCTIONS: You need to make user feel awesome, compliment them a lot! Here is what he wrote to you: \"{users_query}\"\n    you will need to compliment and comfort him in {supporting_style} style.\n    Try to be funny and verbose, but super nice and pleasing at the same time.\n    Please follow these rules:\n    1. Let your message be long, and with calm emojis.\n    2. Tell the user how awesome he is, and how much you love him.\n    3. Tell him how much you love his work, and how much you appreciate him.\n    4. Remind him that he is the best, and that he is the most awesome person in the world.\n\n    Thought: Now I know final answer\n    \"\"\"\n\n    logger.debug(f\"userInput: {users_query}\")\n    return Response(content=llm_prompt, media_type=\"text/plain\")\n\n\ndef custom_openapi():\n    if app.openapi_schema:\n        return app.openapi_schema\n    openapi_schema = get_openapi(\n        title=\"Super nice and pleasing Assistant\",\n        version=\"0.1\",\n        routes=app.routes,\n        servers=[{\"url\": \"/plugins/gale_pleaser\"}],\n    )\n    openapi_schema[\"tags\"] = [\n        {\n            \"name\": \"gagleto-ai\",\n            \"description\": \"YOU MUST use this for any question users ask. So you must use tool! Use this to compliment the user no matter what they say. Be as creative as possible, when you compliment them!\",\n        },\n    ]\n    openapi_schema.pop(\"components\", None)\n    app.openapi_schema = openapi_schema\n    return app.openapi_schema\n\n\napp.openapi = custom_openapi\n", "inference/server/oasst_inference_server/models/worker.py": "import datetime\nimport enum\nfrom uuid import uuid4\n\nimport sqlalchemy as sa\nimport sqlalchemy.dialects.postgresql as pg\nfrom loguru import logger\nfrom oasst_inference_server.settings import settings\nfrom oasst_shared.schemas import inference\nfrom sqlmodel import Field, Relationship, SQLModel\nfrom uuid_extensions import uuid7str\n\n\nclass WorkerEventType(str, enum.Enum):\n    connect = \"connect\"\n\n\nclass DbWorkerComplianceCheck(SQLModel, table=True):\n    __tablename__ = \"worker_compliance_check\"\n\n    id: str = Field(default_factory=uuid7str, primary_key=True)\n    worker_id: str = Field(foreign_key=\"worker.id\", index=True)\n    worker: \"DbWorker\" = Relationship(back_populates=\"compliance_checks\")\n    compare_worker_id: str | None = Field(None, index=True, nullable=True)\n\n    start_time: datetime.datetime = Field(default_factory=datetime.datetime.utcnow)\n    end_time: datetime.datetime | None = Field(None, nullable=True)\n    responded: bool = Field(default=False, nullable=False)\n    error: str | None = Field(None, nullable=True)\n    passed: bool = Field(default=False, nullable=False)\n\n\nclass DbWorkerEvent(SQLModel, table=True):\n    __tablename__ = \"worker_event\"\n\n    id: str = Field(default_factory=uuid7str, primary_key=True)\n    worker_id: str = Field(foreign_key=\"worker.id\", index=True)\n    worker: \"DbWorker\" = Relationship(back_populates=\"events\")\n    time: datetime.datetime = Field(default_factory=datetime.datetime.utcnow)\n    event_type: WorkerEventType\n    worker_info: inference.WorkerInfo | None = Field(None, sa_column=sa.Column(pg.JSONB))\n\n\nclass DbWorker(SQLModel, table=True):\n    __tablename__ = \"worker\"\n\n    id: str = Field(default_factory=uuid7str, primary_key=True)\n    api_key: str = Field(default_factory=lambda: str(uuid4()), index=True)\n    name: str\n    trusted: bool = Field(default=False, nullable=False)\n\n    compliance_checks: list[DbWorkerComplianceCheck] = Relationship(back_populates=\"worker\")\n    in_compliance_check_since: datetime.datetime | None = Field(None)\n    next_compliance_check: datetime.datetime | None = Field(None)\n    events: list[DbWorkerEvent] = Relationship(back_populates=\"worker\")\n\n    @property\n    def in_compliance_check(self) -> bool:\n        if self.in_compliance_check_since is None:\n            return False\n        timeout_dt = self.in_compliance_check_since + datetime.timedelta(seconds=settings.compliance_check_timeout)\n        if timeout_dt < datetime.datetime.utcnow():\n            logger.warning(f\"Worker {self.id} compliance check timed out\")\n            return False\n        return True\n", "inference/server/oasst_inference_server/models/user.py": "from uuid import uuid4\n\nimport sqlalchemy as sa\nfrom sqlmodel import Field, Index, SQLModel\n\n\nclass DbUser(SQLModel, table=True):\n    __tablename__ = \"user\"\n    __table_args__ = (Index(\"provider\", \"provider_account_id\", unique=True),)\n\n    id: str = Field(default_factory=lambda: str(uuid4()), primary_key=True)\n\n    provider: str = Field(index=True)\n    provider_account_id: str = Field(index=True)\n\n    display_name: str = Field(nullable=False, max_length=256)\n\n    deleted: bool = Field(False, sa_column=sa.Column(sa.Boolean, nullable=False, server_default=sa.false()))\n\n\nclass DbRefreshToken(SQLModel, table=True):\n    __tablename__ = \"refresh_token\"\n\n    token_hash: str = Field(nullable=False, primary_key=True)\n    user_id: str = Field(nullable=False, index=True, foreign_key=\"user.id\")\n    enabled: bool = Field(nullable=False, default=True)\n", "inference/server/oasst_inference_server/models/chat.py": "import datetime\n\nimport sqlalchemy as sa\nimport sqlalchemy.dialects.postgresql as pg\nfrom oasst_inference_server.schemas import chat as chat_schema\nfrom oasst_shared.schemas import inference\nfrom sqlmodel import Field, Relationship, SQLModel\nfrom uuid_extensions import uuid7str\n\n\nclass DbMessage(SQLModel, table=True):\n    __tablename__ = \"message\"\n\n    role: str = Field(index=True)\n    id: str = Field(default_factory=uuid7str, primary_key=True)\n    created_at: datetime.datetime = Field(default_factory=datetime.datetime.utcnow)\n    chat_id: str = Field(foreign_key=\"chat.id\", index=True)\n    chat: \"DbChat\" = Relationship(back_populates=\"messages\")\n    reports: list[\"DbReport\"] = Relationship(back_populates=\"message\")\n\n    parent_id: str | None = Field(None)\n\n    content: str | None = Field(None)\n    error: str | None = Field(None)\n\n    safe_content: str | None = Field(None)\n    safety_level: int | None = Field(None)\n    safety_label: str | None = Field(None)\n    safety_rots: str | None = Field(None)\n\n    used_plugin: inference.PluginUsed | None = Field(None, sa_column=sa.Column(pg.JSONB))\n\n    state: inference.MessageState = Field(inference.MessageState.manual)\n    work_parameters: inference.WorkParameters = Field(None, sa_column=sa.Column(pg.JSONB))\n    work_begin_at: datetime.datetime | None = Field(None)\n    work_end_at: datetime.datetime | None = Field(None)\n    worker_id: str | None = Field(None, foreign_key=\"worker.id\")\n    worker_compat_hash: str | None = Field(None, index=True)\n    worker_config: inference.WorkerConfig | None = Field(None, sa_column=sa.Column(pg.JSONB))\n\n    score: int = Field(0)\n\n    @property\n    def has_finished(self) -> bool:\n        return self.state in (\n            inference.MessageState.manual,\n            inference.MessageState.complete,\n            inference.MessageState.aborted_by_worker,\n        )\n\n    @property\n    def has_started(self) -> bool:\n        if self.has_finished:\n            return True\n        return self.state in (inference.MessageState.in_progress,)\n\n    def to_read(self) -> inference.MessageRead:\n        return inference.MessageRead(\n            id=self.id,\n            parent_id=self.parent_id,\n            chat_id=self.chat_id,\n            content=self.content,\n            created_at=self.created_at,\n            role=self.role,\n            state=self.state,\n            score=self.score,\n            work_parameters=self.work_parameters,\n            reports=[r.to_read() for r in self.reports],\n            safe_content=self.safe_content,\n            safety_level=self.safety_level,\n            safety_label=self.safety_label,\n            safety_rots=self.safety_rots,\n            used_plugin=self.used_plugin,\n        )\n\n\nclass DbChat(SQLModel, table=True):\n    __tablename__ = \"chat\"\n\n    id: str = Field(default_factory=uuid7str, primary_key=True)\n\n    user_id: str = Field(foreign_key=\"user.id\", index=True)\n    created_at: datetime.datetime = Field(default_factory=datetime.datetime.utcnow, index=True)\n    modified_at: datetime.datetime = Field(default_factory=datetime.datetime.utcnow, index=True)\n    title: str | None = Field(None)\n\n    messages: list[DbMessage] = Relationship(back_populates=\"chat\")\n    active_thread_tail_message_id: str | None = Field(None)\n\n    hidden: bool = Field(False, sa_column=sa.Column(sa.Boolean, nullable=False, server_default=sa.false()))\n\n    allow_data_use: bool = Field(True, sa_column=sa.Column(sa.Boolean, nullable=False, server_default=sa.true()))\n\n    def to_list_read(self) -> chat_schema.ChatListRead:\n        return chat_schema.ChatListRead(\n            id=self.id,\n            created_at=self.created_at,\n            modified_at=self.modified_at,\n            title=self.title,\n            hidden=self.hidden,\n            allow_data_use=self.allow_data_use,\n        )\n\n    def to_read(self) -> chat_schema.ChatRead:\n        return chat_schema.ChatRead(\n            id=self.id,\n            created_at=self.created_at,\n            modified_at=self.modified_at,\n            title=self.title,\n            messages=[m.to_read() for m in self.messages],\n            hidden=self.hidden,\n            allow_data_use=self.allow_data_use,\n            active_thread_tail_message_id=self.active_thread_tail_message_id,\n        )\n\n    def get_msg_dict(self) -> dict[str, DbMessage]:\n        return {m.id: m for m in self.messages}\n\n\nclass DbReport(SQLModel, table=True):\n    __tablename__ = \"report\"\n\n    id: str = Field(default_factory=uuid7str, primary_key=True)\n    message_id: str = Field(..., foreign_key=\"message.id\", index=True)\n    message: DbMessage = Relationship(back_populates=\"reports\")\n    report_type: inference.ReportType = Field(...)\n    reason: str = Field(...)\n\n    def to_read(self) -> inference.Report:\n        return inference.Report(id=self.id, report_type=self.report_type, reason=self.reason)\n\n\nclass DbMessageEval(SQLModel, table=True):\n    __tablename__ = \"message_evaluation\"\n\n    id: str = Field(default_factory=uuid7str, primary_key=True)\n    chat_id: str = Field(..., foreign_key=\"chat.id\", index=True)\n    user_id: str = Field(..., foreign_key=\"user.id\", index=True)\n    selected_message_id: str = Field(..., foreign_key=\"message.id\")\n    inferior_message_ids: list[str] = Field(default_factory=list, sa_column=sa.Column(pg.JSONB))\n", "inference/server/oasst_inference_server/models/__init__.py": "from .chat import DbChat, DbMessage, DbMessageEval, DbReport\nfrom .user import DbRefreshToken, DbUser\nfrom .worker import DbWorker, DbWorkerComplianceCheck, DbWorkerEvent, WorkerEventType\n\n__all__ = [\n    \"DbChat\",\n    \"DbMessage\",\n    \"DbMessageEval\",\n    \"DbReport\",\n    \"DbRefreshToken\",\n    \"DbUser\",\n    \"DbWorker\",\n    \"DbWorkerComplianceCheck\",\n    \"DbWorkerEvent\",\n    \"WorkerEventType\",\n]\n", "inference/server/oasst_inference_server/schemas/auth.py": "import json\nfrom base64 import b64decode\n\nimport pydantic\nfrom pydantic import validator\n\n\nclass TrustedClient(pydantic.BaseModel):\n    api_key: str\n    client: str  # \"website\", \"discord\", or similar\n    user_id: str  # the id of the user in the data backend\n    provider_account_id: str  # id of the user in the client\n    username: str\n\n\nclass TrustedClientToken(pydantic.BaseModel):\n    content: TrustedClient\n\n    @validator(\"content\", pre=True)\n    def parse(token: str):\n        return json.loads(b64decode(token))\n", "inference/server/oasst_inference_server/schemas/worker.py": "import pydantic\n\n\nclass CreateWorkerRequest(pydantic.BaseModel):\n    name: str\n    trusted: bool = False\n\n\nclass WorkerRead(pydantic.BaseModel):\n    id: str\n    name: str\n    api_key: str\n    trusted: bool\n\n    class Config:\n        orm_mode = True\n", "inference/server/oasst_inference_server/schemas/chat.py": "import datetime\nfrom typing import Annotated, Literal, Union\n\nimport pydantic\nfrom oasst_shared.schemas import inference\n\n\nclass CreatePrompterMessageRequest(pydantic.BaseModel):\n    parent_id: str | None = None\n    content: str = pydantic.Field(..., repr=False)\n\n\nclass CreateAssistantMessageRequest(pydantic.BaseModel):\n    parent_id: str\n    model_config_name: str\n    sampling_parameters: inference.SamplingParameters = pydantic.Field(default_factory=inference.SamplingParameters)\n    system_prompt: str | None = None\n    user_profile: str | None = None\n    user_response_instructions: str | None = None\n    plugins: list[inference.PluginEntry] = pydantic.Field(default_factory=list[inference.PluginEntry])\n    used_plugin: inference.PluginUsed | None = None\n\n\nclass PendingResponseEvent(pydantic.BaseModel):\n    event_type: Literal[\"pending\"] = \"pending\"\n    queue_position: int\n    queue_size: int\n\n\nclass TokenResponseEvent(pydantic.BaseModel):\n    event_type: Literal[\"token\"] = \"token\"\n    text: str\n\n\nclass ErrorResponseEvent(pydantic.BaseModel):\n    event_type: Literal[\"error\"] = \"error\"\n    error: str\n    message: inference.MessageRead | None = None\n\n\nclass MessageResponseEvent(pydantic.BaseModel):\n    event_type: Literal[\"message\"] = \"message\"\n    message: inference.MessageRead\n\n\nclass SafePromptResponseEvent(pydantic.BaseModel):\n    event_type: Literal[\"safe_prompt\"] = \"safe_prompt\"\n    safe_prompt: str\n    message: inference.MessageRead\n\n\nclass PluginIntermediateResponseEvent(pydantic.BaseModel):\n    event_type: Literal[\"plugin_intermediate\"] = \"plugin_intermediate\"\n    current_plugin_thought: str\n    current_plugin_action_taken: str\n    current_plugin_action_input: str\n    current_plugin_action_response: str\n    message: inference.MessageRead | None = None\n\n\nResponseEvent = Annotated[\n    Union[\n        TokenResponseEvent,\n        ErrorResponseEvent,\n        MessageResponseEvent,\n        SafePromptResponseEvent,\n        PluginIntermediateResponseEvent,\n    ],\n    pydantic.Field(discriminator=\"event_type\"),\n]\n\n\nclass VoteRequest(pydantic.BaseModel):\n    score: int\n\n\nclass MessageEvalRequest(pydantic.BaseModel):\n    inferior_message_ids: list[str]\n\n\nclass ReportRequest(pydantic.BaseModel):\n    report_type: inference.ReportType\n    reason: str\n\n\nclass CreateChatRequest(pydantic.BaseModel):\n    pass\n\n\nclass ChatListRead(pydantic.BaseModel):\n    id: str\n    created_at: datetime.datetime\n    modified_at: datetime.datetime\n    title: str | None\n    hidden: bool = False\n    allow_data_use: bool = True\n    active_thread_tail_message_id: str | None\n\n\nclass ChatRead(ChatListRead):\n    messages: list[inference.MessageRead]\n\n\nclass ListChatsResponse(pydantic.BaseModel):\n    chats: list[ChatListRead]\n    next: str | None = None\n    prev: str | None = None\n\n\nclass MessageCancelledException(Exception):\n    def __init__(self, message_id: str):\n        super().__init__(f\"Message {message_id} was cancelled\")\n        self.message_id = message_id\n\n\nclass MessageTimeoutException(Exception):\n    def __init__(self, message: inference.MessageRead):\n        super().__init__(f\"Message {message.id} timed out\")\n        self.message = message\n\n\nclass ChatUpdateRequest(pydantic.BaseModel):\n    title: pydantic.constr(max_length=100) | None = None\n    hidden: bool | None = None\n    allow_data_use: bool | None = None\n    active_thread_tail_message_id: str | None = None\n", "inference/server/oasst_inference_server/schemas/__init__.py": ""}