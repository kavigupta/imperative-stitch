{"setup.py": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\nfrom setuptools import setup\n\nwith open('README.rst') as readme_file:\n    readme = readme_file.read()\n\nwith open('HISTORY.rst') as history_file:\n    history = history_file.read()\n\nrequirements = [\n    'face_recognition_models>=0.3.0',\n    'Click>=6.0',\n    'dlib>=19.7',\n    'numpy',\n    'Pillow'\n]\n\ntest_requirements = [\n    'tox',\n    'flake8'\n]\n\nsetup(\n    name='face_recognition',\n    version='1.4.0',\n    description=\"Recognize faces from Python or from the command line\",\n    long_description=readme + '\\n\\n' + history,\n    author=\"Adam Geitgey\",\n    author_email='ageitgey@gmail.com',\n    url='https://github.com/ageitgey/face_recognition',\n    packages=[\n        'face_recognition',\n    ],\n    package_dir={'face_recognition': 'face_recognition'},\n    package_data={\n        'face_recognition': ['models/*.dat']\n    },\n    entry_points={\n        'console_scripts': [\n            'face_recognition=face_recognition.face_recognition_cli:main',\n            'face_detection=face_recognition.face_detection_cli:main'\n        ]\n    },\n    install_requires=requirements,\n    license=\"MIT license\",\n    zip_safe=False,\n    keywords='face_recognition',\n    classifiers=[\n        'Development Status :: 4 - Beta',\n        'Intended Audience :: Developers',\n        'License :: OSI Approved :: MIT License',\n        'Natural Language :: English',\n        'Programming Language :: Python :: 3',\n        'Programming Language :: Python :: 3.5',\n        'Programming Language :: Python :: 3.6',\n        'Programming Language :: Python :: 3.7',\n        'Programming Language :: Python :: 3.8',\n        'Programming Language :: Python :: 3.9',\n    ],\n    test_suite='tests',\n    tests_require=test_requirements\n)\n", "face_recognition/api.py": "# -*- coding: utf-8 -*-\n\nimport PIL.Image\nimport dlib\nimport numpy as np\nfrom PIL import ImageFile\n\ntry:\n    import face_recognition_models\nexcept Exception:\n    print(\"Please install `face_recognition_models` with this command before using `face_recognition`:\\n\")\n    print(\"pip install git+https://github.com/ageitgey/face_recognition_models\")\n    quit()\n\nImageFile.LOAD_TRUNCATED_IMAGES = True\n\nface_detector = dlib.get_frontal_face_detector()\n\npredictor_68_point_model = face_recognition_models.pose_predictor_model_location()\npose_predictor_68_point = dlib.shape_predictor(predictor_68_point_model)\n\npredictor_5_point_model = face_recognition_models.pose_predictor_five_point_model_location()\npose_predictor_5_point = dlib.shape_predictor(predictor_5_point_model)\n\ncnn_face_detection_model = face_recognition_models.cnn_face_detector_model_location()\ncnn_face_detector = dlib.cnn_face_detection_model_v1(cnn_face_detection_model)\n\nface_recognition_model = face_recognition_models.face_recognition_model_location()\nface_encoder = dlib.face_recognition_model_v1(face_recognition_model)\n\n\ndef _rect_to_css(rect):\n    \"\"\"\n    Convert a dlib 'rect' object to a plain tuple in (top, right, bottom, left) order\n\n    :param rect: a dlib 'rect' object\n    :return: a plain tuple representation of the rect in (top, right, bottom, left) order\n    \"\"\"\n    return rect.top(), rect.right(), rect.bottom(), rect.left()\n\n\ndef _css_to_rect(css):\n    \"\"\"\n    Convert a tuple in (top, right, bottom, left) order to a dlib `rect` object\n\n    :param css:  plain tuple representation of the rect in (top, right, bottom, left) order\n    :return: a dlib `rect` object\n    \"\"\"\n    return dlib.rectangle(css[3], css[0], css[1], css[2])\n\n\ndef _trim_css_to_bounds(css, image_shape):\n    \"\"\"\n    Make sure a tuple in (top, right, bottom, left) order is within the bounds of the image.\n\n    :param css:  plain tuple representation of the rect in (top, right, bottom, left) order\n    :param image_shape: numpy shape of the image array\n    :return: a trimmed plain tuple representation of the rect in (top, right, bottom, left) order\n    \"\"\"\n    return max(css[0], 0), min(css[1], image_shape[1]), min(css[2], image_shape[0]), max(css[3], 0)\n\n\ndef face_distance(face_encodings, face_to_compare):\n    \"\"\"\n    Given a list of face encodings, compare them to a known face encoding and get a euclidean distance\n    for each comparison face. The distance tells you how similar the faces are.\n\n    :param face_encodings: List of face encodings to compare\n    :param face_to_compare: A face encoding to compare against\n    :return: A numpy ndarray with the distance for each face in the same order as the 'faces' array\n    \"\"\"\n    if len(face_encodings) == 0:\n        return np.empty((0))\n\n    return np.linalg.norm(face_encodings - face_to_compare, axis=1)\n\n\ndef load_image_file(file, mode='RGB'):\n    \"\"\"\n    Loads an image file (.jpg, .png, etc) into a numpy array\n\n    :param file: image file name or file object to load\n    :param mode: format to convert the image to. Only 'RGB' (8-bit RGB, 3 channels) and 'L' (black and white) are supported.\n    :return: image contents as numpy array\n    \"\"\"\n    im = PIL.Image.open(file)\n    if mode:\n        im = im.convert(mode)\n    return np.array(im)\n\n\ndef _raw_face_locations(img, number_of_times_to_upsample=1, model=\"hog\"):\n    \"\"\"\n    Returns an array of bounding boxes of human faces in a image\n\n    :param img: An image (as a numpy array)\n    :param number_of_times_to_upsample: How many times to upsample the image looking for faces. Higher numbers find smaller faces.\n    :param model: Which face detection model to use. \"hog\" is less accurate but faster on CPUs. \"cnn\" is a more accurate\n                  deep-learning model which is GPU/CUDA accelerated (if available). The default is \"hog\".\n    :return: A list of dlib 'rect' objects of found face locations\n    \"\"\"\n    if model == \"cnn\":\n        return cnn_face_detector(img, number_of_times_to_upsample)\n    else:\n        return face_detector(img, number_of_times_to_upsample)\n\n\ndef face_locations(img, number_of_times_to_upsample=1, model=\"hog\"):\n    \"\"\"\n    Returns an array of bounding boxes of human faces in a image\n\n    :param img: An image (as a numpy array)\n    :param number_of_times_to_upsample: How many times to upsample the image looking for faces. Higher numbers find smaller faces.\n    :param model: Which face detection model to use. \"hog\" is less accurate but faster on CPUs. \"cnn\" is a more accurate\n                  deep-learning model which is GPU/CUDA accelerated (if available). The default is \"hog\".\n    :return: A list of tuples of found face locations in css (top, right, bottom, left) order\n    \"\"\"\n    if model == \"cnn\":\n        return [_trim_css_to_bounds(_rect_to_css(face.rect), img.shape) for face in _raw_face_locations(img, number_of_times_to_upsample, \"cnn\")]\n    else:\n        return [_trim_css_to_bounds(_rect_to_css(face), img.shape) for face in _raw_face_locations(img, number_of_times_to_upsample, model)]\n\n\ndef _raw_face_locations_batched(images, number_of_times_to_upsample=1, batch_size=128):\n    \"\"\"\n    Returns an 2d array of dlib rects of human faces in a image using the cnn face detector\n\n    :param images: A list of images (each as a numpy array)\n    :param number_of_times_to_upsample: How many times to upsample the image looking for faces. Higher numbers find smaller faces.\n    :return: A list of dlib 'rect' objects of found face locations\n    \"\"\"\n    return cnn_face_detector(images, number_of_times_to_upsample, batch_size=batch_size)\n\n\ndef batch_face_locations(images, number_of_times_to_upsample=1, batch_size=128):\n    \"\"\"\n    Returns an 2d array of bounding boxes of human faces in a image using the cnn face detector\n    If you are using a GPU, this can give you much faster results since the GPU\n    can process batches of images at once. If you aren't using a GPU, you don't need this function.\n\n    :param images: A list of images (each as a numpy array)\n    :param number_of_times_to_upsample: How many times to upsample the image looking for faces. Higher numbers find smaller faces.\n    :param batch_size: How many images to include in each GPU processing batch.\n    :return: A list of tuples of found face locations in css (top, right, bottom, left) order\n    \"\"\"\n    def convert_cnn_detections_to_css(detections):\n        return [_trim_css_to_bounds(_rect_to_css(face.rect), images[0].shape) for face in detections]\n\n    raw_detections_batched = _raw_face_locations_batched(images, number_of_times_to_upsample, batch_size)\n\n    return list(map(convert_cnn_detections_to_css, raw_detections_batched))\n\n\ndef _raw_face_landmarks(face_image, face_locations=None, model=\"large\"):\n    if face_locations is None:\n        face_locations = _raw_face_locations(face_image)\n    else:\n        face_locations = [_css_to_rect(face_location) for face_location in face_locations]\n\n    pose_predictor = pose_predictor_68_point\n\n    if model == \"small\":\n        pose_predictor = pose_predictor_5_point\n\n    return [pose_predictor(face_image, face_location) for face_location in face_locations]\n\n\ndef face_landmarks(face_image, face_locations=None, model=\"large\"):\n    \"\"\"\n    Given an image, returns a dict of face feature locations (eyes, nose, etc) for each face in the image\n\n    :param face_image: image to search\n    :param face_locations: Optionally provide a list of face locations to check.\n    :param model: Optional - which model to use. \"large\" (default) or \"small\" which only returns 5 points but is faster.\n    :return: A list of dicts of face feature locations (eyes, nose, etc)\n    \"\"\"\n    landmarks = _raw_face_landmarks(face_image, face_locations, model)\n    landmarks_as_tuples = [[(p.x, p.y) for p in landmark.parts()] for landmark in landmarks]\n\n    # For a definition of each point index, see https://cdn-images-1.medium.com/max/1600/1*AbEg31EgkbXSQehuNJBlWg.png\n    if model == 'large':\n        return [{\n            \"chin\": points[0:17],\n            \"left_eyebrow\": points[17:22],\n            \"right_eyebrow\": points[22:27],\n            \"nose_bridge\": points[27:31],\n            \"nose_tip\": points[31:36],\n            \"left_eye\": points[36:42],\n            \"right_eye\": points[42:48],\n            \"top_lip\": points[48:55] + [points[64]] + [points[63]] + [points[62]] + [points[61]] + [points[60]],\n            \"bottom_lip\": points[54:60] + [points[48]] + [points[60]] + [points[67]] + [points[66]] + [points[65]] + [points[64]]\n        } for points in landmarks_as_tuples]\n    elif model == 'small':\n        return [{\n            \"nose_tip\": [points[4]],\n            \"left_eye\": points[2:4],\n            \"right_eye\": points[0:2],\n        } for points in landmarks_as_tuples]\n    else:\n        raise ValueError(\"Invalid landmarks model type. Supported models are ['small', 'large'].\")\n\n\ndef face_encodings(face_image, known_face_locations=None, num_jitters=1, model=\"small\"):\n    \"\"\"\n    Given an image, return the 128-dimension face encoding for each face in the image.\n\n    :param face_image: The image that contains one or more faces\n    :param known_face_locations: Optional - the bounding boxes of each face if you already know them.\n    :param num_jitters: How many times to re-sample the face when calculating encoding. Higher is more accurate, but slower (i.e. 100 is 100x slower)\n    :param model: Optional - which model to use. \"large\" or \"small\" (default) which only returns 5 points but is faster.\n    :return: A list of 128-dimensional face encodings (one for each face in the image)\n    \"\"\"\n    raw_landmarks = _raw_face_landmarks(face_image, known_face_locations, model)\n    return [np.array(face_encoder.compute_face_descriptor(face_image, raw_landmark_set, num_jitters)) for raw_landmark_set in raw_landmarks]\n\n\ndef compare_faces(known_face_encodings, face_encoding_to_check, tolerance=0.6):\n    \"\"\"\n    Compare a list of face encodings against a candidate encoding to see if they match.\n\n    :param known_face_encodings: A list of known face encodings\n    :param face_encoding_to_check: A single face encoding to compare against the list\n    :param tolerance: How much distance between faces to consider it a match. Lower is more strict. 0.6 is typical best performance.\n    :return: A list of True/False values indicating which known_face_encodings match the face encoding to check\n    \"\"\"\n    return list(face_distance(known_face_encodings, face_encoding_to_check) <= tolerance)\n", "face_recognition/face_recognition_cli.py": "# -*- coding: utf-8 -*-\nfrom __future__ import print_function\nimport click\nimport os\nimport re\nimport face_recognition.api as face_recognition\nimport multiprocessing\nimport itertools\nimport sys\nimport PIL.Image\nimport numpy as np\n\n\ndef scan_known_people(known_people_folder):\n    known_names = []\n    known_face_encodings = []\n\n    for file in image_files_in_folder(known_people_folder):\n        basename = os.path.splitext(os.path.basename(file))[0]\n        img = face_recognition.load_image_file(file)\n        encodings = face_recognition.face_encodings(img)\n\n        if len(encodings) > 1:\n            click.echo(\"WARNING: More than one face found in {}. Only considering the first face.\".format(file))\n\n        if len(encodings) == 0:\n            click.echo(\"WARNING: No faces found in {}. Ignoring file.\".format(file))\n        else:\n            known_names.append(basename)\n            known_face_encodings.append(encodings[0])\n\n    return known_names, known_face_encodings\n\n\ndef print_result(filename, name, distance, show_distance=False):\n    if show_distance:\n        print(\"{},{},{}\".format(filename, name, distance))\n    else:\n        print(\"{},{}\".format(filename, name))\n\n\ndef test_image(image_to_check, known_names, known_face_encodings, tolerance=0.6, show_distance=False):\n    unknown_image = face_recognition.load_image_file(image_to_check)\n\n    # Scale down image if it's giant so things run a little faster\n    if max(unknown_image.shape) > 1600:\n        pil_img = PIL.Image.fromarray(unknown_image)\n        pil_img.thumbnail((1600, 1600), PIL.Image.LANCZOS)\n        unknown_image = np.array(pil_img)\n\n    unknown_encodings = face_recognition.face_encodings(unknown_image)\n\n    for unknown_encoding in unknown_encodings:\n        distances = face_recognition.face_distance(known_face_encodings, unknown_encoding)\n        result = list(distances <= tolerance)\n\n        if True in result:\n            [print_result(image_to_check, name, distance, show_distance) for is_match, name, distance in zip(result, known_names, distances) if is_match]\n        else:\n            print_result(image_to_check, \"unknown_person\", None, show_distance)\n\n    if not unknown_encodings:\n        # print out fact that no faces were found in image\n        print_result(image_to_check, \"no_persons_found\", None, show_distance)\n\n\ndef image_files_in_folder(folder):\n    return [os.path.join(folder, f) for f in os.listdir(folder) if re.match(r'.*\\.(jpg|jpeg|png)', f, flags=re.I)]\n\n\ndef process_images_in_process_pool(images_to_check, known_names, known_face_encodings, number_of_cpus, tolerance, show_distance):\n    if number_of_cpus == -1:\n        processes = None\n    else:\n        processes = number_of_cpus\n\n    # macOS will crash due to a bug in libdispatch if you don't use 'forkserver'\n    context = multiprocessing\n    if \"forkserver\" in multiprocessing.get_all_start_methods():\n        context = multiprocessing.get_context(\"forkserver\")\n\n    pool = context.Pool(processes=processes)\n\n    function_parameters = zip(\n        images_to_check,\n        itertools.repeat(known_names),\n        itertools.repeat(known_face_encodings),\n        itertools.repeat(tolerance),\n        itertools.repeat(show_distance)\n    )\n\n    pool.starmap(test_image, function_parameters)\n\n\n@click.command()\n@click.argument('known_people_folder')\n@click.argument('image_to_check')\n@click.option('--cpus', default=1, help='number of CPU cores to use in parallel (can speed up processing lots of images). -1 means \"use all in system\"')\n@click.option('--tolerance', default=0.6, help='Tolerance for face comparisons. Default is 0.6. Lower this if you get multiple matches for the same person.')\n@click.option('--show-distance', default=False, type=bool, help='Output face distance. Useful for tweaking tolerance setting.')\ndef main(known_people_folder, image_to_check, cpus, tolerance, show_distance):\n    known_names, known_face_encodings = scan_known_people(known_people_folder)\n\n    # Multi-core processing only supported on Python 3.4 or greater\n    if (sys.version_info < (3, 4)) and cpus != 1:\n        click.echo(\"WARNING: Multi-processing support requires Python 3.4 or greater. Falling back to single-threaded processing!\")\n        cpus = 1\n\n    if os.path.isdir(image_to_check):\n        if cpus == 1:\n            [test_image(image_file, known_names, known_face_encodings, tolerance, show_distance) for image_file in image_files_in_folder(image_to_check)]\n        else:\n            process_images_in_process_pool(image_files_in_folder(image_to_check), known_names, known_face_encodings, cpus, tolerance, show_distance)\n    else:\n        test_image(image_to_check, known_names, known_face_encodings, tolerance, show_distance)\n\n\nif __name__ == \"__main__\":\n    main()\n", "face_recognition/face_detection_cli.py": "# -*- coding: utf-8 -*-\nfrom __future__ import print_function\nimport click\nimport os\nimport re\nimport face_recognition.api as face_recognition\nimport multiprocessing\nimport sys\nimport itertools\n\n\ndef print_result(filename, location):\n    top, right, bottom, left = location\n    print(\"{},{},{},{},{}\".format(filename, top, right, bottom, left))\n\n\ndef test_image(image_to_check, model, upsample):\n    unknown_image = face_recognition.load_image_file(image_to_check)\n    face_locations = face_recognition.face_locations(unknown_image, number_of_times_to_upsample=upsample, model=model)\n\n    for face_location in face_locations:\n        print_result(image_to_check, face_location)\n\n\ndef image_files_in_folder(folder):\n    return [os.path.join(folder, f) for f in os.listdir(folder) if re.match(r'.*\\.(jpg|jpeg|png)', f, flags=re.I)]\n\n\ndef process_images_in_process_pool(images_to_check, number_of_cpus, model, upsample):\n    if number_of_cpus == -1:\n        processes = None\n    else:\n        processes = number_of_cpus\n\n    # macOS will crash due to a bug in libdispatch if you don't use 'forkserver'\n    context = multiprocessing\n    if \"forkserver\" in multiprocessing.get_all_start_methods():\n        context = multiprocessing.get_context(\"forkserver\")\n\n    pool = context.Pool(processes=processes)\n\n    function_parameters = zip(\n        images_to_check,\n        itertools.repeat(model),\n        itertools.repeat(upsample),\n    )\n\n    pool.starmap(test_image, function_parameters)\n\n\n@click.command()\n@click.argument('image_to_check')\n@click.option('--cpus', default=1, help='number of CPU cores to use in parallel. -1 means \"use all in system\"')\n@click.option('--model', default=\"hog\", help='Which face detection model to use. Options are \"hog\" or \"cnn\".')\n@click.option('--upsample', default=0, help='How many times to upsample the image looking for faces. Higher numbers find smaller faces.')\ndef main(image_to_check, cpus, model, upsample):\n    # Multi-core processing only supported on Python 3.4 or greater\n    if (sys.version_info < (3, 4)) and cpus != 1:\n        click.echo(\"WARNING: Multi-processing support requires Python 3.4 or greater. Falling back to single-threaded processing!\")\n        cpus = 1\n\n    if os.path.isdir(image_to_check):\n        if cpus == 1:\n            [test_image(image_file, model, upsample) for image_file in image_files_in_folder(image_to_check)]\n        else:\n            process_images_in_process_pool(image_files_in_folder(image_to_check), cpus, model, upsample)\n    else:\n        test_image(image_to_check, model, upsample)\n\n\nif __name__ == \"__main__\":\n    main()\n", "face_recognition/__init__.py": "# -*- coding: utf-8 -*-\n\n__author__ = \"\"\"Adam Geitgey\"\"\"\n__email__ = 'ageitgey@gmail.com'\n__version__ = '1.4.0'\n\nfrom .api import load_image_file, face_locations, batch_face_locations, face_landmarks, face_encodings, compare_faces, face_distance\n", "docs/conf.py": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n#\n# face_recognition documentation build configuration file, created by\n# sphinx-quickstart on Tue Jul  9 22:26:36 2013.\n#\n# This file is execfile()d with the current directory set to its\n# containing dir.\n#\n# Note that not all possible configuration values are present in this\n# autogenerated file.\n#\n# All configuration values have a default; values that are commented out\n# serve to show the default.\n\nimport sys\nimport os\nfrom unittest.mock import MagicMock\n\nclass Mock(MagicMock):\n    @classmethod\n    def __getattr__(cls, name):\n            return MagicMock()\n\nMOCK_MODULES = ['face_recognition_models', 'Click', 'dlib', 'numpy', 'PIL']\nsys.modules.update((mod_name, Mock()) for mod_name in MOCK_MODULES)\n\n# If extensions (or modules to document with autodoc) are in another\n# directory, add these directories to sys.path here. If the directory is\n# relative to the documentation root, use os.path.abspath to make it\n# absolute, like shown here.\n#sys.path.insert(0, os.path.abspath('.'))\n\n# Get the project root dir, which is the parent dir of this\ncwd = os.getcwd()\nproject_root = os.path.dirname(cwd)\n\n# Insert the project root dir as the first element in the PYTHONPATH.\n# This lets us ensure that the source package is imported, and that its\n# version is used.\nsys.path.insert(0, project_root)\n\nimport face_recognition\n\n# -- General configuration ---------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#needs_sphinx = '1.0'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named 'sphinx.ext.*') or your custom ones.\nextensions = ['sphinx.ext.autodoc', 'sphinx.ext.viewcode']\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = ['_templates']\n\n# The suffix of source filenames.\nsource_suffix = '.rst'\n\n# The encoding of source files.\n#source_encoding = 'utf-8-sig'\n\n# The master toctree document.\nmaster_doc = 'index'\n\n# General information about the project.\nproject = u'Face Recognition'\ncopyright = u\"2017, Adam Geitgey\"\n\n# The version info for the project you're documenting, acts as replacement\n# for |version| and |release|, also used in various other places throughout\n# the built documents.\n#\n# The short X.Y version.\nversion = face_recognition.__version__\n# The full version, including alpha/beta/rc tags.\nrelease = face_recognition.__version__\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#language = None\n\n# There are two options for replacing |today|: either, you set today to\n# some non-false value, then it is used:\n#today = ''\n# Else, today_fmt is used as the format for a strftime call.\n#today_fmt = '%B %d, %Y'\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\nexclude_patterns = ['_build']\n\n# The reST default role (used for this markup: `text`) to use for all\n# documents.\n#default_role = None\n\n# If true, '()' will be appended to :func: etc. cross-reference text.\n#add_function_parentheses = True\n\n# If true, the current module name will be prepended to all description\n# unit titles (such as .. function::).\n#add_module_names = True\n\n# If true, sectionauthor and moduleauthor directives will be shown in the\n# output. They are ignored by default.\n#show_authors = False\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = 'sphinx'\n\n# A list of ignored prefixes for module index sorting.\n#modindex_common_prefix = []\n\n# If true, keep warnings as \"system message\" paragraphs in the built\n# documents.\n#keep_warnings = False\n\n\n# -- Options for HTML output -------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\nhtml_theme = 'default'\n\n# Theme options are theme-specific and customize the look and feel of a\n# theme further.  For a list of options available for each theme, see the\n# documentation.\n#html_theme_options = {}\n\n# Add any paths that contain custom themes here, relative to this directory.\n#html_theme_path = []\n\n# The name for this set of Sphinx documents.  If None, it defaults to\n# \"<project> v<release> documentation\".\n#html_title = None\n\n# A shorter title for the navigation bar.  Default is the same as\n# html_title.\n#html_short_title = None\n\n# The name of an image file (relative to this directory) to place at the\n# top of the sidebar.\n#html_logo = None\n\n# The name of an image file (within the static path) to use as favicon\n# of the docs.  This file should be a Windows icon file (.ico) being\n# 16x16 or 32x32 pixels large.\n#html_favicon = None\n\n# Add any paths that contain custom static files (such as style sheets)\n# here, relative to this directory. They are copied after the builtin\n# static files, so a file named \"default.css\" will overwrite the builtin\n# \"default.css\".\nhtml_static_path = ['_static']\n\n# If not '', a 'Last updated on:' timestamp is inserted at every page\n# bottom, using the given strftime format.\n#html_last_updated_fmt = '%b %d, %Y'\n\n# If true, SmartyPants will be used to convert quotes and dashes to\n# typographically correct entities.\n#html_use_smartypants = True\n\n# Custom sidebar templates, maps document names to template names.\n#html_sidebars = {}\n\n# Additional templates that should be rendered to pages, maps page names\n# to template names.\n#html_additional_pages = {}\n\n# If false, no module index is generated.\n#html_domain_indices = True\n\n# If false, no index is generated.\n#html_use_index = True\n\n# If true, the index is split into individual pages for each letter.\n#html_split_index = False\n\n# If true, links to the reST sources are added to the pages.\n#html_show_sourcelink = True\n\n# If true, \"Created using Sphinx\" is shown in the HTML footer.\n# Default is True.\n#html_show_sphinx = True\n\n# If true, \"(C) Copyright ...\" is shown in the HTML footer.\n# Default is True.\n#html_show_copyright = True\n\n# If true, an OpenSearch description file will be output, and all pages\n# will contain a <link> tag referring to it.  The value of this option\n# must be the base URL from which the finished HTML is served.\n#html_use_opensearch = ''\n\n# This is the file name suffix for HTML files (e.g. \".xhtml\").\n#html_file_suffix = None\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = 'face_recognitiondoc'\n\n\n# -- Options for LaTeX output ------------------------------------------\n\nlatex_elements = {\n    # The paper size ('letterpaper' or 'a4paper').\n    #'papersize': 'letterpaper',\n\n    # The font size ('10pt', '11pt' or '12pt').\n    #'pointsize': '10pt',\n\n    # Additional stuff for the LaTeX preamble.\n    #'preamble': '',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title, author, documentclass\n# [howto/manual]).\nlatex_documents = [\n    ('index', 'face_recognition.tex',\n     u'Face Recognition Documentation',\n     u'Adam Geitgey', 'manual'),\n]\n\n# The name of an image file (relative to this directory) to place at\n# the top of the title page.\n#latex_logo = None\n\n# For \"manual\" documents, if this is true, then toplevel headings\n# are parts, not chapters.\n#latex_use_parts = False\n\n# If true, show page references after internal links.\n#latex_show_pagerefs = False\n\n# If true, show URL addresses after external links.\n#latex_show_urls = False\n\n# Documents to append as an appendix to all manuals.\n#latex_appendices = []\n\n# If false, no module index is generated.\n#latex_domain_indices = True\n\n\n# -- Options for manual page output ------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [\n    ('index', 'face_recognition',\n     u'Face Recognition Documentation',\n     [u'Adam Geitgey'], 1)\n]\n\n# If true, show URL addresses after external links.\n#man_show_urls = False\n\n\n# -- Options for Texinfo output ----------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n    ('index', 'face_recognition',\n     u'Face Recognition Documentation',\n     u'Adam Geitgey',\n     'face_recognition',\n     'One line description of project.',\n     'Miscellaneous'),\n]\n\n# Documents to append as an appendix to all manuals.\n#texinfo_appendices = []\n\n# If false, no module index is generated.\n#texinfo_domain_indices = True\n\n# How to display URL addresses: 'footnote', 'no', or 'inline'.\n#texinfo_show_urls = 'footnote'\n\n# If true, do not generate a @detailmenu in the \"Top\" node's menu.\n#texinfo_no_detailmenu = False\n", "tests/test_face_recognition.py": "#!/usr/bin/env python\n# -*- coding: utf-8 -*-\n\n\"\"\"\ntest_face_recognition\n----------------------------------\n\nTests for `face_recognition` module.\n\"\"\"\n\n\nimport unittest\nimport os\nimport numpy as np\nfrom click.testing import CliRunner\n\nfrom face_recognition import api\nfrom face_recognition import face_recognition_cli\nfrom face_recognition import face_detection_cli\n\n\nclass Test_face_recognition(unittest.TestCase):\n\n    def test_load_image_file(self):\n        img = api.load_image_file(os.path.join(os.path.dirname(__file__), 'test_images', 'obama.jpg'))\n        self.assertEqual(img.shape, (1137, 910, 3))\n\n    def test_load_image_file_32bit(self):\n        img = api.load_image_file(os.path.join(os.path.dirname(__file__), 'test_images', '32bit.png'))\n        self.assertEqual(img.shape, (1200, 626, 3))\n\n    def test_raw_face_locations(self):\n        img = api.load_image_file(os.path.join(os.path.dirname(__file__), 'test_images', 'obama.jpg'))\n        detected_faces = api._raw_face_locations(img)\n\n        self.assertEqual(len(detected_faces), 1)\n        self.assertEqual(detected_faces[0].top(), 142)\n        self.assertEqual(detected_faces[0].bottom(), 409)\n\n    def test_cnn_raw_face_locations(self):\n        img = api.load_image_file(os.path.join(os.path.dirname(__file__), 'test_images', 'obama.jpg'))\n        detected_faces = api._raw_face_locations(img, model=\"cnn\")\n\n        self.assertEqual(len(detected_faces), 1)\n        self.assertAlmostEqual(detected_faces[0].rect.top(), 144, delta=25)\n        self.assertAlmostEqual(detected_faces[0].rect.bottom(), 389, delta=25)\n\n    def test_raw_face_locations_32bit_image(self):\n        img = api.load_image_file(os.path.join(os.path.dirname(__file__), 'test_images', '32bit.png'))\n        detected_faces = api._raw_face_locations(img)\n\n        self.assertEqual(len(detected_faces), 1)\n        self.assertEqual(detected_faces[0].top(), 290)\n        self.assertEqual(detected_faces[0].bottom(), 558)\n\n    def test_cnn_raw_face_locations_32bit_image(self):\n        img = api.load_image_file(os.path.join(os.path.dirname(__file__), 'test_images', '32bit.png'))\n        detected_faces = api._raw_face_locations(img, model=\"cnn\")\n\n        self.assertEqual(len(detected_faces), 1)\n        self.assertAlmostEqual(detected_faces[0].rect.top(), 259, delta=25)\n        self.assertAlmostEqual(detected_faces[0].rect.bottom(), 552, delta=25)\n\n    def test_face_locations(self):\n        img = api.load_image_file(os.path.join(os.path.dirname(__file__), 'test_images', 'obama.jpg'))\n        detected_faces = api.face_locations(img)\n\n        self.assertEqual(len(detected_faces), 1)\n        self.assertEqual(detected_faces[0], (142, 617, 409, 349))\n\n    def test_cnn_face_locations(self):\n        img = api.load_image_file(os.path.join(os.path.dirname(__file__), 'test_images', 'obama.jpg'))\n        detected_faces = api.face_locations(img, model=\"cnn\")\n\n        self.assertEqual(len(detected_faces), 1)\n        self.assertAlmostEqual(detected_faces[0][0], 144, delta=25)\n        self.assertAlmostEqual(detected_faces[0][1], 608, delta=25)\n        self.assertAlmostEqual(detected_faces[0][2], 389, delta=25)\n        self.assertAlmostEqual(detected_faces[0][3], 363, delta=25)\n\n    def test_partial_face_locations(self):\n        img = api.load_image_file(os.path.join(os.path.dirname(__file__), 'test_images', 'obama_partial_face.jpg'))\n        detected_faces = api.face_locations(img)\n\n        self.assertEqual(len(detected_faces), 1)\n        self.assertEqual(detected_faces[0], (142, 191, 365, 0))\n\n        img = api.load_image_file(os.path.join(os.path.dirname(__file__), 'test_images', 'obama_partial_face2.jpg'))\n        detected_faces = api.face_locations(img)\n\n        self.assertEqual(len(detected_faces), 1)\n        self.assertEqual(detected_faces[0], (142, 551, 409, 349))\n\n    def test_raw_face_locations_batched(self):\n        img = api.load_image_file(os.path.join(os.path.dirname(__file__), 'test_images', 'obama.jpg'))\n        images = [img, img, img]\n        batched_detected_faces = api._raw_face_locations_batched(images, number_of_times_to_upsample=0)\n\n        for detected_faces in batched_detected_faces:\n            self.assertEqual(len(detected_faces), 1)\n            self.assertEqual(detected_faces[0].rect.top(), 154)\n            self.assertEqual(detected_faces[0].rect.bottom(), 390)\n\n    def test_batched_face_locations(self):\n        img = api.load_image_file(os.path.join(os.path.dirname(__file__), 'test_images', 'obama.jpg'))\n        images = [img, img, img]\n\n        batched_detected_faces = api.batch_face_locations(images, number_of_times_to_upsample=0)\n\n        for detected_faces in batched_detected_faces:\n            self.assertEqual(len(detected_faces), 1)\n            self.assertEqual(detected_faces[0], (154, 611, 390, 375))\n\n    def test_raw_face_landmarks(self):\n        img = api.load_image_file(os.path.join(os.path.dirname(__file__), 'test_images', 'obama.jpg'))\n        face_landmarks = api._raw_face_landmarks(img)\n        example_landmark = face_landmarks[0].parts()[10]\n\n        self.assertEqual(len(face_landmarks), 1)\n        self.assertEqual(face_landmarks[0].num_parts, 68)\n        self.assertEqual((example_landmark.x, example_landmark.y), (552, 399))\n\n    def test_face_landmarks(self):\n        img = api.load_image_file(os.path.join(os.path.dirname(__file__), 'test_images', 'obama.jpg'))\n        face_landmarks = api.face_landmarks(img)\n\n        self.assertEqual(\n            set(face_landmarks[0].keys()),\n            set(['chin', 'left_eyebrow', 'right_eyebrow', 'nose_bridge',\n                 'nose_tip', 'left_eye', 'right_eye', 'top_lip',\n                 'bottom_lip']))\n        self.assertEqual(\n            face_landmarks[0]['chin'],\n            [(369, 220), (372, 254), (378, 289), (384, 322), (395, 353),\n             (414, 382), (437, 407), (464, 424), (495, 428), (527, 420),\n             (552, 399), (576, 372), (594, 344), (604, 314), (610, 282),\n             (613, 250), (615, 219)])\n\n    def test_face_landmarks_small_model(self):\n        img = api.load_image_file(os.path.join(os.path.dirname(__file__), 'test_images', 'obama.jpg'))\n        face_landmarks = api.face_landmarks(img, model=\"small\")\n\n        self.assertEqual(\n            set(face_landmarks[0].keys()),\n            set(['nose_tip', 'left_eye', 'right_eye']))\n        self.assertEqual(face_landmarks[0]['nose_tip'], [(496, 295)])\n\n    def test_face_encodings(self):\n        img = api.load_image_file(os.path.join(os.path.dirname(__file__), 'test_images', 'obama.jpg'))\n        encodings = api.face_encodings(img)\n\n        self.assertEqual(len(encodings), 1)\n        self.assertEqual(len(encodings[0]), 128)\n\n    def test_face_encodings_large_model(self):\n        img = api.load_image_file(os.path.join(os.path.dirname(__file__), 'test_images', 'obama.jpg'))\n        encodings = api.face_encodings(img, model='large')\n\n        self.assertEqual(len(encodings), 1)\n        self.assertEqual(len(encodings[0]), 128)\n\n    def test_face_distance(self):\n        img_a1 = api.load_image_file(os.path.join(os.path.dirname(__file__), 'test_images', 'obama.jpg'))\n        img_a2 = api.load_image_file(os.path.join(os.path.dirname(__file__), 'test_images', 'obama2.jpg'))\n        img_a3 = api.load_image_file(os.path.join(os.path.dirname(__file__), 'test_images', 'obama3.jpg'))\n\n        img_b1 = api.load_image_file(os.path.join(os.path.dirname(__file__), 'test_images', 'biden.jpg'))\n\n        face_encoding_a1 = api.face_encodings(img_a1)[0]\n        face_encoding_a2 = api.face_encodings(img_a2)[0]\n        face_encoding_a3 = api.face_encodings(img_a3)[0]\n        face_encoding_b1 = api.face_encodings(img_b1)[0]\n\n        faces_to_compare = [\n            face_encoding_a2,\n            face_encoding_a3,\n            face_encoding_b1]\n\n        distance_results = api.face_distance(faces_to_compare, face_encoding_a1)\n\n        # 0.6 is the default face distance match threshold. So we'll spot-check that the numbers returned\n        # are above or below that based on if they should match (since the exact numbers could vary).\n        self.assertEqual(type(distance_results), np.ndarray)\n        self.assertLessEqual(distance_results[0], 0.6)\n        self.assertLessEqual(distance_results[1], 0.6)\n        self.assertGreater(distance_results[2], 0.6)\n\n    def test_face_distance_empty_lists(self):\n        img = api.load_image_file(os.path.join(os.path.dirname(__file__), 'test_images', 'biden.jpg'))\n        face_encoding = api.face_encodings(img)[0]\n\n        # empty python list\n        faces_to_compare = []\n\n        distance_results = api.face_distance(faces_to_compare, face_encoding)\n        self.assertEqual(type(distance_results), np.ndarray)\n        self.assertEqual(len(distance_results), 0)\n\n        # empty numpy list\n        faces_to_compare = np.array([])\n\n        distance_results = api.face_distance(faces_to_compare, face_encoding)\n        self.assertEqual(type(distance_results), np.ndarray)\n        self.assertEqual(len(distance_results), 0)\n\n    def test_compare_faces(self):\n        img_a1 = api.load_image_file(os.path.join(os.path.dirname(__file__), 'test_images', 'obama.jpg'))\n        img_a2 = api.load_image_file(os.path.join(os.path.dirname(__file__), 'test_images', 'obama2.jpg'))\n        img_a3 = api.load_image_file(os.path.join(os.path.dirname(__file__), 'test_images', 'obama3.jpg'))\n\n        img_b1 = api.load_image_file(os.path.join(os.path.dirname(__file__), 'test_images', 'biden.jpg'))\n\n        face_encoding_a1 = api.face_encodings(img_a1)[0]\n        face_encoding_a2 = api.face_encodings(img_a2)[0]\n        face_encoding_a3 = api.face_encodings(img_a3)[0]\n        face_encoding_b1 = api.face_encodings(img_b1)[0]\n\n        faces_to_compare = [\n            face_encoding_a2,\n            face_encoding_a3,\n            face_encoding_b1]\n\n        match_results = api.compare_faces(faces_to_compare, face_encoding_a1)\n\n        self.assertEqual(type(match_results), list)\n        self.assertTrue(match_results[0])\n        self.assertTrue(match_results[1])\n        self.assertFalse(match_results[2])\n\n    def test_compare_faces_empty_lists(self):\n        img = api.load_image_file(os.path.join(os.path.dirname(__file__), 'test_images', 'biden.jpg'))\n        face_encoding = api.face_encodings(img)[0]\n\n        # empty python list\n        faces_to_compare = []\n\n        match_results = api.compare_faces(faces_to_compare, face_encoding)\n        self.assertEqual(type(match_results), list)\n        self.assertListEqual(match_results, [])\n\n        # empty numpy list\n        faces_to_compare = np.array([])\n\n        match_results = api.compare_faces(faces_to_compare, face_encoding)\n        self.assertEqual(type(match_results), list)\n        self.assertListEqual(match_results, [])\n\n    def test_command_line_interface_options(self):\n        target_string = 'Show this message and exit.'\n        runner = CliRunner()\n        help_result = runner.invoke(face_recognition_cli.main, ['--help'])\n        self.assertEqual(help_result.exit_code, 0)\n        self.assertTrue(target_string in help_result.output)\n\n    def test_command_line_interface(self):\n        target_string = 'obama.jpg,obama'\n        runner = CliRunner()\n        image_folder = os.path.join(os.path.dirname(__file__), 'test_images')\n        image_file = os.path.join(os.path.dirname(__file__), 'test_images', 'obama.jpg')\n\n        result = runner.invoke(face_recognition_cli.main, args=[image_folder, image_file])\n\n        self.assertEqual(result.exit_code, 0)\n        self.assertTrue(target_string in result.output)\n\n    def test_command_line_interface_big_image(self):\n        target_string = 'obama3.jpg,obama'\n        runner = CliRunner()\n        image_folder = os.path.join(os.path.dirname(__file__), 'test_images')\n        image_file = os.path.join(os.path.dirname(__file__), 'test_images', 'obama3.jpg')\n\n        result = runner.invoke(face_recognition_cli.main, args=[image_folder, image_file])\n\n        self.assertEqual(result.exit_code, 0)\n        self.assertTrue(target_string in result.output)\n\n    def test_command_line_interface_tolerance(self):\n        target_string = 'obama.jpg,obama'\n        runner = CliRunner()\n        image_folder = os.path.join(os.path.dirname(__file__), 'test_images')\n        image_file = os.path.join(os.path.dirname(__file__), 'test_images', 'obama.jpg')\n\n        result = runner.invoke(face_recognition_cli.main, args=[image_folder, image_file, \"--tolerance\", \"0.55\"])\n\n        self.assertEqual(result.exit_code, 0)\n        self.assertTrue(target_string in result.output)\n\n    def test_command_line_interface_show_distance(self):\n        target_string = 'obama.jpg,obama,0.0'\n        runner = CliRunner()\n        image_folder = os.path.join(os.path.dirname(__file__), 'test_images')\n        image_file = os.path.join(os.path.dirname(__file__), 'test_images', 'obama.jpg')\n\n        result = runner.invoke(face_recognition_cli.main, args=[image_folder, image_file, \"--show-distance\", \"1\"])\n\n        self.assertEqual(result.exit_code, 0)\n        self.assertTrue(target_string in result.output)\n\n    def test_fd_command_line_interface_options(self):\n        target_string = 'Show this message and exit.'\n        runner = CliRunner()\n        help_result = runner.invoke(face_detection_cli.main, ['--help'])\n        self.assertEqual(help_result.exit_code, 0)\n        self.assertTrue(target_string in help_result.output)\n\n    def test_fd_command_line_interface(self):\n        runner = CliRunner()\n        image_file = os.path.join(os.path.dirname(__file__), 'test_images', 'obama.jpg')\n\n        result = runner.invoke(face_detection_cli.main, args=[image_file])\n        self.assertEqual(result.exit_code, 0)\n        parts = result.output.split(\",\")\n        self.assertTrue(\"obama.jpg\" in parts[0])\n        self.assertEqual(len(parts), 5)\n\n    def test_fd_command_line_interface_folder(self):\n        runner = CliRunner()\n        image_file = os.path.join(os.path.dirname(__file__), 'test_images')\n\n        result = runner.invoke(face_detection_cli.main, args=[image_file])\n        self.assertEqual(result.exit_code, 0)\n        self.assertTrue(\"obama_partial_face2.jpg\" in result.output)\n        self.assertTrue(\"obama.jpg\" in result.output)\n        self.assertTrue(\"obama2.jpg\" in result.output)\n        self.assertTrue(\"obama3.jpg\" in result.output)\n        self.assertTrue(\"biden.jpg\" in result.output)\n\n    def test_fd_command_line_interface_hog_model(self):\n        target_string = 'obama.jpg'\n        runner = CliRunner()\n        image_file = os.path.join(os.path.dirname(__file__), 'test_images', 'obama.jpg')\n\n        result = runner.invoke(face_detection_cli.main, args=[image_file, \"--model\", \"hog\"])\n        self.assertEqual(result.exit_code, 0)\n        self.assertTrue(target_string in result.output)\n\n    def test_fd_command_line_interface_cnn_model(self):\n        target_string = 'obama.jpg'\n        runner = CliRunner()\n        image_file = os.path.join(os.path.dirname(__file__), 'test_images', 'obama.jpg')\n\n        result = runner.invoke(face_detection_cli.main, args=[image_file, \"--model\", \"cnn\"])\n        self.assertEqual(result.exit_code, 0)\n        self.assertTrue(target_string in result.output)\n", "tests/__init__.py": "# -*- coding: utf-8 -*-\n", "examples/find_faces_in_picture.py": "from PIL import Image\nimport face_recognition\n\n# Load the jpg file into a numpy array\nimage = face_recognition.load_image_file(\"biden.jpg\")\n\n# Find all the faces in the image using the default HOG-based model.\n# This method is fairly accurate, but not as accurate as the CNN model and not GPU accelerated.\n# See also: find_faces_in_picture_cnn.py\nface_locations = face_recognition.face_locations(image)\n\nprint(\"I found {} face(s) in this photograph.\".format(len(face_locations)))\n\nfor face_location in face_locations:\n\n    # Print the location of each face in this image\n    top, right, bottom, left = face_location\n    print(\"A face is located at pixel location Top: {}, Left: {}, Bottom: {}, Right: {}\".format(top, left, bottom, right))\n\n    # You can access the actual face itself like this:\n    face_image = image[top:bottom, left:right]\n    pil_image = Image.fromarray(face_image)\n    pil_image.show()\n", "examples/web_service_example_Simplified_Chinese.py": "# \u8fd9\u662f\u4e00\u4e2a\u975e\u5e38\u7b80\u5355\u7684\u4f7f\u7528Web\u670d\u52a1\u4e0a\u4f20\u56fe\u7247\u8fd0\u884c\u4eba\u8138\u8bc6\u522b\u7684\u6848\u4f8b\uff0c\u540e\u7aef\u670d\u52a1\u5668\u4f1a\u8bc6\u522b\u8fd9\u5f20\u56fe\u7247\u662f\u4e0d\u662f\u5965\u5df4\u9a6c\uff0c\u5e76\u628a\u8bc6\u522b\u7ed3\u679c\u4ee5json\u952e\u503c\u5bf9\u8f93\u51fa\n# \u6bd4\u5982\uff1a\u8fd0\u884c\u4ee5\u4e0b\u4ee3\u7801\n# $ curl -XPOST -F \"file=@obama2.jpg\" http://127.0.0.1:5001\n# \u4f1a\u8fd4\u56de\uff1a\n# {\n#  \"face_found_in_image\": true,\n#  \"is_picture_of_obama\": true\n# }\n#\n# \u672c\u9879\u76ee\u57fa\u4e8eFlask\u6846\u67b6\u7684\u6848\u4f8b http://flask.pocoo.org/docs/0.12/patterns/fileuploads/\n\n# \u63d0\u793a\uff1a\u8fd0\u884c\u672c\u6848\u4f8b\u9700\u8981\u5b89\u88c5Flask\uff0c\u4f60\u53ef\u4ee5\u7528\u4e0b\u9762\u7684\u4ee3\u7801\u5b89\u88c5Flask\n# $ pip3 install flask\n\nimport face_recognition\nfrom flask import Flask, jsonify, request, redirect\n\n# You can change this to any folder on your system\nALLOWED_EXTENSIONS = {'png', 'jpg', 'jpeg', 'gif'}\n\napp = Flask(__name__)\n\n\ndef allowed_file(filename):\n    return '.' in filename and \\\n           filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS\n\n\n@app.route('/', methods=['GET', 'POST'])\ndef upload_image():\n    # \u68c0\u6d4b\u56fe\u7247\u662f\u5426\u4e0a\u4f20\u6210\u529f\n    if request.method == 'POST':\n        if 'file' not in request.files:\n            return redirect(request.url)\n\n        file = request.files['file']\n\n        if file.filename == '':\n            return redirect(request.url)\n\n        if file and allowed_file(file.filename):\n            # \u56fe\u7247\u4e0a\u4f20\u6210\u529f\uff0c\u68c0\u6d4b\u56fe\u7247\u4e2d\u7684\u4eba\u8138\n            return detect_faces_in_image(file)\n\n    # \u56fe\u7247\u4e0a\u4f20\u5931\u8d25\uff0c\u8f93\u51fa\u4ee5\u4e0bhtml\u4ee3\u7801\n    return '''\n    <!doctype html>\n    <title>Is this a picture of Obama?</title>\n    <h1>Upload a picture and see if it's a picture of Obama!</h1>\n    <form method=\"POST\" enctype=\"multipart/form-data\">\n      <input type=\"file\" name=\"file\">\n      <input type=\"submit\" value=\"Upload\">\n    </form>\n    '''\n\n\ndef detect_faces_in_image(file_stream):\n    # \u7528face_recognition.face_encodings(img)\u63a5\u53e3\u63d0\u524d\u628a\u5965\u5df4\u9a6c\u4eba\u8138\u7684\u7f16\u7801\u5f55\u5165\n    known_face_encoding = [-0.09634063,  0.12095481, -0.00436332, -0.07643753,  0.0080383,\n                            0.01902981, -0.07184699, -0.09383309,  0.18518871, -0.09588896,\n                            0.23951106,  0.0986533 , -0.22114635, -0.1363683 ,  0.04405268,\n                            0.11574756, -0.19899382, -0.09597053, -0.11969153, -0.12277931,\n                            0.03416885, -0.00267565,  0.09203379,  0.04713435, -0.12731361,\n                           -0.35371891, -0.0503444 , -0.17841317, -0.00310897, -0.09844551,\n                           -0.06910533, -0.00503746, -0.18466514, -0.09851682,  0.02903969,\n                           -0.02174894,  0.02261871,  0.0032102 ,  0.20312519,  0.02999607,\n                           -0.11646006,  0.09432904,  0.02774341,  0.22102901,  0.26725179,\n                            0.06896867, -0.00490024, -0.09441824,  0.11115381, -0.22592428,\n                            0.06230862,  0.16559327,  0.06232892,  0.03458837,  0.09459756,\n                           -0.18777156,  0.00654241,  0.08582542, -0.13578284,  0.0150229 ,\n                            0.00670836, -0.08195844, -0.04346499,  0.03347827,  0.20310158,\n                            0.09987706, -0.12370517, -0.06683611,  0.12704916, -0.02160804,\n                            0.00984683,  0.00766284, -0.18980607, -0.19641446, -0.22800779,\n                            0.09010898,  0.39178532,  0.18818057, -0.20875394,  0.03097027,\n                           -0.21300618,  0.02532415,  0.07938635,  0.01000703, -0.07719778,\n                           -0.12651891, -0.04318593,  0.06219772,  0.09163868,  0.05039065,\n                           -0.04922386,  0.21839413, -0.02394437,  0.06173781,  0.0292527 ,\n                            0.06160797, -0.15553983, -0.02440624, -0.17509389, -0.0630486 ,\n                            0.01428208, -0.03637431,  0.03971229,  0.13983178, -0.23006812,\n                            0.04999552,  0.0108454 , -0.03970895,  0.02501768,  0.08157793,\n                           -0.03224047, -0.04502571,  0.0556995 , -0.24374914,  0.25514284,\n                            0.24795187,  0.04060191,  0.17597422,  0.07966681,  0.01920104,\n                           -0.01194376, -0.02300822, -0.17204897, -0.0596558 ,  0.05307484,\n                            0.07417042,  0.07126575,  0.00209804]\n\n    # \u8f7d\u5165\u7528\u6237\u4e0a\u4f20\u7684\u56fe\u7247\n    img = face_recognition.load_image_file(file_stream)\n    # \u4e3a\u7528\u6237\u4e0a\u4f20\u7684\u56fe\u7247\u4e2d\u7684\u4eba\u8138\u7f16\u7801\n    unknown_face_encodings = face_recognition.face_encodings(img)\n\n    face_found = False\n    is_obama = False\n\n    if len(unknown_face_encodings) > 0:\n        face_found = True\n        # \u770b\u770b\u56fe\u7247\u4e2d\u7684\u7b2c\u4e00\u5f20\u8138\u662f\u4e0d\u662f\u5965\u5df4\u9a6c\n        match_results = face_recognition.compare_faces([known_face_encoding], unknown_face_encodings[0])\n        if match_results[0]:\n            is_obama = True\n\n    # \u8bb2\u8bc6\u522b\u7ed3\u679c\u4ee5json\u952e\u503c\u5bf9\u7684\u6570\u636e\u7ed3\u6784\u8f93\u51fa\n    result = {\n        \"face_found_in_image\": face_found,\n        \"is_picture_of_obama\": is_obama\n    }\n    return jsonify(result)\n\n\nif __name__ == \"__main__\":\n    app.run(host='0.0.0.0', port=5001, debug=True)\n", "examples/facerec_from_webcam_faster.py": "import face_recognition\nimport cv2\nimport numpy as np\n\n# This is a demo of running face recognition on live video from your webcam. It's a little more complicated than the\n# other example, but it includes some basic performance tweaks to make things run a lot faster:\n#   1. Process each video frame at 1/4 resolution (though still display it at full resolution)\n#   2. Only detect faces in every other frame of video.\n\n# PLEASE NOTE: This example requires OpenCV (the `cv2` library) to be installed only to read from your webcam.\n# OpenCV is *not* required to use the face_recognition library. It's only required if you want to run this\n# specific demo. If you have trouble installing it, try any of the other demos that don't require it instead.\n\n# Get a reference to webcam #0 (the default one)\nvideo_capture = cv2.VideoCapture(0)\n\n# Load a sample picture and learn how to recognize it.\nobama_image = face_recognition.load_image_file(\"obama.jpg\")\nobama_face_encoding = face_recognition.face_encodings(obama_image)[0]\n\n# Load a second sample picture and learn how to recognize it.\nbiden_image = face_recognition.load_image_file(\"biden.jpg\")\nbiden_face_encoding = face_recognition.face_encodings(biden_image)[0]\n\n# Create arrays of known face encodings and their names\nknown_face_encodings = [\n    obama_face_encoding,\n    biden_face_encoding\n]\nknown_face_names = [\n    \"Barack Obama\",\n    \"Joe Biden\"\n]\n\n# Initialize some variables\nface_locations = []\nface_encodings = []\nface_names = []\nprocess_this_frame = True\n\nwhile True:\n    # Grab a single frame of video\n    ret, frame = video_capture.read()\n\n    # Only process every other frame of video to save time\n    if process_this_frame:\n        # Resize frame of video to 1/4 size for faster face recognition processing\n        small_frame = cv2.resize(frame, (0, 0), fx=0.25, fy=0.25)\n\n        # Convert the image from BGR color (which OpenCV uses) to RGB color (which face_recognition uses)\n        rgb_small_frame = small_frame[:, :, ::-1]\n        \n        # Find all the faces and face encodings in the current frame of video\n        face_locations = face_recognition.face_locations(rgb_small_frame)\n        face_encodings = face_recognition.face_encodings(rgb_small_frame, face_locations)\n\n        face_names = []\n        for face_encoding in face_encodings:\n            # See if the face is a match for the known face(s)\n            matches = face_recognition.compare_faces(known_face_encodings, face_encoding)\n            name = \"Unknown\"\n\n            # # If a match was found in known_face_encodings, just use the first one.\n            # if True in matches:\n            #     first_match_index = matches.index(True)\n            #     name = known_face_names[first_match_index]\n\n            # Or instead, use the known face with the smallest distance to the new face\n            face_distances = face_recognition.face_distance(known_face_encodings, face_encoding)\n            best_match_index = np.argmin(face_distances)\n            if matches[best_match_index]:\n                name = known_face_names[best_match_index]\n\n            face_names.append(name)\n\n    process_this_frame = not process_this_frame\n\n\n    # Display the results\n    for (top, right, bottom, left), name in zip(face_locations, face_names):\n        # Scale back up face locations since the frame we detected in was scaled to 1/4 size\n        top *= 4\n        right *= 4\n        bottom *= 4\n        left *= 4\n\n        # Draw a box around the face\n        cv2.rectangle(frame, (left, top), (right, bottom), (0, 0, 255), 2)\n\n        # Draw a label with a name below the face\n        cv2.rectangle(frame, (left, bottom - 35), (right, bottom), (0, 0, 255), cv2.FILLED)\n        font = cv2.FONT_HERSHEY_DUPLEX\n        cv2.putText(frame, name, (left + 6, bottom - 6), font, 1.0, (255, 255, 255), 1)\n\n    # Display the resulting image\n    cv2.imshow('Video', frame)\n\n    # Hit 'q' on the keyboard to quit!\n    if cv2.waitKey(1) & 0xFF == ord('q'):\n        break\n\n# Release handle to the webcam\nvideo_capture.release()\ncv2.destroyAllWindows()\n", "examples/face_recognition_knn.py": "\"\"\"\nThis is an example of using the k-nearest-neighbors (KNN) algorithm for face recognition.\n\nWhen should I use this example?\nThis example is useful when you wish to recognize a large set of known people,\nand make a prediction for an unknown person in a feasible computation time.\n\nAlgorithm Description:\nThe knn classifier is first trained on a set of labeled (known) faces and can then predict the person\nin an unknown image by finding the k most similar faces (images with closet face-features under euclidean distance)\nin its training set, and performing a majority vote (possibly weighted) on their label.\n\nFor example, if k=3, and the three closest face images to the given image in the training set are one image of Biden\nand two images of Obama, The result would be 'Obama'.\n\n* This implementation uses a weighted vote, such that the votes of closer-neighbors are weighted more heavily.\n\nUsage:\n\n1. Prepare a set of images of the known people you want to recognize. Organize the images in a single directory\n   with a sub-directory for each known person.\n\n2. Then, call the 'train' function with the appropriate parameters. Make sure to pass in the 'model_save_path' if you\n   want to save the model to disk so you can re-use the model without having to re-train it.\n\n3. Call 'predict' and pass in your trained model to recognize the people in an unknown image.\n\nNOTE: This example requires scikit-learn to be installed! You can install it with pip:\n\n$ pip3 install scikit-learn\n\n\"\"\"\n\nimport math\nfrom sklearn import neighbors\nimport os\nimport os.path\nimport pickle\nfrom PIL import Image, ImageDraw\nimport face_recognition\nfrom face_recognition.face_recognition_cli import image_files_in_folder\n\nALLOWED_EXTENSIONS = {'png', 'jpg', 'jpeg'}\n\n\ndef train(train_dir, model_save_path=None, n_neighbors=None, knn_algo='ball_tree', verbose=False):\n    \"\"\"\n    Trains a k-nearest neighbors classifier for face recognition.\n\n    :param train_dir: directory that contains a sub-directory for each known person, with its name.\n\n     (View in source code to see train_dir example tree structure)\n\n     Structure:\n        <train_dir>/\n        \u251c\u2500\u2500 <person1>/\n        \u2502   \u251c\u2500\u2500 <somename1>.jpeg\n        \u2502   \u251c\u2500\u2500 <somename2>.jpeg\n        \u2502   \u251c\u2500\u2500 ...\n        \u251c\u2500\u2500 <person2>/\n        \u2502   \u251c\u2500\u2500 <somename1>.jpeg\n        \u2502   \u2514\u2500\u2500 <somename2>.jpeg\n        \u2514\u2500\u2500 ...\n\n    :param model_save_path: (optional) path to save model on disk\n    :param n_neighbors: (optional) number of neighbors to weigh in classification. Chosen automatically if not specified\n    :param knn_algo: (optional) underlying data structure to support knn.default is ball_tree\n    :param verbose: verbosity of training\n    :return: returns knn classifier that was trained on the given data.\n    \"\"\"\n    X = []\n    y = []\n\n    # Loop through each person in the training set\n    for class_dir in os.listdir(train_dir):\n        if not os.path.isdir(os.path.join(train_dir, class_dir)):\n            continue\n\n        # Loop through each training image for the current person\n        for img_path in image_files_in_folder(os.path.join(train_dir, class_dir)):\n            image = face_recognition.load_image_file(img_path)\n            face_bounding_boxes = face_recognition.face_locations(image)\n\n            if len(face_bounding_boxes) != 1:\n                # If there are no people (or too many people) in a training image, skip the image.\n                if verbose:\n                    print(\"Image {} not suitable for training: {}\".format(img_path, \"Didn't find a face\" if len(face_bounding_boxes) < 1 else \"Found more than one face\"))\n            else:\n                # Add face encoding for current image to the training set\n                X.append(face_recognition.face_encodings(image, known_face_locations=face_bounding_boxes)[0])\n                y.append(class_dir)\n\n    # Determine how many neighbors to use for weighting in the KNN classifier\n    if n_neighbors is None:\n        n_neighbors = int(round(math.sqrt(len(X))))\n        if verbose:\n            print(\"Chose n_neighbors automatically:\", n_neighbors)\n\n    # Create and train the KNN classifier\n    knn_clf = neighbors.KNeighborsClassifier(n_neighbors=n_neighbors, algorithm=knn_algo, weights='distance')\n    knn_clf.fit(X, y)\n\n    # Save the trained KNN classifier\n    if model_save_path is not None:\n        with open(model_save_path, 'wb') as f:\n            pickle.dump(knn_clf, f)\n\n    return knn_clf\n\n\ndef predict(X_img_path, knn_clf=None, model_path=None, distance_threshold=0.6):\n    \"\"\"\n    Recognizes faces in given image using a trained KNN classifier\n\n    :param X_img_path: path to image to be recognized\n    :param knn_clf: (optional) a knn classifier object. if not specified, model_save_path must be specified.\n    :param model_path: (optional) path to a pickled knn classifier. if not specified, model_save_path must be knn_clf.\n    :param distance_threshold: (optional) distance threshold for face classification. the larger it is, the more chance\n           of mis-classifying an unknown person as a known one.\n    :return: a list of names and face locations for the recognized faces in the image: [(name, bounding box), ...].\n        For faces of unrecognized persons, the name 'unknown' will be returned.\n    \"\"\"\n    if not os.path.isfile(X_img_path) or os.path.splitext(X_img_path)[1][1:] not in ALLOWED_EXTENSIONS:\n        raise Exception(\"Invalid image path: {}\".format(X_img_path))\n\n    if knn_clf is None and model_path is None:\n        raise Exception(\"Must supply knn classifier either thourgh knn_clf or model_path\")\n\n    # Load a trained KNN model (if one was passed in)\n    if knn_clf is None:\n        with open(model_path, 'rb') as f:\n            knn_clf = pickle.load(f)\n\n    # Load image file and find face locations\n    X_img = face_recognition.load_image_file(X_img_path)\n    X_face_locations = face_recognition.face_locations(X_img)\n\n    # If no faces are found in the image, return an empty result.\n    if len(X_face_locations) == 0:\n        return []\n\n    # Find encodings for faces in the test iamge\n    faces_encodings = face_recognition.face_encodings(X_img, known_face_locations=X_face_locations)\n\n    # Use the KNN model to find the best matches for the test face\n    closest_distances = knn_clf.kneighbors(faces_encodings, n_neighbors=1)\n    are_matches = [closest_distances[0][i][0] <= distance_threshold for i in range(len(X_face_locations))]\n\n    # Predict classes and remove classifications that aren't within the threshold\n    return [(pred, loc) if rec else (\"unknown\", loc) for pred, loc, rec in zip(knn_clf.predict(faces_encodings), X_face_locations, are_matches)]\n\n\ndef show_prediction_labels_on_image(img_path, predictions):\n    \"\"\"\n    Shows the face recognition results visually.\n\n    :param img_path: path to image to be recognized\n    :param predictions: results of the predict function\n    :return:\n    \"\"\"\n    pil_image = Image.open(img_path).convert(\"RGB\")\n    draw = ImageDraw.Draw(pil_image)\n\n    for name, (top, right, bottom, left) in predictions:\n        # Draw a box around the face using the Pillow module\n        draw.rectangle(((left, top), (right, bottom)), outline=(0, 0, 255))\n\n        # There's a bug in Pillow where it blows up with non-UTF-8 text\n        # when using the default bitmap font\n        name = name.encode(\"UTF-8\")\n\n        # Draw a label with a name below the face\n        text_width, text_height = draw.textsize(name)\n        draw.rectangle(((left, bottom - text_height - 10), (right, bottom)), fill=(0, 0, 255), outline=(0, 0, 255))\n        draw.text((left + 6, bottom - text_height - 5), name, fill=(255, 255, 255, 255))\n\n    # Remove the drawing library from memory as per the Pillow docs\n    del draw\n\n    # Display the resulting image\n    pil_image.show()\n\n\nif __name__ == \"__main__\":\n    # STEP 1: Train the KNN classifier and save it to disk\n    # Once the model is trained and saved, you can skip this step next time.\n    print(\"Training KNN classifier...\")\n    classifier = train(\"knn_examples/train\", model_save_path=\"trained_knn_model.clf\", n_neighbors=2)\n    print(\"Training complete!\")\n\n    # STEP 2: Using the trained classifier, make predictions for unknown images\n    for image_file in os.listdir(\"knn_examples/test\"):\n        full_file_path = os.path.join(\"knn_examples/test\", image_file)\n\n        print(\"Looking for faces in {}\".format(image_file))\n\n        # Find all people in the image using a trained classifier model\n        # Note: You can pass in either a classifier file name or a classifier model instance\n        predictions = predict(full_file_path, model_path=\"trained_knn_model.clf\")\n\n        # Print results on the console\n        for name, (top, right, bottom, left) in predictions:\n            print(\"- Found {} at ({}, {})\".format(name, left, top))\n\n        # Display results overlaid on an image\n        show_prediction_labels_on_image(os.path.join(\"knn_examples/test\", image_file), predictions)\n", "examples/digital_makeup.py": "from PIL import Image, ImageDraw\nimport face_recognition\n\n# Load the jpg file into a numpy array\nimage = face_recognition.load_image_file(\"biden.jpg\")\n\n# Find all facial features in all the faces in the image\nface_landmarks_list = face_recognition.face_landmarks(image)\n\npil_image = Image.fromarray(image)\nfor face_landmarks in face_landmarks_list:\n    d = ImageDraw.Draw(pil_image, 'RGBA')\n\n    # Make the eyebrows into a nightmare\n    d.polygon(face_landmarks['left_eyebrow'], fill=(68, 54, 39, 128))\n    d.polygon(face_landmarks['right_eyebrow'], fill=(68, 54, 39, 128))\n    d.line(face_landmarks['left_eyebrow'], fill=(68, 54, 39, 150), width=5)\n    d.line(face_landmarks['right_eyebrow'], fill=(68, 54, 39, 150), width=5)\n\n    # Gloss the lips\n    d.polygon(face_landmarks['top_lip'], fill=(150, 0, 0, 128))\n    d.polygon(face_landmarks['bottom_lip'], fill=(150, 0, 0, 128))\n    d.line(face_landmarks['top_lip'], fill=(150, 0, 0, 64), width=8)\n    d.line(face_landmarks['bottom_lip'], fill=(150, 0, 0, 64), width=8)\n\n    # Sparkle the eyes\n    d.polygon(face_landmarks['left_eye'], fill=(255, 255, 255, 30))\n    d.polygon(face_landmarks['right_eye'], fill=(255, 255, 255, 30))\n\n    # Apply some eyeliner\n    d.line(face_landmarks['left_eye'] + [face_landmarks['left_eye'][0]], fill=(0, 0, 0, 110), width=6)\n    d.line(face_landmarks['right_eye'] + [face_landmarks['right_eye'][0]], fill=(0, 0, 0, 110), width=6)\n\n    pil_image.show()\n", "examples/facerec_from_video_file.py": "import face_recognition\nimport cv2\n\n# This is a demo of running face recognition on a video file and saving the results to a new video file.\n#\n# PLEASE NOTE: This example requires OpenCV (the `cv2` library) to be installed only to read from your webcam.\n# OpenCV is *not* required to use the face_recognition library. It's only required if you want to run this\n# specific demo. If you have trouble installing it, try any of the other demos that don't require it instead.\n\n# Open the input movie file\ninput_movie = cv2.VideoCapture(\"hamilton_clip.mp4\")\nlength = int(input_movie.get(cv2.CAP_PROP_FRAME_COUNT))\n\n# Create an output movie file (make sure resolution/frame rate matches input video!)\nfourcc = cv2.VideoWriter_fourcc(*'XVID')\noutput_movie = cv2.VideoWriter('output.avi', fourcc, 29.97, (640, 360))\n\n# Load some sample pictures and learn how to recognize them.\nlmm_image = face_recognition.load_image_file(\"lin-manuel-miranda.png\")\nlmm_face_encoding = face_recognition.face_encodings(lmm_image)[0]\n\nal_image = face_recognition.load_image_file(\"alex-lacamoire.png\")\nal_face_encoding = face_recognition.face_encodings(al_image)[0]\n\nknown_faces = [\n    lmm_face_encoding,\n    al_face_encoding\n]\n\n# Initialize some variables\nface_locations = []\nface_encodings = []\nface_names = []\nframe_number = 0\n\nwhile True:\n    # Grab a single frame of video\n    ret, frame = input_movie.read()\n    frame_number += 1\n\n    # Quit when the input video file ends\n    if not ret:\n        break\n\n    # Convert the image from BGR color (which OpenCV uses) to RGB color (which face_recognition uses)\n    rgb_frame = frame[:, :, ::-1]\n\n    # Find all the faces and face encodings in the current frame of video\n    face_locations = face_recognition.face_locations(rgb_frame)\n    face_encodings = face_recognition.face_encodings(rgb_frame, face_locations)\n\n    face_names = []\n    for face_encoding in face_encodings:\n        # See if the face is a match for the known face(s)\n        match = face_recognition.compare_faces(known_faces, face_encoding, tolerance=0.50)\n\n        # If you had more than 2 faces, you could make this logic a lot prettier\n        # but I kept it simple for the demo\n        name = None\n        if match[0]:\n            name = \"Lin-Manuel Miranda\"\n        elif match[1]:\n            name = \"Alex Lacamoire\"\n\n        face_names.append(name)\n\n    # Label the results\n    for (top, right, bottom, left), name in zip(face_locations, face_names):\n        if not name:\n            continue\n\n        # Draw a box around the face\n        cv2.rectangle(frame, (left, top), (right, bottom), (0, 0, 255), 2)\n\n        # Draw a label with a name below the face\n        cv2.rectangle(frame, (left, bottom - 25), (right, bottom), (0, 0, 255), cv2.FILLED)\n        font = cv2.FONT_HERSHEY_DUPLEX\n        cv2.putText(frame, name, (left + 6, bottom - 6), font, 0.5, (255, 255, 255), 1)\n\n    # Write the resulting image to the output video file\n    print(\"Writing frame {} / {}\".format(frame_number, length))\n    output_movie.write(frame)\n\n# All done!\ninput_movie.release()\ncv2.destroyAllWindows()\n", "examples/face_recognition_svm.py": "# Train multiple images per person\n# Find and recognize faces in an image using a SVC with scikit-learn\n\n\"\"\"\nStructure:\n        <test_image>.jpg\n        <train_dir>/\n            <person_1>/\n                <person_1_face-1>.jpg\n                <person_1_face-2>.jpg\n                .\n                .\n                <person_1_face-n>.jpg\n           <person_2>/\n                <person_2_face-1>.jpg\n                <person_2_face-2>.jpg\n                .\n                .\n                <person_2_face-n>.jpg\n            .\n            .\n            <person_n>/\n                <person_n_face-1>.jpg\n                <person_n_face-2>.jpg\n                .\n                .\n                <person_n_face-n>.jpg\n\"\"\"\n\nimport face_recognition\nfrom sklearn import svm\nimport os\n\n# Training the SVC classifier\n\n# The training data would be all the face encodings from all the known images and the labels are their names\nencodings = []\nnames = []\n\n# Training directory\ntrain_dir = os.listdir('/train_dir/')\n\n# Loop through each person in the training directory\nfor person in train_dir:\n    pix = os.listdir(\"/train_dir/\" + person)\n\n    # Loop through each training image for the current person\n    for person_img in pix:\n        # Get the face encodings for the face in each image file\n        face = face_recognition.load_image_file(\"/train_dir/\" + person + \"/\" + person_img)\n        face_bounding_boxes = face_recognition.face_locations(face)\n\n        #If training image contains exactly one face\n        if len(face_bounding_boxes) == 1:\n            face_enc = face_recognition.face_encodings(face)[0]\n            # Add face encoding for current image with corresponding label (name) to the training data\n            encodings.append(face_enc)\n            names.append(person)\n        else:\n            print(person + \"/\" + person_img + \" was skipped and can't be used for training\")\n\n# Create and train the SVC classifier\nclf = svm.SVC(gamma='scale')\nclf.fit(encodings,names)\n\n# Load the test image with unknown faces into a numpy array\ntest_image = face_recognition.load_image_file('test_image.jpg')\n\n# Find all the faces in the test image using the default HOG-based model\nface_locations = face_recognition.face_locations(test_image)\nno = len(face_locations)\nprint(\"Number of faces detected: \", no)\n\n# Predict all the faces in the test image using the trained classifier\nprint(\"Found:\")\nfor i in range(no):\n    test_image_enc = face_recognition.face_encodings(test_image)[i]\n    name = clf.predict([test_image_enc])\n    print(*name)\n", "examples/facerec_from_webcam_multiprocessing.py": "import face_recognition\nimport cv2\nfrom multiprocessing import Process, Manager, cpu_count, set_start_method\nimport time\nimport numpy\nimport threading\nimport platform\n\n\n# This is a little bit complicated (but fast) example of running face recognition on live video from your webcam.\n# This example is using multiprocess.\n\n# PLEASE NOTE: This example requires OpenCV (the `cv2` library) to be installed only to read from your webcam.\n# OpenCV is *not* required to use the face_recognition library. It's only required if you want to run this\n# specific demo. If you have trouble installing it, try any of the other demos that don't require it instead.\n\n\n# Get next worker's id\ndef next_id(current_id, worker_num):\n    if current_id == worker_num:\n        return 1\n    else:\n        return current_id + 1\n\n\n# Get previous worker's id\ndef prev_id(current_id, worker_num):\n    if current_id == 1:\n        return worker_num\n    else:\n        return current_id - 1\n\n\n# A subprocess use to capture frames.\ndef capture(read_frame_list, Global, worker_num):\n    # Get a reference to webcam #0 (the default one)\n    video_capture = cv2.VideoCapture(0)\n    # video_capture.set(3, 640)  # Width of the frames in the video stream.\n    # video_capture.set(4, 480)  # Height of the frames in the video stream.\n    # video_capture.set(5, 30) # Frame rate.\n    print(\"Width: %d, Height: %d, FPS: %d\" % (video_capture.get(3), video_capture.get(4), video_capture.get(5)))\n\n    while not Global.is_exit:\n        # If it's time to read a frame\n        if Global.buff_num != next_id(Global.read_num, worker_num):\n            # Grab a single frame of video\n            ret, frame = video_capture.read()\n            read_frame_list[Global.buff_num] = frame\n            Global.buff_num = next_id(Global.buff_num, worker_num)\n        else:\n            time.sleep(0.01)\n\n    # Release webcam\n    video_capture.release()\n\n\n# Many subprocess use to process frames.\ndef process(worker_id, read_frame_list, write_frame_list, Global, worker_num):\n    known_face_encodings = Global.known_face_encodings\n    known_face_names = Global.known_face_names\n    while not Global.is_exit:\n\n        # Wait to read\n        while Global.read_num != worker_id or Global.read_num != prev_id(Global.buff_num, worker_num):\n            # If the user has requested to end the app, then stop waiting for webcam frames\n            if Global.is_exit:\n                break\n\n            time.sleep(0.01)\n\n        # Delay to make the video look smoother\n        time.sleep(Global.frame_delay)\n\n        # Read a single frame from frame list\n        frame_process = read_frame_list[worker_id]\n\n        # Expect next worker to read frame\n        Global.read_num = next_id(Global.read_num, worker_num)\n\n        # Convert the image from BGR color (which OpenCV uses) to RGB color (which face_recognition uses)\n        rgb_frame = frame_process[:, :, ::-1]\n\n        # Find all the faces and face encodings in the frame of video, cost most time\n        face_locations = face_recognition.face_locations(rgb_frame)\n        face_encodings = face_recognition.face_encodings(rgb_frame, face_locations)\n\n        # Loop through each face in this frame of video\n        for (top, right, bottom, left), face_encoding in zip(face_locations, face_encodings):\n            # See if the face is a match for the known face(s)\n            matches = face_recognition.compare_faces(known_face_encodings, face_encoding)\n\n            name = \"Unknown\"\n\n            # If a match was found in known_face_encodings, just use the first one.\n            if True in matches:\n                first_match_index = matches.index(True)\n                name = known_face_names[first_match_index]\n\n            # Draw a box around the face\n            cv2.rectangle(frame_process, (left, top), (right, bottom), (0, 0, 255), 2)\n\n            # Draw a label with a name below the face\n            cv2.rectangle(frame_process, (left, bottom - 35), (right, bottom), (0, 0, 255), cv2.FILLED)\n            font = cv2.FONT_HERSHEY_DUPLEX\n            cv2.putText(frame_process, name, (left + 6, bottom - 6), font, 1.0, (255, 255, 255), 1)\n\n        # Wait to write\n        while Global.write_num != worker_id:\n            time.sleep(0.01)\n\n        # Send frame to global\n        write_frame_list[worker_id] = frame_process\n\n        # Expect next worker to write frame\n        Global.write_num = next_id(Global.write_num, worker_num)\n\n\nif __name__ == '__main__':\n\n    # Fix Bug on MacOS\n    if platform.system() == 'Darwin':\n        set_start_method('forkserver')\n\n    # Global variables\n    Global = Manager().Namespace()\n    Global.buff_num = 1\n    Global.read_num = 1\n    Global.write_num = 1\n    Global.frame_delay = 0\n    Global.is_exit = False\n    read_frame_list = Manager().dict()\n    write_frame_list = Manager().dict()\n\n    # Number of workers (subprocess use to process frames)\n    if cpu_count() > 2:\n        worker_num = cpu_count() - 1  # 1 for capturing frames\n    else:\n        worker_num = 2\n\n    # Subprocess list\n    p = []\n\n    # Create a thread to capture frames (if uses subprocess, it will crash on Mac)\n    p.append(threading.Thread(target=capture, args=(read_frame_list, Global, worker_num,)))\n    p[0].start()\n\n    # Load a sample picture and learn how to recognize it.\n    obama_image = face_recognition.load_image_file(\"obama.jpg\")\n    obama_face_encoding = face_recognition.face_encodings(obama_image)[0]\n\n    # Load a second sample picture and learn how to recognize it.\n    biden_image = face_recognition.load_image_file(\"biden.jpg\")\n    biden_face_encoding = face_recognition.face_encodings(biden_image)[0]\n\n    # Create arrays of known face encodings and their names\n    Global.known_face_encodings = [\n        obama_face_encoding,\n        biden_face_encoding\n    ]\n    Global.known_face_names = [\n        \"Barack Obama\",\n        \"Joe Biden\"\n    ]\n\n    # Create workers\n    for worker_id in range(1, worker_num + 1):\n        p.append(Process(target=process, args=(worker_id, read_frame_list, write_frame_list, Global, worker_num,)))\n        p[worker_id].start()\n\n    # Start to show video\n    last_num = 1\n    fps_list = []\n    tmp_time = time.time()\n    while not Global.is_exit:\n        while Global.write_num != last_num:\n            last_num = int(Global.write_num)\n\n            # Calculate fps\n            delay = time.time() - tmp_time\n            tmp_time = time.time()\n            fps_list.append(delay)\n            if len(fps_list) > 5 * worker_num:\n                fps_list.pop(0)\n            fps = len(fps_list) / numpy.sum(fps_list)\n            print(\"fps: %.2f\" % fps)\n\n            # Calculate frame delay, in order to make the video look smoother.\n            # When fps is higher, should use a smaller ratio, or fps will be limited in a lower value.\n            # Larger ratio can make the video look smoother, but fps will hard to become higher.\n            # Smaller ratio can make fps higher, but the video looks not too smoother.\n            # The ratios below are tested many times.\n            if fps < 6:\n                Global.frame_delay = (1 / fps) * 0.75\n            elif fps < 20:\n                Global.frame_delay = (1 / fps) * 0.5\n            elif fps < 30:\n                Global.frame_delay = (1 / fps) * 0.25\n            else:\n                Global.frame_delay = 0\n\n            # Display the resulting image\n            cv2.imshow('Video', write_frame_list[prev_id(Global.write_num, worker_num)])\n\n        # Hit 'q' on the keyboard to quit!\n        if cv2.waitKey(1) & 0xFF == ord('q'):\n            Global.is_exit = True\n            break\n\n        time.sleep(0.01)\n\n    # Quit\n    cv2.destroyAllWindows()\n", "examples/benchmark.py": "import timeit\n\n# Note: This example is only tested with Python 3 (not Python 2)\n\n# This is a very simple benchmark to give you an idea of how fast each step of face recognition will run on your system.\n# Notice that face detection gets very slow at large image sizes. So you might consider running face detection on a\n# scaled down version of your image and then running face encodings on the the full size image.\n\nTEST_IMAGES = [\n    \"obama-240p.jpg\",\n    \"obama-480p.jpg\",\n    \"obama-720p.jpg\",\n    \"obama-1080p.jpg\"\n]\n\n\ndef run_test(setup, test, iterations_per_test=5, tests_to_run=10):\n    fastest_execution = min(timeit.Timer(test, setup=setup).repeat(tests_to_run, iterations_per_test))\n    execution_time = fastest_execution / iterations_per_test\n    fps = 1.0 / execution_time\n    return execution_time, fps\n\n\nsetup_locate_faces = \"\"\"\nimport face_recognition\n\nimage = face_recognition.load_image_file(\"{}\")\n\"\"\"\n\ntest_locate_faces = \"\"\"\nface_locations = face_recognition.face_locations(image)\n\"\"\"\n\nsetup_face_landmarks = \"\"\"\nimport face_recognition\n\nimage = face_recognition.load_image_file(\"{}\")\nface_locations = face_recognition.face_locations(image)\n\"\"\"\n\ntest_face_landmarks = \"\"\"\nlandmarks = face_recognition.face_landmarks(image, face_locations=face_locations)[0]\n\"\"\"\n\nsetup_encode_face = \"\"\"\nimport face_recognition\n\nimage = face_recognition.load_image_file(\"{}\")\nface_locations = face_recognition.face_locations(image)\n\"\"\"\n\ntest_encode_face = \"\"\"\nencoding = face_recognition.face_encodings(image, known_face_locations=face_locations)[0]\n\"\"\"\n\nsetup_end_to_end = \"\"\"\nimport face_recognition\n\nimage = face_recognition.load_image_file(\"{}\")\n\"\"\"\n\ntest_end_to_end = \"\"\"\nencoding = face_recognition.face_encodings(image)[0]\n\"\"\"\n\nprint(\"Benchmarks (Note: All benchmarks are only using a single CPU core)\")\nprint()\n\nfor image in TEST_IMAGES:\n    size = image.split(\"-\")[1].split(\".\")[0]\n    print(\"Timings at {}:\".format(size))\n\n    print(\" - Face locations: {:.4f}s ({:.2f} fps)\".format(*run_test(setup_locate_faces.format(image), test_locate_faces)))\n    print(\" - Face landmarks: {:.4f}s ({:.2f} fps)\".format(*run_test(setup_face_landmarks.format(image), test_face_landmarks)))\n    print(\" - Encode face (inc. landmarks): {:.4f}s ({:.2f} fps)\".format(*run_test(setup_encode_face.format(image), test_encode_face)))\n    print(\" - End-to-end: {:.4f}s ({:.2f} fps)\".format(*run_test(setup_end_to_end.format(image), test_end_to_end)))\n    print()\n", "examples/facerec_from_webcam.py": "import face_recognition\nimport cv2\nimport numpy as np\n\n# This is a super simple (but slow) example of running face recognition on live video from your webcam.\n# There's a second example that's a little more complicated but runs faster.\n\n# PLEASE NOTE: This example requires OpenCV (the `cv2` library) to be installed only to read from your webcam.\n# OpenCV is *not* required to use the face_recognition library. It's only required if you want to run this\n# specific demo. If you have trouble installing it, try any of the other demos that don't require it instead.\n\n# Get a reference to webcam #0 (the default one)\nvideo_capture = cv2.VideoCapture(0)\n\n# Load a sample picture and learn how to recognize it.\nobama_image = face_recognition.load_image_file(\"obama.jpg\")\nobama_face_encoding = face_recognition.face_encodings(obama_image)[0]\n\n# Load a second sample picture and learn how to recognize it.\nbiden_image = face_recognition.load_image_file(\"biden.jpg\")\nbiden_face_encoding = face_recognition.face_encodings(biden_image)[0]\n\n# Create arrays of known face encodings and their names\nknown_face_encodings = [\n    obama_face_encoding,\n    biden_face_encoding\n]\nknown_face_names = [\n    \"Barack Obama\",\n    \"Joe Biden\"\n]\n\nwhile True:\n    # Grab a single frame of video\n    ret, frame = video_capture.read()\n\n    # Convert the image from BGR color (which OpenCV uses) to RGB color (which face_recognition uses)\n    rgb_frame = frame[:, :, ::-1]\n\n    # Find all the faces and face enqcodings in the frame of video\n    face_locations = face_recognition.face_locations(rgb_frame)\n    face_encodings = face_recognition.face_encodings(rgb_frame, face_locations)\n\n    # Loop through each face in this frame of video\n    for (top, right, bottom, left), face_encoding in zip(face_locations, face_encodings):\n        # See if the face is a match for the known face(s)\n        matches = face_recognition.compare_faces(known_face_encodings, face_encoding)\n\n        name = \"Unknown\"\n\n        # If a match was found in known_face_encodings, just use the first one.\n        # if True in matches:\n        #     first_match_index = matches.index(True)\n        #     name = known_face_names[first_match_index]\n\n        # Or instead, use the known face with the smallest distance to the new face\n        face_distances = face_recognition.face_distance(known_face_encodings, face_encoding)\n        best_match_index = np.argmin(face_distances)\n        if matches[best_match_index]:\n            name = known_face_names[best_match_index]\n\n        # Draw a box around the face\n        cv2.rectangle(frame, (left, top), (right, bottom), (0, 0, 255), 2)\n\n        # Draw a label with a name below the face\n        cv2.rectangle(frame, (left, bottom - 35), (right, bottom), (0, 0, 255), cv2.FILLED)\n        font = cv2.FONT_HERSHEY_DUPLEX\n        cv2.putText(frame, name, (left + 6, bottom - 6), font, 1.0, (255, 255, 255), 1)\n\n    # Display the resulting image\n    cv2.imshow('Video', frame)\n\n    # Hit 'q' on the keyboard to quit!\n    if cv2.waitKey(1) & 0xFF == ord('q'):\n        break\n\n# Release handle to the webcam\nvideo_capture.release()\ncv2.destroyAllWindows()\n", "examples/blink_detection.py": "#!/usr/bin/env python3\n\n\n# This is a demo of detecting eye status from the users camera. If the users eyes are closed for EYES_CLOSED seconds, the system will start printing out \"EYES CLOSED\"\n# to the terminal until the user presses and holds the spacebar to acknowledge\n\n# this demo must be run with sudo privileges for the keyboard module to work\n\n# PLEASE NOTE: This example requires OpenCV (the `cv2` library) to be installed only to read from your webcam.\n# OpenCV is *not* required to use the face_recognition library. It's only required if you want to run this\n# specific demo. If you have trouble installing it, try any of the other demos that don't require it instead.\n\n# imports\nimport face_recognition\nimport cv2\nimport time\nfrom scipy.spatial import distance as dist\n\nEYES_CLOSED_SECONDS = 5\n\ndef main():\n    closed_count = 0\n    video_capture = cv2.VideoCapture(0)\n\n    ret, frame = video_capture.read(0)\n    # cv2.VideoCapture.release()\n    small_frame = cv2.resize(frame, (0, 0), fx=0.25, fy=0.25)\n    rgb_small_frame = small_frame[:, :, ::-1]\n\n    face_landmarks_list = face_recognition.face_landmarks(rgb_small_frame)\n    process = True\n\n    while True:\n        ret, frame = video_capture.read(0)\n\n        # get it into the correct format\n        small_frame = cv2.resize(frame, (0, 0), fx=0.25, fy=0.25)\n        rgb_small_frame = small_frame[:, :, ::-1]\n\n\n\n        # get the correct face landmarks\n        \n        if process:\n            face_landmarks_list = face_recognition.face_landmarks(rgb_small_frame)\n\n            # get eyes\n            for face_landmark in face_landmarks_list:\n                left_eye = face_landmark['left_eye']\n                right_eye = face_landmark['right_eye']\n\n\n                color = (255,0,0)\n                thickness = 2\n\n                cv2.rectangle(small_frame, left_eye[0], right_eye[-1], color, thickness)\n\n                cv2.imshow('Video', small_frame)\n\n                ear_left = get_ear(left_eye)\n                ear_right = get_ear(right_eye)\n\n                closed = ear_left < 0.2 and ear_right < 0.2\n\n                if (closed):\n                    closed_count += 1\n\n                else:\n                    closed_count = 0\n\n                if (closed_count >= EYES_CLOSED_SECONDS):\n                    asleep = True\n                    while (asleep): #continue this loop until they wake up and acknowledge music\n                        print(\"EYES CLOSED\")\n\n                        if cv2.waitKey(1) == 32: #Wait for space key  \n                            asleep = False\n                            print(\"EYES OPENED\")\n                    closed_count = 0\n\n        process = not process\n        key = cv2.waitKey(1) & 0xFF\n        if key == ord(\"q\"):\n            break\n\ndef get_ear(eye):\n\n\t# compute the euclidean distances between the two sets of\n\t# vertical eye landmarks (x, y)-coordinates\n\tA = dist.euclidean(eye[1], eye[5])\n\tB = dist.euclidean(eye[2], eye[4])\n \n\t# compute the euclidean distance between the horizontal\n\t# eye landmark (x, y)-coordinates\n\tC = dist.euclidean(eye[0], eye[3])\n \n\t# compute the eye aspect ratio\n\tear = (A + B) / (2.0 * C)\n \n\t# return the eye aspect ratio\n\treturn ear\n\nif __name__ == \"__main__\":\n    main()\n\n", "examples/find_faces_in_picture_cnn.py": "from PIL import Image\nimport face_recognition\n\n# Load the jpg file into a numpy array\nimage = face_recognition.load_image_file(\"biden.jpg\")\n\n# Find all the faces in the image using a pre-trained convolutional neural network.\n# This method is more accurate than the default HOG model, but it's slower\n# unless you have an nvidia GPU and dlib compiled with CUDA extensions. But if you do,\n# this will use GPU acceleration and perform well.\n# See also: find_faces_in_picture.py\nface_locations = face_recognition.face_locations(image, number_of_times_to_upsample=0, model=\"cnn\")\n\nprint(\"I found {} face(s) in this photograph.\".format(len(face_locations)))\n\nfor face_location in face_locations:\n\n    # Print the location of each face in this image\n    top, right, bottom, left = face_location\n    print(\"A face is located at pixel location Top: {}, Left: {}, Bottom: {}, Right: {}\".format(top, left, bottom, right))\n\n    # You can access the actual face itself like this:\n    face_image = image[top:bottom, left:right]\n    pil_image = Image.fromarray(face_image)\n    pil_image.show()\n", "examples/web_service_example.py": "# This is a _very simple_ example of a web service that recognizes faces in uploaded images.\n# Upload an image file and it will check if the image contains a picture of Barack Obama.\n# The result is returned as json. For example:\n#\n# $ curl -XPOST -F \"file=@obama2.jpg\" http://127.0.0.1:5001\n#\n# Returns:\n#\n# {\n#  \"face_found_in_image\": true,\n#  \"is_picture_of_obama\": true\n# }\n#\n# This example is based on the Flask file upload example: http://flask.pocoo.org/docs/0.12/patterns/fileuploads/\n\n# NOTE: This example requires flask to be installed! You can install it with pip:\n# $ pip3 install flask\n\nimport face_recognition\nfrom flask import Flask, jsonify, request, redirect\n\n# You can change this to any folder on your system\nALLOWED_EXTENSIONS = {'png', 'jpg', 'jpeg', 'gif'}\n\napp = Flask(__name__)\n\n\ndef allowed_file(filename):\n    return '.' in filename and \\\n           filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS\n\n\n@app.route('/', methods=['GET', 'POST'])\ndef upload_image():\n    # Check if a valid image file was uploaded\n    if request.method == 'POST':\n        if 'file' not in request.files:\n            return redirect(request.url)\n\n        file = request.files['file']\n\n        if file.filename == '':\n            return redirect(request.url)\n\n        if file and allowed_file(file.filename):\n            # The image file seems valid! Detect faces and return the result.\n            return detect_faces_in_image(file)\n\n    # If no valid image file was uploaded, show the file upload form:\n    return '''\n    <!doctype html>\n    <title>Is this a picture of Obama?</title>\n    <h1>Upload a picture and see if it's a picture of Obama!</h1>\n    <form method=\"POST\" enctype=\"multipart/form-data\">\n      <input type=\"file\" name=\"file\">\n      <input type=\"submit\" value=\"Upload\">\n    </form>\n    '''\n\n\ndef detect_faces_in_image(file_stream):\n    # Pre-calculated face encoding of Obama generated with face_recognition.face_encodings(img)\n    known_face_encoding = [-0.09634063,  0.12095481, -0.00436332, -0.07643753,  0.0080383,\n                            0.01902981, -0.07184699, -0.09383309,  0.18518871, -0.09588896,\n                            0.23951106,  0.0986533 , -0.22114635, -0.1363683 ,  0.04405268,\n                            0.11574756, -0.19899382, -0.09597053, -0.11969153, -0.12277931,\n                            0.03416885, -0.00267565,  0.09203379,  0.04713435, -0.12731361,\n                           -0.35371891, -0.0503444 , -0.17841317, -0.00310897, -0.09844551,\n                           -0.06910533, -0.00503746, -0.18466514, -0.09851682,  0.02903969,\n                           -0.02174894,  0.02261871,  0.0032102 ,  0.20312519,  0.02999607,\n                           -0.11646006,  0.09432904,  0.02774341,  0.22102901,  0.26725179,\n                            0.06896867, -0.00490024, -0.09441824,  0.11115381, -0.22592428,\n                            0.06230862,  0.16559327,  0.06232892,  0.03458837,  0.09459756,\n                           -0.18777156,  0.00654241,  0.08582542, -0.13578284,  0.0150229 ,\n                            0.00670836, -0.08195844, -0.04346499,  0.03347827,  0.20310158,\n                            0.09987706, -0.12370517, -0.06683611,  0.12704916, -0.02160804,\n                            0.00984683,  0.00766284, -0.18980607, -0.19641446, -0.22800779,\n                            0.09010898,  0.39178532,  0.18818057, -0.20875394,  0.03097027,\n                           -0.21300618,  0.02532415,  0.07938635,  0.01000703, -0.07719778,\n                           -0.12651891, -0.04318593,  0.06219772,  0.09163868,  0.05039065,\n                           -0.04922386,  0.21839413, -0.02394437,  0.06173781,  0.0292527 ,\n                            0.06160797, -0.15553983, -0.02440624, -0.17509389, -0.0630486 ,\n                            0.01428208, -0.03637431,  0.03971229,  0.13983178, -0.23006812,\n                            0.04999552,  0.0108454 , -0.03970895,  0.02501768,  0.08157793,\n                           -0.03224047, -0.04502571,  0.0556995 , -0.24374914,  0.25514284,\n                            0.24795187,  0.04060191,  0.17597422,  0.07966681,  0.01920104,\n                           -0.01194376, -0.02300822, -0.17204897, -0.0596558 ,  0.05307484,\n                            0.07417042,  0.07126575,  0.00209804]\n\n    # Load the uploaded image file\n    img = face_recognition.load_image_file(file_stream)\n    # Get face encodings for any faces in the uploaded image\n    unknown_face_encodings = face_recognition.face_encodings(img)\n\n    face_found = False\n    is_obama = False\n\n    if len(unknown_face_encodings) > 0:\n        face_found = True\n        # See if the first face in the uploaded image matches the known face of Obama\n        match_results = face_recognition.compare_faces([known_face_encoding], unknown_face_encodings[0])\n        if match_results[0]:\n            is_obama = True\n\n    # Return the result as json\n    result = {\n        \"face_found_in_image\": face_found,\n        \"is_picture_of_obama\": is_obama\n    }\n    return jsonify(result)\n\nif __name__ == \"__main__\":\n    app.run(host='0.0.0.0', port=5001, debug=True)\n", "examples/facerec_on_raspberry_pi_Simplified_Chinese.py": "# \u8fd9\u662f\u4e00\u4e2a\u5728\u6811\u8393\u6d3e\u4e0a\u8fd0\u884c\u4eba\u8138\u8bc6\u522b\u7684\u6848\u4f8b\n# \u672c\u6848\u4f8b\u4f1a\u5728\u547d\u4ee4\u884c\u63a7\u5236\u9762\u677f\u4e0a\u8f93\u51fa\u8bc6\u522b\u51fa\u7684\u4eba\u8138\u6570\u91cf\u548c\u8eab\u4efd\u7ed3\u679c\u3002\n\n# \u4f60\u9700\u8981\u4e00\u4e2a2\u4ee3\u4ee5\u4e0a\u7684\u6811\u8393\u6d3e\uff0c\u5e76\u5728\u6811\u8393\u6d3e\u4e0a\u5b89\u88c5face_recognition\uff0c\u5e76\u8fde\u63a5\u4e0apicamera\u6444\u50cf\u5934\n# \u5e76\u786e\u4fddpicamera\u8fd9\u4e2a\u6a21\u5757\u5df2\u7ecf\u5b89\u88c5\uff08\u6811\u8393\u6d3e\u4e00\u822c\u4f1a\u5185\u7f6e\u5b89\u88c5\uff09\n# \u4f60\u53ef\u4ee5\u53c2\u8003\u8fd9\u4e2a\u6559\u7a0b\u914d\u5236\u4f60\u7684\u6811\u8393\u6d3e\uff1a\n# https://gist.github.com/ageitgey/1ac8dbe8572f3f533df6269dab35df65\n\nimport face_recognition\nimport picamera\nimport numpy as np\n\n# \u4f60\u9700\u8981\u5728sudo raspi-config\u4e2d\u628acamera\u529f\u80fd\u6253\u5f00\ncamera = picamera.PiCamera()\ncamera.resolution = (320, 240)\noutput = np.empty((240, 320, 3), dtype=np.uint8)\n\n# \u8f7d\u5165\u6837\u672c\u56fe\u7247\uff08\u5965\u5df4\u9a6c\u548c\u62dc\u767b\uff09\nprint(\"Loading known face image(s)\")\nobama_image = face_recognition.load_image_file(\"obama_small.jpg\")\nobama_face_encoding = face_recognition.face_encodings(obama_image)[0]\n\n# \u521d\u59cb\u5316\u53d8\u91cf\nface_locations = []\nface_encodings = []\n\nwhile True:\n    print(\"Capturing image.\")\n    # \u4ee5numpy array\u7684\u6570\u636e\u7ed3\u6784\u4ecepicamera\u6444\u50cf\u5934\u4e2d\u83b7\u53d6\u4e00\u5e27\u56fe\u7247\n    camera.capture(output, format=\"rgb\")\n\n    # \u83b7\u5f97\u6240\u6709\u4eba\u8138\u7684\u4f4d\u7f6e\u4ee5\u53ca\u5b83\u4eec\u7684\u7f16\u7801\n    face_locations = face_recognition.face_locations(output)\n    print(\"Found {} faces in image.\".format(len(face_locations)))\n    face_encodings = face_recognition.face_encodings(output, face_locations)\n\n    # \u5c06\u6bcf\u4e00\u4e2a\u4eba\u8138\u4e0e\u5df2\u77e5\u6837\u672c\u56fe\u7247\u6bd4\u5bf9\n    for face_encoding in face_encodings:\n        # \u770b\u662f\u5426\u5c5e\u4e8e\u5965\u5df4\u9a6c\u6216\u8005\u62dc\u767b\n        match = face_recognition.compare_faces([obama_face_encoding], face_encoding)\n        name = \"<Unknown Person>\"\n\n        if match[0]:\n            name = \"Barack Obama\"\n\n        print(\"I see someone named {}!\".format(name))\n", "examples/find_faces_in_batches.py": "import face_recognition\nimport cv2\n\n# This code finds all faces in a list of images using the CNN model.\n#\n# This demo is for the _special case_ when you need to find faces in LOTS of images very quickly and all the images\n# are the exact same size. This is common in video processing applications where you have lots of video frames\n# to process.\n#\n# If you are processing a lot of images and using a GPU with CUDA, batch processing can be ~3x faster then processing\n# single images at a time. But if you aren't using a GPU, then batch processing isn't going to be very helpful.\n#\n# PLEASE NOTE: This example requires OpenCV (the `cv2` library) to be installed only to read the video file.\n# OpenCV is *not* required to use the face_recognition library. It's only required if you want to run this\n# specific demo. If you have trouble installing it, try any of the other demos that don't require it instead.\n\n# Open video file\nvideo_capture = cv2.VideoCapture(\"short_hamilton_clip.mp4\")\n\nframes = []\nframe_count = 0\n\nwhile video_capture.isOpened():\n    # Grab a single frame of video\n    ret, frame = video_capture.read()\n\n    # Bail out when the video file ends\n    if not ret:\n        break\n\n    # Convert the image from BGR color (which OpenCV uses) to RGB color (which face_recognition uses)\n    frame = frame[:, :, ::-1]\n\n    # Save each frame of the video to a list\n    frame_count += 1\n    frames.append(frame)\n\n    # Every 128 frames (the default batch size), batch process the list of frames to find faces\n    if len(frames) == 128:\n        batch_of_face_locations = face_recognition.batch_face_locations(frames, number_of_times_to_upsample=0)\n\n        # Now let's list all the faces we found in all 128 frames\n        for frame_number_in_batch, face_locations in enumerate(batch_of_face_locations):\n            number_of_faces_in_frame = len(face_locations)\n\n            frame_number = frame_count - 128 + frame_number_in_batch\n            print(\"I found {} face(s) in frame #{}.\".format(number_of_faces_in_frame, frame_number))\n\n            for face_location in face_locations:\n                # Print the location of each face in this frame\n                top, right, bottom, left = face_location\n                print(\" - A face is located at pixel location Top: {}, Left: {}, Bottom: {}, Right: {}\".format(top, left, bottom, right))\n\n        # Clear the frames array to start the next batch\n        frames = []\n", "examples/identify_and_draw_boxes_on_faces.py": "import face_recognition\nfrom PIL import Image, ImageDraw\nimport numpy as np\n\n# This is an example of running face recognition on a single image\n# and drawing a box around each person that was identified.\n\n# Load a sample picture and learn how to recognize it.\nobama_image = face_recognition.load_image_file(\"obama.jpg\")\nobama_face_encoding = face_recognition.face_encodings(obama_image)[0]\n\n# Load a second sample picture and learn how to recognize it.\nbiden_image = face_recognition.load_image_file(\"biden.jpg\")\nbiden_face_encoding = face_recognition.face_encodings(biden_image)[0]\n\n# Create arrays of known face encodings and their names\nknown_face_encodings = [\n    obama_face_encoding,\n    biden_face_encoding\n]\nknown_face_names = [\n    \"Barack Obama\",\n    \"Joe Biden\"\n]\n\n# Load an image with an unknown face\nunknown_image = face_recognition.load_image_file(\"two_people.jpg\")\n\n# Find all the faces and face encodings in the unknown image\nface_locations = face_recognition.face_locations(unknown_image)\nface_encodings = face_recognition.face_encodings(unknown_image, face_locations)\n\n# Convert the image to a PIL-format image so that we can draw on top of it with the Pillow library\n# See http://pillow.readthedocs.io/ for more about PIL/Pillow\npil_image = Image.fromarray(unknown_image)\n# Create a Pillow ImageDraw Draw instance to draw with\ndraw = ImageDraw.Draw(pil_image)\n\n# Loop through each face found in the unknown image\nfor (top, right, bottom, left), face_encoding in zip(face_locations, face_encodings):\n    # See if the face is a match for the known face(s)\n    matches = face_recognition.compare_faces(known_face_encodings, face_encoding)\n\n    name = \"Unknown\"\n\n    # If a match was found in known_face_encodings, just use the first one.\n    # if True in matches:\n    #     first_match_index = matches.index(True)\n    #     name = known_face_names[first_match_index]\n\n    # Or instead, use the known face with the smallest distance to the new face\n    face_distances = face_recognition.face_distance(known_face_encodings, face_encoding)\n    best_match_index = np.argmin(face_distances)\n    if matches[best_match_index]:\n        name = known_face_names[best_match_index]\n\n    # Draw a box around the face using the Pillow module\n    draw.rectangle(((left, top), (right, bottom)), outline=(0, 0, 255))\n\n    # Draw a label with a name below the face\n    text_width, text_height = draw.textsize(name)\n    draw.rectangle(((left, bottom - text_height - 10), (right, bottom)), fill=(0, 0, 255), outline=(0, 0, 255))\n    draw.text((left + 6, bottom - text_height - 5), name, fill=(255, 255, 255, 255))\n\n\n# Remove the drawing library from memory as per the Pillow docs\ndel draw\n\n# Display the resulting image\npil_image.show()\n\n# You can also save a copy of the new image to disk if you want by uncommenting this line\n# pil_image.save(\"image_with_boxes.jpg\")\n", "examples/recognize_faces_in_pictures.py": "import face_recognition\n\n# Load the jpg files into numpy arrays\nbiden_image = face_recognition.load_image_file(\"biden.jpg\")\nobama_image = face_recognition.load_image_file(\"obama.jpg\")\nunknown_image = face_recognition.load_image_file(\"obama2.jpg\")\n\n# Get the face encodings for each face in each image file\n# Since there could be more than one face in each image, it returns a list of encodings.\n# But since I know each image only has one face, I only care about the first encoding in each image, so I grab index 0.\ntry:\n    biden_face_encoding = face_recognition.face_encodings(biden_image)[0]\n    obama_face_encoding = face_recognition.face_encodings(obama_image)[0]\n    unknown_face_encoding = face_recognition.face_encodings(unknown_image)[0]\nexcept IndexError:\n    print(\"I wasn't able to locate any faces in at least one of the images. Check the image files. Aborting...\")\n    quit()\n\nknown_faces = [\n    biden_face_encoding,\n    obama_face_encoding\n]\n\n# results is an array of True/False telling if the unknown face matched anyone in the known_faces array\nresults = face_recognition.compare_faces(known_faces, unknown_face_encoding)\n\nprint(\"Is the unknown face a picture of Biden? {}\".format(results[0]))\nprint(\"Is the unknown face a picture of Obama? {}\".format(results[1]))\nprint(\"Is the unknown face a new person that we've never seen before? {}\".format(not True in results))\n", "examples/facerec_ipcamera_knn.py": "\"\"\"\nThis is an example of using the k-nearest-neighbors (KNN) algorithm for face recognition.\n\nWhen should I use this example?\nThis example is useful when you wish to recognize a large set of known people,\nand make a prediction for an unknown person in a feasible computation time.\n\nAlgorithm Description:\nThe knn classifier is first trained on a set of labeled (known) faces and can then predict the person\nin a live stream by finding the k most similar faces (images with closet face-features under euclidean distance)\nin its training set, and performing a majority vote (possibly weighted) on their label.\n\nFor example, if k=3, and the three closest face images to the given image in the training set are one image of Biden\nand two images of Obama, The result would be 'Obama'.\n\n* This implementation uses a weighted vote, such that the votes of closer-neighbors are weighted more heavily.\n\nUsage:\n\n1. Prepare a set of images of the known people you want to recognize. Organize the images in a single directory\n   with a sub-directory for each known person.\n\n2. Then, call the 'train' function with the appropriate parameters. Make sure to pass in the 'model_save_path' if you\n   want to save the model to disk so you can re-use the model without having to re-train it.\n\n3. Call 'predict' and pass in your trained model to recognize the people in a live video stream.\n\nNOTE: This example requires scikit-learn, opencv and numpy to be installed! You can install it with pip:\n\n$ pip3 install scikit-learn\n$ pip3 install numpy\n$ pip3 install opencv-contrib-python\n\n\"\"\"\n\nimport cv2\nimport math\nfrom sklearn import neighbors\nimport os\nimport os.path\nimport pickle\nfrom PIL import Image, ImageDraw\nimport face_recognition\nfrom face_recognition.face_recognition_cli import image_files_in_folder\nimport numpy as np\n\n\nALLOWED_EXTENSIONS = {'png', 'jpg', 'jpeg', 'JPG'}\n\n\ndef train(train_dir, model_save_path=None, n_neighbors=None, knn_algo='ball_tree', verbose=False):\n    \"\"\"\n    Trains a k-nearest neighbors classifier for face recognition.\n\n    :param train_dir: directory that contains a sub-directory for each known person, with its name.\n\n     (View in source code to see train_dir example tree structure)\n\n     Structure:\n        <train_dir>/\n        \u251c\u2500\u2500 <person1>/\n        \u2502   \u251c\u2500\u2500 <somename1>.jpeg\n        \u2502   \u251c\u2500\u2500 <somename2>.jpeg\n        \u2502   \u251c\u2500\u2500 ...\n        \u251c\u2500\u2500 <person2>/\n        \u2502   \u251c\u2500\u2500 <somename1>.jpeg\n        \u2502   \u2514\u2500\u2500 <somename2>.jpeg\n        \u2514\u2500\u2500 ...\n\n    :param model_save_path: (optional) path to save model on disk\n    :param n_neighbors: (optional) number of neighbors to weigh in classification. Chosen automatically if not specified\n    :param knn_algo: (optional) underlying data structure to support knn.default is ball_tree\n    :param verbose: verbosity of training\n    :return: returns knn classifier that was trained on the given data.\n    \"\"\"\n    X = []\n    y = []\n\n    # Loop through each person in the training set\n    for class_dir in os.listdir(train_dir):\n        if not os.path.isdir(os.path.join(train_dir, class_dir)):\n            continue\n\n        # Loop through each training image for the current person\n        for img_path in image_files_in_folder(os.path.join(train_dir, class_dir)):\n            image = face_recognition.load_image_file(img_path)\n            face_bounding_boxes = face_recognition.face_locations(image)\n\n            if len(face_bounding_boxes) != 1:\n                # If there are no people (or too many people) in a training image, skip the image.\n                if verbose:\n                    print(\"Image {} not suitable for training: {}\".format(img_path, \"Didn't find a face\" if len(face_bounding_boxes) < 1 else \"Found more than one face\"))\n            else:\n                # Add face encoding for current image to the training set\n                X.append(face_recognition.face_encodings(image, known_face_locations=face_bounding_boxes)[0])\n                y.append(class_dir)\n\n    # Determine how many neighbors to use for weighting in the KNN classifier\n    if n_neighbors is None:\n        n_neighbors = int(round(math.sqrt(len(X))))\n        if verbose:\n            print(\"Chose n_neighbors automatically:\", n_neighbors)\n\n    # Create and train the KNN classifier\n    knn_clf = neighbors.KNeighborsClassifier(n_neighbors=n_neighbors, algorithm=knn_algo, weights='distance')\n    knn_clf.fit(X, y)\n\n    # Save the trained KNN classifier\n    if model_save_path is not None:\n        with open(model_save_path, 'wb') as f:\n            pickle.dump(knn_clf, f)\n\n    return knn_clf\n\n\ndef predict(X_frame, knn_clf=None, model_path=None, distance_threshold=0.5):\n    \"\"\"\n    Recognizes faces in given image using a trained KNN classifier\n\n    :param X_frame: frame to do the prediction on.\n    :param knn_clf: (optional) a knn classifier object. if not specified, model_save_path must be specified.\n    :param model_path: (optional) path to a pickled knn classifier. if not specified, model_save_path must be knn_clf.\n    :param distance_threshold: (optional) distance threshold for face classification. the larger it is, the more chance\n           of mis-classifying an unknown person as a known one.\n    :return: a list of names and face locations for the recognized faces in the image: [(name, bounding box), ...].\n        For faces of unrecognized persons, the name 'unknown' will be returned.\n    \"\"\"\n    if knn_clf is None and model_path is None:\n        raise Exception(\"Must supply knn classifier either thourgh knn_clf or model_path\")\n\n    # Load a trained KNN model (if one was passed in)\n    if knn_clf is None:\n        with open(model_path, 'rb') as f:\n            knn_clf = pickle.load(f)\n\n    X_face_locations = face_recognition.face_locations(X_frame)\n\n    # If no faces are found in the image, return an empty result.\n    if len(X_face_locations) == 0:\n        return []\n\n    # Find encodings for faces in the test image\n    faces_encodings = face_recognition.face_encodings(X_frame, known_face_locations=X_face_locations)\n\n    # Use the KNN model to find the best matches for the test face\n    closest_distances = knn_clf.kneighbors(faces_encodings, n_neighbors=1)\n    are_matches = [closest_distances[0][i][0] <= distance_threshold for i in range(len(X_face_locations))]\n\n    # Predict classes and remove classifications that aren't within the threshold\n    return [(pred, loc) if rec else (\"unknown\", loc) for pred, loc, rec in zip(knn_clf.predict(faces_encodings), X_face_locations, are_matches)]\n\n\ndef show_prediction_labels_on_image(frame, predictions):\n    \"\"\"\n    Shows the face recognition results visually.\n\n    :param frame: frame to show the predictions on\n    :param predictions: results of the predict function\n    :return opencv suited image to be fitting with cv2.imshow fucntion:\n    \"\"\"\n    pil_image = Image.fromarray(frame)\n    draw = ImageDraw.Draw(pil_image)\n\n    for name, (top, right, bottom, left) in predictions:\n        # enlarge the predictions for the full sized image.\n        top *= 2\n        right *= 2\n        bottom *= 2\n        left *= 2\n        # Draw a box around the face using the Pillow module\n        draw.rectangle(((left, top), (right, bottom)), outline=(0, 0, 255))\n\n        # There's a bug in Pillow where it blows up with non-UTF-8 text\n        # when using the default bitmap font\n        name = name.encode(\"UTF-8\")\n\n        # Draw a label with a name below the face\n        text_width, text_height = draw.textsize(name)\n        draw.rectangle(((left, bottom - text_height - 10), (right, bottom)), fill=(0, 0, 255), outline=(0, 0, 255))\n        draw.text((left + 6, bottom - text_height - 5), name, fill=(255, 255, 255, 255))\n\n    # Remove the drawing library from memory as per the Pillow docs.\n    del draw\n    # Save image in open-cv format to be able to show it.\n\n    opencvimage = np.array(pil_image)\n    return opencvimage\n\n\nif __name__ == \"__main__\":\n    print(\"Training KNN classifier...\")\n    classifier = train(\"knn_examples/train\", model_save_path=\"trained_knn_model.clf\", n_neighbors=2)\n    print(\"Training complete!\")\n    # process one frame in every 30 frames for speed\n    process_this_frame = 29\n    print('Setting cameras up...')\n    # multiple cameras can be used with the format url = 'http://username:password@camera_ip:port'\n    url = 'http://admin:admin@192.168.0.106:8081/'\n    cap = cv2.VideoCapture(url)\n    while 1 > 0:\n        ret, frame = cap.read()\n        if ret:\n            # Different resizing options can be chosen based on desired program runtime.\n            # Image resizing for more stable streaming\n            img = cv2.resize(frame, (0, 0), fx=0.5, fy=0.5)\n            process_this_frame = process_this_frame + 1\n            if process_this_frame % 30 == 0:\n                predictions = predict(img, model_path=\"trained_knn_model.clf\")\n            frame = show_prediction_labels_on_image(frame, predictions)\n            cv2.imshow('camera', frame)\n            if ord('q') == cv2.waitKey(10):\n                cap.release()\n                cv2.destroyAllWindows()\n                exit(0)\n", "examples/facerec_on_raspberry_pi.py": "# This is a demo of running face recognition on a Raspberry Pi.\n# This program will print out the names of anyone it recognizes to the console.\n\n# To run this, you need a Raspberry Pi 2 (or greater) with face_recognition and\n# the picamera[array] module installed.\n# You can follow this installation instructions to get your RPi set up:\n# https://gist.github.com/ageitgey/1ac8dbe8572f3f533df6269dab35df65\n\nimport face_recognition\nimport picamera\nimport numpy as np\n\n# Get a reference to the Raspberry Pi camera.\n# If this fails, make sure you have a camera connected to the RPi and that you\n# enabled your camera in raspi-config and rebooted first.\ncamera = picamera.PiCamera()\ncamera.resolution = (320, 240)\noutput = np.empty((240, 320, 3), dtype=np.uint8)\n\n# Load a sample picture and learn how to recognize it.\nprint(\"Loading known face image(s)\")\nobama_image = face_recognition.load_image_file(\"obama_small.jpg\")\nobama_face_encoding = face_recognition.face_encodings(obama_image)[0]\n\n# Initialize some variables\nface_locations = []\nface_encodings = []\n\nwhile True:\n    print(\"Capturing image.\")\n    # Grab a single frame of video from the RPi camera as a numpy array\n    camera.capture(output, format=\"rgb\")\n\n    # Find all the faces and face encodings in the current frame of video\n    face_locations = face_recognition.face_locations(output)\n    print(\"Found {} faces in image.\".format(len(face_locations)))\n    face_encodings = face_recognition.face_encodings(output, face_locations)\n\n    # Loop over each face found in the frame to see if it's someone we know.\n    for face_encoding in face_encodings:\n        # See if the face is a match for the known face(s)\n        match = face_recognition.compare_faces([obama_face_encoding], face_encoding)\n        name = \"<Unknown Person>\"\n\n        if match[0]:\n            name = \"Barack Obama\"\n\n        print(\"I see someone named {}!\".format(name))\n", "examples/find_facial_features_in_picture.py": "from PIL import Image, ImageDraw\nimport face_recognition\n\n# Load the jpg file into a numpy array\nimage = face_recognition.load_image_file(\"two_people.jpg\")\n\n# Find all facial features in all the faces in the image\nface_landmarks_list = face_recognition.face_landmarks(image)\n\nprint(\"I found {} face(s) in this photograph.\".format(len(face_landmarks_list)))\n\n# Create a PIL imagedraw object so we can draw on the picture\npil_image = Image.fromarray(image)\nd = ImageDraw.Draw(pil_image)\n\nfor face_landmarks in face_landmarks_list:\n\n    # Print the location of each facial feature in this image\n    for facial_feature in face_landmarks.keys():\n        print(\"The {} in this face has the following points: {}\".format(facial_feature, face_landmarks[facial_feature]))\n\n    # Let's trace out each facial feature in the image with a line!\n    for facial_feature in face_landmarks.keys():\n        d.line(face_landmarks[facial_feature], width=5)\n\n# Show the picture\npil_image.show()\n", "examples/face_distance.py": "import face_recognition\n\n# Often instead of just checking if two faces match or not (True or False), it's helpful to see how similar they are.\n# You can do that by using the face_distance function.\n\n# The model was trained in a way that faces with a distance of 0.6 or less should be a match. But if you want to\n# be more strict, you can look for a smaller face distance. For example, using a 0.55 cutoff would reduce false\n# positive matches at the risk of more false negatives.\n\n# Note: This isn't exactly the same as a \"percent match\". The scale isn't linear. But you can assume that images with a\n# smaller distance are more similar to each other than ones with a larger distance.\n\n# Load some images to compare against\nknown_obama_image = face_recognition.load_image_file(\"obama.jpg\")\nknown_biden_image = face_recognition.load_image_file(\"biden.jpg\")\n\n# Get the face encodings for the known images\nobama_face_encoding = face_recognition.face_encodings(known_obama_image)[0]\nbiden_face_encoding = face_recognition.face_encodings(known_biden_image)[0]\n\nknown_encodings = [\n    obama_face_encoding,\n    biden_face_encoding\n]\n\n# Load a test image and get encondings for it\nimage_to_test = face_recognition.load_image_file(\"obama2.jpg\")\nimage_to_test_encoding = face_recognition.face_encodings(image_to_test)[0]\n\n# See how far apart the test image is from the known faces\nface_distances = face_recognition.face_distance(known_encodings, image_to_test_encoding)\n\nfor i, face_distance in enumerate(face_distances):\n    print(\"The test image has a distance of {:.2} from known image #{}\".format(face_distance, i))\n    print(\"- With a normal cutoff of 0.6, would the test image match the known image? {}\".format(face_distance < 0.6))\n    print(\"- With a very strict cutoff of 0.5, would the test image match the known image? {}\".format(face_distance < 0.5))\n    print()\n", "examples/blur_faces_on_webcam.py": "import face_recognition\nimport cv2\n\n# This is a demo of blurring faces in video.\n\n# PLEASE NOTE: This example requires OpenCV (the `cv2` library) to be installed only to read from your webcam.\n# OpenCV is *not* required to use the face_recognition library. It's only required if you want to run this\n# specific demo. If you have trouble installing it, try any of the other demos that don't require it instead.\n\n# Get a reference to webcam #0 (the default one)\nvideo_capture = cv2.VideoCapture(0)\n\n# Initialize some variables\nface_locations = []\n\nwhile True:\n    # Grab a single frame of video\n    ret, frame = video_capture.read()\n\n    # Resize frame of video to 1/4 size for faster face detection processing\n    small_frame = cv2.resize(frame, (0, 0), fx=0.25, fy=0.25)\n\n    # Find all the faces and face encodings in the current frame of video\n    face_locations = face_recognition.face_locations(small_frame, model=\"cnn\")\n\n    # Display the results\n    for top, right, bottom, left in face_locations:\n        # Scale back up face locations since the frame we detected in was scaled to 1/4 size\n        top *= 4\n        right *= 4\n        bottom *= 4\n        left *= 4\n\n        # Extract the region of the image that contains the face\n        face_image = frame[top:bottom, left:right]\n\n        # Blur the face image\n        face_image = cv2.GaussianBlur(face_image, (99, 99), 30)\n\n        # Put the blurred face region back into the frame image\n        frame[top:bottom, left:right] = face_image\n\n    # Display the resulting image\n    cv2.imshow('Video', frame)\n\n    # Hit 'q' on the keyboard to quit!\n    if cv2.waitKey(1) & 0xFF == ord('q'):\n        break\n\n# Release handle to the webcam\nvideo_capture.release()\ncv2.destroyAllWindows()\n"}