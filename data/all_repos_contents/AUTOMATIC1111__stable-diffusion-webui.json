{"launch.py": "from modules import launch_utils\n\nargs = launch_utils.args\npython = launch_utils.python\ngit = launch_utils.git\nindex_url = launch_utils.index_url\ndir_repos = launch_utils.dir_repos\n\ncommit_hash = launch_utils.commit_hash\ngit_tag = launch_utils.git_tag\n\nrun = launch_utils.run\nis_installed = launch_utils.is_installed\nrepo_dir = launch_utils.repo_dir\n\nrun_pip = launch_utils.run_pip\ncheck_run_python = launch_utils.check_run_python\ngit_clone = launch_utils.git_clone\ngit_pull_recursive = launch_utils.git_pull_recursive\nlist_extensions = launch_utils.list_extensions\nrun_extension_installer = launch_utils.run_extension_installer\nprepare_environment = launch_utils.prepare_environment\nconfigure_for_tests = launch_utils.configure_for_tests\nstart = launch_utils.start\n\n\ndef main():\n    if args.dump_sysinfo:\n        filename = launch_utils.dump_sysinfo()\n\n        print(f\"Sysinfo saved as {filename}. Exiting...\")\n\n        exit(0)\n\n    launch_utils.startup_timer.record(\"initial startup\")\n\n    with launch_utils.startup_timer.subcategory(\"prepare environment\"):\n        if not args.skip_prepare_environment:\n            prepare_environment()\n\n    if args.test_server:\n        configure_for_tests()\n\n    start()\n\n\nif __name__ == \"__main__\":\n    main()\n", "webui.py": "from __future__ import annotations\n\nimport os\nimport time\n\nfrom modules import timer\nfrom modules import initialize_util\nfrom modules import initialize\n\nstartup_timer = timer.startup_timer\nstartup_timer.record(\"launcher\")\n\ninitialize.imports()\n\ninitialize.check_versions()\n\n\ndef create_api(app):\n    from modules.api.api import Api\n    from modules.call_queue import queue_lock\n\n    api = Api(app, queue_lock)\n    return api\n\n\ndef api_only():\n    from fastapi import FastAPI\n    from modules.shared_cmd_options import cmd_opts\n\n    initialize.initialize()\n\n    app = FastAPI()\n    initialize_util.setup_middleware(app)\n    api = create_api(app)\n\n    from modules import script_callbacks\n    script_callbacks.before_ui_callback()\n    script_callbacks.app_started_callback(None, app)\n\n    print(f\"Startup time: {startup_timer.summary()}.\")\n    api.launch(\n        server_name=initialize_util.gradio_server_name(),\n        port=cmd_opts.port if cmd_opts.port else 7861,\n        root_path=f\"/{cmd_opts.subpath}\" if cmd_opts.subpath else \"\"\n    )\n\n\ndef webui():\n    from modules.shared_cmd_options import cmd_opts\n\n    launch_api = cmd_opts.api\n    initialize.initialize()\n\n    from modules import shared, ui_tempdir, script_callbacks, ui, progress, ui_extra_networks\n\n    while 1:\n        if shared.opts.clean_temp_dir_at_start:\n            ui_tempdir.cleanup_tmpdr()\n            startup_timer.record(\"cleanup temp dir\")\n\n        script_callbacks.before_ui_callback()\n        startup_timer.record(\"scripts before_ui_callback\")\n\n        shared.demo = ui.create_ui()\n        startup_timer.record(\"create ui\")\n\n        if not cmd_opts.no_gradio_queue:\n            shared.demo.queue(64)\n\n        gradio_auth_creds = list(initialize_util.get_gradio_auth_creds()) or None\n\n        auto_launch_browser = False\n        if os.getenv('SD_WEBUI_RESTARTING') != '1':\n            if shared.opts.auto_launch_browser == \"Remote\" or cmd_opts.autolaunch:\n                auto_launch_browser = True\n            elif shared.opts.auto_launch_browser == \"Local\":\n                auto_launch_browser = not cmd_opts.webui_is_non_local\n\n        app, local_url, share_url = shared.demo.launch(\n            share=cmd_opts.share,\n            server_name=initialize_util.gradio_server_name(),\n            server_port=cmd_opts.port,\n            ssl_keyfile=cmd_opts.tls_keyfile,\n            ssl_certfile=cmd_opts.tls_certfile,\n            ssl_verify=cmd_opts.disable_tls_verify,\n            debug=cmd_opts.gradio_debug,\n            auth=gradio_auth_creds,\n            inbrowser=auto_launch_browser,\n            prevent_thread_lock=True,\n            allowed_paths=cmd_opts.gradio_allowed_path,\n            app_kwargs={\n                \"docs_url\": \"/docs\",\n                \"redoc_url\": \"/redoc\",\n            },\n            root_path=f\"/{cmd_opts.subpath}\" if cmd_opts.subpath else \"\",\n        )\n\n        startup_timer.record(\"gradio launch\")\n\n        # gradio uses a very open CORS policy via app.user_middleware, which makes it possible for\n        # an attacker to trick the user into opening a malicious HTML page, which makes a request to the\n        # running web ui and do whatever the attacker wants, including installing an extension and\n        # running its code. We disable this here. Suggested by RyotaK.\n        app.user_middleware = [x for x in app.user_middleware if x.cls.__name__ != 'CORSMiddleware']\n\n        initialize_util.setup_middleware(app)\n\n        progress.setup_progress_api(app)\n        ui.setup_ui_api(app)\n\n        if launch_api:\n            create_api(app)\n\n        ui_extra_networks.add_pages_to_demo(app)\n\n        startup_timer.record(\"add APIs\")\n\n        with startup_timer.subcategory(\"app_started_callback\"):\n            script_callbacks.app_started_callback(shared.demo, app)\n\n        timer.startup_record = startup_timer.dump()\n        print(f\"Startup time: {startup_timer.summary()}.\")\n\n        try:\n            while True:\n                server_command = shared.state.wait_for_server_command(timeout=5)\n                if server_command:\n                    if server_command in (\"stop\", \"restart\"):\n                        break\n                    else:\n                        print(f\"Unknown server command: {server_command}\")\n        except KeyboardInterrupt:\n            print('Caught KeyboardInterrupt, stopping...')\n            server_command = \"stop\"\n\n        if server_command == \"stop\":\n            print(\"Stopping server...\")\n            # If we catch a keyboard interrupt, we want to stop the server and exit.\n            shared.demo.close()\n            break\n\n        # disable auto launch webui in browser for subsequent UI Reload\n        os.environ.setdefault('SD_WEBUI_RESTARTING', '1')\n\n        print('Restarting UI...')\n        shared.demo.close()\n        time.sleep(0.5)\n        startup_timer.reset()\n        script_callbacks.app_reload_callback()\n        startup_timer.record(\"app reload callback\")\n        script_callbacks.script_unloaded_callback()\n        startup_timer.record(\"scripts unloaded callback\")\n        initialize.initialize_rest(reload_script_modules=True)\n\n\nif __name__ == \"__main__\":\n    from modules.shared_cmd_options import cmd_opts\n\n    if cmd_opts.nowebui:\n        api_only()\n    else:\n        webui()\n", "extensions-builtin/postprocessing-for-training/scripts/postprocessing_caption.py": "from modules import scripts_postprocessing, ui_components, deepbooru, shared\nimport gradio as gr\n\n\nclass ScriptPostprocessingCeption(scripts_postprocessing.ScriptPostprocessing):\n    name = \"Caption\"\n    order = 4040\n\n    def ui(self):\n        with ui_components.InputAccordion(False, label=\"Caption\") as enable:\n            option = gr.CheckboxGroup(value=[\"Deepbooru\"], choices=[\"Deepbooru\", \"BLIP\"], show_label=False)\n\n        return {\n            \"enable\": enable,\n            \"option\": option,\n        }\n\n    def process(self, pp: scripts_postprocessing.PostprocessedImage, enable, option):\n        if not enable:\n            return\n\n        captions = [pp.caption]\n\n        if \"Deepbooru\" in option:\n            captions.append(deepbooru.model.tag(pp.image))\n\n        if \"BLIP\" in option:\n            captions.append(shared.interrogator.interrogate(pp.image.convert(\"RGB\")))\n\n        pp.caption = \", \".join([x for x in captions if x])\n", "extensions-builtin/postprocessing-for-training/scripts/postprocessing_autosized_crop.py": "from PIL import Image\n\nfrom modules import scripts_postprocessing, ui_components\nimport gradio as gr\n\n\ndef center_crop(image: Image, w: int, h: int):\n    iw, ih = image.size\n    if ih / h < iw / w:\n        sw = w * ih / h\n        box = (iw - sw) / 2, 0, iw - (iw - sw) / 2, ih\n    else:\n        sh = h * iw / w\n        box = 0, (ih - sh) / 2, iw, ih - (ih - sh) / 2\n    return image.resize((w, h), Image.Resampling.LANCZOS, box)\n\n\ndef multicrop_pic(image: Image, mindim, maxdim, minarea, maxarea, objective, threshold):\n    iw, ih = image.size\n    err = lambda w, h: 1 - (lambda x: x if x < 1 else 1 / x)(iw / ih / (w / h))\n    wh = max(((w, h) for w in range(mindim, maxdim + 1, 64) for h in range(mindim, maxdim + 1, 64)\n              if minarea <= w * h <= maxarea and err(w, h) <= threshold),\n             key=lambda wh: (wh[0] * wh[1], -err(*wh))[::1 if objective == 'Maximize area' else -1],\n             default=None\n             )\n    return wh and center_crop(image, *wh)\n\n\nclass ScriptPostprocessingAutosizedCrop(scripts_postprocessing.ScriptPostprocessing):\n    name = \"Auto-sized crop\"\n    order = 4020\n\n    def ui(self):\n        with ui_components.InputAccordion(False, label=\"Auto-sized crop\") as enable:\n            gr.Markdown('Each image is center-cropped with an automatically chosen width and height.')\n            with gr.Row():\n                mindim = gr.Slider(minimum=64, maximum=2048, step=8, label=\"Dimension lower bound\", value=384, elem_id=\"postprocess_multicrop_mindim\")\n                maxdim = gr.Slider(minimum=64, maximum=2048, step=8, label=\"Dimension upper bound\", value=768, elem_id=\"postprocess_multicrop_maxdim\")\n            with gr.Row():\n                minarea = gr.Slider(minimum=64 * 64, maximum=2048 * 2048, step=1, label=\"Area lower bound\", value=64 * 64, elem_id=\"postprocess_multicrop_minarea\")\n                maxarea = gr.Slider(minimum=64 * 64, maximum=2048 * 2048, step=1, label=\"Area upper bound\", value=640 * 640, elem_id=\"postprocess_multicrop_maxarea\")\n            with gr.Row():\n                objective = gr.Radio([\"Maximize area\", \"Minimize error\"], value=\"Maximize area\", label=\"Resizing objective\", elem_id=\"postprocess_multicrop_objective\")\n                threshold = gr.Slider(minimum=0, maximum=1, step=0.01, label=\"Error threshold\", value=0.1, elem_id=\"postprocess_multicrop_threshold\")\n\n        return {\n            \"enable\": enable,\n            \"mindim\": mindim,\n            \"maxdim\": maxdim,\n            \"minarea\": minarea,\n            \"maxarea\": maxarea,\n            \"objective\": objective,\n            \"threshold\": threshold,\n        }\n\n    def process(self, pp: scripts_postprocessing.PostprocessedImage, enable, mindim, maxdim, minarea, maxarea, objective, threshold):\n        if not enable:\n            return\n\n        cropped = multicrop_pic(pp.image, mindim, maxdim, minarea, maxarea, objective, threshold)\n        if cropped is not None:\n            pp.image = cropped\n        else:\n            print(f\"skipped {pp.image.width}x{pp.image.height} image (can't find suitable size within error threshold)\")\n", "extensions-builtin/postprocessing-for-training/scripts/postprocessing_focal_crop.py": "\nfrom modules import scripts_postprocessing, ui_components, errors\nimport gradio as gr\n\nfrom modules.textual_inversion import autocrop\n\n\nclass ScriptPostprocessingFocalCrop(scripts_postprocessing.ScriptPostprocessing):\n    name = \"Auto focal point crop\"\n    order = 4010\n\n    def ui(self):\n        with ui_components.InputAccordion(False, label=\"Auto focal point crop\") as enable:\n            face_weight = gr.Slider(label='Focal point face weight', value=0.9, minimum=0.0, maximum=1.0, step=0.05, elem_id=\"postprocess_focal_crop_face_weight\")\n            entropy_weight = gr.Slider(label='Focal point entropy weight', value=0.15, minimum=0.0, maximum=1.0, step=0.05, elem_id=\"postprocess_focal_crop_entropy_weight\")\n            edges_weight = gr.Slider(label='Focal point edges weight', value=0.5, minimum=0.0, maximum=1.0, step=0.05, elem_id=\"postprocess_focal_crop_edges_weight\")\n            debug = gr.Checkbox(label='Create debug image', elem_id=\"train_process_focal_crop_debug\")\n\n        return {\n            \"enable\": enable,\n            \"face_weight\": face_weight,\n            \"entropy_weight\": entropy_weight,\n            \"edges_weight\": edges_weight,\n            \"debug\": debug,\n        }\n\n    def process(self, pp: scripts_postprocessing.PostprocessedImage, enable, face_weight, entropy_weight, edges_weight, debug):\n        if not enable:\n            return\n\n        if not pp.shared.target_width or not pp.shared.target_height:\n            return\n\n        dnn_model_path = None\n        try:\n            dnn_model_path = autocrop.download_and_cache_models()\n        except Exception:\n            errors.report(\"Unable to load face detection model for auto crop selection. Falling back to lower quality haar method.\", exc_info=True)\n\n        autocrop_settings = autocrop.Settings(\n            crop_width=pp.shared.target_width,\n            crop_height=pp.shared.target_height,\n            face_points_weight=face_weight,\n            entropy_points_weight=entropy_weight,\n            corner_points_weight=edges_weight,\n            annotate_image=debug,\n            dnn_model_path=dnn_model_path,\n        )\n\n        result, *others = autocrop.crop_image(pp.image, autocrop_settings)\n\n        pp.image = result\n        pp.extra_images = [pp.create_copy(x, nametags=[\"focal-crop-debug\"], disable_processing=True) for x in others]\n\n", "extensions-builtin/postprocessing-for-training/scripts/postprocessing_split_oversized.py": "import math\n\nfrom modules import scripts_postprocessing, ui_components\nimport gradio as gr\n\n\ndef split_pic(image, inverse_xy, width, height, overlap_ratio):\n    if inverse_xy:\n        from_w, from_h = image.height, image.width\n        to_w, to_h = height, width\n    else:\n        from_w, from_h = image.width, image.height\n        to_w, to_h = width, height\n    h = from_h * to_w // from_w\n    if inverse_xy:\n        image = image.resize((h, to_w))\n    else:\n        image = image.resize((to_w, h))\n\n    split_count = math.ceil((h - to_h * overlap_ratio) / (to_h * (1.0 - overlap_ratio)))\n    y_step = (h - to_h) / (split_count - 1)\n    for i in range(split_count):\n        y = int(y_step * i)\n        if inverse_xy:\n            splitted = image.crop((y, 0, y + to_h, to_w))\n        else:\n            splitted = image.crop((0, y, to_w, y + to_h))\n        yield splitted\n\n\nclass ScriptPostprocessingSplitOversized(scripts_postprocessing.ScriptPostprocessing):\n    name = \"Split oversized images\"\n    order = 4000\n\n    def ui(self):\n        with ui_components.InputAccordion(False, label=\"Split oversized images\") as enable:\n            with gr.Row():\n                split_threshold = gr.Slider(label='Threshold', value=0.5, minimum=0.0, maximum=1.0, step=0.05, elem_id=\"postprocess_split_threshold\")\n                overlap_ratio = gr.Slider(label='Overlap ratio', value=0.2, minimum=0.0, maximum=0.9, step=0.05, elem_id=\"postprocess_overlap_ratio\")\n\n        return {\n            \"enable\": enable,\n            \"split_threshold\": split_threshold,\n            \"overlap_ratio\": overlap_ratio,\n        }\n\n    def process(self, pp: scripts_postprocessing.PostprocessedImage, enable, split_threshold, overlap_ratio):\n        if not enable:\n            return\n\n        width = pp.shared.target_width\n        height = pp.shared.target_height\n\n        if not width or not height:\n            return\n\n        if pp.image.height > pp.image.width:\n            ratio = (pp.image.width * height) / (pp.image.height * width)\n            inverse_xy = False\n        else:\n            ratio = (pp.image.height * width) / (pp.image.width * height)\n            inverse_xy = True\n\n        if ratio >= 1.0 or ratio > split_threshold:\n            return\n\n        result, *others = split_pic(pp.image, inverse_xy, width, height, overlap_ratio)\n\n        pp.image = result\n        pp.extra_images = [pp.create_copy(x) for x in others]\n\n", "extensions-builtin/postprocessing-for-training/scripts/postprocessing_create_flipped_copies.py": "from PIL import ImageOps, Image\n\nfrom modules import scripts_postprocessing, ui_components\nimport gradio as gr\n\n\nclass ScriptPostprocessingCreateFlippedCopies(scripts_postprocessing.ScriptPostprocessing):\n    name = \"Create flipped copies\"\n    order = 4030\n\n    def ui(self):\n        with ui_components.InputAccordion(False, label=\"Create flipped copies\") as enable:\n            with gr.Row():\n                option = gr.CheckboxGroup(value=[\"Horizontal\"], choices=[\"Horizontal\", \"Vertical\", \"Both\"], show_label=False)\n\n        return {\n            \"enable\": enable,\n            \"option\": option,\n        }\n\n    def process(self, pp: scripts_postprocessing.PostprocessedImage, enable, option):\n        if not enable:\n            return\n\n        if \"Horizontal\" in option:\n            pp.extra_images.append(ImageOps.mirror(pp.image))\n\n        if \"Vertical\" in option:\n            pp.extra_images.append(pp.image.transpose(Image.Transpose.FLIP_TOP_BOTTOM))\n\n        if \"Both\" in option:\n            pp.extra_images.append(pp.image.transpose(Image.Transpose.FLIP_TOP_BOTTOM).transpose(Image.Transpose.FLIP_LEFT_RIGHT))\n", "extensions-builtin/canvas-zoom-and-pan/scripts/hotkey_config.py": "import gradio as gr\nfrom modules import shared\n\nshared.options_templates.update(shared.options_section(('canvas_hotkey', \"Canvas Hotkeys\"), {\n    \"canvas_hotkey_zoom\": shared.OptionInfo(\"Alt\", \"Zoom canvas\", gr.Radio, {\"choices\": [\"Shift\",\"Ctrl\", \"Alt\"]}).info(\"If you choose 'Shift' you cannot scroll horizontally, 'Alt' can cause a little trouble in firefox\"),\n    \"canvas_hotkey_adjust\": shared.OptionInfo(\"Ctrl\", \"Adjust brush size\", gr.Radio, {\"choices\": [\"Shift\",\"Ctrl\", \"Alt\"]}).info(\"If you choose 'Shift' you cannot scroll horizontally, 'Alt' can cause a little trouble in firefox\"),\n    \"canvas_hotkey_shrink_brush\": shared.OptionInfo(\"Q\", \"Shrink the brush size\"),\n    \"canvas_hotkey_grow_brush\": shared.OptionInfo(\"W\", \"Enlarge the brush size\"),\n    \"canvas_hotkey_move\": shared.OptionInfo(\"F\", \"Moving the canvas\").info(\"To work correctly in firefox, turn off 'Automatically search the page text when typing' in the browser settings\"),\n    \"canvas_hotkey_fullscreen\": shared.OptionInfo(\"S\", \"Fullscreen Mode, maximizes the picture so that it fits into the screen and stretches it to its full width \"),\n    \"canvas_hotkey_reset\": shared.OptionInfo(\"R\", \"Reset zoom and canvas position\"),\n    \"canvas_hotkey_overlap\": shared.OptionInfo(\"O\", \"Toggle overlap\").info(\"Technical button, needed for testing\"),\n    \"canvas_show_tooltip\": shared.OptionInfo(True, \"Enable tooltip on the canvas\"),\n    \"canvas_auto_expand\": shared.OptionInfo(True, \"Automatically expands an image that does not fit completely in the canvas area, similar to manually pressing the S and R buttons\"),\n    \"canvas_blur_prompt\": shared.OptionInfo(False, \"Take the focus off the prompt when working with a canvas\"),\n    \"canvas_disabled_functions\": shared.OptionInfo([\"Overlap\"], \"Disable function that you don't use\", gr.CheckboxGroup, {\"choices\": [\"Zoom\",\"Adjust brush size\",\"Hotkey enlarge brush\",\"Hotkey shrink brush\",\"Moving canvas\",\"Fullscreen\",\"Reset Zoom\",\"Overlap\"]}),\n}))\n", "extensions-builtin/ScuNET/preload.py": "import os\nfrom modules import paths\n\n\ndef preload(parser):\n    parser.add_argument(\"--scunet-models-path\", type=str, help=\"Path to directory with ScuNET model file(s).\", default=os.path.join(paths.models_path, 'ScuNET'))\n", "extensions-builtin/ScuNET/scripts/scunet_model.py": "import sys\n\nimport PIL.Image\n\nimport modules.upscaler\nfrom modules import devices, errors, modelloader, script_callbacks, shared, upscaler_utils\n\n\nclass UpscalerScuNET(modules.upscaler.Upscaler):\n    def __init__(self, dirname):\n        self.name = \"ScuNET\"\n        self.model_name = \"ScuNET GAN\"\n        self.model_name2 = \"ScuNET PSNR\"\n        self.model_url = \"https://github.com/cszn/KAIR/releases/download/v1.0/scunet_color_real_gan.pth\"\n        self.model_url2 = \"https://github.com/cszn/KAIR/releases/download/v1.0/scunet_color_real_psnr.pth\"\n        self.user_path = dirname\n        super().__init__()\n        model_paths = self.find_models(ext_filter=[\".pth\"])\n        scalers = []\n        add_model2 = True\n        for file in model_paths:\n            if file.startswith(\"http\"):\n                name = self.model_name\n            else:\n                name = modelloader.friendly_name(file)\n            if name == self.model_name2 or file == self.model_url2:\n                add_model2 = False\n            try:\n                scaler_data = modules.upscaler.UpscalerData(name, file, self, 4)\n                scalers.append(scaler_data)\n            except Exception:\n                errors.report(f\"Error loading ScuNET model: {file}\", exc_info=True)\n        if add_model2:\n            scaler_data2 = modules.upscaler.UpscalerData(self.model_name2, self.model_url2, self)\n            scalers.append(scaler_data2)\n        self.scalers = scalers\n\n    def do_upscale(self, img: PIL.Image.Image, selected_file):\n        devices.torch_gc()\n        try:\n            model = self.load_model(selected_file)\n        except Exception as e:\n            print(f\"ScuNET: Unable to load model from {selected_file}: {e}\", file=sys.stderr)\n            return img\n\n        img = upscaler_utils.upscale_2(\n            img,\n            model,\n            tile_size=shared.opts.SCUNET_tile,\n            tile_overlap=shared.opts.SCUNET_tile_overlap,\n            scale=1,  # ScuNET is a denoising model, not an upscaler\n            desc='ScuNET',\n        )\n        devices.torch_gc()\n        return img\n\n    def load_model(self, path: str):\n        device = devices.get_device_for('scunet')\n        if path.startswith(\"http\"):\n            # TODO: this doesn't use `path` at all?\n            filename = modelloader.load_file_from_url(self.model_url, model_dir=self.model_download_path, file_name=f\"{self.name}.pth\")\n        else:\n            filename = path\n        return modelloader.load_spandrel_model(filename, device=device, expected_architecture='SCUNet')\n\n\ndef on_ui_settings():\n    import gradio as gr\n\n    shared.opts.add_option(\"SCUNET_tile\", shared.OptionInfo(256, \"Tile size for SCUNET upscalers.\", gr.Slider, {\"minimum\": 0, \"maximum\": 512, \"step\": 16}, section=('upscaling', \"Upscaling\")).info(\"0 = no tiling\"))\n    shared.opts.add_option(\"SCUNET_tile_overlap\", shared.OptionInfo(8, \"Tile overlap for SCUNET upscalers.\", gr.Slider, {\"minimum\": 0, \"maximum\": 64, \"step\": 1}, section=('upscaling', \"Upscaling\")).info(\"Low values = visible seam\"))\n\n\nscript_callbacks.on_ui_settings(on_ui_settings)\n", "extensions-builtin/hypertile/hypertile.py": "\"\"\"\nHypertile module for splitting attention layers in SD-1.5 U-Net and SD-1.5 VAE\nWarn: The patch works well only if the input image has a width and height that are multiples of 128\nOriginal author: @tfernd Github: https://github.com/tfernd/HyperTile\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom dataclasses import dataclass\nfrom typing import Callable\n\nfrom functools import wraps, cache\n\nimport math\nimport torch.nn as nn\nimport random\n\nfrom einops import rearrange\n\n\n@dataclass\nclass HypertileParams:\n    depth = 0\n    layer_name = \"\"\n    tile_size: int = 0\n    swap_size: int = 0\n    aspect_ratio: float = 1.0\n    forward = None\n    enabled = False\n\n\n\n# TODO add SD-XL layers\nDEPTH_LAYERS = {\n    0: [\n        # SD 1.5 U-Net (diffusers)\n        \"down_blocks.0.attentions.0.transformer_blocks.0.attn1\",\n        \"down_blocks.0.attentions.1.transformer_blocks.0.attn1\",\n        \"up_blocks.3.attentions.0.transformer_blocks.0.attn1\",\n        \"up_blocks.3.attentions.1.transformer_blocks.0.attn1\",\n        \"up_blocks.3.attentions.2.transformer_blocks.0.attn1\",\n        # SD 1.5 U-Net (ldm)\n        \"input_blocks.1.1.transformer_blocks.0.attn1\",\n        \"input_blocks.2.1.transformer_blocks.0.attn1\",\n        \"output_blocks.9.1.transformer_blocks.0.attn1\",\n        \"output_blocks.10.1.transformer_blocks.0.attn1\",\n        \"output_blocks.11.1.transformer_blocks.0.attn1\",\n        # SD 1.5 VAE\n        \"decoder.mid_block.attentions.0\",\n        \"decoder.mid.attn_1\",\n    ],\n    1: [\n        # SD 1.5 U-Net (diffusers)\n        \"down_blocks.1.attentions.0.transformer_blocks.0.attn1\",\n        \"down_blocks.1.attentions.1.transformer_blocks.0.attn1\",\n        \"up_blocks.2.attentions.0.transformer_blocks.0.attn1\",\n        \"up_blocks.2.attentions.1.transformer_blocks.0.attn1\",\n        \"up_blocks.2.attentions.2.transformer_blocks.0.attn1\",\n        # SD 1.5 U-Net (ldm)\n        \"input_blocks.4.1.transformer_blocks.0.attn1\",\n        \"input_blocks.5.1.transformer_blocks.0.attn1\",\n        \"output_blocks.6.1.transformer_blocks.0.attn1\",\n        \"output_blocks.7.1.transformer_blocks.0.attn1\",\n        \"output_blocks.8.1.transformer_blocks.0.attn1\",\n    ],\n    2: [\n        # SD 1.5 U-Net (diffusers)\n        \"down_blocks.2.attentions.0.transformer_blocks.0.attn1\",\n        \"down_blocks.2.attentions.1.transformer_blocks.0.attn1\",\n        \"up_blocks.1.attentions.0.transformer_blocks.0.attn1\",\n        \"up_blocks.1.attentions.1.transformer_blocks.0.attn1\",\n        \"up_blocks.1.attentions.2.transformer_blocks.0.attn1\",\n        # SD 1.5 U-Net (ldm)\n        \"input_blocks.7.1.transformer_blocks.0.attn1\",\n        \"input_blocks.8.1.transformer_blocks.0.attn1\",\n        \"output_blocks.3.1.transformer_blocks.0.attn1\",\n        \"output_blocks.4.1.transformer_blocks.0.attn1\",\n        \"output_blocks.5.1.transformer_blocks.0.attn1\",\n    ],\n    3: [\n        # SD 1.5 U-Net (diffusers)\n        \"mid_block.attentions.0.transformer_blocks.0.attn1\",\n        # SD 1.5 U-Net (ldm)\n        \"middle_block.1.transformer_blocks.0.attn1\",\n    ],\n}\n# XL layers, thanks for GitHub@gel-crabs for the help\nDEPTH_LAYERS_XL = {\n    0: [\n        # SD 1.5 U-Net (diffusers)\n        \"down_blocks.0.attentions.0.transformer_blocks.0.attn1\",\n        \"down_blocks.0.attentions.1.transformer_blocks.0.attn1\",\n        \"up_blocks.3.attentions.0.transformer_blocks.0.attn1\",\n        \"up_blocks.3.attentions.1.transformer_blocks.0.attn1\",\n        \"up_blocks.3.attentions.2.transformer_blocks.0.attn1\",\n        # SD 1.5 U-Net (ldm)\n        \"input_blocks.4.1.transformer_blocks.0.attn1\",\n        \"input_blocks.5.1.transformer_blocks.0.attn1\",\n        \"output_blocks.3.1.transformer_blocks.0.attn1\",\n        \"output_blocks.4.1.transformer_blocks.0.attn1\",\n        \"output_blocks.5.1.transformer_blocks.0.attn1\",\n        # SD 1.5 VAE\n        \"decoder.mid_block.attentions.0\",\n        \"decoder.mid.attn_1\",\n    ],\n    1: [\n        # SD 1.5 U-Net (diffusers)\n        #\"down_blocks.1.attentions.0.transformer_blocks.0.attn1\",\n        #\"down_blocks.1.attentions.1.transformer_blocks.0.attn1\",\n        #\"up_blocks.2.attentions.0.transformer_blocks.0.attn1\",\n        #\"up_blocks.2.attentions.1.transformer_blocks.0.attn1\",\n        #\"up_blocks.2.attentions.2.transformer_blocks.0.attn1\",\n        # SD 1.5 U-Net (ldm)\n        \"input_blocks.4.1.transformer_blocks.1.attn1\",\n        \"input_blocks.5.1.transformer_blocks.1.attn1\",\n        \"output_blocks.3.1.transformer_blocks.1.attn1\",\n        \"output_blocks.4.1.transformer_blocks.1.attn1\",\n        \"output_blocks.5.1.transformer_blocks.1.attn1\",\n        \"input_blocks.7.1.transformer_blocks.0.attn1\",\n        \"input_blocks.8.1.transformer_blocks.0.attn1\",\n        \"output_blocks.0.1.transformer_blocks.0.attn1\",\n        \"output_blocks.1.1.transformer_blocks.0.attn1\",\n        \"output_blocks.2.1.transformer_blocks.0.attn1\",\n        \"input_blocks.7.1.transformer_blocks.1.attn1\",\n        \"input_blocks.8.1.transformer_blocks.1.attn1\",\n        \"output_blocks.0.1.transformer_blocks.1.attn1\",\n        \"output_blocks.1.1.transformer_blocks.1.attn1\",\n        \"output_blocks.2.1.transformer_blocks.1.attn1\",\n        \"input_blocks.7.1.transformer_blocks.2.attn1\",\n        \"input_blocks.8.1.transformer_blocks.2.attn1\",\n        \"output_blocks.0.1.transformer_blocks.2.attn1\",\n        \"output_blocks.1.1.transformer_blocks.2.attn1\",\n        \"output_blocks.2.1.transformer_blocks.2.attn1\",\n        \"input_blocks.7.1.transformer_blocks.3.attn1\",\n        \"input_blocks.8.1.transformer_blocks.3.attn1\",\n        \"output_blocks.0.1.transformer_blocks.3.attn1\",\n        \"output_blocks.1.1.transformer_blocks.3.attn1\",\n        \"output_blocks.2.1.transformer_blocks.3.attn1\",\n        \"input_blocks.7.1.transformer_blocks.4.attn1\",\n        \"input_blocks.8.1.transformer_blocks.4.attn1\",\n        \"output_blocks.0.1.transformer_blocks.4.attn1\",\n        \"output_blocks.1.1.transformer_blocks.4.attn1\",\n        \"output_blocks.2.1.transformer_blocks.4.attn1\",\n        \"input_blocks.7.1.transformer_blocks.5.attn1\",\n        \"input_blocks.8.1.transformer_blocks.5.attn1\",\n        \"output_blocks.0.1.transformer_blocks.5.attn1\",\n        \"output_blocks.1.1.transformer_blocks.5.attn1\",\n        \"output_blocks.2.1.transformer_blocks.5.attn1\",\n        \"input_blocks.7.1.transformer_blocks.6.attn1\",\n        \"input_blocks.8.1.transformer_blocks.6.attn1\",\n        \"output_blocks.0.1.transformer_blocks.6.attn1\",\n        \"output_blocks.1.1.transformer_blocks.6.attn1\",\n        \"output_blocks.2.1.transformer_blocks.6.attn1\",\n        \"input_blocks.7.1.transformer_blocks.7.attn1\",\n        \"input_blocks.8.1.transformer_blocks.7.attn1\",\n        \"output_blocks.0.1.transformer_blocks.7.attn1\",\n        \"output_blocks.1.1.transformer_blocks.7.attn1\",\n        \"output_blocks.2.1.transformer_blocks.7.attn1\",\n        \"input_blocks.7.1.transformer_blocks.8.attn1\",\n        \"input_blocks.8.1.transformer_blocks.8.attn1\",\n        \"output_blocks.0.1.transformer_blocks.8.attn1\",\n        \"output_blocks.1.1.transformer_blocks.8.attn1\",\n        \"output_blocks.2.1.transformer_blocks.8.attn1\",\n        \"input_blocks.7.1.transformer_blocks.9.attn1\",\n        \"input_blocks.8.1.transformer_blocks.9.attn1\",\n        \"output_blocks.0.1.transformer_blocks.9.attn1\",\n        \"output_blocks.1.1.transformer_blocks.9.attn1\",\n        \"output_blocks.2.1.transformer_blocks.9.attn1\",\n    ],\n    2: [\n        # SD 1.5 U-Net (diffusers)\n        \"mid_block.attentions.0.transformer_blocks.0.attn1\",\n        # SD 1.5 U-Net (ldm)\n        \"middle_block.1.transformer_blocks.0.attn1\",\n        \"middle_block.1.transformer_blocks.1.attn1\",\n        \"middle_block.1.transformer_blocks.2.attn1\",\n        \"middle_block.1.transformer_blocks.3.attn1\",\n        \"middle_block.1.transformer_blocks.4.attn1\",\n        \"middle_block.1.transformer_blocks.5.attn1\",\n        \"middle_block.1.transformer_blocks.6.attn1\",\n        \"middle_block.1.transformer_blocks.7.attn1\",\n        \"middle_block.1.transformer_blocks.8.attn1\",\n        \"middle_block.1.transformer_blocks.9.attn1\",\n    ],\n    3 : [] # TODO - separate layers for SD-XL\n}\n\n\nRNG_INSTANCE = random.Random()\n\n@cache\ndef get_divisors(value: int, min_value: int, /, max_options: int = 1) -> list[int]:\n    \"\"\"\n    Returns divisors of value that\n        x * min_value <= value\n    in big -> small order, amount of divisors is limited by max_options\n    \"\"\"\n    max_options = max(1, max_options) # at least 1 option should be returned\n    min_value = min(min_value, value)\n    divisors = [i for i in range(min_value, value + 1) if value % i == 0] # divisors in small -> big order\n    ns = [value // i for i in divisors[:max_options]]  # has at least 1 element # big -> small order\n    return ns\n\n\ndef random_divisor(value: int, min_value: int, /, max_options: int = 1) -> int:\n    \"\"\"\n    Returns a random divisor of value that\n        x * min_value <= value\n    if max_options is 1, the behavior is deterministic\n    \"\"\"\n    ns = get_divisors(value, min_value, max_options=max_options) # get cached divisors\n    idx = RNG_INSTANCE.randint(0, len(ns) - 1)\n\n    return ns[idx]\n\n\ndef set_hypertile_seed(seed: int) -> None:\n    RNG_INSTANCE.seed(seed)\n\n\n@cache\ndef largest_tile_size_available(width: int, height: int) -> int:\n    \"\"\"\n    Calculates the largest tile size available for a given width and height\n    Tile size is always a power of 2\n    \"\"\"\n    gcd = math.gcd(width, height)\n    largest_tile_size_available = 1\n    while gcd % (largest_tile_size_available * 2) == 0:\n        largest_tile_size_available *= 2\n    return largest_tile_size_available\n\n\ndef iterative_closest_divisors(hw:int, aspect_ratio:float) -> tuple[int, int]:\n    \"\"\"\n    Finds h and w such that h*w = hw and h/w = aspect_ratio\n    We check all possible divisors of hw and return the closest to the aspect ratio\n    \"\"\"\n    divisors = [i for i in range(2, hw + 1) if hw % i == 0] # all divisors of hw\n    pairs = [(i, hw // i) for i in divisors] # all pairs of divisors of hw\n    ratios = [w/h for h, w in pairs] # all ratios of pairs of divisors of hw\n    closest_ratio = min(ratios, key=lambda x: abs(x - aspect_ratio)) # closest ratio to aspect_ratio\n    closest_pair = pairs[ratios.index(closest_ratio)] # closest pair of divisors to aspect_ratio\n    return closest_pair\n\n\n@cache\ndef find_hw_candidates(hw:int, aspect_ratio:float) -> tuple[int, int]:\n    \"\"\"\n    Finds h and w such that h*w = hw and h/w = aspect_ratio\n    \"\"\"\n    h, w = round(math.sqrt(hw * aspect_ratio)), round(math.sqrt(hw / aspect_ratio))\n    # find h and w such that h*w = hw and h/w = aspect_ratio\n    if h * w != hw:\n        w_candidate = hw / h\n        # check if w is an integer\n        if not w_candidate.is_integer():\n            h_candidate = hw / w\n            # check if h is an integer\n            if not h_candidate.is_integer():\n                return iterative_closest_divisors(hw, aspect_ratio)\n            else:\n                h = int(h_candidate)\n        else:\n            w = int(w_candidate)\n    return h, w\n\n\ndef self_attn_forward(params: HypertileParams, scale_depth=True) -> Callable:\n\n    @wraps(params.forward)\n    def wrapper(*args, **kwargs):\n        if not params.enabled:\n            return params.forward(*args, **kwargs)\n\n        latent_tile_size = max(128, params.tile_size) // 8\n        x = args[0]\n\n        # VAE\n        if x.ndim == 4:\n            b, c, h, w = x.shape\n\n            nh = random_divisor(h, latent_tile_size, params.swap_size)\n            nw = random_divisor(w, latent_tile_size, params.swap_size)\n\n            if nh * nw > 1:\n                x = rearrange(x, \"b c (nh h) (nw w) -> (b nh nw) c h w\", nh=nh, nw=nw)  # split into nh * nw tiles\n\n            out = params.forward(x, *args[1:], **kwargs)\n\n            if nh * nw > 1:\n                out = rearrange(out, \"(b nh nw) c h w -> b c (nh h) (nw w)\", nh=nh, nw=nw)\n\n        # U-Net\n        else:\n            hw: int = x.size(1)\n            h, w = find_hw_candidates(hw, params.aspect_ratio)\n            assert h * w == hw, f\"Invalid aspect ratio {params.aspect_ratio} for input of shape {x.shape}, hw={hw}, h={h}, w={w}\"\n\n            factor = 2 ** params.depth if scale_depth else 1\n            nh = random_divisor(h, latent_tile_size * factor, params.swap_size)\n            nw = random_divisor(w, latent_tile_size * factor, params.swap_size)\n\n            if nh * nw > 1:\n                x = rearrange(x, \"b (nh h nw w) c -> (b nh nw) (h w) c\", h=h // nh, w=w // nw, nh=nh, nw=nw)\n\n            out = params.forward(x, *args[1:], **kwargs)\n\n            if nh * nw > 1:\n                out = rearrange(out, \"(b nh nw) hw c -> b nh nw hw c\", nh=nh, nw=nw)\n                out = rearrange(out, \"b nh nw (h w) c -> b (nh h nw w) c\", h=h // nh, w=w // nw)\n\n        return out\n\n    return wrapper\n\n\ndef hypertile_hook_model(model: nn.Module, width, height, *, enable=False, tile_size_max=128, swap_size=1, max_depth=3, is_sdxl=False):\n    hypertile_layers = getattr(model, \"__webui_hypertile_layers\", None)\n    if hypertile_layers is None:\n        if not enable:\n            return\n\n        hypertile_layers = {}\n        layers = DEPTH_LAYERS_XL if is_sdxl else DEPTH_LAYERS\n\n        for depth in range(4):\n            for layer_name, module in model.named_modules():\n                if any(layer_name.endswith(try_name) for try_name in layers[depth]):\n                    params = HypertileParams()\n                    module.__webui_hypertile_params = params\n                    params.forward = module.forward\n                    params.depth = depth\n                    params.layer_name = layer_name\n                    module.forward = self_attn_forward(params)\n\n                    hypertile_layers[layer_name] = 1\n\n        model.__webui_hypertile_layers = hypertile_layers\n\n    aspect_ratio = width / height\n    tile_size = min(largest_tile_size_available(width, height), tile_size_max)\n\n    for layer_name, module in model.named_modules():\n        if layer_name in hypertile_layers:\n            params = module.__webui_hypertile_params\n\n            params.tile_size = tile_size\n            params.swap_size = swap_size\n            params.aspect_ratio = aspect_ratio\n            params.enabled = enable and params.depth <= max_depth\n", "extensions-builtin/hypertile/scripts/hypertile_script.py": "import hypertile\nfrom modules import scripts, script_callbacks, shared\nfrom scripts.hypertile_xyz import add_axis_options\n\n\nclass ScriptHypertile(scripts.Script):\n    name = \"Hypertile\"\n\n    def title(self):\n        return self.name\n\n    def show(self, is_img2img):\n        return scripts.AlwaysVisible\n\n    def process(self, p, *args):\n        hypertile.set_hypertile_seed(p.all_seeds[0])\n\n        configure_hypertile(p.width, p.height, enable_unet=shared.opts.hypertile_enable_unet)\n\n        self.add_infotext(p)\n\n    def before_hr(self, p, *args):\n\n        enable = shared.opts.hypertile_enable_unet_secondpass or shared.opts.hypertile_enable_unet\n\n        # exclusive hypertile seed for the second pass\n        if enable:\n            hypertile.set_hypertile_seed(p.all_seeds[0])\n\n        configure_hypertile(p.hr_upscale_to_x, p.hr_upscale_to_y, enable_unet=enable)\n\n        if enable and not shared.opts.hypertile_enable_unet:\n            p.extra_generation_params[\"Hypertile U-Net second pass\"] = True\n\n            self.add_infotext(p, add_unet_params=True)\n\n    def add_infotext(self, p, add_unet_params=False):\n        def option(name):\n            value = getattr(shared.opts, name)\n            default_value = shared.opts.get_default(name)\n            return None if value == default_value else value\n\n        if shared.opts.hypertile_enable_unet:\n            p.extra_generation_params[\"Hypertile U-Net\"] = True\n\n        if shared.opts.hypertile_enable_unet or add_unet_params:\n            p.extra_generation_params[\"Hypertile U-Net max depth\"] = option('hypertile_max_depth_unet')\n            p.extra_generation_params[\"Hypertile U-Net max tile size\"] = option('hypertile_max_tile_unet')\n            p.extra_generation_params[\"Hypertile U-Net swap size\"] = option('hypertile_swap_size_unet')\n\n        if shared.opts.hypertile_enable_vae:\n            p.extra_generation_params[\"Hypertile VAE\"] = True\n            p.extra_generation_params[\"Hypertile VAE max depth\"] = option('hypertile_max_depth_vae')\n            p.extra_generation_params[\"Hypertile VAE max tile size\"] = option('hypertile_max_tile_vae')\n            p.extra_generation_params[\"Hypertile VAE swap size\"] = option('hypertile_swap_size_vae')\n\n\ndef configure_hypertile(width, height, enable_unet=True):\n    hypertile.hypertile_hook_model(\n        shared.sd_model.first_stage_model,\n        width,\n        height,\n        swap_size=shared.opts.hypertile_swap_size_vae,\n        max_depth=shared.opts.hypertile_max_depth_vae,\n        tile_size_max=shared.opts.hypertile_max_tile_vae,\n        enable=shared.opts.hypertile_enable_vae,\n    )\n\n    hypertile.hypertile_hook_model(\n        shared.sd_model.model,\n        width,\n        height,\n        swap_size=shared.opts.hypertile_swap_size_unet,\n        max_depth=shared.opts.hypertile_max_depth_unet,\n        tile_size_max=shared.opts.hypertile_max_tile_unet,\n        enable=enable_unet,\n        is_sdxl=shared.sd_model.is_sdxl\n    )\n\n\ndef on_ui_settings():\n    import gradio as gr\n\n    options = {\n        \"hypertile_explanation\": shared.OptionHTML(\"\"\"\n    <a href='https://github.com/tfernd/HyperTile'>Hypertile</a> optimizes the self-attention layer within U-Net and VAE models,\n    resulting in a reduction in computation time ranging from 1 to 4 times. The larger the generated image is, the greater the\n    benefit.\n    \"\"\"),\n\n        \"hypertile_enable_unet\": shared.OptionInfo(False, \"Enable Hypertile U-Net\", infotext=\"Hypertile U-Net\").info(\"enables hypertile for all modes, including hires fix second pass; noticeable change in details of the generated picture\"),\n        \"hypertile_enable_unet_secondpass\": shared.OptionInfo(False, \"Enable Hypertile U-Net for hires fix second pass\", infotext=\"Hypertile U-Net second pass\").info(\"enables hypertile just for hires fix second pass - regardless of whether the above setting is enabled\"),\n        \"hypertile_max_depth_unet\": shared.OptionInfo(3, \"Hypertile U-Net max depth\", gr.Slider, {\"minimum\": 0, \"maximum\": 3, \"step\": 1}, infotext=\"Hypertile U-Net max depth\").info(\"larger = more neural network layers affected; minor effect on performance\"),\n        \"hypertile_max_tile_unet\": shared.OptionInfo(256, \"Hypertile U-Net max tile size\", gr.Slider, {\"minimum\": 0, \"maximum\": 512, \"step\": 16}, infotext=\"Hypertile U-Net max tile size\").info(\"larger = worse performance\"),\n        \"hypertile_swap_size_unet\": shared.OptionInfo(3, \"Hypertile U-Net swap size\", gr.Slider, {\"minimum\": 0, \"maximum\": 64, \"step\": 1}, infotext=\"Hypertile U-Net swap size\"),\n\n        \"hypertile_enable_vae\": shared.OptionInfo(False, \"Enable Hypertile VAE\", infotext=\"Hypertile VAE\").info(\"minimal change in the generated picture\"),\n        \"hypertile_max_depth_vae\": shared.OptionInfo(3, \"Hypertile VAE max depth\", gr.Slider, {\"minimum\": 0, \"maximum\": 3, \"step\": 1}, infotext=\"Hypertile VAE max depth\"),\n        \"hypertile_max_tile_vae\": shared.OptionInfo(128, \"Hypertile VAE max tile size\", gr.Slider, {\"minimum\": 0, \"maximum\": 512, \"step\": 16}, infotext=\"Hypertile VAE max tile size\"),\n        \"hypertile_swap_size_vae\": shared.OptionInfo(3, \"Hypertile VAE swap size \", gr.Slider, {\"minimum\": 0, \"maximum\": 64, \"step\": 1}, infotext=\"Hypertile VAE swap size\"),\n    }\n\n    for name, opt in options.items():\n        opt.section = ('hypertile', \"Hypertile\")\n        shared.opts.add_option(name, opt)\n\n\nscript_callbacks.on_ui_settings(on_ui_settings)\nscript_callbacks.on_before_ui(add_axis_options)\n", "extensions-builtin/hypertile/scripts/hypertile_xyz.py": "from modules import scripts\nfrom modules.shared import opts\n\nxyz_grid = [x for x in scripts.scripts_data if x.script_class.__module__ == \"xyz_grid.py\"][0].module\n\ndef int_applier(value_name:str, min_range:int = -1, max_range:int = -1):\n    \"\"\"\n    Returns a function that applies the given value to the given value_name in opts.data.\n    \"\"\"\n    def validate(value_name:str, value:str):\n        value = int(value)\n        # validate value\n        if not min_range == -1:\n            assert value >= min_range, f\"Value {value} for {value_name} must be greater than or equal to {min_range}\"\n        if not max_range == -1:\n            assert value <= max_range, f\"Value {value} for {value_name} must be less than or equal to {max_range}\"\n    def apply_int(p, x, xs):\n        validate(value_name, x)\n        opts.data[value_name] = int(x)\n    return apply_int\n\ndef bool_applier(value_name:str):\n    \"\"\"\n    Returns a function that applies the given value to the given value_name in opts.data.\n    \"\"\"\n    def validate(value_name:str, value:str):\n        assert value.lower() in [\"true\", \"false\"], f\"Value {value} for {value_name} must be either true or false\"\n    def apply_bool(p, x, xs):\n        validate(value_name, x)\n        value_boolean = x.lower() == \"true\"\n        opts.data[value_name] = value_boolean\n    return apply_bool\n\ndef add_axis_options():\n    extra_axis_options = [\n        xyz_grid.AxisOption(\"[Hypertile] Unet First pass Enabled\", str, bool_applier(\"hypertile_enable_unet\"), choices=xyz_grid.boolean_choice(reverse=True)),\n        xyz_grid.AxisOption(\"[Hypertile] Unet Second pass Enabled\", str, bool_applier(\"hypertile_enable_unet_secondpass\"), choices=xyz_grid.boolean_choice(reverse=True)),\n        xyz_grid.AxisOption(\"[Hypertile] Unet Max Depth\", int, int_applier(\"hypertile_max_depth_unet\", 0, 3), choices=lambda: [str(x) for x in range(4)]),\n        xyz_grid.AxisOption(\"[Hypertile] Unet Max Tile Size\", int, int_applier(\"hypertile_max_tile_unet\", 0, 512)),\n        xyz_grid.AxisOption(\"[Hypertile] Unet Swap Size\", int, int_applier(\"hypertile_swap_size_unet\", 0, 64)),\n        xyz_grid.AxisOption(\"[Hypertile] VAE Enabled\", str, bool_applier(\"hypertile_enable_vae\"), choices=xyz_grid.boolean_choice(reverse=True)),\n        xyz_grid.AxisOption(\"[Hypertile] VAE Max Depth\", int, int_applier(\"hypertile_max_depth_vae\", 0, 3), choices=lambda: [str(x) for x in range(4)]),\n        xyz_grid.AxisOption(\"[Hypertile] VAE Max Tile Size\", int, int_applier(\"hypertile_max_tile_vae\", 0, 512)),\n        xyz_grid.AxisOption(\"[Hypertile] VAE Swap Size\", int, int_applier(\"hypertile_swap_size_vae\", 0, 64)),\n    ]\n    set_a = {opt.label for opt in xyz_grid.axis_options}\n    set_b = {opt.label for opt in extra_axis_options}\n    if set_a.intersection(set_b):\n        return\n\n    xyz_grid.axis_options.extend(extra_axis_options)\n", "extensions-builtin/Lora/network_norm.py": "import network\n\n\nclass ModuleTypeNorm(network.ModuleType):\n    def create_module(self, net: network.Network, weights: network.NetworkWeights):\n        if all(x in weights.w for x in [\"w_norm\", \"b_norm\"]):\n            return NetworkModuleNorm(net, weights)\n\n        return None\n\n\nclass NetworkModuleNorm(network.NetworkModule):\n    def __init__(self,  net: network.Network, weights: network.NetworkWeights):\n        super().__init__(net, weights)\n\n        self.w_norm = weights.w.get(\"w_norm\")\n        self.b_norm = weights.w.get(\"b_norm\")\n\n    def calc_updown(self, orig_weight):\n        output_shape = self.w_norm.shape\n        updown = self.w_norm.to(orig_weight.device)\n\n        if self.b_norm is not None:\n            ex_bias = self.b_norm.to(orig_weight.device)\n        else:\n            ex_bias = None\n\n        return self.finalize_updown(updown, orig_weight, output_shape, ex_bias)\n", "extensions-builtin/Lora/network_full.py": "import network\n\n\nclass ModuleTypeFull(network.ModuleType):\n    def create_module(self, net: network.Network, weights: network.NetworkWeights):\n        if all(x in weights.w for x in [\"diff\"]):\n            return NetworkModuleFull(net, weights)\n\n        return None\n\n\nclass NetworkModuleFull(network.NetworkModule):\n    def __init__(self,  net: network.Network, weights: network.NetworkWeights):\n        super().__init__(net, weights)\n\n        self.weight = weights.w.get(\"diff\")\n        self.ex_bias = weights.w.get(\"diff_b\")\n\n    def calc_updown(self, orig_weight):\n        output_shape = self.weight.shape\n        updown = self.weight.to(orig_weight.device)\n        if self.ex_bias is not None:\n            ex_bias = self.ex_bias.to(orig_weight.device)\n        else:\n            ex_bias = None\n\n        return self.finalize_updown(updown, orig_weight, output_shape, ex_bias)\n", "extensions-builtin/Lora/lyco_helpers.py": "import torch\n\n\ndef make_weight_cp(t, wa, wb):\n    temp = torch.einsum('i j k l, j r -> i r k l', t, wb)\n    return torch.einsum('i j k l, i r -> r j k l', temp, wa)\n\n\ndef rebuild_conventional(up, down, shape, dyn_dim=None):\n    up = up.reshape(up.size(0), -1)\n    down = down.reshape(down.size(0), -1)\n    if dyn_dim is not None:\n        up = up[:, :dyn_dim]\n        down = down[:dyn_dim, :]\n    return (up @ down).reshape(shape)\n\n\ndef rebuild_cp_decomposition(up, down, mid):\n    up = up.reshape(up.size(0), -1)\n    down = down.reshape(down.size(0), -1)\n    return torch.einsum('n m k l, i n, m j -> i j k l', mid, up, down)\n\n\n# copied from https://github.com/KohakuBlueleaf/LyCORIS/blob/dev/lycoris/modules/lokr.py\ndef factorization(dimension: int, factor:int=-1) -> tuple[int, int]:\n    '''\n    return a tuple of two value of input dimension decomposed by the number closest to factor\n    second value is higher or equal than first value.\n\n    In LoRA with Kroneckor Product, first value is a value for weight scale.\n    secon value is a value for weight.\n\n    Because of non-commutative property, A\u2297B \u2260 B\u2297A. Meaning of two matrices is slightly different.\n\n    examples)\n    factor\n        -1               2                4               8               16               ...\n    127 -> 1, 127   127 -> 1, 127    127 -> 1, 127   127 -> 1, 127   127 -> 1, 127\n    128 -> 8, 16    128 -> 2, 64     128 -> 4, 32    128 -> 8, 16    128 -> 8, 16\n    250 -> 10, 25   250 -> 2, 125    250 -> 2, 125   250 -> 5, 50    250 -> 10, 25\n    360 -> 8, 45    360 -> 2, 180    360 -> 4, 90    360 -> 8, 45    360 -> 12, 30\n    512 -> 16, 32   512 -> 2, 256    512 -> 4, 128   512 -> 8, 64    512 -> 16, 32\n    1024 -> 32, 32  1024 -> 2, 512   1024 -> 4, 256  1024 -> 8, 128  1024 -> 16, 64\n    '''\n\n    if factor > 0 and (dimension % factor) == 0:\n        m = factor\n        n = dimension // factor\n        if m > n:\n            n, m = m, n\n        return m, n\n    if factor < 0:\n        factor = dimension\n    m, n = 1, dimension\n    length = m + n\n    while m<n:\n        new_m = m + 1\n        while dimension%new_m != 0:\n            new_m += 1\n        new_n = dimension // new_m\n        if new_m + new_n > length or new_m>factor:\n            break\n        else:\n            m, n = new_m, new_n\n    if m > n:\n        n, m = m, n\n    return m, n\n\n", "extensions-builtin/Lora/network_lora.py": "import torch\n\nimport lyco_helpers\nimport network\nfrom modules import devices\n\n\nclass ModuleTypeLora(network.ModuleType):\n    def create_module(self, net: network.Network, weights: network.NetworkWeights):\n        if all(x in weights.w for x in [\"lora_up.weight\", \"lora_down.weight\"]):\n            return NetworkModuleLora(net, weights)\n\n        return None\n\n\nclass NetworkModuleLora(network.NetworkModule):\n    def __init__(self,  net: network.Network, weights: network.NetworkWeights):\n        super().__init__(net, weights)\n\n        self.up_model = self.create_module(weights.w, \"lora_up.weight\")\n        self.down_model = self.create_module(weights.w, \"lora_down.weight\")\n        self.mid_model = self.create_module(weights.w, \"lora_mid.weight\", none_ok=True)\n\n        self.dim = weights.w[\"lora_down.weight\"].shape[0]\n\n    def create_module(self, weights, key, none_ok=False):\n        weight = weights.get(key)\n\n        if weight is None and none_ok:\n            return None\n\n        is_linear = type(self.sd_module) in [torch.nn.Linear, torch.nn.modules.linear.NonDynamicallyQuantizableLinear, torch.nn.MultiheadAttention]\n        is_conv = type(self.sd_module) in [torch.nn.Conv2d]\n\n        if is_linear:\n            weight = weight.reshape(weight.shape[0], -1)\n            module = torch.nn.Linear(weight.shape[1], weight.shape[0], bias=False)\n        elif is_conv and key == \"lora_down.weight\" or key == \"dyn_up\":\n            if len(weight.shape) == 2:\n                weight = weight.reshape(weight.shape[0], -1, 1, 1)\n\n            if weight.shape[2] != 1 or weight.shape[3] != 1:\n                module = torch.nn.Conv2d(weight.shape[1], weight.shape[0], self.sd_module.kernel_size, self.sd_module.stride, self.sd_module.padding, bias=False)\n            else:\n                module = torch.nn.Conv2d(weight.shape[1], weight.shape[0], (1, 1), bias=False)\n        elif is_conv and key == \"lora_mid.weight\":\n            module = torch.nn.Conv2d(weight.shape[1], weight.shape[0], self.sd_module.kernel_size, self.sd_module.stride, self.sd_module.padding, bias=False)\n        elif is_conv and key == \"lora_up.weight\" or key == \"dyn_down\":\n            module = torch.nn.Conv2d(weight.shape[1], weight.shape[0], (1, 1), bias=False)\n        else:\n            raise AssertionError(f'Lora layer {self.network_key} matched a layer with unsupported type: {type(self.sd_module).__name__}')\n\n        with torch.no_grad():\n            if weight.shape != module.weight.shape:\n                weight = weight.reshape(module.weight.shape)\n            module.weight.copy_(weight)\n\n        module.to(device=devices.cpu, dtype=devices.dtype)\n        module.weight.requires_grad_(False)\n\n        return module\n\n    def calc_updown(self, orig_weight):\n        up = self.up_model.weight.to(orig_weight.device)\n        down = self.down_model.weight.to(orig_weight.device)\n\n        output_shape = [up.size(0), down.size(1)]\n        if self.mid_model is not None:\n            # cp-decomposition\n            mid = self.mid_model.weight.to(orig_weight.device)\n            updown = lyco_helpers.rebuild_cp_decomposition(up, down, mid)\n            output_shape += mid.shape[2:]\n        else:\n            if len(down.shape) == 4:\n                output_shape += down.shape[2:]\n            updown = lyco_helpers.rebuild_conventional(up, down, output_shape, self.network.dyn_dim)\n\n        return self.finalize_updown(updown, orig_weight, output_shape)\n\n    def forward(self, x, y):\n        self.up_model.to(device=devices.device)\n        self.down_model.to(device=devices.device)\n\n        return y + self.up_model(self.down_model(x)) * self.multiplier() * self.calc_scale()\n\n\n", "extensions-builtin/Lora/lora_logger.py": "import sys\nimport copy\nimport logging\n\n\nclass ColoredFormatter(logging.Formatter):\n    COLORS = {\n        \"DEBUG\": \"\\033[0;36m\",  # CYAN\n        \"INFO\": \"\\033[0;32m\",  # GREEN\n        \"WARNING\": \"\\033[0;33m\",  # YELLOW\n        \"ERROR\": \"\\033[0;31m\",  # RED\n        \"CRITICAL\": \"\\033[0;37;41m\",  # WHITE ON RED\n        \"RESET\": \"\\033[0m\",  # RESET COLOR\n    }\n\n    def format(self, record):\n        colored_record = copy.copy(record)\n        levelname = colored_record.levelname\n        seq = self.COLORS.get(levelname, self.COLORS[\"RESET\"])\n        colored_record.levelname = f\"{seq}{levelname}{self.COLORS['RESET']}\"\n        return super().format(colored_record)\n\n\nlogger = logging.getLogger(\"lora\")\nlogger.propagate = False\n\n\nif not logger.handlers:\n    handler = logging.StreamHandler(sys.stdout)\n    handler.setFormatter(\n        ColoredFormatter(\"[%(name)s]-%(levelname)s: %(message)s\")\n    )\n    logger.addHandler(handler)\n", "extensions-builtin/Lora/network_ia3.py": "import network\n\n\nclass ModuleTypeIa3(network.ModuleType):\n    def create_module(self, net: network.Network, weights: network.NetworkWeights):\n        if all(x in weights.w for x in [\"weight\"]):\n            return NetworkModuleIa3(net, weights)\n\n        return None\n\n\nclass NetworkModuleIa3(network.NetworkModule):\n    def __init__(self,  net: network.Network, weights: network.NetworkWeights):\n        super().__init__(net, weights)\n\n        self.w = weights.w[\"weight\"]\n        self.on_input = weights.w[\"on_input\"].item()\n\n    def calc_updown(self, orig_weight):\n        w = self.w.to(orig_weight.device)\n\n        output_shape = [w.size(0), orig_weight.size(1)]\n        if self.on_input:\n            output_shape.reverse()\n        else:\n            w = w.reshape(-1, 1)\n\n        updown = orig_weight * w\n\n        return self.finalize_updown(updown, orig_weight, output_shape)\n", "extensions-builtin/Lora/lora_patches.py": "import torch\n\nimport networks\nfrom modules import patches\n\n\nclass LoraPatches:\n    def __init__(self):\n        self.Linear_forward = patches.patch(__name__, torch.nn.Linear, 'forward', networks.network_Linear_forward)\n        self.Linear_load_state_dict = patches.patch(__name__, torch.nn.Linear, '_load_from_state_dict', networks.network_Linear_load_state_dict)\n        self.Conv2d_forward = patches.patch(__name__, torch.nn.Conv2d, 'forward', networks.network_Conv2d_forward)\n        self.Conv2d_load_state_dict = patches.patch(__name__, torch.nn.Conv2d, '_load_from_state_dict', networks.network_Conv2d_load_state_dict)\n        self.GroupNorm_forward = patches.patch(__name__, torch.nn.GroupNorm, 'forward', networks.network_GroupNorm_forward)\n        self.GroupNorm_load_state_dict = patches.patch(__name__, torch.nn.GroupNorm, '_load_from_state_dict', networks.network_GroupNorm_load_state_dict)\n        self.LayerNorm_forward = patches.patch(__name__, torch.nn.LayerNorm, 'forward', networks.network_LayerNorm_forward)\n        self.LayerNorm_load_state_dict = patches.patch(__name__, torch.nn.LayerNorm, '_load_from_state_dict', networks.network_LayerNorm_load_state_dict)\n        self.MultiheadAttention_forward = patches.patch(__name__, torch.nn.MultiheadAttention, 'forward', networks.network_MultiheadAttention_forward)\n        self.MultiheadAttention_load_state_dict = patches.patch(__name__, torch.nn.MultiheadAttention, '_load_from_state_dict', networks.network_MultiheadAttention_load_state_dict)\n\n    def undo(self):\n        self.Linear_forward = patches.undo(__name__, torch.nn.Linear, 'forward')\n        self.Linear_load_state_dict = patches.undo(__name__, torch.nn.Linear, '_load_from_state_dict')\n        self.Conv2d_forward = patches.undo(__name__, torch.nn.Conv2d, 'forward')\n        self.Conv2d_load_state_dict = patches.undo(__name__, torch.nn.Conv2d, '_load_from_state_dict')\n        self.GroupNorm_forward = patches.undo(__name__, torch.nn.GroupNorm, 'forward')\n        self.GroupNorm_load_state_dict = patches.undo(__name__, torch.nn.GroupNorm, '_load_from_state_dict')\n        self.LayerNorm_forward = patches.undo(__name__, torch.nn.LayerNorm, 'forward')\n        self.LayerNorm_load_state_dict = patches.undo(__name__, torch.nn.LayerNorm, '_load_from_state_dict')\n        self.MultiheadAttention_forward = patches.undo(__name__, torch.nn.MultiheadAttention, 'forward')\n        self.MultiheadAttention_load_state_dict = patches.undo(__name__, torch.nn.MultiheadAttention, '_load_from_state_dict')\n\n", "extensions-builtin/Lora/ui_extra_networks_lora.py": "import os\n\nimport network\nimport networks\n\nfrom modules import shared, ui_extra_networks\nfrom modules.ui_extra_networks import quote_js\nfrom ui_edit_user_metadata import LoraUserMetadataEditor\n\n\nclass ExtraNetworksPageLora(ui_extra_networks.ExtraNetworksPage):\n    def __init__(self):\n        super().__init__('Lora')\n\n    def refresh(self):\n        networks.list_available_networks()\n\n    def create_item(self, name, index=None, enable_filter=True):\n        lora_on_disk = networks.available_networks.get(name)\n        if lora_on_disk is None:\n            return\n\n        path, ext = os.path.splitext(lora_on_disk.filename)\n\n        alias = lora_on_disk.get_alias()\n\n        search_terms = [self.search_terms_from_path(lora_on_disk.filename)]\n        if lora_on_disk.hash:\n            search_terms.append(lora_on_disk.hash)\n        item = {\n            \"name\": name,\n            \"filename\": lora_on_disk.filename,\n            \"shorthash\": lora_on_disk.shorthash,\n            \"preview\": self.find_preview(path) or self.find_embedded_preview(path, name, lora_on_disk.metadata),\n            \"description\": self.find_description(path),\n            \"search_terms\": search_terms,\n            \"local_preview\": f\"{path}.{shared.opts.samples_format}\",\n            \"metadata\": lora_on_disk.metadata,\n            \"sort_keys\": {'default': index, **self.get_sort_keys(lora_on_disk.filename)},\n            \"sd_version\": lora_on_disk.sd_version.name,\n        }\n\n        self.read_user_metadata(item)\n        activation_text = item[\"user_metadata\"].get(\"activation text\")\n        preferred_weight = item[\"user_metadata\"].get(\"preferred weight\", 0.0)\n        item[\"prompt\"] = quote_js(f\"<lora:{alias}:\") + \" + \" + (str(preferred_weight) if preferred_weight else \"opts.extra_networks_default_multiplier\") + \" + \" + quote_js(\">\")\n\n        if activation_text:\n            item[\"prompt\"] += \" + \" + quote_js(\" \" + activation_text)\n\n        negative_prompt = item[\"user_metadata\"].get(\"negative text\")\n        item[\"negative_prompt\"] = quote_js(\"\")\n        if negative_prompt:\n            item[\"negative_prompt\"] = quote_js('(' + negative_prompt + ':1)')\n\n        sd_version = item[\"user_metadata\"].get(\"sd version\")\n        if sd_version in network.SdVersion.__members__:\n            item[\"sd_version\"] = sd_version\n            sd_version = network.SdVersion[sd_version]\n        else:\n            sd_version = lora_on_disk.sd_version\n\n        if shared.opts.lora_show_all or not enable_filter:\n            pass\n        elif sd_version == network.SdVersion.Unknown:\n            model_version = network.SdVersion.SDXL if shared.sd_model.is_sdxl else network.SdVersion.SD2 if shared.sd_model.is_sd2 else network.SdVersion.SD1\n            if model_version.name in shared.opts.lora_hide_unknown_for_versions:\n                return None\n        elif shared.sd_model.is_sdxl and sd_version != network.SdVersion.SDXL:\n            return None\n        elif shared.sd_model.is_sd2 and sd_version != network.SdVersion.SD2:\n            return None\n        elif shared.sd_model.is_sd1 and sd_version != network.SdVersion.SD1:\n            return None\n\n        return item\n\n    def list_items(self):\n        # instantiate a list to protect against concurrent modification\n        names = list(networks.available_networks)\n        for index, name in enumerate(names):\n            item = self.create_item(name, index)\n            if item is not None:\n                yield item\n\n    def allowed_directories_for_previews(self):\n        return [shared.cmd_opts.lora_dir, shared.cmd_opts.lyco_dir_backcompat]\n\n    def create_user_metadata_editor(self, ui, tabname):\n        return LoraUserMetadataEditor(ui, tabname, self)\n", "extensions-builtin/Lora/network_hada.py": "import lyco_helpers\nimport network\n\n\nclass ModuleTypeHada(network.ModuleType):\n    def create_module(self, net: network.Network, weights: network.NetworkWeights):\n        if all(x in weights.w for x in [\"hada_w1_a\", \"hada_w1_b\", \"hada_w2_a\", \"hada_w2_b\"]):\n            return NetworkModuleHada(net, weights)\n\n        return None\n\n\nclass NetworkModuleHada(network.NetworkModule):\n    def __init__(self,  net: network.Network, weights: network.NetworkWeights):\n        super().__init__(net, weights)\n\n        if hasattr(self.sd_module, 'weight'):\n            self.shape = self.sd_module.weight.shape\n\n        self.w1a = weights.w[\"hada_w1_a\"]\n        self.w1b = weights.w[\"hada_w1_b\"]\n        self.dim = self.w1b.shape[0]\n        self.w2a = weights.w[\"hada_w2_a\"]\n        self.w2b = weights.w[\"hada_w2_b\"]\n\n        self.t1 = weights.w.get(\"hada_t1\")\n        self.t2 = weights.w.get(\"hada_t2\")\n\n    def calc_updown(self, orig_weight):\n        w1a = self.w1a.to(orig_weight.device)\n        w1b = self.w1b.to(orig_weight.device)\n        w2a = self.w2a.to(orig_weight.device)\n        w2b = self.w2b.to(orig_weight.device)\n\n        output_shape = [w1a.size(0), w1b.size(1)]\n\n        if self.t1 is not None:\n            output_shape = [w1a.size(1), w1b.size(1)]\n            t1 = self.t1.to(orig_weight.device)\n            updown1 = lyco_helpers.make_weight_cp(t1, w1a, w1b)\n            output_shape += t1.shape[2:]\n        else:\n            if len(w1b.shape) == 4:\n                output_shape += w1b.shape[2:]\n            updown1 = lyco_helpers.rebuild_conventional(w1a, w1b, output_shape)\n\n        if self.t2 is not None:\n            t2 = self.t2.to(orig_weight.device)\n            updown2 = lyco_helpers.make_weight_cp(t2, w2a, w2b)\n        else:\n            updown2 = lyco_helpers.rebuild_conventional(w2a, w2b, output_shape)\n\n        updown = updown1 * updown2\n\n        return self.finalize_updown(updown, orig_weight, output_shape)\n", "extensions-builtin/Lora/network_glora.py": "\nimport network\n\nclass ModuleTypeGLora(network.ModuleType):\n    def create_module(self, net: network.Network, weights: network.NetworkWeights):\n        if all(x in weights.w for x in [\"a1.weight\", \"a2.weight\", \"alpha\", \"b1.weight\", \"b2.weight\"]):\n            return NetworkModuleGLora(net, weights)\n\n        return None\n\n# adapted from https://github.com/KohakuBlueleaf/LyCORIS\nclass NetworkModuleGLora(network.NetworkModule):\n    def __init__(self,  net: network.Network, weights: network.NetworkWeights):\n        super().__init__(net, weights)\n\n        if hasattr(self.sd_module, 'weight'):\n            self.shape = self.sd_module.weight.shape\n\n        self.w1a = weights.w[\"a1.weight\"]\n        self.w1b = weights.w[\"b1.weight\"]\n        self.w2a = weights.w[\"a2.weight\"]\n        self.w2b = weights.w[\"b2.weight\"]\n\n    def calc_updown(self, orig_weight):\n        w1a = self.w1a.to(orig_weight.device)\n        w1b = self.w1b.to(orig_weight.device)\n        w2a = self.w2a.to(orig_weight.device)\n        w2b = self.w2b.to(orig_weight.device)\n\n        output_shape = [w1a.size(0), w1b.size(1)]\n        updown = ((w2b @ w1b) + ((orig_weight.to(dtype = w1a.dtype) @ w2a) @ w1a))\n\n        return self.finalize_updown(updown, orig_weight, output_shape)\n", "extensions-builtin/Lora/lora.py": "import networks\n\nlist_available_loras = networks.list_available_networks\n\navailable_loras = networks.available_networks\navailable_lora_aliases = networks.available_network_aliases\navailable_lora_hash_lookup = networks.available_network_hash_lookup\nforbidden_lora_aliases = networks.forbidden_network_aliases\nloaded_loras = networks.loaded_networks\n", "extensions-builtin/Lora/preload.py": "import os\nfrom modules import paths\nfrom modules.paths_internal import normalized_filepath\n\n\ndef preload(parser):\n    parser.add_argument(\"--lora-dir\", type=normalized_filepath, help=\"Path to directory with Lora networks.\", default=os.path.join(paths.models_path, 'Lora'))\n    parser.add_argument(\"--lyco-dir-backcompat\", type=normalized_filepath, help=\"Path to directory with LyCORIS networks (for backawards compatibility; can also use --lyco-dir).\", default=os.path.join(paths.models_path, 'LyCORIS'))\n", "extensions-builtin/Lora/networks.py": "import gradio as gr\nimport logging\nimport os\nimport re\n\nimport lora_patches\nimport network\nimport network_lora\nimport network_glora\nimport network_hada\nimport network_ia3\nimport network_lokr\nimport network_full\nimport network_norm\nimport network_oft\n\nimport torch\nfrom typing import Union\n\nfrom modules import shared, devices, sd_models, errors, scripts, sd_hijack\nimport modules.textual_inversion.textual_inversion as textual_inversion\n\nfrom lora_logger import logger\n\nmodule_types = [\n    network_lora.ModuleTypeLora(),\n    network_hada.ModuleTypeHada(),\n    network_ia3.ModuleTypeIa3(),\n    network_lokr.ModuleTypeLokr(),\n    network_full.ModuleTypeFull(),\n    network_norm.ModuleTypeNorm(),\n    network_glora.ModuleTypeGLora(),\n    network_oft.ModuleTypeOFT(),\n]\n\n\nre_digits = re.compile(r\"\\d+\")\nre_x_proj = re.compile(r\"(.*)_([qkv]_proj)$\")\nre_compiled = {}\n\nsuffix_conversion = {\n    \"attentions\": {},\n    \"resnets\": {\n        \"conv1\": \"in_layers_2\",\n        \"conv2\": \"out_layers_3\",\n        \"norm1\": \"in_layers_0\",\n        \"norm2\": \"out_layers_0\",\n        \"time_emb_proj\": \"emb_layers_1\",\n        \"conv_shortcut\": \"skip_connection\",\n    }\n}\n\n\ndef convert_diffusers_name_to_compvis(key, is_sd2):\n    def match(match_list, regex_text):\n        regex = re_compiled.get(regex_text)\n        if regex is None:\n            regex = re.compile(regex_text)\n            re_compiled[regex_text] = regex\n\n        r = re.match(regex, key)\n        if not r:\n            return False\n\n        match_list.clear()\n        match_list.extend([int(x) if re.match(re_digits, x) else x for x in r.groups()])\n        return True\n\n    m = []\n\n    if match(m, r\"lora_unet_conv_in(.*)\"):\n        return f'diffusion_model_input_blocks_0_0{m[0]}'\n\n    if match(m, r\"lora_unet_conv_out(.*)\"):\n        return f'diffusion_model_out_2{m[0]}'\n\n    if match(m, r\"lora_unet_time_embedding_linear_(\\d+)(.*)\"):\n        return f\"diffusion_model_time_embed_{m[0] * 2 - 2}{m[1]}\"\n\n    if match(m, r\"lora_unet_down_blocks_(\\d+)_(attentions|resnets)_(\\d+)_(.+)\"):\n        suffix = suffix_conversion.get(m[1], {}).get(m[3], m[3])\n        return f\"diffusion_model_input_blocks_{1 + m[0] * 3 + m[2]}_{1 if m[1] == 'attentions' else 0}_{suffix}\"\n\n    if match(m, r\"lora_unet_mid_block_(attentions|resnets)_(\\d+)_(.+)\"):\n        suffix = suffix_conversion.get(m[0], {}).get(m[2], m[2])\n        return f\"diffusion_model_middle_block_{1 if m[0] == 'attentions' else m[1] * 2}_{suffix}\"\n\n    if match(m, r\"lora_unet_up_blocks_(\\d+)_(attentions|resnets)_(\\d+)_(.+)\"):\n        suffix = suffix_conversion.get(m[1], {}).get(m[3], m[3])\n        return f\"diffusion_model_output_blocks_{m[0] * 3 + m[2]}_{1 if m[1] == 'attentions' else 0}_{suffix}\"\n\n    if match(m, r\"lora_unet_down_blocks_(\\d+)_downsamplers_0_conv\"):\n        return f\"diffusion_model_input_blocks_{3 + m[0] * 3}_0_op\"\n\n    if match(m, r\"lora_unet_up_blocks_(\\d+)_upsamplers_0_conv\"):\n        return f\"diffusion_model_output_blocks_{2 + m[0] * 3}_{2 if m[0]>0 else 1}_conv\"\n\n    if match(m, r\"lora_te_text_model_encoder_layers_(\\d+)_(.+)\"):\n        if is_sd2:\n            if 'mlp_fc1' in m[1]:\n                return f\"model_transformer_resblocks_{m[0]}_{m[1].replace('mlp_fc1', 'mlp_c_fc')}\"\n            elif 'mlp_fc2' in m[1]:\n                return f\"model_transformer_resblocks_{m[0]}_{m[1].replace('mlp_fc2', 'mlp_c_proj')}\"\n            else:\n                return f\"model_transformer_resblocks_{m[0]}_{m[1].replace('self_attn', 'attn')}\"\n\n        return f\"transformer_text_model_encoder_layers_{m[0]}_{m[1]}\"\n\n    if match(m, r\"lora_te2_text_model_encoder_layers_(\\d+)_(.+)\"):\n        if 'mlp_fc1' in m[1]:\n            return f\"1_model_transformer_resblocks_{m[0]}_{m[1].replace('mlp_fc1', 'mlp_c_fc')}\"\n        elif 'mlp_fc2' in m[1]:\n            return f\"1_model_transformer_resblocks_{m[0]}_{m[1].replace('mlp_fc2', 'mlp_c_proj')}\"\n        else:\n            return f\"1_model_transformer_resblocks_{m[0]}_{m[1].replace('self_attn', 'attn')}\"\n\n    return key\n\n\ndef assign_network_names_to_compvis_modules(sd_model):\n    network_layer_mapping = {}\n\n    if shared.sd_model.is_sdxl:\n        for i, embedder in enumerate(shared.sd_model.conditioner.embedders):\n            if not hasattr(embedder, 'wrapped'):\n                continue\n\n            for name, module in embedder.wrapped.named_modules():\n                network_name = f'{i}_{name.replace(\".\", \"_\")}'\n                network_layer_mapping[network_name] = module\n                module.network_layer_name = network_name\n    else:\n        for name, module in shared.sd_model.cond_stage_model.wrapped.named_modules():\n            network_name = name.replace(\".\", \"_\")\n            network_layer_mapping[network_name] = module\n            module.network_layer_name = network_name\n\n    for name, module in shared.sd_model.model.named_modules():\n        network_name = name.replace(\".\", \"_\")\n        network_layer_mapping[network_name] = module\n        module.network_layer_name = network_name\n\n    sd_model.network_layer_mapping = network_layer_mapping\n\n\ndef load_network(name, network_on_disk):\n    net = network.Network(name, network_on_disk)\n    net.mtime = os.path.getmtime(network_on_disk.filename)\n\n    sd = sd_models.read_state_dict(network_on_disk.filename)\n\n    # this should not be needed but is here as an emergency fix for an unknown error people are experiencing in 1.2.0\n    if not hasattr(shared.sd_model, 'network_layer_mapping'):\n        assign_network_names_to_compvis_modules(shared.sd_model)\n\n    keys_failed_to_match = {}\n    is_sd2 = 'model_transformer_resblocks' in shared.sd_model.network_layer_mapping\n\n    matched_networks = {}\n    bundle_embeddings = {}\n\n    for key_network, weight in sd.items():\n        key_network_without_network_parts, _, network_part = key_network.partition(\".\")\n\n        if key_network_without_network_parts == \"bundle_emb\":\n            emb_name, vec_name = network_part.split(\".\", 1)\n            emb_dict = bundle_embeddings.get(emb_name, {})\n            if vec_name.split('.')[0] == 'string_to_param':\n                _, k2 = vec_name.split('.', 1)\n                emb_dict['string_to_param'] = {k2: weight}\n            else:\n                emb_dict[vec_name] = weight\n            bundle_embeddings[emb_name] = emb_dict\n\n        key = convert_diffusers_name_to_compvis(key_network_without_network_parts, is_sd2)\n        sd_module = shared.sd_model.network_layer_mapping.get(key, None)\n\n        if sd_module is None:\n            m = re_x_proj.match(key)\n            if m:\n                sd_module = shared.sd_model.network_layer_mapping.get(m.group(1), None)\n\n        # SDXL loras seem to already have correct compvis keys, so only need to replace \"lora_unet\" with \"diffusion_model\"\n        if sd_module is None and \"lora_unet\" in key_network_without_network_parts:\n            key = key_network_without_network_parts.replace(\"lora_unet\", \"diffusion_model\")\n            sd_module = shared.sd_model.network_layer_mapping.get(key, None)\n        elif sd_module is None and \"lora_te1_text_model\" in key_network_without_network_parts:\n            key = key_network_without_network_parts.replace(\"lora_te1_text_model\", \"0_transformer_text_model\")\n            sd_module = shared.sd_model.network_layer_mapping.get(key, None)\n\n            # some SD1 Loras also have correct compvis keys\n            if sd_module is None:\n                key = key_network_without_network_parts.replace(\"lora_te1_text_model\", \"transformer_text_model\")\n                sd_module = shared.sd_model.network_layer_mapping.get(key, None)\n\n        # kohya_ss OFT module\n        elif sd_module is None and \"oft_unet\" in key_network_without_network_parts:\n            key = key_network_without_network_parts.replace(\"oft_unet\", \"diffusion_model\")\n            sd_module = shared.sd_model.network_layer_mapping.get(key, None)\n\n        # KohakuBlueLeaf OFT module\n        if sd_module is None and \"oft_diag\" in key:\n            key = key_network_without_network_parts.replace(\"lora_unet\", \"diffusion_model\")\n            key = key_network_without_network_parts.replace(\"lora_te1_text_model\", \"0_transformer_text_model\")\n            sd_module = shared.sd_model.network_layer_mapping.get(key, None)\n\n        if sd_module is None:\n            keys_failed_to_match[key_network] = key\n            continue\n\n        if key not in matched_networks:\n            matched_networks[key] = network.NetworkWeights(network_key=key_network, sd_key=key, w={}, sd_module=sd_module)\n\n        matched_networks[key].w[network_part] = weight\n\n    for key, weights in matched_networks.items():\n        net_module = None\n        for nettype in module_types:\n            net_module = nettype.create_module(net, weights)\n            if net_module is not None:\n                break\n\n        if net_module is None:\n            raise AssertionError(f\"Could not find a module type (out of {', '.join([x.__class__.__name__ for x in module_types])}) that would accept those keys: {', '.join(weights.w)}\")\n\n        net.modules[key] = net_module\n\n    embeddings = {}\n    for emb_name, data in bundle_embeddings.items():\n        embedding = textual_inversion.create_embedding_from_data(data, emb_name, filename=network_on_disk.filename + \"/\" + emb_name)\n        embedding.loaded = None\n        embeddings[emb_name] = embedding\n\n    net.bundle_embeddings = embeddings\n\n    if keys_failed_to_match:\n        logging.debug(f\"Network {network_on_disk.filename} didn't match keys: {keys_failed_to_match}\")\n\n    return net\n\n\ndef purge_networks_from_memory():\n    while len(networks_in_memory) > shared.opts.lora_in_memory_limit and len(networks_in_memory) > 0:\n        name = next(iter(networks_in_memory))\n        networks_in_memory.pop(name, None)\n\n    devices.torch_gc()\n\n\ndef load_networks(names, te_multipliers=None, unet_multipliers=None, dyn_dims=None):\n    emb_db = sd_hijack.model_hijack.embedding_db\n    already_loaded = {}\n\n    for net in loaded_networks:\n        if net.name in names:\n            already_loaded[net.name] = net\n        for emb_name, embedding in net.bundle_embeddings.items():\n            if embedding.loaded:\n                emb_db.register_embedding_by_name(None, shared.sd_model, emb_name)\n\n    loaded_networks.clear()\n\n    networks_on_disk = [available_networks.get(name, None) if name.lower() in forbidden_network_aliases else available_network_aliases.get(name, None) for name in names]\n    if any(x is None for x in networks_on_disk):\n        list_available_networks()\n\n        networks_on_disk = [available_networks.get(name, None) if name.lower() in forbidden_network_aliases else available_network_aliases.get(name, None) for name in names]\n\n    failed_to_load_networks = []\n\n    for i, (network_on_disk, name) in enumerate(zip(networks_on_disk, names)):\n        net = already_loaded.get(name, None)\n\n        if network_on_disk is not None:\n            if net is None:\n                net = networks_in_memory.get(name)\n\n            if net is None or os.path.getmtime(network_on_disk.filename) > net.mtime:\n                try:\n                    net = load_network(name, network_on_disk)\n\n                    networks_in_memory.pop(name, None)\n                    networks_in_memory[name] = net\n                except Exception as e:\n                    errors.display(e, f\"loading network {network_on_disk.filename}\")\n                    continue\n\n            net.mentioned_name = name\n\n            network_on_disk.read_hash()\n\n        if net is None:\n            failed_to_load_networks.append(name)\n            logging.info(f\"Couldn't find network with name {name}\")\n            continue\n\n        net.te_multiplier = te_multipliers[i] if te_multipliers else 1.0\n        net.unet_multiplier = unet_multipliers[i] if unet_multipliers else 1.0\n        net.dyn_dim = dyn_dims[i] if dyn_dims else 1.0\n        loaded_networks.append(net)\n\n        for emb_name, embedding in net.bundle_embeddings.items():\n            if embedding.loaded is None and emb_name in emb_db.word_embeddings:\n                logger.warning(\n                    f'Skip bundle embedding: \"{emb_name}\"'\n                    ' as it was already loaded from embeddings folder'\n                )\n                continue\n\n            embedding.loaded = False\n            if emb_db.expected_shape == -1 or emb_db.expected_shape == embedding.shape:\n                embedding.loaded = True\n                emb_db.register_embedding(embedding, shared.sd_model)\n            else:\n                emb_db.skipped_embeddings[name] = embedding\n\n    if failed_to_load_networks:\n        lora_not_found_message = f'Lora not found: {\", \".join(failed_to_load_networks)}'\n        sd_hijack.model_hijack.comments.append(lora_not_found_message)\n        if shared.opts.lora_not_found_warning_console:\n            print(f'\\n{lora_not_found_message}\\n')\n        if shared.opts.lora_not_found_gradio_warning:\n            gr.Warning(lora_not_found_message)\n\n    purge_networks_from_memory()\n\n\ndef network_restore_weights_from_backup(self: Union[torch.nn.Conv2d, torch.nn.Linear, torch.nn.GroupNorm, torch.nn.LayerNorm, torch.nn.MultiheadAttention]):\n    weights_backup = getattr(self, \"network_weights_backup\", None)\n    bias_backup = getattr(self, \"network_bias_backup\", None)\n\n    if weights_backup is None and bias_backup is None:\n        return\n\n    if weights_backup is not None:\n        if isinstance(self, torch.nn.MultiheadAttention):\n            self.in_proj_weight.copy_(weights_backup[0])\n            self.out_proj.weight.copy_(weights_backup[1])\n        else:\n            self.weight.copy_(weights_backup)\n\n    if bias_backup is not None:\n        if isinstance(self, torch.nn.MultiheadAttention):\n            self.out_proj.bias.copy_(bias_backup)\n        else:\n            self.bias.copy_(bias_backup)\n    else:\n        if isinstance(self, torch.nn.MultiheadAttention):\n            self.out_proj.bias = None\n        else:\n            self.bias = None\n\n\ndef network_apply_weights(self: Union[torch.nn.Conv2d, torch.nn.Linear, torch.nn.GroupNorm, torch.nn.LayerNorm, torch.nn.MultiheadAttention]):\n    \"\"\"\n    Applies the currently selected set of networks to the weights of torch layer self.\n    If weights already have this particular set of networks applied, does nothing.\n    If not, restores original weights from backup and alters weights according to networks.\n    \"\"\"\n\n    network_layer_name = getattr(self, 'network_layer_name', None)\n    if network_layer_name is None:\n        return\n\n    current_names = getattr(self, \"network_current_names\", ())\n    wanted_names = tuple((x.name, x.te_multiplier, x.unet_multiplier, x.dyn_dim) for x in loaded_networks)\n\n    weights_backup = getattr(self, \"network_weights_backup\", None)\n    if weights_backup is None and wanted_names != ():\n        if current_names != ():\n            raise RuntimeError(\"no backup weights found and current weights are not unchanged\")\n\n        if isinstance(self, torch.nn.MultiheadAttention):\n            weights_backup = (self.in_proj_weight.to(devices.cpu, copy=True), self.out_proj.weight.to(devices.cpu, copy=True))\n        else:\n            weights_backup = self.weight.to(devices.cpu, copy=True)\n\n        self.network_weights_backup = weights_backup\n\n    bias_backup = getattr(self, \"network_bias_backup\", None)\n    if bias_backup is None:\n        if isinstance(self, torch.nn.MultiheadAttention) and self.out_proj.bias is not None:\n            bias_backup = self.out_proj.bias.to(devices.cpu, copy=True)\n        elif getattr(self, 'bias', None) is not None:\n            bias_backup = self.bias.to(devices.cpu, copy=True)\n        else:\n            bias_backup = None\n        self.network_bias_backup = bias_backup\n\n    if current_names != wanted_names:\n        network_restore_weights_from_backup(self)\n\n        for net in loaded_networks:\n            module = net.modules.get(network_layer_name, None)\n            if module is not None and hasattr(self, 'weight'):\n                try:\n                    with torch.no_grad():\n                        if getattr(self, 'fp16_weight', None) is None:\n                            weight = self.weight\n                            bias = self.bias\n                        else:\n                            weight = self.fp16_weight.clone().to(self.weight.device)\n                            bias = getattr(self, 'fp16_bias', None)\n                            if bias is not None:\n                                bias = bias.clone().to(self.bias.device)\n                        updown, ex_bias = module.calc_updown(weight)\n\n                        if len(weight.shape) == 4 and weight.shape[1] == 9:\n                            # inpainting model. zero pad updown to make channel[1]  4 to 9\n                            updown = torch.nn.functional.pad(updown, (0, 0, 0, 0, 0, 5))\n\n                        self.weight.copy_((weight.to(dtype=updown.dtype) + updown).to(dtype=self.weight.dtype))\n                        if ex_bias is not None and hasattr(self, 'bias'):\n                            if self.bias is None:\n                                self.bias = torch.nn.Parameter(ex_bias).to(self.weight.dtype)\n                            else:\n                                self.bias.copy_((bias + ex_bias).to(dtype=self.bias.dtype))\n                except RuntimeError as e:\n                    logging.debug(f\"Network {net.name} layer {network_layer_name}: {e}\")\n                    extra_network_lora.errors[net.name] = extra_network_lora.errors.get(net.name, 0) + 1\n\n                continue\n\n            module_q = net.modules.get(network_layer_name + \"_q_proj\", None)\n            module_k = net.modules.get(network_layer_name + \"_k_proj\", None)\n            module_v = net.modules.get(network_layer_name + \"_v_proj\", None)\n            module_out = net.modules.get(network_layer_name + \"_out_proj\", None)\n\n            if isinstance(self, torch.nn.MultiheadAttention) and module_q and module_k and module_v and module_out:\n                try:\n                    with torch.no_grad():\n                        # Send \"real\" orig_weight into MHA's lora module\n                        qw, kw, vw = self.in_proj_weight.chunk(3, 0)\n                        updown_q, _ = module_q.calc_updown(qw)\n                        updown_k, _ = module_k.calc_updown(kw)\n                        updown_v, _ = module_v.calc_updown(vw)\n                        del qw, kw, vw\n                        updown_qkv = torch.vstack([updown_q, updown_k, updown_v])\n                        updown_out, ex_bias = module_out.calc_updown(self.out_proj.weight)\n\n                        self.in_proj_weight += updown_qkv\n                        self.out_proj.weight += updown_out\n                    if ex_bias is not None:\n                        if self.out_proj.bias is None:\n                            self.out_proj.bias = torch.nn.Parameter(ex_bias)\n                        else:\n                            self.out_proj.bias += ex_bias\n\n                except RuntimeError as e:\n                    logging.debug(f\"Network {net.name} layer {network_layer_name}: {e}\")\n                    extra_network_lora.errors[net.name] = extra_network_lora.errors.get(net.name, 0) + 1\n\n                continue\n\n            if module is None:\n                continue\n\n            logging.debug(f\"Network {net.name} layer {network_layer_name}: couldn't find supported operation\")\n            extra_network_lora.errors[net.name] = extra_network_lora.errors.get(net.name, 0) + 1\n\n        self.network_current_names = wanted_names\n\n\ndef network_forward(org_module, input, original_forward):\n    \"\"\"\n    Old way of applying Lora by executing operations during layer's forward.\n    Stacking many loras this way results in big performance degradation.\n    \"\"\"\n\n    if len(loaded_networks) == 0:\n        return original_forward(org_module, input)\n\n    input = devices.cond_cast_unet(input)\n\n    network_restore_weights_from_backup(org_module)\n    network_reset_cached_weight(org_module)\n\n    y = original_forward(org_module, input)\n\n    network_layer_name = getattr(org_module, 'network_layer_name', None)\n    for lora in loaded_networks:\n        module = lora.modules.get(network_layer_name, None)\n        if module is None:\n            continue\n\n        y = module.forward(input, y)\n\n    return y\n\n\ndef network_reset_cached_weight(self: Union[torch.nn.Conv2d, torch.nn.Linear]):\n    self.network_current_names = ()\n    self.network_weights_backup = None\n    self.network_bias_backup = None\n\n\ndef network_Linear_forward(self, input):\n    if shared.opts.lora_functional:\n        return network_forward(self, input, originals.Linear_forward)\n\n    network_apply_weights(self)\n\n    return originals.Linear_forward(self, input)\n\n\ndef network_Linear_load_state_dict(self, *args, **kwargs):\n    network_reset_cached_weight(self)\n\n    return originals.Linear_load_state_dict(self, *args, **kwargs)\n\n\ndef network_Conv2d_forward(self, input):\n    if shared.opts.lora_functional:\n        return network_forward(self, input, originals.Conv2d_forward)\n\n    network_apply_weights(self)\n\n    return originals.Conv2d_forward(self, input)\n\n\ndef network_Conv2d_load_state_dict(self, *args, **kwargs):\n    network_reset_cached_weight(self)\n\n    return originals.Conv2d_load_state_dict(self, *args, **kwargs)\n\n\ndef network_GroupNorm_forward(self, input):\n    if shared.opts.lora_functional:\n        return network_forward(self, input, originals.GroupNorm_forward)\n\n    network_apply_weights(self)\n\n    return originals.GroupNorm_forward(self, input)\n\n\ndef network_GroupNorm_load_state_dict(self, *args, **kwargs):\n    network_reset_cached_weight(self)\n\n    return originals.GroupNorm_load_state_dict(self, *args, **kwargs)\n\n\ndef network_LayerNorm_forward(self, input):\n    if shared.opts.lora_functional:\n        return network_forward(self, input, originals.LayerNorm_forward)\n\n    network_apply_weights(self)\n\n    return originals.LayerNorm_forward(self, input)\n\n\ndef network_LayerNorm_load_state_dict(self, *args, **kwargs):\n    network_reset_cached_weight(self)\n\n    return originals.LayerNorm_load_state_dict(self, *args, **kwargs)\n\n\ndef network_MultiheadAttention_forward(self, *args, **kwargs):\n    network_apply_weights(self)\n\n    return originals.MultiheadAttention_forward(self, *args, **kwargs)\n\n\ndef network_MultiheadAttention_load_state_dict(self, *args, **kwargs):\n    network_reset_cached_weight(self)\n\n    return originals.MultiheadAttention_load_state_dict(self, *args, **kwargs)\n\n\ndef list_available_networks():\n    available_networks.clear()\n    available_network_aliases.clear()\n    forbidden_network_aliases.clear()\n    available_network_hash_lookup.clear()\n    forbidden_network_aliases.update({\"none\": 1, \"Addams\": 1})\n\n    os.makedirs(shared.cmd_opts.lora_dir, exist_ok=True)\n\n    candidates = list(shared.walk_files(shared.cmd_opts.lora_dir, allowed_extensions=[\".pt\", \".ckpt\", \".safetensors\"]))\n    candidates += list(shared.walk_files(shared.cmd_opts.lyco_dir_backcompat, allowed_extensions=[\".pt\", \".ckpt\", \".safetensors\"]))\n    for filename in candidates:\n        if os.path.isdir(filename):\n            continue\n\n        name = os.path.splitext(os.path.basename(filename))[0]\n        try:\n            entry = network.NetworkOnDisk(name, filename)\n        except OSError:  # should catch FileNotFoundError and PermissionError etc.\n            errors.report(f\"Failed to load network {name} from {filename}\", exc_info=True)\n            continue\n\n        available_networks[name] = entry\n\n        if entry.alias in available_network_aliases:\n            forbidden_network_aliases[entry.alias.lower()] = 1\n\n        available_network_aliases[name] = entry\n        available_network_aliases[entry.alias] = entry\n\n\nre_network_name = re.compile(r\"(.*)\\s*\\([0-9a-fA-F]+\\)\")\n\n\ndef infotext_pasted(infotext, params):\n    if \"AddNet Module 1\" in [x[1] for x in scripts.scripts_txt2img.infotext_fields]:\n        return  # if the other extension is active, it will handle those fields, no need to do anything\n\n    added = []\n\n    for k in params:\n        if not k.startswith(\"AddNet Model \"):\n            continue\n\n        num = k[13:]\n\n        if params.get(\"AddNet Module \" + num) != \"LoRA\":\n            continue\n\n        name = params.get(\"AddNet Model \" + num)\n        if name is None:\n            continue\n\n        m = re_network_name.match(name)\n        if m:\n            name = m.group(1)\n\n        multiplier = params.get(\"AddNet Weight A \" + num, \"1.0\")\n\n        added.append(f\"<lora:{name}:{multiplier}>\")\n\n    if added:\n        params[\"Prompt\"] += \"\\n\" + \"\".join(added)\n\n\noriginals: lora_patches.LoraPatches = None\n\nextra_network_lora = None\n\navailable_networks = {}\navailable_network_aliases = {}\nloaded_networks = []\nloaded_bundle_embeddings = {}\nnetworks_in_memory = {}\navailable_network_hash_lookup = {}\nforbidden_network_aliases = {}\n\nlist_available_networks()\n", "extensions-builtin/Lora/extra_networks_lora.py": "from modules import extra_networks, shared\nimport networks\n\n\nclass ExtraNetworkLora(extra_networks.ExtraNetwork):\n    def __init__(self):\n        super().__init__('lora')\n\n        self.errors = {}\n        \"\"\"mapping of network names to the number of errors the network had during operation\"\"\"\n\n    def activate(self, p, params_list):\n        additional = shared.opts.sd_lora\n\n        self.errors.clear()\n\n        if additional != \"None\" and additional in networks.available_networks and not any(x for x in params_list if x.items[0] == additional):\n            p.all_prompts = [x + f\"<lora:{additional}:{shared.opts.extra_networks_default_multiplier}>\" for x in p.all_prompts]\n            params_list.append(extra_networks.ExtraNetworkParams(items=[additional, shared.opts.extra_networks_default_multiplier]))\n\n        names = []\n        te_multipliers = []\n        unet_multipliers = []\n        dyn_dims = []\n        for params in params_list:\n            assert params.items\n\n            names.append(params.positional[0])\n\n            te_multiplier = float(params.positional[1]) if len(params.positional) > 1 else 1.0\n            te_multiplier = float(params.named.get(\"te\", te_multiplier))\n\n            unet_multiplier = float(params.positional[2]) if len(params.positional) > 2 else te_multiplier\n            unet_multiplier = float(params.named.get(\"unet\", unet_multiplier))\n\n            dyn_dim = int(params.positional[3]) if len(params.positional) > 3 else None\n            dyn_dim = int(params.named[\"dyn\"]) if \"dyn\" in params.named else dyn_dim\n\n            te_multipliers.append(te_multiplier)\n            unet_multipliers.append(unet_multiplier)\n            dyn_dims.append(dyn_dim)\n\n        networks.load_networks(names, te_multipliers, unet_multipliers, dyn_dims)\n\n        if shared.opts.lora_add_hashes_to_infotext:\n            network_hashes = []\n            for item in networks.loaded_networks:\n                shorthash = item.network_on_disk.shorthash\n                if not shorthash:\n                    continue\n\n                alias = item.mentioned_name\n                if not alias:\n                    continue\n\n                alias = alias.replace(\":\", \"\").replace(\",\", \"\")\n\n                network_hashes.append(f\"{alias}: {shorthash}\")\n\n            if network_hashes:\n                p.extra_generation_params[\"Lora hashes\"] = \", \".join(network_hashes)\n\n    def deactivate(self, p):\n        if self.errors:\n            p.comment(\"Networks with errors: \" + \", \".join(f\"{k} ({v})\" for k, v in self.errors.items()))\n\n            self.errors.clear()\n", "extensions-builtin/Lora/network.py": "from __future__ import annotations\nimport os\nfrom collections import namedtuple\nimport enum\n\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom modules import sd_models, cache, errors, hashes, shared\n\nNetworkWeights = namedtuple('NetworkWeights', ['network_key', 'sd_key', 'w', 'sd_module'])\n\nmetadata_tags_order = {\"ss_sd_model_name\": 1, \"ss_resolution\": 2, \"ss_clip_skip\": 3, \"ss_num_train_images\": 10, \"ss_tag_frequency\": 20}\n\n\nclass SdVersion(enum.Enum):\n    Unknown = 1\n    SD1 = 2\n    SD2 = 3\n    SDXL = 4\n\n\nclass NetworkOnDisk:\n    def __init__(self, name, filename):\n        self.name = name\n        self.filename = filename\n        self.metadata = {}\n        self.is_safetensors = os.path.splitext(filename)[1].lower() == \".safetensors\"\n\n        def read_metadata():\n            metadata = sd_models.read_metadata_from_safetensors(filename)\n\n            return metadata\n\n        if self.is_safetensors:\n            try:\n                self.metadata = cache.cached_data_for_file('safetensors-metadata', \"lora/\" + self.name, filename, read_metadata)\n            except Exception as e:\n                errors.display(e, f\"reading lora {filename}\")\n\n        if self.metadata:\n            m = {}\n            for k, v in sorted(self.metadata.items(), key=lambda x: metadata_tags_order.get(x[0], 999)):\n                m[k] = v\n\n            self.metadata = m\n\n        self.alias = self.metadata.get('ss_output_name', self.name)\n\n        self.hash = None\n        self.shorthash = None\n        self.set_hash(\n            self.metadata.get('sshs_model_hash') or\n            hashes.sha256_from_cache(self.filename, \"lora/\" + self.name, use_addnet_hash=self.is_safetensors) or\n            ''\n        )\n\n        self.sd_version = self.detect_version()\n\n    def detect_version(self):\n        if str(self.metadata.get('ss_base_model_version', \"\")).startswith(\"sdxl_\"):\n            return SdVersion.SDXL\n        elif str(self.metadata.get('ss_v2', \"\")) == \"True\":\n            return SdVersion.SD2\n        elif len(self.metadata):\n            return SdVersion.SD1\n\n        return SdVersion.Unknown\n\n    def set_hash(self, v):\n        self.hash = v\n        self.shorthash = self.hash[0:12]\n\n        if self.shorthash:\n            import networks\n            networks.available_network_hash_lookup[self.shorthash] = self\n\n    def read_hash(self):\n        if not self.hash:\n            self.set_hash(hashes.sha256(self.filename, \"lora/\" + self.name, use_addnet_hash=self.is_safetensors) or '')\n\n    def get_alias(self):\n        import networks\n        if shared.opts.lora_preferred_name == \"Filename\" or self.alias.lower() in networks.forbidden_network_aliases:\n            return self.name\n        else:\n            return self.alias\n\n\nclass Network:  # LoraModule\n    def __init__(self, name, network_on_disk: NetworkOnDisk):\n        self.name = name\n        self.network_on_disk = network_on_disk\n        self.te_multiplier = 1.0\n        self.unet_multiplier = 1.0\n        self.dyn_dim = None\n        self.modules = {}\n        self.bundle_embeddings = {}\n        self.mtime = None\n\n        self.mentioned_name = None\n        \"\"\"the text that was used to add the network to prompt - can be either name or an alias\"\"\"\n\n\nclass ModuleType:\n    def create_module(self, net: Network, weights: NetworkWeights) -> Network | None:\n        return None\n\n\nclass NetworkModule:\n    def __init__(self, net: Network, weights: NetworkWeights):\n        self.network = net\n        self.network_key = weights.network_key\n        self.sd_key = weights.sd_key\n        self.sd_module = weights.sd_module\n\n        if hasattr(self.sd_module, 'weight'):\n            self.shape = self.sd_module.weight.shape\n        elif isinstance(self.sd_module, nn.MultiheadAttention):\n            # For now, only self-attn use Pytorch's MHA\n            # So assume all qkvo proj have same shape\n            self.shape = self.sd_module.out_proj.weight.shape\n        else:\n            self.shape = None\n\n        self.ops = None\n        self.extra_kwargs = {}\n        if isinstance(self.sd_module, nn.Conv2d):\n            self.ops = F.conv2d\n            self.extra_kwargs = {\n                'stride': self.sd_module.stride,\n                'padding': self.sd_module.padding\n            }\n        elif isinstance(self.sd_module, nn.Linear):\n            self.ops = F.linear\n        elif isinstance(self.sd_module, nn.LayerNorm):\n            self.ops = F.layer_norm\n            self.extra_kwargs = {\n                'normalized_shape': self.sd_module.normalized_shape,\n                'eps': self.sd_module.eps\n            }\n        elif isinstance(self.sd_module, nn.GroupNorm):\n            self.ops = F.group_norm\n            self.extra_kwargs = {\n                'num_groups': self.sd_module.num_groups,\n                'eps': self.sd_module.eps\n            }\n\n        self.dim = None\n        self.bias = weights.w.get(\"bias\")\n        self.alpha = weights.w[\"alpha\"].item() if \"alpha\" in weights.w else None\n        self.scale = weights.w[\"scale\"].item() if \"scale\" in weights.w else None\n\n        self.dora_scale = weights.w.get(\"dora_scale\", None)\n        self.dora_norm_dims = len(self.shape) - 1\n\n    def multiplier(self):\n        if 'transformer' in self.sd_key[:20]:\n            return self.network.te_multiplier\n        else:\n            return self.network.unet_multiplier\n\n    def calc_scale(self):\n        if self.scale is not None:\n            return self.scale\n        if self.dim is not None and self.alpha is not None:\n            return self.alpha / self.dim\n\n        return 1.0\n\n    def apply_weight_decompose(self, updown, orig_weight):\n        # Match the device/dtype\n        orig_weight = orig_weight.to(updown.dtype)\n        dora_scale = self.dora_scale.to(device=orig_weight.device, dtype=updown.dtype)\n        updown = updown.to(orig_weight.device)\n\n        merged_scale1 = updown + orig_weight\n        merged_scale1_norm = (\n            merged_scale1.transpose(0, 1)\n            .reshape(merged_scale1.shape[1], -1)\n            .norm(dim=1, keepdim=True)\n            .reshape(merged_scale1.shape[1], *[1] * self.dora_norm_dims)\n            .transpose(0, 1)\n        )\n\n        dora_merged = (\n            merged_scale1 * (dora_scale / merged_scale1_norm)\n        )\n        final_updown = dora_merged - orig_weight\n        return final_updown\n\n    def finalize_updown(self, updown, orig_weight, output_shape, ex_bias=None):\n        if self.bias is not None:\n            updown = updown.reshape(self.bias.shape)\n            updown += self.bias.to(orig_weight.device, dtype=updown.dtype)\n            updown = updown.reshape(output_shape)\n\n        if len(output_shape) == 4:\n            updown = updown.reshape(output_shape)\n\n        if orig_weight.size().numel() == updown.size().numel():\n            updown = updown.reshape(orig_weight.shape)\n\n        if ex_bias is not None:\n            ex_bias = ex_bias * self.multiplier()\n\n        if self.dora_scale is not None:\n            updown = self.apply_weight_decompose(updown, orig_weight)\n\n        return updown * self.calc_scale() * self.multiplier(), ex_bias\n\n    def calc_updown(self, target):\n        raise NotImplementedError()\n\n    def forward(self, x, y):\n        \"\"\"A general forward implementation for all modules\"\"\"\n        if self.ops is None:\n            raise NotImplementedError()\n        else:\n            updown, ex_bias = self.calc_updown(self.sd_module.weight)\n            return y + self.ops(x, weight=updown, bias=ex_bias, **self.extra_kwargs)\n\n", "extensions-builtin/Lora/network_lokr.py": "import torch\n\nimport lyco_helpers\nimport network\n\n\nclass ModuleTypeLokr(network.ModuleType):\n    def create_module(self, net: network.Network, weights: network.NetworkWeights):\n        has_1 = \"lokr_w1\" in weights.w or (\"lokr_w1_a\" in weights.w and \"lokr_w1_b\" in weights.w)\n        has_2 = \"lokr_w2\" in weights.w or (\"lokr_w2_a\" in weights.w and \"lokr_w2_b\" in weights.w)\n        if has_1 and has_2:\n            return NetworkModuleLokr(net, weights)\n\n        return None\n\n\ndef make_kron(orig_shape, w1, w2):\n    if len(w2.shape) == 4:\n        w1 = w1.unsqueeze(2).unsqueeze(2)\n    w2 = w2.contiguous()\n    return torch.kron(w1, w2).reshape(orig_shape)\n\n\nclass NetworkModuleLokr(network.NetworkModule):\n    def __init__(self,  net: network.Network, weights: network.NetworkWeights):\n        super().__init__(net, weights)\n\n        self.w1 = weights.w.get(\"lokr_w1\")\n        self.w1a = weights.w.get(\"lokr_w1_a\")\n        self.w1b = weights.w.get(\"lokr_w1_b\")\n        self.dim = self.w1b.shape[0] if self.w1b is not None else self.dim\n        self.w2 = weights.w.get(\"lokr_w2\")\n        self.w2a = weights.w.get(\"lokr_w2_a\")\n        self.w2b = weights.w.get(\"lokr_w2_b\")\n        self.dim = self.w2b.shape[0] if self.w2b is not None else self.dim\n        self.t2 = weights.w.get(\"lokr_t2\")\n\n    def calc_updown(self, orig_weight):\n        if self.w1 is not None:\n            w1 = self.w1.to(orig_weight.device)\n        else:\n            w1a = self.w1a.to(orig_weight.device)\n            w1b = self.w1b.to(orig_weight.device)\n            w1 = w1a @ w1b\n\n        if self.w2 is not None:\n            w2 = self.w2.to(orig_weight.device)\n        elif self.t2 is None:\n            w2a = self.w2a.to(orig_weight.device)\n            w2b = self.w2b.to(orig_weight.device)\n            w2 = w2a @ w2b\n        else:\n            t2 = self.t2.to(orig_weight.device)\n            w2a = self.w2a.to(orig_weight.device)\n            w2b = self.w2b.to(orig_weight.device)\n            w2 = lyco_helpers.make_weight_cp(t2, w2a, w2b)\n\n        output_shape = [w1.size(0) * w2.size(0), w1.size(1) * w2.size(1)]\n        if len(orig_weight.shape) == 4:\n            output_shape = orig_weight.shape\n\n        updown = make_kron(output_shape, w1, w2)\n\n        return self.finalize_updown(updown, orig_weight, output_shape)\n", "extensions-builtin/Lora/network_oft.py": "import torch\nimport network\nfrom einops import rearrange\n\n\nclass ModuleTypeOFT(network.ModuleType):\n    def create_module(self, net: network.Network, weights: network.NetworkWeights):\n        if all(x in weights.w for x in [\"oft_blocks\"]) or all(x in weights.w for x in [\"oft_diag\"]):\n            return NetworkModuleOFT(net, weights)\n\n        return None\n\n# Supports both kohya-ss' implementation of COFT  https://github.com/kohya-ss/sd-scripts/blob/main/networks/oft.py\n# and KohakuBlueleaf's implementation of OFT/COFT https://github.com/KohakuBlueleaf/LyCORIS/blob/dev/lycoris/modules/diag_oft.py\nclass NetworkModuleOFT(network.NetworkModule):\n    def __init__(self,  net: network.Network, weights: network.NetworkWeights):\n\n        super().__init__(net, weights)\n\n        self.lin_module = None\n        self.org_module: list[torch.Module] = [self.sd_module]\n\n        self.scale = 1.0\n        self.is_R = False\n        self.is_boft = False\n\n        # kohya-ss/New LyCORIS OFT/BOFT\n        if \"oft_blocks\" in weights.w.keys():\n            self.oft_blocks = weights.w[\"oft_blocks\"] # (num_blocks, block_size, block_size)\n            self.alpha = weights.w.get(\"alpha\", None) # alpha is constraint\n            self.dim = self.oft_blocks.shape[0] # lora dim\n        # Old LyCORIS OFT\n        elif \"oft_diag\" in weights.w.keys():\n            self.is_R = True\n            self.oft_blocks = weights.w[\"oft_diag\"]\n            # self.alpha is unused\n            self.dim = self.oft_blocks.shape[1] # (num_blocks, block_size, block_size)\n\n        is_linear = type(self.sd_module) in [torch.nn.Linear, torch.nn.modules.linear.NonDynamicallyQuantizableLinear]\n        is_conv = type(self.sd_module) in [torch.nn.Conv2d]\n        is_other_linear = type(self.sd_module) in [torch.nn.MultiheadAttention] # unsupported\n\n        if is_linear:\n            self.out_dim = self.sd_module.out_features\n        elif is_conv:\n            self.out_dim = self.sd_module.out_channels\n        elif is_other_linear:\n            self.out_dim = self.sd_module.embed_dim\n\n        # LyCORIS BOFT\n        if self.oft_blocks.dim() == 4:\n            self.is_boft = True\n        self.rescale = weights.w.get('rescale', None)\n        if self.rescale is not None and not is_other_linear:\n            self.rescale = self.rescale.reshape(-1, *[1]*(self.org_module[0].weight.dim() - 1))\n\n        self.num_blocks = self.dim\n        self.block_size = self.out_dim // self.dim\n        self.constraint = (0 if self.alpha is None else self.alpha) * self.out_dim\n        if self.is_R:\n            self.constraint = None\n            self.block_size = self.dim\n            self.num_blocks = self.out_dim // self.dim\n        elif self.is_boft:\n            self.boft_m = self.oft_blocks.shape[0]\n            self.num_blocks = self.oft_blocks.shape[1]\n            self.block_size = self.oft_blocks.shape[2]\n            self.boft_b = self.block_size\n\n    def calc_updown(self, orig_weight):\n        oft_blocks = self.oft_blocks.to(orig_weight.device)\n        eye = torch.eye(self.block_size, device=oft_blocks.device)\n\n        if not self.is_R:\n            block_Q = oft_blocks - oft_blocks.transpose(-1, -2) # ensure skew-symmetric orthogonal matrix\n            if self.constraint != 0:\n                norm_Q = torch.norm(block_Q.flatten())\n                new_norm_Q = torch.clamp(norm_Q, max=self.constraint.to(oft_blocks.device))\n                block_Q = block_Q * ((new_norm_Q + 1e-8) / (norm_Q + 1e-8))\n            oft_blocks = torch.matmul(eye + block_Q, (eye - block_Q).float().inverse())\n\n        R = oft_blocks.to(orig_weight.device)\n\n        if not self.is_boft:\n            # This errors out for MultiheadAttention, might need to be handled up-stream\n            merged_weight = rearrange(orig_weight, '(k n) ... -> k n ...', k=self.num_blocks, n=self.block_size)\n            merged_weight = torch.einsum(\n                'k n m, k n ... -> k m ...',\n                R,\n                merged_weight\n            )\n            merged_weight = rearrange(merged_weight, 'k m ... -> (k m) ...')\n        else:\n            # TODO: determine correct value for scale\n            scale = 1.0\n            m = self.boft_m\n            b = self.boft_b\n            r_b = b // 2\n            inp = orig_weight\n            for i in range(m):\n                bi = R[i] # b_num, b_size, b_size\n                if i == 0:\n                    # Apply multiplier/scale and rescale into first weight\n                    bi = bi * scale + (1 - scale) * eye\n                inp = rearrange(inp, \"(c g k) ... -> (c k g) ...\", g=2, k=2**i * r_b)\n                inp = rearrange(inp, \"(d b) ... -> d b ...\", b=b)\n                inp = torch.einsum(\"b i j, b j ... -> b i ...\", bi, inp)\n                inp = rearrange(inp, \"d b ... -> (d b) ...\")\n                inp = rearrange(inp, \"(c k g) ... -> (c g k) ...\", g=2, k=2**i * r_b)\n            merged_weight = inp\n\n        # Rescale mechanism\n        if self.rescale is not None:\n            merged_weight = self.rescale.to(merged_weight) * merged_weight\n\n        updown = merged_weight.to(orig_weight.device) - orig_weight.to(merged_weight.dtype)\n        output_shape = orig_weight.shape\n        return self.finalize_updown(updown, orig_weight, output_shape)\n", "extensions-builtin/Lora/scripts/lora_script.py": "import re\n\nimport gradio as gr\nfrom fastapi import FastAPI\n\nimport network\nimport networks\nimport lora  # noqa:F401\nimport lora_patches\nimport extra_networks_lora\nimport ui_extra_networks_lora\nfrom modules import script_callbacks, ui_extra_networks, extra_networks, shared\n\n\ndef unload():\n    networks.originals.undo()\n\n\ndef before_ui():\n    ui_extra_networks.register_page(ui_extra_networks_lora.ExtraNetworksPageLora())\n\n    networks.extra_network_lora = extra_networks_lora.ExtraNetworkLora()\n    extra_networks.register_extra_network(networks.extra_network_lora)\n    extra_networks.register_extra_network_alias(networks.extra_network_lora, \"lyco\")\n\n\nnetworks.originals = lora_patches.LoraPatches()\n\nscript_callbacks.on_model_loaded(networks.assign_network_names_to_compvis_modules)\nscript_callbacks.on_script_unloaded(unload)\nscript_callbacks.on_before_ui(before_ui)\nscript_callbacks.on_infotext_pasted(networks.infotext_pasted)\n\n\nshared.options_templates.update(shared.options_section(('extra_networks', \"Extra Networks\"), {\n    \"sd_lora\": shared.OptionInfo(\"None\", \"Add network to prompt\", gr.Dropdown, lambda: {\"choices\": [\"None\", *networks.available_networks]}, refresh=networks.list_available_networks),\n    \"lora_preferred_name\": shared.OptionInfo(\"Alias from file\", \"When adding to prompt, refer to Lora by\", gr.Radio, {\"choices\": [\"Alias from file\", \"Filename\"]}),\n    \"lora_add_hashes_to_infotext\": shared.OptionInfo(True, \"Add Lora hashes to infotext\"),\n    \"lora_show_all\": shared.OptionInfo(False, \"Always show all networks on the Lora page\").info(\"otherwise, those detected as for incompatible version of Stable Diffusion will be hidden\"),\n    \"lora_hide_unknown_for_versions\": shared.OptionInfo([], \"Hide networks of unknown versions for model versions\", gr.CheckboxGroup, {\"choices\": [\"SD1\", \"SD2\", \"SDXL\"]}),\n    \"lora_in_memory_limit\": shared.OptionInfo(0, \"Number of Lora networks to keep cached in memory\", gr.Number, {\"precision\": 0}),\n    \"lora_not_found_warning_console\": shared.OptionInfo(False, \"Lora not found warning in console\"),\n    \"lora_not_found_gradio_warning\": shared.OptionInfo(False, \"Lora not found warning popup in webui\"),\n}))\n\n\nshared.options_templates.update(shared.options_section(('compatibility', \"Compatibility\"), {\n    \"lora_functional\": shared.OptionInfo(False, \"Lora/Networks: use old method that takes longer when you have multiple Loras active and produces same results as kohya-ss/sd-webui-additional-networks extension\"),\n}))\n\n\ndef create_lora_json(obj: network.NetworkOnDisk):\n    return {\n        \"name\": obj.name,\n        \"alias\": obj.alias,\n        \"path\": obj.filename,\n        \"metadata\": obj.metadata,\n    }\n\n\ndef api_networks(_: gr.Blocks, app: FastAPI):\n    @app.get(\"/sdapi/v1/loras\")\n    async def get_loras():\n        return [create_lora_json(obj) for obj in networks.available_networks.values()]\n\n    @app.post(\"/sdapi/v1/refresh-loras\")\n    async def refresh_loras():\n        return networks.list_available_networks()\n\n\nscript_callbacks.on_app_started(api_networks)\n\nre_lora = re.compile(\"<lora:([^:]+):\")\n\n\ndef infotext_pasted(infotext, d):\n    hashes = d.get(\"Lora hashes\")\n    if not hashes:\n        return\n\n    hashes = [x.strip().split(':', 1) for x in hashes.split(\",\")]\n    hashes = {x[0].strip().replace(\",\", \"\"): x[1].strip() for x in hashes}\n\n    def network_replacement(m):\n        alias = m.group(1)\n        shorthash = hashes.get(alias)\n        if shorthash is None:\n            return m.group(0)\n\n        network_on_disk = networks.available_network_hash_lookup.get(shorthash)\n        if network_on_disk is None:\n            return m.group(0)\n\n        return f'<lora:{network_on_disk.get_alias()}:'\n\n    d[\"Prompt\"] = re.sub(re_lora, network_replacement, d[\"Prompt\"])\n\n\nscript_callbacks.on_infotext_pasted(infotext_pasted)\n\nshared.opts.onchange(\"lora_in_memory_limit\", networks.purge_networks_from_memory)\n", "extensions-builtin/SwinIR/preload.py": "import os\nfrom modules import paths\n\n\ndef preload(parser):\n    parser.add_argument(\"--swinir-models-path\", type=str, help=\"Path to directory with SwinIR model file(s).\", default=os.path.join(paths.models_path, 'SwinIR'))\n", "extensions-builtin/SwinIR/scripts/swinir_model.py": "import logging\nimport sys\n\nimport torch\nfrom PIL import Image\n\nfrom modules import devices, modelloader, script_callbacks, shared, upscaler_utils\nfrom modules.upscaler import Upscaler, UpscalerData\n\nSWINIR_MODEL_URL = \"https://github.com/JingyunLiang/SwinIR/releases/download/v0.0/003_realSR_BSRGAN_DFOWMFC_s64w8_SwinIR-L_x4_GAN.pth\"\n\nlogger = logging.getLogger(__name__)\n\n\nclass UpscalerSwinIR(Upscaler):\n    def __init__(self, dirname):\n        self._cached_model = None           # keep the model when SWIN_torch_compile is on to prevent re-compile every runs\n        self._cached_model_config = None    # to clear '_cached_model' when changing model (v1/v2) or settings\n        self.name = \"SwinIR\"\n        self.model_url = SWINIR_MODEL_URL\n        self.model_name = \"SwinIR 4x\"\n        self.user_path = dirname\n        super().__init__()\n        scalers = []\n        model_files = self.find_models(ext_filter=[\".pt\", \".pth\"])\n        for model in model_files:\n            if model.startswith(\"http\"):\n                name = self.model_name\n            else:\n                name = modelloader.friendly_name(model)\n            model_data = UpscalerData(name, model, self)\n            scalers.append(model_data)\n        self.scalers = scalers\n\n    def do_upscale(self, img: Image.Image, model_file: str) -> Image.Image:\n        current_config = (model_file, shared.opts.SWIN_tile)\n\n        if self._cached_model_config == current_config:\n            model = self._cached_model\n        else:\n            try:\n                model = self.load_model(model_file)\n            except Exception as e:\n                print(f\"Failed loading SwinIR model {model_file}: {e}\", file=sys.stderr)\n                return img\n            self._cached_model = model\n            self._cached_model_config = current_config\n\n        img = upscaler_utils.upscale_2(\n            img,\n            model,\n            tile_size=shared.opts.SWIN_tile,\n            tile_overlap=shared.opts.SWIN_tile_overlap,\n            scale=model.scale,\n            desc=\"SwinIR\",\n        )\n        devices.torch_gc()\n        return img\n\n    def load_model(self, path, scale=4):\n        if path.startswith(\"http\"):\n            filename = modelloader.load_file_from_url(\n                url=path,\n                model_dir=self.model_download_path,\n                file_name=f\"{self.model_name.replace(' ', '_')}.pth\",\n            )\n        else:\n            filename = path\n\n        model_descriptor = modelloader.load_spandrel_model(\n            filename,\n            device=self._get_device(),\n            prefer_half=(devices.dtype == torch.float16),\n            expected_architecture=\"SwinIR\",\n        )\n        if getattr(shared.opts, 'SWIN_torch_compile', False):\n            try:\n                model_descriptor.model.compile()\n            except Exception:\n                logger.warning(\"Failed to compile SwinIR model, fallback to JIT\", exc_info=True)\n        return model_descriptor\n\n    def _get_device(self):\n        return devices.get_device_for('swinir')\n\n\ndef on_ui_settings():\n    import gradio as gr\n\n    shared.opts.add_option(\"SWIN_tile\", shared.OptionInfo(192, \"Tile size for all SwinIR.\", gr.Slider, {\"minimum\": 16, \"maximum\": 512, \"step\": 16}, section=('upscaling', \"Upscaling\")))\n    shared.opts.add_option(\"SWIN_tile_overlap\", shared.OptionInfo(8, \"Tile overlap, in pixels for SwinIR. Low values = visible seam.\", gr.Slider, {\"minimum\": 0, \"maximum\": 48, \"step\": 1}, section=('upscaling', \"Upscaling\")))\n    shared.opts.add_option(\"SWIN_torch_compile\", shared.OptionInfo(False, \"Use torch.compile to accelerate SwinIR.\", gr.Checkbox, {\"interactive\": True}, section=('upscaling', \"Upscaling\")).info(\"Takes longer on first run\"))\n\n\nscript_callbacks.on_ui_settings(on_ui_settings)\n", "extensions-builtin/extra-options-section/scripts/extra_options_section.py": "import math\n\nimport gradio as gr\nfrom modules import scripts, shared, ui_components, ui_settings, infotext_utils, errors\nfrom modules.ui_components import FormColumn\n\n\nclass ExtraOptionsSection(scripts.Script):\n    section = \"extra_options\"\n\n    def __init__(self):\n        self.comps = None\n        self.setting_names = None\n\n    def title(self):\n        return \"Extra options\"\n\n    def show(self, is_img2img):\n        return scripts.AlwaysVisible\n\n    def ui(self, is_img2img):\n        self.comps = []\n        self.setting_names = []\n        self.infotext_fields = []\n        extra_options = shared.opts.extra_options_img2img if is_img2img else shared.opts.extra_options_txt2img\n        elem_id_tabname = \"extra_options_\" + (\"img2img\" if is_img2img else \"txt2img\")\n\n        mapping = {k: v for v, k in infotext_utils.infotext_to_setting_name_mapping}\n\n        with gr.Blocks() as interface:\n            with gr.Accordion(\"Options\", open=False, elem_id=elem_id_tabname) if shared.opts.extra_options_accordion and extra_options else gr.Group(elem_id=elem_id_tabname):\n\n                row_count = math.ceil(len(extra_options) / shared.opts.extra_options_cols)\n\n                for row in range(row_count):\n                    with gr.Row():\n                        for col in range(shared.opts.extra_options_cols):\n                            index = row * shared.opts.extra_options_cols + col\n                            if index >= len(extra_options):\n                                break\n\n                            setting_name = extra_options[index]\n\n                            with FormColumn():\n                                try:\n                                    comp = ui_settings.create_setting_component(setting_name)\n                                except KeyError:\n                                    errors.report(f\"Can't add extra options for {setting_name} in ui\")\n                                    continue\n\n                            self.comps.append(comp)\n                            self.setting_names.append(setting_name)\n\n                            setting_infotext_name = mapping.get(setting_name)\n                            if setting_infotext_name is not None:\n                                self.infotext_fields.append((comp, setting_infotext_name))\n\n        def get_settings_values():\n            res = [ui_settings.get_value_for_setting(key) for key in self.setting_names]\n            return res[0] if len(res) == 1 else res\n\n        interface.load(fn=get_settings_values, inputs=[], outputs=self.comps, queue=False, show_progress=False)\n\n        return self.comps\n\n    def before_process(self, p, *args):\n        for name, value in zip(self.setting_names, args):\n            if name not in p.override_settings:\n                p.override_settings[name] = value\n\n\nshared.options_templates.update(shared.options_section(('settings_in_ui', \"Settings in UI\", \"ui\"), {\n    \"settings_in_ui\": shared.OptionHTML(\"\"\"\nThis page allows you to add some settings to the main interface of txt2img and img2img tabs.\n\"\"\"),\n    \"extra_options_txt2img\": shared.OptionInfo([], \"Settings for txt2img\", ui_components.DropdownMulti, lambda: {\"choices\": list(shared.opts.data_labels.keys())}).js(\"info\", \"settingsHintsShowQuicksettings\").info(\"setting entries that also appear in txt2img interfaces\").needs_reload_ui(),\n    \"extra_options_img2img\": shared.OptionInfo([], \"Settings for img2img\", ui_components.DropdownMulti, lambda: {\"choices\": list(shared.opts.data_labels.keys())}).js(\"info\", \"settingsHintsShowQuicksettings\").info(\"setting entries that also appear in img2img interfaces\").needs_reload_ui(),\n    \"extra_options_cols\": shared.OptionInfo(1, \"Number of columns for added settings\", gr.Slider, {\"step\": 1, \"minimum\": 1, \"maximum\": 20}).info(\"displayed amount will depend on the actual browser window width\").needs_reload_ui(),\n    \"extra_options_accordion\": shared.OptionInfo(False, \"Place added settings into an accordion\").needs_reload_ui()\n}))\n\n\n", "extensions-builtin/LDSR/sd_hijack_ddpm_v1.py": "# This script is copied from the compvis/stable-diffusion repo (aka the SD V1 repo)\n# Original filename: ldm/models/diffusion/ddpm.py\n# The purpose to reinstate the old DDPM logic which works with VQ, whereas the V2 one doesn't\n# Some models such as LDSR require VQ to work correctly\n# The classes are suffixed with \"V1\" and added back to the \"ldm.models.diffusion.ddpm\" module\n\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport pytorch_lightning as pl\nfrom torch.optim.lr_scheduler import LambdaLR\nfrom einops import rearrange, repeat\nfrom contextlib import contextmanager\nfrom functools import partial\nfrom tqdm import tqdm\nfrom torchvision.utils import make_grid\nfrom pytorch_lightning.utilities.distributed import rank_zero_only\n\nfrom ldm.util import log_txt_as_img, exists, default, ismap, isimage, mean_flat, count_params, instantiate_from_config\nfrom ldm.modules.ema import LitEma\nfrom ldm.modules.distributions.distributions import normal_kl, DiagonalGaussianDistribution\nfrom ldm.models.autoencoder import VQModelInterface, IdentityFirstStage, AutoencoderKL\nfrom ldm.modules.diffusionmodules.util import make_beta_schedule, extract_into_tensor, noise_like\nfrom ldm.models.diffusion.ddim import DDIMSampler\n\nimport ldm.models.diffusion.ddpm\n\n__conditioning_keys__ = {'concat': 'c_concat',\n                         'crossattn': 'c_crossattn',\n                         'adm': 'y'}\n\n\ndef disabled_train(self, mode=True):\n    \"\"\"Overwrite model.train with this function to make sure train/eval mode\n    does not change anymore.\"\"\"\n    return self\n\n\ndef uniform_on_device(r1, r2, shape, device):\n    return (r1 - r2) * torch.rand(*shape, device=device) + r2\n\n\nclass DDPMV1(pl.LightningModule):\n    # classic DDPM with Gaussian diffusion, in image space\n    def __init__(self,\n                 unet_config,\n                 timesteps=1000,\n                 beta_schedule=\"linear\",\n                 loss_type=\"l2\",\n                 ckpt_path=None,\n                 ignore_keys=None,\n                 load_only_unet=False,\n                 monitor=\"val/loss\",\n                 use_ema=True,\n                 first_stage_key=\"image\",\n                 image_size=256,\n                 channels=3,\n                 log_every_t=100,\n                 clip_denoised=True,\n                 linear_start=1e-4,\n                 linear_end=2e-2,\n                 cosine_s=8e-3,\n                 given_betas=None,\n                 original_elbo_weight=0.,\n                 v_posterior=0.,  # weight for choosing posterior variance as sigma = (1-v) * beta_tilde + v * beta\n                 l_simple_weight=1.,\n                 conditioning_key=None,\n                 parameterization=\"eps\",  # all assuming fixed variance schedules\n                 scheduler_config=None,\n                 use_positional_encodings=False,\n                 learn_logvar=False,\n                 logvar_init=0.,\n                 ):\n        super().__init__()\n        assert parameterization in [\"eps\", \"x0\"], 'currently only supporting \"eps\" and \"x0\"'\n        self.parameterization = parameterization\n        print(f\"{self.__class__.__name__}: Running in {self.parameterization}-prediction mode\")\n        self.cond_stage_model = None\n        self.clip_denoised = clip_denoised\n        self.log_every_t = log_every_t\n        self.first_stage_key = first_stage_key\n        self.image_size = image_size  # try conv?\n        self.channels = channels\n        self.use_positional_encodings = use_positional_encodings\n        self.model = DiffusionWrapperV1(unet_config, conditioning_key)\n        count_params(self.model, verbose=True)\n        self.use_ema = use_ema\n        if self.use_ema:\n            self.model_ema = LitEma(self.model)\n            print(f\"Keeping EMAs of {len(list(self.model_ema.buffers()))}.\")\n\n        self.use_scheduler = scheduler_config is not None\n        if self.use_scheduler:\n            self.scheduler_config = scheduler_config\n\n        self.v_posterior = v_posterior\n        self.original_elbo_weight = original_elbo_weight\n        self.l_simple_weight = l_simple_weight\n\n        if monitor is not None:\n            self.monitor = monitor\n        if ckpt_path is not None:\n            self.init_from_ckpt(ckpt_path, ignore_keys=ignore_keys or [], only_model=load_only_unet)\n\n        self.register_schedule(given_betas=given_betas, beta_schedule=beta_schedule, timesteps=timesteps,\n                               linear_start=linear_start, linear_end=linear_end, cosine_s=cosine_s)\n\n        self.loss_type = loss_type\n\n        self.learn_logvar = learn_logvar\n        self.logvar = torch.full(fill_value=logvar_init, size=(self.num_timesteps,))\n        if self.learn_logvar:\n            self.logvar = nn.Parameter(self.logvar, requires_grad=True)\n\n\n    def register_schedule(self, given_betas=None, beta_schedule=\"linear\", timesteps=1000,\n                          linear_start=1e-4, linear_end=2e-2, cosine_s=8e-3):\n        if exists(given_betas):\n            betas = given_betas\n        else:\n            betas = make_beta_schedule(beta_schedule, timesteps, linear_start=linear_start, linear_end=linear_end,\n                                       cosine_s=cosine_s)\n        alphas = 1. - betas\n        alphas_cumprod = np.cumprod(alphas, axis=0)\n        alphas_cumprod_prev = np.append(1., alphas_cumprod[:-1])\n\n        timesteps, = betas.shape\n        self.num_timesteps = int(timesteps)\n        self.linear_start = linear_start\n        self.linear_end = linear_end\n        assert alphas_cumprod.shape[0] == self.num_timesteps, 'alphas have to be defined for each timestep'\n\n        to_torch = partial(torch.tensor, dtype=torch.float32)\n\n        self.register_buffer('betas', to_torch(betas))\n        self.register_buffer('alphas_cumprod', to_torch(alphas_cumprod))\n        self.register_buffer('alphas_cumprod_prev', to_torch(alphas_cumprod_prev))\n\n        # calculations for diffusion q(x_t | x_{t-1}) and others\n        self.register_buffer('sqrt_alphas_cumprod', to_torch(np.sqrt(alphas_cumprod)))\n        self.register_buffer('sqrt_one_minus_alphas_cumprod', to_torch(np.sqrt(1. - alphas_cumprod)))\n        self.register_buffer('log_one_minus_alphas_cumprod', to_torch(np.log(1. - alphas_cumprod)))\n        self.register_buffer('sqrt_recip_alphas_cumprod', to_torch(np.sqrt(1. / alphas_cumprod)))\n        self.register_buffer('sqrt_recipm1_alphas_cumprod', to_torch(np.sqrt(1. / alphas_cumprod - 1)))\n\n        # calculations for posterior q(x_{t-1} | x_t, x_0)\n        posterior_variance = (1 - self.v_posterior) * betas * (1. - alphas_cumprod_prev) / (\n                    1. - alphas_cumprod) + self.v_posterior * betas\n        # above: equal to 1. / (1. / (1. - alpha_cumprod_tm1) + alpha_t / beta_t)\n        self.register_buffer('posterior_variance', to_torch(posterior_variance))\n        # below: log calculation clipped because the posterior variance is 0 at the beginning of the diffusion chain\n        self.register_buffer('posterior_log_variance_clipped', to_torch(np.log(np.maximum(posterior_variance, 1e-20))))\n        self.register_buffer('posterior_mean_coef1', to_torch(\n            betas * np.sqrt(alphas_cumprod_prev) / (1. - alphas_cumprod)))\n        self.register_buffer('posterior_mean_coef2', to_torch(\n            (1. - alphas_cumprod_prev) * np.sqrt(alphas) / (1. - alphas_cumprod)))\n\n        if self.parameterization == \"eps\":\n            lvlb_weights = self.betas ** 2 / (\n                        2 * self.posterior_variance * to_torch(alphas) * (1 - self.alphas_cumprod))\n        elif self.parameterization == \"x0\":\n            lvlb_weights = 0.5 * np.sqrt(torch.Tensor(alphas_cumprod)) / (2. * 1 - torch.Tensor(alphas_cumprod))\n        else:\n            raise NotImplementedError(\"mu not supported\")\n        # TODO how to choose this term\n        lvlb_weights[0] = lvlb_weights[1]\n        self.register_buffer('lvlb_weights', lvlb_weights, persistent=False)\n        assert not torch.isnan(self.lvlb_weights).all()\n\n    @contextmanager\n    def ema_scope(self, context=None):\n        if self.use_ema:\n            self.model_ema.store(self.model.parameters())\n            self.model_ema.copy_to(self.model)\n            if context is not None:\n                print(f\"{context}: Switched to EMA weights\")\n        try:\n            yield None\n        finally:\n            if self.use_ema:\n                self.model_ema.restore(self.model.parameters())\n                if context is not None:\n                    print(f\"{context}: Restored training weights\")\n\n    def init_from_ckpt(self, path, ignore_keys=None, only_model=False):\n        sd = torch.load(path, map_location=\"cpu\")\n        if \"state_dict\" in list(sd.keys()):\n            sd = sd[\"state_dict\"]\n        keys = list(sd.keys())\n        for k in keys:\n            for ik in ignore_keys or []:\n                if k.startswith(ik):\n                    print(\"Deleting key {} from state_dict.\".format(k))\n                    del sd[k]\n        missing, unexpected = self.load_state_dict(sd, strict=False) if not only_model else self.model.load_state_dict(\n            sd, strict=False)\n        print(f\"Restored from {path} with {len(missing)} missing and {len(unexpected)} unexpected keys\")\n        if missing:\n            print(f\"Missing Keys: {missing}\")\n        if unexpected:\n            print(f\"Unexpected Keys: {unexpected}\")\n\n    def q_mean_variance(self, x_start, t):\n        \"\"\"\n        Get the distribution q(x_t | x_0).\n        :param x_start: the [N x C x ...] tensor of noiseless inputs.\n        :param t: the number of diffusion steps (minus 1). Here, 0 means one step.\n        :return: A tuple (mean, variance, log_variance), all of x_start's shape.\n        \"\"\"\n        mean = (extract_into_tensor(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start)\n        variance = extract_into_tensor(1.0 - self.alphas_cumprod, t, x_start.shape)\n        log_variance = extract_into_tensor(self.log_one_minus_alphas_cumprod, t, x_start.shape)\n        return mean, variance, log_variance\n\n    def predict_start_from_noise(self, x_t, t, noise):\n        return (\n                extract_into_tensor(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t -\n                extract_into_tensor(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape) * noise\n        )\n\n    def q_posterior(self, x_start, x_t, t):\n        posterior_mean = (\n                extract_into_tensor(self.posterior_mean_coef1, t, x_t.shape) * x_start +\n                extract_into_tensor(self.posterior_mean_coef2, t, x_t.shape) * x_t\n        )\n        posterior_variance = extract_into_tensor(self.posterior_variance, t, x_t.shape)\n        posterior_log_variance_clipped = extract_into_tensor(self.posterior_log_variance_clipped, t, x_t.shape)\n        return posterior_mean, posterior_variance, posterior_log_variance_clipped\n\n    def p_mean_variance(self, x, t, clip_denoised: bool):\n        model_out = self.model(x, t)\n        if self.parameterization == \"eps\":\n            x_recon = self.predict_start_from_noise(x, t=t, noise=model_out)\n        elif self.parameterization == \"x0\":\n            x_recon = model_out\n        if clip_denoised:\n            x_recon.clamp_(-1., 1.)\n\n        model_mean, posterior_variance, posterior_log_variance = self.q_posterior(x_start=x_recon, x_t=x, t=t)\n        return model_mean, posterior_variance, posterior_log_variance\n\n    @torch.no_grad()\n    def p_sample(self, x, t, clip_denoised=True, repeat_noise=False):\n        b, *_, device = *x.shape, x.device\n        model_mean, _, model_log_variance = self.p_mean_variance(x=x, t=t, clip_denoised=clip_denoised)\n        noise = noise_like(x.shape, device, repeat_noise)\n        # no noise when t == 0\n        nonzero_mask = (1 - (t == 0).float()).reshape(b, *((1,) * (len(x.shape) - 1)))\n        return model_mean + nonzero_mask * (0.5 * model_log_variance).exp() * noise\n\n    @torch.no_grad()\n    def p_sample_loop(self, shape, return_intermediates=False):\n        device = self.betas.device\n        b = shape[0]\n        img = torch.randn(shape, device=device)\n        intermediates = [img]\n        for i in tqdm(reversed(range(0, self.num_timesteps)), desc='Sampling t', total=self.num_timesteps):\n            img = self.p_sample(img, torch.full((b,), i, device=device, dtype=torch.long),\n                                clip_denoised=self.clip_denoised)\n            if i % self.log_every_t == 0 or i == self.num_timesteps - 1:\n                intermediates.append(img)\n        if return_intermediates:\n            return img, intermediates\n        return img\n\n    @torch.no_grad()\n    def sample(self, batch_size=16, return_intermediates=False):\n        image_size = self.image_size\n        channels = self.channels\n        return self.p_sample_loop((batch_size, channels, image_size, image_size),\n                                  return_intermediates=return_intermediates)\n\n    def q_sample(self, x_start, t, noise=None):\n        noise = default(noise, lambda: torch.randn_like(x_start))\n        return (extract_into_tensor(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start +\n                extract_into_tensor(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape) * noise)\n\n    def get_loss(self, pred, target, mean=True):\n        if self.loss_type == 'l1':\n            loss = (target - pred).abs()\n            if mean:\n                loss = loss.mean()\n        elif self.loss_type == 'l2':\n            if mean:\n                loss = torch.nn.functional.mse_loss(target, pred)\n            else:\n                loss = torch.nn.functional.mse_loss(target, pred, reduction='none')\n        else:\n            raise NotImplementedError(\"unknown loss type '{loss_type}'\")\n\n        return loss\n\n    def p_losses(self, x_start, t, noise=None):\n        noise = default(noise, lambda: torch.randn_like(x_start))\n        x_noisy = self.q_sample(x_start=x_start, t=t, noise=noise)\n        model_out = self.model(x_noisy, t)\n\n        loss_dict = {}\n        if self.parameterization == \"eps\":\n            target = noise\n        elif self.parameterization == \"x0\":\n            target = x_start\n        else:\n            raise NotImplementedError(f\"Parameterization {self.parameterization} not yet supported\")\n\n        loss = self.get_loss(model_out, target, mean=False).mean(dim=[1, 2, 3])\n\n        log_prefix = 'train' if self.training else 'val'\n\n        loss_dict.update({f'{log_prefix}/loss_simple': loss.mean()})\n        loss_simple = loss.mean() * self.l_simple_weight\n\n        loss_vlb = (self.lvlb_weights[t] * loss).mean()\n        loss_dict.update({f'{log_prefix}/loss_vlb': loss_vlb})\n\n        loss = loss_simple + self.original_elbo_weight * loss_vlb\n\n        loss_dict.update({f'{log_prefix}/loss': loss})\n\n        return loss, loss_dict\n\n    def forward(self, x, *args, **kwargs):\n        # b, c, h, w, device, img_size, = *x.shape, x.device, self.image_size\n        # assert h == img_size and w == img_size, f'height and width of image must be {img_size}'\n        t = torch.randint(0, self.num_timesteps, (x.shape[0],), device=self.device).long()\n        return self.p_losses(x, t, *args, **kwargs)\n\n    def get_input(self, batch, k):\n        x = batch[k]\n        if len(x.shape) == 3:\n            x = x[..., None]\n        x = rearrange(x, 'b h w c -> b c h w')\n        x = x.to(memory_format=torch.contiguous_format).float()\n        return x\n\n    def shared_step(self, batch):\n        x = self.get_input(batch, self.first_stage_key)\n        loss, loss_dict = self(x)\n        return loss, loss_dict\n\n    def training_step(self, batch, batch_idx):\n        loss, loss_dict = self.shared_step(batch)\n\n        self.log_dict(loss_dict, prog_bar=True,\n                      logger=True, on_step=True, on_epoch=True)\n\n        self.log(\"global_step\", self.global_step,\n                 prog_bar=True, logger=True, on_step=True, on_epoch=False)\n\n        if self.use_scheduler:\n            lr = self.optimizers().param_groups[0]['lr']\n            self.log('lr_abs', lr, prog_bar=True, logger=True, on_step=True, on_epoch=False)\n\n        return loss\n\n    @torch.no_grad()\n    def validation_step(self, batch, batch_idx):\n        _, loss_dict_no_ema = self.shared_step(batch)\n        with self.ema_scope():\n            _, loss_dict_ema = self.shared_step(batch)\n            loss_dict_ema = {key + '_ema': loss_dict_ema[key] for key in loss_dict_ema}\n        self.log_dict(loss_dict_no_ema, prog_bar=False, logger=True, on_step=False, on_epoch=True)\n        self.log_dict(loss_dict_ema, prog_bar=False, logger=True, on_step=False, on_epoch=True)\n\n    def on_train_batch_end(self, *args, **kwargs):\n        if self.use_ema:\n            self.model_ema(self.model)\n\n    def _get_rows_from_list(self, samples):\n        n_imgs_per_row = len(samples)\n        denoise_grid = rearrange(samples, 'n b c h w -> b n c h w')\n        denoise_grid = rearrange(denoise_grid, 'b n c h w -> (b n) c h w')\n        denoise_grid = make_grid(denoise_grid, nrow=n_imgs_per_row)\n        return denoise_grid\n\n    @torch.no_grad()\n    def log_images(self, batch, N=8, n_row=2, sample=True, return_keys=None, **kwargs):\n        log = {}\n        x = self.get_input(batch, self.first_stage_key)\n        N = min(x.shape[0], N)\n        n_row = min(x.shape[0], n_row)\n        x = x.to(self.device)[:N]\n        log[\"inputs\"] = x\n\n        # get diffusion row\n        diffusion_row = []\n        x_start = x[:n_row]\n\n        for t in range(self.num_timesteps):\n            if t % self.log_every_t == 0 or t == self.num_timesteps - 1:\n                t = repeat(torch.tensor([t]), '1 -> b', b=n_row)\n                t = t.to(self.device).long()\n                noise = torch.randn_like(x_start)\n                x_noisy = self.q_sample(x_start=x_start, t=t, noise=noise)\n                diffusion_row.append(x_noisy)\n\n        log[\"diffusion_row\"] = self._get_rows_from_list(diffusion_row)\n\n        if sample:\n            # get denoise row\n            with self.ema_scope(\"Plotting\"):\n                samples, denoise_row = self.sample(batch_size=N, return_intermediates=True)\n\n            log[\"samples\"] = samples\n            log[\"denoise_row\"] = self._get_rows_from_list(denoise_row)\n\n        if return_keys:\n            if np.intersect1d(list(log.keys()), return_keys).shape[0] == 0:\n                return log\n            else:\n                return {key: log[key] for key in return_keys}\n        return log\n\n    def configure_optimizers(self):\n        lr = self.learning_rate\n        params = list(self.model.parameters())\n        if self.learn_logvar:\n            params = params + [self.logvar]\n        opt = torch.optim.AdamW(params, lr=lr)\n        return opt\n\n\nclass LatentDiffusionV1(DDPMV1):\n    \"\"\"main class\"\"\"\n    def __init__(self,\n                 first_stage_config,\n                 cond_stage_config,\n                 num_timesteps_cond=None,\n                 cond_stage_key=\"image\",\n                 cond_stage_trainable=False,\n                 concat_mode=True,\n                 cond_stage_forward=None,\n                 conditioning_key=None,\n                 scale_factor=1.0,\n                 scale_by_std=False,\n                 *args, **kwargs):\n        self.num_timesteps_cond = default(num_timesteps_cond, 1)\n        self.scale_by_std = scale_by_std\n        assert self.num_timesteps_cond <= kwargs['timesteps']\n        # for backwards compatibility after implementation of DiffusionWrapper\n        if conditioning_key is None:\n            conditioning_key = 'concat' if concat_mode else 'crossattn'\n        if cond_stage_config == '__is_unconditional__':\n            conditioning_key = None\n        ckpt_path = kwargs.pop(\"ckpt_path\", None)\n        ignore_keys = kwargs.pop(\"ignore_keys\", [])\n        super().__init__(*args, conditioning_key=conditioning_key, **kwargs)\n        self.concat_mode = concat_mode\n        self.cond_stage_trainable = cond_stage_trainable\n        self.cond_stage_key = cond_stage_key\n        try:\n            self.num_downs = len(first_stage_config.params.ddconfig.ch_mult) - 1\n        except Exception:\n            self.num_downs = 0\n        if not scale_by_std:\n            self.scale_factor = scale_factor\n        else:\n            self.register_buffer('scale_factor', torch.tensor(scale_factor))\n        self.instantiate_first_stage(first_stage_config)\n        self.instantiate_cond_stage(cond_stage_config)\n        self.cond_stage_forward = cond_stage_forward\n        self.clip_denoised = False\n        self.bbox_tokenizer = None\n\n        self.restarted_from_ckpt = False\n        if ckpt_path is not None:\n            self.init_from_ckpt(ckpt_path, ignore_keys)\n            self.restarted_from_ckpt = True\n\n    def make_cond_schedule(self, ):\n        self.cond_ids = torch.full(size=(self.num_timesteps,), fill_value=self.num_timesteps - 1, dtype=torch.long)\n        ids = torch.round(torch.linspace(0, self.num_timesteps - 1, self.num_timesteps_cond)).long()\n        self.cond_ids[:self.num_timesteps_cond] = ids\n\n    @rank_zero_only\n    @torch.no_grad()\n    def on_train_batch_start(self, batch, batch_idx, dataloader_idx):\n        # only for very first batch\n        if self.scale_by_std and self.current_epoch == 0 and self.global_step == 0 and batch_idx == 0 and not self.restarted_from_ckpt:\n            assert self.scale_factor == 1., 'rather not use custom rescaling and std-rescaling simultaneously'\n            # set rescale weight to 1./std of encodings\n            print(\"### USING STD-RESCALING ###\")\n            x = super().get_input(batch, self.first_stage_key)\n            x = x.to(self.device)\n            encoder_posterior = self.encode_first_stage(x)\n            z = self.get_first_stage_encoding(encoder_posterior).detach()\n            del self.scale_factor\n            self.register_buffer('scale_factor', 1. / z.flatten().std())\n            print(f\"setting self.scale_factor to {self.scale_factor}\")\n            print(\"### USING STD-RESCALING ###\")\n\n    def register_schedule(self,\n                          given_betas=None, beta_schedule=\"linear\", timesteps=1000,\n                          linear_start=1e-4, linear_end=2e-2, cosine_s=8e-3):\n        super().register_schedule(given_betas, beta_schedule, timesteps, linear_start, linear_end, cosine_s)\n\n        self.shorten_cond_schedule = self.num_timesteps_cond > 1\n        if self.shorten_cond_schedule:\n            self.make_cond_schedule()\n\n    def instantiate_first_stage(self, config):\n        model = instantiate_from_config(config)\n        self.first_stage_model = model.eval()\n        self.first_stage_model.train = disabled_train\n        for param in self.first_stage_model.parameters():\n            param.requires_grad = False\n\n    def instantiate_cond_stage(self, config):\n        if not self.cond_stage_trainable:\n            if config == \"__is_first_stage__\":\n                print(\"Using first stage also as cond stage.\")\n                self.cond_stage_model = self.first_stage_model\n            elif config == \"__is_unconditional__\":\n                print(f\"Training {self.__class__.__name__} as an unconditional model.\")\n                self.cond_stage_model = None\n                # self.be_unconditional = True\n            else:\n                model = instantiate_from_config(config)\n                self.cond_stage_model = model.eval()\n                self.cond_stage_model.train = disabled_train\n                for param in self.cond_stage_model.parameters():\n                    param.requires_grad = False\n        else:\n            assert config != '__is_first_stage__'\n            assert config != '__is_unconditional__'\n            model = instantiate_from_config(config)\n            self.cond_stage_model = model\n\n    def _get_denoise_row_from_list(self, samples, desc='', force_no_decoder_quantization=False):\n        denoise_row = []\n        for zd in tqdm(samples, desc=desc):\n            denoise_row.append(self.decode_first_stage(zd.to(self.device),\n                                                            force_not_quantize=force_no_decoder_quantization))\n        n_imgs_per_row = len(denoise_row)\n        denoise_row = torch.stack(denoise_row)  # n_log_step, n_row, C, H, W\n        denoise_grid = rearrange(denoise_row, 'n b c h w -> b n c h w')\n        denoise_grid = rearrange(denoise_grid, 'b n c h w -> (b n) c h w')\n        denoise_grid = make_grid(denoise_grid, nrow=n_imgs_per_row)\n        return denoise_grid\n\n    def get_first_stage_encoding(self, encoder_posterior):\n        if isinstance(encoder_posterior, DiagonalGaussianDistribution):\n            z = encoder_posterior.sample()\n        elif isinstance(encoder_posterior, torch.Tensor):\n            z = encoder_posterior\n        else:\n            raise NotImplementedError(f\"encoder_posterior of type '{type(encoder_posterior)}' not yet implemented\")\n        return self.scale_factor * z\n\n    def get_learned_conditioning(self, c):\n        if self.cond_stage_forward is None:\n            if hasattr(self.cond_stage_model, 'encode') and callable(self.cond_stage_model.encode):\n                c = self.cond_stage_model.encode(c)\n                if isinstance(c, DiagonalGaussianDistribution):\n                    c = c.mode()\n            else:\n                c = self.cond_stage_model(c)\n        else:\n            assert hasattr(self.cond_stage_model, self.cond_stage_forward)\n            c = getattr(self.cond_stage_model, self.cond_stage_forward)(c)\n        return c\n\n    def meshgrid(self, h, w):\n        y = torch.arange(0, h).view(h, 1, 1).repeat(1, w, 1)\n        x = torch.arange(0, w).view(1, w, 1).repeat(h, 1, 1)\n\n        arr = torch.cat([y, x], dim=-1)\n        return arr\n\n    def delta_border(self, h, w):\n        \"\"\"\n        :param h: height\n        :param w: width\n        :return: normalized distance to image border,\n         wtith min distance = 0 at border and max dist = 0.5 at image center\n        \"\"\"\n        lower_right_corner = torch.tensor([h - 1, w - 1]).view(1, 1, 2)\n        arr = self.meshgrid(h, w) / lower_right_corner\n        dist_left_up = torch.min(arr, dim=-1, keepdims=True)[0]\n        dist_right_down = torch.min(1 - arr, dim=-1, keepdims=True)[0]\n        edge_dist = torch.min(torch.cat([dist_left_up, dist_right_down], dim=-1), dim=-1)[0]\n        return edge_dist\n\n    def get_weighting(self, h, w, Ly, Lx, device):\n        weighting = self.delta_border(h, w)\n        weighting = torch.clip(weighting, self.split_input_params[\"clip_min_weight\"],\n                               self.split_input_params[\"clip_max_weight\"], )\n        weighting = weighting.view(1, h * w, 1).repeat(1, 1, Ly * Lx).to(device)\n\n        if self.split_input_params[\"tie_braker\"]:\n            L_weighting = self.delta_border(Ly, Lx)\n            L_weighting = torch.clip(L_weighting,\n                                     self.split_input_params[\"clip_min_tie_weight\"],\n                                     self.split_input_params[\"clip_max_tie_weight\"])\n\n            L_weighting = L_weighting.view(1, 1, Ly * Lx).to(device)\n            weighting = weighting * L_weighting\n        return weighting\n\n    def get_fold_unfold(self, x, kernel_size, stride, uf=1, df=1):  # todo load once not every time, shorten code\n        \"\"\"\n        :param x: img of size (bs, c, h, w)\n        :return: n img crops of size (n, bs, c, kernel_size[0], kernel_size[1])\n        \"\"\"\n        bs, nc, h, w = x.shape\n\n        # number of crops in image\n        Ly = (h - kernel_size[0]) // stride[0] + 1\n        Lx = (w - kernel_size[1]) // stride[1] + 1\n\n        if uf == 1 and df == 1:\n            fold_params = dict(kernel_size=kernel_size, dilation=1, padding=0, stride=stride)\n            unfold = torch.nn.Unfold(**fold_params)\n\n            fold = torch.nn.Fold(output_size=x.shape[2:], **fold_params)\n\n            weighting = self.get_weighting(kernel_size[0], kernel_size[1], Ly, Lx, x.device).to(x.dtype)\n            normalization = fold(weighting).view(1, 1, h, w)  # normalizes the overlap\n            weighting = weighting.view((1, 1, kernel_size[0], kernel_size[1], Ly * Lx))\n\n        elif uf > 1 and df == 1:\n            fold_params = dict(kernel_size=kernel_size, dilation=1, padding=0, stride=stride)\n            unfold = torch.nn.Unfold(**fold_params)\n\n            fold_params2 = dict(kernel_size=(kernel_size[0] * uf, kernel_size[0] * uf),\n                                dilation=1, padding=0,\n                                stride=(stride[0] * uf, stride[1] * uf))\n            fold = torch.nn.Fold(output_size=(x.shape[2] * uf, x.shape[3] * uf), **fold_params2)\n\n            weighting = self.get_weighting(kernel_size[0] * uf, kernel_size[1] * uf, Ly, Lx, x.device).to(x.dtype)\n            normalization = fold(weighting).view(1, 1, h * uf, w * uf)  # normalizes the overlap\n            weighting = weighting.view((1, 1, kernel_size[0] * uf, kernel_size[1] * uf, Ly * Lx))\n\n        elif df > 1 and uf == 1:\n            fold_params = dict(kernel_size=kernel_size, dilation=1, padding=0, stride=stride)\n            unfold = torch.nn.Unfold(**fold_params)\n\n            fold_params2 = dict(kernel_size=(kernel_size[0] // df, kernel_size[0] // df),\n                                dilation=1, padding=0,\n                                stride=(stride[0] // df, stride[1] // df))\n            fold = torch.nn.Fold(output_size=(x.shape[2] // df, x.shape[3] // df), **fold_params2)\n\n            weighting = self.get_weighting(kernel_size[0] // df, kernel_size[1] // df, Ly, Lx, x.device).to(x.dtype)\n            normalization = fold(weighting).view(1, 1, h // df, w // df)  # normalizes the overlap\n            weighting = weighting.view((1, 1, kernel_size[0] // df, kernel_size[1] // df, Ly * Lx))\n\n        else:\n            raise NotImplementedError\n\n        return fold, unfold, normalization, weighting\n\n    @torch.no_grad()\n    def get_input(self, batch, k, return_first_stage_outputs=False, force_c_encode=False,\n                  cond_key=None, return_original_cond=False, bs=None):\n        x = super().get_input(batch, k)\n        if bs is not None:\n            x = x[:bs]\n        x = x.to(self.device)\n        encoder_posterior = self.encode_first_stage(x)\n        z = self.get_first_stage_encoding(encoder_posterior).detach()\n\n        if self.model.conditioning_key is not None:\n            if cond_key is None:\n                cond_key = self.cond_stage_key\n            if cond_key != self.first_stage_key:\n                if cond_key in ['caption', 'coordinates_bbox']:\n                    xc = batch[cond_key]\n                elif cond_key == 'class_label':\n                    xc = batch\n                else:\n                    xc = super().get_input(batch, cond_key).to(self.device)\n            else:\n                xc = x\n            if not self.cond_stage_trainable or force_c_encode:\n                if isinstance(xc, dict) or isinstance(xc, list):\n                    # import pudb; pudb.set_trace()\n                    c = self.get_learned_conditioning(xc)\n                else:\n                    c = self.get_learned_conditioning(xc.to(self.device))\n            else:\n                c = xc\n            if bs is not None:\n                c = c[:bs]\n\n            if self.use_positional_encodings:\n                pos_x, pos_y = self.compute_latent_shifts(batch)\n                ckey = __conditioning_keys__[self.model.conditioning_key]\n                c = {ckey: c, 'pos_x': pos_x, 'pos_y': pos_y}\n\n        else:\n            c = None\n            xc = None\n            if self.use_positional_encodings:\n                pos_x, pos_y = self.compute_latent_shifts(batch)\n                c = {'pos_x': pos_x, 'pos_y': pos_y}\n        out = [z, c]\n        if return_first_stage_outputs:\n            xrec = self.decode_first_stage(z)\n            out.extend([x, xrec])\n        if return_original_cond:\n            out.append(xc)\n        return out\n\n    @torch.no_grad()\n    def decode_first_stage(self, z, predict_cids=False, force_not_quantize=False):\n        if predict_cids:\n            if z.dim() == 4:\n                z = torch.argmax(z.exp(), dim=1).long()\n            z = self.first_stage_model.quantize.get_codebook_entry(z, shape=None)\n            z = rearrange(z, 'b h w c -> b c h w').contiguous()\n\n        z = 1. / self.scale_factor * z\n\n        if hasattr(self, \"split_input_params\"):\n            if self.split_input_params[\"patch_distributed_vq\"]:\n                ks = self.split_input_params[\"ks\"]  # eg. (128, 128)\n                stride = self.split_input_params[\"stride\"]  # eg. (64, 64)\n                uf = self.split_input_params[\"vqf\"]\n                bs, nc, h, w = z.shape\n                if ks[0] > h or ks[1] > w:\n                    ks = (min(ks[0], h), min(ks[1], w))\n                    print(\"reducing Kernel\")\n\n                if stride[0] > h or stride[1] > w:\n                    stride = (min(stride[0], h), min(stride[1], w))\n                    print(\"reducing stride\")\n\n                fold, unfold, normalization, weighting = self.get_fold_unfold(z, ks, stride, uf=uf)\n\n                z = unfold(z)  # (bn, nc * prod(**ks), L)\n                # 1. Reshape to img shape\n                z = z.view((z.shape[0], -1, ks[0], ks[1], z.shape[-1]))  # (bn, nc, ks[0], ks[1], L )\n\n                # 2. apply model loop over last dim\n                if isinstance(self.first_stage_model, VQModelInterface):\n                    output_list = [self.first_stage_model.decode(z[:, :, :, :, i],\n                                                                 force_not_quantize=predict_cids or force_not_quantize)\n                                   for i in range(z.shape[-1])]\n                else:\n\n                    output_list = [self.first_stage_model.decode(z[:, :, :, :, i])\n                                   for i in range(z.shape[-1])]\n\n                o = torch.stack(output_list, axis=-1)  # # (bn, nc, ks[0], ks[1], L)\n                o = o * weighting\n                # Reverse 1. reshape to img shape\n                o = o.view((o.shape[0], -1, o.shape[-1]))  # (bn, nc * ks[0] * ks[1], L)\n                # stitch crops together\n                decoded = fold(o)\n                decoded = decoded / normalization  # norm is shape (1, 1, h, w)\n                return decoded\n            else:\n                if isinstance(self.first_stage_model, VQModelInterface):\n                    return self.first_stage_model.decode(z, force_not_quantize=predict_cids or force_not_quantize)\n                else:\n                    return self.first_stage_model.decode(z)\n\n        else:\n            if isinstance(self.first_stage_model, VQModelInterface):\n                return self.first_stage_model.decode(z, force_not_quantize=predict_cids or force_not_quantize)\n            else:\n                return self.first_stage_model.decode(z)\n\n    # same as above but without decorator\n    def differentiable_decode_first_stage(self, z, predict_cids=False, force_not_quantize=False):\n        if predict_cids:\n            if z.dim() == 4:\n                z = torch.argmax(z.exp(), dim=1).long()\n            z = self.first_stage_model.quantize.get_codebook_entry(z, shape=None)\n            z = rearrange(z, 'b h w c -> b c h w').contiguous()\n\n        z = 1. / self.scale_factor * z\n\n        if hasattr(self, \"split_input_params\"):\n            if self.split_input_params[\"patch_distributed_vq\"]:\n                ks = self.split_input_params[\"ks\"]  # eg. (128, 128)\n                stride = self.split_input_params[\"stride\"]  # eg. (64, 64)\n                uf = self.split_input_params[\"vqf\"]\n                bs, nc, h, w = z.shape\n                if ks[0] > h or ks[1] > w:\n                    ks = (min(ks[0], h), min(ks[1], w))\n                    print(\"reducing Kernel\")\n\n                if stride[0] > h or stride[1] > w:\n                    stride = (min(stride[0], h), min(stride[1], w))\n                    print(\"reducing stride\")\n\n                fold, unfold, normalization, weighting = self.get_fold_unfold(z, ks, stride, uf=uf)\n\n                z = unfold(z)  # (bn, nc * prod(**ks), L)\n                # 1. Reshape to img shape\n                z = z.view((z.shape[0], -1, ks[0], ks[1], z.shape[-1]))  # (bn, nc, ks[0], ks[1], L )\n\n                # 2. apply model loop over last dim\n                if isinstance(self.first_stage_model, VQModelInterface):\n                    output_list = [self.first_stage_model.decode(z[:, :, :, :, i],\n                                                                 force_not_quantize=predict_cids or force_not_quantize)\n                                   for i in range(z.shape[-1])]\n                else:\n\n                    output_list = [self.first_stage_model.decode(z[:, :, :, :, i])\n                                   for i in range(z.shape[-1])]\n\n                o = torch.stack(output_list, axis=-1)  # # (bn, nc, ks[0], ks[1], L)\n                o = o * weighting\n                # Reverse 1. reshape to img shape\n                o = o.view((o.shape[0], -1, o.shape[-1]))  # (bn, nc * ks[0] * ks[1], L)\n                # stitch crops together\n                decoded = fold(o)\n                decoded = decoded / normalization  # norm is shape (1, 1, h, w)\n                return decoded\n            else:\n                if isinstance(self.first_stage_model, VQModelInterface):\n                    return self.first_stage_model.decode(z, force_not_quantize=predict_cids or force_not_quantize)\n                else:\n                    return self.first_stage_model.decode(z)\n\n        else:\n            if isinstance(self.first_stage_model, VQModelInterface):\n                return self.first_stage_model.decode(z, force_not_quantize=predict_cids or force_not_quantize)\n            else:\n                return self.first_stage_model.decode(z)\n\n    @torch.no_grad()\n    def encode_first_stage(self, x):\n        if hasattr(self, \"split_input_params\"):\n            if self.split_input_params[\"patch_distributed_vq\"]:\n                ks = self.split_input_params[\"ks\"]  # eg. (128, 128)\n                stride = self.split_input_params[\"stride\"]  # eg. (64, 64)\n                df = self.split_input_params[\"vqf\"]\n                self.split_input_params['original_image_size'] = x.shape[-2:]\n                bs, nc, h, w = x.shape\n                if ks[0] > h or ks[1] > w:\n                    ks = (min(ks[0], h), min(ks[1], w))\n                    print(\"reducing Kernel\")\n\n                if stride[0] > h or stride[1] > w:\n                    stride = (min(stride[0], h), min(stride[1], w))\n                    print(\"reducing stride\")\n\n                fold, unfold, normalization, weighting = self.get_fold_unfold(x, ks, stride, df=df)\n                z = unfold(x)  # (bn, nc * prod(**ks), L)\n                # Reshape to img shape\n                z = z.view((z.shape[0], -1, ks[0], ks[1], z.shape[-1]))  # (bn, nc, ks[0], ks[1], L )\n\n                output_list = [self.first_stage_model.encode(z[:, :, :, :, i])\n                               for i in range(z.shape[-1])]\n\n                o = torch.stack(output_list, axis=-1)\n                o = o * weighting\n\n                # Reverse reshape to img shape\n                o = o.view((o.shape[0], -1, o.shape[-1]))  # (bn, nc * ks[0] * ks[1], L)\n                # stitch crops together\n                decoded = fold(o)\n                decoded = decoded / normalization\n                return decoded\n\n            else:\n                return self.first_stage_model.encode(x)\n        else:\n            return self.first_stage_model.encode(x)\n\n    def shared_step(self, batch, **kwargs):\n        x, c = self.get_input(batch, self.first_stage_key)\n        loss = self(x, c)\n        return loss\n\n    def forward(self, x, c, *args, **kwargs):\n        t = torch.randint(0, self.num_timesteps, (x.shape[0],), device=self.device).long()\n        if self.model.conditioning_key is not None:\n            assert c is not None\n            if self.cond_stage_trainable:\n                c = self.get_learned_conditioning(c)\n            if self.shorten_cond_schedule:  # TODO: drop this option\n                tc = self.cond_ids[t].to(self.device)\n                c = self.q_sample(x_start=c, t=tc, noise=torch.randn_like(c.float()))\n        return self.p_losses(x, c, t, *args, **kwargs)\n\n    def apply_model(self, x_noisy, t, cond, return_ids=False):\n\n        if isinstance(cond, dict):\n            # hybrid case, cond is expected to be a dict\n            pass\n        else:\n            if not isinstance(cond, list):\n                cond = [cond]\n            key = 'c_concat' if self.model.conditioning_key == 'concat' else 'c_crossattn'\n            cond = {key: cond}\n\n        if hasattr(self, \"split_input_params\"):\n            assert len(cond) == 1  # todo can only deal with one conditioning atm\n            assert not return_ids\n            ks = self.split_input_params[\"ks\"]  # eg. (128, 128)\n            stride = self.split_input_params[\"stride\"]  # eg. (64, 64)\n\n            h, w = x_noisy.shape[-2:]\n\n            fold, unfold, normalization, weighting = self.get_fold_unfold(x_noisy, ks, stride)\n\n            z = unfold(x_noisy)  # (bn, nc * prod(**ks), L)\n            # Reshape to img shape\n            z = z.view((z.shape[0], -1, ks[0], ks[1], z.shape[-1]))  # (bn, nc, ks[0], ks[1], L )\n            z_list = [z[:, :, :, :, i] for i in range(z.shape[-1])]\n\n            if self.cond_stage_key in [\"image\", \"LR_image\", \"segmentation\",\n                                       'bbox_img'] and self.model.conditioning_key:  # todo check for completeness\n                c_key = next(iter(cond.keys()))  # get key\n                c = next(iter(cond.values()))  # get value\n                assert (len(c) == 1)  # todo extend to list with more than one elem\n                c = c[0]  # get element\n\n                c = unfold(c)\n                c = c.view((c.shape[0], -1, ks[0], ks[1], c.shape[-1]))  # (bn, nc, ks[0], ks[1], L )\n\n                cond_list = [{c_key: [c[:, :, :, :, i]]} for i in range(c.shape[-1])]\n\n            elif self.cond_stage_key == 'coordinates_bbox':\n                assert 'original_image_size' in self.split_input_params, 'BoundingBoxRescaling is missing original_image_size'\n\n                # assuming padding of unfold is always 0 and its dilation is always 1\n                n_patches_per_row = int((w - ks[0]) / stride[0] + 1)\n                full_img_h, full_img_w = self.split_input_params['original_image_size']\n                # as we are operating on latents, we need the factor from the original image size to the\n                # spatial latent size to properly rescale the crops for regenerating the bbox annotations\n                num_downs = self.first_stage_model.encoder.num_resolutions - 1\n                rescale_latent = 2 ** (num_downs)\n\n                # get top left positions of patches as conforming for the bbbox tokenizer, therefore we\n                # need to rescale the tl patch coordinates to be in between (0,1)\n                tl_patch_coordinates = [(rescale_latent * stride[0] * (patch_nr % n_patches_per_row) / full_img_w,\n                                         rescale_latent * stride[1] * (patch_nr // n_patches_per_row) / full_img_h)\n                                        for patch_nr in range(z.shape[-1])]\n\n                # patch_limits are tl_coord, width and height coordinates as (x_tl, y_tl, h, w)\n                patch_limits = [(x_tl, y_tl,\n                                 rescale_latent * ks[0] / full_img_w,\n                                 rescale_latent * ks[1] / full_img_h) for x_tl, y_tl in tl_patch_coordinates]\n                # patch_values = [(np.arange(x_tl,min(x_tl+ks, 1.)),np.arange(y_tl,min(y_tl+ks, 1.))) for x_tl, y_tl in tl_patch_coordinates]\n\n                # tokenize crop coordinates for the bounding boxes of the respective patches\n                patch_limits_tknzd = [torch.LongTensor(self.bbox_tokenizer._crop_encoder(bbox))[None].to(self.device)\n                                      for bbox in patch_limits]  # list of length l with tensors of shape (1, 2)\n                print(patch_limits_tknzd[0].shape)\n                # cut tknzd crop position from conditioning\n                assert isinstance(cond, dict), 'cond must be dict to be fed into model'\n                cut_cond = cond['c_crossattn'][0][..., :-2].to(self.device)\n                print(cut_cond.shape)\n\n                adapted_cond = torch.stack([torch.cat([cut_cond, p], dim=1) for p in patch_limits_tknzd])\n                adapted_cond = rearrange(adapted_cond, 'l b n -> (l b) n')\n                print(adapted_cond.shape)\n                adapted_cond = self.get_learned_conditioning(adapted_cond)\n                print(adapted_cond.shape)\n                adapted_cond = rearrange(adapted_cond, '(l b) n d -> l b n d', l=z.shape[-1])\n                print(adapted_cond.shape)\n\n                cond_list = [{'c_crossattn': [e]} for e in adapted_cond]\n\n            else:\n                cond_list = [cond for i in range(z.shape[-1])]  # Todo make this more efficient\n\n            # apply model by loop over crops\n            output_list = [self.model(z_list[i], t, **cond_list[i]) for i in range(z.shape[-1])]\n            assert not isinstance(output_list[0],\n                                  tuple)  # todo cant deal with multiple model outputs check this never happens\n\n            o = torch.stack(output_list, axis=-1)\n            o = o * weighting\n            # Reverse reshape to img shape\n            o = o.view((o.shape[0], -1, o.shape[-1]))  # (bn, nc * ks[0] * ks[1], L)\n            # stitch crops together\n            x_recon = fold(o) / normalization\n\n        else:\n            x_recon = self.model(x_noisy, t, **cond)\n\n        if isinstance(x_recon, tuple) and not return_ids:\n            return x_recon[0]\n        else:\n            return x_recon\n\n    def _predict_eps_from_xstart(self, x_t, t, pred_xstart):\n        return (extract_into_tensor(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t - pred_xstart) / \\\n               extract_into_tensor(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape)\n\n    def _prior_bpd(self, x_start):\n        \"\"\"\n        Get the prior KL term for the variational lower-bound, measured in\n        bits-per-dim.\n        This term can't be optimized, as it only depends on the encoder.\n        :param x_start: the [N x C x ...] tensor of inputs.\n        :return: a batch of [N] KL values (in bits), one per batch element.\n        \"\"\"\n        batch_size = x_start.shape[0]\n        t = torch.tensor([self.num_timesteps - 1] * batch_size, device=x_start.device)\n        qt_mean, _, qt_log_variance = self.q_mean_variance(x_start, t)\n        kl_prior = normal_kl(mean1=qt_mean, logvar1=qt_log_variance, mean2=0.0, logvar2=0.0)\n        return mean_flat(kl_prior) / np.log(2.0)\n\n    def p_losses(self, x_start, cond, t, noise=None):\n        noise = default(noise, lambda: torch.randn_like(x_start))\n        x_noisy = self.q_sample(x_start=x_start, t=t, noise=noise)\n        model_output = self.apply_model(x_noisy, t, cond)\n\n        loss_dict = {}\n        prefix = 'train' if self.training else 'val'\n\n        if self.parameterization == \"x0\":\n            target = x_start\n        elif self.parameterization == \"eps\":\n            target = noise\n        else:\n            raise NotImplementedError()\n\n        loss_simple = self.get_loss(model_output, target, mean=False).mean([1, 2, 3])\n        loss_dict.update({f'{prefix}/loss_simple': loss_simple.mean()})\n\n        logvar_t = self.logvar[t].to(self.device)\n        loss = loss_simple / torch.exp(logvar_t) + logvar_t\n        # loss = loss_simple / torch.exp(self.logvar) + self.logvar\n        if self.learn_logvar:\n            loss_dict.update({f'{prefix}/loss_gamma': loss.mean()})\n            loss_dict.update({'logvar': self.logvar.data.mean()})\n\n        loss = self.l_simple_weight * loss.mean()\n\n        loss_vlb = self.get_loss(model_output, target, mean=False).mean(dim=(1, 2, 3))\n        loss_vlb = (self.lvlb_weights[t] * loss_vlb).mean()\n        loss_dict.update({f'{prefix}/loss_vlb': loss_vlb})\n        loss += (self.original_elbo_weight * loss_vlb)\n        loss_dict.update({f'{prefix}/loss': loss})\n\n        return loss, loss_dict\n\n    def p_mean_variance(self, x, c, t, clip_denoised: bool, return_codebook_ids=False, quantize_denoised=False,\n                        return_x0=False, score_corrector=None, corrector_kwargs=None):\n        t_in = t\n        model_out = self.apply_model(x, t_in, c, return_ids=return_codebook_ids)\n\n        if score_corrector is not None:\n            assert self.parameterization == \"eps\"\n            model_out = score_corrector.modify_score(self, model_out, x, t, c, **corrector_kwargs)\n\n        if return_codebook_ids:\n            model_out, logits = model_out\n\n        if self.parameterization == \"eps\":\n            x_recon = self.predict_start_from_noise(x, t=t, noise=model_out)\n        elif self.parameterization == \"x0\":\n            x_recon = model_out\n        else:\n            raise NotImplementedError()\n\n        if clip_denoised:\n            x_recon.clamp_(-1., 1.)\n        if quantize_denoised:\n            x_recon, _, [_, _, indices] = self.first_stage_model.quantize(x_recon)\n        model_mean, posterior_variance, posterior_log_variance = self.q_posterior(x_start=x_recon, x_t=x, t=t)\n        if return_codebook_ids:\n            return model_mean, posterior_variance, posterior_log_variance, logits\n        elif return_x0:\n            return model_mean, posterior_variance, posterior_log_variance, x_recon\n        else:\n            return model_mean, posterior_variance, posterior_log_variance\n\n    @torch.no_grad()\n    def p_sample(self, x, c, t, clip_denoised=False, repeat_noise=False,\n                 return_codebook_ids=False, quantize_denoised=False, return_x0=False,\n                 temperature=1., noise_dropout=0., score_corrector=None, corrector_kwargs=None):\n        b, *_, device = *x.shape, x.device\n        outputs = self.p_mean_variance(x=x, c=c, t=t, clip_denoised=clip_denoised,\n                                       return_codebook_ids=return_codebook_ids,\n                                       quantize_denoised=quantize_denoised,\n                                       return_x0=return_x0,\n                                       score_corrector=score_corrector, corrector_kwargs=corrector_kwargs)\n        if return_codebook_ids:\n            raise DeprecationWarning(\"Support dropped.\")\n            model_mean, _, model_log_variance, logits = outputs\n        elif return_x0:\n            model_mean, _, model_log_variance, x0 = outputs\n        else:\n            model_mean, _, model_log_variance = outputs\n\n        noise = noise_like(x.shape, device, repeat_noise) * temperature\n        if noise_dropout > 0.:\n            noise = torch.nn.functional.dropout(noise, p=noise_dropout)\n        # no noise when t == 0\n        nonzero_mask = (1 - (t == 0).float()).reshape(b, *((1,) * (len(x.shape) - 1)))\n\n        if return_codebook_ids:\n            return model_mean + nonzero_mask * (0.5 * model_log_variance).exp() * noise, logits.argmax(dim=1)\n        if return_x0:\n            return model_mean + nonzero_mask * (0.5 * model_log_variance).exp() * noise, x0\n        else:\n            return model_mean + nonzero_mask * (0.5 * model_log_variance).exp() * noise\n\n    @torch.no_grad()\n    def progressive_denoising(self, cond, shape, verbose=True, callback=None, quantize_denoised=False,\n                              img_callback=None, mask=None, x0=None, temperature=1., noise_dropout=0.,\n                              score_corrector=None, corrector_kwargs=None, batch_size=None, x_T=None, start_T=None,\n                              log_every_t=None):\n        if not log_every_t:\n            log_every_t = self.log_every_t\n        timesteps = self.num_timesteps\n        if batch_size is not None:\n            b = batch_size if batch_size is not None else shape[0]\n            shape = [batch_size] + list(shape)\n        else:\n            b = batch_size = shape[0]\n        if x_T is None:\n            img = torch.randn(shape, device=self.device)\n        else:\n            img = x_T\n        intermediates = []\n        if cond is not None:\n            if isinstance(cond, dict):\n                cond = {key: cond[key][:batch_size] if not isinstance(cond[key], list) else\n                [x[:batch_size] for x in cond[key]] for key in cond}\n            else:\n                cond = [c[:batch_size] for c in cond] if isinstance(cond, list) else cond[:batch_size]\n\n        if start_T is not None:\n            timesteps = min(timesteps, start_T)\n        iterator = tqdm(reversed(range(0, timesteps)), desc='Progressive Generation',\n                        total=timesteps) if verbose else reversed(\n            range(0, timesteps))\n        if type(temperature) == float:\n            temperature = [temperature] * timesteps\n\n        for i in iterator:\n            ts = torch.full((b,), i, device=self.device, dtype=torch.long)\n            if self.shorten_cond_schedule:\n                assert self.model.conditioning_key != 'hybrid'\n                tc = self.cond_ids[ts].to(cond.device)\n                cond = self.q_sample(x_start=cond, t=tc, noise=torch.randn_like(cond))\n\n            img, x0_partial = self.p_sample(img, cond, ts,\n                                            clip_denoised=self.clip_denoised,\n                                            quantize_denoised=quantize_denoised, return_x0=True,\n                                            temperature=temperature[i], noise_dropout=noise_dropout,\n                                            score_corrector=score_corrector, corrector_kwargs=corrector_kwargs)\n            if mask is not None:\n                assert x0 is not None\n                img_orig = self.q_sample(x0, ts)\n                img = img_orig * mask + (1. - mask) * img\n\n            if i % log_every_t == 0 or i == timesteps - 1:\n                intermediates.append(x0_partial)\n            if callback:\n                callback(i)\n            if img_callback:\n                img_callback(img, i)\n        return img, intermediates\n\n    @torch.no_grad()\n    def p_sample_loop(self, cond, shape, return_intermediates=False,\n                      x_T=None, verbose=True, callback=None, timesteps=None, quantize_denoised=False,\n                      mask=None, x0=None, img_callback=None, start_T=None,\n                      log_every_t=None):\n\n        if not log_every_t:\n            log_every_t = self.log_every_t\n        device = self.betas.device\n        b = shape[0]\n        if x_T is None:\n            img = torch.randn(shape, device=device)\n        else:\n            img = x_T\n\n        intermediates = [img]\n        if timesteps is None:\n            timesteps = self.num_timesteps\n\n        if start_T is not None:\n            timesteps = min(timesteps, start_T)\n        iterator = tqdm(reversed(range(0, timesteps)), desc='Sampling t', total=timesteps) if verbose else reversed(\n            range(0, timesteps))\n\n        if mask is not None:\n            assert x0 is not None\n            assert x0.shape[2:3] == mask.shape[2:3]  # spatial size has to match\n\n        for i in iterator:\n            ts = torch.full((b,), i, device=device, dtype=torch.long)\n            if self.shorten_cond_schedule:\n                assert self.model.conditioning_key != 'hybrid'\n                tc = self.cond_ids[ts].to(cond.device)\n                cond = self.q_sample(x_start=cond, t=tc, noise=torch.randn_like(cond))\n\n            img = self.p_sample(img, cond, ts,\n                                clip_denoised=self.clip_denoised,\n                                quantize_denoised=quantize_denoised)\n            if mask is not None:\n                img_orig = self.q_sample(x0, ts)\n                img = img_orig * mask + (1. - mask) * img\n\n            if i % log_every_t == 0 or i == timesteps - 1:\n                intermediates.append(img)\n            if callback:\n                callback(i)\n            if img_callback:\n                img_callback(img, i)\n\n        if return_intermediates:\n            return img, intermediates\n        return img\n\n    @torch.no_grad()\n    def sample(self, cond, batch_size=16, return_intermediates=False, x_T=None,\n               verbose=True, timesteps=None, quantize_denoised=False,\n               mask=None, x0=None, shape=None,**kwargs):\n        if shape is None:\n            shape = (batch_size, self.channels, self.image_size, self.image_size)\n        if cond is not None:\n            if isinstance(cond, dict):\n                cond = {key: cond[key][:batch_size] if not isinstance(cond[key], list) else\n                [x[:batch_size] for x in cond[key]] for key in cond}\n            else:\n                cond = [c[:batch_size] for c in cond] if isinstance(cond, list) else cond[:batch_size]\n        return self.p_sample_loop(cond,\n                                  shape,\n                                  return_intermediates=return_intermediates, x_T=x_T,\n                                  verbose=verbose, timesteps=timesteps, quantize_denoised=quantize_denoised,\n                                  mask=mask, x0=x0)\n\n    @torch.no_grad()\n    def sample_log(self,cond,batch_size,ddim, ddim_steps,**kwargs):\n\n        if ddim:\n            ddim_sampler = DDIMSampler(self)\n            shape = (self.channels, self.image_size, self.image_size)\n            samples, intermediates =ddim_sampler.sample(ddim_steps,batch_size,\n                                                        shape,cond,verbose=False,**kwargs)\n\n        else:\n            samples, intermediates = self.sample(cond=cond, batch_size=batch_size,\n                                                 return_intermediates=True,**kwargs)\n\n        return samples, intermediates\n\n\n    @torch.no_grad()\n    def log_images(self, batch, N=8, n_row=4, sample=True, ddim_steps=200, ddim_eta=1., return_keys=None,\n                   quantize_denoised=True, inpaint=True, plot_denoise_rows=False, plot_progressive_rows=True,\n                   plot_diffusion_rows=True, **kwargs):\n\n        use_ddim = ddim_steps is not None\n\n        log = {}\n        z, c, x, xrec, xc = self.get_input(batch, self.first_stage_key,\n                                           return_first_stage_outputs=True,\n                                           force_c_encode=True,\n                                           return_original_cond=True,\n                                           bs=N)\n        N = min(x.shape[0], N)\n        n_row = min(x.shape[0], n_row)\n        log[\"inputs\"] = x\n        log[\"reconstruction\"] = xrec\n        if self.model.conditioning_key is not None:\n            if hasattr(self.cond_stage_model, \"decode\"):\n                xc = self.cond_stage_model.decode(c)\n                log[\"conditioning\"] = xc\n            elif self.cond_stage_key in [\"caption\"]:\n                xc = log_txt_as_img((x.shape[2], x.shape[3]), batch[\"caption\"])\n                log[\"conditioning\"] = xc\n            elif self.cond_stage_key == 'class_label':\n                xc = log_txt_as_img((x.shape[2], x.shape[3]), batch[\"human_label\"])\n                log['conditioning'] = xc\n            elif isimage(xc):\n                log[\"conditioning\"] = xc\n            if ismap(xc):\n                log[\"original_conditioning\"] = self.to_rgb(xc)\n\n        if plot_diffusion_rows:\n            # get diffusion row\n            diffusion_row = []\n            z_start = z[:n_row]\n            for t in range(self.num_timesteps):\n                if t % self.log_every_t == 0 or t == self.num_timesteps - 1:\n                    t = repeat(torch.tensor([t]), '1 -> b', b=n_row)\n                    t = t.to(self.device).long()\n                    noise = torch.randn_like(z_start)\n                    z_noisy = self.q_sample(x_start=z_start, t=t, noise=noise)\n                    diffusion_row.append(self.decode_first_stage(z_noisy))\n\n            diffusion_row = torch.stack(diffusion_row)  # n_log_step, n_row, C, H, W\n            diffusion_grid = rearrange(diffusion_row, 'n b c h w -> b n c h w')\n            diffusion_grid = rearrange(diffusion_grid, 'b n c h w -> (b n) c h w')\n            diffusion_grid = make_grid(diffusion_grid, nrow=diffusion_row.shape[0])\n            log[\"diffusion_row\"] = diffusion_grid\n\n        if sample:\n            # get denoise row\n            with self.ema_scope(\"Plotting\"):\n                samples, z_denoise_row = self.sample_log(cond=c,batch_size=N,ddim=use_ddim,\n                                                         ddim_steps=ddim_steps,eta=ddim_eta)\n                # samples, z_denoise_row = self.sample(cond=c, batch_size=N, return_intermediates=True)\n            x_samples = self.decode_first_stage(samples)\n            log[\"samples\"] = x_samples\n            if plot_denoise_rows:\n                denoise_grid = self._get_denoise_row_from_list(z_denoise_row)\n                log[\"denoise_row\"] = denoise_grid\n\n            if quantize_denoised and not isinstance(self.first_stage_model, AutoencoderKL) and not isinstance(\n                    self.first_stage_model, IdentityFirstStage):\n                # also display when quantizing x0 while sampling\n                with self.ema_scope(\"Plotting Quantized Denoised\"):\n                    samples, z_denoise_row = self.sample_log(cond=c,batch_size=N,ddim=use_ddim,\n                                                             ddim_steps=ddim_steps,eta=ddim_eta,\n                                                             quantize_denoised=True)\n                    # samples, z_denoise_row = self.sample(cond=c, batch_size=N, return_intermediates=True,\n                    #                                      quantize_denoised=True)\n                x_samples = self.decode_first_stage(samples.to(self.device))\n                log[\"samples_x0_quantized\"] = x_samples\n\n            if inpaint:\n                # make a simple center square\n                h, w = z.shape[2], z.shape[3]\n                mask = torch.ones(N, h, w).to(self.device)\n                # zeros will be filled in\n                mask[:, h // 4:3 * h // 4, w // 4:3 * w // 4] = 0.\n                mask = mask[:, None, ...]\n                with self.ema_scope(\"Plotting Inpaint\"):\n\n                    samples, _ = self.sample_log(cond=c,batch_size=N,ddim=use_ddim, eta=ddim_eta,\n                                                ddim_steps=ddim_steps, x0=z[:N], mask=mask)\n                x_samples = self.decode_first_stage(samples.to(self.device))\n                log[\"samples_inpainting\"] = x_samples\n                log[\"mask\"] = mask\n\n                # outpaint\n                with self.ema_scope(\"Plotting Outpaint\"):\n                    samples, _ = self.sample_log(cond=c, batch_size=N, ddim=use_ddim,eta=ddim_eta,\n                                                ddim_steps=ddim_steps, x0=z[:N], mask=mask)\n                x_samples = self.decode_first_stage(samples.to(self.device))\n                log[\"samples_outpainting\"] = x_samples\n\n        if plot_progressive_rows:\n            with self.ema_scope(\"Plotting Progressives\"):\n                img, progressives = self.progressive_denoising(c,\n                                                               shape=(self.channels, self.image_size, self.image_size),\n                                                               batch_size=N)\n            prog_row = self._get_denoise_row_from_list(progressives, desc=\"Progressive Generation\")\n            log[\"progressive_row\"] = prog_row\n\n        if return_keys:\n            if np.intersect1d(list(log.keys()), return_keys).shape[0] == 0:\n                return log\n            else:\n                return {key: log[key] for key in return_keys}\n        return log\n\n    def configure_optimizers(self):\n        lr = self.learning_rate\n        params = list(self.model.parameters())\n        if self.cond_stage_trainable:\n            print(f\"{self.__class__.__name__}: Also optimizing conditioner params!\")\n            params = params + list(self.cond_stage_model.parameters())\n        if self.learn_logvar:\n            print('Diffusion model optimizing logvar')\n            params.append(self.logvar)\n        opt = torch.optim.AdamW(params, lr=lr)\n        if self.use_scheduler:\n            assert 'target' in self.scheduler_config\n            scheduler = instantiate_from_config(self.scheduler_config)\n\n            print(\"Setting up LambdaLR scheduler...\")\n            scheduler = [\n                {\n                    'scheduler': LambdaLR(opt, lr_lambda=scheduler.schedule),\n                    'interval': 'step',\n                    'frequency': 1\n                }]\n            return [opt], scheduler\n        return opt\n\n    @torch.no_grad()\n    def to_rgb(self, x):\n        x = x.float()\n        if not hasattr(self, \"colorize\"):\n            self.colorize = torch.randn(3, x.shape[1], 1, 1).to(x)\n        x = nn.functional.conv2d(x, weight=self.colorize)\n        x = 2. * (x - x.min()) / (x.max() - x.min()) - 1.\n        return x\n\n\nclass DiffusionWrapperV1(pl.LightningModule):\n    def __init__(self, diff_model_config, conditioning_key):\n        super().__init__()\n        self.diffusion_model = instantiate_from_config(diff_model_config)\n        self.conditioning_key = conditioning_key\n        assert self.conditioning_key in [None, 'concat', 'crossattn', 'hybrid', 'adm']\n\n    def forward(self, x, t, c_concat: list = None, c_crossattn: list = None):\n        if self.conditioning_key is None:\n            out = self.diffusion_model(x, t)\n        elif self.conditioning_key == 'concat':\n            xc = torch.cat([x] + c_concat, dim=1)\n            out = self.diffusion_model(xc, t)\n        elif self.conditioning_key == 'crossattn':\n            cc = torch.cat(c_crossattn, 1)\n            out = self.diffusion_model(x, t, context=cc)\n        elif self.conditioning_key == 'hybrid':\n            xc = torch.cat([x] + c_concat, dim=1)\n            cc = torch.cat(c_crossattn, 1)\n            out = self.diffusion_model(xc, t, context=cc)\n        elif self.conditioning_key == 'adm':\n            cc = c_crossattn[0]\n            out = self.diffusion_model(x, t, y=cc)\n        else:\n            raise NotImplementedError()\n\n        return out\n\n\nclass Layout2ImgDiffusionV1(LatentDiffusionV1):\n    # TODO: move all layout-specific hacks to this class\n    def __init__(self, cond_stage_key, *args, **kwargs):\n        assert cond_stage_key == 'coordinates_bbox', 'Layout2ImgDiffusion only for cond_stage_key=\"coordinates_bbox\"'\n        super().__init__(*args, cond_stage_key=cond_stage_key, **kwargs)\n\n    def log_images(self, batch, N=8, *args, **kwargs):\n        logs = super().log_images(*args, batch=batch, N=N, **kwargs)\n\n        key = 'train' if self.training else 'validation'\n        dset = self.trainer.datamodule.datasets[key]\n        mapper = dset.conditional_builders[self.cond_stage_key]\n\n        bbox_imgs = []\n        map_fn = lambda catno: dset.get_textual_label(dset.get_category_id(catno))\n        for tknzd_bbox in batch[self.cond_stage_key][:N]:\n            bboximg = mapper.plot(tknzd_bbox.detach().cpu(), map_fn, (256, 256))\n            bbox_imgs.append(bboximg)\n\n        cond_img = torch.stack(bbox_imgs, dim=0)\n        logs['bbox_image'] = cond_img\n        return logs\n\nldm.models.diffusion.ddpm.DDPMV1 = DDPMV1\nldm.models.diffusion.ddpm.LatentDiffusionV1 = LatentDiffusionV1\nldm.models.diffusion.ddpm.DiffusionWrapperV1 = DiffusionWrapperV1\nldm.models.diffusion.ddpm.Layout2ImgDiffusionV1 = Layout2ImgDiffusionV1\n", "extensions-builtin/LDSR/ldsr_model_arch.py": "import os\nimport gc\nimport time\n\nimport numpy as np\nimport torch\nimport torchvision\nfrom PIL import Image\nfrom einops import rearrange, repeat\nfrom omegaconf import OmegaConf\nimport safetensors.torch\n\nfrom ldm.models.diffusion.ddim import DDIMSampler\nfrom ldm.util import instantiate_from_config, ismap\nfrom modules import shared, sd_hijack, devices\n\ncached_ldsr_model: torch.nn.Module = None\n\n\n# Create LDSR Class\nclass LDSR:\n    def load_model_from_config(self, half_attention):\n        global cached_ldsr_model\n\n        if shared.opts.ldsr_cached and cached_ldsr_model is not None:\n            print(\"Loading model from cache\")\n            model: torch.nn.Module = cached_ldsr_model\n        else:\n            print(f\"Loading model from {self.modelPath}\")\n            _, extension = os.path.splitext(self.modelPath)\n            if extension.lower() == \".safetensors\":\n                pl_sd = safetensors.torch.load_file(self.modelPath, device=\"cpu\")\n            else:\n                pl_sd = torch.load(self.modelPath, map_location=\"cpu\")\n            sd = pl_sd[\"state_dict\"] if \"state_dict\" in pl_sd else pl_sd\n            config = OmegaConf.load(self.yamlPath)\n            config.model.target = \"ldm.models.diffusion.ddpm.LatentDiffusionV1\"\n            model: torch.nn.Module = instantiate_from_config(config.model)\n            model.load_state_dict(sd, strict=False)\n            model = model.to(shared.device)\n            if half_attention:\n                model = model.half()\n            if shared.cmd_opts.opt_channelslast:\n                model = model.to(memory_format=torch.channels_last)\n\n            sd_hijack.model_hijack.hijack(model) # apply optimization\n            model.eval()\n\n            if shared.opts.ldsr_cached:\n                cached_ldsr_model = model\n\n        return {\"model\": model}\n\n    def __init__(self, model_path, yaml_path):\n        self.modelPath = model_path\n        self.yamlPath = yaml_path\n\n    @staticmethod\n    def run(model, selected_path, custom_steps, eta):\n        example = get_cond(selected_path)\n\n        n_runs = 1\n        guider = None\n        ckwargs = None\n        ddim_use_x0_pred = False\n        temperature = 1.\n        eta = eta\n        custom_shape = None\n\n        height, width = example[\"image\"].shape[1:3]\n        split_input = height >= 128 and width >= 128\n\n        if split_input:\n            ks = 128\n            stride = 64\n            vqf = 4  #\n            model.split_input_params = {\"ks\": (ks, ks), \"stride\": (stride, stride),\n                                        \"vqf\": vqf,\n                                        \"patch_distributed_vq\": True,\n                                        \"tie_braker\": False,\n                                        \"clip_max_weight\": 0.5,\n                                        \"clip_min_weight\": 0.01,\n                                        \"clip_max_tie_weight\": 0.5,\n                                        \"clip_min_tie_weight\": 0.01}\n        else:\n            if hasattr(model, \"split_input_params\"):\n                delattr(model, \"split_input_params\")\n\n        x_t = None\n        logs = None\n        for _ in range(n_runs):\n            if custom_shape is not None:\n                x_t = torch.randn(1, custom_shape[1], custom_shape[2], custom_shape[3]).to(model.device)\n                x_t = repeat(x_t, '1 c h w -> b c h w', b=custom_shape[0])\n\n            logs = make_convolutional_sample(example, model,\n                                             custom_steps=custom_steps,\n                                             eta=eta, quantize_x0=False,\n                                             custom_shape=custom_shape,\n                                             temperature=temperature, noise_dropout=0.,\n                                             corrector=guider, corrector_kwargs=ckwargs, x_T=x_t,\n                                             ddim_use_x0_pred=ddim_use_x0_pred\n                                             )\n        return logs\n\n    def super_resolution(self, image, steps=100, target_scale=2, half_attention=False):\n        model = self.load_model_from_config(half_attention)\n\n        # Run settings\n        diffusion_steps = int(steps)\n        eta = 1.0\n\n\n        gc.collect()\n        devices.torch_gc()\n\n        im_og = image\n        width_og, height_og = im_og.size\n        # If we can adjust the max upscale size, then the 4 below should be our variable\n        down_sample_rate = target_scale / 4\n        wd = width_og * down_sample_rate\n        hd = height_og * down_sample_rate\n        width_downsampled_pre = int(np.ceil(wd))\n        height_downsampled_pre = int(np.ceil(hd))\n\n        if down_sample_rate != 1:\n            print(\n                f'Downsampling from [{width_og}, {height_og}] to [{width_downsampled_pre}, {height_downsampled_pre}]')\n            im_og = im_og.resize((width_downsampled_pre, height_downsampled_pre), Image.LANCZOS)\n        else:\n            print(f\"Down sample rate is 1 from {target_scale} / 4 (Not downsampling)\")\n\n        # pad width and height to multiples of 64, pads with the edge values of image to avoid artifacts\n        pad_w, pad_h = np.max(((2, 2), np.ceil(np.array(im_og.size) / 64).astype(int)), axis=0) * 64 - im_og.size\n        im_padded = Image.fromarray(np.pad(np.array(im_og), ((0, pad_h), (0, pad_w), (0, 0)), mode='edge'))\n\n        logs = self.run(model[\"model\"], im_padded, diffusion_steps, eta)\n\n        sample = logs[\"sample\"]\n        sample = sample.detach().cpu()\n        sample = torch.clamp(sample, -1., 1.)\n        sample = (sample + 1.) / 2. * 255\n        sample = sample.numpy().astype(np.uint8)\n        sample = np.transpose(sample, (0, 2, 3, 1))\n        a = Image.fromarray(sample[0])\n\n        # remove padding\n        a = a.crop((0, 0) + tuple(np.array(im_og.size) * 4))\n\n        del model\n        gc.collect()\n        devices.torch_gc()\n\n        return a\n\n\ndef get_cond(selected_path):\n    example = {}\n    up_f = 4\n    c = selected_path.convert('RGB')\n    c = torch.unsqueeze(torchvision.transforms.ToTensor()(c), 0)\n    c_up = torchvision.transforms.functional.resize(c, size=[up_f * c.shape[2], up_f * c.shape[3]],\n                                                    antialias=True)\n    c_up = rearrange(c_up, '1 c h w -> 1 h w c')\n    c = rearrange(c, '1 c h w -> 1 h w c')\n    c = 2. * c - 1.\n\n    c = c.to(shared.device)\n    example[\"LR_image\"] = c\n    example[\"image\"] = c_up\n\n    return example\n\n\n@torch.no_grad()\ndef convsample_ddim(model, cond, steps, shape, eta=1.0, callback=None, normals_sequence=None,\n                    mask=None, x0=None, quantize_x0=False, temperature=1., score_corrector=None,\n                    corrector_kwargs=None, x_t=None\n                    ):\n    ddim = DDIMSampler(model)\n    bs = shape[0]\n    shape = shape[1:]\n    print(f\"Sampling with eta = {eta}; steps: {steps}\")\n    samples, intermediates = ddim.sample(steps, batch_size=bs, shape=shape, conditioning=cond, callback=callback,\n                                         normals_sequence=normals_sequence, quantize_x0=quantize_x0, eta=eta,\n                                         mask=mask, x0=x0, temperature=temperature, verbose=False,\n                                         score_corrector=score_corrector,\n                                         corrector_kwargs=corrector_kwargs, x_t=x_t)\n\n    return samples, intermediates\n\n\n@torch.no_grad()\ndef make_convolutional_sample(batch, model, custom_steps=None, eta=1.0, quantize_x0=False, custom_shape=None, temperature=1., noise_dropout=0., corrector=None,\n                              corrector_kwargs=None, x_T=None, ddim_use_x0_pred=False):\n    log = {}\n\n    z, c, x, xrec, xc = model.get_input(batch, model.first_stage_key,\n                                        return_first_stage_outputs=True,\n                                        force_c_encode=not (hasattr(model, 'split_input_params')\n                                                            and model.cond_stage_key == 'coordinates_bbox'),\n                                        return_original_cond=True)\n\n    if custom_shape is not None:\n        z = torch.randn(custom_shape)\n        print(f\"Generating {custom_shape[0]} samples of shape {custom_shape[1:]}\")\n\n    z0 = None\n\n    log[\"input\"] = x\n    log[\"reconstruction\"] = xrec\n\n    if ismap(xc):\n        log[\"original_conditioning\"] = model.to_rgb(xc)\n        if hasattr(model, 'cond_stage_key'):\n            log[model.cond_stage_key] = model.to_rgb(xc)\n\n    else:\n        log[\"original_conditioning\"] = xc if xc is not None else torch.zeros_like(x)\n        if model.cond_stage_model:\n            log[model.cond_stage_key] = xc if xc is not None else torch.zeros_like(x)\n            if model.cond_stage_key == 'class_label':\n                log[model.cond_stage_key] = xc[model.cond_stage_key]\n\n    with model.ema_scope(\"Plotting\"):\n        t0 = time.time()\n\n        sample, intermediates = convsample_ddim(model, c, steps=custom_steps, shape=z.shape,\n                                                eta=eta,\n                                                quantize_x0=quantize_x0, mask=None, x0=z0,\n                                                temperature=temperature, score_corrector=corrector, corrector_kwargs=corrector_kwargs,\n                                                x_t=x_T)\n        t1 = time.time()\n\n        if ddim_use_x0_pred:\n            sample = intermediates['pred_x0'][-1]\n\n    x_sample = model.decode_first_stage(sample)\n\n    try:\n        x_sample_noquant = model.decode_first_stage(sample, force_not_quantize=True)\n        log[\"sample_noquant\"] = x_sample_noquant\n        log[\"sample_diff\"] = torch.abs(x_sample_noquant - x_sample)\n    except Exception:\n        pass\n\n    log[\"sample\"] = x_sample\n    log[\"time\"] = t1 - t0\n\n    return log\n", "extensions-builtin/LDSR/vqvae_quantize.py": "# Vendored from https://raw.githubusercontent.com/CompVis/taming-transformers/24268930bf1dce879235a7fddd0b2355b84d7ea6/taming/modules/vqvae/quantize.py,\n# where the license is as follows:\n#\n# Copyright (c) 2020 Patrick Esser and Robin Rombach and Bj\u00f6rn Ommer\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the \"Software\"), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in all\n# copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\n# EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF\n# MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.\n# IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM,\n# DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR\n# OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE\n# OR OTHER DEALINGS IN THE SOFTWARE./\n\nimport torch\nimport torch.nn as nn\nimport numpy as np\nfrom einops import rearrange\n\n\nclass VectorQuantizer2(nn.Module):\n    \"\"\"\n    Improved version over VectorQuantizer, can be used as a drop-in replacement. Mostly\n    avoids costly matrix multiplications and allows for post-hoc remapping of indices.\n    \"\"\"\n\n    # NOTE: due to a bug the beta term was applied to the wrong term. for\n    # backwards compatibility we use the buggy version by default, but you can\n    # specify legacy=False to fix it.\n    def __init__(self, n_e, e_dim, beta, remap=None, unknown_index=\"random\",\n                 sane_index_shape=False, legacy=True):\n        super().__init__()\n        self.n_e = n_e\n        self.e_dim = e_dim\n        self.beta = beta\n        self.legacy = legacy\n\n        self.embedding = nn.Embedding(self.n_e, self.e_dim)\n        self.embedding.weight.data.uniform_(-1.0 / self.n_e, 1.0 / self.n_e)\n\n        self.remap = remap\n        if self.remap is not None:\n            self.register_buffer(\"used\", torch.tensor(np.load(self.remap)))\n            self.re_embed = self.used.shape[0]\n            self.unknown_index = unknown_index  # \"random\" or \"extra\" or integer\n            if self.unknown_index == \"extra\":\n                self.unknown_index = self.re_embed\n                self.re_embed = self.re_embed + 1\n            print(f\"Remapping {self.n_e} indices to {self.re_embed} indices. \"\n                  f\"Using {self.unknown_index} for unknown indices.\")\n        else:\n            self.re_embed = n_e\n\n        self.sane_index_shape = sane_index_shape\n\n    def remap_to_used(self, inds):\n        ishape = inds.shape\n        assert len(ishape) > 1\n        inds = inds.reshape(ishape[0], -1)\n        used = self.used.to(inds)\n        match = (inds[:, :, None] == used[None, None, ...]).long()\n        new = match.argmax(-1)\n        unknown = match.sum(2) < 1\n        if self.unknown_index == \"random\":\n            new[unknown] = torch.randint(0, self.re_embed, size=new[unknown].shape).to(device=new.device)\n        else:\n            new[unknown] = self.unknown_index\n        return new.reshape(ishape)\n\n    def unmap_to_all(self, inds):\n        ishape = inds.shape\n        assert len(ishape) > 1\n        inds = inds.reshape(ishape[0], -1)\n        used = self.used.to(inds)\n        if self.re_embed > self.used.shape[0]:  # extra token\n            inds[inds >= self.used.shape[0]] = 0  # simply set to zero\n        back = torch.gather(used[None, :][inds.shape[0] * [0], :], 1, inds)\n        return back.reshape(ishape)\n\n    def forward(self, z, temp=None, rescale_logits=False, return_logits=False):\n        assert temp is None or temp == 1.0, \"Only for interface compatible with Gumbel\"\n        assert rescale_logits is False, \"Only for interface compatible with Gumbel\"\n        assert return_logits is False, \"Only for interface compatible with Gumbel\"\n        # reshape z -> (batch, height, width, channel) and flatten\n        z = rearrange(z, 'b c h w -> b h w c').contiguous()\n        z_flattened = z.view(-1, self.e_dim)\n        # distances from z to embeddings e_j (z - e)^2 = z^2 + e^2 - 2 e * z\n\n        d = torch.sum(z_flattened ** 2, dim=1, keepdim=True) + \\\n            torch.sum(self.embedding.weight ** 2, dim=1) - 2 * \\\n            torch.einsum('bd,dn->bn', z_flattened, rearrange(self.embedding.weight, 'n d -> d n'))\n\n        min_encoding_indices = torch.argmin(d, dim=1)\n        z_q = self.embedding(min_encoding_indices).view(z.shape)\n        perplexity = None\n        min_encodings = None\n\n        # compute loss for embedding\n        if not self.legacy:\n            loss = self.beta * torch.mean((z_q.detach() - z) ** 2) + \\\n                   torch.mean((z_q - z.detach()) ** 2)\n        else:\n            loss = torch.mean((z_q.detach() - z) ** 2) + self.beta * \\\n                   torch.mean((z_q - z.detach()) ** 2)\n\n        # preserve gradients\n        z_q = z + (z_q - z).detach()\n\n        # reshape back to match original input shape\n        z_q = rearrange(z_q, 'b h w c -> b c h w').contiguous()\n\n        if self.remap is not None:\n            min_encoding_indices = min_encoding_indices.reshape(z.shape[0], -1)  # add batch axis\n            min_encoding_indices = self.remap_to_used(min_encoding_indices)\n            min_encoding_indices = min_encoding_indices.reshape(-1, 1)  # flatten\n\n        if self.sane_index_shape:\n            min_encoding_indices = min_encoding_indices.reshape(\n                z_q.shape[0], z_q.shape[2], z_q.shape[3])\n\n        return z_q, loss, (perplexity, min_encodings, min_encoding_indices)\n\n    def get_codebook_entry(self, indices, shape):\n        # shape specifying (batch, height, width, channel)\n        if self.remap is not None:\n            indices = indices.reshape(shape[0], -1)  # add batch axis\n            indices = self.unmap_to_all(indices)\n            indices = indices.reshape(-1)  # flatten again\n\n        # get quantized latent vectors\n        z_q = self.embedding(indices)\n\n        if shape is not None:\n            z_q = z_q.view(shape)\n            # reshape back to match original input shape\n            z_q = z_q.permute(0, 3, 1, 2).contiguous()\n\n        return z_q\n", "extensions-builtin/LDSR/preload.py": "import os\nfrom modules import paths\n\n\ndef preload(parser):\n    parser.add_argument(\"--ldsr-models-path\", type=str, help=\"Path to directory with LDSR model file(s).\", default=os.path.join(paths.models_path, 'LDSR'))\n", "extensions-builtin/LDSR/sd_hijack_autoencoder.py": "# The content of this file comes from the ldm/models/autoencoder.py file of the compvis/stable-diffusion repo\n# The VQModel & VQModelInterface were subsequently removed from ldm/models/autoencoder.py when we moved to the stability-ai/stablediffusion repo\n# As the LDSR upscaler relies on VQModel & VQModelInterface, the hijack aims to put them back into the ldm.models.autoencoder\nimport numpy as np\nimport torch\nimport pytorch_lightning as pl\nimport torch.nn.functional as F\nfrom contextlib import contextmanager\n\nfrom torch.optim.lr_scheduler import LambdaLR\n\nfrom ldm.modules.ema import LitEma\nfrom vqvae_quantize import VectorQuantizer2 as VectorQuantizer\nfrom ldm.modules.diffusionmodules.model import Encoder, Decoder\nfrom ldm.util import instantiate_from_config\n\nimport ldm.models.autoencoder\nfrom packaging import version\n\nclass VQModel(pl.LightningModule):\n    def __init__(self,\n                 ddconfig,\n                 lossconfig,\n                 n_embed,\n                 embed_dim,\n                 ckpt_path=None,\n                 ignore_keys=None,\n                 image_key=\"image\",\n                 colorize_nlabels=None,\n                 monitor=None,\n                 batch_resize_range=None,\n                 scheduler_config=None,\n                 lr_g_factor=1.0,\n                 remap=None,\n                 sane_index_shape=False, # tell vector quantizer to return indices as bhw\n                 use_ema=False\n                 ):\n        super().__init__()\n        self.embed_dim = embed_dim\n        self.n_embed = n_embed\n        self.image_key = image_key\n        self.encoder = Encoder(**ddconfig)\n        self.decoder = Decoder(**ddconfig)\n        self.loss = instantiate_from_config(lossconfig)\n        self.quantize = VectorQuantizer(n_embed, embed_dim, beta=0.25,\n                                        remap=remap,\n                                        sane_index_shape=sane_index_shape)\n        self.quant_conv = torch.nn.Conv2d(ddconfig[\"z_channels\"], embed_dim, 1)\n        self.post_quant_conv = torch.nn.Conv2d(embed_dim, ddconfig[\"z_channels\"], 1)\n        if colorize_nlabels is not None:\n            assert type(colorize_nlabels)==int\n            self.register_buffer(\"colorize\", torch.randn(3, colorize_nlabels, 1, 1))\n        if monitor is not None:\n            self.monitor = monitor\n        self.batch_resize_range = batch_resize_range\n        if self.batch_resize_range is not None:\n            print(f\"{self.__class__.__name__}: Using per-batch resizing in range {batch_resize_range}.\")\n\n        self.use_ema = use_ema\n        if self.use_ema:\n            self.model_ema = LitEma(self)\n            print(f\"Keeping EMAs of {len(list(self.model_ema.buffers()))}.\")\n\n        if ckpt_path is not None:\n            self.init_from_ckpt(ckpt_path, ignore_keys=ignore_keys or [])\n        self.scheduler_config = scheduler_config\n        self.lr_g_factor = lr_g_factor\n\n    @contextmanager\n    def ema_scope(self, context=None):\n        if self.use_ema:\n            self.model_ema.store(self.parameters())\n            self.model_ema.copy_to(self)\n            if context is not None:\n                print(f\"{context}: Switched to EMA weights\")\n        try:\n            yield None\n        finally:\n            if self.use_ema:\n                self.model_ema.restore(self.parameters())\n                if context is not None:\n                    print(f\"{context}: Restored training weights\")\n\n    def init_from_ckpt(self, path, ignore_keys=None):\n        sd = torch.load(path, map_location=\"cpu\")[\"state_dict\"]\n        keys = list(sd.keys())\n        for k in keys:\n            for ik in ignore_keys or []:\n                if k.startswith(ik):\n                    print(\"Deleting key {} from state_dict.\".format(k))\n                    del sd[k]\n        missing, unexpected = self.load_state_dict(sd, strict=False)\n        print(f\"Restored from {path} with {len(missing)} missing and {len(unexpected)} unexpected keys\")\n        if missing:\n            print(f\"Missing Keys: {missing}\")\n        if unexpected:\n            print(f\"Unexpected Keys: {unexpected}\")\n\n    def on_train_batch_end(self, *args, **kwargs):\n        if self.use_ema:\n            self.model_ema(self)\n\n    def encode(self, x):\n        h = self.encoder(x)\n        h = self.quant_conv(h)\n        quant, emb_loss, info = self.quantize(h)\n        return quant, emb_loss, info\n\n    def encode_to_prequant(self, x):\n        h = self.encoder(x)\n        h = self.quant_conv(h)\n        return h\n\n    def decode(self, quant):\n        quant = self.post_quant_conv(quant)\n        dec = self.decoder(quant)\n        return dec\n\n    def decode_code(self, code_b):\n        quant_b = self.quantize.embed_code(code_b)\n        dec = self.decode(quant_b)\n        return dec\n\n    def forward(self, input, return_pred_indices=False):\n        quant, diff, (_,_,ind) = self.encode(input)\n        dec = self.decode(quant)\n        if return_pred_indices:\n            return dec, diff, ind\n        return dec, diff\n\n    def get_input(self, batch, k):\n        x = batch[k]\n        if len(x.shape) == 3:\n            x = x[..., None]\n        x = x.permute(0, 3, 1, 2).to(memory_format=torch.contiguous_format).float()\n        if self.batch_resize_range is not None:\n            lower_size = self.batch_resize_range[0]\n            upper_size = self.batch_resize_range[1]\n            if self.global_step <= 4:\n                # do the first few batches with max size to avoid later oom\n                new_resize = upper_size\n            else:\n                new_resize = np.random.choice(np.arange(lower_size, upper_size+16, 16))\n            if new_resize != x.shape[2]:\n                x = F.interpolate(x, size=new_resize, mode=\"bicubic\")\n            x = x.detach()\n        return x\n\n    def training_step(self, batch, batch_idx, optimizer_idx):\n        # https://github.com/pytorch/pytorch/issues/37142\n        # try not to fool the heuristics\n        x = self.get_input(batch, self.image_key)\n        xrec, qloss, ind = self(x, return_pred_indices=True)\n\n        if optimizer_idx == 0:\n            # autoencode\n            aeloss, log_dict_ae = self.loss(qloss, x, xrec, optimizer_idx, self.global_step,\n                                            last_layer=self.get_last_layer(), split=\"train\",\n                                            predicted_indices=ind)\n\n            self.log_dict(log_dict_ae, prog_bar=False, logger=True, on_step=True, on_epoch=True)\n            return aeloss\n\n        if optimizer_idx == 1:\n            # discriminator\n            discloss, log_dict_disc = self.loss(qloss, x, xrec, optimizer_idx, self.global_step,\n                                            last_layer=self.get_last_layer(), split=\"train\")\n            self.log_dict(log_dict_disc, prog_bar=False, logger=True, on_step=True, on_epoch=True)\n            return discloss\n\n    def validation_step(self, batch, batch_idx):\n        log_dict = self._validation_step(batch, batch_idx)\n        with self.ema_scope():\n            self._validation_step(batch, batch_idx, suffix=\"_ema\")\n        return log_dict\n\n    def _validation_step(self, batch, batch_idx, suffix=\"\"):\n        x = self.get_input(batch, self.image_key)\n        xrec, qloss, ind = self(x, return_pred_indices=True)\n        aeloss, log_dict_ae = self.loss(qloss, x, xrec, 0,\n                                        self.global_step,\n                                        last_layer=self.get_last_layer(),\n                                        split=\"val\"+suffix,\n                                        predicted_indices=ind\n                                        )\n\n        discloss, log_dict_disc = self.loss(qloss, x, xrec, 1,\n                                            self.global_step,\n                                            last_layer=self.get_last_layer(),\n                                            split=\"val\"+suffix,\n                                            predicted_indices=ind\n                                            )\n        rec_loss = log_dict_ae[f\"val{suffix}/rec_loss\"]\n        self.log(f\"val{suffix}/rec_loss\", rec_loss,\n                   prog_bar=True, logger=True, on_step=False, on_epoch=True, sync_dist=True)\n        self.log(f\"val{suffix}/aeloss\", aeloss,\n                   prog_bar=True, logger=True, on_step=False, on_epoch=True, sync_dist=True)\n        if version.parse(pl.__version__) >= version.parse('1.4.0'):\n            del log_dict_ae[f\"val{suffix}/rec_loss\"]\n        self.log_dict(log_dict_ae)\n        self.log_dict(log_dict_disc)\n        return self.log_dict\n\n    def configure_optimizers(self):\n        lr_d = self.learning_rate\n        lr_g = self.lr_g_factor*self.learning_rate\n        print(\"lr_d\", lr_d)\n        print(\"lr_g\", lr_g)\n        opt_ae = torch.optim.Adam(list(self.encoder.parameters())+\n                                  list(self.decoder.parameters())+\n                                  list(self.quantize.parameters())+\n                                  list(self.quant_conv.parameters())+\n                                  list(self.post_quant_conv.parameters()),\n                                  lr=lr_g, betas=(0.5, 0.9))\n        opt_disc = torch.optim.Adam(self.loss.discriminator.parameters(),\n                                    lr=lr_d, betas=(0.5, 0.9))\n\n        if self.scheduler_config is not None:\n            scheduler = instantiate_from_config(self.scheduler_config)\n\n            print(\"Setting up LambdaLR scheduler...\")\n            scheduler = [\n                {\n                    'scheduler': LambdaLR(opt_ae, lr_lambda=scheduler.schedule),\n                    'interval': 'step',\n                    'frequency': 1\n                },\n                {\n                    'scheduler': LambdaLR(opt_disc, lr_lambda=scheduler.schedule),\n                    'interval': 'step',\n                    'frequency': 1\n                },\n            ]\n            return [opt_ae, opt_disc], scheduler\n        return [opt_ae, opt_disc], []\n\n    def get_last_layer(self):\n        return self.decoder.conv_out.weight\n\n    def log_images(self, batch, only_inputs=False, plot_ema=False, **kwargs):\n        log = {}\n        x = self.get_input(batch, self.image_key)\n        x = x.to(self.device)\n        if only_inputs:\n            log[\"inputs\"] = x\n            return log\n        xrec, _ = self(x)\n        if x.shape[1] > 3:\n            # colorize with random projection\n            assert xrec.shape[1] > 3\n            x = self.to_rgb(x)\n            xrec = self.to_rgb(xrec)\n        log[\"inputs\"] = x\n        log[\"reconstructions\"] = xrec\n        if plot_ema:\n            with self.ema_scope():\n                xrec_ema, _ = self(x)\n                if x.shape[1] > 3:\n                    xrec_ema = self.to_rgb(xrec_ema)\n                log[\"reconstructions_ema\"] = xrec_ema\n        return log\n\n    def to_rgb(self, x):\n        assert self.image_key == \"segmentation\"\n        if not hasattr(self, \"colorize\"):\n            self.register_buffer(\"colorize\", torch.randn(3, x.shape[1], 1, 1).to(x))\n        x = F.conv2d(x, weight=self.colorize)\n        x = 2.*(x-x.min())/(x.max()-x.min()) - 1.\n        return x\n\n\nclass VQModelInterface(VQModel):\n    def __init__(self, embed_dim, *args, **kwargs):\n        super().__init__(*args, embed_dim=embed_dim, **kwargs)\n        self.embed_dim = embed_dim\n\n    def encode(self, x):\n        h = self.encoder(x)\n        h = self.quant_conv(h)\n        return h\n\n    def decode(self, h, force_not_quantize=False):\n        # also go through quantization layer\n        if not force_not_quantize:\n            quant, emb_loss, info = self.quantize(h)\n        else:\n            quant = h\n        quant = self.post_quant_conv(quant)\n        dec = self.decoder(quant)\n        return dec\n\nldm.models.autoencoder.VQModel = VQModel\nldm.models.autoencoder.VQModelInterface = VQModelInterface\n", "extensions-builtin/LDSR/scripts/ldsr_model.py": "import os\n\nfrom modules.modelloader import load_file_from_url\nfrom modules.upscaler import Upscaler, UpscalerData\nfrom ldsr_model_arch import LDSR\nfrom modules import shared, script_callbacks, errors\nimport sd_hijack_autoencoder  # noqa: F401\nimport sd_hijack_ddpm_v1  # noqa: F401\n\n\nclass UpscalerLDSR(Upscaler):\n    def __init__(self, user_path):\n        self.name = \"LDSR\"\n        self.user_path = user_path\n        self.model_url = \"https://heibox.uni-heidelberg.de/f/578df07c8fc04ffbadf3/?dl=1\"\n        self.yaml_url = \"https://heibox.uni-heidelberg.de/f/31a76b13ea27482981b4/?dl=1\"\n        super().__init__()\n        scaler_data = UpscalerData(\"LDSR\", None, self)\n        self.scalers = [scaler_data]\n\n    def load_model(self, path: str):\n        # Remove incorrect project.yaml file if too big\n        yaml_path = os.path.join(self.model_path, \"project.yaml\")\n        old_model_path = os.path.join(self.model_path, \"model.pth\")\n        new_model_path = os.path.join(self.model_path, \"model.ckpt\")\n\n        local_model_paths = self.find_models(ext_filter=[\".ckpt\", \".safetensors\"])\n        local_ckpt_path = next(iter([local_model for local_model in local_model_paths if local_model.endswith(\"model.ckpt\")]), None)\n        local_safetensors_path = next(iter([local_model for local_model in local_model_paths if local_model.endswith(\"model.safetensors\")]), None)\n        local_yaml_path = next(iter([local_model for local_model in local_model_paths if local_model.endswith(\"project.yaml\")]), None)\n\n        if os.path.exists(yaml_path):\n            statinfo = os.stat(yaml_path)\n            if statinfo.st_size >= 10485760:\n                print(\"Removing invalid LDSR YAML file.\")\n                os.remove(yaml_path)\n\n        if os.path.exists(old_model_path):\n            print(\"Renaming model from model.pth to model.ckpt\")\n            os.rename(old_model_path, new_model_path)\n\n        if local_safetensors_path is not None and os.path.exists(local_safetensors_path):\n            model = local_safetensors_path\n        else:\n            model = local_ckpt_path or load_file_from_url(self.model_url, model_dir=self.model_download_path, file_name=\"model.ckpt\")\n\n        yaml = local_yaml_path or load_file_from_url(self.yaml_url, model_dir=self.model_download_path, file_name=\"project.yaml\")\n\n        return LDSR(model, yaml)\n\n    def do_upscale(self, img, path):\n        try:\n            ldsr = self.load_model(path)\n        except Exception:\n            errors.report(f\"Failed loading LDSR model {path}\", exc_info=True)\n            return img\n        ddim_steps = shared.opts.ldsr_steps\n        return ldsr.super_resolution(img, ddim_steps, self.scale)\n\n\ndef on_ui_settings():\n    import gradio as gr\n\n    shared.opts.add_option(\"ldsr_steps\", shared.OptionInfo(100, \"LDSR processing steps. Lower = faster\", gr.Slider, {\"minimum\": 1, \"maximum\": 200, \"step\": 1}, section=('upscaling', \"Upscaling\")))\n    shared.opts.add_option(\"ldsr_cached\", shared.OptionInfo(False, \"Cache LDSR model in memory\", gr.Checkbox, {\"interactive\": True}, section=('upscaling', \"Upscaling\")))\n\n\nscript_callbacks.on_ui_settings(on_ui_settings)\n", "extensions-builtin/soft-inpainting/scripts/soft_inpainting.py": "import numpy as np\nimport gradio as gr\nimport math\nfrom modules.ui_components import InputAccordion\nimport modules.scripts as scripts\n\n\nclass SoftInpaintingSettings:\n    def __init__(self,\n                 mask_blend_power,\n                 mask_blend_scale,\n                 inpaint_detail_preservation,\n                 composite_mask_influence,\n                 composite_difference_threshold,\n                 composite_difference_contrast):\n        self.mask_blend_power = mask_blend_power\n        self.mask_blend_scale = mask_blend_scale\n        self.inpaint_detail_preservation = inpaint_detail_preservation\n        self.composite_mask_influence = composite_mask_influence\n        self.composite_difference_threshold = composite_difference_threshold\n        self.composite_difference_contrast = composite_difference_contrast\n\n    def add_generation_params(self, dest):\n        dest[enabled_gen_param_label] = True\n        dest[gen_param_labels.mask_blend_power] = self.mask_blend_power\n        dest[gen_param_labels.mask_blend_scale] = self.mask_blend_scale\n        dest[gen_param_labels.inpaint_detail_preservation] = self.inpaint_detail_preservation\n        dest[gen_param_labels.composite_mask_influence] = self.composite_mask_influence\n        dest[gen_param_labels.composite_difference_threshold] = self.composite_difference_threshold\n        dest[gen_param_labels.composite_difference_contrast] = self.composite_difference_contrast\n\n\n# ------------------- Methods -------------------\n\ndef processing_uses_inpainting(p):\n    # TODO: Figure out a better way to determine if inpainting is being used by p\n    if getattr(p, \"image_mask\", None) is not None:\n        return True\n\n    if getattr(p, \"mask\", None) is not None:\n        return True\n\n    if getattr(p, \"nmask\", None) is not None:\n        return True\n\n    return False\n\n\ndef latent_blend(settings, a, b, t):\n    \"\"\"\n    Interpolates two latent image representations according to the parameter t,\n    where the interpolated vectors' magnitudes are also interpolated separately.\n    The \"detail_preservation\" factor biases the magnitude interpolation towards\n    the larger of the two magnitudes.\n    \"\"\"\n    import torch\n\n    # NOTE: We use inplace operations wherever possible.\n\n    if len(t.shape) == 3:\n        # [4][w][h] to [1][4][w][h]\n        t2 = t.unsqueeze(0)\n        # [4][w][h] to [1][1][w][h] - the [4] seem redundant.\n        t3 = t[0].unsqueeze(0).unsqueeze(0)\n    else:\n        t2 = t\n        t3 = t[:, 0][:, None]\n\n    one_minus_t2 = 1 - t2\n    one_minus_t3 = 1 - t3\n\n    # Linearly interpolate the image vectors.\n    a_scaled = a * one_minus_t2\n    b_scaled = b * t2\n    image_interp = a_scaled\n    image_interp.add_(b_scaled)\n    result_type = image_interp.dtype\n    del a_scaled, b_scaled, t2, one_minus_t2\n\n    # Calculate the magnitude of the interpolated vectors. (We will remove this magnitude.)\n    # 64-bit operations are used here to allow large exponents.\n    current_magnitude = torch.norm(image_interp, p=2, dim=1, keepdim=True).to(torch.float64).add_(0.00001)\n\n    # Interpolate the powered magnitudes, then un-power them (bring them back to a power of 1).\n    a_magnitude = torch.norm(a, p=2, dim=1, keepdim=True).to(torch.float64).pow_(\n        settings.inpaint_detail_preservation) * one_minus_t3\n    b_magnitude = torch.norm(b, p=2, dim=1, keepdim=True).to(torch.float64).pow_(\n        settings.inpaint_detail_preservation) * t3\n    desired_magnitude = a_magnitude\n    desired_magnitude.add_(b_magnitude).pow_(1 / settings.inpaint_detail_preservation)\n    del a_magnitude, b_magnitude, t3, one_minus_t3\n\n    # Change the linearly interpolated image vectors' magnitudes to the value we want.\n    # This is the last 64-bit operation.\n    image_interp_scaling_factor = desired_magnitude\n    image_interp_scaling_factor.div_(current_magnitude)\n    image_interp_scaling_factor = image_interp_scaling_factor.to(result_type)\n    image_interp_scaled = image_interp\n    image_interp_scaled.mul_(image_interp_scaling_factor)\n    del current_magnitude\n    del desired_magnitude\n    del image_interp\n    del image_interp_scaling_factor\n    del result_type\n\n    return image_interp_scaled\n\n\ndef get_modified_nmask(settings, nmask, sigma):\n    \"\"\"\n    Converts a negative mask representing the transparency of the original latent vectors being overlaid\n    to a mask that is scaled according to the denoising strength for this step.\n\n    Where:\n        0 = fully opaque, infinite density, fully masked\n        1 = fully transparent, zero density, fully unmasked\n\n    We bring this transparency to a power, as this allows one to simulate N number of blending operations\n    where N can be any positive real value. Using this one can control the balance of influence between\n    the denoiser and the original latents according to the sigma value.\n\n    NOTE: \"mask\" is not used\n    \"\"\"\n    import torch\n    return torch.pow(nmask, (sigma ** settings.mask_blend_power) * settings.mask_blend_scale)\n\n\ndef apply_adaptive_masks(\n        settings: SoftInpaintingSettings,\n        nmask,\n        latent_orig,\n        latent_processed,\n        overlay_images,\n        width, height,\n        paste_to):\n    import torch\n    import modules.processing as proc\n    import modules.images as images\n    from PIL import Image, ImageOps, ImageFilter\n\n    # TODO: Bias the blending according to the latent mask, add adjustable parameter for bias control.\n    if len(nmask.shape) == 3:\n        latent_mask = nmask[0].float()\n    else:\n        latent_mask = nmask[:, 0].float()\n    # convert the original mask into a form we use to scale distances for thresholding\n    mask_scalar = 1 - (torch.clamp(latent_mask, min=0, max=1) ** (settings.mask_blend_scale / 2))\n    mask_scalar = (0.5 * (1 - settings.composite_mask_influence)\n                   + mask_scalar * settings.composite_mask_influence)\n    mask_scalar = mask_scalar / (1.00001 - mask_scalar)\n    mask_scalar = mask_scalar.cpu().numpy()\n\n    latent_distance = torch.norm(latent_processed - latent_orig, p=2, dim=1)\n\n    kernel, kernel_center = get_gaussian_kernel(stddev_radius=1.5, max_radius=2)\n\n    masks_for_overlay = []\n\n    for i, (distance_map, overlay_image) in enumerate(zip(latent_distance, overlay_images)):\n        converted_mask = distance_map.float().cpu().numpy()\n        converted_mask = weighted_histogram_filter(converted_mask, kernel, kernel_center,\n                                                   percentile_min=0.9, percentile_max=1, min_width=1)\n        converted_mask = weighted_histogram_filter(converted_mask, kernel, kernel_center,\n                                                   percentile_min=0.25, percentile_max=0.75, min_width=1)\n\n        # The distance at which opacity of original decreases to 50%\n        if len(mask_scalar.shape) == 3:\n            if mask_scalar.shape[0] > i:\n                half_weighted_distance = settings.composite_difference_threshold * mask_scalar[i]\n            else:\n                half_weighted_distance = settings.composite_difference_threshold * mask_scalar[0]\n        else:\n            half_weighted_distance = settings.composite_difference_threshold * mask_scalar\n\n        converted_mask = converted_mask / half_weighted_distance\n\n        converted_mask = 1 / (1 + converted_mask ** settings.composite_difference_contrast)\n        converted_mask = smootherstep(converted_mask)\n        converted_mask = 1 - converted_mask\n        converted_mask = 255. * converted_mask\n        converted_mask = converted_mask.astype(np.uint8)\n        converted_mask = Image.fromarray(converted_mask)\n        converted_mask = images.resize_image(2, converted_mask, width, height)\n        converted_mask = proc.create_binary_mask(converted_mask, round=False)\n\n        # Remove aliasing artifacts using a gaussian blur.\n        converted_mask = converted_mask.filter(ImageFilter.GaussianBlur(radius=4))\n\n        # Expand the mask to fit the whole image if needed.\n        if paste_to is not None:\n            converted_mask = proc.uncrop(converted_mask,\n                                         (overlay_image.width, overlay_image.height),\n                                         paste_to)\n\n        masks_for_overlay.append(converted_mask)\n\n        image_masked = Image.new('RGBa', (overlay_image.width, overlay_image.height))\n        image_masked.paste(overlay_image.convert(\"RGBA\").convert(\"RGBa\"),\n                           mask=ImageOps.invert(converted_mask.convert('L')))\n\n        overlay_images[i] = image_masked.convert('RGBA')\n\n    return masks_for_overlay\n\n\ndef apply_masks(\n        settings,\n        nmask,\n        overlay_images,\n        width, height,\n        paste_to):\n    import torch\n    import modules.processing as proc\n    import modules.images as images\n    from PIL import Image, ImageOps, ImageFilter\n\n    converted_mask = nmask[0].float()\n    converted_mask = torch.clamp(converted_mask, min=0, max=1).pow_(settings.mask_blend_scale / 2)\n    converted_mask = 255. * converted_mask\n    converted_mask = converted_mask.cpu().numpy().astype(np.uint8)\n    converted_mask = Image.fromarray(converted_mask)\n    converted_mask = images.resize_image(2, converted_mask, width, height)\n    converted_mask = proc.create_binary_mask(converted_mask, round=False)\n\n    # Remove aliasing artifacts using a gaussian blur.\n    converted_mask = converted_mask.filter(ImageFilter.GaussianBlur(radius=4))\n\n    # Expand the mask to fit the whole image if needed.\n    if paste_to is not None:\n        converted_mask = proc.uncrop(converted_mask,\n                                     (width, height),\n                                     paste_to)\n\n    masks_for_overlay = []\n\n    for i, overlay_image in enumerate(overlay_images):\n        masks_for_overlay[i] = converted_mask\n\n        image_masked = Image.new('RGBa', (overlay_image.width, overlay_image.height))\n        image_masked.paste(overlay_image.convert(\"RGBA\").convert(\"RGBa\"),\n                           mask=ImageOps.invert(converted_mask.convert('L')))\n\n        overlay_images[i] = image_masked.convert('RGBA')\n\n    return masks_for_overlay\n\n\ndef weighted_histogram_filter(img, kernel, kernel_center, percentile_min=0.0, percentile_max=1.0, min_width=1.0):\n    \"\"\"\n    Generalization convolution filter capable of applying\n    weighted mean, median, maximum, and minimum filters\n    parametrically using an arbitrary kernel.\n\n    Args:\n        img (nparray):\n            The image, a 2-D array of floats, to which the filter is being applied.\n        kernel (nparray):\n            The kernel, a 2-D array of floats.\n        kernel_center (nparray):\n            The kernel center coordinate, a 1-D array with two elements.\n        percentile_min (float):\n            The lower bound of the histogram window used by the filter,\n            from 0 to 1.\n        percentile_max (float):\n            The upper bound of the histogram window used by the filter,\n            from 0 to 1.\n        min_width (float):\n            The minimum size of the histogram window bounds, in weight units.\n            Must be greater than 0.\n\n    Returns:\n        (nparray): A filtered copy of the input image \"img\", a 2-D array of floats.\n    \"\"\"\n\n    # Converts an index tuple into a vector.\n    def vec(x):\n        return np.array(x)\n\n    kernel_min = -kernel_center\n    kernel_max = vec(kernel.shape) - kernel_center\n\n    def weighted_histogram_filter_single(idx):\n        idx = vec(idx)\n        min_index = np.maximum(0, idx + kernel_min)\n        max_index = np.minimum(vec(img.shape), idx + kernel_max)\n        window_shape = max_index - min_index\n\n        class WeightedElement:\n            \"\"\"\n            An element of the histogram, its weight\n            and bounds.\n            \"\"\"\n\n            def __init__(self, value, weight):\n                self.value: float = value\n                self.weight: float = weight\n                self.window_min: float = 0.0\n                self.window_max: float = 1.0\n\n        # Collect the values in the image as WeightedElements,\n        # weighted by their corresponding kernel values.\n        values = []\n        for window_tup in np.ndindex(tuple(window_shape)):\n            window_index = vec(window_tup)\n            image_index = window_index + min_index\n            centered_kernel_index = image_index - idx\n            kernel_index = centered_kernel_index + kernel_center\n            element = WeightedElement(img[tuple(image_index)], kernel[tuple(kernel_index)])\n            values.append(element)\n\n        def sort_key(x: WeightedElement):\n            return x.value\n\n        values.sort(key=sort_key)\n\n        # Calculate the height of the stack (sum)\n        # and each sample's range they occupy in the stack\n        sum = 0\n        for i in range(len(values)):\n            values[i].window_min = sum\n            sum += values[i].weight\n            values[i].window_max = sum\n\n        # Calculate what range of this stack (\"window\")\n        # we want to get the weighted average across.\n        window_min = sum * percentile_min\n        window_max = sum * percentile_max\n        window_width = window_max - window_min\n\n        # Ensure the window is within the stack and at least a certain size.\n        if window_width < min_width:\n            window_center = (window_min + window_max) / 2\n            window_min = window_center - min_width / 2\n            window_max = window_center + min_width / 2\n\n            if window_max > sum:\n                window_max = sum\n                window_min = sum - min_width\n\n            if window_min < 0:\n                window_min = 0\n                window_max = min_width\n\n        value = 0\n        value_weight = 0\n\n        # Get the weighted average of all the samples\n        # that overlap with the window, weighted\n        # by the size of their overlap.\n        for i in range(len(values)):\n            if window_min >= values[i].window_max:\n                continue\n            if window_max <= values[i].window_min:\n                break\n\n            s = max(window_min, values[i].window_min)\n            e = min(window_max, values[i].window_max)\n            w = e - s\n\n            value += values[i].value * w\n            value_weight += w\n\n        return value / value_weight if value_weight != 0 else 0\n\n    img_out = img.copy()\n\n    # Apply the kernel operation over each pixel.\n    for index in np.ndindex(img.shape):\n        img_out[index] = weighted_histogram_filter_single(index)\n\n    return img_out\n\n\ndef smoothstep(x):\n    \"\"\"\n    The smoothstep function, input should be clamped to 0-1 range.\n    Turns a diagonal line (f(x) = x) into a sigmoid-like curve.\n    \"\"\"\n    return x * x * (3 - 2 * x)\n\n\ndef smootherstep(x):\n    \"\"\"\n    The smootherstep function, input should be clamped to 0-1 range.\n    Turns a diagonal line (f(x) = x) into a sigmoid-like curve.\n    \"\"\"\n    return x * x * x * (x * (6 * x - 15) + 10)\n\n\ndef get_gaussian_kernel(stddev_radius=1.0, max_radius=2):\n    \"\"\"\n    Creates a Gaussian kernel with thresholded edges.\n\n    Args:\n        stddev_radius (float):\n            Standard deviation of the gaussian kernel, in pixels.\n        max_radius (int):\n            The size of the filter kernel. The number of pixels is (max_radius*2+1) ** 2.\n            The kernel is thresholded so that any values one pixel beyond this radius\n            is weighted at 0.\n\n    Returns:\n        (nparray, nparray): A kernel array (shape: (N, N)), its center coordinate (shape: (2))\n    \"\"\"\n\n    # Evaluates a 0-1 normalized gaussian function for a given square distance from the mean.\n    def gaussian(sqr_mag):\n        return math.exp(-sqr_mag / (stddev_radius * stddev_radius))\n\n    # Helper function for converting a tuple to an array.\n    def vec(x):\n        return np.array(x)\n\n    \"\"\"\n    Since a gaussian is unbounded, we need to limit ourselves\n    to a finite range.\n    We taper the ends off at the end of that range so they equal zero\n    while preserving the maximum value of 1 at the mean.\n    \"\"\"\n    zero_radius = max_radius + 1.0\n    gauss_zero = gaussian(zero_radius * zero_radius)\n    gauss_kernel_scale = 1 / (1 - gauss_zero)\n\n    def gaussian_kernel_func(coordinate):\n        x = coordinate[0] ** 2.0 + coordinate[1] ** 2.0\n        x = gaussian(x)\n        x -= gauss_zero\n        x *= gauss_kernel_scale\n        x = max(0.0, x)\n        return x\n\n    size = max_radius * 2 + 1\n    kernel_center = max_radius\n    kernel = np.zeros((size, size))\n\n    for index in np.ndindex(kernel.shape):\n        kernel[index] = gaussian_kernel_func(vec(index) - kernel_center)\n\n    return kernel, kernel_center\n\n\n# ------------------- Constants -------------------\n\n\ndefault = SoftInpaintingSettings(1, 0.5, 4, 0, 0.5, 2)\n\nenabled_ui_label = \"Soft inpainting\"\nenabled_gen_param_label = \"Soft inpainting enabled\"\nenabled_el_id = \"soft_inpainting_enabled\"\n\nui_labels = SoftInpaintingSettings(\n    \"Schedule bias\",\n    \"Preservation strength\",\n    \"Transition contrast boost\",\n    \"Mask influence\",\n    \"Difference threshold\",\n    \"Difference contrast\")\n\nui_info = SoftInpaintingSettings(\n    \"Shifts when preservation of original content occurs during denoising.\",\n    \"How strongly partially masked content should be preserved.\",\n    \"Amplifies the contrast that may be lost in partially masked regions.\",\n    \"How strongly the original mask should bias the difference threshold.\",\n    \"How much an image region can change before the original pixels are not blended in anymore.\",\n    \"How sharp the transition should be between blended and not blended.\")\n\ngen_param_labels = SoftInpaintingSettings(\n    \"Soft inpainting schedule bias\",\n    \"Soft inpainting preservation strength\",\n    \"Soft inpainting transition contrast boost\",\n    \"Soft inpainting mask influence\",\n    \"Soft inpainting difference threshold\",\n    \"Soft inpainting difference contrast\")\n\nel_ids = SoftInpaintingSettings(\n    \"mask_blend_power\",\n    \"mask_blend_scale\",\n    \"inpaint_detail_preservation\",\n    \"composite_mask_influence\",\n    \"composite_difference_threshold\",\n    \"composite_difference_contrast\")\n\n\n# ------------------- Script -------------------\n\n\nclass Script(scripts.Script):\n    def __init__(self):\n        self.section = \"inpaint\"\n        self.masks_for_overlay = None\n        self.overlay_images = None\n\n    def title(self):\n        return \"Soft Inpainting\"\n\n    def show(self, is_img2img):\n        return scripts.AlwaysVisible if is_img2img else False\n\n    def ui(self, is_img2img):\n        if not is_img2img:\n            return\n\n        with InputAccordion(False, label=enabled_ui_label, elem_id=enabled_el_id) as soft_inpainting_enabled:\n            with gr.Group():\n                gr.Markdown(\n                    \"\"\"\n                    Soft inpainting allows you to **seamlessly blend original content with inpainted content** according to the mask opacity.\n                    **High _Mask blur_** values are recommended!\n                    \"\"\")\n\n                power = \\\n                    gr.Slider(label=ui_labels.mask_blend_power,\n                              info=ui_info.mask_blend_power,\n                              minimum=0,\n                              maximum=8,\n                              step=0.1,\n                              value=default.mask_blend_power,\n                              elem_id=el_ids.mask_blend_power)\n                scale = \\\n                    gr.Slider(label=ui_labels.mask_blend_scale,\n                              info=ui_info.mask_blend_scale,\n                              minimum=0,\n                              maximum=8,\n                              step=0.05,\n                              value=default.mask_blend_scale,\n                              elem_id=el_ids.mask_blend_scale)\n                detail = \\\n                    gr.Slider(label=ui_labels.inpaint_detail_preservation,\n                              info=ui_info.inpaint_detail_preservation,\n                              minimum=1,\n                              maximum=32,\n                              step=0.5,\n                              value=default.inpaint_detail_preservation,\n                              elem_id=el_ids.inpaint_detail_preservation)\n\n                gr.Markdown(\n                    \"\"\"\n                    ### Pixel Composite Settings\n                    \"\"\")\n\n                mask_inf = \\\n                    gr.Slider(label=ui_labels.composite_mask_influence,\n                              info=ui_info.composite_mask_influence,\n                              minimum=0,\n                              maximum=1,\n                              step=0.05,\n                              value=default.composite_mask_influence,\n                              elem_id=el_ids.composite_mask_influence)\n\n                dif_thresh = \\\n                    gr.Slider(label=ui_labels.composite_difference_threshold,\n                              info=ui_info.composite_difference_threshold,\n                              minimum=0,\n                              maximum=8,\n                              step=0.25,\n                              value=default.composite_difference_threshold,\n                              elem_id=el_ids.composite_difference_threshold)\n\n                dif_contr = \\\n                    gr.Slider(label=ui_labels.composite_difference_contrast,\n                              info=ui_info.composite_difference_contrast,\n                              minimum=0,\n                              maximum=8,\n                              step=0.25,\n                              value=default.composite_difference_contrast,\n                              elem_id=el_ids.composite_difference_contrast)\n\n                with gr.Accordion(\"Help\", open=False):\n                    gr.Markdown(\n                        f\"\"\"\n                        ### {ui_labels.mask_blend_power}\n\n                        The blending strength of original content is scaled proportionally with the decreasing noise level values at each step (sigmas).\n                        This ensures that the influence of the denoiser and original content preservation is roughly balanced at each step.\n                        This balance can be shifted using this parameter, controlling whether earlier or later steps have stronger preservation.\n\n                        - **Below 1**: Stronger preservation near the end (with low sigma)\n                        - **1**: Balanced (proportional to sigma)\n                        - **Above 1**: Stronger preservation in the beginning (with high sigma)\n                        \"\"\")\n                    gr.Markdown(\n                        f\"\"\"\n                        ### {ui_labels.mask_blend_scale}\n\n                        Skews whether partially masked image regions should be more likely to preserve the original content or favor inpainted content.\n                        This may need to be adjusted depending on the {ui_labels.mask_blend_power}, CFG Scale, prompt and Denoising strength.\n\n                        - **Low values**: Favors generated content.\n                        - **High values**: Favors original content.\n                        \"\"\")\n                    gr.Markdown(\n                        f\"\"\"\n                        ### {ui_labels.inpaint_detail_preservation}\n\n                        This parameter controls how the original latent vectors and denoised latent vectors are interpolated.\n                        With higher values, the magnitude of the resulting blended vector will be closer to the maximum of the two interpolated vectors.\n                        This can prevent the loss of contrast that occurs with linear interpolation.\n\n                        - **Low values**: Softer blending, details may fade.\n                        - **High values**: Stronger contrast, may over-saturate colors.\n                        \"\"\")\n\n                    gr.Markdown(\n                        \"\"\"\n                        ## Pixel Composite Settings\n\n                        Masks are generated based on how much a part of the image changed after denoising.\n                        These masks are used to blend the original and final images together.\n                        If the difference is low, the original pixels are used instead of the pixels returned by the inpainting process.\n                        \"\"\")\n\n                    gr.Markdown(\n                        f\"\"\"\n                        ### {ui_labels.composite_mask_influence}\n\n                        This parameter controls how much the mask should bias this sensitivity to difference.\n\n                        - **0**: Ignore the mask, only consider differences in image content.\n                        - **1**: Follow the mask closely despite image content changes.\n                        \"\"\")\n\n                    gr.Markdown(\n                        f\"\"\"\n                        ### {ui_labels.composite_difference_threshold}\n\n                        This value represents the difference at which the original pixels will have less than 50% opacity.\n\n                        - **Low values**: Two images patches must be almost the same in order to retain original pixels.\n                        - **High values**: Two images patches can be very different and still retain original pixels.\n                        \"\"\")\n\n                    gr.Markdown(\n                        f\"\"\"\n                        ### {ui_labels.composite_difference_contrast}\n\n                        This value represents the contrast between the opacity of the original and inpainted content.\n\n                        - **Low values**: The blend will be more gradual and have longer transitions, but may cause ghosting.\n                        - **High values**: Ghosting will be less common, but transitions may be very sudden.\n                        \"\"\")\n\n        self.infotext_fields = [(soft_inpainting_enabled, enabled_gen_param_label),\n                                (power, gen_param_labels.mask_blend_power),\n                                (scale, gen_param_labels.mask_blend_scale),\n                                (detail, gen_param_labels.inpaint_detail_preservation),\n                                (mask_inf, gen_param_labels.composite_mask_influence),\n                                (dif_thresh, gen_param_labels.composite_difference_threshold),\n                                (dif_contr, gen_param_labels.composite_difference_contrast)]\n\n        self.paste_field_names = []\n        for _, field_name in self.infotext_fields:\n            self.paste_field_names.append(field_name)\n\n        return [soft_inpainting_enabled,\n                power,\n                scale,\n                detail,\n                mask_inf,\n                dif_thresh,\n                dif_contr]\n\n    def process(self, p, enabled, power, scale, detail_preservation, mask_inf, dif_thresh, dif_contr):\n        if not enabled:\n            return\n\n        if not processing_uses_inpainting(p):\n            return\n\n        # Shut off the rounding it normally does.\n        p.mask_round = False\n\n        settings = SoftInpaintingSettings(power, scale, detail_preservation, mask_inf, dif_thresh, dif_contr)\n\n        # p.extra_generation_params[\"Mask rounding\"] = False\n        settings.add_generation_params(p.extra_generation_params)\n\n    def on_mask_blend(self, p, mba: scripts.MaskBlendArgs, enabled, power, scale, detail_preservation, mask_inf,\n                      dif_thresh, dif_contr):\n        if not enabled:\n            return\n\n        if not processing_uses_inpainting(p):\n            return\n\n        if mba.is_final_blend:\n            mba.blended_latent = mba.current_latent\n            return\n\n        settings = SoftInpaintingSettings(power, scale, detail_preservation, mask_inf, dif_thresh, dif_contr)\n\n        # todo: Why is sigma 2D? Both values are the same.\n        mba.blended_latent = latent_blend(settings,\n                                          mba.init_latent,\n                                          mba.current_latent,\n                                          get_modified_nmask(settings, mba.nmask, mba.sigma[0]))\n\n    def post_sample(self, p, ps: scripts.PostSampleArgs, enabled, power, scale, detail_preservation, mask_inf,\n                    dif_thresh, dif_contr):\n        if not enabled:\n            return\n\n        if not processing_uses_inpainting(p):\n            return\n\n        nmask = getattr(p, \"nmask\", None)\n        if nmask is None:\n            return\n\n        from modules import images\n        from modules.shared import opts\n\n        settings = SoftInpaintingSettings(power, scale, detail_preservation, mask_inf, dif_thresh, dif_contr)\n\n        # since the original code puts holes in the existing overlay images,\n        # we have to rebuild them.\n        self.overlay_images = []\n        for img in p.init_images:\n\n            image = images.flatten(img, opts.img2img_background_color)\n\n            if p.paste_to is None and p.resize_mode != 3:\n                image = images.resize_image(p.resize_mode, image, p.width, p.height)\n\n            self.overlay_images.append(image.convert('RGBA'))\n\n        if len(p.init_images) == 1:\n            self.overlay_images = self.overlay_images * p.batch_size\n\n        if getattr(ps.samples, 'already_decoded', False):\n            self.masks_for_overlay = apply_masks(settings=settings,\n                                                 nmask=nmask,\n                                                 overlay_images=self.overlay_images,\n                                                 width=p.width,\n                                                 height=p.height,\n                                                 paste_to=p.paste_to)\n        else:\n            self.masks_for_overlay = apply_adaptive_masks(settings=settings,\n                                                          nmask=nmask,\n                                                          latent_orig=p.init_latent,\n                                                          latent_processed=ps.samples,\n                                                          overlay_images=self.overlay_images,\n                                                          width=p.width,\n                                                          height=p.height,\n                                                          paste_to=p.paste_to)\n\n    def postprocess_maskoverlay(self, p, ppmo: scripts.PostProcessMaskOverlayArgs, enabled, power, scale,\n                                detail_preservation, mask_inf, dif_thresh, dif_contr):\n        if not enabled:\n            return\n\n        if not processing_uses_inpainting(p):\n            return\n\n        if self.masks_for_overlay is None:\n            return\n\n        if self.overlay_images is None:\n            return\n\n        ppmo.mask_for_overlay = self.masks_for_overlay[ppmo.index]\n        ppmo.overlay_image = self.overlay_images[ppmo.index]\n", "scripts/poor_mans_outpainting.py": "import math\n\nimport modules.scripts as scripts\nimport gradio as gr\nfrom PIL import Image, ImageDraw\n\nfrom modules import images, devices\nfrom modules.processing import Processed, process_images\nfrom modules.shared import opts, state\n\n\nclass Script(scripts.Script):\n    def title(self):\n        return \"Poor man's outpainting\"\n\n    def show(self, is_img2img):\n        return is_img2img\n\n    def ui(self, is_img2img):\n        if not is_img2img:\n            return None\n\n        pixels = gr.Slider(label=\"Pixels to expand\", minimum=8, maximum=256, step=8, value=128, elem_id=self.elem_id(\"pixels\"))\n        mask_blur = gr.Slider(label='Mask blur', minimum=0, maximum=64, step=1, value=4, elem_id=self.elem_id(\"mask_blur\"))\n        inpainting_fill = gr.Radio(label='Masked content', choices=['fill', 'original', 'latent noise', 'latent nothing'], value='fill', type=\"index\", elem_id=self.elem_id(\"inpainting_fill\"))\n        direction = gr.CheckboxGroup(label=\"Outpainting direction\", choices=['left', 'right', 'up', 'down'], value=['left', 'right', 'up', 'down'], elem_id=self.elem_id(\"direction\"))\n\n        return [pixels, mask_blur, inpainting_fill, direction]\n\n    def run(self, p, pixels, mask_blur, inpainting_fill, direction):\n        initial_seed = None\n        initial_info = None\n\n        p.mask_blur = mask_blur * 2\n        p.inpainting_fill = inpainting_fill\n        p.inpaint_full_res = False\n\n        left = pixels if \"left\" in direction else 0\n        right = pixels if \"right\" in direction else 0\n        up = pixels if \"up\" in direction else 0\n        down = pixels if \"down\" in direction else 0\n\n        init_img = p.init_images[0]\n        target_w = math.ceil((init_img.width + left + right) / 64) * 64\n        target_h = math.ceil((init_img.height + up + down) / 64) * 64\n\n        if left > 0:\n            left = left * (target_w - init_img.width) // (left + right)\n        if right > 0:\n            right = target_w - init_img.width - left\n\n        if up > 0:\n            up = up * (target_h - init_img.height) // (up + down)\n\n        if down > 0:\n            down = target_h - init_img.height - up\n\n        img = Image.new(\"RGB\", (target_w, target_h))\n        img.paste(init_img, (left, up))\n\n        mask = Image.new(\"L\", (img.width, img.height), \"white\")\n        draw = ImageDraw.Draw(mask)\n        draw.rectangle((\n            left + (mask_blur * 2 if left > 0 else 0),\n            up + (mask_blur * 2 if up > 0 else 0),\n            mask.width - right - (mask_blur * 2 if right > 0 else 0),\n            mask.height - down - (mask_blur * 2 if down > 0 else 0)\n        ), fill=\"black\")\n\n        latent_mask = Image.new(\"L\", (img.width, img.height), \"white\")\n        latent_draw = ImageDraw.Draw(latent_mask)\n        latent_draw.rectangle((\n             left + (mask_blur//2 if left > 0 else 0),\n             up + (mask_blur//2 if up > 0 else 0),\n             mask.width - right - (mask_blur//2 if right > 0 else 0),\n             mask.height - down - (mask_blur//2 if down > 0 else 0)\n        ), fill=\"black\")\n\n        devices.torch_gc()\n\n        grid = images.split_grid(img, tile_w=p.width, tile_h=p.height, overlap=pixels)\n        grid_mask = images.split_grid(mask, tile_w=p.width, tile_h=p.height, overlap=pixels)\n        grid_latent_mask = images.split_grid(latent_mask, tile_w=p.width, tile_h=p.height, overlap=pixels)\n\n        p.n_iter = 1\n        p.batch_size = 1\n        p.do_not_save_grid = True\n        p.do_not_save_samples = True\n\n        work = []\n        work_mask = []\n        work_latent_mask = []\n        work_results = []\n\n        for (y, h, row), (_, _, row_mask), (_, _, row_latent_mask) in zip(grid.tiles, grid_mask.tiles, grid_latent_mask.tiles):\n            for tiledata, tiledata_mask, tiledata_latent_mask in zip(row, row_mask, row_latent_mask):\n                x, w = tiledata[0:2]\n\n                if x >= left and x+w <= img.width - right and y >= up and y+h <= img.height - down:\n                    continue\n\n                work.append(tiledata[2])\n                work_mask.append(tiledata_mask[2])\n                work_latent_mask.append(tiledata_latent_mask[2])\n\n        batch_count = len(work)\n        print(f\"Poor man's outpainting will process a total of {len(work)} images tiled as {len(grid.tiles[0][2])}x{len(grid.tiles)}.\")\n\n        state.job_count = batch_count\n\n        for i in range(batch_count):\n            p.init_images = [work[i]]\n            p.image_mask = work_mask[i]\n            p.latent_mask = work_latent_mask[i]\n\n            state.job = f\"Batch {i + 1} out of {batch_count}\"\n            processed = process_images(p)\n\n            if initial_seed is None:\n                initial_seed = processed.seed\n                initial_info = processed.info\n\n            p.seed = processed.seed + 1\n            work_results += processed.images\n\n\n        image_index = 0\n        for y, h, row in grid.tiles:\n            for tiledata in row:\n                x, w = tiledata[0:2]\n\n                if x >= left and x+w <= img.width - right and y >= up and y+h <= img.height - down:\n                    continue\n\n                tiledata[2] = work_results[image_index] if image_index < len(work_results) else Image.new(\"RGB\", (p.width, p.height))\n                image_index += 1\n\n        combined_image = images.combine_grid(grid)\n\n        if opts.samples_save:\n            images.save_image(combined_image, p.outpath_samples, \"\", initial_seed, p.prompt, opts.samples_format, info=initial_info, p=p)\n\n        processed = Processed(p, [combined_image], initial_seed, initial_info)\n\n        return processed\n\n", "scripts/sd_upscale.py": "import math\n\nimport modules.scripts as scripts\nimport gradio as gr\nfrom PIL import Image\n\nfrom modules import processing, shared, images, devices\nfrom modules.processing import Processed\nfrom modules.shared import opts, state\n\n\nclass Script(scripts.Script):\n    def title(self):\n        return \"SD upscale\"\n\n    def show(self, is_img2img):\n        return is_img2img\n\n    def ui(self, is_img2img):\n        info = gr.HTML(\"<p style=\\\"margin-bottom:0.75em\\\">Will upscale the image by the selected scale factor; use width and height sliders to set tile size</p>\")\n        overlap = gr.Slider(minimum=0, maximum=256, step=16, label='Tile overlap', value=64, elem_id=self.elem_id(\"overlap\"))\n        scale_factor = gr.Slider(minimum=1.0, maximum=4.0, step=0.05, label='Scale Factor', value=2.0, elem_id=self.elem_id(\"scale_factor\"))\n        upscaler_index = gr.Radio(label='Upscaler', choices=[x.name for x in shared.sd_upscalers], value=shared.sd_upscalers[0].name, type=\"index\", elem_id=self.elem_id(\"upscaler_index\"))\n\n        return [info, overlap, upscaler_index, scale_factor]\n\n    def run(self, p, _, overlap, upscaler_index, scale_factor):\n        if isinstance(upscaler_index, str):\n            upscaler_index = [x.name.lower() for x in shared.sd_upscalers].index(upscaler_index.lower())\n        processing.fix_seed(p)\n        upscaler = shared.sd_upscalers[upscaler_index]\n\n        p.extra_generation_params[\"SD upscale overlap\"] = overlap\n        p.extra_generation_params[\"SD upscale upscaler\"] = upscaler.name\n\n        initial_info = None\n        seed = p.seed\n\n        init_img = p.init_images[0]\n        init_img = images.flatten(init_img, opts.img2img_background_color)\n\n        if upscaler.name != \"None\":\n            img = upscaler.scaler.upscale(init_img, scale_factor, upscaler.data_path)\n        else:\n            img = init_img\n\n        devices.torch_gc()\n\n        grid = images.split_grid(img, tile_w=p.width, tile_h=p.height, overlap=overlap)\n\n        batch_size = p.batch_size\n        upscale_count = p.n_iter\n        p.n_iter = 1\n        p.do_not_save_grid = True\n        p.do_not_save_samples = True\n\n        work = []\n\n        for _y, _h, row in grid.tiles:\n            for tiledata in row:\n                work.append(tiledata[2])\n\n        batch_count = math.ceil(len(work) / batch_size)\n        state.job_count = batch_count * upscale_count\n\n        print(f\"SD upscaling will process a total of {len(work)} images tiled as {len(grid.tiles[0][2])}x{len(grid.tiles)} per upscale in a total of {state.job_count} batches.\")\n\n        result_images = []\n        for n in range(upscale_count):\n            start_seed = seed + n\n            p.seed = start_seed\n\n            work_results = []\n            for i in range(batch_count):\n                p.batch_size = batch_size\n                p.init_images = work[i * batch_size:(i + 1) * batch_size]\n\n                state.job = f\"Batch {i + 1 + n * batch_count} out of {state.job_count}\"\n                processed = processing.process_images(p)\n\n                if initial_info is None:\n                    initial_info = processed.info\n\n                p.seed = processed.seed + 1\n                work_results += processed.images\n\n            image_index = 0\n            for _y, _h, row in grid.tiles:\n                for tiledata in row:\n                    tiledata[2] = work_results[image_index] if image_index < len(work_results) else Image.new(\"RGB\", (p.width, p.height))\n                    image_index += 1\n\n            combined_image = images.combine_grid(grid)\n            result_images.append(combined_image)\n\n            if opts.samples_save:\n                images.save_image(combined_image, p.outpath_samples, \"\", start_seed, p.prompt, opts.samples_format, info=initial_info, p=p)\n\n        processed = Processed(p, result_images, seed, initial_info)\n\n        return processed\n", "scripts/postprocessing_codeformer.py": "from PIL import Image\nimport numpy as np\n\nfrom modules import scripts_postprocessing, codeformer_model, ui_components\nimport gradio as gr\n\n\nclass ScriptPostprocessingCodeFormer(scripts_postprocessing.ScriptPostprocessing):\n    name = \"CodeFormer\"\n    order = 3000\n\n    def ui(self):\n        with ui_components.InputAccordion(False, label=\"CodeFormer\") as enable:\n            with gr.Row():\n                codeformer_visibility = gr.Slider(minimum=0.0, maximum=1.0, step=0.001, label=\"Visibility\", value=1.0, elem_id=\"extras_codeformer_visibility\")\n                codeformer_weight = gr.Slider(minimum=0.0, maximum=1.0, step=0.001, label=\"Weight (0 = maximum effect, 1 = minimum effect)\", value=0, elem_id=\"extras_codeformer_weight\")\n\n        return {\n            \"enable\": enable,\n            \"codeformer_visibility\": codeformer_visibility,\n            \"codeformer_weight\": codeformer_weight,\n        }\n\n    def process(self, pp: scripts_postprocessing.PostprocessedImage, enable, codeformer_visibility, codeformer_weight):\n        if codeformer_visibility == 0 or not enable:\n            return\n\n        restored_img = codeformer_model.codeformer.restore(np.array(pp.image.convert(\"RGB\"), dtype=np.uint8), w=codeformer_weight)\n        res = Image.fromarray(restored_img)\n\n        if codeformer_visibility < 1.0:\n            res = Image.blend(pp.image, res, codeformer_visibility)\n\n        pp.image = res\n        pp.info[\"CodeFormer visibility\"] = round(codeformer_visibility, 3)\n        pp.info[\"CodeFormer weight\"] = round(codeformer_weight, 3)\n", "scripts/postprocessing_upscale.py": "import re\n\nfrom PIL import Image\nimport numpy as np\n\nfrom modules import scripts_postprocessing, shared\nimport gradio as gr\n\nfrom modules.ui_components import FormRow, ToolButton, InputAccordion\nfrom modules.ui import switch_values_symbol\n\nupscale_cache = {}\n\n\ndef limit_size_by_one_dimention(w, h, limit):\n    if h > w and h > limit:\n        w = limit * w // h\n        h = limit\n    elif w > limit:\n        h = limit * h // w\n        w = limit\n\n    return int(w), int(h)\n\n\nclass ScriptPostprocessingUpscale(scripts_postprocessing.ScriptPostprocessing):\n    name = \"Upscale\"\n    order = 1000\n\n    def ui(self):\n        selected_tab = gr.Number(value=0, visible=False)\n\n        with InputAccordion(True, label=\"Upscale\", elem_id=\"extras_upscale\") as upscale_enabled:\n            with FormRow():\n                extras_upscaler_1 = gr.Dropdown(label='Upscaler 1', elem_id=\"extras_upscaler_1\", choices=[x.name for x in shared.sd_upscalers], value=shared.sd_upscalers[0].name)\n\n            with FormRow():\n                extras_upscaler_2 = gr.Dropdown(label='Upscaler 2', elem_id=\"extras_upscaler_2\", choices=[x.name for x in shared.sd_upscalers], value=shared.sd_upscalers[0].name)\n                extras_upscaler_2_visibility = gr.Slider(minimum=0.0, maximum=1.0, step=0.001, label=\"Upscaler 2 visibility\", value=0.0, elem_id=\"extras_upscaler_2_visibility\")\n\n            with FormRow():\n                with gr.Tabs(elem_id=\"extras_resize_mode\"):\n                    with gr.TabItem('Scale by', elem_id=\"extras_scale_by_tab\") as tab_scale_by:\n                        with gr.Row():\n                            with gr.Column(scale=4):\n                                upscaling_resize = gr.Slider(minimum=1.0, maximum=8.0, step=0.05, label=\"Resize\", value=4, elem_id=\"extras_upscaling_resize\")\n                            with gr.Column(scale=1, min_width=160):\n                                max_side_length = gr.Number(label=\"Max side length\", value=0, elem_id=\"extras_upscale_max_side_length\", tooltip=\"If any of two sides of the image ends up larger than specified, will downscale it to fit. 0 = no limit.\", min_width=160, step=8, minimum=0)\n\n                    with gr.TabItem('Scale to', elem_id=\"extras_scale_to_tab\") as tab_scale_to:\n                        with FormRow():\n                            with gr.Column(elem_id=\"upscaling_column_size\", scale=4):\n                                upscaling_resize_w = gr.Slider(minimum=64, maximum=8192, step=8, label=\"Width\", value=512, elem_id=\"extras_upscaling_resize_w\")\n                                upscaling_resize_h = gr.Slider(minimum=64, maximum=8192, step=8, label=\"Height\", value=512, elem_id=\"extras_upscaling_resize_h\")\n                            with gr.Column(elem_id=\"upscaling_dimensions_row\", scale=1, elem_classes=\"dimensions-tools\"):\n                                upscaling_res_switch_btn = ToolButton(value=switch_values_symbol, elem_id=\"upscaling_res_switch_btn\", tooltip=\"Switch width/height\")\n                                upscaling_crop = gr.Checkbox(label='Crop to fit', value=True, elem_id=\"extras_upscaling_crop\")\n\n        def on_selected_upscale_method(upscale_method):\n            if not shared.opts.set_scale_by_when_changing_upscaler:\n                return gr.update()\n\n            match = re.search(r'(\\d)[xX]|[xX](\\d)', upscale_method)\n            if not match:\n                return gr.update()\n\n            return gr.update(value=int(match.group(1) or match.group(2)))\n\n        upscaling_res_switch_btn.click(lambda w, h: (h, w), inputs=[upscaling_resize_w, upscaling_resize_h], outputs=[upscaling_resize_w, upscaling_resize_h], show_progress=False)\n        tab_scale_by.select(fn=lambda: 0, inputs=[], outputs=[selected_tab])\n        tab_scale_to.select(fn=lambda: 1, inputs=[], outputs=[selected_tab])\n\n        extras_upscaler_1.change(on_selected_upscale_method, inputs=[extras_upscaler_1], outputs=[upscaling_resize], show_progress=\"hidden\")\n\n        return {\n            \"upscale_enabled\": upscale_enabled,\n            \"upscale_mode\": selected_tab,\n            \"upscale_by\": upscaling_resize,\n            \"max_side_length\": max_side_length,\n            \"upscale_to_width\": upscaling_resize_w,\n            \"upscale_to_height\": upscaling_resize_h,\n            \"upscale_crop\": upscaling_crop,\n            \"upscaler_1_name\": extras_upscaler_1,\n            \"upscaler_2_name\": extras_upscaler_2,\n            \"upscaler_2_visibility\": extras_upscaler_2_visibility,\n        }\n\n    def upscale(self, image, info, upscaler, upscale_mode, upscale_by, max_side_length, upscale_to_width, upscale_to_height, upscale_crop):\n        if upscale_mode == 1:\n            upscale_by = max(upscale_to_width/image.width, upscale_to_height/image.height)\n            info[\"Postprocess upscale to\"] = f\"{upscale_to_width}x{upscale_to_height}\"\n        else:\n            info[\"Postprocess upscale by\"] = upscale_by\n            if max_side_length != 0 and max(*image.size)*upscale_by > max_side_length:\n                upscale_mode = 1\n                upscale_crop = False\n                upscale_to_width, upscale_to_height = limit_size_by_one_dimention(image.width*upscale_by, image.height*upscale_by, max_side_length)\n                upscale_by = max(upscale_to_width/image.width, upscale_to_height/image.height)\n                info[\"Max side length\"] = max_side_length\n\n        cache_key = (hash(np.array(image.getdata()).tobytes()), upscaler.name, upscale_mode, upscale_by,  upscale_to_width, upscale_to_height, upscale_crop)\n        cached_image = upscale_cache.pop(cache_key, None)\n\n        if cached_image is not None:\n            image = cached_image\n        else:\n            image = upscaler.scaler.upscale(image, upscale_by, upscaler.data_path)\n\n        upscale_cache[cache_key] = image\n        if len(upscale_cache) > shared.opts.upscaling_max_images_in_cache:\n            upscale_cache.pop(next(iter(upscale_cache), None), None)\n\n        if upscale_mode == 1 and upscale_crop:\n            cropped = Image.new(\"RGB\", (upscale_to_width, upscale_to_height))\n            cropped.paste(image, box=(upscale_to_width // 2 - image.width // 2, upscale_to_height // 2 - image.height // 2))\n            image = cropped\n            info[\"Postprocess crop to\"] = f\"{image.width}x{image.height}\"\n\n        return image\n\n    def process_firstpass(self, pp: scripts_postprocessing.PostprocessedImage, upscale_enabled=True, upscale_mode=1, upscale_by=2.0, max_side_length=0, upscale_to_width=None, upscale_to_height=None, upscale_crop=False, upscaler_1_name=None, upscaler_2_name=None, upscaler_2_visibility=0.0):\n        if upscale_mode == 1:\n            pp.shared.target_width = upscale_to_width\n            pp.shared.target_height = upscale_to_height\n        else:\n            pp.shared.target_width = int(pp.image.width * upscale_by)\n            pp.shared.target_height = int(pp.image.height * upscale_by)\n\n            pp.shared.target_width, pp.shared.target_height = limit_size_by_one_dimention(pp.shared.target_width, pp.shared.target_height, max_side_length)\n\n    def process(self, pp: scripts_postprocessing.PostprocessedImage, upscale_enabled=True, upscale_mode=1, upscale_by=2.0, max_side_length=0, upscale_to_width=None, upscale_to_height=None, upscale_crop=False, upscaler_1_name=None, upscaler_2_name=None, upscaler_2_visibility=0.0):\n        if not upscale_enabled:\n            return\n\n        upscaler_1_name = upscaler_1_name\n        if upscaler_1_name == \"None\":\n            upscaler_1_name = None\n\n        upscaler1 = next(iter([x for x in shared.sd_upscalers if x.name == upscaler_1_name]), None)\n        assert upscaler1 or (upscaler_1_name is None), f'could not find upscaler named {upscaler_1_name}'\n\n        if not upscaler1:\n            return\n\n        upscaler_2_name = upscaler_2_name\n        if upscaler_2_name == \"None\":\n            upscaler_2_name = None\n\n        upscaler2 = next(iter([x for x in shared.sd_upscalers if x.name == upscaler_2_name and x.name != \"None\"]), None)\n        assert upscaler2 or (upscaler_2_name is None), f'could not find upscaler named {upscaler_2_name}'\n\n        upscaled_image = self.upscale(pp.image, pp.info, upscaler1, upscale_mode, upscale_by, max_side_length, upscale_to_width, upscale_to_height, upscale_crop)\n        pp.info[\"Postprocess upscaler\"] = upscaler1.name\n\n        if upscaler2 and upscaler_2_visibility > 0:\n            second_upscale = self.upscale(pp.image, pp.info, upscaler2, upscale_mode, upscale_by, max_side_length, upscale_to_width, upscale_to_height, upscale_crop)\n            if upscaled_image.mode != second_upscale.mode:\n                second_upscale = second_upscale.convert(upscaled_image.mode)\n            upscaled_image = Image.blend(upscaled_image, second_upscale, upscaler_2_visibility)\n\n            pp.info[\"Postprocess upscaler 2\"] = upscaler2.name\n\n        pp.image = upscaled_image\n\n    def image_changed(self):\n        upscale_cache.clear()\n\n\nclass ScriptPostprocessingUpscaleSimple(ScriptPostprocessingUpscale):\n    name = \"Simple Upscale\"\n    order = 900\n\n    def ui(self):\n        with FormRow():\n            upscaler_name = gr.Dropdown(label='Upscaler', choices=[x.name for x in shared.sd_upscalers], value=shared.sd_upscalers[0].name)\n            upscale_by = gr.Slider(minimum=0.05, maximum=8.0, step=0.05, label=\"Upscale by\", value=2)\n\n        return {\n            \"upscale_by\": upscale_by,\n            \"upscaler_name\": upscaler_name,\n        }\n\n    def process_firstpass(self, pp: scripts_postprocessing.PostprocessedImage, upscale_by=2.0, upscaler_name=None):\n        pp.shared.target_width = int(pp.image.width * upscale_by)\n        pp.shared.target_height = int(pp.image.height * upscale_by)\n\n    def process(self, pp: scripts_postprocessing.PostprocessedImage, upscale_by=2.0, upscaler_name=None):\n        if upscaler_name is None or upscaler_name == \"None\":\n            return\n\n        upscaler1 = next(iter([x for x in shared.sd_upscalers if x.name == upscaler_name]), None)\n        assert upscaler1, f'could not find upscaler named {upscaler_name}'\n\n        pp.image = self.upscale(pp.image, pp.info, upscaler1, 0, upscale_by, 0, 0, 0, False)\n        pp.info[\"Postprocess upscaler\"] = upscaler1.name\n", "scripts/loopback.py": "import math\n\nimport gradio as gr\nimport modules.scripts as scripts\nfrom modules import deepbooru, images, processing, shared\nfrom modules.processing import Processed\nfrom modules.shared import opts, state\n\n\nclass Script(scripts.Script):\n    def title(self):\n        return \"Loopback\"\n\n    def show(self, is_img2img):\n        return is_img2img\n\n    def ui(self, is_img2img):\n        loops = gr.Slider(minimum=1, maximum=32, step=1, label='Loops', value=4, elem_id=self.elem_id(\"loops\"))\n        final_denoising_strength = gr.Slider(minimum=0, maximum=1, step=0.01, label='Final denoising strength', value=0.5, elem_id=self.elem_id(\"final_denoising_strength\"))\n        denoising_curve = gr.Dropdown(label=\"Denoising strength curve\", choices=[\"Aggressive\", \"Linear\", \"Lazy\"], value=\"Linear\")\n        append_interrogation = gr.Dropdown(label=\"Append interrogated prompt at each iteration\", choices=[\"None\", \"CLIP\", \"DeepBooru\"], value=\"None\")\n\n        return [loops, final_denoising_strength, denoising_curve, append_interrogation]\n\n    def run(self, p, loops, final_denoising_strength, denoising_curve, append_interrogation):\n        processing.fix_seed(p)\n        batch_count = p.n_iter\n        p.extra_generation_params = {\n            \"Final denoising strength\": final_denoising_strength,\n            \"Denoising curve\": denoising_curve\n        }\n\n        p.batch_size = 1\n        p.n_iter = 1\n\n        info = None\n        initial_seed = None\n        initial_info = None\n        initial_denoising_strength = p.denoising_strength\n\n        grids = []\n        all_images = []\n        original_init_image = p.init_images\n        original_prompt = p.prompt\n        original_inpainting_fill = p.inpainting_fill\n        state.job_count = loops * batch_count\n\n        initial_color_corrections = [processing.setup_color_correction(p.init_images[0])]\n\n        def calculate_denoising_strength(loop):\n            strength = initial_denoising_strength\n\n            if loops == 1:\n                return strength\n\n            progress = loop / (loops - 1)\n            if denoising_curve == \"Aggressive\":\n                strength = math.sin((progress) * math.pi * 0.5)\n            elif denoising_curve == \"Lazy\":\n                strength = 1 - math.cos((progress) * math.pi * 0.5)\n            else:\n                strength = progress\n\n            change = (final_denoising_strength - initial_denoising_strength) * strength\n            return initial_denoising_strength + change\n\n        history = []\n\n        for n in range(batch_count):\n            # Reset to original init image at the start of each batch\n            p.init_images = original_init_image\n\n            # Reset to original denoising strength\n            p.denoising_strength = initial_denoising_strength\n\n            last_image = None\n\n            for i in range(loops):\n                p.n_iter = 1\n                p.batch_size = 1\n                p.do_not_save_grid = True\n\n                if opts.img2img_color_correction:\n                    p.color_corrections = initial_color_corrections\n\n                if append_interrogation != \"None\":\n                    p.prompt = f\"{original_prompt}, \" if original_prompt else \"\"\n                    if append_interrogation == \"CLIP\":\n                        p.prompt += shared.interrogator.interrogate(p.init_images[0])\n                    elif append_interrogation == \"DeepBooru\":\n                        p.prompt += deepbooru.model.tag(p.init_images[0])\n\n                state.job = f\"Iteration {i + 1}/{loops}, batch {n + 1}/{batch_count}\"\n\n                processed = processing.process_images(p)\n\n                # Generation cancelled.\n                if state.interrupted or state.stopping_generation:\n                    break\n\n                if initial_seed is None:\n                    initial_seed = processed.seed\n                    initial_info = processed.info\n\n                p.seed = processed.seed + 1\n                p.denoising_strength = calculate_denoising_strength(i + 1)\n\n                if state.skipped:\n                    break\n\n                last_image = processed.images[0]\n                p.init_images = [last_image]\n                p.inpainting_fill = 1 # Set \"masked content\" to \"original\" for next loop.\n\n                if batch_count == 1:\n                    history.append(last_image)\n                    all_images.append(last_image)\n\n            if batch_count > 1 and not state.skipped and not state.interrupted:\n                history.append(last_image)\n                all_images.append(last_image)\n\n            p.inpainting_fill = original_inpainting_fill\n\n            if state.interrupted or state.stopping_generation:\n                break\n\n        if len(history) > 1:\n            grid = images.image_grid(history, rows=1)\n            if opts.grid_save:\n                images.save_image(grid, p.outpath_grids, \"grid\", initial_seed, p.prompt, opts.grid_format, info=info, short_filename=not opts.grid_extended_filename, grid=True, p=p)\n\n            if opts.return_grid:\n                grids.append(grid)\n\n        all_images = grids + all_images\n\n        processed = Processed(p, all_images, initial_seed, initial_info)\n\n        return processed\n", "scripts/prompts_from_file.py": "import copy\nimport random\nimport shlex\n\nimport modules.scripts as scripts\nimport gradio as gr\n\nfrom modules import sd_samplers, errors, sd_models\nfrom modules.processing import Processed, process_images\nfrom modules.shared import state\n\n\ndef process_model_tag(tag):\n    info = sd_models.get_closet_checkpoint_match(tag)\n    assert info is not None, f'Unknown checkpoint: {tag}'\n    return info.name\n\n\ndef process_string_tag(tag):\n    return tag\n\n\ndef process_int_tag(tag):\n    return int(tag)\n\n\ndef process_float_tag(tag):\n    return float(tag)\n\n\ndef process_boolean_tag(tag):\n    return True if (tag == \"true\") else False\n\n\nprompt_tags = {\n    \"sd_model\": process_model_tag,\n    \"outpath_samples\": process_string_tag,\n    \"outpath_grids\": process_string_tag,\n    \"prompt_for_display\": process_string_tag,\n    \"prompt\": process_string_tag,\n    \"negative_prompt\": process_string_tag,\n    \"styles\": process_string_tag,\n    \"seed\": process_int_tag,\n    \"subseed_strength\": process_float_tag,\n    \"subseed\": process_int_tag,\n    \"seed_resize_from_h\": process_int_tag,\n    \"seed_resize_from_w\": process_int_tag,\n    \"sampler_index\": process_int_tag,\n    \"sampler_name\": process_string_tag,\n    \"batch_size\": process_int_tag,\n    \"n_iter\": process_int_tag,\n    \"steps\": process_int_tag,\n    \"cfg_scale\": process_float_tag,\n    \"width\": process_int_tag,\n    \"height\": process_int_tag,\n    \"restore_faces\": process_boolean_tag,\n    \"tiling\": process_boolean_tag,\n    \"do_not_save_samples\": process_boolean_tag,\n    \"do_not_save_grid\": process_boolean_tag\n}\n\n\ndef cmdargs(line):\n    args = shlex.split(line)\n    pos = 0\n    res = {}\n\n    while pos < len(args):\n        arg = args[pos]\n\n        assert arg.startswith(\"--\"), f'must start with \"--\": {arg}'\n        assert pos+1 < len(args), f'missing argument for command line option {arg}'\n\n        tag = arg[2:]\n\n        if tag == \"prompt\" or tag == \"negative_prompt\":\n            pos += 1\n            prompt = args[pos]\n            pos += 1\n            while pos < len(args) and not args[pos].startswith(\"--\"):\n                prompt += \" \"\n                prompt += args[pos]\n                pos += 1\n            res[tag] = prompt\n            continue\n\n\n        func = prompt_tags.get(tag, None)\n        assert func, f'unknown commandline option: {arg}'\n\n        val = args[pos+1]\n        if tag == \"sampler_name\":\n            val = sd_samplers.samplers_map.get(val.lower(), None)\n\n        res[tag] = func(val)\n\n        pos += 2\n\n    return res\n\n\ndef load_prompt_file(file):\n    if file is None:\n        return None, gr.update(), gr.update(lines=7)\n    else:\n        lines = [x.strip() for x in file.decode('utf8', errors='ignore').split(\"\\n\")]\n        return None, \"\\n\".join(lines), gr.update(lines=7)\n\n\nclass Script(scripts.Script):\n    def title(self):\n        return \"Prompts from file or textbox\"\n\n    def ui(self, is_img2img):\n        checkbox_iterate = gr.Checkbox(label=\"Iterate seed every line\", value=False, elem_id=self.elem_id(\"checkbox_iterate\"))\n        checkbox_iterate_batch = gr.Checkbox(label=\"Use same random seed for all lines\", value=False, elem_id=self.elem_id(\"checkbox_iterate_batch\"))\n        prompt_position = gr.Radio([\"start\", \"end\"], label=\"Insert prompts at the\", elem_id=self.elem_id(\"prompt_position\"), value=\"start\")\n\n        prompt_txt = gr.Textbox(label=\"List of prompt inputs\", lines=1, elem_id=self.elem_id(\"prompt_txt\"))\n        file = gr.File(label=\"Upload prompt inputs\", type='binary', elem_id=self.elem_id(\"file\"))\n\n        file.change(fn=load_prompt_file, inputs=[file], outputs=[file, prompt_txt, prompt_txt], show_progress=False)\n\n        # We start at one line. When the text changes, we jump to seven lines, or two lines if no \\n.\n        # We don't shrink back to 1, because that causes the control to ignore [enter], and it may\n        # be unclear to the user that shift-enter is needed.\n        prompt_txt.change(lambda tb: gr.update(lines=7) if (\"\\n\" in tb) else gr.update(lines=2), inputs=[prompt_txt], outputs=[prompt_txt], show_progress=False)\n        return [checkbox_iterate, checkbox_iterate_batch, prompt_position, prompt_txt]\n\n    def run(self, p, checkbox_iterate, checkbox_iterate_batch, prompt_position, prompt_txt: str):\n        lines = [x for x in (x.strip() for x in prompt_txt.splitlines()) if x]\n\n        p.do_not_save_grid = True\n\n        job_count = 0\n        jobs = []\n\n        for line in lines:\n            if \"--\" in line:\n                try:\n                    args = cmdargs(line)\n                except Exception:\n                    errors.report(f\"Error parsing line {line} as commandline\", exc_info=True)\n                    args = {\"prompt\": line}\n            else:\n                args = {\"prompt\": line}\n\n            job_count += args.get(\"n_iter\", p.n_iter)\n\n            jobs.append(args)\n\n        print(f\"Will process {len(lines)} lines in {job_count} jobs.\")\n        if (checkbox_iterate or checkbox_iterate_batch) and p.seed == -1:\n            p.seed = int(random.randrange(4294967294))\n\n        state.job_count = job_count\n\n        images = []\n        all_prompts = []\n        infotexts = []\n        for args in jobs:\n            state.job = f\"{state.job_no + 1} out of {state.job_count}\"\n\n            copy_p = copy.copy(p)\n            for k, v in args.items():\n                if k == \"sd_model\":\n                    copy_p.override_settings['sd_model_checkpoint'] = v\n                else:\n                    setattr(copy_p, k, v)\n\n            if args.get(\"prompt\") and p.prompt:\n                if prompt_position == \"start\":\n                    copy_p.prompt = args.get(\"prompt\") + \" \" + p.prompt\n                else:\n                    copy_p.prompt = p.prompt + \" \" + args.get(\"prompt\")\n\n            if args.get(\"negative_prompt\") and p.negative_prompt:\n                if prompt_position == \"start\":\n                    copy_p.negative_prompt = args.get(\"negative_prompt\") + \" \" + p.negative_prompt\n                else:\n                    copy_p.negative_prompt = p.negative_prompt + \" \" + args.get(\"negative_prompt\")\n\n            proc = process_images(copy_p)\n            images += proc.images\n\n            if checkbox_iterate:\n                p.seed = p.seed + (p.batch_size * p.n_iter)\n            all_prompts += proc.all_prompts\n            infotexts += proc.infotexts\n\n        return Processed(p, images, p.seed, \"\", all_prompts=all_prompts, infotexts=infotexts)\n", "scripts/postprocessing_gfpgan.py": "from PIL import Image\nimport numpy as np\n\nfrom modules import scripts_postprocessing, gfpgan_model, ui_components\nimport gradio as gr\n\n\nclass ScriptPostprocessingGfpGan(scripts_postprocessing.ScriptPostprocessing):\n    name = \"GFPGAN\"\n    order = 2000\n\n    def ui(self):\n        with ui_components.InputAccordion(False, label=\"GFPGAN\") as enable:\n            gfpgan_visibility = gr.Slider(minimum=0.0, maximum=1.0, step=0.001, label=\"Visibility\", value=1.0, elem_id=\"extras_gfpgan_visibility\")\n\n        return {\n            \"enable\": enable,\n            \"gfpgan_visibility\": gfpgan_visibility,\n        }\n\n    def process(self, pp: scripts_postprocessing.PostprocessedImage, enable, gfpgan_visibility):\n        if gfpgan_visibility == 0 or not enable:\n            return\n\n        restored_img = gfpgan_model.gfpgan_fix_faces(np.array(pp.image.convert(\"RGB\"), dtype=np.uint8))\n        res = Image.fromarray(restored_img)\n\n        if gfpgan_visibility < 1.0:\n            res = Image.blend(pp.image, res, gfpgan_visibility)\n\n        pp.image = res\n        pp.info[\"GFPGAN visibility\"] = round(gfpgan_visibility, 3)\n", "scripts/prompt_matrix.py": "import math\n\nimport modules.scripts as scripts\nimport gradio as gr\n\nfrom modules import images\nfrom modules.processing import process_images\nfrom modules.shared import opts, state\nimport modules.sd_samplers\n\n\ndef draw_xy_grid(xs, ys, x_label, y_label, cell):\n    res = []\n\n    ver_texts = [[images.GridAnnotation(y_label(y))] for y in ys]\n    hor_texts = [[images.GridAnnotation(x_label(x))] for x in xs]\n\n    first_processed = None\n\n    state.job_count = len(xs) * len(ys)\n\n    for iy, y in enumerate(ys):\n        for ix, x in enumerate(xs):\n            state.job = f\"{ix + iy * len(xs) + 1} out of {len(xs) * len(ys)}\"\n\n            processed = cell(x, y)\n            if first_processed is None:\n                first_processed = processed\n\n            res.append(processed.images[0])\n\n    grid = images.image_grid(res, rows=len(ys))\n    grid = images.draw_grid_annotations(grid, res[0].width, res[0].height, hor_texts, ver_texts)\n\n    first_processed.images = [grid]\n\n    return first_processed\n\n\nclass Script(scripts.Script):\n    def title(self):\n        return \"Prompt matrix\"\n\n    def ui(self, is_img2img):\n        gr.HTML('<br />')\n        with gr.Row():\n            with gr.Column():\n                put_at_start = gr.Checkbox(label='Put variable parts at start of prompt', value=False, elem_id=self.elem_id(\"put_at_start\"))\n                different_seeds = gr.Checkbox(label='Use different seed for each picture', value=False, elem_id=self.elem_id(\"different_seeds\"))\n            with gr.Column():\n                prompt_type = gr.Radio([\"positive\", \"negative\"], label=\"Select prompt\", elem_id=self.elem_id(\"prompt_type\"), value=\"positive\")\n                variations_delimiter = gr.Radio([\"comma\", \"space\"], label=\"Select joining char\", elem_id=self.elem_id(\"variations_delimiter\"), value=\"comma\")\n            with gr.Column():\n                margin_size = gr.Slider(label=\"Grid margins (px)\", minimum=0, maximum=500, value=0, step=2, elem_id=self.elem_id(\"margin_size\"))\n\n        return [put_at_start, different_seeds, prompt_type, variations_delimiter, margin_size]\n\n    def run(self, p, put_at_start, different_seeds, prompt_type, variations_delimiter, margin_size):\n        modules.processing.fix_seed(p)\n        # Raise error if promp type is not positive or negative\n        if prompt_type not in [\"positive\", \"negative\"]:\n            raise ValueError(f\"Unknown prompt type {prompt_type}\")\n        # Raise error if variations delimiter is not comma or space\n        if variations_delimiter not in [\"comma\", \"space\"]:\n            raise ValueError(f\"Unknown variations delimiter {variations_delimiter}\")\n\n        prompt = p.prompt if prompt_type == \"positive\" else p.negative_prompt\n        original_prompt = prompt[0] if type(prompt) == list else prompt\n        positive_prompt = p.prompt[0] if type(p.prompt) == list else p.prompt\n\n        delimiter = \", \" if variations_delimiter == \"comma\" else \" \"\n\n        all_prompts = []\n        prompt_matrix_parts = original_prompt.split(\"|\")\n        combination_count = 2 ** (len(prompt_matrix_parts) - 1)\n        for combination_num in range(combination_count):\n            selected_prompts = [text.strip().strip(',') for n, text in enumerate(prompt_matrix_parts[1:]) if combination_num & (1 << n)]\n\n            if put_at_start:\n                selected_prompts = selected_prompts + [prompt_matrix_parts[0]]\n            else:\n                selected_prompts = [prompt_matrix_parts[0]] + selected_prompts\n\n            all_prompts.append(delimiter.join(selected_prompts))\n\n        p.n_iter = math.ceil(len(all_prompts) / p.batch_size)\n        p.do_not_save_grid = True\n\n        print(f\"Prompt matrix will create {len(all_prompts)} images using a total of {p.n_iter} batches.\")\n\n        if prompt_type == \"positive\":\n            p.prompt = all_prompts\n        else:\n            p.negative_prompt = all_prompts\n        p.seed = [p.seed + (i if different_seeds else 0) for i in range(len(all_prompts))]\n        p.prompt_for_display = positive_prompt\n        processed = process_images(p)\n\n        grid = images.image_grid(processed.images, p.batch_size, rows=1 << ((len(prompt_matrix_parts) - 1) // 2))\n        grid = images.draw_prompt_matrix(grid, processed.images[0].width, processed.images[0].height, prompt_matrix_parts, margin_size)\n        processed.images.insert(0, grid)\n        processed.index_of_first_image = 1\n        processed.infotexts.insert(0, processed.infotexts[0])\n\n        if opts.grid_save:\n            images.save_image(processed.images[0], p.outpath_grids, \"prompt_matrix\", extension=opts.grid_format, prompt=original_prompt, seed=processed.seed, grid=True, p=p)\n\n        return processed\n", "scripts/img2imgalt.py": "from collections import namedtuple\n\nimport numpy as np\nfrom tqdm import trange\n\nimport modules.scripts as scripts\nimport gradio as gr\n\nfrom modules import processing, shared, sd_samplers, sd_samplers_common\n\nimport torch\nimport k_diffusion as K\n\ndef find_noise_for_image(p, cond, uncond, cfg_scale, steps):\n    x = p.init_latent\n\n    s_in = x.new_ones([x.shape[0]])\n    if shared.sd_model.parameterization == \"v\":\n        dnw = K.external.CompVisVDenoiser(shared.sd_model)\n        skip = 1\n    else:\n        dnw = K.external.CompVisDenoiser(shared.sd_model)\n        skip = 0\n    sigmas = dnw.get_sigmas(steps).flip(0)\n\n    shared.state.sampling_steps = steps\n\n    for i in trange(1, len(sigmas)):\n        shared.state.sampling_step += 1\n\n        x_in = torch.cat([x] * 2)\n        sigma_in = torch.cat([sigmas[i] * s_in] * 2)\n        cond_in = torch.cat([uncond, cond])\n\n        image_conditioning = torch.cat([p.image_conditioning] * 2)\n        cond_in = {\"c_concat\": [image_conditioning], \"c_crossattn\": [cond_in]}\n\n        c_out, c_in = [K.utils.append_dims(k, x_in.ndim) for k in dnw.get_scalings(sigma_in)[skip:]]\n        t = dnw.sigma_to_t(sigma_in)\n\n        eps = shared.sd_model.apply_model(x_in * c_in, t, cond=cond_in)\n        denoised_uncond, denoised_cond = (x_in + eps * c_out).chunk(2)\n\n        denoised = denoised_uncond + (denoised_cond - denoised_uncond) * cfg_scale\n\n        d = (x - denoised) / sigmas[i]\n        dt = sigmas[i] - sigmas[i - 1]\n\n        x = x + d * dt\n\n        sd_samplers_common.store_latent(x)\n\n        # This shouldn't be necessary, but solved some VRAM issues\n        del x_in, sigma_in, cond_in, c_out, c_in, t,\n        del eps, denoised_uncond, denoised_cond, denoised, d, dt\n\n    shared.state.nextjob()\n\n    return x / x.std()\n\n\nCached = namedtuple(\"Cached\", [\"noise\", \"cfg_scale\", \"steps\", \"latent\", \"original_prompt\", \"original_negative_prompt\", \"sigma_adjustment\"])\n\n\n# Based on changes suggested by briansemrau in https://github.com/AUTOMATIC1111/stable-diffusion-webui/issues/736\ndef find_noise_for_image_sigma_adjustment(p, cond, uncond, cfg_scale, steps):\n    x = p.init_latent\n\n    s_in = x.new_ones([x.shape[0]])\n    if shared.sd_model.parameterization == \"v\":\n        dnw = K.external.CompVisVDenoiser(shared.sd_model)\n        skip = 1\n    else:\n        dnw = K.external.CompVisDenoiser(shared.sd_model)\n        skip = 0\n    sigmas = dnw.get_sigmas(steps).flip(0)\n\n    shared.state.sampling_steps = steps\n\n    for i in trange(1, len(sigmas)):\n        shared.state.sampling_step += 1\n\n        x_in = torch.cat([x] * 2)\n        sigma_in = torch.cat([sigmas[i - 1] * s_in] * 2)\n        cond_in = torch.cat([uncond, cond])\n\n        image_conditioning = torch.cat([p.image_conditioning] * 2)\n        cond_in = {\"c_concat\": [image_conditioning], \"c_crossattn\": [cond_in]}\n\n        c_out, c_in = [K.utils.append_dims(k, x_in.ndim) for k in dnw.get_scalings(sigma_in)[skip:]]\n\n        if i == 1:\n            t = dnw.sigma_to_t(torch.cat([sigmas[i] * s_in] * 2))\n        else:\n            t = dnw.sigma_to_t(sigma_in)\n\n        eps = shared.sd_model.apply_model(x_in * c_in, t, cond=cond_in)\n        denoised_uncond, denoised_cond = (x_in + eps * c_out).chunk(2)\n\n        denoised = denoised_uncond + (denoised_cond - denoised_uncond) * cfg_scale\n\n        if i == 1:\n            d = (x - denoised) / (2 * sigmas[i])\n        else:\n            d = (x - denoised) / sigmas[i - 1]\n\n        dt = sigmas[i] - sigmas[i - 1]\n        x = x + d * dt\n\n        sd_samplers_common.store_latent(x)\n\n        # This shouldn't be necessary, but solved some VRAM issues\n        del x_in, sigma_in, cond_in, c_out, c_in, t,\n        del eps, denoised_uncond, denoised_cond, denoised, d, dt\n\n    shared.state.nextjob()\n\n    return x / sigmas[-1]\n\n\nclass Script(scripts.Script):\n    def __init__(self):\n        self.cache = None\n\n    def title(self):\n        return \"img2img alternative test\"\n\n    def show(self, is_img2img):\n        return is_img2img\n\n    def ui(self, is_img2img):\n        info = gr.Markdown('''\n        * `CFG Scale` should be 2 or lower.\n        ''')\n\n        override_sampler = gr.Checkbox(label=\"Override `Sampling method` to Euler?(this method is built for it)\", value=True, elem_id=self.elem_id(\"override_sampler\"))\n\n        override_prompt = gr.Checkbox(label=\"Override `prompt` to the same value as `original prompt`?(and `negative prompt`)\", value=True, elem_id=self.elem_id(\"override_prompt\"))\n        original_prompt = gr.Textbox(label=\"Original prompt\", lines=1, elem_id=self.elem_id(\"original_prompt\"))\n        original_negative_prompt = gr.Textbox(label=\"Original negative prompt\", lines=1, elem_id=self.elem_id(\"original_negative_prompt\"))\n\n        override_steps = gr.Checkbox(label=\"Override `Sampling Steps` to the same value as `Decode steps`?\", value=True, elem_id=self.elem_id(\"override_steps\"))\n        st = gr.Slider(label=\"Decode steps\", minimum=1, maximum=150, step=1, value=50, elem_id=self.elem_id(\"st\"))\n\n        override_strength = gr.Checkbox(label=\"Override `Denoising strength` to 1?\", value=True, elem_id=self.elem_id(\"override_strength\"))\n\n        cfg = gr.Slider(label=\"Decode CFG scale\", minimum=0.0, maximum=15.0, step=0.1, value=1.0, elem_id=self.elem_id(\"cfg\"))\n        randomness = gr.Slider(label=\"Randomness\", minimum=0.0, maximum=1.0, step=0.01, value=0.0, elem_id=self.elem_id(\"randomness\"))\n        sigma_adjustment = gr.Checkbox(label=\"Sigma adjustment for finding noise for image\", value=False, elem_id=self.elem_id(\"sigma_adjustment\"))\n\n        return [\n            info,\n            override_sampler,\n            override_prompt, original_prompt, original_negative_prompt,\n            override_steps, st,\n            override_strength,\n            cfg, randomness, sigma_adjustment,\n        ]\n\n    def run(self, p, _, override_sampler, override_prompt, original_prompt, original_negative_prompt, override_steps, st, override_strength, cfg, randomness, sigma_adjustment):\n        # Override\n        if override_sampler:\n            p.sampler_name = \"Euler\"\n        if override_prompt:\n            p.prompt = original_prompt\n            p.negative_prompt = original_negative_prompt\n        if override_steps:\n            p.steps = st\n        if override_strength:\n            p.denoising_strength = 1.0\n\n        def sample_extra(conditioning, unconditional_conditioning, seeds, subseeds, subseed_strength, prompts):\n            lat = (p.init_latent.cpu().numpy() * 10).astype(int)\n\n            same_params = self.cache is not None and self.cache.cfg_scale == cfg and self.cache.steps == st \\\n                                and self.cache.original_prompt == original_prompt \\\n                                and self.cache.original_negative_prompt == original_negative_prompt \\\n                                and self.cache.sigma_adjustment == sigma_adjustment\n            same_everything = same_params and self.cache.latent.shape == lat.shape and np.abs(self.cache.latent-lat).sum() < 100\n\n            if same_everything:\n                rec_noise = self.cache.noise\n            else:\n                shared.state.job_count += 1\n                cond = p.sd_model.get_learned_conditioning(p.batch_size * [original_prompt])\n                uncond = p.sd_model.get_learned_conditioning(p.batch_size * [original_negative_prompt])\n                if sigma_adjustment:\n                    rec_noise = find_noise_for_image_sigma_adjustment(p, cond, uncond, cfg, st)\n                else:\n                    rec_noise = find_noise_for_image(p, cond, uncond, cfg, st)\n                self.cache = Cached(rec_noise, cfg, st, lat, original_prompt, original_negative_prompt, sigma_adjustment)\n\n            rand_noise = processing.create_random_tensors(p.init_latent.shape[1:], seeds=seeds, subseeds=subseeds, subseed_strength=p.subseed_strength, seed_resize_from_h=p.seed_resize_from_h, seed_resize_from_w=p.seed_resize_from_w, p=p)\n\n            combined_noise = ((1 - randomness) * rec_noise + randomness * rand_noise) / ((randomness**2 + (1-randomness)**2) ** 0.5)\n\n            sampler = sd_samplers.create_sampler(p.sampler_name, p.sd_model)\n\n            sigmas = sampler.model_wrap.get_sigmas(p.steps)\n\n            noise_dt = combined_noise - (p.init_latent / sigmas[0])\n\n            p.seed = p.seed + 1\n\n            return sampler.sample_img2img(p, p.init_latent, noise_dt, conditioning, unconditional_conditioning, image_conditioning=p.image_conditioning)\n\n        p.sample = sample_extra\n\n        p.extra_generation_params[\"Decode prompt\"] = original_prompt\n        p.extra_generation_params[\"Decode negative prompt\"] = original_negative_prompt\n        p.extra_generation_params[\"Decode CFG scale\"] = cfg\n        p.extra_generation_params[\"Decode steps\"] = st\n        p.extra_generation_params[\"Randomness\"] = randomness\n        p.extra_generation_params[\"Sigma Adjustment\"] = sigma_adjustment\n\n        processed = processing.process_images(p)\n\n        return processed\n", "scripts/custom_code.py": "import modules.scripts as scripts\nimport gradio as gr\nimport ast\nimport copy\n\nfrom modules.processing import Processed\nfrom modules.shared import cmd_opts\n\n\ndef convertExpr2Expression(expr):\n    expr.lineno = 0\n    expr.col_offset = 0\n    result = ast.Expression(expr.value, lineno=0, col_offset = 0)\n\n    return result\n\n\ndef exec_with_return(code, module):\n    \"\"\"\n    like exec() but can return values\n    https://stackoverflow.com/a/52361938/5862977\n    \"\"\"\n    code_ast = ast.parse(code)\n\n    init_ast = copy.deepcopy(code_ast)\n    init_ast.body = code_ast.body[:-1]\n\n    last_ast = copy.deepcopy(code_ast)\n    last_ast.body = code_ast.body[-1:]\n\n    exec(compile(init_ast, \"<ast>\", \"exec\"), module.__dict__)\n    if type(last_ast.body[0]) == ast.Expr:\n        return eval(compile(convertExpr2Expression(last_ast.body[0]), \"<ast>\", \"eval\"), module.__dict__)\n    else:\n        exec(compile(last_ast, \"<ast>\", \"exec\"), module.__dict__)\n\n\nclass Script(scripts.Script):\n\n    def title(self):\n        return \"Custom code\"\n\n    def show(self, is_img2img):\n        return cmd_opts.allow_code\n\n    def ui(self, is_img2img):\n        example = \"\"\"from modules.processing import process_images\n\np.width = 768\np.height = 768\np.batch_size = 2\np.steps = 10\n\nreturn process_images(p)\n\"\"\"\n\n\n        code = gr.Code(value=example, language=\"python\", label=\"Python code\", elem_id=self.elem_id(\"code\"))\n        indent_level = gr.Number(label='Indent level', value=2, precision=0, elem_id=self.elem_id(\"indent_level\"))\n\n        return [code, indent_level]\n\n    def run(self, p, code, indent_level):\n        assert cmd_opts.allow_code, '--allow-code option must be enabled'\n\n        display_result_data = [[], -1, \"\"]\n\n        def display(imgs, s=display_result_data[1], i=display_result_data[2]):\n            display_result_data[0] = imgs\n            display_result_data[1] = s\n            display_result_data[2] = i\n\n        from types import ModuleType\n        module = ModuleType(\"testmodule\")\n        module.__dict__.update(globals())\n        module.p = p\n        module.display = display\n\n        indent = \" \" * indent_level\n        indented = code.replace('\\n', f\"\\n{indent}\")\n        body = f\"\"\"def __webuitemp__():\n{indent}{indented}\n__webuitemp__()\"\"\"\n\n        result = exec_with_return(body, module)\n\n        if isinstance(result, Processed):\n            return result\n\n        return Processed(p, *display_result_data)\n", "scripts/outpainting_mk_2.py": "import math\n\nimport numpy as np\nimport skimage\n\nimport modules.scripts as scripts\nimport gradio as gr\nfrom PIL import Image, ImageDraw\n\nfrom modules import images\nfrom modules.processing import Processed, process_images\nfrom modules.shared import opts, state\n\n\n# this function is taken from https://github.com/parlance-zz/g-diffuser-bot\ndef get_matched_noise(_np_src_image, np_mask_rgb, noise_q=1, color_variation=0.05):\n    # helper fft routines that keep ortho normalization and auto-shift before and after fft\n    def _fft2(data):\n        if data.ndim > 2:  # has channels\n            out_fft = np.zeros((data.shape[0], data.shape[1], data.shape[2]), dtype=np.complex128)\n            for c in range(data.shape[2]):\n                c_data = data[:, :, c]\n                out_fft[:, :, c] = np.fft.fft2(np.fft.fftshift(c_data), norm=\"ortho\")\n                out_fft[:, :, c] = np.fft.ifftshift(out_fft[:, :, c])\n        else:  # one channel\n            out_fft = np.zeros((data.shape[0], data.shape[1]), dtype=np.complex128)\n            out_fft[:, :] = np.fft.fft2(np.fft.fftshift(data), norm=\"ortho\")\n            out_fft[:, :] = np.fft.ifftshift(out_fft[:, :])\n\n        return out_fft\n\n    def _ifft2(data):\n        if data.ndim > 2:  # has channels\n            out_ifft = np.zeros((data.shape[0], data.shape[1], data.shape[2]), dtype=np.complex128)\n            for c in range(data.shape[2]):\n                c_data = data[:, :, c]\n                out_ifft[:, :, c] = np.fft.ifft2(np.fft.fftshift(c_data), norm=\"ortho\")\n                out_ifft[:, :, c] = np.fft.ifftshift(out_ifft[:, :, c])\n        else:  # one channel\n            out_ifft = np.zeros((data.shape[0], data.shape[1]), dtype=np.complex128)\n            out_ifft[:, :] = np.fft.ifft2(np.fft.fftshift(data), norm=\"ortho\")\n            out_ifft[:, :] = np.fft.ifftshift(out_ifft[:, :])\n\n        return out_ifft\n\n    def _get_gaussian_window(width, height, std=3.14, mode=0):\n        window_scale_x = float(width / min(width, height))\n        window_scale_y = float(height / min(width, height))\n\n        window = np.zeros((width, height))\n        x = (np.arange(width) / width * 2. - 1.) * window_scale_x\n        for y in range(height):\n            fy = (y / height * 2. - 1.) * window_scale_y\n            if mode == 0:\n                window[:, y] = np.exp(-(x ** 2 + fy ** 2) * std)\n            else:\n                window[:, y] = (1 / ((x ** 2 + 1.) * (fy ** 2 + 1.))) ** (std / 3.14)  # hey wait a minute that's not gaussian\n\n        return window\n\n    def _get_masked_window_rgb(np_mask_grey, hardness=1.):\n        np_mask_rgb = np.zeros((np_mask_grey.shape[0], np_mask_grey.shape[1], 3))\n        if hardness != 1.:\n            hardened = np_mask_grey[:] ** hardness\n        else:\n            hardened = np_mask_grey[:]\n        for c in range(3):\n            np_mask_rgb[:, :, c] = hardened[:]\n        return np_mask_rgb\n\n    width = _np_src_image.shape[0]\n    height = _np_src_image.shape[1]\n    num_channels = _np_src_image.shape[2]\n\n    _np_src_image[:] * (1. - np_mask_rgb)\n    np_mask_grey = (np.sum(np_mask_rgb, axis=2) / 3.)\n    img_mask = np_mask_grey > 1e-6\n    ref_mask = np_mask_grey < 1e-3\n\n    windowed_image = _np_src_image * (1. - _get_masked_window_rgb(np_mask_grey))\n    windowed_image /= np.max(windowed_image)\n    windowed_image += np.average(_np_src_image) * np_mask_rgb  # / (1.-np.average(np_mask_rgb))  # rather than leave the masked area black, we get better results from fft by filling the average unmasked color\n\n    src_fft = _fft2(windowed_image)  # get feature statistics from masked src img\n    src_dist = np.absolute(src_fft)\n    src_phase = src_fft / src_dist\n\n    # create a generator with a static seed to make outpainting deterministic / only follow global seed\n    rng = np.random.default_rng(0)\n\n    noise_window = _get_gaussian_window(width, height, mode=1)  # start with simple gaussian noise\n    noise_rgb = rng.random((width, height, num_channels))\n    noise_grey = (np.sum(noise_rgb, axis=2) / 3.)\n    noise_rgb *= color_variation  # the colorfulness of the starting noise is blended to greyscale with a parameter\n    for c in range(num_channels):\n        noise_rgb[:, :, c] += (1. - color_variation) * noise_grey\n\n    noise_fft = _fft2(noise_rgb)\n    for c in range(num_channels):\n        noise_fft[:, :, c] *= noise_window\n    noise_rgb = np.real(_ifft2(noise_fft))\n    shaped_noise_fft = _fft2(noise_rgb)\n    shaped_noise_fft[:, :, :] = np.absolute(shaped_noise_fft[:, :, :]) ** 2 * (src_dist ** noise_q) * src_phase  # perform the actual shaping\n\n    brightness_variation = 0.  # color_variation # todo: temporarily tying brightness variation to color variation for now\n    contrast_adjusted_np_src = _np_src_image[:] * (brightness_variation + 1.) - brightness_variation * 2.\n\n    # scikit-image is used for histogram matching, very convenient!\n    shaped_noise = np.real(_ifft2(shaped_noise_fft))\n    shaped_noise -= np.min(shaped_noise)\n    shaped_noise /= np.max(shaped_noise)\n    shaped_noise[img_mask, :] = skimage.exposure.match_histograms(shaped_noise[img_mask, :] ** 1., contrast_adjusted_np_src[ref_mask, :], channel_axis=1)\n    shaped_noise = _np_src_image[:] * (1. - np_mask_rgb) + shaped_noise * np_mask_rgb\n\n    matched_noise = shaped_noise[:]\n\n    return np.clip(matched_noise, 0., 1.)\n\n\n\nclass Script(scripts.Script):\n    def title(self):\n        return \"Outpainting mk2\"\n\n    def show(self, is_img2img):\n        return is_img2img\n\n    def ui(self, is_img2img):\n        if not is_img2img:\n            return None\n\n        info = gr.HTML(\"<p style=\\\"margin-bottom:0.75em\\\">Recommended settings: Sampling Steps: 80-100, Sampler: Euler a, Denoising strength: 0.8</p>\")\n\n        pixels = gr.Slider(label=\"Pixels to expand\", minimum=8, maximum=256, step=8, value=128, elem_id=self.elem_id(\"pixels\"))\n        mask_blur = gr.Slider(label='Mask blur', minimum=0, maximum=64, step=1, value=8, elem_id=self.elem_id(\"mask_blur\"))\n        direction = gr.CheckboxGroup(label=\"Outpainting direction\", choices=['left', 'right', 'up', 'down'], value=['left', 'right', 'up', 'down'], elem_id=self.elem_id(\"direction\"))\n        noise_q = gr.Slider(label=\"Fall-off exponent (lower=higher detail)\", minimum=0.0, maximum=4.0, step=0.01, value=1.0, elem_id=self.elem_id(\"noise_q\"))\n        color_variation = gr.Slider(label=\"Color variation\", minimum=0.0, maximum=1.0, step=0.01, value=0.05, elem_id=self.elem_id(\"color_variation\"))\n\n        return [info, pixels, mask_blur, direction, noise_q, color_variation]\n\n    def run(self, p, _, pixels, mask_blur, direction, noise_q, color_variation):\n        initial_seed_and_info = [None, None]\n\n        process_width = p.width\n        process_height = p.height\n\n        p.inpaint_full_res = False\n        p.inpainting_fill = 1\n        p.do_not_save_samples = True\n        p.do_not_save_grid = True\n\n        left = pixels if \"left\" in direction else 0\n        right = pixels if \"right\" in direction else 0\n        up = pixels if \"up\" in direction else 0\n        down = pixels if \"down\" in direction else 0\n\n        if left > 0 or right > 0:\n            mask_blur_x = mask_blur\n        else:\n            mask_blur_x = 0\n\n        if up > 0 or down > 0:\n            mask_blur_y = mask_blur\n        else:\n            mask_blur_y = 0\n\n        p.mask_blur_x = mask_blur_x*4\n        p.mask_blur_y = mask_blur_y*4\n\n        init_img = p.init_images[0]\n        target_w = math.ceil((init_img.width + left + right) / 64) * 64\n        target_h = math.ceil((init_img.height + up + down) / 64) * 64\n\n        if left > 0:\n            left = left * (target_w - init_img.width) // (left + right)\n\n        if right > 0:\n            right = target_w - init_img.width - left\n\n        if up > 0:\n            up = up * (target_h - init_img.height) // (up + down)\n\n        if down > 0:\n            down = target_h - init_img.height - up\n\n        def expand(init, count, expand_pixels, is_left=False, is_right=False, is_top=False, is_bottom=False):\n            is_horiz = is_left or is_right\n            is_vert = is_top or is_bottom\n            pixels_horiz = expand_pixels if is_horiz else 0\n            pixels_vert = expand_pixels if is_vert else 0\n\n            images_to_process = []\n            output_images = []\n            for n in range(count):\n                res_w = init[n].width + pixels_horiz\n                res_h = init[n].height + pixels_vert\n                process_res_w = math.ceil(res_w / 64) * 64\n                process_res_h = math.ceil(res_h / 64) * 64\n\n                img = Image.new(\"RGB\", (process_res_w, process_res_h))\n                img.paste(init[n], (pixels_horiz if is_left else 0, pixels_vert if is_top else 0))\n                mask = Image.new(\"RGB\", (process_res_w, process_res_h), \"white\")\n                draw = ImageDraw.Draw(mask)\n                draw.rectangle((\n                    expand_pixels + mask_blur_x if is_left else 0,\n                    expand_pixels + mask_blur_y if is_top else 0,\n                    mask.width - expand_pixels - mask_blur_x if is_right else res_w,\n                    mask.height - expand_pixels - mask_blur_y if is_bottom else res_h,\n                ), fill=\"black\")\n\n                np_image = (np.asarray(img) / 255.0).astype(np.float64)\n                np_mask = (np.asarray(mask) / 255.0).astype(np.float64)\n                noised = get_matched_noise(np_image, np_mask, noise_q, color_variation)\n                output_images.append(Image.fromarray(np.clip(noised * 255., 0., 255.).astype(np.uint8), mode=\"RGB\"))\n\n                target_width = min(process_width, init[n].width + pixels_horiz) if is_horiz else img.width\n                target_height = min(process_height, init[n].height + pixels_vert) if is_vert else img.height\n                p.width = target_width if is_horiz else img.width\n                p.height = target_height if is_vert else img.height\n\n                crop_region = (\n                    0 if is_left else output_images[n].width - target_width,\n                    0 if is_top else output_images[n].height - target_height,\n                    target_width if is_left else output_images[n].width,\n                    target_height if is_top else output_images[n].height,\n                )\n                mask = mask.crop(crop_region)\n                p.image_mask = mask\n\n                image_to_process = output_images[n].crop(crop_region)\n                images_to_process.append(image_to_process)\n\n            p.init_images = images_to_process\n\n            latent_mask = Image.new(\"RGB\", (p.width, p.height), \"white\")\n            draw = ImageDraw.Draw(latent_mask)\n            draw.rectangle((\n                expand_pixels + mask_blur_x * 2 if is_left else 0,\n                expand_pixels + mask_blur_y * 2 if is_top else 0,\n                mask.width - expand_pixels - mask_blur_x * 2 if is_right else res_w,\n                mask.height - expand_pixels - mask_blur_y * 2 if is_bottom else res_h,\n            ), fill=\"black\")\n            p.latent_mask = latent_mask\n\n            proc = process_images(p)\n\n            if initial_seed_and_info[0] is None:\n                initial_seed_and_info[0] = proc.seed\n                initial_seed_and_info[1] = proc.info\n\n            for n in range(count):\n                output_images[n].paste(proc.images[n], (0 if is_left else output_images[n].width - proc.images[n].width, 0 if is_top else output_images[n].height - proc.images[n].height))\n                output_images[n] = output_images[n].crop((0, 0, res_w, res_h))\n\n            return output_images\n\n        batch_count = p.n_iter\n        batch_size = p.batch_size\n        p.n_iter = 1\n        state.job_count = batch_count * ((1 if left > 0 else 0) + (1 if right > 0 else 0) + (1 if up > 0 else 0) + (1 if down > 0 else 0))\n        all_processed_images = []\n\n        for i in range(batch_count):\n            imgs = [init_img] * batch_size\n            state.job = f\"Batch {i + 1} out of {batch_count}\"\n\n            if left > 0:\n                imgs = expand(imgs, batch_size, left, is_left=True)\n            if right > 0:\n                imgs = expand(imgs, batch_size, right, is_right=True)\n            if up > 0:\n                imgs = expand(imgs, batch_size, up, is_top=True)\n            if down > 0:\n                imgs = expand(imgs, batch_size, down, is_bottom=True)\n\n            all_processed_images += imgs\n\n        all_images = all_processed_images\n\n        combined_grid_image = images.image_grid(all_processed_images)\n        unwanted_grid_because_of_img_count = len(all_processed_images) < 2 and opts.grid_only_if_multiple\n        if opts.return_grid and not unwanted_grid_because_of_img_count:\n            all_images = [combined_grid_image] + all_processed_images\n\n        res = Processed(p, all_images, initial_seed_and_info[0], initial_seed_and_info[1])\n\n        if opts.samples_save:\n            for img in all_processed_images:\n                images.save_image(img, p.outpath_samples, \"\", res.seed, p.prompt, opts.samples_format, info=res.info, p=p)\n\n        if opts.grid_save and not unwanted_grid_because_of_img_count:\n            images.save_image(combined_grid_image, p.outpath_grids, \"grid\", res.seed, p.prompt, opts.grid_format, info=res.info, short_filename=not opts.grid_extended_filename, grid=True, p=p)\n\n        return res\n", "scripts/xyz_grid.py": "from collections import namedtuple\nfrom copy import copy\nfrom itertools import permutations, chain\nimport random\nimport csv\nimport os.path\nfrom io import StringIO\nfrom PIL import Image\nimport numpy as np\n\nimport modules.scripts as scripts\nimport gradio as gr\n\nfrom modules import images, sd_samplers, processing, sd_models, sd_vae, sd_schedulers, errors\nfrom modules.processing import process_images, Processed, StableDiffusionProcessingTxt2Img\nfrom modules.shared import opts, state\nimport modules.shared as shared\nimport modules.sd_samplers\nimport modules.sd_models\nimport modules.sd_vae\nimport re\n\nfrom modules.ui_components import ToolButton\n\nfill_values_symbol = \"\\U0001f4d2\"  # \ud83d\udcd2\n\nAxisInfo = namedtuple('AxisInfo', ['axis', 'values'])\n\n\ndef apply_field(field):\n    def fun(p, x, xs):\n        setattr(p, field, x)\n\n    return fun\n\n\ndef apply_prompt(p, x, xs):\n    if xs[0] not in p.prompt and xs[0] not in p.negative_prompt:\n        raise RuntimeError(f\"Prompt S/R did not find {xs[0]} in prompt or negative prompt.\")\n\n    p.prompt = p.prompt.replace(xs[0], x)\n    p.negative_prompt = p.negative_prompt.replace(xs[0], x)\n\n\ndef apply_order(p, x, xs):\n    token_order = []\n\n    # Initially grab the tokens from the prompt, so they can be replaced in order of earliest seen\n    for token in x:\n        token_order.append((p.prompt.find(token), token))\n\n    token_order.sort(key=lambda t: t[0])\n\n    prompt_parts = []\n\n    # Split the prompt up, taking out the tokens\n    for _, token in token_order:\n        n = p.prompt.find(token)\n        prompt_parts.append(p.prompt[0:n])\n        p.prompt = p.prompt[n + len(token):]\n\n    # Rebuild the prompt with the tokens in the order we want\n    prompt_tmp = \"\"\n    for idx, part in enumerate(prompt_parts):\n        prompt_tmp += part\n        prompt_tmp += x[idx]\n    p.prompt = prompt_tmp + p.prompt\n\n\ndef confirm_samplers(p, xs):\n    for x in xs:\n        if x.lower() not in sd_samplers.samplers_map:\n            raise RuntimeError(f\"Unknown sampler: {x}\")\n\n\ndef apply_checkpoint(p, x, xs):\n    info = modules.sd_models.get_closet_checkpoint_match(x)\n    if info is None:\n        raise RuntimeError(f\"Unknown checkpoint: {x}\")\n    p.override_settings['sd_model_checkpoint'] = info.name\n\n\ndef confirm_checkpoints(p, xs):\n    for x in xs:\n        if modules.sd_models.get_closet_checkpoint_match(x) is None:\n            raise RuntimeError(f\"Unknown checkpoint: {x}\")\n\n\ndef confirm_checkpoints_or_none(p, xs):\n    for x in xs:\n        if x in (None, \"\", \"None\", \"none\"):\n            continue\n\n        if modules.sd_models.get_closet_checkpoint_match(x) is None:\n            raise RuntimeError(f\"Unknown checkpoint: {x}\")\n\n\ndef apply_clip_skip(p, x, xs):\n    opts.data[\"CLIP_stop_at_last_layers\"] = x\n\n\ndef apply_upscale_latent_space(p, x, xs):\n    if x.lower().strip() != '0':\n        opts.data[\"use_scale_latent_for_hires_fix\"] = True\n    else:\n        opts.data[\"use_scale_latent_for_hires_fix\"] = False\n\n\ndef apply_size(p, x: str, xs) -> None:\n    try:\n        width, _, height = x.partition('x')\n        width = int(width.strip())\n        height = int(height.strip())\n        p.width = width\n        p.height = height\n    except ValueError:\n        print(f\"Invalid size in XYZ plot: {x}\")\n\n\ndef find_vae(name: str):\n    if name.lower() in ['auto', 'automatic']:\n        return modules.sd_vae.unspecified\n    if name.lower() == 'none':\n        return None\n    else:\n        choices = [x for x in sorted(modules.sd_vae.vae_dict, key=lambda x: len(x)) if name.lower().strip() in x.lower()]\n        if len(choices) == 0:\n            print(f\"No VAE found for {name}; using automatic\")\n            return modules.sd_vae.unspecified\n        else:\n            return modules.sd_vae.vae_dict[choices[0]]\n\n\ndef apply_vae(p, x, xs):\n    modules.sd_vae.reload_vae_weights(shared.sd_model, vae_file=find_vae(x))\n\n\ndef apply_styles(p: StableDiffusionProcessingTxt2Img, x: str, _):\n    p.styles.extend(x.split(','))\n\n\ndef apply_uni_pc_order(p, x, xs):\n    opts.data[\"uni_pc_order\"] = min(x, p.steps - 1)\n\n\ndef apply_face_restore(p, opt, x):\n    opt = opt.lower()\n    if opt == 'codeformer':\n        is_active = True\n        p.face_restoration_model = 'CodeFormer'\n    elif opt == 'gfpgan':\n        is_active = True\n        p.face_restoration_model = 'GFPGAN'\n    else:\n        is_active = opt in ('true', 'yes', 'y', '1')\n\n    p.restore_faces = is_active\n\n\ndef apply_override(field, boolean: bool = False):\n    def fun(p, x, xs):\n        if boolean:\n            x = True if x.lower() == \"true\" else False\n        p.override_settings[field] = x\n    return fun\n\n\ndef boolean_choice(reverse: bool = False):\n    def choice():\n        return [\"False\", \"True\"] if reverse else [\"True\", \"False\"]\n    return choice\n\n\ndef format_value_add_label(p, opt, x):\n    if type(x) == float:\n        x = round(x, 8)\n\n    return f\"{opt.label}: {x}\"\n\n\ndef format_value(p, opt, x):\n    if type(x) == float:\n        x = round(x, 8)\n    return x\n\n\ndef format_value_join_list(p, opt, x):\n    return \", \".join(x)\n\n\ndef do_nothing(p, x, xs):\n    pass\n\n\ndef format_nothing(p, opt, x):\n    return \"\"\n\n\ndef format_remove_path(p, opt, x):\n    return os.path.basename(x)\n\n\ndef str_permutations(x):\n    \"\"\"dummy function for specifying it in AxisOption's type when you want to get a list of permutations\"\"\"\n    return x\n\n\ndef list_to_csv_string(data_list):\n    with StringIO() as o:\n        csv.writer(o).writerow(data_list)\n        return o.getvalue().strip()\n\n\ndef csv_string_to_list_strip(data_str):\n    return list(map(str.strip, chain.from_iterable(csv.reader(StringIO(data_str)))))\n\n\nclass AxisOption:\n    def __init__(self, label, type, apply, format_value=format_value_add_label, confirm=None, cost=0.0, choices=None, prepare=None):\n        self.label = label\n        self.type = type\n        self.apply = apply\n        self.format_value = format_value\n        self.confirm = confirm\n        self.cost = cost\n        self.prepare = prepare\n        self.choices = choices\n\n\nclass AxisOptionImg2Img(AxisOption):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.is_img2img = True\n\n\nclass AxisOptionTxt2Img(AxisOption):\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.is_img2img = False\n\n\naxis_options = [\n    AxisOption(\"Nothing\", str, do_nothing, format_value=format_nothing),\n    AxisOption(\"Seed\", int, apply_field(\"seed\")),\n    AxisOption(\"Var. seed\", int, apply_field(\"subseed\")),\n    AxisOption(\"Var. strength\", float, apply_field(\"subseed_strength\")),\n    AxisOption(\"Steps\", int, apply_field(\"steps\")),\n    AxisOptionTxt2Img(\"Hires steps\", int, apply_field(\"hr_second_pass_steps\")),\n    AxisOption(\"CFG Scale\", float, apply_field(\"cfg_scale\")),\n    AxisOptionImg2Img(\"Image CFG Scale\", float, apply_field(\"image_cfg_scale\")),\n    AxisOption(\"Prompt S/R\", str, apply_prompt, format_value=format_value),\n    AxisOption(\"Prompt order\", str_permutations, apply_order, format_value=format_value_join_list),\n    AxisOptionTxt2Img(\"Sampler\", str, apply_field(\"sampler_name\"), format_value=format_value, confirm=confirm_samplers, choices=lambda: [x.name for x in sd_samplers.samplers if x.name not in opts.hide_samplers]),\n    AxisOptionTxt2Img(\"Hires sampler\", str, apply_field(\"hr_sampler_name\"), confirm=confirm_samplers, choices=lambda: [x.name for x in sd_samplers.samplers_for_img2img if x.name not in opts.hide_samplers]),\n    AxisOptionImg2Img(\"Sampler\", str, apply_field(\"sampler_name\"), format_value=format_value, confirm=confirm_samplers, choices=lambda: [x.name for x in sd_samplers.samplers_for_img2img if x.name not in opts.hide_samplers]),\n    AxisOption(\"Checkpoint name\", str, apply_checkpoint, format_value=format_remove_path, confirm=confirm_checkpoints, cost=1.0, choices=lambda: sorted(sd_models.checkpoints_list, key=str.casefold)),\n    AxisOption(\"Negative Guidance minimum sigma\", float, apply_field(\"s_min_uncond\")),\n    AxisOption(\"Sigma Churn\", float, apply_field(\"s_churn\")),\n    AxisOption(\"Sigma min\", float, apply_field(\"s_tmin\")),\n    AxisOption(\"Sigma max\", float, apply_field(\"s_tmax\")),\n    AxisOption(\"Sigma noise\", float, apply_field(\"s_noise\")),\n    AxisOption(\"Schedule type\", str, apply_field(\"scheduler\"), choices=lambda: [x.label for x in sd_schedulers.schedulers]),\n    AxisOption(\"Schedule min sigma\", float, apply_override(\"sigma_min\")),\n    AxisOption(\"Schedule max sigma\", float, apply_override(\"sigma_max\")),\n    AxisOption(\"Schedule rho\", float, apply_override(\"rho\")),\n    AxisOption(\"Eta\", float, apply_field(\"eta\")),\n    AxisOption(\"Clip skip\", int, apply_clip_skip),\n    AxisOption(\"Denoising\", float, apply_field(\"denoising_strength\")),\n    AxisOption(\"Initial noise multiplier\", float, apply_field(\"initial_noise_multiplier\")),\n    AxisOption(\"Extra noise\", float, apply_override(\"img2img_extra_noise\")),\n    AxisOptionTxt2Img(\"Hires upscaler\", str, apply_field(\"hr_upscaler\"), choices=lambda: [*shared.latent_upscale_modes, *[x.name for x in shared.sd_upscalers]]),\n    AxisOptionImg2Img(\"Cond. Image Mask Weight\", float, apply_field(\"inpainting_mask_weight\")),\n    AxisOption(\"VAE\", str, apply_vae, cost=0.7, choices=lambda: ['None'] + list(sd_vae.vae_dict)),\n    AxisOption(\"Styles\", str, apply_styles, choices=lambda: list(shared.prompt_styles.styles)),\n    AxisOption(\"UniPC Order\", int, apply_uni_pc_order, cost=0.5),\n    AxisOption(\"Face restore\", str, apply_face_restore, format_value=format_value),\n    AxisOption(\"Token merging ratio\", float, apply_override('token_merging_ratio')),\n    AxisOption(\"Token merging ratio high-res\", float, apply_override('token_merging_ratio_hr')),\n    AxisOption(\"Always discard next-to-last sigma\", str, apply_override('always_discard_next_to_last_sigma', boolean=True), choices=boolean_choice(reverse=True)),\n    AxisOption(\"SGM noise multiplier\", str, apply_override('sgm_noise_multiplier', boolean=True), choices=boolean_choice(reverse=True)),\n    AxisOption(\"Refiner checkpoint\", str, apply_field('refiner_checkpoint'), format_value=format_remove_path, confirm=confirm_checkpoints_or_none, cost=1.0, choices=lambda: ['None'] + sorted(sd_models.checkpoints_list, key=str.casefold)),\n    AxisOption(\"Refiner switch at\", float, apply_field('refiner_switch_at')),\n    AxisOption(\"RNG source\", str, apply_override(\"randn_source\"), choices=lambda: [\"GPU\", \"CPU\", \"NV\"]),\n    AxisOption(\"FP8 mode\", str, apply_override(\"fp8_storage\"), cost=0.9, choices=lambda: [\"Disable\", \"Enable for SDXL\", \"Enable\"]),\n    AxisOption(\"Size\", str, apply_size),\n]\n\n\ndef draw_xyz_grid(p, xs, ys, zs, x_labels, y_labels, z_labels, cell, draw_legend, include_lone_images, include_sub_grids, first_axes_processed, second_axes_processed, margin_size):\n    hor_texts = [[images.GridAnnotation(x)] for x in x_labels]\n    ver_texts = [[images.GridAnnotation(y)] for y in y_labels]\n    title_texts = [[images.GridAnnotation(z)] for z in z_labels]\n\n    list_size = (len(xs) * len(ys) * len(zs))\n\n    processed_result = None\n\n    state.job_count = list_size * p.n_iter\n\n    def process_cell(x, y, z, ix, iy, iz):\n        nonlocal processed_result\n\n        def index(ix, iy, iz):\n            return ix + iy * len(xs) + iz * len(xs) * len(ys)\n\n        state.job = f\"{index(ix, iy, iz) + 1} out of {list_size}\"\n\n        processed: Processed = cell(x, y, z, ix, iy, iz)\n\n        if processed_result is None:\n            # Use our first processed result object as a template container to hold our full results\n            processed_result = copy(processed)\n            processed_result.images = [None] * list_size\n            processed_result.all_prompts = [None] * list_size\n            processed_result.all_seeds = [None] * list_size\n            processed_result.infotexts = [None] * list_size\n            processed_result.index_of_first_image = 1\n\n        idx = index(ix, iy, iz)\n        if processed.images:\n            # Non-empty list indicates some degree of success.\n            processed_result.images[idx] = processed.images[0]\n            processed_result.all_prompts[idx] = processed.prompt\n            processed_result.all_seeds[idx] = processed.seed\n            processed_result.infotexts[idx] = processed.infotexts[0]\n        else:\n            cell_mode = \"P\"\n            cell_size = (processed_result.width, processed_result.height)\n            if processed_result.images[0] is not None:\n                cell_mode = processed_result.images[0].mode\n                # This corrects size in case of batches:\n                cell_size = processed_result.images[0].size\n            processed_result.images[idx] = Image.new(cell_mode, cell_size)\n\n    if first_axes_processed == 'x':\n        for ix, x in enumerate(xs):\n            if second_axes_processed == 'y':\n                for iy, y in enumerate(ys):\n                    for iz, z in enumerate(zs):\n                        process_cell(x, y, z, ix, iy, iz)\n            else:\n                for iz, z in enumerate(zs):\n                    for iy, y in enumerate(ys):\n                        process_cell(x, y, z, ix, iy, iz)\n    elif first_axes_processed == 'y':\n        for iy, y in enumerate(ys):\n            if second_axes_processed == 'x':\n                for ix, x in enumerate(xs):\n                    for iz, z in enumerate(zs):\n                        process_cell(x, y, z, ix, iy, iz)\n            else:\n                for iz, z in enumerate(zs):\n                    for ix, x in enumerate(xs):\n                        process_cell(x, y, z, ix, iy, iz)\n    elif first_axes_processed == 'z':\n        for iz, z in enumerate(zs):\n            if second_axes_processed == 'x':\n                for ix, x in enumerate(xs):\n                    for iy, y in enumerate(ys):\n                        process_cell(x, y, z, ix, iy, iz)\n            else:\n                for iy, y in enumerate(ys):\n                    for ix, x in enumerate(xs):\n                        process_cell(x, y, z, ix, iy, iz)\n\n    if not processed_result:\n        # Should never happen, I've only seen it on one of four open tabs and it needed to refresh.\n        print(\"Unexpected error: Processing could not begin, you may need to refresh the tab or restart the service.\")\n        return Processed(p, [])\n    elif not any(processed_result.images):\n        print(\"Unexpected error: draw_xyz_grid failed to return even a single processed image\")\n        return Processed(p, [])\n\n    z_count = len(zs)\n\n    for i in range(z_count):\n        start_index = (i * len(xs) * len(ys)) + i\n        end_index = start_index + len(xs) * len(ys)\n        grid = images.image_grid(processed_result.images[start_index:end_index], rows=len(ys))\n        if draw_legend:\n            grid = images.draw_grid_annotations(grid, processed_result.images[start_index].size[0], processed_result.images[start_index].size[1], hor_texts, ver_texts, margin_size)\n        processed_result.images.insert(i, grid)\n        processed_result.all_prompts.insert(i, processed_result.all_prompts[start_index])\n        processed_result.all_seeds.insert(i, processed_result.all_seeds[start_index])\n        processed_result.infotexts.insert(i, processed_result.infotexts[start_index])\n\n    sub_grid_size = processed_result.images[0].size\n    z_grid = images.image_grid(processed_result.images[:z_count], rows=1)\n    if draw_legend:\n        z_grid = images.draw_grid_annotations(z_grid, sub_grid_size[0], sub_grid_size[1], title_texts, [[images.GridAnnotation()]])\n    processed_result.images.insert(0, z_grid)\n    # TODO: Deeper aspects of the program rely on grid info being misaligned between metadata arrays, which is not ideal.\n    # processed_result.all_prompts.insert(0, processed_result.all_prompts[0])\n    # processed_result.all_seeds.insert(0, processed_result.all_seeds[0])\n    processed_result.infotexts.insert(0, processed_result.infotexts[0])\n\n    return processed_result\n\n\nclass SharedSettingsStackHelper(object):\n    def __enter__(self):\n        self.CLIP_stop_at_last_layers = opts.CLIP_stop_at_last_layers\n        self.vae = opts.sd_vae\n        self.uni_pc_order = opts.uni_pc_order\n\n    def __exit__(self, exc_type, exc_value, tb):\n        opts.data[\"sd_vae\"] = self.vae\n        opts.data[\"uni_pc_order\"] = self.uni_pc_order\n        modules.sd_models.reload_model_weights()\n        modules.sd_vae.reload_vae_weights()\n\n        opts.data[\"CLIP_stop_at_last_layers\"] = self.CLIP_stop_at_last_layers\n\n\nre_range = re.compile(r\"\\s*([+-]?\\s*\\d+)\\s*-\\s*([+-]?\\s*\\d+)(?:\\s*\\(([+-]\\d+)\\s*\\))?\\s*\")\nre_range_float = re.compile(r\"\\s*([+-]?\\s*\\d+(?:.\\d*)?)\\s*-\\s*([+-]?\\s*\\d+(?:.\\d*)?)(?:\\s*\\(([+-]\\d+(?:.\\d*)?)\\s*\\))?\\s*\")\n\nre_range_count = re.compile(r\"\\s*([+-]?\\s*\\d+)\\s*-\\s*([+-]?\\s*\\d+)(?:\\s*\\[(\\d+)\\s*])?\\s*\")\nre_range_count_float = re.compile(r\"\\s*([+-]?\\s*\\d+(?:.\\d*)?)\\s*-\\s*([+-]?\\s*\\d+(?:.\\d*)?)(?:\\s*\\[(\\d+(?:.\\d*)?)\\s*])?\\s*\")\n\n\nclass Script(scripts.Script):\n    def title(self):\n        return \"X/Y/Z plot\"\n\n    def ui(self, is_img2img):\n        self.current_axis_options = [x for x in axis_options if type(x) == AxisOption or x.is_img2img == is_img2img]\n\n        with gr.Row():\n            with gr.Column(scale=19):\n                with gr.Row():\n                    x_type = gr.Dropdown(label=\"X type\", choices=[x.label for x in self.current_axis_options], value=self.current_axis_options[1].label, type=\"index\", elem_id=self.elem_id(\"x_type\"))\n                    x_values = gr.Textbox(label=\"X values\", lines=1, elem_id=self.elem_id(\"x_values\"))\n                    x_values_dropdown = gr.Dropdown(label=\"X values\", visible=False, multiselect=True, interactive=True)\n                    fill_x_button = ToolButton(value=fill_values_symbol, elem_id=\"xyz_grid_fill_x_tool_button\", visible=False)\n\n                with gr.Row():\n                    y_type = gr.Dropdown(label=\"Y type\", choices=[x.label for x in self.current_axis_options], value=self.current_axis_options[0].label, type=\"index\", elem_id=self.elem_id(\"y_type\"))\n                    y_values = gr.Textbox(label=\"Y values\", lines=1, elem_id=self.elem_id(\"y_values\"))\n                    y_values_dropdown = gr.Dropdown(label=\"Y values\", visible=False, multiselect=True, interactive=True)\n                    fill_y_button = ToolButton(value=fill_values_symbol, elem_id=\"xyz_grid_fill_y_tool_button\", visible=False)\n\n                with gr.Row():\n                    z_type = gr.Dropdown(label=\"Z type\", choices=[x.label for x in self.current_axis_options], value=self.current_axis_options[0].label, type=\"index\", elem_id=self.elem_id(\"z_type\"))\n                    z_values = gr.Textbox(label=\"Z values\", lines=1, elem_id=self.elem_id(\"z_values\"))\n                    z_values_dropdown = gr.Dropdown(label=\"Z values\", visible=False, multiselect=True, interactive=True)\n                    fill_z_button = ToolButton(value=fill_values_symbol, elem_id=\"xyz_grid_fill_z_tool_button\", visible=False)\n\n        with gr.Row(variant=\"compact\", elem_id=\"axis_options\"):\n            with gr.Column():\n                draw_legend = gr.Checkbox(label='Draw legend', value=True, elem_id=self.elem_id(\"draw_legend\"))\n                no_fixed_seeds = gr.Checkbox(label='Keep -1 for seeds', value=False, elem_id=self.elem_id(\"no_fixed_seeds\"))\n                with gr.Row():\n                    vary_seeds_x = gr.Checkbox(label='Vary seeds for X', value=False, min_width=80, elem_id=self.elem_id(\"vary_seeds_x\"), tooltip=\"Use different seeds for images along X axis.\")\n                    vary_seeds_y = gr.Checkbox(label='Vary seeds for Y', value=False, min_width=80, elem_id=self.elem_id(\"vary_seeds_y\"), tooltip=\"Use different seeds for images along Y axis.\")\n                    vary_seeds_z = gr.Checkbox(label='Vary seeds for Z', value=False, min_width=80, elem_id=self.elem_id(\"vary_seeds_z\"), tooltip=\"Use different seeds for images along Z axis.\")\n            with gr.Column():\n                include_lone_images = gr.Checkbox(label='Include Sub Images', value=False, elem_id=self.elem_id(\"include_lone_images\"))\n                include_sub_grids = gr.Checkbox(label='Include Sub Grids', value=False, elem_id=self.elem_id(\"include_sub_grids\"))\n                csv_mode = gr.Checkbox(label='Use text inputs instead of dropdowns', value=False, elem_id=self.elem_id(\"csv_mode\"))\n            with gr.Column():\n                margin_size = gr.Slider(label=\"Grid margins (px)\", minimum=0, maximum=500, value=0, step=2, elem_id=self.elem_id(\"margin_size\"))\n\n        with gr.Row(variant=\"compact\", elem_id=\"swap_axes\"):\n            swap_xy_axes_button = gr.Button(value=\"Swap X/Y axes\", elem_id=\"xy_grid_swap_axes_button\")\n            swap_yz_axes_button = gr.Button(value=\"Swap Y/Z axes\", elem_id=\"yz_grid_swap_axes_button\")\n            swap_xz_axes_button = gr.Button(value=\"Swap X/Z axes\", elem_id=\"xz_grid_swap_axes_button\")\n\n        def swap_axes(axis1_type, axis1_values, axis1_values_dropdown, axis2_type, axis2_values, axis2_values_dropdown):\n            return self.current_axis_options[axis2_type].label, axis2_values, axis2_values_dropdown, self.current_axis_options[axis1_type].label, axis1_values, axis1_values_dropdown\n\n        xy_swap_args = [x_type, x_values, x_values_dropdown, y_type, y_values, y_values_dropdown]\n        swap_xy_axes_button.click(swap_axes, inputs=xy_swap_args, outputs=xy_swap_args)\n        yz_swap_args = [y_type, y_values, y_values_dropdown, z_type, z_values, z_values_dropdown]\n        swap_yz_axes_button.click(swap_axes, inputs=yz_swap_args, outputs=yz_swap_args)\n        xz_swap_args = [x_type, x_values, x_values_dropdown, z_type, z_values, z_values_dropdown]\n        swap_xz_axes_button.click(swap_axes, inputs=xz_swap_args, outputs=xz_swap_args)\n\n        def fill(axis_type, csv_mode):\n            axis = self.current_axis_options[axis_type]\n            if axis.choices:\n                if csv_mode:\n                    return list_to_csv_string(axis.choices()), gr.update()\n                else:\n                    return gr.update(), axis.choices()\n            else:\n                return gr.update(), gr.update()\n\n        fill_x_button.click(fn=fill, inputs=[x_type, csv_mode], outputs=[x_values, x_values_dropdown])\n        fill_y_button.click(fn=fill, inputs=[y_type, csv_mode], outputs=[y_values, y_values_dropdown])\n        fill_z_button.click(fn=fill, inputs=[z_type, csv_mode], outputs=[z_values, z_values_dropdown])\n\n        def select_axis(axis_type, axis_values, axis_values_dropdown, csv_mode):\n            axis_type = axis_type or 0  # if axle type is None set to 0\n\n            choices = self.current_axis_options[axis_type].choices\n            has_choices = choices is not None\n\n            if has_choices:\n                choices = choices()\n                if csv_mode:\n                    if axis_values_dropdown:\n                        axis_values = list_to_csv_string(list(filter(lambda x: x in choices, axis_values_dropdown)))\n                        axis_values_dropdown = []\n                else:\n                    if axis_values:\n                        axis_values_dropdown = list(filter(lambda x: x in choices, csv_string_to_list_strip(axis_values)))\n                        axis_values = \"\"\n\n            return (gr.Button.update(visible=has_choices), gr.Textbox.update(visible=not has_choices or csv_mode, value=axis_values),\n                    gr.update(choices=choices if has_choices else None, visible=has_choices and not csv_mode, value=axis_values_dropdown))\n\n        x_type.change(fn=select_axis, inputs=[x_type, x_values, x_values_dropdown, csv_mode], outputs=[fill_x_button, x_values, x_values_dropdown])\n        y_type.change(fn=select_axis, inputs=[y_type, y_values, y_values_dropdown, csv_mode], outputs=[fill_y_button, y_values, y_values_dropdown])\n        z_type.change(fn=select_axis, inputs=[z_type, z_values, z_values_dropdown, csv_mode], outputs=[fill_z_button, z_values, z_values_dropdown])\n\n        def change_choice_mode(csv_mode, x_type, x_values, x_values_dropdown, y_type, y_values, y_values_dropdown, z_type, z_values, z_values_dropdown):\n            _fill_x_button, _x_values, _x_values_dropdown = select_axis(x_type, x_values, x_values_dropdown, csv_mode)\n            _fill_y_button, _y_values, _y_values_dropdown = select_axis(y_type, y_values, y_values_dropdown, csv_mode)\n            _fill_z_button, _z_values, _z_values_dropdown = select_axis(z_type, z_values, z_values_dropdown, csv_mode)\n            return _fill_x_button, _x_values, _x_values_dropdown, _fill_y_button, _y_values, _y_values_dropdown, _fill_z_button, _z_values, _z_values_dropdown\n\n        csv_mode.change(fn=change_choice_mode, inputs=[csv_mode, x_type, x_values, x_values_dropdown, y_type, y_values, y_values_dropdown, z_type, z_values, z_values_dropdown], outputs=[fill_x_button, x_values, x_values_dropdown, fill_y_button, y_values, y_values_dropdown, fill_z_button, z_values, z_values_dropdown])\n\n        def get_dropdown_update_from_params(axis, params):\n            val_key = f\"{axis} Values\"\n            vals = params.get(val_key, \"\")\n            valslist = csv_string_to_list_strip(vals)\n            return gr.update(value=valslist)\n\n        self.infotext_fields = (\n            (x_type, \"X Type\"),\n            (x_values, \"X Values\"),\n            (x_values_dropdown, lambda params: get_dropdown_update_from_params(\"X\", params)),\n            (y_type, \"Y Type\"),\n            (y_values, \"Y Values\"),\n            (y_values_dropdown, lambda params: get_dropdown_update_from_params(\"Y\", params)),\n            (z_type, \"Z Type\"),\n            (z_values, \"Z Values\"),\n            (z_values_dropdown, lambda params: get_dropdown_update_from_params(\"Z\", params)),\n        )\n\n        return [x_type, x_values, x_values_dropdown, y_type, y_values, y_values_dropdown, z_type, z_values, z_values_dropdown, draw_legend, include_lone_images, include_sub_grids, no_fixed_seeds, vary_seeds_x, vary_seeds_y, vary_seeds_z, margin_size, csv_mode]\n\n    def run(self, p, x_type, x_values, x_values_dropdown, y_type, y_values, y_values_dropdown, z_type, z_values, z_values_dropdown, draw_legend, include_lone_images, include_sub_grids, no_fixed_seeds, vary_seeds_x, vary_seeds_y, vary_seeds_z, margin_size, csv_mode):\n        x_type, y_type, z_type = x_type or 0, y_type or 0, z_type or 0  # if axle type is None set to 0\n\n        if not no_fixed_seeds:\n            modules.processing.fix_seed(p)\n\n        if not opts.return_grid:\n            p.batch_size = 1\n\n        def process_axis(opt, vals, vals_dropdown):\n            if opt.label == 'Nothing':\n                return [0]\n\n            if opt.choices is not None and not csv_mode:\n                valslist = vals_dropdown\n            elif opt.prepare is not None:\n                valslist = opt.prepare(vals)\n            else:\n                valslist = csv_string_to_list_strip(vals)\n\n            if opt.type == int:\n                valslist_ext = []\n\n                for val in valslist:\n                    if val.strip() == '':\n                        continue\n                    m = re_range.fullmatch(val)\n                    mc = re_range_count.fullmatch(val)\n                    if m is not None:\n                        start = int(m.group(1))\n                        end = int(m.group(2))+1\n                        step = int(m.group(3)) if m.group(3) is not None else 1\n\n                        valslist_ext += list(range(start, end, step))\n                    elif mc is not None:\n                        start = int(mc.group(1))\n                        end = int(mc.group(2))\n                        num = int(mc.group(3)) if mc.group(3) is not None else 1\n\n                        valslist_ext += [int(x) for x in np.linspace(start=start, stop=end, num=num).tolist()]\n                    else:\n                        valslist_ext.append(val)\n\n                valslist = valslist_ext\n            elif opt.type == float:\n                valslist_ext = []\n\n                for val in valslist:\n                    if val.strip() == '':\n                        continue\n                    m = re_range_float.fullmatch(val)\n                    mc = re_range_count_float.fullmatch(val)\n                    if m is not None:\n                        start = float(m.group(1))\n                        end = float(m.group(2))\n                        step = float(m.group(3)) if m.group(3) is not None else 1\n\n                        valslist_ext += np.arange(start, end + step, step).tolist()\n                    elif mc is not None:\n                        start = float(mc.group(1))\n                        end = float(mc.group(2))\n                        num = int(mc.group(3)) if mc.group(3) is not None else 1\n\n                        valslist_ext += np.linspace(start=start, stop=end, num=num).tolist()\n                    else:\n                        valslist_ext.append(val)\n\n                valslist = valslist_ext\n            elif opt.type == str_permutations:\n                valslist = list(permutations(valslist))\n\n            valslist = [opt.type(x) for x in valslist]\n\n            # Confirm options are valid before starting\n            if opt.confirm:\n                opt.confirm(p, valslist)\n\n            return valslist\n\n        x_opt = self.current_axis_options[x_type]\n        if x_opt.choices is not None and not csv_mode:\n            x_values = list_to_csv_string(x_values_dropdown)\n        xs = process_axis(x_opt, x_values, x_values_dropdown)\n\n        y_opt = self.current_axis_options[y_type]\n        if y_opt.choices is not None and not csv_mode:\n            y_values = list_to_csv_string(y_values_dropdown)\n        ys = process_axis(y_opt, y_values, y_values_dropdown)\n\n        z_opt = self.current_axis_options[z_type]\n        if z_opt.choices is not None and not csv_mode:\n            z_values = list_to_csv_string(z_values_dropdown)\n        zs = process_axis(z_opt, z_values, z_values_dropdown)\n\n        # this could be moved to common code, but unlikely to be ever triggered anywhere else\n        Image.MAX_IMAGE_PIXELS = None  # disable check in Pillow and rely on check below to allow large custom image sizes\n        grid_mp = round(len(xs) * len(ys) * len(zs) * p.width * p.height / 1000000)\n        assert grid_mp < opts.img_max_size_mp, f'Error: Resulting grid would be too large ({grid_mp} MPixels) (max configured size is {opts.img_max_size_mp} MPixels)'\n\n        def fix_axis_seeds(axis_opt, axis_list):\n            if axis_opt.label in ['Seed', 'Var. seed']:\n                return [int(random.randrange(4294967294)) if val is None or val == '' or val == -1 else val for val in axis_list]\n            else:\n                return axis_list\n\n        if not no_fixed_seeds:\n            xs = fix_axis_seeds(x_opt, xs)\n            ys = fix_axis_seeds(y_opt, ys)\n            zs = fix_axis_seeds(z_opt, zs)\n\n        if x_opt.label == 'Steps':\n            total_steps = sum(xs) * len(ys) * len(zs)\n        elif y_opt.label == 'Steps':\n            total_steps = sum(ys) * len(xs) * len(zs)\n        elif z_opt.label == 'Steps':\n            total_steps = sum(zs) * len(xs) * len(ys)\n        else:\n            total_steps = p.steps * len(xs) * len(ys) * len(zs)\n\n        if isinstance(p, StableDiffusionProcessingTxt2Img) and p.enable_hr:\n            if x_opt.label == \"Hires steps\":\n                total_steps += sum(xs) * len(ys) * len(zs)\n            elif y_opt.label == \"Hires steps\":\n                total_steps += sum(ys) * len(xs) * len(zs)\n            elif z_opt.label == \"Hires steps\":\n                total_steps += sum(zs) * len(xs) * len(ys)\n            elif p.hr_second_pass_steps:\n                total_steps += p.hr_second_pass_steps * len(xs) * len(ys) * len(zs)\n            else:\n                total_steps *= 2\n\n        total_steps *= p.n_iter\n\n        image_cell_count = p.n_iter * p.batch_size\n        cell_console_text = f\"; {image_cell_count} images per cell\" if image_cell_count > 1 else \"\"\n        plural_s = 's' if len(zs) > 1 else ''\n        print(f\"X/Y/Z plot will create {len(xs) * len(ys) * len(zs) * image_cell_count} images on {len(zs)} {len(xs)}x{len(ys)} grid{plural_s}{cell_console_text}. (Total steps to process: {total_steps})\")\n        shared.total_tqdm.updateTotal(total_steps)\n\n        state.xyz_plot_x = AxisInfo(x_opt, xs)\n        state.xyz_plot_y = AxisInfo(y_opt, ys)\n        state.xyz_plot_z = AxisInfo(z_opt, zs)\n\n        # If one of the axes is very slow to change between (like SD model\n        # checkpoint), then make sure it is in the outer iteration of the nested\n        # `for` loop.\n        first_axes_processed = 'z'\n        second_axes_processed = 'y'\n        if x_opt.cost > y_opt.cost and x_opt.cost > z_opt.cost:\n            first_axes_processed = 'x'\n            if y_opt.cost > z_opt.cost:\n                second_axes_processed = 'y'\n            else:\n                second_axes_processed = 'z'\n        elif y_opt.cost > x_opt.cost and y_opt.cost > z_opt.cost:\n            first_axes_processed = 'y'\n            if x_opt.cost > z_opt.cost:\n                second_axes_processed = 'x'\n            else:\n                second_axes_processed = 'z'\n        elif z_opt.cost > x_opt.cost and z_opt.cost > y_opt.cost:\n            first_axes_processed = 'z'\n            if x_opt.cost > y_opt.cost:\n                second_axes_processed = 'x'\n            else:\n                second_axes_processed = 'y'\n\n        grid_infotext = [None] * (1 + len(zs))\n\n        def cell(x, y, z, ix, iy, iz):\n            if shared.state.interrupted or state.stopping_generation:\n                return Processed(p, [], p.seed, \"\")\n\n            pc = copy(p)\n            pc.styles = pc.styles[:]\n            x_opt.apply(pc, x, xs)\n            y_opt.apply(pc, y, ys)\n            z_opt.apply(pc, z, zs)\n\n            xdim = len(xs) if vary_seeds_x else 1\n            ydim = len(ys) if vary_seeds_y else 1\n\n            if vary_seeds_x:\n               pc.seed += ix\n            if vary_seeds_y:\n               pc.seed += iy * xdim\n            if vary_seeds_z:\n               pc.seed += iz * xdim * ydim\n\n            try:\n                res = process_images(pc)\n            except Exception as e:\n                errors.display(e, \"generating image for xyz plot\")\n\n                res = Processed(p, [], p.seed, \"\")\n\n            # Sets subgrid infotexts\n            subgrid_index = 1 + iz\n            if grid_infotext[subgrid_index] is None and ix == 0 and iy == 0:\n                pc.extra_generation_params = copy(pc.extra_generation_params)\n                pc.extra_generation_params['Script'] = self.title()\n\n                if x_opt.label != 'Nothing':\n                    pc.extra_generation_params[\"X Type\"] = x_opt.label\n                    pc.extra_generation_params[\"X Values\"] = x_values\n                    if x_opt.label in [\"Seed\", \"Var. seed\"] and not no_fixed_seeds:\n                        pc.extra_generation_params[\"Fixed X Values\"] = \", \".join([str(x) for x in xs])\n\n                if y_opt.label != 'Nothing':\n                    pc.extra_generation_params[\"Y Type\"] = y_opt.label\n                    pc.extra_generation_params[\"Y Values\"] = y_values\n                    if y_opt.label in [\"Seed\", \"Var. seed\"] and not no_fixed_seeds:\n                        pc.extra_generation_params[\"Fixed Y Values\"] = \", \".join([str(y) for y in ys])\n\n                grid_infotext[subgrid_index] = processing.create_infotext(pc, pc.all_prompts, pc.all_seeds, pc.all_subseeds)\n\n            # Sets main grid infotext\n            if grid_infotext[0] is None and ix == 0 and iy == 0 and iz == 0:\n                pc.extra_generation_params = copy(pc.extra_generation_params)\n\n                if z_opt.label != 'Nothing':\n                    pc.extra_generation_params[\"Z Type\"] = z_opt.label\n                    pc.extra_generation_params[\"Z Values\"] = z_values\n                    if z_opt.label in [\"Seed\", \"Var. seed\"] and not no_fixed_seeds:\n                        pc.extra_generation_params[\"Fixed Z Values\"] = \", \".join([str(z) for z in zs])\n\n                grid_infotext[0] = processing.create_infotext(pc, pc.all_prompts, pc.all_seeds, pc.all_subseeds)\n\n            return res\n\n        with SharedSettingsStackHelper():\n            processed = draw_xyz_grid(\n                p,\n                xs=xs,\n                ys=ys,\n                zs=zs,\n                x_labels=[x_opt.format_value(p, x_opt, x) for x in xs],\n                y_labels=[y_opt.format_value(p, y_opt, y) for y in ys],\n                z_labels=[z_opt.format_value(p, z_opt, z) for z in zs],\n                cell=cell,\n                draw_legend=draw_legend,\n                include_lone_images=include_lone_images,\n                include_sub_grids=include_sub_grids,\n                first_axes_processed=first_axes_processed,\n                second_axes_processed=second_axes_processed,\n                margin_size=margin_size\n            )\n\n        if not processed.images:\n            # It broke, no further handling needed.\n            return processed\n\n        z_count = len(zs)\n\n        # Set the grid infotexts to the real ones with extra_generation_params (1 main grid + z_count sub-grids)\n        processed.infotexts[:1+z_count] = grid_infotext[:1+z_count]\n\n        if not include_lone_images:\n            # Don't need sub-images anymore, drop from list:\n            processed.images = processed.images[:z_count+1]\n\n        if opts.grid_save:\n            # Auto-save main and sub-grids:\n            grid_count = z_count + 1 if z_count > 1 else 1\n            for g in range(grid_count):\n                # TODO: See previous comment about intentional data misalignment.\n                adj_g = g-1 if g > 0 else g\n                images.save_image(processed.images[g], p.outpath_grids, \"xyz_grid\", info=processed.infotexts[g], extension=opts.grid_format, prompt=processed.all_prompts[adj_g], seed=processed.all_seeds[adj_g], grid=True, p=processed)\n                if not include_sub_grids:  # if not include_sub_grids then skip saving after the first grid\n                    break\n\n        if not include_sub_grids:\n            # Done with sub-grids, drop all related information:\n            for _ in range(z_count):\n                del processed.images[1]\n                del processed.all_prompts[1]\n                del processed.all_seeds[1]\n                del processed.infotexts[1]\n\n        return processed\n", "modules/sysinfo.py": "import json\nimport os\nimport sys\n\nimport platform\nimport hashlib\nimport pkg_resources\nimport psutil\nimport re\n\nimport launch\nfrom modules import paths_internal, timer, shared, extensions, errors\n\nchecksum_token = \"DontStealMyGamePlz__WINNERS_DONT_USE_DRUGS__DONT_COPY_THAT_FLOPPY\"\nenvironment_whitelist = {\n    \"GIT\",\n    \"INDEX_URL\",\n    \"WEBUI_LAUNCH_LIVE_OUTPUT\",\n    \"GRADIO_ANALYTICS_ENABLED\",\n    \"PYTHONPATH\",\n    \"TORCH_INDEX_URL\",\n    \"TORCH_COMMAND\",\n    \"REQS_FILE\",\n    \"XFORMERS_PACKAGE\",\n    \"CLIP_PACKAGE\",\n    \"OPENCLIP_PACKAGE\",\n    \"ASSETS_REPO\",\n    \"STABLE_DIFFUSION_REPO\",\n    \"K_DIFFUSION_REPO\",\n    \"BLIP_REPO\",\n    \"ASSETS_COMMIT_HASH\",\n    \"STABLE_DIFFUSION_COMMIT_HASH\",\n    \"K_DIFFUSION_COMMIT_HASH\",\n    \"BLIP_COMMIT_HASH\",\n    \"COMMANDLINE_ARGS\",\n    \"IGNORE_CMD_ARGS_ERRORS\",\n}\n\n\ndef pretty_bytes(num, suffix=\"B\"):\n    for unit in [\"\", \"K\", \"M\", \"G\", \"T\", \"P\", \"E\", \"Z\", \"Y\"]:\n        if abs(num) < 1024 or unit == 'Y':\n            return f\"{num:.0f}{unit}{suffix}\"\n        num /= 1024\n\n\ndef get():\n    res = get_dict()\n\n    text = json.dumps(res, ensure_ascii=False, indent=4)\n\n    h = hashlib.sha256(text.encode(\"utf8\"))\n    text = text.replace(checksum_token, h.hexdigest())\n\n    return text\n\n\nre_checksum = re.compile(r'\"Checksum\": \"([0-9a-fA-F]{64})\"')\n\n\ndef check(x):\n    m = re.search(re_checksum, x)\n    if not m:\n        return False\n\n    replaced = re.sub(re_checksum, f'\"Checksum\": \"{checksum_token}\"', x)\n\n    h = hashlib.sha256(replaced.encode(\"utf8\"))\n    return h.hexdigest() == m.group(1)\n\n\ndef get_dict():\n    ram = psutil.virtual_memory()\n\n    res = {\n        \"Platform\": platform.platform(),\n        \"Python\": platform.python_version(),\n        \"Version\": launch.git_tag(),\n        \"Commit\": launch.commit_hash(),\n        \"Script path\": paths_internal.script_path,\n        \"Data path\": paths_internal.data_path,\n        \"Extensions dir\": paths_internal.extensions_dir,\n        \"Checksum\": checksum_token,\n        \"Commandline\": get_argv(),\n        \"Torch env info\": get_torch_sysinfo(),\n        \"Exceptions\": errors.get_exceptions(),\n        \"CPU\": {\n            \"model\": platform.processor(),\n            \"count logical\": psutil.cpu_count(logical=True),\n            \"count physical\": psutil.cpu_count(logical=False),\n        },\n        \"RAM\": {\n            x: pretty_bytes(getattr(ram, x, 0)) for x in [\"total\", \"used\", \"free\", \"active\", \"inactive\", \"buffers\", \"cached\", \"shared\"] if getattr(ram, x, 0) != 0\n        },\n        \"Extensions\": get_extensions(enabled=True),\n        \"Inactive extensions\": get_extensions(enabled=False),\n        \"Environment\": get_environment(),\n        \"Config\": get_config(),\n        \"Startup\": timer.startup_record,\n        \"Packages\": sorted([f\"{pkg.key}=={pkg.version}\" for pkg in pkg_resources.working_set]),\n    }\n\n    return res\n\n\ndef get_environment():\n    return {k: os.environ[k] for k in sorted(os.environ) if k in environment_whitelist}\n\n\ndef get_argv():\n    res = []\n\n    for v in sys.argv:\n        if shared.cmd_opts.gradio_auth and shared.cmd_opts.gradio_auth == v:\n            res.append(\"<hidden>\")\n            continue\n\n        if shared.cmd_opts.api_auth and shared.cmd_opts.api_auth == v:\n            res.append(\"<hidden>\")\n            continue\n\n        res.append(v)\n\n    return res\n\nre_newline = re.compile(r\"\\r*\\n\")\n\n\ndef get_torch_sysinfo():\n    try:\n        import torch.utils.collect_env\n        info = torch.utils.collect_env.get_env_info()._asdict()\n\n        return {k: re.split(re_newline, str(v)) if \"\\n\" in str(v) else v for k, v in info.items()}\n    except Exception as e:\n        return str(e)\n\n\ndef get_extensions(*, enabled):\n\n    try:\n        def to_json(x: extensions.Extension):\n            return {\n                \"name\": x.name,\n                \"path\": x.path,\n                \"version\": x.version,\n                \"branch\": x.branch,\n                \"remote\": x.remote,\n            }\n\n        return [to_json(x) for x in extensions.extensions if not x.is_builtin and x.enabled == enabled]\n    except Exception as e:\n        return str(e)\n\n\ndef get_config():\n    try:\n        return shared.opts.data\n    except Exception as e:\n        return str(e)\n", "modules/sd_samplers_lcm.py": "import torch\n\nfrom k_diffusion import utils, sampling\nfrom k_diffusion.external import DiscreteEpsDDPMDenoiser\nfrom k_diffusion.sampling import default_noise_sampler, trange\n\nfrom modules import shared, sd_samplers_cfg_denoiser, sd_samplers_kdiffusion, sd_samplers_common\n\n\nclass LCMCompVisDenoiser(DiscreteEpsDDPMDenoiser):\n    def __init__(self, model):\n        timesteps = 1000\n        original_timesteps = 50     # LCM Original Timesteps (default=50, for current version of LCM)\n        self.skip_steps = timesteps // original_timesteps\n\n        alphas_cumprod_valid = torch.zeros((original_timesteps), dtype=torch.float32, device=model.device)\n        for x in range(original_timesteps):\n            alphas_cumprod_valid[original_timesteps - 1 - x] = model.alphas_cumprod[timesteps - 1 - x * self.skip_steps]\n\n        super().__init__(model, alphas_cumprod_valid, quantize=None)\n\n\n    def get_sigmas(self, n=None,):\n        if n is None:\n            return sampling.append_zero(self.sigmas.flip(0))\n\n        start = self.sigma_to_t(self.sigma_max)\n        end = self.sigma_to_t(self.sigma_min)\n\n        t = torch.linspace(start, end, n, device=shared.sd_model.device)\n\n        return sampling.append_zero(self.t_to_sigma(t))\n\n\n    def sigma_to_t(self, sigma, quantize=None):\n        log_sigma = sigma.log()\n        dists = log_sigma - self.log_sigmas[:, None]\n        return dists.abs().argmin(dim=0).view(sigma.shape) * self.skip_steps + (self.skip_steps - 1)\n\n\n    def t_to_sigma(self, timestep):\n        t = torch.clamp(((timestep - (self.skip_steps - 1)) / self.skip_steps).float(), min=0, max=(len(self.sigmas) - 1))\n        return super().t_to_sigma(t)\n\n\n    def get_eps(self, *args, **kwargs):\n        return self.inner_model.apply_model(*args, **kwargs)\n\n\n    def get_scaled_out(self, sigma, output, input):\n        sigma_data = 0.5\n        scaled_timestep = utils.append_dims(self.sigma_to_t(sigma), output.ndim) * 10.0\n\n        c_skip = sigma_data**2 / (scaled_timestep**2 + sigma_data**2)\n        c_out = scaled_timestep / (scaled_timestep**2 + sigma_data**2) ** 0.5\n\n        return c_out * output + c_skip * input\n\n\n    def forward(self, input, sigma, **kwargs):\n        c_out, c_in = [utils.append_dims(x, input.ndim) for x in self.get_scalings(sigma)]\n        eps = self.get_eps(input * c_in, self.sigma_to_t(sigma), **kwargs)\n        return self.get_scaled_out(sigma, input + eps * c_out, input)\n\n\ndef sample_lcm(model, x, sigmas, extra_args=None, callback=None, disable=None, noise_sampler=None):\n    extra_args = {} if extra_args is None else extra_args\n    noise_sampler = default_noise_sampler(x) if noise_sampler is None else noise_sampler\n    s_in = x.new_ones([x.shape[0]])\n\n    for i in trange(len(sigmas) - 1, disable=disable):\n        denoised = model(x, sigmas[i] * s_in, **extra_args)\n\n        if callback is not None:\n            callback({'x': x, 'i': i, 'sigma': sigmas[i], 'sigma_hat': sigmas[i], 'denoised': denoised})\n\n        x = denoised\n        if sigmas[i + 1] > 0:\n            x += sigmas[i + 1] * noise_sampler(sigmas[i], sigmas[i + 1])\n    return x\n\n\nclass CFGDenoiserLCM(sd_samplers_cfg_denoiser.CFGDenoiser):\n    @property\n    def inner_model(self):\n        if self.model_wrap is None:\n            denoiser = LCMCompVisDenoiser\n            self.model_wrap = denoiser(shared.sd_model)\n\n        return self.model_wrap\n\n\nclass LCMSampler(sd_samplers_kdiffusion.KDiffusionSampler):\n    def __init__(self, funcname, sd_model, options=None):\n        super().__init__(funcname, sd_model, options)\n        self.model_wrap_cfg = CFGDenoiserLCM(self)\n        self.model_wrap = self.model_wrap_cfg.inner_model\n\n\nsamplers_lcm = [('LCM', sample_lcm, ['k_lcm'], {})]\nsamplers_data_lcm = [\n    sd_samplers_common.SamplerData(label, lambda model, funcname=funcname: LCMSampler(funcname, model), aliases, options)\n    for label, funcname, aliases, options in samplers_lcm\n]\n", "modules/shared_options.py": "import os\nimport gradio as gr\n\nfrom modules import localization, ui_components, shared_items, shared, interrogate, shared_gradio_themes, util, sd_emphasis\nfrom modules.paths_internal import models_path, script_path, data_path, sd_configs_path, sd_default_config, sd_model_file, default_sd_model_file, extensions_dir, extensions_builtin_dir, default_output_dir  # noqa: F401\nfrom modules.shared_cmd_options import cmd_opts\nfrom modules.options import options_section, OptionInfo, OptionHTML, categories\n\noptions_templates = {}\nhide_dirs = shared.hide_dirs\n\nrestricted_opts = {\n    \"samples_filename_pattern\",\n    \"directories_filename_pattern\",\n    \"outdir_samples\",\n    \"outdir_txt2img_samples\",\n    \"outdir_img2img_samples\",\n    \"outdir_extras_samples\",\n    \"outdir_grids\",\n    \"outdir_txt2img_grids\",\n    \"outdir_save\",\n    \"outdir_init_images\",\n    \"temp_dir\",\n    \"clean_temp_dir_at_start\",\n}\n\ncategories.register_category(\"saving\", \"Saving images\")\ncategories.register_category(\"sd\", \"Stable Diffusion\")\ncategories.register_category(\"ui\", \"User Interface\")\ncategories.register_category(\"system\", \"System\")\ncategories.register_category(\"postprocessing\", \"Postprocessing\")\ncategories.register_category(\"training\", \"Training\")\n\noptions_templates.update(options_section(('saving-images', \"Saving images/grids\", \"saving\"), {\n    \"samples_save\": OptionInfo(True, \"Always save all generated images\"),\n    \"samples_format\": OptionInfo('png', 'File format for images'),\n    \"samples_filename_pattern\": OptionInfo(\"\", \"Images filename pattern\", component_args=hide_dirs).link(\"wiki\", \"https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Custom-Images-Filename-Name-and-Subdirectory\"),\n    \"save_images_add_number\": OptionInfo(True, \"Add number to filename when saving\", component_args=hide_dirs),\n    \"save_images_replace_action\": OptionInfo(\"Replace\", \"Saving the image to an existing file\", gr.Radio, {\"choices\": [\"Replace\", \"Add number suffix\"], **hide_dirs}),\n    \"grid_save\": OptionInfo(True, \"Always save all generated image grids\"),\n    \"grid_format\": OptionInfo('png', 'File format for grids'),\n    \"grid_extended_filename\": OptionInfo(False, \"Add extended info (seed, prompt) to filename when saving grid\"),\n    \"grid_only_if_multiple\": OptionInfo(True, \"Do not save grids consisting of one picture\"),\n    \"grid_prevent_empty_spots\": OptionInfo(False, \"Prevent empty spots in grid (when set to autodetect)\"),\n    \"grid_zip_filename_pattern\": OptionInfo(\"\", \"Archive filename pattern\", component_args=hide_dirs).link(\"wiki\", \"https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Custom-Images-Filename-Name-and-Subdirectory\"),\n    \"n_rows\": OptionInfo(-1, \"Grid row count; use -1 for autodetect and 0 for it to be same as batch size\", gr.Slider, {\"minimum\": -1, \"maximum\": 16, \"step\": 1}),\n    \"font\": OptionInfo(\"\", \"Font for image grids that have text\"),\n    \"grid_text_active_color\": OptionInfo(\"#000000\", \"Text color for image grids\", ui_components.FormColorPicker, {}),\n    \"grid_text_inactive_color\": OptionInfo(\"#999999\", \"Inactive text color for image grids\", ui_components.FormColorPicker, {}),\n    \"grid_background_color\": OptionInfo(\"#ffffff\", \"Background color for image grids\", ui_components.FormColorPicker, {}),\n\n    \"save_images_before_face_restoration\": OptionInfo(False, \"Save a copy of image before doing face restoration.\"),\n    \"save_images_before_highres_fix\": OptionInfo(False, \"Save a copy of image before applying highres fix.\"),\n    \"save_images_before_color_correction\": OptionInfo(False, \"Save a copy of image before applying color correction to img2img results\"),\n    \"save_mask\": OptionInfo(False, \"For inpainting, save a copy of the greyscale mask\"),\n    \"save_mask_composite\": OptionInfo(False, \"For inpainting, save a masked composite\"),\n    \"jpeg_quality\": OptionInfo(80, \"Quality for saved jpeg images\", gr.Slider, {\"minimum\": 1, \"maximum\": 100, \"step\": 1}),\n    \"webp_lossless\": OptionInfo(False, \"Use lossless compression for webp images\"),\n    \"export_for_4chan\": OptionInfo(True, \"Save copy of large images as JPG\").info(\"if the file size is above the limit, or either width or height are above the limit\"),\n    \"img_downscale_threshold\": OptionInfo(4.0, \"File size limit for the above option, MB\", gr.Number),\n    \"target_side_length\": OptionInfo(4000, \"Width/height limit for the above option, in pixels\", gr.Number),\n    \"img_max_size_mp\": OptionInfo(200, \"Maximum image size\", gr.Number).info(\"in megapixels\"),\n\n    \"use_original_name_batch\": OptionInfo(True, \"Use original name for output filename during batch process in extras tab\"),\n    \"use_upscaler_name_as_suffix\": OptionInfo(False, \"Use upscaler name as filename suffix in the extras tab\"),\n    \"save_selected_only\": OptionInfo(True, \"When using 'Save' button, only save a single selected image\"),\n    \"save_init_img\": OptionInfo(False, \"Save init images when using img2img\"),\n\n    \"temp_dir\":  OptionInfo(\"\", \"Directory for temporary images; leave empty for default\"),\n    \"clean_temp_dir_at_start\": OptionInfo(False, \"Cleanup non-default temporary directory when starting webui\"),\n\n    \"save_incomplete_images\": OptionInfo(False, \"Save incomplete images\").info(\"save images that has been interrupted in mid-generation; even if not saved, they will still show up in webui output.\"),\n\n    \"notification_audio\": OptionInfo(True, \"Play notification sound after image generation\").info(\"notification.mp3 should be present in the root directory\").needs_reload_ui(),\n    \"notification_volume\": OptionInfo(100, \"Notification sound volume\", gr.Slider, {\"minimum\": 0, \"maximum\": 100, \"step\": 1}).info(\"in %\"),\n}))\n\noptions_templates.update(options_section(('saving-paths', \"Paths for saving\", \"saving\"), {\n    \"outdir_samples\": OptionInfo(\"\", \"Output directory for images; if empty, defaults to three directories below\", component_args=hide_dirs),\n    \"outdir_txt2img_samples\": OptionInfo(util.truncate_path(os.path.join(default_output_dir, 'txt2img-images')), 'Output directory for txt2img images', component_args=hide_dirs),\n    \"outdir_img2img_samples\": OptionInfo(util.truncate_path(os.path.join(default_output_dir, 'img2img-images')), 'Output directory for img2img images', component_args=hide_dirs),\n    \"outdir_extras_samples\": OptionInfo(util.truncate_path(os.path.join(default_output_dir, 'extras-images')), 'Output directory for images from extras tab', component_args=hide_dirs),\n    \"outdir_grids\": OptionInfo(\"\", \"Output directory for grids; if empty, defaults to two directories below\", component_args=hide_dirs),\n    \"outdir_txt2img_grids\": OptionInfo(util.truncate_path(os.path.join(default_output_dir, 'txt2img-grids')), 'Output directory for txt2img grids', component_args=hide_dirs),\n    \"outdir_img2img_grids\": OptionInfo(util.truncate_path(os.path.join(default_output_dir, 'img2img-grids')), 'Output directory for img2img grids', component_args=hide_dirs),\n    \"outdir_save\": OptionInfo(util.truncate_path(os.path.join(data_path, 'log', 'images')), \"Directory for saving images using the Save button\", component_args=hide_dirs),\n    \"outdir_init_images\": OptionInfo(util.truncate_path(os.path.join(default_output_dir, 'init-images')), \"Directory for saving init images when using img2img\", component_args=hide_dirs),\n}))\n\noptions_templates.update(options_section(('saving-to-dirs', \"Saving to a directory\", \"saving\"), {\n    \"save_to_dirs\": OptionInfo(True, \"Save images to a subdirectory\"),\n    \"grid_save_to_dirs\": OptionInfo(True, \"Save grids to a subdirectory\"),\n    \"use_save_to_dirs_for_ui\": OptionInfo(False, \"When using \\\"Save\\\" button, save images to a subdirectory\"),\n    \"directories_filename_pattern\": OptionInfo(\"[date]\", \"Directory name pattern\", component_args=hide_dirs).link(\"wiki\", \"https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Custom-Images-Filename-Name-and-Subdirectory\"),\n    \"directories_max_prompt_words\": OptionInfo(8, \"Max prompt words for [prompt_words] pattern\", gr.Slider, {\"minimum\": 1, \"maximum\": 20, \"step\": 1, **hide_dirs}),\n}))\n\noptions_templates.update(options_section(('upscaling', \"Upscaling\", \"postprocessing\"), {\n    \"ESRGAN_tile\": OptionInfo(192, \"Tile size for ESRGAN upscalers.\", gr.Slider, {\"minimum\": 0, \"maximum\": 512, \"step\": 16}).info(\"0 = no tiling\"),\n    \"ESRGAN_tile_overlap\": OptionInfo(8, \"Tile overlap for ESRGAN upscalers.\", gr.Slider, {\"minimum\": 0, \"maximum\": 48, \"step\": 1}).info(\"Low values = visible seam\"),\n    \"realesrgan_enabled_models\": OptionInfo([\"R-ESRGAN 4x+\", \"R-ESRGAN 4x+ Anime6B\"], \"Select which Real-ESRGAN models to show in the web UI.\", gr.CheckboxGroup, lambda: {\"choices\": shared_items.realesrgan_models_names()}),\n    \"dat_enabled_models\": OptionInfo([\"DAT x2\", \"DAT x3\", \"DAT x4\"], \"Select which DAT models to show in the web UI.\", gr.CheckboxGroup, lambda: {\"choices\": shared_items.dat_models_names()}),\n    \"DAT_tile\": OptionInfo(192, \"Tile size for DAT upscalers.\", gr.Slider, {\"minimum\": 0, \"maximum\": 512, \"step\": 16}).info(\"0 = no tiling\"),\n    \"DAT_tile_overlap\": OptionInfo(8, \"Tile overlap for DAT upscalers.\", gr.Slider, {\"minimum\": 0, \"maximum\": 48, \"step\": 1}).info(\"Low values = visible seam\"),\n    \"upscaler_for_img2img\": OptionInfo(None, \"Upscaler for img2img\", gr.Dropdown, lambda: {\"choices\": [x.name for x in shared.sd_upscalers]}),\n    \"set_scale_by_when_changing_upscaler\": OptionInfo(False, \"Automatically set the Scale by factor based on the name of the selected Upscaler.\"),\n}))\n\noptions_templates.update(options_section(('face-restoration', \"Face restoration\", \"postprocessing\"), {\n    \"face_restoration\": OptionInfo(False, \"Restore faces\", infotext='Face restoration').info(\"will use a third-party model on generation result to reconstruct faces\"),\n    \"face_restoration_model\": OptionInfo(\"CodeFormer\", \"Face restoration model\", gr.Radio, lambda: {\"choices\": [x.name() for x in shared.face_restorers]}),\n    \"code_former_weight\": OptionInfo(0.5, \"CodeFormer weight\", gr.Slider, {\"minimum\": 0, \"maximum\": 1, \"step\": 0.01}).info(\"0 = maximum effect; 1 = minimum effect\"),\n    \"face_restoration_unload\": OptionInfo(False, \"Move face restoration model from VRAM into RAM after processing\"),\n}))\n\noptions_templates.update(options_section(('system', \"System\", \"system\"), {\n    \"auto_launch_browser\": OptionInfo(\"Local\", \"Automatically open webui in browser on startup\", gr.Radio, lambda: {\"choices\": [\"Disable\", \"Local\", \"Remote\"]}),\n    \"enable_console_prompts\": OptionInfo(shared.cmd_opts.enable_console_prompts, \"Print prompts to console when generating with txt2img and img2img.\"),\n    \"show_warnings\": OptionInfo(False, \"Show warnings in console.\").needs_reload_ui(),\n    \"show_gradio_deprecation_warnings\": OptionInfo(True, \"Show gradio deprecation warnings in console.\").needs_reload_ui(),\n    \"memmon_poll_rate\": OptionInfo(8, \"VRAM usage polls per second during generation.\", gr.Slider, {\"minimum\": 0, \"maximum\": 40, \"step\": 1}).info(\"0 = disable\"),\n    \"samples_log_stdout\": OptionInfo(False, \"Always print all generation info to standard output\"),\n    \"multiple_tqdm\": OptionInfo(True, \"Add a second progress bar to the console that shows progress for an entire job.\"),\n    \"enable_upscale_progressbar\": OptionInfo(True, \"Show a progress bar in the console for tiled upscaling.\"),\n    \"print_hypernet_extra\": OptionInfo(False, \"Print extra hypernetwork information to console.\"),\n    \"list_hidden_files\": OptionInfo(True, \"Load models/files in hidden directories\").info(\"directory is hidden if its name starts with \\\".\\\"\"),\n    \"disable_mmap_load_safetensors\": OptionInfo(False, \"Disable memmapping for loading .safetensors files.\").info(\"fixes very slow loading speed in some cases\"),\n    \"hide_ldm_prints\": OptionInfo(True, \"Prevent Stability-AI's ldm/sgm modules from printing noise to console.\"),\n    \"dump_stacks_on_signal\": OptionInfo(False, \"Print stack traces before exiting the program with ctrl+c.\"),\n}))\n\noptions_templates.update(options_section(('API', \"API\", \"system\"), {\n    \"api_enable_requests\": OptionInfo(True, \"Allow http:// and https:// URLs for input images in API\", restrict_api=True),\n    \"api_forbid_local_requests\": OptionInfo(True, \"Forbid URLs to local resources\", restrict_api=True),\n    \"api_useragent\": OptionInfo(\"\", \"User agent for requests\", restrict_api=True),\n}))\n\noptions_templates.update(options_section(('training', \"Training\", \"training\"), {\n    \"unload_models_when_training\": OptionInfo(False, \"Move VAE and CLIP to RAM when training if possible. Saves VRAM.\"),\n    \"pin_memory\": OptionInfo(False, \"Turn on pin_memory for DataLoader. Makes training slightly faster but can increase memory usage.\"),\n    \"save_optimizer_state\": OptionInfo(False, \"Saves Optimizer state as separate *.optim file. Training of embedding or HN can be resumed with the matching optim file.\"),\n    \"save_training_settings_to_txt\": OptionInfo(True, \"Save textual inversion and hypernet settings to a text file whenever training starts.\"),\n    \"dataset_filename_word_regex\": OptionInfo(\"\", \"Filename word regex\"),\n    \"dataset_filename_join_string\": OptionInfo(\" \", \"Filename join string\"),\n    \"training_image_repeats_per_epoch\": OptionInfo(1, \"Number of repeats for a single input image per epoch; used only for displaying epoch number\", gr.Number, {\"precision\": 0}),\n    \"training_write_csv_every\": OptionInfo(500, \"Save an csv containing the loss to log directory every N steps, 0 to disable\"),\n    \"training_xattention_optimizations\": OptionInfo(False, \"Use cross attention optimizations while training\"),\n    \"training_enable_tensorboard\": OptionInfo(False, \"Enable tensorboard logging.\"),\n    \"training_tensorboard_save_images\": OptionInfo(False, \"Save generated images within tensorboard.\"),\n    \"training_tensorboard_flush_every\": OptionInfo(120, \"How often, in seconds, to flush the pending tensorboard events and summaries to disk.\"),\n}))\n\noptions_templates.update(options_section(('sd', \"Stable Diffusion\", \"sd\"), {\n    \"sd_model_checkpoint\": OptionInfo(None, \"Stable Diffusion checkpoint\", gr.Dropdown, lambda: {\"choices\": shared_items.list_checkpoint_tiles(shared.opts.sd_checkpoint_dropdown_use_short)}, refresh=shared_items.refresh_checkpoints, infotext='Model hash'),\n    \"sd_checkpoints_limit\": OptionInfo(1, \"Maximum number of checkpoints loaded at the same time\", gr.Slider, {\"minimum\": 1, \"maximum\": 10, \"step\": 1}),\n    \"sd_checkpoints_keep_in_cpu\": OptionInfo(True, \"Only keep one model on device\").info(\"will keep models other than the currently used one in RAM rather than VRAM\"),\n    \"sd_checkpoint_cache\": OptionInfo(0, \"Checkpoints to cache in RAM\", gr.Slider, {\"minimum\": 0, \"maximum\": 10, \"step\": 1}).info(\"obsolete; set to 0 and use the two settings above instead\"),\n    \"sd_unet\": OptionInfo(\"Automatic\", \"SD Unet\", gr.Dropdown, lambda: {\"choices\": shared_items.sd_unet_items()}, refresh=shared_items.refresh_unet_list).info(\"choose Unet model: Automatic = use one with same filename as checkpoint; None = use Unet from checkpoint\"),\n    \"enable_quantization\": OptionInfo(False, \"Enable quantization in K samplers for sharper and cleaner results. This may change existing seeds\").needs_reload_ui(),\n    \"emphasis\": OptionInfo(\"Original\", \"Emphasis mode\", gr.Radio, lambda: {\"choices\": [x.name for x in sd_emphasis.options]}, infotext=\"Emphasis\").info(\"makes it possible to make model to pay (more:1.1) or (less:0.9) attention to text when you use the syntax in prompt; \" + sd_emphasis.get_options_descriptions()),\n    \"enable_batch_seeds\": OptionInfo(True, \"Make K-diffusion samplers produce same images in a batch as when making a single image\"),\n    \"comma_padding_backtrack\": OptionInfo(20, \"Prompt word wrap length limit\", gr.Slider, {\"minimum\": 0, \"maximum\": 74, \"step\": 1}).info(\"in tokens - for texts shorter than specified, if they don't fit into 75 token limit, move them to the next 75 token chunk\"),\n    \"CLIP_stop_at_last_layers\": OptionInfo(1, \"Clip skip\", gr.Slider, {\"minimum\": 1, \"maximum\": 12, \"step\": 1}, infotext=\"Clip skip\").link(\"wiki\", \"https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Features#clip-skip\").info(\"ignore last layers of CLIP network; 1 ignores none, 2 ignores one layer\"),\n    \"upcast_attn\": OptionInfo(False, \"Upcast cross attention layer to float32\"),\n    \"randn_source\": OptionInfo(\"GPU\", \"Random number generator source.\", gr.Radio, {\"choices\": [\"GPU\", \"CPU\", \"NV\"]}, infotext=\"RNG\").info(\"changes seeds drastically; use CPU to produce the same picture across different videocard vendors; use NV to produce same picture as on NVidia videocards\"),\n    \"tiling\": OptionInfo(False, \"Tiling\", infotext='Tiling').info(\"produce a tileable picture\"),\n    \"hires_fix_refiner_pass\": OptionInfo(\"second pass\", \"Hires fix: which pass to enable refiner for\", gr.Radio, {\"choices\": [\"first pass\", \"second pass\", \"both passes\"]}, infotext=\"Hires refiner\"),\n}))\n\noptions_templates.update(options_section(('sdxl', \"Stable Diffusion XL\", \"sd\"), {\n    \"sdxl_crop_top\": OptionInfo(0, \"crop top coordinate\"),\n    \"sdxl_crop_left\": OptionInfo(0, \"crop left coordinate\"),\n    \"sdxl_refiner_low_aesthetic_score\": OptionInfo(2.5, \"SDXL low aesthetic score\", gr.Number).info(\"used for refiner model negative prompt\"),\n    \"sdxl_refiner_high_aesthetic_score\": OptionInfo(6.0, \"SDXL high aesthetic score\", gr.Number).info(\"used for refiner model prompt\"),\n}))\n\noptions_templates.update(options_section(('vae', \"VAE\", \"sd\"), {\n    \"sd_vae_explanation\": OptionHTML(\"\"\"\n<abbr title='Variational autoencoder'>VAE</abbr> is a neural network that transforms a standard <abbr title='red/green/blue'>RGB</abbr>\nimage into latent space representation and back. Latent space representation is what stable diffusion is working on during sampling\n(i.e. when the progress bar is between empty and full). For txt2img, VAE is used to create a resulting image after the sampling is finished.\nFor img2img, VAE is used to process user's input image before the sampling, and to create an image after sampling.\n\"\"\"),\n    \"sd_vae_checkpoint_cache\": OptionInfo(0, \"VAE Checkpoints to cache in RAM\", gr.Slider, {\"minimum\": 0, \"maximum\": 10, \"step\": 1}),\n    \"sd_vae\": OptionInfo(\"Automatic\", \"SD VAE\", gr.Dropdown, lambda: {\"choices\": shared_items.sd_vae_items()}, refresh=shared_items.refresh_vae_list, infotext='VAE').info(\"choose VAE model: Automatic = use one with same filename as checkpoint; None = use VAE from checkpoint\"),\n    \"sd_vae_overrides_per_model_preferences\": OptionInfo(True, \"Selected VAE overrides per-model preferences\").info(\"you can set per-model VAE either by editing user metadata for checkpoints, or by making the VAE have same name as checkpoint\"),\n    \"auto_vae_precision_bfloat16\": OptionInfo(False, \"Automatically convert VAE to bfloat16\").info(\"triggers when a tensor with NaNs is produced in VAE; disabling the option in this case will result in a black square image; if enabled, overrides the option below\"),\n    \"auto_vae_precision\": OptionInfo(True, \"Automatically revert VAE to 32-bit floats\").info(\"triggers when a tensor with NaNs is produced in VAE; disabling the option in this case will result in a black square image\"),\n    \"sd_vae_encode_method\": OptionInfo(\"Full\", \"VAE type for encode\", gr.Radio, {\"choices\": [\"Full\", \"TAESD\"]}, infotext='VAE Encoder').info(\"method to encode image to latent (use in img2img, hires-fix or inpaint mask)\"),\n    \"sd_vae_decode_method\": OptionInfo(\"Full\", \"VAE type for decode\", gr.Radio, {\"choices\": [\"Full\", \"TAESD\"]}, infotext='VAE Decoder').info(\"method to decode latent to image\"),\n}))\n\noptions_templates.update(options_section(('img2img', \"img2img\", \"sd\"), {\n    \"inpainting_mask_weight\": OptionInfo(1.0, \"Inpainting conditioning mask strength\", gr.Slider, {\"minimum\": 0.0, \"maximum\": 1.0, \"step\": 0.01}, infotext='Conditional mask weight'),\n    \"initial_noise_multiplier\": OptionInfo(1.0, \"Noise multiplier for img2img\", gr.Slider, {\"minimum\": 0.0, \"maximum\": 1.5, \"step\": 0.001}, infotext='Noise multiplier'),\n    \"img2img_extra_noise\": OptionInfo(0.0, \"Extra noise multiplier for img2img and hires fix\", gr.Slider, {\"minimum\": 0.0, \"maximum\": 1.0, \"step\": 0.01}, infotext='Extra noise').info(\"0 = disabled (default); should be lower than denoising strength\"),\n    \"img2img_color_correction\": OptionInfo(False, \"Apply color correction to img2img results to match original colors.\"),\n    \"img2img_fix_steps\": OptionInfo(False, \"With img2img, do exactly the amount of steps the slider specifies.\").info(\"normally you'd do less with less denoising\"),\n    \"img2img_background_color\": OptionInfo(\"#ffffff\", \"With img2img, fill transparent parts of the input image with this color.\", ui_components.FormColorPicker, {}),\n    \"img2img_editor_height\": OptionInfo(720, \"Height of the image editor\", gr.Slider, {\"minimum\": 80, \"maximum\": 1600, \"step\": 1}).info(\"in pixels\").needs_reload_ui(),\n    \"img2img_sketch_default_brush_color\": OptionInfo(\"#ffffff\", \"Sketch initial brush color\", ui_components.FormColorPicker, {}).info(\"default brush color of img2img sketch\").needs_reload_ui(),\n    \"img2img_inpaint_mask_brush_color\": OptionInfo(\"#ffffff\", \"Inpaint mask brush color\", ui_components.FormColorPicker,  {}).info(\"brush color of inpaint mask\").needs_reload_ui(),\n    \"img2img_inpaint_sketch_default_brush_color\": OptionInfo(\"#ffffff\", \"Inpaint sketch initial brush color\", ui_components.FormColorPicker, {}).info(\"default brush color of img2img inpaint sketch\").needs_reload_ui(),\n    \"return_mask\": OptionInfo(False, \"For inpainting, include the greyscale mask in results for web\"),\n    \"return_mask_composite\": OptionInfo(False, \"For inpainting, include masked composite in results for web\"),\n    \"img2img_batch_show_results_limit\": OptionInfo(32, \"Show the first N batch img2img results in UI\", gr.Slider, {\"minimum\": -1, \"maximum\": 1000, \"step\": 1}).info('0: disable, -1: show all images. Too many images can cause lag'),\n    \"overlay_inpaint\": OptionInfo(True, \"Overlay original for inpaint\").info(\"when inpainting, overlay the original image over the areas that weren't inpainted.\"),\n}))\n\noptions_templates.update(options_section(('optimizations', \"Optimizations\", \"sd\"), {\n    \"cross_attention_optimization\": OptionInfo(\"Automatic\", \"Cross attention optimization\", gr.Dropdown, lambda: {\"choices\": shared_items.cross_attention_optimizations()}),\n    \"s_min_uncond\": OptionInfo(0.0, \"Negative Guidance minimum sigma\", gr.Slider, {\"minimum\": 0.0, \"maximum\": 15.0, \"step\": 0.01}).link(\"PR\", \"https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/9177\").info(\"skip negative prompt for some steps when the image is almost ready; 0=disable, higher=faster\"),\n    \"token_merging_ratio\": OptionInfo(0.0, \"Token merging ratio\", gr.Slider, {\"minimum\": 0.0, \"maximum\": 0.9, \"step\": 0.1}, infotext='Token merging ratio').link(\"PR\", \"https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/9256\").info(\"0=disable, higher=faster\"),\n    \"token_merging_ratio_img2img\": OptionInfo(0.0, \"Token merging ratio for img2img\", gr.Slider, {\"minimum\": 0.0, \"maximum\": 0.9, \"step\": 0.1}).info(\"only applies if non-zero and overrides above\"),\n    \"token_merging_ratio_hr\": OptionInfo(0.0, \"Token merging ratio for high-res pass\", gr.Slider, {\"minimum\": 0.0, \"maximum\": 0.9, \"step\": 0.1}, infotext='Token merging ratio hr').info(\"only applies if non-zero and overrides above\"),\n    \"pad_cond_uncond\": OptionInfo(False, \"Pad prompt/negative prompt\", infotext='Pad conds').info(\"improves performance when prompt and negative prompt have different lengths; changes seeds\"),\n    \"pad_cond_uncond_v0\": OptionInfo(False, \"Pad prompt/negative prompt (v0)\", infotext='Pad conds v0').info(\"alternative implementation for the above; used prior to 1.6.0 for DDIM sampler; overrides the above if set; WARNING: truncates negative prompt if it's too long; changes seeds\"),\n    \"persistent_cond_cache\": OptionInfo(True, \"Persistent cond cache\").info(\"do not recalculate conds from prompts if prompts have not changed since previous calculation\"),\n    \"batch_cond_uncond\": OptionInfo(True, \"Batch cond/uncond\").info(\"do both conditional and unconditional denoising in one batch; uses a bit more VRAM during sampling, but improves speed; previously this was controlled by --always-batch-cond-uncond commandline argument\"),\n    \"fp8_storage\": OptionInfo(\"Disable\", \"FP8 weight\", gr.Radio, {\"choices\": [\"Disable\", \"Enable for SDXL\", \"Enable\"]}).info(\"Use FP8 to store Linear/Conv layers' weight. Require pytorch>=2.1.0.\"),\n    \"cache_fp16_weight\": OptionInfo(False, \"Cache FP16 weight for LoRA\").info(\"Cache fp16 weight when enabling FP8, will increase the quality of LoRA. Use more system ram.\"),\n}))\n\noptions_templates.update(options_section(('compatibility', \"Compatibility\", \"sd\"), {\n    \"auto_backcompat\": OptionInfo(True, \"Automatic backward compatibility\").info(\"automatically enable options for backwards compatibility when importing generation parameters from infotext that has program version.\"),\n    \"use_old_emphasis_implementation\": OptionInfo(False, \"Use old emphasis implementation. Can be useful to reproduce old seeds.\"),\n    \"use_old_karras_scheduler_sigmas\": OptionInfo(False, \"Use old karras scheduler sigmas (0.1 to 10).\"),\n    \"no_dpmpp_sde_batch_determinism\": OptionInfo(False, \"Do not make DPM++ SDE deterministic across different batch sizes.\"),\n    \"use_old_hires_fix_width_height\": OptionInfo(False, \"For hires fix, use width/height sliders to set final resolution rather than first pass (disables Upscale by, Resize width/height to).\"),\n    \"dont_fix_second_order_samplers_schedule\": OptionInfo(False, \"Do not fix prompt schedule for second order samplers.\"),\n    \"hires_fix_use_firstpass_conds\": OptionInfo(False, \"For hires fix, calculate conds of second pass using extra networks of first pass.\"),\n    \"use_old_scheduling\": OptionInfo(False, \"Use old prompt editing timelines.\", infotext=\"Old prompt editing timelines\").info(\"For [red:green:N]; old: If N < 1, it's a fraction of steps (and hires fix uses range from 0 to 1), if N >= 1, it's an absolute number of steps; new: If N has a decimal point in it, it's a fraction of steps (and hires fix uses range from 1 to 2), othewrwise it's an absolute number of steps\"),\n    \"use_downcasted_alpha_bar\": OptionInfo(False, \"Downcast model alphas_cumprod to fp16 before sampling. For reproducing old seeds.\", infotext=\"Downcast alphas_cumprod\"),\n    \"refiner_switch_by_sample_steps\": OptionInfo(False, \"Switch to refiner by sampling steps instead of model timesteps. Old behavior for refiner.\", infotext=\"Refiner switch by sampling steps\")\n}))\n\noptions_templates.update(options_section(('interrogate', \"Interrogate\"), {\n    \"interrogate_keep_models_in_memory\": OptionInfo(False, \"Keep models in VRAM\"),\n    \"interrogate_return_ranks\": OptionInfo(False, \"Include ranks of model tags matches in results.\").info(\"booru only\"),\n    \"interrogate_clip_num_beams\": OptionInfo(1, \"BLIP: num_beams\", gr.Slider, {\"minimum\": 1, \"maximum\": 16, \"step\": 1}),\n    \"interrogate_clip_min_length\": OptionInfo(24, \"BLIP: minimum description length\", gr.Slider, {\"minimum\": 1, \"maximum\": 128, \"step\": 1}),\n    \"interrogate_clip_max_length\": OptionInfo(48, \"BLIP: maximum description length\", gr.Slider, {\"minimum\": 1, \"maximum\": 256, \"step\": 1}),\n    \"interrogate_clip_dict_limit\": OptionInfo(1500, \"CLIP: maximum number of lines in text file\").info(\"0 = No limit\"),\n    \"interrogate_clip_skip_categories\": OptionInfo([], \"CLIP: skip inquire categories\", gr.CheckboxGroup, lambda: {\"choices\": interrogate.category_types()}, refresh=interrogate.category_types),\n    \"interrogate_deepbooru_score_threshold\": OptionInfo(0.5, \"deepbooru: score threshold\", gr.Slider, {\"minimum\": 0, \"maximum\": 1, \"step\": 0.01}),\n    \"deepbooru_sort_alpha\": OptionInfo(True, \"deepbooru: sort tags alphabetically\").info(\"if not: sort by score\"),\n    \"deepbooru_use_spaces\": OptionInfo(True, \"deepbooru: use spaces in tags\").info(\"if not: use underscores\"),\n    \"deepbooru_escape\": OptionInfo(True, \"deepbooru: escape (\\\\) brackets\").info(\"so they are used as literal brackets and not for emphasis\"),\n    \"deepbooru_filter_tags\": OptionInfo(\"\", \"deepbooru: filter out those tags\").info(\"separate by comma\"),\n}))\n\noptions_templates.update(options_section(('extra_networks', \"Extra Networks\", \"sd\"), {\n    \"extra_networks_show_hidden_directories\": OptionInfo(True, \"Show hidden directories\").info(\"directory is hidden if its name starts with \\\".\\\".\"),\n    \"extra_networks_dir_button_function\": OptionInfo(False, \"Add a '/' to the beginning of directory buttons\").info(\"Buttons will display the contents of the selected directory without acting as a search filter.\"),\n    \"extra_networks_hidden_models\": OptionInfo(\"When searched\", \"Show cards for models in hidden directories\", gr.Radio, {\"choices\": [\"Always\", \"When searched\", \"Never\"]}).info('\"When searched\" option will only show the item when the search string has 4 characters or more'),\n    \"extra_networks_default_multiplier\": OptionInfo(1.0, \"Default multiplier for extra networks\", gr.Slider, {\"minimum\": 0.0, \"maximum\": 2.0, \"step\": 0.01}),\n    \"extra_networks_card_width\": OptionInfo(0, \"Card width for Extra Networks\").info(\"in pixels\"),\n    \"extra_networks_card_height\": OptionInfo(0, \"Card height for Extra Networks\").info(\"in pixels\"),\n    \"extra_networks_card_text_scale\": OptionInfo(1.0, \"Card text scale\", gr.Slider, {\"minimum\": 0.0, \"maximum\": 2.0, \"step\": 0.01}).info(\"1 = original size\"),\n    \"extra_networks_card_show_desc\": OptionInfo(True, \"Show description on card\"),\n    \"extra_networks_card_description_is_html\": OptionInfo(False, \"Treat card description as HTML\"),\n    \"extra_networks_card_order_field\": OptionInfo(\"Path\", \"Default order field for Extra Networks cards\", gr.Dropdown, {\"choices\": ['Path', 'Name', 'Date Created', 'Date Modified']}).needs_reload_ui(),\n    \"extra_networks_card_order\": OptionInfo(\"Ascending\", \"Default order for Extra Networks cards\", gr.Dropdown, {\"choices\": ['Ascending', 'Descending']}).needs_reload_ui(),\n    \"extra_networks_tree_view_style\": OptionInfo(\"Dirs\", \"Extra Networks directory view style\", gr.Radio, {\"choices\": [\"Tree\", \"Dirs\"]}).needs_reload_ui(),\n    \"extra_networks_tree_view_default_enabled\": OptionInfo(True, \"Show the Extra Networks directory view by default\").needs_reload_ui(),\n    \"extra_networks_tree_view_default_width\": OptionInfo(180, \"Default width for the Extra Networks directory tree view\", gr.Number).needs_reload_ui(),\n    \"extra_networks_add_text_separator\": OptionInfo(\" \", \"Extra networks separator\").info(\"extra text to add before <...> when adding extra network to prompt\"),\n    \"ui_extra_networks_tab_reorder\": OptionInfo(\"\", \"Extra networks tab order\").needs_reload_ui(),\n    \"textual_inversion_print_at_load\": OptionInfo(False, \"Print a list of Textual Inversion embeddings when loading model\"),\n    \"textual_inversion_add_hashes_to_infotext\": OptionInfo(True, \"Add Textual Inversion hashes to infotext\"),\n    \"sd_hypernetwork\": OptionInfo(\"None\", \"Add hypernetwork to prompt\", gr.Dropdown, lambda: {\"choices\": [\"None\", *shared.hypernetworks]}, refresh=shared_items.reload_hypernetworks),\n}))\n\noptions_templates.update(options_section(('ui_prompt_editing', \"Prompt editing\", \"ui\"), {\n    \"keyedit_precision_attention\": OptionInfo(0.1, \"Precision for (attention:1.1) when editing the prompt with Ctrl+up/down\", gr.Slider, {\"minimum\": 0.01, \"maximum\": 0.2, \"step\": 0.001}),\n    \"keyedit_precision_extra\": OptionInfo(0.05, \"Precision for <extra networks:0.9> when editing the prompt with Ctrl+up/down\", gr.Slider, {\"minimum\": 0.01, \"maximum\": 0.2, \"step\": 0.001}),\n    \"keyedit_delimiters\": OptionInfo(r\".,\\/!?%^*;:{}=`~() \", \"Word delimiters when editing the prompt with Ctrl+up/down\"),\n    \"keyedit_delimiters_whitespace\": OptionInfo([\"Tab\", \"Carriage Return\", \"Line Feed\"], \"Ctrl+up/down whitespace delimiters\", gr.CheckboxGroup, lambda: {\"choices\": [\"Tab\", \"Carriage Return\", \"Line Feed\"]}),\n    \"keyedit_move\": OptionInfo(True, \"Alt+left/right moves prompt elements\"),\n    \"disable_token_counters\": OptionInfo(False, \"Disable prompt token counters\"),\n    \"include_styles_into_token_counters\": OptionInfo(True, \"Count tokens of enabled styles\").info(\"When calculating how many tokens the prompt has, also consider tokens added by enabled styles.\"),\n}))\n\noptions_templates.update(options_section(('ui_gallery', \"Gallery\", \"ui\"), {\n    \"return_grid\": OptionInfo(True, \"Show grid in gallery\"),\n    \"do_not_show_images\": OptionInfo(False, \"Do not show any images in gallery\"),\n    \"js_modal_lightbox\": OptionInfo(True, \"Full page image viewer: enable\"),\n    \"js_modal_lightbox_initially_zoomed\": OptionInfo(True, \"Full page image viewer: show images zoomed in by default\"),\n    \"js_modal_lightbox_gamepad\": OptionInfo(False, \"Full page image viewer: navigate with gamepad\"),\n    \"js_modal_lightbox_gamepad_repeat\": OptionInfo(250, \"Full page image viewer: gamepad repeat period\").info(\"in milliseconds\"),\n    \"sd_webui_modal_lightbox_icon_opacity\": OptionInfo(1, \"Full page image viewer: control icon unfocused opacity\", gr.Slider, {\"minimum\": 0.0, \"maximum\": 1, \"step\": 0.01}, onchange=shared.reload_gradio_theme).info('for mouse only').needs_reload_ui(),\n    \"sd_webui_modal_lightbox_toolbar_opacity\": OptionInfo(0.9, \"Full page image viewer: tool bar opacity\", gr.Slider, {\"minimum\": 0.0, \"maximum\": 1, \"step\": 0.01}, onchange=shared.reload_gradio_theme).info('for mouse only').needs_reload_ui(),\n    \"gallery_height\": OptionInfo(\"\", \"Gallery height\", gr.Textbox).info(\"can be any valid CSS value, for example 768px or 20em\").needs_reload_ui(),\n    \"open_dir_button_choice\": OptionInfo(\"Subdirectory\", \"What directory the [\ud83d\udcc2] button opens\", gr.Radio, {\"choices\": [\"Output Root\", \"Subdirectory\", \"Subdirectory (even temp dir)\"]}),\n}))\n\noptions_templates.update(options_section(('ui_alternatives', \"UI alternatives\", \"ui\"), {\n    \"compact_prompt_box\": OptionInfo(False, \"Compact prompt layout\").info(\"puts prompt and negative prompt inside the Generate tab, leaving more vertical space for the image on the right\").needs_reload_ui(),\n    \"samplers_in_dropdown\": OptionInfo(True, \"Use dropdown for sampler selection instead of radio group\").needs_reload_ui(),\n    \"dimensions_and_batch_together\": OptionInfo(True, \"Show Width/Height and Batch sliders in same row\").needs_reload_ui(),\n    \"sd_checkpoint_dropdown_use_short\": OptionInfo(False, \"Checkpoint dropdown: use filenames without paths\").info(\"models in subdirectories like photo/sd15.ckpt will be listed as just sd15.ckpt\"),\n    \"hires_fix_show_sampler\": OptionInfo(False, \"Hires fix: show hires checkpoint and sampler selection\").needs_reload_ui(),\n    \"hires_fix_show_prompts\": OptionInfo(False, \"Hires fix: show hires prompt and negative prompt\").needs_reload_ui(),\n    \"txt2img_settings_accordion\": OptionInfo(False, \"Settings in txt2img hidden under Accordion\").needs_reload_ui(),\n    \"img2img_settings_accordion\": OptionInfo(False, \"Settings in img2img hidden under Accordion\").needs_reload_ui(),\n    \"interrupt_after_current\": OptionInfo(True, \"Don't Interrupt in the middle\").info(\"when using Interrupt button, if generating more than one image, stop after the generation of an image has finished, instead of immediately\"),\n}))\n\noptions_templates.update(options_section(('ui', \"User interface\", \"ui\"), {\n    \"localization\": OptionInfo(\"None\", \"Localization\", gr.Dropdown, lambda: {\"choices\": [\"None\"] + list(localization.localizations.keys())}, refresh=lambda: localization.list_localizations(cmd_opts.localizations_dir)).needs_reload_ui(),\n    \"quicksettings_list\": OptionInfo([\"sd_model_checkpoint\"], \"Quicksettings list\", ui_components.DropdownMulti, lambda: {\"choices\": list(shared.opts.data_labels.keys())}).js(\"info\", \"settingsHintsShowQuicksettings\").info(\"setting entries that appear at the top of page rather than in settings tab\").needs_reload_ui(),\n    \"ui_tab_order\": OptionInfo([], \"UI tab order\", ui_components.DropdownMulti, lambda: {\"choices\": list(shared.tab_names)}).needs_reload_ui(),\n    \"hidden_tabs\": OptionInfo([], \"Hidden UI tabs\", ui_components.DropdownMulti, lambda: {\"choices\": list(shared.tab_names)}).needs_reload_ui(),\n    \"ui_reorder_list\": OptionInfo([], \"UI item order for txt2img/img2img tabs\", ui_components.DropdownMulti, lambda: {\"choices\": list(shared_items.ui_reorder_categories())}).info(\"selected items appear first\").needs_reload_ui(),\n    \"gradio_theme\": OptionInfo(\"Default\", \"Gradio theme\", ui_components.DropdownEditable, lambda: {\"choices\": [\"Default\"] + shared_gradio_themes.gradio_hf_hub_themes}).info(\"you can also manually enter any of themes from the <a href='https://huggingface.co/spaces/gradio/theme-gallery'>gallery</a>.\").needs_reload_ui(),\n    \"gradio_themes_cache\": OptionInfo(True, \"Cache gradio themes locally\").info(\"disable to update the selected Gradio theme\"),\n    \"show_progress_in_title\": OptionInfo(True, \"Show generation progress in window title.\"),\n    \"send_seed\": OptionInfo(True, \"Send seed when sending prompt or image to other interface\"),\n    \"send_size\": OptionInfo(True, \"Send size when sending prompt or image to another interface\"),\n    \"enable_reloading_ui_scripts\": OptionInfo(False, \"Reload UI scripts when using Reload UI option\").info(\"useful for developing: if you make changes to UI scripts code, it is applied when the UI is reloded.\"),\n\n}))\n\n\noptions_templates.update(options_section(('infotext', \"Infotext\", \"ui\"), {\n    \"infotext_explanation\": OptionHTML(\"\"\"\nInfotext is what this software calls the text that contains generation parameters and can be used to generate the same picture again.\nIt is displayed in UI below the image. To use infotext, paste it into the prompt and click the \u2199\ufe0f paste button.\n\"\"\"),\n    \"enable_pnginfo\": OptionInfo(True, \"Write infotext to metadata of the generated image\"),\n    \"save_txt\": OptionInfo(False, \"Create a text file with infotext next to every generated image\"),\n\n    \"add_model_name_to_info\": OptionInfo(True, \"Add model name to infotext\"),\n    \"add_model_hash_to_info\": OptionInfo(True, \"Add model hash to infotext\"),\n    \"add_vae_name_to_info\": OptionInfo(True, \"Add VAE name to infotext\"),\n    \"add_vae_hash_to_info\": OptionInfo(True, \"Add VAE hash to infotext\"),\n    \"add_user_name_to_info\": OptionInfo(False, \"Add user name to infotext when authenticated\"),\n    \"add_version_to_infotext\": OptionInfo(True, \"Add program version to infotext\"),\n    \"disable_weights_auto_swap\": OptionInfo(True, \"Disregard checkpoint information from pasted infotext\").info(\"when reading generation parameters from text into UI\"),\n    \"infotext_skip_pasting\": OptionInfo([], \"Disregard fields from pasted infotext\", ui_components.DropdownMulti, lambda: {\"choices\": shared_items.get_infotext_names()}),\n    \"infotext_styles\": OptionInfo(\"Apply if any\", \"Infer styles from prompts of pasted infotext\", gr.Radio, {\"choices\": [\"Ignore\", \"Apply\", \"Discard\", \"Apply if any\"]}).info(\"when reading generation parameters from text into UI)\").html(\"\"\"<ul style='margin-left: 1.5em'>\n<li>Ignore: keep prompt and styles dropdown as it is.</li>\n<li>Apply: remove style text from prompt, always replace styles dropdown value with found styles (even if none are found).</li>\n<li>Discard: remove style text from prompt, keep styles dropdown as it is.</li>\n<li>Apply if any: remove style text from prompt; if any styles are found in prompt, put them into styles dropdown, otherwise keep it as it is.</li>\n</ul>\"\"\"),\n\n}))\n\noptions_templates.update(options_section(('ui', \"Live previews\", \"ui\"), {\n    \"show_progressbar\": OptionInfo(True, \"Show progressbar\"),\n    \"live_previews_enable\": OptionInfo(True, \"Show live previews of the created image\"),\n    \"live_previews_image_format\": OptionInfo(\"png\", \"Live preview file format\", gr.Radio, {\"choices\": [\"jpeg\", \"png\", \"webp\"]}),\n    \"show_progress_grid\": OptionInfo(True, \"Show previews of all images generated in a batch as a grid\"),\n    \"show_progress_every_n_steps\": OptionInfo(10, \"Live preview display period\", gr.Slider, {\"minimum\": -1, \"maximum\": 32, \"step\": 1}).info(\"in sampling steps - show new live preview image every N sampling steps; -1 = only show after completion of batch\"),\n    \"show_progress_type\": OptionInfo(\"Approx NN\", \"Live preview method\", gr.Radio, {\"choices\": [\"Full\", \"Approx NN\", \"Approx cheap\", \"TAESD\"]}).info(\"Full = slow but pretty; Approx NN and TAESD = fast but low quality; Approx cheap = super fast but terrible otherwise\"),\n    \"live_preview_allow_lowvram_full\": OptionInfo(False, \"Allow Full live preview method with lowvram/medvram\").info(\"If not, Approx NN will be used instead; Full live preview method is very detrimental to speed if lowvram/medvram optimizations are enabled\"),\n    \"live_preview_content\": OptionInfo(\"Prompt\", \"Live preview subject\", gr.Radio, {\"choices\": [\"Combined\", \"Prompt\", \"Negative prompt\"]}),\n    \"live_preview_refresh_period\": OptionInfo(1000, \"Progressbar and preview update period\").info(\"in milliseconds\"),\n    \"live_preview_fast_interrupt\": OptionInfo(False, \"Return image with chosen live preview method on interrupt\").info(\"makes interrupts faster\"),\n    \"js_live_preview_in_modal_lightbox\": OptionInfo(False, \"Show Live preview in full page image viewer\"),\n}))\n\noptions_templates.update(options_section(('sampler-params', \"Sampler parameters\", \"sd\"), {\n    \"hide_samplers\": OptionInfo([], \"Hide samplers in user interface\", gr.CheckboxGroup, lambda: {\"choices\": [x.name for x in shared_items.list_samplers()]}).needs_reload_ui(),\n    \"eta_ddim\": OptionInfo(0.0, \"Eta for DDIM\", gr.Slider, {\"minimum\": 0.0, \"maximum\": 1.0, \"step\": 0.01}, infotext='Eta DDIM').info(\"noise multiplier; higher = more unpredictable results\"),\n    \"eta_ancestral\": OptionInfo(1.0, \"Eta for k-diffusion samplers\", gr.Slider, {\"minimum\": 0.0, \"maximum\": 1.0, \"step\": 0.01}, infotext='Eta').info(\"noise multiplier; currently only applies to ancestral samplers (i.e. Euler a) and SDE samplers\"),\n    \"ddim_discretize\": OptionInfo('uniform', \"img2img DDIM discretize\", gr.Radio, {\"choices\": ['uniform', 'quad']}),\n    's_churn': OptionInfo(0.0, \"sigma churn\", gr.Slider, {\"minimum\": 0.0, \"maximum\": 100.0, \"step\": 0.01}, infotext='Sigma churn').info('amount of stochasticity; only applies to Euler, Heun, and DPM2'),\n    's_tmin':  OptionInfo(0.0, \"sigma tmin\",  gr.Slider, {\"minimum\": 0.0, \"maximum\": 10.0, \"step\": 0.01}, infotext='Sigma tmin').info('enable stochasticity; start value of the sigma range; only applies to Euler, Heun, and DPM2'),\n    's_tmax':  OptionInfo(0.0, \"sigma tmax\",  gr.Slider, {\"minimum\": 0.0, \"maximum\": 999.0, \"step\": 0.01}, infotext='Sigma tmax').info(\"0 = inf; end value of the sigma range; only applies to Euler, Heun, and DPM2\"),\n    's_noise': OptionInfo(1.0, \"sigma noise\", gr.Slider, {\"minimum\": 0.0, \"maximum\": 1.1, \"step\": 0.001}, infotext='Sigma noise').info('amount of additional noise to counteract loss of detail during sampling'),\n    'sigma_min': OptionInfo(0.0, \"sigma min\", gr.Number, infotext='Schedule min sigma').info(\"0 = default (~0.03); minimum noise strength for k-diffusion noise scheduler\"),\n    'sigma_max': OptionInfo(0.0, \"sigma max\", gr.Number, infotext='Schedule max sigma').info(\"0 = default (~14.6); maximum noise strength for k-diffusion noise scheduler\"),\n    'rho':  OptionInfo(0.0, \"rho\", gr.Number, infotext='Schedule rho').info(\"0 = default (7 for karras, 1 for polyexponential); higher values result in a steeper noise schedule (decreases faster)\"),\n    'eta_noise_seed_delta': OptionInfo(0, \"Eta noise seed delta\", gr.Number, {\"precision\": 0}, infotext='ENSD').info(\"ENSD; does not improve anything, just produces different results for ancestral samplers - only useful for reproducing images\"),\n    'always_discard_next_to_last_sigma': OptionInfo(False, \"Always discard next-to-last sigma\", infotext='Discard penultimate sigma').link(\"PR\", \"https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/6044\"),\n    'sgm_noise_multiplier': OptionInfo(False, \"SGM noise multiplier\", infotext='SGM noise multiplier').link(\"PR\", \"https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/12818\").info(\"Match initial noise to official SDXL implementation - only useful for reproducing images\"),\n    'uni_pc_variant': OptionInfo(\"bh1\", \"UniPC variant\", gr.Radio, {\"choices\": [\"bh1\", \"bh2\", \"vary_coeff\"]}, infotext='UniPC variant'),\n    'uni_pc_skip_type': OptionInfo(\"time_uniform\", \"UniPC skip type\", gr.Radio, {\"choices\": [\"time_uniform\", \"time_quadratic\", \"logSNR\"]}, infotext='UniPC skip type'),\n    'uni_pc_order': OptionInfo(3, \"UniPC order\", gr.Slider, {\"minimum\": 1, \"maximum\": 50, \"step\": 1}, infotext='UniPC order').info(\"must be < sampling steps\"),\n    'uni_pc_lower_order_final': OptionInfo(True, \"UniPC lower order final\", infotext='UniPC lower order final'),\n    'sd_noise_schedule': OptionInfo(\"Default\", \"Noise schedule for sampling\", gr.Radio, {\"choices\": [\"Default\", \"Zero Terminal SNR\"]}, infotext=\"Noise Schedule\").info(\"for use with zero terminal SNR trained models\")\n}))\n\noptions_templates.update(options_section(('postprocessing', \"Postprocessing\", \"postprocessing\"), {\n    'postprocessing_enable_in_main_ui': OptionInfo([], \"Enable postprocessing operations in txt2img and img2img tabs\", ui_components.DropdownMulti, lambda: {\"choices\": [x.name for x in shared_items.postprocessing_scripts()]}),\n    'postprocessing_disable_in_extras': OptionInfo([], \"Disable postprocessing operations in extras tab\", ui_components.DropdownMulti, lambda: {\"choices\": [x.name for x in shared_items.postprocessing_scripts()]}),\n    'postprocessing_operation_order': OptionInfo([], \"Postprocessing operation order\", ui_components.DropdownMulti, lambda: {\"choices\": [x.name for x in shared_items.postprocessing_scripts()]}),\n    'upscaling_max_images_in_cache': OptionInfo(5, \"Maximum number of images in upscaling cache\", gr.Slider, {\"minimum\": 0, \"maximum\": 10, \"step\": 1}),\n    'postprocessing_existing_caption_action': OptionInfo(\"Ignore\", \"Action for existing captions\", gr.Radio, {\"choices\": [\"Ignore\", \"Keep\", \"Prepend\", \"Append\"]}).info(\"when generating captions using postprocessing; Ignore = use generated; Keep = use original; Prepend/Append = combine both\"),\n}))\n\noptions_templates.update(options_section((None, \"Hidden options\"), {\n    \"disabled_extensions\": OptionInfo([], \"Disable these extensions\"),\n    \"disable_all_extensions\": OptionInfo(\"none\", \"Disable all extensions (preserves the list of disabled extensions)\", gr.Radio, {\"choices\": [\"none\", \"extra\", \"all\"]}),\n    \"restore_config_state_file\": OptionInfo(\"\", \"Config state file to restore from, under 'config-states/' folder\"),\n    \"sd_checkpoint_hash\": OptionInfo(\"\", \"SHA256 hash of the current checkpoint\"),\n}))\n", "modules/xlmr.py": "from transformers import BertPreTrainedModel, BertConfig\nimport torch.nn as nn\nimport torch\nfrom transformers.models.xlm_roberta.configuration_xlm_roberta import XLMRobertaConfig\nfrom transformers import XLMRobertaModel,XLMRobertaTokenizer\nfrom typing import Optional\n\nfrom modules import torch_utils\n\n\nclass BertSeriesConfig(BertConfig):\n    def __init__(self, vocab_size=30522, hidden_size=768, num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072, hidden_act=\"gelu\", hidden_dropout_prob=0.1, attention_probs_dropout_prob=0.1, max_position_embeddings=512, type_vocab_size=2, initializer_range=0.02, layer_norm_eps=1e-12, pad_token_id=0, position_embedding_type=\"absolute\", use_cache=True, classifier_dropout=None,project_dim=512, pooler_fn=\"average\",learn_encoder=False,model_type='bert',**kwargs):\n\n        super().__init__(vocab_size, hidden_size, num_hidden_layers, num_attention_heads, intermediate_size, hidden_act, hidden_dropout_prob, attention_probs_dropout_prob, max_position_embeddings, type_vocab_size, initializer_range, layer_norm_eps, pad_token_id, position_embedding_type, use_cache, classifier_dropout, **kwargs)\n        self.project_dim = project_dim\n        self.pooler_fn = pooler_fn\n        self.learn_encoder = learn_encoder\n\nclass RobertaSeriesConfig(XLMRobertaConfig):\n    def __init__(self, pad_token_id=1, bos_token_id=0, eos_token_id=2,project_dim=512,pooler_fn='cls',learn_encoder=False, **kwargs):\n        super().__init__(pad_token_id=pad_token_id, bos_token_id=bos_token_id, eos_token_id=eos_token_id, **kwargs)\n        self.project_dim = project_dim\n        self.pooler_fn = pooler_fn\n        self.learn_encoder = learn_encoder\n\n\nclass BertSeriesModelWithTransformation(BertPreTrainedModel):\n\n    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n    _keys_to_ignore_on_load_missing = [r\"position_ids\", r\"predictions.decoder.bias\"]\n    config_class = BertSeriesConfig\n\n    def __init__(self, config=None, **kargs):\n        # modify initialization for autoloading\n        if config is None:\n            config = XLMRobertaConfig()\n            config.attention_probs_dropout_prob= 0.1\n            config.bos_token_id=0\n            config.eos_token_id=2\n            config.hidden_act='gelu'\n            config.hidden_dropout_prob=0.1\n            config.hidden_size=1024\n            config.initializer_range=0.02\n            config.intermediate_size=4096\n            config.layer_norm_eps=1e-05\n            config.max_position_embeddings=514\n\n            config.num_attention_heads=16\n            config.num_hidden_layers=24\n            config.output_past=True\n            config.pad_token_id=1\n            config.position_embedding_type= \"absolute\"\n\n            config.type_vocab_size= 1\n            config.use_cache=True\n            config.vocab_size= 250002\n            config.project_dim = 768\n            config.learn_encoder = False\n        super().__init__(config)\n        self.roberta = XLMRobertaModel(config)\n        self.transformation = nn.Linear(config.hidden_size,config.project_dim)\n        self.pre_LN=nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n        self.tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-large')\n        self.pooler = lambda x: x[:,0]\n        self.post_init()\n\n    def encode(self,c):\n        device = torch_utils.get_param(self).device\n        text = self.tokenizer(c,\n                        truncation=True,\n                        max_length=77,\n                        return_length=False,\n                        return_overflowing_tokens=False,\n                        padding=\"max_length\",\n                        return_tensors=\"pt\")\n        text[\"input_ids\"] = torch.tensor(text[\"input_ids\"]).to(device)\n        text[\"attention_mask\"] = torch.tensor(\n            text['attention_mask']).to(device)\n        features = self(**text)\n        return features['projection_state']\n\n    def forward(\n        self,\n        input_ids: Optional[torch.Tensor] = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        token_type_ids: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.Tensor] = None,\n        head_mask: Optional[torch.Tensor] = None,\n        inputs_embeds: Optional[torch.Tensor] = None,\n        encoder_hidden_states: Optional[torch.Tensor] = None,\n        encoder_attention_mask: Optional[torch.Tensor] = None,\n        output_attentions: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n    ) :\n        r\"\"\"\n        \"\"\"\n\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n\n        outputs = self.roberta(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            encoder_hidden_states=encoder_hidden_states,\n            encoder_attention_mask=encoder_attention_mask,\n            output_attentions=output_attentions,\n            output_hidden_states=True,\n            return_dict=return_dict,\n        )\n\n        # last module outputs\n        sequence_output = outputs[0]\n\n\n        # project every module\n        sequence_output_ln = self.pre_LN(sequence_output)\n\n        # pooler\n        pooler_output = self.pooler(sequence_output_ln)\n        pooler_output = self.transformation(pooler_output)\n        projection_state = self.transformation(outputs.last_hidden_state)\n\n        return {\n            'pooler_output':pooler_output,\n            'last_hidden_state':outputs.last_hidden_state,\n            'hidden_states':outputs.hidden_states,\n            'attentions':outputs.attentions,\n            'projection_state':projection_state,\n            'sequence_out': sequence_output\n        }\n\n\nclass RobertaSeriesModelWithTransformation(BertSeriesModelWithTransformation):\n    base_model_prefix = 'roberta'\n    config_class= RobertaSeriesConfig\n", "modules/hat_model.py": "import os\nimport sys\n\nfrom modules import modelloader, devices\nfrom modules.shared import opts\nfrom modules.upscaler import Upscaler, UpscalerData\nfrom modules.upscaler_utils import upscale_with_model\n\n\nclass UpscalerHAT(Upscaler):\n    def __init__(self, dirname):\n        self.name = \"HAT\"\n        self.scalers = []\n        self.user_path = dirname\n        super().__init__()\n        for file in self.find_models(ext_filter=[\".pt\", \".pth\"]):\n            name = modelloader.friendly_name(file)\n            scale = 4  # TODO: scale might not be 4, but we can't know without loading the model\n            scaler_data = UpscalerData(name, file, upscaler=self, scale=scale)\n            self.scalers.append(scaler_data)\n\n    def do_upscale(self, img, selected_model):\n        try:\n            model = self.load_model(selected_model)\n        except Exception as e:\n            print(f\"Unable to load HAT model {selected_model}: {e}\", file=sys.stderr)\n            return img\n        model.to(devices.device_esrgan)  # TODO: should probably be device_hat\n        return upscale_with_model(\n            model,\n            img,\n            tile_size=opts.ESRGAN_tile,  # TODO: should probably be HAT_tile\n            tile_overlap=opts.ESRGAN_tile_overlap,  # TODO: should probably be HAT_tile_overlap\n        )\n\n    def load_model(self, path: str):\n        if not os.path.isfile(path):\n            raise FileNotFoundError(f\"Model file {path} not found\")\n        return modelloader.load_spandrel_model(\n            path,\n            device=devices.device_esrgan,  # TODO: should probably be device_hat\n            expected_architecture='HAT',\n        )\n", "modules/call_queue.py": "from functools import wraps\nimport html\nimport time\n\nfrom modules import shared, progress, errors, devices, fifo_lock\n\nqueue_lock = fifo_lock.FIFOLock()\n\n\ndef wrap_queued_call(func):\n    def f(*args, **kwargs):\n        with queue_lock:\n            res = func(*args, **kwargs)\n\n        return res\n\n    return f\n\n\ndef wrap_gradio_gpu_call(func, extra_outputs=None):\n    @wraps(func)\n    def f(*args, **kwargs):\n\n        # if the first argument is a string that says \"task(...)\", it is treated as a job id\n        if args and type(args[0]) == str and args[0].startswith(\"task(\") and args[0].endswith(\")\"):\n            id_task = args[0]\n            progress.add_task_to_queue(id_task)\n        else:\n            id_task = None\n\n        with queue_lock:\n            shared.state.begin(job=id_task)\n            progress.start_task(id_task)\n\n            try:\n                res = func(*args, **kwargs)\n                progress.record_results(id_task, res)\n            finally:\n                progress.finish_task(id_task)\n\n            shared.state.end()\n\n        return res\n\n    return wrap_gradio_call(f, extra_outputs=extra_outputs, add_stats=True)\n\n\ndef wrap_gradio_call(func, extra_outputs=None, add_stats=False):\n    @wraps(func)\n    def f(*args, extra_outputs_array=extra_outputs, **kwargs):\n        run_memmon = shared.opts.memmon_poll_rate > 0 and not shared.mem_mon.disabled and add_stats\n        if run_memmon:\n            shared.mem_mon.monitor()\n        t = time.perf_counter()\n\n        try:\n            res = list(func(*args, **kwargs))\n        except Exception as e:\n            # When printing out our debug argument list,\n            # do not print out more than a 100 KB of text\n            max_debug_str_len = 131072\n            message = \"Error completing request\"\n            arg_str = f\"Arguments: {args} {kwargs}\"[:max_debug_str_len]\n            if len(arg_str) > max_debug_str_len:\n                arg_str += f\" (Argument list truncated at {max_debug_str_len}/{len(arg_str)} characters)\"\n            errors.report(f\"{message}\\n{arg_str}\", exc_info=True)\n\n            shared.state.job = \"\"\n            shared.state.job_count = 0\n\n            if extra_outputs_array is None:\n                extra_outputs_array = [None, '']\n\n            error_message = f'{type(e).__name__}: {e}'\n            res = extra_outputs_array + [f\"<div class='error'>{html.escape(error_message)}</div>\"]\n\n        devices.torch_gc()\n\n        shared.state.skipped = False\n        shared.state.interrupted = False\n        shared.state.stopping_generation = False\n        shared.state.job_count = 0\n\n        if not add_stats:\n            return tuple(res)\n\n        elapsed = time.perf_counter() - t\n        elapsed_m = int(elapsed // 60)\n        elapsed_s = elapsed % 60\n        elapsed_text = f\"{elapsed_s:.1f} sec.\"\n        if elapsed_m > 0:\n            elapsed_text = f\"{elapsed_m} min. \"+elapsed_text\n\n        if run_memmon:\n            mem_stats = {k: -(v//-(1024*1024)) for k, v in shared.mem_mon.stop().items()}\n            active_peak = mem_stats['active_peak']\n            reserved_peak = mem_stats['reserved_peak']\n            sys_peak = mem_stats['system_peak']\n            sys_total = mem_stats['total']\n            sys_pct = sys_peak/max(sys_total, 1) * 100\n\n            toltip_a = \"Active: peak amount of video memory used during generation (excluding cached data)\"\n            toltip_r = \"Reserved: total amount of video memory allocated by the Torch library \"\n            toltip_sys = \"System: peak amount of video memory allocated by all running programs, out of total capacity\"\n\n            text_a = f\"<abbr title='{toltip_a}'>A</abbr>: <span class='measurement'>{active_peak/1024:.2f} GB</span>\"\n            text_r = f\"<abbr title='{toltip_r}'>R</abbr>: <span class='measurement'>{reserved_peak/1024:.2f} GB</span>\"\n            text_sys = f\"<abbr title='{toltip_sys}'>Sys</abbr>: <span class='measurement'>{sys_peak/1024:.1f}/{sys_total/1024:g} GB</span> ({sys_pct:.1f}%)\"\n\n            vram_html = f\"<p class='vram'>{text_a}, <wbr>{text_r}, <wbr>{text_sys}</p>\"\n        else:\n            vram_html = ''\n\n        # last item is always HTML\n        res[-1] += f\"<div class='performance'><p class='time'>Time taken: <wbr><span class='measurement'>{elapsed_text}</span></p>{vram_html}</div>\"\n\n        return tuple(res)\n\n    return f\n", "modules/sd_vae.py": "import os\nimport collections\nfrom dataclasses import dataclass\n\nfrom modules import paths, shared, devices, script_callbacks, sd_models, extra_networks, lowvram, sd_hijack, hashes\n\nimport glob\nfrom copy import deepcopy\n\n\nvae_path = os.path.abspath(os.path.join(paths.models_path, \"VAE\"))\nvae_ignore_keys = {\"model_ema.decay\", \"model_ema.num_updates\"}\nvae_dict = {}\n\n\nbase_vae = None\nloaded_vae_file = None\ncheckpoint_info = None\n\ncheckpoints_loaded = collections.OrderedDict()\n\n\ndef get_loaded_vae_name():\n    if loaded_vae_file is None:\n        return None\n\n    return os.path.basename(loaded_vae_file)\n\n\ndef get_loaded_vae_hash():\n    if loaded_vae_file is None:\n        return None\n\n    sha256 = hashes.sha256(loaded_vae_file, 'vae')\n\n    return sha256[0:10] if sha256 else None\n\n\ndef get_base_vae(model):\n    if base_vae is not None and checkpoint_info == model.sd_checkpoint_info and model:\n        return base_vae\n    return None\n\n\ndef store_base_vae(model):\n    global base_vae, checkpoint_info\n    if checkpoint_info != model.sd_checkpoint_info:\n        assert not loaded_vae_file, \"Trying to store non-base VAE!\"\n        base_vae = deepcopy(model.first_stage_model.state_dict())\n        checkpoint_info = model.sd_checkpoint_info\n\n\ndef delete_base_vae():\n    global base_vae, checkpoint_info\n    base_vae = None\n    checkpoint_info = None\n\n\ndef restore_base_vae(model):\n    global loaded_vae_file\n    if base_vae is not None and checkpoint_info == model.sd_checkpoint_info:\n        print(\"Restoring base VAE\")\n        _load_vae_dict(model, base_vae)\n        loaded_vae_file = None\n    delete_base_vae()\n\n\ndef get_filename(filepath):\n    return os.path.basename(filepath)\n\n\ndef refresh_vae_list():\n    vae_dict.clear()\n\n    paths = [\n        os.path.join(sd_models.model_path, '**/*.vae.ckpt'),\n        os.path.join(sd_models.model_path, '**/*.vae.pt'),\n        os.path.join(sd_models.model_path, '**/*.vae.safetensors'),\n        os.path.join(vae_path, '**/*.ckpt'),\n        os.path.join(vae_path, '**/*.pt'),\n        os.path.join(vae_path, '**/*.safetensors'),\n    ]\n\n    if shared.cmd_opts.ckpt_dir is not None and os.path.isdir(shared.cmd_opts.ckpt_dir):\n        paths += [\n            os.path.join(shared.cmd_opts.ckpt_dir, '**/*.vae.ckpt'),\n            os.path.join(shared.cmd_opts.ckpt_dir, '**/*.vae.pt'),\n            os.path.join(shared.cmd_opts.ckpt_dir, '**/*.vae.safetensors'),\n        ]\n\n    if shared.cmd_opts.vae_dir is not None and os.path.isdir(shared.cmd_opts.vae_dir):\n        paths += [\n            os.path.join(shared.cmd_opts.vae_dir, '**/*.ckpt'),\n            os.path.join(shared.cmd_opts.vae_dir, '**/*.pt'),\n            os.path.join(shared.cmd_opts.vae_dir, '**/*.safetensors'),\n        ]\n\n    candidates = []\n    for path in paths:\n        candidates += glob.iglob(path, recursive=True)\n\n    for filepath in candidates:\n        name = get_filename(filepath)\n        vae_dict[name] = filepath\n\n    vae_dict.update(dict(sorted(vae_dict.items(), key=lambda item: shared.natural_sort_key(item[0]))))\n\n\ndef find_vae_near_checkpoint(checkpoint_file):\n    checkpoint_path = os.path.basename(checkpoint_file).rsplit('.', 1)[0]\n    for vae_file in vae_dict.values():\n        if os.path.basename(vae_file).startswith(checkpoint_path):\n            return vae_file\n\n    return None\n\n\n@dataclass\nclass VaeResolution:\n    vae: str = None\n    source: str = None\n    resolved: bool = True\n\n    def tuple(self):\n        return self.vae, self.source\n\n\ndef is_automatic():\n    return shared.opts.sd_vae in {\"Automatic\", \"auto\"}  # \"auto\" for people with old config\n\n\ndef resolve_vae_from_setting() -> VaeResolution:\n    if shared.opts.sd_vae == \"None\":\n        return VaeResolution()\n\n    vae_from_options = vae_dict.get(shared.opts.sd_vae, None)\n    if vae_from_options is not None:\n        return VaeResolution(vae_from_options, 'specified in settings')\n\n    if not is_automatic():\n        print(f\"Couldn't find VAE named {shared.opts.sd_vae}; using None instead\")\n\n    return VaeResolution(resolved=False)\n\n\ndef resolve_vae_from_user_metadata(checkpoint_file) -> VaeResolution:\n    metadata = extra_networks.get_user_metadata(checkpoint_file)\n    vae_metadata = metadata.get(\"vae\", None)\n    if vae_metadata is not None and vae_metadata != \"Automatic\":\n        if vae_metadata == \"None\":\n            return VaeResolution()\n\n        vae_from_metadata = vae_dict.get(vae_metadata, None)\n        if vae_from_metadata is not None:\n            return VaeResolution(vae_from_metadata, \"from user metadata\")\n\n    return VaeResolution(resolved=False)\n\n\ndef resolve_vae_near_checkpoint(checkpoint_file) -> VaeResolution:\n    vae_near_checkpoint = find_vae_near_checkpoint(checkpoint_file)\n    if vae_near_checkpoint is not None and (not shared.opts.sd_vae_overrides_per_model_preferences or is_automatic()):\n        return VaeResolution(vae_near_checkpoint, 'found near the checkpoint')\n\n    return VaeResolution(resolved=False)\n\n\ndef resolve_vae(checkpoint_file) -> VaeResolution:\n    if shared.cmd_opts.vae_path is not None:\n        return VaeResolution(shared.cmd_opts.vae_path, 'from commandline argument')\n\n    if shared.opts.sd_vae_overrides_per_model_preferences and not is_automatic():\n        return resolve_vae_from_setting()\n\n    res = resolve_vae_from_user_metadata(checkpoint_file)\n    if res.resolved:\n        return res\n\n    res = resolve_vae_near_checkpoint(checkpoint_file)\n    if res.resolved:\n        return res\n\n    res = resolve_vae_from_setting()\n\n    return res\n\n\ndef load_vae_dict(filename, map_location):\n    vae_ckpt = sd_models.read_state_dict(filename, map_location=map_location)\n    vae_dict_1 = {k: v for k, v in vae_ckpt.items() if k[0:4] != \"loss\" and k not in vae_ignore_keys}\n    return vae_dict_1\n\n\ndef load_vae(model, vae_file=None, vae_source=\"from unknown source\"):\n    global vae_dict, base_vae, loaded_vae_file\n    # save_settings = False\n\n    cache_enabled = shared.opts.sd_vae_checkpoint_cache > 0\n\n    if vae_file:\n        if cache_enabled and vae_file in checkpoints_loaded:\n            # use vae checkpoint cache\n            print(f\"Loading VAE weights {vae_source}: cached {get_filename(vae_file)}\")\n            store_base_vae(model)\n            _load_vae_dict(model, checkpoints_loaded[vae_file])\n        else:\n            assert os.path.isfile(vae_file), f\"VAE {vae_source} doesn't exist: {vae_file}\"\n            print(f\"Loading VAE weights {vae_source}: {vae_file}\")\n            store_base_vae(model)\n\n            vae_dict_1 = load_vae_dict(vae_file, map_location=shared.weight_load_location)\n            _load_vae_dict(model, vae_dict_1)\n\n            if cache_enabled:\n                # cache newly loaded vae\n                checkpoints_loaded[vae_file] = vae_dict_1.copy()\n\n        # clean up cache if limit is reached\n        if cache_enabled:\n            while len(checkpoints_loaded) > shared.opts.sd_vae_checkpoint_cache + 1: # we need to count the current model\n                checkpoints_loaded.popitem(last=False)  # LRU\n\n        # If vae used is not in dict, update it\n        # It will be removed on refresh though\n        vae_opt = get_filename(vae_file)\n        if vae_opt not in vae_dict:\n            vae_dict[vae_opt] = vae_file\n\n    elif loaded_vae_file:\n        restore_base_vae(model)\n\n    loaded_vae_file = vae_file\n    model.base_vae = base_vae\n    model.loaded_vae_file = loaded_vae_file\n\n\n# don't call this from outside\ndef _load_vae_dict(model, vae_dict_1):\n    model.first_stage_model.load_state_dict(vae_dict_1)\n    model.first_stage_model.to(devices.dtype_vae)\n\n\ndef clear_loaded_vae():\n    global loaded_vae_file\n    loaded_vae_file = None\n\n\nunspecified = object()\n\n\ndef reload_vae_weights(sd_model=None, vae_file=unspecified):\n    if not sd_model:\n        sd_model = shared.sd_model\n\n    checkpoint_info = sd_model.sd_checkpoint_info\n    checkpoint_file = checkpoint_info.filename\n\n    if vae_file == unspecified:\n        vae_file, vae_source = resolve_vae(checkpoint_file).tuple()\n    else:\n        vae_source = \"from function argument\"\n\n    if loaded_vae_file == vae_file:\n        return\n\n    if sd_model.lowvram:\n        lowvram.send_everything_to_cpu()\n    else:\n        sd_model.to(devices.cpu)\n\n    sd_hijack.model_hijack.undo_hijack(sd_model)\n\n    load_vae(sd_model, vae_file, vae_source)\n\n    sd_hijack.model_hijack.hijack(sd_model)\n\n    if not sd_model.lowvram:\n        sd_model.to(devices.device)\n\n    script_callbacks.model_loaded_callback(sd_model)\n\n    print(\"VAE weights loaded.\")\n    return sd_model\n", "modules/ui_extra_networks_hypernets.py": "import os\n\nfrom modules import shared, ui_extra_networks\nfrom modules.ui_extra_networks import quote_js\nfrom modules.hashes import sha256_from_cache\n\n\nclass ExtraNetworksPageHypernetworks(ui_extra_networks.ExtraNetworksPage):\n    def __init__(self):\n        super().__init__('Hypernetworks')\n\n    def refresh(self):\n        shared.reload_hypernetworks()\n\n    def create_item(self, name, index=None, enable_filter=True):\n        full_path = shared.hypernetworks.get(name)\n        if full_path is None:\n            return\n\n        path, ext = os.path.splitext(full_path)\n        sha256 = sha256_from_cache(full_path, f'hypernet/{name}')\n        shorthash = sha256[0:10] if sha256 else None\n        search_terms = [self.search_terms_from_path(path)]\n        if sha256:\n            search_terms.append(sha256)\n        return {\n            \"name\": name,\n            \"filename\": full_path,\n            \"shorthash\": shorthash,\n            \"preview\": self.find_preview(path),\n            \"description\": self.find_description(path),\n            \"search_terms\": search_terms,\n            \"prompt\": quote_js(f\"<hypernet:{name}:\") + \" + opts.extra_networks_default_multiplier + \" + quote_js(\">\"),\n            \"local_preview\": f\"{path}.preview.{shared.opts.samples_format}\",\n            \"sort_keys\": {'default': index, **self.get_sort_keys(path + ext)},\n        }\n\n    def list_items(self):\n        # instantiate a list to protect against concurrent modification\n        names = list(shared.hypernetworks)\n        for index, name in enumerate(names):\n            item = self.create_item(name, index)\n            if item is not None:\n                yield item\n\n    def allowed_directories_for_previews(self):\n        return [shared.cmd_opts.hypernetwork_dir]\n\n", "modules/sd_hijack_clip.py": "import math\nfrom collections import namedtuple\n\nimport torch\n\nfrom modules import prompt_parser, devices, sd_hijack, sd_emphasis\nfrom modules.shared import opts\n\n\nclass PromptChunk:\n    \"\"\"\n    This object contains token ids, weight (multipliers:1.4) and textual inversion embedding info for a chunk of prompt.\n    If a prompt is short, it is represented by one PromptChunk, otherwise, multiple are necessary.\n    Each PromptChunk contains an exact amount of tokens - 77, which includes one for start and end token,\n    so just 75 tokens from prompt.\n    \"\"\"\n\n    def __init__(self):\n        self.tokens = []\n        self.multipliers = []\n        self.fixes = []\n\n\nPromptChunkFix = namedtuple('PromptChunkFix', ['offset', 'embedding'])\n\"\"\"An object of this type is a marker showing that textual inversion embedding's vectors have to placed at offset in the prompt\nchunk. Those objects are found in PromptChunk.fixes and, are placed into FrozenCLIPEmbedderWithCustomWordsBase.hijack.fixes, and finally\nare applied by sd_hijack.EmbeddingsWithFixes's forward function.\"\"\"\n\n\nclass FrozenCLIPEmbedderWithCustomWordsBase(torch.nn.Module):\n    \"\"\"A pytorch module that is a wrapper for FrozenCLIPEmbedder module. it enhances FrozenCLIPEmbedder, making it possible to\n    have unlimited prompt length and assign weights to tokens in prompt.\n    \"\"\"\n\n    def __init__(self, wrapped, hijack):\n        super().__init__()\n\n        self.wrapped = wrapped\n        \"\"\"Original FrozenCLIPEmbedder module; can also be FrozenOpenCLIPEmbedder or xlmr.BertSeriesModelWithTransformation,\n        depending on model.\"\"\"\n\n        self.hijack: sd_hijack.StableDiffusionModelHijack = hijack\n        self.chunk_length = 75\n\n        self.is_trainable = getattr(wrapped, 'is_trainable', False)\n        self.input_key = getattr(wrapped, 'input_key', 'txt')\n        self.legacy_ucg_val = None\n\n    def empty_chunk(self):\n        \"\"\"creates an empty PromptChunk and returns it\"\"\"\n\n        chunk = PromptChunk()\n        chunk.tokens = [self.id_start] + [self.id_end] * (self.chunk_length + 1)\n        chunk.multipliers = [1.0] * (self.chunk_length + 2)\n        return chunk\n\n    def get_target_prompt_token_count(self, token_count):\n        \"\"\"returns the maximum number of tokens a prompt of a known length can have before it requires one more PromptChunk to be represented\"\"\"\n\n        return math.ceil(max(token_count, 1) / self.chunk_length) * self.chunk_length\n\n    def tokenize(self, texts):\n        \"\"\"Converts a batch of texts into a batch of token ids\"\"\"\n\n        raise NotImplementedError\n\n    def encode_with_transformers(self, tokens):\n        \"\"\"\n        converts a batch of token ids (in python lists) into a single tensor with numeric representation of those tokens;\n        All python lists with tokens are assumed to have same length, usually 77.\n        if input is a list with B elements and each element has T tokens, expected output shape is (B, T, C), where C depends on\n        model - can be 768 and 1024.\n        Among other things, this call will read self.hijack.fixes, apply it to its inputs, and clear it (setting it to None).\n        \"\"\"\n\n        raise NotImplementedError\n\n    def encode_embedding_init_text(self, init_text, nvpt):\n        \"\"\"Converts text into a tensor with this text's tokens' embeddings. Note that those are embeddings before they are passed through\n        transformers. nvpt is used as a maximum length in tokens. If text produces less teokens than nvpt, only this many is returned.\"\"\"\n\n        raise NotImplementedError\n\n    def tokenize_line(self, line):\n        \"\"\"\n        this transforms a single prompt into a list of PromptChunk objects - as many as needed to\n        represent the prompt.\n        Returns the list and the total number of tokens in the prompt.\n        \"\"\"\n\n        if opts.emphasis != \"None\":\n            parsed = prompt_parser.parse_prompt_attention(line)\n        else:\n            parsed = [[line, 1.0]]\n\n        tokenized = self.tokenize([text for text, _ in parsed])\n\n        chunks = []\n        chunk = PromptChunk()\n        token_count = 0\n        last_comma = -1\n\n        def next_chunk(is_last=False):\n            \"\"\"puts current chunk into the list of results and produces the next one - empty;\n            if is_last is true, tokens <end-of-text> tokens at the end won't add to token_count\"\"\"\n            nonlocal token_count\n            nonlocal last_comma\n            nonlocal chunk\n\n            if is_last:\n                token_count += len(chunk.tokens)\n            else:\n                token_count += self.chunk_length\n\n            to_add = self.chunk_length - len(chunk.tokens)\n            if to_add > 0:\n                chunk.tokens += [self.id_end] * to_add\n                chunk.multipliers += [1.0] * to_add\n\n            chunk.tokens = [self.id_start] + chunk.tokens + [self.id_end]\n            chunk.multipliers = [1.0] + chunk.multipliers + [1.0]\n\n            last_comma = -1\n            chunks.append(chunk)\n            chunk = PromptChunk()\n\n        for tokens, (text, weight) in zip(tokenized, parsed):\n            if text == 'BREAK' and weight == -1:\n                next_chunk()\n                continue\n\n            position = 0\n            while position < len(tokens):\n                token = tokens[position]\n\n                if token == self.comma_token:\n                    last_comma = len(chunk.tokens)\n\n                # this is when we are at the end of allotted 75 tokens for the current chunk, and the current token is not a comma. opts.comma_padding_backtrack\n                # is a setting that specifies that if there is a comma nearby, the text after the comma should be moved out of this chunk and into the next.\n                elif opts.comma_padding_backtrack != 0 and len(chunk.tokens) == self.chunk_length and last_comma != -1 and len(chunk.tokens) - last_comma <= opts.comma_padding_backtrack:\n                    break_location = last_comma + 1\n\n                    reloc_tokens = chunk.tokens[break_location:]\n                    reloc_mults = chunk.multipliers[break_location:]\n\n                    chunk.tokens = chunk.tokens[:break_location]\n                    chunk.multipliers = chunk.multipliers[:break_location]\n\n                    next_chunk()\n                    chunk.tokens = reloc_tokens\n                    chunk.multipliers = reloc_mults\n\n                if len(chunk.tokens) == self.chunk_length:\n                    next_chunk()\n\n                embedding, embedding_length_in_tokens = self.hijack.embedding_db.find_embedding_at_position(tokens, position)\n                if embedding is None:\n                    chunk.tokens.append(token)\n                    chunk.multipliers.append(weight)\n                    position += 1\n                    continue\n\n                emb_len = int(embedding.vectors)\n                if len(chunk.tokens) + emb_len > self.chunk_length:\n                    next_chunk()\n\n                chunk.fixes.append(PromptChunkFix(len(chunk.tokens), embedding))\n\n                chunk.tokens += [0] * emb_len\n                chunk.multipliers += [weight] * emb_len\n                position += embedding_length_in_tokens\n\n        if chunk.tokens or not chunks:\n            next_chunk(is_last=True)\n\n        return chunks, token_count\n\n    def process_texts(self, texts):\n        \"\"\"\n        Accepts a list of texts and calls tokenize_line() on each, with cache. Returns the list of results and maximum\n        length, in tokens, of all texts.\n        \"\"\"\n\n        token_count = 0\n\n        cache = {}\n        batch_chunks = []\n        for line in texts:\n            if line in cache:\n                chunks = cache[line]\n            else:\n                chunks, current_token_count = self.tokenize_line(line)\n                token_count = max(current_token_count, token_count)\n\n                cache[line] = chunks\n\n            batch_chunks.append(chunks)\n\n        return batch_chunks, token_count\n\n    def forward(self, texts):\n        \"\"\"\n        Accepts an array of texts; Passes texts through transformers network to create a tensor with numerical representation of those texts.\n        Returns a tensor with shape of (B, T, C), where B is length of the array; T is length, in tokens, of texts (including padding) - T will\n        be a multiple of 77; and C is dimensionality of each token - for SD1 it's 768, for SD2 it's 1024, and for SDXL it's 1280.\n        An example shape returned by this function can be: (2, 77, 768).\n        For SDXL, instead of returning one tensor avobe, it returns a tuple with two: the other one with shape (B, 1280) with pooled values.\n        Webui usually sends just one text at a time through this function - the only time when texts is an array with more than one element\n        is when you do prompt editing: \"a picture of a [cat:dog:0.4] eating ice cream\"\n        \"\"\"\n\n        if opts.use_old_emphasis_implementation:\n            import modules.sd_hijack_clip_old\n            return modules.sd_hijack_clip_old.forward_old(self, texts)\n\n        batch_chunks, token_count = self.process_texts(texts)\n\n        used_embeddings = {}\n        chunk_count = max([len(x) for x in batch_chunks])\n\n        zs = []\n        for i in range(chunk_count):\n            batch_chunk = [chunks[i] if i < len(chunks) else self.empty_chunk() for chunks in batch_chunks]\n\n            tokens = [x.tokens for x in batch_chunk]\n            multipliers = [x.multipliers for x in batch_chunk]\n            self.hijack.fixes = [x.fixes for x in batch_chunk]\n\n            for fixes in self.hijack.fixes:\n                for _position, embedding in fixes:\n                    used_embeddings[embedding.name] = embedding\n            devices.torch_npu_set_device()\n            z = self.process_tokens(tokens, multipliers)\n            zs.append(z)\n\n        if opts.textual_inversion_add_hashes_to_infotext and used_embeddings:\n            hashes = []\n            for name, embedding in used_embeddings.items():\n                shorthash = embedding.shorthash\n                if not shorthash:\n                    continue\n\n                name = name.replace(\":\", \"\").replace(\",\", \"\")\n                hashes.append(f\"{name}: {shorthash}\")\n\n            if hashes:\n                if self.hijack.extra_generation_params.get(\"TI hashes\"):\n                    hashes.append(self.hijack.extra_generation_params.get(\"TI hashes\"))\n                self.hijack.extra_generation_params[\"TI hashes\"] = \", \".join(hashes)\n\n        if any(x for x in texts if \"(\" in x or \"[\" in x) and opts.emphasis != \"Original\":\n            self.hijack.extra_generation_params[\"Emphasis\"] = opts.emphasis\n\n        if getattr(self.wrapped, 'return_pooled', False):\n            return torch.hstack(zs), zs[0].pooled\n        else:\n            return torch.hstack(zs)\n\n    def process_tokens(self, remade_batch_tokens, batch_multipliers):\n        \"\"\"\n        sends one single prompt chunk to be encoded by transformers neural network.\n        remade_batch_tokens is a batch of tokens - a list, where every element is a list of tokens; usually\n        there are exactly 77 tokens in the list. batch_multipliers is the same but for multipliers instead of tokens.\n        Multipliers are used to give more or less weight to the outputs of transformers network. Each multiplier\n        corresponds to one token.\n        \"\"\"\n        tokens = torch.asarray(remade_batch_tokens).to(devices.device)\n\n        # this is for SD2: SD1 uses the same token for padding and end of text, while SD2 uses different ones.\n        if self.id_end != self.id_pad:\n            for batch_pos in range(len(remade_batch_tokens)):\n                index = remade_batch_tokens[batch_pos].index(self.id_end)\n                tokens[batch_pos, index+1:tokens.shape[1]] = self.id_pad\n\n        z = self.encode_with_transformers(tokens)\n\n        pooled = getattr(z, 'pooled', None)\n\n        emphasis = sd_emphasis.get_current_option(opts.emphasis)()\n        emphasis.tokens = remade_batch_tokens\n        emphasis.multipliers = torch.asarray(batch_multipliers).to(devices.device)\n        emphasis.z = z\n\n        emphasis.after_transformers()\n\n        z = emphasis.z\n\n        if pooled is not None:\n            z.pooled = pooled\n\n        return z\n\n\nclass FrozenCLIPEmbedderWithCustomWords(FrozenCLIPEmbedderWithCustomWordsBase):\n    def __init__(self, wrapped, hijack):\n        super().__init__(wrapped, hijack)\n        self.tokenizer = wrapped.tokenizer\n\n        vocab = self.tokenizer.get_vocab()\n\n        self.comma_token = vocab.get(',</w>', None)\n\n        self.token_mults = {}\n        tokens_with_parens = [(k, v) for k, v in vocab.items() if '(' in k or ')' in k or '[' in k or ']' in k]\n        for text, ident in tokens_with_parens:\n            mult = 1.0\n            for c in text:\n                if c == '[':\n                    mult /= 1.1\n                if c == ']':\n                    mult *= 1.1\n                if c == '(':\n                    mult *= 1.1\n                if c == ')':\n                    mult /= 1.1\n\n            if mult != 1.0:\n                self.token_mults[ident] = mult\n\n        self.id_start = self.wrapped.tokenizer.bos_token_id\n        self.id_end = self.wrapped.tokenizer.eos_token_id\n        self.id_pad = self.id_end\n\n    def tokenize(self, texts):\n        tokenized = self.wrapped.tokenizer(texts, truncation=False, add_special_tokens=False)[\"input_ids\"]\n\n        return tokenized\n\n    def encode_with_transformers(self, tokens):\n        outputs = self.wrapped.transformer(input_ids=tokens, output_hidden_states=-opts.CLIP_stop_at_last_layers)\n\n        if opts.CLIP_stop_at_last_layers > 1:\n            z = outputs.hidden_states[-opts.CLIP_stop_at_last_layers]\n            z = self.wrapped.transformer.text_model.final_layer_norm(z)\n        else:\n            z = outputs.last_hidden_state\n\n        return z\n\n    def encode_embedding_init_text(self, init_text, nvpt):\n        embedding_layer = self.wrapped.transformer.text_model.embeddings\n        ids = self.wrapped.tokenizer(init_text, max_length=nvpt, return_tensors=\"pt\", add_special_tokens=False)[\"input_ids\"]\n        embedded = embedding_layer.token_embedding.wrapped(ids.to(embedding_layer.token_embedding.wrapped.weight.device)).squeeze(0)\n\n        return embedded\n\n\nclass FrozenCLIPEmbedderForSDXLWithCustomWords(FrozenCLIPEmbedderWithCustomWords):\n    def __init__(self, wrapped, hijack):\n        super().__init__(wrapped, hijack)\n\n    def encode_with_transformers(self, tokens):\n        outputs = self.wrapped.transformer(input_ids=tokens, output_hidden_states=self.wrapped.layer == \"hidden\")\n\n        if self.wrapped.layer == \"last\":\n            z = outputs.last_hidden_state\n        else:\n            z = outputs.hidden_states[self.wrapped.layer_idx]\n\n        return z\n", "modules/shared_init.py": "import os\n\nimport torch\n\nfrom modules import shared\nfrom modules.shared import cmd_opts\n\n\ndef initialize():\n    \"\"\"Initializes fields inside the shared module in a controlled manner.\n\n    Should be called early because some other modules you can import mingt need these fields to be already set.\n    \"\"\"\n\n    os.makedirs(cmd_opts.hypernetwork_dir, exist_ok=True)\n\n    from modules import options, shared_options\n    shared.options_templates = shared_options.options_templates\n    shared.opts = options.Options(shared_options.options_templates, shared_options.restricted_opts)\n    shared.restricted_opts = shared_options.restricted_opts\n    try:\n        shared.opts.load(shared.config_filename)\n    except FileNotFoundError:\n        pass\n\n    from modules import devices\n    devices.device, devices.device_interrogate, devices.device_gfpgan, devices.device_esrgan, devices.device_codeformer = \\\n        (devices.cpu if any(y in cmd_opts.use_cpu for y in [x, 'all']) else devices.get_optimal_device() for x in ['sd', 'interrogate', 'gfpgan', 'esrgan', 'codeformer'])\n\n    devices.dtype = torch.float32 if cmd_opts.no_half else torch.float16\n    devices.dtype_vae = torch.float32 if cmd_opts.no_half or cmd_opts.no_half_vae else torch.float16\n    devices.dtype_inference = torch.float32 if cmd_opts.precision == 'full' else devices.dtype\n\n    shared.device = devices.device\n    shared.weight_load_location = None if cmd_opts.lowram else \"cpu\"\n\n    from modules import shared_state\n    shared.state = shared_state.State()\n\n    from modules import styles\n    shared.prompt_styles = styles.StyleDatabase(shared.styles_filename)\n\n    from modules import interrogate\n    shared.interrogator = interrogate.InterrogateModels(\"interrogate\")\n\n    from modules import shared_total_tqdm\n    shared.total_tqdm = shared_total_tqdm.TotalTQDM()\n\n    from modules import memmon, devices\n    shared.mem_mon = memmon.MemUsageMonitor(\"MemMon\", devices.device, shared.opts)\n    shared.mem_mon.start()\n\n", "modules/timer.py": "import time\nimport argparse\n\n\nclass TimerSubcategory:\n    def __init__(self, timer, category):\n        self.timer = timer\n        self.category = category\n        self.start = None\n        self.original_base_category = timer.base_category\n\n    def __enter__(self):\n        self.start = time.time()\n        self.timer.base_category = self.original_base_category + self.category + \"/\"\n        self.timer.subcategory_level += 1\n\n        if self.timer.print_log:\n            print(f\"{'  ' * self.timer.subcategory_level}{self.category}:\")\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        elapsed_for_subcategroy = time.time() - self.start\n        self.timer.base_category = self.original_base_category\n        self.timer.add_time_to_record(self.original_base_category + self.category, elapsed_for_subcategroy)\n        self.timer.subcategory_level -= 1\n        self.timer.record(self.category, disable_log=True)\n\n\nclass Timer:\n    def __init__(self, print_log=False):\n        self.start = time.time()\n        self.records = {}\n        self.total = 0\n        self.base_category = ''\n        self.print_log = print_log\n        self.subcategory_level = 0\n\n    def elapsed(self):\n        end = time.time()\n        res = end - self.start\n        self.start = end\n        return res\n\n    def add_time_to_record(self, category, amount):\n        if category not in self.records:\n            self.records[category] = 0\n\n        self.records[category] += amount\n\n    def record(self, category, extra_time=0, disable_log=False):\n        e = self.elapsed()\n\n        self.add_time_to_record(self.base_category + category, e + extra_time)\n\n        self.total += e + extra_time\n\n        if self.print_log and not disable_log:\n            print(f\"{'  ' * self.subcategory_level}{category}: done in {e + extra_time:.3f}s\")\n\n    def subcategory(self, name):\n        self.elapsed()\n\n        subcat = TimerSubcategory(self, name)\n        return subcat\n\n    def summary(self):\n        res = f\"{self.total:.1f}s\"\n\n        additions = [(category, time_taken) for category, time_taken in self.records.items() if time_taken >= 0.1 and '/' not in category]\n        if not additions:\n            return res\n\n        res += \" (\"\n        res += \", \".join([f\"{category}: {time_taken:.1f}s\" for category, time_taken in additions])\n        res += \")\"\n\n        return res\n\n    def dump(self):\n        return {'total': self.total, 'records': self.records}\n\n    def reset(self):\n        self.__init__()\n\n\nparser = argparse.ArgumentParser(add_help=False)\nparser.add_argument(\"--log-startup\", action='store_true', help=\"print a detailed log of what's happening at startup\")\nargs = parser.parse_known_args()[0]\n\nstartup_timer = Timer(print_log=args.log_startup)\n\nstartup_record = None\n", "modules/localization.py": "import json\nimport os\n\nfrom modules import errors, scripts\n\nlocalizations = {}\n\n\ndef list_localizations(dirname):\n    localizations.clear()\n\n    for file in os.listdir(dirname):\n        fn, ext = os.path.splitext(file)\n        if ext.lower() != \".json\":\n            continue\n\n        localizations[fn] = [os.path.join(dirname, file)]\n\n    for file in scripts.list_scripts(\"localizations\", \".json\"):\n        fn, ext = os.path.splitext(file.filename)\n        if fn not in localizations:\n            localizations[fn] = []\n        localizations[fn].append(file.path)\n\n\ndef localization_js(current_localization_name: str) -> str:\n    fns = localizations.get(current_localization_name, None)\n    data = {}\n    if fns is not None:\n        for fn in fns:\n            try:\n                with open(fn, \"r\", encoding=\"utf8\") as file:\n                    data.update(json.load(file))\n            except Exception:\n                errors.report(f\"Error loading localization from {fn}\", exc_info=True)\n\n    return f\"window.localization = {json.dumps(data)}\"\n", "modules/ui_extra_networks.py": "import functools\nimport os.path\nimport urllib.parse\nfrom base64 import b64decode\nfrom io import BytesIO\nfrom pathlib import Path\nfrom typing import Optional, Union\nfrom dataclasses import dataclass\n\nfrom modules import shared, ui_extra_networks_user_metadata, errors, extra_networks, util\nfrom modules.images import read_info_from_image, save_image_with_geninfo\nimport gradio as gr\nimport json\nimport html\nfrom fastapi.exceptions import HTTPException\nfrom PIL import Image\n\nfrom modules.infotext_utils import image_from_url_text\n\nextra_pages = []\nallowed_dirs = set()\ndefault_allowed_preview_extensions = [\"png\", \"jpg\", \"jpeg\", \"webp\", \"gif\"]\n\n@functools.cache\ndef allowed_preview_extensions_with_extra(extra_extensions=None):\n    return set(default_allowed_preview_extensions) | set(extra_extensions or [])\n\n\ndef allowed_preview_extensions():\n    return allowed_preview_extensions_with_extra((shared.opts.samples_format, ))\n\n\n@dataclass\nclass ExtraNetworksItem:\n    \"\"\"Wrapper for dictionaries representing ExtraNetworks items.\"\"\"\n    item: dict\n\n\ndef get_tree(paths: Union[str, list[str]], items: dict[str, ExtraNetworksItem]) -> dict:\n    \"\"\"Recursively builds a directory tree.\n\n    Args:\n        paths: Path or list of paths to directories. These paths are treated as roots from which\n            the tree will be built.\n        items: A dictionary associating filepaths to an ExtraNetworksItem instance.\n\n    Returns:\n        The result directory tree.\n    \"\"\"\n    if isinstance(paths, (str,)):\n        paths = [paths]\n\n    def _get_tree(_paths: list[str], _root: str):\n        _res = {}\n        for path in _paths:\n            relpath = os.path.relpath(path, _root)\n            if os.path.isdir(path):\n                dir_items = os.listdir(path)\n                # Ignore empty directories.\n                if not dir_items:\n                    continue\n                dir_tree = _get_tree([os.path.join(path, x) for x in dir_items], _root)\n                # We only want to store non-empty folders in the tree.\n                if dir_tree:\n                    _res[relpath] = dir_tree\n            else:\n                if path not in items:\n                    continue\n                # Add the ExtraNetworksItem to the result.\n                _res[relpath] = items[path]\n        return _res\n\n    res = {}\n    # Handle each root directory separately.\n    # Each root WILL have a key/value at the root of the result dict though\n    # the value can be an empty dict if the directory is empty. We want these\n    # placeholders for empty dirs so we can inform the user later.\n    for path in paths:\n        root = os.path.dirname(path)\n        relpath = os.path.relpath(path, root)\n        # Wrap the path in a list since that is what the `_get_tree` expects.\n        res[relpath] = _get_tree([path], root)\n        if res[relpath]:\n            # We need to pull the inner path out one for these root dirs.\n            res[relpath] = res[relpath][relpath]\n\n    return res\n\ndef register_page(page):\n    \"\"\"registers extra networks page for the UI; recommend doing it in on_before_ui() callback for extensions\"\"\"\n\n    extra_pages.append(page)\n    allowed_dirs.clear()\n    allowed_dirs.update(set(sum([x.allowed_directories_for_previews() for x in extra_pages], [])))\n\n\ndef fetch_file(filename: str = \"\"):\n    from starlette.responses import FileResponse\n\n    if not os.path.isfile(filename):\n        raise HTTPException(status_code=404, detail=\"File not found\")\n\n    if not any(Path(x).absolute() in Path(filename).absolute().parents for x in allowed_dirs):\n        raise ValueError(f\"File cannot be fetched: {filename}. Must be in one of directories registered by extra pages.\")\n\n    ext = os.path.splitext(filename)[1].lower()[1:]\n    if ext not in allowed_preview_extensions():\n        raise ValueError(f\"File cannot be fetched: {filename}. Extensions allowed: {allowed_preview_extensions()}.\")\n\n    # would profit from returning 304\n    return FileResponse(filename, headers={\"Accept-Ranges\": \"bytes\"})\n\n\ndef fetch_cover_images(page: str = \"\", item: str = \"\", index: int = 0):\n    from starlette.responses import Response\n\n    page = next(iter([x for x in extra_pages if x.name == page]), None)\n    if page is None:\n        raise HTTPException(status_code=404, detail=\"File not found\")\n\n    metadata = page.metadata.get(item)\n    if metadata is None:\n        raise HTTPException(status_code=404, detail=\"File not found\")\n\n    cover_images = json.loads(metadata.get('ssmd_cover_images', {}))\n    image = cover_images[index] if index < len(cover_images) else None\n    if not image:\n        raise HTTPException(status_code=404, detail=\"File not found\")\n\n    try:\n        image = Image.open(BytesIO(b64decode(image)))\n        buffer = BytesIO()\n        image.save(buffer, format=image.format)\n        return Response(content=buffer.getvalue(), media_type=image.get_format_mimetype())\n    except Exception as err:\n        raise ValueError(f\"File cannot be fetched: {item}. Failed to load cover image.\") from err\n\n\ndef get_metadata(page: str = \"\", item: str = \"\"):\n    from starlette.responses import JSONResponse\n\n    page = next(iter([x for x in extra_pages if x.name == page]), None)\n    if page is None:\n        return JSONResponse({})\n\n    metadata = page.metadata.get(item)\n    if metadata is None:\n        return JSONResponse({})\n\n    metadata = {i:metadata[i] for i in metadata if i != 'ssmd_cover_images'}  # those are cover images, and they are too big to display in UI as text\n\n    return JSONResponse({\"metadata\": json.dumps(metadata, indent=4, ensure_ascii=False)})\n\n\ndef get_single_card(page: str = \"\", tabname: str = \"\", name: str = \"\"):\n    from starlette.responses import JSONResponse\n\n    page = next(iter([x for x in extra_pages if x.name == page]), None)\n\n    try:\n        item = page.create_item(name, enable_filter=False)\n        page.items[name] = item\n    except Exception as e:\n        errors.display(e, \"creating item for extra network\")\n        item = page.items.get(name)\n\n    page.read_user_metadata(item, use_cache=False)\n    item_html = page.create_item_html(tabname, item, shared.html(\"extra-networks-card.html\"))\n\n    return JSONResponse({\"html\": item_html})\n\n\ndef add_pages_to_demo(app):\n    app.add_api_route(\"/sd_extra_networks/thumb\", fetch_file, methods=[\"GET\"])\n    app.add_api_route(\"/sd_extra_networks/cover-images\", fetch_cover_images, methods=[\"GET\"])\n    app.add_api_route(\"/sd_extra_networks/metadata\", get_metadata, methods=[\"GET\"])\n    app.add_api_route(\"/sd_extra_networks/get-single-card\", get_single_card, methods=[\"GET\"])\n\n\ndef quote_js(s):\n    s = s.replace('\\\\', '\\\\\\\\')\n    s = s.replace('\"', '\\\\\"')\n    return f'\"{s}\"'\n\n\nclass ExtraNetworksPage:\n    def __init__(self, title):\n        self.title = title\n        self.name = title.lower()\n        # This is the actual name of the extra networks tab (not txt2img/img2img).\n        self.extra_networks_tabname = self.name.replace(\" \", \"_\")\n        self.allow_prompt = True\n        self.allow_negative_prompt = False\n        self.metadata = {}\n        self.items = {}\n        self.lister = util.MassFileLister()\n        # HTML Templates\n        self.pane_tpl = shared.html(\"extra-networks-pane.html\")\n        self.pane_content_tree_tpl = shared.html(\"extra-networks-pane-tree.html\")\n        self.pane_content_dirs_tpl = shared.html(\"extra-networks-pane-dirs.html\")\n        self.card_tpl = shared.html(\"extra-networks-card.html\")\n        self.btn_tree_tpl = shared.html(\"extra-networks-tree-button.html\")\n        self.btn_copy_path_tpl = shared.html(\"extra-networks-copy-path-button.html\")\n        self.btn_metadata_tpl = shared.html(\"extra-networks-metadata-button.html\")\n        self.btn_edit_item_tpl = shared.html(\"extra-networks-edit-item-button.html\")\n\n    def refresh(self):\n        pass\n\n    def read_user_metadata(self, item, use_cache=True):\n        filename = item.get(\"filename\", None)\n        metadata = extra_networks.get_user_metadata(filename, lister=self.lister if use_cache else None)\n\n        desc = metadata.get(\"description\", None)\n        if desc is not None:\n            item[\"description\"] = desc\n\n        item[\"user_metadata\"] = metadata\n\n    def link_preview(self, filename):\n        quoted_filename = urllib.parse.quote(filename.replace('\\\\', '/'))\n        mtime, _ = self.lister.mctime(filename)\n        return f\"./sd_extra_networks/thumb?filename={quoted_filename}&mtime={mtime}\"\n\n    def search_terms_from_path(self, filename, possible_directories=None):\n        abspath = os.path.abspath(filename)\n        for parentdir in (possible_directories if possible_directories is not None else self.allowed_directories_for_previews()):\n            parentdir = os.path.dirname(os.path.abspath(parentdir))\n            if abspath.startswith(parentdir):\n                return os.path.relpath(abspath, parentdir)\n\n        return \"\"\n\n    def create_item_html(\n        self,\n        tabname: str,\n        item: dict,\n        template: Optional[str] = None,\n    ) -> Union[str, dict]:\n        \"\"\"Generates HTML for a single ExtraNetworks Item.\n\n        Args:\n            tabname: The name of the active tab.\n            item: Dictionary containing item information.\n            template: Optional template string to use.\n\n        Returns:\n            If a template is passed: HTML string generated for this item.\n                Can be empty if the item is not meant to be shown.\n            If no template is passed: A dictionary containing the generated item's attributes.\n        \"\"\"\n        preview = item.get(\"preview\", None)\n        style_height = f\"height: {shared.opts.extra_networks_card_height}px;\" if shared.opts.extra_networks_card_height else ''\n        style_width = f\"width: {shared.opts.extra_networks_card_width}px;\" if shared.opts.extra_networks_card_width else ''\n        style_font_size = f\"font-size: {shared.opts.extra_networks_card_text_scale*100}%;\"\n        card_style = style_height + style_width + style_font_size\n        background_image = f'<img src=\"{html.escape(preview)}\" class=\"preview\" loading=\"lazy\">' if preview else ''\n\n        onclick = item.get(\"onclick\", None)\n        if onclick is None:\n            # Don't quote prompt/neg_prompt since they are stored as js strings already.\n            onclick_js_tpl = \"cardClicked('{tabname}', {prompt}, {neg_prompt}, {allow_neg});\"\n            onclick = onclick_js_tpl.format(\n                **{\n                    \"tabname\": tabname,\n                    \"prompt\": item[\"prompt\"],\n                    \"neg_prompt\": item.get(\"negative_prompt\", \"''\"),\n                    \"allow_neg\": str(self.allow_negative_prompt).lower(),\n                }\n            )\n            onclick = html.escape(onclick)\n\n        btn_copy_path = self.btn_copy_path_tpl.format(**{\"filename\": item[\"filename\"]})\n        btn_metadata = \"\"\n        metadata = item.get(\"metadata\")\n        if metadata:\n            btn_metadata = self.btn_metadata_tpl.format(\n                **{\n                    \"extra_networks_tabname\": self.extra_networks_tabname,\n                }\n            )\n        btn_edit_item = self.btn_edit_item_tpl.format(\n            **{\n                \"tabname\": tabname,\n                \"extra_networks_tabname\": self.extra_networks_tabname,\n            }\n        )\n\n        local_path = \"\"\n        filename = item.get(\"filename\", \"\")\n        for reldir in self.allowed_directories_for_previews():\n            absdir = os.path.abspath(reldir)\n\n            if filename.startswith(absdir):\n                local_path = filename[len(absdir):]\n\n        # if this is true, the item must not be shown in the default view, and must instead only be\n        # shown when searching for it\n        if shared.opts.extra_networks_hidden_models == \"Always\":\n            search_only = False\n        else:\n            search_only = \"/.\" in local_path or \"\\\\.\" in local_path\n\n        if search_only and shared.opts.extra_networks_hidden_models == \"Never\":\n            return \"\"\n\n        sort_keys = \" \".join(\n            [\n                f'data-sort-{k}=\"{html.escape(str(v))}\"'\n                for k, v in item.get(\"sort_keys\", {}).items()\n            ]\n        ).strip()\n\n        search_terms_html = \"\"\n        search_term_template = \"<span class='hidden {class}'>{search_term}</span>\"\n        for search_term in item.get(\"search_terms\", []):\n            search_terms_html += search_term_template.format(\n                **{\n                    \"class\": f\"search_terms{' search_only' if search_only else ''}\",\n                    \"search_term\": search_term,\n                }\n            )\n\n        description = (item.get(\"description\", \"\") or \"\" if shared.opts.extra_networks_card_show_desc else \"\")\n        if not shared.opts.extra_networks_card_description_is_html:\n            description = html.escape(description)\n\n        # Some items here might not be used depending on HTML template used.\n        args = {\n            \"background_image\": background_image,\n            \"card_clicked\": onclick,\n            \"copy_path_button\": btn_copy_path,\n            \"description\": description,\n            \"edit_button\": btn_edit_item,\n            \"local_preview\": quote_js(item[\"local_preview\"]),\n            \"metadata_button\": btn_metadata,\n            \"name\": html.escape(item[\"name\"]),\n            \"prompt\": item.get(\"prompt\", None),\n            \"save_card_preview\": html.escape(f\"return saveCardPreview(event, '{tabname}', '{item['local_preview']}');\"),\n            \"search_only\": \" search_only\" if search_only else \"\",\n            \"search_terms\": search_terms_html,\n            \"sort_keys\": sort_keys,\n            \"style\": card_style,\n            \"tabname\": tabname,\n            \"extra_networks_tabname\": self.extra_networks_tabname,\n        }\n\n        if template:\n            return template.format(**args)\n        else:\n            return args\n\n    def create_tree_dir_item_html(\n        self,\n        tabname: str,\n        dir_path: str,\n        content: Optional[str] = None,\n    ) -> Optional[str]:\n        \"\"\"Generates HTML for a directory item in the tree.\n\n        The generated HTML is of the format:\n        ```html\n        <li class=\"tree-list-item tree-list-item--has-subitem\">\n            <div class=\"tree-list-content tree-list-content-dir\"></div>\n            <ul class=\"tree-list tree-list--subgroup\">\n                {content}\n            </ul>\n        </li>\n        ```\n\n        Args:\n            tabname: The name of the active tab.\n            dir_path: Path to the directory for this item.\n            content: Optional HTML string that will be wrapped by this <ul>.\n\n        Returns:\n            HTML formatted string.\n        \"\"\"\n        if not content:\n            return None\n\n        btn = self.btn_tree_tpl.format(\n            **{\n                \"search_terms\": \"\",\n                \"subclass\": \"tree-list-content-dir\",\n                \"tabname\": tabname,\n                \"extra_networks_tabname\": self.extra_networks_tabname,\n                \"onclick_extra\": \"\",\n                \"data_path\": dir_path,\n                \"data_hash\": \"\",\n                \"action_list_item_action_leading\": \"<i class='tree-list-item-action-chevron'></i>\",\n                \"action_list_item_visual_leading\": \"\ud83d\uddc0\",\n                \"action_list_item_label\": os.path.basename(dir_path),\n                \"action_list_item_visual_trailing\": \"\",\n                \"action_list_item_action_trailing\": \"\",\n            }\n        )\n        ul = f\"<ul class='tree-list tree-list--subgroup' hidden>{content}</ul>\"\n        return (\n            \"<li class='tree-list-item tree-list-item--has-subitem' data-tree-entry-type='dir'>\"\n            f\"{btn}{ul}\"\n            \"</li>\"\n        )\n\n    def create_tree_file_item_html(self, tabname: str, file_path: str, item: dict) -> str:\n        \"\"\"Generates HTML for a file item in the tree.\n\n        The generated HTML is of the format:\n        ```html\n        <li class=\"tree-list-item tree-list-item--subitem\">\n            <span data-filterable-item-text hidden></span>\n            <div class=\"tree-list-content tree-list-content-file\"></div>\n        </li>\n        ```\n\n        Args:\n            tabname: The name of the active tab.\n            file_path: The path to the file for this item.\n            item: Dictionary containing the item information.\n\n        Returns:\n            HTML formatted string.\n        \"\"\"\n        item_html_args = self.create_item_html(tabname, item)\n        action_buttons = \"\".join(\n            [\n                item_html_args[\"copy_path_button\"],\n                item_html_args[\"metadata_button\"],\n                item_html_args[\"edit_button\"],\n            ]\n        )\n        action_buttons = f\"<div class=\\\"button-row\\\">{action_buttons}</div>\"\n        btn = self.btn_tree_tpl.format(\n            **{\n                \"search_terms\": \"\",\n                \"subclass\": \"tree-list-content-file\",\n                \"tabname\": tabname,\n                \"extra_networks_tabname\": self.extra_networks_tabname,\n                \"onclick_extra\": item_html_args[\"card_clicked\"],\n                \"data_path\": file_path,\n                \"data_hash\": item[\"shorthash\"],\n                \"action_list_item_action_leading\": \"<i class='tree-list-item-action-chevron'></i>\",\n                \"action_list_item_visual_leading\": \"\ud83d\uddce\",\n                \"action_list_item_label\": item[\"name\"],\n                \"action_list_item_visual_trailing\": \"\",\n                \"action_list_item_action_trailing\": action_buttons,\n            }\n        )\n        return (\n            \"<li class='tree-list-item tree-list-item--subitem' data-tree-entry-type='file'>\"\n            f\"{btn}\"\n            \"</li>\"\n        )\n\n    def create_tree_view_html(self, tabname: str) -> str:\n        \"\"\"Generates HTML for displaying folders in a tree view.\n\n        Args:\n            tabname: The name of the active tab.\n\n        Returns:\n            HTML string generated for this tree view.\n        \"\"\"\n        res = \"\"\n\n        # Setup the tree dictionary.\n        roots = self.allowed_directories_for_previews()\n        tree_items = {v[\"filename\"]: ExtraNetworksItem(v) for v in self.items.values()}\n        tree = get_tree([os.path.abspath(x) for x in roots], items=tree_items)\n\n        if not tree:\n            return res\n\n        def _build_tree(data: Optional[dict[str, ExtraNetworksItem]] = None) -> Optional[str]:\n            \"\"\"Recursively builds HTML for a tree.\n\n            Args:\n                data: Dictionary representing a directory tree. Can be NoneType.\n                    Data keys should be absolute paths from the root and values\n                    should be subdirectory trees or an ExtraNetworksItem.\n\n            Returns:\n                If data is not None: HTML string\n                Else: None\n            \"\"\"\n            if not data:\n                return None\n\n            # Lists for storing <li> items html for directories and files separately.\n            _dir_li = []\n            _file_li = []\n\n            for k, v in sorted(data.items(), key=lambda x: shared.natural_sort_key(x[0])):\n                if isinstance(v, (ExtraNetworksItem,)):\n                    _file_li.append(self.create_tree_file_item_html(tabname, k, v.item))\n                else:\n                    _dir_li.append(self.create_tree_dir_item_html(tabname, k, _build_tree(v)))\n\n            # Directories should always be displayed before files so we order them here.\n            return \"\".join(_dir_li) + \"\".join(_file_li)\n\n        # Add each root directory to the tree.\n        for k, v in sorted(tree.items(), key=lambda x: shared.natural_sort_key(x[0])):\n            item_html = self.create_tree_dir_item_html(tabname, k, _build_tree(v))\n            # Only add non-empty entries to the tree.\n            if item_html is not None:\n                res += item_html\n\n        return f\"<ul class='tree-list tree-list--tree'>{res}</ul>\"\n\n    def create_dirs_view_html(self, tabname: str) -> str:\n        \"\"\"Generates HTML for displaying folders.\"\"\"\n\n        subdirs = {}\n        for parentdir in [os.path.abspath(x) for x in self.allowed_directories_for_previews()]:\n            for root, dirs, _ in sorted(os.walk(parentdir, followlinks=True), key=lambda x: shared.natural_sort_key(x[0])):\n                for dirname in sorted(dirs, key=shared.natural_sort_key):\n                    x = os.path.join(root, dirname)\n\n                    if not os.path.isdir(x):\n                        continue\n\n                    subdir = os.path.abspath(x)[len(parentdir):]\n\n                    if shared.opts.extra_networks_dir_button_function:\n                        if not subdir.startswith(os.path.sep):\n                            subdir = os.path.sep + subdir\n                    else:\n                        while subdir.startswith(os.path.sep):\n                            subdir = subdir[1:]\n\n                    is_empty = len(os.listdir(x)) == 0\n                    if not is_empty and not subdir.endswith(os.path.sep):\n                        subdir = subdir + os.path.sep\n\n                    if (os.path.sep + \".\" in subdir or subdir.startswith(\".\")) and not shared.opts.extra_networks_show_hidden_directories:\n                        continue\n\n                    subdirs[subdir] = 1\n\n        if subdirs:\n            subdirs = {\"\": 1, **subdirs}\n\n        subdirs_html = \"\".join([f\"\"\"\n        <button class='lg secondary gradio-button custom-button{\" search-all\" if subdir == \"\" else \"\"}' onclick='extraNetworksSearchButton(\"{tabname}\", \"{self.extra_networks_tabname}\", event)'>\n        {html.escape(subdir if subdir != \"\" else \"all\")}\n        </button>\n        \"\"\" for subdir in subdirs])\n\n        return subdirs_html\n\n    def create_card_view_html(self, tabname: str, *, none_message) -> str:\n        \"\"\"Generates HTML for the network Card View section for a tab.\n\n        This HTML goes into the `extra-networks-pane.html` <div> with\n        `id='{tabname}_{extra_networks_tabname}_cards`.\n\n        Args:\n            tabname: The name of the active tab.\n            none_message: HTML text to show when there are no cards.\n\n        Returns:\n            HTML formatted string.\n        \"\"\"\n        res = []\n        for item in self.items.values():\n            res.append(self.create_item_html(tabname, item, self.card_tpl))\n\n        if not res:\n            dirs = \"\".join([f\"<li>{x}</li>\" for x in self.allowed_directories_for_previews()])\n            res = [none_message or shared.html(\"extra-networks-no-cards.html\").format(dirs=dirs)]\n\n        return \"\".join(res)\n\n    def create_html(self, tabname, *, empty=False):\n        \"\"\"Generates an HTML string for the current pane.\n\n        The generated HTML uses `extra-networks-pane.html` as a template.\n\n        Args:\n            tabname: The name of the active tab.\n            empty: create an empty HTML page with no items\n\n        Returns:\n            HTML formatted string.\n        \"\"\"\n        self.lister.reset()\n        self.metadata = {}\n\n        items_list = [] if empty else self.list_items()\n        self.items = {x[\"name\"]: x for x in items_list}\n\n        # Populate the instance metadata for each item.\n        for item in self.items.values():\n            metadata = item.get(\"metadata\")\n            if metadata:\n                self.metadata[item[\"name\"]] = metadata\n\n            if \"user_metadata\" not in item:\n                self.read_user_metadata(item)\n\n        show_tree = shared.opts.extra_networks_tree_view_default_enabled\n\n        page_params = {\n            \"tabname\": tabname,\n            \"extra_networks_tabname\": self.extra_networks_tabname,\n            \"data_sortdir\": shared.opts.extra_networks_card_order,\n            \"sort_path_active\": ' extra-network-control--enabled' if shared.opts.extra_networks_card_order_field == 'Path' else '',\n            \"sort_name_active\": ' extra-network-control--enabled' if shared.opts.extra_networks_card_order_field == 'Name' else '',\n            \"sort_date_created_active\": ' extra-network-control--enabled' if shared.opts.extra_networks_card_order_field == 'Date Created' else '',\n            \"sort_date_modified_active\": ' extra-network-control--enabled' if shared.opts.extra_networks_card_order_field == 'Date Modified' else '',\n            \"tree_view_btn_extra_class\": \"extra-network-control--enabled\" if show_tree else \"\",\n            \"items_html\": self.create_card_view_html(tabname, none_message=\"Loading...\" if empty else None),\n            \"extra_networks_tree_view_default_width\": shared.opts.extra_networks_tree_view_default_width,\n            \"tree_view_div_default_display_class\": \"\" if show_tree else \"extra-network-dirs-hidden\",\n        }\n\n        if shared.opts.extra_networks_tree_view_style == \"Tree\":\n            pane_content = self.pane_content_tree_tpl.format(**page_params, tree_html=self.create_tree_view_html(tabname))\n        else:\n            pane_content = self.pane_content_dirs_tpl.format(**page_params, dirs_html=self.create_dirs_view_html(tabname))\n\n        return self.pane_tpl.format(**page_params, pane_content=pane_content)\n\n    def create_item(self, name, index=None):\n        raise NotImplementedError()\n\n    def list_items(self):\n        raise NotImplementedError()\n\n    def allowed_directories_for_previews(self):\n        return []\n\n    def get_sort_keys(self, path):\n        \"\"\"\n        List of default keys used for sorting in the UI.\n        \"\"\"\n        pth = Path(path)\n        mtime, ctime = self.lister.mctime(path)\n        return {\n            \"date_created\": int(mtime),\n            \"date_modified\": int(ctime),\n            \"name\": pth.name.lower(),\n            \"path\": str(pth).lower(),\n        }\n\n    def find_preview(self, path):\n        \"\"\"\n        Find a preview PNG for a given path (without extension) and call link_preview on it.\n        \"\"\"\n\n        potential_files = sum([[f\"{path}.{ext}\", f\"{path}.preview.{ext}\"] for ext in allowed_preview_extensions()], [])\n\n        for file in potential_files:\n            if self.lister.exists(file):\n                return self.link_preview(file)\n\n        return None\n\n    def find_embedded_preview(self, path, name, metadata):\n        \"\"\"\n        Find if embedded preview exists in safetensors metadata and return endpoint for it.\n        \"\"\"\n\n        file = f\"{path}.safetensors\"\n        if self.lister.exists(file) and 'ssmd_cover_images' in metadata and len(list(filter(None, json.loads(metadata['ssmd_cover_images'])))) > 0:\n            return f\"./sd_extra_networks/cover-images?page={self.extra_networks_tabname}&item={name}\"\n\n        return None\n\n    def find_description(self, path):\n        \"\"\"\n        Find and read a description file for a given path (without extension).\n        \"\"\"\n        for file in [f\"{path}.txt\", f\"{path}.description.txt\"]:\n            if not self.lister.exists(file):\n                continue\n\n            try:\n                with open(file, \"r\", encoding=\"utf-8\", errors=\"replace\") as f:\n                    return f.read()\n            except OSError:\n                pass\n        return None\n\n    def create_user_metadata_editor(self, ui, tabname):\n        return ui_extra_networks_user_metadata.UserMetadataEditor(ui, tabname, self)\n\n\ndef initialize():\n    extra_pages.clear()\n\n\ndef register_default_pages():\n    from modules.ui_extra_networks_textual_inversion import ExtraNetworksPageTextualInversion\n    from modules.ui_extra_networks_hypernets import ExtraNetworksPageHypernetworks\n    from modules.ui_extra_networks_checkpoints import ExtraNetworksPageCheckpoints\n    register_page(ExtraNetworksPageTextualInversion())\n    register_page(ExtraNetworksPageHypernetworks())\n    register_page(ExtraNetworksPageCheckpoints())\n\n\nclass ExtraNetworksUi:\n    def __init__(self):\n        self.pages = None\n        \"\"\"gradio HTML components related to extra networks' pages\"\"\"\n\n        self.page_contents = None\n        \"\"\"HTML content of the above; empty initially, filled when extra pages have to be shown\"\"\"\n\n        self.stored_extra_pages = None\n\n        self.button_save_preview = None\n        self.preview_target_filename = None\n\n        self.tabname = None\n\n\ndef pages_in_preferred_order(pages):\n    tab_order = [x.lower().strip() for x in shared.opts.ui_extra_networks_tab_reorder.split(\",\")]\n\n    def tab_name_score(name):\n        name = name.lower()\n        for i, possible_match in enumerate(tab_order):\n            if possible_match in name:\n                return i\n\n        return len(pages)\n\n    tab_scores = {page.name: (tab_name_score(page.name), original_index) for original_index, page in enumerate(pages)}\n\n    return sorted(pages, key=lambda x: tab_scores[x.name])\n\n\ndef create_ui(interface: gr.Blocks, unrelated_tabs, tabname):\n    ui = ExtraNetworksUi()\n    ui.pages = []\n    ui.pages_contents = []\n    ui.user_metadata_editors = []\n    ui.stored_extra_pages = pages_in_preferred_order(extra_pages.copy())\n    ui.tabname = tabname\n\n    related_tabs = []\n\n    for page in ui.stored_extra_pages:\n        with gr.Tab(page.title, elem_id=f\"{tabname}_{page.extra_networks_tabname}\", elem_classes=[\"extra-page\"]) as tab:\n            with gr.Column(elem_id=f\"{tabname}_{page.extra_networks_tabname}_prompts\", elem_classes=[\"extra-page-prompts\"]):\n                pass\n\n            elem_id = f\"{tabname}_{page.extra_networks_tabname}_cards_html\"\n            page_elem = gr.HTML(page.create_html(tabname, empty=True), elem_id=elem_id)\n            ui.pages.append(page_elem)\n            editor = page.create_user_metadata_editor(ui, tabname)\n            editor.create_ui()\n            ui.user_metadata_editors.append(editor)\n            related_tabs.append(tab)\n\n    ui.button_save_preview = gr.Button('Save preview', elem_id=f\"{tabname}_save_preview\", visible=False)\n    ui.preview_target_filename = gr.Textbox('Preview save filename', elem_id=f\"{tabname}_preview_filename\", visible=False)\n\n    for tab in unrelated_tabs:\n        tab.select(fn=None, _js=f\"function(){{extraNetworksUnrelatedTabSelected('{tabname}');}}\", inputs=[], outputs=[], show_progress=False)\n\n    for page, tab in zip(ui.stored_extra_pages, related_tabs):\n        jscode = (\n            \"function(){{\"\n            f\"extraNetworksTabSelected('{tabname}', '{tabname}_{page.extra_networks_tabname}_prompts', {str(page.allow_prompt).lower()}, {str(page.allow_negative_prompt).lower()}, '{tabname}_{page.extra_networks_tabname}');\"\n            f\"applyExtraNetworkFilter('{tabname}_{page.extra_networks_tabname}');\"\n            \"}}\"\n        )\n        tab.select(fn=None, _js=jscode, inputs=[], outputs=[], show_progress=False)\n\n        def refresh():\n            for pg in ui.stored_extra_pages:\n                pg.refresh()\n            create_html()\n            return ui.pages_contents\n\n        button_refresh = gr.Button(\"Refresh\", elem_id=f\"{tabname}_{page.extra_networks_tabname}_extra_refresh_internal\", visible=False)\n        button_refresh.click(fn=refresh, inputs=[], outputs=ui.pages).then(fn=lambda: None, _js=\"function(){ \" + f\"applyExtraNetworkFilter('{tabname}_{page.extra_networks_tabname}');\" + \" }\").then(fn=lambda: None, _js='setupAllResizeHandles')\n\n    def create_html():\n        ui.pages_contents = [pg.create_html(ui.tabname) for pg in ui.stored_extra_pages]\n\n    def pages_html():\n        if not ui.pages_contents:\n            create_html()\n        return ui.pages_contents\n\n    interface.load(fn=pages_html, inputs=[], outputs=ui.pages).then(fn=lambda: None, _js='setupAllResizeHandles')\n\n    return ui\n\n\ndef path_is_parent(parent_path, child_path):\n    parent_path = os.path.abspath(parent_path)\n    child_path = os.path.abspath(child_path)\n\n    return child_path.startswith(parent_path)\n\n\ndef setup_ui(ui, gallery):\n    def save_preview(index, images, filename):\n        # this function is here for backwards compatibility and likely will be removed soon\n\n        if len(images) == 0:\n            print(\"There is no image in gallery to save as a preview.\")\n            return [page.create_html(ui.tabname) for page in ui.stored_extra_pages]\n\n        index = int(index)\n        index = 0 if index < 0 else index\n        index = len(images) - 1 if index >= len(images) else index\n\n        img_info = images[index if index >= 0 else 0]\n        image = image_from_url_text(img_info)\n        geninfo, items = read_info_from_image(image)\n\n        is_allowed = False\n        for extra_page in ui.stored_extra_pages:\n            if any(path_is_parent(x, filename) for x in extra_page.allowed_directories_for_previews()):\n                is_allowed = True\n                break\n\n        assert is_allowed, f'writing to {filename} is not allowed'\n\n        save_image_with_geninfo(image, geninfo, filename)\n\n        return [page.create_html(ui.tabname) for page in ui.stored_extra_pages]\n\n    ui.button_save_preview.click(\n        fn=save_preview,\n        _js=\"function(x, y, z){return [selected_gallery_index(), y, z]}\",\n        inputs=[ui.preview_target_filename, gallery, ui.preview_target_filename],\n        outputs=[*ui.pages]\n    )\n\n    for editor in ui.user_metadata_editors:\n        editor.setup_ui(gallery)\n", "modules/img2img.py": "import os\nfrom contextlib import closing\nfrom pathlib import Path\n\nimport numpy as np\nfrom PIL import Image, ImageOps, ImageFilter, ImageEnhance, UnidentifiedImageError\nimport gradio as gr\n\nfrom modules import images\nfrom modules.infotext_utils import create_override_settings_dict, parse_generation_parameters\nfrom modules.processing import Processed, StableDiffusionProcessingImg2Img, process_images\nfrom modules.shared import opts, state\nfrom modules.sd_models import get_closet_checkpoint_match\nimport modules.shared as shared\nimport modules.processing as processing\nfrom modules.ui import plaintext_to_html\nimport modules.scripts\n\n\ndef process_batch(p, input_dir, output_dir, inpaint_mask_dir, args, to_scale=False, scale_by=1.0, use_png_info=False, png_info_props=None, png_info_dir=None):\n    output_dir = output_dir.strip()\n    processing.fix_seed(p)\n\n    batch_images = list(shared.walk_files(input_dir, allowed_extensions=(\".png\", \".jpg\", \".jpeg\", \".webp\", \".tif\", \".tiff\")))\n\n    is_inpaint_batch = False\n    if inpaint_mask_dir:\n        inpaint_masks = shared.listfiles(inpaint_mask_dir)\n        is_inpaint_batch = bool(inpaint_masks)\n\n        if is_inpaint_batch:\n            print(f\"\\nInpaint batch is enabled. {len(inpaint_masks)} masks found.\")\n\n    print(f\"Will process {len(batch_images)} images, creating {p.n_iter * p.batch_size} new images for each.\")\n\n    state.job_count = len(batch_images) * p.n_iter\n\n    # extract \"default\" params to use in case getting png info fails\n    prompt = p.prompt\n    negative_prompt = p.negative_prompt\n    seed = p.seed\n    cfg_scale = p.cfg_scale\n    sampler_name = p.sampler_name\n    steps = p.steps\n    override_settings = p.override_settings\n    sd_model_checkpoint_override = get_closet_checkpoint_match(override_settings.get(\"sd_model_checkpoint\", None))\n    batch_results = None\n    discard_further_results = False\n    for i, image in enumerate(batch_images):\n        state.job = f\"{i+1} out of {len(batch_images)}\"\n        if state.skipped:\n            state.skipped = False\n\n        if state.interrupted or state.stopping_generation:\n            break\n\n        try:\n            img = images.read(image)\n        except UnidentifiedImageError as e:\n            print(e)\n            continue\n        # Use the EXIF orientation of photos taken by smartphones.\n        img = ImageOps.exif_transpose(img)\n\n        if to_scale:\n            p.width = int(img.width * scale_by)\n            p.height = int(img.height * scale_by)\n\n        p.init_images = [img] * p.batch_size\n\n        image_path = Path(image)\n        if is_inpaint_batch:\n            # try to find corresponding mask for an image using simple filename matching\n            if len(inpaint_masks) == 1:\n                mask_image_path = inpaint_masks[0]\n            else:\n                # try to find corresponding mask for an image using simple filename matching\n                mask_image_dir = Path(inpaint_mask_dir)\n                masks_found = list(mask_image_dir.glob(f\"{image_path.stem}.*\"))\n\n                if len(masks_found) == 0:\n                    print(f\"Warning: mask is not found for {image_path} in {mask_image_dir}. Skipping it.\")\n                    continue\n\n                # it should contain only 1 matching mask\n                # otherwise user has many masks with the same name but different extensions\n                mask_image_path = masks_found[0]\n\n            mask_image = images.read(mask_image_path)\n            p.image_mask = mask_image\n\n        if use_png_info:\n            try:\n                info_img = img\n                if png_info_dir:\n                    info_img_path = os.path.join(png_info_dir, os.path.basename(image))\n                    info_img = images.read(info_img_path)\n                geninfo, _ = images.read_info_from_image(info_img)\n                parsed_parameters = parse_generation_parameters(geninfo)\n                parsed_parameters = {k: v for k, v in parsed_parameters.items() if k in (png_info_props or {})}\n            except Exception:\n                parsed_parameters = {}\n\n            p.prompt = prompt + (\" \" + parsed_parameters[\"Prompt\"] if \"Prompt\" in parsed_parameters else \"\")\n            p.negative_prompt = negative_prompt + (\" \" + parsed_parameters[\"Negative prompt\"] if \"Negative prompt\" in parsed_parameters else \"\")\n            p.seed = int(parsed_parameters.get(\"Seed\", seed))\n            p.cfg_scale = float(parsed_parameters.get(\"CFG scale\", cfg_scale))\n            p.sampler_name = parsed_parameters.get(\"Sampler\", sampler_name)\n            p.steps = int(parsed_parameters.get(\"Steps\", steps))\n\n            model_info = get_closet_checkpoint_match(parsed_parameters.get(\"Model hash\", None))\n            if model_info is not None:\n                p.override_settings['sd_model_checkpoint'] = model_info.name\n            elif sd_model_checkpoint_override:\n                p.override_settings['sd_model_checkpoint'] = sd_model_checkpoint_override\n            else:\n                p.override_settings.pop(\"sd_model_checkpoint\", None)\n\n        if output_dir:\n            p.outpath_samples = output_dir\n            p.override_settings['save_to_dirs'] = False\n            p.override_settings['save_images_replace_action'] = \"Add number suffix\"\n            if p.n_iter > 1 or p.batch_size > 1:\n                p.override_settings['samples_filename_pattern'] = f'{image_path.stem}-[generation_number]'\n            else:\n                p.override_settings['samples_filename_pattern'] = f'{image_path.stem}'\n\n        proc = modules.scripts.scripts_img2img.run(p, *args)\n\n        if proc is None:\n            p.override_settings.pop('save_images_replace_action', None)\n            proc = process_images(p)\n\n        if not discard_further_results and proc:\n            if batch_results:\n                batch_results.images.extend(proc.images)\n                batch_results.infotexts.extend(proc.infotexts)\n            else:\n                batch_results = proc\n\n            if 0 <= shared.opts.img2img_batch_show_results_limit < len(batch_results.images):\n                discard_further_results = True\n                batch_results.images = batch_results.images[:int(shared.opts.img2img_batch_show_results_limit)]\n                batch_results.infotexts = batch_results.infotexts[:int(shared.opts.img2img_batch_show_results_limit)]\n\n    return batch_results\n\n\ndef img2img(id_task: str, request: gr.Request, mode: int, prompt: str, negative_prompt: str, prompt_styles, init_img, sketch, init_img_with_mask, inpaint_color_sketch, inpaint_color_sketch_orig, init_img_inpaint, init_mask_inpaint, mask_blur: int, mask_alpha: float, inpainting_fill: int, n_iter: int, batch_size: int, cfg_scale: float, image_cfg_scale: float, denoising_strength: float, selected_scale_tab: int, height: int, width: int, scale_by: float, resize_mode: int, inpaint_full_res: bool, inpaint_full_res_padding: int, inpainting_mask_invert: int, img2img_batch_input_dir: str, img2img_batch_output_dir: str, img2img_batch_inpaint_mask_dir: str, override_settings_texts, img2img_batch_use_png_info: bool, img2img_batch_png_info_props: list, img2img_batch_png_info_dir: str, *args):\n    override_settings = create_override_settings_dict(override_settings_texts)\n\n    is_batch = mode == 5\n\n    if mode == 0:  # img2img\n        image = init_img\n        mask = None\n    elif mode == 1:  # img2img sketch\n        image = sketch\n        mask = None\n    elif mode == 2:  # inpaint\n        image, mask = init_img_with_mask[\"image\"], init_img_with_mask[\"mask\"]\n        mask = processing.create_binary_mask(mask)\n    elif mode == 3:  # inpaint sketch\n        image = inpaint_color_sketch\n        orig = inpaint_color_sketch_orig or inpaint_color_sketch\n        pred = np.any(np.array(image) != np.array(orig), axis=-1)\n        mask = Image.fromarray(pred.astype(np.uint8) * 255, \"L\")\n        mask = ImageEnhance.Brightness(mask).enhance(1 - mask_alpha / 100)\n        blur = ImageFilter.GaussianBlur(mask_blur)\n        image = Image.composite(image.filter(blur), orig, mask.filter(blur))\n    elif mode == 4:  # inpaint upload mask\n        image = init_img_inpaint\n        mask = init_mask_inpaint\n    else:\n        image = None\n        mask = None\n\n    image = images.fix_image(image)\n    mask = images.fix_image(mask)\n\n    if selected_scale_tab == 1 and not is_batch:\n        assert image, \"Can't scale by because no image is selected\"\n\n        width = int(image.width * scale_by)\n        height = int(image.height * scale_by)\n\n    assert 0. <= denoising_strength <= 1., 'can only work with strength in [0.0, 1.0]'\n\n    p = StableDiffusionProcessingImg2Img(\n        sd_model=shared.sd_model,\n        outpath_samples=opts.outdir_samples or opts.outdir_img2img_samples,\n        outpath_grids=opts.outdir_grids or opts.outdir_img2img_grids,\n        prompt=prompt,\n        negative_prompt=negative_prompt,\n        styles=prompt_styles,\n        batch_size=batch_size,\n        n_iter=n_iter,\n        cfg_scale=cfg_scale,\n        width=width,\n        height=height,\n        init_images=[image],\n        mask=mask,\n        mask_blur=mask_blur,\n        inpainting_fill=inpainting_fill,\n        resize_mode=resize_mode,\n        denoising_strength=denoising_strength,\n        image_cfg_scale=image_cfg_scale,\n        inpaint_full_res=inpaint_full_res,\n        inpaint_full_res_padding=inpaint_full_res_padding,\n        inpainting_mask_invert=inpainting_mask_invert,\n        override_settings=override_settings,\n    )\n\n    p.scripts = modules.scripts.scripts_img2img\n    p.script_args = args\n\n    p.user = request.username\n\n    if shared.opts.enable_console_prompts:\n        print(f\"\\nimg2img: {prompt}\", file=shared.progress_print_out)\n\n    with closing(p):\n        if is_batch:\n            assert not shared.cmd_opts.hide_ui_dir_config, \"Launched with --hide-ui-dir-config, batch img2img disabled\"\n            processed = process_batch(p, img2img_batch_input_dir, img2img_batch_output_dir, img2img_batch_inpaint_mask_dir, args, to_scale=selected_scale_tab == 1, scale_by=scale_by, use_png_info=img2img_batch_use_png_info, png_info_props=img2img_batch_png_info_props, png_info_dir=img2img_batch_png_info_dir)\n\n            if processed is None:\n                processed = Processed(p, [], p.seed, \"\")\n        else:\n            processed = modules.scripts.scripts_img2img.run(p, *args)\n            if processed is None:\n                processed = process_images(p)\n\n    shared.total_tqdm.clear()\n\n    generation_info_js = processed.js()\n    if opts.samples_log_stdout:\n        print(generation_info_js)\n\n    if opts.do_not_show_images:\n        processed.images = []\n\n    return processed.images, generation_info_js, plaintext_to_html(processed.info), plaintext_to_html(processed.comments, classname=\"comments\")\n", "modules/sd_hijack_open_clip.py": "import open_clip.tokenizer\nimport torch\n\nfrom modules import sd_hijack_clip, devices\nfrom modules.shared import opts\n\ntokenizer = open_clip.tokenizer._tokenizer\n\n\nclass FrozenOpenCLIPEmbedderWithCustomWords(sd_hijack_clip.FrozenCLIPEmbedderWithCustomWordsBase):\n    def __init__(self, wrapped, hijack):\n        super().__init__(wrapped, hijack)\n\n        self.comma_token = [v for k, v in tokenizer.encoder.items() if k == ',</w>'][0]\n        self.id_start = tokenizer.encoder[\"<start_of_text>\"]\n        self.id_end = tokenizer.encoder[\"<end_of_text>\"]\n        self.id_pad = 0\n\n    def tokenize(self, texts):\n        assert not opts.use_old_emphasis_implementation, 'Old emphasis implementation not supported for Open Clip'\n\n        tokenized = [tokenizer.encode(text) for text in texts]\n\n        return tokenized\n\n    def encode_with_transformers(self, tokens):\n        # set self.wrapped.layer_idx here according to opts.CLIP_stop_at_last_layers\n        z = self.wrapped.encode_with_transformer(tokens)\n\n        return z\n\n    def encode_embedding_init_text(self, init_text, nvpt):\n        ids = tokenizer.encode(init_text)\n        ids = torch.asarray([ids], device=devices.device, dtype=torch.int)\n        embedded = self.wrapped.model.token_embedding.wrapped(ids).squeeze(0)\n\n        return embedded\n\n\nclass FrozenOpenCLIPEmbedder2WithCustomWords(sd_hijack_clip.FrozenCLIPEmbedderWithCustomWordsBase):\n    def __init__(self, wrapped, hijack):\n        super().__init__(wrapped, hijack)\n\n        self.comma_token = [v for k, v in tokenizer.encoder.items() if k == ',</w>'][0]\n        self.id_start = tokenizer.encoder[\"<start_of_text>\"]\n        self.id_end = tokenizer.encoder[\"<end_of_text>\"]\n        self.id_pad = 0\n\n    def tokenize(self, texts):\n        assert not opts.use_old_emphasis_implementation, 'Old emphasis implementation not supported for Open Clip'\n\n        tokenized = [tokenizer.encode(text) for text in texts]\n\n        return tokenized\n\n    def encode_with_transformers(self, tokens):\n        d = self.wrapped.encode_with_transformer(tokens)\n        z = d[self.wrapped.layer]\n\n        pooled = d.get(\"pooled\")\n        if pooled is not None:\n            z.pooled = pooled\n\n        return z\n\n    def encode_embedding_init_text(self, init_text, nvpt):\n        ids = tokenizer.encode(init_text)\n        ids = torch.asarray([ids], device=devices.device, dtype=torch.int)\n        embedded = self.wrapped.model.token_embedding.wrapped(ids.to(self.wrapped.model.token_embedding.wrapped.weight.device)).squeeze(0)\n\n        return embedded\n", "modules/restart.py": "import os\nfrom pathlib import Path\n\nfrom modules.paths_internal import script_path\n\n\ndef is_restartable() -> bool:\n    \"\"\"\n    Return True if the webui is restartable (i.e. there is something watching to restart it with)\n    \"\"\"\n    return bool(os.environ.get('SD_WEBUI_RESTART'))\n\n\ndef restart_program() -> None:\n    \"\"\"creates file tmp/restart and immediately stops the process, which webui.bat/webui.sh interpret as a command to start webui again\"\"\"\n\n    tmpdir = Path(script_path) / \"tmp\"\n    tmpdir.mkdir(parents=True, exist_ok=True)\n    (tmpdir / \"restart\").touch()\n\n    stop_program()\n\n\ndef stop_program() -> None:\n    os._exit(0)\n", "modules/sd_samplers_kdiffusion.py": "import torch\nimport inspect\nimport k_diffusion.sampling\nfrom modules import sd_samplers_common, sd_samplers_extra, sd_samplers_cfg_denoiser, sd_schedulers\nfrom modules.sd_samplers_cfg_denoiser import CFGDenoiser  # noqa: F401\nfrom modules.script_callbacks import ExtraNoiseParams, extra_noise_callback\n\nfrom modules.shared import opts\nimport modules.shared as shared\n\nsamplers_k_diffusion = [\n    ('DPM++ 2M', 'sample_dpmpp_2m', ['k_dpmpp_2m'], {'scheduler': 'karras'}),\n    ('DPM++ SDE', 'sample_dpmpp_sde', ['k_dpmpp_sde'], {'scheduler': 'karras', \"second_order\": True, \"brownian_noise\": True}),\n    ('DPM++ 2M SDE', 'sample_dpmpp_2m_sde', ['k_dpmpp_2m_sde'], {'scheduler': 'exponential', \"brownian_noise\": True}),\n    ('DPM++ 2M SDE Heun', 'sample_dpmpp_2m_sde', ['k_dpmpp_2m_sde_heun'], {'scheduler': 'exponential', \"brownian_noise\": True, \"solver_type\": \"heun\"}),\n    ('DPM++ 2S a', 'sample_dpmpp_2s_ancestral', ['k_dpmpp_2s_a'], {'scheduler': 'karras', \"uses_ensd\": True, \"second_order\": True}),\n    ('DPM++ 3M SDE', 'sample_dpmpp_3m_sde', ['k_dpmpp_3m_sde'], {'scheduler': 'exponential', 'discard_next_to_last_sigma': True, \"brownian_noise\": True}),\n    ('Euler a', 'sample_euler_ancestral', ['k_euler_a', 'k_euler_ancestral'], {\"uses_ensd\": True}),\n    ('Euler', 'sample_euler', ['k_euler'], {}),\n    ('LMS', 'sample_lms', ['k_lms'], {}),\n    ('Heun', 'sample_heun', ['k_heun'], {\"second_order\": True}),\n    ('DPM2', 'sample_dpm_2', ['k_dpm_2'], {'scheduler': 'karras', 'discard_next_to_last_sigma': True, \"second_order\": True}),\n    ('DPM2 a', 'sample_dpm_2_ancestral', ['k_dpm_2_a'], {'scheduler': 'karras', 'discard_next_to_last_sigma': True, \"uses_ensd\": True, \"second_order\": True}),\n    ('DPM fast', 'sample_dpm_fast', ['k_dpm_fast'], {\"uses_ensd\": True}),\n    ('DPM adaptive', 'sample_dpm_adaptive', ['k_dpm_ad'], {\"uses_ensd\": True}),\n    ('Restart', sd_samplers_extra.restart_sampler, ['restart'], {'scheduler': 'karras', \"second_order\": True}),\n]\n\n\nsamplers_data_k_diffusion = [\n    sd_samplers_common.SamplerData(label, lambda model, funcname=funcname: KDiffusionSampler(funcname, model), aliases, options)\n    for label, funcname, aliases, options in samplers_k_diffusion\n    if callable(funcname) or hasattr(k_diffusion.sampling, funcname)\n]\n\nsampler_extra_params = {\n    'sample_euler': ['s_churn', 's_tmin', 's_tmax', 's_noise'],\n    'sample_heun': ['s_churn', 's_tmin', 's_tmax', 's_noise'],\n    'sample_dpm_2': ['s_churn', 's_tmin', 's_tmax', 's_noise'],\n    'sample_dpm_fast': ['s_noise'],\n    'sample_dpm_2_ancestral': ['s_noise'],\n    'sample_dpmpp_2s_ancestral': ['s_noise'],\n    'sample_dpmpp_sde': ['s_noise'],\n    'sample_dpmpp_2m_sde': ['s_noise'],\n    'sample_dpmpp_3m_sde': ['s_noise'],\n}\n\nk_diffusion_samplers_map = {x.name: x for x in samplers_data_k_diffusion}\nk_diffusion_scheduler = {x.name: x.function for x in sd_schedulers.schedulers}\n\n\nclass CFGDenoiserKDiffusion(sd_samplers_cfg_denoiser.CFGDenoiser):\n    @property\n    def inner_model(self):\n        if self.model_wrap is None:\n            denoiser = k_diffusion.external.CompVisVDenoiser if shared.sd_model.parameterization == \"v\" else k_diffusion.external.CompVisDenoiser\n            self.model_wrap = denoiser(shared.sd_model, quantize=shared.opts.enable_quantization)\n\n        return self.model_wrap\n\n\nclass KDiffusionSampler(sd_samplers_common.Sampler):\n    def __init__(self, funcname, sd_model, options=None):\n        super().__init__(funcname)\n\n        self.extra_params = sampler_extra_params.get(funcname, [])\n\n        self.options = options or {}\n        self.func = funcname if callable(funcname) else getattr(k_diffusion.sampling, self.funcname)\n\n        self.model_wrap_cfg = CFGDenoiserKDiffusion(self)\n        self.model_wrap = self.model_wrap_cfg.inner_model\n\n    def get_sigmas(self, p, steps):\n        discard_next_to_last_sigma = self.config is not None and self.config.options.get('discard_next_to_last_sigma', False)\n        if opts.always_discard_next_to_last_sigma and not discard_next_to_last_sigma:\n            discard_next_to_last_sigma = True\n            p.extra_generation_params[\"Discard penultimate sigma\"] = True\n\n        steps += 1 if discard_next_to_last_sigma else 0\n\n        scheduler_name = (p.hr_scheduler if p.is_hr_pass else p.scheduler) or 'Automatic'\n        if scheduler_name == 'Automatic':\n            scheduler_name = self.config.options.get('scheduler', None)\n\n        scheduler = sd_schedulers.schedulers_map.get(scheduler_name)\n\n        m_sigma_min, m_sigma_max = self.model_wrap.sigmas[0].item(), self.model_wrap.sigmas[-1].item()\n        sigma_min, sigma_max = (0.1, 10) if opts.use_old_karras_scheduler_sigmas else (m_sigma_min, m_sigma_max)\n\n        if p.sampler_noise_scheduler_override:\n            sigmas = p.sampler_noise_scheduler_override(steps)\n        elif scheduler is None or scheduler.function is None:\n            sigmas = self.model_wrap.get_sigmas(steps)\n        else:\n            sigmas_kwargs = {'sigma_min': sigma_min, 'sigma_max': sigma_max}\n\n            if scheduler.label != 'Automatic' and not p.is_hr_pass:\n                p.extra_generation_params[\"Schedule type\"] = scheduler.label\n            elif scheduler.label != p.extra_generation_params.get(\"Schedule type\"):\n                p.extra_generation_params[\"Hires schedule type\"] = scheduler.label\n\n            if opts.sigma_min != 0 and opts.sigma_min != m_sigma_min:\n                sigmas_kwargs['sigma_min'] = opts.sigma_min\n                p.extra_generation_params[\"Schedule min sigma\"] = opts.sigma_min\n\n            if opts.sigma_max != 0 and opts.sigma_max != m_sigma_max:\n                sigmas_kwargs['sigma_max'] = opts.sigma_max\n                p.extra_generation_params[\"Schedule max sigma\"] = opts.sigma_max\n\n            if scheduler.default_rho != -1 and opts.rho != 0 and opts.rho != scheduler.default_rho:\n                sigmas_kwargs['rho'] = opts.rho\n                p.extra_generation_params[\"Schedule rho\"] = opts.rho\n\n            if scheduler.need_inner_model:\n                sigmas_kwargs['inner_model'] = self.model_wrap\n\n            sigmas = scheduler.function(n=steps, **sigmas_kwargs, device=shared.device)\n\n        if discard_next_to_last_sigma:\n            sigmas = torch.cat([sigmas[:-2], sigmas[-1:]])\n\n        return sigmas\n\n    def sample_img2img(self, p, x, noise, conditioning, unconditional_conditioning, steps=None, image_conditioning=None):\n        steps, t_enc = sd_samplers_common.setup_img2img_steps(p, steps)\n\n        sigmas = self.get_sigmas(p, steps)\n        sigma_sched = sigmas[steps - t_enc - 1:]\n\n        xi = x + noise * sigma_sched[0]\n\n        if opts.img2img_extra_noise > 0:\n            p.extra_generation_params[\"Extra noise\"] = opts.img2img_extra_noise\n            extra_noise_params = ExtraNoiseParams(noise, x, xi)\n            extra_noise_callback(extra_noise_params)\n            noise = extra_noise_params.noise\n            xi += noise * opts.img2img_extra_noise\n\n        extra_params_kwargs = self.initialize(p)\n        parameters = inspect.signature(self.func).parameters\n\n        if 'sigma_min' in parameters:\n            ## last sigma is zero which isn't allowed by DPM Fast & Adaptive so taking value before last\n            extra_params_kwargs['sigma_min'] = sigma_sched[-2]\n        if 'sigma_max' in parameters:\n            extra_params_kwargs['sigma_max'] = sigma_sched[0]\n        if 'n' in parameters:\n            extra_params_kwargs['n'] = len(sigma_sched) - 1\n        if 'sigma_sched' in parameters:\n            extra_params_kwargs['sigma_sched'] = sigma_sched\n        if 'sigmas' in parameters:\n            extra_params_kwargs['sigmas'] = sigma_sched\n\n        if self.config.options.get('brownian_noise', False):\n            noise_sampler = self.create_noise_sampler(x, sigmas, p)\n            extra_params_kwargs['noise_sampler'] = noise_sampler\n\n        if self.config.options.get('solver_type', None) == 'heun':\n            extra_params_kwargs['solver_type'] = 'heun'\n\n        self.model_wrap_cfg.init_latent = x\n        self.last_latent = x\n        self.sampler_extra_args = {\n            'cond': conditioning,\n            'image_cond': image_conditioning,\n            'uncond': unconditional_conditioning,\n            'cond_scale': p.cfg_scale,\n            's_min_uncond': self.s_min_uncond\n        }\n\n        samples = self.launch_sampling(t_enc + 1, lambda: self.func(self.model_wrap_cfg, xi, extra_args=self.sampler_extra_args, disable=False, callback=self.callback_state, **extra_params_kwargs))\n\n        self.add_infotext(p)\n\n        return samples\n\n    def sample(self, p, x, conditioning, unconditional_conditioning, steps=None, image_conditioning=None):\n        steps = steps or p.steps\n\n        sigmas = self.get_sigmas(p, steps)\n\n        if opts.sgm_noise_multiplier:\n            p.extra_generation_params[\"SGM noise multiplier\"] = True\n            x = x * torch.sqrt(1.0 + sigmas[0] ** 2.0)\n        else:\n            x = x * sigmas[0]\n\n        extra_params_kwargs = self.initialize(p)\n        parameters = inspect.signature(self.func).parameters\n\n        if 'n' in parameters:\n            extra_params_kwargs['n'] = steps\n\n        if 'sigma_min' in parameters:\n            extra_params_kwargs['sigma_min'] = self.model_wrap.sigmas[0].item()\n            extra_params_kwargs['sigma_max'] = self.model_wrap.sigmas[-1].item()\n\n        if 'sigmas' in parameters:\n            extra_params_kwargs['sigmas'] = sigmas\n\n        if self.config.options.get('brownian_noise', False):\n            noise_sampler = self.create_noise_sampler(x, sigmas, p)\n            extra_params_kwargs['noise_sampler'] = noise_sampler\n\n        if self.config.options.get('solver_type', None) == 'heun':\n            extra_params_kwargs['solver_type'] = 'heun'\n\n        self.last_latent = x\n        self.sampler_extra_args = {\n            'cond': conditioning,\n            'image_cond': image_conditioning,\n            'uncond': unconditional_conditioning,\n            'cond_scale': p.cfg_scale,\n            's_min_uncond': self.s_min_uncond\n        }\n\n        samples = self.launch_sampling(steps, lambda: self.func(self.model_wrap_cfg, x, extra_args=self.sampler_extra_args, disable=False, callback=self.callback_state, **extra_params_kwargs))\n\n        self.add_infotext(p)\n\n        return samples\n\n\n", "modules/txt2img.py": "import json\nfrom contextlib import closing\n\nimport modules.scripts\nfrom modules import processing, infotext_utils\nfrom modules.infotext_utils import create_override_settings_dict, parse_generation_parameters\nfrom modules.shared import opts\nimport modules.shared as shared\nfrom modules.ui import plaintext_to_html\nfrom PIL import Image\nimport gradio as gr\n\n\ndef txt2img_create_processing(id_task: str, request: gr.Request, prompt: str, negative_prompt: str, prompt_styles, n_iter: int, batch_size: int, cfg_scale: float, height: int, width: int, enable_hr: bool, denoising_strength: float, hr_scale: float, hr_upscaler: str, hr_second_pass_steps: int, hr_resize_x: int, hr_resize_y: int, hr_checkpoint_name: str, hr_sampler_name: str, hr_scheduler: str, hr_prompt: str, hr_negative_prompt, override_settings_texts, *args, force_enable_hr=False):\n    override_settings = create_override_settings_dict(override_settings_texts)\n\n    if force_enable_hr:\n        enable_hr = True\n\n    p = processing.StableDiffusionProcessingTxt2Img(\n        sd_model=shared.sd_model,\n        outpath_samples=opts.outdir_samples or opts.outdir_txt2img_samples,\n        outpath_grids=opts.outdir_grids or opts.outdir_txt2img_grids,\n        prompt=prompt,\n        styles=prompt_styles,\n        negative_prompt=negative_prompt,\n        batch_size=batch_size,\n        n_iter=n_iter,\n        cfg_scale=cfg_scale,\n        width=width,\n        height=height,\n        enable_hr=enable_hr,\n        denoising_strength=denoising_strength,\n        hr_scale=hr_scale,\n        hr_upscaler=hr_upscaler,\n        hr_second_pass_steps=hr_second_pass_steps,\n        hr_resize_x=hr_resize_x,\n        hr_resize_y=hr_resize_y,\n        hr_checkpoint_name=None if hr_checkpoint_name == 'Use same checkpoint' else hr_checkpoint_name,\n        hr_sampler_name=None if hr_sampler_name == 'Use same sampler' else hr_sampler_name,\n        hr_scheduler=None if hr_scheduler == 'Use same scheduler' else hr_scheduler,\n        hr_prompt=hr_prompt,\n        hr_negative_prompt=hr_negative_prompt,\n        override_settings=override_settings,\n    )\n\n    p.scripts = modules.scripts.scripts_txt2img\n    p.script_args = args\n\n    p.user = request.username\n\n    if shared.opts.enable_console_prompts:\n        print(f\"\\ntxt2img: {prompt}\", file=shared.progress_print_out)\n\n    return p\n\n\ndef txt2img_upscale(id_task: str, request: gr.Request, gallery, gallery_index, generation_info, *args):\n    assert len(gallery) > 0, 'No image to upscale'\n    assert 0 <= gallery_index < len(gallery), f'Bad image index: {gallery_index}'\n\n    p = txt2img_create_processing(id_task, request, *args, force_enable_hr=True)\n    p.batch_size = 1\n    p.n_iter = 1\n    # txt2img_upscale attribute that signifies this is called by txt2img_upscale\n    p.txt2img_upscale = True\n\n    geninfo = json.loads(generation_info)\n\n    image_info = gallery[gallery_index] if 0 <= gallery_index < len(gallery) else gallery[0]\n    p.firstpass_image = infotext_utils.image_from_url_text(image_info)\n\n    parameters = parse_generation_parameters(geninfo.get('infotexts')[gallery_index], [])\n    p.seed = parameters.get('Seed', -1)\n    p.subseed = parameters.get('Variation seed', -1)\n\n    p.override_settings['save_images_before_highres_fix'] = False\n\n    with closing(p):\n        processed = modules.scripts.scripts_txt2img.run(p, *p.script_args)\n\n        if processed is None:\n            processed = processing.process_images(p)\n\n    shared.total_tqdm.clear()\n\n    new_gallery = []\n    for i, image in enumerate(gallery):\n        if i == gallery_index:\n            geninfo[\"infotexts\"][gallery_index: gallery_index+1] = processed.infotexts\n            new_gallery.extend(processed.images)\n        else:\n            fake_image = Image.new(mode=\"RGB\", size=(1, 1))\n            fake_image.already_saved_as = image[\"name\"].rsplit('?', 1)[0]\n            new_gallery.append(fake_image)\n\n    geninfo[\"infotexts\"][gallery_index] = processed.info\n\n    return new_gallery, json.dumps(geninfo), plaintext_to_html(processed.info), plaintext_to_html(processed.comments, classname=\"comments\")\n\n\ndef txt2img(id_task: str, request: gr.Request, *args):\n    p = txt2img_create_processing(id_task, request, *args)\n\n    with closing(p):\n        processed = modules.scripts.scripts_txt2img.run(p, *p.script_args)\n\n        if processed is None:\n            processed = processing.process_images(p)\n\n    shared.total_tqdm.clear()\n\n    generation_info_js = processed.js()\n    if opts.samples_log_stdout:\n        print(generation_info_js)\n\n    if opts.do_not_show_images:\n        processed.images = []\n\n    return processed.images, generation_info_js, plaintext_to_html(processed.info), plaintext_to_html(processed.comments, classname=\"comments\")\n", "modules/options.py": "import os\nimport json\nimport sys\nfrom dataclasses import dataclass\n\nimport gradio as gr\n\nfrom modules import errors\nfrom modules.shared_cmd_options import cmd_opts\nfrom modules.paths_internal import script_path\n\n\nclass OptionInfo:\n    def __init__(self, default=None, label=\"\", component=None, component_args=None, onchange=None, section=None, refresh=None, comment_before='', comment_after='', infotext=None, restrict_api=False, category_id=None):\n        self.default = default\n        self.label = label\n        self.component = component\n        self.component_args = component_args\n        self.onchange = onchange\n        self.section = section\n        self.category_id = category_id\n        self.refresh = refresh\n        self.do_not_save = False\n\n        self.comment_before = comment_before\n        \"\"\"HTML text that will be added after label in UI\"\"\"\n\n        self.comment_after = comment_after\n        \"\"\"HTML text that will be added before label in UI\"\"\"\n\n        self.infotext = infotext\n\n        self.restrict_api = restrict_api\n        \"\"\"If True, the setting will not be accessible via API\"\"\"\n\n    def link(self, label, url):\n        self.comment_before += f\"[<a href='{url}' target='_blank'>{label}</a>]\"\n        return self\n\n    def js(self, label, js_func):\n        self.comment_before += f\"[<a onclick='{js_func}(); return false'>{label}</a>]\"\n        return self\n\n    def info(self, info):\n        self.comment_after += f\"<span class='info'>({info})</span>\"\n        return self\n\n    def html(self, html):\n        self.comment_after += html\n        return self\n\n    def needs_restart(self):\n        self.comment_after += \" <span class='info'>(requires restart)</span>\"\n        return self\n\n    def needs_reload_ui(self):\n        self.comment_after += \" <span class='info'>(requires Reload UI)</span>\"\n        return self\n\n\nclass OptionHTML(OptionInfo):\n    def __init__(self, text):\n        super().__init__(str(text).strip(), label='', component=lambda **kwargs: gr.HTML(elem_classes=\"settings-info\", **kwargs))\n\n        self.do_not_save = True\n\n\ndef options_section(section_identifier, options_dict):\n    for v in options_dict.values():\n        if len(section_identifier) == 2:\n            v.section = section_identifier\n        elif len(section_identifier) == 3:\n            v.section = section_identifier[0:2]\n            v.category_id = section_identifier[2]\n\n    return options_dict\n\n\noptions_builtin_fields = {\"data_labels\", \"data\", \"restricted_opts\", \"typemap\"}\n\n\nclass Options:\n    typemap = {int: float}\n\n    def __init__(self, data_labels: dict[str, OptionInfo], restricted_opts):\n        self.data_labels = data_labels\n        self.data = {k: v.default for k, v in self.data_labels.items() if not v.do_not_save}\n        self.restricted_opts = restricted_opts\n\n    def __setattr__(self, key, value):\n        if key in options_builtin_fields:\n            return super(Options, self).__setattr__(key, value)\n\n        if self.data is not None:\n            if key in self.data or key in self.data_labels:\n\n                # Check that settings aren't globally frozen\n                assert not cmd_opts.freeze_settings, \"changing settings is disabled\"\n\n                # Get the info related to the setting being changed\n                info = self.data_labels.get(key, None)\n                if info.do_not_save:\n                    return\n\n                # Restrict component arguments\n                comp_args = info.component_args if info else None\n                if isinstance(comp_args, dict) and comp_args.get('visible', True) is False:\n                    raise RuntimeError(f\"not possible to set '{key}' because it is restricted\")\n\n                # Check that this section isn't frozen\n                if cmd_opts.freeze_settings_in_sections is not None:\n                    frozen_sections = list(map(str.strip, cmd_opts.freeze_settings_in_sections.split(','))) # Trim whitespace from section names\n                    section_key = info.section[0]\n                    section_name = info.section[1]\n                    assert section_key not in frozen_sections, f\"not possible to set '{key}' because settings in section '{section_name}' ({section_key}) are frozen with --freeze-settings-in-sections\"\n\n                # Check that this section of the settings isn't frozen\n                if cmd_opts.freeze_specific_settings is not None:\n                    frozen_keys = list(map(str.strip, cmd_opts.freeze_specific_settings.split(','))) # Trim whitespace from setting keys\n                    assert key not in frozen_keys, f\"not possible to set '{key}' because this setting is frozen with --freeze-specific-settings\"\n\n                # Check shorthand option which disables editing options in \"saving-paths\"\n                if cmd_opts.hide_ui_dir_config and key in self.restricted_opts:\n                    raise RuntimeError(f\"not possible to set '{key}' because it is restricted with --hide_ui_dir_config\")\n\n                self.data[key] = value\n                return\n\n        return super(Options, self).__setattr__(key, value)\n\n    def __getattr__(self, item):\n        if item in options_builtin_fields:\n            return super(Options, self).__getattribute__(item)\n\n        if self.data is not None:\n            if item in self.data:\n                return self.data[item]\n\n        if item in self.data_labels:\n            return self.data_labels[item].default\n\n        return super(Options, self).__getattribute__(item)\n\n    def set(self, key, value, is_api=False, run_callbacks=True):\n        \"\"\"sets an option and calls its onchange callback, returning True if the option changed and False otherwise\"\"\"\n\n        oldval = self.data.get(key, None)\n        if oldval == value:\n            return False\n\n        option = self.data_labels[key]\n        if option.do_not_save:\n            return False\n\n        if is_api and option.restrict_api:\n            return False\n\n        try:\n            setattr(self, key, value)\n        except RuntimeError:\n            return False\n\n        if run_callbacks and option.onchange is not None:\n            try:\n                option.onchange()\n            except Exception as e:\n                errors.display(e, f\"changing setting {key} to {value}\")\n                setattr(self, key, oldval)\n                return False\n\n        return True\n\n    def get_default(self, key):\n        \"\"\"returns the default value for the key\"\"\"\n\n        data_label = self.data_labels.get(key)\n        if data_label is None:\n            return None\n\n        return data_label.default\n\n    def save(self, filename):\n        assert not cmd_opts.freeze_settings, \"saving settings is disabled\"\n\n        with open(filename, \"w\", encoding=\"utf8\") as file:\n            json.dump(self.data, file, indent=4, ensure_ascii=False)\n\n    def same_type(self, x, y):\n        if x is None or y is None:\n            return True\n\n        type_x = self.typemap.get(type(x), type(x))\n        type_y = self.typemap.get(type(y), type(y))\n\n        return type_x == type_y\n\n    def load(self, filename):\n        try:\n            with open(filename, \"r\", encoding=\"utf8\") as file:\n                self.data = json.load(file)\n        except FileNotFoundError:\n            self.data = {}\n        except Exception:\n            errors.report(f'\\nCould not load settings\\nThe config file \"{filename}\" is likely corrupted\\nIt has been moved to the \"tmp/config.json\"\\nReverting config to default\\n\\n''', exc_info=True)\n            os.replace(filename, os.path.join(script_path, \"tmp\", \"config.json\"))\n            self.data = {}\n        # 1.6.0 VAE defaults\n        if self.data.get('sd_vae_as_default') is not None and self.data.get('sd_vae_overrides_per_model_preferences') is None:\n            self.data['sd_vae_overrides_per_model_preferences'] = not self.data.get('sd_vae_as_default')\n\n        # 1.1.1 quicksettings list migration\n        if self.data.get('quicksettings') is not None and self.data.get('quicksettings_list') is None:\n            self.data['quicksettings_list'] = [i.strip() for i in self.data.get('quicksettings').split(',')]\n\n        # 1.4.0 ui_reorder\n        if isinstance(self.data.get('ui_reorder'), str) and self.data.get('ui_reorder') and \"ui_reorder_list\" not in self.data:\n            self.data['ui_reorder_list'] = [i.strip() for i in self.data.get('ui_reorder').split(',')]\n\n        bad_settings = 0\n        for k, v in self.data.items():\n            info = self.data_labels.get(k, None)\n            if info is not None and not self.same_type(info.default, v):\n                print(f\"Warning: bad setting value: {k}: {v} ({type(v).__name__}; expected {type(info.default).__name__})\", file=sys.stderr)\n                bad_settings += 1\n\n        if bad_settings > 0:\n            print(f\"The program is likely to not work with bad settings.\\nSettings file: {filename}\\nEither fix the file, or delete it and restart.\", file=sys.stderr)\n\n    def onchange(self, key, func, call=True):\n        item = self.data_labels.get(key)\n        item.onchange = func\n\n        if call:\n            func()\n\n    def dumpjson(self):\n        d = {k: self.data.get(k, v.default) for k, v in self.data_labels.items()}\n        d[\"_comments_before\"] = {k: v.comment_before for k, v in self.data_labels.items() if v.comment_before is not None}\n        d[\"_comments_after\"] = {k: v.comment_after for k, v in self.data_labels.items() if v.comment_after is not None}\n\n        item_categories = {}\n        for item in self.data_labels.values():\n            if item.section[0] is None:\n                continue\n\n            category = categories.mapping.get(item.category_id)\n            category = \"Uncategorized\" if category is None else category.label\n            if category not in item_categories:\n                item_categories[category] = item.section[1]\n\n        # _categories is a list of pairs: [section, category]. Each section (a setting page) will get a special heading above it with the category as text.\n        d[\"_categories\"] = [[v, k] for k, v in item_categories.items()] + [[\"Defaults\", \"Other\"]]\n\n        return json.dumps(d)\n\n    def add_option(self, key, info):\n        self.data_labels[key] = info\n        if key not in self.data and not info.do_not_save:\n            self.data[key] = info.default\n\n    def reorder(self):\n        \"\"\"Reorder settings so that:\n            - all items related to section always go together\n            - all sections belonging to a category go together\n            - sections inside a category are ordered alphabetically\n            - categories are ordered by creation order\n\n        Category is a superset of sections: for category \"postprocessing\" there could be multiple sections: \"face restoration\", \"upscaling\".\n\n        This function also changes items' category_id so that all items belonging to a section have the same category_id.\n        \"\"\"\n\n        category_ids = {}\n        section_categories = {}\n\n        settings_items = self.data_labels.items()\n        for _, item in settings_items:\n            if item.section not in section_categories:\n                section_categories[item.section] = item.category_id\n\n        for _, item in settings_items:\n            item.category_id = section_categories.get(item.section)\n\n        for category_id in categories.mapping:\n            if category_id not in category_ids:\n                category_ids[category_id] = len(category_ids)\n\n        def sort_key(x):\n            item: OptionInfo = x[1]\n            category_order = category_ids.get(item.category_id, len(category_ids))\n            section_order = item.section[1]\n\n            return category_order, section_order\n\n        self.data_labels = dict(sorted(settings_items, key=sort_key))\n\n    def cast_value(self, key, value):\n        \"\"\"casts an arbitrary to the same type as this setting's value with key\n        Example: cast_value(\"eta_noise_seed_delta\", \"12\") -> returns 12 (an int rather than str)\n        \"\"\"\n\n        if value is None:\n            return None\n\n        default_value = self.data_labels[key].default\n        if default_value is None:\n            default_value = getattr(self, key, None)\n        if default_value is None:\n            return None\n\n        expected_type = type(default_value)\n        if expected_type == bool and value == \"False\":\n            value = False\n        else:\n            value = expected_type(value)\n\n        return value\n\n\n@dataclass\nclass OptionsCategory:\n    id: str\n    label: str\n\nclass OptionsCategories:\n    def __init__(self):\n        self.mapping = {}\n\n    def register_category(self, category_id, label):\n        if category_id in self.mapping:\n            return category_id\n\n        self.mapping[category_id] = OptionsCategory(category_id, label)\n\n\ncategories = OptionsCategories()\n", "modules/paths_internal.py": "\"\"\"this module defines internal paths used by program and is safe to import before dependencies are installed in launch.py\"\"\"\n\nimport argparse\nimport os\nimport sys\nimport shlex\nfrom pathlib import Path\n\n\nnormalized_filepath = lambda filepath: str(Path(filepath).absolute())\n\ncommandline_args = os.environ.get('COMMANDLINE_ARGS', \"\")\nsys.argv += shlex.split(commandline_args)\n\ncwd = os.getcwd()\nmodules_path = os.path.dirname(os.path.realpath(__file__))\nscript_path = os.path.dirname(modules_path)\n\nsd_configs_path = os.path.join(script_path, \"configs\")\nsd_default_config = os.path.join(sd_configs_path, \"v1-inference.yaml\")\nsd_model_file = os.path.join(script_path, 'model.ckpt')\ndefault_sd_model_file = sd_model_file\n\n# Parse the --data-dir flag first so we can use it as a base for our other argument default values\nparser_pre = argparse.ArgumentParser(add_help=False)\nparser_pre.add_argument(\"--data-dir\", type=str, default=os.path.dirname(modules_path), help=\"base path where all user data is stored\", )\ncmd_opts_pre = parser_pre.parse_known_args()[0]\n\ndata_path = cmd_opts_pre.data_dir\n\nmodels_path = os.path.join(data_path, \"models\")\nextensions_dir = os.path.join(data_path, \"extensions\")\nextensions_builtin_dir = os.path.join(script_path, \"extensions-builtin\")\nconfig_states_dir = os.path.join(script_path, \"config_states\")\ndefault_output_dir = os.path.join(data_path, \"outputs\")\n\nroboto_ttf_file = os.path.join(modules_path, 'Roboto-Regular.ttf')\n", "modules/sd_unet.py": "import torch.nn\n\nfrom modules import script_callbacks, shared, devices\n\nunet_options = []\ncurrent_unet_option = None\ncurrent_unet = None\noriginal_forward = None  # not used, only left temporarily for compatibility\n\ndef list_unets():\n    new_unets = script_callbacks.list_unets_callback()\n\n    unet_options.clear()\n    unet_options.extend(new_unets)\n\n\ndef get_unet_option(option=None):\n    option = option or shared.opts.sd_unet\n\n    if option == \"None\":\n        return None\n\n    if option == \"Automatic\":\n        name = shared.sd_model.sd_checkpoint_info.model_name\n\n        options = [x for x in unet_options if x.model_name == name]\n\n        option = options[0].label if options else \"None\"\n\n    return next(iter([x for x in unet_options if x.label == option]), None)\n\n\ndef apply_unet(option=None):\n    global current_unet_option\n    global current_unet\n\n    new_option = get_unet_option(option)\n    if new_option == current_unet_option:\n        return\n\n    if current_unet is not None:\n        print(f\"Dectivating unet: {current_unet.option.label}\")\n        current_unet.deactivate()\n\n    current_unet_option = new_option\n    if current_unet_option is None:\n        current_unet = None\n\n        if not shared.sd_model.lowvram:\n            shared.sd_model.model.diffusion_model.to(devices.device)\n\n        return\n\n    shared.sd_model.model.diffusion_model.to(devices.cpu)\n    devices.torch_gc()\n\n    current_unet = current_unet_option.create_unet()\n    current_unet.option = current_unet_option\n    print(f\"Activating unet: {current_unet.option.label}\")\n    current_unet.activate()\n\n\nclass SdUnetOption:\n    model_name = None\n    \"\"\"name of related checkpoint - this option will be selected automatically for unet if the name of checkpoint matches this\"\"\"\n\n    label = None\n    \"\"\"name of the unet in UI\"\"\"\n\n    def create_unet(self):\n        \"\"\"returns SdUnet object to be used as a Unet instead of built-in unet when making pictures\"\"\"\n        raise NotImplementedError()\n\n\nclass SdUnet(torch.nn.Module):\n    def forward(self, x, timesteps, context, *args, **kwargs):\n        raise NotImplementedError()\n\n    def activate(self):\n        pass\n\n    def deactivate(self):\n        pass\n\n\ndef create_unet_forward(original_forward):\n    def UNetModel_forward(self, x, timesteps=None, context=None, *args, **kwargs):\n        if current_unet is not None:\n            return current_unet.forward(x, timesteps, context, *args, **kwargs)\n\n        return original_forward(self, x, timesteps, context, *args, **kwargs)\n\n    return UNetModel_forward\n\n", "modules/ui_toprow.py": "import gradio as gr\n\nfrom modules import shared, ui_prompt_styles\nimport modules.images\n\nfrom modules.ui_components import ToolButton\n\n\nclass Toprow:\n    \"\"\"Creates a top row UI with prompts, generate button, styles, extra little buttons for things, and enables some functionality related to their operation\"\"\"\n\n    prompt = None\n    prompt_img = None\n    negative_prompt = None\n\n    button_interrogate = None\n    button_deepbooru = None\n\n    interrupt = None\n    interrupting = None\n    skip = None\n    submit = None\n\n    paste = None\n    clear_prompt_button = None\n    apply_styles = None\n    restore_progress_button = None\n\n    token_counter = None\n    token_button = None\n    negative_token_counter = None\n    negative_token_button = None\n\n    ui_styles = None\n\n    submit_box = None\n\n    def __init__(self, is_img2img, is_compact=False, id_part=None):\n        if id_part is None:\n            id_part = \"img2img\" if is_img2img else \"txt2img\"\n\n        self.id_part = id_part\n        self.is_img2img = is_img2img\n        self.is_compact = is_compact\n\n        if not is_compact:\n            with gr.Row(elem_id=f\"{id_part}_toprow\", variant=\"compact\"):\n                self.create_classic_toprow()\n        else:\n            self.create_submit_box()\n\n    def create_classic_toprow(self):\n        self.create_prompts()\n\n        with gr.Column(scale=1, elem_id=f\"{self.id_part}_actions_column\"):\n            self.create_submit_box()\n\n            self.create_tools_row()\n\n            self.create_styles_ui()\n\n    def create_inline_toprow_prompts(self):\n        if not self.is_compact:\n            return\n\n        self.create_prompts()\n\n        with gr.Row(elem_classes=[\"toprow-compact-stylerow\"]):\n            with gr.Column(elem_classes=[\"toprow-compact-tools\"]):\n                self.create_tools_row()\n            with gr.Column():\n                self.create_styles_ui()\n\n    def create_inline_toprow_image(self):\n        if not self.is_compact:\n            return\n\n        self.submit_box.render()\n\n    def create_prompts(self):\n        with gr.Column(elem_id=f\"{self.id_part}_prompt_container\", elem_classes=[\"prompt-container-compact\"] if self.is_compact else [], scale=6):\n            with gr.Row(elem_id=f\"{self.id_part}_prompt_row\", elem_classes=[\"prompt-row\"]):\n                self.prompt = gr.Textbox(label=\"Prompt\", elem_id=f\"{self.id_part}_prompt\", show_label=False, lines=3, placeholder=\"Prompt\\n(Press Ctrl+Enter to generate, Alt+Enter to skip, Esc to interrupt)\", elem_classes=[\"prompt\"])\n                self.prompt_img = gr.File(label=\"\", elem_id=f\"{self.id_part}_prompt_image\", file_count=\"single\", type=\"binary\", visible=False)\n\n            with gr.Row(elem_id=f\"{self.id_part}_neg_prompt_row\", elem_classes=[\"prompt-row\"]):\n                self.negative_prompt = gr.Textbox(label=\"Negative prompt\", elem_id=f\"{self.id_part}_neg_prompt\", show_label=False, lines=3, placeholder=\"Negative prompt\\n(Press Ctrl+Enter to generate, Alt+Enter to skip, Esc to interrupt)\", elem_classes=[\"prompt\"])\n\n        self.prompt_img.change(\n            fn=modules.images.image_data,\n            inputs=[self.prompt_img],\n            outputs=[self.prompt, self.prompt_img],\n            show_progress=False,\n        )\n\n    def create_submit_box(self):\n        with gr.Row(elem_id=f\"{self.id_part}_generate_box\", elem_classes=[\"generate-box\"] + ([\"generate-box-compact\"] if self.is_compact else []), render=not self.is_compact) as submit_box:\n            self.submit_box = submit_box\n\n            self.interrupt = gr.Button('Interrupt', elem_id=f\"{self.id_part}_interrupt\", elem_classes=\"generate-box-interrupt\", tooltip=\"End generation immediately or after completing current batch\")\n            self.skip = gr.Button('Skip', elem_id=f\"{self.id_part}_skip\", elem_classes=\"generate-box-skip\", tooltip=\"Stop generation of current batch and continues onto next batch\")\n            self.interrupting = gr.Button('Interrupting...', elem_id=f\"{self.id_part}_interrupting\", elem_classes=\"generate-box-interrupting\", tooltip=\"Interrupting generation...\")\n            self.submit = gr.Button('Generate', elem_id=f\"{self.id_part}_generate\", variant='primary', tooltip=\"Right click generate forever menu\")\n\n            def interrupt_function():\n                if not shared.state.stopping_generation and shared.state.job_count > 1 and shared.opts.interrupt_after_current:\n                    shared.state.stop_generating()\n                    gr.Info(\"Generation will stop after finishing this image, click again to stop immediately.\")\n                else:\n                    shared.state.interrupt()\n\n            self.skip.click(fn=shared.state.skip)\n            self.interrupt.click(fn=interrupt_function, _js='function(){ showSubmitInterruptingPlaceholder(\"' + self.id_part + '\"); }')\n            self.interrupting.click(fn=interrupt_function)\n\n    def create_tools_row(self):\n        with gr.Row(elem_id=f\"{self.id_part}_tools\"):\n            from modules.ui import paste_symbol, clear_prompt_symbol, restore_progress_symbol\n\n            self.paste = ToolButton(value=paste_symbol, elem_id=\"paste\", tooltip=\"Read generation parameters from prompt or last generation if prompt is empty into user interface.\")\n            self.clear_prompt_button = ToolButton(value=clear_prompt_symbol, elem_id=f\"{self.id_part}_clear_prompt\", tooltip=\"Clear prompt\")\n            self.apply_styles = ToolButton(value=ui_prompt_styles.styles_materialize_symbol, elem_id=f\"{self.id_part}_style_apply\", tooltip=\"Apply all selected styles to prompts.\")\n\n            if self.is_img2img:\n                self.button_interrogate = ToolButton('\ud83d\udcce', tooltip='Interrogate CLIP - use CLIP neural network to create a text describing the image, and put it into the prompt field', elem_id=\"interrogate\")\n                self.button_deepbooru = ToolButton('\ud83d\udce6', tooltip='Interrogate DeepBooru - use DeepBooru neural network to create a text describing the image, and put it into the prompt field', elem_id=\"deepbooru\")\n\n            self.restore_progress_button = ToolButton(value=restore_progress_symbol, elem_id=f\"{self.id_part}_restore_progress\", visible=False, tooltip=\"Restore progress\")\n\n            self.token_counter = gr.HTML(value=\"<span>0/75</span>\", elem_id=f\"{self.id_part}_token_counter\", elem_classes=[\"token-counter\"], visible=False)\n            self.token_button = gr.Button(visible=False, elem_id=f\"{self.id_part}_token_button\")\n            self.negative_token_counter = gr.HTML(value=\"<span>0/75</span>\", elem_id=f\"{self.id_part}_negative_token_counter\", elem_classes=[\"token-counter\"], visible=False)\n            self.negative_token_button = gr.Button(visible=False, elem_id=f\"{self.id_part}_negative_token_button\")\n\n            self.clear_prompt_button.click(\n                fn=lambda *x: x,\n                _js=\"confirm_clear_prompt\",\n                inputs=[self.prompt, self.negative_prompt],\n                outputs=[self.prompt, self.negative_prompt],\n            )\n\n    def create_styles_ui(self):\n        self.ui_styles = ui_prompt_styles.UiPromptStyles(self.id_part, self.prompt, self.negative_prompt)\n        self.ui_styles.setup_apply_button(self.apply_styles)\n", "modules/sub_quadratic_attention.py": "# original source:\n#   https://github.com/AminRezaei0x443/memory-efficient-attention/blob/1bc0d9e6ac5f82ea43a375135c4e1d3896ee1694/memory_efficient_attention/attention_torch.py\n# license:\n#   MIT License (see Memory Efficient Attention under the Licenses section in the web UI interface for the full license)\n# credit:\n#   Amin Rezaei (original author)\n#   Alex Birch (optimized algorithm for 3D tensors, at the expense of removing bias, masking and callbacks)\n#   brkirch (modified to use torch.narrow instead of dynamic_slice implementation)\n# implementation of:\n#   Self-attention Does Not Need O(n2) Memory\":\n#   https://arxiv.org/abs/2112.05682v2\n\nfrom functools import partial\nimport torch\nfrom torch import Tensor\nfrom torch.utils.checkpoint import checkpoint\nimport math\nfrom typing import Optional, NamedTuple\n\n\ndef narrow_trunc(\n    input: Tensor,\n    dim: int,\n    start: int,\n    length: int\n) -> Tensor:\n    return torch.narrow(input, dim, start, length if input.shape[dim] >= start + length else input.shape[dim] - start)\n\n\nclass AttnChunk(NamedTuple):\n    exp_values: Tensor\n    exp_weights_sum: Tensor\n    max_score: Tensor\n\n\nclass SummarizeChunk:\n    @staticmethod\n    def __call__(\n        query: Tensor,\n        key: Tensor,\n        value: Tensor,\n    ) -> AttnChunk: ...\n\n\nclass ComputeQueryChunkAttn:\n    @staticmethod\n    def __call__(\n        query: Tensor,\n        key: Tensor,\n        value: Tensor,\n    ) -> Tensor: ...\n\n\ndef _summarize_chunk(\n    query: Tensor,\n    key: Tensor,\n    value: Tensor,\n    scale: float,\n) -> AttnChunk:\n    attn_weights = torch.baddbmm(\n        torch.zeros(1, 1, 1, device=query.device, dtype=query.dtype),\n        query,\n        key.transpose(1,2),\n        alpha=scale,\n        beta=0,\n    )\n    max_score, _ = torch.max(attn_weights, -1, keepdim=True)\n    max_score = max_score.detach()\n    exp_weights = torch.exp(attn_weights - max_score)\n    exp_values = torch.bmm(exp_weights, value) if query.device.type == 'mps' else torch.bmm(exp_weights, value.to(exp_weights.dtype)).to(value.dtype)\n    max_score = max_score.squeeze(-1)\n    return AttnChunk(exp_values, exp_weights.sum(dim=-1), max_score)\n\n\ndef _query_chunk_attention(\n    query: Tensor,\n    key: Tensor,\n    value: Tensor,\n    summarize_chunk: SummarizeChunk,\n    kv_chunk_size: int,\n) -> Tensor:\n    batch_x_heads, k_tokens, k_channels_per_head = key.shape\n    _, _, v_channels_per_head = value.shape\n\n    def chunk_scanner(chunk_idx: int) -> AttnChunk:\n        key_chunk = narrow_trunc(\n            key,\n            1,\n            chunk_idx,\n            kv_chunk_size\n        )\n        value_chunk = narrow_trunc(\n            value,\n            1,\n            chunk_idx,\n            kv_chunk_size\n        )\n        return summarize_chunk(query, key_chunk, value_chunk)\n\n    chunks: list[AttnChunk] = [\n        chunk_scanner(chunk) for chunk in torch.arange(0, k_tokens, kv_chunk_size)\n    ]\n    acc_chunk = AttnChunk(*map(torch.stack, zip(*chunks)))\n    chunk_values, chunk_weights, chunk_max = acc_chunk\n\n    global_max, _ = torch.max(chunk_max, 0, keepdim=True)\n    max_diffs = torch.exp(chunk_max - global_max)\n    chunk_values *= torch.unsqueeze(max_diffs, -1)\n    chunk_weights *= max_diffs\n\n    all_values = chunk_values.sum(dim=0)\n    all_weights = torch.unsqueeze(chunk_weights, -1).sum(dim=0)\n    return all_values / all_weights\n\n\n# TODO: refactor CrossAttention#get_attention_scores to share code with this\ndef _get_attention_scores_no_kv_chunking(\n    query: Tensor,\n    key: Tensor,\n    value: Tensor,\n    scale: float,\n) -> Tensor:\n    attn_scores = torch.baddbmm(\n        torch.zeros(1, 1, 1, device=query.device, dtype=query.dtype),\n        query,\n        key.transpose(1,2),\n        alpha=scale,\n        beta=0,\n    )\n    attn_probs = attn_scores.softmax(dim=-1)\n    del attn_scores\n    hidden_states_slice = torch.bmm(attn_probs, value) if query.device.type == 'mps' else torch.bmm(attn_probs, value.to(attn_probs.dtype)).to(value.dtype)\n    return hidden_states_slice\n\n\nclass ScannedChunk(NamedTuple):\n    chunk_idx: int\n    attn_chunk: AttnChunk\n\n\ndef efficient_dot_product_attention(\n    query: Tensor,\n    key: Tensor,\n    value: Tensor,\n    query_chunk_size=1024,\n    kv_chunk_size: Optional[int] = None,\n    kv_chunk_size_min: Optional[int] = None,\n    use_checkpoint=True,\n):\n    \"\"\"Computes efficient dot-product attention given query, key, and value.\n      This is efficient version of attention presented in\n      https://arxiv.org/abs/2112.05682v2 which comes with O(sqrt(n)) memory requirements.\n      Args:\n        query: queries for calculating attention with shape of\n          `[batch * num_heads, tokens, channels_per_head]`.\n        key: keys for calculating attention with shape of\n          `[batch * num_heads, tokens, channels_per_head]`.\n        value: values to be used in attention with shape of\n          `[batch * num_heads, tokens, channels_per_head]`.\n        query_chunk_size: int: query chunks size\n        kv_chunk_size: Optional[int]: key/value chunks size. if None: defaults to sqrt(key_tokens)\n        kv_chunk_size_min: Optional[int]: key/value minimum chunk size. only considered when kv_chunk_size is None. changes `sqrt(key_tokens)` into `max(sqrt(key_tokens), kv_chunk_size_min)`, to ensure our chunk sizes don't get too small (smaller chunks = more chunks = less concurrent work done).\n        use_checkpoint: bool: whether to use checkpointing (recommended True for training, False for inference)\n      Returns:\n        Output of shape `[batch * num_heads, query_tokens, channels_per_head]`.\n      \"\"\"\n    batch_x_heads, q_tokens, q_channels_per_head = query.shape\n    _, k_tokens, _ = key.shape\n    scale = q_channels_per_head ** -0.5\n\n    kv_chunk_size = min(kv_chunk_size or int(math.sqrt(k_tokens)), k_tokens)\n    if kv_chunk_size_min is not None:\n        kv_chunk_size = max(kv_chunk_size, kv_chunk_size_min)\n\n    def get_query_chunk(chunk_idx: int) -> Tensor:\n        return narrow_trunc(\n            query,\n            1,\n            chunk_idx,\n            min(query_chunk_size, q_tokens)\n        )\n\n    summarize_chunk: SummarizeChunk = partial(_summarize_chunk, scale=scale)\n    summarize_chunk: SummarizeChunk = partial(checkpoint, summarize_chunk) if use_checkpoint else summarize_chunk\n    compute_query_chunk_attn: ComputeQueryChunkAttn = partial(\n        _get_attention_scores_no_kv_chunking,\n        scale=scale\n    ) if k_tokens <= kv_chunk_size else (\n        # fast-path for when there's just 1 key-value chunk per query chunk (this is just sliced attention btw)\n        partial(\n            _query_chunk_attention,\n            kv_chunk_size=kv_chunk_size,\n            summarize_chunk=summarize_chunk,\n        )\n    )\n\n    if q_tokens <= query_chunk_size:\n        # fast-path for when there's just 1 query chunk\n        return compute_query_chunk_attn(\n            query=query,\n            key=key,\n            value=value,\n        )\n\n    res = torch.zeros_like(query)\n    for i in range(math.ceil(q_tokens / query_chunk_size)):\n        attn_scores = compute_query_chunk_attn(\n            query=get_query_chunk(i * query_chunk_size),\n            key=key,\n            value=value,\n        )\n\n        res[:, i * query_chunk_size:i * query_chunk_size + attn_scores.shape[1], :] = attn_scores\n\n    return res\n", "modules/hashes.py": "import hashlib\nimport os.path\n\nfrom modules import shared\nimport modules.cache\n\ndump_cache = modules.cache.dump_cache\ncache = modules.cache.cache\n\n\ndef calculate_sha256(filename):\n    hash_sha256 = hashlib.sha256()\n    blksize = 1024 * 1024\n\n    with open(filename, \"rb\") as f:\n        for chunk in iter(lambda: f.read(blksize), b\"\"):\n            hash_sha256.update(chunk)\n\n    return hash_sha256.hexdigest()\n\n\ndef sha256_from_cache(filename, title, use_addnet_hash=False):\n    hashes = cache(\"hashes-addnet\") if use_addnet_hash else cache(\"hashes\")\n    try:\n        ondisk_mtime = os.path.getmtime(filename)\n    except FileNotFoundError:\n        return None\n\n    if title not in hashes:\n        return None\n\n    cached_sha256 = hashes[title].get(\"sha256\", None)\n    cached_mtime = hashes[title].get(\"mtime\", 0)\n\n    if ondisk_mtime > cached_mtime or cached_sha256 is None:\n        return None\n\n    return cached_sha256\n\n\ndef sha256(filename, title, use_addnet_hash=False):\n    hashes = cache(\"hashes-addnet\") if use_addnet_hash else cache(\"hashes\")\n\n    sha256_value = sha256_from_cache(filename, title, use_addnet_hash)\n    if sha256_value is not None:\n        return sha256_value\n\n    if shared.cmd_opts.no_hashing:\n        return None\n\n    print(f\"Calculating sha256 for {filename}: \", end='')\n    if use_addnet_hash:\n        with open(filename, \"rb\") as file:\n            sha256_value = addnet_hash_safetensors(file)\n    else:\n        sha256_value = calculate_sha256(filename)\n    print(f\"{sha256_value}\")\n\n    hashes[title] = {\n        \"mtime\": os.path.getmtime(filename),\n        \"sha256\": sha256_value,\n    }\n\n    dump_cache()\n\n    return sha256_value\n\n\ndef addnet_hash_safetensors(b):\n    \"\"\"kohya-ss hash for safetensors from https://github.com/kohya-ss/sd-scripts/blob/main/library/train_util.py\"\"\"\n    hash_sha256 = hashlib.sha256()\n    blksize = 1024 * 1024\n\n    b.seek(0)\n    header = b.read(8)\n    n = int.from_bytes(header, \"little\")\n\n    offset = n + 8\n    b.seek(offset)\n    for chunk in iter(lambda: b.read(blksize), b\"\"):\n        hash_sha256.update(chunk)\n\n    return hash_sha256.hexdigest()\n\n", "modules/config_states.py": "\"\"\"\nSupports saving and restoring webui and extensions from a known working set of commits\n\"\"\"\n\nimport os\nimport json\nimport tqdm\n\nfrom datetime import datetime\nimport git\n\nfrom modules import shared, extensions, errors\nfrom modules.paths_internal import script_path, config_states_dir\n\nall_config_states = {}\n\n\ndef list_config_states():\n    global all_config_states\n\n    all_config_states.clear()\n    os.makedirs(config_states_dir, exist_ok=True)\n\n    config_states = []\n    for filename in os.listdir(config_states_dir):\n        if filename.endswith(\".json\"):\n            path = os.path.join(config_states_dir, filename)\n            try:\n                with open(path, \"r\", encoding=\"utf-8\") as f:\n                    j = json.load(f)\n                    assert \"created_at\" in j, '\"created_at\" does not exist'\n                    j[\"filepath\"] = path\n                    config_states.append(j)\n            except Exception as e:\n                print(f'[ERROR]: Config states {path}, {e}')\n\n    config_states = sorted(config_states, key=lambda cs: cs[\"created_at\"], reverse=True)\n\n    for cs in config_states:\n        timestamp = datetime.fromtimestamp(cs[\"created_at\"]).strftime('%Y-%m-%d %H:%M:%S')\n        name = cs.get(\"name\", \"Config\")\n        full_name = f\"{name}: {timestamp}\"\n        all_config_states[full_name] = cs\n\n    return all_config_states\n\n\ndef get_webui_config():\n    webui_repo = None\n\n    try:\n        if os.path.exists(os.path.join(script_path, \".git\")):\n            webui_repo = git.Repo(script_path)\n    except Exception:\n        errors.report(f\"Error reading webui git info from {script_path}\", exc_info=True)\n\n    webui_remote = None\n    webui_commit_hash = None\n    webui_commit_date = None\n    webui_branch = None\n    if webui_repo and not webui_repo.bare:\n        try:\n            webui_remote = next(webui_repo.remote().urls, None)\n            head = webui_repo.head.commit\n            webui_commit_date = webui_repo.head.commit.committed_date\n            webui_commit_hash = head.hexsha\n            webui_branch = webui_repo.active_branch.name\n\n        except Exception:\n            webui_remote = None\n\n    return {\n        \"remote\": webui_remote,\n        \"commit_hash\": webui_commit_hash,\n        \"commit_date\": webui_commit_date,\n        \"branch\": webui_branch,\n    }\n\n\ndef get_extension_config():\n    ext_config = {}\n\n    for ext in extensions.extensions:\n        ext.read_info_from_repo()\n\n        entry = {\n            \"name\": ext.name,\n            \"path\": ext.path,\n            \"enabled\": ext.enabled,\n            \"is_builtin\": ext.is_builtin,\n            \"remote\": ext.remote,\n            \"commit_hash\": ext.commit_hash,\n            \"commit_date\": ext.commit_date,\n            \"branch\": ext.branch,\n            \"have_info_from_repo\": ext.have_info_from_repo\n        }\n\n        ext_config[ext.name] = entry\n\n    return ext_config\n\n\ndef get_config():\n    creation_time = datetime.now().timestamp()\n    webui_config = get_webui_config()\n    ext_config = get_extension_config()\n\n    return {\n        \"created_at\": creation_time,\n        \"webui\": webui_config,\n        \"extensions\": ext_config\n    }\n\n\ndef restore_webui_config(config):\n    print(\"* Restoring webui state...\")\n\n    if \"webui\" not in config:\n        print(\"Error: No webui data saved to config\")\n        return\n\n    webui_config = config[\"webui\"]\n\n    if \"commit_hash\" not in webui_config:\n        print(\"Error: No commit saved to webui config\")\n        return\n\n    webui_commit_hash = webui_config.get(\"commit_hash\", None)\n    webui_repo = None\n\n    try:\n        if os.path.exists(os.path.join(script_path, \".git\")):\n            webui_repo = git.Repo(script_path)\n    except Exception:\n        errors.report(f\"Error reading webui git info from {script_path}\", exc_info=True)\n        return\n\n    try:\n        webui_repo.git.fetch(all=True)\n        webui_repo.git.reset(webui_commit_hash, hard=True)\n        print(f\"* Restored webui to commit {webui_commit_hash}.\")\n    except Exception:\n        errors.report(f\"Error restoring webui to commit{webui_commit_hash}\")\n\n\ndef restore_extension_config(config):\n    print(\"* Restoring extension state...\")\n\n    if \"extensions\" not in config:\n        print(\"Error: No extension data saved to config\")\n        return\n\n    ext_config = config[\"extensions\"]\n\n    results = []\n    disabled = []\n\n    for ext in tqdm.tqdm(extensions.extensions):\n        if ext.is_builtin:\n            continue\n\n        ext.read_info_from_repo()\n        current_commit = ext.commit_hash\n\n        if ext.name not in ext_config:\n            ext.disabled = True\n            disabled.append(ext.name)\n            results.append((ext, current_commit[:8], False, \"Saved extension state not found in config, marking as disabled\"))\n            continue\n\n        entry = ext_config[ext.name]\n\n        if \"commit_hash\" in entry and entry[\"commit_hash\"]:\n            try:\n                ext.fetch_and_reset_hard(entry[\"commit_hash\"])\n                ext.read_info_from_repo()\n                if current_commit != entry[\"commit_hash\"]:\n                    results.append((ext, current_commit[:8], True, entry[\"commit_hash\"][:8]))\n            except Exception as ex:\n                results.append((ext, current_commit[:8], False, ex))\n        else:\n            results.append((ext, current_commit[:8], False, \"No commit hash found in config\"))\n\n        if not entry.get(\"enabled\", False):\n            ext.disabled = True\n            disabled.append(ext.name)\n        else:\n            ext.disabled = False\n\n    shared.opts.disabled_extensions = disabled\n    shared.opts.save(shared.config_filename)\n\n    print(\"* Finished restoring extensions. Results:\")\n    for ext, prev_commit, success, result in results:\n        if success:\n            print(f\"  + {ext.name}: {prev_commit} -> {result}\")\n        else:\n            print(f\"  ! {ext.name}: FAILURE ({result})\")\n", "modules/styles.py": "from __future__ import annotations\nfrom pathlib import Path\nfrom modules import errors\nimport csv\nimport os\nimport typing\nimport shutil\n\n\nclass PromptStyle(typing.NamedTuple):\n    name: str\n    prompt: str | None\n    negative_prompt: str | None\n    path: str | None = None\n\n\ndef merge_prompts(style_prompt: str, prompt: str) -> str:\n    if \"{prompt}\" in style_prompt:\n        res = style_prompt.replace(\"{prompt}\", prompt)\n    else:\n        parts = filter(None, (prompt.strip(), style_prompt.strip()))\n        res = \", \".join(parts)\n\n    return res\n\n\ndef apply_styles_to_prompt(prompt, styles):\n    for style in styles:\n        prompt = merge_prompts(style, prompt)\n\n    return prompt\n\n\ndef extract_style_text_from_prompt(style_text, prompt):\n    \"\"\"This function extracts the text from a given prompt based on a provided style text. It checks if the style text contains the placeholder {prompt} or if it appears at the end of the prompt. If a match is found, it returns True along with the extracted text. Otherwise, it returns False and the original prompt.\n\n    extract_style_text_from_prompt(\"masterpiece\", \"1girl, art by greg, masterpiece\") outputs (True, \"1girl, art by greg\")\n    extract_style_text_from_prompt(\"masterpiece, {prompt}\", \"masterpiece, 1girl, art by greg\") outputs (True, \"1girl, art by greg\")\n    extract_style_text_from_prompt(\"masterpiece, {prompt}\", \"exquisite, 1girl, art by greg\") outputs (False, \"exquisite, 1girl, art by greg\")\n    \"\"\"\n\n    stripped_prompt = prompt.strip()\n    stripped_style_text = style_text.strip()\n\n    if \"{prompt}\" in stripped_style_text:\n        left, _, right = stripped_style_text.partition(\"{prompt}\")\n        if stripped_prompt.startswith(left) and stripped_prompt.endswith(right):\n            prompt = stripped_prompt[len(left):len(stripped_prompt)-len(right)]\n            return True, prompt\n    else:\n        if stripped_prompt.endswith(stripped_style_text):\n            prompt = stripped_prompt[:len(stripped_prompt)-len(stripped_style_text)]\n\n            if prompt.endswith(', '):\n                prompt = prompt[:-2]\n\n            return True, prompt\n\n    return False, prompt\n\n\ndef extract_original_prompts(style: PromptStyle, prompt, negative_prompt):\n    \"\"\"\n    Takes a style and compares it to the prompt and negative prompt. If the style\n    matches, returns True plus the prompt and negative prompt with the style text\n    removed. Otherwise, returns False with the original prompt and negative prompt.\n    \"\"\"\n    if not style.prompt and not style.negative_prompt:\n        return False, prompt, negative_prompt\n\n    match_positive, extracted_positive = extract_style_text_from_prompt(style.prompt, prompt)\n    if not match_positive:\n        return False, prompt, negative_prompt\n\n    match_negative, extracted_negative = extract_style_text_from_prompt(style.negative_prompt, negative_prompt)\n    if not match_negative:\n        return False, prompt, negative_prompt\n\n    return True, extracted_positive, extracted_negative\n\n\nclass StyleDatabase:\n    def __init__(self, paths: list[str | Path]):\n        self.no_style = PromptStyle(\"None\", \"\", \"\", None)\n        self.styles = {}\n        self.paths = paths\n        self.all_styles_files: list[Path] = []\n\n        folder, file = os.path.split(self.paths[0])\n        if '*' in file or '?' in file:\n            # if the first path is a wildcard pattern, find the first match else use \"folder/styles.csv\" as the default path\n            self.default_path = next(Path(folder).glob(file), Path(os.path.join(folder, 'styles.csv')))\n            self.paths.insert(0, self.default_path)\n        else:\n            self.default_path = Path(self.paths[0])\n\n        self.prompt_fields = [field for field in PromptStyle._fields if field != \"path\"]\n\n        self.reload()\n\n    def reload(self):\n        \"\"\"\n        Clears the style database and reloads the styles from the CSV file(s)\n        matching the path used to initialize the database.\n        \"\"\"\n        self.styles.clear()\n\n        # scans for all styles files\n        all_styles_files = []\n        for pattern in self.paths:\n            folder, file = os.path.split(pattern)\n            if '*' in file or '?' in file:\n                found_files = Path(folder).glob(file)\n                [all_styles_files.append(file) for file in found_files]\n            else:\n                # if os.path.exists(pattern):\n                all_styles_files.append(Path(pattern))\n\n        # Remove any duplicate entries\n        seen = set()\n        self.all_styles_files = [s for s in all_styles_files if not (s in seen or seen.add(s))]\n\n        for styles_file in self.all_styles_files:\n            if len(all_styles_files) > 1:\n                # add divider when more than styles file\n                # '---------------- STYLES ----------------'\n                divider = f' {styles_file.stem.upper()} '.center(40, '-')\n                self.styles[divider] = PromptStyle(f\"{divider}\", None, None, \"do_not_save\")\n            if styles_file.is_file():\n                self.load_from_csv(styles_file)\n\n    def load_from_csv(self, path: str | Path):\n        try:\n            with open(path, \"r\", encoding=\"utf-8-sig\", newline=\"\") as file:\n                reader = csv.DictReader(file, skipinitialspace=True)\n                for row in reader:\n                    # Ignore empty rows or rows starting with a comment\n                    if not row or row[\"name\"].startswith(\"#\"):\n                        continue\n                    # Support loading old CSV format with \"name, text\"-columns\n                    prompt = row[\"prompt\"] if \"prompt\" in row else row[\"text\"]\n                    negative_prompt = row.get(\"negative_prompt\", \"\")\n                    # Add style to database\n                    self.styles[row[\"name\"]] = PromptStyle(\n                        row[\"name\"], prompt, negative_prompt, str(path)\n                    )\n        except Exception:\n            errors.report(f'Error loading styles from {path}: ', exc_info=True)\n\n    def get_style_paths(self) -> set:\n        \"\"\"Returns a set of all distinct paths of files that styles are loaded from.\"\"\"\n        # Update any styles without a path to the default path\n        for style in list(self.styles.values()):\n            if not style.path:\n                self.styles[style.name] = style._replace(path=str(self.default_path))\n\n        # Create a list of all distinct paths, including the default path\n        style_paths = set()\n        style_paths.add(str(self.default_path))\n        for _, style in self.styles.items():\n            if style.path:\n                style_paths.add(style.path)\n\n        # Remove any paths for styles that are just list dividers\n        style_paths.discard(\"do_not_save\")\n\n        return style_paths\n\n    def get_style_prompts(self, styles):\n        return [self.styles.get(x, self.no_style).prompt for x in styles]\n\n    def get_negative_style_prompts(self, styles):\n        return [self.styles.get(x, self.no_style).negative_prompt for x in styles]\n\n    def apply_styles_to_prompt(self, prompt, styles):\n        return apply_styles_to_prompt(\n            prompt, [self.styles.get(x, self.no_style).prompt for x in styles]\n        )\n\n    def apply_negative_styles_to_prompt(self, prompt, styles):\n        return apply_styles_to_prompt(\n            prompt, [self.styles.get(x, self.no_style).negative_prompt for x in styles]\n        )\n\n    def save_styles(self, path: str = None) -> None:\n        # The path argument is deprecated, but kept for backwards compatibility\n\n        style_paths = self.get_style_paths()\n\n        csv_names = [os.path.split(path)[1].lower() for path in style_paths]\n\n        for style_path in style_paths:\n            # Always keep a backup file around\n            if os.path.exists(style_path):\n                shutil.copy(style_path, f\"{style_path}.bak\")\n\n            # Write the styles to the CSV file\n            with open(style_path, \"w\", encoding=\"utf-8-sig\", newline=\"\") as file:\n                writer = csv.DictWriter(file, fieldnames=self.prompt_fields)\n                writer.writeheader()\n                for style in (s for s in self.styles.values() if s.path == style_path):\n                    # Skip style list dividers, e.g. \"STYLES.CSV\"\n                    if style.name.lower().strip(\"# \") in csv_names:\n                        continue\n                    # Write style fields, ignoring the path field\n                    writer.writerow(\n                        {k: v for k, v in style._asdict().items() if k != \"path\"}\n                    )\n\n    def extract_styles_from_prompt(self, prompt, negative_prompt):\n        extracted = []\n\n        applicable_styles = list(self.styles.values())\n\n        while True:\n            found_style = None\n\n            for style in applicable_styles:\n                is_match, new_prompt, new_neg_prompt = extract_original_prompts(\n                    style, prompt, negative_prompt\n                )\n                if is_match:\n                    found_style = style\n                    prompt = new_prompt\n                    negative_prompt = new_neg_prompt\n                    break\n\n            if not found_style:\n                break\n\n            applicable_styles.remove(found_style)\n            extracted.append(found_style.name)\n\n        return list(reversed(extracted)), prompt, negative_prompt\n", "modules/extra_networks_hypernet.py": "from modules import extra_networks, shared\nfrom modules.hypernetworks import hypernetwork\n\n\nclass ExtraNetworkHypernet(extra_networks.ExtraNetwork):\n    def __init__(self):\n        super().__init__('hypernet')\n\n    def activate(self, p, params_list):\n        additional = shared.opts.sd_hypernetwork\n\n        if additional != \"None\" and additional in shared.hypernetworks and not any(x for x in params_list if x.items[0] == additional):\n            hypernet_prompt_text = f\"<hypernet:{additional}:{shared.opts.extra_networks_default_multiplier}>\"\n            p.all_prompts = [f\"{prompt}{hypernet_prompt_text}\" for prompt in p.all_prompts]\n            params_list.append(extra_networks.ExtraNetworkParams(items=[additional, shared.opts.extra_networks_default_multiplier]))\n\n        names = []\n        multipliers = []\n        for params in params_list:\n            assert params.items\n\n            names.append(params.items[0])\n            multipliers.append(float(params.items[1]) if len(params.items) > 1 else 1.0)\n\n        hypernetwork.load_hypernetworks(names, multipliers)\n\n    def deactivate(self, p):\n        pass\n", "modules/shared_items.py": "import html\nimport sys\n\nfrom modules import script_callbacks, scripts, ui_components\nfrom modules.options import OptionHTML, OptionInfo\nfrom modules.shared_cmd_options import cmd_opts\n\n\ndef realesrgan_models_names():\n    import modules.realesrgan_model\n    return [x.name for x in modules.realesrgan_model.get_realesrgan_models(None)]\n\n\ndef dat_models_names():\n    import modules.dat_model\n    return [x.name for x in modules.dat_model.get_dat_models(None)]\n\n\ndef postprocessing_scripts():\n    import modules.scripts\n\n    return modules.scripts.scripts_postproc.scripts\n\n\ndef sd_vae_items():\n    import modules.sd_vae\n\n    return [\"Automatic\", \"None\"] + list(modules.sd_vae.vae_dict)\n\n\ndef refresh_vae_list():\n    import modules.sd_vae\n\n    modules.sd_vae.refresh_vae_list()\n\n\ndef cross_attention_optimizations():\n    import modules.sd_hijack\n\n    return [\"Automatic\"] + [x.title() for x in modules.sd_hijack.optimizers] + [\"None\"]\n\n\ndef sd_unet_items():\n    import modules.sd_unet\n\n    return [\"Automatic\"] + [x.label for x in modules.sd_unet.unet_options] + [\"None\"]\n\n\ndef refresh_unet_list():\n    import modules.sd_unet\n\n    modules.sd_unet.list_unets()\n\n\ndef list_checkpoint_tiles(use_short=False):\n    import modules.sd_models\n    return modules.sd_models.checkpoint_tiles(use_short)\n\n\ndef refresh_checkpoints():\n    import modules.sd_models\n    return modules.sd_models.list_models()\n\n\ndef list_samplers():\n    import modules.sd_samplers\n    return modules.sd_samplers.all_samplers\n\n\ndef reload_hypernetworks():\n    from modules.hypernetworks import hypernetwork\n    from modules import shared\n\n    shared.hypernetworks = hypernetwork.list_hypernetworks(cmd_opts.hypernetwork_dir)\n\n\ndef get_infotext_names():\n    from modules import infotext_utils, shared\n    res = {}\n\n    for info in shared.opts.data_labels.values():\n        if info.infotext:\n            res[info.infotext] = 1\n\n    for tab_data in infotext_utils.paste_fields.values():\n        for _, name in tab_data.get(\"fields\") or []:\n            if isinstance(name, str):\n                res[name] = 1\n\n    return list(res)\n\n\nui_reorder_categories_builtin_items = [\n    \"prompt\",\n    \"image\",\n    \"inpaint\",\n    \"sampler\",\n    \"accordions\",\n    \"checkboxes\",\n    \"dimensions\",\n    \"cfg\",\n    \"denoising\",\n    \"seed\",\n    \"batch\",\n    \"override_settings\",\n]\n\n\ndef ui_reorder_categories():\n    from modules import scripts\n\n    yield from ui_reorder_categories_builtin_items\n\n    sections = {}\n    for script in scripts.scripts_txt2img.scripts + scripts.scripts_img2img.scripts:\n        if isinstance(script.section, str) and script.section not in ui_reorder_categories_builtin_items:\n            sections[script.section] = 1\n\n    yield from sections\n\n    yield \"scripts\"\n\n\ndef callbacks_order_settings():\n    options = {\n        \"sd_vae_explanation\": OptionHTML(\"\"\"\n    For categories below, callbacks added to dropdowns happen before others, in order listed.\n    \"\"\"),\n\n    }\n\n    callback_options = {}\n\n    for category, _ in script_callbacks.enumerate_callbacks():\n        callback_options[category] = script_callbacks.ordered_callbacks(category, enable_user_sort=False)\n\n    for method_name in scripts.scripts_txt2img.callback_names:\n        callback_options[\"script_\" + method_name] = scripts.scripts_txt2img.create_ordered_callbacks_list(method_name, enable_user_sort=False)\n\n    for method_name in scripts.scripts_img2img.callback_names:\n        callbacks = callback_options.get(\"script_\" + method_name, [])\n\n        for addition in scripts.scripts_img2img.create_ordered_callbacks_list(method_name, enable_user_sort=False):\n            if any(x.name == addition.name for x in callbacks):\n                continue\n\n            callbacks.append(addition)\n\n        callback_options[\"script_\" + method_name] = callbacks\n\n    for category, callbacks in callback_options.items():\n        if not callbacks:\n            continue\n\n        option_info = OptionInfo([], f\"{category} callback priority\", ui_components.DropdownMulti, {\"choices\": [x.name for x in callbacks]})\n        option_info.needs_restart()\n        option_info.html(\"<div class='info'>Default order: <ol>\" + \"\".join(f\"<li>{html.escape(x.name)}</li>\\n\" for x in callbacks) + \"</ol></div>\")\n        options['prioritized_callbacks_' + category] = option_info\n\n    return options\n\n\nclass Shared(sys.modules[__name__].__class__):\n    \"\"\"\n    this class is here to provide sd_model field as a property, so that it can be created and loaded on demand rather than\n    at program startup.\n    \"\"\"\n\n    sd_model_val = None\n\n    @property\n    def sd_model(self):\n        import modules.sd_models\n\n        return modules.sd_models.model_data.get_sd_model()\n\n    @sd_model.setter\n    def sd_model(self, value):\n        import modules.sd_models\n\n        modules.sd_models.model_data.set_sd_model(value)\n\n\nsys.modules['modules.shared'].__class__ = Shared\n", "modules/rng_philox.py": "\"\"\"RNG imitiating torch cuda randn on CPU. You are welcome.\n\nUsage:\n\n```\ng = Generator(seed=0)\nprint(g.randn(shape=(3, 4)))\n```\n\nExpected output:\n```\n[[-0.92466259 -0.42534415 -2.6438457   0.14518388]\n [-0.12086647 -0.57972564 -0.62285122 -0.32838709]\n [-1.07454231 -0.36314407 -1.67105067  2.26550497]]\n```\n\"\"\"\n\nimport numpy as np\n\nphilox_m = [0xD2511F53, 0xCD9E8D57]\nphilox_w = [0x9E3779B9, 0xBB67AE85]\n\ntwo_pow32_inv = np.array([2.3283064e-10], dtype=np.float32)\ntwo_pow32_inv_2pi = np.array([2.3283064e-10 * 6.2831855], dtype=np.float32)\n\n\ndef uint32(x):\n    \"\"\"Converts (N,) np.uint64 array into (2, N) np.unit32 array.\"\"\"\n    return x.view(np.uint32).reshape(-1, 2).transpose(1, 0)\n\n\ndef philox4_round(counter, key):\n    \"\"\"A single round of the Philox 4x32 random number generator.\"\"\"\n\n    v1 = uint32(counter[0].astype(np.uint64) * philox_m[0])\n    v2 = uint32(counter[2].astype(np.uint64) * philox_m[1])\n\n    counter[0] = v2[1] ^ counter[1] ^ key[0]\n    counter[1] = v2[0]\n    counter[2] = v1[1] ^ counter[3] ^ key[1]\n    counter[3] = v1[0]\n\n\ndef philox4_32(counter, key, rounds=10):\n    \"\"\"Generates 32-bit random numbers using the Philox 4x32 random number generator.\n\n    Parameters:\n        counter (numpy.ndarray): A 4xN array of 32-bit integers representing the counter values (offset into generation).\n        key (numpy.ndarray): A 2xN array of 32-bit integers representing the key values (seed).\n        rounds (int): The number of rounds to perform.\n\n    Returns:\n        numpy.ndarray: A 4xN array of 32-bit integers containing the generated random numbers.\n    \"\"\"\n\n    for _ in range(rounds - 1):\n        philox4_round(counter, key)\n\n        key[0] = key[0] + philox_w[0]\n        key[1] = key[1] + philox_w[1]\n\n    philox4_round(counter, key)\n    return counter\n\n\ndef box_muller(x, y):\n    \"\"\"Returns just the first out of two numbers generated by Box\u2013Muller transform algorithm.\"\"\"\n    u = x * two_pow32_inv + two_pow32_inv / 2\n    v = y * two_pow32_inv_2pi + two_pow32_inv_2pi / 2\n\n    s = np.sqrt(-2.0 * np.log(u))\n\n    r1 = s * np.sin(v)\n    return r1.astype(np.float32)\n\n\nclass Generator:\n    \"\"\"RNG that produces same outputs as torch.randn(..., device='cuda') on CPU\"\"\"\n\n    def __init__(self, seed):\n        self.seed = seed\n        self.offset = 0\n\n    def randn(self, shape):\n        \"\"\"Generate a sequence of n standard normal random variables using the Philox 4x32 random number generator and the Box-Muller transform.\"\"\"\n\n        n = 1\n        for x in shape:\n            n *= x\n\n        counter = np.zeros((4, n), dtype=np.uint32)\n        counter[0] = self.offset\n        counter[2] = np.arange(n, dtype=np.uint32)  # up to 2^32 numbers can be generated - if you want more you'd need to spill into counter[3]\n        self.offset += 1\n\n        key = np.empty(n, dtype=np.uint64)\n        key.fill(self.seed)\n        key = uint32(key)\n\n        g = philox4_32(counter, key)\n\n        return box_muller(g[0], g[1]).reshape(shape)  # discard g[2] and g[3]\n", "modules/safe.py": "# this code is adapted from the script contributed by anon from /h/\n\nimport pickle\nimport collections\n\nimport torch\nimport numpy\nimport _codecs\nimport zipfile\nimport re\n\n\n# PyTorch 1.13 and later have _TypedStorage renamed to TypedStorage\nfrom modules import errors\n\nTypedStorage = torch.storage.TypedStorage if hasattr(torch.storage, 'TypedStorage') else torch.storage._TypedStorage\n\ndef encode(*args):\n    out = _codecs.encode(*args)\n    return out\n\n\nclass RestrictedUnpickler(pickle.Unpickler):\n    extra_handler = None\n\n    def persistent_load(self, saved_id):\n        assert saved_id[0] == 'storage'\n\n        try:\n            return TypedStorage(_internal=True)\n        except TypeError:\n            return TypedStorage()  # PyTorch before 2.0 does not have the _internal argument\n\n    def find_class(self, module, name):\n        if self.extra_handler is not None:\n            res = self.extra_handler(module, name)\n            if res is not None:\n                return res\n\n        if module == 'collections' and name == 'OrderedDict':\n            return getattr(collections, name)\n        if module == 'torch._utils' and name in ['_rebuild_tensor_v2', '_rebuild_parameter', '_rebuild_device_tensor_from_numpy']:\n            return getattr(torch._utils, name)\n        if module == 'torch' and name in ['FloatStorage', 'HalfStorage', 'IntStorage', 'LongStorage', 'DoubleStorage', 'ByteStorage', 'float32', 'BFloat16Storage']:\n            return getattr(torch, name)\n        if module == 'torch.nn.modules.container' and name in ['ParameterDict']:\n            return getattr(torch.nn.modules.container, name)\n        if module == 'numpy.core.multiarray' and name in ['scalar', '_reconstruct']:\n            return getattr(numpy.core.multiarray, name)\n        if module == 'numpy' and name in ['dtype', 'ndarray']:\n            return getattr(numpy, name)\n        if module == '_codecs' and name == 'encode':\n            return encode\n        if module == \"pytorch_lightning.callbacks\" and name == 'model_checkpoint':\n            import pytorch_lightning.callbacks\n            return pytorch_lightning.callbacks.model_checkpoint\n        if module == \"pytorch_lightning.callbacks.model_checkpoint\" and name == 'ModelCheckpoint':\n            import pytorch_lightning.callbacks.model_checkpoint\n            return pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint\n        if module == \"__builtin__\" and name == 'set':\n            return set\n\n        # Forbid everything else.\n        raise Exception(f\"global '{module}/{name}' is forbidden\")\n\n\n# Regular expression that accepts 'dirname/version', 'dirname/data.pkl', and 'dirname/data/<number>'\nallowed_zip_names_re = re.compile(r\"^([^/]+)/((data/\\d+)|version|(data\\.pkl))$\")\ndata_pkl_re = re.compile(r\"^([^/]+)/data\\.pkl$\")\n\ndef check_zip_filenames(filename, names):\n    for name in names:\n        if allowed_zip_names_re.match(name):\n            continue\n\n        raise Exception(f\"bad file inside {filename}: {name}\")\n\n\ndef check_pt(filename, extra_handler):\n    try:\n\n        # new pytorch format is a zip file\n        with zipfile.ZipFile(filename) as z:\n            check_zip_filenames(filename, z.namelist())\n\n            # find filename of data.pkl in zip file: '<directory name>/data.pkl'\n            data_pkl_filenames = [f for f in z.namelist() if data_pkl_re.match(f)]\n            if len(data_pkl_filenames) == 0:\n                raise Exception(f\"data.pkl not found in {filename}\")\n            if len(data_pkl_filenames) > 1:\n                raise Exception(f\"Multiple data.pkl found in {filename}\")\n            with z.open(data_pkl_filenames[0]) as file:\n                unpickler = RestrictedUnpickler(file)\n                unpickler.extra_handler = extra_handler\n                unpickler.load()\n\n    except zipfile.BadZipfile:\n\n        # if it's not a zip file, it's an old pytorch format, with five objects written to pickle\n        with open(filename, \"rb\") as file:\n            unpickler = RestrictedUnpickler(file)\n            unpickler.extra_handler = extra_handler\n            for _ in range(5):\n                unpickler.load()\n\n\ndef load(filename, *args, **kwargs):\n    return load_with_extra(filename, *args, extra_handler=global_extra_handler, **kwargs)\n\n\ndef load_with_extra(filename, extra_handler=None, *args, **kwargs):\n    \"\"\"\n    this function is intended to be used by extensions that want to load models with\n    some extra classes in them that the usual unpickler would find suspicious.\n\n    Use the extra_handler argument to specify a function that takes module and field name as text,\n    and returns that field's value:\n\n    ```python\n    def extra(module, name):\n        if module == 'collections' and name == 'OrderedDict':\n            return collections.OrderedDict\n\n        return None\n\n    safe.load_with_extra('model.pt', extra_handler=extra)\n    ```\n\n    The alternative to this is just to use safe.unsafe_torch_load('model.pt'), which as the name implies is\n    definitely unsafe.\n    \"\"\"\n\n    from modules import shared\n\n    try:\n        if not shared.cmd_opts.disable_safe_unpickle:\n            check_pt(filename, extra_handler)\n\n    except pickle.UnpicklingError:\n        errors.report(\n            f\"Error verifying pickled file from {filename}\\n\"\n            \"-----> !!!! The file is most likely corrupted !!!! <-----\\n\"\n            \"You can skip this check with --disable-safe-unpickle commandline argument, but that is not going to help you.\\n\\n\",\n            exc_info=True,\n        )\n        return None\n    except Exception:\n        errors.report(\n            f\"Error verifying pickled file from {filename}\\n\"\n            f\"The file may be malicious, so the program is not going to read it.\\n\"\n            f\"You can skip this check with --disable-safe-unpickle commandline argument.\\n\\n\",\n            exc_info=True,\n        )\n        return None\n\n    return unsafe_torch_load(filename, *args, **kwargs)\n\n\nclass Extra:\n    \"\"\"\n    A class for temporarily setting the global handler for when you can't explicitly call load_with_extra\n    (because it's not your code making the torch.load call). The intended use is like this:\n\n```\nimport torch\nfrom modules import safe\n\ndef handler(module, name):\n    if module == 'torch' and name in ['float64', 'float16']:\n        return getattr(torch, name)\n\n    return None\n\nwith safe.Extra(handler):\n    x = torch.load('model.pt')\n```\n    \"\"\"\n\n    def __init__(self, handler):\n        self.handler = handler\n\n    def __enter__(self):\n        global global_extra_handler\n\n        assert global_extra_handler is None, 'already inside an Extra() block'\n        global_extra_handler = self.handler\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        global global_extra_handler\n\n        global_extra_handler = None\n\n\nunsafe_torch_load = torch.load\ntorch.load = load\nglobal_extra_handler = None\n", "modules/shared_state.py": "import datetime\nimport logging\nimport threading\nimport time\n\nfrom modules import errors, shared, devices\nfrom typing import Optional\n\nlog = logging.getLogger(__name__)\n\n\nclass State:\n    skipped = False\n    interrupted = False\n    stopping_generation = False\n    job = \"\"\n    job_no = 0\n    job_count = 0\n    processing_has_refined_job_count = False\n    job_timestamp = '0'\n    sampling_step = 0\n    sampling_steps = 0\n    current_latent = None\n    current_image = None\n    current_image_sampling_step = 0\n    id_live_preview = 0\n    textinfo = None\n    time_start = None\n    server_start = None\n    _server_command_signal = threading.Event()\n    _server_command: Optional[str] = None\n\n    def __init__(self):\n        self.server_start = time.time()\n\n    @property\n    def need_restart(self) -> bool:\n        # Compatibility getter for need_restart.\n        return self.server_command == \"restart\"\n\n    @need_restart.setter\n    def need_restart(self, value: bool) -> None:\n        # Compatibility setter for need_restart.\n        if value:\n            self.server_command = \"restart\"\n\n    @property\n    def server_command(self):\n        return self._server_command\n\n    @server_command.setter\n    def server_command(self, value: Optional[str]) -> None:\n        \"\"\"\n        Set the server command to `value` and signal that it's been set.\n        \"\"\"\n        self._server_command = value\n        self._server_command_signal.set()\n\n    def wait_for_server_command(self, timeout: Optional[float] = None) -> Optional[str]:\n        \"\"\"\n        Wait for server command to get set; return and clear the value and signal.\n        \"\"\"\n        if self._server_command_signal.wait(timeout):\n            self._server_command_signal.clear()\n            req = self._server_command\n            self._server_command = None\n            return req\n        return None\n\n    def request_restart(self) -> None:\n        self.interrupt()\n        self.server_command = \"restart\"\n        log.info(\"Received restart request\")\n\n    def skip(self):\n        self.skipped = True\n        log.info(\"Received skip request\")\n\n    def interrupt(self):\n        self.interrupted = True\n        log.info(\"Received interrupt request\")\n\n    def stop_generating(self):\n        self.stopping_generation = True\n        log.info(\"Received stop generating request\")\n\n    def nextjob(self):\n        if shared.opts.live_previews_enable and shared.opts.show_progress_every_n_steps == -1:\n            self.do_set_current_image()\n\n        self.job_no += 1\n        self.sampling_step = 0\n        self.current_image_sampling_step = 0\n\n    def dict(self):\n        obj = {\n            \"skipped\": self.skipped,\n            \"interrupted\": self.interrupted,\n            \"stopping_generation\": self.stopping_generation,\n            \"job\": self.job,\n            \"job_count\": self.job_count,\n            \"job_timestamp\": self.job_timestamp,\n            \"job_no\": self.job_no,\n            \"sampling_step\": self.sampling_step,\n            \"sampling_steps\": self.sampling_steps,\n        }\n\n        return obj\n\n    def begin(self, job: str = \"(unknown)\"):\n        self.sampling_step = 0\n        self.time_start = time.time()\n        self.job_count = -1\n        self.processing_has_refined_job_count = False\n        self.job_no = 0\n        self.job_timestamp = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n        self.current_latent = None\n        self.current_image = None\n        self.current_image_sampling_step = 0\n        self.id_live_preview = 0\n        self.skipped = False\n        self.interrupted = False\n        self.stopping_generation = False\n        self.textinfo = None\n        self.job = job\n        devices.torch_gc()\n        log.info(\"Starting job %s\", job)\n\n    def end(self):\n        duration = time.time() - self.time_start\n        log.info(\"Ending job %s (%.2f seconds)\", self.job, duration)\n        self.job = \"\"\n        self.job_count = 0\n\n        devices.torch_gc()\n\n    def set_current_image(self):\n        \"\"\"if enough sampling steps have been made after the last call to this, sets self.current_image from self.current_latent, and modifies self.id_live_preview accordingly\"\"\"\n        if not shared.parallel_processing_allowed:\n            return\n\n        if self.sampling_step - self.current_image_sampling_step >= shared.opts.show_progress_every_n_steps and shared.opts.live_previews_enable and shared.opts.show_progress_every_n_steps != -1:\n            self.do_set_current_image()\n\n    def do_set_current_image(self):\n        if self.current_latent is None:\n            return\n\n        import modules.sd_samplers\n\n        try:\n            if shared.opts.show_progress_grid:\n                self.assign_current_image(modules.sd_samplers.samples_to_image_grid(self.current_latent))\n            else:\n                self.assign_current_image(modules.sd_samplers.sample_to_image(self.current_latent))\n\n            self.current_image_sampling_step = self.sampling_step\n\n        except Exception:\n            # when switching models during generation, VAE would be on CPU, so creating an image will fail.\n            # we silently ignore this error\n            errors.record_exception()\n\n    def assign_current_image(self, image):\n        if shared.opts.live_previews_image_format == 'jpeg' and image.mode == 'RGBA':\n            image = image.convert('RGB')\n        self.current_image = image\n        self.id_live_preview += 1\n", "modules/sd_vae_taesd.py": "\"\"\"\nTiny AutoEncoder for Stable Diffusion\n(DNN for encoding / decoding SD's latent space)\n\nhttps://github.com/madebyollin/taesd\n\"\"\"\nimport os\nimport torch\nimport torch.nn as nn\n\nfrom modules import devices, paths_internal, shared\n\nsd_vae_taesd_models = {}\n\n\ndef conv(n_in, n_out, **kwargs):\n    return nn.Conv2d(n_in, n_out, 3, padding=1, **kwargs)\n\n\nclass Clamp(nn.Module):\n    @staticmethod\n    def forward(x):\n        return torch.tanh(x / 3) * 3\n\n\nclass Block(nn.Module):\n    def __init__(self, n_in, n_out):\n        super().__init__()\n        self.conv = nn.Sequential(conv(n_in, n_out), nn.ReLU(), conv(n_out, n_out), nn.ReLU(), conv(n_out, n_out))\n        self.skip = nn.Conv2d(n_in, n_out, 1, bias=False) if n_in != n_out else nn.Identity()\n        self.fuse = nn.ReLU()\n\n    def forward(self, x):\n        return self.fuse(self.conv(x) + self.skip(x))\n\n\ndef decoder():\n    return nn.Sequential(\n        Clamp(), conv(4, 64), nn.ReLU(),\n        Block(64, 64), Block(64, 64), Block(64, 64), nn.Upsample(scale_factor=2), conv(64, 64, bias=False),\n        Block(64, 64), Block(64, 64), Block(64, 64), nn.Upsample(scale_factor=2), conv(64, 64, bias=False),\n        Block(64, 64), Block(64, 64), Block(64, 64), nn.Upsample(scale_factor=2), conv(64, 64, bias=False),\n        Block(64, 64), conv(64, 3),\n    )\n\n\ndef encoder():\n    return nn.Sequential(\n        conv(3, 64), Block(64, 64),\n        conv(64, 64, stride=2, bias=False), Block(64, 64), Block(64, 64), Block(64, 64),\n        conv(64, 64, stride=2, bias=False), Block(64, 64), Block(64, 64), Block(64, 64),\n        conv(64, 64, stride=2, bias=False), Block(64, 64), Block(64, 64), Block(64, 64),\n        conv(64, 4),\n    )\n\n\nclass TAESDDecoder(nn.Module):\n    latent_magnitude = 3\n    latent_shift = 0.5\n\n    def __init__(self, decoder_path=\"taesd_decoder.pth\"):\n        \"\"\"Initialize pretrained TAESD on the given device from the given checkpoints.\"\"\"\n        super().__init__()\n        self.decoder = decoder()\n        self.decoder.load_state_dict(\n            torch.load(decoder_path, map_location='cpu' if devices.device.type != 'cuda' else None))\n\n\nclass TAESDEncoder(nn.Module):\n    latent_magnitude = 3\n    latent_shift = 0.5\n\n    def __init__(self, encoder_path=\"taesd_encoder.pth\"):\n        \"\"\"Initialize pretrained TAESD on the given device from the given checkpoints.\"\"\"\n        super().__init__()\n        self.encoder = encoder()\n        self.encoder.load_state_dict(\n            torch.load(encoder_path, map_location='cpu' if devices.device.type != 'cuda' else None))\n\n\ndef download_model(model_path, model_url):\n    if not os.path.exists(model_path):\n        os.makedirs(os.path.dirname(model_path), exist_ok=True)\n\n        print(f'Downloading TAESD model to: {model_path}')\n        torch.hub.download_url_to_file(model_url, model_path)\n\n\ndef decoder_model():\n    model_name = \"taesdxl_decoder.pth\" if getattr(shared.sd_model, 'is_sdxl', False) else \"taesd_decoder.pth\"\n    loaded_model = sd_vae_taesd_models.get(model_name)\n\n    if loaded_model is None:\n        model_path = os.path.join(paths_internal.models_path, \"VAE-taesd\", model_name)\n        download_model(model_path, 'https://github.com/madebyollin/taesd/raw/main/' + model_name)\n\n        if os.path.exists(model_path):\n            loaded_model = TAESDDecoder(model_path)\n            loaded_model.eval()\n            loaded_model.to(devices.device, devices.dtype)\n            sd_vae_taesd_models[model_name] = loaded_model\n        else:\n            raise FileNotFoundError('TAESD model not found')\n\n    return loaded_model.decoder\n\n\ndef encoder_model():\n    model_name = \"taesdxl_encoder.pth\" if getattr(shared.sd_model, 'is_sdxl', False) else \"taesd_encoder.pth\"\n    loaded_model = sd_vae_taesd_models.get(model_name)\n\n    if loaded_model is None:\n        model_path = os.path.join(paths_internal.models_path, \"VAE-taesd\", model_name)\n        download_model(model_path, 'https://github.com/madebyollin/taesd/raw/main/' + model_name)\n\n        if os.path.exists(model_path):\n            loaded_model = TAESDEncoder(model_path)\n            loaded_model.eval()\n            loaded_model.to(devices.device, devices.dtype)\n            sd_vae_taesd_models[model_name] = loaded_model\n        else:\n            raise FileNotFoundError('TAESD model not found')\n\n    return loaded_model.encoder\n", "modules/sd_samplers_extra.py": "import torch\nimport tqdm\nimport k_diffusion.sampling\n\n\n@torch.no_grad()\ndef restart_sampler(model, x, sigmas, extra_args=None, callback=None, disable=None, s_noise=1., restart_list=None):\n    \"\"\"Implements restart sampling in Restart Sampling for Improving Generative Processes (2023)\n    Restart_list format: {min_sigma: [ restart_steps, restart_times, max_sigma]}\n    If restart_list is None: will choose restart_list automatically, otherwise will use the given restart_list\n    \"\"\"\n    extra_args = {} if extra_args is None else extra_args\n    s_in = x.new_ones([x.shape[0]])\n    step_id = 0\n    from k_diffusion.sampling import to_d, get_sigmas_karras\n\n    def heun_step(x, old_sigma, new_sigma, second_order=True):\n        nonlocal step_id\n        denoised = model(x, old_sigma * s_in, **extra_args)\n        d = to_d(x, old_sigma, denoised)\n        if callback is not None:\n            callback({'x': x, 'i': step_id, 'sigma': new_sigma, 'sigma_hat': old_sigma, 'denoised': denoised})\n        dt = new_sigma - old_sigma\n        if new_sigma == 0 or not second_order:\n            # Euler method\n            x = x + d * dt\n        else:\n            # Heun's method\n            x_2 = x + d * dt\n            denoised_2 = model(x_2, new_sigma * s_in, **extra_args)\n            d_2 = to_d(x_2, new_sigma, denoised_2)\n            d_prime = (d + d_2) / 2\n            x = x + d_prime * dt\n        step_id += 1\n        return x\n\n    steps = sigmas.shape[0] - 1\n    if restart_list is None:\n        if steps >= 20:\n            restart_steps = 9\n            restart_times = 1\n            if steps >= 36:\n                restart_steps = steps // 4\n                restart_times = 2\n            sigmas = get_sigmas_karras(steps - restart_steps * restart_times, sigmas[-2].item(), sigmas[0].item(), device=sigmas.device)\n            restart_list = {0.1: [restart_steps + 1, restart_times, 2]}\n        else:\n            restart_list = {}\n\n    restart_list = {int(torch.argmin(abs(sigmas - key), dim=0)): value for key, value in restart_list.items()}\n\n    step_list = []\n    for i in range(len(sigmas) - 1):\n        step_list.append((sigmas[i], sigmas[i + 1]))\n        if i + 1 in restart_list:\n            restart_steps, restart_times, restart_max = restart_list[i + 1]\n            min_idx = i + 1\n            max_idx = int(torch.argmin(abs(sigmas - restart_max), dim=0))\n            if max_idx < min_idx:\n                sigma_restart = get_sigmas_karras(restart_steps, sigmas[min_idx].item(), sigmas[max_idx].item(), device=sigmas.device)[:-1]\n                while restart_times > 0:\n                    restart_times -= 1\n                    step_list.extend(zip(sigma_restart[:-1], sigma_restart[1:]))\n\n    last_sigma = None\n    for old_sigma, new_sigma in tqdm.tqdm(step_list, disable=disable):\n        if last_sigma is None:\n            last_sigma = old_sigma\n        elif last_sigma < old_sigma:\n            x = x + k_diffusion.sampling.torch.randn_like(x) * s_noise * (old_sigma ** 2 - last_sigma ** 2) ** 0.5\n        x = heun_step(x, old_sigma, new_sigma)\n        last_sigma = new_sigma\n\n    return x\n", "modules/esrgan_model.py": "from modules import modelloader, devices, errors\nfrom modules.shared import opts\nfrom modules.upscaler import Upscaler, UpscalerData\nfrom modules.upscaler_utils import upscale_with_model\n\n\nclass UpscalerESRGAN(Upscaler):\n    def __init__(self, dirname):\n        self.name = \"ESRGAN\"\n        self.model_url = \"https://github.com/cszn/KAIR/releases/download/v1.0/ESRGAN.pth\"\n        self.model_name = \"ESRGAN_4x\"\n        self.scalers = []\n        self.user_path = dirname\n        super().__init__()\n        model_paths = self.find_models(ext_filter=[\".pt\", \".pth\"])\n        scalers = []\n        if len(model_paths) == 0:\n            scaler_data = UpscalerData(self.model_name, self.model_url, self, 4)\n            scalers.append(scaler_data)\n        for file in model_paths:\n            if file.startswith(\"http\"):\n                name = self.model_name\n            else:\n                name = modelloader.friendly_name(file)\n\n            scaler_data = UpscalerData(name, file, self, 4)\n            self.scalers.append(scaler_data)\n\n    def do_upscale(self, img, selected_model):\n        try:\n            model = self.load_model(selected_model)\n        except Exception:\n            errors.report(f\"Unable to load ESRGAN model {selected_model}\", exc_info=True)\n            return img\n        model.to(devices.device_esrgan)\n        return esrgan_upscale(model, img)\n\n    def load_model(self, path: str):\n        if path.startswith(\"http\"):\n            # TODO: this doesn't use `path` at all?\n            filename = modelloader.load_file_from_url(\n                url=self.model_url,\n                model_dir=self.model_download_path,\n                file_name=f\"{self.model_name}.pth\",\n            )\n        else:\n            filename = path\n\n        return modelloader.load_spandrel_model(\n            filename,\n            device=('cpu' if devices.device_esrgan.type == 'mps' else None),\n            expected_architecture='ESRGAN',\n        )\n\n\ndef esrgan_upscale(model, img):\n    return upscale_with_model(\n        model,\n        img,\n        tile_size=opts.ESRGAN_tile,\n        tile_overlap=opts.ESRGAN_tile_overlap,\n    )\n", "modules/sd_models_xl.py": "from __future__ import annotations\n\nimport torch\n\nimport sgm.models.diffusion\nimport sgm.modules.diffusionmodules.denoiser_scaling\nimport sgm.modules.diffusionmodules.discretizer\nfrom modules import devices, shared, prompt_parser\nfrom modules import torch_utils\n\n\ndef get_learned_conditioning(self: sgm.models.diffusion.DiffusionEngine, batch: prompt_parser.SdConditioning | list[str]):\n    for embedder in self.conditioner.embedders:\n        embedder.ucg_rate = 0.0\n\n    width = getattr(batch, 'width', 1024) or 1024\n    height = getattr(batch, 'height', 1024) or 1024\n    is_negative_prompt = getattr(batch, 'is_negative_prompt', False)\n    aesthetic_score = shared.opts.sdxl_refiner_low_aesthetic_score if is_negative_prompt else shared.opts.sdxl_refiner_high_aesthetic_score\n\n    devices_args = dict(device=devices.device, dtype=devices.dtype)\n\n    sdxl_conds = {\n        \"txt\": batch,\n        \"original_size_as_tuple\": torch.tensor([height, width], **devices_args).repeat(len(batch), 1),\n        \"crop_coords_top_left\": torch.tensor([shared.opts.sdxl_crop_top, shared.opts.sdxl_crop_left], **devices_args).repeat(len(batch), 1),\n        \"target_size_as_tuple\": torch.tensor([height, width], **devices_args).repeat(len(batch), 1),\n        \"aesthetic_score\": torch.tensor([aesthetic_score], **devices_args).repeat(len(batch), 1),\n    }\n\n    force_zero_negative_prompt = is_negative_prompt and all(x == '' for x in batch)\n    c = self.conditioner(sdxl_conds, force_zero_embeddings=['txt'] if force_zero_negative_prompt else [])\n\n    return c\n\n\ndef apply_model(self: sgm.models.diffusion.DiffusionEngine, x, t, cond):\n    sd = self.model.state_dict()\n    diffusion_model_input = sd.get('diffusion_model.input_blocks.0.0.weight', None)\n    if diffusion_model_input is not None:\n        if diffusion_model_input.shape[1] == 9:\n            x = torch.cat([x] + cond['c_concat'], dim=1)\n\n    return self.model(x, t, cond)\n\n\ndef get_first_stage_encoding(self, x):  # SDXL's encode_first_stage does everything so get_first_stage_encoding is just there for compatibility\n    return x\n\n\nsgm.models.diffusion.DiffusionEngine.get_learned_conditioning = get_learned_conditioning\nsgm.models.diffusion.DiffusionEngine.apply_model = apply_model\nsgm.models.diffusion.DiffusionEngine.get_first_stage_encoding = get_first_stage_encoding\n\n\ndef encode_embedding_init_text(self: sgm.modules.GeneralConditioner, init_text, nvpt):\n    res = []\n\n    for embedder in [embedder for embedder in self.embedders if hasattr(embedder, 'encode_embedding_init_text')]:\n        encoded = embedder.encode_embedding_init_text(init_text, nvpt)\n        res.append(encoded)\n\n    return torch.cat(res, dim=1)\n\n\ndef tokenize(self: sgm.modules.GeneralConditioner, texts):\n    for embedder in [embedder for embedder in self.embedders if hasattr(embedder, 'tokenize')]:\n        return embedder.tokenize(texts)\n\n    raise AssertionError('no tokenizer available')\n\n\n\ndef process_texts(self, texts):\n    for embedder in [embedder for embedder in self.embedders if hasattr(embedder, 'process_texts')]:\n        return embedder.process_texts(texts)\n\n\ndef get_target_prompt_token_count(self, token_count):\n    for embedder in [embedder for embedder in self.embedders if hasattr(embedder, 'get_target_prompt_token_count')]:\n        return embedder.get_target_prompt_token_count(token_count)\n\n\n# those additions to GeneralConditioner make it possible to use it as model.cond_stage_model from SD1.5 in exist\nsgm.modules.GeneralConditioner.encode_embedding_init_text = encode_embedding_init_text\nsgm.modules.GeneralConditioner.tokenize = tokenize\nsgm.modules.GeneralConditioner.process_texts = process_texts\nsgm.modules.GeneralConditioner.get_target_prompt_token_count = get_target_prompt_token_count\n\n\ndef extend_sdxl(model):\n    \"\"\"this adds a bunch of parameters to make SDXL model look a bit more like SD1.5 to the rest of the codebase.\"\"\"\n\n    dtype = torch_utils.get_param(model.model.diffusion_model).dtype\n    model.model.diffusion_model.dtype = dtype\n    model.model.conditioning_key = 'crossattn'\n    model.cond_stage_key = 'txt'\n    # model.cond_stage_model will be set in sd_hijack\n\n    model.parameterization = \"v\" if isinstance(model.denoiser.scaling, sgm.modules.diffusionmodules.denoiser_scaling.VScaling) else \"eps\"\n\n    discretization = sgm.modules.diffusionmodules.discretizer.LegacyDDPMDiscretization()\n    model.alphas_cumprod = torch.asarray(discretization.alphas_cumprod, device=devices.device, dtype=torch.float32)\n\n    model.conditioner.wrapped = torch.nn.Module()\n\n\nsgm.modules.attention.print = shared.ldm_print\nsgm.modules.diffusionmodules.model.print = shared.ldm_print\nsgm.modules.diffusionmodules.openaimodel.print = shared.ldm_print\nsgm.modules.encoders.modules.print = shared.ldm_print\n\n# this gets the code to load the vanilla attention that we override\nsgm.modules.attention.SDP_IS_AVAILABLE = True\nsgm.modules.attention.XFORMERS_IS_AVAILABLE = False\n", "modules/masking.py": "from PIL import Image, ImageFilter, ImageOps\n\n\ndef get_crop_region_v2(mask, pad=0):\n    \"\"\"\n    Finds a rectangular region that contains all masked ares in a mask.\n    Returns None if mask is completely black mask (all 0)\n\n    Parameters:\n    mask: PIL.Image.Image L mode or numpy 1d array\n    pad: int number of pixels that the region will be extended on all sides\n    Returns: (x1, y1, x2, y2) | None\n\n    Introduced post 1.9.0\n    \"\"\"\n    mask = mask if isinstance(mask, Image.Image) else Image.fromarray(mask)\n    if box := mask.getbbox():\n        x1, y1, x2, y2 = box\n        return (max(x1 - pad, 0), max(y1 - pad, 0), min(x2 + pad, mask.size[0]), min(y2 + pad, mask.size[1])) if pad else box\n\n\ndef get_crop_region(mask, pad=0):\n    \"\"\"\n    Same function as get_crop_region_v2 but handles completely black mask (all 0) differently\n    when mask all black still return coordinates but the coordinates may be invalid ie x2>x1 or y2>y1\n    Notes: it is possible for the coordinates to be \"valid\" again if pad size is sufficiently large\n    (mask_size.x-pad, mask_size.y-pad, pad, pad)\n\n    Extension developer should use get_crop_region_v2 instead unless for compatibility considerations.\n    \"\"\"\n    mask = mask if isinstance(mask, Image.Image) else Image.fromarray(mask)\n    if box := get_crop_region_v2(mask, pad):\n        return box\n    x1, y1 = mask.size\n    x2 = y2 = 0\n    return max(x1 - pad, 0), max(y1 - pad, 0), min(x2 + pad, mask.size[0]), min(y2 + pad, mask.size[1])\n\n\ndef expand_crop_region(crop_region, processing_width, processing_height, image_width, image_height):\n    \"\"\"expands crop region get_crop_region() to match the ratio of the image the region will processed in; returns expanded region\n    for example, if user drew mask in a 128x32 region, and the dimensions for processing are 512x512, the region will be expanded to 128x128.\"\"\"\n\n    x1, y1, x2, y2 = crop_region\n\n    ratio_crop_region = (x2 - x1) / (y2 - y1)\n    ratio_processing = processing_width / processing_height\n\n    if ratio_crop_region > ratio_processing:\n        desired_height = (x2 - x1) / ratio_processing\n        desired_height_diff = int(desired_height - (y2-y1))\n        y1 -= desired_height_diff//2\n        y2 += desired_height_diff - desired_height_diff//2\n        if y2 >= image_height:\n            diff = y2 - image_height\n            y2 -= diff\n            y1 -= diff\n        if y1 < 0:\n            y2 -= y1\n            y1 -= y1\n        if y2 >= image_height:\n            y2 = image_height\n    else:\n        desired_width = (y2 - y1) * ratio_processing\n        desired_width_diff = int(desired_width - (x2-x1))\n        x1 -= desired_width_diff//2\n        x2 += desired_width_diff - desired_width_diff//2\n        if x2 >= image_width:\n            diff = x2 - image_width\n            x2 -= diff\n            x1 -= diff\n        if x1 < 0:\n            x2 -= x1\n            x1 -= x1\n        if x2 >= image_width:\n            x2 = image_width\n\n    return x1, y1, x2, y2\n\n\ndef fill(image, mask):\n    \"\"\"fills masked regions with colors from image using blur. Not extremely effective.\"\"\"\n\n    image_mod = Image.new('RGBA', (image.width, image.height))\n\n    image_masked = Image.new('RGBa', (image.width, image.height))\n    image_masked.paste(image.convert(\"RGBA\").convert(\"RGBa\"), mask=ImageOps.invert(mask.convert('L')))\n\n    image_masked = image_masked.convert('RGBa')\n\n    for radius, repeats in [(256, 1), (64, 1), (16, 2), (4, 4), (2, 2), (0, 1)]:\n        blurred = image_masked.filter(ImageFilter.GaussianBlur(radius)).convert('RGBA')\n        for _ in range(repeats):\n            image_mod.alpha_composite(blurred)\n\n    return image_mod.convert(\"RGB\")\n\n", "modules/sd_hijack_xlmr.py": "import torch\n\nfrom modules import sd_hijack_clip, devices\n\n\nclass FrozenXLMREmbedderWithCustomWords(sd_hijack_clip.FrozenCLIPEmbedderWithCustomWords):\n    def __init__(self, wrapped, hijack):\n        super().__init__(wrapped, hijack)\n\n        self.id_start = wrapped.config.bos_token_id\n        self.id_end = wrapped.config.eos_token_id\n        self.id_pad = wrapped.config.pad_token_id\n\n        self.comma_token = self.tokenizer.get_vocab().get(',', None)  # alt diffusion doesn't have </w> bits for comma\n\n    def encode_with_transformers(self, tokens):\n        # there's no CLIP Skip here because all hidden layers have size of 1024 and the last one uses a\n        # trained layer to transform those 1024 into 768 for unet; so you can't choose which transformer\n        # layer to work with - you have to use the last\n\n        attention_mask = (tokens != self.id_pad).to(device=tokens.device, dtype=torch.int64)\n        features = self.wrapped(input_ids=tokens, attention_mask=attention_mask)\n        z = features['projection_state']\n\n        return z\n\n    def encode_embedding_init_text(self, init_text, nvpt):\n        embedding_layer = self.wrapped.roberta.embeddings\n        ids = self.wrapped.tokenizer(init_text, max_length=nvpt, return_tensors=\"pt\", add_special_tokens=False)[\"input_ids\"]\n        embedded = embedding_layer.token_embedding.wrapped(ids.to(devices.device)).squeeze(0)\n\n        return embedded\n", "modules/ui_prompt_styles.py": "import gradio as gr\n\nfrom modules import shared, ui_common, ui_components, styles\n\nstyles_edit_symbol = '\\U0001f58c\\uFE0F'  # \ud83d\udd8c\ufe0f\nstyles_materialize_symbol = '\\U0001f4cb'  # \ud83d\udccb\nstyles_copy_symbol = '\\U0001f4dd'  # \ud83d\udcdd\n\n\ndef select_style(name):\n    style = shared.prompt_styles.styles.get(name)\n    existing = style is not None\n    empty = not name\n\n    prompt = style.prompt if style else gr.update()\n    negative_prompt = style.negative_prompt if style else gr.update()\n\n    return prompt, negative_prompt, gr.update(visible=existing), gr.update(visible=not empty)\n\n\ndef save_style(name, prompt, negative_prompt):\n    if not name:\n        return gr.update(visible=False)\n\n    existing_style = shared.prompt_styles.styles.get(name)\n    path = existing_style.path if existing_style is not None else None\n\n    style = styles.PromptStyle(name, prompt, negative_prompt, path)\n    shared.prompt_styles.styles[style.name] = style\n    shared.prompt_styles.save_styles()\n\n    return gr.update(visible=True)\n\n\ndef delete_style(name):\n    if name == \"\":\n        return\n\n    shared.prompt_styles.styles.pop(name, None)\n    shared.prompt_styles.save_styles()\n\n    return '', '', ''\n\n\ndef materialize_styles(prompt, negative_prompt, styles):\n    prompt = shared.prompt_styles.apply_styles_to_prompt(prompt, styles)\n    negative_prompt = shared.prompt_styles.apply_negative_styles_to_prompt(negative_prompt, styles)\n\n    return [gr.Textbox.update(value=prompt), gr.Textbox.update(value=negative_prompt), gr.Dropdown.update(value=[])]\n\n\ndef refresh_styles():\n    return gr.update(choices=list(shared.prompt_styles.styles)), gr.update(choices=list(shared.prompt_styles.styles))\n\n\nclass UiPromptStyles:\n    def __init__(self, tabname, main_ui_prompt, main_ui_negative_prompt):\n        self.tabname = tabname\n        self.main_ui_prompt = main_ui_prompt\n        self.main_ui_negative_prompt = main_ui_negative_prompt\n\n        with gr.Row(elem_id=f\"{tabname}_styles_row\"):\n            self.dropdown = gr.Dropdown(label=\"Styles\", show_label=False, elem_id=f\"{tabname}_styles\", choices=list(shared.prompt_styles.styles), value=[], multiselect=True, tooltip=\"Styles\")\n            edit_button = ui_components.ToolButton(value=styles_edit_symbol, elem_id=f\"{tabname}_styles_edit_button\", tooltip=\"Edit styles\")\n\n        with gr.Box(elem_id=f\"{tabname}_styles_dialog\", elem_classes=\"popup-dialog\") as styles_dialog:\n            with gr.Row():\n                self.selection = gr.Dropdown(label=\"Styles\", elem_id=f\"{tabname}_styles_edit_select\", choices=list(shared.prompt_styles.styles), value=[], allow_custom_value=True, info=\"Styles allow you to add custom text to prompt. Use the {prompt} token in style text, and it will be replaced with user's prompt when applying style. Otherwise, style's text will be added to the end of the prompt.\")\n                ui_common.create_refresh_button([self.dropdown, self.selection], shared.prompt_styles.reload, lambda: {\"choices\": list(shared.prompt_styles.styles)}, f\"refresh_{tabname}_styles\")\n                self.materialize = ui_components.ToolButton(value=styles_materialize_symbol, elem_id=f\"{tabname}_style_apply_dialog\", tooltip=\"Apply all selected styles from the style selection dropdown in main UI to the prompt.\")\n                self.copy = ui_components.ToolButton(value=styles_copy_symbol, elem_id=f\"{tabname}_style_copy\", tooltip=\"Copy main UI prompt to style.\")\n\n            with gr.Row():\n                self.prompt = gr.Textbox(label=\"Prompt\", show_label=True, elem_id=f\"{tabname}_edit_style_prompt\", lines=3, elem_classes=[\"prompt\"])\n\n            with gr.Row():\n                self.neg_prompt = gr.Textbox(label=\"Negative prompt\", show_label=True, elem_id=f\"{tabname}_edit_style_neg_prompt\", lines=3, elem_classes=[\"prompt\"])\n\n            with gr.Row():\n                self.save = gr.Button('Save', variant='primary', elem_id=f'{tabname}_edit_style_save', visible=False)\n                self.delete = gr.Button('Delete', variant='primary', elem_id=f'{tabname}_edit_style_delete', visible=False)\n                self.close = gr.Button('Close', variant='secondary', elem_id=f'{tabname}_edit_style_close')\n\n        self.selection.change(\n            fn=select_style,\n            inputs=[self.selection],\n            outputs=[self.prompt, self.neg_prompt, self.delete, self.save],\n            show_progress=False,\n        )\n\n        self.save.click(\n            fn=save_style,\n            inputs=[self.selection, self.prompt, self.neg_prompt],\n            outputs=[self.delete],\n            show_progress=False,\n        ).then(refresh_styles, outputs=[self.dropdown, self.selection], show_progress=False)\n\n        self.delete.click(\n            fn=delete_style,\n            _js='function(name){ if(name == \"\") return \"\"; return confirm(\"Delete style \" + name + \"?\") ? name : \"\"; }',\n            inputs=[self.selection],\n            outputs=[self.selection, self.prompt, self.neg_prompt],\n            show_progress=False,\n        ).then(refresh_styles, outputs=[self.dropdown, self.selection], show_progress=False)\n\n        self.setup_apply_button(self.materialize)\n\n        self.copy.click(\n            fn=lambda p, n: (p, n),\n            inputs=[main_ui_prompt, main_ui_negative_prompt],\n            outputs=[self.prompt, self.neg_prompt],\n            show_progress=False,\n        )\n\n        ui_common.setup_dialog(button_show=edit_button, dialog=styles_dialog, button_close=self.close)\n\n    def setup_apply_button(self, button):\n        button.click(\n            fn=materialize_styles,\n            inputs=[self.main_ui_prompt, self.main_ui_negative_prompt, self.dropdown],\n            outputs=[self.main_ui_prompt, self.main_ui_negative_prompt, self.dropdown],\n            show_progress=False,\n        ).then(fn=None, _js=\"function(){update_\"+self.tabname+\"_tokens(); closePopup();}\", show_progress=False)\n", "modules/scripts.py": "import os\nimport re\nimport sys\nimport inspect\nfrom collections import namedtuple\nfrom dataclasses import dataclass\n\nimport gradio as gr\n\nfrom modules import shared, paths, script_callbacks, extensions, script_loading, scripts_postprocessing, errors, timer, util\n\ntopological_sort = util.topological_sort\n\nAlwaysVisible = object()\n\nclass MaskBlendArgs:\n    def __init__(self, current_latent, nmask, init_latent, mask, blended_latent, denoiser=None, sigma=None):\n        self.current_latent = current_latent\n        self.nmask = nmask\n        self.init_latent = init_latent\n        self.mask = mask\n        self.blended_latent = blended_latent\n\n        self.denoiser = denoiser\n        self.is_final_blend = denoiser is None\n        self.sigma = sigma\n\nclass PostSampleArgs:\n    def __init__(self, samples):\n        self.samples = samples\n\nclass PostprocessImageArgs:\n    def __init__(self, image):\n        self.image = image\n\nclass PostProcessMaskOverlayArgs:\n    def __init__(self, index, mask_for_overlay, overlay_image):\n        self.index = index\n        self.mask_for_overlay = mask_for_overlay\n        self.overlay_image = overlay_image\n\nclass PostprocessBatchListArgs:\n    def __init__(self, images):\n        self.images = images\n\n\n@dataclass\nclass OnComponent:\n    component: gr.blocks.Block\n\n\nclass Script:\n    name = None\n    \"\"\"script's internal name derived from title\"\"\"\n\n    section = None\n    \"\"\"name of UI section that the script's controls will be placed into\"\"\"\n\n    filename = None\n    args_from = None\n    args_to = None\n    alwayson = False\n\n    is_txt2img = False\n    is_img2img = False\n    tabname = None\n\n    group = None\n    \"\"\"A gr.Group component that has all script's UI inside it.\"\"\"\n\n    create_group = True\n    \"\"\"If False, for alwayson scripts, a group component will not be created.\"\"\"\n\n    infotext_fields = None\n    \"\"\"if set in ui(), this is a list of pairs of gradio component + text; the text will be used when\n    parsing infotext to set the value for the component; see ui.py's txt2img_paste_fields for an example\n    \"\"\"\n\n    paste_field_names = None\n    \"\"\"if set in ui(), this is a list of names of infotext fields; the fields will be sent through the\n    various \"Send to <X>\" buttons when clicked\n    \"\"\"\n\n    api_info = None\n    \"\"\"Generated value of type modules.api.models.ScriptInfo with information about the script for API\"\"\"\n\n    on_before_component_elem_id = None\n    \"\"\"list of callbacks to be called before a component with an elem_id is created\"\"\"\n\n    on_after_component_elem_id = None\n    \"\"\"list of callbacks to be called after a component with an elem_id is created\"\"\"\n\n    setup_for_ui_only = False\n    \"\"\"If true, the script setup will only be run in Gradio UI, not in API\"\"\"\n\n    controls = None\n    \"\"\"A list of controls returned by the ui().\"\"\"\n\n    def title(self):\n        \"\"\"this function should return the title of the script. This is what will be displayed in the dropdown menu.\"\"\"\n\n        raise NotImplementedError()\n\n    def ui(self, is_img2img):\n        \"\"\"this function should create gradio UI elements. See https://gradio.app/docs/#components\n        The return value should be an array of all components that are used in processing.\n        Values of those returned components will be passed to run() and process() functions.\n        \"\"\"\n\n        pass\n\n    def show(self, is_img2img):\n        \"\"\"\n        is_img2img is True if this function is called for the img2img interface, and False otherwise\n\n        This function should return:\n         - False if the script should not be shown in UI at all\n         - True if the script should be shown in UI if it's selected in the scripts dropdown\n         - script.AlwaysVisible if the script should be shown in UI at all times\n         \"\"\"\n\n        return True\n\n    def run(self, p, *args):\n        \"\"\"\n        This function is called if the script has been selected in the script dropdown.\n        It must do all processing and return the Processed object with results, same as\n        one returned by processing.process_images.\n\n        Usually the processing is done by calling the processing.process_images function.\n\n        args contains all values returned by components from ui()\n        \"\"\"\n\n        pass\n\n    def setup(self, p, *args):\n        \"\"\"For AlwaysVisible scripts, this function is called when the processing object is set up, before any processing starts.\n        args contains all values returned by components from ui().\n        \"\"\"\n        pass\n\n    def before_process(self, p, *args):\n        \"\"\"\n        This function is called very early during processing begins for AlwaysVisible scripts.\n        You can modify the processing object (p) here, inject hooks, etc.\n        args contains all values returned by components from ui()\n        \"\"\"\n\n        pass\n\n    def process(self, p, *args):\n        \"\"\"\n        This function is called before processing begins for AlwaysVisible scripts.\n        You can modify the processing object (p) here, inject hooks, etc.\n        args contains all values returned by components from ui()\n        \"\"\"\n\n        pass\n\n    def before_process_batch(self, p, *args, **kwargs):\n        \"\"\"\n        Called before extra networks are parsed from the prompt, so you can add\n        new extra network keywords to the prompt with this callback.\n\n        **kwargs will have those items:\n          - batch_number - index of current batch, from 0 to number of batches-1\n          - prompts - list of prompts for current batch; you can change contents of this list but changing the number of entries will likely break things\n          - seeds - list of seeds for current batch\n          - subseeds - list of subseeds for current batch\n        \"\"\"\n\n        pass\n\n    def after_extra_networks_activate(self, p, *args, **kwargs):\n        \"\"\"\n        Called after extra networks activation, before conds calculation\n        allow modification of the network after extra networks activation been applied\n        won't be call if p.disable_extra_networks\n\n        **kwargs will have those items:\n          - batch_number - index of current batch, from 0 to number of batches-1\n          - prompts - list of prompts for current batch; you can change contents of this list but changing the number of entries will likely break things\n          - seeds - list of seeds for current batch\n          - subseeds - list of subseeds for current batch\n          - extra_network_data - list of ExtraNetworkParams for current stage\n        \"\"\"\n        pass\n\n    def process_batch(self, p, *args, **kwargs):\n        \"\"\"\n        Same as process(), but called for every batch.\n\n        **kwargs will have those items:\n          - batch_number - index of current batch, from 0 to number of batches-1\n          - prompts - list of prompts for current batch; you can change contents of this list but changing the number of entries will likely break things\n          - seeds - list of seeds for current batch\n          - subseeds - list of subseeds for current batch\n        \"\"\"\n\n        pass\n\n    def postprocess_batch(self, p, *args, **kwargs):\n        \"\"\"\n        Same as process_batch(), but called for every batch after it has been generated.\n\n        **kwargs will have same items as process_batch, and also:\n          - batch_number - index of current batch, from 0 to number of batches-1\n          - images - torch tensor with all generated images, with values ranging from 0 to 1;\n        \"\"\"\n\n        pass\n\n    def postprocess_batch_list(self, p, pp: PostprocessBatchListArgs, *args, **kwargs):\n        \"\"\"\n        Same as postprocess_batch(), but receives batch images as a list of 3D tensors instead of a 4D tensor.\n        This is useful when you want to update the entire batch instead of individual images.\n\n        You can modify the postprocessing object (pp) to update the images in the batch, remove images, add images, etc.\n        If the number of images is different from the batch size when returning,\n        then the script has the responsibility to also update the following attributes in the processing object (p):\n          - p.prompts\n          - p.negative_prompts\n          - p.seeds\n          - p.subseeds\n\n        **kwargs will have same items as process_batch, and also:\n          - batch_number - index of current batch, from 0 to number of batches-1\n        \"\"\"\n\n        pass\n\n    def on_mask_blend(self, p, mba: MaskBlendArgs, *args):\n        \"\"\"\n        Called in inpainting mode when the original content is blended with the inpainted content.\n        This is called at every step in the denoising process and once at the end.\n        If is_final_blend is true, this is called for the final blending stage.\n        Otherwise, denoiser and sigma are defined and may be used to inform the procedure.\n        \"\"\"\n\n        pass\n\n    def post_sample(self, p, ps: PostSampleArgs, *args):\n        \"\"\"\n        Called after the samples have been generated,\n        but before they have been decoded by the VAE, if applicable.\n        Check getattr(samples, 'already_decoded', False) to test if the images are decoded.\n        \"\"\"\n\n        pass\n\n    def postprocess_image(self, p, pp: PostprocessImageArgs, *args):\n        \"\"\"\n        Called for every image after it has been generated.\n        \"\"\"\n\n        pass\n\n    def postprocess_maskoverlay(self, p, ppmo: PostProcessMaskOverlayArgs, *args):\n        \"\"\"\n        Called for every image after it has been generated.\n        \"\"\"\n\n        pass\n\n    def postprocess_image_after_composite(self, p, pp: PostprocessImageArgs, *args):\n        \"\"\"\n        Called for every image after it has been generated.\n        Same as postprocess_image but after inpaint_full_res composite\n        So that it operates on the full image instead of the inpaint_full_res crop region.\n        \"\"\"\n\n        pass\n\n    def postprocess(self, p, processed, *args):\n        \"\"\"\n        This function is called after processing ends for AlwaysVisible scripts.\n        args contains all values returned by components from ui()\n        \"\"\"\n\n        pass\n\n    def before_component(self, component, **kwargs):\n        \"\"\"\n        Called before a component is created.\n        Use elem_id/label fields of kwargs to figure out which component it is.\n        This can be useful to inject your own components somewhere in the middle of vanilla UI.\n        You can return created components in the ui() function to add them to the list of arguments for your processing functions\n        \"\"\"\n\n        pass\n\n    def after_component(self, component, **kwargs):\n        \"\"\"\n        Called after a component is created. Same as above.\n        \"\"\"\n\n        pass\n\n    def on_before_component(self, callback, *, elem_id):\n        \"\"\"\n        Calls callback before a component is created. The callback function is called with a single argument of type OnComponent.\n\n        May be called in show() or ui() - but it may be too late in latter as some components may already be created.\n\n        This function is an alternative to before_component in that it also cllows to run before a component is created, but\n        it doesn't require to be called for every created component - just for the one you need.\n        \"\"\"\n        if self.on_before_component_elem_id is None:\n            self.on_before_component_elem_id = []\n\n        self.on_before_component_elem_id.append((elem_id, callback))\n\n    def on_after_component(self, callback, *, elem_id):\n        \"\"\"\n        Calls callback after a component is created. The callback function is called with a single argument of type OnComponent.\n        \"\"\"\n        if self.on_after_component_elem_id is None:\n            self.on_after_component_elem_id = []\n\n        self.on_after_component_elem_id.append((elem_id, callback))\n\n    def describe(self):\n        \"\"\"unused\"\"\"\n        return \"\"\n\n    def elem_id(self, item_id):\n        \"\"\"helper function to generate id for a HTML element, constructs final id out of script name, tab and user-supplied item_id\"\"\"\n\n        need_tabname = self.show(True) == self.show(False)\n        tabkind = 'img2img' if self.is_img2img else 'txt2img'\n        tabname = f\"{tabkind}_\" if need_tabname else \"\"\n        title = re.sub(r'[^a-z_0-9]', '', re.sub(r'\\s', '_', self.title().lower()))\n\n        return f'script_{tabname}{title}_{item_id}'\n\n    def before_hr(self, p, *args):\n        \"\"\"\n        This function is called before hires fix start.\n        \"\"\"\n        pass\n\n\nclass ScriptBuiltinUI(Script):\n    setup_for_ui_only = True\n\n    def elem_id(self, item_id):\n        \"\"\"helper function to generate id for a HTML element, constructs final id out of tab and user-supplied item_id\"\"\"\n\n        need_tabname = self.show(True) == self.show(False)\n        tabname = ('img2img' if self.is_img2img else 'txt2img') + \"_\" if need_tabname else \"\"\n\n        return f'{tabname}{item_id}'\n\n    def show(self, is_img2img):\n        return AlwaysVisible\n\n\ncurrent_basedir = paths.script_path\n\n\ndef basedir():\n    \"\"\"returns the base directory for the current script. For scripts in the main scripts directory,\n    this is the main directory (where webui.py resides), and for scripts in extensions directory\n    (ie extensions/aesthetic/script/aesthetic.py), this is extension's directory (extensions/aesthetic)\n    \"\"\"\n    return current_basedir\n\n\nScriptFile = namedtuple(\"ScriptFile\", [\"basedir\", \"filename\", \"path\"])\n\nscripts_data = []\npostprocessing_scripts_data = []\nScriptClassData = namedtuple(\"ScriptClassData\", [\"script_class\", \"path\", \"basedir\", \"module\"])\n\n\n@dataclass\nclass ScriptWithDependencies:\n    script_canonical_name: str\n    file: ScriptFile\n    requires: list\n    load_before: list\n    load_after: list\n\n\ndef list_scripts(scriptdirname, extension, *, include_extensions=True):\n    scripts = {}\n\n    loaded_extensions = {ext.canonical_name: ext for ext in extensions.active()}\n    loaded_extensions_scripts = {ext.canonical_name: [] for ext in extensions.active()}\n\n    # build script dependency map\n    root_script_basedir = os.path.join(paths.script_path, scriptdirname)\n    if os.path.exists(root_script_basedir):\n        for filename in sorted(os.listdir(root_script_basedir)):\n            if not os.path.isfile(os.path.join(root_script_basedir, filename)):\n                continue\n\n            if os.path.splitext(filename)[1].lower() != extension:\n                continue\n\n            script_file = ScriptFile(paths.script_path, filename, os.path.join(root_script_basedir, filename))\n            scripts[filename] = ScriptWithDependencies(filename, script_file, [], [], [])\n\n    if include_extensions:\n        for ext in extensions.active():\n            extension_scripts_list = ext.list_files(scriptdirname, extension)\n            for extension_script in extension_scripts_list:\n                if not os.path.isfile(extension_script.path):\n                    continue\n\n                script_canonical_name = (\"builtin/\" if ext.is_builtin else \"\") + ext.canonical_name + \"/\" + extension_script.filename\n                relative_path = scriptdirname + \"/\" + extension_script.filename\n\n                script = ScriptWithDependencies(\n                    script_canonical_name=script_canonical_name,\n                    file=extension_script,\n                    requires=ext.metadata.get_script_requirements(\"Requires\", relative_path, scriptdirname),\n                    load_before=ext.metadata.get_script_requirements(\"Before\", relative_path, scriptdirname),\n                    load_after=ext.metadata.get_script_requirements(\"After\", relative_path, scriptdirname),\n                )\n\n                scripts[script_canonical_name] = script\n                loaded_extensions_scripts[ext.canonical_name].append(script)\n\n    for script_canonical_name, script in scripts.items():\n        # load before requires inverse dependency\n        # in this case, append the script name into the load_after list of the specified script\n        for load_before in script.load_before:\n            # if this requires an individual script to be loaded before\n            other_script = scripts.get(load_before)\n            if other_script:\n                other_script.load_after.append(script_canonical_name)\n\n            # if this requires an extension\n            other_extension_scripts = loaded_extensions_scripts.get(load_before)\n            if other_extension_scripts:\n                for other_script in other_extension_scripts:\n                    other_script.load_after.append(script_canonical_name)\n\n        # if After mentions an extension, remove it and instead add all of its scripts\n        for load_after in list(script.load_after):\n            if load_after not in scripts and load_after in loaded_extensions_scripts:\n                script.load_after.remove(load_after)\n\n                for other_script in loaded_extensions_scripts.get(load_after, []):\n                    script.load_after.append(other_script.script_canonical_name)\n\n    dependencies = {}\n\n    for script_canonical_name, script in scripts.items():\n        for required_script in script.requires:\n            if required_script not in scripts and required_script not in loaded_extensions:\n                errors.report(f'Script \"{script_canonical_name}\" requires \"{required_script}\" to be loaded, but it is not.', exc_info=False)\n\n        dependencies[script_canonical_name] = script.load_after\n\n    ordered_scripts = topological_sort(dependencies)\n    scripts_list = [scripts[script_canonical_name].file for script_canonical_name in ordered_scripts]\n\n    return scripts_list\n\n\ndef list_files_with_name(filename):\n    res = []\n\n    dirs = [paths.script_path] + [ext.path for ext in extensions.active()]\n\n    for dirpath in dirs:\n        if not os.path.isdir(dirpath):\n            continue\n\n        path = os.path.join(dirpath, filename)\n        if os.path.isfile(path):\n            res.append(path)\n\n    return res\n\n\ndef load_scripts():\n    global current_basedir\n    scripts_data.clear()\n    postprocessing_scripts_data.clear()\n    script_callbacks.clear_callbacks()\n\n    scripts_list = list_scripts(\"scripts\", \".py\") + list_scripts(\"modules/processing_scripts\", \".py\", include_extensions=False)\n\n    syspath = sys.path\n\n    def register_scripts_from_module(module):\n        for script_class in module.__dict__.values():\n            if not inspect.isclass(script_class):\n                continue\n\n            if issubclass(script_class, Script):\n                scripts_data.append(ScriptClassData(script_class, scriptfile.path, scriptfile.basedir, module))\n            elif issubclass(script_class, scripts_postprocessing.ScriptPostprocessing):\n                postprocessing_scripts_data.append(ScriptClassData(script_class, scriptfile.path, scriptfile.basedir, module))\n\n    # here the scripts_list is already ordered\n    # processing_script is not considered though\n    for scriptfile in scripts_list:\n        try:\n            if scriptfile.basedir != paths.script_path:\n                sys.path = [scriptfile.basedir] + sys.path\n            current_basedir = scriptfile.basedir\n\n            script_module = script_loading.load_module(scriptfile.path)\n            register_scripts_from_module(script_module)\n\n        except Exception:\n            errors.report(f\"Error loading script: {scriptfile.filename}\", exc_info=True)\n\n        finally:\n            sys.path = syspath\n            current_basedir = paths.script_path\n            timer.startup_timer.record(scriptfile.filename)\n\n    global scripts_txt2img, scripts_img2img, scripts_postproc\n\n    scripts_txt2img = ScriptRunner()\n    scripts_img2img = ScriptRunner()\n    scripts_postproc = scripts_postprocessing.ScriptPostprocessingRunner()\n\n\ndef wrap_call(func, filename, funcname, *args, default=None, **kwargs):\n    try:\n        return func(*args, **kwargs)\n    except Exception:\n        errors.report(f\"Error calling: {filename}/{funcname}\", exc_info=True)\n\n    return default\n\n\nclass ScriptRunner:\n    def __init__(self):\n        self.scripts = []\n        self.selectable_scripts = []\n        self.alwayson_scripts = []\n        self.titles = []\n        self.title_map = {}\n        self.infotext_fields = []\n        self.paste_field_names = []\n        self.inputs = [None]\n\n        self.callback_map = {}\n        self.callback_names = [\n            'before_process',\n            'process',\n            'before_process_batch',\n            'after_extra_networks_activate',\n            'process_batch',\n            'postprocess',\n            'postprocess_batch',\n            'postprocess_batch_list',\n            'post_sample',\n            'on_mask_blend',\n            'postprocess_image',\n            'postprocess_maskoverlay',\n            'postprocess_image_after_composite',\n            'before_component',\n            'after_component',\n        ]\n\n        self.on_before_component_elem_id = {}\n        \"\"\"dict of callbacks to be called before an element is created; key=elem_id, value=list of callbacks\"\"\"\n\n        self.on_after_component_elem_id = {}\n        \"\"\"dict of callbacks to be called after an element is created; key=elem_id, value=list of callbacks\"\"\"\n\n    def initialize_scripts(self, is_img2img):\n        from modules import scripts_auto_postprocessing\n\n        self.scripts.clear()\n        self.alwayson_scripts.clear()\n        self.selectable_scripts.clear()\n\n        auto_processing_scripts = scripts_auto_postprocessing.create_auto_preprocessing_script_data()\n\n        for script_data in auto_processing_scripts + scripts_data:\n            try:\n                script = script_data.script_class()\n            except Exception:\n                errors.report(f\"Error # failed to initialize Script {script_data.module}: \", exc_info=True)\n                continue\n\n            script.filename = script_data.path\n            script.is_txt2img = not is_img2img\n            script.is_img2img = is_img2img\n            script.tabname = \"img2img\" if is_img2img else \"txt2img\"\n\n            visibility = script.show(script.is_img2img)\n\n            if visibility == AlwaysVisible:\n                self.scripts.append(script)\n                self.alwayson_scripts.append(script)\n                script.alwayson = True\n\n            elif visibility:\n                self.scripts.append(script)\n                self.selectable_scripts.append(script)\n\n        self.callback_map.clear()\n\n        self.apply_on_before_component_callbacks()\n\n    def apply_on_before_component_callbacks(self):\n        for script in self.scripts:\n            on_before = script.on_before_component_elem_id or []\n            on_after = script.on_after_component_elem_id or []\n\n            for elem_id, callback in on_before:\n                if elem_id not in self.on_before_component_elem_id:\n                    self.on_before_component_elem_id[elem_id] = []\n\n                self.on_before_component_elem_id[elem_id].append((callback, script))\n\n            for elem_id, callback in on_after:\n                if elem_id not in self.on_after_component_elem_id:\n                    self.on_after_component_elem_id[elem_id] = []\n\n                self.on_after_component_elem_id[elem_id].append((callback, script))\n\n            on_before.clear()\n            on_after.clear()\n\n    def create_script_ui(self, script):\n\n        script.args_from = len(self.inputs)\n        script.args_to = len(self.inputs)\n\n        try:\n            self.create_script_ui_inner(script)\n        except Exception:\n            errors.report(f\"Error creating UI for {script.name}: \", exc_info=True)\n\n    def create_script_ui_inner(self, script):\n        import modules.api.models as api_models\n\n        controls = wrap_call(script.ui, script.filename, \"ui\", script.is_img2img)\n        script.controls = controls\n\n        if controls is None:\n            return\n\n        script.name = wrap_call(script.title, script.filename, \"title\", default=script.filename).lower()\n\n        api_args = []\n\n        for control in controls:\n            control.custom_script_source = os.path.basename(script.filename)\n\n            arg_info = api_models.ScriptArg(label=control.label or \"\")\n\n            for field in (\"value\", \"minimum\", \"maximum\", \"step\"):\n                v = getattr(control, field, None)\n                if v is not None:\n                    setattr(arg_info, field, v)\n\n            choices = getattr(control, 'choices', None)  # as of gradio 3.41, some items in choices are strings, and some are tuples where the first elem is the string\n            if choices is not None:\n                arg_info.choices = [x[0] if isinstance(x, tuple) else x for x in choices]\n\n            api_args.append(arg_info)\n\n        script.api_info = api_models.ScriptInfo(\n            name=script.name,\n            is_img2img=script.is_img2img,\n            is_alwayson=script.alwayson,\n            args=api_args,\n        )\n\n        if script.infotext_fields is not None:\n            self.infotext_fields += script.infotext_fields\n\n        if script.paste_field_names is not None:\n            self.paste_field_names += script.paste_field_names\n\n        self.inputs += controls\n        script.args_to = len(self.inputs)\n\n    def setup_ui_for_section(self, section, scriptlist=None):\n        if scriptlist is None:\n            scriptlist = self.alwayson_scripts\n\n        for script in scriptlist:\n            if script.alwayson and script.section != section:\n                continue\n\n            if script.create_group:\n                with gr.Group(visible=script.alwayson) as group:\n                    self.create_script_ui(script)\n\n                script.group = group\n            else:\n                self.create_script_ui(script)\n\n    def prepare_ui(self):\n        self.inputs = [None]\n\n    def setup_ui(self):\n        all_titles = [wrap_call(script.title, script.filename, \"title\") or script.filename for script in self.scripts]\n        self.title_map = {title.lower(): script for title, script in zip(all_titles, self.scripts)}\n        self.titles = [wrap_call(script.title, script.filename, \"title\") or f\"{script.filename} [error]\" for script in self.selectable_scripts]\n\n        self.setup_ui_for_section(None)\n\n        dropdown = gr.Dropdown(label=\"Script\", elem_id=\"script_list\", choices=[\"None\"] + self.titles, value=\"None\", type=\"index\")\n        self.inputs[0] = dropdown\n\n        self.setup_ui_for_section(None, self.selectable_scripts)\n\n        def select_script(script_index):\n            if script_index is None:\n                script_index = 0\n            selected_script = self.selectable_scripts[script_index - 1] if script_index>0 else None\n\n            return [gr.update(visible=selected_script == s) for s in self.selectable_scripts]\n\n        def init_field(title):\n            \"\"\"called when an initial value is set from ui-config.json to show script's UI components\"\"\"\n\n            if title == 'None':\n                return\n\n            script_index = self.titles.index(title)\n            self.selectable_scripts[script_index].group.visible = True\n\n        dropdown.init_field = init_field\n\n        dropdown.change(\n            fn=select_script,\n            inputs=[dropdown],\n            outputs=[script.group for script in self.selectable_scripts]\n        )\n\n        self.script_load_ctr = 0\n\n        def onload_script_visibility(params):\n            title = params.get('Script', None)\n            if title:\n                try:\n                    title_index = self.titles.index(title)\n                    visibility = title_index == self.script_load_ctr\n                    self.script_load_ctr = (self.script_load_ctr + 1) % len(self.titles)\n                    return gr.update(visible=visibility)\n                except ValueError:\n                    params['Script'] = None\n                    massage = f'Cannot find Script: \"{title}\"'\n                    print(massage)\n                    gr.Warning(massage)\n            return gr.update(visible=False)\n\n        self.infotext_fields.append((dropdown, lambda x: gr.update(value=x.get('Script', 'None'))))\n        self.infotext_fields.extend([(script.group, onload_script_visibility) for script in self.selectable_scripts])\n\n        self.apply_on_before_component_callbacks()\n\n        return self.inputs\n\n    def run(self, p, *args):\n        script_index = args[0]\n\n        if script_index == 0 or script_index is None:\n            return None\n\n        script = self.selectable_scripts[script_index-1]\n\n        if script is None:\n            return None\n\n        script_args = args[script.args_from:script.args_to]\n        processed = script.run(p, *script_args)\n\n        shared.total_tqdm.clear()\n\n        return processed\n\n    def list_scripts_for_method(self, method_name):\n        if method_name in ('before_component', 'after_component'):\n            return self.scripts\n        else:\n            return self.alwayson_scripts\n\n    def create_ordered_callbacks_list(self,  method_name, *, enable_user_sort=True):\n        script_list = self.list_scripts_for_method(method_name)\n        category = f'script_{method_name}'\n        callbacks = []\n\n        for script in script_list:\n            if getattr(script.__class__, method_name, None) == getattr(Script, method_name, None):\n                continue\n\n            script_callbacks.add_callback(callbacks, script, category=category, name=script.__class__.__name__, filename=script.filename)\n\n        return script_callbacks.sort_callbacks(category, callbacks, enable_user_sort=enable_user_sort)\n\n    def ordered_callbacks(self, method_name, *, enable_user_sort=True):\n        script_list = self.list_scripts_for_method(method_name)\n        category = f'script_{method_name}'\n\n        scrpts_len, callbacks = self.callback_map.get(category, (-1, None))\n\n        if callbacks is None or scrpts_len != len(script_list):\n            callbacks = self.create_ordered_callbacks_list(method_name, enable_user_sort=enable_user_sort)\n            self.callback_map[category] = len(script_list), callbacks\n\n        return callbacks\n\n    def ordered_scripts(self, method_name):\n        return [x.callback for x in self.ordered_callbacks(method_name)]\n\n    def before_process(self, p):\n        for script in self.ordered_scripts('before_process'):\n            try:\n                script_args = p.script_args[script.args_from:script.args_to]\n                script.before_process(p, *script_args)\n            except Exception:\n                errors.report(f\"Error running before_process: {script.filename}\", exc_info=True)\n\n    def process(self, p):\n        for script in self.ordered_scripts('process'):\n            try:\n                script_args = p.script_args[script.args_from:script.args_to]\n                script.process(p, *script_args)\n            except Exception:\n                errors.report(f\"Error running process: {script.filename}\", exc_info=True)\n\n    def before_process_batch(self, p, **kwargs):\n        for script in self.ordered_scripts('before_process_batch'):\n            try:\n                script_args = p.script_args[script.args_from:script.args_to]\n                script.before_process_batch(p, *script_args, **kwargs)\n            except Exception:\n                errors.report(f\"Error running before_process_batch: {script.filename}\", exc_info=True)\n\n    def after_extra_networks_activate(self, p, **kwargs):\n        for script in self.ordered_scripts('after_extra_networks_activate'):\n            try:\n                script_args = p.script_args[script.args_from:script.args_to]\n                script.after_extra_networks_activate(p, *script_args, **kwargs)\n            except Exception:\n                errors.report(f\"Error running after_extra_networks_activate: {script.filename}\", exc_info=True)\n\n    def process_batch(self, p, **kwargs):\n        for script in self.ordered_scripts('process_batch'):\n            try:\n                script_args = p.script_args[script.args_from:script.args_to]\n                script.process_batch(p, *script_args, **kwargs)\n            except Exception:\n                errors.report(f\"Error running process_batch: {script.filename}\", exc_info=True)\n\n    def postprocess(self, p, processed):\n        for script in self.ordered_scripts('postprocess'):\n            try:\n                script_args = p.script_args[script.args_from:script.args_to]\n                script.postprocess(p, processed, *script_args)\n            except Exception:\n                errors.report(f\"Error running postprocess: {script.filename}\", exc_info=True)\n\n    def postprocess_batch(self, p, images, **kwargs):\n        for script in self.ordered_scripts('postprocess_batch'):\n            try:\n                script_args = p.script_args[script.args_from:script.args_to]\n                script.postprocess_batch(p, *script_args, images=images, **kwargs)\n            except Exception:\n                errors.report(f\"Error running postprocess_batch: {script.filename}\", exc_info=True)\n\n    def postprocess_batch_list(self, p, pp: PostprocessBatchListArgs, **kwargs):\n        for script in self.ordered_scripts('postprocess_batch_list'):\n            try:\n                script_args = p.script_args[script.args_from:script.args_to]\n                script.postprocess_batch_list(p, pp, *script_args, **kwargs)\n            except Exception:\n                errors.report(f\"Error running postprocess_batch_list: {script.filename}\", exc_info=True)\n\n    def post_sample(self, p, ps: PostSampleArgs):\n        for script in self.ordered_scripts('post_sample'):\n            try:\n                script_args = p.script_args[script.args_from:script.args_to]\n                script.post_sample(p, ps, *script_args)\n            except Exception:\n                errors.report(f\"Error running post_sample: {script.filename}\", exc_info=True)\n\n    def on_mask_blend(self, p, mba: MaskBlendArgs):\n        for script in self.ordered_scripts('on_mask_blend'):\n            try:\n                script_args = p.script_args[script.args_from:script.args_to]\n                script.on_mask_blend(p, mba, *script_args)\n            except Exception:\n                errors.report(f\"Error running post_sample: {script.filename}\", exc_info=True)\n\n    def postprocess_image(self, p, pp: PostprocessImageArgs):\n        for script in self.ordered_scripts('postprocess_image'):\n            try:\n                script_args = p.script_args[script.args_from:script.args_to]\n                script.postprocess_image(p, pp, *script_args)\n            except Exception:\n                errors.report(f\"Error running postprocess_image: {script.filename}\", exc_info=True)\n\n    def postprocess_maskoverlay(self, p, ppmo: PostProcessMaskOverlayArgs):\n        for script in self.ordered_scripts('postprocess_maskoverlay'):\n            try:\n                script_args = p.script_args[script.args_from:script.args_to]\n                script.postprocess_maskoverlay(p, ppmo, *script_args)\n            except Exception:\n                errors.report(f\"Error running postprocess_image: {script.filename}\", exc_info=True)\n\n    def postprocess_image_after_composite(self, p, pp: PostprocessImageArgs):\n        for script in self.ordered_scripts('postprocess_image_after_composite'):\n            try:\n                script_args = p.script_args[script.args_from:script.args_to]\n                script.postprocess_image_after_composite(p, pp, *script_args)\n            except Exception:\n                errors.report(f\"Error running postprocess_image_after_composite: {script.filename}\", exc_info=True)\n\n    def before_component(self, component, **kwargs):\n        for callback, script in self.on_before_component_elem_id.get(kwargs.get(\"elem_id\"), []):\n            try:\n                callback(OnComponent(component=component))\n            except Exception:\n                errors.report(f\"Error running on_before_component: {script.filename}\", exc_info=True)\n\n        for script in self.ordered_scripts('before_component'):\n            try:\n                script.before_component(component, **kwargs)\n            except Exception:\n                errors.report(f\"Error running before_component: {script.filename}\", exc_info=True)\n\n    def after_component(self, component, **kwargs):\n        for callback, script in self.on_after_component_elem_id.get(component.elem_id, []):\n            try:\n                callback(OnComponent(component=component))\n            except Exception:\n                errors.report(f\"Error running on_after_component: {script.filename}\", exc_info=True)\n\n        for script in self.ordered_scripts('after_component'):\n            try:\n                script.after_component(component, **kwargs)\n            except Exception:\n                errors.report(f\"Error running after_component: {script.filename}\", exc_info=True)\n\n    def script(self, title):\n        return self.title_map.get(title.lower())\n\n    def reload_sources(self, cache):\n        for si, script in list(enumerate(self.scripts)):\n            args_from = script.args_from\n            args_to = script.args_to\n            filename = script.filename\n\n            module = cache.get(filename, None)\n            if module is None:\n                module = script_loading.load_module(script.filename)\n                cache[filename] = module\n\n            for script_class in module.__dict__.values():\n                if type(script_class) == type and issubclass(script_class, Script):\n                    self.scripts[si] = script_class()\n                    self.scripts[si].filename = filename\n                    self.scripts[si].args_from = args_from\n                    self.scripts[si].args_to = args_to\n\n    def before_hr(self, p):\n        for script in self.ordered_scripts('before_hr'):\n            try:\n                script_args = p.script_args[script.args_from:script.args_to]\n                script.before_hr(p, *script_args)\n            except Exception:\n                errors.report(f\"Error running before_hr: {script.filename}\", exc_info=True)\n\n    def setup_scrips(self, p, *, is_ui=True):\n        for script in self.ordered_scripts('setup'):\n            if not is_ui and script.setup_for_ui_only:\n                continue\n\n            try:\n                script_args = p.script_args[script.args_from:script.args_to]\n                script.setup(p, *script_args)\n            except Exception:\n                errors.report(f\"Error running setup: {script.filename}\", exc_info=True)\n\n    def set_named_arg(self, args, script_name, arg_elem_id, value, fuzzy=False):\n        \"\"\"Locate an arg of a specific script in script_args and set its value\n        Args:\n            args: all script args of process p, p.script_args\n            script_name: the name target script name to\n            arg_elem_id: the elem_id of the target arg\n            value: the value to set\n            fuzzy: if True, arg_elem_id can be a substring of the control.elem_id else exact match\n        Returns:\n            Updated script args\n        when script_name in not found or arg_elem_id is not found in script controls, raise RuntimeError\n        \"\"\"\n        script = next((x for x in self.scripts if x.name == script_name), None)\n        if script is None:\n            raise RuntimeError(f\"script {script_name} not found\")\n\n        for i, control in enumerate(script.controls):\n            if arg_elem_id in control.elem_id if fuzzy else arg_elem_id == control.elem_id:\n                index = script.args_from + i\n\n                if isinstance(args, tuple):\n                    return args[:index] + (value,) + args[index + 1:]\n                elif isinstance(args, list):\n                    args[index] = value\n                    return args\n                else:\n                    raise RuntimeError(f\"args is not a list or tuple, but {type(args)}\")\n        raise RuntimeError(f\"arg_elem_id {arg_elem_id} not found in script {script_name}\")\n\n\nscripts_txt2img: ScriptRunner = None\nscripts_img2img: ScriptRunner = None\nscripts_postproc: scripts_postprocessing.ScriptPostprocessingRunner = None\nscripts_current: ScriptRunner = None\n\n\ndef reload_script_body_only():\n    cache = {}\n    scripts_txt2img.reload_sources(cache)\n    scripts_img2img.reload_sources(cache)\n\n\nreload_scripts = load_scripts  # compatibility alias\n", "modules/sd_disable_initialization.py": "import ldm.modules.encoders.modules\nimport open_clip\nimport torch\nimport transformers.utils.hub\n\nfrom modules import shared\n\n\nclass ReplaceHelper:\n    def __init__(self):\n        self.replaced = []\n\n    def replace(self, obj, field, func):\n        original = getattr(obj, field, None)\n        if original is None:\n            return None\n\n        self.replaced.append((obj, field, original))\n        setattr(obj, field, func)\n\n        return original\n\n    def restore(self):\n        for obj, field, original in self.replaced:\n            setattr(obj, field, original)\n\n        self.replaced.clear()\n\n\nclass DisableInitialization(ReplaceHelper):\n    \"\"\"\n    When an object of this class enters a `with` block, it starts:\n    - preventing torch's layer initialization functions from working\n    - changes CLIP and OpenCLIP to not download model weights\n    - changes CLIP to not make requests to check if there is a new version of a file you already have\n\n    When it leaves the block, it reverts everything to how it was before.\n\n    Use it like this:\n    ```\n    with DisableInitialization():\n        do_things()\n    ```\n    \"\"\"\n\n    def __init__(self, disable_clip=True):\n        super().__init__()\n        self.disable_clip = disable_clip\n\n    def replace(self, obj, field, func):\n        original = getattr(obj, field, None)\n        if original is None:\n            return None\n\n        self.replaced.append((obj, field, original))\n        setattr(obj, field, func)\n\n        return original\n\n    def __enter__(self):\n        def do_nothing(*args, **kwargs):\n            pass\n\n        def create_model_and_transforms_without_pretrained(*args, pretrained=None, **kwargs):\n            return self.create_model_and_transforms(*args, pretrained=None, **kwargs)\n\n        def CLIPTextModel_from_pretrained(pretrained_model_name_or_path, *model_args, **kwargs):\n            res = self.CLIPTextModel_from_pretrained(None, *model_args, config=pretrained_model_name_or_path, state_dict={}, **kwargs)\n            res.name_or_path = pretrained_model_name_or_path\n            return res\n\n        def transformers_modeling_utils_load_pretrained_model(*args, **kwargs):\n            args = args[0:3] + ('/', ) + args[4:]  # resolved_archive_file; must set it to something to prevent what seems to be a bug\n            return self.transformers_modeling_utils_load_pretrained_model(*args, **kwargs)\n\n        def transformers_utils_hub_get_file_from_cache(original, url, *args, **kwargs):\n\n            # this file is always 404, prevent making request\n            if url == 'https://huggingface.co/openai/clip-vit-large-patch14/resolve/main/added_tokens.json' or url == 'openai/clip-vit-large-patch14' and args[0] == 'added_tokens.json':\n                return None\n\n            try:\n                res = original(url, *args, local_files_only=True, **kwargs)\n                if res is None:\n                    res = original(url, *args, local_files_only=False, **kwargs)\n                return res\n            except Exception:\n                return original(url, *args, local_files_only=False, **kwargs)\n\n        def transformers_utils_hub_get_from_cache(url, *args, local_files_only=False, **kwargs):\n            return transformers_utils_hub_get_file_from_cache(self.transformers_utils_hub_get_from_cache, url, *args, **kwargs)\n\n        def transformers_tokenization_utils_base_cached_file(url, *args, local_files_only=False, **kwargs):\n            return transformers_utils_hub_get_file_from_cache(self.transformers_tokenization_utils_base_cached_file, url, *args, **kwargs)\n\n        def transformers_configuration_utils_cached_file(url, *args, local_files_only=False, **kwargs):\n            return transformers_utils_hub_get_file_from_cache(self.transformers_configuration_utils_cached_file, url, *args, **kwargs)\n\n        self.replace(torch.nn.init, 'kaiming_uniform_', do_nothing)\n        self.replace(torch.nn.init, '_no_grad_normal_', do_nothing)\n        self.replace(torch.nn.init, '_no_grad_uniform_', do_nothing)\n\n        if self.disable_clip:\n            self.create_model_and_transforms = self.replace(open_clip, 'create_model_and_transforms', create_model_and_transforms_without_pretrained)\n            self.CLIPTextModel_from_pretrained = self.replace(ldm.modules.encoders.modules.CLIPTextModel, 'from_pretrained', CLIPTextModel_from_pretrained)\n            self.transformers_modeling_utils_load_pretrained_model = self.replace(transformers.modeling_utils.PreTrainedModel, '_load_pretrained_model', transformers_modeling_utils_load_pretrained_model)\n            self.transformers_tokenization_utils_base_cached_file = self.replace(transformers.tokenization_utils_base, 'cached_file', transformers_tokenization_utils_base_cached_file)\n            self.transformers_configuration_utils_cached_file = self.replace(transformers.configuration_utils, 'cached_file', transformers_configuration_utils_cached_file)\n            self.transformers_utils_hub_get_from_cache = self.replace(transformers.utils.hub, 'get_from_cache', transformers_utils_hub_get_from_cache)\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.restore()\n\n\nclass InitializeOnMeta(ReplaceHelper):\n    \"\"\"\n    Context manager that causes all parameters for linear/conv2d/mha layers to be allocated on meta device,\n    which results in those parameters having no values and taking no memory. model.to() will be broken and\n    will need to be repaired by using LoadStateDictOnMeta below when loading params from state dict.\n\n    Usage:\n    ```\n    with sd_disable_initialization.InitializeOnMeta():\n        sd_model = instantiate_from_config(sd_config.model)\n    ```\n    \"\"\"\n\n    def __enter__(self):\n        if shared.cmd_opts.disable_model_loading_ram_optimization:\n            return\n\n        def set_device(x):\n            x[\"device\"] = \"meta\"\n            return x\n\n        linear_init = self.replace(torch.nn.Linear, '__init__', lambda *args, **kwargs: linear_init(*args, **set_device(kwargs)))\n        conv2d_init = self.replace(torch.nn.Conv2d, '__init__', lambda *args, **kwargs: conv2d_init(*args, **set_device(kwargs)))\n        mha_init = self.replace(torch.nn.MultiheadAttention, '__init__', lambda *args, **kwargs: mha_init(*args, **set_device(kwargs)))\n        self.replace(torch.nn.Module, 'to', lambda *args, **kwargs: None)\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.restore()\n\n\nclass LoadStateDictOnMeta(ReplaceHelper):\n    \"\"\"\n    Context manager that allows to read parameters from state_dict into a model that has some of its parameters in the meta device.\n    As those parameters are read from state_dict, they will be deleted from it, so by the end state_dict will be mostly empty, to save memory.\n    Meant to be used together with InitializeOnMeta above.\n\n    Usage:\n    ```\n    with sd_disable_initialization.LoadStateDictOnMeta(state_dict):\n        model.load_state_dict(state_dict, strict=False)\n    ```\n    \"\"\"\n\n    def __init__(self, state_dict, device, weight_dtype_conversion=None):\n        super().__init__()\n        self.state_dict = state_dict\n        self.device = device\n        self.weight_dtype_conversion = weight_dtype_conversion or {}\n        self.default_dtype = self.weight_dtype_conversion.get('')\n\n    def get_weight_dtype(self, key):\n        key_first_term, _ = key.split('.', 1)\n        return self.weight_dtype_conversion.get(key_first_term, self.default_dtype)\n\n    def __enter__(self):\n        if shared.cmd_opts.disable_model_loading_ram_optimization:\n            return\n\n        sd = self.state_dict\n        device = self.device\n\n        def load_from_state_dict(original, module, state_dict, prefix, *args, **kwargs):\n            used_param_keys = []\n\n            for name, param in module._parameters.items():\n                if param is None:\n                    continue\n\n                key = prefix + name\n                sd_param = sd.pop(key, None)\n                if sd_param is not None:\n                    state_dict[key] = sd_param.to(dtype=self.get_weight_dtype(key))\n                    used_param_keys.append(key)\n\n                if param.is_meta:\n                    dtype = sd_param.dtype if sd_param is not None else param.dtype\n                    module._parameters[name] = torch.nn.parameter.Parameter(torch.zeros_like(param, device=device, dtype=dtype), requires_grad=param.requires_grad)\n\n            for name in module._buffers:\n                key = prefix + name\n\n                sd_param = sd.pop(key, None)\n                if sd_param is not None:\n                    state_dict[key] = sd_param\n                    used_param_keys.append(key)\n\n            original(module, state_dict, prefix, *args, **kwargs)\n\n            for key in used_param_keys:\n                state_dict.pop(key, None)\n\n        def load_state_dict(original, module, state_dict, strict=True):\n            \"\"\"torch makes a lot of copies of the dictionary with weights, so just deleting entries from state_dict does not help\n            because the same values are stored in multiple copies of the dict. The trick used here is to give torch a dict with\n            all weights on meta device, i.e. deleted, and then it doesn't matter how many copies torch makes.\n\n            In _load_from_state_dict, the correct weight will be obtained from a single dict with the right weights (sd).\n\n            The dangerous thing about this is if _load_from_state_dict is not called, (if some exotic module overloads\n            the function and does not call the original) the state dict will just fail to load because weights\n            would be on the meta device.\n            \"\"\"\n\n            if state_dict is sd:\n                state_dict = {k: v.to(device=\"meta\", dtype=v.dtype) for k, v in state_dict.items()}\n\n            original(module, state_dict, strict=strict)\n\n        module_load_state_dict = self.replace(torch.nn.Module, 'load_state_dict', lambda *args, **kwargs: load_state_dict(module_load_state_dict, *args, **kwargs))\n        module_load_from_state_dict = self.replace(torch.nn.Module, '_load_from_state_dict', lambda *args, **kwargs: load_from_state_dict(module_load_from_state_dict, *args, **kwargs))\n        linear_load_from_state_dict = self.replace(torch.nn.Linear, '_load_from_state_dict', lambda *args, **kwargs: load_from_state_dict(linear_load_from_state_dict, *args, **kwargs))\n        conv2d_load_from_state_dict = self.replace(torch.nn.Conv2d, '_load_from_state_dict', lambda *args, **kwargs: load_from_state_dict(conv2d_load_from_state_dict, *args, **kwargs))\n        mha_load_from_state_dict = self.replace(torch.nn.MultiheadAttention, '_load_from_state_dict', lambda *args, **kwargs: load_from_state_dict(mha_load_from_state_dict, *args, **kwargs))\n        layer_norm_load_from_state_dict = self.replace(torch.nn.LayerNorm, '_load_from_state_dict', lambda *args, **kwargs: load_from_state_dict(layer_norm_load_from_state_dict, *args, **kwargs))\n        group_norm_load_from_state_dict = self.replace(torch.nn.GroupNorm, '_load_from_state_dict', lambda *args, **kwargs: load_from_state_dict(group_norm_load_from_state_dict, *args, **kwargs))\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.restore()\n", "modules/import_hook.py": "import sys\n\n# this will break any attempt to import xformers which will prevent stability diffusion repo from trying to use it\nif \"--xformers\" not in \"\".join(sys.argv):\n    sys.modules[\"xformers\"] = None\n\n# Hack to fix a changed import in torchvision 0.17+, which otherwise breaks\n# basicsr; see https://github.com/AUTOMATIC1111/stable-diffusion-webui/issues/13985\ntry:\n    import torchvision.transforms.functional_tensor  # noqa: F401\nexcept ImportError:\n    try:\n        import torchvision.transforms.functional as functional\n        sys.modules[\"torchvision.transforms.functional_tensor\"] = functional\n    except ImportError:\n        pass  # shrug...\n", "modules/sd_hijack_ip2p.py": "import os.path\n\n\ndef should_hijack_ip2p(checkpoint_info):\n    from modules import sd_models_config\n\n    ckpt_basename = os.path.basename(checkpoint_info.filename).lower()\n    cfg_basename = os.path.basename(sd_models_config.find_checkpoint_config_near_filename(checkpoint_info)).lower()\n\n    return \"pix2pix\" in ckpt_basename and \"pix2pix\" not in cfg_basename\n", "modules/paths.py": "import os\nimport sys\nfrom modules.paths_internal import models_path, script_path, data_path, extensions_dir, extensions_builtin_dir, cwd  # noqa: F401\n\nimport modules.safe  # noqa: F401\n\n\ndef mute_sdxl_imports():\n    \"\"\"create fake modules that SDXL wants to import but doesn't actually use for our purposes\"\"\"\n\n    class Dummy:\n        pass\n\n    module = Dummy()\n    module.LPIPS = None\n    sys.modules['taming.modules.losses.lpips'] = module\n\n    module = Dummy()\n    module.StableDataModuleFromConfig = None\n    sys.modules['sgm.data'] = module\n\n\n# data_path = cmd_opts_pre.data\nsys.path.insert(0, script_path)\n\n# search for directory of stable diffusion in following places\nsd_path = None\npossible_sd_paths = [os.path.join(script_path, 'repositories/stable-diffusion-stability-ai'), '.', os.path.dirname(script_path)]\nfor possible_sd_path in possible_sd_paths:\n    if os.path.exists(os.path.join(possible_sd_path, 'ldm/models/diffusion/ddpm.py')):\n        sd_path = os.path.abspath(possible_sd_path)\n        break\n\nassert sd_path is not None, f\"Couldn't find Stable Diffusion in any of: {possible_sd_paths}\"\n\nmute_sdxl_imports()\n\npath_dirs = [\n    (sd_path, 'ldm', 'Stable Diffusion', []),\n    (os.path.join(sd_path, '../generative-models'), 'sgm', 'Stable Diffusion XL', [\"sgm\"]),\n    (os.path.join(sd_path, '../BLIP'), 'models/blip.py', 'BLIP', []),\n    (os.path.join(sd_path, '../k-diffusion'), 'k_diffusion/sampling.py', 'k_diffusion', [\"atstart\"]),\n]\n\npaths = {}\n\nfor d, must_exist, what, options in path_dirs:\n    must_exist_path = os.path.abspath(os.path.join(script_path, d, must_exist))\n    if not os.path.exists(must_exist_path):\n        print(f\"Warning: {what} not found at path {must_exist_path}\", file=sys.stderr)\n    else:\n        d = os.path.abspath(d)\n        if \"atstart\" in options:\n            sys.path.insert(0, d)\n        elif \"sgm\" in options:\n            # Stable Diffusion XL repo has scripts dir with __init__.py in it which ruins every extension's scripts dir, so we\n            # import sgm and remove it from sys.path so that when a script imports scripts.something, it doesbn't use sgm's scripts dir.\n\n            sys.path.insert(0, d)\n            import sgm  # noqa: F401\n            sys.path.pop(0)\n        else:\n            sys.path.append(d)\n        paths[what] = d\n", "modules/progress.py": "import base64\nimport io\nimport time\n\nimport gradio as gr\nfrom pydantic import BaseModel, Field\n\nfrom modules.shared import opts\n\nimport modules.shared as shared\nfrom collections import OrderedDict\nimport string\nimport random\nfrom typing import List\n\ncurrent_task = None\npending_tasks = OrderedDict()\nfinished_tasks = []\nrecorded_results = []\nrecorded_results_limit = 2\n\n\ndef start_task(id_task):\n    global current_task\n\n    current_task = id_task\n    pending_tasks.pop(id_task, None)\n\n\ndef finish_task(id_task):\n    global current_task\n\n    if current_task == id_task:\n        current_task = None\n\n    finished_tasks.append(id_task)\n    if len(finished_tasks) > 16:\n        finished_tasks.pop(0)\n\ndef create_task_id(task_type):\n    N = 7\n    res = ''.join(random.choices(string.ascii_uppercase +\n    string.digits, k=N))\n    return f\"task({task_type}-{res})\"\n\ndef record_results(id_task, res):\n    recorded_results.append((id_task, res))\n    if len(recorded_results) > recorded_results_limit:\n        recorded_results.pop(0)\n\n\ndef add_task_to_queue(id_job):\n    pending_tasks[id_job] = time.time()\n\nclass PendingTasksResponse(BaseModel):\n    size: int = Field(title=\"Pending task size\")\n    tasks: List[str] = Field(title=\"Pending task ids\")\n\nclass ProgressRequest(BaseModel):\n    id_task: str = Field(default=None, title=\"Task ID\", description=\"id of the task to get progress for\")\n    id_live_preview: int = Field(default=-1, title=\"Live preview image ID\", description=\"id of last received last preview image\")\n    live_preview: bool = Field(default=True, title=\"Include live preview\", description=\"boolean flag indicating whether to include the live preview image\")\n\n\nclass ProgressResponse(BaseModel):\n    active: bool = Field(title=\"Whether the task is being worked on right now\")\n    queued: bool = Field(title=\"Whether the task is in queue\")\n    completed: bool = Field(title=\"Whether the task has already finished\")\n    progress: float = Field(default=None, title=\"Progress\", description=\"The progress with a range of 0 to 1\")\n    eta: float = Field(default=None, title=\"ETA in secs\")\n    live_preview: str = Field(default=None, title=\"Live preview image\", description=\"Current live preview; a data: uri\")\n    id_live_preview: int = Field(default=None, title=\"Live preview image ID\", description=\"Send this together with next request to prevent receiving same image\")\n    textinfo: str = Field(default=None, title=\"Info text\", description=\"Info text used by WebUI.\")\n\n\ndef setup_progress_api(app):\n    app.add_api_route(\"/internal/pending-tasks\", get_pending_tasks, methods=[\"GET\"])\n    return app.add_api_route(\"/internal/progress\", progressapi, methods=[\"POST\"], response_model=ProgressResponse)\n\n\ndef get_pending_tasks():\n    pending_tasks_ids = list(pending_tasks)\n    pending_len = len(pending_tasks_ids)\n    return PendingTasksResponse(size=pending_len, tasks=pending_tasks_ids)\n\n\ndef progressapi(req: ProgressRequest):\n    active = req.id_task == current_task\n    queued = req.id_task in pending_tasks\n    completed = req.id_task in finished_tasks\n\n    if not active:\n        textinfo = \"Waiting...\"\n        if queued:\n            sorted_queued = sorted(pending_tasks.keys(), key=lambda x: pending_tasks[x])\n            queue_index = sorted_queued.index(req.id_task)\n            textinfo = \"In queue: {}/{}\".format(queue_index + 1, len(sorted_queued))\n        return ProgressResponse(active=active, queued=queued, completed=completed, id_live_preview=-1, textinfo=textinfo)\n\n    progress = 0\n\n    job_count, job_no = shared.state.job_count, shared.state.job_no\n    sampling_steps, sampling_step = shared.state.sampling_steps, shared.state.sampling_step\n\n    if job_count > 0:\n        progress += job_no / job_count\n    if sampling_steps > 0 and job_count > 0:\n        progress += 1 / job_count * sampling_step / sampling_steps\n\n    progress = min(progress, 1)\n\n    elapsed_since_start = time.time() - shared.state.time_start\n    predicted_duration = elapsed_since_start / progress if progress > 0 else None\n    eta = predicted_duration - elapsed_since_start if predicted_duration is not None else None\n\n    live_preview = None\n    id_live_preview = req.id_live_preview\n\n    if opts.live_previews_enable and req.live_preview:\n        shared.state.set_current_image()\n        if shared.state.id_live_preview != req.id_live_preview:\n            image = shared.state.current_image\n            if image is not None:\n                buffered = io.BytesIO()\n\n                if opts.live_previews_image_format == \"png\":\n                    # using optimize for large images takes an enormous amount of time\n                    if max(*image.size) <= 256:\n                        save_kwargs = {\"optimize\": True}\n                    else:\n                        save_kwargs = {\"optimize\": False, \"compress_level\": 1}\n\n                else:\n                    save_kwargs = {}\n\n                image.save(buffered, format=opts.live_previews_image_format, **save_kwargs)\n                base64_image = base64.b64encode(buffered.getvalue()).decode('ascii')\n                live_preview = f\"data:image/{opts.live_previews_image_format};base64,{base64_image}\"\n                id_live_preview = shared.state.id_live_preview\n\n    return ProgressResponse(active=active, queued=queued, completed=completed, progress=progress, eta=eta, live_preview=live_preview, id_live_preview=id_live_preview, textinfo=shared.state.textinfo)\n\n\ndef restore_progress(id_task):\n    while id_task == current_task or id_task in pending_tasks:\n        time.sleep(0.1)\n\n    res = next(iter([x[1] for x in recorded_results if id_task == x[0]]), None)\n    if res is not None:\n        return res\n\n    return gr.update(), gr.update(), gr.update(), f\"Couldn't restore progress for {id_task}: results either have been discarded or never were obtained\"\n", "modules/torch_utils.py": "from __future__ import annotations\n\nimport torch.nn\n\n\ndef get_param(model) -> torch.nn.Parameter:\n    \"\"\"\n    Find the first parameter in a model or module.\n    \"\"\"\n    if hasattr(model, \"model\") and hasattr(model.model, \"parameters\"):\n        # Unpeel a model descriptor to get at the actual Torch module.\n        model = model.model\n\n    for param in model.parameters():\n        return param\n\n    raise ValueError(f\"No parameters found in model {model!r}\")\n", "modules/rng.py": "import torch\n\nfrom modules import devices, rng_philox, shared\n\n\ndef randn(seed, shape, generator=None):\n    \"\"\"Generate a tensor with random numbers from a normal distribution using seed.\n\n    Uses the seed parameter to set the global torch seed; to generate more with that seed, use randn_like/randn_without_seed.\"\"\"\n\n    manual_seed(seed)\n\n    if shared.opts.randn_source == \"NV\":\n        return torch.asarray((generator or nv_rng).randn(shape), device=devices.device)\n\n    if shared.opts.randn_source == \"CPU\" or devices.device.type == 'mps':\n        return torch.randn(shape, device=devices.cpu, generator=generator).to(devices.device)\n\n    return torch.randn(shape, device=devices.device, generator=generator)\n\n\ndef randn_local(seed, shape):\n    \"\"\"Generate a tensor with random numbers from a normal distribution using seed.\n\n    Does not change the global random number generator. You can only generate the seed's first tensor using this function.\"\"\"\n\n    if shared.opts.randn_source == \"NV\":\n        rng = rng_philox.Generator(seed)\n        return torch.asarray(rng.randn(shape), device=devices.device)\n\n    local_device = devices.cpu if shared.opts.randn_source == \"CPU\" or devices.device.type == 'mps' else devices.device\n    local_generator = torch.Generator(local_device).manual_seed(int(seed))\n    return torch.randn(shape, device=local_device, generator=local_generator).to(devices.device)\n\n\ndef randn_like(x):\n    \"\"\"Generate a tensor with random numbers from a normal distribution using the previously initialized generator.\n\n    Use either randn() or manual_seed() to initialize the generator.\"\"\"\n\n    if shared.opts.randn_source == \"NV\":\n        return torch.asarray(nv_rng.randn(x.shape), device=x.device, dtype=x.dtype)\n\n    if shared.opts.randn_source == \"CPU\" or x.device.type == 'mps':\n        return torch.randn_like(x, device=devices.cpu).to(x.device)\n\n    return torch.randn_like(x)\n\n\ndef randn_without_seed(shape, generator=None):\n    \"\"\"Generate a tensor with random numbers from a normal distribution using the previously initialized generator.\n\n    Use either randn() or manual_seed() to initialize the generator.\"\"\"\n\n    if shared.opts.randn_source == \"NV\":\n        return torch.asarray((generator or nv_rng).randn(shape), device=devices.device)\n\n    if shared.opts.randn_source == \"CPU\" or devices.device.type == 'mps':\n        return torch.randn(shape, device=devices.cpu, generator=generator).to(devices.device)\n\n    return torch.randn(shape, device=devices.device, generator=generator)\n\n\ndef manual_seed(seed):\n    \"\"\"Set up a global random number generator using the specified seed.\"\"\"\n\n    if shared.opts.randn_source == \"NV\":\n        global nv_rng\n        nv_rng = rng_philox.Generator(seed)\n        return\n\n    torch.manual_seed(seed)\n\n\ndef create_generator(seed):\n    if shared.opts.randn_source == \"NV\":\n        return rng_philox.Generator(seed)\n\n    device = devices.cpu if shared.opts.randn_source == \"CPU\" or devices.device.type == 'mps' else devices.device\n    generator = torch.Generator(device).manual_seed(int(seed))\n    return generator\n\n\n# from https://discuss.pytorch.org/t/help-regarding-slerp-function-for-generative-model-sampling/32475/3\ndef slerp(val, low, high):\n    low_norm = low/torch.norm(low, dim=1, keepdim=True)\n    high_norm = high/torch.norm(high, dim=1, keepdim=True)\n    dot = (low_norm*high_norm).sum(1)\n\n    if dot.mean() > 0.9995:\n        return low * val + high * (1 - val)\n\n    omega = torch.acos(dot)\n    so = torch.sin(omega)\n    res = (torch.sin((1.0-val)*omega)/so).unsqueeze(1)*low + (torch.sin(val*omega)/so).unsqueeze(1) * high\n    return res\n\n\nclass ImageRNG:\n    def __init__(self, shape, seeds, subseeds=None, subseed_strength=0.0, seed_resize_from_h=0, seed_resize_from_w=0):\n        self.shape = tuple(map(int, shape))\n        self.seeds = seeds\n        self.subseeds = subseeds\n        self.subseed_strength = subseed_strength\n        self.seed_resize_from_h = seed_resize_from_h\n        self.seed_resize_from_w = seed_resize_from_w\n\n        self.generators = [create_generator(seed) for seed in seeds]\n\n        self.is_first = True\n\n    def first(self):\n        noise_shape = self.shape if self.seed_resize_from_h <= 0 or self.seed_resize_from_w <= 0 else (self.shape[0], int(self.seed_resize_from_h) // 8, int(self.seed_resize_from_w // 8))\n\n        xs = []\n\n        for i, (seed, generator) in enumerate(zip(self.seeds, self.generators)):\n            subnoise = None\n            if self.subseeds is not None and self.subseed_strength != 0:\n                subseed = 0 if i >= len(self.subseeds) else self.subseeds[i]\n                subnoise = randn(subseed, noise_shape)\n\n            if noise_shape != self.shape:\n                noise = randn(seed, noise_shape)\n            else:\n                noise = randn(seed, self.shape, generator=generator)\n\n            if subnoise is not None:\n                noise = slerp(self.subseed_strength, noise, subnoise)\n\n            if noise_shape != self.shape:\n                x = randn(seed, self.shape, generator=generator)\n                dx = (self.shape[2] - noise_shape[2]) // 2\n                dy = (self.shape[1] - noise_shape[1]) // 2\n                w = noise_shape[2] if dx >= 0 else noise_shape[2] + 2 * dx\n                h = noise_shape[1] if dy >= 0 else noise_shape[1] + 2 * dy\n                tx = 0 if dx < 0 else dx\n                ty = 0 if dy < 0 else dy\n                dx = max(-dx, 0)\n                dy = max(-dy, 0)\n\n                x[:, ty:ty + h, tx:tx + w] = noise[:, dy:dy + h, dx:dx + w]\n                noise = x\n\n            xs.append(noise)\n\n        eta_noise_seed_delta = shared.opts.eta_noise_seed_delta or 0\n        if eta_noise_seed_delta:\n            self.generators = [create_generator(seed + eta_noise_seed_delta) for seed in self.seeds]\n\n        return torch.stack(xs).to(shared.device)\n\n    def next(self):\n        if self.is_first:\n            self.is_first = False\n            return self.first()\n\n        xs = []\n        for generator in self.generators:\n            x = randn_without_seed(self.shape, generator=generator)\n            xs.append(x)\n\n        return torch.stack(xs).to(shared.device)\n\n\ndevices.randn = randn\ndevices.randn_local = randn_local\ndevices.randn_like = randn_like\ndevices.randn_without_seed = randn_without_seed\ndevices.manual_seed = manual_seed\n", "modules/processing.py": "from __future__ import annotations\nimport json\nimport logging\nimport math\nimport os\nimport sys\nimport hashlib\nfrom dataclasses import dataclass, field\n\nimport torch\nimport numpy as np\nfrom PIL import Image, ImageOps\nimport random\nimport cv2\nfrom skimage import exposure\nfrom typing import Any\n\nimport modules.sd_hijack\nfrom modules import devices, prompt_parser, masking, sd_samplers, lowvram, infotext_utils, extra_networks, sd_vae_approx, scripts, sd_samplers_common, sd_unet, errors, rng\nfrom modules.rng import slerp # noqa: F401\nfrom modules.sd_hijack import model_hijack\nfrom modules.sd_samplers_common import images_tensor_to_samples, decode_first_stage, approximation_indexes\nfrom modules.shared import opts, cmd_opts, state\nimport modules.shared as shared\nimport modules.paths as paths\nimport modules.face_restoration\nimport modules.images as images\nimport modules.styles\nimport modules.sd_models as sd_models\nimport modules.sd_vae as sd_vae\nfrom ldm.data.util import AddMiDaS\nfrom ldm.models.diffusion.ddpm import LatentDepth2ImageDiffusion\n\nfrom einops import repeat, rearrange\nfrom blendmodes.blend import blendLayers, BlendType\n\n\n# some of those options should not be changed at all because they would break the model, so I removed them from options.\nopt_C = 4\nopt_f = 8\n\n\ndef setup_color_correction(image):\n    logging.info(\"Calibrating color correction.\")\n    correction_target = cv2.cvtColor(np.asarray(image.copy()), cv2.COLOR_RGB2LAB)\n    return correction_target\n\n\ndef apply_color_correction(correction, original_image):\n    logging.info(\"Applying color correction.\")\n    image = Image.fromarray(cv2.cvtColor(exposure.match_histograms(\n        cv2.cvtColor(\n            np.asarray(original_image),\n            cv2.COLOR_RGB2LAB\n        ),\n        correction,\n        channel_axis=2\n    ), cv2.COLOR_LAB2RGB).astype(\"uint8\"))\n\n    image = blendLayers(image, original_image, BlendType.LUMINOSITY)\n\n    return image.convert('RGB')\n\n\ndef uncrop(image, dest_size, paste_loc):\n    x, y, w, h = paste_loc\n    base_image = Image.new('RGBA', dest_size)\n    image = images.resize_image(1, image, w, h)\n    base_image.paste(image, (x, y))\n    image = base_image\n\n    return image\n\n\ndef apply_overlay(image, paste_loc, overlay):\n    if overlay is None:\n        return image, image.copy()\n\n    if paste_loc is not None:\n        image = uncrop(image, (overlay.width, overlay.height), paste_loc)\n\n    original_denoised_image = image.copy()\n\n    image = image.convert('RGBA')\n    image.alpha_composite(overlay)\n    image = image.convert('RGB')\n\n    return image, original_denoised_image\n\ndef create_binary_mask(image, round=True):\n    if image.mode == 'RGBA' and image.getextrema()[-1] != (255, 255):\n        if round:\n            image = image.split()[-1].convert(\"L\").point(lambda x: 255 if x > 128 else 0)\n        else:\n            image = image.split()[-1].convert(\"L\")\n    else:\n        image = image.convert('L')\n    return image\n\ndef txt2img_image_conditioning(sd_model, x, width, height):\n    if sd_model.model.conditioning_key in {'hybrid', 'concat'}: # Inpainting models\n\n        # The \"masked-image\" in this case will just be all 0.5 since the entire image is masked.\n        image_conditioning = torch.ones(x.shape[0], 3, height, width, device=x.device) * 0.5\n        image_conditioning = images_tensor_to_samples(image_conditioning, approximation_indexes.get(opts.sd_vae_encode_method))\n\n        # Add the fake full 1s mask to the first dimension.\n        image_conditioning = torch.nn.functional.pad(image_conditioning, (0, 0, 0, 0, 1, 0), value=1.0)\n        image_conditioning = image_conditioning.to(x.dtype)\n\n        return image_conditioning\n\n    elif sd_model.model.conditioning_key == \"crossattn-adm\": # UnCLIP models\n\n        return x.new_zeros(x.shape[0], 2*sd_model.noise_augmentor.time_embed.dim, dtype=x.dtype, device=x.device)\n\n    else:\n        sd = sd_model.model.state_dict()\n        diffusion_model_input = sd.get('diffusion_model.input_blocks.0.0.weight', None)\n        if diffusion_model_input is not None:\n            if diffusion_model_input.shape[1] == 9:\n                # The \"masked-image\" in this case will just be all 0.5 since the entire image is masked.\n                image_conditioning = torch.ones(x.shape[0], 3, height, width, device=x.device) * 0.5\n                image_conditioning = images_tensor_to_samples(image_conditioning,\n                                                              approximation_indexes.get(opts.sd_vae_encode_method))\n\n                # Add the fake full 1s mask to the first dimension.\n                image_conditioning = torch.nn.functional.pad(image_conditioning, (0, 0, 0, 0, 1, 0), value=1.0)\n                image_conditioning = image_conditioning.to(x.dtype)\n\n                return image_conditioning\n\n        # Dummy zero conditioning if we're not using inpainting or unclip models.\n        # Still takes up a bit of memory, but no encoder call.\n        # Pretty sure we can just make this a 1x1 image since its not going to be used besides its batch size.\n        return x.new_zeros(x.shape[0], 5, 1, 1, dtype=x.dtype, device=x.device)\n\n\n@dataclass(repr=False)\nclass StableDiffusionProcessing:\n    sd_model: object = None\n    outpath_samples: str = None\n    outpath_grids: str = None\n    prompt: str = \"\"\n    prompt_for_display: str = None\n    negative_prompt: str = \"\"\n    styles: list[str] = None\n    seed: int = -1\n    subseed: int = -1\n    subseed_strength: float = 0\n    seed_resize_from_h: int = -1\n    seed_resize_from_w: int = -1\n    seed_enable_extras: bool = True\n    sampler_name: str = None\n    scheduler: str = None\n    batch_size: int = 1\n    n_iter: int = 1\n    steps: int = 50\n    cfg_scale: float = 7.0\n    width: int = 512\n    height: int = 512\n    restore_faces: bool = None\n    tiling: bool = None\n    do_not_save_samples: bool = False\n    do_not_save_grid: bool = False\n    extra_generation_params: dict[str, Any] = None\n    overlay_images: list = None\n    eta: float = None\n    do_not_reload_embeddings: bool = False\n    denoising_strength: float = None\n    ddim_discretize: str = None\n    s_min_uncond: float = None\n    s_churn: float = None\n    s_tmax: float = None\n    s_tmin: float = None\n    s_noise: float = None\n    override_settings: dict[str, Any] = None\n    override_settings_restore_afterwards: bool = True\n    sampler_index: int = None\n    refiner_checkpoint: str = None\n    refiner_switch_at: float = None\n    token_merging_ratio = 0\n    token_merging_ratio_hr = 0\n    disable_extra_networks: bool = False\n    firstpass_image: Image = None\n\n    scripts_value: scripts.ScriptRunner = field(default=None, init=False)\n    script_args_value: list = field(default=None, init=False)\n    scripts_setup_complete: bool = field(default=False, init=False)\n\n    cached_uc = [None, None]\n    cached_c = [None, None]\n\n    comments: dict = None\n    sampler: sd_samplers_common.Sampler | None = field(default=None, init=False)\n    is_using_inpainting_conditioning: bool = field(default=False, init=False)\n    paste_to: tuple | None = field(default=None, init=False)\n\n    is_hr_pass: bool = field(default=False, init=False)\n\n    c: tuple = field(default=None, init=False)\n    uc: tuple = field(default=None, init=False)\n\n    rng: rng.ImageRNG | None = field(default=None, init=False)\n    step_multiplier: int = field(default=1, init=False)\n    color_corrections: list = field(default=None, init=False)\n\n    all_prompts: list = field(default=None, init=False)\n    all_negative_prompts: list = field(default=None, init=False)\n    all_seeds: list = field(default=None, init=False)\n    all_subseeds: list = field(default=None, init=False)\n    iteration: int = field(default=0, init=False)\n    main_prompt: str = field(default=None, init=False)\n    main_negative_prompt: str = field(default=None, init=False)\n\n    prompts: list = field(default=None, init=False)\n    negative_prompts: list = field(default=None, init=False)\n    seeds: list = field(default=None, init=False)\n    subseeds: list = field(default=None, init=False)\n    extra_network_data: dict = field(default=None, init=False)\n\n    user: str = field(default=None, init=False)\n\n    sd_model_name: str = field(default=None, init=False)\n    sd_model_hash: str = field(default=None, init=False)\n    sd_vae_name: str = field(default=None, init=False)\n    sd_vae_hash: str = field(default=None, init=False)\n\n    is_api: bool = field(default=False, init=False)\n\n    def __post_init__(self):\n        if self.sampler_index is not None:\n            print(\"sampler_index argument for StableDiffusionProcessing does not do anything; use sampler_name\", file=sys.stderr)\n\n        self.comments = {}\n\n        if self.styles is None:\n            self.styles = []\n\n        self.sampler_noise_scheduler_override = None\n        self.s_min_uncond = self.s_min_uncond if self.s_min_uncond is not None else opts.s_min_uncond\n        self.s_churn = self.s_churn if self.s_churn is not None else opts.s_churn\n        self.s_tmin = self.s_tmin if self.s_tmin is not None else opts.s_tmin\n        self.s_tmax = (self.s_tmax if self.s_tmax is not None else opts.s_tmax) or float('inf')\n        self.s_noise = self.s_noise if self.s_noise is not None else opts.s_noise\n\n        self.extra_generation_params = self.extra_generation_params or {}\n        self.override_settings = self.override_settings or {}\n        self.script_args = self.script_args or {}\n\n        self.refiner_checkpoint_info = None\n\n        if not self.seed_enable_extras:\n            self.subseed = -1\n            self.subseed_strength = 0\n            self.seed_resize_from_h = 0\n            self.seed_resize_from_w = 0\n\n        self.cached_uc = StableDiffusionProcessing.cached_uc\n        self.cached_c = StableDiffusionProcessing.cached_c\n\n    @property\n    def sd_model(self):\n        return shared.sd_model\n\n    @sd_model.setter\n    def sd_model(self, value):\n        pass\n\n    @property\n    def scripts(self):\n        return self.scripts_value\n\n    @scripts.setter\n    def scripts(self, value):\n        self.scripts_value = value\n\n        if self.scripts_value and self.script_args_value and not self.scripts_setup_complete:\n            self.setup_scripts()\n\n    @property\n    def script_args(self):\n        return self.script_args_value\n\n    @script_args.setter\n    def script_args(self, value):\n        self.script_args_value = value\n\n        if self.scripts_value and self.script_args_value and not self.scripts_setup_complete:\n            self.setup_scripts()\n\n    def setup_scripts(self):\n        self.scripts_setup_complete = True\n\n        self.scripts.setup_scrips(self, is_ui=not self.is_api)\n\n    def comment(self, text):\n        self.comments[text] = 1\n\n    def txt2img_image_conditioning(self, x, width=None, height=None):\n        self.is_using_inpainting_conditioning = self.sd_model.model.conditioning_key in {'hybrid', 'concat'}\n\n        return txt2img_image_conditioning(self.sd_model, x, width or self.width, height or self.height)\n\n    def depth2img_image_conditioning(self, source_image):\n        # Use the AddMiDaS helper to Format our source image to suit the MiDaS model\n        transformer = AddMiDaS(model_type=\"dpt_hybrid\")\n        transformed = transformer({\"jpg\": rearrange(source_image[0], \"c h w -> h w c\")})\n        midas_in = torch.from_numpy(transformed[\"midas_in\"][None, ...]).to(device=shared.device)\n        midas_in = repeat(midas_in, \"1 ... -> n ...\", n=self.batch_size)\n\n        conditioning_image = images_tensor_to_samples(source_image*0.5+0.5, approximation_indexes.get(opts.sd_vae_encode_method))\n        conditioning = torch.nn.functional.interpolate(\n            self.sd_model.depth_model(midas_in),\n            size=conditioning_image.shape[2:],\n            mode=\"bicubic\",\n            align_corners=False,\n        )\n\n        (depth_min, depth_max) = torch.aminmax(conditioning)\n        conditioning = 2. * (conditioning - depth_min) / (depth_max - depth_min) - 1.\n        return conditioning\n\n    def edit_image_conditioning(self, source_image):\n        conditioning_image = shared.sd_model.encode_first_stage(source_image).mode()\n\n        return conditioning_image\n\n    def unclip_image_conditioning(self, source_image):\n        c_adm = self.sd_model.embedder(source_image)\n        if self.sd_model.noise_augmentor is not None:\n            noise_level = 0 # TODO: Allow other noise levels?\n            c_adm, noise_level_emb = self.sd_model.noise_augmentor(c_adm, noise_level=repeat(torch.tensor([noise_level]).to(c_adm.device), '1 -> b', b=c_adm.shape[0]))\n            c_adm = torch.cat((c_adm, noise_level_emb), 1)\n        return c_adm\n\n    def inpainting_image_conditioning(self, source_image, latent_image, image_mask=None, round_image_mask=True):\n        self.is_using_inpainting_conditioning = True\n\n        # Handle the different mask inputs\n        if image_mask is not None:\n            if torch.is_tensor(image_mask):\n                conditioning_mask = image_mask\n            else:\n                conditioning_mask = np.array(image_mask.convert(\"L\"))\n                conditioning_mask = conditioning_mask.astype(np.float32) / 255.0\n                conditioning_mask = torch.from_numpy(conditioning_mask[None, None])\n\n                if round_image_mask:\n                    # Caller is requesting a discretized mask as input, so we round to either 1.0 or 0.0\n                    conditioning_mask = torch.round(conditioning_mask)\n\n        else:\n            conditioning_mask = source_image.new_ones(1, 1, *source_image.shape[-2:])\n\n        # Create another latent image, this time with a masked version of the original input.\n        # Smoothly interpolate between the masked and unmasked latent conditioning image using a parameter.\n        conditioning_mask = conditioning_mask.to(device=source_image.device, dtype=source_image.dtype)\n        conditioning_image = torch.lerp(\n            source_image,\n            source_image * (1.0 - conditioning_mask),\n            getattr(self, \"inpainting_mask_weight\", shared.opts.inpainting_mask_weight)\n        )\n\n        # Encode the new masked image using first stage of network.\n        conditioning_image = self.sd_model.get_first_stage_encoding(self.sd_model.encode_first_stage(conditioning_image))\n\n        # Create the concatenated conditioning tensor to be fed to `c_concat`\n        conditioning_mask = torch.nn.functional.interpolate(conditioning_mask, size=latent_image.shape[-2:])\n        conditioning_mask = conditioning_mask.expand(conditioning_image.shape[0], -1, -1, -1)\n        image_conditioning = torch.cat([conditioning_mask, conditioning_image], dim=1)\n        image_conditioning = image_conditioning.to(shared.device).type(self.sd_model.dtype)\n\n        return image_conditioning\n\n    def img2img_image_conditioning(self, source_image, latent_image, image_mask=None, round_image_mask=True):\n        source_image = devices.cond_cast_float(source_image)\n\n        # HACK: Using introspection as the Depth2Image model doesn't appear to uniquely\n        # identify itself with a field common to all models. The conditioning_key is also hybrid.\n        if isinstance(self.sd_model, LatentDepth2ImageDiffusion):\n            return self.depth2img_image_conditioning(source_image)\n\n        if self.sd_model.cond_stage_key == \"edit\":\n            return self.edit_image_conditioning(source_image)\n\n        if self.sampler.conditioning_key in {'hybrid', 'concat'}:\n            return self.inpainting_image_conditioning(source_image, latent_image, image_mask=image_mask, round_image_mask=round_image_mask)\n\n        if self.sampler.conditioning_key == \"crossattn-adm\":\n            return self.unclip_image_conditioning(source_image)\n\n        sd = self.sampler.model_wrap.inner_model.model.state_dict()\n        diffusion_model_input = sd.get('diffusion_model.input_blocks.0.0.weight', None)\n        if diffusion_model_input is not None:\n            if diffusion_model_input.shape[1] == 9:\n                return self.inpainting_image_conditioning(source_image, latent_image, image_mask=image_mask)\n\n        # Dummy zero conditioning if we're not using inpainting or depth model.\n        return latent_image.new_zeros(latent_image.shape[0], 5, 1, 1)\n\n    def init(self, all_prompts, all_seeds, all_subseeds):\n        pass\n\n    def sample(self, conditioning, unconditional_conditioning, seeds, subseeds, subseed_strength, prompts):\n        raise NotImplementedError()\n\n    def close(self):\n        self.sampler = None\n        self.c = None\n        self.uc = None\n        if not opts.persistent_cond_cache:\n            StableDiffusionProcessing.cached_c = [None, None]\n            StableDiffusionProcessing.cached_uc = [None, None]\n\n    def get_token_merging_ratio(self, for_hr=False):\n        if for_hr:\n            return self.token_merging_ratio_hr or opts.token_merging_ratio_hr or self.token_merging_ratio or opts.token_merging_ratio\n\n        return self.token_merging_ratio or opts.token_merging_ratio\n\n    def setup_prompts(self):\n        if isinstance(self.prompt,list):\n            self.all_prompts = self.prompt\n        elif isinstance(self.negative_prompt, list):\n            self.all_prompts = [self.prompt] * len(self.negative_prompt)\n        else:\n            self.all_prompts = self.batch_size * self.n_iter * [self.prompt]\n\n        if isinstance(self.negative_prompt, list):\n            self.all_negative_prompts = self.negative_prompt\n        else:\n            self.all_negative_prompts = [self.negative_prompt] * len(self.all_prompts)\n\n        if len(self.all_prompts) != len(self.all_negative_prompts):\n            raise RuntimeError(f\"Received a different number of prompts ({len(self.all_prompts)}) and negative prompts ({len(self.all_negative_prompts)})\")\n\n        self.all_prompts = [shared.prompt_styles.apply_styles_to_prompt(x, self.styles) for x in self.all_prompts]\n        self.all_negative_prompts = [shared.prompt_styles.apply_negative_styles_to_prompt(x, self.styles) for x in self.all_negative_prompts]\n\n        self.main_prompt = self.all_prompts[0]\n        self.main_negative_prompt = self.all_negative_prompts[0]\n\n    def cached_params(self, required_prompts, steps, extra_network_data, hires_steps=None, use_old_scheduling=False):\n        \"\"\"Returns parameters that invalidate the cond cache if changed\"\"\"\n\n        return (\n            required_prompts,\n            steps,\n            hires_steps,\n            use_old_scheduling,\n            opts.CLIP_stop_at_last_layers,\n            shared.sd_model.sd_checkpoint_info,\n            extra_network_data,\n            opts.sdxl_crop_left,\n            opts.sdxl_crop_top,\n            self.width,\n            self.height,\n            opts.fp8_storage,\n            opts.cache_fp16_weight,\n            opts.emphasis,\n        )\n\n    def get_conds_with_caching(self, function, required_prompts, steps, caches, extra_network_data, hires_steps=None):\n        \"\"\"\n        Returns the result of calling function(shared.sd_model, required_prompts, steps)\n        using a cache to store the result if the same arguments have been used before.\n\n        cache is an array containing two elements. The first element is a tuple\n        representing the previously used arguments, or None if no arguments\n        have been used before. The second element is where the previously\n        computed result is stored.\n\n        caches is a list with items described above.\n        \"\"\"\n\n        if shared.opts.use_old_scheduling:\n            old_schedules = prompt_parser.get_learned_conditioning_prompt_schedules(required_prompts, steps, hires_steps, False)\n            new_schedules = prompt_parser.get_learned_conditioning_prompt_schedules(required_prompts, steps, hires_steps, True)\n            if old_schedules != new_schedules:\n                self.extra_generation_params[\"Old prompt editing timelines\"] = True\n\n        cached_params = self.cached_params(required_prompts, steps, extra_network_data, hires_steps, shared.opts.use_old_scheduling)\n\n        for cache in caches:\n            if cache[0] is not None and cached_params == cache[0]:\n                return cache[1]\n\n        cache = caches[0]\n\n        with devices.autocast():\n            cache[1] = function(shared.sd_model, required_prompts, steps, hires_steps, shared.opts.use_old_scheduling)\n\n        cache[0] = cached_params\n        return cache[1]\n\n    def setup_conds(self):\n        prompts = prompt_parser.SdConditioning(self.prompts, width=self.width, height=self.height)\n        negative_prompts = prompt_parser.SdConditioning(self.negative_prompts, width=self.width, height=self.height, is_negative_prompt=True)\n\n        sampler_config = sd_samplers.find_sampler_config(self.sampler_name)\n        total_steps = sampler_config.total_steps(self.steps) if sampler_config else self.steps\n        self.step_multiplier = total_steps // self.steps\n        self.firstpass_steps = total_steps\n\n        self.uc = self.get_conds_with_caching(prompt_parser.get_learned_conditioning, negative_prompts, total_steps, [self.cached_uc], self.extra_network_data)\n        self.c = self.get_conds_with_caching(prompt_parser.get_multicond_learned_conditioning, prompts, total_steps, [self.cached_c], self.extra_network_data)\n\n    def get_conds(self):\n        return self.c, self.uc\n\n    def parse_extra_network_prompts(self):\n        self.prompts, self.extra_network_data = extra_networks.parse_prompts(self.prompts)\n\n    def save_samples(self) -> bool:\n        \"\"\"Returns whether generated images need to be written to disk\"\"\"\n        return opts.samples_save and not self.do_not_save_samples and (opts.save_incomplete_images or not state.interrupted and not state.skipped)\n\n\nclass Processed:\n    def __init__(self, p: StableDiffusionProcessing, images_list, seed=-1, info=\"\", subseed=None, all_prompts=None, all_negative_prompts=None, all_seeds=None, all_subseeds=None, index_of_first_image=0, infotexts=None, comments=\"\"):\n        self.images = images_list\n        self.prompt = p.prompt\n        self.negative_prompt = p.negative_prompt\n        self.seed = seed\n        self.subseed = subseed\n        self.subseed_strength = p.subseed_strength\n        self.info = info\n        self.comments = \"\".join(f\"{comment}\\n\" for comment in p.comments)\n        self.width = p.width\n        self.height = p.height\n        self.sampler_name = p.sampler_name\n        self.cfg_scale = p.cfg_scale\n        self.image_cfg_scale = getattr(p, 'image_cfg_scale', None)\n        self.steps = p.steps\n        self.batch_size = p.batch_size\n        self.restore_faces = p.restore_faces\n        self.face_restoration_model = opts.face_restoration_model if p.restore_faces else None\n        self.sd_model_name = p.sd_model_name\n        self.sd_model_hash = p.sd_model_hash\n        self.sd_vae_name = p.sd_vae_name\n        self.sd_vae_hash = p.sd_vae_hash\n        self.seed_resize_from_w = p.seed_resize_from_w\n        self.seed_resize_from_h = p.seed_resize_from_h\n        self.denoising_strength = getattr(p, 'denoising_strength', None)\n        self.extra_generation_params = p.extra_generation_params\n        self.index_of_first_image = index_of_first_image\n        self.styles = p.styles\n        self.job_timestamp = state.job_timestamp\n        self.clip_skip = opts.CLIP_stop_at_last_layers\n        self.token_merging_ratio = p.token_merging_ratio\n        self.token_merging_ratio_hr = p.token_merging_ratio_hr\n\n        self.eta = p.eta\n        self.ddim_discretize = p.ddim_discretize\n        self.s_churn = p.s_churn\n        self.s_tmin = p.s_tmin\n        self.s_tmax = p.s_tmax\n        self.s_noise = p.s_noise\n        self.s_min_uncond = p.s_min_uncond\n        self.sampler_noise_scheduler_override = p.sampler_noise_scheduler_override\n        self.prompt = self.prompt if not isinstance(self.prompt, list) else self.prompt[0]\n        self.negative_prompt = self.negative_prompt if not isinstance(self.negative_prompt, list) else self.negative_prompt[0]\n        self.seed = int(self.seed if not isinstance(self.seed, list) else self.seed[0]) if self.seed is not None else -1\n        self.subseed = int(self.subseed if not isinstance(self.subseed, list) else self.subseed[0]) if self.subseed is not None else -1\n        self.is_using_inpainting_conditioning = p.is_using_inpainting_conditioning\n\n        self.all_prompts = all_prompts or p.all_prompts or [self.prompt]\n        self.all_negative_prompts = all_negative_prompts or p.all_negative_prompts or [self.negative_prompt]\n        self.all_seeds = all_seeds or p.all_seeds or [self.seed]\n        self.all_subseeds = all_subseeds or p.all_subseeds or [self.subseed]\n        self.infotexts = infotexts or [info]\n        self.version = program_version()\n\n    def js(self):\n        obj = {\n            \"prompt\": self.all_prompts[0],\n            \"all_prompts\": self.all_prompts,\n            \"negative_prompt\": self.all_negative_prompts[0],\n            \"all_negative_prompts\": self.all_negative_prompts,\n            \"seed\": self.seed,\n            \"all_seeds\": self.all_seeds,\n            \"subseed\": self.subseed,\n            \"all_subseeds\": self.all_subseeds,\n            \"subseed_strength\": self.subseed_strength,\n            \"width\": self.width,\n            \"height\": self.height,\n            \"sampler_name\": self.sampler_name,\n            \"cfg_scale\": self.cfg_scale,\n            \"steps\": self.steps,\n            \"batch_size\": self.batch_size,\n            \"restore_faces\": self.restore_faces,\n            \"face_restoration_model\": self.face_restoration_model,\n            \"sd_model_name\": self.sd_model_name,\n            \"sd_model_hash\": self.sd_model_hash,\n            \"sd_vae_name\": self.sd_vae_name,\n            \"sd_vae_hash\": self.sd_vae_hash,\n            \"seed_resize_from_w\": self.seed_resize_from_w,\n            \"seed_resize_from_h\": self.seed_resize_from_h,\n            \"denoising_strength\": self.denoising_strength,\n            \"extra_generation_params\": self.extra_generation_params,\n            \"index_of_first_image\": self.index_of_first_image,\n            \"infotexts\": self.infotexts,\n            \"styles\": self.styles,\n            \"job_timestamp\": self.job_timestamp,\n            \"clip_skip\": self.clip_skip,\n            \"is_using_inpainting_conditioning\": self.is_using_inpainting_conditioning,\n            \"version\": self.version,\n        }\n\n        return json.dumps(obj, default=lambda o: None)\n\n    def infotext(self, p: StableDiffusionProcessing, index):\n        return create_infotext(p, self.all_prompts, self.all_seeds, self.all_subseeds, comments=[], position_in_batch=index % self.batch_size, iteration=index // self.batch_size)\n\n    def get_token_merging_ratio(self, for_hr=False):\n        return self.token_merging_ratio_hr if for_hr else self.token_merging_ratio\n\n\ndef create_random_tensors(shape, seeds, subseeds=None, subseed_strength=0.0, seed_resize_from_h=0, seed_resize_from_w=0, p=None):\n    g = rng.ImageRNG(shape, seeds, subseeds=subseeds, subseed_strength=subseed_strength, seed_resize_from_h=seed_resize_from_h, seed_resize_from_w=seed_resize_from_w)\n    return g.next()\n\n\nclass DecodedSamples(list):\n    already_decoded = True\n\n\ndef decode_latent_batch(model, batch, target_device=None, check_for_nans=False):\n    samples = DecodedSamples()\n\n    for i in range(batch.shape[0]):\n        sample = decode_first_stage(model, batch[i:i + 1])[0]\n\n        if check_for_nans:\n\n            try:\n                devices.test_for_nans(sample, \"vae\")\n            except devices.NansException as e:\n                if shared.opts.auto_vae_precision_bfloat16:\n                    autofix_dtype = torch.bfloat16\n                    autofix_dtype_text = \"bfloat16\"\n                    autofix_dtype_setting = \"Automatically convert VAE to bfloat16\"\n                    autofix_dtype_comment = \"\"\n                elif shared.opts.auto_vae_precision:\n                    autofix_dtype = torch.float32\n                    autofix_dtype_text = \"32-bit float\"\n                    autofix_dtype_setting = \"Automatically revert VAE to 32-bit floats\"\n                    autofix_dtype_comment = \"\\nTo always start with 32-bit VAE, use --no-half-vae commandline flag.\"\n                else:\n                    raise e\n\n                if devices.dtype_vae == autofix_dtype:\n                    raise e\n\n                errors.print_error_explanation(\n                    \"A tensor with all NaNs was produced in VAE.\\n\"\n                    f\"Web UI will now convert VAE into {autofix_dtype_text} and retry.\\n\"\n                    f\"To disable this behavior, disable the '{autofix_dtype_setting}' setting.{autofix_dtype_comment}\"\n                )\n\n                devices.dtype_vae = autofix_dtype\n                model.first_stage_model.to(devices.dtype_vae)\n                batch = batch.to(devices.dtype_vae)\n\n                sample = decode_first_stage(model, batch[i:i + 1])[0]\n\n        if target_device is not None:\n            sample = sample.to(target_device)\n\n        samples.append(sample)\n\n    return samples\n\n\ndef get_fixed_seed(seed):\n    if seed == '' or seed is None:\n        seed = -1\n    elif isinstance(seed, str):\n        try:\n            seed = int(seed)\n        except Exception:\n            seed = -1\n\n    if seed == -1:\n        return int(random.randrange(4294967294))\n\n    return seed\n\n\ndef fix_seed(p):\n    p.seed = get_fixed_seed(p.seed)\n    p.subseed = get_fixed_seed(p.subseed)\n\n\ndef program_version():\n    import launch\n\n    res = launch.git_tag()\n    if res == \"<none>\":\n        res = None\n\n    return res\n\n\ndef create_infotext(p, all_prompts, all_seeds, all_subseeds, comments=None, iteration=0, position_in_batch=0, use_main_prompt=False, index=None, all_negative_prompts=None):\n    \"\"\"\n    this function is used to generate the infotext that is stored in the generated images, it's contains the parameters that are required to generate the imagee\n    Args:\n        p: StableDiffusionProcessing\n        all_prompts: list[str]\n        all_seeds: list[int]\n        all_subseeds: list[int]\n        comments: list[str]\n        iteration: int\n        position_in_batch: int\n        use_main_prompt: bool\n        index: int\n        all_negative_prompts: list[str]\n\n    Returns: str\n\n    Extra generation params\n    p.extra_generation_params dictionary allows for additional parameters to be added to the infotext\n    this can be use by the base webui or extensions.\n    To add a new entry, add a new key value pair, the dictionary key will be used as the key of the parameter in the infotext\n    the value generation_params can be defined as:\n        - str | None\n        - List[str|None]\n        - callable func(**kwargs) -> str | None\n\n    When defined as a string, it will be used as without extra processing; this is this most common use case.\n\n    Defining as a list allows for parameter that changes across images in the job, for example, the 'Seed' parameter.\n    The list should have the same length as the total number of images in the entire job.\n\n    Defining as a callable function allows parameter cannot be generated earlier or when extra logic is required.\n    For example 'Hires prompt', due to reasons the hr_prompt might be changed by process in the pipeline or extensions\n    and may vary across different images, defining as a static string or list would not work.\n\n    The function takes locals() as **kwargs, as such will have access to variables like 'p' and 'index'.\n    the base signature of the function should be:\n        func(**kwargs) -> str | None\n    optionally it can have additional arguments that will be used in the function:\n        func(p, index, **kwargs) -> str | None\n    note: for better future compatibility even though this function will have access to all variables in the locals(),\n        it is recommended to only use the arguments present in the function signature of create_infotext.\n    For actual implementation examples, see StableDiffusionProcessingTxt2Img.init > get_hr_prompt.\n    \"\"\"\n\n    if use_main_prompt:\n        index = 0\n    elif index is None:\n        index = position_in_batch + iteration * p.batch_size\n\n    if all_negative_prompts is None:\n        all_negative_prompts = p.all_negative_prompts\n\n    clip_skip = getattr(p, 'clip_skip', opts.CLIP_stop_at_last_layers)\n    enable_hr = getattr(p, 'enable_hr', False)\n    token_merging_ratio = p.get_token_merging_ratio()\n    token_merging_ratio_hr = p.get_token_merging_ratio(for_hr=True)\n\n    prompt_text = p.main_prompt if use_main_prompt else all_prompts[index]\n    negative_prompt = p.main_negative_prompt if use_main_prompt else all_negative_prompts[index]\n\n    uses_ensd = opts.eta_noise_seed_delta != 0\n    if uses_ensd:\n        uses_ensd = sd_samplers_common.is_sampler_using_eta_noise_seed_delta(p)\n\n    generation_params = {\n        \"Steps\": p.steps,\n        \"Sampler\": p.sampler_name,\n        \"Schedule type\": p.scheduler,\n        \"CFG scale\": p.cfg_scale,\n        \"Image CFG scale\": getattr(p, 'image_cfg_scale', None),\n        \"Seed\": p.all_seeds[0] if use_main_prompt else all_seeds[index],\n        \"Face restoration\": opts.face_restoration_model if p.restore_faces else None,\n        \"Size\": f\"{p.width}x{p.height}\",\n        \"Model hash\": p.sd_model_hash if opts.add_model_hash_to_info else None,\n        \"Model\": p.sd_model_name if opts.add_model_name_to_info else None,\n        \"FP8 weight\": opts.fp8_storage if devices.fp8 else None,\n        \"Cache FP16 weight for LoRA\": opts.cache_fp16_weight if devices.fp8 else None,\n        \"VAE hash\": p.sd_vae_hash if opts.add_vae_hash_to_info else None,\n        \"VAE\": p.sd_vae_name if opts.add_vae_name_to_info else None,\n        \"Variation seed\": (None if p.subseed_strength == 0 else (p.all_subseeds[0] if use_main_prompt else all_subseeds[index])),\n        \"Variation seed strength\": (None if p.subseed_strength == 0 else p.subseed_strength),\n        \"Seed resize from\": (None if p.seed_resize_from_w <= 0 or p.seed_resize_from_h <= 0 else f\"{p.seed_resize_from_w}x{p.seed_resize_from_h}\"),\n        \"Denoising strength\": p.extra_generation_params.get(\"Denoising strength\"),\n        \"Conditional mask weight\": getattr(p, \"inpainting_mask_weight\", shared.opts.inpainting_mask_weight) if p.is_using_inpainting_conditioning else None,\n        \"Clip skip\": None if clip_skip <= 1 else clip_skip,\n        \"ENSD\": opts.eta_noise_seed_delta if uses_ensd else None,\n        \"Token merging ratio\": None if token_merging_ratio == 0 else token_merging_ratio,\n        \"Token merging ratio hr\": None if not enable_hr or token_merging_ratio_hr == 0 else token_merging_ratio_hr,\n        \"Init image hash\": getattr(p, 'init_img_hash', None),\n        \"RNG\": opts.randn_source if opts.randn_source != \"GPU\" else None,\n        \"NGMS\": None if p.s_min_uncond == 0 else p.s_min_uncond,\n        \"Tiling\": \"True\" if p.tiling else None,\n        **p.extra_generation_params,\n        \"Version\": program_version() if opts.add_version_to_infotext else None,\n        \"User\": p.user if opts.add_user_name_to_info else None,\n    }\n\n    for key, value in generation_params.items():\n        try:\n            if isinstance(value, list):\n                generation_params[key] = value[index]\n            elif callable(value):\n                generation_params[key] = value(**locals())\n        except Exception:\n            errors.report(f'Error creating infotext for key \"{key}\"', exc_info=True)\n            generation_params[key] = None\n\n    generation_params_text = \", \".join([k if k == v else f'{k}: {infotext_utils.quote(v)}' for k, v in generation_params.items() if v is not None])\n\n    negative_prompt_text = f\"\\nNegative prompt: {negative_prompt}\" if negative_prompt else \"\"\n\n    return f\"{prompt_text}{negative_prompt_text}\\n{generation_params_text}\".strip()\n\n\ndef process_images(p: StableDiffusionProcessing) -> Processed:\n    if p.scripts is not None:\n        p.scripts.before_process(p)\n\n    stored_opts = {k: opts.data[k] if k in opts.data else opts.get_default(k) for k in p.override_settings.keys() if k in opts.data}\n\n    try:\n        # if no checkpoint override or the override checkpoint can't be found, remove override entry and load opts checkpoint\n        # and if after running refiner, the refiner model is not unloaded - webui swaps back to main model here, if model over is present it will be reloaded afterwards\n        if sd_models.checkpoint_aliases.get(p.override_settings.get('sd_model_checkpoint')) is None:\n            p.override_settings.pop('sd_model_checkpoint', None)\n            sd_models.reload_model_weights()\n\n        for k, v in p.override_settings.items():\n            opts.set(k, v, is_api=True, run_callbacks=False)\n\n            if k == 'sd_model_checkpoint':\n                sd_models.reload_model_weights()\n\n            if k == 'sd_vae':\n                sd_vae.reload_vae_weights()\n\n        sd_models.apply_token_merging(p.sd_model, p.get_token_merging_ratio())\n\n        res = process_images_inner(p)\n\n    finally:\n        sd_models.apply_token_merging(p.sd_model, 0)\n\n        # restore opts to original state\n        if p.override_settings_restore_afterwards:\n            for k, v in stored_opts.items():\n                setattr(opts, k, v)\n\n                if k == 'sd_vae':\n                    sd_vae.reload_vae_weights()\n\n    return res\n\n\ndef process_images_inner(p: StableDiffusionProcessing) -> Processed:\n    \"\"\"this is the main loop that both txt2img and img2img use; it calls func_init once inside all the scopes and func_sample once per batch\"\"\"\n\n    if isinstance(p.prompt, list):\n        assert(len(p.prompt) > 0)\n    else:\n        assert p.prompt is not None\n\n    devices.torch_gc()\n\n    seed = get_fixed_seed(p.seed)\n    subseed = get_fixed_seed(p.subseed)\n\n    if p.restore_faces is None:\n        p.restore_faces = opts.face_restoration\n\n    if p.tiling is None:\n        p.tiling = opts.tiling\n\n    if p.refiner_checkpoint not in (None, \"\", \"None\", \"none\"):\n        p.refiner_checkpoint_info = sd_models.get_closet_checkpoint_match(p.refiner_checkpoint)\n        if p.refiner_checkpoint_info is None:\n            raise Exception(f'Could not find checkpoint with name {p.refiner_checkpoint}')\n\n    p.sd_model_name = shared.sd_model.sd_checkpoint_info.name_for_extra\n    p.sd_model_hash = shared.sd_model.sd_model_hash\n    p.sd_vae_name = sd_vae.get_loaded_vae_name()\n    p.sd_vae_hash = sd_vae.get_loaded_vae_hash()\n\n    modules.sd_hijack.model_hijack.apply_circular(p.tiling)\n    modules.sd_hijack.model_hijack.clear_comments()\n\n    p.setup_prompts()\n\n    if isinstance(seed, list):\n        p.all_seeds = seed\n    else:\n        p.all_seeds = [int(seed) + (x if p.subseed_strength == 0 else 0) for x in range(len(p.all_prompts))]\n\n    if isinstance(subseed, list):\n        p.all_subseeds = subseed\n    else:\n        p.all_subseeds = [int(subseed) + x for x in range(len(p.all_prompts))]\n\n    if os.path.exists(cmd_opts.embeddings_dir) and not p.do_not_reload_embeddings:\n        model_hijack.embedding_db.load_textual_inversion_embeddings()\n\n    if p.scripts is not None:\n        p.scripts.process(p)\n\n    infotexts = []\n    output_images = []\n    with torch.no_grad(), p.sd_model.ema_scope():\n        with devices.autocast():\n            p.init(p.all_prompts, p.all_seeds, p.all_subseeds)\n\n            # for OSX, loading the model during sampling changes the generated picture, so it is loaded here\n            if shared.opts.live_previews_enable and opts.show_progress_type == \"Approx NN\":\n                sd_vae_approx.model()\n\n            sd_unet.apply_unet()\n\n        if state.job_count == -1:\n            state.job_count = p.n_iter\n\n        for n in range(p.n_iter):\n            p.iteration = n\n\n            if state.skipped:\n                state.skipped = False\n\n            if state.interrupted or state.stopping_generation:\n                break\n\n            sd_models.reload_model_weights()  # model can be changed for example by refiner\n\n            p.prompts = p.all_prompts[n * p.batch_size:(n + 1) * p.batch_size]\n            p.negative_prompts = p.all_negative_prompts[n * p.batch_size:(n + 1) * p.batch_size]\n            p.seeds = p.all_seeds[n * p.batch_size:(n + 1) * p.batch_size]\n            p.subseeds = p.all_subseeds[n * p.batch_size:(n + 1) * p.batch_size]\n\n            p.rng = rng.ImageRNG((opt_C, p.height // opt_f, p.width // opt_f), p.seeds, subseeds=p.subseeds, subseed_strength=p.subseed_strength, seed_resize_from_h=p.seed_resize_from_h, seed_resize_from_w=p.seed_resize_from_w)\n\n            if p.scripts is not None:\n                p.scripts.before_process_batch(p, batch_number=n, prompts=p.prompts, seeds=p.seeds, subseeds=p.subseeds)\n\n            if len(p.prompts) == 0:\n                break\n\n            p.parse_extra_network_prompts()\n\n            if not p.disable_extra_networks:\n                with devices.autocast():\n                    extra_networks.activate(p, p.extra_network_data)\n\n            if p.scripts is not None:\n                p.scripts.process_batch(p, batch_number=n, prompts=p.prompts, seeds=p.seeds, subseeds=p.subseeds)\n\n            p.setup_conds()\n\n            p.extra_generation_params.update(model_hijack.extra_generation_params)\n\n            # params.txt should be saved after scripts.process_batch, since the\n            # infotext could be modified by that callback\n            # Example: a wildcard processed by process_batch sets an extra model\n            # strength, which is saved as \"Model Strength: 1.0\" in the infotext\n            if n == 0 and not cmd_opts.no_prompt_history:\n                with open(os.path.join(paths.data_path, \"params.txt\"), \"w\", encoding=\"utf8\") as file:\n                    processed = Processed(p, [])\n                    file.write(processed.infotext(p, 0))\n\n            for comment in model_hijack.comments:\n                p.comment(comment)\n\n            if p.n_iter > 1:\n                shared.state.job = f\"Batch {n+1} out of {p.n_iter}\"\n\n            sd_models.apply_alpha_schedule_override(p.sd_model, p)\n\n            with devices.without_autocast() if devices.unet_needs_upcast else devices.autocast():\n                samples_ddim = p.sample(conditioning=p.c, unconditional_conditioning=p.uc, seeds=p.seeds, subseeds=p.subseeds, subseed_strength=p.subseed_strength, prompts=p.prompts)\n\n            if p.scripts is not None:\n                ps = scripts.PostSampleArgs(samples_ddim)\n                p.scripts.post_sample(p, ps)\n                samples_ddim = ps.samples\n\n            if getattr(samples_ddim, 'already_decoded', False):\n                x_samples_ddim = samples_ddim\n            else:\n                if opts.sd_vae_decode_method != 'Full':\n                    p.extra_generation_params['VAE Decoder'] = opts.sd_vae_decode_method\n                x_samples_ddim = decode_latent_batch(p.sd_model, samples_ddim, target_device=devices.cpu, check_for_nans=True)\n\n            x_samples_ddim = torch.stack(x_samples_ddim).float()\n            x_samples_ddim = torch.clamp((x_samples_ddim + 1.0) / 2.0, min=0.0, max=1.0)\n\n            del samples_ddim\n\n            if lowvram.is_enabled(shared.sd_model):\n                lowvram.send_everything_to_cpu()\n\n            devices.torch_gc()\n\n            state.nextjob()\n\n            if p.scripts is not None:\n                p.scripts.postprocess_batch(p, x_samples_ddim, batch_number=n)\n\n                p.prompts = p.all_prompts[n * p.batch_size:(n + 1) * p.batch_size]\n                p.negative_prompts = p.all_negative_prompts[n * p.batch_size:(n + 1) * p.batch_size]\n\n                batch_params = scripts.PostprocessBatchListArgs(list(x_samples_ddim))\n                p.scripts.postprocess_batch_list(p, batch_params, batch_number=n)\n                x_samples_ddim = batch_params.images\n\n            def infotext(index=0, use_main_prompt=False):\n                return create_infotext(p, p.prompts, p.seeds, p.subseeds, use_main_prompt=use_main_prompt, index=index, all_negative_prompts=p.negative_prompts)\n\n            save_samples = p.save_samples()\n\n            for i, x_sample in enumerate(x_samples_ddim):\n                p.batch_index = i\n\n                x_sample = 255. * np.moveaxis(x_sample.cpu().numpy(), 0, 2)\n                x_sample = x_sample.astype(np.uint8)\n\n                if p.restore_faces:\n                    if save_samples and opts.save_images_before_face_restoration:\n                        images.save_image(Image.fromarray(x_sample), p.outpath_samples, \"\", p.seeds[i], p.prompts[i], opts.samples_format, info=infotext(i), p=p, suffix=\"-before-face-restoration\")\n\n                    devices.torch_gc()\n\n                    x_sample = modules.face_restoration.restore_faces(x_sample)\n                    devices.torch_gc()\n\n                image = Image.fromarray(x_sample)\n\n                if p.scripts is not None:\n                    pp = scripts.PostprocessImageArgs(image)\n                    p.scripts.postprocess_image(p, pp)\n                    image = pp.image\n\n                mask_for_overlay = getattr(p, \"mask_for_overlay\", None)\n\n                if not shared.opts.overlay_inpaint:\n                    overlay_image = None\n                elif getattr(p, \"overlay_images\", None) is not None and i < len(p.overlay_images):\n                    overlay_image = p.overlay_images[i]\n                else:\n                    overlay_image = None\n\n                if p.scripts is not None:\n                    ppmo = scripts.PostProcessMaskOverlayArgs(i, mask_for_overlay, overlay_image)\n                    p.scripts.postprocess_maskoverlay(p, ppmo)\n                    mask_for_overlay, overlay_image = ppmo.mask_for_overlay, ppmo.overlay_image\n\n                if p.color_corrections is not None and i < len(p.color_corrections):\n                    if save_samples and opts.save_images_before_color_correction:\n                        image_without_cc, _ = apply_overlay(image, p.paste_to, overlay_image)\n                        images.save_image(image_without_cc, p.outpath_samples, \"\", p.seeds[i], p.prompts[i], opts.samples_format, info=infotext(i), p=p, suffix=\"-before-color-correction\")\n                    image = apply_color_correction(p.color_corrections[i], image)\n\n                # If the intention is to show the output from the model\n                # that is being composited over the original image,\n                # we need to keep the original image around\n                # and use it in the composite step.\n                image, original_denoised_image = apply_overlay(image, p.paste_to, overlay_image)\n\n                if p.scripts is not None:\n                    pp = scripts.PostprocessImageArgs(image)\n                    p.scripts.postprocess_image_after_composite(p, pp)\n                    image = pp.image\n\n                if save_samples:\n                    images.save_image(image, p.outpath_samples, \"\", p.seeds[i], p.prompts[i], opts.samples_format, info=infotext(i), p=p)\n\n                text = infotext(i)\n                infotexts.append(text)\n                if opts.enable_pnginfo:\n                    image.info[\"parameters\"] = text\n                output_images.append(image)\n\n                if mask_for_overlay is not None:\n                    if opts.return_mask or opts.save_mask:\n                        image_mask = mask_for_overlay.convert('RGB')\n                        if save_samples and opts.save_mask:\n                            images.save_image(image_mask, p.outpath_samples, \"\", p.seeds[i], p.prompts[i], opts.samples_format, info=infotext(i), p=p, suffix=\"-mask\")\n                        if opts.return_mask:\n                            output_images.append(image_mask)\n\n                    if opts.return_mask_composite or opts.save_mask_composite:\n                        image_mask_composite = Image.composite(original_denoised_image.convert('RGBA').convert('RGBa'), Image.new('RGBa', image.size), images.resize_image(2, mask_for_overlay, image.width, image.height).convert('L')).convert('RGBA')\n                        if save_samples and opts.save_mask_composite:\n                            images.save_image(image_mask_composite, p.outpath_samples, \"\", p.seeds[i], p.prompts[i], opts.samples_format, info=infotext(i), p=p, suffix=\"-mask-composite\")\n                        if opts.return_mask_composite:\n                            output_images.append(image_mask_composite)\n\n            del x_samples_ddim\n\n            devices.torch_gc()\n\n        if not infotexts:\n            infotexts.append(Processed(p, []).infotext(p, 0))\n\n        p.color_corrections = None\n\n        index_of_first_image = 0\n        unwanted_grid_because_of_img_count = len(output_images) < 2 and opts.grid_only_if_multiple\n        if (opts.return_grid or opts.grid_save) and not p.do_not_save_grid and not unwanted_grid_because_of_img_count:\n            grid = images.image_grid(output_images, p.batch_size)\n\n            if opts.return_grid:\n                text = infotext(use_main_prompt=True)\n                infotexts.insert(0, text)\n                if opts.enable_pnginfo:\n                    grid.info[\"parameters\"] = text\n                output_images.insert(0, grid)\n                index_of_first_image = 1\n            if opts.grid_save:\n                images.save_image(grid, p.outpath_grids, \"grid\", p.all_seeds[0], p.all_prompts[0], opts.grid_format, info=infotext(use_main_prompt=True), short_filename=not opts.grid_extended_filename, p=p, grid=True)\n\n    if not p.disable_extra_networks and p.extra_network_data:\n        extra_networks.deactivate(p, p.extra_network_data)\n\n    devices.torch_gc()\n\n    res = Processed(\n        p,\n        images_list=output_images,\n        seed=p.all_seeds[0],\n        info=infotexts[0],\n        subseed=p.all_subseeds[0],\n        index_of_first_image=index_of_first_image,\n        infotexts=infotexts,\n    )\n\n    if p.scripts is not None:\n        p.scripts.postprocess(p, res)\n\n    return res\n\n\ndef old_hires_fix_first_pass_dimensions(width, height):\n    \"\"\"old algorithm for auto-calculating first pass size\"\"\"\n\n    desired_pixel_count = 512 * 512\n    actual_pixel_count = width * height\n    scale = math.sqrt(desired_pixel_count / actual_pixel_count)\n    width = math.ceil(scale * width / 64) * 64\n    height = math.ceil(scale * height / 64) * 64\n\n    return width, height\n\n\n@dataclass(repr=False)\nclass StableDiffusionProcessingTxt2Img(StableDiffusionProcessing):\n    enable_hr: bool = False\n    denoising_strength: float = 0.75\n    firstphase_width: int = 0\n    firstphase_height: int = 0\n    hr_scale: float = 2.0\n    hr_upscaler: str = None\n    hr_second_pass_steps: int = 0\n    hr_resize_x: int = 0\n    hr_resize_y: int = 0\n    hr_checkpoint_name: str = None\n    hr_sampler_name: str = None\n    hr_scheduler: str = None\n    hr_prompt: str = ''\n    hr_negative_prompt: str = ''\n    force_task_id: str = None\n\n    cached_hr_uc = [None, None]\n    cached_hr_c = [None, None]\n\n    hr_checkpoint_info: dict = field(default=None, init=False)\n    hr_upscale_to_x: int = field(default=0, init=False)\n    hr_upscale_to_y: int = field(default=0, init=False)\n    truncate_x: int = field(default=0, init=False)\n    truncate_y: int = field(default=0, init=False)\n    applied_old_hires_behavior_to: tuple = field(default=None, init=False)\n    latent_scale_mode: dict = field(default=None, init=False)\n    hr_c: tuple | None = field(default=None, init=False)\n    hr_uc: tuple | None = field(default=None, init=False)\n    all_hr_prompts: list = field(default=None, init=False)\n    all_hr_negative_prompts: list = field(default=None, init=False)\n    hr_prompts: list = field(default=None, init=False)\n    hr_negative_prompts: list = field(default=None, init=False)\n    hr_extra_network_data: list = field(default=None, init=False)\n\n    def __post_init__(self):\n        super().__post_init__()\n\n        if self.firstphase_width != 0 or self.firstphase_height != 0:\n            self.hr_upscale_to_x = self.width\n            self.hr_upscale_to_y = self.height\n            self.width = self.firstphase_width\n            self.height = self.firstphase_height\n\n        self.cached_hr_uc = StableDiffusionProcessingTxt2Img.cached_hr_uc\n        self.cached_hr_c = StableDiffusionProcessingTxt2Img.cached_hr_c\n\n    def calculate_target_resolution(self):\n        if opts.use_old_hires_fix_width_height and self.applied_old_hires_behavior_to != (self.width, self.height):\n            self.hr_resize_x = self.width\n            self.hr_resize_y = self.height\n            self.hr_upscale_to_x = self.width\n            self.hr_upscale_to_y = self.height\n\n            self.width, self.height = old_hires_fix_first_pass_dimensions(self.width, self.height)\n            self.applied_old_hires_behavior_to = (self.width, self.height)\n\n        if self.hr_resize_x == 0 and self.hr_resize_y == 0:\n            self.extra_generation_params[\"Hires upscale\"] = self.hr_scale\n            self.hr_upscale_to_x = int(self.width * self.hr_scale)\n            self.hr_upscale_to_y = int(self.height * self.hr_scale)\n        else:\n            self.extra_generation_params[\"Hires resize\"] = f\"{self.hr_resize_x}x{self.hr_resize_y}\"\n\n            if self.hr_resize_y == 0:\n                self.hr_upscale_to_x = self.hr_resize_x\n                self.hr_upscale_to_y = self.hr_resize_x * self.height // self.width\n            elif self.hr_resize_x == 0:\n                self.hr_upscale_to_x = self.hr_resize_y * self.width // self.height\n                self.hr_upscale_to_y = self.hr_resize_y\n            else:\n                target_w = self.hr_resize_x\n                target_h = self.hr_resize_y\n                src_ratio = self.width / self.height\n                dst_ratio = self.hr_resize_x / self.hr_resize_y\n\n                if src_ratio < dst_ratio:\n                    self.hr_upscale_to_x = self.hr_resize_x\n                    self.hr_upscale_to_y = self.hr_resize_x * self.height // self.width\n                else:\n                    self.hr_upscale_to_x = self.hr_resize_y * self.width // self.height\n                    self.hr_upscale_to_y = self.hr_resize_y\n\n                self.truncate_x = (self.hr_upscale_to_x - target_w) // opt_f\n                self.truncate_y = (self.hr_upscale_to_y - target_h) // opt_f\n\n    def init(self, all_prompts, all_seeds, all_subseeds):\n        if self.enable_hr:\n            self.extra_generation_params[\"Denoising strength\"] = self.denoising_strength\n\n            if self.hr_checkpoint_name and self.hr_checkpoint_name != 'Use same checkpoint':\n                self.hr_checkpoint_info = sd_models.get_closet_checkpoint_match(self.hr_checkpoint_name)\n\n                if self.hr_checkpoint_info is None:\n                    raise Exception(f'Could not find checkpoint with name {self.hr_checkpoint_name}')\n\n                self.extra_generation_params[\"Hires checkpoint\"] = self.hr_checkpoint_info.short_title\n\n            if self.hr_sampler_name is not None and self.hr_sampler_name != self.sampler_name:\n                self.extra_generation_params[\"Hires sampler\"] = self.hr_sampler_name\n\n            def get_hr_prompt(p, index, prompt_text, **kwargs):\n                hr_prompt = p.all_hr_prompts[index]\n                return hr_prompt if hr_prompt != prompt_text else None\n\n            def get_hr_negative_prompt(p, index, negative_prompt, **kwargs):\n                hr_negative_prompt = p.all_hr_negative_prompts[index]\n                return hr_negative_prompt if hr_negative_prompt != negative_prompt else None\n\n            self.extra_generation_params[\"Hires prompt\"] = get_hr_prompt\n            self.extra_generation_params[\"Hires negative prompt\"] = get_hr_negative_prompt\n\n            self.extra_generation_params[\"Hires schedule type\"] = None  # to be set in sd_samplers_kdiffusion.py\n\n            if self.hr_scheduler is None:\n                self.hr_scheduler = self.scheduler\n\n            self.latent_scale_mode = shared.latent_upscale_modes.get(self.hr_upscaler, None) if self.hr_upscaler is not None else shared.latent_upscale_modes.get(shared.latent_upscale_default_mode, \"nearest\")\n            if self.enable_hr and self.latent_scale_mode is None:\n                if not any(x.name == self.hr_upscaler for x in shared.sd_upscalers):\n                    raise Exception(f\"could not find upscaler named {self.hr_upscaler}\")\n\n            self.calculate_target_resolution()\n\n            if not state.processing_has_refined_job_count:\n                if state.job_count == -1:\n                    state.job_count = self.n_iter\n                if getattr(self, 'txt2img_upscale', False):\n                    total_steps = (self.hr_second_pass_steps or self.steps) * state.job_count\n                else:\n                    total_steps = (self.steps + (self.hr_second_pass_steps or self.steps)) * state.job_count\n                shared.total_tqdm.updateTotal(total_steps)\n                state.job_count = state.job_count * 2\n                state.processing_has_refined_job_count = True\n\n            if self.hr_second_pass_steps:\n                self.extra_generation_params[\"Hires steps\"] = self.hr_second_pass_steps\n\n            if self.hr_upscaler is not None:\n                self.extra_generation_params[\"Hires upscaler\"] = self.hr_upscaler\n\n    def sample(self, conditioning, unconditional_conditioning, seeds, subseeds, subseed_strength, prompts):\n        self.sampler = sd_samplers.create_sampler(self.sampler_name, self.sd_model)\n\n        if self.firstpass_image is not None and self.enable_hr:\n            # here we don't need to generate image, we just take self.firstpass_image and prepare it for hires fix\n\n            if self.latent_scale_mode is None:\n                image = np.array(self.firstpass_image).astype(np.float32) / 255.0 * 2.0 - 1.0\n                image = np.moveaxis(image, 2, 0)\n\n                samples = None\n                decoded_samples = torch.asarray(np.expand_dims(image, 0))\n\n            else:\n                image = np.array(self.firstpass_image).astype(np.float32) / 255.0\n                image = np.moveaxis(image, 2, 0)\n                image = torch.from_numpy(np.expand_dims(image, axis=0))\n                image = image.to(shared.device, dtype=devices.dtype_vae)\n\n                if opts.sd_vae_encode_method != 'Full':\n                    self.extra_generation_params['VAE Encoder'] = opts.sd_vae_encode_method\n\n                samples = images_tensor_to_samples(image, approximation_indexes.get(opts.sd_vae_encode_method), self.sd_model)\n                decoded_samples = None\n                devices.torch_gc()\n\n        else:\n            # here we generate an image normally\n\n            x = self.rng.next()\n            samples = self.sampler.sample(self, x, conditioning, unconditional_conditioning, image_conditioning=self.txt2img_image_conditioning(x))\n            del x\n\n            if not self.enable_hr:\n                return samples\n\n            devices.torch_gc()\n\n            if self.latent_scale_mode is None:\n                decoded_samples = torch.stack(decode_latent_batch(self.sd_model, samples, target_device=devices.cpu, check_for_nans=True)).to(dtype=torch.float32)\n            else:\n                decoded_samples = None\n\n        with sd_models.SkipWritingToConfig():\n            sd_models.reload_model_weights(info=self.hr_checkpoint_info)\n\n        return self.sample_hr_pass(samples, decoded_samples, seeds, subseeds, subseed_strength, prompts)\n\n    def sample_hr_pass(self, samples, decoded_samples, seeds, subseeds, subseed_strength, prompts):\n        if shared.state.interrupted:\n            return samples\n\n        self.is_hr_pass = True\n        target_width = self.hr_upscale_to_x\n        target_height = self.hr_upscale_to_y\n\n        def save_intermediate(image, index):\n            \"\"\"saves image before applying hires fix, if enabled in options; takes as an argument either an image or batch with latent space images\"\"\"\n\n            if not self.save_samples() or not opts.save_images_before_highres_fix:\n                return\n\n            if not isinstance(image, Image.Image):\n                image = sd_samplers.sample_to_image(image, index, approximation=0)\n\n            info = create_infotext(self, self.all_prompts, self.all_seeds, self.all_subseeds, [], iteration=self.iteration, position_in_batch=index)\n            images.save_image(image, self.outpath_samples, \"\", seeds[index], prompts[index], opts.samples_format, info=info, p=self, suffix=\"-before-highres-fix\")\n\n        img2img_sampler_name = self.hr_sampler_name or self.sampler_name\n\n        self.sampler = sd_samplers.create_sampler(img2img_sampler_name, self.sd_model)\n\n        if self.latent_scale_mode is not None:\n            for i in range(samples.shape[0]):\n                save_intermediate(samples, i)\n\n            samples = torch.nn.functional.interpolate(samples, size=(target_height // opt_f, target_width // opt_f), mode=self.latent_scale_mode[\"mode\"], antialias=self.latent_scale_mode[\"antialias\"])\n\n            # Avoid making the inpainting conditioning unless necessary as\n            # this does need some extra compute to decode / encode the image again.\n            if getattr(self, \"inpainting_mask_weight\", shared.opts.inpainting_mask_weight) < 1.0:\n                image_conditioning = self.img2img_image_conditioning(decode_first_stage(self.sd_model, samples), samples)\n            else:\n                image_conditioning = self.txt2img_image_conditioning(samples)\n        else:\n            lowres_samples = torch.clamp((decoded_samples + 1.0) / 2.0, min=0.0, max=1.0)\n\n            batch_images = []\n            for i, x_sample in enumerate(lowres_samples):\n                x_sample = 255. * np.moveaxis(x_sample.cpu().numpy(), 0, 2)\n                x_sample = x_sample.astype(np.uint8)\n                image = Image.fromarray(x_sample)\n\n                save_intermediate(image, i)\n\n                image = images.resize_image(0, image, target_width, target_height, upscaler_name=self.hr_upscaler)\n                image = np.array(image).astype(np.float32) / 255.0\n                image = np.moveaxis(image, 2, 0)\n                batch_images.append(image)\n\n            decoded_samples = torch.from_numpy(np.array(batch_images))\n            decoded_samples = decoded_samples.to(shared.device, dtype=devices.dtype_vae)\n\n            if opts.sd_vae_encode_method != 'Full':\n                self.extra_generation_params['VAE Encoder'] = opts.sd_vae_encode_method\n            samples = images_tensor_to_samples(decoded_samples, approximation_indexes.get(opts.sd_vae_encode_method))\n\n            image_conditioning = self.img2img_image_conditioning(decoded_samples, samples)\n\n        shared.state.nextjob()\n\n        samples = samples[:, :, self.truncate_y//2:samples.shape[2]-(self.truncate_y+1)//2, self.truncate_x//2:samples.shape[3]-(self.truncate_x+1)//2]\n\n        self.rng = rng.ImageRNG(samples.shape[1:], self.seeds, subseeds=self.subseeds, subseed_strength=self.subseed_strength, seed_resize_from_h=self.seed_resize_from_h, seed_resize_from_w=self.seed_resize_from_w)\n        noise = self.rng.next()\n\n        # GC now before running the next img2img to prevent running out of memory\n        devices.torch_gc()\n\n        if not self.disable_extra_networks:\n            with devices.autocast():\n                extra_networks.activate(self, self.hr_extra_network_data)\n\n        with devices.autocast():\n            self.calculate_hr_conds()\n\n        sd_models.apply_token_merging(self.sd_model, self.get_token_merging_ratio(for_hr=True))\n\n        if self.scripts is not None:\n            self.scripts.before_hr(self)\n\n        samples = self.sampler.sample_img2img(self, samples, noise, self.hr_c, self.hr_uc, steps=self.hr_second_pass_steps or self.steps, image_conditioning=image_conditioning)\n\n        sd_models.apply_token_merging(self.sd_model, self.get_token_merging_ratio())\n\n        self.sampler = None\n        devices.torch_gc()\n\n        decoded_samples = decode_latent_batch(self.sd_model, samples, target_device=devices.cpu, check_for_nans=True)\n\n        self.is_hr_pass = False\n        return decoded_samples\n\n    def close(self):\n        super().close()\n        self.hr_c = None\n        self.hr_uc = None\n        if not opts.persistent_cond_cache:\n            StableDiffusionProcessingTxt2Img.cached_hr_uc = [None, None]\n            StableDiffusionProcessingTxt2Img.cached_hr_c = [None, None]\n\n    def setup_prompts(self):\n        super().setup_prompts()\n\n        if not self.enable_hr:\n            return\n\n        if self.hr_prompt == '':\n            self.hr_prompt = self.prompt\n\n        if self.hr_negative_prompt == '':\n            self.hr_negative_prompt = self.negative_prompt\n\n        if isinstance(self.hr_prompt, list):\n            self.all_hr_prompts = self.hr_prompt\n        else:\n            self.all_hr_prompts = self.batch_size * self.n_iter * [self.hr_prompt]\n\n        if isinstance(self.hr_negative_prompt, list):\n            self.all_hr_negative_prompts = self.hr_negative_prompt\n        else:\n            self.all_hr_negative_prompts = self.batch_size * self.n_iter * [self.hr_negative_prompt]\n\n        self.all_hr_prompts = [shared.prompt_styles.apply_styles_to_prompt(x, self.styles) for x in self.all_hr_prompts]\n        self.all_hr_negative_prompts = [shared.prompt_styles.apply_negative_styles_to_prompt(x, self.styles) for x in self.all_hr_negative_prompts]\n\n    def calculate_hr_conds(self):\n        if self.hr_c is not None:\n            return\n\n        hr_prompts = prompt_parser.SdConditioning(self.hr_prompts, width=self.hr_upscale_to_x, height=self.hr_upscale_to_y)\n        hr_negative_prompts = prompt_parser.SdConditioning(self.hr_negative_prompts, width=self.hr_upscale_to_x, height=self.hr_upscale_to_y, is_negative_prompt=True)\n\n        sampler_config = sd_samplers.find_sampler_config(self.hr_sampler_name or self.sampler_name)\n        steps = self.hr_second_pass_steps or self.steps\n        total_steps = sampler_config.total_steps(steps) if sampler_config else steps\n\n        self.hr_uc = self.get_conds_with_caching(prompt_parser.get_learned_conditioning, hr_negative_prompts, self.firstpass_steps, [self.cached_hr_uc, self.cached_uc], self.hr_extra_network_data, total_steps)\n        self.hr_c = self.get_conds_with_caching(prompt_parser.get_multicond_learned_conditioning, hr_prompts, self.firstpass_steps, [self.cached_hr_c, self.cached_c], self.hr_extra_network_data, total_steps)\n\n    def setup_conds(self):\n        if self.is_hr_pass:\n            # if we are in hr pass right now, the call is being made from the refiner, and we don't need to setup firstpass cons or switch model\n            self.hr_c = None\n            self.calculate_hr_conds()\n            return\n\n        super().setup_conds()\n\n        self.hr_uc = None\n        self.hr_c = None\n\n        if self.enable_hr and self.hr_checkpoint_info is None:\n            if shared.opts.hires_fix_use_firstpass_conds:\n                self.calculate_hr_conds()\n\n            elif lowvram.is_enabled(shared.sd_model) and shared.sd_model.sd_checkpoint_info == sd_models.select_checkpoint():  # if in lowvram mode, we need to calculate conds right away, before the cond NN is unloaded\n                with devices.autocast():\n                    extra_networks.activate(self, self.hr_extra_network_data)\n\n                self.calculate_hr_conds()\n\n                with devices.autocast():\n                    extra_networks.activate(self, self.extra_network_data)\n\n    def get_conds(self):\n        if self.is_hr_pass:\n            return self.hr_c, self.hr_uc\n\n        return super().get_conds()\n\n    def parse_extra_network_prompts(self):\n        res = super().parse_extra_network_prompts()\n\n        if self.enable_hr:\n            self.hr_prompts = self.all_hr_prompts[self.iteration * self.batch_size:(self.iteration + 1) * self.batch_size]\n            self.hr_negative_prompts = self.all_hr_negative_prompts[self.iteration * self.batch_size:(self.iteration + 1) * self.batch_size]\n\n            self.hr_prompts, self.hr_extra_network_data = extra_networks.parse_prompts(self.hr_prompts)\n\n        return res\n\n\n@dataclass(repr=False)\nclass StableDiffusionProcessingImg2Img(StableDiffusionProcessing):\n    init_images: list = None\n    resize_mode: int = 0\n    denoising_strength: float = 0.75\n    image_cfg_scale: float = None\n    mask: Any = None\n    mask_blur_x: int = 4\n    mask_blur_y: int = 4\n    mask_blur: int = None\n    mask_round: bool = True\n    inpainting_fill: int = 0\n    inpaint_full_res: bool = True\n    inpaint_full_res_padding: int = 0\n    inpainting_mask_invert: int = 0\n    initial_noise_multiplier: float = None\n    latent_mask: Image = None\n    force_task_id: str = None\n\n    image_mask: Any = field(default=None, init=False)\n\n    nmask: torch.Tensor = field(default=None, init=False)\n    image_conditioning: torch.Tensor = field(default=None, init=False)\n    init_img_hash: str = field(default=None, init=False)\n    mask_for_overlay: Image = field(default=None, init=False)\n    init_latent: torch.Tensor = field(default=None, init=False)\n\n    def __post_init__(self):\n        super().__post_init__()\n\n        self.image_mask = self.mask\n        self.mask = None\n        self.initial_noise_multiplier = opts.initial_noise_multiplier if self.initial_noise_multiplier is None else self.initial_noise_multiplier\n\n    @property\n    def mask_blur(self):\n        if self.mask_blur_x == self.mask_blur_y:\n            return self.mask_blur_x\n        return None\n\n    @mask_blur.setter\n    def mask_blur(self, value):\n        if isinstance(value, int):\n            self.mask_blur_x = value\n            self.mask_blur_y = value\n\n    def init(self, all_prompts, all_seeds, all_subseeds):\n        self.extra_generation_params[\"Denoising strength\"] = self.denoising_strength\n\n        self.image_cfg_scale: float = self.image_cfg_scale if shared.sd_model.cond_stage_key == \"edit\" else None\n\n        self.sampler = sd_samplers.create_sampler(self.sampler_name, self.sd_model)\n        crop_region = None\n\n        image_mask = self.image_mask\n\n        if image_mask is not None:\n            # image_mask is passed in as RGBA by Gradio to support alpha masks,\n            # but we still want to support binary masks.\n            image_mask = create_binary_mask(image_mask, round=self.mask_round)\n\n            if self.inpainting_mask_invert:\n                image_mask = ImageOps.invert(image_mask)\n                self.extra_generation_params[\"Mask mode\"] = \"Inpaint not masked\"\n\n            if self.mask_blur_x > 0:\n                np_mask = np.array(image_mask)\n                kernel_size = 2 * int(2.5 * self.mask_blur_x + 0.5) + 1\n                np_mask = cv2.GaussianBlur(np_mask, (kernel_size, 1), self.mask_blur_x)\n                image_mask = Image.fromarray(np_mask)\n\n            if self.mask_blur_y > 0:\n                np_mask = np.array(image_mask)\n                kernel_size = 2 * int(2.5 * self.mask_blur_y + 0.5) + 1\n                np_mask = cv2.GaussianBlur(np_mask, (1, kernel_size), self.mask_blur_y)\n                image_mask = Image.fromarray(np_mask)\n\n            if self.mask_blur_x > 0 or self.mask_blur_y > 0:\n                self.extra_generation_params[\"Mask blur\"] = self.mask_blur\n\n            if self.inpaint_full_res:\n                self.mask_for_overlay = image_mask\n                mask = image_mask.convert('L')\n                crop_region = masking.get_crop_region_v2(mask, self.inpaint_full_res_padding)\n                if crop_region:\n                    crop_region = masking.expand_crop_region(crop_region, self.width, self.height, mask.width, mask.height)\n                    x1, y1, x2, y2 = crop_region\n                    mask = mask.crop(crop_region)\n                    image_mask = images.resize_image(2, mask, self.width, self.height)\n                    self.paste_to = (x1, y1, x2-x1, y2-y1)\n                    self.extra_generation_params[\"Inpaint area\"] = \"Only masked\"\n                    self.extra_generation_params[\"Masked area padding\"] = self.inpaint_full_res_padding\n                else:\n                    crop_region = None\n                    image_mask = None\n                    self.mask_for_overlay = None\n                    self.inpaint_full_res = False\n                    massage = 'Unable to perform \"Inpaint Only mask\" because mask is blank, switch to img2img mode.'\n                    model_hijack.comments.append(massage)\n                    logging.info(massage)\n            else:\n                image_mask = images.resize_image(self.resize_mode, image_mask, self.width, self.height)\n                np_mask = np.array(image_mask)\n                np_mask = np.clip((np_mask.astype(np.float32)) * 2, 0, 255).astype(np.uint8)\n                self.mask_for_overlay = Image.fromarray(np_mask)\n\n            self.overlay_images = []\n\n        latent_mask = self.latent_mask if self.latent_mask is not None else image_mask\n\n        add_color_corrections = opts.img2img_color_correction and self.color_corrections is None\n        if add_color_corrections:\n            self.color_corrections = []\n        imgs = []\n        for img in self.init_images:\n\n            # Save init image\n            if opts.save_init_img:\n                self.init_img_hash = hashlib.md5(img.tobytes()).hexdigest()\n                images.save_image(img, path=opts.outdir_init_images, basename=None, forced_filename=self.init_img_hash, save_to_dirs=False, existing_info=img.info)\n\n            image = images.flatten(img, opts.img2img_background_color)\n\n            if crop_region is None and self.resize_mode != 3:\n                image = images.resize_image(self.resize_mode, image, self.width, self.height)\n\n            if image_mask is not None:\n                if self.mask_for_overlay.size != (image.width, image.height):\n                    self.mask_for_overlay = images.resize_image(self.resize_mode, self.mask_for_overlay, image.width, image.height)\n                image_masked = Image.new('RGBa', (image.width, image.height))\n                image_masked.paste(image.convert(\"RGBA\").convert(\"RGBa\"), mask=ImageOps.invert(self.mask_for_overlay.convert('L')))\n\n                self.overlay_images.append(image_masked.convert('RGBA'))\n\n            # crop_region is not None if we are doing inpaint full res\n            if crop_region is not None:\n                image = image.crop(crop_region)\n                image = images.resize_image(2, image, self.width, self.height)\n\n            if image_mask is not None:\n                if self.inpainting_fill != 1:\n                    image = masking.fill(image, latent_mask)\n\n                    if self.inpainting_fill == 0:\n                        self.extra_generation_params[\"Masked content\"] = 'fill'\n\n            if add_color_corrections:\n                self.color_corrections.append(setup_color_correction(image))\n\n            image = np.array(image).astype(np.float32) / 255.0\n            image = np.moveaxis(image, 2, 0)\n\n            imgs.append(image)\n\n        if len(imgs) == 1:\n            batch_images = np.expand_dims(imgs[0], axis=0).repeat(self.batch_size, axis=0)\n            if self.overlay_images is not None:\n                self.overlay_images = self.overlay_images * self.batch_size\n\n            if self.color_corrections is not None and len(self.color_corrections) == 1:\n                self.color_corrections = self.color_corrections * self.batch_size\n\n        elif len(imgs) <= self.batch_size:\n            self.batch_size = len(imgs)\n            batch_images = np.array(imgs)\n        else:\n            raise RuntimeError(f\"bad number of images passed: {len(imgs)}; expecting {self.batch_size} or less\")\n\n        image = torch.from_numpy(batch_images)\n        image = image.to(shared.device, dtype=devices.dtype_vae)\n\n        if opts.sd_vae_encode_method != 'Full':\n            self.extra_generation_params['VAE Encoder'] = opts.sd_vae_encode_method\n\n        self.init_latent = images_tensor_to_samples(image, approximation_indexes.get(opts.sd_vae_encode_method), self.sd_model)\n        devices.torch_gc()\n\n        if self.resize_mode == 3:\n            self.init_latent = torch.nn.functional.interpolate(self.init_latent, size=(self.height // opt_f, self.width // opt_f), mode=\"bilinear\")\n\n        if image_mask is not None:\n            init_mask = latent_mask\n            latmask = init_mask.convert('RGB').resize((self.init_latent.shape[3], self.init_latent.shape[2]))\n            latmask = np.moveaxis(np.array(latmask, dtype=np.float32), 2, 0) / 255\n            latmask = latmask[0]\n            if self.mask_round:\n                latmask = np.around(latmask)\n            latmask = np.tile(latmask[None], (4, 1, 1))\n\n            self.mask = torch.asarray(1.0 - latmask).to(shared.device).type(self.sd_model.dtype)\n            self.nmask = torch.asarray(latmask).to(shared.device).type(self.sd_model.dtype)\n\n            # this needs to be fixed to be done in sample() using actual seeds for batches\n            if self.inpainting_fill == 2:\n                self.init_latent = self.init_latent * self.mask + create_random_tensors(self.init_latent.shape[1:], all_seeds[0:self.init_latent.shape[0]]) * self.nmask\n                self.extra_generation_params[\"Masked content\"] = 'latent noise'\n\n            elif self.inpainting_fill == 3:\n                self.init_latent = self.init_latent * self.mask\n                self.extra_generation_params[\"Masked content\"] = 'latent nothing'\n\n        self.image_conditioning = self.img2img_image_conditioning(image * 2 - 1, self.init_latent, image_mask, self.mask_round)\n\n    def sample(self, conditioning, unconditional_conditioning, seeds, subseeds, subseed_strength, prompts):\n        x = self.rng.next()\n\n        if self.initial_noise_multiplier != 1.0:\n            self.extra_generation_params[\"Noise multiplier\"] = self.initial_noise_multiplier\n            x *= self.initial_noise_multiplier\n\n        samples = self.sampler.sample_img2img(self, self.init_latent, x, conditioning, unconditional_conditioning, image_conditioning=self.image_conditioning)\n\n        if self.mask is not None:\n            blended_samples = samples * self.nmask + self.init_latent * self.mask\n\n            if self.scripts is not None:\n                mba = scripts.MaskBlendArgs(samples, self.nmask, self.init_latent, self.mask, blended_samples)\n                self.scripts.on_mask_blend(self, mba)\n                blended_samples = mba.blended_latent\n\n            samples = blended_samples\n\n        del x\n        devices.torch_gc()\n\n        return samples\n\n    def get_token_merging_ratio(self, for_hr=False):\n        return self.token_merging_ratio or (\"token_merging_ratio\" in self.override_settings and opts.token_merging_ratio) or opts.token_merging_ratio_img2img or opts.token_merging_ratio\n", "modules/sd_samplers_cfg_denoiser.py": "import torch\nfrom modules import prompt_parser, devices, sd_samplers_common\n\nfrom modules.shared import opts, state\nimport modules.shared as shared\nfrom modules.script_callbacks import CFGDenoiserParams, cfg_denoiser_callback\nfrom modules.script_callbacks import CFGDenoisedParams, cfg_denoised_callback\nfrom modules.script_callbacks import AfterCFGCallbackParams, cfg_after_cfg_callback\n\n\ndef catenate_conds(conds):\n    if not isinstance(conds[0], dict):\n        return torch.cat(conds)\n\n    return {key: torch.cat([x[key] for x in conds]) for key in conds[0].keys()}\n\n\ndef subscript_cond(cond, a, b):\n    if not isinstance(cond, dict):\n        return cond[a:b]\n\n    return {key: vec[a:b] for key, vec in cond.items()}\n\n\ndef pad_cond(tensor, repeats, empty):\n    if not isinstance(tensor, dict):\n        return torch.cat([tensor, empty.repeat((tensor.shape[0], repeats, 1))], axis=1)\n\n    tensor['crossattn'] = pad_cond(tensor['crossattn'], repeats, empty)\n    return tensor\n\n\nclass CFGDenoiser(torch.nn.Module):\n    \"\"\"\n    Classifier free guidance denoiser. A wrapper for stable diffusion model (specifically for unet)\n    that can take a noisy picture and produce a noise-free picture using two guidances (prompts)\n    instead of one. Originally, the second prompt is just an empty string, but we use non-empty\n    negative prompt.\n    \"\"\"\n\n    def __init__(self, sampler):\n        super().__init__()\n        self.model_wrap = None\n        self.mask = None\n        self.nmask = None\n        self.init_latent = None\n        self.steps = None\n        \"\"\"number of steps as specified by user in UI\"\"\"\n\n        self.total_steps = None\n        \"\"\"expected number of calls to denoiser calculated from self.steps and specifics of the selected sampler\"\"\"\n\n        self.step = 0\n        self.image_cfg_scale = None\n        self.padded_cond_uncond = False\n        self.padded_cond_uncond_v0 = False\n        self.sampler = sampler\n        self.model_wrap = None\n        self.p = None\n\n        # NOTE: masking before denoising can cause the original latents to be oversmoothed\n        # as the original latents do not have noise\n        self.mask_before_denoising = False\n\n    @property\n    def inner_model(self):\n        raise NotImplementedError()\n\n    def combine_denoised(self, x_out, conds_list, uncond, cond_scale):\n        denoised_uncond = x_out[-uncond.shape[0]:]\n        denoised = torch.clone(denoised_uncond)\n\n        for i, conds in enumerate(conds_list):\n            for cond_index, weight in conds:\n                denoised[i] += (x_out[cond_index] - denoised_uncond[i]) * (weight * cond_scale)\n\n        return denoised\n\n    def combine_denoised_for_edit_model(self, x_out, cond_scale):\n        out_cond, out_img_cond, out_uncond = x_out.chunk(3)\n        denoised = out_uncond + cond_scale * (out_cond - out_img_cond) + self.image_cfg_scale * (out_img_cond - out_uncond)\n\n        return denoised\n\n    def get_pred_x0(self, x_in, x_out, sigma):\n        return x_out\n\n    def update_inner_model(self):\n        self.model_wrap = None\n\n        c, uc = self.p.get_conds()\n        self.sampler.sampler_extra_args['cond'] = c\n        self.sampler.sampler_extra_args['uncond'] = uc\n\n    def pad_cond_uncond(self, cond, uncond):\n        empty = shared.sd_model.cond_stage_model_empty_prompt\n        num_repeats = (cond.shape[1] - uncond.shape[1]) // empty.shape[1]\n\n        if num_repeats < 0:\n            cond = pad_cond(cond, -num_repeats, empty)\n            self.padded_cond_uncond = True\n        elif num_repeats > 0:\n            uncond = pad_cond(uncond, num_repeats, empty)\n            self.padded_cond_uncond = True\n\n        return cond, uncond\n\n    def pad_cond_uncond_v0(self, cond, uncond):\n        \"\"\"\n        Pads the 'uncond' tensor to match the shape of the 'cond' tensor.\n\n        If 'uncond' is a dictionary, it is assumed that the 'crossattn' key holds the tensor to be padded.\n        If 'uncond' is a tensor, it is padded directly.\n\n        If the number of columns in 'uncond' is less than the number of columns in 'cond', the last column of 'uncond'\n        is repeated to match the number of columns in 'cond'.\n\n        If the number of columns in 'uncond' is greater than the number of columns in 'cond', 'uncond' is truncated\n        to match the number of columns in 'cond'.\n\n        Args:\n            cond (torch.Tensor or DictWithShape): The condition tensor to match the shape of 'uncond'.\n            uncond (torch.Tensor or DictWithShape): The tensor to be padded, or a dictionary containing the tensor to be padded.\n\n        Returns:\n            tuple: A tuple containing the 'cond' tensor and the padded 'uncond' tensor.\n\n        Note:\n            This is the padding that was always used in DDIM before version 1.6.0\n        \"\"\"\n\n        is_dict_cond = isinstance(uncond, dict)\n        uncond_vec = uncond['crossattn'] if is_dict_cond else uncond\n\n        if uncond_vec.shape[1] < cond.shape[1]:\n            last_vector = uncond_vec[:, -1:]\n            last_vector_repeated = last_vector.repeat([1, cond.shape[1] - uncond_vec.shape[1], 1])\n            uncond_vec = torch.hstack([uncond_vec, last_vector_repeated])\n            self.padded_cond_uncond_v0 = True\n        elif uncond_vec.shape[1] > cond.shape[1]:\n            uncond_vec = uncond_vec[:, :cond.shape[1]]\n            self.padded_cond_uncond_v0 = True\n\n        if is_dict_cond:\n            uncond['crossattn'] = uncond_vec\n        else:\n            uncond = uncond_vec\n\n        return cond, uncond\n\n    def forward(self, x, sigma, uncond, cond, cond_scale, s_min_uncond, image_cond):\n        if state.interrupted or state.skipped:\n            raise sd_samplers_common.InterruptedException\n\n        if sd_samplers_common.apply_refiner(self, sigma):\n            cond = self.sampler.sampler_extra_args['cond']\n            uncond = self.sampler.sampler_extra_args['uncond']\n\n        # at self.image_cfg_scale == 1.0 produced results for edit model are the same as with normal sampling,\n        # so is_edit_model is set to False to support AND composition.\n        is_edit_model = shared.sd_model.cond_stage_key == \"edit\" and self.image_cfg_scale is not None and self.image_cfg_scale != 1.0\n\n        conds_list, tensor = prompt_parser.reconstruct_multicond_batch(cond, self.step)\n        uncond = prompt_parser.reconstruct_cond_batch(uncond, self.step)\n\n        assert not is_edit_model or all(len(conds) == 1 for conds in conds_list), \"AND is not supported for InstructPix2Pix checkpoint (unless using Image CFG scale = 1.0)\"\n\n        # If we use masks, blending between the denoised and original latent images occurs here.\n        def apply_blend(current_latent):\n            blended_latent = current_latent * self.nmask + self.init_latent * self.mask\n\n            if self.p.scripts is not None:\n                from modules import scripts\n                mba = scripts.MaskBlendArgs(current_latent, self.nmask, self.init_latent, self.mask, blended_latent, denoiser=self, sigma=sigma)\n                self.p.scripts.on_mask_blend(self.p, mba)\n                blended_latent = mba.blended_latent\n\n            return blended_latent\n\n        # Blend in the original latents (before)\n        if self.mask_before_denoising and self.mask is not None:\n            x = apply_blend(x)\n\n        batch_size = len(conds_list)\n        repeats = [len(conds_list[i]) for i in range(batch_size)]\n\n        if shared.sd_model.model.conditioning_key == \"crossattn-adm\":\n            image_uncond = torch.zeros_like(image_cond)\n            make_condition_dict = lambda c_crossattn, c_adm: {\"c_crossattn\": [c_crossattn], \"c_adm\": c_adm}\n        else:\n            image_uncond = image_cond\n            if isinstance(uncond, dict):\n                make_condition_dict = lambda c_crossattn, c_concat: {**c_crossattn, \"c_concat\": [c_concat]}\n            else:\n                make_condition_dict = lambda c_crossattn, c_concat: {\"c_crossattn\": [c_crossattn], \"c_concat\": [c_concat]}\n\n        if not is_edit_model:\n            x_in = torch.cat([torch.stack([x[i] for _ in range(n)]) for i, n in enumerate(repeats)] + [x])\n            sigma_in = torch.cat([torch.stack([sigma[i] for _ in range(n)]) for i, n in enumerate(repeats)] + [sigma])\n            image_cond_in = torch.cat([torch.stack([image_cond[i] for _ in range(n)]) for i, n in enumerate(repeats)] + [image_uncond])\n        else:\n            x_in = torch.cat([torch.stack([x[i] for _ in range(n)]) for i, n in enumerate(repeats)] + [x] + [x])\n            sigma_in = torch.cat([torch.stack([sigma[i] for _ in range(n)]) for i, n in enumerate(repeats)] + [sigma] + [sigma])\n            image_cond_in = torch.cat([torch.stack([image_cond[i] for _ in range(n)]) for i, n in enumerate(repeats)] + [image_uncond] + [torch.zeros_like(self.init_latent)])\n\n        denoiser_params = CFGDenoiserParams(x_in, image_cond_in, sigma_in, state.sampling_step, state.sampling_steps, tensor, uncond, self)\n        cfg_denoiser_callback(denoiser_params)\n        x_in = denoiser_params.x\n        image_cond_in = denoiser_params.image_cond\n        sigma_in = denoiser_params.sigma\n        tensor = denoiser_params.text_cond\n        uncond = denoiser_params.text_uncond\n        skip_uncond = False\n\n        # alternating uncond allows for higher thresholds without the quality loss normally expected from raising it\n        if self.step % 2 and s_min_uncond > 0 and sigma[0] < s_min_uncond and not is_edit_model:\n            skip_uncond = True\n            x_in = x_in[:-batch_size]\n            sigma_in = sigma_in[:-batch_size]\n\n        self.padded_cond_uncond = False\n        self.padded_cond_uncond_v0 = False\n        if shared.opts.pad_cond_uncond_v0 and tensor.shape[1] != uncond.shape[1]:\n            tensor, uncond = self.pad_cond_uncond_v0(tensor, uncond)\n        elif shared.opts.pad_cond_uncond and tensor.shape[1] != uncond.shape[1]:\n            tensor, uncond = self.pad_cond_uncond(tensor, uncond)\n\n        if tensor.shape[1] == uncond.shape[1] or skip_uncond:\n            if is_edit_model:\n                cond_in = catenate_conds([tensor, uncond, uncond])\n            elif skip_uncond:\n                cond_in = tensor\n            else:\n                cond_in = catenate_conds([tensor, uncond])\n\n            if shared.opts.batch_cond_uncond:\n                x_out = self.inner_model(x_in, sigma_in, cond=make_condition_dict(cond_in, image_cond_in))\n            else:\n                x_out = torch.zeros_like(x_in)\n                for batch_offset in range(0, x_out.shape[0], batch_size):\n                    a = batch_offset\n                    b = a + batch_size\n                    x_out[a:b] = self.inner_model(x_in[a:b], sigma_in[a:b], cond=make_condition_dict(subscript_cond(cond_in, a, b), image_cond_in[a:b]))\n        else:\n            x_out = torch.zeros_like(x_in)\n            batch_size = batch_size*2 if shared.opts.batch_cond_uncond else batch_size\n            for batch_offset in range(0, tensor.shape[0], batch_size):\n                a = batch_offset\n                b = min(a + batch_size, tensor.shape[0])\n\n                if not is_edit_model:\n                    c_crossattn = subscript_cond(tensor, a, b)\n                else:\n                    c_crossattn = torch.cat([tensor[a:b]], uncond)\n\n                x_out[a:b] = self.inner_model(x_in[a:b], sigma_in[a:b], cond=make_condition_dict(c_crossattn, image_cond_in[a:b]))\n\n            if not skip_uncond:\n                x_out[-uncond.shape[0]:] = self.inner_model(x_in[-uncond.shape[0]:], sigma_in[-uncond.shape[0]:], cond=make_condition_dict(uncond, image_cond_in[-uncond.shape[0]:]))\n\n        denoised_image_indexes = [x[0][0] for x in conds_list]\n        if skip_uncond:\n            fake_uncond = torch.cat([x_out[i:i+1] for i in denoised_image_indexes])\n            x_out = torch.cat([x_out, fake_uncond])  # we skipped uncond denoising, so we put cond-denoised image to where the uncond-denoised image should be\n\n        denoised_params = CFGDenoisedParams(x_out, state.sampling_step, state.sampling_steps, self.inner_model)\n        cfg_denoised_callback(denoised_params)\n\n        devices.test_for_nans(x_out, \"unet\")\n\n        if is_edit_model:\n            denoised = self.combine_denoised_for_edit_model(x_out, cond_scale)\n        elif skip_uncond:\n            denoised = self.combine_denoised(x_out, conds_list, uncond, 1.0)\n        else:\n            denoised = self.combine_denoised(x_out, conds_list, uncond, cond_scale)\n\n        # Blend in the original latents (after)\n        if not self.mask_before_denoising and self.mask is not None:\n            denoised = apply_blend(denoised)\n\n        self.sampler.last_latent = self.get_pred_x0(torch.cat([x_in[i:i + 1] for i in denoised_image_indexes]), torch.cat([x_out[i:i + 1] for i in denoised_image_indexes]), sigma)\n\n        if opts.live_preview_content == \"Prompt\":\n            preview = self.sampler.last_latent\n        elif opts.live_preview_content == \"Negative prompt\":\n            preview = self.get_pred_x0(x_in[-uncond.shape[0]:], x_out[-uncond.shape[0]:], sigma)\n        else:\n            preview = self.get_pred_x0(torch.cat([x_in[i:i+1] for i in denoised_image_indexes]), torch.cat([denoised[i:i+1] for i in denoised_image_indexes]), sigma)\n\n        sd_samplers_common.store_latent(preview)\n\n        after_cfg_callback_params = AfterCFGCallbackParams(denoised, state.sampling_step, state.sampling_steps)\n        cfg_after_cfg_callback(after_cfg_callback_params)\n        denoised = after_cfg_callback_params.x\n\n        self.step += 1\n        return denoised\n\n", "modules/fifo_lock.py": "import threading\nimport collections\n\n\n# reference: https://gist.github.com/vitaliyp/6d54dd76ca2c3cdfc1149d33007dc34a\nclass FIFOLock(object):\n    def __init__(self):\n        self._lock = threading.Lock()\n        self._inner_lock = threading.Lock()\n        self._pending_threads = collections.deque()\n\n    def acquire(self, blocking=True):\n        with self._inner_lock:\n            lock_acquired = self._lock.acquire(False)\n            if lock_acquired:\n                return True\n            elif not blocking:\n                return False\n\n            release_event = threading.Event()\n            self._pending_threads.append(release_event)\n\n        release_event.wait()\n        return self._lock.acquire()\n\n    def release(self):\n        with self._inner_lock:\n            if self._pending_threads:\n                release_event = self._pending_threads.popleft()\n                release_event.set()\n\n            self._lock.release()\n\n    __enter__ = acquire\n\n    def __exit__(self, t, v, tb):\n        self.release()\n", "modules/sd_vae_approx.py": "import os\n\nimport torch\nfrom torch import nn\nfrom modules import devices, paths, shared\n\nsd_vae_approx_models = {}\n\n\nclass VAEApprox(nn.Module):\n    def __init__(self):\n        super(VAEApprox, self).__init__()\n        self.conv1 = nn.Conv2d(4, 8, (7, 7))\n        self.conv2 = nn.Conv2d(8, 16, (5, 5))\n        self.conv3 = nn.Conv2d(16, 32, (3, 3))\n        self.conv4 = nn.Conv2d(32, 64, (3, 3))\n        self.conv5 = nn.Conv2d(64, 32, (3, 3))\n        self.conv6 = nn.Conv2d(32, 16, (3, 3))\n        self.conv7 = nn.Conv2d(16, 8, (3, 3))\n        self.conv8 = nn.Conv2d(8, 3, (3, 3))\n\n    def forward(self, x):\n        extra = 11\n        x = nn.functional.interpolate(x, (x.shape[2] * 2, x.shape[3] * 2))\n        x = nn.functional.pad(x, (extra, extra, extra, extra))\n\n        for layer in [self.conv1, self.conv2, self.conv3, self.conv4, self.conv5, self.conv6, self.conv7, self.conv8, ]:\n            x = layer(x)\n            x = nn.functional.leaky_relu(x, 0.1)\n\n        return x\n\n\ndef download_model(model_path, model_url):\n    if not os.path.exists(model_path):\n        os.makedirs(os.path.dirname(model_path), exist_ok=True)\n\n        print(f'Downloading VAEApprox model to: {model_path}')\n        torch.hub.download_url_to_file(model_url, model_path)\n\n\ndef model():\n    model_name = \"vaeapprox-sdxl.pt\" if getattr(shared.sd_model, 'is_sdxl', False) else \"model.pt\"\n    loaded_model = sd_vae_approx_models.get(model_name)\n\n    if loaded_model is None:\n        model_path = os.path.join(paths.models_path, \"VAE-approx\", model_name)\n        if not os.path.exists(model_path):\n            model_path = os.path.join(paths.script_path, \"models\", \"VAE-approx\", model_name)\n\n        if not os.path.exists(model_path):\n            model_path = os.path.join(paths.models_path, \"VAE-approx\", model_name)\n            download_model(model_path, 'https://github.com/AUTOMATIC1111/stable-diffusion-webui/releases/download/v1.0.0-pre/' + model_name)\n\n        loaded_model = VAEApprox()\n        loaded_model.load_state_dict(torch.load(model_path, map_location='cpu' if devices.device.type != 'cuda' else None))\n        loaded_model.eval()\n        loaded_model.to(devices.device, devices.dtype)\n        sd_vae_approx_models[model_name] = loaded_model\n\n    return loaded_model\n\n\ndef cheap_approximation(sample):\n    # https://discuss.huggingface.co/t/decoding-latents-to-rgb-without-upscaling/23204/2\n\n    if shared.sd_model.is_sdxl:\n        coeffs = [\n            [ 0.3448,  0.4168,  0.4395],\n            [-0.1953, -0.0290,  0.0250],\n            [ 0.1074,  0.0886, -0.0163],\n            [-0.3730, -0.2499, -0.2088],\n        ]\n    else:\n        coeffs = [\n            [ 0.298,  0.207,  0.208],\n            [ 0.187,  0.286,  0.173],\n            [-0.158,  0.189,  0.264],\n            [-0.184, -0.271, -0.473],\n        ]\n\n    coefs = torch.tensor(coeffs).to(sample.device)\n\n    x_sample = torch.einsum(\"...lxy,lr -> ...rxy\", sample, coefs)\n\n    return x_sample\n", "modules/ui_tempdir.py": "import os\nimport tempfile\nfrom collections import namedtuple\nfrom pathlib import Path\n\nimport gradio.components\n\nfrom PIL import PngImagePlugin\n\nfrom modules import shared\n\n\nSavedfile = namedtuple(\"Savedfile\", [\"name\"])\n\n\ndef register_tmp_file(gradio, filename):\n    if hasattr(gradio, 'temp_file_sets'):  # gradio 3.15\n        gradio.temp_file_sets[0] = gradio.temp_file_sets[0] | {os.path.abspath(filename)}\n\n    if hasattr(gradio, 'temp_dirs'):  # gradio 3.9\n        gradio.temp_dirs = gradio.temp_dirs | {os.path.abspath(os.path.dirname(filename))}\n\n\ndef check_tmp_file(gradio, filename):\n    if hasattr(gradio, 'temp_file_sets'):\n        return any(filename in fileset for fileset in gradio.temp_file_sets)\n\n    if hasattr(gradio, 'temp_dirs'):\n        return any(Path(temp_dir).resolve() in Path(filename).resolve().parents for temp_dir in gradio.temp_dirs)\n\n    return False\n\n\ndef save_pil_to_file(self, pil_image, dir=None, format=\"png\"):\n    already_saved_as = getattr(pil_image, 'already_saved_as', None)\n    if already_saved_as and os.path.isfile(already_saved_as):\n        register_tmp_file(shared.demo, already_saved_as)\n        filename_with_mtime = f'{already_saved_as}?{os.path.getmtime(already_saved_as)}'\n        register_tmp_file(shared.demo, filename_with_mtime)\n        return filename_with_mtime\n\n    if shared.opts.temp_dir != \"\":\n        dir = shared.opts.temp_dir\n    else:\n        os.makedirs(dir, exist_ok=True)\n\n    use_metadata = False\n    metadata = PngImagePlugin.PngInfo()\n    for key, value in pil_image.info.items():\n        if isinstance(key, str) and isinstance(value, str):\n            metadata.add_text(key, value)\n            use_metadata = True\n\n    file_obj = tempfile.NamedTemporaryFile(delete=False, suffix=\".png\", dir=dir)\n    pil_image.save(file_obj, pnginfo=(metadata if use_metadata else None))\n    return file_obj.name\n\n\ndef install_ui_tempdir_override():\n    \"\"\"override save to file function so that it also writes PNG info\"\"\"\n    gradio.components.IOComponent.pil_to_temp_file = save_pil_to_file\n\n\ndef on_tmpdir_changed():\n    if shared.opts.temp_dir == \"\" or shared.demo is None:\n        return\n\n    os.makedirs(shared.opts.temp_dir, exist_ok=True)\n\n    register_tmp_file(shared.demo, os.path.join(shared.opts.temp_dir, \"x\"))\n\n\ndef cleanup_tmpdr():\n    temp_dir = shared.opts.temp_dir\n    if temp_dir == \"\" or not os.path.isdir(temp_dir):\n        return\n\n    for root, _, files in os.walk(temp_dir, topdown=False):\n        for name in files:\n            _, extension = os.path.splitext(name)\n            if extension != \".png\":\n                continue\n\n            filename = os.path.join(root, name)\n            os.remove(filename)\n\n\ndef is_gradio_temp_path(path):\n    \"\"\"\n    Check if the path is a temp dir used by gradio\n    \"\"\"\n    path = Path(path)\n    if shared.opts.temp_dir and path.is_relative_to(shared.opts.temp_dir):\n        return True\n    if gradio_temp_dir := os.environ.get(\"GRADIO_TEMP_DIR\"):\n        if path.is_relative_to(gradio_temp_dir):\n            return True\n    if path.is_relative_to(Path(tempfile.gettempdir()) / \"gradio\"):\n        return True\n    return False\n", "modules/sd_samplers_common.py": "import inspect\nfrom collections import namedtuple\nimport numpy as np\nimport torch\nfrom PIL import Image\nfrom modules import devices, images, sd_vae_approx, sd_samplers, sd_vae_taesd, shared, sd_models\nfrom modules.shared import opts, state\nimport k_diffusion.sampling\n\n\nSamplerDataTuple = namedtuple('SamplerData', ['name', 'constructor', 'aliases', 'options'])\n\n\nclass SamplerData(SamplerDataTuple):\n    def total_steps(self, steps):\n        if self.options.get(\"second_order\", False):\n            steps = steps * 2\n\n        return steps\n\n\ndef setup_img2img_steps(p, steps=None):\n    if opts.img2img_fix_steps or steps is not None:\n        requested_steps = (steps or p.steps)\n        steps = int(requested_steps / min(p.denoising_strength, 0.999)) if p.denoising_strength > 0 else 0\n        t_enc = requested_steps - 1\n    else:\n        steps = p.steps\n        t_enc = int(min(p.denoising_strength, 0.999) * steps)\n\n    return steps, t_enc\n\n\napproximation_indexes = {\"Full\": 0, \"Approx NN\": 1, \"Approx cheap\": 2, \"TAESD\": 3}\n\n\ndef samples_to_images_tensor(sample, approximation=None, model=None):\n    \"\"\"Transforms 4-channel latent space images into 3-channel RGB image tensors, with values in range [-1, 1].\"\"\"\n\n    if approximation is None or (shared.state.interrupted and opts.live_preview_fast_interrupt):\n        approximation = approximation_indexes.get(opts.show_progress_type, 0)\n\n        from modules import lowvram\n        if approximation == 0 and lowvram.is_enabled(shared.sd_model) and not shared.opts.live_preview_allow_lowvram_full:\n            approximation = 1\n\n    if approximation == 2:\n        x_sample = sd_vae_approx.cheap_approximation(sample)\n    elif approximation == 1:\n        x_sample = sd_vae_approx.model()(sample.to(devices.device, devices.dtype)).detach()\n    elif approximation == 3:\n        x_sample = sd_vae_taesd.decoder_model()(sample.to(devices.device, devices.dtype)).detach()\n        x_sample = x_sample * 2 - 1\n    else:\n        if model is None:\n            model = shared.sd_model\n        with devices.without_autocast(): # fixes an issue with unstable VAEs that are flaky even in fp32\n            x_sample = model.decode_first_stage(sample.to(model.first_stage_model.dtype))\n\n    return x_sample\n\n\ndef single_sample_to_image(sample, approximation=None):\n    x_sample = samples_to_images_tensor(sample.unsqueeze(0), approximation)[0] * 0.5 + 0.5\n\n    x_sample = torch.clamp(x_sample, min=0.0, max=1.0)\n    x_sample = 255. * np.moveaxis(x_sample.cpu().numpy(), 0, 2)\n    x_sample = x_sample.astype(np.uint8)\n\n    return Image.fromarray(x_sample)\n\n\ndef decode_first_stage(model, x):\n    x = x.to(devices.dtype_vae)\n    approx_index = approximation_indexes.get(opts.sd_vae_decode_method, 0)\n    return samples_to_images_tensor(x, approx_index, model)\n\n\ndef sample_to_image(samples, index=0, approximation=None):\n    return single_sample_to_image(samples[index], approximation)\n\n\ndef samples_to_image_grid(samples, approximation=None):\n    return images.image_grid([single_sample_to_image(sample, approximation) for sample in samples])\n\n\ndef images_tensor_to_samples(image, approximation=None, model=None):\n    '''image[0, 1] -> latent'''\n    if approximation is None:\n        approximation = approximation_indexes.get(opts.sd_vae_encode_method, 0)\n\n    if approximation == 3:\n        image = image.to(devices.device, devices.dtype)\n        x_latent = sd_vae_taesd.encoder_model()(image)\n    else:\n        if model is None:\n            model = shared.sd_model\n        model.first_stage_model.to(devices.dtype_vae)\n\n        image = image.to(shared.device, dtype=devices.dtype_vae)\n        image = image * 2 - 1\n        if len(image) > 1:\n            x_latent = torch.stack([\n                model.get_first_stage_encoding(\n                    model.encode_first_stage(torch.unsqueeze(img, 0))\n                )[0]\n                for img in image\n            ])\n        else:\n            x_latent = model.get_first_stage_encoding(model.encode_first_stage(image))\n\n    return x_latent\n\n\ndef store_latent(decoded):\n    state.current_latent = decoded\n\n    if opts.live_previews_enable and opts.show_progress_every_n_steps > 0 and shared.state.sampling_step % opts.show_progress_every_n_steps == 0:\n        if not shared.parallel_processing_allowed:\n            shared.state.assign_current_image(sample_to_image(decoded))\n\n\ndef is_sampler_using_eta_noise_seed_delta(p):\n    \"\"\"returns whether sampler from config will use eta noise seed delta for image creation\"\"\"\n\n    sampler_config = sd_samplers.find_sampler_config(p.sampler_name)\n\n    eta = p.eta\n\n    if eta is None and p.sampler is not None:\n        eta = p.sampler.eta\n\n    if eta is None and sampler_config is not None:\n        eta = 0 if sampler_config.options.get(\"default_eta_is_0\", False) else 1.0\n\n    if eta == 0:\n        return False\n\n    return sampler_config.options.get(\"uses_ensd\", False)\n\n\nclass InterruptedException(BaseException):\n    pass\n\n\ndef replace_torchsde_browinan():\n    import torchsde._brownian.brownian_interval\n\n    def torchsde_randn(size, dtype, device, seed):\n        return devices.randn_local(seed, size).to(device=device, dtype=dtype)\n\n    torchsde._brownian.brownian_interval._randn = torchsde_randn\n\n\nreplace_torchsde_browinan()\n\n\ndef apply_refiner(cfg_denoiser, sigma=None):\n    if opts.refiner_switch_by_sample_steps or sigma is None:\n        completed_ratio = cfg_denoiser.step / cfg_denoiser.total_steps\n        cfg_denoiser.p.extra_generation_params[\"Refiner switch by sampling steps\"] = True\n\n    else:\n        # torch.max(sigma) only to handle rare case where we might have different sigmas in the same batch\n        try:\n            timestep = torch.argmin(torch.abs(cfg_denoiser.inner_model.sigmas - torch.max(sigma)))\n        except AttributeError:  # for samplers that don't use sigmas (DDIM) sigma is actually the timestep\n            timestep = torch.max(sigma).to(dtype=int)\n        completed_ratio = (999 - timestep) / 1000\n\n    refiner_switch_at = cfg_denoiser.p.refiner_switch_at\n    refiner_checkpoint_info = cfg_denoiser.p.refiner_checkpoint_info\n\n    if refiner_switch_at is not None and completed_ratio < refiner_switch_at:\n        return False\n\n    if refiner_checkpoint_info is None or shared.sd_model.sd_checkpoint_info == refiner_checkpoint_info:\n        return False\n\n    if getattr(cfg_denoiser.p, \"enable_hr\", False):\n        is_second_pass = cfg_denoiser.p.is_hr_pass\n\n        if opts.hires_fix_refiner_pass == \"first pass\" and is_second_pass:\n            return False\n\n        if opts.hires_fix_refiner_pass == \"second pass\" and not is_second_pass:\n            return False\n\n        if opts.hires_fix_refiner_pass != \"second pass\":\n            cfg_denoiser.p.extra_generation_params['Hires refiner'] = opts.hires_fix_refiner_pass\n\n    cfg_denoiser.p.extra_generation_params['Refiner'] = refiner_checkpoint_info.short_title\n    cfg_denoiser.p.extra_generation_params['Refiner switch at'] = refiner_switch_at\n\n    with sd_models.SkipWritingToConfig():\n        sd_models.reload_model_weights(info=refiner_checkpoint_info)\n\n    devices.torch_gc()\n    cfg_denoiser.p.setup_conds()\n    cfg_denoiser.update_inner_model()\n\n    return True\n\n\nclass TorchHijack:\n    \"\"\"This is here to replace torch.randn_like of k-diffusion.\n\n    k-diffusion has random_sampler argument for most samplers, but not for all, so\n    this is needed to properly replace every use of torch.randn_like.\n\n    We need to replace to make images generated in batches to be same as images generated individually.\"\"\"\n\n    def __init__(self, p):\n        self.rng = p.rng\n\n    def __getattr__(self, item):\n        if item == 'randn_like':\n            return self.randn_like\n\n        if hasattr(torch, item):\n            return getattr(torch, item)\n\n        raise AttributeError(f\"'{type(self).__name__}' object has no attribute '{item}'\")\n\n    def randn_like(self, x):\n        return self.rng.next()\n\n\nclass Sampler:\n    def __init__(self, funcname):\n        self.funcname = funcname\n        self.func = funcname\n        self.extra_params = []\n        self.sampler_noises = None\n        self.stop_at = None\n        self.eta = None\n        self.config: SamplerData = None  # set by the function calling the constructor\n        self.last_latent = None\n        self.s_min_uncond = None\n        self.s_churn = 0.0\n        self.s_tmin = 0.0\n        self.s_tmax = float('inf')\n        self.s_noise = 1.0\n\n        self.eta_option_field = 'eta_ancestral'\n        self.eta_infotext_field = 'Eta'\n        self.eta_default = 1.0\n\n        self.conditioning_key = shared.sd_model.model.conditioning_key\n\n        self.p = None\n        self.model_wrap_cfg = None\n        self.sampler_extra_args = None\n        self.options = {}\n\n    def callback_state(self, d):\n        step = d['i']\n\n        if self.stop_at is not None and step > self.stop_at:\n            raise InterruptedException\n\n        state.sampling_step = step\n        shared.total_tqdm.update()\n\n    def launch_sampling(self, steps, func):\n        self.model_wrap_cfg.steps = steps\n        self.model_wrap_cfg.total_steps = self.config.total_steps(steps)\n        state.sampling_steps = steps\n        state.sampling_step = 0\n\n        try:\n            return func()\n        except RecursionError:\n            print(\n                'Encountered RecursionError during sampling, returning last latent. '\n                'rho >5 with a polyexponential scheduler may cause this error. '\n                'You should try to use a smaller rho value instead.'\n            )\n            return self.last_latent\n        except InterruptedException:\n            return self.last_latent\n\n    def number_of_needed_noises(self, p):\n        return p.steps\n\n    def initialize(self, p) -> dict:\n        self.p = p\n        self.model_wrap_cfg.p = p\n        self.model_wrap_cfg.mask = p.mask if hasattr(p, 'mask') else None\n        self.model_wrap_cfg.nmask = p.nmask if hasattr(p, 'nmask') else None\n        self.model_wrap_cfg.step = 0\n        self.model_wrap_cfg.image_cfg_scale = getattr(p, 'image_cfg_scale', None)\n        self.eta = p.eta if p.eta is not None else getattr(opts, self.eta_option_field, 0.0)\n        self.s_min_uncond = getattr(p, 's_min_uncond', 0.0)\n\n        k_diffusion.sampling.torch = TorchHijack(p)\n\n        extra_params_kwargs = {}\n        for param_name in self.extra_params:\n            if hasattr(p, param_name) and param_name in inspect.signature(self.func).parameters:\n                extra_params_kwargs[param_name] = getattr(p, param_name)\n\n        if 'eta' in inspect.signature(self.func).parameters:\n            if self.eta != self.eta_default:\n                p.extra_generation_params[self.eta_infotext_field] = self.eta\n\n            extra_params_kwargs['eta'] = self.eta\n\n        if len(self.extra_params) > 0:\n            s_churn = getattr(opts, 's_churn', p.s_churn)\n            s_tmin = getattr(opts, 's_tmin', p.s_tmin)\n            s_tmax = getattr(opts, 's_tmax', p.s_tmax) or self.s_tmax # 0 = inf\n            s_noise = getattr(opts, 's_noise', p.s_noise)\n\n            if 's_churn' in extra_params_kwargs and s_churn != self.s_churn:\n                extra_params_kwargs['s_churn'] = s_churn\n                p.s_churn = s_churn\n                p.extra_generation_params['Sigma churn'] = s_churn\n            if 's_tmin' in extra_params_kwargs and s_tmin != self.s_tmin:\n                extra_params_kwargs['s_tmin'] = s_tmin\n                p.s_tmin = s_tmin\n                p.extra_generation_params['Sigma tmin'] = s_tmin\n            if 's_tmax' in extra_params_kwargs and s_tmax != self.s_tmax:\n                extra_params_kwargs['s_tmax'] = s_tmax\n                p.s_tmax = s_tmax\n                p.extra_generation_params['Sigma tmax'] = s_tmax\n            if 's_noise' in extra_params_kwargs and s_noise != self.s_noise:\n                extra_params_kwargs['s_noise'] = s_noise\n                p.s_noise = s_noise\n                p.extra_generation_params['Sigma noise'] = s_noise\n\n        return extra_params_kwargs\n\n    def create_noise_sampler(self, x, sigmas, p):\n        \"\"\"For DPM++ SDE: manually create noise sampler to enable deterministic results across different batch sizes\"\"\"\n        if shared.opts.no_dpmpp_sde_batch_determinism:\n            return None\n\n        from k_diffusion.sampling import BrownianTreeNoiseSampler\n        sigma_min, sigma_max = sigmas[sigmas > 0].min(), sigmas.max()\n        current_iter_seeds = p.all_seeds[p.iteration * p.batch_size:(p.iteration + 1) * p.batch_size]\n        return BrownianTreeNoiseSampler(x, sigma_min, sigma_max, seed=current_iter_seeds)\n\n    def sample(self, p, x, conditioning, unconditional_conditioning, steps=None, image_conditioning=None):\n        raise NotImplementedError()\n\n    def sample_img2img(self, p, x, noise, conditioning, unconditional_conditioning, steps=None, image_conditioning=None):\n        raise NotImplementedError()\n\n    def add_infotext(self, p):\n        if self.model_wrap_cfg.padded_cond_uncond:\n            p.extra_generation_params[\"Pad conds\"] = True\n\n        if self.model_wrap_cfg.padded_cond_uncond_v0:\n            p.extra_generation_params[\"Pad conds v0\"] = True\n", "modules/ui_components.py": "import gradio as gr\n\n\nclass FormComponent:\n    def get_expected_parent(self):\n        return gr.components.Form\n\n\ngr.Dropdown.get_expected_parent = FormComponent.get_expected_parent\n\n\nclass ToolButton(FormComponent, gr.Button):\n    \"\"\"Small button with single emoji as text, fits inside gradio forms\"\"\"\n\n    def __init__(self, *args, **kwargs):\n        classes = kwargs.pop(\"elem_classes\", [])\n        super().__init__(*args, elem_classes=[\"tool\", *classes], **kwargs)\n\n    def get_block_name(self):\n        return \"button\"\n\n\nclass ResizeHandleRow(gr.Row):\n    \"\"\"Same as gr.Row but fits inside gradio forms\"\"\"\n\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n\n        self.elem_classes.append(\"resize-handle-row\")\n\n    def get_block_name(self):\n        return \"row\"\n\n\nclass FormRow(FormComponent, gr.Row):\n    \"\"\"Same as gr.Row but fits inside gradio forms\"\"\"\n\n    def get_block_name(self):\n        return \"row\"\n\n\nclass FormColumn(FormComponent, gr.Column):\n    \"\"\"Same as gr.Column but fits inside gradio forms\"\"\"\n\n    def get_block_name(self):\n        return \"column\"\n\n\nclass FormGroup(FormComponent, gr.Group):\n    \"\"\"Same as gr.Group but fits inside gradio forms\"\"\"\n\n    def get_block_name(self):\n        return \"group\"\n\n\nclass FormHTML(FormComponent, gr.HTML):\n    \"\"\"Same as gr.HTML but fits inside gradio forms\"\"\"\n\n    def get_block_name(self):\n        return \"html\"\n\n\nclass FormColorPicker(FormComponent, gr.ColorPicker):\n    \"\"\"Same as gr.ColorPicker but fits inside gradio forms\"\"\"\n\n    def get_block_name(self):\n        return \"colorpicker\"\n\n\nclass DropdownMulti(FormComponent, gr.Dropdown):\n    \"\"\"Same as gr.Dropdown but always multiselect\"\"\"\n    def __init__(self, **kwargs):\n        super().__init__(multiselect=True, **kwargs)\n\n    def get_block_name(self):\n        return \"dropdown\"\n\n\nclass DropdownEditable(FormComponent, gr.Dropdown):\n    \"\"\"Same as gr.Dropdown but allows editing value\"\"\"\n    def __init__(self, **kwargs):\n        super().__init__(allow_custom_value=True, **kwargs)\n\n    def get_block_name(self):\n        return \"dropdown\"\n\n\nclass InputAccordion(gr.Checkbox):\n    \"\"\"A gr.Accordion that can be used as an input - returns True if open, False if closed.\n\n    Actually just a hidden checkbox, but creates an accordion that follows and is followed by the state of the checkbox.\n    \"\"\"\n\n    global_index = 0\n\n    def __init__(self, value, **kwargs):\n        self.accordion_id = kwargs.get('elem_id')\n        if self.accordion_id is None:\n            self.accordion_id = f\"input-accordion-{InputAccordion.global_index}\"\n            InputAccordion.global_index += 1\n\n        kwargs_checkbox = {\n            **kwargs,\n            \"elem_id\": f\"{self.accordion_id}-checkbox\",\n            \"visible\": False,\n        }\n        super().__init__(value, **kwargs_checkbox)\n\n        self.change(fn=None, _js='function(checked){ inputAccordionChecked(\"' + self.accordion_id + '\", checked); }', inputs=[self])\n\n        kwargs_accordion = {\n            **kwargs,\n            \"elem_id\": self.accordion_id,\n            \"label\": kwargs.get('label', 'Accordion'),\n            \"elem_classes\": ['input-accordion'],\n            \"open\": value,\n        }\n        self.accordion = gr.Accordion(**kwargs_accordion)\n\n    def extra(self):\n        \"\"\"Allows you to put something into the label of the accordion.\n\n        Use it like this:\n\n        ```\n        with InputAccordion(False, label=\"Accordion\") as acc:\n            with acc.extra():\n                FormHTML(value=\"hello\", min_width=0)\n\n            ...\n        ```\n        \"\"\"\n\n        return gr.Column(elem_id=self.accordion_id + '-extra', elem_classes='input-accordion-extra', min_width=0)\n\n    def __enter__(self):\n        self.accordion.__enter__()\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.accordion.__exit__(exc_type, exc_val, exc_tb)\n\n    def get_block_name(self):\n        return \"checkbox\"\n\n", "modules/gitpython_hack.py": "from __future__ import annotations\n\nimport io\nimport subprocess\n\nimport git\n\n\nclass Git(git.Git):\n    \"\"\"\n    Git subclassed to never use persistent processes.\n    \"\"\"\n\n    def _get_persistent_cmd(self, attr_name, cmd_name, *args, **kwargs):\n        raise NotImplementedError(f\"Refusing to use persistent process: {attr_name} ({cmd_name} {args} {kwargs})\")\n\n    def get_object_header(self, ref: str | bytes) -> tuple[str, str, int]:\n        ret = subprocess.check_output(\n            [self.GIT_PYTHON_GIT_EXECUTABLE, \"cat-file\", \"--batch-check\"],\n            input=self._prepare_ref(ref),\n            cwd=self._working_dir,\n            timeout=2,\n        )\n        return self._parse_object_header(ret)\n\n    def stream_object_data(self, ref: str) -> tuple[str, str, int, Git.CatFileContentStream]:\n        # Not really streaming, per se; this buffers the entire object in memory.\n        # Shouldn't be a problem for our use case, since we're only using this for\n        # object headers (commit objects).\n        ret = subprocess.check_output(\n            [self.GIT_PYTHON_GIT_EXECUTABLE, \"cat-file\", \"--batch\"],\n            input=self._prepare_ref(ref),\n            cwd=self._working_dir,\n            timeout=30,\n        )\n        bio = io.BytesIO(ret)\n        hexsha, typename, size = self._parse_object_header(bio.readline())\n        return (hexsha, typename, size, self.CatFileContentStream(size, bio))\n\n\nclass Repo(git.Repo):\n    GitCommandWrapperType = Git\n", "modules/face_restoration_utils.py": "from __future__ import annotations\n\nimport logging\nimport os\nfrom functools import cached_property\nfrom typing import TYPE_CHECKING, Callable\n\nimport cv2\nimport numpy as np\nimport torch\n\nfrom modules import devices, errors, face_restoration, shared\n\nif TYPE_CHECKING:\n    from facexlib.utils.face_restoration_helper import FaceRestoreHelper\n\nlogger = logging.getLogger(__name__)\n\n\ndef bgr_image_to_rgb_tensor(img: np.ndarray) -> torch.Tensor:\n    \"\"\"Convert a BGR NumPy image in [0..1] range to a PyTorch RGB float32 tensor.\"\"\"\n    assert img.shape[2] == 3, \"image must be RGB\"\n    if img.dtype == \"float64\":\n        img = img.astype(\"float32\")\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    return torch.from_numpy(img.transpose(2, 0, 1)).float()\n\n\ndef rgb_tensor_to_bgr_image(tensor: torch.Tensor, *, min_max=(0.0, 1.0)) -> np.ndarray:\n    \"\"\"\n    Convert a PyTorch RGB tensor in range `min_max` to a BGR NumPy image in [0..1] range.\n    \"\"\"\n    tensor = tensor.squeeze(0).float().detach().cpu().clamp_(*min_max)\n    tensor = (tensor - min_max[0]) / (min_max[1] - min_max[0])\n    assert tensor.dim() == 3, \"tensor must be RGB\"\n    img_np = tensor.numpy().transpose(1, 2, 0)\n    if img_np.shape[2] == 1:  # gray image, no RGB/BGR required\n        return np.squeeze(img_np, axis=2)\n    return cv2.cvtColor(img_np, cv2.COLOR_BGR2RGB)\n\n\ndef create_face_helper(device) -> FaceRestoreHelper:\n    from facexlib.detection import retinaface\n    from facexlib.utils.face_restoration_helper import FaceRestoreHelper\n    if hasattr(retinaface, 'device'):\n        retinaface.device = device\n    return FaceRestoreHelper(\n        upscale_factor=1,\n        face_size=512,\n        crop_ratio=(1, 1),\n        det_model='retinaface_resnet50',\n        save_ext='png',\n        use_parse=True,\n        device=device,\n    )\n\n\ndef restore_with_face_helper(\n    np_image: np.ndarray,\n    face_helper: FaceRestoreHelper,\n    restore_face: Callable[[torch.Tensor], torch.Tensor],\n) -> np.ndarray:\n    \"\"\"\n    Find faces in the image using face_helper, restore them using restore_face, and paste them back into the image.\n\n    `restore_face` should take a cropped face image and return a restored face image.\n    \"\"\"\n    from torchvision.transforms.functional import normalize\n    np_image = np_image[:, :, ::-1]\n    original_resolution = np_image.shape[0:2]\n\n    try:\n        logger.debug(\"Detecting faces...\")\n        face_helper.clean_all()\n        face_helper.read_image(np_image)\n        face_helper.get_face_landmarks_5(only_center_face=False, resize=640, eye_dist_threshold=5)\n        face_helper.align_warp_face()\n        logger.debug(\"Found %d faces, restoring\", len(face_helper.cropped_faces))\n        for cropped_face in face_helper.cropped_faces:\n            cropped_face_t = bgr_image_to_rgb_tensor(cropped_face / 255.0)\n            normalize(cropped_face_t, (0.5, 0.5, 0.5), (0.5, 0.5, 0.5), inplace=True)\n            cropped_face_t = cropped_face_t.unsqueeze(0).to(devices.device_codeformer)\n\n            try:\n                with torch.no_grad():\n                    cropped_face_t = restore_face(cropped_face_t)\n                devices.torch_gc()\n            except Exception:\n                errors.report('Failed face-restoration inference', exc_info=True)\n\n            restored_face = rgb_tensor_to_bgr_image(cropped_face_t, min_max=(-1, 1))\n            restored_face = (restored_face * 255.0).astype('uint8')\n            face_helper.add_restored_face(restored_face)\n\n        logger.debug(\"Merging restored faces into image\")\n        face_helper.get_inverse_affine(None)\n        img = face_helper.paste_faces_to_input_image()\n        img = img[:, :, ::-1]\n        if original_resolution != img.shape[0:2]:\n            img = cv2.resize(\n                img,\n                (0, 0),\n                fx=original_resolution[1] / img.shape[1],\n                fy=original_resolution[0] / img.shape[0],\n                interpolation=cv2.INTER_LINEAR,\n            )\n        logger.debug(\"Face restoration complete\")\n    finally:\n        face_helper.clean_all()\n    return img\n\n\nclass CommonFaceRestoration(face_restoration.FaceRestoration):\n    net: torch.Module | None\n    model_url: str\n    model_download_name: str\n\n    def __init__(self, model_path: str):\n        super().__init__()\n        self.net = None\n        self.model_path = model_path\n        os.makedirs(model_path, exist_ok=True)\n\n    @cached_property\n    def face_helper(self) -> FaceRestoreHelper:\n        return create_face_helper(self.get_device())\n\n    def send_model_to(self, device):\n        if self.net:\n            logger.debug(\"Sending %s to %s\", self.net, device)\n            self.net.to(device)\n        if self.face_helper:\n            logger.debug(\"Sending face helper to %s\", device)\n            self.face_helper.face_det.to(device)\n            self.face_helper.face_parse.to(device)\n\n    def get_device(self):\n        raise NotImplementedError(\"get_device must be implemented by subclasses\")\n\n    def load_net(self) -> torch.Module:\n        raise NotImplementedError(\"load_net must be implemented by subclasses\")\n\n    def restore_with_helper(\n        self,\n        np_image: np.ndarray,\n        restore_face: Callable[[torch.Tensor], torch.Tensor],\n    ) -> np.ndarray:\n        try:\n            if self.net is None:\n                self.net = self.load_net()\n        except Exception:\n            logger.warning(\"Unable to load face-restoration model\", exc_info=True)\n            return np_image\n\n        try:\n            self.send_model_to(self.get_device())\n            return restore_with_face_helper(np_image, self.face_helper, restore_face)\n        finally:\n            if shared.opts.face_restoration_unload:\n                self.send_model_to(devices.cpu)\n\n\ndef patch_facexlib(dirname: str) -> None:\n    import facexlib.detection\n    import facexlib.parsing\n\n    det_facex_load_file_from_url = facexlib.detection.load_file_from_url\n    par_facex_load_file_from_url = facexlib.parsing.load_file_from_url\n\n    def update_kwargs(kwargs):\n        return dict(kwargs, save_dir=dirname, model_dir=None)\n\n    def facex_load_file_from_url(**kwargs):\n        return det_facex_load_file_from_url(**update_kwargs(kwargs))\n\n    def facex_load_file_from_url2(**kwargs):\n        return par_facex_load_file_from_url(**update_kwargs(kwargs))\n\n    facexlib.detection.load_file_from_url = facex_load_file_from_url\n    facexlib.parsing.load_file_from_url = facex_load_file_from_url2\n", "modules/realesrgan_model.py": "import os\n\nfrom modules import modelloader, errors\nfrom modules.shared import cmd_opts, opts\nfrom modules.upscaler import Upscaler, UpscalerData\nfrom modules.upscaler_utils import upscale_with_model\n\n\nclass UpscalerRealESRGAN(Upscaler):\n    def __init__(self, path):\n        self.name = \"RealESRGAN\"\n        self.user_path = path\n        super().__init__()\n        self.enable = True\n        self.scalers = []\n        scalers = get_realesrgan_models(self)\n\n        local_model_paths = self.find_models(ext_filter=[\".pth\"])\n        for scaler in scalers:\n            if scaler.local_data_path.startswith(\"http\"):\n                filename = modelloader.friendly_name(scaler.local_data_path)\n                local_model_candidates = [local_model for local_model in local_model_paths if local_model.endswith(f\"{filename}.pth\")]\n                if local_model_candidates:\n                    scaler.local_data_path = local_model_candidates[0]\n\n            if scaler.name in opts.realesrgan_enabled_models:\n                self.scalers.append(scaler)\n\n    def do_upscale(self, img, path):\n        if not self.enable:\n            return img\n\n        try:\n            info = self.load_model(path)\n        except Exception:\n            errors.report(f\"Unable to load RealESRGAN model {path}\", exc_info=True)\n            return img\n\n        model_descriptor = modelloader.load_spandrel_model(\n            info.local_data_path,\n            device=self.device,\n            prefer_half=(not cmd_opts.no_half and not cmd_opts.upcast_sampling),\n            expected_architecture=\"ESRGAN\",  # \"RealESRGAN\" isn't a specific thing for Spandrel\n        )\n        return upscale_with_model(\n            model_descriptor,\n            img,\n            tile_size=opts.ESRGAN_tile,\n            tile_overlap=opts.ESRGAN_tile_overlap,\n            # TODO: `outscale`?\n        )\n\n    def load_model(self, path):\n        for scaler in self.scalers:\n            if scaler.data_path == path:\n                if scaler.local_data_path.startswith(\"http\"):\n                    scaler.local_data_path = modelloader.load_file_from_url(\n                        scaler.data_path,\n                        model_dir=self.model_download_path,\n                    )\n                if not os.path.exists(scaler.local_data_path):\n                    raise FileNotFoundError(f\"RealESRGAN data missing: {scaler.local_data_path}\")\n                return scaler\n        raise ValueError(f\"Unable to find model info: {path}\")\n\n\ndef get_realesrgan_models(scaler: UpscalerRealESRGAN):\n    return [\n        UpscalerData(\n            name=\"R-ESRGAN General 4xV3\",\n            path=\"https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.5.0/realesr-general-x4v3.pth\",\n            scale=4,\n            upscaler=scaler,\n        ),\n        UpscalerData(\n            name=\"R-ESRGAN General WDN 4xV3\",\n            path=\"https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.5.0/realesr-general-wdn-x4v3.pth\",\n            scale=4,\n            upscaler=scaler,\n        ),\n        UpscalerData(\n            name=\"R-ESRGAN AnimeVideo\",\n            path=\"https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.5.0/realesr-animevideov3.pth\",\n            scale=4,\n            upscaler=scaler,\n        ),\n        UpscalerData(\n            name=\"R-ESRGAN 4x+\",\n            path=\"https://github.com/xinntao/Real-ESRGAN/releases/download/v0.1.0/RealESRGAN_x4plus.pth\",\n            scale=4,\n            upscaler=scaler,\n        ),\n        UpscalerData(\n            name=\"R-ESRGAN 4x+ Anime6B\",\n            path=\"https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.2.4/RealESRGAN_x4plus_anime_6B.pth\",\n            scale=4,\n            upscaler=scaler,\n        ),\n        UpscalerData(\n            name=\"R-ESRGAN 2x+\",\n            path=\"https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.1/RealESRGAN_x2plus.pth\",\n            scale=2,\n            upscaler=scaler,\n        ),\n    ]\n", "modules/infotext_versions.py": "from modules import shared\nfrom packaging import version\nimport re\n\n\nv160 = version.parse(\"1.6.0\")\nv170_tsnr = version.parse(\"v1.7.0-225\")\nv180 = version.parse(\"1.8.0\")\nv180_hr_styles = version.parse(\"1.8.0-139\")\n\n\ndef parse_version(text):\n    if text is None:\n        return None\n\n    m = re.match(r'([^-]+-[^-]+)-.*', text)\n    if m:\n        text = m.group(1)\n\n    try:\n        return version.parse(text)\n    except Exception:\n        return None\n\n\ndef backcompat(d):\n    \"\"\"Checks infotext Version field, and enables backwards compatibility options according to it.\"\"\"\n\n    if not shared.opts.auto_backcompat:\n        return\n\n    ver = parse_version(d.get(\"Version\"))\n    if ver is None:\n        return\n\n    if ver < v160 and '[' in d.get('Prompt', ''):\n        d[\"Old prompt editing timelines\"] = True\n\n    if ver < v160 and d.get('Sampler', '') in ('DDIM', 'PLMS'):\n        d[\"Pad conds v0\"] = True\n\n    if ver < v170_tsnr:\n        d[\"Downcast alphas_cumprod\"] = True\n\n    if ver < v180 and d.get('Refiner'):\n        d[\"Refiner switch by sampling steps\"] = True\n", "modules/devices.py": "import sys\nimport contextlib\nfrom functools import lru_cache\n\nimport torch\nfrom modules import errors, shared, npu_specific\n\nif sys.platform == \"darwin\":\n    from modules import mac_specific\n\nif shared.cmd_opts.use_ipex:\n    from modules import xpu_specific\n\n\ndef has_xpu() -> bool:\n    return shared.cmd_opts.use_ipex and xpu_specific.has_xpu\n\n\ndef has_mps() -> bool:\n    if sys.platform != \"darwin\":\n        return False\n    else:\n        return mac_specific.has_mps\n\n\ndef cuda_no_autocast(device_id=None) -> bool:\n    if device_id is None:\n        device_id = get_cuda_device_id()\n    return (\n        torch.cuda.get_device_capability(device_id) == (7, 5)\n        and torch.cuda.get_device_name(device_id).startswith(\"NVIDIA GeForce GTX 16\")\n    )\n\n\ndef get_cuda_device_id():\n    return (\n        int(shared.cmd_opts.device_id)\n        if shared.cmd_opts.device_id is not None and shared.cmd_opts.device_id.isdigit()\n        else 0\n    ) or torch.cuda.current_device()\n\n\ndef get_cuda_device_string():\n    if shared.cmd_opts.device_id is not None:\n        return f\"cuda:{shared.cmd_opts.device_id}\"\n\n    return \"cuda\"\n\n\ndef get_optimal_device_name():\n    if torch.cuda.is_available():\n        return get_cuda_device_string()\n\n    if has_mps():\n        return \"mps\"\n\n    if has_xpu():\n        return xpu_specific.get_xpu_device_string()\n\n    if npu_specific.has_npu:\n        return npu_specific.get_npu_device_string()\n\n    return \"cpu\"\n\n\ndef get_optimal_device():\n    return torch.device(get_optimal_device_name())\n\n\ndef get_device_for(task):\n    if task in shared.cmd_opts.use_cpu or \"all\" in shared.cmd_opts.use_cpu:\n        return cpu\n\n    return get_optimal_device()\n\n\ndef torch_gc():\n\n    if torch.cuda.is_available():\n        with torch.cuda.device(get_cuda_device_string()):\n            torch.cuda.empty_cache()\n            torch.cuda.ipc_collect()\n\n    if has_mps():\n        mac_specific.torch_mps_gc()\n\n    if has_xpu():\n        xpu_specific.torch_xpu_gc()\n\n    if npu_specific.has_npu:\n        torch_npu_set_device()\n        npu_specific.torch_npu_gc()\n\n\ndef torch_npu_set_device():\n    # Work around due to bug in torch_npu, revert me after fixed, @see https://gitee.com/ascend/pytorch/issues/I8KECW?from=project-issue\n    if npu_specific.has_npu:\n        torch.npu.set_device(0)\n\n\ndef enable_tf32():\n    if torch.cuda.is_available():\n\n        # enabling benchmark option seems to enable a range of cards to do fp16 when they otherwise can't\n        # see https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/4407\n        if cuda_no_autocast():\n            torch.backends.cudnn.benchmark = True\n\n        torch.backends.cuda.matmul.allow_tf32 = True\n        torch.backends.cudnn.allow_tf32 = True\n\n\nerrors.run(enable_tf32, \"Enabling TF32\")\n\ncpu: torch.device = torch.device(\"cpu\")\nfp8: bool = False\ndevice: torch.device = None\ndevice_interrogate: torch.device = None\ndevice_gfpgan: torch.device = None\ndevice_esrgan: torch.device = None\ndevice_codeformer: torch.device = None\ndtype: torch.dtype = torch.float16\ndtype_vae: torch.dtype = torch.float16\ndtype_unet: torch.dtype = torch.float16\ndtype_inference: torch.dtype = torch.float16\nunet_needs_upcast = False\n\n\ndef cond_cast_unet(input):\n    return input.to(dtype_unet) if unet_needs_upcast else input\n\n\ndef cond_cast_float(input):\n    return input.float() if unet_needs_upcast else input\n\n\nnv_rng = None\npatch_module_list = [\n    torch.nn.Linear,\n    torch.nn.Conv2d,\n    torch.nn.MultiheadAttention,\n    torch.nn.GroupNorm,\n    torch.nn.LayerNorm,\n]\n\n\ndef manual_cast_forward(target_dtype):\n    def forward_wrapper(self, *args, **kwargs):\n        if any(\n            isinstance(arg, torch.Tensor) and arg.dtype != target_dtype\n            for arg in args\n        ):\n            args = [arg.to(target_dtype) if isinstance(arg, torch.Tensor) else arg for arg in args]\n            kwargs = {k: v.to(target_dtype) if isinstance(v, torch.Tensor) else v for k, v in kwargs.items()}\n\n        org_dtype = target_dtype\n        for param in self.parameters():\n            if param.dtype != target_dtype:\n                org_dtype = param.dtype\n                break\n\n        if org_dtype != target_dtype:\n            self.to(target_dtype)\n        result = self.org_forward(*args, **kwargs)\n        if org_dtype != target_dtype:\n            self.to(org_dtype)\n\n        if target_dtype != dtype_inference:\n            if isinstance(result, tuple):\n                result = tuple(\n                    i.to(dtype_inference)\n                    if isinstance(i, torch.Tensor)\n                    else i\n                    for i in result\n                )\n            elif isinstance(result, torch.Tensor):\n                result = result.to(dtype_inference)\n        return result\n    return forward_wrapper\n\n\n@contextlib.contextmanager\ndef manual_cast(target_dtype):\n    applied = False\n    for module_type in patch_module_list:\n        if hasattr(module_type, \"org_forward\"):\n            continue\n        applied = True\n        org_forward = module_type.forward\n        if module_type == torch.nn.MultiheadAttention:\n            module_type.forward = manual_cast_forward(torch.float32)\n        else:\n            module_type.forward = manual_cast_forward(target_dtype)\n        module_type.org_forward = org_forward\n    try:\n        yield None\n    finally:\n        if applied:\n            for module_type in patch_module_list:\n                if hasattr(module_type, \"org_forward\"):\n                    module_type.forward = module_type.org_forward\n                    delattr(module_type, \"org_forward\")\n\n\ndef autocast(disable=False):\n    if disable:\n        return contextlib.nullcontext()\n\n    if fp8 and device==cpu:\n        return torch.autocast(\"cpu\", dtype=torch.bfloat16, enabled=True)\n\n    if fp8 and dtype_inference == torch.float32:\n        return manual_cast(dtype)\n\n    if dtype == torch.float32 or dtype_inference == torch.float32:\n        return contextlib.nullcontext()\n\n    if has_xpu() or has_mps() or cuda_no_autocast():\n        return manual_cast(dtype)\n\n    return torch.autocast(\"cuda\")\n\n\ndef without_autocast(disable=False):\n    return torch.autocast(\"cuda\", enabled=False) if torch.is_autocast_enabled() and not disable else contextlib.nullcontext()\n\n\nclass NansException(Exception):\n    pass\n\n\ndef test_for_nans(x, where):\n    if shared.cmd_opts.disable_nan_check:\n        return\n\n    if not torch.all(torch.isnan(x)).item():\n        return\n\n    if where == \"unet\":\n        message = \"A tensor with all NaNs was produced in Unet.\"\n\n        if not shared.cmd_opts.no_half:\n            message += \" This could be either because there's not enough precision to represent the picture, or because your video card does not support half type. Try setting the \\\"Upcast cross attention layer to float32\\\" option in Settings > Stable Diffusion or using the --no-half commandline argument to fix this.\"\n\n    elif where == \"vae\":\n        message = \"A tensor with all NaNs was produced in VAE.\"\n\n        if not shared.cmd_opts.no_half and not shared.cmd_opts.no_half_vae:\n            message += \" This could be because there's not enough precision to represent the picture. Try adding --no-half-vae commandline argument to fix this.\"\n    else:\n        message = \"A tensor with all NaNs was produced.\"\n\n    message += \" Use --disable-nan-check commandline argument to disable this check.\"\n\n    raise NansException(message)\n\n\n@lru_cache\ndef first_time_calculation():\n    \"\"\"\n    just do any calculation with pytorch layers - the first time this is done it allocaltes about 700MB of memory and\n    spends about 2.7 seconds doing that, at least with NVidia.\n    \"\"\"\n\n    x = torch.zeros((1, 1)).to(device, dtype)\n    linear = torch.nn.Linear(1, 1).to(device, dtype)\n    linear(x)\n\n    x = torch.zeros((1, 1, 3, 3)).to(device, dtype)\n    conv2d = torch.nn.Conv2d(1, 1, (3, 3)).to(device, dtype)\n    conv2d(x)\n", "modules/initialize.py": "import importlib\nimport logging\nimport os\nimport sys\nimport warnings\nfrom threading import Thread\n\nfrom modules.timer import startup_timer\n\n\ndef imports():\n    logging.getLogger(\"torch.distributed.nn\").setLevel(logging.ERROR)  # sshh...\n    logging.getLogger(\"xformers\").addFilter(lambda record: 'A matching Triton is not available' not in record.getMessage())\n\n    import torch  # noqa: F401\n    startup_timer.record(\"import torch\")\n    import pytorch_lightning  # noqa: F401\n    startup_timer.record(\"import torch\")\n    warnings.filterwarnings(action=\"ignore\", category=DeprecationWarning, module=\"pytorch_lightning\")\n    warnings.filterwarnings(action=\"ignore\", category=UserWarning, module=\"torchvision\")\n\n    os.environ.setdefault('GRADIO_ANALYTICS_ENABLED', 'False')\n    import gradio  # noqa: F401\n    startup_timer.record(\"import gradio\")\n\n    from modules import paths, timer, import_hook, errors  # noqa: F401\n    startup_timer.record(\"setup paths\")\n\n    import ldm.modules.encoders.modules  # noqa: F401\n    startup_timer.record(\"import ldm\")\n\n    import sgm.modules.encoders.modules  # noqa: F401\n    startup_timer.record(\"import sgm\")\n\n    from modules import shared_init\n    shared_init.initialize()\n    startup_timer.record(\"initialize shared\")\n\n    from modules import processing, gradio_extensons, ui  # noqa: F401\n    startup_timer.record(\"other imports\")\n\n\ndef check_versions():\n    from modules.shared_cmd_options import cmd_opts\n\n    if not cmd_opts.skip_version_check:\n        from modules import errors\n        errors.check_versions()\n\n\ndef initialize():\n    from modules import initialize_util\n    initialize_util.fix_torch_version()\n    initialize_util.fix_pytorch_lightning()\n    initialize_util.fix_asyncio_event_loop_policy()\n    initialize_util.validate_tls_options()\n    initialize_util.configure_sigint_handler()\n    initialize_util.configure_opts_onchange()\n\n    from modules import sd_models\n    sd_models.setup_model()\n    startup_timer.record(\"setup SD model\")\n\n    from modules.shared_cmd_options import cmd_opts\n\n    from modules import codeformer_model\n    warnings.filterwarnings(action=\"ignore\", category=UserWarning, module=\"torchvision.transforms.functional_tensor\")\n    codeformer_model.setup_model(cmd_opts.codeformer_models_path)\n    startup_timer.record(\"setup codeformer\")\n\n    from modules import gfpgan_model\n    gfpgan_model.setup_model(cmd_opts.gfpgan_models_path)\n    startup_timer.record(\"setup gfpgan\")\n\n    initialize_rest(reload_script_modules=False)\n\n\ndef initialize_rest(*, reload_script_modules=False):\n    \"\"\"\n    Called both from initialize() and when reloading the webui.\n    \"\"\"\n    from modules.shared_cmd_options import cmd_opts\n\n    from modules import sd_samplers\n    sd_samplers.set_samplers()\n    startup_timer.record(\"set samplers\")\n\n    from modules import extensions\n    extensions.list_extensions()\n    startup_timer.record(\"list extensions\")\n\n    from modules import initialize_util\n    initialize_util.restore_config_state_file()\n    startup_timer.record(\"restore config state file\")\n\n    from modules import shared, upscaler, scripts\n    if cmd_opts.ui_debug_mode:\n        shared.sd_upscalers = upscaler.UpscalerLanczos().scalers\n        scripts.load_scripts()\n        return\n\n    from modules import sd_models\n    sd_models.list_models()\n    startup_timer.record(\"list SD models\")\n\n    from modules import localization\n    localization.list_localizations(cmd_opts.localizations_dir)\n    startup_timer.record(\"list localizations\")\n\n    with startup_timer.subcategory(\"load scripts\"):\n        scripts.load_scripts()\n\n    if reload_script_modules and shared.opts.enable_reloading_ui_scripts:\n        for module in [module for name, module in sys.modules.items() if name.startswith(\"modules.ui\")]:\n            importlib.reload(module)\n        startup_timer.record(\"reload script modules\")\n\n    from modules import modelloader\n    modelloader.load_upscalers()\n    startup_timer.record(\"load upscalers\")\n\n    from modules import sd_vae\n    sd_vae.refresh_vae_list()\n    startup_timer.record(\"refresh VAE\")\n\n    from modules import textual_inversion\n    textual_inversion.textual_inversion.list_textual_inversion_templates()\n    startup_timer.record(\"refresh textual inversion templates\")\n\n    from modules import script_callbacks, sd_hijack_optimizations, sd_hijack\n    script_callbacks.on_list_optimizers(sd_hijack_optimizations.list_optimizers)\n    sd_hijack.list_optimizers()\n    startup_timer.record(\"scripts list_optimizers\")\n\n    from modules import sd_unet\n    sd_unet.list_unets()\n    startup_timer.record(\"scripts list_unets\")\n\n    def load_model():\n        \"\"\"\n        Accesses shared.sd_model property to load model.\n        After it's available, if it has been loaded before this access by some extension,\n        its optimization may be None because the list of optimizers has not been filled\n        by that time, so we apply optimization again.\n        \"\"\"\n        from modules import devices\n        devices.torch_npu_set_device()\n\n        shared.sd_model  # noqa: B018\n\n        if sd_hijack.current_optimizer is None:\n            sd_hijack.apply_optimizations()\n\n        devices.first_time_calculation()\n    if not shared.cmd_opts.skip_load_model_at_start:\n        Thread(target=load_model).start()\n\n    from modules import shared_items\n    shared_items.reload_hypernetworks()\n    startup_timer.record(\"reload hypernetworks\")\n\n    from modules import ui_extra_networks\n    ui_extra_networks.initialize()\n    ui_extra_networks.register_default_pages()\n\n    from modules import extra_networks\n    extra_networks.initialize()\n    extra_networks.register_default_extra_networks()\n    startup_timer.record(\"initialize extra networks\")\n", "modules/scripts_auto_postprocessing.py": "from modules import scripts, scripts_postprocessing, shared\n\n\nclass ScriptPostprocessingForMainUI(scripts.Script):\n    def __init__(self, script_postproc):\n        self.script: scripts_postprocessing.ScriptPostprocessing = script_postproc\n        self.postprocessing_controls = None\n\n    def title(self):\n        return self.script.name\n\n    def show(self, is_img2img):\n        return scripts.AlwaysVisible\n\n    def ui(self, is_img2img):\n        self.postprocessing_controls = self.script.ui()\n        return self.postprocessing_controls.values()\n\n    def postprocess_image(self, p, script_pp, *args):\n        args_dict = dict(zip(self.postprocessing_controls, args))\n\n        pp = scripts_postprocessing.PostprocessedImage(script_pp.image)\n        pp.info = {}\n        self.script.process(pp, **args_dict)\n        p.extra_generation_params.update(pp.info)\n        script_pp.image = pp.image\n\n\ndef create_auto_preprocessing_script_data():\n    from modules import scripts\n\n    res = []\n\n    for name in shared.opts.postprocessing_enable_in_main_ui:\n        script = next(iter([x for x in scripts.postprocessing_scripts_data if x.script_class.name == name]), None)\n        if script is None:\n            continue\n\n        constructor = lambda s=script: ScriptPostprocessingForMainUI(s.script_class())\n        res.append(scripts.ScriptClassData(script_class=constructor, path=script.path, basedir=script.basedir, module=script.module))\n\n    return res\n", "modules/sd_schedulers.py": "import dataclasses\n\nimport torch\n\nimport k_diffusion\n\n\n@dataclasses.dataclass\nclass Scheduler:\n    name: str\n    label: str\n    function: any\n\n    default_rho: float = -1\n    need_inner_model: bool = False\n    aliases: list = None\n\n\ndef uniform(n, sigma_min, sigma_max, inner_model, device):\n    return inner_model.get_sigmas(n)\n\n\ndef sgm_uniform(n, sigma_min, sigma_max, inner_model, device):\n    start = inner_model.sigma_to_t(torch.tensor(sigma_max))\n    end = inner_model.sigma_to_t(torch.tensor(sigma_min))\n    sigs = [\n        inner_model.t_to_sigma(ts)\n        for ts in torch.linspace(start, end, n + 1)[:-1]\n    ]\n    sigs += [0.0]\n    return torch.FloatTensor(sigs).to(device)\n\n\nschedulers = [\n    Scheduler('automatic', 'Automatic', None),\n    Scheduler('uniform', 'Uniform', uniform, need_inner_model=True),\n    Scheduler('karras', 'Karras', k_diffusion.sampling.get_sigmas_karras, default_rho=7.0),\n    Scheduler('exponential', 'Exponential', k_diffusion.sampling.get_sigmas_exponential),\n    Scheduler('polyexponential', 'Polyexponential', k_diffusion.sampling.get_sigmas_polyexponential, default_rho=1.0),\n    Scheduler('sgm_uniform', 'SGM Uniform', sgm_uniform, need_inner_model=True, aliases=[\"SGMUniform\"]),\n]\n\nschedulers_map = {**{x.name: x for x in schedulers}, **{x.label: x for x in schedulers}}\n", "modules/ui_gradio_extensions.py": "import os\nimport gradio as gr\n\nfrom modules import localization, shared, scripts, util\nfrom modules.paths import script_path, data_path\n\n\ndef webpath(fn):\n    return f'file={util.truncate_path(fn)}?{os.path.getmtime(fn)}'\n\n\ndef javascript_html():\n    # Ensure localization is in `window` before scripts\n    head = f'<script type=\"text/javascript\">{localization.localization_js(shared.opts.localization)}</script>\\n'\n\n    script_js = os.path.join(script_path, \"script.js\")\n    head += f'<script type=\"text/javascript\" src=\"{webpath(script_js)}\"></script>\\n'\n\n    for script in scripts.list_scripts(\"javascript\", \".js\"):\n        head += f'<script type=\"text/javascript\" src=\"{webpath(script.path)}\"></script>\\n'\n\n    for script in scripts.list_scripts(\"javascript\", \".mjs\"):\n        head += f'<script type=\"module\" src=\"{webpath(script.path)}\"></script>\\n'\n\n    if shared.cmd_opts.theme:\n        head += f'<script type=\"text/javascript\">set_theme(\\\"{shared.cmd_opts.theme}\\\");</script>\\n'\n\n    return head\n\n\ndef css_html():\n    head = \"\"\n\n    def stylesheet(fn):\n        return f'<link rel=\"stylesheet\" property=\"stylesheet\" href=\"{webpath(fn)}\">'\n\n    for cssfile in scripts.list_files_with_name(\"style.css\"):\n        head += stylesheet(cssfile)\n\n    user_css = os.path.join(data_path, \"user.css\")\n    if os.path.exists(user_css):\n        head += stylesheet(user_css)\n\n    return head\n\n\ndef reload_javascript():\n    js = javascript_html()\n    css = css_html()\n\n    def template_response(*args, **kwargs):\n        res = shared.GradioTemplateResponseOriginal(*args, **kwargs)\n        res.body = res.body.replace(b'</head>', f'{js}</head>'.encode(\"utf8\"))\n        res.body = res.body.replace(b'</body>', f'{css}</body>'.encode(\"utf8\"))\n        res.init_headers()\n        return res\n\n    gr.routes.templates.TemplateResponse = template_response\n\n\nif not hasattr(shared, 'GradioTemplateResponseOriginal'):\n    shared.GradioTemplateResponseOriginal = gr.routes.templates.TemplateResponse\n", "modules/gradio_extensons.py": "import gradio as gr\n\nfrom modules import scripts, ui_tempdir, patches\n\n\ndef add_classes_to_gradio_component(comp):\n    \"\"\"\n    this adds gradio-* to the component for css styling (ie gradio-button to gr.Button), as well as some others\n    \"\"\"\n\n    comp.elem_classes = [f\"gradio-{comp.get_block_name()}\", *(comp.elem_classes or [])]\n\n    if getattr(comp, 'multiselect', False):\n        comp.elem_classes.append('multiselect')\n\n\ndef IOComponent_init(self, *args, **kwargs):\n    self.webui_tooltip = kwargs.pop('tooltip', None)\n\n    if scripts.scripts_current is not None:\n        scripts.scripts_current.before_component(self, **kwargs)\n\n    scripts.script_callbacks.before_component_callback(self, **kwargs)\n\n    res = original_IOComponent_init(self, *args, **kwargs)\n\n    add_classes_to_gradio_component(self)\n\n    scripts.script_callbacks.after_component_callback(self, **kwargs)\n\n    if scripts.scripts_current is not None:\n        scripts.scripts_current.after_component(self, **kwargs)\n\n    return res\n\n\ndef Block_get_config(self):\n    config = original_Block_get_config(self)\n\n    webui_tooltip = getattr(self, 'webui_tooltip', None)\n    if webui_tooltip:\n        config[\"webui_tooltip\"] = webui_tooltip\n\n    config.pop('example_inputs', None)\n\n    return config\n\n\ndef BlockContext_init(self, *args, **kwargs):\n    if scripts.scripts_current is not None:\n        scripts.scripts_current.before_component(self, **kwargs)\n\n    scripts.script_callbacks.before_component_callback(self, **kwargs)\n\n    res = original_BlockContext_init(self, *args, **kwargs)\n\n    add_classes_to_gradio_component(self)\n\n    scripts.script_callbacks.after_component_callback(self, **kwargs)\n\n    if scripts.scripts_current is not None:\n        scripts.scripts_current.after_component(self, **kwargs)\n\n    return res\n\n\ndef Blocks_get_config_file(self, *args, **kwargs):\n    config = original_Blocks_get_config_file(self, *args, **kwargs)\n\n    for comp_config in config[\"components\"]:\n        if \"example_inputs\" in comp_config:\n            comp_config[\"example_inputs\"] = {\"serialized\": []}\n\n    return config\n\n\noriginal_IOComponent_init = patches.patch(__name__, obj=gr.components.IOComponent, field=\"__init__\", replacement=IOComponent_init)\noriginal_Block_get_config = patches.patch(__name__, obj=gr.blocks.Block, field=\"get_config\", replacement=Block_get_config)\noriginal_BlockContext_init = patches.patch(__name__, obj=gr.blocks.BlockContext, field=\"__init__\", replacement=BlockContext_init)\noriginal_Blocks_get_config_file = patches.patch(__name__, obj=gr.blocks.Blocks, field=\"get_config_file\", replacement=Blocks_get_config_file)\n\n\nui_tempdir.install_ui_tempdir_override()\n", "modules/lowvram.py": "import torch\nfrom modules import devices, shared\n\nmodule_in_gpu = None\ncpu = torch.device(\"cpu\")\n\n\ndef send_everything_to_cpu():\n    global module_in_gpu\n\n    if module_in_gpu is not None:\n        module_in_gpu.to(cpu)\n\n    module_in_gpu = None\n\n\ndef is_needed(sd_model):\n    return shared.cmd_opts.lowvram or shared.cmd_opts.medvram or shared.cmd_opts.medvram_sdxl and hasattr(sd_model, 'conditioner')\n\n\ndef apply(sd_model):\n    enable = is_needed(sd_model)\n    shared.parallel_processing_allowed = not enable\n\n    if enable:\n        setup_for_low_vram(sd_model, not shared.cmd_opts.lowvram)\n    else:\n        sd_model.lowvram = False\n\n\ndef setup_for_low_vram(sd_model, use_medvram):\n    if getattr(sd_model, 'lowvram', False):\n        return\n\n    sd_model.lowvram = True\n\n    parents = {}\n\n    def send_me_to_gpu(module, _):\n        \"\"\"send this module to GPU; send whatever tracked module was previous in GPU to CPU;\n        we add this as forward_pre_hook to a lot of modules and this way all but one of them will\n        be in CPU\n        \"\"\"\n        global module_in_gpu\n\n        module = parents.get(module, module)\n\n        if module_in_gpu == module:\n            return\n\n        if module_in_gpu is not None:\n            module_in_gpu.to(cpu)\n\n        module.to(devices.device)\n        module_in_gpu = module\n\n    # see below for register_forward_pre_hook;\n    # first_stage_model does not use forward(), it uses encode/decode, so register_forward_pre_hook is\n    # useless here, and we just replace those methods\n\n    first_stage_model = sd_model.first_stage_model\n    first_stage_model_encode = sd_model.first_stage_model.encode\n    first_stage_model_decode = sd_model.first_stage_model.decode\n\n    def first_stage_model_encode_wrap(x):\n        send_me_to_gpu(first_stage_model, None)\n        return first_stage_model_encode(x)\n\n    def first_stage_model_decode_wrap(z):\n        send_me_to_gpu(first_stage_model, None)\n        return first_stage_model_decode(z)\n\n    to_remain_in_cpu = [\n        (sd_model, 'first_stage_model'),\n        (sd_model, 'depth_model'),\n        (sd_model, 'embedder'),\n        (sd_model, 'model'),\n        (sd_model, 'embedder'),\n    ]\n\n    is_sdxl = hasattr(sd_model, 'conditioner')\n    is_sd2 = not is_sdxl and hasattr(sd_model.cond_stage_model, 'model')\n\n    if is_sdxl:\n        to_remain_in_cpu.append((sd_model, 'conditioner'))\n    elif is_sd2:\n        to_remain_in_cpu.append((sd_model.cond_stage_model, 'model'))\n    else:\n        to_remain_in_cpu.append((sd_model.cond_stage_model, 'transformer'))\n\n    # remove several big modules: cond, first_stage, depth/embedder (if applicable), and unet from the model\n    stored = []\n    for obj, field in to_remain_in_cpu:\n        module = getattr(obj, field, None)\n        stored.append(module)\n        setattr(obj, field, None)\n\n    # send the model to GPU.\n    sd_model.to(devices.device)\n\n    # put modules back. the modules will be in CPU.\n    for (obj, field), module in zip(to_remain_in_cpu, stored):\n        setattr(obj, field, module)\n\n    # register hooks for those the first three models\n    if is_sdxl:\n        sd_model.conditioner.register_forward_pre_hook(send_me_to_gpu)\n    elif is_sd2:\n        sd_model.cond_stage_model.model.register_forward_pre_hook(send_me_to_gpu)\n        sd_model.cond_stage_model.model.token_embedding.register_forward_pre_hook(send_me_to_gpu)\n        parents[sd_model.cond_stage_model.model] = sd_model.cond_stage_model\n        parents[sd_model.cond_stage_model.model.token_embedding] = sd_model.cond_stage_model\n    else:\n        sd_model.cond_stage_model.transformer.register_forward_pre_hook(send_me_to_gpu)\n        parents[sd_model.cond_stage_model.transformer] = sd_model.cond_stage_model\n\n    sd_model.first_stage_model.register_forward_pre_hook(send_me_to_gpu)\n    sd_model.first_stage_model.encode = first_stage_model_encode_wrap\n    sd_model.first_stage_model.decode = first_stage_model_decode_wrap\n    if sd_model.depth_model:\n        sd_model.depth_model.register_forward_pre_hook(send_me_to_gpu)\n    if sd_model.embedder:\n        sd_model.embedder.register_forward_pre_hook(send_me_to_gpu)\n\n    if use_medvram:\n        sd_model.model.register_forward_pre_hook(send_me_to_gpu)\n    else:\n        diff_model = sd_model.model.diffusion_model\n\n        # the third remaining model is still too big for 4 GB, so we also do the same for its submodules\n        # so that only one of them is in GPU at a time\n        stored = diff_model.input_blocks, diff_model.middle_block, diff_model.output_blocks, diff_model.time_embed\n        diff_model.input_blocks, diff_model.middle_block, diff_model.output_blocks, diff_model.time_embed = None, None, None, None\n        sd_model.model.to(devices.device)\n        diff_model.input_blocks, diff_model.middle_block, diff_model.output_blocks, diff_model.time_embed = stored\n\n        # install hooks for bits of third model\n        diff_model.time_embed.register_forward_pre_hook(send_me_to_gpu)\n        for block in diff_model.input_blocks:\n            block.register_forward_pre_hook(send_me_to_gpu)\n        diff_model.middle_block.register_forward_pre_hook(send_me_to_gpu)\n        for block in diff_model.output_blocks:\n            block.register_forward_pre_hook(send_me_to_gpu)\n\n\ndef is_enabled(sd_model):\n    return sd_model.lowvram\n", "modules/face_restoration.py": "from modules import shared\n\n\nclass FaceRestoration:\n    def name(self):\n        return \"None\"\n\n    def restore(self, np_image):\n        return np_image\n\n\ndef restore_faces(np_image):\n    face_restorers = [x for x in shared.face_restorers if x.name() == shared.opts.face_restoration_model or shared.opts.face_restoration_model is None]\n    if len(face_restorers) == 0:\n        return np_image\n\n    face_restorer = face_restorers[0]\n\n    return face_restorer.restore(np_image)\n", "modules/infotext_utils.py": "from __future__ import annotations\nimport base64\nimport io\nimport json\nimport os\nimport re\nimport sys\n\nimport gradio as gr\nfrom modules.paths import data_path\nfrom modules import shared, ui_tempdir, script_callbacks, processing, infotext_versions, images, prompt_parser, errors\nfrom PIL import Image\n\nsys.modules['modules.generation_parameters_copypaste'] = sys.modules[__name__]  # alias for old name\n\nre_param_code = r'\\s*(\\w[\\w \\-/]+):\\s*(\"(?:\\\\.|[^\\\\\"])+\"|[^,]*)(?:,|$)'\nre_param = re.compile(re_param_code)\nre_imagesize = re.compile(r\"^(\\d+)x(\\d+)$\")\nre_hypernet_hash = re.compile(\"\\(([0-9a-f]+)\\)$\")\ntype_of_gr_update = type(gr.update())\n\n\nclass ParamBinding:\n    def __init__(self, paste_button, tabname, source_text_component=None, source_image_component=None, source_tabname=None, override_settings_component=None, paste_field_names=None):\n        self.paste_button = paste_button\n        self.tabname = tabname\n        self.source_text_component = source_text_component\n        self.source_image_component = source_image_component\n        self.source_tabname = source_tabname\n        self.override_settings_component = override_settings_component\n        self.paste_field_names = paste_field_names or []\n\n\nclass PasteField(tuple):\n    def __new__(cls, component, target, *, api=None):\n        return super().__new__(cls, (component, target))\n\n    def __init__(self, component, target, *, api=None):\n        super().__init__()\n\n        self.api = api\n        self.component = component\n        self.label = target if isinstance(target, str) else None\n        self.function = target if callable(target) else None\n\n\npaste_fields: dict[str, dict] = {}\nregistered_param_bindings: list[ParamBinding] = []\n\n\ndef reset():\n    paste_fields.clear()\n    registered_param_bindings.clear()\n\n\ndef quote(text):\n    if ',' not in str(text) and '\\n' not in str(text) and ':' not in str(text):\n        return text\n\n    return json.dumps(text, ensure_ascii=False)\n\n\ndef unquote(text):\n    if len(text) == 0 or text[0] != '\"' or text[-1] != '\"':\n        return text\n\n    try:\n        return json.loads(text)\n    except Exception:\n        return text\n\n\ndef image_from_url_text(filedata):\n    if filedata is None:\n        return None\n\n    if type(filedata) == list and filedata and type(filedata[0]) == dict and filedata[0].get(\"is_file\", False):\n        filedata = filedata[0]\n\n    if type(filedata) == dict and filedata.get(\"is_file\", False):\n        filename = filedata[\"name\"]\n        is_in_right_dir = ui_tempdir.check_tmp_file(shared.demo, filename)\n        assert is_in_right_dir, 'trying to open image file outside of allowed directories'\n\n        filename = filename.rsplit('?', 1)[0]\n        return images.read(filename)\n\n    if type(filedata) == list:\n        if len(filedata) == 0:\n            return None\n\n        filedata = filedata[0]\n\n    if filedata.startswith(\"data:image/png;base64,\"):\n        filedata = filedata[len(\"data:image/png;base64,\"):]\n\n    filedata = base64.decodebytes(filedata.encode('utf-8'))\n    image = images.read(io.BytesIO(filedata))\n    return image\n\n\ndef add_paste_fields(tabname, init_img, fields, override_settings_component=None):\n\n    if fields:\n        for i in range(len(fields)):\n            if not isinstance(fields[i], PasteField):\n                fields[i] = PasteField(*fields[i])\n\n    paste_fields[tabname] = {\"init_img\": init_img, \"fields\": fields, \"override_settings_component\": override_settings_component}\n\n    # backwards compatibility for existing extensions\n    import modules.ui\n    if tabname == 'txt2img':\n        modules.ui.txt2img_paste_fields = fields\n    elif tabname == 'img2img':\n        modules.ui.img2img_paste_fields = fields\n\n\ndef create_buttons(tabs_list):\n    buttons = {}\n    for tab in tabs_list:\n        buttons[tab] = gr.Button(f\"Send to {tab}\", elem_id=f\"{tab}_tab\")\n    return buttons\n\n\ndef bind_buttons(buttons, send_image, send_generate_info):\n    \"\"\"old function for backwards compatibility; do not use this, use register_paste_params_button\"\"\"\n    for tabname, button in buttons.items():\n        source_text_component = send_generate_info if isinstance(send_generate_info, gr.components.Component) else None\n        source_tabname = send_generate_info if isinstance(send_generate_info, str) else None\n\n        register_paste_params_button(ParamBinding(paste_button=button, tabname=tabname, source_text_component=source_text_component, source_image_component=send_image, source_tabname=source_tabname))\n\n\ndef register_paste_params_button(binding: ParamBinding):\n    registered_param_bindings.append(binding)\n\n\ndef connect_paste_params_buttons():\n    for binding in registered_param_bindings:\n        destination_image_component = paste_fields[binding.tabname][\"init_img\"]\n        fields = paste_fields[binding.tabname][\"fields\"]\n        override_settings_component = binding.override_settings_component or paste_fields[binding.tabname][\"override_settings_component\"]\n\n        destination_width_component = next(iter([field for field, name in fields if name == \"Size-1\"] if fields else []), None)\n        destination_height_component = next(iter([field for field, name in fields if name == \"Size-2\"] if fields else []), None)\n\n        if binding.source_image_component and destination_image_component:\n            if isinstance(binding.source_image_component, gr.Gallery):\n                func = send_image_and_dimensions if destination_width_component else image_from_url_text\n                jsfunc = \"extract_image_from_gallery\"\n            else:\n                func = send_image_and_dimensions if destination_width_component else lambda x: x\n                jsfunc = None\n\n            binding.paste_button.click(\n                fn=func,\n                _js=jsfunc,\n                inputs=[binding.source_image_component],\n                outputs=[destination_image_component, destination_width_component, destination_height_component] if destination_width_component else [destination_image_component],\n                show_progress=False,\n            )\n\n        if binding.source_text_component is not None and fields is not None:\n            connect_paste(binding.paste_button, fields, binding.source_text_component, override_settings_component, binding.tabname)\n\n        if binding.source_tabname is not None and fields is not None:\n            paste_field_names = ['Prompt', 'Negative prompt', 'Steps', 'Face restoration'] + ([\"Seed\"] if shared.opts.send_seed else []) + binding.paste_field_names\n            binding.paste_button.click(\n                fn=lambda *x: x,\n                inputs=[field for field, name in paste_fields[binding.source_tabname][\"fields\"] if name in paste_field_names],\n                outputs=[field for field, name in fields if name in paste_field_names],\n                show_progress=False,\n            )\n\n        binding.paste_button.click(\n            fn=None,\n            _js=f\"switch_to_{binding.tabname}\",\n            inputs=None,\n            outputs=None,\n            show_progress=False,\n        )\n\n\ndef send_image_and_dimensions(x):\n    if isinstance(x, Image.Image):\n        img = x\n    else:\n        img = image_from_url_text(x)\n\n    if shared.opts.send_size and isinstance(img, Image.Image):\n        w = img.width\n        h = img.height\n    else:\n        w = gr.update()\n        h = gr.update()\n\n    return img, w, h\n\n\ndef restore_old_hires_fix_params(res):\n    \"\"\"for infotexts that specify old First pass size parameter, convert it into\n    width, height, and hr scale\"\"\"\n\n    firstpass_width = res.get('First pass size-1', None)\n    firstpass_height = res.get('First pass size-2', None)\n\n    if shared.opts.use_old_hires_fix_width_height:\n        hires_width = int(res.get(\"Hires resize-1\", 0))\n        hires_height = int(res.get(\"Hires resize-2\", 0))\n\n        if hires_width and hires_height:\n            res['Size-1'] = hires_width\n            res['Size-2'] = hires_height\n            return\n\n    if firstpass_width is None or firstpass_height is None:\n        return\n\n    firstpass_width, firstpass_height = int(firstpass_width), int(firstpass_height)\n    width = int(res.get(\"Size-1\", 512))\n    height = int(res.get(\"Size-2\", 512))\n\n    if firstpass_width == 0 or firstpass_height == 0:\n        firstpass_width, firstpass_height = processing.old_hires_fix_first_pass_dimensions(width, height)\n\n    res['Size-1'] = firstpass_width\n    res['Size-2'] = firstpass_height\n    res['Hires resize-1'] = width\n    res['Hires resize-2'] = height\n\n\ndef parse_generation_parameters(x: str, skip_fields: list[str] | None = None):\n    \"\"\"parses generation parameters string, the one you see in text field under the picture in UI:\n```\ngirl with an artist's beret, determined, blue eyes, desert scene, computer monitors, heavy makeup, by Alphonse Mucha and Charlie Bowater, ((eyeshadow)), (coquettish), detailed, intricate\nNegative prompt: ugly, fat, obese, chubby, (((deformed))), [blurry], bad anatomy, disfigured, poorly drawn face, mutation, mutated, (extra_limb), (ugly), (poorly drawn hands), messy drawing\nSteps: 20, Sampler: Euler a, CFG scale: 7, Seed: 965400086, Size: 512x512, Model hash: 45dee52b\n```\n\n    returns a dict with field values\n    \"\"\"\n    if skip_fields is None:\n        skip_fields = shared.opts.infotext_skip_pasting\n\n    res = {}\n\n    prompt = \"\"\n    negative_prompt = \"\"\n\n    done_with_prompt = False\n\n    *lines, lastline = x.strip().split(\"\\n\")\n    if len(re_param.findall(lastline)) < 3:\n        lines.append(lastline)\n        lastline = ''\n\n    for line in lines:\n        line = line.strip()\n        if line.startswith(\"Negative prompt:\"):\n            done_with_prompt = True\n            line = line[16:].strip()\n        if done_with_prompt:\n            negative_prompt += (\"\" if negative_prompt == \"\" else \"\\n\") + line\n        else:\n            prompt += (\"\" if prompt == \"\" else \"\\n\") + line\n\n    for k, v in re_param.findall(lastline):\n        try:\n            if v[0] == '\"' and v[-1] == '\"':\n                v = unquote(v)\n\n            m = re_imagesize.match(v)\n            if m is not None:\n                res[f\"{k}-1\"] = m.group(1)\n                res[f\"{k}-2\"] = m.group(2)\n            else:\n                res[k] = v\n        except Exception:\n            print(f\"Error parsing \\\"{k}: {v}\\\"\")\n\n    # Extract styles from prompt\n    if shared.opts.infotext_styles != \"Ignore\":\n        found_styles, prompt_no_styles, negative_prompt_no_styles = shared.prompt_styles.extract_styles_from_prompt(prompt, negative_prompt)\n\n        same_hr_styles = True\n        if (\"Hires prompt\" in res or \"Hires negative prompt\" in res) and (infotext_ver > infotext_versions.v180_hr_styles if (infotext_ver := infotext_versions.parse_version(res.get(\"Version\"))) else True):\n            hr_prompt, hr_negative_prompt = res.get(\"Hires prompt\", prompt), res.get(\"Hires negative prompt\", negative_prompt)\n            hr_found_styles, hr_prompt_no_styles, hr_negative_prompt_no_styles = shared.prompt_styles.extract_styles_from_prompt(hr_prompt, hr_negative_prompt)\n            if same_hr_styles := found_styles == hr_found_styles:\n                res[\"Hires prompt\"] = '' if hr_prompt_no_styles == prompt_no_styles else hr_prompt_no_styles\n                res['Hires negative prompt'] = '' if hr_negative_prompt_no_styles == negative_prompt_no_styles else hr_negative_prompt_no_styles\n\n        if same_hr_styles:\n            prompt, negative_prompt = prompt_no_styles, negative_prompt_no_styles\n            if (shared.opts.infotext_styles == \"Apply if any\" and found_styles) or shared.opts.infotext_styles == \"Apply\":\n                res['Styles array'] = found_styles\n\n    res[\"Prompt\"] = prompt\n    res[\"Negative prompt\"] = negative_prompt\n\n    # Missing CLIP skip means it was set to 1 (the default)\n    if \"Clip skip\" not in res:\n        res[\"Clip skip\"] = \"1\"\n\n    hypernet = res.get(\"Hypernet\", None)\n    if hypernet is not None:\n        res[\"Prompt\"] += f\"\"\"<hypernet:{hypernet}:{res.get(\"Hypernet strength\", \"1.0\")}>\"\"\"\n\n    if \"Hires resize-1\" not in res:\n        res[\"Hires resize-1\"] = 0\n        res[\"Hires resize-2\"] = 0\n\n    if \"Hires sampler\" not in res:\n        res[\"Hires sampler\"] = \"Use same sampler\"\n\n    if \"Hires schedule type\" not in res:\n        res[\"Hires schedule type\"] = \"Use same scheduler\"\n\n    if \"Hires checkpoint\" not in res:\n        res[\"Hires checkpoint\"] = \"Use same checkpoint\"\n\n    if \"Hires prompt\" not in res:\n        res[\"Hires prompt\"] = \"\"\n\n    if \"Hires negative prompt\" not in res:\n        res[\"Hires negative prompt\"] = \"\"\n\n    if \"Mask mode\" not in res:\n        res[\"Mask mode\"] = \"Inpaint masked\"\n\n    if \"Masked content\" not in res:\n        res[\"Masked content\"] = 'original'\n\n    if \"Inpaint area\" not in res:\n        res[\"Inpaint area\"] = \"Whole picture\"\n\n    if \"Masked area padding\" not in res:\n        res[\"Masked area padding\"] = 32\n\n    restore_old_hires_fix_params(res)\n\n    # Missing RNG means the default was set, which is GPU RNG\n    if \"RNG\" not in res:\n        res[\"RNG\"] = \"GPU\"\n\n    if \"Schedule type\" not in res:\n        res[\"Schedule type\"] = \"Automatic\"\n\n    if \"Schedule max sigma\" not in res:\n        res[\"Schedule max sigma\"] = 0\n\n    if \"Schedule min sigma\" not in res:\n        res[\"Schedule min sigma\"] = 0\n\n    if \"Schedule rho\" not in res:\n        res[\"Schedule rho\"] = 0\n\n    if \"VAE Encoder\" not in res:\n        res[\"VAE Encoder\"] = \"Full\"\n\n    if \"VAE Decoder\" not in res:\n        res[\"VAE Decoder\"] = \"Full\"\n\n    if \"FP8 weight\" not in res:\n        res[\"FP8 weight\"] = \"Disable\"\n\n    if \"Cache FP16 weight for LoRA\" not in res and res[\"FP8 weight\"] != \"Disable\":\n        res[\"Cache FP16 weight for LoRA\"] = False\n\n    prompt_attention = prompt_parser.parse_prompt_attention(prompt)\n    prompt_attention += prompt_parser.parse_prompt_attention(negative_prompt)\n    prompt_uses_emphasis = len(prompt_attention) != len([p for p in prompt_attention if p[1] == 1.0 or p[0] == 'BREAK'])\n    if \"Emphasis\" not in res and prompt_uses_emphasis:\n        res[\"Emphasis\"] = \"Original\"\n\n    if \"Refiner switch by sampling steps\" not in res:\n        res[\"Refiner switch by sampling steps\"] = False\n\n    infotext_versions.backcompat(res)\n\n    for key in skip_fields:\n        res.pop(key, None)\n\n    return res\n\n\ninfotext_to_setting_name_mapping = [\n\n]\n\"\"\"Mapping of infotext labels to setting names. Only left for backwards compatibility - use OptionInfo(..., infotext='...') instead.\nExample content:\n\ninfotext_to_setting_name_mapping = [\n    ('Conditional mask weight', 'inpainting_mask_weight'),\n    ('Model hash', 'sd_model_checkpoint'),\n    ('ENSD', 'eta_noise_seed_delta'),\n    ('Schedule type', 'k_sched_type'),\n]\n\"\"\"\n\n\ndef create_override_settings_dict(text_pairs):\n    \"\"\"creates processing's override_settings parameters from gradio's multiselect\n\n    Example input:\n        ['Clip skip: 2', 'Model hash: e6e99610c4', 'ENSD: 31337']\n\n    Example output:\n        {'CLIP_stop_at_last_layers': 2, 'sd_model_checkpoint': 'e6e99610c4', 'eta_noise_seed_delta': 31337}\n    \"\"\"\n\n    res = {}\n\n    params = {}\n    for pair in text_pairs:\n        k, v = pair.split(\":\", maxsplit=1)\n\n        params[k] = v.strip()\n\n    mapping = [(info.infotext, k) for k, info in shared.opts.data_labels.items() if info.infotext]\n    for param_name, setting_name in mapping + infotext_to_setting_name_mapping:\n        value = params.get(param_name, None)\n\n        if value is None:\n            continue\n\n        res[setting_name] = shared.opts.cast_value(setting_name, value)\n\n    return res\n\n\ndef get_override_settings(params, *, skip_fields=None):\n    \"\"\"Returns a list of settings overrides from the infotext parameters dictionary.\n\n    This function checks the `params` dictionary for any keys that correspond to settings in `shared.opts` and returns\n    a list of tuples containing the parameter name, setting name, and new value cast to correct type.\n\n    It checks for conditions before adding an override:\n    - ignores settings that match the current value\n    - ignores parameter keys present in skip_fields argument.\n\n    Example input:\n        {\"Clip skip\": \"2\"}\n\n    Example output:\n        [(\"Clip skip\", \"CLIP_stop_at_last_layers\", 2)]\n    \"\"\"\n\n    res = []\n\n    mapping = [(info.infotext, k) for k, info in shared.opts.data_labels.items() if info.infotext]\n    for param_name, setting_name in mapping + infotext_to_setting_name_mapping:\n        if param_name in (skip_fields or {}):\n            continue\n\n        v = params.get(param_name, None)\n        if v is None:\n            continue\n\n        if setting_name == \"sd_model_checkpoint\" and shared.opts.disable_weights_auto_swap:\n            continue\n\n        v = shared.opts.cast_value(setting_name, v)\n        current_value = getattr(shared.opts, setting_name, None)\n\n        if v == current_value:\n            continue\n\n        res.append((param_name, setting_name, v))\n\n    return res\n\n\ndef connect_paste(button, paste_fields, input_comp, override_settings_component, tabname):\n    def paste_func(prompt):\n        if not prompt and not shared.cmd_opts.hide_ui_dir_config and not shared.cmd_opts.no_prompt_history:\n            filename = os.path.join(data_path, \"params.txt\")\n            try:\n                with open(filename, \"r\", encoding=\"utf8\") as file:\n                    prompt = file.read()\n            except OSError:\n                pass\n\n        params = parse_generation_parameters(prompt)\n        script_callbacks.infotext_pasted_callback(prompt, params)\n        res = []\n\n        for output, key in paste_fields:\n            if callable(key):\n                try:\n                    v = key(params)\n                except Exception:\n                    errors.report(f\"Error executing {key}\", exc_info=True)\n                    v = None\n            else:\n                v = params.get(key, None)\n\n            if v is None:\n                res.append(gr.update())\n            elif isinstance(v, type_of_gr_update):\n                res.append(v)\n            else:\n                try:\n                    valtype = type(output.value)\n\n                    if valtype == bool and v == \"False\":\n                        val = False\n                    elif valtype == int:\n                        val = float(v)\n                    else:\n                        val = valtype(v)\n\n                    res.append(gr.update(value=val))\n                except Exception:\n                    res.append(gr.update())\n\n        return res\n\n    if override_settings_component is not None:\n        already_handled_fields = {key: 1 for _, key in paste_fields}\n\n        def paste_settings(params):\n            vals = get_override_settings(params, skip_fields=already_handled_fields)\n\n            vals_pairs = [f\"{infotext_text}: {value}\" for infotext_text, setting_name, value in vals]\n\n            return gr.Dropdown.update(value=vals_pairs, choices=vals_pairs, visible=bool(vals_pairs))\n\n        paste_fields = paste_fields + [(override_settings_component, paste_settings)]\n\n    button.click(\n        fn=paste_func,\n        inputs=[input_comp],\n        outputs=[x[0] for x in paste_fields],\n        show_progress=False,\n    )\n    button.click(\n        fn=None,\n        _js=f\"recalculate_prompts_{tabname}\",\n        inputs=[],\n        outputs=[],\n        show_progress=False,\n    )\n\n", "modules/sd_hijack_checkpoint.py": "from torch.utils.checkpoint import checkpoint\n\nimport ldm.modules.attention\nimport ldm.modules.diffusionmodules.openaimodel\n\n\ndef BasicTransformerBlock_forward(self, x, context=None):\n    return checkpoint(self._forward, x, context)\n\n\ndef AttentionBlock_forward(self, x):\n    return checkpoint(self._forward, x)\n\n\ndef ResBlock_forward(self, x, emb):\n    return checkpoint(self._forward, x, emb)\n\n\nstored = []\n\n\ndef add():\n    if len(stored) != 0:\n        return\n\n    stored.extend([\n        ldm.modules.attention.BasicTransformerBlock.forward,\n        ldm.modules.diffusionmodules.openaimodel.ResBlock.forward,\n        ldm.modules.diffusionmodules.openaimodel.AttentionBlock.forward\n    ])\n\n    ldm.modules.attention.BasicTransformerBlock.forward = BasicTransformerBlock_forward\n    ldm.modules.diffusionmodules.openaimodel.ResBlock.forward = ResBlock_forward\n    ldm.modules.diffusionmodules.openaimodel.AttentionBlock.forward = AttentionBlock_forward\n\n\ndef remove():\n    if len(stored) == 0:\n        return\n\n    ldm.modules.attention.BasicTransformerBlock.forward = stored[0]\n    ldm.modules.diffusionmodules.openaimodel.ResBlock.forward = stored[1]\n    ldm.modules.diffusionmodules.openaimodel.AttentionBlock.forward = stored[2]\n\n    stored.clear()\n\n", "modules/sd_hijack_unet.py": "import torch\nfrom packaging import version\n\nfrom modules import devices\nfrom modules.sd_hijack_utils import CondFunc\n\n\nclass TorchHijackForUnet:\n    \"\"\"\n    This is torch, but with cat that resizes tensors to appropriate dimensions if they do not match;\n    this makes it possible to create pictures with dimensions that are multiples of 8 rather than 64\n    \"\"\"\n\n    def __getattr__(self, item):\n        if item == 'cat':\n            return self.cat\n\n        if hasattr(torch, item):\n            return getattr(torch, item)\n\n        raise AttributeError(f\"'{type(self).__name__}' object has no attribute '{item}'\")\n\n    def cat(self, tensors, *args, **kwargs):\n        if len(tensors) == 2:\n            a, b = tensors\n            if a.shape[-2:] != b.shape[-2:]:\n                a = torch.nn.functional.interpolate(a, b.shape[-2:], mode=\"nearest\")\n\n            tensors = (a, b)\n\n        return torch.cat(tensors, *args, **kwargs)\n\n\nth = TorchHijackForUnet()\n\n\n# Below are monkey patches to enable upcasting a float16 UNet for float32 sampling\ndef apply_model(orig_func, self, x_noisy, t, cond, **kwargs):\n\n    if isinstance(cond, dict):\n        for y in cond.keys():\n            if isinstance(cond[y], list):\n                cond[y] = [x.to(devices.dtype_unet) if isinstance(x, torch.Tensor) else x for x in cond[y]]\n            else:\n                cond[y] = cond[y].to(devices.dtype_unet) if isinstance(cond[y], torch.Tensor) else cond[y]\n\n    with devices.autocast():\n        return orig_func(self, x_noisy.to(devices.dtype_unet), t.to(devices.dtype_unet), cond, **kwargs).float()\n\n\nclass GELUHijack(torch.nn.GELU, torch.nn.Module):\n    def __init__(self, *args, **kwargs):\n        torch.nn.GELU.__init__(self, *args, **kwargs)\n    def forward(self, x):\n        if devices.unet_needs_upcast:\n            return torch.nn.GELU.forward(self.float(), x.float()).to(devices.dtype_unet)\n        else:\n            return torch.nn.GELU.forward(self, x)\n\n\nddpm_edit_hijack = None\ndef hijack_ddpm_edit():\n    global ddpm_edit_hijack\n    if not ddpm_edit_hijack:\n        CondFunc('modules.models.diffusion.ddpm_edit.LatentDiffusion.decode_first_stage', first_stage_sub, first_stage_cond)\n        CondFunc('modules.models.diffusion.ddpm_edit.LatentDiffusion.encode_first_stage', first_stage_sub, first_stage_cond)\n        ddpm_edit_hijack = CondFunc('modules.models.diffusion.ddpm_edit.LatentDiffusion.apply_model', apply_model, unet_needs_upcast)\n\n\nunet_needs_upcast = lambda *args, **kwargs: devices.unet_needs_upcast\nCondFunc('ldm.models.diffusion.ddpm.LatentDiffusion.apply_model', apply_model, unet_needs_upcast)\nCondFunc('ldm.modules.diffusionmodules.openaimodel.timestep_embedding', lambda orig_func, timesteps, *args, **kwargs: orig_func(timesteps, *args, **kwargs).to(torch.float32 if timesteps.dtype == torch.int64 else devices.dtype_unet), unet_needs_upcast)\nif version.parse(torch.__version__) <= version.parse(\"1.13.2\") or torch.cuda.is_available():\n    CondFunc('ldm.modules.diffusionmodules.util.GroupNorm32.forward', lambda orig_func, self, *args, **kwargs: orig_func(self.float(), *args, **kwargs), unet_needs_upcast)\n    CondFunc('ldm.modules.attention.GEGLU.forward', lambda orig_func, self, x: orig_func(self.float(), x.float()).to(devices.dtype_unet), unet_needs_upcast)\n    CondFunc('open_clip.transformer.ResidualAttentionBlock.__init__', lambda orig_func, *args, **kwargs: kwargs.update({'act_layer': GELUHijack}) and False or orig_func(*args, **kwargs), lambda _, *args, **kwargs: kwargs.get('act_layer') is None or kwargs['act_layer'] == torch.nn.GELU)\n\nfirst_stage_cond = lambda _, self, *args, **kwargs: devices.unet_needs_upcast and self.model.diffusion_model.dtype == torch.float16\nfirst_stage_sub = lambda orig_func, self, x, **kwargs: orig_func(self, x.to(devices.dtype_vae), **kwargs)\nCondFunc('ldm.models.diffusion.ddpm.LatentDiffusion.decode_first_stage', first_stage_sub, first_stage_cond)\nCondFunc('ldm.models.diffusion.ddpm.LatentDiffusion.encode_first_stage', first_stage_sub, first_stage_cond)\nCondFunc('ldm.models.diffusion.ddpm.LatentDiffusion.get_first_stage_encoding', lambda orig_func, *args, **kwargs: orig_func(*args, **kwargs).float(), first_stage_cond)\n\nCondFunc('sgm.modules.diffusionmodules.wrappers.OpenAIWrapper.forward', apply_model, unet_needs_upcast)\nCondFunc('sgm.modules.diffusionmodules.openaimodel.timestep_embedding', lambda orig_func, timesteps, *args, **kwargs: orig_func(timesteps, *args, **kwargs).to(torch.float32 if timesteps.dtype == torch.int64 else devices.dtype_unet), unet_needs_upcast)\n", "modules/postprocessing.py": "import os\n\nfrom PIL import Image\n\nfrom modules import shared, images, devices, scripts, scripts_postprocessing, ui_common, infotext_utils\nfrom modules.shared import opts\n\n\ndef run_postprocessing(extras_mode, image, image_folder, input_dir, output_dir, show_extras_results, *args, save_output: bool = True):\n    devices.torch_gc()\n\n    shared.state.begin(job=\"extras\")\n\n    outputs = []\n\n    def get_images(extras_mode, image, image_folder, input_dir):\n        if extras_mode == 1:\n            for img in image_folder:\n                if isinstance(img, Image.Image):\n                    image = images.fix_image(img)\n                    fn = ''\n                else:\n                    image = images.read(os.path.abspath(img.name))\n                    fn = os.path.splitext(img.orig_name)[0]\n                yield image, fn\n        elif extras_mode == 2:\n            assert not shared.cmd_opts.hide_ui_dir_config, '--hide-ui-dir-config option must be disabled'\n            assert input_dir, 'input directory not selected'\n\n            image_list = shared.listfiles(input_dir)\n            for filename in image_list:\n                yield filename, filename\n        else:\n            assert image, 'image not selected'\n            yield image, None\n\n    if extras_mode == 2 and output_dir != '':\n        outpath = output_dir\n    else:\n        outpath = opts.outdir_samples or opts.outdir_extras_samples\n\n    infotext = ''\n\n    data_to_process = list(get_images(extras_mode, image, image_folder, input_dir))\n    shared.state.job_count = len(data_to_process)\n\n    for image_placeholder, name in data_to_process:\n        image_data: Image.Image\n\n        shared.state.nextjob()\n        shared.state.textinfo = name\n        shared.state.skipped = False\n\n        if shared.state.interrupted:\n            break\n\n        if isinstance(image_placeholder, str):\n            try:\n                image_data = images.read(image_placeholder)\n            except Exception:\n                continue\n        else:\n            image_data = image_placeholder\n\n        parameters, existing_pnginfo = images.read_info_from_image(image_data)\n        if parameters:\n            existing_pnginfo[\"parameters\"] = parameters\n\n        initial_pp = scripts_postprocessing.PostprocessedImage(image_data if image_data.mode in (\"RGBA\", \"RGB\") else image_data.convert(\"RGB\"))\n\n        scripts.scripts_postproc.run(initial_pp, args)\n\n        if shared.state.skipped:\n            continue\n\n        used_suffixes = {}\n        for pp in [initial_pp, *initial_pp.extra_images]:\n            suffix = pp.get_suffix(used_suffixes)\n\n            if opts.use_original_name_batch and name is not None:\n                basename = os.path.splitext(os.path.basename(name))[0]\n                forced_filename = basename + suffix\n            else:\n                basename = ''\n                forced_filename = None\n\n            infotext = \", \".join([k if k == v else f'{k}: {infotext_utils.quote(v)}' for k, v in pp.info.items() if v is not None])\n\n            if opts.enable_pnginfo:\n                pp.image.info = existing_pnginfo\n                pp.image.info[\"postprocessing\"] = infotext\n\n            shared.state.assign_current_image(pp.image)\n\n            if save_output:\n                fullfn, _ = images.save_image(pp.image, path=outpath, basename=basename, extension=opts.samples_format, info=infotext, short_filename=True, no_prompt=True, grid=False, pnginfo_section_name=\"extras\", existing_info=existing_pnginfo, forced_filename=forced_filename, suffix=suffix)\n\n                if pp.caption:\n                    caption_filename = os.path.splitext(fullfn)[0] + \".txt\"\n                    existing_caption = \"\"\n                    try:\n                        with open(caption_filename, encoding=\"utf8\") as file:\n                            existing_caption = file.read().strip()\n                    except FileNotFoundError:\n                        pass\n\n                    action = shared.opts.postprocessing_existing_caption_action\n                    if action == 'Prepend' and existing_caption:\n                        caption = f\"{existing_caption} {pp.caption}\"\n                    elif action == 'Append' and existing_caption:\n                        caption = f\"{pp.caption} {existing_caption}\"\n                    elif action == 'Keep' and existing_caption:\n                        caption = existing_caption\n                    else:\n                        caption = pp.caption\n\n                    caption = caption.strip()\n                    if caption:\n                        with open(caption_filename, \"w\", encoding=\"utf8\") as file:\n                            file.write(caption)\n\n            if extras_mode != 2 or show_extras_results:\n                outputs.append(pp.image)\n\n    devices.torch_gc()\n    shared.state.end()\n    return outputs, ui_common.plaintext_to_html(infotext), ''\n\n\ndef run_postprocessing_webui(id_task, *args, **kwargs):\n    return run_postprocessing(*args, **kwargs)\n\n\ndef run_extras(extras_mode, resize_mode, image, image_folder, input_dir, output_dir, show_extras_results, gfpgan_visibility, codeformer_visibility, codeformer_weight, upscaling_resize, upscaling_resize_w, upscaling_resize_h, upscaling_crop, extras_upscaler_1, extras_upscaler_2, extras_upscaler_2_visibility, upscale_first: bool, save_output: bool = True, max_side_length: int = 0):\n    \"\"\"old handler for API\"\"\"\n\n    args = scripts.scripts_postproc.create_args_for_run({\n        \"Upscale\": {\n            \"upscale_enabled\": True,\n            \"upscale_mode\": resize_mode,\n            \"upscale_by\": upscaling_resize,\n            \"max_side_length\": max_side_length,\n            \"upscale_to_width\": upscaling_resize_w,\n            \"upscale_to_height\": upscaling_resize_h,\n            \"upscale_crop\": upscaling_crop,\n            \"upscaler_1_name\": extras_upscaler_1,\n            \"upscaler_2_name\": extras_upscaler_2,\n            \"upscaler_2_visibility\": extras_upscaler_2_visibility,\n        },\n        \"GFPGAN\": {\n            \"enable\": True,\n            \"gfpgan_visibility\": gfpgan_visibility,\n        },\n        \"CodeFormer\": {\n            \"enable\": True,\n            \"codeformer_visibility\": codeformer_visibility,\n            \"codeformer_weight\": codeformer_weight,\n        },\n    })\n\n    return run_postprocessing(extras_mode, image, image_folder, input_dir, output_dir, show_extras_results, *args, save_output=save_output)\n", "modules/extensions.py": "from __future__ import annotations\n\nimport configparser\nimport dataclasses\nimport os\nimport threading\nimport re\n\nfrom modules import shared, errors, cache, scripts\nfrom modules.gitpython_hack import Repo\nfrom modules.paths_internal import extensions_dir, extensions_builtin_dir, script_path  # noqa: F401\n\nextensions: list[Extension] = []\nextension_paths: dict[str, Extension] = {}\nloaded_extensions: dict[str, Exception] = {}\n\n\nos.makedirs(extensions_dir, exist_ok=True)\n\n\ndef active():\n    if shared.cmd_opts.disable_all_extensions or shared.opts.disable_all_extensions == \"all\":\n        return []\n    elif shared.cmd_opts.disable_extra_extensions or shared.opts.disable_all_extensions == \"extra\":\n        return [x for x in extensions if x.enabled and x.is_builtin]\n    else:\n        return [x for x in extensions if x.enabled]\n\n\n@dataclasses.dataclass\nclass CallbackOrderInfo:\n    name: str\n    before: list\n    after: list\n\n\nclass ExtensionMetadata:\n    filename = \"metadata.ini\"\n    config: configparser.ConfigParser\n    canonical_name: str\n    requires: list\n\n    def __init__(self, path, canonical_name):\n        self.config = configparser.ConfigParser()\n\n        filepath = os.path.join(path, self.filename)\n        # `self.config.read()` will quietly swallow OSErrors (which FileNotFoundError is),\n        # so no need to check whether the file exists beforehand.\n        try:\n            self.config.read(filepath)\n        except Exception:\n            errors.report(f\"Error reading {self.filename} for extension {canonical_name}.\", exc_info=True)\n\n        self.canonical_name = self.config.get(\"Extension\", \"Name\", fallback=canonical_name)\n        self.canonical_name = canonical_name.lower().strip()\n\n        self.requires = None\n\n    def get_script_requirements(self, field, section, extra_section=None):\n        \"\"\"reads a list of requirements from the config; field is the name of the field in the ini file,\n        like Requires or Before, and section is the name of the [section] in the ini file; additionally,\n        reads more requirements from [extra_section] if specified.\"\"\"\n\n        x = self.config.get(section, field, fallback='')\n\n        if extra_section:\n            x = x + ', ' + self.config.get(extra_section, field, fallback='')\n\n        listed_requirements = self.parse_list(x.lower())\n        res = []\n\n        for requirement in listed_requirements:\n            loaded_requirements = (x for x in requirement.split(\"|\") if x in loaded_extensions)\n            relevant_requirement = next(loaded_requirements, requirement)\n            res.append(relevant_requirement)\n\n        return res\n\n    def parse_list(self, text):\n        \"\"\"converts a line from config (\"ext1 ext2, ext3  \") into a python list ([\"ext1\", \"ext2\", \"ext3\"])\"\"\"\n\n        if not text:\n            return []\n\n        # both \",\" and \" \" are accepted as separator\n        return [x for x in re.split(r\"[,\\s]+\", text.strip()) if x]\n\n    def list_callback_order_instructions(self):\n        for section in self.config.sections():\n            if not section.startswith(\"callbacks/\"):\n                continue\n\n            callback_name = section[10:]\n\n            if not callback_name.startswith(self.canonical_name):\n                errors.report(f\"Callback order section for extension {self.canonical_name} is referencing the wrong extension: {section}\")\n                continue\n\n            before = self.parse_list(self.config.get(section, 'Before', fallback=''))\n            after = self.parse_list(self.config.get(section, 'After', fallback=''))\n\n            yield CallbackOrderInfo(callback_name, before, after)\n\n\nclass Extension:\n    lock = threading.Lock()\n    cached_fields = ['remote', 'commit_date', 'branch', 'commit_hash', 'version']\n    metadata: ExtensionMetadata\n\n    def __init__(self, name, path, enabled=True, is_builtin=False, metadata=None):\n        self.name = name\n        self.path = path\n        self.enabled = enabled\n        self.status = ''\n        self.can_update = False\n        self.is_builtin = is_builtin\n        self.commit_hash = ''\n        self.commit_date = None\n        self.version = ''\n        self.branch = None\n        self.remote = None\n        self.have_info_from_repo = False\n        self.metadata = metadata if metadata else ExtensionMetadata(self.path, name.lower())\n        self.canonical_name = metadata.canonical_name\n\n    def to_dict(self):\n        return {x: getattr(self, x) for x in self.cached_fields}\n\n    def from_dict(self, d):\n        for field in self.cached_fields:\n            setattr(self, field, d[field])\n\n    def read_info_from_repo(self):\n        if self.is_builtin or self.have_info_from_repo:\n            return\n\n        def read_from_repo():\n            with self.lock:\n                if self.have_info_from_repo:\n                    return\n\n                self.do_read_info_from_repo()\n\n                return self.to_dict()\n\n        try:\n            d = cache.cached_data_for_file('extensions-git', self.name, os.path.join(self.path, \".git\"), read_from_repo)\n            self.from_dict(d)\n        except FileNotFoundError:\n            pass\n        self.status = 'unknown' if self.status == '' else self.status\n\n    def do_read_info_from_repo(self):\n        repo = None\n        try:\n            if os.path.exists(os.path.join(self.path, \".git\")):\n                repo = Repo(self.path)\n        except Exception:\n            errors.report(f\"Error reading github repository info from {self.path}\", exc_info=True)\n\n        if repo is None or repo.bare:\n            self.remote = None\n        else:\n            try:\n                self.remote = next(repo.remote().urls, None)\n                commit = repo.head.commit\n                self.commit_date = commit.committed_date\n                if repo.active_branch:\n                    self.branch = repo.active_branch.name\n                self.commit_hash = commit.hexsha\n                self.version = self.commit_hash[:8]\n\n            except Exception:\n                errors.report(f\"Failed reading extension data from Git repository ({self.name})\", exc_info=True)\n                self.remote = None\n\n        self.have_info_from_repo = True\n\n    def list_files(self, subdir, extension):\n        dirpath = os.path.join(self.path, subdir)\n        if not os.path.isdir(dirpath):\n            return []\n\n        res = []\n        for filename in sorted(os.listdir(dirpath)):\n            res.append(scripts.ScriptFile(self.path, filename, os.path.join(dirpath, filename)))\n\n        res = [x for x in res if os.path.splitext(x.path)[1].lower() == extension and os.path.isfile(x.path)]\n\n        return res\n\n    def check_updates(self):\n        repo = Repo(self.path)\n        for fetch in repo.remote().fetch(dry_run=True):\n            if self.branch and fetch.name != f'{repo.remote().name}/{self.branch}':\n                continue\n            if fetch.flags != fetch.HEAD_UPTODATE:\n                self.can_update = True\n                self.status = \"new commits\"\n                return\n\n        try:\n            origin = repo.rev_parse('origin')\n            if repo.head.commit != origin:\n                self.can_update = True\n                self.status = \"behind HEAD\"\n                return\n        except Exception:\n            self.can_update = False\n            self.status = \"unknown (remote error)\"\n            return\n\n        self.can_update = False\n        self.status = \"latest\"\n\n    def fetch_and_reset_hard(self, commit='origin'):\n        repo = Repo(self.path)\n        # Fix: `error: Your local changes to the following files would be overwritten by merge`,\n        # because WSL2 Docker set 755 file permissions instead of 644, this results to the error.\n        repo.git.fetch(all=True)\n        repo.git.reset(commit, hard=True)\n        self.have_info_from_repo = False\n\n\ndef list_extensions():\n    extensions.clear()\n    extension_paths.clear()\n    loaded_extensions.clear()\n\n    if shared.cmd_opts.disable_all_extensions:\n        print(\"*** \\\"--disable-all-extensions\\\" arg was used, will not load any extensions ***\")\n    elif shared.opts.disable_all_extensions == \"all\":\n        print(\"*** \\\"Disable all extensions\\\" option was set, will not load any extensions ***\")\n    elif shared.cmd_opts.disable_extra_extensions:\n        print(\"*** \\\"--disable-extra-extensions\\\" arg was used, will only load built-in extensions ***\")\n    elif shared.opts.disable_all_extensions == \"extra\":\n        print(\"*** \\\"Disable all extensions\\\" option was set, will only load built-in extensions ***\")\n\n\n    # scan through extensions directory and load metadata\n    for dirname in [extensions_builtin_dir, extensions_dir]:\n        if not os.path.isdir(dirname):\n            continue\n\n        for extension_dirname in sorted(os.listdir(dirname)):\n            path = os.path.join(dirname, extension_dirname)\n            if not os.path.isdir(path):\n                continue\n\n            canonical_name = extension_dirname\n            metadata = ExtensionMetadata(path, canonical_name)\n\n            # check for duplicated canonical names\n            already_loaded_extension = loaded_extensions.get(metadata.canonical_name)\n            if already_loaded_extension is not None:\n                errors.report(f'Duplicate canonical name \"{canonical_name}\" found in extensions \"{extension_dirname}\" and \"{already_loaded_extension.name}\". Former will be discarded.', exc_info=False)\n                continue\n\n            is_builtin = dirname == extensions_builtin_dir\n            extension = Extension(name=extension_dirname, path=path, enabled=extension_dirname not in shared.opts.disabled_extensions, is_builtin=is_builtin, metadata=metadata)\n            extensions.append(extension)\n            extension_paths[extension.path] = extension\n            loaded_extensions[canonical_name] = extension\n\n    for extension in extensions:\n        extension.metadata.requires = extension.metadata.get_script_requirements(\"Requires\", \"Extension\")\n\n    # check for requirements\n    for extension in extensions:\n        if not extension.enabled:\n            continue\n\n        for req in extension.metadata.requires:\n            required_extension = loaded_extensions.get(req)\n            if required_extension is None:\n                errors.report(f'Extension \"{extension.name}\" requires \"{req}\" which is not installed.', exc_info=False)\n                continue\n\n            if not required_extension.enabled:\n                errors.report(f'Extension \"{extension.name}\" requires \"{required_extension.name}\" which is disabled.', exc_info=False)\n                continue\n\n\ndef find_extension(filename):\n    parentdir = os.path.dirname(os.path.realpath(filename))\n\n    while parentdir != filename:\n        extension = extension_paths.get(parentdir)\n        if extension is not None:\n            return extension\n\n        filename = parentdir\n        parentdir = os.path.dirname(filename)\n\n    return None\n\n", "modules/util.py": "import os\nimport re\n\nfrom modules import shared\nfrom modules.paths_internal import script_path, cwd\n\n\ndef natural_sort_key(s, regex=re.compile('([0-9]+)')):\n    return [int(text) if text.isdigit() else text.lower() for text in regex.split(s)]\n\n\ndef listfiles(dirname):\n    filenames = [os.path.join(dirname, x) for x in sorted(os.listdir(dirname), key=natural_sort_key) if not x.startswith(\".\")]\n    return [file for file in filenames if os.path.isfile(file)]\n\n\ndef html_path(filename):\n    return os.path.join(script_path, \"html\", filename)\n\n\ndef html(filename):\n    path = html_path(filename)\n\n    try:\n        with open(path, encoding=\"utf8\") as file:\n            return file.read()\n    except OSError:\n        return \"\"\n\n\ndef walk_files(path, allowed_extensions=None):\n    if not os.path.exists(path):\n        return\n\n    if allowed_extensions is not None:\n        allowed_extensions = set(allowed_extensions)\n\n    items = list(os.walk(path, followlinks=True))\n    items = sorted(items, key=lambda x: natural_sort_key(x[0]))\n\n    for root, _, files in items:\n        for filename in sorted(files, key=natural_sort_key):\n            if allowed_extensions is not None:\n                _, ext = os.path.splitext(filename)\n                if ext.lower() not in allowed_extensions:\n                    continue\n\n            if not shared.opts.list_hidden_files and (\"/.\" in root or \"\\\\.\" in root):\n                continue\n\n            yield os.path.join(root, filename)\n\n\ndef ldm_print(*args, **kwargs):\n    if shared.opts.hide_ldm_prints:\n        return\n\n    print(*args, **kwargs)\n\n\ndef truncate_path(target_path, base_path=cwd):\n    abs_target, abs_base = os.path.abspath(target_path), os.path.abspath(base_path)\n    try:\n        if os.path.commonpath([abs_target, abs_base]) == abs_base:\n            return os.path.relpath(abs_target, abs_base)\n    except ValueError:\n        pass\n    return abs_target\n\n\nclass MassFileListerCachedDir:\n    \"\"\"A class that caches file metadata for a specific directory.\"\"\"\n\n    def __init__(self, dirname):\n        self.files = None\n        self.files_cased = None\n        self.dirname = dirname\n\n        stats = ((x.name, x.stat(follow_symlinks=False)) for x in os.scandir(self.dirname))\n        files = [(n, s.st_mtime, s.st_ctime) for n, s in stats]\n        self.files = {x[0].lower(): x for x in files}\n        self.files_cased = {x[0]: x for x in files}\n\n    def update_entry(self, filename):\n        \"\"\"Add a file to the cache\"\"\"\n        file_path = os.path.join(self.dirname, filename)\n        try:\n            stat = os.stat(file_path)\n            entry = (filename, stat.st_mtime, stat.st_ctime)\n            self.files[filename.lower()] = entry\n            self.files_cased[filename] = entry\n        except FileNotFoundError as e:\n            print(f'MassFileListerCachedDir.add_entry: \"{file_path}\" {e}')\n\n\nclass MassFileLister:\n    \"\"\"A class that provides a way to check for the existence and mtime/ctile of files without doing more than one stat call per file.\"\"\"\n\n    def __init__(self):\n        self.cached_dirs = {}\n\n    def find(self, path):\n        \"\"\"\n        Find the metadata for a file at the given path.\n\n        Returns:\n            tuple or None: A tuple of (name, mtime, ctime) if the file exists, or None if it does not.\n        \"\"\"\n\n        dirname, filename = os.path.split(path)\n\n        cached_dir = self.cached_dirs.get(dirname)\n        if cached_dir is None:\n            cached_dir = MassFileListerCachedDir(dirname)\n            self.cached_dirs[dirname] = cached_dir\n\n        stats = cached_dir.files_cased.get(filename)\n        if stats is not None:\n            return stats\n\n        stats = cached_dir.files.get(filename.lower())\n        if stats is None:\n            return None\n\n        try:\n            os_stats = os.stat(path, follow_symlinks=False)\n            return filename, os_stats.st_mtime, os_stats.st_ctime\n        except Exception:\n            return None\n\n    def exists(self, path):\n        \"\"\"Check if a file exists at the given path.\"\"\"\n\n        return self.find(path) is not None\n\n    def mctime(self, path):\n        \"\"\"\n        Get the modification and creation times for a file at the given path.\n\n        Returns:\n            tuple: A tuple of (mtime, ctime) if the file exists, or (0, 0) if it does not.\n        \"\"\"\n\n        stats = self.find(path)\n        return (0, 0) if stats is None else stats[1:3]\n\n    def reset(self):\n        \"\"\"Clear the cache of all directories.\"\"\"\n        self.cached_dirs.clear()\n\n    def update_file_entry(self, path):\n        \"\"\"Update the cache for a specific directory.\"\"\"\n        dirname, filename = os.path.split(path)\n        if cached_dir := self.cached_dirs.get(dirname):\n            cached_dir.update_entry(filename)\n\ndef topological_sort(dependencies):\n    \"\"\"Accepts a dictionary mapping name to its dependencies, returns a list of names ordered according to dependencies.\n    Ignores errors relating to missing dependeencies or circular dependencies\n    \"\"\"\n\n    visited = {}\n    result = []\n\n    def inner(name):\n        visited[name] = True\n\n        for dep in dependencies.get(name, []):\n            if dep in dependencies and dep not in visited:\n                inner(dep)\n\n        result.append(name)\n\n    for depname in dependencies:\n        if depname not in visited:\n            inner(depname)\n\n    return result\n\n\ndef open_folder(path):\n    \"\"\"Open a folder in the file manager of the respect OS.\"\"\"\n    # import at function level to avoid potential issues\n    import gradio as gr\n    import platform\n    import sys\n    import subprocess\n\n    if not os.path.exists(path):\n        msg = f'Folder \"{path}\" does not exist. after you save an image, the folder will be created.'\n        print(msg)\n        gr.Info(msg)\n        return\n    elif not os.path.isdir(path):\n        msg = f\"\"\"\nWARNING\nAn open_folder request was made with an path that is not a folder.\nThis could be an error or a malicious attempt to run code on your computer.\nRequested path was: {path}\n\"\"\"\n        print(msg, file=sys.stderr)\n        gr.Warning(msg)\n        return\n\n    path = os.path.normpath(path)\n    if platform.system() == \"Windows\":\n        os.startfile(path)\n    elif platform.system() == \"Darwin\":\n        subprocess.Popen([\"open\", path])\n    elif \"microsoft-standard-WSL2\" in platform.uname().release:\n        subprocess.Popen([\"wsl-open\", path])\n    else:\n        subprocess.Popen([\"xdg-open\", path])\n", "modules/deepbooru_model.py": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nfrom modules import devices\n\n# see https://github.com/AUTOMATIC1111/TorchDeepDanbooru for more\n\n\nclass DeepDanbooruModel(nn.Module):\n    def __init__(self):\n        super(DeepDanbooruModel, self).__init__()\n\n        self.tags = []\n\n        self.n_Conv_0 = nn.Conv2d(kernel_size=(7, 7), in_channels=3, out_channels=64, stride=(2, 2))\n        self.n_MaxPool_0 = nn.MaxPool2d(kernel_size=(3, 3), stride=(2, 2))\n        self.n_Conv_1 = nn.Conv2d(kernel_size=(1, 1), in_channels=64, out_channels=256)\n        self.n_Conv_2 = nn.Conv2d(kernel_size=(1, 1), in_channels=64, out_channels=64)\n        self.n_Conv_3 = nn.Conv2d(kernel_size=(3, 3), in_channels=64, out_channels=64)\n        self.n_Conv_4 = nn.Conv2d(kernel_size=(1, 1), in_channels=64, out_channels=256)\n        self.n_Conv_5 = nn.Conv2d(kernel_size=(1, 1), in_channels=256, out_channels=64)\n        self.n_Conv_6 = nn.Conv2d(kernel_size=(3, 3), in_channels=64, out_channels=64)\n        self.n_Conv_7 = nn.Conv2d(kernel_size=(1, 1), in_channels=64, out_channels=256)\n        self.n_Conv_8 = nn.Conv2d(kernel_size=(1, 1), in_channels=256, out_channels=64)\n        self.n_Conv_9 = nn.Conv2d(kernel_size=(3, 3), in_channels=64, out_channels=64)\n        self.n_Conv_10 = nn.Conv2d(kernel_size=(1, 1), in_channels=64, out_channels=256)\n        self.n_Conv_11 = nn.Conv2d(kernel_size=(1, 1), in_channels=256, out_channels=512, stride=(2, 2))\n        self.n_Conv_12 = nn.Conv2d(kernel_size=(1, 1), in_channels=256, out_channels=128)\n        self.n_Conv_13 = nn.Conv2d(kernel_size=(3, 3), in_channels=128, out_channels=128, stride=(2, 2))\n        self.n_Conv_14 = nn.Conv2d(kernel_size=(1, 1), in_channels=128, out_channels=512)\n        self.n_Conv_15 = nn.Conv2d(kernel_size=(1, 1), in_channels=512, out_channels=128)\n        self.n_Conv_16 = nn.Conv2d(kernel_size=(3, 3), in_channels=128, out_channels=128)\n        self.n_Conv_17 = nn.Conv2d(kernel_size=(1, 1), in_channels=128, out_channels=512)\n        self.n_Conv_18 = nn.Conv2d(kernel_size=(1, 1), in_channels=512, out_channels=128)\n        self.n_Conv_19 = nn.Conv2d(kernel_size=(3, 3), in_channels=128, out_channels=128)\n        self.n_Conv_20 = nn.Conv2d(kernel_size=(1, 1), in_channels=128, out_channels=512)\n        self.n_Conv_21 = nn.Conv2d(kernel_size=(1, 1), in_channels=512, out_channels=128)\n        self.n_Conv_22 = nn.Conv2d(kernel_size=(3, 3), in_channels=128, out_channels=128)\n        self.n_Conv_23 = nn.Conv2d(kernel_size=(1, 1), in_channels=128, out_channels=512)\n        self.n_Conv_24 = nn.Conv2d(kernel_size=(1, 1), in_channels=512, out_channels=128)\n        self.n_Conv_25 = nn.Conv2d(kernel_size=(3, 3), in_channels=128, out_channels=128)\n        self.n_Conv_26 = nn.Conv2d(kernel_size=(1, 1), in_channels=128, out_channels=512)\n        self.n_Conv_27 = nn.Conv2d(kernel_size=(1, 1), in_channels=512, out_channels=128)\n        self.n_Conv_28 = nn.Conv2d(kernel_size=(3, 3), in_channels=128, out_channels=128)\n        self.n_Conv_29 = nn.Conv2d(kernel_size=(1, 1), in_channels=128, out_channels=512)\n        self.n_Conv_30 = nn.Conv2d(kernel_size=(1, 1), in_channels=512, out_channels=128)\n        self.n_Conv_31 = nn.Conv2d(kernel_size=(3, 3), in_channels=128, out_channels=128)\n        self.n_Conv_32 = nn.Conv2d(kernel_size=(1, 1), in_channels=128, out_channels=512)\n        self.n_Conv_33 = nn.Conv2d(kernel_size=(1, 1), in_channels=512, out_channels=128)\n        self.n_Conv_34 = nn.Conv2d(kernel_size=(3, 3), in_channels=128, out_channels=128)\n        self.n_Conv_35 = nn.Conv2d(kernel_size=(1, 1), in_channels=128, out_channels=512)\n        self.n_Conv_36 = nn.Conv2d(kernel_size=(1, 1), in_channels=512, out_channels=1024, stride=(2, 2))\n        self.n_Conv_37 = nn.Conv2d(kernel_size=(1, 1), in_channels=512, out_channels=256)\n        self.n_Conv_38 = nn.Conv2d(kernel_size=(3, 3), in_channels=256, out_channels=256, stride=(2, 2))\n        self.n_Conv_39 = nn.Conv2d(kernel_size=(1, 1), in_channels=256, out_channels=1024)\n        self.n_Conv_40 = nn.Conv2d(kernel_size=(1, 1), in_channels=1024, out_channels=256)\n        self.n_Conv_41 = nn.Conv2d(kernel_size=(3, 3), in_channels=256, out_channels=256)\n        self.n_Conv_42 = nn.Conv2d(kernel_size=(1, 1), in_channels=256, out_channels=1024)\n        self.n_Conv_43 = nn.Conv2d(kernel_size=(1, 1), in_channels=1024, out_channels=256)\n        self.n_Conv_44 = nn.Conv2d(kernel_size=(3, 3), in_channels=256, out_channels=256)\n        self.n_Conv_45 = nn.Conv2d(kernel_size=(1, 1), in_channels=256, out_channels=1024)\n        self.n_Conv_46 = nn.Conv2d(kernel_size=(1, 1), in_channels=1024, out_channels=256)\n        self.n_Conv_47 = nn.Conv2d(kernel_size=(3, 3), in_channels=256, out_channels=256)\n        self.n_Conv_48 = nn.Conv2d(kernel_size=(1, 1), in_channels=256, out_channels=1024)\n        self.n_Conv_49 = nn.Conv2d(kernel_size=(1, 1), in_channels=1024, out_channels=256)\n        self.n_Conv_50 = nn.Conv2d(kernel_size=(3, 3), in_channels=256, out_channels=256)\n        self.n_Conv_51 = nn.Conv2d(kernel_size=(1, 1), in_channels=256, out_channels=1024)\n        self.n_Conv_52 = nn.Conv2d(kernel_size=(1, 1), in_channels=1024, out_channels=256)\n        self.n_Conv_53 = nn.Conv2d(kernel_size=(3, 3), in_channels=256, out_channels=256)\n        self.n_Conv_54 = nn.Conv2d(kernel_size=(1, 1), in_channels=256, out_channels=1024)\n        self.n_Conv_55 = nn.Conv2d(kernel_size=(1, 1), in_channels=1024, out_channels=256)\n        self.n_Conv_56 = nn.Conv2d(kernel_size=(3, 3), in_channels=256, out_channels=256)\n        self.n_Conv_57 = nn.Conv2d(kernel_size=(1, 1), in_channels=256, out_channels=1024)\n        self.n_Conv_58 = nn.Conv2d(kernel_size=(1, 1), in_channels=1024, out_channels=256)\n        self.n_Conv_59 = nn.Conv2d(kernel_size=(3, 3), in_channels=256, out_channels=256)\n        self.n_Conv_60 = nn.Conv2d(kernel_size=(1, 1), in_channels=256, out_channels=1024)\n        self.n_Conv_61 = nn.Conv2d(kernel_size=(1, 1), in_channels=1024, out_channels=256)\n        self.n_Conv_62 = nn.Conv2d(kernel_size=(3, 3), in_channels=256, out_channels=256)\n        self.n_Conv_63 = nn.Conv2d(kernel_size=(1, 1), in_channels=256, out_channels=1024)\n        self.n_Conv_64 = nn.Conv2d(kernel_size=(1, 1), in_channels=1024, out_channels=256)\n        self.n_Conv_65 = nn.Conv2d(kernel_size=(3, 3), in_channels=256, out_channels=256)\n        self.n_Conv_66 = nn.Conv2d(kernel_size=(1, 1), in_channels=256, out_channels=1024)\n        self.n_Conv_67 = nn.Conv2d(kernel_size=(1, 1), in_channels=1024, out_channels=256)\n        self.n_Conv_68 = nn.Conv2d(kernel_size=(3, 3), in_channels=256, out_channels=256)\n        self.n_Conv_69 = nn.Conv2d(kernel_size=(1, 1), in_channels=256, out_channels=1024)\n        self.n_Conv_70 = nn.Conv2d(kernel_size=(1, 1), in_channels=1024, out_channels=256)\n        self.n_Conv_71 = nn.Conv2d(kernel_size=(3, 3), in_channels=256, out_channels=256)\n        self.n_Conv_72 = nn.Conv2d(kernel_size=(1, 1), in_channels=256, out_channels=1024)\n        self.n_Conv_73 = nn.Conv2d(kernel_size=(1, 1), in_channels=1024, out_channels=256)\n        self.n_Conv_74 = nn.Conv2d(kernel_size=(3, 3), in_channels=256, out_channels=256)\n        self.n_Conv_75 = nn.Conv2d(kernel_size=(1, 1), in_channels=256, out_channels=1024)\n        self.n_Conv_76 = nn.Conv2d(kernel_size=(1, 1), in_channels=1024, out_channels=256)\n        self.n_Conv_77 = nn.Conv2d(kernel_size=(3, 3), in_channels=256, out_channels=256)\n        self.n_Conv_78 = nn.Conv2d(kernel_size=(1, 1), in_channels=256, out_channels=1024)\n        self.n_Conv_79 = nn.Conv2d(kernel_size=(1, 1), in_channels=1024, out_channels=256)\n        self.n_Conv_80 = nn.Conv2d(kernel_size=(3, 3), in_channels=256, out_channels=256)\n        self.n_Conv_81 = nn.Conv2d(kernel_size=(1, 1), in_channels=256, out_channels=1024)\n        self.n_Conv_82 = nn.Conv2d(kernel_size=(1, 1), in_channels=1024, out_channels=256)\n        self.n_Conv_83 = nn.Conv2d(kernel_size=(3, 3), in_channels=256, out_channels=256)\n        self.n_Conv_84 = nn.Conv2d(kernel_size=(1, 1), in_channels=256, out_channels=1024)\n        self.n_Conv_85 = nn.Conv2d(kernel_size=(1, 1), in_channels=1024, out_channels=256)\n        self.n_Conv_86 = nn.Conv2d(kernel_size=(3, 3), in_channels=256, out_channels=256)\n        self.n_Conv_87 = nn.Conv2d(kernel_size=(1, 1), in_channels=256, out_channels=1024)\n        self.n_Conv_88 = nn.Conv2d(kernel_size=(1, 1), in_channels=1024, out_channels=256)\n        self.n_Conv_89 = nn.Conv2d(kernel_size=(3, 3), in_channels=256, out_channels=256)\n        self.n_Conv_90 = nn.Conv2d(kernel_size=(1, 1), in_channels=256, out_channels=1024)\n        self.n_Conv_91 = nn.Conv2d(kernel_size=(1, 1), in_channels=1024, out_channels=256)\n        self.n_Conv_92 = nn.Conv2d(kernel_size=(3, 3), in_channels=256, out_channels=256)\n        self.n_Conv_93 = nn.Conv2d(kernel_size=(1, 1), in_channels=256, out_channels=1024)\n        self.n_Conv_94 = nn.Conv2d(kernel_size=(1, 1), in_channels=1024, out_channels=256)\n        self.n_Conv_95 = nn.Conv2d(kernel_size=(3, 3), in_channels=256, out_channels=256)\n        self.n_Conv_96 = nn.Conv2d(kernel_size=(1, 1), in_channels=256, out_channels=1024)\n        self.n_Conv_97 = nn.Conv2d(kernel_size=(1, 1), in_channels=1024, out_channels=256)\n        self.n_Conv_98 = nn.Conv2d(kernel_size=(3, 3), in_channels=256, out_channels=256, stride=(2, 2))\n        self.n_Conv_99 = nn.Conv2d(kernel_size=(1, 1), in_channels=256, out_channels=1024)\n        self.n_Conv_100 = nn.Conv2d(kernel_size=(1, 1), in_channels=1024, out_channels=1024, stride=(2, 2))\n        self.n_Conv_101 = nn.Conv2d(kernel_size=(1, 1), in_channels=1024, out_channels=256)\n        self.n_Conv_102 = nn.Conv2d(kernel_size=(3, 3), in_channels=256, out_channels=256)\n        self.n_Conv_103 = nn.Conv2d(kernel_size=(1, 1), in_channels=256, out_channels=1024)\n        self.n_Conv_104 = nn.Conv2d(kernel_size=(1, 1), in_channels=1024, out_channels=256)\n        self.n_Conv_105 = nn.Conv2d(kernel_size=(3, 3), in_channels=256, out_channels=256)\n        self.n_Conv_106 = nn.Conv2d(kernel_size=(1, 1), in_channels=256, out_channels=1024)\n        self.n_Conv_107 = nn.Conv2d(kernel_size=(1, 1), in_channels=1024, out_channels=256)\n        self.n_Conv_108 = nn.Conv2d(kernel_size=(3, 3), in_channels=256, out_channels=256)\n        self.n_Conv_109 = nn.Conv2d(kernel_size=(1, 1), in_channels=256, out_channels=1024)\n        self.n_Conv_110 = nn.Conv2d(kernel_size=(1, 1), in_channels=1024, out_channels=256)\n        self.n_Conv_111 = nn.Conv2d(kernel_size=(3, 3), in_channels=256, out_channels=256)\n        self.n_Conv_112 = nn.Conv2d(kernel_size=(1, 1), in_channels=256, out_channels=1024)\n        self.n_Conv_113 = nn.Conv2d(kernel_size=(1, 1), in_channels=1024, out_channels=256)\n        self.n_Conv_114 = nn.Conv2d(kernel_size=(3, 3), in_channels=256, out_channels=256)\n        self.n_Conv_115 = nn.Conv2d(kernel_size=(1, 1), in_channels=256, out_channels=1024)\n        self.n_Conv_116 = nn.Conv2d(kernel_size=(1, 1), in_channels=1024, out_channels=256)\n        self.n_Conv_117 = nn.Conv2d(kernel_size=(3, 3), in_channels=256, out_channels=256)\n        self.n_Conv_118 = nn.Conv2d(kernel_size=(1, 1), in_channels=256, out_channels=1024)\n        self.n_Conv_119 = nn.Conv2d(kernel_size=(1, 1), in_channels=1024, out_channels=256)\n        self.n_Conv_120 = nn.Conv2d(kernel_size=(3, 3), in_channels=256, out_channels=256)\n        self.n_Conv_121 = nn.Conv2d(kernel_size=(1, 1), in_channels=256, out_channels=1024)\n        self.n_Conv_122 = nn.Conv2d(kernel_size=(1, 1), in_channels=1024, out_channels=256)\n        self.n_Conv_123 = nn.Conv2d(kernel_size=(3, 3), in_channels=256, out_channels=256)\n        self.n_Conv_124 = nn.Conv2d(kernel_size=(1, 1), in_channels=256, out_channels=1024)\n        self.n_Conv_125 = nn.Conv2d(kernel_size=(1, 1), in_channels=1024, out_channels=256)\n        self.n_Conv_126 = nn.Conv2d(kernel_size=(3, 3), in_channels=256, out_channels=256)\n        self.n_Conv_127 = nn.Conv2d(kernel_size=(1, 1), in_channels=256, out_channels=1024)\n        self.n_Conv_128 = nn.Conv2d(kernel_size=(1, 1), in_channels=1024, out_channels=256)\n        self.n_Conv_129 = nn.Conv2d(kernel_size=(3, 3), in_channels=256, out_channels=256)\n        self.n_Conv_130 = nn.Conv2d(kernel_size=(1, 1), in_channels=256, out_channels=1024)\n        self.n_Conv_131 = nn.Conv2d(kernel_size=(1, 1), in_channels=1024, out_channels=256)\n        self.n_Conv_132 = nn.Conv2d(kernel_size=(3, 3), in_channels=256, out_channels=256)\n        self.n_Conv_133 = nn.Conv2d(kernel_size=(1, 1), in_channels=256, out_channels=1024)\n        self.n_Conv_134 = nn.Conv2d(kernel_size=(1, 1), in_channels=1024, out_channels=256)\n        self.n_Conv_135 = nn.Conv2d(kernel_size=(3, 3), in_channels=256, out_channels=256)\n        self.n_Conv_136 = nn.Conv2d(kernel_size=(1, 1), in_channels=256, out_channels=1024)\n        self.n_Conv_137 = nn.Conv2d(kernel_size=(1, 1), in_channels=1024, out_channels=256)\n        self.n_Conv_138 = nn.Conv2d(kernel_size=(3, 3), in_channels=256, out_channels=256)\n        self.n_Conv_139 = nn.Conv2d(kernel_size=(1, 1), in_channels=256, out_channels=1024)\n        self.n_Conv_140 = nn.Conv2d(kernel_size=(1, 1), in_channels=1024, out_channels=256)\n        self.n_Conv_141 = nn.Conv2d(kernel_size=(3, 3), in_channels=256, out_channels=256)\n        self.n_Conv_142 = nn.Conv2d(kernel_size=(1, 1), in_channels=256, out_channels=1024)\n        self.n_Conv_143 = nn.Conv2d(kernel_size=(1, 1), in_channels=1024, out_channels=256)\n        self.n_Conv_144 = nn.Conv2d(kernel_size=(3, 3), in_channels=256, out_channels=256)\n        self.n_Conv_145 = nn.Conv2d(kernel_size=(1, 1), in_channels=256, out_channels=1024)\n        self.n_Conv_146 = nn.Conv2d(kernel_size=(1, 1), in_channels=1024, out_channels=256)\n        self.n_Conv_147 = nn.Conv2d(kernel_size=(3, 3), in_channels=256, out_channels=256)\n        self.n_Conv_148 = nn.Conv2d(kernel_size=(1, 1), in_channels=256, out_channels=1024)\n        self.n_Conv_149 = nn.Conv2d(kernel_size=(1, 1), in_channels=1024, out_channels=256)\n        self.n_Conv_150 = nn.Conv2d(kernel_size=(3, 3), in_channels=256, out_channels=256)\n        self.n_Conv_151 = nn.Conv2d(kernel_size=(1, 1), in_channels=256, out_channels=1024)\n        self.n_Conv_152 = nn.Conv2d(kernel_size=(1, 1), in_channels=1024, out_channels=256)\n        self.n_Conv_153 = nn.Conv2d(kernel_size=(3, 3), in_channels=256, out_channels=256)\n        self.n_Conv_154 = nn.Conv2d(kernel_size=(1, 1), in_channels=256, out_channels=1024)\n        self.n_Conv_155 = nn.Conv2d(kernel_size=(1, 1), in_channels=1024, out_channels=256)\n        self.n_Conv_156 = nn.Conv2d(kernel_size=(3, 3), in_channels=256, out_channels=256)\n        self.n_Conv_157 = nn.Conv2d(kernel_size=(1, 1), in_channels=256, out_channels=1024)\n        self.n_Conv_158 = nn.Conv2d(kernel_size=(1, 1), in_channels=1024, out_channels=2048, stride=(2, 2))\n        self.n_Conv_159 = nn.Conv2d(kernel_size=(1, 1), in_channels=1024, out_channels=512)\n        self.n_Conv_160 = nn.Conv2d(kernel_size=(3, 3), in_channels=512, out_channels=512, stride=(2, 2))\n        self.n_Conv_161 = nn.Conv2d(kernel_size=(1, 1), in_channels=512, out_channels=2048)\n        self.n_Conv_162 = nn.Conv2d(kernel_size=(1, 1), in_channels=2048, out_channels=512)\n        self.n_Conv_163 = nn.Conv2d(kernel_size=(3, 3), in_channels=512, out_channels=512)\n        self.n_Conv_164 = nn.Conv2d(kernel_size=(1, 1), in_channels=512, out_channels=2048)\n        self.n_Conv_165 = nn.Conv2d(kernel_size=(1, 1), in_channels=2048, out_channels=512)\n        self.n_Conv_166 = nn.Conv2d(kernel_size=(3, 3), in_channels=512, out_channels=512)\n        self.n_Conv_167 = nn.Conv2d(kernel_size=(1, 1), in_channels=512, out_channels=2048)\n        self.n_Conv_168 = nn.Conv2d(kernel_size=(1, 1), in_channels=2048, out_channels=4096, stride=(2, 2))\n        self.n_Conv_169 = nn.Conv2d(kernel_size=(1, 1), in_channels=2048, out_channels=1024)\n        self.n_Conv_170 = nn.Conv2d(kernel_size=(3, 3), in_channels=1024, out_channels=1024, stride=(2, 2))\n        self.n_Conv_171 = nn.Conv2d(kernel_size=(1, 1), in_channels=1024, out_channels=4096)\n        self.n_Conv_172 = nn.Conv2d(kernel_size=(1, 1), in_channels=4096, out_channels=1024)\n        self.n_Conv_173 = nn.Conv2d(kernel_size=(3, 3), in_channels=1024, out_channels=1024)\n        self.n_Conv_174 = nn.Conv2d(kernel_size=(1, 1), in_channels=1024, out_channels=4096)\n        self.n_Conv_175 = nn.Conv2d(kernel_size=(1, 1), in_channels=4096, out_channels=1024)\n        self.n_Conv_176 = nn.Conv2d(kernel_size=(3, 3), in_channels=1024, out_channels=1024)\n        self.n_Conv_177 = nn.Conv2d(kernel_size=(1, 1), in_channels=1024, out_channels=4096)\n        self.n_Conv_178 = nn.Conv2d(kernel_size=(1, 1), in_channels=4096, out_channels=9176, bias=False)\n\n    def forward(self, *inputs):\n        t_358, = inputs\n        t_359 = t_358.permute(*[0, 3, 1, 2])\n        t_359_padded = F.pad(t_359, [2, 3, 2, 3], value=0)\n        t_360 = self.n_Conv_0(t_359_padded.to(self.n_Conv_0.bias.dtype) if devices.unet_needs_upcast else t_359_padded)\n        t_361 = F.relu(t_360)\n        t_361 = F.pad(t_361, [0, 1, 0, 1], value=float('-inf'))\n        t_362 = self.n_MaxPool_0(t_361)\n        t_363 = self.n_Conv_1(t_362)\n        t_364 = self.n_Conv_2(t_362)\n        t_365 = F.relu(t_364)\n        t_365_padded = F.pad(t_365, [1, 1, 1, 1], value=0)\n        t_366 = self.n_Conv_3(t_365_padded)\n        t_367 = F.relu(t_366)\n        t_368 = self.n_Conv_4(t_367)\n        t_369 = torch.add(t_368, t_363)\n        t_370 = F.relu(t_369)\n        t_371 = self.n_Conv_5(t_370)\n        t_372 = F.relu(t_371)\n        t_372_padded = F.pad(t_372, [1, 1, 1, 1], value=0)\n        t_373 = self.n_Conv_6(t_372_padded)\n        t_374 = F.relu(t_373)\n        t_375 = self.n_Conv_7(t_374)\n        t_376 = torch.add(t_375, t_370)\n        t_377 = F.relu(t_376)\n        t_378 = self.n_Conv_8(t_377)\n        t_379 = F.relu(t_378)\n        t_379_padded = F.pad(t_379, [1, 1, 1, 1], value=0)\n        t_380 = self.n_Conv_9(t_379_padded)\n        t_381 = F.relu(t_380)\n        t_382 = self.n_Conv_10(t_381)\n        t_383 = torch.add(t_382, t_377)\n        t_384 = F.relu(t_383)\n        t_385 = self.n_Conv_11(t_384)\n        t_386 = self.n_Conv_12(t_384)\n        t_387 = F.relu(t_386)\n        t_387_padded = F.pad(t_387, [0, 1, 0, 1], value=0)\n        t_388 = self.n_Conv_13(t_387_padded)\n        t_389 = F.relu(t_388)\n        t_390 = self.n_Conv_14(t_389)\n        t_391 = torch.add(t_390, t_385)\n        t_392 = F.relu(t_391)\n        t_393 = self.n_Conv_15(t_392)\n        t_394 = F.relu(t_393)\n        t_394_padded = F.pad(t_394, [1, 1, 1, 1], value=0)\n        t_395 = self.n_Conv_16(t_394_padded)\n        t_396 = F.relu(t_395)\n        t_397 = self.n_Conv_17(t_396)\n        t_398 = torch.add(t_397, t_392)\n        t_399 = F.relu(t_398)\n        t_400 = self.n_Conv_18(t_399)\n        t_401 = F.relu(t_400)\n        t_401_padded = F.pad(t_401, [1, 1, 1, 1], value=0)\n        t_402 = self.n_Conv_19(t_401_padded)\n        t_403 = F.relu(t_402)\n        t_404 = self.n_Conv_20(t_403)\n        t_405 = torch.add(t_404, t_399)\n        t_406 = F.relu(t_405)\n        t_407 = self.n_Conv_21(t_406)\n        t_408 = F.relu(t_407)\n        t_408_padded = F.pad(t_408, [1, 1, 1, 1], value=0)\n        t_409 = self.n_Conv_22(t_408_padded)\n        t_410 = F.relu(t_409)\n        t_411 = self.n_Conv_23(t_410)\n        t_412 = torch.add(t_411, t_406)\n        t_413 = F.relu(t_412)\n        t_414 = self.n_Conv_24(t_413)\n        t_415 = F.relu(t_414)\n        t_415_padded = F.pad(t_415, [1, 1, 1, 1], value=0)\n        t_416 = self.n_Conv_25(t_415_padded)\n        t_417 = F.relu(t_416)\n        t_418 = self.n_Conv_26(t_417)\n        t_419 = torch.add(t_418, t_413)\n        t_420 = F.relu(t_419)\n        t_421 = self.n_Conv_27(t_420)\n        t_422 = F.relu(t_421)\n        t_422_padded = F.pad(t_422, [1, 1, 1, 1], value=0)\n        t_423 = self.n_Conv_28(t_422_padded)\n        t_424 = F.relu(t_423)\n        t_425 = self.n_Conv_29(t_424)\n        t_426 = torch.add(t_425, t_420)\n        t_427 = F.relu(t_426)\n        t_428 = self.n_Conv_30(t_427)\n        t_429 = F.relu(t_428)\n        t_429_padded = F.pad(t_429, [1, 1, 1, 1], value=0)\n        t_430 = self.n_Conv_31(t_429_padded)\n        t_431 = F.relu(t_430)\n        t_432 = self.n_Conv_32(t_431)\n        t_433 = torch.add(t_432, t_427)\n        t_434 = F.relu(t_433)\n        t_435 = self.n_Conv_33(t_434)\n        t_436 = F.relu(t_435)\n        t_436_padded = F.pad(t_436, [1, 1, 1, 1], value=0)\n        t_437 = self.n_Conv_34(t_436_padded)\n        t_438 = F.relu(t_437)\n        t_439 = self.n_Conv_35(t_438)\n        t_440 = torch.add(t_439, t_434)\n        t_441 = F.relu(t_440)\n        t_442 = self.n_Conv_36(t_441)\n        t_443 = self.n_Conv_37(t_441)\n        t_444 = F.relu(t_443)\n        t_444_padded = F.pad(t_444, [0, 1, 0, 1], value=0)\n        t_445 = self.n_Conv_38(t_444_padded)\n        t_446 = F.relu(t_445)\n        t_447 = self.n_Conv_39(t_446)\n        t_448 = torch.add(t_447, t_442)\n        t_449 = F.relu(t_448)\n        t_450 = self.n_Conv_40(t_449)\n        t_451 = F.relu(t_450)\n        t_451_padded = F.pad(t_451, [1, 1, 1, 1], value=0)\n        t_452 = self.n_Conv_41(t_451_padded)\n        t_453 = F.relu(t_452)\n        t_454 = self.n_Conv_42(t_453)\n        t_455 = torch.add(t_454, t_449)\n        t_456 = F.relu(t_455)\n        t_457 = self.n_Conv_43(t_456)\n        t_458 = F.relu(t_457)\n        t_458_padded = F.pad(t_458, [1, 1, 1, 1], value=0)\n        t_459 = self.n_Conv_44(t_458_padded)\n        t_460 = F.relu(t_459)\n        t_461 = self.n_Conv_45(t_460)\n        t_462 = torch.add(t_461, t_456)\n        t_463 = F.relu(t_462)\n        t_464 = self.n_Conv_46(t_463)\n        t_465 = F.relu(t_464)\n        t_465_padded = F.pad(t_465, [1, 1, 1, 1], value=0)\n        t_466 = self.n_Conv_47(t_465_padded)\n        t_467 = F.relu(t_466)\n        t_468 = self.n_Conv_48(t_467)\n        t_469 = torch.add(t_468, t_463)\n        t_470 = F.relu(t_469)\n        t_471 = self.n_Conv_49(t_470)\n        t_472 = F.relu(t_471)\n        t_472_padded = F.pad(t_472, [1, 1, 1, 1], value=0)\n        t_473 = self.n_Conv_50(t_472_padded)\n        t_474 = F.relu(t_473)\n        t_475 = self.n_Conv_51(t_474)\n        t_476 = torch.add(t_475, t_470)\n        t_477 = F.relu(t_476)\n        t_478 = self.n_Conv_52(t_477)\n        t_479 = F.relu(t_478)\n        t_479_padded = F.pad(t_479, [1, 1, 1, 1], value=0)\n        t_480 = self.n_Conv_53(t_479_padded)\n        t_481 = F.relu(t_480)\n        t_482 = self.n_Conv_54(t_481)\n        t_483 = torch.add(t_482, t_477)\n        t_484 = F.relu(t_483)\n        t_485 = self.n_Conv_55(t_484)\n        t_486 = F.relu(t_485)\n        t_486_padded = F.pad(t_486, [1, 1, 1, 1], value=0)\n        t_487 = self.n_Conv_56(t_486_padded)\n        t_488 = F.relu(t_487)\n        t_489 = self.n_Conv_57(t_488)\n        t_490 = torch.add(t_489, t_484)\n        t_491 = F.relu(t_490)\n        t_492 = self.n_Conv_58(t_491)\n        t_493 = F.relu(t_492)\n        t_493_padded = F.pad(t_493, [1, 1, 1, 1], value=0)\n        t_494 = self.n_Conv_59(t_493_padded)\n        t_495 = F.relu(t_494)\n        t_496 = self.n_Conv_60(t_495)\n        t_497 = torch.add(t_496, t_491)\n        t_498 = F.relu(t_497)\n        t_499 = self.n_Conv_61(t_498)\n        t_500 = F.relu(t_499)\n        t_500_padded = F.pad(t_500, [1, 1, 1, 1], value=0)\n        t_501 = self.n_Conv_62(t_500_padded)\n        t_502 = F.relu(t_501)\n        t_503 = self.n_Conv_63(t_502)\n        t_504 = torch.add(t_503, t_498)\n        t_505 = F.relu(t_504)\n        t_506 = self.n_Conv_64(t_505)\n        t_507 = F.relu(t_506)\n        t_507_padded = F.pad(t_507, [1, 1, 1, 1], value=0)\n        t_508 = self.n_Conv_65(t_507_padded)\n        t_509 = F.relu(t_508)\n        t_510 = self.n_Conv_66(t_509)\n        t_511 = torch.add(t_510, t_505)\n        t_512 = F.relu(t_511)\n        t_513 = self.n_Conv_67(t_512)\n        t_514 = F.relu(t_513)\n        t_514_padded = F.pad(t_514, [1, 1, 1, 1], value=0)\n        t_515 = self.n_Conv_68(t_514_padded)\n        t_516 = F.relu(t_515)\n        t_517 = self.n_Conv_69(t_516)\n        t_518 = torch.add(t_517, t_512)\n        t_519 = F.relu(t_518)\n        t_520 = self.n_Conv_70(t_519)\n        t_521 = F.relu(t_520)\n        t_521_padded = F.pad(t_521, [1, 1, 1, 1], value=0)\n        t_522 = self.n_Conv_71(t_521_padded)\n        t_523 = F.relu(t_522)\n        t_524 = self.n_Conv_72(t_523)\n        t_525 = torch.add(t_524, t_519)\n        t_526 = F.relu(t_525)\n        t_527 = self.n_Conv_73(t_526)\n        t_528 = F.relu(t_527)\n        t_528_padded = F.pad(t_528, [1, 1, 1, 1], value=0)\n        t_529 = self.n_Conv_74(t_528_padded)\n        t_530 = F.relu(t_529)\n        t_531 = self.n_Conv_75(t_530)\n        t_532 = torch.add(t_531, t_526)\n        t_533 = F.relu(t_532)\n        t_534 = self.n_Conv_76(t_533)\n        t_535 = F.relu(t_534)\n        t_535_padded = F.pad(t_535, [1, 1, 1, 1], value=0)\n        t_536 = self.n_Conv_77(t_535_padded)\n        t_537 = F.relu(t_536)\n        t_538 = self.n_Conv_78(t_537)\n        t_539 = torch.add(t_538, t_533)\n        t_540 = F.relu(t_539)\n        t_541 = self.n_Conv_79(t_540)\n        t_542 = F.relu(t_541)\n        t_542_padded = F.pad(t_542, [1, 1, 1, 1], value=0)\n        t_543 = self.n_Conv_80(t_542_padded)\n        t_544 = F.relu(t_543)\n        t_545 = self.n_Conv_81(t_544)\n        t_546 = torch.add(t_545, t_540)\n        t_547 = F.relu(t_546)\n        t_548 = self.n_Conv_82(t_547)\n        t_549 = F.relu(t_548)\n        t_549_padded = F.pad(t_549, [1, 1, 1, 1], value=0)\n        t_550 = self.n_Conv_83(t_549_padded)\n        t_551 = F.relu(t_550)\n        t_552 = self.n_Conv_84(t_551)\n        t_553 = torch.add(t_552, t_547)\n        t_554 = F.relu(t_553)\n        t_555 = self.n_Conv_85(t_554)\n        t_556 = F.relu(t_555)\n        t_556_padded = F.pad(t_556, [1, 1, 1, 1], value=0)\n        t_557 = self.n_Conv_86(t_556_padded)\n        t_558 = F.relu(t_557)\n        t_559 = self.n_Conv_87(t_558)\n        t_560 = torch.add(t_559, t_554)\n        t_561 = F.relu(t_560)\n        t_562 = self.n_Conv_88(t_561)\n        t_563 = F.relu(t_562)\n        t_563_padded = F.pad(t_563, [1, 1, 1, 1], value=0)\n        t_564 = self.n_Conv_89(t_563_padded)\n        t_565 = F.relu(t_564)\n        t_566 = self.n_Conv_90(t_565)\n        t_567 = torch.add(t_566, t_561)\n        t_568 = F.relu(t_567)\n        t_569 = self.n_Conv_91(t_568)\n        t_570 = F.relu(t_569)\n        t_570_padded = F.pad(t_570, [1, 1, 1, 1], value=0)\n        t_571 = self.n_Conv_92(t_570_padded)\n        t_572 = F.relu(t_571)\n        t_573 = self.n_Conv_93(t_572)\n        t_574 = torch.add(t_573, t_568)\n        t_575 = F.relu(t_574)\n        t_576 = self.n_Conv_94(t_575)\n        t_577 = F.relu(t_576)\n        t_577_padded = F.pad(t_577, [1, 1, 1, 1], value=0)\n        t_578 = self.n_Conv_95(t_577_padded)\n        t_579 = F.relu(t_578)\n        t_580 = self.n_Conv_96(t_579)\n        t_581 = torch.add(t_580, t_575)\n        t_582 = F.relu(t_581)\n        t_583 = self.n_Conv_97(t_582)\n        t_584 = F.relu(t_583)\n        t_584_padded = F.pad(t_584, [0, 1, 0, 1], value=0)\n        t_585 = self.n_Conv_98(t_584_padded)\n        t_586 = F.relu(t_585)\n        t_587 = self.n_Conv_99(t_586)\n        t_588 = self.n_Conv_100(t_582)\n        t_589 = torch.add(t_587, t_588)\n        t_590 = F.relu(t_589)\n        t_591 = self.n_Conv_101(t_590)\n        t_592 = F.relu(t_591)\n        t_592_padded = F.pad(t_592, [1, 1, 1, 1], value=0)\n        t_593 = self.n_Conv_102(t_592_padded)\n        t_594 = F.relu(t_593)\n        t_595 = self.n_Conv_103(t_594)\n        t_596 = torch.add(t_595, t_590)\n        t_597 = F.relu(t_596)\n        t_598 = self.n_Conv_104(t_597)\n        t_599 = F.relu(t_598)\n        t_599_padded = F.pad(t_599, [1, 1, 1, 1], value=0)\n        t_600 = self.n_Conv_105(t_599_padded)\n        t_601 = F.relu(t_600)\n        t_602 = self.n_Conv_106(t_601)\n        t_603 = torch.add(t_602, t_597)\n        t_604 = F.relu(t_603)\n        t_605 = self.n_Conv_107(t_604)\n        t_606 = F.relu(t_605)\n        t_606_padded = F.pad(t_606, [1, 1, 1, 1], value=0)\n        t_607 = self.n_Conv_108(t_606_padded)\n        t_608 = F.relu(t_607)\n        t_609 = self.n_Conv_109(t_608)\n        t_610 = torch.add(t_609, t_604)\n        t_611 = F.relu(t_610)\n        t_612 = self.n_Conv_110(t_611)\n        t_613 = F.relu(t_612)\n        t_613_padded = F.pad(t_613, [1, 1, 1, 1], value=0)\n        t_614 = self.n_Conv_111(t_613_padded)\n        t_615 = F.relu(t_614)\n        t_616 = self.n_Conv_112(t_615)\n        t_617 = torch.add(t_616, t_611)\n        t_618 = F.relu(t_617)\n        t_619 = self.n_Conv_113(t_618)\n        t_620 = F.relu(t_619)\n        t_620_padded = F.pad(t_620, [1, 1, 1, 1], value=0)\n        t_621 = self.n_Conv_114(t_620_padded)\n        t_622 = F.relu(t_621)\n        t_623 = self.n_Conv_115(t_622)\n        t_624 = torch.add(t_623, t_618)\n        t_625 = F.relu(t_624)\n        t_626 = self.n_Conv_116(t_625)\n        t_627 = F.relu(t_626)\n        t_627_padded = F.pad(t_627, [1, 1, 1, 1], value=0)\n        t_628 = self.n_Conv_117(t_627_padded)\n        t_629 = F.relu(t_628)\n        t_630 = self.n_Conv_118(t_629)\n        t_631 = torch.add(t_630, t_625)\n        t_632 = F.relu(t_631)\n        t_633 = self.n_Conv_119(t_632)\n        t_634 = F.relu(t_633)\n        t_634_padded = F.pad(t_634, [1, 1, 1, 1], value=0)\n        t_635 = self.n_Conv_120(t_634_padded)\n        t_636 = F.relu(t_635)\n        t_637 = self.n_Conv_121(t_636)\n        t_638 = torch.add(t_637, t_632)\n        t_639 = F.relu(t_638)\n        t_640 = self.n_Conv_122(t_639)\n        t_641 = F.relu(t_640)\n        t_641_padded = F.pad(t_641, [1, 1, 1, 1], value=0)\n        t_642 = self.n_Conv_123(t_641_padded)\n        t_643 = F.relu(t_642)\n        t_644 = self.n_Conv_124(t_643)\n        t_645 = torch.add(t_644, t_639)\n        t_646 = F.relu(t_645)\n        t_647 = self.n_Conv_125(t_646)\n        t_648 = F.relu(t_647)\n        t_648_padded = F.pad(t_648, [1, 1, 1, 1], value=0)\n        t_649 = self.n_Conv_126(t_648_padded)\n        t_650 = F.relu(t_649)\n        t_651 = self.n_Conv_127(t_650)\n        t_652 = torch.add(t_651, t_646)\n        t_653 = F.relu(t_652)\n        t_654 = self.n_Conv_128(t_653)\n        t_655 = F.relu(t_654)\n        t_655_padded = F.pad(t_655, [1, 1, 1, 1], value=0)\n        t_656 = self.n_Conv_129(t_655_padded)\n        t_657 = F.relu(t_656)\n        t_658 = self.n_Conv_130(t_657)\n        t_659 = torch.add(t_658, t_653)\n        t_660 = F.relu(t_659)\n        t_661 = self.n_Conv_131(t_660)\n        t_662 = F.relu(t_661)\n        t_662_padded = F.pad(t_662, [1, 1, 1, 1], value=0)\n        t_663 = self.n_Conv_132(t_662_padded)\n        t_664 = F.relu(t_663)\n        t_665 = self.n_Conv_133(t_664)\n        t_666 = torch.add(t_665, t_660)\n        t_667 = F.relu(t_666)\n        t_668 = self.n_Conv_134(t_667)\n        t_669 = F.relu(t_668)\n        t_669_padded = F.pad(t_669, [1, 1, 1, 1], value=0)\n        t_670 = self.n_Conv_135(t_669_padded)\n        t_671 = F.relu(t_670)\n        t_672 = self.n_Conv_136(t_671)\n        t_673 = torch.add(t_672, t_667)\n        t_674 = F.relu(t_673)\n        t_675 = self.n_Conv_137(t_674)\n        t_676 = F.relu(t_675)\n        t_676_padded = F.pad(t_676, [1, 1, 1, 1], value=0)\n        t_677 = self.n_Conv_138(t_676_padded)\n        t_678 = F.relu(t_677)\n        t_679 = self.n_Conv_139(t_678)\n        t_680 = torch.add(t_679, t_674)\n        t_681 = F.relu(t_680)\n        t_682 = self.n_Conv_140(t_681)\n        t_683 = F.relu(t_682)\n        t_683_padded = F.pad(t_683, [1, 1, 1, 1], value=0)\n        t_684 = self.n_Conv_141(t_683_padded)\n        t_685 = F.relu(t_684)\n        t_686 = self.n_Conv_142(t_685)\n        t_687 = torch.add(t_686, t_681)\n        t_688 = F.relu(t_687)\n        t_689 = self.n_Conv_143(t_688)\n        t_690 = F.relu(t_689)\n        t_690_padded = F.pad(t_690, [1, 1, 1, 1], value=0)\n        t_691 = self.n_Conv_144(t_690_padded)\n        t_692 = F.relu(t_691)\n        t_693 = self.n_Conv_145(t_692)\n        t_694 = torch.add(t_693, t_688)\n        t_695 = F.relu(t_694)\n        t_696 = self.n_Conv_146(t_695)\n        t_697 = F.relu(t_696)\n        t_697_padded = F.pad(t_697, [1, 1, 1, 1], value=0)\n        t_698 = self.n_Conv_147(t_697_padded)\n        t_699 = F.relu(t_698)\n        t_700 = self.n_Conv_148(t_699)\n        t_701 = torch.add(t_700, t_695)\n        t_702 = F.relu(t_701)\n        t_703 = self.n_Conv_149(t_702)\n        t_704 = F.relu(t_703)\n        t_704_padded = F.pad(t_704, [1, 1, 1, 1], value=0)\n        t_705 = self.n_Conv_150(t_704_padded)\n        t_706 = F.relu(t_705)\n        t_707 = self.n_Conv_151(t_706)\n        t_708 = torch.add(t_707, t_702)\n        t_709 = F.relu(t_708)\n        t_710 = self.n_Conv_152(t_709)\n        t_711 = F.relu(t_710)\n        t_711_padded = F.pad(t_711, [1, 1, 1, 1], value=0)\n        t_712 = self.n_Conv_153(t_711_padded)\n        t_713 = F.relu(t_712)\n        t_714 = self.n_Conv_154(t_713)\n        t_715 = torch.add(t_714, t_709)\n        t_716 = F.relu(t_715)\n        t_717 = self.n_Conv_155(t_716)\n        t_718 = F.relu(t_717)\n        t_718_padded = F.pad(t_718, [1, 1, 1, 1], value=0)\n        t_719 = self.n_Conv_156(t_718_padded)\n        t_720 = F.relu(t_719)\n        t_721 = self.n_Conv_157(t_720)\n        t_722 = torch.add(t_721, t_716)\n        t_723 = F.relu(t_722)\n        t_724 = self.n_Conv_158(t_723)\n        t_725 = self.n_Conv_159(t_723)\n        t_726 = F.relu(t_725)\n        t_726_padded = F.pad(t_726, [0, 1, 0, 1], value=0)\n        t_727 = self.n_Conv_160(t_726_padded)\n        t_728 = F.relu(t_727)\n        t_729 = self.n_Conv_161(t_728)\n        t_730 = torch.add(t_729, t_724)\n        t_731 = F.relu(t_730)\n        t_732 = self.n_Conv_162(t_731)\n        t_733 = F.relu(t_732)\n        t_733_padded = F.pad(t_733, [1, 1, 1, 1], value=0)\n        t_734 = self.n_Conv_163(t_733_padded)\n        t_735 = F.relu(t_734)\n        t_736 = self.n_Conv_164(t_735)\n        t_737 = torch.add(t_736, t_731)\n        t_738 = F.relu(t_737)\n        t_739 = self.n_Conv_165(t_738)\n        t_740 = F.relu(t_739)\n        t_740_padded = F.pad(t_740, [1, 1, 1, 1], value=0)\n        t_741 = self.n_Conv_166(t_740_padded)\n        t_742 = F.relu(t_741)\n        t_743 = self.n_Conv_167(t_742)\n        t_744 = torch.add(t_743, t_738)\n        t_745 = F.relu(t_744)\n        t_746 = self.n_Conv_168(t_745)\n        t_747 = self.n_Conv_169(t_745)\n        t_748 = F.relu(t_747)\n        t_748_padded = F.pad(t_748, [0, 1, 0, 1], value=0)\n        t_749 = self.n_Conv_170(t_748_padded)\n        t_750 = F.relu(t_749)\n        t_751 = self.n_Conv_171(t_750)\n        t_752 = torch.add(t_751, t_746)\n        t_753 = F.relu(t_752)\n        t_754 = self.n_Conv_172(t_753)\n        t_755 = F.relu(t_754)\n        t_755_padded = F.pad(t_755, [1, 1, 1, 1], value=0)\n        t_756 = self.n_Conv_173(t_755_padded)\n        t_757 = F.relu(t_756)\n        t_758 = self.n_Conv_174(t_757)\n        t_759 = torch.add(t_758, t_753)\n        t_760 = F.relu(t_759)\n        t_761 = self.n_Conv_175(t_760)\n        t_762 = F.relu(t_761)\n        t_762_padded = F.pad(t_762, [1, 1, 1, 1], value=0)\n        t_763 = self.n_Conv_176(t_762_padded)\n        t_764 = F.relu(t_763)\n        t_765 = self.n_Conv_177(t_764)\n        t_766 = torch.add(t_765, t_760)\n        t_767 = F.relu(t_766)\n        t_768 = self.n_Conv_178(t_767)\n        t_769 = F.avg_pool2d(t_768, kernel_size=t_768.shape[-2:])\n        t_770 = torch.squeeze(t_769, 3)\n        t_770 = torch.squeeze(t_770, 2)\n        t_771 = torch.sigmoid(t_770)\n        return t_771\n\n    def load_state_dict(self, state_dict, **kwargs):\n        self.tags = state_dict.get('tags', [])\n\n        super(DeepDanbooruModel, self).load_state_dict({k: v for k, v in state_dict.items() if k != 'tags'})\n\n", "modules/mac_specific.py": "import logging\n\nimport torch\nfrom torch import Tensor\nimport platform\nfrom modules.sd_hijack_utils import CondFunc\nfrom packaging import version\nfrom modules import shared\n\nlog = logging.getLogger(__name__)\n\n\n# before torch version 1.13, has_mps is only available in nightly pytorch and macOS 12.3+,\n# use check `getattr` and try it for compatibility.\n# in torch version 1.13, backends.mps.is_available() and backends.mps.is_built() are introduced in to check mps availability,\n# since torch 2.0.1+ nightly build, getattr(torch, 'has_mps', False) was deprecated, see https://github.com/pytorch/pytorch/pull/103279\ndef check_for_mps() -> bool:\n    if version.parse(torch.__version__) <= version.parse(\"2.0.1\"):\n        if not getattr(torch, 'has_mps', False):\n            return False\n        try:\n            torch.zeros(1).to(torch.device(\"mps\"))\n            return True\n        except Exception:\n            return False\n    else:\n        return torch.backends.mps.is_available() and torch.backends.mps.is_built()\n\n\nhas_mps = check_for_mps()\n\n\ndef torch_mps_gc() -> None:\n    try:\n        if shared.state.current_latent is not None:\n            log.debug(\"`current_latent` is set, skipping MPS garbage collection\")\n            return\n        from torch.mps import empty_cache\n        empty_cache()\n    except Exception:\n        log.warning(\"MPS garbage collection failed\", exc_info=True)\n\n\n# MPS workaround for https://github.com/pytorch/pytorch/issues/89784\ndef cumsum_fix(input, cumsum_func, *args, **kwargs):\n    if input.device.type == 'mps':\n        output_dtype = kwargs.get('dtype', input.dtype)\n        if output_dtype == torch.int64:\n            return cumsum_func(input.cpu(), *args, **kwargs).to(input.device)\n        elif output_dtype == torch.bool or cumsum_needs_int_fix and (output_dtype == torch.int8 or output_dtype == torch.int16):\n            return cumsum_func(input.to(torch.int32), *args, **kwargs).to(torch.int64)\n    return cumsum_func(input, *args, **kwargs)\n\n\n# MPS workaround for https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/14046\ndef interpolate_with_fp32_fallback(orig_func, *args, **kwargs) -> Tensor:\n    try:\n        return orig_func(*args, **kwargs)\n    except RuntimeError as e:\n        if \"not implemented for\" in str(e) and \"Half\" in str(e):\n            input_tensor = args[0]\n            return orig_func(input_tensor.to(torch.float32), *args[1:], **kwargs).to(input_tensor.dtype)\n        else:\n            print(f\"An unexpected RuntimeError occurred: {str(e)}\")\n\nif has_mps:\n    if platform.mac_ver()[0].startswith(\"13.2.\"):\n        # MPS workaround for https://github.com/pytorch/pytorch/issues/95188, thanks to danieldk (https://github.com/explosion/curated-transformers/pull/124)\n        CondFunc('torch.nn.functional.linear', lambda _, input, weight, bias: (torch.matmul(input, weight.t()) + bias) if bias is not None else torch.matmul(input, weight.t()), lambda _, input, weight, bias: input.numel() > 10485760)\n\n    if version.parse(torch.__version__) < version.parse(\"1.13\"):\n        # PyTorch 1.13 doesn't need these fixes but unfortunately is slower and has regressions that prevent training from working\n\n        # MPS workaround for https://github.com/pytorch/pytorch/issues/79383\n        CondFunc('torch.Tensor.to', lambda orig_func, self, *args, **kwargs: orig_func(self.contiguous(), *args, **kwargs),\n                                                          lambda _, self, *args, **kwargs: self.device.type != 'mps' and (args and isinstance(args[0], torch.device) and args[0].type == 'mps' or isinstance(kwargs.get('device'), torch.device) and kwargs['device'].type == 'mps'))\n        # MPS workaround for https://github.com/pytorch/pytorch/issues/80800\n        CondFunc('torch.nn.functional.layer_norm', lambda orig_func, *args, **kwargs: orig_func(*([args[0].contiguous()] + list(args[1:])), **kwargs),\n                                                                                        lambda _, *args, **kwargs: args and isinstance(args[0], torch.Tensor) and args[0].device.type == 'mps')\n        # MPS workaround for https://github.com/pytorch/pytorch/issues/90532\n        CondFunc('torch.Tensor.numpy', lambda orig_func, self, *args, **kwargs: orig_func(self.detach(), *args, **kwargs), lambda _, self, *args, **kwargs: self.requires_grad)\n    elif version.parse(torch.__version__) > version.parse(\"1.13.1\"):\n        cumsum_needs_int_fix = not torch.Tensor([1,2]).to(torch.device(\"mps\")).equal(torch.ShortTensor([1,1]).to(torch.device(\"mps\")).cumsum(0))\n        cumsum_fix_func = lambda orig_func, input, *args, **kwargs: cumsum_fix(input, orig_func, *args, **kwargs)\n        CondFunc('torch.cumsum', cumsum_fix_func, None)\n        CondFunc('torch.Tensor.cumsum', cumsum_fix_func, None)\n        CondFunc('torch.narrow', lambda orig_func, *args, **kwargs: orig_func(*args, **kwargs).clone(), None)\n\n        # MPS workaround for https://github.com/pytorch/pytorch/issues/96113\n        CondFunc('torch.nn.functional.layer_norm', lambda orig_func, x, normalized_shape, weight, bias, eps, **kwargs: orig_func(x.float(), normalized_shape, weight.float() if weight is not None else None, bias.float() if bias is not None else bias, eps).to(x.dtype), lambda _, input, *args, **kwargs: len(args) == 4 and input.device.type == 'mps')\n\n        # MPS workaround for https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/14046\n        CondFunc('torch.nn.functional.interpolate', interpolate_with_fp32_fallback, None)\n\n        # MPS workaround for https://github.com/pytorch/pytorch/issues/92311\n        if platform.processor() == 'i386':\n            for funcName in ['torch.argmax', 'torch.Tensor.argmax']:\n                CondFunc(funcName, lambda _, input, *args, **kwargs: torch.max(input.float() if input.dtype == torch.int64 else input, *args, **kwargs)[1], lambda _, input, *args, **kwargs: input.device.type == 'mps')\n", "modules/interrogate.py": "import os\nimport sys\nfrom collections import namedtuple\nfrom pathlib import Path\nimport re\n\nimport torch\nimport torch.hub\n\nfrom torchvision import transforms\nfrom torchvision.transforms.functional import InterpolationMode\n\nfrom modules import devices, paths, shared, lowvram, modelloader, errors, torch_utils\n\nblip_image_eval_size = 384\nclip_model_name = 'ViT-L/14'\n\nCategory = namedtuple(\"Category\", [\"name\", \"topn\", \"items\"])\n\nre_topn = re.compile(r\"\\.top(\\d+)$\")\n\ndef category_types():\n    return [f.stem for f in Path(shared.interrogator.content_dir).glob('*.txt')]\n\n\ndef download_default_clip_interrogate_categories(content_dir):\n    print(\"Downloading CLIP categories...\")\n\n    tmpdir = f\"{content_dir}_tmp\"\n    category_types = [\"artists\", \"flavors\", \"mediums\", \"movements\"]\n\n    try:\n        os.makedirs(tmpdir, exist_ok=True)\n        for category_type in category_types:\n            torch.hub.download_url_to_file(f\"https://raw.githubusercontent.com/pharmapsychotic/clip-interrogator/main/clip_interrogator/data/{category_type}.txt\", os.path.join(tmpdir, f\"{category_type}.txt\"))\n        os.rename(tmpdir, content_dir)\n\n    except Exception as e:\n        errors.display(e, \"downloading default CLIP interrogate categories\")\n    finally:\n        if os.path.exists(tmpdir):\n            os.removedirs(tmpdir)\n\n\nclass InterrogateModels:\n    blip_model = None\n    clip_model = None\n    clip_preprocess = None\n    dtype = None\n    running_on_cpu = None\n\n    def __init__(self, content_dir):\n        self.loaded_categories = None\n        self.skip_categories = []\n        self.content_dir = content_dir\n        self.running_on_cpu = devices.device_interrogate == torch.device(\"cpu\")\n\n    def categories(self):\n        if not os.path.exists(self.content_dir):\n            download_default_clip_interrogate_categories(self.content_dir)\n\n        if self.loaded_categories is not None and self.skip_categories == shared.opts.interrogate_clip_skip_categories:\n           return self.loaded_categories\n\n        self.loaded_categories = []\n\n        if os.path.exists(self.content_dir):\n            self.skip_categories = shared.opts.interrogate_clip_skip_categories\n            category_types = []\n            for filename in Path(self.content_dir).glob('*.txt'):\n                category_types.append(filename.stem)\n                if filename.stem in self.skip_categories:\n                    continue\n                m = re_topn.search(filename.stem)\n                topn = 1 if m is None else int(m.group(1))\n                with open(filename, \"r\", encoding=\"utf8\") as file:\n                    lines = [x.strip() for x in file.readlines()]\n\n                self.loaded_categories.append(Category(name=filename.stem, topn=topn, items=lines))\n\n        return self.loaded_categories\n\n    def create_fake_fairscale(self):\n        class FakeFairscale:\n            def checkpoint_wrapper(self):\n                pass\n\n        sys.modules[\"fairscale.nn.checkpoint.checkpoint_activations\"] = FakeFairscale\n\n    def load_blip_model(self):\n        self.create_fake_fairscale()\n        import models.blip\n\n        files = modelloader.load_models(\n            model_path=os.path.join(paths.models_path, \"BLIP\"),\n            model_url='https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_caption_capfilt_large.pth',\n            ext_filter=[\".pth\"],\n            download_name='model_base_caption_capfilt_large.pth',\n        )\n\n        blip_model = models.blip.blip_decoder(pretrained=files[0], image_size=blip_image_eval_size, vit='base', med_config=os.path.join(paths.paths[\"BLIP\"], \"configs\", \"med_config.json\"))\n        blip_model.eval()\n\n        return blip_model\n\n    def load_clip_model(self):\n        import clip\n\n        if self.running_on_cpu:\n            model, preprocess = clip.load(clip_model_name, device=\"cpu\", download_root=shared.cmd_opts.clip_models_path)\n        else:\n            model, preprocess = clip.load(clip_model_name, download_root=shared.cmd_opts.clip_models_path)\n\n        model.eval()\n        model = model.to(devices.device_interrogate)\n\n        return model, preprocess\n\n    def load(self):\n        if self.blip_model is None:\n            self.blip_model = self.load_blip_model()\n            if not shared.cmd_opts.no_half and not self.running_on_cpu:\n                self.blip_model = self.blip_model.half()\n\n        self.blip_model = self.blip_model.to(devices.device_interrogate)\n\n        if self.clip_model is None:\n            self.clip_model, self.clip_preprocess = self.load_clip_model()\n            if not shared.cmd_opts.no_half and not self.running_on_cpu:\n                self.clip_model = self.clip_model.half()\n\n        self.clip_model = self.clip_model.to(devices.device_interrogate)\n\n        self.dtype = torch_utils.get_param(self.clip_model).dtype\n\n    def send_clip_to_ram(self):\n        if not shared.opts.interrogate_keep_models_in_memory:\n            if self.clip_model is not None:\n                self.clip_model = self.clip_model.to(devices.cpu)\n\n    def send_blip_to_ram(self):\n        if not shared.opts.interrogate_keep_models_in_memory:\n            if self.blip_model is not None:\n                self.blip_model = self.blip_model.to(devices.cpu)\n\n    def unload(self):\n        self.send_clip_to_ram()\n        self.send_blip_to_ram()\n\n        devices.torch_gc()\n\n    def rank(self, image_features, text_array, top_count=1):\n        import clip\n\n        devices.torch_gc()\n\n        if shared.opts.interrogate_clip_dict_limit != 0:\n            text_array = text_array[0:int(shared.opts.interrogate_clip_dict_limit)]\n\n        top_count = min(top_count, len(text_array))\n        text_tokens = clip.tokenize(list(text_array), truncate=True).to(devices.device_interrogate)\n        text_features = self.clip_model.encode_text(text_tokens).type(self.dtype)\n        text_features /= text_features.norm(dim=-1, keepdim=True)\n\n        similarity = torch.zeros((1, len(text_array))).to(devices.device_interrogate)\n        for i in range(image_features.shape[0]):\n            similarity += (100.0 * image_features[i].unsqueeze(0) @ text_features.T).softmax(dim=-1)\n        similarity /= image_features.shape[0]\n\n        top_probs, top_labels = similarity.cpu().topk(top_count, dim=-1)\n        return [(text_array[top_labels[0][i].numpy()], (top_probs[0][i].numpy()*100)) for i in range(top_count)]\n\n    def generate_caption(self, pil_image):\n        gpu_image = transforms.Compose([\n            transforms.Resize((blip_image_eval_size, blip_image_eval_size), interpolation=InterpolationMode.BICUBIC),\n            transforms.ToTensor(),\n            transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n        ])(pil_image).unsqueeze(0).type(self.dtype).to(devices.device_interrogate)\n\n        with torch.no_grad():\n            caption = self.blip_model.generate(gpu_image, sample=False, num_beams=shared.opts.interrogate_clip_num_beams, min_length=shared.opts.interrogate_clip_min_length, max_length=shared.opts.interrogate_clip_max_length)\n\n        return caption[0]\n\n    def interrogate(self, pil_image):\n        res = \"\"\n        shared.state.begin(job=\"interrogate\")\n        try:\n            lowvram.send_everything_to_cpu()\n            devices.torch_gc()\n\n            self.load()\n\n            caption = self.generate_caption(pil_image)\n            self.send_blip_to_ram()\n            devices.torch_gc()\n\n            res = caption\n\n            clip_image = self.clip_preprocess(pil_image).unsqueeze(0).type(self.dtype).to(devices.device_interrogate)\n\n            with torch.no_grad(), devices.autocast():\n                image_features = self.clip_model.encode_image(clip_image).type(self.dtype)\n\n                image_features /= image_features.norm(dim=-1, keepdim=True)\n\n                for cat in self.categories():\n                    matches = self.rank(image_features, cat.items, top_count=cat.topn)\n                    for match, score in matches:\n                        if shared.opts.interrogate_return_ranks:\n                            res += f\", ({match}:{score/100:.3f})\"\n                        else:\n                            res += f\", {match}\"\n\n        except Exception:\n            errors.report(\"Error interrogating\", exc_info=True)\n            res += \"<error>\"\n\n        self.unload()\n        shared.state.end()\n\n        return res\n", "modules/cmd_args.py": "import argparse\nimport json\nimport os\nfrom modules.paths_internal import normalized_filepath, models_path, script_path, data_path, extensions_dir, extensions_builtin_dir, sd_default_config, sd_model_file  # noqa: F401\n\nparser = argparse.ArgumentParser()\n\nparser.add_argument(\"-f\", action='store_true', help=argparse.SUPPRESS)  # allows running as root; implemented outside of webui\nparser.add_argument(\"--update-all-extensions\", action='store_true', help=\"launch.py argument: download updates for all extensions when starting the program\")\nparser.add_argument(\"--skip-python-version-check\", action='store_true', help=\"launch.py argument: do not check python version\")\nparser.add_argument(\"--skip-torch-cuda-test\", action='store_true', help=\"launch.py argument: do not check if CUDA is able to work properly\")\nparser.add_argument(\"--reinstall-xformers\", action='store_true', help=\"launch.py argument: install the appropriate version of xformers even if you have some version already installed\")\nparser.add_argument(\"--reinstall-torch\", action='store_true', help=\"launch.py argument: install the appropriate version of torch even if you have some version already installed\")\nparser.add_argument(\"--update-check\", action='store_true', help=\"launch.py argument: check for updates at startup\")\nparser.add_argument(\"--test-server\", action='store_true', help=\"launch.py argument: configure server for testing\")\nparser.add_argument(\"--log-startup\", action='store_true', help=\"launch.py argument: print a detailed log of what's happening at startup\")\nparser.add_argument(\"--skip-prepare-environment\", action='store_true', help=\"launch.py argument: skip all environment preparation\")\nparser.add_argument(\"--skip-install\", action='store_true', help=\"launch.py argument: skip installation of packages\")\nparser.add_argument(\"--dump-sysinfo\", action='store_true', help=\"launch.py argument: dump limited sysinfo file (without information about extensions, options) to disk and quit\")\nparser.add_argument(\"--loglevel\", type=str, help=\"log level; one of: CRITICAL, ERROR, WARNING, INFO, DEBUG\", default=None)\nparser.add_argument(\"--do-not-download-clip\", action='store_true', help=\"do not download CLIP model even if it's not included in the checkpoint\")\nparser.add_argument(\"--data-dir\", type=normalized_filepath, default=os.path.dirname(os.path.dirname(os.path.realpath(__file__))), help=\"base path where all user data is stored\")\nparser.add_argument(\"--config\", type=normalized_filepath, default=sd_default_config, help=\"path to config which constructs model\",)\nparser.add_argument(\"--ckpt\", type=normalized_filepath, default=sd_model_file, help=\"path to checkpoint of stable diffusion model; if specified, this checkpoint will be added to the list of checkpoints and loaded\",)\nparser.add_argument(\"--ckpt-dir\", type=normalized_filepath, default=None, help=\"Path to directory with stable diffusion checkpoints\")\nparser.add_argument(\"--vae-dir\", type=normalized_filepath, default=None, help=\"Path to directory with VAE files\")\nparser.add_argument(\"--gfpgan-dir\", type=normalized_filepath, help=\"GFPGAN directory\", default=('./src/gfpgan' if os.path.exists('./src/gfpgan') else './GFPGAN'))\nparser.add_argument(\"--gfpgan-model\", type=normalized_filepath, help=\"GFPGAN model file name\", default=None)\nparser.add_argument(\"--no-half\", action='store_true', help=\"do not switch the model to 16-bit floats\")\nparser.add_argument(\"--no-half-vae\", action='store_true', help=\"do not switch the VAE model to 16-bit floats\")\nparser.add_argument(\"--no-progressbar-hiding\", action='store_true', help=\"do not hide progressbar in gradio UI (we hide it because it slows down ML if you have hardware acceleration in browser)\")\nparser.add_argument(\"--max-batch-count\", type=int, default=16, help=\"maximum batch count value for the UI\")\nparser.add_argument(\"--embeddings-dir\", type=normalized_filepath, default=os.path.join(data_path, 'embeddings'), help=\"embeddings directory for textual inversion (default: embeddings)\")\nparser.add_argument(\"--textual-inversion-templates-dir\", type=normalized_filepath, default=os.path.join(script_path, 'textual_inversion_templates'), help=\"directory with textual inversion templates\")\nparser.add_argument(\"--hypernetwork-dir\", type=normalized_filepath, default=os.path.join(models_path, 'hypernetworks'), help=\"hypernetwork directory\")\nparser.add_argument(\"--localizations-dir\", type=normalized_filepath, default=os.path.join(script_path, 'localizations'), help=\"localizations directory\")\nparser.add_argument(\"--allow-code\", action='store_true', help=\"allow custom script execution from webui\")\nparser.add_argument(\"--medvram\", action='store_true', help=\"enable stable diffusion model optimizations for sacrificing a little speed for low VRM usage\")\nparser.add_argument(\"--medvram-sdxl\", action='store_true', help=\"enable --medvram optimization just for SDXL models\")\nparser.add_argument(\"--lowvram\", action='store_true', help=\"enable stable diffusion model optimizations for sacrificing a lot of speed for very low VRM usage\")\nparser.add_argument(\"--lowram\", action='store_true', help=\"load stable diffusion checkpoint weights to VRAM instead of RAM\")\nparser.add_argument(\"--always-batch-cond-uncond\", action='store_true', help=\"does not do anything\")\nparser.add_argument(\"--unload-gfpgan\", action='store_true', help=\"does not do anything.\")\nparser.add_argument(\"--precision\", type=str, help=\"evaluate at this precision\", choices=[\"full\", \"autocast\"], default=\"autocast\")\nparser.add_argument(\"--upcast-sampling\", action='store_true', help=\"upcast sampling. No effect with --no-half. Usually produces similar results to --no-half with better performance while using less memory.\")\nparser.add_argument(\"--share\", action='store_true', help=\"use share=True for gradio and make the UI accessible through their site\")\nparser.add_argument(\"--ngrok\", type=str, help=\"ngrok authtoken, alternative to gradio --share\", default=None)\nparser.add_argument(\"--ngrok-region\", type=str, help=\"does not do anything.\", default=\"\")\nparser.add_argument(\"--ngrok-options\", type=json.loads, help='The options to pass to ngrok in JSON format, e.g.: \\'{\"authtoken_from_env\":true, \"basic_auth\":\"user:password\", \"oauth_provider\":\"google\", \"oauth_allow_emails\":\"user@asdf.com\"}\\'', default=dict())\nparser.add_argument(\"--enable-insecure-extension-access\", action='store_true', help=\"enable extensions tab regardless of other options\")\nparser.add_argument(\"--codeformer-models-path\", type=normalized_filepath, help=\"Path to directory with codeformer model file(s).\", default=os.path.join(models_path, 'Codeformer'))\nparser.add_argument(\"--gfpgan-models-path\", type=normalized_filepath, help=\"Path to directory with GFPGAN model file(s).\", default=os.path.join(models_path, 'GFPGAN'))\nparser.add_argument(\"--esrgan-models-path\", type=normalized_filepath, help=\"Path to directory with ESRGAN model file(s).\", default=os.path.join(models_path, 'ESRGAN'))\nparser.add_argument(\"--bsrgan-models-path\", type=normalized_filepath, help=\"Path to directory with BSRGAN model file(s).\", default=os.path.join(models_path, 'BSRGAN'))\nparser.add_argument(\"--realesrgan-models-path\", type=normalized_filepath, help=\"Path to directory with RealESRGAN model file(s).\", default=os.path.join(models_path, 'RealESRGAN'))\nparser.add_argument(\"--dat-models-path\", type=normalized_filepath, help=\"Path to directory with DAT model file(s).\", default=os.path.join(models_path, 'DAT'))\nparser.add_argument(\"--clip-models-path\", type=normalized_filepath, help=\"Path to directory with CLIP model file(s).\", default=None)\nparser.add_argument(\"--xformers\", action='store_true', help=\"enable xformers for cross attention layers\")\nparser.add_argument(\"--force-enable-xformers\", action='store_true', help=\"enable xformers for cross attention layers regardless of whether the checking code thinks you can run it; do not make bug reports if this fails to work\")\nparser.add_argument(\"--xformers-flash-attention\", action='store_true', help=\"enable xformers with Flash Attention to improve reproducibility (supported for SD2.x or variant only)\")\nparser.add_argument(\"--deepdanbooru\", action='store_true', help=\"does not do anything\")\nparser.add_argument(\"--opt-split-attention\", action='store_true', help=\"prefer Doggettx's cross-attention layer optimization for automatic choice of optimization\")\nparser.add_argument(\"--opt-sub-quad-attention\", action='store_true', help=\"prefer memory efficient sub-quadratic cross-attention layer optimization for automatic choice of optimization\")\nparser.add_argument(\"--sub-quad-q-chunk-size\", type=int, help=\"query chunk size for the sub-quadratic cross-attention layer optimization to use\", default=1024)\nparser.add_argument(\"--sub-quad-kv-chunk-size\", type=int, help=\"kv chunk size for the sub-quadratic cross-attention layer optimization to use\", default=None)\nparser.add_argument(\"--sub-quad-chunk-threshold\", type=int, help=\"the percentage of VRAM threshold for the sub-quadratic cross-attention layer optimization to use chunking\", default=None)\nparser.add_argument(\"--opt-split-attention-invokeai\", action='store_true', help=\"prefer InvokeAI's cross-attention layer optimization for automatic choice of optimization\")\nparser.add_argument(\"--opt-split-attention-v1\", action='store_true', help=\"prefer older version of split attention optimization for automatic choice of optimization\")\nparser.add_argument(\"--opt-sdp-attention\", action='store_true', help=\"prefer scaled dot product cross-attention layer optimization for automatic choice of optimization; requires PyTorch 2.*\")\nparser.add_argument(\"--opt-sdp-no-mem-attention\", action='store_true', help=\"prefer scaled dot product cross-attention layer optimization without memory efficient attention for automatic choice of optimization, makes image generation deterministic; requires PyTorch 2.*\")\nparser.add_argument(\"--disable-opt-split-attention\", action='store_true', help=\"prefer no cross-attention layer optimization for automatic choice of optimization\")\nparser.add_argument(\"--disable-nan-check\", action='store_true', help=\"do not check if produced images/latent spaces have nans; useful for running without a checkpoint in CI\")\nparser.add_argument(\"--use-cpu\", nargs='+', help=\"use CPU as torch device for specified modules\", default=[], type=str.lower)\nparser.add_argument(\"--use-ipex\", action=\"store_true\", help=\"use Intel XPU as torch device\")\nparser.add_argument(\"--disable-model-loading-ram-optimization\", action='store_true', help=\"disable an optimization that reduces RAM use when loading a model\")\nparser.add_argument(\"--listen\", action='store_true', help=\"launch gradio with 0.0.0.0 as server name, allowing to respond to network requests\")\nparser.add_argument(\"--port\", type=int, help=\"launch gradio with given server port, you need root/admin rights for ports < 1024, defaults to 7860 if available\", default=None)\nparser.add_argument(\"--show-negative-prompt\", action='store_true', help=\"does not do anything\", default=False)\nparser.add_argument(\"--ui-config-file\", type=str, help=\"filename to use for ui configuration\", default=os.path.join(data_path, 'ui-config.json'))\nparser.add_argument(\"--hide-ui-dir-config\", action='store_true', help=\"hide directory configuration from webui\", default=False)\nparser.add_argument(\"--freeze-settings\", action='store_true', help=\"disable editing of all settings globally\", default=False)\nparser.add_argument(\"--freeze-settings-in-sections\", type=str, help='disable editing settings in specific sections of the settings page by specifying a comma-delimited list such like \"saving-images,upscaling\". The list of setting names can be found in the modules/shared_options.py file', default=None)\nparser.add_argument(\"--freeze-specific-settings\", type=str, help='disable editing of individual settings by specifying a comma-delimited list like \"samples_save,samples_format\". The list of setting names can be found in the config.json file', default=None)\nparser.add_argument(\"--ui-settings-file\", type=str, help=\"filename to use for ui settings\", default=os.path.join(data_path, 'config.json'))\nparser.add_argument(\"--gradio-debug\",  action='store_true', help=\"launch gradio with --debug option\")\nparser.add_argument(\"--gradio-auth\", type=str, help='set gradio authentication like \"username:password\"; or comma-delimit multiple like \"u1:p1,u2:p2,u3:p3\"', default=None)\nparser.add_argument(\"--gradio-auth-path\", type=normalized_filepath, help='set gradio authentication file path ex. \"/path/to/auth/file\" same auth format as --gradio-auth', default=None)\nparser.add_argument(\"--gradio-img2img-tool\", type=str, help='does not do anything')\nparser.add_argument(\"--gradio-inpaint-tool\", type=str, help=\"does not do anything\")\nparser.add_argument(\"--gradio-allowed-path\", action='append', help=\"add path to gradio's allowed_paths, make it possible to serve files from it\", default=[data_path])\nparser.add_argument(\"--opt-channelslast\", action='store_true', help=\"change memory type for stable diffusion to channels last\")\nparser.add_argument(\"--styles-file\", type=str, action='append', help=\"path or wildcard path of styles files, allow multiple entries.\", default=[])\nparser.add_argument(\"--autolaunch\", action='store_true', help=\"open the webui URL in the system's default browser upon launch\", default=False)\nparser.add_argument(\"--theme\", type=str, help=\"launches the UI with light or dark theme\", default=None)\nparser.add_argument(\"--use-textbox-seed\", action='store_true', help=\"use textbox for seeds in UI (no up/down, but possible to input long seeds)\", default=False)\nparser.add_argument(\"--disable-console-progressbars\", action='store_true', help=\"do not output progressbars to console\", default=False)\nparser.add_argument(\"--enable-console-prompts\", action='store_true', help=\"does not do anything\", default=False)  # Legacy compatibility, use as default value shared.opts.enable_console_prompts\nparser.add_argument('--vae-path', type=normalized_filepath, help='Checkpoint to use as VAE; setting this argument disables all settings related to VAE', default=None)\nparser.add_argument(\"--disable-safe-unpickle\", action='store_true', help=\"disable checking pytorch models for malicious code\", default=False)\nparser.add_argument(\"--api\", action='store_true', help=\"use api=True to launch the API together with the webui (use --nowebui instead for only the API)\")\nparser.add_argument(\"--api-auth\", type=str, help='Set authentication for API like \"username:password\"; or comma-delimit multiple like \"u1:p1,u2:p2,u3:p3\"', default=None)\nparser.add_argument(\"--api-log\", action='store_true', help=\"use api-log=True to enable logging of all API requests\")\nparser.add_argument(\"--nowebui\", action='store_true', help=\"use api=True to launch the API instead of the webui\")\nparser.add_argument(\"--ui-debug-mode\", action='store_true', help=\"Don't load model to quickly launch UI\")\nparser.add_argument(\"--device-id\", type=str, help=\"Select the default CUDA device to use (export CUDA_VISIBLE_DEVICES=0,1,etc might be needed before)\", default=None)\nparser.add_argument(\"--administrator\", action='store_true', help=\"Administrator rights\", default=False)\nparser.add_argument(\"--cors-allow-origins\", type=str, help=\"Allowed CORS origin(s) in the form of a comma-separated list (no spaces)\", default=None)\nparser.add_argument(\"--cors-allow-origins-regex\", type=str, help=\"Allowed CORS origin(s) in the form of a single regular expression\", default=None)\nparser.add_argument(\"--tls-keyfile\", type=str, help=\"Partially enables TLS, requires --tls-certfile to fully function\", default=None)\nparser.add_argument(\"--tls-certfile\", type=str, help=\"Partially enables TLS, requires --tls-keyfile to fully function\", default=None)\nparser.add_argument(\"--disable-tls-verify\", action=\"store_false\", help=\"When passed, enables the use of self-signed certificates.\", default=None)\nparser.add_argument(\"--server-name\", type=str, help=\"Sets hostname of server\", default=None)\nparser.add_argument(\"--gradio-queue\", action='store_true', help=\"does not do anything\", default=True)\nparser.add_argument(\"--no-gradio-queue\", action='store_true', help=\"Disables gradio queue; causes the webpage to use http requests instead of websockets; was the default in earlier versions\")\nparser.add_argument(\"--skip-version-check\", action='store_true', help=\"Do not check versions of torch and xformers\")\nparser.add_argument(\"--no-hashing\", action='store_true', help=\"disable sha256 hashing of checkpoints to help loading performance\", default=False)\nparser.add_argument(\"--no-download-sd-model\", action='store_true', help=\"don't download SD1.5 model even if no model is found in --ckpt-dir\", default=False)\nparser.add_argument('--subpath', type=str, help='customize the subpath for gradio, use with reverse proxy')\nparser.add_argument('--add-stop-route', action='store_true', help='does not do anything')\nparser.add_argument('--api-server-stop', action='store_true', help='enable server stop/restart/kill via api')\nparser.add_argument('--timeout-keep-alive', type=int, default=30, help='set timeout_keep_alive for uvicorn')\nparser.add_argument(\"--disable-all-extensions\", action='store_true', help=\"prevent all extensions from running regardless of any other settings\", default=False)\nparser.add_argument(\"--disable-extra-extensions\", action='store_true', help=\"prevent all extensions except built-in from running regardless of any other settings\", default=False)\nparser.add_argument(\"--skip-load-model-at-start\", action='store_true', help=\"if load a model at web start, only take effect when --nowebui\")\nparser.add_argument(\"--unix-filenames-sanitization\", action='store_true', help=\"allow any symbols except '/' in filenames. May conflict with your browser and file system\")\nparser.add_argument(\"--filenames-max-length\", type=int, default=128, help='maximal length of filenames of saved images. If you override it, it can conflict with your file system')\nparser.add_argument(\"--no-prompt-history\", action='store_true', help=\"disable read prompt from last generation feature; settings this argument will not create '--data_path/params.txt' file\")\n", "modules/ui_postprocessing.py": "import gradio as gr\nfrom modules import scripts, shared, ui_common, postprocessing, call_queue, ui_toprow\nimport modules.infotext_utils as parameters_copypaste\nfrom modules.ui_components import ResizeHandleRow\n\n\ndef create_ui():\n    dummy_component = gr.Label(visible=False)\n    tab_index = gr.Number(value=0, visible=False)\n\n    with ResizeHandleRow(equal_height=False, variant='compact'):\n        with gr.Column(variant='compact'):\n            with gr.Tabs(elem_id=\"mode_extras\"):\n                with gr.TabItem('Single Image', id=\"single_image\", elem_id=\"extras_single_tab\") as tab_single:\n                    extras_image = gr.Image(label=\"Source\", source=\"upload\", interactive=True, type=\"pil\", elem_id=\"extras_image\", image_mode=\"RGBA\")\n\n                with gr.TabItem('Batch Process', id=\"batch_process\", elem_id=\"extras_batch_process_tab\") as tab_batch:\n                    image_batch = gr.Files(label=\"Batch Process\", interactive=True, elem_id=\"extras_image_batch\")\n\n                with gr.TabItem('Batch from Directory', id=\"batch_from_directory\", elem_id=\"extras_batch_directory_tab\") as tab_batch_dir:\n                    extras_batch_input_dir = gr.Textbox(label=\"Input directory\", **shared.hide_dirs, placeholder=\"A directory on the same machine where the server is running.\", elem_id=\"extras_batch_input_dir\")\n                    extras_batch_output_dir = gr.Textbox(label=\"Output directory\", **shared.hide_dirs, placeholder=\"Leave blank to save images to the default path.\", elem_id=\"extras_batch_output_dir\")\n                    show_extras_results = gr.Checkbox(label='Show result images', value=True, elem_id=\"extras_show_extras_results\")\n\n            script_inputs = scripts.scripts_postproc.setup_ui()\n\n        with gr.Column():\n            toprow = ui_toprow.Toprow(is_compact=True, is_img2img=False, id_part=\"extras\")\n            toprow.create_inline_toprow_image()\n            submit = toprow.submit\n\n            output_panel = ui_common.create_output_panel(\"extras\", shared.opts.outdir_extras_samples)\n\n    tab_single.select(fn=lambda: 0, inputs=[], outputs=[tab_index])\n    tab_batch.select(fn=lambda: 1, inputs=[], outputs=[tab_index])\n    tab_batch_dir.select(fn=lambda: 2, inputs=[], outputs=[tab_index])\n\n    submit.click(\n        fn=call_queue.wrap_gradio_gpu_call(postprocessing.run_postprocessing_webui, extra_outputs=[None, '']),\n        _js=\"submit_extras\",\n        inputs=[\n            dummy_component,\n            tab_index,\n            extras_image,\n            image_batch,\n            extras_batch_input_dir,\n            extras_batch_output_dir,\n            show_extras_results,\n            *script_inputs\n        ],\n        outputs=[\n            output_panel.gallery,\n            output_panel.generation_info,\n            output_panel.html_log,\n        ],\n        show_progress=False,\n    )\n\n    parameters_copypaste.add_paste_fields(\"extras\", extras_image, None)\n\n    extras_image.change(\n        fn=scripts.scripts_postproc.image_changed,\n        inputs=[], outputs=[]\n    )\n", "modules/shared_gradio_themes.py": "import os\n\nimport gradio as gr\n\nfrom modules import errors, shared\nfrom modules.paths_internal import script_path\n\n\n# https://huggingface.co/datasets/freddyaboulton/gradio-theme-subdomains/resolve/main/subdomains.json\ngradio_hf_hub_themes = [\n    \"gradio/base\",\n    \"gradio/glass\",\n    \"gradio/monochrome\",\n    \"gradio/seafoam\",\n    \"gradio/soft\",\n    \"gradio/dracula_test\",\n    \"abidlabs/dracula_test\",\n    \"abidlabs/Lime\",\n    \"abidlabs/pakistan\",\n    \"Ama434/neutral-barlow\",\n    \"dawood/microsoft_windows\",\n    \"finlaymacklon/smooth_slate\",\n    \"Franklisi/darkmode\",\n    \"freddyaboulton/dracula_revamped\",\n    \"freddyaboulton/test-blue\",\n    \"gstaff/xkcd\",\n    \"Insuz/Mocha\",\n    \"Insuz/SimpleIndigo\",\n    \"JohnSmith9982/small_and_pretty\",\n    \"nota-ai/theme\",\n    \"nuttea/Softblue\",\n    \"ParityError/Anime\",\n    \"reilnuud/polite\",\n    \"remilia/Ghostly\",\n    \"rottenlittlecreature/Moon_Goblin\",\n    \"step-3-profit/Midnight-Deep\",\n    \"Taithrah/Minimal\",\n    \"ysharma/huggingface\",\n    \"ysharma/steampunk\",\n    \"NoCrypt/miku\"\n]\n\n\ndef reload_gradio_theme(theme_name=None):\n    if not theme_name:\n        theme_name = shared.opts.gradio_theme\n\n    default_theme_args = dict(\n        font=[\"Source Sans Pro\", 'ui-sans-serif', 'system-ui', 'sans-serif'],\n        font_mono=['IBM Plex Mono', 'ui-monospace', 'Consolas', 'monospace'],\n    )\n\n    if theme_name == \"Default\":\n        shared.gradio_theme = gr.themes.Default(**default_theme_args)\n    else:\n        try:\n            theme_cache_dir = os.path.join(script_path, 'tmp', 'gradio_themes')\n            theme_cache_path = os.path.join(theme_cache_dir, f'{theme_name.replace(\"/\", \"_\")}.json')\n            if shared.opts.gradio_themes_cache and os.path.exists(theme_cache_path):\n                shared.gradio_theme = gr.themes.ThemeClass.load(theme_cache_path)\n            else:\n                os.makedirs(theme_cache_dir, exist_ok=True)\n                shared.gradio_theme = gr.themes.ThemeClass.from_hub(theme_name)\n                shared.gradio_theme.dump(theme_cache_path)\n        except Exception as e:\n            errors.display(e, \"changing gradio theme\")\n            shared.gradio_theme = gr.themes.Default(**default_theme_args)\n\n    # append additional values gradio_theme\n    shared.gradio_theme.sd_webui_modal_lightbox_toolbar_opacity = shared.opts.sd_webui_modal_lightbox_toolbar_opacity\n    shared.gradio_theme.sd_webui_modal_lightbox_icon_opacity = shared.opts.sd_webui_modal_lightbox_icon_opacity\n", "modules/ui.py": "import datetime\nimport mimetypes\nimport os\nimport sys\nfrom functools import reduce\nimport warnings\nfrom contextlib import ExitStack\n\nimport gradio as gr\nimport gradio.utils\nimport numpy as np\nfrom PIL import Image, PngImagePlugin  # noqa: F401\nfrom modules.call_queue import wrap_gradio_gpu_call, wrap_queued_call, wrap_gradio_call\n\nfrom modules import gradio_extensons, sd_schedulers  # noqa: F401\nfrom modules import sd_hijack, sd_models, script_callbacks, ui_extensions, deepbooru, extra_networks, ui_common, ui_postprocessing, progress, ui_loadsave, shared_items, ui_settings, timer, sysinfo, ui_checkpoint_merger, scripts, sd_samplers, processing, ui_extra_networks, ui_toprow, launch_utils\nfrom modules.ui_components import FormRow, FormGroup, ToolButton, FormHTML, InputAccordion, ResizeHandleRow\nfrom modules.paths import script_path\nfrom modules.ui_common import create_refresh_button\nfrom modules.ui_gradio_extensions import reload_javascript\n\nfrom modules.shared import opts, cmd_opts\n\nimport modules.infotext_utils as parameters_copypaste\nimport modules.hypernetworks.ui as hypernetworks_ui\nimport modules.textual_inversion.ui as textual_inversion_ui\nimport modules.textual_inversion.textual_inversion as textual_inversion\nimport modules.shared as shared\nfrom modules import prompt_parser\nfrom modules.sd_hijack import model_hijack\nfrom modules.infotext_utils import image_from_url_text, PasteField\n\ncreate_setting_component = ui_settings.create_setting_component\n\nwarnings.filterwarnings(\"default\" if opts.show_warnings else \"ignore\", category=UserWarning)\nwarnings.filterwarnings(\"default\" if opts.show_gradio_deprecation_warnings else \"ignore\", category=gr.deprecation.GradioDeprecationWarning)\n\n# this is a fix for Windows users. Without it, javascript files will be served with text/html content-type and the browser will not show any UI\nmimetypes.init()\nmimetypes.add_type('application/javascript', '.js')\n\n# Likewise, add explicit content-type header for certain missing image types\nmimetypes.add_type('image/webp', '.webp')\n\nif not cmd_opts.share and not cmd_opts.listen:\n    # fix gradio phoning home\n    gradio.utils.version_check = lambda: None\n    gradio.utils.get_local_ip_address = lambda: '127.0.0.1'\n\nif cmd_opts.ngrok is not None:\n    import modules.ngrok as ngrok\n    print('ngrok authtoken detected, trying to connect...')\n    ngrok.connect(\n        cmd_opts.ngrok,\n        cmd_opts.port if cmd_opts.port is not None else 7860,\n        cmd_opts.ngrok_options\n        )\n\n\ndef gr_show(visible=True):\n    return {\"visible\": visible, \"__type__\": \"update\"}\n\n\nsample_img2img = \"assets/stable-samples/img2img/sketch-mountains-input.jpg\"\nsample_img2img = sample_img2img if os.path.exists(sample_img2img) else None\n\n# Using constants for these since the variation selector isn't visible.\n# Important that they exactly match script.js for tooltip to work.\nrandom_symbol = '\\U0001f3b2\\ufe0f'  # \ud83c\udfb2\ufe0f\nreuse_symbol = '\\u267b\\ufe0f'  # \u267b\ufe0f\npaste_symbol = '\\u2199\\ufe0f'  # \u2199\nrefresh_symbol = '\\U0001f504'  # \ud83d\udd04\nsave_style_symbol = '\\U0001f4be'  # \ud83d\udcbe\napply_style_symbol = '\\U0001f4cb'  # \ud83d\udccb\nclear_prompt_symbol = '\\U0001f5d1\\ufe0f'  # \ud83d\uddd1\ufe0f\nextra_networks_symbol = '\\U0001F3B4'  # \ud83c\udfb4\nswitch_values_symbol = '\\U000021C5' # \u21c5\nrestore_progress_symbol = '\\U0001F300' # \ud83c\udf00\ndetect_image_size_symbol = '\\U0001F4D0'  # \ud83d\udcd0\n\n\nplaintext_to_html = ui_common.plaintext_to_html\n\n\ndef send_gradio_gallery_to_image(x):\n    if len(x) == 0:\n        return None\n    return image_from_url_text(x[0])\n\n\ndef calc_resolution_hires(enable, width, height, hr_scale, hr_resize_x, hr_resize_y):\n    if not enable:\n        return \"\"\n\n    p = processing.StableDiffusionProcessingTxt2Img(width=width, height=height, enable_hr=True, hr_scale=hr_scale, hr_resize_x=hr_resize_x, hr_resize_y=hr_resize_y)\n    p.calculate_target_resolution()\n\n    return f\"from <span class='resolution'>{p.width}x{p.height}</span> to <span class='resolution'>{p.hr_resize_x or p.hr_upscale_to_x}x{p.hr_resize_y or p.hr_upscale_to_y}</span>\"\n\n\ndef resize_from_to_html(width, height, scale_by):\n    target_width = int(width * scale_by)\n    target_height = int(height * scale_by)\n\n    if not target_width or not target_height:\n        return \"no image selected\"\n\n    return f\"resize: from <span class='resolution'>{width}x{height}</span> to <span class='resolution'>{target_width}x{target_height}</span>\"\n\n\ndef process_interrogate(interrogation_function, mode, ii_input_dir, ii_output_dir, *ii_singles):\n    if mode in {0, 1, 3, 4}:\n        return [interrogation_function(ii_singles[mode]), None]\n    elif mode == 2:\n        return [interrogation_function(ii_singles[mode][\"image\"]), None]\n    elif mode == 5:\n        assert not shared.cmd_opts.hide_ui_dir_config, \"Launched with --hide-ui-dir-config, batch img2img disabled\"\n        images = shared.listfiles(ii_input_dir)\n        print(f\"Will process {len(images)} images.\")\n        if ii_output_dir != \"\":\n            os.makedirs(ii_output_dir, exist_ok=True)\n        else:\n            ii_output_dir = ii_input_dir\n\n        for image in images:\n            img = Image.open(image)\n            filename = os.path.basename(image)\n            left, _ = os.path.splitext(filename)\n            print(interrogation_function(img), file=open(os.path.join(ii_output_dir, f\"{left}.txt\"), 'a', encoding='utf-8'))\n\n        return [gr.update(), None]\n\n\ndef interrogate(image):\n    prompt = shared.interrogator.interrogate(image.convert(\"RGB\"))\n    return gr.update() if prompt is None else prompt\n\n\ndef interrogate_deepbooru(image):\n    prompt = deepbooru.model.tag(image)\n    return gr.update() if prompt is None else prompt\n\n\ndef connect_clear_prompt(button):\n    \"\"\"Given clear button, prompt, and token_counter objects, setup clear prompt button click event\"\"\"\n    button.click(\n        _js=\"clear_prompt\",\n        fn=None,\n        inputs=[],\n        outputs=[],\n    )\n\n\ndef update_token_counter(text, steps, styles, *, is_positive=True):\n    params = script_callbacks.BeforeTokenCounterParams(text, steps, styles, is_positive=is_positive)\n    script_callbacks.before_token_counter_callback(params)\n    text = params.prompt\n    steps = params.steps\n    styles = params.styles\n    is_positive = params.is_positive\n\n    if shared.opts.include_styles_into_token_counters:\n        apply_styles = shared.prompt_styles.apply_styles_to_prompt if is_positive else shared.prompt_styles.apply_negative_styles_to_prompt\n        text = apply_styles(text, styles)\n\n    try:\n        text, _ = extra_networks.parse_prompt(text)\n\n        if is_positive:\n            _, prompt_flat_list, _ = prompt_parser.get_multicond_prompt_list([text])\n        else:\n            prompt_flat_list = [text]\n\n        prompt_schedules = prompt_parser.get_learned_conditioning_prompt_schedules(prompt_flat_list, steps)\n\n    except Exception:\n        # a parsing error can happen here during typing, and we don't want to bother the user with\n        # messages related to it in console\n        prompt_schedules = [[[steps, text]]]\n\n    flat_prompts = reduce(lambda list1, list2: list1+list2, prompt_schedules)\n    prompts = [prompt_text for step, prompt_text in flat_prompts]\n    token_count, max_length = max([model_hijack.get_prompt_lengths(prompt) for prompt in prompts], key=lambda args: args[0])\n    return f\"<span class='gr-box gr-text-input'>{token_count}/{max_length}</span>\"\n\n\ndef update_negative_prompt_token_counter(*args):\n    return update_token_counter(*args, is_positive=False)\n\n\ndef setup_progressbar(*args, **kwargs):\n    pass\n\n\ndef apply_setting(key, value):\n    if value is None:\n        return gr.update()\n\n    if shared.cmd_opts.freeze_settings:\n        return gr.update()\n\n    # dont allow model to be swapped when model hash exists in prompt\n    if key == \"sd_model_checkpoint\" and opts.disable_weights_auto_swap:\n        return gr.update()\n\n    if key == \"sd_model_checkpoint\":\n        ckpt_info = sd_models.get_closet_checkpoint_match(value)\n\n        if ckpt_info is not None:\n            value = ckpt_info.title\n        else:\n            return gr.update()\n\n    comp_args = opts.data_labels[key].component_args\n    if comp_args and isinstance(comp_args, dict) and comp_args.get('visible') is False:\n        return\n\n    valtype = type(opts.data_labels[key].default)\n    oldval = opts.data.get(key, None)\n    opts.data[key] = valtype(value) if valtype != type(None) else value\n    if oldval != value and opts.data_labels[key].onchange is not None:\n        opts.data_labels[key].onchange()\n\n    opts.save(shared.config_filename)\n    return getattr(opts, key)\n\n\ndef create_output_panel(tabname, outdir, toprow=None):\n    return ui_common.create_output_panel(tabname, outdir, toprow)\n\n\ndef ordered_ui_categories():\n    user_order = {x.strip(): i * 2 + 1 for i, x in enumerate(shared.opts.ui_reorder_list)}\n\n    for _, category in sorted(enumerate(shared_items.ui_reorder_categories()), key=lambda x: user_order.get(x[1], x[0] * 2 + 0)):\n        yield category\n\n\ndef create_override_settings_dropdown(tabname, row):\n    dropdown = gr.Dropdown([], label=\"Override settings\", visible=False, elem_id=f\"{tabname}_override_settings\", multiselect=True)\n\n    dropdown.change(\n        fn=lambda x: gr.Dropdown.update(visible=bool(x)),\n        inputs=[dropdown],\n        outputs=[dropdown],\n    )\n\n    return dropdown\n\n\ndef create_ui():\n    import modules.img2img\n    import modules.txt2img\n\n    reload_javascript()\n\n    parameters_copypaste.reset()\n\n    settings = ui_settings.UiSettings()\n    settings.register_settings()\n\n    scripts.scripts_current = scripts.scripts_txt2img\n    scripts.scripts_txt2img.initialize_scripts(is_img2img=False)\n\n    with gr.Blocks(analytics_enabled=False) as txt2img_interface:\n        toprow = ui_toprow.Toprow(is_img2img=False, is_compact=shared.opts.compact_prompt_box)\n\n        dummy_component = gr.Label(visible=False)\n\n        extra_tabs = gr.Tabs(elem_id=\"txt2img_extra_tabs\", elem_classes=[\"extra-networks\"])\n        extra_tabs.__enter__()\n\n        with gr.Tab(\"Generation\", id=\"txt2img_generation\") as txt2img_generation_tab, ResizeHandleRow(equal_height=False):\n            with ExitStack() as stack:\n                if shared.opts.txt2img_settings_accordion:\n                    stack.enter_context(gr.Accordion(\"Open for Settings\", open=False))\n                stack.enter_context(gr.Column(variant='compact', elem_id=\"txt2img_settings\"))\n\n                scripts.scripts_txt2img.prepare_ui()\n\n                for category in ordered_ui_categories():\n                    if category == \"prompt\":\n                        toprow.create_inline_toprow_prompts()\n\n                    elif category == \"dimensions\":\n                        with FormRow():\n                            with gr.Column(elem_id=\"txt2img_column_size\", scale=4):\n                                width = gr.Slider(minimum=64, maximum=2048, step=8, label=\"Width\", value=512, elem_id=\"txt2img_width\")\n                                height = gr.Slider(minimum=64, maximum=2048, step=8, label=\"Height\", value=512, elem_id=\"txt2img_height\")\n\n                            with gr.Column(elem_id=\"txt2img_dimensions_row\", scale=1, elem_classes=\"dimensions-tools\"):\n                                res_switch_btn = ToolButton(value=switch_values_symbol, elem_id=\"txt2img_res_switch_btn\", tooltip=\"Switch width/height\")\n\n                            if opts.dimensions_and_batch_together:\n                                with gr.Column(elem_id=\"txt2img_column_batch\"):\n                                    batch_count = gr.Slider(minimum=1, step=1, label='Batch count', value=1, elem_id=\"txt2img_batch_count\")\n                                    batch_size = gr.Slider(minimum=1, maximum=8, step=1, label='Batch size', value=1, elem_id=\"txt2img_batch_size\")\n\n                    elif category == \"cfg\":\n                        with gr.Row():\n                            cfg_scale = gr.Slider(minimum=1.0, maximum=30.0, step=0.5, label='CFG Scale', value=7.0, elem_id=\"txt2img_cfg_scale\")\n\n                    elif category == \"checkboxes\":\n                        with FormRow(elem_classes=\"checkboxes-row\", variant=\"compact\"):\n                            pass\n\n                    elif category == \"accordions\":\n                        with gr.Row(elem_id=\"txt2img_accordions\", elem_classes=\"accordions\"):\n                            with InputAccordion(False, label=\"Hires. fix\", elem_id=\"txt2img_hr\") as enable_hr:\n                                with enable_hr.extra():\n                                    hr_final_resolution = FormHTML(value=\"\", elem_id=\"txtimg_hr_finalres\", label=\"Upscaled resolution\", interactive=False, min_width=0)\n\n                                with FormRow(elem_id=\"txt2img_hires_fix_row1\", variant=\"compact\"):\n                                    hr_upscaler = gr.Dropdown(label=\"Upscaler\", elem_id=\"txt2img_hr_upscaler\", choices=[*shared.latent_upscale_modes, *[x.name for x in shared.sd_upscalers]], value=shared.latent_upscale_default_mode)\n                                    hr_second_pass_steps = gr.Slider(minimum=0, maximum=150, step=1, label='Hires steps', value=0, elem_id=\"txt2img_hires_steps\")\n                                    denoising_strength = gr.Slider(minimum=0.0, maximum=1.0, step=0.01, label='Denoising strength', value=0.7, elem_id=\"txt2img_denoising_strength\")\n\n                                with FormRow(elem_id=\"txt2img_hires_fix_row2\", variant=\"compact\"):\n                                    hr_scale = gr.Slider(minimum=1.0, maximum=4.0, step=0.05, label=\"Upscale by\", value=2.0, elem_id=\"txt2img_hr_scale\")\n                                    hr_resize_x = gr.Slider(minimum=0, maximum=2048, step=8, label=\"Resize width to\", value=0, elem_id=\"txt2img_hr_resize_x\")\n                                    hr_resize_y = gr.Slider(minimum=0, maximum=2048, step=8, label=\"Resize height to\", value=0, elem_id=\"txt2img_hr_resize_y\")\n\n                                with FormRow(elem_id=\"txt2img_hires_fix_row3\", variant=\"compact\", visible=opts.hires_fix_show_sampler) as hr_sampler_container:\n\n                                    hr_checkpoint_name = gr.Dropdown(label='Checkpoint', elem_id=\"hr_checkpoint\", choices=[\"Use same checkpoint\"] + modules.sd_models.checkpoint_tiles(use_short=True), value=\"Use same checkpoint\")\n                                    create_refresh_button(hr_checkpoint_name, modules.sd_models.list_models, lambda: {\"choices\": [\"Use same checkpoint\"] + modules.sd_models.checkpoint_tiles(use_short=True)}, \"hr_checkpoint_refresh\")\n\n                                    hr_sampler_name = gr.Dropdown(label='Hires sampling method', elem_id=\"hr_sampler\", choices=[\"Use same sampler\"] + sd_samplers.visible_sampler_names(), value=\"Use same sampler\")\n                                    hr_scheduler = gr.Dropdown(label='Hires schedule type', elem_id=\"hr_scheduler\", choices=[\"Use same scheduler\"] + [x.label for x in sd_schedulers.schedulers], value=\"Use same scheduler\")\n\n                                with FormRow(elem_id=\"txt2img_hires_fix_row4\", variant=\"compact\", visible=opts.hires_fix_show_prompts) as hr_prompts_container:\n                                    with gr.Column(scale=80):\n                                        with gr.Row():\n                                            hr_prompt = gr.Textbox(label=\"Hires prompt\", elem_id=\"hires_prompt\", show_label=False, lines=3, placeholder=\"Prompt for hires fix pass.\\nLeave empty to use the same prompt as in first pass.\", elem_classes=[\"prompt\"])\n                                    with gr.Column(scale=80):\n                                        with gr.Row():\n                                            hr_negative_prompt = gr.Textbox(label=\"Hires negative prompt\", elem_id=\"hires_neg_prompt\", show_label=False, lines=3, placeholder=\"Negative prompt for hires fix pass.\\nLeave empty to use the same negative prompt as in first pass.\", elem_classes=[\"prompt\"])\n\n                            scripts.scripts_txt2img.setup_ui_for_section(category)\n\n                    elif category == \"batch\":\n                        if not opts.dimensions_and_batch_together:\n                            with FormRow(elem_id=\"txt2img_column_batch\"):\n                                batch_count = gr.Slider(minimum=1, step=1, label='Batch count', value=1, elem_id=\"txt2img_batch_count\")\n                                batch_size = gr.Slider(minimum=1, maximum=8, step=1, label='Batch size', value=1, elem_id=\"txt2img_batch_size\")\n\n                    elif category == \"override_settings\":\n                        with FormRow(elem_id=\"txt2img_override_settings_row\") as row:\n                            override_settings = create_override_settings_dropdown('txt2img', row)\n\n                    elif category == \"scripts\":\n                        with FormGroup(elem_id=\"txt2img_script_container\"):\n                            custom_inputs = scripts.scripts_txt2img.setup_ui()\n\n                    if category not in {\"accordions\"}:\n                        scripts.scripts_txt2img.setup_ui_for_section(category)\n\n            hr_resolution_preview_inputs = [enable_hr, width, height, hr_scale, hr_resize_x, hr_resize_y]\n\n            for component in hr_resolution_preview_inputs:\n                event = component.release if isinstance(component, gr.Slider) else component.change\n\n                event(\n                    fn=calc_resolution_hires,\n                    inputs=hr_resolution_preview_inputs,\n                    outputs=[hr_final_resolution],\n                    show_progress=False,\n                )\n                event(\n                    None,\n                    _js=\"onCalcResolutionHires\",\n                    inputs=hr_resolution_preview_inputs,\n                    outputs=[],\n                    show_progress=False,\n                )\n\n            output_panel = create_output_panel(\"txt2img\", opts.outdir_txt2img_samples, toprow)\n\n            txt2img_inputs = [\n                dummy_component,\n                toprow.prompt,\n                toprow.negative_prompt,\n                toprow.ui_styles.dropdown,\n                batch_count,\n                batch_size,\n                cfg_scale,\n                height,\n                width,\n                enable_hr,\n                denoising_strength,\n                hr_scale,\n                hr_upscaler,\n                hr_second_pass_steps,\n                hr_resize_x,\n                hr_resize_y,\n                hr_checkpoint_name,\n                hr_sampler_name,\n                hr_scheduler,\n                hr_prompt,\n                hr_negative_prompt,\n                override_settings,\n            ] + custom_inputs\n\n            txt2img_outputs = [\n                output_panel.gallery,\n                output_panel.generation_info,\n                output_panel.infotext,\n                output_panel.html_log,\n            ]\n\n            txt2img_args = dict(\n                fn=wrap_gradio_gpu_call(modules.txt2img.txt2img, extra_outputs=[None, '', '']),\n                _js=\"submit\",\n                inputs=txt2img_inputs,\n                outputs=txt2img_outputs,\n                show_progress=False,\n            )\n\n            toprow.prompt.submit(**txt2img_args)\n            toprow.submit.click(**txt2img_args)\n\n            output_panel.button_upscale.click(\n                fn=wrap_gradio_gpu_call(modules.txt2img.txt2img_upscale, extra_outputs=[None, '', '']),\n                _js=\"submit_txt2img_upscale\",\n                inputs=txt2img_inputs[0:1] + [output_panel.gallery, dummy_component, output_panel.generation_info] + txt2img_inputs[1:],\n                outputs=txt2img_outputs,\n                show_progress=False,\n            )\n\n            res_switch_btn.click(fn=None, _js=\"function(){switchWidthHeight('txt2img')}\", inputs=None, outputs=None, show_progress=False)\n\n            toprow.restore_progress_button.click(\n                fn=progress.restore_progress,\n                _js=\"restoreProgressTxt2img\",\n                inputs=[dummy_component],\n                outputs=[\n                    output_panel.gallery,\n                    output_panel.generation_info,\n                    output_panel.infotext,\n                    output_panel.html_log,\n                ],\n                show_progress=False,\n            )\n\n            txt2img_paste_fields = [\n                PasteField(toprow.prompt, \"Prompt\", api=\"prompt\"),\n                PasteField(toprow.negative_prompt, \"Negative prompt\", api=\"negative_prompt\"),\n                PasteField(cfg_scale, \"CFG scale\", api=\"cfg_scale\"),\n                PasteField(width, \"Size-1\", api=\"width\"),\n                PasteField(height, \"Size-2\", api=\"height\"),\n                PasteField(batch_size, \"Batch size\", api=\"batch_size\"),\n                PasteField(toprow.ui_styles.dropdown, lambda d: d[\"Styles array\"] if isinstance(d.get(\"Styles array\"), list) else gr.update(), api=\"styles\"),\n                PasteField(denoising_strength, \"Denoising strength\", api=\"denoising_strength\"),\n                PasteField(enable_hr, lambda d: \"Denoising strength\" in d and (\"Hires upscale\" in d or \"Hires upscaler\" in d or \"Hires resize-1\" in d), api=\"enable_hr\"),\n                PasteField(hr_scale, \"Hires upscale\", api=\"hr_scale\"),\n                PasteField(hr_upscaler, \"Hires upscaler\", api=\"hr_upscaler\"),\n                PasteField(hr_second_pass_steps, \"Hires steps\", api=\"hr_second_pass_steps\"),\n                PasteField(hr_resize_x, \"Hires resize-1\", api=\"hr_resize_x\"),\n                PasteField(hr_resize_y, \"Hires resize-2\", api=\"hr_resize_y\"),\n                PasteField(hr_checkpoint_name, \"Hires checkpoint\", api=\"hr_checkpoint_name\"),\n                PasteField(hr_sampler_name, sd_samplers.get_hr_sampler_from_infotext, api=\"hr_sampler_name\"),\n                PasteField(hr_scheduler, sd_samplers.get_hr_scheduler_from_infotext, api=\"hr_scheduler\"),\n                PasteField(hr_sampler_container, lambda d: gr.update(visible=True) if d.get(\"Hires sampler\", \"Use same sampler\") != \"Use same sampler\" or d.get(\"Hires checkpoint\", \"Use same checkpoint\") != \"Use same checkpoint\" or d.get(\"Hires schedule type\", \"Use same scheduler\") != \"Use same scheduler\" else gr.update()),\n                PasteField(hr_prompt, \"Hires prompt\", api=\"hr_prompt\"),\n                PasteField(hr_negative_prompt, \"Hires negative prompt\", api=\"hr_negative_prompt\"),\n                PasteField(hr_prompts_container, lambda d: gr.update(visible=True) if d.get(\"Hires prompt\", \"\") != \"\" or d.get(\"Hires negative prompt\", \"\") != \"\" else gr.update()),\n                *scripts.scripts_txt2img.infotext_fields\n            ]\n            parameters_copypaste.add_paste_fields(\"txt2img\", None, txt2img_paste_fields, override_settings)\n            parameters_copypaste.register_paste_params_button(parameters_copypaste.ParamBinding(\n                paste_button=toprow.paste, tabname=\"txt2img\", source_text_component=toprow.prompt, source_image_component=None,\n            ))\n\n            steps = scripts.scripts_txt2img.script('Sampler').steps\n\n            txt2img_preview_params = [\n                toprow.prompt,\n                toprow.negative_prompt,\n                steps,\n                scripts.scripts_txt2img.script('Sampler').sampler_name,\n                cfg_scale,\n                scripts.scripts_txt2img.script('Seed').seed,\n                width,\n                height,\n            ]\n\n            toprow.ui_styles.dropdown.change(fn=wrap_queued_call(update_token_counter), inputs=[toprow.prompt, steps, toprow.ui_styles.dropdown], outputs=[toprow.token_counter])\n            toprow.ui_styles.dropdown.change(fn=wrap_queued_call(update_negative_prompt_token_counter), inputs=[toprow.negative_prompt, steps, toprow.ui_styles.dropdown], outputs=[toprow.negative_token_counter])\n            toprow.token_button.click(fn=wrap_queued_call(update_token_counter), inputs=[toprow.prompt, steps, toprow.ui_styles.dropdown], outputs=[toprow.token_counter])\n            toprow.negative_token_button.click(fn=wrap_queued_call(update_negative_prompt_token_counter), inputs=[toprow.negative_prompt, steps, toprow.ui_styles.dropdown], outputs=[toprow.negative_token_counter])\n\n        extra_networks_ui = ui_extra_networks.create_ui(txt2img_interface, [txt2img_generation_tab], 'txt2img')\n        ui_extra_networks.setup_ui(extra_networks_ui, output_panel.gallery)\n\n        extra_tabs.__exit__()\n\n    scripts.scripts_current = scripts.scripts_img2img\n    scripts.scripts_img2img.initialize_scripts(is_img2img=True)\n\n    with gr.Blocks(analytics_enabled=False) as img2img_interface:\n        toprow = ui_toprow.Toprow(is_img2img=True, is_compact=shared.opts.compact_prompt_box)\n\n        extra_tabs = gr.Tabs(elem_id=\"img2img_extra_tabs\", elem_classes=[\"extra-networks\"])\n        extra_tabs.__enter__()\n\n        with gr.Tab(\"Generation\", id=\"img2img_generation\") as img2img_generation_tab, ResizeHandleRow(equal_height=False):\n            with ExitStack() as stack:\n                if shared.opts.img2img_settings_accordion:\n                    stack.enter_context(gr.Accordion(\"Open for Settings\", open=False))\n                stack.enter_context(gr.Column(variant='compact', elem_id=\"img2img_settings\"))\n\n                copy_image_buttons = []\n                copy_image_destinations = {}\n\n                def add_copy_image_controls(tab_name, elem):\n                    with gr.Row(variant=\"compact\", elem_id=f\"img2img_copy_to_{tab_name}\"):\n                        gr.HTML(\"Copy image to: \", elem_id=f\"img2img_label_copy_to_{tab_name}\")\n\n                        for title, name in zip(['img2img', 'sketch', 'inpaint', 'inpaint sketch'], ['img2img', 'sketch', 'inpaint', 'inpaint_sketch']):\n                            if name == tab_name:\n                                gr.Button(title, interactive=False)\n                                copy_image_destinations[name] = elem\n                                continue\n\n                            button = gr.Button(title)\n                            copy_image_buttons.append((button, name, elem))\n\n                scripts.scripts_img2img.prepare_ui()\n\n                for category in ordered_ui_categories():\n                    if category == \"prompt\":\n                        toprow.create_inline_toprow_prompts()\n\n                    if category == \"image\":\n                        with gr.Tabs(elem_id=\"mode_img2img\"):\n                            img2img_selected_tab = gr.Number(value=0, visible=False)\n\n                            with gr.TabItem('img2img', id='img2img', elem_id=\"img2img_img2img_tab\") as tab_img2img:\n                                init_img = gr.Image(label=\"Image for img2img\", elem_id=\"img2img_image\", show_label=False, source=\"upload\", interactive=True, type=\"pil\", tool=\"editor\", image_mode=\"RGBA\", height=opts.img2img_editor_height)\n                                add_copy_image_controls('img2img', init_img)\n\n                            with gr.TabItem('Sketch', id='img2img_sketch', elem_id=\"img2img_img2img_sketch_tab\") as tab_sketch:\n                                sketch = gr.Image(label=\"Image for img2img\", elem_id=\"img2img_sketch\", show_label=False, source=\"upload\", interactive=True, type=\"pil\", tool=\"color-sketch\", image_mode=\"RGB\", height=opts.img2img_editor_height, brush_color=opts.img2img_sketch_default_brush_color)\n                                add_copy_image_controls('sketch', sketch)\n\n                            with gr.TabItem('Inpaint', id='inpaint', elem_id=\"img2img_inpaint_tab\") as tab_inpaint:\n                                init_img_with_mask = gr.Image(label=\"Image for inpainting with mask\", show_label=False, elem_id=\"img2maskimg\", source=\"upload\", interactive=True, type=\"pil\", tool=\"sketch\", image_mode=\"RGBA\", height=opts.img2img_editor_height, brush_color=opts.img2img_inpaint_mask_brush_color)\n                                add_copy_image_controls('inpaint', init_img_with_mask)\n\n                            with gr.TabItem('Inpaint sketch', id='inpaint_sketch', elem_id=\"img2img_inpaint_sketch_tab\") as tab_inpaint_color:\n                                inpaint_color_sketch = gr.Image(label=\"Color sketch inpainting\", show_label=False, elem_id=\"inpaint_sketch\", source=\"upload\", interactive=True, type=\"pil\", tool=\"color-sketch\", image_mode=\"RGB\", height=opts.img2img_editor_height, brush_color=opts.img2img_inpaint_sketch_default_brush_color)\n                                inpaint_color_sketch_orig = gr.State(None)\n                                add_copy_image_controls('inpaint_sketch', inpaint_color_sketch)\n\n                                def update_orig(image, state):\n                                    if image is not None:\n                                        same_size = state is not None and state.size == image.size\n                                        has_exact_match = np.any(np.all(np.array(image) == np.array(state), axis=-1))\n                                        edited = same_size and has_exact_match\n                                        return image if not edited or state is None else state\n\n                                inpaint_color_sketch.change(update_orig, [inpaint_color_sketch, inpaint_color_sketch_orig], inpaint_color_sketch_orig)\n\n                            with gr.TabItem('Inpaint upload', id='inpaint_upload', elem_id=\"img2img_inpaint_upload_tab\") as tab_inpaint_upload:\n                                init_img_inpaint = gr.Image(label=\"Image for img2img\", show_label=False, source=\"upload\", interactive=True, type=\"pil\", elem_id=\"img_inpaint_base\")\n                                init_mask_inpaint = gr.Image(label=\"Mask\", source=\"upload\", interactive=True, type=\"pil\", image_mode=\"RGBA\", elem_id=\"img_inpaint_mask\")\n\n                            with gr.TabItem('Batch', id='batch', elem_id=\"img2img_batch_tab\") as tab_batch:\n                                hidden = '<br>Disabled when launched with --hide-ui-dir-config.' if shared.cmd_opts.hide_ui_dir_config else ''\n                                gr.HTML(\n                                    \"<p style='padding-bottom: 1em;' class=\\\"text-gray-500\\\">Process images in a directory on the same machine where the server is running.\" +\n                                    \"<br>Use an empty output directory to save pictures normally instead of writing to the output directory.\" +\n                                    f\"<br>Add inpaint batch mask directory to enable inpaint batch processing.\"\n                                    f\"{hidden}</p>\"\n                                )\n                                img2img_batch_input_dir = gr.Textbox(label=\"Input directory\", **shared.hide_dirs, elem_id=\"img2img_batch_input_dir\")\n                                img2img_batch_output_dir = gr.Textbox(label=\"Output directory\", **shared.hide_dirs, elem_id=\"img2img_batch_output_dir\")\n                                img2img_batch_inpaint_mask_dir = gr.Textbox(label=\"Inpaint batch mask directory (required for inpaint batch processing only)\", **shared.hide_dirs, elem_id=\"img2img_batch_inpaint_mask_dir\")\n                                with gr.Accordion(\"PNG info\", open=False):\n                                    img2img_batch_use_png_info = gr.Checkbox(label=\"Append png info to prompts\", **shared.hide_dirs, elem_id=\"img2img_batch_use_png_info\")\n                                    img2img_batch_png_info_dir = gr.Textbox(label=\"PNG info directory\", **shared.hide_dirs, placeholder=\"Leave empty to use input directory\", elem_id=\"img2img_batch_png_info_dir\")\n                                    img2img_batch_png_info_props = gr.CheckboxGroup([\"Prompt\", \"Negative prompt\", \"Seed\", \"CFG scale\", \"Sampler\", \"Steps\", \"Model hash\"], label=\"Parameters to take from png info\", info=\"Prompts from png info will be appended to prompts set in ui.\")\n\n                            img2img_tabs = [tab_img2img, tab_sketch, tab_inpaint, tab_inpaint_color, tab_inpaint_upload, tab_batch]\n\n                            for i, tab in enumerate(img2img_tabs):\n                                tab.select(fn=lambda tabnum=i: tabnum, inputs=[], outputs=[img2img_selected_tab])\n\n                        def copy_image(img):\n                            if isinstance(img, dict) and 'image' in img:\n                                return img['image']\n\n                            return img\n\n                        for button, name, elem in copy_image_buttons:\n                            button.click(\n                                fn=copy_image,\n                                inputs=[elem],\n                                outputs=[copy_image_destinations[name]],\n                            )\n                            button.click(\n                                fn=lambda: None,\n                                _js=f\"switch_to_{name.replace(' ', '_')}\",\n                                inputs=[],\n                                outputs=[],\n                            )\n\n                        with FormRow():\n                            resize_mode = gr.Radio(label=\"Resize mode\", elem_id=\"resize_mode\", choices=[\"Just resize\", \"Crop and resize\", \"Resize and fill\", \"Just resize (latent upscale)\"], type=\"index\", value=\"Just resize\")\n\n                    elif category == \"dimensions\":\n                        with FormRow():\n                            with gr.Column(elem_id=\"img2img_column_size\", scale=4):\n                                selected_scale_tab = gr.Number(value=0, visible=False)\n\n                                with gr.Tabs():\n                                    with gr.Tab(label=\"Resize to\", elem_id=\"img2img_tab_resize_to\") as tab_scale_to:\n                                        with FormRow():\n                                            with gr.Column(elem_id=\"img2img_column_size\", scale=4):\n                                                width = gr.Slider(minimum=64, maximum=2048, step=8, label=\"Width\", value=512, elem_id=\"img2img_width\")\n                                                height = gr.Slider(minimum=64, maximum=2048, step=8, label=\"Height\", value=512, elem_id=\"img2img_height\")\n                                            with gr.Column(elem_id=\"img2img_dimensions_row\", scale=1, elem_classes=\"dimensions-tools\"):\n                                                res_switch_btn = ToolButton(value=switch_values_symbol, elem_id=\"img2img_res_switch_btn\", tooltip=\"Switch width/height\")\n                                                detect_image_size_btn = ToolButton(value=detect_image_size_symbol, elem_id=\"img2img_detect_image_size_btn\", tooltip=\"Auto detect size from img2img\")\n\n                                    with gr.Tab(label=\"Resize by\", elem_id=\"img2img_tab_resize_by\") as tab_scale_by:\n                                        scale_by = gr.Slider(minimum=0.05, maximum=4.0, step=0.05, label=\"Scale\", value=1.0, elem_id=\"img2img_scale\")\n\n                                        with FormRow():\n                                            scale_by_html = FormHTML(resize_from_to_html(0, 0, 0.0), elem_id=\"img2img_scale_resolution_preview\")\n                                            gr.Slider(label=\"Unused\", elem_id=\"img2img_unused_scale_by_slider\")\n                                            button_update_resize_to = gr.Button(visible=False, elem_id=\"img2img_update_resize_to\")\n\n                                    on_change_args = dict(\n                                        fn=resize_from_to_html,\n                                        _js=\"currentImg2imgSourceResolution\",\n                                        inputs=[dummy_component, dummy_component, scale_by],\n                                        outputs=scale_by_html,\n                                        show_progress=False,\n                                    )\n\n                                    scale_by.release(**on_change_args)\n                                    button_update_resize_to.click(**on_change_args)\n\n                            tab_scale_to.select(fn=lambda: 0, inputs=[], outputs=[selected_scale_tab])\n                            tab_scale_by.select(fn=lambda: 1, inputs=[], outputs=[selected_scale_tab])\n\n                            if opts.dimensions_and_batch_together:\n                                with gr.Column(elem_id=\"img2img_column_batch\"):\n                                    batch_count = gr.Slider(minimum=1, step=1, label='Batch count', value=1, elem_id=\"img2img_batch_count\")\n                                    batch_size = gr.Slider(minimum=1, maximum=8, step=1, label='Batch size', value=1, elem_id=\"img2img_batch_size\")\n\n                    elif category == \"denoising\":\n                        denoising_strength = gr.Slider(minimum=0.0, maximum=1.0, step=0.01, label='Denoising strength', value=0.75, elem_id=\"img2img_denoising_strength\")\n\n                    elif category == \"cfg\":\n                        with gr.Row():\n                            cfg_scale = gr.Slider(minimum=1.0, maximum=30.0, step=0.5, label='CFG Scale', value=7.0, elem_id=\"img2img_cfg_scale\")\n                            image_cfg_scale = gr.Slider(minimum=0, maximum=3.0, step=0.05, label='Image CFG Scale', value=1.5, elem_id=\"img2img_image_cfg_scale\", visible=False)\n\n                    elif category == \"checkboxes\":\n                        with FormRow(elem_classes=\"checkboxes-row\", variant=\"compact\"):\n                            pass\n\n                    elif category == \"accordions\":\n                        with gr.Row(elem_id=\"img2img_accordions\", elem_classes=\"accordions\"):\n                            scripts.scripts_img2img.setup_ui_for_section(category)\n\n                    elif category == \"batch\":\n                        if not opts.dimensions_and_batch_together:\n                            with FormRow(elem_id=\"img2img_column_batch\"):\n                                batch_count = gr.Slider(minimum=1, step=1, label='Batch count', value=1, elem_id=\"img2img_batch_count\")\n                                batch_size = gr.Slider(minimum=1, maximum=8, step=1, label='Batch size', value=1, elem_id=\"img2img_batch_size\")\n\n                    elif category == \"override_settings\":\n                        with FormRow(elem_id=\"img2img_override_settings_row\") as row:\n                            override_settings = create_override_settings_dropdown('img2img', row)\n\n                    elif category == \"scripts\":\n                        with FormGroup(elem_id=\"img2img_script_container\"):\n                            custom_inputs = scripts.scripts_img2img.setup_ui()\n\n                    elif category == \"inpaint\":\n                        with FormGroup(elem_id=\"inpaint_controls\", visible=False) as inpaint_controls:\n                            with FormRow():\n                                mask_blur = gr.Slider(label='Mask blur', minimum=0, maximum=64, step=1, value=4, elem_id=\"img2img_mask_blur\")\n                                mask_alpha = gr.Slider(label=\"Mask transparency\", visible=False, elem_id=\"img2img_mask_alpha\")\n\n                            with FormRow():\n                                inpainting_mask_invert = gr.Radio(label='Mask mode', choices=['Inpaint masked', 'Inpaint not masked'], value='Inpaint masked', type=\"index\", elem_id=\"img2img_mask_mode\")\n\n                            with FormRow():\n                                inpainting_fill = gr.Radio(label='Masked content', choices=['fill', 'original', 'latent noise', 'latent nothing'], value='original', type=\"index\", elem_id=\"img2img_inpainting_fill\")\n\n                            with FormRow():\n                                with gr.Column():\n                                    inpaint_full_res = gr.Radio(label=\"Inpaint area\", choices=[\"Whole picture\", \"Only masked\"], type=\"index\", value=\"Whole picture\", elem_id=\"img2img_inpaint_full_res\")\n\n                                with gr.Column(scale=4):\n                                    inpaint_full_res_padding = gr.Slider(label='Only masked padding, pixels', minimum=0, maximum=256, step=4, value=32, elem_id=\"img2img_inpaint_full_res_padding\")\n\n                    if category not in {\"accordions\"}:\n                        scripts.scripts_img2img.setup_ui_for_section(category)\n\n            # the code below is meant to update the resolution label after the image in the image selection UI has changed.\n            # as it is now the event keeps firing continuously for inpaint edits, which ruins the page with constant requests.\n            # I assume this must be a gradio bug and for now we'll just do it for non-inpaint inputs.\n            for component in [init_img, sketch]:\n                component.change(fn=lambda: None, _js=\"updateImg2imgResizeToTextAfterChangingImage\", inputs=[], outputs=[], show_progress=False)\n\n            def select_img2img_tab(tab):\n                return gr.update(visible=tab in [2, 3, 4]), gr.update(visible=tab == 3),\n\n            for i, elem in enumerate(img2img_tabs):\n                elem.select(\n                    fn=lambda tab=i: select_img2img_tab(tab),\n                    inputs=[],\n                    outputs=[inpaint_controls, mask_alpha],\n                )\n\n            output_panel = create_output_panel(\"img2img\", opts.outdir_img2img_samples, toprow)\n\n            img2img_args = dict(\n                fn=wrap_gradio_gpu_call(modules.img2img.img2img, extra_outputs=[None, '', '']),\n                _js=\"submit_img2img\",\n                inputs=[\n                    dummy_component,\n                    dummy_component,\n                    toprow.prompt,\n                    toprow.negative_prompt,\n                    toprow.ui_styles.dropdown,\n                    init_img,\n                    sketch,\n                    init_img_with_mask,\n                    inpaint_color_sketch,\n                    inpaint_color_sketch_orig,\n                    init_img_inpaint,\n                    init_mask_inpaint,\n                    mask_blur,\n                    mask_alpha,\n                    inpainting_fill,\n                    batch_count,\n                    batch_size,\n                    cfg_scale,\n                    image_cfg_scale,\n                    denoising_strength,\n                    selected_scale_tab,\n                    height,\n                    width,\n                    scale_by,\n                    resize_mode,\n                    inpaint_full_res,\n                    inpaint_full_res_padding,\n                    inpainting_mask_invert,\n                    img2img_batch_input_dir,\n                    img2img_batch_output_dir,\n                    img2img_batch_inpaint_mask_dir,\n                    override_settings,\n                    img2img_batch_use_png_info,\n                    img2img_batch_png_info_props,\n                    img2img_batch_png_info_dir,\n                ] + custom_inputs,\n                outputs=[\n                    output_panel.gallery,\n                    output_panel.generation_info,\n                    output_panel.infotext,\n                    output_panel.html_log,\n                ],\n                show_progress=False,\n            )\n\n            interrogate_args = dict(\n                _js=\"get_img2img_tab_index\",\n                inputs=[\n                    dummy_component,\n                    img2img_batch_input_dir,\n                    img2img_batch_output_dir,\n                    init_img,\n                    sketch,\n                    init_img_with_mask,\n                    inpaint_color_sketch,\n                    init_img_inpaint,\n                ],\n                outputs=[toprow.prompt, dummy_component],\n            )\n\n            toprow.prompt.submit(**img2img_args)\n            toprow.submit.click(**img2img_args)\n\n            res_switch_btn.click(fn=None, _js=\"function(){switchWidthHeight('img2img')}\", inputs=None, outputs=None, show_progress=False)\n\n            detect_image_size_btn.click(\n                fn=lambda w, h, _: (w or gr.update(), h or gr.update()),\n                _js=\"currentImg2imgSourceResolution\",\n                inputs=[dummy_component, dummy_component, dummy_component],\n                outputs=[width, height],\n                show_progress=False,\n            )\n\n            toprow.restore_progress_button.click(\n                fn=progress.restore_progress,\n                _js=\"restoreProgressImg2img\",\n                inputs=[dummy_component],\n                outputs=[\n                    output_panel.gallery,\n                    output_panel.generation_info,\n                    output_panel.infotext,\n                    output_panel.html_log,\n                ],\n                show_progress=False,\n            )\n\n            toprow.button_interrogate.click(\n                fn=lambda *args: process_interrogate(interrogate, *args),\n                **interrogate_args,\n            )\n\n            toprow.button_deepbooru.click(\n                fn=lambda *args: process_interrogate(interrogate_deepbooru, *args),\n                **interrogate_args,\n            )\n\n            steps = scripts.scripts_img2img.script('Sampler').steps\n\n            toprow.ui_styles.dropdown.change(fn=wrap_queued_call(update_token_counter), inputs=[toprow.prompt, steps, toprow.ui_styles.dropdown], outputs=[toprow.token_counter])\n            toprow.ui_styles.dropdown.change(fn=wrap_queued_call(update_negative_prompt_token_counter), inputs=[toprow.negative_prompt, steps, toprow.ui_styles.dropdown], outputs=[toprow.negative_token_counter])\n            toprow.token_button.click(fn=update_token_counter, inputs=[toprow.prompt, steps, toprow.ui_styles.dropdown], outputs=[toprow.token_counter])\n            toprow.negative_token_button.click(fn=wrap_queued_call(update_negative_prompt_token_counter), inputs=[toprow.negative_prompt, steps, toprow.ui_styles.dropdown], outputs=[toprow.negative_token_counter])\n\n            img2img_paste_fields = [\n                (toprow.prompt, \"Prompt\"),\n                (toprow.negative_prompt, \"Negative prompt\"),\n                (cfg_scale, \"CFG scale\"),\n                (image_cfg_scale, \"Image CFG scale\"),\n                (width, \"Size-1\"),\n                (height, \"Size-2\"),\n                (batch_size, \"Batch size\"),\n                (toprow.ui_styles.dropdown, lambda d: d[\"Styles array\"] if isinstance(d.get(\"Styles array\"), list) else gr.update()),\n                (denoising_strength, \"Denoising strength\"),\n                (mask_blur, \"Mask blur\"),\n                (inpainting_mask_invert, 'Mask mode'),\n                (inpainting_fill, 'Masked content'),\n                (inpaint_full_res, 'Inpaint area'),\n                (inpaint_full_res_padding, 'Masked area padding'),\n                *scripts.scripts_img2img.infotext_fields\n            ]\n            parameters_copypaste.add_paste_fields(\"img2img\", init_img, img2img_paste_fields, override_settings)\n            parameters_copypaste.add_paste_fields(\"inpaint\", init_img_with_mask, img2img_paste_fields, override_settings)\n            parameters_copypaste.register_paste_params_button(parameters_copypaste.ParamBinding(\n                paste_button=toprow.paste, tabname=\"img2img\", source_text_component=toprow.prompt, source_image_component=None,\n            ))\n\n        extra_networks_ui_img2img = ui_extra_networks.create_ui(img2img_interface, [img2img_generation_tab], 'img2img')\n        ui_extra_networks.setup_ui(extra_networks_ui_img2img, output_panel.gallery)\n\n        extra_tabs.__exit__()\n\n    scripts.scripts_current = None\n\n    with gr.Blocks(analytics_enabled=False) as extras_interface:\n        ui_postprocessing.create_ui()\n\n    with gr.Blocks(analytics_enabled=False) as pnginfo_interface:\n        with ResizeHandleRow(equal_height=False):\n            with gr.Column(variant='panel'):\n                image = gr.Image(elem_id=\"pnginfo_image\", label=\"Source\", source=\"upload\", interactive=True, type=\"pil\")\n\n            with gr.Column(variant='panel'):\n                html = gr.HTML()\n                generation_info = gr.Textbox(visible=False, elem_id=\"pnginfo_generation_info\")\n                html2 = gr.HTML()\n                with gr.Row():\n                    buttons = parameters_copypaste.create_buttons([\"txt2img\", \"img2img\", \"inpaint\", \"extras\"])\n\n                for tabname, button in buttons.items():\n                    parameters_copypaste.register_paste_params_button(parameters_copypaste.ParamBinding(\n                        paste_button=button, tabname=tabname, source_text_component=generation_info, source_image_component=image,\n                    ))\n\n        image.change(\n            fn=wrap_gradio_call(modules.extras.run_pnginfo),\n            inputs=[image],\n            outputs=[html, generation_info, html2],\n        )\n\n    modelmerger_ui = ui_checkpoint_merger.UiCheckpointMerger()\n\n    with gr.Blocks(analytics_enabled=False) as train_interface:\n        with gr.Row(equal_height=False):\n            gr.HTML(value=\"<p style='margin-bottom: 0.7em'>See <b><a href=\\\"https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Textual-Inversion\\\">wiki</a></b> for detailed explanation.</p>\")\n\n        with ResizeHandleRow(variant=\"compact\", equal_height=False):\n            with gr.Tabs(elem_id=\"train_tabs\"):\n\n                with gr.Tab(label=\"Create embedding\", id=\"create_embedding\"):\n                    new_embedding_name = gr.Textbox(label=\"Name\", elem_id=\"train_new_embedding_name\")\n                    initialization_text = gr.Textbox(label=\"Initialization text\", value=\"*\", elem_id=\"train_initialization_text\")\n                    nvpt = gr.Slider(label=\"Number of vectors per token\", minimum=1, maximum=75, step=1, value=1, elem_id=\"train_nvpt\")\n                    overwrite_old_embedding = gr.Checkbox(value=False, label=\"Overwrite Old Embedding\", elem_id=\"train_overwrite_old_embedding\")\n\n                    with gr.Row():\n                        with gr.Column(scale=3):\n                            gr.HTML(value=\"\")\n\n                        with gr.Column():\n                            create_embedding = gr.Button(value=\"Create embedding\", variant='primary', elem_id=\"train_create_embedding\")\n\n                with gr.Tab(label=\"Create hypernetwork\", id=\"create_hypernetwork\"):\n                    new_hypernetwork_name = gr.Textbox(label=\"Name\", elem_id=\"train_new_hypernetwork_name\")\n                    new_hypernetwork_sizes = gr.CheckboxGroup(label=\"Modules\", value=[\"768\", \"320\", \"640\", \"1280\"], choices=[\"768\", \"1024\", \"320\", \"640\", \"1280\"], elem_id=\"train_new_hypernetwork_sizes\")\n                    new_hypernetwork_layer_structure = gr.Textbox(\"1, 2, 1\", label=\"Enter hypernetwork layer structure\", placeholder=\"1st and last digit must be 1. ex:'1, 2, 1'\", elem_id=\"train_new_hypernetwork_layer_structure\")\n                    new_hypernetwork_activation_func = gr.Dropdown(value=\"linear\", label=\"Select activation function of hypernetwork. Recommended : Swish / Linear(none)\", choices=hypernetworks_ui.keys, elem_id=\"train_new_hypernetwork_activation_func\")\n                    new_hypernetwork_initialization_option = gr.Dropdown(value = \"Normal\", label=\"Select Layer weights initialization. Recommended: Kaiming for relu-like, Xavier for sigmoid-like, Normal otherwise\", choices=[\"Normal\", \"KaimingUniform\", \"KaimingNormal\", \"XavierUniform\", \"XavierNormal\"], elem_id=\"train_new_hypernetwork_initialization_option\")\n                    new_hypernetwork_add_layer_norm = gr.Checkbox(label=\"Add layer normalization\", elem_id=\"train_new_hypernetwork_add_layer_norm\")\n                    new_hypernetwork_use_dropout = gr.Checkbox(label=\"Use dropout\", elem_id=\"train_new_hypernetwork_use_dropout\")\n                    new_hypernetwork_dropout_structure = gr.Textbox(\"0, 0, 0\", label=\"Enter hypernetwork Dropout structure (or empty). Recommended : 0~0.35 incrementing sequence: 0, 0.05, 0.15\", placeholder=\"1st and last digit must be 0 and values should be between 0 and 1. ex:'0, 0.01, 0'\")\n                    overwrite_old_hypernetwork = gr.Checkbox(value=False, label=\"Overwrite Old Hypernetwork\", elem_id=\"train_overwrite_old_hypernetwork\")\n\n                    with gr.Row():\n                        with gr.Column(scale=3):\n                            gr.HTML(value=\"\")\n\n                        with gr.Column():\n                            create_hypernetwork = gr.Button(value=\"Create hypernetwork\", variant='primary', elem_id=\"train_create_hypernetwork\")\n\n                def get_textual_inversion_template_names():\n                    return sorted(textual_inversion.textual_inversion_templates)\n\n                with gr.Tab(label=\"Train\", id=\"train\"):\n                    gr.HTML(value=\"<p style='margin-bottom: 0.7em'>Train an embedding or Hypernetwork; you must specify a directory with a set of 1:1 ratio images <a href=\\\"https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Textual-Inversion\\\" style=\\\"font-weight:bold;\\\">[wiki]</a></p>\")\n                    with FormRow():\n                        train_embedding_name = gr.Dropdown(label='Embedding', elem_id=\"train_embedding\", choices=sorted(sd_hijack.model_hijack.embedding_db.word_embeddings.keys()))\n                        create_refresh_button(train_embedding_name, sd_hijack.model_hijack.embedding_db.load_textual_inversion_embeddings, lambda: {\"choices\": sorted(sd_hijack.model_hijack.embedding_db.word_embeddings.keys())}, \"refresh_train_embedding_name\")\n\n                        train_hypernetwork_name = gr.Dropdown(label='Hypernetwork', elem_id=\"train_hypernetwork\", choices=sorted(shared.hypernetworks))\n                        create_refresh_button(train_hypernetwork_name, shared.reload_hypernetworks, lambda: {\"choices\": sorted(shared.hypernetworks)}, \"refresh_train_hypernetwork_name\")\n\n                    with FormRow():\n                        embedding_learn_rate = gr.Textbox(label='Embedding Learning rate', placeholder=\"Embedding Learning rate\", value=\"0.005\", elem_id=\"train_embedding_learn_rate\")\n                        hypernetwork_learn_rate = gr.Textbox(label='Hypernetwork Learning rate', placeholder=\"Hypernetwork Learning rate\", value=\"0.00001\", elem_id=\"train_hypernetwork_learn_rate\")\n\n                    with FormRow():\n                        clip_grad_mode = gr.Dropdown(value=\"disabled\", label=\"Gradient Clipping\", choices=[\"disabled\", \"value\", \"norm\"])\n                        clip_grad_value = gr.Textbox(placeholder=\"Gradient clip value\", value=\"0.1\", show_label=False)\n\n                    with FormRow():\n                        batch_size = gr.Number(label='Batch size', value=1, precision=0, elem_id=\"train_batch_size\")\n                        gradient_step = gr.Number(label='Gradient accumulation steps', value=1, precision=0, elem_id=\"train_gradient_step\")\n\n                    dataset_directory = gr.Textbox(label='Dataset directory', placeholder=\"Path to directory with input images\", elem_id=\"train_dataset_directory\")\n                    log_directory = gr.Textbox(label='Log directory', placeholder=\"Path to directory where to write outputs\", value=\"textual_inversion\", elem_id=\"train_log_directory\")\n\n                    with FormRow():\n                        template_file = gr.Dropdown(label='Prompt template', value=\"style_filewords.txt\", elem_id=\"train_template_file\", choices=get_textual_inversion_template_names())\n                        create_refresh_button(template_file, textual_inversion.list_textual_inversion_templates, lambda: {\"choices\": get_textual_inversion_template_names()}, \"refrsh_train_template_file\")\n\n                    training_width = gr.Slider(minimum=64, maximum=2048, step=8, label=\"Width\", value=512, elem_id=\"train_training_width\")\n                    training_height = gr.Slider(minimum=64, maximum=2048, step=8, label=\"Height\", value=512, elem_id=\"train_training_height\")\n                    varsize = gr.Checkbox(label=\"Do not resize images\", value=False, elem_id=\"train_varsize\")\n                    steps = gr.Number(label='Max steps', value=100000, precision=0, elem_id=\"train_steps\")\n\n                    with FormRow():\n                        create_image_every = gr.Number(label='Save an image to log directory every N steps, 0 to disable', value=500, precision=0, elem_id=\"train_create_image_every\")\n                        save_embedding_every = gr.Number(label='Save a copy of embedding to log directory every N steps, 0 to disable', value=500, precision=0, elem_id=\"train_save_embedding_every\")\n\n                    use_weight = gr.Checkbox(label=\"Use PNG alpha channel as loss weight\", value=False, elem_id=\"use_weight\")\n\n                    save_image_with_stored_embedding = gr.Checkbox(label='Save images with embedding in PNG chunks', value=True, elem_id=\"train_save_image_with_stored_embedding\")\n                    preview_from_txt2img = gr.Checkbox(label='Read parameters (prompt, etc...) from txt2img tab when making previews', value=False, elem_id=\"train_preview_from_txt2img\")\n\n                    shuffle_tags = gr.Checkbox(label=\"Shuffle tags by ',' when creating prompts.\", value=False, elem_id=\"train_shuffle_tags\")\n                    tag_drop_out = gr.Slider(minimum=0, maximum=1, step=0.1, label=\"Drop out tags when creating prompts.\", value=0, elem_id=\"train_tag_drop_out\")\n\n                    latent_sampling_method = gr.Radio(label='Choose latent sampling method', value=\"once\", choices=['once', 'deterministic', 'random'], elem_id=\"train_latent_sampling_method\")\n\n                    with gr.Row():\n                        train_embedding = gr.Button(value=\"Train Embedding\", variant='primary', elem_id=\"train_train_embedding\")\n                        interrupt_training = gr.Button(value=\"Interrupt\", elem_id=\"train_interrupt_training\")\n                        train_hypernetwork = gr.Button(value=\"Train Hypernetwork\", variant='primary', elem_id=\"train_train_hypernetwork\")\n\n                params = script_callbacks.UiTrainTabParams(txt2img_preview_params)\n\n                script_callbacks.ui_train_tabs_callback(params)\n\n            with gr.Column(elem_id='ti_gallery_container'):\n                ti_output = gr.Text(elem_id=\"ti_output\", value=\"\", show_label=False)\n                gr.Gallery(label='Output', show_label=False, elem_id='ti_gallery', columns=4)\n                gr.HTML(elem_id=\"ti_progress\", value=\"\")\n                ti_outcome = gr.HTML(elem_id=\"ti_error\", value=\"\")\n\n        create_embedding.click(\n            fn=textual_inversion_ui.create_embedding,\n            inputs=[\n                new_embedding_name,\n                initialization_text,\n                nvpt,\n                overwrite_old_embedding,\n            ],\n            outputs=[\n                train_embedding_name,\n                ti_output,\n                ti_outcome,\n            ]\n        )\n\n        create_hypernetwork.click(\n            fn=hypernetworks_ui.create_hypernetwork,\n            inputs=[\n                new_hypernetwork_name,\n                new_hypernetwork_sizes,\n                overwrite_old_hypernetwork,\n                new_hypernetwork_layer_structure,\n                new_hypernetwork_activation_func,\n                new_hypernetwork_initialization_option,\n                new_hypernetwork_add_layer_norm,\n                new_hypernetwork_use_dropout,\n                new_hypernetwork_dropout_structure\n            ],\n            outputs=[\n                train_hypernetwork_name,\n                ti_output,\n                ti_outcome,\n            ]\n        )\n\n        train_embedding.click(\n            fn=wrap_gradio_gpu_call(textual_inversion_ui.train_embedding, extra_outputs=[gr.update()]),\n            _js=\"start_training_textual_inversion\",\n            inputs=[\n                dummy_component,\n                train_embedding_name,\n                embedding_learn_rate,\n                batch_size,\n                gradient_step,\n                dataset_directory,\n                log_directory,\n                training_width,\n                training_height,\n                varsize,\n                steps,\n                clip_grad_mode,\n                clip_grad_value,\n                shuffle_tags,\n                tag_drop_out,\n                latent_sampling_method,\n                use_weight,\n                create_image_every,\n                save_embedding_every,\n                template_file,\n                save_image_with_stored_embedding,\n                preview_from_txt2img,\n                *txt2img_preview_params,\n            ],\n            outputs=[\n                ti_output,\n                ti_outcome,\n            ]\n        )\n\n        train_hypernetwork.click(\n            fn=wrap_gradio_gpu_call(hypernetworks_ui.train_hypernetwork, extra_outputs=[gr.update()]),\n            _js=\"start_training_textual_inversion\",\n            inputs=[\n                dummy_component,\n                train_hypernetwork_name,\n                hypernetwork_learn_rate,\n                batch_size,\n                gradient_step,\n                dataset_directory,\n                log_directory,\n                training_width,\n                training_height,\n                varsize,\n                steps,\n                clip_grad_mode,\n                clip_grad_value,\n                shuffle_tags,\n                tag_drop_out,\n                latent_sampling_method,\n                use_weight,\n                create_image_every,\n                save_embedding_every,\n                template_file,\n                preview_from_txt2img,\n                *txt2img_preview_params,\n            ],\n            outputs=[\n                ti_output,\n                ti_outcome,\n            ]\n        )\n\n        interrupt_training.click(\n            fn=lambda: shared.state.interrupt(),\n            inputs=[],\n            outputs=[],\n        )\n\n    loadsave = ui_loadsave.UiLoadsave(cmd_opts.ui_config_file)\n    ui_settings_from_file = loadsave.ui_settings.copy()\n\n    settings.create_ui(loadsave, dummy_component)\n\n    interfaces = [\n        (txt2img_interface, \"txt2img\", \"txt2img\"),\n        (img2img_interface, \"img2img\", \"img2img\"),\n        (extras_interface, \"Extras\", \"extras\"),\n        (pnginfo_interface, \"PNG Info\", \"pnginfo\"),\n        (modelmerger_ui.blocks, \"Checkpoint Merger\", \"modelmerger\"),\n        (train_interface, \"Train\", \"train\"),\n    ]\n\n    interfaces += script_callbacks.ui_tabs_callback()\n    interfaces += [(settings.interface, \"Settings\", \"settings\")]\n\n    extensions_interface = ui_extensions.create_ui()\n    interfaces += [(extensions_interface, \"Extensions\", \"extensions\")]\n\n    shared.tab_names = []\n    for _interface, label, _ifid in interfaces:\n        shared.tab_names.append(label)\n\n    with gr.Blocks(theme=shared.gradio_theme, analytics_enabled=False, title=\"Stable Diffusion\") as demo:\n        settings.add_quicksettings()\n\n        parameters_copypaste.connect_paste_params_buttons()\n\n        with gr.Tabs(elem_id=\"tabs\") as tabs:\n            tab_order = {k: i for i, k in enumerate(opts.ui_tab_order)}\n            sorted_interfaces = sorted(interfaces, key=lambda x: tab_order.get(x[1], 9999))\n\n            for interface, label, ifid in sorted_interfaces:\n                if label in shared.opts.hidden_tabs:\n                    continue\n                with gr.TabItem(label, id=ifid, elem_id=f\"tab_{ifid}\"):\n                    interface.render()\n\n                if ifid not in [\"extensions\", \"settings\"]:\n                    loadsave.add_block(interface, ifid)\n\n            loadsave.add_component(f\"webui/Tabs@{tabs.elem_id}\", tabs)\n\n            loadsave.setup_ui()\n\n        if os.path.exists(os.path.join(script_path, \"notification.mp3\")) and shared.opts.notification_audio:\n            gr.Audio(interactive=False, value=os.path.join(script_path, \"notification.mp3\"), elem_id=\"audio_notification\", visible=False)\n\n        footer = shared.html(\"footer.html\")\n        footer = footer.format(versions=versions_html(), api_docs=\"/docs\" if shared.cmd_opts.api else \"https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/API\")\n        gr.HTML(footer, elem_id=\"footer\")\n\n        settings.add_functionality(demo)\n\n        update_image_cfg_scale_visibility = lambda: gr.update(visible=shared.sd_model and shared.sd_model.cond_stage_key == \"edit\")\n        settings.text_settings.change(fn=update_image_cfg_scale_visibility, inputs=[], outputs=[image_cfg_scale])\n        demo.load(fn=update_image_cfg_scale_visibility, inputs=[], outputs=[image_cfg_scale])\n\n        modelmerger_ui.setup_ui(dummy_component=dummy_component, sd_model_checkpoint_component=settings.component_dict['sd_model_checkpoint'])\n\n    if ui_settings_from_file != loadsave.ui_settings:\n        loadsave.dump_defaults()\n    demo.ui_loadsave = loadsave\n\n    return demo\n\n\ndef versions_html():\n    import torch\n    import launch\n\n    python_version = \".\".join([str(x) for x in sys.version_info[0:3]])\n    commit = launch.commit_hash()\n    tag = launch.git_tag()\n\n    if shared.xformers_available:\n        import xformers\n        xformers_version = xformers.__version__\n    else:\n        xformers_version = \"N/A\"\n\n    return f\"\"\"\nversion: <a href=\"https://github.com/AUTOMATIC1111/stable-diffusion-webui/commit/{commit}\">{tag}</a>\n&#x2000;\u2022&#x2000;\npython: <span title=\"{sys.version}\">{python_version}</span>\n&#x2000;\u2022&#x2000;\ntorch: {getattr(torch, '__long_version__',torch.__version__)}\n&#x2000;\u2022&#x2000;\nxformers: {xformers_version}\n&#x2000;\u2022&#x2000;\ngradio: {gr.__version__}\n&#x2000;\u2022&#x2000;\ncheckpoint: <a id=\"sd_checkpoint_hash\">N/A</a>\n\"\"\"\n\n\ndef setup_ui_api(app):\n    from pydantic import BaseModel, Field\n\n    class QuicksettingsHint(BaseModel):\n        name: str = Field(title=\"Name of the quicksettings field\")\n        label: str = Field(title=\"Label of the quicksettings field\")\n\n    def quicksettings_hint():\n        return [QuicksettingsHint(name=k, label=v.label) for k, v in opts.data_labels.items()]\n\n    app.add_api_route(\"/internal/quicksettings-hint\", quicksettings_hint, methods=[\"GET\"], response_model=list[QuicksettingsHint])\n\n    app.add_api_route(\"/internal/ping\", lambda: {}, methods=[\"GET\"])\n\n    app.add_api_route(\"/internal/profile-startup\", lambda: timer.startup_record, methods=[\"GET\"])\n\n    def download_sysinfo(attachment=False):\n        from fastapi.responses import PlainTextResponse\n\n        text = sysinfo.get()\n        filename = f\"sysinfo-{datetime.datetime.utcnow().strftime('%Y-%m-%d-%H-%M')}.json\"\n\n        return PlainTextResponse(text, headers={'Content-Disposition': f'{\"attachment\" if attachment else \"inline\"}; filename=\"{filename}\"'})\n\n    app.add_api_route(\"/internal/sysinfo\", download_sysinfo, methods=[\"GET\"])\n    app.add_api_route(\"/internal/sysinfo-download\", lambda: download_sysinfo(attachment=True), methods=[\"GET\"])\n\n    import fastapi.staticfiles\n    app.mount(\"/webui-assets\", fastapi.staticfiles.StaticFiles(directory=launch_utils.repo_dir('stable-diffusion-webui-assets')), name=\"webui-assets\")\n", "modules/patches.py": "from collections import defaultdict\n\n\ndef patch(key, obj, field, replacement):\n    \"\"\"Replaces a function in a module or a class.\n\n    Also stores the original function in this module, possible to be retrieved via original(key, obj, field).\n    If the function is already replaced by this caller (key), an exception is raised -- use undo() before that.\n\n    Arguments:\n        key: identifying information for who is doing the replacement. You can use __name__.\n        obj: the module or the class\n        field: name of the function as a string\n        replacement: the new function\n\n    Returns:\n        the original function\n    \"\"\"\n\n    patch_key = (obj, field)\n    if patch_key in originals[key]:\n        raise RuntimeError(f\"patch for {field} is already applied\")\n\n    original_func = getattr(obj, field)\n    originals[key][patch_key] = original_func\n\n    setattr(obj, field, replacement)\n\n    return original_func\n\n\ndef undo(key, obj, field):\n    \"\"\"Undoes the peplacement by the patch().\n\n    If the function is not replaced, raises an exception.\n\n    Arguments:\n        key: identifying information for who is doing the replacement. You can use __name__.\n        obj: the module or the class\n        field: name of the function as a string\n\n    Returns:\n        Always None\n    \"\"\"\n\n    patch_key = (obj, field)\n\n    if patch_key not in originals[key]:\n        raise RuntimeError(f\"there is no patch for {field} to undo\")\n\n    original_func = originals[key].pop(patch_key)\n    setattr(obj, field, original_func)\n\n    return None\n\n\ndef original(key, obj, field):\n    \"\"\"Returns the original function for the patch created by the patch() function\"\"\"\n    patch_key = (obj, field)\n\n    return originals[key].get(patch_key, None)\n\n\noriginals = defaultdict(dict)\n", "modules/errors.py": "import sys\nimport textwrap\nimport traceback\n\n\nexception_records = []\n\n\ndef format_traceback(tb):\n    return [[f\"{x.filename}, line {x.lineno}, {x.name}\", x.line] for x in traceback.extract_tb(tb)]\n\n\ndef format_exception(e, tb):\n    return {\"exception\": str(e), \"traceback\": format_traceback(tb)}\n\n\ndef get_exceptions():\n    try:\n        return list(reversed(exception_records))\n    except Exception as e:\n        return str(e)\n\n\ndef record_exception():\n    _, e, tb = sys.exc_info()\n    if e is None:\n        return\n\n    if exception_records and exception_records[-1] == e:\n        return\n\n    exception_records.append(format_exception(e, tb))\n\n    if len(exception_records) > 5:\n        exception_records.pop(0)\n\n\ndef report(message: str, *, exc_info: bool = False) -> None:\n    \"\"\"\n    Print an error message to stderr, with optional traceback.\n    \"\"\"\n\n    record_exception()\n\n    for line in message.splitlines():\n        print(\"***\", line, file=sys.stderr)\n    if exc_info:\n        print(textwrap.indent(traceback.format_exc(), \"    \"), file=sys.stderr)\n        print(\"---\", file=sys.stderr)\n\n\ndef print_error_explanation(message):\n    record_exception()\n\n    lines = message.strip().split(\"\\n\")\n    max_len = max([len(x) for x in lines])\n\n    print('=' * max_len, file=sys.stderr)\n    for line in lines:\n        print(line, file=sys.stderr)\n    print('=' * max_len, file=sys.stderr)\n\n\ndef display(e: Exception, task, *, full_traceback=False):\n    record_exception()\n\n    print(f\"{task or 'error'}: {type(e).__name__}\", file=sys.stderr)\n    te = traceback.TracebackException.from_exception(e)\n    if full_traceback:\n        # include frames leading up to the try-catch block\n        te.stack = traceback.StackSummary(traceback.extract_stack()[:-2] + te.stack)\n    print(*te.format(), sep=\"\", file=sys.stderr)\n\n    message = str(e)\n    if \"copying a param with shape torch.Size([640, 1024]) from checkpoint, the shape in current model is torch.Size([640, 768])\" in message:\n        print_error_explanation(\"\"\"\nThe most likely cause of this is you are trying to load Stable Diffusion 2.0 model without specifying its config file.\nSee https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Features#stable-diffusion-20 for how to solve this.\n        \"\"\")\n\n\nalready_displayed = {}\n\n\ndef display_once(e: Exception, task):\n    record_exception()\n\n    if task in already_displayed:\n        return\n\n    display(e, task)\n\n    already_displayed[task] = 1\n\n\ndef run(code, task):\n    try:\n        code()\n    except Exception as e:\n        display(task, e)\n\n\ndef check_versions():\n    from packaging import version\n    from modules import shared\n\n    import torch\n    import gradio\n\n    expected_torch_version = \"2.1.2\"\n    expected_xformers_version = \"0.0.23.post1\"\n    expected_gradio_version = \"3.41.2\"\n\n    if version.parse(torch.__version__) < version.parse(expected_torch_version):\n        print_error_explanation(f\"\"\"\nYou are running torch {torch.__version__}.\nThe program is tested to work with torch {expected_torch_version}.\nTo reinstall the desired version, run with commandline flag --reinstall-torch.\nBeware that this will cause a lot of large files to be downloaded, as well as\nthere are reports of issues with training tab on the latest version.\n\nUse --skip-version-check commandline argument to disable this check.\n        \"\"\".strip())\n\n    if shared.xformers_available:\n        import xformers\n\n        if version.parse(xformers.__version__) < version.parse(expected_xformers_version):\n            print_error_explanation(f\"\"\"\nYou are running xformers {xformers.__version__}.\nThe program is tested to work with xformers {expected_xformers_version}.\nTo reinstall the desired version, run with commandline flag --reinstall-xformers.\n\nUse --skip-version-check commandline argument to disable this check.\n            \"\"\".strip())\n\n    if gradio.__version__ != expected_gradio_version:\n        print_error_explanation(f\"\"\"\nYou are running gradio {gradio.__version__}.\nThe program is designed to work with gradio {expected_gradio_version}.\nUsing a different version of gradio is extremely likely to break the program.\n\nReasons why you have the mismatched gradio version can be:\n  - you use --skip-install flag.\n  - you use webui.py to start the program instead of launch.py.\n  - an extension installs the incompatible gradio version.\n\nUse --skip-version-check commandline argument to disable this check.\n        \"\"\".strip())\n\n", "modules/npu_specific.py": "import importlib\nimport torch\n\nfrom modules import shared\n\n\ndef check_for_npu():\n    if importlib.util.find_spec(\"torch_npu\") is None:\n        return False\n    import torch_npu\n\n    try:\n        # Will raise a RuntimeError if no NPU is found\n        _ = torch_npu.npu.device_count()\n        return torch.npu.is_available()\n    except RuntimeError:\n        return False\n\n\ndef get_npu_device_string():\n    if shared.cmd_opts.device_id is not None:\n        return f\"npu:{shared.cmd_opts.device_id}\"\n    return \"npu:0\"\n\n\ndef torch_npu_gc():\n    with torch.npu.device(get_npu_device_string()):\n        torch.npu.empty_cache()\n\n\nhas_npu = check_for_npu()\n", "modules/prompt_parser.py": "from __future__ import annotations\n\nimport re\nfrom collections import namedtuple\nimport lark\n\n# a prompt like this: \"fantasy landscape with a [mountain:lake:0.25] and [an oak:a christmas tree:0.75][ in foreground::0.6][: in background:0.25] [shoddy:masterful:0.5]\"\n# will be represented with prompt_schedule like this (assuming steps=100):\n# [25, 'fantasy landscape with a mountain and an oak in foreground shoddy']\n# [50, 'fantasy landscape with a lake and an oak in foreground in background shoddy']\n# [60, 'fantasy landscape with a lake and an oak in foreground in background masterful']\n# [75, 'fantasy landscape with a lake and an oak in background masterful']\n# [100, 'fantasy landscape with a lake and a christmas tree in background masterful']\n\nschedule_parser = lark.Lark(r\"\"\"\n!start: (prompt | /[][():]/+)*\nprompt: (emphasized | scheduled | alternate | plain | WHITESPACE)*\n!emphasized: \"(\" prompt \")\"\n        | \"(\" prompt \":\" prompt \")\"\n        | \"[\" prompt \"]\"\nscheduled: \"[\" [prompt \":\"] prompt \":\" [WHITESPACE] NUMBER [WHITESPACE] \"]\"\nalternate: \"[\" prompt (\"|\" [prompt])+ \"]\"\nWHITESPACE: /\\s+/\nplain: /([^\\\\\\[\\]():|]|\\\\.)+/\n%import common.SIGNED_NUMBER -> NUMBER\n\"\"\")\n\ndef get_learned_conditioning_prompt_schedules(prompts, base_steps, hires_steps=None, use_old_scheduling=False):\n    \"\"\"\n    >>> g = lambda p: get_learned_conditioning_prompt_schedules([p], 10)[0]\n    >>> g(\"test\")\n    [[10, 'test']]\n    >>> g(\"a [b:3]\")\n    [[3, 'a '], [10, 'a b']]\n    >>> g(\"a [b: 3]\")\n    [[3, 'a '], [10, 'a b']]\n    >>> g(\"a [[[b]]:2]\")\n    [[2, 'a '], [10, 'a [[b]]']]\n    >>> g(\"[(a:2):3]\")\n    [[3, ''], [10, '(a:2)']]\n    >>> g(\"a [b : c : 1] d\")\n    [[1, 'a b  d'], [10, 'a  c  d']]\n    >>> g(\"a[b:[c:d:2]:1]e\")\n    [[1, 'abe'], [2, 'ace'], [10, 'ade']]\n    >>> g(\"a [unbalanced\")\n    [[10, 'a [unbalanced']]\n    >>> g(\"a [b:.5] c\")\n    [[5, 'a  c'], [10, 'a b c']]\n    >>> g(\"a [{b|d{:.5] c\")  # not handling this right now\n    [[5, 'a  c'], [10, 'a {b|d{ c']]\n    >>> g(\"((a][:b:c [d:3]\")\n    [[3, '((a][:b:c '], [10, '((a][:b:c d']]\n    >>> g(\"[a|(b:1.1)]\")\n    [[1, 'a'], [2, '(b:1.1)'], [3, 'a'], [4, '(b:1.1)'], [5, 'a'], [6, '(b:1.1)'], [7, 'a'], [8, '(b:1.1)'], [9, 'a'], [10, '(b:1.1)']]\n    >>> g(\"[fe|]male\")\n    [[1, 'female'], [2, 'male'], [3, 'female'], [4, 'male'], [5, 'female'], [6, 'male'], [7, 'female'], [8, 'male'], [9, 'female'], [10, 'male']]\n    >>> g(\"[fe|||]male\")\n    [[1, 'female'], [2, 'male'], [3, 'male'], [4, 'male'], [5, 'female'], [6, 'male'], [7, 'male'], [8, 'male'], [9, 'female'], [10, 'male']]\n    >>> g = lambda p: get_learned_conditioning_prompt_schedules([p], 10, 10)[0]\n    >>> g(\"a [b:.5] c\")\n    [[10, 'a b c']]\n    >>> g(\"a [b:1.5] c\")\n    [[5, 'a  c'], [10, 'a b c']]\n    \"\"\"\n\n    if hires_steps is None or use_old_scheduling:\n        int_offset = 0\n        flt_offset = 0\n        steps = base_steps\n    else:\n        int_offset = base_steps\n        flt_offset = 1.0\n        steps = hires_steps\n\n    def collect_steps(steps, tree):\n        res = [steps]\n\n        class CollectSteps(lark.Visitor):\n            def scheduled(self, tree):\n                s = tree.children[-2]\n                v = float(s)\n                if use_old_scheduling:\n                    v = v*steps if v<1 else v\n                else:\n                    if \".\" in s:\n                        v = (v - flt_offset) * steps\n                    else:\n                        v = (v - int_offset)\n                tree.children[-2] = min(steps, int(v))\n                if tree.children[-2] >= 1:\n                    res.append(tree.children[-2])\n\n            def alternate(self, tree):\n                res.extend(range(1, steps+1))\n\n        CollectSteps().visit(tree)\n        return sorted(set(res))\n\n    def at_step(step, tree):\n        class AtStep(lark.Transformer):\n            def scheduled(self, args):\n                before, after, _, when, _ = args\n                yield before or () if step <= when else after\n            def alternate(self, args):\n                args = [\"\" if not arg else arg for arg in args]\n                yield args[(step - 1) % len(args)]\n            def start(self, args):\n                def flatten(x):\n                    if isinstance(x, str):\n                        yield x\n                    else:\n                        for gen in x:\n                            yield from flatten(gen)\n                return ''.join(flatten(args))\n            def plain(self, args):\n                yield args[0].value\n            def __default__(self, data, children, meta):\n                for child in children:\n                    yield child\n        return AtStep().transform(tree)\n\n    def get_schedule(prompt):\n        try:\n            tree = schedule_parser.parse(prompt)\n        except lark.exceptions.LarkError:\n            if 0:\n                import traceback\n                traceback.print_exc()\n            return [[steps, prompt]]\n        return [[t, at_step(t, tree)] for t in collect_steps(steps, tree)]\n\n    promptdict = {prompt: get_schedule(prompt) for prompt in set(prompts)}\n    return [promptdict[prompt] for prompt in prompts]\n\n\nScheduledPromptConditioning = namedtuple(\"ScheduledPromptConditioning\", [\"end_at_step\", \"cond\"])\n\n\nclass SdConditioning(list):\n    \"\"\"\n    A list with prompts for stable diffusion's conditioner model.\n    Can also specify width and height of created image - SDXL needs it.\n    \"\"\"\n    def __init__(self, prompts, is_negative_prompt=False, width=None, height=None, copy_from=None):\n        super().__init__()\n        self.extend(prompts)\n\n        if copy_from is None:\n            copy_from = prompts\n\n        self.is_negative_prompt = is_negative_prompt or getattr(copy_from, 'is_negative_prompt', False)\n        self.width = width or getattr(copy_from, 'width', None)\n        self.height = height or getattr(copy_from, 'height', None)\n\n\n\ndef get_learned_conditioning(model, prompts: SdConditioning | list[str], steps, hires_steps=None, use_old_scheduling=False):\n    \"\"\"converts a list of prompts into a list of prompt schedules - each schedule is a list of ScheduledPromptConditioning, specifying the comdition (cond),\n    and the sampling step at which this condition is to be replaced by the next one.\n\n    Input:\n    (model, ['a red crown', 'a [blue:green:5] jeweled crown'], 20)\n\n    Output:\n    [\n        [\n            ScheduledPromptConditioning(end_at_step=20, cond=tensor([[-0.3886,  0.0229, -0.0523,  ..., -0.4901, -0.3066,  0.0674], ..., [ 0.3317, -0.5102, -0.4066,  ...,  0.4119, -0.7647, -1.0160]], device='cuda:0'))\n        ],\n        [\n            ScheduledPromptConditioning(end_at_step=5, cond=tensor([[-0.3886,  0.0229, -0.0522,  ..., -0.4901, -0.3067,  0.0673], ..., [-0.0192,  0.3867, -0.4644,  ...,  0.1135, -0.3696, -0.4625]], device='cuda:0')),\n            ScheduledPromptConditioning(end_at_step=20, cond=tensor([[-0.3886,  0.0229, -0.0522,  ..., -0.4901, -0.3067,  0.0673], ..., [-0.7352, -0.4356, -0.7888,  ...,  0.6994, -0.4312, -1.2593]], device='cuda:0'))\n        ]\n    ]\n    \"\"\"\n    res = []\n\n    prompt_schedules = get_learned_conditioning_prompt_schedules(prompts, steps, hires_steps, use_old_scheduling)\n    cache = {}\n\n    for prompt, prompt_schedule in zip(prompts, prompt_schedules):\n\n        cached = cache.get(prompt, None)\n        if cached is not None:\n            res.append(cached)\n            continue\n\n        texts = SdConditioning([x[1] for x in prompt_schedule], copy_from=prompts)\n        conds = model.get_learned_conditioning(texts)\n\n        cond_schedule = []\n        for i, (end_at_step, _) in enumerate(prompt_schedule):\n            if isinstance(conds, dict):\n                cond = {k: v[i] for k, v in conds.items()}\n            else:\n                cond = conds[i]\n\n            cond_schedule.append(ScheduledPromptConditioning(end_at_step, cond))\n\n        cache[prompt] = cond_schedule\n        res.append(cond_schedule)\n\n    return res\n\n\nre_AND = re.compile(r\"\\bAND\\b\")\nre_weight = re.compile(r\"^((?:\\s|.)*?)(?:\\s*:\\s*([-+]?(?:\\d+\\.?|\\d*\\.\\d+)))?\\s*$\")\n\n\ndef get_multicond_prompt_list(prompts: SdConditioning | list[str]):\n    res_indexes = []\n\n    prompt_indexes = {}\n    prompt_flat_list = SdConditioning(prompts)\n    prompt_flat_list.clear()\n\n    for prompt in prompts:\n        subprompts = re_AND.split(prompt)\n\n        indexes = []\n        for subprompt in subprompts:\n            match = re_weight.search(subprompt)\n\n            text, weight = match.groups() if match is not None else (subprompt, 1.0)\n\n            weight = float(weight) if weight is not None else 1.0\n\n            index = prompt_indexes.get(text, None)\n            if index is None:\n                index = len(prompt_flat_list)\n                prompt_flat_list.append(text)\n                prompt_indexes[text] = index\n\n            indexes.append((index, weight))\n\n        res_indexes.append(indexes)\n\n    return res_indexes, prompt_flat_list, prompt_indexes\n\n\nclass ComposableScheduledPromptConditioning:\n    def __init__(self, schedules, weight=1.0):\n        self.schedules: list[ScheduledPromptConditioning] = schedules\n        self.weight: float = weight\n\n\nclass MulticondLearnedConditioning:\n    def __init__(self, shape, batch):\n        self.shape: tuple = shape  # the shape field is needed to send this object to DDIM/PLMS\n        self.batch: list[list[ComposableScheduledPromptConditioning]] = batch\n\n\ndef get_multicond_learned_conditioning(model, prompts, steps, hires_steps=None, use_old_scheduling=False) -> MulticondLearnedConditioning:\n    \"\"\"same as get_learned_conditioning, but returns a list of ScheduledPromptConditioning along with the weight objects for each prompt.\n    For each prompt, the list is obtained by splitting the prompt using the AND separator.\n\n    https://energy-based-model.github.io/Compositional-Visual-Generation-with-Composable-Diffusion-Models/\n    \"\"\"\n\n    res_indexes, prompt_flat_list, prompt_indexes = get_multicond_prompt_list(prompts)\n\n    learned_conditioning = get_learned_conditioning(model, prompt_flat_list, steps, hires_steps, use_old_scheduling)\n\n    res = []\n    for indexes in res_indexes:\n        res.append([ComposableScheduledPromptConditioning(learned_conditioning[i], weight) for i, weight in indexes])\n\n    return MulticondLearnedConditioning(shape=(len(prompts),), batch=res)\n\n\nclass DictWithShape(dict):\n    def __init__(self, x, shape):\n        super().__init__()\n        self.update(x)\n\n    @property\n    def shape(self):\n        return self[\"crossattn\"].shape\n\n\ndef reconstruct_cond_batch(c: list[list[ScheduledPromptConditioning]], current_step):\n    param = c[0][0].cond\n    is_dict = isinstance(param, dict)\n\n    if is_dict:\n        dict_cond = param\n        res = {k: torch.zeros((len(c),) + param.shape, device=param.device, dtype=param.dtype) for k, param in dict_cond.items()}\n        res = DictWithShape(res, (len(c),) + dict_cond['crossattn'].shape)\n    else:\n        res = torch.zeros((len(c),) + param.shape, device=param.device, dtype=param.dtype)\n\n    for i, cond_schedule in enumerate(c):\n        target_index = 0\n        for current, entry in enumerate(cond_schedule):\n            if current_step <= entry.end_at_step:\n                target_index = current\n                break\n\n        if is_dict:\n            for k, param in cond_schedule[target_index].cond.items():\n                res[k][i] = param\n        else:\n            res[i] = cond_schedule[target_index].cond\n\n    return res\n\n\ndef stack_conds(tensors):\n    # if prompts have wildly different lengths above the limit we'll get tensors of different shapes\n    # and won't be able to torch.stack them. So this fixes that.\n    token_count = max([x.shape[0] for x in tensors])\n    for i in range(len(tensors)):\n        if tensors[i].shape[0] != token_count:\n            last_vector = tensors[i][-1:]\n            last_vector_repeated = last_vector.repeat([token_count - tensors[i].shape[0], 1])\n            tensors[i] = torch.vstack([tensors[i], last_vector_repeated])\n\n    return torch.stack(tensors)\n\n\n\ndef reconstruct_multicond_batch(c: MulticondLearnedConditioning, current_step):\n    param = c.batch[0][0].schedules[0].cond\n\n    tensors = []\n    conds_list = []\n\n    for composable_prompts in c.batch:\n        conds_for_batch = []\n\n        for composable_prompt in composable_prompts:\n            target_index = 0\n            for current, entry in enumerate(composable_prompt.schedules):\n                if current_step <= entry.end_at_step:\n                    target_index = current\n                    break\n\n            conds_for_batch.append((len(tensors), composable_prompt.weight))\n            tensors.append(composable_prompt.schedules[target_index].cond)\n\n        conds_list.append(conds_for_batch)\n\n    if isinstance(tensors[0], dict):\n        keys = list(tensors[0].keys())\n        stacked = {k: stack_conds([x[k] for x in tensors]) for k in keys}\n        stacked = DictWithShape(stacked, stacked['crossattn'].shape)\n    else:\n        stacked = stack_conds(tensors).to(device=param.device, dtype=param.dtype)\n\n    return conds_list, stacked\n\n\nre_attention = re.compile(r\"\"\"\n\\\\\\(|\n\\\\\\)|\n\\\\\\[|\n\\\\]|\n\\\\\\\\|\n\\\\|\n\\(|\n\\[|\n:\\s*([+-]?[.\\d]+)\\s*\\)|\n\\)|\n]|\n[^\\\\()\\[\\]:]+|\n:\n\"\"\", re.X)\n\nre_break = re.compile(r\"\\s*\\bBREAK\\b\\s*\", re.S)\n\ndef parse_prompt_attention(text):\n    \"\"\"\n    Parses a string with attention tokens and returns a list of pairs: text and its associated weight.\n    Accepted tokens are:\n      (abc) - increases attention to abc by a multiplier of 1.1\n      (abc:3.12) - increases attention to abc by a multiplier of 3.12\n      [abc] - decreases attention to abc by a multiplier of 1.1\n      \\( - literal character '('\n      \\[ - literal character '['\n      \\) - literal character ')'\n      \\] - literal character ']'\n      \\\\ - literal character '\\'\n      anything else - just text\n\n    >>> parse_prompt_attention('normal text')\n    [['normal text', 1.0]]\n    >>> parse_prompt_attention('an (important) word')\n    [['an ', 1.0], ['important', 1.1], [' word', 1.0]]\n    >>> parse_prompt_attention('(unbalanced')\n    [['unbalanced', 1.1]]\n    >>> parse_prompt_attention('\\(literal\\]')\n    [['(literal]', 1.0]]\n    >>> parse_prompt_attention('(unnecessary)(parens)')\n    [['unnecessaryparens', 1.1]]\n    >>> parse_prompt_attention('a (((house:1.3)) [on] a (hill:0.5), sun, (((sky))).')\n    [['a ', 1.0],\n     ['house', 1.5730000000000004],\n     [' ', 1.1],\n     ['on', 1.0],\n     [' a ', 1.1],\n     ['hill', 0.55],\n     [', sun, ', 1.1],\n     ['sky', 1.4641000000000006],\n     ['.', 1.1]]\n    \"\"\"\n\n    res = []\n    round_brackets = []\n    square_brackets = []\n\n    round_bracket_multiplier = 1.1\n    square_bracket_multiplier = 1 / 1.1\n\n    def multiply_range(start_position, multiplier):\n        for p in range(start_position, len(res)):\n            res[p][1] *= multiplier\n\n    for m in re_attention.finditer(text):\n        text = m.group(0)\n        weight = m.group(1)\n\n        if text.startswith('\\\\'):\n            res.append([text[1:], 1.0])\n        elif text == '(':\n            round_brackets.append(len(res))\n        elif text == '[':\n            square_brackets.append(len(res))\n        elif weight is not None and round_brackets:\n            multiply_range(round_brackets.pop(), float(weight))\n        elif text == ')' and round_brackets:\n            multiply_range(round_brackets.pop(), round_bracket_multiplier)\n        elif text == ']' and square_brackets:\n            multiply_range(square_brackets.pop(), square_bracket_multiplier)\n        else:\n            parts = re.split(re_break, text)\n            for i, part in enumerate(parts):\n                if i > 0:\n                    res.append([\"BREAK\", -1])\n                res.append([part, 1.0])\n\n    for pos in round_brackets:\n        multiply_range(pos, round_bracket_multiplier)\n\n    for pos in square_brackets:\n        multiply_range(pos, square_bracket_multiplier)\n\n    if len(res) == 0:\n        res = [[\"\", 1.0]]\n\n    # merge runs of identical weights\n    i = 0\n    while i + 1 < len(res):\n        if res[i][1] == res[i + 1][1]:\n            res[i][0] += res[i + 1][0]\n            res.pop(i + 1)\n        else:\n            i += 1\n\n    return res\n\nif __name__ == \"__main__\":\n    import doctest\n    doctest.testmod(optionflags=doctest.NORMALIZE_WHITESPACE)\nelse:\n    import torch  # doctest faster\n", "modules/sd_hijack_utils.py": "import importlib\n\nclass CondFunc:\n    def __new__(cls, orig_func, sub_func, cond_func):\n        self = super(CondFunc, cls).__new__(cls)\n        if isinstance(orig_func, str):\n            func_path = orig_func.split('.')\n            for i in range(len(func_path)-1, -1, -1):\n                try:\n                    resolved_obj = importlib.import_module('.'.join(func_path[:i]))\n                    break\n                except ImportError:\n                    pass\n            try:\n                for attr_name in func_path[i:-1]:\n                    resolved_obj = getattr(resolved_obj, attr_name)\n                orig_func = getattr(resolved_obj, func_path[-1])\n                setattr(resolved_obj, func_path[-1], lambda *args, **kwargs: self(*args, **kwargs))\n            except AttributeError:\n                print(f\"Warning: Failed to resolve {orig_func} for CondFunc hijack\")\n                pass\n        self.__init__(orig_func, sub_func, cond_func)\n        return lambda *args, **kwargs: self(*args, **kwargs)\n    def __init__(self, orig_func, sub_func, cond_func):\n        self.__orig_func = orig_func\n        self.__sub_func = sub_func\n        self.__cond_func = cond_func\n    def __call__(self, *args, **kwargs):\n        if not self.__cond_func or self.__cond_func(self.__orig_func, *args, **kwargs):\n            return self.__sub_func(self.__orig_func, *args, **kwargs)\n        else:\n            return self.__orig_func(*args, **kwargs)\n", "modules/shared_cmd_options.py": "import os\n\nimport launch\nfrom modules import cmd_args, script_loading\nfrom modules.paths_internal import models_path, script_path, data_path, sd_configs_path, sd_default_config, sd_model_file, default_sd_model_file, extensions_dir, extensions_builtin_dir  # noqa: F401\n\nparser = cmd_args.parser\n\nscript_loading.preload_extensions(extensions_dir, parser, extension_list=launch.list_extensions(launch.args.ui_settings_file))\nscript_loading.preload_extensions(extensions_builtin_dir, parser)\n\nif os.environ.get('IGNORE_CMD_ARGS_ERRORS', None) is None:\n    cmd_opts = parser.parse_args()\nelse:\n    cmd_opts, _ = parser.parse_known_args()\n\ncmd_opts.webui_is_non_local = any([cmd_opts.share, cmd_opts.listen, cmd_opts.ngrok, cmd_opts.server_name])\ncmd_opts.disable_extension_access = cmd_opts.webui_is_non_local and not cmd_opts.enable_insecure_extension_access\n", "modules/sd_emphasis.py": "from __future__ import annotations\nimport torch\n\n\nclass Emphasis:\n    \"\"\"Emphasis class decides how to death with (emphasized:1.1) text in prompts\"\"\"\n\n    name: str = \"Base\"\n    description: str = \"\"\n\n    tokens: list[list[int]]\n    \"\"\"tokens from the chunk of the prompt\"\"\"\n\n    multipliers: torch.Tensor\n    \"\"\"tensor with multipliers, once for each token\"\"\"\n\n    z: torch.Tensor\n    \"\"\"output of cond transformers network (CLIP)\"\"\"\n\n    def after_transformers(self):\n        \"\"\"Called after cond transformers network has processed the chunk of the prompt; this function should modify self.z to apply the emphasis\"\"\"\n\n        pass\n\n\nclass EmphasisNone(Emphasis):\n    name = \"None\"\n    description = \"disable the mechanism entirely and treat (:.1.1) as literal characters\"\n\n\nclass EmphasisIgnore(Emphasis):\n    name = \"Ignore\"\n    description = \"treat all empasised words as if they have no emphasis\"\n\n\nclass EmphasisOriginal(Emphasis):\n    name = \"Original\"\n    description = \"the original emphasis implementation\"\n\n    def after_transformers(self):\n        original_mean = self.z.mean()\n        self.z = self.z * self.multipliers.reshape(self.multipliers.shape + (1,)).expand(self.z.shape)\n\n        # restoring original mean is likely not correct, but it seems to work well to prevent artifacts that happen otherwise\n        new_mean = self.z.mean()\n        self.z = self.z * (original_mean / new_mean)\n\n\nclass EmphasisOriginalNoNorm(EmphasisOriginal):\n    name = \"No norm\"\n    description = \"same as original, but without normalization (seems to work better for SDXL)\"\n\n    def after_transformers(self):\n        self.z = self.z * self.multipliers.reshape(self.multipliers.shape + (1,)).expand(self.z.shape)\n\n\ndef get_current_option(emphasis_option_name):\n    return next(iter([x for x in options if x.name == emphasis_option_name]), EmphasisOriginal)\n\n\ndef get_options_descriptions():\n    return \", \".join(f\"{x.name}: {x.description}\" for x in options)\n\n\noptions = [\n    EmphasisNone,\n    EmphasisIgnore,\n    EmphasisOriginal,\n    EmphasisOriginalNoNorm,\n]\n", "modules/codeformer_model.py": "from __future__ import annotations\n\nimport logging\n\nimport torch\n\nfrom modules import (\n    devices,\n    errors,\n    face_restoration,\n    face_restoration_utils,\n    modelloader,\n    shared,\n)\n\nlogger = logging.getLogger(__name__)\n\nmodel_url = 'https://github.com/sczhou/CodeFormer/releases/download/v0.1.0/codeformer.pth'\nmodel_download_name = 'codeformer-v0.1.0.pth'\n\n# used by e.g. postprocessing_codeformer.py\ncodeformer: face_restoration.FaceRestoration | None = None\n\n\nclass FaceRestorerCodeFormer(face_restoration_utils.CommonFaceRestoration):\n    def name(self):\n        return \"CodeFormer\"\n\n    def load_net(self) -> torch.Module:\n        for model_path in modelloader.load_models(\n            model_path=self.model_path,\n            model_url=model_url,\n            command_path=self.model_path,\n            download_name=model_download_name,\n            ext_filter=['.pth'],\n        ):\n            return modelloader.load_spandrel_model(\n                model_path,\n                device=devices.device_codeformer,\n                expected_architecture='CodeFormer',\n            ).model\n        raise ValueError(\"No codeformer model found\")\n\n    def get_device(self):\n        return devices.device_codeformer\n\n    def restore(self, np_image, w: float | None = None):\n        if w is None:\n            w = getattr(shared.opts, \"code_former_weight\", 0.5)\n\n        def restore_face(cropped_face_t):\n            assert self.net is not None\n            return self.net(cropped_face_t, weight=w, adain=True)[0]\n\n        return self.restore_with_helper(np_image, restore_face)\n\n\ndef setup_model(dirname: str) -> None:\n    global codeformer\n    try:\n        codeformer = FaceRestorerCodeFormer(dirname)\n        shared.face_restorers.append(codeformer)\n    except Exception:\n        errors.report(\"Error setting up CodeFormer\", exc_info=True)\n", "modules/ui_checkpoint_merger.py": "\nimport gradio as gr\n\nfrom modules import sd_models, sd_vae, errors, extras, call_queue\nfrom modules.ui_components import FormRow\nfrom modules.ui_common import create_refresh_button\n\n\ndef update_interp_description(value):\n    interp_description_css = \"<p style='margin-bottom: 2.5em'>{}</p>\"\n    interp_descriptions = {\n        \"No interpolation\": interp_description_css.format(\"No interpolation will be used. Requires one model; A. Allows for format conversion and VAE baking.\"),\n        \"Weighted sum\": interp_description_css.format(\"A weighted sum will be used for interpolation. Requires two models; A and B. The result is calculated as A * (1 - M) + B * M\"),\n        \"Add difference\": interp_description_css.format(\"The difference between the last two models will be added to the first. Requires three models; A, B and C. The result is calculated as A + (B - C) * M\")\n    }\n    return interp_descriptions[value]\n\n\ndef modelmerger(*args):\n    try:\n        results = extras.run_modelmerger(*args)\n    except Exception as e:\n        errors.report(\"Error loading/saving model file\", exc_info=True)\n        sd_models.list_models()  # to remove the potentially missing models from the list\n        return [*[gr.Dropdown.update(choices=sd_models.checkpoint_tiles()) for _ in range(4)], f\"Error merging checkpoints: {e}\"]\n    return results\n\n\nclass UiCheckpointMerger:\n    def __init__(self):\n        with gr.Blocks(analytics_enabled=False) as modelmerger_interface:\n            with gr.Row(equal_height=False):\n                with gr.Column(variant='compact'):\n                    self.interp_description = gr.HTML(value=update_interp_description(\"Weighted sum\"), elem_id=\"modelmerger_interp_description\")\n\n                    with FormRow(elem_id=\"modelmerger_models\"):\n                        self.primary_model_name = gr.Dropdown(sd_models.checkpoint_tiles(), elem_id=\"modelmerger_primary_model_name\", label=\"Primary model (A)\")\n                        create_refresh_button(self.primary_model_name, sd_models.list_models, lambda: {\"choices\": sd_models.checkpoint_tiles()}, \"refresh_checkpoint_A\")\n\n                        self.secondary_model_name = gr.Dropdown(sd_models.checkpoint_tiles(), elem_id=\"modelmerger_secondary_model_name\", label=\"Secondary model (B)\")\n                        create_refresh_button(self.secondary_model_name, sd_models.list_models, lambda: {\"choices\": sd_models.checkpoint_tiles()}, \"refresh_checkpoint_B\")\n\n                        self.tertiary_model_name = gr.Dropdown(sd_models.checkpoint_tiles(), elem_id=\"modelmerger_tertiary_model_name\", label=\"Tertiary model (C)\")\n                        create_refresh_button(self.tertiary_model_name, sd_models.list_models, lambda: {\"choices\": sd_models.checkpoint_tiles()}, \"refresh_checkpoint_C\")\n\n                    self.custom_name = gr.Textbox(label=\"Custom Name (Optional)\", elem_id=\"modelmerger_custom_name\")\n                    self.interp_amount = gr.Slider(minimum=0.0, maximum=1.0, step=0.05, label='Multiplier (M) - set to 0 to get model A', value=0.3, elem_id=\"modelmerger_interp_amount\")\n                    self.interp_method = gr.Radio(choices=[\"No interpolation\", \"Weighted sum\", \"Add difference\"], value=\"Weighted sum\", label=\"Interpolation Method\", elem_id=\"modelmerger_interp_method\")\n                    self.interp_method.change(fn=update_interp_description, inputs=[self.interp_method], outputs=[self.interp_description])\n\n                    with FormRow():\n                        self.checkpoint_format = gr.Radio(choices=[\"ckpt\", \"safetensors\"], value=\"safetensors\", label=\"Checkpoint format\", elem_id=\"modelmerger_checkpoint_format\")\n                        self.save_as_half = gr.Checkbox(value=False, label=\"Save as float16\", elem_id=\"modelmerger_save_as_half\")\n\n                    with FormRow():\n                        with gr.Column():\n                            self.config_source = gr.Radio(choices=[\"A, B or C\", \"B\", \"C\", \"Don't\"], value=\"A, B or C\", label=\"Copy config from\", type=\"index\", elem_id=\"modelmerger_config_method\")\n\n                        with gr.Column():\n                            with FormRow():\n                                self.bake_in_vae = gr.Dropdown(choices=[\"None\"] + list(sd_vae.vae_dict), value=\"None\", label=\"Bake in VAE\", elem_id=\"modelmerger_bake_in_vae\")\n                                create_refresh_button(self.bake_in_vae, sd_vae.refresh_vae_list, lambda: {\"choices\": [\"None\"] + list(sd_vae.vae_dict)}, \"modelmerger_refresh_bake_in_vae\")\n\n                    with FormRow():\n                        self.discard_weights = gr.Textbox(value=\"\", label=\"Discard weights with matching name\", elem_id=\"modelmerger_discard_weights\")\n\n                    with gr.Accordion(\"Metadata\", open=False) as metadata_editor:\n                        with FormRow():\n                            self.save_metadata = gr.Checkbox(value=True, label=\"Save metadata\", elem_id=\"modelmerger_save_metadata\")\n                            self.add_merge_recipe = gr.Checkbox(value=True, label=\"Add merge recipe metadata\", elem_id=\"modelmerger_add_recipe\")\n                            self.copy_metadata_fields = gr.Checkbox(value=True, label=\"Copy metadata from merged models\", elem_id=\"modelmerger_copy_metadata\")\n\n                        self.metadata_json = gr.TextArea('{}', label=\"Metadata in JSON format\")\n                        self.read_metadata = gr.Button(\"Read metadata from selected checkpoints\")\n\n                    with FormRow():\n                        self.modelmerger_merge = gr.Button(elem_id=\"modelmerger_merge\", value=\"Merge\", variant='primary')\n\n                with gr.Column(variant='compact', elem_id=\"modelmerger_results_container\"):\n                    with gr.Group(elem_id=\"modelmerger_results_panel\"):\n                        self.modelmerger_result = gr.HTML(elem_id=\"modelmerger_result\", show_label=False)\n\n        self.metadata_editor = metadata_editor\n        self.blocks = modelmerger_interface\n\n    def setup_ui(self, dummy_component, sd_model_checkpoint_component):\n        self.checkpoint_format.change(lambda fmt: gr.update(visible=fmt == 'safetensors'), inputs=[self.checkpoint_format], outputs=[self.metadata_editor], show_progress=False)\n\n        self.read_metadata.click(extras.read_metadata, inputs=[self.primary_model_name, self.secondary_model_name, self.tertiary_model_name], outputs=[self.metadata_json])\n\n        self.modelmerger_merge.click(fn=lambda: '', inputs=[], outputs=[self.modelmerger_result])\n        self.modelmerger_merge.click(\n            fn=call_queue.wrap_gradio_gpu_call(modelmerger, extra_outputs=lambda: [gr.update() for _ in range(4)]),\n            _js='modelmerger',\n            inputs=[\n                dummy_component,\n                self.primary_model_name,\n                self.secondary_model_name,\n                self.tertiary_model_name,\n                self.interp_method,\n                self.interp_amount,\n                self.save_as_half,\n                self.custom_name,\n                self.checkpoint_format,\n                self.config_source,\n                self.bake_in_vae,\n                self.discard_weights,\n                self.save_metadata,\n                self.add_merge_recipe,\n                self.copy_metadata_fields,\n                self.metadata_json,\n            ],\n            outputs=[\n                self.primary_model_name,\n                self.secondary_model_name,\n                self.tertiary_model_name,\n                sd_model_checkpoint_component,\n                self.modelmerger_result,\n            ]\n        )\n\n        # Required as a workaround for change() event not triggering when loading values from ui-config.json\n        self.interp_description.value = update_interp_description(self.interp_method.value)\n\n", "modules/script_loading.py": "import os\nimport importlib.util\n\nfrom modules import errors\n\n\nloaded_scripts = {}\n\n\ndef load_module(path):\n    module_spec = importlib.util.spec_from_file_location(os.path.basename(path), path)\n    module = importlib.util.module_from_spec(module_spec)\n    module_spec.loader.exec_module(module)\n\n    loaded_scripts[path] = module\n    return module\n\n\ndef preload_extensions(extensions_dir, parser, extension_list=None):\n    if not os.path.isdir(extensions_dir):\n        return\n\n    extensions = extension_list if extension_list is not None else os.listdir(extensions_dir)\n    for dirname in sorted(extensions):\n        preload_script = os.path.join(extensions_dir, dirname, \"preload.py\")\n        if not os.path.isfile(preload_script):\n            continue\n\n        try:\n            module = load_module(preload_script)\n            if hasattr(module, 'preload'):\n                module.preload(parser)\n\n        except Exception:\n            errors.report(f\"Error running preload() for {preload_script}\", exc_info=True)\n", "modules/gfpgan_model.py": "from __future__ import annotations\n\nimport logging\nimport os\n\nimport torch\n\nfrom modules import (\n    devices,\n    errors,\n    face_restoration,\n    face_restoration_utils,\n    modelloader,\n    shared,\n)\n\nlogger = logging.getLogger(__name__)\nmodel_url = \"https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.4.pth\"\nmodel_download_name = \"GFPGANv1.4.pth\"\ngfpgan_face_restorer: face_restoration.FaceRestoration | None = None\n\n\nclass FaceRestorerGFPGAN(face_restoration_utils.CommonFaceRestoration):\n    def name(self):\n        return \"GFPGAN\"\n\n    def get_device(self):\n        return devices.device_gfpgan\n\n    def load_net(self) -> torch.Module:\n        for model_path in modelloader.load_models(\n            model_path=self.model_path,\n            model_url=model_url,\n            command_path=self.model_path,\n            download_name=model_download_name,\n            ext_filter=['.pth'],\n        ):\n            if 'GFPGAN' in os.path.basename(model_path):\n                model = modelloader.load_spandrel_model(\n                    model_path,\n                    device=self.get_device(),\n                    expected_architecture='GFPGAN',\n                ).model\n                model.different_w = True  # see https://github.com/chaiNNer-org/spandrel/pull/81\n                return model\n        raise ValueError(\"No GFPGAN model found\")\n\n    def restore(self, np_image):\n        def restore_face(cropped_face_t):\n            assert self.net is not None\n            return self.net(cropped_face_t, return_rgb=False)[0]\n\n        return self.restore_with_helper(np_image, restore_face)\n\n\ndef gfpgan_fix_faces(np_image):\n    if gfpgan_face_restorer:\n        return gfpgan_face_restorer.restore(np_image)\n    logger.warning(\"GFPGAN face restorer not set up\")\n    return np_image\n\n\ndef setup_model(dirname: str) -> None:\n    global gfpgan_face_restorer\n\n    try:\n        face_restoration_utils.patch_facexlib(dirname)\n        gfpgan_face_restorer = FaceRestorerGFPGAN(model_path=dirname)\n        shared.face_restorers.append(gfpgan_face_restorer)\n    except Exception:\n        errors.report(\"Error setting up GFPGAN\", exc_info=True)\n", "modules/sd_samplers.py": "from __future__ import annotations\n\nimport functools\n\nfrom modules import sd_samplers_kdiffusion, sd_samplers_timesteps, sd_samplers_lcm, shared, sd_samplers_common, sd_schedulers\n\n# imports for functions that previously were here and are used by other modules\nsamples_to_image_grid = sd_samplers_common.samples_to_image_grid\nsample_to_image = sd_samplers_common.sample_to_image\n\nall_samplers = [\n    *sd_samplers_kdiffusion.samplers_data_k_diffusion,\n    *sd_samplers_timesteps.samplers_data_timesteps,\n    *sd_samplers_lcm.samplers_data_lcm,\n]\nall_samplers_map = {x.name: x for x in all_samplers}\n\nsamplers: list[sd_samplers_common.SamplerData] = []\nsamplers_for_img2img: list[sd_samplers_common.SamplerData] = []\nsamplers_map = {}\nsamplers_hidden = {}\n\n\ndef find_sampler_config(name):\n    if name is not None:\n        config = all_samplers_map.get(name, None)\n    else:\n        config = all_samplers[0]\n\n    return config\n\n\ndef create_sampler(name, model):\n    config = find_sampler_config(name)\n\n    assert config is not None, f'bad sampler name: {name}'\n\n    if model.is_sdxl and config.options.get(\"no_sdxl\", False):\n        raise Exception(f\"Sampler {config.name} is not supported for SDXL\")\n\n    sampler = config.constructor(model)\n    sampler.config = config\n\n    return sampler\n\n\ndef set_samplers():\n    global samplers, samplers_for_img2img, samplers_hidden\n\n    samplers_hidden = set(shared.opts.hide_samplers)\n    samplers = all_samplers\n    samplers_for_img2img = all_samplers\n\n    samplers_map.clear()\n    for sampler in all_samplers:\n        samplers_map[sampler.name.lower()] = sampler.name\n        for alias in sampler.aliases:\n            samplers_map[alias.lower()] = sampler.name\n\n\ndef visible_sampler_names():\n    return [x.name for x in samplers if x.name not in samplers_hidden]\n\n\ndef visible_samplers():\n    return [x for x in samplers if x.name not in samplers_hidden]\n\n\ndef get_sampler_from_infotext(d: dict):\n    return get_sampler_and_scheduler(d.get(\"Sampler\"), d.get(\"Schedule type\"))[0]\n\n\ndef get_scheduler_from_infotext(d: dict):\n    return get_sampler_and_scheduler(d.get(\"Sampler\"), d.get(\"Schedule type\"))[1]\n\n\ndef get_hr_sampler_and_scheduler(d: dict):\n    hr_sampler = d.get(\"Hires sampler\", \"Use same sampler\")\n    sampler = d.get(\"Sampler\") if hr_sampler == \"Use same sampler\" else hr_sampler\n\n    hr_scheduler = d.get(\"Hires schedule type\", \"Use same scheduler\")\n    scheduler = d.get(\"Schedule type\") if hr_scheduler == \"Use same scheduler\" else hr_scheduler\n\n    sampler, scheduler = get_sampler_and_scheduler(sampler, scheduler)\n\n    sampler = sampler if sampler != d.get(\"Sampler\") else \"Use same sampler\"\n    scheduler = scheduler if scheduler != d.get(\"Schedule type\") else \"Use same scheduler\"\n\n    return sampler, scheduler\n\n\ndef get_hr_sampler_from_infotext(d: dict):\n    return get_hr_sampler_and_scheduler(d)[0]\n\n\ndef get_hr_scheduler_from_infotext(d: dict):\n    return get_hr_sampler_and_scheduler(d)[1]\n\n\n@functools.cache\ndef get_sampler_and_scheduler(sampler_name, scheduler_name):\n    default_sampler = samplers[0]\n    found_scheduler = sd_schedulers.schedulers_map.get(scheduler_name, sd_schedulers.schedulers[0])\n\n    name = sampler_name or default_sampler.name\n\n    for scheduler in sd_schedulers.schedulers:\n        name_options = [scheduler.label, scheduler.name, *(scheduler.aliases or [])]\n\n        for name_option in name_options:\n            if name.endswith(\" \" + name_option):\n                found_scheduler = scheduler\n                name = name[0:-(len(name_option) + 1)]\n                break\n\n    sampler = all_samplers_map.get(name, default_sampler)\n\n    # revert back to Automatic if it's the default scheduler for the selected sampler\n    if sampler.options.get('scheduler', None) == found_scheduler.name:\n        found_scheduler = sd_schedulers.schedulers[0]\n\n    return sampler.name, found_scheduler.label\n\n\nset_samplers()\n", "modules/upscaler.py": "import os\nfrom abc import abstractmethod\n\nimport PIL\nfrom PIL import Image\n\nimport modules.shared\nfrom modules import modelloader, shared\n\nLANCZOS = (Image.Resampling.LANCZOS if hasattr(Image, 'Resampling') else Image.LANCZOS)\nNEAREST = (Image.Resampling.NEAREST if hasattr(Image, 'Resampling') else Image.NEAREST)\n\n\nclass Upscaler:\n    name = None\n    model_path = None\n    model_name = None\n    model_url = None\n    enable = True\n    filter = None\n    model = None\n    user_path = None\n    scalers: list\n    tile = True\n\n    def __init__(self, create_dirs=False):\n        self.mod_pad_h = None\n        self.tile_size = modules.shared.opts.ESRGAN_tile\n        self.tile_pad = modules.shared.opts.ESRGAN_tile_overlap\n        self.device = modules.shared.device\n        self.img = None\n        self.output = None\n        self.scale = 1\n        self.half = not modules.shared.cmd_opts.no_half\n        self.pre_pad = 0\n        self.mod_scale = None\n        self.model_download_path = None\n\n        if self.model_path is None and self.name:\n            self.model_path = os.path.join(shared.models_path, self.name)\n        if self.model_path and create_dirs:\n            os.makedirs(self.model_path, exist_ok=True)\n\n        try:\n            import cv2  # noqa: F401\n            self.can_tile = True\n        except Exception:\n            pass\n\n    @abstractmethod\n    def do_upscale(self, img: PIL.Image, selected_model: str):\n        return img\n\n    def upscale(self, img: PIL.Image, scale, selected_model: str = None):\n        self.scale = scale\n        dest_w = int((img.width * scale) // 8 * 8)\n        dest_h = int((img.height * scale) // 8 * 8)\n\n        for _ in range(3):\n            if img.width >= dest_w and img.height >= dest_h and scale != 1:\n                break\n\n            if shared.state.interrupted:\n                break\n\n            shape = (img.width, img.height)\n\n            img = self.do_upscale(img, selected_model)\n\n            if shape == (img.width, img.height):\n                break\n\n        if img.width != dest_w or img.height != dest_h:\n            img = img.resize((int(dest_w), int(dest_h)), resample=LANCZOS)\n\n        return img\n\n    @abstractmethod\n    def load_model(self, path: str):\n        pass\n\n    def find_models(self, ext_filter=None) -> list:\n        return modelloader.load_models(model_path=self.model_path, model_url=self.model_url, command_path=self.user_path, ext_filter=ext_filter)\n\n    def update_status(self, prompt):\n        print(f\"\\nextras: {prompt}\", file=shared.progress_print_out)\n\n\nclass UpscalerData:\n    name = None\n    data_path = None\n    scale: int = 4\n    scaler: Upscaler = None\n    model: None\n\n    def __init__(self, name: str, path: str, upscaler: Upscaler = None, scale: int = 4, model=None):\n        self.name = name\n        self.data_path = path\n        self.local_data_path = path\n        self.scaler = upscaler\n        self.scale = scale\n        self.model = model\n\n    def __repr__(self):\n        return f\"<UpscalerData name={self.name} path={self.data_path} scale={self.scale}>\"\n\n\nclass UpscalerNone(Upscaler):\n    name = \"None\"\n    scalers = []\n\n    def load_model(self, path):\n        pass\n\n    def do_upscale(self, img, selected_model=None):\n        return img\n\n    def __init__(self, dirname=None):\n        super().__init__(False)\n        self.scalers = [UpscalerData(\"None\", None, self)]\n\n\nclass UpscalerLanczos(Upscaler):\n    scalers = []\n\n    def do_upscale(self, img, selected_model=None):\n        return img.resize((int(img.width * self.scale), int(img.height * self.scale)), resample=LANCZOS)\n\n    def load_model(self, _):\n        pass\n\n    def __init__(self, dirname=None):\n        super().__init__(False)\n        self.name = \"Lanczos\"\n        self.scalers = [UpscalerData(\"Lanczos\", None, self)]\n\n\nclass UpscalerNearest(Upscaler):\n    scalers = []\n\n    def do_upscale(self, img, selected_model=None):\n        return img.resize((int(img.width * self.scale), int(img.height * self.scale)), resample=NEAREST)\n\n    def load_model(self, _):\n        pass\n\n    def __init__(self, dirname=None):\n        super().__init__(False)\n        self.name = \"Nearest\"\n        self.scalers = [UpscalerData(\"Nearest\", None, self)]\n", "modules/cache.py": "import json\nimport os\nimport os.path\nimport threading\n\nimport diskcache\nimport tqdm\n\nfrom modules.paths import data_path, script_path\n\ncache_filename = os.environ.get('SD_WEBUI_CACHE_FILE', os.path.join(data_path, \"cache.json\"))\ncache_dir = os.environ.get('SD_WEBUI_CACHE_DIR', os.path.join(data_path, \"cache\"))\ncaches = {}\ncache_lock = threading.Lock()\n\n\ndef dump_cache():\n    \"\"\"old function for dumping cache to disk; does nothing since diskcache.\"\"\"\n\n    pass\n\n\ndef make_cache(subsection: str) -> diskcache.Cache:\n    return diskcache.Cache(\n        os.path.join(cache_dir, subsection),\n        size_limit=2**32,  # 4 GB, culling oldest first\n        disk_min_file_size=2**18,  # keep up to 256KB in Sqlite\n    )\n\n\ndef convert_old_cached_data():\n    try:\n        with open(cache_filename, \"r\", encoding=\"utf8\") as file:\n            data = json.load(file)\n    except FileNotFoundError:\n        return\n    except Exception:\n        os.replace(cache_filename, os.path.join(script_path, \"tmp\", \"cache.json\"))\n        print('[ERROR] issue occurred while trying to read cache.json; old cache has been moved to tmp/cache.json')\n        return\n\n    total_count = sum(len(keyvalues) for keyvalues in data.values())\n\n    with tqdm.tqdm(total=total_count, desc=\"converting cache\") as progress:\n        for subsection, keyvalues in data.items():\n            cache_obj = caches.get(subsection)\n            if cache_obj is None:\n                cache_obj = make_cache(subsection)\n                caches[subsection] = cache_obj\n\n            for key, value in keyvalues.items():\n                cache_obj[key] = value\n                progress.update(1)\n\n\ndef cache(subsection):\n    \"\"\"\n    Retrieves or initializes a cache for a specific subsection.\n\n    Parameters:\n        subsection (str): The subsection identifier for the cache.\n\n    Returns:\n        diskcache.Cache: The cache data for the specified subsection.\n    \"\"\"\n\n    cache_obj = caches.get(subsection)\n    if not cache_obj:\n        with cache_lock:\n            if not os.path.exists(cache_dir) and os.path.isfile(cache_filename):\n                convert_old_cached_data()\n\n            cache_obj = caches.get(subsection)\n            if not cache_obj:\n                cache_obj = make_cache(subsection)\n                caches[subsection] = cache_obj\n\n    return cache_obj\n\n\ndef cached_data_for_file(subsection, title, filename, func):\n    \"\"\"\n    Retrieves or generates data for a specific file, using a caching mechanism.\n\n    Parameters:\n        subsection (str): The subsection of the cache to use.\n        title (str): The title of the data entry in the subsection of the cache.\n        filename (str): The path to the file to be checked for modifications.\n        func (callable): A function that generates the data if it is not available in the cache.\n\n    Returns:\n        dict or None: The cached or generated data, or None if data generation fails.\n\n    The `cached_data_for_file` function implements a caching mechanism for data stored in files.\n    It checks if the data associated with the given `title` is present in the cache and compares the\n    modification time of the file with the cached modification time. If the file has been modified,\n    the cache is considered invalid and the data is regenerated using the provided `func`.\n    Otherwise, the cached data is returned.\n\n    If the data generation fails, None is returned to indicate the failure. Otherwise, the generated\n    or cached data is returned as a dictionary.\n    \"\"\"\n\n    existing_cache = cache(subsection)\n    ondisk_mtime = os.path.getmtime(filename)\n\n    entry = existing_cache.get(title)\n    if entry:\n        cached_mtime = entry.get(\"mtime\", 0)\n        if ondisk_mtime > cached_mtime:\n            entry = None\n\n    if not entry or 'value' not in entry:\n        value = func()\n        if value is None:\n            return None\n\n        entry = {'mtime': ondisk_mtime, 'value': value}\n        existing_cache[title] = entry\n\n        dump_cache()\n\n    return entry['value']\n", "modules/scripts_postprocessing.py": "import dataclasses\nimport os\nimport gradio as gr\n\nfrom modules import errors, shared\n\n\n@dataclasses.dataclass\nclass PostprocessedImageSharedInfo:\n    target_width: int = None\n    target_height: int = None\n\n\nclass PostprocessedImage:\n    def __init__(self, image):\n        self.image = image\n        self.info = {}\n        self.shared = PostprocessedImageSharedInfo()\n        self.extra_images = []\n        self.nametags = []\n        self.disable_processing = False\n        self.caption = None\n\n    def get_suffix(self, used_suffixes=None):\n        used_suffixes = {} if used_suffixes is None else used_suffixes\n        suffix = \"-\".join(self.nametags)\n        if suffix:\n            suffix = \"-\" + suffix\n\n        if suffix not in used_suffixes:\n            used_suffixes[suffix] = 1\n            return suffix\n\n        for i in range(1, 100):\n            proposed_suffix = suffix + \"-\" + str(i)\n\n            if proposed_suffix not in used_suffixes:\n                used_suffixes[proposed_suffix] = 1\n                return proposed_suffix\n\n        return suffix\n\n    def create_copy(self, new_image, *, nametags=None, disable_processing=False):\n        pp = PostprocessedImage(new_image)\n        pp.shared = self.shared\n        pp.nametags = self.nametags.copy()\n        pp.info = self.info.copy()\n        pp.disable_processing = disable_processing\n\n        if nametags is not None:\n            pp.nametags += nametags\n\n        return pp\n\n\nclass ScriptPostprocessing:\n    filename = None\n    controls = None\n    args_from = None\n    args_to = None\n\n    order = 1000\n    \"\"\"scripts will be ordred by this value in postprocessing UI\"\"\"\n\n    name = None\n    \"\"\"this function should return the title of the script.\"\"\"\n\n    group = None\n    \"\"\"A gr.Group component that has all script's UI inside it\"\"\"\n\n    def ui(self):\n        \"\"\"\n        This function should create gradio UI elements. See https://gradio.app/docs/#components\n        The return value should be a dictionary that maps parameter names to components used in processing.\n        Values of those components will be passed to process() function.\n        \"\"\"\n\n        pass\n\n    def process(self, pp: PostprocessedImage, **args):\n        \"\"\"\n        This function is called to postprocess the image.\n        args contains a dictionary with all values returned by components from ui()\n        \"\"\"\n\n        pass\n\n    def process_firstpass(self, pp: PostprocessedImage, **args):\n        \"\"\"\n        Called for all scripts before calling process(). Scripts can examine the image here and set fields\n        of the pp object to communicate things to other scripts.\n        args contains a dictionary with all values returned by components from ui()\n        \"\"\"\n\n        pass\n\n    def image_changed(self):\n        pass\n\n\ndef wrap_call(func, filename, funcname, *args, default=None, **kwargs):\n    try:\n        res = func(*args, **kwargs)\n        return res\n    except Exception as e:\n        errors.display(e, f\"calling {filename}/{funcname}\")\n\n    return default\n\n\nclass ScriptPostprocessingRunner:\n    def __init__(self):\n        self.scripts = None\n        self.ui_created = False\n\n    def initialize_scripts(self, scripts_data):\n        self.scripts = []\n\n        for script_data in scripts_data:\n            script: ScriptPostprocessing = script_data.script_class()\n            script.filename = script_data.path\n\n            if script.name == \"Simple Upscale\":\n                continue\n\n            self.scripts.append(script)\n\n    def create_script_ui(self, script, inputs):\n        script.args_from = len(inputs)\n        script.args_to = len(inputs)\n\n        script.controls = wrap_call(script.ui, script.filename, \"ui\")\n\n        for control in script.controls.values():\n            control.custom_script_source = os.path.basename(script.filename)\n\n        inputs += list(script.controls.values())\n        script.args_to = len(inputs)\n\n    def scripts_in_preferred_order(self):\n        if self.scripts is None:\n            import modules.scripts\n            self.initialize_scripts(modules.scripts.postprocessing_scripts_data)\n\n        scripts_order = shared.opts.postprocessing_operation_order\n        scripts_filter_out = set(shared.opts.postprocessing_disable_in_extras)\n\n        def script_score(name):\n            for i, possible_match in enumerate(scripts_order):\n                if possible_match == name:\n                    return i\n\n            return len(self.scripts)\n\n        filtered_scripts = [script for script in self.scripts if script.name not in scripts_filter_out]\n        script_scores = {script.name: (script_score(script.name), script.order, script.name, original_index) for original_index, script in enumerate(filtered_scripts)}\n\n        return sorted(filtered_scripts, key=lambda x: script_scores[x.name])\n\n    def setup_ui(self):\n        inputs = []\n\n        for script in self.scripts_in_preferred_order():\n            with gr.Row() as group:\n                self.create_script_ui(script, inputs)\n\n            script.group = group\n\n        self.ui_created = True\n        return inputs\n\n    def run(self, pp: PostprocessedImage, args):\n        scripts = []\n\n        for script in self.scripts_in_preferred_order():\n            script_args = args[script.args_from:script.args_to]\n\n            process_args = {}\n            for (name, _component), value in zip(script.controls.items(), script_args):\n                process_args[name] = value\n\n            scripts.append((script, process_args))\n\n        for script, process_args in scripts:\n            script.process_firstpass(pp, **process_args)\n\n        all_images = [pp]\n\n        for script, process_args in scripts:\n            if shared.state.skipped:\n                break\n\n            shared.state.job = script.name\n\n            for single_image in all_images.copy():\n\n                if not single_image.disable_processing:\n                    script.process(single_image, **process_args)\n\n                for extra_image in single_image.extra_images:\n                    if not isinstance(extra_image, PostprocessedImage):\n                        extra_image = single_image.create_copy(extra_image)\n\n                    all_images.append(extra_image)\n\n                single_image.extra_images.clear()\n\n        pp.extra_images = all_images[1:]\n\n    def create_args_for_run(self, scripts_args):\n        if not self.ui_created:\n            with gr.Blocks(analytics_enabled=False):\n                self.setup_ui()\n\n        scripts = self.scripts_in_preferred_order()\n        args = [None] * max([x.args_to for x in scripts])\n\n        for script in scripts:\n            script_args_dict = scripts_args.get(script.name, None)\n            if script_args_dict is not None:\n\n                for i, name in enumerate(script.controls):\n                    args[script.args_from + i] = script_args_dict.get(name, None)\n\n        return args\n\n    def image_changed(self):\n        for script in self.scripts_in_preferred_order():\n            script.image_changed()\n\n", "modules/modelloader.py": "from __future__ import annotations\n\nimport importlib\nimport logging\nimport os\nfrom typing import TYPE_CHECKING\nfrom urllib.parse import urlparse\n\nimport torch\n\nfrom modules import shared\nfrom modules.upscaler import Upscaler, UpscalerLanczos, UpscalerNearest, UpscalerNone\n\nif TYPE_CHECKING:\n    import spandrel\n\nlogger = logging.getLogger(__name__)\n\n\ndef load_file_from_url(\n    url: str,\n    *,\n    model_dir: str,\n    progress: bool = True,\n    file_name: str | None = None,\n) -> str:\n    \"\"\"Download a file from `url` into `model_dir`, using the file present if possible.\n\n    Returns the path to the downloaded file.\n    \"\"\"\n    os.makedirs(model_dir, exist_ok=True)\n    if not file_name:\n        parts = urlparse(url)\n        file_name = os.path.basename(parts.path)\n    cached_file = os.path.abspath(os.path.join(model_dir, file_name))\n    if not os.path.exists(cached_file):\n        print(f'Downloading: \"{url}\" to {cached_file}\\n')\n        from torch.hub import download_url_to_file\n        download_url_to_file(url, cached_file, progress=progress)\n    return cached_file\n\n\ndef load_models(model_path: str, model_url: str = None, command_path: str = None, ext_filter=None, download_name=None, ext_blacklist=None) -> list:\n    \"\"\"\n    A one-and done loader to try finding the desired models in specified directories.\n\n    @param download_name: Specify to download from model_url immediately.\n    @param model_url: If no other models are found, this will be downloaded on upscale.\n    @param model_path: The location to store/find models in.\n    @param command_path: A command-line argument to search for models in first.\n    @param ext_filter: An optional list of filename extensions to filter by\n    @return: A list of paths containing the desired model(s)\n    \"\"\"\n    output = []\n\n    try:\n        places = []\n\n        if command_path is not None and command_path != model_path:\n            pretrained_path = os.path.join(command_path, 'experiments/pretrained_models')\n            if os.path.exists(pretrained_path):\n                print(f\"Appending path: {pretrained_path}\")\n                places.append(pretrained_path)\n            elif os.path.exists(command_path):\n                places.append(command_path)\n\n        places.append(model_path)\n\n        for place in places:\n            for full_path in shared.walk_files(place, allowed_extensions=ext_filter):\n                if os.path.islink(full_path) and not os.path.exists(full_path):\n                    print(f\"Skipping broken symlink: {full_path}\")\n                    continue\n                if ext_blacklist is not None and any(full_path.endswith(x) for x in ext_blacklist):\n                    continue\n                if full_path not in output:\n                    output.append(full_path)\n\n        if model_url is not None and len(output) == 0:\n            if download_name is not None:\n                output.append(load_file_from_url(model_url, model_dir=places[0], file_name=download_name))\n            else:\n                output.append(model_url)\n\n    except Exception:\n        pass\n\n    return output\n\n\ndef friendly_name(file: str):\n    if file.startswith(\"http\"):\n        file = urlparse(file).path\n\n    file = os.path.basename(file)\n    model_name, extension = os.path.splitext(file)\n    return model_name\n\n\ndef load_upscalers():\n    # We can only do this 'magic' method to dynamically load upscalers if they are referenced,\n    # so we'll try to import any _model.py files before looking in __subclasses__\n    modules_dir = os.path.join(shared.script_path, \"modules\")\n    for file in os.listdir(modules_dir):\n        if \"_model.py\" in file:\n            model_name = file.replace(\"_model.py\", \"\")\n            full_model = f\"modules.{model_name}_model\"\n            try:\n                importlib.import_module(full_model)\n            except Exception:\n                pass\n\n    data = []\n    commandline_options = vars(shared.cmd_opts)\n\n    # some of upscaler classes will not go away after reloading their modules, and we'll end\n    # up with two copies of those classes. The newest copy will always be the last in the list,\n    # so we go from end to beginning and ignore duplicates\n    used_classes = {}\n    for cls in reversed(Upscaler.__subclasses__()):\n        classname = str(cls)\n        if classname not in used_classes:\n            used_classes[classname] = cls\n\n    for cls in reversed(used_classes.values()):\n        name = cls.__name__\n        cmd_name = f\"{name.lower().replace('upscaler', '')}_models_path\"\n        commandline_model_path = commandline_options.get(cmd_name, None)\n        scaler = cls(commandline_model_path)\n        scaler.user_path = commandline_model_path\n        scaler.model_download_path = commandline_model_path or scaler.model_path\n        data += scaler.scalers\n\n    shared.sd_upscalers = sorted(\n        data,\n        # Special case for UpscalerNone keeps it at the beginning of the list.\n        key=lambda x: x.name.lower() if not isinstance(x.scaler, (UpscalerNone, UpscalerLanczos, UpscalerNearest)) else \"\"\n    )\n\n\ndef load_spandrel_model(\n    path: str | os.PathLike,\n    *,\n    device: str | torch.device | None,\n    prefer_half: bool = False,\n    dtype: str | torch.dtype | None = None,\n    expected_architecture: str | None = None,\n) -> spandrel.ModelDescriptor:\n    import spandrel\n    model_descriptor = spandrel.ModelLoader(device=device).load_from_file(str(path))\n    if expected_architecture and model_descriptor.architecture != expected_architecture:\n        logger.warning(\n            f\"Model {path!r} is not a {expected_architecture!r} model (got {model_descriptor.architecture!r})\",\n        )\n    half = False\n    if prefer_half:\n        if model_descriptor.supports_half:\n            model_descriptor.model.half()\n            half = True\n        else:\n            logger.info(\"Model %s does not support half precision, ignoring --half\", path)\n    if dtype:\n        model_descriptor.model.to(dtype=dtype)\n    model_descriptor.model.eval()\n    logger.debug(\n        \"Loaded %s from %s (device=%s, half=%s, dtype=%s)\",\n        model_descriptor, path, device, half, dtype,\n    )\n    return model_descriptor\n", "modules/ui_extra_networks_textual_inversion.py": "import os\n\nfrom modules import ui_extra_networks, sd_hijack, shared\nfrom modules.ui_extra_networks import quote_js\n\n\nclass ExtraNetworksPageTextualInversion(ui_extra_networks.ExtraNetworksPage):\n    def __init__(self):\n        super().__init__('Textual Inversion')\n        self.allow_negative_prompt = True\n\n    def refresh(self):\n        sd_hijack.model_hijack.embedding_db.load_textual_inversion_embeddings(force_reload=True)\n\n    def create_item(self, name, index=None, enable_filter=True):\n        embedding = sd_hijack.model_hijack.embedding_db.word_embeddings.get(name)\n        if embedding is None:\n            return\n\n        path, ext = os.path.splitext(embedding.filename)\n        search_terms = [self.search_terms_from_path(embedding.filename)]\n        if embedding.hash:\n            search_terms.append(embedding.hash)\n        return {\n            \"name\": name,\n            \"filename\": embedding.filename,\n            \"shorthash\": embedding.shorthash,\n            \"preview\": self.find_preview(path),\n            \"description\": self.find_description(path),\n            \"search_terms\": search_terms,\n            \"prompt\": quote_js(embedding.name),\n            \"local_preview\": f\"{path}.preview.{shared.opts.samples_format}\",\n            \"sort_keys\": {'default': index, **self.get_sort_keys(embedding.filename)},\n        }\n\n    def list_items(self):\n        # instantiate a list to protect against concurrent modification\n        names = list(sd_hijack.model_hijack.embedding_db.word_embeddings)\n        for index, name in enumerate(names):\n            item = self.create_item(name, index)\n            if item is not None:\n                yield item\n\n    def allowed_directories_for_previews(self):\n        return list(sd_hijack.model_hijack.embedding_db.embedding_dirs)\n", "modules/initialize_util.py": "import json\nimport os\nimport signal\nimport sys\nimport re\n\nfrom modules.timer import startup_timer\n\n\ndef gradio_server_name():\n    from modules.shared_cmd_options import cmd_opts\n\n    if cmd_opts.server_name:\n        return cmd_opts.server_name\n    else:\n        return \"0.0.0.0\" if cmd_opts.listen else None\n\n\ndef fix_torch_version():\n    import torch\n\n    # Truncate version number of nightly/local build of PyTorch to not cause exceptions with CodeFormer or Safetensors\n    if \".dev\" in torch.__version__ or \"+git\" in torch.__version__:\n        torch.__long_version__ = torch.__version__\n        torch.__version__ = re.search(r'[\\d.]+[\\d]', torch.__version__).group(0)\n\ndef fix_pytorch_lightning():\n    # Checks if pytorch_lightning.utilities.distributed already exists in the sys.modules cache\n    if 'pytorch_lightning.utilities.distributed' not in sys.modules:\n        import pytorch_lightning\n        # Lets the user know that the library was not found and then will set it to pytorch_lightning.utilities.rank_zero\n        print(\"Pytorch_lightning.distributed not found, attempting pytorch_lightning.rank_zero\")\n        sys.modules[\"pytorch_lightning.utilities.distributed\"] = pytorch_lightning.utilities.rank_zero\n\ndef fix_asyncio_event_loop_policy():\n    \"\"\"\n        The default `asyncio` event loop policy only automatically creates\n        event loops in the main threads. Other threads must create event\n        loops explicitly or `asyncio.get_event_loop` (and therefore\n        `.IOLoop.current`) will fail. Installing this policy allows event\n        loops to be created automatically on any thread, matching the\n        behavior of Tornado versions prior to 5.0 (or 5.0 on Python 2).\n    \"\"\"\n\n    import asyncio\n\n    if sys.platform == \"win32\" and hasattr(asyncio, \"WindowsSelectorEventLoopPolicy\"):\n        # \"Any thread\" and \"selector\" should be orthogonal, but there's not a clean\n        # interface for composing policies so pick the right base.\n        _BasePolicy = asyncio.WindowsSelectorEventLoopPolicy  # type: ignore\n    else:\n        _BasePolicy = asyncio.DefaultEventLoopPolicy\n\n    class AnyThreadEventLoopPolicy(_BasePolicy):  # type: ignore\n        \"\"\"Event loop policy that allows loop creation on any thread.\n        Usage::\n\n            asyncio.set_event_loop_policy(AnyThreadEventLoopPolicy())\n        \"\"\"\n\n        def get_event_loop(self) -> asyncio.AbstractEventLoop:\n            try:\n                return super().get_event_loop()\n            except (RuntimeError, AssertionError):\n                # This was an AssertionError in python 3.4.2 (which ships with debian jessie)\n                # and changed to a RuntimeError in 3.4.3.\n                # \"There is no current event loop in thread %r\"\n                loop = self.new_event_loop()\n                self.set_event_loop(loop)\n                return loop\n\n    asyncio.set_event_loop_policy(AnyThreadEventLoopPolicy())\n\n\ndef restore_config_state_file():\n    from modules import shared, config_states\n\n    config_state_file = shared.opts.restore_config_state_file\n    if config_state_file == \"\":\n        return\n\n    shared.opts.restore_config_state_file = \"\"\n    shared.opts.save(shared.config_filename)\n\n    if os.path.isfile(config_state_file):\n        print(f\"*** About to restore extension state from file: {config_state_file}\")\n        with open(config_state_file, \"r\", encoding=\"utf-8\") as f:\n            config_state = json.load(f)\n            config_states.restore_extension_config(config_state)\n        startup_timer.record(\"restore extension config\")\n    elif config_state_file:\n        print(f\"!!! Config state backup not found: {config_state_file}\")\n\n\ndef validate_tls_options():\n    from modules.shared_cmd_options import cmd_opts\n\n    if not (cmd_opts.tls_keyfile and cmd_opts.tls_certfile):\n        return\n\n    try:\n        if not os.path.exists(cmd_opts.tls_keyfile):\n            print(\"Invalid path to TLS keyfile given\")\n        if not os.path.exists(cmd_opts.tls_certfile):\n            print(f\"Invalid path to TLS certfile: '{cmd_opts.tls_certfile}'\")\n    except TypeError:\n        cmd_opts.tls_keyfile = cmd_opts.tls_certfile = None\n        print(\"TLS setup invalid, running webui without TLS\")\n    else:\n        print(\"Running with TLS\")\n    startup_timer.record(\"TLS\")\n\n\ndef get_gradio_auth_creds():\n    \"\"\"\n    Convert the gradio_auth and gradio_auth_path commandline arguments into\n    an iterable of (username, password) tuples.\n    \"\"\"\n    from modules.shared_cmd_options import cmd_opts\n\n    def process_credential_line(s):\n        s = s.strip()\n        if not s:\n            return None\n        return tuple(s.split(':', 1))\n\n    if cmd_opts.gradio_auth:\n        for cred in cmd_opts.gradio_auth.split(','):\n            cred = process_credential_line(cred)\n            if cred:\n                yield cred\n\n    if cmd_opts.gradio_auth_path:\n        with open(cmd_opts.gradio_auth_path, 'r', encoding=\"utf8\") as file:\n            for line in file.readlines():\n                for cred in line.strip().split(','):\n                    cred = process_credential_line(cred)\n                    if cred:\n                        yield cred\n\n\ndef dumpstacks():\n    import threading\n    import traceback\n\n    id2name = {th.ident: th.name for th in threading.enumerate()}\n    code = []\n    for threadId, stack in sys._current_frames().items():\n        code.append(f\"\\n# Thread: {id2name.get(threadId, '')}({threadId})\")\n        for filename, lineno, name, line in traceback.extract_stack(stack):\n            code.append(f\"\"\"File: \"{filename}\", line {lineno}, in {name}\"\"\")\n            if line:\n                code.append(\"  \" + line.strip())\n\n    print(\"\\n\".join(code))\n\n\ndef configure_sigint_handler():\n    # make the program just exit at ctrl+c without waiting for anything\n\n    from modules import shared\n\n    def sigint_handler(sig, frame):\n        print(f'Interrupted with signal {sig} in {frame}')\n\n        if shared.opts.dump_stacks_on_signal:\n            dumpstacks()\n\n        os._exit(0)\n\n    if not os.environ.get(\"COVERAGE_RUN\"):\n        # Don't install the immediate-quit handler when running under coverage,\n        # as then the coverage report won't be generated.\n        signal.signal(signal.SIGINT, sigint_handler)\n\n\ndef configure_opts_onchange():\n    from modules import shared, sd_models, sd_vae, ui_tempdir, sd_hijack\n    from modules.call_queue import wrap_queued_call\n\n    shared.opts.onchange(\"sd_model_checkpoint\", wrap_queued_call(lambda: sd_models.reload_model_weights()), call=False)\n    shared.opts.onchange(\"sd_vae\", wrap_queued_call(lambda: sd_vae.reload_vae_weights()), call=False)\n    shared.opts.onchange(\"sd_vae_overrides_per_model_preferences\", wrap_queued_call(lambda: sd_vae.reload_vae_weights()), call=False)\n    shared.opts.onchange(\"temp_dir\", ui_tempdir.on_tmpdir_changed)\n    shared.opts.onchange(\"gradio_theme\", shared.reload_gradio_theme)\n    shared.opts.onchange(\"cross_attention_optimization\", wrap_queued_call(lambda: sd_hijack.model_hijack.redo_hijack(shared.sd_model)), call=False)\n    shared.opts.onchange(\"fp8_storage\", wrap_queued_call(lambda: sd_models.reload_model_weights()), call=False)\n    shared.opts.onchange(\"cache_fp16_weight\", wrap_queued_call(lambda: sd_models.reload_model_weights(forced_reload=True)), call=False)\n    startup_timer.record(\"opts onchange\")\n\n\ndef setup_middleware(app):\n    from starlette.middleware.gzip import GZipMiddleware\n\n    app.middleware_stack = None  # reset current middleware to allow modifying user provided list\n    app.add_middleware(GZipMiddleware, minimum_size=1000)\n    configure_cors_middleware(app)\n    app.build_middleware_stack()  # rebuild middleware stack on-the-fly\n\n\ndef configure_cors_middleware(app):\n    from starlette.middleware.cors import CORSMiddleware\n    from modules.shared_cmd_options import cmd_opts\n\n    cors_options = {\n        \"allow_methods\": [\"*\"],\n        \"allow_headers\": [\"*\"],\n        \"allow_credentials\": True,\n    }\n    if cmd_opts.cors_allow_origins:\n        cors_options[\"allow_origins\"] = cmd_opts.cors_allow_origins.split(',')\n    if cmd_opts.cors_allow_origins_regex:\n        cors_options[\"allow_origin_regex\"] = cmd_opts.cors_allow_origins_regex\n    app.add_middleware(CORSMiddleware, **cors_options)\n\n", "modules/ngrok.py": "import ngrok\n\n# Connect to ngrok for ingress\ndef connect(token, port, options):\n    account = None\n    if token is None:\n        token = 'None'\n    else:\n        if ':' in token:\n            # token = authtoken:username:password\n            token, username, password = token.split(':', 2)\n            account = f\"{username}:{password}\"\n\n    # For all options see: https://github.com/ngrok/ngrok-py/blob/main/examples/ngrok-connect-full.py\n    if not options.get('authtoken_from_env'):\n        options['authtoken'] = token\n    if account:\n        options['basic_auth'] = account\n    if not options.get('session_metadata'):\n        options['session_metadata'] = 'stable-diffusion-webui'\n\n\n    try:\n        public_url = ngrok.connect(f\"127.0.0.1:{port}\", **options).url()\n    except Exception as e:\n        print(f'Invalid ngrok authtoken? ngrok connection aborted due to: {e}\\n'\n              f'Your token: {token}, get the right one on https://dashboard.ngrok.com/get-started/your-authtoken')\n    else:\n        print(f'ngrok connected to localhost:{port}! URL: {public_url}\\n'\n               'You can use this link after the launch is complete.')\n", "modules/sd_hijack_optimizations.py": "from __future__ import annotations\nimport math\nimport psutil\nimport platform\n\nimport torch\nfrom torch import einsum\n\nfrom ldm.util import default\nfrom einops import rearrange\n\nfrom modules import shared, errors, devices, sub_quadratic_attention\nfrom modules.hypernetworks import hypernetwork\n\nimport ldm.modules.attention\nimport ldm.modules.diffusionmodules.model\n\nimport sgm.modules.attention\nimport sgm.modules.diffusionmodules.model\n\ndiffusionmodules_model_AttnBlock_forward = ldm.modules.diffusionmodules.model.AttnBlock.forward\nsgm_diffusionmodules_model_AttnBlock_forward = sgm.modules.diffusionmodules.model.AttnBlock.forward\n\n\nclass SdOptimization:\n    name: str = None\n    label: str | None = None\n    cmd_opt: str | None = None\n    priority: int = 0\n\n    def title(self):\n        if self.label is None:\n            return self.name\n\n        return f\"{self.name} - {self.label}\"\n\n    def is_available(self):\n        return True\n\n    def apply(self):\n        pass\n\n    def undo(self):\n        ldm.modules.attention.CrossAttention.forward = hypernetwork.attention_CrossAttention_forward\n        ldm.modules.diffusionmodules.model.AttnBlock.forward = diffusionmodules_model_AttnBlock_forward\n\n        sgm.modules.attention.CrossAttention.forward = hypernetwork.attention_CrossAttention_forward\n        sgm.modules.diffusionmodules.model.AttnBlock.forward = sgm_diffusionmodules_model_AttnBlock_forward\n\n\nclass SdOptimizationXformers(SdOptimization):\n    name = \"xformers\"\n    cmd_opt = \"xformers\"\n    priority = 100\n\n    def is_available(self):\n        return shared.cmd_opts.force_enable_xformers or (shared.xformers_available and torch.cuda.is_available() and (6, 0) <= torch.cuda.get_device_capability(shared.device) <= (9, 0))\n\n    def apply(self):\n        ldm.modules.attention.CrossAttention.forward = xformers_attention_forward\n        ldm.modules.diffusionmodules.model.AttnBlock.forward = xformers_attnblock_forward\n        sgm.modules.attention.CrossAttention.forward = xformers_attention_forward\n        sgm.modules.diffusionmodules.model.AttnBlock.forward = xformers_attnblock_forward\n\n\nclass SdOptimizationSdpNoMem(SdOptimization):\n    name = \"sdp-no-mem\"\n    label = \"scaled dot product without memory efficient attention\"\n    cmd_opt = \"opt_sdp_no_mem_attention\"\n    priority = 80\n\n    def is_available(self):\n        return hasattr(torch.nn.functional, \"scaled_dot_product_attention\") and callable(torch.nn.functional.scaled_dot_product_attention)\n\n    def apply(self):\n        ldm.modules.attention.CrossAttention.forward = scaled_dot_product_no_mem_attention_forward\n        ldm.modules.diffusionmodules.model.AttnBlock.forward = sdp_no_mem_attnblock_forward\n        sgm.modules.attention.CrossAttention.forward = scaled_dot_product_no_mem_attention_forward\n        sgm.modules.diffusionmodules.model.AttnBlock.forward = sdp_no_mem_attnblock_forward\n\n\nclass SdOptimizationSdp(SdOptimizationSdpNoMem):\n    name = \"sdp\"\n    label = \"scaled dot product\"\n    cmd_opt = \"opt_sdp_attention\"\n    priority = 70\n\n    def apply(self):\n        ldm.modules.attention.CrossAttention.forward = scaled_dot_product_attention_forward\n        ldm.modules.diffusionmodules.model.AttnBlock.forward = sdp_attnblock_forward\n        sgm.modules.attention.CrossAttention.forward = scaled_dot_product_attention_forward\n        sgm.modules.diffusionmodules.model.AttnBlock.forward = sdp_attnblock_forward\n\n\nclass SdOptimizationSubQuad(SdOptimization):\n    name = \"sub-quadratic\"\n    cmd_opt = \"opt_sub_quad_attention\"\n\n    @property\n    def priority(self):\n        return 1000 if shared.device.type == 'mps' else 10\n\n    def apply(self):\n        ldm.modules.attention.CrossAttention.forward = sub_quad_attention_forward\n        ldm.modules.diffusionmodules.model.AttnBlock.forward = sub_quad_attnblock_forward\n        sgm.modules.attention.CrossAttention.forward = sub_quad_attention_forward\n        sgm.modules.diffusionmodules.model.AttnBlock.forward = sub_quad_attnblock_forward\n\n\nclass SdOptimizationV1(SdOptimization):\n    name = \"V1\"\n    label = \"original v1\"\n    cmd_opt = \"opt_split_attention_v1\"\n    priority = 10\n\n    def apply(self):\n        ldm.modules.attention.CrossAttention.forward = split_cross_attention_forward_v1\n        sgm.modules.attention.CrossAttention.forward = split_cross_attention_forward_v1\n\n\nclass SdOptimizationInvokeAI(SdOptimization):\n    name = \"InvokeAI\"\n    cmd_opt = \"opt_split_attention_invokeai\"\n\n    @property\n    def priority(self):\n        return 1000 if shared.device.type != 'mps' and not torch.cuda.is_available() else 10\n\n    def apply(self):\n        ldm.modules.attention.CrossAttention.forward = split_cross_attention_forward_invokeAI\n        sgm.modules.attention.CrossAttention.forward = split_cross_attention_forward_invokeAI\n\n\nclass SdOptimizationDoggettx(SdOptimization):\n    name = \"Doggettx\"\n    cmd_opt = \"opt_split_attention\"\n    priority = 90\n\n    def apply(self):\n        ldm.modules.attention.CrossAttention.forward = split_cross_attention_forward\n        ldm.modules.diffusionmodules.model.AttnBlock.forward = cross_attention_attnblock_forward\n        sgm.modules.attention.CrossAttention.forward = split_cross_attention_forward\n        sgm.modules.diffusionmodules.model.AttnBlock.forward = cross_attention_attnblock_forward\n\n\ndef list_optimizers(res):\n    res.extend([\n        SdOptimizationXformers(),\n        SdOptimizationSdpNoMem(),\n        SdOptimizationSdp(),\n        SdOptimizationSubQuad(),\n        SdOptimizationV1(),\n        SdOptimizationInvokeAI(),\n        SdOptimizationDoggettx(),\n    ])\n\n\nif shared.cmd_opts.xformers or shared.cmd_opts.force_enable_xformers:\n    try:\n        import xformers.ops\n        shared.xformers_available = True\n    except Exception:\n        errors.report(\"Cannot import xformers\", exc_info=True)\n\n\ndef get_available_vram():\n    if shared.device.type == 'cuda':\n        stats = torch.cuda.memory_stats(shared.device)\n        mem_active = stats['active_bytes.all.current']\n        mem_reserved = stats['reserved_bytes.all.current']\n        mem_free_cuda, _ = torch.cuda.mem_get_info(torch.cuda.current_device())\n        mem_free_torch = mem_reserved - mem_active\n        mem_free_total = mem_free_cuda + mem_free_torch\n        return mem_free_total\n    else:\n        return psutil.virtual_memory().available\n\n\n# see https://github.com/basujindal/stable-diffusion/pull/117 for discussion\ndef split_cross_attention_forward_v1(self, x, context=None, mask=None, **kwargs):\n    h = self.heads\n\n    q_in = self.to_q(x)\n    context = default(context, x)\n\n    context_k, context_v = hypernetwork.apply_hypernetworks(shared.loaded_hypernetworks, context)\n    k_in = self.to_k(context_k)\n    v_in = self.to_v(context_v)\n    del context, context_k, context_v, x\n\n    q, k, v = (rearrange(t, 'b n (h d) -> (b h) n d', h=h) for t in (q_in, k_in, v_in))\n    del q_in, k_in, v_in\n\n    dtype = q.dtype\n    if shared.opts.upcast_attn:\n        q, k, v = q.float(), k.float(), v.float()\n\n    with devices.without_autocast(disable=not shared.opts.upcast_attn):\n        r1 = torch.zeros(q.shape[0], q.shape[1], v.shape[2], device=q.device, dtype=q.dtype)\n        for i in range(0, q.shape[0], 2):\n            end = i + 2\n            s1 = einsum('b i d, b j d -> b i j', q[i:end], k[i:end])\n            s1 *= self.scale\n\n            s2 = s1.softmax(dim=-1)\n            del s1\n\n            r1[i:end] = einsum('b i j, b j d -> b i d', s2, v[i:end])\n            del s2\n        del q, k, v\n\n    r1 = r1.to(dtype)\n\n    r2 = rearrange(r1, '(b h) n d -> b n (h d)', h=h)\n    del r1\n\n    return self.to_out(r2)\n\n\n# taken from https://github.com/Doggettx/stable-diffusion and modified\ndef split_cross_attention_forward(self, x, context=None, mask=None, **kwargs):\n    h = self.heads\n\n    q_in = self.to_q(x)\n    context = default(context, x)\n\n    context_k, context_v = hypernetwork.apply_hypernetworks(shared.loaded_hypernetworks, context)\n    k_in = self.to_k(context_k)\n    v_in = self.to_v(context_v)\n\n    dtype = q_in.dtype\n    if shared.opts.upcast_attn:\n        q_in, k_in, v_in = q_in.float(), k_in.float(), v_in if v_in.device.type == 'mps' else v_in.float()\n\n    with devices.without_autocast(disable=not shared.opts.upcast_attn):\n        k_in = k_in * self.scale\n\n        del context, x\n\n        q, k, v = (rearrange(t, 'b n (h d) -> (b h) n d', h=h) for t in (q_in, k_in, v_in))\n        del q_in, k_in, v_in\n\n        r1 = torch.zeros(q.shape[0], q.shape[1], v.shape[2], device=q.device, dtype=q.dtype)\n\n        mem_free_total = get_available_vram()\n\n        gb = 1024 ** 3\n        tensor_size = q.shape[0] * q.shape[1] * k.shape[1] * q.element_size()\n        modifier = 3 if q.element_size() == 2 else 2.5\n        mem_required = tensor_size * modifier\n        steps = 1\n\n        if mem_required > mem_free_total:\n            steps = 2 ** (math.ceil(math.log(mem_required / mem_free_total, 2)))\n            # print(f\"Expected tensor size:{tensor_size/gb:0.1f}GB, cuda free:{mem_free_cuda/gb:0.1f}GB \"\n            #       f\"torch free:{mem_free_torch/gb:0.1f} total:{mem_free_total/gb:0.1f} steps:{steps}\")\n\n        if steps > 64:\n            max_res = math.floor(math.sqrt(math.sqrt(mem_free_total / 2.5)) / 8) * 64\n            raise RuntimeError(f'Not enough memory, use lower resolution (max approx. {max_res}x{max_res}). '\n                               f'Need: {mem_required / 64 / gb:0.1f}GB free, Have:{mem_free_total / gb:0.1f}GB free')\n\n        slice_size = q.shape[1] // steps\n        for i in range(0, q.shape[1], slice_size):\n            end = min(i + slice_size, q.shape[1])\n            s1 = einsum('b i d, b j d -> b i j', q[:, i:end], k)\n\n            s2 = s1.softmax(dim=-1, dtype=q.dtype)\n            del s1\n\n            r1[:, i:end] = einsum('b i j, b j d -> b i d', s2, v)\n            del s2\n\n        del q, k, v\n\n    r1 = r1.to(dtype)\n\n    r2 = rearrange(r1, '(b h) n d -> b n (h d)', h=h)\n    del r1\n\n    return self.to_out(r2)\n\n\n# -- Taken from https://github.com/invoke-ai/InvokeAI and modified --\nmem_total_gb = psutil.virtual_memory().total // (1 << 30)\n\n\ndef einsum_op_compvis(q, k, v):\n    s = einsum('b i d, b j d -> b i j', q, k)\n    s = s.softmax(dim=-1, dtype=s.dtype)\n    return einsum('b i j, b j d -> b i d', s, v)\n\n\ndef einsum_op_slice_0(q, k, v, slice_size):\n    r = torch.zeros(q.shape[0], q.shape[1], v.shape[2], device=q.device, dtype=q.dtype)\n    for i in range(0, q.shape[0], slice_size):\n        end = i + slice_size\n        r[i:end] = einsum_op_compvis(q[i:end], k[i:end], v[i:end])\n    return r\n\n\ndef einsum_op_slice_1(q, k, v, slice_size):\n    r = torch.zeros(q.shape[0], q.shape[1], v.shape[2], device=q.device, dtype=q.dtype)\n    for i in range(0, q.shape[1], slice_size):\n        end = i + slice_size\n        r[:, i:end] = einsum_op_compvis(q[:, i:end], k, v)\n    return r\n\n\ndef einsum_op_mps_v1(q, k, v):\n    if q.shape[0] * q.shape[1] <= 2**16: # (512x512) max q.shape[1]: 4096\n        return einsum_op_compvis(q, k, v)\n    else:\n        slice_size = math.floor(2**30 / (q.shape[0] * q.shape[1]))\n        if slice_size % 4096 == 0:\n            slice_size -= 1\n        return einsum_op_slice_1(q, k, v, slice_size)\n\n\ndef einsum_op_mps_v2(q, k, v):\n    if mem_total_gb > 8 and q.shape[0] * q.shape[1] <= 2**16:\n        return einsum_op_compvis(q, k, v)\n    else:\n        return einsum_op_slice_0(q, k, v, 1)\n\n\ndef einsum_op_tensor_mem(q, k, v, max_tensor_mb):\n    size_mb = q.shape[0] * q.shape[1] * k.shape[1] * q.element_size() // (1 << 20)\n    if size_mb <= max_tensor_mb:\n        return einsum_op_compvis(q, k, v)\n    div = 1 << int((size_mb - 1) / max_tensor_mb).bit_length()\n    if div <= q.shape[0]:\n        return einsum_op_slice_0(q, k, v, q.shape[0] // div)\n    return einsum_op_slice_1(q, k, v, max(q.shape[1] // div, 1))\n\n\ndef einsum_op_cuda(q, k, v):\n    stats = torch.cuda.memory_stats(q.device)\n    mem_active = stats['active_bytes.all.current']\n    mem_reserved = stats['reserved_bytes.all.current']\n    mem_free_cuda, _ = torch.cuda.mem_get_info(q.device)\n    mem_free_torch = mem_reserved - mem_active\n    mem_free_total = mem_free_cuda + mem_free_torch\n    # Divide factor of safety as there's copying and fragmentation\n    return einsum_op_tensor_mem(q, k, v, mem_free_total / 3.3 / (1 << 20))\n\n\ndef einsum_op(q, k, v):\n    if q.device.type == 'cuda':\n        return einsum_op_cuda(q, k, v)\n\n    if q.device.type == 'mps':\n        if mem_total_gb >= 32 and q.shape[0] % 32 != 0 and q.shape[0] * q.shape[1] < 2**18:\n            return einsum_op_mps_v1(q, k, v)\n        return einsum_op_mps_v2(q, k, v)\n\n    # Smaller slices are faster due to L2/L3/SLC caches.\n    # Tested on i7 with 8MB L3 cache.\n    return einsum_op_tensor_mem(q, k, v, 32)\n\n\ndef split_cross_attention_forward_invokeAI(self, x, context=None, mask=None, **kwargs):\n    h = self.heads\n\n    q = self.to_q(x)\n    context = default(context, x)\n\n    context_k, context_v = hypernetwork.apply_hypernetworks(shared.loaded_hypernetworks, context)\n    k = self.to_k(context_k)\n    v = self.to_v(context_v)\n    del context, context_k, context_v, x\n\n    dtype = q.dtype\n    if shared.opts.upcast_attn:\n        q, k, v = q.float(), k.float(), v if v.device.type == 'mps' else v.float()\n\n    with devices.without_autocast(disable=not shared.opts.upcast_attn):\n        k = k * self.scale\n\n        q, k, v = (rearrange(t, 'b n (h d) -> (b h) n d', h=h) for t in (q, k, v))\n        r = einsum_op(q, k, v)\n    r = r.to(dtype)\n    return self.to_out(rearrange(r, '(b h) n d -> b n (h d)', h=h))\n\n# -- End of code from https://github.com/invoke-ai/InvokeAI --\n\n\n# Based on Birch-san's modified implementation of sub-quadratic attention from https://github.com/Birch-san/diffusers/pull/1\n# The sub_quad_attention_forward function is under the MIT License listed under Memory Efficient Attention in the Licenses section of the web UI interface\ndef sub_quad_attention_forward(self, x, context=None, mask=None, **kwargs):\n    assert mask is None, \"attention-mask not currently implemented for SubQuadraticCrossAttnProcessor.\"\n\n    h = self.heads\n\n    q = self.to_q(x)\n    context = default(context, x)\n\n    context_k, context_v = hypernetwork.apply_hypernetworks(shared.loaded_hypernetworks, context)\n    k = self.to_k(context_k)\n    v = self.to_v(context_v)\n    del context, context_k, context_v, x\n\n    q = q.unflatten(-1, (h, -1)).transpose(1,2).flatten(end_dim=1)\n    k = k.unflatten(-1, (h, -1)).transpose(1,2).flatten(end_dim=1)\n    v = v.unflatten(-1, (h, -1)).transpose(1,2).flatten(end_dim=1)\n\n    if q.device.type == 'mps':\n        q, k, v = q.contiguous(), k.contiguous(), v.contiguous()\n\n    dtype = q.dtype\n    if shared.opts.upcast_attn:\n        q, k = q.float(), k.float()\n\n    x = sub_quad_attention(q, k, v, q_chunk_size=shared.cmd_opts.sub_quad_q_chunk_size, kv_chunk_size=shared.cmd_opts.sub_quad_kv_chunk_size, chunk_threshold=shared.cmd_opts.sub_quad_chunk_threshold, use_checkpoint=self.training)\n\n    x = x.to(dtype)\n\n    x = x.unflatten(0, (-1, h)).transpose(1,2).flatten(start_dim=2)\n\n    out_proj, dropout = self.to_out\n    x = out_proj(x)\n    x = dropout(x)\n\n    return x\n\n\ndef sub_quad_attention(q, k, v, q_chunk_size=1024, kv_chunk_size=None, kv_chunk_size_min=None, chunk_threshold=None, use_checkpoint=True):\n    bytes_per_token = torch.finfo(q.dtype).bits//8\n    batch_x_heads, q_tokens, _ = q.shape\n    _, k_tokens, _ = k.shape\n    qk_matmul_size_bytes = batch_x_heads * bytes_per_token * q_tokens * k_tokens\n\n    if chunk_threshold is None:\n        if q.device.type == 'mps':\n            chunk_threshold_bytes = 268435456 * (2 if platform.processor() == 'i386' else bytes_per_token)\n        else:\n            chunk_threshold_bytes = int(get_available_vram() * 0.7)\n    elif chunk_threshold == 0:\n        chunk_threshold_bytes = None\n    else:\n        chunk_threshold_bytes = int(0.01 * chunk_threshold * get_available_vram())\n\n    if kv_chunk_size_min is None and chunk_threshold_bytes is not None:\n        kv_chunk_size_min = chunk_threshold_bytes // (batch_x_heads * bytes_per_token * (k.shape[2] + v.shape[2]))\n    elif kv_chunk_size_min == 0:\n        kv_chunk_size_min = None\n\n    if chunk_threshold_bytes is not None and qk_matmul_size_bytes <= chunk_threshold_bytes:\n        # the big matmul fits into our memory limit; do everything in 1 chunk,\n        # i.e. send it down the unchunked fast-path\n        kv_chunk_size = k_tokens\n\n    with devices.without_autocast(disable=q.dtype == v.dtype):\n        return sub_quadratic_attention.efficient_dot_product_attention(\n            q,\n            k,\n            v,\n            query_chunk_size=q_chunk_size,\n            kv_chunk_size=kv_chunk_size,\n            kv_chunk_size_min = kv_chunk_size_min,\n            use_checkpoint=use_checkpoint,\n        )\n\n\ndef get_xformers_flash_attention_op(q, k, v):\n    if not shared.cmd_opts.xformers_flash_attention:\n        return None\n\n    try:\n        flash_attention_op = xformers.ops.MemoryEfficientAttentionFlashAttentionOp\n        fw, bw = flash_attention_op\n        if fw.supports(xformers.ops.fmha.Inputs(query=q, key=k, value=v, attn_bias=None)):\n            return flash_attention_op\n    except Exception as e:\n        errors.display_once(e, \"enabling flash attention\")\n\n    return None\n\n\ndef xformers_attention_forward(self, x, context=None, mask=None, **kwargs):\n    h = self.heads\n    q_in = self.to_q(x)\n    context = default(context, x)\n\n    context_k, context_v = hypernetwork.apply_hypernetworks(shared.loaded_hypernetworks, context)\n    k_in = self.to_k(context_k)\n    v_in = self.to_v(context_v)\n\n    q, k, v = (rearrange(t, 'b n (h d) -> b n h d', h=h) for t in (q_in, k_in, v_in))\n    del q_in, k_in, v_in\n\n    dtype = q.dtype\n    if shared.opts.upcast_attn:\n        q, k, v = q.float(), k.float(), v.float()\n\n    out = xformers.ops.memory_efficient_attention(q, k, v, attn_bias=None, op=get_xformers_flash_attention_op(q, k, v))\n\n    out = out.to(dtype)\n\n    out = rearrange(out, 'b n h d -> b n (h d)', h=h)\n    return self.to_out(out)\n\n\n# Based on Diffusers usage of scaled dot product attention from https://github.com/huggingface/diffusers/blob/c7da8fd23359a22d0df2741688b5b4f33c26df21/src/diffusers/models/cross_attention.py\n# The scaled_dot_product_attention_forward function contains parts of code under Apache-2.0 license listed under Scaled Dot Product Attention in the Licenses section of the web UI interface\ndef scaled_dot_product_attention_forward(self, x, context=None, mask=None, **kwargs):\n    batch_size, sequence_length, inner_dim = x.shape\n\n    if mask is not None:\n        mask = self.prepare_attention_mask(mask, sequence_length, batch_size)\n        mask = mask.view(batch_size, self.heads, -1, mask.shape[-1])\n\n    h = self.heads\n    q_in = self.to_q(x)\n    context = default(context, x)\n\n    context_k, context_v = hypernetwork.apply_hypernetworks(shared.loaded_hypernetworks, context)\n    k_in = self.to_k(context_k)\n    v_in = self.to_v(context_v)\n\n    head_dim = inner_dim // h\n    q = q_in.view(batch_size, -1, h, head_dim).transpose(1, 2)\n    k = k_in.view(batch_size, -1, h, head_dim).transpose(1, 2)\n    v = v_in.view(batch_size, -1, h, head_dim).transpose(1, 2)\n\n    del q_in, k_in, v_in\n\n    dtype = q.dtype\n    if shared.opts.upcast_attn:\n        q, k, v = q.float(), k.float(), v.float()\n\n    # the output of sdp = (batch, num_heads, seq_len, head_dim)\n    hidden_states = torch.nn.functional.scaled_dot_product_attention(\n        q, k, v, attn_mask=mask, dropout_p=0.0, is_causal=False\n    )\n\n    hidden_states = hidden_states.transpose(1, 2).reshape(batch_size, -1, h * head_dim)\n    hidden_states = hidden_states.to(dtype)\n\n    # linear proj\n    hidden_states = self.to_out[0](hidden_states)\n    # dropout\n    hidden_states = self.to_out[1](hidden_states)\n    return hidden_states\n\n\ndef scaled_dot_product_no_mem_attention_forward(self, x, context=None, mask=None, **kwargs):\n    with torch.backends.cuda.sdp_kernel(enable_flash=True, enable_math=True, enable_mem_efficient=False):\n        return scaled_dot_product_attention_forward(self, x, context, mask)\n\n\ndef cross_attention_attnblock_forward(self, x):\n        h_ = x\n        h_ = self.norm(h_)\n        q1 = self.q(h_)\n        k1 = self.k(h_)\n        v = self.v(h_)\n\n        # compute attention\n        b, c, h, w = q1.shape\n\n        q2 = q1.reshape(b, c, h*w)\n        del q1\n\n        q = q2.permute(0, 2, 1)   # b,hw,c\n        del q2\n\n        k = k1.reshape(b, c, h*w) # b,c,hw\n        del k1\n\n        h_ = torch.zeros_like(k, device=q.device)\n\n        mem_free_total = get_available_vram()\n\n        tensor_size = q.shape[0] * q.shape[1] * k.shape[2] * q.element_size()\n        mem_required = tensor_size * 2.5\n        steps = 1\n\n        if mem_required > mem_free_total:\n            steps = 2**(math.ceil(math.log(mem_required / mem_free_total, 2)))\n\n        slice_size = q.shape[1] // steps if (q.shape[1] % steps) == 0 else q.shape[1]\n        for i in range(0, q.shape[1], slice_size):\n            end = i + slice_size\n\n            w1 = torch.bmm(q[:, i:end], k)     # b,hw,hw    w[b,i,j]=sum_c q[b,i,c]k[b,c,j]\n            w2 = w1 * (int(c)**(-0.5))\n            del w1\n            w3 = torch.nn.functional.softmax(w2, dim=2, dtype=q.dtype)\n            del w2\n\n            # attend to values\n            v1 = v.reshape(b, c, h*w)\n            w4 = w3.permute(0, 2, 1)   # b,hw,hw (first hw of k, second of q)\n            del w3\n\n            h_[:, :, i:end] = torch.bmm(v1, w4)     # b, c,hw (hw of q) h_[b,c,j] = sum_i v[b,c,i] w_[b,i,j]\n            del v1, w4\n\n        h2 = h_.reshape(b, c, h, w)\n        del h_\n\n        h3 = self.proj_out(h2)\n        del h2\n\n        h3 += x\n\n        return h3\n\n\ndef xformers_attnblock_forward(self, x):\n    try:\n        h_ = x\n        h_ = self.norm(h_)\n        q = self.q(h_)\n        k = self.k(h_)\n        v = self.v(h_)\n        b, c, h, w = q.shape\n        q, k, v = (rearrange(t, 'b c h w -> b (h w) c') for t in (q, k, v))\n        dtype = q.dtype\n        if shared.opts.upcast_attn:\n            q, k = q.float(), k.float()\n        q = q.contiguous()\n        k = k.contiguous()\n        v = v.contiguous()\n        out = xformers.ops.memory_efficient_attention(q, k, v, op=get_xformers_flash_attention_op(q, k, v))\n        out = out.to(dtype)\n        out = rearrange(out, 'b (h w) c -> b c h w', h=h)\n        out = self.proj_out(out)\n        return x + out\n    except NotImplementedError:\n        return cross_attention_attnblock_forward(self, x)\n\n\ndef sdp_attnblock_forward(self, x):\n    h_ = x\n    h_ = self.norm(h_)\n    q = self.q(h_)\n    k = self.k(h_)\n    v = self.v(h_)\n    b, c, h, w = q.shape\n    q, k, v = (rearrange(t, 'b c h w -> b (h w) c') for t in (q, k, v))\n    dtype = q.dtype\n    if shared.opts.upcast_attn:\n        q, k, v = q.float(), k.float(), v.float()\n    q = q.contiguous()\n    k = k.contiguous()\n    v = v.contiguous()\n    out = torch.nn.functional.scaled_dot_product_attention(q, k, v, dropout_p=0.0, is_causal=False)\n    out = out.to(dtype)\n    out = rearrange(out, 'b (h w) c -> b c h w', h=h)\n    out = self.proj_out(out)\n    return x + out\n\n\ndef sdp_no_mem_attnblock_forward(self, x):\n    with torch.backends.cuda.sdp_kernel(enable_flash=True, enable_math=True, enable_mem_efficient=False):\n        return sdp_attnblock_forward(self, x)\n\n\ndef sub_quad_attnblock_forward(self, x):\n    h_ = x\n    h_ = self.norm(h_)\n    q = self.q(h_)\n    k = self.k(h_)\n    v = self.v(h_)\n    b, c, h, w = q.shape\n    q, k, v = (rearrange(t, 'b c h w -> b (h w) c') for t in (q, k, v))\n    q = q.contiguous()\n    k = k.contiguous()\n    v = v.contiguous()\n    out = sub_quad_attention(q, k, v, q_chunk_size=shared.cmd_opts.sub_quad_q_chunk_size, kv_chunk_size=shared.cmd_opts.sub_quad_kv_chunk_size, chunk_threshold=shared.cmd_opts.sub_quad_chunk_threshold, use_checkpoint=self.training)\n    out = rearrange(out, 'b (h w) c -> b c h w', h=h)\n    out = self.proj_out(out)\n    return x + out\n", "modules/extra_networks.py": "import json\nimport os\nimport re\nimport logging\nfrom collections import defaultdict\n\nfrom modules import errors\n\nextra_network_registry = {}\nextra_network_aliases = {}\n\n\ndef initialize():\n    extra_network_registry.clear()\n    extra_network_aliases.clear()\n\n\ndef register_extra_network(extra_network):\n    extra_network_registry[extra_network.name] = extra_network\n\n\ndef register_extra_network_alias(extra_network, alias):\n    extra_network_aliases[alias] = extra_network\n\n\ndef register_default_extra_networks():\n    from modules.extra_networks_hypernet import ExtraNetworkHypernet\n    register_extra_network(ExtraNetworkHypernet())\n\n\nclass ExtraNetworkParams:\n    def __init__(self, items=None):\n        self.items = items or []\n        self.positional = []\n        self.named = {}\n\n        for item in self.items:\n            parts = item.split('=', 2) if isinstance(item, str) else [item]\n            if len(parts) == 2:\n                self.named[parts[0]] = parts[1]\n            else:\n                self.positional.append(item)\n\n    def __eq__(self, other):\n        return self.items == other.items\n\n\nclass ExtraNetwork:\n    def __init__(self, name):\n        self.name = name\n\n    def activate(self, p, params_list):\n        \"\"\"\n        Called by processing on every run. Whatever the extra network is meant to do should be activated here.\n        Passes arguments related to this extra network in params_list.\n        User passes arguments by specifying this in his prompt:\n\n        <name:arg1:arg2:arg3>\n\n        Where name matches the name of this ExtraNetwork object, and arg1:arg2:arg3 are any natural number of text arguments\n        separated by colon.\n\n        Even if the user does not mention this ExtraNetwork in his prompt, the call will still be made, with empty params_list -\n        in this case, all effects of this extra networks should be disabled.\n\n        Can be called multiple times before deactivate() - each new call should override the previous call completely.\n\n        For example, if this ExtraNetwork's name is 'hypernet' and user's prompt is:\n\n        > \"1girl, <hypernet:agm:1.1> <extrasupernet:master:12:13:14> <hypernet:ray>\"\n\n        params_list will be:\n\n        [\n            ExtraNetworkParams(items=[\"agm\", \"1.1\"]),\n            ExtraNetworkParams(items=[\"ray\"])\n        ]\n\n        \"\"\"\n        raise NotImplementedError\n\n    def deactivate(self, p):\n        \"\"\"\n        Called at the end of processing for housekeeping. No need to do anything here.\n        \"\"\"\n\n        raise NotImplementedError\n\n\ndef lookup_extra_networks(extra_network_data):\n    \"\"\"returns a dict mapping ExtraNetwork objects to lists of arguments for those extra networks.\n\n    Example input:\n    {\n        'lora': [<modules.extra_networks.ExtraNetworkParams object at 0x0000020690D58310>],\n        'lyco': [<modules.extra_networks.ExtraNetworkParams object at 0x0000020690D58F70>],\n        'hypernet': [<modules.extra_networks.ExtraNetworkParams object at 0x0000020690D5A800>]\n    }\n\n    Example output:\n\n    {\n        <extra_networks_lora.ExtraNetworkLora object at 0x0000020581BEECE0>: [<modules.extra_networks.ExtraNetworkParams object at 0x0000020690D58310>, <modules.extra_networks.ExtraNetworkParams object at 0x0000020690D58F70>],\n        <modules.extra_networks_hypernet.ExtraNetworkHypernet object at 0x0000020581BEEE60>: [<modules.extra_networks.ExtraNetworkParams object at 0x0000020690D5A800>]\n    }\n    \"\"\"\n\n    res = {}\n\n    for extra_network_name, extra_network_args in list(extra_network_data.items()):\n        extra_network = extra_network_registry.get(extra_network_name, None)\n        alias = extra_network_aliases.get(extra_network_name, None)\n\n        if alias is not None and extra_network is None:\n            extra_network = alias\n\n        if extra_network is None:\n            logging.info(f\"Skipping unknown extra network: {extra_network_name}\")\n            continue\n\n        res.setdefault(extra_network, []).extend(extra_network_args)\n\n    return res\n\n\ndef activate(p, extra_network_data):\n    \"\"\"call activate for extra networks in extra_network_data in specified order, then call\n    activate for all remaining registered networks with an empty argument list\"\"\"\n\n    activated = []\n\n    for extra_network, extra_network_args in lookup_extra_networks(extra_network_data).items():\n\n        try:\n            extra_network.activate(p, extra_network_args)\n            activated.append(extra_network)\n        except Exception as e:\n            errors.display(e, f\"activating extra network {extra_network.name} with arguments {extra_network_args}\")\n\n    for extra_network_name, extra_network in extra_network_registry.items():\n        if extra_network in activated:\n            continue\n\n        try:\n            extra_network.activate(p, [])\n        except Exception as e:\n            errors.display(e, f\"activating extra network {extra_network_name}\")\n\n    if p.scripts is not None:\n        p.scripts.after_extra_networks_activate(p, batch_number=p.iteration, prompts=p.prompts, seeds=p.seeds, subseeds=p.subseeds, extra_network_data=extra_network_data)\n\n\ndef deactivate(p, extra_network_data):\n    \"\"\"call deactivate for extra networks in extra_network_data in specified order, then call\n    deactivate for all remaining registered networks\"\"\"\n\n    data = lookup_extra_networks(extra_network_data)\n\n    for extra_network in data:\n        try:\n            extra_network.deactivate(p)\n        except Exception as e:\n            errors.display(e, f\"deactivating extra network {extra_network.name}\")\n\n    for extra_network_name, extra_network in extra_network_registry.items():\n        if extra_network in data:\n            continue\n\n        try:\n            extra_network.deactivate(p)\n        except Exception as e:\n            errors.display(e, f\"deactivating unmentioned extra network {extra_network_name}\")\n\n\nre_extra_net = re.compile(r\"<(\\w+):([^>]+)>\")\n\n\ndef parse_prompt(prompt):\n    res = defaultdict(list)\n\n    def found(m):\n        name = m.group(1)\n        args = m.group(2)\n\n        res[name].append(ExtraNetworkParams(items=args.split(\":\")))\n\n        return \"\"\n\n    prompt = re.sub(re_extra_net, found, prompt)\n\n    return prompt, res\n\n\ndef parse_prompts(prompts):\n    res = []\n    extra_data = None\n\n    for prompt in prompts:\n        updated_prompt, parsed_extra_data = parse_prompt(prompt)\n\n        if extra_data is None:\n            extra_data = parsed_extra_data\n\n        res.append(updated_prompt)\n\n    return res, extra_data\n\n\ndef get_user_metadata(filename, lister=None):\n    if filename is None:\n        return {}\n\n    basename, ext = os.path.splitext(filename)\n    metadata_filename = basename + '.json'\n\n    metadata = {}\n    try:\n        exists = lister.exists(metadata_filename) if lister else os.path.exists(metadata_filename)\n        if exists:\n            with open(metadata_filename, \"r\", encoding=\"utf8\") as file:\n                metadata = json.load(file)\n    except Exception as e:\n        errors.display(e, f\"reading extra network user metadata from {metadata_filename}\")\n\n    return metadata\n", "modules/xlmr_m18.py": "from transformers import BertPreTrainedModel,BertConfig\nimport torch.nn as nn\nimport torch\nfrom transformers.models.xlm_roberta.configuration_xlm_roberta import XLMRobertaConfig\nfrom transformers import XLMRobertaModel,XLMRobertaTokenizer\nfrom typing import Optional\nfrom modules import torch_utils\n\n\nclass BertSeriesConfig(BertConfig):\n    def __init__(self, vocab_size=30522, hidden_size=768, num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072, hidden_act=\"gelu\", hidden_dropout_prob=0.1, attention_probs_dropout_prob=0.1, max_position_embeddings=512, type_vocab_size=2, initializer_range=0.02, layer_norm_eps=1e-12, pad_token_id=0, position_embedding_type=\"absolute\", use_cache=True, classifier_dropout=None,project_dim=512, pooler_fn=\"average\",learn_encoder=False,model_type='bert',**kwargs):\n\n        super().__init__(vocab_size, hidden_size, num_hidden_layers, num_attention_heads, intermediate_size, hidden_act, hidden_dropout_prob, attention_probs_dropout_prob, max_position_embeddings, type_vocab_size, initializer_range, layer_norm_eps, pad_token_id, position_embedding_type, use_cache, classifier_dropout, **kwargs)\n        self.project_dim = project_dim\n        self.pooler_fn = pooler_fn\n        self.learn_encoder = learn_encoder\n\nclass RobertaSeriesConfig(XLMRobertaConfig):\n    def __init__(self, pad_token_id=1, bos_token_id=0, eos_token_id=2,project_dim=512,pooler_fn='cls',learn_encoder=False, **kwargs):\n        super().__init__(pad_token_id=pad_token_id, bos_token_id=bos_token_id, eos_token_id=eos_token_id, **kwargs)\n        self.project_dim = project_dim\n        self.pooler_fn = pooler_fn\n        self.learn_encoder = learn_encoder\n\n\nclass BertSeriesModelWithTransformation(BertPreTrainedModel):\n\n    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n    _keys_to_ignore_on_load_missing = [r\"position_ids\", r\"predictions.decoder.bias\"]\n    config_class = BertSeriesConfig\n\n    def __init__(self, config=None, **kargs):\n        # modify initialization for autoloading\n        if config is None:\n            config = XLMRobertaConfig()\n            config.attention_probs_dropout_prob= 0.1\n            config.bos_token_id=0\n            config.eos_token_id=2\n            config.hidden_act='gelu'\n            config.hidden_dropout_prob=0.1\n            config.hidden_size=1024\n            config.initializer_range=0.02\n            config.intermediate_size=4096\n            config.layer_norm_eps=1e-05\n            config.max_position_embeddings=514\n\n            config.num_attention_heads=16\n            config.num_hidden_layers=24\n            config.output_past=True\n            config.pad_token_id=1\n            config.position_embedding_type= \"absolute\"\n\n            config.type_vocab_size= 1\n            config.use_cache=True\n            config.vocab_size= 250002\n            config.project_dim = 1024\n            config.learn_encoder = False\n        super().__init__(config)\n        self.roberta = XLMRobertaModel(config)\n        self.transformation = nn.Linear(config.hidden_size,config.project_dim)\n        # self.pre_LN=nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n        self.tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-large')\n        # self.pooler = lambda x: x[:,0]\n        # self.post_init()\n\n        self.has_pre_transformation = True\n        if self.has_pre_transformation:\n            self.transformation_pre = nn.Linear(config.hidden_size, config.project_dim)\n            self.pre_LN = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n        self.post_init()\n\n    def encode(self,c):\n        device = torch_utils.get_param(self).device\n        text = self.tokenizer(c,\n                        truncation=True,\n                        max_length=77,\n                        return_length=False,\n                        return_overflowing_tokens=False,\n                        padding=\"max_length\",\n                        return_tensors=\"pt\")\n        text[\"input_ids\"] = torch.tensor(text[\"input_ids\"]).to(device)\n        text[\"attention_mask\"] = torch.tensor(\n            text['attention_mask']).to(device)\n        features = self(**text)\n        return features['projection_state']\n\n    def forward(\n        self,\n        input_ids: Optional[torch.Tensor] = None,\n        attention_mask: Optional[torch.Tensor] = None,\n        token_type_ids: Optional[torch.Tensor] = None,\n        position_ids: Optional[torch.Tensor] = None,\n        head_mask: Optional[torch.Tensor] = None,\n        inputs_embeds: Optional[torch.Tensor] = None,\n        encoder_hidden_states: Optional[torch.Tensor] = None,\n        encoder_attention_mask: Optional[torch.Tensor] = None,\n        output_attentions: Optional[bool] = None,\n        return_dict: Optional[bool] = None,\n        output_hidden_states: Optional[bool] = None,\n    ) :\n        r\"\"\"\n        \"\"\"\n\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n\n        outputs = self.roberta(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            encoder_hidden_states=encoder_hidden_states,\n            encoder_attention_mask=encoder_attention_mask,\n            output_attentions=output_attentions,\n            output_hidden_states=True,\n            return_dict=return_dict,\n        )\n\n        # # last module outputs\n        # sequence_output = outputs[0]\n\n\n        # # project every module\n        # sequence_output_ln = self.pre_LN(sequence_output)\n\n        # # pooler\n        # pooler_output = self.pooler(sequence_output_ln)\n        # pooler_output = self.transformation(pooler_output)\n        # projection_state = self.transformation(outputs.last_hidden_state)\n\n        if self.has_pre_transformation:\n            sequence_output2 = outputs[\"hidden_states\"][-2]\n            sequence_output2 = self.pre_LN(sequence_output2)\n            projection_state2 = self.transformation_pre(sequence_output2)\n\n            return {\n                \"projection_state\": projection_state2,\n                \"last_hidden_state\": outputs.last_hidden_state,\n                \"hidden_states\": outputs.hidden_states,\n                \"attentions\": outputs.attentions,\n            }\n        else:\n            projection_state = self.transformation(outputs.last_hidden_state)\n            return {\n                \"projection_state\": projection_state,\n                \"last_hidden_state\": outputs.last_hidden_state,\n                \"hidden_states\": outputs.hidden_states,\n                \"attentions\": outputs.attentions,\n            }\n\n\n        # return {\n        #     'pooler_output':pooler_output,\n        #     'last_hidden_state':outputs.last_hidden_state,\n        #     'hidden_states':outputs.hidden_states,\n        #     'attentions':outputs.attentions,\n        #     'projection_state':projection_state,\n        #     'sequence_out': sequence_output\n        # }\n\n\nclass RobertaSeriesModelWithTransformation(BertSeriesModelWithTransformation):\n    base_model_prefix = 'roberta'\n    config_class= RobertaSeriesConfig\n", "modules/ui_settings.py": "import gradio as gr\n\nfrom modules import ui_common, shared, script_callbacks, scripts, sd_models, sysinfo, timer, shared_items\nfrom modules.call_queue import wrap_gradio_call\nfrom modules.options import options_section\nfrom modules.shared import opts\nfrom modules.ui_components import FormRow\nfrom modules.ui_gradio_extensions import reload_javascript\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\n\n\ndef get_value_for_setting(key):\n    value = getattr(opts, key)\n\n    info = opts.data_labels[key]\n    args = info.component_args() if callable(info.component_args) else info.component_args or {}\n    args = {k: v for k, v in args.items() if k not in {'precision'}}\n\n    return gr.update(value=value, **args)\n\n\ndef create_setting_component(key, is_quicksettings=False):\n    def fun():\n        return opts.data[key] if key in opts.data else opts.data_labels[key].default\n\n    info = opts.data_labels[key]\n    t = type(info.default)\n\n    args = info.component_args() if callable(info.component_args) else info.component_args\n\n    if info.component is not None:\n        comp = info.component\n    elif t == str:\n        comp = gr.Textbox\n    elif t == int:\n        comp = gr.Number\n    elif t == bool:\n        comp = gr.Checkbox\n    else:\n        raise Exception(f'bad options item type: {t} for key {key}')\n\n    elem_id = f\"setting_{key}\"\n\n    if info.refresh is not None:\n        if is_quicksettings:\n            res = comp(label=info.label, value=fun(), elem_id=elem_id, **(args or {}))\n            ui_common.create_refresh_button(res, info.refresh, info.component_args, f\"refresh_{key}\")\n        else:\n            with FormRow():\n                res = comp(label=info.label, value=fun(), elem_id=elem_id, **(args or {}))\n                ui_common.create_refresh_button(res, info.refresh, info.component_args, f\"refresh_{key}\")\n    else:\n        res = comp(label=info.label, value=fun(), elem_id=elem_id, **(args or {}))\n\n    return res\n\n\nclass UiSettings:\n    submit = None\n    result = None\n    interface = None\n    components = None\n    component_dict = None\n    dummy_component = None\n    quicksettings_list = None\n    quicksettings_names = None\n    text_settings = None\n    show_all_pages = None\n    show_one_page = None\n    search_input = None\n\n    def run_settings(self, *args):\n        changed = []\n\n        for key, value, comp in zip(opts.data_labels.keys(), args, self.components):\n            assert comp == self.dummy_component or opts.same_type(value, opts.data_labels[key].default), f\"Bad value for setting {key}: {value}; expecting {type(opts.data_labels[key].default).__name__}\"\n\n        for key, value, comp in zip(opts.data_labels.keys(), args, self.components):\n            if comp == self.dummy_component:\n                continue\n\n            if opts.set(key, value):\n                changed.append(key)\n\n        try:\n            opts.save(shared.config_filename)\n        except RuntimeError:\n            return opts.dumpjson(), f'{len(changed)} settings changed without save: {\", \".join(changed)}.'\n        return opts.dumpjson(), f'{len(changed)} settings changed{\": \" if changed else \"\"}{\", \".join(changed)}.'\n\n    def run_settings_single(self, value, key):\n        if not opts.same_type(value, opts.data_labels[key].default):\n            return gr.update(visible=True), opts.dumpjson()\n\n        if value is None or not opts.set(key, value):\n            return gr.update(value=getattr(opts, key)), opts.dumpjson()\n\n        opts.save(shared.config_filename)\n\n        return get_value_for_setting(key), opts.dumpjson()\n\n    def register_settings(self):\n        script_callbacks.ui_settings_callback()\n\n    def create_ui(self, loadsave, dummy_component):\n        self.components = []\n        self.component_dict = {}\n        self.dummy_component = dummy_component\n\n        shared.settings_components = self.component_dict\n\n        # we add this as late as possible so that scripts have already registered their callbacks\n        opts.data_labels.update(options_section(('callbacks', \"Callbacks\", \"system\"), {\n            **shared_items.callbacks_order_settings(),\n        }))\n\n        opts.reorder()\n\n        with gr.Blocks(analytics_enabled=False) as settings_interface:\n            with gr.Row():\n                with gr.Column(scale=6):\n                    self.submit = gr.Button(value=\"Apply settings\", variant='primary', elem_id=\"settings_submit\")\n                with gr.Column():\n                    restart_gradio = gr.Button(value='Reload UI', variant='primary', elem_id=\"settings_restart_gradio\")\n\n            self.result = gr.HTML(elem_id=\"settings_result\")\n\n            self.quicksettings_names = opts.quicksettings_list\n            self.quicksettings_names = {x: i for i, x in enumerate(self.quicksettings_names) if x != 'quicksettings'}\n\n            self.quicksettings_list = []\n\n            previous_section = None\n            current_tab = None\n            current_row = None\n            with gr.Tabs(elem_id=\"settings\"):\n                for i, (k, item) in enumerate(opts.data_labels.items()):\n                    section_must_be_skipped = item.section[0] is None\n\n                    if previous_section != item.section and not section_must_be_skipped:\n                        elem_id, text = item.section\n\n                        if current_tab is not None:\n                            current_row.__exit__()\n                            current_tab.__exit__()\n\n                        gr.Group()\n                        current_tab = gr.TabItem(elem_id=f\"settings_{elem_id}\", label=text)\n                        current_tab.__enter__()\n                        current_row = gr.Column(elem_id=f\"column_settings_{elem_id}\", variant='compact')\n                        current_row.__enter__()\n\n                        previous_section = item.section\n\n                    if k in self.quicksettings_names and not shared.cmd_opts.freeze_settings:\n                        self.quicksettings_list.append((i, k, item))\n                        self.components.append(dummy_component)\n                    elif section_must_be_skipped:\n                        self.components.append(dummy_component)\n                    else:\n                        component = create_setting_component(k)\n                        self.component_dict[k] = component\n                        self.components.append(component)\n\n                if current_tab is not None:\n                    current_row.__exit__()\n                    current_tab.__exit__()\n\n                with gr.TabItem(\"Defaults\", id=\"defaults\", elem_id=\"settings_tab_defaults\"):\n                    loadsave.create_ui()\n\n                with gr.TabItem(\"Sysinfo\", id=\"sysinfo\", elem_id=\"settings_tab_sysinfo\"):\n                    gr.HTML('<a href=\"./internal/sysinfo-download\" class=\"sysinfo_big_link\" download>Download system info</a><br /><a href=\"./internal/sysinfo\" target=\"_blank\">(or open as text in a new page)</a>', elem_id=\"sysinfo_download\")\n\n                    with gr.Row():\n                        with gr.Column(scale=1):\n                            sysinfo_check_file = gr.File(label=\"Check system info for validity\", type='binary')\n                        with gr.Column(scale=1):\n                            sysinfo_check_output = gr.HTML(\"\", elem_id=\"sysinfo_validity\")\n                        with gr.Column(scale=100):\n                            pass\n\n                with gr.TabItem(\"Actions\", id=\"actions\", elem_id=\"settings_tab_actions\"):\n                    request_notifications = gr.Button(value='Request browser notifications', elem_id=\"request_notifications\")\n                    download_localization = gr.Button(value='Download localization template', elem_id=\"download_localization\")\n                    reload_script_bodies = gr.Button(value='Reload custom script bodies (No ui updates, No restart)', variant='secondary', elem_id=\"settings_reload_script_bodies\")\n                    with gr.Row():\n                        unload_sd_model = gr.Button(value='Unload SD checkpoint to RAM', elem_id=\"sett_unload_sd_model\")\n                        reload_sd_model = gr.Button(value='Load SD checkpoint to VRAM from RAM', elem_id=\"sett_reload_sd_model\")\n                    with gr.Row():\n                        calculate_all_checkpoint_hash = gr.Button(value='Calculate hash for all checkpoint', elem_id=\"calculate_all_checkpoint_hash\")\n                        calculate_all_checkpoint_hash_threads = gr.Number(value=1, label=\"Number of parallel calculations\", elem_id=\"calculate_all_checkpoint_hash_threads\", precision=0, minimum=1)\n\n                with gr.TabItem(\"Licenses\", id=\"licenses\", elem_id=\"settings_tab_licenses\"):\n                    gr.HTML(shared.html(\"licenses.html\"), elem_id=\"licenses\")\n\n                self.show_all_pages = gr.Button(value=\"Show all pages\", elem_id=\"settings_show_all_pages\")\n                self.show_one_page = gr.Button(value=\"Show only one page\", elem_id=\"settings_show_one_page\", visible=False)\n                self.show_one_page.click(lambda: None)\n\n                self.search_input = gr.Textbox(value=\"\", elem_id=\"settings_search\", max_lines=1, placeholder=\"Search...\", show_label=False)\n\n                self.text_settings = gr.Textbox(elem_id=\"settings_json\", value=lambda: opts.dumpjson(), visible=False)\n\n            def call_func_and_return_text(func, text):\n                def handler():\n                    t = timer.Timer()\n                    func()\n                    t.record(text)\n\n                    return f'{text} in {t.total:.1f}s'\n\n                return handler\n\n            unload_sd_model.click(\n                fn=call_func_and_return_text(sd_models.unload_model_weights, 'Unloaded the checkpoint'),\n                inputs=[],\n                outputs=[self.result]\n            )\n\n            reload_sd_model.click(\n                fn=call_func_and_return_text(lambda: sd_models.send_model_to_device(shared.sd_model), 'Loaded the checkpoint'),\n                inputs=[],\n                outputs=[self.result]\n            )\n\n            request_notifications.click(\n                fn=lambda: None,\n                inputs=[],\n                outputs=[],\n                _js='function(){}'\n            )\n\n            download_localization.click(\n                fn=lambda: None,\n                inputs=[],\n                outputs=[],\n                _js='download_localization'\n            )\n\n            def reload_scripts():\n                scripts.reload_script_body_only()\n                reload_javascript()  # need to refresh the html page\n\n            reload_script_bodies.click(\n                fn=reload_scripts,\n                inputs=[],\n                outputs=[]\n            )\n\n            restart_gradio.click(\n                fn=shared.state.request_restart,\n                _js='restart_reload',\n                inputs=[],\n                outputs=[],\n            )\n\n            def check_file(x):\n                if x is None:\n                    return ''\n\n                if sysinfo.check(x.decode('utf8', errors='ignore')):\n                    return 'Valid'\n\n                return 'Invalid'\n\n            sysinfo_check_file.change(\n                fn=check_file,\n                inputs=[sysinfo_check_file],\n                outputs=[sysinfo_check_output],\n            )\n\n            def calculate_all_checkpoint_hash_fn(max_thread):\n                checkpoints_list = sd_models.checkpoints_list.values()\n                with ThreadPoolExecutor(max_workers=max_thread) as executor:\n                    futures = [executor.submit(checkpoint.calculate_shorthash) for checkpoint in checkpoints_list]\n                    completed = 0\n                    for _ in as_completed(futures):\n                        completed += 1\n                        print(f\"{completed} / {len(checkpoints_list)} \")\n                    print(\"Finish calculating hash for all checkpoints\")\n\n            calculate_all_checkpoint_hash.click(\n                fn=calculate_all_checkpoint_hash_fn,\n                inputs=[calculate_all_checkpoint_hash_threads],\n            )\n\n        self.interface = settings_interface\n\n    def add_quicksettings(self):\n        with gr.Row(elem_id=\"quicksettings\", variant=\"compact\"):\n            for _i, k, _item in sorted(self.quicksettings_list, key=lambda x: self.quicksettings_names.get(x[1], x[0])):\n                component = create_setting_component(k, is_quicksettings=True)\n                self.component_dict[k] = component\n\n    def add_functionality(self, demo):\n        self.submit.click(\n            fn=wrap_gradio_call(lambda *args: self.run_settings(*args), extra_outputs=[gr.update()]),\n            inputs=self.components,\n            outputs=[self.text_settings, self.result],\n        )\n\n        for _i, k, _item in self.quicksettings_list:\n            component = self.component_dict[k]\n            info = opts.data_labels[k]\n\n            if isinstance(component, gr.Textbox):\n                methods = [component.submit, component.blur]\n            elif hasattr(component, 'release'):\n                methods = [component.release]\n            else:\n                methods = [component.change]\n\n            for method in methods:\n                method(\n                    fn=lambda value, k=k: self.run_settings_single(value, key=k),\n                    inputs=[component],\n                    outputs=[component, self.text_settings],\n                    show_progress=info.refresh is not None,\n                )\n\n        button_set_checkpoint = gr.Button('Change checkpoint', elem_id='change_checkpoint', visible=False)\n        button_set_checkpoint.click(\n            fn=lambda value, _: self.run_settings_single(value, key='sd_model_checkpoint'),\n            _js=\"function(v){ var res = desiredCheckpointName; desiredCheckpointName = ''; return [res || v, null]; }\",\n            inputs=[self.component_dict['sd_model_checkpoint'], self.dummy_component],\n            outputs=[self.component_dict['sd_model_checkpoint'], self.text_settings],\n        )\n\n        component_keys = [k for k in opts.data_labels.keys() if k in self.component_dict]\n\n        def get_settings_values():\n            return [get_value_for_setting(key) for key in component_keys]\n\n        demo.load(\n            fn=get_settings_values,\n            inputs=[],\n            outputs=[self.component_dict[k] for k in component_keys],\n            queue=False,\n        )\n\n    def search(self, text):\n        print(text)\n\n        return [gr.update(visible=text in (comp.label or \"\")) for comp in self.components]\n", "modules/ui_common.py": "import csv\nimport dataclasses\nimport json\nimport html\nimport os\n\nimport gradio as gr\n\nfrom modules import call_queue, shared, ui_tempdir, util\nfrom modules.infotext_utils import image_from_url_text\nimport modules.images\nfrom modules.ui_components import ToolButton\nimport modules.infotext_utils as parameters_copypaste\n\nfolder_symbol = '\\U0001f4c2'  # \ud83d\udcc2\nrefresh_symbol = '\\U0001f504'  # \ud83d\udd04\n\n\ndef update_generation_info(generation_info, html_info, img_index):\n    try:\n        generation_info = json.loads(generation_info)\n        if img_index < 0 or img_index >= len(generation_info[\"infotexts\"]):\n            return html_info, gr.update()\n        return plaintext_to_html(generation_info[\"infotexts\"][img_index]), gr.update()\n    except Exception:\n        pass\n    # if the json parse or anything else fails, just return the old html_info\n    return html_info, gr.update()\n\n\ndef plaintext_to_html(text, classname=None):\n    content = \"<br>\\n\".join(html.escape(x) for x in text.split('\\n'))\n\n    return f\"<p class='{classname}'>{content}</p>\" if classname else f\"<p>{content}</p>\"\n\n\ndef update_logfile(logfile_path, fields):\n    \"\"\"Update a logfile from old format to new format to maintain CSV integrity.\"\"\"\n    with open(logfile_path, \"r\", encoding=\"utf8\", newline=\"\") as file:\n        reader = csv.reader(file)\n        rows = list(reader)\n\n    # blank file: leave it as is\n    if not rows:\n        return\n\n    # file is already synced, do nothing\n    if len(rows[0]) == len(fields):\n        return\n\n    rows[0] = fields\n\n    # append new fields to each row as empty values\n    for row in rows[1:]:\n        while len(row) < len(fields):\n            row.append(\"\")\n\n    with open(logfile_path, \"w\", encoding=\"utf8\", newline=\"\") as file:\n        writer = csv.writer(file)\n        writer.writerows(rows)\n\n\ndef save_files(js_data, images, do_make_zip, index):\n    filenames = []\n    fullfns = []\n    parsed_infotexts = []\n\n    # quick dictionary to class object conversion. Its necessary due apply_filename_pattern requiring it\n    class MyObject:\n        def __init__(self, d=None):\n            if d is not None:\n                for key, value in d.items():\n                    setattr(self, key, value)\n\n    data = json.loads(js_data)\n    p = MyObject(data)\n\n    path = shared.opts.outdir_save\n    save_to_dirs = shared.opts.use_save_to_dirs_for_ui\n    extension: str = shared.opts.samples_format\n    start_index = 0\n\n    if index > -1 and shared.opts.save_selected_only and (index >= data[\"index_of_first_image\"]):  # ensures we are looking at a specific non-grid picture, and we have save_selected_only\n        images = [images[index]]\n        start_index = index\n\n    os.makedirs(shared.opts.outdir_save, exist_ok=True)\n\n    fields = [\n        \"prompt\",\n        \"seed\",\n        \"width\",\n        \"height\",\n        \"sampler\",\n        \"cfgs\",\n        \"steps\",\n        \"filename\",\n        \"negative_prompt\",\n        \"sd_model_name\",\n        \"sd_model_hash\",\n    ]\n    logfile_path = os.path.join(shared.opts.outdir_save, \"log.csv\")\n\n    # NOTE: ensure csv integrity when fields are added by\n    # updating headers and padding with delimiters where needed\n    if os.path.exists(logfile_path):\n        update_logfile(logfile_path, fields)\n\n    with open(logfile_path, \"a\", encoding=\"utf8\", newline='') as file:\n        at_start = file.tell() == 0\n        writer = csv.writer(file)\n        if at_start:\n            writer.writerow(fields)\n\n        for image_index, filedata in enumerate(images, start_index):\n            image = image_from_url_text(filedata)\n\n            is_grid = image_index < p.index_of_first_image\n\n            p.batch_index = image_index-1\n\n            parameters = parameters_copypaste.parse_generation_parameters(data[\"infotexts\"][image_index], [])\n            parsed_infotexts.append(parameters)\n            fullfn, txt_fullfn = modules.images.save_image(image, path, \"\", seed=parameters['Seed'], prompt=parameters['Prompt'], extension=extension, info=p.infotexts[image_index], grid=is_grid, p=p, save_to_dirs=save_to_dirs)\n\n            filename = os.path.relpath(fullfn, path)\n            filenames.append(filename)\n            fullfns.append(fullfn)\n            if txt_fullfn:\n                filenames.append(os.path.basename(txt_fullfn))\n                fullfns.append(txt_fullfn)\n\n        writer.writerow([parsed_infotexts[0]['Prompt'], parsed_infotexts[0]['Seed'], data[\"width\"], data[\"height\"], data[\"sampler_name\"], data[\"cfg_scale\"], data[\"steps\"], filenames[0], parsed_infotexts[0]['Negative prompt'], data[\"sd_model_name\"], data[\"sd_model_hash\"]])\n\n    # Make Zip\n    if do_make_zip:\n        p.all_seeds = [parameters['Seed'] for parameters in parsed_infotexts]\n        namegen = modules.images.FilenameGenerator(p, parsed_infotexts[0]['Seed'], parsed_infotexts[0]['Prompt'], image, True)\n        zip_filename = namegen.apply(shared.opts.grid_zip_filename_pattern or \"[datetime]_[[model_name]]_[seed]-[seed_last]\")\n        zip_filepath = os.path.join(path, f\"{zip_filename}.zip\")\n\n        from zipfile import ZipFile\n        with ZipFile(zip_filepath, \"w\") as zip_file:\n            for i in range(len(fullfns)):\n                with open(fullfns[i], mode=\"rb\") as f:\n                    zip_file.writestr(filenames[i], f.read())\n        fullfns.insert(0, zip_filepath)\n\n    return gr.File.update(value=fullfns, visible=True), plaintext_to_html(f\"Saved: {filenames[0]}\")\n\n\n@dataclasses.dataclass\nclass OutputPanel:\n    gallery = None\n    generation_info = None\n    infotext = None\n    html_log = None\n    button_upscale = None\n\n\ndef create_output_panel(tabname, outdir, toprow=None):\n    res = OutputPanel()\n\n    def open_folder(f, images=None, index=None):\n        if shared.cmd_opts.hide_ui_dir_config:\n            return\n\n        try:\n            if 'Sub' in shared.opts.open_dir_button_choice:\n                image_dir = os.path.split(images[index][\"name\"].rsplit('?', 1)[0])[0]\n                if 'temp' in shared.opts.open_dir_button_choice or not ui_tempdir.is_gradio_temp_path(image_dir):\n                    f = image_dir\n        except Exception:\n            pass\n\n        util.open_folder(f)\n\n    with gr.Column(elem_id=f\"{tabname}_results\"):\n        if toprow:\n            toprow.create_inline_toprow_image()\n\n        with gr.Column(variant='panel', elem_id=f\"{tabname}_results_panel\"):\n            with gr.Group(elem_id=f\"{tabname}_gallery_container\"):\n                res.gallery = gr.Gallery(label='Output', show_label=False, elem_id=f\"{tabname}_gallery\", columns=4, preview=True, height=shared.opts.gallery_height or None)\n\n            with gr.Row(elem_id=f\"image_buttons_{tabname}\", elem_classes=\"image-buttons\"):\n                open_folder_button = ToolButton(folder_symbol, elem_id=f'{tabname}_open_folder', visible=not shared.cmd_opts.hide_ui_dir_config, tooltip=\"Open images output directory.\")\n\n                if tabname != \"extras\":\n                    save = ToolButton('\ud83d\udcbe', elem_id=f'save_{tabname}', tooltip=f\"Save the image to a dedicated directory ({shared.opts.outdir_save}).\")\n                    save_zip = ToolButton('\ud83d\uddc3\ufe0f', elem_id=f'save_zip_{tabname}', tooltip=f\"Save zip archive with images to a dedicated directory ({shared.opts.outdir_save})\")\n\n                buttons = {\n                    'img2img': ToolButton('\ud83d\uddbc\ufe0f', elem_id=f'{tabname}_send_to_img2img', tooltip=\"Send image and generation parameters to img2img tab.\"),\n                    'inpaint': ToolButton('\ud83c\udfa8\ufe0f', elem_id=f'{tabname}_send_to_inpaint', tooltip=\"Send image and generation parameters to img2img inpaint tab.\"),\n                    'extras': ToolButton('\ud83d\udcd0', elem_id=f'{tabname}_send_to_extras', tooltip=\"Send image and generation parameters to extras tab.\")\n                }\n\n                if tabname == 'txt2img':\n                    res.button_upscale = ToolButton('\u2728', elem_id=f'{tabname}_upscale', tooltip=\"Create an upscaled version of the current image using hires fix settings.\")\n\n            open_folder_button.click(\n                fn=lambda images, index: open_folder(shared.opts.outdir_samples or outdir, images, index),\n                _js=\"(y, w) => [y, selected_gallery_index()]\",\n                inputs=[\n                    res.gallery,\n                    open_folder_button,  # placeholder for index\n                ],\n                outputs=[],\n            )\n\n            if tabname != \"extras\":\n                download_files = gr.File(None, file_count=\"multiple\", interactive=False, show_label=False, visible=False, elem_id=f'download_files_{tabname}')\n\n                with gr.Group():\n                    res.infotext = gr.HTML(elem_id=f'html_info_{tabname}', elem_classes=\"infotext\")\n                    res.html_log = gr.HTML(elem_id=f'html_log_{tabname}', elem_classes=\"html-log\")\n\n                    res.generation_info = gr.Textbox(visible=False, elem_id=f'generation_info_{tabname}')\n                    if tabname == 'txt2img' or tabname == 'img2img':\n                        generation_info_button = gr.Button(visible=False, elem_id=f\"{tabname}_generation_info_button\")\n                        generation_info_button.click(\n                            fn=update_generation_info,\n                            _js=\"function(x, y, z){ return [x, y, selected_gallery_index()] }\",\n                            inputs=[res.generation_info, res.infotext, res.infotext],\n                            outputs=[res.infotext, res.infotext],\n                            show_progress=False,\n                        )\n\n                    save.click(\n                        fn=call_queue.wrap_gradio_call(save_files),\n                        _js=\"(x, y, z, w) => [x, y, false, selected_gallery_index()]\",\n                        inputs=[\n                            res.generation_info,\n                            res.gallery,\n                            res.infotext,\n                            res.infotext,\n                        ],\n                        outputs=[\n                            download_files,\n                            res.html_log,\n                        ],\n                        show_progress=False,\n                    )\n\n                    save_zip.click(\n                        fn=call_queue.wrap_gradio_call(save_files),\n                        _js=\"(x, y, z, w) => [x, y, true, selected_gallery_index()]\",\n                        inputs=[\n                            res.generation_info,\n                            res.gallery,\n                            res.infotext,\n                            res.infotext,\n                        ],\n                        outputs=[\n                            download_files,\n                            res.html_log,\n                        ]\n                    )\n\n            else:\n                res.generation_info = gr.HTML(elem_id=f'html_info_x_{tabname}')\n                res.infotext = gr.HTML(elem_id=f'html_info_{tabname}', elem_classes=\"infotext\")\n                res.html_log = gr.HTML(elem_id=f'html_log_{tabname}')\n\n            paste_field_names = []\n            if tabname == \"txt2img\":\n                paste_field_names = modules.scripts.scripts_txt2img.paste_field_names\n            elif tabname == \"img2img\":\n                paste_field_names = modules.scripts.scripts_img2img.paste_field_names\n\n            for paste_tabname, paste_button in buttons.items():\n                parameters_copypaste.register_paste_params_button(parameters_copypaste.ParamBinding(\n                    paste_button=paste_button, tabname=paste_tabname, source_tabname=\"txt2img\" if tabname == \"txt2img\" else None, source_image_component=res.gallery,\n                    paste_field_names=paste_field_names\n                ))\n\n    return res\n\n\ndef create_refresh_button(refresh_component, refresh_method, refreshed_args, elem_id):\n    refresh_components = refresh_component if isinstance(refresh_component, list) else [refresh_component]\n\n    label = None\n    for comp in refresh_components:\n        label = getattr(comp, 'label', None)\n        if label is not None:\n            break\n\n    def refresh():\n        refresh_method()\n        args = refreshed_args() if callable(refreshed_args) else refreshed_args\n\n        for k, v in args.items():\n            for comp in refresh_components:\n                setattr(comp, k, v)\n\n        return [gr.update(**(args or {})) for _ in refresh_components] if len(refresh_components) > 1 else gr.update(**(args or {}))\n\n    refresh_button = ToolButton(value=refresh_symbol, elem_id=elem_id, tooltip=f\"{label}: refresh\" if label else \"Refresh\")\n    refresh_button.click(\n        fn=refresh,\n        inputs=[],\n        outputs=refresh_components\n    )\n    return refresh_button\n\n\ndef setup_dialog(button_show, dialog, *, button_close=None):\n    \"\"\"Sets up the UI so that the dialog (gr.Box) is invisible, and is only shown when buttons_show is clicked, in a fullscreen modal window.\"\"\"\n\n    dialog.visible = False\n\n    button_show.click(\n        fn=lambda: gr.update(visible=True),\n        inputs=[],\n        outputs=[dialog],\n    ).then(fn=None, _js=\"function(){ popupId('\" + dialog.elem_id + \"'); }\")\n\n    if button_close:\n        button_close.click(fn=None, _js=\"closePopup\")\n\n", "modules/ui_extensions.py": "import json\nimport os\nimport threading\nimport time\nfrom datetime import datetime, timezone\n\nimport git\n\nimport gradio as gr\nimport html\nimport shutil\nimport errno\n\nfrom modules import extensions, shared, paths, config_states, errors, restart\nfrom modules.paths_internal import config_states_dir\nfrom modules.call_queue import wrap_gradio_gpu_call\n\navailable_extensions = {\"extensions\": []}\nSTYLE_PRIMARY = ' style=\"color: var(--primary-400)\"'\n\n\ndef check_access():\n    assert not shared.cmd_opts.disable_extension_access, \"extension access disabled because of command line flags\"\n\n\ndef apply_and_restart(disable_list, update_list, disable_all):\n    check_access()\n\n    disabled = json.loads(disable_list)\n    assert type(disabled) == list, f\"wrong disable_list data for apply_and_restart: {disable_list}\"\n\n    update = json.loads(update_list)\n    assert type(update) == list, f\"wrong update_list data for apply_and_restart: {update_list}\"\n\n    if update:\n        save_config_state(\"Backup (pre-update)\")\n\n    update = set(update)\n\n    for ext in extensions.extensions:\n        if ext.name not in update:\n            continue\n\n        try:\n            ext.fetch_and_reset_hard()\n        except Exception:\n            errors.report(f\"Error getting updates for {ext.name}\", exc_info=True)\n\n    shared.opts.disabled_extensions = disabled\n    shared.opts.disable_all_extensions = disable_all\n    shared.opts.save(shared.config_filename)\n\n    if restart.is_restartable():\n        restart.restart_program()\n    else:\n        restart.stop_program()\n\n\ndef save_config_state(name):\n    current_config_state = config_states.get_config()\n\n    name = os.path.basename(name or \"Config\")\n\n    current_config_state[\"name\"] = name\n    timestamp = datetime.now().strftime('%Y_%m_%d-%H_%M_%S')\n    filename = os.path.join(config_states_dir, f\"{timestamp}_{name}.json\")\n    print(f\"Saving backup of webui/extension state to {filename}.\")\n    with open(filename, \"w\", encoding=\"utf-8\") as f:\n        json.dump(current_config_state, f, indent=4, ensure_ascii=False)\n    config_states.list_config_states()\n    new_value = next(iter(config_states.all_config_states.keys()), \"Current\")\n    new_choices = [\"Current\"] + list(config_states.all_config_states.keys())\n    return gr.Dropdown.update(value=new_value, choices=new_choices), f\"<span>Saved current webui/extension state to \\\"{filename}\\\"</span>\"\n\n\ndef restore_config_state(confirmed, config_state_name, restore_type):\n    if config_state_name == \"Current\":\n        return \"<span>Select a config to restore from.</span>\"\n    if not confirmed:\n        return \"<span>Cancelled.</span>\"\n\n    check_access()\n\n    config_state = config_states.all_config_states[config_state_name]\n\n    print(f\"*** Restoring webui state from backup: {restore_type} ***\")\n\n    if restore_type == \"extensions\" or restore_type == \"both\":\n        shared.opts.restore_config_state_file = config_state[\"filepath\"]\n        shared.opts.save(shared.config_filename)\n\n    if restore_type == \"webui\" or restore_type == \"both\":\n        config_states.restore_webui_config(config_state)\n\n    shared.state.request_restart()\n\n    return \"\"\n\n\ndef check_updates(id_task, disable_list):\n    check_access()\n\n    disabled = json.loads(disable_list)\n    assert type(disabled) == list, f\"wrong disable_list data for apply_and_restart: {disable_list}\"\n\n    exts = [ext for ext in extensions.extensions if ext.remote is not None and ext.name not in disabled]\n    shared.state.job_count = len(exts)\n\n    for ext in exts:\n        shared.state.textinfo = ext.name\n\n        try:\n            ext.check_updates()\n        except FileNotFoundError as e:\n            if 'FETCH_HEAD' not in str(e):\n                raise\n        except Exception:\n            errors.report(f\"Error checking updates for {ext.name}\", exc_info=True)\n\n        shared.state.nextjob()\n\n    return extension_table(), \"\"\n\n\ndef make_commit_link(commit_hash, remote, text=None):\n    if text is None:\n        text = commit_hash[:8]\n    if remote.startswith(\"https://github.com/\"):\n        if remote.endswith(\".git\"):\n            remote = remote[:-4]\n        href = remote + \"/commit/\" + commit_hash\n        return f'<a href=\"{href}\" target=\"_blank\">{text}</a>'\n    else:\n        return text\n\n\ndef extension_table():\n    code = f\"\"\"<!-- {time.time()} -->\n    <table id=\"extensions\">\n        <thead>\n            <tr>\n                <th>\n                    <input class=\"gr-check-radio gr-checkbox all_extensions_toggle\" type=\"checkbox\" {'checked=\"checked\"' if all(ext.enabled for ext in extensions.extensions) else ''} onchange=\"toggle_all_extensions(event)\" />\n                    <abbr title=\"Use checkbox to enable the extension; it will be enabled or disabled when you click apply button\">Extension</abbr>\n                </th>\n                <th>URL</th>\n                <th>Branch</th>\n                <th>Version</th>\n                <th>Date</th>\n                <th><abbr title=\"Use checkbox to mark the extension for update; it will be updated when you click apply button\">Update</abbr></th>\n            </tr>\n        </thead>\n        <tbody>\n    \"\"\"\n\n    for ext in extensions.extensions:\n        ext: extensions.Extension\n        ext.read_info_from_repo()\n\n        remote = f\"\"\"<a href=\"{html.escape(ext.remote or '')}\" target=\"_blank\">{html.escape(\"built-in\" if ext.is_builtin else ext.remote or '')}</a>\"\"\"\n\n        if ext.can_update:\n            ext_status = f\"\"\"<label><input class=\"gr-check-radio gr-checkbox\" name=\"update_{html.escape(ext.name)}\" checked=\"checked\" type=\"checkbox\">{html.escape(ext.status)}</label>\"\"\"\n        else:\n            ext_status = ext.status\n\n        style = \"\"\n        if shared.cmd_opts.disable_extra_extensions and not ext.is_builtin or shared.opts.disable_all_extensions == \"extra\" and not ext.is_builtin or shared.cmd_opts.disable_all_extensions or shared.opts.disable_all_extensions == \"all\":\n            style = STYLE_PRIMARY\n\n        version_link = ext.version\n        if ext.commit_hash and ext.remote:\n            version_link = make_commit_link(ext.commit_hash, ext.remote, ext.version)\n\n        code += f\"\"\"\n            <tr>\n                <td><label{style}><input class=\"gr-check-radio gr-checkbox extension_toggle\" name=\"enable_{html.escape(ext.name)}\" type=\"checkbox\" {'checked=\"checked\"' if ext.enabled else ''} onchange=\"toggle_extension(event)\" />{html.escape(ext.name)}</label></td>\n                <td>{remote}</td>\n                <td>{ext.branch}</td>\n                <td>{version_link}</td>\n                <td>{datetime.fromtimestamp(ext.commit_date) if ext.commit_date else \"\"}</td>\n                <td{' class=\"extension_status\"' if ext.remote is not None else ''}>{ext_status}</td>\n            </tr>\n    \"\"\"\n\n    code += \"\"\"\n        </tbody>\n    </table>\n    \"\"\"\n\n    return code\n\n\ndef update_config_states_table(state_name):\n    if state_name == \"Current\":\n        config_state = config_states.get_config()\n    else:\n        config_state = config_states.all_config_states[state_name]\n\n    config_name = config_state.get(\"name\", \"Config\")\n    created_date = datetime.fromtimestamp(config_state[\"created_at\"]).strftime('%Y-%m-%d %H:%M:%S')\n    filepath = config_state.get(\"filepath\", \"<unknown>\")\n\n    try:\n        webui_remote = config_state[\"webui\"][\"remote\"] or \"\"\n        webui_branch = config_state[\"webui\"][\"branch\"]\n        webui_commit_hash = config_state[\"webui\"][\"commit_hash\"] or \"<unknown>\"\n        webui_commit_date = config_state[\"webui\"][\"commit_date\"]\n        if webui_commit_date:\n            webui_commit_date = time.asctime(time.gmtime(webui_commit_date))\n        else:\n            webui_commit_date = \"<unknown>\"\n\n        remote = f\"\"\"<a href=\"{html.escape(webui_remote)}\" target=\"_blank\">{html.escape(webui_remote or '')}</a>\"\"\"\n        commit_link = make_commit_link(webui_commit_hash, webui_remote)\n        date_link = make_commit_link(webui_commit_hash, webui_remote, webui_commit_date)\n\n        current_webui = config_states.get_webui_config()\n\n        style_remote = \"\"\n        style_branch = \"\"\n        style_commit = \"\"\n        if current_webui[\"remote\"] != webui_remote:\n            style_remote = STYLE_PRIMARY\n        if current_webui[\"branch\"] != webui_branch:\n            style_branch = STYLE_PRIMARY\n        if current_webui[\"commit_hash\"] != webui_commit_hash:\n            style_commit = STYLE_PRIMARY\n\n        code = f\"\"\"<!-- {time.time()} -->\n<h2>Config Backup: {config_name}</h2>\n<div><b>Filepath:</b> {filepath}</div>\n<div><b>Created at:</b> {created_date}</div>\n<h2>WebUI State</h2>\n<table id=\"config_state_webui\">\n    <thead>\n        <tr>\n            <th>URL</th>\n            <th>Branch</th>\n            <th>Commit</th>\n            <th>Date</th>\n        </tr>\n    </thead>\n    <tbody>\n        <tr>\n            <td>\n                <label{style_remote}>{remote}</label>\n            </td>\n            <td>\n                <label{style_branch}>{webui_branch}</label>\n            </td>\n            <td>\n                <label{style_commit}>{commit_link}</label>\n            </td>\n            <td>\n                <label{style_commit}>{date_link}</label>\n            </td>\n        </tr>\n    </tbody>\n</table>\n<h2>Extension State</h2>\n<table id=\"config_state_extensions\">\n    <thead>\n        <tr>\n            <th>Extension</th>\n            <th>URL</th>\n            <th>Branch</th>\n            <th>Commit</th>\n            <th>Date</th>\n        </tr>\n    </thead>\n    <tbody>\n\"\"\"\n\n        ext_map = {ext.name: ext for ext in extensions.extensions}\n\n        for ext_name, ext_conf in config_state[\"extensions\"].items():\n            ext_remote = ext_conf[\"remote\"] or \"\"\n            ext_branch = ext_conf[\"branch\"] or \"<unknown>\"\n            ext_enabled = ext_conf[\"enabled\"]\n            ext_commit_hash = ext_conf[\"commit_hash\"] or \"<unknown>\"\n            ext_commit_date = ext_conf[\"commit_date\"]\n            if ext_commit_date:\n                ext_commit_date = time.asctime(time.gmtime(ext_commit_date))\n            else:\n                ext_commit_date = \"<unknown>\"\n\n            remote = f\"\"\"<a href=\"{html.escape(ext_remote)}\" target=\"_blank\">{html.escape(ext_remote or '')}</a>\"\"\"\n            commit_link = make_commit_link(ext_commit_hash, ext_remote)\n            date_link = make_commit_link(ext_commit_hash, ext_remote, ext_commit_date)\n\n            style_enabled = \"\"\n            style_remote = \"\"\n            style_branch = \"\"\n            style_commit = \"\"\n            if ext_name in ext_map:\n                current_ext = ext_map[ext_name]\n                current_ext.read_info_from_repo()\n                if current_ext.enabled != ext_enabled:\n                    style_enabled = STYLE_PRIMARY\n                if current_ext.remote != ext_remote:\n                    style_remote = STYLE_PRIMARY\n                if current_ext.branch != ext_branch:\n                    style_branch = STYLE_PRIMARY\n                if current_ext.commit_hash != ext_commit_hash:\n                    style_commit = STYLE_PRIMARY\n\n            code += f\"\"\"        <tr>\n            <td><label{style_enabled}><input class=\"gr-check-radio gr-checkbox\" type=\"checkbox\" disabled=\"true\" {'checked=\"checked\"' if ext_enabled else ''}>{html.escape(ext_name)}</label></td>\n            <td><label{style_remote}>{remote}</label></td>\n            <td><label{style_branch}>{ext_branch}</label></td>\n            <td><label{style_commit}>{commit_link}</label></td>\n            <td><label{style_commit}>{date_link}</label></td>\n        </tr>\n\"\"\"\n\n        code += \"\"\"    </tbody>\n</table>\"\"\"\n\n    except Exception as e:\n        print(f\"[ERROR]: Config states {filepath}, {e}\")\n        code = f\"\"\"<!-- {time.time()} -->\n<h2>Config Backup: {config_name}</h2>\n<div><b>Filepath:</b> {filepath}</div>\n<div><b>Created at:</b> {created_date}</div>\n<h2>This file is corrupted</h2>\"\"\"\n\n    return code\n\n\ndef normalize_git_url(url):\n    if url is None:\n        return \"\"\n\n    url = url.replace(\".git\", \"\")\n    return url\n\n\ndef get_extension_dirname_from_url(url):\n    *parts, last_part = url.split('/')\n    return normalize_git_url(last_part)\n\n\ndef install_extension_from_url(dirname, url, branch_name=None):\n    check_access()\n\n    if isinstance(dirname, str):\n        dirname = dirname.strip()\n    if isinstance(url, str):\n        url = url.strip()\n\n    assert url, 'No URL specified'\n\n    if dirname is None or dirname == \"\":\n        dirname = get_extension_dirname_from_url(url)\n\n    target_dir = os.path.join(extensions.extensions_dir, dirname)\n    assert not os.path.exists(target_dir), f'Extension directory already exists: {target_dir}'\n\n    normalized_url = normalize_git_url(url)\n    if any(x for x in extensions.extensions if normalize_git_url(x.remote) == normalized_url):\n        raise Exception(f'Extension with this URL is already installed: {url}')\n\n    tmpdir = os.path.join(paths.data_path, \"tmp\", dirname)\n\n    try:\n        shutil.rmtree(tmpdir, True)\n        if not branch_name:\n            # if no branch is specified, use the default branch\n            with git.Repo.clone_from(url, tmpdir, filter=['blob:none']) as repo:\n                repo.remote().fetch()\n                for submodule in repo.submodules:\n                    submodule.update()\n        else:\n            with git.Repo.clone_from(url, tmpdir, filter=['blob:none'], branch=branch_name) as repo:\n                repo.remote().fetch()\n                for submodule in repo.submodules:\n                    submodule.update()\n        try:\n            os.rename(tmpdir, target_dir)\n        except OSError as err:\n            if err.errno == errno.EXDEV:\n                # Cross device link, typical in docker or when tmp/ and extensions/ are on different file systems\n                # Since we can't use a rename, do the slower but more versatile shutil.move()\n                shutil.move(tmpdir, target_dir)\n            else:\n                # Something else, not enough free space, permissions, etc.  rethrow it so that it gets handled.\n                raise err\n\n        import launch\n        launch.run_extension_installer(target_dir)\n\n        extensions.list_extensions()\n        return [extension_table(), html.escape(f\"Installed into {target_dir}. Use Installed tab to restart.\")]\n    finally:\n        shutil.rmtree(tmpdir, True)\n\n\ndef install_extension_from_index(url, hide_tags, sort_column, filter_text):\n    ext_table, message = install_extension_from_url(None, url)\n\n    code, _ = refresh_available_extensions_from_data(hide_tags, sort_column, filter_text)\n\n    return code, ext_table, message, ''\n\n\ndef refresh_available_extensions(url, hide_tags, sort_column):\n    global available_extensions\n\n    import urllib.request\n    with urllib.request.urlopen(url) as response:\n        text = response.read()\n\n    available_extensions = json.loads(text)\n\n    code, tags = refresh_available_extensions_from_data(hide_tags, sort_column)\n\n    return url, code, gr.CheckboxGroup.update(choices=tags), '', ''\n\n\ndef refresh_available_extensions_for_tags(hide_tags, sort_column, filter_text):\n    code, _ = refresh_available_extensions_from_data(hide_tags, sort_column, filter_text)\n\n    return code, ''\n\n\ndef search_extensions(filter_text, hide_tags, sort_column):\n    code, _ = refresh_available_extensions_from_data(hide_tags, sort_column, filter_text)\n\n    return code, ''\n\n\nsort_ordering = [\n    # (reverse, order_by_function)\n    (True, lambda x: x.get('added', 'z')),\n    (False, lambda x: x.get('added', 'z')),\n    (False, lambda x: x.get('name', 'z')),\n    (True, lambda x: x.get('name', 'z')),\n    (False, lambda x: 'z'),\n    (True, lambda x: x.get('commit_time', '')),\n    (True, lambda x: x.get('created_at', '')),\n    (True, lambda x: x.get('stars', 0)),\n]\n\n\ndef get_date(info: dict, key):\n    try:\n        return datetime.strptime(info.get(key), \"%Y-%m-%dT%H:%M:%SZ\").replace(tzinfo=timezone.utc).astimezone().strftime(\"%Y-%m-%d\")\n    except (ValueError, TypeError):\n        return ''\n\n\ndef refresh_available_extensions_from_data(hide_tags, sort_column, filter_text=\"\"):\n    extlist = available_extensions[\"extensions\"]\n    installed_extensions = {extension.name for extension in extensions.extensions}\n    installed_extension_urls = {normalize_git_url(extension.remote) for extension in extensions.extensions if extension.remote is not None}\n\n    tags = available_extensions.get(\"tags\", {})\n    tags_to_hide = set(hide_tags)\n    hidden = 0\n\n    code = f\"\"\"<!-- {time.time()} -->\n    <table id=\"available_extensions\">\n        <thead>\n            <tr>\n                <th>Extension</th>\n                <th>Description</th>\n                <th>Action</th>\n            </tr>\n        </thead>\n        <tbody>\n    \"\"\"\n\n    sort_reverse, sort_function = sort_ordering[sort_column if 0 <= sort_column < len(sort_ordering) else 0]\n\n    for ext in sorted(extlist, key=sort_function, reverse=sort_reverse):\n        name = ext.get(\"name\", \"noname\")\n        stars = int(ext.get(\"stars\", 0))\n        added = ext.get('added', 'unknown')\n        update_time = get_date(ext, 'commit_time')\n        create_time = get_date(ext, 'created_at')\n        url = ext.get(\"url\", None)\n        description = ext.get(\"description\", \"\")\n        extension_tags = ext.get(\"tags\", [])\n\n        if url is None:\n            continue\n\n        existing = get_extension_dirname_from_url(url) in installed_extensions or normalize_git_url(url) in installed_extension_urls\n        extension_tags = extension_tags + [\"installed\"] if existing else extension_tags\n\n        if any(x for x in extension_tags if x in tags_to_hide):\n            hidden += 1\n            continue\n\n        if filter_text and filter_text.strip():\n            if filter_text.lower() not in html.escape(name).lower() and filter_text.lower() not in html.escape(description).lower():\n                hidden += 1\n                continue\n\n        install_code = f\"\"\"<button onclick=\"install_extension_from_index(this, '{html.escape(url)}')\" {\"disabled=disabled\" if existing else \"\"} class=\"lg secondary gradio-button custom-button\">{\"Install\" if not existing else \"Installed\"}</button>\"\"\"\n\n        tags_text = \", \".join([f\"<span class='extension-tag' title='{tags.get(x, '')}'>{x}</span>\" for x in extension_tags])\n\n        code += f\"\"\"\n            <tr>\n                <td><a href=\"{html.escape(url)}\" target=\"_blank\">{html.escape(name)}</a><br />{tags_text}</td>\n                <td>{html.escape(description)}<p class=\"info\">\n                <span class=\"date_added\">Update: {html.escape(update_time)}  Added: {html.escape(added)}  Created: {html.escape(create_time)}</span><span class=\"star_count\">stars: <b>{stars}</b></a></p></td>\n                <td>{install_code}</td>\n            </tr>\n\n        \"\"\"\n\n        for tag in [x for x in extension_tags if x not in tags]:\n            tags[tag] = tag\n\n    code += \"\"\"\n        </tbody>\n    </table>\n    \"\"\"\n\n    if hidden > 0:\n        code += f\"<p>Extension hidden: {hidden}</p>\"\n\n    return code, list(tags)\n\n\ndef preload_extensions_git_metadata():\n    for extension in extensions.extensions:\n        extension.read_info_from_repo()\n\n\ndef create_ui():\n    import modules.ui\n\n    config_states.list_config_states()\n\n    threading.Thread(target=preload_extensions_git_metadata).start()\n\n    with gr.Blocks(analytics_enabled=False) as ui:\n        with gr.Tabs(elem_id=\"tabs_extensions\"):\n            with gr.TabItem(\"Installed\", id=\"installed\"):\n\n                with gr.Row(elem_id=\"extensions_installed_top\"):\n                    apply_label = (\"Apply and restart UI\" if restart.is_restartable() else \"Apply and quit\")\n                    apply = gr.Button(value=apply_label, variant=\"primary\")\n                    check = gr.Button(value=\"Check for updates\")\n                    extensions_disable_all = gr.Radio(label=\"Disable all extensions\", choices=[\"none\", \"extra\", \"all\"], value=shared.opts.disable_all_extensions, elem_id=\"extensions_disable_all\")\n                    extensions_disabled_list = gr.Text(elem_id=\"extensions_disabled_list\", visible=False, container=False)\n                    extensions_update_list = gr.Text(elem_id=\"extensions_update_list\", visible=False, container=False)\n                    refresh = gr.Button(value='Refresh', variant=\"compact\")\n\n                html = \"\"\n\n                if shared.cmd_opts.disable_all_extensions or shared.cmd_opts.disable_extra_extensions or shared.opts.disable_all_extensions != \"none\":\n                    if shared.cmd_opts.disable_all_extensions:\n                        msg = '\"--disable-all-extensions\" was used, remove it to load all extensions again'\n                    elif shared.opts.disable_all_extensions != \"none\":\n                        msg = '\"Disable all extensions\" was set, change it to \"none\" to load all extensions again'\n                    elif shared.cmd_opts.disable_extra_extensions:\n                        msg = '\"--disable-extra-extensions\" was used, remove it to load all extensions again'\n                    html = f'<span style=\"color: var(--primary-400);\">{msg}</span>'\n\n                with gr.Row():\n                    info = gr.HTML(html)\n\n                with gr.Row(elem_classes=\"progress-container\"):\n                    extensions_table = gr.HTML('Loading...', elem_id=\"extensions_installed_html\")\n\n                ui.load(fn=extension_table, inputs=[], outputs=[extensions_table], show_progress=False)\n                refresh.click(fn=extension_table, inputs=[], outputs=[extensions_table], show_progress=False)\n\n                apply.click(\n                    fn=apply_and_restart,\n                    _js=\"extensions_apply\",\n                    inputs=[extensions_disabled_list, extensions_update_list, extensions_disable_all],\n                    outputs=[],\n                )\n\n                check.click(\n                    fn=wrap_gradio_gpu_call(check_updates, extra_outputs=[gr.update()]),\n                    _js=\"extensions_check\",\n                    inputs=[info, extensions_disabled_list],\n                    outputs=[extensions_table, info],\n                )\n\n            with gr.TabItem(\"Available\", id=\"available\"):\n                with gr.Row():\n                    refresh_available_extensions_button = gr.Button(value=\"Load from:\", variant=\"primary\")\n                    extensions_index_url = os.environ.get('WEBUI_EXTENSIONS_INDEX', \"https://raw.githubusercontent.com/AUTOMATIC1111/stable-diffusion-webui-extensions/master/index.json\")\n                    available_extensions_index = gr.Text(value=extensions_index_url, label=\"Extension index URL\", container=False)\n                    extension_to_install = gr.Text(elem_id=\"extension_to_install\", visible=False)\n                    install_extension_button = gr.Button(elem_id=\"install_extension_button\", visible=False)\n\n                with gr.Row():\n                    hide_tags = gr.CheckboxGroup(value=[\"ads\", \"localization\", \"installed\"], label=\"Hide extensions with tags\", choices=[\"script\", \"ads\", \"localization\", \"installed\"])\n                    sort_column = gr.Radio(value=\"newest first\", label=\"Order\", choices=[\"newest first\", \"oldest first\", \"a-z\", \"z-a\", \"internal order\",'update time', 'create time', \"stars\"], type=\"index\")\n\n                with gr.Row():\n                    search_extensions_text = gr.Text(label=\"Search\", container=False)\n\n                install_result = gr.HTML()\n                available_extensions_table = gr.HTML()\n\n                refresh_available_extensions_button.click(\n                    fn=modules.ui.wrap_gradio_call(refresh_available_extensions, extra_outputs=[gr.update(), gr.update(), gr.update(), gr.update()]),\n                    inputs=[available_extensions_index, hide_tags, sort_column],\n                    outputs=[available_extensions_index, available_extensions_table, hide_tags, search_extensions_text, install_result],\n                )\n\n                install_extension_button.click(\n                    fn=modules.ui.wrap_gradio_call(install_extension_from_index, extra_outputs=[gr.update(), gr.update()]),\n                    inputs=[extension_to_install, hide_tags, sort_column, search_extensions_text],\n                    outputs=[available_extensions_table, extensions_table, install_result],\n                )\n\n                search_extensions_text.change(\n                    fn=modules.ui.wrap_gradio_call(search_extensions, extra_outputs=[gr.update()]),\n                    inputs=[search_extensions_text, hide_tags, sort_column],\n                    outputs=[available_extensions_table, install_result],\n                )\n\n                hide_tags.change(\n                    fn=modules.ui.wrap_gradio_call(refresh_available_extensions_for_tags, extra_outputs=[gr.update()]),\n                    inputs=[hide_tags, sort_column, search_extensions_text],\n                    outputs=[available_extensions_table, install_result]\n                )\n\n                sort_column.change(\n                    fn=modules.ui.wrap_gradio_call(refresh_available_extensions_for_tags, extra_outputs=[gr.update()]),\n                    inputs=[hide_tags, sort_column, search_extensions_text],\n                    outputs=[available_extensions_table, install_result]\n                )\n\n            with gr.TabItem(\"Install from URL\", id=\"install_from_url\"):\n                install_url = gr.Text(label=\"URL for extension's git repository\")\n                install_branch = gr.Text(label=\"Specific branch name\", placeholder=\"Leave empty for default main branch\")\n                install_dirname = gr.Text(label=\"Local directory name\", placeholder=\"Leave empty for auto\")\n                install_button = gr.Button(value=\"Install\", variant=\"primary\")\n                install_result = gr.HTML(elem_id=\"extension_install_result\")\n\n                install_button.click(\n                    fn=modules.ui.wrap_gradio_call(lambda *args: [gr.update(), *install_extension_from_url(*args)], extra_outputs=[gr.update(), gr.update()]),\n                    inputs=[install_dirname, install_url, install_branch],\n                    outputs=[install_url, extensions_table, install_result],\n                )\n\n            with gr.TabItem(\"Backup/Restore\"):\n                with gr.Row(elem_id=\"extensions_backup_top_row\"):\n                    config_states_list = gr.Dropdown(label=\"Saved Configs\", elem_id=\"extension_backup_saved_configs\", value=\"Current\", choices=[\"Current\"] + list(config_states.all_config_states.keys()))\n                    modules.ui.create_refresh_button(config_states_list, config_states.list_config_states, lambda: {\"choices\": [\"Current\"] + list(config_states.all_config_states.keys())}, \"refresh_config_states\")\n                    config_restore_type = gr.Radio(label=\"State to restore\", choices=[\"extensions\", \"webui\", \"both\"], value=\"extensions\", elem_id=\"extension_backup_restore_type\")\n                    config_restore_button = gr.Button(value=\"Restore Selected Config\", variant=\"primary\", elem_id=\"extension_backup_restore\")\n                with gr.Row(elem_id=\"extensions_backup_top_row2\"):\n                    config_save_name = gr.Textbox(\"\", placeholder=\"Config Name\", show_label=False)\n                    config_save_button = gr.Button(value=\"Save Current Config\")\n\n                config_states_info = gr.HTML(\"\")\n                config_states_table = gr.HTML(\"Loading...\")\n                ui.load(fn=update_config_states_table, inputs=[config_states_list], outputs=[config_states_table])\n\n                config_save_button.click(fn=save_config_state, inputs=[config_save_name], outputs=[config_states_list, config_states_info])\n\n                dummy_component = gr.Label(visible=False)\n                config_restore_button.click(fn=restore_config_state, _js=\"config_state_confirm_restore\", inputs=[dummy_component, config_states_list, config_restore_type], outputs=[config_states_info])\n\n                config_states_list.change(\n                    fn=update_config_states_table,\n                    inputs=[config_states_list],\n                    outputs=[config_states_table],\n                )\n\n\n    return ui\n", "modules/extras.py": "import os\nimport re\nimport shutil\nimport json\n\n\nimport torch\nimport tqdm\n\nfrom modules import shared, images, sd_models, sd_vae, sd_models_config, errors\nfrom modules.ui_common import plaintext_to_html\nimport gradio as gr\nimport safetensors.torch\n\n\ndef run_pnginfo(image):\n    if image is None:\n        return '', '', ''\n\n    geninfo, items = images.read_info_from_image(image)\n    items = {**{'parameters': geninfo}, **items}\n\n    info = ''\n    for key, text in items.items():\n        info += f\"\"\"\n<div>\n<p><b>{plaintext_to_html(str(key))}</b></p>\n<p>{plaintext_to_html(str(text))}</p>\n</div>\n\"\"\".strip()+\"\\n\"\n\n    if len(info) == 0:\n        message = \"Nothing found in the image.\"\n        info = f\"<div><p>{message}<p></div>\"\n\n    return '', geninfo, info\n\n\ndef create_config(ckpt_result, config_source, a, b, c):\n    def config(x):\n        res = sd_models_config.find_checkpoint_config_near_filename(x) if x else None\n        return res if res != shared.sd_default_config else None\n\n    if config_source == 0:\n        cfg = config(a) or config(b) or config(c)\n    elif config_source == 1:\n        cfg = config(b)\n    elif config_source == 2:\n        cfg = config(c)\n    else:\n        cfg = None\n\n    if cfg is None:\n        return\n\n    filename, _ = os.path.splitext(ckpt_result)\n    checkpoint_filename = filename + \".yaml\"\n\n    print(\"Copying config:\")\n    print(\"   from:\", cfg)\n    print(\"     to:\", checkpoint_filename)\n    shutil.copyfile(cfg, checkpoint_filename)\n\n\ncheckpoint_dict_skip_on_merge = [\"cond_stage_model.transformer.text_model.embeddings.position_ids\"]\n\n\ndef to_half(tensor, enable):\n    if enable and tensor.dtype == torch.float:\n        return tensor.half()\n\n    return tensor\n\n\ndef read_metadata(primary_model_name, secondary_model_name, tertiary_model_name):\n    metadata = {}\n\n    for checkpoint_name in [primary_model_name, secondary_model_name, tertiary_model_name]:\n        checkpoint_info = sd_models.checkpoints_list.get(checkpoint_name, None)\n        if checkpoint_info is None:\n            continue\n\n        metadata.update(checkpoint_info.metadata)\n\n    return json.dumps(metadata, indent=4, ensure_ascii=False)\n\n\ndef run_modelmerger(id_task, primary_model_name, secondary_model_name, tertiary_model_name, interp_method, multiplier, save_as_half, custom_name, checkpoint_format, config_source, bake_in_vae, discard_weights, save_metadata, add_merge_recipe, copy_metadata_fields, metadata_json):\n    shared.state.begin(job=\"model-merge\")\n\n    def fail(message):\n        shared.state.textinfo = message\n        shared.state.end()\n        return [*[gr.update() for _ in range(4)], message]\n\n    def weighted_sum(theta0, theta1, alpha):\n        return ((1 - alpha) * theta0) + (alpha * theta1)\n\n    def get_difference(theta1, theta2):\n        return theta1 - theta2\n\n    def add_difference(theta0, theta1_2_diff, alpha):\n        return theta0 + (alpha * theta1_2_diff)\n\n    def filename_weighted_sum():\n        a = primary_model_info.model_name\n        b = secondary_model_info.model_name\n        Ma = round(1 - multiplier, 2)\n        Mb = round(multiplier, 2)\n\n        return f\"{Ma}({a}) + {Mb}({b})\"\n\n    def filename_add_difference():\n        a = primary_model_info.model_name\n        b = secondary_model_info.model_name\n        c = tertiary_model_info.model_name\n        M = round(multiplier, 2)\n\n        return f\"{a} + {M}({b} - {c})\"\n\n    def filename_nothing():\n        return primary_model_info.model_name\n\n    theta_funcs = {\n        \"Weighted sum\": (filename_weighted_sum, None, weighted_sum),\n        \"Add difference\": (filename_add_difference, get_difference, add_difference),\n        \"No interpolation\": (filename_nothing, None, None),\n    }\n    filename_generator, theta_func1, theta_func2 = theta_funcs[interp_method]\n    shared.state.job_count = (1 if theta_func1 else 0) + (1 if theta_func2 else 0)\n\n    if not primary_model_name:\n        return fail(\"Failed: Merging requires a primary model.\")\n\n    primary_model_info = sd_models.checkpoints_list[primary_model_name]\n\n    if theta_func2 and not secondary_model_name:\n        return fail(\"Failed: Merging requires a secondary model.\")\n\n    secondary_model_info = sd_models.checkpoints_list[secondary_model_name] if theta_func2 else None\n\n    if theta_func1 and not tertiary_model_name:\n        return fail(f\"Failed: Interpolation method ({interp_method}) requires a tertiary model.\")\n\n    tertiary_model_info = sd_models.checkpoints_list[tertiary_model_name] if theta_func1 else None\n\n    result_is_inpainting_model = False\n    result_is_instruct_pix2pix_model = False\n\n    if theta_func2:\n        shared.state.textinfo = \"Loading B\"\n        print(f\"Loading {secondary_model_info.filename}...\")\n        theta_1 = sd_models.read_state_dict(secondary_model_info.filename, map_location='cpu')\n    else:\n        theta_1 = None\n\n    if theta_func1:\n        shared.state.textinfo = \"Loading C\"\n        print(f\"Loading {tertiary_model_info.filename}...\")\n        theta_2 = sd_models.read_state_dict(tertiary_model_info.filename, map_location='cpu')\n\n        shared.state.textinfo = 'Merging B and C'\n        shared.state.sampling_steps = len(theta_1.keys())\n        for key in tqdm.tqdm(theta_1.keys()):\n            if key in checkpoint_dict_skip_on_merge:\n                continue\n\n            if 'model' in key:\n                if key in theta_2:\n                    t2 = theta_2.get(key, torch.zeros_like(theta_1[key]))\n                    theta_1[key] = theta_func1(theta_1[key], t2)\n                else:\n                    theta_1[key] = torch.zeros_like(theta_1[key])\n\n            shared.state.sampling_step += 1\n        del theta_2\n\n        shared.state.nextjob()\n\n    shared.state.textinfo = f\"Loading {primary_model_info.filename}...\"\n    print(f\"Loading {primary_model_info.filename}...\")\n    theta_0 = sd_models.read_state_dict(primary_model_info.filename, map_location='cpu')\n\n    print(\"Merging...\")\n    shared.state.textinfo = 'Merging A and B'\n    shared.state.sampling_steps = len(theta_0.keys())\n    for key in tqdm.tqdm(theta_0.keys()):\n        if theta_1 and 'model' in key and key in theta_1:\n\n            if key in checkpoint_dict_skip_on_merge:\n                continue\n\n            a = theta_0[key]\n            b = theta_1[key]\n\n            # this enables merging an inpainting model (A) with another one (B);\n            # where normal model would have 4 channels, for latenst space, inpainting model would\n            # have another 4 channels for unmasked picture's latent space, plus one channel for mask, for a total of 9\n            if a.shape != b.shape and a.shape[0:1] + a.shape[2:] == b.shape[0:1] + b.shape[2:]:\n                if a.shape[1] == 4 and b.shape[1] == 9:\n                    raise RuntimeError(\"When merging inpainting model with a normal one, A must be the inpainting model.\")\n                if a.shape[1] == 4 and b.shape[1] == 8:\n                    raise RuntimeError(\"When merging instruct-pix2pix model with a normal one, A must be the instruct-pix2pix model.\")\n\n                if a.shape[1] == 8 and b.shape[1] == 4:#If we have an Instruct-Pix2Pix model...\n                    theta_0[key][:, 0:4, :, :] = theta_func2(a[:, 0:4, :, :], b, multiplier)#Merge only the vectors the models have in common.  Otherwise we get an error due to dimension mismatch.\n                    result_is_instruct_pix2pix_model = True\n                else:\n                    assert a.shape[1] == 9 and b.shape[1] == 4, f\"Bad dimensions for merged layer {key}: A={a.shape}, B={b.shape}\"\n                    theta_0[key][:, 0:4, :, :] = theta_func2(a[:, 0:4, :, :], b, multiplier)\n                    result_is_inpainting_model = True\n            else:\n                theta_0[key] = theta_func2(a, b, multiplier)\n\n            theta_0[key] = to_half(theta_0[key], save_as_half)\n\n        shared.state.sampling_step += 1\n\n    del theta_1\n\n    bake_in_vae_filename = sd_vae.vae_dict.get(bake_in_vae, None)\n    if bake_in_vae_filename is not None:\n        print(f\"Baking in VAE from {bake_in_vae_filename}\")\n        shared.state.textinfo = 'Baking in VAE'\n        vae_dict = sd_vae.load_vae_dict(bake_in_vae_filename, map_location='cpu')\n\n        for key in vae_dict.keys():\n            theta_0_key = 'first_stage_model.' + key\n            if theta_0_key in theta_0:\n                theta_0[theta_0_key] = to_half(vae_dict[key], save_as_half)\n\n        del vae_dict\n\n    if save_as_half and not theta_func2:\n        for key in theta_0.keys():\n            theta_0[key] = to_half(theta_0[key], save_as_half)\n\n    if discard_weights:\n        regex = re.compile(discard_weights)\n        for key in list(theta_0):\n            if re.search(regex, key):\n                theta_0.pop(key, None)\n\n    ckpt_dir = shared.cmd_opts.ckpt_dir or sd_models.model_path\n\n    filename = filename_generator() if custom_name == '' else custom_name\n    filename += \".inpainting\" if result_is_inpainting_model else \"\"\n    filename += \".instruct-pix2pix\" if result_is_instruct_pix2pix_model else \"\"\n    filename += \".\" + checkpoint_format\n\n    output_modelname = os.path.join(ckpt_dir, filename)\n\n    shared.state.nextjob()\n    shared.state.textinfo = \"Saving\"\n    print(f\"Saving to {output_modelname}...\")\n\n    metadata = {}\n\n    if save_metadata and copy_metadata_fields:\n        if primary_model_info:\n            metadata.update(primary_model_info.metadata)\n        if secondary_model_info:\n            metadata.update(secondary_model_info.metadata)\n        if tertiary_model_info:\n            metadata.update(tertiary_model_info.metadata)\n\n    if save_metadata:\n        try:\n            metadata.update(json.loads(metadata_json))\n        except Exception as e:\n            errors.display(e, \"readin metadata from json\")\n\n        metadata[\"format\"] = \"pt\"\n\n    if save_metadata and add_merge_recipe:\n        merge_recipe = {\n            \"type\": \"webui\", # indicate this model was merged with webui's built-in merger\n            \"primary_model_hash\": primary_model_info.sha256,\n            \"secondary_model_hash\": secondary_model_info.sha256 if secondary_model_info else None,\n            \"tertiary_model_hash\": tertiary_model_info.sha256 if tertiary_model_info else None,\n            \"interp_method\": interp_method,\n            \"multiplier\": multiplier,\n            \"save_as_half\": save_as_half,\n            \"custom_name\": custom_name,\n            \"config_source\": config_source,\n            \"bake_in_vae\": bake_in_vae,\n            \"discard_weights\": discard_weights,\n            \"is_inpainting\": result_is_inpainting_model,\n            \"is_instruct_pix2pix\": result_is_instruct_pix2pix_model\n        }\n\n        sd_merge_models = {}\n\n        def add_model_metadata(checkpoint_info):\n            checkpoint_info.calculate_shorthash()\n            sd_merge_models[checkpoint_info.sha256] = {\n                \"name\": checkpoint_info.name,\n                \"legacy_hash\": checkpoint_info.hash,\n                \"sd_merge_recipe\": checkpoint_info.metadata.get(\"sd_merge_recipe\", None)\n            }\n\n            sd_merge_models.update(checkpoint_info.metadata.get(\"sd_merge_models\", {}))\n\n        add_model_metadata(primary_model_info)\n        if secondary_model_info:\n            add_model_metadata(secondary_model_info)\n        if tertiary_model_info:\n            add_model_metadata(tertiary_model_info)\n\n        metadata[\"sd_merge_recipe\"] = json.dumps(merge_recipe)\n        metadata[\"sd_merge_models\"] = json.dumps(sd_merge_models)\n\n    _, extension = os.path.splitext(output_modelname)\n    if extension.lower() == \".safetensors\":\n        safetensors.torch.save_file(theta_0, output_modelname, metadata=metadata if len(metadata)>0 else None)\n    else:\n        torch.save(theta_0, output_modelname)\n\n    sd_models.list_models()\n    created_model = next((ckpt for ckpt in sd_models.checkpoints_list.values() if ckpt.name == filename), None)\n    if created_model:\n        created_model.calculate_shorthash()\n\n    create_config(output_modelname, config_source, primary_model_info, secondary_model_info, tertiary_model_info)\n\n    print(f\"Checkpoint saved to {output_modelname}.\")\n    shared.state.textinfo = \"Checkpoint saved\"\n    shared.state.end()\n\n    return [*[gr.Dropdown.update(choices=sd_models.checkpoint_tiles()) for _ in range(4)], \"Checkpoint saved to \" + output_modelname]\n", "modules/deepbooru.py": "import os\nimport re\n\nimport torch\nimport numpy as np\n\nfrom modules import modelloader, paths, deepbooru_model, devices, images, shared\n\nre_special = re.compile(r'([\\\\()])')\n\n\nclass DeepDanbooru:\n    def __init__(self):\n        self.model = None\n\n    def load(self):\n        if self.model is not None:\n            return\n\n        files = modelloader.load_models(\n            model_path=os.path.join(paths.models_path, \"torch_deepdanbooru\"),\n            model_url='https://github.com/AUTOMATIC1111/TorchDeepDanbooru/releases/download/v1/model-resnet_custom_v3.pt',\n            ext_filter=[\".pt\"],\n            download_name='model-resnet_custom_v3.pt',\n        )\n\n        self.model = deepbooru_model.DeepDanbooruModel()\n        self.model.load_state_dict(torch.load(files[0], map_location=\"cpu\"))\n\n        self.model.eval()\n        self.model.to(devices.cpu, devices.dtype)\n\n    def start(self):\n        self.load()\n        self.model.to(devices.device)\n\n    def stop(self):\n        if not shared.opts.interrogate_keep_models_in_memory:\n            self.model.to(devices.cpu)\n            devices.torch_gc()\n\n    def tag(self, pil_image):\n        self.start()\n        res = self.tag_multi(pil_image)\n        self.stop()\n\n        return res\n\n    def tag_multi(self, pil_image, force_disable_ranks=False):\n        threshold = shared.opts.interrogate_deepbooru_score_threshold\n        use_spaces = shared.opts.deepbooru_use_spaces\n        use_escape = shared.opts.deepbooru_escape\n        alpha_sort = shared.opts.deepbooru_sort_alpha\n        include_ranks = shared.opts.interrogate_return_ranks and not force_disable_ranks\n\n        pic = images.resize_image(2, pil_image.convert(\"RGB\"), 512, 512)\n        a = np.expand_dims(np.array(pic, dtype=np.float32), 0) / 255\n\n        with torch.no_grad(), devices.autocast():\n            x = torch.from_numpy(a).to(devices.device)\n            y = self.model(x)[0].detach().cpu().numpy()\n\n        probability_dict = {}\n\n        for tag, probability in zip(self.model.tags, y):\n            if probability < threshold:\n                continue\n\n            if tag.startswith(\"rating:\"):\n                continue\n\n            probability_dict[tag] = probability\n\n        if alpha_sort:\n            tags = sorted(probability_dict)\n        else:\n            tags = [tag for tag, _ in sorted(probability_dict.items(), key=lambda x: -x[1])]\n\n        res = []\n\n        filtertags = {x.strip().replace(' ', '_') for x in shared.opts.deepbooru_filter_tags.split(\",\")}\n\n        for tag in [x for x in tags if x not in filtertags]:\n            probability = probability_dict[tag]\n            tag_outformat = tag\n            if use_spaces:\n                tag_outformat = tag_outformat.replace('_', ' ')\n            if use_escape:\n                tag_outformat = re.sub(re_special, r'\\\\\\1', tag_outformat)\n            if include_ranks:\n                tag_outformat = f\"({tag_outformat}:{probability:.3f})\"\n\n            res.append(tag_outformat)\n\n        return \", \".join(res)\n\n\nmodel = DeepDanbooru()\n", "modules/images.py": "from __future__ import annotations\n\nimport datetime\nimport functools\nimport pytz\nimport io\nimport math\nimport os\nfrom collections import namedtuple\nimport re\n\nimport numpy as np\nimport piexif\nimport piexif.helper\nfrom PIL import Image, ImageFont, ImageDraw, ImageColor, PngImagePlugin, ImageOps\n# pillow_avif needs to be imported somewhere in code for it to work\nimport pillow_avif # noqa: F401\nimport string\nimport json\nimport hashlib\n\nfrom modules import sd_samplers, shared, script_callbacks, errors\nfrom modules.paths_internal import roboto_ttf_file\nfrom modules.shared import opts\n\nLANCZOS = (Image.Resampling.LANCZOS if hasattr(Image, 'Resampling') else Image.LANCZOS)\n\n\ndef get_font(fontsize: int):\n    try:\n        return ImageFont.truetype(opts.font or roboto_ttf_file, fontsize)\n    except Exception:\n        return ImageFont.truetype(roboto_ttf_file, fontsize)\n\n\ndef image_grid(imgs, batch_size=1, rows=None):\n    if rows is None:\n        if opts.n_rows > 0:\n            rows = opts.n_rows\n        elif opts.n_rows == 0:\n            rows = batch_size\n        elif opts.grid_prevent_empty_spots:\n            rows = math.floor(math.sqrt(len(imgs)))\n            while len(imgs) % rows != 0:\n                rows -= 1\n        else:\n            rows = math.sqrt(len(imgs))\n            rows = round(rows)\n    if rows > len(imgs):\n        rows = len(imgs)\n\n    cols = math.ceil(len(imgs) / rows)\n\n    params = script_callbacks.ImageGridLoopParams(imgs, cols, rows)\n    script_callbacks.image_grid_callback(params)\n\n    w, h = imgs[0].size\n    grid = Image.new('RGB', size=(params.cols * w, params.rows * h), color='black')\n\n    for i, img in enumerate(params.imgs):\n        grid.paste(img, box=(i % params.cols * w, i // params.cols * h))\n\n    return grid\n\n\nclass Grid(namedtuple(\"_Grid\", [\"tiles\", \"tile_w\", \"tile_h\", \"image_w\", \"image_h\", \"overlap\"])):\n    @property\n    def tile_count(self) -> int:\n        \"\"\"\n        The total number of tiles in the grid.\n        \"\"\"\n        return sum(len(row[2]) for row in self.tiles)\n\n\ndef split_grid(image: Image.Image, tile_w: int = 512, tile_h: int = 512, overlap: int = 64) -> Grid:\n    w, h = image.size\n\n    non_overlap_width = tile_w - overlap\n    non_overlap_height = tile_h - overlap\n\n    cols = math.ceil((w - overlap) / non_overlap_width)\n    rows = math.ceil((h - overlap) / non_overlap_height)\n\n    dx = (w - tile_w) / (cols - 1) if cols > 1 else 0\n    dy = (h - tile_h) / (rows - 1) if rows > 1 else 0\n\n    grid = Grid([], tile_w, tile_h, w, h, overlap)\n    for row in range(rows):\n        row_images = []\n\n        y = int(row * dy)\n\n        if y + tile_h >= h:\n            y = h - tile_h\n\n        for col in range(cols):\n            x = int(col * dx)\n\n            if x + tile_w >= w:\n                x = w - tile_w\n\n            tile = image.crop((x, y, x + tile_w, y + tile_h))\n\n            row_images.append([x, tile_w, tile])\n\n        grid.tiles.append([y, tile_h, row_images])\n\n    return grid\n\n\ndef combine_grid(grid):\n    def make_mask_image(r):\n        r = r * 255 / grid.overlap\n        r = r.astype(np.uint8)\n        return Image.fromarray(r, 'L')\n\n    mask_w = make_mask_image(np.arange(grid.overlap, dtype=np.float32).reshape((1, grid.overlap)).repeat(grid.tile_h, axis=0))\n    mask_h = make_mask_image(np.arange(grid.overlap, dtype=np.float32).reshape((grid.overlap, 1)).repeat(grid.image_w, axis=1))\n\n    combined_image = Image.new(\"RGB\", (grid.image_w, grid.image_h))\n    for y, h, row in grid.tiles:\n        combined_row = Image.new(\"RGB\", (grid.image_w, h))\n        for x, w, tile in row:\n            if x == 0:\n                combined_row.paste(tile, (0, 0))\n                continue\n\n            combined_row.paste(tile.crop((0, 0, grid.overlap, h)), (x, 0), mask=mask_w)\n            combined_row.paste(tile.crop((grid.overlap, 0, w, h)), (x + grid.overlap, 0))\n\n        if y == 0:\n            combined_image.paste(combined_row, (0, 0))\n            continue\n\n        combined_image.paste(combined_row.crop((0, 0, combined_row.width, grid.overlap)), (0, y), mask=mask_h)\n        combined_image.paste(combined_row.crop((0, grid.overlap, combined_row.width, h)), (0, y + grid.overlap))\n\n    return combined_image\n\n\nclass GridAnnotation:\n    def __init__(self, text='', is_active=True):\n        self.text = text\n        self.is_active = is_active\n        self.size = None\n\n\ndef draw_grid_annotations(im, width, height, hor_texts, ver_texts, margin=0):\n\n    color_active = ImageColor.getcolor(opts.grid_text_active_color, 'RGB')\n    color_inactive = ImageColor.getcolor(opts.grid_text_inactive_color, 'RGB')\n    color_background = ImageColor.getcolor(opts.grid_background_color, 'RGB')\n\n    def wrap(drawing, text, font, line_length):\n        lines = ['']\n        for word in text.split():\n            line = f'{lines[-1]} {word}'.strip()\n            if drawing.textlength(line, font=font) <= line_length:\n                lines[-1] = line\n            else:\n                lines.append(word)\n        return lines\n\n    def draw_texts(drawing, draw_x, draw_y, lines, initial_fnt, initial_fontsize):\n        for line in lines:\n            fnt = initial_fnt\n            fontsize = initial_fontsize\n            while drawing.multiline_textsize(line.text, font=fnt)[0] > line.allowed_width and fontsize > 0:\n                fontsize -= 1\n                fnt = get_font(fontsize)\n            drawing.multiline_text((draw_x, draw_y + line.size[1] / 2), line.text, font=fnt, fill=color_active if line.is_active else color_inactive, anchor=\"mm\", align=\"center\")\n\n            if not line.is_active:\n                drawing.line((draw_x - line.size[0] // 2, draw_y + line.size[1] // 2, draw_x + line.size[0] // 2, draw_y + line.size[1] // 2), fill=color_inactive, width=4)\n\n            draw_y += line.size[1] + line_spacing\n\n    fontsize = (width + height) // 25\n    line_spacing = fontsize // 2\n\n    fnt = get_font(fontsize)\n\n    pad_left = 0 if sum([sum([len(line.text) for line in lines]) for lines in ver_texts]) == 0 else width * 3 // 4\n\n    cols = im.width // width\n    rows = im.height // height\n\n    assert cols == len(hor_texts), f'bad number of horizontal texts: {len(hor_texts)}; must be {cols}'\n    assert rows == len(ver_texts), f'bad number of vertical texts: {len(ver_texts)}; must be {rows}'\n\n    calc_img = Image.new(\"RGB\", (1, 1), color_background)\n    calc_d = ImageDraw.Draw(calc_img)\n\n    for texts, allowed_width in zip(hor_texts + ver_texts, [width] * len(hor_texts) + [pad_left] * len(ver_texts)):\n        items = [] + texts\n        texts.clear()\n\n        for line in items:\n            wrapped = wrap(calc_d, line.text, fnt, allowed_width)\n            texts += [GridAnnotation(x, line.is_active) for x in wrapped]\n\n        for line in texts:\n            bbox = calc_d.multiline_textbbox((0, 0), line.text, font=fnt)\n            line.size = (bbox[2] - bbox[0], bbox[3] - bbox[1])\n            line.allowed_width = allowed_width\n\n    hor_text_heights = [sum([line.size[1] + line_spacing for line in lines]) - line_spacing for lines in hor_texts]\n    ver_text_heights = [sum([line.size[1] + line_spacing for line in lines]) - line_spacing * len(lines) for lines in ver_texts]\n\n    pad_top = 0 if sum(hor_text_heights) == 0 else max(hor_text_heights) + line_spacing * 2\n\n    result = Image.new(\"RGB\", (im.width + pad_left + margin * (cols-1), im.height + pad_top + margin * (rows-1)), color_background)\n\n    for row in range(rows):\n        for col in range(cols):\n            cell = im.crop((width * col, height * row, width * (col+1), height * (row+1)))\n            result.paste(cell, (pad_left + (width + margin) * col, pad_top + (height + margin) * row))\n\n    d = ImageDraw.Draw(result)\n\n    for col in range(cols):\n        x = pad_left + (width + margin) * col + width / 2\n        y = pad_top / 2 - hor_text_heights[col] / 2\n\n        draw_texts(d, x, y, hor_texts[col], fnt, fontsize)\n\n    for row in range(rows):\n        x = pad_left / 2\n        y = pad_top + (height + margin) * row + height / 2 - ver_text_heights[row] / 2\n\n        draw_texts(d, x, y, ver_texts[row], fnt, fontsize)\n\n    return result\n\n\ndef draw_prompt_matrix(im, width, height, all_prompts, margin=0):\n    prompts = all_prompts[1:]\n    boundary = math.ceil(len(prompts) / 2)\n\n    prompts_horiz = prompts[:boundary]\n    prompts_vert = prompts[boundary:]\n\n    hor_texts = [[GridAnnotation(x, is_active=pos & (1 << i) != 0) for i, x in enumerate(prompts_horiz)] for pos in range(1 << len(prompts_horiz))]\n    ver_texts = [[GridAnnotation(x, is_active=pos & (1 << i) != 0) for i, x in enumerate(prompts_vert)] for pos in range(1 << len(prompts_vert))]\n\n    return draw_grid_annotations(im, width, height, hor_texts, ver_texts, margin)\n\n\ndef resize_image(resize_mode, im, width, height, upscaler_name=None):\n    \"\"\"\n    Resizes an image with the specified resize_mode, width, and height.\n\n    Args:\n        resize_mode: The mode to use when resizing the image.\n            0: Resize the image to the specified width and height.\n            1: Resize the image to fill the specified width and height, maintaining the aspect ratio, and then center the image within the dimensions, cropping the excess.\n            2: Resize the image to fit within the specified width and height, maintaining the aspect ratio, and then center the image within the dimensions, filling empty with data from image.\n        im: The image to resize.\n        width: The width to resize the image to.\n        height: The height to resize the image to.\n        upscaler_name: The name of the upscaler to use. If not provided, defaults to opts.upscaler_for_img2img.\n    \"\"\"\n\n    upscaler_name = upscaler_name or opts.upscaler_for_img2img\n\n    def resize(im, w, h):\n        if upscaler_name is None or upscaler_name == \"None\" or im.mode == 'L':\n            return im.resize((w, h), resample=LANCZOS)\n\n        scale = max(w / im.width, h / im.height)\n\n        if scale > 1.0:\n            upscalers = [x for x in shared.sd_upscalers if x.name == upscaler_name]\n            if len(upscalers) == 0:\n                upscaler = shared.sd_upscalers[0]\n                print(f\"could not find upscaler named {upscaler_name or '<empty string>'}, using {upscaler.name} as a fallback\")\n            else:\n                upscaler = upscalers[0]\n\n            im = upscaler.scaler.upscale(im, scale, upscaler.data_path)\n\n        if im.width != w or im.height != h:\n            im = im.resize((w, h), resample=LANCZOS)\n\n        return im\n\n    if resize_mode == 0:\n        res = resize(im, width, height)\n\n    elif resize_mode == 1:\n        ratio = width / height\n        src_ratio = im.width / im.height\n\n        src_w = width if ratio > src_ratio else im.width * height // im.height\n        src_h = height if ratio <= src_ratio else im.height * width // im.width\n\n        resized = resize(im, src_w, src_h)\n        res = Image.new(\"RGB\", (width, height))\n        res.paste(resized, box=(width // 2 - src_w // 2, height // 2 - src_h // 2))\n\n    else:\n        ratio = width / height\n        src_ratio = im.width / im.height\n\n        src_w = width if ratio < src_ratio else im.width * height // im.height\n        src_h = height if ratio >= src_ratio else im.height * width // im.width\n\n        resized = resize(im, src_w, src_h)\n        res = Image.new(\"RGB\", (width, height))\n        res.paste(resized, box=(width // 2 - src_w // 2, height // 2 - src_h // 2))\n\n        if ratio < src_ratio:\n            fill_height = height // 2 - src_h // 2\n            if fill_height > 0:\n                res.paste(resized.resize((width, fill_height), box=(0, 0, width, 0)), box=(0, 0))\n                res.paste(resized.resize((width, fill_height), box=(0, resized.height, width, resized.height)), box=(0, fill_height + src_h))\n        elif ratio > src_ratio:\n            fill_width = width // 2 - src_w // 2\n            if fill_width > 0:\n                res.paste(resized.resize((fill_width, height), box=(0, 0, 0, height)), box=(0, 0))\n                res.paste(resized.resize((fill_width, height), box=(resized.width, 0, resized.width, height)), box=(fill_width + src_w, 0))\n\n    return res\n\n\nif not shared.cmd_opts.unix_filenames_sanitization:\n    invalid_filename_chars = '#<>:\"/\\\\|?*\\n\\r\\t'\nelse:\n    invalid_filename_chars = '/'\ninvalid_filename_prefix = ' '\ninvalid_filename_postfix = ' .'\nre_nonletters = re.compile(r'[\\s' + string.punctuation + ']+')\nre_pattern = re.compile(r\"(.*?)(?:\\[([^\\[\\]]+)\\]|$)\")\nre_pattern_arg = re.compile(r\"(.*)<([^>]*)>$\")\nmax_filename_part_length = shared.cmd_opts.filenames_max_length\nNOTHING_AND_SKIP_PREVIOUS_TEXT = object()\n\n\ndef sanitize_filename_part(text, replace_spaces=True):\n    if text is None:\n        return None\n\n    if replace_spaces:\n        text = text.replace(' ', '_')\n\n    text = text.translate({ord(x): '_' for x in invalid_filename_chars})\n    text = text.lstrip(invalid_filename_prefix)[:max_filename_part_length]\n    text = text.rstrip(invalid_filename_postfix)\n    return text\n\n\n@functools.cache\ndef get_scheduler_str(sampler_name, scheduler_name):\n    \"\"\"Returns {Scheduler} if the scheduler is applicable to the sampler\"\"\"\n    if scheduler_name == 'Automatic':\n        config = sd_samplers.find_sampler_config(sampler_name)\n        scheduler_name = config.options.get('scheduler', 'Automatic')\n    return scheduler_name.capitalize()\n\n\n@functools.cache\ndef get_sampler_scheduler_str(sampler_name, scheduler_name):\n    \"\"\"Returns the '{Sampler} {Scheduler}' if the scheduler is applicable to the sampler\"\"\"\n    return f'{sampler_name} {get_scheduler_str(sampler_name, scheduler_name)}'\n\n\ndef get_sampler_scheduler(p, sampler):\n    \"\"\"Returns '{Sampler} {Scheduler}' / '{Scheduler}' / 'NOTHING_AND_SKIP_PREVIOUS_TEXT'\"\"\"\n    if hasattr(p, 'scheduler') and hasattr(p, 'sampler_name'):\n        if sampler:\n            sampler_scheduler = get_sampler_scheduler_str(p.sampler_name, p.scheduler)\n        else:\n            sampler_scheduler = get_scheduler_str(p.sampler_name, p.scheduler)\n        return sanitize_filename_part(sampler_scheduler, replace_spaces=False)\n    return NOTHING_AND_SKIP_PREVIOUS_TEXT\n\n\nclass FilenameGenerator:\n    replacements = {\n        'seed': lambda self: self.seed if self.seed is not None else '',\n        'seed_first': lambda self: self.seed if self.p.batch_size == 1 else self.p.all_seeds[0],\n        'seed_last': lambda self: NOTHING_AND_SKIP_PREVIOUS_TEXT if self.p.batch_size == 1 else self.p.all_seeds[-1],\n        'steps': lambda self:  self.p and self.p.steps,\n        'cfg': lambda self: self.p and self.p.cfg_scale,\n        'width': lambda self: self.image.width,\n        'height': lambda self: self.image.height,\n        'styles': lambda self: self.p and sanitize_filename_part(\", \".join([style for style in self.p.styles if not style == \"None\"]) or \"None\", replace_spaces=False),\n        'sampler': lambda self: self.p and sanitize_filename_part(self.p.sampler_name, replace_spaces=False),\n        'sampler_scheduler': lambda self: self.p and get_sampler_scheduler(self.p, True),\n        'scheduler': lambda self: self.p and get_sampler_scheduler(self.p, False),\n        'model_hash': lambda self: getattr(self.p, \"sd_model_hash\", shared.sd_model.sd_model_hash),\n        'model_name': lambda self: sanitize_filename_part(shared.sd_model.sd_checkpoint_info.name_for_extra, replace_spaces=False),\n        'date': lambda self: datetime.datetime.now().strftime('%Y-%m-%d'),\n        'datetime': lambda self, *args: self.datetime(*args),  # accepts formats: [datetime], [datetime<Format>], [datetime<Format><Time Zone>]\n        'job_timestamp': lambda self: getattr(self.p, \"job_timestamp\", shared.state.job_timestamp),\n        'prompt_hash': lambda self, *args: self.string_hash(self.prompt, *args),\n        'negative_prompt_hash': lambda self, *args: self.string_hash(self.p.negative_prompt, *args),\n        'full_prompt_hash': lambda self, *args: self.string_hash(f\"{self.p.prompt} {self.p.negative_prompt}\", *args),  # a space in between to create a unique string\n        'prompt': lambda self: sanitize_filename_part(self.prompt),\n        'prompt_no_styles': lambda self: self.prompt_no_style(),\n        'prompt_spaces': lambda self: sanitize_filename_part(self.prompt, replace_spaces=False),\n        'prompt_words': lambda self: self.prompt_words(),\n        'batch_number': lambda self: NOTHING_AND_SKIP_PREVIOUS_TEXT if self.p.batch_size == 1 or self.zip else self.p.batch_index + 1,\n        'batch_size': lambda self: self.p.batch_size,\n        'generation_number': lambda self: NOTHING_AND_SKIP_PREVIOUS_TEXT if (self.p.n_iter == 1 and self.p.batch_size == 1) or self.zip else self.p.iteration * self.p.batch_size + self.p.batch_index + 1,\n        'hasprompt': lambda self, *args: self.hasprompt(*args),  # accepts formats:[hasprompt<prompt1|default><prompt2>..]\n        'clip_skip': lambda self: opts.data[\"CLIP_stop_at_last_layers\"],\n        'denoising': lambda self: self.p.denoising_strength if self.p and self.p.denoising_strength else NOTHING_AND_SKIP_PREVIOUS_TEXT,\n        'user': lambda self: self.p.user,\n        'vae_filename': lambda self: self.get_vae_filename(),\n        'none': lambda self: '',  # Overrides the default, so you can get just the sequence number\n        'image_hash': lambda self, *args: self.image_hash(*args)  # accepts formats: [image_hash<length>] default full hash\n    }\n    default_time_format = '%Y%m%d%H%M%S'\n\n    def __init__(self, p, seed, prompt, image, zip=False):\n        self.p = p\n        self.seed = seed\n        self.prompt = prompt\n        self.image = image\n        self.zip = zip\n\n    def get_vae_filename(self):\n        \"\"\"Get the name of the VAE file.\"\"\"\n\n        import modules.sd_vae as sd_vae\n\n        if sd_vae.loaded_vae_file is None:\n            return \"NoneType\"\n\n        file_name = os.path.basename(sd_vae.loaded_vae_file)\n        split_file_name = file_name.split('.')\n        if len(split_file_name) > 1 and split_file_name[0] == '':\n            return split_file_name[1]  # if the first character of the filename is \".\" then [1] is obtained.\n        else:\n            return split_file_name[0]\n\n\n    def hasprompt(self, *args):\n        lower = self.prompt.lower()\n        if self.p is None or self.prompt is None:\n            return None\n        outres = \"\"\n        for arg in args:\n            if arg != \"\":\n                division = arg.split(\"|\")\n                expected = division[0].lower()\n                default = division[1] if len(division) > 1 else \"\"\n                if lower.find(expected) >= 0:\n                    outres = f'{outres}{expected}'\n                else:\n                    outres = outres if default == \"\" else f'{outres}{default}'\n        return sanitize_filename_part(outres)\n\n    def prompt_no_style(self):\n        if self.p is None or self.prompt is None:\n            return None\n\n        prompt_no_style = self.prompt\n        for style in shared.prompt_styles.get_style_prompts(self.p.styles):\n            if style:\n                for part in style.split(\"{prompt}\"):\n                    prompt_no_style = prompt_no_style.replace(part, \"\").replace(\", ,\", \",\").strip().strip(',')\n\n                prompt_no_style = prompt_no_style.replace(style, \"\").strip().strip(',').strip()\n\n        return sanitize_filename_part(prompt_no_style, replace_spaces=False)\n\n    def prompt_words(self):\n        words = [x for x in re_nonletters.split(self.prompt or \"\") if x]\n        if len(words) == 0:\n            words = [\"empty\"]\n        return sanitize_filename_part(\" \".join(words[0:opts.directories_max_prompt_words]), replace_spaces=False)\n\n    def datetime(self, *args):\n        time_datetime = datetime.datetime.now()\n\n        time_format = args[0] if (args and args[0] != \"\") else self.default_time_format\n        try:\n            time_zone = pytz.timezone(args[1]) if len(args) > 1 else None\n        except pytz.exceptions.UnknownTimeZoneError:\n            time_zone = None\n\n        time_zone_time = time_datetime.astimezone(time_zone)\n        try:\n            formatted_time = time_zone_time.strftime(time_format)\n        except (ValueError, TypeError):\n            formatted_time = time_zone_time.strftime(self.default_time_format)\n\n        return sanitize_filename_part(formatted_time, replace_spaces=False)\n\n    def image_hash(self, *args):\n        length = int(args[0]) if (args and args[0] != \"\") else None\n        return hashlib.sha256(self.image.tobytes()).hexdigest()[0:length]\n\n    def string_hash(self, text, *args):\n        length = int(args[0]) if (args and args[0] != \"\") else 8\n        return hashlib.sha256(text.encode()).hexdigest()[0:length]\n\n    def apply(self, x):\n        res = ''\n\n        for m in re_pattern.finditer(x):\n            text, pattern = m.groups()\n\n            if pattern is None:\n                res += text\n                continue\n\n            pattern_args = []\n            while True:\n                m = re_pattern_arg.match(pattern)\n                if m is None:\n                    break\n\n                pattern, arg = m.groups()\n                pattern_args.insert(0, arg)\n\n            fun = self.replacements.get(pattern.lower())\n            if fun is not None:\n                try:\n                    replacement = fun(self, *pattern_args)\n                except Exception:\n                    replacement = None\n                    errors.report(f\"Error adding [{pattern}] to filename\", exc_info=True)\n\n                if replacement == NOTHING_AND_SKIP_PREVIOUS_TEXT:\n                    continue\n                elif replacement is not None:\n                    res += text + str(replacement)\n                    continue\n\n            res += f'{text}[{pattern}]'\n\n        return res\n\n\ndef get_next_sequence_number(path, basename):\n    \"\"\"\n    Determines and returns the next sequence number to use when saving an image in the specified directory.\n\n    The sequence starts at 0.\n    \"\"\"\n    result = -1\n    if basename != '':\n        basename = f\"{basename}-\"\n\n    prefix_length = len(basename)\n    for p in os.listdir(path):\n        if p.startswith(basename):\n            parts = os.path.splitext(p[prefix_length:])[0].split('-')  # splits the filename (removing the basename first if one is defined, so the sequence number is always the first element)\n            try:\n                result = max(int(parts[0]), result)\n            except ValueError:\n                pass\n\n    return result + 1\n\n\ndef save_image_with_geninfo(image, geninfo, filename, extension=None, existing_pnginfo=None, pnginfo_section_name='parameters'):\n    \"\"\"\n    Saves image to filename, including geninfo as text information for generation info.\n    For PNG images, geninfo is added to existing pnginfo dictionary using the pnginfo_section_name argument as key.\n    For JPG images, there's no dictionary and geninfo just replaces the EXIF description.\n    \"\"\"\n\n    if extension is None:\n        extension = os.path.splitext(filename)[1]\n\n    image_format = Image.registered_extensions()[extension]\n\n    if extension.lower() == '.png':\n        existing_pnginfo = existing_pnginfo or {}\n        if opts.enable_pnginfo:\n            existing_pnginfo[pnginfo_section_name] = geninfo\n\n        if opts.enable_pnginfo:\n            pnginfo_data = PngImagePlugin.PngInfo()\n            for k, v in (existing_pnginfo or {}).items():\n                pnginfo_data.add_text(k, str(v))\n        else:\n            pnginfo_data = None\n\n        image.save(filename, format=image_format, quality=opts.jpeg_quality, pnginfo=pnginfo_data)\n\n    elif extension.lower() in (\".jpg\", \".jpeg\", \".webp\"):\n        if image.mode == 'RGBA':\n            image = image.convert(\"RGB\")\n        elif image.mode == 'I;16':\n            image = image.point(lambda p: p * 0.0038910505836576).convert(\"RGB\" if extension.lower() == \".webp\" else \"L\")\n\n        image.save(filename, format=image_format, quality=opts.jpeg_quality, lossless=opts.webp_lossless)\n\n        if opts.enable_pnginfo and geninfo is not None:\n            exif_bytes = piexif.dump({\n                \"Exif\": {\n                    piexif.ExifIFD.UserComment: piexif.helper.UserComment.dump(geninfo or \"\", encoding=\"unicode\")\n                },\n            })\n\n            piexif.insert(exif_bytes, filename)\n    elif extension.lower() == '.avif':\n        if opts.enable_pnginfo and geninfo is not None:\n            exif_bytes = piexif.dump({\n                \"Exif\": {\n                    piexif.ExifIFD.UserComment: piexif.helper.UserComment.dump(geninfo or \"\", encoding=\"unicode\")\n                },\n            })\n\n\n        image.save(filename,format=image_format, exif=exif_bytes)\n    elif extension.lower() == \".gif\":\n        image.save(filename, format=image_format, comment=geninfo)\n    else:\n        image.save(filename, format=image_format, quality=opts.jpeg_quality)\n\n\ndef save_image(image, path, basename, seed=None, prompt=None, extension='png', info=None, short_filename=False, no_prompt=False, grid=False, pnginfo_section_name='parameters', p=None, existing_info=None, forced_filename=None, suffix=\"\", save_to_dirs=None):\n    \"\"\"Save an image.\n\n    Args:\n        image (`PIL.Image`):\n            The image to be saved.\n        path (`str`):\n            The directory to save the image. Note, the option `save_to_dirs` will make the image to be saved into a sub directory.\n        basename (`str`):\n            The base filename which will be applied to `filename pattern`.\n        seed, prompt, short_filename,\n        extension (`str`):\n            Image file extension, default is `png`.\n        pngsectionname (`str`):\n            Specify the name of the section which `info` will be saved in.\n        info (`str` or `PngImagePlugin.iTXt`):\n            PNG info chunks.\n        existing_info (`dict`):\n            Additional PNG info. `existing_info == {pngsectionname: info, ...}`\n        no_prompt:\n            TODO I don't know its meaning.\n        p (`StableDiffusionProcessing`)\n        forced_filename (`str`):\n            If specified, `basename` and filename pattern will be ignored.\n        save_to_dirs (bool):\n            If true, the image will be saved into a subdirectory of `path`.\n\n    Returns: (fullfn, txt_fullfn)\n        fullfn (`str`):\n            The full path of the saved imaged.\n        txt_fullfn (`str` or None):\n            If a text file is saved for this image, this will be its full path. Otherwise None.\n    \"\"\"\n    namegen = FilenameGenerator(p, seed, prompt, image)\n\n    # WebP and JPG formats have maximum dimension limits of 16383 and 65535 respectively. switch to PNG which has a much higher limit\n    if (image.height > 65535 or image.width > 65535) and extension.lower() in (\"jpg\", \"jpeg\") or (image.height > 16383 or image.width > 16383) and extension.lower() == \"webp\":\n        print('Image dimensions too large; saving as PNG')\n        extension = \".png\"\n\n    if save_to_dirs is None:\n        save_to_dirs = (grid and opts.grid_save_to_dirs) or (not grid and opts.save_to_dirs and not no_prompt)\n\n    if save_to_dirs:\n        dirname = namegen.apply(opts.directories_filename_pattern or \"[prompt_words]\").lstrip(' ').rstrip('\\\\ /')\n        path = os.path.join(path, dirname)\n\n    os.makedirs(path, exist_ok=True)\n\n    if forced_filename is None:\n        if short_filename or seed is None:\n            file_decoration = \"\"\n        elif opts.save_to_dirs:\n            file_decoration = opts.samples_filename_pattern or \"[seed]\"\n        else:\n            file_decoration = opts.samples_filename_pattern or \"[seed]-[prompt_spaces]\"\n\n        file_decoration = namegen.apply(file_decoration) + suffix\n\n        add_number = opts.save_images_add_number or file_decoration == ''\n\n        if file_decoration != \"\" and add_number:\n            file_decoration = f\"-{file_decoration}\"\n\n        if add_number:\n            basecount = get_next_sequence_number(path, basename)\n            fullfn = None\n            for i in range(500):\n                fn = f\"{basecount + i:05}\" if basename == '' else f\"{basename}-{basecount + i:04}\"\n                fullfn = os.path.join(path, f\"{fn}{file_decoration}.{extension}\")\n                if not os.path.exists(fullfn):\n                    break\n        else:\n            fullfn = os.path.join(path, f\"{file_decoration}.{extension}\")\n    else:\n        fullfn = os.path.join(path, f\"{forced_filename}.{extension}\")\n\n    pnginfo = existing_info or {}\n    if info is not None:\n        pnginfo[pnginfo_section_name] = info\n\n    params = script_callbacks.ImageSaveParams(image, p, fullfn, pnginfo)\n    script_callbacks.before_image_saved_callback(params)\n\n    image = params.image\n    fullfn = params.filename\n    info = params.pnginfo.get(pnginfo_section_name, None)\n\n    def _atomically_save_image(image_to_save, filename_without_extension, extension):\n        \"\"\"\n        save image with .tmp extension to avoid race condition when another process detects new image in the directory\n        \"\"\"\n        temp_file_path = f\"{filename_without_extension}.tmp\"\n\n        save_image_with_geninfo(image_to_save, info, temp_file_path, extension, existing_pnginfo=params.pnginfo, pnginfo_section_name=pnginfo_section_name)\n\n        filename = filename_without_extension + extension\n        if shared.opts.save_images_replace_action != \"Replace\":\n            n = 0\n            while os.path.exists(filename):\n                n += 1\n                filename = f\"{filename_without_extension}-{n}{extension}\"\n        os.replace(temp_file_path, filename)\n\n    fullfn_without_extension, extension = os.path.splitext(params.filename)\n    if hasattr(os, 'statvfs'):\n        max_name_len = os.statvfs(path).f_namemax\n        fullfn_without_extension = fullfn_without_extension[:max_name_len - max(4, len(extension))]\n        params.filename = fullfn_without_extension + extension\n        fullfn = params.filename\n    _atomically_save_image(image, fullfn_without_extension, extension)\n\n    image.already_saved_as = fullfn\n\n    oversize = image.width > opts.target_side_length or image.height > opts.target_side_length\n    if opts.export_for_4chan and (oversize or os.stat(fullfn).st_size > opts.img_downscale_threshold * 1024 * 1024):\n        ratio = image.width / image.height\n        resize_to = None\n        if oversize and ratio > 1:\n            resize_to = round(opts.target_side_length), round(image.height * opts.target_side_length / image.width)\n        elif oversize:\n            resize_to = round(image.width * opts.target_side_length / image.height), round(opts.target_side_length)\n\n        if resize_to is not None:\n            try:\n                # Resizing image with LANCZOS could throw an exception if e.g. image mode is I;16\n                image = image.resize(resize_to, LANCZOS)\n            except Exception:\n                image = image.resize(resize_to)\n        try:\n            _atomically_save_image(image, fullfn_without_extension, \".jpg\")\n        except Exception as e:\n            errors.display(e, \"saving image as downscaled JPG\")\n\n    if opts.save_txt and info is not None:\n        txt_fullfn = f\"{fullfn_without_extension}.txt\"\n        with open(txt_fullfn, \"w\", encoding=\"utf8\") as file:\n            file.write(f\"{info}\\n\")\n    else:\n        txt_fullfn = None\n\n    script_callbacks.image_saved_callback(params)\n\n    return fullfn, txt_fullfn\n\n\nIGNORED_INFO_KEYS = {\n    'jfif', 'jfif_version', 'jfif_unit', 'jfif_density', 'dpi', 'exif',\n    'loop', 'background', 'timestamp', 'duration', 'progressive', 'progression',\n    'icc_profile', 'chromaticity', 'photoshop',\n}\n\n\ndef read_info_from_image(image: Image.Image) -> tuple[str | None, dict]:\n    items = (image.info or {}).copy()\n\n    geninfo = items.pop('parameters', None)\n\n    if \"exif\" in items:\n        exif_data = items[\"exif\"]\n        try:\n            exif = piexif.load(exif_data)\n        except OSError:\n            # memory / exif was not valid so piexif tried to read from a file\n            exif = None\n        exif_comment = (exif or {}).get(\"Exif\", {}).get(piexif.ExifIFD.UserComment, b'')\n        try:\n            exif_comment = piexif.helper.UserComment.load(exif_comment)\n        except ValueError:\n            exif_comment = exif_comment.decode('utf8', errors=\"ignore\")\n\n        if exif_comment:\n            geninfo = exif_comment\n    elif \"comment\" in items: # for gif\n        geninfo = items[\"comment\"].decode('utf8', errors=\"ignore\")\n\n    for field in IGNORED_INFO_KEYS:\n        items.pop(field, None)\n\n    if items.get(\"Software\", None) == \"NovelAI\":\n        try:\n            json_info = json.loads(items[\"Comment\"])\n            sampler = sd_samplers.samplers_map.get(json_info[\"sampler\"], \"Euler a\")\n\n            geninfo = f\"\"\"{items[\"Description\"]}\nNegative prompt: {json_info[\"uc\"]}\nSteps: {json_info[\"steps\"]}, Sampler: {sampler}, CFG scale: {json_info[\"scale\"]}, Seed: {json_info[\"seed\"]}, Size: {image.width}x{image.height}, Clip skip: 2, ENSD: 31337\"\"\"\n        except Exception:\n            errors.report(\"Error parsing NovelAI image generation parameters\", exc_info=True)\n\n    return geninfo, items\n\n\ndef image_data(data):\n    import gradio as gr\n\n    try:\n        image = read(io.BytesIO(data))\n        textinfo, _ = read_info_from_image(image)\n        return textinfo, None\n    except Exception:\n        pass\n\n    try:\n        text = data.decode('utf8')\n        assert len(text) < 10000\n        return text, None\n\n    except Exception:\n        pass\n\n    return gr.update(), None\n\n\ndef flatten(img, bgcolor):\n    \"\"\"replaces transparency with bgcolor (example: \"#ffffff\"), returning an RGB mode image with no transparency\"\"\"\n\n    if img.mode == \"RGBA\":\n        background = Image.new('RGBA', img.size, bgcolor)\n        background.paste(img, mask=img)\n        img = background\n\n    return img.convert('RGB')\n\n\ndef read(fp, **kwargs):\n    image = Image.open(fp, **kwargs)\n    image = fix_image(image)\n\n    return image\n\n\ndef fix_image(image: Image.Image):\n    if image is None:\n        return None\n\n    try:\n        image = ImageOps.exif_transpose(image)\n        image = fix_png_transparency(image)\n    except Exception:\n        pass\n\n    return image\n\n\ndef fix_png_transparency(image: Image.Image):\n    if image.mode not in (\"RGB\", \"P\") or not isinstance(image.info.get(\"transparency\"), bytes):\n        return image\n\n    image = image.convert(\"RGBA\")\n    return image\n", "modules/sd_samplers_timesteps.py": "import torch\nimport inspect\nimport sys\nfrom modules import devices, sd_samplers_common, sd_samplers_timesteps_impl\nfrom modules.sd_samplers_cfg_denoiser import CFGDenoiser\nfrom modules.script_callbacks import ExtraNoiseParams, extra_noise_callback\n\nfrom modules.shared import opts\nimport modules.shared as shared\n\nsamplers_timesteps = [\n    ('DDIM', sd_samplers_timesteps_impl.ddim, ['ddim'], {}),\n    ('PLMS', sd_samplers_timesteps_impl.plms, ['plms'], {}),\n    ('UniPC', sd_samplers_timesteps_impl.unipc, ['unipc'], {}),\n]\n\n\nsamplers_data_timesteps = [\n    sd_samplers_common.SamplerData(label, lambda model, funcname=funcname: CompVisSampler(funcname, model), aliases, options)\n    for label, funcname, aliases, options in samplers_timesteps\n]\n\n\nclass CompVisTimestepsDenoiser(torch.nn.Module):\n    def __init__(self, model, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.inner_model = model\n\n    def forward(self, input, timesteps, **kwargs):\n        return self.inner_model.apply_model(input, timesteps, **kwargs)\n\n\nclass CompVisTimestepsVDenoiser(torch.nn.Module):\n    def __init__(self, model, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.inner_model = model\n\n    def predict_eps_from_z_and_v(self, x_t, t, v):\n        return torch.sqrt(self.inner_model.alphas_cumprod)[t.to(torch.int), None, None, None] * v + torch.sqrt(1 - self.inner_model.alphas_cumprod)[t.to(torch.int), None, None, None] * x_t\n\n    def forward(self, input, timesteps, **kwargs):\n        model_output = self.inner_model.apply_model(input, timesteps, **kwargs)\n        e_t = self.predict_eps_from_z_and_v(input, timesteps, model_output)\n        return e_t\n\n\nclass CFGDenoiserTimesteps(CFGDenoiser):\n\n    def __init__(self, sampler):\n        super().__init__(sampler)\n\n        self.alphas = shared.sd_model.alphas_cumprod\n        self.mask_before_denoising = True\n\n    def get_pred_x0(self, x_in, x_out, sigma):\n        ts = sigma.to(dtype=int)\n\n        a_t = self.alphas[ts][:, None, None, None]\n        sqrt_one_minus_at = (1 - a_t).sqrt()\n\n        pred_x0 = (x_in - sqrt_one_minus_at * x_out) / a_t.sqrt()\n\n        return pred_x0\n\n    @property\n    def inner_model(self):\n        if self.model_wrap is None:\n            denoiser = CompVisTimestepsVDenoiser if shared.sd_model.parameterization == \"v\" else CompVisTimestepsDenoiser\n            self.model_wrap = denoiser(shared.sd_model)\n\n        return self.model_wrap\n\n\nclass CompVisSampler(sd_samplers_common.Sampler):\n    def __init__(self, funcname, sd_model):\n        super().__init__(funcname)\n\n        self.eta_option_field = 'eta_ddim'\n        self.eta_infotext_field = 'Eta DDIM'\n        self.eta_default = 0.0\n\n        self.model_wrap_cfg = CFGDenoiserTimesteps(self)\n        self.model_wrap = self.model_wrap_cfg.inner_model\n\n    def get_timesteps(self, p, steps):\n        discard_next_to_last_sigma = self.config is not None and self.config.options.get('discard_next_to_last_sigma', False)\n        if opts.always_discard_next_to_last_sigma and not discard_next_to_last_sigma:\n            discard_next_to_last_sigma = True\n            p.extra_generation_params[\"Discard penultimate sigma\"] = True\n\n        steps += 1 if discard_next_to_last_sigma else 0\n\n        timesteps = torch.clip(torch.asarray(list(range(0, 1000, 1000 // steps)), device=devices.device) + 1, 0, 999)\n\n        return timesteps\n\n    def sample_img2img(self, p, x, noise, conditioning, unconditional_conditioning, steps=None, image_conditioning=None):\n        steps, t_enc = sd_samplers_common.setup_img2img_steps(p, steps)\n\n        timesteps = self.get_timesteps(p, steps)\n        timesteps_sched = timesteps[:t_enc]\n\n        alphas_cumprod = shared.sd_model.alphas_cumprod\n        sqrt_alpha_cumprod = torch.sqrt(alphas_cumprod[timesteps[t_enc]])\n        sqrt_one_minus_alpha_cumprod = torch.sqrt(1 - alphas_cumprod[timesteps[t_enc]])\n\n        xi = x * sqrt_alpha_cumprod + noise * sqrt_one_minus_alpha_cumprod\n\n        if opts.img2img_extra_noise > 0:\n            p.extra_generation_params[\"Extra noise\"] = opts.img2img_extra_noise\n            extra_noise_params = ExtraNoiseParams(noise, x, xi)\n            extra_noise_callback(extra_noise_params)\n            noise = extra_noise_params.noise\n            xi += noise * opts.img2img_extra_noise * sqrt_alpha_cumprod\n\n        extra_params_kwargs = self.initialize(p)\n        parameters = inspect.signature(self.func).parameters\n\n        if 'timesteps' in parameters:\n            extra_params_kwargs['timesteps'] = timesteps_sched\n        if 'is_img2img' in parameters:\n            extra_params_kwargs['is_img2img'] = True\n\n        self.model_wrap_cfg.init_latent = x\n        self.last_latent = x\n        self.sampler_extra_args = {\n            'cond': conditioning,\n            'image_cond': image_conditioning,\n            'uncond': unconditional_conditioning,\n            'cond_scale': p.cfg_scale,\n            's_min_uncond': self.s_min_uncond\n        }\n\n        samples = self.launch_sampling(t_enc + 1, lambda: self.func(self.model_wrap_cfg, xi, extra_args=self.sampler_extra_args, disable=False, callback=self.callback_state, **extra_params_kwargs))\n\n        self.add_infotext(p)\n\n        return samples\n\n    def sample(self, p, x, conditioning, unconditional_conditioning, steps=None, image_conditioning=None):\n        steps = steps or p.steps\n        timesteps = self.get_timesteps(p, steps)\n\n        extra_params_kwargs = self.initialize(p)\n        parameters = inspect.signature(self.func).parameters\n\n        if 'timesteps' in parameters:\n            extra_params_kwargs['timesteps'] = timesteps\n\n        self.last_latent = x\n        self.sampler_extra_args = {\n            'cond': conditioning,\n            'image_cond': image_conditioning,\n            'uncond': unconditional_conditioning,\n            'cond_scale': p.cfg_scale,\n            's_min_uncond': self.s_min_uncond\n        }\n        samples = self.launch_sampling(steps, lambda: self.func(self.model_wrap_cfg, x, extra_args=self.sampler_extra_args, disable=False, callback=self.callback_state, **extra_params_kwargs))\n\n        self.add_infotext(p)\n\n        return samples\n\n\nsys.modules['modules.sd_samplers_compvis'] = sys.modules[__name__]\nVanillaStableDiffusionSampler = CompVisSampler  # temp. compatibility with older extensions\n", "modules/ui_loadsave.py": "import json\nimport os\n\nimport gradio as gr\n\nfrom modules import errors\nfrom modules.ui_components import ToolButton, InputAccordion\n\n\ndef radio_choices(comp):  # gradio 3.41 changes choices from list of values to list of pairs\n    return [x[0] if isinstance(x, tuple) else x for x in getattr(comp, 'choices', [])]\n\n\nclass UiLoadsave:\n    \"\"\"allows saving and restoring default values for gradio components\"\"\"\n\n    def __init__(self, filename):\n        self.filename = filename\n        self.ui_settings = {}\n        self.component_mapping = {}\n        self.error_loading = False\n        self.finalized_ui = False\n\n        self.ui_defaults_view = None\n        self.ui_defaults_apply = None\n        self.ui_defaults_review = None\n\n        try:\n            self.ui_settings = self.read_from_file()\n        except FileNotFoundError:\n            pass\n        except Exception as e:\n            self.error_loading = True\n            errors.display(e, \"loading settings\")\n\n    def add_component(self, path, x):\n        \"\"\"adds component to the registry of tracked components\"\"\"\n\n        assert not self.finalized_ui\n\n        def apply_field(obj, field, condition=None, init_field=None):\n            key = f\"{path}/{field}\"\n\n            if getattr(obj, 'custom_script_source', None) is not None:\n                key = f\"customscript/{obj.custom_script_source}/{key}\"\n\n            if getattr(obj, 'do_not_save_to_config', False):\n                return\n\n            saved_value = self.ui_settings.get(key, None)\n\n            if isinstance(obj, gr.Accordion) and isinstance(x, InputAccordion) and field == 'value':\n                field = 'open'\n\n            if saved_value is None:\n                self.ui_settings[key] = getattr(obj, field)\n            elif condition and not condition(saved_value):\n                pass\n            else:\n                if isinstance(obj, gr.Textbox) and field == 'value':  # due to an undesirable behavior of gr.Textbox, if you give it an int value instead of str, everything dies\n                    saved_value = str(saved_value)\n                elif isinstance(obj, gr.Number) and field == 'value':\n                    try:\n                        saved_value = float(saved_value)\n                    except ValueError:\n                        return\n\n                setattr(obj, field, saved_value)\n                if init_field is not None:\n                    init_field(saved_value)\n\n            if field == 'value' and key not in self.component_mapping:\n                self.component_mapping[key] = obj\n\n        if type(x) in [gr.Slider, gr.Radio, gr.Checkbox, gr.Textbox, gr.Number, gr.Dropdown, ToolButton, gr.Button] and x.visible:\n            apply_field(x, 'visible')\n\n        if type(x) == gr.Slider:\n            apply_field(x, 'value')\n            apply_field(x, 'minimum')\n            apply_field(x, 'maximum')\n            apply_field(x, 'step')\n\n        if type(x) == gr.Radio:\n            apply_field(x, 'value', lambda val: val in radio_choices(x))\n\n        if type(x) == gr.Checkbox:\n            apply_field(x, 'value')\n\n        if type(x) == gr.Textbox:\n            apply_field(x, 'value')\n\n        if type(x) == gr.Number:\n            apply_field(x, 'value')\n\n        if type(x) == gr.Dropdown:\n            def check_dropdown(val):\n                choices = radio_choices(x)\n                if getattr(x, 'multiselect', False):\n                    return all(value in choices for value in val)\n                else:\n                    return val in choices\n\n            apply_field(x, 'value', check_dropdown, getattr(x, 'init_field', None))\n\n        if type(x) == InputAccordion:\n            if hasattr(x, 'custom_script_source'):\n                x.accordion.custom_script_source = x.custom_script_source\n            if x.accordion.visible:\n                apply_field(x.accordion, 'visible')\n            apply_field(x, 'value')\n            apply_field(x.accordion, 'value')\n\n        def check_tab_id(tab_id):\n            tab_items = list(filter(lambda e: isinstance(e, gr.TabItem), x.children))\n            if type(tab_id) == str:\n                tab_ids = [t.id for t in tab_items]\n                return tab_id in tab_ids\n            elif type(tab_id) == int:\n                return 0 <= tab_id < len(tab_items)\n            else:\n                return False\n\n        if type(x) == gr.Tabs:\n            apply_field(x, 'selected', check_tab_id)\n\n    def add_block(self, x, path=\"\"):\n        \"\"\"adds all components inside a gradio block x to the registry of tracked components\"\"\"\n\n        if hasattr(x, 'children'):\n            if isinstance(x, gr.Tabs) and x.elem_id is not None:\n                # Tabs element can't have a label, have to use elem_id instead\n                self.add_component(f\"{path}/Tabs@{x.elem_id}\", x)\n            for c in x.children:\n                self.add_block(c, path)\n        elif x.label is not None:\n            self.add_component(f\"{path}/{x.label}\", x)\n        elif isinstance(x, gr.Button) and x.value is not None:\n            self.add_component(f\"{path}/{x.value}\", x)\n\n    def read_from_file(self):\n        with open(self.filename, \"r\", encoding=\"utf8\") as file:\n            return json.load(file)\n\n    def write_to_file(self, current_ui_settings):\n        with open(self.filename, \"w\", encoding=\"utf8\") as file:\n            json.dump(current_ui_settings, file, indent=4, ensure_ascii=False)\n\n    def dump_defaults(self):\n        \"\"\"saves default values to a file unless the file is present and there was an error loading default values at start\"\"\"\n\n        if self.error_loading and os.path.exists(self.filename):\n            return\n\n        self.write_to_file(self.ui_settings)\n\n    def iter_changes(self, current_ui_settings, values):\n        \"\"\"\n        given a dictionary with defaults from a file and current values from gradio elements, returns\n        an iterator over tuples of values that are not the same between the file and the current;\n        tuple contents are: path, old value, new value\n        \"\"\"\n\n        for (path, component), new_value in zip(self.component_mapping.items(), values):\n            old_value = current_ui_settings.get(path)\n\n            choices = radio_choices(component)\n            if isinstance(new_value, int) and choices:\n                if new_value >= len(choices):\n                    continue\n\n                new_value = choices[new_value]\n                if isinstance(new_value, tuple):\n                    new_value = new_value[0]\n\n            if new_value == old_value:\n                continue\n\n            if old_value is None and new_value == '' or new_value == []:\n                continue\n\n            yield path, old_value, new_value\n\n    def ui_view(self, *values):\n        text = [\"<table><thead><tr><th>Path</th><th>Old value</th><th>New value</th></thead><tbody>\"]\n\n        for path, old_value, new_value in self.iter_changes(self.read_from_file(), values):\n            if old_value is None:\n                old_value = \"<span class='ui-defaults-none'>None</span>\"\n\n            text.append(f\"<tr><td>{path}</td><td>{old_value}</td><td>{new_value}</td></tr>\")\n\n        if len(text) == 1:\n            text.append(\"<tr><td colspan=3>No changes</td></tr>\")\n\n        text.append(\"</tbody>\")\n        return \"\".join(text)\n\n    def ui_apply(self, *values):\n        num_changed = 0\n\n        current_ui_settings = self.read_from_file()\n\n        for path, _, new_value in self.iter_changes(current_ui_settings.copy(), values):\n            num_changed += 1\n            current_ui_settings[path] = new_value\n\n        if num_changed == 0:\n            return \"No changes.\"\n\n        self.write_to_file(current_ui_settings)\n\n        return f\"Wrote {num_changed} changes.\"\n\n    def create_ui(self):\n        \"\"\"creates ui elements for editing defaults UI, without adding any logic to them\"\"\"\n\n        gr.HTML(\n            f\"This page allows you to change default values in UI elements on other tabs.<br />\"\n            f\"Make your changes, press 'View changes' to review the changed default values,<br />\"\n            f\"then press 'Apply' to write them to {self.filename}.<br />\"\n            f\"New defaults will apply after you restart the UI.<br />\"\n        )\n\n        with gr.Row():\n            self.ui_defaults_view = gr.Button(value='View changes', elem_id=\"ui_defaults_view\", variant=\"secondary\")\n            self.ui_defaults_apply = gr.Button(value='Apply', elem_id=\"ui_defaults_apply\", variant=\"primary\")\n\n        self.ui_defaults_review = gr.HTML(\"\")\n\n    def setup_ui(self):\n        \"\"\"adds logic to elements created with create_ui; all add_block class must be made before this\"\"\"\n\n        assert not self.finalized_ui\n        self.finalized_ui = True\n\n        self.ui_defaults_view.click(fn=self.ui_view, inputs=list(self.component_mapping.values()), outputs=[self.ui_defaults_review])\n        self.ui_defaults_apply.click(fn=self.ui_apply, inputs=list(self.component_mapping.values()), outputs=[self.ui_defaults_review])\n", "modules/sd_samplers_compvis.py": "", "modules/memmon.py": "import threading\nimport time\nfrom collections import defaultdict\n\nimport torch\n\n\nclass MemUsageMonitor(threading.Thread):\n    run_flag = None\n    device = None\n    disabled = False\n    opts = None\n    data = None\n\n    def __init__(self, name, device, opts):\n        threading.Thread.__init__(self)\n        self.name = name\n        self.device = device\n        self.opts = opts\n\n        self.daemon = True\n        self.run_flag = threading.Event()\n        self.data = defaultdict(int)\n\n        try:\n            self.cuda_mem_get_info()\n            torch.cuda.memory_stats(self.device)\n        except Exception as e:  # AMD or whatever\n            print(f\"Warning: caught exception '{e}', memory monitor disabled\")\n            self.disabled = True\n\n    def cuda_mem_get_info(self):\n        index = self.device.index if self.device.index is not None else torch.cuda.current_device()\n        return torch.cuda.mem_get_info(index)\n\n    def run(self):\n        if self.disabled:\n            return\n\n        while True:\n            self.run_flag.wait()\n\n            torch.cuda.reset_peak_memory_stats()\n            self.data.clear()\n\n            if self.opts.memmon_poll_rate <= 0:\n                self.run_flag.clear()\n                continue\n\n            self.data[\"min_free\"] = self.cuda_mem_get_info()[0]\n\n            while self.run_flag.is_set():\n                free, total = self.cuda_mem_get_info()\n                self.data[\"min_free\"] = min(self.data[\"min_free\"], free)\n\n                time.sleep(1 / self.opts.memmon_poll_rate)\n\n    def dump_debug(self):\n        print(self, 'recorded data:')\n        for k, v in self.read().items():\n            print(k, -(v // -(1024 ** 2)))\n\n        print(self, 'raw torch memory stats:')\n        tm = torch.cuda.memory_stats(self.device)\n        for k, v in tm.items():\n            if 'bytes' not in k:\n                continue\n            print('\\t' if 'peak' in k else '', k, -(v // -(1024 ** 2)))\n\n        print(torch.cuda.memory_summary())\n\n    def monitor(self):\n        self.run_flag.set()\n\n    def read(self):\n        if not self.disabled:\n            free, total = self.cuda_mem_get_info()\n            self.data[\"free\"] = free\n            self.data[\"total\"] = total\n\n            torch_stats = torch.cuda.memory_stats(self.device)\n            self.data[\"active\"] = torch_stats[\"active.all.current\"]\n            self.data[\"active_peak\"] = torch_stats[\"active_bytes.all.peak\"]\n            self.data[\"reserved\"] = torch_stats[\"reserved_bytes.all.current\"]\n            self.data[\"reserved_peak\"] = torch_stats[\"reserved_bytes.all.peak\"]\n            self.data[\"system_peak\"] = total - self.data[\"min_free\"]\n\n        return self.data\n\n    def stop(self):\n        self.run_flag.clear()\n        return self.read()\n", "modules/sd_samplers_timesteps_impl.py": "import torch\nimport tqdm\nimport k_diffusion.sampling\nimport numpy as np\n\nfrom modules import shared\nfrom modules.models.diffusion.uni_pc import uni_pc\n\n\n@torch.no_grad()\ndef ddim(model, x, timesteps, extra_args=None, callback=None, disable=None, eta=0.0):\n    alphas_cumprod = model.inner_model.inner_model.alphas_cumprod\n    alphas = alphas_cumprod[timesteps]\n    alphas_prev = alphas_cumprod[torch.nn.functional.pad(timesteps[:-1], pad=(1, 0))].to(torch.float64 if x.device.type != 'mps' and x.device.type != 'xpu' else torch.float32)\n    sqrt_one_minus_alphas = torch.sqrt(1 - alphas)\n    sigmas = eta * np.sqrt((1 - alphas_prev.cpu().numpy()) / (1 - alphas.cpu()) * (1 - alphas.cpu() / alphas_prev.cpu().numpy()))\n\n    extra_args = {} if extra_args is None else extra_args\n    s_in = x.new_ones((x.shape[0]))\n    s_x = x.new_ones((x.shape[0], 1, 1, 1))\n    for i in tqdm.trange(len(timesteps) - 1, disable=disable):\n        index = len(timesteps) - 1 - i\n\n        e_t = model(x, timesteps[index].item() * s_in, **extra_args)\n\n        a_t = alphas[index].item() * s_x\n        a_prev = alphas_prev[index].item() * s_x\n        sigma_t = sigmas[index].item() * s_x\n        sqrt_one_minus_at = sqrt_one_minus_alphas[index].item() * s_x\n\n        pred_x0 = (x - sqrt_one_minus_at * e_t) / a_t.sqrt()\n        dir_xt = (1. - a_prev - sigma_t ** 2).sqrt() * e_t\n        noise = sigma_t * k_diffusion.sampling.torch.randn_like(x)\n        x = a_prev.sqrt() * pred_x0 + dir_xt + noise\n\n        if callback is not None:\n            callback({'x': x, 'i': i, 'sigma': 0, 'sigma_hat': 0, 'denoised': pred_x0})\n\n    return x\n\n\n@torch.no_grad()\ndef plms(model, x, timesteps, extra_args=None, callback=None, disable=None):\n    alphas_cumprod = model.inner_model.inner_model.alphas_cumprod\n    alphas = alphas_cumprod[timesteps]\n    alphas_prev = alphas_cumprod[torch.nn.functional.pad(timesteps[:-1], pad=(1, 0))].to(torch.float64 if x.device.type != 'mps' and x.device.type != 'xpu' else torch.float32)\n    sqrt_one_minus_alphas = torch.sqrt(1 - alphas)\n\n    extra_args = {} if extra_args is None else extra_args\n    s_in = x.new_ones([x.shape[0]])\n    s_x = x.new_ones((x.shape[0], 1, 1, 1))\n    old_eps = []\n\n    def get_x_prev_and_pred_x0(e_t, index):\n        # select parameters corresponding to the currently considered timestep\n        a_t = alphas[index].item() * s_x\n        a_prev = alphas_prev[index].item() * s_x\n        sqrt_one_minus_at = sqrt_one_minus_alphas[index].item() * s_x\n\n        # current prediction for x_0\n        pred_x0 = (x - sqrt_one_minus_at * e_t) / a_t.sqrt()\n\n        # direction pointing to x_t\n        dir_xt = (1. - a_prev).sqrt() * e_t\n        x_prev = a_prev.sqrt() * pred_x0 + dir_xt\n        return x_prev, pred_x0\n\n    for i in tqdm.trange(len(timesteps) - 1, disable=disable):\n        index = len(timesteps) - 1 - i\n        ts = timesteps[index].item() * s_in\n        t_next = timesteps[max(index - 1, 0)].item() * s_in\n\n        e_t = model(x, ts, **extra_args)\n\n        if len(old_eps) == 0:\n            # Pseudo Improved Euler (2nd order)\n            x_prev, pred_x0 = get_x_prev_and_pred_x0(e_t, index)\n            e_t_next = model(x_prev, t_next, **extra_args)\n            e_t_prime = (e_t + e_t_next) / 2\n        elif len(old_eps) == 1:\n            # 2nd order Pseudo Linear Multistep (Adams-Bashforth)\n            e_t_prime = (3 * e_t - old_eps[-1]) / 2\n        elif len(old_eps) == 2:\n            # 3nd order Pseudo Linear Multistep (Adams-Bashforth)\n            e_t_prime = (23 * e_t - 16 * old_eps[-1] + 5 * old_eps[-2]) / 12\n        else:\n            # 4nd order Pseudo Linear Multistep (Adams-Bashforth)\n            e_t_prime = (55 * e_t - 59 * old_eps[-1] + 37 * old_eps[-2] - 9 * old_eps[-3]) / 24\n\n        x_prev, pred_x0 = get_x_prev_and_pred_x0(e_t_prime, index)\n\n        old_eps.append(e_t)\n        if len(old_eps) >= 4:\n            old_eps.pop(0)\n\n        x = x_prev\n\n        if callback is not None:\n            callback({'x': x, 'i': i, 'sigma': 0, 'sigma_hat': 0, 'denoised': pred_x0})\n\n    return x\n\n\nclass UniPCCFG(uni_pc.UniPC):\n    def __init__(self, cfg_model, extra_args, callback, *args, **kwargs):\n        super().__init__(None, *args, **kwargs)\n\n        def after_update(x, model_x):\n            callback({'x': x, 'i': self.index, 'sigma': 0, 'sigma_hat': 0, 'denoised': model_x})\n            self.index += 1\n\n        self.cfg_model = cfg_model\n        self.extra_args = extra_args\n        self.callback = callback\n        self.index = 0\n        self.after_update = after_update\n\n    def get_model_input_time(self, t_continuous):\n        return (t_continuous - 1. / self.noise_schedule.total_N) * 1000.\n\n    def model(self, x, t):\n        t_input = self.get_model_input_time(t)\n\n        res = self.cfg_model(x, t_input, **self.extra_args)\n\n        return res\n\n\ndef unipc(model, x, timesteps, extra_args=None, callback=None, disable=None, is_img2img=False):\n    alphas_cumprod = model.inner_model.inner_model.alphas_cumprod\n\n    ns = uni_pc.NoiseScheduleVP('discrete', alphas_cumprod=alphas_cumprod)\n    t_start = timesteps[-1] / 1000 + 1 / 1000 if is_img2img else None  # this is likely off by a bit - if someone wants to fix it please by all means\n    unipc_sampler = UniPCCFG(model, extra_args, callback, ns, predict_x0=True, thresholding=False, variant=shared.opts.uni_pc_variant)\n    x = unipc_sampler.sample(x, steps=len(timesteps), t_start=t_start, skip_type=shared.opts.uni_pc_skip_type, method=\"multistep\", order=shared.opts.uni_pc_order, lower_order_final=shared.opts.uni_pc_lower_order_final)\n\n    return x\n", "modules/sd_hijack.py": "import torch\nfrom torch.nn.functional import silu\nfrom types import MethodType\n\nfrom modules import devices, sd_hijack_optimizations, shared, script_callbacks, errors, sd_unet, patches\nfrom modules.hypernetworks import hypernetwork\nfrom modules.shared import cmd_opts\nfrom modules import sd_hijack_clip, sd_hijack_open_clip, sd_hijack_unet, sd_hijack_xlmr, xlmr, xlmr_m18\n\nimport ldm.modules.attention\nimport ldm.modules.diffusionmodules.model\nimport ldm.modules.diffusionmodules.openaimodel\nimport ldm.models.diffusion.ddpm\nimport ldm.models.diffusion.ddim\nimport ldm.models.diffusion.plms\nimport ldm.modules.encoders.modules\n\nimport sgm.modules.attention\nimport sgm.modules.diffusionmodules.model\nimport sgm.modules.diffusionmodules.openaimodel\nimport sgm.modules.encoders.modules\n\nattention_CrossAttention_forward = ldm.modules.attention.CrossAttention.forward\ndiffusionmodules_model_nonlinearity = ldm.modules.diffusionmodules.model.nonlinearity\ndiffusionmodules_model_AttnBlock_forward = ldm.modules.diffusionmodules.model.AttnBlock.forward\n\n# new memory efficient cross attention blocks do not support hypernets and we already\n# have memory efficient cross attention anyway, so this disables SD2.0's memory efficient cross attention\nldm.modules.attention.MemoryEfficientCrossAttention = ldm.modules.attention.CrossAttention\nldm.modules.attention.BasicTransformerBlock.ATTENTION_MODES[\"softmax-xformers\"] = ldm.modules.attention.CrossAttention\n\n# silence new console spam from SD2\nldm.modules.attention.print = shared.ldm_print\nldm.modules.diffusionmodules.model.print = shared.ldm_print\nldm.util.print = shared.ldm_print\nldm.models.diffusion.ddpm.print = shared.ldm_print\n\noptimizers = []\ncurrent_optimizer: sd_hijack_optimizations.SdOptimization = None\n\nldm_patched_forward = sd_unet.create_unet_forward(ldm.modules.diffusionmodules.openaimodel.UNetModel.forward)\nldm_original_forward = patches.patch(__file__, ldm.modules.diffusionmodules.openaimodel.UNetModel, \"forward\", ldm_patched_forward)\n\nsgm_patched_forward = sd_unet.create_unet_forward(sgm.modules.diffusionmodules.openaimodel.UNetModel.forward)\nsgm_original_forward = patches.patch(__file__, sgm.modules.diffusionmodules.openaimodel.UNetModel, \"forward\", sgm_patched_forward)\n\n\ndef list_optimizers():\n    new_optimizers = script_callbacks.list_optimizers_callback()\n\n    new_optimizers = [x for x in new_optimizers if x.is_available()]\n\n    new_optimizers = sorted(new_optimizers, key=lambda x: x.priority, reverse=True)\n\n    optimizers.clear()\n    optimizers.extend(new_optimizers)\n\n\ndef apply_optimizations(option=None):\n    global current_optimizer\n\n    undo_optimizations()\n\n    if len(optimizers) == 0:\n        # a script can access the model very early, and optimizations would not be filled by then\n        current_optimizer = None\n        return ''\n\n    ldm.modules.diffusionmodules.model.nonlinearity = silu\n    ldm.modules.diffusionmodules.openaimodel.th = sd_hijack_unet.th\n\n    sgm.modules.diffusionmodules.model.nonlinearity = silu\n    sgm.modules.diffusionmodules.openaimodel.th = sd_hijack_unet.th\n\n    if current_optimizer is not None:\n        current_optimizer.undo()\n        current_optimizer = None\n\n    selection = option or shared.opts.cross_attention_optimization\n    if selection == \"Automatic\" and len(optimizers) > 0:\n        matching_optimizer = next(iter([x for x in optimizers if x.cmd_opt and getattr(shared.cmd_opts, x.cmd_opt, False)]), optimizers[0])\n    else:\n        matching_optimizer = next(iter([x for x in optimizers if x.title() == selection]), None)\n\n    if selection == \"None\":\n        matching_optimizer = None\n    elif selection == \"Automatic\" and shared.cmd_opts.disable_opt_split_attention:\n        matching_optimizer = None\n    elif matching_optimizer is None:\n        matching_optimizer = optimizers[0]\n\n    if matching_optimizer is not None:\n        print(f\"Applying attention optimization: {matching_optimizer.name}... \", end='')\n        matching_optimizer.apply()\n        print(\"done.\")\n        current_optimizer = matching_optimizer\n        return current_optimizer.name\n    else:\n        print(\"Disabling attention optimization\")\n        return ''\n\n\ndef undo_optimizations():\n    ldm.modules.diffusionmodules.model.nonlinearity = diffusionmodules_model_nonlinearity\n    ldm.modules.attention.CrossAttention.forward = hypernetwork.attention_CrossAttention_forward\n    ldm.modules.diffusionmodules.model.AttnBlock.forward = diffusionmodules_model_AttnBlock_forward\n\n    sgm.modules.diffusionmodules.model.nonlinearity = diffusionmodules_model_nonlinearity\n    sgm.modules.attention.CrossAttention.forward = hypernetwork.attention_CrossAttention_forward\n    sgm.modules.diffusionmodules.model.AttnBlock.forward = diffusionmodules_model_AttnBlock_forward\n\n\ndef fix_checkpoint():\n    \"\"\"checkpoints are now added and removed in embedding/hypernet code, since torch doesn't want\n    checkpoints to be added when not training (there's a warning)\"\"\"\n\n    pass\n\n\ndef weighted_loss(sd_model, pred, target, mean=True):\n    #Calculate the weight normally, but ignore the mean\n    loss = sd_model._old_get_loss(pred, target, mean=False)\n\n    #Check if we have weights available\n    weight = getattr(sd_model, '_custom_loss_weight', None)\n    if weight is not None:\n        loss *= weight\n\n    #Return the loss, as mean if specified\n    return loss.mean() if mean else loss\n\ndef weighted_forward(sd_model, x, c, w, *args, **kwargs):\n    try:\n        #Temporarily append weights to a place accessible during loss calc\n        sd_model._custom_loss_weight = w\n\n        #Replace 'get_loss' with a weight-aware one. Otherwise we need to reimplement 'forward' completely\n        #Keep 'get_loss', but don't overwrite the previous old_get_loss if it's already set\n        if not hasattr(sd_model, '_old_get_loss'):\n            sd_model._old_get_loss = sd_model.get_loss\n        sd_model.get_loss = MethodType(weighted_loss, sd_model)\n\n        #Run the standard forward function, but with the patched 'get_loss'\n        return sd_model.forward(x, c, *args, **kwargs)\n    finally:\n        try:\n            #Delete temporary weights if appended\n            del sd_model._custom_loss_weight\n        except AttributeError:\n            pass\n\n        #If we have an old loss function, reset the loss function to the original one\n        if hasattr(sd_model, '_old_get_loss'):\n            sd_model.get_loss = sd_model._old_get_loss\n            del sd_model._old_get_loss\n\ndef apply_weighted_forward(sd_model):\n    #Add new function 'weighted_forward' that can be called to calc weighted loss\n    sd_model.weighted_forward = MethodType(weighted_forward, sd_model)\n\ndef undo_weighted_forward(sd_model):\n    try:\n        del sd_model.weighted_forward\n    except AttributeError:\n        pass\n\n\nclass StableDiffusionModelHijack:\n    fixes = None\n    layers = None\n    circular_enabled = False\n    clip = None\n    optimization_method = None\n\n    def __init__(self):\n        import modules.textual_inversion.textual_inversion\n\n        self.extra_generation_params = {}\n        self.comments = []\n\n        self.embedding_db = modules.textual_inversion.textual_inversion.EmbeddingDatabase()\n        self.embedding_db.add_embedding_dir(cmd_opts.embeddings_dir)\n\n    def apply_optimizations(self, option=None):\n        try:\n            self.optimization_method = apply_optimizations(option)\n        except Exception as e:\n            errors.display(e, \"applying cross attention optimization\")\n            undo_optimizations()\n\n    def convert_sdxl_to_ssd(self, m):\n        \"\"\"Converts an SDXL model to a Segmind Stable Diffusion model (see https://huggingface.co/segmind/SSD-1B)\"\"\"\n\n        delattr(m.model.diffusion_model.middle_block, '1')\n        delattr(m.model.diffusion_model.middle_block, '2')\n        for i in ['9', '8', '7', '6', '5', '4']:\n            delattr(m.model.diffusion_model.input_blocks[7][1].transformer_blocks, i)\n            delattr(m.model.diffusion_model.input_blocks[8][1].transformer_blocks, i)\n            delattr(m.model.diffusion_model.output_blocks[0][1].transformer_blocks, i)\n            delattr(m.model.diffusion_model.output_blocks[1][1].transformer_blocks, i)\n        delattr(m.model.diffusion_model.output_blocks[4][1].transformer_blocks, '1')\n        delattr(m.model.diffusion_model.output_blocks[5][1].transformer_blocks, '1')\n        devices.torch_gc()\n\n    def hijack(self, m):\n        conditioner = getattr(m, 'conditioner', None)\n        if conditioner:\n            text_cond_models = []\n\n            for i in range(len(conditioner.embedders)):\n                embedder = conditioner.embedders[i]\n                typename = type(embedder).__name__\n                if typename == 'FrozenOpenCLIPEmbedder':\n                    embedder.model.token_embedding = EmbeddingsWithFixes(embedder.model.token_embedding, self)\n                    conditioner.embedders[i] = sd_hijack_open_clip.FrozenOpenCLIPEmbedderWithCustomWords(embedder, self)\n                    text_cond_models.append(conditioner.embedders[i])\n                if typename == 'FrozenCLIPEmbedder':\n                    model_embeddings = embedder.transformer.text_model.embeddings\n                    model_embeddings.token_embedding = EmbeddingsWithFixes(model_embeddings.token_embedding, self)\n                    conditioner.embedders[i] = sd_hijack_clip.FrozenCLIPEmbedderForSDXLWithCustomWords(embedder, self)\n                    text_cond_models.append(conditioner.embedders[i])\n                if typename == 'FrozenOpenCLIPEmbedder2':\n                    embedder.model.token_embedding = EmbeddingsWithFixes(embedder.model.token_embedding, self, textual_inversion_key='clip_g')\n                    conditioner.embedders[i] = sd_hijack_open_clip.FrozenOpenCLIPEmbedder2WithCustomWords(embedder, self)\n                    text_cond_models.append(conditioner.embedders[i])\n\n            if len(text_cond_models) == 1:\n                m.cond_stage_model = text_cond_models[0]\n            else:\n                m.cond_stage_model = conditioner\n\n        if type(m.cond_stage_model) == xlmr.BertSeriesModelWithTransformation or type(m.cond_stage_model) == xlmr_m18.BertSeriesModelWithTransformation:\n            model_embeddings = m.cond_stage_model.roberta.embeddings\n            model_embeddings.token_embedding = EmbeddingsWithFixes(model_embeddings.word_embeddings, self)\n            m.cond_stage_model = sd_hijack_xlmr.FrozenXLMREmbedderWithCustomWords(m.cond_stage_model, self)\n\n        elif type(m.cond_stage_model) == ldm.modules.encoders.modules.FrozenCLIPEmbedder:\n            model_embeddings = m.cond_stage_model.transformer.text_model.embeddings\n            model_embeddings.token_embedding = EmbeddingsWithFixes(model_embeddings.token_embedding, self)\n            m.cond_stage_model = sd_hijack_clip.FrozenCLIPEmbedderWithCustomWords(m.cond_stage_model, self)\n\n        elif type(m.cond_stage_model) == ldm.modules.encoders.modules.FrozenOpenCLIPEmbedder:\n            m.cond_stage_model.model.token_embedding = EmbeddingsWithFixes(m.cond_stage_model.model.token_embedding, self)\n            m.cond_stage_model = sd_hijack_open_clip.FrozenOpenCLIPEmbedderWithCustomWords(m.cond_stage_model, self)\n\n        apply_weighted_forward(m)\n        if m.cond_stage_key == \"edit\":\n            sd_hijack_unet.hijack_ddpm_edit()\n\n        self.apply_optimizations()\n\n        self.clip = m.cond_stage_model\n\n        def flatten(el):\n            flattened = [flatten(children) for children in el.children()]\n            res = [el]\n            for c in flattened:\n                res += c\n            return res\n\n        self.layers = flatten(m)\n\n        import modules.models.diffusion.ddpm_edit\n\n        if isinstance(m, ldm.models.diffusion.ddpm.LatentDiffusion):\n            sd_unet.original_forward = ldm_original_forward\n        elif isinstance(m, modules.models.diffusion.ddpm_edit.LatentDiffusion):\n            sd_unet.original_forward = ldm_original_forward\n        elif isinstance(m, sgm.models.diffusion.DiffusionEngine):\n            sd_unet.original_forward = sgm_original_forward\n        else:\n            sd_unet.original_forward = None\n\n\n    def undo_hijack(self, m):\n        conditioner = getattr(m, 'conditioner', None)\n        if conditioner:\n            for i in range(len(conditioner.embedders)):\n                embedder = conditioner.embedders[i]\n                if isinstance(embedder, (sd_hijack_open_clip.FrozenOpenCLIPEmbedderWithCustomWords, sd_hijack_open_clip.FrozenOpenCLIPEmbedder2WithCustomWords)):\n                    embedder.wrapped.model.token_embedding = embedder.wrapped.model.token_embedding.wrapped\n                    conditioner.embedders[i] = embedder.wrapped\n                if isinstance(embedder, sd_hijack_clip.FrozenCLIPEmbedderForSDXLWithCustomWords):\n                    embedder.wrapped.transformer.text_model.embeddings.token_embedding = embedder.wrapped.transformer.text_model.embeddings.token_embedding.wrapped\n                    conditioner.embedders[i] = embedder.wrapped\n\n            if hasattr(m, 'cond_stage_model'):\n                delattr(m, 'cond_stage_model')\n\n        elif type(m.cond_stage_model) == sd_hijack_xlmr.FrozenXLMREmbedderWithCustomWords:\n            m.cond_stage_model = m.cond_stage_model.wrapped\n\n        elif type(m.cond_stage_model) == sd_hijack_clip.FrozenCLIPEmbedderWithCustomWords:\n            m.cond_stage_model = m.cond_stage_model.wrapped\n\n            model_embeddings = m.cond_stage_model.transformer.text_model.embeddings\n            if type(model_embeddings.token_embedding) == EmbeddingsWithFixes:\n                model_embeddings.token_embedding = model_embeddings.token_embedding.wrapped\n        elif type(m.cond_stage_model) == sd_hijack_open_clip.FrozenOpenCLIPEmbedderWithCustomWords:\n            m.cond_stage_model.wrapped.model.token_embedding = m.cond_stage_model.wrapped.model.token_embedding.wrapped\n            m.cond_stage_model = m.cond_stage_model.wrapped\n\n        undo_optimizations()\n        undo_weighted_forward(m)\n\n        self.apply_circular(False)\n        self.layers = None\n        self.clip = None\n\n\n    def apply_circular(self, enable):\n        if self.circular_enabled == enable:\n            return\n\n        self.circular_enabled = enable\n\n        for layer in [layer for layer in self.layers if type(layer) == torch.nn.Conv2d]:\n            layer.padding_mode = 'circular' if enable else 'zeros'\n\n    def clear_comments(self):\n        self.comments = []\n        self.extra_generation_params = {}\n\n    def get_prompt_lengths(self, text):\n        if self.clip is None:\n            return \"-\", \"-\"\n\n        _, token_count = self.clip.process_texts([text])\n\n        return token_count, self.clip.get_target_prompt_token_count(token_count)\n\n    def redo_hijack(self, m):\n        self.undo_hijack(m)\n        self.hijack(m)\n\n\nclass EmbeddingsWithFixes(torch.nn.Module):\n    def __init__(self, wrapped, embeddings, textual_inversion_key='clip_l'):\n        super().__init__()\n        self.wrapped = wrapped\n        self.embeddings = embeddings\n        self.textual_inversion_key = textual_inversion_key\n\n    def forward(self, input_ids):\n        batch_fixes = self.embeddings.fixes\n        self.embeddings.fixes = None\n\n        inputs_embeds = self.wrapped(input_ids)\n\n        if batch_fixes is None or len(batch_fixes) == 0 or max([len(x) for x in batch_fixes]) == 0:\n            return inputs_embeds\n\n        vecs = []\n        for fixes, tensor in zip(batch_fixes, inputs_embeds):\n            for offset, embedding in fixes:\n                vec = embedding.vec[self.textual_inversion_key] if isinstance(embedding.vec, dict) else embedding.vec\n                emb = devices.cond_cast_unet(vec)\n                emb_len = min(tensor.shape[0] - offset - 1, emb.shape[0])\n                tensor = torch.cat([tensor[0:offset + 1], emb[0:emb_len], tensor[offset + 1 + emb_len:]])\n\n            vecs.append(tensor)\n\n        return torch.stack(vecs)\n\n\ndef add_circular_option_to_conv_2d():\n    conv2d_constructor = torch.nn.Conv2d.__init__\n\n    def conv2d_constructor_circular(self, *args, **kwargs):\n        return conv2d_constructor(self, *args, padding_mode='circular', **kwargs)\n\n    torch.nn.Conv2d.__init__ = conv2d_constructor_circular\n\n\nmodel_hijack = StableDiffusionModelHijack()\n\n\ndef register_buffer(self, name, attr):\n    \"\"\"\n    Fix register buffer bug for Mac OS.\n    \"\"\"\n\n    if type(attr) == torch.Tensor:\n        if attr.device != devices.device:\n            attr = attr.to(device=devices.device, dtype=(torch.float32 if devices.device.type == 'mps' else None))\n\n    setattr(self, name, attr)\n\n\nldm.models.diffusion.ddim.DDIMSampler.register_buffer = register_buffer\nldm.models.diffusion.plms.PLMSSampler.register_buffer = register_buffer\n", "modules/launch_utils.py": "# this scripts installs necessary requirements and launches main program in webui.py\nimport logging\nimport re\nimport subprocess\nimport os\nimport shutil\nimport sys\nimport importlib.util\nimport importlib.metadata\nimport platform\nimport json\nfrom functools import lru_cache\n\nfrom modules import cmd_args, errors\nfrom modules.paths_internal import script_path, extensions_dir\nfrom modules.timer import startup_timer\nfrom modules import logging_config\n\nargs, _ = cmd_args.parser.parse_known_args()\nlogging_config.setup_logging(args.loglevel)\n\npython = sys.executable\ngit = os.environ.get('GIT', \"git\")\nindex_url = os.environ.get('INDEX_URL', \"\")\ndir_repos = \"repositories\"\n\n# Whether to default to printing command output\ndefault_command_live = (os.environ.get('WEBUI_LAUNCH_LIVE_OUTPUT') == \"1\")\n\nos.environ.setdefault('GRADIO_ANALYTICS_ENABLED', 'False')\n\n\ndef check_python_version():\n    is_windows = platform.system() == \"Windows\"\n    major = sys.version_info.major\n    minor = sys.version_info.minor\n    micro = sys.version_info.micro\n\n    if is_windows:\n        supported_minors = [10]\n    else:\n        supported_minors = [7, 8, 9, 10, 11]\n\n    if not (major == 3 and minor in supported_minors):\n        import modules.errors\n\n        modules.errors.print_error_explanation(f\"\"\"\nINCOMPATIBLE PYTHON VERSION\n\nThis program is tested with 3.10.6 Python, but you have {major}.{minor}.{micro}.\nIf you encounter an error with \"RuntimeError: Couldn't install torch.\" message,\nor any other error regarding unsuccessful package (library) installation,\nplease downgrade (or upgrade) to the latest version of 3.10 Python\nand delete current Python and \"venv\" folder in WebUI's directory.\n\nYou can download 3.10 Python from here: https://www.python.org/downloads/release/python-3106/\n\n{\"Alternatively, use a binary release of WebUI: https://github.com/AUTOMATIC1111/stable-diffusion-webui/releases/tag/v1.0.0-pre\" if is_windows else \"\"}\n\nUse --skip-python-version-check to suppress this warning.\n\"\"\")\n\n\n@lru_cache()\ndef commit_hash():\n    try:\n        return subprocess.check_output([git, \"-C\", script_path, \"rev-parse\", \"HEAD\"], shell=False, encoding='utf8').strip()\n    except Exception:\n        return \"<none>\"\n\n\n@lru_cache()\ndef git_tag():\n    try:\n        return subprocess.check_output([git, \"-C\", script_path, \"describe\", \"--tags\"], shell=False, encoding='utf8').strip()\n    except Exception:\n        try:\n\n            changelog_md = os.path.join(os.path.dirname(os.path.dirname(__file__)), \"CHANGELOG.md\")\n            with open(changelog_md, \"r\", encoding=\"utf-8\") as file:\n                line = next((line.strip() for line in file if line.strip()), \"<none>\")\n                line = line.replace(\"## \", \"\")\n                return line\n        except Exception:\n            return \"<none>\"\n\n\ndef run(command, desc=None, errdesc=None, custom_env=None, live: bool = default_command_live) -> str:\n    if desc is not None:\n        print(desc)\n\n    run_kwargs = {\n        \"args\": command,\n        \"shell\": True,\n        \"env\": os.environ if custom_env is None else custom_env,\n        \"encoding\": 'utf8',\n        \"errors\": 'ignore',\n    }\n\n    if not live:\n        run_kwargs[\"stdout\"] = run_kwargs[\"stderr\"] = subprocess.PIPE\n\n    result = subprocess.run(**run_kwargs)\n\n    if result.returncode != 0:\n        error_bits = [\n            f\"{errdesc or 'Error running command'}.\",\n            f\"Command: {command}\",\n            f\"Error code: {result.returncode}\",\n        ]\n        if result.stdout:\n            error_bits.append(f\"stdout: {result.stdout}\")\n        if result.stderr:\n            error_bits.append(f\"stderr: {result.stderr}\")\n        raise RuntimeError(\"\\n\".join(error_bits))\n\n    return (result.stdout or \"\")\n\n\ndef is_installed(package):\n    try:\n        dist = importlib.metadata.distribution(package)\n    except importlib.metadata.PackageNotFoundError:\n        try:\n            spec = importlib.util.find_spec(package)\n        except ModuleNotFoundError:\n            return False\n\n        return spec is not None\n\n    return dist is not None\n\n\ndef repo_dir(name):\n    return os.path.join(script_path, dir_repos, name)\n\n\ndef run_pip(command, desc=None, live=default_command_live):\n    if args.skip_install:\n        return\n\n    index_url_line = f' --index-url {index_url}' if index_url != '' else ''\n    return run(f'\"{python}\" -m pip {command} --prefer-binary{index_url_line}', desc=f\"Installing {desc}\", errdesc=f\"Couldn't install {desc}\", live=live)\n\n\ndef check_run_python(code: str) -> bool:\n    result = subprocess.run([python, \"-c\", code], capture_output=True, shell=False)\n    return result.returncode == 0\n\n\ndef git_fix_workspace(dir, name):\n    run(f'\"{git}\" -C \"{dir}\" fetch --refetch --no-auto-gc', f\"Fetching all contents for {name}\", f\"Couldn't fetch {name}\", live=True)\n    run(f'\"{git}\" -C \"{dir}\" gc --aggressive --prune=now', f\"Pruning {name}\", f\"Couldn't prune {name}\", live=True)\n    return\n\n\ndef run_git(dir, name, command, desc=None, errdesc=None, custom_env=None, live: bool = default_command_live, autofix=True):\n    try:\n        return run(f'\"{git}\" -C \"{dir}\" {command}', desc=desc, errdesc=errdesc, custom_env=custom_env, live=live)\n    except RuntimeError:\n        if not autofix:\n            raise\n\n    print(f\"{errdesc}, attempting autofix...\")\n    git_fix_workspace(dir, name)\n\n    return run(f'\"{git}\" -C \"{dir}\" {command}', desc=desc, errdesc=errdesc, custom_env=custom_env, live=live)\n\n\ndef git_clone(url, dir, name, commithash=None):\n    # TODO clone into temporary dir and move if successful\n\n    if os.path.exists(dir):\n        if commithash is None:\n            return\n\n        current_hash = run_git(dir, name, 'rev-parse HEAD', None, f\"Couldn't determine {name}'s hash: {commithash}\", live=False).strip()\n        if current_hash == commithash:\n            return\n\n        if run_git(dir, name, 'config --get remote.origin.url', None, f\"Couldn't determine {name}'s origin URL\", live=False).strip() != url:\n            run_git(dir, name, f'remote set-url origin \"{url}\"', None, f\"Failed to set {name}'s origin URL\", live=False)\n\n        run_git(dir, name, 'fetch', f\"Fetching updates for {name}...\", f\"Couldn't fetch {name}\", autofix=False)\n\n        run_git(dir, name, f'checkout {commithash}', f\"Checking out commit for {name} with hash: {commithash}...\", f\"Couldn't checkout commit {commithash} for {name}\", live=True)\n\n        return\n\n    try:\n        run(f'\"{git}\" clone --config core.filemode=false \"{url}\" \"{dir}\"', f\"Cloning {name} into {dir}...\", f\"Couldn't clone {name}\", live=True)\n    except RuntimeError:\n        shutil.rmtree(dir, ignore_errors=True)\n        raise\n\n    if commithash is not None:\n        run(f'\"{git}\" -C \"{dir}\" checkout {commithash}', None, \"Couldn't checkout {name}'s hash: {commithash}\")\n\n\ndef git_pull_recursive(dir):\n    for subdir, _, _ in os.walk(dir):\n        if os.path.exists(os.path.join(subdir, '.git')):\n            try:\n                output = subprocess.check_output([git, '-C', subdir, 'pull', '--autostash'])\n                print(f\"Pulled changes for repository in '{subdir}':\\n{output.decode('utf-8').strip()}\\n\")\n            except subprocess.CalledProcessError as e:\n                print(f\"Couldn't perform 'git pull' on repository in '{subdir}':\\n{e.output.decode('utf-8').strip()}\\n\")\n\n\ndef version_check(commit):\n    try:\n        import requests\n        commits = requests.get('https://api.github.com/repos/AUTOMATIC1111/stable-diffusion-webui/branches/master').json()\n        if commit != \"<none>\" and commits['commit']['sha'] != commit:\n            print(\"--------------------------------------------------------\")\n            print(\"| You are not up to date with the most recent release. |\")\n            print(\"| Consider running `git pull` to update.               |\")\n            print(\"--------------------------------------------------------\")\n        elif commits['commit']['sha'] == commit:\n            print(\"You are up to date with the most recent release.\")\n        else:\n            print(\"Not a git clone, can't perform version check.\")\n    except Exception as e:\n        print(\"version check failed\", e)\n\n\ndef run_extension_installer(extension_dir):\n    path_installer = os.path.join(extension_dir, \"install.py\")\n    if not os.path.isfile(path_installer):\n        return\n\n    try:\n        env = os.environ.copy()\n        env['PYTHONPATH'] = f\"{os.path.abspath('.')}{os.pathsep}{env.get('PYTHONPATH', '')}\"\n\n        stdout = run(f'\"{python}\" \"{path_installer}\"', errdesc=f\"Error running install.py for extension {extension_dir}\", custom_env=env).strip()\n        if stdout:\n            print(stdout)\n    except Exception as e:\n        errors.report(str(e))\n\n\ndef list_extensions(settings_file):\n    settings = {}\n\n    try:\n        with open(settings_file, \"r\", encoding=\"utf8\") as file:\n            settings = json.load(file)\n    except FileNotFoundError:\n        pass\n    except Exception:\n        errors.report(f'\\nCould not load settings\\nThe config file \"{settings_file}\" is likely corrupted\\nIt has been moved to the \"tmp/config.json\"\\nReverting config to default\\n\\n''', exc_info=True)\n        os.replace(settings_file, os.path.join(script_path, \"tmp\", \"config.json\"))\n\n    disabled_extensions = set(settings.get('disabled_extensions', []))\n    disable_all_extensions = settings.get('disable_all_extensions', 'none')\n\n    if disable_all_extensions != 'none' or args.disable_extra_extensions or args.disable_all_extensions or not os.path.isdir(extensions_dir):\n        return []\n\n    return [x for x in os.listdir(extensions_dir) if x not in disabled_extensions]\n\n\ndef run_extensions_installers(settings_file):\n    if not os.path.isdir(extensions_dir):\n        return\n\n    with startup_timer.subcategory(\"run extensions installers\"):\n        for dirname_extension in list_extensions(settings_file):\n            logging.debug(f\"Installing {dirname_extension}\")\n\n            path = os.path.join(extensions_dir, dirname_extension)\n\n            if os.path.isdir(path):\n                run_extension_installer(path)\n                startup_timer.record(dirname_extension)\n\n\nre_requirement = re.compile(r\"\\s*([-_a-zA-Z0-9]+)\\s*(?:==\\s*([-+_.a-zA-Z0-9]+))?\\s*\")\n\n\ndef requirements_met(requirements_file):\n    \"\"\"\n    Does a simple parse of a requirements.txt file to determine if all rerqirements in it\n    are already installed. Returns True if so, False if not installed or parsing fails.\n    \"\"\"\n\n    import importlib.metadata\n    import packaging.version\n\n    with open(requirements_file, \"r\", encoding=\"utf8\") as file:\n        for line in file:\n            if line.strip() == \"\":\n                continue\n\n            m = re.match(re_requirement, line)\n            if m is None:\n                return False\n\n            package = m.group(1).strip()\n            version_required = (m.group(2) or \"\").strip()\n\n            if version_required == \"\":\n                continue\n\n            try:\n                version_installed = importlib.metadata.version(package)\n            except Exception:\n                return False\n\n            if packaging.version.parse(version_required) != packaging.version.parse(version_installed):\n                return False\n\n    return True\n\n\ndef prepare_environment():\n    torch_index_url = os.environ.get('TORCH_INDEX_URL', \"https://download.pytorch.org/whl/cu121\")\n    torch_command = os.environ.get('TORCH_COMMAND', f\"pip install torch==2.1.2 torchvision==0.16.2 --extra-index-url {torch_index_url}\")\n    if args.use_ipex:\n        if platform.system() == \"Windows\":\n            # The \"Nuullll/intel-extension-for-pytorch\" wheels were built from IPEX source for Intel Arc GPU: https://github.com/intel/intel-extension-for-pytorch/tree/xpu-main\n            # This is NOT an Intel official release so please use it at your own risk!!\n            # See https://github.com/Nuullll/intel-extension-for-pytorch/releases/tag/v2.0.110%2Bxpu-master%2Bdll-bundle for details.\n            #\n            # Strengths (over official IPEX 2.0.110 windows release):\n            #   - AOT build (for Arc GPU only) to eliminate JIT compilation overhead: https://github.com/intel/intel-extension-for-pytorch/issues/399\n            #   - Bundles minimal oneAPI 2023.2 dependencies into the python wheels, so users don't need to install oneAPI for the whole system.\n            #   - Provides a compatible torchvision wheel: https://github.com/intel/intel-extension-for-pytorch/issues/465\n            # Limitation:\n            #   - Only works for python 3.10\n            url_prefix = \"https://github.com/Nuullll/intel-extension-for-pytorch/releases/download/v2.0.110%2Bxpu-master%2Bdll-bundle\"\n            torch_command = os.environ.get('TORCH_COMMAND', f\"pip install {url_prefix}/torch-2.0.0a0+gite9ebda2-cp310-cp310-win_amd64.whl {url_prefix}/torchvision-0.15.2a0+fa99a53-cp310-cp310-win_amd64.whl {url_prefix}/intel_extension_for_pytorch-2.0.110+gitc6ea20b-cp310-cp310-win_amd64.whl\")\n        else:\n            # Using official IPEX release for linux since it's already an AOT build.\n            # However, users still have to install oneAPI toolkit and activate oneAPI environment manually.\n            # See https://intel.github.io/intel-extension-for-pytorch/index.html#installation for details.\n            torch_index_url = os.environ.get('TORCH_INDEX_URL', \"https://pytorch-extension.intel.com/release-whl/stable/xpu/us/\")\n            torch_command = os.environ.get('TORCH_COMMAND', f\"pip install torch==2.0.0a0 intel-extension-for-pytorch==2.0.110+gitba7f6c1 --extra-index-url {torch_index_url}\")\n    requirements_file = os.environ.get('REQS_FILE', \"requirements_versions.txt\")\n    requirements_file_for_npu = os.environ.get('REQS_FILE_FOR_NPU', \"requirements_npu.txt\")\n\n    xformers_package = os.environ.get('XFORMERS_PACKAGE', 'xformers==0.0.23.post1')\n    clip_package = os.environ.get('CLIP_PACKAGE', \"https://github.com/openai/CLIP/archive/d50d76daa670286dd6cacf3bcd80b5e4823fc8e1.zip\")\n    openclip_package = os.environ.get('OPENCLIP_PACKAGE', \"https://github.com/mlfoundations/open_clip/archive/bb6e834e9c70d9c27d0dc3ecedeebeaeb1ffad6b.zip\")\n\n    assets_repo = os.environ.get('ASSETS_REPO', \"https://github.com/AUTOMATIC1111/stable-diffusion-webui-assets.git\")\n    stable_diffusion_repo = os.environ.get('STABLE_DIFFUSION_REPO', \"https://github.com/Stability-AI/stablediffusion.git\")\n    stable_diffusion_xl_repo = os.environ.get('STABLE_DIFFUSION_XL_REPO', \"https://github.com/Stability-AI/generative-models.git\")\n    k_diffusion_repo = os.environ.get('K_DIFFUSION_REPO', 'https://github.com/crowsonkb/k-diffusion.git')\n    blip_repo = os.environ.get('BLIP_REPO', 'https://github.com/salesforce/BLIP.git')\n\n    assets_commit_hash = os.environ.get('ASSETS_COMMIT_HASH', \"6f7db241d2f8ba7457bac5ca9753331f0c266917\")\n    stable_diffusion_commit_hash = os.environ.get('STABLE_DIFFUSION_COMMIT_HASH', \"cf1d67a6fd5ea1aa600c4df58e5b47da45f6bdbf\")\n    stable_diffusion_xl_commit_hash = os.environ.get('STABLE_DIFFUSION_XL_COMMIT_HASH', \"45c443b316737a4ab6e40413d7794a7f5657c19f\")\n    k_diffusion_commit_hash = os.environ.get('K_DIFFUSION_COMMIT_HASH', \"ab527a9a6d347f364e3d185ba6d714e22d80cb3c\")\n    blip_commit_hash = os.environ.get('BLIP_COMMIT_HASH', \"48211a1594f1321b00f14c9f7a5b4813144b2fb9\")\n\n    try:\n        # the existence of this file is a signal to webui.sh/bat that webui needs to be restarted when it stops execution\n        os.remove(os.path.join(script_path, \"tmp\", \"restart\"))\n        os.environ.setdefault('SD_WEBUI_RESTARTING', '1')\n    except OSError:\n        pass\n\n    if not args.skip_python_version_check:\n        check_python_version()\n\n    startup_timer.record(\"checks\")\n\n    commit = commit_hash()\n    tag = git_tag()\n    startup_timer.record(\"git version info\")\n\n    print(f\"Python {sys.version}\")\n    print(f\"Version: {tag}\")\n    print(f\"Commit hash: {commit}\")\n\n    if args.reinstall_torch or not is_installed(\"torch\") or not is_installed(\"torchvision\"):\n        run(f'\"{python}\" -m {torch_command}', \"Installing torch and torchvision\", \"Couldn't install torch\", live=True)\n        startup_timer.record(\"install torch\")\n\n    if args.use_ipex:\n        args.skip_torch_cuda_test = True\n    if not args.skip_torch_cuda_test and not check_run_python(\"import torch; assert torch.cuda.is_available()\"):\n        raise RuntimeError(\n            'Torch is not able to use GPU; '\n            'add --skip-torch-cuda-test to COMMANDLINE_ARGS variable to disable this check'\n        )\n    startup_timer.record(\"torch GPU test\")\n\n    if not is_installed(\"clip\"):\n        run_pip(f\"install {clip_package}\", \"clip\")\n        startup_timer.record(\"install clip\")\n\n    if not is_installed(\"open_clip\"):\n        run_pip(f\"install {openclip_package}\", \"open_clip\")\n        startup_timer.record(\"install open_clip\")\n\n    if (not is_installed(\"xformers\") or args.reinstall_xformers) and args.xformers:\n        run_pip(f\"install -U -I --no-deps {xformers_package}\", \"xformers\")\n        startup_timer.record(\"install xformers\")\n\n    if not is_installed(\"ngrok\") and args.ngrok:\n        run_pip(\"install ngrok\", \"ngrok\")\n        startup_timer.record(\"install ngrok\")\n\n    os.makedirs(os.path.join(script_path, dir_repos), exist_ok=True)\n\n    git_clone(assets_repo, repo_dir('stable-diffusion-webui-assets'), \"assets\", assets_commit_hash)\n    git_clone(stable_diffusion_repo, repo_dir('stable-diffusion-stability-ai'), \"Stable Diffusion\", stable_diffusion_commit_hash)\n    git_clone(stable_diffusion_xl_repo, repo_dir('generative-models'), \"Stable Diffusion XL\", stable_diffusion_xl_commit_hash)\n    git_clone(k_diffusion_repo, repo_dir('k-diffusion'), \"K-diffusion\", k_diffusion_commit_hash)\n    git_clone(blip_repo, repo_dir('BLIP'), \"BLIP\", blip_commit_hash)\n\n    startup_timer.record(\"clone repositores\")\n\n    if not os.path.isfile(requirements_file):\n        requirements_file = os.path.join(script_path, requirements_file)\n\n    if not requirements_met(requirements_file):\n        run_pip(f\"install -r \\\"{requirements_file}\\\"\", \"requirements\")\n        startup_timer.record(\"install requirements\")\n\n    if not os.path.isfile(requirements_file_for_npu):\n        requirements_file_for_npu = os.path.join(script_path, requirements_file_for_npu)\n\n    if \"torch_npu\" in torch_command and not requirements_met(requirements_file_for_npu):\n        run_pip(f\"install -r \\\"{requirements_file_for_npu}\\\"\", \"requirements_for_npu\")\n        startup_timer.record(\"install requirements_for_npu\")\n\n    if not args.skip_install:\n        run_extensions_installers(settings_file=args.ui_settings_file)\n\n    if args.update_check:\n        version_check(commit)\n        startup_timer.record(\"check version\")\n\n    if args.update_all_extensions:\n        git_pull_recursive(extensions_dir)\n        startup_timer.record(\"update extensions\")\n\n    if \"--exit\" in sys.argv:\n        print(\"Exiting because of --exit argument\")\n        exit(0)\n\n\n\ndef configure_for_tests():\n    if \"--api\" not in sys.argv:\n        sys.argv.append(\"--api\")\n    if \"--ckpt\" not in sys.argv:\n        sys.argv.append(\"--ckpt\")\n        sys.argv.append(os.path.join(script_path, \"test/test_files/empty.pt\"))\n    if \"--skip-torch-cuda-test\" not in sys.argv:\n        sys.argv.append(\"--skip-torch-cuda-test\")\n    if \"--disable-nan-check\" not in sys.argv:\n        sys.argv.append(\"--disable-nan-check\")\n\n    os.environ['COMMANDLINE_ARGS'] = \"\"\n\n\ndef start():\n    print(f\"Launching {'API server' if '--nowebui' in sys.argv else 'Web UI'} with arguments: {' '.join(sys.argv[1:])}\")\n    import webui\n    if '--nowebui' in sys.argv:\n        webui.api_only()\n    else:\n        webui.webui()\n\n\ndef dump_sysinfo():\n    from modules import sysinfo\n    import datetime\n\n    text = sysinfo.get()\n    filename = f\"sysinfo-{datetime.datetime.utcnow().strftime('%Y-%m-%d-%H-%M')}.json\"\n\n    with open(filename, \"w\", encoding=\"utf8\") as file:\n        file.write(text)\n\n    return filename\n", "modules/shared_total_tqdm.py": "import tqdm\n\nfrom modules import shared\n\n\nclass TotalTQDM:\n    def __init__(self):\n        self._tqdm = None\n\n    def reset(self):\n        self._tqdm = tqdm.tqdm(\n            desc=\"Total progress\",\n            total=shared.state.job_count * shared.state.sampling_steps,\n            position=1,\n            file=shared.progress_print_out\n        )\n\n    def update(self):\n        if not shared.opts.multiple_tqdm or shared.cmd_opts.disable_console_progressbars:\n            return\n        if self._tqdm is None:\n            self.reset()\n        self._tqdm.update()\n\n    def updateTotal(self, new_total):\n        if not shared.opts.multiple_tqdm or shared.cmd_opts.disable_console_progressbars:\n            return\n        if self._tqdm is None:\n            self.reset()\n        self._tqdm.total = new_total\n\n    def clear(self):\n        if self._tqdm is not None:\n            self._tqdm.refresh()\n            self._tqdm.close()\n            self._tqdm = None\n\n", "modules/script_callbacks.py": "from __future__ import annotations\n\nimport dataclasses\nimport inspect\nimport os\nfrom typing import Optional, Any\n\nfrom fastapi import FastAPI\nfrom gradio import Blocks\n\nfrom modules import errors, timer, extensions, shared, util\n\n\ndef report_exception(c, job):\n    errors.report(f\"Error executing callback {job} for {c.script}\", exc_info=True)\n\n\nclass ImageSaveParams:\n    def __init__(self, image, p, filename, pnginfo):\n        self.image = image\n        \"\"\"the PIL image itself\"\"\"\n\n        self.p = p\n        \"\"\"p object with processing parameters; either StableDiffusionProcessing or an object with same fields\"\"\"\n\n        self.filename = filename\n        \"\"\"name of file that the image would be saved to\"\"\"\n\n        self.pnginfo = pnginfo\n        \"\"\"dictionary with parameters for image's PNG info data; infotext will have the key 'parameters'\"\"\"\n\n\nclass ExtraNoiseParams:\n    def __init__(self, noise, x, xi):\n        self.noise = noise\n        \"\"\"Random noise generated by the seed\"\"\"\n\n        self.x = x\n        \"\"\"Latent representation of the image\"\"\"\n\n        self.xi = xi\n        \"\"\"Noisy latent representation of the image\"\"\"\n\n\nclass CFGDenoiserParams:\n    def __init__(self, x, image_cond, sigma, sampling_step, total_sampling_steps, text_cond, text_uncond, denoiser=None):\n        self.x = x\n        \"\"\"Latent image representation in the process of being denoised\"\"\"\n\n        self.image_cond = image_cond\n        \"\"\"Conditioning image\"\"\"\n\n        self.sigma = sigma\n        \"\"\"Current sigma noise step value\"\"\"\n\n        self.sampling_step = sampling_step\n        \"\"\"Current Sampling step number\"\"\"\n\n        self.total_sampling_steps = total_sampling_steps\n        \"\"\"Total number of sampling steps planned\"\"\"\n\n        self.text_cond = text_cond\n        \"\"\" Encoder hidden states of text conditioning from prompt\"\"\"\n\n        self.text_uncond = text_uncond\n        \"\"\" Encoder hidden states of text conditioning from negative prompt\"\"\"\n\n        self.denoiser = denoiser\n        \"\"\"Current CFGDenoiser object with processing parameters\"\"\"\n\n\nclass CFGDenoisedParams:\n    def __init__(self, x, sampling_step, total_sampling_steps, inner_model):\n        self.x = x\n        \"\"\"Latent image representation in the process of being denoised\"\"\"\n\n        self.sampling_step = sampling_step\n        \"\"\"Current Sampling step number\"\"\"\n\n        self.total_sampling_steps = total_sampling_steps\n        \"\"\"Total number of sampling steps planned\"\"\"\n\n        self.inner_model = inner_model\n        \"\"\"Inner model reference used for denoising\"\"\"\n\n\nclass AfterCFGCallbackParams:\n    def __init__(self, x, sampling_step, total_sampling_steps):\n        self.x = x\n        \"\"\"Latent image representation in the process of being denoised\"\"\"\n\n        self.sampling_step = sampling_step\n        \"\"\"Current Sampling step number\"\"\"\n\n        self.total_sampling_steps = total_sampling_steps\n        \"\"\"Total number of sampling steps planned\"\"\"\n\n\nclass UiTrainTabParams:\n    def __init__(self, txt2img_preview_params):\n        self.txt2img_preview_params = txt2img_preview_params\n\n\nclass ImageGridLoopParams:\n    def __init__(self, imgs, cols, rows):\n        self.imgs = imgs\n        self.cols = cols\n        self.rows = rows\n\n\n@dataclasses.dataclass\nclass BeforeTokenCounterParams:\n    prompt: str\n    steps: int\n    styles: list\n\n    is_positive: bool = True\n\n\n@dataclasses.dataclass\nclass ScriptCallback:\n    script: str\n    callback: any\n    name: str = \"unnamed\"\n\n\ndef add_callback(callbacks, fun, *, name=None, category='unknown', filename=None):\n    if filename is None:\n        stack = [x for x in inspect.stack() if x.filename != __file__]\n        filename = stack[0].filename if stack else 'unknown file'\n\n    extension = extensions.find_extension(filename)\n    extension_name = extension.canonical_name if extension else 'base'\n\n    callback_name = f\"{extension_name}/{os.path.basename(filename)}/{category}\"\n    if name is not None:\n        callback_name += f'/{name}'\n\n    unique_callback_name = callback_name\n    for index in range(1000):\n        existing = any(x.name == unique_callback_name for x in callbacks)\n        if not existing:\n            break\n\n        unique_callback_name = f'{callback_name}-{index+1}'\n\n    callbacks.append(ScriptCallback(filename, fun, unique_callback_name))\n\n\ndef sort_callbacks(category, unordered_callbacks, *, enable_user_sort=True):\n    callbacks = unordered_callbacks.copy()\n    callback_lookup = {x.name: x for x in callbacks}\n    dependencies = {}\n\n    order_instructions = {}\n    for extension in extensions.extensions:\n        for order_instruction in extension.metadata.list_callback_order_instructions():\n            if order_instruction.name in callback_lookup:\n                if order_instruction.name not in order_instructions:\n                    order_instructions[order_instruction.name] = []\n\n                order_instructions[order_instruction.name].append(order_instruction)\n\n    if order_instructions:\n        for callback in callbacks:\n            dependencies[callback.name] = []\n\n        for callback in callbacks:\n            for order_instruction in order_instructions.get(callback.name, []):\n                for after in order_instruction.after:\n                    if after not in callback_lookup:\n                        continue\n\n                    dependencies[callback.name].append(after)\n\n                for before in order_instruction.before:\n                    if before not in callback_lookup:\n                        continue\n\n                    dependencies[before].append(callback.name)\n\n        sorted_names = util.topological_sort(dependencies)\n        callbacks = [callback_lookup[x] for x in sorted_names]\n\n    if enable_user_sort:\n        for name in reversed(getattr(shared.opts, 'prioritized_callbacks_' + category, [])):\n            index = next((i for i, callback in enumerate(callbacks) if callback.name == name), None)\n            if index is not None:\n                callbacks.insert(0, callbacks.pop(index))\n\n    return callbacks\n\n\ndef ordered_callbacks(category, unordered_callbacks=None, *, enable_user_sort=True):\n    if unordered_callbacks is None:\n        unordered_callbacks = callback_map.get('callbacks_' + category, [])\n\n    if not enable_user_sort:\n        return sort_callbacks(category, unordered_callbacks, enable_user_sort=False)\n\n    callbacks = ordered_callbacks_map.get(category)\n    if callbacks is not None and len(callbacks) == len(unordered_callbacks):\n        return callbacks\n\n    callbacks = sort_callbacks(category, unordered_callbacks)\n\n    ordered_callbacks_map[category] = callbacks\n    return callbacks\n\n\ndef enumerate_callbacks():\n    for category, callbacks in callback_map.items():\n        if category.startswith('callbacks_'):\n            category = category[10:]\n\n        yield category, callbacks\n\n\ncallback_map = dict(\n    callbacks_app_started=[],\n    callbacks_model_loaded=[],\n    callbacks_ui_tabs=[],\n    callbacks_ui_train_tabs=[],\n    callbacks_ui_settings=[],\n    callbacks_before_image_saved=[],\n    callbacks_image_saved=[],\n    callbacks_extra_noise=[],\n    callbacks_cfg_denoiser=[],\n    callbacks_cfg_denoised=[],\n    callbacks_cfg_after_cfg=[],\n    callbacks_before_component=[],\n    callbacks_after_component=[],\n    callbacks_image_grid=[],\n    callbacks_infotext_pasted=[],\n    callbacks_script_unloaded=[],\n    callbacks_before_ui=[],\n    callbacks_on_reload=[],\n    callbacks_list_optimizers=[],\n    callbacks_list_unets=[],\n    callbacks_before_token_counter=[],\n)\n\nordered_callbacks_map = {}\n\n\ndef clear_callbacks():\n    for callback_list in callback_map.values():\n        callback_list.clear()\n\n    ordered_callbacks_map.clear()\n\n\ndef app_started_callback(demo: Optional[Blocks], app: FastAPI):\n    for c in ordered_callbacks('app_started'):\n        try:\n            c.callback(demo, app)\n            timer.startup_timer.record(os.path.basename(c.script))\n        except Exception:\n            report_exception(c, 'app_started_callback')\n\n\ndef app_reload_callback():\n    for c in ordered_callbacks('on_reload'):\n        try:\n            c.callback()\n        except Exception:\n            report_exception(c, 'callbacks_on_reload')\n\n\ndef model_loaded_callback(sd_model):\n    for c in ordered_callbacks('model_loaded'):\n        try:\n            c.callback(sd_model)\n        except Exception:\n            report_exception(c, 'model_loaded_callback')\n\n\ndef ui_tabs_callback():\n    res = []\n\n    for c in ordered_callbacks('ui_tabs'):\n        try:\n            res += c.callback() or []\n        except Exception:\n            report_exception(c, 'ui_tabs_callback')\n\n    return res\n\n\ndef ui_train_tabs_callback(params: UiTrainTabParams):\n    for c in ordered_callbacks('ui_train_tabs'):\n        try:\n            c.callback(params)\n        except Exception:\n            report_exception(c, 'callbacks_ui_train_tabs')\n\n\ndef ui_settings_callback():\n    for c in ordered_callbacks('ui_settings'):\n        try:\n            c.callback()\n        except Exception:\n            report_exception(c, 'ui_settings_callback')\n\n\ndef before_image_saved_callback(params: ImageSaveParams):\n    for c in ordered_callbacks('before_image_saved'):\n        try:\n            c.callback(params)\n        except Exception:\n            report_exception(c, 'before_image_saved_callback')\n\n\ndef image_saved_callback(params: ImageSaveParams):\n    for c in ordered_callbacks('image_saved'):\n        try:\n            c.callback(params)\n        except Exception:\n            report_exception(c, 'image_saved_callback')\n\n\ndef extra_noise_callback(params: ExtraNoiseParams):\n    for c in ordered_callbacks('extra_noise'):\n        try:\n            c.callback(params)\n        except Exception:\n            report_exception(c, 'callbacks_extra_noise')\n\n\ndef cfg_denoiser_callback(params: CFGDenoiserParams):\n    for c in ordered_callbacks('cfg_denoiser'):\n        try:\n            c.callback(params)\n        except Exception:\n            report_exception(c, 'cfg_denoiser_callback')\n\n\ndef cfg_denoised_callback(params: CFGDenoisedParams):\n    for c in ordered_callbacks('cfg_denoised'):\n        try:\n            c.callback(params)\n        except Exception:\n            report_exception(c, 'cfg_denoised_callback')\n\n\ndef cfg_after_cfg_callback(params: AfterCFGCallbackParams):\n    for c in ordered_callbacks('cfg_after_cfg'):\n        try:\n            c.callback(params)\n        except Exception:\n            report_exception(c, 'cfg_after_cfg_callback')\n\n\ndef before_component_callback(component, **kwargs):\n    for c in ordered_callbacks('before_component'):\n        try:\n            c.callback(component, **kwargs)\n        except Exception:\n            report_exception(c, 'before_component_callback')\n\n\ndef after_component_callback(component, **kwargs):\n    for c in ordered_callbacks('after_component'):\n        try:\n            c.callback(component, **kwargs)\n        except Exception:\n            report_exception(c, 'after_component_callback')\n\n\ndef image_grid_callback(params: ImageGridLoopParams):\n    for c in ordered_callbacks('image_grid'):\n        try:\n            c.callback(params)\n        except Exception:\n            report_exception(c, 'image_grid')\n\n\ndef infotext_pasted_callback(infotext: str, params: dict[str, Any]):\n    for c in ordered_callbacks('infotext_pasted'):\n        try:\n            c.callback(infotext, params)\n        except Exception:\n            report_exception(c, 'infotext_pasted')\n\n\ndef script_unloaded_callback():\n    for c in reversed(ordered_callbacks('script_unloaded')):\n        try:\n            c.callback()\n        except Exception:\n            report_exception(c, 'script_unloaded')\n\n\ndef before_ui_callback():\n    for c in reversed(ordered_callbacks('before_ui')):\n        try:\n            c.callback()\n        except Exception:\n            report_exception(c, 'before_ui')\n\n\ndef list_optimizers_callback():\n    res = []\n\n    for c in ordered_callbacks('list_optimizers'):\n        try:\n            c.callback(res)\n        except Exception:\n            report_exception(c, 'list_optimizers')\n\n    return res\n\n\ndef list_unets_callback():\n    res = []\n\n    for c in ordered_callbacks('list_unets'):\n        try:\n            c.callback(res)\n        except Exception:\n            report_exception(c, 'list_unets')\n\n    return res\n\n\ndef before_token_counter_callback(params: BeforeTokenCounterParams):\n    for c in ordered_callbacks('before_token_counter'):\n        try:\n            c.callback(params)\n        except Exception:\n            report_exception(c, 'before_token_counter')\n\n\ndef remove_current_script_callbacks():\n    stack = [x for x in inspect.stack() if x.filename != __file__]\n    filename = stack[0].filename if stack else 'unknown file'\n    if filename == 'unknown file':\n        return\n    for callback_list in callback_map.values():\n        for callback_to_remove in [cb for cb in callback_list if cb.script == filename]:\n            callback_list.remove(callback_to_remove)\n    for ordered_callbacks_list in ordered_callbacks_map.values():\n        for callback_to_remove in [cb for cb in ordered_callbacks_list if cb.script == filename]:\n            ordered_callbacks_list.remove(callback_to_remove)\n\n\ndef remove_callbacks_for_function(callback_func):\n    for callback_list in callback_map.values():\n        for callback_to_remove in [cb for cb in callback_list if cb.callback == callback_func]:\n            callback_list.remove(callback_to_remove)\n    for ordered_callback_list in ordered_callbacks_map.values():\n        for callback_to_remove in [cb for cb in ordered_callback_list if cb.callback == callback_func]:\n            ordered_callback_list.remove(callback_to_remove)\n\n\ndef on_app_started(callback, *, name=None):\n    \"\"\"register a function to be called when the webui started, the gradio `Block` component and\n    fastapi `FastAPI` object are passed as the arguments\"\"\"\n    add_callback(callback_map['callbacks_app_started'], callback, name=name, category='app_started')\n\n\ndef on_before_reload(callback, *, name=None):\n    \"\"\"register a function to be called just before the server reloads.\"\"\"\n    add_callback(callback_map['callbacks_on_reload'], callback, name=name, category='on_reload')\n\n\ndef on_model_loaded(callback, *, name=None):\n    \"\"\"register a function to be called when the stable diffusion model is created; the model is\n    passed as an argument; this function is also called when the script is reloaded. \"\"\"\n    add_callback(callback_map['callbacks_model_loaded'], callback, name=name, category='model_loaded')\n\n\ndef on_ui_tabs(callback, *, name=None):\n    \"\"\"register a function to be called when the UI is creating new tabs.\n    The function must either return a None, which means no new tabs to be added, or a list, where\n    each element is a tuple:\n        (gradio_component, title, elem_id)\n\n    gradio_component is a gradio component to be used for contents of the tab (usually gr.Blocks)\n    title is tab text displayed to user in the UI\n    elem_id is HTML id for the tab\n    \"\"\"\n    add_callback(callback_map['callbacks_ui_tabs'], callback, name=name, category='ui_tabs')\n\n\ndef on_ui_train_tabs(callback, *, name=None):\n    \"\"\"register a function to be called when the UI is creating new tabs for the train tab.\n    Create your new tabs with gr.Tab.\n    \"\"\"\n    add_callback(callback_map['callbacks_ui_train_tabs'], callback, name=name, category='ui_train_tabs')\n\n\ndef on_ui_settings(callback, *, name=None):\n    \"\"\"register a function to be called before UI settings are populated; add your settings\n    by using shared.opts.add_option(shared.OptionInfo(...)) \"\"\"\n    add_callback(callback_map['callbacks_ui_settings'], callback, name=name, category='ui_settings')\n\n\ndef on_before_image_saved(callback, *, name=None):\n    \"\"\"register a function to be called before an image is saved to a file.\n    The callback is called with one argument:\n        - params: ImageSaveParams - parameters the image is to be saved with. You can change fields in this object.\n    \"\"\"\n    add_callback(callback_map['callbacks_before_image_saved'], callback, name=name, category='before_image_saved')\n\n\ndef on_image_saved(callback, *, name=None):\n    \"\"\"register a function to be called after an image is saved to a file.\n    The callback is called with one argument:\n        - params: ImageSaveParams - parameters the image was saved with. Changing fields in this object does nothing.\n    \"\"\"\n    add_callback(callback_map['callbacks_image_saved'], callback, name=name, category='image_saved')\n\n\ndef on_extra_noise(callback, *, name=None):\n    \"\"\"register a function to be called before adding extra noise in img2img or hires fix;\n    The callback is called with one argument:\n        - params: ExtraNoiseParams - contains noise determined by seed and latent representation of image\n    \"\"\"\n    add_callback(callback_map['callbacks_extra_noise'], callback, name=name, category='extra_noise')\n\n\ndef on_cfg_denoiser(callback, *, name=None):\n    \"\"\"register a function to be called in the kdiffussion cfg_denoiser method after building the inner model inputs.\n    The callback is called with one argument:\n        - params: CFGDenoiserParams - parameters to be passed to the inner model and sampling state details.\n    \"\"\"\n    add_callback(callback_map['callbacks_cfg_denoiser'], callback, name=name, category='cfg_denoiser')\n\n\ndef on_cfg_denoised(callback, *, name=None):\n    \"\"\"register a function to be called in the kdiffussion cfg_denoiser method after building the inner model inputs.\n    The callback is called with one argument:\n        - params: CFGDenoisedParams - parameters to be passed to the inner model and sampling state details.\n    \"\"\"\n    add_callback(callback_map['callbacks_cfg_denoised'], callback, name=name, category='cfg_denoised')\n\n\ndef on_cfg_after_cfg(callback, *, name=None):\n    \"\"\"register a function to be called in the kdiffussion cfg_denoiser method after cfg calculations are completed.\n    The callback is called with one argument:\n        - params: AfterCFGCallbackParams - parameters to be passed to the script for post-processing after cfg calculation.\n    \"\"\"\n    add_callback(callback_map['callbacks_cfg_after_cfg'], callback, name=name, category='cfg_after_cfg')\n\n\ndef on_before_component(callback, *, name=None):\n    \"\"\"register a function to be called before a component is created.\n    The callback is called with arguments:\n        - component - gradio component that is about to be created.\n        - **kwargs - args to gradio.components.IOComponent.__init__ function\n\n    Use elem_id/label fields of kwargs to figure out which component it is.\n    This can be useful to inject your own components somewhere in the middle of vanilla UI.\n    \"\"\"\n    add_callback(callback_map['callbacks_before_component'], callback, name=name, category='before_component')\n\n\ndef on_after_component(callback, *, name=None):\n    \"\"\"register a function to be called after a component is created. See on_before_component for more.\"\"\"\n    add_callback(callback_map['callbacks_after_component'], callback, name=name, category='after_component')\n\n\ndef on_image_grid(callback, *, name=None):\n    \"\"\"register a function to be called before making an image grid.\n    The callback is called with one argument:\n       - params: ImageGridLoopParams - parameters to be used for grid creation. Can be modified.\n    \"\"\"\n    add_callback(callback_map['callbacks_image_grid'], callback, name=name, category='image_grid')\n\n\ndef on_infotext_pasted(callback, *, name=None):\n    \"\"\"register a function to be called before applying an infotext.\n    The callback is called with two arguments:\n       - infotext: str - raw infotext.\n       - result: dict[str, any] - parsed infotext parameters.\n    \"\"\"\n    add_callback(callback_map['callbacks_infotext_pasted'], callback, name=name, category='infotext_pasted')\n\n\ndef on_script_unloaded(callback, *, name=None):\n    \"\"\"register a function to be called before the script is unloaded. Any hooks/hijacks/monkeying about that\n    the script did should be reverted here\"\"\"\n\n    add_callback(callback_map['callbacks_script_unloaded'], callback, name=name, category='script_unloaded')\n\n\ndef on_before_ui(callback, *, name=None):\n    \"\"\"register a function to be called before the UI is created.\"\"\"\n\n    add_callback(callback_map['callbacks_before_ui'], callback, name=name, category='before_ui')\n\n\ndef on_list_optimizers(callback, *, name=None):\n    \"\"\"register a function to be called when UI is making a list of cross attention optimization options.\n    The function will be called with one argument, a list, and shall add objects of type modules.sd_hijack_optimizations.SdOptimization\n    to it.\"\"\"\n\n    add_callback(callback_map['callbacks_list_optimizers'], callback, name=name, category='list_optimizers')\n\n\ndef on_list_unets(callback, *, name=None):\n    \"\"\"register a function to be called when UI is making a list of alternative options for unet.\n    The function will be called with one argument, a list, and shall add objects of type modules.sd_unet.SdUnetOption to it.\"\"\"\n\n    add_callback(callback_map['callbacks_list_unets'], callback, name=name, category='list_unets')\n\n\ndef on_before_token_counter(callback, *, name=None):\n    \"\"\"register a function to be called when UI is counting tokens for a prompt.\n    The function will be called with one argument of type BeforeTokenCounterParams, and should modify its fields if necessary.\"\"\"\n\n    add_callback(callback_map['callbacks_before_token_counter'], callback, name=name, category='before_token_counter')\n", "modules/sd_models_types.py": "from ldm.models.diffusion.ddpm import LatentDiffusion\nfrom typing import TYPE_CHECKING\n\n\nif TYPE_CHECKING:\n    from modules.sd_models import CheckpointInfo\n\n\nclass WebuiSdModel(LatentDiffusion):\n    \"\"\"This class is not actually instantinated, but its fields are created and fieeld by webui\"\"\"\n\n    lowvram: bool\n    \"\"\"True if lowvram/medvram optimizations are enabled -- see modules.lowvram for more info\"\"\"\n\n    sd_model_hash: str\n    \"\"\"short hash, 10 first characters of SHA1 hash of the model file; may be None if --no-hashing flag is used\"\"\"\n\n    sd_model_checkpoint: str\n    \"\"\"path to the file on disk that model weights were obtained from\"\"\"\n\n    sd_checkpoint_info: 'CheckpointInfo'\n    \"\"\"structure with additional information about the file with model's weights\"\"\"\n\n    is_sdxl: bool\n    \"\"\"True if the model's architecture is SDXL or SSD\"\"\"\n\n    is_ssd: bool\n    \"\"\"True if the model is SSD\"\"\"\n\n    is_sd2: bool\n    \"\"\"True if the model's architecture is SD 2.x\"\"\"\n\n    is_sd1: bool\n    \"\"\"True if the model's architecture is SD 1.x\"\"\"\n", "modules/sd_models_config.py": "import os\n\nimport torch\n\nfrom modules import shared, paths, sd_disable_initialization, devices\n\nsd_configs_path = shared.sd_configs_path\nsd_repo_configs_path = os.path.join(paths.paths['Stable Diffusion'], \"configs\", \"stable-diffusion\")\nsd_xl_repo_configs_path = os.path.join(paths.paths['Stable Diffusion XL'], \"configs\", \"inference\")\n\n\nconfig_default = shared.sd_default_config\nconfig_sd2 = os.path.join(sd_repo_configs_path, \"v2-inference.yaml\")\nconfig_sd2v = os.path.join(sd_repo_configs_path, \"v2-inference-v.yaml\")\nconfig_sd2_inpainting = os.path.join(sd_repo_configs_path, \"v2-inpainting-inference.yaml\")\nconfig_sdxl = os.path.join(sd_xl_repo_configs_path, \"sd_xl_base.yaml\")\nconfig_sdxl_refiner = os.path.join(sd_xl_repo_configs_path, \"sd_xl_refiner.yaml\")\nconfig_sdxl_inpainting = os.path.join(sd_configs_path, \"sd_xl_inpaint.yaml\")\nconfig_depth_model = os.path.join(sd_repo_configs_path, \"v2-midas-inference.yaml\")\nconfig_unclip = os.path.join(sd_repo_configs_path, \"v2-1-stable-unclip-l-inference.yaml\")\nconfig_unopenclip = os.path.join(sd_repo_configs_path, \"v2-1-stable-unclip-h-inference.yaml\")\nconfig_inpainting = os.path.join(sd_configs_path, \"v1-inpainting-inference.yaml\")\nconfig_instruct_pix2pix = os.path.join(sd_configs_path, \"instruct-pix2pix.yaml\")\nconfig_alt_diffusion = os.path.join(sd_configs_path, \"alt-diffusion-inference.yaml\")\nconfig_alt_diffusion_m18 = os.path.join(sd_configs_path, \"alt-diffusion-m18-inference.yaml\")\n\ndef is_using_v_parameterization_for_sd2(state_dict):\n    \"\"\"\n    Detects whether unet in state_dict is using v-parameterization. Returns True if it is. You're welcome.\n    \"\"\"\n\n    import ldm.modules.diffusionmodules.openaimodel\n\n    device = devices.cpu\n\n    with sd_disable_initialization.DisableInitialization():\n        unet = ldm.modules.diffusionmodules.openaimodel.UNetModel(\n            use_checkpoint=True,\n            use_fp16=False,\n            image_size=32,\n            in_channels=4,\n            out_channels=4,\n            model_channels=320,\n            attention_resolutions=[4, 2, 1],\n            num_res_blocks=2,\n            channel_mult=[1, 2, 4, 4],\n            num_head_channels=64,\n            use_spatial_transformer=True,\n            use_linear_in_transformer=True,\n            transformer_depth=1,\n            context_dim=1024,\n            legacy=False\n        )\n        unet.eval()\n\n    with torch.no_grad():\n        unet_sd = {k.replace(\"model.diffusion_model.\", \"\"): v for k, v in state_dict.items() if \"model.diffusion_model.\" in k}\n        unet.load_state_dict(unet_sd, strict=True)\n        unet.to(device=device, dtype=torch.float)\n\n        test_cond = torch.ones((1, 2, 1024), device=device) * 0.5\n        x_test = torch.ones((1, 4, 8, 8), device=device) * 0.5\n\n        out = (unet(x_test, torch.asarray([999], device=device), context=test_cond) - x_test).mean().item()\n\n    return out < -1\n\n\ndef guess_model_config_from_state_dict(sd, filename):\n    sd2_cond_proj_weight = sd.get('cond_stage_model.model.transformer.resblocks.0.attn.in_proj_weight', None)\n    diffusion_model_input = sd.get('model.diffusion_model.input_blocks.0.0.weight', None)\n    sd2_variations_weight = sd.get('embedder.model.ln_final.weight', None)\n\n    if sd.get('conditioner.embedders.1.model.ln_final.weight', None) is not None:\n        if diffusion_model_input.shape[1] == 9:\n            return config_sdxl_inpainting\n        else:\n            return config_sdxl\n    if sd.get('conditioner.embedders.0.model.ln_final.weight', None) is not None:\n        return config_sdxl_refiner\n    elif sd.get('depth_model.model.pretrained.act_postprocess3.0.project.0.bias', None) is not None:\n        return config_depth_model\n    elif sd2_variations_weight is not None and sd2_variations_weight.shape[0] == 768:\n        return config_unclip\n    elif sd2_variations_weight is not None and sd2_variations_weight.shape[0] == 1024:\n        return config_unopenclip\n\n    if sd2_cond_proj_weight is not None and sd2_cond_proj_weight.shape[1] == 1024:\n        if diffusion_model_input.shape[1] == 9:\n            return config_sd2_inpainting\n        elif is_using_v_parameterization_for_sd2(sd):\n            return config_sd2v\n        else:\n            return config_sd2\n\n    if diffusion_model_input is not None:\n        if diffusion_model_input.shape[1] == 9:\n            return config_inpainting\n        if diffusion_model_input.shape[1] == 8:\n            return config_instruct_pix2pix\n\n\n    if sd.get('cond_stage_model.roberta.embeddings.word_embeddings.weight', None) is not None:\n        if sd.get('cond_stage_model.transformation.weight').size()[0] == 1024:\n            return config_alt_diffusion_m18\n        return config_alt_diffusion\n\n    return config_default\n\n\ndef find_checkpoint_config(state_dict, info):\n    if info is None:\n        return guess_model_config_from_state_dict(state_dict, \"\")\n\n    config = find_checkpoint_config_near_filename(info)\n    if config is not None:\n        return config\n\n    return guess_model_config_from_state_dict(state_dict, info.filename)\n\n\ndef find_checkpoint_config_near_filename(info):\n    if info is None:\n        return None\n\n    config = f\"{os.path.splitext(info.filename)[0]}.yaml\"\n    if os.path.exists(config):\n        return config\n\n    return None\n\n", "modules/sd_models.py": "import collections\nimport os\nimport sys\nimport threading\n\nimport torch\nimport re\nimport safetensors.torch\nfrom omegaconf import OmegaConf, ListConfig\nfrom urllib import request\nimport ldm.modules.midas as midas\n\nfrom ldm.util import instantiate_from_config\n\nfrom modules import paths, shared, modelloader, devices, script_callbacks, sd_vae, sd_disable_initialization, errors, hashes, sd_models_config, sd_unet, sd_models_xl, cache, extra_networks, processing, lowvram, sd_hijack, patches\nfrom modules.timer import Timer\nfrom modules.shared import opts\nimport tomesd\nimport numpy as np\n\nmodel_dir = \"Stable-diffusion\"\nmodel_path = os.path.abspath(os.path.join(paths.models_path, model_dir))\n\ncheckpoints_list = {}\ncheckpoint_aliases = {}\ncheckpoint_alisases = checkpoint_aliases  # for compatibility with old name\ncheckpoints_loaded = collections.OrderedDict()\n\n\ndef replace_key(d, key, new_key, value):\n    keys = list(d.keys())\n\n    d[new_key] = value\n\n    if key not in keys:\n        return d\n\n    index = keys.index(key)\n    keys[index] = new_key\n\n    new_d = {k: d[k] for k in keys}\n\n    d.clear()\n    d.update(new_d)\n    return d\n\n\nclass CheckpointInfo:\n    def __init__(self, filename):\n        self.filename = filename\n        abspath = os.path.abspath(filename)\n        abs_ckpt_dir = os.path.abspath(shared.cmd_opts.ckpt_dir) if shared.cmd_opts.ckpt_dir is not None else None\n\n        self.is_safetensors = os.path.splitext(filename)[1].lower() == \".safetensors\"\n\n        if abs_ckpt_dir and abspath.startswith(abs_ckpt_dir):\n            name = abspath.replace(abs_ckpt_dir, '')\n        elif abspath.startswith(model_path):\n            name = abspath.replace(model_path, '')\n        else:\n            name = os.path.basename(filename)\n\n        if name.startswith(\"\\\\\") or name.startswith(\"/\"):\n            name = name[1:]\n\n        def read_metadata():\n            metadata = read_metadata_from_safetensors(filename)\n            self.modelspec_thumbnail = metadata.pop('modelspec.thumbnail', None)\n\n            return metadata\n\n        self.metadata = {}\n        if self.is_safetensors:\n            try:\n                self.metadata = cache.cached_data_for_file('safetensors-metadata', \"checkpoint/\" + name, filename, read_metadata)\n            except Exception as e:\n                errors.display(e, f\"reading metadata for {filename}\")\n\n        self.name = name\n        self.name_for_extra = os.path.splitext(os.path.basename(filename))[0]\n        self.model_name = os.path.splitext(name.replace(\"/\", \"_\").replace(\"\\\\\", \"_\"))[0]\n        self.hash = model_hash(filename)\n\n        self.sha256 = hashes.sha256_from_cache(self.filename, f\"checkpoint/{name}\")\n        self.shorthash = self.sha256[0:10] if self.sha256 else None\n\n        self.title = name if self.shorthash is None else f'{name} [{self.shorthash}]'\n        self.short_title = self.name_for_extra if self.shorthash is None else f'{self.name_for_extra} [{self.shorthash}]'\n\n        self.ids = [self.hash, self.model_name, self.title, name, self.name_for_extra, f'{name} [{self.hash}]']\n        if self.shorthash:\n            self.ids += [self.shorthash, self.sha256, f'{self.name} [{self.shorthash}]', f'{self.name_for_extra} [{self.shorthash}]']\n\n    def register(self):\n        checkpoints_list[self.title] = self\n        for id in self.ids:\n            checkpoint_aliases[id] = self\n\n    def calculate_shorthash(self):\n        self.sha256 = hashes.sha256(self.filename, f\"checkpoint/{self.name}\")\n        if self.sha256 is None:\n            return\n\n        shorthash = self.sha256[0:10]\n        if self.shorthash == self.sha256[0:10]:\n            return self.shorthash\n\n        self.shorthash = shorthash\n\n        if self.shorthash not in self.ids:\n            self.ids += [self.shorthash, self.sha256, f'{self.name} [{self.shorthash}]', f'{self.name_for_extra} [{self.shorthash}]']\n\n        old_title = self.title\n        self.title = f'{self.name} [{self.shorthash}]'\n        self.short_title = f'{self.name_for_extra} [{self.shorthash}]'\n\n        replace_key(checkpoints_list, old_title, self.title, self)\n        self.register()\n\n        return self.shorthash\n\n\ntry:\n    # this silences the annoying \"Some weights of the model checkpoint were not used when initializing...\" message at start.\n    from transformers import logging, CLIPModel  # noqa: F401\n\n    logging.set_verbosity_error()\nexcept Exception:\n    pass\n\n\ndef setup_model():\n    \"\"\"called once at startup to do various one-time tasks related to SD models\"\"\"\n\n    os.makedirs(model_path, exist_ok=True)\n\n    enable_midas_autodownload()\n    patch_given_betas()\n\n\ndef checkpoint_tiles(use_short=False):\n    return [x.short_title if use_short else x.title for x in checkpoints_list.values()]\n\n\ndef list_models():\n    checkpoints_list.clear()\n    checkpoint_aliases.clear()\n\n    cmd_ckpt = shared.cmd_opts.ckpt\n    if shared.cmd_opts.no_download_sd_model or cmd_ckpt != shared.sd_model_file or os.path.exists(cmd_ckpt):\n        model_url = None\n    else:\n        model_url = f\"{shared.hf_endpoint}/runwayml/stable-diffusion-v1-5/resolve/main/v1-5-pruned-emaonly.safetensors\"\n\n    model_list = modelloader.load_models(model_path=model_path, model_url=model_url, command_path=shared.cmd_opts.ckpt_dir, ext_filter=[\".ckpt\", \".safetensors\"], download_name=\"v1-5-pruned-emaonly.safetensors\", ext_blacklist=[\".vae.ckpt\", \".vae.safetensors\"])\n\n    if os.path.exists(cmd_ckpt):\n        checkpoint_info = CheckpointInfo(cmd_ckpt)\n        checkpoint_info.register()\n\n        shared.opts.data['sd_model_checkpoint'] = checkpoint_info.title\n    elif cmd_ckpt is not None and cmd_ckpt != shared.default_sd_model_file:\n        print(f\"Checkpoint in --ckpt argument not found (Possible it was moved to {model_path}: {cmd_ckpt}\", file=sys.stderr)\n\n    for filename in model_list:\n        checkpoint_info = CheckpointInfo(filename)\n        checkpoint_info.register()\n\n\nre_strip_checksum = re.compile(r\"\\s*\\[[^]]+]\\s*$\")\n\n\ndef get_closet_checkpoint_match(search_string):\n    if not search_string:\n        return None\n\n    checkpoint_info = checkpoint_aliases.get(search_string, None)\n    if checkpoint_info is not None:\n        return checkpoint_info\n\n    found = sorted([info for info in checkpoints_list.values() if search_string in info.title], key=lambda x: len(x.title))\n    if found:\n        return found[0]\n\n    search_string_without_checksum = re.sub(re_strip_checksum, '', search_string)\n    found = sorted([info for info in checkpoints_list.values() if search_string_without_checksum in info.title], key=lambda x: len(x.title))\n    if found:\n        return found[0]\n\n    return None\n\n\ndef model_hash(filename):\n    \"\"\"old hash that only looks at a small part of the file and is prone to collisions\"\"\"\n\n    try:\n        with open(filename, \"rb\") as file:\n            import hashlib\n            m = hashlib.sha256()\n\n            file.seek(0x100000)\n            m.update(file.read(0x10000))\n            return m.hexdigest()[0:8]\n    except FileNotFoundError:\n        return 'NOFILE'\n\n\ndef select_checkpoint():\n    \"\"\"Raises `FileNotFoundError` if no checkpoints are found.\"\"\"\n    model_checkpoint = shared.opts.sd_model_checkpoint\n\n    checkpoint_info = checkpoint_aliases.get(model_checkpoint, None)\n    if checkpoint_info is not None:\n        return checkpoint_info\n\n    if len(checkpoints_list) == 0:\n        error_message = \"No checkpoints found. When searching for checkpoints, looked at:\"\n        if shared.cmd_opts.ckpt is not None:\n            error_message += f\"\\n - file {os.path.abspath(shared.cmd_opts.ckpt)}\"\n        error_message += f\"\\n - directory {model_path}\"\n        if shared.cmd_opts.ckpt_dir is not None:\n            error_message += f\"\\n - directory {os.path.abspath(shared.cmd_opts.ckpt_dir)}\"\n        error_message += \"Can't run without a checkpoint. Find and place a .ckpt or .safetensors file into any of those locations.\"\n        raise FileNotFoundError(error_message)\n\n    checkpoint_info = next(iter(checkpoints_list.values()))\n    if model_checkpoint is not None:\n        print(f\"Checkpoint {model_checkpoint} not found; loading fallback {checkpoint_info.title}\", file=sys.stderr)\n\n    return checkpoint_info\n\n\ncheckpoint_dict_replacements_sd1 = {\n    'cond_stage_model.transformer.embeddings.': 'cond_stage_model.transformer.text_model.embeddings.',\n    'cond_stage_model.transformer.encoder.': 'cond_stage_model.transformer.text_model.encoder.',\n    'cond_stage_model.transformer.final_layer_norm.': 'cond_stage_model.transformer.text_model.final_layer_norm.',\n}\n\ncheckpoint_dict_replacements_sd2_turbo = { # Converts SD 2.1 Turbo from SGM to LDM format.\n    'conditioner.embedders.0.': 'cond_stage_model.',\n}\n\n\ndef transform_checkpoint_dict_key(k, replacements):\n    for text, replacement in replacements.items():\n        if k.startswith(text):\n            k = replacement + k[len(text):]\n\n    return k\n\n\ndef get_state_dict_from_checkpoint(pl_sd):\n    pl_sd = pl_sd.pop(\"state_dict\", pl_sd)\n    pl_sd.pop(\"state_dict\", None)\n\n    is_sd2_turbo = 'conditioner.embedders.0.model.ln_final.weight' in pl_sd and pl_sd['conditioner.embedders.0.model.ln_final.weight'].size()[0] == 1024\n\n    sd = {}\n    for k, v in pl_sd.items():\n        if is_sd2_turbo:\n            new_key = transform_checkpoint_dict_key(k, checkpoint_dict_replacements_sd2_turbo)\n        else:\n            new_key = transform_checkpoint_dict_key(k, checkpoint_dict_replacements_sd1)\n\n        if new_key is not None:\n            sd[new_key] = v\n\n    pl_sd.clear()\n    pl_sd.update(sd)\n\n    return pl_sd\n\n\ndef read_metadata_from_safetensors(filename):\n    import json\n\n    with open(filename, mode=\"rb\") as file:\n        metadata_len = file.read(8)\n        metadata_len = int.from_bytes(metadata_len, \"little\")\n        json_start = file.read(2)\n\n        assert metadata_len > 2 and json_start in (b'{\"', b\"{'\"), f\"{filename} is not a safetensors file\"\n        json_data = json_start + file.read(metadata_len-2)\n        json_obj = json.loads(json_data)\n\n        res = {}\n        for k, v in json_obj.get(\"__metadata__\", {}).items():\n            res[k] = v\n            if isinstance(v, str) and v[0:1] == '{':\n                try:\n                    res[k] = json.loads(v)\n                except Exception:\n                    pass\n\n        return res\n\n\ndef read_state_dict(checkpoint_file, print_global_state=False, map_location=None):\n    _, extension = os.path.splitext(checkpoint_file)\n    if extension.lower() == \".safetensors\":\n        device = map_location or shared.weight_load_location or devices.get_optimal_device_name()\n\n        if not shared.opts.disable_mmap_load_safetensors:\n            pl_sd = safetensors.torch.load_file(checkpoint_file, device=device)\n        else:\n            pl_sd = safetensors.torch.load(open(checkpoint_file, 'rb').read())\n            pl_sd = {k: v.to(device) for k, v in pl_sd.items()}\n    else:\n        pl_sd = torch.load(checkpoint_file, map_location=map_location or shared.weight_load_location)\n\n    if print_global_state and \"global_step\" in pl_sd:\n        print(f\"Global Step: {pl_sd['global_step']}\")\n\n    sd = get_state_dict_from_checkpoint(pl_sd)\n    return sd\n\n\ndef get_checkpoint_state_dict(checkpoint_info: CheckpointInfo, timer):\n    sd_model_hash = checkpoint_info.calculate_shorthash()\n    timer.record(\"calculate hash\")\n\n    if checkpoint_info in checkpoints_loaded:\n        # use checkpoint cache\n        print(f\"Loading weights [{sd_model_hash}] from cache\")\n        # move to end as latest\n        checkpoints_loaded.move_to_end(checkpoint_info)\n        return checkpoints_loaded[checkpoint_info]\n\n    print(f\"Loading weights [{sd_model_hash}] from {checkpoint_info.filename}\")\n    res = read_state_dict(checkpoint_info.filename)\n    timer.record(\"load weights from disk\")\n\n    return res\n\n\nclass SkipWritingToConfig:\n    \"\"\"This context manager prevents load_model_weights from writing checkpoint name to the config when it loads weight.\"\"\"\n\n    skip = False\n    previous = None\n\n    def __enter__(self):\n        self.previous = SkipWritingToConfig.skip\n        SkipWritingToConfig.skip = True\n        return self\n\n    def __exit__(self, exc_type, exc_value, exc_traceback):\n        SkipWritingToConfig.skip = self.previous\n\n\ndef check_fp8(model):\n    if model is None:\n        return None\n    if devices.get_optimal_device_name() == \"mps\":\n        enable_fp8 = False\n    elif shared.opts.fp8_storage == \"Enable\":\n        enable_fp8 = True\n    elif getattr(model, \"is_sdxl\", False) and shared.opts.fp8_storage == \"Enable for SDXL\":\n        enable_fp8 = True\n    else:\n        enable_fp8 = False\n    return enable_fp8\n\n\ndef load_model_weights(model, checkpoint_info: CheckpointInfo, state_dict, timer):\n    sd_model_hash = checkpoint_info.calculate_shorthash()\n    timer.record(\"calculate hash\")\n\n    if devices.fp8:\n        # prevent model to load state dict in fp8\n        model.half()\n\n    if not SkipWritingToConfig.skip:\n        shared.opts.data[\"sd_model_checkpoint\"] = checkpoint_info.title\n\n    if state_dict is None:\n        state_dict = get_checkpoint_state_dict(checkpoint_info, timer)\n\n    model.is_sdxl = hasattr(model, 'conditioner')\n    model.is_sd2 = not model.is_sdxl and hasattr(model.cond_stage_model, 'model')\n    model.is_sd1 = not model.is_sdxl and not model.is_sd2\n    model.is_ssd = model.is_sdxl and 'model.diffusion_model.middle_block.1.transformer_blocks.0.attn1.to_q.weight' not in state_dict.keys()\n    if model.is_sdxl:\n        sd_models_xl.extend_sdxl(model)\n\n    if model.is_ssd:\n        sd_hijack.model_hijack.convert_sdxl_to_ssd(model)\n\n    if shared.opts.sd_checkpoint_cache > 0:\n        # cache newly loaded model\n        checkpoints_loaded[checkpoint_info] = state_dict.copy()\n\n    model.load_state_dict(state_dict, strict=False)\n    timer.record(\"apply weights to model\")\n\n    del state_dict\n\n    if shared.cmd_opts.opt_channelslast:\n        model.to(memory_format=torch.channels_last)\n        timer.record(\"apply channels_last\")\n\n    if shared.cmd_opts.no_half:\n        model.float()\n        model.alphas_cumprod_original = model.alphas_cumprod\n        devices.dtype_unet = torch.float32\n        timer.record(\"apply float()\")\n    else:\n        vae = model.first_stage_model\n        depth_model = getattr(model, 'depth_model', None)\n\n        # with --no-half-vae, remove VAE from model when doing half() to prevent its weights from being converted to float16\n        if shared.cmd_opts.no_half_vae:\n            model.first_stage_model = None\n        # with --upcast-sampling, don't convert the depth model weights to float16\n        if shared.cmd_opts.upcast_sampling and depth_model:\n            model.depth_model = None\n\n        alphas_cumprod = model.alphas_cumprod\n        model.alphas_cumprod = None\n        model.half()\n        model.alphas_cumprod = alphas_cumprod\n        model.alphas_cumprod_original = alphas_cumprod\n        model.first_stage_model = vae\n        if depth_model:\n            model.depth_model = depth_model\n\n        devices.dtype_unet = torch.float16\n        timer.record(\"apply half()\")\n\n    apply_alpha_schedule_override(model)\n\n    for module in model.modules():\n        if hasattr(module, 'fp16_weight'):\n            del module.fp16_weight\n        if hasattr(module, 'fp16_bias'):\n            del module.fp16_bias\n\n    if check_fp8(model):\n        devices.fp8 = True\n        first_stage = model.first_stage_model\n        model.first_stage_model = None\n        for module in model.modules():\n            if isinstance(module, (torch.nn.Conv2d, torch.nn.Linear)):\n                if shared.opts.cache_fp16_weight:\n                    module.fp16_weight = module.weight.data.clone().cpu().half()\n                    if module.bias is not None:\n                        module.fp16_bias = module.bias.data.clone().cpu().half()\n                module.to(torch.float8_e4m3fn)\n        model.first_stage_model = first_stage\n        timer.record(\"apply fp8\")\n    else:\n        devices.fp8 = False\n\n    devices.unet_needs_upcast = shared.cmd_opts.upcast_sampling and devices.dtype == torch.float16 and devices.dtype_unet == torch.float16\n\n    model.first_stage_model.to(devices.dtype_vae)\n    timer.record(\"apply dtype to VAE\")\n\n    # clean up cache if limit is reached\n    while len(checkpoints_loaded) > shared.opts.sd_checkpoint_cache:\n        checkpoints_loaded.popitem(last=False)\n\n    model.sd_model_hash = sd_model_hash\n    model.sd_model_checkpoint = checkpoint_info.filename\n    model.sd_checkpoint_info = checkpoint_info\n    shared.opts.data[\"sd_checkpoint_hash\"] = checkpoint_info.sha256\n\n    if hasattr(model, 'logvar'):\n        model.logvar = model.logvar.to(devices.device)  # fix for training\n\n    sd_vae.delete_base_vae()\n    sd_vae.clear_loaded_vae()\n    vae_file, vae_source = sd_vae.resolve_vae(checkpoint_info.filename).tuple()\n    sd_vae.load_vae(model, vae_file, vae_source)\n    timer.record(\"load VAE\")\n\n\ndef enable_midas_autodownload():\n    \"\"\"\n    Gives the ldm.modules.midas.api.load_model function automatic downloading.\n\n    When the 512-depth-ema model, and other future models like it, is loaded,\n    it calls midas.api.load_model to load the associated midas depth model.\n    This function applies a wrapper to download the model to the correct\n    location automatically.\n    \"\"\"\n\n    midas_path = os.path.join(paths.models_path, 'midas')\n\n    # stable-diffusion-stability-ai hard-codes the midas model path to\n    # a location that differs from where other scripts using this model look.\n    # HACK: Overriding the path here.\n    for k, v in midas.api.ISL_PATHS.items():\n        file_name = os.path.basename(v)\n        midas.api.ISL_PATHS[k] = os.path.join(midas_path, file_name)\n\n    midas_urls = {\n        \"dpt_large\": \"https://github.com/intel-isl/DPT/releases/download/1_0/dpt_large-midas-2f21e586.pt\",\n        \"dpt_hybrid\": \"https://github.com/intel-isl/DPT/releases/download/1_0/dpt_hybrid-midas-501f0c75.pt\",\n        \"midas_v21\": \"https://github.com/AlexeyAB/MiDaS/releases/download/midas_dpt/midas_v21-f6b98070.pt\",\n        \"midas_v21_small\": \"https://github.com/AlexeyAB/MiDaS/releases/download/midas_dpt/midas_v21_small-70d6b9c8.pt\",\n    }\n\n    midas.api.load_model_inner = midas.api.load_model\n\n    def load_model_wrapper(model_type):\n        path = midas.api.ISL_PATHS[model_type]\n        if not os.path.exists(path):\n            if not os.path.exists(midas_path):\n                os.mkdir(midas_path)\n\n            print(f\"Downloading midas model weights for {model_type} to {path}\")\n            request.urlretrieve(midas_urls[model_type], path)\n            print(f\"{model_type} downloaded\")\n\n        return midas.api.load_model_inner(model_type)\n\n    midas.api.load_model = load_model_wrapper\n\n\ndef patch_given_betas():\n    import ldm.models.diffusion.ddpm\n\n    def patched_register_schedule(*args, **kwargs):\n        \"\"\"a modified version of register_schedule function that converts plain list from Omegaconf into numpy\"\"\"\n\n        if isinstance(args[1], ListConfig):\n            args = (args[0], np.array(args[1]), *args[2:])\n\n        original_register_schedule(*args, **kwargs)\n\n    original_register_schedule = patches.patch(__name__, ldm.models.diffusion.ddpm.DDPM, 'register_schedule', patched_register_schedule)\n\n\ndef repair_config(sd_config):\n\n    if not hasattr(sd_config.model.params, \"use_ema\"):\n        sd_config.model.params.use_ema = False\n\n    if hasattr(sd_config.model.params, 'unet_config'):\n        if shared.cmd_opts.no_half:\n            sd_config.model.params.unet_config.params.use_fp16 = False\n        elif shared.cmd_opts.upcast_sampling:\n            sd_config.model.params.unet_config.params.use_fp16 = True\n\n    if getattr(sd_config.model.params.first_stage_config.params.ddconfig, \"attn_type\", None) == \"vanilla-xformers\" and not shared.xformers_available:\n        sd_config.model.params.first_stage_config.params.ddconfig.attn_type = \"vanilla\"\n\n    # For UnCLIP-L, override the hardcoded karlo directory\n    if hasattr(sd_config.model.params, \"noise_aug_config\") and hasattr(sd_config.model.params.noise_aug_config.params, \"clip_stats_path\"):\n        karlo_path = os.path.join(paths.models_path, 'karlo')\n        sd_config.model.params.noise_aug_config.params.clip_stats_path = sd_config.model.params.noise_aug_config.params.clip_stats_path.replace(\"checkpoints/karlo_models\", karlo_path)\n\n\ndef rescale_zero_terminal_snr_abar(alphas_cumprod):\n    alphas_bar_sqrt = alphas_cumprod.sqrt()\n\n    # Store old values.\n    alphas_bar_sqrt_0 = alphas_bar_sqrt[0].clone()\n    alphas_bar_sqrt_T = alphas_bar_sqrt[-1].clone()\n\n    # Shift so the last timestep is zero.\n    alphas_bar_sqrt -= (alphas_bar_sqrt_T)\n\n    # Scale so the first timestep is back to the old value.\n    alphas_bar_sqrt *= alphas_bar_sqrt_0 / (alphas_bar_sqrt_0 - alphas_bar_sqrt_T)\n\n    # Convert alphas_bar_sqrt to betas\n    alphas_bar = alphas_bar_sqrt ** 2  # Revert sqrt\n    alphas_bar[-1] = 4.8973451890853435e-08\n    return alphas_bar\n\n\ndef apply_alpha_schedule_override(sd_model, p=None):\n    \"\"\"\n    Applies an override to the alpha schedule of the model according to settings.\n    - downcasts the alpha schedule to half precision\n    - rescales the alpha schedule to have zero terminal SNR\n    \"\"\"\n\n    if not hasattr(sd_model, 'alphas_cumprod') or not hasattr(sd_model, 'alphas_cumprod_original'):\n        return\n\n    sd_model.alphas_cumprod = sd_model.alphas_cumprod_original.to(shared.device)\n\n    if opts.use_downcasted_alpha_bar:\n        if p is not None:\n            p.extra_generation_params['Downcast alphas_cumprod'] = opts.use_downcasted_alpha_bar\n        sd_model.alphas_cumprod = sd_model.alphas_cumprod.half().to(shared.device)\n\n    if opts.sd_noise_schedule == \"Zero Terminal SNR\":\n        if p is not None:\n            p.extra_generation_params['Noise Schedule'] = opts.sd_noise_schedule\n        sd_model.alphas_cumprod = rescale_zero_terminal_snr_abar(sd_model.alphas_cumprod).to(shared.device)\n\n\nsd1_clip_weight = 'cond_stage_model.transformer.text_model.embeddings.token_embedding.weight'\nsd2_clip_weight = 'cond_stage_model.model.transformer.resblocks.0.attn.in_proj_weight'\nsdxl_clip_weight = 'conditioner.embedders.1.model.ln_final.weight'\nsdxl_refiner_clip_weight = 'conditioner.embedders.0.model.ln_final.weight'\n\n\nclass SdModelData:\n    def __init__(self):\n        self.sd_model = None\n        self.loaded_sd_models = []\n        self.was_loaded_at_least_once = False\n        self.lock = threading.Lock()\n\n    def get_sd_model(self):\n        if self.was_loaded_at_least_once:\n            return self.sd_model\n\n        if self.sd_model is None:\n            with self.lock:\n                if self.sd_model is not None or self.was_loaded_at_least_once:\n                    return self.sd_model\n\n                try:\n                    load_model()\n\n                except Exception as e:\n                    errors.display(e, \"loading stable diffusion model\", full_traceback=True)\n                    print(\"\", file=sys.stderr)\n                    print(\"Stable diffusion model failed to load\", file=sys.stderr)\n                    self.sd_model = None\n\n        return self.sd_model\n\n    def set_sd_model(self, v, already_loaded=False):\n        self.sd_model = v\n        if already_loaded:\n            sd_vae.base_vae = getattr(v, \"base_vae\", None)\n            sd_vae.loaded_vae_file = getattr(v, \"loaded_vae_file\", None)\n            sd_vae.checkpoint_info = v.sd_checkpoint_info\n\n        try:\n            self.loaded_sd_models.remove(v)\n        except ValueError:\n            pass\n\n        if v is not None:\n            self.loaded_sd_models.insert(0, v)\n\n\nmodel_data = SdModelData()\n\n\ndef get_empty_cond(sd_model):\n\n    p = processing.StableDiffusionProcessingTxt2Img()\n    extra_networks.activate(p, {})\n\n    if hasattr(sd_model, 'conditioner'):\n        d = sd_model.get_learned_conditioning([\"\"])\n        return d['crossattn']\n    else:\n        return sd_model.cond_stage_model([\"\"])\n\n\ndef send_model_to_cpu(m):\n    if m.lowvram:\n        lowvram.send_everything_to_cpu()\n    else:\n        m.to(devices.cpu)\n\n    devices.torch_gc()\n\n\ndef model_target_device(m):\n    if lowvram.is_needed(m):\n        return devices.cpu\n    else:\n        return devices.device\n\n\ndef send_model_to_device(m):\n    lowvram.apply(m)\n\n    if not m.lowvram:\n        m.to(shared.device)\n\n\ndef send_model_to_trash(m):\n    m.to(device=\"meta\")\n    devices.torch_gc()\n\n\ndef load_model(checkpoint_info=None, already_loaded_state_dict=None):\n    from modules import sd_hijack\n    checkpoint_info = checkpoint_info or select_checkpoint()\n\n    timer = Timer()\n\n    if model_data.sd_model:\n        send_model_to_trash(model_data.sd_model)\n        model_data.sd_model = None\n        devices.torch_gc()\n\n    timer.record(\"unload existing model\")\n\n    if already_loaded_state_dict is not None:\n        state_dict = already_loaded_state_dict\n    else:\n        state_dict = get_checkpoint_state_dict(checkpoint_info, timer)\n\n    checkpoint_config = sd_models_config.find_checkpoint_config(state_dict, checkpoint_info)\n    clip_is_included_into_sd = any(x for x in [sd1_clip_weight, sd2_clip_weight, sdxl_clip_weight, sdxl_refiner_clip_weight] if x in state_dict)\n\n    timer.record(\"find config\")\n\n    sd_config = OmegaConf.load(checkpoint_config)\n    repair_config(sd_config)\n\n    timer.record(\"load config\")\n\n    print(f\"Creating model from config: {checkpoint_config}\")\n\n    sd_model = None\n    try:\n        with sd_disable_initialization.DisableInitialization(disable_clip=clip_is_included_into_sd or shared.cmd_opts.do_not_download_clip):\n            with sd_disable_initialization.InitializeOnMeta():\n                sd_model = instantiate_from_config(sd_config.model)\n\n    except Exception as e:\n        errors.display(e, \"creating model quickly\", full_traceback=True)\n\n    if sd_model is None:\n        print('Failed to create model quickly; will retry using slow method.', file=sys.stderr)\n\n        with sd_disable_initialization.InitializeOnMeta():\n            sd_model = instantiate_from_config(sd_config.model)\n\n    sd_model.used_config = checkpoint_config\n\n    timer.record(\"create model\")\n\n    if shared.cmd_opts.no_half:\n        weight_dtype_conversion = None\n    else:\n        weight_dtype_conversion = {\n            'first_stage_model': None,\n            'alphas_cumprod': None,\n            '': torch.float16,\n        }\n\n    with sd_disable_initialization.LoadStateDictOnMeta(state_dict, device=model_target_device(sd_model), weight_dtype_conversion=weight_dtype_conversion):\n        load_model_weights(sd_model, checkpoint_info, state_dict, timer)\n    timer.record(\"load weights from state dict\")\n\n    send_model_to_device(sd_model)\n    timer.record(\"move model to device\")\n\n    sd_hijack.model_hijack.hijack(sd_model)\n\n    timer.record(\"hijack\")\n\n    sd_model.eval()\n    model_data.set_sd_model(sd_model)\n    model_data.was_loaded_at_least_once = True\n\n    sd_hijack.model_hijack.embedding_db.load_textual_inversion_embeddings(force_reload=True)  # Reload embeddings after model load as they may or may not fit the model\n\n    timer.record(\"load textual inversion embeddings\")\n\n    script_callbacks.model_loaded_callback(sd_model)\n\n    timer.record(\"scripts callbacks\")\n\n    with devices.autocast(), torch.no_grad():\n        sd_model.cond_stage_model_empty_prompt = get_empty_cond(sd_model)\n\n    timer.record(\"calculate empty prompt\")\n\n    print(f\"Model loaded in {timer.summary()}.\")\n\n    return sd_model\n\n\ndef reuse_model_from_already_loaded(sd_model, checkpoint_info, timer):\n    \"\"\"\n    Checks if the desired checkpoint from checkpoint_info is not already loaded in model_data.loaded_sd_models.\n    If it is loaded, returns that (moving it to GPU if necessary, and moving the currently loadded model to CPU if necessary).\n    If not, returns the model that can be used to load weights from checkpoint_info's file.\n    If no such model exists, returns None.\n    Additionally deletes loaded models that are over the limit set in settings (sd_checkpoints_limit).\n    \"\"\"\n\n    if sd_model is not None and sd_model.sd_checkpoint_info.filename == checkpoint_info.filename:\n        return sd_model\n\n    if shared.opts.sd_checkpoints_keep_in_cpu:\n        send_model_to_cpu(sd_model)\n        timer.record(\"send model to cpu\")\n\n    already_loaded = None\n    for i in reversed(range(len(model_data.loaded_sd_models))):\n        loaded_model = model_data.loaded_sd_models[i]\n        if loaded_model.sd_checkpoint_info.filename == checkpoint_info.filename:\n            already_loaded = loaded_model\n            continue\n\n        if len(model_data.loaded_sd_models) > shared.opts.sd_checkpoints_limit > 0:\n            print(f\"Unloading model {len(model_data.loaded_sd_models)} over the limit of {shared.opts.sd_checkpoints_limit}: {loaded_model.sd_checkpoint_info.title}\")\n            del model_data.loaded_sd_models[i]\n            send_model_to_trash(loaded_model)\n            timer.record(\"send model to trash\")\n\n    if already_loaded is not None:\n        send_model_to_device(already_loaded)\n        timer.record(\"send model to device\")\n\n        model_data.set_sd_model(already_loaded, already_loaded=True)\n\n        if not SkipWritingToConfig.skip:\n            shared.opts.data[\"sd_model_checkpoint\"] = already_loaded.sd_checkpoint_info.title\n            shared.opts.data[\"sd_checkpoint_hash\"] = already_loaded.sd_checkpoint_info.sha256\n\n        print(f\"Using already loaded model {already_loaded.sd_checkpoint_info.title}: done in {timer.summary()}\")\n        sd_vae.reload_vae_weights(already_loaded)\n        return model_data.sd_model\n    elif shared.opts.sd_checkpoints_limit > 1 and len(model_data.loaded_sd_models) < shared.opts.sd_checkpoints_limit:\n        print(f\"Loading model {checkpoint_info.title} ({len(model_data.loaded_sd_models) + 1} out of {shared.opts.sd_checkpoints_limit})\")\n\n        model_data.sd_model = None\n        load_model(checkpoint_info)\n        return model_data.sd_model\n    elif len(model_data.loaded_sd_models) > 0:\n        sd_model = model_data.loaded_sd_models.pop()\n        model_data.sd_model = sd_model\n\n        sd_vae.base_vae = getattr(sd_model, \"base_vae\", None)\n        sd_vae.loaded_vae_file = getattr(sd_model, \"loaded_vae_file\", None)\n        sd_vae.checkpoint_info = sd_model.sd_checkpoint_info\n\n        print(f\"Reusing loaded model {sd_model.sd_checkpoint_info.title} to load {checkpoint_info.title}\")\n        return sd_model\n    else:\n        return None\n\n\ndef reload_model_weights(sd_model=None, info=None, forced_reload=False):\n    checkpoint_info = info or select_checkpoint()\n\n    timer = Timer()\n\n    if not sd_model:\n        sd_model = model_data.sd_model\n\n    if sd_model is None:  # previous model load failed\n        current_checkpoint_info = None\n    else:\n        current_checkpoint_info = sd_model.sd_checkpoint_info\n        if check_fp8(sd_model) != devices.fp8:\n            # load from state dict again to prevent extra numerical errors\n            forced_reload = True\n        elif sd_model.sd_model_checkpoint == checkpoint_info.filename and not forced_reload:\n            return sd_model\n\n    sd_model = reuse_model_from_already_loaded(sd_model, checkpoint_info, timer)\n    if not forced_reload and sd_model is not None and sd_model.sd_checkpoint_info.filename == checkpoint_info.filename:\n        return sd_model\n\n    if sd_model is not None:\n        sd_unet.apply_unet(\"None\")\n        send_model_to_cpu(sd_model)\n        sd_hijack.model_hijack.undo_hijack(sd_model)\n\n    state_dict = get_checkpoint_state_dict(checkpoint_info, timer)\n\n    checkpoint_config = sd_models_config.find_checkpoint_config(state_dict, checkpoint_info)\n\n    timer.record(\"find config\")\n\n    if sd_model is None or checkpoint_config != sd_model.used_config:\n        if sd_model is not None:\n            send_model_to_trash(sd_model)\n\n        load_model(checkpoint_info, already_loaded_state_dict=state_dict)\n        return model_data.sd_model\n\n    try:\n        load_model_weights(sd_model, checkpoint_info, state_dict, timer)\n    except Exception:\n        print(\"Failed to load checkpoint, restoring previous\")\n        load_model_weights(sd_model, current_checkpoint_info, None, timer)\n        raise\n    finally:\n        sd_hijack.model_hijack.hijack(sd_model)\n        timer.record(\"hijack\")\n\n        if not sd_model.lowvram:\n            sd_model.to(devices.device)\n            timer.record(\"move model to device\")\n\n        script_callbacks.model_loaded_callback(sd_model)\n        timer.record(\"script callbacks\")\n\n    print(f\"Weights loaded in {timer.summary()}.\")\n\n    model_data.set_sd_model(sd_model)\n    sd_unet.apply_unet()\n\n    return sd_model\n\n\ndef unload_model_weights(sd_model=None, info=None):\n    send_model_to_cpu(sd_model or shared.sd_model)\n\n    return sd_model\n\n\ndef apply_token_merging(sd_model, token_merging_ratio):\n    \"\"\"\n    Applies speed and memory optimizations from tomesd.\n    \"\"\"\n\n    current_token_merging_ratio = getattr(sd_model, 'applied_token_merged_ratio', 0)\n\n    if current_token_merging_ratio == token_merging_ratio:\n        return\n\n    if current_token_merging_ratio > 0:\n        tomesd.remove_patch(sd_model)\n\n    if token_merging_ratio > 0:\n        tomesd.apply_patch(\n            sd_model,\n            ratio=token_merging_ratio,\n            use_rand=False,  # can cause issues with some samplers\n            merge_attn=True,\n            merge_crossattn=False,\n            merge_mlp=False\n        )\n\n    sd_model.applied_token_merged_ratio = token_merging_ratio\n", "modules/sd_hijack_clip_old.py": "from modules import sd_hijack_clip\nfrom modules import shared\n\n\ndef process_text_old(self: sd_hijack_clip.FrozenCLIPEmbedderWithCustomWordsBase, texts):\n    id_start = self.id_start\n    id_end = self.id_end\n    maxlen = self.wrapped.max_length  # you get to stay at 77\n    used_custom_terms = []\n    remade_batch_tokens = []\n    hijack_comments = []\n    hijack_fixes = []\n    token_count = 0\n\n    cache = {}\n    batch_tokens = self.tokenize(texts)\n    batch_multipliers = []\n    for tokens in batch_tokens:\n        tuple_tokens = tuple(tokens)\n\n        if tuple_tokens in cache:\n            remade_tokens, fixes, multipliers = cache[tuple_tokens]\n        else:\n            fixes = []\n            remade_tokens = []\n            multipliers = []\n            mult = 1.0\n\n            i = 0\n            while i < len(tokens):\n                token = tokens[i]\n\n                embedding, embedding_length_in_tokens = self.hijack.embedding_db.find_embedding_at_position(tokens, i)\n\n                mult_change = self.token_mults.get(token) if shared.opts.emphasis != \"None\" else None\n                if mult_change is not None:\n                    mult *= mult_change\n                    i += 1\n                elif embedding is None:\n                    remade_tokens.append(token)\n                    multipliers.append(mult)\n                    i += 1\n                else:\n                    emb_len = int(embedding.vec.shape[0])\n                    fixes.append((len(remade_tokens), embedding))\n                    remade_tokens += [0] * emb_len\n                    multipliers += [mult] * emb_len\n                    used_custom_terms.append((embedding.name, embedding.checksum()))\n                    i += embedding_length_in_tokens\n\n            if len(remade_tokens) > maxlen - 2:\n                vocab = {v: k for k, v in self.wrapped.tokenizer.get_vocab().items()}\n                ovf = remade_tokens[maxlen - 2:]\n                overflowing_words = [vocab.get(int(x), \"\") for x in ovf]\n                overflowing_text = self.wrapped.tokenizer.convert_tokens_to_string(''.join(overflowing_words))\n                hijack_comments.append(f\"Warning: too many input tokens; some ({len(overflowing_words)}) have been truncated:\\n{overflowing_text}\\n\")\n\n            token_count = len(remade_tokens)\n            remade_tokens = remade_tokens + [id_end] * (maxlen - 2 - len(remade_tokens))\n            remade_tokens = [id_start] + remade_tokens[0:maxlen - 2] + [id_end]\n            cache[tuple_tokens] = (remade_tokens, fixes, multipliers)\n\n        multipliers = multipliers + [1.0] * (maxlen - 2 - len(multipliers))\n        multipliers = [1.0] + multipliers[0:maxlen - 2] + [1.0]\n\n        remade_batch_tokens.append(remade_tokens)\n        hijack_fixes.append(fixes)\n        batch_multipliers.append(multipliers)\n    return batch_multipliers, remade_batch_tokens, used_custom_terms, hijack_comments, hijack_fixes, token_count\n\n\ndef forward_old(self: sd_hijack_clip.FrozenCLIPEmbedderWithCustomWordsBase, texts):\n    batch_multipliers, remade_batch_tokens, used_custom_terms, hijack_comments, hijack_fixes, token_count = process_text_old(self, texts)\n\n    self.hijack.comments += hijack_comments\n\n    if used_custom_terms:\n        embedding_names = \", \".join(f\"{word} [{checksum}]\" for word, checksum in used_custom_terms)\n        self.hijack.comments.append(f\"Used embeddings: {embedding_names}\")\n\n    self.hijack.fixes = hijack_fixes\n    return self.process_tokens(remade_batch_tokens, batch_multipliers)\n", "modules/upscaler_utils.py": "import logging\nfrom typing import Callable\n\nimport numpy as np\nimport torch\nimport tqdm\nfrom PIL import Image\n\nfrom modules import devices, images, shared, torch_utils\n\nlogger = logging.getLogger(__name__)\n\n\ndef pil_image_to_torch_bgr(img: Image.Image) -> torch.Tensor:\n    img = np.array(img.convert(\"RGB\"))\n    img = img[:, :, ::-1]  # flip RGB to BGR\n    img = np.transpose(img, (2, 0, 1))  # HWC to CHW\n    img = np.ascontiguousarray(img) / 255  # Rescale to [0, 1]\n    return torch.from_numpy(img)\n\n\ndef torch_bgr_to_pil_image(tensor: torch.Tensor) -> Image.Image:\n    if tensor.ndim == 4:\n        # If we're given a tensor with a batch dimension, squeeze it out\n        # (but only if it's a batch of size 1).\n        if tensor.shape[0] != 1:\n            raise ValueError(f\"{tensor.shape} does not describe a BCHW tensor\")\n        tensor = tensor.squeeze(0)\n    assert tensor.ndim == 3, f\"{tensor.shape} does not describe a CHW tensor\"\n    # TODO: is `tensor.float().cpu()...numpy()` the most efficient idiom?\n    arr = tensor.float().cpu().clamp_(0, 1).numpy()  # clamp\n    arr = 255.0 * np.moveaxis(arr, 0, 2)  # CHW to HWC, rescale\n    arr = arr.round().astype(np.uint8)\n    arr = arr[:, :, ::-1]  # flip BGR to RGB\n    return Image.fromarray(arr, \"RGB\")\n\n\ndef upscale_pil_patch(model, img: Image.Image) -> Image.Image:\n    \"\"\"\n    Upscale a given PIL image using the given model.\n    \"\"\"\n    param = torch_utils.get_param(model)\n\n    with torch.no_grad():\n        tensor = pil_image_to_torch_bgr(img).unsqueeze(0)  # add batch dimension\n        tensor = tensor.to(device=param.device, dtype=param.dtype)\n        with devices.without_autocast():\n            return torch_bgr_to_pil_image(model(tensor))\n\n\ndef upscale_with_model(\n    model: Callable[[torch.Tensor], torch.Tensor],\n    img: Image.Image,\n    *,\n    tile_size: int,\n    tile_overlap: int = 0,\n    desc=\"tiled upscale\",\n) -> Image.Image:\n    if tile_size <= 0:\n        logger.debug(\"Upscaling %s without tiling\", img)\n        output = upscale_pil_patch(model, img)\n        logger.debug(\"=> %s\", output)\n        return output\n\n    grid = images.split_grid(img, tile_size, tile_size, tile_overlap)\n    newtiles = []\n\n    with tqdm.tqdm(total=grid.tile_count, desc=desc, disable=not shared.opts.enable_upscale_progressbar) as p:\n        for y, h, row in grid.tiles:\n            newrow = []\n            for x, w, tile in row:\n                if shared.state.interrupted:\n                    return img\n                output = upscale_pil_patch(model, tile)\n                scale_factor = output.width // tile.width\n                newrow.append([x * scale_factor, w * scale_factor, output])\n                p.update(1)\n            newtiles.append([y * scale_factor, h * scale_factor, newrow])\n\n    newgrid = images.Grid(\n        newtiles,\n        tile_w=grid.tile_w * scale_factor,\n        tile_h=grid.tile_h * scale_factor,\n        image_w=grid.image_w * scale_factor,\n        image_h=grid.image_h * scale_factor,\n        overlap=grid.overlap * scale_factor,\n    )\n    return images.combine_grid(newgrid)\n\n\ndef tiled_upscale_2(\n    img: torch.Tensor,\n    model,\n    *,\n    tile_size: int,\n    tile_overlap: int,\n    scale: int,\n    device: torch.device,\n    desc=\"Tiled upscale\",\n):\n    # Alternative implementation of `upscale_with_model` originally used by\n    # SwinIR and ScuNET.  It differs from `upscale_with_model` in that tiling and\n    # weighting is done in PyTorch space, as opposed to `images.Grid` doing it in\n    # Pillow space without weighting.\n\n    b, c, h, w = img.size()\n    tile_size = min(tile_size, h, w)\n\n    if tile_size <= 0:\n        logger.debug(\"Upscaling %s without tiling\", img.shape)\n        return model(img)\n\n    stride = tile_size - tile_overlap\n    h_idx_list = list(range(0, h - tile_size, stride)) + [h - tile_size]\n    w_idx_list = list(range(0, w - tile_size, stride)) + [w - tile_size]\n    result = torch.zeros(\n        b,\n        c,\n        h * scale,\n        w * scale,\n        device=device,\n        dtype=img.dtype,\n    )\n    weights = torch.zeros_like(result)\n    logger.debug(\"Upscaling %s to %s with tiles\", img.shape, result.shape)\n    with tqdm.tqdm(total=len(h_idx_list) * len(w_idx_list), desc=desc, disable=not shared.opts.enable_upscale_progressbar) as pbar:\n        for h_idx in h_idx_list:\n            if shared.state.interrupted or shared.state.skipped:\n                break\n\n            for w_idx in w_idx_list:\n                if shared.state.interrupted or shared.state.skipped:\n                    break\n\n                # Only move this patch to the device if it's not already there.\n                in_patch = img[\n                    ...,\n                    h_idx : h_idx + tile_size,\n                    w_idx : w_idx + tile_size,\n                ].to(device=device)\n\n                out_patch = model(in_patch)\n\n                result[\n                    ...,\n                    h_idx * scale : (h_idx + tile_size) * scale,\n                    w_idx * scale : (w_idx + tile_size) * scale,\n                ].add_(out_patch)\n\n                out_patch_mask = torch.ones_like(out_patch)\n\n                weights[\n                    ...,\n                    h_idx * scale : (h_idx + tile_size) * scale,\n                    w_idx * scale : (w_idx + tile_size) * scale,\n                ].add_(out_patch_mask)\n\n                pbar.update(1)\n\n    output = result.div_(weights)\n\n    return output\n\n\ndef upscale_2(\n    img: Image.Image,\n    model,\n    *,\n    tile_size: int,\n    tile_overlap: int,\n    scale: int,\n    desc: str,\n):\n    \"\"\"\n    Convenience wrapper around `tiled_upscale_2` that handles PIL images.\n    \"\"\"\n    param = torch_utils.get_param(model)\n    tensor = pil_image_to_torch_bgr(img).to(dtype=param.dtype).unsqueeze(0)  # add batch dimension\n\n    with torch.no_grad():\n        output = tiled_upscale_2(\n            tensor,\n            model,\n            tile_size=tile_size,\n            tile_overlap=tile_overlap,\n            scale=scale,\n            desc=desc,\n            device=param.device,\n        )\n    return torch_bgr_to_pil_image(output)\n", "modules/shared.py": "import os\nimport sys\n\nimport gradio as gr\n\nfrom modules import shared_cmd_options, shared_gradio_themes, options, shared_items, sd_models_types\nfrom modules.paths_internal import models_path, script_path, data_path, sd_configs_path, sd_default_config, sd_model_file, default_sd_model_file, extensions_dir, extensions_builtin_dir  # noqa: F401\nfrom modules import util\nfrom typing import TYPE_CHECKING\n\nif TYPE_CHECKING:\n    from modules import shared_state, styles, interrogate, shared_total_tqdm, memmon\n\ncmd_opts = shared_cmd_options.cmd_opts\nparser = shared_cmd_options.parser\n\nbatch_cond_uncond = True  # old field, unused now in favor of shared.opts.batch_cond_uncond\nparallel_processing_allowed = True\nstyles_filename = cmd_opts.styles_file = cmd_opts.styles_file if len(cmd_opts.styles_file) > 0 else [os.path.join(data_path, 'styles.csv')]\nconfig_filename = cmd_opts.ui_settings_file\nhide_dirs = {\"visible\": not cmd_opts.hide_ui_dir_config}\n\ndemo: gr.Blocks = None\n\ndevice: str = None\n\nweight_load_location: str = None\n\nxformers_available = False\n\nhypernetworks = {}\n\nloaded_hypernetworks = []\n\nstate: 'shared_state.State' = None\n\nprompt_styles: 'styles.StyleDatabase' = None\n\ninterrogator: 'interrogate.InterrogateModels' = None\n\nface_restorers = []\n\noptions_templates: dict = None\nopts: options.Options = None\nrestricted_opts: set[str] = None\n\nsd_model: sd_models_types.WebuiSdModel = None\n\nsettings_components: dict = None\n\"\"\"assigned from ui.py, a mapping on setting names to gradio components repsponsible for those settings\"\"\"\n\ntab_names = []\n\nlatent_upscale_default_mode = \"Latent\"\nlatent_upscale_modes = {\n    \"Latent\": {\"mode\": \"bilinear\", \"antialias\": False},\n    \"Latent (antialiased)\": {\"mode\": \"bilinear\", \"antialias\": True},\n    \"Latent (bicubic)\": {\"mode\": \"bicubic\", \"antialias\": False},\n    \"Latent (bicubic antialiased)\": {\"mode\": \"bicubic\", \"antialias\": True},\n    \"Latent (nearest)\": {\"mode\": \"nearest\", \"antialias\": False},\n    \"Latent (nearest-exact)\": {\"mode\": \"nearest-exact\", \"antialias\": False},\n}\n\nsd_upscalers = []\n\nclip_model = None\n\nprogress_print_out = sys.stdout\n\ngradio_theme = gr.themes.Base()\n\ntotal_tqdm: 'shared_total_tqdm.TotalTQDM' = None\n\nmem_mon: 'memmon.MemUsageMonitor' = None\n\noptions_section = options.options_section\nOptionInfo = options.OptionInfo\nOptionHTML = options.OptionHTML\n\nnatural_sort_key = util.natural_sort_key\nlistfiles = util.listfiles\nhtml_path = util.html_path\nhtml = util.html\nwalk_files = util.walk_files\nldm_print = util.ldm_print\n\nreload_gradio_theme = shared_gradio_themes.reload_gradio_theme\n\nlist_checkpoint_tiles = shared_items.list_checkpoint_tiles\nrefresh_checkpoints = shared_items.refresh_checkpoints\nlist_samplers = shared_items.list_samplers\nreload_hypernetworks = shared_items.reload_hypernetworks\n\nhf_endpoint = os.getenv('HF_ENDPOINT', 'https://huggingface.co')\n", "modules/ui_extra_networks_checkpoints.py": "import html\nimport os\n\nfrom modules import shared, ui_extra_networks, sd_models\nfrom modules.ui_extra_networks_checkpoints_user_metadata import CheckpointUserMetadataEditor\n\n\nclass ExtraNetworksPageCheckpoints(ui_extra_networks.ExtraNetworksPage):\n    def __init__(self):\n        super().__init__('Checkpoints')\n\n        self.allow_prompt = False\n\n    def refresh(self):\n        shared.refresh_checkpoints()\n\n    def create_item(self, name, index=None, enable_filter=True):\n        checkpoint: sd_models.CheckpointInfo = sd_models.checkpoint_aliases.get(name)\n        if checkpoint is None:\n            return\n\n        path, ext = os.path.splitext(checkpoint.filename)\n        search_terms = [self.search_terms_from_path(checkpoint.filename)]\n        if checkpoint.sha256:\n            search_terms.append(checkpoint.sha256)\n        return {\n            \"name\": checkpoint.name_for_extra,\n            \"filename\": checkpoint.filename,\n            \"shorthash\": checkpoint.shorthash,\n            \"preview\": self.find_preview(path),\n            \"description\": self.find_description(path),\n            \"search_terms\": search_terms,\n            \"onclick\": html.escape(f\"return selectCheckpoint({ui_extra_networks.quote_js(name)})\"),\n            \"local_preview\": f\"{path}.{shared.opts.samples_format}\",\n            \"metadata\": checkpoint.metadata,\n            \"sort_keys\": {'default': index, **self.get_sort_keys(checkpoint.filename)},\n        }\n\n    def list_items(self):\n        # instantiate a list to protect against concurrent modification\n        names = list(sd_models.checkpoints_list)\n        for index, name in enumerate(names):\n            item = self.create_item(name, index)\n            if item is not None:\n                yield item\n\n    def allowed_directories_for_previews(self):\n        return [v for v in [shared.cmd_opts.ckpt_dir, sd_models.model_path] if v is not None]\n\n    def create_user_metadata_editor(self, ui, tabname):\n        return CheckpointUserMetadataEditor(ui, tabname, self)\n", "modules/xpu_specific.py": "from modules import shared\nfrom modules.sd_hijack_utils import CondFunc\n\nhas_ipex = False\ntry:\n    import torch\n    import intel_extension_for_pytorch as ipex # noqa: F401\n    has_ipex = True\nexcept Exception:\n    pass\n\n\ndef check_for_xpu():\n    return has_ipex and hasattr(torch, 'xpu') and torch.xpu.is_available()\n\n\ndef get_xpu_device_string():\n    if shared.cmd_opts.device_id is not None:\n        return f\"xpu:{shared.cmd_opts.device_id}\"\n    return \"xpu\"\n\n\ndef torch_xpu_gc():\n    with torch.xpu.device(get_xpu_device_string()):\n        torch.xpu.empty_cache()\n\n\nhas_xpu = check_for_xpu()\n\n\n# Arc GPU cannot allocate a single block larger than 4GB: https://github.com/intel/compute-runtime/issues/627\n# Here we implement a slicing algorithm to split large batch size into smaller chunks,\n# so that SDPA of each chunk wouldn't require any allocation larger than ARC_SINGLE_ALLOCATION_LIMIT.\n# The heuristic limit (TOTAL_VRAM // 8) is tuned for Intel Arc A770 16G and Arc A750 8G,\n# which is the best trade-off between VRAM usage and performance.\nARC_SINGLE_ALLOCATION_LIMIT = {}\norig_sdp_attn_func = torch.nn.functional.scaled_dot_product_attention\ndef torch_xpu_scaled_dot_product_attention(\n    query, key, value, attn_mask=None, dropout_p=0.0, is_causal=False, *args, **kwargs\n):\n    # cast to same dtype first\n    key = key.to(query.dtype)\n    value = value.to(query.dtype)\n    if attn_mask is not None and attn_mask.dtype != torch.bool:\n        attn_mask = attn_mask.to(query.dtype)\n\n    N = query.shape[:-2]  # Batch size\n    L = query.size(-2)  # Target sequence length\n    E = query.size(-1)  # Embedding dimension of the query and key\n    S = key.size(-2)  # Source sequence length\n    Ev = value.size(-1)  # Embedding dimension of the value\n\n    total_batch_size = torch.numel(torch.empty(N))\n    device_id = query.device.index\n    if device_id not in ARC_SINGLE_ALLOCATION_LIMIT:\n        ARC_SINGLE_ALLOCATION_LIMIT[device_id] = min(torch.xpu.get_device_properties(device_id).total_memory // 8, 4 * 1024 * 1024 * 1024)\n    batch_size_limit = max(1, ARC_SINGLE_ALLOCATION_LIMIT[device_id] // (L * S * query.element_size()))\n\n    if total_batch_size <= batch_size_limit:\n        return orig_sdp_attn_func(\n            query,\n            key,\n            value,\n            attn_mask,\n            dropout_p,\n            is_causal,\n            *args, **kwargs\n        )\n\n    query = torch.reshape(query, (-1, L, E))\n    key = torch.reshape(key, (-1, S, E))\n    value = torch.reshape(value, (-1, S, Ev))\n    if attn_mask is not None:\n        attn_mask = attn_mask.view(-1, L, S)\n    chunk_count = (total_batch_size + batch_size_limit - 1) // batch_size_limit\n    outputs = []\n    for i in range(chunk_count):\n        attn_mask_chunk = (\n            None\n            if attn_mask is None\n            else attn_mask[i * batch_size_limit : (i + 1) * batch_size_limit, :, :]\n        )\n        chunk_output = orig_sdp_attn_func(\n            query[i * batch_size_limit : (i + 1) * batch_size_limit, :, :],\n            key[i * batch_size_limit : (i + 1) * batch_size_limit, :, :],\n            value[i * batch_size_limit : (i + 1) * batch_size_limit, :, :],\n            attn_mask_chunk,\n            dropout_p,\n            is_causal,\n            *args, **kwargs\n        )\n        outputs.append(chunk_output)\n    result = torch.cat(outputs, dim=0)\n    return torch.reshape(result, (*N, L, Ev))\n\n\ndef is_xpu_device(device: str | torch.device = None):\n    if device is None:\n        return False\n    if isinstance(device, str):\n        return device.startswith(\"xpu\")\n    return device.type == \"xpu\"\n\n\nif has_xpu:\n    try:\n        # torch.Generator supports \"xpu\" device since 2.1\n        torch.Generator(\"xpu\")\n    except RuntimeError:\n        # W/A for https://github.com/intel/intel-extension-for-pytorch/issues/452: torch.Generator API doesn't support XPU device (for torch < 2.1)\n        CondFunc('torch.Generator',\n            lambda orig_func, device=None: torch.xpu.Generator(device),\n            lambda orig_func, device=None: is_xpu_device(device))\n\n    # W/A for some OPs that could not handle different input dtypes\n    CondFunc('torch.nn.functional.layer_norm',\n        lambda orig_func, input, normalized_shape=None, weight=None, *args, **kwargs:\n        orig_func(input.to(weight.data.dtype), normalized_shape, weight, *args, **kwargs),\n        lambda orig_func, input, normalized_shape=None, weight=None, *args, **kwargs:\n        weight is not None and input.dtype != weight.data.dtype)\n    CondFunc('torch.nn.modules.GroupNorm.forward',\n        lambda orig_func, self, input: orig_func(self, input.to(self.weight.data.dtype)),\n        lambda orig_func, self, input: input.dtype != self.weight.data.dtype)\n    CondFunc('torch.nn.modules.linear.Linear.forward',\n        lambda orig_func, self, input: orig_func(self, input.to(self.weight.data.dtype)),\n        lambda orig_func, self, input: input.dtype != self.weight.data.dtype)\n    CondFunc('torch.nn.modules.conv.Conv2d.forward',\n        lambda orig_func, self, input: orig_func(self, input.to(self.weight.data.dtype)),\n        lambda orig_func, self, input: input.dtype != self.weight.data.dtype)\n    CondFunc('torch.bmm',\n        lambda orig_func, input, mat2, out=None: orig_func(input.to(mat2.dtype), mat2, out=out),\n        lambda orig_func, input, mat2, out=None: input.dtype != mat2.dtype)\n    CondFunc('torch.cat',\n        lambda orig_func, tensors, dim=0, out=None: orig_func([t.to(tensors[0].dtype) for t in tensors], dim=dim, out=out),\n        lambda orig_func, tensors, dim=0, out=None: not all(t.dtype == tensors[0].dtype for t in tensors))\n    CondFunc('torch.nn.functional.scaled_dot_product_attention',\n        lambda orig_func, *args, **kwargs: torch_xpu_scaled_dot_product_attention(*args, **kwargs),\n        lambda orig_func, query, *args, **kwargs: query.is_xpu)\n", "modules/dat_model.py": "import os\n\nfrom modules import modelloader, errors\nfrom modules.shared import cmd_opts, opts\nfrom modules.upscaler import Upscaler, UpscalerData\nfrom modules.upscaler_utils import upscale_with_model\n\n\nclass UpscalerDAT(Upscaler):\n    def __init__(self, user_path):\n        self.name = \"DAT\"\n        self.user_path = user_path\n        self.scalers = []\n        super().__init__()\n\n        for file in self.find_models(ext_filter=[\".pt\", \".pth\"]):\n            name = modelloader.friendly_name(file)\n            scaler_data = UpscalerData(name, file, upscaler=self, scale=None)\n            self.scalers.append(scaler_data)\n\n        for model in get_dat_models(self):\n            if model.name in opts.dat_enabled_models:\n                self.scalers.append(model)\n\n    def do_upscale(self, img, path):\n        try:\n            info = self.load_model(path)\n        except Exception:\n            errors.report(f\"Unable to load DAT model {path}\", exc_info=True)\n            return img\n\n        model_descriptor = modelloader.load_spandrel_model(\n            info.local_data_path,\n            device=self.device,\n            prefer_half=(not cmd_opts.no_half and not cmd_opts.upcast_sampling),\n            expected_architecture=\"DAT\",\n        )\n        return upscale_with_model(\n            model_descriptor,\n            img,\n            tile_size=opts.DAT_tile,\n            tile_overlap=opts.DAT_tile_overlap,\n        )\n\n    def load_model(self, path):\n        for scaler in self.scalers:\n            if scaler.data_path == path:\n                if scaler.local_data_path.startswith(\"http\"):\n                    scaler.local_data_path = modelloader.load_file_from_url(\n                        scaler.data_path,\n                        model_dir=self.model_download_path,\n                    )\n                if not os.path.exists(scaler.local_data_path):\n                    raise FileNotFoundError(f\"DAT data missing: {scaler.local_data_path}\")\n                return scaler\n        raise ValueError(f\"Unable to find model info: {path}\")\n\n\ndef get_dat_models(scaler):\n    return [\n        UpscalerData(\n            name=\"DAT x2\",\n            path=\"https://github.com/n0kovo/dat_upscaler_models/raw/main/DAT/DAT_x2.pth\",\n            scale=2,\n            upscaler=scaler,\n        ),\n        UpscalerData(\n            name=\"DAT x3\",\n            path=\"https://github.com/n0kovo/dat_upscaler_models/raw/main/DAT/DAT_x3.pth\",\n            scale=3,\n            upscaler=scaler,\n        ),\n        UpscalerData(\n            name=\"DAT x4\",\n            path=\"https://github.com/n0kovo/dat_upscaler_models/raw/main/DAT/DAT_x4.pth\",\n            scale=4,\n            upscaler=scaler,\n        ),\n    ]\n", "modules/logging_config.py": "import logging\nimport os\n\ntry:\n    from tqdm import tqdm\n\n\n    class TqdmLoggingHandler(logging.Handler):\n        def __init__(self, fallback_handler: logging.Handler):\n            super().__init__()\n            self.fallback_handler = fallback_handler\n\n        def emit(self, record):\n            try:\n                # If there are active tqdm progress bars,\n                # attempt to not interfere with them.\n                if tqdm._instances:\n                    tqdm.write(self.format(record))\n                else:\n                    self.fallback_handler.emit(record)\n            except Exception:\n                self.fallback_handler.emit(record)\n\nexcept ImportError:\n    TqdmLoggingHandler = None\n\n\ndef setup_logging(loglevel):\n    if loglevel is None:\n        loglevel = os.environ.get(\"SD_WEBUI_LOG_LEVEL\")\n\n    if not loglevel:\n        return\n\n    if logging.root.handlers:\n        # Already configured, do not interfere\n        return\n\n    formatter = logging.Formatter(\n        '%(asctime)s %(levelname)s [%(name)s] %(message)s',\n        '%Y-%m-%d %H:%M:%S',\n    )\n\n    if os.environ.get(\"SD_WEBUI_RICH_LOG\"):\n        from rich.logging import RichHandler\n        handler = RichHandler()\n    else:\n        handler = logging.StreamHandler()\n        handler.setFormatter(formatter)\n\n    if TqdmLoggingHandler:\n        handler = TqdmLoggingHandler(handler)\n\n    handler.setFormatter(formatter)\n\n    log_level = getattr(logging, loglevel.upper(), None) or logging.INFO\n    logging.root.setLevel(log_level)\n    logging.root.addHandler(handler)\n", "modules/models/diffusion/ddpm_edit.py": "\"\"\"\nwild mixture of\nhttps://github.com/lucidrains/denoising-diffusion-pytorch/blob/7706bdfc6f527f58d33f84b7b522e61e6e3164b3/denoising_diffusion_pytorch/denoising_diffusion_pytorch.py\nhttps://github.com/openai/improved-diffusion/blob/e94489283bb876ac1477d5dd7709bbbd2d9902ce/improved_diffusion/gaussian_diffusion.py\nhttps://github.com/CompVis/taming-transformers\n-- merci\n\"\"\"\n\n# File modified by authors of InstructPix2Pix from original (https://github.com/CompVis/stable-diffusion).\n# See more details in LICENSE.\n\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport pytorch_lightning as pl\nfrom torch.optim.lr_scheduler import LambdaLR\nfrom einops import rearrange, repeat\nfrom contextlib import contextmanager\nfrom functools import partial\nfrom tqdm import tqdm\nfrom torchvision.utils import make_grid\nfrom pytorch_lightning.utilities.distributed import rank_zero_only\n\nfrom ldm.util import log_txt_as_img, exists, default, ismap, isimage, mean_flat, count_params, instantiate_from_config\nfrom ldm.modules.ema import LitEma\nfrom ldm.modules.distributions.distributions import normal_kl, DiagonalGaussianDistribution\nfrom ldm.models.autoencoder import IdentityFirstStage, AutoencoderKL\nfrom ldm.modules.diffusionmodules.util import make_beta_schedule, extract_into_tensor, noise_like\nfrom ldm.models.diffusion.ddim import DDIMSampler\n\ntry:\n    from ldm.models.autoencoder import VQModelInterface\nexcept Exception:\n    class VQModelInterface:\n        pass\n\n__conditioning_keys__ = {'concat': 'c_concat',\n                         'crossattn': 'c_crossattn',\n                         'adm': 'y'}\n\n\ndef disabled_train(self, mode=True):\n    \"\"\"Overwrite model.train with this function to make sure train/eval mode\n    does not change anymore.\"\"\"\n    return self\n\n\ndef uniform_on_device(r1, r2, shape, device):\n    return (r1 - r2) * torch.rand(*shape, device=device) + r2\n\n\nclass DDPM(pl.LightningModule):\n    # classic DDPM with Gaussian diffusion, in image space\n    def __init__(self,\n                 unet_config,\n                 timesteps=1000,\n                 beta_schedule=\"linear\",\n                 loss_type=\"l2\",\n                 ckpt_path=None,\n                 ignore_keys=None,\n                 load_only_unet=False,\n                 monitor=\"val/loss\",\n                 use_ema=True,\n                 first_stage_key=\"image\",\n                 image_size=256,\n                 channels=3,\n                 log_every_t=100,\n                 clip_denoised=True,\n                 linear_start=1e-4,\n                 linear_end=2e-2,\n                 cosine_s=8e-3,\n                 given_betas=None,\n                 original_elbo_weight=0.,\n                 v_posterior=0.,  # weight for choosing posterior variance as sigma = (1-v) * beta_tilde + v * beta\n                 l_simple_weight=1.,\n                 conditioning_key=None,\n                 parameterization=\"eps\",  # all assuming fixed variance schedules\n                 scheduler_config=None,\n                 use_positional_encodings=False,\n                 learn_logvar=False,\n                 logvar_init=0.,\n                 load_ema=True,\n                 ):\n        super().__init__()\n        assert parameterization in [\"eps\", \"x0\"], 'currently only supporting \"eps\" and \"x0\"'\n        self.parameterization = parameterization\n        print(f\"{self.__class__.__name__}: Running in {self.parameterization}-prediction mode\")\n        self.cond_stage_model = None\n        self.clip_denoised = clip_denoised\n        self.log_every_t = log_every_t\n        self.first_stage_key = first_stage_key\n        self.image_size = image_size  # try conv?\n        self.channels = channels\n        self.use_positional_encodings = use_positional_encodings\n        self.model = DiffusionWrapper(unet_config, conditioning_key)\n        count_params(self.model, verbose=True)\n        self.use_ema = use_ema\n\n        self.use_scheduler = scheduler_config is not None\n        if self.use_scheduler:\n            self.scheduler_config = scheduler_config\n\n        self.v_posterior = v_posterior\n        self.original_elbo_weight = original_elbo_weight\n        self.l_simple_weight = l_simple_weight\n\n        if monitor is not None:\n            self.monitor = monitor\n\n        if self.use_ema and load_ema:\n            self.model_ema = LitEma(self.model)\n            print(f\"Keeping EMAs of {len(list(self.model_ema.buffers()))}.\")\n\n        if ckpt_path is not None:\n            self.init_from_ckpt(ckpt_path, ignore_keys=ignore_keys or [], only_model=load_only_unet)\n\n            # If initialing from EMA-only checkpoint, create EMA model after loading.\n            if self.use_ema and not load_ema:\n                self.model_ema = LitEma(self.model)\n                print(f\"Keeping EMAs of {len(list(self.model_ema.buffers()))}.\")\n\n        self.register_schedule(given_betas=given_betas, beta_schedule=beta_schedule, timesteps=timesteps,\n                               linear_start=linear_start, linear_end=linear_end, cosine_s=cosine_s)\n\n        self.loss_type = loss_type\n\n        self.learn_logvar = learn_logvar\n        self.logvar = torch.full(fill_value=logvar_init, size=(self.num_timesteps,))\n        if self.learn_logvar:\n            self.logvar = nn.Parameter(self.logvar, requires_grad=True)\n\n\n    def register_schedule(self, given_betas=None, beta_schedule=\"linear\", timesteps=1000,\n                          linear_start=1e-4, linear_end=2e-2, cosine_s=8e-3):\n        if exists(given_betas):\n            betas = given_betas\n        else:\n            betas = make_beta_schedule(beta_schedule, timesteps, linear_start=linear_start, linear_end=linear_end,\n                                       cosine_s=cosine_s)\n        alphas = 1. - betas\n        alphas_cumprod = np.cumprod(alphas, axis=0)\n        alphas_cumprod_prev = np.append(1., alphas_cumprod[:-1])\n\n        timesteps, = betas.shape\n        self.num_timesteps = int(timesteps)\n        self.linear_start = linear_start\n        self.linear_end = linear_end\n        assert alphas_cumprod.shape[0] == self.num_timesteps, 'alphas have to be defined for each timestep'\n\n        to_torch = partial(torch.tensor, dtype=torch.float32)\n\n        self.register_buffer('betas', to_torch(betas))\n        self.register_buffer('alphas_cumprod', to_torch(alphas_cumprod))\n        self.register_buffer('alphas_cumprod_prev', to_torch(alphas_cumprod_prev))\n\n        # calculations for diffusion q(x_t | x_{t-1}) and others\n        self.register_buffer('sqrt_alphas_cumprod', to_torch(np.sqrt(alphas_cumprod)))\n        self.register_buffer('sqrt_one_minus_alphas_cumprod', to_torch(np.sqrt(1. - alphas_cumprod)))\n        self.register_buffer('log_one_minus_alphas_cumprod', to_torch(np.log(1. - alphas_cumprod)))\n        self.register_buffer('sqrt_recip_alphas_cumprod', to_torch(np.sqrt(1. / alphas_cumprod)))\n        self.register_buffer('sqrt_recipm1_alphas_cumprod', to_torch(np.sqrt(1. / alphas_cumprod - 1)))\n\n        # calculations for posterior q(x_{t-1} | x_t, x_0)\n        posterior_variance = (1 - self.v_posterior) * betas * (1. - alphas_cumprod_prev) / (\n                    1. - alphas_cumprod) + self.v_posterior * betas\n        # above: equal to 1. / (1. / (1. - alpha_cumprod_tm1) + alpha_t / beta_t)\n        self.register_buffer('posterior_variance', to_torch(posterior_variance))\n        # below: log calculation clipped because the posterior variance is 0 at the beginning of the diffusion chain\n        self.register_buffer('posterior_log_variance_clipped', to_torch(np.log(np.maximum(posterior_variance, 1e-20))))\n        self.register_buffer('posterior_mean_coef1', to_torch(\n            betas * np.sqrt(alphas_cumprod_prev) / (1. - alphas_cumprod)))\n        self.register_buffer('posterior_mean_coef2', to_torch(\n            (1. - alphas_cumprod_prev) * np.sqrt(alphas) / (1. - alphas_cumprod)))\n\n        if self.parameterization == \"eps\":\n            lvlb_weights = self.betas ** 2 / (\n                        2 * self.posterior_variance * to_torch(alphas) * (1 - self.alphas_cumprod))\n        elif self.parameterization == \"x0\":\n            lvlb_weights = 0.5 * np.sqrt(torch.Tensor(alphas_cumprod)) / (2. * 1 - torch.Tensor(alphas_cumprod))\n        else:\n            raise NotImplementedError(\"mu not supported\")\n        # TODO how to choose this term\n        lvlb_weights[0] = lvlb_weights[1]\n        self.register_buffer('lvlb_weights', lvlb_weights, persistent=False)\n        assert not torch.isnan(self.lvlb_weights).all()\n\n    @contextmanager\n    def ema_scope(self, context=None):\n        if self.use_ema:\n            self.model_ema.store(self.model.parameters())\n            self.model_ema.copy_to(self.model)\n            if context is not None:\n                print(f\"{context}: Switched to EMA weights\")\n        try:\n            yield None\n        finally:\n            if self.use_ema:\n                self.model_ema.restore(self.model.parameters())\n                if context is not None:\n                    print(f\"{context}: Restored training weights\")\n\n    def init_from_ckpt(self, path, ignore_keys=None, only_model=False):\n        ignore_keys = ignore_keys or []\n\n        sd = torch.load(path, map_location=\"cpu\")\n        if \"state_dict\" in list(sd.keys()):\n            sd = sd[\"state_dict\"]\n        keys = list(sd.keys())\n\n        # Our model adds additional channels to the first layer to condition on an input image.\n        # For the first layer, copy existing channel weights and initialize new channel weights to zero.\n        input_keys = [\n            \"model.diffusion_model.input_blocks.0.0.weight\",\n            \"model_ema.diffusion_modelinput_blocks00weight\",\n        ]\n\n        self_sd = self.state_dict()\n        for input_key in input_keys:\n            if input_key not in sd or input_key not in self_sd:\n                continue\n\n            input_weight = self_sd[input_key]\n\n            if input_weight.size() != sd[input_key].size():\n                print(f\"Manual init: {input_key}\")\n                input_weight.zero_()\n                input_weight[:, :4, :, :].copy_(sd[input_key])\n                ignore_keys.append(input_key)\n\n        for k in keys:\n            for ik in ignore_keys:\n                if k.startswith(ik):\n                    print(f\"Deleting key {k} from state_dict.\")\n                    del sd[k]\n        missing, unexpected = self.load_state_dict(sd, strict=False) if not only_model else self.model.load_state_dict(\n            sd, strict=False)\n        print(f\"Restored from {path} with {len(missing)} missing and {len(unexpected)} unexpected keys\")\n        if missing:\n            print(f\"Missing Keys: {missing}\")\n        if unexpected:\n            print(f\"Unexpected Keys: {unexpected}\")\n\n    def q_mean_variance(self, x_start, t):\n        \"\"\"\n        Get the distribution q(x_t | x_0).\n        :param x_start: the [N x C x ...] tensor of noiseless inputs.\n        :param t: the number of diffusion steps (minus 1). Here, 0 means one step.\n        :return: A tuple (mean, variance, log_variance), all of x_start's shape.\n        \"\"\"\n        mean = (extract_into_tensor(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start)\n        variance = extract_into_tensor(1.0 - self.alphas_cumprod, t, x_start.shape)\n        log_variance = extract_into_tensor(self.log_one_minus_alphas_cumprod, t, x_start.shape)\n        return mean, variance, log_variance\n\n    def predict_start_from_noise(self, x_t, t, noise):\n        return (\n                extract_into_tensor(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t -\n                extract_into_tensor(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape) * noise\n        )\n\n    def q_posterior(self, x_start, x_t, t):\n        posterior_mean = (\n                extract_into_tensor(self.posterior_mean_coef1, t, x_t.shape) * x_start +\n                extract_into_tensor(self.posterior_mean_coef2, t, x_t.shape) * x_t\n        )\n        posterior_variance = extract_into_tensor(self.posterior_variance, t, x_t.shape)\n        posterior_log_variance_clipped = extract_into_tensor(self.posterior_log_variance_clipped, t, x_t.shape)\n        return posterior_mean, posterior_variance, posterior_log_variance_clipped\n\n    def p_mean_variance(self, x, t, clip_denoised: bool):\n        model_out = self.model(x, t)\n        if self.parameterization == \"eps\":\n            x_recon = self.predict_start_from_noise(x, t=t, noise=model_out)\n        elif self.parameterization == \"x0\":\n            x_recon = model_out\n        if clip_denoised:\n            x_recon.clamp_(-1., 1.)\n\n        model_mean, posterior_variance, posterior_log_variance = self.q_posterior(x_start=x_recon, x_t=x, t=t)\n        return model_mean, posterior_variance, posterior_log_variance\n\n    @torch.no_grad()\n    def p_sample(self, x, t, clip_denoised=True, repeat_noise=False):\n        b, *_, device = *x.shape, x.device\n        model_mean, _, model_log_variance = self.p_mean_variance(x=x, t=t, clip_denoised=clip_denoised)\n        noise = noise_like(x.shape, device, repeat_noise)\n        # no noise when t == 0\n        nonzero_mask = (1 - (t == 0).float()).reshape(b, *((1,) * (len(x.shape) - 1)))\n        return model_mean + nonzero_mask * (0.5 * model_log_variance).exp() * noise\n\n    @torch.no_grad()\n    def p_sample_loop(self, shape, return_intermediates=False):\n        device = self.betas.device\n        b = shape[0]\n        img = torch.randn(shape, device=device)\n        intermediates = [img]\n        for i in tqdm(reversed(range(0, self.num_timesteps)), desc='Sampling t', total=self.num_timesteps):\n            img = self.p_sample(img, torch.full((b,), i, device=device, dtype=torch.long),\n                                clip_denoised=self.clip_denoised)\n            if i % self.log_every_t == 0 or i == self.num_timesteps - 1:\n                intermediates.append(img)\n        if return_intermediates:\n            return img, intermediates\n        return img\n\n    @torch.no_grad()\n    def sample(self, batch_size=16, return_intermediates=False):\n        image_size = self.image_size\n        channels = self.channels\n        return self.p_sample_loop((batch_size, channels, image_size, image_size),\n                                  return_intermediates=return_intermediates)\n\n    def q_sample(self, x_start, t, noise=None):\n        noise = default(noise, lambda: torch.randn_like(x_start))\n        return (extract_into_tensor(self.sqrt_alphas_cumprod, t, x_start.shape) * x_start +\n                extract_into_tensor(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape) * noise)\n\n    def get_loss(self, pred, target, mean=True):\n        if self.loss_type == 'l1':\n            loss = (target - pred).abs()\n            if mean:\n                loss = loss.mean()\n        elif self.loss_type == 'l2':\n            if mean:\n                loss = torch.nn.functional.mse_loss(target, pred)\n            else:\n                loss = torch.nn.functional.mse_loss(target, pred, reduction='none')\n        else:\n            raise NotImplementedError(\"unknown loss type '{loss_type}'\")\n\n        return loss\n\n    def p_losses(self, x_start, t, noise=None):\n        noise = default(noise, lambda: torch.randn_like(x_start))\n        x_noisy = self.q_sample(x_start=x_start, t=t, noise=noise)\n        model_out = self.model(x_noisy, t)\n\n        loss_dict = {}\n        if self.parameterization == \"eps\":\n            target = noise\n        elif self.parameterization == \"x0\":\n            target = x_start\n        else:\n            raise NotImplementedError(f\"Parameterization {self.parameterization} not yet supported\")\n\n        loss = self.get_loss(model_out, target, mean=False).mean(dim=[1, 2, 3])\n\n        log_prefix = 'train' if self.training else 'val'\n\n        loss_dict.update({f'{log_prefix}/loss_simple': loss.mean()})\n        loss_simple = loss.mean() * self.l_simple_weight\n\n        loss_vlb = (self.lvlb_weights[t] * loss).mean()\n        loss_dict.update({f'{log_prefix}/loss_vlb': loss_vlb})\n\n        loss = loss_simple + self.original_elbo_weight * loss_vlb\n\n        loss_dict.update({f'{log_prefix}/loss': loss})\n\n        return loss, loss_dict\n\n    def forward(self, x, *args, **kwargs):\n        # b, c, h, w, device, img_size, = *x.shape, x.device, self.image_size\n        # assert h == img_size and w == img_size, f'height and width of image must be {img_size}'\n        t = torch.randint(0, self.num_timesteps, (x.shape[0],), device=self.device).long()\n        return self.p_losses(x, t, *args, **kwargs)\n\n    def get_input(self, batch, k):\n        return batch[k]\n\n    def shared_step(self, batch):\n        x = self.get_input(batch, self.first_stage_key)\n        loss, loss_dict = self(x)\n        return loss, loss_dict\n\n    def training_step(self, batch, batch_idx):\n        loss, loss_dict = self.shared_step(batch)\n\n        self.log_dict(loss_dict, prog_bar=True,\n                      logger=True, on_step=True, on_epoch=True)\n\n        self.log(\"global_step\", self.global_step,\n                 prog_bar=True, logger=True, on_step=True, on_epoch=False)\n\n        if self.use_scheduler:\n            lr = self.optimizers().param_groups[0]['lr']\n            self.log('lr_abs', lr, prog_bar=True, logger=True, on_step=True, on_epoch=False)\n\n        return loss\n\n    @torch.no_grad()\n    def validation_step(self, batch, batch_idx):\n        _, loss_dict_no_ema = self.shared_step(batch)\n        with self.ema_scope():\n            _, loss_dict_ema = self.shared_step(batch)\n            loss_dict_ema = {f\"{key}_ema\": loss_dict_ema[key] for key in loss_dict_ema}\n        self.log_dict(loss_dict_no_ema, prog_bar=False, logger=True, on_step=False, on_epoch=True)\n        self.log_dict(loss_dict_ema, prog_bar=False, logger=True, on_step=False, on_epoch=True)\n\n    def on_train_batch_end(self, *args, **kwargs):\n        if self.use_ema:\n            self.model_ema(self.model)\n\n    def _get_rows_from_list(self, samples):\n        n_imgs_per_row = len(samples)\n        denoise_grid = rearrange(samples, 'n b c h w -> b n c h w')\n        denoise_grid = rearrange(denoise_grid, 'b n c h w -> (b n) c h w')\n        denoise_grid = make_grid(denoise_grid, nrow=n_imgs_per_row)\n        return denoise_grid\n\n    @torch.no_grad()\n    def log_images(self, batch, N=8, n_row=2, sample=True, return_keys=None, **kwargs):\n        log = {}\n        x = self.get_input(batch, self.first_stage_key)\n        N = min(x.shape[0], N)\n        n_row = min(x.shape[0], n_row)\n        x = x.to(self.device)[:N]\n        log[\"inputs\"] = x\n\n        # get diffusion row\n        diffusion_row = []\n        x_start = x[:n_row]\n\n        for t in range(self.num_timesteps):\n            if t % self.log_every_t == 0 or t == self.num_timesteps - 1:\n                t = repeat(torch.tensor([t]), '1 -> b', b=n_row)\n                t = t.to(self.device).long()\n                noise = torch.randn_like(x_start)\n                x_noisy = self.q_sample(x_start=x_start, t=t, noise=noise)\n                diffusion_row.append(x_noisy)\n\n        log[\"diffusion_row\"] = self._get_rows_from_list(diffusion_row)\n\n        if sample:\n            # get denoise row\n            with self.ema_scope(\"Plotting\"):\n                samples, denoise_row = self.sample(batch_size=N, return_intermediates=True)\n\n            log[\"samples\"] = samples\n            log[\"denoise_row\"] = self._get_rows_from_list(denoise_row)\n\n        if return_keys:\n            if np.intersect1d(list(log.keys()), return_keys).shape[0] == 0:\n                return log\n            else:\n                return {key: log[key] for key in return_keys}\n        return log\n\n    def configure_optimizers(self):\n        lr = self.learning_rate\n        params = list(self.model.parameters())\n        if self.learn_logvar:\n            params = params + [self.logvar]\n        opt = torch.optim.AdamW(params, lr=lr)\n        return opt\n\n\nclass LatentDiffusion(DDPM):\n    \"\"\"main class\"\"\"\n    def __init__(self,\n                 first_stage_config,\n                 cond_stage_config,\n                 num_timesteps_cond=None,\n                 cond_stage_key=\"image\",\n                 cond_stage_trainable=False,\n                 concat_mode=True,\n                 cond_stage_forward=None,\n                 conditioning_key=None,\n                 scale_factor=1.0,\n                 scale_by_std=False,\n                 load_ema=True,\n                 *args, **kwargs):\n        self.num_timesteps_cond = default(num_timesteps_cond, 1)\n        self.scale_by_std = scale_by_std\n        assert self.num_timesteps_cond <= kwargs['timesteps']\n        # for backwards compatibility after implementation of DiffusionWrapper\n        if conditioning_key is None:\n            conditioning_key = 'concat' if concat_mode else 'crossattn'\n        if cond_stage_config == '__is_unconditional__':\n            conditioning_key = None\n        ckpt_path = kwargs.pop(\"ckpt_path\", None)\n        ignore_keys = kwargs.pop(\"ignore_keys\", [])\n        super().__init__(*args, conditioning_key=conditioning_key, load_ema=load_ema, **kwargs)\n        self.concat_mode = concat_mode\n        self.cond_stage_trainable = cond_stage_trainable\n        self.cond_stage_key = cond_stage_key\n        try:\n            self.num_downs = len(first_stage_config.params.ddconfig.ch_mult) - 1\n        except Exception:\n            self.num_downs = 0\n        if not scale_by_std:\n            self.scale_factor = scale_factor\n        else:\n            self.register_buffer('scale_factor', torch.tensor(scale_factor))\n        self.instantiate_first_stage(first_stage_config)\n        self.instantiate_cond_stage(cond_stage_config)\n        self.cond_stage_forward = cond_stage_forward\n        self.clip_denoised = False\n        self.bbox_tokenizer = None\n\n        self.restarted_from_ckpt = False\n        if ckpt_path is not None:\n            self.init_from_ckpt(ckpt_path, ignore_keys)\n            self.restarted_from_ckpt = True\n\n            if self.use_ema and not load_ema:\n                self.model_ema = LitEma(self.model)\n                print(f\"Keeping EMAs of {len(list(self.model_ema.buffers()))}.\")\n\n    def make_cond_schedule(self, ):\n        self.cond_ids = torch.full(size=(self.num_timesteps,), fill_value=self.num_timesteps - 1, dtype=torch.long)\n        ids = torch.round(torch.linspace(0, self.num_timesteps - 1, self.num_timesteps_cond)).long()\n        self.cond_ids[:self.num_timesteps_cond] = ids\n\n    @rank_zero_only\n    @torch.no_grad()\n    def on_train_batch_start(self, batch, batch_idx, dataloader_idx):\n        # only for very first batch\n        if self.scale_by_std and self.current_epoch == 0 and self.global_step == 0 and batch_idx == 0 and not self.restarted_from_ckpt:\n            assert self.scale_factor == 1., 'rather not use custom rescaling and std-rescaling simultaneously'\n            # set rescale weight to 1./std of encodings\n            print(\"### USING STD-RESCALING ###\")\n            x = super().get_input(batch, self.first_stage_key)\n            x = x.to(self.device)\n            encoder_posterior = self.encode_first_stage(x)\n            z = self.get_first_stage_encoding(encoder_posterior).detach()\n            del self.scale_factor\n            self.register_buffer('scale_factor', 1. / z.flatten().std())\n            print(f\"setting self.scale_factor to {self.scale_factor}\")\n            print(\"### USING STD-RESCALING ###\")\n\n    def register_schedule(self,\n                          given_betas=None, beta_schedule=\"linear\", timesteps=1000,\n                          linear_start=1e-4, linear_end=2e-2, cosine_s=8e-3):\n        super().register_schedule(given_betas, beta_schedule, timesteps, linear_start, linear_end, cosine_s)\n\n        self.shorten_cond_schedule = self.num_timesteps_cond > 1\n        if self.shorten_cond_schedule:\n            self.make_cond_schedule()\n\n    def instantiate_first_stage(self, config):\n        model = instantiate_from_config(config)\n        self.first_stage_model = model.eval()\n        self.first_stage_model.train = disabled_train\n        for param in self.first_stage_model.parameters():\n            param.requires_grad = False\n\n    def instantiate_cond_stage(self, config):\n        if not self.cond_stage_trainable:\n            if config == \"__is_first_stage__\":\n                print(\"Using first stage also as cond stage.\")\n                self.cond_stage_model = self.first_stage_model\n            elif config == \"__is_unconditional__\":\n                print(f\"Training {self.__class__.__name__} as an unconditional model.\")\n                self.cond_stage_model = None\n                # self.be_unconditional = True\n            else:\n                model = instantiate_from_config(config)\n                self.cond_stage_model = model.eval()\n                self.cond_stage_model.train = disabled_train\n                for param in self.cond_stage_model.parameters():\n                    param.requires_grad = False\n        else:\n            assert config != '__is_first_stage__'\n            assert config != '__is_unconditional__'\n            model = instantiate_from_config(config)\n            self.cond_stage_model = model\n\n    def _get_denoise_row_from_list(self, samples, desc='', force_no_decoder_quantization=False):\n        denoise_row = []\n        for zd in tqdm(samples, desc=desc):\n            denoise_row.append(self.decode_first_stage(zd.to(self.device),\n                                                            force_not_quantize=force_no_decoder_quantization))\n        n_imgs_per_row = len(denoise_row)\n        denoise_row = torch.stack(denoise_row)  # n_log_step, n_row, C, H, W\n        denoise_grid = rearrange(denoise_row, 'n b c h w -> b n c h w')\n        denoise_grid = rearrange(denoise_grid, 'b n c h w -> (b n) c h w')\n        denoise_grid = make_grid(denoise_grid, nrow=n_imgs_per_row)\n        return denoise_grid\n\n    def get_first_stage_encoding(self, encoder_posterior):\n        if isinstance(encoder_posterior, DiagonalGaussianDistribution):\n            z = encoder_posterior.sample()\n        elif isinstance(encoder_posterior, torch.Tensor):\n            z = encoder_posterior\n        else:\n            raise NotImplementedError(f\"encoder_posterior of type '{type(encoder_posterior)}' not yet implemented\")\n        return self.scale_factor * z\n\n    def get_learned_conditioning(self, c):\n        if self.cond_stage_forward is None:\n            if hasattr(self.cond_stage_model, 'encode') and callable(self.cond_stage_model.encode):\n                c = self.cond_stage_model.encode(c)\n                if isinstance(c, DiagonalGaussianDistribution):\n                    c = c.mode()\n            else:\n                c = self.cond_stage_model(c)\n        else:\n            assert hasattr(self.cond_stage_model, self.cond_stage_forward)\n            c = getattr(self.cond_stage_model, self.cond_stage_forward)(c)\n        return c\n\n    def meshgrid(self, h, w):\n        y = torch.arange(0, h).view(h, 1, 1).repeat(1, w, 1)\n        x = torch.arange(0, w).view(1, w, 1).repeat(h, 1, 1)\n\n        arr = torch.cat([y, x], dim=-1)\n        return arr\n\n    def delta_border(self, h, w):\n        \"\"\"\n        :param h: height\n        :param w: width\n        :return: normalized distance to image border,\n         wtith min distance = 0 at border and max dist = 0.5 at image center\n        \"\"\"\n        lower_right_corner = torch.tensor([h - 1, w - 1]).view(1, 1, 2)\n        arr = self.meshgrid(h, w) / lower_right_corner\n        dist_left_up = torch.min(arr, dim=-1, keepdims=True)[0]\n        dist_right_down = torch.min(1 - arr, dim=-1, keepdims=True)[0]\n        edge_dist = torch.min(torch.cat([dist_left_up, dist_right_down], dim=-1), dim=-1)[0]\n        return edge_dist\n\n    def get_weighting(self, h, w, Ly, Lx, device):\n        weighting = self.delta_border(h, w)\n        weighting = torch.clip(weighting, self.split_input_params[\"clip_min_weight\"],\n                               self.split_input_params[\"clip_max_weight\"], )\n        weighting = weighting.view(1, h * w, 1).repeat(1, 1, Ly * Lx).to(device)\n\n        if self.split_input_params[\"tie_braker\"]:\n            L_weighting = self.delta_border(Ly, Lx)\n            L_weighting = torch.clip(L_weighting,\n                                     self.split_input_params[\"clip_min_tie_weight\"],\n                                     self.split_input_params[\"clip_max_tie_weight\"])\n\n            L_weighting = L_weighting.view(1, 1, Ly * Lx).to(device)\n            weighting = weighting * L_weighting\n        return weighting\n\n    def get_fold_unfold(self, x, kernel_size, stride, uf=1, df=1):  # todo load once not every time, shorten code\n        \"\"\"\n        :param x: img of size (bs, c, h, w)\n        :return: n img crops of size (n, bs, c, kernel_size[0], kernel_size[1])\n        \"\"\"\n        bs, nc, h, w = x.shape\n\n        # number of crops in image\n        Ly = (h - kernel_size[0]) // stride[0] + 1\n        Lx = (w - kernel_size[1]) // stride[1] + 1\n\n        if uf == 1 and df == 1:\n            fold_params = dict(kernel_size=kernel_size, dilation=1, padding=0, stride=stride)\n            unfold = torch.nn.Unfold(**fold_params)\n\n            fold = torch.nn.Fold(output_size=x.shape[2:], **fold_params)\n\n            weighting = self.get_weighting(kernel_size[0], kernel_size[1], Ly, Lx, x.device).to(x.dtype)\n            normalization = fold(weighting).view(1, 1, h, w)  # normalizes the overlap\n            weighting = weighting.view((1, 1, kernel_size[0], kernel_size[1], Ly * Lx))\n\n        elif uf > 1 and df == 1:\n            fold_params = dict(kernel_size=kernel_size, dilation=1, padding=0, stride=stride)\n            unfold = torch.nn.Unfold(**fold_params)\n\n            fold_params2 = dict(kernel_size=(kernel_size[0] * uf, kernel_size[0] * uf),\n                                dilation=1, padding=0,\n                                stride=(stride[0] * uf, stride[1] * uf))\n            fold = torch.nn.Fold(output_size=(x.shape[2] * uf, x.shape[3] * uf), **fold_params2)\n\n            weighting = self.get_weighting(kernel_size[0] * uf, kernel_size[1] * uf, Ly, Lx, x.device).to(x.dtype)\n            normalization = fold(weighting).view(1, 1, h * uf, w * uf)  # normalizes the overlap\n            weighting = weighting.view((1, 1, kernel_size[0] * uf, kernel_size[1] * uf, Ly * Lx))\n\n        elif df > 1 and uf == 1:\n            fold_params = dict(kernel_size=kernel_size, dilation=1, padding=0, stride=stride)\n            unfold = torch.nn.Unfold(**fold_params)\n\n            fold_params2 = dict(kernel_size=(kernel_size[0] // df, kernel_size[0] // df),\n                                dilation=1, padding=0,\n                                stride=(stride[0] // df, stride[1] // df))\n            fold = torch.nn.Fold(output_size=(x.shape[2] // df, x.shape[3] // df), **fold_params2)\n\n            weighting = self.get_weighting(kernel_size[0] // df, kernel_size[1] // df, Ly, Lx, x.device).to(x.dtype)\n            normalization = fold(weighting).view(1, 1, h // df, w // df)  # normalizes the overlap\n            weighting = weighting.view((1, 1, kernel_size[0] // df, kernel_size[1] // df, Ly * Lx))\n\n        else:\n            raise NotImplementedError\n\n        return fold, unfold, normalization, weighting\n\n    @torch.no_grad()\n    def get_input(self, batch, k, return_first_stage_outputs=False, force_c_encode=False,\n                  cond_key=None, return_original_cond=False, bs=None, uncond=0.05):\n        x = super().get_input(batch, k)\n        if bs is not None:\n            x = x[:bs]\n        x = x.to(self.device)\n        encoder_posterior = self.encode_first_stage(x)\n        z = self.get_first_stage_encoding(encoder_posterior).detach()\n        cond_key = cond_key or self.cond_stage_key\n        xc = super().get_input(batch, cond_key)\n        if bs is not None:\n            xc[\"c_crossattn\"] = xc[\"c_crossattn\"][:bs]\n            xc[\"c_concat\"] = xc[\"c_concat\"][:bs]\n        cond = {}\n\n        # To support classifier-free guidance, randomly drop out only text conditioning 5%, only image conditioning 5%, and both 5%.\n        random = torch.rand(x.size(0), device=x.device)\n        prompt_mask = rearrange(random < 2 * uncond, \"n -> n 1 1\")\n        input_mask = 1 - rearrange((random >= uncond).float() * (random < 3 * uncond).float(), \"n -> n 1 1 1\")\n\n        null_prompt = self.get_learned_conditioning([\"\"])\n        cond[\"c_crossattn\"] = [torch.where(prompt_mask, null_prompt, self.get_learned_conditioning(xc[\"c_crossattn\"]).detach())]\n        cond[\"c_concat\"] = [input_mask * self.encode_first_stage((xc[\"c_concat\"].to(self.device))).mode().detach()]\n\n        out = [z, cond]\n        if return_first_stage_outputs:\n            xrec = self.decode_first_stage(z)\n            out.extend([x, xrec])\n        if return_original_cond:\n            out.append(xc)\n        return out\n\n    @torch.no_grad()\n    def decode_first_stage(self, z, predict_cids=False, force_not_quantize=False):\n        if predict_cids:\n            if z.dim() == 4:\n                z = torch.argmax(z.exp(), dim=1).long()\n            z = self.first_stage_model.quantize.get_codebook_entry(z, shape=None)\n            z = rearrange(z, 'b h w c -> b c h w').contiguous()\n\n        z = 1. / self.scale_factor * z\n\n        if hasattr(self, \"split_input_params\"):\n            if self.split_input_params[\"patch_distributed_vq\"]:\n                ks = self.split_input_params[\"ks\"]  # eg. (128, 128)\n                stride = self.split_input_params[\"stride\"]  # eg. (64, 64)\n                uf = self.split_input_params[\"vqf\"]\n                bs, nc, h, w = z.shape\n                if ks[0] > h or ks[1] > w:\n                    ks = (min(ks[0], h), min(ks[1], w))\n                    print(\"reducing Kernel\")\n\n                if stride[0] > h or stride[1] > w:\n                    stride = (min(stride[0], h), min(stride[1], w))\n                    print(\"reducing stride\")\n\n                fold, unfold, normalization, weighting = self.get_fold_unfold(z, ks, stride, uf=uf)\n\n                z = unfold(z)  # (bn, nc * prod(**ks), L)\n                # 1. Reshape to img shape\n                z = z.view((z.shape[0], -1, ks[0], ks[1], z.shape[-1]))  # (bn, nc, ks[0], ks[1], L )\n\n                # 2. apply model loop over last dim\n                if isinstance(self.first_stage_model, VQModelInterface):\n                    output_list = [self.first_stage_model.decode(z[:, :, :, :, i],\n                                                                 force_not_quantize=predict_cids or force_not_quantize)\n                                   for i in range(z.shape[-1])]\n                else:\n\n                    output_list = [self.first_stage_model.decode(z[:, :, :, :, i])\n                                   for i in range(z.shape[-1])]\n\n                o = torch.stack(output_list, axis=-1)  # # (bn, nc, ks[0], ks[1], L)\n                o = o * weighting\n                # Reverse 1. reshape to img shape\n                o = o.view((o.shape[0], -1, o.shape[-1]))  # (bn, nc * ks[0] * ks[1], L)\n                # stitch crops together\n                decoded = fold(o)\n                decoded = decoded / normalization  # norm is shape (1, 1, h, w)\n                return decoded\n            else:\n                if isinstance(self.first_stage_model, VQModelInterface):\n                    return self.first_stage_model.decode(z, force_not_quantize=predict_cids or force_not_quantize)\n                else:\n                    return self.first_stage_model.decode(z)\n\n        else:\n            if isinstance(self.first_stage_model, VQModelInterface):\n                return self.first_stage_model.decode(z, force_not_quantize=predict_cids or force_not_quantize)\n            else:\n                return self.first_stage_model.decode(z)\n\n    # same as above but without decorator\n    def differentiable_decode_first_stage(self, z, predict_cids=False, force_not_quantize=False):\n        if predict_cids:\n            if z.dim() == 4:\n                z = torch.argmax(z.exp(), dim=1).long()\n            z = self.first_stage_model.quantize.get_codebook_entry(z, shape=None)\n            z = rearrange(z, 'b h w c -> b c h w').contiguous()\n\n        z = 1. / self.scale_factor * z\n\n        if hasattr(self, \"split_input_params\"):\n            if self.split_input_params[\"patch_distributed_vq\"]:\n                ks = self.split_input_params[\"ks\"]  # eg. (128, 128)\n                stride = self.split_input_params[\"stride\"]  # eg. (64, 64)\n                uf = self.split_input_params[\"vqf\"]\n                bs, nc, h, w = z.shape\n                if ks[0] > h or ks[1] > w:\n                    ks = (min(ks[0], h), min(ks[1], w))\n                    print(\"reducing Kernel\")\n\n                if stride[0] > h or stride[1] > w:\n                    stride = (min(stride[0], h), min(stride[1], w))\n                    print(\"reducing stride\")\n\n                fold, unfold, normalization, weighting = self.get_fold_unfold(z, ks, stride, uf=uf)\n\n                z = unfold(z)  # (bn, nc * prod(**ks), L)\n                # 1. Reshape to img shape\n                z = z.view((z.shape[0], -1, ks[0], ks[1], z.shape[-1]))  # (bn, nc, ks[0], ks[1], L )\n\n                # 2. apply model loop over last dim\n                if isinstance(self.first_stage_model, VQModelInterface):\n                    output_list = [self.first_stage_model.decode(z[:, :, :, :, i],\n                                                                 force_not_quantize=predict_cids or force_not_quantize)\n                                   for i in range(z.shape[-1])]\n                else:\n\n                    output_list = [self.first_stage_model.decode(z[:, :, :, :, i])\n                                   for i in range(z.shape[-1])]\n\n                o = torch.stack(output_list, axis=-1)  # # (bn, nc, ks[0], ks[1], L)\n                o = o * weighting\n                # Reverse 1. reshape to img shape\n                o = o.view((o.shape[0], -1, o.shape[-1]))  # (bn, nc * ks[0] * ks[1], L)\n                # stitch crops together\n                decoded = fold(o)\n                decoded = decoded / normalization  # norm is shape (1, 1, h, w)\n                return decoded\n            else:\n                if isinstance(self.first_stage_model, VQModelInterface):\n                    return self.first_stage_model.decode(z, force_not_quantize=predict_cids or force_not_quantize)\n                else:\n                    return self.first_stage_model.decode(z)\n\n        else:\n            if isinstance(self.first_stage_model, VQModelInterface):\n                return self.first_stage_model.decode(z, force_not_quantize=predict_cids or force_not_quantize)\n            else:\n                return self.first_stage_model.decode(z)\n\n    @torch.no_grad()\n    def encode_first_stage(self, x):\n        if hasattr(self, \"split_input_params\"):\n            if self.split_input_params[\"patch_distributed_vq\"]:\n                ks = self.split_input_params[\"ks\"]  # eg. (128, 128)\n                stride = self.split_input_params[\"stride\"]  # eg. (64, 64)\n                df = self.split_input_params[\"vqf\"]\n                self.split_input_params['original_image_size'] = x.shape[-2:]\n                bs, nc, h, w = x.shape\n                if ks[0] > h or ks[1] > w:\n                    ks = (min(ks[0], h), min(ks[1], w))\n                    print(\"reducing Kernel\")\n\n                if stride[0] > h or stride[1] > w:\n                    stride = (min(stride[0], h), min(stride[1], w))\n                    print(\"reducing stride\")\n\n                fold, unfold, normalization, weighting = self.get_fold_unfold(x, ks, stride, df=df)\n                z = unfold(x)  # (bn, nc * prod(**ks), L)\n                # Reshape to img shape\n                z = z.view((z.shape[0], -1, ks[0], ks[1], z.shape[-1]))  # (bn, nc, ks[0], ks[1], L )\n\n                output_list = [self.first_stage_model.encode(z[:, :, :, :, i])\n                               for i in range(z.shape[-1])]\n\n                o = torch.stack(output_list, axis=-1)\n                o = o * weighting\n\n                # Reverse reshape to img shape\n                o = o.view((o.shape[0], -1, o.shape[-1]))  # (bn, nc * ks[0] * ks[1], L)\n                # stitch crops together\n                decoded = fold(o)\n                decoded = decoded / normalization\n                return decoded\n\n            else:\n                return self.first_stage_model.encode(x)\n        else:\n            return self.first_stage_model.encode(x)\n\n    def shared_step(self, batch, **kwargs):\n        x, c = self.get_input(batch, self.first_stage_key)\n        loss = self(x, c)\n        return loss\n\n    def forward(self, x, c, *args, **kwargs):\n        t = torch.randint(0, self.num_timesteps, (x.shape[0],), device=self.device).long()\n        if self.model.conditioning_key is not None:\n            assert c is not None\n            if self.cond_stage_trainable:\n                c = self.get_learned_conditioning(c)\n            if self.shorten_cond_schedule:  # TODO: drop this option\n                tc = self.cond_ids[t].to(self.device)\n                c = self.q_sample(x_start=c, t=tc, noise=torch.randn_like(c.float()))\n        return self.p_losses(x, c, t, *args, **kwargs)\n\n    def apply_model(self, x_noisy, t, cond, return_ids=False):\n\n        if isinstance(cond, dict):\n            # hybrid case, cond is expected to be a dict\n            pass\n        else:\n            if not isinstance(cond, list):\n                cond = [cond]\n            key = 'c_concat' if self.model.conditioning_key == 'concat' else 'c_crossattn'\n            cond = {key: cond}\n\n        if hasattr(self, \"split_input_params\"):\n            assert len(cond) == 1  # todo can only deal with one conditioning atm\n            assert not return_ids\n            ks = self.split_input_params[\"ks\"]  # eg. (128, 128)\n            stride = self.split_input_params[\"stride\"]  # eg. (64, 64)\n\n            h, w = x_noisy.shape[-2:]\n\n            fold, unfold, normalization, weighting = self.get_fold_unfold(x_noisy, ks, stride)\n\n            z = unfold(x_noisy)  # (bn, nc * prod(**ks), L)\n            # Reshape to img shape\n            z = z.view((z.shape[0], -1, ks[0], ks[1], z.shape[-1]))  # (bn, nc, ks[0], ks[1], L )\n            z_list = [z[:, :, :, :, i] for i in range(z.shape[-1])]\n\n            if self.cond_stage_key in [\"image\", \"LR_image\", \"segmentation\",\n                                       'bbox_img'] and self.model.conditioning_key:  # todo check for completeness\n                c_key = next(iter(cond.keys()))  # get key\n                c = next(iter(cond.values()))  # get value\n                assert (len(c) == 1)  # todo extend to list with more than one elem\n                c = c[0]  # get element\n\n                c = unfold(c)\n                c = c.view((c.shape[0], -1, ks[0], ks[1], c.shape[-1]))  # (bn, nc, ks[0], ks[1], L )\n\n                cond_list = [{c_key: [c[:, :, :, :, i]]} for i in range(c.shape[-1])]\n\n            elif self.cond_stage_key == 'coordinates_bbox':\n                assert 'original_image_size' in self.split_input_params, 'BoundingBoxRescaling is missing original_image_size'\n\n                # assuming padding of unfold is always 0 and its dilation is always 1\n                n_patches_per_row = int((w - ks[0]) / stride[0] + 1)\n                full_img_h, full_img_w = self.split_input_params['original_image_size']\n                # as we are operating on latents, we need the factor from the original image size to the\n                # spatial latent size to properly rescale the crops for regenerating the bbox annotations\n                num_downs = self.first_stage_model.encoder.num_resolutions - 1\n                rescale_latent = 2 ** (num_downs)\n\n                # get top left positions of patches as conforming for the bbbox tokenizer, therefore we\n                # need to rescale the tl patch coordinates to be in between (0,1)\n                tl_patch_coordinates = [(rescale_latent * stride[0] * (patch_nr % n_patches_per_row) / full_img_w,\n                                         rescale_latent * stride[1] * (patch_nr // n_patches_per_row) / full_img_h)\n                                        for patch_nr in range(z.shape[-1])]\n\n                # patch_limits are tl_coord, width and height coordinates as (x_tl, y_tl, h, w)\n                patch_limits = [(x_tl, y_tl,\n                                 rescale_latent * ks[0] / full_img_w,\n                                 rescale_latent * ks[1] / full_img_h) for x_tl, y_tl in tl_patch_coordinates]\n                # patch_values = [(np.arange(x_tl,min(x_tl+ks, 1.)),np.arange(y_tl,min(y_tl+ks, 1.))) for x_tl, y_tl in tl_patch_coordinates]\n\n                # tokenize crop coordinates for the bounding boxes of the respective patches\n                patch_limits_tknzd = [torch.LongTensor(self.bbox_tokenizer._crop_encoder(bbox))[None].to(self.device)\n                                      for bbox in patch_limits]  # list of length l with tensors of shape (1, 2)\n                print(patch_limits_tknzd[0].shape)\n                # cut tknzd crop position from conditioning\n                assert isinstance(cond, dict), 'cond must be dict to be fed into model'\n                cut_cond = cond['c_crossattn'][0][..., :-2].to(self.device)\n                print(cut_cond.shape)\n\n                adapted_cond = torch.stack([torch.cat([cut_cond, p], dim=1) for p in patch_limits_tknzd])\n                adapted_cond = rearrange(adapted_cond, 'l b n -> (l b) n')\n                print(adapted_cond.shape)\n                adapted_cond = self.get_learned_conditioning(adapted_cond)\n                print(adapted_cond.shape)\n                adapted_cond = rearrange(adapted_cond, '(l b) n d -> l b n d', l=z.shape[-1])\n                print(adapted_cond.shape)\n\n                cond_list = [{'c_crossattn': [e]} for e in adapted_cond]\n\n            else:\n                cond_list = [cond for i in range(z.shape[-1])]  # Todo make this more efficient\n\n            # apply model by loop over crops\n            output_list = [self.model(z_list[i], t, **cond_list[i]) for i in range(z.shape[-1])]\n            assert not isinstance(output_list[0],\n                                  tuple)  # todo cant deal with multiple model outputs check this never happens\n\n            o = torch.stack(output_list, axis=-1)\n            o = o * weighting\n            # Reverse reshape to img shape\n            o = o.view((o.shape[0], -1, o.shape[-1]))  # (bn, nc * ks[0] * ks[1], L)\n            # stitch crops together\n            x_recon = fold(o) / normalization\n\n        else:\n            x_recon = self.model(x_noisy, t, **cond)\n\n        if isinstance(x_recon, tuple) and not return_ids:\n            return x_recon[0]\n        else:\n            return x_recon\n\n    def _predict_eps_from_xstart(self, x_t, t, pred_xstart):\n        return (extract_into_tensor(self.sqrt_recip_alphas_cumprod, t, x_t.shape) * x_t - pred_xstart) / \\\n               extract_into_tensor(self.sqrt_recipm1_alphas_cumprod, t, x_t.shape)\n\n    def _prior_bpd(self, x_start):\n        \"\"\"\n        Get the prior KL term for the variational lower-bound, measured in\n        bits-per-dim.\n        This term can't be optimized, as it only depends on the encoder.\n        :param x_start: the [N x C x ...] tensor of inputs.\n        :return: a batch of [N] KL values (in bits), one per batch element.\n        \"\"\"\n        batch_size = x_start.shape[0]\n        t = torch.tensor([self.num_timesteps - 1] * batch_size, device=x_start.device)\n        qt_mean, _, qt_log_variance = self.q_mean_variance(x_start, t)\n        kl_prior = normal_kl(mean1=qt_mean, logvar1=qt_log_variance, mean2=0.0, logvar2=0.0)\n        return mean_flat(kl_prior) / np.log(2.0)\n\n    def p_losses(self, x_start, cond, t, noise=None):\n        noise = default(noise, lambda: torch.randn_like(x_start))\n        x_noisy = self.q_sample(x_start=x_start, t=t, noise=noise)\n        model_output = self.apply_model(x_noisy, t, cond)\n\n        loss_dict = {}\n        prefix = 'train' if self.training else 'val'\n\n        if self.parameterization == \"x0\":\n            target = x_start\n        elif self.parameterization == \"eps\":\n            target = noise\n        else:\n            raise NotImplementedError()\n\n        loss_simple = self.get_loss(model_output, target, mean=False).mean([1, 2, 3])\n        loss_dict.update({f'{prefix}/loss_simple': loss_simple.mean()})\n\n        logvar_t = self.logvar[t].to(self.device)\n        loss = loss_simple / torch.exp(logvar_t) + logvar_t\n        # loss = loss_simple / torch.exp(self.logvar) + self.logvar\n        if self.learn_logvar:\n            loss_dict.update({f'{prefix}/loss_gamma': loss.mean()})\n            loss_dict.update({'logvar': self.logvar.data.mean()})\n\n        loss = self.l_simple_weight * loss.mean()\n\n        loss_vlb = self.get_loss(model_output, target, mean=False).mean(dim=(1, 2, 3))\n        loss_vlb = (self.lvlb_weights[t] * loss_vlb).mean()\n        loss_dict.update({f'{prefix}/loss_vlb': loss_vlb})\n        loss += (self.original_elbo_weight * loss_vlb)\n        loss_dict.update({f'{prefix}/loss': loss})\n\n        return loss, loss_dict\n\n    def p_mean_variance(self, x, c, t, clip_denoised: bool, return_codebook_ids=False, quantize_denoised=False,\n                        return_x0=False, score_corrector=None, corrector_kwargs=None):\n        t_in = t\n        model_out = self.apply_model(x, t_in, c, return_ids=return_codebook_ids)\n\n        if score_corrector is not None:\n            assert self.parameterization == \"eps\"\n            model_out = score_corrector.modify_score(self, model_out, x, t, c, **corrector_kwargs)\n\n        if return_codebook_ids:\n            model_out, logits = model_out\n\n        if self.parameterization == \"eps\":\n            x_recon = self.predict_start_from_noise(x, t=t, noise=model_out)\n        elif self.parameterization == \"x0\":\n            x_recon = model_out\n        else:\n            raise NotImplementedError()\n\n        if clip_denoised:\n            x_recon.clamp_(-1., 1.)\n        if quantize_denoised:\n            x_recon, _, [_, _, indices] = self.first_stage_model.quantize(x_recon)\n        model_mean, posterior_variance, posterior_log_variance = self.q_posterior(x_start=x_recon, x_t=x, t=t)\n        if return_codebook_ids:\n            return model_mean, posterior_variance, posterior_log_variance, logits\n        elif return_x0:\n            return model_mean, posterior_variance, posterior_log_variance, x_recon\n        else:\n            return model_mean, posterior_variance, posterior_log_variance\n\n    @torch.no_grad()\n    def p_sample(self, x, c, t, clip_denoised=False, repeat_noise=False,\n                 return_codebook_ids=False, quantize_denoised=False, return_x0=False,\n                 temperature=1., noise_dropout=0., score_corrector=None, corrector_kwargs=None):\n        b, *_, device = *x.shape, x.device\n        outputs = self.p_mean_variance(x=x, c=c, t=t, clip_denoised=clip_denoised,\n                                       return_codebook_ids=return_codebook_ids,\n                                       quantize_denoised=quantize_denoised,\n                                       return_x0=return_x0,\n                                       score_corrector=score_corrector, corrector_kwargs=corrector_kwargs)\n        if return_codebook_ids:\n            raise DeprecationWarning(\"Support dropped.\")\n            model_mean, _, model_log_variance, logits = outputs\n        elif return_x0:\n            model_mean, _, model_log_variance, x0 = outputs\n        else:\n            model_mean, _, model_log_variance = outputs\n\n        noise = noise_like(x.shape, device, repeat_noise) * temperature\n        if noise_dropout > 0.:\n            noise = torch.nn.functional.dropout(noise, p=noise_dropout)\n        # no noise when t == 0\n        nonzero_mask = (1 - (t == 0).float()).reshape(b, *((1,) * (len(x.shape) - 1)))\n\n        if return_codebook_ids:\n            return model_mean + nonzero_mask * (0.5 * model_log_variance).exp() * noise, logits.argmax(dim=1)\n        if return_x0:\n            return model_mean + nonzero_mask * (0.5 * model_log_variance).exp() * noise, x0\n        else:\n            return model_mean + nonzero_mask * (0.5 * model_log_variance).exp() * noise\n\n    @torch.no_grad()\n    def progressive_denoising(self, cond, shape, verbose=True, callback=None, quantize_denoised=False,\n                              img_callback=None, mask=None, x0=None, temperature=1., noise_dropout=0.,\n                              score_corrector=None, corrector_kwargs=None, batch_size=None, x_T=None, start_T=None,\n                              log_every_t=None):\n        if not log_every_t:\n            log_every_t = self.log_every_t\n        timesteps = self.num_timesteps\n        if batch_size is not None:\n            b = batch_size if batch_size is not None else shape[0]\n            shape = [batch_size] + list(shape)\n        else:\n            b = batch_size = shape[0]\n        if x_T is None:\n            img = torch.randn(shape, device=self.device)\n        else:\n            img = x_T\n        intermediates = []\n        if cond is not None:\n            if isinstance(cond, dict):\n                cond = {key: cond[key][:batch_size] if not isinstance(cond[key], list) else\n                [x[:batch_size] for x in cond[key]] for key in cond}\n            else:\n                cond = [c[:batch_size] for c in cond] if isinstance(cond, list) else cond[:batch_size]\n\n        if start_T is not None:\n            timesteps = min(timesteps, start_T)\n        iterator = tqdm(reversed(range(0, timesteps)), desc='Progressive Generation',\n                        total=timesteps) if verbose else reversed(\n            range(0, timesteps))\n        if type(temperature) == float:\n            temperature = [temperature] * timesteps\n\n        for i in iterator:\n            ts = torch.full((b,), i, device=self.device, dtype=torch.long)\n            if self.shorten_cond_schedule:\n                assert self.model.conditioning_key != 'hybrid'\n                tc = self.cond_ids[ts].to(cond.device)\n                cond = self.q_sample(x_start=cond, t=tc, noise=torch.randn_like(cond))\n\n            img, x0_partial = self.p_sample(img, cond, ts,\n                                            clip_denoised=self.clip_denoised,\n                                            quantize_denoised=quantize_denoised, return_x0=True,\n                                            temperature=temperature[i], noise_dropout=noise_dropout,\n                                            score_corrector=score_corrector, corrector_kwargs=corrector_kwargs)\n            if mask is not None:\n                assert x0 is not None\n                img_orig = self.q_sample(x0, ts)\n                img = img_orig * mask + (1. - mask) * img\n\n            if i % log_every_t == 0 or i == timesteps - 1:\n                intermediates.append(x0_partial)\n            if callback:\n                callback(i)\n            if img_callback:\n                img_callback(img, i)\n        return img, intermediates\n\n    @torch.no_grad()\n    def p_sample_loop(self, cond, shape, return_intermediates=False,\n                      x_T=None, verbose=True, callback=None, timesteps=None, quantize_denoised=False,\n                      mask=None, x0=None, img_callback=None, start_T=None,\n                      log_every_t=None):\n\n        if not log_every_t:\n            log_every_t = self.log_every_t\n        device = self.betas.device\n        b = shape[0]\n        if x_T is None:\n            img = torch.randn(shape, device=device)\n        else:\n            img = x_T\n\n        intermediates = [img]\n        if timesteps is None:\n            timesteps = self.num_timesteps\n\n        if start_T is not None:\n            timesteps = min(timesteps, start_T)\n        iterator = tqdm(reversed(range(0, timesteps)), desc='Sampling t', total=timesteps) if verbose else reversed(\n            range(0, timesteps))\n\n        if mask is not None:\n            assert x0 is not None\n            assert x0.shape[2:3] == mask.shape[2:3]  # spatial size has to match\n\n        for i in iterator:\n            ts = torch.full((b,), i, device=device, dtype=torch.long)\n            if self.shorten_cond_schedule:\n                assert self.model.conditioning_key != 'hybrid'\n                tc = self.cond_ids[ts].to(cond.device)\n                cond = self.q_sample(x_start=cond, t=tc, noise=torch.randn_like(cond))\n\n            img = self.p_sample(img, cond, ts,\n                                clip_denoised=self.clip_denoised,\n                                quantize_denoised=quantize_denoised)\n            if mask is not None:\n                img_orig = self.q_sample(x0, ts)\n                img = img_orig * mask + (1. - mask) * img\n\n            if i % log_every_t == 0 or i == timesteps - 1:\n                intermediates.append(img)\n            if callback:\n                callback(i)\n            if img_callback:\n                img_callback(img, i)\n\n        if return_intermediates:\n            return img, intermediates\n        return img\n\n    @torch.no_grad()\n    def sample(self, cond, batch_size=16, return_intermediates=False, x_T=None,\n               verbose=True, timesteps=None, quantize_denoised=False,\n               mask=None, x0=None, shape=None,**kwargs):\n        if shape is None:\n            shape = (batch_size, self.channels, self.image_size, self.image_size)\n        if cond is not None:\n            if isinstance(cond, dict):\n                cond = {key: cond[key][:batch_size] if not isinstance(cond[key], list) else\n                [x[:batch_size] for x in cond[key]] for key in cond}\n            else:\n                cond = [c[:batch_size] for c in cond] if isinstance(cond, list) else cond[:batch_size]\n        return self.p_sample_loop(cond,\n                                  shape,\n                                  return_intermediates=return_intermediates, x_T=x_T,\n                                  verbose=verbose, timesteps=timesteps, quantize_denoised=quantize_denoised,\n                                  mask=mask, x0=x0)\n\n    @torch.no_grad()\n    def sample_log(self,cond,batch_size,ddim, ddim_steps,**kwargs):\n\n        if ddim:\n            ddim_sampler = DDIMSampler(self)\n            shape = (self.channels, self.image_size, self.image_size)\n            samples, intermediates =ddim_sampler.sample(ddim_steps,batch_size,\n                                                        shape,cond,verbose=False,**kwargs)\n\n        else:\n            samples, intermediates = self.sample(cond=cond, batch_size=batch_size,\n                                                 return_intermediates=True,**kwargs)\n\n        return samples, intermediates\n\n\n    @torch.no_grad()\n    def log_images(self, batch, N=4, n_row=4, sample=True, ddim_steps=200, ddim_eta=1., return_keys=None,\n                   quantize_denoised=True, inpaint=False, plot_denoise_rows=False, plot_progressive_rows=False,\n                   plot_diffusion_rows=False, **kwargs):\n\n        use_ddim = False\n\n        log = {}\n        z, c, x, xrec, xc = self.get_input(batch, self.first_stage_key,\n                                           return_first_stage_outputs=True,\n                                           force_c_encode=True,\n                                           return_original_cond=True,\n                                           bs=N, uncond=0)\n        N = min(x.shape[0], N)\n        n_row = min(x.shape[0], n_row)\n        log[\"inputs\"] = x\n        log[\"reals\"] = xc[\"c_concat\"]\n        log[\"reconstruction\"] = xrec\n        if self.model.conditioning_key is not None:\n            if hasattr(self.cond_stage_model, \"decode\"):\n                xc = self.cond_stage_model.decode(c)\n                log[\"conditioning\"] = xc\n            elif self.cond_stage_key in [\"caption\"]:\n                xc = log_txt_as_img((x.shape[2], x.shape[3]), batch[\"caption\"])\n                log[\"conditioning\"] = xc\n            elif self.cond_stage_key == 'class_label':\n                xc = log_txt_as_img((x.shape[2], x.shape[3]), batch[\"human_label\"])\n                log['conditioning'] = xc\n            elif isimage(xc):\n                log[\"conditioning\"] = xc\n            if ismap(xc):\n                log[\"original_conditioning\"] = self.to_rgb(xc)\n\n        if plot_diffusion_rows:\n            # get diffusion row\n            diffusion_row = []\n            z_start = z[:n_row]\n            for t in range(self.num_timesteps):\n                if t % self.log_every_t == 0 or t == self.num_timesteps - 1:\n                    t = repeat(torch.tensor([t]), '1 -> b', b=n_row)\n                    t = t.to(self.device).long()\n                    noise = torch.randn_like(z_start)\n                    z_noisy = self.q_sample(x_start=z_start, t=t, noise=noise)\n                    diffusion_row.append(self.decode_first_stage(z_noisy))\n\n            diffusion_row = torch.stack(diffusion_row)  # n_log_step, n_row, C, H, W\n            diffusion_grid = rearrange(diffusion_row, 'n b c h w -> b n c h w')\n            diffusion_grid = rearrange(diffusion_grid, 'b n c h w -> (b n) c h w')\n            diffusion_grid = make_grid(diffusion_grid, nrow=diffusion_row.shape[0])\n            log[\"diffusion_row\"] = diffusion_grid\n\n        if sample:\n            # get denoise row\n            with self.ema_scope(\"Plotting\"):\n                samples, z_denoise_row = self.sample_log(cond=c,batch_size=N,ddim=use_ddim,\n                                                         ddim_steps=ddim_steps,eta=ddim_eta)\n                # samples, z_denoise_row = self.sample(cond=c, batch_size=N, return_intermediates=True)\n            x_samples = self.decode_first_stage(samples)\n            log[\"samples\"] = x_samples\n            if plot_denoise_rows:\n                denoise_grid = self._get_denoise_row_from_list(z_denoise_row)\n                log[\"denoise_row\"] = denoise_grid\n\n            if quantize_denoised and not isinstance(self.first_stage_model, AutoencoderKL) and not isinstance(\n                    self.first_stage_model, IdentityFirstStage):\n                # also display when quantizing x0 while sampling\n                with self.ema_scope(\"Plotting Quantized Denoised\"):\n                    samples, z_denoise_row = self.sample_log(cond=c,batch_size=N,ddim=use_ddim,\n                                                             ddim_steps=ddim_steps,eta=ddim_eta,\n                                                             quantize_denoised=True)\n                    # samples, z_denoise_row = self.sample(cond=c, batch_size=N, return_intermediates=True,\n                    #                                      quantize_denoised=True)\n                x_samples = self.decode_first_stage(samples.to(self.device))\n                log[\"samples_x0_quantized\"] = x_samples\n\n            if inpaint:\n                # make a simple center square\n                h, w = z.shape[2], z.shape[3]\n                mask = torch.ones(N, h, w).to(self.device)\n                # zeros will be filled in\n                mask[:, h // 4:3 * h // 4, w // 4:3 * w // 4] = 0.\n                mask = mask[:, None, ...]\n                with self.ema_scope(\"Plotting Inpaint\"):\n\n                    samples, _ = self.sample_log(cond=c,batch_size=N,ddim=use_ddim, eta=ddim_eta,\n                                                ddim_steps=ddim_steps, x0=z[:N], mask=mask)\n                x_samples = self.decode_first_stage(samples.to(self.device))\n                log[\"samples_inpainting\"] = x_samples\n                log[\"mask\"] = mask\n\n                # outpaint\n                with self.ema_scope(\"Plotting Outpaint\"):\n                    samples, _ = self.sample_log(cond=c, batch_size=N, ddim=use_ddim,eta=ddim_eta,\n                                                ddim_steps=ddim_steps, x0=z[:N], mask=mask)\n                x_samples = self.decode_first_stage(samples.to(self.device))\n                log[\"samples_outpainting\"] = x_samples\n\n        if plot_progressive_rows:\n            with self.ema_scope(\"Plotting Progressives\"):\n                img, progressives = self.progressive_denoising(c,\n                                                               shape=(self.channels, self.image_size, self.image_size),\n                                                               batch_size=N)\n            prog_row = self._get_denoise_row_from_list(progressives, desc=\"Progressive Generation\")\n            log[\"progressive_row\"] = prog_row\n\n        if return_keys:\n            if np.intersect1d(list(log.keys()), return_keys).shape[0] == 0:\n                return log\n            else:\n                return {key: log[key] for key in return_keys}\n        return log\n\n    def configure_optimizers(self):\n        lr = self.learning_rate\n        params = list(self.model.parameters())\n        if self.cond_stage_trainable:\n            print(f\"{self.__class__.__name__}: Also optimizing conditioner params!\")\n            params = params + list(self.cond_stage_model.parameters())\n        if self.learn_logvar:\n            print('Diffusion model optimizing logvar')\n            params.append(self.logvar)\n        opt = torch.optim.AdamW(params, lr=lr)\n        if self.use_scheduler:\n            assert 'target' in self.scheduler_config\n            scheduler = instantiate_from_config(self.scheduler_config)\n\n            print(\"Setting up LambdaLR scheduler...\")\n            scheduler = [\n                {\n                    'scheduler': LambdaLR(opt, lr_lambda=scheduler.schedule),\n                    'interval': 'step',\n                    'frequency': 1\n                }]\n            return [opt], scheduler\n        return opt\n\n    @torch.no_grad()\n    def to_rgb(self, x):\n        x = x.float()\n        if not hasattr(self, \"colorize\"):\n            self.colorize = torch.randn(3, x.shape[1], 1, 1).to(x)\n        x = nn.functional.conv2d(x, weight=self.colorize)\n        x = 2. * (x - x.min()) / (x.max() - x.min()) - 1.\n        return x\n\n\nclass DiffusionWrapper(pl.LightningModule):\n    def __init__(self, diff_model_config, conditioning_key):\n        super().__init__()\n        self.diffusion_model = instantiate_from_config(diff_model_config)\n        self.conditioning_key = conditioning_key\n        assert self.conditioning_key in [None, 'concat', 'crossattn', 'hybrid', 'adm']\n\n    def forward(self, x, t, c_concat: list = None, c_crossattn: list = None):\n        if self.conditioning_key is None:\n            out = self.diffusion_model(x, t)\n        elif self.conditioning_key == 'concat':\n            xc = torch.cat([x] + c_concat, dim=1)\n            out = self.diffusion_model(xc, t)\n        elif self.conditioning_key == 'crossattn':\n            cc = torch.cat(c_crossattn, 1)\n            out = self.diffusion_model(x, t, context=cc)\n        elif self.conditioning_key == 'hybrid':\n            xc = torch.cat([x] + c_concat, dim=1)\n            cc = torch.cat(c_crossattn, 1)\n            out = self.diffusion_model(xc, t, context=cc)\n        elif self.conditioning_key == 'adm':\n            cc = c_crossattn[0]\n            out = self.diffusion_model(x, t, y=cc)\n        else:\n            raise NotImplementedError()\n\n        return out\n\n\nclass Layout2ImgDiffusion(LatentDiffusion):\n    # TODO: move all layout-specific hacks to this class\n    def __init__(self, cond_stage_key, *args, **kwargs):\n        assert cond_stage_key == 'coordinates_bbox', 'Layout2ImgDiffusion only for cond_stage_key=\"coordinates_bbox\"'\n        super().__init__(*args, cond_stage_key=cond_stage_key, **kwargs)\n\n    def log_images(self, batch, N=8, *args, **kwargs):\n        logs = super().log_images(*args, batch=batch, N=N, **kwargs)\n\n        key = 'train' if self.training else 'validation'\n        dset = self.trainer.datamodule.datasets[key]\n        mapper = dset.conditional_builders[self.cond_stage_key]\n\n        bbox_imgs = []\n        map_fn = lambda catno: dset.get_textual_label(dset.get_category_id(catno))\n        for tknzd_bbox in batch[self.cond_stage_key][:N]:\n            bboximg = mapper.plot(tknzd_bbox.detach().cpu(), map_fn, (256, 256))\n            bbox_imgs.append(bboximg)\n\n        cond_img = torch.stack(bbox_imgs, dim=0)\n        logs['bbox_image'] = cond_img\n        return logs\n", "modules/models/diffusion/uni_pc/uni_pc.py": "import torch\nimport math\nimport tqdm\n\n\nclass NoiseScheduleVP:\n    def __init__(\n            self,\n            schedule='discrete',\n            betas=None,\n            alphas_cumprod=None,\n            continuous_beta_0=0.1,\n            continuous_beta_1=20.,\n        ):\n        \"\"\"Create a wrapper class for the forward SDE (VP type).\n\n        ***\n        Update: We support discrete-time diffusion models by implementing a picewise linear interpolation for log_alpha_t.\n                We recommend to use schedule='discrete' for the discrete-time diffusion models, especially for high-resolution images.\n        ***\n\n        The forward SDE ensures that the condition distribution q_{t|0}(x_t | x_0) = N ( alpha_t * x_0, sigma_t^2 * I ).\n        We further define lambda_t = log(alpha_t) - log(sigma_t), which is the half-logSNR (described in the DPM-Solver paper).\n        Therefore, we implement the functions for computing alpha_t, sigma_t and lambda_t. For t in [0, T], we have:\n\n            log_alpha_t = self.marginal_log_mean_coeff(t)\n            sigma_t = self.marginal_std(t)\n            lambda_t = self.marginal_lambda(t)\n\n        Moreover, as lambda(t) is an invertible function, we also support its inverse function:\n\n            t = self.inverse_lambda(lambda_t)\n\n        ===============================================================\n\n        We support both discrete-time DPMs (trained on n = 0, 1, ..., N-1) and continuous-time DPMs (trained on t in [t_0, T]).\n\n        1. For discrete-time DPMs:\n\n            For discrete-time DPMs trained on n = 0, 1, ..., N-1, we convert the discrete steps to continuous time steps by:\n                t_i = (i + 1) / N\n            e.g. for N = 1000, we have t_0 = 1e-3 and T = t_{N-1} = 1.\n            We solve the corresponding diffusion ODE from time T = 1 to time t_0 = 1e-3.\n\n            Args:\n                betas: A `torch.Tensor`. The beta array for the discrete-time DPM. (See the original DDPM paper for details)\n                alphas_cumprod: A `torch.Tensor`. The cumprod alphas for the discrete-time DPM. (See the original DDPM paper for details)\n\n            Note that we always have alphas_cumprod = cumprod(betas). Therefore, we only need to set one of `betas` and `alphas_cumprod`.\n\n            **Important**:  Please pay special attention for the args for `alphas_cumprod`:\n                The `alphas_cumprod` is the \\hat{alpha_n} arrays in the notations of DDPM. Specifically, DDPMs assume that\n                    q_{t_n | 0}(x_{t_n} | x_0) = N ( \\sqrt{\\hat{alpha_n}} * x_0, (1 - \\hat{alpha_n}) * I ).\n                Therefore, the notation \\hat{alpha_n} is different from the notation alpha_t in DPM-Solver. In fact, we have\n                    alpha_{t_n} = \\sqrt{\\hat{alpha_n}},\n                and\n                    log(alpha_{t_n}) = 0.5 * log(\\hat{alpha_n}).\n\n\n        2. For continuous-time DPMs:\n\n            We support two types of VPSDEs: linear (DDPM) and cosine (improved-DDPM). The hyperparameters for the noise\n            schedule are the default settings in DDPM and improved-DDPM:\n\n            Args:\n                beta_min: A `float` number. The smallest beta for the linear schedule.\n                beta_max: A `float` number. The largest beta for the linear schedule.\n                cosine_s: A `float` number. The hyperparameter in the cosine schedule.\n                cosine_beta_max: A `float` number. The hyperparameter in the cosine schedule.\n                T: A `float` number. The ending time of the forward process.\n\n        ===============================================================\n\n        Args:\n            schedule: A `str`. The noise schedule of the forward SDE. 'discrete' for discrete-time DPMs,\n                    'linear' or 'cosine' for continuous-time DPMs.\n        Returns:\n            A wrapper object of the forward SDE (VP type).\n\n        ===============================================================\n\n        Example:\n\n        # For discrete-time DPMs, given betas (the beta array for n = 0, 1, ..., N - 1):\n        >>> ns = NoiseScheduleVP('discrete', betas=betas)\n\n        # For discrete-time DPMs, given alphas_cumprod (the \\hat{alpha_n} array for n = 0, 1, ..., N - 1):\n        >>> ns = NoiseScheduleVP('discrete', alphas_cumprod=alphas_cumprod)\n\n        # For continuous-time DPMs (VPSDE), linear schedule:\n        >>> ns = NoiseScheduleVP('linear', continuous_beta_0=0.1, continuous_beta_1=20.)\n\n        \"\"\"\n\n        if schedule not in ['discrete', 'linear', 'cosine']:\n            raise ValueError(f\"Unsupported noise schedule {schedule}. The schedule needs to be 'discrete' or 'linear' or 'cosine'\")\n\n        self.schedule = schedule\n        if schedule == 'discrete':\n            if betas is not None:\n                log_alphas = 0.5 * torch.log(1 - betas).cumsum(dim=0)\n            else:\n                assert alphas_cumprod is not None\n                log_alphas = 0.5 * torch.log(alphas_cumprod)\n            self.total_N = len(log_alphas)\n            self.T = 1.\n            self.t_array = torch.linspace(0., 1., self.total_N + 1)[1:].reshape((1, -1))\n            self.log_alpha_array = log_alphas.reshape((1, -1,))\n        else:\n            self.total_N = 1000\n            self.beta_0 = continuous_beta_0\n            self.beta_1 = continuous_beta_1\n            self.cosine_s = 0.008\n            self.cosine_beta_max = 999.\n            self.cosine_t_max = math.atan(self.cosine_beta_max * (1. + self.cosine_s) / math.pi) * 2. * (1. + self.cosine_s) / math.pi - self.cosine_s\n            self.cosine_log_alpha_0 = math.log(math.cos(self.cosine_s / (1. + self.cosine_s) * math.pi / 2.))\n            self.schedule = schedule\n            if schedule == 'cosine':\n                # For the cosine schedule, T = 1 will have numerical issues. So we manually set the ending time T.\n                # Note that T = 0.9946 may be not the optimal setting. However, we find it works well.\n                self.T = 0.9946\n            else:\n                self.T = 1.\n\n    def marginal_log_mean_coeff(self, t):\n        \"\"\"\n        Compute log(alpha_t) of a given continuous-time label t in [0, T].\n        \"\"\"\n        if self.schedule == 'discrete':\n            return interpolate_fn(t.reshape((-1, 1)), self.t_array.to(t.device), self.log_alpha_array.to(t.device)).reshape((-1))\n        elif self.schedule == 'linear':\n            return -0.25 * t ** 2 * (self.beta_1 - self.beta_0) - 0.5 * t * self.beta_0\n        elif self.schedule == 'cosine':\n            log_alpha_fn = lambda s: torch.log(torch.cos((s + self.cosine_s) / (1. + self.cosine_s) * math.pi / 2.))\n            log_alpha_t =  log_alpha_fn(t) - self.cosine_log_alpha_0\n            return log_alpha_t\n\n    def marginal_alpha(self, t):\n        \"\"\"\n        Compute alpha_t of a given continuous-time label t in [0, T].\n        \"\"\"\n        return torch.exp(self.marginal_log_mean_coeff(t))\n\n    def marginal_std(self, t):\n        \"\"\"\n        Compute sigma_t of a given continuous-time label t in [0, T].\n        \"\"\"\n        return torch.sqrt(1. - torch.exp(2. * self.marginal_log_mean_coeff(t)))\n\n    def marginal_lambda(self, t):\n        \"\"\"\n        Compute lambda_t = log(alpha_t) - log(sigma_t) of a given continuous-time label t in [0, T].\n        \"\"\"\n        log_mean_coeff = self.marginal_log_mean_coeff(t)\n        log_std = 0.5 * torch.log(1. - torch.exp(2. * log_mean_coeff))\n        return log_mean_coeff - log_std\n\n    def inverse_lambda(self, lamb):\n        \"\"\"\n        Compute the continuous-time label t in [0, T] of a given half-logSNR lambda_t.\n        \"\"\"\n        if self.schedule == 'linear':\n            tmp = 2. * (self.beta_1 - self.beta_0) * torch.logaddexp(-2. * lamb, torch.zeros((1,)).to(lamb))\n            Delta = self.beta_0**2 + tmp\n            return tmp / (torch.sqrt(Delta) + self.beta_0) / (self.beta_1 - self.beta_0)\n        elif self.schedule == 'discrete':\n            log_alpha = -0.5 * torch.logaddexp(torch.zeros((1,)).to(lamb.device), -2. * lamb)\n            t = interpolate_fn(log_alpha.reshape((-1, 1)), torch.flip(self.log_alpha_array.to(lamb.device), [1]), torch.flip(self.t_array.to(lamb.device), [1]))\n            return t.reshape((-1,))\n        else:\n            log_alpha = -0.5 * torch.logaddexp(-2. * lamb, torch.zeros((1,)).to(lamb))\n            t_fn = lambda log_alpha_t: torch.arccos(torch.exp(log_alpha_t + self.cosine_log_alpha_0)) * 2. * (1. + self.cosine_s) / math.pi - self.cosine_s\n            t = t_fn(log_alpha)\n            return t\n\n\ndef model_wrapper(\n    model,\n    noise_schedule,\n    model_type=\"noise\",\n    model_kwargs=None,\n    guidance_type=\"uncond\",\n    #condition=None,\n    #unconditional_condition=None,\n    guidance_scale=1.,\n    classifier_fn=None,\n    classifier_kwargs=None,\n):\n    \"\"\"Create a wrapper function for the noise prediction model.\n\n    DPM-Solver needs to solve the continuous-time diffusion ODEs. For DPMs trained on discrete-time labels, we need to\n    firstly wrap the model function to a noise prediction model that accepts the continuous time as the input.\n\n    We support four types of the diffusion model by setting `model_type`:\n\n        1. \"noise\": noise prediction model. (Trained by predicting noise).\n\n        2. \"x_start\": data prediction model. (Trained by predicting the data x_0 at time 0).\n\n        3. \"v\": velocity prediction model. (Trained by predicting the velocity).\n            The \"v\" prediction is derivation detailed in Appendix D of [1], and is used in Imagen-Video [2].\n\n            [1] Salimans, Tim, and Jonathan Ho. \"Progressive distillation for fast sampling of diffusion models.\"\n                arXiv preprint arXiv:2202.00512 (2022).\n            [2] Ho, Jonathan, et al. \"Imagen Video: High Definition Video Generation with Diffusion Models.\"\n                arXiv preprint arXiv:2210.02303 (2022).\n\n        4. \"score\": marginal score function. (Trained by denoising score matching).\n            Note that the score function and the noise prediction model follows a simple relationship:\n            ```\n                noise(x_t, t) = -sigma_t * score(x_t, t)\n            ```\n\n    We support three types of guided sampling by DPMs by setting `guidance_type`:\n        1. \"uncond\": unconditional sampling by DPMs.\n            The input `model` has the following format:\n            ``\n                model(x, t_input, **model_kwargs) -> noise | x_start | v | score\n            ``\n\n        2. \"classifier\": classifier guidance sampling [3] by DPMs and another classifier.\n            The input `model` has the following format:\n            ``\n                model(x, t_input, **model_kwargs) -> noise | x_start | v | score\n            ``\n\n            The input `classifier_fn` has the following format:\n            ``\n                classifier_fn(x, t_input, cond, **classifier_kwargs) -> logits(x, t_input, cond)\n            ``\n\n            [3] P. Dhariwal and A. Q. Nichol, \"Diffusion models beat GANs on image synthesis,\"\n                in Advances in Neural Information Processing Systems, vol. 34, 2021, pp. 8780-8794.\n\n        3. \"classifier-free\": classifier-free guidance sampling by conditional DPMs.\n            The input `model` has the following format:\n            ``\n                model(x, t_input, cond, **model_kwargs) -> noise | x_start | v | score\n            ``\n            And if cond == `unconditional_condition`, the model output is the unconditional DPM output.\n\n            [4] Ho, Jonathan, and Tim Salimans. \"Classifier-free diffusion guidance.\"\n                arXiv preprint arXiv:2207.12598 (2022).\n\n\n    The `t_input` is the time label of the model, which may be discrete-time labels (i.e. 0 to 999)\n    or continuous-time labels (i.e. epsilon to T).\n\n    We wrap the model function to accept only `x` and `t_continuous` as inputs, and outputs the predicted noise:\n    ``\n        def model_fn(x, t_continuous) -> noise:\n            t_input = get_model_input_time(t_continuous)\n            return noise_pred(model, x, t_input, **model_kwargs)\n    ``\n    where `t_continuous` is the continuous time labels (i.e. epsilon to T). And we use `model_fn` for DPM-Solver.\n\n    ===============================================================\n\n    Args:\n        model: A diffusion model with the corresponding format described above.\n        noise_schedule: A noise schedule object, such as NoiseScheduleVP.\n        model_type: A `str`. The parameterization type of the diffusion model.\n                    \"noise\" or \"x_start\" or \"v\" or \"score\".\n        model_kwargs: A `dict`. A dict for the other inputs of the model function.\n        guidance_type: A `str`. The type of the guidance for sampling.\n                    \"uncond\" or \"classifier\" or \"classifier-free\".\n        condition: A pytorch tensor. The condition for the guided sampling.\n                    Only used for \"classifier\" or \"classifier-free\" guidance type.\n        unconditional_condition: A pytorch tensor. The condition for the unconditional sampling.\n                    Only used for \"classifier-free\" guidance type.\n        guidance_scale: A `float`. The scale for the guided sampling.\n        classifier_fn: A classifier function. Only used for the classifier guidance.\n        classifier_kwargs: A `dict`. A dict for the other inputs of the classifier function.\n    Returns:\n        A noise prediction model that accepts the noised data and the continuous time as the inputs.\n    \"\"\"\n\n    model_kwargs = model_kwargs or {}\n    classifier_kwargs = classifier_kwargs or {}\n\n    def get_model_input_time(t_continuous):\n        \"\"\"\n        Convert the continuous-time `t_continuous` (in [epsilon, T]) to the model input time.\n        For discrete-time DPMs, we convert `t_continuous` in [1 / N, 1] to `t_input` in [0, 1000 * (N - 1) / N].\n        For continuous-time DPMs, we just use `t_continuous`.\n        \"\"\"\n        if noise_schedule.schedule == 'discrete':\n            return (t_continuous - 1. / noise_schedule.total_N) * 1000.\n        else:\n            return t_continuous\n\n    def noise_pred_fn(x, t_continuous, cond=None):\n        if t_continuous.reshape((-1,)).shape[0] == 1:\n            t_continuous = t_continuous.expand((x.shape[0]))\n        t_input = get_model_input_time(t_continuous)\n        if cond is None:\n            output = model(x, t_input, None, **model_kwargs)\n        else:\n            output = model(x, t_input, cond, **model_kwargs)\n        if model_type == \"noise\":\n            return output\n        elif model_type == \"x_start\":\n            alpha_t, sigma_t = noise_schedule.marginal_alpha(t_continuous), noise_schedule.marginal_std(t_continuous)\n            dims = x.dim()\n            return (x - expand_dims(alpha_t, dims) * output) / expand_dims(sigma_t, dims)\n        elif model_type == \"v\":\n            alpha_t, sigma_t = noise_schedule.marginal_alpha(t_continuous), noise_schedule.marginal_std(t_continuous)\n            dims = x.dim()\n            return expand_dims(alpha_t, dims) * output + expand_dims(sigma_t, dims) * x\n        elif model_type == \"score\":\n            sigma_t = noise_schedule.marginal_std(t_continuous)\n            dims = x.dim()\n            return -expand_dims(sigma_t, dims) * output\n\n    def cond_grad_fn(x, t_input, condition):\n        \"\"\"\n        Compute the gradient of the classifier, i.e. nabla_{x} log p_t(cond | x_t).\n        \"\"\"\n        with torch.enable_grad():\n            x_in = x.detach().requires_grad_(True)\n            log_prob = classifier_fn(x_in, t_input, condition, **classifier_kwargs)\n            return torch.autograd.grad(log_prob.sum(), x_in)[0]\n\n    def model_fn(x, t_continuous, condition, unconditional_condition):\n        \"\"\"\n        The noise predicition model function that is used for DPM-Solver.\n        \"\"\"\n        if t_continuous.reshape((-1,)).shape[0] == 1:\n            t_continuous = t_continuous.expand((x.shape[0]))\n        if guidance_type == \"uncond\":\n            return noise_pred_fn(x, t_continuous)\n        elif guidance_type == \"classifier\":\n            assert classifier_fn is not None\n            t_input = get_model_input_time(t_continuous)\n            cond_grad = cond_grad_fn(x, t_input, condition)\n            sigma_t = noise_schedule.marginal_std(t_continuous)\n            noise = noise_pred_fn(x, t_continuous)\n            return noise - guidance_scale * expand_dims(sigma_t, dims=cond_grad.dim()) * cond_grad\n        elif guidance_type == \"classifier-free\":\n            if guidance_scale == 1. or unconditional_condition is None:\n                return noise_pred_fn(x, t_continuous, cond=condition)\n            else:\n                x_in = torch.cat([x] * 2)\n                t_in = torch.cat([t_continuous] * 2)\n                if isinstance(condition, dict):\n                    assert isinstance(unconditional_condition, dict)\n                    c_in = {}\n                    for k in condition:\n                        if isinstance(condition[k], list):\n                            c_in[k] = [torch.cat([\n                                unconditional_condition[k][i],\n                                condition[k][i]]) for i in range(len(condition[k]))]\n                        else:\n                            c_in[k] = torch.cat([\n                                unconditional_condition[k],\n                                condition[k]])\n                elif isinstance(condition, list):\n                    c_in = []\n                    assert isinstance(unconditional_condition, list)\n                    for i in range(len(condition)):\n                        c_in.append(torch.cat([unconditional_condition[i], condition[i]]))\n                else:\n                    c_in = torch.cat([unconditional_condition, condition])\n                noise_uncond, noise = noise_pred_fn(x_in, t_in, cond=c_in).chunk(2)\n                return noise_uncond + guidance_scale * (noise - noise_uncond)\n\n    assert model_type in [\"noise\", \"x_start\", \"v\"]\n    assert guidance_type in [\"uncond\", \"classifier\", \"classifier-free\"]\n    return model_fn\n\n\nclass UniPC:\n    def __init__(\n        self,\n        model_fn,\n        noise_schedule,\n        predict_x0=True,\n        thresholding=False,\n        max_val=1.,\n        variant='bh1',\n        condition=None,\n        unconditional_condition=None,\n        before_sample=None,\n        after_sample=None,\n        after_update=None\n    ):\n        \"\"\"Construct a UniPC.\n\n        We support both data_prediction and noise_prediction.\n        \"\"\"\n        self.model_fn_ = model_fn\n        self.noise_schedule = noise_schedule\n        self.variant = variant\n        self.predict_x0 = predict_x0\n        self.thresholding = thresholding\n        self.max_val = max_val\n        self.condition = condition\n        self.unconditional_condition = unconditional_condition\n        self.before_sample = before_sample\n        self.after_sample = after_sample\n        self.after_update = after_update\n\n    def dynamic_thresholding_fn(self, x0, t=None):\n        \"\"\"\n        The dynamic thresholding method.\n        \"\"\"\n        dims = x0.dim()\n        p = self.dynamic_thresholding_ratio\n        s = torch.quantile(torch.abs(x0).reshape((x0.shape[0], -1)), p, dim=1)\n        s = expand_dims(torch.maximum(s, self.thresholding_max_val * torch.ones_like(s).to(s.device)), dims)\n        x0 = torch.clamp(x0, -s, s) / s\n        return x0\n\n    def model(self, x, t):\n        cond = self.condition\n        uncond = self.unconditional_condition\n        if self.before_sample is not None:\n            x, t, cond, uncond = self.before_sample(x, t, cond, uncond)\n        res = self.model_fn_(x, t, cond, uncond)\n        if self.after_sample is not None:\n            x, t, cond, uncond, res = self.after_sample(x, t, cond, uncond, res)\n\n        if isinstance(res, tuple):\n            # (None, pred_x0)\n            res = res[1]\n\n        return res\n\n    def noise_prediction_fn(self, x, t):\n        \"\"\"\n        Return the noise prediction model.\n        \"\"\"\n        return self.model(x, t)\n\n    def data_prediction_fn(self, x, t):\n        \"\"\"\n        Return the data prediction model (with thresholding).\n        \"\"\"\n        noise = self.noise_prediction_fn(x, t)\n        dims = x.dim()\n        alpha_t, sigma_t = self.noise_schedule.marginal_alpha(t), self.noise_schedule.marginal_std(t)\n        x0 = (x - expand_dims(sigma_t, dims) * noise) / expand_dims(alpha_t, dims)\n        if self.thresholding:\n            p = 0.995   # A hyperparameter in the paper of \"Imagen\" [1].\n            s = torch.quantile(torch.abs(x0).reshape((x0.shape[0], -1)), p, dim=1)\n            s = expand_dims(torch.maximum(s, self.max_val * torch.ones_like(s).to(s.device)), dims)\n            x0 = torch.clamp(x0, -s, s) / s\n        return x0\n\n    def model_fn(self, x, t):\n        \"\"\"\n        Convert the model to the noise prediction model or the data prediction model.\n        \"\"\"\n        if self.predict_x0:\n            return self.data_prediction_fn(x, t)\n        else:\n            return self.noise_prediction_fn(x, t)\n\n    def get_time_steps(self, skip_type, t_T, t_0, N, device):\n        \"\"\"Compute the intermediate time steps for sampling.\n        \"\"\"\n        if skip_type == 'logSNR':\n            lambda_T = self.noise_schedule.marginal_lambda(torch.tensor(t_T).to(device))\n            lambda_0 = self.noise_schedule.marginal_lambda(torch.tensor(t_0).to(device))\n            logSNR_steps = torch.linspace(lambda_T.cpu().item(), lambda_0.cpu().item(), N + 1).to(device)\n            return self.noise_schedule.inverse_lambda(logSNR_steps)\n        elif skip_type == 'time_uniform':\n            return torch.linspace(t_T, t_0, N + 1).to(device)\n        elif skip_type == 'time_quadratic':\n            t_order = 2\n            t = torch.linspace(t_T**(1. / t_order), t_0**(1. / t_order), N + 1).pow(t_order).to(device)\n            return t\n        else:\n            raise ValueError(f\"Unsupported skip_type {skip_type}, need to be 'logSNR' or 'time_uniform' or 'time_quadratic'\")\n\n    def get_orders_and_timesteps_for_singlestep_solver(self, steps, order, skip_type, t_T, t_0, device):\n        \"\"\"\n        Get the order of each step for sampling by the singlestep DPM-Solver.\n        \"\"\"\n        if order == 3:\n            K = steps // 3 + 1\n            if steps % 3 == 0:\n                orders = [3,] * (K - 2) + [2, 1]\n            elif steps % 3 == 1:\n                orders = [3,] * (K - 1) + [1]\n            else:\n                orders = [3,] * (K - 1) + [2]\n        elif order == 2:\n            if steps % 2 == 0:\n                K = steps // 2\n                orders = [2,] * K\n            else:\n                K = steps // 2 + 1\n                orders = [2,] * (K - 1) + [1]\n        elif order == 1:\n            K = steps\n            orders = [1,] * steps\n        else:\n            raise ValueError(\"'order' must be '1' or '2' or '3'.\")\n        if skip_type == 'logSNR':\n            # To reproduce the results in DPM-Solver paper\n            timesteps_outer = self.get_time_steps(skip_type, t_T, t_0, K, device)\n        else:\n            timesteps_outer = self.get_time_steps(skip_type, t_T, t_0, steps, device)[torch.cumsum(torch.tensor([0,] + orders), 0).to(device)]\n        return timesteps_outer, orders\n\n    def denoise_to_zero_fn(self, x, s):\n        \"\"\"\n        Denoise at the final step, which is equivalent to solve the ODE from lambda_s to infty by first-order discretization.\n        \"\"\"\n        return self.data_prediction_fn(x, s)\n\n    def multistep_uni_pc_update(self, x, model_prev_list, t_prev_list, t, order, **kwargs):\n        if len(t.shape) == 0:\n            t = t.view(-1)\n        if 'bh' in self.variant:\n            return self.multistep_uni_pc_bh_update(x, model_prev_list, t_prev_list, t, order, **kwargs)\n        else:\n            assert self.variant == 'vary_coeff'\n            return self.multistep_uni_pc_vary_update(x, model_prev_list, t_prev_list, t, order, **kwargs)\n\n    def multistep_uni_pc_vary_update(self, x, model_prev_list, t_prev_list, t, order, use_corrector=True):\n        #print(f'using unified predictor-corrector with order {order} (solver type: vary coeff)')\n        ns = self.noise_schedule\n        assert order <= len(model_prev_list)\n\n        # first compute rks\n        t_prev_0 = t_prev_list[-1]\n        lambda_prev_0 = ns.marginal_lambda(t_prev_0)\n        lambda_t = ns.marginal_lambda(t)\n        model_prev_0 = model_prev_list[-1]\n        sigma_prev_0, sigma_t = ns.marginal_std(t_prev_0), ns.marginal_std(t)\n        log_alpha_t = ns.marginal_log_mean_coeff(t)\n        alpha_t = torch.exp(log_alpha_t)\n\n        h = lambda_t - lambda_prev_0\n\n        rks = []\n        D1s = []\n        for i in range(1, order):\n            t_prev_i = t_prev_list[-(i + 1)]\n            model_prev_i = model_prev_list[-(i + 1)]\n            lambda_prev_i = ns.marginal_lambda(t_prev_i)\n            rk = (lambda_prev_i - lambda_prev_0) / h\n            rks.append(rk)\n            D1s.append((model_prev_i - model_prev_0) / rk)\n\n        rks.append(1.)\n        rks = torch.tensor(rks, device=x.device)\n\n        K = len(rks)\n        # build C matrix\n        C = []\n\n        col = torch.ones_like(rks)\n        for k in range(1, K + 1):\n            C.append(col)\n            col = col * rks / (k + 1)\n        C = torch.stack(C, dim=1)\n\n        if len(D1s) > 0:\n            D1s = torch.stack(D1s, dim=1) # (B, K)\n            C_inv_p = torch.linalg.inv(C[:-1, :-1])\n            A_p = C_inv_p\n\n        if use_corrector:\n            #print('using corrector')\n            C_inv = torch.linalg.inv(C)\n            A_c = C_inv\n\n        hh = -h if self.predict_x0 else h\n        h_phi_1 = torch.expm1(hh)\n        h_phi_ks = []\n        factorial_k = 1\n        h_phi_k = h_phi_1\n        for k in range(1, K + 2):\n            h_phi_ks.append(h_phi_k)\n            h_phi_k = h_phi_k / hh - 1 / factorial_k\n            factorial_k *= (k + 1)\n\n        model_t = None\n        if self.predict_x0:\n            x_t_ = (\n                sigma_t / sigma_prev_0 * x\n                - alpha_t * h_phi_1 * model_prev_0\n            )\n            # now predictor\n            x_t = x_t_\n            if len(D1s) > 0:\n                # compute the residuals for predictor\n                for k in range(K - 1):\n                    x_t = x_t - alpha_t * h_phi_ks[k + 1] * torch.einsum('bkchw,k->bchw', D1s, A_p[k])\n            # now corrector\n            if use_corrector:\n                model_t = self.model_fn(x_t, t)\n                D1_t = (model_t - model_prev_0)\n                x_t = x_t_\n                k = 0\n                for k in range(K - 1):\n                    x_t = x_t - alpha_t * h_phi_ks[k + 1] * torch.einsum('bkchw,k->bchw', D1s, A_c[k][:-1])\n                x_t = x_t - alpha_t * h_phi_ks[K] * (D1_t * A_c[k][-1])\n        else:\n            log_alpha_prev_0, log_alpha_t = ns.marginal_log_mean_coeff(t_prev_0), ns.marginal_log_mean_coeff(t)\n            x_t_ = (\n                (torch.exp(log_alpha_t - log_alpha_prev_0)) * x\n                - (sigma_t * h_phi_1) * model_prev_0\n            )\n            # now predictor\n            x_t = x_t_\n            if len(D1s) > 0:\n                # compute the residuals for predictor\n                for k in range(K - 1):\n                    x_t = x_t - sigma_t * h_phi_ks[k + 1] * torch.einsum('bkchw,k->bchw', D1s, A_p[k])\n            # now corrector\n            if use_corrector:\n                model_t = self.model_fn(x_t, t)\n                D1_t = (model_t - model_prev_0)\n                x_t = x_t_\n                k = 0\n                for k in range(K - 1):\n                    x_t = x_t - sigma_t * h_phi_ks[k + 1] * torch.einsum('bkchw,k->bchw', D1s, A_c[k][:-1])\n                x_t = x_t - sigma_t * h_phi_ks[K] * (D1_t * A_c[k][-1])\n        return x_t, model_t\n\n    def multistep_uni_pc_bh_update(self, x, model_prev_list, t_prev_list, t, order, x_t=None, use_corrector=True):\n        #print(f'using unified predictor-corrector with order {order} (solver type: B(h))')\n        ns = self.noise_schedule\n        assert order <= len(model_prev_list)\n        dims = x.dim()\n\n        # first compute rks\n        t_prev_0 = t_prev_list[-1]\n        lambda_prev_0 = ns.marginal_lambda(t_prev_0)\n        lambda_t = ns.marginal_lambda(t)\n        model_prev_0 = model_prev_list[-1]\n        sigma_prev_0, sigma_t = ns.marginal_std(t_prev_0), ns.marginal_std(t)\n        log_alpha_prev_0, log_alpha_t = ns.marginal_log_mean_coeff(t_prev_0), ns.marginal_log_mean_coeff(t)\n        alpha_t = torch.exp(log_alpha_t)\n\n        h = lambda_t - lambda_prev_0\n\n        rks = []\n        D1s = []\n        for i in range(1, order):\n            t_prev_i = t_prev_list[-(i + 1)]\n            model_prev_i = model_prev_list[-(i + 1)]\n            lambda_prev_i = ns.marginal_lambda(t_prev_i)\n            rk = ((lambda_prev_i - lambda_prev_0) / h)[0]\n            rks.append(rk)\n            D1s.append((model_prev_i - model_prev_0) / rk)\n\n        rks.append(1.)\n        rks = torch.tensor(rks, device=x.device)\n\n        R = []\n        b = []\n\n        hh = -h[0] if self.predict_x0 else h[0]\n        h_phi_1 = torch.expm1(hh) # h\\phi_1(h) = e^h - 1\n        h_phi_k = h_phi_1 / hh - 1\n\n        factorial_i = 1\n\n        if self.variant == 'bh1':\n            B_h = hh\n        elif self.variant == 'bh2':\n            B_h = torch.expm1(hh)\n        else:\n            raise NotImplementedError()\n\n        for i in range(1, order + 1):\n            R.append(torch.pow(rks, i - 1))\n            b.append(h_phi_k * factorial_i / B_h)\n            factorial_i *= (i + 1)\n            h_phi_k = h_phi_k / hh - 1 / factorial_i\n\n        R = torch.stack(R)\n        b = torch.tensor(b, device=x.device)\n\n        # now predictor\n        use_predictor = len(D1s) > 0 and x_t is None\n        if len(D1s) > 0:\n            D1s = torch.stack(D1s, dim=1) # (B, K)\n            if x_t is None:\n                # for order 2, we use a simplified version\n                if order == 2:\n                    rhos_p = torch.tensor([0.5], device=b.device)\n                else:\n                    rhos_p = torch.linalg.solve(R[:-1, :-1], b[:-1])\n        else:\n            D1s = None\n\n        if use_corrector:\n            #print('using corrector')\n            # for order 1, we use a simplified version\n            if order == 1:\n                rhos_c = torch.tensor([0.5], device=b.device)\n            else:\n                rhos_c = torch.linalg.solve(R, b)\n\n        model_t = None\n        if self.predict_x0:\n            x_t_ = (\n                expand_dims(sigma_t / sigma_prev_0, dims) * x\n                - expand_dims(alpha_t * h_phi_1, dims)* model_prev_0\n            )\n\n            if x_t is None:\n                if use_predictor:\n                    pred_res = torch.einsum('k,bkchw->bchw', rhos_p, D1s)\n                else:\n                    pred_res = 0\n                x_t = x_t_ - expand_dims(alpha_t * B_h, dims) * pred_res\n\n            if use_corrector:\n                model_t = self.model_fn(x_t, t)\n                if D1s is not None:\n                    corr_res = torch.einsum('k,bkchw->bchw', rhos_c[:-1], D1s)\n                else:\n                    corr_res = 0\n                D1_t = (model_t - model_prev_0)\n                x_t = x_t_ - expand_dims(alpha_t * B_h, dims) * (corr_res + rhos_c[-1] * D1_t)\n        else:\n            x_t_ = (\n                expand_dims(torch.exp(log_alpha_t - log_alpha_prev_0), dims) * x\n                - expand_dims(sigma_t * h_phi_1, dims) * model_prev_0\n            )\n            if x_t is None:\n                if use_predictor:\n                    pred_res = torch.einsum('k,bkchw->bchw', rhos_p, D1s)\n                else:\n                    pred_res = 0\n                x_t = x_t_ - expand_dims(sigma_t * B_h, dims) * pred_res\n\n            if use_corrector:\n                model_t = self.model_fn(x_t, t)\n                if D1s is not None:\n                    corr_res = torch.einsum('k,bkchw->bchw', rhos_c[:-1], D1s)\n                else:\n                    corr_res = 0\n                D1_t = (model_t - model_prev_0)\n                x_t = x_t_ - expand_dims(sigma_t * B_h, dims) * (corr_res + rhos_c[-1] * D1_t)\n        return x_t, model_t\n\n\n    def sample(self, x, steps=20, t_start=None, t_end=None, order=3, skip_type='time_uniform',\n        method='singlestep', lower_order_final=True, denoise_to_zero=False, solver_type='dpm_solver',\n        atol=0.0078, rtol=0.05, corrector=False,\n    ):\n        t_0 = 1. / self.noise_schedule.total_N if t_end is None else t_end\n        t_T = self.noise_schedule.T if t_start is None else t_start\n        device = x.device\n        if method == 'multistep':\n            assert steps >= order, \"UniPC order must be < sampling steps\"\n            timesteps = self.get_time_steps(skip_type=skip_type, t_T=t_T, t_0=t_0, N=steps, device=device)\n            #print(f\"Running UniPC Sampling with {timesteps.shape[0]} timesteps, order {order}\")\n            assert timesteps.shape[0] - 1 == steps\n            with torch.no_grad():\n                vec_t = timesteps[0].expand((x.shape[0]))\n                model_prev_list = [self.model_fn(x, vec_t)]\n                t_prev_list = [vec_t]\n                with tqdm.tqdm(total=steps) as pbar:\n                    # Init the first `order` values by lower order multistep DPM-Solver.\n                    for init_order in range(1, order):\n                        vec_t = timesteps[init_order].expand(x.shape[0])\n                        x, model_x = self.multistep_uni_pc_update(x, model_prev_list, t_prev_list, vec_t, init_order, use_corrector=True)\n                        if model_x is None:\n                            model_x = self.model_fn(x, vec_t)\n                        if self.after_update is not None:\n                            self.after_update(x, model_x)\n                        model_prev_list.append(model_x)\n                        t_prev_list.append(vec_t)\n                        pbar.update()\n\n                    for step in range(order, steps + 1):\n                        vec_t = timesteps[step].expand(x.shape[0])\n                        if lower_order_final:\n                            step_order = min(order, steps + 1 - step)\n                        else:\n                            step_order = order\n                        #print('this step order:', step_order)\n                        if step == steps:\n                            #print('do not run corrector at the last step')\n                            use_corrector = False\n                        else:\n                            use_corrector = True\n                        x, model_x =  self.multistep_uni_pc_update(x, model_prev_list, t_prev_list, vec_t, step_order, use_corrector=use_corrector)\n                        if self.after_update is not None:\n                            self.after_update(x, model_x)\n                        for i in range(order - 1):\n                            t_prev_list[i] = t_prev_list[i + 1]\n                            model_prev_list[i] = model_prev_list[i + 1]\n                        t_prev_list[-1] = vec_t\n                        # We do not need to evaluate the final model value.\n                        if step < steps:\n                            if model_x is None:\n                                model_x = self.model_fn(x, vec_t)\n                            model_prev_list[-1] = model_x\n                        pbar.update()\n        else:\n            raise NotImplementedError()\n        if denoise_to_zero:\n            x = self.denoise_to_zero_fn(x, torch.ones((x.shape[0],)).to(device) * t_0)\n        return x\n\n\n#############################################################\n# other utility functions\n#############################################################\n\ndef interpolate_fn(x, xp, yp):\n    \"\"\"\n    A piecewise linear function y = f(x), using xp and yp as keypoints.\n    We implement f(x) in a differentiable way (i.e. applicable for autograd).\n    The function f(x) is well-defined for all x-axis. (For x beyond the bounds of xp, we use the outmost points of xp to define the linear function.)\n\n    Args:\n        x: PyTorch tensor with shape [N, C], where N is the batch size, C is the number of channels (we use C = 1 for DPM-Solver).\n        xp: PyTorch tensor with shape [C, K], where K is the number of keypoints.\n        yp: PyTorch tensor with shape [C, K].\n    Returns:\n        The function values f(x), with shape [N, C].\n    \"\"\"\n    N, K = x.shape[0], xp.shape[1]\n    all_x = torch.cat([x.unsqueeze(2), xp.unsqueeze(0).repeat((N, 1, 1))], dim=2)\n    sorted_all_x, x_indices = torch.sort(all_x, dim=2)\n    x_idx = torch.argmin(x_indices, dim=2)\n    cand_start_idx = x_idx - 1\n    start_idx = torch.where(\n        torch.eq(x_idx, 0),\n        torch.tensor(1, device=x.device),\n        torch.where(\n            torch.eq(x_idx, K), torch.tensor(K - 2, device=x.device), cand_start_idx,\n        ),\n    )\n    end_idx = torch.where(torch.eq(start_idx, cand_start_idx), start_idx + 2, start_idx + 1)\n    start_x = torch.gather(sorted_all_x, dim=2, index=start_idx.unsqueeze(2)).squeeze(2)\n    end_x = torch.gather(sorted_all_x, dim=2, index=end_idx.unsqueeze(2)).squeeze(2)\n    start_idx2 = torch.where(\n        torch.eq(x_idx, 0),\n        torch.tensor(0, device=x.device),\n        torch.where(\n            torch.eq(x_idx, K), torch.tensor(K - 2, device=x.device), cand_start_idx,\n        ),\n    )\n    y_positions_expanded = yp.unsqueeze(0).expand(N, -1, -1)\n    start_y = torch.gather(y_positions_expanded, dim=2, index=start_idx2.unsqueeze(2)).squeeze(2)\n    end_y = torch.gather(y_positions_expanded, dim=2, index=(start_idx2 + 1).unsqueeze(2)).squeeze(2)\n    cand = start_y + (x - start_x) * (end_y - start_y) / (end_x - start_x)\n    return cand\n\n\ndef expand_dims(v, dims):\n    \"\"\"\n    Expand the tensor `v` to the dim `dims`.\n\n    Args:\n        `v`: a PyTorch tensor with shape [N].\n        `dim`: a `int`.\n    Returns:\n        a PyTorch tensor with shape [N, 1, 1, ..., 1] and the total dimension is `dims`.\n    \"\"\"\n    return v[(...,) + (None,)*(dims - 1)]\n", "modules/models/diffusion/uni_pc/sampler.py": "\"\"\"SAMPLING ONLY.\"\"\"\n\nimport torch\n\nfrom .uni_pc import NoiseScheduleVP, model_wrapper, UniPC\nfrom modules import shared, devices\n\n\nclass UniPCSampler(object):\n    def __init__(self, model, **kwargs):\n        super().__init__()\n        self.model = model\n        to_torch = lambda x: x.clone().detach().to(torch.float32).to(model.device)\n        self.before_sample = None\n        self.after_sample = None\n        self.register_buffer('alphas_cumprod', to_torch(model.alphas_cumprod))\n\n    def register_buffer(self, name, attr):\n        if type(attr) == torch.Tensor:\n            if attr.device != devices.device:\n                attr = attr.to(devices.device)\n        setattr(self, name, attr)\n\n    def set_hooks(self, before_sample, after_sample, after_update):\n        self.before_sample = before_sample\n        self.after_sample = after_sample\n        self.after_update = after_update\n\n    @torch.no_grad()\n    def sample(self,\n               S,\n               batch_size,\n               shape,\n               conditioning=None,\n               callback=None,\n               normals_sequence=None,\n               img_callback=None,\n               quantize_x0=False,\n               eta=0.,\n               mask=None,\n               x0=None,\n               temperature=1.,\n               noise_dropout=0.,\n               score_corrector=None,\n               corrector_kwargs=None,\n               verbose=True,\n               x_T=None,\n               log_every_t=100,\n               unconditional_guidance_scale=1.,\n               unconditional_conditioning=None,\n               # this has to come in the same format as the conditioning, # e.g. as encoded tokens, ...\n               **kwargs\n               ):\n        if conditioning is not None:\n            if isinstance(conditioning, dict):\n                ctmp = conditioning[list(conditioning.keys())[0]]\n                while isinstance(ctmp, list):\n                    ctmp = ctmp[0]\n                cbs = ctmp.shape[0]\n                if cbs != batch_size:\n                    print(f\"Warning: Got {cbs} conditionings but batch-size is {batch_size}\")\n\n            elif isinstance(conditioning, list):\n                for ctmp in conditioning:\n                    if ctmp.shape[0] != batch_size:\n                        print(f\"Warning: Got {cbs} conditionings but batch-size is {batch_size}\")\n\n            else:\n                if conditioning.shape[0] != batch_size:\n                    print(f\"Warning: Got {conditioning.shape[0]} conditionings but batch-size is {batch_size}\")\n\n        # sampling\n        C, H, W = shape\n        size = (batch_size, C, H, W)\n        # print(f'Data shape for UniPC sampling is {size}')\n\n        device = self.model.betas.device\n        if x_T is None:\n            img = torch.randn(size, device=device)\n        else:\n            img = x_T\n\n        ns = NoiseScheduleVP('discrete', alphas_cumprod=self.alphas_cumprod)\n\n        # SD 1.X is \"noise\", SD 2.X is \"v\"\n        model_type = \"v\" if self.model.parameterization == \"v\" else \"noise\"\n\n        model_fn = model_wrapper(\n            lambda x, t, c: self.model.apply_model(x, t, c),\n            ns,\n            model_type=model_type,\n            guidance_type=\"classifier-free\",\n            #condition=conditioning,\n            #unconditional_condition=unconditional_conditioning,\n            guidance_scale=unconditional_guidance_scale,\n        )\n\n        uni_pc = UniPC(model_fn, ns, predict_x0=True, thresholding=False, variant=shared.opts.uni_pc_variant, condition=conditioning, unconditional_condition=unconditional_conditioning, before_sample=self.before_sample, after_sample=self.after_sample, after_update=self.after_update)\n        x = uni_pc.sample(img, steps=S, skip_type=shared.opts.uni_pc_skip_type, method=\"multistep\", order=shared.opts.uni_pc_order, lower_order_final=shared.opts.uni_pc_lower_order_final)\n\n        return x.to(device), None\n", "modules/models/diffusion/uni_pc/__init__.py": "from .sampler import UniPCSampler  # noqa: F401\n", "modules/hypernetworks/hypernetwork.py": "import datetime\nimport glob\nimport html\nimport os\nimport inspect\nfrom contextlib import closing\n\nimport modules.textual_inversion.dataset\nimport torch\nimport tqdm\nfrom einops import rearrange, repeat\nfrom ldm.util import default\nfrom modules import devices, sd_models, shared, sd_samplers, hashes, sd_hijack_checkpoint, errors\nfrom modules.textual_inversion import textual_inversion, saving_settings\nfrom modules.textual_inversion.learn_schedule import LearnRateScheduler\nfrom torch import einsum\nfrom torch.nn.init import normal_, xavier_normal_, xavier_uniform_, kaiming_normal_, kaiming_uniform_, zeros_\n\nfrom collections import deque\nfrom statistics import stdev, mean\n\n\noptimizer_dict = {optim_name : cls_obj for optim_name, cls_obj in inspect.getmembers(torch.optim, inspect.isclass) if optim_name != \"Optimizer\"}\n\nclass HypernetworkModule(torch.nn.Module):\n    activation_dict = {\n        \"linear\": torch.nn.Identity,\n        \"relu\": torch.nn.ReLU,\n        \"leakyrelu\": torch.nn.LeakyReLU,\n        \"elu\": torch.nn.ELU,\n        \"swish\": torch.nn.Hardswish,\n        \"tanh\": torch.nn.Tanh,\n        \"sigmoid\": torch.nn.Sigmoid,\n    }\n    activation_dict.update({cls_name.lower(): cls_obj for cls_name, cls_obj in inspect.getmembers(torch.nn.modules.activation) if inspect.isclass(cls_obj) and cls_obj.__module__ == 'torch.nn.modules.activation'})\n\n    def __init__(self, dim, state_dict=None, layer_structure=None, activation_func=None, weight_init='Normal',\n                 add_layer_norm=False, activate_output=False, dropout_structure=None):\n        super().__init__()\n\n        self.multiplier = 1.0\n\n        assert layer_structure is not None, \"layer_structure must not be None\"\n        assert layer_structure[0] == 1, \"Multiplier Sequence should start with size 1!\"\n        assert layer_structure[-1] == 1, \"Multiplier Sequence should end with size 1!\"\n\n        linears = []\n        for i in range(len(layer_structure) - 1):\n\n            # Add a fully-connected layer\n            linears.append(torch.nn.Linear(int(dim * layer_structure[i]), int(dim * layer_structure[i+1])))\n\n            # Add an activation func except last layer\n            if activation_func == \"linear\" or activation_func is None or (i >= len(layer_structure) - 2 and not activate_output):\n                pass\n            elif activation_func in self.activation_dict:\n                linears.append(self.activation_dict[activation_func]())\n            else:\n                raise RuntimeError(f'hypernetwork uses an unsupported activation function: {activation_func}')\n\n            # Add layer normalization\n            if add_layer_norm:\n                linears.append(torch.nn.LayerNorm(int(dim * layer_structure[i+1])))\n\n            # Everything should be now parsed into dropout structure, and applied here.\n            # Since we only have dropouts after layers, dropout structure should start with 0 and end with 0.\n            if dropout_structure is not None and dropout_structure[i+1] > 0:\n                assert 0 < dropout_structure[i+1] < 1, \"Dropout probability should be 0 or float between 0 and 1!\"\n                linears.append(torch.nn.Dropout(p=dropout_structure[i+1]))\n            # Code explanation : [1, 2, 1] -> dropout is missing when last_layer_dropout is false. [1, 2, 2, 1] -> [0, 0.3, 0, 0], when its True, [0, 0.3, 0.3, 0].\n\n        self.linear = torch.nn.Sequential(*linears)\n\n        if state_dict is not None:\n            self.fix_old_state_dict(state_dict)\n            self.load_state_dict(state_dict)\n        else:\n            for layer in self.linear:\n                if type(layer) == torch.nn.Linear or type(layer) == torch.nn.LayerNorm:\n                    w, b = layer.weight.data, layer.bias.data\n                    if weight_init == \"Normal\" or type(layer) == torch.nn.LayerNorm:\n                        normal_(w, mean=0.0, std=0.01)\n                        normal_(b, mean=0.0, std=0)\n                    elif weight_init == 'XavierUniform':\n                        xavier_uniform_(w)\n                        zeros_(b)\n                    elif weight_init == 'XavierNormal':\n                        xavier_normal_(w)\n                        zeros_(b)\n                    elif weight_init == 'KaimingUniform':\n                        kaiming_uniform_(w, nonlinearity='leaky_relu' if 'leakyrelu' == activation_func else 'relu')\n                        zeros_(b)\n                    elif weight_init == 'KaimingNormal':\n                        kaiming_normal_(w, nonlinearity='leaky_relu' if 'leakyrelu' == activation_func else 'relu')\n                        zeros_(b)\n                    else:\n                        raise KeyError(f\"Key {weight_init} is not defined as initialization!\")\n        devices.torch_npu_set_device()\n        self.to(devices.device)\n\n    def fix_old_state_dict(self, state_dict):\n        changes = {\n            'linear1.bias': 'linear.0.bias',\n            'linear1.weight': 'linear.0.weight',\n            'linear2.bias': 'linear.1.bias',\n            'linear2.weight': 'linear.1.weight',\n        }\n\n        for fr, to in changes.items():\n            x = state_dict.get(fr, None)\n            if x is None:\n                continue\n\n            del state_dict[fr]\n            state_dict[to] = x\n\n    def forward(self, x):\n        return x + self.linear(x) * (self.multiplier if not self.training else 1)\n\n    def trainables(self):\n        layer_structure = []\n        for layer in self.linear:\n            if type(layer) == torch.nn.Linear or type(layer) == torch.nn.LayerNorm:\n                layer_structure += [layer.weight, layer.bias]\n        return layer_structure\n\n\n#param layer_structure : sequence used for length, use_dropout : controlling boolean, last_layer_dropout : for compatibility check.\ndef parse_dropout_structure(layer_structure, use_dropout, last_layer_dropout):\n    if layer_structure is None:\n        layer_structure = [1, 2, 1]\n    if not use_dropout:\n        return [0] * len(layer_structure)\n    dropout_values = [0]\n    dropout_values.extend([0.3] * (len(layer_structure) - 3))\n    if last_layer_dropout:\n        dropout_values.append(0.3)\n    else:\n        dropout_values.append(0)\n    dropout_values.append(0)\n    return dropout_values\n\n\nclass Hypernetwork:\n    filename = None\n    name = None\n\n    def __init__(self, name=None, enable_sizes=None, layer_structure=None, activation_func=None, weight_init=None, add_layer_norm=False, use_dropout=False, activate_output=False, **kwargs):\n        self.filename = None\n        self.name = name\n        self.layers = {}\n        self.step = 0\n        self.sd_checkpoint = None\n        self.sd_checkpoint_name = None\n        self.layer_structure = layer_structure\n        self.activation_func = activation_func\n        self.weight_init = weight_init\n        self.add_layer_norm = add_layer_norm\n        self.use_dropout = use_dropout\n        self.activate_output = activate_output\n        self.last_layer_dropout = kwargs.get('last_layer_dropout', True)\n        self.dropout_structure = kwargs.get('dropout_structure', None)\n        if self.dropout_structure is None:\n            self.dropout_structure = parse_dropout_structure(self.layer_structure, self.use_dropout, self.last_layer_dropout)\n        self.optimizer_name = None\n        self.optimizer_state_dict = None\n        self.optional_info = None\n\n        for size in enable_sizes or []:\n            self.layers[size] = (\n                HypernetworkModule(size, None, self.layer_structure, self.activation_func, self.weight_init,\n                                   self.add_layer_norm, self.activate_output, dropout_structure=self.dropout_structure),\n                HypernetworkModule(size, None, self.layer_structure, self.activation_func, self.weight_init,\n                                   self.add_layer_norm, self.activate_output, dropout_structure=self.dropout_structure),\n            )\n        self.eval()\n\n    def weights(self):\n        res = []\n        for layers in self.layers.values():\n            for layer in layers:\n                res += layer.parameters()\n        return res\n\n    def train(self, mode=True):\n        for layers in self.layers.values():\n            for layer in layers:\n                layer.train(mode=mode)\n                for param in layer.parameters():\n                    param.requires_grad = mode\n\n    def to(self, device):\n        for layers in self.layers.values():\n            for layer in layers:\n                layer.to(device)\n\n        return self\n\n    def set_multiplier(self, multiplier):\n        for layers in self.layers.values():\n            for layer in layers:\n                layer.multiplier = multiplier\n\n        return self\n\n    def eval(self):\n        for layers in self.layers.values():\n            for layer in layers:\n                layer.eval()\n                for param in layer.parameters():\n                    param.requires_grad = False\n\n    def save(self, filename):\n        state_dict = {}\n        optimizer_saved_dict = {}\n\n        for k, v in self.layers.items():\n            state_dict[k] = (v[0].state_dict(), v[1].state_dict())\n\n        state_dict['step'] = self.step\n        state_dict['name'] = self.name\n        state_dict['layer_structure'] = self.layer_structure\n        state_dict['activation_func'] = self.activation_func\n        state_dict['is_layer_norm'] = self.add_layer_norm\n        state_dict['weight_initialization'] = self.weight_init\n        state_dict['sd_checkpoint'] = self.sd_checkpoint\n        state_dict['sd_checkpoint_name'] = self.sd_checkpoint_name\n        state_dict['activate_output'] = self.activate_output\n        state_dict['use_dropout'] = self.use_dropout\n        state_dict['dropout_structure'] = self.dropout_structure\n        state_dict['last_layer_dropout'] = (self.dropout_structure[-2] != 0) if self.dropout_structure is not None else self.last_layer_dropout\n        state_dict['optional_info'] = self.optional_info if self.optional_info else None\n\n        if self.optimizer_name is not None:\n            optimizer_saved_dict['optimizer_name'] = self.optimizer_name\n\n        torch.save(state_dict, filename)\n        if shared.opts.save_optimizer_state and self.optimizer_state_dict:\n            optimizer_saved_dict['hash'] = self.shorthash()\n            optimizer_saved_dict['optimizer_state_dict'] = self.optimizer_state_dict\n            torch.save(optimizer_saved_dict, filename + '.optim')\n\n    def load(self, filename):\n        self.filename = filename\n        if self.name is None:\n            self.name = os.path.splitext(os.path.basename(filename))[0]\n\n        state_dict = torch.load(filename, map_location='cpu')\n\n        self.layer_structure = state_dict.get('layer_structure', [1, 2, 1])\n        self.optional_info = state_dict.get('optional_info', None)\n        self.activation_func = state_dict.get('activation_func', None)\n        self.weight_init = state_dict.get('weight_initialization', 'Normal')\n        self.add_layer_norm = state_dict.get('is_layer_norm', False)\n        self.dropout_structure = state_dict.get('dropout_structure', None)\n        self.use_dropout = True if self.dropout_structure is not None and any(self.dropout_structure) else state_dict.get('use_dropout', False)\n        self.activate_output = state_dict.get('activate_output', True)\n        self.last_layer_dropout = state_dict.get('last_layer_dropout', False)\n        # Dropout structure should have same length as layer structure, Every digits should be in [0,1), and last digit must be 0.\n        if self.dropout_structure is None:\n            self.dropout_structure = parse_dropout_structure(self.layer_structure, self.use_dropout, self.last_layer_dropout)\n\n        if shared.opts.print_hypernet_extra:\n            if self.optional_info is not None:\n                print(f\"  INFO:\\n {self.optional_info}\\n\")\n\n            print(f\"  Layer structure: {self.layer_structure}\")\n            print(f\"  Activation function: {self.activation_func}\")\n            print(f\"  Weight initialization: {self.weight_init}\")\n            print(f\"  Layer norm: {self.add_layer_norm}\")\n            print(f\"  Dropout usage: {self.use_dropout}\" )\n            print(f\"  Activate last layer: {self.activate_output}\")\n            print(f\"  Dropout structure: {self.dropout_structure}\")\n\n        optimizer_saved_dict = torch.load(self.filename + '.optim', map_location='cpu') if os.path.exists(self.filename + '.optim') else {}\n\n        if self.shorthash() == optimizer_saved_dict.get('hash', None):\n            self.optimizer_state_dict = optimizer_saved_dict.get('optimizer_state_dict', None)\n        else:\n            self.optimizer_state_dict = None\n        if self.optimizer_state_dict:\n            self.optimizer_name = optimizer_saved_dict.get('optimizer_name', 'AdamW')\n            if shared.opts.print_hypernet_extra:\n                print(\"Loaded existing optimizer from checkpoint\")\n                print(f\"Optimizer name is {self.optimizer_name}\")\n        else:\n            self.optimizer_name = \"AdamW\"\n            if shared.opts.print_hypernet_extra:\n                print(\"No saved optimizer exists in checkpoint\")\n\n        for size, sd in state_dict.items():\n            if type(size) == int:\n                self.layers[size] = (\n                    HypernetworkModule(size, sd[0], self.layer_structure, self.activation_func, self.weight_init,\n                                       self.add_layer_norm, self.activate_output, self.dropout_structure),\n                    HypernetworkModule(size, sd[1], self.layer_structure, self.activation_func, self.weight_init,\n                                       self.add_layer_norm, self.activate_output, self.dropout_structure),\n                )\n\n        self.name = state_dict.get('name', self.name)\n        self.step = state_dict.get('step', 0)\n        self.sd_checkpoint = state_dict.get('sd_checkpoint', None)\n        self.sd_checkpoint_name = state_dict.get('sd_checkpoint_name', None)\n        self.eval()\n\n    def shorthash(self):\n        sha256 = hashes.sha256(self.filename, f'hypernet/{self.name}')\n\n        return sha256[0:10] if sha256 else None\n\n\ndef list_hypernetworks(path):\n    res = {}\n    for filename in sorted(glob.iglob(os.path.join(path, '**/*.pt'), recursive=True), key=str.lower):\n        name = os.path.splitext(os.path.basename(filename))[0]\n        # Prevent a hypothetical \"None.pt\" from being listed.\n        if name != \"None\":\n            res[name] = filename\n    return res\n\n\ndef load_hypernetwork(name):\n    path = shared.hypernetworks.get(name, None)\n\n    if path is None:\n        return None\n\n    try:\n        hypernetwork = Hypernetwork()\n        hypernetwork.load(path)\n        return hypernetwork\n    except Exception:\n        errors.report(f\"Error loading hypernetwork {path}\", exc_info=True)\n        return None\n\n\ndef load_hypernetworks(names, multipliers=None):\n    already_loaded = {}\n\n    for hypernetwork in shared.loaded_hypernetworks:\n        if hypernetwork.name in names:\n            already_loaded[hypernetwork.name] = hypernetwork\n\n    shared.loaded_hypernetworks.clear()\n\n    for i, name in enumerate(names):\n        hypernetwork = already_loaded.get(name, None)\n        if hypernetwork is None:\n            hypernetwork = load_hypernetwork(name)\n\n        if hypernetwork is None:\n            continue\n\n        hypernetwork.set_multiplier(multipliers[i] if multipliers else 1.0)\n        shared.loaded_hypernetworks.append(hypernetwork)\n\n\ndef apply_single_hypernetwork(hypernetwork, context_k, context_v, layer=None):\n    hypernetwork_layers = (hypernetwork.layers if hypernetwork is not None else {}).get(context_k.shape[2], None)\n\n    if hypernetwork_layers is None:\n        return context_k, context_v\n\n    if layer is not None:\n        layer.hyper_k = hypernetwork_layers[0]\n        layer.hyper_v = hypernetwork_layers[1]\n\n    context_k = devices.cond_cast_unet(hypernetwork_layers[0](devices.cond_cast_float(context_k)))\n    context_v = devices.cond_cast_unet(hypernetwork_layers[1](devices.cond_cast_float(context_v)))\n    return context_k, context_v\n\n\ndef apply_hypernetworks(hypernetworks, context, layer=None):\n    context_k = context\n    context_v = context\n    for hypernetwork in hypernetworks:\n        context_k, context_v = apply_single_hypernetwork(hypernetwork, context_k, context_v, layer)\n\n    return context_k, context_v\n\n\ndef attention_CrossAttention_forward(self, x, context=None, mask=None, **kwargs):\n    h = self.heads\n\n    q = self.to_q(x)\n    context = default(context, x)\n\n    context_k, context_v = apply_hypernetworks(shared.loaded_hypernetworks, context, self)\n    k = self.to_k(context_k)\n    v = self.to_v(context_v)\n\n    q, k, v = (rearrange(t, 'b n (h d) -> (b h) n d', h=h) for t in (q, k, v))\n\n    sim = einsum('b i d, b j d -> b i j', q, k) * self.scale\n\n    if mask is not None:\n        mask = rearrange(mask, 'b ... -> b (...)')\n        max_neg_value = -torch.finfo(sim.dtype).max\n        mask = repeat(mask, 'b j -> (b h) () j', h=h)\n        sim.masked_fill_(~mask, max_neg_value)\n\n    # attention, what we cannot get enough of\n    attn = sim.softmax(dim=-1)\n\n    out = einsum('b i j, b j d -> b i d', attn, v)\n    out = rearrange(out, '(b h) n d -> b n (h d)', h=h)\n    return self.to_out(out)\n\n\ndef stack_conds(conds):\n    if len(conds) == 1:\n        return torch.stack(conds)\n\n    # same as in reconstruct_multicond_batch\n    token_count = max([x.shape[0] for x in conds])\n    for i in range(len(conds)):\n        if conds[i].shape[0] != token_count:\n            last_vector = conds[i][-1:]\n            last_vector_repeated = last_vector.repeat([token_count - conds[i].shape[0], 1])\n            conds[i] = torch.vstack([conds[i], last_vector_repeated])\n\n    return torch.stack(conds)\n\n\ndef statistics(data):\n    if len(data) < 2:\n        std = 0\n    else:\n        std = stdev(data)\n    total_information = f\"loss:{mean(data):.3f}\" + u\"\\u00B1\" + f\"({std/ (len(data) ** 0.5):.3f})\"\n    recent_data = data[-32:]\n    if len(recent_data) < 2:\n        std = 0\n    else:\n        std = stdev(recent_data)\n    recent_information = f\"recent 32 loss:{mean(recent_data):.3f}\" + u\"\\u00B1\" + f\"({std / (len(recent_data) ** 0.5):.3f})\"\n    return total_information, recent_information\n\n\ndef create_hypernetwork(name, enable_sizes, overwrite_old, layer_structure=None, activation_func=None, weight_init=None, add_layer_norm=False, use_dropout=False, dropout_structure=None):\n    # Remove illegal characters from name.\n    name = \"\".join( x for x in name if (x.isalnum() or x in \"._- \"))\n    assert name, \"Name cannot be empty!\"\n\n    fn = os.path.join(shared.cmd_opts.hypernetwork_dir, f\"{name}.pt\")\n    if not overwrite_old:\n        assert not os.path.exists(fn), f\"file {fn} already exists\"\n\n    if type(layer_structure) == str:\n        layer_structure = [float(x.strip()) for x in layer_structure.split(\",\")]\n\n    if use_dropout and dropout_structure and type(dropout_structure) == str:\n        dropout_structure = [float(x.strip()) for x in dropout_structure.split(\",\")]\n    else:\n        dropout_structure = [0] * len(layer_structure)\n\n    hypernet = modules.hypernetworks.hypernetwork.Hypernetwork(\n        name=name,\n        enable_sizes=[int(x) for x in enable_sizes],\n        layer_structure=layer_structure,\n        activation_func=activation_func,\n        weight_init=weight_init,\n        add_layer_norm=add_layer_norm,\n        use_dropout=use_dropout,\n        dropout_structure=dropout_structure\n    )\n    hypernet.save(fn)\n\n    shared.reload_hypernetworks()\n\n\ndef train_hypernetwork(id_task, hypernetwork_name: str, learn_rate: float, batch_size: int, gradient_step: int, data_root: str, log_directory: str, training_width: int, training_height: int, varsize: bool, steps: int, clip_grad_mode: str, clip_grad_value: float, shuffle_tags: bool, tag_drop_out: bool, latent_sampling_method: str, use_weight: bool, create_image_every: int, save_hypernetwork_every: int, template_filename: str, preview_from_txt2img: bool, preview_prompt: str, preview_negative_prompt: str, preview_steps: int, preview_sampler_name: str, preview_cfg_scale: float, preview_seed: int, preview_width: int, preview_height: int):\n    from modules import images, processing\n\n    save_hypernetwork_every = save_hypernetwork_every or 0\n    create_image_every = create_image_every or 0\n    template_file = textual_inversion.textual_inversion_templates.get(template_filename, None)\n    textual_inversion.validate_train_inputs(hypernetwork_name, learn_rate, batch_size, gradient_step, data_root, template_file, template_filename, steps, save_hypernetwork_every, create_image_every, log_directory, name=\"hypernetwork\")\n    template_file = template_file.path\n\n    path = shared.hypernetworks.get(hypernetwork_name, None)\n    hypernetwork = Hypernetwork()\n    hypernetwork.load(path)\n    shared.loaded_hypernetworks = [hypernetwork]\n\n    shared.state.job = \"train-hypernetwork\"\n    shared.state.textinfo = \"Initializing hypernetwork training...\"\n    shared.state.job_count = steps\n\n    hypernetwork_name = hypernetwork_name.rsplit('(', 1)[0]\n    filename = os.path.join(shared.cmd_opts.hypernetwork_dir, f'{hypernetwork_name}.pt')\n\n    log_directory = os.path.join(log_directory, datetime.datetime.now().strftime(\"%Y-%m-%d\"), hypernetwork_name)\n    unload = shared.opts.unload_models_when_training\n\n    if save_hypernetwork_every > 0:\n        hypernetwork_dir = os.path.join(log_directory, \"hypernetworks\")\n        os.makedirs(hypernetwork_dir, exist_ok=True)\n    else:\n        hypernetwork_dir = None\n\n    if create_image_every > 0:\n        images_dir = os.path.join(log_directory, \"images\")\n        os.makedirs(images_dir, exist_ok=True)\n    else:\n        images_dir = None\n\n    checkpoint = sd_models.select_checkpoint()\n\n    initial_step = hypernetwork.step or 0\n    if initial_step >= steps:\n        shared.state.textinfo = \"Model has already been trained beyond specified max steps\"\n        return hypernetwork, filename\n\n    scheduler = LearnRateScheduler(learn_rate, steps, initial_step)\n\n    clip_grad = torch.nn.utils.clip_grad_value_ if clip_grad_mode == \"value\" else torch.nn.utils.clip_grad_norm_ if clip_grad_mode == \"norm\" else None\n    if clip_grad:\n        clip_grad_sched = LearnRateScheduler(clip_grad_value, steps, initial_step, verbose=False)\n\n    if shared.opts.training_enable_tensorboard:\n        tensorboard_writer = textual_inversion.tensorboard_setup(log_directory)\n\n    # dataset loading may take a while, so input validations and early returns should be done before this\n    shared.state.textinfo = f\"Preparing dataset from {html.escape(data_root)}...\"\n\n    pin_memory = shared.opts.pin_memory\n\n    ds = modules.textual_inversion.dataset.PersonalizedBase(data_root=data_root, width=training_width, height=training_height, repeats=shared.opts.training_image_repeats_per_epoch, placeholder_token=hypernetwork_name, model=shared.sd_model, cond_model=shared.sd_model.cond_stage_model, device=devices.device, template_file=template_file, include_cond=True, batch_size=batch_size, gradient_step=gradient_step, shuffle_tags=shuffle_tags, tag_drop_out=tag_drop_out, latent_sampling_method=latent_sampling_method, varsize=varsize, use_weight=use_weight)\n\n    if shared.opts.save_training_settings_to_txt:\n        saved_params = dict(\n            model_name=checkpoint.model_name, model_hash=checkpoint.shorthash, num_of_dataset_images=len(ds),\n            **{field: getattr(hypernetwork, field) for field in ['layer_structure', 'activation_func', 'weight_init', 'add_layer_norm', 'use_dropout', ]}\n        )\n        saving_settings.save_settings_to_file(log_directory, {**saved_params, **locals()})\n\n    latent_sampling_method = ds.latent_sampling_method\n\n    dl = modules.textual_inversion.dataset.PersonalizedDataLoader(ds, latent_sampling_method=latent_sampling_method, batch_size=ds.batch_size, pin_memory=pin_memory)\n\n    old_parallel_processing_allowed = shared.parallel_processing_allowed\n\n    if unload:\n        shared.parallel_processing_allowed = False\n        shared.sd_model.cond_stage_model.to(devices.cpu)\n        shared.sd_model.first_stage_model.to(devices.cpu)\n\n    weights = hypernetwork.weights()\n    hypernetwork.train()\n\n    # Here we use optimizer from saved HN, or we can specify as UI option.\n    if hypernetwork.optimizer_name in optimizer_dict:\n        optimizer = optimizer_dict[hypernetwork.optimizer_name](params=weights, lr=scheduler.learn_rate)\n        optimizer_name = hypernetwork.optimizer_name\n    else:\n        print(f\"Optimizer type {hypernetwork.optimizer_name} is not defined!\")\n        optimizer = torch.optim.AdamW(params=weights, lr=scheduler.learn_rate)\n        optimizer_name = 'AdamW'\n\n    if hypernetwork.optimizer_state_dict:  # This line must be changed if Optimizer type can be different from saved optimizer.\n        try:\n            optimizer.load_state_dict(hypernetwork.optimizer_state_dict)\n        except RuntimeError as e:\n            print(\"Cannot resume from saved optimizer!\")\n            print(e)\n\n    scaler = torch.cuda.amp.GradScaler()\n\n    batch_size = ds.batch_size\n    gradient_step = ds.gradient_step\n    # n steps = batch_size * gradient_step * n image processed\n    steps_per_epoch = len(ds) // batch_size // gradient_step\n    max_steps_per_epoch = len(ds) // batch_size - (len(ds) // batch_size) % gradient_step\n    loss_step = 0\n    _loss_step = 0 #internal\n    # size = len(ds.indexes)\n    # loss_dict = defaultdict(lambda : deque(maxlen = 1024))\n    loss_logging = deque(maxlen=len(ds) * 3)  # this should be configurable parameter, this is 3 * epoch(dataset size)\n    # losses = torch.zeros((size,))\n    # previous_mean_losses = [0]\n    # previous_mean_loss = 0\n    # print(\"Mean loss of {} elements\".format(size))\n\n    steps_without_grad = 0\n\n    last_saved_file = \"<none>\"\n    last_saved_image = \"<none>\"\n    forced_filename = \"<none>\"\n\n    pbar = tqdm.tqdm(total=steps - initial_step)\n    try:\n        sd_hijack_checkpoint.add()\n\n        for _ in range((steps-initial_step) * gradient_step):\n            if scheduler.finished:\n                break\n            if shared.state.interrupted:\n                break\n            for j, batch in enumerate(dl):\n                # works as a drop_last=True for gradient accumulation\n                if j == max_steps_per_epoch:\n                    break\n                scheduler.apply(optimizer, hypernetwork.step)\n                if scheduler.finished:\n                    break\n                if shared.state.interrupted:\n                    break\n\n                if clip_grad:\n                    clip_grad_sched.step(hypernetwork.step)\n\n                with devices.autocast():\n                    x = batch.latent_sample.to(devices.device, non_blocking=pin_memory)\n                    if use_weight:\n                        w = batch.weight.to(devices.device, non_blocking=pin_memory)\n                    if tag_drop_out != 0 or shuffle_tags:\n                        shared.sd_model.cond_stage_model.to(devices.device)\n                        c = shared.sd_model.cond_stage_model(batch.cond_text).to(devices.device, non_blocking=pin_memory)\n                        shared.sd_model.cond_stage_model.to(devices.cpu)\n                    else:\n                        c = stack_conds(batch.cond).to(devices.device, non_blocking=pin_memory)\n                    if use_weight:\n                        loss = shared.sd_model.weighted_forward(x, c, w)[0] / gradient_step\n                        del w\n                    else:\n                        loss = shared.sd_model.forward(x, c)[0] / gradient_step\n                    del x\n                    del c\n\n                    _loss_step += loss.item()\n                scaler.scale(loss).backward()\n\n                # go back until we reach gradient accumulation steps\n                if (j + 1) % gradient_step != 0:\n                    continue\n                loss_logging.append(_loss_step)\n                if clip_grad:\n                    clip_grad(weights, clip_grad_sched.learn_rate)\n\n                scaler.step(optimizer)\n                scaler.update()\n                hypernetwork.step += 1\n                pbar.update()\n                optimizer.zero_grad(set_to_none=True)\n                loss_step = _loss_step\n                _loss_step = 0\n\n                steps_done = hypernetwork.step + 1\n\n                epoch_num = hypernetwork.step // steps_per_epoch\n                epoch_step = hypernetwork.step % steps_per_epoch\n\n                description = f\"Training hypernetwork [Epoch {epoch_num}: {epoch_step+1}/{steps_per_epoch}]loss: {loss_step:.7f}\"\n                pbar.set_description(description)\n                if hypernetwork_dir is not None and steps_done % save_hypernetwork_every == 0:\n                    # Before saving, change name to match current checkpoint.\n                    hypernetwork_name_every = f'{hypernetwork_name}-{steps_done}'\n                    last_saved_file = os.path.join(hypernetwork_dir, f'{hypernetwork_name_every}.pt')\n                    hypernetwork.optimizer_name = optimizer_name\n                    if shared.opts.save_optimizer_state:\n                        hypernetwork.optimizer_state_dict = optimizer.state_dict()\n                    save_hypernetwork(hypernetwork, checkpoint, hypernetwork_name, last_saved_file)\n                    hypernetwork.optimizer_state_dict = None  # dereference it after saving, to save memory.\n\n\n\n                if shared.opts.training_enable_tensorboard:\n                    epoch_num = hypernetwork.step // len(ds)\n                    epoch_step = hypernetwork.step - (epoch_num * len(ds)) + 1\n                    mean_loss = sum(loss_logging) / len(loss_logging)\n                    textual_inversion.tensorboard_add(tensorboard_writer, loss=mean_loss, global_step=hypernetwork.step, step=epoch_step, learn_rate=scheduler.learn_rate, epoch_num=epoch_num)\n\n                textual_inversion.write_loss(log_directory, \"hypernetwork_loss.csv\", hypernetwork.step, steps_per_epoch, {\n                    \"loss\": f\"{loss_step:.7f}\",\n                    \"learn_rate\": scheduler.learn_rate\n                })\n\n                if images_dir is not None and steps_done % create_image_every == 0:\n                    forced_filename = f'{hypernetwork_name}-{steps_done}'\n                    last_saved_image = os.path.join(images_dir, forced_filename)\n                    hypernetwork.eval()\n                    rng_state = torch.get_rng_state()\n                    cuda_rng_state = None\n                    if torch.cuda.is_available():\n                        cuda_rng_state = torch.cuda.get_rng_state_all()\n                    shared.sd_model.cond_stage_model.to(devices.device)\n                    shared.sd_model.first_stage_model.to(devices.device)\n\n                    p = processing.StableDiffusionProcessingTxt2Img(\n                        sd_model=shared.sd_model,\n                        do_not_save_grid=True,\n                        do_not_save_samples=True,\n                    )\n\n                    p.disable_extra_networks = True\n\n                    if preview_from_txt2img:\n                        p.prompt = preview_prompt\n                        p.negative_prompt = preview_negative_prompt\n                        p.steps = preview_steps\n                        p.sampler_name = sd_samplers.samplers_map[preview_sampler_name.lower()]\n                        p.cfg_scale = preview_cfg_scale\n                        p.seed = preview_seed\n                        p.width = preview_width\n                        p.height = preview_height\n                    else:\n                        p.prompt = batch.cond_text[0]\n                        p.steps = 20\n                        p.width = training_width\n                        p.height = training_height\n\n                    preview_text = p.prompt\n\n                    with closing(p):\n                        processed = processing.process_images(p)\n                        image = processed.images[0] if len(processed.images) > 0 else None\n\n                    if unload:\n                        shared.sd_model.cond_stage_model.to(devices.cpu)\n                        shared.sd_model.first_stage_model.to(devices.cpu)\n                    torch.set_rng_state(rng_state)\n                    if torch.cuda.is_available():\n                        torch.cuda.set_rng_state_all(cuda_rng_state)\n                    hypernetwork.train()\n                    if image is not None:\n                        shared.state.assign_current_image(image)\n                        if shared.opts.training_enable_tensorboard and shared.opts.training_tensorboard_save_images:\n                            textual_inversion.tensorboard_add_image(tensorboard_writer,\n                                                                    f\"Validation at epoch {epoch_num}\", image,\n                                                                    hypernetwork.step)\n                        last_saved_image, last_text_info = images.save_image(image, images_dir, \"\", p.seed, p.prompt, shared.opts.samples_format, processed.infotexts[0], p=p, forced_filename=forced_filename, save_to_dirs=False)\n                        last_saved_image += f\", prompt: {preview_text}\"\n\n                shared.state.job_no = hypernetwork.step\n\n                shared.state.textinfo = f\"\"\"\n<p>\nLoss: {loss_step:.7f}<br/>\nStep: {steps_done}<br/>\nLast prompt: {html.escape(batch.cond_text[0])}<br/>\nLast saved hypernetwork: {html.escape(last_saved_file)}<br/>\nLast saved image: {html.escape(last_saved_image)}<br/>\n</p>\n\"\"\"\n    except Exception:\n        errors.report(\"Exception in training hypernetwork\", exc_info=True)\n    finally:\n        pbar.leave = False\n        pbar.close()\n        hypernetwork.eval()\n        sd_hijack_checkpoint.remove()\n\n\n\n    filename = os.path.join(shared.cmd_opts.hypernetwork_dir, f'{hypernetwork_name}.pt')\n    hypernetwork.optimizer_name = optimizer_name\n    if shared.opts.save_optimizer_state:\n        hypernetwork.optimizer_state_dict = optimizer.state_dict()\n    save_hypernetwork(hypernetwork, checkpoint, hypernetwork_name, filename)\n\n    del optimizer\n    hypernetwork.optimizer_state_dict = None  # dereference it after saving, to save memory.\n    shared.sd_model.cond_stage_model.to(devices.device)\n    shared.sd_model.first_stage_model.to(devices.device)\n    shared.parallel_processing_allowed = old_parallel_processing_allowed\n\n    return hypernetwork, filename\n\ndef save_hypernetwork(hypernetwork, checkpoint, hypernetwork_name, filename):\n    old_hypernetwork_name = hypernetwork.name\n    old_sd_checkpoint = hypernetwork.sd_checkpoint if hasattr(hypernetwork, \"sd_checkpoint\") else None\n    old_sd_checkpoint_name = hypernetwork.sd_checkpoint_name if hasattr(hypernetwork, \"sd_checkpoint_name\") else None\n    try:\n        hypernetwork.sd_checkpoint = checkpoint.shorthash\n        hypernetwork.sd_checkpoint_name = checkpoint.model_name\n        hypernetwork.name = hypernetwork_name\n        hypernetwork.save(filename)\n    except:\n        hypernetwork.sd_checkpoint = old_sd_checkpoint\n        hypernetwork.sd_checkpoint_name = old_sd_checkpoint_name\n        hypernetwork.name = old_hypernetwork_name\n        raise\n", "modules/hypernetworks/ui.py": "import html\n\nimport gradio as gr\nimport modules.hypernetworks.hypernetwork\nfrom modules import devices, sd_hijack, shared\n\nnot_available = [\"hardswish\", \"multiheadattention\"]\nkeys = [x for x in modules.hypernetworks.hypernetwork.HypernetworkModule.activation_dict if x not in not_available]\n\n\ndef create_hypernetwork(name, enable_sizes, overwrite_old, layer_structure=None, activation_func=None, weight_init=None, add_layer_norm=False, use_dropout=False, dropout_structure=None):\n    filename = modules.hypernetworks.hypernetwork.create_hypernetwork(name, enable_sizes, overwrite_old, layer_structure, activation_func, weight_init, add_layer_norm, use_dropout, dropout_structure)\n\n    return gr.Dropdown.update(choices=sorted(shared.hypernetworks)), f\"Created: {filename}\", \"\"\n\n\ndef train_hypernetwork(*args):\n    shared.loaded_hypernetworks = []\n\n    assert not shared.cmd_opts.lowvram, 'Training models with lowvram is not possible'\n\n    try:\n        sd_hijack.undo_optimizations()\n\n        hypernetwork, filename = modules.hypernetworks.hypernetwork.train_hypernetwork(*args)\n\n        res = f\"\"\"\nTraining {'interrupted' if shared.state.interrupted else 'finished'} at {hypernetwork.step} steps.\nHypernetwork saved to {html.escape(filename)}\n\"\"\"\n        return res, \"\"\n    except Exception:\n        raise\n    finally:\n        shared.sd_model.cond_stage_model.to(devices.device)\n        shared.sd_model.first_stage_model.to(devices.device)\n        sd_hijack.apply_optimizations()\n\n", "modules/api/api.py": "import base64\nimport io\nimport os\nimport time\nimport datetime\nimport uvicorn\nimport ipaddress\nimport requests\nimport gradio as gr\nfrom threading import Lock\nfrom io import BytesIO\nfrom fastapi import APIRouter, Depends, FastAPI, Request, Response\nfrom fastapi.security import HTTPBasic, HTTPBasicCredentials\nfrom fastapi.exceptions import HTTPException\nfrom fastapi.responses import JSONResponse\nfrom fastapi.encoders import jsonable_encoder\nfrom secrets import compare_digest\n\nimport modules.shared as shared\nfrom modules import sd_samplers, deepbooru, sd_hijack, images, scripts, ui, postprocessing, errors, restart, shared_items, script_callbacks, infotext_utils, sd_models, sd_schedulers\nfrom modules.api import models\nfrom modules.shared import opts\nfrom modules.processing import StableDiffusionProcessingTxt2Img, StableDiffusionProcessingImg2Img, process_images\nfrom modules.textual_inversion.textual_inversion import create_embedding, train_embedding\nfrom modules.hypernetworks.hypernetwork import create_hypernetwork, train_hypernetwork\nfrom PIL import PngImagePlugin\nfrom modules.sd_models_config import find_checkpoint_config_near_filename\nfrom modules.realesrgan_model import get_realesrgan_models\nfrom modules import devices\nfrom typing import Any\nimport piexif\nimport piexif.helper\nfrom contextlib import closing\nfrom modules.progress import create_task_id, add_task_to_queue, start_task, finish_task, current_task\n\ndef script_name_to_index(name, scripts):\n    try:\n        return [script.title().lower() for script in scripts].index(name.lower())\n    except Exception as e:\n        raise HTTPException(status_code=422, detail=f\"Script '{name}' not found\") from e\n\n\ndef validate_sampler_name(name):\n    config = sd_samplers.all_samplers_map.get(name, None)\n    if config is None:\n        raise HTTPException(status_code=404, detail=\"Sampler not found\")\n\n    return name\n\n\ndef setUpscalers(req: dict):\n    reqDict = vars(req)\n    reqDict['extras_upscaler_1'] = reqDict.pop('upscaler_1', None)\n    reqDict['extras_upscaler_2'] = reqDict.pop('upscaler_2', None)\n    return reqDict\n\n\ndef verify_url(url):\n    \"\"\"Returns True if the url refers to a global resource.\"\"\"\n\n    import socket\n    from urllib.parse import urlparse\n    try:\n        parsed_url = urlparse(url)\n        domain_name = parsed_url.netloc\n        host = socket.gethostbyname_ex(domain_name)\n        for ip in host[2]:\n            ip_addr = ipaddress.ip_address(ip)\n            if not ip_addr.is_global:\n                return False\n    except Exception:\n        return False\n\n    return True\n\n\ndef decode_base64_to_image(encoding):\n    if encoding.startswith(\"http://\") or encoding.startswith(\"https://\"):\n        if not opts.api_enable_requests:\n            raise HTTPException(status_code=500, detail=\"Requests not allowed\")\n\n        if opts.api_forbid_local_requests and not verify_url(encoding):\n            raise HTTPException(status_code=500, detail=\"Request to local resource not allowed\")\n\n        headers = {'user-agent': opts.api_useragent} if opts.api_useragent else {}\n        response = requests.get(encoding, timeout=30, headers=headers)\n        try:\n            image = images.read(BytesIO(response.content))\n            return image\n        except Exception as e:\n            raise HTTPException(status_code=500, detail=\"Invalid image url\") from e\n\n    if encoding.startswith(\"data:image/\"):\n        encoding = encoding.split(\";\")[1].split(\",\")[1]\n    try:\n        image = images.read(BytesIO(base64.b64decode(encoding)))\n        return image\n    except Exception as e:\n        raise HTTPException(status_code=500, detail=\"Invalid encoded image\") from e\n\n\ndef encode_pil_to_base64(image):\n    with io.BytesIO() as output_bytes:\n        if isinstance(image, str):\n            return image\n        if opts.samples_format.lower() == 'png':\n            use_metadata = False\n            metadata = PngImagePlugin.PngInfo()\n            for key, value in image.info.items():\n                if isinstance(key, str) and isinstance(value, str):\n                    metadata.add_text(key, value)\n                    use_metadata = True\n            image.save(output_bytes, format=\"PNG\", pnginfo=(metadata if use_metadata else None), quality=opts.jpeg_quality)\n\n        elif opts.samples_format.lower() in (\"jpg\", \"jpeg\", \"webp\"):\n            if image.mode == \"RGBA\":\n                image = image.convert(\"RGB\")\n            parameters = image.info.get('parameters', None)\n            exif_bytes = piexif.dump({\n                \"Exif\": { piexif.ExifIFD.UserComment: piexif.helper.UserComment.dump(parameters or \"\", encoding=\"unicode\") }\n            })\n            if opts.samples_format.lower() in (\"jpg\", \"jpeg\"):\n                image.save(output_bytes, format=\"JPEG\", exif = exif_bytes, quality=opts.jpeg_quality)\n            else:\n                image.save(output_bytes, format=\"WEBP\", exif = exif_bytes, quality=opts.jpeg_quality)\n\n        else:\n            raise HTTPException(status_code=500, detail=\"Invalid image format\")\n\n        bytes_data = output_bytes.getvalue()\n\n    return base64.b64encode(bytes_data)\n\n\ndef api_middleware(app: FastAPI):\n    rich_available = False\n    try:\n        if os.environ.get('WEBUI_RICH_EXCEPTIONS', None) is not None:\n            import anyio  # importing just so it can be placed on silent list\n            import starlette  # importing just so it can be placed on silent list\n            from rich.console import Console\n            console = Console()\n            rich_available = True\n    except Exception:\n        pass\n\n    @app.middleware(\"http\")\n    async def log_and_time(req: Request, call_next):\n        ts = time.time()\n        res: Response = await call_next(req)\n        duration = str(round(time.time() - ts, 4))\n        res.headers[\"X-Process-Time\"] = duration\n        endpoint = req.scope.get('path', 'err')\n        if shared.cmd_opts.api_log and endpoint.startswith('/sdapi'):\n            print('API {t} {code} {prot}/{ver} {method} {endpoint} {cli} {duration}'.format(\n                t=datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S.%f\"),\n                code=res.status_code,\n                ver=req.scope.get('http_version', '0.0'),\n                cli=req.scope.get('client', ('0:0.0.0', 0))[0],\n                prot=req.scope.get('scheme', 'err'),\n                method=req.scope.get('method', 'err'),\n                endpoint=endpoint,\n                duration=duration,\n            ))\n        return res\n\n    def handle_exception(request: Request, e: Exception):\n        err = {\n            \"error\": type(e).__name__,\n            \"detail\": vars(e).get('detail', ''),\n            \"body\": vars(e).get('body', ''),\n            \"errors\": str(e),\n        }\n        if not isinstance(e, HTTPException):  # do not print backtrace on known httpexceptions\n            message = f\"API error: {request.method}: {request.url} {err}\"\n            if rich_available:\n                print(message)\n                console.print_exception(show_locals=True, max_frames=2, extra_lines=1, suppress=[anyio, starlette], word_wrap=False, width=min([console.width, 200]))\n            else:\n                errors.report(message, exc_info=True)\n        return JSONResponse(status_code=vars(e).get('status_code', 500), content=jsonable_encoder(err))\n\n    @app.middleware(\"http\")\n    async def exception_handling(request: Request, call_next):\n        try:\n            return await call_next(request)\n        except Exception as e:\n            return handle_exception(request, e)\n\n    @app.exception_handler(Exception)\n    async def fastapi_exception_handler(request: Request, e: Exception):\n        return handle_exception(request, e)\n\n    @app.exception_handler(HTTPException)\n    async def http_exception_handler(request: Request, e: HTTPException):\n        return handle_exception(request, e)\n\n\nclass Api:\n    def __init__(self, app: FastAPI, queue_lock: Lock):\n        if shared.cmd_opts.api_auth:\n            self.credentials = {}\n            for auth in shared.cmd_opts.api_auth.split(\",\"):\n                user, password = auth.split(\":\")\n                self.credentials[user] = password\n\n        self.router = APIRouter()\n        self.app = app\n        self.queue_lock = queue_lock\n        api_middleware(self.app)\n        self.add_api_route(\"/sdapi/v1/txt2img\", self.text2imgapi, methods=[\"POST\"], response_model=models.TextToImageResponse)\n        self.add_api_route(\"/sdapi/v1/img2img\", self.img2imgapi, methods=[\"POST\"], response_model=models.ImageToImageResponse)\n        self.add_api_route(\"/sdapi/v1/extra-single-image\", self.extras_single_image_api, methods=[\"POST\"], response_model=models.ExtrasSingleImageResponse)\n        self.add_api_route(\"/sdapi/v1/extra-batch-images\", self.extras_batch_images_api, methods=[\"POST\"], response_model=models.ExtrasBatchImagesResponse)\n        self.add_api_route(\"/sdapi/v1/png-info\", self.pnginfoapi, methods=[\"POST\"], response_model=models.PNGInfoResponse)\n        self.add_api_route(\"/sdapi/v1/progress\", self.progressapi, methods=[\"GET\"], response_model=models.ProgressResponse)\n        self.add_api_route(\"/sdapi/v1/interrogate\", self.interrogateapi, methods=[\"POST\"])\n        self.add_api_route(\"/sdapi/v1/interrupt\", self.interruptapi, methods=[\"POST\"])\n        self.add_api_route(\"/sdapi/v1/skip\", self.skip, methods=[\"POST\"])\n        self.add_api_route(\"/sdapi/v1/options\", self.get_config, methods=[\"GET\"], response_model=models.OptionsModel)\n        self.add_api_route(\"/sdapi/v1/options\", self.set_config, methods=[\"POST\"])\n        self.add_api_route(\"/sdapi/v1/cmd-flags\", self.get_cmd_flags, methods=[\"GET\"], response_model=models.FlagsModel)\n        self.add_api_route(\"/sdapi/v1/samplers\", self.get_samplers, methods=[\"GET\"], response_model=list[models.SamplerItem])\n        self.add_api_route(\"/sdapi/v1/schedulers\", self.get_schedulers, methods=[\"GET\"], response_model=list[models.SchedulerItem])\n        self.add_api_route(\"/sdapi/v1/upscalers\", self.get_upscalers, methods=[\"GET\"], response_model=list[models.UpscalerItem])\n        self.add_api_route(\"/sdapi/v1/latent-upscale-modes\", self.get_latent_upscale_modes, methods=[\"GET\"], response_model=list[models.LatentUpscalerModeItem])\n        self.add_api_route(\"/sdapi/v1/sd-models\", self.get_sd_models, methods=[\"GET\"], response_model=list[models.SDModelItem])\n        self.add_api_route(\"/sdapi/v1/sd-vae\", self.get_sd_vaes, methods=[\"GET\"], response_model=list[models.SDVaeItem])\n        self.add_api_route(\"/sdapi/v1/hypernetworks\", self.get_hypernetworks, methods=[\"GET\"], response_model=list[models.HypernetworkItem])\n        self.add_api_route(\"/sdapi/v1/face-restorers\", self.get_face_restorers, methods=[\"GET\"], response_model=list[models.FaceRestorerItem])\n        self.add_api_route(\"/sdapi/v1/realesrgan-models\", self.get_realesrgan_models, methods=[\"GET\"], response_model=list[models.RealesrganItem])\n        self.add_api_route(\"/sdapi/v1/prompt-styles\", self.get_prompt_styles, methods=[\"GET\"], response_model=list[models.PromptStyleItem])\n        self.add_api_route(\"/sdapi/v1/embeddings\", self.get_embeddings, methods=[\"GET\"], response_model=models.EmbeddingsResponse)\n        self.add_api_route(\"/sdapi/v1/refresh-embeddings\", self.refresh_embeddings, methods=[\"POST\"])\n        self.add_api_route(\"/sdapi/v1/refresh-checkpoints\", self.refresh_checkpoints, methods=[\"POST\"])\n        self.add_api_route(\"/sdapi/v1/refresh-vae\", self.refresh_vae, methods=[\"POST\"])\n        self.add_api_route(\"/sdapi/v1/create/embedding\", self.create_embedding, methods=[\"POST\"], response_model=models.CreateResponse)\n        self.add_api_route(\"/sdapi/v1/create/hypernetwork\", self.create_hypernetwork, methods=[\"POST\"], response_model=models.CreateResponse)\n        self.add_api_route(\"/sdapi/v1/train/embedding\", self.train_embedding, methods=[\"POST\"], response_model=models.TrainResponse)\n        self.add_api_route(\"/sdapi/v1/train/hypernetwork\", self.train_hypernetwork, methods=[\"POST\"], response_model=models.TrainResponse)\n        self.add_api_route(\"/sdapi/v1/memory\", self.get_memory, methods=[\"GET\"], response_model=models.MemoryResponse)\n        self.add_api_route(\"/sdapi/v1/unload-checkpoint\", self.unloadapi, methods=[\"POST\"])\n        self.add_api_route(\"/sdapi/v1/reload-checkpoint\", self.reloadapi, methods=[\"POST\"])\n        self.add_api_route(\"/sdapi/v1/scripts\", self.get_scripts_list, methods=[\"GET\"], response_model=models.ScriptsList)\n        self.add_api_route(\"/sdapi/v1/script-info\", self.get_script_info, methods=[\"GET\"], response_model=list[models.ScriptInfo])\n        self.add_api_route(\"/sdapi/v1/extensions\", self.get_extensions_list, methods=[\"GET\"], response_model=list[models.ExtensionItem])\n\n        if shared.cmd_opts.api_server_stop:\n            self.add_api_route(\"/sdapi/v1/server-kill\", self.kill_webui, methods=[\"POST\"])\n            self.add_api_route(\"/sdapi/v1/server-restart\", self.restart_webui, methods=[\"POST\"])\n            self.add_api_route(\"/sdapi/v1/server-stop\", self.stop_webui, methods=[\"POST\"])\n\n        self.default_script_arg_txt2img = []\n        self.default_script_arg_img2img = []\n\n        txt2img_script_runner = scripts.scripts_txt2img\n        img2img_script_runner = scripts.scripts_img2img\n\n        if not txt2img_script_runner.scripts or not img2img_script_runner.scripts:\n            ui.create_ui()\n\n        if not txt2img_script_runner.scripts:\n            txt2img_script_runner.initialize_scripts(False)\n        if not self.default_script_arg_txt2img:\n            self.default_script_arg_txt2img = self.init_default_script_args(txt2img_script_runner)\n\n        if not img2img_script_runner.scripts:\n            img2img_script_runner.initialize_scripts(True)\n        if not self.default_script_arg_img2img:\n            self.default_script_arg_img2img = self.init_default_script_args(img2img_script_runner)\n\n\n\n    def add_api_route(self, path: str, endpoint, **kwargs):\n        if shared.cmd_opts.api_auth:\n            return self.app.add_api_route(path, endpoint, dependencies=[Depends(self.auth)], **kwargs)\n        return self.app.add_api_route(path, endpoint, **kwargs)\n\n    def auth(self, credentials: HTTPBasicCredentials = Depends(HTTPBasic())):\n        if credentials.username in self.credentials:\n            if compare_digest(credentials.password, self.credentials[credentials.username]):\n                return True\n\n        raise HTTPException(status_code=401, detail=\"Incorrect username or password\", headers={\"WWW-Authenticate\": \"Basic\"})\n\n    def get_selectable_script(self, script_name, script_runner):\n        if script_name is None or script_name == \"\":\n            return None, None\n\n        script_idx = script_name_to_index(script_name, script_runner.selectable_scripts)\n        script = script_runner.selectable_scripts[script_idx]\n        return script, script_idx\n\n    def get_scripts_list(self):\n        t2ilist = [script.name for script in scripts.scripts_txt2img.scripts if script.name is not None]\n        i2ilist = [script.name for script in scripts.scripts_img2img.scripts if script.name is not None]\n\n        return models.ScriptsList(txt2img=t2ilist, img2img=i2ilist)\n\n    def get_script_info(self):\n        res = []\n\n        for script_list in [scripts.scripts_txt2img.scripts, scripts.scripts_img2img.scripts]:\n            res += [script.api_info for script in script_list if script.api_info is not None]\n\n        return res\n\n    def get_script(self, script_name, script_runner):\n        if script_name is None or script_name == \"\":\n            return None, None\n\n        script_idx = script_name_to_index(script_name, script_runner.scripts)\n        return script_runner.scripts[script_idx]\n\n    def init_default_script_args(self, script_runner):\n        #find max idx from the scripts in runner and generate a none array to init script_args\n        last_arg_index = 1\n        for script in script_runner.scripts:\n            if last_arg_index < script.args_to:\n                last_arg_index = script.args_to\n        # None everywhere except position 0 to initialize script args\n        script_args = [None]*last_arg_index\n        script_args[0] = 0\n\n        # get default values\n        with gr.Blocks(): # will throw errors calling ui function without this\n            for script in script_runner.scripts:\n                if script.ui(script.is_img2img):\n                    ui_default_values = []\n                    for elem in script.ui(script.is_img2img):\n                        ui_default_values.append(elem.value)\n                    script_args[script.args_from:script.args_to] = ui_default_values\n        return script_args\n\n    def init_script_args(self, request, default_script_args, selectable_scripts, selectable_idx, script_runner, *, input_script_args=None):\n        script_args = default_script_args.copy()\n\n        if input_script_args is not None:\n            for index, value in input_script_args.items():\n                script_args[index] = value\n\n        # position 0 in script_arg is the idx+1 of the selectable script that is going to be run when using scripts.scripts_*2img.run()\n        if selectable_scripts:\n            script_args[selectable_scripts.args_from:selectable_scripts.args_to] = request.script_args\n            script_args[0] = selectable_idx + 1\n\n        # Now check for always on scripts\n        if request.alwayson_scripts:\n            for alwayson_script_name in request.alwayson_scripts.keys():\n                alwayson_script = self.get_script(alwayson_script_name, script_runner)\n                if alwayson_script is None:\n                    raise HTTPException(status_code=422, detail=f\"always on script {alwayson_script_name} not found\")\n                # Selectable script in always on script param check\n                if alwayson_script.alwayson is False:\n                    raise HTTPException(status_code=422, detail=\"Cannot have a selectable script in the always on scripts params\")\n                # always on script with no arg should always run so you don't really need to add them to the requests\n                if \"args\" in request.alwayson_scripts[alwayson_script_name]:\n                    # min between arg length in scriptrunner and arg length in the request\n                    for idx in range(0, min((alwayson_script.args_to - alwayson_script.args_from), len(request.alwayson_scripts[alwayson_script_name][\"args\"]))):\n                        script_args[alwayson_script.args_from + idx] = request.alwayson_scripts[alwayson_script_name][\"args\"][idx]\n        return script_args\n\n    def apply_infotext(self, request, tabname, *, script_runner=None, mentioned_script_args=None):\n        \"\"\"Processes `infotext` field from the `request`, and sets other fields of the `request` according to what's in infotext.\n\n        If request already has a field set, and that field is encountered in infotext too, the value from infotext is ignored.\n\n        Additionally, fills `mentioned_script_args` dict with index: value pairs for script arguments read from infotext.\n        \"\"\"\n\n        if not request.infotext:\n            return {}\n\n        possible_fields = infotext_utils.paste_fields[tabname][\"fields\"]\n        set_fields = request.model_dump(exclude_unset=True) if hasattr(request, \"request\") else request.dict(exclude_unset=True)  # pydantic v1/v2 have differenrt names for this\n        params = infotext_utils.parse_generation_parameters(request.infotext)\n\n        def get_field_value(field, params):\n            value = field.function(params) if field.function else params.get(field.label)\n            if value is None:\n                return None\n\n            if field.api in request.__fields__:\n                target_type = request.__fields__[field.api].type_\n            else:\n                target_type = type(field.component.value)\n\n            if target_type == type(None):\n                return None\n\n            if isinstance(value, dict) and value.get('__type__') == 'generic_update':  # this is a gradio.update rather than a value\n                value = value.get('value')\n\n            if value is not None and not isinstance(value, target_type):\n                value = target_type(value)\n\n            return value\n\n        for field in possible_fields:\n            if not field.api:\n                continue\n\n            if field.api in set_fields:\n                continue\n\n            value = get_field_value(field, params)\n            if value is not None:\n                setattr(request, field.api, value)\n\n        if request.override_settings is None:\n            request.override_settings = {}\n\n        overridden_settings = infotext_utils.get_override_settings(params)\n        for _, setting_name, value in overridden_settings:\n            if setting_name not in request.override_settings:\n                request.override_settings[setting_name] = value\n\n        if script_runner is not None and mentioned_script_args is not None:\n            indexes = {v: i for i, v in enumerate(script_runner.inputs)}\n            script_fields = ((field, indexes[field.component]) for field in possible_fields if field.component in indexes)\n\n            for field, index in script_fields:\n                value = get_field_value(field, params)\n\n                if value is None:\n                    continue\n\n                mentioned_script_args[index] = value\n\n        return params\n\n    def text2imgapi(self, txt2imgreq: models.StableDiffusionTxt2ImgProcessingAPI):\n        task_id = txt2imgreq.force_task_id or create_task_id(\"txt2img\")\n\n        script_runner = scripts.scripts_txt2img\n\n        infotext_script_args = {}\n        self.apply_infotext(txt2imgreq, \"txt2img\", script_runner=script_runner, mentioned_script_args=infotext_script_args)\n\n        selectable_scripts, selectable_script_idx = self.get_selectable_script(txt2imgreq.script_name, script_runner)\n\n        populate = txt2imgreq.copy(update={  # Override __init__ params\n            \"sampler_name\": validate_sampler_name(txt2imgreq.sampler_name or txt2imgreq.sampler_index),\n            \"do_not_save_samples\": not txt2imgreq.save_images,\n            \"do_not_save_grid\": not txt2imgreq.save_images,\n        })\n        if populate.sampler_name:\n            populate.sampler_index = None  # prevent a warning later on\n\n        args = vars(populate)\n        args.pop('script_name', None)\n        args.pop('script_args', None) # will refeed them to the pipeline directly after initializing them\n        args.pop('alwayson_scripts', None)\n        args.pop('infotext', None)\n\n        script_args = self.init_script_args(txt2imgreq, self.default_script_arg_txt2img, selectable_scripts, selectable_script_idx, script_runner, input_script_args=infotext_script_args)\n\n        send_images = args.pop('send_images', True)\n        args.pop('save_images', None)\n\n        add_task_to_queue(task_id)\n\n        with self.queue_lock:\n            with closing(StableDiffusionProcessingTxt2Img(sd_model=shared.sd_model, **args)) as p:\n                p.is_api = True\n                p.scripts = script_runner\n                p.outpath_grids = opts.outdir_txt2img_grids\n                p.outpath_samples = opts.outdir_txt2img_samples\n\n                try:\n                    shared.state.begin(job=\"scripts_txt2img\")\n                    start_task(task_id)\n                    if selectable_scripts is not None:\n                        p.script_args = script_args\n                        processed = scripts.scripts_txt2img.run(p, *p.script_args) # Need to pass args as list here\n                    else:\n                        p.script_args = tuple(script_args) # Need to pass args as tuple here\n                        processed = process_images(p)\n                    finish_task(task_id)\n                finally:\n                    shared.state.end()\n                    shared.total_tqdm.clear()\n\n        b64images = list(map(encode_pil_to_base64, processed.images)) if send_images else []\n\n        return models.TextToImageResponse(images=b64images, parameters=vars(txt2imgreq), info=processed.js())\n\n    def img2imgapi(self, img2imgreq: models.StableDiffusionImg2ImgProcessingAPI):\n        task_id = img2imgreq.force_task_id or create_task_id(\"img2img\")\n\n        init_images = img2imgreq.init_images\n        if init_images is None:\n            raise HTTPException(status_code=404, detail=\"Init image not found\")\n\n        mask = img2imgreq.mask\n        if mask:\n            mask = decode_base64_to_image(mask)\n\n        script_runner = scripts.scripts_img2img\n\n        infotext_script_args = {}\n        self.apply_infotext(img2imgreq, \"img2img\", script_runner=script_runner, mentioned_script_args=infotext_script_args)\n\n        selectable_scripts, selectable_script_idx = self.get_selectable_script(img2imgreq.script_name, script_runner)\n\n        populate = img2imgreq.copy(update={  # Override __init__ params\n            \"sampler_name\": validate_sampler_name(img2imgreq.sampler_name or img2imgreq.sampler_index),\n            \"do_not_save_samples\": not img2imgreq.save_images,\n            \"do_not_save_grid\": not img2imgreq.save_images,\n            \"mask\": mask,\n        })\n        if populate.sampler_name:\n            populate.sampler_index = None  # prevent a warning later on\n\n        args = vars(populate)\n        args.pop('include_init_images', None)  # this is meant to be done by \"exclude\": True in model, but it's for a reason that I cannot determine.\n        args.pop('script_name', None)\n        args.pop('script_args', None)  # will refeed them to the pipeline directly after initializing them\n        args.pop('alwayson_scripts', None)\n        args.pop('infotext', None)\n\n        script_args = self.init_script_args(img2imgreq, self.default_script_arg_img2img, selectable_scripts, selectable_script_idx, script_runner, input_script_args=infotext_script_args)\n\n        send_images = args.pop('send_images', True)\n        args.pop('save_images', None)\n\n        add_task_to_queue(task_id)\n\n        with self.queue_lock:\n            with closing(StableDiffusionProcessingImg2Img(sd_model=shared.sd_model, **args)) as p:\n                p.init_images = [decode_base64_to_image(x) for x in init_images]\n                p.is_api = True\n                p.scripts = script_runner\n                p.outpath_grids = opts.outdir_img2img_grids\n                p.outpath_samples = opts.outdir_img2img_samples\n\n                try:\n                    shared.state.begin(job=\"scripts_img2img\")\n                    start_task(task_id)\n                    if selectable_scripts is not None:\n                        p.script_args = script_args\n                        processed = scripts.scripts_img2img.run(p, *p.script_args) # Need to pass args as list here\n                    else:\n                        p.script_args = tuple(script_args) # Need to pass args as tuple here\n                        processed = process_images(p)\n                    finish_task(task_id)\n                finally:\n                    shared.state.end()\n                    shared.total_tqdm.clear()\n\n        b64images = list(map(encode_pil_to_base64, processed.images)) if send_images else []\n\n        if not img2imgreq.include_init_images:\n            img2imgreq.init_images = None\n            img2imgreq.mask = None\n\n        return models.ImageToImageResponse(images=b64images, parameters=vars(img2imgreq), info=processed.js())\n\n    def extras_single_image_api(self, req: models.ExtrasSingleImageRequest):\n        reqDict = setUpscalers(req)\n\n        reqDict['image'] = decode_base64_to_image(reqDict['image'])\n\n        with self.queue_lock:\n            result = postprocessing.run_extras(extras_mode=0, image_folder=\"\", input_dir=\"\", output_dir=\"\", save_output=False, **reqDict)\n\n        return models.ExtrasSingleImageResponse(image=encode_pil_to_base64(result[0][0]), html_info=result[1])\n\n    def extras_batch_images_api(self, req: models.ExtrasBatchImagesRequest):\n        reqDict = setUpscalers(req)\n\n        image_list = reqDict.pop('imageList', [])\n        image_folder = [decode_base64_to_image(x.data) for x in image_list]\n\n        with self.queue_lock:\n            result = postprocessing.run_extras(extras_mode=1, image_folder=image_folder, image=\"\", input_dir=\"\", output_dir=\"\", save_output=False, **reqDict)\n\n        return models.ExtrasBatchImagesResponse(images=list(map(encode_pil_to_base64, result[0])), html_info=result[1])\n\n    def pnginfoapi(self, req: models.PNGInfoRequest):\n        image = decode_base64_to_image(req.image.strip())\n        if image is None:\n            return models.PNGInfoResponse(info=\"\")\n\n        geninfo, items = images.read_info_from_image(image)\n        if geninfo is None:\n            geninfo = \"\"\n\n        params = infotext_utils.parse_generation_parameters(geninfo)\n        script_callbacks.infotext_pasted_callback(geninfo, params)\n\n        return models.PNGInfoResponse(info=geninfo, items=items, parameters=params)\n\n    def progressapi(self, req: models.ProgressRequest = Depends()):\n        # copy from check_progress_call of ui.py\n\n        if shared.state.job_count == 0:\n            return models.ProgressResponse(progress=0, eta_relative=0, state=shared.state.dict(), textinfo=shared.state.textinfo)\n\n        # avoid dividing zero\n        progress = 0.01\n\n        if shared.state.job_count > 0:\n            progress += shared.state.job_no / shared.state.job_count\n        if shared.state.sampling_steps > 0:\n            progress += 1 / shared.state.job_count * shared.state.sampling_step / shared.state.sampling_steps\n\n        time_since_start = time.time() - shared.state.time_start\n        eta = (time_since_start/progress)\n        eta_relative = eta-time_since_start\n\n        progress = min(progress, 1)\n\n        shared.state.set_current_image()\n\n        current_image = None\n        if shared.state.current_image and not req.skip_current_image:\n            current_image = encode_pil_to_base64(shared.state.current_image)\n\n        return models.ProgressResponse(progress=progress, eta_relative=eta_relative, state=shared.state.dict(), current_image=current_image, textinfo=shared.state.textinfo, current_task=current_task)\n\n    def interrogateapi(self, interrogatereq: models.InterrogateRequest):\n        image_b64 = interrogatereq.image\n        if image_b64 is None:\n            raise HTTPException(status_code=404, detail=\"Image not found\")\n\n        img = decode_base64_to_image(image_b64)\n        img = img.convert('RGB')\n\n        # Override object param\n        with self.queue_lock:\n            if interrogatereq.model == \"clip\":\n                processed = shared.interrogator.interrogate(img)\n            elif interrogatereq.model == \"deepdanbooru\":\n                processed = deepbooru.model.tag(img)\n            else:\n                raise HTTPException(status_code=404, detail=\"Model not found\")\n\n        return models.InterrogateResponse(caption=processed)\n\n    def interruptapi(self):\n        shared.state.interrupt()\n\n        return {}\n\n    def unloadapi(self):\n        sd_models.unload_model_weights()\n\n        return {}\n\n    def reloadapi(self):\n        sd_models.send_model_to_device(shared.sd_model)\n\n        return {}\n\n    def skip(self):\n        shared.state.skip()\n\n    def get_config(self):\n        options = {}\n        for key in shared.opts.data.keys():\n            metadata = shared.opts.data_labels.get(key)\n            if(metadata is not None):\n                options.update({key: shared.opts.data.get(key, shared.opts.data_labels.get(key).default)})\n            else:\n                options.update({key: shared.opts.data.get(key, None)})\n\n        return options\n\n    def set_config(self, req: dict[str, Any]):\n        checkpoint_name = req.get(\"sd_model_checkpoint\", None)\n        if checkpoint_name is not None and checkpoint_name not in sd_models.checkpoint_aliases:\n            raise RuntimeError(f\"model {checkpoint_name!r} not found\")\n\n        for k, v in req.items():\n            shared.opts.set(k, v, is_api=True)\n\n        shared.opts.save(shared.config_filename)\n        return\n\n    def get_cmd_flags(self):\n        return vars(shared.cmd_opts)\n\n    def get_samplers(self):\n        return [{\"name\": sampler[0], \"aliases\":sampler[2], \"options\":sampler[3]} for sampler in sd_samplers.all_samplers]\n\n    def get_schedulers(self):\n        return [\n            {\n                \"name\": scheduler.name,\n                \"label\": scheduler.label,\n                \"aliases\": scheduler.aliases,\n                \"default_rho\": scheduler.default_rho,\n                \"need_inner_model\": scheduler.need_inner_model,\n            }\n            for scheduler in sd_schedulers.schedulers]\n\n    def get_upscalers(self):\n        return [\n            {\n                \"name\": upscaler.name,\n                \"model_name\": upscaler.scaler.model_name,\n                \"model_path\": upscaler.data_path,\n                \"model_url\": None,\n                \"scale\": upscaler.scale,\n            }\n            for upscaler in shared.sd_upscalers\n        ]\n\n    def get_latent_upscale_modes(self):\n        return [\n            {\n                \"name\": upscale_mode,\n            }\n            for upscale_mode in [*(shared.latent_upscale_modes or {})]\n        ]\n\n    def get_sd_models(self):\n        import modules.sd_models as sd_models\n        return [{\"title\": x.title, \"model_name\": x.model_name, \"hash\": x.shorthash, \"sha256\": x.sha256, \"filename\": x.filename, \"config\": find_checkpoint_config_near_filename(x)} for x in sd_models.checkpoints_list.values()]\n\n    def get_sd_vaes(self):\n        import modules.sd_vae as sd_vae\n        return [{\"model_name\": x, \"filename\": sd_vae.vae_dict[x]} for x in sd_vae.vae_dict.keys()]\n\n    def get_hypernetworks(self):\n        return [{\"name\": name, \"path\": shared.hypernetworks[name]} for name in shared.hypernetworks]\n\n    def get_face_restorers(self):\n        return [{\"name\":x.name(), \"cmd_dir\": getattr(x, \"cmd_dir\", None)} for x in shared.face_restorers]\n\n    def get_realesrgan_models(self):\n        return [{\"name\":x.name,\"path\":x.data_path, \"scale\":x.scale} for x in get_realesrgan_models(None)]\n\n    def get_prompt_styles(self):\n        styleList = []\n        for k in shared.prompt_styles.styles:\n            style = shared.prompt_styles.styles[k]\n            styleList.append({\"name\":style[0], \"prompt\": style[1], \"negative_prompt\": style[2]})\n\n        return styleList\n\n    def get_embeddings(self):\n        db = sd_hijack.model_hijack.embedding_db\n\n        def convert_embedding(embedding):\n            return {\n                \"step\": embedding.step,\n                \"sd_checkpoint\": embedding.sd_checkpoint,\n                \"sd_checkpoint_name\": embedding.sd_checkpoint_name,\n                \"shape\": embedding.shape,\n                \"vectors\": embedding.vectors,\n            }\n\n        def convert_embeddings(embeddings):\n            return {embedding.name: convert_embedding(embedding) for embedding in embeddings.values()}\n\n        return {\n            \"loaded\": convert_embeddings(db.word_embeddings),\n            \"skipped\": convert_embeddings(db.skipped_embeddings),\n        }\n\n    def refresh_embeddings(self):\n        with self.queue_lock:\n            sd_hijack.model_hijack.embedding_db.load_textual_inversion_embeddings(force_reload=True)\n\n    def refresh_checkpoints(self):\n        with self.queue_lock:\n            shared.refresh_checkpoints()\n\n    def refresh_vae(self):\n        with self.queue_lock:\n            shared_items.refresh_vae_list()\n\n    def create_embedding(self, args: dict):\n        try:\n            shared.state.begin(job=\"create_embedding\")\n            filename = create_embedding(**args) # create empty embedding\n            sd_hijack.model_hijack.embedding_db.load_textual_inversion_embeddings() # reload embeddings so new one can be immediately used\n            return models.CreateResponse(info=f\"create embedding filename: {filename}\")\n        except AssertionError as e:\n            return models.TrainResponse(info=f\"create embedding error: {e}\")\n        finally:\n            shared.state.end()\n\n\n    def create_hypernetwork(self, args: dict):\n        try:\n            shared.state.begin(job=\"create_hypernetwork\")\n            filename = create_hypernetwork(**args) # create empty embedding\n            return models.CreateResponse(info=f\"create hypernetwork filename: {filename}\")\n        except AssertionError as e:\n            return models.TrainResponse(info=f\"create hypernetwork error: {e}\")\n        finally:\n            shared.state.end()\n\n    def train_embedding(self, args: dict):\n        try:\n            shared.state.begin(job=\"train_embedding\")\n            apply_optimizations = shared.opts.training_xattention_optimizations\n            error = None\n            filename = ''\n            if not apply_optimizations:\n                sd_hijack.undo_optimizations()\n            try:\n                embedding, filename = train_embedding(**args) # can take a long time to complete\n            except Exception as e:\n                error = e\n            finally:\n                if not apply_optimizations:\n                    sd_hijack.apply_optimizations()\n            return models.TrainResponse(info=f\"train embedding complete: filename: {filename} error: {error}\")\n        except Exception as msg:\n            return models.TrainResponse(info=f\"train embedding error: {msg}\")\n        finally:\n            shared.state.end()\n\n    def train_hypernetwork(self, args: dict):\n        try:\n            shared.state.begin(job=\"train_hypernetwork\")\n            shared.loaded_hypernetworks = []\n            apply_optimizations = shared.opts.training_xattention_optimizations\n            error = None\n            filename = ''\n            if not apply_optimizations:\n                sd_hijack.undo_optimizations()\n            try:\n                hypernetwork, filename = train_hypernetwork(**args)\n            except Exception as e:\n                error = e\n            finally:\n                shared.sd_model.cond_stage_model.to(devices.device)\n                shared.sd_model.first_stage_model.to(devices.device)\n                if not apply_optimizations:\n                    sd_hijack.apply_optimizations()\n                shared.state.end()\n            return models.TrainResponse(info=f\"train embedding complete: filename: {filename} error: {error}\")\n        except Exception as exc:\n            return models.TrainResponse(info=f\"train embedding error: {exc}\")\n        finally:\n            shared.state.end()\n\n    def get_memory(self):\n        try:\n            import os\n            import psutil\n            process = psutil.Process(os.getpid())\n            res = process.memory_info() # only rss is cross-platform guaranteed so we dont rely on other values\n            ram_total = 100 * res.rss / process.memory_percent() # and total memory is calculated as actual value is not cross-platform safe\n            ram = { 'free': ram_total - res.rss, 'used': res.rss, 'total': ram_total }\n        except Exception as err:\n            ram = { 'error': f'{err}' }\n        try:\n            import torch\n            if torch.cuda.is_available():\n                s = torch.cuda.mem_get_info()\n                system = { 'free': s[0], 'used': s[1] - s[0], 'total': s[1] }\n                s = dict(torch.cuda.memory_stats(shared.device))\n                allocated = { 'current': s['allocated_bytes.all.current'], 'peak': s['allocated_bytes.all.peak'] }\n                reserved = { 'current': s['reserved_bytes.all.current'], 'peak': s['reserved_bytes.all.peak'] }\n                active = { 'current': s['active_bytes.all.current'], 'peak': s['active_bytes.all.peak'] }\n                inactive = { 'current': s['inactive_split_bytes.all.current'], 'peak': s['inactive_split_bytes.all.peak'] }\n                warnings = { 'retries': s['num_alloc_retries'], 'oom': s['num_ooms'] }\n                cuda = {\n                    'system': system,\n                    'active': active,\n                    'allocated': allocated,\n                    'reserved': reserved,\n                    'inactive': inactive,\n                    'events': warnings,\n                }\n            else:\n                cuda = {'error': 'unavailable'}\n        except Exception as err:\n            cuda = {'error': f'{err}'}\n        return models.MemoryResponse(ram=ram, cuda=cuda)\n\n    def get_extensions_list(self):\n        from modules import extensions\n        extensions.list_extensions()\n        ext_list = []\n        for ext in extensions.extensions:\n            ext: extensions.Extension\n            ext.read_info_from_repo()\n            if ext.remote is not None:\n                ext_list.append({\n                    \"name\": ext.name,\n                    \"remote\": ext.remote,\n                    \"branch\": ext.branch,\n                    \"commit_hash\":ext.commit_hash,\n                    \"commit_date\":ext.commit_date,\n                    \"version\":ext.version,\n                    \"enabled\":ext.enabled\n                })\n        return ext_list\n\n    def launch(self, server_name, port, root_path):\n        self.app.include_router(self.router)\n        uvicorn.run(\n            self.app,\n            host=server_name,\n            port=port,\n            timeout_keep_alive=shared.cmd_opts.timeout_keep_alive,\n            root_path=root_path,\n            ssl_keyfile=shared.cmd_opts.tls_keyfile,\n            ssl_certfile=shared.cmd_opts.tls_certfile\n        )\n\n    def kill_webui(self):\n        restart.stop_program()\n\n    def restart_webui(self):\n        if restart.is_restartable():\n            restart.restart_program()\n        return Response(status_code=501)\n\n    def stop_webui(request):\n        shared.state.server_command = \"stop\"\n        return Response(\"Stopping.\")\n\n", "modules/api/models.py": "import inspect\n\nfrom pydantic import BaseModel, Field, create_model\nfrom typing import Any, Optional, Literal\nfrom inflection import underscore\nfrom modules.processing import StableDiffusionProcessingTxt2Img, StableDiffusionProcessingImg2Img\nfrom modules.shared import sd_upscalers, opts, parser\n\nAPI_NOT_ALLOWED = [\n    \"self\",\n    \"kwargs\",\n    \"sd_model\",\n    \"outpath_samples\",\n    \"outpath_grids\",\n    \"sampler_index\",\n    # \"do_not_save_samples\",\n    # \"do_not_save_grid\",\n    \"extra_generation_params\",\n    \"overlay_images\",\n    \"do_not_reload_embeddings\",\n    \"seed_enable_extras\",\n    \"prompt_for_display\",\n    \"sampler_noise_scheduler_override\",\n    \"ddim_discretize\"\n]\n\nclass ModelDef(BaseModel):\n    \"\"\"Assistance Class for Pydantic Dynamic Model Generation\"\"\"\n\n    field: str\n    field_alias: str\n    field_type: Any\n    field_value: Any\n    field_exclude: bool = False\n\n\nclass PydanticModelGenerator:\n    \"\"\"\n    Takes in created classes and stubs them out in a way FastAPI/Pydantic is happy about:\n    source_data is a snapshot of the default values produced by the class\n    params are the names of the actual keys required by __init__\n    \"\"\"\n\n    def __init__(\n        self,\n        model_name: str = None,\n        class_instance = None,\n        additional_fields = None,\n    ):\n        def field_type_generator(k, v):\n            field_type = v.annotation\n\n            if field_type == 'Image':\n                # images are sent as base64 strings via API\n                field_type = 'str'\n\n            return Optional[field_type]\n\n        def merge_class_params(class_):\n            all_classes = list(filter(lambda x: x is not object, inspect.getmro(class_)))\n            parameters = {}\n            for classes in all_classes:\n                parameters = {**parameters, **inspect.signature(classes.__init__).parameters}\n            return parameters\n\n        self._model_name = model_name\n        self._class_data = merge_class_params(class_instance)\n\n        self._model_def = [\n            ModelDef(\n                field=underscore(k),\n                field_alias=k,\n                field_type=field_type_generator(k, v),\n                field_value=None if isinstance(v.default, property) else v.default\n            )\n            for (k,v) in self._class_data.items() if k not in API_NOT_ALLOWED\n        ]\n\n        for fields in additional_fields:\n            self._model_def.append(ModelDef(\n                field=underscore(fields[\"key\"]),\n                field_alias=fields[\"key\"],\n                field_type=fields[\"type\"],\n                field_value=fields[\"default\"],\n                field_exclude=fields[\"exclude\"] if \"exclude\" in fields else False))\n\n    def generate_model(self):\n        \"\"\"\n        Creates a pydantic BaseModel\n        from the json and overrides provided at initialization\n        \"\"\"\n        fields = {\n            d.field: (d.field_type, Field(default=d.field_value, alias=d.field_alias, exclude=d.field_exclude)) for d in self._model_def\n        }\n        DynamicModel = create_model(self._model_name, **fields)\n        DynamicModel.__config__.allow_population_by_field_name = True\n        DynamicModel.__config__.allow_mutation = True\n        return DynamicModel\n\nStableDiffusionTxt2ImgProcessingAPI = PydanticModelGenerator(\n    \"StableDiffusionProcessingTxt2Img\",\n    StableDiffusionProcessingTxt2Img,\n    [\n        {\"key\": \"sampler_index\", \"type\": str, \"default\": \"Euler\"},\n        {\"key\": \"script_name\", \"type\": str, \"default\": None},\n        {\"key\": \"script_args\", \"type\": list, \"default\": []},\n        {\"key\": \"send_images\", \"type\": bool, \"default\": True},\n        {\"key\": \"save_images\", \"type\": bool, \"default\": False},\n        {\"key\": \"alwayson_scripts\", \"type\": dict, \"default\": {}},\n        {\"key\": \"force_task_id\", \"type\": str, \"default\": None},\n        {\"key\": \"infotext\", \"type\": str, \"default\": None},\n    ]\n).generate_model()\n\nStableDiffusionImg2ImgProcessingAPI = PydanticModelGenerator(\n    \"StableDiffusionProcessingImg2Img\",\n    StableDiffusionProcessingImg2Img,\n    [\n        {\"key\": \"sampler_index\", \"type\": str, \"default\": \"Euler\"},\n        {\"key\": \"init_images\", \"type\": list, \"default\": None},\n        {\"key\": \"denoising_strength\", \"type\": float, \"default\": 0.75},\n        {\"key\": \"mask\", \"type\": str, \"default\": None},\n        {\"key\": \"include_init_images\", \"type\": bool, \"default\": False, \"exclude\" : True},\n        {\"key\": \"script_name\", \"type\": str, \"default\": None},\n        {\"key\": \"script_args\", \"type\": list, \"default\": []},\n        {\"key\": \"send_images\", \"type\": bool, \"default\": True},\n        {\"key\": \"save_images\", \"type\": bool, \"default\": False},\n        {\"key\": \"alwayson_scripts\", \"type\": dict, \"default\": {}},\n        {\"key\": \"force_task_id\", \"type\": str, \"default\": None},\n        {\"key\": \"infotext\", \"type\": str, \"default\": None},\n    ]\n).generate_model()\n\nclass TextToImageResponse(BaseModel):\n    images: list[str] = Field(default=None, title=\"Image\", description=\"The generated image in base64 format.\")\n    parameters: dict\n    info: str\n\nclass ImageToImageResponse(BaseModel):\n    images: list[str] = Field(default=None, title=\"Image\", description=\"The generated image in base64 format.\")\n    parameters: dict\n    info: str\n\nclass ExtrasBaseRequest(BaseModel):\n    resize_mode: Literal[0, 1] = Field(default=0, title=\"Resize Mode\", description=\"Sets the resize mode: 0 to upscale by upscaling_resize amount, 1 to upscale up to upscaling_resize_h x upscaling_resize_w.\")\n    show_extras_results: bool = Field(default=True, title=\"Show results\", description=\"Should the backend return the generated image?\")\n    gfpgan_visibility: float = Field(default=0, title=\"GFPGAN Visibility\", ge=0, le=1, allow_inf_nan=False, description=\"Sets the visibility of GFPGAN, values should be between 0 and 1.\")\n    codeformer_visibility: float = Field(default=0, title=\"CodeFormer Visibility\", ge=0, le=1, allow_inf_nan=False, description=\"Sets the visibility of CodeFormer, values should be between 0 and 1.\")\n    codeformer_weight: float = Field(default=0, title=\"CodeFormer Weight\", ge=0, le=1, allow_inf_nan=False, description=\"Sets the weight of CodeFormer, values should be between 0 and 1.\")\n    upscaling_resize: float = Field(default=2, title=\"Upscaling Factor\", gt=0, description=\"By how much to upscale the image, only used when resize_mode=0.\")\n    upscaling_resize_w: int = Field(default=512, title=\"Target Width\", ge=1, description=\"Target width for the upscaler to hit. Only used when resize_mode=1.\")\n    upscaling_resize_h: int = Field(default=512, title=\"Target Height\", ge=1, description=\"Target height for the upscaler to hit. Only used when resize_mode=1.\")\n    upscaling_crop: bool = Field(default=True, title=\"Crop to fit\", description=\"Should the upscaler crop the image to fit in the chosen size?\")\n    upscaler_1: str = Field(default=\"None\", title=\"Main upscaler\", description=f\"The name of the main upscaler to use, it has to be one of this list: {' , '.join([x.name for x in sd_upscalers])}\")\n    upscaler_2: str = Field(default=\"None\", title=\"Secondary upscaler\", description=f\"The name of the secondary upscaler to use, it has to be one of this list: {' , '.join([x.name for x in sd_upscalers])}\")\n    extras_upscaler_2_visibility: float = Field(default=0, title=\"Secondary upscaler visibility\", ge=0, le=1, allow_inf_nan=False, description=\"Sets the visibility of secondary upscaler, values should be between 0 and 1.\")\n    upscale_first: bool = Field(default=False, title=\"Upscale first\", description=\"Should the upscaler run before restoring faces?\")\n\nclass ExtraBaseResponse(BaseModel):\n    html_info: str = Field(title=\"HTML info\", description=\"A series of HTML tags containing the process info.\")\n\nclass ExtrasSingleImageRequest(ExtrasBaseRequest):\n    image: str = Field(default=\"\", title=\"Image\", description=\"Image to work on, must be a Base64 string containing the image's data.\")\n\nclass ExtrasSingleImageResponse(ExtraBaseResponse):\n    image: str = Field(default=None, title=\"Image\", description=\"The generated image in base64 format.\")\n\nclass FileData(BaseModel):\n    data: str = Field(title=\"File data\", description=\"Base64 representation of the file\")\n    name: str = Field(title=\"File name\")\n\nclass ExtrasBatchImagesRequest(ExtrasBaseRequest):\n    imageList: list[FileData] = Field(title=\"Images\", description=\"List of images to work on. Must be Base64 strings\")\n\nclass ExtrasBatchImagesResponse(ExtraBaseResponse):\n    images: list[str] = Field(title=\"Images\", description=\"The generated images in base64 format.\")\n\nclass PNGInfoRequest(BaseModel):\n    image: str = Field(title=\"Image\", description=\"The base64 encoded PNG image\")\n\nclass PNGInfoResponse(BaseModel):\n    info: str = Field(title=\"Image info\", description=\"A string with the parameters used to generate the image\")\n    items: dict = Field(title=\"Items\", description=\"A dictionary containing all the other fields the image had\")\n    parameters: dict = Field(title=\"Parameters\", description=\"A dictionary with parsed generation info fields\")\n\nclass ProgressRequest(BaseModel):\n    skip_current_image: bool = Field(default=False, title=\"Skip current image\", description=\"Skip current image serialization\")\n\nclass ProgressResponse(BaseModel):\n    progress: float = Field(title=\"Progress\", description=\"The progress with a range of 0 to 1\")\n    eta_relative: float = Field(title=\"ETA in secs\")\n    state: dict = Field(title=\"State\", description=\"The current state snapshot\")\n    current_image: str = Field(default=None, title=\"Current image\", description=\"The current image in base64 format. opts.show_progress_every_n_steps is required for this to work.\")\n    textinfo: str = Field(default=None, title=\"Info text\", description=\"Info text used by WebUI.\")\n\nclass InterrogateRequest(BaseModel):\n    image: str = Field(default=\"\", title=\"Image\", description=\"Image to work on, must be a Base64 string containing the image's data.\")\n    model: str = Field(default=\"clip\", title=\"Model\", description=\"The interrogate model used.\")\n\nclass InterrogateResponse(BaseModel):\n    caption: str = Field(default=None, title=\"Caption\", description=\"The generated caption for the image.\")\n\nclass TrainResponse(BaseModel):\n    info: str = Field(title=\"Train info\", description=\"Response string from train embedding or hypernetwork task.\")\n\nclass CreateResponse(BaseModel):\n    info: str = Field(title=\"Create info\", description=\"Response string from create embedding or hypernetwork task.\")\n\nfields = {}\nfor key, metadata in opts.data_labels.items():\n    value = opts.data.get(key)\n    optType = opts.typemap.get(type(metadata.default), type(metadata.default)) if metadata.default else Any\n\n    if metadata is not None:\n        fields.update({key: (Optional[optType], Field(default=metadata.default, description=metadata.label))})\n    else:\n        fields.update({key: (Optional[optType], Field())})\n\nOptionsModel = create_model(\"Options\", **fields)\n\nflags = {}\n_options = vars(parser)['_option_string_actions']\nfor key in _options:\n    if(_options[key].dest != 'help'):\n        flag = _options[key]\n        _type = str\n        if _options[key].default is not None:\n            _type = type(_options[key].default)\n        flags.update({flag.dest: (_type, Field(default=flag.default, description=flag.help))})\n\nFlagsModel = create_model(\"Flags\", **flags)\n\nclass SamplerItem(BaseModel):\n    name: str = Field(title=\"Name\")\n    aliases: list[str] = Field(title=\"Aliases\")\n    options: dict[str, str] = Field(title=\"Options\")\n\nclass SchedulerItem(BaseModel):\n    name: str = Field(title=\"Name\")\n    label: str = Field(title=\"Label\")\n    aliases: Optional[list[str]] = Field(title=\"Aliases\")\n    default_rho: Optional[float] = Field(title=\"Default Rho\")\n    need_inner_model: Optional[bool] = Field(title=\"Needs Inner Model\")\n\nclass UpscalerItem(BaseModel):\n    name: str = Field(title=\"Name\")\n    model_name: Optional[str] = Field(title=\"Model Name\")\n    model_path: Optional[str] = Field(title=\"Path\")\n    model_url: Optional[str] = Field(title=\"URL\")\n    scale: Optional[float] = Field(title=\"Scale\")\n\nclass LatentUpscalerModeItem(BaseModel):\n    name: str = Field(title=\"Name\")\n\nclass SDModelItem(BaseModel):\n    title: str = Field(title=\"Title\")\n    model_name: str = Field(title=\"Model Name\")\n    hash: Optional[str] = Field(title=\"Short hash\")\n    sha256: Optional[str] = Field(title=\"sha256 hash\")\n    filename: str = Field(title=\"Filename\")\n    config: Optional[str] = Field(title=\"Config file\")\n\nclass SDVaeItem(BaseModel):\n    model_name: str = Field(title=\"Model Name\")\n    filename: str = Field(title=\"Filename\")\n\nclass HypernetworkItem(BaseModel):\n    name: str = Field(title=\"Name\")\n    path: Optional[str] = Field(title=\"Path\")\n\nclass FaceRestorerItem(BaseModel):\n    name: str = Field(title=\"Name\")\n    cmd_dir: Optional[str] = Field(title=\"Path\")\n\nclass RealesrganItem(BaseModel):\n    name: str = Field(title=\"Name\")\n    path: Optional[str] = Field(title=\"Path\")\n    scale: Optional[int] = Field(title=\"Scale\")\n\nclass PromptStyleItem(BaseModel):\n    name: str = Field(title=\"Name\")\n    prompt: Optional[str] = Field(title=\"Prompt\")\n    negative_prompt: Optional[str] = Field(title=\"Negative Prompt\")\n\n\nclass EmbeddingItem(BaseModel):\n    step: Optional[int] = Field(title=\"Step\", description=\"The number of steps that were used to train this embedding, if available\")\n    sd_checkpoint: Optional[str] = Field(title=\"SD Checkpoint\", description=\"The hash of the checkpoint this embedding was trained on, if available\")\n    sd_checkpoint_name: Optional[str] = Field(title=\"SD Checkpoint Name\", description=\"The name of the checkpoint this embedding was trained on, if available. Note that this is the name that was used by the trainer; for a stable identifier, use `sd_checkpoint` instead\")\n    shape: int = Field(title=\"Shape\", description=\"The length of each individual vector in the embedding\")\n    vectors: int = Field(title=\"Vectors\", description=\"The number of vectors in the embedding\")\n\nclass EmbeddingsResponse(BaseModel):\n    loaded: dict[str, EmbeddingItem] = Field(title=\"Loaded\", description=\"Embeddings loaded for the current model\")\n    skipped: dict[str, EmbeddingItem] = Field(title=\"Skipped\", description=\"Embeddings skipped for the current model (likely due to architecture incompatibility)\")\n\nclass MemoryResponse(BaseModel):\n    ram: dict = Field(title=\"RAM\", description=\"System memory stats\")\n    cuda: dict = Field(title=\"CUDA\", description=\"nVidia CUDA memory stats\")\n\n\nclass ScriptsList(BaseModel):\n    txt2img: list = Field(default=None, title=\"Txt2img\", description=\"Titles of scripts (txt2img)\")\n    img2img: list = Field(default=None, title=\"Img2img\", description=\"Titles of scripts (img2img)\")\n\n\nclass ScriptArg(BaseModel):\n    label: str = Field(default=None, title=\"Label\", description=\"Name of the argument in UI\")\n    value: Optional[Any] = Field(default=None, title=\"Value\", description=\"Default value of the argument\")\n    minimum: Optional[Any] = Field(default=None, title=\"Minimum\", description=\"Minimum allowed value for the argumentin UI\")\n    maximum: Optional[Any] = Field(default=None, title=\"Minimum\", description=\"Maximum allowed value for the argumentin UI\")\n    step: Optional[Any] = Field(default=None, title=\"Minimum\", description=\"Step for changing value of the argumentin UI\")\n    choices: Optional[list[str]] = Field(default=None, title=\"Choices\", description=\"Possible values for the argument\")\n\n\nclass ScriptInfo(BaseModel):\n    name: str = Field(default=None, title=\"Name\", description=\"Script name\")\n    is_alwayson: bool = Field(default=None, title=\"IsAlwayson\", description=\"Flag specifying whether this script is an alwayson script\")\n    is_img2img: bool = Field(default=None, title=\"IsImg2img\", description=\"Flag specifying whether this script is an img2img script\")\n    args: list[ScriptArg] = Field(title=\"Arguments\", description=\"List of script's arguments\")\n\nclass ExtensionItem(BaseModel):\n    name: str = Field(title=\"Name\", description=\"Extension name\")\n    remote: str = Field(title=\"Remote\", description=\"Extension Repository URL\")\n    branch: str = Field(title=\"Branch\", description=\"Extension Repository Branch\")\n    commit_hash: str = Field(title=\"Commit Hash\", description=\"Extension Repository Commit Hash\")\n    version: str = Field(title=\"Version\", description=\"Extension Version\")\n    commit_date: str = Field(title=\"Commit Date\", description=\"Extension Repository Commit Date\")\n    enabled: bool = Field(title=\"Enabled\", description=\"Flag specifying whether this extension is enabled\")\n", "modules/processing_scripts/refiner.py": "import gradio as gr\n\nfrom modules import scripts, sd_models\nfrom modules.infotext_utils import PasteField\nfrom modules.ui_common import create_refresh_button\nfrom modules.ui_components import InputAccordion\n\n\nclass ScriptRefiner(scripts.ScriptBuiltinUI):\n    section = \"accordions\"\n    create_group = False\n\n    def __init__(self):\n        pass\n\n    def title(self):\n        return \"Refiner\"\n\n    def show(self, is_img2img):\n        return scripts.AlwaysVisible\n\n    def ui(self, is_img2img):\n        with InputAccordion(False, label=\"Refiner\", elem_id=self.elem_id(\"enable\")) as enable_refiner:\n            with gr.Row():\n                refiner_checkpoint = gr.Dropdown(label='Checkpoint', elem_id=self.elem_id(\"checkpoint\"), choices=sd_models.checkpoint_tiles(), value='', tooltip=\"switch to another model in the middle of generation\")\n                create_refresh_button(refiner_checkpoint, sd_models.list_models, lambda: {\"choices\": sd_models.checkpoint_tiles()}, self.elem_id(\"checkpoint_refresh\"))\n\n                refiner_switch_at = gr.Slider(value=0.8, label=\"Switch at\", minimum=0.01, maximum=1.0, step=0.01, elem_id=self.elem_id(\"switch_at\"), tooltip=\"fraction of sampling steps when the switch to refiner model should happen; 1=never, 0.5=switch in the middle of generation\")\n\n        def lookup_checkpoint(title):\n            info = sd_models.get_closet_checkpoint_match(title)\n            return None if info is None else info.title\n\n        self.infotext_fields = [\n            PasteField(enable_refiner, lambda d: 'Refiner' in d),\n            PasteField(refiner_checkpoint, lambda d: lookup_checkpoint(d.get('Refiner')), api=\"refiner_checkpoint\"),\n            PasteField(refiner_switch_at, 'Refiner switch at', api=\"refiner_switch_at\"),\n        ]\n\n        return enable_refiner, refiner_checkpoint, refiner_switch_at\n\n    def setup(self, p, enable_refiner, refiner_checkpoint, refiner_switch_at):\n        # the actual implementation is in sd_samplers_common.py, apply_refiner\n\n        if not enable_refiner or refiner_checkpoint in (None, \"\", \"None\"):\n            p.refiner_checkpoint = None\n            p.refiner_switch_at = None\n        else:\n            p.refiner_checkpoint = refiner_checkpoint\n            p.refiner_switch_at = refiner_switch_at\n", "modules/processing_scripts/sampler.py": "import gradio as gr\n\nfrom modules import scripts, sd_samplers, sd_schedulers, shared\nfrom modules.infotext_utils import PasteField\nfrom modules.ui_components import FormRow, FormGroup\n\n\nclass ScriptSampler(scripts.ScriptBuiltinUI):\n    section = \"sampler\"\n\n    def __init__(self):\n        self.steps = None\n        self.sampler_name = None\n        self.scheduler = None\n\n    def title(self):\n        return \"Sampler\"\n\n    def ui(self, is_img2img):\n        sampler_names = [x.name for x in sd_samplers.visible_samplers()]\n        scheduler_names = [x.label for x in sd_schedulers.schedulers]\n\n        if shared.opts.samplers_in_dropdown:\n            with FormRow(elem_id=f\"sampler_selection_{self.tabname}\"):\n                self.sampler_name = gr.Dropdown(label='Sampling method', elem_id=f\"{self.tabname}_sampling\", choices=sampler_names, value=sampler_names[0])\n                self.scheduler = gr.Dropdown(label='Schedule type', elem_id=f\"{self.tabname}_scheduler\", choices=scheduler_names, value=scheduler_names[0])\n                self.steps = gr.Slider(minimum=1, maximum=150, step=1, elem_id=f\"{self.tabname}_steps\", label=\"Sampling steps\", value=20)\n        else:\n            with FormGroup(elem_id=f\"sampler_selection_{self.tabname}\"):\n                self.steps = gr.Slider(minimum=1, maximum=150, step=1, elem_id=f\"{self.tabname}_steps\", label=\"Sampling steps\", value=20)\n                self.sampler_name = gr.Radio(label='Sampling method', elem_id=f\"{self.tabname}_sampling\", choices=sampler_names, value=sampler_names[0])\n                self.scheduler = gr.Dropdown(label='Schedule type', elem_id=f\"{self.tabname}_scheduler\", choices=scheduler_names, value=scheduler_names[0])\n\n        self.infotext_fields = [\n            PasteField(self.steps, \"Steps\", api=\"steps\"),\n            PasteField(self.sampler_name, sd_samplers.get_sampler_from_infotext, api=\"sampler_name\"),\n            PasteField(self.scheduler, sd_samplers.get_scheduler_from_infotext, api=\"scheduler\"),\n        ]\n\n        return self.steps, self.sampler_name, self.scheduler\n\n    def setup(self, p, steps, sampler_name, scheduler):\n        p.steps = steps\n        p.sampler_name = sampler_name\n        p.scheduler = scheduler\n", "modules/processing_scripts/comments.py": "from modules import scripts, shared, script_callbacks\nimport re\n\n\ndef strip_comments(text):\n    text = re.sub('(^|\\n)#[^\\n]*(\\n|$)', '\\n', text)  # while line comment\n    text = re.sub('#[^\\n]*(\\n|$)', '\\n', text)  # in the middle of the line comment\n\n    return text\n\n\nclass ScriptStripComments(scripts.Script):\n    def title(self):\n        return \"Comments\"\n\n    def show(self, is_img2img):\n        return scripts.AlwaysVisible\n\n    def process(self, p, *args):\n        if not shared.opts.enable_prompt_comments:\n            return\n\n        p.all_prompts = [strip_comments(x) for x in p.all_prompts]\n        p.all_negative_prompts = [strip_comments(x) for x in p.all_negative_prompts]\n\n        p.main_prompt = strip_comments(p.main_prompt)\n        p.main_negative_prompt = strip_comments(p.main_negative_prompt)\n\n        if getattr(p, 'enable_hr', False):\n            p.all_hr_prompts = [strip_comments(x) for x in p.all_hr_prompts]\n            p.all_hr_negative_prompts = [strip_comments(x) for x in p.all_hr_negative_prompts]\n\n            p.hr_prompt = strip_comments(p.hr_prompt)\n            p.hr_negative_prompt = strip_comments(p.hr_negative_prompt)\n\n\ndef before_token_counter(params: script_callbacks.BeforeTokenCounterParams):\n    if not shared.opts.enable_prompt_comments:\n        return\n\n    params.prompt = strip_comments(params.prompt)\n\n\nscript_callbacks.on_before_token_counter(before_token_counter)\n\n\nshared.options_templates.update(shared.options_section(('sd', \"Stable Diffusion\", \"sd\"), {\n    \"enable_prompt_comments\": shared.OptionInfo(True, \"Enable comments\").info(\"Use # anywhere in the prompt to hide the text between # and the end of the line from the generation.\"),\n}))\n", "modules/processing_scripts/seed.py": "import json\n\nimport gradio as gr\n\nfrom modules import scripts, ui, errors\nfrom modules.infotext_utils import PasteField\nfrom modules.shared import cmd_opts\nfrom modules.ui_components import ToolButton\nfrom modules import infotext_utils\n\n\nclass ScriptSeed(scripts.ScriptBuiltinUI):\n    section = \"seed\"\n    create_group = False\n\n    def __init__(self):\n        self.seed = None\n        self.reuse_seed = None\n        self.reuse_subseed = None\n\n    def title(self):\n        return \"Seed\"\n\n    def show(self, is_img2img):\n        return scripts.AlwaysVisible\n\n    def ui(self, is_img2img):\n        with gr.Row(elem_id=self.elem_id(\"seed_row\")):\n            if cmd_opts.use_textbox_seed:\n                self.seed = gr.Textbox(label='Seed', value=\"\", elem_id=self.elem_id(\"seed\"), min_width=100)\n            else:\n                self.seed = gr.Number(label='Seed', value=-1, elem_id=self.elem_id(\"seed\"), min_width=100, precision=0)\n\n            random_seed = ToolButton(ui.random_symbol, elem_id=self.elem_id(\"random_seed\"), tooltip=\"Set seed to -1, which will cause a new random number to be used every time\")\n            reuse_seed = ToolButton(ui.reuse_symbol, elem_id=self.elem_id(\"reuse_seed\"), tooltip=\"Reuse seed from last generation, mostly useful if it was randomized\")\n\n            seed_checkbox = gr.Checkbox(label='Extra', elem_id=self.elem_id(\"subseed_show\"), value=False)\n\n        with gr.Group(visible=False, elem_id=self.elem_id(\"seed_extras\")) as seed_extras:\n            with gr.Row(elem_id=self.elem_id(\"subseed_row\")):\n                subseed = gr.Number(label='Variation seed', value=-1, elem_id=self.elem_id(\"subseed\"), precision=0)\n                random_subseed = ToolButton(ui.random_symbol, elem_id=self.elem_id(\"random_subseed\"))\n                reuse_subseed = ToolButton(ui.reuse_symbol, elem_id=self.elem_id(\"reuse_subseed\"))\n                subseed_strength = gr.Slider(label='Variation strength', value=0.0, minimum=0, maximum=1, step=0.01, elem_id=self.elem_id(\"subseed_strength\"))\n\n            with gr.Row(elem_id=self.elem_id(\"seed_resize_from_row\")):\n                seed_resize_from_w = gr.Slider(minimum=0, maximum=2048, step=8, label=\"Resize seed from width\", value=0, elem_id=self.elem_id(\"seed_resize_from_w\"))\n                seed_resize_from_h = gr.Slider(minimum=0, maximum=2048, step=8, label=\"Resize seed from height\", value=0, elem_id=self.elem_id(\"seed_resize_from_h\"))\n\n        random_seed.click(fn=None, _js=\"function(){setRandomSeed('\" + self.elem_id(\"seed\") + \"')}\", show_progress=False, inputs=[], outputs=[])\n        random_subseed.click(fn=None, _js=\"function(){setRandomSeed('\" + self.elem_id(\"subseed\") + \"')}\", show_progress=False, inputs=[], outputs=[])\n\n        seed_checkbox.change(lambda x: gr.update(visible=x), show_progress=False, inputs=[seed_checkbox], outputs=[seed_extras])\n\n        self.infotext_fields = [\n            PasteField(self.seed, \"Seed\", api=\"seed\"),\n            PasteField(seed_checkbox, lambda d: \"Variation seed\" in d or \"Seed resize from-1\" in d),\n            PasteField(subseed, \"Variation seed\", api=\"subseed\"),\n            PasteField(subseed_strength, \"Variation seed strength\", api=\"subseed_strength\"),\n            PasteField(seed_resize_from_w, \"Seed resize from-1\", api=\"seed_resize_from_h\"),\n            PasteField(seed_resize_from_h, \"Seed resize from-2\", api=\"seed_resize_from_w\"),\n        ]\n\n        self.on_after_component(lambda x: connect_reuse_seed(self.seed, reuse_seed, x.component, False), elem_id=f'generation_info_{self.tabname}')\n        self.on_after_component(lambda x: connect_reuse_seed(subseed, reuse_subseed, x.component, True), elem_id=f'generation_info_{self.tabname}')\n\n        return self.seed, seed_checkbox, subseed, subseed_strength, seed_resize_from_w, seed_resize_from_h\n\n    def setup(self, p, seed, seed_checkbox, subseed, subseed_strength, seed_resize_from_w, seed_resize_from_h):\n        p.seed = seed\n\n        if seed_checkbox and subseed_strength > 0:\n            p.subseed = subseed\n            p.subseed_strength = subseed_strength\n\n        if seed_checkbox and seed_resize_from_w > 0 and seed_resize_from_h > 0:\n            p.seed_resize_from_w = seed_resize_from_w\n            p.seed_resize_from_h = seed_resize_from_h\n\n\ndef connect_reuse_seed(seed: gr.Number, reuse_seed: gr.Button, generation_info: gr.Textbox, is_subseed):\n    \"\"\" Connects a 'reuse (sub)seed' button's click event so that it copies last used\n        (sub)seed value from generation info the to the seed field. If copying subseed and subseed strength\n        was 0, i.e. no variation seed was used, it copies the normal seed value instead.\"\"\"\n\n    def copy_seed(gen_info_string: str, index):\n        res = -1\n        try:\n            gen_info = json.loads(gen_info_string)\n            infotext = gen_info.get('infotexts')[index]\n            gen_parameters = infotext_utils.parse_generation_parameters(infotext, [])\n            res = int(gen_parameters.get('Variation seed' if is_subseed else 'Seed', -1))\n        except Exception:\n            if gen_info_string:\n                errors.report(f\"Error retrieving seed from generation info: {gen_info_string}\", exc_info=True)\n\n        return [res, gr.update()]\n\n    reuse_seed.click(\n        fn=copy_seed,\n        _js=\"(x, y) => [x, selected_gallery_index()]\",\n        show_progress=False,\n        inputs=[generation_info, seed],\n        outputs=[seed, seed]\n    )\n", "modules/textual_inversion/autocrop.py": "import cv2\nimport requests\nimport os\nimport numpy as np\nfrom PIL import ImageDraw\nfrom modules import paths_internal\nfrom pkg_resources import parse_version\n\nGREEN = \"#0F0\"\nBLUE = \"#00F\"\nRED = \"#F00\"\n\n\ndef crop_image(im, settings):\n    \"\"\" Intelligently crop an image to the subject matter \"\"\"\n\n    scale_by = 1\n    if is_landscape(im.width, im.height):\n        scale_by = settings.crop_height / im.height\n    elif is_portrait(im.width, im.height):\n        scale_by = settings.crop_width / im.width\n    elif is_square(im.width, im.height):\n        if is_square(settings.crop_width, settings.crop_height):\n            scale_by = settings.crop_width / im.width\n        elif is_landscape(settings.crop_width, settings.crop_height):\n            scale_by = settings.crop_width / im.width\n        elif is_portrait(settings.crop_width, settings.crop_height):\n            scale_by = settings.crop_height / im.height\n\n    im = im.resize((int(im.width * scale_by), int(im.height * scale_by)))\n    im_debug = im.copy()\n\n    focus = focal_point(im_debug, settings)\n\n    # take the focal point and turn it into crop coordinates that try to center over the focal\n    # point but then get adjusted back into the frame\n    y_half = int(settings.crop_height / 2)\n    x_half = int(settings.crop_width / 2)\n\n    x1 = focus.x - x_half\n    if x1 < 0:\n        x1 = 0\n    elif x1 + settings.crop_width > im.width:\n        x1 = im.width - settings.crop_width\n\n    y1 = focus.y - y_half\n    if y1 < 0:\n        y1 = 0\n    elif y1 + settings.crop_height > im.height:\n        y1 = im.height - settings.crop_height\n\n    x2 = x1 + settings.crop_width\n    y2 = y1 + settings.crop_height\n\n    crop = [x1, y1, x2, y2]\n\n    results = []\n\n    results.append(im.crop(tuple(crop)))\n\n    if settings.annotate_image:\n        d = ImageDraw.Draw(im_debug)\n        rect = list(crop)\n        rect[2] -= 1\n        rect[3] -= 1\n        d.rectangle(rect, outline=GREEN)\n        results.append(im_debug)\n        if settings.desktop_view_image:\n            im_debug.show()\n\n    return results\n\n\ndef focal_point(im, settings):\n    corner_points = image_corner_points(im, settings) if settings.corner_points_weight > 0 else []\n    entropy_points = image_entropy_points(im, settings) if settings.entropy_points_weight > 0 else []\n    face_points = image_face_points(im, settings) if settings.face_points_weight > 0 else []\n\n    pois = []\n\n    weight_pref_total = 0\n    if corner_points:\n        weight_pref_total += settings.corner_points_weight\n    if entropy_points:\n        weight_pref_total += settings.entropy_points_weight\n    if face_points:\n        weight_pref_total += settings.face_points_weight\n\n    corner_centroid = None\n    if corner_points:\n        corner_centroid = centroid(corner_points)\n        corner_centroid.weight = settings.corner_points_weight / weight_pref_total\n        pois.append(corner_centroid)\n\n    entropy_centroid = None\n    if entropy_points:\n        entropy_centroid = centroid(entropy_points)\n        entropy_centroid.weight = settings.entropy_points_weight / weight_pref_total\n        pois.append(entropy_centroid)\n\n    face_centroid = None\n    if face_points:\n        face_centroid = centroid(face_points)\n        face_centroid.weight = settings.face_points_weight / weight_pref_total\n        pois.append(face_centroid)\n\n    average_point = poi_average(pois, settings)\n\n    if settings.annotate_image:\n        d = ImageDraw.Draw(im)\n        max_size = min(im.width, im.height) * 0.07\n        if corner_centroid is not None:\n            color = BLUE\n            box = corner_centroid.bounding(max_size * corner_centroid.weight)\n            d.text((box[0], box[1] - 15), f\"Edge: {corner_centroid.weight:.02f}\", fill=color)\n            d.ellipse(box, outline=color)\n            if len(corner_points) > 1:\n                for f in corner_points:\n                    d.rectangle(f.bounding(4), outline=color)\n        if entropy_centroid is not None:\n            color = \"#ff0\"\n            box = entropy_centroid.bounding(max_size * entropy_centroid.weight)\n            d.text((box[0], box[1] - 15), f\"Entropy: {entropy_centroid.weight:.02f}\", fill=color)\n            d.ellipse(box, outline=color)\n            if len(entropy_points) > 1:\n                for f in entropy_points:\n                    d.rectangle(f.bounding(4), outline=color)\n        if face_centroid is not None:\n            color = RED\n            box = face_centroid.bounding(max_size * face_centroid.weight)\n            d.text((box[0], box[1] - 15), f\"Face: {face_centroid.weight:.02f}\", fill=color)\n            d.ellipse(box, outline=color)\n            if len(face_points) > 1:\n                for f in face_points:\n                    d.rectangle(f.bounding(4), outline=color)\n\n        d.ellipse(average_point.bounding(max_size), outline=GREEN)\n\n    return average_point\n\n\ndef image_face_points(im, settings):\n    if settings.dnn_model_path is not None:\n        detector = cv2.FaceDetectorYN.create(\n            settings.dnn_model_path,\n            \"\",\n            (im.width, im.height),\n            0.9,  # score threshold\n            0.3,  # nms threshold\n            5000  # keep top k before nms\n        )\n        faces = detector.detect(np.array(im))\n        results = []\n        if faces[1] is not None:\n            for face in faces[1]:\n                x = face[0]\n                y = face[1]\n                w = face[2]\n                h = face[3]\n                results.append(\n                    PointOfInterest(\n                        int(x + (w * 0.5)),  # face focus left/right is center\n                        int(y + (h * 0.33)),  # face focus up/down is close to the top of the head\n                        size=w,\n                        weight=1 / len(faces[1])\n                    )\n                )\n        return results\n    else:\n        np_im = np.array(im)\n        gray = cv2.cvtColor(np_im, cv2.COLOR_BGR2GRAY)\n\n        tries = [\n            [f'{cv2.data.haarcascades}haarcascade_eye.xml', 0.01],\n            [f'{cv2.data.haarcascades}haarcascade_frontalface_default.xml', 0.05],\n            [f'{cv2.data.haarcascades}haarcascade_profileface.xml', 0.05],\n            [f'{cv2.data.haarcascades}haarcascade_frontalface_alt.xml', 0.05],\n            [f'{cv2.data.haarcascades}haarcascade_frontalface_alt2.xml', 0.05],\n            [f'{cv2.data.haarcascades}haarcascade_frontalface_alt_tree.xml', 0.05],\n            [f'{cv2.data.haarcascades}haarcascade_eye_tree_eyeglasses.xml', 0.05],\n            [f'{cv2.data.haarcascades}haarcascade_upperbody.xml', 0.05]\n        ]\n        for t in tries:\n            classifier = cv2.CascadeClassifier(t[0])\n            minsize = int(min(im.width, im.height) * t[1])  # at least N percent of the smallest side\n            try:\n                faces = classifier.detectMultiScale(gray, scaleFactor=1.1,\n                                                    minNeighbors=7, minSize=(minsize, minsize),\n                                                    flags=cv2.CASCADE_SCALE_IMAGE)\n            except Exception:\n                continue\n\n            if faces:\n                rects = [[f[0], f[1], f[0] + f[2], f[1] + f[3]] for f in faces]\n                return [PointOfInterest((r[0] + r[2]) // 2, (r[1] + r[3]) // 2, size=abs(r[0] - r[2]),\n                                        weight=1 / len(rects)) for r in rects]\n    return []\n\n\ndef image_corner_points(im, settings):\n    grayscale = im.convert(\"L\")\n\n    # naive attempt at preventing focal points from collecting at watermarks near the bottom\n    gd = ImageDraw.Draw(grayscale)\n    gd.rectangle([0, im.height * .9, im.width, im.height], fill=\"#999\")\n\n    np_im = np.array(grayscale)\n\n    points = cv2.goodFeaturesToTrack(\n        np_im,\n        maxCorners=100,\n        qualityLevel=0.04,\n        minDistance=min(grayscale.width, grayscale.height) * 0.06,\n        useHarrisDetector=False,\n    )\n\n    if points is None:\n        return []\n\n    focal_points = []\n    for point in points:\n        x, y = point.ravel()\n        focal_points.append(PointOfInterest(x, y, size=4, weight=1 / len(points)))\n\n    return focal_points\n\n\ndef image_entropy_points(im, settings):\n    landscape = im.height < im.width\n    portrait = im.height > im.width\n    if landscape:\n        move_idx = [0, 2]\n        move_max = im.size[0]\n    elif portrait:\n        move_idx = [1, 3]\n        move_max = im.size[1]\n    else:\n        return []\n\n    e_max = 0\n    crop_current = [0, 0, settings.crop_width, settings.crop_height]\n    crop_best = crop_current\n    while crop_current[move_idx[1]] < move_max:\n        crop = im.crop(tuple(crop_current))\n        e = image_entropy(crop)\n\n        if (e > e_max):\n            e_max = e\n            crop_best = list(crop_current)\n\n        crop_current[move_idx[0]] += 4\n        crop_current[move_idx[1]] += 4\n\n    x_mid = int(crop_best[0] + settings.crop_width / 2)\n    y_mid = int(crop_best[1] + settings.crop_height / 2)\n\n    return [PointOfInterest(x_mid, y_mid, size=25, weight=1.0)]\n\n\ndef image_entropy(im):\n    # greyscale image entropy\n    # band = np.asarray(im.convert(\"L\"))\n    band = np.asarray(im.convert(\"1\"), dtype=np.uint8)\n    hist, _ = np.histogram(band, bins=range(0, 256))\n    hist = hist[hist > 0]\n    return -np.log2(hist / hist.sum()).sum()\n\n\ndef centroid(pois):\n    x = [poi.x for poi in pois]\n    y = [poi.y for poi in pois]\n    return PointOfInterest(sum(x) / len(pois), sum(y) / len(pois))\n\n\ndef poi_average(pois, settings):\n    weight = 0.0\n    x = 0.0\n    y = 0.0\n    for poi in pois:\n        weight += poi.weight\n        x += poi.x * poi.weight\n        y += poi.y * poi.weight\n    avg_x = round(weight and x / weight)\n    avg_y = round(weight and y / weight)\n\n    return PointOfInterest(avg_x, avg_y)\n\n\ndef is_landscape(w, h):\n    return w > h\n\n\ndef is_portrait(w, h):\n    return h > w\n\n\ndef is_square(w, h):\n    return w == h\n\n\nmodel_dir_opencv = os.path.join(paths_internal.models_path, 'opencv')\nif parse_version(cv2.__version__) >= parse_version('4.8'):\n    model_file_path = os.path.join(model_dir_opencv, 'face_detection_yunet_2023mar.onnx')\n    model_url = 'https://github.com/opencv/opencv_zoo/blob/b6e370b10f641879a87890d44e42173077154a05/models/face_detection_yunet/face_detection_yunet_2023mar.onnx?raw=true'\nelse:\n    model_file_path = os.path.join(model_dir_opencv, 'face_detection_yunet.onnx')\n    model_url = 'https://github.com/opencv/opencv_zoo/blob/91fb0290f50896f38a0ab1e558b74b16bc009428/models/face_detection_yunet/face_detection_yunet_2022mar.onnx?raw=true'\n\n\ndef download_and_cache_models():\n    if not os.path.exists(model_file_path):\n        os.makedirs(model_dir_opencv, exist_ok=True)\n        print(f\"downloading face detection model from '{model_url}' to '{model_file_path}'\")\n        response = requests.get(model_url)\n        with open(model_file_path, \"wb\") as f:\n            f.write(response.content)\n    return model_file_path\n\n\nclass PointOfInterest:\n    def __init__(self, x, y, weight=1.0, size=10):\n        self.x = x\n        self.y = y\n        self.weight = weight\n        self.size = size\n\n    def bounding(self, size):\n        return [\n            self.x - size // 2,\n            self.y - size // 2,\n            self.x + size // 2,\n            self.y + size // 2\n        ]\n\n\nclass Settings:\n    def __init__(self, crop_width=512, crop_height=512, corner_points_weight=0.5, entropy_points_weight=0.5, face_points_weight=0.5, annotate_image=False, dnn_model_path=None):\n        self.crop_width = crop_width\n        self.crop_height = crop_height\n        self.corner_points_weight = corner_points_weight\n        self.entropy_points_weight = entropy_points_weight\n        self.face_points_weight = face_points_weight\n        self.annotate_image = annotate_image\n        self.desktop_view_image = False\n        self.dnn_model_path = dnn_model_path\n", "modules/textual_inversion/textual_inversion.py": "import os\nfrom collections import namedtuple\nfrom contextlib import closing\n\nimport torch\nimport tqdm\nimport html\nimport datetime\nimport csv\nimport safetensors.torch\n\nimport numpy as np\nfrom PIL import Image, PngImagePlugin\n\nfrom modules import shared, devices, sd_hijack, sd_models, images, sd_samplers, sd_hijack_checkpoint, errors, hashes\nimport modules.textual_inversion.dataset\nfrom modules.textual_inversion.learn_schedule import LearnRateScheduler\n\nfrom modules.textual_inversion.image_embedding import embedding_to_b64, embedding_from_b64, insert_image_data_embed, extract_image_data_embed, caption_image_overlay\nfrom modules.textual_inversion.saving_settings import save_settings_to_file\n\n\nTextualInversionTemplate = namedtuple(\"TextualInversionTemplate\", [\"name\", \"path\"])\ntextual_inversion_templates = {}\n\n\ndef list_textual_inversion_templates():\n    textual_inversion_templates.clear()\n\n    for root, _, fns in os.walk(shared.cmd_opts.textual_inversion_templates_dir):\n        for fn in fns:\n            path = os.path.join(root, fn)\n\n            textual_inversion_templates[fn] = TextualInversionTemplate(fn, path)\n\n    return textual_inversion_templates\n\n\nclass Embedding:\n    def __init__(self, vec, name, step=None):\n        self.vec = vec\n        self.name = name\n        self.step = step\n        self.shape = None\n        self.vectors = 0\n        self.cached_checksum = None\n        self.sd_checkpoint = None\n        self.sd_checkpoint_name = None\n        self.optimizer_state_dict = None\n        self.filename = None\n        self.hash = None\n        self.shorthash = None\n\n    def save(self, filename):\n        embedding_data = {\n            \"string_to_token\": {\"*\": 265},\n            \"string_to_param\": {\"*\": self.vec},\n            \"name\": self.name,\n            \"step\": self.step,\n            \"sd_checkpoint\": self.sd_checkpoint,\n            \"sd_checkpoint_name\": self.sd_checkpoint_name,\n        }\n\n        torch.save(embedding_data, filename)\n\n        if shared.opts.save_optimizer_state and self.optimizer_state_dict is not None:\n            optimizer_saved_dict = {\n                'hash': self.checksum(),\n                'optimizer_state_dict': self.optimizer_state_dict,\n            }\n            torch.save(optimizer_saved_dict, f\"{filename}.optim\")\n\n    def checksum(self):\n        if self.cached_checksum is not None:\n            return self.cached_checksum\n\n        def const_hash(a):\n            r = 0\n            for v in a:\n                r = (r * 281 ^ int(v) * 997) & 0xFFFFFFFF\n            return r\n\n        self.cached_checksum = f'{const_hash(self.vec.reshape(-1) * 100) & 0xffff:04x}'\n        return self.cached_checksum\n\n    def set_hash(self, v):\n        self.hash = v\n        self.shorthash = self.hash[0:12]\n\n\nclass DirWithTextualInversionEmbeddings:\n    def __init__(self, path):\n        self.path = path\n        self.mtime = None\n\n    def has_changed(self):\n        if not os.path.isdir(self.path):\n            return False\n\n        mt = os.path.getmtime(self.path)\n        if self.mtime is None or mt > self.mtime:\n            return True\n\n    def update(self):\n        if not os.path.isdir(self.path):\n            return\n\n        self.mtime = os.path.getmtime(self.path)\n\n\nclass EmbeddingDatabase:\n    def __init__(self):\n        self.ids_lookup = {}\n        self.word_embeddings = {}\n        self.skipped_embeddings = {}\n        self.expected_shape = -1\n        self.embedding_dirs = {}\n        self.previously_displayed_embeddings = ()\n\n    def add_embedding_dir(self, path):\n        self.embedding_dirs[path] = DirWithTextualInversionEmbeddings(path)\n\n    def clear_embedding_dirs(self):\n        self.embedding_dirs.clear()\n\n    def register_embedding(self, embedding, model):\n        return self.register_embedding_by_name(embedding, model, embedding.name)\n\n    def register_embedding_by_name(self, embedding, model, name):\n        ids = model.cond_stage_model.tokenize([name])[0]\n        first_id = ids[0]\n        if first_id not in self.ids_lookup:\n            self.ids_lookup[first_id] = []\n        if name in self.word_embeddings:\n            # remove old one from the lookup list\n            lookup = [x for x in self.ids_lookup[first_id] if x[1].name!=name]\n        else:\n            lookup = self.ids_lookup[first_id]\n        if embedding is not None:\n            lookup += [(ids, embedding)]\n        self.ids_lookup[first_id] = sorted(lookup, key=lambda x: len(x[0]), reverse=True)\n        if embedding is None:\n            # unregister embedding with specified name\n            if name in self.word_embeddings:\n                del self.word_embeddings[name]\n            if len(self.ids_lookup[first_id])==0:\n                del self.ids_lookup[first_id]\n            return None\n        self.word_embeddings[name] = embedding\n        return embedding\n\n    def get_expected_shape(self):\n        devices.torch_npu_set_device()\n        vec = shared.sd_model.cond_stage_model.encode_embedding_init_text(\",\", 1)\n        return vec.shape[1]\n\n    def load_from_file(self, path, filename):\n        name, ext = os.path.splitext(filename)\n        ext = ext.upper()\n\n        if ext in ['.PNG', '.WEBP', '.JXL', '.AVIF']:\n            _, second_ext = os.path.splitext(name)\n            if second_ext.upper() == '.PREVIEW':\n                return\n\n            embed_image = Image.open(path)\n            if hasattr(embed_image, 'text') and 'sd-ti-embedding' in embed_image.text:\n                data = embedding_from_b64(embed_image.text['sd-ti-embedding'])\n                name = data.get('name', name)\n            else:\n                data = extract_image_data_embed(embed_image)\n                if data:\n                    name = data.get('name', name)\n                else:\n                    # if data is None, means this is not an embedding, just a preview image\n                    return\n        elif ext in ['.BIN', '.PT']:\n            data = torch.load(path, map_location=\"cpu\")\n        elif ext in ['.SAFETENSORS']:\n            data = safetensors.torch.load_file(path, device=\"cpu\")\n        else:\n            return\n\n        embedding = create_embedding_from_data(data, name, filename=filename, filepath=path)\n\n        if self.expected_shape == -1 or self.expected_shape == embedding.shape:\n            self.register_embedding(embedding, shared.sd_model)\n        else:\n            self.skipped_embeddings[name] = embedding\n\n    def load_from_dir(self, embdir):\n        if not os.path.isdir(embdir.path):\n            return\n\n        for root, _, fns in os.walk(embdir.path, followlinks=True):\n            for fn in fns:\n                try:\n                    fullfn = os.path.join(root, fn)\n\n                    if os.stat(fullfn).st_size == 0:\n                        continue\n\n                    self.load_from_file(fullfn, fn)\n                except Exception:\n                    errors.report(f\"Error loading embedding {fn}\", exc_info=True)\n                    continue\n\n    def load_textual_inversion_embeddings(self, force_reload=False):\n        if not force_reload:\n            need_reload = False\n            for embdir in self.embedding_dirs.values():\n                if embdir.has_changed():\n                    need_reload = True\n                    break\n\n            if not need_reload:\n                return\n\n        self.ids_lookup.clear()\n        self.word_embeddings.clear()\n        self.skipped_embeddings.clear()\n        self.expected_shape = self.get_expected_shape()\n\n        for embdir in self.embedding_dirs.values():\n            self.load_from_dir(embdir)\n            embdir.update()\n\n        # re-sort word_embeddings because load_from_dir may not load in alphabetic order.\n        # using a temporary copy so we don't reinitialize self.word_embeddings in case other objects have a reference to it.\n        sorted_word_embeddings = {e.name: e for e in sorted(self.word_embeddings.values(), key=lambda e: e.name.lower())}\n        self.word_embeddings.clear()\n        self.word_embeddings.update(sorted_word_embeddings)\n\n        displayed_embeddings = (tuple(self.word_embeddings.keys()), tuple(self.skipped_embeddings.keys()))\n        if shared.opts.textual_inversion_print_at_load and self.previously_displayed_embeddings != displayed_embeddings:\n            self.previously_displayed_embeddings = displayed_embeddings\n            print(f\"Textual inversion embeddings loaded({len(self.word_embeddings)}): {', '.join(self.word_embeddings.keys())}\")\n            if self.skipped_embeddings:\n                print(f\"Textual inversion embeddings skipped({len(self.skipped_embeddings)}): {', '.join(self.skipped_embeddings.keys())}\")\n\n    def find_embedding_at_position(self, tokens, offset):\n        token = tokens[offset]\n        possible_matches = self.ids_lookup.get(token, None)\n\n        if possible_matches is None:\n            return None, None\n\n        for ids, embedding in possible_matches:\n            if tokens[offset:offset + len(ids)] == ids:\n                return embedding, len(ids)\n\n        return None, None\n\n\ndef create_embedding(name, num_vectors_per_token, overwrite_old, init_text='*'):\n    cond_model = shared.sd_model.cond_stage_model\n\n    with devices.autocast():\n        cond_model([\"\"])  # will send cond model to GPU if lowvram/medvram is active\n\n    #cond_model expects at least some text, so we provide '*' as backup.\n    embedded = cond_model.encode_embedding_init_text(init_text or '*', num_vectors_per_token)\n    vec = torch.zeros((num_vectors_per_token, embedded.shape[1]), device=devices.device)\n\n    #Only copy if we provided an init_text, otherwise keep vectors as zeros\n    if init_text:\n        for i in range(num_vectors_per_token):\n            vec[i] = embedded[i * int(embedded.shape[0]) // num_vectors_per_token]\n\n    # Remove illegal characters from name.\n    name = \"\".join( x for x in name if (x.isalnum() or x in \"._- \"))\n    fn = os.path.join(shared.cmd_opts.embeddings_dir, f\"{name}.pt\")\n    if not overwrite_old:\n        assert not os.path.exists(fn), f\"file {fn} already exists\"\n\n    embedding = Embedding(vec, name)\n    embedding.step = 0\n    embedding.save(fn)\n\n    return fn\n\n\ndef create_embedding_from_data(data, name, filename='unknown embedding file', filepath=None):\n    if 'string_to_param' in data:  # textual inversion embeddings\n        param_dict = data['string_to_param']\n        param_dict = getattr(param_dict, '_parameters', param_dict)  # fix for torch 1.12.1 loading saved file from torch 1.11\n        assert len(param_dict) == 1, 'embedding file has multiple terms in it'\n        emb = next(iter(param_dict.items()))[1]\n        vec = emb.detach().to(devices.device, dtype=torch.float32)\n        shape = vec.shape[-1]\n        vectors = vec.shape[0]\n    elif type(data) == dict and 'clip_g' in data and 'clip_l' in data:  # SDXL embedding\n        vec = {k: v.detach().to(devices.device, dtype=torch.float32) for k, v in data.items()}\n        shape = data['clip_g'].shape[-1] + data['clip_l'].shape[-1]\n        vectors = data['clip_g'].shape[0]\n    elif type(data) == dict and type(next(iter(data.values()))) == torch.Tensor:  # diffuser concepts\n        assert len(data.keys()) == 1, 'embedding file has multiple terms in it'\n\n        emb = next(iter(data.values()))\n        if len(emb.shape) == 1:\n            emb = emb.unsqueeze(0)\n        vec = emb.detach().to(devices.device, dtype=torch.float32)\n        shape = vec.shape[-1]\n        vectors = vec.shape[0]\n    else:\n        raise Exception(f\"Couldn't identify {filename} as neither textual inversion embedding nor diffuser concept.\")\n\n    embedding = Embedding(vec, name)\n    embedding.step = data.get('step', None)\n    embedding.sd_checkpoint = data.get('sd_checkpoint', None)\n    embedding.sd_checkpoint_name = data.get('sd_checkpoint_name', None)\n    embedding.vectors = vectors\n    embedding.shape = shape\n\n    if filepath:\n        embedding.filename = filepath\n        embedding.set_hash(hashes.sha256(filepath, \"textual_inversion/\" + name) or '')\n\n    return embedding\n\n\ndef write_loss(log_directory, filename, step, epoch_len, values):\n    if shared.opts.training_write_csv_every == 0:\n        return\n\n    if step % shared.opts.training_write_csv_every != 0:\n        return\n    write_csv_header = False if os.path.exists(os.path.join(log_directory, filename)) else True\n\n    with open(os.path.join(log_directory, filename), \"a+\", newline='') as fout:\n        csv_writer = csv.DictWriter(fout, fieldnames=[\"step\", \"epoch\", \"epoch_step\", *(values.keys())])\n\n        if write_csv_header:\n            csv_writer.writeheader()\n\n        epoch = (step - 1) // epoch_len\n        epoch_step = (step - 1) % epoch_len\n\n        csv_writer.writerow({\n            \"step\": step,\n            \"epoch\": epoch,\n            \"epoch_step\": epoch_step,\n            **values,\n        })\n\ndef tensorboard_setup(log_directory):\n    from torch.utils.tensorboard import SummaryWriter\n    os.makedirs(os.path.join(log_directory, \"tensorboard\"), exist_ok=True)\n    return SummaryWriter(\n            log_dir=os.path.join(log_directory, \"tensorboard\"),\n            flush_secs=shared.opts.training_tensorboard_flush_every)\n\ndef tensorboard_add(tensorboard_writer, loss, global_step, step, learn_rate, epoch_num):\n    tensorboard_add_scaler(tensorboard_writer, \"Loss/train\", loss, global_step)\n    tensorboard_add_scaler(tensorboard_writer, f\"Loss/train/epoch-{epoch_num}\", loss, step)\n    tensorboard_add_scaler(tensorboard_writer, \"Learn rate/train\", learn_rate, global_step)\n    tensorboard_add_scaler(tensorboard_writer, f\"Learn rate/train/epoch-{epoch_num}\", learn_rate, step)\n\ndef tensorboard_add_scaler(tensorboard_writer, tag, value, step):\n    tensorboard_writer.add_scalar(tag=tag,\n        scalar_value=value, global_step=step)\n\ndef tensorboard_add_image(tensorboard_writer, tag, pil_image, step):\n    # Convert a pil image to a torch tensor\n    img_tensor = torch.as_tensor(np.array(pil_image, copy=True))\n    img_tensor = img_tensor.view(pil_image.size[1], pil_image.size[0],\n        len(pil_image.getbands()))\n    img_tensor = img_tensor.permute((2, 0, 1))\n\n    tensorboard_writer.add_image(tag, img_tensor, global_step=step)\n\ndef validate_train_inputs(model_name, learn_rate, batch_size, gradient_step, data_root, template_file, template_filename, steps, save_model_every, create_image_every, log_directory, name=\"embedding\"):\n    assert model_name, f\"{name} not selected\"\n    assert learn_rate, \"Learning rate is empty or 0\"\n    assert isinstance(batch_size, int), \"Batch size must be integer\"\n    assert batch_size > 0, \"Batch size must be positive\"\n    assert isinstance(gradient_step, int), \"Gradient accumulation step must be integer\"\n    assert gradient_step > 0, \"Gradient accumulation step must be positive\"\n    assert data_root, \"Dataset directory is empty\"\n    assert os.path.isdir(data_root), \"Dataset directory doesn't exist\"\n    assert os.listdir(data_root), \"Dataset directory is empty\"\n    assert template_filename, \"Prompt template file not selected\"\n    assert template_file, f\"Prompt template file {template_filename} not found\"\n    assert os.path.isfile(template_file.path), f\"Prompt template file {template_filename} doesn't exist\"\n    assert steps, \"Max steps is empty or 0\"\n    assert isinstance(steps, int), \"Max steps must be integer\"\n    assert steps > 0, \"Max steps must be positive\"\n    assert isinstance(save_model_every, int), \"Save {name} must be integer\"\n    assert save_model_every >= 0, \"Save {name} must be positive or 0\"\n    assert isinstance(create_image_every, int), \"Create image must be integer\"\n    assert create_image_every >= 0, \"Create image must be positive or 0\"\n    if save_model_every or create_image_every:\n        assert log_directory, \"Log directory is empty\"\n\n\ndef train_embedding(id_task, embedding_name, learn_rate, batch_size, gradient_step, data_root, log_directory, training_width, training_height, varsize, steps, clip_grad_mode, clip_grad_value, shuffle_tags, tag_drop_out, latent_sampling_method, use_weight, create_image_every, save_embedding_every, template_filename, save_image_with_stored_embedding, preview_from_txt2img, preview_prompt, preview_negative_prompt, preview_steps, preview_sampler_name, preview_cfg_scale, preview_seed, preview_width, preview_height):\n    from modules import processing\n\n    save_embedding_every = save_embedding_every or 0\n    create_image_every = create_image_every or 0\n    template_file = textual_inversion_templates.get(template_filename, None)\n    validate_train_inputs(embedding_name, learn_rate, batch_size, gradient_step, data_root, template_file, template_filename, steps, save_embedding_every, create_image_every, log_directory, name=\"embedding\")\n    template_file = template_file.path\n\n    shared.state.job = \"train-embedding\"\n    shared.state.textinfo = \"Initializing textual inversion training...\"\n    shared.state.job_count = steps\n\n    filename = os.path.join(shared.cmd_opts.embeddings_dir, f'{embedding_name}.pt')\n\n    log_directory = os.path.join(log_directory, datetime.datetime.now().strftime(\"%Y-%m-%d\"), embedding_name)\n    unload = shared.opts.unload_models_when_training\n\n    if save_embedding_every > 0:\n        embedding_dir = os.path.join(log_directory, \"embeddings\")\n        os.makedirs(embedding_dir, exist_ok=True)\n    else:\n        embedding_dir = None\n\n    if create_image_every > 0:\n        images_dir = os.path.join(log_directory, \"images\")\n        os.makedirs(images_dir, exist_ok=True)\n    else:\n        images_dir = None\n\n    if create_image_every > 0 and save_image_with_stored_embedding:\n        images_embeds_dir = os.path.join(log_directory, \"image_embeddings\")\n        os.makedirs(images_embeds_dir, exist_ok=True)\n    else:\n        images_embeds_dir = None\n\n    hijack = sd_hijack.model_hijack\n\n    embedding = hijack.embedding_db.word_embeddings[embedding_name]\n    checkpoint = sd_models.select_checkpoint()\n\n    initial_step = embedding.step or 0\n    if initial_step >= steps:\n        shared.state.textinfo = \"Model has already been trained beyond specified max steps\"\n        return embedding, filename\n\n    scheduler = LearnRateScheduler(learn_rate, steps, initial_step)\n    clip_grad = torch.nn.utils.clip_grad_value_ if clip_grad_mode == \"value\" else \\\n        torch.nn.utils.clip_grad_norm_ if clip_grad_mode == \"norm\" else \\\n        None\n    if clip_grad:\n        clip_grad_sched = LearnRateScheduler(clip_grad_value, steps, initial_step, verbose=False)\n    # dataset loading may take a while, so input validations and early returns should be done before this\n    shared.state.textinfo = f\"Preparing dataset from {html.escape(data_root)}...\"\n    old_parallel_processing_allowed = shared.parallel_processing_allowed\n\n    tensorboard_writer = None\n    if shared.opts.training_enable_tensorboard:\n        try:\n            tensorboard_writer = tensorboard_setup(log_directory)\n        except ImportError:\n            errors.report(\"Error initializing tensorboard\", exc_info=True)\n\n    pin_memory = shared.opts.pin_memory\n\n    ds = modules.textual_inversion.dataset.PersonalizedBase(data_root=data_root, width=training_width, height=training_height, repeats=shared.opts.training_image_repeats_per_epoch, placeholder_token=embedding_name, model=shared.sd_model, cond_model=shared.sd_model.cond_stage_model, device=devices.device, template_file=template_file, batch_size=batch_size, gradient_step=gradient_step, shuffle_tags=shuffle_tags, tag_drop_out=tag_drop_out, latent_sampling_method=latent_sampling_method, varsize=varsize, use_weight=use_weight)\n\n    if shared.opts.save_training_settings_to_txt:\n        save_settings_to_file(log_directory, {**dict(model_name=checkpoint.model_name, model_hash=checkpoint.shorthash, num_of_dataset_images=len(ds), num_vectors_per_token=len(embedding.vec)), **locals()})\n\n    latent_sampling_method = ds.latent_sampling_method\n\n    dl = modules.textual_inversion.dataset.PersonalizedDataLoader(ds, latent_sampling_method=latent_sampling_method, batch_size=ds.batch_size, pin_memory=pin_memory)\n\n    if unload:\n        shared.parallel_processing_allowed = False\n        shared.sd_model.first_stage_model.to(devices.cpu)\n\n    embedding.vec.requires_grad = True\n    optimizer = torch.optim.AdamW([embedding.vec], lr=scheduler.learn_rate, weight_decay=0.0)\n    if shared.opts.save_optimizer_state:\n        optimizer_state_dict = None\n        if os.path.exists(f\"{filename}.optim\"):\n            optimizer_saved_dict = torch.load(f\"{filename}.optim\", map_location='cpu')\n            if embedding.checksum() == optimizer_saved_dict.get('hash', None):\n                optimizer_state_dict = optimizer_saved_dict.get('optimizer_state_dict', None)\n\n        if optimizer_state_dict is not None:\n            optimizer.load_state_dict(optimizer_state_dict)\n            print(\"Loaded existing optimizer from checkpoint\")\n        else:\n            print(\"No saved optimizer exists in checkpoint\")\n\n    scaler = torch.cuda.amp.GradScaler()\n\n    batch_size = ds.batch_size\n    gradient_step = ds.gradient_step\n    # n steps = batch_size * gradient_step * n image processed\n    steps_per_epoch = len(ds) // batch_size // gradient_step\n    max_steps_per_epoch = len(ds) // batch_size - (len(ds) // batch_size) % gradient_step\n    loss_step = 0\n    _loss_step = 0 #internal\n\n    last_saved_file = \"<none>\"\n    last_saved_image = \"<none>\"\n    forced_filename = \"<none>\"\n    embedding_yet_to_be_embedded = False\n\n    is_training_inpainting_model = shared.sd_model.model.conditioning_key in {'hybrid', 'concat'}\n    img_c = None\n\n    pbar = tqdm.tqdm(total=steps - initial_step)\n    try:\n        sd_hijack_checkpoint.add()\n\n        for _ in range((steps-initial_step) * gradient_step):\n            if scheduler.finished:\n                break\n            if shared.state.interrupted:\n                break\n            for j, batch in enumerate(dl):\n                # works as a drop_last=True for gradient accumulation\n                if j == max_steps_per_epoch:\n                    break\n                scheduler.apply(optimizer, embedding.step)\n                if scheduler.finished:\n                    break\n                if shared.state.interrupted:\n                    break\n\n                if clip_grad:\n                    clip_grad_sched.step(embedding.step)\n\n                with devices.autocast():\n                    x = batch.latent_sample.to(devices.device, non_blocking=pin_memory)\n                    if use_weight:\n                        w = batch.weight.to(devices.device, non_blocking=pin_memory)\n                    c = shared.sd_model.cond_stage_model(batch.cond_text)\n\n                    if is_training_inpainting_model:\n                        if img_c is None:\n                            img_c = processing.txt2img_image_conditioning(shared.sd_model, c, training_width, training_height)\n\n                        cond = {\"c_concat\": [img_c], \"c_crossattn\": [c]}\n                    else:\n                        cond = c\n\n                    if use_weight:\n                        loss = shared.sd_model.weighted_forward(x, cond, w)[0] / gradient_step\n                        del w\n                    else:\n                        loss = shared.sd_model.forward(x, cond)[0] / gradient_step\n                    del x\n\n                    _loss_step += loss.item()\n                scaler.scale(loss).backward()\n\n                # go back until we reach gradient accumulation steps\n                if (j + 1) % gradient_step != 0:\n                    continue\n\n                if clip_grad:\n                    clip_grad(embedding.vec, clip_grad_sched.learn_rate)\n\n                scaler.step(optimizer)\n                scaler.update()\n                embedding.step += 1\n                pbar.update()\n                optimizer.zero_grad(set_to_none=True)\n                loss_step = _loss_step\n                _loss_step = 0\n\n                steps_done = embedding.step + 1\n\n                epoch_num = embedding.step // steps_per_epoch\n                epoch_step = embedding.step % steps_per_epoch\n\n                description = f\"Training textual inversion [Epoch {epoch_num}: {epoch_step+1}/{steps_per_epoch}] loss: {loss_step:.7f}\"\n                pbar.set_description(description)\n                if embedding_dir is not None and steps_done % save_embedding_every == 0:\n                    # Before saving, change name to match current checkpoint.\n                    embedding_name_every = f'{embedding_name}-{steps_done}'\n                    last_saved_file = os.path.join(embedding_dir, f'{embedding_name_every}.pt')\n                    save_embedding(embedding, optimizer, checkpoint, embedding_name_every, last_saved_file, remove_cached_checksum=True)\n                    embedding_yet_to_be_embedded = True\n\n                write_loss(log_directory, \"textual_inversion_loss.csv\", embedding.step, steps_per_epoch, {\n                    \"loss\": f\"{loss_step:.7f}\",\n                    \"learn_rate\": scheduler.learn_rate\n                })\n\n                if images_dir is not None and steps_done % create_image_every == 0:\n                    forced_filename = f'{embedding_name}-{steps_done}'\n                    last_saved_image = os.path.join(images_dir, forced_filename)\n\n                    shared.sd_model.first_stage_model.to(devices.device)\n\n                    p = processing.StableDiffusionProcessingTxt2Img(\n                        sd_model=shared.sd_model,\n                        do_not_save_grid=True,\n                        do_not_save_samples=True,\n                        do_not_reload_embeddings=True,\n                    )\n\n                    if preview_from_txt2img:\n                        p.prompt = preview_prompt\n                        p.negative_prompt = preview_negative_prompt\n                        p.steps = preview_steps\n                        p.sampler_name = sd_samplers.samplers_map[preview_sampler_name.lower()]\n                        p.cfg_scale = preview_cfg_scale\n                        p.seed = preview_seed\n                        p.width = preview_width\n                        p.height = preview_height\n                    else:\n                        p.prompt = batch.cond_text[0]\n                        p.steps = 20\n                        p.width = training_width\n                        p.height = training_height\n\n                    preview_text = p.prompt\n\n                    with closing(p):\n                        processed = processing.process_images(p)\n                        image = processed.images[0] if len(processed.images) > 0 else None\n\n                    if unload:\n                        shared.sd_model.first_stage_model.to(devices.cpu)\n\n                    if image is not None:\n                        shared.state.assign_current_image(image)\n\n                        last_saved_image, last_text_info = images.save_image(image, images_dir, \"\", p.seed, p.prompt, shared.opts.samples_format, processed.infotexts[0], p=p, forced_filename=forced_filename, save_to_dirs=False)\n                        last_saved_image += f\", prompt: {preview_text}\"\n\n                        if tensorboard_writer and shared.opts.training_tensorboard_save_images:\n                            tensorboard_add_image(tensorboard_writer, f\"Validation at epoch {epoch_num}\", image, embedding.step)\n\n                    if save_image_with_stored_embedding and os.path.exists(last_saved_file) and embedding_yet_to_be_embedded:\n\n                        last_saved_image_chunks = os.path.join(images_embeds_dir, f'{embedding_name}-{steps_done}.png')\n\n                        info = PngImagePlugin.PngInfo()\n                        data = torch.load(last_saved_file)\n                        info.add_text(\"sd-ti-embedding\", embedding_to_b64(data))\n\n                        title = f\"<{data.get('name', '???')}>\"\n\n                        try:\n                            vectorSize = list(data['string_to_param'].values())[0].shape[0]\n                        except Exception:\n                            vectorSize = '?'\n\n                        checkpoint = sd_models.select_checkpoint()\n                        footer_left = checkpoint.model_name\n                        footer_mid = f'[{checkpoint.shorthash}]'\n                        footer_right = f'{vectorSize}v {steps_done}s'\n\n                        captioned_image = caption_image_overlay(image, title, footer_left, footer_mid, footer_right)\n                        captioned_image = insert_image_data_embed(captioned_image, data)\n\n                        captioned_image.save(last_saved_image_chunks, \"PNG\", pnginfo=info)\n                        embedding_yet_to_be_embedded = False\n\n                    last_saved_image, last_text_info = images.save_image(image, images_dir, \"\", p.seed, p.prompt, shared.opts.samples_format, processed.infotexts[0], p=p, forced_filename=forced_filename, save_to_dirs=False)\n                    last_saved_image += f\", prompt: {preview_text}\"\n\n                shared.state.job_no = embedding.step\n\n                shared.state.textinfo = f\"\"\"\n<p>\nLoss: {loss_step:.7f}<br/>\nStep: {steps_done}<br/>\nLast prompt: {html.escape(batch.cond_text[0])}<br/>\nLast saved embedding: {html.escape(last_saved_file)}<br/>\nLast saved image: {html.escape(last_saved_image)}<br/>\n</p>\n\"\"\"\n        filename = os.path.join(shared.cmd_opts.embeddings_dir, f'{embedding_name}.pt')\n        save_embedding(embedding, optimizer, checkpoint, embedding_name, filename, remove_cached_checksum=True)\n    except Exception:\n        errors.report(\"Error training embedding\", exc_info=True)\n    finally:\n        pbar.leave = False\n        pbar.close()\n        shared.sd_model.first_stage_model.to(devices.device)\n        shared.parallel_processing_allowed = old_parallel_processing_allowed\n        sd_hijack_checkpoint.remove()\n\n    return embedding, filename\n\n\ndef save_embedding(embedding, optimizer, checkpoint, embedding_name, filename, remove_cached_checksum=True):\n    old_embedding_name = embedding.name\n    old_sd_checkpoint = embedding.sd_checkpoint if hasattr(embedding, \"sd_checkpoint\") else None\n    old_sd_checkpoint_name = embedding.sd_checkpoint_name if hasattr(embedding, \"sd_checkpoint_name\") else None\n    old_cached_checksum = embedding.cached_checksum if hasattr(embedding, \"cached_checksum\") else None\n    try:\n        embedding.sd_checkpoint = checkpoint.shorthash\n        embedding.sd_checkpoint_name = checkpoint.model_name\n        if remove_cached_checksum:\n            embedding.cached_checksum = None\n        embedding.name = embedding_name\n        embedding.optimizer_state_dict = optimizer.state_dict()\n        embedding.save(filename)\n    except:\n        embedding.sd_checkpoint = old_sd_checkpoint\n        embedding.sd_checkpoint_name = old_sd_checkpoint_name\n        embedding.name = old_embedding_name\n        embedding.cached_checksum = old_cached_checksum\n        raise\n", "modules/textual_inversion/ui.py": "import html\n\nimport gradio as gr\n\nimport modules.textual_inversion.textual_inversion\nfrom modules import sd_hijack, shared\n\n\ndef create_embedding(name, initialization_text, nvpt, overwrite_old):\n    filename = modules.textual_inversion.textual_inversion.create_embedding(name, nvpt, overwrite_old, init_text=initialization_text)\n\n    sd_hijack.model_hijack.embedding_db.load_textual_inversion_embeddings()\n\n    return gr.Dropdown.update(choices=sorted(sd_hijack.model_hijack.embedding_db.word_embeddings.keys())), f\"Created: {filename}\", \"\"\n\n\ndef train_embedding(*args):\n\n    assert not shared.cmd_opts.lowvram, 'Training models with lowvram not possible'\n\n    apply_optimizations = shared.opts.training_xattention_optimizations\n    try:\n        if not apply_optimizations:\n            sd_hijack.undo_optimizations()\n\n        embedding, filename = modules.textual_inversion.textual_inversion.train_embedding(*args)\n\n        res = f\"\"\"\nTraining {'interrupted' if shared.state.interrupted else 'finished'} at {embedding.step} steps.\nEmbedding saved to {html.escape(filename)}\n\"\"\"\n        return res, \"\"\n    except Exception:\n        raise\n    finally:\n        if not apply_optimizations:\n            sd_hijack.apply_optimizations()\n\n", "modules/textual_inversion/learn_schedule.py": "import tqdm\n\n\nclass LearnScheduleIterator:\n    def __init__(self, learn_rate, max_steps, cur_step=0):\n        \"\"\"\n        specify learn_rate as \"0.001:100, 0.00001:1000, 1e-5:10000\" to have lr of 0.001 until step 100, 0.00001 until 1000, and 1e-5 until 10000\n        \"\"\"\n\n        pairs = learn_rate.split(',')\n        self.rates = []\n        self.it = 0\n        self.maxit = 0\n        try:\n            for pair in pairs:\n                if not pair.strip():\n                    continue\n                tmp = pair.split(':')\n                if len(tmp) == 2:\n                    step = int(tmp[1])\n                    if step > cur_step:\n                        self.rates.append((float(tmp[0]), min(step, max_steps)))\n                        self.maxit += 1\n                        if step > max_steps:\n                            return\n                    elif step == -1:\n                        self.rates.append((float(tmp[0]), max_steps))\n                        self.maxit += 1\n                        return\n                else:\n                    self.rates.append((float(tmp[0]), max_steps))\n                    self.maxit += 1\n                    return\n            assert self.rates\n        except (ValueError, AssertionError) as e:\n            raise Exception('Invalid learning rate schedule. It should be a number or, for example, like \"0.001:100, 0.00001:1000, 1e-5:10000\" to have lr of 0.001 until step 100, 0.00001 until 1000, and 1e-5 until 10000.') from e\n\n\n    def __iter__(self):\n        return self\n\n    def __next__(self):\n        if self.it < self.maxit:\n            self.it += 1\n            return self.rates[self.it - 1]\n        else:\n            raise StopIteration\n\n\nclass LearnRateScheduler:\n    def __init__(self, learn_rate, max_steps, cur_step=0, verbose=True):\n        self.schedules = LearnScheduleIterator(learn_rate, max_steps, cur_step)\n        (self.learn_rate,  self.end_step) = next(self.schedules)\n        self.verbose = verbose\n\n        if self.verbose:\n            print(f'Training at rate of {self.learn_rate} until step {self.end_step}')\n\n        self.finished = False\n\n    def step(self, step_number):\n        if step_number < self.end_step:\n            return False\n\n        try:\n            (self.learn_rate, self.end_step) = next(self.schedules)\n        except StopIteration:\n            self.finished = True\n            return False\n        return True\n\n    def apply(self, optimizer, step_number):\n        if not self.step(step_number):\n            return\n\n        if self.verbose:\n            tqdm.tqdm.write(f'Training at rate of {self.learn_rate} until step {self.end_step}')\n\n        for pg in optimizer.param_groups:\n            pg['lr'] = self.learn_rate\n\n", "modules/textual_inversion/saving_settings.py": "import datetime\nimport json\nimport os\n\nsaved_params_shared = {\n    \"batch_size\",\n    \"clip_grad_mode\",\n    \"clip_grad_value\",\n    \"create_image_every\",\n    \"data_root\",\n    \"gradient_step\",\n    \"initial_step\",\n    \"latent_sampling_method\",\n    \"learn_rate\",\n    \"log_directory\",\n    \"model_hash\",\n    \"model_name\",\n    \"num_of_dataset_images\",\n    \"steps\",\n    \"template_file\",\n    \"training_height\",\n    \"training_width\",\n}\nsaved_params_ti = {\n    \"embedding_name\",\n    \"num_vectors_per_token\",\n    \"save_embedding_every\",\n    \"save_image_with_stored_embedding\",\n}\nsaved_params_hypernet = {\n    \"activation_func\",\n    \"add_layer_norm\",\n    \"hypernetwork_name\",\n    \"layer_structure\",\n    \"save_hypernetwork_every\",\n    \"use_dropout\",\n    \"weight_init\",\n}\nsaved_params_all = saved_params_shared | saved_params_ti | saved_params_hypernet\nsaved_params_previews = {\n    \"preview_cfg_scale\",\n    \"preview_height\",\n    \"preview_negative_prompt\",\n    \"preview_prompt\",\n    \"preview_sampler_index\",\n    \"preview_seed\",\n    \"preview_steps\",\n    \"preview_width\",\n}\n\n\ndef save_settings_to_file(log_directory, all_params):\n    now = datetime.datetime.now()\n    params = {\"datetime\": now.strftime(\"%Y-%m-%d %H:%M:%S\")}\n\n    keys = saved_params_all\n    if all_params.get('preview_from_txt2img'):\n        keys = keys | saved_params_previews\n\n    params.update({k: v for k, v in all_params.items() if k in keys})\n\n    filename = f'settings-{now.strftime(\"%Y-%m-%d-%H-%M-%S\")}.json'\n    with open(os.path.join(log_directory, filename), \"w\") as file:\n        json.dump(params, file, indent=4)\n", "modules/textual_inversion/image_embedding.py": "import base64\nimport json\nimport os.path\nimport warnings\nimport logging\n\nimport numpy as np\nimport zlib\nfrom PIL import Image, ImageDraw\nimport torch\n\nlogger = logging.getLogger(__name__)\n\n\nclass EmbeddingEncoder(json.JSONEncoder):\n    def default(self, obj):\n        if isinstance(obj, torch.Tensor):\n            return {'TORCHTENSOR': obj.cpu().detach().numpy().tolist()}\n        return json.JSONEncoder.default(self, obj)\n\n\nclass EmbeddingDecoder(json.JSONDecoder):\n    def __init__(self, *args, **kwargs):\n        json.JSONDecoder.__init__(self, *args, object_hook=self.object_hook, **kwargs)\n\n    def object_hook(self, d):\n        if 'TORCHTENSOR' in d:\n            return torch.from_numpy(np.array(d['TORCHTENSOR']))\n        return d\n\n\ndef embedding_to_b64(data):\n    d = json.dumps(data, cls=EmbeddingEncoder)\n    return base64.b64encode(d.encode())\n\n\ndef embedding_from_b64(data):\n    d = base64.b64decode(data)\n    return json.loads(d, cls=EmbeddingDecoder)\n\n\ndef lcg(m=2**32, a=1664525, c=1013904223, seed=0):\n    while True:\n        seed = (a * seed + c) % m\n        yield seed % 255\n\n\ndef xor_block(block):\n    g = lcg()\n    randblock = np.array([next(g) for _ in range(np.prod(block.shape))]).astype(np.uint8).reshape(block.shape)\n    return np.bitwise_xor(block.astype(np.uint8), randblock & 0x0F)\n\n\ndef style_block(block, sequence):\n    im = Image.new('RGB', (block.shape[1], block.shape[0]))\n    draw = ImageDraw.Draw(im)\n    i = 0\n    for x in range(-6, im.size[0], 8):\n        for yi, y in enumerate(range(-6, im.size[1], 8)):\n            offset = 0\n            if yi % 2 == 0:\n                offset = 4\n            shade = sequence[i % len(sequence)]\n            i += 1\n            draw.ellipse((x+offset, y, x+6+offset, y+6), fill=(shade, shade, shade))\n\n    fg = np.array(im).astype(np.uint8) & 0xF0\n\n    return block ^ fg\n\n\ndef insert_image_data_embed(image, data):\n    d = 3\n    data_compressed = zlib.compress(json.dumps(data, cls=EmbeddingEncoder).encode(), level=9)\n    data_np_ = np.frombuffer(data_compressed, np.uint8).copy()\n    data_np_high = data_np_ >> 4\n    data_np_low = data_np_ & 0x0F\n\n    h = image.size[1]\n    next_size = data_np_low.shape[0] + (h-(data_np_low.shape[0] % h))\n    next_size = next_size + ((h*d)-(next_size % (h*d)))\n\n    data_np_low = np.resize(data_np_low, next_size)\n    data_np_low = data_np_low.reshape((h, -1, d))\n\n    data_np_high = np.resize(data_np_high, next_size)\n    data_np_high = data_np_high.reshape((h, -1, d))\n\n    edge_style = list(data['string_to_param'].values())[0].cpu().detach().numpy().tolist()[0][:1024]\n    edge_style = (np.abs(edge_style)/np.max(np.abs(edge_style))*255).astype(np.uint8)\n\n    data_np_low = style_block(data_np_low, sequence=edge_style)\n    data_np_low = xor_block(data_np_low)\n    data_np_high = style_block(data_np_high, sequence=edge_style[::-1])\n    data_np_high = xor_block(data_np_high)\n\n    im_low = Image.fromarray(data_np_low, mode='RGB')\n    im_high = Image.fromarray(data_np_high, mode='RGB')\n\n    background = Image.new('RGB', (image.size[0]+im_low.size[0]+im_high.size[0]+2, image.size[1]), (0, 0, 0))\n    background.paste(im_low, (0, 0))\n    background.paste(image, (im_low.size[0]+1, 0))\n    background.paste(im_high, (im_low.size[0]+1+image.size[0]+1, 0))\n\n    return background\n\n\ndef crop_black(img, tol=0):\n    mask = (img > tol).all(2)\n    mask0, mask1 = mask.any(0), mask.any(1)\n    col_start, col_end = mask0.argmax(), mask.shape[1]-mask0[::-1].argmax()\n    row_start, row_end = mask1.argmax(), mask.shape[0]-mask1[::-1].argmax()\n    return img[row_start:row_end, col_start:col_end]\n\n\ndef extract_image_data_embed(image):\n    d = 3\n    outarr = crop_black(np.array(image.convert('RGB').getdata()).reshape(image.size[1], image.size[0], d).astype(np.uint8)) & 0x0F\n    black_cols = np.where(np.sum(outarr, axis=(0, 2)) == 0)\n    if black_cols[0].shape[0] < 2:\n        logger.debug(f'{os.path.basename(getattr(image, \"filename\", \"unknown image file\"))}: no embedded information found.')\n        return None\n\n    data_block_lower = outarr[:, :black_cols[0].min(), :].astype(np.uint8)\n    data_block_upper = outarr[:, black_cols[0].max()+1:, :].astype(np.uint8)\n\n    data_block_lower = xor_block(data_block_lower)\n    data_block_upper = xor_block(data_block_upper)\n\n    data_block = (data_block_upper << 4) | (data_block_lower)\n    data_block = data_block.flatten().tobytes()\n\n    data = zlib.decompress(data_block)\n    return json.loads(data, cls=EmbeddingDecoder)\n\n\ndef caption_image_overlay(srcimage, title, footerLeft, footerMid, footerRight, textfont=None):\n    from modules.images import get_font\n    if textfont:\n        warnings.warn(\n            'passing in a textfont to caption_image_overlay is deprecated and does nothing',\n            DeprecationWarning,\n            stacklevel=2,\n        )\n    from math import cos\n\n    image = srcimage.copy()\n    fontsize = 32\n    factor = 1.5\n    gradient = Image.new('RGBA', (1, image.size[1]), color=(0, 0, 0, 0))\n    for y in range(image.size[1]):\n        mag = 1-cos(y/image.size[1]*factor)\n        mag = max(mag, 1-cos((image.size[1]-y)/image.size[1]*factor*1.1))\n        gradient.putpixel((0, y), (0, 0, 0, int(mag*255)))\n    image = Image.alpha_composite(image.convert('RGBA'), gradient.resize(image.size))\n\n    draw = ImageDraw.Draw(image)\n\n    font = get_font(fontsize)\n    padding = 10\n\n    _, _, w, h = draw.textbbox((0, 0), title, font=font)\n    fontsize = min(int(fontsize * (((image.size[0]*0.75)-(padding*4))/w)), 72)\n    font = get_font(fontsize)\n    _, _, w, h = draw.textbbox((0, 0), title, font=font)\n    draw.text((padding, padding), title, anchor='lt', font=font, fill=(255, 255, 255, 230))\n\n    _, _, w, h = draw.textbbox((0, 0), footerLeft, font=font)\n    fontsize_left = min(int(fontsize * (((image.size[0]/3)-(padding))/w)), 72)\n    _, _, w, h = draw.textbbox((0, 0), footerMid, font=font)\n    fontsize_mid = min(int(fontsize * (((image.size[0]/3)-(padding))/w)), 72)\n    _, _, w, h = draw.textbbox((0, 0), footerRight, font=font)\n    fontsize_right = min(int(fontsize * (((image.size[0]/3)-(padding))/w)), 72)\n\n    font = get_font(min(fontsize_left, fontsize_mid, fontsize_right))\n\n    draw.text((padding, image.size[1]-padding),               footerLeft, anchor='ls', font=font, fill=(255, 255, 255, 230))\n    draw.text((image.size[0]/2, image.size[1]-padding),       footerMid, anchor='ms', font=font, fill=(255, 255, 255, 230))\n    draw.text((image.size[0]-padding, image.size[1]-padding), footerRight, anchor='rs', font=font, fill=(255, 255, 255, 230))\n\n    return image\n\n\nif __name__ == '__main__':\n\n    testEmbed = Image.open('test_embedding.png')\n    data = extract_image_data_embed(testEmbed)\n    assert data is not None\n\n    data = embedding_from_b64(testEmbed.text['sd-ti-embedding'])\n    assert data is not None\n\n    image = Image.new('RGBA', (512, 512), (255, 255, 200, 255))\n    cap_image = caption_image_overlay(image, 'title', 'footerLeft', 'footerMid', 'footerRight')\n\n    test_embed = {'string_to_param': {'*': torch.from_numpy(np.random.random((2, 4096)))}}\n\n    embedded_image = insert_image_data_embed(cap_image, test_embed)\n\n    retrieved_embed = extract_image_data_embed(embedded_image)\n\n    assert str(retrieved_embed) == str(test_embed)\n\n    embedded_image2 = insert_image_data_embed(cap_image, retrieved_embed)\n\n    assert embedded_image == embedded_image2\n\n    g = lcg()\n    shared_random = np.array([next(g) for _ in range(100)]).astype(np.uint8).tolist()\n\n    reference_random = [253, 242, 127,  44, 157,  27, 239, 133,  38,  79, 167,   4, 177,\n                         95, 130,  79,  78,  14,  52, 215, 220, 194, 126,  28, 240, 179,\n                        160, 153, 149,  50, 105,  14,  21, 218, 199,  18,  54, 198, 193,\n                         38, 128,  19,  53, 195, 124,  75, 205,  12,   6, 145,   0,  28,\n                         30, 148,   8,  45, 218, 171,  55, 249,  97, 166,  12,  35,   0,\n                         41, 221, 122, 215, 170,  31, 113, 186,  97, 119,  31,  23, 185,\n                         66, 140,  30,  41,  37,  63, 137, 109, 216,  55, 159, 145,  82,\n                         204, 86,  73, 222,  44, 198, 118, 240,  97]\n\n    assert shared_random == reference_random\n\n    hunna_kay_random_sum = sum(np.array([next(g) for _ in range(100000)]).astype(np.uint8).tolist())\n\n    assert 12731374 == hunna_kay_random_sum\n"}