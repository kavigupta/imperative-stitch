{"setup.py": "#!/usr/bin/env python\n\nimport io\n\nfrom setuptools import setup, find_packages\n\n\nsetup(\n    name='jmespath',\n    version='1.0.1',\n    description='JSON Matching Expressions',\n    long_description=io.open('README.rst', encoding='utf-8').read(),\n    author='James Saryerwinnie',\n    author_email='js@jamesls.com',\n    url='https://github.com/jmespath/jmespath.py',\n    scripts=['bin/jp.py'],\n    packages=find_packages(exclude=['tests']),\n    license='MIT',\n    python_requires='>=3.7',\n    classifiers=[\n        'Development Status :: 5 - Production/Stable',\n        'Intended Audience :: Developers',\n        'Natural Language :: English',\n        'License :: OSI Approved :: MIT License',\n        'Programming Language :: Python',\n        'Programming Language :: Python :: 3',\n        'Programming Language :: Python :: 3.7',\n        'Programming Language :: Python :: 3.8',\n        'Programming Language :: Python :: 3.9',\n        'Programming Language :: Python :: 3.10',\n        'Programming Language :: Python :: 3.11',\n        'Programming Language :: Python :: Implementation :: CPython',\n        'Programming Language :: Python :: Implementation :: PyPy',\n    ],\n)\n", "extra/test_hypothesis.py": "# Test suite using hypothesis to generate test cases.\n# This is in a standalone module so that these tests\n# can a) be run separately and b) allow for customization\n# via env var for longer runs in travis.\nimport os\nimport numbers\n\nfrom hypothesis import given, settings, assume, HealthCheck\nimport hypothesis.strategies as st\n\nfrom jmespath import lexer\nfrom jmespath import parser\nfrom jmespath import exceptions\nfrom jmespath.functions import Functions\n\n\nJSON_NUMBERS = (st.integers() | st.floats(allow_nan=False,\n                                          allow_infinity=False))\n\nRANDOM_JSON = st.recursive(\n    JSON_NUMBERS | st.booleans() | st.text() | st.none(),\n    lambda children: st.lists(children) | st.dictionaries(st.text(), children)\n)\n\nMAX_EXAMPLES = int(os.environ.get('JP_MAX_EXAMPLES', 1000))\nBASE_SETTINGS = {\n    'max_examples': MAX_EXAMPLES,\n    'suppress_health_check': [HealthCheck.too_slow,\n                              HealthCheck.filter_too_much,\n                              HealthCheck.data_too_large],\n}\n\n\n# For all of these tests they verify these properties:\n# either the operation succeeds or it raises a JMESPathError.\n# If any other exception is raised then we error out.\n@settings(**BASE_SETTINGS)\n@given(st.text())\ndef test_lexer_api(expr):\n    try:\n        tokens = list(lexer.Lexer().tokenize(expr))\n    except exceptions.EmptyExpressionError:\n        return\n    except exceptions.LexerError as e:\n        assert e.lex_position >= 0, e.lex_position\n        assert e.lex_position < len(expr), e.lex_position\n        if expr:\n            assert expr[e.lex_position] == e.token_value[0], (\n                \"Lex position does not match first token char.\\n\"\n                \"Expression: %s\\n%s != %s\" % (expr, expr[e.lex_position],\n                                              e.token_value[0])\n            )\n        return\n    except Exception as e:\n        raise AssertionError(\"Non JMESPathError raised: %s\" % e)\n    assert isinstance(tokens, list)\n    # Token starting positions must be unique, can't have two\n    # tokens with the same start position.\n    start_locations = [t['start'] for t in tokens]\n    assert len(set(start_locations)) == len(start_locations), (\n        \"Tokens must have unique starting locations.\")\n    # Starting positions must be increasing (i.e sorted).\n    assert sorted(start_locations) == start_locations, (\n        \"Tokens must have increasing start locations.\")\n    # Last token is always EOF.\n    assert tokens[-1]['type'] == 'eof'\n\n\n@settings(**BASE_SETTINGS)\n@given(st.text())\ndef test_parser_api_from_str(expr):\n    # Same a lexer above with the assumption that we're parsing\n    # a valid sequence of tokens.\n    try:\n        list(lexer.Lexer().tokenize(expr))\n    except exceptions.JMESPathError as e:\n        # We want to try to parse things that tokenize\n        # properly.\n        assume(False)\n    try:\n        ast = parser.Parser().parse(expr)\n    except exceptions.JMESPathError as e:\n        return\n    except Exception as e:\n        raise AssertionError(\"Non JMESPathError raised: %s\" % e)\n    assert isinstance(ast.parsed, dict)\n    assert 'type' in ast.parsed\n    assert 'children' in ast.parsed\n    assert isinstance(ast.parsed['children'], list)\n\n\n@settings(**BASE_SETTINGS)\n@given(expr=st.text(), data=RANDOM_JSON)\ndef test_search_api(expr, data):\n    try:\n        ast = parser.Parser().parse(expr)\n    except exceptions.JMESPathError as e:\n        # We want to try to parse things that tokenize\n        # properly.\n        assume(False)\n    try:\n        ast.search(data)\n    except exceptions.JMESPathError as e:\n        return\n    except Exception as e:\n        raise AssertionError(\"Non JMESPathError raised: %s\" % e)\n\n\n# Additional property tests for functions.\n\n@given(arg=JSON_NUMBERS)\ndef test_abs(arg):\n    assert Functions().call_function('abs', [arg]) >= 0\n\n\n@given(arg=st.lists(JSON_NUMBERS))\ndef test_avg(arg):\n    result = Functions().call_function('avg', [arg])\n    if result is not None:\n        assert isinstance(result, numbers.Number)\n\n\n@given(arg=st.lists(st.floats() | st.booleans() | st.text() | st.none(),\n                    min_size=1))\ndef test_not_null(arg):\n    result = Functions().call_function('not_null', arg)\n    if result is not None:\n        assert result in arg\n\n\n@given(arg=RANDOM_JSON)\ndef test_to_number(arg):\n    result = Functions().call_function('to_number', [arg])\n    if result is not None:\n        assert isinstance(result, numbers.Number)\n", "jmespath/lexer.py": "import string\nimport warnings\nfrom json import loads\n\nfrom jmespath.exceptions import LexerError, EmptyExpressionError\n\n\nclass Lexer(object):\n    START_IDENTIFIER = set(string.ascii_letters + '_')\n    VALID_IDENTIFIER = set(string.ascii_letters + string.digits + '_')\n    VALID_NUMBER = set(string.digits)\n    WHITESPACE = set(\" \\t\\n\\r\")\n    SIMPLE_TOKENS = {\n        '.': 'dot',\n        '*': 'star',\n        ']': 'rbracket',\n        ',': 'comma',\n        ':': 'colon',\n        '@': 'current',\n        '(': 'lparen',\n        ')': 'rparen',\n        '{': 'lbrace',\n        '}': 'rbrace',\n    }\n\n    def tokenize(self, expression):\n        self._initialize_for_expression(expression)\n        while self._current is not None:\n            if self._current in self.SIMPLE_TOKENS:\n                yield {'type': self.SIMPLE_TOKENS[self._current],\n                       'value': self._current,\n                       'start': self._position, 'end': self._position + 1}\n                self._next()\n            elif self._current in self.START_IDENTIFIER:\n                start = self._position\n                buff = self._current\n                while self._next() in self.VALID_IDENTIFIER:\n                    buff += self._current\n                yield {'type': 'unquoted_identifier', 'value': buff,\n                       'start': start, 'end': start + len(buff)}\n            elif self._current in self.WHITESPACE:\n                self._next()\n            elif self._current == '[':\n                start = self._position\n                next_char = self._next()\n                if next_char == ']':\n                    self._next()\n                    yield {'type': 'flatten', 'value': '[]',\n                           'start': start, 'end': start + 2}\n                elif next_char == '?':\n                    self._next()\n                    yield {'type': 'filter', 'value': '[?',\n                           'start': start, 'end': start + 2}\n                else:\n                    yield {'type': 'lbracket', 'value': '[',\n                           'start': start, 'end': start + 1}\n            elif self._current == \"'\":\n                yield self._consume_raw_string_literal()\n            elif self._current == '|':\n                yield self._match_or_else('|', 'or', 'pipe')\n            elif self._current == '&':\n                yield self._match_or_else('&', 'and', 'expref')\n            elif self._current == '`':\n                yield self._consume_literal()\n            elif self._current in self.VALID_NUMBER:\n                start = self._position\n                buff = self._consume_number()\n                yield {'type': 'number', 'value': int(buff),\n                       'start': start, 'end': start + len(buff)}\n            elif self._current == '-':\n                # Negative number.\n                start = self._position\n                buff = self._consume_number()\n                if len(buff) > 1:\n                    yield {'type': 'number', 'value': int(buff),\n                           'start': start, 'end': start + len(buff)}\n                else:\n                    raise LexerError(lexer_position=start,\n                                     lexer_value=buff,\n                                     message=\"Unknown token '%s'\" % buff)\n            elif self._current == '\"':\n                yield self._consume_quoted_identifier()\n            elif self._current == '<':\n                yield self._match_or_else('=', 'lte', 'lt')\n            elif self._current == '>':\n                yield self._match_or_else('=', 'gte', 'gt')\n            elif self._current == '!':\n                yield self._match_or_else('=', 'ne', 'not')\n            elif self._current == '=':\n                if self._next() == '=':\n                    yield {'type': 'eq', 'value': '==',\n                        'start': self._position - 1, 'end': self._position}\n                    self._next()\n                else:\n                    if self._current is None:\n                        # If we're at the EOF, we never advanced\n                        # the position so we don't need to rewind\n                        # it back one location.\n                        position = self._position\n                    else:\n                        position = self._position - 1\n                    raise LexerError(\n                        lexer_position=position,\n                        lexer_value='=',\n                        message=\"Unknown token '='\")\n            else:\n                raise LexerError(lexer_position=self._position,\n                                 lexer_value=self._current,\n                                 message=\"Unknown token %s\" % self._current)\n        yield {'type': 'eof', 'value': '',\n               'start': self._length, 'end': self._length}\n\n    def _consume_number(self):\n        start = self._position\n        buff = self._current\n        while self._next() in self.VALID_NUMBER:\n            buff += self._current\n        return buff\n\n    def _initialize_for_expression(self, expression):\n        if not expression:\n            raise EmptyExpressionError()\n        self._position = 0\n        self._expression = expression\n        self._chars = list(self._expression)\n        self._current = self._chars[self._position]\n        self._length = len(self._expression)\n\n    def _next(self):\n        if self._position == self._length - 1:\n            self._current = None\n        else:\n            self._position += 1\n            self._current = self._chars[self._position]\n        return self._current\n\n    def _consume_until(self, delimiter):\n        # Consume until the delimiter is reached,\n        # allowing for the delimiter to be escaped with \"\\\".\n        start = self._position\n        buff = ''\n        self._next()\n        while self._current != delimiter:\n            if self._current == '\\\\':\n                buff += '\\\\'\n                self._next()\n            if self._current is None:\n                # We're at the EOF.\n                raise LexerError(lexer_position=start,\n                                 lexer_value=self._expression[start:],\n                                 message=\"Unclosed %s delimiter\" % delimiter)\n            buff += self._current\n            self._next()\n        # Skip the closing delimiter.\n        self._next()\n        return buff\n\n    def _consume_literal(self):\n        start = self._position\n        lexeme = self._consume_until('`').replace('\\\\`', '`')\n        try:\n            # Assume it is valid JSON and attempt to parse.\n            parsed_json = loads(lexeme)\n        except ValueError:\n            try:\n                # Invalid JSON values should be converted to quoted\n                # JSON strings during the JEP-12 deprecation period.\n                parsed_json = loads('\"%s\"' % lexeme.lstrip())\n                warnings.warn(\"deprecated string literal syntax\",\n                              PendingDeprecationWarning)\n            except ValueError:\n                raise LexerError(lexer_position=start,\n                                 lexer_value=self._expression[start:],\n                                 message=\"Bad token %s\" % lexeme)\n        token_len = self._position - start\n        return {'type': 'literal', 'value': parsed_json,\n                'start': start, 'end': token_len}\n\n    def _consume_quoted_identifier(self):\n        start = self._position\n        lexeme = '\"' + self._consume_until('\"') + '\"'\n        try:\n            token_len = self._position - start\n            return {'type': 'quoted_identifier', 'value': loads(lexeme),\n                    'start': start, 'end': token_len}\n        except ValueError as e:\n            error_message = str(e).split(':')[0]\n            raise LexerError(lexer_position=start,\n                             lexer_value=lexeme,\n                             message=error_message)\n\n    def _consume_raw_string_literal(self):\n        start = self._position\n        lexeme = self._consume_until(\"'\").replace(\"\\\\'\", \"'\")\n        token_len = self._position - start\n        return {'type': 'literal', 'value': lexeme,\n                'start': start, 'end': token_len}\n\n    def _match_or_else(self, expected, match_type, else_type):\n        start = self._position\n        current = self._current\n        next_char = self._next()\n        if next_char == expected:\n            self._next()\n            return {'type': match_type, 'value': current + next_char,\n                    'start': start, 'end': start + 1}\n        return {'type': else_type, 'value': current,\n                'start': start, 'end': start}\n", "jmespath/ast.py": "# AST nodes have this structure:\n# {\"type\": <node type>\", children: [], \"value\": \"\"}\n\n\ndef comparator(name, first, second):\n    return {'type': 'comparator', 'children': [first, second], 'value': name}\n\n\ndef current_node():\n    return {'type': 'current', 'children': []}\n\n\ndef expref(expression):\n    return {'type': 'expref', 'children': [expression]}\n\n\ndef function_expression(name, args):\n    return {'type': 'function_expression', 'children': args, 'value': name}\n\n\ndef field(name):\n    return {\"type\": \"field\", \"children\": [], \"value\": name}\n\n\ndef filter_projection(left, right, comparator):\n    return {'type': 'filter_projection', 'children': [left, right, comparator]}\n\n\ndef flatten(node):\n    return {'type': 'flatten', 'children': [node]}\n\n\ndef identity():\n    return {\"type\": \"identity\", 'children': []}\n\n\ndef index(index):\n    return {\"type\": \"index\", \"value\": index, \"children\": []}\n\n\ndef index_expression(children):\n    return {\"type\": \"index_expression\", 'children': children}\n\n\ndef key_val_pair(key_name, node):\n    return {\"type\": \"key_val_pair\", 'children': [node], \"value\": key_name}\n\n\ndef literal(literal_value):\n    return {'type': 'literal', 'value': literal_value, 'children': []}\n\n\ndef multi_select_dict(nodes):\n    return {\"type\": \"multi_select_dict\", \"children\": nodes}\n\n\ndef multi_select_list(nodes):\n    return {\"type\": \"multi_select_list\", \"children\": nodes}\n\n\ndef or_expression(left, right):\n    return {\"type\": \"or_expression\", \"children\": [left, right]}\n\n\ndef and_expression(left, right):\n    return {\"type\": \"and_expression\", \"children\": [left, right]}\n\n\ndef not_expression(expr):\n    return {\"type\": \"not_expression\", \"children\": [expr]}\n\n\ndef pipe(left, right):\n    return {'type': 'pipe', 'children': [left, right]}\n\n\ndef projection(left, right):\n    return {'type': 'projection', 'children': [left, right]}\n\n\ndef subexpression(children):\n    return {\"type\": \"subexpression\", 'children': children}\n\n\ndef slice(start, end, step):\n    return {\"type\": \"slice\", \"children\": [start, end, step]}\n\n\ndef value_projection(left, right):\n    return {'type': 'value_projection', 'children': [left, right]}\n", "jmespath/exceptions.py": "from jmespath.compat import with_str_method\n\n\nclass JMESPathError(ValueError):\n    pass\n\n\n@with_str_method\nclass ParseError(JMESPathError):\n    _ERROR_MESSAGE = 'Invalid jmespath expression'\n    def __init__(self, lex_position, token_value, token_type,\n                 msg=_ERROR_MESSAGE):\n        super(ParseError, self).__init__(lex_position, token_value, token_type)\n        self.lex_position = lex_position\n        self.token_value = token_value\n        self.token_type = token_type.upper()\n        self.msg = msg\n        # Whatever catches the ParseError can fill in the full expression\n        self.expression = None\n\n    def __str__(self):\n        # self.lex_position +1 to account for the starting double quote char.\n        underline = ' ' * (self.lex_position + 1) + '^'\n        return (\n            '%s: Parse error at column %s, '\n            'token \"%s\" (%s), for expression:\\n\"%s\"\\n%s' % (\n                self.msg, self.lex_position, self.token_value, self.token_type,\n                self.expression, underline))\n\n\n@with_str_method\nclass IncompleteExpressionError(ParseError):\n    def set_expression(self, expression):\n        self.expression = expression\n        self.lex_position = len(expression)\n        self.token_type = None\n        self.token_value = None\n\n    def __str__(self):\n        # self.lex_position +1 to account for the starting double quote char.\n        underline = ' ' * (self.lex_position + 1) + '^'\n        return (\n            'Invalid jmespath expression: Incomplete expression:\\n'\n            '\"%s\"\\n%s' % (self.expression, underline))\n\n\n@with_str_method\nclass LexerError(ParseError):\n    def __init__(self, lexer_position, lexer_value, message, expression=None):\n        self.lexer_position = lexer_position\n        self.lexer_value = lexer_value\n        self.message = message\n        super(LexerError, self).__init__(lexer_position,\n                                         lexer_value,\n                                         message)\n        # Whatever catches LexerError can set this.\n        self.expression = expression\n\n    def __str__(self):\n        underline = ' ' * self.lexer_position + '^'\n        return 'Bad jmespath expression: %s:\\n%s\\n%s' % (\n            self.message, self.expression, underline)\n\n\n@with_str_method\nclass ArityError(ParseError):\n    def __init__(self, expected, actual, name):\n        self.expected_arity = expected\n        self.actual_arity = actual\n        self.function_name = name\n        self.expression = None\n\n    def __str__(self):\n        return (\"Expected %s %s for function %s(), \"\n                \"received %s\" % (\n                    self.expected_arity,\n                    self._pluralize('argument', self.expected_arity),\n                    self.function_name,\n                    self.actual_arity))\n\n    def _pluralize(self, word, count):\n        if count == 1:\n            return word\n        else:\n            return word + 's'\n\n\n@with_str_method\nclass VariadictArityError(ArityError):\n    def __str__(self):\n        return (\"Expected at least %s %s for function %s(), \"\n                \"received %s\" % (\n                    self.expected_arity,\n                    self._pluralize('argument', self.expected_arity),\n                    self.function_name,\n                    self.actual_arity))\n\n\n@with_str_method\nclass JMESPathTypeError(JMESPathError):\n    def __init__(self, function_name, current_value, actual_type,\n                 expected_types):\n        self.function_name = function_name\n        self.current_value = current_value\n        self.actual_type = actual_type\n        self.expected_types = expected_types\n\n    def __str__(self):\n        return ('In function %s(), invalid type for value: %s, '\n                'expected one of: %s, received: \"%s\"' % (\n                    self.function_name, self.current_value,\n                    self.expected_types, self.actual_type))\n\n\nclass EmptyExpressionError(JMESPathError):\n    def __init__(self):\n        super(EmptyExpressionError, self).__init__(\n            \"Invalid JMESPath expression: cannot be empty.\")\n\n\nclass UnknownFunctionError(JMESPathError):\n    pass\n", "jmespath/functions.py": "import math\nimport json\n\nfrom jmespath import exceptions\nfrom jmespath.compat import string_type as STRING_TYPE\nfrom jmespath.compat import get_methods\n\n\n# python types -> jmespath types\nTYPES_MAP = {\n    'bool': 'boolean',\n    'list': 'array',\n    'dict': 'object',\n    'NoneType': 'null',\n    'unicode': 'string',\n    'str': 'string',\n    'float': 'number',\n    'int': 'number',\n    'long': 'number',\n    'OrderedDict': 'object',\n    '_Projection': 'array',\n    '_Expression': 'expref',\n}\n\n\n# jmespath types -> python types\nREVERSE_TYPES_MAP = {\n    'boolean': ('bool',),\n    'array': ('list', '_Projection'),\n    'object': ('dict', 'OrderedDict',),\n    'null': ('NoneType',),\n    'string': ('unicode', 'str'),\n    'number': ('float', 'int', 'long'),\n    'expref': ('_Expression',),\n}\n\n\ndef signature(*arguments):\n    def _record_signature(func):\n        func.signature = arguments\n        return func\n    return _record_signature\n\n\nclass FunctionRegistry(type):\n    def __init__(cls, name, bases, attrs):\n        cls._populate_function_table()\n        super(FunctionRegistry, cls).__init__(name, bases, attrs)\n\n    def _populate_function_table(cls):\n        function_table = {}\n        # Any method with a @signature decorator that also\n        # starts with \"_func_\" is registered as a function.\n        # _func_max_by -> max_by function.\n        for name, method in get_methods(cls):\n            if not name.startswith('_func_'):\n                continue\n            signature = getattr(method, 'signature', None)\n            if signature is not None:\n                function_table[name[6:]] = {\n                    'function': method,\n                    'signature': signature,\n                }\n        cls.FUNCTION_TABLE = function_table\n\n\nclass Functions(metaclass=FunctionRegistry):\n\n    FUNCTION_TABLE = {\n    }\n\n    def call_function(self, function_name, resolved_args):\n        try:\n            spec = self.FUNCTION_TABLE[function_name]\n        except KeyError:\n            raise exceptions.UnknownFunctionError(\n                \"Unknown function: %s()\" % function_name)\n        function = spec['function']\n        signature = spec['signature']\n        self._validate_arguments(resolved_args, signature, function_name)\n        return function(self, *resolved_args)\n\n    def _validate_arguments(self, args, signature, function_name):\n        if signature and signature[-1].get('variadic'):\n            if len(args) < len(signature):\n                raise exceptions.VariadictArityError(\n                    len(signature), len(args), function_name)\n        elif len(args) != len(signature):\n            raise exceptions.ArityError(\n                len(signature), len(args), function_name)\n        return self._type_check(args, signature, function_name)\n\n    def _type_check(self, actual, signature, function_name):\n        for i in range(len(signature)):\n            allowed_types = signature[i]['types']\n            if allowed_types:\n                self._type_check_single(actual[i], allowed_types,\n                                        function_name)\n\n    def _type_check_single(self, current, types, function_name):\n        # Type checking involves checking the top level type,\n        # and in the case of arrays, potentially checking the types\n        # of each element.\n        allowed_types, allowed_subtypes = self._get_allowed_pytypes(types)\n        # We're not using isinstance() on purpose.\n        # The type model for jmespath does not map\n        # 1-1 with python types (booleans are considered\n        # integers in python for example).\n        actual_typename = type(current).__name__\n        if actual_typename not in allowed_types:\n            raise exceptions.JMESPathTypeError(\n                function_name, current,\n                self._convert_to_jmespath_type(actual_typename), types)\n        # If we're dealing with a list type, we can have\n        # additional restrictions on the type of the list\n        # elements (for example a function can require a\n        # list of numbers or a list of strings).\n        # Arrays are the only types that can have subtypes.\n        if allowed_subtypes:\n            self._subtype_check(current, allowed_subtypes,\n                                types, function_name)\n\n    def _get_allowed_pytypes(self, types):\n        allowed_types = []\n        allowed_subtypes = []\n        for t in types:\n            type_ = t.split('-', 1)\n            if len(type_) == 2:\n                type_, subtype = type_\n                allowed_subtypes.append(REVERSE_TYPES_MAP[subtype])\n            else:\n                type_ = type_[0]\n            allowed_types.extend(REVERSE_TYPES_MAP[type_])\n        return allowed_types, allowed_subtypes\n\n    def _subtype_check(self, current, allowed_subtypes, types, function_name):\n        if len(allowed_subtypes) == 1:\n            # The easy case, we know up front what type\n            # we need to validate.\n            allowed_subtypes = allowed_subtypes[0]\n            for element in current:\n                actual_typename = type(element).__name__\n                if actual_typename not in allowed_subtypes:\n                    raise exceptions.JMESPathTypeError(\n                        function_name, element, actual_typename, types)\n        elif len(allowed_subtypes) > 1 and current:\n            # Dynamic type validation.  Based on the first\n            # type we see, we validate that the remaining types\n            # match.\n            first = type(current[0]).__name__\n            for subtypes in allowed_subtypes:\n                if first in subtypes:\n                    allowed = subtypes\n                    break\n            else:\n                raise exceptions.JMESPathTypeError(\n                    function_name, current[0], first, types)\n            for element in current:\n                actual_typename = type(element).__name__\n                if actual_typename not in allowed:\n                    raise exceptions.JMESPathTypeError(\n                        function_name, element, actual_typename, types)\n\n    @signature({'types': ['number']})\n    def _func_abs(self, arg):\n        return abs(arg)\n\n    @signature({'types': ['array-number']})\n    def _func_avg(self, arg):\n        if arg:\n            return sum(arg) / len(arg)\n        else:\n            return None\n\n    @signature({'types': [], 'variadic': True})\n    def _func_not_null(self, *arguments):\n        for argument in arguments:\n            if argument is not None:\n                return argument\n\n    @signature({'types': []})\n    def _func_to_array(self, arg):\n        if isinstance(arg, list):\n            return arg\n        else:\n            return [arg]\n\n    @signature({'types': []})\n    def _func_to_string(self, arg):\n        if isinstance(arg, STRING_TYPE):\n            return arg\n        else:\n            return json.dumps(arg, separators=(',', ':'),\n                              default=str)\n\n    @signature({'types': []})\n    def _func_to_number(self, arg):\n        if isinstance(arg, (list, dict, bool)):\n            return None\n        elif arg is None:\n            return None\n        elif isinstance(arg, (int, float)):\n            return arg\n        else:\n            try:\n                return int(arg)\n            except ValueError:\n                try:\n                    return float(arg)\n                except ValueError:\n                    return None\n\n    @signature({'types': ['array', 'string']}, {'types': []})\n    def _func_contains(self, subject, search):\n        return search in subject\n\n    @signature({'types': ['string', 'array', 'object']})\n    def _func_length(self, arg):\n        return len(arg)\n\n    @signature({'types': ['string']}, {'types': ['string']})\n    def _func_ends_with(self, search, suffix):\n        return search.endswith(suffix)\n\n    @signature({'types': ['string']}, {'types': ['string']})\n    def _func_starts_with(self, search, suffix):\n        return search.startswith(suffix)\n\n    @signature({'types': ['array', 'string']})\n    def _func_reverse(self, arg):\n        if isinstance(arg, STRING_TYPE):\n            return arg[::-1]\n        else:\n            return list(reversed(arg))\n\n    @signature({\"types\": ['number']})\n    def _func_ceil(self, arg):\n        return math.ceil(arg)\n\n    @signature({\"types\": ['number']})\n    def _func_floor(self, arg):\n        return math.floor(arg)\n\n    @signature({\"types\": ['string']}, {\"types\": ['array-string']})\n    def _func_join(self, separator, array):\n        return separator.join(array)\n\n    @signature({'types': ['expref']}, {'types': ['array']})\n    def _func_map(self, expref, arg):\n        result = []\n        for element in arg:\n            result.append(expref.visit(expref.expression, element))\n        return result\n\n    @signature({\"types\": ['array-number', 'array-string']})\n    def _func_max(self, arg):\n        if arg:\n            return max(arg)\n        else:\n            return None\n\n    @signature({\"types\": [\"object\"], \"variadic\": True})\n    def _func_merge(self, *arguments):\n        merged = {}\n        for arg in arguments:\n            merged.update(arg)\n        return merged\n\n    @signature({\"types\": ['array-number', 'array-string']})\n    def _func_min(self, arg):\n        if arg:\n            return min(arg)\n        else:\n            return None\n\n    @signature({\"types\": ['array-string', 'array-number']})\n    def _func_sort(self, arg):\n        return list(sorted(arg))\n\n    @signature({\"types\": ['array-number']})\n    def _func_sum(self, arg):\n        return sum(arg)\n\n    @signature({\"types\": ['object']})\n    def _func_keys(self, arg):\n        # To be consistent with .values()\n        # should we also return the indices of a list?\n        return list(arg.keys())\n\n    @signature({\"types\": ['object']})\n    def _func_values(self, arg):\n        return list(arg.values())\n\n    @signature({'types': []})\n    def _func_type(self, arg):\n        if isinstance(arg, STRING_TYPE):\n            return \"string\"\n        elif isinstance(arg, bool):\n            return \"boolean\"\n        elif isinstance(arg, list):\n            return \"array\"\n        elif isinstance(arg, dict):\n            return \"object\"\n        elif isinstance(arg, (float, int)):\n            return \"number\"\n        elif arg is None:\n            return \"null\"\n\n    @signature({'types': ['array']}, {'types': ['expref']})\n    def _func_sort_by(self, array, expref):\n        if not array:\n            return array\n        # sort_by allows for the expref to be either a number of\n        # a string, so we have some special logic to handle this.\n        # We evaluate the first array element and verify that it's\n        # either a string of a number.  We then create a key function\n        # that validates that type, which requires that remaining array\n        # elements resolve to the same type as the first element.\n        required_type = self._convert_to_jmespath_type(\n            type(expref.visit(expref.expression, array[0])).__name__)\n        if required_type not in ['number', 'string']:\n            raise exceptions.JMESPathTypeError(\n                'sort_by', array[0], required_type, ['string', 'number'])\n        keyfunc = self._create_key_func(expref,\n                                        [required_type],\n                                        'sort_by')\n        return list(sorted(array, key=keyfunc))\n\n    @signature({'types': ['array']}, {'types': ['expref']})\n    def _func_min_by(self, array, expref):\n        keyfunc = self._create_key_func(expref,\n                                        ['number', 'string'],\n                                        'min_by')\n        if array:\n            return min(array, key=keyfunc)\n        else:\n            return None\n\n    @signature({'types': ['array']}, {'types': ['expref']})\n    def _func_max_by(self, array, expref):\n        keyfunc = self._create_key_func(expref,\n                                        ['number', 'string'],\n                                        'max_by')\n        if array:\n            return max(array, key=keyfunc)\n        else:\n            return None\n\n    def _create_key_func(self, expref, allowed_types, function_name):\n        def keyfunc(x):\n            result = expref.visit(expref.expression, x)\n            actual_typename = type(result).__name__\n            jmespath_type = self._convert_to_jmespath_type(actual_typename)\n            # allowed_types is in term of jmespath types, not python types.\n            if jmespath_type not in allowed_types:\n                raise exceptions.JMESPathTypeError(\n                    function_name, result, jmespath_type, allowed_types)\n            return result\n        return keyfunc\n\n    def _convert_to_jmespath_type(self, pyobject):\n        return TYPES_MAP.get(pyobject, 'unknown')\n", "jmespath/__init__.py": "from jmespath import parser\nfrom jmespath.visitor import Options\n\n__version__ = '1.0.1'\n\n\ndef compile(expression):\n    return parser.Parser().parse(expression)\n\n\ndef search(expression, data, options=None):\n    return parser.Parser().parse(expression).search(data, options=options)\n", "jmespath/parser.py": "\"\"\"Top down operator precedence parser.\n\nThis is an implementation of Vaughan R. Pratt's\n\"Top Down Operator Precedence\" parser.\n(http://dl.acm.org/citation.cfm?doid=512927.512931).\n\nThese are some additional resources that help explain the\ngeneral idea behind a Pratt parser:\n\n* http://effbot.org/zone/simple-top-down-parsing.htm\n* http://javascript.crockford.com/tdop/tdop.html\n\nA few notes on the implementation.\n\n* All the nud/led tokens are on the Parser class itself, and are dispatched\n  using getattr().  This keeps all the parsing logic contained to a single\n  class.\n* We use two passes through the data.  One to create a list of token,\n  then one pass through the tokens to create the AST.  While the lexer actually\n  yields tokens, we convert it to a list so we can easily implement two tokens\n  of lookahead.  A previous implementation used a fixed circular buffer, but it\n  was significantly slower.  Also, the average jmespath expression typically\n  does not have a large amount of token so this is not an issue.  And\n  interestingly enough, creating a token list first is actually faster than\n  consuming from the token iterator one token at a time.\n\n\"\"\"\nimport random\n\nfrom jmespath import lexer\nfrom jmespath.compat import with_repr_method\nfrom jmespath import ast\nfrom jmespath import exceptions\nfrom jmespath import visitor\n\n\nclass Parser(object):\n    BINDING_POWER = {\n        'eof': 0,\n        'unquoted_identifier': 0,\n        'quoted_identifier': 0,\n        'literal': 0,\n        'rbracket': 0,\n        'rparen': 0,\n        'comma': 0,\n        'rbrace': 0,\n        'number': 0,\n        'current': 0,\n        'expref': 0,\n        'colon': 0,\n        'pipe': 1,\n        'or': 2,\n        'and': 3,\n        'eq': 5,\n        'gt': 5,\n        'lt': 5,\n        'gte': 5,\n        'lte': 5,\n        'ne': 5,\n        'flatten': 9,\n        # Everything above stops a projection.\n        'star': 20,\n        'filter': 21,\n        'dot': 40,\n        'not': 45,\n        'lbrace': 50,\n        'lbracket': 55,\n        'lparen': 60,\n    }\n    # The maximum binding power for a token that can stop\n    # a projection.\n    _PROJECTION_STOP = 10\n    # The _MAX_SIZE most recent expressions are cached in\n    # _CACHE dict.\n    _CACHE = {}\n    _MAX_SIZE = 128\n\n    def __init__(self, lookahead=2):\n        self.tokenizer = None\n        self._tokens = [None] * lookahead\n        self._buffer_size = lookahead\n        self._index = 0\n\n    def parse(self, expression):\n        cached = self._CACHE.get(expression)\n        if cached is not None:\n            return cached\n        parsed_result = self._do_parse(expression)\n        self._CACHE[expression] = parsed_result\n        if len(self._CACHE) > self._MAX_SIZE:\n            self._free_cache_entries()\n        return parsed_result\n\n    def _do_parse(self, expression):\n        try:\n            return self._parse(expression)\n        except exceptions.LexerError as e:\n            e.expression = expression\n            raise\n        except exceptions.IncompleteExpressionError as e:\n            e.set_expression(expression)\n            raise\n        except exceptions.ParseError as e:\n            e.expression = expression\n            raise\n\n    def _parse(self, expression):\n        self.tokenizer = lexer.Lexer().tokenize(expression)\n        self._tokens = list(self.tokenizer)\n        self._index = 0\n        parsed = self._expression(binding_power=0)\n        if not self._current_token() == 'eof':\n            t = self._lookahead_token(0)\n            raise exceptions.ParseError(t['start'], t['value'], t['type'],\n                                        \"Unexpected token: %s\" % t['value'])\n        return ParsedResult(expression, parsed)\n\n    def _expression(self, binding_power=0):\n        left_token = self._lookahead_token(0)\n        self._advance()\n        nud_function = getattr(\n            self, '_token_nud_%s' % left_token['type'],\n            self._error_nud_token)\n        left = nud_function(left_token)\n        current_token = self._current_token()\n        while binding_power < self.BINDING_POWER[current_token]:\n            led = getattr(self, '_token_led_%s' % current_token, None)\n            if led is None:\n                error_token = self._lookahead_token(0)\n                self._error_led_token(error_token)\n            else:\n                self._advance()\n                left = led(left)\n                current_token = self._current_token()\n        return left\n\n    def _token_nud_literal(self, token):\n        return ast.literal(token['value'])\n\n    def _token_nud_unquoted_identifier(self, token):\n        return ast.field(token['value'])\n\n    def _token_nud_quoted_identifier(self, token):\n        field = ast.field(token['value'])\n        # You can't have a quoted identifier as a function\n        # name.\n        if self._current_token() == 'lparen':\n            t = self._lookahead_token(0)\n            raise exceptions.ParseError(\n                0, t['value'], t['type'],\n                'Quoted identifier not allowed for function names.')\n        return field\n\n    def _token_nud_star(self, token):\n        left = ast.identity()\n        if self._current_token() == 'rbracket':\n            right = ast.identity()\n        else:\n            right = self._parse_projection_rhs(self.BINDING_POWER['star'])\n        return ast.value_projection(left, right)\n\n    def _token_nud_filter(self, token):\n        return self._token_led_filter(ast.identity())\n\n    def _token_nud_lbrace(self, token):\n        return self._parse_multi_select_hash()\n\n    def _token_nud_lparen(self, token):\n        expression = self._expression()\n        self._match('rparen')\n        return expression\n\n    def _token_nud_flatten(self, token):\n        left = ast.flatten(ast.identity())\n        right = self._parse_projection_rhs(\n            self.BINDING_POWER['flatten'])\n        return ast.projection(left, right)\n\n    def _token_nud_not(self, token):\n        expr = self._expression(self.BINDING_POWER['not'])\n        return ast.not_expression(expr)\n\n    def _token_nud_lbracket(self, token):\n        if self._current_token() in ['number', 'colon']:\n            right = self._parse_index_expression()\n            # We could optimize this and remove the identity() node.\n            # We don't really need an index_expression node, we can\n            # just use emit an index node here if we're not dealing\n            # with a slice.\n            return self._project_if_slice(ast.identity(), right)\n        elif self._current_token() == 'star' and \\\n                self._lookahead(1) == 'rbracket':\n            self._advance()\n            self._advance()\n            right = self._parse_projection_rhs(self.BINDING_POWER['star'])\n            return ast.projection(ast.identity(), right)\n        else:\n            return self._parse_multi_select_list()\n\n    def _parse_index_expression(self):\n        # We're here:\n        # [<current>\n        #  ^\n        #  | current token\n        if (self._lookahead(0) == 'colon' or\n                self._lookahead(1) == 'colon'):\n            return self._parse_slice_expression()\n        else:\n            # Parse the syntax [number]\n            node = ast.index(self._lookahead_token(0)['value'])\n            self._advance()\n            self._match('rbracket')\n            return node\n\n    def _parse_slice_expression(self):\n        # [start:end:step]\n        # Where start, end, and step are optional.\n        # The last colon is optional as well.\n        parts = [None, None, None]\n        index = 0\n        current_token = self._current_token()\n        while not current_token == 'rbracket' and index < 3:\n            if current_token == 'colon':\n                index += 1\n                if index == 3:\n                    self._raise_parse_error_for_token(\n                        self._lookahead_token(0), 'syntax error')\n                self._advance()\n            elif current_token == 'number':\n                parts[index] = self._lookahead_token(0)['value']\n                self._advance()\n            else:\n                self._raise_parse_error_for_token(\n                    self._lookahead_token(0), 'syntax error')\n            current_token = self._current_token()\n        self._match('rbracket')\n        return ast.slice(*parts)\n\n    def _token_nud_current(self, token):\n        return ast.current_node()\n\n    def _token_nud_expref(self, token):\n        expression = self._expression(self.BINDING_POWER['expref'])\n        return ast.expref(expression)\n\n    def _token_led_dot(self, left):\n        if not self._current_token() == 'star':\n            right = self._parse_dot_rhs(self.BINDING_POWER['dot'])\n            if left['type'] == 'subexpression':\n                left['children'].append(right)\n                return left\n            else:\n                return ast.subexpression([left, right])\n        else:\n            # We're creating a projection.\n            self._advance()\n            right = self._parse_projection_rhs(\n                self.BINDING_POWER['dot'])\n            return ast.value_projection(left, right)\n\n    def _token_led_pipe(self, left):\n        right = self._expression(self.BINDING_POWER['pipe'])\n        return ast.pipe(left, right)\n\n    def _token_led_or(self, left):\n        right = self._expression(self.BINDING_POWER['or'])\n        return ast.or_expression(left, right)\n\n    def _token_led_and(self, left):\n        right = self._expression(self.BINDING_POWER['and'])\n        return ast.and_expression(left, right)\n\n    def _token_led_lparen(self, left):\n        if left['type'] != 'field':\n            #  0 - first func arg or closing paren.\n            # -1 - '(' token\n            # -2 - invalid function \"name\".\n            prev_t = self._lookahead_token(-2)\n            raise exceptions.ParseError(\n                prev_t['start'], prev_t['value'], prev_t['type'],\n                \"Invalid function name '%s'\" % prev_t['value'])\n        name = left['value']\n        args = []\n        while not self._current_token() == 'rparen':\n            expression = self._expression()\n            if self._current_token() == 'comma':\n                self._match('comma')\n            args.append(expression)\n        self._match('rparen')\n        function_node = ast.function_expression(name, args)\n        return function_node\n\n    def _token_led_filter(self, left):\n        # Filters are projections.\n        condition = self._expression(0)\n        self._match('rbracket')\n        if self._current_token() == 'flatten':\n            right = ast.identity()\n        else:\n            right = self._parse_projection_rhs(self.BINDING_POWER['filter'])\n        return ast.filter_projection(left, right, condition)\n\n    def _token_led_eq(self, left):\n        return self._parse_comparator(left, 'eq')\n\n    def _token_led_ne(self, left):\n        return self._parse_comparator(left, 'ne')\n\n    def _token_led_gt(self, left):\n        return self._parse_comparator(left, 'gt')\n\n    def _token_led_gte(self, left):\n        return self._parse_comparator(left, 'gte')\n\n    def _token_led_lt(self, left):\n        return self._parse_comparator(left, 'lt')\n\n    def _token_led_lte(self, left):\n        return self._parse_comparator(left, 'lte')\n\n    def _token_led_flatten(self, left):\n        left = ast.flatten(left)\n        right = self._parse_projection_rhs(\n            self.BINDING_POWER['flatten'])\n        return ast.projection(left, right)\n\n    def _token_led_lbracket(self, left):\n        token = self._lookahead_token(0)\n        if token['type'] in ['number', 'colon']:\n            right = self._parse_index_expression()\n            if left['type'] == 'index_expression':\n                # Optimization: if the left node is an index expr,\n                # we can avoid creating another node and instead just add\n                # the right node as a child of the left.\n                left['children'].append(right)\n                return left\n            else:\n                return self._project_if_slice(left, right)\n        else:\n            # We have a projection\n            self._match('star')\n            self._match('rbracket')\n            right = self._parse_projection_rhs(self.BINDING_POWER['star'])\n            return ast.projection(left, right)\n\n    def _project_if_slice(self, left, right):\n        index_expr = ast.index_expression([left, right])\n        if right['type'] == 'slice':\n            return ast.projection(\n                index_expr,\n                self._parse_projection_rhs(self.BINDING_POWER['star']))\n        else:\n            return index_expr\n\n    def _parse_comparator(self, left, comparator):\n        right = self._expression(self.BINDING_POWER[comparator])\n        return ast.comparator(comparator, left, right)\n\n    def _parse_multi_select_list(self):\n        expressions = []\n        while True:\n            expression = self._expression()\n            expressions.append(expression)\n            if self._current_token() == 'rbracket':\n                break\n            else:\n                self._match('comma')\n        self._match('rbracket')\n        return ast.multi_select_list(expressions)\n\n    def _parse_multi_select_hash(self):\n        pairs = []\n        while True:\n            key_token = self._lookahead_token(0)\n            # Before getting the token value, verify it's\n            # an identifier.\n            self._match_multiple_tokens(\n                token_types=['quoted_identifier', 'unquoted_identifier'])\n            key_name = key_token['value']\n            self._match('colon')\n            value = self._expression(0)\n            node = ast.key_val_pair(key_name=key_name, node=value)\n            pairs.append(node)\n            if self._current_token() == 'comma':\n                self._match('comma')\n            elif self._current_token() == 'rbrace':\n                self._match('rbrace')\n                break\n        return ast.multi_select_dict(nodes=pairs)\n\n    def _parse_projection_rhs(self, binding_power):\n        # Parse the right hand side of the projection.\n        if self.BINDING_POWER[self._current_token()] < self._PROJECTION_STOP:\n            # BP of 10 are all the tokens that stop a projection.\n            right = ast.identity()\n        elif self._current_token() == 'lbracket':\n            right = self._expression(binding_power)\n        elif self._current_token() == 'filter':\n            right = self._expression(binding_power)\n        elif self._current_token() == 'dot':\n            self._match('dot')\n            right = self._parse_dot_rhs(binding_power)\n        else:\n            self._raise_parse_error_for_token(self._lookahead_token(0),\n                                              'syntax error')\n        return right\n\n    def _parse_dot_rhs(self, binding_power):\n        # From the grammar:\n        # expression '.' ( identifier /\n        #                  multi-select-list /\n        #                  multi-select-hash /\n        #                  function-expression /\n        #                  *\n        # In terms of tokens that means that after a '.',\n        # you can have:\n        lookahead = self._current_token()\n        # Common case \"foo.bar\", so first check for an identifier.\n        if lookahead in ['quoted_identifier', 'unquoted_identifier', 'star']:\n            return self._expression(binding_power)\n        elif lookahead == 'lbracket':\n            self._match('lbracket')\n            return self._parse_multi_select_list()\n        elif lookahead == 'lbrace':\n            self._match('lbrace')\n            return self._parse_multi_select_hash()\n        else:\n            t = self._lookahead_token(0)\n            allowed = ['quoted_identifier', 'unquoted_identifier',\n                       'lbracket', 'lbrace']\n            msg = (\n                \"Expecting: %s, got: %s\" % (allowed, t['type'])\n            )\n            self._raise_parse_error_for_token(t, msg)\n\n    def _error_nud_token(self, token):\n        if token['type'] == 'eof':\n            raise exceptions.IncompleteExpressionError(\n                token['start'], token['value'], token['type'])\n        self._raise_parse_error_for_token(token, 'invalid token')\n\n    def _error_led_token(self, token):\n        self._raise_parse_error_for_token(token, 'invalid token')\n\n    def _match(self, token_type=None):\n        # inline'd self._current_token()\n        if self._current_token() == token_type:\n            # inline'd self._advance()\n            self._advance()\n        else:\n            self._raise_parse_error_maybe_eof(\n                token_type, self._lookahead_token(0))\n\n    def _match_multiple_tokens(self, token_types):\n        if self._current_token() not in token_types:\n            self._raise_parse_error_maybe_eof(\n                token_types, self._lookahead_token(0))\n        self._advance()\n\n    def _advance(self):\n        self._index += 1\n\n    def _current_token(self):\n        return self._tokens[self._index]['type']\n\n    def _lookahead(self, number):\n        return self._tokens[self._index + number]['type']\n\n    def _lookahead_token(self, number):\n        return self._tokens[self._index + number]\n\n    def _raise_parse_error_for_token(self, token, reason):\n        lex_position = token['start']\n        actual_value = token['value']\n        actual_type = token['type']\n        raise exceptions.ParseError(lex_position, actual_value,\n                                    actual_type, reason)\n\n    def _raise_parse_error_maybe_eof(self, expected_type, token):\n        lex_position = token['start']\n        actual_value = token['value']\n        actual_type = token['type']\n        if actual_type == 'eof':\n            raise exceptions.IncompleteExpressionError(\n                lex_position, actual_value, actual_type)\n        message = 'Expecting: %s, got: %s' % (expected_type,\n                                              actual_type)\n        raise exceptions.ParseError(\n            lex_position, actual_value, actual_type, message)\n\n    def _free_cache_entries(self):\n        for key in random.sample(list(self._CACHE.keys()), int(self._MAX_SIZE / 2)):\n            self._CACHE.pop(key, None)\n\n    @classmethod\n    def purge(cls):\n        \"\"\"Clear the expression compilation cache.\"\"\"\n        cls._CACHE.clear()\n\n\n@with_repr_method\nclass ParsedResult(object):\n    def __init__(self, expression, parsed):\n        self.expression = expression\n        self.parsed = parsed\n\n    def search(self, value, options=None):\n        interpreter = visitor.TreeInterpreter(options)\n        result = interpreter.visit(self.parsed, value)\n        return result\n\n    def _render_dot_file(self):\n        \"\"\"Render the parsed AST as a dot file.\n\n        Note that this is marked as an internal method because\n        the AST is an implementation detail and is subject\n        to change.  This method can be used to help troubleshoot\n        or for development purposes, but is not considered part\n        of the public supported API.  Use at your own risk.\n\n        \"\"\"\n        renderer = visitor.GraphvizVisitor()\n        contents = renderer.visit(self.parsed)\n        return contents\n\n    def __repr__(self):\n        return repr(self.parsed)\n", "jmespath/visitor.py": "import operator\n\nfrom jmespath import functions\nfrom jmespath.compat import string_type\nfrom numbers import Number\n\n\ndef _equals(x, y):\n    if _is_special_number_case(x, y):\n        return False\n    else:\n        return x == y\n\n\ndef _is_special_number_case(x, y):\n    # We need to special case comparing 0 or 1 to\n    # True/False.  While normally comparing any\n    # integer other than 0/1 to True/False will always\n    # return False.  However 0/1 have this:\n    # >>> 0 == True\n    # False\n    # >>> 0 == False\n    # True\n    # >>> 1 == True\n    # True\n    # >>> 1 == False\n    # False\n    #\n    # Also need to consider that:\n    # >>> 0 in [True, False]\n    # True\n    if _is_actual_number(x) and x in (0, 1):\n        return isinstance(y, bool)\n    elif _is_actual_number(y) and y in (0, 1):\n        return isinstance(x, bool)\n\n\ndef _is_comparable(x):\n    # The spec doesn't officially support string types yet,\n    # but enough people are relying on this behavior that\n    # it's been added back.  This should eventually become\n    # part of the official spec.\n    return _is_actual_number(x) or isinstance(x, string_type)\n\n\ndef _is_actual_number(x):\n    # We need to handle python's quirkiness with booleans,\n    # specifically:\n    #\n    # >>> isinstance(False, int)\n    # True\n    # >>> isinstance(True, int)\n    # True\n    if isinstance(x, bool):\n        return False\n    return isinstance(x, Number)\n\n\nclass Options(object):\n    \"\"\"Options to control how a JMESPath function is evaluated.\"\"\"\n    def __init__(self, dict_cls=None, custom_functions=None):\n        #: The class to use when creating a dict.  The interpreter\n        #  may create dictionaries during the evaluation of a JMESPath\n        #  expression.  For example, a multi-select hash will\n        #  create a dictionary.  By default we use a dict() type.\n        #  You can set this value to change what dict type is used.\n        #  The most common reason you would change this is if you\n        #  want to set a collections.OrderedDict so that you can\n        #  have predictable key ordering.\n        self.dict_cls = dict_cls\n        self.custom_functions = custom_functions\n\n\nclass _Expression(object):\n    def __init__(self, expression, interpreter):\n        self.expression = expression\n        self.interpreter = interpreter\n\n    def visit(self, node, *args, **kwargs):\n        return self.interpreter.visit(node, *args, **kwargs)\n\n\nclass Visitor(object):\n    def __init__(self):\n        self._method_cache = {}\n\n    def visit(self, node, *args, **kwargs):\n        node_type = node['type']\n        method = self._method_cache.get(node_type)\n        if method is None:\n            method = getattr(\n                self, 'visit_%s' % node['type'], self.default_visit)\n            self._method_cache[node_type] = method\n        return method(node, *args, **kwargs)\n\n    def default_visit(self, node, *args, **kwargs):\n        raise NotImplementedError(\"default_visit\")\n\n\nclass TreeInterpreter(Visitor):\n    COMPARATOR_FUNC = {\n        'eq': _equals,\n        'ne': lambda x, y: not _equals(x, y),\n        'lt': operator.lt,\n        'gt': operator.gt,\n        'lte': operator.le,\n        'gte': operator.ge\n    }\n    _EQUALITY_OPS = ['eq', 'ne']\n    MAP_TYPE = dict\n\n    def __init__(self, options=None):\n        super(TreeInterpreter, self).__init__()\n        self._dict_cls = self.MAP_TYPE\n        if options is None:\n            options = Options()\n        self._options = options\n        if options.dict_cls is not None:\n            self._dict_cls = self._options.dict_cls\n        if options.custom_functions is not None:\n            self._functions = self._options.custom_functions\n        else:\n            self._functions = functions.Functions()\n\n    def default_visit(self, node, *args, **kwargs):\n        raise NotImplementedError(node['type'])\n\n    def visit_subexpression(self, node, value):\n        result = value\n        for node in node['children']:\n            result = self.visit(node, result)\n        return result\n\n    def visit_field(self, node, value):\n        try:\n            return value.get(node['value'])\n        except AttributeError:\n            return None\n\n    def visit_comparator(self, node, value):\n        # Common case: comparator is == or !=\n        comparator_func = self.COMPARATOR_FUNC[node['value']]\n        if node['value'] in self._EQUALITY_OPS:\n            return comparator_func(\n                self.visit(node['children'][0], value),\n                self.visit(node['children'][1], value)\n            )\n        else:\n            # Ordering operators are only valid for numbers.\n            # Evaluating any other type with a comparison operator\n            # will yield a None value.\n            left = self.visit(node['children'][0], value)\n            right = self.visit(node['children'][1], value)\n            num_types = (int, float)\n            if not (_is_comparable(left) and\n                    _is_comparable(right)):\n                return None\n            return comparator_func(left, right)\n\n    def visit_current(self, node, value):\n        return value\n\n    def visit_expref(self, node, value):\n        return _Expression(node['children'][0], self)\n\n    def visit_function_expression(self, node, value):\n        resolved_args = []\n        for child in node['children']:\n            current = self.visit(child, value)\n            resolved_args.append(current)\n        return self._functions.call_function(node['value'], resolved_args)\n\n    def visit_filter_projection(self, node, value):\n        base = self.visit(node['children'][0], value)\n        if not isinstance(base, list):\n            return None\n        comparator_node = node['children'][2]\n        collected = []\n        for element in base:\n            if self._is_true(self.visit(comparator_node, element)):\n                current = self.visit(node['children'][1], element)\n                if current is not None:\n                    collected.append(current)\n        return collected\n\n    def visit_flatten(self, node, value):\n        base = self.visit(node['children'][0], value)\n        if not isinstance(base, list):\n            # Can't flatten the object if it's not a list.\n            return None\n        merged_list = []\n        for element in base:\n            if isinstance(element, list):\n                merged_list.extend(element)\n            else:\n                merged_list.append(element)\n        return merged_list\n\n    def visit_identity(self, node, value):\n        return value\n\n    def visit_index(self, node, value):\n        # Even though we can index strings, we don't\n        # want to support that.\n        if not isinstance(value, list):\n            return None\n        try:\n            return value[node['value']]\n        except IndexError:\n            return None\n\n    def visit_index_expression(self, node, value):\n        result = value\n        for node in node['children']:\n            result = self.visit(node, result)\n        return result\n\n    def visit_slice(self, node, value):\n        if not isinstance(value, list):\n            return None\n        s = slice(*node['children'])\n        return value[s]\n\n    def visit_key_val_pair(self, node, value):\n        return self.visit(node['children'][0], value)\n\n    def visit_literal(self, node, value):\n        return node['value']\n\n    def visit_multi_select_dict(self, node, value):\n        if value is None:\n            return None\n        collected = self._dict_cls()\n        for child in node['children']:\n            collected[child['value']] = self.visit(child, value)\n        return collected\n\n    def visit_multi_select_list(self, node, value):\n        if value is None:\n            return None\n        collected = []\n        for child in node['children']:\n            collected.append(self.visit(child, value))\n        return collected\n\n    def visit_or_expression(self, node, value):\n        matched = self.visit(node['children'][0], value)\n        if self._is_false(matched):\n            matched = self.visit(node['children'][1], value)\n        return matched\n\n    def visit_and_expression(self, node, value):\n        matched = self.visit(node['children'][0], value)\n        if self._is_false(matched):\n            return matched\n        return self.visit(node['children'][1], value)\n\n    def visit_not_expression(self, node, value):\n        original_result = self.visit(node['children'][0], value)\n        if _is_actual_number(original_result) and original_result == 0:\n            # Special case for 0, !0 should be false, not true.\n            # 0 is not a special cased integer in jmespath.\n            return False\n        return not original_result\n\n    def visit_pipe(self, node, value):\n        result = value\n        for node in node['children']:\n            result = self.visit(node, result)\n        return result\n\n    def visit_projection(self, node, value):\n        base = self.visit(node['children'][0], value)\n        if not isinstance(base, list):\n            return None\n        collected = []\n        for element in base:\n            current = self.visit(node['children'][1], element)\n            if current is not None:\n                collected.append(current)\n        return collected\n\n    def visit_value_projection(self, node, value):\n        base = self.visit(node['children'][0], value)\n        try:\n            base = base.values()\n        except AttributeError:\n            return None\n        collected = []\n        for element in base:\n            current = self.visit(node['children'][1], element)\n            if current is not None:\n                collected.append(current)\n        return collected\n\n    def _is_false(self, value):\n        # This looks weird, but we're explicitly using equality checks\n        # because the truth/false values are different between\n        # python and jmespath.\n        return (value == '' or value == [] or value == {} or value is None or\n                value is False)\n\n    def _is_true(self, value):\n        return not self._is_false(value)\n\n\nclass GraphvizVisitor(Visitor):\n    def __init__(self):\n        super(GraphvizVisitor, self).__init__()\n        self._lines = []\n        self._count = 1\n\n    def visit(self, node, *args, **kwargs):\n        self._lines.append('digraph AST {')\n        current = '%s%s' % (node['type'], self._count)\n        self._count += 1\n        self._visit(node, current)\n        self._lines.append('}')\n        return '\\n'.join(self._lines)\n\n    def _visit(self, node, current):\n        self._lines.append('%s [label=\"%s(%s)\"]' % (\n            current, node['type'], node.get('value', '')))\n        for child in node.get('children', []):\n            child_name = '%s%s' % (child['type'], self._count)\n            self._count += 1\n            self._lines.append('  %s -> %s' % (current, child_name))\n            self._visit(child, child_name)\n", "jmespath/compat.py": "import sys\nimport inspect\nfrom itertools import zip_longest\n\n\ntext_type = str\nstring_type = str\n\n\ndef with_str_method(cls):\n    # In python3, we don't need to do anything, we return a str type.\n    return cls\n\ndef with_repr_method(cls):\n    return cls\n\ndef get_methods(cls):\n    for name, method in inspect.getmembers(cls, predicate=inspect.isfunction):\n        yield name, method\n", "tests/test_custom_functions.py": "import unittest\n\nimport jmespath\nfrom jmespath import functions\n\n\nclass CustomFunctions(functions.Functions):\n    @functions.signature({'types': ['string', 'array', 'object', 'null']})\n    def _func_length0(self, s):\n        return 0 if s is None else len(s)\n\n\nclass TestCustomFunctions(unittest.TestCase):\n    def setUp(self):\n        self.options = jmespath.Options(custom_functions=CustomFunctions())\n\n    def test_null_to_nonetype(self):\n        data = {\n            'a': {\n                'b': [1, 2, 3]\n            }\n        }\n\n        self.assertEqual(jmespath.search('length0(a.b)', data, self.options), 3)\n        self.assertEqual(jmespath.search('length0(a.c)', data, self.options), 0)\n", "tests/test_search.py": "import sys\nimport decimal\nfrom tests import unittest, OrderedDict\n\nimport jmespath\nimport jmespath.functions\n\n\nclass TestSearchOptions(unittest.TestCase):\n    def test_can_provide_dict_cls(self):\n        result = jmespath.search(\n            '{a: a, b: b, c: c}.*',\n            {'c': 'c', 'b': 'b', 'a': 'a', 'd': 'd'},\n            options=jmespath.Options(dict_cls=OrderedDict))\n        self.assertEqual(result, ['a', 'b', 'c'])\n\n    def test_can_provide_custom_functions(self):\n        class CustomFunctions(jmespath.functions.Functions):\n            @jmespath.functions.signature(\n                {'types': ['number']},\n                {'types': ['number']})\n            def _func_custom_add(self, x, y):\n                return x + y\n\n            @jmespath.functions.signature(\n                {'types': ['number']},\n                {'types': ['number']})\n            def _func_my_subtract(self, x, y):\n                return x - y\n\n\n        options = jmespath.Options(custom_functions=CustomFunctions())\n        self.assertEqual(\n            jmespath.search('custom_add(`1`, `2`)', {}, options=options),\n            3\n        )\n        self.assertEqual(\n            jmespath.search('my_subtract(`10`, `3`)', {}, options=options),\n            7\n        )\n        # Should still be able to use the original functions without\n        # any interference from the CustomFunctions class.\n        self.assertEqual(\n            jmespath.search('length(`[1, 2]`)', {}), 2\n        )\n\n\n\nclass TestPythonSpecificCases(unittest.TestCase):\n    def test_can_compare_strings(self):\n        # This is python specific behavior that's not in the official spec\n        # yet, but this was regression from 0.9.0 so it's been added back.\n        self.assertTrue(jmespath.search('a < b', {'a': '2016', 'b': '2017'}))\n\n    @unittest.skipIf(not hasattr(sys, 'maxint'), 'Test requires long() type')\n    def test_can_handle_long_ints(self):\n        result = sys.maxint + 1\n        self.assertEqual(jmespath.search('[?a >= `1`].a', [{'a': result}]),\n                         [result])\n\n    def test_can_handle_decimals_as_numeric_type(self):\n        result = decimal.Decimal('3')\n        self.assertEqual(jmespath.search('[?a >= `1`].a', [{'a': result}]),\n                         [result])\n", "tests/test_parser.py": "#!/usr/bin/env python\nimport re\nimport random\nimport string\nimport threading\nfrom tests import unittest, OrderedDict\n\nfrom jmespath import parser\nfrom jmespath import visitor\nfrom jmespath import ast\nfrom jmespath import exceptions\n\n\nclass TestParser(unittest.TestCase):\n    def setUp(self):\n        self.parser = parser.Parser()\n\n    def assert_parsed_ast(self, expression, expected_ast):\n        parsed = self.parser.parse(expression)\n        self.assertEqual(parsed.parsed, expected_ast)\n\n    def test_parse_empty_string_raises_exception(self):\n        with self.assertRaises(exceptions.EmptyExpressionError):\n            self.parser.parse('')\n\n    def test_field(self):\n        self.assert_parsed_ast('foo', ast.field('foo'))\n\n    def test_dot_syntax(self):\n        self.assert_parsed_ast('foo.bar',\n                               ast.subexpression([ast.field('foo'),\n                                                  ast.field('bar')]))\n\n    def test_multiple_dots(self):\n        parsed = self.parser.parse('foo.bar.baz')\n        self.assertEqual(\n            parsed.search({'foo': {'bar': {'baz': 'correct'}}}), 'correct')\n\n    def test_index(self):\n        parsed = self.parser.parse('foo[1]')\n        self.assertEqual(\n            parsed.search({'foo': ['zero', 'one', 'two']}),\n            'one')\n\n    def test_quoted_subexpression(self):\n        self.assert_parsed_ast('\"foo\".\"bar\"',\n                               ast.subexpression([\n                                   ast.field('foo'),\n                                   ast.field('bar')]))\n\n    def test_wildcard(self):\n        parsed = self.parser.parse('foo[*]')\n        self.assertEqual(\n            parsed.search({'foo': ['zero', 'one', 'two']}),\n            ['zero', 'one', 'two'])\n\n    def test_wildcard_with_children(self):\n        parsed = self.parser.parse('foo[*].bar')\n        self.assertEqual(\n            parsed.search({'foo': [{'bar': 'one'}, {'bar': 'two'}]}),\n            ['one', 'two'])\n\n    def test_or_expression(self):\n        parsed = self.parser.parse('foo || bar')\n        self.assertEqual(parsed.search({'foo': 'foo'}), 'foo')\n        self.assertEqual(parsed.search({'bar': 'bar'}), 'bar')\n        self.assertEqual(parsed.search({'foo': 'foo', 'bar': 'bar'}), 'foo')\n        self.assertEqual(parsed.search({'bad': 'bad'}), None)\n\n    def test_complex_or_expression(self):\n        parsed = self.parser.parse('foo.foo || foo.bar')\n        self.assertEqual(parsed.search({'foo': {'foo': 'foo'}}), 'foo')\n        self.assertEqual(parsed.search({'foo': {'bar': 'bar'}}), 'bar')\n        self.assertEqual(parsed.search({'foo': {'baz': 'baz'}}), None)\n\n    def test_or_repr(self):\n        self.assert_parsed_ast('foo || bar', ast.or_expression(ast.field('foo'),\n                                                               ast.field('bar')))\n\n    def test_unicode_literals_escaped(self):\n        self.assert_parsed_ast(r'`\"\\u2713\"`', ast.literal(u'\\u2713'))\n\n    def test_multiselect(self):\n        parsed = self.parser.parse('foo.{bar: bar,baz: baz}')\n        self.assertEqual(\n            parsed.search({'foo': {'bar': 'bar', 'baz': 'baz', 'qux': 'qux'}}),\n            {'bar': 'bar', 'baz': 'baz'})\n\n    def test_multiselect_subexpressions(self):\n        parsed = self.parser.parse('foo.{\"bar.baz\": bar.baz, qux: qux}')\n        self.assertEqual(\n            parsed.search({'foo': {'bar': {'baz': 'CORRECT'}, 'qux': 'qux'}}),\n            {'bar.baz': 'CORRECT', 'qux': 'qux'})\n\n    def test_multiselect_with_all_quoted_keys(self):\n        parsed = self.parser.parse('foo.{\"bar\": bar.baz, \"qux\": qux}')\n        result = parsed.search({'foo': {'bar': {'baz': 'CORRECT'}, 'qux': 'qux'}})\n        self.assertEqual(result, {\"bar\": \"CORRECT\", \"qux\": \"qux\"})\n\n    def test_function_call_with_and_statement(self):\n        self.assert_parsed_ast(\n            'f(@ && @)',\n            {'children': [{'children': [{'children': [], 'type': 'current'},\n                                        {'children': [], 'type': 'current'}],\n                           'type': 'and_expression'}],\n             'type': 'function_expression',\n             'value': 'f'})\n\n\nclass TestErrorMessages(unittest.TestCase):\n\n    def setUp(self):\n        self.parser = parser.Parser()\n\n    def assert_error_message(self, expression, error_message,\n                             exception=exceptions.ParseError):\n        try:\n            self.parser.parse(expression)\n        except exception as e:\n            self.assertEqual(error_message, str(e))\n            return\n        except Exception as e:\n            self.fail(\n                \"Unexpected error raised (%s: %s) for bad expression: %s\" %\n                (e.__class__.__name__, e, expression))\n        else:\n            self.fail(\n                \"ParseError not raised for bad expression: %s\" % expression)\n\n    def test_bad_parse(self):\n        with self.assertRaises(exceptions.ParseError):\n            self.parser.parse('foo]baz')\n\n    def test_bad_parse_error_message(self):\n        error_message = (\n            'Unexpected token: ]: Parse error at column 3, '\n            'token \"]\" (RBRACKET), for expression:\\n'\n            '\"foo]baz\"\\n'\n            '    ^')\n        self.assert_error_message('foo]baz', error_message)\n\n    def test_bad_parse_error_message_with_multiselect(self):\n        error_message = (\n            'Invalid jmespath expression: Incomplete expression:\\n'\n            '\"foo.{bar: baz,bar: bar\"\\n'\n            '                       ^')\n        self.assert_error_message('foo.{bar: baz,bar: bar', error_message)\n\n    def test_incomplete_expression_with_missing_paren(self):\n        error_message = (\n            'Invalid jmespath expression: Incomplete expression:\\n'\n            '\"length(@,\"\\n'\n            '          ^')\n        self.assert_error_message('length(@,', error_message)\n\n    def test_bad_lexer_values(self):\n        error_message = (\n            'Bad jmespath expression: '\n            'Unclosed \" delimiter:\\n'\n            'foo.\"bar\\n'\n            '    ^')\n        self.assert_error_message('foo.\"bar', error_message,\n                                  exception=exceptions.LexerError)\n\n    def test_bad_unicode_string(self):\n        # This error message is straight from the JSON parser\n        # and pypy has a slightly different error message,\n        # so we're not using assert_error_message.\n        error_message = re.compile(\n            r'Bad jmespath expression: '\n            r'Invalid \\\\uXXXX escape.*\\\\uAZ12', re.DOTALL)\n        with self.assertRaisesRegex(exceptions.LexerError, error_message):\n            self.parser.parse(r'\"\\uAZ12\"')\n\n\nclass TestParserWildcards(unittest.TestCase):\n    def setUp(self):\n        self.parser = parser.Parser()\n        self.data = {\n            'foo': [\n                {'bar': [{'baz': 'one'}, {'baz': 'two'}]},\n                {'bar': [{'baz': 'three'}, {'baz': 'four'}, {'baz': 'five'}]},\n            ]\n        }\n\n    def test_multiple_index_wildcards(self):\n        parsed = self.parser.parse('foo[*].bar[*].baz')\n        self.assertEqual(parsed.search(self.data),\n                         [['one', 'two'], ['three', 'four', 'five']])\n\n    def test_wildcard_mix_with_indices(self):\n        parsed = self.parser.parse('foo[*].bar[0].baz')\n        self.assertEqual(parsed.search(self.data),\n                         ['one', 'three'])\n\n    def test_wildcard_mix_last(self):\n        parsed = self.parser.parse('foo[0].bar[*].baz')\n        self.assertEqual(parsed.search(self.data),\n                         ['one', 'two'])\n\n    def test_indices_out_of_bounds(self):\n        parsed = self.parser.parse('foo[*].bar[2].baz')\n        self.assertEqual(parsed.search(self.data),\n                         ['five'])\n\n    def test_root_indices(self):\n        parsed = self.parser.parse('[0]')\n        self.assertEqual(parsed.search(['one', 'two']), 'one')\n\n    def test_root_wildcard(self):\n        parsed = self.parser.parse('*.foo')\n        data = {'top1': {'foo': 'bar'}, 'top2': {'foo': 'baz'},\n                'top3': {'notfoo': 'notfoo'}}\n        # Sorted is being used because the order of the keys are not\n        # required to be in any specific order.\n        self.assertEqual(sorted(parsed.search(data)), sorted(['bar', 'baz']))\n        self.assertEqual(sorted(self.parser.parse('*.notfoo').search(data)),\n                         sorted(['notfoo']))\n\n    def test_only_wildcard(self):\n        parsed = self.parser.parse('*')\n        data = {'foo': 'a', 'bar': 'b', 'baz': 'c'}\n        self.assertEqual(sorted(parsed.search(data)), sorted(['a', 'b', 'c']))\n\n    def test_escape_sequences(self):\n        self.assertEqual(self.parser.parse(r'\"foo\\tbar\"').search(\n            {'foo\\tbar': 'baz'}), 'baz')\n        self.assertEqual(self.parser.parse(r'\"foo\\nbar\"').search(\n            {'foo\\nbar': 'baz'}), 'baz')\n        self.assertEqual(self.parser.parse(r'\"foo\\bbar\"').search(\n            {'foo\\bbar': 'baz'}), 'baz')\n        self.assertEqual(self.parser.parse(r'\"foo\\fbar\"').search(\n            {'foo\\fbar': 'baz'}), 'baz')\n        self.assertEqual(self.parser.parse(r'\"foo\\rbar\"').search(\n            {'foo\\rbar': 'baz'}), 'baz')\n\n    def test_consecutive_escape_sequences(self):\n        parsed = self.parser.parse(r'\"foo\\\\nbar\"')\n        self.assertEqual(parsed.search({'foo\\\\nbar': 'baz'}), 'baz')\n\n        parsed = self.parser.parse(r'\"foo\\n\\t\\rbar\"')\n        self.assertEqual(parsed.search({'foo\\n\\t\\rbar': 'baz'}), 'baz')\n\n    def test_escape_sequence_at_end_of_string_not_allowed(self):\n        with self.assertRaises(ValueError):\n            self.parser.parse('foobar\\\\')\n\n    def test_wildcard_with_multiselect(self):\n        parsed = self.parser.parse('foo.*.{a: a, b: b}')\n        data = {\n            'foo': {\n                'one': {\n                    'a': {'c': 'CORRECT', 'd': 'other'},\n                    'b': {'c': 'ALSOCORRECT', 'd': 'other'},\n                },\n                'two': {\n                    'a': {'c': 'CORRECT', 'd': 'other'},\n                    'c': {'c': 'WRONG', 'd': 'other'},\n                },\n            }\n        }\n        match = parsed.search(data)\n        self.assertEqual(len(match), 2)\n        self.assertIn('a', match[0])\n        self.assertIn('b', match[0])\n        self.assertIn('a', match[1])\n        self.assertIn('b', match[1])\n\n\nclass TestMergedLists(unittest.TestCase):\n    def setUp(self):\n        self.parser = parser.Parser()\n        self.data = {\n            \"foo\": [\n                [[\"one\", \"two\"], [\"three\", \"four\"]],\n                [[\"five\", \"six\"], [\"seven\", \"eight\"]],\n                [[\"nine\"], [\"ten\"]]\n            ]\n        }\n\n    def test_merge_with_indices(self):\n        parsed = self.parser.parse('foo[][0]')\n        match = parsed.search(self.data)\n        self.assertEqual(match, [\"one\", \"three\", \"five\", \"seven\",\n                                 \"nine\", \"ten\"])\n\n    def test_trailing_merged_operator(self):\n        parsed = self.parser.parse('foo[]')\n        match = parsed.search(self.data)\n        self.assertEqual(\n            match,\n            [[\"one\", \"two\"], [\"three\", \"four\"],\n             [\"five\", \"six\"], [\"seven\", \"eight\"],\n             [\"nine\"], [\"ten\"]])\n\n\nclass TestParserCaching(unittest.TestCase):\n    def test_compile_lots_of_expressions(self):\n        # We have to be careful here because this is an implementation detail\n        # that should be abstracted from the user, but we need to make sure we\n        # exercise the code and that it doesn't blow up.\n        p = parser.Parser()\n        compiled = []\n        compiled2 = []\n        for i in range(parser.Parser._MAX_SIZE + 1):\n            compiled.append(p.parse('foo%s' % i))\n        # Rerun the test and half of these entries should be from the\n        # cache but they should still be equal to compiled.\n        for i in range(parser.Parser._MAX_SIZE + 1):\n            compiled2.append(p.parse('foo%s' % i))\n        self.assertEqual(len(compiled), len(compiled2))\n        self.assertEqual(\n            [expr.parsed for expr in compiled],\n            [expr.parsed for expr in compiled2])\n\n    def test_cache_purge(self):\n        p = parser.Parser()\n        first = p.parse('foo')\n        cached = p.parse('foo')\n        p.purge()\n        second = p.parse('foo')\n        self.assertEqual(first.parsed,\n                         second.parsed)\n        self.assertEqual(first.parsed,\n                         cached.parsed)\n\n    def test_thread_safety_of_cache(self):\n        errors = []\n        expressions = [\n            ''.join(random.choice(string.ascii_letters) for _ in range(3))\n            for _ in range(2000)\n        ]\n        def worker():\n            p = parser.Parser()\n            for expression in expressions:\n                try:\n                    p.parse(expression)\n                except Exception as e:\n                    errors.append(e)\n\n        threads = []\n        for i in range(10):\n            threads.append(threading.Thread(target=worker))\n        for thread in threads:\n            thread.start()\n        for thread in threads:\n            thread.join()\n\n        self.assertEqual(errors, [])\n\n\nclass TestParserAddsExpressionAttribute(unittest.TestCase):\n    def test_expression_available_from_parser(self):\n        p = parser.Parser()\n        parsed = p.parse('foo.bar')\n        self.assertEqual(parsed.expression, 'foo.bar')\n\n\nclass TestParsedResultAddsOptions(unittest.TestCase):\n    def test_can_have_ordered_dict(self):\n        p = parser.Parser()\n        parsed = p.parse('{a: a, b: b, c: c}')\n        options = visitor.Options(dict_cls=OrderedDict)\n        result = parsed.search(\n            {\"c\": \"c\", \"b\": \"b\", \"a\": \"a\"}, options=options)\n        # The order should be 'a', 'b' because we're using an\n        # OrderedDict\n        self.assertEqual(list(result), ['a', 'b', 'c'])\n\n\nclass TestRenderGraphvizFile(unittest.TestCase):\n    def test_dot_file_rendered(self):\n        p = parser.Parser()\n        result = p.parse('foo')\n        dot_contents = result._render_dot_file()\n        self.assertEqual(dot_contents,\n                         'digraph AST {\\nfield1 [label=\"field(foo)\"]\\n}')\n\n    def test_dot_file_subexpr(self):\n        p = parser.Parser()\n        result = p.parse('foo.bar')\n        dot_contents = result._render_dot_file()\n        self.assertEqual(\n            dot_contents,\n            'digraph AST {\\n'\n            'subexpression1 [label=\"subexpression()\"]\\n'\n            '  subexpression1 -> field2\\n'\n            'field2 [label=\"field(foo)\"]\\n'\n            '  subexpression1 -> field3\\n'\n            'field3 [label=\"field(bar)\"]\\n}')\n\n\nif __name__ == '__main__':\n    unittest.main()\n", "tests/test_functions.py": "#!/usr/bin/env python\nfrom tests import unittest\nfrom datetime import datetime, timedelta\nimport json\n\nimport jmespath\nfrom jmespath import exceptions\n\n\nclass TestFunctions(unittest.TestCase):\n\n    def test_can_max_datetimes(self):\n        # This is python specific behavior, but JMESPath does not specify what\n        # you should do with language specific types.  We're going to add the\n        # ability that ``to_string`` will always default to str()'ing values it\n        # doesn't understand.\n        data = [datetime.now(), datetime.now() + timedelta(seconds=1)]\n        result = jmespath.search('max([*].to_string(@))', data)\n        self.assertEqual(json.loads(result), str(data[-1]))\n\n    def test_type_error_messages(self):\n        with self.assertRaises(exceptions.JMESPathTypeError) as e:\n            jmespath.search('length(@)', 2)\n        exception = e.exception\n        # 1. Function name should be in error message\n        self.assertIn('length()', str(exception))\n        # 2. Mention it's an invalid type\n        self.assertIn('invalid type for value: 2', str(exception))\n        # 3. Mention the valid types:\n        self.assertIn(\"expected one of: ['string', 'array', 'object']\",\n                      str(exception))\n        # 4. Mention the actual type.\n        self.assertIn('received: \"number\"', str(exception))\n\n    def test_singular_in_error_message(self):\n        with self.assertRaises(exceptions.ArityError) as e:\n            jmespath.search('length(@, @)', [0, 1])\n        exception = e.exception\n        self.assertEqual(\n            str(exception),\n            'Expected 1 argument for function length(), received 2')\n\n    def test_error_message_is_pluralized(self):\n        with self.assertRaises(exceptions.ArityError) as e:\n            jmespath.search('sort_by(@)', [0, 1])\n        exception = e.exception\n        self.assertEqual(\n            str(exception),\n            'Expected 2 arguments for function sort_by(), received 1')\n\n    def test_variadic_is_pluralized(self):\n        with self.assertRaises(exceptions.VariadictArityError) as e:\n            jmespath.search('not_null()', 'foo')\n        exception = e.exception\n        self.assertEqual(\n            str(exception),\n            'Expected at least 1 argument for function not_null(), received 0')\n", "tests/test_lexer.py": "from tests import unittest\n\nfrom jmespath import lexer\nfrom jmespath.exceptions import LexerError, EmptyExpressionError\n\n\nclass TestRegexLexer(unittest.TestCase):\n\n    def setUp(self):\n        self.lexer = lexer.Lexer()\n\n    def assert_tokens(self, actual, expected):\n        # The expected tokens only need to specify the\n        # type and value.  The line/column numbers are not\n        # checked, and we use assertEqual for the tests\n        # that check those line numbers.\n        stripped = []\n        for item in actual:\n            stripped.append({'type': item['type'], 'value': item['value']})\n        # Every tokenization should end in eof, so we automatically\n        # check that value, strip it off the end, and then\n        # verify the remaining tokens against the expected.\n        # That way the tests don't need to add eof to every\n        # assert_tokens call.\n        self.assertEqual(stripped[-1]['type'], 'eof')\n        stripped.pop()\n        self.assertEqual(stripped, expected)\n\n    def test_empty_string(self):\n        with self.assertRaises(EmptyExpressionError):\n            list(self.lexer.tokenize(''))\n\n    def test_field(self):\n        tokens = list(self.lexer.tokenize('foo'))\n        self.assert_tokens(tokens, [{'type': 'unquoted_identifier',\n                                     'value': 'foo'}])\n\n    def test_number(self):\n        tokens = list(self.lexer.tokenize('24'))\n        self.assert_tokens(tokens, [{'type': 'number',\n                                     'value': 24}])\n\n    def test_negative_number(self):\n        tokens = list(self.lexer.tokenize('-24'))\n        self.assert_tokens(tokens, [{'type': 'number',\n                                     'value': -24}])\n\n    def test_quoted_identifier(self):\n        tokens = list(self.lexer.tokenize('\"foobar\"'))\n        self.assert_tokens(tokens, [{'type': 'quoted_identifier',\n                                     'value': \"foobar\"}])\n\n    def test_json_escaped_value(self):\n        tokens = list(self.lexer.tokenize('\"\\u2713\"'))\n        self.assert_tokens(tokens, [{'type': 'quoted_identifier',\n                                     'value': u\"\\u2713\"}])\n\n    def test_number_expressions(self):\n        tokens = list(self.lexer.tokenize('foo.bar.baz'))\n        self.assert_tokens(tokens, [\n            {'type': 'unquoted_identifier', 'value': 'foo'},\n            {'type': 'dot', 'value': '.'},\n            {'type': 'unquoted_identifier', 'value': 'bar'},\n            {'type': 'dot', 'value': '.'},\n            {'type': 'unquoted_identifier', 'value': 'baz'},\n        ])\n\n    def test_space_separated(self):\n        tokens = list(self.lexer.tokenize('foo.bar[*].baz | a || b'))\n        self.assert_tokens(tokens, [\n            {'type': 'unquoted_identifier', 'value': 'foo'},\n            {'type': 'dot', 'value': '.'},\n            {'type': 'unquoted_identifier', 'value': 'bar'},\n            {'type': 'lbracket', 'value': '['},\n            {'type': 'star', 'value': '*'},\n            {'type': 'rbracket', 'value': ']'},\n            {'type': 'dot', 'value': '.'},\n            {'type': 'unquoted_identifier', 'value': 'baz'},\n            {'type': 'pipe', 'value': '|'},\n            {'type': 'unquoted_identifier', 'value': 'a'},\n            {'type': 'or', 'value': '||'},\n            {'type': 'unquoted_identifier', 'value': 'b'},\n        ])\n\n    def test_literal(self):\n        tokens = list(self.lexer.tokenize('`[0, 1]`'))\n        self.assert_tokens(tokens, [\n            {'type': 'literal', 'value': [0, 1]},\n        ])\n\n    def test_literal_string(self):\n        tokens = list(self.lexer.tokenize('`foobar`'))\n        self.assert_tokens(tokens, [\n            {'type': 'literal', 'value': \"foobar\"},\n        ])\n\n    def test_literal_number(self):\n        tokens = list(self.lexer.tokenize('`2`'))\n        self.assert_tokens(tokens, [\n            {'type': 'literal', 'value': 2},\n        ])\n\n    def test_literal_with_invalid_json(self):\n        with self.assertRaises(LexerError):\n            list(self.lexer.tokenize('`foo\"bar`'))\n\n    def test_literal_with_empty_string(self):\n        tokens = list(self.lexer.tokenize('``'))\n        self.assert_tokens(tokens, [{'type': 'literal', 'value': ''}])\n\n    def test_position_information(self):\n        tokens = list(self.lexer.tokenize('foo'))\n        self.assertEqual(\n            tokens,\n            [{'type': 'unquoted_identifier', 'value': 'foo',\n              'start': 0, 'end': 3},\n              {'type': 'eof', 'value': '', 'start': 3, 'end': 3}]\n        )\n\n    def test_position_multiple_tokens(self):\n        tokens = list(self.lexer.tokenize('foo.bar'))\n        self.assertEqual(\n            tokens,\n            [{'type': 'unquoted_identifier', 'value': 'foo',\n              'start': 0, 'end': 3},\n             {'type': 'dot', 'value': '.',\n              'start': 3, 'end': 4},\n             {'type': 'unquoted_identifier', 'value': 'bar',\n              'start': 4, 'end': 7},\n             {'type': 'eof', 'value': '',\n              'start': 7, 'end': 7},\n             ]\n        )\n\n    def test_adds_quotes_when_invalid_json(self):\n        tokens = list(self.lexer.tokenize('`{{}`'))\n        self.assertEqual(\n            tokens,\n            [{'type': 'literal', 'value': '{{}',\n              'start': 0, 'end': 4},\n             {'type': 'eof', 'value': '',\n              'start': 5, 'end': 5}\n             ]\n        )\n\n    def test_unknown_character(self):\n        with self.assertRaises(LexerError) as e:\n            tokens = list(self.lexer.tokenize('foo[0^]'))\n\n    def test_bad_first_character(self):\n        with self.assertRaises(LexerError):\n            tokens = list(self.lexer.tokenize('^foo[0]'))\n\n    def test_unknown_character_with_identifier(self):\n        with self.assertRaisesRegex(LexerError, \"Unknown token\"):\n            list(self.lexer.tokenize('foo-bar'))\n\n\nif __name__ == '__main__':\n    unittest.main()\n", "tests/__init__.py": "import sys\nfrom jmespath import ast\n\nimport unittest\nimport json\nfrom collections import OrderedDict\n\n\n# Helper method used to create an s-expression\n# of the AST to make unit test assertions easier.\n# You get a nice string diff on assert failures.\ndef as_s_expression(node):\n    parts = []\n    _as_s_expression(node, parts)\n    return ''.join(parts)\n\n\ndef _as_s_expression(node, parts):\n    parts.append(\"(%s\" % (node.__class__.__name__.lower()))\n    if isinstance(node, ast.Field):\n        parts.append(\" %s\" % node.name)\n    elif isinstance(node, ast.FunctionExpression):\n        parts.append(\" %s\" % node.name)\n    elif isinstance(node, ast.KeyValPair):\n        parts.append(\" %s\" % node.key_name)\n    for child in node.children:\n        parts.append(\" \")\n        _as_s_expression(child, parts)\n    parts.append(\")\")\n\n\n", "tests/test_compliance.py": "import os\nfrom pprint import pformat\nfrom tests import OrderedDict\nfrom tests import json\n\nimport pytest\n\nfrom jmespath.visitor import Options\n\n\nTEST_DIR = os.path.dirname(os.path.abspath(__file__))\nCOMPLIANCE_DIR = os.path.join(TEST_DIR, 'compliance')\nLEGACY_DIR = os.path.join(TEST_DIR, 'legacy')\nNOT_SPECIFIED = object()\nOPTIONS = Options(dict_cls=OrderedDict)\n\n\ndef _compliance_tests(requested_test_type):\n    for full_path in _walk_files():\n        if full_path.endswith('.json'):\n            for given, test_type, test_data in load_cases(full_path):\n                t = test_data\n                # Benchmark tests aren't run as part of the normal\n                # test suite, so we only care about 'result' and\n                # 'error' test_types.\n                if test_type == 'result' and test_type == requested_test_type:\n                    yield (given, t['expression'],\n                           t['result'], os.path.basename(full_path))\n                elif test_type == 'error' and test_type == requested_test_type:\n                    yield (given, t['expression'],\n                           t['error'], os.path.basename(full_path))\n\n\ndef _walk_files():\n    # Check for a shortcut when running the tests interactively.\n    # If a JMESPATH_TEST is defined, that file is used as the\n    # only test to run.  Useful when doing feature development.\n    single_file = os.environ.get('JMESPATH_TEST')\n    if single_file is not None:\n        yield os.path.abspath(single_file)\n    else:\n        for root, dirnames, filenames in os.walk(TEST_DIR):\n            for filename in filenames:\n                yield os.path.join(root, filename)\n        for root, dirnames, filenames in os.walk(LEGACY_DIR):\n            for filename in filenames:\n                yield os.path.join(root, filename)\n\n\ndef load_cases(full_path):\n    all_test_data = json.load(open(full_path), object_pairs_hook=OrderedDict)\n    for test_data in all_test_data:\n        given = test_data['given']\n        for case in test_data['cases']:\n            if 'result' in case:\n                test_type = 'result'\n            elif 'error' in case:\n                test_type = 'error'\n            elif 'bench' in case:\n                test_type = 'bench'\n            else:\n                raise RuntimeError(\"Unknown test type: %s\" % json.dumps(case))\n            yield (given, test_type, case)\n\n\n@pytest.mark.parametrize(\n    'given, expression, expected, filename',\n    _compliance_tests('result')\n)\ndef test_expression(given, expression, expected, filename):\n    import jmespath.parser\n    try:\n        parsed = jmespath.compile(expression)\n    except ValueError as e:\n        raise AssertionError(\n            'jmespath expression failed to compile: \"%s\", error: %s\"' %\n            (expression, e))\n    actual = parsed.search(given, options=OPTIONS)\n    expected_repr = json.dumps(expected, indent=4)\n    actual_repr = json.dumps(actual, indent=4)\n    error_msg = (\"\\n\\n  (%s) The expression '%s' was suppose to give:\\n%s\\n\"\n                 \"Instead it matched:\\n%s\\nparsed as:\\n%s\\ngiven:\\n%s\" % (\n                     filename, expression, expected_repr,\n                     actual_repr, pformat(parsed.parsed),\n                     json.dumps(given, indent=4)))\n    error_msg = error_msg.replace(r'\\n', '\\n')\n    assert actual == expected, error_msg\n\n\n@pytest.mark.parametrize(\n    'given, expression, error, filename',\n    _compliance_tests('error')\n)\ndef test_error_expression(given, expression, error, filename):\n    import jmespath.parser\n    if error not in ('syntax', 'invalid-type',\n                     'unknown-function', 'invalid-arity', 'invalid-value'):\n        raise RuntimeError(\"Unknown error type '%s'\" % error)\n    try:\n        parsed = jmespath.compile(expression)\n        parsed.search(given)\n    except ValueError:\n        # Test passes, it raised a parse error as expected.\n        pass\n    except Exception as e:\n        # Failure because an unexpected exception was raised.\n        error_msg = (\"\\n\\n  (%s) The expression '%s' was suppose to be a \"\n                     \"syntax error, but it raised an unexpected error:\\n\\n%s\" % (\n                         filename, expression, e))\n        error_msg = error_msg.replace(r'\\n', '\\n')\n        raise AssertionError(error_msg)\n    else:\n        error_msg = (\"\\n\\n  (%s) The expression '%s' was suppose to be a \"\n                     \"syntax error, but it successfully parsed as:\\n\\n%s\" % (\n                         filename, expression, pformat(parsed.parsed)))\n        error_msg = error_msg.replace(r'\\n', '\\n')\n        raise AssertionError(error_msg)\n", "bin/jp.py": "#!/usr/bin/env python\n\nimport sys\nimport json\nimport argparse\nfrom pprint import pformat\n\nimport jmespath\nfrom jmespath import exceptions\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('expression')\n    parser.add_argument('-f', '--filename',\n                        help=('The filename containing the input data.  '\n                              'If a filename is not given then data is '\n                              'read from stdin.'))\n    parser.add_argument('--ast', action='store_true',\n                        help=('Pretty print the AST, do not search the data.'))\n    args = parser.parse_args()\n    expression = args.expression\n    if args.ast:\n        # Only print the AST\n        expression = jmespath.compile(args.expression)\n        sys.stdout.write(pformat(expression.parsed))\n        sys.stdout.write('\\n')\n        return 0\n    if args.filename:\n        with open(args.filename, 'r') as f:\n            data = json.load(f)\n    else:\n        data = sys.stdin.read()\n        data = json.loads(data)\n    try:\n        sys.stdout.write(json.dumps(\n            jmespath.search(expression, data), indent=4, ensure_ascii=False))\n        sys.stdout.write('\\n')\n    except exceptions.ArityError as e:\n        sys.stderr.write(\"invalid-arity: %s\\n\" % e)\n        return 1\n    except exceptions.JMESPathTypeError as e:\n        sys.stderr.write(\"invalid-type: %s\\n\" % e)\n        return 1\n    except exceptions.UnknownFunctionError as e:\n        sys.stderr.write(\"unknown-function: %s\\n\" % e)\n        return 1\n    except exceptions.ParseError as e:\n        sys.stderr.write(\"syntax-error: %s\\n\" % e)\n        return 1\n\n\nif __name__ == '__main__':\n    sys.exit(main())\n", "perf/perftest.py": "#!/usr/bin/env python\n\"\"\"Generate performance diagnostics.\n\nThe purpose of this script is to generate performance diagnostics for\nvarious jmespath expressions to be able to track the performance\nover time.  The test files are data driven similar to the\ncompliance tests.\n\"\"\"\nimport argparse\nimport time\nimport os\nimport json\nimport sys\nimport timeit\n\n_clock = timeit.default_timer\n\n\nfrom jmespath.parser import Parser\nfrom jmespath.lexer import Lexer\n\n\nBENCHMARK_FILE = os.path.join(\n    os.path.dirname(os.path.dirname(os.path.abspath(__file__))),\n    'tests',\n    'compliance',\n    'benchmarks.json')\nAPPROX_RUN_TIME = 0.5\n\n\ndef run_tests(tests):\n    times = []\n    for test in tests:\n        given = test['given']\n        expression = test['expression']\n        result = test['result']\n        should_search = test['bench_type'] == 'full'\n        lex_time = _lex_time(expression)\n        parse_time = _parse_time(expression)\n        if should_search:\n            search_time = _search_time(expression, given)\n            combined_time = _combined_time(expression, given, result)\n        else:\n            search_time = 0\n            combined_time = 0\n        sys.stdout.write(\n            \"lex_time: %10.5fus, parse_time: %10.5fus, search_time: %10.5fus \"\n            \"combined_time: %10.5fus \" % (1000000 * lex_time,\n                                          1000000 * parse_time,\n                                          1000000 * search_time,\n                                          1000000 * combined_time))\n        sys.stdout.write(\"name: %s\\n\" % test['name'])\n\n\ndef _lex_time(expression, clock=_clock):\n    lex = Lexer()\n    duration = 0\n    i = 0\n    while True:\n        i += 1\n        start = clock()\n        list(lex.tokenize(expression))\n        end = clock()\n        total = end - start\n        duration += total\n        if duration >= APPROX_RUN_TIME:\n            break\n    return duration / i\n\n\ndef _search_time(expression, given, clock=_clock):\n    p = Parser()\n    parsed = p.parse(expression)\n    duration =  0\n    i = 0\n    while True:\n        i += 1\n        start = clock()\n        parsed.search(given)\n        end = clock()\n        total = end - start\n        duration += total\n        if duration >= APPROX_RUN_TIME:\n            break\n    return duration / i\n\n\ndef _parse_time(expression, clock=_clock):\n    best = float('inf')\n    p = Parser()\n    duration = 0\n    i = 0\n    while True:\n        i += 1\n        p.purge()\n        start = clock()\n        p.parse(expression)\n        end = clock()\n        total = end - start\n        duration += total\n        if duration >= APPROX_RUN_TIME:\n            break\n    return duration / i\n\n\ndef _combined_time(expression, given, result, clock=_clock):\n    best = float('inf')\n    p = Parser()\n    duration = 0\n    i = 0\n    while True:\n        i += 1\n        p.purge()\n        start = clock()\n        r = p.parse(expression).search(given)\n        end = clock()\n        total = end - start\n        if r != result:\n            raise RuntimeError(\"Unexpected result, received: %s, \"\n                               \"expected: %s\" % (r, result))\n        duration += total\n        if duration >= APPROX_RUN_TIME:\n            break\n    return duration / i\n\n\ndef load_tests(filename):\n    loaded = []\n    with open(filename) as f:\n        data = json.load(f)\n    if isinstance(data, list):\n        for i, d in enumerate(data):\n            _add_cases(d, loaded, '%s-%s' % (filename, i))\n    else:\n        _add_cases(data, loaded, filename)\n    return loaded\n\n\ndef _add_cases(data, loaded, filename):\n    for case in data['cases']:\n        current = {\n            'given': data['given'],\n            'name': case.get('comment', case['expression']),\n            'expression': case['expression'],\n            'result': case.get('result'),\n            'bench_type': case['bench'],\n        }\n        loaded.append(current)\n    return loaded\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument('-f', '--filename', default=BENCHMARK_FILE)\n    args = parser.parse_args()\n    collected_tests = []\n    collected_tests.extend(load_tests(args.filename))\n    run_tests(collected_tests)\n\n\nif __name__ == '__main__':\n    main()\n"}