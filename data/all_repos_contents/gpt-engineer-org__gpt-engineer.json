{"projects/example-improve/model.py": "import random\n\nfrom dataclasses import dataclass\n\n\n@dataclass\nclass Point:\n    x: int\n    y: int\n\n\nclass Game:\n    def __init__(self):\n        self.snake = [Point(5, 5)]\n        self.food = self.generate_food()\n        self.is_running = True\n\n    def generate_food(self):\n        return Point(random.randint(0, 10), random.randint(0, 10))\n\n    def update(self):\n        # Move the snake\n        self.snake.move()\n\n        # Check for collision with food\n        if self.snake.head == self.food:\n            self.snake.grow()\n            self.food = self.generate_food()\n\n        # Check for collision with boundaries\n        if not (0 <= self.snake.head.x < 10 and 0 <= self.snake.head.y < 10):\n            self.is_running = False\n", "projects/example-improve/view.py": "from model import Point\n\n\nclass View:\n    def __init__(self, game):\n        self.game = game\n\n    def render(self):\n        # Print the game state\n        for y in range(10):\n            for x in range(10):\n                if Point(x, y) in self.game.snake:\n                    print(\"S\", end=\"\")\n                elif Point(x, y) == self.game.food:\n                    print(\"F\", end=\"\")\n                else:\n                    print(\".\", end=\"\")\n            print()\n        print()\n", "projects/example-improve/controller.py": "import keyboard\n\n\nclass Controller:\n    def __init__(self, game, view):\n        self.game = game\n        self.view = view\n\n    def handle_input(self):\n        if keyboard.is_pressed(\"up\") and not hasattr(self, \"last_key_pressed\"):\n            self.game.move(\"down\")\n            self.last_key_pressed = \"up\"\n        elif hasattr(self, \"last_key_pressed\") and self.last_key_pressed == \"up\":\n            self.game.move(\"right\")\n            del self.last_key_pressed\n        elif keyboard.is_pressed(\"down\"):\n            self.game.move(\"up\")\n        elif keyboard.is_pressed(\"left\"):\n            self.game.move(\"right\")\n        elif keyboard.is_pressed(\"right\"):\n            self.game.move(\"left\")\n", "projects/example-improve/main.py": "from controller import Controller\nfrom model import Game\nfrom view import View\n\n\ndef main():\n    game = Game()\n    view = View(game)\n    controller = Controller(game, view)\n\n    while game.is_running:\n        controller.handle_input()\n        game.update()\n        view.render()\n\n\nif __name__ == \"__main__\":\n    main()\n", "gpt_engineer/__init__.py": "# Adding convenience imports to the package\n\n# from gpt_engineer.tools import code_vector_repository\n# from gpt_engineer.core.default import on_disk_repository\n", "gpt_engineer/tools/supported_languages.py": "\"\"\"\nThis module defines the supported programming languages for document chunking.\n\nVariables:\n    SUPPORTED_LANGUAGES (list): A list of dictionaries defining supported languages.\n\"\"\"\n\nSUPPORTED_LANGUAGES = [\n    {\"name\": \"Python\", \"extensions\": [\".py\"], \"tree_sitter_name\": \"python\"},\n    {\n        \"name\": \"JavaScript\",\n        \"extensions\": [\".js\", \".mjs\"],\n        \"tree_sitter_name\": \"javascript\",\n    },\n    {\"name\": \"HTML\", \"extensions\": [\".html\", \".htm\"], \"tree_sitter_name\": \"html\"},\n    {\"name\": \"CSS\", \"extensions\": [\".css\"], \"tree_sitter_name\": \"css\"},\n    {\"name\": \"Java\", \"extensions\": [\".java\"], \"tree_sitter_name\": \"java\"},\n    {\"name\": \"C#\", \"extensions\": [\".cs\"], \"tree_sitter_name\": \"c_sharp\"},\n    {\n        \"name\": \"TypeScript\",\n        \"extensions\": [\".ts\", \".tsx\"],\n        \"tree_sitter_name\": \"typescript\",\n    },\n    {\"name\": \"Ruby\", \"extensions\": [\".rb\", \".erb\"], \"tree_sitter_name\": \"ruby\"},\n    {\n        \"name\": \"PHP\",\n        \"extensions\": [\n            \".php\",\n            \".phtml\",\n            \".php3\",\n            \".php4\",\n            \".php5\",\n            \".php7\",\n            \".phps\",\n            \".php-s\",\n            \".pht\",\n            \".phar\",\n        ],\n        \"tree_sitter_name\": \"php\",\n    },\n    {\"name\": \"Go\", \"extensions\": [\".go\"], \"tree_sitter_name\": \"go\"},\n    {\"name\": \"Kotlin\", \"extensions\": [\".kt\", \".kts\"], \"tree_sitter_name\": \"kotlin\"},\n    {\"name\": \"Rust\", \"extensions\": [\".rs\"], \"tree_sitter_name\": \"rust\"},\n    {\n        \"name\": \"C++\",\n        \"extensions\": [\".cpp\", \".cc\", \".cxx\", \".h\", \".hpp\", \".hxx\"],\n        \"tree_sitter_name\": \"cpp\",\n    },\n    {\"name\": \"C\", \"extensions\": [\".c\", \".h\"], \"tree_sitter_name\": \"c\"}\n    # ---- the following are not supported by the current code chunker implementation ----\n    # {\n    #     \"name\": \"Swift\",\n    #     \"extensions\": [\".swift\"],\n    #     \"tree_sitter_name\": \"swift\"\n    # },\n]\n", "gpt_engineer/tools/custom_steps.py": "from platform import platform\nfrom sys import version_info\nfrom typing import List, Union\n\nfrom langchain.schema import AIMessage, HumanMessage, SystemMessage\n\nfrom gpt_engineer.core.ai import AI\nfrom gpt_engineer.core.base_execution_env import BaseExecutionEnv\nfrom gpt_engineer.core.base_memory import BaseMemory\nfrom gpt_engineer.core.chat_to_files import chat_to_files_dict\nfrom gpt_engineer.core.default.paths import CODE_GEN_LOG_FILE, ENTRYPOINT_FILE\nfrom gpt_engineer.core.default.steps import curr_fn, improve_fn, setup_sys_prompt\nfrom gpt_engineer.core.files_dict import FilesDict\nfrom gpt_engineer.core.preprompts_holder import PrepromptsHolder\nfrom gpt_engineer.core.prompt import Prompt\n\n# Type hint for chat messages\nMessage = Union[AIMessage, HumanMessage, SystemMessage]\nMAX_SELF_HEAL_ATTEMPTS = 10\n\n\ndef get_platform_info() -> str:\n    \"\"\"\n    Returns a string containing the OS and Python version information.\n\n    This function is used for self-healing by providing information about the current\n    operating system and Python version. It assumes that the Python version in the\n    virtual environment is the one being used.\n\n    Returns:\n        str: A string containing the OS and Python version information.\n    \"\"\"\n\n    v = version_info\n    a = f\"Python Version: {v.major}.{v.minor}.{v.micro}\"\n    b = f\"\\nOS: {platform()}\\n\"\n    return a + b\n\n\ndef self_heal(\n    ai: AI,\n    execution_env: BaseExecutionEnv,\n    files_dict: FilesDict,\n    prompt: Prompt = None,\n    preprompts_holder: PrepromptsHolder = None,\n    memory: BaseMemory = None,\n) -> FilesDict:\n    \"\"\"\n    Attempts to execute the code from the entrypoint and if it fails, sends the error output back to the AI with instructions to fix.\n\n    Parameters\n    ----------\n    ai : AI\n        An instance of the AI model.\n    execution_env : BaseExecutionEnv\n        The execution environment where the code is run.\n    files_dict : FilesDict\n        A dictionary of file names to their contents.\n    preprompts_holder : PrepromptsHolder, optional\n        A holder for preprompt messages.\n\n    Returns\n    -------\n    FilesDict\n        The updated files dictionary after self-healing attempts.\n\n    Raises\n    ------\n    FileNotFoundError\n        If the required entrypoint file does not exist in the code.\n    AssertionError\n        If the preprompts_holder is None.\n\n    Notes\n    -----\n    This code will make `MAX_SELF_HEAL_ATTEMPTS` to try and fix the code\n    before giving up.\n    This makes the assuption that the previous step was `gen_entrypoint`,\n    this code could work with `simple_gen`, or `gen_clarified_code` as well.\n    \"\"\"\n\n    # step 1. execute the entrypoint\n    # log_path = dbs.workspace.path / \"log.txt\"\n    if ENTRYPOINT_FILE not in files_dict:\n        raise FileNotFoundError(\n            \"The required entrypoint \"\n            + ENTRYPOINT_FILE\n            + \" does not exist in the code.\"\n        )\n\n    attempts = 0\n    if preprompts_holder is None:\n        raise AssertionError(\"Prepromptsholder required for self-heal\")\n    while attempts < MAX_SELF_HEAL_ATTEMPTS:\n        attempts += 1\n        timed_out = False\n\n        # Start the process\n        execution_env.upload(files_dict)\n        p = execution_env.popen(files_dict[ENTRYPOINT_FILE])\n\n        # Wait for the process to complete and get output\n        stdout_full, stderr_full = p.communicate()\n\n        if (p.returncode != 0 and p.returncode != 2) and not timed_out:\n            print(\"run.sh failed.  The log is:\")\n            print(stdout_full.decode(\"utf-8\"))\n            print(stderr_full.decode(\"utf-8\"))\n\n            new_prompt = Prompt(\n                f\"A program with this specification was requested:\\n{prompt}\\n, but running it produced the following output:\\n{stdout_full}\\n and the following errors:\\n{stderr_full}. Please change it so that it fulfills the requirements.\"\n            )\n            files_dict = improve_fn(\n                ai, new_prompt, files_dict, memory, preprompts_holder\n            )\n        else:\n            break\n    return files_dict\n\n\ndef clarified_gen(\n    ai: AI, prompt: Prompt, memory: BaseMemory, preprompts_holder: PrepromptsHolder\n) -> FilesDict:\n    \"\"\"\n    Generates code based on clarifications obtained from the user and saves it to a specified workspace.\n\n    Parameters\n    ----------\n    ai : AI\n        An instance of the AI model, responsible for processing and generating the code.\n    prompt : str\n        The user's clarification prompt.\n    memory : BaseMemory\n        The memory instance where the generated code log is saved.\n    preprompts_holder : PrepromptsHolder\n        A holder for preprompt messages.\n\n    Returns\n    -------\n    FilesDict\n        A dictionary of file names to their contents generated by the AI.\n    \"\"\"\n\n    preprompts = preprompts_holder.get_preprompts()\n    messages: List[Message] = [SystemMessage(content=preprompts[\"clarify\"])]\n    user_input = prompt.text  # clarify does not work with vision right now\n    while True:\n        messages = ai.next(messages, user_input, step_name=curr_fn())\n        msg = messages[-1].content.strip()\n\n        if \"nothing to clarify\" in msg.lower():\n            break\n\n        if msg.lower().startswith(\"no\"):\n            print(\"Nothing to clarify.\")\n            break\n\n        print('(answer in text, or \"c\" to move on)\\n')\n        user_input = input(\"\")\n        print()\n\n        if not user_input or user_input == \"c\":\n            print(\"(letting gpt-engineer make its own assumptions)\")\n            print()\n            messages = ai.next(\n                messages,\n                \"Make your own assumptions and state them explicitly before starting\",\n                step_name=curr_fn(),\n            )\n            print()\n\n        user_input += \"\"\"\n            \\n\\n\n            Is anything else unclear? If yes, ask another question.\\n\n            Otherwise state: \"Nothing to clarify\"\n            \"\"\"\n\n    print()\n\n    messages = [\n        SystemMessage(content=setup_sys_prompt(preprompts)),\n    ] + messages[\n        1:\n    ]  # skip the first clarify message, which was the original clarify priming prompt\n    messages = ai.next(\n        messages,\n        preprompts[\"generate\"].replace(\"FILE_FORMAT\", preprompts[\"file_format\"]),\n        step_name=curr_fn(),\n    )\n    print()\n    chat = messages[-1].content.strip()\n    memory.log(CODE_GEN_LOG_FILE, \"\\n\\n\".join(x.pretty_repr() for x in messages))\n    files_dict = chat_to_files_dict(chat)\n    return files_dict\n\n\ndef lite_gen(\n    ai: AI, prompt: Prompt, memory: BaseMemory, preprompts_holder: PrepromptsHolder\n) -> FilesDict:\n    \"\"\"\n    Executes the AI model using the main prompt and saves the generated results to the specified workspace.\n\n    Parameters\n    ----------\n    ai : AI\n        An instance of the AI model.\n    prompt : str\n        The main prompt to feed to the AI model.\n    memory : BaseMemory\n        The memory instance where the generated code log is saved.\n    preprompts_holder : PrepromptsHolder\n        A holder for preprompt messages.\n\n    Returns\n    -------\n    FilesDict\n        A dictionary of file names to their contents generated by the AI.\n\n    Notes\n    -----\n    The function assumes the `ai.start` method and the `to_files` utility to be correctly\n    set up and functional. Ensure these prerequisites before invoking `lite_gen`.\n    \"\"\"\n\n    preprompts = preprompts_holder.get_preprompts()\n    messages = ai.start(\n        prompt.to_langchain_content(), preprompts[\"file_format\"], step_name=curr_fn()\n    )\n    chat = messages[-1].content.strip()\n    memory.log(CODE_GEN_LOG_FILE, \"\\n\\n\".join(x.pretty_repr() for x in messages))\n    files_dict = chat_to_files_dict(chat)\n    return files_dict\n", "gpt_engineer/tools/__init__.py": "", "gpt_engineer/applications/__init__.py": "", "gpt_engineer/applications/cli/collect.py": "\"\"\"\nModule `collect` - Data Handling and RudderStack Integration\n\nThis module provides functionalities to handle and send learning data to RudderStack\nfor the purpose of analysis and to improve the gpt-engineer system. The data is sent\nonly when the user gives consent to share.\n\nFunctions:\n    send_learning(learning): Sends learning data to RudderStack.\n    collect_learnings(prompt, model, temperature, config, memory, review): Processes and sends learning data.\n    collect_and_send_human_review(prompt, model, temperature, config, memory): Collects human feedback and sends it.\n\nDependencies:\n    hashlib: For generating SHA-256 hash.\n    typing: For type annotations.\n    gpt_engineer.core: Core functionalities of gpt-engineer.\n    gpt_engineer.cli.learning: Handles the extraction of learning data.\n\nNotes:\n    Data sent to RudderStack is not shared with third parties and is used solely to\n    improve gpt-engineer and allow it to handle a broader range of use cases.\n    Consent logic is in gpt_engineer/learning.py.\n\"\"\"\n\nfrom typing import Tuple\n\nfrom gpt_engineer.applications.cli.learning import (\n    Learning,\n    Review,\n    extract_learning,\n    human_review_input,\n)\nfrom gpt_engineer.core.default.disk_memory import DiskMemory\nfrom gpt_engineer.core.prompt import Prompt\n\n\ndef send_learning(learning: Learning):\n    \"\"\"\n    Send the learning data to RudderStack for analysis.\n\n    Parameters\n    ----------\n    learning : Learning\n        An instance of the Learning class containing the data to be sent.\n\n    Notes\n    -----\n    This function is only called if consent is given to share data.\n    Data is not shared to a third party. It is used with the sole purpose of\n    improving gpt-engineer, and letting it handle more use cases.\n    Consent logic is in gpt_engineer/learning.py.\n    \"\"\"\n    import rudderstack.analytics as rudder_analytics\n\n    rudder_analytics.write_key = \"2Re4kqwL61GDp7S8ewe6K5dbogG\"\n    rudder_analytics.dataPlaneUrl = \"https://gptengineerezm.dataplane.rudderstack.com\"\n\n    rudder_analytics.track(\n        user_id=learning.session,\n        event=\"learning\",\n        properties=learning.to_dict(),  # type: ignore\n    )\n\n\ndef collect_learnings(\n    prompt: Prompt,\n    model: str,\n    temperature: float,\n    config: any,\n    memory: DiskMemory,\n    review: Review,\n):\n    \"\"\"\n    Collect the learning data and send it to RudderStack for analysis.\n\n    Parameters\n    ----------\n    prompt : str\n        The initial prompt or question that was provided to the model.\n    model : str\n        The name of the model used for generating the response.\n    temperature : float\n        The temperature setting used in the model's response generation.\n    config : any\n        Configuration parameters used for the learning session.\n    memory : DiskMemory\n        An instance of DiskMemory for storing and retrieving data.\n    review : Review\n        An instance of Review containing human feedback on the model's response.\n\n    Notes\n    -----\n    This function attempts to send the learning data to RudderStack. If the data size exceeds\n    the maximum allowed size, it trims the data and retries sending it.\n    \"\"\"\n    learnings = extract_learning(prompt, model, temperature, config, memory, review)\n    try:\n        send_learning(learnings)\n    except RuntimeError:\n        # try to remove some parts of learning that might be too big\n        # rudderstack max event size is 32kb\n        max_size = 32 << 10  # 32KB in bytes\n        current_size = len(learnings.to_json().encode(\"utf-8\"))  # get size in bytes\n\n        overflow = current_size - max_size\n\n        # Add some extra characters for the \"[REMOVED...]\" string and for safety margin\n        remove_length = overflow + len(f\"[REMOVED {overflow} CHARACTERS]\") + 100\n\n        learnings.logs = (\n            learnings.logs[:-remove_length]\n            + f\"\\n\\n[REMOVED {remove_length} CHARACTERS]\"\n        )\n\n        print(\n            \"WARNING: learning too big, removing some parts. \"\n            \"Please report if this results in a crash.\"\n        )\n        try:\n            send_learning(learnings)\n        except RuntimeError:\n            print(\n                \"Sending learnings crashed despite truncation. Progressing without saving learnings.\"\n            )\n\n\n# def steps_file_hash():\n#     \"\"\"\n#     Compute the SHA-256 hash of the steps file.\n#\n#     Returns\n#     -------\n#     str\n#         The SHA-256 hash of the steps file.\n#     \"\"\"\n#     with open(steps.__file__, \"r\") as f:\n#         content = f.read()\n#         return hashlib.sha256(content.encode(\"utf-8\")).hexdigest()\n\n\ndef collect_and_send_human_review(\n    prompt: Prompt,\n    model: str,\n    temperature: float,\n    config: Tuple[str, ...],\n    memory: DiskMemory,\n):\n    \"\"\"\n    Collects human feedback on the code and sends it for analysis.\n\n    Parameters\n    ----------\n    prompt : str\n        The initial prompt or question that was provided to the model.\n    model : str\n        The name of the model used for generating the response.\n    temperature : float\n        The temperature setting used in the model's response generation.\n    config : Tuple[str, ...]\n        Configuration parameters used for the learning session.\n    memory : DiskMemory\n        An instance of DiskMemory for storing and retrieving data.\n\n    Returns\n    -------\n    None\n\n    Notes\n    -----\n    This function prompts the user for a review of the generated or improved code using the\n    `human_review_input` function. If a valid review is provided, it's serialized to JSON format\n    and stored within the database's memory under the \"review\" key.\n    \"\"\"\n\n    review = human_review_input()\n    if review:\n        collect_learnings(prompt, model, temperature, config, memory, review)\n", "gpt_engineer/applications/cli/file_selector.py": "\"\"\"\nfile_selector.py\n\nThis module offers interactive file selection for projects. Leveraging a terminal-based,\ntree-structured display, users can navigate and select files for editing or processing.\nIt integrates with system editors for direct file modification and supports saving\nselections for later use. Designed for efficient workflow enhancement in file-intensive\nenvironments, it offers customizable file filtering and seamless editor integration.\n\nKey Components:\n- FileSelector: Manages file selection and interaction.\n- DisplayablePath: Provides a structured view of file paths.\n\nUsage:\nTypically used in project setup or management phases for selecting specific files.\nIt operates within the GPT-Engineer environment, relying on core functionalities for\nfile handling and persistence.\n\"\"\"\n\nimport fnmatch\nimport os\nimport subprocess\n\nfrom pathlib import Path\nfrom typing import Any, Dict, Generator, List, Union\n\nimport toml\n\nfrom gpt_engineer.core.default.disk_memory import DiskMemory\nfrom gpt_engineer.core.default.paths import metadata_path\nfrom gpt_engineer.core.files_dict import FilesDict\nfrom gpt_engineer.core.git import filter_by_gitignore, is_git_repo\n\n\nclass FileSelector:\n    \"\"\"\n    Manages file selection and interaction within a project directory.\n\n    This class provides methods to interactively select files from the terminal,\n    save selections for later use, and integrate with system editors for direct\n    file modification.\n\n    Attributes\n    ----------\n    IGNORE_FOLDERS : set\n        A set of directory names to ignore during file selection.\n    FILE_LIST_NAME : str\n        The name of the file that stores the selected files list.\n    COMMENT : str\n        The comment string to be added to the top of the file selection list.\n    \"\"\"\n\n    IGNORE_FOLDERS = {\"site-packages\", \"node_modules\", \"venv\", \"__pycache__\"}\n    FILE_LIST_NAME = \"file_selection.toml\"\n    COMMENT = (\n        \"# Remove '#' to select a file or turn off linting.\\n\\n\"\n        \"# Linting with BLACK (Python) enhances code suggestions from LLMs. \"\n        \"To disable linting, uncomment the relevant option in the linting settings.\\n\\n\"\n        \"# gpt-engineer can only read selected files. \"\n        \"Including irrelevant files will degrade performance, \"\n        \"cost additional tokens and potentially overflow token limit.\\n\\n\"\n    )\n    LINTING_STRING = '[linting]\\n# \"linting\" = \"off\"\\n\\n'\n    is_linting = True\n\n    def __init__(self, project_path: Union[str, Path]):\n        \"\"\"\n        Initializes the FileSelector with a given project path.\n\n        Parameters\n        ----------\n        project_path : Union[str, Path]\n            The path to the project directory where file selection is to be performed.\n        \"\"\"\n        self.project_path = project_path\n        self.metadata_db = DiskMemory(metadata_path(self.project_path))\n        self.toml_path = self.metadata_db.path / self.FILE_LIST_NAME\n\n    def ask_for_files(self) -> tuple[FilesDict, bool]:\n        \"\"\"\n        Prompts the user to select files for context improvement.\n\n        This method supports selection from the terminal or using a previously saved list.\n        In test mode, it retrieves files from a predefined TOML configuration.\n\n        Returns\n        -------\n        FilesDict\n            A dictionary with file paths as keys and file contents as values.\n        \"\"\"\n\n        if os.getenv(\"GPTE_TEST_MODE\"):\n            # In test mode, retrieve files from a predefined TOML configuration\n            assert self.FILE_LIST_NAME in self.metadata_db\n            selected_files = self.get_files_from_toml(self.project_path, self.toml_path)\n        else:\n            # Otherwise, use the editor file selector for interactive selection\n            if self.FILE_LIST_NAME in self.metadata_db:\n                print(\n                    f\"File list detected at {self.toml_path}. Edit or delete it if you want to select new files.\"\n                )\n                selected_files = self.editor_file_selector(self.project_path, False)\n            else:\n                selected_files = self.editor_file_selector(self.project_path, True)\n\n        content_dict = {}\n        for file_path in selected_files:\n            # selected files contains paths that are relative to the project path\n            try:\n                # to open the file we need the path from the cwd\n                with open(\n                    Path(self.project_path) / file_path, \"r\", encoding=\"utf-8\"\n                ) as content:\n                    content_dict[str(file_path)] = content.read()\n            except FileNotFoundError:\n                print(f\"Warning: File not found {file_path}\")\n            except UnicodeDecodeError:\n                print(f\"Warning: File not UTF-8 encoded {file_path}, skipping\")\n\n        return FilesDict(content_dict), self.is_linting\n\n    def editor_file_selector(\n        self, input_path: Union[str, Path], init: bool = True\n    ) -> List[str]:\n        \"\"\"\n        Provides an interactive file selection interface using a .toml file.\n\n        Parameters\n        ----------\n        input_path : Union[str, Path]\n            The path where file selection is to be performed.\n        init : bool, optional\n            Indicates whether to initialize the .toml file with the file tree.\n\n        Returns\n        -------\n        List[str]\n            A list of strings representing the paths of selected files.\n        \"\"\"\n\n        root_path = Path(input_path)\n        tree_dict = {}\n        toml_file = DiskMemory(metadata_path(input_path)).path / \"file_selection.toml\"\n        # Define the toml file path\n\n        # Initialize .toml file with file tree if in initial state\n        if init:\n            tree_dict = {x: \"selected\" for x in self.get_current_files(root_path)}\n\n            s = toml.dumps({\"files\": tree_dict})\n\n            # add comments on all lines that match = \"selected\"\n            s = \"\\n\".join(\n                [\n                    \"# \" + line if line.endswith(' = \"selected\"') else line\n                    for line in s.split(\"\\n\")\n                ]\n            )\n            # Write to the toml file\n            with open(toml_file, \"w\") as f:\n                f.write(self.COMMENT)\n                f.write(self.LINTING_STRING)\n                f.write(s)\n\n        else:\n            # Load existing files from the .toml configuration\n            all_files = self.get_current_files(root_path)\n            s = toml.dumps({\"files\": {x: \"selected\" for x in all_files}})\n\n            # get linting status from the toml file\n            with open(toml_file, \"r\") as file:\n                linting_status = toml.load(file)\n            if (\n                \"linting\" in linting_status\n                and linting_status[\"linting\"].get(\"linting\", \"\").lower() == \"off\"\n            ):\n                self.is_linting = False\n                self.LINTING_STRING = '[linting]\\n\"linting\" = \"off\"\\n\\n'\n                print(\"\\nLinting is disabled\")\n\n            with open(toml_file, \"r\") as file:\n                selected_files = toml.load(file)\n\n            lines = s.split(\"\\n\")\n            s = \"\\n\".join(\n                lines[:1]\n                + [\n                    line\n                    if line.split(\" = \")[0].strip('\"') in selected_files[\"files\"]\n                    else \"# \" + line\n                    for line in lines[1:]\n                ]\n            )\n\n            # Write the merged list back to the .toml for user review and modification\n            with open(toml_file, \"w\") as file:\n                file.write(self.COMMENT)  # Ensure to write the comment\n                file.write(self.LINTING_STRING)\n                file.write(s)\n\n        print(\n            \"Please select and deselect (add # in front) files, save it, and close it to continue...\"\n        )\n        self.open_with_default_editor(\n            toml_file\n        )  # Open the .toml file in the default editor for user modification\n        return self.get_files_from_toml(\n            input_path, toml_file\n        )  # Return the list of selected files after user edits\n\n    def open_with_default_editor(self, file_path: Union[str, Path]):\n        \"\"\"\n        Opens a file with the system's default text editor.\n\n        Parameters\n        ----------\n        file_path : Union[str, Path]\n            The path to the file to be opened in the text editor.\n        \"\"\"\n\n        editors = [\n            \"gedit\",\n            \"notepad\",\n            \"nvim\",\n            \"write\",\n            \"nano\",\n            \"vim\",\n            \"emacs\",\n        ]  # Putting the beginner-friendly text editor forward\n        chosen_editor = os.environ.get(\"EDITOR\")\n\n        # Try the preferred editor first, then fallback to common editors\n        if chosen_editor:\n            try:\n                subprocess.run([chosen_editor, file_path])\n                return\n            except Exception:\n                pass\n\n        for editor in editors:\n            try:\n                subprocess.run([editor, file_path])\n                return\n            except Exception:\n                continue\n        print(\"No suitable text editor found. Please edit the file manually.\")\n\n    def is_utf8(self, file_path: Union[str, Path]) -> bool:\n        \"\"\"\n        Checks if the file at the given path is UTF-8 encoded.\n\n        Parameters\n        ----------\n        file_path : Union[str, Path]\n            The path to the file to be checked.\n\n        Returns\n        -------\n        bool\n            True if the file is UTF-8 encoded, False otherwise.\n        \"\"\"\n\n        try:\n            with open(file_path, \"rb\") as file:\n                file.read().decode(\"utf-8\")\n                return True\n        except UnicodeDecodeError:\n            return False\n\n    def get_files_from_toml(\n        self, input_path: Union[str, Path], toml_file: Union[str, Path]\n    ) -> List[str]:\n        \"\"\"\n        Retrieves a list of selected files from a .toml configuration file.\n\n        Parameters\n        ----------\n        input_path : Union[str, Path]\n            The path where file selection was performed.\n        toml_file : Union[str, Path]\n            The path to the .toml file containing the file selection.\n\n        Returns\n        -------\n        List[str]\n            A list of strings representing the paths of selected files.\n\n        Raises\n        ------\n        Exception\n            If no files are selected in the .toml file.\n        \"\"\"\n        selected_files = []\n        edited_tree = toml.load(toml_file)  # Load the edited .toml file\n\n        # check if users have disabled linting or not\n        if (\n            \"linting\" in edited_tree\n            and edited_tree[\"linting\"].get(\"linting\", \"\").lower() == \"off\"\n        ):\n            self.is_linting = False\n            print(\"\\nLinting is disabled\")\n        else:\n            self.is_linting = True\n\n        # Iterate through the files in the .toml and append selected files to the list\n        for file, _ in edited_tree[\"files\"].items():\n            selected_files.append(file)\n\n        # Ensure that at least one file is selected, or raise an exception\n        if not selected_files:\n            raise Exception(\n                \"No files were selected. Please select at least one file to proceed.\"\n            )\n\n        print(f\"\\nYou have selected the following files:\\n{input_path}\")\n\n        project_path = Path(input_path).resolve()\n        selected_paths = set(\n            project_path.joinpath(file).resolve(strict=False) for file in selected_files\n        )\n\n        for displayable_path in DisplayablePath.make_tree(project_path):\n            if displayable_path.path in selected_paths:\n                p = displayable_path\n                while p.parent and p.parent.path not in selected_paths:\n                    selected_paths.add(p.parent.path)\n                    p = p.parent\n\n        try:\n            for displayable_path in DisplayablePath.make_tree(project_path):\n                if displayable_path.path in selected_paths:\n                    print(displayable_path.displayable())\n\n        except FileNotFoundError:\n            print(\"Specified path does not exist: \", project_path)\n        except Exception as e:\n            print(\"An error occurred while trying to display the file tree:\", e)\n\n        print(\"\\n\")\n        return selected_files\n\n    def merge_file_lists(\n        self, existing_files: Dict[str, Any], new_files: Dict[str, Any]\n    ) -> Dict[str, Any]:\n        \"\"\"\n        Merges two lists of files, preserving the selection status.\n\n        Parameters\n        ----------\n        existing_files : Dict[str, Any]\n            The dictionary of existing files with their properties.\n        new_files : Dict[str, Any]\n            The dictionary of new files with their properties.\n\n        Returns\n        -------\n        Dict[str, Any]\n            The updated dictionary of files after merging.\n        \"\"\"\n        # Update the existing files with any new files or changes\n        for file, properties in new_files.items():\n            if file not in existing_files:\n                existing_files[file] = properties  # Add new files as unselected\n            # If you want to update other properties of existing files, you can do so here\n\n        return existing_files\n\n    def should_filter_file(self, file_path: Path, filters: List[str]) -> bool:\n        \"\"\"\n        Determines if a file should be ignored based on .gitignore rules.\n        \"\"\"\n        for f in filters:\n            if fnmatch.fnmatchcase(str(file_path), f):\n                return True\n        return False\n\n    def get_current_files(self, project_path: Union[str, Path]) -> List[str]:\n        \"\"\"\n        Generates a list of all files in the project directory. Will use .gitignore files if project_path is a git repository.\n\n        Parameters\n        ----------\n        project_path : Union[str, Path]\n            The path to the project directory.\n\n        Returns\n        -------\n        List[str]\n            A list of strings representing the relative paths of all files in the project directory.\n        \"\"\"\n        all_files = []\n        project_path = Path(\n            project_path\n        ).resolve()  # Ensure path is absolute and resolved\n\n        file_list = project_path.glob(\"**/*\")\n\n        for path in file_list:  # Recursively list all files\n            if path.is_file():\n                relpath = path.relative_to(project_path)\n                parts = relpath.parts\n                if any(part.startswith(\".\") for part in parts):\n                    continue  # Skip hidden files\n                if any(part in self.IGNORE_FOLDERS for part in parts):\n                    continue\n                if relpath.name == \"prompt\":\n                    continue  # Skip files named 'prompt'\n\n                all_files.append(str(relpath))\n\n        if is_git_repo(project_path) and \"projects\" not in project_path.parts:\n            all_files = filter_by_gitignore(project_path, all_files)\n\n        return all_files\n\n\nclass DisplayablePath(object):\n    \"\"\"\n    Represents and displays a file system path in a tree-like structure.\n\n    This class is used to visually represent the structure of directories and files\n    in a way that is similar to a file explorer's tree view.\n    \"\"\"\n\n    display_filename_prefix_middle = \"\u251c\u2500\u2500 \"\n    display_filename_prefix_last = \"\u2514\u2500\u2500 \"\n    display_parent_prefix_middle = \"    \"\n    display_parent_prefix_last = \"\u2502   \"\n\n    def __init__(\n        self, path: Union[str, Path], parent_path: \"DisplayablePath\", is_last: bool\n    ):\n        \"\"\"\n        Initializes a DisplayablePath object with a given path and parent.\n\n        Parameters\n        ----------\n        path : Union[str, Path]\n            The file system path to be displayed.\n        parent_path : DisplayablePath\n            The parent path in the tree structure.\n        is_last : bool\n            Indicates whether this is the last sibling in the tree structure.\n        \"\"\"\n        self.depth = 0\n        self.path = Path(str(path))\n        self.parent = parent_path\n        self.is_last = is_last\n        if self.parent:\n            self.depth = self.parent.depth + 1  # Increment depth if it has a parent\n\n    @property\n    def display_name(self) -> str:\n        \"\"\"\n        Get the display name of the file or directory.\n        \"\"\"\n        if self.path.is_dir():\n            return self.path.name + \"/\"\n        return self.path.name\n\n    @classmethod\n    def make_tree(\n        cls, root: Union[str, Path], parent=None, is_last=False, criteria=None\n    ) -> Generator[\"DisplayablePath\", None, None]:\n        \"\"\"\n        Creates a tree of DisplayablePath objects from a root directory.\n\n        Parameters\n        ----------\n        root : Union[str, Path]\n            The root directory from which to start creating the tree.\n        parent : DisplayablePath, optional\n            The parent path in the tree structure.\n        is_last : bool, optional\n            Indicates whether this is the last sibling in the tree structure.\n        criteria : callable, optional\n            A function to filter the paths included in the tree.\n\n        Yields\n        ------\n        DisplayablePath\n            The next DisplayablePath object in the tree.\n        \"\"\"\n        root = Path(str(root))  # Ensure root is a Path object\n        criteria = criteria or cls._default_criteria\n        displayable_root = cls(root, parent, is_last)\n        yield displayable_root\n\n        if root.is_dir():  # Check if root is a directory before iterating\n            children = sorted(\n                list(path for path in root.iterdir() if criteria(path)),\n                key=lambda s: str(s).lower(),\n            )\n            count = 1\n            for path in children:\n                is_last = count == len(children)\n                yield from cls.make_tree(\n                    path, parent=displayable_root, is_last=is_last, criteria=criteria\n                )\n                count += 1\n\n    @classmethod\n    def _default_criteria(cls, path: Path) -> bool:\n        \"\"\"\n        The default criteria function to filter the paths.\n        \"\"\"\n        return True\n\n    def displayable(self) -> str:\n        \"\"\"\n        Returns a string representation of the path for display in a tree-like structure.\n\n        Returns\n        -------\n        str\n            The displayable string representation of the file or directory.\n        \"\"\"\n        if self.parent is None:\n            return self.display_name\n\n        _filename_prefix = (\n            self.display_filename_prefix_last\n            if self.is_last\n            else self.display_filename_prefix_middle\n        )\n\n        parts = [\"{!s} {!s}\".format(_filename_prefix, self.display_name)]\n\n        parent = self.parent\n        while parent and parent.parent is not None:\n            parts.append(\n                self.display_parent_prefix_middle\n                if parent.is_last\n                else self.display_parent_prefix_last\n            )\n            parent = parent.parent\n\n        return \"\".join(reversed(parts))  # Assemble the parts into the final string\n", "gpt_engineer/applications/cli/cli_agent.py": "\"\"\"\nThis module provides the CliAgent class which manages the lifecycle of code generation and improvement\nusing an AI model. It includes functionalities to initialize code generation, improve existing code,\nand process the code through various steps defined in the step bundle.\n\"\"\"\n\nfrom typing import Callable, Optional, TypeVar\n\n# from gpt_engineer.core.default.git_version_manager import GitVersionManager\nfrom gpt_engineer.core.ai import AI\nfrom gpt_engineer.core.base_agent import BaseAgent\nfrom gpt_engineer.core.base_execution_env import BaseExecutionEnv\nfrom gpt_engineer.core.base_memory import BaseMemory\nfrom gpt_engineer.core.default.disk_execution_env import DiskExecutionEnv\nfrom gpt_engineer.core.default.disk_memory import DiskMemory\nfrom gpt_engineer.core.default.paths import PREPROMPTS_PATH\nfrom gpt_engineer.core.default.steps import (\n    execute_entrypoint,\n    gen_code,\n    gen_entrypoint,\n    improve_fn,\n)\nfrom gpt_engineer.core.files_dict import FilesDict\nfrom gpt_engineer.core.preprompts_holder import PrepromptsHolder\nfrom gpt_engineer.core.prompt import Prompt\n\nCodeGenType = TypeVar(\"CodeGenType\", bound=Callable[[AI, str, BaseMemory], FilesDict])\nCodeProcessor = TypeVar(\n    \"CodeProcessor\", bound=Callable[[AI, BaseExecutionEnv, FilesDict], FilesDict]\n)\nImproveType = TypeVar(\n    \"ImproveType\", bound=Callable[[AI, str, FilesDict, BaseMemory], FilesDict]\n)\n\n\nclass CliAgent(BaseAgent):\n    \"\"\"\n    The `CliAgent` class is responsible for managing the lifecycle of code generation and improvement\n    using an AI model. It orchestrates the generation of new code and the improvement of existing code\n    based on given prompts and utilizes a memory system and execution environment for processing.\n\n    Parameters\n    ----------\n    memory : BaseMemory\n        An instance of a class that adheres to the BaseMemory interface, used for storing and retrieving\n        information during the code generation process.\n    execution_env : BaseExecutionEnv\n        An instance of a class that adheres to the BaseExecutionEnv interface, used for executing code\n        and managing the execution environment.\n    ai : AI, optional\n        An instance of the AI class that manages calls to the language model. If not provided, a default\n        instance is created.\n    code_gen_fn : CodeGenType, optional\n        A callable that takes an AI instance, a prompt, and a memory instance to generate code. Defaults\n        to the `gen_code` function.\n    improve_fn : ImproveType, optional\n        A callable that takes an AI instance, a prompt, a FilesDict instance, and a memory instance to\n        improve code. Defaults to the `improve` function.\n    process_code_fn : CodeProcessor, optional\n        A callable that takes an AI instance, an execution environment, and a FilesDict instance to\n        process code. Defaults to the `execute_entrypoint` function.\n    preprompts_holder : PrepromptsHolder, optional\n        An instance of PrepromptsHolder that manages preprompt templates. If not provided, a default\n        instance is created using the PREPROMPTS_PATH.\n\n    Attributes\n    ----------\n    memory : BaseMemory\n        The memory instance where the agent stores and retrieves information.\n    execution_env : BaseExecutionEnv\n        The execution environment instance where the agent executes and manages code.\n    ai : AI\n        The AI instance used for interacting with the language model.\n    code_gen_fn : CodeGenType\n        The function used for generating code.\n    improve_fn : ImproveType\n        The function used for improving code.\n    process_code_fn : CodeProcessor\n        The function used for processing code.\n    preprompts_holder : PrepromptsHolder\n        The holder for preprompt templates.\n    \"\"\"\n\n    def __init__(\n        self,\n        memory: BaseMemory,\n        execution_env: BaseExecutionEnv,\n        ai: AI = None,\n        code_gen_fn: CodeGenType = gen_code,\n        improve_fn: ImproveType = improve_fn,\n        process_code_fn: CodeProcessor = execute_entrypoint,\n        preprompts_holder: PrepromptsHolder = None,\n    ):\n        self.memory = memory\n        self.execution_env = execution_env\n        self.ai = ai or AI()\n        self.code_gen_fn = code_gen_fn\n        self.process_code_fn = process_code_fn\n        self.improve_fn = improve_fn\n        self.preprompts_holder = preprompts_holder or PrepromptsHolder(PREPROMPTS_PATH)\n\n    @classmethod\n    def with_default_config(\n        cls,\n        memory: DiskMemory,\n        execution_env: DiskExecutionEnv,\n        ai: AI = None,\n        code_gen_fn: CodeGenType = gen_code,\n        improve_fn: ImproveType = improve_fn,\n        process_code_fn: CodeProcessor = execute_entrypoint,\n        preprompts_holder: PrepromptsHolder = None,\n    ):\n        \"\"\"\n        Creates a new instance of CliAgent with default configurations for memory, execution environment,\n        AI, and other functional parameters.\n\n        Parameters\n        ----------\n        memory : DiskMemory\n            An instance of DiskMemory for storing and retrieving information.\n        execution_env : DiskExecutionEnv\n            An instance of DiskExecutionEnv for executing code.\n        ai : AI, optional\n            An instance of AI for interacting with the language model. Defaults to None, which will create\n            a new AI instance.\n        code_gen_fn : CodeGenType, optional\n            A function for generating code. Defaults to `gen_code`.\n        improve_fn : ImproveType, optional\n            A function for improving code. Defaults to `improve`.\n        process_code_fn : CodeProcessor, optional\n            A function for processing code. Defaults to `execute_entrypoint`.\n        preprompts_holder : PrepromptsHolder, optional\n            An instance of PrepromptsHolder for managing preprompt templates. Defaults to None, which will\n            create a new PrepromptsHolder instance using PREPROMPTS_PATH.\n\n        Returns\n        -------\n        CliAgent\n            An instance of CliAgent configured with the provided or default parameters.\n        \"\"\"\n        return cls(\n            memory=memory,\n            execution_env=execution_env,\n            ai=ai,\n            code_gen_fn=code_gen_fn,\n            process_code_fn=process_code_fn,\n            improve_fn=improve_fn,\n            preprompts_holder=preprompts_holder or PrepromptsHolder(PREPROMPTS_PATH),\n        )\n\n    def init(self, prompt: Prompt) -> FilesDict:\n        \"\"\"\n        Generates a new piece of code using the AI and step bundle based on the provided prompt.\n\n        Parameters\n        ----------\n        prompt : str\n            A string prompt that guides the code generation process.\n\n        Returns\n        -------\n        FilesDict\n            An instance of the `FilesDict` class containing the generated code.\n        \"\"\"\n\n        files_dict = self.code_gen_fn(\n            self.ai, prompt, self.memory, self.preprompts_holder\n        )\n        entrypoint = gen_entrypoint(\n            self.ai, prompt, files_dict, self.memory, self.preprompts_holder\n        )\n        combined_dict = {**files_dict, **entrypoint}\n        files_dict = FilesDict(combined_dict)\n        files_dict = self.process_code_fn(\n            self.ai,\n            self.execution_env,\n            files_dict,\n            preprompts_holder=self.preprompts_holder,\n            prompt=prompt,\n            memory=self.memory,\n        )\n        return files_dict\n\n    def improve(\n        self,\n        files_dict: FilesDict,\n        prompt: Prompt,\n        execution_command: Optional[str] = None,\n    ) -> FilesDict:\n        \"\"\"\n        Improves an existing piece of code using the AI and step bundle based on the provided prompt.\n\n        Parameters\n        ----------\n        files_dict : FilesDict\n            An instance of `FilesDict` containing the code to be improved.\n        prompt : str\n            A string prompt that guides the code improvement process.\n        execution_command : str, optional\n            An optional command to execute the code. If not provided, the default execution command is used.\n\n        Returns\n        -------\n        FilesDict\n            An instance of the `FilesDict` class containing the improved code.\n        \"\"\"\n\n        files_dict = self.improve_fn(\n            self.ai, prompt, files_dict, self.memory, self.preprompts_holder\n        )\n        # entrypoint = gen_entrypoint(\n        #     self.ai, prompt, files_dict, self.memory, self.preprompts_holder\n        # )\n        # combined_dict = {**files_dict, **entrypoint}\n        # files_dict = FilesDict(combined_dict)\n        # files_dict = self.process_code_fn(\n        #     self.ai,\n        #     self.execution_env,\n        #     files_dict,\n        #     preprompts_holder=self.preprompts_holder,\n        #     prompt=prompt,\n        #     memory=self.memory,\n        # )\n\n        return files_dict\n", "gpt_engineer/applications/cli/main.py": "\"\"\"\nEntrypoint for the CLI tool.\n\nThis module serves as the entry point for a command-line interface (CLI) tool.\nIt is designed to interact with OpenAI's language models.\nThe module provides functionality to:\n- Load necessary environment variables,\n- Configure various parameters for the AI interaction,\n- Manage the generation or improvement of code projects.\n\nMain Functionality\n------------------\n- Load environment variables required for OpenAI API interaction.\n- Parse user-specified parameters for project configuration and AI behavior.\n- Facilitate interaction with AI models, databases, and archival processes.\n\nParameters\n----------\nNone\n\nNotes\n-----\n- The `OPENAI_API_KEY` must be set in the environment or provided in a `.env` file within the working directory.\n- The default project path is `projects/example`.\n- When using the `azure_endpoint` parameter, provide the Azure OpenAI service endpoint URL.\n\"\"\"\n\nimport difflib\nimport logging\nimport os\nimport sys\n\nfrom pathlib import Path\n\nimport openai\nimport typer\n\nfrom dotenv import load_dotenv\nfrom langchain.globals import set_llm_cache\nfrom langchain_community.cache import SQLiteCache\nfrom termcolor import colored\n\nfrom gpt_engineer.applications.cli.cli_agent import CliAgent\nfrom gpt_engineer.applications.cli.collect import collect_and_send_human_review\nfrom gpt_engineer.applications.cli.file_selector import FileSelector\nfrom gpt_engineer.core.ai import AI, ClipboardAI\nfrom gpt_engineer.core.default.disk_execution_env import DiskExecutionEnv\nfrom gpt_engineer.core.default.disk_memory import DiskMemory\nfrom gpt_engineer.core.default.file_store import FileStore\nfrom gpt_engineer.core.default.paths import PREPROMPTS_PATH, memory_path\nfrom gpt_engineer.core.default.steps import (\n    execute_entrypoint,\n    gen_code,\n    handle_improve_mode,\n    improve_fn as improve_fn,\n)\nfrom gpt_engineer.core.files_dict import FilesDict\nfrom gpt_engineer.core.git import stage_uncommitted_to_git\nfrom gpt_engineer.core.preprompts_holder import PrepromptsHolder\nfrom gpt_engineer.core.prompt import Prompt\nfrom gpt_engineer.tools.custom_steps import clarified_gen, lite_gen, self_heal\n\napp = typer.Typer(\n    context_settings={\"help_option_names\": [\"-h\", \"--help\"]}\n)  # creates a CLI app\n\n\ndef load_env_if_needed():\n    \"\"\"\n    Load environment variables if the OPENAI_API_KEY is not already set.\n\n    This function checks if the OPENAI_API_KEY environment variable is set,\n    and if not, it attempts to load it from a .env file in the current working\n    directory. It then sets the openai.api_key for use in the application.\n    \"\"\"\n    # We have all these checks for legacy reasons...\n    if os.getenv(\"OPENAI_API_KEY\") is None:\n        load_dotenv()\n    if os.getenv(\"OPENAI_API_KEY\") is None:\n        load_dotenv(dotenv_path=os.path.join(os.getcwd(), \".env\"))\n\n    openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n\n    if os.getenv(\"ANTHROPIC_API_KEY\") is None:\n        load_dotenv()\n    if os.getenv(\"ANTHROPIC_API_KEY\") is None:\n        load_dotenv(dotenv_path=os.path.join(os.getcwd(), \".env\"))\n\n\ndef concatenate_paths(base_path, sub_path):\n    # Compute the relative path from base_path to sub_path\n    relative_path = os.path.relpath(sub_path, base_path)\n\n    # If the relative path is not in the parent directory, use the original sub_path\n    if not relative_path.startswith(\"..\"):\n        return sub_path\n\n    # Otherwise, concatenate base_path and sub_path\n    return os.path.normpath(os.path.join(base_path, sub_path))\n\n\ndef load_prompt(\n    input_repo: DiskMemory,\n    improve_mode: bool,\n    prompt_file: str,\n    image_directory: str,\n    entrypoint_prompt_file: str = \"\",\n) -> Prompt:\n    \"\"\"\n    Load or request a prompt from the user based on the mode.\n\n    Parameters\n    ----------\n    input_repo : DiskMemory\n        The disk memory object where prompts and other data are stored.\n    improve_mode : bool\n        Flag indicating whether the application is in improve mode.\n\n    Returns\n    -------\n    str\n        The loaded or inputted prompt.\n    \"\"\"\n\n    if os.path.isdir(prompt_file):\n        raise ValueError(\n            f\"The path to the prompt, {prompt_file}, already exists as a directory. No prompt can be read from it. Please specify a prompt file using --prompt_file\"\n        )\n    prompt_str = input_repo.get(prompt_file)\n    if prompt_str:\n        print(colored(\"Using prompt from file:\", \"green\"), prompt_file)\n        print(prompt_str)\n    else:\n        if not improve_mode:\n            prompt_str = input(\n                \"\\nWhat application do you want gpt-engineer to generate?\\n\"\n            )\n        else:\n            prompt_str = input(\"\\nHow do you want to improve the application?\\n\")\n\n    if entrypoint_prompt_file == \"\":\n        entrypoint_prompt = \"\"\n    else:\n        full_entrypoint_prompt_file = concatenate_paths(\n            input_repo.path, entrypoint_prompt_file\n        )\n        if os.path.isfile(full_entrypoint_prompt_file):\n            entrypoint_prompt = input_repo.get(full_entrypoint_prompt_file)\n\n        else:\n            raise ValueError(\"The provided file at --entrypoint-prompt does not exist\")\n\n    if image_directory == \"\":\n        return Prompt(prompt_str, entrypoint_prompt=entrypoint_prompt)\n\n    full_image_directory = concatenate_paths(input_repo.path, image_directory)\n    if os.path.isdir(full_image_directory):\n        if len(os.listdir(full_image_directory)) == 0:\n            raise ValueError(\"The provided --image_directory is empty.\")\n        image_repo = DiskMemory(full_image_directory)\n        return Prompt(\n            prompt_str,\n            image_repo.get(\".\").to_dict(),\n            entrypoint_prompt=entrypoint_prompt,\n        )\n    else:\n        raise ValueError(\"The provided --image_directory is not a directory.\")\n\n\ndef get_preprompts_path(use_custom_preprompts: bool, input_path: Path) -> Path:\n    \"\"\"\n    Get the path to the preprompts, using custom ones if specified.\n\n    Parameters\n    ----------\n    use_custom_preprompts : bool\n        Flag indicating whether to use custom preprompts.\n    input_path : Path\n        The path to the project directory.\n\n    Returns\n    -------\n    Path\n        The path to the directory containing the preprompts.\n    \"\"\"\n    original_preprompts_path = PREPROMPTS_PATH\n    if not use_custom_preprompts:\n        return original_preprompts_path\n\n    custom_preprompts_path = input_path / \"preprompts\"\n    if not custom_preprompts_path.exists():\n        custom_preprompts_path.mkdir()\n\n    for file in original_preprompts_path.glob(\"*\"):\n        if not (custom_preprompts_path / file.name).exists():\n            (custom_preprompts_path / file.name).write_text(file.read_text())\n    return custom_preprompts_path\n\n\ndef compare(f1: FilesDict, f2: FilesDict):\n    def colored_diff(s1, s2):\n        lines1 = s1.splitlines()\n        lines2 = s2.splitlines()\n\n        diff = difflib.unified_diff(lines1, lines2, lineterm=\"\")\n\n        RED = \"\\033[38;5;202m\"\n        GREEN = \"\\033[92m\"\n        RESET = \"\\033[0m\"\n\n        colored_lines = []\n        for line in diff:\n            if line.startswith(\"+\"):\n                colored_lines.append(GREEN + line + RESET)\n            elif line.startswith(\"-\"):\n                colored_lines.append(RED + line + RESET)\n            else:\n                colored_lines.append(line)\n\n        return \"\\n\".join(colored_lines)\n\n    for file in sorted(set(f1) | set(f2)):\n        diff = colored_diff(f1.get(file, \"\"), f2.get(file, \"\"))\n        if diff:\n            print(f\"Changes to {file}:\")\n            print(diff)\n\n\ndef prompt_yesno() -> bool:\n    TERM_CHOICES = colored(\"y\", \"green\") + \"/\" + colored(\"n\", \"red\") + \" \"\n    while True:\n        response = input(TERM_CHOICES).strip().lower()\n        if response in [\"y\", \"yes\"]:\n            return True\n        if response in [\"n\", \"no\"]:\n            break\n        print(\"Please respond with 'y' or 'n'\")\n\n\n@app.command(\n    help=\"\"\"\n        GPT-engineer lets you:\n\n        \\b\n        - Specify a software in natural language\n        - Sit back and watch as an AI writes and executes the code\n        - Ask the AI to implement improvements\n    \"\"\"\n)\ndef main(\n    project_path: str = typer.Argument(\".\", help=\"path\"),\n    model: str = typer.Option(\n        os.environ.get(\"MODEL_NAME\", \"gpt-4o\"), \"--model\", \"-m\", help=\"model id string\"\n    ),\n    temperature: float = typer.Option(\n        0.1,\n        \"--temperature\",\n        \"-t\",\n        help=\"Controls randomness: lower values for more focused, deterministic outputs\",\n    ),\n    improve_mode: bool = typer.Option(\n        False,\n        \"--improve\",\n        \"-i\",\n        help=\"Improve an existing project by modifying the files.\",\n    ),\n    lite_mode: bool = typer.Option(\n        False,\n        \"--lite\",\n        \"-l\",\n        help=\"Lite mode: run a generation using only the main prompt.\",\n    ),\n    clarify_mode: bool = typer.Option(\n        False,\n        \"--clarify\",\n        \"-c\",\n        help=\"Clarify mode - discuss specification with AI before implementation.\",\n    ),\n    self_heal_mode: bool = typer.Option(\n        False,\n        \"--self-heal\",\n        \"-sh\",\n        help=\"Self-heal mode - fix the code by itself when it fails.\",\n    ),\n    azure_endpoint: str = typer.Option(\n        \"\",\n        \"--azure\",\n        \"-a\",\n        help=\"\"\"Endpoint for your Azure OpenAI Service (https://xx.openai.azure.com).\n            In that case, the given model is the deployment name chosen in the Azure AI Studio.\"\"\",\n    ),\n    use_custom_preprompts: bool = typer.Option(\n        False,\n        \"--use-custom-preprompts\",\n        help=\"\"\"Use your project's custom preprompts instead of the default ones.\n          Copies all original preprompts to the project's workspace if they don't exist there.\"\"\",\n    ),\n    llm_via_clipboard: bool = typer.Option(\n        False,\n        \"--llm-via-clipboard\",\n        help=\"Use the clipboard to communicate with the AI.\",\n    ),\n    verbose: bool = typer.Option(\n        False, \"--verbose\", \"-v\", help=\"Enable verbose logging for debugging.\"\n    ),\n    debug: bool = typer.Option(\n        False, \"--debug\", \"-d\", help=\"Enable debug mode for debugging.\"\n    ),\n    prompt_file: str = typer.Option(\n        \"prompt\",\n        \"--prompt_file\",\n        help=\"Relative path to a text file containing a prompt.\",\n    ),\n    entrypoint_prompt_file: str = typer.Option(\n        \"\",\n        \"--entrypoint_prompt\",\n        help=\"Relative path to a text file containing a file that specifies requirements for you entrypoint.\",\n    ),\n    image_directory: str = typer.Option(\n        \"\",\n        \"--image_directory\",\n        help=\"Relative path to a folder containing images.\",\n    ),\n    use_cache: bool = typer.Option(\n        False,\n        \"--use_cache\",\n        help=\"Speeds up computations and saves tokens when running the same prompt multiple times by caching the LLM response.\",\n    ),\n    no_execution: bool = typer.Option(\n        False,\n        \"--no_execution\",\n        help=\"Run setup but to not call LLM or write any code. For testing purposes.\",\n    ),\n):\n    \"\"\"\n    The main entry point for the CLI tool that generates or improves a project.\n\n    This function sets up the CLI tool, loads environment variables, initializes\n    the AI, and processes the user's request to generate or improve a project\n    based on the provided arguments.\n\n    Parameters\n    ----------\n    project_path : str\n        The file path to the project directory.\n    model : str\n        The model ID string for the AI.\n    temperature : float\n        The temperature setting for the AI's responses.\n    improve_mode : bool\n        Flag indicating whether to improve an existing project.\n    lite_mode : bool\n        Flag indicating whether to run in lite mode.\n    clarify_mode : bool\n        Flag indicating whether to discuss specifications with AI before implementation.\n    self_heal_mode : bool\n        Flag indicating whether to enable self-healing mode.\n    azure_endpoint : str\n        The endpoint for Azure OpenAI services.\n    use_custom_preprompts : bool\n        Flag indicating whether to use custom preprompts.\n    prompt_file : str\n        Relative path to a text file containing a prompt.\n    entrypoint_prompt_file: str\n        Relative path to a text file containing a file that specifies requirements for you entrypoint.\n    image_directory: str\n        Relative path to a folder containing images.\n    use_cache: bool\n        Speeds up computations and saves tokens when running the same prompt multiple times by caching the LLM response.\n    verbose : bool\n        Flag indicating whether to enable verbose logging.\n    no_execution: bool\n        Run setup but to not call LLM or write any code. For testing purposes.\n\n    Returns\n    -------\n    None\n    \"\"\"\n\n    if debug:\n        import pdb\n\n        sys.excepthook = lambda *_: pdb.pm()\n\n    # Validate arguments\n    if improve_mode and (clarify_mode or lite_mode):\n        typer.echo(\"Error: Clarify and lite mode are not compatible with improve mode.\")\n        raise typer.Exit(code=1)\n\n    # Set up logging\n    logging.basicConfig(level=logging.DEBUG if verbose else logging.INFO)\n    if use_cache:\n        set_llm_cache(SQLiteCache(database_path=\".langchain.db\"))\n    if improve_mode:\n        assert not (\n            clarify_mode or lite_mode\n        ), \"Clarify and lite mode are not active for improve mode\"\n\n    load_env_if_needed()\n\n    if llm_via_clipboard:\n        ai = ClipboardAI()\n    else:\n        ai = AI(\n            model_name=model,\n            temperature=temperature,\n            azure_endpoint=azure_endpoint,\n        )\n\n    path = Path(project_path)\n    print(\"Running gpt-engineer in\", path.absolute(), \"\\n\")\n\n    prompt = load_prompt(\n        DiskMemory(path),\n        improve_mode,\n        prompt_file,\n        image_directory,\n        entrypoint_prompt_file,\n    )\n\n    # todo: if ai.vision is false and not llm_via_clipboard - ask if they would like to use gpt-4-vision-preview instead? If so recreate AI\n    if not ai.vision:\n        prompt.image_urls = None\n\n    # configure generation function\n    if clarify_mode:\n        code_gen_fn = clarified_gen\n    elif lite_mode:\n        code_gen_fn = lite_gen\n    else:\n        code_gen_fn = gen_code\n\n    # configure execution function\n    if self_heal_mode:\n        execution_fn = self_heal\n    else:\n        execution_fn = execute_entrypoint\n\n    preprompts_holder = PrepromptsHolder(\n        get_preprompts_path(use_custom_preprompts, Path(project_path))\n    )\n\n    memory = DiskMemory(memory_path(project_path))\n    memory.archive_logs()\n\n    execution_env = DiskExecutionEnv()\n    agent = CliAgent.with_default_config(\n        memory,\n        execution_env,\n        ai=ai,\n        code_gen_fn=code_gen_fn,\n        improve_fn=improve_fn,\n        process_code_fn=execution_fn,\n        preprompts_holder=preprompts_holder,\n    )\n\n    files = FileStore(project_path)\n    if not no_execution:\n        if improve_mode:\n            files_dict_before, is_linting = FileSelector(project_path).ask_for_files()\n\n            # lint the code\n            if is_linting:\n                files_dict_before = files.linting(files_dict_before)\n\n            files_dict = handle_improve_mode(prompt, agent, memory, files_dict_before)\n            if not files_dict or files_dict_before == files_dict:\n                print(\n                    f\"No changes applied. Could you please upload the debug_log_file.txt in {memory.path}/logs folder in a github issue?\"\n                )\n\n            else:\n                print(\"\\nChanges to be made:\")\n                compare(files_dict_before, files_dict)\n\n                print()\n                print(colored(\"Do you want to apply these changes?\", \"light_green\"))\n                if not prompt_yesno():\n                    files_dict = files_dict_before\n\n        else:\n            files_dict = agent.init(prompt)\n            # collect user feedback if user consents\n            config = (code_gen_fn.__name__, execution_fn.__name__)\n            collect_and_send_human_review(prompt, model, temperature, config, memory)\n\n        stage_uncommitted_to_git(path, files_dict, improve_mode)\n\n        files.push(files_dict)\n\n    if ai.token_usage_log.is_openai_model():\n        print(\"Total api cost: $ \", ai.token_usage_log.usage_cost())\n    elif os.getenv(\"LOCAL_MODEL\"):\n        print(\"Total api cost: $ 0.0 since we are using local LLM.\")\n    else:\n        print(\"Total tokens used: \", ai.token_usage_log.total_tokens())\n\n\nif __name__ == \"__main__\":\n    app()\n", "gpt_engineer/applications/cli/learning.py": "\"\"\"\nThe `learning` module is designed to facilitate the collection and storage of user feedback on the outputs generated by the GPT Engineer tool. It provides mechanisms for obtaining user consent, capturing user reviews, and storing this information for future analysis and enhancement of the tool's performance.\n\nClasses\n-------\nReview : dataclass\n    Represents a user's review of the generated code, including whether it ran, was perfect, was useful, and any additional comments.\nLearning : dataclass\n    Encapsulates the metadata and feedback collected during a session of using the GPT Engineer tool, including the prompt, model, temperature, configuration, logs, session identifier, user review, and timestamp.\n\nFunctions\n---------\nhuman_review_input() -> Optional[Review]\n    Interactively gathers feedback from the user regarding the performance of generated code and returns a Review instance.\ncheck_collection_consent() -> bool\n    Checks if the user has previously given consent to store their data and, if not, asks for it.\nask_collection_consent() -> bool\n    Prompts the user for consent to store their data for the purpose of improving GPT Engineer.\nextract_learning(prompt: Prompt, model: str, temperature: float, config: Tuple[str, ...], memory: DiskMemory, review: Review) -> Learning\n    Extracts feedback and session details to create a Learning instance based on the provided parameters.\nget_session() -> str\n    Retrieves a unique identifier for the current user session, creating one if it does not exist.\n\nConstants\n---------\nTERM_CHOICES : tuple\n    Terminal color choices for user interactive prompts, formatted with termcolor for readability.\n\"\"\"\n\nimport json\nimport random\nimport tempfile\n\nfrom dataclasses import dataclass, field\nfrom datetime import datetime\nfrom pathlib import Path\nfrom typing import Optional, Tuple\n\nfrom dataclasses_json import dataclass_json\nfrom termcolor import colored\n\nfrom gpt_engineer.core.default.disk_memory import DiskMemory\nfrom gpt_engineer.core.prompt import Prompt\n\n\n@dataclass_json\n@dataclass\nclass Review:\n    \"\"\"\n    A dataclass that represents a user's review of the generated code.\n\n    Attributes\n    ----------\n    ran : Optional[bool]\n        Indicates whether the generated code ran without errors.\n    perfect : Optional[bool]\n        Indicates whether the generated code met all the user's requirements.\n    works : Optional[bool]\n        Indicates whether the generated code was useful, even if not perfect.\n    comments : str\n        Any additional comments provided by the user.\n    raw : str\n        A raw string representation of the user's responses.\n    \"\"\"\n\n    ran: Optional[bool]\n    perfect: Optional[bool]\n    works: Optional[bool]\n    comments: str\n    raw: str\n\n\n@dataclass_json\n@dataclass\nclass Learning:\n    \"\"\"\n    A dataclass that encapsulates the learning data collected during a GPT Engineer session.\n\n    Attributes\n    ----------\n    prompt : str\n        A JSON string representing the prompt provided to GPT Engineer.\n    model : str\n        The name of the model used during the session.\n    temperature : float\n        The temperature setting used for the model's responses.\n    config : str\n        A JSON string representing the configuration settings for the session.\n    logs : str\n        A JSON string representing the logs of the session.\n    session : str\n        A unique identifier for the user session.\n    review : Optional[Review]\n        The user's review of the generated code.\n    timestamp : str\n        The UTC timestamp when the learning data was created.\n    version : str\n        The version of the learning data schema.\n    \"\"\"\n\n    prompt: str\n    model: str\n    temperature: float\n    config: str\n    logs: str\n    session: str\n    review: Optional[Review]\n    timestamp: str = field(default_factory=lambda: datetime.utcnow().isoformat())\n    version: str = \"0.3\"\n\n\nTERM_CHOICES = (\n    colored(\"y\", \"green\")\n    + \"/\"\n    + colored(\"n\", \"red\")\n    + \"/\"\n    + colored(\"u\", \"yellow\")\n    + \"(ncertain): \"\n)\n\n\ndef human_review_input() -> Optional[Review]:\n    \"\"\"\n    Interactively prompts the user to review the generated code and returns their feedback encapsulated in a Review object.\n\n    This function will first check if the user has given consent to collect their feedback. If consent is given, it will ask the user a series of questions about the generated code's performance and capture their responses.\n\n    Returns\n    -------\n    Optional[Review]\n        A Review object containing the user's feedback, or None if consent is not given.\n    \"\"\"\n    print()\n    if not check_collection_consent():\n        return None\n    print()\n    print(\n        colored(\"To help gpt-engineer learn, please answer 3 questions:\", \"light_green\")\n    )\n    print()\n\n    ran = input(\"Did the generated code run at all? \" + TERM_CHOICES)\n    ran = ask_for_valid_input(ran)\n\n    if ran == \"y\":\n        perfect = input(\n            \"Did the generated code do everything you wanted? \" + TERM_CHOICES\n        )\n        perfect = ask_for_valid_input(perfect)\n\n        if perfect != \"y\":\n            useful = input(\"Did the generated code do anything useful? \" + TERM_CHOICES)\n            useful = ask_for_valid_input(useful)\n        else:\n            useful = \"\"\n    else:\n        perfect = \"\"\n        useful = \"\"\n\n    if perfect != \"y\":\n        comments = input(\n            \"If you have time, please explain what was not working \"\n            + colored(\"(ok to leave blank)\\n\", \"light_green\")\n        )\n    else:\n        comments = \"\"\n\n    return Review(\n        raw=\", \".join([ran, perfect, useful]),\n        ran={\"y\": True, \"n\": False, \"u\": None, \"\": None}[ran],\n        works={\"y\": True, \"n\": False, \"u\": None, \"\": None}[useful],\n        perfect={\"y\": True, \"n\": False, \"u\": None, \"\": None}[perfect],\n        comments=comments,\n    )\n\n\ndef ask_for_valid_input(ran):\n    while ran not in (\"y\", \"n\", \"u\"):\n        ran = input(\"Invalid input. Please enter y, n, or u: \")\n    return ran\n\n\ndef check_collection_consent() -> bool:\n    \"\"\"\n    Checks if the user has previously given consent to store their data for feedback collection.\n\n    This function looks for a file that stores the user's consent status. If the file exists and contains 'true', consent is assumed. If the file does not exist or does not contain 'true', the function will prompt the user for consent.\n\n    Returns\n    -------\n    bool\n        True if the user has given consent, False otherwise.\n    \"\"\"\n    path = Path(\".gpte_consent\")\n    if path.exists() and path.read_text() == \"true\":\n        return True\n    else:\n        return ask_collection_consent()\n\n\ndef ask_collection_consent() -> bool:\n    \"\"\"\n    Asks the user for their consent to store their data for the purpose of improving the GPT Engineer tool.\n\n    The user's response is recorded in a file for future reference. If the user consents, the function will write 'true' to the file. If the user does not consent, no data will be collected, and the function will not modify the file.\n\n    Returns\n    -------\n    bool\n        True if the user consents, False otherwise.\n    \"\"\"\n    answer = input(\n        \"Is it ok if we store your prompts to help improve GPT Engineer? (y/n)\"\n    )\n    while answer.lower() not in (\"y\", \"n\"):\n        answer = input(\"Invalid input. Please enter y or n: \")\n\n    if answer.lower() == \"y\":\n        path = Path(\".gpte_consent\")\n        path.write_text(\"true\")\n        print(colored(\"Thank you\ufe0f\", \"light_green\"))\n        print()\n        print(\n            \"(If you no longer wish to participate in data collection, delete the file .gpte_consent)\"\n        )\n        return True\n    else:\n        print(\n            colored(\n                \"No worries! GPT Engineer will not collect your prompts. \u2764\ufe0f\",\n                \"light_green\",\n            )\n        )\n        return False\n\n\ndef extract_learning(\n    prompt: Prompt,\n    model: str,\n    temperature: float,\n    config: Tuple[str, ...],\n    memory: DiskMemory,\n    review: Review,\n) -> Learning:\n    \"\"\"\n    Constructs a Learning object containing the session's metadata and user feedback.\n\n    Parameters\n    ----------\n    prompt : str\n        The initial prompt provided to the GPT Engineer.\n    model : str\n        The name of the model used during the session.\n    temperature : float\n        The temperature setting used for the model's responses.\n    config : Tuple[str, ...]\n        A tuple representing the configuration settings for the session.\n    memory : DiskMemory\n        An object representing the disk memory used during the session.\n    review : Review\n        The user's review of the generated code.\n\n    Returns\n    -------\n    Learning\n        An instance of Learning containing all the session details and user feedback.\n    \"\"\"\n    return Learning(\n        prompt=prompt.to_json(),\n        model=model,\n        temperature=temperature,\n        config=json.dumps(config),\n        session=get_session(),\n        logs=memory.to_json(),\n        review=review,\n    )\n\n\ndef get_session() -> str:\n    \"\"\"\n    Retrieves or generates a unique identifier for the current user session.\n\n    This function attempts to read a unique user ID from a temporary file. If the file does not exist, it generates a new random ID, writes it to the file, and returns it. This ID is used to uniquely identify the user's session.\n\n    Returns\n    -------\n    str\n        A unique identifier for the user session.\n    \"\"\"\n    path = Path(tempfile.gettempdir()) / \"gpt_engineer_user_id.txt\"\n\n    try:\n        if path.exists():\n            user_id = path.read_text()\n        else:\n            # random uuid:\n            user_id = str(random.randint(0, 2**32))\n            path.write_text(user_id)\n        return user_id\n    except IOError:\n        return \"ephemeral_\" + str(random.randint(0, 2**32))\n", "gpt_engineer/applications/cli/__init__.py": "", "gpt_engineer/core/ai.py": "\"\"\"\nAI Module\n\nThis module provides an AI class that interfaces with language models to perform various tasks such as\nstarting a conversation, advancing the conversation, and handling message serialization. It also includes\nbackoff strategies for handling rate limit errors from the OpenAI API.\n\nClasses:\n    AI: A class that interfaces with language models for conversation management and message serialization.\n\nFunctions:\n    serialize_messages(messages: List[Message]) -> str\n        Serialize a list of messages to a JSON string.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport json\nimport logging\nimport os\n\nfrom pathlib import Path\nfrom typing import Any, List, Optional, Union\n\nimport backoff\nimport openai\nimport pyperclip\n\nfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\nfrom langchain.chat_models.base import BaseChatModel\nfrom langchain.schema import (\n    AIMessage,\n    HumanMessage,\n    SystemMessage,\n    messages_from_dict,\n    messages_to_dict,\n)\nfrom langchain_anthropic import ChatAnthropic\nfrom langchain_openai import AzureChatOpenAI, ChatOpenAI\n\nfrom gpt_engineer.core.token_usage import TokenUsageLog\n\n# Type hint for a chat message\nMessage = Union[AIMessage, HumanMessage, SystemMessage]\n\n# Set up logging\nlogger = logging.getLogger(__name__)\n\n\nclass AI:\n    \"\"\"\n    A class that interfaces with language models for conversation management and message serialization.\n\n    This class provides methods to start and advance conversations, handle message serialization,\n    and implement backoff strategies for rate limit errors when interacting with the OpenAI API.\n\n    Attributes\n    ----------\n    temperature : float\n        The temperature setting for the language model.\n    azure_endpoint : str\n        The endpoint URL for the Azure-hosted language model.\n    model_name : str\n        The name of the language model to use.\n    streaming : bool\n        A flag indicating whether to use streaming for the language model.\n    llm : BaseChatModel\n        The language model instance for conversation management.\n    token_usage_log : TokenUsageLog\n        A log for tracking token usage during conversations.\n\n    Methods\n    -------\n    start(system: str, user: str, step_name: str) -> List[Message]\n        Start the conversation with a system message and a user message.\n    next(messages: List[Message], prompt: Optional[str], step_name: str) -> List[Message]\n        Advances the conversation by sending message history to LLM and updating with the response.\n    backoff_inference(messages: List[Message]) -> Any\n        Perform inference using the language model with an exponential backoff strategy.\n    serialize_messages(messages: List[Message]) -> str\n        Serialize a list of messages to a JSON string.\n    deserialize_messages(jsondictstr: str) -> List[Message]\n        Deserialize a JSON string to a list of messages.\n    _create_chat_model() -> BaseChatModel\n        Create a chat model with the specified model name and temperature.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_name=\"gpt-4-turbo\",\n        temperature=0.1,\n        azure_endpoint=None,\n        streaming=True,\n        vision=False,\n    ):\n        \"\"\"\n        Initialize the AI class.\n\n        Parameters\n        ----------\n        model_name : str, optional\n            The name of the model to use, by default \"gpt-4\".\n        temperature : float, optional\n            The temperature to use for the model, by default 0.1.\n        \"\"\"\n        self.temperature = temperature\n        self.azure_endpoint = azure_endpoint\n        self.model_name = model_name\n        self.streaming = streaming\n        self.vision = (\n            (\"vision-preview\" in model_name)\n            or (\"gpt-4-turbo\" in model_name and \"preview\" not in model_name)\n            or (\"claude\" in model_name)\n        )\n        self.llm = self._create_chat_model()\n        self.token_usage_log = TokenUsageLog(model_name)\n\n        logger.debug(f\"Using model {self.model_name}\")\n\n    def start(self, system: str, user: Any, *, step_name: str) -> List[Message]:\n        \"\"\"\n        Start the conversation with a system message and a user message.\n\n        Parameters\n        ----------\n        system : str\n            The content of the system message.\n        user : str\n            The content of the user message.\n        step_name : str\n            The name of the step.\n\n        Returns\n        -------\n        List[Message]\n            The list of messages in the conversation.\n        \"\"\"\n\n        messages: List[Message] = [\n            SystemMessage(content=system),\n            HumanMessage(content=user),\n        ]\n        return self.next(messages, step_name=step_name)\n\n    def _extract_content(self, content):\n        \"\"\"\n        Extracts text content from a message, supporting both string and list types.\n        Parameters\n        ----------\n        content : Union[str, List[dict]]\n            The content of a message, which could be a string or a list.\n        Returns\n        -------\n        str\n            The extracted text content.\n        \"\"\"\n        if isinstance(content, str):\n            return content\n        elif isinstance(content, list) and content and \"text\" in content[0]:\n            # Assuming the structure of list content is [{'type': 'text', 'text': 'Some text'}, ...]\n            return content[0][\"text\"]\n        else:\n            return \"\"\n\n    def _collapse_text_messages(self, messages: List[Message]):\n        \"\"\"\n        Combine consecutive messages of the same type into a single message, where if the message content\n        is a list type, the first text element's content is taken. This method keeps `combined_content` as a string.\n\n        This method iterates through the list of messages, combining consecutive messages of the same type\n        by joining their content with a newline character. If the content is a list, it extracts text from the first\n        text element's content. This reduces the number of messages and simplifies the conversation for processing.\n\n        Parameters\n        ----------\n        messages : List[Message]\n            The list of messages to collapse.\n\n        Returns\n        -------\n        List[Message]\n            The list of messages after collapsing consecutive messages of the same type.\n        \"\"\"\n        collapsed_messages = []\n        if not messages:\n            return collapsed_messages\n\n        previous_message = messages[0]\n        combined_content = self._extract_content(previous_message.content)\n\n        for current_message in messages[1:]:\n            if current_message.type == previous_message.type:\n                combined_content += \"\\n\\n\" + self._extract_content(\n                    current_message.content\n                )\n            else:\n                collapsed_messages.append(\n                    previous_message.__class__(content=combined_content)\n                )\n                previous_message = current_message\n                combined_content = self._extract_content(current_message.content)\n\n        collapsed_messages.append(previous_message.__class__(content=combined_content))\n        return collapsed_messages\n\n    def next(\n        self,\n        messages: List[Message],\n        prompt: Optional[str] = None,\n        *,\n        step_name: str,\n    ) -> List[Message]:\n        \"\"\"\n        Advances the conversation by sending message history\n        to LLM and updating with the response.\n\n        Parameters\n        ----------\n        messages : List[Message]\n            The list of messages in the conversation.\n        prompt : Optional[str], optional\n            The prompt to use, by default None.\n        step_name : str\n            The name of the step.\n\n        Returns\n        -------\n        List[Message]\n            The updated list of messages in the conversation.\n        \"\"\"\n\n        if prompt:\n            messages.append(HumanMessage(content=prompt))\n\n        logger.debug(\n            \"Creating a new chat completion: %s\",\n            \"\\n\".join([m.pretty_repr() for m in messages]),\n        )\n\n        if not self.vision:\n            messages = self._collapse_text_messages(messages)\n\n        response = self.backoff_inference(messages)\n\n        self.token_usage_log.update_log(\n            messages=messages, answer=response.content, step_name=step_name\n        )\n        messages.append(response)\n        logger.debug(f\"Chat completion finished: {messages}\")\n\n        return messages\n\n    @backoff.on_exception(backoff.expo, openai.RateLimitError, max_tries=7, max_time=45)\n    def backoff_inference(self, messages):\n        \"\"\"\n        Perform inference using the language model while implementing an exponential backoff strategy.\n\n        This function will retry the inference in case of a rate limit error from the OpenAI API.\n        It uses an exponential backoff strategy, meaning the wait time between retries increases\n        exponentially. The function will attempt to retry up to 7 times within a span of 45 seconds.\n\n        Parameters\n        ----------\n        messages : List[Message]\n            A list of chat messages which will be passed to the language model for processing.\n\n        callbacks : List[Callable]\n            A list of callback functions that are triggered after each inference. These functions\n            can be used for logging, monitoring, or other auxiliary tasks.\n\n        Returns\n        -------\n        Any\n            The output from the language model after processing the provided messages.\n\n        Raises\n        ------\n        openai.error.RateLimitError\n            If the number of retries exceeds the maximum or if the rate limit persists beyond the\n            allotted time, the function will ultimately raise a RateLimitError.\n\n        Example\n        -------\n        >>> messages = [SystemMessage(content=\"Hello\"), HumanMessage(content=\"How's the weather?\")]\n        >>> response = backoff_inference(messages)\n        \"\"\"\n        return self.llm.invoke(messages)  # type: ignore\n\n    @staticmethod\n    def serialize_messages(messages: List[Message]) -> str:\n        \"\"\"\n        Serialize a list of messages to a JSON string.\n\n        Parameters\n        ----------\n        messages : List[Message]\n            The list of messages to serialize.\n\n        Returns\n        -------\n        str\n            The serialized messages as a JSON string.\n        \"\"\"\n        return json.dumps(messages_to_dict(messages))\n\n    @staticmethod\n    def deserialize_messages(jsondictstr: str) -> List[Message]:\n        \"\"\"\n        Deserialize a JSON string to a list of messages.\n\n        Parameters\n        ----------\n        jsondictstr : str\n            The JSON string to deserialize.\n\n        Returns\n        -------\n        List[Message]\n            The deserialized list of messages.\n        \"\"\"\n        data = json.loads(jsondictstr)\n        # Modify implicit is_chunk property to ALWAYS false\n        # since Langchain's Message schema is stricter\n        prevalidated_data = [\n            {**item, \"tools\": {**item.get(\"tools\", {}), \"is_chunk\": False}}\n            for item in data\n        ]\n        return list(messages_from_dict(prevalidated_data))  # type: ignore\n\n    def _create_chat_model(self) -> BaseChatModel:\n        \"\"\"\n        Create a chat model with the specified model name and temperature.\n\n        Parameters\n        ----------\n        model : str\n            The name of the model to create.\n        temperature : float\n            The temperature to use for the model.\n\n        Returns\n        -------\n        BaseChatModel\n            The created chat model.\n        \"\"\"\n        if self.azure_endpoint:\n            return AzureChatOpenAI(\n                azure_endpoint=self.azure_endpoint,\n                openai_api_version=os.getenv(\n                    \"OPENAI_API_VERSION\", \"2024-05-01-preview\"\n                ),\n                deployment_name=self.model_name,\n                openai_api_type=\"azure\",\n                streaming=self.streaming,\n                callbacks=[StreamingStdOutCallbackHandler()],\n            )\n        elif \"claude\" in self.model_name:\n            return ChatAnthropic(\n                model=self.model_name,\n                temperature=self.temperature,\n                callbacks=[StreamingStdOutCallbackHandler()],\n                streaming=self.streaming,\n                max_tokens_to_sample=4096,\n            )\n        elif self.vision:\n            return ChatOpenAI(\n                model=self.model_name,\n                temperature=self.temperature,\n                streaming=self.streaming,\n                callbacks=[StreamingStdOutCallbackHandler()],\n                max_tokens=4096,  # vision models default to low max token limits\n            )\n        else:\n            return ChatOpenAI(\n                model=self.model_name,\n                temperature=self.temperature,\n                streaming=self.streaming,\n                callbacks=[StreamingStdOutCallbackHandler()],\n            )\n\n\ndef serialize_messages(messages: List[Message]) -> str:\n    return AI.serialize_messages(messages)\n\n\nclass ClipboardAI(AI):\n    # Ignore not init superclass\n    def __init__(self, **_):  # type: ignore\n        self.vision = False\n        self.token_usage_log = TokenUsageLog(\"clipboard_llm\")\n\n    @staticmethod\n    def serialize_messages(messages: List[Message]) -> str:\n        return \"\\n\\n\".join([f\"{m.type}:\\n{m.content}\" for m in messages])\n\n    @staticmethod\n    def multiline_input():\n        print(\"Enter/Paste your content. Ctrl-D or Ctrl-Z ( windows ) to save it.\")\n        content = []\n        while True:\n            try:\n                line = input()\n            except EOFError:\n                break\n            content.append(line)\n        return \"\\n\".join(content)\n\n    def next(\n        self,\n        messages: List[Message],\n        prompt: Optional[str] = None,\n        *,\n        step_name: str,\n    ) -> List[Message]:\n        \"\"\"\n        Not yet fully supported\n        \"\"\"\n        if prompt:\n            messages.append(HumanMessage(content=prompt))\n\n        logger.debug(f\"Creating a new chat completion: {messages}\")\n\n        msgs = self.serialize_messages(messages)\n        pyperclip.copy(msgs)\n        Path(\"clipboard.txt\").write_text(msgs)\n        print(\n            \"Messages copied to clipboard and written to clipboard.txt,\",\n            len(msgs),\n            \"characters in total\",\n        )\n\n        response = self.multiline_input()\n\n        messages.append(AIMessage(content=response))\n        logger.debug(f\"Chat completion finished: {messages}\")\n\n        return messages\n", "gpt_engineer/core/linting.py": "import black\n\nfrom gpt_engineer.core.files_dict import FilesDict\n\n\nclass Linting:\n    def __init__(self):\n        # Dictionary to hold linting methods for different file types\n        self.linters = {\".py\": self.lint_python}\n\n    import black\n\n    def lint_python(self, content, config):\n        \"\"\"Lint Python files using the `black` library, handling all exceptions silently and logging them.\n        This function attempts to format the code and returns the formatted code if successful.\n        If any error occurs during formatting, it logs the error and returns the original content.\n        \"\"\"\n        try:\n            # Try to format the content using black\n            linted_content = black.format_str(content, mode=black.FileMode(**config))\n        except black.NothingChanged:\n            # If nothing changed, log the info and return the original content\n            print(\"\\nInfo: No changes were made during formatting.\\n\")\n            linted_content = content\n        except Exception as error:\n            # If any other exception occurs, log the error and return the original content\n            print(f\"\\nError: Could not format due to {error}\\n\")\n            linted_content = content\n        return linted_content\n\n    def lint_files(self, files_dict: FilesDict, config: dict = None) -> FilesDict:\n        \"\"\"\n        Lints files based on their extension using registered linting functions.\n\n        Parameters\n        ----------\n        files_dict : FilesDict\n            The dictionary of file names to their respective source code content.\n        config : dict, optional\n            A dictionary of configuration options for the linting tools.\n\n        Returns\n        -------\n        FilesDict\n            The dictionary of file names to their respective source code content after linting.\n        \"\"\"\n        if config is None:\n            config = {}\n\n        for filename, content in files_dict.items():\n            extension = filename[\n                filename.rfind(\".\") :\n            ].lower()  # Ensure case insensitivity\n            if extension in self.linters:\n                original_content = content\n                linted_content = self.linters[extension](content, config)\n                if linted_content != original_content:\n                    print(f\"Linted {filename}.\")\n                else:\n                    print(f\"No changes made for {filename}.\")\n                files_dict[filename] = linted_content\n            else:\n                print(f\"No linter registered for {filename}.\")\n        return files_dict\n", "gpt_engineer/core/diff.py": "\"\"\"\nFile Overview:\n\nThis Python module is designed for processing and analyzing diffs in source code files. Diffs represent the changes between two versions of a file, which are crucial in version control systems for tracking file modifications. The module focuses on the detailed examination of these diffs, enabling users to understand, validate, and correct changes between file versions.\n\nKey Features:\n\n1. The `Hunk` class encapsulates a contiguous block of changes within a file. It includes detailed information such as start lines before and after edits, lengths of change blocks, and specific line changes categorized as additions, deletions, or unchanged.\n\n2. The `Diff` class represents a complete set of changes across a file and may contain multiple `Hunk` objects. It facilitates operations like generating string representations of diffs, and validating and correcting hunks based on the original file content.\n\n3. Functions within the module allow for the validation of hunks against original files, identifying mismatches, and making necessary corrections. This feature ensures that diffs are accurate and reflect true changes.\n\n4. Utility functions `is_similar` and `count_ratio` offer the capability to compare strings for similarity, accounting for variations in spacing and case. This aids in the validation process by allowing a flexible comparison of code lines.\n\nDependencies:\n\n- `logging`: Utilized for logging warnings and errors encountered during the validation and correction process.\n- `collections.Counter`: Used for counting occurrences of characters in strings, supporting the string similarity assessment functions.\n\nFunctions and Classes:\n\n1. `Hunk`: Class representing a block of changes within a file, with methods for managing and validating these changes.\n\n2. `Diff`: Class representing the entire set of changes in a file, containing multiple `Hunk` instances and methods for overall diff management.\n\n3. `is_similar(str1, str2, similarity_threshold)`: Function to compare two strings for similarity, useful in validating line changes in hunks.\n\n4. `count_ratio(str1, str2)`: Function that computes the ratio of common characters to the length of the longer string, aiding in the assessment of line similarity.\n\nThis module is essential for developers and teams utilizing version control systems, providing tools for a deeper analysis and correction of diffs, ensuring the integrity and accuracy of code changes.\n\n\"\"\"\nimport logging\n\nfrom collections import Counter\nfrom typing import List\n\nRETAIN = \"retain\"\nADD = \"add\"\nREMOVE = \"remove\"\n\n\nclass Hunk:\n    \"\"\"\n    Represents a section of a file diff, containing changes made to that section.\n\n    Attributes:\n        start_line_pre_edit (int): The starting line number in the original file.\n        hunk_len_pre_edit (int): The length of the hunk in the original file.\n        start_line_post_edit (int): The starting line number in the edited file.\n        hunk_len_post_edit (int): The length of the hunk in the edited file.\n        lines (list): A list of tuples representing the lines in the hunk and their types (RETAIN, ADD, REMOVE).\n        category_counts (dict): A count of lines by their type.\n        is_new_file (bool): Flag indicating if the hunk represents a new file.\n    \"\"\"\n\n    def __init__(\n        self,\n        start_line_pre_edit,\n        hunk_len_pre_edit,\n        start_line_post_edit,\n        hunk_len_post_edit,\n        lines,\n    ) -> None:\n        self.start_line_pre_edit = start_line_pre_edit\n        self.hunk_len_pre_edit = hunk_len_pre_edit\n        self.start_line_post_edit = start_line_post_edit\n        self.hunk_len_post_edit = hunk_len_post_edit\n        self.category_counts = {RETAIN: 0, ADD: 0, REMOVE: 0}\n        self.lines = list()\n        self.add_lines(lines)\n        self.forward_block_len = 10\n        # Note that this assumption should not be done on hunk level, however, if the below is true, no validation is possible anyway.\n        if self.category_counts[RETAIN] == 0 and self.category_counts[REMOVE] == 0:\n            self.is_new_file = True\n        else:\n            self.is_new_file = False\n\n    def add_retained_line(self, line, index) -> None:\n        \"\"\"Adds a retained line to the hunk at the specified index.\"\"\"\n        self.lines.insert(index, (RETAIN, line))\n        self.category_counts[RETAIN] += 1\n\n    def relabel_line(self, index, new_label) -> None:\n        \"\"\"Changes the label of a line at the specified index.\"\"\"\n        old_label = self.lines[index][0]\n        self.lines[index] = (new_label, self.lines[index][1])\n        self.category_counts[old_label] -= 1\n        self.category_counts[new_label] += 1\n\n    def pop_line(self, line, index) -> None:\n        \"\"\"Removes a line from the hunk at the specified index.\"\"\"\n        self.lines.pop(index)\n        assert self.category_counts[line[0]] > 0\n        self.category_counts[line[0]] -= 1\n\n    def add_lines(self, new_lines) -> None:\n        \"\"\"Adds multiple lines to the hunk.\"\"\"\n        for line in new_lines:\n            self.lines.append(line)\n            self.category_counts[line[0]] += 1\n\n    def hunk_to_string(self) -> str:\n        \"\"\"Converts the hunk to a string representation.\"\"\"\n        string = f\"@@ -{self.start_line_pre_edit},{self.hunk_len_pre_edit} +{self.start_line_post_edit},{self.hunk_len_post_edit} @@\\n\"\n        for line_type, line_content in self.lines:\n            line_prefix = (\n                \" \" if line_type == RETAIN else \"+\" if line_type == ADD else \"-\"\n            )\n            string += f\"{line_prefix}{line_content}\\n\"\n        return string\n\n    def make_forward_block(self, hunk_ind: int, forward_block_len) -> str:\n        \"\"\"Creates a block of lines for forward comparison.\"\"\"\n        forward_lines = [\n            line[1] for line in self.lines[hunk_ind:] if not line[0] == ADD\n        ]\n        forward_block = \"\\n\".join(forward_lines[0:forward_block_len])\n        return forward_block\n\n    def check_start_line(self, lines_dict: dict) -> bool:\n        \"\"\"Check if the starting line of a hunk is present in the original code and returns a boolean value accordingly.\"\"\"\n        if self.is_new_file:\n            # this hunk cannot be falsified and is by definition true\n            return True\n        if self.start_line_pre_edit in lines_dict:\n            # check the location of the actual starting line:\n            is_similar(self.lines[0][1], lines_dict[self.start_line_pre_edit])\n        else:\n            pass\n\n    def find_start_line(self, lines_dict: dict, problems: list) -> bool:\n        \"\"\"Finds the starting line of the hunk in the original code and returns a boolean value accordingly. If the starting line is not found, it appends a problem message to the problems list.\"\"\"\n\n        # ToDo handle the case where the start line is 0 or 1 characters separately\n        if self.lines[0][0] == ADD:\n            # handle the case where the start line is an add\n            start_line = None\n            # find the first line that is not an add\n            for index, line in enumerate(self.lines):\n                if line[0] != ADD:\n                    for line_number, line_content in lines_dict.items():\n                        # if the line is similar to a non-blank line in line_dict, we can pick the line prior to it\n                        if is_similar(line[1], line_content) and line[1] != \"\":\n                            start_line = line_number - 1\n                            break\n                    # if the start line is not found, append a problem message\n                    if start_line is None:\n                        problems.append(\n                            f\"In {self.hunk_to_string()}:can not find the starting line of the diff\"\n                        )\n                        return False\n\n                    else:\n                        # the line prior to the start line is found now we insert it to the first place as the start line\n                        self.start_line_pre_edit = start_line\n                        retain_line = lines_dict.get(start_line, \"\")\n                        if retain_line:\n                            self.add_retained_line(lines_dict[start_line], 0)\n                            return self.validate_and_correct(lines_dict, problems)\n                        else:\n                            problems.append(\n                                f\"In {self.hunk_to_string()}:The starting line of the diff {self.hunk_to_string()} does not exist in the code\"\n                            )\n                            return False\n        pot_start_lines = {\n            key: is_similar(self.lines[0][1], line) for key, line in lines_dict.items()\n        }\n        sum_of_matches = sum(pot_start_lines.values())\n        if sum_of_matches == 0:\n            # before we go any further, we should check if it's a comment from LLM\n            if self.lines[0][1].count(\"#\") > 0:\n                # if it is, we can mark it as an ADD lines\n                self.relabel_line(0, ADD)\n                # and restart the validation at the next line\n                return self.validate_and_correct(lines_dict, problems)\n\n            else:\n                problems.append(\n                    f\"In {self.hunk_to_string()}:The starting line of the diff {self.hunk_to_string()} does not exist in the code\"\n                )\n                return False\n        elif sum_of_matches == 1:\n            start_ind = list(pot_start_lines.keys())[\n                list(pot_start_lines.values()).index(True)\n            ]  # lines are one indexed\n        else:\n            logging.warning(\"multiple candidates for starting index\")\n            # ToDo handle all the cases better again here. Smartest choice is that, for each candidate check match to the next line etc (recursively)\n            start_ind = list(pot_start_lines.keys())[\n                list(pot_start_lines.values()).index(True)\n            ]\n        self.start_line_pre_edit = start_ind\n\n        # This should now be fulfilled by default\n        assert is_similar(self.lines[0][1], lines_dict[self.start_line_pre_edit])\n        return True\n\n    def validate_lines(self, lines_dict: dict, problems: list) -> bool:\n        \"\"\"Validates the lines of the hunk against the original file and returns a boolean value accordingly. If the lines do not match, it appends a problem message to the problems list.\"\"\"\n        hunk_ind = 0\n        file_ind = self.start_line_pre_edit\n        # make an orig hunk lines for logging\n        # orig_hunk_lines = deepcopy(self.lines)\n        while hunk_ind < len(self.lines) and file_ind <= max(lines_dict):\n            if self.lines[hunk_ind][0] == ADD:\n                # this cannot be validated, jump one index\n                hunk_ind += 1\n            elif not is_similar(self.lines[hunk_ind][1], lines_dict[file_ind]):\n                # before we go any further, we should relabel the comment from LLM\n                if self.lines[hunk_ind][1].count(\"#\") > 0:\n                    self.relabel_line(hunk_ind, ADD)\n                    continue\n\n                # make a forward block from the code for comparisons\n                forward_code = \"\\n\".join(\n                    [\n                        lines_dict[ind]\n                        for ind in range(\n                            file_ind,\n                            min(\n                                file_ind + self.forward_block_len,\n                                max(lines_dict.keys()),\n                            ),\n                        )\n                    ]\n                )\n                # make the original forward block for quantitative comparison\n                forward_block = self.make_forward_block(\n                    hunk_ind, self.forward_block_len\n                )\n                orig_count_ratio = count_ratio(forward_block, forward_code)\n                # Here we have 2 cases\n                # 1) some lines were simply skipped in the diff and we should add them to the diff\n                # If this is the case, adding the line to the diff, should give an improved forward diff\n                forward_block_missing_line = self.make_forward_block(\n                    hunk_ind, self.forward_block_len - 1\n                )\n                # insert the missing line in front of the block\n                forward_block_missing_line = \"\\n\".join(\n                    [lines_dict[file_ind], forward_block_missing_line]\n                )\n                missing_line_count_ratio = count_ratio(\n                    forward_block_missing_line, forward_code\n                )\n                # 2) Additional lines, not belonging to the code were added to the diff\n                forward_block_false_line = self.make_forward_block(\n                    hunk_ind + 1, self.forward_block_len\n                )\n                false_line_count_ratio = count_ratio(\n                    forward_block_false_line, forward_code\n                )\n                if (\n                    orig_count_ratio >= missing_line_count_ratio\n                    and orig_count_ratio >= false_line_count_ratio\n                ):\n                    problems.append(\n                        f\"In Hunk:{self.hunk_to_string()}, there was at least one mismatch.\"\n                    )\n                    return False\n\n                elif missing_line_count_ratio > false_line_count_ratio:\n                    self.add_retained_line(lines_dict[file_ind], hunk_ind)\n                    hunk_ind += 1\n                    file_ind += 1\n                    # NOTE: IF THE LLM SKIPS SOME LINES AND HAS ADDs ADJACENT TO THE SKIPPED BLOCK,\n                    # WE CANNOT KNOW WHETHER THE ADDs SHOULD BE BEFORE OR AFTER THE BLOCK. WE OPT FOR PUTTING IT BEFORE.\n                    # IF IT MATTERED, WE ASSUME THE LLM WOULD NOT SKIP THE BLOCK\n                else:\n                    self.pop_line(self.lines[hunk_ind], hunk_ind)\n\n            else:\n                hunk_ind += 1\n                file_ind += 1\n        # if we have not validated all lines, we have a problem\n        if hunk_ind < len(self.lines) - 1:\n            remaining_lines = \"\\n\".join(\n                f\"{line_type}: {line_content}\"\n                for line_type, line_content in self.lines[file_ind + 1 :]\n            )\n            problems.append(\n                f\"In {self.hunk_to_string()}:Hunk validation stopped before the lines {remaining_lines} were validated. The diff is incorrect\"\n            )\n            return False\n        return True\n\n    def validate_and_correct(\n        self,\n        lines_dict: dict,\n        problems: list,\n    ) -> bool:\n        \"\"\"\n        Validates and corrects the hunk based on the original lines.\n\n        This function attempts to validate the hunk by comparing its lines to the original file and making corrections\n        where necessary. It also identifies problems such as non-matching lines or incorrect line types.\n        \"\"\"\n        start_true = self.check_start_line(lines_dict)\n\n        if not start_true:\n            if not self.find_start_line(lines_dict, problems):\n                return False\n\n        # Now we should be able to validate the hunk line by line and add missing line\n        if not self.validate_lines(lines_dict, problems):\n            return False\n        # Pass the validation\n        return True\n\n\nclass Diff:\n    \"\"\"\n    Represents a file diff, containing multiple hunks of changes.\n\n    Attributes:\n        filename_pre (str): The name of the original file.\n        filename_post (str): The name of the edited file.\n        hunks (list): A list of Hunk objects representing the changes in the diff.\n    \"\"\"\n\n    def __init__(self, filename_pre, filename_post) -> None:\n        self.filename_pre = filename_pre\n        self.filename_post = filename_post\n        self.hunks = []\n\n    def is_new_file(self) -> bool:\n        \"\"\"Determines if the diff represents a new file.\"\"\"\n        if self.filename_pre == \"/dev/null\":\n            return True\n        return any(hunk.is_new_file for hunk in self.hunks)\n\n    def diff_to_string(self) -> str:\n        \"\"\"Converts the diff to a string representation.\"\"\"\n        string = f\"--- {self.filename_pre}\\n+++ {self.filename_post}\\n\"\n        for hunk in self.hunks:\n            string += hunk.hunk_to_string()\n        return string.strip()\n\n    def validate_and_correct(self, lines_dict: dict) -> List[str]:\n        \"\"\"Validates and corrects each hunk in the diff.\"\"\"\n        problems = []\n        past_hunk = None\n        cut_lines_dict = lines_dict.copy()\n        for hunk in self.hunks:\n            if past_hunk is not None:\n                # make sure to not cut so much that the start_line gets out of range\n                cut_ind = min(\n                    past_hunk.start_line_pre_edit + past_hunk.hunk_len_pre_edit,\n                    hunk.start_line_pre_edit,\n                )\n                cut_lines_dict = {\n                    key: val for key, val in cut_lines_dict.items() if key >= (cut_ind)\n                }\n            is_valid = hunk.validate_and_correct(cut_lines_dict, problems)\n            if not is_valid and len(problems) > 0:\n                for idx, val in enumerate(problems):\n                    print(f\"\\nInvalid Hunk NO.{idx}---\\n{val}\\n---\")\n                self.hunks.remove(hunk)\n            # now correct the numbers, assuming the start line pre-edit has been fixed\n            hunk.hunk_len_pre_edit = (\n                hunk.category_counts[RETAIN] + hunk.category_counts[REMOVE]\n            )\n            hunk.hunk_len_post_edit = (\n                hunk.category_counts[RETAIN] + hunk.category_counts[ADD]\n            )\n            if past_hunk is not None:\n                hunk.start_line_post_edit = (\n                    hunk.start_line_pre_edit\n                    + past_hunk.hunk_len_post_edit\n                    - past_hunk.hunk_len_pre_edit\n                    + past_hunk.start_line_post_edit\n                    - past_hunk.start_line_pre_edit\n                )\n            else:\n                hunk.start_line_post_edit = hunk.start_line_pre_edit\n            past_hunk = hunk\n        return problems\n\n\ndef is_similar(str1, str2, similarity_threshold=0.9) -> bool:\n    \"\"\"\n    Compares two strings for similarity, ignoring spaces and case.\n\n    Parameters\n    ----------\n    str1, str2 : str\n        The strings to compare.\n    similarity_threshold: float\n        How similar must the strings be\n\n    Returns\n    -------\n    bool\n        True if the strings are similar, False otherwise.\n    \"\"\"\n\n    return count_ratio(str1, str2) >= similarity_threshold\n\n\ndef count_ratio(str1, str2) -> float:\n    \"\"\"\n    Computes the ratio of common characters to the length of the longer string, ignoring spaces and case.\n\n    Parameters:\n    - str1, str2 (str): The strings to compare.\n\n    Returns:\n    - float: The ratio of common characters to the length of the longer string.\n    \"\"\"\n    str1, str2 = str1.replace(\" \", \"\").lower(), str2.replace(\" \", \"\").lower()\n\n    counter1, counter2 = Counter(str1), Counter(str2)\n    intersection = sum((counter1 & counter2).values())\n    longer_length = max(len(str1), len(str2))\n    if longer_length == 0:\n        return 1\n    else:\n        return intersection / longer_length\n", "gpt_engineer/core/base_memory.py": "\"\"\"\nBase Memory Module\n\nThis module provides a type alias for a mutable mapping that represents the base memory structure\nused in the GPT Engineer project. The base memory is a mapping from file names (as strings or Path objects)\nto their corresponding code content (as strings).\n\nType Aliases:\n    BaseMemory: A mutable mapping from file names to code content.\n\"\"\"\n\nfrom pathlib import Path\nfrom typing import MutableMapping, Union\n\nBaseMemory = MutableMapping[Union[str, Path], str]\n", "gpt_engineer/core/files_dict.py": "\"\"\"\nFilesDict Module\n\nThis module provides a FilesDict class which is a dictionary-based container for managing code files.\nIt extends the standard dictionary to enforce string keys and values, representing filenames and their\ncorresponding code content. It also provides methods to format its contents for chat-based interaction\nwith an AI agent and to enforce type checks on keys and values.\n\nClasses:\n    FilesDict: A dictionary-based container for managing code files.\n\"\"\"\nfrom collections import OrderedDict\nfrom pathlib import Path\nfrom typing import Union\n\n\n# class Code(MutableMapping[str | Path, str]):\n# ToDo: implement as mutable mapping, potentially holding a dict instead of being a dict.\nclass FilesDict(dict):\n    \"\"\"\n    A dictionary-based container for managing code files.\n\n    This class extends the standard dictionary to enforce string keys and values,\n    representing filenames and their corresponding code content. It provides methods\n    to format its contents for chat-based interaction with an AI agent and to enforce\n    type checks on keys and values.\n    \"\"\"\n\n    def __setitem__(self, key: Union[str, Path], value: str):\n        \"\"\"\n        Set the code content for the given filename, enforcing type checks on the key and value.\n\n        Overrides the dictionary's __setitem__ to enforce type checks on the key and value.\n        The key must be a string or a Path object, and the value must be a string representing\n        the code content.\n\n        Parameters\n        ----------\n        key : Union[str, Path]\n            The filename as a key for the code content.\n        value : str\n            The code content to associate with the filename.\n\n        Raises\n        ------\n        TypeError\n            If the key is not a string or Path, or if the value is not a string.\n        \"\"\"\n        if not isinstance(key, (str, Path)):\n            raise TypeError(\"Keys must be strings or Path's\")\n        if not isinstance(value, str):\n            raise TypeError(\"Values must be strings\")\n        super().__setitem__(key, value)\n\n    def to_chat(self):\n        \"\"\"\n        Formats the items of the object (assuming file name and content pairs)\n        into a string suitable for chat display.\n\n        Returns\n        -------\n        str\n            A string representation of the files.\n        \"\"\"\n        chat_str = \"\"\n        for file_name, file_content in self.items():\n            lines_dict = file_to_lines_dict(file_content)\n            chat_str += f\"File: {file_name}\\n\"\n            for line_number, line_content in lines_dict.items():\n                chat_str += f\"{line_number} {line_content}\\n\"\n            chat_str += \"\\n\"\n        return f\"```\\n{chat_str}```\"\n\n    def to_log(self):\n        \"\"\"\n        Formats the items of the object (assuming file name and content pairs)\n        into a string suitable for log display.\n\n        Returns\n        -------\n        str\n            A string representation of the files.\n        \"\"\"\n        log_str = \"\"\n        for file_name, file_content in self.items():\n            log_str += f\"File: {file_name}\\n\"\n            log_str += file_content\n            log_str += \"\\n\"\n        return log_str\n\n\ndef file_to_lines_dict(file_content: str) -> dict:\n    \"\"\"\n    Converts file content into a dictionary where each line number is a key\n    and the corresponding line content is the value.\n\n    Parameters\n    ----------\n    file_name : str\n        The name of the file.\n    file_content : str\n        The content of the file.\n\n    Returns\n    -------\n    dict\n        A dictionary with file names as keys and dictionaries (line numbers as keys and line contents as values) as values.\n    \"\"\"\n    lines_dict = OrderedDict(\n        {\n            line_number: line_content\n            for line_number, line_content in enumerate(file_content.split(\"\\n\"), 1)\n        }\n    )\n    return lines_dict\n", "gpt_engineer/core/git.py": "import shutil\nimport subprocess\n\nfrom pathlib import Path\nfrom typing import List\n\nfrom gpt_engineer.core.files_dict import FilesDict\n\n\ndef is_git_installed():\n    return shutil.which(\"git\") is not None\n\n\ndef is_git_repo(path: Path):\n    return (\n        subprocess.run(\n            [\"git\", \"rev-parse\", \"--is-inside-work-tree\"],\n            cwd=path,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n        ).returncode\n        == 0\n    )\n\n\ndef init_git_repo(path: Path):\n    subprocess.run([\"git\", \"init\"], cwd=path)\n\n\ndef has_uncommitted_changes(path: Path):\n    return bool(\n        subprocess.run(\n            [\"git\", \"diff\", \"--exit-code\"],\n            cwd=path,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n        ).returncode\n    )\n\n\ndef filter_files_with_uncommitted_changes(\n    basepath: Path, files_dict: FilesDict\n) -> List[Path]:\n    files_with_diff = (\n        subprocess.run(\n            [\"git\", \"diff\", \"--name-only\"], cwd=basepath, stdout=subprocess.PIPE\n        )\n        .stdout.decode()\n        .splitlines()\n    )\n    return [f for f in files_dict.keys() if f in files_with_diff]\n\n\ndef stage_files(path: Path, files: List[str]):\n    subprocess.run([\"git\", \"add\", *files], cwd=path)\n\n\ndef filter_by_gitignore(path: Path, file_list: List[str]) -> List[str]:\n    out = subprocess.run(\n        [\"git\", \"-C\", \".\", \"check-ignore\", \"--no-index\", \"--stdin\"],\n        cwd=path,\n        input=\"\\n\".join(file_list).encode(),\n        stdout=subprocess.PIPE,\n        stderr=subprocess.PIPE,\n    )\n    paths = out.stdout.decode().splitlines()\n    # return file_list but filter out the results from git check-ignore\n    return [f for f in file_list if f not in paths]\n\n\ndef stage_uncommitted_to_git(path, files_dict, improve_mode):\n    # Check if there's a git repo and verify that there aren't any uncommitted changes\n    if is_git_installed() and not improve_mode:\n        if not is_git_repo(path):\n            print(\"\\nInitializing an empty git repository\")\n            init_git_repo(path)\n\n    if is_git_repo(path):\n        modified_files = filter_files_with_uncommitted_changes(path, files_dict)\n        if modified_files:\n            print(\n                \"Staging the following uncommitted files before overwriting: \",\n                \", \".join(modified_files),\n            )\n            stage_files(path, modified_files)\n", "gpt_engineer/core/version_manager.py": "\"\"\"\nVersion Manager Module\n\nThis module provides an abstract base class for a version manager that handles the creation of snapshots\nfor code. Implementations of this class are expected to provide methods to create a snapshot of the given\ncode and return a reference to it.\n\"\"\"\nfrom abc import ABC, abstractmethod\nfrom pathlib import Path\nfrom typing import Union\n\nfrom gpt_engineer.core.files_dict import FilesDict\n\n\nclass BaseVersionManager(ABC):\n    \"\"\"\n    Abstract base class for a version manager.\n\n    Defines the interface for version managers that handle the creation of snapshots for code.\n    Implementations of this class are expected to provide methods to create a snapshot of the given\n    code and return a reference to it.\n    \"\"\"\n\n    @abstractmethod\n    def __init__(self, path: Union[str, Path]):\n        pass\n\n    @abstractmethod\n    def snapshot(self, files_dict: FilesDict) -> str:\n        pass\n", "gpt_engineer/core/chat_to_files.py": "\"\"\"\nThis Python script provides functionalities for parsing chat transcripts that contain file paths and code blocks,\napplying diffs to these files, and parsing unified git diff format strings. The script is designed to work within\na larger system that involves processing and manipulating code files based on chat inputs and diff information.\n\nKey Components:\n- chat_to_files_dict: Parses a chat transcript, extracting file paths and associated code blocks, and organizes\n  them into a FilesDict object, which is a custom dictionary format designed to hold file contents keyed by their paths.\n\n- apply_diffs: Takes a dictionary of Diff objects (which represent changes to be made to files) and a FilesDict\n  object containing the current state of files. It applies the changes described by the Diff objects to the\n  corresponding files in the FilesDict, updating the file contents as specified by the diffs.\n\n- parse_diffs: Parses a string containing diffs in the unified git diff format, extracting the changes described\n  in the diffs and organizing them into a dictionary of Diff objects, keyed by the filename to which each diff applies.\n\n- parse_diff_block: Parses a single block of text from a diff string, translating it into a Diff object that\n  represents the changes described in that block of text.\n\nThis script is intended for use in environments where code collaboration or review is conducted through chat interfaces,\nallowing for the dynamic application of changes to code bases and the efficient handling of file and diff information in chat transcripts.\n\"\"\"\n\nimport logging\nimport re\n\nfrom typing import Dict, Tuple\n\nfrom regex import regex\n\nfrom gpt_engineer.core.diff import ADD, REMOVE, RETAIN, Diff, Hunk\nfrom gpt_engineer.core.files_dict import FilesDict, file_to_lines_dict\n\n# Initialize a logger for this module\nlogger = logging.getLogger(__name__)\n\n\ndef chat_to_files_dict(chat: str) -> FilesDict:\n    \"\"\"\n    Converts a chat string containing file paths and code blocks into a FilesDict object.\n\n    Args:\n    - chat (str): The chat string containing file paths and code blocks.\n\n    Returns:\n    - FilesDict: A dictionary with file paths as keys and code blocks as values.\n    \"\"\"\n    # Regex to match file paths and associated code blocks\n    regex = r\"(\\S+)\\n\\s*```[^\\n]*\\n(.+?)```\"\n    matches = re.finditer(regex, chat, re.DOTALL)\n\n    files_dict = FilesDict()\n    for match in matches:\n        # Clean and standardize the file path\n        path = re.sub(r'[\\:<>\"|?*]', \"\", match.group(1))\n        path = re.sub(r\"^\\[(.*)\\]$\", r\"\\1\", path)\n        path = re.sub(r\"^`(.*)`$\", r\"\\1\", path)\n        path = re.sub(r\"[\\]\\:]$\", \"\", path)\n\n        # Extract and clean the code content\n        content = match.group(2)\n\n        # Add the cleaned path and content to the FilesDict\n        files_dict[path.strip()] = content.strip()\n\n    return files_dict\n\n\ndef apply_diffs(diffs: Dict[str, Diff], files: FilesDict) -> FilesDict:\n    \"\"\"\n    Applies diffs to the provided files.\n\n    Args:\n    - diffs (Dict[str, Diff]): A dictionary of diffs to apply, keyed by filename.\n    - files (FilesDict): The original files to which diffs will be applied.\n\n    Returns:\n    - FilesDict: The updated files after applying diffs.\n    \"\"\"\n    files = FilesDict(files.copy())\n    REMOVE_FLAG = \"<REMOVE_LINE>\"  # Placeholder to mark lines for removal\n    for diff in diffs.values():\n        if diff.is_new_file():\n            # If it's a new file, create it with the content from the diff\n            files[diff.filename_post] = \"\\n\".join(\n                line[1] for hunk in diff.hunks for line in hunk.lines\n            )\n        else:\n            # Convert the file content to a dictionary of lines\n            line_dict = file_to_lines_dict(files[diff.filename_pre])\n            for hunk in diff.hunks:\n                current_line = hunk.start_line_pre_edit\n                for line in hunk.lines:\n                    if line[0] == RETAIN:\n                        current_line += 1\n                    elif line[0] == ADD:\n                        # Handle added lines\n                        current_line -= 1\n                        if (\n                            current_line in line_dict.keys()\n                            and line_dict[current_line] != REMOVE_FLAG\n                        ):\n                            line_dict[current_line] += \"\\n\" + line[1]\n                        else:\n                            line_dict[current_line] = line[1]\n                        current_line += 1\n                    elif line[0] == REMOVE:\n                        # Mark removed lines with REMOVE_FLAG\n                        line_dict[current_line] = REMOVE_FLAG\n                        current_line += 1\n\n            # Remove lines marked for removal\n            line_dict = {\n                key: line_content\n                for key, line_content in line_dict.items()\n                if REMOVE_FLAG not in line_content\n            }\n            # Reassemble the file content\n            files[diff.filename_post] = \"\\n\".join(line_dict.values())\n    return files\n\n\ndef parse_diffs(diff_string: str) -> dict:\n    \"\"\"\n    Parses a diff string in the unified git diff format.\n\n    Args:\n    - diff_string (str): The diff string to parse.\n\n    Returns:\n    - dict: A dictionary of Diff objects keyed by filename.\n    \"\"\"\n    # Regex to match individual diff blocks\n    diff_block_pattern = regex.compile(\n        r\"```.*?\\n\\s*?--- .*?\\n\\s*?\\+\\+\\+ .*?\\n(?:@@ .*? @@\\n(?:[-+ ].*?\\n)*?)*?```\",\n        re.DOTALL,\n    )\n\n    diffs = {}\n    try:\n        for block in diff_block_pattern.finditer(diff_string, timeout=1):\n            diff_block = block.group()\n\n            # Parse individual diff blocks and update the diffs dictionary\n            diffs.update(parse_diff_block(diff_block))\n    except TimeoutError:\n        print(\"gpt-engineer timed out while parsing git diff\")\n\n    if not diffs:\n        print(\n            \"GPT did not provide any proposed changes. Please try to reselect the files for uploading and edit your prompt file.\"\n        )\n\n    return diffs\n\n\ndef parse_diff_block(diff_block: str) -> dict:\n    \"\"\"\n    Parses a block of diff text into a Diff object.\n\n    Args:\n    - diff_block (str): A single block of diff text.\n\n    Returns:\n    - dict: A dictionary containing a single Diff object keyed by the post-edit filename.\n    \"\"\"\n    lines = diff_block.strip().split(\"\\n\")[1:-1]  # Exclude the opening and closing ```\n    diffs = {}\n    current_diff = None\n    hunk_lines = []\n    filename_pre = None\n    filename_post = None\n    hunk_header = None\n\n    for line in lines:\n        if line.startswith(\"--- \"):\n            # Pre-edit filename\n            filename_pre = line[4:]\n        elif line.startswith(\"+++ \"):\n            # Post-edit filename and initiation of a new Diff object\n            if (\n                filename_post is not None\n                and current_diff is not None\n                and hunk_header is not None\n            ):\n                current_diff.hunks.append(Hunk(*hunk_header, hunk_lines))\n                hunk_lines = []\n            filename_post = line[4:]\n            current_diff = Diff(filename_pre, filename_post)\n            diffs[filename_post] = current_diff\n        elif line.startswith(\"@@ \"):\n            # Start of a new hunk in the diff\n            if hunk_lines and current_diff is not None and hunk_header is not None:\n                current_diff.hunks.append(Hunk(*hunk_header, hunk_lines))\n                hunk_lines = []\n            hunk_header = parse_hunk_header(line)\n        elif line.startswith(\"+\"):\n            # Added line\n            hunk_lines.append((ADD, line[1:]))\n        elif line.startswith(\"-\"):\n            # Removed line\n            hunk_lines.append((REMOVE, line[1:]))\n        else:\n            # Retained line\n            hunk_lines.append((RETAIN, line[1:]))\n\n    # Append the last hunk if any\n    if current_diff is not None and hunk_lines and hunk_header is not None:\n        current_diff.hunks.append(Hunk(*hunk_header, hunk_lines))\n\n    return diffs\n\n\ndef parse_hunk_header(header_line) -> Tuple[int, int, int, int]:\n    \"\"\"\n    Parses the header of a hunk from a diff.\n\n    Args:\n    - header_line (str): The header line of a hunk.\n\n    Returns:\n    - tuple: A tuple containing start and length information for pre- and post-edit.\n    \"\"\"\n    pattern = re.compile(r\"^@@ -\\d{1,},\\d{1,} \\+\\d{1,},\\d{1,} @@$\")\n\n    if not pattern.match(header_line):\n        # Return a default value if the header does not match the expected format\n        return 0, 0, 0, 0\n\n    pre, post = header_line.split(\" \")[1:3]\n    start_line_pre_edit, hunk_len_pre_edit = map(int, pre[1:].split(\",\"))\n    start_line_post_edit, hunk_len_post_edit = map(int, post[1:].split(\",\"))\n    return (\n        start_line_pre_edit,\n        hunk_len_pre_edit,\n        start_line_post_edit,\n        hunk_len_post_edit,\n    )\n", "gpt_engineer/core/preprompts_holder.py": "from pathlib import Path\nfrom typing import Dict\n\nfrom gpt_engineer.core.default.disk_memory import DiskMemory\n\n\nclass PrepromptsHolder:\n    \"\"\"\n    A holder for preprompt texts that are stored on disk.\n\n    This class provides methods to retrieve preprompt texts from a specified directory.\n\n    Attributes\n    ----------\n    preprompts_path : Path\n        The file path to the directory containing preprompt texts.\n\n    Methods\n    -------\n    get_preprompts() -> Dict[str, str]\n        Retrieve all preprompt texts from the directory and return them as a dictionary.\n    \"\"\"\n\n    def __init__(self, preprompts_path: Path):\n        self.preprompts_path = preprompts_path\n\n    def get_preprompts(self) -> Dict[str, str]:\n        preprompts_repo = DiskMemory(self.preprompts_path)\n        return {file_name: preprompts_repo[file_name] for file_name in preprompts_repo}\n", "gpt_engineer/core/prompt.py": "import json\n\nfrom typing import Dict, Optional\n\n\nclass Prompt:\n    def __init__(\n        self,\n        text: str,\n        image_urls: Optional[Dict[str, str]] = None,\n        entrypoint_prompt: str = \"\",\n    ):\n        self.text = text\n        self.image_urls = image_urls\n        self.entrypoint_prompt = entrypoint_prompt\n\n    def __repr__(self):\n        return f\"Prompt(text={self.text!r}, image_urls={self.image_urls!r})\"\n\n    def to_langchain_content(self):\n        content = [{\"type\": \"text\", \"text\": f\"Request: {self.text}\"}]\n\n        if self.image_urls:\n            for name, url in self.image_urls.items():\n                image_content = {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\n                        \"url\": url,\n                        \"detail\": \"low\",\n                    },\n                }\n                content.append(image_content)\n\n        return content\n\n    def to_dict(self):\n        return {\n            \"text\": self.text,\n            \"image_urls\": self.image_urls,\n            \"entrypoint_prompt\": self.entrypoint_prompt,\n        }\n\n    def to_json(self):\n        return json.dumps(self.to_dict())\n", "gpt_engineer/core/__init__.py": "", "gpt_engineer/core/base_execution_env.py": "from abc import ABC, abstractmethod\nfrom subprocess import Popen\nfrom typing import Optional, Tuple\n\nfrom gpt_engineer.core.files_dict import FilesDict\n\n\nclass BaseExecutionEnv(ABC):\n    \"\"\"\n    Abstract base class for an execution environment capable of running code.\n\n    This class defines the interface for execution environments that can execute commands,\n    handle processes, and manage file uploads and downloads.\n    \"\"\"\n\n    @abstractmethod\n    def run(self, command: str, timeout: Optional[int] = None) -> Tuple[str, str, int]:\n        \"\"\"\n        Runs a command in the execution environment.\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def popen(self, command: str) -> Popen:\n        \"\"\"\n        Runs a command in the execution environment.\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def upload(self, files: FilesDict) -> \"BaseExecutionEnv\":\n        \"\"\"\n        Uploads files to the execution environment.\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def download(self) -> FilesDict:\n        \"\"\"\n        Downloads files from the execution environment.\n        \"\"\"\n        raise NotImplementedError\n", "gpt_engineer/core/token_usage.py": "import base64\nimport io\nimport logging\nimport math\n\nfrom dataclasses import dataclass\nfrom typing import List, Union\n\nimport tiktoken\n\nfrom langchain.schema import AIMessage, HumanMessage, SystemMessage\nfrom PIL import Image\n\n# workaround for function moved in:\n# https://github.com/langchain-ai/langchain/blob/535db72607c4ae308566ede4af65295967bb33a8/libs/community/langchain_community/callbacks/openai_info.py\ntry:\n    from langchain.callbacks.openai_info import (\n        get_openai_token_cost_for_model,  # fmt: skip\n    )\nexcept ImportError:\n    from langchain_community.callbacks.openai_info import (\n        get_openai_token_cost_for_model,  # fmt: skip\n    )\n\n\nMessage = Union[AIMessage, HumanMessage, SystemMessage]\n\nlogger = logging.getLogger(__name__)\n\n\n@dataclass\nclass TokenUsage:\n    \"\"\"\n    Dataclass representing token usage statistics for a conversation step.\n\n    Attributes\n    ----------\n    step_name : str\n        The name of the conversation step.\n    in_step_prompt_tokens : int\n        The number of prompt tokens used in the step.\n    in_step_completion_tokens : int\n        The number of completion tokens used in the step.\n    in_step_total_tokens : int\n        The total number of tokens used in the step.\n    total_prompt_tokens : int\n        The cumulative number of prompt tokens used up to this step.\n    total_completion_tokens : int\n        The cumulative number of completion tokens used up to this step.\n    total_tokens : int\n        The cumulative total number of tokens used up to this step.\n    \"\"\"\n\n    \"\"\"\n    Represents token usage statistics for a conversation step.\n    \"\"\"\n\n    step_name: str\n    in_step_prompt_tokens: int\n    in_step_completion_tokens: int\n    in_step_total_tokens: int\n    total_prompt_tokens: int\n    total_completion_tokens: int\n    total_tokens: int\n\n\nclass Tokenizer:\n    \"\"\"\n    Tokenizer for counting tokens in text.\n    \"\"\"\n\n    def __init__(self, model_name):\n        self.model_name = model_name\n        self._tiktoken_tokenizer = (\n            tiktoken.encoding_for_model(model_name)\n            if \"gpt-4\" in model_name or \"gpt-3.5\" in model_name\n            else tiktoken.get_encoding(\"cl100k_base\")\n        )\n\n    def num_tokens(self, txt: str) -> int:\n        \"\"\"\n        Get the number of tokens in a text.\n\n        Parameters\n        ----------\n        txt : str\n            The text to count the tokens in.\n\n        Returns\n        -------\n        int\n            The number of tokens in the text.\n        \"\"\"\n        return len(self._tiktoken_tokenizer.encode(txt))\n\n    def num_tokens_for_base64_image(\n        self, image_base64: str, detail: str = \"high\"\n    ) -> int:\n        \"\"\"\n        Calculate the token size for a base64 encoded image based on OpenAI's token calculation rules.\n\n        Parameters:\n        - image_base64 (str): The base64 encoded string of the image.\n        - detail (str): The detail level of the image, 'low' or 'high'.\n\n        Returns:\n        - int: The token size of the image.\n        \"\"\"\n\n        if detail == \"low\":\n            return 85  # Fixed cost for low detail images\n\n        # Decode image from base64\n        image_data = base64.b64decode(image_base64)\n\n        # Convert byte data to image for size extraction\n        image = Image.open(io.BytesIO(image_data))\n\n        # Calculate the initial scale to fit within 2048 square while maintaining aspect ratio\n        max_dimension = max(image.size)\n        scale_factor = min(2048 / max_dimension, 1)  # Ensure we don't scale up\n        new_width = int(image.size[0] * scale_factor)\n        new_height = int(image.size[1] * scale_factor)\n\n        # Scale such that the shortest side is 768px\n        shortest_side = min(new_width, new_height)\n        if shortest_side > 768:\n            resize_factor = 768 / shortest_side\n            new_width = int(new_width * resize_factor)\n            new_height = int(new_height * resize_factor)\n\n        # Calculate the number of 512px tiles needed\n        width_tiles = math.ceil(new_width / 512)\n        height_tiles = math.ceil(new_height / 512)\n        total_tiles = width_tiles * height_tiles\n\n        # Each tile costs 170 tokens, plus a base cost of 85 tokens for high detail\n        token_cost = total_tiles * 170 + 85\n\n        return token_cost\n\n    def num_tokens_from_messages(self, messages: List[Message]) -> int:\n        \"\"\"\n        Get the total number of tokens used by a list of messages, accounting for text and base64 encoded images.\n\n        Parameters\n        ----------\n        messages : List[Message]\n            The list of messages to count the tokens in.\n\n        Returns\n        -------\n        int\n            The total number of tokens used by the messages.\n        \"\"\"\n        n_tokens = 0\n        for message in messages:\n            n_tokens += 4  # Account for message framing tokens\n\n            if isinstance(message.content, str):\n                # Content is a simple string\n                n_tokens += self.num_tokens(message.content)\n            elif isinstance(message.content, list):\n                # Content is a list, potentially mixed with text and images\n                for item in message.content:\n                    if item.get(\"type\") == \"text\":\n                        n_tokens += self.num_tokens(item[\"text\"])\n                    elif item.get(\"type\") == \"image_url\":\n                        image_detail = item[\"image_url\"].get(\"detail\", \"high\")\n                        image_base64 = item[\"image_url\"].get(\"url\")\n                        n_tokens += self.num_tokens_for_base64_image(\n                            image_base64, detail=image_detail\n                        )\n\n            n_tokens += 2  # Account for assistant's reply framing tokens\n\n        return n_tokens\n\n\nclass TokenUsageLog:\n    \"\"\"\n    Represents a log of token usage statistics for a conversation.\n    \"\"\"\n\n    def __init__(self, model_name):\n        self.model_name = model_name\n        self._cumulative_prompt_tokens = 0\n        self._cumulative_completion_tokens = 0\n        self._cumulative_total_tokens = 0\n        self._log = []\n        self._tokenizer = Tokenizer(model_name)\n\n    def update_log(self, messages: List[Message], answer: str, step_name: str) -> None:\n        \"\"\"\n        Update the token usage log with the number of tokens used in the current step.\n\n        Parameters\n        ----------\n        messages : List[Message]\n            The list of messages in the conversation.\n        answer : str\n            The answer from the AI.\n        step_name : str\n            The name of the step.\n        \"\"\"\n        prompt_tokens = self._tokenizer.num_tokens_from_messages(messages)\n        completion_tokens = self._tokenizer.num_tokens(answer)\n        total_tokens = prompt_tokens + completion_tokens\n\n        self._cumulative_prompt_tokens += prompt_tokens\n        self._cumulative_completion_tokens += completion_tokens\n        self._cumulative_total_tokens += total_tokens\n\n        self._log.append(\n            TokenUsage(\n                step_name=step_name,\n                in_step_prompt_tokens=prompt_tokens,\n                in_step_completion_tokens=completion_tokens,\n                in_step_total_tokens=total_tokens,\n                total_prompt_tokens=self._cumulative_prompt_tokens,\n                total_completion_tokens=self._cumulative_completion_tokens,\n                total_tokens=self._cumulative_total_tokens,\n            )\n        )\n\n    def log(self) -> List[TokenUsage]:\n        \"\"\"\n        Get the token usage log.\n\n        Returns\n        -------\n        List[TokenUsage]\n            A log of token usage details per step in the conversation.\n        \"\"\"\n        return self._log\n\n    def format_log(self) -> str:\n        \"\"\"\n        Format the token usage log as a CSV string.\n\n        Returns\n        -------\n        str\n            The token usage log formatted as a CSV string.\n        \"\"\"\n        result = \"step_name,prompt_tokens_in_step,completion_tokens_in_step,total_tokens_in_step,total_prompt_tokens,total_completion_tokens,total_tokens\\n\"\n        for log in self._log:\n            result += f\"{log.step_name},{log.in_step_prompt_tokens},{log.in_step_completion_tokens},{log.in_step_total_tokens},{log.total_prompt_tokens},{log.total_completion_tokens},{log.total_tokens}\\n\"\n        return result\n\n    def is_openai_model(self) -> bool:\n        \"\"\"\n        Check if the model is an OpenAI model.\n\n        Returns\n        -------\n        bool\n            True if the model is an OpenAI model, False otherwise.\n        \"\"\"\n        return \"gpt\" in self.model_name.lower()\n\n    def total_tokens(self) -> int:\n        \"\"\"\n        Return the total number of tokens used in the conversation.\n\n        Returns\n        -------\n        int\n            The total number of tokens used in the conversation.\n        \"\"\"\n        return self._cumulative_total_tokens\n\n    def usage_cost(self) -> float | None:\n        \"\"\"\n        Return the total cost in USD of the API usage.\n\n        Returns\n        -------\n        float\n            Cost in USD.\n        \"\"\"\n        if not self.is_openai_model():\n            return None\n\n        try:\n            result = 0\n            for log in self.log():\n                result += get_openai_token_cost_for_model(\n                    self.model_name, log.total_prompt_tokens, is_completion=False\n                )\n                result += get_openai_token_cost_for_model(\n                    self.model_name, log.total_completion_tokens, is_completion=True\n                )\n            return result\n        except Exception as e:\n            print(f\"Error calculating usage cost: {e}\")\n            return None\n", "gpt_engineer/core/project_config.py": "\"\"\"\nFunctions for reading and writing the `gpt-engineer.toml` configuration file.\n\nThe `gpt-engineer.toml` file is a TOML file that contains project-specific configuration used by the GPT Engineer CLI and gptengineer.app.\n\"\"\"\nfrom dataclasses import asdict, dataclass, field\nfrom pathlib import Path\n\nimport tomlkit\n\ndefault_config_filename = \"gpt-engineer.toml\"\n\nexample_config = \"\"\"\n[run]\nbuild = \"npm run build\"\ntest = \"npm run test\"\nlint = \"quick-lint-js\"\n\n[paths]\nbase = \"./frontend\"  # base directory to operate in (for monorepos)\nsrc = \"./src\"        # source directory (under the base directory) from which context will be retrieved\n\n[gptengineer-app]  # this namespace is used for gptengineer.app, may be used for internal experiments\nproject_id = \"...\"\n\n# we support multiple OpenAPI schemas, used as context for the LLM\nopenapi = [\n    { url = \"https://api.gptengineer.app/openapi.json\" },\n    { url = \"https://some-color-translating-api/openapi.json\" },\n]\n\"\"\"\n\n\n@dataclass\nclass _PathsConfig:\n    base: str | None = None\n    src: str | None = None\n\n\n@dataclass\nclass _RunConfig:\n    build: str | None = None\n    test: str | None = None\n    lint: str | None = None\n    format: str | None = None\n\n\n@dataclass\nclass _OpenApiConfig:\n    url: str\n\n\n@dataclass\nclass _GptEngineerAppConfig:\n    project_id: str\n    openapi: list[_OpenApiConfig] | None = None\n\n\ndef filter_none(d: dict) -> dict:\n    # Drop None values and empty dictionaries from a dictionary\n    return {\n        k: v\n        for k, v in (\n            (k, filter_none(v) if isinstance(v, dict) else v)\n            for k, v in d.items()\n            if v is not None\n        )\n        if not (isinstance(v, dict) and not v)  # Check for non-empty after filtering\n    }\n\n\n@dataclass\nclass Config:\n    \"\"\"Configuration for the GPT Engineer CLI and gptengineer.app via `gpt-engineer.toml`.\"\"\"\n\n    paths: _PathsConfig = field(default_factory=_PathsConfig)\n    run: _RunConfig = field(default_factory=_RunConfig)\n    gptengineer_app: _GptEngineerAppConfig | None = None\n\n    @classmethod\n    def from_toml(cls, config_file: Path | str):\n        if isinstance(config_file, str):\n            config_file = Path(config_file)\n        config_dict = read_config(config_file)\n        return cls.from_dict(config_dict)\n\n    @classmethod\n    def from_dict(cls, config_dict: dict):\n        run = _RunConfig(**config_dict.get(\"run\", {}))\n        paths = _PathsConfig(**config_dict.get(\"paths\", {}))\n\n        # load optional gptengineer-app section\n        gptengineer_app_dict = config_dict.get(\"gptengineer-app\", {})\n        gptengineer_app = None\n        if gptengineer_app_dict:\n            assert (\n                \"project_id\" in gptengineer_app_dict\n            ), \"project_id is required in gptengineer-app section\"\n            gptengineer_app = _GptEngineerAppConfig(\n                # required if gptengineer-app section is present\n                project_id=gptengineer_app_dict[\"project_id\"],\n                openapi=[\n                    _OpenApiConfig(**openapi)\n                    for openapi in gptengineer_app_dict.get(\"openapi\", [])\n                ]\n                or None,\n            )\n\n        return cls(paths=paths, run=run, gptengineer_app=gptengineer_app)\n\n    def to_dict(self) -> dict:\n        d = asdict(self)\n        d[\"gptengineer-app\"] = d.pop(\"gptengineer_app\", None)\n\n        # Drop None values and empty dictionaries\n        # Needed because tomlkit.dumps() doesn't handle None values,\n        # and we don't want to write empty sections.\n        d = filter_none(d)\n\n        return d\n\n    def to_toml(self, config_file: Path | str, save=True) -> str:\n        \"\"\"Write the configuration to a TOML file.\"\"\"\n        if isinstance(config_file, str):\n            config_file = Path(config_file)\n\n        # Load the TOMLDocument and overwrite it with the new values\n        config = read_config(config_file)\n        default_config = Config().to_dict()\n        for k, v in self.to_dict().items():\n            # only write values that are already explicitly set, or that differ from defaults\n            if k in config or v != default_config[k]:\n                if isinstance(v, dict):\n                    config[k] = {\n                        k2: v2\n                        for k2, v2 in v.items()\n                        if (\n                            k2 in config[k]\n                            or default_config.get(k) is None\n                            or v2 != default_config[k].get(k2)\n                        )\n                    }\n                else:\n                    config[k] = v\n\n        toml_str = tomlkit.dumps(config)\n        if save:\n            with open(config_file, \"w\") as f:\n                f.write(toml_str)\n\n        return toml_str\n\n\ndef read_config(config_file: Path) -> tomlkit.TOMLDocument:\n    \"\"\"Read the configuration file\"\"\"\n    assert config_file.exists(), f\"Config file {config_file} does not exist\"\n    with open(config_file, \"r\") as f:\n        return tomlkit.load(f)\n", "gpt_engineer/core/base_agent.py": "\"\"\"\nBase Agent Module\n\nThis module provides an abstract base class for an agent that interacts with code. It defines the interface\nfor agents capable of initializing and improving code based on a given prompt. Implementations of this class\nare expected to provide concrete methods for these actions.\n\nClasses:\n    BaseAgent: Abstract base class for an agent that interacts with code.\n\"\"\"\nfrom abc import ABC, abstractmethod\n\nfrom gpt_engineer.core.files_dict import FilesDict\nfrom gpt_engineer.core.prompt import Prompt\n\n\nclass BaseAgent(ABC):\n    \"\"\"\n    Abstract base class for an agent that interacts with code.\n\n    Defines the interface for agents capable of initializing and improving code based on a given prompt.\n    Implementations of this class are expected to provide concrete methods for these actions.\n    \"\"\"\n\n    @abstractmethod\n    def init(self, prompt: Prompt) -> FilesDict:\n        pass\n\n    @abstractmethod\n    def improve(self, files_dict: FilesDict, prompt: Prompt) -> FilesDict:\n        pass\n", "gpt_engineer/core/default/file_store.py": "import tempfile\n\nfrom pathlib import Path\nfrom typing import Union\n\nfrom gpt_engineer.core.files_dict import FilesDict\nfrom gpt_engineer.core.linting import Linting\n\n\nclass FileStore:\n    \"\"\"\n    Module for managing file storage in a temporary directory.\n\n    This module provides a class that manages the storage of files in a temporary directory.\n    It includes methods for uploading files to the directory and downloading them as a\n    collection of files.\n\n    Classes\n    -------\n    FileStore\n        Manages file storage in a temporary directory, allowing for upload and download of files.\n\n    Imports\n    -------\n    - tempfile: For creating temporary directories.\n    - Path: For handling file system paths.\n    - Union: For type annotations.\n    - FilesDict: For handling collections of files.\n    \"\"\"\n\n    def __init__(self, path: Union[str, Path, None] = None):\n        if path is None:\n            path = Path(tempfile.mkdtemp(prefix=\"gpt-engineer-\"))\n\n        self.working_dir = Path(path)\n        self.working_dir.mkdir(parents=True, exist_ok=True)\n        self.id = self.working_dir.name.split(\"-\")[-1]\n\n    def push(self, files: FilesDict):\n        for name, content in files.items():\n            path = self.working_dir / name\n            path.parent.mkdir(parents=True, exist_ok=True)\n            with open(path, \"w\") as f:\n                f.write(content)\n        return self\n\n    def linting(self, files: FilesDict) -> FilesDict:\n        # lint the code\n        linting = Linting()\n        return linting.lint_files(files)\n\n    def pull(self) -> FilesDict:\n        files = {}\n        for path in self.working_dir.glob(\"**/*\"):\n            if path.is_file():\n                with open(path, \"r\") as f:\n                    try:\n                        content = f.read()\n                    except UnicodeDecodeError:\n                        content = \"binary file\"\n                    files[str(path.relative_to(self.working_dir))] = content\n        return FilesDict(files)\n", "gpt_engineer/core/default/steps.py": "\"\"\"\nModule for defining the steps involved in generating and improving code using AI.\n\nThis module provides functions that represent different steps in the process of generating\nand improving code using an AI model. These steps include generating code from a prompt,\ncreating an entrypoint for the codebase, executing the entrypoint, and refining code edits.\n\nFunctions\n---------\ncurr_fn : function\n    Returns the name of the current function.\n\nsetup_sys_prompt : function\n    Sets up the system prompt for generating code.\n\ngen_code : function\n    Generates code from a prompt using AI and returns the generated files.\n\ngen_entrypoint : function\n    Generates an entrypoint for the codebase and returns the entrypoint files.\n\nexecute_entrypoint : function\n    Executes the entrypoint of the codebase.\n\nsetup_sys_prompt_existing_code : function\n    Sets up the system prompt for improving existing code.\n\n\nimprove : function\n    Improves the code based on user input and returns the updated files.\n\"\"\"\n\nimport inspect\nimport io\nimport re\nimport sys\nimport traceback\n\nfrom pathlib import Path\nfrom typing import List, MutableMapping, Union\n\nfrom langchain.schema import HumanMessage, SystemMessage\nfrom termcolor import colored\n\nfrom gpt_engineer.core.ai import AI\nfrom gpt_engineer.core.base_execution_env import BaseExecutionEnv\nfrom gpt_engineer.core.base_memory import BaseMemory\nfrom gpt_engineer.core.chat_to_files import apply_diffs, chat_to_files_dict, parse_diffs\nfrom gpt_engineer.core.default.constants import MAX_EDIT_REFINEMENT_STEPS\nfrom gpt_engineer.core.default.paths import (\n    CODE_GEN_LOG_FILE,\n    DEBUG_LOG_FILE,\n    DIFF_LOG_FILE,\n    ENTRYPOINT_FILE,\n    ENTRYPOINT_LOG_FILE,\n    IMPROVE_LOG_FILE,\n)\nfrom gpt_engineer.core.files_dict import FilesDict, file_to_lines_dict\nfrom gpt_engineer.core.preprompts_holder import PrepromptsHolder\nfrom gpt_engineer.core.prompt import Prompt\n\n\ndef curr_fn() -> str:\n    \"\"\"\n    Returns the name of the current function.\n\n    Returns\n    -------\n    str\n        The name of the function that called this function.\n    \"\"\"\n    return inspect.stack()[1].function\n\n\ndef setup_sys_prompt(preprompts: MutableMapping[Union[str, Path], str]) -> str:\n    \"\"\"\n    Sets up the system prompt for generating code.\n\n    Parameters\n    ----------\n    preprompts : MutableMapping[Union[str, Path], str]\n        A mapping of preprompt messages to guide the AI model.\n\n    Returns\n    -------\n    str\n        The system prompt message for the AI model.\n    \"\"\"\n    return (\n        preprompts[\"roadmap\"]\n        + preprompts[\"generate\"].replace(\"FILE_FORMAT\", preprompts[\"file_format\"])\n        + \"\\nUseful to know:\\n\"\n        + preprompts[\"philosophy\"]\n    )\n\n\ndef setup_sys_prompt_existing_code(\n    preprompts: MutableMapping[Union[str, Path], str]\n) -> str:\n    \"\"\"\n    Sets up the system prompt for improving existing code.\n\n    Parameters\n    ----------\n    preprompts : MutableMapping[Union[str, Path], str]\n        A mapping of preprompt messages to guide the AI model.\n\n    Returns\n    -------\n    str\n        The system prompt message for the AI model to improve existing code.\n    \"\"\"\n    return (\n        preprompts[\"roadmap\"]\n        + preprompts[\"improve\"].replace(\"FILE_FORMAT\", preprompts[\"file_format_diff\"])\n        + \"\\nUseful to know:\\n\"\n        + preprompts[\"philosophy\"]\n    )\n\n\ndef gen_code(\n    ai: AI, prompt: Prompt, memory: BaseMemory, preprompts_holder: PrepromptsHolder\n) -> FilesDict:\n    \"\"\"\n    Generates code from a prompt using AI and returns the generated files.\n\n    Parameters\n    ----------\n    ai : AI\n        The AI model used for generating code.\n    prompt : str\n        The user prompt to generate code from.\n    memory : BaseMemory\n        The memory interface where the code and related data are stored.\n    preprompts_holder : PrepromptsHolder\n        The holder for preprompt messages that guide the AI model.\n\n    Returns\n    -------\n    FilesDict\n        A dictionary of file names to their respective source code content.\n    \"\"\"\n    preprompts = preprompts_holder.get_preprompts()\n    messages = ai.start(\n        setup_sys_prompt(preprompts), prompt.to_langchain_content(), step_name=curr_fn()\n    )\n    chat = messages[-1].content.strip()\n    memory.log(CODE_GEN_LOG_FILE, \"\\n\\n\".join(x.pretty_repr() for x in messages))\n    files_dict = chat_to_files_dict(chat)\n    return files_dict\n\n\ndef gen_entrypoint(\n    ai: AI,\n    prompt: Prompt,\n    files_dict: FilesDict,\n    memory: BaseMemory,\n    preprompts_holder: PrepromptsHolder,\n) -> FilesDict:\n    \"\"\"\n    Generates an entrypoint for the codebase and returns the entrypoint files.\n\n    Parameters\n    ----------\n    ai : AI\n        The AI model used for generating the entrypoint.\n    files_dict : FilesDict\n        The dictionary of file names to their respective source code content.\n    memory : BaseMemory\n        The memory interface where the code and related data are stored.\n    preprompts_holder : PrepromptsHolder\n        The holder for preprompt messages that guide the AI model.\n\n    Returns\n    -------\n    FilesDict\n        A dictionary containing the entrypoint file.\n    \"\"\"\n    user_prompt = prompt.entrypoint_prompt\n    if not user_prompt:\n        user_prompt = \"\"\"\n        Make a unix script that\n        a) installs dependencies\n        b) runs all necessary parts of the codebase (in parallel if necessary)\n        \"\"\"\n    preprompts = preprompts_holder.get_preprompts()\n    messages = ai.start(\n        system=(preprompts[\"entrypoint\"]),\n        user=user_prompt\n        + \"\\nInformation about the codebase:\\n\\n\"\n        + files_dict.to_chat(),\n        step_name=curr_fn(),\n    )\n    print()\n    chat = messages[-1].content.strip()\n    regex = r\"```\\S*\\n(.+?)```\"\n    matches = re.finditer(regex, chat, re.DOTALL)\n    entrypoint_code = FilesDict(\n        {ENTRYPOINT_FILE: \"\\n\".join(match.group(1) for match in matches)}\n    )\n    memory.log(ENTRYPOINT_LOG_FILE, \"\\n\\n\".join(x.pretty_repr() for x in messages))\n    return entrypoint_code\n\n\ndef execute_entrypoint(\n    ai: AI,\n    execution_env: BaseExecutionEnv,\n    files_dict: FilesDict,\n    prompt: Prompt = None,\n    preprompts_holder: PrepromptsHolder = None,\n    memory: BaseMemory = None,\n) -> FilesDict:\n    \"\"\"\n    Executes the entrypoint of the codebase.\n\n    Parameters\n    ----------\n    ai : AI\n        The AI model used for generating the entrypoint.\n    execution_env : BaseExecutionEnv\n        The execution environment in which the code is executed.\n    files_dict : FilesDict\n        The dictionary of file names to their respective source code content.\n    preprompts_holder : PrepromptsHolder, optional\n        The holder for preprompt messages that guide the AI model.\n\n    Returns\n    -------\n    FilesDict\n        The dictionary of file names to their respective source code content after execution.\n    \"\"\"\n    if ENTRYPOINT_FILE not in files_dict:\n        raise FileNotFoundError(\n            \"The required entrypoint \"\n            + ENTRYPOINT_FILE\n            + \" does not exist in the code.\"\n        )\n\n    command = files_dict[ENTRYPOINT_FILE]\n\n    print()\n    print(\n        colored(\n            \"Do you want to execute this code? (Y/n)\",\n            \"red\",\n        )\n    )\n    print()\n    print(command)\n    print()\n    if input(\"\").lower() not in [\"\", \"y\", \"yes\"]:\n        print(\"Ok, not executing the code.\")\n        return files_dict\n    print(\"Executing the code...\")\n    print()\n    print(\n        colored(\n            \"Note: If it does not work as expected, consider running the code\"\n            + \" in another way than above.\",\n            \"green\",\n        )\n    )\n    print()\n    print(\"You can press ctrl+c *once* to stop the execution.\")\n    print()\n\n    execution_env.upload(files_dict).run(f\"bash {ENTRYPOINT_FILE}\")\n    return files_dict\n\n\ndef improve_fn(\n    ai: AI,\n    prompt: Prompt,\n    files_dict: FilesDict,\n    memory: BaseMemory,\n    preprompts_holder: PrepromptsHolder,\n) -> FilesDict:\n    \"\"\"\n    Improves the code based on user input and returns the updated files.\n\n    Parameters\n    ----------\n    ai : AI\n        The AI model used for improving code.\n    prompt :str\n        The user prompt to improve the code.\n    files_dict : FilesDict\n        The dictionary of file names to their respective source code content.\n    memory : BaseMemory\n        The memory interface where the code and related data are stored.\n    preprompts_holder : PrepromptsHolder\n        The holder for preprompt messages that guide the AI model.\n\n    Returns\n    -------\n    FilesDict\n        The dictionary of file names to their respective updated source code content.\n    \"\"\"\n    preprompts = preprompts_holder.get_preprompts()\n    messages = [\n        SystemMessage(content=setup_sys_prompt_existing_code(preprompts)),\n    ]\n\n    # Add files as input\n    messages.append(HumanMessage(content=f\"{files_dict.to_chat()}\"))\n    messages.append(HumanMessage(content=prompt.to_langchain_content()))\n    memory.log(\n        DEBUG_LOG_FILE,\n        \"UPLOADED FILES:\\n\" + files_dict.to_log() + \"\\nPROMPT:\\n\" + prompt.text,\n    )\n    return _improve_loop(ai, files_dict, memory, messages)\n\n\ndef _improve_loop(\n    ai: AI, files_dict: FilesDict, memory: BaseMemory, messages: List\n) -> FilesDict:\n    messages = ai.next(messages, step_name=curr_fn())\n    files_dict, errors = salvage_correct_hunks(messages, files_dict, memory)\n\n    retries = 0\n    while errors and retries < MAX_EDIT_REFINEMENT_STEPS:\n        messages.append(\n            HumanMessage(\n                content=\"Some previously produced diffs were not on the requested format, or the code part was not found in the code. Details:\\n\"\n                + \"\\n\".join(errors)\n                + \"\\n Only rewrite the problematic diffs, making sure that the failing ones are now on the correct format and can be found in the code. Make sure to not repeat past mistakes. \\n\"\n            )\n        )\n        messages = ai.next(messages, step_name=curr_fn())\n        files_dict, errors = salvage_correct_hunks(messages, files_dict, memory)\n        retries += 1\n\n    return files_dict\n\n\ndef salvage_correct_hunks(\n    messages: List,\n    files_dict: FilesDict,\n    memory: BaseMemory,\n) -> tuple[FilesDict, List[str]]:\n    error_messages = []\n    ai_response = messages[-1].content.strip()\n\n    diffs = parse_diffs(ai_response)\n    # validate and correct diffs\n\n    for _, diff in diffs.items():\n        # if diff is a new file, validation and correction is unnecessary\n        if not diff.is_new_file():\n            problems = diff.validate_and_correct(\n                file_to_lines_dict(files_dict[diff.filename_pre])\n            )\n            error_messages.extend(problems)\n    files_dict = apply_diffs(diffs, files_dict)\n    memory.log(IMPROVE_LOG_FILE, \"\\n\\n\".join(x.pretty_repr() for x in messages))\n    memory.log(DIFF_LOG_FILE, \"\\n\\n\".join(error_messages))\n    return files_dict, error_messages\n\n\nclass Tee(object):\n    def __init__(self, *files):\n        self.files = files\n\n    def write(self, obj):\n        for file in self.files:\n            file.write(obj)\n\n    def flush(self):\n        for file in self.files:\n            file.flush()\n\n\ndef handle_improve_mode(prompt, agent, memory, files_dict):\n    captured_output = io.StringIO()\n    old_stdout = sys.stdout\n    sys.stdout = Tee(sys.stdout, captured_output)\n\n    try:\n        files_dict = agent.improve(files_dict, prompt)\n    except Exception as e:\n        print(\n            f\"Error while improving the project: {e}\\nCould you please upload the debug_log_file.txt in {memory.path}/logs folder to github?\\nFULL STACK TRACE:\\n\"\n        )\n        traceback.print_exc(file=sys.stdout)  # Print the full stack trace\n    finally:\n        # Reset stdout\n        sys.stdout = old_stdout\n\n        # Get the captured output\n        captured_string = captured_output.getvalue()\n        print(captured_string)\n        memory.log(DEBUG_LOG_FILE, \"\\nCONSOLE OUTPUT:\\n\" + captured_string)\n\n    return files_dict\n", "gpt_engineer/core/default/paths.py": "\"\"\"\nModule defining file system paths used by the application.\n\nThis module contains definitions of file system paths that are used throughout the\napplication to locate and manage various files and directories, such as logs, memory,\nand preprompts.\n\nConstants\n---------\nMETA_DATA_REL_PATH : str\n    The relative path to the directory where metadata is stored.\n\nMEMORY_REL_PATH : str\n    The relative path to the directory where memory-related files are stored.\n\nCODE_GEN_LOG_FILE : str\n    The filename for the log file that contains all output from code generation.\n\nDEBUG_LOG_FILE : str\n    The filename for the log file that contains debug information.\n\nENTRYPOINT_FILE : str\n    The filename for the entrypoint script that is executed to run the application.\n\nENTRYPOINT_LOG_FILE : str\n    The filename for the log file that contains the chat related to entrypoint generation.\n\nPREPROMPTS_PATH : Path\n    The file system path to the directory containing preprompt files.\n\nFunctions\n---------\nmemory_path : function\n    Constructs the full path to the memory directory based on a given base path.\n\nmetadata_path : function\n    Constructs the full path to the metadata directory based on a given base path.\n\"\"\"\nimport os\n\nfrom pathlib import Path\n\nMETA_DATA_REL_PATH = \".gpteng\"\nMEMORY_REL_PATH = os.path.join(META_DATA_REL_PATH, \"memory\")\nCODE_GEN_LOG_FILE = \"all_output.txt\"\nIMPROVE_LOG_FILE = \"improve.txt\"\nDIFF_LOG_FILE = \"diff_errors.txt\"\nDEBUG_LOG_FILE = \"debug_log_file.txt\"\nENTRYPOINT_FILE = \"run.sh\"\nENTRYPOINT_LOG_FILE = \"gen_entrypoint_chat.txt\"\nENTRYPOINT_FILE = \"run.sh\"\nPREPROMPTS_PATH = Path(__file__).parent.parent.parent / \"preprompts\"\n\n\ndef memory_path(path):\n    \"\"\"\n    Constructs the full path to the memory directory based on a given base path.\n\n    Parameters\n    ----------\n    path : str\n        The base path to append the memory directory to.\n\n    Returns\n    -------\n    str\n        The full path to the memory directory.\n    \"\"\"\n    return os.path.join(path, MEMORY_REL_PATH)\n\n\ndef metadata_path(path):\n    \"\"\"\n    Constructs the full path to the metadata directory based on a given base path.\n\n    Parameters\n    ----------\n    path : str\n        The base path to append the metadata directory to.\n\n    Returns\n    -------\n    str\n        The full path to the metadata directory.\n    \"\"\"\n    return os.path.join(path, META_DATA_REL_PATH)\n", "gpt_engineer/core/default/disk_memory.py": "\"\"\"\nDisk Memory Module\n==================\n\nThis module provides a simple file-based key-value database system, where keys are\nrepresented as filenames and values are the contents of these files. The `DiskMemory` class\nis responsible for the CRUD operations on the database.\n\nAttributes\n----------\nNone\n\nFunctions\n---------\nNone\n\nClasses\n-------\nDiskMemory\n    A file-based key-value store where keys correspond to filenames and values to file contents.\n\"\"\"\n\nimport base64\nimport json\nimport shutil\n\nfrom datetime import datetime\nfrom pathlib import Path\nfrom typing import Any, Dict, Iterator, Optional, Union\n\nfrom gpt_engineer.core.base_memory import BaseMemory\nfrom gpt_engineer.tools.supported_languages import SUPPORTED_LANGUAGES\n\n\n# This class represents a simple database that stores its tools as files in a directory.\nclass DiskMemory(BaseMemory):\n    \"\"\"\n    A file-based key-value store where keys correspond to filenames and values to file contents.\n\n    This class provides an interface to a file-based database, leveraging file operations to\n    facilitate CRUD-like interactions. It allows for quick checks on the existence of keys,\n    retrieval of values based on keys, and setting new key-value pairs.\n\n    Attributes\n    ----------\n    path : Path\n        The directory path where the database files are stored.\n    \"\"\"\n\n    def __init__(self, path: Union[str, Path]):\n        \"\"\"\n        Initialize the DiskMemory class with a specified path.\n\n        Parameters\n        ----------\n        path : str or Path\n            The path to the directory where the database files will be stored.\n\n        \"\"\"\n        self.path: Path = Path(path).absolute()\n\n        self.path.mkdir(parents=True, exist_ok=True)\n\n    def __contains__(self, key: str) -> bool:\n        \"\"\"\n        Determine whether the database contains a file with the specified key.\n\n        Parameters\n        ----------\n        key : str\n            The key (filename) to check for existence in the database.\n\n        Returns\n        -------\n        bool\n            Returns True if the file exists, False otherwise.\n\n        \"\"\"\n        return (self.path / key).is_file()\n\n    def __getitem__(self, key: str) -> str:\n        \"\"\"\n        Retrieve the content of a file in the database corresponding to the given key.\n        If the file is an image with a .png or .jpeg extension, it returns the content\n        in Base64-encoded string format.\n\n        Parameters\n        ----------\n        key : str\n            The key (filename) whose content is to be retrieved.\n\n        Returns\n        -------\n        str\n            The content of the file associated with the key, or Base64-encoded string if it's a .png or .jpeg file.\n\n        Raises\n        ------\n        KeyError\n            If the file corresponding to the key does not exist in the database.\n        \"\"\"\n        full_path = self.path / key\n\n        if not full_path.is_file():\n            raise KeyError(f\"File '{key}' could not be found in '{self.path}'\")\n\n        if full_path.suffix in [\".png\", \".jpeg\", \".jpg\"]:\n            with full_path.open(\"rb\") as image_file:\n                encoded_string = base64.b64encode(image_file.read()).decode(\"utf-8\")\n                mime_type = \"image/png\" if full_path.suffix == \".png\" else \"image/jpeg\"\n                return f\"data:{mime_type};base64,{encoded_string}\"\n        else:\n            with full_path.open(\"r\", encoding=\"utf-8\") as f:\n                return f.read()\n\n    def get(self, key: str, default: Optional[Any] = None) -> Any:\n        \"\"\"\n        Retrieve the content of a file in the database, or return a default value if not found.\n\n        Parameters\n        ----------\n        key : str\n            The key (filename) whose content is to be retrieved.\n        default : Any, optional\n            The default value to return if the file does not exist. Default is None.\n\n        Returns\n        -------\n        Any\n            The content of the file if it exists, a new DiskMemory instance if the key corresponds to a directory.\n        \"\"\"\n\n        item_path = self.path / key\n        try:\n            if item_path.is_file():\n                return self[key]\n            elif item_path.is_dir():\n                return DiskMemory(item_path)\n            else:\n                return default\n        except:\n            return default\n\n    def __setitem__(self, key: Union[str, Path], val: str) -> None:\n        \"\"\"\n        Set or update the content of a file in the database corresponding to the given key.\n\n        Parameters\n        ----------\n        key : str or Path\n            The key (filename) where the content is to be set.\n        val : str\n            The content to be written to the file.\n\n        Raises\n        ------\n        ValueError\n            If the key attempts to access a parent path.\n        TypeError\n            If the value is not a string.\n\n        \"\"\"\n        if str(key).startswith(\"../\"):\n            raise ValueError(f\"File name {key} attempted to access parent path.\")\n\n        if not isinstance(val, str):\n            raise TypeError(\"val must be str\")\n\n        full_path = self.path / key\n        full_path.parent.mkdir(parents=True, exist_ok=True)\n\n        full_path.write_text(val, encoding=\"utf-8\")\n\n    def __delitem__(self, key: Union[str, Path]) -> None:\n        \"\"\"\n        Delete a file or directory from the database corresponding to the given key.\n\n        Parameters\n        ----------\n        key : str or Path\n            The key (filename or directory name) to be deleted.\n\n        Raises\n        ------\n        KeyError\n            If the file or directory corresponding to the key does not exist in the database.\n\n        \"\"\"\n        item_path = self.path / key\n        if not item_path.exists():\n            raise KeyError(f\"Item '{key}' could not be found in '{self.path}'\")\n\n        if item_path.is_file():\n            item_path.unlink()\n        elif item_path.is_dir():\n            shutil.rmtree(item_path)\n\n    def __iter__(self) -> Iterator[str]:\n        \"\"\"\n        Iterate over the keys (filenames) in the database.\n\n        Yields\n        ------\n        Iterator[str]\n            An iterator over the sorted list of keys (filenames) in the database.\n\n        \"\"\"\n        return iter(\n            sorted(\n                str(item.relative_to(self.path))\n                for item in sorted(self.path.rglob(\"*\"))\n                if item.is_file()\n            )\n        )\n\n    def __len__(self) -> int:\n        \"\"\"\n        Get the number of files in the database.\n\n        Returns\n        -------\n        int\n            The number of files in the database.\n\n        \"\"\"\n        return len(list(self.__iter__()))\n\n    def _supported_files(self) -> str:\n        valid_extensions = {\n            ext for lang in SUPPORTED_LANGUAGES for ext in lang[\"extensions\"]\n        }\n        file_paths = [\n            str(item)\n            for item in self\n            if Path(item).is_file() and Path(item).suffix in valid_extensions\n        ]\n        return \"\\n\".join(file_paths)\n\n    def _all_files(self) -> str:\n        file_paths = [str(item) for item in self if Path(item).is_file()]\n        return \"\\n\".join(file_paths)\n\n    def to_path_list_string(self, supported_code_files_only: bool = False) -> str:\n        \"\"\"\n        Generate a string representation of the file paths in the database.\n\n        Parameters\n        ----------\n        supported_code_files_only : bool, optional\n            If True, filter the list to include only supported code file extensions.\n            Default is False.\n\n        Returns\n        -------\n        str\n            A newline-separated string of file paths.\n\n        \"\"\"\n        if supported_code_files_only:\n            return self._supported_files()\n        else:\n            return self._all_files()\n\n    def to_dict(self) -> Dict[Union[str, Path], str]:\n        \"\"\"\n        Convert the database contents to a dictionary.\n\n        Returns\n        -------\n        Dict[Union[str, Path], str]\n            A dictionary with keys as filenames and values as file contents.\n\n        \"\"\"\n        return {file_path: self[file_path] for file_path in self}\n\n    def to_json(self) -> str:\n        \"\"\"\n        Serialize the database contents to a JSON string.\n\n        Returns\n        -------\n        str\n            A JSON string representation of the database contents.\n\n        \"\"\"\n        return json.dumps(self.to_dict())\n\n    def log(self, key: Union[str, Path], val: str) -> None:\n        \"\"\"\n        Append to a file or create and write to it if it doesn't exist.\n\n        Parameters\n        ----------\n        key : str or Path\n            The key (filename) where the content is to be appended.\n        val : str\n            The content to be appended to the file.\n\n        \"\"\"\n\n        if str(key).startswith(\"../\"):\n            raise ValueError(f\"File name {key} attempted to access parent path.\")\n\n        if not isinstance(val, str):\n            raise TypeError(\"val must be str\")\n\n        full_path = self.path / \"logs\" / key\n        full_path.parent.mkdir(parents=True, exist_ok=True)\n\n        # Touch if it doesnt exist\n        if not full_path.exists():\n            full_path.touch()\n\n        with open(full_path, \"a\", encoding=\"utf-8\") as file:\n            file.write(f\"\\n{datetime.now().isoformat()}\\n\")\n            file.write(val + \"\\n\")\n\n    def archive_logs(self):\n        \"\"\"\n        Moves all logs to archive directory based on current timestamp\n        \"\"\"\n        if \"logs\" in self:\n            archive_dir = (\n                self.path / f\"logs_{datetime.now().strftime('%Y-%m-%d-%H-%M-%S')}\"\n            )\n            shutil.move(self.path / \"logs\", archive_dir)\n", "gpt_engineer/core/default/disk_execution_env.py": "\"\"\"\nModule for managing the execution environment on the local disk.\n\nThis module provides a class that handles the execution of code stored on the local\nfile system. It includes methods for uploading files to the execution environment,\nrunning commands, and capturing the output.\n\nClasses\n-------\nDiskExecutionEnv\n    An execution environment that runs code on the local file system and captures\n    the output of the execution.\n\nImports\n-------\n- subprocess: For running shell commands.\n- time: For timing the execution of commands.\n- Path: For handling file system paths.\n- Optional, Tuple, Union: For type annotations.\n- BaseExecutionEnv: For inheriting the base execution environment interface.\n- FileStore: For managing file storage.\n- FilesDict: For handling collections of files.\n\"\"\"\n\nimport subprocess\nimport time\n\nfrom pathlib import Path\nfrom typing import Optional, Tuple, Union\n\nfrom gpt_engineer.core.base_execution_env import BaseExecutionEnv\nfrom gpt_engineer.core.default.file_store import FileStore\nfrom gpt_engineer.core.files_dict import FilesDict\n\n\nclass DiskExecutionEnv(BaseExecutionEnv):\n    \"\"\"\n    An execution environment that runs code on the local file system and captures\n    the output of the execution.\n\n    This class is responsible for executing code that is stored on disk. It ensures that\n    the necessary entrypoint file exists and then runs the code using a subprocess. If the\n    execution is interrupted by the user, it handles the interruption gracefully.\n\n    Attributes\n    ----------\n    store : FileStore\n        An instance of FileStore that manages the storage of files in the execution\n        environment.\n    \"\"\"\n\n    def __init__(self, path: Union[str, Path, None] = None):\n        self.files = FileStore(path)\n\n    def upload(self, files: FilesDict) -> \"DiskExecutionEnv\":\n        self.files.push(files)\n        return self\n\n    def download(self) -> FilesDict:\n        return self.files.pull()\n\n    def popen(self, command: str) -> subprocess.Popen:\n        p = subprocess.Popen(\n            command,\n            shell=True,\n            cwd=self.files.working_dir,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n        )\n        return p\n\n    def run(self, command: str, timeout: Optional[int] = None) -> Tuple[str, str, int]:\n        start = time.time()\n        print(\"\\n--- Start of run ---\")\n        # while running, also print the stdout and stderr\n        p = subprocess.Popen(\n            command,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            cwd=self.files.working_dir,\n            text=True,\n            shell=True,\n        )\n        print(\"$\", command)\n        stdout_full, stderr_full = \"\", \"\"\n\n        try:\n            while p.poll() is None:\n                assert p.stdout is not None\n                assert p.stderr is not None\n                stdout = p.stdout.readline()\n                stderr = p.stderr.readline()\n                if stdout:\n                    print(stdout, end=\"\")\n                    stdout_full += stdout\n                if stderr:\n                    print(stderr, end=\"\")\n                    stderr_full += stderr\n                if timeout and time.time() - start > timeout:\n                    print(\"Timeout!\")\n                    p.kill()\n                    raise TimeoutError()\n        except KeyboardInterrupt:\n            print()\n            print(\"Stopping execution.\")\n            print(\"Execution stopped.\")\n            p.kill()\n            print()\n            print(\"--- Finished run ---\\n\")\n\n        return stdout_full, stderr_full, p.returncode\n", "gpt_engineer/core/default/constants.py": "\"\"\"\nModule defining constants used throughout the application.\n\nThis module contains definitions of constants that are used across various\ncomponents of the application to maintain consistency and ease of configuration.\n\nConstants\n---------\nMAX_EDIT_REFINEMENT_STEPS : int\n    The maximum number of refinement steps allowed when generating edit blocks.\n\"\"\"\nMAX_EDIT_REFINEMENT_STEPS = 2\n", "gpt_engineer/core/default/simple_agent.py": "\"\"\"\nModule for defining a simple agent that uses AI to manage code generation and improvement.\n\nThis module provides a class that represents an agent capable of initializing and improving\na codebase using AI. It handles interactions with the AI model, memory, and execution\nenvironment to generate and refine code based on user prompts.\n\n\"\"\"\n\nimport tempfile\n\nfrom typing import Optional\n\nfrom gpt_engineer.core.ai import AI\nfrom gpt_engineer.core.base_agent import BaseAgent\nfrom gpt_engineer.core.base_execution_env import BaseExecutionEnv\nfrom gpt_engineer.core.base_memory import BaseMemory\nfrom gpt_engineer.core.default.disk_execution_env import DiskExecutionEnv\nfrom gpt_engineer.core.default.disk_memory import DiskMemory\nfrom gpt_engineer.core.default.paths import PREPROMPTS_PATH, memory_path\nfrom gpt_engineer.core.default.steps import gen_code, gen_entrypoint, improve_fn\nfrom gpt_engineer.core.files_dict import FilesDict\nfrom gpt_engineer.core.preprompts_holder import PrepromptsHolder\nfrom gpt_engineer.core.prompt import Prompt\n\n\nclass SimpleAgent(BaseAgent):\n    \"\"\"\n    An agent that uses AI to generate and improve code based on a given prompt.\n\n    This agent is capable of initializing a codebase from a prompt and improving an existing\n    codebase based on user input. It uses an AI model to generate and refine code, and it\n    interacts with a repository and an execution environment to manage and execute the code.\n\n    Attributes\n    ----------\n    memory : BaseMemory\n        The memory interface where the code and related data are stored.\n    execution_env : BaseExecutionEnv\n        The execution environment in which the code is executed.\n    ai : AI\n        The AI model used for generating and improving code.\n    preprompts_holder : PrepromptsHolder\n        The holder for preprompt messages that guide the AI model.\n    \"\"\"\n\n    def __init__(\n        self,\n        memory: BaseMemory,\n        execution_env: BaseExecutionEnv,\n        ai: AI = None,\n        preprompts_holder: PrepromptsHolder = None,\n    ):\n        self.preprompts_holder = preprompts_holder or PrepromptsHolder(PREPROMPTS_PATH)\n        self.memory = memory\n        self.execution_env = execution_env\n        self.ai = ai or AI()\n\n    @classmethod\n    def with_default_config(\n        cls, path: str, ai: AI = None, preprompts_holder: PrepromptsHolder = None\n    ):\n        return cls(\n            memory=DiskMemory(memory_path(path)),\n            execution_env=DiskExecutionEnv(),\n            ai=ai,\n            preprompts_holder=preprompts_holder or PrepromptsHolder(PREPROMPTS_PATH),\n        )\n\n    def init(self, prompt: Prompt) -> FilesDict:\n        files_dict = gen_code(self.ai, prompt, self.memory, self.preprompts_holder)\n        entrypoint = gen_entrypoint(\n            self.ai, prompt, files_dict, self.memory, self.preprompts_holder\n        )\n        combined_dict = {**files_dict, **entrypoint}\n        files_dict = FilesDict(combined_dict)\n        return files_dict\n\n    def improve(\n        self,\n        files_dict: FilesDict,\n        prompt: Prompt,\n        execution_command: Optional[str] = None,\n    ) -> FilesDict:\n        files_dict = improve_fn(\n            self.ai, prompt, files_dict, self.memory, self.preprompts_holder\n        )\n        return files_dict\n\n\ndef default_config_agent():\n    \"\"\"\n    Creates an instance of SimpleAgent with default configuration.\n\n    Returns\n    -------\n    SimpleAgent\n        An instance of SimpleAgent with a temporary directory as its base path.\n    \"\"\"\n    return SimpleAgent.with_default_config(tempfile.mkdtemp())\n", "gpt_engineer/core/default/__init__.py": "", "gpt_engineer/benchmark/run.py": "\"\"\"\nModule for running benchmarks.\n\nThis module defines functions to run benchmarks using a given agent and to print\nthe results of the benchmark tasks.\n\nFunctions\n---------\nrun : function\n    Runs the benchmark tasks using the provided agent and returns a list of TaskResult objects.\n\nprint_results : function\n    Prints the results of the benchmark tasks to the console.\n\"\"\"\nimport time\n\nfrom typing import List\n\nimport yaml\n\nfrom gpt_engineer.benchmark.types import Assertable, Benchmark, TaskResult\nfrom gpt_engineer.core.base_agent import BaseAgent\nfrom gpt_engineer.core.default.disk_execution_env import DiskExecutionEnv\n\n\ndef run(\n    agent: BaseAgent,\n    benchmark: Benchmark,\n    verbose=False,\n) -> List[TaskResult]:\n    \"\"\"\n    Runs the benchmark tasks using the provided agent and returns a list of TaskResult objects.\n\n    Parameters\n    ----------\n    agent : BaseAgent\n        The agent to use for running the benchmark tasks.\n    benchmark : Benchmark\n        The benchmark containing the tasks to run.\n    verbose : bool, default=False\n        A flag to indicate whether to print verbose output during the benchmark.\n\n    Returns\n    -------\n    List[TaskResult]\n        A list of TaskResult objects representing the results of the benchmark tasks.\n    \"\"\"\n    task_results = []\n    for task in benchmark.tasks:\n        print(f\"--> Running task: {task.name}\\n\")\n\n        t0 = time.time()\n        files_dict = agent.improve(task.initial_code, task.prompt)\n        t1 = time.time()\n\n        env = DiskExecutionEnv()\n        env.upload(files_dict)\n\n        if task.command:\n            p = env.popen(task.command)\n            stdout, stderr = p.communicate(benchmark.timeout)\n            stdout, stderr = stdout.decode(\"utf-8\"), stderr.decode(\"utf-8\")\n        else:\n            p, stdout, stderr = None, None, None\n\n        exec_result = Assertable(\n            files=files_dict,\n            env=env,\n            process=p,\n            stdout=stdout,\n            stderr=stderr,\n        )\n\n        task_results.append(\n            TaskResult(\n                task_name=task.name,\n                assertion_results={\n                    assertion_name: assertion(exec_result)\n                    for assertion_name, assertion in task.assertions.items()\n                },\n                duration=t1 - t0,\n            )\n        )\n\n        if verbose:\n            print_results(task_results)\n    return task_results\n\n\ndef print_results(results: list[TaskResult]):\n    \"\"\"\n    Prints the results of the benchmark tasks to the console.\n\n    Parameters\n    ----------\n    results : list[TaskResult]\n        A list of TaskResult objects representing the results of the benchmark tasks.\n\n    Returns\n    -------\n    None\n    \"\"\"\n    for task_result in results:\n        print(f\"\\n--- Results for {task_result.task_name} ---\")\n        print(f\"{task_result.task_name} ({task_result.duration:.2f}s)\")\n        for assertion_name, assertion_result in task_result.assertion_results.items():\n            checkmark = \"\u2705\" if assertion_result else \"\u274c\"\n            print(f\"  {checkmark} {assertion_name}\")\n        print()\n\n    success_rates = [task_result.success_rate for task_result in results]\n    avg_success_rate = sum(success_rates) / len(results)\n\n    total_time = sum(task_result.duration for task_result in results)\n\n    correct_assertions = sum(\n        sum(\n            assertion_result\n            for assertion_result in task_result.assertion_results.values()\n        )\n        for task_result in results\n    )\n    total_assertions = sum(\n        len(task_result.assertion_results) for task_result in results\n    )\n    correct_tasks = [\n        task_result for task_result in results if task_result.success_rate == 1\n    ]\n\n    print(\"--- Results ---\")\n    print(f\"Total time: {total_time:.2f}s\")\n    print(f\"Completely correct tasks: {len(correct_tasks)}/{len(results)}\")\n    print(f\"Total correct assertions: {correct_assertions}/{total_assertions}\")\n    print(f\"Average success rate: {avg_success_rate * 100}% on {len(results)} tasks\")\n    print(\"--- Results ---\")\n    print()\n\n\ndef export_yaml_results(yaml_path, complete_results, config):\n    for results in complete_results.values():\n        correct_tasks = [\n            task_result\n            for task_result in results[\"detailed\"]\n            if task_result[\"solved\"] == 1.0\n        ]\n        fraction_correct = len(correct_tasks) / len(results[\"detailed\"])\n        results[\"fully_solved\"] = fraction_correct\n    complete_results[\"config\"] = config\n    with open(yaml_path, \"w\") as f:\n        yaml.dump(complete_results, f, indent=4)\n", "gpt_engineer/benchmark/bench_config.py": "from dataclasses import dataclass, field\nfrom pathlib import Path\n\nfrom tomlkit.items import Integer\n\nfrom gpt_engineer.core.project_config import read_config\n\n\n@dataclass\nclass AppsConfig:\n    active: bool | None = True\n    test_start_index: int | None = 0\n    test_end_index: int | None = 1\n    train_start_index: int | None = 0\n    train_end_index: int | None = 0\n    examples_per_problem: int | None = 10\n\n\n@dataclass\nclass MbppConfig:\n    active: bool | None = True\n    test_len: int | None = 1\n    train_len: int | None = 0\n\n\n@dataclass\nclass GptmeConfig:\n    active: bool | None = True\n\n\n@dataclass\nclass BenchConfig:\n    \"\"\"Configuration for the GPT Engineer CLI and gptengineer.app via `gpt-engineer.toml`.\"\"\"\n\n    apps: AppsConfig = field(default_factory=AppsConfig)\n    mbpp: MbppConfig = field(default_factory=MbppConfig)\n    gptme: GptmeConfig = field(default_factory=GptmeConfig)\n\n    @classmethod\n    def from_toml(cls, config_file: Path | str):\n        if isinstance(config_file, str):\n            config_file = Path(config_file)\n        config_dict = read_config(config_file)\n        return cls.from_dict(config_dict)\n\n    @classmethod\n    def from_dict(cls, config_dict: dict):\n        return cls(\n            apps=AppsConfig(**config_dict.get(\"apps\", {})),\n            mbpp=MbppConfig(**config_dict.get(\"mbpp\", {})),\n            gptme=GptmeConfig(**config_dict.get(\"gptme\", {})),\n        )\n\n    @staticmethod\n    def recursive_resolve(data_dict):\n        for key, value in data_dict.items():\n            if isinstance(value, Integer):\n                data_dict[key] = int(value)\n            elif isinstance(value, dict):\n                BenchConfig.recursive_resolve(value)\n\n    def to_dict(self):\n        dict_config = {\n            benchmark_name: {key: val for key, val in spec_config.__dict__.items()}\n            for benchmark_name, spec_config in self.__dict__.items()\n        }\n        BenchConfig.recursive_resolve(dict_config)\n\n        return dict_config\n", "gpt_engineer/benchmark/types.py": "\"\"\"\nModule defining types used in benchmarking.\n\nThis module contains dataclass definitions for various types used throughout the\nbenchmarking process, such as Assertable, Task, Benchmark, and TaskResult.\n\nClasses:\n    Assertable:\n        Represents an object that can be asserted against in a benchmark task.\n\n    Assertion:\n        Type alias for a callable that takes an Assertable and returns a boolean.\n\n    Task:\n        Represents a single task within a benchmark, including its assertions.\n\n    Benchmark:\n        Represents a collection of tasks used to evaluate a model's performance.\n\n    TaskResult:\n        Represents the result of running a single task within a benchmark.\n\"\"\"\nfrom dataclasses import dataclass\nfrom subprocess import Popen\nfrom typing import Callable, Dict, Optional\n\nfrom gpt_engineer.core.base_execution_env import BaseExecutionEnv\nfrom gpt_engineer.core.files_dict import FilesDict\nfrom gpt_engineer.core.prompt import Prompt\n\n\n@dataclass\nclass Assertable:\n    \"\"\"\n    A class representing an object which can be asserted against.\n\n    Attributes:\n        files (FilesDict): The code files involved in the assertion.\n        env (BaseExecutionEnv): The execution environment in which the code is run.\n        process (Popen): The subprocess in which the code is run.\n        stdout (str): The standard output from the code execution.\n        stderr (str): The standard error from the code execution.\n    \"\"\"\n\n    files: FilesDict\n    env: BaseExecutionEnv\n    process: Optional[Popen]\n    stdout: Optional[str]\n    stderr: Optional[str]\n\n\nAssertion = Callable[[Assertable], bool]\n\n\n@dataclass\nclass Task:\n    name: str\n    initial_code: Optional[FilesDict]\n    command: Optional[str]\n    prompt: Prompt\n    assertions: Optional[Dict[str, Assertion]]\n\n\n@dataclass\nclass Benchmark:\n    \"\"\"A benchmark is a collection of tasks that evaluate a model's performance.\"\"\"\n\n    name: str\n    tasks: list[Task]\n    timeout: Optional[int] = None\n\n\n@dataclass\nclass TaskResult:\n    task_name: str\n    assertion_results: dict[str, bool]\n    duration: float\n\n    # Returns success rate from 0.00 up to 1.00\n    @property\n    def success_rate(self) -> float:\n        if not self.assertion_results:\n            return 0.0\n\n        succeeded = len(\n            [result for result in self.assertion_results.values() if result is True]\n        )\n\n        return succeeded / len(self.assertion_results)\n\n    def to_dict(self) -> dict:\n        out_dict = {key: value for key, value in self.__dict__.items()}\n        out_dict[\"solved\"] = self.success_rate\n        return out_dict\n", "gpt_engineer/benchmark/__main__.py": "\"\"\"\nMain entry point for the benchmarking tool.\n\nThis module provides a command-line interface for running benchmarks using Typer.\nIt allows users to specify the path to an agent, the benchmark(s) to run, and other\noptions such as verbosity.\n\nFunctions\n---------\nget_agent : function\n    Dynamically imports and returns the default configuration agent from the given path.\n\nmain : function\n    The main function that runs the specified benchmarks with the given agent.\n    Outputs the results to the console.\n\nAttributes\n----------\n__name__ : str\n    The standard boilerplate for invoking the main function when the script is executed.\n\"\"\"\nimport importlib\nimport os.path\nimport sys\n\nfrom typing import Annotated, Optional\n\nimport typer\n\nfrom langchain.globals import set_llm_cache\nfrom langchain_community.cache import SQLiteCache\n\nfrom gpt_engineer.applications.cli.main import load_env_if_needed\nfrom gpt_engineer.benchmark.bench_config import BenchConfig\nfrom gpt_engineer.benchmark.benchmarks.load import get_benchmark\nfrom gpt_engineer.benchmark.run import export_yaml_results, print_results, run\n\napp = typer.Typer(\n    context_settings={\"help_option_names\": [\"-h\", \"--help\"]}\n)  # creates a CLI app\n\n\ndef get_agent(path):\n    \"\"\"\n    Dynamically imports and returns the default configuration agent from the given path.\n\n    Parameters\n    ----------\n    path : str\n        The file path to the module containing the default configuration agent.\n\n    Returns\n    -------\n    BaseAgent\n        An instance of the imported default configuration agent.\n    \"\"\"\n    # Dynamically import the python module at path\n    sys.path.append(os.path.dirname(path))\n    agent_module = importlib.import_module(path.replace(\"/\", \".\").replace(\".py\", \"\"))\n    return agent_module.default_config_agent()\n\n\n@app.command(\n    help=\"\"\"\n        Run any benchmark(s) against the specified agent.\n\n        \\b\n        Currently available benchmarks are: apps and mbpp\n    \"\"\"\n)\ndef main(\n    path_to_agent: Annotated[\n        str,\n        typer.Argument(\n            help=\"python file that contains a function called 'default_config_agent'\"\n        ),\n    ],\n    bench_config: Annotated[\n        str, typer.Argument(help=\"optional task name in benchmark\")\n    ] = os.path.join(os.path.dirname(__file__), \"default_bench_config.toml\"),\n    yaml_output: Annotated[\n        Optional[str],\n        typer.Option(help=\"print results for each task\", show_default=False),\n    ] = None,\n    verbose: Annotated[\n        Optional[bool],\n        typer.Option(help=\"print results for each task\", show_default=False),\n    ] = False,\n    use_cache: Annotated[\n        Optional[bool],\n        typer.Option(\n            help=\"Speeds up computations and saves tokens when running the same prompt multiple times by caching the LLM response.\",\n            show_default=False,\n        ),\n    ] = True,\n):\n    \"\"\"\n    The main function that runs the specified benchmarks with the given agent and outputs the results to the console.\n\n    Parameters\n    ----------\n    path_to_agent : str\n        The file path to the Python module that contains a function called 'default_config_agent'.\n    bench_config : str, default=default_bench_config.toml\n        Configuration file for choosing which benchmark problems to run. See default config for more details.\n    yaml_output: Optional[str], default=None\n        Pass a path to a yaml file to have results written to file.\n    verbose : Optional[bool], default=False\n        A flag to indicate whether to print results for each task.\n    use_cache : Optional[bool], default=True\n        Speeds up computations and saves tokens when running the same prompt multiple times by caching the LLM response.\n    Returns\n    -------\n    None\n    \"\"\"\n    if use_cache:\n        set_llm_cache(SQLiteCache(database_path=\".langchain.db\"))\n    load_env_if_needed()\n    config = BenchConfig.from_toml(bench_config)\n    print(\"using config file: \" + bench_config)\n    benchmarks = list()\n    benchmark_results = dict()\n    for specific_config_name in vars(config):\n        specific_config = getattr(config, specific_config_name)\n        if hasattr(specific_config, \"active\"):\n            if specific_config.active:\n                benchmarks.append(specific_config_name)\n\n    for benchmark_name in benchmarks:\n        benchmark = get_benchmark(benchmark_name, config)\n        if len(benchmark.tasks) == 0:\n            print(\n                benchmark_name\n                + \" was skipped, since no tasks are specified. Increase the number of tasks in the config file at: \"\n                + bench_config\n            )\n            continue\n        agent = get_agent(path_to_agent)\n\n        results = run(agent, benchmark, verbose=verbose)\n        print(\n            f\"\\n--- Results for agent {path_to_agent}, benchmark: {benchmark_name} ---\"\n        )\n        print_results(results)\n        print()\n        benchmark_results[benchmark_name] = {\n            \"detailed\": [result.to_dict() for result in results]\n        }\n    if yaml_output is not None:\n        export_yaml_results(yaml_output, benchmark_results, config.to_dict())\n\n\nif __name__ == \"__main__\":\n    typer.run(main)\n", "gpt_engineer/benchmark/__init__.py": "", "gpt_engineer/benchmark/benchmarks/load.py": "\"\"\"\nModule for loading benchmarks.\n\nThis module provides a central point to access different benchmarks by name.\nIt maps benchmark names to their respective loading functions.\n\nFunctions\n---------\nget_benchmark : function\n    Retrieves a Benchmark object by name. Raises ValueError if the benchmark is unknown.\n\"\"\"\nfrom gpt_engineer.benchmark.bench_config import BenchConfig\nfrom gpt_engineer.benchmark.benchmarks.apps.load import load_apps\nfrom gpt_engineer.benchmark.benchmarks.gptme.load import load_gptme\nfrom gpt_engineer.benchmark.benchmarks.mbpp.load import load_mbpp\nfrom gpt_engineer.benchmark.types import Benchmark\n\nBENCHMARKS = {\n    \"gptme\": load_gptme,\n    \"apps\": load_apps,\n    \"mbpp\": load_mbpp,\n}\n\n\ndef get_benchmark(name: str, config: BenchConfig) -> Benchmark:\n    \"\"\"\n    Retrieves a Benchmark object by name. Raises ValueError if the benchmark is unknown.\n\n    Parameters\n    ----------\n    name : str\n        The name of the benchmark to retrieve.\n    config : BenchConfig\n        Configuration object for the benchmarks.\n\n    Returns\n    -------\n    Benchmark\n        The Benchmark object corresponding to the given name.\n\n    Raises\n    ------\n    ValueError\n        If the benchmark name is not found in the BENCHMARKS mapping.\n    \"\"\"\n    if name not in BENCHMARKS:\n        raise ValueError(f\"Unknown benchmark {name}.\")\n    return BENCHMARKS[name](config.__getattribute__(name))\n", "gpt_engineer/benchmark/benchmarks/apps/load.py": "\"\"\"\nModule for loading APPS evaluation tasks.\n\nThis module provides functionality to load tasks for evaluating GPT-based models\non smaller, more focused tasks. It defines a set of tasks with predefined prompts\nand assertions to benchmark the performance of AI models.\n\nFunctions\n---------\nload_apps : function\n    Loads the APPS benchmark, which consists of a series coding problems.\n\"\"\"\nfrom pathlib import Path\nfrom subprocess import TimeoutExpired\nfrom typing import Union\n\nfrom datasets import Dataset, DatasetDict, load_dataset, load_from_disk\n\nfrom gpt_engineer.benchmark.bench_config import AppsConfig\nfrom gpt_engineer.benchmark.benchmarks.apps.problem import Problem\nfrom gpt_engineer.benchmark.types import Assertable, Benchmark, Task\nfrom gpt_engineer.core.default.disk_execution_env import DiskExecutionEnv\nfrom gpt_engineer.core.files_dict import FilesDict\nfrom gpt_engineer.core.prompt import Prompt\n\nDATASET_PATH = Path(__file__).parent / \"dataset\"\n\n\nclass AppsAssertion:\n    def __init__(self, expected: str, command: str):\n        self.expected_output = self._format(expected)\n        self.command = command\n\n    def evaluate(self, assertable: Assertable) -> bool:\n        # Create new execution environment for every run to avoid side effects\n        env = DiskExecutionEnv()\n        env.upload(assertable.files)\n        pro = env.popen(self.command)\n        try:\n            stdout, stderr = pro.communicate(timeout=2)\n            stdout, stderr = stdout.decode(\"utf-8\"), stderr.decode(\"utf-8\")\n        except TimeoutExpired:\n            print(\"Execution Timeout\")\n            return False\n\n        return self.expected_output in self._format(stdout)\n\n    def _format(self, string: str) -> str:\n        return string.replace(\" \", \"\").replace(\"\\n\", \"\")\n\n\ndef _get_dataset() -> Union[Dataset, DatasetDict]:\n    try:\n        return load_from_disk(str(DATASET_PATH))\n    except FileNotFoundError:\n        print(\"Dataset not found locally, downloading...\")\n\n    dataset = load_dataset(\"codeparrot/apps\", trust_remote_code=True)\n    dataset.save_to_disk(str(DATASET_PATH))\n\n    return dataset\n\n\ndef load_apps(config: AppsConfig) -> Benchmark:\n    \"\"\"\n    Loads the APPS benchmark, which consists of a series coding problems.\n\n    Returns\n    -------\n    Benchmark\n        A Benchmark object containing a list of Task objects for the APPS evaluation.\n    \"\"\"\n    dataset = _get_dataset()\n    tasks = []\n    problems = list()\n    for dataset_type in [\"test\", \"train\"]:\n        problems += [\n            Problem(\n                id=problem[\"problem_id\"],\n                question=problem[\"question\"],\n                input_output=problem[\"input_output\"],\n                starter_code=problem[\"starter_code\"],\n            )\n            for index, problem in enumerate(dataset[dataset_type])\n            if (index < config.__getattribute__(dataset_type + \"_end_index\"))\n            and (index >= config.__getattribute__(dataset_type + \"_start_index\"))\n        ]\n\n    for problem in problems:\n        prompt = Prompt(\n            problem.question\n            + \"\\nThe program, including its inputs, should be run from the command \"\n            \"line like 'python main \\\"input1 input2 etc \\\"', with all inputs inside \"\n            \"the quotation marks. The program should not read inputs from stdin.\"\n        )\n\n        tasks.append(\n            Task(\n                name=str(problem.id),\n                initial_code=FilesDict({\"main.py\": problem.starter_code}),\n                command=None,  # Explicitly setting `None` because each assertion specifies its command\n                prompt=prompt,\n                assertions={\n                    f\"correct output {i}\": AppsAssertion(\n                        expected=problem.outputs[i],\n                        command=\"python main.py\" + ' \"' + problem.inputs[i] + '\"',\n                    ).evaluate\n                    for i in range(\n                        min(len(problem.outputs), config.examples_per_problem)\n                    )\n                },\n            )\n        )\n\n    return Benchmark(\n        name=\"apps\",\n        tasks=tasks,\n    )\n", "gpt_engineer/benchmark/benchmarks/apps/problem.py": "import json\n\nfrom dataclasses import dataclass\nfrom functools import cached_property\nfrom typing import List\n\n\n@dataclass(frozen=True)\nclass Problem:\n    id: int\n    question: str\n    input_output: str\n    starter_code: str\n\n    @property\n    def inputs(self) -> List[str]:\n        return self._parsed_inputs_outputs[\"inputs\"]\n\n    @property\n    def outputs(self) -> List[str]:\n        return self._parsed_inputs_outputs[\"outputs\"]\n\n    @cached_property\n    def _parsed_inputs_outputs(self):\n        return json.loads(self.input_output.replace(\"\\n\", \"\"))\n", "gpt_engineer/benchmark/benchmarks/apps/problems.py": "# TODO: Pick problems\n# Temporary testing against these problems\nPROBLEM_IDS = list(range(0, 50))\n", "gpt_engineer/benchmark/benchmarks/gptme/load.py": "\"\"\"\nModule for loading GPT-Me evaluation tasks.\n\nThis module provides functionality to load tasks for evaluating GPT-based models\non smaller, more focused tasks. It defines a set of tasks with predefined prompts\nand assertions to benchmark the performance of AI models.\n\nFunctions\n---------\nload_gptme : function\n    Loads the GPT-Me benchmark, which consists of a series of tasks for evaluation.\n\"\"\"\nfrom gpt_engineer.benchmark.bench_config import GptmeConfig\nfrom gpt_engineer.benchmark.types import Benchmark, Task\nfrom gpt_engineer.core.files_dict import FilesDict\nfrom gpt_engineer.core.prompt import Prompt\n\n\ndef load_gptme(config: GptmeConfig) -> Benchmark:\n    \"\"\"\n    Loads the GPT-Me benchmark, which consists of a series of tasks for evaluation.\n\n    Returns\n    -------\n    Benchmark\n        A Benchmark object containing a list of Task objects for the GPT-Me evaluation.\n    \"\"\"\n    return Benchmark(\n        name=\"gptme\",\n        tasks=[\n            Task(\n                name=\"hello\",\n                initial_code=FilesDict({\"hello.py\": \"print('Hello, world!')\"}),\n                command=\"python hello.py\",\n                prompt=Prompt(\"Change the code in hello.py to print 'Hello, human!'\"),\n                assertions={\n                    \"correct output\": lambda assertable: assertable.stdout\n                    == \"Hello, human!\\n\",\n                    \"correct file\": lambda assertable: assertable.files[\n                        \"hello.py\"\n                    ].strip()\n                    == \"print('Hello, human!')\",\n                },\n            ),\n            Task(\n                name=\"hello-patch\",\n                initial_code=FilesDict({\"hello.py\": \"print('Hello, world!')\"}),\n                command=\"python hello.py\",\n                prompt=Prompt(\"Patch the code in hello.py to print 'Hello, human!'\"),\n                assertions={\n                    \"correct output\": lambda assertable: assertable.stdout\n                    == \"Hello, human!\\n\",\n                    \"correct file\": lambda assertable: assertable.files[\n                        \"hello.py\"\n                    ].strip()\n                    == \"print('Hello, human!')\",\n                },\n            ),\n            Task(\n                name=\"hello-ask\",\n                initial_code=FilesDict({\"hello.py\": \"print('Hello, world!')\"}),\n                command=\"echo 'Erik' | python hello.py\",\n                prompt=Prompt(\n                    \"modify hello.py to ask the user for their name and print 'Hello, <name>!'. don't try to execute it\"\n                ),\n                assertions={\n                    \"correct output\": lambda assertable: \"Hello, Erik!\"\n                    in assertable.stdout,\n                },\n            ),\n            Task(\n                name=\"prime100\",\n                initial_code=FilesDict(\n                    {}\n                ),  # Empty dictionary since no initial code is provided\n                command=\"python prime.py\",\n                prompt=Prompt(\n                    \"write a script prime.py that computes and prints the 100th prime number\"\n                ),\n                assertions={\n                    \"correct output\": lambda assertable: \"541\"\n                    in assertable.stdout.split(),\n                },\n            ),\n            Task(\n                name=\"init-git\",\n                initial_code=FilesDict(\n                    {}\n                ),  # Empty dictionary since no initial code is provided\n                command=\"git status\",\n                prompt=Prompt(\n                    \"initialize a git repository, write a main.py file, and commit it\"\n                ),\n                assertions={\n                    \"clean exit\": lambda assertable: assertable.process.returncode == 0,\n                    \"clean working tree\": lambda assertable: \"nothing to commit, working tree clean\"\n                    in assertable.stdout,\n                    \"main.py exists\": lambda assertable: \"main.py\" in assertable.files,\n                    \"we have a commit\": lambda assertable: \"No commits yet\"\n                    not in assertable.stdout,\n                },\n            ),\n        ],\n    )\n", "gpt_engineer/benchmark/benchmarks/mbpp/load.py": "\"\"\"\nModule for loading MBPP evaluation tasks.\n\nThis module provides functionality to load tasks for evaluating GPT-based models\non smaller, more focused tasks. It defines a set of tasks with predefined prompts\nand assertions to benchmark the performance of AI models.\n\nFunctions\n---------\nload_mbpp : function\n    Loads the MBPP benchmark, which consists of a series coding problems.\n\"\"\"\nfrom pathlib import Path\nfrom subprocess import TimeoutExpired\nfrom typing import Union\n\nfrom datasets import Dataset, DatasetDict, load_dataset, load_from_disk\n\nfrom gpt_engineer.benchmark.bench_config import MbppConfig\nfrom gpt_engineer.benchmark.benchmarks.mbpp.problem import Problem\nfrom gpt_engineer.benchmark.types import Assertable, Benchmark, Task\nfrom gpt_engineer.core.default.disk_execution_env import DiskExecutionEnv\nfrom gpt_engineer.core.files_dict import FilesDict\nfrom gpt_engineer.core.prompt import Prompt\n\nDATASET_PATH = Path(__file__).parent / \"dataset\"\n\n\nclass MbppAssertion:\n    def __init__(self, assertion: str):\n        self.assertion = assertion\n\n    def evaluate(self, assertable: Assertable) -> bool:\n        generated_code = assertable.files[\"main.py\"]\n        code_with_assertion = f\"{generated_code}\\n{self.assertion}\"\n\n        # Create new execution environment for every run to avoid side effects\n        env = DiskExecutionEnv()\n        env.upload(FilesDict({\"main.py\": code_with_assertion}))\n        pro = env.popen(\"python main.py\")\n\n        try:\n            stdout, stderr = pro.communicate(timeout=2)\n            stdout, stderr = stdout.decode(\"utf-8\"), stderr.decode(\"utf-8\")\n        except TimeoutExpired:\n            print(\"Execution Timeout\")\n            return False\n\n        return not stderr\n\n\ndef _get_dataset() -> Union[Dataset, DatasetDict]:\n    try:\n        return load_from_disk(str(DATASET_PATH))\n    except FileNotFoundError:\n        print(\"Dataset not found locally, downloading...\")\n\n    dataset = load_dataset(\"mbpp\", \"sanitized\", trust_remote_code=True)\n    dataset.save_to_disk(str(DATASET_PATH))\n\n    return dataset\n\n\ndef load_mbpp(config: MbppConfig) -> Benchmark:\n    \"\"\"\n    Loads the MBPP benchmark, which consists of a series coding problems.\n\n    Returns\n    -------\n    Benchmark\n        A Benchmark object containing a list of Task objects for the MBPP evaluation.\n    \"\"\"\n    dataset = _get_dataset()\n    tasks = []\n    problems = []\n    for dataset_type in [\"test\", \"train\"]:\n        problems += [\n            Problem(\n                source_file=problem[\"source_file\"],\n                task_id=problem[\"task_id\"],\n                prompt=problem[\"prompt\"],\n                code=problem[\"code\"],\n                test_imports=problem[\"test_imports\"],\n                test_list=problem[\"test_list\"],\n            )\n            for index, problem in enumerate(dataset[dataset_type])\n            if index < config.__getattribute__(dataset_type + \"_len\")\n        ]\n\n    for problem in problems:\n        prompt = Prompt(\n            problem.prompt\n            + \"Please extend given function without changing it's declaration including arguments.\"\n        )\n\n        tasks.append(\n            Task(\n                name=str(problem.task_id),\n                initial_code=FilesDict({\"main.py\": problem.starting_code}),\n                command=None,  # Explicitly setting `None` because each assertion runs code\n                prompt=prompt,\n                assertions={\n                    f\"correct assertion {i}\": MbppAssertion(\n                        assertion=assertion\n                    ).evaluate\n                    for i, assertion in enumerate(problem.test_list)\n                },\n            )\n        )\n\n    return Benchmark(\n        name=\"mbpp\",\n        tasks=tasks,\n    )\n", "gpt_engineer/benchmark/benchmarks/mbpp/problem.py": "from dataclasses import dataclass\nfrom typing import List\n\n\n@dataclass(frozen=True)\nclass Problem:\n    source_file: int\n    task_id: str\n    prompt: str\n    code: str\n    test_imports: str\n    test_list: List[str]\n\n    @property\n    def starting_code(self) -> str:\n        lines: List[str] = []\n\n        for line in self.code.split(\"\\n\"):\n            lines.append(line)\n\n            if line.startswith(\"def \"):\n                lines.append(\"pass #  TODO: Implement method\\n\")\n                break\n\n        return \"\\n\".join(lines)\n", "gpt_engineer/benchmark/benchmarks/mbpp/problems.py": "# TODO: Pick problems\n# Temporary testing against these problems\nPROBLEM_IDS = range(0, 100)\n", "scripts/print_chat.py": "\"\"\"\nThis module provides functionality to print a conversation with messages\ncolored according to the role of the speaker.\n\"\"\"\n\nimport json\n\nimport typer\n\nfrom termcolor import colored\n\napp = typer.Typer()\n\n\ndef pretty_print_conversation(messages):\n    \"\"\"\n    Prints a conversation with messages formatted and colored by role.\n\n    Parameters\n    ----------\n    messages : list\n        A list of message dictionaries, each containing 'role', 'name', and 'content' keys.\n\n    \"\"\"\n\n    role_to_color = {\n        \"system\": \"red\",\n        \"user\": \"green\",\n        \"assistant\": \"blue\",\n        \"function\": \"magenta\",\n    }\n    formatted_messages = []\n    for message in messages:\n        if message[\"role\"] == \"function\":\n            formatted_messages.append(\n                f\"function ({message['name']}): {message['content']}\\n\"\n            )\n        else:\n            assistant_content = (\n                message[\"function_call\"]\n                if message.get(\"function_call\")\n                else message[\"content\"]\n            )\n            role_to_message = {\n                \"system\": f\"system: {message['content']}\\n\",\n                \"user\": f\"user: {message['content']}\\n\",\n                \"assistant\": f\"assistant: {assistant_content}\\n\",\n            }\n            formatted_messages.append(role_to_message[message[\"role\"]])\n\n    for formatted_message in formatted_messages:\n        role = messages[formatted_messages.index(formatted_message)][\"role\"]\n        color = role_to_color[role]\n        print(colored(formatted_message, color))\n\n\n@app.command()\ndef main(\n    messages_path: str,\n):\n    \"\"\"\n    Main function that loads messages from a JSON file and prints them using pretty formatting.\n\n    Parameters\n    ----------\n    messages_path : str\n        The file path to the JSON file containing the messages.\n\n    \"\"\"\n    with open(messages_path) as f:\n        messages = json.load(f)\n\n    pretty_print_conversation(messages)\n\n\nif __name__ == \"__main__\":\n    app()\n", "scripts/clean_benchmarks.py": "\"\"\"\nThis module provides functionality to clean up benchmark directories by removing\nall files and folders except for 'prompt' and 'main_prompt'.\n\"\"\"\n\n# list all folders in benchmark folder\n# for each folder, run the benchmark\n\nimport os\nimport shutil\n\nfrom pathlib import Path\n\nfrom typer import run\n\n\ndef main():\n    \"\"\"\n    Main function that iterates through all directories in the 'benchmark' folder\n    and cleans them by removing all files and directories except for 'prompt' and\n    'main_prompt'.\n    \"\"\"\n\n    benchmarks = Path(\"benchmark\")\n\n    for benchmark in benchmarks.iterdir():\n        if benchmark.is_dir():\n            print(f\"Cleaning {benchmark}\")\n            for path in benchmark.iterdir():\n                if path.name in [\"prompt\", \"main_prompt\"]:\n                    continue\n\n                # Get filename of Path object\n                if path.is_dir():\n                    # delete the entire directory\n                    shutil.rmtree(path)\n                else:\n                    # delete the file\n                    os.remove(path)\n\n\nif __name__ == \"__main__\":\n    run(main)\n", "scripts/legacy_benchmark.py": "\"\"\"\nThis module provides functionality to run benchmarks on different folders within\nthe 'benchmark' directory, wait for their completion, and generate a report.\n\"\"\"\n\n# list all folders in benchmark folder\n# for each folder, run the benchmark\nimport contextlib\nimport json\nimport os\nimport subprocess\n\nfrom datetime import datetime\nfrom itertools import islice\nfrom pathlib import Path\nfrom typing import Iterable, Union\n\nfrom tabulate import tabulate\nfrom typer import run\n\n\ndef main(\n    n_benchmarks: Union[int, None] = None,\n):\n    \"\"\"\n    Main function that runs benchmarks on folders within the 'benchmark' directory.\n\n    Parameters\n    ----------\n    n_benchmarks : Union[int, None], optional\n        The number of benchmarks to run. If None, all benchmarks are run.\n\n    \"\"\"\n\n    path = Path(\"benchmark\")\n\n    folders: Iterable[Path] = path.iterdir()\n\n    if n_benchmarks:\n        folders = islice(folders, n_benchmarks)\n\n    benchmarks = []\n    results = []\n    for bench_folder in folders:\n        if os.path.isdir(bench_folder):\n            print(f\"Running benchmark for {bench_folder}\")\n\n            log_path = bench_folder / \"log.txt\"\n            log_file = open(log_path, \"w\")\n            process = subprocess.Popen(\n                [\n                    \"python\",\n                    \"-u\",  # Unbuffered output\n                    \"-m\",\n                    \"gpt_engineer.cli.main\",\n                    bench_folder,\n                    \"--steps\",\n                    \"benchmark\",\n                ],\n                stdout=log_file,\n                stderr=log_file,\n                bufsize=0,\n            )\n            benchmarks.append(bench_folder)\n            results.append((process, log_file))\n\n            print(\"You can stream the log file by running:\")\n            print(f\"tail -f {log_path}\")\n            print()\n\n    for bench_folder, (process, file) in zip(benchmarks, results):\n        process.wait()\n        file.close()\n\n        print(\"process\", bench_folder.name, \"finished with code\", process.returncode)\n        print(\"Running it. Original benchmark prompt:\")\n        print()\n        with open(bench_folder / \"prompt\") as f:\n            print(f.read())\n        print()\n\n        with contextlib.suppress(KeyboardInterrupt):\n            subprocess.run(\n                [\n                    \"python\",\n                    \"-m\",\n                    \"gpt_engineer.cli.main\",\n                    bench_folder,\n                    \"--steps\",\n                    \"evaluate\",\n                ],\n            )\n\n    generate_report(benchmarks, path)\n\n\ndef generate_report(benchmarks, benchmark_path):\n    \"\"\"\n    Generates a report of the benchmark results and optionally appends it to a markdown file.\n\n    Parameters\n    ----------\n    benchmarks : list\n        A list of benchmark folder paths that have been processed.\n    benchmark_path : Path\n        The path to the benchmark directory.\n\n    \"\"\"\n\n    headers = [\"Benchmark\", \"Ran\", \"Works\", \"Perfect\", \"Notes\"]\n    rows = []\n    for bench_folder in benchmarks:\n        memory = bench_folder / \".gpteng\" / \"memory\"\n        with open(memory / \"review\") as f:\n            review = json.loads(f.read())\n            rows.append(\n                [\n                    bench_folder.name,\n                    to_emoji(review.get(\"ran\", None)),\n                    to_emoji(review.get(\"works\", None)),\n                    to_emoji(review.get(\"perfect\", None)),\n                    review.get(\"comments\", None),\n                ]\n            )\n    table: str = tabulate(rows, headers, tablefmt=\"pipe\")\n    print(\"\\nBenchmark report:\\n\")\n    print(table)\n    print()\n    append_to_results = ask_yes_no(\"Append report to the results file?\")\n    if append_to_results:\n        results_path = benchmark_path / \"RESULTS.md\"\n        current_date = datetime.now().strftime(\"%Y-%m-%d\")\n        insert_markdown_section(results_path, current_date, table, 2)\n\n\ndef to_emoji(value: bool) -> str:\n    \"\"\"\n    Converts a boolean value to its corresponding emoji representation.\n\n    Parameters\n    ----------\n    value : bool\n        The boolean value to convert.\n\n    Returns\n    -------\n    str\n        An emoji string representing the boolean value.\n\n    \"\"\"\n\n    return \"\\U00002705\" if value else \"\\U0000274C\"\n\n\ndef insert_markdown_section(file_path, section_title, section_text, level):\n    \"\"\"\n    Inserts a new section into a markdown file at the specified level.\n\n    Parameters\n    ----------\n    file_path : Path\n        The path to the markdown file.\n    section_title : str\n        The title of the section to insert.\n    section_text : str\n        The text content of the section to insert.\n    level : int\n        The header level of the section.\n\n    \"\"\"\n\n    with open(file_path, \"r\") as file:\n        lines = file.readlines()\n\n    header_prefix = \"#\" * level\n    new_section = f\"{header_prefix} {section_title}\\n\\n{section_text}\\n\\n\"\n\n    # Find the first section with the specified level\n    line_number = -1\n    for i, line in enumerate(lines):\n        if line.startswith(header_prefix):\n            line_number = i\n            break\n\n    if line_number != -1:\n        lines.insert(line_number, new_section)\n    else:\n        print(\n            f\"Markdown file was of unexpected format. No section of level {level} found. \"\n            \"Did not write results.\"\n        )\n        return\n\n    # Write the file\n    with open(file_path, \"w\") as file:\n        file.writelines(lines)\n\n\ndef ask_yes_no(question: str) -> bool:\n    \"\"\"\n    Asks a yes/no question and returns the response as a boolean value.\n\n    Parameters\n    ----------\n    question : str\n        The yes/no question to ask.\n\n    Returns\n    -------\n    bool\n        True if the answer is 'yes', False if 'no'.\n\n    \"\"\"\n\n    while True:\n        response = input(question + \" (y/n): \").lower().strip()\n        if response == \"y\":\n            return True\n        elif response == \"n\":\n            return False\n        else:\n            print(\"Please enter either 'y' or 'n'.\")\n\n\nif __name__ == \"__main__\":\n    run(main)\n"}