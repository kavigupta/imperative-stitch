{"projects/example-improve/model.py": "import random\n\nfrom dataclasses import dataclass\n\n\n@dataclass\nclass Point:\n    x: int\n    y: int\n\n\nclass Game:\n    def __init__(self):\n        self.snake = [Point(5, 5)]\n        self.food = self.generate_food()\n        self.is_running = True\n\n    def generate_food(self):\n        return Point(random.randint(0, 10), random.randint(0, 10))\n\n    def update(self):\n        # Move the snake\n        self.snake.move()\n\n        # Check for collision with food\n        if self.snake.head == self.food:\n            self.snake.grow()\n            self.food = self.generate_food()\n\n        # Check for collision with boundaries\n        if not (0 <= self.snake.head.x < 10 and 0 <= self.snake.head.y < 10):\n            self.is_running = False\n", "projects/example-improve/view.py": "from model import Point\n\n\nclass View:\n    def __init__(self, game):\n        self.game = game\n\n    def render(self):\n        # Print the game state\n        for y in range(10):\n            for x in range(10):\n                if Point(x, y) in self.game.snake:\n                    print(\"S\", end=\"\")\n                elif Point(x, y) == self.game.food:\n                    print(\"F\", end=\"\")\n                else:\n                    print(\".\", end=\"\")\n            print()\n        print()\n", "projects/example-improve/controller.py": "import keyboard\n\n\nclass Controller:\n    def __init__(self, game, view):\n        self.game = game\n        self.view = view\n\n    def handle_input(self):\n        if keyboard.is_pressed(\"up\") and not hasattr(self, \"last_key_pressed\"):\n            self.game.move(\"down\")\n            self.last_key_pressed = \"up\"\n        elif hasattr(self, \"last_key_pressed\") and self.last_key_pressed == \"up\":\n            self.game.move(\"right\")\n            del self.last_key_pressed\n        elif keyboard.is_pressed(\"down\"):\n            self.game.move(\"up\")\n        elif keyboard.is_pressed(\"left\"):\n            self.game.move(\"right\")\n        elif keyboard.is_pressed(\"right\"):\n            self.game.move(\"left\")\n", "projects/example-improve/main.py": "from controller import Controller\nfrom model import Game\nfrom view import View\n\n\ndef main():\n    game = Game()\n    view = View(game)\n    controller = Controller(game, view)\n\n    while game.is_running:\n        controller.handle_input()\n        game.update()\n        view.render()\n\n\nif __name__ == \"__main__\":\n    main()\n", "gpt_engineer/__init__.py": "# Adding convenience imports to the package\n\n# from gpt_engineer.tools import code_vector_repository\n# from gpt_engineer.core.default import on_disk_repository\n", "gpt_engineer/tools/supported_languages.py": "\"\"\"\nThis module defines the supported programming languages for document chunking.\n\nVariables:\n    SUPPORTED_LANGUAGES (list): A list of dictionaries defining supported languages.\n\"\"\"\n\nSUPPORTED_LANGUAGES = [\n    {\"name\": \"Python\", \"extensions\": [\".py\"], \"tree_sitter_name\": \"python\"},\n    {\n        \"name\": \"JavaScript\",\n        \"extensions\": [\".js\", \".mjs\"],\n        \"tree_sitter_name\": \"javascript\",\n    },\n    {\"name\": \"HTML\", \"extensions\": [\".html\", \".htm\"], \"tree_sitter_name\": \"html\"},\n    {\"name\": \"CSS\", \"extensions\": [\".css\"], \"tree_sitter_name\": \"css\"},\n    {\"name\": \"Java\", \"extensions\": [\".java\"], \"tree_sitter_name\": \"java\"},\n    {\"name\": \"C#\", \"extensions\": [\".cs\"], \"tree_sitter_name\": \"c_sharp\"},\n    {\n        \"name\": \"TypeScript\",\n        \"extensions\": [\".ts\", \".tsx\"],\n        \"tree_sitter_name\": \"typescript\",\n    },\n    {\"name\": \"Ruby\", \"extensions\": [\".rb\", \".erb\"], \"tree_sitter_name\": \"ruby\"},\n    {\n        \"name\": \"PHP\",\n        \"extensions\": [\n            \".php\",\n            \".phtml\",\n            \".php3\",\n            \".php4\",\n            \".php5\",\n            \".php7\",\n            \".phps\",\n            \".php-s\",\n            \".pht\",\n            \".phar\",\n        ],\n        \"tree_sitter_name\": \"php\",\n    },\n    {\"name\": \"Go\", \"extensions\": [\".go\"], \"tree_sitter_name\": \"go\"},\n    {\"name\": \"Kotlin\", \"extensions\": [\".kt\", \".kts\"], \"tree_sitter_name\": \"kotlin\"},\n    {\"name\": \"Rust\", \"extensions\": [\".rs\"], \"tree_sitter_name\": \"rust\"},\n    {\n        \"name\": \"C++\",\n        \"extensions\": [\".cpp\", \".cc\", \".cxx\", \".h\", \".hpp\", \".hxx\"],\n        \"tree_sitter_name\": \"cpp\",\n    },\n    {\"name\": \"C\", \"extensions\": [\".c\", \".h\"], \"tree_sitter_name\": \"c\"}\n    # ---- the following are not supported by the current code chunker implementation ----\n    # {\n    #     \"name\": \"Swift\",\n    #     \"extensions\": [\".swift\"],\n    #     \"tree_sitter_name\": \"swift\"\n    # },\n]\n", "gpt_engineer/tools/custom_steps.py": "from platform import platform\nfrom sys import version_info\nfrom typing import List, Union\n\nfrom langchain.schema import AIMessage, HumanMessage, SystemMessage\n\nfrom gpt_engineer.core.ai import AI\nfrom gpt_engineer.core.base_execution_env import BaseExecutionEnv\nfrom gpt_engineer.core.base_memory import BaseMemory\nfrom gpt_engineer.core.chat_to_files import chat_to_files_dict\nfrom gpt_engineer.core.default.paths import CODE_GEN_LOG_FILE, ENTRYPOINT_FILE\nfrom gpt_engineer.core.default.steps import curr_fn, improve_fn, setup_sys_prompt\nfrom gpt_engineer.core.files_dict import FilesDict\nfrom gpt_engineer.core.preprompts_holder import PrepromptsHolder\nfrom gpt_engineer.core.prompt import Prompt\n\n# Type hint for chat messages\nMessage = Union[AIMessage, HumanMessage, SystemMessage]\nMAX_SELF_HEAL_ATTEMPTS = 10\n\n\ndef get_platform_info() -> str:\n    \"\"\"\n    Returns a string containing the OS and Python version information.\n\n    This function is used for self-healing by providing information about the current\n    operating system and Python version. It assumes that the Python version in the\n    virtual environment is the one being used.\n\n    Returns:\n        str: A string containing the OS and Python version information.\n    \"\"\"\n\n    v = version_info\n    a = f\"Python Version: {v.major}.{v.minor}.{v.micro}\"\n    b = f\"\\nOS: {platform()}\\n\"\n    return a + b\n\n\ndef self_heal(\n    ai: AI,\n    execution_env: BaseExecutionEnv,\n    files_dict: FilesDict,\n    prompt: Prompt = None,\n    preprompts_holder: PrepromptsHolder = None,\n    memory: BaseMemory = None,\n) -> FilesDict:\n    \"\"\"\n    Attempts to execute the code from the entrypoint and if it fails, sends the error output back to the AI with instructions to fix.\n\n    Parameters\n    ----------\n    ai : AI\n        An instance of the AI model.\n    execution_env : BaseExecutionEnv\n        The execution environment where the code is run.\n    files_dict : FilesDict\n        A dictionary of file names to their contents.\n    preprompts_holder : PrepromptsHolder, optional\n        A holder for preprompt messages.\n\n    Returns\n    -------\n    FilesDict\n        The updated files dictionary after self-healing attempts.\n\n    Raises\n    ------\n    FileNotFoundError\n        If the required entrypoint file does not exist in the code.\n    AssertionError\n        If the preprompts_holder is None.\n\n    Notes\n    -----\n    This code will make `MAX_SELF_HEAL_ATTEMPTS` to try and fix the code\n    before giving up.\n    This makes the assuption that the previous step was `gen_entrypoint`,\n    this code could work with `simple_gen`, or `gen_clarified_code` as well.\n    \"\"\"\n\n    # step 1. execute the entrypoint\n    # log_path = dbs.workspace.path / \"log.txt\"\n    if ENTRYPOINT_FILE not in files_dict:\n        raise FileNotFoundError(\n            \"The required entrypoint \"\n            + ENTRYPOINT_FILE\n            + \" does not exist in the code.\"\n        )\n\n    attempts = 0\n    if preprompts_holder is None:\n        raise AssertionError(\"Prepromptsholder required for self-heal\")\n    while attempts < MAX_SELF_HEAL_ATTEMPTS:\n        attempts += 1\n        timed_out = False\n\n        # Start the process\n        execution_env.upload(files_dict)\n        p = execution_env.popen(files_dict[ENTRYPOINT_FILE])\n\n        # Wait for the process to complete and get output\n        stdout_full, stderr_full = p.communicate()\n\n        if (p.returncode != 0 and p.returncode != 2) and not timed_out:\n            print(\"run.sh failed.  The log is:\")\n            print(stdout_full.decode(\"utf-8\"))\n            print(stderr_full.decode(\"utf-8\"))\n\n            new_prompt = Prompt(\n                f\"A program with this specification was requested:\\n{prompt}\\n, but running it produced the following output:\\n{stdout_full}\\n and the following errors:\\n{stderr_full}. Please change it so that it fulfills the requirements.\"\n            )\n            files_dict = improve_fn(\n                ai, new_prompt, files_dict, memory, preprompts_holder\n            )\n        else:\n            break\n    return files_dict\n\n\ndef clarified_gen(\n    ai: AI, prompt: Prompt, memory: BaseMemory, preprompts_holder: PrepromptsHolder\n) -> FilesDict:\n    \"\"\"\n    Generates code based on clarifications obtained from the user and saves it to a specified workspace.\n\n    Parameters\n    ----------\n    ai : AI\n        An instance of the AI model, responsible for processing and generating the code.\n    prompt : str\n        The user's clarification prompt.\n    memory : BaseMemory\n        The memory instance where the generated code log is saved.\n    preprompts_holder : PrepromptsHolder\n        A holder for preprompt messages.\n\n    Returns\n    -------\n    FilesDict\n        A dictionary of file names to their contents generated by the AI.\n    \"\"\"\n\n    preprompts = preprompts_holder.get_preprompts()\n    messages: List[Message] = [SystemMessage(content=preprompts[\"clarify\"])]\n    user_input = prompt.text  # clarify does not work with vision right now\n    while True:\n        messages = ai.next(messages, user_input, step_name=curr_fn())\n        msg = messages[-1].content.strip()\n\n        if \"nothing to clarify\" in msg.lower():\n            break\n\n        if msg.lower().startswith(\"no\"):\n            print(\"Nothing to clarify.\")\n            break\n\n        print('(answer in text, or \"c\" to move on)\\n')\n        user_input = input(\"\")\n        print()\n\n        if not user_input or user_input == \"c\":\n            print(\"(letting gpt-engineer make its own assumptions)\")\n            print()\n            messages = ai.next(\n                messages,\n                \"Make your own assumptions and state them explicitly before starting\",\n                step_name=curr_fn(),\n            )\n            print()\n\n        user_input += \"\"\"\n            \\n\\n\n            Is anything else unclear? If yes, ask another question.\\n\n            Otherwise state: \"Nothing to clarify\"\n            \"\"\"\n\n    print()\n\n    messages = [\n        SystemMessage(content=setup_sys_prompt(preprompts)),\n    ] + messages[\n        1:\n    ]  # skip the first clarify message, which was the original clarify priming prompt\n    messages = ai.next(\n        messages,\n        preprompts[\"generate\"].replace(\"FILE_FORMAT\", preprompts[\"file_format\"]),\n        step_name=curr_fn(),\n    )\n    print()\n    chat = messages[-1].content.strip()\n    memory.log(CODE_GEN_LOG_FILE, \"\\n\\n\".join(x.pretty_repr() for x in messages))\n    files_dict = chat_to_files_dict(chat)\n    return files_dict\n\n\ndef lite_gen(\n    ai: AI, prompt: Prompt, memory: BaseMemory, preprompts_holder: PrepromptsHolder\n) -> FilesDict:\n    \"\"\"\n    Executes the AI model using the main prompt and saves the generated results to the specified workspace.\n\n    Parameters\n    ----------\n    ai : AI\n        An instance of the AI model.\n    prompt : str\n        The main prompt to feed to the AI model.\n    memory : BaseMemory\n        The memory instance where the generated code log is saved.\n    preprompts_holder : PrepromptsHolder\n        A holder for preprompt messages.\n\n    Returns\n    -------\n    FilesDict\n        A dictionary of file names to their contents generated by the AI.\n\n    Notes\n    -----\n    The function assumes the `ai.start` method and the `to_files` utility to be correctly\n    set up and functional. Ensure these prerequisites before invoking `lite_gen`.\n    \"\"\"\n\n    preprompts = preprompts_holder.get_preprompts()\n    messages = ai.start(\n        prompt.to_langchain_content(), preprompts[\"file_format\"], step_name=curr_fn()\n    )\n    chat = messages[-1].content.strip()\n    memory.log(CODE_GEN_LOG_FILE, \"\\n\\n\".join(x.pretty_repr() for x in messages))\n    files_dict = chat_to_files_dict(chat)\n    return files_dict\n", "gpt_engineer/tools/__init__.py": "", "gpt_engineer/applications/__init__.py": "", "gpt_engineer/applications/cli/collect.py": "\"\"\"\nModule `collect` - Data Handling and RudderStack Integration\n\nThis module provides functionalities to handle and send learning data to RudderStack\nfor the purpose of analysis and to improve the gpt-engineer system. The data is sent\nonly when the user gives consent to share.\n\nFunctions:\n    send_learning(learning): Sends learning data to RudderStack.\n    collect_learnings(prompt, model, temperature, config, memory, review): Processes and sends learning data.\n    collect_and_send_human_review(prompt, model, temperature, config, memory): Collects human feedback and sends it.\n\nDependencies:\n    hashlib: For generating SHA-256 hash.\n    typing: For type annotations.\n    gpt_engineer.core: Core functionalities of gpt-engineer.\n    gpt_engineer.cli.learning: Handles the extraction of learning data.\n\nNotes:\n    Data sent to RudderStack is not shared with third parties and is used solely to\n    improve gpt-engineer and allow it to handle a broader range of use cases.\n    Consent logic is in gpt_engineer/learning.py.\n\"\"\"\n\nfrom typing import Tuple\n\nfrom gpt_engineer.applications.cli.learning import (\n    Learning,\n    Review,\n    extract_learning,\n    human_review_input,\n)\nfrom gpt_engineer.core.default.disk_memory import DiskMemory\nfrom gpt_engineer.core.prompt import Prompt\n\n\ndef send_learning(learning: Learning):\n    \"\"\"\n    Send the learning data to RudderStack for analysis.\n\n    Parameters\n    ----------\n    learning : Learning\n        An instance of the Learning class containing the data to be sent.\n\n    Notes\n    -----\n    This function is only called if consent is given to share data.\n    Data is not shared to a third party. It is used with the sole purpose of\n    improving gpt-engineer, and letting it handle more use cases.\n    Consent logic is in gpt_engineer/learning.py.\n    \"\"\"\n    import rudderstack.analytics as rudder_analytics\n\n    rudder_analytics.write_key = \"2Re4kqwL61GDp7S8ewe6K5dbogG\"\n    rudder_analytics.dataPlaneUrl = \"https://gptengineerezm.dataplane.rudderstack.com\"\n\n    rudder_analytics.track(\n        user_id=learning.session,\n        event=\"learning\",\n        properties=learning.to_dict(),  # type: ignore\n    )\n\n\ndef collect_learnings(\n    prompt: Prompt,\n    model: str,\n    temperature: float,\n    config: any,\n    memory: DiskMemory,\n    review: Review,\n):\n    \"\"\"\n    Collect the learning data and send it to RudderStack for analysis.\n\n    Parameters\n    ----------\n    prompt : str\n        The initial prompt or question that was provided to the model.\n    model : str\n        The name of the model used for generating the response.\n    temperature : float\n        The temperature setting used in the model's response generation.\n    config : any\n        Configuration parameters used for the learning session.\n    memory : DiskMemory\n        An instance of DiskMemory for storing and retrieving data.\n    review : Review\n        An instance of Review containing human feedback on the model's response.\n\n    Notes\n    -----\n    This function attempts to send the learning data to RudderStack. If the data size exceeds\n    the maximum allowed size, it trims the data and retries sending it.\n    \"\"\"\n    learnings = extract_learning(prompt, model, temperature, config, memory, review)\n    try:\n        send_learning(learnings)\n    except RuntimeError:\n        # try to remove some parts of learning that might be too big\n        # rudderstack max event size is 32kb\n        max_size = 32 << 10  # 32KB in bytes\n        current_size = len(learnings.to_json().encode(\"utf-8\"))  # get size in bytes\n\n        overflow = current_size - max_size\n\n        # Add some extra characters for the \"[REMOVED...]\" string and for safety margin\n        remove_length = overflow + len(f\"[REMOVED {overflow} CHARACTERS]\") + 100\n\n        learnings.logs = (\n            learnings.logs[:-remove_length]\n            + f\"\\n\\n[REMOVED {remove_length} CHARACTERS]\"\n        )\n\n        print(\n            \"WARNING: learning too big, removing some parts. \"\n            \"Please report if this results in a crash.\"\n        )\n        try:\n            send_learning(learnings)\n        except RuntimeError:\n            print(\n                \"Sending learnings crashed despite truncation. Progressing without saving learnings.\"\n            )\n\n\n# def steps_file_hash():\n#     \"\"\"\n#     Compute the SHA-256 hash of the steps file.\n#\n#     Returns\n#     -------\n#     str\n#         The SHA-256 hash of the steps file.\n#     \"\"\"\n#     with open(steps.__file__, \"r\") as f:\n#         content = f.read()\n#         return hashlib.sha256(content.encode(\"utf-8\")).hexdigest()\n\n\ndef collect_and_send_human_review(\n    prompt: Prompt,\n    model: str,\n    temperature: float,\n    config: Tuple[str, ...],\n    memory: DiskMemory,\n):\n    \"\"\"\n    Collects human feedback on the code and sends it for analysis.\n\n    Parameters\n    ----------\n    prompt : str\n        The initial prompt or question that was provided to the model.\n    model : str\n        The name of the model used for generating the response.\n    temperature : float\n        The temperature setting used in the model's response generation.\n    config : Tuple[str, ...]\n        Configuration parameters used for the learning session.\n    memory : DiskMemory\n        An instance of DiskMemory for storing and retrieving data.\n\n    Returns\n    -------\n    None\n\n    Notes\n    -----\n    This function prompts the user for a review of the generated or improved code using the\n    `human_review_input` function. If a valid review is provided, it's serialized to JSON format\n    and stored within the database's memory under the \"review\" key.\n    \"\"\"\n\n    review = human_review_input()\n    if review:\n        collect_learnings(prompt, model, temperature, config, memory, review)\n", "gpt_engineer/applications/cli/file_selector.py": "\"\"\"\nfile_selector.py\n\nThis module offers interactive file selection for projects. Leveraging a terminal-based,\ntree-structured display, users can navigate and select files for editing or processing.\nIt integrates with system editors for direct file modification and supports saving\nselections for later use. Designed for efficient workflow enhancement in file-intensive\nenvironments, it offers customizable file filtering and seamless editor integration.\n\nKey Components:\n- FileSelector: Manages file selection and interaction.\n- DisplayablePath: Provides a structured view of file paths.\n\nUsage:\nTypically used in project setup or management phases for selecting specific files.\nIt operates within the GPT-Engineer environment, relying on core functionalities for\nfile handling and persistence.\n\"\"\"\n\nimport fnmatch\nimport os\nimport subprocess\n\nfrom pathlib import Path\nfrom typing import Any, Dict, Generator, List, Union\n\nimport toml\n\nfrom gpt_engineer.core.default.disk_memory import DiskMemory\nfrom gpt_engineer.core.default.paths import metadata_path\nfrom gpt_engineer.core.files_dict import FilesDict\nfrom gpt_engineer.core.git import filter_by_gitignore, is_git_repo\n\n\nclass FileSelector:\n    \"\"\"\n    Manages file selection and interaction within a project directory.\n\n    This class provides methods to interactively select files from the terminal,\n    save selections for later use, and integrate with system editors for direct\n    file modification.\n\n    Attributes\n    ----------\n    IGNORE_FOLDERS : set\n        A set of directory names to ignore during file selection.\n    FILE_LIST_NAME : str\n        The name of the file that stores the selected files list.\n    COMMENT : str\n        The comment string to be added to the top of the file selection list.\n    \"\"\"\n\n    IGNORE_FOLDERS = {\"site-packages\", \"node_modules\", \"venv\", \"__pycache__\"}\n    FILE_LIST_NAME = \"file_selection.toml\"\n    COMMENT = (\n        \"# Remove '#' to select a file or turn off linting.\\n\\n\"\n        \"# Linting with BLACK (Python) enhances code suggestions from LLMs. \"\n        \"To disable linting, uncomment the relevant option in the linting settings.\\n\\n\"\n        \"# gpt-engineer can only read selected files. \"\n        \"Including irrelevant files will degrade performance, \"\n        \"cost additional tokens and potentially overflow token limit.\\n\\n\"\n    )\n    LINTING_STRING = '[linting]\\n# \"linting\" = \"off\"\\n\\n'\n    is_linting = True\n\n    def __init__(self, project_path: Union[str, Path]):\n        \"\"\"\n        Initializes the FileSelector with a given project path.\n\n        Parameters\n        ----------\n        project_path : Union[str, Path]\n            The path to the project directory where file selection is to be performed.\n        \"\"\"\n        self.project_path = project_path\n        self.metadata_db = DiskMemory(metadata_path(self.project_path))\n        self.toml_path = self.metadata_db.path / self.FILE_LIST_NAME\n\n    def ask_for_files(self) -> tuple[FilesDict, bool]:\n        \"\"\"\n        Prompts the user to select files for context improvement.\n\n        This method supports selection from the terminal or using a previously saved list.\n        In test mode, it retrieves files from a predefined TOML configuration.\n\n        Returns\n        -------\n        FilesDict\n            A dictionary with file paths as keys and file contents as values.\n        \"\"\"\n\n        if os.getenv(\"GPTE_TEST_MODE\"):\n            # In test mode, retrieve files from a predefined TOML configuration\n            assert self.FILE_LIST_NAME in self.metadata_db\n            selected_files = self.get_files_from_toml(self.project_path, self.toml_path)\n        else:\n            # Otherwise, use the editor file selector for interactive selection\n            if self.FILE_LIST_NAME in self.metadata_db:\n                print(\n                    f\"File list detected at {self.toml_path}. Edit or delete it if you want to select new files.\"\n                )\n                selected_files = self.editor_file_selector(self.project_path, False)\n            else:\n                selected_files = self.editor_file_selector(self.project_path, True)\n\n        content_dict = {}\n        for file_path in selected_files:\n            # selected files contains paths that are relative to the project path\n            try:\n                # to open the file we need the path from the cwd\n                with open(\n                    Path(self.project_path) / file_path, \"r\", encoding=\"utf-8\"\n                ) as content:\n                    content_dict[str(file_path)] = content.read()\n            except FileNotFoundError:\n                print(f\"Warning: File not found {file_path}\")\n            except UnicodeDecodeError:\n                print(f\"Warning: File not UTF-8 encoded {file_path}, skipping\")\n\n        return FilesDict(content_dict), self.is_linting\n\n    def editor_file_selector(\n        self, input_path: Union[str, Path], init: bool = True\n    ) -> List[str]:\n        \"\"\"\n        Provides an interactive file selection interface using a .toml file.\n\n        Parameters\n        ----------\n        input_path : Union[str, Path]\n            The path where file selection is to be performed.\n        init : bool, optional\n            Indicates whether to initialize the .toml file with the file tree.\n\n        Returns\n        -------\n        List[str]\n            A list of strings representing the paths of selected files.\n        \"\"\"\n\n        root_path = Path(input_path)\n        tree_dict = {}\n        toml_file = DiskMemory(metadata_path(input_path)).path / \"file_selection.toml\"\n        # Define the toml file path\n\n        # Initialize .toml file with file tree if in initial state\n        if init:\n            tree_dict = {x: \"selected\" for x in self.get_current_files(root_path)}\n\n            s = toml.dumps({\"files\": tree_dict})\n\n            # add comments on all lines that match = \"selected\"\n            s = \"\\n\".join(\n                [\n                    \"# \" + line if line.endswith(' = \"selected\"') else line\n                    for line in s.split(\"\\n\")\n                ]\n            )\n            # Write to the toml file\n            with open(toml_file, \"w\") as f:\n                f.write(self.COMMENT)\n                f.write(self.LINTING_STRING)\n                f.write(s)\n\n        else:\n            # Load existing files from the .toml configuration\n            all_files = self.get_current_files(root_path)\n            s = toml.dumps({\"files\": {x: \"selected\" for x in all_files}})\n\n            # get linting status from the toml file\n            with open(toml_file, \"r\") as file:\n                linting_status = toml.load(file)\n            if (\n                \"linting\" in linting_status\n                and linting_status[\"linting\"].get(\"linting\", \"\").lower() == \"off\"\n            ):\n                self.is_linting = False\n                self.LINTING_STRING = '[linting]\\n\"linting\" = \"off\"\\n\\n'\n                print(\"\\nLinting is disabled\")\n\n            with open(toml_file, \"r\") as file:\n                selected_files = toml.load(file)\n\n            lines = s.split(\"\\n\")\n            s = \"\\n\".join(\n                lines[:1]\n                + [\n                    line\n                    if line.split(\" = \")[0].strip('\"') in selected_files[\"files\"]\n                    else \"# \" + line\n                    for line in lines[1:]\n                ]\n            )\n\n            # Write the merged list back to the .toml for user review and modification\n            with open(toml_file, \"w\") as file:\n                file.write(self.COMMENT)  # Ensure to write the comment\n                file.write(self.LINTING_STRING)\n                file.write(s)\n\n        print(\n            \"Please select and deselect (add # in front) files, save it, and close it to continue...\"\n        )\n        self.open_with_default_editor(\n            toml_file\n        )  # Open the .toml file in the default editor for user modification\n        return self.get_files_from_toml(\n            input_path, toml_file\n        )  # Return the list of selected files after user edits\n\n    def open_with_default_editor(self, file_path: Union[str, Path]):\n        \"\"\"\n        Opens a file with the system's default text editor.\n\n        Parameters\n        ----------\n        file_path : Union[str, Path]\n            The path to the file to be opened in the text editor.\n        \"\"\"\n\n        editors = [\n            \"gedit\",\n            \"notepad\",\n            \"nvim\",\n            \"write\",\n            \"nano\",\n            \"vim\",\n            \"emacs\",\n        ]  # Putting the beginner-friendly text editor forward\n        chosen_editor = os.environ.get(\"EDITOR\")\n\n        # Try the preferred editor first, then fallback to common editors\n        if chosen_editor:\n            try:\n                subprocess.run([chosen_editor, file_path])\n                return\n            except Exception:\n                pass\n\n        for editor in editors:\n            try:\n                subprocess.run([editor, file_path])\n                return\n            except Exception:\n                continue\n        print(\"No suitable text editor found. Please edit the file manually.\")\n\n    def is_utf8(self, file_path: Union[str, Path]) -> bool:\n        \"\"\"\n        Checks if the file at the given path is UTF-8 encoded.\n\n        Parameters\n        ----------\n        file_path : Union[str, Path]\n            The path to the file to be checked.\n\n        Returns\n        -------\n        bool\n            True if the file is UTF-8 encoded, False otherwise.\n        \"\"\"\n\n        try:\n            with open(file_path, \"rb\") as file:\n                file.read().decode(\"utf-8\")\n                return True\n        except UnicodeDecodeError:\n            return False\n\n    def get_files_from_toml(\n        self, input_path: Union[str, Path], toml_file: Union[str, Path]\n    ) -> List[str]:\n        \"\"\"\n        Retrieves a list of selected files from a .toml configuration file.\n\n        Parameters\n        ----------\n        input_path : Union[str, Path]\n            The path where file selection was performed.\n        toml_file : Union[str, Path]\n            The path to the .toml file containing the file selection.\n\n        Returns\n        -------\n        List[str]\n            A list of strings representing the paths of selected files.\n\n        Raises\n        ------\n        Exception\n            If no files are selected in the .toml file.\n        \"\"\"\n        selected_files = []\n        edited_tree = toml.load(toml_file)  # Load the edited .toml file\n\n        # check if users have disabled linting or not\n        if (\n            \"linting\" in edited_tree\n            and edited_tree[\"linting\"].get(\"linting\", \"\").lower() == \"off\"\n        ):\n            self.is_linting = False\n            print(\"\\nLinting is disabled\")\n        else:\n            self.is_linting = True\n\n        # Iterate through the files in the .toml and append selected files to the list\n        for file, _ in edited_tree[\"files\"].items():\n            selected_files.append(file)\n\n        # Ensure that at least one file is selected, or raise an exception\n        if not selected_files:\n            raise Exception(\n                \"No files were selected. Please select at least one file to proceed.\"\n            )\n\n        print(f\"\\nYou have selected the following files:\\n{input_path}\")\n\n        project_path = Path(input_path).resolve()\n        selected_paths = set(\n            project_path.joinpath(file).resolve(strict=False) for file in selected_files\n        )\n\n        for displayable_path in DisplayablePath.make_tree(project_path):\n            if displayable_path.path in selected_paths:\n                p = displayable_path\n                while p.parent and p.parent.path not in selected_paths:\n                    selected_paths.add(p.parent.path)\n                    p = p.parent\n\n        try:\n            for displayable_path in DisplayablePath.make_tree(project_path):\n                if displayable_path.path in selected_paths:\n                    print(displayable_path.displayable())\n\n        except FileNotFoundError:\n            print(\"Specified path does not exist: \", project_path)\n        except Exception as e:\n            print(\"An error occurred while trying to display the file tree:\", e)\n\n        print(\"\\n\")\n        return selected_files\n\n    def merge_file_lists(\n        self, existing_files: Dict[str, Any], new_files: Dict[str, Any]\n    ) -> Dict[str, Any]:\n        \"\"\"\n        Merges two lists of files, preserving the selection status.\n\n        Parameters\n        ----------\n        existing_files : Dict[str, Any]\n            The dictionary of existing files with their properties.\n        new_files : Dict[str, Any]\n            The dictionary of new files with their properties.\n\n        Returns\n        -------\n        Dict[str, Any]\n            The updated dictionary of files after merging.\n        \"\"\"\n        # Update the existing files with any new files or changes\n        for file, properties in new_files.items():\n            if file not in existing_files:\n                existing_files[file] = properties  # Add new files as unselected\n            # If you want to update other properties of existing files, you can do so here\n\n        return existing_files\n\n    def should_filter_file(self, file_path: Path, filters: List[str]) -> bool:\n        \"\"\"\n        Determines if a file should be ignored based on .gitignore rules.\n        \"\"\"\n        for f in filters:\n            if fnmatch.fnmatchcase(str(file_path), f):\n                return True\n        return False\n\n    def get_current_files(self, project_path: Union[str, Path]) -> List[str]:\n        \"\"\"\n        Generates a list of all files in the project directory. Will use .gitignore files if project_path is a git repository.\n\n        Parameters\n        ----------\n        project_path : Union[str, Path]\n            The path to the project directory.\n\n        Returns\n        -------\n        List[str]\n            A list of strings representing the relative paths of all files in the project directory.\n        \"\"\"\n        all_files = []\n        project_path = Path(\n            project_path\n        ).resolve()  # Ensure path is absolute and resolved\n\n        file_list = project_path.glob(\"**/*\")\n\n        for path in file_list:  # Recursively list all files\n            if path.is_file():\n                relpath = path.relative_to(project_path)\n                parts = relpath.parts\n                if any(part.startswith(\".\") for part in parts):\n                    continue  # Skip hidden files\n                if any(part in self.IGNORE_FOLDERS for part in parts):\n                    continue\n                if relpath.name == \"prompt\":\n                    continue  # Skip files named 'prompt'\n\n                all_files.append(str(relpath))\n\n        if is_git_repo(project_path) and \"projects\" not in project_path.parts:\n            all_files = filter_by_gitignore(project_path, all_files)\n\n        return all_files\n\n\nclass DisplayablePath(object):\n    \"\"\"\n    Represents and displays a file system path in a tree-like structure.\n\n    This class is used to visually represent the structure of directories and files\n    in a way that is similar to a file explorer's tree view.\n    \"\"\"\n\n    display_filename_prefix_middle = \"\u251c\u2500\u2500 \"\n    display_filename_prefix_last = \"\u2514\u2500\u2500 \"\n    display_parent_prefix_middle = \"    \"\n    display_parent_prefix_last = \"\u2502   \"\n\n    def __init__(\n        self, path: Union[str, Path], parent_path: \"DisplayablePath\", is_last: bool\n    ):\n        \"\"\"\n        Initializes a DisplayablePath object with a given path and parent.\n\n        Parameters\n        ----------\n        path : Union[str, Path]\n            The file system path to be displayed.\n        parent_path : DisplayablePath\n            The parent path in the tree structure.\n        is_last : bool\n            Indicates whether this is the last sibling in the tree structure.\n        \"\"\"\n        self.depth = 0\n        self.path = Path(str(path))\n        self.parent = parent_path\n        self.is_last = is_last\n        if self.parent:\n            self.depth = self.parent.depth + 1  # Increment depth if it has a parent\n\n    @property\n    def display_name(self) -> str:\n        \"\"\"\n        Get the display name of the file or directory.\n        \"\"\"\n        if self.path.is_dir():\n            return self.path.name + \"/\"\n        return self.path.name\n\n    @classmethod\n    def make_tree(\n        cls, root: Union[str, Path], parent=None, is_last=False, criteria=None\n    ) -> Generator[\"DisplayablePath\", None, None]:\n        \"\"\"\n        Creates a tree of DisplayablePath objects from a root directory.\n\n        Parameters\n        ----------\n        root : Union[str, Path]\n            The root directory from which to start creating the tree.\n        parent : DisplayablePath, optional\n            The parent path in the tree structure.\n        is_last : bool, optional\n            Indicates whether this is the last sibling in the tree structure.\n        criteria : callable, optional\n            A function to filter the paths included in the tree.\n\n        Yields\n        ------\n        DisplayablePath\n            The next DisplayablePath object in the tree.\n        \"\"\"\n        root = Path(str(root))  # Ensure root is a Path object\n        criteria = criteria or cls._default_criteria\n        displayable_root = cls(root, parent, is_last)\n        yield displayable_root\n\n        if root.is_dir():  # Check if root is a directory before iterating\n            children = sorted(\n                list(path for path in root.iterdir() if criteria(path)),\n                key=lambda s: str(s).lower(),\n            )\n            count = 1\n            for path in children:\n                is_last = count == len(children)\n                yield from cls.make_tree(\n                    path, parent=displayable_root, is_last=is_last, criteria=criteria\n                )\n                count += 1\n\n    @classmethod\n    def _default_criteria(cls, path: Path) -> bool:\n        \"\"\"\n        The default criteria function to filter the paths.\n        \"\"\"\n        return True\n\n    def displayable(self) -> str:\n        \"\"\"\n        Returns a string representation of the path for display in a tree-like structure.\n\n        Returns\n        -------\n        str\n            The displayable string representation of the file or directory.\n        \"\"\"\n        if self.parent is None:\n            return self.display_name\n\n        _filename_prefix = (\n            self.display_filename_prefix_last\n            if self.is_last\n            else self.display_filename_prefix_middle\n        )\n\n        parts = [\"{!s} {!s}\".format(_filename_prefix, self.display_name)]\n\n        parent = self.parent\n        while parent and parent.parent is not None:\n            parts.append(\n                self.display_parent_prefix_middle\n                if parent.is_last\n                else self.display_parent_prefix_last\n            )\n            parent = parent.parent\n\n        return \"\".join(reversed(parts))  # Assemble the parts into the final string\n", "gpt_engineer/applications/cli/cli_agent.py": "\"\"\"\nThis module provides the CliAgent class which manages the lifecycle of code generation and improvement\nusing an AI model. It includes functionalities to initialize code generation, improve existing code,\nand process the code through various steps defined in the step bundle.\n\"\"\"\n\nfrom typing import Callable, Optional, TypeVar\n\n# from gpt_engineer.core.default.git_version_manager import GitVersionManager\nfrom gpt_engineer.core.ai import AI\nfrom gpt_engineer.core.base_agent import BaseAgent\nfrom gpt_engineer.core.base_execution_env import BaseExecutionEnv\nfrom gpt_engineer.core.base_memory import BaseMemory\nfrom gpt_engineer.core.default.disk_execution_env import DiskExecutionEnv\nfrom gpt_engineer.core.default.disk_memory import DiskMemory\nfrom gpt_engineer.core.default.paths import PREPROMPTS_PATH\nfrom gpt_engineer.core.default.steps import (\n    execute_entrypoint,\n    gen_code,\n    gen_entrypoint,\n    improve_fn,\n)\nfrom gpt_engineer.core.files_dict import FilesDict\nfrom gpt_engineer.core.preprompts_holder import PrepromptsHolder\nfrom gpt_engineer.core.prompt import Prompt\n\nCodeGenType = TypeVar(\"CodeGenType\", bound=Callable[[AI, str, BaseMemory], FilesDict])\nCodeProcessor = TypeVar(\n    \"CodeProcessor\", bound=Callable[[AI, BaseExecutionEnv, FilesDict], FilesDict]\n)\nImproveType = TypeVar(\n    \"ImproveType\", bound=Callable[[AI, str, FilesDict, BaseMemory], FilesDict]\n)\n\n\nclass CliAgent(BaseAgent):\n    \"\"\"\n    The `CliAgent` class is responsible for managing the lifecycle of code generation and improvement\n    using an AI model. It orchestrates the generation of new code and the improvement of existing code\n    based on given prompts and utilizes a memory system and execution environment for processing.\n\n    Parameters\n    ----------\n    memory : BaseMemory\n        An instance of a class that adheres to the BaseMemory interface, used for storing and retrieving\n        information during the code generation process.\n    execution_env : BaseExecutionEnv\n        An instance of a class that adheres to the BaseExecutionEnv interface, used for executing code\n        and managing the execution environment.\n    ai : AI, optional\n        An instance of the AI class that manages calls to the language model. If not provided, a default\n        instance is created.\n    code_gen_fn : CodeGenType, optional\n        A callable that takes an AI instance, a prompt, and a memory instance to generate code. Defaults\n        to the `gen_code` function.\n    improve_fn : ImproveType, optional\n        A callable that takes an AI instance, a prompt, a FilesDict instance, and a memory instance to\n        improve code. Defaults to the `improve` function.\n    process_code_fn : CodeProcessor, optional\n        A callable that takes an AI instance, an execution environment, and a FilesDict instance to\n        process code. Defaults to the `execute_entrypoint` function.\n    preprompts_holder : PrepromptsHolder, optional\n        An instance of PrepromptsHolder that manages preprompt templates. If not provided, a default\n        instance is created using the PREPROMPTS_PATH.\n\n    Attributes\n    ----------\n    memory : BaseMemory\n        The memory instance where the agent stores and retrieves information.\n    execution_env : BaseExecutionEnv\n        The execution environment instance where the agent executes and manages code.\n    ai : AI\n        The AI instance used for interacting with the language model.\n    code_gen_fn : CodeGenType\n        The function used for generating code.\n    improve_fn : ImproveType\n        The function used for improving code.\n    process_code_fn : CodeProcessor\n        The function used for processing code.\n    preprompts_holder : PrepromptsHolder\n        The holder for preprompt templates.\n    \"\"\"\n\n    def __init__(\n        self,\n        memory: BaseMemory,\n        execution_env: BaseExecutionEnv,\n        ai: AI = None,\n        code_gen_fn: CodeGenType = gen_code,\n        improve_fn: ImproveType = improve_fn,\n        process_code_fn: CodeProcessor = execute_entrypoint,\n        preprompts_holder: PrepromptsHolder = None,\n    ):\n        self.memory = memory\n        self.execution_env = execution_env\n        self.ai = ai or AI()\n        self.code_gen_fn = code_gen_fn\n        self.process_code_fn = process_code_fn\n        self.improve_fn = improve_fn\n        self.preprompts_holder = preprompts_holder or PrepromptsHolder(PREPROMPTS_PATH)\n\n    @classmethod\n    def with_default_config(\n        cls,\n        memory: DiskMemory,\n        execution_env: DiskExecutionEnv,\n        ai: AI = None,\n        code_gen_fn: CodeGenType = gen_code,\n        improve_fn: ImproveType = improve_fn,\n        process_code_fn: CodeProcessor = execute_entrypoint,\n        preprompts_holder: PrepromptsHolder = None,\n    ):\n        \"\"\"\n        Creates a new instance of CliAgent with default configurations for memory, execution environment,\n        AI, and other functional parameters.\n\n        Parameters\n        ----------\n        memory : DiskMemory\n            An instance of DiskMemory for storing and retrieving information.\n        execution_env : DiskExecutionEnv\n            An instance of DiskExecutionEnv for executing code.\n        ai : AI, optional\n            An instance of AI for interacting with the language model. Defaults to None, which will create\n            a new AI instance.\n        code_gen_fn : CodeGenType, optional\n            A function for generating code. Defaults to `gen_code`.\n        improve_fn : ImproveType, optional\n            A function for improving code. Defaults to `improve`.\n        process_code_fn : CodeProcessor, optional\n            A function for processing code. Defaults to `execute_entrypoint`.\n        preprompts_holder : PrepromptsHolder, optional\n            An instance of PrepromptsHolder for managing preprompt templates. Defaults to None, which will\n            create a new PrepromptsHolder instance using PREPROMPTS_PATH.\n\n        Returns\n        -------\n        CliAgent\n            An instance of CliAgent configured with the provided or default parameters.\n        \"\"\"\n        return cls(\n            memory=memory,\n            execution_env=execution_env,\n            ai=ai,\n            code_gen_fn=code_gen_fn,\n            process_code_fn=process_code_fn,\n            improve_fn=improve_fn,\n            preprompts_holder=preprompts_holder or PrepromptsHolder(PREPROMPTS_PATH),\n        )\n\n    def init(self, prompt: Prompt) -> FilesDict:\n        \"\"\"\n        Generates a new piece of code using the AI and step bundle based on the provided prompt.\n\n        Parameters\n        ----------\n        prompt : str\n            A string prompt that guides the code generation process.\n\n        Returns\n        -------\n        FilesDict\n            An instance of the `FilesDict` class containing the generated code.\n        \"\"\"\n\n        files_dict = self.code_gen_fn(\n            self.ai, prompt, self.memory, self.preprompts_holder\n        )\n        entrypoint = gen_entrypoint(\n            self.ai, prompt, files_dict, self.memory, self.preprompts_holder\n        )\n        combined_dict = {**files_dict, **entrypoint}\n        files_dict = FilesDict(combined_dict)\n        files_dict = self.process_code_fn(\n            self.ai,\n            self.execution_env,\n            files_dict,\n            preprompts_holder=self.preprompts_holder,\n            prompt=prompt,\n            memory=self.memory,\n        )\n        return files_dict\n\n    def improve(\n        self,\n        files_dict: FilesDict,\n        prompt: Prompt,\n        execution_command: Optional[str] = None,\n    ) -> FilesDict:\n        \"\"\"\n        Improves an existing piece of code using the AI and step bundle based on the provided prompt.\n\n        Parameters\n        ----------\n        files_dict : FilesDict\n            An instance of `FilesDict` containing the code to be improved.\n        prompt : str\n            A string prompt that guides the code improvement process.\n        execution_command : str, optional\n            An optional command to execute the code. If not provided, the default execution command is used.\n\n        Returns\n        -------\n        FilesDict\n            An instance of the `FilesDict` class containing the improved code.\n        \"\"\"\n\n        files_dict = self.improve_fn(\n            self.ai, prompt, files_dict, self.memory, self.preprompts_holder\n        )\n        # entrypoint = gen_entrypoint(\n        #     self.ai, prompt, files_dict, self.memory, self.preprompts_holder\n        # )\n        # combined_dict = {**files_dict, **entrypoint}\n        # files_dict = FilesDict(combined_dict)\n        # files_dict = self.process_code_fn(\n        #     self.ai,\n        #     self.execution_env,\n        #     files_dict,\n        #     preprompts_holder=self.preprompts_holder,\n        #     prompt=prompt,\n        #     memory=self.memory,\n        # )\n\n        return files_dict\n", "gpt_engineer/applications/cli/main.py": "\"\"\"\nEntrypoint for the CLI tool.\n\nThis module serves as the entry point for a command-line interface (CLI) tool.\nIt is designed to interact with OpenAI's language models.\nThe module provides functionality to:\n- Load necessary environment variables,\n- Configure various parameters for the AI interaction,\n- Manage the generation or improvement of code projects.\n\nMain Functionality\n------------------\n- Load environment variables required for OpenAI API interaction.\n- Parse user-specified parameters for project configuration and AI behavior.\n- Facilitate interaction with AI models, databases, and archival processes.\n\nParameters\n----------\nNone\n\nNotes\n-----\n- The `OPENAI_API_KEY` must be set in the environment or provided in a `.env` file within the working directory.\n- The default project path is `projects/example`.\n- When using the `azure_endpoint` parameter, provide the Azure OpenAI service endpoint URL.\n\"\"\"\n\nimport difflib\nimport logging\nimport os\nimport sys\n\nfrom pathlib import Path\n\nimport openai\nimport typer\n\nfrom dotenv import load_dotenv\nfrom langchain.globals import set_llm_cache\nfrom langchain_community.cache import SQLiteCache\nfrom termcolor import colored\n\nfrom gpt_engineer.applications.cli.cli_agent import CliAgent\nfrom gpt_engineer.applications.cli.collect import collect_and_send_human_review\nfrom gpt_engineer.applications.cli.file_selector import FileSelector\nfrom gpt_engineer.core.ai import AI, ClipboardAI\nfrom gpt_engineer.core.default.disk_execution_env import DiskExecutionEnv\nfrom gpt_engineer.core.default.disk_memory import DiskMemory\nfrom gpt_engineer.core.default.file_store import FileStore\nfrom gpt_engineer.core.default.paths import PREPROMPTS_PATH, memory_path\nfrom gpt_engineer.core.default.steps import (\n    execute_entrypoint,\n    gen_code,\n    handle_improve_mode,\n    improve_fn as improve_fn,\n)\nfrom gpt_engineer.core.files_dict import FilesDict\nfrom gpt_engineer.core.git import stage_uncommitted_to_git\nfrom gpt_engineer.core.preprompts_holder import PrepromptsHolder\nfrom gpt_engineer.core.prompt import Prompt\nfrom gpt_engineer.tools.custom_steps import clarified_gen, lite_gen, self_heal\n\napp = typer.Typer(\n    context_settings={\"help_option_names\": [\"-h\", \"--help\"]}\n)  # creates a CLI app\n\n\ndef load_env_if_needed():\n    \"\"\"\n    Load environment variables if the OPENAI_API_KEY is not already set.\n\n    This function checks if the OPENAI_API_KEY environment variable is set,\n    and if not, it attempts to load it from a .env file in the current working\n    directory. It then sets the openai.api_key for use in the application.\n    \"\"\"\n    # We have all these checks for legacy reasons...\n    if os.getenv(\"OPENAI_API_KEY\") is None:\n        load_dotenv()\n    if os.getenv(\"OPENAI_API_KEY\") is None:\n        load_dotenv(dotenv_path=os.path.join(os.getcwd(), \".env\"))\n\n    openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n\n    if os.getenv(\"ANTHROPIC_API_KEY\") is None:\n        load_dotenv()\n    if os.getenv(\"ANTHROPIC_API_KEY\") is None:\n        load_dotenv(dotenv_path=os.path.join(os.getcwd(), \".env\"))\n\n\ndef concatenate_paths(base_path, sub_path):\n    # Compute the relative path from base_path to sub_path\n    relative_path = os.path.relpath(sub_path, base_path)\n\n    # If the relative path is not in the parent directory, use the original sub_path\n    if not relative_path.startswith(\"..\"):\n        return sub_path\n\n    # Otherwise, concatenate base_path and sub_path\n    return os.path.normpath(os.path.join(base_path, sub_path))\n\n\ndef load_prompt(\n    input_repo: DiskMemory,\n    improve_mode: bool,\n    prompt_file: str,\n    image_directory: str,\n    entrypoint_prompt_file: str = \"\",\n) -> Prompt:\n    \"\"\"\n    Load or request a prompt from the user based on the mode.\n\n    Parameters\n    ----------\n    input_repo : DiskMemory\n        The disk memory object where prompts and other data are stored.\n    improve_mode : bool\n        Flag indicating whether the application is in improve mode.\n\n    Returns\n    -------\n    str\n        The loaded or inputted prompt.\n    \"\"\"\n\n    if os.path.isdir(prompt_file):\n        raise ValueError(\n            f\"The path to the prompt, {prompt_file}, already exists as a directory. No prompt can be read from it. Please specify a prompt file using --prompt_file\"\n        )\n    prompt_str = input_repo.get(prompt_file)\n    if prompt_str:\n        print(colored(\"Using prompt from file:\", \"green\"), prompt_file)\n        print(prompt_str)\n    else:\n        if not improve_mode:\n            prompt_str = input(\n                \"\\nWhat application do you want gpt-engineer to generate?\\n\"\n            )\n        else:\n            prompt_str = input(\"\\nHow do you want to improve the application?\\n\")\n\n    if entrypoint_prompt_file == \"\":\n        entrypoint_prompt = \"\"\n    else:\n        full_entrypoint_prompt_file = concatenate_paths(\n            input_repo.path, entrypoint_prompt_file\n        )\n        if os.path.isfile(full_entrypoint_prompt_file):\n            entrypoint_prompt = input_repo.get(full_entrypoint_prompt_file)\n\n        else:\n            raise ValueError(\"The provided file at --entrypoint-prompt does not exist\")\n\n    if image_directory == \"\":\n        return Prompt(prompt_str, entrypoint_prompt=entrypoint_prompt)\n\n    full_image_directory = concatenate_paths(input_repo.path, image_directory)\n    if os.path.isdir(full_image_directory):\n        if len(os.listdir(full_image_directory)) == 0:\n            raise ValueError(\"The provided --image_directory is empty.\")\n        image_repo = DiskMemory(full_image_directory)\n        return Prompt(\n            prompt_str,\n            image_repo.get(\".\").to_dict(),\n            entrypoint_prompt=entrypoint_prompt,\n        )\n    else:\n        raise ValueError(\"The provided --image_directory is not a directory.\")\n\n\ndef get_preprompts_path(use_custom_preprompts: bool, input_path: Path) -> Path:\n    \"\"\"\n    Get the path to the preprompts, using custom ones if specified.\n\n    Parameters\n    ----------\n    use_custom_preprompts : bool\n        Flag indicating whether to use custom preprompts.\n    input_path : Path\n        The path to the project directory.\n\n    Returns\n    -------\n    Path\n        The path to the directory containing the preprompts.\n    \"\"\"\n    original_preprompts_path = PREPROMPTS_PATH\n    if not use_custom_preprompts:\n        return original_preprompts_path\n\n    custom_preprompts_path = input_path / \"preprompts\"\n    if not custom_preprompts_path.exists():\n        custom_preprompts_path.mkdir()\n\n    for file in original_preprompts_path.glob(\"*\"):\n        if not (custom_preprompts_path / file.name).exists():\n            (custom_preprompts_path / file.name).write_text(file.read_text())\n    return custom_preprompts_path\n\n\ndef compare(f1: FilesDict, f2: FilesDict):\n    def colored_diff(s1, s2):\n        lines1 = s1.splitlines()\n        lines2 = s2.splitlines()\n\n        diff = difflib.unified_diff(lines1, lines2, lineterm=\"\")\n\n        RED = \"\\033[38;5;202m\"\n        GREEN = \"\\033[92m\"\n        RESET = \"\\033[0m\"\n\n        colored_lines = []\n        for line in diff:\n            if line.startswith(\"+\"):\n                colored_lines.append(GREEN + line + RESET)\n            elif line.startswith(\"-\"):\n                colored_lines.append(RED + line + RESET)\n            else:\n                colored_lines.append(line)\n\n        return \"\\n\".join(colored_lines)\n\n    for file in sorted(set(f1) | set(f2)):\n        diff = colored_diff(f1.get(file, \"\"), f2.get(file, \"\"))\n        if diff:\n            print(f\"Changes to {file}:\")\n            print(diff)\n\n\ndef prompt_yesno() -> bool:\n    TERM_CHOICES = colored(\"y\", \"green\") + \"/\" + colored(\"n\", \"red\") + \" \"\n    while True:\n        response = input(TERM_CHOICES).strip().lower()\n        if response in [\"y\", \"yes\"]:\n            return True\n        if response in [\"n\", \"no\"]:\n            break\n        print(\"Please respond with 'y' or 'n'\")\n\n\n@app.command(\n    help=\"\"\"\n        GPT-engineer lets you:\n\n        \\b\n        - Specify a software in natural language\n        - Sit back and watch as an AI writes and executes the code\n        - Ask the AI to implement improvements\n    \"\"\"\n)\ndef main(\n    project_path: str = typer.Argument(\".\", help=\"path\"),\n    model: str = typer.Option(\n        os.environ.get(\"MODEL_NAME\", \"gpt-4o\"), \"--model\", \"-m\", help=\"model id string\"\n    ),\n    temperature: float = typer.Option(\n        0.1,\n        \"--temperature\",\n        \"-t\",\n        help=\"Controls randomness: lower values for more focused, deterministic outputs\",\n    ),\n    improve_mode: bool = typer.Option(\n        False,\n        \"--improve\",\n        \"-i\",\n        help=\"Improve an existing project by modifying the files.\",\n    ),\n    lite_mode: bool = typer.Option(\n        False,\n        \"--lite\",\n        \"-l\",\n        help=\"Lite mode: run a generation using only the main prompt.\",\n    ),\n    clarify_mode: bool = typer.Option(\n        False,\n        \"--clarify\",\n        \"-c\",\n        help=\"Clarify mode - discuss specification with AI before implementation.\",\n    ),\n    self_heal_mode: bool = typer.Option(\n        False,\n        \"--self-heal\",\n        \"-sh\",\n        help=\"Self-heal mode - fix the code by itself when it fails.\",\n    ),\n    azure_endpoint: str = typer.Option(\n        \"\",\n        \"--azure\",\n        \"-a\",\n        help=\"\"\"Endpoint for your Azure OpenAI Service (https://xx.openai.azure.com).\n            In that case, the given model is the deployment name chosen in the Azure AI Studio.\"\"\",\n    ),\n    use_custom_preprompts: bool = typer.Option(\n        False,\n        \"--use-custom-preprompts\",\n        help=\"\"\"Use your project's custom preprompts instead of the default ones.\n          Copies all original preprompts to the project's workspace if they don't exist there.\"\"\",\n    ),\n    llm_via_clipboard: bool = typer.Option(\n        False,\n        \"--llm-via-clipboard\",\n        help=\"Use the clipboard to communicate with the AI.\",\n    ),\n    verbose: bool = typer.Option(\n        False, \"--verbose\", \"-v\", help=\"Enable verbose logging for debugging.\"\n    ),\n    debug: bool = typer.Option(\n        False, \"--debug\", \"-d\", help=\"Enable debug mode for debugging.\"\n    ),\n    prompt_file: str = typer.Option(\n        \"prompt\",\n        \"--prompt_file\",\n        help=\"Relative path to a text file containing a prompt.\",\n    ),\n    entrypoint_prompt_file: str = typer.Option(\n        \"\",\n        \"--entrypoint_prompt\",\n        help=\"Relative path to a text file containing a file that specifies requirements for you entrypoint.\",\n    ),\n    image_directory: str = typer.Option(\n        \"\",\n        \"--image_directory\",\n        help=\"Relative path to a folder containing images.\",\n    ),\n    use_cache: bool = typer.Option(\n        False,\n        \"--use_cache\",\n        help=\"Speeds up computations and saves tokens when running the same prompt multiple times by caching the LLM response.\",\n    ),\n    no_execution: bool = typer.Option(\n        False,\n        \"--no_execution\",\n        help=\"Run setup but to not call LLM or write any code. For testing purposes.\",\n    ),\n):\n    \"\"\"\n    The main entry point for the CLI tool that generates or improves a project.\n\n    This function sets up the CLI tool, loads environment variables, initializes\n    the AI, and processes the user's request to generate or improve a project\n    based on the provided arguments.\n\n    Parameters\n    ----------\n    project_path : str\n        The file path to the project directory.\n    model : str\n        The model ID string for the AI.\n    temperature : float\n        The temperature setting for the AI's responses.\n    improve_mode : bool\n        Flag indicating whether to improve an existing project.\n    lite_mode : bool\n        Flag indicating whether to run in lite mode.\n    clarify_mode : bool\n        Flag indicating whether to discuss specifications with AI before implementation.\n    self_heal_mode : bool\n        Flag indicating whether to enable self-healing mode.\n    azure_endpoint : str\n        The endpoint for Azure OpenAI services.\n    use_custom_preprompts : bool\n        Flag indicating whether to use custom preprompts.\n    prompt_file : str\n        Relative path to a text file containing a prompt.\n    entrypoint_prompt_file: str\n        Relative path to a text file containing a file that specifies requirements for you entrypoint.\n    image_directory: str\n        Relative path to a folder containing images.\n    use_cache: bool\n        Speeds up computations and saves tokens when running the same prompt multiple times by caching the LLM response.\n    verbose : bool\n        Flag indicating whether to enable verbose logging.\n    no_execution: bool\n        Run setup but to not call LLM or write any code. For testing purposes.\n\n    Returns\n    -------\n    None\n    \"\"\"\n\n    if debug:\n        import pdb\n\n        sys.excepthook = lambda *_: pdb.pm()\n\n    # Validate arguments\n    if improve_mode and (clarify_mode or lite_mode):\n        typer.echo(\"Error: Clarify and lite mode are not compatible with improve mode.\")\n        raise typer.Exit(code=1)\n\n    # Set up logging\n    logging.basicConfig(level=logging.DEBUG if verbose else logging.INFO)\n    if use_cache:\n        set_llm_cache(SQLiteCache(database_path=\".langchain.db\"))\n    if improve_mode:\n        assert not (\n            clarify_mode or lite_mode\n        ), \"Clarify and lite mode are not active for improve mode\"\n\n    load_env_if_needed()\n\n    if llm_via_clipboard:\n        ai = ClipboardAI()\n    else:\n        ai = AI(\n            model_name=model,\n            temperature=temperature,\n            azure_endpoint=azure_endpoint,\n        )\n\n    path = Path(project_path)\n    print(\"Running gpt-engineer in\", path.absolute(), \"\\n\")\n\n    prompt = load_prompt(\n        DiskMemory(path),\n        improve_mode,\n        prompt_file,\n        image_directory,\n        entrypoint_prompt_file,\n    )\n\n    # todo: if ai.vision is false and not llm_via_clipboard - ask if they would like to use gpt-4-vision-preview instead? If so recreate AI\n    if not ai.vision:\n        prompt.image_urls = None\n\n    # configure generation function\n    if clarify_mode:\n        code_gen_fn = clarified_gen\n    elif lite_mode:\n        code_gen_fn = lite_gen\n    else:\n        code_gen_fn = gen_code\n\n    # configure execution function\n    if self_heal_mode:\n        execution_fn = self_heal\n    else:\n        execution_fn = execute_entrypoint\n\n    preprompts_holder = PrepromptsHolder(\n        get_preprompts_path(use_custom_preprompts, Path(project_path))\n    )\n\n    memory = DiskMemory(memory_path(project_path))\n    memory.archive_logs()\n\n    execution_env = DiskExecutionEnv()\n    agent = CliAgent.with_default_config(\n        memory,\n        execution_env,\n        ai=ai,\n        code_gen_fn=code_gen_fn,\n        improve_fn=improve_fn,\n        process_code_fn=execution_fn,\n        preprompts_holder=preprompts_holder,\n    )\n\n    files = FileStore(project_path)\n    if not no_execution:\n        if improve_mode:\n            files_dict_before, is_linting = FileSelector(project_path).ask_for_files()\n\n            # lint the code\n            if is_linting:\n                files_dict_before = files.linting(files_dict_before)\n\n            files_dict = handle_improve_mode(prompt, agent, memory, files_dict_before)\n            if not files_dict or files_dict_before == files_dict:\n                print(\n                    f\"No changes applied. Could you please upload the debug_log_file.txt in {memory.path}/logs folder in a github issue?\"\n                )\n\n            else:\n                print(\"\\nChanges to be made:\")\n                compare(files_dict_before, files_dict)\n\n                print()\n                print(colored(\"Do you want to apply these changes?\", \"light_green\"))\n                if not prompt_yesno():\n                    files_dict = files_dict_before\n\n        else:\n            files_dict = agent.init(prompt)\n            # collect user feedback if user consents\n            config = (code_gen_fn.__name__, execution_fn.__name__)\n            collect_and_send_human_review(prompt, model, temperature, config, memory)\n\n        stage_uncommitted_to_git(path, files_dict, improve_mode)\n\n        files.push(files_dict)\n\n    if ai.token_usage_log.is_openai_model():\n        print(\"Total api cost: $ \", ai.token_usage_log.usage_cost())\n    elif os.getenv(\"LOCAL_MODEL\"):\n        print(\"Total api cost: $ 0.0 since we are using local LLM.\")\n    else:\n        print(\"Total tokens used: \", ai.token_usage_log.total_tokens())\n\n\nif __name__ == \"__main__\":\n    app()\n", "gpt_engineer/applications/cli/learning.py": "\"\"\"\nThe `learning` module is designed to facilitate the collection and storage of user feedback on the outputs generated by the GPT Engineer tool. It provides mechanisms for obtaining user consent, capturing user reviews, and storing this information for future analysis and enhancement of the tool's performance.\n\nClasses\n-------\nReview : dataclass\n    Represents a user's review of the generated code, including whether it ran, was perfect, was useful, and any additional comments.\nLearning : dataclass\n    Encapsulates the metadata and feedback collected during a session of using the GPT Engineer tool, including the prompt, model, temperature, configuration, logs, session identifier, user review, and timestamp.\n\nFunctions\n---------\nhuman_review_input() -> Optional[Review]\n    Interactively gathers feedback from the user regarding the performance of generated code and returns a Review instance.\ncheck_collection_consent() -> bool\n    Checks if the user has previously given consent to store their data and, if not, asks for it.\nask_collection_consent() -> bool\n    Prompts the user for consent to store their data for the purpose of improving GPT Engineer.\nextract_learning(prompt: Prompt, model: str, temperature: float, config: Tuple[str, ...], memory: DiskMemory, review: Review) -> Learning\n    Extracts feedback and session details to create a Learning instance based on the provided parameters.\nget_session() -> str\n    Retrieves a unique identifier for the current user session, creating one if it does not exist.\n\nConstants\n---------\nTERM_CHOICES : tuple\n    Terminal color choices for user interactive prompts, formatted with termcolor for readability.\n\"\"\"\n\nimport json\nimport random\nimport tempfile\n\nfrom dataclasses import dataclass, field\nfrom datetime import datetime\nfrom pathlib import Path\nfrom typing import Optional, Tuple\n\nfrom dataclasses_json import dataclass_json\nfrom termcolor import colored\n\nfrom gpt_engineer.core.default.disk_memory import DiskMemory\nfrom gpt_engineer.core.prompt import Prompt\n\n\n@dataclass_json\n@dataclass\nclass Review:\n    \"\"\"\n    A dataclass that represents a user's review of the generated code.\n\n    Attributes\n    ----------\n    ran : Optional[bool]\n        Indicates whether the generated code ran without errors.\n    perfect : Optional[bool]\n        Indicates whether the generated code met all the user's requirements.\n    works : Optional[bool]\n        Indicates whether the generated code was useful, even if not perfect.\n    comments : str\n        Any additional comments provided by the user.\n    raw : str\n        A raw string representation of the user's responses.\n    \"\"\"\n\n    ran: Optional[bool]\n    perfect: Optional[bool]\n    works: Optional[bool]\n    comments: str\n    raw: str\n\n\n@dataclass_json\n@dataclass\nclass Learning:\n    \"\"\"\n    A dataclass that encapsulates the learning data collected during a GPT Engineer session.\n\n    Attributes\n    ----------\n    prompt : str\n        A JSON string representing the prompt provided to GPT Engineer.\n    model : str\n        The name of the model used during the session.\n    temperature : float\n        The temperature setting used for the model's responses.\n    config : str\n        A JSON string representing the configuration settings for the session.\n    logs : str\n        A JSON string representing the logs of the session.\n    session : str\n        A unique identifier for the user session.\n    review : Optional[Review]\n        The user's review of the generated code.\n    timestamp : str\n        The UTC timestamp when the learning data was created.\n    version : str\n        The version of the learning data schema.\n    \"\"\"\n\n    prompt: str\n    model: str\n    temperature: float\n    config: str\n    logs: str\n    session: str\n    review: Optional[Review]\n    timestamp: str = field(default_factory=lambda: datetime.utcnow().isoformat())\n    version: str = \"0.3\"\n\n\nTERM_CHOICES = (\n    colored(\"y\", \"green\")\n    + \"/\"\n    + colored(\"n\", \"red\")\n    + \"/\"\n    + colored(\"u\", \"yellow\")\n    + \"(ncertain): \"\n)\n\n\ndef human_review_input() -> Optional[Review]:\n    \"\"\"\n    Interactively prompts the user to review the generated code and returns their feedback encapsulated in a Review object.\n\n    This function will first check if the user has given consent to collect their feedback. If consent is given, it will ask the user a series of questions about the generated code's performance and capture their responses.\n\n    Returns\n    -------\n    Optional[Review]\n        A Review object containing the user's feedback, or None if consent is not given.\n    \"\"\"\n    print()\n    if not check_collection_consent():\n        return None\n    print()\n    print(\n        colored(\"To help gpt-engineer learn, please answer 3 questions:\", \"light_green\")\n    )\n    print()\n\n    ran = input(\"Did the generated code run at all? \" + TERM_CHOICES)\n    ran = ask_for_valid_input(ran)\n\n    if ran == \"y\":\n        perfect = input(\n            \"Did the generated code do everything you wanted? \" + TERM_CHOICES\n        )\n        perfect = ask_for_valid_input(perfect)\n\n        if perfect != \"y\":\n            useful = input(\"Did the generated code do anything useful? \" + TERM_CHOICES)\n            useful = ask_for_valid_input(useful)\n        else:\n            useful = \"\"\n    else:\n        perfect = \"\"\n        useful = \"\"\n\n    if perfect != \"y\":\n        comments = input(\n            \"If you have time, please explain what was not working \"\n            + colored(\"(ok to leave blank)\\n\", \"light_green\")\n        )\n    else:\n        comments = \"\"\n\n    return Review(\n        raw=\", \".join([ran, perfect, useful]),\n        ran={\"y\": True, \"n\": False, \"u\": None, \"\": None}[ran],\n        works={\"y\": True, \"n\": False, \"u\": None, \"\": None}[useful],\n        perfect={\"y\": True, \"n\": False, \"u\": None, \"\": None}[perfect],\n        comments=comments,\n    )\n\n\ndef ask_for_valid_input(ran):\n    while ran not in (\"y\", \"n\", \"u\"):\n        ran = input(\"Invalid input. Please enter y, n, or u: \")\n    return ran\n\n\ndef check_collection_consent() -> bool:\n    \"\"\"\n    Checks if the user has previously given consent to store their data for feedback collection.\n\n    This function looks for a file that stores the user's consent status. If the file exists and contains 'true', consent is assumed. If the file does not exist or does not contain 'true', the function will prompt the user for consent.\n\n    Returns\n    -------\n    bool\n        True if the user has given consent, False otherwise.\n    \"\"\"\n    path = Path(\".gpte_consent\")\n    if path.exists() and path.read_text() == \"true\":\n        return True\n    else:\n        return ask_collection_consent()\n\n\ndef ask_collection_consent() -> bool:\n    \"\"\"\n    Asks the user for their consent to store their data for the purpose of improving the GPT Engineer tool.\n\n    The user's response is recorded in a file for future reference. If the user consents, the function will write 'true' to the file. If the user does not consent, no data will be collected, and the function will not modify the file.\n\n    Returns\n    -------\n    bool\n        True if the user consents, False otherwise.\n    \"\"\"\n    answer = input(\n        \"Is it ok if we store your prompts to help improve GPT Engineer? (y/n)\"\n    )\n    while answer.lower() not in (\"y\", \"n\"):\n        answer = input(\"Invalid input. Please enter y or n: \")\n\n    if answer.lower() == \"y\":\n        path = Path(\".gpte_consent\")\n        path.write_text(\"true\")\n        print(colored(\"Thank you\ufe0f\", \"light_green\"))\n        print()\n        print(\n            \"(If you no longer wish to participate in data collection, delete the file .gpte_consent)\"\n        )\n        return True\n    else:\n        print(\n            colored(\n                \"No worries! GPT Engineer will not collect your prompts. \u2764\ufe0f\",\n                \"light_green\",\n            )\n        )\n        return False\n\n\ndef extract_learning(\n    prompt: Prompt,\n    model: str,\n    temperature: float,\n    config: Tuple[str, ...],\n    memory: DiskMemory,\n    review: Review,\n) -> Learning:\n    \"\"\"\n    Constructs a Learning object containing the session's metadata and user feedback.\n\n    Parameters\n    ----------\n    prompt : str\n        The initial prompt provided to the GPT Engineer.\n    model : str\n        The name of the model used during the session.\n    temperature : float\n        The temperature setting used for the model's responses.\n    config : Tuple[str, ...]\n        A tuple representing the configuration settings for the session.\n    memory : DiskMemory\n        An object representing the disk memory used during the session.\n    review : Review\n        The user's review of the generated code.\n\n    Returns\n    -------\n    Learning\n        An instance of Learning containing all the session details and user feedback.\n    \"\"\"\n    return Learning(\n        prompt=prompt.to_json(),\n        model=model,\n        temperature=temperature,\n        config=json.dumps(config),\n        session=get_session(),\n        logs=memory.to_json(),\n        review=review,\n    )\n\n\ndef get_session() -> str:\n    \"\"\"\n    Retrieves or generates a unique identifier for the current user session.\n\n    This function attempts to read a unique user ID from a temporary file. If the file does not exist, it generates a new random ID, writes it to the file, and returns it. This ID is used to uniquely identify the user's session.\n\n    Returns\n    -------\n    str\n        A unique identifier for the user session.\n    \"\"\"\n    path = Path(tempfile.gettempdir()) / \"gpt_engineer_user_id.txt\"\n\n    try:\n        if path.exists():\n            user_id = path.read_text()\n        else:\n            # random uuid:\n            user_id = str(random.randint(0, 2**32))\n            path.write_text(user_id)\n        return user_id\n    except IOError:\n        return \"ephemeral_\" + str(random.randint(0, 2**32))\n", "gpt_engineer/applications/cli/__init__.py": "", "gpt_engineer/core/ai.py": "\"\"\"\nAI Module\n\nThis module provides an AI class that interfaces with language models to perform various tasks such as\nstarting a conversation, advancing the conversation, and handling message serialization. It also includes\nbackoff strategies for handling rate limit errors from the OpenAI API.\n\nClasses:\n    AI: A class that interfaces with language models for conversation management and message serialization.\n\nFunctions:\n    serialize_messages(messages: List[Message]) -> str\n        Serialize a list of messages to a JSON string.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport json\nimport logging\nimport os\n\nfrom pathlib import Path\nfrom typing import Any, List, Optional, Union\n\nimport backoff\nimport openai\nimport pyperclip\n\nfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\nfrom langchain.chat_models.base import BaseChatModel\nfrom langchain.schema import (\n    AIMessage,\n    HumanMessage,\n    SystemMessage,\n    messages_from_dict,\n    messages_to_dict,\n)\nfrom langchain_anthropic import ChatAnthropic\nfrom langchain_openai import AzureChatOpenAI, ChatOpenAI\n\nfrom gpt_engineer.core.token_usage import TokenUsageLog\n\n# Type hint for a chat message\nMessage = Union[AIMessage, HumanMessage, SystemMessage]\n\n# Set up logging\nlogger = logging.getLogger(__name__)\n\n\nclass AI:\n    \"\"\"\n    A class that interfaces with language models for conversation management and message serialization.\n\n    This class provides methods to start and advance conversations, handle message serialization,\n    and implement backoff strategies for rate limit errors when interacting with the OpenAI API.\n\n    Attributes\n    ----------\n    temperature : float\n        The temperature setting for the language model.\n    azure_endpoint : str\n        The endpoint URL for the Azure-hosted language model.\n    model_name : str\n        The name of the language model to use.\n    streaming : bool\n        A flag indicating whether to use streaming for the language model.\n    llm : BaseChatModel\n        The language model instance for conversation management.\n    token_usage_log : TokenUsageLog\n        A log for tracking token usage during conversations.\n\n    Methods\n    -------\n    start(system: str, user: str, step_name: str) -> List[Message]\n        Start the conversation with a system message and a user message.\n    next(messages: List[Message], prompt: Optional[str], step_name: str) -> List[Message]\n        Advances the conversation by sending message history to LLM and updating with the response.\n    backoff_inference(messages: List[Message]) -> Any\n        Perform inference using the language model with an exponential backoff strategy.\n    serialize_messages(messages: List[Message]) -> str\n        Serialize a list of messages to a JSON string.\n    deserialize_messages(jsondictstr: str) -> List[Message]\n        Deserialize a JSON string to a list of messages.\n    _create_chat_model() -> BaseChatModel\n        Create a chat model with the specified model name and temperature.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_name=\"gpt-4-turbo\",\n        temperature=0.1,\n        azure_endpoint=None,\n        streaming=True,\n        vision=False,\n    ):\n        \"\"\"\n        Initialize the AI class.\n\n        Parameters\n        ----------\n        model_name : str, optional\n            The name of the model to use, by default \"gpt-4\".\n        temperature : float, optional\n            The temperature to use for the model, by default 0.1.\n        \"\"\"\n        self.temperature = temperature\n        self.azure_endpoint = azure_endpoint\n        self.model_name = model_name\n        self.streaming = streaming\n        self.vision = (\n            (\"vision-preview\" in model_name)\n            or (\"gpt-4-turbo\" in model_name and \"preview\" not in model_name)\n            or (\"claude\" in model_name)\n        )\n        self.llm = self._create_chat_model()\n        self.token_usage_log = TokenUsageLog(model_name)\n\n        logger.debug(f\"Using model {self.model_name}\")\n\n    def start(self, system: str, user: Any, *, step_name: str) -> List[Message]:\n        \"\"\"\n        Start the conversation with a system message and a user message.\n\n        Parameters\n        ----------\n        system : str\n            The content of the system message.\n        user : str\n            The content of the user message.\n        step_name : str\n            The name of the step.\n\n        Returns\n        -------\n        List[Message]\n            The list of messages in the conversation.\n        \"\"\"\n\n        messages: List[Message] = [\n            SystemMessage(content=system),\n            HumanMessage(content=user),\n        ]\n        return self.next(messages, step_name=step_name)\n\n    def _extract_content(self, content):\n        \"\"\"\n        Extracts text content from a message, supporting both string and list types.\n        Parameters\n        ----------\n        content : Union[str, List[dict]]\n            The content of a message, which could be a string or a list.\n        Returns\n        -------\n        str\n            The extracted text content.\n        \"\"\"\n        if isinstance(content, str):\n            return content\n        elif isinstance(content, list) and content and \"text\" in content[0]:\n            # Assuming the structure of list content is [{'type': 'text', 'text': 'Some text'}, ...]\n            return content[0][\"text\"]\n        else:\n            return \"\"\n\n    def _collapse_text_messages(self, messages: List[Message]):\n        \"\"\"\n        Combine consecutive messages of the same type into a single message, where if the message content\n        is a list type, the first text element's content is taken. This method keeps `combined_content` as a string.\n\n        This method iterates through the list of messages, combining consecutive messages of the same type\n        by joining their content with a newline character. If the content is a list, it extracts text from the first\n        text element's content. This reduces the number of messages and simplifies the conversation for processing.\n\n        Parameters\n        ----------\n        messages : List[Message]\n            The list of messages to collapse.\n\n        Returns\n        -------\n        List[Message]\n            The list of messages after collapsing consecutive messages of the same type.\n        \"\"\"\n        collapsed_messages = []\n        if not messages:\n            return collapsed_messages\n\n        previous_message = messages[0]\n        combined_content = self._extract_content(previous_message.content)\n\n        for current_message in messages[1:]:\n            if current_message.type == previous_message.type:\n                combined_content += \"\\n\\n\" + self._extract_content(\n                    current_message.content\n                )\n            else:\n                collapsed_messages.append(\n                    previous_message.__class__(content=combined_content)\n                )\n                previous_message = current_message\n                combined_content = self._extract_content(current_message.content)\n\n        collapsed_messages.append(previous_message.__class__(content=combined_content))\n        return collapsed_messages\n\n    def next(\n        self,\n        messages: List[Message],\n        prompt: Optional[str] = None,\n        *,\n        step_name: str,\n    ) -> List[Message]:\n        \"\"\"\n        Advances the conversation by sending message history\n        to LLM and updating with the response.\n\n        Parameters\n        ----------\n        messages : List[Message]\n            The list of messages in the conversation.\n        prompt : Optional[str], optional\n            The prompt to use, by default None.\n        step_name : str\n            The name of the step.\n\n        Returns\n        -------\n        List[Message]\n            The updated list of messages in the conversation.\n        \"\"\"\n\n        if prompt:\n            messages.append(HumanMessage(content=prompt))\n\n        logger.debug(\n            \"Creating a new chat completion: %s\",\n            \"\\n\".join([m.pretty_repr() for m in messages]),\n        )\n\n        if not self.vision:\n            messages = self._collapse_text_messages(messages)\n\n        response = self.backoff_inference(messages)\n\n        self.token_usage_log.update_log(\n            messages=messages, answer=response.content, step_name=step_name\n        )\n        messages.append(response)\n        logger.debug(f\"Chat completion finished: {messages}\")\n\n        return messages\n\n    @backoff.on_exception(backoff.expo, openai.RateLimitError, max_tries=7, max_time=45)\n    def backoff_inference(self, messages):\n        \"\"\"\n        Perform inference using the language model while implementing an exponential backoff strategy.\n\n        This function will retry the inference in case of a rate limit error from the OpenAI API.\n        It uses an exponential backoff strategy, meaning the wait time between retries increases\n        exponentially. The function will attempt to retry up to 7 times within a span of 45 seconds.\n\n        Parameters\n        ----------\n        messages : List[Message]\n            A list of chat messages which will be passed to the language model for processing.\n\n        callbacks : List[Callable]\n            A list of callback functions that are triggered after each inference. These functions\n            can be used for logging, monitoring, or other auxiliary tasks.\n\n        Returns\n        -------\n        Any\n            The output from the language model after processing the provided messages.\n\n        Raises\n        ------\n        openai.error.RateLimitError\n            If the number of retries exceeds the maximum or if the rate limit persists beyond the\n            allotted time, the function will ultimately raise a RateLimitError.\n\n        Example\n        -------\n        >>> messages = [SystemMessage(content=\"Hello\"), HumanMessage(content=\"How's the weather?\")]\n        >>> response = backoff_inference(messages)\n        \"\"\"\n        return self.llm.invoke(messages)  # type: ignore\n\n    @staticmethod\n    def serialize_messages(messages: List[Message]) -> str:\n        \"\"\"\n        Serialize a list of messages to a JSON string.\n\n        Parameters\n        ----------\n        messages : List[Message]\n            The list of messages to serialize.\n\n        Returns\n        -------\n        str\n            The serialized messages as a JSON string.\n        \"\"\"\n        return json.dumps(messages_to_dict(messages))\n\n    @staticmethod\n    def deserialize_messages(jsondictstr: str) -> List[Message]:\n        \"\"\"\n        Deserialize a JSON string to a list of messages.\n\n        Parameters\n        ----------\n        jsondictstr : str\n            The JSON string to deserialize.\n\n        Returns\n        -------\n        List[Message]\n            The deserialized list of messages.\n        \"\"\"\n        data = json.loads(jsondictstr)\n        # Modify implicit is_chunk property to ALWAYS false\n        # since Langchain's Message schema is stricter\n        prevalidated_data = [\n            {**item, \"tools\": {**item.get(\"tools\", {}), \"is_chunk\": False}}\n            for item in data\n        ]\n        return list(messages_from_dict(prevalidated_data))  # type: ignore\n\n    def _create_chat_model(self) -> BaseChatModel:\n        \"\"\"\n        Create a chat model with the specified model name and temperature.\n\n        Parameters\n        ----------\n        model : str\n            The name of the model to create.\n        temperature : float\n            The temperature to use for the model.\n\n        Returns\n        -------\n        BaseChatModel\n            The created chat model.\n        \"\"\"\n        if self.azure_endpoint:\n            return AzureChatOpenAI(\n                azure_endpoint=self.azure_endpoint,\n                openai_api_version=os.getenv(\n                    \"OPENAI_API_VERSION\", \"2024-05-01-preview\"\n                ),\n                deployment_name=self.model_name,\n                openai_api_type=\"azure\",\n                streaming=self.streaming,\n                callbacks=[StreamingStdOutCallbackHandler()],\n            )\n        elif \"claude\" in self.model_name:\n            return ChatAnthropic(\n                model=self.model_name,\n                temperature=self.temperature,\n                callbacks=[StreamingStdOutCallbackHandler()],\n                streaming=self.streaming,\n                max_tokens_to_sample=4096,\n            )\n        elif self.vision:\n            return ChatOpenAI(\n                model=self.model_name,\n                temperature=self.temperature,\n                streaming=self.streaming,\n                callbacks=[StreamingStdOutCallbackHandler()],\n                max_tokens=4096,  # vision models default to low max token limits\n            )\n        else:\n            return ChatOpenAI(\n                model=self.model_name,\n                temperature=self.temperature,\n                streaming=self.streaming,\n                callbacks=[StreamingStdOutCallbackHandler()],\n            )\n\n\ndef serialize_messages(messages: List[Message]) -> str:\n    return AI.serialize_messages(messages)\n\n\nclass ClipboardAI(AI):\n    # Ignore not init superclass\n    def __init__(self, **_):  # type: ignore\n        self.vision = False\n        self.token_usage_log = TokenUsageLog(\"clipboard_llm\")\n\n    @staticmethod\n    def serialize_messages(messages: List[Message]) -> str:\n        return \"\\n\\n\".join([f\"{m.type}:\\n{m.content}\" for m in messages])\n\n    @staticmethod\n    def multiline_input():\n        print(\"Enter/Paste your content. Ctrl-D or Ctrl-Z ( windows ) to save it.\")\n        content = []\n        while True:\n            try:\n                line = input()\n            except EOFError:\n                break\n            content.append(line)\n        return \"\\n\".join(content)\n\n    def next(\n        self,\n        messages: List[Message],\n        prompt: Optional[str] = None,\n        *,\n        step_name: str,\n    ) -> List[Message]:\n        \"\"\"\n        Not yet fully supported\n        \"\"\"\n        if prompt:\n            messages.append(HumanMessage(content=prompt))\n\n        logger.debug(f\"Creating a new chat completion: {messages}\")\n\n        msgs = self.serialize_messages(messages)\n        pyperclip.copy(msgs)\n        Path(\"clipboard.txt\").write_text(msgs)\n        print(\n            \"Messages copied to clipboard and written to clipboard.txt,\",\n            len(msgs),\n            \"characters in total\",\n        )\n\n        response = self.multiline_input()\n\n        messages.append(AIMessage(content=response))\n        logger.debug(f\"Chat completion finished: {messages}\")\n\n        return messages\n", "gpt_engineer/core/linting.py": "import black\n\nfrom gpt_engineer.core.files_dict import FilesDict\n\n\nclass Linting:\n    def __init__(self):\n        # Dictionary to hold linting methods for different file types\n        self.linters = {\".py\": self.lint_python}\n\n    import black\n\n    def lint_python(self, content, config):\n        \"\"\"Lint Python files using the `black` library, handling all exceptions silently and logging them.\n        This function attempts to format the code and returns the formatted code if successful.\n        If any error occurs during formatting, it logs the error and returns the original content.\n        \"\"\"\n        try:\n            # Try to format the content using black\n            linted_content = black.format_str(content, mode=black.FileMode(**config))\n        except black.NothingChanged:\n            # If nothing changed, log the info and return the original content\n            print(\"\\nInfo: No changes were made during formatting.\\n\")\n            linted_content = content\n        except Exception as error:\n            # If any other exception occurs, log the error and return the original content\n            print(f\"\\nError: Could not format due to {error}\\n\")\n            linted_content = content\n        return linted_content\n\n    def lint_files(self, files_dict: FilesDict, config: dict = None) -> FilesDict:\n        \"\"\"\n        Lints files based on their extension using registered linting functions.\n\n        Parameters\n        ----------\n        files_dict : FilesDict\n            The dictionary of file names to their respective source code content.\n        config : dict, optional\n            A dictionary of configuration options for the linting tools.\n\n        Returns\n        -------\n        FilesDict\n            The dictionary of file names to their respective source code content after linting.\n        \"\"\"\n        if config is None:\n            config = {}\n\n        for filename, content in files_dict.items():\n            extension = filename[\n                filename.rfind(\".\") :\n            ].lower()  # Ensure case insensitivity\n            if extension in self.linters:\n                original_content = content\n                linted_content = self.linters[extension](content, config)\n                if linted_content != original_content:\n                    print(f\"Linted {filename}.\")\n                else:\n                    print(f\"No changes made for {filename}.\")\n                files_dict[filename] = linted_content\n            else:\n                print(f\"No linter registered for {filename}.\")\n        return files_dict\n", "gpt_engineer/core/diff.py": "\"\"\"\nFile Overview:\n\nThis Python module is designed for processing and analyzing diffs in source code files. Diffs represent the changes between two versions of a file, which are crucial in version control systems for tracking file modifications. The module focuses on the detailed examination of these diffs, enabling users to understand, validate, and correct changes between file versions.\n\nKey Features:\n\n1. The `Hunk` class encapsulates a contiguous block of changes within a file. It includes detailed information such as start lines before and after edits, lengths of change blocks, and specific line changes categorized as additions, deletions, or unchanged.\n\n2. The `Diff` class represents a complete set of changes across a file and may contain multiple `Hunk` objects. It facilitates operations like generating string representations of diffs, and validating and correcting hunks based on the original file content.\n\n3. Functions within the module allow for the validation of hunks against original files, identifying mismatches, and making necessary corrections. This feature ensures that diffs are accurate and reflect true changes.\n\n4. Utility functions `is_similar` and `count_ratio` offer the capability to compare strings for similarity, accounting for variations in spacing and case. This aids in the validation process by allowing a flexible comparison of code lines.\n\nDependencies:\n\n- `logging`: Utilized for logging warnings and errors encountered during the validation and correction process.\n- `collections.Counter`: Used for counting occurrences of characters in strings, supporting the string similarity assessment functions.\n\nFunctions and Classes:\n\n1. `Hunk`: Class representing a block of changes within a file, with methods for managing and validating these changes.\n\n2. `Diff`: Class representing the entire set of changes in a file, containing multiple `Hunk` instances and methods for overall diff management.\n\n3. `is_similar(str1, str2, similarity_threshold)`: Function to compare two strings for similarity, useful in validating line changes in hunks.\n\n4. `count_ratio(str1, str2)`: Function that computes the ratio of common characters to the length of the longer string, aiding in the assessment of line similarity.\n\nThis module is essential for developers and teams utilizing version control systems, providing tools for a deeper analysis and correction of diffs, ensuring the integrity and accuracy of code changes.\n\n\"\"\"\nimport logging\n\nfrom collections import Counter\nfrom typing import List\n\nRETAIN = \"retain\"\nADD = \"add\"\nREMOVE = \"remove\"\n\n\nclass Hunk:\n    \"\"\"\n    Represents a section of a file diff, containing changes made to that section.\n\n    Attributes:\n        start_line_pre_edit (int): The starting line number in the original file.\n        hunk_len_pre_edit (int): The length of the hunk in the original file.\n        start_line_post_edit (int): The starting line number in the edited file.\n        hunk_len_post_edit (int): The length of the hunk in the edited file.\n        lines (list): A list of tuples representing the lines in the hunk and their types (RETAIN, ADD, REMOVE).\n        category_counts (dict): A count of lines by their type.\n        is_new_file (bool): Flag indicating if the hunk represents a new file.\n    \"\"\"\n\n    def __init__(\n        self,\n        start_line_pre_edit,\n        hunk_len_pre_edit,\n        start_line_post_edit,\n        hunk_len_post_edit,\n        lines,\n    ) -> None:\n        self.start_line_pre_edit = start_line_pre_edit\n        self.hunk_len_pre_edit = hunk_len_pre_edit\n        self.start_line_post_edit = start_line_post_edit\n        self.hunk_len_post_edit = hunk_len_post_edit\n        self.category_counts = {RETAIN: 0, ADD: 0, REMOVE: 0}\n        self.lines = list()\n        self.add_lines(lines)\n        self.forward_block_len = 10\n        # Note that this assumption should not be done on hunk level, however, if the below is true, no validation is possible anyway.\n        if self.category_counts[RETAIN] == 0 and self.category_counts[REMOVE] == 0:\n            self.is_new_file = True\n        else:\n            self.is_new_file = False\n\n    def add_retained_line(self, line, index) -> None:\n        \"\"\"Adds a retained line to the hunk at the specified index.\"\"\"\n        self.lines.insert(index, (RETAIN, line))\n        self.category_counts[RETAIN] += 1\n\n    def relabel_line(self, index, new_label) -> None:\n        \"\"\"Changes the label of a line at the specified index.\"\"\"\n        old_label = self.lines[index][0]\n        self.lines[index] = (new_label, self.lines[index][1])\n        self.category_counts[old_label] -= 1\n        self.category_counts[new_label] += 1\n\n    def pop_line(self, line, index) -> None:\n        \"\"\"Removes a line from the hunk at the specified index.\"\"\"\n        self.lines.pop(index)\n        assert self.category_counts[line[0]] > 0\n        self.category_counts[line[0]] -= 1\n\n    def add_lines(self, new_lines) -> None:\n        \"\"\"Adds multiple lines to the hunk.\"\"\"\n        for line in new_lines:\n            self.lines.append(line)\n            self.category_counts[line[0]] += 1\n\n    def hunk_to_string(self) -> str:\n        \"\"\"Converts the hunk to a string representation.\"\"\"\n        string = f\"@@ -{self.start_line_pre_edit},{self.hunk_len_pre_edit} +{self.start_line_post_edit},{self.hunk_len_post_edit} @@\\n\"\n        for line_type, line_content in self.lines:\n            line_prefix = (\n                \" \" if line_type == RETAIN else \"+\" if line_type == ADD else \"-\"\n            )\n            string += f\"{line_prefix}{line_content}\\n\"\n        return string\n\n    def make_forward_block(self, hunk_ind: int, forward_block_len) -> str:\n        \"\"\"Creates a block of lines for forward comparison.\"\"\"\n        forward_lines = [\n            line[1] for line in self.lines[hunk_ind:] if not line[0] == ADD\n        ]\n        forward_block = \"\\n\".join(forward_lines[0:forward_block_len])\n        return forward_block\n\n    def check_start_line(self, lines_dict: dict) -> bool:\n        \"\"\"Check if the starting line of a hunk is present in the original code and returns a boolean value accordingly.\"\"\"\n        if self.is_new_file:\n            # this hunk cannot be falsified and is by definition true\n            return True\n        if self.start_line_pre_edit in lines_dict:\n            # check the location of the actual starting line:\n            is_similar(self.lines[0][1], lines_dict[self.start_line_pre_edit])\n        else:\n            pass\n\n    def find_start_line(self, lines_dict: dict, problems: list) -> bool:\n        \"\"\"Finds the starting line of the hunk in the original code and returns a boolean value accordingly. If the starting line is not found, it appends a problem message to the problems list.\"\"\"\n\n        # ToDo handle the case where the start line is 0 or 1 characters separately\n        if self.lines[0][0] == ADD:\n            # handle the case where the start line is an add\n            start_line = None\n            # find the first line that is not an add\n            for index, line in enumerate(self.lines):\n                if line[0] != ADD:\n                    for line_number, line_content in lines_dict.items():\n                        # if the line is similar to a non-blank line in line_dict, we can pick the line prior to it\n                        if is_similar(line[1], line_content) and line[1] != \"\":\n                            start_line = line_number - 1\n                            break\n                    # if the start line is not found, append a problem message\n                    if start_line is None:\n                        problems.append(\n                            f\"In {self.hunk_to_string()}:can not find the starting line of the diff\"\n                        )\n                        return False\n\n                    else:\n                        # the line prior to the start line is found now we insert it to the first place as the start line\n                        self.start_line_pre_edit = start_line\n                        retain_line = lines_dict.get(start_line, \"\")\n                        if retain_line:\n                            self.add_retained_line(lines_dict[start_line], 0)\n                            return self.validate_and_correct(lines_dict, problems)\n                        else:\n                            problems.append(\n                                f\"In {self.hunk_to_string()}:The starting line of the diff {self.hunk_to_string()} does not exist in the code\"\n                            )\n                            return False\n        pot_start_lines = {\n            key: is_similar(self.lines[0][1], line) for key, line in lines_dict.items()\n        }\n        sum_of_matches = sum(pot_start_lines.values())\n        if sum_of_matches == 0:\n            # before we go any further, we should check if it's a comment from LLM\n            if self.lines[0][1].count(\"#\") > 0:\n                # if it is, we can mark it as an ADD lines\n                self.relabel_line(0, ADD)\n                # and restart the validation at the next line\n                return self.validate_and_correct(lines_dict, problems)\n\n            else:\n                problems.append(\n                    f\"In {self.hunk_to_string()}:The starting line of the diff {self.hunk_to_string()} does not exist in the code\"\n                )\n                return False\n        elif sum_of_matches == 1:\n            start_ind = list(pot_start_lines.keys())[\n                list(pot_start_lines.values()).index(True)\n            ]  # lines are one indexed\n        else:\n            logging.warning(\"multiple candidates for starting index\")\n            # ToDo handle all the cases better again here. Smartest choice is that, for each candidate check match to the next line etc (recursively)\n            start_ind = list(pot_start_lines.keys())[\n                list(pot_start_lines.values()).index(True)\n            ]\n        self.start_line_pre_edit = start_ind\n\n        # This should now be fulfilled by default\n        assert is_similar(self.lines[0][1], lines_dict[self.start_line_pre_edit])\n        return True\n\n    def validate_lines(self, lines_dict: dict, problems: list) -> bool:\n        \"\"\"Validates the lines of the hunk against the original file and returns a boolean value accordingly. If the lines do not match, it appends a problem message to the problems list.\"\"\"\n        hunk_ind = 0\n        file_ind = self.start_line_pre_edit\n        # make an orig hunk lines for logging\n        # orig_hunk_lines = deepcopy(self.lines)\n        while hunk_ind < len(self.lines) and file_ind <= max(lines_dict):\n            if self.lines[hunk_ind][0] == ADD:\n                # this cannot be validated, jump one index\n                hunk_ind += 1\n            elif not is_similar(self.lines[hunk_ind][1], lines_dict[file_ind]):\n                # before we go any further, we should relabel the comment from LLM\n                if self.lines[hunk_ind][1].count(\"#\") > 0:\n                    self.relabel_line(hunk_ind, ADD)\n                    continue\n\n                # make a forward block from the code for comparisons\n                forward_code = \"\\n\".join(\n                    [\n                        lines_dict[ind]\n                        for ind in range(\n                            file_ind,\n                            min(\n                                file_ind + self.forward_block_len,\n                                max(lines_dict.keys()),\n                            ),\n                        )\n                    ]\n                )\n                # make the original forward block for quantitative comparison\n                forward_block = self.make_forward_block(\n                    hunk_ind, self.forward_block_len\n                )\n                orig_count_ratio = count_ratio(forward_block, forward_code)\n                # Here we have 2 cases\n                # 1) some lines were simply skipped in the diff and we should add them to the diff\n                # If this is the case, adding the line to the diff, should give an improved forward diff\n                forward_block_missing_line = self.make_forward_block(\n                    hunk_ind, self.forward_block_len - 1\n                )\n                # insert the missing line in front of the block\n                forward_block_missing_line = \"\\n\".join(\n                    [lines_dict[file_ind], forward_block_missing_line]\n                )\n                missing_line_count_ratio = count_ratio(\n                    forward_block_missing_line, forward_code\n                )\n                # 2) Additional lines, not belonging to the code were added to the diff\n                forward_block_false_line = self.make_forward_block(\n                    hunk_ind + 1, self.forward_block_len\n                )\n                false_line_count_ratio = count_ratio(\n                    forward_block_false_line, forward_code\n                )\n                if (\n                    orig_count_ratio >= missing_line_count_ratio\n                    and orig_count_ratio >= false_line_count_ratio\n                ):\n                    problems.append(\n                        f\"In Hunk:{self.hunk_to_string()}, there was at least one mismatch.\"\n                    )\n                    return False\n\n                elif missing_line_count_ratio > false_line_count_ratio:\n                    self.add_retained_line(lines_dict[file_ind], hunk_ind)\n                    hunk_ind += 1\n                    file_ind += 1\n                    # NOTE: IF THE LLM SKIPS SOME LINES AND HAS ADDs ADJACENT TO THE SKIPPED BLOCK,\n                    # WE CANNOT KNOW WHETHER THE ADDs SHOULD BE BEFORE OR AFTER THE BLOCK. WE OPT FOR PUTTING IT BEFORE.\n                    # IF IT MATTERED, WE ASSUME THE LLM WOULD NOT SKIP THE BLOCK\n                else:\n                    self.pop_line(self.lines[hunk_ind], hunk_ind)\n\n            else:\n                hunk_ind += 1\n                file_ind += 1\n        # if we have not validated all lines, we have a problem\n        if hunk_ind < len(self.lines) - 1:\n            remaining_lines = \"\\n\".join(\n                f\"{line_type}: {line_content}\"\n                for line_type, line_content in self.lines[file_ind + 1 :]\n            )\n            problems.append(\n                f\"In {self.hunk_to_string()}:Hunk validation stopped before the lines {remaining_lines} were validated. The diff is incorrect\"\n            )\n            return False\n        return True\n\n    def validate_and_correct(\n        self,\n        lines_dict: dict,\n        problems: list,\n    ) -> bool:\n        \"\"\"\n        Validates and corrects the hunk based on the original lines.\n\n        This function attempts to validate the hunk by comparing its lines to the original file and making corrections\n        where necessary. It also identifies problems such as non-matching lines or incorrect line types.\n        \"\"\"\n        start_true = self.check_start_line(lines_dict)\n\n        if not start_true:\n            if not self.find_start_line(lines_dict, problems):\n                return False\n\n        # Now we should be able to validate the hunk line by line and add missing line\n        if not self.validate_lines(lines_dict, problems):\n            return False\n        # Pass the validation\n        return True\n\n\nclass Diff:\n    \"\"\"\n    Represents a file diff, containing multiple hunks of changes.\n\n    Attributes:\n        filename_pre (str): The name of the original file.\n        filename_post (str): The name of the edited file.\n        hunks (list): A list of Hunk objects representing the changes in the diff.\n    \"\"\"\n\n    def __init__(self, filename_pre, filename_post) -> None:\n        self.filename_pre = filename_pre\n        self.filename_post = filename_post\n        self.hunks = []\n\n    def is_new_file(self) -> bool:\n        \"\"\"Determines if the diff represents a new file.\"\"\"\n        if self.filename_pre == \"/dev/null\":\n            return True\n        return any(hunk.is_new_file for hunk in self.hunks)\n\n    def diff_to_string(self) -> str:\n        \"\"\"Converts the diff to a string representation.\"\"\"\n        string = f\"--- {self.filename_pre}\\n+++ {self.filename_post}\\n\"\n        for hunk in self.hunks:\n            string += hunk.hunk_to_string()\n        return string.strip()\n\n    def validate_and_correct(self, lines_dict: dict) -> List[str]:\n        \"\"\"Validates and corrects each hunk in the diff.\"\"\"\n        problems = []\n        past_hunk = None\n        cut_lines_dict = lines_dict.copy()\n        for hunk in self.hunks:\n            if past_hunk is not None:\n                # make sure to not cut so much that the start_line gets out of range\n                cut_ind = min(\n                    past_hunk.start_line_pre_edit + past_hunk.hunk_len_pre_edit,\n                    hunk.start_line_pre_edit,\n                )\n                cut_lines_dict = {\n                    key: val for key, val in cut_lines_dict.items() if key >= (cut_ind)\n                }\n            is_valid = hunk.validate_and_correct(cut_lines_dict, problems)\n            if not is_valid and len(problems) > 0:\n                for idx, val in enumerate(problems):\n                    print(f\"\\nInvalid Hunk NO.{idx}---\\n{val}\\n---\")\n                self.hunks.remove(hunk)\n            # now correct the numbers, assuming the start line pre-edit has been fixed\n            hunk.hunk_len_pre_edit = (\n                hunk.category_counts[RETAIN] + hunk.category_counts[REMOVE]\n            )\n            hunk.hunk_len_post_edit = (\n                hunk.category_counts[RETAIN] + hunk.category_counts[ADD]\n            )\n            if past_hunk is not None:\n                hunk.start_line_post_edit = (\n                    hunk.start_line_pre_edit\n                    + past_hunk.hunk_len_post_edit\n                    - past_hunk.hunk_len_pre_edit\n                    + past_hunk.start_line_post_edit\n                    - past_hunk.start_line_pre_edit\n                )\n            else:\n                hunk.start_line_post_edit = hunk.start_line_pre_edit\n            past_hunk = hunk\n        return problems\n\n\ndef is_similar(str1, str2, similarity_threshold=0.9) -> bool:\n    \"\"\"\n    Compares two strings for similarity, ignoring spaces and case.\n\n    Parameters\n    ----------\n    str1, str2 : str\n        The strings to compare.\n    similarity_threshold: float\n        How similar must the strings be\n\n    Returns\n    -------\n    bool\n        True if the strings are similar, False otherwise.\n    \"\"\"\n\n    return count_ratio(str1, str2) >= similarity_threshold\n\n\ndef count_ratio(str1, str2) -> float:\n    \"\"\"\n    Computes the ratio of common characters to the length of the longer string, ignoring spaces and case.\n\n    Parameters:\n    - str1, str2 (str): The strings to compare.\n\n    Returns:\n    - float: The ratio of common characters to the length of the longer string.\n    \"\"\"\n    str1, str2 = str1.replace(\" \", \"\").lower(), str2.replace(\" \", \"\").lower()\n\n    counter1, counter2 = Counter(str1), Counter(str2)\n    intersection = sum((counter1 & counter2).values())\n    longer_length = max(len(str1), len(str2))\n    if longer_length == 0:\n        return 1\n    else:\n        return intersection / longer_length\n", "gpt_engineer/core/base_memory.py": "\"\"\"\nBase Memory Module\n\nThis module provides a type alias for a mutable mapping that represents the base memory structure\nused in the GPT Engineer project. The base memory is a mapping from file names (as strings or Path objects)\nto their corresponding code content (as strings).\n\nType Aliases:\n    BaseMemory: A mutable mapping from file names to code content.\n\"\"\"\n\nfrom pathlib import Path\nfrom typing import MutableMapping, Union\n\nBaseMemory = MutableMapping[Union[str, Path], str]\n", "gpt_engineer/core/files_dict.py": "\"\"\"\nFilesDict Module\n\nThis module provides a FilesDict class which is a dictionary-based container for managing code files.\nIt extends the standard dictionary to enforce string keys and values, representing filenames and their\ncorresponding code content. It also provides methods to format its contents for chat-based interaction\nwith an AI agent and to enforce type checks on keys and values.\n\nClasses:\n    FilesDict: A dictionary-based container for managing code files.\n\"\"\"\nfrom collections import OrderedDict\nfrom pathlib import Path\nfrom typing import Union\n\n\n# class Code(MutableMapping[str | Path, str]):\n# ToDo: implement as mutable mapping, potentially holding a dict instead of being a dict.\nclass FilesDict(dict):\n    \"\"\"\n    A dictionary-based container for managing code files.\n\n    This class extends the standard dictionary to enforce string keys and values,\n    representing filenames and their corresponding code content. It provides methods\n    to format its contents for chat-based interaction with an AI agent and to enforce\n    type checks on keys and values.\n    \"\"\"\n\n    def __setitem__(self, key: Union[str, Path], value: str):\n        \"\"\"\n        Set the code content for the given filename, enforcing type checks on the key and value.\n\n        Overrides the dictionary's __setitem__ to enforce type checks on the key and value.\n        The key must be a string or a Path object, and the value must be a string representing\n        the code content.\n\n        Parameters\n        ----------\n        key : Union[str, Path]\n            The filename as a key for the code content.\n        value : str\n            The code content to associate with the filename.\n\n        Raises\n        ------\n        TypeError\n            If the key is not a string or Path, or if the value is not a string.\n        \"\"\"\n        if not isinstance(key, (str, Path)):\n            raise TypeError(\"Keys must be strings or Path's\")\n        if not isinstance(value, str):\n            raise TypeError(\"Values must be strings\")\n        super().__setitem__(key, value)\n\n    def to_chat(self):\n        \"\"\"\n        Formats the items of the object (assuming file name and content pairs)\n        into a string suitable for chat display.\n\n        Returns\n        -------\n        str\n            A string representation of the files.\n        \"\"\"\n        chat_str = \"\"\n        for file_name, file_content in self.items():\n            lines_dict = file_to_lines_dict(file_content)\n            chat_str += f\"File: {file_name}\\n\"\n            for line_number, line_content in lines_dict.items():\n                chat_str += f\"{line_number} {line_content}\\n\"\n            chat_str += \"\\n\"\n        return f\"```\\n{chat_str}```\"\n\n    def to_log(self):\n        \"\"\"\n        Formats the items of the object (assuming file name and content pairs)\n        into a string suitable for log display.\n\n        Returns\n        -------\n        str\n            A string representation of the files.\n        \"\"\"\n        log_str = \"\"\n        for file_name, file_content in self.items():\n            log_str += f\"File: {file_name}\\n\"\n            log_str += file_content\n            log_str += \"\\n\"\n        return log_str\n\n\ndef file_to_lines_dict(file_content: str) -> dict:\n    \"\"\"\n    Converts file content into a dictionary where each line number is a key\n    and the corresponding line content is the value.\n\n    Parameters\n    ----------\n    file_name : str\n        The name of the file.\n    file_content : str\n        The content of the file.\n\n    Returns\n    -------\n    dict\n        A dictionary with file names as keys and dictionaries (line numbers as keys and line contents as values) as values.\n    \"\"\"\n    lines_dict = OrderedDict(\n        {\n            line_number: line_content\n            for line_number, line_content in enumerate(file_content.split(\"\\n\"), 1)\n        }\n    )\n    return lines_dict\n", "gpt_engineer/core/git.py": "import shutil\nimport subprocess\n\nfrom pathlib import Path\nfrom typing import List\n\nfrom gpt_engineer.core.files_dict import FilesDict\n\n\ndef is_git_installed():\n    return shutil.which(\"git\") is not None\n\n\ndef is_git_repo(path: Path):\n    return (\n        subprocess.run(\n            [\"git\", \"rev-parse\", \"--is-inside-work-tree\"],\n            cwd=path,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n        ).returncode\n        == 0\n    )\n\n\ndef init_git_repo(path: Path):\n    subprocess.run([\"git\", \"init\"], cwd=path)\n\n\ndef has_uncommitted_changes(path: Path):\n    return bool(\n        subprocess.run(\n            [\"git\", \"diff\", \"--exit-code\"],\n            cwd=path,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n        ).returncode\n    )\n\n\ndef filter_files_with_uncommitted_changes(\n    basepath: Path, files_dict: FilesDict\n) -> List[Path]:\n    files_with_diff = (\n        subprocess.run(\n            [\"git\", \"diff\", \"--name-only\"], cwd=basepath, stdout=subprocess.PIPE\n        )\n        .stdout.decode()\n        .splitlines()\n    )\n    return [f for f in files_dict.keys() if f in files_with_diff]\n\n\ndef stage_files(path: Path, files: List[str]):\n    subprocess.run([\"git\", \"add\", *files], cwd=path)\n\n\ndef filter_by_gitignore(path: Path, file_list: List[str]) -> List[str]:\n    out = subprocess.run(\n        [\"git\", \"-C\", \".\", \"check-ignore\", \"--no-index\", \"--stdin\"],\n        cwd=path,\n        input=\"\\n\".join(file_list).encode(),\n        stdout=subprocess.PIPE,\n        stderr=subprocess.PIPE,\n    )\n    paths = out.stdout.decode().splitlines()\n    # return file_list but filter out the results from git check-ignore\n    return [f for f in file_list if f not in paths]\n\n\ndef stage_uncommitted_to_git(path, files_dict, improve_mode):\n    # Check if there's a git repo and verify that there aren't any uncommitted changes\n    if is_git_installed() and not improve_mode:\n        if not is_git_repo(path):\n            print(\"\\nInitializing an empty git repository\")\n            init_git_repo(path)\n\n    if is_git_repo(path):\n        modified_files = filter_files_with_uncommitted_changes(path, files_dict)\n        if modified_files:\n            print(\n                \"Staging the following uncommitted files before overwriting: \",\n                \", \".join(modified_files),\n            )\n            stage_files(path, modified_files)\n", "gpt_engineer/core/version_manager.py": "\"\"\"\nVersion Manager Module\n\nThis module provides an abstract base class for a version manager that handles the creation of snapshots\nfor code. Implementations of this class are expected to provide methods to create a snapshot of the given\ncode and return a reference to it.\n\"\"\"\nfrom abc import ABC, abstractmethod\nfrom pathlib import Path\nfrom typing import Union\n\nfrom gpt_engineer.core.files_dict import FilesDict\n\n\nclass BaseVersionManager(ABC):\n    \"\"\"\n    Abstract base class for a version manager.\n\n    Defines the interface for version managers that handle the creation of snapshots for code.\n    Implementations of this class are expected to provide methods to create a snapshot of the given\n    code and return a reference to it.\n    \"\"\"\n\n    @abstractmethod\n    def __init__(self, path: Union[str, Path]):\n        pass\n\n    @abstractmethod\n    def snapshot(self, files_dict: FilesDict) -> str:\n        pass\n", "gpt_engineer/core/chat_to_files.py": "\"\"\"\nThis Python script provides functionalities for parsing chat transcripts that contain file paths and code blocks,\napplying diffs to these files, and parsing unified git diff format strings. The script is designed to work within\na larger system that involves processing and manipulating code files based on chat inputs and diff information.\n\nKey Components:\n- chat_to_files_dict: Parses a chat transcript, extracting file paths and associated code blocks, and organizes\n  them into a FilesDict object, which is a custom dictionary format designed to hold file contents keyed by their paths.\n\n- apply_diffs: Takes a dictionary of Diff objects (which represent changes to be made to files) and a FilesDict\n  object containing the current state of files. It applies the changes described by the Diff objects to the\n  corresponding files in the FilesDict, updating the file contents as specified by the diffs.\n\n- parse_diffs: Parses a string containing diffs in the unified git diff format, extracting the changes described\n  in the diffs and organizing them into a dictionary of Diff objects, keyed by the filename to which each diff applies.\n\n- parse_diff_block: Parses a single block of text from a diff string, translating it into a Diff object that\n  represents the changes described in that block of text.\n\nThis script is intended for use in environments where code collaboration or review is conducted through chat interfaces,\nallowing for the dynamic application of changes to code bases and the efficient handling of file and diff information in chat transcripts.\n\"\"\"\n\nimport logging\nimport re\n\nfrom typing import Dict, Tuple\n\nfrom regex import regex\n\nfrom gpt_engineer.core.diff import ADD, REMOVE, RETAIN, Diff, Hunk\nfrom gpt_engineer.core.files_dict import FilesDict, file_to_lines_dict\n\n# Initialize a logger for this module\nlogger = logging.getLogger(__name__)\n\n\ndef chat_to_files_dict(chat: str) -> FilesDict:\n    \"\"\"\n    Converts a chat string containing file paths and code blocks into a FilesDict object.\n\n    Args:\n    - chat (str): The chat string containing file paths and code blocks.\n\n    Returns:\n    - FilesDict: A dictionary with file paths as keys and code blocks as values.\n    \"\"\"\n    # Regex to match file paths and associated code blocks\n    regex = r\"(\\S+)\\n\\s*```[^\\n]*\\n(.+?)```\"\n    matches = re.finditer(regex, chat, re.DOTALL)\n\n    files_dict = FilesDict()\n    for match in matches:\n        # Clean and standardize the file path\n        path = re.sub(r'[\\:<>\"|?*]', \"\", match.group(1))\n        path = re.sub(r\"^\\[(.*)\\]$\", r\"\\1\", path)\n        path = re.sub(r\"^`(.*)`$\", r\"\\1\", path)\n        path = re.sub(r\"[\\]\\:]$\", \"\", path)\n\n        # Extract and clean the code content\n        content = match.group(2)\n\n        # Add the cleaned path and content to the FilesDict\n        files_dict[path.strip()] = content.strip()\n\n    return files_dict\n\n\ndef apply_diffs(diffs: Dict[str, Diff], files: FilesDict) -> FilesDict:\n    \"\"\"\n    Applies diffs to the provided files.\n\n    Args:\n    - diffs (Dict[str, Diff]): A dictionary of diffs to apply, keyed by filename.\n    - files (FilesDict): The original files to which diffs will be applied.\n\n    Returns:\n    - FilesDict: The updated files after applying diffs.\n    \"\"\"\n    files = FilesDict(files.copy())\n    REMOVE_FLAG = \"<REMOVE_LINE>\"  # Placeholder to mark lines for removal\n    for diff in diffs.values():\n        if diff.is_new_file():\n            # If it's a new file, create it with the content from the diff\n            files[diff.filename_post] = \"\\n\".join(\n                line[1] for hunk in diff.hunks for line in hunk.lines\n            )\n        else:\n            # Convert the file content to a dictionary of lines\n            line_dict = file_to_lines_dict(files[diff.filename_pre])\n            for hunk in diff.hunks:\n                current_line = hunk.start_line_pre_edit\n                for line in hunk.lines:\n                    if line[0] == RETAIN:\n                        current_line += 1\n                    elif line[0] == ADD:\n                        # Handle added lines\n                        current_line -= 1\n                        if (\n                            current_line in line_dict.keys()\n                            and line_dict[current_line] != REMOVE_FLAG\n                        ):\n                            line_dict[current_line] += \"\\n\" + line[1]\n                        else:\n                            line_dict[current_line] = line[1]\n                        current_line += 1\n                    elif line[0] == REMOVE:\n                        # Mark removed lines with REMOVE_FLAG\n                        line_dict[current_line] = REMOVE_FLAG\n                        current_line += 1\n\n            # Remove lines marked for removal\n            line_dict = {\n                key: line_content\n                for key, line_content in line_dict.items()\n                if REMOVE_FLAG not in line_content\n            }\n            # Reassemble the file content\n            files[diff.filename_post] = \"\\n\".join(line_dict.values())\n    return files\n\n\ndef parse_diffs(diff_string: str) -> dict:\n    \"\"\"\n    Parses a diff string in the unified git diff format.\n\n    Args:\n    - diff_string (str): The diff string to parse.\n\n    Returns:\n    - dict: A dictionary of Diff objects keyed by filename.\n    \"\"\"\n    # Regex to match individual diff blocks\n    diff_block_pattern = regex.compile(\n        r\"```.*?\\n\\s*?--- .*?\\n\\s*?\\+\\+\\+ .*?\\n(?:@@ .*? @@\\n(?:[-+ ].*?\\n)*?)*?```\",\n        re.DOTALL,\n    )\n\n    diffs = {}\n    try:\n        for block in diff_block_pattern.finditer(diff_string, timeout=1):\n            diff_block = block.group()\n\n            # Parse individual diff blocks and update the diffs dictionary\n            diffs.update(parse_diff_block(diff_block))\n    except TimeoutError:\n        print(\"gpt-engineer timed out while parsing git diff\")\n\n    if not diffs:\n        print(\n            \"GPT did not provide any proposed changes. Please try to reselect the files for uploading and edit your prompt file.\"\n        )\n\n    return diffs\n\n\ndef parse_diff_block(diff_block: str) -> dict:\n    \"\"\"\n    Parses a block of diff text into a Diff object.\n\n    Args:\n    - diff_block (str): A single block of diff text.\n\n    Returns:\n    - dict: A dictionary containing a single Diff object keyed by the post-edit filename.\n    \"\"\"\n    lines = diff_block.strip().split(\"\\n\")[1:-1]  # Exclude the opening and closing ```\n    diffs = {}\n    current_diff = None\n    hunk_lines = []\n    filename_pre = None\n    filename_post = None\n    hunk_header = None\n\n    for line in lines:\n        if line.startswith(\"--- \"):\n            # Pre-edit filename\n            filename_pre = line[4:]\n        elif line.startswith(\"+++ \"):\n            # Post-edit filename and initiation of a new Diff object\n            if (\n                filename_post is not None\n                and current_diff is not None\n                and hunk_header is not None\n            ):\n                current_diff.hunks.append(Hunk(*hunk_header, hunk_lines))\n                hunk_lines = []\n            filename_post = line[4:]\n            current_diff = Diff(filename_pre, filename_post)\n            diffs[filename_post] = current_diff\n        elif line.startswith(\"@@ \"):\n            # Start of a new hunk in the diff\n            if hunk_lines and current_diff is not None and hunk_header is not None:\n                current_diff.hunks.append(Hunk(*hunk_header, hunk_lines))\n                hunk_lines = []\n            hunk_header = parse_hunk_header(line)\n        elif line.startswith(\"+\"):\n            # Added line\n            hunk_lines.append((ADD, line[1:]))\n        elif line.startswith(\"-\"):\n            # Removed line\n            hunk_lines.append((REMOVE, line[1:]))\n        else:\n            # Retained line\n            hunk_lines.append((RETAIN, line[1:]))\n\n    # Append the last hunk if any\n    if current_diff is not None and hunk_lines and hunk_header is not None:\n        current_diff.hunks.append(Hunk(*hunk_header, hunk_lines))\n\n    return diffs\n\n\ndef parse_hunk_header(header_line) -> Tuple[int, int, int, int]:\n    \"\"\"\n    Parses the header of a hunk from a diff.\n\n    Args:\n    - header_line (str): The header line of a hunk.\n\n    Returns:\n    - tuple: A tuple containing start and length information for pre- and post-edit.\n    \"\"\"\n    pattern = re.compile(r\"^@@ -\\d{1,},\\d{1,} \\+\\d{1,},\\d{1,} @@$\")\n\n    if not pattern.match(header_line):\n        # Return a default value if the header does not match the expected format\n        return 0, 0, 0, 0\n\n    pre, post = header_line.split(\" \")[1:3]\n    start_line_pre_edit, hunk_len_pre_edit = map(int, pre[1:].split(\",\"))\n    start_line_post_edit, hunk_len_post_edit = map(int, post[1:].split(\",\"))\n    return (\n        start_line_pre_edit,\n        hunk_len_pre_edit,\n        start_line_post_edit,\n        hunk_len_post_edit,\n    )\n", "gpt_engineer/core/preprompts_holder.py": "from pathlib import Path\nfrom typing import Dict\n\nfrom gpt_engineer.core.default.disk_memory import DiskMemory\n\n\nclass PrepromptsHolder:\n    \"\"\"\n    A holder for preprompt texts that are stored on disk.\n\n    This class provides methods to retrieve preprompt texts from a specified directory.\n\n    Attributes\n    ----------\n    preprompts_path : Path\n        The file path to the directory containing preprompt texts.\n\n    Methods\n    -------\n    get_preprompts() -> Dict[str, str]\n        Retrieve all preprompt texts from the directory and return them as a dictionary.\n    \"\"\"\n\n    def __init__(self, preprompts_path: Path):\n        self.preprompts_path = preprompts_path\n\n    def get_preprompts(self) -> Dict[str, str]:\n        preprompts_repo = DiskMemory(self.preprompts_path)\n        return {file_name: preprompts_repo[file_name] for file_name in preprompts_repo}\n", "gpt_engineer/core/prompt.py": "import json\n\nfrom typing import Dict, Optional\n\n\nclass Prompt:\n    def __init__(\n        self,\n        text: str,\n        image_urls: Optional[Dict[str, str]] = None,\n        entrypoint_prompt: str = \"\",\n    ):\n        self.text = text\n        self.image_urls = image_urls\n        self.entrypoint_prompt = entrypoint_prompt\n\n    def __repr__(self):\n        return f\"Prompt(text={self.text!r}, image_urls={self.image_urls!r})\"\n\n    def to_langchain_content(self):\n        content = [{\"type\": \"text\", \"text\": f\"Request: {self.text}\"}]\n\n        if self.image_urls:\n            for name, url in self.image_urls.items():\n                image_content = {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\n                        \"url\": url,\n                        \"detail\": \"low\",\n                    },\n                }\n                content.append(image_content)\n\n        return content\n\n    def to_dict(self):\n        return {\n            \"text\": self.text,\n            \"image_urls\": self.image_urls,\n            \"entrypoint_prompt\": self.entrypoint_prompt,\n        }\n\n    def to_json(self):\n        return json.dumps(self.to_dict())\n", "gpt_engineer/core/__init__.py": "", "gpt_engineer/core/base_execution_env.py": "from abc import ABC, abstractmethod\nfrom subprocess import Popen\nfrom typing import Optional, Tuple\n\nfrom gpt_engineer.core.files_dict import FilesDict\n\n\nclass BaseExecutionEnv(ABC):\n    \"\"\"\n    Abstract base class for an execution environment capable of running code.\n\n    This class defines the interface for execution environments that can execute commands,\n    handle processes, and manage file uploads and downloads.\n    \"\"\"\n\n    @abstractmethod\n    def run(self, command: str, timeout: Optional[int] = None) -> Tuple[str, str, int]:\n        \"\"\"\n        Runs a command in the execution environment.\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def popen(self, command: str) -> Popen:\n        \"\"\"\n        Runs a command in the execution environment.\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def upload(self, files: FilesDict) -> \"BaseExecutionEnv\":\n        \"\"\"\n        Uploads files to the execution environment.\n        \"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def download(self) -> FilesDict:\n        \"\"\"\n        Downloads files from the execution environment.\n        \"\"\"\n        raise NotImplementedError\n", "gpt_engineer/core/token_usage.py": "import base64\nimport io\nimport logging\nimport math\n\nfrom dataclasses import dataclass\nfrom typing import List, Union\n\nimport tiktoken\n\nfrom langchain.schema import AIMessage, HumanMessage, SystemMessage\nfrom PIL import Image\n\n# workaround for function moved in:\n# https://github.com/langchain-ai/langchain/blob/535db72607c4ae308566ede4af65295967bb33a8/libs/community/langchain_community/callbacks/openai_info.py\ntry:\n    from langchain.callbacks.openai_info import (\n        get_openai_token_cost_for_model,  # fmt: skip\n    )\nexcept ImportError:\n    from langchain_community.callbacks.openai_info import (\n        get_openai_token_cost_for_model,  # fmt: skip\n    )\n\n\nMessage = Union[AIMessage, HumanMessage, SystemMessage]\n\nlogger = logging.getLogger(__name__)\n\n\n@dataclass\nclass TokenUsage:\n    \"\"\"\n    Dataclass representing token usage statistics for a conversation step.\n\n    Attributes\n    ----------\n    step_name : str\n        The name of the conversation step.\n    in_step_prompt_tokens : int\n        The number of prompt tokens used in the step.\n    in_step_completion_tokens : int\n        The number of completion tokens used in the step.\n    in_step_total_tokens : int\n        The total number of tokens used in the step.\n    total_prompt_tokens : int\n        The cumulative number of prompt tokens used up to this step.\n    total_completion_tokens : int\n        The cumulative number of completion tokens used up to this step.\n    total_tokens : int\n        The cumulative total number of tokens used up to this step.\n    \"\"\"\n\n    \"\"\"\n    Represents token usage statistics for a conversation step.\n    \"\"\"\n\n    step_name: str\n    in_step_prompt_tokens: int\n    in_step_completion_tokens: int\n    in_step_total_tokens: int\n    total_prompt_tokens: int\n    total_completion_tokens: int\n    total_tokens: int\n\n\nclass Tokenizer:\n    \"\"\"\n    Tokenizer for counting tokens in text.\n    \"\"\"\n\n    def __init__(self, model_name):\n        self.model_name = model_name\n        self._tiktoken_tokenizer = (\n            tiktoken.encoding_for_model(model_name)\n            if \"gpt-4\" in model_name or \"gpt-3.5\" in model_name\n            else tiktoken.get_encoding(\"cl100k_base\")\n        )\n\n    def num_tokens(self, txt: str) -> int:\n        \"\"\"\n        Get the number of tokens in a text.\n\n        Parameters\n        ----------\n        txt : str\n            The text to count the tokens in.\n\n        Returns\n        -------\n        int\n            The number of tokens in the text.\n        \"\"\"\n        return len(self._tiktoken_tokenizer.encode(txt))\n\n    def num_tokens_for_base64_image(\n        self, image_base64: str, detail: str = \"high\"\n    ) -> int:\n        \"\"\"\n        Calculate the token size for a base64 encoded image based on OpenAI's token calculation rules.\n\n        Parameters:\n        - image_base64 (str): The base64 encoded string of the image.\n        - detail (str): The detail level of the image, 'low' or 'high'.\n\n        Returns:\n        - int: The token size of the image.\n        \"\"\"\n\n        if detail == \"low\":\n            return 85  # Fixed cost for low detail images\n\n        # Decode image from base64\n        image_data = base64.b64decode(image_base64)\n\n        # Convert byte data to image for size extraction\n        image = Image.open(io.BytesIO(image_data))\n\n        # Calculate the initial scale to fit within 2048 square while maintaining aspect ratio\n        max_dimension = max(image.size)\n        scale_factor = min(2048 / max_dimension, 1)  # Ensure we don't scale up\n        new_width = int(image.size[0] * scale_factor)\n        new_height = int(image.size[1] * scale_factor)\n\n        # Scale such that the shortest side is 768px\n        shortest_side = min(new_width, new_height)\n        if shortest_side > 768:\n            resize_factor = 768 / shortest_side\n            new_width = int(new_width * resize_factor)\n            new_height = int(new_height * resize_factor)\n\n        # Calculate the number of 512px tiles needed\n        width_tiles = math.ceil(new_width / 512)\n        height_tiles = math.ceil(new_height / 512)\n        total_tiles = width_tiles * height_tiles\n\n        # Each tile costs 170 tokens, plus a base cost of 85 tokens for high detail\n        token_cost = total_tiles * 170 + 85\n\n        return token_cost\n\n    def num_tokens_from_messages(self, messages: List[Message]) -> int:\n        \"\"\"\n        Get the total number of tokens used by a list of messages, accounting for text and base64 encoded images.\n\n        Parameters\n        ----------\n        messages : List[Message]\n            The list of messages to count the tokens in.\n\n        Returns\n        -------\n        int\n            The total number of tokens used by the messages.\n        \"\"\"\n        n_tokens = 0\n        for message in messages:\n            n_tokens += 4  # Account for message framing tokens\n\n            if isinstance(message.content, str):\n                # Content is a simple string\n                n_tokens += self.num_tokens(message.content)\n            elif isinstance(message.content, list):\n                # Content is a list, potentially mixed with text and images\n                for item in message.content:\n                    if item.get(\"type\") == \"text\":\n                        n_tokens += self.num_tokens(item[\"text\"])\n                    elif item.get(\"type\") == \"image_url\":\n                        image_detail = item[\"image_url\"].get(\"detail\", \"high\")\n                        image_base64 = item[\"image_url\"].get(\"url\")\n                        n_tokens += self.num_tokens_for_base64_image(\n                            image_base64, detail=image_detail\n                        )\n\n            n_tokens += 2  # Account for assistant's reply framing tokens\n\n        return n_tokens\n\n\nclass TokenUsageLog:\n    \"\"\"\n    Represents a log of token usage statistics for a conversation.\n    \"\"\"\n\n    def __init__(self, model_name):\n        self.model_name = model_name\n        self._cumulative_prompt_tokens = 0\n        self._cumulative_completion_tokens = 0\n        self._cumulative_total_tokens = 0\n        self._log = []\n        self._tokenizer = Tokenizer(model_name)\n\n    def update_log(self, messages: List[Message], answer: str, step_name: str) -> None:\n        \"\"\"\n        Update the token usage log with the number of tokens used in the current step.\n\n        Parameters\n        ----------\n        messages : List[Message]\n            The list of messages in the conversation.\n        answer : str\n            The answer from the AI.\n        step_name : str\n            The name of the step.\n        \"\"\"\n        prompt_tokens = self._tokenizer.num_tokens_from_messages(messages)\n        completion_tokens = self._tokenizer.num_tokens(answer)\n        total_tokens = prompt_tokens + completion_tokens\n\n        self._cumulative_prompt_tokens += prompt_tokens\n        self._cumulative_completion_tokens += completion_tokens\n        self._cumulative_total_tokens += total_tokens\n\n        self._log.append(\n            TokenUsage(\n                step_name=step_name,\n                in_step_prompt_tokens=prompt_tokens,\n                in_step_completion_tokens=completion_tokens,\n                in_step_total_tokens=total_tokens,\n                total_prompt_tokens=self._cumulative_prompt_tokens,\n                total_completion_tokens=self._cumulative_completion_tokens,\n                total_tokens=self._cumulative_total_tokens,\n            )\n        )\n\n    def log(self) -> List[TokenUsage]:\n        \"\"\"\n        Get the token usage log.\n\n        Returns\n        -------\n        List[TokenUsage]\n            A log of token usage details per step in the conversation.\n        \"\"\"\n        return self._log\n\n    def format_log(self) -> str:\n        \"\"\"\n        Format the token usage log as a CSV string.\n\n        Returns\n        -------\n        str\n            The token usage log formatted as a CSV string.\n        \"\"\"\n        result = \"step_name,prompt_tokens_in_step,completion_tokens_in_step,total_tokens_in_step,total_prompt_tokens,total_completion_tokens,total_tokens\\n\"\n        for log in self._log:\n            result += f\"{log.step_name},{log.in_step_prompt_tokens},{log.in_step_completion_tokens},{log.in_step_total_tokens},{log.total_prompt_tokens},{log.total_completion_tokens},{log.total_tokens}\\n\"\n        return result\n\n    def is_openai_model(self) -> bool:\n        \"\"\"\n        Check if the model is an OpenAI model.\n\n        Returns\n        -------\n        bool\n            True if the model is an OpenAI model, False otherwise.\n        \"\"\"\n        return \"gpt\" in self.model_name.lower()\n\n    def total_tokens(self) -> int:\n        \"\"\"\n        Return the total number of tokens used in the conversation.\n\n        Returns\n        -------\n        int\n            The total number of tokens used in the conversation.\n        \"\"\"\n        return self._cumulative_total_tokens\n\n    def usage_cost(self) -> float | None:\n        \"\"\"\n        Return the total cost in USD of the API usage.\n\n        Returns\n        -------\n        float\n            Cost in USD.\n        \"\"\"\n        if not self.is_openai_model():\n            return None\n\n        try:\n            result = 0\n            for log in self.log():\n                result += get_openai_token_cost_for_model(\n                    self.model_name, log.total_prompt_tokens, is_completion=False\n                )\n                result += get_openai_token_cost_for_model(\n                    self.model_name, log.total_completion_tokens, is_completion=True\n                )\n            return result\n        except Exception as e:\n            print(f\"Error calculating usage cost: {e}\")\n            return None\n", "gpt_engineer/core/project_config.py": "\"\"\"\nFunctions for reading and writing the `gpt-engineer.toml` configuration file.\n\nThe `gpt-engineer.toml` file is a TOML file that contains project-specific configuration used by the GPT Engineer CLI and gptengineer.app.\n\"\"\"\nfrom dataclasses import asdict, dataclass, field\nfrom pathlib import Path\n\nimport tomlkit\n\ndefault_config_filename = \"gpt-engineer.toml\"\n\nexample_config = \"\"\"\n[run]\nbuild = \"npm run build\"\ntest = \"npm run test\"\nlint = \"quick-lint-js\"\n\n[paths]\nbase = \"./frontend\"  # base directory to operate in (for monorepos)\nsrc = \"./src\"        # source directory (under the base directory) from which context will be retrieved\n\n[gptengineer-app]  # this namespace is used for gptengineer.app, may be used for internal experiments\nproject_id = \"...\"\n\n# we support multiple OpenAPI schemas, used as context for the LLM\nopenapi = [\n    { url = \"https://api.gptengineer.app/openapi.json\" },\n    { url = \"https://some-color-translating-api/openapi.json\" },\n]\n\"\"\"\n\n\n@dataclass\nclass _PathsConfig:\n    base: str | None = None\n    src: str | None = None\n\n\n@dataclass\nclass _RunConfig:\n    build: str | None = None\n    test: str | None = None\n    lint: str | None = None\n    format: str | None = None\n\n\n@dataclass\nclass _OpenApiConfig:\n    url: str\n\n\n@dataclass\nclass _GptEngineerAppConfig:\n    project_id: str\n    openapi: list[_OpenApiConfig] | None = None\n\n\ndef filter_none(d: dict) -> dict:\n    # Drop None values and empty dictionaries from a dictionary\n    return {\n        k: v\n        for k, v in (\n            (k, filter_none(v) if isinstance(v, dict) else v)\n            for k, v in d.items()\n            if v is not None\n        )\n        if not (isinstance(v, dict) and not v)  # Check for non-empty after filtering\n    }\n\n\n@dataclass\nclass Config:\n    \"\"\"Configuration for the GPT Engineer CLI and gptengineer.app via `gpt-engineer.toml`.\"\"\"\n\n    paths: _PathsConfig = field(default_factory=_PathsConfig)\n    run: _RunConfig = field(default_factory=_RunConfig)\n    gptengineer_app: _GptEngineerAppConfig | None = None\n\n    @classmethod\n    def from_toml(cls, config_file: Path | str):\n        if isinstance(config_file, str):\n            config_file = Path(config_file)\n        config_dict = read_config(config_file)\n        return cls.from_dict(config_dict)\n\n    @classmethod\n    def from_dict(cls, config_dict: dict):\n        run = _RunConfig(**config_dict.get(\"run\", {}))\n        paths = _PathsConfig(**config_dict.get(\"paths\", {}))\n\n        # load optional gptengineer-app section\n        gptengineer_app_dict = config_dict.get(\"gptengineer-app\", {})\n        gptengineer_app = None\n        if gptengineer_app_dict:\n            assert (\n                \"project_id\" in gptengineer_app_dict\n            ), \"project_id is required in gptengineer-app section\"\n            gptengineer_app = _GptEngineerAppConfig(\n                # required if gptengineer-app section is present\n                project_id=gptengineer_app_dict[\"project_id\"],\n                openapi=[\n                    _OpenApiConfig(**openapi)\n                    for openapi in gptengineer_app_dict.get(\"openapi\", [])\n                ]\n                or None,\n            )\n\n        return cls(paths=paths, run=run, gptengineer_app=gptengineer_app)\n\n    def to_dict(self) -> dict:\n        d = asdict(self)\n        d[\"gptengineer-app\"] = d.pop(\"gptengineer_app\", None)\n\n        # Drop None values and empty dictionaries\n        # Needed because tomlkit.dumps() doesn't handle None values,\n        # and we don't want to write empty sections.\n        d = filter_none(d)\n\n        return d\n\n    def to_toml(self, config_file: Path | str, save=True) -> str:\n        \"\"\"Write the configuration to a TOML file.\"\"\"\n        if isinstance(config_file, str):\n            config_file = Path(config_file)\n\n        # Load the TOMLDocument and overwrite it with the new values\n        config = read_config(config_file)\n        default_config = Config().to_dict()\n        for k, v in self.to_dict().items():\n            # only write values that are already explicitly set, or that differ from defaults\n            if k in config or v != default_config[k]:\n                if isinstance(v, dict):\n                    config[k] = {\n                        k2: v2\n                        for k2, v2 in v.items()\n                        if (\n                            k2 in config[k]\n                            or default_config.get(k) is None\n                            or v2 != default_config[k].get(k2)\n                        )\n                    }\n                else:\n                    config[k] = v\n\n        toml_str = tomlkit.dumps(config)\n        if save:\n            with open(config_file, \"w\") as f:\n                f.write(toml_str)\n\n        return toml_str\n\n\ndef read_config(config_file: Path) -> tomlkit.TOMLDocument:\n    \"\"\"Read the configuration file\"\"\"\n    assert config_file.exists(), f\"Config file {config_file} does not exist\"\n    with open(config_file, \"r\") as f:\n        return tomlkit.load(f)\n", "gpt_engineer/core/base_agent.py": "\"\"\"\nBase Agent Module\n\nThis module provides an abstract base class for an agent that interacts with code. It defines the interface\nfor agents capable of initializing and improving code based on a given prompt. Implementations of this class\nare expected to provide concrete methods for these actions.\n\nClasses:\n    BaseAgent: Abstract base class for an agent that interacts with code.\n\"\"\"\nfrom abc import ABC, abstractmethod\n\nfrom gpt_engineer.core.files_dict import FilesDict\nfrom gpt_engineer.core.prompt import Prompt\n\n\nclass BaseAgent(ABC):\n    \"\"\"\n    Abstract base class for an agent that interacts with code.\n\n    Defines the interface for agents capable of initializing and improving code based on a given prompt.\n    Implementations of this class are expected to provide concrete methods for these actions.\n    \"\"\"\n\n    @abstractmethod\n    def init(self, prompt: Prompt) -> FilesDict:\n        pass\n\n    @abstractmethod\n    def improve(self, files_dict: FilesDict, prompt: Prompt) -> FilesDict:\n        pass\n", "gpt_engineer/core/default/file_store.py": "import tempfile\n\nfrom pathlib import Path\nfrom typing import Union\n\nfrom gpt_engineer.core.files_dict import FilesDict\nfrom gpt_engineer.core.linting import Linting\n\n\nclass FileStore:\n    \"\"\"\n    Module for managing file storage in a temporary directory.\n\n    This module provides a class that manages the storage of files in a temporary directory.\n    It includes methods for uploading files to the directory and downloading them as a\n    collection of files.\n\n    Classes\n    -------\n    FileStore\n        Manages file storage in a temporary directory, allowing for upload and download of files.\n\n    Imports\n    -------\n    - tempfile: For creating temporary directories.\n    - Path: For handling file system paths.\n    - Union: For type annotations.\n    - FilesDict: For handling collections of files.\n    \"\"\"\n\n    def __init__(self, path: Union[str, Path, None] = None):\n        if path is None:\n            path = Path(tempfile.mkdtemp(prefix=\"gpt-engineer-\"))\n\n        self.working_dir = Path(path)\n        self.working_dir.mkdir(parents=True, exist_ok=True)\n        self.id = self.working_dir.name.split(\"-\")[-1]\n\n    def push(self, files: FilesDict):\n        for name, content in files.items():\n            path = self.working_dir / name\n            path.parent.mkdir(parents=True, exist_ok=True)\n            with open(path, \"w\") as f:\n                f.write(content)\n        return self\n\n    def linting(self, files: FilesDict) -> FilesDict:\n        # lint the code\n        linting = Linting()\n        return linting.lint_files(files)\n\n    def pull(self) -> FilesDict:\n        files = {}\n        for path in self.working_dir.glob(\"**/*\"):\n            if path.is_file():\n                with open(path, \"r\") as f:\n                    try:\n                        content = f.read()\n                    except UnicodeDecodeError:\n                        content = \"binary file\"\n                    files[str(path.relative_to(self.working_dir))] = content\n        return FilesDict(files)\n", "gpt_engineer/core/default/steps.py": "\"\"\"\nModule for defining the steps involved in generating and improving code using AI.\n\nThis module provides functions that represent different steps in the process of generating\nand improving code using an AI model. These steps include generating code from a prompt,\ncreating an entrypoint for the codebase, executing the entrypoint, and refining code edits.\n\nFunctions\n---------\ncurr_fn : function\n    Returns the name of the current function.\n\nsetup_sys_prompt : function\n    Sets up the system prompt for generating code.\n\ngen_code : function\n    Generates code from a prompt using AI and returns the generated files.\n\ngen_entrypoint : function\n    Generates an entrypoint for the codebase and returns the entrypoint files.\n\nexecute_entrypoint : function\n    Executes the entrypoint of the codebase.\n\nsetup_sys_prompt_existing_code : function\n    Sets up the system prompt for improving existing code.\n\n\nimprove : function\n    Improves the code based on user input and returns the updated files.\n\"\"\"\n\nimport inspect\nimport io\nimport re\nimport sys\nimport traceback\n\nfrom pathlib import Path\nfrom typing import List, MutableMapping, Union\n\nfrom langchain.schema import HumanMessage, SystemMessage\nfrom termcolor import colored\n\nfrom gpt_engineer.core.ai import AI\nfrom gpt_engineer.core.base_execution_env import BaseExecutionEnv\nfrom gpt_engineer.core.base_memory import BaseMemory\nfrom gpt_engineer.core.chat_to_files import apply_diffs, chat_to_files_dict, parse_diffs\nfrom gpt_engineer.core.default.constants import MAX_EDIT_REFINEMENT_STEPS\nfrom gpt_engineer.core.default.paths import (\n    CODE_GEN_LOG_FILE,\n    DEBUG_LOG_FILE,\n    DIFF_LOG_FILE,\n    ENTRYPOINT_FILE,\n    ENTRYPOINT_LOG_FILE,\n    IMPROVE_LOG_FILE,\n)\nfrom gpt_engineer.core.files_dict import FilesDict, file_to_lines_dict\nfrom gpt_engineer.core.preprompts_holder import PrepromptsHolder\nfrom gpt_engineer.core.prompt import Prompt\n\n\ndef curr_fn() -> str:\n    \"\"\"\n    Returns the name of the current function.\n\n    Returns\n    -------\n    str\n        The name of the function that called this function.\n    \"\"\"\n    return inspect.stack()[1].function\n\n\ndef setup_sys_prompt(preprompts: MutableMapping[Union[str, Path], str]) -> str:\n    \"\"\"\n    Sets up the system prompt for generating code.\n\n    Parameters\n    ----------\n    preprompts : MutableMapping[Union[str, Path], str]\n        A mapping of preprompt messages to guide the AI model.\n\n    Returns\n    -------\n    str\n        The system prompt message for the AI model.\n    \"\"\"\n    return (\n        preprompts[\"roadmap\"]\n        + preprompts[\"generate\"].replace(\"FILE_FORMAT\", preprompts[\"file_format\"])\n        + \"\\nUseful to know:\\n\"\n        + preprompts[\"philosophy\"]\n    )\n\n\ndef setup_sys_prompt_existing_code(\n    preprompts: MutableMapping[Union[str, Path], str]\n) -> str:\n    \"\"\"\n    Sets up the system prompt for improving existing code.\n\n    Parameters\n    ----------\n    preprompts : MutableMapping[Union[str, Path], str]\n        A mapping of preprompt messages to guide the AI model.\n\n    Returns\n    -------\n    str\n        The system prompt message for the AI model to improve existing code.\n    \"\"\"\n    return (\n        preprompts[\"roadmap\"]\n        + preprompts[\"improve\"].replace(\"FILE_FORMAT\", preprompts[\"file_format_diff\"])\n        + \"\\nUseful to know:\\n\"\n        + preprompts[\"philosophy\"]\n    )\n\n\ndef gen_code(\n    ai: AI, prompt: Prompt, memory: BaseMemory, preprompts_holder: PrepromptsHolder\n) -> FilesDict:\n    \"\"\"\n    Generates code from a prompt using AI and returns the generated files.\n\n    Parameters\n    ----------\n    ai : AI\n        The AI model used for generating code.\n    prompt : str\n        The user prompt to generate code from.\n    memory : BaseMemory\n        The memory interface where the code and related data are stored.\n    preprompts_holder : PrepromptsHolder\n        The holder for preprompt messages that guide the AI model.\n\n    Returns\n    -------\n    FilesDict\n        A dictionary of file names to their respective source code content.\n    \"\"\"\n    preprompts = preprompts_holder.get_preprompts()\n    messages = ai.start(\n        setup_sys_prompt(preprompts), prompt.to_langchain_content(), step_name=curr_fn()\n    )\n    chat = messages[-1].content.strip()\n    memory.log(CODE_GEN_LOG_FILE, \"\\n\\n\".join(x.pretty_repr() for x in messages))\n    files_dict = chat_to_files_dict(chat)\n    return files_dict\n\n\ndef gen_entrypoint(\n    ai: AI,\n    prompt: Prompt,\n    files_dict: FilesDict,\n    memory: BaseMemory,\n    preprompts_holder: PrepromptsHolder,\n) -> FilesDict:\n    \"\"\"\n    Generates an entrypoint for the codebase and returns the entrypoint files.\n\n    Parameters\n    ----------\n    ai : AI\n        The AI model used for generating the entrypoint.\n    files_dict : FilesDict\n        The dictionary of file names to their respective source code content.\n    memory : BaseMemory\n        The memory interface where the code and related data are stored.\n    preprompts_holder : PrepromptsHolder\n        The holder for preprompt messages that guide the AI model.\n\n    Returns\n    -------\n    FilesDict\n        A dictionary containing the entrypoint file.\n    \"\"\"\n    user_prompt = prompt.entrypoint_prompt\n    if not user_prompt:\n        user_prompt = \"\"\"\n        Make a unix script that\n        a) installs dependencies\n        b) runs all necessary parts of the codebase (in parallel if necessary)\n        \"\"\"\n    preprompts = preprompts_holder.get_preprompts()\n    messages = ai.start(\n        system=(preprompts[\"entrypoint\"]),\n        user=user_prompt\n        + \"\\nInformation about the codebase:\\n\\n\"\n        + files_dict.to_chat(),\n        step_name=curr_fn(),\n    )\n    print()\n    chat = messages[-1].content.strip()\n    regex = r\"```\\S*\\n(.+?)```\"\n    matches = re.finditer(regex, chat, re.DOTALL)\n    entrypoint_code = FilesDict(\n        {ENTRYPOINT_FILE: \"\\n\".join(match.group(1) for match in matches)}\n    )\n    memory.log(ENTRYPOINT_LOG_FILE, \"\\n\\n\".join(x.pretty_repr() for x in messages))\n    return entrypoint_code\n\n\ndef execute_entrypoint(\n    ai: AI,\n    execution_env: BaseExecutionEnv,\n    files_dict: FilesDict,\n    prompt: Prompt = None,\n    preprompts_holder: PrepromptsHolder = None,\n    memory: BaseMemory = None,\n) -> FilesDict:\n    \"\"\"\n    Executes the entrypoint of the codebase.\n\n    Parameters\n    ----------\n    ai : AI\n        The AI model used for generating the entrypoint.\n    execution_env : BaseExecutionEnv\n        The execution environment in which the code is executed.\n    files_dict : FilesDict\n        The dictionary of file names to their respective source code content.\n    preprompts_holder : PrepromptsHolder, optional\n        The holder for preprompt messages that guide the AI model.\n\n    Returns\n    -------\n    FilesDict\n        The dictionary of file names to their respective source code content after execution.\n    \"\"\"\n    if ENTRYPOINT_FILE not in files_dict:\n        raise FileNotFoundError(\n            \"The required entrypoint \"\n            + ENTRYPOINT_FILE\n            + \" does not exist in the code.\"\n        )\n\n    command = files_dict[ENTRYPOINT_FILE]\n\n    print()\n    print(\n        colored(\n            \"Do you want to execute this code? (Y/n)\",\n            \"red\",\n        )\n    )\n    print()\n    print(command)\n    print()\n    if input(\"\").lower() not in [\"\", \"y\", \"yes\"]:\n        print(\"Ok, not executing the code.\")\n        return files_dict\n    print(\"Executing the code...\")\n    print()\n    print(\n        colored(\n            \"Note: If it does not work as expected, consider running the code\"\n            + \" in another way than above.\",\n            \"green\",\n        )\n    )\n    print()\n    print(\"You can press ctrl+c *once* to stop the execution.\")\n    print()\n\n    execution_env.upload(files_dict).run(f\"bash {ENTRYPOINT_FILE}\")\n    return files_dict\n\n\ndef improve_fn(\n    ai: AI,\n    prompt: Prompt,\n    files_dict: FilesDict,\n    memory: BaseMemory,\n    preprompts_holder: PrepromptsHolder,\n) -> FilesDict:\n    \"\"\"\n    Improves the code based on user input and returns the updated files.\n\n    Parameters\n    ----------\n    ai : AI\n        The AI model used for improving code.\n    prompt :str\n        The user prompt to improve the code.\n    files_dict : FilesDict\n        The dictionary of file names to their respective source code content.\n    memory : BaseMemory\n        The memory interface where the code and related data are stored.\n    preprompts_holder : PrepromptsHolder\n        The holder for preprompt messages that guide the AI model.\n\n    Returns\n    -------\n    FilesDict\n        The dictionary of file names to their respective updated source code content.\n    \"\"\"\n    preprompts = preprompts_holder.get_preprompts()\n    messages = [\n        SystemMessage(content=setup_sys_prompt_existing_code(preprompts)),\n    ]\n\n    # Add files as input\n    messages.append(HumanMessage(content=f\"{files_dict.to_chat()}\"))\n    messages.append(HumanMessage(content=prompt.to_langchain_content()))\n    memory.log(\n        DEBUG_LOG_FILE,\n        \"UPLOADED FILES:\\n\" + files_dict.to_log() + \"\\nPROMPT:\\n\" + prompt.text,\n    )\n    return _improve_loop(ai, files_dict, memory, messages)\n\n\ndef _improve_loop(\n    ai: AI, files_dict: FilesDict, memory: BaseMemory, messages: List\n) -> FilesDict:\n    messages = ai.next(messages, step_name=curr_fn())\n    files_dict, errors = salvage_correct_hunks(messages, files_dict, memory)\n\n    retries = 0\n    while errors and retries < MAX_EDIT_REFINEMENT_STEPS:\n        messages.append(\n            HumanMessage(\n                content=\"Some previously produced diffs were not on the requested format, or the code part was not found in the code. Details:\\n\"\n                + \"\\n\".join(errors)\n                + \"\\n Only rewrite the problematic diffs, making sure that the failing ones are now on the correct format and can be found in the code. Make sure to not repeat past mistakes. \\n\"\n            )\n        )\n        messages = ai.next(messages, step_name=curr_fn())\n        files_dict, errors = salvage_correct_hunks(messages, files_dict, memory)\n        retries += 1\n\n    return files_dict\n\n\ndef salvage_correct_hunks(\n    messages: List,\n    files_dict: FilesDict,\n    memory: BaseMemory,\n) -> tuple[FilesDict, List[str]]:\n    error_messages = []\n    ai_response = messages[-1].content.strip()\n\n    diffs = parse_diffs(ai_response)\n    # validate and correct diffs\n\n    for _, diff in diffs.items():\n        # if diff is a new file, validation and correction is unnecessary\n        if not diff.is_new_file():\n            problems = diff.validate_and_correct(\n                file_to_lines_dict(files_dict[diff.filename_pre])\n            )\n            error_messages.extend(problems)\n    files_dict = apply_diffs(diffs, files_dict)\n    memory.log(IMPROVE_LOG_FILE, \"\\n\\n\".join(x.pretty_repr() for x in messages))\n    memory.log(DIFF_LOG_FILE, \"\\n\\n\".join(error_messages))\n    return files_dict, error_messages\n\n\nclass Tee(object):\n    def __init__(self, *files):\n        self.files = files\n\n    def write(self, obj):\n        for file in self.files:\n            file.write(obj)\n\n    def flush(self):\n        for file in self.files:\n            file.flush()\n\n\ndef handle_improve_mode(prompt, agent, memory, files_dict):\n    captured_output = io.StringIO()\n    old_stdout = sys.stdout\n    sys.stdout = Tee(sys.stdout, captured_output)\n\n    try:\n        files_dict = agent.improve(files_dict, prompt)\n    except Exception as e:\n        print(\n            f\"Error while improving the project: {e}\\nCould you please upload the debug_log_file.txt in {memory.path}/logs folder to github?\\nFULL STACK TRACE:\\n\"\n        )\n        traceback.print_exc(file=sys.stdout)  # Print the full stack trace\n    finally:\n        # Reset stdout\n        sys.stdout = old_stdout\n\n        # Get the captured output\n        captured_string = captured_output.getvalue()\n        print(captured_string)\n        memory.log(DEBUG_LOG_FILE, \"\\nCONSOLE OUTPUT:\\n\" + captured_string)\n\n    return files_dict\n", "gpt_engineer/core/default/paths.py": "\"\"\"\nModule defining file system paths used by the application.\n\nThis module contains definitions of file system paths that are used throughout the\napplication to locate and manage various files and directories, such as logs, memory,\nand preprompts.\n\nConstants\n---------\nMETA_DATA_REL_PATH : str\n    The relative path to the directory where metadata is stored.\n\nMEMORY_REL_PATH : str\n    The relative path to the directory where memory-related files are stored.\n\nCODE_GEN_LOG_FILE : str\n    The filename for the log file that contains all output from code generation.\n\nDEBUG_LOG_FILE : str\n    The filename for the log file that contains debug information.\n\nENTRYPOINT_FILE : str\n    The filename for the entrypoint script that is executed to run the application.\n\nENTRYPOINT_LOG_FILE : str\n    The filename for the log file that contains the chat related to entrypoint generation.\n\nPREPROMPTS_PATH : Path\n    The file system path to the directory containing preprompt files.\n\nFunctions\n---------\nmemory_path : function\n    Constructs the full path to the memory directory based on a given base path.\n\nmetadata_path : function\n    Constructs the full path to the metadata directory based on a given base path.\n\"\"\"\nimport os\n\nfrom pathlib import Path\n\nMETA_DATA_REL_PATH = \".gpteng\"\nMEMORY_REL_PATH = os.path.join(META_DATA_REL_PATH, \"memory\")\nCODE_GEN_LOG_FILE = \"all_output.txt\"\nIMPROVE_LOG_FILE = \"improve.txt\"\nDIFF_LOG_FILE = \"diff_errors.txt\"\nDEBUG_LOG_FILE = \"debug_log_file.txt\"\nENTRYPOINT_FILE = \"run.sh\"\nENTRYPOINT_LOG_FILE = \"gen_entrypoint_chat.txt\"\nENTRYPOINT_FILE = \"run.sh\"\nPREPROMPTS_PATH = Path(__file__).parent.parent.parent / \"preprompts\"\n\n\ndef memory_path(path):\n    \"\"\"\n    Constructs the full path to the memory directory based on a given base path.\n\n    Parameters\n    ----------\n    path : str\n        The base path to append the memory directory to.\n\n    Returns\n    -------\n    str\n        The full path to the memory directory.\n    \"\"\"\n    return os.path.join(path, MEMORY_REL_PATH)\n\n\ndef metadata_path(path):\n    \"\"\"\n    Constructs the full path to the metadata directory based on a given base path.\n\n    Parameters\n    ----------\n    path : str\n        The base path to append the metadata directory to.\n\n    Returns\n    -------\n    str\n        The full path to the metadata directory.\n    \"\"\"\n    return os.path.join(path, META_DATA_REL_PATH)\n", "gpt_engineer/core/default/disk_memory.py": "\"\"\"\nDisk Memory Module\n==================\n\nThis module provides a simple file-based key-value database system, where keys are\nrepresented as filenames and values are the contents of these files. The `DiskMemory` class\nis responsible for the CRUD operations on the database.\n\nAttributes\n----------\nNone\n\nFunctions\n---------\nNone\n\nClasses\n-------\nDiskMemory\n    A file-based key-value store where keys correspond to filenames and values to file contents.\n\"\"\"\n\nimport base64\nimport json\nimport shutil\n\nfrom datetime import datetime\nfrom pathlib import Path\nfrom typing import Any, Dict, Iterator, Optional, Union\n\nfrom gpt_engineer.core.base_memory import BaseMemory\nfrom gpt_engineer.tools.supported_languages import SUPPORTED_LANGUAGES\n\n\n# This class represents a simple database that stores its tools as files in a directory.\nclass DiskMemory(BaseMemory):\n    \"\"\"\n    A file-based key-value store where keys correspond to filenames and values to file contents.\n\n    This class provides an interface to a file-based database, leveraging file operations to\n    facilitate CRUD-like interactions. It allows for quick checks on the existence of keys,\n    retrieval of values based on keys, and setting new key-value pairs.\n\n    Attributes\n    ----------\n    path : Path\n        The directory path where the database files are stored.\n    \"\"\"\n\n    def __init__(self, path: Union[str, Path]):\n        \"\"\"\n        Initialize the DiskMemory class with a specified path.\n\n        Parameters\n        ----------\n        path : str or Path\n            The path to the directory where the database files will be stored.\n\n        \"\"\"\n        self.path: Path = Path(path).absolute()\n\n        self.path.mkdir(parents=True, exist_ok=True)\n\n    def __contains__(self, key: str) -> bool:\n        \"\"\"\n        Determine whether the database contains a file with the specified key.\n\n        Parameters\n        ----------\n        key : str\n            The key (filename) to check for existence in the database.\n\n        Returns\n        -------\n        bool\n            Returns True if the file exists, False otherwise.\n\n        \"\"\"\n        return (self.path / key).is_file()\n\n    def __getitem__(self, key: str) -> str:\n        \"\"\"\n        Retrieve the content of a file in the database corresponding to the given key.\n        If the file is an image with a .png or .jpeg extension, it returns the content\n        in Base64-encoded string format.\n\n        Parameters\n        ----------\n        key : str\n            The key (filename) whose content is to be retrieved.\n\n        Returns\n        -------\n        str\n            The content of the file associated with the key, or Base64-encoded string if it's a .png or .jpeg file.\n\n        Raises\n        ------\n        KeyError\n            If the file corresponding to the key does not exist in the database.\n        \"\"\"\n        full_path = self.path / key\n\n        if not full_path.is_file():\n            raise KeyError(f\"File '{key}' could not be found in '{self.path}'\")\n\n        if full_path.suffix in [\".png\", \".jpeg\", \".jpg\"]:\n            with full_path.open(\"rb\") as image_file:\n                encoded_string = base64.b64encode(image_file.read()).decode(\"utf-8\")\n                mime_type = \"image/png\" if full_path.suffix == \".png\" else \"image/jpeg\"\n                return f\"data:{mime_type};base64,{encoded_string}\"\n        else:\n            with full_path.open(\"r\", encoding=\"utf-8\") as f:\n                return f.read()\n\n    def get(self, key: str, default: Optional[Any] = None) -> Any:\n        \"\"\"\n        Retrieve the content of a file in the database, or return a default value if not found.\n\n        Parameters\n        ----------\n        key : str\n            The key (filename) whose content is to be retrieved.\n        default : Any, optional\n            The default value to return if the file does not exist. Default is None.\n\n        Returns\n        -------\n        Any\n            The content of the file if it exists, a new DiskMemory instance if the key corresponds to a directory.\n        \"\"\"\n\n        item_path = self.path / key\n        try:\n            if item_path.is_file():\n                return self[key]\n            elif item_path.is_dir():\n                return DiskMemory(item_path)\n            else:\n                return default\n        except:\n            return default\n\n    def __setitem__(self, key: Union[str, Path], val: str) -> None:\n        \"\"\"\n        Set or update the content of a file in the database corresponding to the given key.\n\n        Parameters\n        ----------\n        key : str or Path\n            The key (filename) where the content is to be set.\n        val : str\n            The content to be written to the file.\n\n        Raises\n        ------\n        ValueError\n            If the key attempts to access a parent path.\n        TypeError\n            If the value is not a string.\n\n        \"\"\"\n        if str(key).startswith(\"../\"):\n            raise ValueError(f\"File name {key} attempted to access parent path.\")\n\n        if not isinstance(val, str):\n            raise TypeError(\"val must be str\")\n\n        full_path = self.path / key\n        full_path.parent.mkdir(parents=True, exist_ok=True)\n\n        full_path.write_text(val, encoding=\"utf-8\")\n\n    def __delitem__(self, key: Union[str, Path]) -> None:\n        \"\"\"\n        Delete a file or directory from the database corresponding to the given key.\n\n        Parameters\n        ----------\n        key : str or Path\n            The key (filename or directory name) to be deleted.\n\n        Raises\n        ------\n        KeyError\n            If the file or directory corresponding to the key does not exist in the database.\n\n        \"\"\"\n        item_path = self.path / key\n        if not item_path.exists():\n            raise KeyError(f\"Item '{key}' could not be found in '{self.path}'\")\n\n        if item_path.is_file():\n            item_path.unlink()\n        elif item_path.is_dir():\n            shutil.rmtree(item_path)\n\n    def __iter__(self) -> Iterator[str]:\n        \"\"\"\n        Iterate over the keys (filenames) in the database.\n\n        Yields\n        ------\n        Iterator[str]\n            An iterator over the sorted list of keys (filenames) in the database.\n\n        \"\"\"\n        return iter(\n            sorted(\n                str(item.relative_to(self.path))\n                for item in sorted(self.path.rglob(\"*\"))\n                if item.is_file()\n            )\n        )\n\n    def __len__(self) -> int:\n        \"\"\"\n        Get the number of files in the database.\n\n        Returns\n        -------\n        int\n            The number of files in the database.\n\n        \"\"\"\n        return len(list(self.__iter__()))\n\n    def _supported_files(self) -> str:\n        valid_extensions = {\n            ext for lang in SUPPORTED_LANGUAGES for ext in lang[\"extensions\"]\n        }\n        file_paths = [\n            str(item)\n            for item in self\n            if Path(item).is_file() and Path(item).suffix in valid_extensions\n        ]\n        return \"\\n\".join(file_paths)\n\n    def _all_files(self) -> str:\n        file_paths = [str(item) for item in self if Path(item).is_file()]\n        return \"\\n\".join(file_paths)\n\n    def to_path_list_string(self, supported_code_files_only: bool = False) -> str:\n        \"\"\"\n        Generate a string representation of the file paths in the database.\n\n        Parameters\n        ----------\n        supported_code_files_only : bool, optional\n            If True, filter the list to include only supported code file extensions.\n            Default is False.\n\n        Returns\n        -------\n        str\n            A newline-separated string of file paths.\n\n        \"\"\"\n        if supported_code_files_only:\n            return self._supported_files()\n        else:\n            return self._all_files()\n\n    def to_dict(self) -> Dict[Union[str, Path], str]:\n        \"\"\"\n        Convert the database contents to a dictionary.\n\n        Returns\n        -------\n        Dict[Union[str, Path], str]\n            A dictionary with keys as filenames and values as file contents.\n\n        \"\"\"\n        return {file_path: self[file_path] for file_path in self}\n\n    def to_json(self) -> str:\n        \"\"\"\n        Serialize the database contents to a JSON string.\n\n        Returns\n        -------\n        str\n            A JSON string representation of the database contents.\n\n        \"\"\"\n        return json.dumps(self.to_dict())\n\n    def log(self, key: Union[str, Path], val: str) -> None:\n        \"\"\"\n        Append to a file or create and write to it if it doesn't exist.\n\n        Parameters\n        ----------\n        key : str or Path\n            The key (filename) where the content is to be appended.\n        val : str\n            The content to be appended to the file.\n\n        \"\"\"\n\n        if str(key).startswith(\"../\"):\n            raise ValueError(f\"File name {key} attempted to access parent path.\")\n\n        if not isinstance(val, str):\n            raise TypeError(\"val must be str\")\n\n        full_path = self.path / \"logs\" / key\n        full_path.parent.mkdir(parents=True, exist_ok=True)\n\n        # Touch if it doesnt exist\n        if not full_path.exists():\n            full_path.touch()\n\n        with open(full_path, \"a\", encoding=\"utf-8\") as file:\n            file.write(f\"\\n{datetime.now().isoformat()}\\n\")\n            file.write(val + \"\\n\")\n\n    def archive_logs(self):\n        \"\"\"\n        Moves all logs to archive directory based on current timestamp\n        \"\"\"\n        if \"logs\" in self:\n            archive_dir = (\n                self.path / f\"logs_{datetime.now().strftime('%Y-%m-%d-%H-%M-%S')}\"\n            )\n            shutil.move(self.path / \"logs\", archive_dir)\n", "gpt_engineer/core/default/disk_execution_env.py": "\"\"\"\nModule for managing the execution environment on the local disk.\n\nThis module provides a class that handles the execution of code stored on the local\nfile system. It includes methods for uploading files to the execution environment,\nrunning commands, and capturing the output.\n\nClasses\n-------\nDiskExecutionEnv\n    An execution environment that runs code on the local file system and captures\n    the output of the execution.\n\nImports\n-------\n- subprocess: For running shell commands.\n- time: For timing the execution of commands.\n- Path: For handling file system paths.\n- Optional, Tuple, Union: For type annotations.\n- BaseExecutionEnv: For inheriting the base execution environment interface.\n- FileStore: For managing file storage.\n- FilesDict: For handling collections of files.\n\"\"\"\n\nimport subprocess\nimport time\n\nfrom pathlib import Path\nfrom typing import Optional, Tuple, Union\n\nfrom gpt_engineer.core.base_execution_env import BaseExecutionEnv\nfrom gpt_engineer.core.default.file_store import FileStore\nfrom gpt_engineer.core.files_dict import FilesDict\n\n\nclass DiskExecutionEnv(BaseExecutionEnv):\n    \"\"\"\n    An execution environment that runs code on the local file system and captures\n    the output of the execution.\n\n    This class is responsible for executing code that is stored on disk. It ensures that\n    the necessary entrypoint file exists and then runs the code using a subprocess. If the\n    execution is interrupted by the user, it handles the interruption gracefully.\n\n    Attributes\n    ----------\n    store : FileStore\n        An instance of FileStore that manages the storage of files in the execution\n        environment.\n    \"\"\"\n\n    def __init__(self, path: Union[str, Path, None] = None):\n        self.files = FileStore(path)\n\n    def upload(self, files: FilesDict) -> \"DiskExecutionEnv\":\n        self.files.push(files)\n        return self\n\n    def download(self) -> FilesDict:\n        return self.files.pull()\n\n    def popen(self, command: str) -> subprocess.Popen:\n        p = subprocess.Popen(\n            command,\n            shell=True,\n            cwd=self.files.working_dir,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n        )\n        return p\n\n    def run(self, command: str, timeout: Optional[int] = None) -> Tuple[str, str, int]:\n        start = time.time()\n        print(\"\\n--- Start of run ---\")\n        # while running, also print the stdout and stderr\n        p = subprocess.Popen(\n            command,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            cwd=self.files.working_dir,\n            text=True,\n            shell=True,\n        )\n        print(\"$\", command)\n        stdout_full, stderr_full = \"\", \"\"\n\n        try:\n            while p.poll() is None:\n                assert p.stdout is not None\n                assert p.stderr is not None\n                stdout = p.stdout.readline()\n                stderr = p.stderr.readline()\n                if stdout:\n                    print(stdout, end=\"\")\n                    stdout_full += stdout\n                if stderr:\n                    print(stderr, end=\"\")\n                    stderr_full += stderr\n                if timeout and time.time() - start > timeout:\n                    print(\"Timeout!\")\n                    p.kill()\n                    raise TimeoutError()\n        except KeyboardInterrupt:\n            print()\n            print(\"Stopping execution.\")\n            print(\"Execution stopped.\")\n            p.kill()\n            print()\n            print(\"--- Finished run ---\\n\")\n\n        return stdout_full, stderr_full, p.returncode\n", "gpt_engineer/core/default/constants.py": "\"\"\"\nModule defining constants used throughout the application.\n\nThis module contains definitions of constants that are used across various\ncomponents of the application to maintain consistency and ease of configuration.\n\nConstants\n---------\nMAX_EDIT_REFINEMENT_STEPS : int\n    The maximum number of refinement steps allowed when generating edit blocks.\n\"\"\"\nMAX_EDIT_REFINEMENT_STEPS = 2\n", "gpt_engineer/core/default/simple_agent.py": "\"\"\"\nModule for defining a simple agent that uses AI to manage code generation and improvement.\n\nThis module provides a class that represents an agent capable of initializing and improving\na codebase using AI. It handles interactions with the AI model, memory, and execution\nenvironment to generate and refine code based on user prompts.\n\n\"\"\"\n\nimport tempfile\n\nfrom typing import Optional\n\nfrom gpt_engineer.core.ai import AI\nfrom gpt_engineer.core.base_agent import BaseAgent\nfrom gpt_engineer.core.base_execution_env import BaseExecutionEnv\nfrom gpt_engineer.core.base_memory import BaseMemory\nfrom gpt_engineer.core.default.disk_execution_env import DiskExecutionEnv\nfrom gpt_engineer.core.default.disk_memory import DiskMemory\nfrom gpt_engineer.core.default.paths import PREPROMPTS_PATH, memory_path\nfrom gpt_engineer.core.default.steps import gen_code, gen_entrypoint, improve_fn\nfrom gpt_engineer.core.files_dict import FilesDict\nfrom gpt_engineer.core.preprompts_holder import PrepromptsHolder\nfrom gpt_engineer.core.prompt import Prompt\n\n\nclass SimpleAgent(BaseAgent):\n    \"\"\"\n    An agent that uses AI to generate and improve code based on a given prompt.\n\n    This agent is capable of initializing a codebase from a prompt and improving an existing\n    codebase based on user input. It uses an AI model to generate and refine code, and it\n    interacts with a repository and an execution environment to manage and execute the code.\n\n    Attributes\n    ----------\n    memory : BaseMemory\n        The memory interface where the code and related data are stored.\n    execution_env : BaseExecutionEnv\n        The execution environment in which the code is executed.\n    ai : AI\n        The AI model used for generating and improving code.\n    preprompts_holder : PrepromptsHolder\n        The holder for preprompt messages that guide the AI model.\n    \"\"\"\n\n    def __init__(\n        self,\n        memory: BaseMemory,\n        execution_env: BaseExecutionEnv,\n        ai: AI = None,\n        preprompts_holder: PrepromptsHolder = None,\n    ):\n        self.preprompts_holder = preprompts_holder or PrepromptsHolder(PREPROMPTS_PATH)\n        self.memory = memory\n        self.execution_env = execution_env\n        self.ai = ai or AI()\n\n    @classmethod\n    def with_default_config(\n        cls, path: str, ai: AI = None, preprompts_holder: PrepromptsHolder = None\n    ):\n        return cls(\n            memory=DiskMemory(memory_path(path)),\n            execution_env=DiskExecutionEnv(),\n            ai=ai,\n            preprompts_holder=preprompts_holder or PrepromptsHolder(PREPROMPTS_PATH),\n        )\n\n    def init(self, prompt: Prompt) -> FilesDict:\n        files_dict = gen_code(self.ai, prompt, self.memory, self.preprompts_holder)\n        entrypoint = gen_entrypoint(\n            self.ai, prompt, files_dict, self.memory, self.preprompts_holder\n        )\n        combined_dict = {**files_dict, **entrypoint}\n        files_dict = FilesDict(combined_dict)\n        return files_dict\n\n    def improve(\n        self,\n        files_dict: FilesDict,\n        prompt: Prompt,\n        execution_command: Optional[str] = None,\n    ) -> FilesDict:\n        files_dict = improve_fn(\n            self.ai, prompt, files_dict, self.memory, self.preprompts_holder\n        )\n        return files_dict\n\n\ndef default_config_agent():\n    \"\"\"\n    Creates an instance of SimpleAgent with default configuration.\n\n    Returns\n    -------\n    SimpleAgent\n        An instance of SimpleAgent with a temporary directory as its base path.\n    \"\"\"\n    return SimpleAgent.with_default_config(tempfile.mkdtemp())\n", "gpt_engineer/core/default/__init__.py": "", "gpt_engineer/benchmark/run.py": "\"\"\"\nModule for running benchmarks.\n\nThis module defines functions to run benchmarks using a given agent and to print\nthe results of the benchmark tasks.\n\nFunctions\n---------\nrun : function\n    Runs the benchmark tasks using the provided agent and returns a list of TaskResult objects.\n\nprint_results : function\n    Prints the results of the benchmark tasks to the console.\n\"\"\"\nimport time\n\nfrom typing import List\n\nimport yaml\n\nfrom gpt_engineer.benchmark.types import Assertable, Benchmark, TaskResult\nfrom gpt_engineer.core.base_agent import BaseAgent\nfrom gpt_engineer.core.default.disk_execution_env import DiskExecutionEnv\n\n\ndef run(\n    agent: BaseAgent,\n    benchmark: Benchmark,\n    verbose=False,\n) -> List[TaskResult]:\n    \"\"\"\n    Runs the benchmark tasks using the provided agent and returns a list of TaskResult objects.\n\n    Parameters\n    ----------\n    agent : BaseAgent\n        The agent to use for running the benchmark tasks.\n    benchmark : Benchmark\n        The benchmark containing the tasks to run.\n    verbose : bool, default=False\n        A flag to indicate whether to print verbose output during the benchmark.\n\n    Returns\n    -------\n    List[TaskResult]\n        A list of TaskResult objects representing the results of the benchmark tasks.\n    \"\"\"\n    task_results = []\n    for task in benchmark.tasks:\n        print(f\"--> Running task: {task.name}\\n\")\n\n        t0 = time.time()\n        files_dict = agent.improve(task.initial_code, task.prompt)\n        t1 = time.time()\n\n        env = DiskExecutionEnv()\n        env.upload(files_dict)\n\n        if task.command:\n            p = env.popen(task.command)\n            stdout, stderr = p.communicate(benchmark.timeout)\n            stdout, stderr = stdout.decode(\"utf-8\"), stderr.decode(\"utf-8\")\n        else:\n            p, stdout, stderr = None, None, None\n\n        exec_result = Assertable(\n            files=files_dict,\n            env=env,\n            process=p,\n            stdout=stdout,\n            stderr=stderr,\n        )\n\n        task_results.append(\n            TaskResult(\n                task_name=task.name,\n                assertion_results={\n                    assertion_name: assertion(exec_result)\n                    for assertion_name, assertion in task.assertions.items()\n                },\n                duration=t1 - t0,\n            )\n        )\n\n        if verbose:\n            print_results(task_results)\n    return task_results\n\n\ndef print_results(results: list[TaskResult]):\n    \"\"\"\n    Prints the results of the benchmark tasks to the console.\n\n    Parameters\n    ----------\n    results : list[TaskResult]\n        A list of TaskResult objects representing the results of the benchmark tasks.\n\n    Returns\n    -------\n    None\n    \"\"\"\n    for task_result in results:\n        print(f\"\\n--- Results for {task_result.task_name} ---\")\n        print(f\"{task_result.task_name} ({task_result.duration:.2f}s)\")\n        for assertion_name, assertion_result in task_result.assertion_results.items():\n            checkmark = \"\u2705\" if assertion_result else \"\u274c\"\n            print(f\"  {checkmark} {assertion_name}\")\n        print()\n\n    success_rates = [task_result.success_rate for task_result in results]\n    avg_success_rate = sum(success_rates) / len(results)\n\n    total_time = sum(task_result.duration for task_result in results)\n\n    correct_assertions = sum(\n        sum(\n            assertion_result\n            for assertion_result in task_result.assertion_results.values()\n        )\n        for task_result in results\n    )\n    total_assertions = sum(\n        len(task_result.assertion_results) for task_result in results\n    )\n    correct_tasks = [\n        task_result for task_result in results if task_result.success_rate == 1\n    ]\n\n    print(\"--- Results ---\")\n    print(f\"Total time: {total_time:.2f}s\")\n    print(f\"Completely correct tasks: {len(correct_tasks)}/{len(results)}\")\n    print(f\"Total correct assertions: {correct_assertions}/{total_assertions}\")\n    print(f\"Average success rate: {avg_success_rate * 100}% on {len(results)} tasks\")\n    print(\"--- Results ---\")\n    print()\n\n\ndef export_yaml_results(yaml_path, complete_results, config):\n    for results in complete_results.values():\n        correct_tasks = [\n            task_result\n            for task_result in results[\"detailed\"]\n            if task_result[\"solved\"] == 1.0\n        ]\n        fraction_correct = len(correct_tasks) / len(results[\"detailed\"])\n        results[\"fully_solved\"] = fraction_correct\n    complete_results[\"config\"] = config\n    with open(yaml_path, \"w\") as f:\n        yaml.dump(complete_results, f, indent=4)\n", "gpt_engineer/benchmark/bench_config.py": "from dataclasses import dataclass, field\nfrom pathlib import Path\n\nfrom tomlkit.items import Integer\n\nfrom gpt_engineer.core.project_config import read_config\n\n\n@dataclass\nclass AppsConfig:\n    active: bool | None = True\n    test_start_index: int | None = 0\n    test_end_index: int | None = 1\n    train_start_index: int | None = 0\n    train_end_index: int | None = 0\n    examples_per_problem: int | None = 10\n\n\n@dataclass\nclass MbppConfig:\n    active: bool | None = True\n    test_len: int | None = 1\n    train_len: int | None = 0\n\n\n@dataclass\nclass GptmeConfig:\n    active: bool | None = True\n\n\n@dataclass\nclass BenchConfig:\n    \"\"\"Configuration for the GPT Engineer CLI and gptengineer.app via `gpt-engineer.toml`.\"\"\"\n\n    apps: AppsConfig = field(default_factory=AppsConfig)\n    mbpp: MbppConfig = field(default_factory=MbppConfig)\n    gptme: GptmeConfig = field(default_factory=GptmeConfig)\n\n    @classmethod\n    def from_toml(cls, config_file: Path | str):\n        if isinstance(config_file, str):\n            config_file = Path(config_file)\n        config_dict = read_config(config_file)\n        return cls.from_dict(config_dict)\n\n    @classmethod\n    def from_dict(cls, config_dict: dict):\n        return cls(\n            apps=AppsConfig(**config_dict.get(\"apps\", {})),\n            mbpp=MbppConfig(**config_dict.get(\"mbpp\", {})),\n            gptme=GptmeConfig(**config_dict.get(\"gptme\", {})),\n        )\n\n    @staticmethod\n    def recursive_resolve(data_dict):\n        for key, value in data_dict.items():\n            if isinstance(value, Integer):\n                data_dict[key] = int(value)\n            elif isinstance(value, dict):\n                BenchConfig.recursive_resolve(value)\n\n    def to_dict(self):\n        dict_config = {\n            benchmark_name: {key: val for key, val in spec_config.__dict__.items()}\n            for benchmark_name, spec_config in self.__dict__.items()\n        }\n        BenchConfig.recursive_resolve(dict_config)\n\n        return dict_config\n", "gpt_engineer/benchmark/types.py": "\"\"\"\nModule defining types used in benchmarking.\n\nThis module contains dataclass definitions for various types used throughout the\nbenchmarking process, such as Assertable, Task, Benchmark, and TaskResult.\n\nClasses:\n    Assertable:\n        Represents an object that can be asserted against in a benchmark task.\n\n    Assertion:\n        Type alias for a callable that takes an Assertable and returns a boolean.\n\n    Task:\n        Represents a single task within a benchmark, including its assertions.\n\n    Benchmark:\n        Represents a collection of tasks used to evaluate a model's performance.\n\n    TaskResult:\n        Represents the result of running a single task within a benchmark.\n\"\"\"\nfrom dataclasses import dataclass\nfrom subprocess import Popen\nfrom typing import Callable, Dict, Optional\n\nfrom gpt_engineer.core.base_execution_env import BaseExecutionEnv\nfrom gpt_engineer.core.files_dict import FilesDict\nfrom gpt_engineer.core.prompt import Prompt\n\n\n@dataclass\nclass Assertable:\n    \"\"\"\n    A class representing an object which can be asserted against.\n\n    Attributes:\n        files (FilesDict): The code files involved in the assertion.\n        env (BaseExecutionEnv): The execution environment in which the code is run.\n        process (Popen): The subprocess in which the code is run.\n        stdout (str): The standard output from the code execution.\n        stderr (str): The standard error from the code execution.\n    \"\"\"\n\n    files: FilesDict\n    env: BaseExecutionEnv\n    process: Optional[Popen]\n    stdout: Optional[str]\n    stderr: Optional[str]\n\n\nAssertion = Callable[[Assertable], bool]\n\n\n@dataclass\nclass Task:\n    name: str\n    initial_code: Optional[FilesDict]\n    command: Optional[str]\n    prompt: Prompt\n    assertions: Optional[Dict[str, Assertion]]\n\n\n@dataclass\nclass Benchmark:\n    \"\"\"A benchmark is a collection of tasks that evaluate a model's performance.\"\"\"\n\n    name: str\n    tasks: list[Task]\n    timeout: Optional[int] = None\n\n\n@dataclass\nclass TaskResult:\n    task_name: str\n    assertion_results: dict[str, bool]\n    duration: float\n\n    # Returns success rate from 0.00 up to 1.00\n    @property\n    def success_rate(self) -> float:\n        if not self.assertion_results:\n            return 0.0\n\n        succeeded = len(\n            [result for result in self.assertion_results.values() if result is True]\n        )\n\n        return succeeded / len(self.assertion_results)\n\n    def to_dict(self) -> dict:\n        out_dict = {key: value for key, value in self.__dict__.items()}\n        out_dict[\"solved\"] = self.success_rate\n        return out_dict\n", "gpt_engineer/benchmark/__main__.py": "\"\"\"\nMain entry point for the benchmarking tool.\n\nThis module provides a command-line interface for running benchmarks using Typer.\nIt allows users to specify the path to an agent, the benchmark(s) to run, and other\noptions such as verbosity.\n\nFunctions\n---------\nget_agent : function\n    Dynamically imports and returns the default configuration agent from the given path.\n\nmain : function\n    The main function that runs the specified benchmarks with the given agent.\n    Outputs the results to the console.\n\nAttributes\n----------\n__name__ : str\n    The standard boilerplate for invoking the main function when the script is executed.\n\"\"\"\nimport importlib\nimport os.path\nimport sys\n\nfrom typing import Annotated, Optional\n\nimport typer\n\nfrom langchain.globals import set_llm_cache\nfrom langchain_community.cache import SQLiteCache\n\nfrom gpt_engineer.applications.cli.main import load_env_if_needed\nfrom gpt_engineer.benchmark.bench_config import BenchConfig\nfrom gpt_engineer.benchmark.benchmarks.load import get_benchmark\nfrom gpt_engineer.benchmark.run import export_yaml_results, print_results, run\n\napp = typer.Typer(\n    context_settings={\"help_option_names\": [\"-h\", \"--help\"]}\n)  # creates a CLI app\n\n\ndef get_agent(path):\n    \"\"\"\n    Dynamically imports and returns the default configuration agent from the given path.\n\n    Parameters\n    ----------\n    path : str\n        The file path to the module containing the default configuration agent.\n\n    Returns\n    -------\n    BaseAgent\n        An instance of the imported default configuration agent.\n    \"\"\"\n    # Dynamically import the python module at path\n    sys.path.append(os.path.dirname(path))\n    agent_module = importlib.import_module(path.replace(\"/\", \".\").replace(\".py\", \"\"))\n    return agent_module.default_config_agent()\n\n\n@app.command(\n    help=\"\"\"\n        Run any benchmark(s) against the specified agent.\n\n        \\b\n        Currently available benchmarks are: apps and mbpp\n    \"\"\"\n)\ndef main(\n    path_to_agent: Annotated[\n        str,\n        typer.Argument(\n            help=\"python file that contains a function called 'default_config_agent'\"\n        ),\n    ],\n    bench_config: Annotated[\n        str, typer.Argument(help=\"optional task name in benchmark\")\n    ] = os.path.join(os.path.dirname(__file__), \"default_bench_config.toml\"),\n    yaml_output: Annotated[\n        Optional[str],\n        typer.Option(help=\"print results for each task\", show_default=False),\n    ] = None,\n    verbose: Annotated[\n        Optional[bool],\n        typer.Option(help=\"print results for each task\", show_default=False),\n    ] = False,\n    use_cache: Annotated[\n        Optional[bool],\n        typer.Option(\n            help=\"Speeds up computations and saves tokens when running the same prompt multiple times by caching the LLM response.\",\n            show_default=False,\n        ),\n    ] = True,\n):\n    \"\"\"\n    The main function that runs the specified benchmarks with the given agent and outputs the results to the console.\n\n    Parameters\n    ----------\n    path_to_agent : str\n        The file path to the Python module that contains a function called 'default_config_agent'.\n    bench_config : str, default=default_bench_config.toml\n        Configuration file for choosing which benchmark problems to run. See default config for more details.\n    yaml_output: Optional[str], default=None\n        Pass a path to a yaml file to have results written to file.\n    verbose : Optional[bool], default=False\n        A flag to indicate whether to print results for each task.\n    use_cache : Optional[bool], default=True\n        Speeds up computations and saves tokens when running the same prompt multiple times by caching the LLM response.\n    Returns\n    -------\n    None\n    \"\"\"\n    if use_cache:\n        set_llm_cache(SQLiteCache(database_path=\".langchain.db\"))\n    load_env_if_needed()\n    config = BenchConfig.from_toml(bench_config)\n    print(\"using config file: \" + bench_config)\n    benchmarks = list()\n    benchmark_results = dict()\n    for specific_config_name in vars(config):\n        specific_config = getattr(config, specific_config_name)\n        if hasattr(specific_config, \"active\"):\n            if specific_config.active:\n                benchmarks.append(specific_config_name)\n\n    for benchmark_name in benchmarks:\n        benchmark = get_benchmark(benchmark_name, config)\n        if len(benchmark.tasks) == 0:\n            print(\n                benchmark_name\n                + \" was skipped, since no tasks are specified. Increase the number of tasks in the config file at: \"\n                + bench_config\n            )\n            continue\n        agent = get_agent(path_to_agent)\n\n        results = run(agent, benchmark, verbose=verbose)\n        print(\n            f\"\\n--- Results for agent {path_to_agent}, benchmark: {benchmark_name} ---\"\n        )\n        print_results(results)\n        print()\n        benchmark_results[benchmark_name] = {\n            \"detailed\": [result.to_dict() for result in results]\n        }\n    if yaml_output is not None:\n        export_yaml_results(yaml_output, benchmark_results, config.to_dict())\n\n\nif __name__ == \"__main__\":\n    typer.run(main)\n", "gpt_engineer/benchmark/__init__.py": "", "gpt_engineer/benchmark/benchmarks/load.py": "\"\"\"\nModule for loading benchmarks.\n\nThis module provides a central point to access different benchmarks by name.\nIt maps benchmark names to their respective loading functions.\n\nFunctions\n---------\nget_benchmark : function\n    Retrieves a Benchmark object by name. Raises ValueError if the benchmark is unknown.\n\"\"\"\nfrom gpt_engineer.benchmark.bench_config import BenchConfig\nfrom gpt_engineer.benchmark.benchmarks.apps.load import load_apps\nfrom gpt_engineer.benchmark.benchmarks.gptme.load import load_gptme\nfrom gpt_engineer.benchmark.benchmarks.mbpp.load import load_mbpp\nfrom gpt_engineer.benchmark.types import Benchmark\n\nBENCHMARKS = {\n    \"gptme\": load_gptme,\n    \"apps\": load_apps,\n    \"mbpp\": load_mbpp,\n}\n\n\ndef get_benchmark(name: str, config: BenchConfig) -> Benchmark:\n    \"\"\"\n    Retrieves a Benchmark object by name. Raises ValueError if the benchmark is unknown.\n\n    Parameters\n    ----------\n    name : str\n        The name of the benchmark to retrieve.\n    config : BenchConfig\n        Configuration object for the benchmarks.\n\n    Returns\n    -------\n    Benchmark\n        The Benchmark object corresponding to the given name.\n\n    Raises\n    ------\n    ValueError\n        If the benchmark name is not found in the BENCHMARKS mapping.\n    \"\"\"\n    if name not in BENCHMARKS:\n        raise ValueError(f\"Unknown benchmark {name}.\")\n    return BENCHMARKS[name](config.__getattribute__(name))\n", "gpt_engineer/benchmark/benchmarks/apps/load.py": "\"\"\"\nModule for loading APPS evaluation tasks.\n\nThis module provides functionality to load tasks for evaluating GPT-based models\non smaller, more focused tasks. It defines a set of tasks with predefined prompts\nand assertions to benchmark the performance of AI models.\n\nFunctions\n---------\nload_apps : function\n    Loads the APPS benchmark, which consists of a series coding problems.\n\"\"\"\nfrom pathlib import Path\nfrom subprocess import TimeoutExpired\nfrom typing import Union\n\nfrom datasets import Dataset, DatasetDict, load_dataset, load_from_disk\n\nfrom gpt_engineer.benchmark.bench_config import AppsConfig\nfrom gpt_engineer.benchmark.benchmarks.apps.problem import Problem\nfrom gpt_engineer.benchmark.types import Assertable, Benchmark, Task\nfrom gpt_engineer.core.default.disk_execution_env import DiskExecutionEnv\nfrom gpt_engineer.core.files_dict import FilesDict\nfrom gpt_engineer.core.prompt import Prompt\n\nDATASET_PATH = Path(__file__).parent / \"dataset\"\n\n\nclass AppsAssertion:\n    def __init__(self, expected: str, command: str):\n        self.expected_output = self._format(expected)\n        self.command = command\n\n    def evaluate(self, assertable: Assertable) -> bool:\n        # Create new execution environment for every run to avoid side effects\n        env = DiskExecutionEnv()\n        env.upload(assertable.files)\n        pro = env.popen(self.command)\n        try:\n            stdout, stderr = pro.communicate(timeout=2)\n            stdout, stderr = stdout.decode(\"utf-8\"), stderr.decode(\"utf-8\")\n        except TimeoutExpired:\n            print(\"Execution Timeout\")\n            return False\n\n        return self.expected_output in self._format(stdout)\n\n    def _format(self, string: str) -> str:\n        return string.replace(\" \", \"\").replace(\"\\n\", \"\")\n\n\ndef _get_dataset() -> Union[Dataset, DatasetDict]:\n    try:\n        return load_from_disk(str(DATASET_PATH))\n    except FileNotFoundError:\n        print(\"Dataset not found locally, downloading...\")\n\n    dataset = load_dataset(\"codeparrot/apps\", trust_remote_code=True)\n    dataset.save_to_disk(str(DATASET_PATH))\n\n    return dataset\n\n\ndef load_apps(config: AppsConfig) -> Benchmark:\n    \"\"\"\n    Loads the APPS benchmark, which consists of a series coding problems.\n\n    Returns\n    -------\n    Benchmark\n        A Benchmark object containing a list of Task objects for the APPS evaluation.\n    \"\"\"\n    dataset = _get_dataset()\n    tasks = []\n    problems = list()\n    for dataset_type in [\"test\", \"train\"]:\n        problems += [\n            Problem(\n                id=problem[\"problem_id\"],\n                question=problem[\"question\"],\n                input_output=problem[\"input_output\"],\n                starter_code=problem[\"starter_code\"],\n            )\n            for index, problem in enumerate(dataset[dataset_type])\n            if (index < config.__getattribute__(dataset_type + \"_end_index\"))\n            and (index >= config.__getattribute__(dataset_type + \"_start_index\"))\n        ]\n\n    for problem in problems:\n        prompt = Prompt(\n            problem.question\n            + \"\\nThe program, including its inputs, should be run from the command \"\n            \"line like 'python main \\\"input1 input2 etc \\\"', with all inputs inside \"\n            \"the quotation marks. The program should not read inputs from stdin.\"\n        )\n\n        tasks.append(\n            Task(\n                name=str(problem.id),\n                initial_code=FilesDict({\"main.py\": problem.starter_code}),\n                command=None,  # Explicitly setting `None` because each assertion specifies its command\n                prompt=prompt,\n                assertions={\n                    f\"correct output {i}\": AppsAssertion(\n                        expected=problem.outputs[i],\n                        command=\"python main.py\" + ' \"' + problem.inputs[i] + '\"',\n                    ).evaluate\n                    for i in range(\n                        min(len(problem.outputs), config.examples_per_problem)\n                    )\n                },\n            )\n        )\n\n    return Benchmark(\n        name=\"apps\",\n        tasks=tasks,\n    )\n", "gpt_engineer/benchmark/benchmarks/apps/problem.py": "import json\n\nfrom dataclasses import dataclass\nfrom functools import cached_property\nfrom typing import List\n\n\n@dataclass(frozen=True)\nclass Problem:\n    id: int\n    question: str\n    input_output: str\n    starter_code: str\n\n    @property\n    def inputs(self) -> List[str]:\n        return self._parsed_inputs_outputs[\"inputs\"]\n\n    @property\n    def outputs(self) -> List[str]:\n        return self._parsed_inputs_outputs[\"outputs\"]\n\n    @cached_property\n    def _parsed_inputs_outputs(self):\n        return json.loads(self.input_output.replace(\"\\n\", \"\"))\n", "gpt_engineer/benchmark/benchmarks/apps/problems.py": "# TODO: Pick problems\n# Temporary testing against these problems\nPROBLEM_IDS = list(range(0, 50))\n", "gpt_engineer/benchmark/benchmarks/gptme/load.py": "\"\"\"\nModule for loading GPT-Me evaluation tasks.\n\nThis module provides functionality to load tasks for evaluating GPT-based models\non smaller, more focused tasks. It defines a set of tasks with predefined prompts\nand assertions to benchmark the performance of AI models.\n\nFunctions\n---------\nload_gptme : function\n    Loads the GPT-Me benchmark, which consists of a series of tasks for evaluation.\n\"\"\"\nfrom gpt_engineer.benchmark.bench_config import GptmeConfig\nfrom gpt_engineer.benchmark.types import Benchmark, Task\nfrom gpt_engineer.core.files_dict import FilesDict\nfrom gpt_engineer.core.prompt import Prompt\n\n\ndef load_gptme(config: GptmeConfig) -> Benchmark:\n    \"\"\"\n    Loads the GPT-Me benchmark, which consists of a series of tasks for evaluation.\n\n    Returns\n    -------\n    Benchmark\n        A Benchmark object containing a list of Task objects for the GPT-Me evaluation.\n    \"\"\"\n    return Benchmark(\n        name=\"gptme\",\n        tasks=[\n            Task(\n                name=\"hello\",\n                initial_code=FilesDict({\"hello.py\": \"print('Hello, world!')\"}),\n                command=\"python hello.py\",\n                prompt=Prompt(\"Change the code in hello.py to print 'Hello, human!'\"),\n                assertions={\n                    \"correct output\": lambda assertable: assertable.stdout\n                    == \"Hello, human!\\n\",\n                    \"correct file\": lambda assertable: assertable.files[\n                        \"hello.py\"\n                    ].strip()\n                    == \"print('Hello, human!')\",\n                },\n            ),\n            Task(\n                name=\"hello-patch\",\n                initial_code=FilesDict({\"hello.py\": \"print('Hello, world!')\"}),\n                command=\"python hello.py\",\n                prompt=Prompt(\"Patch the code in hello.py to print 'Hello, human!'\"),\n                assertions={\n                    \"correct output\": lambda assertable: assertable.stdout\n                    == \"Hello, human!\\n\",\n                    \"correct file\": lambda assertable: assertable.files[\n                        \"hello.py\"\n                    ].strip()\n                    == \"print('Hello, human!')\",\n                },\n            ),\n            Task(\n                name=\"hello-ask\",\n                initial_code=FilesDict({\"hello.py\": \"print('Hello, world!')\"}),\n                command=\"echo 'Erik' | python hello.py\",\n                prompt=Prompt(\n                    \"modify hello.py to ask the user for their name and print 'Hello, <name>!'. don't try to execute it\"\n                ),\n                assertions={\n                    \"correct output\": lambda assertable: \"Hello, Erik!\"\n                    in assertable.stdout,\n                },\n            ),\n            Task(\n                name=\"prime100\",\n                initial_code=FilesDict(\n                    {}\n                ),  # Empty dictionary since no initial code is provided\n                command=\"python prime.py\",\n                prompt=Prompt(\n                    \"write a script prime.py that computes and prints the 100th prime number\"\n                ),\n                assertions={\n                    \"correct output\": lambda assertable: \"541\"\n                    in assertable.stdout.split(),\n                },\n            ),\n            Task(\n                name=\"init-git\",\n                initial_code=FilesDict(\n                    {}\n                ),  # Empty dictionary since no initial code is provided\n                command=\"git status\",\n                prompt=Prompt(\n                    \"initialize a git repository, write a main.py file, and commit it\"\n                ),\n                assertions={\n                    \"clean exit\": lambda assertable: assertable.process.returncode == 0,\n                    \"clean working tree\": lambda assertable: \"nothing to commit, working tree clean\"\n                    in assertable.stdout,\n                    \"main.py exists\": lambda assertable: \"main.py\" in assertable.files,\n                    \"we have a commit\": lambda assertable: \"No commits yet\"\n                    not in assertable.stdout,\n                },\n            ),\n        ],\n    )\n", "gpt_engineer/benchmark/benchmarks/mbpp/load.py": "\"\"\"\nModule for loading MBPP evaluation tasks.\n\nThis module provides functionality to load tasks for evaluating GPT-based models\non smaller, more focused tasks. It defines a set of tasks with predefined prompts\nand assertions to benchmark the performance of AI models.\n\nFunctions\n---------\nload_mbpp : function\n    Loads the MBPP benchmark, which consists of a series coding problems.\n\"\"\"\nfrom pathlib import Path\nfrom subprocess import TimeoutExpired\nfrom typing import Union\n\nfrom datasets import Dataset, DatasetDict, load_dataset, load_from_disk\n\nfrom gpt_engineer.benchmark.bench_config import MbppConfig\nfrom gpt_engineer.benchmark.benchmarks.mbpp.problem import Problem\nfrom gpt_engineer.benchmark.types import Assertable, Benchmark, Task\nfrom gpt_engineer.core.default.disk_execution_env import DiskExecutionEnv\nfrom gpt_engineer.core.files_dict import FilesDict\nfrom gpt_engineer.core.prompt import Prompt\n\nDATASET_PATH = Path(__file__).parent / \"dataset\"\n\n\nclass MbppAssertion:\n    def __init__(self, assertion: str):\n        self.assertion = assertion\n\n    def evaluate(self, assertable: Assertable) -> bool:\n        generated_code = assertable.files[\"main.py\"]\n        code_with_assertion = f\"{generated_code}\\n{self.assertion}\"\n\n        # Create new execution environment for every run to avoid side effects\n        env = DiskExecutionEnv()\n        env.upload(FilesDict({\"main.py\": code_with_assertion}))\n        pro = env.popen(\"python main.py\")\n\n        try:\n            stdout, stderr = pro.communicate(timeout=2)\n            stdout, stderr = stdout.decode(\"utf-8\"), stderr.decode(\"utf-8\")\n        except TimeoutExpired:\n            print(\"Execution Timeout\")\n            return False\n\n        return not stderr\n\n\ndef _get_dataset() -> Union[Dataset, DatasetDict]:\n    try:\n        return load_from_disk(str(DATASET_PATH))\n    except FileNotFoundError:\n        print(\"Dataset not found locally, downloading...\")\n\n    dataset = load_dataset(\"mbpp\", \"sanitized\", trust_remote_code=True)\n    dataset.save_to_disk(str(DATASET_PATH))\n\n    return dataset\n\n\ndef load_mbpp(config: MbppConfig) -> Benchmark:\n    \"\"\"\n    Loads the MBPP benchmark, which consists of a series coding problems.\n\n    Returns\n    -------\n    Benchmark\n        A Benchmark object containing a list of Task objects for the MBPP evaluation.\n    \"\"\"\n    dataset = _get_dataset()\n    tasks = []\n    problems = []\n    for dataset_type in [\"test\", \"train\"]:\n        problems += [\n            Problem(\n                source_file=problem[\"source_file\"],\n                task_id=problem[\"task_id\"],\n                prompt=problem[\"prompt\"],\n                code=problem[\"code\"],\n                test_imports=problem[\"test_imports\"],\n                test_list=problem[\"test_list\"],\n            )\n            for index, problem in enumerate(dataset[dataset_type])\n            if index < config.__getattribute__(dataset_type + \"_len\")\n        ]\n\n    for problem in problems:\n        prompt = Prompt(\n            problem.prompt\n            + \"Please extend given function without changing it's declaration including arguments.\"\n        )\n\n        tasks.append(\n            Task(\n                name=str(problem.task_id),\n                initial_code=FilesDict({\"main.py\": problem.starting_code}),\n                command=None,  # Explicitly setting `None` because each assertion runs code\n                prompt=prompt,\n                assertions={\n                    f\"correct assertion {i}\": MbppAssertion(\n                        assertion=assertion\n                    ).evaluate\n                    for i, assertion in enumerate(problem.test_list)\n                },\n            )\n        )\n\n    return Benchmark(\n        name=\"mbpp\",\n        tasks=tasks,\n    )\n", "gpt_engineer/benchmark/benchmarks/mbpp/problem.py": "from dataclasses import dataclass\nfrom typing import List\n\n\n@dataclass(frozen=True)\nclass Problem:\n    source_file: int\n    task_id: str\n    prompt: str\n    code: str\n    test_imports: str\n    test_list: List[str]\n\n    @property\n    def starting_code(self) -> str:\n        lines: List[str] = []\n\n        for line in self.code.split(\"\\n\"):\n            lines.append(line)\n\n            if line.startswith(\"def \"):\n                lines.append(\"pass #  TODO: Implement method\\n\")\n                break\n\n        return \"\\n\".join(lines)\n", "gpt_engineer/benchmark/benchmarks/mbpp/problems.py": "# TODO: Pick problems\n# Temporary testing against these problems\nPROBLEM_IDS = range(0, 100)\n", "scripts/print_chat.py": "\"\"\"\nThis module provides functionality to print a conversation with messages\ncolored according to the role of the speaker.\n\"\"\"\n\nimport json\n\nimport typer\n\nfrom termcolor import colored\n\napp = typer.Typer()\n\n\ndef pretty_print_conversation(messages):\n    \"\"\"\n    Prints a conversation with messages formatted and colored by role.\n\n    Parameters\n    ----------\n    messages : list\n        A list of message dictionaries, each containing 'role', 'name', and 'content' keys.\n\n    \"\"\"\n\n    role_to_color = {\n        \"system\": \"red\",\n        \"user\": \"green\",\n        \"assistant\": \"blue\",\n        \"function\": \"magenta\",\n    }\n    formatted_messages = []\n    for message in messages:\n        if message[\"role\"] == \"function\":\n            formatted_messages.append(\n                f\"function ({message['name']}): {message['content']}\\n\"\n            )\n        else:\n            assistant_content = (\n                message[\"function_call\"]\n                if message.get(\"function_call\")\n                else message[\"content\"]\n            )\n            role_to_message = {\n                \"system\": f\"system: {message['content']}\\n\",\n                \"user\": f\"user: {message['content']}\\n\",\n                \"assistant\": f\"assistant: {assistant_content}\\n\",\n            }\n            formatted_messages.append(role_to_message[message[\"role\"]])\n\n    for formatted_message in formatted_messages:\n        role = messages[formatted_messages.index(formatted_message)][\"role\"]\n        color = role_to_color[role]\n        print(colored(formatted_message, color))\n\n\n@app.command()\ndef main(\n    messages_path: str,\n):\n    \"\"\"\n    Main function that loads messages from a JSON file and prints them using pretty formatting.\n\n    Parameters\n    ----------\n    messages_path : str\n        The file path to the JSON file containing the messages.\n\n    \"\"\"\n    with open(messages_path) as f:\n        messages = json.load(f)\n\n    pretty_print_conversation(messages)\n\n\nif __name__ == \"__main__\":\n    app()\n", "scripts/clean_benchmarks.py": "\"\"\"\nThis module provides functionality to clean up benchmark directories by removing\nall files and folders except for 'prompt' and 'main_prompt'.\n\"\"\"\n\n# list all folders in benchmark folder\n# for each folder, run the benchmark\n\nimport os\nimport shutil\n\nfrom pathlib import Path\n\nfrom typer import run\n\n\ndef main():\n    \"\"\"\n    Main function that iterates through all directories in the 'benchmark' folder\n    and cleans them by removing all files and directories except for 'prompt' and\n    'main_prompt'.\n    \"\"\"\n\n    benchmarks = Path(\"benchmark\")\n\n    for benchmark in benchmarks.iterdir():\n        if benchmark.is_dir():\n            print(f\"Cleaning {benchmark}\")\n            for path in benchmark.iterdir():\n                if path.name in [\"prompt\", \"main_prompt\"]:\n                    continue\n\n                # Get filename of Path object\n                if path.is_dir():\n                    # delete the entire directory\n                    shutil.rmtree(path)\n                else:\n                    # delete the file\n                    os.remove(path)\n\n\nif __name__ == \"__main__\":\n    run(main)\n", "scripts/legacy_benchmark.py": "\"\"\"\nThis module provides functionality to run benchmarks on different folders within\nthe 'benchmark' directory, wait for their completion, and generate a report.\n\"\"\"\n\n# list all folders in benchmark folder\n# for each folder, run the benchmark\nimport contextlib\nimport json\nimport os\nimport subprocess\n\nfrom datetime import datetime\nfrom itertools import islice\nfrom pathlib import Path\nfrom typing import Iterable, Union\n\nfrom tabulate import tabulate\nfrom typer import run\n\n\ndef main(\n    n_benchmarks: Union[int, None] = None,\n):\n    \"\"\"\n    Main function that runs benchmarks on folders within the 'benchmark' directory.\n\n    Parameters\n    ----------\n    n_benchmarks : Union[int, None], optional\n        The number of benchmarks to run. If None, all benchmarks are run.\n\n    \"\"\"\n\n    path = Path(\"benchmark\")\n\n    folders: Iterable[Path] = path.iterdir()\n\n    if n_benchmarks:\n        folders = islice(folders, n_benchmarks)\n\n    benchmarks = []\n    results = []\n    for bench_folder in folders:\n        if os.path.isdir(bench_folder):\n            print(f\"Running benchmark for {bench_folder}\")\n\n            log_path = bench_folder / \"log.txt\"\n            log_file = open(log_path, \"w\")\n            process = subprocess.Popen(\n                [\n                    \"python\",\n                    \"-u\",  # Unbuffered output\n                    \"-m\",\n                    \"gpt_engineer.cli.main\",\n                    bench_folder,\n                    \"--steps\",\n                    \"benchmark\",\n                ],\n                stdout=log_file,\n                stderr=log_file,\n                bufsize=0,\n            )\n            benchmarks.append(bench_folder)\n            results.append((process, log_file))\n\n            print(\"You can stream the log file by running:\")\n            print(f\"tail -f {log_path}\")\n            print()\n\n    for bench_folder, (process, file) in zip(benchmarks, results):\n        process.wait()\n        file.close()\n\n        print(\"process\", bench_folder.name, \"finished with code\", process.returncode)\n        print(\"Running it. Original benchmark prompt:\")\n        print()\n        with open(bench_folder / \"prompt\") as f:\n            print(f.read())\n        print()\n\n        with contextlib.suppress(KeyboardInterrupt):\n            subprocess.run(\n                [\n                    \"python\",\n                    \"-m\",\n                    \"gpt_engineer.cli.main\",\n                    bench_folder,\n                    \"--steps\",\n                    \"evaluate\",\n                ],\n            )\n\n    generate_report(benchmarks, path)\n\n\ndef generate_report(benchmarks, benchmark_path):\n    \"\"\"\n    Generates a report of the benchmark results and optionally appends it to a markdown file.\n\n    Parameters\n    ----------\n    benchmarks : list\n        A list of benchmark folder paths that have been processed.\n    benchmark_path : Path\n        The path to the benchmark directory.\n\n    \"\"\"\n\n    headers = [\"Benchmark\", \"Ran\", \"Works\", \"Perfect\", \"Notes\"]\n    rows = []\n    for bench_folder in benchmarks:\n        memory = bench_folder / \".gpteng\" / \"memory\"\n        with open(memory / \"review\") as f:\n            review = json.loads(f.read())\n            rows.append(\n                [\n                    bench_folder.name,\n                    to_emoji(review.get(\"ran\", None)),\n                    to_emoji(review.get(\"works\", None)),\n                    to_emoji(review.get(\"perfect\", None)),\n                    review.get(\"comments\", None),\n                ]\n            )\n    table: str = tabulate(rows, headers, tablefmt=\"pipe\")\n    print(\"\\nBenchmark report:\\n\")\n    print(table)\n    print()\n    append_to_results = ask_yes_no(\"Append report to the results file?\")\n    if append_to_results:\n        results_path = benchmark_path / \"RESULTS.md\"\n        current_date = datetime.now().strftime(\"%Y-%m-%d\")\n        insert_markdown_section(results_path, current_date, table, 2)\n\n\ndef to_emoji(value: bool) -> str:\n    \"\"\"\n    Converts a boolean value to its corresponding emoji representation.\n\n    Parameters\n    ----------\n    value : bool\n        The boolean value to convert.\n\n    Returns\n    -------\n    str\n        An emoji string representing the boolean value.\n\n    \"\"\"\n\n    return \"\\U00002705\" if value else \"\\U0000274C\"\n\n\ndef insert_markdown_section(file_path, section_title, section_text, level):\n    \"\"\"\n    Inserts a new section into a markdown file at the specified level.\n\n    Parameters\n    ----------\n    file_path : Path\n        The path to the markdown file.\n    section_title : str\n        The title of the section to insert.\n    section_text : str\n        The text content of the section to insert.\n    level : int\n        The header level of the section.\n\n    \"\"\"\n\n    with open(file_path, \"r\") as file:\n        lines = file.readlines()\n\n    header_prefix = \"#\" * level\n    new_section = f\"{header_prefix} {section_title}\\n\\n{section_text}\\n\\n\"\n\n    # Find the first section with the specified level\n    line_number = -1\n    for i, line in enumerate(lines):\n        if line.startswith(header_prefix):\n            line_number = i\n            break\n\n    if line_number != -1:\n        lines.insert(line_number, new_section)\n    else:\n        print(\n            f\"Markdown file was of unexpected format. No section of level {level} found. \"\n            \"Did not write results.\"\n        )\n        return\n\n    # Write the file\n    with open(file_path, \"w\") as file:\n        file.writelines(lines)\n\n\ndef ask_yes_no(question: str) -> bool:\n    \"\"\"\n    Asks a yes/no question and returns the response as a boolean value.\n\n    Parameters\n    ----------\n    question : str\n        The yes/no question to ask.\n\n    Returns\n    -------\n    bool\n        True if the answer is 'yes', False if 'no'.\n\n    \"\"\"\n\n    while True:\n        response = input(question + \" (y/n): \").lower().strip()\n        if response == \"y\":\n            return True\n        elif response == \"n\":\n            return False\n        else:\n            print(\"Please enter either 'y' or 'n'.\")\n\n\nif __name__ == \"__main__\":\n    run(main)\n", "scripts/test_api.py": "\"\"\"This is just a demo to test api.py.\"\"\"\n\nfrom time import sleep\n\nimport requests\n\n\ndef post_data(url, extra_arguments):\n    \"\"\"\n    Make an HTTP POST request with extra_arguments as data.\n\n    Parameters\n    ----------\n     url : str\n        The URL to which the POST request should be sent.\n    extra_arguments : dict\n        A dictionary of data that needs to be sent in the POST request.\n\n    Returns\n    -------\n    response\n        The response from the server.\n    \"\"\"\n\n    response = requests.post(url, json=extra_arguments)\n    return response\n\n\nif __name__ == \"__main__\":\n    URL_BASE = \"http://127.0.0.1:8000\"\n\n    arguments = {\n        \"input\": \"We are writing snake in python. MVC components split \\\n        in separate files. Keyboard control.\",  # our prompt\n        \"additional_input\": {\"improve_option\": False},\n    }\n\n    # create a task\n    response = post_data(f\"{URL_BASE}/agent/tasks\", arguments)\n    print(response.json())\n    task_id = response.json()[\"task_id\"]\n\n    sleep(1)  # this is not needed\n\n    # execute the step for our task\n    response = post_data(f\"{URL_BASE}/agent/tasks/{task_id}/steps\", {})\n    print(response.json())\n", "docs/create_api_rst.py": "\"\"\"Script for auto-generating api_reference.rst\"\"\"\nimport glob\nimport re\n\nfrom pathlib import Path\n\nROOT_DIR = Path(__file__).parents[1].absolute()\nprint(ROOT_DIR)\nPKG_DIR = ROOT_DIR / \"gpt_engineer\"\nWRITE_FILE = Path(__file__).parent / \"api_reference.rst\"\n\n\ndef load_members() -> dict:\n    members: dict = {}\n    for py in glob.glob(str(PKG_DIR) + \"/**/*.py\", recursive=True):\n        module = py[len(str(PKG_DIR)) + 1 :].replace(\".py\", \"\").replace(\"/\", \".\")\n        top_level = module.split(\".\")[0]\n        if top_level not in members:\n            members[top_level] = {\"classes\": [], \"functions\": []}\n        with open(py, \"r\") as f:\n            for line in f.readlines():\n                cls = re.findall(r\"^class ([^_].*)\\(\", line)\n                members[top_level][\"classes\"].extend([module + \".\" + c for c in cls])\n                func = re.findall(r\"^def ([^_].*)\\(\", line)\n                afunc = re.findall(r\"^async def ([^_].*)\\(\", line)\n                func_strings = [module + \".\" + f for f in func + afunc]\n                members[top_level][\"functions\"].extend(func_strings)\n    return members\n\n\ndef construct_doc(members: dict) -> str:\n    full_doc = \"\"\"\\\n.. _api_reference:\n\n=============\nAPI Reference\n=============\n\n\"\"\"\n    for module, _members in sorted(members.items(), key=lambda kv: kv[0]):\n        classes = _members[\"classes\"]\n        functions = _members[\"functions\"]\n        if not (classes or functions):\n            continue\n\n        module_title = module.replace(\"_\", \" \").title()\n        if module_title == \"Llms\":\n            module_title = \"LLMs\"\n        section = f\":mod:`gpt_engineer.{module}`: {module_title}\"\n        full_doc += f\"\"\"\\\n{section}\n{'=' * (len(section) + 1)}\n\n.. automodule:: gpt_engineer.{module}\n    :no-members:\n    :no-inherited-members:\n\n\"\"\"\n\n        if classes:\n            cstring = \"\\n    \".join(sorted(classes))\n            full_doc += f\"\"\"\\\nClasses\n--------------\n.. currentmodule:: gpt_engineer\n\n.. autosummary::\n    :toctree: {module}\n    :template: class.rst\n\n    {cstring}\n\n\"\"\"\n        if functions:\n            fstring = \"\\n    \".join(sorted(functions))\n            full_doc += f\"\"\"\\\nFunctions\n--------------\n.. currentmodule:: gpt_engineer\n\n.. autosummary::\n    :toctree: {module}\n\n    {fstring}\n\n\"\"\"\n    return full_doc\n\n\ndef main() -> None:\n    members = load_members()\n    full_doc = construct_doc(members)\n    with open(WRITE_FILE, \"w\") as f:\n        f.write(full_doc)\n\n\nif __name__ == \"__main__\":\n    main()\n", "docs/conf.py": "#!/usr/bin/env python\n#\n# file_processor documentation build configuration file, created by\n# sphinx-quickstart on Fri Jun  9 13:47:02 2017.\n#\n# This file is execfile()d with the current directory set to its\n# containing dir.\n#\n# Note that not all possible configuration values are present in this\n# autogenerated file.\n#\n# All configuration values have a default; values that are commented out\n# serve to show the default.\n\n# If extensions (or modules to document with autodoc) are in another\n# directory, add these directories to sys.path here. If the directory is\n# relative to the documentation root, use os.path.abspath to make it\n# absolute, like shown here.\n#\nimport os\nimport sys\n\nfrom pathlib import Path\n\nimport toml\n\nsys.path.insert(0, os.path.abspath(\"..\"))\n\nROOT_DIR = Path(__file__).parents[1].absolute()\n\nwith open(\"../pyproject.toml\") as f:\n    data = toml.load(f)\n\n\n# The master toctree document.\nmaster_doc = \"index\"\n\n# General information about the project.\nproject = data[\"tool\"][\"poetry\"][\"name\"]\ncopyright = \"2023 Anton Osika\"\nauthor = \" Anton Osika & Contributors\"\n\n# The version info for the project you're documenting, acts as replacement\n# for |version| and |release|, also used in various other places throughout\n# the built documents.\n#\n# The short X.Y version.\nversion = data[\"tool\"][\"poetry\"][\"version\"]\n# The full version, including alpha/beta/rc tags.\nrelease = data[\"tool\"][\"poetry\"][\"version\"]\n\n\n# -- General configuration ---------------------------------------------\n\n# If your documentation needs a minimal Sphinx version, state it here.\n#\n# needs_sphinx = '1.0'\n\n# Add any Sphinx extension module names here, as strings. They can be\n# extensions coming with Sphinx (named 'sphinx.ext.*') or your custom ones.\nextensions = [\n    \"sphinx.ext.autodoc\",\n    \"sphinx.ext.autodoc.typehints\",\n    \"sphinx.ext.autosummary\",\n    \"sphinx.ext.napoleon\",\n    \"sphinx.ext.viewcode\",\n    \"sphinx_copybutton\",\n    \"myst_parser\",\n    \"IPython.sphinxext.ipython_console_highlighting\",\n]\n\n# The suffix(es) of source filenames.\n# You can specify multiple suffix as a list of string:\n\nsource_suffix = [\".rst\", \".md\"]\n\nautodoc_pydantic_model_show_json = False\nautodoc_pydantic_field_list_validators = False\nautodoc_pydantic_config_members = False\nautodoc_pydantic_model_show_config_summary = False\nautodoc_pydantic_model_show_validator_members = False\nautodoc_pydantic_model_show_validator_summary = False\nautodoc_pydantic_model_signature_prefix = \"class\"\nautodoc_pydantic_field_signature_prefix = \"param\"\nautodoc_member_order = \"groupwise\"\nautoclass_content = \"both\"\nautodoc_typehints_format = \"short\"\n\nautodoc_default_options = {\n    \"members\": True,\n    \"show-inheritance\": True,\n    \"inherited-members\": \"BaseModel\",\n    \"undoc-members\": False,\n}\n\n# Add any paths that contain templates here, relative to this directory.\ntemplates_path = [\"_templates\"]\n\n\n# source_suffix = '.rst'\n\n\n# The language for content autogenerated by Sphinx. Refer to documentation\n# for a list of supported languages.\n#\n# This is also used if you do content translation via gettext catalogs.\n# Usually you set \"language\" from the command line for these cases.\nlanguage = \"en\"\n\n# List of patterns, relative to source directory, that match files and\n# directories to ignore when looking for source files.\n# This patterns also effect to html_static_path and html_extra_path\nexclude_patterns = [\"_build\", \"Thumbs.db\", \".DS_Store\"]\n\n# The name of the Pygments (syntax highlighting) style to use.\npygments_style = \"sphinx\"\n\n# If true, `todo` and `todoList` produce output, else they produce nothing.\ntodo_include_todos = False\n\n\n# -- Options for HTML output -------------------------------------------\n\n# The theme to use for HTML and HTML Help pages.  See the documentation for\n# a list of builtin themes.\n#\n# html_theme = 'alabaster'\nhtml_theme = \"sphinx_rtd_theme\"\n\n# Theme options are theme-specific and customize the look and feel of a\n# theme further.  For a list of options available for each theme, see the\n# documentation.\n#\n# html_theme_options = {}\n\n# Add any paths that contain custom static files (such as style sheets) here,\n# relative to this directory. They are copied after the builtin static files,\n# so a file named \"default.css\" will overwrite the builtin \"default.css\".\n# html_static_path = [\"_static\"]\n\n\n# -- Options for HTMLHelp output ---------------------------------------\n\n# Output file base name for HTML help builder.\nhtmlhelp_basename = \"gpt_engineerdoc\"\n\n\n# -- Options for LaTeX output ------------------------------------------\n\nlatex_elements = {\n    # The paper size ('letterpaper' or 'a4paper').\n    #\n    # 'papersize': 'letterpaper',\n    # The font size ('10pt', '11pt' or '12pt').\n    #\n    # 'pointsize': '10pt',\n    # Additional stuff for the LaTeX preamble.\n    #\n    # 'preamble': '',\n    # Latex figure (float) alignment\n    #\n    # 'figure_align': 'htbp',\n}\n\n# Grouping the document tree into LaTeX files. List of tuples\n# (source start file, target name, title, author, documentclass\n# [howto, manual, or own class]).\nlatex_documents = [\n    (master_doc, \"gpt_engineer.tex\", \"GPT-ENgineer Documentation\", \"manual\"),\n]\n\n\n# -- Options for manual page output ------------------------------------\n\n# One entry per manual page. List of tuples\n# (source start file, name, description, authors, manual section).\nman_pages = [(master_doc, \"gpt_engineer\", \"GPT-Engineer Documentation\", [author], 1)]\n\n\n# -- Options for Texinfo output ----------------------------------------\n\n# Grouping the document tree into Texinfo files. List of tuples\n# (source start file, target name, title, author,\n#  dir menu entry, description, category)\ntexinfo_documents = [\n    (\n        master_doc,\n        \"gpt_engineer\",\n        \"GPT-Engineer Documentation\",\n        author,\n        \"gpt_engineer\",\n        \"One line description of project.\",\n        \"Miscellaneous\",\n    ),\n]\n\n# generate autosummary even if no references\nautosummary_generate = True\n\nmyst_enable_extensions = [\n    \"colon_fence\",\n]\n\nmyst_all_links_external = True\n", "docs/examples/open_llms/openai_api_interface.py": "import os\n\nfrom openai import OpenAI\n\nclient = OpenAI(\n    base_url=os.getenv(\"OPENAI_API_BASE\"), api_key=os.getenv(\"OPENAI_API_KEY\")\n)\n\nresponse = client.chat.completions.create(\n    model=os.getenv(\"MODEL_NAME\"),\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Provide me with only the code for a simple python function that sums two numbers.\",\n        },\n    ],\n    temperature=0.7,\n    max_tokens=200,\n)\n\nprint(response.choices[0].message.content)\n", "docs/examples/open_llms/langchain_interface.py": "import os\n\nfrom langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\nfrom langchain_openai import ChatOpenAI\n\nmodel = ChatOpenAI(\n    model=os.getenv(\"MODEL_NAME\"),\n    temperature=0.1,\n    callbacks=[StreamingStdOutCallbackHandler()],\n    streaming=True,\n)\n\nprompt = (\n    \"Provide me with only the code for a simple python function that sums two numbers.\"\n)\n\nmodel.invoke(prompt)\n", "tests/test_project_config.py": "import tempfile\n\nimport pytest\n\nfrom gpt_engineer.core.project_config import (\n    Config,\n    _GptEngineerAppConfig,\n    _OpenApiConfig,\n    example_config,\n    filter_none,\n)\n\n\ndef test_config_load():\n    # write example config to a file\n    with tempfile.NamedTemporaryFile(mode=\"w\", delete=False) as f:\n        f.write(example_config)\n\n    # load the config from the file\n    config = Config.from_toml(f.name)\n\n    assert config.paths.base == \"./frontend\"\n    assert config.paths.src == \"./src\"\n    assert config.run.build == \"npm run build\"\n    assert config.run.test == \"npm run test\"\n    assert config.run.lint == \"quick-lint-js\"\n    assert config.gptengineer_app\n    assert config.gptengineer_app.project_id == \"...\"\n    assert config.gptengineer_app.openapi\n    assert (\n        config.gptengineer_app.openapi[0].url\n        == \"https://api.gptengineer.app/openapi.json\"\n    )\n    assert (\n        config.gptengineer_app.openapi[1].url\n        == \"https://some-color-translating-api/openapi.json\"\n    )\n    assert config.to_dict()\n    assert config.to_toml(f.name, save=False)\n\n    # check that write+read is idempotent\n    assert Config.from_toml(f.name) == config\n\n\ndef test_config_defaults():\n    config = Config()\n    assert config.paths.base is None\n    with tempfile.NamedTemporaryFile(mode=\"w\", delete=False) as f:\n        config.to_toml(f.name)\n\n    # check that read+write is idempotent\n    assert Config.from_toml(f.name) == config\n\n    # check that empty (default) config is written as empty string\n    toml_str = config.to_toml(f.name, save=False)\n    assert toml_str == \"\"\n\n\ndef test_config_from_dict():\n    d = {\"gptengineer-app\": {\"project_id\": \"...\"}}  # minimal example\n    config = Config.from_dict(d)\n    assert config.gptengineer_app\n    assert config.gptengineer_app.project_id == \"...\"\n    config_dict = config.to_dict()\n\n    # check that the config dict matches the input dict exactly (no keys/defaults added)\n    assert config_dict == d\n\n\ndef test_config_from_dict_with_openapi():\n    # A good test because it has 3 levels of nesting\n    d = {\n        \"gptengineer-app\": {\n            \"project_id\": \"...\",\n            \"openapi\": [\n                {\"url\": \"https://api.gptengineer.app/openapi.json\"},\n            ],\n        }\n    }\n    config = Config.from_dict(d)\n    assert config.gptengineer_app\n    assert config.gptengineer_app.project_id == \"...\"\n    assert config.gptengineer_app.openapi\n    assert (\n        config.gptengineer_app.openapi[0].url\n        == \"https://api.gptengineer.app/openapi.json\"\n    )\n\n\ndef test_config_load_partial():\n    # Loads a partial config, and checks that the rest is not set (i.e. None)\n    example_config = \"\"\"\n[gptengineer-app]\nproject_id = \"...\"\n\"\"\".strip()\n    with tempfile.NamedTemporaryFile(mode=\"w\", delete=False) as f:\n        f.write(example_config)\n\n    config = Config.from_toml(f.name)\n    assert config.gptengineer_app\n    assert config.gptengineer_app.project_id == \"...\"\n    assert config.to_dict()\n    toml_str = config.to_toml(f.name, save=False)\n    assert toml_str == example_config\n\n    # check that write+read is idempotent\n    assert Config.from_toml(f.name) == config\n\n\ndef test_config_update():\n    example_config = \"\"\"\n[gptengineer-app]\nproject_id = \"...\"\n\"\"\".strip()\n    with tempfile.NamedTemporaryFile(mode=\"w\", delete=False) as f:\n        f.write(example_config)\n    config = Config.from_toml(f.name)\n    config.gptengineer_app = _GptEngineerAppConfig(\n        project_id=\"...\",\n        openapi=[_OpenApiConfig(url=\"https://api.gptengineer.app/openapi.json\")],\n    )\n    config.to_toml(f.name)\n    assert Config.from_toml(f.name) == config\n\n\n@pytest.mark.parametrize(\n    \"input_dict,expected\",\n    [\n        ({\"a\": 1, \"b\": None}, {\"a\": 1}),\n        ({\"a\": 1, \"b\": {\"c\": None, \"d\": 2}}, {\"a\": 1, \"b\": {\"d\": 2}}),\n        ({\"a\": 1, \"b\": {}}, {\"a\": 1}),\n        ({\"a\": 1, \"b\": {\"c\": None}}, {\"a\": 1}),\n        (\n            {\"a\": {\"b\": {\"c\": None}}, \"d\": {\"e\": {\"f\": 2}}, \"g\": None},\n            {\"d\": {\"e\": {\"f\": 2}}},\n        ),\n        (\n            {\"a\": 1, \"b\": {\"c\": None, \"d\": {\"e\": None, \"f\": {}}}, \"g\": {\"h\": 2}},\n            {\"a\": 1, \"g\": {\"h\": 2}},\n        ),\n    ],\n)\ndef test_filter_none(input_dict, expected):\n    assert filter_none(input_dict) == expected\n", "tests/mock_ai.py": "from typing import Any, List, Optional\n\n\nclass MockAI:\n    def __init__(self, response: List):\n        self.responses = iter(response)\n\n    def start(self, system: str, user: Any, *, step_name: str) -> List[str]:\n        return [next(self.responses)]\n\n    def next(\n        self, messages: List[str], prompt: Optional[str] = None, *, step_name: str\n    ) -> List[str]:\n        return [next(self.responses)]\n", "tests/__init__.py": "", "tests/test_install.py": "\"\"\"\nTests for successful installation of the package.\n\"\"\"\n\nimport shutil\nimport subprocess\nimport sys\nimport venv\n\nfrom pathlib import Path\n\nimport pytest\n\n# Define the directory for the virtual environment.\nVENV_DIR = \"./venv_test_installation\"\n\n\n@pytest.fixture(scope=\"module\", autouse=True)\ndef venv_setup_teardown():\n    \"\"\"\n    A pytest fixture that sets up and tears down a virtual environment for testing.\n    This fixture is automatically used for all tests in this module.\n\n    The fixture:\n    - Creates a virtual environment.\n    - Installs Poetry in the virtual environment.\n    - Installs dependencies using Poetry.\n    - Cleans up by removing the virtual environment after tests are completed.\n    \"\"\"\n    try:\n        # Create a virtual environment with pip available.\n        venv.create(VENV_DIR, with_pip=True, clear=True)\n\n        # Install Poetry in the virtual environment.\n        subprocess.run(\n            [f\"{VENV_DIR}/bin/python\", \"-m\", \"pip\", \"install\", \"poetry\"], check=True\n        )\n\n        # Install the package and its dependencies using Poetry.\n        subprocess.run([f\"{VENV_DIR}/bin/poetry\", \"install\"], cwd=\".\", check=True)\n\n        # Provide the setup environment to the test functions.\n        yield\n    except Exception as e:\n        # Skip tests if the environment setup fails.\n        pytest.skip(f\"Could not create venv or install dependencies: {str(e)}\")\n    finally:\n        # Clean up by removing the virtual environment after tests.\n        shutil.rmtree(VENV_DIR)\n\n\ndef test_installation():\n    \"\"\"\n    Test to ensure that the package can be installed using Poetry in the virtual environment.\n    \"\"\"\n    # Determine the correct Poetry executable path based on the operating system.\n    poetry_executable = (\n        f\"{VENV_DIR}/bin/poetry\"\n        if sys.platform != \"win32\"\n        else f\"{VENV_DIR}/Scripts/poetry.exe\"\n    )\n\n    # Run Poetry install and capture its output.\n    result = subprocess.run([poetry_executable, \"install\"], capture_output=True)\n\n    # Assert that the installation was successful.\n    assert (\n        result.returncode == 0\n    ), f\"Install via poetry failed: {result.stderr.decode()}\"\n\n\ndef test_cli_execution():\n    \"\"\"\n    Test to verify that the command-line interface (CLI) of the package works as expected.\n    This test assumes that the 'gpt-engineer' command is available and operational after installation.\n    \"\"\"\n    # Run the 'gpt-engineer' command with the '--help' option and capture its output.\n    result = subprocess.run(\n        args=[\"gpt-engineer\", \"--help\"], capture_output=True, text=True\n    )\n\n    # Assert that the CLI command executed successfully.\n    assert (\n        result.returncode == 0\n    ), f\"gpt-engineer command failed with message: {result.stderr}\"\n\n\n@pytest.mark.requires_key\ndef test_installed_main_execution(tmp_path, monkeypatch):\n    # Ignore git installation check\n    monkeypatch.setattr(\"gpt_engineer.core.git.is_git_installed\", lambda: False)\n    tmp_path = Path(tmp_path)\n    p = tmp_path / \"projects/example\"\n    p.mkdir(parents=True)\n    (p / \"prompt\").write_text(\"make a program that prints the outcome of 4+4\")\n    proc = subprocess.Popen(\n        [\"gpte\", str(p)],\n        stdin=subprocess.PIPE,\n        stdout=subprocess.PIPE,\n        text=True,\n        cwd=tmp_path,\n    )\n\n    inputs = \"Y\\nn\"\n    output, _ = proc.communicate(inputs)\n\n    assert \"8\" in output\n", "tests/tools/example_snake_files.py": "PYTHON = \"\"\"\nimport keyboard\nimport random\nfrom dataclasses import dataclass\n\n\nclass View:\n    def __init__(self, game):\n        self.game = game\n\n    def render(self):\n        # Print the game state\n        for y in range(10):\n            for x in range(10):\n                if Point(x, y) in self.game.snake:\n                    print('S', end='')\n                elif Point(x, y) == self.game.food:\n                    print('F', end='')\n                else:\n                    print('.', end='')\n            print()\n        print()\n\n\n@dataclass\nclass Point:\n    x: int\n    y: int\n\nclass Game:\n    def __init__(self):\n        self.snake = [Point(5, 5)]\n        self.food = self.generate_food()\n        self.is_running = True\n\n    def generate_food(self):\n        return Point(random.randint(0, 10), random.randint(0, 10))\n\n    def update(self):\n        # Move the snake\n        self.snake.move()\n\n        # Check for collision with food\n        if self.snake.head == self.food:\n            self.snake.grow()\n            self.food = self.generate_food()\n\n        # Check for collision with boundaries\n        if not (0 <= self.snake.head.x < 10 and 0 <= self.snake.head.y < 10):\n            self.is_running = False\n\nclass Controller:\n    def __init__(self, game, view):\n        self.game = game\n        self.view = view\n\n    def handle_input(self):\n        if keyboard.is_pressed('up'):\n            self.game.move('up')\n        elif keyboard.is_pressed('down'):\n            self.game.move('down')\n        elif keyboard.is_pressed('left'):\n            self.game.move('left')\n        elif keyboard.is_pressed('right'):\n            self.game.move('right')\n\ndef main():\n    game = Game()\n    view = View(game)\n    controller = Controller(game, view)\n\n    while game.is_running:\n        controller.handle_input()\n        game.update()\n        view.render()\n\nif __name__ == \"__main__\":\n    main()\n\"\"\"\n\n\nHTML = \"\"\"\n<!DOCTYPE html>\n<html>\n<head>\n    <title>Snake Game</title>\n    <link rel=\"stylesheet\" type=\"text/css\" href=\"styles.css\">\n</head>\n<body>\n    <h1>Snake Game</h1>\n    <canvas id=\"gameCanvas\" width=\"400\" height=\"400\"></canvas>\n    <h2 id=\"score\">Score: 0</h2>\n    <script src=\"script.js\"></script>\n</body>\n</html>\n\"\"\"\n\nCSS = \"\"\"\nbody {\n    display: flex;\n    justify-content: center;\n    align-items: center;\n    height: 100vh;\n    background-color: #000;\n    color: #fff;\n    font-family: Arial, sans-serif;\n}\n\n#gameCanvas {\n    border: 1px solid #fff;\n}\n\nh1, h2 {\n    text-align: center;\n}\n\n\"\"\"\n\nJAVASCRIPT = \"\"\"\nvar canvas = document.getElementById('gameCanvas');\nvar context = canvas.getContext('2d');\n\nvar box = 20;\nvar score = 0;\nvar food = null;\n\nvar snake = [];\nsnake[0] = { x: 10 * box, y: 10 * box };\n\ndocument.addEventListener('keydown', direction);\nvar d;\n\nfunction direction(event) {\n    if (event.keyCode == 37 && d != \"RIGHT\") {\n        d = \"LEFT\";\n    } else if (event.keyCode == 38 && d != \"DOWN\") {\n        d = \"UP\";\n    } else if (event.keyCode == 39 && d != \"LEFT\") {\n        d = \"RIGHT\";\n    } else if (event.keyCode == 40 && d != \"UP\") {\n        d = \"DOWN\";\n    }\n}\n\nfunction draw() {\n    context.clearRect(0, 0, canvas.width, canvas.height);\n    for (var i = 0; i < snake.length; i++) {\n        context.fillStyle = (i == 0) ? \"green\" : \"white\";\n        context.fillRect(snake[i].x, snake[i].y, box, box);\n    }\n\n    if (food == null) {\n        food = {\n            x: Math.floor(Math.random() * 19 + 1) * box,\n            y: Math.floor(Math.random() * 19 + 1) * box\n        }\n    }\n\n    context.fillStyle = \"red\";\n    context.fillRect(food.x, food.y, box, box);\n\n    var snakeX = snake[0].x;\n    var snakeY = snake[0].y;\n\n    if (d == \"LEFT\") snakeX -= box;\n    if (d == \"UP\") snakeY -= box;\n    if (d == \"RIGHT\") snakeX += box;\n    if (d == \"DOWN\") snakeY += box;\n\n    if (snakeX == food.x && snakeY == food.y) {\n        score++;\n        food = null;\n    } else {\n        snake.pop();\n    }\n\n    var newHead = {\n        x: snakeX,\n        y: snakeY\n    }\n\n    if (snakeX < 0 || snakeY < 0 || snakeX > 19 * box || snakeY > 19 * box || collision(newHead, snake)) {\n        clearInterval(game);\n    }\n\n    snake.unshift(newHead);\n\n    document.getElementById('score').innerHTML = \"Score: \" + score;\n}\n\nfunction collision(head, array) {\n    for (var i = 0; i < array.length; i++) {\n        if (head.x == array[i].x && head.y == array[i].y) {\n            return true;\n        }\n    }\n    return false;\n}\n\nvar game = setInterval(draw, 100);\n\"\"\"\n\nJAVA = \"\"\"\nimport java.awt.Color;\nimport java.awt.Dimension;\nimport java.awt.Font;\nimport java.awt.FontMetrics;\nimport java.awt.Graphics;\nimport java.awt.Image;\nimport java.awt.Toolkit;\nimport java.awt.event.ActionEvent;\nimport java.awt.event.ActionListener;\nimport java.awt.event.KeyAdapter;\nimport java.awt.event.KeyEvent;\nimport javax.swing.ImageIcon;\nimport javax.swing.JPanel;\nimport javax.swing.Timer;\n\npublic class SnakeGame extends JPanel implements ActionListener {\n\n    private final int B_WIDTH = 300;\n    private final int B_HEIGHT = 300;\n    private final int DOT_SIZE = 10;\n    private final int ALL_DOTS = 900;\n    private final int RAND_POS = 29;\n    private final int DELAY = 140;\n\n    private final int x[] = new int[ALL_DOTS];\n    private final int y[] = new int[ALL_DOTS];\n\n    private int dots;\n    private int apple_x;\n    private int apple_y;\n\n    private boolean leftDirection = false;\n    private boolean rightDirection = true;\n    private boolean upDirection = false;\n    private boolean downDirection = false;\n    private boolean inGame = true;\n\n    private Timer timer;\n    private Image ball;\n    private Image apple;\n    private Image head;\n\n    public SnakeGame() {\n\n        initBoard();\n    }\n\n    private void initBoard() {\n\n        addKeyListener(new TAdapter());\n        setBackground(Color.black);\n        setFocusable(true);\n\n        setPreferredSize(new Dimension(B_WIDTH, B_HEIGHT));\n        loadImages();\n        initGame();\n    }\n\n    private void loadImages() {\n\n        ImageIcon iid = new ImageIcon(\"src/resources/dot.png\");\n        ball = iid.getImage();\n\n        ImageIcon iia = new ImageIcon(\"src/resources/apple.png\");\n        apple = iia.getImage();\n\n        ImageIcon iih = new ImageIcon(\"src/resources/head.png\");\n        head = iih.getImage();\n    }\n\n    private void initGame() {\n\n        dots = 3;\n\n        for (int z = 0; z < dots; z++) {\n            x[z] = 50 - z * 10;\n            y[z] = 50;\n        }\n\n        locateApple();\n\n        timer = new Timer(DELAY, this);\n        timer.start();\n    }\n\n    @Override\n    public void paintComponent(Graphics g) {\n        super.paintComponent(g);\n\n        doDrawing(g);\n    }\n\n    private void doDrawing(Graphics g) {\n\n        if (inGame) {\n\n            g.drawImage(apple, apple_x, apple_y, this);\n\n            for (int z = 0; z < dots; z++) {\n                if (z == 0) {\n                    g.drawImage(head, x[z], y[z], this);\n                } else {\n                    g.drawImage(ball, x[z], y[z], this);\n                }\n            }\n\n            Toolkit.getDefaultToolkit().sync();\n\n        } else {\n\n            gameOver(g);\n        }\n    }\n\n    private void gameOver(Graphics g) {\n\n        String msg = \"Game Over\";\n        Font small = new Font(\"Helvetica\", Font.BOLD, 14);\n        FontMetrics metr = getFontMetrics(small);\n\n        g.setColor(Color.white);\n        g.setFont(small);\n        g.drawString(msg, (B_WIDTH - metr.stringWidth(msg)) / 2, B_HEIGHT / 2);\n    }\n\n    private void checkApple() {\n\n        if ((x[0] == apple_x) && (y[0] == apple_y)) {\n\n            dots++;\n            locateApple();\n        }\n    }\n\n    private void move() {\n\n        for (int z = dots; z > 0; z--) {\n            x[z] = x[(z - 1)];\n            y[z] = y[(z - 1)];\n        }\n\n        if (leftDirection) {\n            x[0] -= DOT_SIZE;\n        }\n\n        if (rightDirection) {\n            x[0] += DOT_SIZE;\n        }\n\n        if (upDirection) {\n            y[0] -= DOT_SIZE;\n        }\n\n        if (downDirection) {\n            y[0] += DOT_SIZE;\n        }\n    }\n\n    private void checkCollision() {\n\n        for (int z = dots; z > 0; z--) {\n\n            if ((z > 4) && (x[0] == x[z]) && (y[0] == y[z])) {\n                inGame = false;\n            }\n        }\n\n        if (y[0] >= B_HEIGHT) {\n            inGame = false;\n        }\n\n        if (y[0] < 0) {\n            inGame = false;\n        }\n\n        if (x[0] >= B_WIDTH) {\n            inGame = false;\n        }\n\n        if (x[0] < 0) {\n            inGame = false;\n        }\n\n        if (!inGame) {\n            timer.stop();\n        }\n    }\n\n    private void locateApple() {\n\n        int r = (int) (Math.random() * RAND_POS);\n        apple_x = ((r * DOT_SIZE));\n\n        r = (int) (Math.random() * RAND_POS);\n        apple_y = ((r * DOT_SIZE));\n    }\n\n    @Override\n    public void actionPerformed(ActionEvent e) {\n\n        if (inGame) {\n\n            checkApple();\n            checkCollision();\n            move();\n        }\n\n        repaint();\n    }\n\n    private class TAdapter extends KeyAdapter {\n\n        @Override\n        public void keyPressed(KeyEvent e) {\n\n            int key = e.getKeyCode();\n\n            if ((key == KeyEvent.VK_LEFT) && (!rightDirection)) {\n                leftDirection = true;\n                upDirection = false;\n                downDirection = false;\n            }\n\n            if ((key == KeyEvent.VK_RIGHT) && (!leftDirection)) {\n                rightDirection = true;\n                upDirection = false;\n                downDirection = false;\n            }\n\n            if ((key == KeyEvent.VK_UP) && (!downDirection)) {\n                upDirection = true;\n                rightDirection = false;\n                leftDirection = false;\n            }\n\n            if ((key == KeyEvent.VK_DOWN) && (!upDirection)) {\n                downDirection = true;\n                rightDirection = false;\n                leftDirection = false;\n            }\n        }\n    }\n}\n\"\"\"\n\nC_SHARP = \"\"\"\nusing System;\nusing System.Collections.Generic;\nusing System.Linq;\nusing System.Threading;\nusing System.Threading.Tasks;\n\nnamespace SnakeGame\n{\n    // Model\n    public class Game\n    {\n        public Snake Snake { get; set; }\n        public Point Food { get; set; }\n        public int Score { get; set; }\n        public bool Over { get; set; }\n\n        public Game()\n        {\n            Snake = new Snake();\n            Food = new Point();\n            Score = 0;\n            Over = false;\n        }\n    }\n\n    public class Snake\n    {\n        public Queue<Point> Body { get; set; }\n        public Direction Direction { get; set; }\n\n        public Snake()\n        {\n            Body = new Queue<Point>();\n            Direction = Direction.Right;\n        }\n    }\n\n    public class Point\n    {\n        public int X { get; set; }\n        public int Y { get; set; }\n    }\n\n    public enum Direction\n    {\n        Up,\n        Down,\n        Left,\n        Right\n    }\n\n    // View\n    public class GameView\n    {\n        public void Draw(Game game)\n        {\n            Console.Clear();\n            foreach (var point in game.Snake.Body)\n            {\n                Console.SetCursorPosition(point.X, point.Y);\n                Console.Write(\"O\");\n            }\n            Console.SetCursorPosition(game.Food.X, game.Food.Y);\n            Console.Write(\"F\");\n            Console.SetCursorPosition(0, 0);\n            Console.Write(\"Score: \" + game.Score);\n        }\n    }\n\n    // Controller\n    public class GameController\n    {\n        private Game game;\n        private GameView view;\n\n        public GameController(Game game, GameView view)\n        {\n            this.game = game;\n            this.view = view;\n        }\n\n        public void Start()\n        {\n            while (!game.Over)\n            {\n                Thread.Sleep(100);\n                MoveSnake();\n                CheckCollision();\n                view.Draw(game);\n            }\n        }\n\n        private void MoveSnake()\n        {\n            var head = game.Snake.Body.Last();\n            var newHead = new Point { X = head.X, Y = head.Y };\n            switch (game.Snake.Direction)\n            {\n                case Direction.Up:\n                    newHead.Y--;\n                    break;\n                case Direction.Down:\n                    newHead.Y++;\n                    break;\n                case Direction.Left:\n                    newHead.X--;\n                    break;\n                case Direction.Right:\n                    newHead.X++;\n                    break;\n            }\n            game.Snake.Body.Enqueue(newHead);\n            if (newHead.X == game.Food.X && newHead.Y == game.Food.Y)\n            {\n                game.Score++;\n                game.Food = new Point { X = new Random().Next(1, 10), Y = new Random().Next(1, 10) };\n            }\n            else\n            {\n                game.Snake.Body.Dequeue();\n            }\n        }\n\n        private void CheckCollision()\n        {\n            var head = game.Snake.Body.Last();\n            if (head.X < 0 || head.Y < 0 || head.X >= 10 || head.Y >= 10)\n            {\n                game.Over = true;\n            }\n            if (game.Snake.Body.Take(game.Snake.Body.Count - 1).Any(p => p.X == head.X && p.Y == head.Y))\n            {\n                game.Over = true;\n            }\n        }\n    }\n\n    class Program\n    {\n        static void Main(string[] args)\n        {\n            var game = new Game();\n            var view = new GameView();\n            var controller = new GameController(game, view);\n            controller.Start();\n        }\n    }\n}\n\"\"\"\n\nTYPESCRIPT = \"\"\"\n// Importing necessary modules\nimport { Application, Graphics, Keyboard } from 'pixi.js';\n\n// Defining the Model class\nclass Model {\n    // The snake's body is represented as an array of points\n    body: Array<{x: number, y: number}>;\n\n    constructor() {\n        this.body = [{x: 0, y: 0}];\n    }\n\n    // Method to move the snake\n    move(direction: {x: number, y: number}) {\n        // Add a new head in the direction of movement\n        this.body.unshift({\n            x: this.body[0].x + direction.x,\n            y: this.body[0].y + direction.y\n        });\n\n        // Remove the tail\n        this.body.pop();\n    }\n}\n\n// Defining the View class\nclass View {\n    // The view needs a reference to the model and the PIXI application\n    model: Model;\n    app: Application;\n    graphics: Graphics;\n\n    constructor(model: Model, app: Application) {\n        this.model = model;\n        this.app = app;\n        this.graphics = new Graphics();\n        this.app.stage.addChild(this.graphics);\n    }\n\n    // Method to render the snake\n    render() {\n        // Clear the previous frame\n        this.graphics.clear();\n\n        // Draw each part of the snake's body\n        for (let part of this.model.body) {\n            this.graphics.beginFill(0xFFFFFF);\n            this.graphics.drawRect(part.x * 10, part.y * 10, 10, 10);\n            this.graphics.endFill();\n        }\n    }\n}\n\n// Defining the Controller class\nclass Controller {\n    // The controller needs a reference to the model and the view\n    model: Model;\n    view: View;\n    direction: {x: number, y: number};\n\n    constructor(model: Model, view: View) {\n        this.model = model;\n        this.view = view;\n        this.direction = {x: 1, y: 0};\n\n        // Set up keyboard controls\n        window.addEventListener('keydown', (e) => this.handleKeydown(e));\n    }\n\n    // Method to handle keyboard input\n    handleKeydown(event: KeyboardEvent) {\n        switch (event.key) {\n            case 'ArrowUp':\n                this.direction = {x: 0, y: -1};\n                break;\n            case 'ArrowDown':\n                this.direction = {x: 0, y: 1};\n                break;\n            case 'ArrowLeft':\n                this.direction = {x: -1, y: 0};\n                break;\n            case 'ArrowRight':\n                this.direction = {x: 1, y: 0};\n                break;\n        }\n    }\n\n    // Method to update the game state\n    update() {\n        this.model.move(this.direction);\n        this.view.render();\n    }\n}\n\n// Create the PIXI application\nlet app = new Application({width: 800, height: 600});\n\n// Create the MVC components\nlet model = new Model();\nlet view = new View(model, app);\nlet controller = new Controller(model, view);\n\n// Start the game loop\nsetInterval(() => controller.update(), 100);\n\"\"\"\n\nRUBY = \"\"\"\nrequire 'io/console'\n\n# Model\nclass Game\n  attr_accessor :score, :snake, :food\n\n  def initialize\n    @score = 0\n    @snake = [[2, 2]]\n    @food = [6, 4]\n  end\n\n  def move(direction)\n    head = @snake.first.dup\n    case direction\n    when 'up'\n      head[0] -= 1\n    when 'down'\n      head[0] += 1\n    when 'left'\n      head[1] -= 1\n    when 'right'\n      head[1] += 1\n    end\n    @snake.unshift(head)\n\n    if @snake.first == @food\n      @score += 1\n      @food = [rand(1..8), rand(1..8)]\n    else\n      @snake.pop\n    end\n  end\n\n  def game_over?\n    head = @snake.first\n    @snake[1..-1].include?(head) || head[0] == 0 || head[1] == 0 || head[0] == 9 || head[1] == 9\n  end\nend\n\n# View\nclass View\n  def render(game)\n    system('clear')\n    puts \"Score: #{game.score}\"\n    (0..9).each do |i|\n      (0..9).each do |j|\n        if game.snake.include?([i, j])\n          print 'S'\n        elsif game.food == [i, j]\n          print 'F'\n        else\n          print '.'\n        end\n      end\n      puts\n    end\n  end\nend\n\n# Controller\nclass Controller\n  def initialize\n    @game = Game.new\n    @view = View.new\n    @direction = 'right'\n  end\n\n  def play\n    loop do\n      @view.render(@game)\n      break if @game.game_over?\n\n      input = IO.console.getch\n      case input\n      when 'w'\n        @direction = 'up'\n      when 's'\n        @direction = 'down'\n      when 'a'\n        @direction = 'left'\n      when 'd'\n        @direction = 'right'\n      end\n      @game.move(@direction)\n    end\n    puts \"Game Over! Your score was #{@game.score}.\"\n  end\nend\n\nController.new.play\n\n\"\"\"\n\nPHP = \"\"\"\n<?php\n// Model\nclass Snake {\n    public $body;\n    public $direction;\n\n    public function __construct() {\n        $this->body = array(array(2, 0), array(1, 0), array(0, 0));\n        $this->direction = 'right';\n    }\n\n    public function move() {\n        $head = current($this->body);\n        switch($this->direction) {\n            case 'right':\n                $this->body[] = array($head[0] + 1, $head[1]);\n                break;\n            case 'left':\n                $this->body[] = array($head[0] - 1, $head[1]);\n                break;\n            case 'up':\n                $this->body[] = array($head[0], $head[1] - 1);\n                break;\n            case 'down':\n                $this->body[] = array($head[0], $head[1] + 1);\n                break;\n        }\n        array_shift($this->body);\n    }\n\n    public function changeDirection($new_direction) {\n        $this->direction = $new_direction;\n    }\n}\n\n// View\nclass GameView {\n    public function render($snake) {\n        $output = '';\n        for ($y=0; $y<20; $y++) {\n            for ($x=0; $x<20; $x++) {\n                if (in_array(array($x, $y), $snake->body)) {\n                    $output .= 'X';\n                } else {\n                    $output .= ' ';\n                }\n            }\n            $output .= \"\\n\";\n        }\n        echo $output;\n    }\n}\n\n// Controller\nclass GameController {\n    public $snake;\n    public $view;\n\n    public function __construct() {\n        $this->snake = new Snake();\n        $this->view = new GameView();\n    }\n\n    public function start() {\n        while (true) {\n            $this->view->render($this->snake);\n            $this->snake->move();\n            sleep(1);\n        }\n    }\n\n    public function changeDirection($new_direction) {\n        $this->snake->changeDirection($new_direction);\n    }\n}\n\n// Game loop\n$game = new GameController();\n$game->start();\n?>\n\"\"\"\n\nSWIFT = \"\"\"\nimport Foundation\nimport Cocoa\n\n// MARK: - Model\nstruct Point {\n    var x: Int\n    var y: Int\n}\n\nclass Snake {\n    var body: [Point]\n    var direction: Direction\n\n    init(startPoint: Point) {\n        body = [startPoint]\n        direction = .right\n    }\n\n    func move() {\n        let head = body.first!\n        var newHead = head\n\n        switch direction {\n        case .up:\n            newHead.y += 1\n        case .down:\n            newHead.y -= 1\n        case .left:\n            newHead.x -= 1\n        case .right:\n            newHead.x += 1\n        }\n\n        body.insert(newHead, at: 0)\n        body.removeLast()\n    }\n\n    func grow() {\n        let tail = body.last!\n        body.append(tail)\n    }\n}\n\nenum Direction {\n    case up\n    case down\n    case left\n    case right\n}\n\n// MARK: - View\nclass GameView {\n    func draw(snake: Snake) {\n        for point in snake.body {\n            print(\"O\", terminator: \"\")\n        }\n        print(\"\\n\")\n    }\n}\n\n// MARK: - Controller\nclass GameController {\n    var snake: Snake\n    var view: GameView\n\n    init() {\n        snake = Snake(startPoint: Point(x: 0, y: 0))\n        view = GameView()\n    }\n\n    func start() {\n        while true {\n            snake.move()\n            view.draw(snake: snake)\n            sleep(1)\n        }\n    }\n\n    func handleKey(key: String) {\n        switch key {\n        case \"w\":\n            snake.direction = .up\n        case \"s\":\n            snake.direction = .down\n        case \"a\":\n            snake.direction = .left\n        case \"d\":\n            snake.direction = .right\n        default:\n            break\n        }\n    }\n}\n\n// MARK: - Main\nlet gameController = GameController()\ngameController.start()\n\"\"\"\n\nGO = \"\"\"\npackage main\n\nimport (\n\t\"fmt\"\n\t\"os\"\n\t\"os/exec\"\n\t\"time\"\n\t\"math/rand\"\n\t\"bufio\"\n\t\"syscall\"\n\t\"unsafe\"\n)\n\n// Model\ntype Point struct {\n\tX int\n\tY int\n}\n\ntype Snake struct {\n\tBody []Point\n\tDir  string\n}\n\ntype Game struct {\n\tSnake       Snake\n\tFood        Point\n\tScore       int\n\tWidth       int\n\tHeight      int\n}\n\n// View\nfunc (game *Game) Render() {\n\tclearScreen()\n\tfor y := 0; y < game.Height; y++ {\n\t\tfor x := 0; x < game.Width; x++ {\n\t\t\tpoint := Point{X: x, Y: y}\n\t\t\tswitch {\n\t\t\tcase point == game.Food:\n\t\t\t\tfmt.Print(\"F\")\n\t\t\tcase game.Snake.Contains(point):\n\t\t\t\tfmt.Print(\"S\")\n\t\t\tdefault:\n\t\t\t\tfmt.Print(\" \")\n\t\t\t}\n\t\t}\n\t\tfmt.Println()\n\t}\n\tfmt.Println(\"Score:\", game.Score)\n}\n\n// Controller\nfunc (game *Game) Update() {\n\thead := game.Snake.Body[0]\n\tswitch game.Snake.Dir {\n\tcase \"up\":\n\t\thead.Y--\n\tcase \"down\":\n\t\thead.Y++\n\tcase \"left\":\n\t\thead.X--\n\tcase \"right\":\n\t\thead.X++\n\t}\n\n\tif head.X < 0 || head.Y < 0 || head.X >= game.Width || head.Y >= game.Height {\n\t\tgame.Score = -1\n\t\treturn\n\t}\n\n\tif game.Snake.Contains(head) {\n\t\tgame.Score = -1\n\t\treturn\n\t}\n\n\tif head == game.Food {\n\t\tgame.Score++\n\t\tgame.Food = Point{rand.Intn(game.Width), rand.Intn(game.Height)}\n\t} else {\n\t\tgame.Snake.Body = game.Snake.Body[:len(game.Snake.Body)-1]\n\t}\n\n\tgame.Snake.Body = append([]Point{head}, game.Snake.Body...)\n}\n\nfunc (snake *Snake) Contains(point Point) bool {\n\tfor _, bodyPoint := range snake.Body {\n\t\tif bodyPoint == point {\n\t\t\treturn true\n\t\t}\n\t}\n\treturn false\n}\n\nfunc clearScreen() {\n\tcmd := exec.Command(\"clear\")\n\tcmd.Stdout = os.Stdout\n\tcmd.Run()\n}\n\nfunc main() {\n\tgame := &Game{\n\t\tSnake: Snake{\n\t\t\tBody: []Point{{10, 10}},\n\t\t\tDir:  \"right\",\n\t\t},\n\t\tFood:   Point{15, 15},\n\t\tScore:  0,\n\t\tWidth:  20,\n\t\tHeight: 20,\n\t}\n\n\tgo func() {\n\t\treader := bufio.NewReader(os.Stdin)\n\t\tfor {\n\t\t\tchar, _, err := reader.ReadRune()\n\t\t\tif err != nil {\n\t\t\t\tpanic(err)\n\t\t\t}\n\n\t\t\tswitch char {\n\t\t\tcase 'w':\n\t\t\t\tgame.Snake.Dir = \"up\"\n\t\t\tcase 's':\n\t\t\t\tgame.Snake.Dir = \"down\"\n\t\t\tcase 'a':\n\t\t\t\tgame.Snake.Dir = \"left\"\n\t\t\tcase 'd':\n\t\t\t\tgame.Snake.Dir = \"right\"\n\t\t\t}\n\t\t}\n\t}()\n\n\tfor game.Score >= 0 {\n\t\tgame.Render()\n\t\ttime.Sleep(time.Second / 5)\n\t\tgame.Update()\n\t}\n}\n\"\"\"\n\nKOTLIN = \"\"\"\nimport java.awt.Color\nimport java.awt.Dimension\nimport java.awt.Font\nimport java.awt.FontMetrics\nimport java.awt.Graphics\nimport java.awt.Image\nimport java.awt.Toolkit\nimport java.awt.event.ActionEvent\nimport java.awt.event.ActionListener\nimport java.awt.event.KeyAdapter\nimport java.awt.event.KeyEvent\nimport javax.swing.ImageIcon\nimport javax.swing.JPanel\nimport javax.swing.Timer\n\nclass Board : JPanel(), ActionListener {\n\n    private val B_WIDTH = 300\n    private val B_HEIGHT = 300\n    private val DOT_SIZE = 10\n    private val ALL_DOTS = 900\n    private val RAND_POS = 29\n    private val DELAY = 140\n\n    private val x = IntArray(ALL_DOTS)\n    private val y = IntArray(ALL_DOTS)\n\n    private var dots: Int = 0\n    private var apple_x: Int = 0\n    private var apple_y: Int = 0\n\n    private var leftDirection = false\n    private var rightDirection = true\n    private var upDirection = false\n    private var downDirection = false\n    private var inGame = true\n\n    private lateinit var timer: Timer\n    private lateinit var apple: Image\n    private lateinit var dot: Image\n    private lateinit var head: Image\n\n    init {\n        initBoard()\n    }\n\n    private fun initBoard() {\n\n        addKeyListener(TAdapter())\n        background = Color.black\n        isFocusable = true\n\n        preferredSize = Dimension(B_WIDTH, B_HEIGHT)\n        loadImages()\n        initGame()\n    }\n\n    private fun loadImages() {\n\n        val iid = ImageIcon(\"src/resources/apple.png\")\n        apple = iid.image\n\n        val iid2 = ImageIcon(\"src/resources/dot.png\")\n        dot = iid2.image\n\n        val iid3 = ImageIcon(\"src/resources/head.png\")\n        head = iid3.image\n    }\n\n    private fun initGame() {\n\n        dots = 3\n\n        for (z in 0 until dots) {\n            x[z] = 50 - z * 10\n            y[z] = 50\n        }\n\n        locateApple()\n\n        timer = Timer(DELAY, this)\n        timer.start()\n    }\n\n    override fun paintComponent(g: Graphics) {\n        super.paintComponent(g)\n\n        doDrawing(g)\n    }\n\n    private fun doDrawing(g: Graphics) {\n\n        if (inGame) {\n\n            g.drawImage(apple, apple_x, apple_y, this)\n\n            for (z in 0 until dots) {\n                if (z == 0) {\n                    g.drawImage(head, x[z], y[z], this)\n                } else {\n                    g.drawImage(dot, x[z], y[z], this)\n                }\n            }\n\n            Toolkit.getDefaultToolkit().sync()\n\n        } else {\n\n            gameOver(g)\n        }\n    }\n\n    private fun gameOver(g: Graphics) {\n\n        val msg = \"Game Over\"\n        val font = Font(\"Helvetica\", Font.BOLD, 14)\n        val metrics: FontMetrics = this.getFontMetrics(font)\n\n        g.color = Color.white\n        g.font = font\n        g.drawString(msg, (B_WIDTH - metrics.stringWidth(msg)) / 2, B_HEIGHT / 2)\n    }\n\n    private fun checkApple() {\n\n        if (x[0] == apple_x && y[0] == apple_y) {\n\n            dots++\n            locateApple()\n        }\n    }\n\n    private fun move() {\n\n        for (z in dots downTo 1) {\n            x[z] = x[z - 1]\n            y[z] = y[z - 1]\n        }\n\n        if (leftDirection) {\n            x[0] -= DOT_SIZE\n        }\n\n        if (rightDirection) {\n            x[0] += DOT_SIZE\n        }\n\n        if (upDirection) {\n            y[0] -= DOT_SIZE\n        }\n\n        if (downDirection) {\n            y[0] += DOT_SIZE\n        }\n    }\n\n    private fun checkCollision() {\n\n        for (z in dots downTo 1) {\n            if (z > 4 && x[0] == x[z] && y[0] == y[z]) {\n                inGame = false\n            }\n        }\n\n        if (y[0] >= B_HEIGHT) {\n            inGame = false\n        }\n\n        if (y[0] < 0) {\n            inGame = false\n        }\n\n        if (x[0] >= B_WIDTH) {\n            inGame = false\n        }\n\n        if (x[0] < 0) {\n            inGame = false\n        }\n\n        if (!inGame) {\n            timer.stop()\n        }\n    }\n\n    private fun locateApple() {\n\n        val r = (Math.random() * RAND_POS).toInt()\n        apple_x = r * DOT_SIZE\n\n        r = (Math.random() * RAND_POS).toInt()\n        apple_y = r * DOT_SIZE\n    }\n\n    override fun actionPerformed(e: ActionEvent) {\n\n        if (inGame) {\n\n            checkApple()\n            checkCollision()\n            move()\n        }\n\n        repaint()\n    }\n\n    private inner class TAdapter : KeyAdapter() {\n\n        override fun keyPressed(e: KeyEvent) {\n\n            val key = e.keyCode\n\n            if (key == KeyEvent.VK_LEFT && !rightDirection) {\n                leftDirection = true\n                upDirection = false\n                downDirection = false\n            }\n\n            if (key == KeyEvent.VK_RIGHT && !leftDirection) {\n                rightDirection = true\n                upDirection = false\n                downDirection = false\n            }\n\n            if (key == KeyEvent.VK_UP && !downDirection) {\n                upDirection = true\n                rightDirection = false\n                leftDirection = false\n            }\n\n            if (key == KeyEvent.VK_DOWN && !upDirection) {\n                downDirection = true\n                rightDirection = false\n                leftDirection = false\n            }\n        }\n    }\n}\n\"\"\"\nRUST = \"\"\"\nextern crate termion;\n\nuse std::io;\nuse std::io::stdout;\nuse std::io::Write;\nuse std::thread;\nuse std::time::Duration;\nuse termion::raw::IntoRawMode;\nuse termion::input::TermRead;\nuse termion::event::Key;\n\n// Define the size of the game board\nconst BOARD_SIZE: usize = 10;\n\n// Define the game state\nstruct GameState {\n    snake: Snake,\n    food: Food,\n}\n\n// Define the snake\nstruct Snake {\n    body: Vec<(usize, usize)>,\n    direction: Direction,\n}\n\n// Define the food\nstruct Food {\n    position: (usize, usize),\n}\n\n// Define the possible directions the snake can move\nenum Direction {\n    Up,\n    Down,\n    Left,\n    Right,\n}\n\n// Implement the game state\nimpl GameState {\n    fn new() -> GameState {\n        GameState {\n            snake: Snake::new(),\n            food: Food::new(),\n        }\n    }\n\n    // Update the game state\n    fn update(&mut self) {\n        self.snake.move();\n        if self.snake.eats(&self.food) {\n            self.food = Food::new();\n        }\n    }\n}\n\n// Implement the snake\nimpl Snake {\n    fn new() -> Snake {\n        Snake {\n            body: vec![(BOARD_SIZE / 2, BOARD_SIZE / 2)],\n            direction: Direction::Right,\n        }\n    }\n\n    // Move the snake in the current direction\n    fn move(&mut self) {\n        let (head_x, head_y) = self.body[0];\n        match self.direction {\n            Direction::Up => self.body.insert(0, (head_x, head_y - 1)),\n            Direction::Down => self.body.insert(0, (head_x, head_y + 1)),\n            Direction::Left => self.body.insert(0, (head_x - 1, head_y)),\n            Direction::Right => self.body.insert(0, (head_x + 1, head_y)),\n        }\n        self.body.pop();\n    }\n\n    // Check if the snake eats the food\n    fn eats(&self, food: &Food) -> bool {\n        self.body[0] == food.position\n    }\n}\n\n// Implement the food\nimpl Food {\n    fn new() -> Food {\n        Food {\n            position: (rand::random::<usize>() % BOARD_SIZE, rand::random::<usize>() % BOARD_SIZE),\n        }\n    }\n}\n\n// Implement the view\nfn render(game_state: &GameState) {\n    for y in 0..BOARD_SIZE {\n        for x in 0..BOARD_SIZE {\n            if game_state.snake.body.contains(&(x, y)) {\n                print!(\"S\");\n            } else if game_state.food.position == (x, y) {\n                print!(\"F\");\n            } else {\n                print!(\" \");\n            }\n        }\n        println!();\n    }\n}\n\n// Implement the controller\nfn controller(game_state: &mut GameState) {\n    let stdin = io::stdin();\n    for c in stdin.keys() {\n        match c.unwrap() {\n            Key::Char('q') => break,\n            Key::Up => game_state.snake.direction = Direction::Up,\n            Key::Down => game_state.snake.direction = Direction::Down,\n            Key::Left => game_state.snake.direction = Direction::Left,\n            Key::Right => game_state.snake.direction = Direction::Right,\n            _ => {}\n        }\n    }\n}\n\nfn main() {\n    let mut game_state = GameState::new();\n    let mut stdout = stdout().into_raw_mode().unwrap();\n\n    loop {\n        write!(stdout, \"{}\", termion::clear::All).unwrap();\n        render(&game_state);\n        stdout.flush().unwrap();\n        game_state.update();\n        thread::sleep(Duration::from_millis(1000));\n    }\n}\n\"\"\"\n\nC_PLUS_PLUS = \"\"\"\n#include <iostream>\n#include <conio.h>\n#include <windows.h>\nusing namespace std;\n\nbool gameOver;\nconst int width = 20;\nconst int height = 20;\nint x, y, fruitX, fruitY, score;\nint tailX[100], tailY[100];\nint nTail;\nenum eDirecton { STOP = 0, LEFT, RIGHT, UP, DOWN};\neDirecton dir;\n\nvoid Setup()\n{\n    gameOver = false;\n    dir = STOP;\n    x = width / 2;\n    y = height / 2;\n    fruitX = rand() % width;\n    fruitY = rand() % height;\n    score = 0;\n}\nvoid Draw()\n{\n    system(\"cls\");\n    for (int i = 0; i < width+2; i++)\n        cout << \"#\";\n    cout << endl;\n\n    for (int i = 0; i < height; i++)\n    {\n        for (int j = 0; j < width; j++)\n        {\n            if (j == 0)\n                cout << \"#\";\n            if (i == y && j == x)\n                cout << \"*\";\n            else if (i == fruitY && j == fruitX)\n                cout << \"%\";\n            else\n            {\n\n                bool print = false;\n                for (int k = 0; k < nTail ; k++)\n                {\n                    if (tailX[k] == j && tailY[k] == i)\n                    {\n                        cout << \"*\"; print = true;\n                    }\n                }\n                if (!print)\n                    cout << \" \";\n\n            }\n\n            if (j == width - 1)\n                cout << \"#\";\n        }\n        cout << endl;\n    }\n\n    for (int i = 0; i < width+2; i++)\n        cout << \"#\";\n    cout << endl;\n    cout << \"Score:\" << score << endl;\n\n}\n\nvoid Input()\n{\n    if (_kbhit())\n    {\n        switch (_getch())\n        {\n        case 'a':\n            dir = LEFT;\n            break;\n        case 'd':\n            dir = RIGHT;\n            break;\n        case 'w':\n            dir = UP;\n            break;\n        case 's':\n            dir = DOWN;\n            break;\n        case 'x':\n            gameOver = true;\n            break;\n        }\n    }\n}\n\nvoid algorithm()\n{\n    int prevX = tailX[0];\n    int prevY = tailY[0];\n    int prev2X, prev2Y;\n    tailX[0] = x;\n    tailY[0] = y;\n\n    for(int i = 1; i < nTail ; i++)\n    {\n        prev2X = tailX[i];\n        prev2Y = tailY[i];\n        tailX[i] = prevX;\n        tailY[i] = prevY;\n        prevX = prev2X;\n        prevY = prev2Y;\n    }\n\n    switch (dir)\n    {\n    case LEFT:\n        x--;\n        break;\n    case RIGHT:\n        x++;\n        break;\n    case UP:\n        y--;\n        break;\n    case DOWN:\n        y++;\n        break;\n    default:\n        break;\n    }\n    if (x >= width)\n        x = 0; else if (x < 0) x = width - 1;\n    if (y >= height)\n        y = 0; else if (y < 0) y = height - 1;\n\n    for (int i = 0; i < nTail ; i++)\n        if (tailX[i] == x && tailY[i] == y)\n            gameOver = true;\n\n    if (x == fruitX && y == fruitY)\n    {\n        score += 10;\n        fruitX = rand() % width;\n        fruitY = rand() % height;\n        nTail++;\n    }\n}\n\nint main()\n{\n    Setup();\n    while (!gameOver)\n    {\n        Draw();\n        Input();\n        algorithm();\n    }\n    return 0;\n}\n\"\"\"\n\nC = \"\"\"\n#include <stdio.h>\n#include <conio.h>\n#include <windows.h>\n#include <stdlib.h>\n\n#define WIDTH 20\n#define HEIGHT 20\n#define MAX_SNAKE_SIZE WIDTH *HEIGHT\n\n// Model\ntypedef struct {\n    int x, y;\n} Point;\n\ntypedef struct {\n    Point body[MAX_SNAKE_SIZE];\n    int size;\n    Point direction;\n} Snake;\n\ntypedef struct {\n    Point position;\n    int isEaten;\n} Fruit;\n\n// View\nvoid gotoxy(int x, int y) {\n    COORD coord;\n    coord.X = x;\n    coord.Y = y;\n    SetConsoleCursorPosition(GetStdHandle(STD_OUTPUT_HANDLE), coord);\n}\n\nvoid drawBoard() {\n    int i;\n    for (i = 0; i < WIDTH + 2; i++) {\n        gotoxy(i, 0);\n        printf(\"#\");\n        gotoxy(i, HEIGHT + 1);\n        printf(\"#\");\n    }\n    for (i = 0; i < HEIGHT + 2; i++) {\n        gotoxy(0, i);\n        printf(\"#\");\n        gotoxy(WIDTH + 1, i);\n        printf(\"#\");\n    }\n}\n\nvoid drawSnake(Snake* snake) {\n    int i;\n    for (i = 0; i < snake->size; i++) {\n        gotoxy(snake->body[i].x, snake->body[i].y);\n        printf(\"*\");\n    }\n}\n\nvoid drawFruit(Fruit* fruit) {\n    gotoxy(fruit->position.x, fruit->position.y);\n    printf(\"@\");\n}\n\n// Controller\nvoid initGame(Snake* snake, Fruit* fruit) {\n    snake->size = 1;\n    snake->body[0].x = WIDTH / 2;\n    snake->body[0].y = HEIGHT / 2;\n    snake->direction.x = 0;\n    snake->direction.y = 1;\n\n    fruit->position.x = rand() % WIDTH;\n    fruit->position.y = rand() % HEIGHT;\n    fruit->isEaten = 0;\n}\n\nvoid updateSnake(Snake* snake) {\n    memmove(&snake->body[1], &snake->body[0], sizeof(Point) * (snake->size - 1));\n    snake->body[0].x += snake->direction.x;\n    snake->body[0].y += snake->direction.y;\n}\n\nvoid updateFruit(Snake* snake, Fruit* fruit) {\n    if (snake->body[0].x == fruit->position.x && snake->body[0].y == fruit->position.y) {\n        fruit->isEaten = 1;\n        snake->size++;\n    }\n    if (fruit->isEaten) {\n        fruit->position.x = rand() % WIDTH;\n        fruit->position.y = rand() % HEIGHT;\n        fruit->isEaten = 0;\n    }\n}\n\nvoid updateDirection(Snake* snake, char key) {\n    switch (key) {\n    case 'w':\n        snake->direction.x = 0;\n        snake->direction.y = -1;\n        break;\n    case 's':\n        snake->direction.x = 0;\n        snake->direction.y = 1;\n        break;\n    case 'a':\n        snake->direction.x = -1;\n        snake->direction.y = 0;\n        break;\n    case 'd':\n        snake->direction.x = 1;\n        snake->direction.y = 0;\n        break;\n    }\n}\n\nint isGameOver(Snake* snake) {\n    if (snake->body[0].x <= 0 || snake->body[0].x >= WIDTH || snake->body[0].y <= 0 || snake->body[0].y >= HEIGHT)\n        return 1;\n    int i;\n    for (i = 1; i < snake->size; i++) {\n        if (snake->body[0].x == snake->body[i].x && snake->body[0].y == snake->body[i].y)\n            return 1;\n    }\n    return 0;\n}\n\nint main() {\n    Snake snake;\n    Fruit fruit;\n    char key;\n\n    initGame(&snake, &fruit);\n\n    while (1) {\n        drawBoard();\n        drawSnake(&snake);\n        drawFruit(&fruit);\n\n        if (_kbhit()) {\n            key = _getch();\n            updateDirection(&snake, key);\n        }\n\n        updateSnake(&snake);\n        updateFruit(&snake, &fruit);\n\n        if (isGameOver(&snake)) {\n            break;\n        }\n\n        Sleep(100);\n        system(\"cls\");\n    }\n\n    printf(\"Game Over!\\n\");\n\n    return 0;\n}\n\"\"\"\n", "tests/applications/__init__.py": "", "tests/applications/cli/test_cli_agent.py": "import os\nimport tempfile\n\nimport pytest\n\nfrom langchain.schema import AIMessage\n\nfrom gpt_engineer.applications.cli.cli_agent import CliAgent\nfrom gpt_engineer.core.default.disk_execution_env import DiskExecutionEnv\nfrom gpt_engineer.core.default.disk_memory import DiskMemory\n\n# from gpt_engineer.core.default.git_version_manager import GitVersionManager\nfrom gpt_engineer.core.default.paths import ENTRYPOINT_FILE, memory_path\nfrom gpt_engineer.core.files_dict import FilesDict\nfrom gpt_engineer.core.prompt import Prompt\nfrom gpt_engineer.tools.custom_steps import clarified_gen, lite_gen\nfrom tests.mock_ai import MockAI\n\n\ndef test_init_standard_config(monkeypatch):\n    monkeypatch.setattr(\"builtins.input\", lambda _: \"y\")\n    temp_dir = tempfile.mkdtemp()\n    memory = DiskMemory(memory_path(temp_dir))\n    execution_env = DiskExecutionEnv()\n    mock_ai = MockAI(\n        [\n            AIMessage(\n                \"hello_world.py\\n```\\nwith open('output.txt', 'w') as file:\\n    file.write('Hello World!')\\n```\"\n            ),\n            AIMessage(\"```run.sh\\npython3 hello_world.py\\n```\"),\n        ],\n    )\n    cli_agent = CliAgent.with_default_config(memory, execution_env, ai=mock_ai)\n    outfile = \"output.txt\"\n    os.path.join(temp_dir, outfile)\n    code = cli_agent.init(\n        Prompt(\n            f\"Make a program that prints 'Hello World!' to a file called '{outfile}'\"\n        )\n    )\n\n    env = DiskExecutionEnv()\n    env.upload(code).run(f\"bash {ENTRYPOINT_FILE}\")\n    code = env.download()\n\n    assert outfile in code\n    assert code[outfile] == \"Hello World!\"\n\n\ndef test_init_lite_config(monkeypatch):\n    monkeypatch.setattr(\"builtins.input\", lambda _: \"y\")\n    temp_dir = tempfile.mkdtemp()\n    memory = DiskMemory(memory_path(temp_dir))\n    # version_manager = GitVersionManager(temp_dir)\n    execution_env = DiskExecutionEnv()\n    mock_ai = MockAI(\n        [\n            AIMessage(\n                \"hello_world.py\\n```\\nwith open('output.txt', 'w') as file:\\n    file.write('Hello World!')\\n```\"\n            ),\n            AIMessage(\"```run.sh\\npython3 hello_world.py\\n```\"),\n        ],\n    )\n    cli_agent = CliAgent.with_default_config(\n        memory, execution_env, ai=mock_ai, code_gen_fn=lite_gen\n    )\n    outfile = \"output.txt\"\n    os.path.join(temp_dir, outfile)\n    code = cli_agent.init(\n        Prompt(\n            f\"Make a program that prints 'Hello World!' to a file called '{outfile}'\"\n        )\n    )\n\n    env = DiskExecutionEnv()\n    env.upload(code).run(f\"bash {ENTRYPOINT_FILE}\")\n    code = env.download()\n\n    assert outfile in code\n    assert code[outfile].strip() == \"Hello World!\"\n\n\ndef test_init_clarified_gen_config(monkeypatch):\n    monkeypatch.setattr(\"builtins.input\", lambda _: \"y\")\n    temp_dir = tempfile.mkdtemp()\n    memory = DiskMemory(memory_path(temp_dir))\n    execution_env = DiskExecutionEnv()\n    mock_ai = MockAI(\n        [\n            AIMessage(\"nothing to clarify\"),\n            AIMessage(\n                \"hello_world.py\\n```\\nwith open('output.txt', 'w') as file:\\n    file.write('Hello World!')\\n```\"\n            ),\n            AIMessage(\"```run.sh\\npython3 hello_world.py\\n```\"),\n        ],\n    )\n    cli_agent = CliAgent.with_default_config(\n        memory, execution_env, ai=mock_ai, code_gen_fn=clarified_gen\n    )\n    outfile = \"output.txt\"\n    code = cli_agent.init(\n        Prompt(\n            f\"Make a program that prints 'Hello World!' to a file called '{outfile} either using python or javascript'\"\n        )\n    )\n\n    env = DiskExecutionEnv()\n    env.upload(code).run(f\"bash {ENTRYPOINT_FILE}\")\n    code = env.download()\n\n    assert outfile in code\n    assert code[outfile].strip() == \"Hello World!\"\n\n\ndef test_improve_standard_config(monkeypatch):\n    monkeypatch.setattr(\"builtins.input\", lambda _: \"y\")\n    temp_dir = tempfile.mkdtemp()\n    code = FilesDict(\n        {\n            \"main.py\": \"def write_hello_world_to_file(filename):\\n    \\\"\\\"\\\"\\n    Writes 'Hello World!' to the specified file.\\n    \\n    :param filename: The name of the file to write to.\\n    \\\"\\\"\\\"\\n    with open(filename, 'w') as file:\\n        file.write('Hello World!')\\n\\nif __name__ == \\\"__main__\\\":\\n    output_filename = 'output.txt'\\n    write_hello_world_to_file(output_filename)\",\n            \"requirements.txt\": \"# No dependencies required\",\n            \"run.sh\": \"python3 main.py\\n\",\n        }\n    )\n    memory = DiskMemory(memory_path(temp_dir))\n    # version_manager = GitVersionManager(temp_dir)\n    execution_env = DiskExecutionEnv()\n    mock_ai = MockAI(\n        [\n            AIMessage(\n                \"```diff\\n--- main.py\\n+++ main.py\\n@@ -7,3 +7,3 @@\\n     with open(filename, 'w') as file:\\n-        file.write('Hello World!')\\n+        file.write('!dlroW olleH')\\n```\"\n            )\n        ]\n    )\n    cli_agent = CliAgent.with_default_config(memory, execution_env, ai=mock_ai)\n\n    code = cli_agent.improve(\n        code,\n        Prompt(\n            \"Change the program so that it prints '!dlroW olleH' instead of 'Hello World!'\"\n        ),\n    )\n\n    env = DiskExecutionEnv()\n    env.upload(code).run(f\"bash {ENTRYPOINT_FILE}\")\n    code = env.download()\n\n    outfile = \"output.txt\"\n    assert outfile in code\n    assert code[outfile] == \"!dlroW olleH\"\n\n\nif __name__ == \"__main__\":\n    pytest.main()\n", "tests/applications/cli/test_learning.py": "from unittest import mock\n\nfrom gpt_engineer.applications.cli import learning\nfrom gpt_engineer.applications.cli.learning import Learning\nfrom gpt_engineer.core.default.disk_memory import DiskMemory\nfrom gpt_engineer.core.prompt import Prompt\n\n\ndef test_human_review_input_no_concent_returns_none():\n    with mock.patch.object(learning, \"check_collection_consent\", return_value=False):\n        result = learning.human_review_input()\n\n    assert result is None\n\n\ndef test_human_review_input_consent_code_ran_no_comments():\n    with (\n        mock.patch.object(learning, \"check_collection_consent\", return_value=True),\n        mock.patch(\"builtins.input\", return_value=\"y\"),\n    ):\n        result = learning.human_review_input()\n\n    assert result.raw == \"y, y, \"\n    assert result.ran is True\n    assert result.works is None\n    assert result.comments == \"\"\n\n\ndef test_human_review_input_consent_code_ran_not_perfect_but_useful_no_comments():\n    with (\n        mock.patch.object(learning, \"check_collection_consent\", return_value=True),\n        mock.patch(\"builtins.input\", side_effect=[\"y\", \"n\", \"y\", \"\"]),\n    ):\n        result = learning.human_review_input()\n\n    assert result.raw == \"y, n, y\"\n    assert result.ran is True\n    assert result.works is True\n    assert result.comments == \"\"\n\n\ndef test_check_collection_consent_yes():\n    gpte_consent_mock = mock.Mock()\n    gpte_consent_mock.exists.return_value = True\n    gpte_consent_mock.read_text.return_value = \"true\"\n\n    with mock.patch.object(learning, \"Path\", return_value=gpte_consent_mock):\n        result = learning.check_collection_consent()\n\n    assert result is True\n\n\ndef test_check_collection_consent_no_ask_collection_consent():\n    with mock.patch.object(learning, \"Path\") as gpte_consent_mock:\n        gpte_consent_mock.exists.return_value = True\n        gpte_consent_mock.read_text.return_value = \"false\"\n\n        with mock.patch.object(learning, \"ask_collection_consent\", return_value=True):\n            result = learning.check_collection_consent()\n\n    assert result is True\n\n\ndef test_ask_collection_consent_yes():\n    with mock.patch(\"builtins.input\", return_value=\"y\"):\n        result = learning.ask_collection_consent()\n\n    assert result is True\n\n\ndef test_ask_collection_consent_no():\n    with mock.patch(\"builtins.input\", return_value=\"n\"):\n        result = learning.ask_collection_consent()\n\n    assert result is False\n\n\ndef test_extract_learning():\n    review = learning.Review(\n        raw=\"y, n, y\",\n        ran=True,\n        works=True,\n        perfect=False,\n        comments=\"The code is not perfect\",\n    )\n    memory = mock.Mock(spec=DiskMemory)\n    memory.to_json.return_value = {\"prompt\": \"prompt\"}\n\n    result = learning.extract_learning(\n        Prompt(\"prompt\"),\n        \"model_name\",\n        0.01,\n        (\"prompt_tokens\", \"completion_tokens\"),\n        memory,\n        review,\n    )\n\n    assert isinstance(result, Learning)\n\n\ndef test_get_session():\n    with mock.patch.object(learning, \"Path\") as path_mock:\n        # can be better tested with pyfakefs.\n        path_mock.return_value.__truediv__.return_value.exists.return_value = False\n\n        with mock.patch.object(learning, \"random\") as random_mock:\n            random_mock.randint.return_value = 42\n            result = learning.get_session()\n\n        assert result == \"42\"\n", "tests/applications/cli/test_main.py": "import dataclasses\nimport functools\nimport inspect\nimport os\nimport shutil\nimport tempfile\n\nfrom argparse import Namespace\nfrom unittest.mock import patch\n\nimport pytest\nimport typer\n\nimport gpt_engineer.applications.cli.main as main\n\nfrom gpt_engineer.applications.cli.main import load_prompt\nfrom gpt_engineer.core.default.disk_memory import DiskMemory\nfrom gpt_engineer.core.prompt import Prompt\n\n\n@functools.wraps(dataclasses.make_dataclass)\ndef dcommand(typer_f, **kwargs):\n    required = True\n\n    def field_desc(name, param):\n        nonlocal required\n\n        t = param.annotation or \"typing.Any\"\n        if param.default.default is not ...:\n            required = False\n            return name, t, dataclasses.field(default=param.default.default)\n\n        if not required:\n            raise ValueError(\"Required value after optional\")\n\n        return name, t\n\n    kwargs.setdefault(\"cls_name\", typer_f.__name__)\n\n    params = inspect.signature(typer_f).parameters\n    kwargs[\"fields\"] = [field_desc(k, v) for k, v in params.items()]\n\n    @functools.wraps(typer_f)\n    def dcommand_decorator(function_or_class):\n        assert callable(function_or_class)\n\n        ka = dict(kwargs)\n        ns = Namespace(**(ka.pop(\"namespace\", None) or {}))\n        if isinstance(function_or_class, type):\n            ka[\"bases\"] = *ka.get(\"bases\", ()), function_or_class\n        else:\n            ns.__call__ = function_or_class\n\n        ka[\"namespace\"] = vars(ns)\n        return dataclasses.make_dataclass(**ka)\n\n    return dcommand_decorator\n\n\n@dcommand(main.main)\nclass DefaultArgumentsMain:\n    def __call__(self):\n        attribute_dict = vars(self)\n        main.main(**attribute_dict)\n\n\ndef input_generator():\n    yield \"y\"  # First response\n    while True:\n        yield \"n\"  # Subsequent responses\n\n\nprompt_text = \"Make a python program that writes 'hello' to a file called 'output.txt'\"\n\n\nclass TestMain:\n    #  Runs gpt-engineer cli interface for many parameter configurations, BUT DOES NOT CODEGEN! Only testing cli.\n    def test_default_settings_generate_project(self, tmp_path, monkeypatch):\n        p = tmp_path / \"projects/example\"\n        p.mkdir(parents=True)\n        (p / \"prompt\").write_text(prompt_text)\n        args = DefaultArgumentsMain(str(p), llm_via_clipboard=True, no_execution=True)\n        args()\n\n    #  Runs gpt-engineer with improve mode and improves an existing project in the specified path.\n    def test_improve_existing_project(self, tmp_path, monkeypatch):\n        p = tmp_path / \"projects/example\"\n        p.mkdir(parents=True)\n        (p / \"prompt\").write_text(prompt_text)\n        args = DefaultArgumentsMain(\n            str(p), improve_mode=True, llm_via_clipboard=True, no_execution=True\n        )\n        args()\n\n        # def improve_generator():\n        #     yield \"y\"\n        #     while True:\n        #         yield \"n\"  # Subsequent responses\n        #\n        # gen = improve_generator()\n        # monkeypatch.setattr(\"builtins.input\", lambda _: next(gen))\n        # p = tmp_path / \"projects/example\"\n        # p.mkdir(parents=True)\n        # (p / \"prompt\").write_text(prompt_text)\n        # (p / \"main.py\").write_text(\"The program will be written in this file\")\n        # meta_p = p / META_DATA_REL_PATH\n        # meta_p.mkdir(parents=True)\n        # (meta_p / \"file_selection.toml\").write_text(\n        #     \"\"\"\n        # [files]\n        # \"main.py\" = \"selected\"\n        #             \"\"\"\n        # )\n        # os.environ[\"GPTE_TEST_MODE\"] = \"True\"\n        # simplified_main(str(p), \"improve\")\n        # DiskExecutionEnv(path=p)\n        # del os.environ[\"GPTE_TEST_MODE\"]\n\n    #  Runs gpt-engineer with lite mode and generates a project with only the main prompt.\n    def test_lite_mode_generate_project(self, tmp_path, monkeypatch):\n        p = tmp_path / \"projects/example\"\n        p.mkdir(parents=True)\n        (p / \"prompt\").write_text(prompt_text)\n        args = DefaultArgumentsMain(\n            str(p), lite_mode=True, llm_via_clipboard=True, no_execution=True\n        )\n        args()\n\n    #  Runs gpt-engineer with clarify mode and generates a project after discussing the specification with the AI.\n    def test_clarify_mode_generate_project(self, tmp_path, monkeypatch):\n        p = tmp_path / \"projects/example\"\n        p.mkdir(parents=True)\n        (p / \"prompt\").write_text(prompt_text)\n        args = DefaultArgumentsMain(\n            str(p), clarify_mode=True, llm_via_clipboard=True, no_execution=True\n        )\n        args()\n\n    #  Runs gpt-engineer with self-heal mode and generates a project after discussing the specification with the AI and self-healing the code.\n    def test_self_heal_mode_generate_project(self, tmp_path, monkeypatch):\n        p = tmp_path / \"projects/example\"\n        p.mkdir(parents=True)\n        (p / \"prompt\").write_text(prompt_text)\n        args = DefaultArgumentsMain(\n            str(p), self_heal_mode=True, llm_via_clipboard=True, no_execution=True\n        )\n        args()\n\n    def test_clarify_lite_improve_mode_generate_project(self, tmp_path, monkeypatch):\n        p = tmp_path / \"projects/example\"\n        p.mkdir(parents=True)\n        (p / \"prompt\").write_text(prompt_text)\n        args = DefaultArgumentsMain(\n            str(p),\n            improve_mode=True,\n            lite_mode=True,\n            clarify_mode=True,\n            llm_via_clipboard=True,\n            no_execution=True,\n        )\n        pytest.raises(typer.Exit, args)\n\n    #  Tests the creation of a log file in improve mode.\n\n\nclass TestLoadPrompt:\n    #  Load prompt from existing file in input_repo\n    def test_load_prompt_existing_file(self):\n        with tempfile.TemporaryDirectory() as tmp_dir:\n            input_repo = DiskMemory(tmp_dir)\n            prompt_file = \"prompt.txt\"\n            prompt_content = \"This is the prompt\"\n            input_repo[prompt_file] = prompt_content\n\n            improve_mode = False\n            image_directory = \"\"\n\n            result = load_prompt(input_repo, improve_mode, prompt_file, image_directory)\n\n            assert isinstance(result, Prompt)\n            assert result.text == prompt_content\n            assert result.image_urls is None\n\n    #  Prompt file does not exist in input_repo, and improve_mode is False\n    def test_load_prompt_no_file_improve_mode_false(self):\n        with tempfile.TemporaryDirectory() as tmp_dir:\n            input_repo = DiskMemory(tmp_dir)\n            prompt_file = \"prompt.txt\"\n\n            improve_mode = False\n            image_directory = \"\"\n\n            with patch(\n                \"builtins.input\",\n                return_value=\"What application do you want gpt-engineer to generate?\",\n            ):\n                result = load_prompt(\n                    input_repo, improve_mode, prompt_file, image_directory\n                )\n\n            assert isinstance(result, Prompt)\n            assert (\n                result.text == \"What application do you want gpt-engineer to generate?\"\n            )\n            assert result.image_urls is None\n\n    #  Prompt file is a directory\n    def test_load_prompt_directory_file(self):\n        with tempfile.TemporaryDirectory() as tmp_dir:\n            input_repo = DiskMemory(tmp_dir)\n            prompt_file = os.path.join(tmp_dir, \"prompt\")\n\n            os.makedirs(os.path.join(tmp_dir, prompt_file))\n\n            improve_mode = False\n            image_directory = \"\"\n\n            with pytest.raises(ValueError):\n                load_prompt(input_repo, improve_mode, prompt_file, image_directory)\n\n    #  Prompt file is empty\n    def test_load_prompt_empty_file(self):\n        with tempfile.TemporaryDirectory() as tmp_dir:\n            input_repo = DiskMemory(tmp_dir)\n            prompt_file = \"prompt.txt\"\n            input_repo[prompt_file] = \"\"\n\n            improve_mode = False\n            image_directory = \"\"\n\n            with patch(\n                \"builtins.input\",\n                return_value=\"What application do you want gpt-engineer to generate?\",\n            ):\n                result = load_prompt(\n                    input_repo, improve_mode, prompt_file, image_directory\n                )\n\n            assert isinstance(result, Prompt)\n            assert (\n                result.text == \"What application do you want gpt-engineer to generate?\"\n            )\n            assert result.image_urls is None\n\n    #  image_directory does not exist in input_repo\n    def test_load_prompt_no_image_directory(self):\n        with tempfile.TemporaryDirectory() as tmp_dir:\n            input_repo = DiskMemory(tmp_dir)\n            prompt_file = \"prompt.txt\"\n            prompt_content = \"This is the prompt\"\n            input_repo[prompt_file] = prompt_content\n\n            improve_mode = False\n            image_directory = \"tests/test_data\"\n            shutil.copytree(image_directory, os.path.join(tmp_dir, image_directory))\n\n            result = load_prompt(input_repo, improve_mode, prompt_file, image_directory)\n\n            assert isinstance(result, Prompt)\n            assert result.text == prompt_content\n            assert \"mona_lisa.jpg\" in result.image_urls\n\n\n#     def test_log_creation_in_improve_mode(self, tmp_path, monkeypatch):\n#         def improve_generator():\n#             yield \"y\"\n#             while True:\n#                 yield \"n\"  # Subsequent responses\n#\n#         gen = improve_generator()\n#         monkeypatch.setattr(\"builtins.input\", lambda _: next(gen))\n#         p = tmp_path / \"projects/example\"\n#         p.mkdir(parents=True)\n#         (p / \"prompt\").write_text(prompt_text)\n#         (p / \"main.py\").write_text(\"The program will be written in this file\")\n#         meta_p = p / META_DATA_REL_PATH\n#         meta_p.mkdir(parents=True)\n#         (meta_p / \"file_selection.toml\").write_text(\n#             \"\"\"\n#         [files]\n#         \"main.py\" = \"selected\"\n#                     \"\"\"\n#         )\n#         os.environ[\"GPTE_TEST_MODE\"] = \"True\"\n#         simplified_main(str(p), \"improve\")\n#         DiskExecutionEnv(path=p)\n#         assert (\n#             (p / f\".gpteng/memory/{DEBUG_LOG_FILE}\").read_text().strip()\n#             == \"\"\"UPLOADED FILES:\n# ```\n# File: main.py\n# 1 The program will be written in this file\n#\n# ```\n# PROMPT:\n# Make a python program that writes 'hello' to a file called 'output.txt'\n# CONSOLE OUTPUT:\"\"\"\n#         )\n#         del os.environ[\"GPTE_TEST_MODE\"]\n#\n#     def test_log_creation_in_improve_mode_with_failing_diff(\n#         self, tmp_path, monkeypatch\n#     ):\n#         def improve_generator():\n#             yield \"y\"\n#             while True:\n#                 yield \"n\"  # Subsequent responses\n#\n#         def mock_salvage_correct_hunks(\n#             messages: List, files_dict: FilesDict, error_message: List\n#         ) -> FilesDict:\n#             # create a falling diff\n#             messages[\n#                 -1\n#             ].content = \"\"\"To create a Python program that writes 'hello' to a file called 'output.txt', we will need to perform the following steps:\n#\n# 1. Open the file 'output.txt' in write mode.\n# 2. Write the string 'hello' to the file.\n# 3. Close the file to ensure the data is written and the file is not left open.\n#\n# Here is the implementation of the program in the `main.py` file:\n#\n# ```diff\n# --- main.py\n# +++ main.py\n# @@ -0,0 +1,9 @@\n# -create falling diff\n# ```\n#\n# This concludes a fully working implementation.\"\"\"\n#             # Call the original function with modified messages or define your own logic\n#             return salvage_correct_hunks(messages, files_dict, error_message)\n#\n#         gen = improve_generator()\n#         monkeypatch.setattr(\"builtins.input\", lambda _: next(gen))\n#         monkeypatch.setattr(\n#             \"gpt_engineer.core.default.steps.salvage_correct_hunks\",\n#             mock_salvage_correct_hunks,\n#         )\n#         p = tmp_path / \"projects/example\"\n#         p.mkdir(parents=True)\n#         (p / \"prompt\").write_text(prompt_text)\n#         (p / \"main.py\").write_text(\"The program will be written in this file\")\n#         meta_p = p / META_DATA_REL_PATH\n#         meta_p.mkdir(parents=True)\n#         (meta_p / \"file_selection.toml\").write_text(\n#             \"\"\"\n#         [files]\n#         \"main.py\" = \"selected\"\n#                     \"\"\"\n#         )\n#         os.environ[\"GPTE_TEST_MODE\"] = \"True\"\n#         simplified_main(str(p), \"improve\")\n#         DiskExecutionEnv(path=p)\n#         assert (\n#             (p / f\".gpteng/memory/{DEBUG_LOG_FILE}\").read_text().strip()\n#             == \"\"\"UPLOADED FILES:\n# ```\n# File: main.py\n# 1 The program will be written in this file\n#\n# ```\n# PROMPT:\n# Make a python program that writes 'hello' to a file called 'output.txt'\n# CONSOLE OUTPUT:\n# Invalid hunk: @@ -0,0 +1,9 @@\n# -create falling diff\n#\n# Invalid hunk: @@ -0,0 +1,9 @@\n# -create falling diff\"\"\"\n#         )\n#         del os.environ[\"GPTE_TEST_MODE\"]\n#\n#     def test_log_creation_in_improve_mode_with_unexpected_exceptions(\n#         self, tmp_path, monkeypatch\n#     ):\n#         def improve_generator():\n#             yield \"y\"\n#             while True:\n#                 yield \"n\"  # Subsequent responses\n#\n#         def mock_salvage_correct_hunks(\n#             messages: List, files_dict: FilesDict, error_message: List\n#         ) -> FilesDict:\n#             raise Exception(\"Mock exception in salvage_correct_hunks\")\n#\n#         gen = improve_generator()\n#         monkeypatch.setattr(\"builtins.input\", lambda _: next(gen))\n#         monkeypatch.setattr(\n#             \"gpt_engineer.core.default.steps.salvage_correct_hunks\",\n#             mock_salvage_correct_hunks,\n#         )\n#         p = tmp_path / \"projects/example\"\n#         p.mkdir(parents=True)\n#         (p / \"prompt\").write_text(prompt_text)\n#         (p / \"main.py\").write_text(\"The program will be written in this file\")\n#         meta_p = p / META_DATA_REL_PATH\n#         meta_p.mkdir(parents=True)\n#         (meta_p / \"file_selection.toml\").write_text(\n#             \"\"\"\n#         [files]\n#         \"main.py\" = \"selected\"\n#                     \"\"\"\n#         )\n#         os.environ[\"GPTE_TEST_MODE\"] = \"True\"\n#         simplified_main(str(p), \"improve\")\n#         DiskExecutionEnv(path=p)\n#         assert (\n#             (p / f\".gpteng/memory/{DEBUG_LOG_FILE}\").read_text().strip()\n#             == \"\"\"UPLOADED FILES:\n# ```\n# File: main.py\n# 1 The program will be written in this file\n#\n# ```\n# PROMPT:\n# Make a python program that writes 'hello' to a file called 'output.txt'\n# CONSOLE OUTPUT:\n# Error while improving the project: Mock exception in salvage_correct_hunks\"\"\"\n#         )\n#         del os.environ[\"GPTE_TEST_MODE\"]\n", "tests/applications/cli/__init__.py": "", "tests/applications/cli/test_collection_consent.py": "\"\"\"\nTests for the revised data collection consent mechanism in the cli/learning module.\n\"\"\"\n\nfrom pathlib import Path\nfrom unittest.mock import patch\n\nimport pytest\n\nfrom gpt_engineer.applications.cli.learning import (\n    ask_collection_consent,\n    check_collection_consent,\n)\n\n\n# Use a fixture to clean up created files after each test\n@pytest.fixture\ndef cleanup():\n    yield\n    if Path(\".gpte_consent\").exists():\n        Path(\".gpte_consent\").unlink()\n\n\n\"\"\"\nTest the following 4 scenarios for check_collection_consent():\n    * The .gpte_consent file exists and its content is \"true\".\n    * The .gpte_consent file exists but its content is not \"true\".\n    * The .gpte_consent file does not exist and the user gives consent when asked.\n    * The .gpte_consent file does not exist and the user does not give consent when asked.\n\"\"\"\n\n\ndef test_check_consent_file_exists_and_true(cleanup):\n    Path(\".gpte_consent\").write_text(\"true\")\n    assert check_collection_consent() is True\n\n\ndef test_check_consent_file_exists_and_false(cleanup):\n    Path(\".gpte_consent\").write_text(\"false\")\n    with patch(\"builtins.input\", side_effect=[\"n\"]):\n        assert check_collection_consent() is False\n\n\ndef test_check_consent_file_not_exists_and_user_says_yes(cleanup):\n    with patch(\"builtins.input\", side_effect=[\"y\"]):\n        assert check_collection_consent() is True\n    assert Path(\".gpte_consent\").exists()\n    assert Path(\".gpte_consent\").read_text() == \"true\"\n\n\ndef test_check_consent_file_not_exists_and_user_says_no(cleanup):\n    with patch(\"builtins.input\", side_effect=[\"n\"]):\n        assert check_collection_consent() is False\n    assert not Path(\".gpte_consent\").exists()\n\n\n\"\"\"\nTest the following 4 scenarios for ask_collection_consent():\n    1. The user immediately gives consent with \"y\":\n        * The .gpte_consent file is created with content \"true\".\n        * The function returns True.\n    2. The user immediately denies consent with \"n\":\n        * The .gpte_consent file is not created.\n        * The function returns False.\n    3. The user first provides an invalid response, then gives consent with \"y\":\n        * The user is re-prompted after the invalid input.\n        * The .gpte_consent file is created with content \"true\".\n        * The function returns True.\n    4. The user first provides an invalid response, then denies consent with \"n\":\n        * The user is re-prompted after the invalid input.\n        * The .gpte_consent file is not created.\n        * The function returns False.\n\"\"\"\n\n\ndef test_ask_collection_consent_yes(cleanup):\n    with patch(\"builtins.input\", side_effect=[\"y\"]):\n        result = ask_collection_consent()\n    assert Path(\".gpte_consent\").exists()\n    assert Path(\".gpte_consent\").read_text() == \"true\"\n    assert result is True\n\n\ndef test_ask_collection_consent_no(cleanup):\n    with patch(\"builtins.input\", side_effect=[\"n\"]):\n        result = ask_collection_consent()\n    assert not Path(\".gpte_consent\").exists()\n    assert result is False\n\n\ndef test_ask_collection_consent_invalid_then_yes(cleanup):\n    with patch(\"builtins.input\", side_effect=[\"invalid\", \"y\"]):\n        result = ask_collection_consent()\n    assert Path(\".gpte_consent\").exists()\n    assert Path(\".gpte_consent\").read_text() == \"true\"\n    assert result is True\n\n\ndef test_ask_collection_consent_invalid_then_no(cleanup):\n    with patch(\"builtins.input\", side_effect=[\"invalid\", \"n\"]):\n        result = ask_collection_consent()\n    assert not Path(\".gpte_consent\").exists()\n    assert result is False\n", "tests/applications/cli/test_collect.py": "\"\"\"\nTests the collect_learnings function in the cli/collect module.\n\"\"\"\n\nimport pytest\n\n# def test_collect_learnings(monkeypatch):\n#     monkeypatch.setattr(rudder_analytics, \"track\", MagicMock())\n#\n#     model = \"test_model\"\n#     temperature = 0.5\n#     steps = [simple_gen]\n#     dbs = FileRepositories(\n#         OnDiskRepository(\"/tmp\"),\n#         OnDiskRepository(\"/tmp\"),\n#         OnDiskRepository(\"/tmp\"),\n#         OnDiskRepository(\"/tmp\"),\n#         OnDiskRepository(\"/tmp\"),\n#         OnDiskRepository(\"/tmp\"),\n#         OnDiskRepository(\"/tmp\"),\n#     )\n#     dbs.input = {\n#         \"prompt\": \"test prompt\\n with newlines\",\n#         \"feedback\": \"test feedback\",\n#     }\n#     code = \"this is output\\n\\nit contains code\"\n#     dbs.logs = {steps[0].__name__: json.dumps([{\"role\": \"system\", \"content\": code}])}\n#     dbs.memory = {\"all_output.txt\": \"test workspace\\n\" + code}\n#\n#     collect_learnings(model, temperature, steps, dbs)\n#\n#     learnings = extract_learning(\n#         model, temperature, steps, dbs, steps_file_hash=steps_file_hash()\n#     )\n#     assert rudder_analytics.track.call_count == 1\n#     assert rudder_analytics.track.call_args[1][\"event\"] == \"learning\"\n#     a = {\n#         k: v\n#         for k, v in rudder_analytics.track.call_args[1][\"properties\"].items()\n#         if k != \"timestamp\"\n#     }\n#     b = {k: v for k, v in learnings.to_dict().items() if k != \"timestamp\"}\n#     assert a == b\n#\n#     assert json.dumps(code) in learnings.logs\n#     assert code in learnings.workspace\n\n\nif __name__ == \"__main__\":\n    pytest.main([\"-v\"])\n", "tests/core/test_token_usage.py": "import base64\nimport csv\nimport io\nimport os\n\nfrom io import StringIO\nfrom pathlib import Path\n\nfrom langchain.schema import HumanMessage, SystemMessage\nfrom PIL import Image\n\nfrom gpt_engineer.core.token_usage import Tokenizer, TokenUsageLog\n\n\ndef test_format_log():\n    # arrange\n    token_usage_log = TokenUsageLog(\"gpt-4\")\n    request_messages = [\n        SystemMessage(content=\"my system message\"),\n        HumanMessage(content=\"my user prompt\"),\n    ]\n    response = \"response from model\"\n\n    # act\n    token_usage_log.update_log(request_messages, response, \"step 1\")\n    token_usage_log.update_log(request_messages, response, \"step 2\")\n    csv_log = token_usage_log.format_log()\n\n    # assert\n    csv_rows = list(csv.reader(StringIO(csv_log)))\n\n    assert len(csv_rows) == 3\n\n    assert all(len(row) == 7 for row in csv_rows)\n\n\ndef test_usage_cost():\n    # arrange\n    token_usage_log = TokenUsageLog(\"gpt-4\")\n    request_messages = [\n        SystemMessage(content=\"my system message\"),\n        HumanMessage(content=\"my user prompt\"),\n    ]\n    response = \"response from model\"\n\n    # act\n    token_usage_log.update_log(request_messages, response, \"step 1\")\n    token_usage_log.update_log(request_messages, response, \"step 2\")\n    usage_cost = token_usage_log.usage_cost()\n\n    # assert\n    assert usage_cost > 0\n\n\ndef test_image_tokenizer():\n    # Arrange\n    token_usage_log = Tokenizer(\"gpt-4\")\n    image_path = Path(__file__).parent.parent / \"test_data\" / \"mona_lisa.jpg\"\n    # Check if the image file exists\n    if not os.path.isfile(image_path):\n        raise FileNotFoundError(f\"Image file not found: {image_path}\")\n\n    # Act\n    with Image.open(image_path) as img:\n        # Convert RGBA to RGB\n        if img.mode == \"RGBA\":\n            img = img.convert(\"RGB\")\n\n        buffered = io.BytesIO()\n        img.save(buffered, format=\"JPEG\")\n        image_base64 = base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n\n    # Calculate the token cost of the base64 encoded image\n    image_token_cost = token_usage_log.num_tokens_for_base64_image(image_base64)\n\n    # Assert\n    assert image_token_cost == 1105\n\n\ndef test_list_type_message_with_image():\n    # Arrange\n    token_usage_log = TokenUsageLog(\"gpt-4\")\n\n    request_messages = [\n        SystemMessage(content=\"My system message\"),\n        HumanMessage(\n            content=[\n                {\"type\": \"text\", \"text\": \"My user message\"},\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\n                        \"url\": \"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAgAAAAIAQMAAAD+wSzIAAAABlBMVEX///+/v7+jQ3Y5AAAADklEQVQI12P4AIX8EAgALgAD/aNpbtEAAAAASUVORK5CYII\",\n                        \"detail\": \"low\",\n                    },\n                },\n            ]\n        ),\n    ]\n    response = \"response from model\"\n\n    # Act\n    token_usage_log.update_log(request_messages, response, \"list message with image\")\n\n    # Since this is the first (and only) log entry, the in-step total tokens should match our expected total\n    expected_total_tokens = 106\n\n    # Assert\n    assert (\n        token_usage_log.log()[-1].in_step_total_tokens == expected_total_tokens\n    ), f\"Expected {expected_total_tokens} tokens, got {token_usage_log.log()[-1].in_step_total_tokens}\"\n", "tests/core/test_chat_to_files.py": "import os\n\nfrom typing import Dict, Tuple\n\nimport pytest\n\nfrom gpt_engineer.core.chat_to_files import parse_diffs\nfrom gpt_engineer.core.diff import is_similar\nfrom gpt_engineer.core.files_dict import file_to_lines_dict\n\nTHIS_FILE_DIR = os.path.dirname(os.path.abspath(__file__))\n\nexample_diff = \"\"\"\nIrrelevant line to be ignored\n\nanother irrelevant line to be ignored\n```diff\n--- example.txt\n+++ example.txt\n@@ -12,3 +12,4 @@\n     sample text 1\n     sample text 2\n+    added extra line here\n-    original text A\n+    updated original text A with changes\n@@ -35,4 +36,5 @@\n         checking status:\n-            perform operation X\n+            perform operation X only if specific condition holds\n+                new operation related to condition\n         evaluating next step:\n-            execute step Y\n+            revised execution of step Y\n```\n\"\"\"\n\nexample_multiple_diffs = \"\"\"\nI apologize for the oversight. Let's correct the `calculator.py` file with the proper git diff format, ensuring that the context lines match the original code exactly.\n\n```diff\n--- calculator.py\n+++ calculator.py\n@@ -1,3 +1,3 @@\n class Calculator:\n-    def add(self, a, b):\n-        return a - b  # Logical\n+    def add(self, a, b):  # Adds two numbers\n+        return a + b\n```\n\nNow, let's create the `main.py` file with the correct git diff format:\n\n```diff\n--- /dev/null\n+++ main.py\n@@ -0,0 +1,7 @@\n+from calculator import Calculator\n+\n+# Function to demonstrate the usage of the Calculator class\n+def main():\n+    calc = Calculator()\n+if __name__ == \"__main__\":\n+    main()\n```\n\nThese changes should now correctly apply to the provided code and create a simple calculator program with a command-line interface.\n\n\n\"\"\"\n\nexample_line_dist_diff = \"\"\"\nIrrelevant line to be ignored\n\nanother irrelevant line to be ignored\n```diff\n--- example.txt\n+++ example.txt\n@@ -10,4 +13,5 @@\n     sample text 1\n     sample text 2\n+    added extra line here\n-    original text A\n+    updated original text A with changes\n@@ -33,14 +363,5 @@\n         checking status:\n-            perform operation X\n+            perform operation X only if specific condition holds\n+                new operation related to condition\n         evaluating next step:\n-            execute step Y\n+            revised execution of step Y\n```\n\"\"\"\n\nadd_example = \"\"\"\nUninteresting stuff\n```diff\n--- /dev/null\n+++ new_file.txt\n@@ -0,0 +1,3 @@\n+First example line\n+\n+Last example line\n```\n\"\"\"\n\nfile_example = \"\"\"# Introduction\n\n@Analysis\nOverview: outcomes\n%\nBackground: *context*\n\nMethod: []\n!\nTheories: ?\n> Leading up...\n    sample text 1\n    sample text 2\n    original text A\na\nChallenges: ~\n\nPerspectives: <>\n\nStrategy: {#}\n+\nOutcomes: ^^^\n\nFuture: |||\n\nx\n\nY\n\nZ\n\n\n\ncode\n         checking status:\n            perform operation X\n         evaluating next step:\n            execute step Y\nEnd.\n\nConclusion: ***\n\"\"\"\n\n\n# Single function tests\ndef test_basic_similarity():\n    assert is_similar(\"abc\", \"cab\")\n    assert not is_similar(\"abc\", \"def\")\n\n\ndef test_case_insensitivity_and_whitespace():\n    assert is_similar(\"A b C\", \"c a b\")\n    assert not is_similar(\"Abc\", \"D e F\")\n\n\ndef test_length_and_character_frequency():\n    assert is_similar(\"aabbc\", \"bacba\")\n    assert not is_similar(\"aabbcc\", \"abbcc\")\n\n\ndef test_edge_cases():\n    assert not is_similar(\"\", \"a\")\n    assert is_similar(\"a\", \"a\")\n\n\ndef insert_string_in_lined_string(string, to_insert, line_number):\n    split_string = string.split(\"\\n\")\n    split_string.insert(line_number - 1, to_insert)\n    return \"\\n\".join(split_string)\n\n\ndef test_diff_changing_one_file():\n    diffs = parse_diffs(example_diff)\n    for filename, diff in diffs.items():\n        string_diff = diff.diff_to_string()\n    correct_diff = \"\\n\".join(example_diff.strip().split(\"\\n\")[4:-1])\n    assert string_diff == correct_diff\n\n\ndef test_diff_adding_one_file():\n    add_diff = parse_diffs(add_example)\n    for filename, diff in add_diff.items():\n        string_add_diff = diff.diff_to_string()\n    correct_add_diff = \"\\n\".join(add_example.strip().split(\"\\n\")[2:-1])\n    assert string_add_diff == correct_add_diff\n\n\ndef test_diff_changing_two_files():\n    merged_diff = parse_diffs(example_diff + add_example)\n    correct_diff = \"\\n\".join(example_diff.strip().split(\"\\n\")[4:-1])\n    correct_add_diff = \"\\n\".join(add_example.strip().split(\"\\n\")[2:-1])\n    assert merged_diff[\"example.txt\"].diff_to_string() == correct_diff\n    assert merged_diff[\"new_file.txt\"].diff_to_string() == correct_add_diff\n\n\ndef test_validate_diff_correct():\n    lines_dict = file_to_lines_dict(file_example)\n    diffs = parse_diffs(example_diff)\n    # This is a test in its own right since it full of exceptions, would something go wrong\n    list(diffs.values())[0].validate_and_correct(lines_dict)\n\n\ndef test_correct_distorted_numbers():\n    lines_dict = file_to_lines_dict(file_example)\n    diffs = parse_diffs(example_line_dist_diff)\n    # This is a test in its own right since it full of exceptions, would something go wrong\n    list(diffs.values())[0].validate_and_correct(lines_dict)\n    correct_diff = \"\\n\".join(example_diff.strip().split(\"\\n\")[4:-1])\n    assert diffs[\"example.txt\"].diff_to_string() == correct_diff\n\n\ndef test_correct_skipped_lines():\n    distorted_example = insert_string_in_lined_string(\n        file_example, \"#\\n#comment\\n#\\n#\", 14\n    )\n    diffs = parse_diffs(example_diff)\n    list(diffs.values())[0].validate_and_correct(file_to_lines_dict(distorted_example))\n    with open(\n        os.path.join(\n            THIS_FILE_DIR,\n            \"improve_function_test_cases\",\n            \"corrected_diff_from_missing_lines\",\n        ),\n        \"r\",\n    ) as f:\n        corrected_diff_from_missing_lines = f.read()\n    assert (\n        diffs[\"example.txt\"].diff_to_string().strip()\n        == corrected_diff_from_missing_lines.strip()\n    )\n\n\ndef test_correct_skipped_lines_and_number_correction():\n    distorted_example = insert_string_in_lined_string(\n        file_example, \"#\\n#comment\\n#\\n#\", 14\n    )\n    diffs = parse_diffs(example_line_dist_diff)\n    # list(diffs.values())[0].validate_and_correct(file_to_lines_dict(distorted_example))\n    for diff in diffs.values():\n        problems = diff.validate_and_correct(file_to_lines_dict(distorted_example))\n        print(problems)\n    with open(\n        os.path.join(\n            THIS_FILE_DIR,\n            \"improve_function_test_cases\",\n            \"corrected_diff_from_missing_lines\",\n        ),\n        \"r\",\n    ) as f:\n        corrected_diff_from_missing_lines = f.read()\n    assert (\n        diffs[\"example.txt\"].diff_to_string().strip()\n        == corrected_diff_from_missing_lines.strip()\n    )\n\n\ndef test_diff_regex():\n    diff = parse_diffs(example_diff)\n    assert len(diff) == 1\n\n    diffs = parse_diffs(example_multiple_diffs)\n    assert len(diffs) == 2\n\n\ndef parse_chats_with_regex(\n    diff_file_name: str, code_file_name: str\n) -> Tuple[str, str, Dict]:\n    # Load the diff\n    with open(\n        os.path.join(THIS_FILE_DIR, \"improve_function_test_cases\", diff_file_name), \"r\"\n    ) as f:\n        diff_content = f.read()\n\n    # Load the corresponding code\n    with open(\n        os.path.join(THIS_FILE_DIR, \"improve_function_test_cases\", code_file_name), \"r\"\n    ) as f:\n        code_content = f.read()\n\n    # Parse the diffs\n    diffs = parse_diffs(diff_content)\n\n    return diff_content, code_content, diffs\n\n\n# test parse diff\ndef test_controller_diff():\n    parse_chats_with_regex(\"controller_chat\", \"controller_code\")\n\n\ndef test_simple_calculator_diff():\n    parse_chats_with_regex(\"simple_calculator_chat\", \"simple_calculator_code\")\n\n\ndef test_complex_temperature_converter_diff():\n    parse_chats_with_regex(\"temperature_converter_chat\", \"temperature_converter_code\")\n\n\ndef test_complex_task_master_diff():\n    parse_chats_with_regex(\"task_master_chat\", \"task_master_code\")\n\n\ndef test_long_file_diff():\n    parse_chats_with_regex(\"wheaties_example_chat\", \"wheaties_example_code\")\n\n\nif __name__ == \"__main__\":\n    pytest.main()\n", "tests/core/test_git.py": "import subprocess\nimport tempfile\n\nfrom pathlib import Path\n\nfrom gpt_engineer.core.git import (\n    filter_by_gitignore,\n    filter_files_with_uncommitted_changes,\n    init_git_repo,\n    is_git_installed,\n    is_git_repo,\n    stage_files,\n)\n\n\ndef test_verify_git_installed():\n    # If git isn't installed we can't run any git tests either way\n    assert is_git_installed()\n\n\ndef test_init_git_repo():\n    with tempfile.TemporaryDirectory() as tmpdir:\n        path = Path(tmpdir)\n        init_git_repo(path)\n        assert is_git_repo(path)\n\n\ndef test_stage_files():\n    with tempfile.TemporaryDirectory() as tmpdir:\n        path = Path(tmpdir)\n        init_git_repo(path)\n\n        # Create a file and stage it\n        file = path / \"test.txt\"\n        file.write_text(\"test\")\n\n        stage_files(path, [\"test.txt\"])\n\n        # Check if the file is staged\n        assert (\n            subprocess.run(\n                [\"git\", \"diff\", \"--cached\", \"--name-only\"],\n                cwd=path,\n                stdout=subprocess.PIPE,\n            )\n            .stdout.decode()\n            .strip()\n            == \"test.txt\"\n        )\n\n\ndef test_filter_by_gitignore():\n    with tempfile.TemporaryDirectory() as tmpdir:\n        path = Path(tmpdir)\n        init_git_repo(path)\n\n        # Create a .gitignore file\n        gitignore = path / \".gitignore\"\n        gitignore.write_text(\"*.txt\")\n        assert filter_by_gitignore(path, [\"test.txt\"]) == []\n\n\ndef test_filter_by_uncommitted_changes():\n    with tempfile.TemporaryDirectory() as tmpdir:\n        path = Path(tmpdir)\n        init_git_repo(path)\n\n        # Create a file and commit it\n        file = path / \"test.txt\"\n        file.write_text(\"test\")\n\n        subprocess.run([\"git\", \"add\", \"test.txt\"], cwd=path)\n        subprocess.run([\"git\", \"commit\", \"-m\", \"test\"], cwd=path)\n\n        # Update the file\n        file.write_text(\"test2\")\n\n        # Check if the file is staged\n        assert filter_files_with_uncommitted_changes(path, {\"test.txt\": \"test\"}) == [\n            \"test.txt\"\n        ]\n\n\ndef test_filter_by_uncommitted_changes_ignore_staged_files():\n    with tempfile.TemporaryDirectory() as tmpdir:\n        path = Path(tmpdir)\n        init_git_repo(path)\n\n        # Create a file but and stage it\n        file = path / \"test.txt\"\n        file.write_text(\"test\")\n        subprocess.run([\"git\", \"add\", \"test.txt\"], cwd=path)\n\n        # Check if the file is staged\n        assert filter_files_with_uncommitted_changes(path, {\"test.txt\": \"test\"}) == []\n\n\ndef test_filter_by_uncommitted_changes_ignore_untracked():\n    with tempfile.TemporaryDirectory() as tmpdir:\n        path = Path(tmpdir)\n        init_git_repo(path)\n\n        # Create a file but don't track it\n        file = path / \"test.txt\"\n        file.write_text(\"test\")\n\n        # Check if the file is staged\n        assert filter_files_with_uncommitted_changes(path, {\"test.txt\": \"test\"}) == []\n", "tests/core/test_ai.py": "from langchain.chat_models.base import BaseChatModel\nfrom langchain_community.chat_models.fake import FakeListChatModel\n\nfrom gpt_engineer.core.ai import AI\n\n\ndef mock_create_chat_model(self) -> BaseChatModel:\n    return FakeListChatModel(responses=[\"response1\", \"response2\", \"response3\"])\n\n\ndef test_start(monkeypatch):\n    monkeypatch.setattr(AI, \"_create_chat_model\", mock_create_chat_model)\n\n    ai = AI(\"gpt-4\")\n\n    # act\n    response_messages = ai.start(\"system prompt\", \"user prompt\", step_name=\"step name\")\n\n    # assert\n    assert response_messages[-1].content == \"response1\"\n\n\ndef test_next(monkeypatch):\n    # arrange\n    monkeypatch.setattr(AI, \"_create_chat_model\", mock_create_chat_model)\n\n    ai = AI(\"gpt-4\")\n    response_messages = ai.start(\"system prompt\", \"user prompt\", step_name=\"step name\")\n\n    # act\n    response_messages = ai.next(\n        response_messages, \"next user prompt\", step_name=\"step name\"\n    )\n\n    # assert\n    assert response_messages[-1].content == \"response2\"\n\n\ndef test_token_logging(monkeypatch):\n    # arrange\n    monkeypatch.setattr(AI, \"_create_chat_model\", mock_create_chat_model)\n\n    ai = AI(\"gpt-4\")\n\n    # act\n    response_messages = ai.start(\"system prompt\", \"user prompt\", step_name=\"step name\")\n    usageCostAfterStart = ai.token_usage_log.usage_cost()\n    ai.next(response_messages, \"next user prompt\", step_name=\"step name\")\n    usageCostAfterNext = ai.token_usage_log.usage_cost()\n\n    # assert\n    assert usageCostAfterStart > 0\n    assert usageCostAfterNext > usageCostAfterStart\n", "tests/core/test_salvage_correct_hunks.py": "import os\nimport shutil\n\nfrom typing import List\n\nimport pytest\n\nfrom langchain_core.messages import AIMessage\n\nfrom gpt_engineer.core.default.disk_memory import DiskMemory\nfrom gpt_engineer.core.default.paths import memory_path\nfrom gpt_engineer.core.default.steps import salvage_correct_hunks\nfrom gpt_engineer.core.files_dict import FilesDict\n\nTEST_FILES_DIR = os.path.dirname(os.path.abspath(__file__))\nmemory = DiskMemory(memory_path(\".\"))\n\n\ndef get_file_content(file_path: str) -> str:\n    with open(\n        os.path.join(TEST_FILES_DIR, \"improve_function_test_cases\", file_path), \"r\"\n    ) as f:\n        return f.read()\n\n\ndef message_builder(chat_path: str) -> List[AIMessage]:\n    chat_content = get_file_content(chat_path)\n\n    json = {\n        \"lc\": 1,\n        \"type\": \"constructor\",\n        \"id\": [\"langchain\", \"schema\", \"messages\", \"AIMessage\"],\n        \"kwargs\": {\n            \"content\": chat_content,\n            \"additional_kwargs\": {},\n            \"response_metadata\": {\"finish_reason\": \"stop\"},\n            \"name\": None,\n            \"id\": None,\n            \"example\": False,\n        },\n    }\n\n    return [AIMessage(**json[\"kwargs\"])]\n\n\ndef test_validation_and_apply_complex_diff():\n    files = FilesDict({\"taskmaster.py\": get_file_content(\"task_master_code\")})\n    salvage_correct_hunks(message_builder(\"task_master_chat\"), files, memory)\n\n\ndef test_validation_and_apply_long_diff():\n    files = FilesDict({\"VMClonetest.ps1\": get_file_content(\"wheaties_example_code\")})\n    salvage_correct_hunks(message_builder(\"wheaties_example_chat\"), files, memory)\n\n\ndef test_validation_and_apply_wrong_diff():\n    files = FilesDict(\n        {\"src/components/SocialLinks.tsx\": get_file_content(\"vgvishesh_example_code\")}\n    )\n    salvage_correct_hunks(message_builder(\"vgvishesh_example_chat\"), files, memory)\n\n\ndef test_validation_and_apply_non_change_diff():\n    files = FilesDict({\"src/App.tsx\": get_file_content(\"vgvishesh_example_2_code\")})\n    salvage_correct_hunks(message_builder(\"vgvishesh_example_2_chat\"), files, memory)\n\n\ndef test_validation_and_apply_diff_on_apps_benchmark_6():\n    files = FilesDict({\"main.py\": get_file_content(\"apps_benchmark_6_code\")})\n    salvage_correct_hunks(message_builder(\"apps_benchmark_6_chat\"), files, memory)\n\n\ndef test_validation_and_apply_diff_on_apps_benchmark_6_v2():\n    files = FilesDict({\"main.py\": get_file_content(\"apps_benchmark_6_v2_code\")})\n    salvage_correct_hunks(message_builder(\"apps_benchmark_6_v2_chat\"), files, memory)\n\n\ndef test_create_two_new_files():\n    files = FilesDict({\"main.py\": get_file_content(\"create_two_new_files_code\")})\n    salvage_correct_hunks(message_builder(\"create_two_new_files_chat\"), files, memory)\n\n\ndef test_theo_case():\n    files = FilesDict({\"dockerfile\": get_file_content(\"theo_case_code\")})\n    updated_files, _ = salvage_correct_hunks(\n        message_builder(\"theo_case_chat\"), files, memory\n    )\n    print(updated_files[\"dockerfile\"])\n    print(updated_files[\"run.py\"])\n\n\ndef test_clean_up_folder(clean_up_folder):\n    # The folder should be deleted after the test is run\n    assert True\n\n\n@pytest.fixture\ndef clean_up_folder():\n    yield\n    # Teardown code: delete a folder and all its contents\n    print(\"cleaning up\")\n    folder_path = os.path.join(os.path.dirname(__file__), \".gpteng\")\n    shutil.rmtree(folder_path, ignore_errors=True)\n\n\nif __name__ == \"__main__\":\n    pytest.main()\n", "tests/core/__init__.py": "", "tests/core/default/test_disk_execution_env.py": "import tempfile\nimport unittest\n\nfrom unittest.mock import MagicMock, patch\n\nfrom gpt_engineer.core.default.disk_execution_env import DiskExecutionEnv\n\n# from gpt_engineer.core.default.git_version_manager import GitVersionManager\nfrom gpt_engineer.core.default.paths import ENTRYPOINT_FILE\nfrom gpt_engineer.core.files_dict import FilesDict\n\n\nclass TestOnDiskExecutionEnv(unittest.TestCase):\n    def setUp(self):\n        self.temp_dir = tempfile.TemporaryDirectory()\n        self.env = DiskExecutionEnv()\n\n    def tearDown(self):\n        self.temp_dir.cleanup()\n\n    def test_successful_execution(self):\n        entrypoint_content = \"\"\"\n        python -m venv venv\n        source venv/bin/activate\n        python script.py\n        \"\"\"\n        code = {\n            ENTRYPOINT_FILE: entrypoint_content,\n            \"script.py\": \"print('This is a test script')\",\n        }\n        with patch(\"subprocess.Popen\") as mock_popen:\n            mock_popen.return_value.wait.return_value = 0\n            process = self.env.upload(FilesDict(code)).popen(f\"bash {ENTRYPOINT_FILE}\")\n            self.assertIsNotNone(process)\n            mock_popen.assert_called_once()\n\n    def test_missing_entrypoint(self):\n        code = {\"script.py\": \"print('This is a test script')\"}\n        p = self.env.upload(FilesDict(code)).popen(f\"bash {ENTRYPOINT_FILE}\")\n        p.communicate()\n        assert p.returncode != 0\n\n    def test_keyboard_interrupt_handling(self):\n        entrypoint_content = \"\"\"\n        python script.py\n        \"\"\"\n        code = {\n            ENTRYPOINT_FILE: entrypoint_content,\n            \"script.py\": \"print('This is a test script')\",\n        }\n        with patch(\"subprocess.Popen\") as mock_popen:\n            mock_process = MagicMock()\n            mock_process.poll.side_effect = KeyboardInterrupt\n            mock_popen.return_value = mock_process\n            stdout_full, stderr_full, returncode = self.env.upload(FilesDict(code)).run(\n                f\"bash {ENTRYPOINT_FILE}\"\n            )\n            mock_process.kill.assert_called_once()\n\n    def test_execution_with_output(self):\n        entrypoint_content = \"\"\"\n        python script.py\n        \"\"\"\n        code = {\n            ENTRYPOINT_FILE: entrypoint_content,\n            \"script.py\": \"import sys; print('Out'); sys.stderr.write('Error')\",\n        }\n        with patch(\"subprocess.Popen\") as mock_popen:\n            process = MagicMock()\n            process.wait.return_value = 0\n            process.communicate.return_value = (b\"Out\\n\", b\"Error\\n\")\n            mock_popen.return_value = process\n            process = self.env.upload(FilesDict(code)).popen(f\"bash {ENTRYPOINT_FILE}\")\n            stdout, stderr = process.communicate()\n            self.assertEqual(stdout, b\"Out\\n\")\n            self.assertEqual(stderr, b\"Error\\n\")\n\n\nif __name__ == \"__main__\":\n    unittest.main()\n", "tests/core/default/test_disk_file_repository.py": "import pytest\n\nfrom gpt_engineer.core.default.disk_memory import DiskMemory\n\n\ndef test_DB_operations(tmp_path):\n    # Test initialization\n    db = DiskMemory(tmp_path)\n\n    # Test __setitem__\n    db[\"test_key\"] = \"test_value\"\n\n    assert (tmp_path / \"test_key\").is_file()\n\n    # Test __getitem__\n    val = db[\"test_key\"]\n\n    assert val == \"test_value\"\n\n\ndef test_large_files(tmp_path):\n    db = DiskMemory(tmp_path)\n    large_content = \"a\" * (10**6)  # 1MB of tools\n\n    # Test write large files\n    db[\"large_file\"] = large_content\n\n    # Test read large files\n    assert db[\"large_file\"] == large_content\n\n\ndef test_concurrent_access(tmp_path):\n    import threading\n\n    db = DiskMemory(tmp_path)\n\n    num_threads = 10\n    num_writes = 1000\n\n    def write_to_db(thread_id):\n        for i in range(num_writes):\n            key = f\"thread{thread_id}_write{i}\"\n            db[key] = str(i)\n\n    threads = []\n    for thread_id in range(num_threads):\n        t = threading.Thread(target=write_to_db, args=(thread_id,))\n        t.start()\n        threads.append(t)\n\n    for t in threads:\n        t.join()\n\n    # Verify that all expected tools was written\n    for thread_id in range(num_threads):\n        for i in range(num_writes):\n            key = f\"thread{thread_id}_write{i}\"\n            assert key in db  # using __contains__ now\n            assert db[key] == str(i)\n\n\ndef test_error_messages(tmp_path):\n    db = DiskMemory(tmp_path)\n\n    # Test error on getting non-existent key\n    with pytest.raises(KeyError):\n        db[\"non_existent\"]\n\n    with pytest.raises(TypeError) as e:\n        db[\"key\"] = [\"Invalid\", \"value\"]\n\n    assert str(e.value) == \"val must be str\"\n\n\n# Generated by CodiumAI\n\n\nclass TestOnDiskRepository:\n    #  can set and get a value for a key\n    def test_set_and_get_value(self, tmp_path):\n        db = DiskMemory(tmp_path)\n        db[\"test_key\"] = \"test_value\"\n\n        assert (tmp_path / \"test_key\").is_file()\n        val = db[\"test_key\"]\n\n        assert val == \"test_value\"\n\n    #  can check if a key exists in the database\n    def test_key_exists(self, tmp_path):\n        db = DiskMemory(tmp_path)\n        db[\"test_key\"] = \"test_value\"\n\n        assert \"test_key\" in db\n        assert \"nonexistent_key\" not in db\n\n    #  can fetch a default value if a key does not exist\n    def test_fetch_default_value(self, tmp_path):\n        db = DiskMemory(tmp_path)\n        default_val = \"default_value\"\n\n        assert db.get(\"nonexistent_key\", default_val) == default_val\n\n    #  can delete a file or directory in the database\n    def test_delete_file_or_directory(self, tmp_path):\n        db = DiskMemory(tmp_path)\n        db[\"test_file\"] = \"test_content\"\n        db[\"test_directory/test_file\"] = \"test_content\"\n\n        del db[\"test_file\"]\n        del db[\"test_directory\"]\n\n        assert not (tmp_path / \"test_file\").exists()\n        assert not (tmp_path / \"test_directory\").exists()\n\n    #  can iterate over all files in the database\n    def test_iterate_files(self, tmp_path):\n        db = DiskMemory(tmp_path)\n        db[\"file1.txt\"] = \"content1\"\n        db[\"file2.txt\"] = \"content2\"\n        db[\"directory/file3.txt\"] = \"content3\"\n\n        files = list(db)\n\n        assert len(files) == 3\n        assert \"file1.txt\" in files\n        assert \"file2.txt\" in files\n        assert \"directory/file3.txt\" in files\n\n    #  raises a KeyError if a non-existent key is accessed\n    def test_key_error(self, tmp_path):\n        db = DiskMemory(tmp_path)\n\n        with pytest.raises(KeyError):\n            _ = db[\"nonexistent_key\"]\n\n    #  raises a ValueError if a file name attempts to access parent path\n    def test_value_error(self, tmp_path):\n        db = DiskMemory(tmp_path)\n\n        with pytest.raises(ValueError):\n            db[\"../file.txt\"] = \"content\"\n\n    #  raises a TypeError if a non-string value is set for a key\n    def test_type_error(self, tmp_path):\n        db = DiskMemory(tmp_path)\n\n        with pytest.raises(TypeError):\n            db[\"test_key\"] = 123\n\n    #  can handle large file contents\n    def test_large_file_contents(self, tmp_path):\n        db = DiskMemory(tmp_path)\n        large_content = \"a\" * (10**6)  # 1MB of tools\n        db[\"large_file\"] = large_content\n\n        assert db[\"large_file\"] == large_content\n", "tests/core/default/test_steps.py": "# Generated by CodiumAI\nimport tempfile\n\nfrom unittest.mock import MagicMock\n\nimport pytest\n\nfrom langchain.schema import SystemMessage\n\nfrom gpt_engineer.core.ai import AI\nfrom gpt_engineer.core.default.disk_memory import DiskMemory\nfrom gpt_engineer.core.default.paths import ENTRYPOINT_FILE, PREPROMPTS_PATH\nfrom gpt_engineer.core.default.steps import (\n    curr_fn,\n    gen_code,\n    gen_entrypoint,\n    improve_fn,\n    setup_sys_prompt,\n    setup_sys_prompt_existing_code,\n)\nfrom gpt_engineer.core.files_dict import FilesDict\nfrom gpt_engineer.core.linting import Linting\nfrom gpt_engineer.core.preprompts_holder import PrepromptsHolder\nfrom gpt_engineer.core.prompt import Prompt\n\nfactorial_program = \"\"\"\nTo implement a function that calculates the factorial of a number in Python, we will create a simple Python module with a single function `factorial`. The factorial of a non-negative integer `n` is the product of all positive integers less than or equal to `n`. It is denoted by `n!`. The factorial of 0 is defined to be 1.\n\nLet's start by creating the `factorial.py` file which will contain our `factorial` function.\n\nfactorial.py\n```python\ndef factorial(n: int) -> int:\n    if n < 0:\n        raise ValueError(\"Factorial is not defined for negative numbers\")\n    elif n == 0:\n        return 1\n    else:\n        result = 1\n        for i in range(1, n + 1):\n            result *= i\n        return result\n\nif __name__ == \"__main__\":\nimport sys\n\nif len(sys.argv) != 2:\n    print(\"Usage: python factorial.py <number>\")\n    sys.exit(1)\n\ntry:\n    number = int(sys.argv[1])\n    print(f\"The factorial of {number} is {factorial(number)}\")\nexcept ValueError as e:\n    print(e)\n    sys.exit(1)\n```\n\nNow, let's create a `requirements.txt` file to specify the dependencies for this module. Since we are not using any external libraries, the `requirements.txt` file will be empty, but it's a good practice to include it in Python projects.\n\nrequirements.txt\n```\n# No dependencies required\n```\nThis concludes a fully working implementation.```\n\"\"\"\n\nfactorial_entrypoint = \"\"\"\nIrrelevant explanations\n```sh\npython3 -m venv venv\nsource venv/bin/activate\npip install -r requirements.txt\npytest test_factorial.py\n```\n    \"\"\"\n\n\nclass TestGenCode:\n    #  Generates code based on a given prompt using an AI model.\n    def test_generates_code_using_ai_model(self):\n        # Mock AI class\n        class MockAI:\n            def start(self, sys_prompt, user_prompt, step_name):\n                return [SystemMessage(content=factorial_program)]\n\n        ai = MockAI()\n        prompt = Prompt(\"Write a function that calculates the factorial of a number.\")\n\n        memory = DiskMemory(tempfile.mkdtemp())\n        preprompts_holder = PrepromptsHolder(PREPROMPTS_PATH)\n        code = gen_code(ai, prompt, memory, preprompts_holder)\n\n        assert isinstance(code, FilesDict)\n        assert len(code) == 2\n        # assert CODE_GEN_LOG_FILE in memory\n        # assert memory[CODE_GEN_LOG_FILE] == factorial_program.strip()\n\n    #  The generated code is saved to disk.\n    def test_generated_code_saved_to_disk(self):\n        # Mock AI class\n        class MockAI:\n            def start(self, sys_prompt, user_prompt, step_name):\n                return [SystemMessage(content=factorial_program)]\n\n        ai = MockAI()\n        prompt = Prompt(\"Write a function that calculates the factorial of a number.\")\n        memory = DiskMemory(tempfile.mkdtemp())\n        preprompts_holder = PrepromptsHolder(PREPROMPTS_PATH)\n        code = gen_code(ai, prompt, memory, preprompts_holder)\n\n        assert isinstance(code, FilesDict)\n        assert len(code) == 2\n        # assert CODE_GEN_LOG_FILE in memory\n        # assert memory[CODE_GEN_LOG_FILE] == factorial_program.strip()\n\n    #  Raises TypeError if keys are not strings or Path objects.\n    def test_raises_type_error_if_keys_not_strings_or_path_objects(self):\n        # Mock AI class\n        class MockAI:\n            def start(self, sys_prompt, user_prompt, step_name):\n                return [SystemMessage(content=factorial_program)]\n\n        ai = MockAI()\n        prompt = Prompt(\"Write a function that calculates the factorial of a number.\")\n        memory = DiskMemory(tempfile.mkdtemp())\n        preprompts_holder = PrepromptsHolder(PREPROMPTS_PATH)\n        with pytest.raises(TypeError):\n            code = gen_code(ai, prompt, memory, preprompts_holder)\n            code[123] = \"code\"\n\n    #  Raises TypeError if values are not strings.\n    def test_raises_type_error_if_values_not_strings(self):\n        # Mock AI class\n        class MockAI:\n            def start(self, sys_prompt, user_prompt, step_name):\n                return [SystemMessage(content=factorial_program)]\n\n        ai = MockAI()\n        prompt = Prompt(\"Write a function that calculates the factorial of a number.\")\n        memory = DiskMemory(tempfile.mkdtemp())\n        preprompts_holder = PrepromptsHolder(PREPROMPTS_PATH)\n        with pytest.raises(TypeError):\n            code = gen_code(ai, prompt, memory, preprompts_holder)\n            code[\"file.py\"] = 123\n\n    #  Raises KeyError if the file does not exist in the database.\n    def test_raises_key_error_if_file_not_exist_in_database(self):\n        # Mock AI class\n        class MockAI:\n            def start(self, sys_prompt, user_prompt, step_name):\n                return [SystemMessage(content=factorial_program)]\n\n        ai = MockAI()\n        prompt = Prompt(\"Write a function that calculates the factorial of a number.\")\n        memory = DiskMemory(tempfile.mkdtemp())\n        preprompts_holder = PrepromptsHolder(PREPROMPTS_PATH)\n        with pytest.raises(KeyError):\n            code = gen_code(ai, prompt, memory, preprompts_holder)\n            code[\"nonexistent_file.py\"]\n\n\nclass TestStepUtilities:\n    def test_called_from_function(self):\n        # Arrange\n        def test_function():\n            return curr_fn()\n\n        expected_name = \"test_function\"\n\n        # Act\n        actual_name = test_function()\n\n        # Assert\n        assert actual_name == expected_name\n\n    def test_constructs_system_prompt_with_predefined_instructions_and_philosophies(\n        self,\n    ):\n        preprompts_holder = PrepromptsHolder(PREPROMPTS_PATH)\n        preprompts = preprompts_holder.get_preprompts()\n        sys_prompt = setup_sys_prompt(preprompts)\n        expected_prompt = (\n            preprompts[\"roadmap\"]\n            + preprompts[\"generate\"].replace(\"FILE_FORMAT\", preprompts[\"file_format\"])\n            + \"\\nUseful to know:\\n\"\n            + preprompts[\"philosophy\"]\n        )\n        assert sys_prompt == expected_prompt\n\n    def test_constructs_system_prompt(self):\n        preprompts_holder = PrepromptsHolder(PREPROMPTS_PATH)\n        preprompts = preprompts_holder.get_preprompts()\n        expected_prompt = (\n            preprompts[\"roadmap\"]\n            + preprompts[\"improve\"].replace(\n                \"FILE_FORMAT\", preprompts[\"file_format_diff\"]\n            )\n            + \"\\nUseful to know:\\n\"\n            + preprompts[\"philosophy\"]\n        )\n        actual_prompt = setup_sys_prompt_existing_code(preprompts)\n        assert actual_prompt == expected_prompt\n\n\nclass TestGenEntrypoint:\n    class MockAI:\n        def __init__(self, content):\n            self.content = content\n\n        def start(self, system, user, step_name):\n            return [SystemMessage(content=self.content)]\n\n    #  The function receives valid input and generates a valid entry point script.\n    def test_valid_input_generates_valid_entrypoint(self):\n        # Mock AI class\n\n        ai_mock = TestGenEntrypoint.MockAI(factorial_entrypoint)\n        code = FilesDict()\n        tempdir = tempfile.mkdtemp()\n        memory = DiskMemory(tempdir)\n        prompt = Prompt(\"\")\n        # Act\n        preprompts_holder = PrepromptsHolder(PREPROMPTS_PATH)\n        entrypoint_code = gen_entrypoint(\n            ai_mock, prompt, code, memory, preprompts_holder\n        )\n\n        # Assert\n        assert ENTRYPOINT_FILE in entrypoint_code\n        assert isinstance(entrypoint_code[ENTRYPOINT_FILE], str)\n        assert (\n            entrypoint_code[ENTRYPOINT_FILE]\n            == \"\"\"python3 -m venv venv\nsource venv/bin/activate\npip install -r requirements.txt\npytest test_factorial.py\n\"\"\"\n        )\n        # assert ENTRYPOINT_LOG_FILE in memory\n        # assert isinstance(memory[ENTRYPOINT_LOG_FILE], str)\n        # assert memory[ENTRYPOINT_LOG_FILE] == factorial_entrypoint.strip()\n\n    #  The function receives an empty codebase and returns an empty entry point script.\n    def test_empty_codebase_returns_empty_entrypoint(self):\n        # Arrange\n        ai_mock = TestGenEntrypoint.MockAI(\"Irrelevant explanation\")\n\n        code = FilesDict()\n        tempdir = tempfile.mkdtemp()\n        memory = DiskMemory(tempdir)\n        prompt = Prompt(\"\")\n        # Act\n        preprompts_holder = PrepromptsHolder(PREPROMPTS_PATH)\n        entrypoint_code = gen_entrypoint(\n            ai_mock, prompt, code, memory, preprompts_holder\n        )\n\n        # Assert\n        assert ENTRYPOINT_FILE in entrypoint_code\n        assert isinstance(entrypoint_code[ENTRYPOINT_FILE], str)\n        assert entrypoint_code[ENTRYPOINT_FILE] == \"\"\n        # assert ENTRYPOINT_LOG_FILE in memory\n        # assert isinstance(memory[ENTRYPOINT_LOG_FILE], str)\n        # assert memory[ENTRYPOINT_LOG_FILE] == \"Irrelevant explanation\"\n\n\nclass TestImprove:\n    def test_improve_existing_code(self, tmp_path):\n        # Mock the AI class\n        ai_patch = \"\"\"\nSome introductory text.\n```diff\n--- main.py\n+++ main.py\n@@ -1,1 +1,1 @@\n-print('Hello, World!')\n+print('Goodbye, World!')\n```\n\"\"\"\n        ai_mock = MagicMock(spec=AI)\n        ai_mock.next.return_value = [SystemMessage(content=ai_patch)]\n\n        # Create a Code object with existing code\n        code = FilesDict(\n            {\n                \"main.py\": \"print('Hello, World!')\",\n                \"requirements.txt\": \"numpy==1.18.1\",\n                \"README.md\": \"This is a sample code repository.\",\n            }\n        )\n\n        # Create a BaseRepository object for memory\n        memory = DiskMemory(tmp_path)\n\n        # Define the user prompt\n        prompt = Prompt(\n            \"Change the program to print 'Goodbye, World!' instead of 'Hello, World!'\"\n        )\n\n        # Call the improve function\n        preprompts_holder = PrepromptsHolder(PREPROMPTS_PATH)\n        improved_code = improve_fn(ai_mock, prompt, code, memory, preprompts_holder)\n\n        # Assert that the code was improved correctly\n        expected_code = FilesDict(\n            {\n                \"main.py\": \"print('Goodbye, World!')\",\n                \"requirements.txt\": \"numpy==1.18.1\",\n                \"README.md\": \"This is a sample code repository.\",\n            }\n        )\n        assert improved_code == expected_code\n\n    def test_lint_python(self):\n        linting = Linting()\n        content = \"print('Hello, world! ')\"\n        config = {\"line_length\": 50}\n        linted_content = linting.lint_python(content, config)\n        assert linted_content is not None, \"Linted content should not be None\"\n\n    def test_lint_files(self):\n        linting = Linting()\n        files_dict = FilesDict({\"test.py\": \"print('Hello, world! ')\"})\n        config = {\"line_length\": 50}\n        linted_files_dict = linting.lint_files(files_dict, config)\n        assert linted_files_dict is not None, \"Linted files dict should not be None\"\n        assert isinstance(\n            linted_files_dict, FilesDict\n        ), \"Output should be an instance of FilesDict\"\n        assert (\n            \"test.py\" in linted_files_dict\n        ), \"test.py should be in the linted files dict\"\n", "tests/core/default/test_simple_agent.py": "import tempfile\n\nimport pytest\n\nfrom langchain.schema import AIMessage\n\nfrom gpt_engineer.core.default.disk_execution_env import DiskExecutionEnv\nfrom gpt_engineer.core.default.paths import ENTRYPOINT_FILE\nfrom gpt_engineer.core.default.simple_agent import SimpleAgent\nfrom gpt_engineer.core.files_dict import FilesDict\nfrom gpt_engineer.core.prompt import Prompt\nfrom tests.mock_ai import MockAI\n\n\ndef test_init():\n    temp_dir = tempfile.mkdtemp()\n    mock_ai = MockAI(\n        [\n            AIMessage(\n                \"hello_world.py\\n```\\nwith open('output.txt', 'w') as file:\\n    file.write('Hello World!')\\n```\"\n            ),\n            AIMessage(\"```run.sh\\npython3 hello_world.py\\n```\"),\n        ],\n    )\n    lean_agent = SimpleAgent.with_default_config(temp_dir, mock_ai)\n    outfile = \"output.txt\"\n    code = lean_agent.init(\n        Prompt(\n            f\"Make a program that prints 'Hello World!' to a file called '{outfile}'\"\n        )\n    )\n\n    env = DiskExecutionEnv()\n    env.upload(code).run(f\"bash {ENTRYPOINT_FILE}\")\n    code = env.download()\n\n    assert outfile in code\n    assert code[outfile] == \"Hello World!\"\n\n\ndef test_improve():\n    temp_dir = tempfile.mkdtemp()\n    code = FilesDict(\n        {\n            \"main.py\": \"def write_hello_world_to_file(filename):\\n    \\\"\\\"\\\"\\n    Writes 'Hello World!' to the specified file.\\n    \\n    :param filename: The name of the file to write to.\\n    \\\"\\\"\\\"\\n    with open(filename, 'w') as file:\\n        file.write('Hello World!')\\n\\nif __name__ == \\\"__main__\\\":\\n    output_filename = 'output.txt'\\n    write_hello_world_to_file(output_filename)\",\n            \"requirements.txt\": \"# No dependencies required\",\n            \"run.sh\": \"python3 main.py\\n\",\n        }\n    )\n    mock_ai = MockAI(\n        [\n            AIMessage(\n                \"```diff\\n--- main.py\\n+++ main.py\\n@@ -7,3 +7,3 @@\\n     with open(filename, 'w') as file:\\n-        file.write('Hello World!')\\n+        file.write('!dlroW olleH')\\n```\"\n            )\n        ]\n    )\n    lean_agent = SimpleAgent.with_default_config(temp_dir, mock_ai)\n    code = lean_agent.improve(\n        code,\n        Prompt(\n            \"Change the program so that it prints '!dlroW olleH' instead of 'Hello World!' \"\n        ),\n        f\"bash {ENTRYPOINT_FILE}\",\n    )\n\n    env = DiskExecutionEnv()\n    env.upload(code).run(f\"bash {ENTRYPOINT_FILE}\")\n    code = env.download()\n\n    outfile = \"output.txt\"\n    assert outfile in code\n    assert code[outfile] == \"!dlroW olleH\"\n\n\nif __name__ == \"__main__\":\n    pytest.main()\n", "tests/core/default/__init__.py": "", "tests/benchmark/test_BenchConfig.py": "# Generated by CodiumAI\n\nimport pytest\n\nfrom gpt_engineer.benchmark.bench_config import (\n    AppsConfig,\n    BenchConfig,\n    GptmeConfig,\n    MbppConfig,\n)\n\n\nclass TestBenchConfig:\n    #  Creating a BenchConfig object with default values should return an instance of BenchConfig with all attributes set to their default values.\n    def test_default_values(self):\n        config = BenchConfig()\n        assert isinstance(config.apps, AppsConfig)\n        assert isinstance(config.mbpp, MbppConfig)\n        assert isinstance(config.gptme, GptmeConfig)\n        assert config.apps.active is True\n        assert config.apps.test_start_index == 0\n        assert config.apps.test_end_index == 1\n        assert config.apps.train_start_index == 0\n        assert config.apps.train_end_index == 0\n        assert config.mbpp.active is True\n        assert config.mbpp.test_len == 1\n        assert config.mbpp.train_len == 0\n        assert config.gptme.active is True\n\n    #  Creating a BenchConfig object with specific values should return an instance of BenchConfig with the specified attributes set to the specified values.\n    def test_specific_values(self):\n        config = BenchConfig(\n            apps=AppsConfig(\n                active=False,\n                test_start_index=1,\n                test_end_index=2,\n                train_start_index=3,\n                train_end_index=4,\n            ),\n            mbpp=MbppConfig(active=False, test_len=5, train_len=6),\n            gptme=GptmeConfig(active=False),\n        )\n        assert isinstance(config.apps, AppsConfig)\n        assert isinstance(config.mbpp, MbppConfig)\n        assert isinstance(config.gptme, GptmeConfig)\n        assert config.apps.active is False\n        assert config.apps.test_start_index == 1\n        assert config.apps.test_end_index == 2\n        assert config.apps.train_start_index == 3\n        assert config.apps.train_end_index == 4\n        assert config.mbpp.active is False\n        assert config.mbpp.test_len == 5\n        assert config.mbpp.train_len == 6\n        assert config.gptme.active is False\n\n    #  Calling the from_dict method with a valid dictionary should return an instance of BenchConfig with attributes set according to the values in the dictionary.\n    def test_from_dict_valid_dict(self):\n        config_dict = {\n            \"apps\": {\n                \"active\": False,\n                \"test_start_index\": 1,\n                \"test_end_index\": 2,\n                \"train_start_index\": 3,\n                \"train_end_index\": 4,\n            },\n            \"mbpp\": {\"active\": False, \"test_len\": 5, \"train_len\": 6},\n            \"gptme\": {\"active\": False},\n        }\n        config = BenchConfig.from_dict(config_dict)\n        assert isinstance(config.apps, AppsConfig)\n        assert isinstance(config.mbpp, MbppConfig)\n        assert isinstance(config.gptme, GptmeConfig)\n        assert config.apps.active is False\n        assert config.apps.test_start_index == 1\n        assert config.apps.test_end_index == 2\n        assert config.apps.train_start_index == 3\n        assert config.apps.train_end_index == 4\n        assert config.mbpp.active is False\n        assert config.mbpp.test_len == 5\n        assert config.mbpp.train_len == 6\n        assert config.gptme.active is False\n\n    #  Calling the from_toml method with an invalid path to a TOML file should raise an appropriate exception.\n    def test_from_toml_invalid_path(self):\n        config_file = \"invalid_config.toml\"\n        with pytest.raises(Exception):\n            BenchConfig.from_toml(config_file)\n"}