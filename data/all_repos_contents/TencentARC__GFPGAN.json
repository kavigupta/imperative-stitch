{"cog_predict.py": "# flake8: noqa\n# This file is used for deploying replicate models\n# running: cog predict -i img=@inputs/whole_imgs/10045.png -i version='v1.4' -i scale=2\n# push: cog push r8.im/tencentarc/gfpgan\n# push (backup): cog push r8.im/xinntao/gfpgan\n\nimport os\n\nos.system('python setup.py develop')\nos.system('pip install realesrgan')\n\nimport cv2\nimport shutil\nimport tempfile\nimport torch\nfrom basicsr.archs.srvgg_arch import SRVGGNetCompact\n\nfrom gfpgan import GFPGANer\n\ntry:\n    from cog import BasePredictor, Input, Path\n    from realesrgan.utils import RealESRGANer\nexcept Exception:\n    print('please install cog and realesrgan package')\n\n\nclass Predictor(BasePredictor):\n\n    def setup(self):\n        os.makedirs('output', exist_ok=True)\n        # download weights\n        if not os.path.exists('gfpgan/weights/realesr-general-x4v3.pth'):\n            os.system(\n                'wget https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.5.0/realesr-general-x4v3.pth -P ./gfpgan/weights'\n            )\n        if not os.path.exists('gfpgan/weights/GFPGANv1.2.pth'):\n            os.system(\n                'wget https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.2.pth -P ./gfpgan/weights')\n        if not os.path.exists('gfpgan/weights/GFPGANv1.3.pth'):\n            os.system(\n                'wget https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.3.pth -P ./gfpgan/weights')\n        if not os.path.exists('gfpgan/weights/GFPGANv1.4.pth'):\n            os.system(\n                'wget https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.4.pth -P ./gfpgan/weights')\n        if not os.path.exists('gfpgan/weights/RestoreFormer.pth'):\n            os.system(\n                'wget https://github.com/TencentARC/GFPGAN/releases/download/v1.3.4/RestoreFormer.pth -P ./gfpgan/weights'\n            )\n\n        # background enhancer with RealESRGAN\n        model = SRVGGNetCompact(num_in_ch=3, num_out_ch=3, num_feat=64, num_conv=32, upscale=4, act_type='prelu')\n        model_path = 'gfpgan/weights/realesr-general-x4v3.pth'\n        half = True if torch.cuda.is_available() else False\n        self.upsampler = RealESRGANer(\n            scale=4, model_path=model_path, model=model, tile=0, tile_pad=10, pre_pad=0, half=half)\n\n        # Use GFPGAN for face enhancement\n        self.face_enhancer = GFPGANer(\n            model_path='gfpgan/weights/GFPGANv1.4.pth',\n            upscale=2,\n            arch='clean',\n            channel_multiplier=2,\n            bg_upsampler=self.upsampler)\n        self.current_version = 'v1.4'\n\n    def predict(\n            self,\n            img: Path = Input(description='Input'),\n            version: str = Input(\n                description='GFPGAN version. v1.3: better quality. v1.4: more details and better identity.',\n                choices=['v1.2', 'v1.3', 'v1.4', 'RestoreFormer'],\n                default='v1.4'),\n            scale: float = Input(description='Rescaling factor', default=2),\n    ) -> Path:\n        weight = 0.5\n        print(img, version, scale, weight)\n        try:\n            extension = os.path.splitext(os.path.basename(str(img)))[1]\n            img = cv2.imread(str(img), cv2.IMREAD_UNCHANGED)\n            if len(img.shape) == 3 and img.shape[2] == 4:\n                img_mode = 'RGBA'\n            elif len(img.shape) == 2:\n                img_mode = None\n                img = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n            else:\n                img_mode = None\n\n            h, w = img.shape[0:2]\n            if h < 300:\n                img = cv2.resize(img, (w * 2, h * 2), interpolation=cv2.INTER_LANCZOS4)\n\n            if self.current_version != version:\n                if version == 'v1.2':\n                    self.face_enhancer = GFPGANer(\n                        model_path='gfpgan/weights/GFPGANv1.2.pth',\n                        upscale=2,\n                        arch='clean',\n                        channel_multiplier=2,\n                        bg_upsampler=self.upsampler)\n                    self.current_version = 'v1.2'\n                elif version == 'v1.3':\n                    self.face_enhancer = GFPGANer(\n                        model_path='gfpgan/weights/GFPGANv1.3.pth',\n                        upscale=2,\n                        arch='clean',\n                        channel_multiplier=2,\n                        bg_upsampler=self.upsampler)\n                    self.current_version = 'v1.3'\n                elif version == 'v1.4':\n                    self.face_enhancer = GFPGANer(\n                        model_path='gfpgan/weights/GFPGANv1.4.pth',\n                        upscale=2,\n                        arch='clean',\n                        channel_multiplier=2,\n                        bg_upsampler=self.upsampler)\n                    self.current_version = 'v1.4'\n                elif version == 'RestoreFormer':\n                    self.face_enhancer = GFPGANer(\n                        model_path='gfpgan/weights/RestoreFormer.pth',\n                        upscale=2,\n                        arch='RestoreFormer',\n                        channel_multiplier=2,\n                        bg_upsampler=self.upsampler)\n\n            try:\n                _, _, output = self.face_enhancer.enhance(\n                    img, has_aligned=False, only_center_face=False, paste_back=True, weight=weight)\n            except RuntimeError as error:\n                print('Error', error)\n\n            try:\n                if scale != 2:\n                    interpolation = cv2.INTER_AREA if scale < 2 else cv2.INTER_LANCZOS4\n                    h, w = img.shape[0:2]\n                    output = cv2.resize(output, (int(w * scale / 2), int(h * scale / 2)), interpolation=interpolation)\n            except Exception as error:\n                print('wrong scale input.', error)\n\n            if img_mode == 'RGBA':  # RGBA images should be saved in png format\n                extension = 'png'\n            # save_path = f'output/out.{extension}'\n            # cv2.imwrite(save_path, output)\n            out_path = Path(tempfile.mkdtemp()) / f'out.{extension}'\n            cv2.imwrite(str(out_path), output)\n        except Exception as error:\n            print('global exception: ', error)\n        finally:\n            clean_folder('output')\n        return out_path\n\n\ndef clean_folder(folder):\n    for filename in os.listdir(folder):\n        file_path = os.path.join(folder, filename)\n        try:\n            if os.path.isfile(file_path) or os.path.islink(file_path):\n                os.unlink(file_path)\n            elif os.path.isdir(file_path):\n                shutil.rmtree(file_path)\n        except Exception as e:\n            print(f'Failed to delete {file_path}. Reason: {e}')\n", "setup.py": "#!/usr/bin/env python\n\nfrom setuptools import find_packages, setup\n\nimport os\nimport subprocess\nimport time\n\nversion_file = 'gfpgan/version.py'\n\n\ndef readme():\n    with open('README.md', encoding='utf-8') as f:\n        content = f.read()\n    return content\n\n\ndef get_git_hash():\n\n    def _minimal_ext_cmd(cmd):\n        # construct minimal environment\n        env = {}\n        for k in ['SYSTEMROOT', 'PATH', 'HOME']:\n            v = os.environ.get(k)\n            if v is not None:\n                env[k] = v\n        # LANGUAGE is used on win32\n        env['LANGUAGE'] = 'C'\n        env['LANG'] = 'C'\n        env['LC_ALL'] = 'C'\n        out = subprocess.Popen(cmd, stdout=subprocess.PIPE, env=env).communicate()[0]\n        return out\n\n    try:\n        out = _minimal_ext_cmd(['git', 'rev-parse', 'HEAD'])\n        sha = out.strip().decode('ascii')\n    except OSError:\n        sha = 'unknown'\n\n    return sha\n\n\ndef get_hash():\n    if os.path.exists('.git'):\n        sha = get_git_hash()[:7]\n    else:\n        sha = 'unknown'\n\n    return sha\n\n\ndef write_version_py():\n    content = \"\"\"# GENERATED VERSION FILE\n# TIME: {}\n__version__ = '{}'\n__gitsha__ = '{}'\nversion_info = ({})\n\"\"\"\n    sha = get_hash()\n    with open('VERSION', 'r') as f:\n        SHORT_VERSION = f.read().strip()\n    VERSION_INFO = ', '.join([x if x.isdigit() else f'\"{x}\"' for x in SHORT_VERSION.split('.')])\n\n    version_file_str = content.format(time.asctime(), SHORT_VERSION, sha, VERSION_INFO)\n    with open(version_file, 'w') as f:\n        f.write(version_file_str)\n\n\ndef get_version():\n    with open(version_file, 'r') as f:\n        exec(compile(f.read(), version_file, 'exec'))\n    return locals()['__version__']\n\n\ndef get_requirements(filename='requirements.txt'):\n    here = os.path.dirname(os.path.realpath(__file__))\n    with open(os.path.join(here, filename), 'r') as f:\n        requires = [line.replace('\\n', '') for line in f.readlines()]\n    return requires\n\n\nif __name__ == '__main__':\n    write_version_py()\n    setup(\n        name='gfpgan',\n        version=get_version(),\n        description='GFPGAN aims at developing Practical Algorithms for Real-world Face Restoration',\n        long_description=readme(),\n        long_description_content_type='text/markdown',\n        author='Xintao Wang',\n        author_email='xintao.wang@outlook.com',\n        keywords='computer vision, pytorch, image restoration, super-resolution, face restoration, gan, gfpgan',\n        url='https://github.com/TencentARC/GFPGAN',\n        include_package_data=True,\n        packages=find_packages(exclude=('options', 'datasets', 'experiments', 'results', 'tb_logger', 'wandb')),\n        classifiers=[\n            'Development Status :: 4 - Beta',\n            'License :: OSI Approved :: Apache Software License',\n            'Operating System :: OS Independent',\n            'Programming Language :: Python :: 3',\n            'Programming Language :: Python :: 3.7',\n            'Programming Language :: Python :: 3.8',\n        ],\n        license='Apache License Version 2.0',\n        setup_requires=['cython', 'numpy'],\n        install_requires=get_requirements(),\n        zip_safe=False)\n", "inference_gfpgan.py": "import argparse\nimport cv2\nimport glob\nimport numpy as np\nimport os\nimport torch\nfrom basicsr.utils import imwrite\n\nfrom gfpgan import GFPGANer\n\n\ndef main():\n    \"\"\"Inference demo for GFPGAN (for users).\n    \"\"\"\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\n        '-i',\n        '--input',\n        type=str,\n        default='inputs/whole_imgs',\n        help='Input image or folder. Default: inputs/whole_imgs')\n    parser.add_argument('-o', '--output', type=str, default='results', help='Output folder. Default: results')\n    # we use version to select models, which is more user-friendly\n    parser.add_argument(\n        '-v', '--version', type=str, default='1.3', help='GFPGAN model version. Option: 1 | 1.2 | 1.3. Default: 1.3')\n    parser.add_argument(\n        '-s', '--upscale', type=int, default=2, help='The final upsampling scale of the image. Default: 2')\n\n    parser.add_argument(\n        '--bg_upsampler', type=str, default='realesrgan', help='background upsampler. Default: realesrgan')\n    parser.add_argument(\n        '--bg_tile',\n        type=int,\n        default=400,\n        help='Tile size for background sampler, 0 for no tile during testing. Default: 400')\n    parser.add_argument('--suffix', type=str, default=None, help='Suffix of the restored faces')\n    parser.add_argument('--only_center_face', action='store_true', help='Only restore the center face')\n    parser.add_argument('--aligned', action='store_true', help='Input are aligned faces')\n    parser.add_argument(\n        '--ext',\n        type=str,\n        default='auto',\n        help='Image extension. Options: auto | jpg | png, auto means using the same extension as inputs. Default: auto')\n    parser.add_argument('-w', '--weight', type=float, default=0.5, help='Adjustable weights.')\n    args = parser.parse_args()\n\n    args = parser.parse_args()\n\n    # ------------------------ input & output ------------------------\n    if args.input.endswith('/'):\n        args.input = args.input[:-1]\n    if os.path.isfile(args.input):\n        img_list = [args.input]\n    else:\n        img_list = sorted(glob.glob(os.path.join(args.input, '*')))\n\n    os.makedirs(args.output, exist_ok=True)\n\n    # ------------------------ set up background upsampler ------------------------\n    if args.bg_upsampler == 'realesrgan':\n        if not torch.cuda.is_available():  # CPU\n            import warnings\n            warnings.warn('The unoptimized RealESRGAN is slow on CPU. We do not use it. '\n                          'If you really want to use it, please modify the corresponding codes.')\n            bg_upsampler = None\n        else:\n            from basicsr.archs.rrdbnet_arch import RRDBNet\n            from realesrgan import RealESRGANer\n            model = RRDBNet(num_in_ch=3, num_out_ch=3, num_feat=64, num_block=23, num_grow_ch=32, scale=2)\n            bg_upsampler = RealESRGANer(\n                scale=2,\n                model_path='https://github.com/xinntao/Real-ESRGAN/releases/download/v0.2.1/RealESRGAN_x2plus.pth',\n                model=model,\n                tile=args.bg_tile,\n                tile_pad=10,\n                pre_pad=0,\n                half=True)  # need to set False in CPU mode\n    else:\n        bg_upsampler = None\n\n    # ------------------------ set up GFPGAN restorer ------------------------\n    if args.version == '1':\n        arch = 'original'\n        channel_multiplier = 1\n        model_name = 'GFPGANv1'\n        url = 'https://github.com/TencentARC/GFPGAN/releases/download/v0.1.0/GFPGANv1.pth'\n    elif args.version == '1.2':\n        arch = 'clean'\n        channel_multiplier = 2\n        model_name = 'GFPGANCleanv1-NoCE-C2'\n        url = 'https://github.com/TencentARC/GFPGAN/releases/download/v0.2.0/GFPGANCleanv1-NoCE-C2.pth'\n    elif args.version == '1.3':\n        arch = 'clean'\n        channel_multiplier = 2\n        model_name = 'GFPGANv1.3'\n        url = 'https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.3.pth'\n    elif args.version == '1.4':\n        arch = 'clean'\n        channel_multiplier = 2\n        model_name = 'GFPGANv1.4'\n        url = 'https://github.com/TencentARC/GFPGAN/releases/download/v1.3.0/GFPGANv1.4.pth'\n    elif args.version == 'RestoreFormer':\n        arch = 'RestoreFormer'\n        channel_multiplier = 2\n        model_name = 'RestoreFormer'\n        url = 'https://github.com/TencentARC/GFPGAN/releases/download/v1.3.4/RestoreFormer.pth'\n    else:\n        raise ValueError(f'Wrong model version {args.version}.')\n\n    # determine model paths\n    model_path = os.path.join('experiments/pretrained_models', model_name + '.pth')\n    if not os.path.isfile(model_path):\n        model_path = os.path.join('gfpgan/weights', model_name + '.pth')\n    if not os.path.isfile(model_path):\n        # download pre-trained models from url\n        model_path = url\n\n    restorer = GFPGANer(\n        model_path=model_path,\n        upscale=args.upscale,\n        arch=arch,\n        channel_multiplier=channel_multiplier,\n        bg_upsampler=bg_upsampler)\n\n    # ------------------------ restore ------------------------\n    for img_path in img_list:\n        # read image\n        img_name = os.path.basename(img_path)\n        print(f'Processing {img_name} ...')\n        basename, ext = os.path.splitext(img_name)\n        input_img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n\n        # restore faces and background if necessary\n        cropped_faces, restored_faces, restored_img = restorer.enhance(\n            input_img,\n            has_aligned=args.aligned,\n            only_center_face=args.only_center_face,\n            paste_back=True,\n            weight=args.weight)\n\n        # save faces\n        for idx, (cropped_face, restored_face) in enumerate(zip(cropped_faces, restored_faces)):\n            # save cropped face\n            save_crop_path = os.path.join(args.output, 'cropped_faces', f'{basename}_{idx:02d}.png')\n            imwrite(cropped_face, save_crop_path)\n            # save restored face\n            if args.suffix is not None:\n                save_face_name = f'{basename}_{idx:02d}_{args.suffix}.png'\n            else:\n                save_face_name = f'{basename}_{idx:02d}.png'\n            save_restore_path = os.path.join(args.output, 'restored_faces', save_face_name)\n            imwrite(restored_face, save_restore_path)\n            # save comparison image\n            cmp_img = np.concatenate((cropped_face, restored_face), axis=1)\n            imwrite(cmp_img, os.path.join(args.output, 'cmp', f'{basename}_{idx:02d}.png'))\n\n        # save restored img\n        if restored_img is not None:\n            if args.ext == 'auto':\n                extension = ext[1:]\n            else:\n                extension = args.ext\n\n            if args.suffix is not None:\n                save_restore_path = os.path.join(args.output, 'restored_imgs', f'{basename}_{args.suffix}.{extension}')\n            else:\n                save_restore_path = os.path.join(args.output, 'restored_imgs', f'{basename}.{extension}')\n            imwrite(restored_img, save_restore_path)\n\n    print(f'Results are in the [{args.output}] folder.')\n\n\nif __name__ == '__main__':\n    main()\n", "gfpgan/train.py": "# flake8: noqa\nimport os.path as osp\nfrom basicsr.train import train_pipeline\n\nimport gfpgan.archs\nimport gfpgan.data\nimport gfpgan.models\n\nif __name__ == '__main__':\n    root_path = osp.abspath(osp.join(__file__, osp.pardir, osp.pardir))\n    train_pipeline(root_path)\n", "gfpgan/utils.py": "import cv2\nimport os\nimport torch\nfrom basicsr.utils import img2tensor, tensor2img\nfrom basicsr.utils.download_util import load_file_from_url\nfrom facexlib.utils.face_restoration_helper import FaceRestoreHelper\nfrom torchvision.transforms.functional import normalize\n\nfrom gfpgan.archs.gfpgan_bilinear_arch import GFPGANBilinear\nfrom gfpgan.archs.gfpganv1_arch import GFPGANv1\nfrom gfpgan.archs.gfpganv1_clean_arch import GFPGANv1Clean\n\nROOT_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\n\n\nclass GFPGANer():\n    \"\"\"Helper for restoration with GFPGAN.\n\n    It will detect and crop faces, and then resize the faces to 512x512.\n    GFPGAN is used to restored the resized faces.\n    The background is upsampled with the bg_upsampler.\n    Finally, the faces will be pasted back to the upsample background image.\n\n    Args:\n        model_path (str): The path to the GFPGAN model. It can be urls (will first download it automatically).\n        upscale (float): The upscale of the final output. Default: 2.\n        arch (str): The GFPGAN architecture. Option: clean | original. Default: clean.\n        channel_multiplier (int): Channel multiplier for large networks of StyleGAN2. Default: 2.\n        bg_upsampler (nn.Module): The upsampler for the background. Default: None.\n    \"\"\"\n\n    def __init__(self, model_path, upscale=2, arch='clean', channel_multiplier=2, bg_upsampler=None, device=None):\n        self.upscale = upscale\n        self.bg_upsampler = bg_upsampler\n\n        # initialize model\n        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') if device is None else device\n        # initialize the GFP-GAN\n        if arch == 'clean':\n            self.gfpgan = GFPGANv1Clean(\n                out_size=512,\n                num_style_feat=512,\n                channel_multiplier=channel_multiplier,\n                decoder_load_path=None,\n                fix_decoder=False,\n                num_mlp=8,\n                input_is_latent=True,\n                different_w=True,\n                narrow=1,\n                sft_half=True)\n        elif arch == 'bilinear':\n            self.gfpgan = GFPGANBilinear(\n                out_size=512,\n                num_style_feat=512,\n                channel_multiplier=channel_multiplier,\n                decoder_load_path=None,\n                fix_decoder=False,\n                num_mlp=8,\n                input_is_latent=True,\n                different_w=True,\n                narrow=1,\n                sft_half=True)\n        elif arch == 'original':\n            self.gfpgan = GFPGANv1(\n                out_size=512,\n                num_style_feat=512,\n                channel_multiplier=channel_multiplier,\n                decoder_load_path=None,\n                fix_decoder=True,\n                num_mlp=8,\n                input_is_latent=True,\n                different_w=True,\n                narrow=1,\n                sft_half=True)\n        elif arch == 'RestoreFormer':\n            from gfpgan.archs.restoreformer_arch import RestoreFormer\n            self.gfpgan = RestoreFormer()\n        # initialize face helper\n        self.face_helper = FaceRestoreHelper(\n            upscale,\n            face_size=512,\n            crop_ratio=(1, 1),\n            det_model='retinaface_resnet50',\n            save_ext='png',\n            use_parse=True,\n            device=self.device,\n            model_rootpath='gfpgan/weights')\n\n        if model_path.startswith('https://'):\n            model_path = load_file_from_url(\n                url=model_path, model_dir=os.path.join(ROOT_DIR, 'gfpgan/weights'), progress=True, file_name=None)\n        loadnet = torch.load(model_path)\n        if 'params_ema' in loadnet:\n            keyname = 'params_ema'\n        else:\n            keyname = 'params'\n        self.gfpgan.load_state_dict(loadnet[keyname], strict=True)\n        self.gfpgan.eval()\n        self.gfpgan = self.gfpgan.to(self.device)\n\n    @torch.no_grad()\n    def enhance(self, img, has_aligned=False, only_center_face=False, paste_back=True, weight=0.5):\n        self.face_helper.clean_all()\n\n        if has_aligned:  # the inputs are already aligned\n            img = cv2.resize(img, (512, 512))\n            self.face_helper.cropped_faces = [img]\n        else:\n            self.face_helper.read_image(img)\n            # get face landmarks for each face\n            self.face_helper.get_face_landmarks_5(only_center_face=only_center_face, eye_dist_threshold=5)\n            # eye_dist_threshold=5: skip faces whose eye distance is smaller than 5 pixels\n            # TODO: even with eye_dist_threshold, it will still introduce wrong detections and restorations.\n            # align and warp each face\n            self.face_helper.align_warp_face()\n\n        # face restoration\n        for cropped_face in self.face_helper.cropped_faces:\n            # prepare data\n            cropped_face_t = img2tensor(cropped_face / 255., bgr2rgb=True, float32=True)\n            normalize(cropped_face_t, (0.5, 0.5, 0.5), (0.5, 0.5, 0.5), inplace=True)\n            cropped_face_t = cropped_face_t.unsqueeze(0).to(self.device)\n\n            try:\n                output = self.gfpgan(cropped_face_t, return_rgb=False, weight=weight)[0]\n                # convert to image\n                restored_face = tensor2img(output.squeeze(0), rgb2bgr=True, min_max=(-1, 1))\n            except RuntimeError as error:\n                print(f'\\tFailed inference for GFPGAN: {error}.')\n                restored_face = cropped_face\n\n            restored_face = restored_face.astype('uint8')\n            self.face_helper.add_restored_face(restored_face)\n\n        if not has_aligned and paste_back:\n            # upsample the background\n            if self.bg_upsampler is not None:\n                # Now only support RealESRGAN for upsampling background\n                bg_img = self.bg_upsampler.enhance(img, outscale=self.upscale)[0]\n            else:\n                bg_img = None\n\n            self.face_helper.get_inverse_affine(None)\n            # paste each restored face to the input image\n            restored_img = self.face_helper.paste_faces_to_input_image(upsample_img=bg_img)\n            return self.face_helper.cropped_faces, self.face_helper.restored_faces, restored_img\n        else:\n            return self.face_helper.cropped_faces, self.face_helper.restored_faces, None\n", "gfpgan/__init__.py": "# flake8: noqa\nfrom .archs import *\nfrom .data import *\nfrom .models import *\nfrom .utils import *\n\n# from .version import *\n", "gfpgan/models/gfpgan_model.py": "import math\nimport os.path as osp\nimport torch\nfrom basicsr.archs import build_network\nfrom basicsr.losses import build_loss\nfrom basicsr.losses.gan_loss import r1_penalty\nfrom basicsr.metrics import calculate_metric\nfrom basicsr.models.base_model import BaseModel\nfrom basicsr.utils import get_root_logger, imwrite, tensor2img\nfrom basicsr.utils.registry import MODEL_REGISTRY\nfrom collections import OrderedDict\nfrom torch.nn import functional as F\nfrom torchvision.ops import roi_align\nfrom tqdm import tqdm\n\n\n@MODEL_REGISTRY.register()\nclass GFPGANModel(BaseModel):\n    \"\"\"The GFPGAN model for Towards real-world blind face restoratin with generative facial prior\"\"\"\n\n    def __init__(self, opt):\n        super(GFPGANModel, self).__init__(opt)\n        self.idx = 0  # it is used for saving data for check\n\n        # define network\n        self.net_g = build_network(opt['network_g'])\n        self.net_g = self.model_to_device(self.net_g)\n        self.print_network(self.net_g)\n\n        # load pretrained model\n        load_path = self.opt['path'].get('pretrain_network_g', None)\n        if load_path is not None:\n            param_key = self.opt['path'].get('param_key_g', 'params')\n            self.load_network(self.net_g, load_path, self.opt['path'].get('strict_load_g', True), param_key)\n\n        self.log_size = int(math.log(self.opt['network_g']['out_size'], 2))\n\n        if self.is_train:\n            self.init_training_settings()\n\n    def init_training_settings(self):\n        train_opt = self.opt['train']\n\n        # ----------- define net_d ----------- #\n        self.net_d = build_network(self.opt['network_d'])\n        self.net_d = self.model_to_device(self.net_d)\n        self.print_network(self.net_d)\n        # load pretrained model\n        load_path = self.opt['path'].get('pretrain_network_d', None)\n        if load_path is not None:\n            self.load_network(self.net_d, load_path, self.opt['path'].get('strict_load_d', True))\n\n        # ----------- define net_g with Exponential Moving Average (EMA) ----------- #\n        # net_g_ema only used for testing on one GPU and saving. There is no need to wrap with DistributedDataParallel\n        self.net_g_ema = build_network(self.opt['network_g']).to(self.device)\n        # load pretrained model\n        load_path = self.opt['path'].get('pretrain_network_g', None)\n        if load_path is not None:\n            self.load_network(self.net_g_ema, load_path, self.opt['path'].get('strict_load_g', True), 'params_ema')\n        else:\n            self.model_ema(0)  # copy net_g weight\n\n        self.net_g.train()\n        self.net_d.train()\n        self.net_g_ema.eval()\n\n        # ----------- facial component networks ----------- #\n        if ('network_d_left_eye' in self.opt and 'network_d_right_eye' in self.opt and 'network_d_mouth' in self.opt):\n            self.use_facial_disc = True\n        else:\n            self.use_facial_disc = False\n\n        if self.use_facial_disc:\n            # left eye\n            self.net_d_left_eye = build_network(self.opt['network_d_left_eye'])\n            self.net_d_left_eye = self.model_to_device(self.net_d_left_eye)\n            self.print_network(self.net_d_left_eye)\n            load_path = self.opt['path'].get('pretrain_network_d_left_eye')\n            if load_path is not None:\n                self.load_network(self.net_d_left_eye, load_path, True, 'params')\n            # right eye\n            self.net_d_right_eye = build_network(self.opt['network_d_right_eye'])\n            self.net_d_right_eye = self.model_to_device(self.net_d_right_eye)\n            self.print_network(self.net_d_right_eye)\n            load_path = self.opt['path'].get('pretrain_network_d_right_eye')\n            if load_path is not None:\n                self.load_network(self.net_d_right_eye, load_path, True, 'params')\n            # mouth\n            self.net_d_mouth = build_network(self.opt['network_d_mouth'])\n            self.net_d_mouth = self.model_to_device(self.net_d_mouth)\n            self.print_network(self.net_d_mouth)\n            load_path = self.opt['path'].get('pretrain_network_d_mouth')\n            if load_path is not None:\n                self.load_network(self.net_d_mouth, load_path, True, 'params')\n\n            self.net_d_left_eye.train()\n            self.net_d_right_eye.train()\n            self.net_d_mouth.train()\n\n            # ----------- define facial component gan loss ----------- #\n            self.cri_component = build_loss(train_opt['gan_component_opt']).to(self.device)\n\n        # ----------- define losses ----------- #\n        # pixel loss\n        if train_opt.get('pixel_opt'):\n            self.cri_pix = build_loss(train_opt['pixel_opt']).to(self.device)\n        else:\n            self.cri_pix = None\n\n        # perceptual loss\n        if train_opt.get('perceptual_opt'):\n            self.cri_perceptual = build_loss(train_opt['perceptual_opt']).to(self.device)\n        else:\n            self.cri_perceptual = None\n\n        # L1 loss is used in pyramid loss, component style loss and identity loss\n        self.cri_l1 = build_loss(train_opt['L1_opt']).to(self.device)\n\n        # gan loss (wgan)\n        self.cri_gan = build_loss(train_opt['gan_opt']).to(self.device)\n\n        # ----------- define identity loss ----------- #\n        if 'network_identity' in self.opt:\n            self.use_identity = True\n        else:\n            self.use_identity = False\n\n        if self.use_identity:\n            # define identity network\n            self.network_identity = build_network(self.opt['network_identity'])\n            self.network_identity = self.model_to_device(self.network_identity)\n            self.print_network(self.network_identity)\n            load_path = self.opt['path'].get('pretrain_network_identity')\n            if load_path is not None:\n                self.load_network(self.network_identity, load_path, True, None)\n            self.network_identity.eval()\n            for param in self.network_identity.parameters():\n                param.requires_grad = False\n\n        # regularization weights\n        self.r1_reg_weight = train_opt['r1_reg_weight']  # for discriminator\n        self.net_d_iters = train_opt.get('net_d_iters', 1)\n        self.net_d_init_iters = train_opt.get('net_d_init_iters', 0)\n        self.net_d_reg_every = train_opt['net_d_reg_every']\n\n        # set up optimizers and schedulers\n        self.setup_optimizers()\n        self.setup_schedulers()\n\n    def setup_optimizers(self):\n        train_opt = self.opt['train']\n\n        # ----------- optimizer g ----------- #\n        net_g_reg_ratio = 1\n        normal_params = []\n        for _, param in self.net_g.named_parameters():\n            normal_params.append(param)\n        optim_params_g = [{  # add normal params first\n            'params': normal_params,\n            'lr': train_opt['optim_g']['lr']\n        }]\n        optim_type = train_opt['optim_g'].pop('type')\n        lr = train_opt['optim_g']['lr'] * net_g_reg_ratio\n        betas = (0**net_g_reg_ratio, 0.99**net_g_reg_ratio)\n        self.optimizer_g = self.get_optimizer(optim_type, optim_params_g, lr, betas=betas)\n        self.optimizers.append(self.optimizer_g)\n\n        # ----------- optimizer d ----------- #\n        net_d_reg_ratio = self.net_d_reg_every / (self.net_d_reg_every + 1)\n        normal_params = []\n        for _, param in self.net_d.named_parameters():\n            normal_params.append(param)\n        optim_params_d = [{  # add normal params first\n            'params': normal_params,\n            'lr': train_opt['optim_d']['lr']\n        }]\n        optim_type = train_opt['optim_d'].pop('type')\n        lr = train_opt['optim_d']['lr'] * net_d_reg_ratio\n        betas = (0**net_d_reg_ratio, 0.99**net_d_reg_ratio)\n        self.optimizer_d = self.get_optimizer(optim_type, optim_params_d, lr, betas=betas)\n        self.optimizers.append(self.optimizer_d)\n\n        # ----------- optimizers for facial component networks ----------- #\n        if self.use_facial_disc:\n            # setup optimizers for facial component discriminators\n            optim_type = train_opt['optim_component'].pop('type')\n            lr = train_opt['optim_component']['lr']\n            # left eye\n            self.optimizer_d_left_eye = self.get_optimizer(\n                optim_type, self.net_d_left_eye.parameters(), lr, betas=(0.9, 0.99))\n            self.optimizers.append(self.optimizer_d_left_eye)\n            # right eye\n            self.optimizer_d_right_eye = self.get_optimizer(\n                optim_type, self.net_d_right_eye.parameters(), lr, betas=(0.9, 0.99))\n            self.optimizers.append(self.optimizer_d_right_eye)\n            # mouth\n            self.optimizer_d_mouth = self.get_optimizer(\n                optim_type, self.net_d_mouth.parameters(), lr, betas=(0.9, 0.99))\n            self.optimizers.append(self.optimizer_d_mouth)\n\n    def feed_data(self, data):\n        self.lq = data['lq'].to(self.device)\n        if 'gt' in data:\n            self.gt = data['gt'].to(self.device)\n\n        if 'loc_left_eye' in data:\n            # get facial component locations, shape (batch, 4)\n            self.loc_left_eyes = data['loc_left_eye']\n            self.loc_right_eyes = data['loc_right_eye']\n            self.loc_mouths = data['loc_mouth']\n\n        # uncomment to check data\n        # import torchvision\n        # if self.opt['rank'] == 0:\n        #     import os\n        #     os.makedirs('tmp/gt', exist_ok=True)\n        #     os.makedirs('tmp/lq', exist_ok=True)\n        #     print(self.idx)\n        #     torchvision.utils.save_image(\n        #         self.gt, f'tmp/gt/gt_{self.idx}.png', nrow=4, padding=2, normalize=True, range=(-1, 1))\n        #     torchvision.utils.save_image(\n        #         self.lq, f'tmp/lq/lq{self.idx}.png', nrow=4, padding=2, normalize=True, range=(-1, 1))\n        #     self.idx = self.idx + 1\n\n    def construct_img_pyramid(self):\n        \"\"\"Construct image pyramid for intermediate restoration loss\"\"\"\n        pyramid_gt = [self.gt]\n        down_img = self.gt\n        for _ in range(0, self.log_size - 3):\n            down_img = F.interpolate(down_img, scale_factor=0.5, mode='bilinear', align_corners=False)\n            pyramid_gt.insert(0, down_img)\n        return pyramid_gt\n\n    def get_roi_regions(self, eye_out_size=80, mouth_out_size=120):\n        face_ratio = int(self.opt['network_g']['out_size'] / 512)\n        eye_out_size *= face_ratio\n        mouth_out_size *= face_ratio\n\n        rois_eyes = []\n        rois_mouths = []\n        for b in range(self.loc_left_eyes.size(0)):  # loop for batch size\n            # left eye and right eye\n            img_inds = self.loc_left_eyes.new_full((2, 1), b)\n            bbox = torch.stack([self.loc_left_eyes[b, :], self.loc_right_eyes[b, :]], dim=0)  # shape: (2, 4)\n            rois = torch.cat([img_inds, bbox], dim=-1)  # shape: (2, 5)\n            rois_eyes.append(rois)\n            # mouse\n            img_inds = self.loc_left_eyes.new_full((1, 1), b)\n            rois = torch.cat([img_inds, self.loc_mouths[b:b + 1, :]], dim=-1)  # shape: (1, 5)\n            rois_mouths.append(rois)\n\n        rois_eyes = torch.cat(rois_eyes, 0).to(self.device)\n        rois_mouths = torch.cat(rois_mouths, 0).to(self.device)\n\n        # real images\n        all_eyes = roi_align(self.gt, boxes=rois_eyes, output_size=eye_out_size) * face_ratio\n        self.left_eyes_gt = all_eyes[0::2, :, :, :]\n        self.right_eyes_gt = all_eyes[1::2, :, :, :]\n        self.mouths_gt = roi_align(self.gt, boxes=rois_mouths, output_size=mouth_out_size) * face_ratio\n        # output\n        all_eyes = roi_align(self.output, boxes=rois_eyes, output_size=eye_out_size) * face_ratio\n        self.left_eyes = all_eyes[0::2, :, :, :]\n        self.right_eyes = all_eyes[1::2, :, :, :]\n        self.mouths = roi_align(self.output, boxes=rois_mouths, output_size=mouth_out_size) * face_ratio\n\n    def _gram_mat(self, x):\n        \"\"\"Calculate Gram matrix.\n\n        Args:\n            x (torch.Tensor): Tensor with shape of (n, c, h, w).\n\n        Returns:\n            torch.Tensor: Gram matrix.\n        \"\"\"\n        n, c, h, w = x.size()\n        features = x.view(n, c, w * h)\n        features_t = features.transpose(1, 2)\n        gram = features.bmm(features_t) / (c * h * w)\n        return gram\n\n    def gray_resize_for_identity(self, out, size=128):\n        out_gray = (0.2989 * out[:, 0, :, :] + 0.5870 * out[:, 1, :, :] + 0.1140 * out[:, 2, :, :])\n        out_gray = out_gray.unsqueeze(1)\n        out_gray = F.interpolate(out_gray, (size, size), mode='bilinear', align_corners=False)\n        return out_gray\n\n    def optimize_parameters(self, current_iter):\n        # optimize net_g\n        for p in self.net_d.parameters():\n            p.requires_grad = False\n        self.optimizer_g.zero_grad()\n\n        # do not update facial component net_d\n        if self.use_facial_disc:\n            for p in self.net_d_left_eye.parameters():\n                p.requires_grad = False\n            for p in self.net_d_right_eye.parameters():\n                p.requires_grad = False\n            for p in self.net_d_mouth.parameters():\n                p.requires_grad = False\n\n        # image pyramid loss weight\n        pyramid_loss_weight = self.opt['train'].get('pyramid_loss_weight', 0)\n        if pyramid_loss_weight > 0 and current_iter > self.opt['train'].get('remove_pyramid_loss', float('inf')):\n            pyramid_loss_weight = 1e-12  # very small weight to avoid unused param error\n        if pyramid_loss_weight > 0:\n            self.output, out_rgbs = self.net_g(self.lq, return_rgb=True)\n            pyramid_gt = self.construct_img_pyramid()\n        else:\n            self.output, out_rgbs = self.net_g(self.lq, return_rgb=False)\n\n        # get roi-align regions\n        if self.use_facial_disc:\n            self.get_roi_regions(eye_out_size=80, mouth_out_size=120)\n\n        l_g_total = 0\n        loss_dict = OrderedDict()\n        if (current_iter % self.net_d_iters == 0 and current_iter > self.net_d_init_iters):\n            # pixel loss\n            if self.cri_pix:\n                l_g_pix = self.cri_pix(self.output, self.gt)\n                l_g_total += l_g_pix\n                loss_dict['l_g_pix'] = l_g_pix\n\n            # image pyramid loss\n            if pyramid_loss_weight > 0:\n                for i in range(0, self.log_size - 2):\n                    l_pyramid = self.cri_l1(out_rgbs[i], pyramid_gt[i]) * pyramid_loss_weight\n                    l_g_total += l_pyramid\n                    loss_dict[f'l_p_{2**(i+3)}'] = l_pyramid\n\n            # perceptual loss\n            if self.cri_perceptual:\n                l_g_percep, l_g_style = self.cri_perceptual(self.output, self.gt)\n                if l_g_percep is not None:\n                    l_g_total += l_g_percep\n                    loss_dict['l_g_percep'] = l_g_percep\n                if l_g_style is not None:\n                    l_g_total += l_g_style\n                    loss_dict['l_g_style'] = l_g_style\n\n            # gan loss\n            fake_g_pred = self.net_d(self.output)\n            l_g_gan = self.cri_gan(fake_g_pred, True, is_disc=False)\n            l_g_total += l_g_gan\n            loss_dict['l_g_gan'] = l_g_gan\n\n            # facial component loss\n            if self.use_facial_disc:\n                # left eye\n                fake_left_eye, fake_left_eye_feats = self.net_d_left_eye(self.left_eyes, return_feats=True)\n                l_g_gan = self.cri_component(fake_left_eye, True, is_disc=False)\n                l_g_total += l_g_gan\n                loss_dict['l_g_gan_left_eye'] = l_g_gan\n                # right eye\n                fake_right_eye, fake_right_eye_feats = self.net_d_right_eye(self.right_eyes, return_feats=True)\n                l_g_gan = self.cri_component(fake_right_eye, True, is_disc=False)\n                l_g_total += l_g_gan\n                loss_dict['l_g_gan_right_eye'] = l_g_gan\n                # mouth\n                fake_mouth, fake_mouth_feats = self.net_d_mouth(self.mouths, return_feats=True)\n                l_g_gan = self.cri_component(fake_mouth, True, is_disc=False)\n                l_g_total += l_g_gan\n                loss_dict['l_g_gan_mouth'] = l_g_gan\n\n                if self.opt['train'].get('comp_style_weight', 0) > 0:\n                    # get gt feat\n                    _, real_left_eye_feats = self.net_d_left_eye(self.left_eyes_gt, return_feats=True)\n                    _, real_right_eye_feats = self.net_d_right_eye(self.right_eyes_gt, return_feats=True)\n                    _, real_mouth_feats = self.net_d_mouth(self.mouths_gt, return_feats=True)\n\n                    def _comp_style(feat, feat_gt, criterion):\n                        return criterion(self._gram_mat(feat[0]), self._gram_mat(\n                            feat_gt[0].detach())) * 0.5 + criterion(\n                                self._gram_mat(feat[1]), self._gram_mat(feat_gt[1].detach()))\n\n                    # facial component style loss\n                    comp_style_loss = 0\n                    comp_style_loss += _comp_style(fake_left_eye_feats, real_left_eye_feats, self.cri_l1)\n                    comp_style_loss += _comp_style(fake_right_eye_feats, real_right_eye_feats, self.cri_l1)\n                    comp_style_loss += _comp_style(fake_mouth_feats, real_mouth_feats, self.cri_l1)\n                    comp_style_loss = comp_style_loss * self.opt['train']['comp_style_weight']\n                    l_g_total += comp_style_loss\n                    loss_dict['l_g_comp_style_loss'] = comp_style_loss\n\n            # identity loss\n            if self.use_identity:\n                identity_weight = self.opt['train']['identity_weight']\n                # get gray images and resize\n                out_gray = self.gray_resize_for_identity(self.output)\n                gt_gray = self.gray_resize_for_identity(self.gt)\n\n                identity_gt = self.network_identity(gt_gray).detach()\n                identity_out = self.network_identity(out_gray)\n                l_identity = self.cri_l1(identity_out, identity_gt) * identity_weight\n                l_g_total += l_identity\n                loss_dict['l_identity'] = l_identity\n\n            l_g_total.backward()\n            self.optimizer_g.step()\n\n        # EMA\n        self.model_ema(decay=0.5**(32 / (10 * 1000)))\n\n        # ----------- optimize net_d ----------- #\n        for p in self.net_d.parameters():\n            p.requires_grad = True\n        self.optimizer_d.zero_grad()\n        if self.use_facial_disc:\n            for p in self.net_d_left_eye.parameters():\n                p.requires_grad = True\n            for p in self.net_d_right_eye.parameters():\n                p.requires_grad = True\n            for p in self.net_d_mouth.parameters():\n                p.requires_grad = True\n            self.optimizer_d_left_eye.zero_grad()\n            self.optimizer_d_right_eye.zero_grad()\n            self.optimizer_d_mouth.zero_grad()\n\n        fake_d_pred = self.net_d(self.output.detach())\n        real_d_pred = self.net_d(self.gt)\n        l_d = self.cri_gan(real_d_pred, True, is_disc=True) + self.cri_gan(fake_d_pred, False, is_disc=True)\n        loss_dict['l_d'] = l_d\n        # In WGAN, real_score should be positive and fake_score should be negative\n        loss_dict['real_score'] = real_d_pred.detach().mean()\n        loss_dict['fake_score'] = fake_d_pred.detach().mean()\n        l_d.backward()\n\n        # regularization loss\n        if current_iter % self.net_d_reg_every == 0:\n            self.gt.requires_grad = True\n            real_pred = self.net_d(self.gt)\n            l_d_r1 = r1_penalty(real_pred, self.gt)\n            l_d_r1 = (self.r1_reg_weight / 2 * l_d_r1 * self.net_d_reg_every + 0 * real_pred[0])\n            loss_dict['l_d_r1'] = l_d_r1.detach().mean()\n            l_d_r1.backward()\n\n        self.optimizer_d.step()\n\n        # optimize facial component discriminators\n        if self.use_facial_disc:\n            # left eye\n            fake_d_pred, _ = self.net_d_left_eye(self.left_eyes.detach())\n            real_d_pred, _ = self.net_d_left_eye(self.left_eyes_gt)\n            l_d_left_eye = self.cri_component(\n                real_d_pred, True, is_disc=True) + self.cri_gan(\n                    fake_d_pred, False, is_disc=True)\n            loss_dict['l_d_left_eye'] = l_d_left_eye\n            l_d_left_eye.backward()\n            # right eye\n            fake_d_pred, _ = self.net_d_right_eye(self.right_eyes.detach())\n            real_d_pred, _ = self.net_d_right_eye(self.right_eyes_gt)\n            l_d_right_eye = self.cri_component(\n                real_d_pred, True, is_disc=True) + self.cri_gan(\n                    fake_d_pred, False, is_disc=True)\n            loss_dict['l_d_right_eye'] = l_d_right_eye\n            l_d_right_eye.backward()\n            # mouth\n            fake_d_pred, _ = self.net_d_mouth(self.mouths.detach())\n            real_d_pred, _ = self.net_d_mouth(self.mouths_gt)\n            l_d_mouth = self.cri_component(\n                real_d_pred, True, is_disc=True) + self.cri_gan(\n                    fake_d_pred, False, is_disc=True)\n            loss_dict['l_d_mouth'] = l_d_mouth\n            l_d_mouth.backward()\n\n            self.optimizer_d_left_eye.step()\n            self.optimizer_d_right_eye.step()\n            self.optimizer_d_mouth.step()\n\n        self.log_dict = self.reduce_loss_dict(loss_dict)\n\n    def test(self):\n        with torch.no_grad():\n            if hasattr(self, 'net_g_ema'):\n                self.net_g_ema.eval()\n                self.output, _ = self.net_g_ema(self.lq)\n            else:\n                logger = get_root_logger()\n                logger.warning('Do not have self.net_g_ema, use self.net_g.')\n                self.net_g.eval()\n                self.output, _ = self.net_g(self.lq)\n                self.net_g.train()\n\n    def dist_validation(self, dataloader, current_iter, tb_logger, save_img):\n        if self.opt['rank'] == 0:\n            self.nondist_validation(dataloader, current_iter, tb_logger, save_img)\n\n    def nondist_validation(self, dataloader, current_iter, tb_logger, save_img):\n        dataset_name = dataloader.dataset.opt['name']\n        with_metrics = self.opt['val'].get('metrics') is not None\n        use_pbar = self.opt['val'].get('pbar', False)\n\n        if with_metrics:\n            if not hasattr(self, 'metric_results'):  # only execute in the first run\n                self.metric_results = {metric: 0 for metric in self.opt['val']['metrics'].keys()}\n            # initialize the best metric results for each dataset_name (supporting multiple validation datasets)\n            self._initialize_best_metric_results(dataset_name)\n            # zero self.metric_results\n            self.metric_results = {metric: 0 for metric in self.metric_results}\n\n        metric_data = dict()\n        if use_pbar:\n            pbar = tqdm(total=len(dataloader), unit='image')\n\n        for idx, val_data in enumerate(dataloader):\n            img_name = osp.splitext(osp.basename(val_data['lq_path'][0]))[0]\n            self.feed_data(val_data)\n            self.test()\n\n            sr_img = tensor2img(self.output.detach().cpu(), min_max=(-1, 1))\n            metric_data['img'] = sr_img\n            if hasattr(self, 'gt'):\n                gt_img = tensor2img(self.gt.detach().cpu(), min_max=(-1, 1))\n                metric_data['img2'] = gt_img\n                del self.gt\n\n            # tentative for out of GPU memory\n            del self.lq\n            del self.output\n            torch.cuda.empty_cache()\n\n            if save_img:\n                if self.opt['is_train']:\n                    save_img_path = osp.join(self.opt['path']['visualization'], img_name,\n                                             f'{img_name}_{current_iter}.png')\n                else:\n                    if self.opt['val']['suffix']:\n                        save_img_path = osp.join(self.opt['path']['visualization'], dataset_name,\n                                                 f'{img_name}_{self.opt[\"val\"][\"suffix\"]}.png')\n                    else:\n                        save_img_path = osp.join(self.opt['path']['visualization'], dataset_name,\n                                                 f'{img_name}_{self.opt[\"name\"]}.png')\n                imwrite(sr_img, save_img_path)\n\n            if with_metrics:\n                # calculate metrics\n                for name, opt_ in self.opt['val']['metrics'].items():\n                    self.metric_results[name] += calculate_metric(metric_data, opt_)\n            if use_pbar:\n                pbar.update(1)\n                pbar.set_description(f'Test {img_name}')\n        if use_pbar:\n            pbar.close()\n\n        if with_metrics:\n            for metric in self.metric_results.keys():\n                self.metric_results[metric] /= (idx + 1)\n                # update the best metric result\n                self._update_best_metric_result(dataset_name, metric, self.metric_results[metric], current_iter)\n\n            self._log_validation_metric_values(current_iter, dataset_name, tb_logger)\n\n    def _log_validation_metric_values(self, current_iter, dataset_name, tb_logger):\n        log_str = f'Validation {dataset_name}\\n'\n        for metric, value in self.metric_results.items():\n            log_str += f'\\t # {metric}: {value:.4f}'\n            if hasattr(self, 'best_metric_results'):\n                log_str += (f'\\tBest: {self.best_metric_results[dataset_name][metric][\"val\"]:.4f} @ '\n                            f'{self.best_metric_results[dataset_name][metric][\"iter\"]} iter')\n            log_str += '\\n'\n\n        logger = get_root_logger()\n        logger.info(log_str)\n        if tb_logger:\n            for metric, value in self.metric_results.items():\n                tb_logger.add_scalar(f'metrics/{dataset_name}/{metric}', value, current_iter)\n\n    def save(self, epoch, current_iter):\n        # save net_g and net_d\n        self.save_network([self.net_g, self.net_g_ema], 'net_g', current_iter, param_key=['params', 'params_ema'])\n        self.save_network(self.net_d, 'net_d', current_iter)\n        # save component discriminators\n        if self.use_facial_disc:\n            self.save_network(self.net_d_left_eye, 'net_d_left_eye', current_iter)\n            self.save_network(self.net_d_right_eye, 'net_d_right_eye', current_iter)\n            self.save_network(self.net_d_mouth, 'net_d_mouth', current_iter)\n        # save training state\n        self.save_training_state(epoch, current_iter)\n", "gfpgan/models/__init__.py": "import importlib\nfrom basicsr.utils import scandir\nfrom os import path as osp\n\n# automatically scan and import model modules for registry\n# scan all the files that end with '_model.py' under the model folder\nmodel_folder = osp.dirname(osp.abspath(__file__))\nmodel_filenames = [osp.splitext(osp.basename(v))[0] for v in scandir(model_folder) if v.endswith('_model.py')]\n# import all the model modules\n_model_modules = [importlib.import_module(f'gfpgan.models.{file_name}') for file_name in model_filenames]\n", "gfpgan/archs/gfpganv1_clean_arch.py": "import math\nimport random\nimport torch\nfrom basicsr.utils.registry import ARCH_REGISTRY\nfrom torch import nn\nfrom torch.nn import functional as F\n\nfrom .stylegan2_clean_arch import StyleGAN2GeneratorClean\n\n\nclass StyleGAN2GeneratorCSFT(StyleGAN2GeneratorClean):\n    \"\"\"StyleGAN2 Generator with SFT modulation (Spatial Feature Transform).\n\n    It is the clean version without custom compiled CUDA extensions used in StyleGAN2.\n\n    Args:\n        out_size (int): The spatial size of outputs.\n        num_style_feat (int): Channel number of style features. Default: 512.\n        num_mlp (int): Layer number of MLP style layers. Default: 8.\n        channel_multiplier (int): Channel multiplier for large networks of StyleGAN2. Default: 2.\n        narrow (float): The narrow ratio for channels. Default: 1.\n        sft_half (bool): Whether to apply SFT on half of the input channels. Default: False.\n    \"\"\"\n\n    def __init__(self, out_size, num_style_feat=512, num_mlp=8, channel_multiplier=2, narrow=1, sft_half=False):\n        super(StyleGAN2GeneratorCSFT, self).__init__(\n            out_size,\n            num_style_feat=num_style_feat,\n            num_mlp=num_mlp,\n            channel_multiplier=channel_multiplier,\n            narrow=narrow)\n        self.sft_half = sft_half\n\n    def forward(self,\n                styles,\n                conditions,\n                input_is_latent=False,\n                noise=None,\n                randomize_noise=True,\n                truncation=1,\n                truncation_latent=None,\n                inject_index=None,\n                return_latents=False):\n        \"\"\"Forward function for StyleGAN2GeneratorCSFT.\n\n        Args:\n            styles (list[Tensor]): Sample codes of styles.\n            conditions (list[Tensor]): SFT conditions to generators.\n            input_is_latent (bool): Whether input is latent style. Default: False.\n            noise (Tensor | None): Input noise or None. Default: None.\n            randomize_noise (bool): Randomize noise, used when 'noise' is False. Default: True.\n            truncation (float): The truncation ratio. Default: 1.\n            truncation_latent (Tensor | None): The truncation latent tensor. Default: None.\n            inject_index (int | None): The injection index for mixing noise. Default: None.\n            return_latents (bool): Whether to return style latents. Default: False.\n        \"\"\"\n        # style codes -> latents with Style MLP layer\n        if not input_is_latent:\n            styles = [self.style_mlp(s) for s in styles]\n        # noises\n        if noise is None:\n            if randomize_noise:\n                noise = [None] * self.num_layers  # for each style conv layer\n            else:  # use the stored noise\n                noise = [getattr(self.noises, f'noise{i}') for i in range(self.num_layers)]\n        # style truncation\n        if truncation < 1:\n            style_truncation = []\n            for style in styles:\n                style_truncation.append(truncation_latent + truncation * (style - truncation_latent))\n            styles = style_truncation\n        # get style latents with injection\n        if len(styles) == 1:\n            inject_index = self.num_latent\n\n            if styles[0].ndim < 3:\n                # repeat latent code for all the layers\n                latent = styles[0].unsqueeze(1).repeat(1, inject_index, 1)\n            else:  # used for encoder with different latent code for each layer\n                latent = styles[0]\n        elif len(styles) == 2:  # mixing noises\n            if inject_index is None:\n                inject_index = random.randint(1, self.num_latent - 1)\n            latent1 = styles[0].unsqueeze(1).repeat(1, inject_index, 1)\n            latent2 = styles[1].unsqueeze(1).repeat(1, self.num_latent - inject_index, 1)\n            latent = torch.cat([latent1, latent2], 1)\n\n        # main generation\n        out = self.constant_input(latent.shape[0])\n        out = self.style_conv1(out, latent[:, 0], noise=noise[0])\n        skip = self.to_rgb1(out, latent[:, 1])\n\n        i = 1\n        for conv1, conv2, noise1, noise2, to_rgb in zip(self.style_convs[::2], self.style_convs[1::2], noise[1::2],\n                                                        noise[2::2], self.to_rgbs):\n            out = conv1(out, latent[:, i], noise=noise1)\n\n            # the conditions may have fewer levels\n            if i < len(conditions):\n                # SFT part to combine the conditions\n                if self.sft_half:  # only apply SFT to half of the channels\n                    out_same, out_sft = torch.split(out, int(out.size(1) // 2), dim=1)\n                    out_sft = out_sft * conditions[i - 1] + conditions[i]\n                    out = torch.cat([out_same, out_sft], dim=1)\n                else:  # apply SFT to all the channels\n                    out = out * conditions[i - 1] + conditions[i]\n\n            out = conv2(out, latent[:, i + 1], noise=noise2)\n            skip = to_rgb(out, latent[:, i + 2], skip)  # feature back to the rgb space\n            i += 2\n\n        image = skip\n\n        if return_latents:\n            return image, latent\n        else:\n            return image, None\n\n\nclass ResBlock(nn.Module):\n    \"\"\"Residual block with bilinear upsampling/downsampling.\n\n    Args:\n        in_channels (int): Channel number of the input.\n        out_channels (int): Channel number of the output.\n        mode (str): Upsampling/downsampling mode. Options: down | up. Default: down.\n    \"\"\"\n\n    def __init__(self, in_channels, out_channels, mode='down'):\n        super(ResBlock, self).__init__()\n\n        self.conv1 = nn.Conv2d(in_channels, in_channels, 3, 1, 1)\n        self.conv2 = nn.Conv2d(in_channels, out_channels, 3, 1, 1)\n        self.skip = nn.Conv2d(in_channels, out_channels, 1, bias=False)\n        if mode == 'down':\n            self.scale_factor = 0.5\n        elif mode == 'up':\n            self.scale_factor = 2\n\n    def forward(self, x):\n        out = F.leaky_relu_(self.conv1(x), negative_slope=0.2)\n        # upsample/downsample\n        out = F.interpolate(out, scale_factor=self.scale_factor, mode='bilinear', align_corners=False)\n        out = F.leaky_relu_(self.conv2(out), negative_slope=0.2)\n        # skip\n        x = F.interpolate(x, scale_factor=self.scale_factor, mode='bilinear', align_corners=False)\n        skip = self.skip(x)\n        out = out + skip\n        return out\n\n\n@ARCH_REGISTRY.register()\nclass GFPGANv1Clean(nn.Module):\n    \"\"\"The GFPGAN architecture: Unet + StyleGAN2 decoder with SFT.\n\n    It is the clean version without custom compiled CUDA extensions used in StyleGAN2.\n\n    Ref: GFP-GAN: Towards Real-World Blind Face Restoration with Generative Facial Prior.\n\n    Args:\n        out_size (int): The spatial size of outputs.\n        num_style_feat (int): Channel number of style features. Default: 512.\n        channel_multiplier (int): Channel multiplier for large networks of StyleGAN2. Default: 2.\n        decoder_load_path (str): The path to the pre-trained decoder model (usually, the StyleGAN2). Default: None.\n        fix_decoder (bool): Whether to fix the decoder. Default: True.\n\n        num_mlp (int): Layer number of MLP style layers. Default: 8.\n        input_is_latent (bool): Whether input is latent style. Default: False.\n        different_w (bool): Whether to use different latent w for different layers. Default: False.\n        narrow (float): The narrow ratio for channels. Default: 1.\n        sft_half (bool): Whether to apply SFT on half of the input channels. Default: False.\n    \"\"\"\n\n    def __init__(\n            self,\n            out_size,\n            num_style_feat=512,\n            channel_multiplier=1,\n            decoder_load_path=None,\n            fix_decoder=True,\n            # for stylegan decoder\n            num_mlp=8,\n            input_is_latent=False,\n            different_w=False,\n            narrow=1,\n            sft_half=False):\n\n        super(GFPGANv1Clean, self).__init__()\n        self.input_is_latent = input_is_latent\n        self.different_w = different_w\n        self.num_style_feat = num_style_feat\n\n        unet_narrow = narrow * 0.5  # by default, use a half of input channels\n        channels = {\n            '4': int(512 * unet_narrow),\n            '8': int(512 * unet_narrow),\n            '16': int(512 * unet_narrow),\n            '32': int(512 * unet_narrow),\n            '64': int(256 * channel_multiplier * unet_narrow),\n            '128': int(128 * channel_multiplier * unet_narrow),\n            '256': int(64 * channel_multiplier * unet_narrow),\n            '512': int(32 * channel_multiplier * unet_narrow),\n            '1024': int(16 * channel_multiplier * unet_narrow)\n        }\n\n        self.log_size = int(math.log(out_size, 2))\n        first_out_size = 2**(int(math.log(out_size, 2)))\n\n        self.conv_body_first = nn.Conv2d(3, channels[f'{first_out_size}'], 1)\n\n        # downsample\n        in_channels = channels[f'{first_out_size}']\n        self.conv_body_down = nn.ModuleList()\n        for i in range(self.log_size, 2, -1):\n            out_channels = channels[f'{2**(i - 1)}']\n            self.conv_body_down.append(ResBlock(in_channels, out_channels, mode='down'))\n            in_channels = out_channels\n\n        self.final_conv = nn.Conv2d(in_channels, channels['4'], 3, 1, 1)\n\n        # upsample\n        in_channels = channels['4']\n        self.conv_body_up = nn.ModuleList()\n        for i in range(3, self.log_size + 1):\n            out_channels = channels[f'{2**i}']\n            self.conv_body_up.append(ResBlock(in_channels, out_channels, mode='up'))\n            in_channels = out_channels\n\n        # to RGB\n        self.toRGB = nn.ModuleList()\n        for i in range(3, self.log_size + 1):\n            self.toRGB.append(nn.Conv2d(channels[f'{2**i}'], 3, 1))\n\n        if different_w:\n            linear_out_channel = (int(math.log(out_size, 2)) * 2 - 2) * num_style_feat\n        else:\n            linear_out_channel = num_style_feat\n\n        self.final_linear = nn.Linear(channels['4'] * 4 * 4, linear_out_channel)\n\n        # the decoder: stylegan2 generator with SFT modulations\n        self.stylegan_decoder = StyleGAN2GeneratorCSFT(\n            out_size=out_size,\n            num_style_feat=num_style_feat,\n            num_mlp=num_mlp,\n            channel_multiplier=channel_multiplier,\n            narrow=narrow,\n            sft_half=sft_half)\n\n        # load pre-trained stylegan2 model if necessary\n        if decoder_load_path:\n            self.stylegan_decoder.load_state_dict(\n                torch.load(decoder_load_path, map_location=lambda storage, loc: storage)['params_ema'])\n        # fix decoder without updating params\n        if fix_decoder:\n            for _, param in self.stylegan_decoder.named_parameters():\n                param.requires_grad = False\n\n        # for SFT modulations (scale and shift)\n        self.condition_scale = nn.ModuleList()\n        self.condition_shift = nn.ModuleList()\n        for i in range(3, self.log_size + 1):\n            out_channels = channels[f'{2**i}']\n            if sft_half:\n                sft_out_channels = out_channels\n            else:\n                sft_out_channels = out_channels * 2\n            self.condition_scale.append(\n                nn.Sequential(\n                    nn.Conv2d(out_channels, out_channels, 3, 1, 1), nn.LeakyReLU(0.2, True),\n                    nn.Conv2d(out_channels, sft_out_channels, 3, 1, 1)))\n            self.condition_shift.append(\n                nn.Sequential(\n                    nn.Conv2d(out_channels, out_channels, 3, 1, 1), nn.LeakyReLU(0.2, True),\n                    nn.Conv2d(out_channels, sft_out_channels, 3, 1, 1)))\n\n    def forward(self, x, return_latents=False, return_rgb=True, randomize_noise=True, **kwargs):\n        \"\"\"Forward function for GFPGANv1Clean.\n\n        Args:\n            x (Tensor): Input images.\n            return_latents (bool): Whether to return style latents. Default: False.\n            return_rgb (bool): Whether return intermediate rgb images. Default: True.\n            randomize_noise (bool): Randomize noise, used when 'noise' is False. Default: True.\n        \"\"\"\n        conditions = []\n        unet_skips = []\n        out_rgbs = []\n\n        # encoder\n        feat = F.leaky_relu_(self.conv_body_first(x), negative_slope=0.2)\n        for i in range(self.log_size - 2):\n            feat = self.conv_body_down[i](feat)\n            unet_skips.insert(0, feat)\n        feat = F.leaky_relu_(self.final_conv(feat), negative_slope=0.2)\n\n        # style code\n        style_code = self.final_linear(feat.view(feat.size(0), -1))\n        if self.different_w:\n            style_code = style_code.view(style_code.size(0), -1, self.num_style_feat)\n\n        # decode\n        for i in range(self.log_size - 2):\n            # add unet skip\n            feat = feat + unet_skips[i]\n            # ResUpLayer\n            feat = self.conv_body_up[i](feat)\n            # generate scale and shift for SFT layers\n            scale = self.condition_scale[i](feat)\n            conditions.append(scale.clone())\n            shift = self.condition_shift[i](feat)\n            conditions.append(shift.clone())\n            # generate rgb images\n            if return_rgb:\n                out_rgbs.append(self.toRGB[i](feat))\n\n        # decoder\n        image, _ = self.stylegan_decoder([style_code],\n                                         conditions,\n                                         return_latents=return_latents,\n                                         input_is_latent=self.input_is_latent,\n                                         randomize_noise=randomize_noise)\n\n        return image, out_rgbs\n", "gfpgan/archs/gfpganv1_arch.py": "import math\nimport random\nimport torch\nfrom basicsr.archs.stylegan2_arch import (ConvLayer, EqualConv2d, EqualLinear, ResBlock, ScaledLeakyReLU,\n                                          StyleGAN2Generator)\nfrom basicsr.ops.fused_act import FusedLeakyReLU\nfrom basicsr.utils.registry import ARCH_REGISTRY\nfrom torch import nn\nfrom torch.nn import functional as F\n\n\nclass StyleGAN2GeneratorSFT(StyleGAN2Generator):\n    \"\"\"StyleGAN2 Generator with SFT modulation (Spatial Feature Transform).\n\n    Args:\n        out_size (int): The spatial size of outputs.\n        num_style_feat (int): Channel number of style features. Default: 512.\n        num_mlp (int): Layer number of MLP style layers. Default: 8.\n        channel_multiplier (int): Channel multiplier for large networks of StyleGAN2. Default: 2.\n        resample_kernel (list[int]): A list indicating the 1D resample kernel magnitude. A cross production will be\n            applied to extent 1D resample kernel to 2D resample kernel. Default: (1, 3, 3, 1).\n        lr_mlp (float): Learning rate multiplier for mlp layers. Default: 0.01.\n        narrow (float): The narrow ratio for channels. Default: 1.\n        sft_half (bool): Whether to apply SFT on half of the input channels. Default: False.\n    \"\"\"\n\n    def __init__(self,\n                 out_size,\n                 num_style_feat=512,\n                 num_mlp=8,\n                 channel_multiplier=2,\n                 resample_kernel=(1, 3, 3, 1),\n                 lr_mlp=0.01,\n                 narrow=1,\n                 sft_half=False):\n        super(StyleGAN2GeneratorSFT, self).__init__(\n            out_size,\n            num_style_feat=num_style_feat,\n            num_mlp=num_mlp,\n            channel_multiplier=channel_multiplier,\n            resample_kernel=resample_kernel,\n            lr_mlp=lr_mlp,\n            narrow=narrow)\n        self.sft_half = sft_half\n\n    def forward(self,\n                styles,\n                conditions,\n                input_is_latent=False,\n                noise=None,\n                randomize_noise=True,\n                truncation=1,\n                truncation_latent=None,\n                inject_index=None,\n                return_latents=False):\n        \"\"\"Forward function for StyleGAN2GeneratorSFT.\n\n        Args:\n            styles (list[Tensor]): Sample codes of styles.\n            conditions (list[Tensor]): SFT conditions to generators.\n            input_is_latent (bool): Whether input is latent style. Default: False.\n            noise (Tensor | None): Input noise or None. Default: None.\n            randomize_noise (bool): Randomize noise, used when 'noise' is False. Default: True.\n            truncation (float): The truncation ratio. Default: 1.\n            truncation_latent (Tensor | None): The truncation latent tensor. Default: None.\n            inject_index (int | None): The injection index for mixing noise. Default: None.\n            return_latents (bool): Whether to return style latents. Default: False.\n        \"\"\"\n        # style codes -> latents with Style MLP layer\n        if not input_is_latent:\n            styles = [self.style_mlp(s) for s in styles]\n        # noises\n        if noise is None:\n            if randomize_noise:\n                noise = [None] * self.num_layers  # for each style conv layer\n            else:  # use the stored noise\n                noise = [getattr(self.noises, f'noise{i}') for i in range(self.num_layers)]\n        # style truncation\n        if truncation < 1:\n            style_truncation = []\n            for style in styles:\n                style_truncation.append(truncation_latent + truncation * (style - truncation_latent))\n            styles = style_truncation\n        # get style latents with injection\n        if len(styles) == 1:\n            inject_index = self.num_latent\n\n            if styles[0].ndim < 3:\n                # repeat latent code for all the layers\n                latent = styles[0].unsqueeze(1).repeat(1, inject_index, 1)\n            else:  # used for encoder with different latent code for each layer\n                latent = styles[0]\n        elif len(styles) == 2:  # mixing noises\n            if inject_index is None:\n                inject_index = random.randint(1, self.num_latent - 1)\n            latent1 = styles[0].unsqueeze(1).repeat(1, inject_index, 1)\n            latent2 = styles[1].unsqueeze(1).repeat(1, self.num_latent - inject_index, 1)\n            latent = torch.cat([latent1, latent2], 1)\n\n        # main generation\n        out = self.constant_input(latent.shape[0])\n        out = self.style_conv1(out, latent[:, 0], noise=noise[0])\n        skip = self.to_rgb1(out, latent[:, 1])\n\n        i = 1\n        for conv1, conv2, noise1, noise2, to_rgb in zip(self.style_convs[::2], self.style_convs[1::2], noise[1::2],\n                                                        noise[2::2], self.to_rgbs):\n            out = conv1(out, latent[:, i], noise=noise1)\n\n            # the conditions may have fewer levels\n            if i < len(conditions):\n                # SFT part to combine the conditions\n                if self.sft_half:  # only apply SFT to half of the channels\n                    out_same, out_sft = torch.split(out, int(out.size(1) // 2), dim=1)\n                    out_sft = out_sft * conditions[i - 1] + conditions[i]\n                    out = torch.cat([out_same, out_sft], dim=1)\n                else:  # apply SFT to all the channels\n                    out = out * conditions[i - 1] + conditions[i]\n\n            out = conv2(out, latent[:, i + 1], noise=noise2)\n            skip = to_rgb(out, latent[:, i + 2], skip)  # feature back to the rgb space\n            i += 2\n\n        image = skip\n\n        if return_latents:\n            return image, latent\n        else:\n            return image, None\n\n\nclass ConvUpLayer(nn.Module):\n    \"\"\"Convolutional upsampling layer. It uses bilinear upsampler + Conv.\n\n    Args:\n        in_channels (int): Channel number of the input.\n        out_channels (int): Channel number of the output.\n        kernel_size (int): Size of the convolving kernel.\n        stride (int): Stride of the convolution. Default: 1\n        padding (int): Zero-padding added to both sides of the input. Default: 0.\n        bias (bool): If ``True``, adds a learnable bias to the output. Default: ``True``.\n        bias_init_val (float): Bias initialized value. Default: 0.\n        activate (bool): Whether use activateion. Default: True.\n    \"\"\"\n\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 kernel_size,\n                 stride=1,\n                 padding=0,\n                 bias=True,\n                 bias_init_val=0,\n                 activate=True):\n        super(ConvUpLayer, self).__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.kernel_size = kernel_size\n        self.stride = stride\n        self.padding = padding\n        # self.scale is used to scale the convolution weights, which is related to the common initializations.\n        self.scale = 1 / math.sqrt(in_channels * kernel_size**2)\n\n        self.weight = nn.Parameter(torch.randn(out_channels, in_channels, kernel_size, kernel_size))\n\n        if bias and not activate:\n            self.bias = nn.Parameter(torch.zeros(out_channels).fill_(bias_init_val))\n        else:\n            self.register_parameter('bias', None)\n\n        # activation\n        if activate:\n            if bias:\n                self.activation = FusedLeakyReLU(out_channels)\n            else:\n                self.activation = ScaledLeakyReLU(0.2)\n        else:\n            self.activation = None\n\n    def forward(self, x):\n        # bilinear upsample\n        out = F.interpolate(x, scale_factor=2, mode='bilinear', align_corners=False)\n        # conv\n        out = F.conv2d(\n            out,\n            self.weight * self.scale,\n            bias=self.bias,\n            stride=self.stride,\n            padding=self.padding,\n        )\n        # activation\n        if self.activation is not None:\n            out = self.activation(out)\n        return out\n\n\nclass ResUpBlock(nn.Module):\n    \"\"\"Residual block with upsampling.\n\n    Args:\n        in_channels (int): Channel number of the input.\n        out_channels (int): Channel number of the output.\n    \"\"\"\n\n    def __init__(self, in_channels, out_channels):\n        super(ResUpBlock, self).__init__()\n\n        self.conv1 = ConvLayer(in_channels, in_channels, 3, bias=True, activate=True)\n        self.conv2 = ConvUpLayer(in_channels, out_channels, 3, stride=1, padding=1, bias=True, activate=True)\n        self.skip = ConvUpLayer(in_channels, out_channels, 1, bias=False, activate=False)\n\n    def forward(self, x):\n        out = self.conv1(x)\n        out = self.conv2(out)\n        skip = self.skip(x)\n        out = (out + skip) / math.sqrt(2)\n        return out\n\n\n@ARCH_REGISTRY.register()\nclass GFPGANv1(nn.Module):\n    \"\"\"The GFPGAN architecture: Unet + StyleGAN2 decoder with SFT.\n\n    Ref: GFP-GAN: Towards Real-World Blind Face Restoration with Generative Facial Prior.\n\n    Args:\n        out_size (int): The spatial size of outputs.\n        num_style_feat (int): Channel number of style features. Default: 512.\n        channel_multiplier (int): Channel multiplier for large networks of StyleGAN2. Default: 2.\n        resample_kernel (list[int]): A list indicating the 1D resample kernel magnitude. A cross production will be\n            applied to extent 1D resample kernel to 2D resample kernel. Default: (1, 3, 3, 1).\n        decoder_load_path (str): The path to the pre-trained decoder model (usually, the StyleGAN2). Default: None.\n        fix_decoder (bool): Whether to fix the decoder. Default: True.\n\n        num_mlp (int): Layer number of MLP style layers. Default: 8.\n        lr_mlp (float): Learning rate multiplier for mlp layers. Default: 0.01.\n        input_is_latent (bool): Whether input is latent style. Default: False.\n        different_w (bool): Whether to use different latent w for different layers. Default: False.\n        narrow (float): The narrow ratio for channels. Default: 1.\n        sft_half (bool): Whether to apply SFT on half of the input channels. Default: False.\n    \"\"\"\n\n    def __init__(\n            self,\n            out_size,\n            num_style_feat=512,\n            channel_multiplier=1,\n            resample_kernel=(1, 3, 3, 1),\n            decoder_load_path=None,\n            fix_decoder=True,\n            # for stylegan decoder\n            num_mlp=8,\n            lr_mlp=0.01,\n            input_is_latent=False,\n            different_w=False,\n            narrow=1,\n            sft_half=False):\n\n        super(GFPGANv1, self).__init__()\n        self.input_is_latent = input_is_latent\n        self.different_w = different_w\n        self.num_style_feat = num_style_feat\n\n        unet_narrow = narrow * 0.5  # by default, use a half of input channels\n        channels = {\n            '4': int(512 * unet_narrow),\n            '8': int(512 * unet_narrow),\n            '16': int(512 * unet_narrow),\n            '32': int(512 * unet_narrow),\n            '64': int(256 * channel_multiplier * unet_narrow),\n            '128': int(128 * channel_multiplier * unet_narrow),\n            '256': int(64 * channel_multiplier * unet_narrow),\n            '512': int(32 * channel_multiplier * unet_narrow),\n            '1024': int(16 * channel_multiplier * unet_narrow)\n        }\n\n        self.log_size = int(math.log(out_size, 2))\n        first_out_size = 2**(int(math.log(out_size, 2)))\n\n        self.conv_body_first = ConvLayer(3, channels[f'{first_out_size}'], 1, bias=True, activate=True)\n\n        # downsample\n        in_channels = channels[f'{first_out_size}']\n        self.conv_body_down = nn.ModuleList()\n        for i in range(self.log_size, 2, -1):\n            out_channels = channels[f'{2**(i - 1)}']\n            self.conv_body_down.append(ResBlock(in_channels, out_channels, resample_kernel))\n            in_channels = out_channels\n\n        self.final_conv = ConvLayer(in_channels, channels['4'], 3, bias=True, activate=True)\n\n        # upsample\n        in_channels = channels['4']\n        self.conv_body_up = nn.ModuleList()\n        for i in range(3, self.log_size + 1):\n            out_channels = channels[f'{2**i}']\n            self.conv_body_up.append(ResUpBlock(in_channels, out_channels))\n            in_channels = out_channels\n\n        # to RGB\n        self.toRGB = nn.ModuleList()\n        for i in range(3, self.log_size + 1):\n            self.toRGB.append(EqualConv2d(channels[f'{2**i}'], 3, 1, stride=1, padding=0, bias=True, bias_init_val=0))\n\n        if different_w:\n            linear_out_channel = (int(math.log(out_size, 2)) * 2 - 2) * num_style_feat\n        else:\n            linear_out_channel = num_style_feat\n\n        self.final_linear = EqualLinear(\n            channels['4'] * 4 * 4, linear_out_channel, bias=True, bias_init_val=0, lr_mul=1, activation=None)\n\n        # the decoder: stylegan2 generator with SFT modulations\n        self.stylegan_decoder = StyleGAN2GeneratorSFT(\n            out_size=out_size,\n            num_style_feat=num_style_feat,\n            num_mlp=num_mlp,\n            channel_multiplier=channel_multiplier,\n            resample_kernel=resample_kernel,\n            lr_mlp=lr_mlp,\n            narrow=narrow,\n            sft_half=sft_half)\n\n        # load pre-trained stylegan2 model if necessary\n        if decoder_load_path:\n            self.stylegan_decoder.load_state_dict(\n                torch.load(decoder_load_path, map_location=lambda storage, loc: storage)['params_ema'])\n        # fix decoder without updating params\n        if fix_decoder:\n            for _, param in self.stylegan_decoder.named_parameters():\n                param.requires_grad = False\n\n        # for SFT modulations (scale and shift)\n        self.condition_scale = nn.ModuleList()\n        self.condition_shift = nn.ModuleList()\n        for i in range(3, self.log_size + 1):\n            out_channels = channels[f'{2**i}']\n            if sft_half:\n                sft_out_channels = out_channels\n            else:\n                sft_out_channels = out_channels * 2\n            self.condition_scale.append(\n                nn.Sequential(\n                    EqualConv2d(out_channels, out_channels, 3, stride=1, padding=1, bias=True, bias_init_val=0),\n                    ScaledLeakyReLU(0.2),\n                    EqualConv2d(out_channels, sft_out_channels, 3, stride=1, padding=1, bias=True, bias_init_val=1)))\n            self.condition_shift.append(\n                nn.Sequential(\n                    EqualConv2d(out_channels, out_channels, 3, stride=1, padding=1, bias=True, bias_init_val=0),\n                    ScaledLeakyReLU(0.2),\n                    EqualConv2d(out_channels, sft_out_channels, 3, stride=1, padding=1, bias=True, bias_init_val=0)))\n\n    def forward(self, x, return_latents=False, return_rgb=True, randomize_noise=True, **kwargs):\n        \"\"\"Forward function for GFPGANv1.\n\n        Args:\n            x (Tensor): Input images.\n            return_latents (bool): Whether to return style latents. Default: False.\n            return_rgb (bool): Whether return intermediate rgb images. Default: True.\n            randomize_noise (bool): Randomize noise, used when 'noise' is False. Default: True.\n        \"\"\"\n        conditions = []\n        unet_skips = []\n        out_rgbs = []\n\n        # encoder\n        feat = self.conv_body_first(x)\n        for i in range(self.log_size - 2):\n            feat = self.conv_body_down[i](feat)\n            unet_skips.insert(0, feat)\n\n        feat = self.final_conv(feat)\n\n        # style code\n        style_code = self.final_linear(feat.view(feat.size(0), -1))\n        if self.different_w:\n            style_code = style_code.view(style_code.size(0), -1, self.num_style_feat)\n\n        # decode\n        for i in range(self.log_size - 2):\n            # add unet skip\n            feat = feat + unet_skips[i]\n            # ResUpLayer\n            feat = self.conv_body_up[i](feat)\n            # generate scale and shift for SFT layers\n            scale = self.condition_scale[i](feat)\n            conditions.append(scale.clone())\n            shift = self.condition_shift[i](feat)\n            conditions.append(shift.clone())\n            # generate rgb images\n            if return_rgb:\n                out_rgbs.append(self.toRGB[i](feat))\n\n        # decoder\n        image, _ = self.stylegan_decoder([style_code],\n                                         conditions,\n                                         return_latents=return_latents,\n                                         input_is_latent=self.input_is_latent,\n                                         randomize_noise=randomize_noise)\n\n        return image, out_rgbs\n\n\n@ARCH_REGISTRY.register()\nclass FacialComponentDiscriminator(nn.Module):\n    \"\"\"Facial component (eyes, mouth, noise) discriminator used in GFPGAN.\n    \"\"\"\n\n    def __init__(self):\n        super(FacialComponentDiscriminator, self).__init__()\n        # It now uses a VGG-style architectrue with fixed model size\n        self.conv1 = ConvLayer(3, 64, 3, downsample=False, resample_kernel=(1, 3, 3, 1), bias=True, activate=True)\n        self.conv2 = ConvLayer(64, 128, 3, downsample=True, resample_kernel=(1, 3, 3, 1), bias=True, activate=True)\n        self.conv3 = ConvLayer(128, 128, 3, downsample=False, resample_kernel=(1, 3, 3, 1), bias=True, activate=True)\n        self.conv4 = ConvLayer(128, 256, 3, downsample=True, resample_kernel=(1, 3, 3, 1), bias=True, activate=True)\n        self.conv5 = ConvLayer(256, 256, 3, downsample=False, resample_kernel=(1, 3, 3, 1), bias=True, activate=True)\n        self.final_conv = ConvLayer(256, 1, 3, bias=True, activate=False)\n\n    def forward(self, x, return_feats=False, **kwargs):\n        \"\"\"Forward function for FacialComponentDiscriminator.\n\n        Args:\n            x (Tensor): Input images.\n            return_feats (bool): Whether to return intermediate features. Default: False.\n        \"\"\"\n        feat = self.conv1(x)\n        feat = self.conv3(self.conv2(feat))\n        rlt_feats = []\n        if return_feats:\n            rlt_feats.append(feat.clone())\n        feat = self.conv5(self.conv4(feat))\n        if return_feats:\n            rlt_feats.append(feat.clone())\n        out = self.final_conv(feat)\n\n        if return_feats:\n            return out, rlt_feats\n        else:\n            return out, None\n", "gfpgan/archs/stylegan2_clean_arch.py": "import math\nimport random\nimport torch\nfrom basicsr.archs.arch_util import default_init_weights\nfrom basicsr.utils.registry import ARCH_REGISTRY\nfrom torch import nn\nfrom torch.nn import functional as F\n\n\nclass NormStyleCode(nn.Module):\n\n    def forward(self, x):\n        \"\"\"Normalize the style codes.\n\n        Args:\n            x (Tensor): Style codes with shape (b, c).\n\n        Returns:\n            Tensor: Normalized tensor.\n        \"\"\"\n        return x * torch.rsqrt(torch.mean(x**2, dim=1, keepdim=True) + 1e-8)\n\n\nclass ModulatedConv2d(nn.Module):\n    \"\"\"Modulated Conv2d used in StyleGAN2.\n\n    There is no bias in ModulatedConv2d.\n\n    Args:\n        in_channels (int): Channel number of the input.\n        out_channels (int): Channel number of the output.\n        kernel_size (int): Size of the convolving kernel.\n        num_style_feat (int): Channel number of style features.\n        demodulate (bool): Whether to demodulate in the conv layer. Default: True.\n        sample_mode (str | None): Indicating 'upsample', 'downsample' or None. Default: None.\n        eps (float): A value added to the denominator for numerical stability. Default: 1e-8.\n    \"\"\"\n\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 kernel_size,\n                 num_style_feat,\n                 demodulate=True,\n                 sample_mode=None,\n                 eps=1e-8):\n        super(ModulatedConv2d, self).__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.kernel_size = kernel_size\n        self.demodulate = demodulate\n        self.sample_mode = sample_mode\n        self.eps = eps\n\n        # modulation inside each modulated conv\n        self.modulation = nn.Linear(num_style_feat, in_channels, bias=True)\n        # initialization\n        default_init_weights(self.modulation, scale=1, bias_fill=1, a=0, mode='fan_in', nonlinearity='linear')\n\n        self.weight = nn.Parameter(\n            torch.randn(1, out_channels, in_channels, kernel_size, kernel_size) /\n            math.sqrt(in_channels * kernel_size**2))\n        self.padding = kernel_size // 2\n\n    def forward(self, x, style):\n        \"\"\"Forward function.\n\n        Args:\n            x (Tensor): Tensor with shape (b, c, h, w).\n            style (Tensor): Tensor with shape (b, num_style_feat).\n\n        Returns:\n            Tensor: Modulated tensor after convolution.\n        \"\"\"\n        b, c, h, w = x.shape  # c = c_in\n        # weight modulation\n        style = self.modulation(style).view(b, 1, c, 1, 1)\n        # self.weight: (1, c_out, c_in, k, k); style: (b, 1, c, 1, 1)\n        weight = self.weight * style  # (b, c_out, c_in, k, k)\n\n        if self.demodulate:\n            demod = torch.rsqrt(weight.pow(2).sum([2, 3, 4]) + self.eps)\n            weight = weight * demod.view(b, self.out_channels, 1, 1, 1)\n\n        weight = weight.view(b * self.out_channels, c, self.kernel_size, self.kernel_size)\n\n        # upsample or downsample if necessary\n        if self.sample_mode == 'upsample':\n            x = F.interpolate(x, scale_factor=2, mode='bilinear', align_corners=False)\n        elif self.sample_mode == 'downsample':\n            x = F.interpolate(x, scale_factor=0.5, mode='bilinear', align_corners=False)\n\n        b, c, h, w = x.shape\n        x = x.view(1, b * c, h, w)\n        # weight: (b*c_out, c_in, k, k), groups=b\n        out = F.conv2d(x, weight, padding=self.padding, groups=b)\n        out = out.view(b, self.out_channels, *out.shape[2:4])\n\n        return out\n\n    def __repr__(self):\n        return (f'{self.__class__.__name__}(in_channels={self.in_channels}, out_channels={self.out_channels}, '\n                f'kernel_size={self.kernel_size}, demodulate={self.demodulate}, sample_mode={self.sample_mode})')\n\n\nclass StyleConv(nn.Module):\n    \"\"\"Style conv used in StyleGAN2.\n\n    Args:\n        in_channels (int): Channel number of the input.\n        out_channels (int): Channel number of the output.\n        kernel_size (int): Size of the convolving kernel.\n        num_style_feat (int): Channel number of style features.\n        demodulate (bool): Whether demodulate in the conv layer. Default: True.\n        sample_mode (str | None): Indicating 'upsample', 'downsample' or None. Default: None.\n    \"\"\"\n\n    def __init__(self, in_channels, out_channels, kernel_size, num_style_feat, demodulate=True, sample_mode=None):\n        super(StyleConv, self).__init__()\n        self.modulated_conv = ModulatedConv2d(\n            in_channels, out_channels, kernel_size, num_style_feat, demodulate=demodulate, sample_mode=sample_mode)\n        self.weight = nn.Parameter(torch.zeros(1))  # for noise injection\n        self.bias = nn.Parameter(torch.zeros(1, out_channels, 1, 1))\n        self.activate = nn.LeakyReLU(negative_slope=0.2, inplace=True)\n\n    def forward(self, x, style, noise=None):\n        # modulate\n        out = self.modulated_conv(x, style) * 2**0.5  # for conversion\n        # noise injection\n        if noise is None:\n            b, _, h, w = out.shape\n            noise = out.new_empty(b, 1, h, w).normal_()\n        out = out + self.weight * noise\n        # add bias\n        out = out + self.bias\n        # activation\n        out = self.activate(out)\n        return out\n\n\nclass ToRGB(nn.Module):\n    \"\"\"To RGB (image space) from features.\n\n    Args:\n        in_channels (int): Channel number of input.\n        num_style_feat (int): Channel number of style features.\n        upsample (bool): Whether to upsample. Default: True.\n    \"\"\"\n\n    def __init__(self, in_channels, num_style_feat, upsample=True):\n        super(ToRGB, self).__init__()\n        self.upsample = upsample\n        self.modulated_conv = ModulatedConv2d(\n            in_channels, 3, kernel_size=1, num_style_feat=num_style_feat, demodulate=False, sample_mode=None)\n        self.bias = nn.Parameter(torch.zeros(1, 3, 1, 1))\n\n    def forward(self, x, style, skip=None):\n        \"\"\"Forward function.\n\n        Args:\n            x (Tensor): Feature tensor with shape (b, c, h, w).\n            style (Tensor): Tensor with shape (b, num_style_feat).\n            skip (Tensor): Base/skip tensor. Default: None.\n\n        Returns:\n            Tensor: RGB images.\n        \"\"\"\n        out = self.modulated_conv(x, style)\n        out = out + self.bias\n        if skip is not None:\n            if self.upsample:\n                skip = F.interpolate(skip, scale_factor=2, mode='bilinear', align_corners=False)\n            out = out + skip\n        return out\n\n\nclass ConstantInput(nn.Module):\n    \"\"\"Constant input.\n\n    Args:\n        num_channel (int): Channel number of constant input.\n        size (int): Spatial size of constant input.\n    \"\"\"\n\n    def __init__(self, num_channel, size):\n        super(ConstantInput, self).__init__()\n        self.weight = nn.Parameter(torch.randn(1, num_channel, size, size))\n\n    def forward(self, batch):\n        out = self.weight.repeat(batch, 1, 1, 1)\n        return out\n\n\n@ARCH_REGISTRY.register()\nclass StyleGAN2GeneratorClean(nn.Module):\n    \"\"\"Clean version of StyleGAN2 Generator.\n\n    Args:\n        out_size (int): The spatial size of outputs.\n        num_style_feat (int): Channel number of style features. Default: 512.\n        num_mlp (int): Layer number of MLP style layers. Default: 8.\n        channel_multiplier (int): Channel multiplier for large networks of StyleGAN2. Default: 2.\n        narrow (float): Narrow ratio for channels. Default: 1.0.\n    \"\"\"\n\n    def __init__(self, out_size, num_style_feat=512, num_mlp=8, channel_multiplier=2, narrow=1):\n        super(StyleGAN2GeneratorClean, self).__init__()\n        # Style MLP layers\n        self.num_style_feat = num_style_feat\n        style_mlp_layers = [NormStyleCode()]\n        for i in range(num_mlp):\n            style_mlp_layers.extend(\n                [nn.Linear(num_style_feat, num_style_feat, bias=True),\n                 nn.LeakyReLU(negative_slope=0.2, inplace=True)])\n        self.style_mlp = nn.Sequential(*style_mlp_layers)\n        # initialization\n        default_init_weights(self.style_mlp, scale=1, bias_fill=0, a=0.2, mode='fan_in', nonlinearity='leaky_relu')\n\n        # channel list\n        channels = {\n            '4': int(512 * narrow),\n            '8': int(512 * narrow),\n            '16': int(512 * narrow),\n            '32': int(512 * narrow),\n            '64': int(256 * channel_multiplier * narrow),\n            '128': int(128 * channel_multiplier * narrow),\n            '256': int(64 * channel_multiplier * narrow),\n            '512': int(32 * channel_multiplier * narrow),\n            '1024': int(16 * channel_multiplier * narrow)\n        }\n        self.channels = channels\n\n        self.constant_input = ConstantInput(channels['4'], size=4)\n        self.style_conv1 = StyleConv(\n            channels['4'],\n            channels['4'],\n            kernel_size=3,\n            num_style_feat=num_style_feat,\n            demodulate=True,\n            sample_mode=None)\n        self.to_rgb1 = ToRGB(channels['4'], num_style_feat, upsample=False)\n\n        self.log_size = int(math.log(out_size, 2))\n        self.num_layers = (self.log_size - 2) * 2 + 1\n        self.num_latent = self.log_size * 2 - 2\n\n        self.style_convs = nn.ModuleList()\n        self.to_rgbs = nn.ModuleList()\n        self.noises = nn.Module()\n\n        in_channels = channels['4']\n        # noise\n        for layer_idx in range(self.num_layers):\n            resolution = 2**((layer_idx + 5) // 2)\n            shape = [1, 1, resolution, resolution]\n            self.noises.register_buffer(f'noise{layer_idx}', torch.randn(*shape))\n        # style convs and to_rgbs\n        for i in range(3, self.log_size + 1):\n            out_channels = channels[f'{2**i}']\n            self.style_convs.append(\n                StyleConv(\n                    in_channels,\n                    out_channels,\n                    kernel_size=3,\n                    num_style_feat=num_style_feat,\n                    demodulate=True,\n                    sample_mode='upsample'))\n            self.style_convs.append(\n                StyleConv(\n                    out_channels,\n                    out_channels,\n                    kernel_size=3,\n                    num_style_feat=num_style_feat,\n                    demodulate=True,\n                    sample_mode=None))\n            self.to_rgbs.append(ToRGB(out_channels, num_style_feat, upsample=True))\n            in_channels = out_channels\n\n    def make_noise(self):\n        \"\"\"Make noise for noise injection.\"\"\"\n        device = self.constant_input.weight.device\n        noises = [torch.randn(1, 1, 4, 4, device=device)]\n\n        for i in range(3, self.log_size + 1):\n            for _ in range(2):\n                noises.append(torch.randn(1, 1, 2**i, 2**i, device=device))\n\n        return noises\n\n    def get_latent(self, x):\n        return self.style_mlp(x)\n\n    def mean_latent(self, num_latent):\n        latent_in = torch.randn(num_latent, self.num_style_feat, device=self.constant_input.weight.device)\n        latent = self.style_mlp(latent_in).mean(0, keepdim=True)\n        return latent\n\n    def forward(self,\n                styles,\n                input_is_latent=False,\n                noise=None,\n                randomize_noise=True,\n                truncation=1,\n                truncation_latent=None,\n                inject_index=None,\n                return_latents=False):\n        \"\"\"Forward function for StyleGAN2GeneratorClean.\n\n        Args:\n            styles (list[Tensor]): Sample codes of styles.\n            input_is_latent (bool): Whether input is latent style. Default: False.\n            noise (Tensor | None): Input noise or None. Default: None.\n            randomize_noise (bool): Randomize noise, used when 'noise' is False. Default: True.\n            truncation (float): The truncation ratio. Default: 1.\n            truncation_latent (Tensor | None): The truncation latent tensor. Default: None.\n            inject_index (int | None): The injection index for mixing noise. Default: None.\n            return_latents (bool): Whether to return style latents. Default: False.\n        \"\"\"\n        # style codes -> latents with Style MLP layer\n        if not input_is_latent:\n            styles = [self.style_mlp(s) for s in styles]\n        # noises\n        if noise is None:\n            if randomize_noise:\n                noise = [None] * self.num_layers  # for each style conv layer\n            else:  # use the stored noise\n                noise = [getattr(self.noises, f'noise{i}') for i in range(self.num_layers)]\n        # style truncation\n        if truncation < 1:\n            style_truncation = []\n            for style in styles:\n                style_truncation.append(truncation_latent + truncation * (style - truncation_latent))\n            styles = style_truncation\n        # get style latents with injection\n        if len(styles) == 1:\n            inject_index = self.num_latent\n\n            if styles[0].ndim < 3:\n                # repeat latent code for all the layers\n                latent = styles[0].unsqueeze(1).repeat(1, inject_index, 1)\n            else:  # used for encoder with different latent code for each layer\n                latent = styles[0]\n        elif len(styles) == 2:  # mixing noises\n            if inject_index is None:\n                inject_index = random.randint(1, self.num_latent - 1)\n            latent1 = styles[0].unsqueeze(1).repeat(1, inject_index, 1)\n            latent2 = styles[1].unsqueeze(1).repeat(1, self.num_latent - inject_index, 1)\n            latent = torch.cat([latent1, latent2], 1)\n\n        # main generation\n        out = self.constant_input(latent.shape[0])\n        out = self.style_conv1(out, latent[:, 0], noise=noise[0])\n        skip = self.to_rgb1(out, latent[:, 1])\n\n        i = 1\n        for conv1, conv2, noise1, noise2, to_rgb in zip(self.style_convs[::2], self.style_convs[1::2], noise[1::2],\n                                                        noise[2::2], self.to_rgbs):\n            out = conv1(out, latent[:, i], noise=noise1)\n            out = conv2(out, latent[:, i + 1], noise=noise2)\n            skip = to_rgb(out, latent[:, i + 2], skip)  # feature back to the rgb space\n            i += 2\n\n        image = skip\n\n        if return_latents:\n            return image, latent\n        else:\n            return image, None\n", "gfpgan/archs/__init__.py": "import importlib\nfrom basicsr.utils import scandir\nfrom os import path as osp\n\n# automatically scan and import arch modules for registry\n# scan all the files that end with '_arch.py' under the archs folder\narch_folder = osp.dirname(osp.abspath(__file__))\narch_filenames = [osp.splitext(osp.basename(v))[0] for v in scandir(arch_folder) if v.endswith('_arch.py')]\n# import all the arch modules\n_arch_modules = [importlib.import_module(f'gfpgan.archs.{file_name}') for file_name in arch_filenames]\n", "gfpgan/archs/gfpgan_bilinear_arch.py": "import math\nimport random\nimport torch\nfrom basicsr.utils.registry import ARCH_REGISTRY\nfrom torch import nn\n\nfrom .gfpganv1_arch import ResUpBlock\nfrom .stylegan2_bilinear_arch import (ConvLayer, EqualConv2d, EqualLinear, ResBlock, ScaledLeakyReLU,\n                                      StyleGAN2GeneratorBilinear)\n\n\nclass StyleGAN2GeneratorBilinearSFT(StyleGAN2GeneratorBilinear):\n    \"\"\"StyleGAN2 Generator with SFT modulation (Spatial Feature Transform).\n\n    It is the bilinear version. It does not use the complicated UpFirDnSmooth function that is not friendly for\n    deployment. It can be easily converted to the clean version: StyleGAN2GeneratorCSFT.\n\n    Args:\n        out_size (int): The spatial size of outputs.\n        num_style_feat (int): Channel number of style features. Default: 512.\n        num_mlp (int): Layer number of MLP style layers. Default: 8.\n        channel_multiplier (int): Channel multiplier for large networks of StyleGAN2. Default: 2.\n        lr_mlp (float): Learning rate multiplier for mlp layers. Default: 0.01.\n        narrow (float): The narrow ratio for channels. Default: 1.\n        sft_half (bool): Whether to apply SFT on half of the input channels. Default: False.\n    \"\"\"\n\n    def __init__(self,\n                 out_size,\n                 num_style_feat=512,\n                 num_mlp=8,\n                 channel_multiplier=2,\n                 lr_mlp=0.01,\n                 narrow=1,\n                 sft_half=False):\n        super(StyleGAN2GeneratorBilinearSFT, self).__init__(\n            out_size,\n            num_style_feat=num_style_feat,\n            num_mlp=num_mlp,\n            channel_multiplier=channel_multiplier,\n            lr_mlp=lr_mlp,\n            narrow=narrow)\n        self.sft_half = sft_half\n\n    def forward(self,\n                styles,\n                conditions,\n                input_is_latent=False,\n                noise=None,\n                randomize_noise=True,\n                truncation=1,\n                truncation_latent=None,\n                inject_index=None,\n                return_latents=False):\n        \"\"\"Forward function for StyleGAN2GeneratorBilinearSFT.\n\n        Args:\n            styles (list[Tensor]): Sample codes of styles.\n            conditions (list[Tensor]): SFT conditions to generators.\n            input_is_latent (bool): Whether input is latent style. Default: False.\n            noise (Tensor | None): Input noise or None. Default: None.\n            randomize_noise (bool): Randomize noise, used when 'noise' is False. Default: True.\n            truncation (float): The truncation ratio. Default: 1.\n            truncation_latent (Tensor | None): The truncation latent tensor. Default: None.\n            inject_index (int | None): The injection index for mixing noise. Default: None.\n            return_latents (bool): Whether to return style latents. Default: False.\n        \"\"\"\n        # style codes -> latents with Style MLP layer\n        if not input_is_latent:\n            styles = [self.style_mlp(s) for s in styles]\n        # noises\n        if noise is None:\n            if randomize_noise:\n                noise = [None] * self.num_layers  # for each style conv layer\n            else:  # use the stored noise\n                noise = [getattr(self.noises, f'noise{i}') for i in range(self.num_layers)]\n        # style truncation\n        if truncation < 1:\n            style_truncation = []\n            for style in styles:\n                style_truncation.append(truncation_latent + truncation * (style - truncation_latent))\n            styles = style_truncation\n        # get style latents with injection\n        if len(styles) == 1:\n            inject_index = self.num_latent\n\n            if styles[0].ndim < 3:\n                # repeat latent code for all the layers\n                latent = styles[0].unsqueeze(1).repeat(1, inject_index, 1)\n            else:  # used for encoder with different latent code for each layer\n                latent = styles[0]\n        elif len(styles) == 2:  # mixing noises\n            if inject_index is None:\n                inject_index = random.randint(1, self.num_latent - 1)\n            latent1 = styles[0].unsqueeze(1).repeat(1, inject_index, 1)\n            latent2 = styles[1].unsqueeze(1).repeat(1, self.num_latent - inject_index, 1)\n            latent = torch.cat([latent1, latent2], 1)\n\n        # main generation\n        out = self.constant_input(latent.shape[0])\n        out = self.style_conv1(out, latent[:, 0], noise=noise[0])\n        skip = self.to_rgb1(out, latent[:, 1])\n\n        i = 1\n        for conv1, conv2, noise1, noise2, to_rgb in zip(self.style_convs[::2], self.style_convs[1::2], noise[1::2],\n                                                        noise[2::2], self.to_rgbs):\n            out = conv1(out, latent[:, i], noise=noise1)\n\n            # the conditions may have fewer levels\n            if i < len(conditions):\n                # SFT part to combine the conditions\n                if self.sft_half:  # only apply SFT to half of the channels\n                    out_same, out_sft = torch.split(out, int(out.size(1) // 2), dim=1)\n                    out_sft = out_sft * conditions[i - 1] + conditions[i]\n                    out = torch.cat([out_same, out_sft], dim=1)\n                else:  # apply SFT to all the channels\n                    out = out * conditions[i - 1] + conditions[i]\n\n            out = conv2(out, latent[:, i + 1], noise=noise2)\n            skip = to_rgb(out, latent[:, i + 2], skip)  # feature back to the rgb space\n            i += 2\n\n        image = skip\n\n        if return_latents:\n            return image, latent\n        else:\n            return image, None\n\n\n@ARCH_REGISTRY.register()\nclass GFPGANBilinear(nn.Module):\n    \"\"\"The GFPGAN architecture: Unet + StyleGAN2 decoder with SFT.\n\n    It is the bilinear version and it does not use the complicated UpFirDnSmooth function that is not friendly for\n    deployment. It can be easily converted to the clean version: GFPGANv1Clean.\n\n\n    Ref: GFP-GAN: Towards Real-World Blind Face Restoration with Generative Facial Prior.\n\n    Args:\n        out_size (int): The spatial size of outputs.\n        num_style_feat (int): Channel number of style features. Default: 512.\n        channel_multiplier (int): Channel multiplier for large networks of StyleGAN2. Default: 2.\n        decoder_load_path (str): The path to the pre-trained decoder model (usually, the StyleGAN2). Default: None.\n        fix_decoder (bool): Whether to fix the decoder. Default: True.\n\n        num_mlp (int): Layer number of MLP style layers. Default: 8.\n        lr_mlp (float): Learning rate multiplier for mlp layers. Default: 0.01.\n        input_is_latent (bool): Whether input is latent style. Default: False.\n        different_w (bool): Whether to use different latent w for different layers. Default: False.\n        narrow (float): The narrow ratio for channels. Default: 1.\n        sft_half (bool): Whether to apply SFT on half of the input channels. Default: False.\n    \"\"\"\n\n    def __init__(\n            self,\n            out_size,\n            num_style_feat=512,\n            channel_multiplier=1,\n            decoder_load_path=None,\n            fix_decoder=True,\n            # for stylegan decoder\n            num_mlp=8,\n            lr_mlp=0.01,\n            input_is_latent=False,\n            different_w=False,\n            narrow=1,\n            sft_half=False):\n\n        super(GFPGANBilinear, self).__init__()\n        self.input_is_latent = input_is_latent\n        self.different_w = different_w\n        self.num_style_feat = num_style_feat\n\n        unet_narrow = narrow * 0.5  # by default, use a half of input channels\n        channels = {\n            '4': int(512 * unet_narrow),\n            '8': int(512 * unet_narrow),\n            '16': int(512 * unet_narrow),\n            '32': int(512 * unet_narrow),\n            '64': int(256 * channel_multiplier * unet_narrow),\n            '128': int(128 * channel_multiplier * unet_narrow),\n            '256': int(64 * channel_multiplier * unet_narrow),\n            '512': int(32 * channel_multiplier * unet_narrow),\n            '1024': int(16 * channel_multiplier * unet_narrow)\n        }\n\n        self.log_size = int(math.log(out_size, 2))\n        first_out_size = 2**(int(math.log(out_size, 2)))\n\n        self.conv_body_first = ConvLayer(3, channels[f'{first_out_size}'], 1, bias=True, activate=True)\n\n        # downsample\n        in_channels = channels[f'{first_out_size}']\n        self.conv_body_down = nn.ModuleList()\n        for i in range(self.log_size, 2, -1):\n            out_channels = channels[f'{2**(i - 1)}']\n            self.conv_body_down.append(ResBlock(in_channels, out_channels))\n            in_channels = out_channels\n\n        self.final_conv = ConvLayer(in_channels, channels['4'], 3, bias=True, activate=True)\n\n        # upsample\n        in_channels = channels['4']\n        self.conv_body_up = nn.ModuleList()\n        for i in range(3, self.log_size + 1):\n            out_channels = channels[f'{2**i}']\n            self.conv_body_up.append(ResUpBlock(in_channels, out_channels))\n            in_channels = out_channels\n\n        # to RGB\n        self.toRGB = nn.ModuleList()\n        for i in range(3, self.log_size + 1):\n            self.toRGB.append(EqualConv2d(channels[f'{2**i}'], 3, 1, stride=1, padding=0, bias=True, bias_init_val=0))\n\n        if different_w:\n            linear_out_channel = (int(math.log(out_size, 2)) * 2 - 2) * num_style_feat\n        else:\n            linear_out_channel = num_style_feat\n\n        self.final_linear = EqualLinear(\n            channels['4'] * 4 * 4, linear_out_channel, bias=True, bias_init_val=0, lr_mul=1, activation=None)\n\n        # the decoder: stylegan2 generator with SFT modulations\n        self.stylegan_decoder = StyleGAN2GeneratorBilinearSFT(\n            out_size=out_size,\n            num_style_feat=num_style_feat,\n            num_mlp=num_mlp,\n            channel_multiplier=channel_multiplier,\n            lr_mlp=lr_mlp,\n            narrow=narrow,\n            sft_half=sft_half)\n\n        # load pre-trained stylegan2 model if necessary\n        if decoder_load_path:\n            self.stylegan_decoder.load_state_dict(\n                torch.load(decoder_load_path, map_location=lambda storage, loc: storage)['params_ema'])\n        # fix decoder without updating params\n        if fix_decoder:\n            for _, param in self.stylegan_decoder.named_parameters():\n                param.requires_grad = False\n\n        # for SFT modulations (scale and shift)\n        self.condition_scale = nn.ModuleList()\n        self.condition_shift = nn.ModuleList()\n        for i in range(3, self.log_size + 1):\n            out_channels = channels[f'{2**i}']\n            if sft_half:\n                sft_out_channels = out_channels\n            else:\n                sft_out_channels = out_channels * 2\n            self.condition_scale.append(\n                nn.Sequential(\n                    EqualConv2d(out_channels, out_channels, 3, stride=1, padding=1, bias=True, bias_init_val=0),\n                    ScaledLeakyReLU(0.2),\n                    EqualConv2d(out_channels, sft_out_channels, 3, stride=1, padding=1, bias=True, bias_init_val=1)))\n            self.condition_shift.append(\n                nn.Sequential(\n                    EqualConv2d(out_channels, out_channels, 3, stride=1, padding=1, bias=True, bias_init_val=0),\n                    ScaledLeakyReLU(0.2),\n                    EqualConv2d(out_channels, sft_out_channels, 3, stride=1, padding=1, bias=True, bias_init_val=0)))\n\n    def forward(self, x, return_latents=False, return_rgb=True, randomize_noise=True):\n        \"\"\"Forward function for GFPGANBilinear.\n\n        Args:\n            x (Tensor): Input images.\n            return_latents (bool): Whether to return style latents. Default: False.\n            return_rgb (bool): Whether return intermediate rgb images. Default: True.\n            randomize_noise (bool): Randomize noise, used when 'noise' is False. Default: True.\n        \"\"\"\n        conditions = []\n        unet_skips = []\n        out_rgbs = []\n\n        # encoder\n        feat = self.conv_body_first(x)\n        for i in range(self.log_size - 2):\n            feat = self.conv_body_down[i](feat)\n            unet_skips.insert(0, feat)\n\n        feat = self.final_conv(feat)\n\n        # style code\n        style_code = self.final_linear(feat.view(feat.size(0), -1))\n        if self.different_w:\n            style_code = style_code.view(style_code.size(0), -1, self.num_style_feat)\n\n        # decode\n        for i in range(self.log_size - 2):\n            # add unet skip\n            feat = feat + unet_skips[i]\n            # ResUpLayer\n            feat = self.conv_body_up[i](feat)\n            # generate scale and shift for SFT layers\n            scale = self.condition_scale[i](feat)\n            conditions.append(scale.clone())\n            shift = self.condition_shift[i](feat)\n            conditions.append(shift.clone())\n            # generate rgb images\n            if return_rgb:\n                out_rgbs.append(self.toRGB[i](feat))\n\n        # decoder\n        image, _ = self.stylegan_decoder([style_code],\n                                         conditions,\n                                         return_latents=return_latents,\n                                         input_is_latent=self.input_is_latent,\n                                         randomize_noise=randomize_noise)\n\n        return image, out_rgbs\n", "gfpgan/archs/stylegan2_bilinear_arch.py": "import math\nimport random\nimport torch\nfrom basicsr.ops.fused_act import FusedLeakyReLU, fused_leaky_relu\nfrom basicsr.utils.registry import ARCH_REGISTRY\nfrom torch import nn\nfrom torch.nn import functional as F\n\n\nclass NormStyleCode(nn.Module):\n\n    def forward(self, x):\n        \"\"\"Normalize the style codes.\n\n        Args:\n            x (Tensor): Style codes with shape (b, c).\n\n        Returns:\n            Tensor: Normalized tensor.\n        \"\"\"\n        return x * torch.rsqrt(torch.mean(x**2, dim=1, keepdim=True) + 1e-8)\n\n\nclass EqualLinear(nn.Module):\n    \"\"\"Equalized Linear as StyleGAN2.\n\n    Args:\n        in_channels (int): Size of each sample.\n        out_channels (int): Size of each output sample.\n        bias (bool): If set to ``False``, the layer will not learn an additive\n            bias. Default: ``True``.\n        bias_init_val (float): Bias initialized value. Default: 0.\n        lr_mul (float): Learning rate multiplier. Default: 1.\n        activation (None | str): The activation after ``linear`` operation.\n            Supported: 'fused_lrelu', None. Default: None.\n    \"\"\"\n\n    def __init__(self, in_channels, out_channels, bias=True, bias_init_val=0, lr_mul=1, activation=None):\n        super(EqualLinear, self).__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.lr_mul = lr_mul\n        self.activation = activation\n        if self.activation not in ['fused_lrelu', None]:\n            raise ValueError(f'Wrong activation value in EqualLinear: {activation}'\n                             \"Supported ones are: ['fused_lrelu', None].\")\n        self.scale = (1 / math.sqrt(in_channels)) * lr_mul\n\n        self.weight = nn.Parameter(torch.randn(out_channels, in_channels).div_(lr_mul))\n        if bias:\n            self.bias = nn.Parameter(torch.zeros(out_channels).fill_(bias_init_val))\n        else:\n            self.register_parameter('bias', None)\n\n    def forward(self, x):\n        if self.bias is None:\n            bias = None\n        else:\n            bias = self.bias * self.lr_mul\n        if self.activation == 'fused_lrelu':\n            out = F.linear(x, self.weight * self.scale)\n            out = fused_leaky_relu(out, bias)\n        else:\n            out = F.linear(x, self.weight * self.scale, bias=bias)\n        return out\n\n    def __repr__(self):\n        return (f'{self.__class__.__name__}(in_channels={self.in_channels}, '\n                f'out_channels={self.out_channels}, bias={self.bias is not None})')\n\n\nclass ModulatedConv2d(nn.Module):\n    \"\"\"Modulated Conv2d used in StyleGAN2.\n\n    There is no bias in ModulatedConv2d.\n\n    Args:\n        in_channels (int): Channel number of the input.\n        out_channels (int): Channel number of the output.\n        kernel_size (int): Size of the convolving kernel.\n        num_style_feat (int): Channel number of style features.\n        demodulate (bool): Whether to demodulate in the conv layer.\n            Default: True.\n        sample_mode (str | None): Indicating 'upsample', 'downsample' or None.\n            Default: None.\n        eps (float): A value added to the denominator for numerical stability.\n            Default: 1e-8.\n    \"\"\"\n\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 kernel_size,\n                 num_style_feat,\n                 demodulate=True,\n                 sample_mode=None,\n                 eps=1e-8,\n                 interpolation_mode='bilinear'):\n        super(ModulatedConv2d, self).__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.kernel_size = kernel_size\n        self.demodulate = demodulate\n        self.sample_mode = sample_mode\n        self.eps = eps\n        self.interpolation_mode = interpolation_mode\n        if self.interpolation_mode == 'nearest':\n            self.align_corners = None\n        else:\n            self.align_corners = False\n\n        self.scale = 1 / math.sqrt(in_channels * kernel_size**2)\n        # modulation inside each modulated conv\n        self.modulation = EqualLinear(\n            num_style_feat, in_channels, bias=True, bias_init_val=1, lr_mul=1, activation=None)\n\n        self.weight = nn.Parameter(torch.randn(1, out_channels, in_channels, kernel_size, kernel_size))\n        self.padding = kernel_size // 2\n\n    def forward(self, x, style):\n        \"\"\"Forward function.\n\n        Args:\n            x (Tensor): Tensor with shape (b, c, h, w).\n            style (Tensor): Tensor with shape (b, num_style_feat).\n\n        Returns:\n            Tensor: Modulated tensor after convolution.\n        \"\"\"\n        b, c, h, w = x.shape  # c = c_in\n        # weight modulation\n        style = self.modulation(style).view(b, 1, c, 1, 1)\n        # self.weight: (1, c_out, c_in, k, k); style: (b, 1, c, 1, 1)\n        weight = self.scale * self.weight * style  # (b, c_out, c_in, k, k)\n\n        if self.demodulate:\n            demod = torch.rsqrt(weight.pow(2).sum([2, 3, 4]) + self.eps)\n            weight = weight * demod.view(b, self.out_channels, 1, 1, 1)\n\n        weight = weight.view(b * self.out_channels, c, self.kernel_size, self.kernel_size)\n\n        if self.sample_mode == 'upsample':\n            x = F.interpolate(x, scale_factor=2, mode=self.interpolation_mode, align_corners=self.align_corners)\n        elif self.sample_mode == 'downsample':\n            x = F.interpolate(x, scale_factor=0.5, mode=self.interpolation_mode, align_corners=self.align_corners)\n\n        b, c, h, w = x.shape\n        x = x.view(1, b * c, h, w)\n        # weight: (b*c_out, c_in, k, k), groups=b\n        out = F.conv2d(x, weight, padding=self.padding, groups=b)\n        out = out.view(b, self.out_channels, *out.shape[2:4])\n\n        return out\n\n    def __repr__(self):\n        return (f'{self.__class__.__name__}(in_channels={self.in_channels}, '\n                f'out_channels={self.out_channels}, '\n                f'kernel_size={self.kernel_size}, '\n                f'demodulate={self.demodulate}, sample_mode={self.sample_mode})')\n\n\nclass StyleConv(nn.Module):\n    \"\"\"Style conv.\n\n    Args:\n        in_channels (int): Channel number of the input.\n        out_channels (int): Channel number of the output.\n        kernel_size (int): Size of the convolving kernel.\n        num_style_feat (int): Channel number of style features.\n        demodulate (bool): Whether demodulate in the conv layer. Default: True.\n        sample_mode (str | None): Indicating 'upsample', 'downsample' or None.\n            Default: None.\n    \"\"\"\n\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 kernel_size,\n                 num_style_feat,\n                 demodulate=True,\n                 sample_mode=None,\n                 interpolation_mode='bilinear'):\n        super(StyleConv, self).__init__()\n        self.modulated_conv = ModulatedConv2d(\n            in_channels,\n            out_channels,\n            kernel_size,\n            num_style_feat,\n            demodulate=demodulate,\n            sample_mode=sample_mode,\n            interpolation_mode=interpolation_mode)\n        self.weight = nn.Parameter(torch.zeros(1))  # for noise injection\n        self.activate = FusedLeakyReLU(out_channels)\n\n    def forward(self, x, style, noise=None):\n        # modulate\n        out = self.modulated_conv(x, style)\n        # noise injection\n        if noise is None:\n            b, _, h, w = out.shape\n            noise = out.new_empty(b, 1, h, w).normal_()\n        out = out + self.weight * noise\n        # activation (with bias)\n        out = self.activate(out)\n        return out\n\n\nclass ToRGB(nn.Module):\n    \"\"\"To RGB from features.\n\n    Args:\n        in_channels (int): Channel number of input.\n        num_style_feat (int): Channel number of style features.\n        upsample (bool): Whether to upsample. Default: True.\n    \"\"\"\n\n    def __init__(self, in_channels, num_style_feat, upsample=True, interpolation_mode='bilinear'):\n        super(ToRGB, self).__init__()\n        self.upsample = upsample\n        self.interpolation_mode = interpolation_mode\n        if self.interpolation_mode == 'nearest':\n            self.align_corners = None\n        else:\n            self.align_corners = False\n        self.modulated_conv = ModulatedConv2d(\n            in_channels,\n            3,\n            kernel_size=1,\n            num_style_feat=num_style_feat,\n            demodulate=False,\n            sample_mode=None,\n            interpolation_mode=interpolation_mode)\n        self.bias = nn.Parameter(torch.zeros(1, 3, 1, 1))\n\n    def forward(self, x, style, skip=None):\n        \"\"\"Forward function.\n\n        Args:\n            x (Tensor): Feature tensor with shape (b, c, h, w).\n            style (Tensor): Tensor with shape (b, num_style_feat).\n            skip (Tensor): Base/skip tensor. Default: None.\n\n        Returns:\n            Tensor: RGB images.\n        \"\"\"\n        out = self.modulated_conv(x, style)\n        out = out + self.bias\n        if skip is not None:\n            if self.upsample:\n                skip = F.interpolate(\n                    skip, scale_factor=2, mode=self.interpolation_mode, align_corners=self.align_corners)\n            out = out + skip\n        return out\n\n\nclass ConstantInput(nn.Module):\n    \"\"\"Constant input.\n\n    Args:\n        num_channel (int): Channel number of constant input.\n        size (int): Spatial size of constant input.\n    \"\"\"\n\n    def __init__(self, num_channel, size):\n        super(ConstantInput, self).__init__()\n        self.weight = nn.Parameter(torch.randn(1, num_channel, size, size))\n\n    def forward(self, batch):\n        out = self.weight.repeat(batch, 1, 1, 1)\n        return out\n\n\n@ARCH_REGISTRY.register()\nclass StyleGAN2GeneratorBilinear(nn.Module):\n    \"\"\"StyleGAN2 Generator.\n\n    Args:\n        out_size (int): The spatial size of outputs.\n        num_style_feat (int): Channel number of style features. Default: 512.\n        num_mlp (int): Layer number of MLP style layers. Default: 8.\n        channel_multiplier (int): Channel multiplier for large networks of\n            StyleGAN2. Default: 2.\n        lr_mlp (float): Learning rate multiplier for mlp layers. Default: 0.01.\n        narrow (float): Narrow ratio for channels. Default: 1.0.\n    \"\"\"\n\n    def __init__(self,\n                 out_size,\n                 num_style_feat=512,\n                 num_mlp=8,\n                 channel_multiplier=2,\n                 lr_mlp=0.01,\n                 narrow=1,\n                 interpolation_mode='bilinear'):\n        super(StyleGAN2GeneratorBilinear, self).__init__()\n        # Style MLP layers\n        self.num_style_feat = num_style_feat\n        style_mlp_layers = [NormStyleCode()]\n        for i in range(num_mlp):\n            style_mlp_layers.append(\n                EqualLinear(\n                    num_style_feat, num_style_feat, bias=True, bias_init_val=0, lr_mul=lr_mlp,\n                    activation='fused_lrelu'))\n        self.style_mlp = nn.Sequential(*style_mlp_layers)\n\n        channels = {\n            '4': int(512 * narrow),\n            '8': int(512 * narrow),\n            '16': int(512 * narrow),\n            '32': int(512 * narrow),\n            '64': int(256 * channel_multiplier * narrow),\n            '128': int(128 * channel_multiplier * narrow),\n            '256': int(64 * channel_multiplier * narrow),\n            '512': int(32 * channel_multiplier * narrow),\n            '1024': int(16 * channel_multiplier * narrow)\n        }\n        self.channels = channels\n\n        self.constant_input = ConstantInput(channels['4'], size=4)\n        self.style_conv1 = StyleConv(\n            channels['4'],\n            channels['4'],\n            kernel_size=3,\n            num_style_feat=num_style_feat,\n            demodulate=True,\n            sample_mode=None,\n            interpolation_mode=interpolation_mode)\n        self.to_rgb1 = ToRGB(channels['4'], num_style_feat, upsample=False, interpolation_mode=interpolation_mode)\n\n        self.log_size = int(math.log(out_size, 2))\n        self.num_layers = (self.log_size - 2) * 2 + 1\n        self.num_latent = self.log_size * 2 - 2\n\n        self.style_convs = nn.ModuleList()\n        self.to_rgbs = nn.ModuleList()\n        self.noises = nn.Module()\n\n        in_channels = channels['4']\n        # noise\n        for layer_idx in range(self.num_layers):\n            resolution = 2**((layer_idx + 5) // 2)\n            shape = [1, 1, resolution, resolution]\n            self.noises.register_buffer(f'noise{layer_idx}', torch.randn(*shape))\n        # style convs and to_rgbs\n        for i in range(3, self.log_size + 1):\n            out_channels = channels[f'{2**i}']\n            self.style_convs.append(\n                StyleConv(\n                    in_channels,\n                    out_channels,\n                    kernel_size=3,\n                    num_style_feat=num_style_feat,\n                    demodulate=True,\n                    sample_mode='upsample',\n                    interpolation_mode=interpolation_mode))\n            self.style_convs.append(\n                StyleConv(\n                    out_channels,\n                    out_channels,\n                    kernel_size=3,\n                    num_style_feat=num_style_feat,\n                    demodulate=True,\n                    sample_mode=None,\n                    interpolation_mode=interpolation_mode))\n            self.to_rgbs.append(\n                ToRGB(out_channels, num_style_feat, upsample=True, interpolation_mode=interpolation_mode))\n            in_channels = out_channels\n\n    def make_noise(self):\n        \"\"\"Make noise for noise injection.\"\"\"\n        device = self.constant_input.weight.device\n        noises = [torch.randn(1, 1, 4, 4, device=device)]\n\n        for i in range(3, self.log_size + 1):\n            for _ in range(2):\n                noises.append(torch.randn(1, 1, 2**i, 2**i, device=device))\n\n        return noises\n\n    def get_latent(self, x):\n        return self.style_mlp(x)\n\n    def mean_latent(self, num_latent):\n        latent_in = torch.randn(num_latent, self.num_style_feat, device=self.constant_input.weight.device)\n        latent = self.style_mlp(latent_in).mean(0, keepdim=True)\n        return latent\n\n    def forward(self,\n                styles,\n                input_is_latent=False,\n                noise=None,\n                randomize_noise=True,\n                truncation=1,\n                truncation_latent=None,\n                inject_index=None,\n                return_latents=False):\n        \"\"\"Forward function for StyleGAN2Generator.\n\n        Args:\n            styles (list[Tensor]): Sample codes of styles.\n            input_is_latent (bool): Whether input is latent style.\n                Default: False.\n            noise (Tensor | None): Input noise or None. Default: None.\n            randomize_noise (bool): Randomize noise, used when 'noise' is\n                False. Default: True.\n            truncation (float): TODO. Default: 1.\n            truncation_latent (Tensor | None): TODO. Default: None.\n            inject_index (int | None): The injection index for mixing noise.\n                Default: None.\n            return_latents (bool): Whether to return style latents.\n                Default: False.\n        \"\"\"\n        # style codes -> latents with Style MLP layer\n        if not input_is_latent:\n            styles = [self.style_mlp(s) for s in styles]\n        # noises\n        if noise is None:\n            if randomize_noise:\n                noise = [None] * self.num_layers  # for each style conv layer\n            else:  # use the stored noise\n                noise = [getattr(self.noises, f'noise{i}') for i in range(self.num_layers)]\n        # style truncation\n        if truncation < 1:\n            style_truncation = []\n            for style in styles:\n                style_truncation.append(truncation_latent + truncation * (style - truncation_latent))\n            styles = style_truncation\n        # get style latent with injection\n        if len(styles) == 1:\n            inject_index = self.num_latent\n\n            if styles[0].ndim < 3:\n                # repeat latent code for all the layers\n                latent = styles[0].unsqueeze(1).repeat(1, inject_index, 1)\n            else:  # used for encoder with different latent code for each layer\n                latent = styles[0]\n        elif len(styles) == 2:  # mixing noises\n            if inject_index is None:\n                inject_index = random.randint(1, self.num_latent - 1)\n            latent1 = styles[0].unsqueeze(1).repeat(1, inject_index, 1)\n            latent2 = styles[1].unsqueeze(1).repeat(1, self.num_latent - inject_index, 1)\n            latent = torch.cat([latent1, latent2], 1)\n\n        # main generation\n        out = self.constant_input(latent.shape[0])\n        out = self.style_conv1(out, latent[:, 0], noise=noise[0])\n        skip = self.to_rgb1(out, latent[:, 1])\n\n        i = 1\n        for conv1, conv2, noise1, noise2, to_rgb in zip(self.style_convs[::2], self.style_convs[1::2], noise[1::2],\n                                                        noise[2::2], self.to_rgbs):\n            out = conv1(out, latent[:, i], noise=noise1)\n            out = conv2(out, latent[:, i + 1], noise=noise2)\n            skip = to_rgb(out, latent[:, i + 2], skip)\n            i += 2\n\n        image = skip\n\n        if return_latents:\n            return image, latent\n        else:\n            return image, None\n\n\nclass ScaledLeakyReLU(nn.Module):\n    \"\"\"Scaled LeakyReLU.\n\n    Args:\n        negative_slope (float): Negative slope. Default: 0.2.\n    \"\"\"\n\n    def __init__(self, negative_slope=0.2):\n        super(ScaledLeakyReLU, self).__init__()\n        self.negative_slope = negative_slope\n\n    def forward(self, x):\n        out = F.leaky_relu(x, negative_slope=self.negative_slope)\n        return out * math.sqrt(2)\n\n\nclass EqualConv2d(nn.Module):\n    \"\"\"Equalized Linear as StyleGAN2.\n\n    Args:\n        in_channels (int): Channel number of the input.\n        out_channels (int): Channel number of the output.\n        kernel_size (int): Size of the convolving kernel.\n        stride (int): Stride of the convolution. Default: 1\n        padding (int): Zero-padding added to both sides of the input.\n            Default: 0.\n        bias (bool): If ``True``, adds a learnable bias to the output.\n            Default: ``True``.\n        bias_init_val (float): Bias initialized value. Default: 0.\n    \"\"\"\n\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0, bias=True, bias_init_val=0):\n        super(EqualConv2d, self).__init__()\n        self.in_channels = in_channels\n        self.out_channels = out_channels\n        self.kernel_size = kernel_size\n        self.stride = stride\n        self.padding = padding\n        self.scale = 1 / math.sqrt(in_channels * kernel_size**2)\n\n        self.weight = nn.Parameter(torch.randn(out_channels, in_channels, kernel_size, kernel_size))\n        if bias:\n            self.bias = nn.Parameter(torch.zeros(out_channels).fill_(bias_init_val))\n        else:\n            self.register_parameter('bias', None)\n\n    def forward(self, x):\n        out = F.conv2d(\n            x,\n            self.weight * self.scale,\n            bias=self.bias,\n            stride=self.stride,\n            padding=self.padding,\n        )\n\n        return out\n\n    def __repr__(self):\n        return (f'{self.__class__.__name__}(in_channels={self.in_channels}, '\n                f'out_channels={self.out_channels}, '\n                f'kernel_size={self.kernel_size},'\n                f' stride={self.stride}, padding={self.padding}, '\n                f'bias={self.bias is not None})')\n\n\nclass ConvLayer(nn.Sequential):\n    \"\"\"Conv Layer used in StyleGAN2 Discriminator.\n\n    Args:\n        in_channels (int): Channel number of the input.\n        out_channels (int): Channel number of the output.\n        kernel_size (int): Kernel size.\n        downsample (bool): Whether downsample by a factor of 2.\n            Default: False.\n        bias (bool): Whether with bias. Default: True.\n        activate (bool): Whether use activateion. Default: True.\n    \"\"\"\n\n    def __init__(self,\n                 in_channels,\n                 out_channels,\n                 kernel_size,\n                 downsample=False,\n                 bias=True,\n                 activate=True,\n                 interpolation_mode='bilinear'):\n        layers = []\n        self.interpolation_mode = interpolation_mode\n        # downsample\n        if downsample:\n            if self.interpolation_mode == 'nearest':\n                self.align_corners = None\n            else:\n                self.align_corners = False\n\n            layers.append(\n                torch.nn.Upsample(scale_factor=0.5, mode=interpolation_mode, align_corners=self.align_corners))\n        stride = 1\n        self.padding = kernel_size // 2\n        # conv\n        layers.append(\n            EqualConv2d(\n                in_channels, out_channels, kernel_size, stride=stride, padding=self.padding, bias=bias\n                and not activate))\n        # activation\n        if activate:\n            if bias:\n                layers.append(FusedLeakyReLU(out_channels))\n            else:\n                layers.append(ScaledLeakyReLU(0.2))\n\n        super(ConvLayer, self).__init__(*layers)\n\n\nclass ResBlock(nn.Module):\n    \"\"\"Residual block used in StyleGAN2 Discriminator.\n\n    Args:\n        in_channels (int): Channel number of the input.\n        out_channels (int): Channel number of the output.\n    \"\"\"\n\n    def __init__(self, in_channels, out_channels, interpolation_mode='bilinear'):\n        super(ResBlock, self).__init__()\n\n        self.conv1 = ConvLayer(in_channels, in_channels, 3, bias=True, activate=True)\n        self.conv2 = ConvLayer(\n            in_channels,\n            out_channels,\n            3,\n            downsample=True,\n            interpolation_mode=interpolation_mode,\n            bias=True,\n            activate=True)\n        self.skip = ConvLayer(\n            in_channels,\n            out_channels,\n            1,\n            downsample=True,\n            interpolation_mode=interpolation_mode,\n            bias=False,\n            activate=False)\n\n    def forward(self, x):\n        out = self.conv1(x)\n        out = self.conv2(out)\n        skip = self.skip(x)\n        out = (out + skip) / math.sqrt(2)\n        return out\n", "gfpgan/archs/arcface_arch.py": "import torch.nn as nn\nfrom basicsr.utils.registry import ARCH_REGISTRY\n\n\ndef conv3x3(inplanes, outplanes, stride=1):\n    \"\"\"A simple wrapper for 3x3 convolution with padding.\n\n    Args:\n        inplanes (int): Channel number of inputs.\n        outplanes (int): Channel number of outputs.\n        stride (int): Stride in convolution. Default: 1.\n    \"\"\"\n    return nn.Conv2d(inplanes, outplanes, kernel_size=3, stride=stride, padding=1, bias=False)\n\n\nclass BasicBlock(nn.Module):\n    \"\"\"Basic residual block used in the ResNetArcFace architecture.\n\n    Args:\n        inplanes (int): Channel number of inputs.\n        planes (int): Channel number of outputs.\n        stride (int): Stride in convolution. Default: 1.\n        downsample (nn.Module): The downsample module. Default: None.\n    \"\"\"\n    expansion = 1  # output channel expansion ratio\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass IRBlock(nn.Module):\n    \"\"\"Improved residual block (IR Block) used in the ResNetArcFace architecture.\n\n    Args:\n        inplanes (int): Channel number of inputs.\n        planes (int): Channel number of outputs.\n        stride (int): Stride in convolution. Default: 1.\n        downsample (nn.Module): The downsample module. Default: None.\n        use_se (bool): Whether use the SEBlock (squeeze and excitation block). Default: True.\n    \"\"\"\n    expansion = 1  # output channel expansion ratio\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None, use_se=True):\n        super(IRBlock, self).__init__()\n        self.bn0 = nn.BatchNorm2d(inplanes)\n        self.conv1 = conv3x3(inplanes, inplanes)\n        self.bn1 = nn.BatchNorm2d(inplanes)\n        self.prelu = nn.PReLU()\n        self.conv2 = conv3x3(inplanes, planes, stride)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.downsample = downsample\n        self.stride = stride\n        self.use_se = use_se\n        if self.use_se:\n            self.se = SEBlock(planes)\n\n    def forward(self, x):\n        residual = x\n        out = self.bn0(x)\n        out = self.conv1(out)\n        out = self.bn1(out)\n        out = self.prelu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        if self.use_se:\n            out = self.se(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.prelu(out)\n\n        return out\n\n\nclass Bottleneck(nn.Module):\n    \"\"\"Bottleneck block used in the ResNetArcFace architecture.\n\n    Args:\n        inplanes (int): Channel number of inputs.\n        planes (int): Channel number of outputs.\n        stride (int): Stride in convolution. Default: 1.\n        downsample (nn.Module): The downsample module. Default: None.\n    \"\"\"\n    expansion = 4  # output channel expansion ratio\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(Bottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, planes * self.expansion, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n        self.relu = nn.ReLU(inplace=True)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out += residual\n        out = self.relu(out)\n\n        return out\n\n\nclass SEBlock(nn.Module):\n    \"\"\"The squeeze-and-excitation block (SEBlock) used in the IRBlock.\n\n    Args:\n        channel (int): Channel number of inputs.\n        reduction (int): Channel reduction ration. Default: 16.\n    \"\"\"\n\n    def __init__(self, channel, reduction=16):\n        super(SEBlock, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)  # pool to 1x1 without spatial information\n        self.fc = nn.Sequential(\n            nn.Linear(channel, channel // reduction), nn.PReLU(), nn.Linear(channel // reduction, channel),\n            nn.Sigmoid())\n\n    def forward(self, x):\n        b, c, _, _ = x.size()\n        y = self.avg_pool(x).view(b, c)\n        y = self.fc(y).view(b, c, 1, 1)\n        return x * y\n\n\n@ARCH_REGISTRY.register()\nclass ResNetArcFace(nn.Module):\n    \"\"\"ArcFace with ResNet architectures.\n\n    Ref: ArcFace: Additive Angular Margin Loss for Deep Face Recognition.\n\n    Args:\n        block (str): Block used in the ArcFace architecture.\n        layers (tuple(int)): Block numbers in each layer.\n        use_se (bool): Whether use the SEBlock (squeeze and excitation block). Default: True.\n    \"\"\"\n\n    def __init__(self, block, layers, use_se=True):\n        if block == 'IRBlock':\n            block = IRBlock\n        self.inplanes = 64\n        self.use_se = use_se\n        super(ResNetArcFace, self).__init__()\n\n        self.conv1 = nn.Conv2d(1, 64, kernel_size=3, padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.prelu = nn.PReLU()\n        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n        self.bn4 = nn.BatchNorm2d(512)\n        self.dropout = nn.Dropout()\n        self.fc5 = nn.Linear(512 * 8 * 8, 512)\n        self.bn5 = nn.BatchNorm1d(512)\n\n        # initialization\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.xavier_normal_(m.weight)\n            elif isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.BatchNorm1d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.Linear):\n                nn.init.xavier_normal_(m.weight)\n                nn.init.constant_(m.bias, 0)\n\n    def _make_layer(self, block, planes, num_blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion, kernel_size=1, stride=stride, bias=False),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample, use_se=self.use_se))\n        self.inplanes = planes\n        for _ in range(1, num_blocks):\n            layers.append(block(self.inplanes, planes, use_se=self.use_se))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.prelu(x)\n        x = self.maxpool(x)\n\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        x = self.bn4(x)\n        x = self.dropout(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc5(x)\n        x = self.bn5(x)\n\n        return x\n", "gfpgan/archs/restoreformer_arch.py": "\"\"\"Modified from https://github.com/wzhouxiff/RestoreFormer\n\"\"\"\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nclass VectorQuantizer(nn.Module):\n    \"\"\"\n    see https://github.com/MishaLaskin/vqvae/blob/d761a999e2267766400dc646d82d3ac3657771d4/models/quantizer.py\n    ____________________________________________\n    Discretization bottleneck part of the VQ-VAE.\n    Inputs:\n    - n_e : number of embeddings\n    - e_dim : dimension of embedding\n    - beta : commitment cost used in loss term, beta * ||z_e(x)-sg[e]||^2\n    _____________________________________________\n    \"\"\"\n\n    def __init__(self, n_e, e_dim, beta):\n        super(VectorQuantizer, self).__init__()\n        self.n_e = n_e\n        self.e_dim = e_dim\n        self.beta = beta\n\n        self.embedding = nn.Embedding(self.n_e, self.e_dim)\n        self.embedding.weight.data.uniform_(-1.0 / self.n_e, 1.0 / self.n_e)\n\n    def forward(self, z):\n        \"\"\"\n        Inputs the output of the encoder network z and maps it to a discrete\n        one-hot vector that is the index of the closest embedding vector e_j\n        z (continuous) -> z_q (discrete)\n        z.shape = (batch, channel, height, width)\n        quantization pipeline:\n            1. get encoder input (B,C,H,W)\n            2. flatten input to (B*H*W,C)\n        \"\"\"\n        # reshape z -> (batch, height, width, channel) and flatten\n        z = z.permute(0, 2, 3, 1).contiguous()\n        z_flattened = z.view(-1, self.e_dim)\n        # distances from z to embeddings e_j (z - e)^2 = z^2 + e^2 - 2 e * z\n\n        d = torch.sum(z_flattened ** 2, dim=1, keepdim=True) + \\\n            torch.sum(self.embedding.weight**2, dim=1) - 2 * \\\n            torch.matmul(z_flattened, self.embedding.weight.t())\n\n        # could possible replace this here\n        # #\\start...\n        # find closest encodings\n\n        min_value, min_encoding_indices = torch.min(d, dim=1)\n\n        min_encoding_indices = min_encoding_indices.unsqueeze(1)\n\n        min_encodings = torch.zeros(min_encoding_indices.shape[0], self.n_e).to(z)\n        min_encodings.scatter_(1, min_encoding_indices, 1)\n\n        # dtype min encodings: torch.float32\n        # min_encodings shape: torch.Size([2048, 512])\n        # min_encoding_indices.shape: torch.Size([2048, 1])\n\n        # get quantized latent vectors\n        z_q = torch.matmul(min_encodings, self.embedding.weight).view(z.shape)\n        # .........\\end\n\n        # with:\n        # .........\\start\n        # min_encoding_indices = torch.argmin(d, dim=1)\n        # z_q = self.embedding(min_encoding_indices)\n        # ......\\end......... (TODO)\n\n        # compute loss for embedding\n        loss = torch.mean((z_q.detach() - z)**2) + self.beta * torch.mean((z_q - z.detach())**2)\n\n        # preserve gradients\n        z_q = z + (z_q - z).detach()\n\n        # perplexity\n\n        e_mean = torch.mean(min_encodings, dim=0)\n        perplexity = torch.exp(-torch.sum(e_mean * torch.log(e_mean + 1e-10)))\n\n        # reshape back to match original input shape\n        z_q = z_q.permute(0, 3, 1, 2).contiguous()\n\n        return z_q, loss, (perplexity, min_encodings, min_encoding_indices, d)\n\n    def get_codebook_entry(self, indices, shape):\n        # shape specifying (batch, height, width, channel)\n        # TODO: check for more easy handling with nn.Embedding\n        min_encodings = torch.zeros(indices.shape[0], self.n_e).to(indices)\n        min_encodings.scatter_(1, indices[:, None], 1)\n\n        # get quantized latent vectors\n        z_q = torch.matmul(min_encodings.float(), self.embedding.weight)\n\n        if shape is not None:\n            z_q = z_q.view(shape)\n\n            # reshape back to match original input shape\n            z_q = z_q.permute(0, 3, 1, 2).contiguous()\n\n        return z_q\n\n\n# pytorch_diffusion + derived encoder decoder\ndef nonlinearity(x):\n    # swish\n    return x * torch.sigmoid(x)\n\n\ndef Normalize(in_channels):\n    return torch.nn.GroupNorm(num_groups=32, num_channels=in_channels, eps=1e-6, affine=True)\n\n\nclass Upsample(nn.Module):\n\n    def __init__(self, in_channels, with_conv):\n        super().__init__()\n        self.with_conv = with_conv\n        if self.with_conv:\n            self.conv = torch.nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=1, padding=1)\n\n    def forward(self, x):\n        x = torch.nn.functional.interpolate(x, scale_factor=2.0, mode='nearest')\n        if self.with_conv:\n            x = self.conv(x)\n        return x\n\n\nclass Downsample(nn.Module):\n\n    def __init__(self, in_channels, with_conv):\n        super().__init__()\n        self.with_conv = with_conv\n        if self.with_conv:\n            # no asymmetric padding in torch conv, must do it ourselves\n            self.conv = torch.nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=2, padding=0)\n\n    def forward(self, x):\n        if self.with_conv:\n            pad = (0, 1, 0, 1)\n            x = torch.nn.functional.pad(x, pad, mode='constant', value=0)\n            x = self.conv(x)\n        else:\n            x = torch.nn.functional.avg_pool2d(x, kernel_size=2, stride=2)\n        return x\n\n\nclass ResnetBlock(nn.Module):\n\n    def __init__(self, *, in_channels, out_channels=None, conv_shortcut=False, dropout, temb_channels=512):\n        super().__init__()\n        self.in_channels = in_channels\n        out_channels = in_channels if out_channels is None else out_channels\n        self.out_channels = out_channels\n        self.use_conv_shortcut = conv_shortcut\n\n        self.norm1 = Normalize(in_channels)\n        self.conv1 = torch.nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n        if temb_channels > 0:\n            self.temb_proj = torch.nn.Linear(temb_channels, out_channels)\n        self.norm2 = Normalize(out_channels)\n        self.dropout = torch.nn.Dropout(dropout)\n        self.conv2 = torch.nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1)\n        if self.in_channels != self.out_channels:\n            if self.use_conv_shortcut:\n                self.conv_shortcut = torch.nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)\n            else:\n                self.nin_shortcut = torch.nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0)\n\n    def forward(self, x, temb):\n        h = x\n        h = self.norm1(h)\n        h = nonlinearity(h)\n        h = self.conv1(h)\n\n        if temb is not None:\n            h = h + self.temb_proj(nonlinearity(temb))[:, :, None, None]\n\n        h = self.norm2(h)\n        h = nonlinearity(h)\n        h = self.dropout(h)\n        h = self.conv2(h)\n\n        if self.in_channels != self.out_channels:\n            if self.use_conv_shortcut:\n                x = self.conv_shortcut(x)\n            else:\n                x = self.nin_shortcut(x)\n\n        return x + h\n\n\nclass MultiHeadAttnBlock(nn.Module):\n\n    def __init__(self, in_channels, head_size=1):\n        super().__init__()\n        self.in_channels = in_channels\n        self.head_size = head_size\n        self.att_size = in_channels // head_size\n        assert (in_channels % head_size == 0), 'The size of head should be divided by the number of channels.'\n\n        self.norm1 = Normalize(in_channels)\n        self.norm2 = Normalize(in_channels)\n\n        self.q = torch.nn.Conv2d(in_channels, in_channels, kernel_size=1, stride=1, padding=0)\n        self.k = torch.nn.Conv2d(in_channels, in_channels, kernel_size=1, stride=1, padding=0)\n        self.v = torch.nn.Conv2d(in_channels, in_channels, kernel_size=1, stride=1, padding=0)\n        self.proj_out = torch.nn.Conv2d(in_channels, in_channels, kernel_size=1, stride=1, padding=0)\n        self.num = 0\n\n    def forward(self, x, y=None):\n        h_ = x\n        h_ = self.norm1(h_)\n        if y is None:\n            y = h_\n        else:\n            y = self.norm2(y)\n\n        q = self.q(y)\n        k = self.k(h_)\n        v = self.v(h_)\n\n        # compute attention\n        b, c, h, w = q.shape\n        q = q.reshape(b, self.head_size, self.att_size, h * w)\n        q = q.permute(0, 3, 1, 2)  # b, hw, head, att\n\n        k = k.reshape(b, self.head_size, self.att_size, h * w)\n        k = k.permute(0, 3, 1, 2)\n\n        v = v.reshape(b, self.head_size, self.att_size, h * w)\n        v = v.permute(0, 3, 1, 2)\n\n        q = q.transpose(1, 2)\n        v = v.transpose(1, 2)\n        k = k.transpose(1, 2).transpose(2, 3)\n\n        scale = int(self.att_size)**(-0.5)\n        q.mul_(scale)\n        w_ = torch.matmul(q, k)\n        w_ = F.softmax(w_, dim=3)\n\n        w_ = w_.matmul(v)\n\n        w_ = w_.transpose(1, 2).contiguous()  # [b, h*w, head, att]\n        w_ = w_.view(b, h, w, -1)\n        w_ = w_.permute(0, 3, 1, 2)\n\n        w_ = self.proj_out(w_)\n\n        return x + w_\n\n\nclass MultiHeadEncoder(nn.Module):\n\n    def __init__(self,\n                 ch,\n                 out_ch,\n                 ch_mult=(1, 2, 4, 8),\n                 num_res_blocks=2,\n                 attn_resolutions=(16, ),\n                 dropout=0.0,\n                 resamp_with_conv=True,\n                 in_channels=3,\n                 resolution=512,\n                 z_channels=256,\n                 double_z=True,\n                 enable_mid=True,\n                 head_size=1,\n                 **ignore_kwargs):\n        super().__init__()\n        self.ch = ch\n        self.temb_ch = 0\n        self.num_resolutions = len(ch_mult)\n        self.num_res_blocks = num_res_blocks\n        self.resolution = resolution\n        self.in_channels = in_channels\n        self.enable_mid = enable_mid\n\n        # downsampling\n        self.conv_in = torch.nn.Conv2d(in_channels, self.ch, kernel_size=3, stride=1, padding=1)\n\n        curr_res = resolution\n        in_ch_mult = (1, ) + tuple(ch_mult)\n        self.down = nn.ModuleList()\n        for i_level in range(self.num_resolutions):\n            block = nn.ModuleList()\n            attn = nn.ModuleList()\n            block_in = ch * in_ch_mult[i_level]\n            block_out = ch * ch_mult[i_level]\n            for i_block in range(self.num_res_blocks):\n                block.append(\n                    ResnetBlock(\n                        in_channels=block_in, out_channels=block_out, temb_channels=self.temb_ch, dropout=dropout))\n                block_in = block_out\n                if curr_res in attn_resolutions:\n                    attn.append(MultiHeadAttnBlock(block_in, head_size))\n            down = nn.Module()\n            down.block = block\n            down.attn = attn\n            if i_level != self.num_resolutions - 1:\n                down.downsample = Downsample(block_in, resamp_with_conv)\n                curr_res = curr_res // 2\n            self.down.append(down)\n\n        # middle\n        if self.enable_mid:\n            self.mid = nn.Module()\n            self.mid.block_1 = ResnetBlock(\n                in_channels=block_in, out_channels=block_in, temb_channels=self.temb_ch, dropout=dropout)\n            self.mid.attn_1 = MultiHeadAttnBlock(block_in, head_size)\n            self.mid.block_2 = ResnetBlock(\n                in_channels=block_in, out_channels=block_in, temb_channels=self.temb_ch, dropout=dropout)\n\n        # end\n        self.norm_out = Normalize(block_in)\n        self.conv_out = torch.nn.Conv2d(\n            block_in, 2 * z_channels if double_z else z_channels, kernel_size=3, stride=1, padding=1)\n\n    def forward(self, x):\n        hs = {}\n        # timestep embedding\n        temb = None\n\n        # downsampling\n        h = self.conv_in(x)\n        hs['in'] = h\n        for i_level in range(self.num_resolutions):\n            for i_block in range(self.num_res_blocks):\n                h = self.down[i_level].block[i_block](h, temb)\n                if len(self.down[i_level].attn) > 0:\n                    h = self.down[i_level].attn[i_block](h)\n\n            if i_level != self.num_resolutions - 1:\n                # hs.append(h)\n                hs['block_' + str(i_level)] = h\n                h = self.down[i_level].downsample(h)\n\n        # middle\n        # h = hs[-1]\n        if self.enable_mid:\n            h = self.mid.block_1(h, temb)\n            hs['block_' + str(i_level) + '_atten'] = h\n            h = self.mid.attn_1(h)\n            h = self.mid.block_2(h, temb)\n            hs['mid_atten'] = h\n\n        # end\n        h = self.norm_out(h)\n        h = nonlinearity(h)\n        h = self.conv_out(h)\n        # hs.append(h)\n        hs['out'] = h\n\n        return hs\n\n\nclass MultiHeadDecoder(nn.Module):\n\n    def __init__(self,\n                 ch,\n                 out_ch,\n                 ch_mult=(1, 2, 4, 8),\n                 num_res_blocks=2,\n                 attn_resolutions=(16, ),\n                 dropout=0.0,\n                 resamp_with_conv=True,\n                 in_channels=3,\n                 resolution=512,\n                 z_channels=256,\n                 give_pre_end=False,\n                 enable_mid=True,\n                 head_size=1,\n                 **ignorekwargs):\n        super().__init__()\n        self.ch = ch\n        self.temb_ch = 0\n        self.num_resolutions = len(ch_mult)\n        self.num_res_blocks = num_res_blocks\n        self.resolution = resolution\n        self.in_channels = in_channels\n        self.give_pre_end = give_pre_end\n        self.enable_mid = enable_mid\n\n        # compute in_ch_mult, block_in and curr_res at lowest res\n        block_in = ch * ch_mult[self.num_resolutions - 1]\n        curr_res = resolution // 2**(self.num_resolutions - 1)\n        self.z_shape = (1, z_channels, curr_res, curr_res)\n        print('Working with z of shape {} = {} dimensions.'.format(self.z_shape, np.prod(self.z_shape)))\n\n        # z to block_in\n        self.conv_in = torch.nn.Conv2d(z_channels, block_in, kernel_size=3, stride=1, padding=1)\n\n        # middle\n        if self.enable_mid:\n            self.mid = nn.Module()\n            self.mid.block_1 = ResnetBlock(\n                in_channels=block_in, out_channels=block_in, temb_channels=self.temb_ch, dropout=dropout)\n            self.mid.attn_1 = MultiHeadAttnBlock(block_in, head_size)\n            self.mid.block_2 = ResnetBlock(\n                in_channels=block_in, out_channels=block_in, temb_channels=self.temb_ch, dropout=dropout)\n\n        # upsampling\n        self.up = nn.ModuleList()\n        for i_level in reversed(range(self.num_resolutions)):\n            block = nn.ModuleList()\n            attn = nn.ModuleList()\n            block_out = ch * ch_mult[i_level]\n            for i_block in range(self.num_res_blocks + 1):\n                block.append(\n                    ResnetBlock(\n                        in_channels=block_in, out_channels=block_out, temb_channels=self.temb_ch, dropout=dropout))\n                block_in = block_out\n                if curr_res in attn_resolutions:\n                    attn.append(MultiHeadAttnBlock(block_in, head_size))\n            up = nn.Module()\n            up.block = block\n            up.attn = attn\n            if i_level != 0:\n                up.upsample = Upsample(block_in, resamp_with_conv)\n                curr_res = curr_res * 2\n            self.up.insert(0, up)  # prepend to get consistent order\n\n        # end\n        self.norm_out = Normalize(block_in)\n        self.conv_out = torch.nn.Conv2d(block_in, out_ch, kernel_size=3, stride=1, padding=1)\n\n    def forward(self, z):\n        # assert z.shape[1:] == self.z_shape[1:]\n        self.last_z_shape = z.shape\n\n        # timestep embedding\n        temb = None\n\n        # z to block_in\n        h = self.conv_in(z)\n\n        # middle\n        if self.enable_mid:\n            h = self.mid.block_1(h, temb)\n            h = self.mid.attn_1(h)\n            h = self.mid.block_2(h, temb)\n\n        # upsampling\n        for i_level in reversed(range(self.num_resolutions)):\n            for i_block in range(self.num_res_blocks + 1):\n                h = self.up[i_level].block[i_block](h, temb)\n                if len(self.up[i_level].attn) > 0:\n                    h = self.up[i_level].attn[i_block](h)\n            if i_level != 0:\n                h = self.up[i_level].upsample(h)\n\n        # end\n        if self.give_pre_end:\n            return h\n\n        h = self.norm_out(h)\n        h = nonlinearity(h)\n        h = self.conv_out(h)\n        return h\n\n\nclass MultiHeadDecoderTransformer(nn.Module):\n\n    def __init__(self,\n                 ch,\n                 out_ch,\n                 ch_mult=(1, 2, 4, 8),\n                 num_res_blocks=2,\n                 attn_resolutions=(16, ),\n                 dropout=0.0,\n                 resamp_with_conv=True,\n                 in_channels=3,\n                 resolution=512,\n                 z_channels=256,\n                 give_pre_end=False,\n                 enable_mid=True,\n                 head_size=1,\n                 **ignorekwargs):\n        super().__init__()\n        self.ch = ch\n        self.temb_ch = 0\n        self.num_resolutions = len(ch_mult)\n        self.num_res_blocks = num_res_blocks\n        self.resolution = resolution\n        self.in_channels = in_channels\n        self.give_pre_end = give_pre_end\n        self.enable_mid = enable_mid\n\n        # compute in_ch_mult, block_in and curr_res at lowest res\n        block_in = ch * ch_mult[self.num_resolutions - 1]\n        curr_res = resolution // 2**(self.num_resolutions - 1)\n        self.z_shape = (1, z_channels, curr_res, curr_res)\n        print('Working with z of shape {} = {} dimensions.'.format(self.z_shape, np.prod(self.z_shape)))\n\n        # z to block_in\n        self.conv_in = torch.nn.Conv2d(z_channels, block_in, kernel_size=3, stride=1, padding=1)\n\n        # middle\n        if self.enable_mid:\n            self.mid = nn.Module()\n            self.mid.block_1 = ResnetBlock(\n                in_channels=block_in, out_channels=block_in, temb_channels=self.temb_ch, dropout=dropout)\n            self.mid.attn_1 = MultiHeadAttnBlock(block_in, head_size)\n            self.mid.block_2 = ResnetBlock(\n                in_channels=block_in, out_channels=block_in, temb_channels=self.temb_ch, dropout=dropout)\n\n        # upsampling\n        self.up = nn.ModuleList()\n        for i_level in reversed(range(self.num_resolutions)):\n            block = nn.ModuleList()\n            attn = nn.ModuleList()\n            block_out = ch * ch_mult[i_level]\n            for i_block in range(self.num_res_blocks + 1):\n                block.append(\n                    ResnetBlock(\n                        in_channels=block_in, out_channels=block_out, temb_channels=self.temb_ch, dropout=dropout))\n                block_in = block_out\n                if curr_res in attn_resolutions:\n                    attn.append(MultiHeadAttnBlock(block_in, head_size))\n            up = nn.Module()\n            up.block = block\n            up.attn = attn\n            if i_level != 0:\n                up.upsample = Upsample(block_in, resamp_with_conv)\n                curr_res = curr_res * 2\n            self.up.insert(0, up)  # prepend to get consistent order\n\n        # end\n        self.norm_out = Normalize(block_in)\n        self.conv_out = torch.nn.Conv2d(block_in, out_ch, kernel_size=3, stride=1, padding=1)\n\n    def forward(self, z, hs):\n        # assert z.shape[1:] == self.z_shape[1:]\n        # self.last_z_shape = z.shape\n\n        # timestep embedding\n        temb = None\n\n        # z to block_in\n        h = self.conv_in(z)\n\n        # middle\n        if self.enable_mid:\n            h = self.mid.block_1(h, temb)\n            h = self.mid.attn_1(h, hs['mid_atten'])\n            h = self.mid.block_2(h, temb)\n\n        # upsampling\n        for i_level in reversed(range(self.num_resolutions)):\n            for i_block in range(self.num_res_blocks + 1):\n                h = self.up[i_level].block[i_block](h, temb)\n                if len(self.up[i_level].attn) > 0:\n                    h = self.up[i_level].attn[i_block](h, hs['block_' + str(i_level) + '_atten'])\n                    # hfeature = h.clone()\n            if i_level != 0:\n                h = self.up[i_level].upsample(h)\n\n        # end\n        if self.give_pre_end:\n            return h\n\n        h = self.norm_out(h)\n        h = nonlinearity(h)\n        h = self.conv_out(h)\n        return h\n\n\nclass RestoreFormer(nn.Module):\n\n    def __init__(self,\n                 n_embed=1024,\n                 embed_dim=256,\n                 ch=64,\n                 out_ch=3,\n                 ch_mult=(1, 2, 2, 4, 4, 8),\n                 num_res_blocks=2,\n                 attn_resolutions=(16, ),\n                 dropout=0.0,\n                 in_channels=3,\n                 resolution=512,\n                 z_channels=256,\n                 double_z=False,\n                 enable_mid=True,\n                 fix_decoder=False,\n                 fix_codebook=True,\n                 fix_encoder=False,\n                 head_size=8):\n        super(RestoreFormer, self).__init__()\n\n        self.encoder = MultiHeadEncoder(\n            ch=ch,\n            out_ch=out_ch,\n            ch_mult=ch_mult,\n            num_res_blocks=num_res_blocks,\n            attn_resolutions=attn_resolutions,\n            dropout=dropout,\n            in_channels=in_channels,\n            resolution=resolution,\n            z_channels=z_channels,\n            double_z=double_z,\n            enable_mid=enable_mid,\n            head_size=head_size)\n        self.decoder = MultiHeadDecoderTransformer(\n            ch=ch,\n            out_ch=out_ch,\n            ch_mult=ch_mult,\n            num_res_blocks=num_res_blocks,\n            attn_resolutions=attn_resolutions,\n            dropout=dropout,\n            in_channels=in_channels,\n            resolution=resolution,\n            z_channels=z_channels,\n            enable_mid=enable_mid,\n            head_size=head_size)\n\n        self.quantize = VectorQuantizer(n_embed, embed_dim, beta=0.25)\n\n        self.quant_conv = torch.nn.Conv2d(z_channels, embed_dim, 1)\n        self.post_quant_conv = torch.nn.Conv2d(embed_dim, z_channels, 1)\n\n        if fix_decoder:\n            for _, param in self.decoder.named_parameters():\n                param.requires_grad = False\n            for _, param in self.post_quant_conv.named_parameters():\n                param.requires_grad = False\n            for _, param in self.quantize.named_parameters():\n                param.requires_grad = False\n        elif fix_codebook:\n            for _, param in self.quantize.named_parameters():\n                param.requires_grad = False\n\n        if fix_encoder:\n            for _, param in self.encoder.named_parameters():\n                param.requires_grad = False\n\n    def encode(self, x):\n\n        hs = self.encoder(x)\n        h = self.quant_conv(hs['out'])\n        quant, emb_loss, info = self.quantize(h)\n        return quant, emb_loss, info, hs\n\n    def decode(self, quant, hs):\n        quant = self.post_quant_conv(quant)\n        dec = self.decoder(quant, hs)\n\n        return dec\n\n    def forward(self, input, **kwargs):\n        quant, diff, info, hs = self.encode(input)\n        dec = self.decode(quant, hs)\n\n        return dec, None\n", "scripts/parse_landmark.py": "import cv2\nimport json\nimport numpy as np\nimport os\nimport torch\nfrom basicsr.utils import FileClient, imfrombytes\nfrom collections import OrderedDict\n\n# ---------------------------- This script is used to parse facial landmarks ------------------------------------- #\n# Configurations\nsave_img = False\nscale = 0.5  # 0.5 for official FFHQ (512x512), 1 for others\nenlarge_ratio = 1.4  # only for eyes\njson_path = 'ffhq-dataset-v2.json'\nface_path = 'datasets/ffhq/ffhq_512.lmdb'\nsave_path = './FFHQ_eye_mouth_landmarks_512.pth'\n\nprint('Load JSON metadata...')\n# use the official json file in FFHQ dataset\nwith open(json_path, 'rb') as f:\n    json_data = json.load(f, object_pairs_hook=OrderedDict)\n\nprint('Open LMDB file...')\n# read ffhq images\nfile_client = FileClient('lmdb', db_paths=face_path)\nwith open(os.path.join(face_path, 'meta_info.txt')) as fin:\n    paths = [line.split('.')[0] for line in fin]\n\nsave_dict = {}\n\nfor item_idx, item in enumerate(json_data.values()):\n    print(f'\\r{item_idx} / {len(json_data)}, {item[\"image\"][\"file_path\"]} ', end='', flush=True)\n\n    # parse landmarks\n    lm = np.array(item['image']['face_landmarks'])\n    lm = lm * scale\n\n    item_dict = {}\n    # get image\n    if save_img:\n        img_bytes = file_client.get(paths[item_idx])\n        img = imfrombytes(img_bytes, float32=True)\n\n    # get landmarks for each component\n    map_left_eye = list(range(36, 42))\n    map_right_eye = list(range(42, 48))\n    map_mouth = list(range(48, 68))\n\n    # eye_left\n    mean_left_eye = np.mean(lm[map_left_eye], 0)  # (x, y)\n    half_len_left_eye = np.max((np.max(np.max(lm[map_left_eye], 0) - np.min(lm[map_left_eye], 0)) / 2, 16))\n    item_dict['left_eye'] = [mean_left_eye[0], mean_left_eye[1], half_len_left_eye]\n    # mean_left_eye[0] = 512 - mean_left_eye[0]  # for testing flip\n    half_len_left_eye *= enlarge_ratio\n    loc_left_eye = np.hstack((mean_left_eye - half_len_left_eye + 1, mean_left_eye + half_len_left_eye)).astype(int)\n    if save_img:\n        eye_left_img = img[loc_left_eye[1]:loc_left_eye[3], loc_left_eye[0]:loc_left_eye[2], :]\n        cv2.imwrite(f'tmp/{item_idx:08d}_eye_left.png', eye_left_img * 255)\n\n    # eye_right\n    mean_right_eye = np.mean(lm[map_right_eye], 0)\n    half_len_right_eye = np.max((np.max(np.max(lm[map_right_eye], 0) - np.min(lm[map_right_eye], 0)) / 2, 16))\n    item_dict['right_eye'] = [mean_right_eye[0], mean_right_eye[1], half_len_right_eye]\n    # mean_right_eye[0] = 512 - mean_right_eye[0]  # # for testing flip\n    half_len_right_eye *= enlarge_ratio\n    loc_right_eye = np.hstack(\n        (mean_right_eye - half_len_right_eye + 1, mean_right_eye + half_len_right_eye)).astype(int)\n    if save_img:\n        eye_right_img = img[loc_right_eye[1]:loc_right_eye[3], loc_right_eye[0]:loc_right_eye[2], :]\n        cv2.imwrite(f'tmp/{item_idx:08d}_eye_right.png', eye_right_img * 255)\n\n    # mouth\n    mean_mouth = np.mean(lm[map_mouth], 0)\n    half_len_mouth = np.max((np.max(np.max(lm[map_mouth], 0) - np.min(lm[map_mouth], 0)) / 2, 16))\n    item_dict['mouth'] = [mean_mouth[0], mean_mouth[1], half_len_mouth]\n    # mean_mouth[0] = 512 - mean_mouth[0]  # for testing flip\n    loc_mouth = np.hstack((mean_mouth - half_len_mouth + 1, mean_mouth + half_len_mouth)).astype(int)\n    if save_img:\n        mouth_img = img[loc_mouth[1]:loc_mouth[3], loc_mouth[0]:loc_mouth[2], :]\n        cv2.imwrite(f'tmp/{item_idx:08d}_mouth.png', mouth_img * 255)\n\n    save_dict[f'{item_idx:08d}'] = item_dict\n\nprint('Save...')\ntorch.save(save_dict, save_path)\n", "scripts/convert_gfpganv_to_clean.py": "import argparse\nimport math\nimport torch\n\nfrom gfpgan.archs.gfpganv1_clean_arch import GFPGANv1Clean\n\n\ndef modify_checkpoint(checkpoint_bilinear, checkpoint_clean):\n    for ori_k, ori_v in checkpoint_bilinear.items():\n        if 'stylegan_decoder' in ori_k:\n            if 'style_mlp' in ori_k:  # style_mlp_layers\n                lr_mul = 0.01\n                prefix, name, idx, var = ori_k.split('.')\n                idx = (int(idx) * 2) - 1\n                crt_k = f'{prefix}.{name}.{idx}.{var}'\n                if var == 'weight':\n                    _, c_in = ori_v.size()\n                    scale = (1 / math.sqrt(c_in)) * lr_mul\n                    crt_v = ori_v * scale * 2**0.5\n                else:\n                    crt_v = ori_v * lr_mul * 2**0.5\n                checkpoint_clean[crt_k] = crt_v\n            elif 'modulation' in ori_k:  # modulation in StyleConv\n                lr_mul = 1\n                crt_k = ori_k\n                var = ori_k.split('.')[-1]\n                if var == 'weight':\n                    _, c_in = ori_v.size()\n                    scale = (1 / math.sqrt(c_in)) * lr_mul\n                    crt_v = ori_v * scale\n                else:\n                    crt_v = ori_v * lr_mul\n                checkpoint_clean[crt_k] = crt_v\n            elif 'style_conv' in ori_k:\n                # StyleConv in style_conv1 and style_convs\n                if 'activate' in ori_k:  # FusedLeakyReLU\n                    # eg. style_conv1.activate.bias\n                    # eg. style_convs.13.activate.bias\n                    split_rlt = ori_k.split('.')\n                    if len(split_rlt) == 4:\n                        prefix, name, _, var = split_rlt\n                        crt_k = f'{prefix}.{name}.{var}'\n                    elif len(split_rlt) == 5:\n                        prefix, name, idx, _, var = split_rlt\n                        crt_k = f'{prefix}.{name}.{idx}.{var}'\n                    crt_v = ori_v * 2**0.5  # 2**0.5 used in FusedLeakyReLU\n                    c = crt_v.size(0)\n                    checkpoint_clean[crt_k] = crt_v.view(1, c, 1, 1)\n                elif 'modulated_conv' in ori_k:\n                    # eg. style_conv1.modulated_conv.weight\n                    # eg. style_convs.13.modulated_conv.weight\n                    _, c_out, c_in, k1, k2 = ori_v.size()\n                    scale = 1 / math.sqrt(c_in * k1 * k2)\n                    crt_k = ori_k\n                    checkpoint_clean[crt_k] = ori_v * scale\n                elif 'weight' in ori_k:\n                    crt_k = ori_k\n                    checkpoint_clean[crt_k] = ori_v * 2**0.5\n            elif 'to_rgb' in ori_k:  # StyleConv in to_rgb1 and to_rgbs\n                if 'modulated_conv' in ori_k:\n                    # eg. to_rgb1.modulated_conv.weight\n                    # eg. to_rgbs.5.modulated_conv.weight\n                    _, c_out, c_in, k1, k2 = ori_v.size()\n                    scale = 1 / math.sqrt(c_in * k1 * k2)\n                    crt_k = ori_k\n                    checkpoint_clean[crt_k] = ori_v * scale\n                else:\n                    crt_k = ori_k\n                    checkpoint_clean[crt_k] = ori_v\n            else:\n                crt_k = ori_k\n                checkpoint_clean[crt_k] = ori_v\n            # end of 'stylegan_decoder'\n        elif 'conv_body_first' in ori_k or 'final_conv' in ori_k:\n            # key name\n            name, _, var = ori_k.split('.')\n            crt_k = f'{name}.{var}'\n            # weight and bias\n            if var == 'weight':\n                c_out, c_in, k1, k2 = ori_v.size()\n                scale = 1 / math.sqrt(c_in * k1 * k2)\n                checkpoint_clean[crt_k] = ori_v * scale * 2**0.5\n            else:\n                checkpoint_clean[crt_k] = ori_v * 2**0.5\n        elif 'conv_body' in ori_k:\n            if 'conv_body_up' in ori_k:\n                ori_k = ori_k.replace('conv2.weight', 'conv2.1.weight')\n                ori_k = ori_k.replace('skip.weight', 'skip.1.weight')\n            name1, idx1, name2, _, var = ori_k.split('.')\n            crt_k = f'{name1}.{idx1}.{name2}.{var}'\n            if name2 == 'skip':\n                c_out, c_in, k1, k2 = ori_v.size()\n                scale = 1 / math.sqrt(c_in * k1 * k2)\n                checkpoint_clean[crt_k] = ori_v * scale / 2**0.5\n            else:\n                if var == 'weight':\n                    c_out, c_in, k1, k2 = ori_v.size()\n                    scale = 1 / math.sqrt(c_in * k1 * k2)\n                    checkpoint_clean[crt_k] = ori_v * scale\n                else:\n                    checkpoint_clean[crt_k] = ori_v\n                if 'conv1' in ori_k:\n                    checkpoint_clean[crt_k] *= 2**0.5\n        elif 'toRGB' in ori_k:\n            crt_k = ori_k\n            if 'weight' in ori_k:\n                c_out, c_in, k1, k2 = ori_v.size()\n                scale = 1 / math.sqrt(c_in * k1 * k2)\n                checkpoint_clean[crt_k] = ori_v * scale\n            else:\n                checkpoint_clean[crt_k] = ori_v\n        elif 'final_linear' in ori_k:\n            crt_k = ori_k\n            if 'weight' in ori_k:\n                _, c_in = ori_v.size()\n                scale = 1 / math.sqrt(c_in)\n                checkpoint_clean[crt_k] = ori_v * scale\n            else:\n                checkpoint_clean[crt_k] = ori_v\n        elif 'condition' in ori_k:\n            crt_k = ori_k\n            if '0.weight' in ori_k:\n                c_out, c_in, k1, k2 = ori_v.size()\n                scale = 1 / math.sqrt(c_in * k1 * k2)\n                checkpoint_clean[crt_k] = ori_v * scale * 2**0.5\n            elif '0.bias' in ori_k:\n                checkpoint_clean[crt_k] = ori_v * 2**0.5\n            elif '2.weight' in ori_k:\n                c_out, c_in, k1, k2 = ori_v.size()\n                scale = 1 / math.sqrt(c_in * k1 * k2)\n                checkpoint_clean[crt_k] = ori_v * scale\n            elif '2.bias' in ori_k:\n                checkpoint_clean[crt_k] = ori_v\n\n    return checkpoint_clean\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--ori_path', type=str, help='Path to the original model')\n    parser.add_argument('--narrow', type=float, default=1)\n    parser.add_argument('--channel_multiplier', type=float, default=2)\n    parser.add_argument('--save_path', type=str)\n    args = parser.parse_args()\n\n    ori_ckpt = torch.load(args.ori_path)['params_ema']\n\n    net = GFPGANv1Clean(\n        512,\n        num_style_feat=512,\n        channel_multiplier=args.channel_multiplier,\n        decoder_load_path=None,\n        fix_decoder=False,\n        # for stylegan decoder\n        num_mlp=8,\n        input_is_latent=True,\n        different_w=True,\n        narrow=args.narrow,\n        sft_half=True)\n    crt_ckpt = net.state_dict()\n\n    crt_ckpt = modify_checkpoint(ori_ckpt, crt_ckpt)\n    print(f'Save to {args.save_path}.')\n    torch.save(dict(params_ema=crt_ckpt), args.save_path, _use_new_zipfile_serialization=False)\n"}